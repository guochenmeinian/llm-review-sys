# How to Turn Your Knowledge Graph Embeddings into Generative Models

Lorenzo Locente

University of Edinburgh, UK

l.loconte@sms.ed.ac.uk

&Nicola Di Mauro

University of Bari, Italy

nicola.dimauro@uniba.it

Robert Peharz

TU Graz, Austria

robert.peharz@tugraz.at

&Antonio Vergari

University of Edinburgh, UK

avergari@ed.ac.uk

###### Abstract

Some of the most successful knowledge graph embedding (KGE) models for link prediction - CP, RESCAL, TuckER, ComplEx - can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as _circuits_ - constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.

## 1 Introduction

Knowledge graphs (KGs) are a popular way to represent structured domain information as directed graphs encoded as collections of triples (subject, predicate, object), where subjects and objects (entities) are nodes in the graph, and predicates their edge labels. For example, the information that the drug "loxoprofen" interacts with the protein "COX2" is represented as a triple (loxoprofen, interacts, COX2) in the biomedical KG gobl-bioklg . As real-world KGs are often incomplete, we are interested in performing reasoning tasks over them while dealing with missing information. The simplest reasoning task is _link prediction_, i.e., querying for the entities that are connected in a KG by an edge labelled with a certain predicate. For instance, we can retrieve all proteins that the drug "loxoprofen" interacts with by asking the query (loxoprofen, interacts,?).

Knowledge graph embedding (KGE) models are state-of-the-art (SOTA) models for link prediction  that map entities and predicates to sub-symbolic representations, which are used to assign a real-valued degree of existence to triples in order to rank them. For example, the SOTA KGE model ComplEx  assigns (loxoprofen, interacts, phosphoric-acid) and (loxoprofen, interacts, COX2) scores 2.3 and 1.3, hence ranking the first higher than the second in our link prediction example.

This simple example, however, also highlights some opportunities that are missed by KGE models. First, triple scores cannot be directly compared across different queries and across different KGE models, as they can be seen as _negative energies_ and not _normalised probabilities over triples_. To establish a sound probabilistic interpretation and therefore have _probabilities instead of scores_ that can be easily interpreted and compared , we would need to compute the normalisationconstant (or partition function), which is impractical for real-world KGs due to their considerable size (see Section2). Therefore, learning KGE models by maximum-likelihood estimation (MLE) would be computationally infeasible, which is the canonical probabilistic way to learn _a generative model over triples_. A generative model would enable us to sample new triples efficiently, e.g., to generate a surrogate KG whose statistics are consistent with the original one or to do data augmentation . Furthermore, traditional KGE models do not provide a principled way to _guarantee the satisfaction of hard constraints_, which are crucial to ensure trustworthy predictions in safety-critical contexts such as biomedical applications. The result is that predictions of these models can blatantly violate simple constraints such as KG schema definitions. For instance, the triple that the SOTA CompLex ranks higher in our example above violates the semantics of "interacts", i.e., such predicate can only hold between drugs (e.g., loxoprofen) and proteins (e.g., COX2) but phosphoric-acid is not a protein.

Contributions.We show that KGE models that have become a de facto standard, such as CompLex and alternatives based on multilinear score functions [47; 40; 66], can be represented as structured computational graphs, named _circuits_. Under this light, **i)** we propose a different interpretation of these computational graphs and their parameters, to retrieve efficient and yet expressive probabilistic models over triples in a KG, which we name _generative KGE circuits_ (GeKCs) (Section4). We show that **ii)** not only GeKCs can be efficiently trained by exact MLE, but learning them with widely-used discriminative objectives [37; 40; 56; 12] also scales far better over large KGs with millions of entities. In addition, **iii)** we are able to sample new triples exactly and efficiently from GeKCs, and propose a novel metric to measure their quality (Section7.3). Furthermore, by leveraging recent theoretical advances in representing circuits , **iv)** we guarantee that predictions at test time will never violate logical constraints such as domain schema definitions by design (Section5). Finally, our experimental results show that these advantages come with no or minimal loss in terms of link prediction accuracy.

## 2 From KGs and embedding models...

KGs and embedding models.A KG \(\) is a directed multi-graph where nodes are entities and edges are labelled with predicates, i.e., elements of two sets \(\) and \(\), respectively. We define \(\) as a collection of triples \((s,r,o)\), where \(s\), \(r\), \(o\) denote the subject, predicate and object, respectively. A _KG embedding_ (KGE) model maps a triple \((s,r,o)\) to a real scalar via a _score function_\(\). A common recipe to construct differentiable score functions for many state-of-the-art KGE models  is to (i) map entities and predicates to _embeddings_ of rank \(d\), i.e., elements of normed vector spaces (e.g., \(^{d}\)), and (ii) combine the embeddings of subject, predicate and object via multilinear maps. This is the case for KGE models such as CP , RESCAL , TuckER , and CompLex  (see Fig.2). For instance, the score function of CompLex is defined as \(_{}(s,r,o)=(_{s},_{r},_{o}})\) where \(_{s},_{r},_{o}^{d}\) are the complex embeddings of subject, predicate and object, \(,\ ,\ \) denotes a trilinear product, \(\) denotes the complex conjugate operator and \(()\) the real part of complex numbers.

Probabilistic loss-derived interpretation.KGE models have been traditionally interpreted as _energy-based models_ (EBMs) [41; 7; 6; 27; 43]: their score function is assumed to compute the negative energy of a triple. This interpretation induces a distribution over possible KGs by associating a Bernoulli variable, whose parameter is determined by the score function, to every triple . Learning EBMs under this perspective requires using contrastive objectives [7; 48; 56], but several recent works observed that to achieve SOTA link prediction results one needs only to predict

Figure 1: **Which KGE models can be used as efficient generative models of triples?** The score functions of popular KGE models such as CompLex, CP, RESCAL and TuckER can be easily represented as circuits (lilac). However, to retrieve a valid probabilistic circuit (PC, in orange) that encodes a probability distribution over triples (GeKCs) we need to either _restrict its activations to be non-negative_ (in blue, see Section4.1) or _square it_ (in red, see Section4.2).

subjects, objects  or more recently also predicates of triples , i.e., to treat KGEs as _discriminative_ classifiers. Specifically, they are learned by minimising a categorical cross-entropy, e.g., by maximising \( p(o s,r)=(s,r,o)-_{o^{}}(s,r, o^{})\) for object prediction. From this perspective, we observe that we can recover an energy-based interpretation if we assume there exist a joint probability distribution \(p\) over three variables \(S,R,O\), denoting respectively subjects, predicates and objects. Written as a Boltzmann distribution, we have that \(p(S=s,R=r,O=o)=((s,r,o))/Z\), where \(Z=_{s^{}}_{r^{}}_{o^{ }}(s^{},r^{},o^{})\) denotes the partition function. This interpretation is apparently in contrast with the traditional one over possible KGs . We reconcile it with ours in Appendix E. Under this view, we can reinterpret and generalise the recently introduced discriminative objectives  as a weighted _pseudo-log-likelihood_ (PLL) 

\[_{}:=_{(s,r,o)}_{s}  p(s r,o)+_{o} p(o s,r)+_{r} p(r s,o)\] (1)

where \(_{s},_{o},_{r}_{+}\) can differently weigh each term, which is a conditional log-probability that can be computed by summing out either \(s\), \(r\) or \(o\), e.g., to compute \( p(o s,r)\) above. Optimisation is usually carried out by mini-batch gradient ascent  and, given a batch of triples \(B\), we have that exactly computing the PLL objective requires time \((|||B|())\) and space \((|||B|)\) to exploit GPU parallelism ,1 where \(()\) denotes the complexity of evaluating the \(\) once.

Note that the PLL objective (Eq. (1)) is a traditional proxy for learning _generative_ models for which it is infeasible to evaluate the _maximum-likelihood estimation_ (MLE) objective2

\[_{}:=_{(s,r,o)} p(s,r,o)=- || Z+_{(s,r,o)}(s,r,o).\] (2)

In theory, evaluating \( p(s,r,o)\) exactly can be done in polynomial time under our three-variable interpretation, as computing \(Z\) requires \((||^{2}||())\) time, but in practice this cost is still prohibitive for real-world KGs. In fact, it would require summing over \(||\) evaluations of the score function \(\), which for FB15k-237 , the small fragment of Freebase , translates to ~\(10^{11}\) evaluations of \(\). This practical bottleneck hinders the generative capabilities of these models and their ability to yield normalised and interpretable probabilities. Next, we show how we can reinterpret KGE score functions as to retrieve a generative model over triples for which computing \(Z\) exactly can be done in time \(((||+||)())\), making renormalisation feasible.

## 3...to Circuits...

In this section, we show that popular and successful KGE models such as CP, RESCAL, TuckER and ComplEx (see Fig. 2 and Section 2), can be viewed as structured computational graphs that can, in principle, enable summing over all possible triples efficiently. Later, to exploit this efficient summation for marginalisation over triple probabilities, we reinterpret the semantics of these computational graphs as to yield circuits that output valid probabilities. We start with the needed background about circuits and show that some score functions can be readily represented as circuits.

**Definition 1** (Circuit ).: A _circuit_\(\) is a parametrized computational graph over variables \(\) encoding a function \(()\) and comprising three kinds of computational units: _input_, _product_, and _sum_. Each product or sum unit \(n\) receives as inputs the outputs of other units, denoted with the set \((n)\). Each unit \(n\) encodes a function \(_{n}\) defined as: (i) \(l_{n}((n))\) if \(n\) is an input unit, where \(l_{n}\) is a function over variables \((n)\), called its _scope_, (ii) \(_{i(n)}_{i}((_{i}))\) if \(n\) is a product unit, and (iii) \(_{i(n)}_{i}_{i}((_{i}))\) if \(n\) is a sum unit, with \(_{i}\) denoting the weighted sum parameters. The scope of a product or sum unit \(n\) is the union of the scopes of its inputs, i.e., \((n)=_{i(n)}(i)\).

Fig. 2 and Fig. A.1 show examples of circuits. Next, we introduce the two structural properties that enable efficient summation and Prop. 1 certifies that the aforementioned KGEs have these properties.

**Definition 2** (Smoothness and Decomposability).: A circuit is _smooth_ if for every sum unit \(n\), its input units depend all on the same variables, i.e, \( i,j(n)(i)=(j)\). A circuit is _decomposable_ if the inputs of every product unit \(n\) depend on disjoint sets of variables, i.e, \( i,j(n) j(i)(j)=\).

**Proposition 1** (Score functions of KGE models as circuits).: The computational graphs of the score functions \(\) of CP, RESCAL, TuckER and ComplEx are smooth and decomposable circuits over \(=\{S,R,O\}\), whose evaluation cost is \(()(||)\), where \(||\) denotes the number of edges in the circuit, also called its size. For example, the size of the circuit for CP is \(|_{}|(d)\).

Appendix A.1 reports the complete proof by construction for the score functions of these models and the circuit sizes, while Fig. 2 illustrates them. Intuitively, since the score functions of the cited KGE models are based on products and sums as operators, they can be represented as circuits where input units map entities and predicates into the corresponding embedding components (similarly to look-up tables). As the inputs of each sum unit are product units that share the same scope \(\{S,R,O\}\) and fully decompose it in their input units, they satisfy smoothness and decomposability (Def. 2).

Smooth and decomposable circuits enable summing over all possible (partial) assignments to \(\) by (i) performing summations at input units over values in their domains, and (ii) evaluating the circuit once in a feed-forward way [13; 76]. This re-interpretation of score functions allows to "push" summations to the input units of a circuit, greatly reducing complexity, as detailed in the following proposition.

**Proposition 2** (Efficient summations).: Let \(\) be a smooth and decomposable circuit over \(=\{S,R,O\}\) that encodes the score function of a KGE model. The sum \(_{s}_{r}_{o}(s,r,o)\) or any other summation over subjects, predicates or objects can be computed in time \(((||+||)||)\).

However, _these sums are in logarithm space_, as we have that \(p(s,r,o)(s,r,o)\) (see Section 2). As a consequence, summation in this context _does not_ correspond to marginalising variables in probability space. This drives us to reinterpret the semantics of these circuit structures as to operate directly in probability space, rather than in logarithm space, i.e., by encoding non-negative functions.

## 4...to Probabilistic Circuits

We now present how to reinterpret the semantics of the computational graphs of KGE score functions to directly output non-negative values for any input. That is, we cast them as _probabilistic circuits_ (PCs) [13; 76; 19]. First, we define our subclass of PCs that encodes a possibly unnormalised probability distribution over triples in a KG, but allows for efficient marginalisation.

**Definition 3** (Generative KGE circuit).: A _generative KGE circuit_ (GeKC) is a smooth and decomposable PC \(_{}\) over variables \(=\{S,R,O\}\) that encodes a probability distribution over triples, i.e., \(_{}(s,r,o) p(s,r,o)\) for any \((s,r,o)\).

Since our GeKCs are smooth and decomposable (Def. 2) they guarantee the efficient computation of \(Z\) in time \(((||+||)|_{}|)\) (Prop. 2). This is in contrast with existing KGE models, for which it would require the evaluation of the whole score function on each possible triple (Section 2). For example, assume a non-negative CP score function \(^{+}_{}(s,r,o)=_{s},_{r},_ {o}_{+}\) for some embeddings \(_{s},_{r},_{o}^{d}\). Then, we can compute \(Z\) by pushing the outer summations inside the trilinear product, i.e., \(Z=_{s}_{s},_{r} _{r},_{o}_{o}\), which can be done in time \(((||+||) d)\). In the following sections, we propose two ways to turn the computational graphs of CP, RESCAL, TuckER and ComplEx into GeKCs without additional space requirements.

Figure 2: **Interpreting the score functions of CP, RESCAL, TuckER, ComplEx as circuits over 2-dimensional embeddings. Input, product and sum units are coloured in purple, orange and blue, respectively. Output sum units are labelled with the score functions, and their parameters are assumed to be \(1\), if not specified. The detailed construction is presented in Appendix A.1. Given a triple \((s,r,o)\), the input units map subject \(s\), predicate \(r\) and object \(o\) to their embedding entries. Then, the products are evaluated before the weighted sum, which outputs the score of the input triple.**

### Non-negative restriction of a score function

The most natural way of casting existing KGE models to GeKCs is by constraining the computational units of their circuit structures (Section3) to output non-negative values only. We will refer to this conversion method as the _non-negative restriction_ of a model. To achieve the non-negative restriction of CP, RESCAL and TuckER we can simply restrict the embedding values and additional parameters in their score functions to be non-negative, as products and sums are operators closed in \(_{+}\). Thus, each input unit \(n\) over variables \(S\) or \(O\) (resp. \(R\)) in a non-negative GeKC encodes a function \(l_{n}\) (Def.1) modelling an unnormalised categorical distribution over entities (resp. predicates). For example, each \(i\)-th entry \(e_{si}\) of the embedding \(_{s}_{+}^{d}\) associated to an entity \(s\) becomes a parameter of the \(i\)-th unnormalised categorical distribution over \(S\). See Fig.C.1 for an example. We denote the non-negative restriction of these KGEs as CP\({}^{+}\), RESCAL\({}^{+}\) and TuckER\({}^{+}\), respectively.

However, deriving CompLEX\({}^{+}\) by restricting the embedding values of CompLex to be non-negative is not sufficient, because its score function includes a subtraction as it operates on complex numbers. To overcome this, we re-parameterise the imaginary part of each complex embedding to be always greater than or equal to its real part. AppendixC.2 details this procedure. Even if this reparametrisation allows for more flexibility, imposing non-negativity on GeKCs can restrict their ability to capture intricate interactions over subjects, predicates and objects given a fixed number of learnable parameters . We empirically confirm this in our experiments in Section7. Therefore, we propose an alternative way of representing KGEs as PCs via _squaring_.

### Squaring the score function

Squaring works by taking the score function of a KGE model \(\), and multiplying it with itself to obtain \(^{2}=\). Note that \(^{2}\) would be guaranteed to be a PC, as it always outputs non-negative values. The challenge is to represent the product of two circuits as a smooth and decomposable PC as to guarantee efficient marginalisation (Prop.2).3 In general, this task is known to be #P-hard .

However, it can be done efficiently if the two circuits are _compatible_, as we further detail in AppendixB.1. Intuitively, the circuit representations of the score functions \(\) of CP, RESCAL, TuckER and CompLex (see Fig.2) are simple enough that every product unit is defined over the same scope \(\{S,R,O\}\) and fully decomposes it on its input units. As such, these circuits can be easily multiplied with any other smooth and decomposable circuit, a property also known as _omni-compatibility_. This property enables us to build the squared version of these KGE models, which we denote as CP\({}^{2}\), RESCAL\({}^{2}\), TuckER\({}^{2}\) and CompLex\({}^{2}\), as PCs that are still smooth and decomposable. Note that these squared GeKCs do allow for negative parameters, and hence can be more expressive. The next theorem, instead, guarantees that we can normalize them efficiently.

**Theorem 1** (Efficient summations on squared GeKCs).: Performing summations as stated in Prop.2 on CP\({}^{2}\), RESCAL\({}^{2}\), TuckER\({}^{2}\) and CompLex\({}^{2}\) can be done in time \(((||+||)||^{2})\).

For instance, the partition function \(Z\) of CP\({}^{2}\) with embedding size \(d\) would require \(((||+||) d^{2})\) operations to be computed, while a simple feed-forward pass for a batch of \(|B|\) triples is still \((|B| d)\). While in this case marginalisation requires an increase in complexity that is quadratic in \(d\), it is still faster than the brute force approach to compute \(Z\) (see Section2 and AppendixC.4.1).

Figure 3: **GeKCs scale better. Time (in seconds) and peak GPU memory (in GiB as bubble sizes) required for computing the PLL objective and back-propagating through it for a single batch on ogbl-wikikg2, by increasing the batch size and number of entities. See AppendixC.4.3 for details.**Quickly distilling KGEs to squared GeKCs.Consider a squared GeKC obtained by initialising its parameters with those of its energy-based KGE counterpart. If the score function of the original KGE model _always_ assigns non-negative scores to triples, then the "distilled" squared GeKC will output the _same exact ranking of the original model for the answers to any link prediction queries_. Although the premise of the non-negativity of the scores might not be guaranteed, we observe that, in practice, learned KGE models do assign positive scores to all or most of the triples of common KGs (see Appendix D). Therefore, we can use this observation to either instantly distil a GeKC or provide a good heuristic to initialise its parameters and fine-tune them (by MLE or PLL maximisation). In both cases, the result is that we can convert the original KGE model into a GeKC that provides comparable probabilities, enable efficient marginalisation, sampling, and the integration of logical constraints with little or no loss of performance for link prediction (Section 7.1).

### On the Training Efficiency of GeKCs

GeKCs also offer an unexpected opportunity to better scale the computation of the PLL objective (Eq. (1)) on very large knowledge graphs. This is because computing the PLL for a batch of \(|B|\) triples with GeKCs obtained via non-negative restriction and by squaring (Sections 4.1 and 4.2) does not require storing a matrix of size \((|||B|)\) to fully exploit GPU parallelism . For instance, in Appendix C.4.2 we show that computing the PLL for CP  with embedding size \(d\) requires time \((|||B| d)\) and additional space \((|||B|)\). On the other hand, for CP\({}^{2}\) (resp. CP\({}^{+}\)) it requires time \(((||+|B|) d^{2})\) (resp. \(((||+|B|) d)\)) and space \((|B| d)\). Table C.1 summarises similar reduced complexities for other instances of GeKCs, such as ComplEx\({}^{+}\) and ComplEx\({}^{2}\). The reduced time and memory requirements with GeKCs allow us to use larger batch sizes and better scale to large knowledge graphs. Fig. 3 clearly highlights this when measuring the time and GPU memory required to train these models on a KG with millions of entities such as ogbl-wikkg2 .

### Sampling new triples with GeKCs

GeKCs only allowing non-negative parameters, such as CP\({}^{+}\), RESCAL\({}^{+}\) and TuckER\({}^{+}\), support _ancestral sampling_ as sum units can be interpreted as marginalised discrete latent variables, similarly to the latent variable interpretation in mixture models [55; 53] (see Appendix C.3 for details). This is however not possible in general for ComplEx\({}^{+}\) and GeKCs obtained by squaring, as negative parameters break this interpretation. Luckily, as these circuits still support efficient marginalisation (Prop. 2 and Thm. 1) and hence also conditioning, we can perform _inverse transform sampling_. That is, to generate a triple \((s,r,o)\), we can sample in an autoregressive fashion, e.g., first \(s p(S)\), then \(r p(R s)\) and \(o p(O s,r)\), hence requiring only three marginalization steps.

## 5 Injection of Logical Constraints

Converting KGE models to PCs provides the opportunity to "embed" logical constraints in the neural link predictor such that (i) predictions are always guaranteed to satisfy the constraints at test time, and (ii) training can still be done by efficient MLE (or PLL). This is in stark contrast with previous approaches for KGEs, which relax the constraints or enforce them only at training time (see Section 6). Consider, as an example, the problem of integrating the logical constraints induced by a schema of a KG, i.e., enforcing that triples not satisfying a _domain constraint_ have probability zero.

**Definition 4** (Domain constraint).: Given a predicate \(r\) and \(_{S}(r),_{O}(r)\) the sets of all subjects and objects that are semantically coherent with respect to \(r\), a _domain constraint_ is a propositional logic formula defined as

\[K_{r} S_{S}(r) R=r O_{O}(r)(_{u _{S}(r)}S=u) R=r(_{v_{O}(r)}O=v).\] (3)

Given \(=\{r_{1},,r_{m}\}\) a set of predicates, the disjunction \(K K_{r_{1}} K_{r_{m}}\) encodes all the domain constraints that are defined in a KG. An input triple \((s,r,o)\) satisfies \(K\), written as \((s,r,o) K\), if \(s_{S}(r)\) and \(o_{O}(r)\). To design GeKCs such as their predictions always satisfy logical constraints (which might not be necessarily domain constraints), we follow Ahmed et al.  and define a score function to represent a probability distribution \(p_{K}\) that assigns probability mass only to triples that satisfy the constraint \(K\), i.e., \(_{}(s,r,o) c_{K}(s,r,o) p_{K}(s,r,o)\). Here, \(_{}\) is a GeKC and \(c_{K}(s,r,o)=\{(s,r,o) K\}\) is an indicator function that ensures that zero mass is assigned to triples violating \(K\). In words, we are "cutting" the support of \(_{}\), as illustrated in Fig. 4.

Computing \(p_{K}(s,r,o)\) exactly but naively would require computing a new partition function \(Z_{K}=_{s^{}}_{r^{}}_{o^{ }}_{o^{}}(_{}(s^{ },r^{},o^{}) c_{K}(s^{},r^{},o^{}))\), which is impractical as previously discussed (Section 2). Instead, we compile \(c_{K}\) as a smooth and decomposable circuit, sometimes called a constraint or logical circuit [19; 1], by leveraging compilers from the _knowledge compilation_ literature [52; 18]. In a nutshell, \(c_{K}\) is another circuit over variables \(S,R,O\) that outputs 1 if an input triple satisfies the encoded logical constraint \(K\) and 0 otherwise. See Def. A.2 for a formal definition of such circuits. Then, similarly to what we have showed for computing squared circuits that enable efficient marginalisation (Section 4.2), the satisfaction of compatibility between a GeKC \(_{}\) and a constraint circuit \(c_{K}\) enable us to compute \(Z_{K}\) efficiently, as certified by the following theorem.

**Theorem 2** (Tractable integration of constraints in GeKCs).: Let \(c_{K}\) be a constraint circuit encoding a logical constraint \(K\) over variables \(\{S,R,O\}\). Then exactly computing the partition function \(Z_{K}\) of the product \(_{}(s,r,o) c_{K}(s,r,o) p_{K}(s,r,o)\) for any GeKC \(_{}\) derived from CP, RESCAL, TuckER or CompLex (Section 4) can be done in time \(((||+||)|_{}||c_{K}|)\).

In Prop. A.1 we show that the compilation of domain constraints \(K\) (Def. 4) is straightforward and results in a constraint circuit \(c_{K}\) having compact size. For example, the size of the constraint circuit encoding the domain constraints of ogl-biokq is approximately \(|c_{K}|=307 10^{3}\). To put this number in perspective, the size of the circuit for CompLex with embedding size \(1000\) is the much larger \(375 10^{6}\). Since \(|c_{K}|\) is much smaller, by the same argument on the efficiency of GeKCs obtained via squaring (Section 4.3) it results that the integration of logical constraints adds a negligible overhead.

## 6 Related Work

SOTA KGEs and current limitations.A plethora of ways to represent and learn KGEs has been proposed, see  for a review. KGEs such as CP and CompLex are still the de facto go-to choices in many applications [5; 12; 56]. Works performing density estimation in embedding space [78; 11] can sample embeddings, but to sample triples one would need to train a decoder. Several works try to modify training for KGEs as to introduce a penalty for triples that do not satisfy given logical constraints [8; 39; 34; 42; 23; 30], or casting it as a min-max game . Unlike our GeKCs (Section 5), none of these approaches guarantee that test-time predictions satisfy the constraints. Moreover, several heuristics have been proposed to calibrate the probabilistic predictions of KGE models ex-post [61; 79]. As showed in , the triple distribution can be modelled autoregressively as \(p(S,R,O)=p(S) p(O S) p(R S,O)\) where each conditional distribution is encoded by a neural network. However, differently from our GeKCs, integrating constraints exactly or computing _any_ marginal (thus conditional) probability is inefficient. KGE models based on non-negative tensor decompositions  are equivalent to GeKCs obtained by non-negative restriction (Section 4.1), but are generally trained by minimizing different non-probabilistic losses.

Circuits.Circuits provide a unifying framework for several tractable probabilistic models such as sum-product networks (SPNs) and hidden Markov models, which are smooth and decomposable

Figure 4: **Injection of domain constraints. Given a circuit \(c_{K}\) encoding domain constraints and a GeKC \(_{}\), the probability assigned by the product circuit \(_{} c_{K}\) to the inconsistent triple showed in Section 1 is 0, and a positive probability is assigned to consistent triples only, e.g., for the interacts predicate those involving drugs (\(\)) as subjects and proteins (\(\)) as objects. Best viewed in colours.**

PCs , as well as compact logical representations [19; 1]. See [75; 13; 17] for an overview. PCs with negative parameters are also called non-monotonic , but are surprisingly not as well investigated as their monotonic counterparts, i.e., PCs with only non-negative parameters, at least from the learning perspective. Similarly to our construction for ComplEx+ (Appendix C.2), Dennis  constrains the output of the non-monotonic sub-circuits of a larger PC to be less than their monotonic counterparts. Squaring a circuit has been investigating for tractably computing several divergences  and is related to the Born-rule of quantum mechanics .

Circuits for relational data.Logical circuits to compile formulas in first-order logic (FOL)  have been used to reason over relational data, e.g. via exchangeability [68; 49]. Other formalisms such as tractable Markov Logic , probabilistic logic bases , relational SPNs [46; 45] and generative clausal networks  use underlying circuit-like structures to represent probabilistic models over a tractable fragment of FOL formulas. These works assume that every atom in a grounded formula is associated to a random variable, also called the possible world semantics in probabilistic logic programs  and databases (PDBs) . In this semantics, TractOR  casts answering complex queries over KGEs as to performing inference in PDBs. Differently from these works, our GeKCs are models defined over only three variables (Section 2). In Appendix E we reconcile these two semantics by interpreting the probability of a triple to be proportional to that of all KGs containing it.

## 7 Empirical Evaluation

We aim to answer the following research questions: **RQ1**) are GeKCs competitive with commonly used KGEs for link prediction? **RQ2**) Does integrating domain constraints in GeKCs benefit training and prediction?; **RQ3**) how good are the triples sampled from GeKCs?

### Link Prediction (RQ1)

Experimental setting.We evaluate GeKCs on standard KG benchmarks for link prediction5: FB15k-237 , WN18RR  and ogbl-biokg , whose statistics can be found in Appendix F.1. As usual [48; 56; 54], we assess the models for predicting objects (queries \((s,r,?)\)) and subjects (queries \((?,r,o)\)), and report their _mean reciprocal rank_ (MRR) and _fraction of hits at \(k\)_ (Hits@\(k\)) (see Appendix F.2). We remark that our aim in this Section is _not to score the new state-of-the-art link prediction performance on these benchmarks_. Instead, we aim to rigorously assess how close GeKCs can be to commonly used and reasonably tuned KGE models. We focus on CP and ComplEx as they currently are the go-to models of choice for link prediction [40; 56; 12]. We compare them against our GeKCs CP\({}^{+}\), ComplEx\({}^{+}\), CP\({}^{2}\) and ComplEx\({}^{2}\) (Section 4). Appendix F.4 collects all the details about the model hyperparameters and training for reproducibility.

Link prediction results.Table 1 reports the MRR and times for all benchmarks and models when trained by PLL or MLE. First, CP\({}^{2}\) and ComplEx\({}^{2}\) achieve competitive scores when compared to CP and ComplEx. Moreover, CP\({}^{2}\) (resp. ComplEx\({}^{2}\)) always outperforms CP\({}^{+}\) (resp. ComplEx\({}^{+}\)), thus providing empirical evidence that negative embedding values are crucial for model expressiveness. Concerning times, Table F.2 shows that squared GeKCs can train much faster on large KGs (see Section 4.3): CP\({}^{2}\) and ComplEx\({}^{2}\) require less than half the training time of CP and ComplEx on ogbl-biokg, while also unexpectedly scoring the current SOTA MRR on it.6 We experiment also on the much larger ogbl-wikikg2 KG , comprising millions of entities. Even more remarkably, we are

    &  &  &  \\   & PLL & MLE & PLL & MLE & PLL & MLE \\  CP & 0.310 & — & **0.105** & — & 0.831 & — \\ CP\({}^{+}\) & 0.237 & 0.230 & 0.027 & 0.026 & 0.496 & 0.501 \\ CP\({}^{2}\) & **0.315** & 0.282 & **0.104** & 0.091 & **0.848** & 0.829 \\  ComplEx & **0.342** & — & **0.471** & — & 0.829 & — \\ ComplEx\({}^{+}\) & 0.214 & 0.205 & 0.030 & 0.029 & 0.503 & 0.516 \\ ComplEx\({}^{2}\) & 0.334 & 0.300 & 0.420 & 0.391 & **0.858** & 0.840 \\   

Table 1: **GeKCs are competitive with their energy-based counterparts. Best average test MRRs of CP, ComplEx and GeKCs trained with the PLL and MLE objectives (Eqs. (1) and (2)). For standard deviations and training times see Table F.2.**able to score an MRR of 0.572 after just ~3 hours with ComplEx2 trained by PLL with a batch size of \(10^{4}\) and embedding size \(d=100\). To put this in context, we were able to score 0.562 with the best configuration of ComplEx_but after ~3 days_, as we could not fit in memory more than a batch size 500.6 The same trends are shown for the Hits@\(k\) (Table F.3) and likelihood (Table F.4) metrics.

Distilling GeKCs.Table F.5 reports the results achieved by CP2 and ComplEx2 initialised with the parameters of learned CP and ComplEx (see Section 4.2) and confirms we can quickly turn an EBM into GeKC, thus inheriting all the perks of being a tractable generative model.

Calibration study.We also measure how well calibrated the predictions of the models in Table 1 are, which is essential to ensure trustworthiness in critical tasks. For example, given a perfectly calibrated model, for all the triples predicted with a probability of 80%, exactly 80% of them would actually exist . On all KGs but WN18RR, GeKCs achieve lower empirical calibration errors  and better calibrated curves than their counterparts, as we report in F.5.3. The worse performance of all models on WN18RR can be explained by the distribution shift that exists between its training and test split, which we better confirm in Section 7.3.

### Integrating Domain Constraints (RQ2)

We focus on ogbl-biokg, as it contains the domain metadata for each entity (i.e., disease, drug, function, protein, or side effect). Given the entity domains allowed for each predicate, we formulate domain constraints as in Def. 4. First, we want to estimate how likely are the models to predict triples that do not satisfy the domain constraints. We focus on ComplEx and ComplEx2, as they have been shown to achieve the best results in Section 7.1 and introduce d-ComplEx2 as the constraints-aware version of ComplEx2 (Section 5). For each test query \((s,r,?)\) (resp. \((?,r,o)\)), we compute the Sem@\(k\) score  as the average percentage of triples in the first \(k\) positions of the rankings of potential object (resp. subject) completions that satisfy the domain constraints (see Section F.2).

Fig. 5 highlights how both ComplEx and ComplEx2 systematically predict object (or subject) completions that violate domain constraints even for large embedding sizes. For instance, a Sem@1 score of 99% (resp. 99.9%) means that ~3200 (resp. ~320) predicted test triples violate domain constraints. While for ComplEx and ComplEx2 there is no theoretical guarantee of consistent predictions with respect to the domain constraints, d-ComplEx2 always guarantee consistent predictions _by design_. Furthermore, we observe a significant improvement in terms of MRR when integrating constraints for smaller embedding sizes, as reported in Fig. 5.

### Quality of sampled triples (RQ3)

Inspired by the literature on evaluating deep generative models for images, we propose a metric akin to the _kernel Inception distance_ to evaluate the quality of the triples we can sample with GeKCs.

**Definition 5** (Kernel triple distance (KTD)).: Given \(,\) two probability distributions over triples, and a positive definite kernel \(k^{h}^{h}\), we define the _kernel triple distance_\((,)\) as

Figure 5: **GeKCs with domain constraints guarantee domain-consistent predictions. Semantic consistency scores (Sem@\(k\))  on ogbl-biokg achieved by ComplEx, ComplEx2 and its integration with domain constraints (d-ComplEx2) (left), and MRRs computed on test queries (right). ComplEx infers 200+ triples violating constraints as the highest scoring completions (\(k=1\)).**

the squared _maximum mean discrepancy_ between triple latent representations obtained via a map \(^{h}\) that projects triples to an \(h\)-dimensional embedding, i.e.,

\[(,)=_{x,x^{}}[k( (x),(x^{}))]+_{y,y^{}}[k((y), (y^{}))]-2_{x,y}[k((x),(y))].\]

An empirical estimate of the KTD score close to zero indicates that there is little difference between the two triple distributions \(\) and \(\) (see Appendix F.3). For images, \(\) is typically chosen as the last embedding of a SOTA neural classifier. We choose \(\) to be the \(L_{2}\)-normed outputs of the product units of a circuit , specifically the SOTA ComplEx learned by Chen et al.  with \(h=4000\). We choose \(k\) as the polynomial kernel \(k(,)=(^{}+1)^{3}\), following Binkowski et al. .

Table 2 shows the empirical KTD scores computed between the test triples and the generated ones, and Fig. F.1 visualises triple embeddings. We employ two baselines: a uniform probability distribution over all possible triples and NNMFAug , the only work to address triple sampling to the best of our knowledge. We also report the KTD scores for training triples as an empirical lower bound. Squared GeKCs achieve lower KTD scores with respect to the ones obtained by non-negative restriction, confirming again a better estimation of the joint distribution. In addition, they achieve far lower KTD scores than all competitors when learning by MLE (Eq. (2)), which justifies its usage as an objective. Lastly, we confirm the distribution shift on WN18RR: training set KTD scores are far from zero, but even in this challenging scenario, ComplEx\({}^{2}\) scores KTD values that are closer to the training ones.

## 8 Conclusions and Future Work

We proposed to re-interpret the representation and learning of widely used KGE models such as CP, RESCAL, TuckER and ComplEx, as generative models, overcoming some of the classical limitation of their usual EBM interpretation (see Sections 1 and 2). GeKC-variants for other KGE models whose scores are multilinear maps can be readily devised in the same way. Moreover, we conjecture that other KGE models defining score functions having a distance-based semantics such as TransE  and RotatE  can be reinterpreted to be GeKCs as well. Our GeKCs open up a number of interesting future directions. First, we plan to investigate how the enhanced efficiency and calibration of GeKCs can help in complex reasoning tasks beyond link prediction . Second, we can leverage the rich literature on learning the structure of circuits  to devise smaller and sparser KGE circuit architectures that better capture the triple distribution or sporting structural properties that can make reasoning tasks other than marginalisation efficient .