# Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs

Ching-An Cheng

Microsoft Research

chinganc@microsoft.com

&Allen Nie1

Stanford

anie@cs.stanford.edu

&Adith Swaminathan1

Netflix

aswaminathan@netflix.com

Equal contribution

###### Abstract

We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate _end-to-end generative optimization_ - using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/.

## 1 Introduction

Computational workflows that integrate large language models (LLMs), machine learning (ML) models, orchestration, retrievers, tools, etc., power many state-of-the-art AI applications : from chatbots , coding assistants , robots , to multi-agent systems . However designing a computational workflow requires laborious engineering because many heterogeneous parameters (e.g. prompts, orchestration code, and ML hyper-parameters) are involved. Moreover, after deployment any erroneous behaviors of the workflow persist unless a developer manually updates it.

We study a class of optimization problems motivated by automating the design and update of computational workflows. Computational workflows produce optimization problems with heterogeneous parameters, rich feedback (e.g. console output and user's verbal responses), and intricate objectives (beyond maximizing a score). Moreover, a workflow can have interdependent steps (e.g. adaptive orchestration, feedback control loops) and/or involve non-differentiable, semi-black-box, stochastic operations (e.g. ML models, simulations) whose behavior cannot be succinctly captured. As a result, the structure of the computation may change as the parameters and the inputs of the workflow vary.

Due to its complexity, computational workflow optimization is usually framed as a black-box  or algorithm configuration  problem, and is tackled by general techniques like Bayesian Optimization , Evolutionary Algorithms , Reinforcement Learning (RL)  using scalar scores as feedback. But one observation of scalar feedback alone does not provide an improvement signal, so these algorithms are very inefficient when the parameter space is large (e.g. codes or natural language prompts). Recently LLM-based optimizers [11; 12; 13; 14; 15; 16] have been proposed as _generative optimizers_ to improve efficiency, leveraging the prior of generative models learned from large pre-training corpora to optimize complex prompts and codes. In this paper, we investigate how _generative optimization_ can be applied more broadly and systematically to optimize a general computational workflow end-to-end. Appendix B discusses related works in generative optimization.

### Toward Efficient End-to-End Optimization of Computational Workflows

Inspired by back-propagation , we take an end-to-end approach to computational workflow optimization. AutoDiff frameworks [18; 19] have scaled the back-propagation algorithm to optimize differentiable workflows (e.g., neural networks) with billions of parameters. We extend the idea of AutoDiff and design a new framework, Trace, for jointly optimizing all of the heterogeneous parameters in general computational workflows, which may not be differentiable.

Trace treats a computational workflow as a graph like a neural network, where nodes are either inputs, parameters or the results of computation steps, and directional edges denote how nodes are created from others. But, instead of gradients, Trace propagates the _execution trace_ of a workflow (which records the intermediate computed results and how they are used), via the notion of minimal subgraph (see Section 3.3). We show that propagating the execution trace subsumes back-propagation for differentiable workflows, and remains applicable even for non-differentiable workflows. Viewing a workflow as a graph and using its execution trace is standard for software engineering; for instance, human developers use such traces to debug distributed systems . Our novel insight is that execution traces also enable end-to-end generative optimization, because they provide the information needed to relate rich feedback to the parameters in general workflows.

### Example of Trace in Action

Trace is available as a Python library with an API inspired by PyTorch . A user declares the parameters to be optimized using a trainable flag, decorates the workflow with node and bundle wrappers, and runs a Trace optimizer - just like how they would declare and train neural networks.

Consider building an AI agent for the Battleship game (Fig. 1). The agent's policy (Fig. 2a) has two components (reason and act), which are chained together to react to different board configurations. The Battleship environment provides feedback (binary reward in texts) to tell if the agent's action hit the hidden ships, and the goal is to hit all hidden ships as fast as possible. Consider how a human programmer might approach the problem. They may run the policy and change the code based on the

Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.

observed feedback. They may rewrite the code a few times to try different heuristics to solve this problem. They will fix any execution errors (e.g., out-of-bounds exceptions) by using stack traces.

Our Trace framework accomplishes the programmer's goal automatically without adding complexity to the Python code. The user declares reason and act as trainable (Fig. 1(a)) and then runs the agent in a PyTorch-like training loop (Fig. 1(b)). During the execution, Trace records a directed acyclic graph (DAG) (Fig. 1(c)) and uses it to compute the execution trace for optimization. Trace also automatically catches errors (e.g., syntax/semantic errors) and can use them as feedback. In Fig. 1, we show what the agent learns as Trace optimizes2 its policy, where the learned policy is evaluated on new randomly generated games for generalization. The agent can quickly improve its performance and learn strategies that are increasingly complex. Remarkably, there is no mention of the specific Battleship environment API, nor details on how the functions reason and act should behave or adapt in Fig. 1(a). The Trace optimizer figures out all the details through interactions as the computational graph unfolds and the feedback on the output is observed. Beyond code as parameters in this example, we also have experiments in Section 5 where prompts and other heterogenous parameters are optimized.

### A New World of Optimization

The design of Trace is based on a new mathematical setup of iterative optimization, which we call Optimization with Trace Oracle (OPTO). In OPTO, an optimizer selects parameters and receives a computational graph as well as feedback on the computed output. Trace is a tool to efficiently convert the optimization of computational workflows into OPTO problems in practice.

We argue that framing computational workflow optimization as OPTO can lead to faster convergence than a black-box approach. We present a constructive proof: We design a general-purpose efficient generative optimizer called OptoPrime, for OPTO. OptoPrime turns OPTO to a sequence of pseudo-algorithm problems. In each iteration of OPTO, we format the execution trace and output feedback as a pseudo-algorithm question and present it to an LLM for solution (GPT-4 using a ReAct-CoT prompt listed in Appendix G). In experiments, we apply OptoPrime unchanged to many disparate applications like prompt optimization, first-order numerical optimization, hyper-parameter tuning, and robot controller design. We find that the general purpose OptoPrime is competitive with specialized optimizers for each domain, e.g., achieving \(10\%\) higher accuracy on BigBenchHard  when optimizing a DSPy  program compared to their hand-designed optimizer.

Trace, OPTO, and OptoPrime together provide the first tractable algorithm for optimizing general computational workflows end-to-end. The Trace framework _a)_ leverages a workflow's structure and _b)_ can incorporate rich feedback beyond scores, extending AutoDiff to complicated, non-differentiable computational workflows. With Trace, we conjecture that "training deep agent networks" (which fluidly mix computation of tensors, LLMs, and other programmable tools) will soon be possible.

Figure 2: Python Code of the Battleship Example. To build a self-adapting agent with Trace, we only need to annotate some empty functions (reason, act) and set up an optimizer following PyTorch semantics. For space, we trim the docstrings of the empty functions with “...” and list them in Appendix J. Trace then builds a DAG as the workflow executes and updates the parameters (see Fig. 1 for the result).

Optimization with Trace Oracle

OPTO is the foundation of Trace. In this section, we define this graph-based abstraction of iterative optimization and discuss how OPTO covers various computational workflow optimization problems.

PreliminaryWe review the definition of a computational graph (see Fig. 1(c)). A computational graph \(g\) is a DAG, where a node represents an object (such as tensors, strings, etc.) and an edge denotes an input-output relationship. We call a node without parents a root and a node without children a leaf, which are the inputs and outputs of the computational graph. In the context of optimization, some inputs are marked as trainable _parameters_, which are denoted as \(\{X_{}\}\). For a node \(X\), its parents are the inputs to an operator that creates \(X\). The descendants of node \(X\) are those that can be reached from \(X\) following the directed edges; the ancestors are defined conversely. Without loss of generality, we suppose that all computational operators have a unitary output3. In this way, we can associate the operator that creates the child node with the child node, and the full computation can be represented compactly as a DAG without explicitly representing the operators. The execution trace of a computational workflow is defined as the sequence of operations and their execution results invoked when computing the output from a set of inputs; execution traces can be expressed as a computational graph as defined above.

### Problem Definition of OPTO

OPTO is an _abstract_ setup of iterative computational workflow optimization. An OPTO problem instance is defined by a tuple \((,,)\), where \(\) is the parameter space, \(\) is the context of the problem, and \(\) is a Trace Oracle. In each iteration, the optimizer selects a parameter \(\), which can be heterogeneous. Then the Trace Oracle \(\) returns a _trace feedback_, denoted as \(=(f,g)\), where \(g\) is the execution trace represented as a DAG (the parameter is contained in the root nodes of \(g\)), and \(f\) is the feedback provided to exactly one of the output nodes of \(g\). Finally, the optimizer uses the trace feedback \(\) to update the parameter according to the context \(\) and proceeds to the next iteration, as shown in Fig. 3.

In OPTO, the output feedback \(f\) is generic, such as scores, gradients, hints/explanations expressed in natural language, and console messages. The context \(\) provides invariant information to interpret the output feedback \(f\) as well as any known side-information, e.g., desired properties of the parameters. The context \(\) is fixed for an OPTO problem instance (similar to an instruction, or a problem definition), whereas the output feedback \(f\) can change with the parameter \(\) and the resulting computation \(g\). For example, \(\) may be "Minimize a loss function" and \(f\) is a loss. Alternatively, \(\) can be open-ended, like "Follow the feedback" and \(f\) describes how an output should be changed. In Section 3.2, we discuss how to define the context and output feedback when constructing OPTO problems in practice. In this paper, we focus on OPTO problems where \(f\) and \(\) can be expressed compactly in text. This covers a wide range of problems , including those with scalar feedback.

OPTO differs from a black-box setup in that the execution trace \(g\) shows the computational path toward the output, which provides information to construct a parameter update direction from \(f\) and \(\). In the minimization example above, when the execution trace \(g\) is missing, it is unclear how the parameter can be improved given only a point evaluation of \(f\). On the other hand, with \(g\) documenting the functions used to create the output, an update direction (e.g., a gradient) can be derived. We highlight that the structure of the computational graph \(g\) returned by the Trace Oracle \(\) can be different in each iteration in the general case (as in Fig. 3) because some workflows can change with different inputs and parameters.

To ground the OPTO setup, we show how OPTO is related to some existing problems with examples. We discuss other examples like hyperparameter tuning and multi-agent systems in Appendix C.

**Example 1** (Neural network with back-propagation).: The parameters are the weights. \(g\) is the neural computational graph and \(f\) is the loss. An example context \(\) can be "Minimize loss". The back-propagation algorithm can be embedded in the OPTO optimizer, e.g., an OPTO optimizer can use \(\) to compute the propagated gradient at each parameter, and apply a gradient descent update.

Figure 3: Iterations of OPTO. When \(\) is selected, the Trace Oracle \(\) returns trace feedback \(=(f,g)\), where \(g\) is a computational graph using \(\) as input and \(f\) is the feedback given to the output of \(g\).

**Example 2** (Rl).: The parameters are the policy. \(g\) is the trajectory (of states, actions, rewards) resulting from running the policy in a Markov decision process; i.e., \(g\) documents the graphical model of how a generated action is applied to the transition, which then returns the observation and reward. \(f\) can be a success flag. \(\) can be "Maximize return (cumulative rewards)" or "Maximize success".

**Example 3** (Prompt Optimization of an LLM Agent).: The parameters are the prompt of an LLM workflow. \(g\) is the computational graph of the agent and \(f\) is the feedback about the agent's behavior (which can be scores or natural language). \(\) can be "Maximize score" or "Follow the feedback".

## 3 Trace: The Next AutoDiff

We design a framework, Trace, to bring OPTO from an abstract concept to reality. Trace provides a lightweight Python4 tool to implement the Trace Oracle of OPTO for optimizing computational workflows. Through the OPTO framing, Trace separates the design of optimizers and domain-specific components so that general-purpose optimizers can be built that work across diverse domains.

### Design of Trace

Trace is designed based on two primitives:

* node is the wrapper of Python objects. When wrapped, a Python object is registered as a unique node in the computational graph of Trace. A node can be set trainable, which would make the node a parameter in OPTO. In addition, when using node to declare a parameter, one can also describe (in natural language) constraints that the parameter should obey.
* bundle is the decorator to turn Python methods into operators. When a function is decorated, its docstring and source code are recorded as the definition of the operator, which infer how the output feedback should change the parameters. Moreover, functions decorated by bundle can be set trainable as well, which means that the code of the decorated method becomes a parameter.5

For any workflow, using Trace involves the following steps (see Fig. 2). First, the user declares the workflow's parameters using node and bundle, and defines the workflow's conceptual blocks as operators using bundle. Then the user creates an OPTO optimizer (such as OptoPrime in Section 4), and optionally provides the context \(\) for the problem. (A default context \(\) of OptoPrime is "Follow the feedback"). In addition, the user defines a mechanism to provide feedback to the computed result (e.g., scores, natural language suggestions, etc.), in analogy to defining a loss function in neural network training. After the declaration, optimization via Trace repeats the following: _1)_ Execute the decorated workflow. As it runs, a DAG is built in the backend, logging the computed results and their connections. _2)_ Initiate the propagation of the output feedback to the parameters by calling backward. (Any execution error is also treated as feedback; see Appendix D.) Internally, Trace extracts the minimal subgraph \(g\) connecting the parameters and the output and sends the OPTO optimizer the trace feedback \(=(f,g)\). _3)_ Call the OPTO optimizer's step method to update the parameters.

### Using Trace Primitives for Effective Execution Tracing

There are many ways to represent a computational workflow as a computational graph, from abstracting the entire process as one big operator to listing all low-level steps as operators in the graph. In Trace, the level of abstraction is decided by how bundle is applied, as all steps underneath bundle are abstracted as one operator. The design of bundle allows tracing most Python codes, except for those modifying the content of an object reference in place. However, such a case can be avoided by first duplicating the object and then applying the modification to the copied object, similar to how a recurrent neural network is implemented.

Different abstraction choices trade off the graph complexity and the description needed for each operator. Abstracting everything into a single operator makes a trivial graph but requires more descriptions to faithfully capture the workflow. On the other hand, not all details matter in optimization, so exposing every low-level operator can make the graph unnecessarily cluttered. The best representation depends on the application and OPTO optimizer at hand. This problem, we believe, is similar to the design of neural network architectures. Here, we suggest defining the operators tomimic the whiteboard system diagram of the computational workflow. This level of abstraction in our experiments strikes a good balance between the ease of documentation and the graph size.

Apart from architecture design, another design question is what information goes into the context \(\) versus the description of each operator? For a _single_ problem, there is no difference in principle; one can choose to provide details of all operators in \(g\) through the context \(\). However, this will require manually crafting a context for every workflow. We suggest instead providing a description of the operators when they are defined using bundle. Then Trace will automatically generate the workflow-specific information while the same context \(\) is shared across _many_ workflows.

### Backward Feedback Propagation: Realizing the Trace Oracle of OPTO

Trace uses a recursive graph traversal algorithm (Algorithm 1) to propagate feedback in the reversed topological ordering. With different propagators, Algorithm 1 can implement various forward-backward schemes including back-propagation.6 We propose a general propagator, Minimal Subgraph Propagator (MSP), in Algorithm 2. MSP propagates the trace feedback \(=(f,g)\), where \(g\) is implemented as a priority queue. Running Algorithm 1 with MSP (Algorithm 2) together implements the Trace Oracle of OPTO, which extracts the _minimal subgraph_ between parameter nodes and output.7 connecting the parameters and an output. Appendix E proves the following:

**Theorem 1**.: _For a graph with \(N\) nodes and maximum degree \(W\), Algorithms 1 and 2 have time complexity \(O(WN^{2} N)\) and space complexity8\(O(WN)\)._

By contrast, back-propagation has time and space complexities of \(O(WNd^{2})\) and \(O(d)\) respectively, where \(d\) is the maximal dimension of tensors. The difference is because in the most general setting of computational graphs and feedback, the propagated feedback (no matter how it is represented) does not have a constant size and needs the full subgraph.

**Theorem 2**.: _For generic computational graphs of \(N\) nodes, in the worst case, the propagated feedback needs a description length \((N)\) to construct an improvement direction._

Despite the worst case complexity of MSP, in practice the difference is negligible. Since MSP only involves merging priority queues of references, most actual computation happens in the forward pass (and also the optimizer's step method). However for very large problems with millions of nodes in the minimal subgraph, we anticipate that computational issues of MSP could arise.

## 4 Design of the First OPTO Optimizer

We introduce an LLM-based generative optimization algorithm OptoPrime for any text-based OPTO problem. We believe that this is one of many possible optimization algorithms for these problems and there is a large space to be explored for identifying efficient optimization methods for OPTO.

Subgraph RepresentationOne core challenge of designing an LLM-based OPTO optimizer is how to represent the execution trace subgraph \(g\) (which can involve various graph structures and heterogenous data) to LLMs, in a way that LLMs can understand and reason about the downstream effects of parameter updates. We leverage the LLMs' remarkable coding and debugging ability .

We present the trace feedback \((f,g)\) computed by Trace as a pseudo-algorithm problem: the subgraph \(g\) is expressed as a report of code execution with information about the computed values and descriptions of operators in \(g\). Then we prompt the LLM to update the parameters in \(g\) based on feedback \(f\) given to the output. Fig. 4 shows an example. It is crucial to note that even though the lines look like an actual program, it is not the real program but the computational graph defined by bundle (see Section 3.2).

Parameter UpdateWe prompt the LLM with a ReAct-CoT style prompt (Appendix G.2) in one query, asking it to generate reasoning based on the graph, and a suggestion on the parameter changes. If the suggestion can be extracted from the response, we update the parameters.

Optimization MemoryOptoPrime optimizes most workflows reasonably well using just the traced graph and feedback, but it can run into issues when single feedback alone is not informative (e.g., the output feedback is rewards, but there is no description of how the rewards are generated). For robustness, we have a basic memory module in OptoPrime, which tracks the past parameter-feedback pairs as in-context examples. See Appendix G for details.

## 5 Experiments

We evaluate the Trace framework with OptoPrime. We implement the state-of-the-art LLM optimizer OPRO  as a baseline; in comparison with OptoPrime, OPRO does not use the execution trace but relies on the memory of parameter and feedback pairs. For these experiments, we use GPT-4-0125-Preview. We run the experiments on a standard PC with \(16\) GB RAM, and Trace introduces no measurable overhead on executing the workflow. We also conduct experiments to compare Trace and OptoPrime with a concurrent AutoDiff-like framework, TextGrad , which was released after Trace was submitted to NeurIPS. We show that TextGrad can be easily implemented as an optimizer in Trace, and OptoPrime achieves similar or better performance than TextGrad while using much less computation time. In the rest of this section, we will denote Trace+OptoPrime simply as Trace. We report the token usages of all approaches in all experiments in Appendix I.1.

### Validating with Numerical Optimization

First, we want to validate if OptoPrime can solve classical differentiable optimization problems, since they are a special case of OPTO. Consider the problem of \(_{x}|h(x)-y^{*}|\) for a target \(y^{*}\). We construct a synthetic task environment that randomly creates \(y^{*}\) and the computational graph of \(h\) with arbitrarily complex connections between numerical variables (see Appendix I.3 for details). We evaluate OptoPrime (denoted as Trace) and a variant that does not see the graph (Trace Masked); both the optimizers do not use memory. The output feedback is "The output should be <larger/smaller>" (this feedback has the same information as the gradient w.r.t. \(h\)). We compare also the performance of Adam optimizer . We run 30 trials over different randomly generated problems. All methods see the same randomness. Trace is able to match the best-in-class Adam; on the other hand, without access to the full computational graph, the feedback-alone optimizer struggles to find \(x^{*}\) (Figure 4(a)).

### Tuning Hyperparameters to Orchestrate Complex Systems

We tested Trace in a traffic control problem, which is an instance of hyper-parameter tuning. We used UXSim  to simulate traffic at a four-way intersection, where the trainable parameters are 2 integers in \(\), which are the green light duration for each direction of traffic flow. The feedback is the estimated delay experienced by all vehicles due to intersections, and the goal of an optimizer is to minimize the delay using the fewest number of traffic simulations. To this end, this optimizer must find the right trade-off for temporally distributed and variable demands. In Fig. 5 we report the performance of a SOTA heuristic from the traffic control literature, SCATS  as well as two black-box optimization techniques: Gaussian Process Minimization (GP)  and Particle Swarm Optimization (PSO) . All methods use the same starting parameters. Trace denotes OptoPrime using memory, and Trace NoMem denotes OptoPrime without memory. We report further details in

Figure 4: An example pseudo-code report generated by Trace for a program of x = Node(-1.0); z = bar(x) * (bar(x)+1) and the objective of \(_{x}z\).

Appendix I.4. GP and PSO appear bad because \(50\) iterations are insufficient for their convergence; given enough iterations, both will eventually perform well. Trace is quickly competitive with the SCATS heuristic, whereas OPRO is not. Moreover, we find that memory is crucial for Trace to perform well for this task. But we note that Trace consumes extra overhead compared to other methods, since Trace has to materialize the resulting computation graph and query an LLM with effectively a longer prompt than that of OPRO.

### Unifying Prompts and Functions Optimization

Many LLM agents today, e.g., specified by LangChain  and DSPy , have many components. These libraries provide optimization tools to optimize a small portion of their workflows, predominantly the prompt that goes into an LLM call. However, for building self-adapting agents that can modify their own behavior, only allowing the change to one part of a workflow but not others can be limiting. In this experiment, we test Trace's ability in joint prompt optimization and code generation. Specifically, we optimize a given DSPy-based LLM agent and tune its three components: the meta-prompt prompt_template, a function create_prompt that modifies the prompt with the current question, and a function extract_answer that post-processes the output of an LLM call.

We set up an end-to-end prompt-and-code optimization pipeline. We use an automatic evaluation function to compare the LLM's output with the ground truth, which requires the LLM agent to generate outputs not only with the correct answer but also in the correct format (following the guidelines of ). We use the Big-Bench Hard dataset  (15 examples for training, 5 for validation, and the rest for testing). We compare Trace with DSPy's COPRO module (which optimizes the meta-prompt). In Table 1, we show that Trace is able to optimize a DSPy program beyond what DSPy's COPRO optimizer can, especially on algorithmic tasks. This result shows how Trace can concretely improve upon existing LLM prompt optimization libraries. We show learned codes in Appendix J.

### Long-Horizon Robot Manipulator Control

We test the ability of Trace to optimize long-horizon workflows with complex dependencies and to "back-propagate through time". We use Trace to train the controller code (in Python) for a simulated Sawyer robot manipulator. We use the Meta-World environment from LLF-Bench  as the simulator and consider three tasks: Reach, Pick-place, and Push. For each task, LLF-Bench provides a task instruction and meaning of the action space, which we use as the context \(\) of the OPTO problem. The observation is a Python dictionary of vectors, indicating the end-effector position, the goal position, the gripper status, etc. The action space is a 4-dimensional vector to control the relative position of the end-effector and the gripper state. In each time step, the LLF-Bench Meta-World simulator returns the observation and natural language feedback to guide the robot. An

    & BBH all & NLP & Algorithmic & & BBH all & NLP & Algorithmic \\
0-shot & (23 tasks) & (12 tasks) & (11 tasks) & 0-shot & (23 tasks) & (12 tasks) & (11 tasks) \\  DSPy & 41.6 & 53.8 & 32.6 & DSPy + CoT & 70.4 & 73.7 & 68.0 \\ DSPy-PO & 55.3 & 69.0 & 45.2 & DSPy-PO + CoT & 71.6 & 73.9 & 70.0 \\  Trace & **59.5** & **70.9** & **51.1** & Trace + CoT & **78.6** & **75.8** & **80.6** \\   

Table 1: End-to-end workflow optimization for an LLM benchmark (Big-Bench Hard) in 0-shot setup. CoT refers to Chain-of-Thought prompting and PO refers to DSPy’s own prompt optimizer (COPRO). We use Trace to optimize a DSPy program, starting from the same program and prompt template specified by DSPy.

Figure 5: Numerical Optimization and Traffic Optimization Results.

episode ends if the robot successfully solves the problem or because of time-out. We consider an episodic training setting. The initial condition for all iterations in training is the same. We evaluate the learned policy in terms of success, starting from 10 held-out initial conditions. The task horizon is 10 steps, which is sufficient for task completion, and each training iteration has one rollout. The output feedback in OPTO is a textual representation of task success. In addition to the controller code, we also decorated the reset and step functions of the gym environment so that the entire rollout can be traced end-to-end. We compare Trace with OPRO; because of the streaming OPTO setting, our OPRO implementation only proposes one candidate in each iteration, which is then evaluated and provided with the output feedback.

The experimental results are summarized in Fig. 6. Trace denotes OptoPrime using memory, and Trace NoMem denotes OptoPrime without memory. We show learned code in Appendix J. OptoPrime is clearly the top-performing optimizer, especially the version with memory. OPRO is able to solve Reach at the start, but its performance degraded over iterations (this instability was mentioned in ) and gets a similar performance as OptoPrime (without memory) in Push. To validate that the performance of OptoPrime is indeed due to using the execution trace, we include an ablation where we mask out the execution trace, which leads to a significant decline in performance and stability. This experiment features the most complex graph structures among all the experiments. The experimental results here are quite impressive, showing that Trace is able to learn a sophisticated control logic in dozens of interactions, not only working on the training initial conditions but also on held-out testing ones too. We discuss some limitations in Appendix I.6.

### Comparison with TextGrad

After the submission of our work to NeurIPS, another AutoDiff-like framework, TextGrad , was released, which shares the same goal of end-to-end optimizing AI workflows as Trace. In comparison, TextGrad propagates text feedback, whereas Trace propagates minimal subgraphs (see Section 3.3). The graph-based design of Trace, which separates the tracing infrastructure and optimization algorithms, makes it more flexible. In fact, we easily implemented TextGrad as an optimizer in the Trace framework, but the reverse is not possible (because TextGrad couples the infrastructure and the optimization algorithm together). In addition, unlike TextGrad, Trace supports jointly optimizing heterogeneous parameters and can be applied to directly trace a given computational workflow without the need to rewrite the workflow using pre-defined templates. Please see Appendix H for more discussion comparing the two frameworks.

In this experiment, we apply Trace to directly decorate the evaluation code released with the TextGrad library and optimize the parameters following their training/evaluation pipeline line-by-line. This experimental design makes the comparison fair by allowing each optimizer to access the same LLM APIs around the same time, and showcases the flexibility of Trace framework to optimize any computational workflow. We pick the Solution optimization [24, Table 2] and Prompt optimization [24, Table 3] for the reasoning tasks experiments. Please see  for details on the exact setup. We compare OptoPrime, TextGrad9, and a reimplementation10 of TextGrad as an optimizer in Trace. We find that all these algorithms achieve similar success rates in these experiments. One noticeable difference is that OptoPrime is about \(3\)x faster wall-clock time than TextGrad since OptoPrime makes a single call to LLM in each optimization step, whereas TextGrad calls linear to the graph's size.

Figure 6: Learning the feedback control policy (code) for a simulated Sawyer manipulator in LLF-Bench Meta-World. In each iteration (x-axis), one episode of rollout (10 steps) is performed, and then the policy is updated. The mean and standard error of the success rate over 10 seeds are shown.

## 6 Limitations

We highlight that Trace, OPTO and OptoPrime are a first step towards end-to-end generative optimization and building self-adapting workflows. They have limitations in their current form. OPTO captures rich feedback, but it is important to specify a solution concept as well as the feedback source. We provide guidance for feedback design in Section 3.2 and discuss notions of optimality in Appendix F. We believe designing feedback will be as important as designing loss function in deep learning, both of which are open research questions. Also, Trace cannot convert all computational workflows into OPTO problems, e.g., stateful functions that modify their state in place cannot be represented as a DAG without modification, and distributed/parallel computing workflows are incompatible with the current implementation (though in theory Trace can run in an asynchronous way so long as the overall graph does not end up with cycles). Finally, while Trace is designed to be generic and future-proof, the OptoPrime optimizer is preliminary. Although we demonstrated that OptoPrime could work well with moderate-size graphs, it is not a provably optimal algorithm and uses more tokens than OPRO, though, in our experiments, OPRO's performance does not improve even when given a large token budget. The debugging ability and context limits of the LLM used in OptoPrime crucially determine the scale of problems that we can practically address today. Consequently, more research is needed for designing token-efficient generative optimization algorithms.

## 7 Conclusion and Future Work

We created Trace that can convert a computational workflow optimization problem into an OPTO problem, and we demonstrated a tractable OPTO optimizer, OptoPrime. This is just a first step towards a new paradigm of end-to-end _generative optimization_, with exciting avenues for future work. We discuss a few selected ones below. Please see Appendix A for a longer discussion.

In OptoPrime, we connect optimization to an LLM's reasoning capability. Techniques that have been proposed to improve LLM reasoning, e.g. Chain-of-Thought , Few-Shot Prompting , Tool Use , and Multi-Agent Workflows  could also help improve OptoPrime or design new OPTO optimizers. We conjecture that a hybrid workflow of LLM and search algorithms, can enable a truly general-purpose OPTO optimizer. Along the way, we must settle on how to delineate the agent vs. the optimizer. How to trade off the generality of optimizer vs. crafting side-information in the context \(\) to achieve task-specific performance is an open question.

In Trace, we chose a specific propagator (MSP), which maximally preserves information for a general computation graph. We can instead specialize it for specific computations, e.g. to accommodate very large graphs with a hierarchical graph representation. Going a step beyond the basic memory module we experimented with in OptoPrime, we anticipate that an optimizer that can reason about how a workflow will behave under counterfactual parameter settings can be more efficient than OptoPrime and can enable a divide-and-conquer approach to OPTO. More research is needed to study the theoretical properties of OPTO (such as optimization landscape and complexity). We hope our preliminary effort in Appendix F can provide some guidance. Finally, in this paper, we focused on output feedback and context that can be compactly textualized. We anticipate that computational workflows with rich non-textual contexts and output feedback will also benefit from automatic generative optimization through appropriate applications of Trace (e.g., with VLMs).

    & OptoPrime & Time & TextGrad & Time & TextGrad & TextGrad (Reported) \\  & (Trace) & & (24-10-30) & Time & (Trace) & (Reported) \\  MMLU-Machine Learning & **86.6** (0.2) & 1.7 (0.6) & 86.1 (0.5) & 3.5 (1.1) & 86.3 (0.2) & \(88.4\) \\ MMLU-College Physics & **94.1** (0.8) & 1.2 (0.3) & 93.1 (0.7) & 2.3 (0.4) & 93.3 (0.6) & \(95.1\) \\ Google-proof QA & **59.6** (1.3) & 12.2 (1.4) & 53.2 (0.6) & 19.5 (1.9) & 54.0 (0.7) & \(55.0\) \\  BBH Counting & **89.4** (0.1) & 55.9 (4.5) & 89.2 (1.2) & 142.9 (9.3) & 87.6 (1.7) & \(91.9\) \\ BBH Word Sorting & 71.6 (3.1) & 82.5 (10.1) & **72.0** (0.4) & 211.1 (16.8) & 71.4 (2.5) & \(79.8\) \\ GSM8K & **82.5** (0.1) & — & 82.4 (0.6) & — & 82.0 (0.2) & \(81.1\) \\   

Table 2: **Comparison between Trace and TextGrad.** The optimizer is GPT-4o-2024-08-06, and the student model is GPT-35-turbo-1106. The results show the mean and the standard error of success rate of the last iterate computed by 5 seeds. The experiment time reported is in minutes (the time involves not just training but also validation and testing by running TextGrad’s original pipeline); the time of GSM8K experiment is omitted as the experiment time (>8hrs) is determined primarily by the evaluation not optimization.