# Group Fairness in Peer Review

Haris Aziz

UNSW Sydney

haris.aziz@unsw.edu.au &Evi Micha

University of Toronto

emicha@cs.toronto.edu &Nisarg Shah

University of Toronto

nisarg@cs.toronto.edu

###### Abstract

Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a notion of group fairness, called the core, which requires that every possible community (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by withdrawing from a large conference.

We study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment. We use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics.

## 1 Introduction

Due to their large scale, conferences like NeurIPS and AAAI use an automated procedure to assign submitted papers to reviewers. Popular such systems include the Toronto Paper Matching System , Microsoft CMT1, and OpenReview2. The authors submitting their works are often very interested in receiving meaningful and helpful feedback from their peers [2; 3; 4]. Thus, their overall experience with the conference heavily depends on the quality of reviews that their submissions receive.

The typical procedure of assigning papers to reviewers is as follows. First, for each paper-reviewer pair, a similarity score is calculated based on various parameters such as the subject area of the paper and the reviewer, the bids placed by the reviewer, etc. [1; 5; 6; 7; 8]. Then, an assignment is calculated through an optimization problem, where the usual objectives are to maximize either the utilitarian social welfare, which is the total similarity score of all matched paper-reviewer pairs, or the egalitarian social welfare, which is the least total score of reviewers assigned to any submission. Relevant constraints are imposed to ensure that each submission receives an appropriate number of reviews, reviewer workloads are respected, and any conflicts of interest are avoided.

Peng et al.  recently mentioned that a major problem with the prestigious mega conferences is that they constitute the main venues for several communities, and as a result, in some cases, people are asked to review submissions that are beyond their main areas of work. They claim that a reasonable solution is to move to a de-centralized publication process by creating more specialized conferences appropriate for different communities. While specialized conferences definitely have their advantages, the maintenance of large conferences that attract multiple communities is also crucial for the emergence of interdisciplinary ideas that can be reviewed by diverse subject experts. Therefore, it is important to ensure that no group has an incentive to break off due to a feeling of being mistreated by the reviewing procedure of a large conference. In this work, we ask whether itis possible to modify the existing reviewing processes to resolve this issue by treating the various communities satisfactorily. We clarify that the goal is not to build roadblocks to the creation of specialized conferences, but rather to mitigate the harm imposed on communities in large conferences.

To answer this, we look toward the literature on algorithmic fairness. Specifically, we adapt the group fairness notion known as _the core_; to the best of our knowledge, we are the first to introduce it to the peer review setting. For a reviewing assignment to be in the core, it must ensure that no community (subset of researchers) can "deviate" by setting up its own conference in which (a) no author reviews her own submission, (b) each submission from within the community is reviewed by just as many reviewers as before, but now from within the community, (c) each reviewer reviews no more papers than before, and (d) the submissions are assigned to better reviewers, making the community happier. Intuitively, this is a notion of group fairness because it ensure that the treatment provided to every group of participants meets their "entitlement" (which is defined by what the group can achieve on its own). It is also a notion of stability (often also known as _core stability_) because it provides no incentives for any community to break off and get isolated by setting up their own conference instead. Note that this definition provides fairness to _every possible_ community, and not only to predefined groups, as is the case for fairness definitions such as demographic parity and equalized odds that are popular in the machine learning literature . In particular, it ensures fair treatment to emerging interdisciplinary communities even before they become visible.

### Our Contribution

We consider a simple peer review model in which each agent submits (as the sole author) a number of papers to a conference and also serves as a potential reviewer. A reviewing assignment is valid if each paper is reviewed by \(k_{p}\) reviewers, each reviewer reviews no more than \(k_{a}\) papers, and no agent reviews her own submissions. To ensure that a valid assignment always exists, we assume that the maximum number of papers that each agent is allowed to submit is at most \( k_{a}/k_{p}\).

In Section 3, we present an efficient algorithm that always returns a valid assignment in the core under minor conditions on the preferences of the authors. Specifically, our algorithm takes as input only the preference ranking of each author over individual potential reviewers for each of her submissions. Then, it produces an assignment that we prove to be in the core for any manner in which the agent's submission-specific preferences over individual reviewers may be extended to preferences over a set of \(k_{p}\) reviewers assigned to each submission, aggregated over submissions, subject to two mild conditions being satisfied.

In Section 4, we conduct experiments with real data from CVPR and ICLR conferences, and evaluate the price that our algorithm must pay -- in lost utilitarian and egalitarian welfare -- in order to satisfy the core and prevent communities from having an incentive to deviate. We also observe that reviewer assignment methods currently used in practice generate such adverse incentives quite often.

### Related Work

As we mentioned above, usually the first step of a review assignment procedure is to calculate a similarity score for each pair of submission and reviewer which aims to capture the expertise of the reviewer for this submission. The problem of identifying the similarity scores has been extensively studied in the literature . In this work, we assume that the similarity scores are given as an input to our algorithm after they have been calculated from a procedure that is considered as a black box. Importantly, our algorithm does not need the exact values of the similarity scores, but it only requires a ranking of the reviewers for each paper, indicating their relative expertise for this paper.

Given, the similarities scores various methods have been proposed for finding a reviewing assignment. The most famous algorithm is the Toronto Paper Matching System  which is a very broadly applied method and focuses on maximizing the utilitarian welfare, i.e. the sum of the similarities across all assigned reviewers and all papers. This approach has been adopted by other popular conference management systems such as EasyChair3 and HotCRP 4. While this approach optimizes the total welfare, it is possible to discriminate against some papers. Therefore, other methods have focused on finding reviewing assignments that are (also) fair across all papers.

In our model, the review assignment problem is related to exchange problems with endowments , since authors can be viewed as being endowed by their own papers which they wish to exchange with other authors that also serve as reviewers. For the basic exchange problem of housing reallocation, Shapley and Scarf  showed that an algorithm called _Top-Trading-Cycle (TTC)_ finds an allocation which is in the core. The first part of our algorithm uses a variation of TTC where the agents (authors) are incorporated with multiple items (submissions), and constraints related to how many items each agent can get and to how many agents one item should be assigned should be satisfied. In contrast to classical exchange problem with endowments, our model has a distinctive requirement that agents/authors need to give away _all_ their items/papers as the papers need to be reviewed by the agent who gets the paper. As we further explain in Section 3, this difference is crucial and requires further action from our algorithm than simply executing this variation of TTC. Various variations of TTC have been considered in the literature, tailored for different variations of the basic problem, but to the best of our knowledge, none of them can be directly applied in our model. To give an example, Suzuki et al.  consider the case that there are multiple copies of the same object and there are some quotas that should be satisfied, but they assume that each agent gets just one object while here each paper is assigned to multiple distinct reviewers.

## 2 Model

For \(q\), define \([q]\{1,,q\}\). There is a set of agents \(N=[n]\). Each agent \(i\) submits a set of papers \(P_{i}=\{p_{i,1},,p_{i,m_{i}}\}\) for review by her peers, where \(m_{i}\), and is available to review the submissions of her peers. We refer to \(p_{i,}\) as the \(\)-th submission of agent \(i\); when considering the special case of each agent \(i\) having a single submission, we will drop \(\) and simply write \(p_{i}\). Let \(P=_{i N}P_{i}\) be the set of all submissions and \(m=_{i N}m_{i}\) be the total number of submissions.

Assignment.Our goal is to produce a _(reviewing) assignment_\(R:N P\{0,1\}\), where \(R(i,j)=1\) if agent \(i N\) is assigned to review submission \(j P\). With slight abuse of notation, let \(R^{a}_{i}=\{j P:R(i,j)=1\}\) be the set of submissions assigned to agent \(i\) and \(R^{p}_{j}=\{i N:R(i,j)=1\}\) be the set of agents assigned to review submission \(j\). We want the assignment to be _valid_, i.e., satisfy the following constraints:

* Each agent must be assigned at most \(k_{a}\) submissions for review, i.e., \(|R^{a}_{i}| k_{a}, i N\).
* Each submission must be assigned to \(k_{p}\) agents, i.e., \(|R^{p}_{j}|=k_{p}, j P\).
* No agent should review one of her own submissions, i.e., \(R(i,p_{i,})=0, i N,[m_{i}]\).

To ensure that a valid assignment always exists, we impose the constraint that \(m_{i} k_{p} k_{a}\) for each \(i N\), which implies that \(m k_{p} n k_{a}\). Intuitively, this demands that each agent submitting papers be willing to provide as many reviews as the number of reviews assigned to the submissions of any single agent. For further discussion on this condition, see Section 5.

Note that given \(N^{} N\) and \(P^{}_{i} P_{i}\) for each \(i N^{}\) with \(P^{}=_{i N^{}}P^{}_{i}\), the validity requirements above can also be extended to a restricted assignment \(:N^{} P^{}\{0,1\}\). Hereinafter, we will assume validity unless specified otherwise or during the process of building an assignment.

**Preferences.** Each agent \(i N\) has a preference ranking, denoted \(_{i,}\), over the agents in \(N\{i\}\) for reviewing her \(\)-th submission \(p_{i,}\).5 These preferences can be based on a mixture of many factors, such as how qualified the other agents are to review submission \(p_{i,}\), how likely they are to provide a positive review for it, etc. Let \(_{i,}(i^{})\) be the position of agent \(i^{} N\{i\}\) in the ranking. We say that agent \(i\) prefers agent \(i^{}\) to agent \(i^{}\) as a reviewer for \(p_{i,}\) if \(_{i,}(i^{})<_{i,}(i^{})\). Again, in the special case where the agents have a single submission each, we drop \(\) and just write \(_{i}\). Let \(=(_{1,1},,_{1,m_{1}},,_{n,1},,_{n,m_{n}})\).

While our algorithm takes \(\) as input, to reason about its guarantees, we need to define agent preferences over assignments by extending \(\). In particular, an agent is assigned a set of reviewers for each of her submissions, so we need to define her preferences over sets of sets of reviewers. First, we extend to preferences over sets of reviewers for a given submission, and then aggregate preferences across different submissions. Instead of assuming a specific parametric extension (e.g., additive preferences), we allow all possible extensions that satisfy two mild constraints; the group fairness guarantee of our algorithm holds with respect to any such extension.

_Extension to a set of reviewers for one submission:_ Let \(S_{i,}S^{}\) (resp., \(S_{i,}S^{}\)) denote that agent \(i\) strictly (resp., weakly) prefers the set of agents \(S\) to the set of agents \(S^{}\) for her \(\)-th submission \(p_{i,}\). We require only that these preferences satisfy the following mild axiom.

**Definition 1** (Order Separability).: For every disjoint \(S_{1},S_{2},S_{3} N\) with \(|S_{1}|=|S_{2}|>0\), if it holds that \(_{i,}(i^{})<_{i,}(i^{})\) for each \(i^{} S_{1}\) and \(i^{} S_{2}\), then we must have \(S_{1} S_{3}_{i,}S_{2} S_{3}\).

An equivalent reformulation is that between any two sets of reviewers \(S\) and \(T\) with \(|S|=|T|\), ignoring the common reviewers in \(S T\), if the agent strictly prefers every (even the worst) reviewer in \(S T\) to every (even the best) reviewer in \(T S\), then the agent must strictly prefer \(S\) to \(T\).

**Example 1**.: Consider the common example of additive preferences, where each agent \(i\) has a utility function \(u_{i,}:N\{i\}_{ 0}\) over individual reviewers for her \(\)-th submission, inducing her preference ranking \(_{i,}\). In practice, these utilities are sometimes called similarity scores. Her preferences over sets of reviewers are defined via the additive utility function \(u_{i,}(S)_{i^{} S}u_{i,}(i^{})\). It is easy to check that for any disjoint \(S_{1},S_{2},S_{3}\) with \(|S_{1}|=|S_{2}|>0\), \(u_{i,}(i^{})>u_{i,}(i^{})\) for all \(i^{} S_{1}\) and \(i^{} S_{2}\) would indeed imply \(u_{i,}(S_{1} S_{3})>u_{i,}(S_{2} S_{3})\). Additive preferences are just one example from a broad class of extensions satisfying order separability.

_Extension to assignments._ Let us now consider agent preferences over sets of sets of reviewers, or equivalently, over assignments. Let \(R_{i}\) (resp., \(R_{i}\)) denote that agent \(i\) strictly (resp., weakly) prefers assignment \(R\) to assignment \(\). Note that these preferences collate the submission-wise preferences \(_{i,}\) across all submissions of the agent. We require only that the preference extension satisfies the following natural property.

**Definition 2** (Consistency).: For any assignment \(R\), restricted assignment \(\) over any \(N^{} N\) and \(P^{}=_{i N^{}}P^{}_{i}\) (where \(P^{}_{i} P_{i}\) for each \(i N^{}\)), and agent \(i^{*} N^{}\), if it holds that \(R^{p}_{p_{i^{*},}}_{i^{*},}^{p}_{p_{i^{*},}}\) for each \(p_{i^{*},} P^{}_{i}\), then we must have \(R_{i}\).

In words, if an agent weakly prefers \(R\) to \(\) for the set of reviewers assigned to each of her submissions individually, then she must prefer \(R\) to \(\) overall.

**Example 2**.: Let us continue with the previous example of additive utility functions. The preferences of agent \(i\) can be extended additively to assignments using the utility function \(u_{i}(R)=_{p_{i,} P}u_{i,}(R^{p}_{p_{i,}})\). It is again easy to check that if \(u_{i,}(R^{p}_{p_{i,}}) u_{i,}(^{p}_{p_{i,}})\) for each \(p_{i,}\), then \(u_{i}(R) u_{i}()\). Hence, additive preferences are again one example out of a broad class of preference extensions that satisfy consistency.

**Core.** Our goal is to find a group-fair assignment which treats every possible group of agents at least as well as they could be on their own, thus ensuring that no subset of agents has an incentive to deviate and set up their own separate conference. Formally:

**Definition 3** (Core).: An assignment \(R\) is in the core if there is no \(N^{} N\), \(P^{}_{i} P_{i}\) for each \(i N^{}\), and restricted assignment \(\) over \(N^{}\) and \(P^{}=_{i N^{}}P^{}_{i}\) such that \(_{i}R\) for each \(i N^{}\).

In words, if any subset of agents deviate with any subset of their submissions and implement any restricted reviewing assignment, at least one deviating agent would not be strictly better off, thus eliminating the incentive for such a deviation. We also remark that our algorithm takes only the preference rankings over individual reviewers \(\) as input and produces an assignment \(R\) that is guaranteed to be in the core according to _every preference extension_ of \(\) satisfying order separability and consistency.

## 3 CoBRA: An Algorithm for Computing Core-Based Reviewer Assignment

In this section, we prove our main result: when agent preferences are order separable and consistent, an assignment in the core always exists and can be found in polynomial time.

**Techniques and key challenges:** The main algorithm CoBRA (Core-Based Reviewer Assignment), presented as Algorithm 1, uses two other algorithms, PRA-TTC and Filling-Gaps, presented as Algorithm 2 and Algorithm 3, respectively. We remark that PRA-TTC is an adaptation of the popular Top-Trading-Cycles (TTC) mechanism, which is known to produce an assignment in the core for the house reallocation problem (and its variants) . The adaptation mainly incorporates the constraints related to how many papers each reviewer can review and how many reviewers should review each paper. While for \(k_{p}=k_{a}=1\), PRA-TTC is identical with the classic TTC that is used for the house reallocation problem, the main difference of this problem with the review assignment problem is that in the latter each agent should give away her item (i.e. her submission) and obtain the item of another agent. Therefore, by simply executing TTC in the review assignment problem, one can get into a deadlock before producing a valid assignment. For example, consider the case of three agents, each with one submission. Each submission must receive one review (\(k_{p}=1\)) and each agent provides one review (\(k_{a}=1\)). The TTC mechanism may start by assigning agents \(1\) and \(2\) to review each other's submission, but this cannot be extended into a valid assignment because there is no one left to review the submission of agent \(3\). This is where Filling-Gaps comes in; it makes careful edits to the partial assignment produced by the PRA-TTC, and the key difficulty is to prove that this produces a valid assignment while still satisfying the core.

```
Input:\(N,P,,k_{a},k_{p}\) Output:\(R\)
1\(R,L,U\)
2if\(|U|>0\)then
3\(R=\)Filling-Gaps(\(N,P,,k_{a},k_{p},R,L,U\));
```

**ALGORITHM 1**CoBRA

### Description of CoBRA

Before we describe CoBRA in detail, let us introduce some more notation. Let \(m^{*}=_{i N}m_{i}\). For reasons that will become clear later, we want to ensure that \(m_{i}=m^{*}\), for each \(i N\). To achieve that, we add \(m^{*}-m_{i}\) dummy submissions to agent \(i\), and the rankings over reviewers with respect to these submissions are arbitrarily. An assignment is called _partial_ if there are submissions that are reviewed by less than \(k_{p}\) agents. A submission that is reviewed by \(k_{p}\) agents under a partial assignment is called _completely assigned_. Otherwise, it is called _incompletely assigned_. We denote with \(_{i}()\) the set of submissions of \(i\) that are incompletely assigned under a partial assignment \(\). We omit \(\) from the notation when it is clear from context.

CoBRA calls PRA-TTC, and then if needed, it calls Filling-Gaps. Below, we describe the algorithms.

**PRA-TTC.** In order to define PRA-TTC, we first need to introduce the notion of a _preference graph_. Suppose we have a partial assignment \(\). Each agent \(i\) with \(_{i}\) picks one of her incompletely assigned submissions arbitrarily. Without loss of generality, we assume that she picks her \(^{*}\)-th submission. We define the directed preference graph \(G_{}=(N,E_{})\) where each agent is a node and for each \(i\) with \(_{i}\), \((i,i^{}) E_{}\) if and only if \(i^{}\) is ranked highest in \(_{i,^{*}}\) among the agents that don't review \(p_{i,^{*}}\) and review less than \(k_{a}\) submissions. Moreover, for each \(i N\) with \(_{i}=\), we add an edge from \(i\) to \(i^{}\), where \(i^{}\) is an arbitrary agent with \(_{i^{}}\). PRA-TTC starts with an empty assignment, constructs the preference graph and searches for a directed cycle. If such a cycle exists, the algorithm eliminates it as following: For each \((i,i^{})\) that is included in the cycle, it assigns submission \(p_{i,^{*}}\) to \(i^{}\) (if \(i\)'s submissions are already completely assigned, it does nothing) and removes \(p_{i,^{*}}\) from \(_{i}\), if it is now completely assigned. Then, the algorithm updates the preference graph and continues to eliminate cycles in the same way. When there are no left cycles in the preference graph, the algorithm terminates and returns two sets, \(U\) and \(L\). The first set contains all the agents that some of their submissions are incompletely assigned and the set \(L\) contains the last \(k_{p}-|U|+1\) agents whose all submissions became completely assigned.

**Filling-Gaps.** CoBRA calls Filling-Gaps, if the \(U\) that returned from PRA-TTC is non empty. Before we describe the Filling-Gaps, we also need to introduce the notion of a _greedy graph_. Suppose that we have a partial assignment \(\) which indicates a set \(U\) that contains all the agents whose at least one submission is incompletely assigned. We define the directed greedy graph \(G_{}=(U,E_{})\) where \((i,i^{}) E_{}\) if \((i^{},p_{i,})=0\) for some \(p_{i,}_{i}\). In other words, while in the preference graph, agent \(i\) points only to her favourite potential reviewer with respect to one of her incomplete submissions, in the greedy graph agent \(i\) points to any agent in \(U\{i\}\) that could review at least one of her submissions that is incompletely assigned. Filling-Gaps consists of two phases. In the first phase, starting from the partial assignment \(R\) that was created from PRA-TTC, it constructs the greedy graph, searches for cycles and eliminates a cycle by assigning \(p_{i,}\) to agent \(i^{}\) for each \((i,i^{})\) in the cycle that exists due to \(p_{i,}\) (when an edge exists due to multiple submissions, the algorithm chooses one of them arbitrary). Then, it updates \(_{i}\) by removing any \(p_{i,}\) that became completely assigned and also updates \(U\) by moving any \(i\) to \(L\) if \(_{i}\) became empty. It continues by updating the greedy graph and eliminating cycles in the same way. When no more cycles exist in the greedy graph, the algorithm proceeds to the second phase, where in \(|U|\) rounds ensures that the incomplete submissions of each agent become completely assigned.

In the appendix, there is an execution of CoBRA in a small instance.

### Main Result

We are now ready to present our main result.

**Theorem 1**.: _When agent preferences are order separable and consistent, CoBRA returns an assignment in the core in \(O(n^{3})\) time complexity._

Proof.: First, in the next lemma, we show that CoBRA returns a valid assignment. The proof is quite non trivial and several pages long, so we defer it to the supplementary material.

**Lemma 1**.: _CoBRA returns a valid assignment._

Now, we show that the final assignment \(R\) that CoBRA returns is in the core. Note that while it is possible that an assignment of a submission of an agent in \(U L\), that was established during the execution of PRA-TTC, to be removed in the execution of Filling-Gaps, this never happens for submissions that belong to some agent in \(N(U L)\). For the sake of contradiction, assume that \(N^{} N\), with \(P^{}_{i} P_{i}\) for each \(i N^{}\), deviate to a restricted assignment \(\) over \(N^{}\) and \(_{i N^{}}P^{}_{i^{}}\). Note that \(\) is valid only if \(|N^{}|>k_{p}\), as otherwise there is no way each submission in \(_{i N^{}}P^{}_{i}\) to be completely assigned, since no agent can review her own submissions.

We distinguish into two cases and we show that in both cases the assignment is in the core.

**Case I:**\( i N^{}:i L U\). Let \(i^{*} N^{}\) be the first agent in \(N^{}\) whose all submissions became completely assigned in the execution of PRA-TTC. Note that since there exists \(i U L\), we get that \(i^{*} U L\) from the definitions of \(U\) and \(L\). Now, consider any \(p_{i^{*},}\). Let \(Q_{1}=R^{p}_{p_{i^{*},}}(R^{p}_{p_{i^{*},}} ^{p}_{p_{i^{*},}})\) and \(Q_{2}=^{p}_{p_{i^{*},}}(R^{p}_{p_{i^{*},}} ^{p}_{p_{i^{*},}})\). If \(Q_{1}=\), then we have that \(R^{p}_{p_{i^{*},}}=^{p}_{p_{i^{*},}}\) which means that \(R^{p}_{p_{i^{*},}}_{i^{*},}^{p}_{p_{i^{*},}}\). Otherwise, let \(i^{}=*{argmax}_{i Q_{i}}_{i^{*},}(i)\), i.e. \(i^{}\) is ranked at the lowest position in \(_{i^{*},}\) among the agents that review \(p_{i^{*},}\) under \(R\) but not under \(\). Moreover, let \(i^{}=*{argmin}_{i Q_{2}}_{i^{*},}(i)\), i.e. \(i^{}\) is ranked at the highest position in \(_{i^{*},}\) among the agents that review \(p_{i^{*},}\) under \(\) but not under \(R\). We have \(R(i^{},p_{i^{*},})=1\), if and only if \(i^{*}\) has an outgoing edge to \(i^{}\) at some round of PRA-TTC. At the same round, we get that \(i^{}\) can review more submissions, since \(i^{} N^{}\) and if \(i^{*}\) has incompletely assigned submissions, then any \(i N^{}\) has incompletely assigned submissions, and hence \(|R^{a}_{i^{}}|<k_{p} m^{*} k_{a}\). This means that if \(_{i^{*},}(i^{})>_{i^{*},}(i^{})\), then \(i^{*}\) would point \(i^{}\) instead of \(i^{}\). We conclude that \(_{i^{*},}(i^{})<_{i^{*},}(i^{})\). Then, from the definition of \(i^{}\) and \(i^{}\) and from the order separability property we have that \(R^{p}_{p_{i^{*},}}_{i^{*},}^{p}_{p_{i^{*},}}\). Thus, either if \(Q_{1}\) is empty or not, we have that for any \(p_{i^{*},} P^{}_{i}\), it holds that \(R^{p}_{p_{i^{*},}}_{i^{*},}^{p}_{p_{i^{*},}}\) and from consistency, we get that \(R_{i^{*}}\) which is a contradiction.

**Case II:**\( i N^{}:i L U\). In this case we have that \(N^{}=U L\), as \(|U L|=k_{p}+1\). This means that for each \(i U L\) and \([m^{*}]\), \(^{p}_{p_{i,}}=(U L)\{i\}\). Let \(i^{*} L\) be the first agent in \(L\) whose all submissions became completely assigned in the execution of PRA-TTC. Consider any \(p_{i^{*},}\). Note that it is probable that while \(p_{i^{*},}\) was assigned to some agent \(i\) in PRA-TTC, it was moved to another agent \(i^{}\) during the execution of Filling-Gaps. But, \(i^{}\) belongs to \(U\) and we can conclude that if \(p_{i^{*},}\) is assigned to some \(i N U\) at the output of CoBRA, this assignment took place during the execution of PRA-TTC. Now, let \(Q_{1}=R^{p}_{p_{i^{*},}}(R^{p}_{p_{i^{*},}}^{ p}_{p_{i^{*},}})\) and \(Q_{2}=^{p}_{p_{i^{*},}}(R^{p}_{p_{i^{*},}} ^{p}_{p_{i^{*},}})\). If \(Q_{1}=\), then we have that \(R^{p}_{p_{i^{*},}}=^{p}_{p_{i^{*},}}\) which means that \(R^{p}_{p_{i^{*},}}_{i^{*},}^{p}_{p_{i^{*},}}\). If \(Q_{1}\), then \(Q_{1} N(U L)\) and \(Q_{2} U L\) since \(^{p}_{p_{i^{*},}}=U L\). Let \(i^{}=*{argmax}_{i Q_{1}}_{i^{*},}(i)\), i.e. \(i^{}\) is ranked at the lowest position in \(_{i^{*},}\) among the agents that review \(p_{i^{*},}\) under \(R\) but not under \(\). Moreover, let \(i^{}=*{argmin}_{i Q_{2}}_{i^{*},}(i)\), i.e. \(i^{}\) is ranked at the highest position in \(_{i^{*},}\) among the agents that review \(p_{i^{*},}\) under \(\) but not under \(R\). From above, we know that the assignment of \(p_{i^{*},}\) to \(i^{}\) was implemented during the execution of PRA-TTC, since \(i^{} N(U L)\). Hence, with very similar arguments as in the previous case, we will conclude that \(_{i^{*},}(i^{})<_{i^{*},}(i^{})\). We have \(R(i^{},p_{i^{*},})=1\) if and only if \(i^{*}\) has an outgoing edge to \(i^{}\) at some round of PRA-TTC. At this round, we know that \(i^{}\) can review more submissions, since \(i^{} N^{}\) and if \(i^{*}\) has incompletely assigned submissions. This means that if \(_{i^{*},}(i^{})>_{i^{*},}(i^{})\), then \(i^{*}\) would point \(i^{}\) instead of \(i^{}\). Hence, we conclude that\(_{i^{*},}(i^{})<_{i^{*},}(i^{})\). Then, from the definition of \(i^{}\) and \(i^{}\) and from the order separability property we have that \(B^{p}_{p_{*,}}_{i^{*},}^{p}_{p_{*,}}\). Thus, either if \(Q_{1}\) is empty or not, we have that for any \(p_{i^{*},} P^{}_{i}\), it holds that \(R^{p}_{p_{i^{*},}}_{i^{*},}^{p}_{p_{*,}}\) and from consistency we get that \(R_{i^{*}}\) which is a contradiction.

Lastly, we analyze the time complexity of CoBRA. First, we consider the time complexity of PRA-TTC. In each iteration, the algorithm assigns at least one extra reviewer to at least one incompletely-assigned submission. This can continue for at most \(m k_{p} n k_{a}\) iterations, since each submission should be reviewed by \(k_{p}\) reviewers. In each iteration, it takes \(O(n)\) time to find and eliminate a cycle in the preference graph. Then, it takes \(O(n^{2})\) time to update the preference graph, since for each arbitrarily-picked incompletely-assigned submission of each agent, we need to find the most qualified reviewer who can be additionally assigned to it. By all the above, we conclude that the runtime of PRA-TTC is \(O(n^{3})\), by ignoring \(k_{a}\) which is a small constant in practice. After PRA-TTC terminates, CoBRA calls the Filling-Gaps algorithm. However, Lemma 3 ensures that at the end of PRA-TTC, \(|L U| k_{p}+1\), which is also a small constant. And Filling-Gaps only makes local changes that affect these constantly many agents. As such, the running time of Filling-Gaps is constant as well. Therefore, the time complexity of CoBRA is \(O(n^{3})\)

## 4 Experiments

In this section, we empirically compare CoBRA to TPMS , which is widely used (for example, it was used by NeurIPS for many years), and PR4A , which was used in ICML 2020 . As mentioned in the introduction, these algorithms assume the existence of a similarity or affinity score for each pair of reviewer \(i\) and paper \(j\), denoted by \(S(i,j)\). The score (or utility) of a paper under an assignment \(R\), denoted by \(u^{p}_{j}\), is computed as \(u^{p}_{j}_{i R^{p}_{j}}S(i,j)\). TMPS finds an assignment \(R\) that maximizes the utilitarian social welfare (USW), i.e., the total paper score \(_{j P}u^{p}_{j}\), whereas PR4A finds an assignment that maximizes the egalitarian social welfare (ESW), i.e., the minimum paper score \(_{j P}u^{p}_{j}\).6 We use \(k_{a}=k_{p}=3\) in these experiments.7

**Datasets.** We use three conference datasets: from the Conference on Computer Vision and Pattern Recognition (CVPR) in 2017 and 2018, which were both used by Kobren et al. , and from the International Conference on Learning Representations (ICLR) in 2018, which was used by Xu et al. . In the ICLR 2018 dataset, similarity scores and conflicts of interest are also available. While a conflict between a reviewer and a paper does not necessarily indicate authorship, it is the best indication we have available, so, following Xu et al. , we use the conflict information to deduce authorship. Since in our model each submission has one author, and no author can submit more than \( k_{a}/k_{p}=1\) papers, we compute a maximum cardinality matching on the conflict matrix to find author-paper pairs, similarly to what Dhull et al.  did. In this way, we were able to match 883 out of the 911 papers. We disregard any reviewer who does not author any submissions, but note that the addition of more reviewers can only improve the results of our algorithm since these additional reviewers have no incentive to deviate. For the CVPR 2017 and CVPR 2018 datasets, similarity scores was available, but not the conflict information. In both these datasets, there are fewer reviewers than papers. Thus, we constructed artificial authorship relations by sequentially processing papers and matching each paper to the reviewer with the highest score for it, if this reviewer is still unmatched. In this way, we were able to match \(1373\) out of \(2623\) papers from CVPR 2017 and \(2840\) out of \(5062\) papers from CVPR 2018. In the ICLR 2018 and CVPR 2017 datasets, the similarity scores take values in \(\), so we accordingly normalized the CVPR 2018 scores as well.

**Measures.** We are most interested in measuring the extent to which the existing algorithms provide incentives for communities of researchers to deviate. To quantify this, we need to specify the utilities of the authors. We assume that they are additive, i.e., the utility of each author in an assignment is the total similarity score of the \(k_{p}=3\) reviewers assigned to their submission.

_Core violation factor:_ Following the literature , we measure the multiplicative violation of the core (if any) that is incurred by TPMS and PR4A (CoBRA provably does not incur any). This is done by computing the maximum value of \( 1\) for which there exists a subset of authors such that by deviating and implementing some valid reviewing assignment of their papers among themselves, they can each improve their utility by a factor of at least \(\). This can easily be formulated as a binary integer linear program (BILP). Because this optimization is computationally expensive (the most time-consuming component of our experiments), we subsample \(100\) papers8 from each dataset in each run, and report results averaged over \(100\) runs. Note that whenever there exists a subset of authors with zero utility each in the current assignment who can deviate and receive a positive utility each, the core deviation \(\) becomes infinite. We separately measure the percentage of runs in which this happens (in the column #sub-\(\)), and report the average \(\) among the remaining runs in the \(^{*}\) column.

_Core violation probability:_ We also report the percentage of runs in which a core violation exists (i.e., there exists at least one subset of authors who can all strictly improve by deviating from the current assignment). We refer to this as the _core violation probability_ (CV-Pr).

_Social welfare:_ Finally, we also measure the utilitarian and egalitarian social welfare (USW and ESW) defined above, which are the objectives maximized by TPMS and PR4A, respectively.

**Results.** The results are presented in Table 1. As expected, TPMS and PR4A achieve the highest USW and ESW, respectively, on all datasets because they are designed to optimize these objectives. In CVPR 2017, CoBRA and TPMS always end up with zero ESW because this dataset includes many zero similarity scores, but PR4A is able to achieve positive ESW. In all datasets, CoBRA achieves a relatively good approximation with respect to USW, but this is not always the case with respect to ESW. For example, in CVPR 2018, CoBRA achieves \(0.004\) ESW on average whereas PR4A achieves \(0.099\) ESW on average. This may be due to the fact that this dataset also contains many zero similarity scores, and the myopic process of CoBRA locks itself into a bad assignment, which the global optimization performed by PR4A avoids.

While CoBRA suffers some loss in welfare, TPMS and PR4A also generate significant adverse incentives. They incentivize at least one community to deviate in almost every run of each dataset (CV-Pr). While the magnitude of this violation is relatively small when it is finite (except for TPMS in CVPR 2017), TPMS and PR4A also suffer from unbounded core violations in more than half of the runs for CVPR 2017; this may again be due to the fact that many zero similarity scores lead to deviations by groups where each agent has zero utility under the assignments produced by these algorithms.

Of all these results, the high probability of core violation under TPMS and PR4A is perhaps the most shocking result; when communities regularly face adverse incentives, occasional deviations may happen, which can endanger the stability of the conference. That said, CoBRA resolves this issue at a significant loss in fairness (measured by ESW). This points to the need for finding a middle ground where adverse incentives can be minimized without significant loss in fairness or welfare.

  &  &  &  &  &  \\    & & & & & \#sub-\(\) & & \(^{*}\) \\   & CoBRA & \(1.225 0.021\) & \(0.000 0.000\) & \(0\%\) & \(1.000 0.000\) & \(0\%\) \\  & TPMS & \(1.497 0.019\) & \(0.000 0.000\) & \(89\%\) & \(3.134 0.306\) & \(100\%\) \\  & PR4A & \(1.416 0.019\) & \(0.120 0.032\) & \(51\%\) & \(1.700 0.078\) & \(100\%\) \\    & CoBRA & \(0.224 0.004\) & \(0.004 0.001\) & \(0\%\) & \(1.000 0.000\) & \(0\%\) \\  & TPMS & \(0.286 0.005\) & \(0.043 0.004\) & \(0\%\) & \(1.271 0.038\) & \(100\%\) \\  & PR4A & \(0.282 0.005\) & \(0.099 0.001\) & \(0\%\) & \(1.139 0.011\) & \(100\%\) \\    & CoBRA & \(0.166 0.001\) & \(0.028 0.001\) & \(0\%\) & \(1.000 0.000\) & \(0\%\) \\  & TPMS & \(0.184 0.001\) & \(0.048 0.002\) & \(0\%\) & \(1.048 0.008\) & \(90\%\) \\   & PR4A & \(0.179 0.001\) & \(0.082 0.001\) & \(0\%\) & \(1.087 0.009\) & \(100\%\) \\ 

Table 1: Results on CVPR 2017 and 2018, and ICLR 2018.

Discussion

In this work, we propose a way for tackling the poor reviewing problem in large conferences by introducing the concept of "core" as a notion of group fairness in the peer review process. This fairness principle ensures that each subcommunity is treated at least as well as it would be if it was not part of the larger conference community.

We show that under certain --albeit sometimes unrealistic--assumptions, a peer review assignment in the core always exists and can be efficiently found. In the experimental part, we provide evidence that peer review assignment procedures that are currently used in practice, quite often motivate subcommunities to deviate and build their own conferences.

Our theoretical results serve merely as the first step toward using it to find reviewer assignments that treat communities fairly and prevent them from deviating. As such, our algorithm has significant limitations that must be countered before it is ready for deployment in practice. A key limitation is that it only works for single-author submissions, which may be somewhat more realistic for peer review of grant proposals, but unrealistic for computer science conferences. We also assume that each author serves as a potential reviewer; while many conferences require this nowadays, exceptions must be allowed in special circumstances. We also limit the number of submissions by any author to be at most \( k_{a}/k_{p}\), which is a rather small value in practice, and some authors ought to submit more papers than this. We need to make this assumption to theoretically guarantee that a valid assignment exists. An interesting direction is to design an algorithm that can produce a valid assignment in the (approximate) core whenever it exists. Finally, deploying group fairness in real-world peer review processes may require designing algorithms that satisfy it approximately at minimal loss in welfare, as indicated by our experimental results.