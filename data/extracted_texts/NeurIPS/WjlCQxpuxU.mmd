# Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models

Xingyuan Zhang\({}^{1,2}\)

\({}^{1}\)Machine Learning Research Lab, Volkswagen Group, \({}^{2}\)Technical University of Munich,

\({}^{3}\)Eotvos Lorand University Budapest

{xingyuan.zhang,philip.becker-ehmck,maximilian.karl}@volkswagen.de

Philip Becker-Ehmck\({}^{1}\)

\({}^{1}\)Machine Learning Research Lab, Volkswagen Group, \({}^{2}\)Technical University of Munich,

\({}^{3}\)Eotvos Lorand University Budapest

{xingyuan.zhang,philip.becker-ehmck,maximilian.karl}@volkswagen.de

Patrick van der Smagt\({}^{1,3}\)

\({}^{1}\)Machine Learning Research Lab, Volkswagen Group, \({}^{2}\)Technical University of Munich,

\({}^{3}\)Eotvos Lorand University Budapest

{xingyuan.zhang,philip.becker-ehmck,maximilian.karl}@volkswagen.de

Maximilian Karl\({}^{1}\)

\({}^{1}\)Machine Learning Research Lab, Volkswagen Group, \({}^{2}\)Technical University of Munich,

\({}^{3}\)Eotvos Lorand University Budapest

{xingyuan.zhang,philip.becker-ehmck,maximilian.karl}@volkswagen.de

###### Abstract

Unlike most reinforcement learning agents which require an unrealistic amount of environment interactions to learn a new behaviour, humans excel at learning quickly by merely observing and imitating others. This ability highly depends on the fact that humans have a model of their own embodiment that allows them to infer the most likely actions that led to the observed behaviour. In this paper, we propose Action Inference by Maximising Evidence (AIME) to replicate this behaviour using world models. AIME consists of two distinct phases. In the first phase, the agent learns a world model from its past experience to understand its own body by maximising the evidence lower bound (ELBO). While in the second phase, the agent is given some observation-only demonstrations of an expert performing a novel task and tries to imitate the expert's behaviour. AIME achieves this by defining a policy as an inference model and maximising the evidence of the demonstration under the policy and world model. Our method is "zero-shot" in the sense that it does not require further training for the world model or online interactions with the environment after given the demonstration. We empirically validate the zero-shot imitation performance of our method on the Walker and Cheetah embodiment of the DeepMind Control Suite and find it outperforms the state-of-the-art baselines. Code is available at: https://github.com/argmax-ai/aime.

## 1 Introduction

In recent years, deep reinforcement learning (DRL) has enabled intelligent decision-making agents to thrive in multiple fields [1; 2; 3; 4; 5; 6]. However, one of the biggest issues of DRL is sample inefficiency. The dominant framework in DRL is learning from scratch . Thus, most algorithms require an incredible amount of interactions with the environment [1; 2; 3].

In contrast, cortical animals such as humans are able to quickly learn new tasks through just a few trial-and-error attempts, and can further accelerate their learning process by observing others. An important difference between biological learning and the DRL framework is that the former uses past experience for new tasks. When we try a novel task, we use previously learnt components and generalise to solve the new problem efficiently. This process is augmented by imitation learning , which allows us to replicate similar behaviours without direct observation of the underlying muscle movements. If the DRL agents could similarly harness observational data, such as the abundant online video data, the sample efficiency may be dramatically improved . The goal of the problem is related to the traditional well-established Learning from Demonstration (LfD) field from the robotics community [10; 11], but instead of relying on knowledge from the engineers and researchers, e.g. mathematical model of robot's dynamic or primitives, we aim to let the robots learn by itself.

However, directly learning a model from observation-only sequences [12; 13] is insufficient for both biological and technical systems. Without knowing the actions that lead to the observations, the observation sequences are highly stochastic and multi-modal . Trying to infer these unknown actions without prior knowledge of the world is difficult due to the problem of attributing which parts of the observations are influenced by the actions and which parts are governed by normal system evolution or noise.

Therefore, in this work, we hypothesise that in order to make best use of observation-only sequences, an agent has to first understand the notion of an action. This can be achieved by learning a model from an agent's past experiences where both the actions and their consequences, i.e. observations, are available. Given such a learnt model which includes a causal model of actions and their effects, it becomes feasible for an agent to infer an action sequence leading to given observation-only data.

In this work, we propose a novel algorithm, Action Inference by Maximising Evidence (AIME), to try to replicate the imitation ability of humans. The agent first learns a world model from its past experience by maximising the evidence of these experiences. After receiving some observation-only demonstrations of a novel task, the agent tries to _mimic_ the demonstrator by finding an action sequence that makes the demonstration most likely under the learnt model. This procedure is shown in Figure 1.

Our contribution can be summarised as follows:

* We propose AIME, a novel method for imitation from observation. AIME first learns a world model by maximising the evidence of its past experience, then considers the policy as an action inference model and imitates by maximising the evidence of demonstration.
* We conduct experiments with a variety of datasets and tasks to demonstrate the superior performance of AIME compared with other state-of-the-art methods. The results showcase the zero-shot transferability of a learnt world model.

## 2 Problem formulation

Consider an MDP problem defined by the tuple \(\{S,A,T,R\}\), where \(S\) is the state space, \(A\) is the action space, \(T:S A S\) is the dynamic function and \(R:S\) is the reward function. A POMDP adds partial observability upon an MDP with two components: the observation space \(O\) and the emission function \(:S O\). The six components of a POMDP can be categorised into three groups: \(S\), \(A\) and \(T\) define the embodiment of our agent, \(O\) and \(\) define the sensors of our agent and

Figure 1: Overview of AIME algorithm. In phase 1, both observations and actions are provided by the embodiment dataset and the agent learns a variational world model to model the evidence of observations conditioned on the actions. Then the learnt model weights are frozen and transferred to phase 2. In phase 2, only the observations are provided by the demonstration dataset, so the agent needs to infer both states and actions. The action inference is achieved by the policy model which samples actions given a state. The grey lines indicate the world model parameters are frozen in phase 2. Both phases are optimised toward the same objective, i.e. the ELBO.

\(R\) itself defines the task. The goal is to find a policy \(:S A\) which maximises the accumulated reward, i.e. \(_{t}r_{t}\).

In this paper, we want to study imitation learning within a fixed embodiment across different tasks. We presume the existence of two datasets for the same embodiment:

* Embodiment dataset \(D_{}\) contains trajectories \(\{o_{0},a_{0},o_{1},a_{1}\}\) that represent past experiences of interacting with the environment. This dataset provides information about the embodiment for the algorithm to learn a model. For example, in this paper, the dataset is a replay buffer filled while solving some tasks with the same embodiment. But in general, it may be any collection of past experiences of the embodiment.
* Demonstration dataset \(D_{}\) contains a few expert trajectories \(\{o_{0},o_{1},o_{2}\}\) of the embodiment solving a certain task defined by \(R_{}\). The crucial difference between this dataset and the embodiment dataset is that the actions are not provided anymore since they are not observable from a third-person perspective.

The goal of our agent is to use information in \(D_{}\) to learn a policy \(\) from \(D_{}\) which can solve the task defined by \(R_{}\) as well as the expert who generated \(D_{}\). For simplicity, we assume that the two datasets share the same observation space \(O\) and the emission model \(\).

## 3 Methodology

In this section, we describe our proposed method, AIME, in detail. AIME consists of two phases. In the first phase, the knowledge of the embodiment is learnt through a form of world model; while in the second phase, this knowledge is used to imitate the expert.

### Phase 1: Model Learning

In the first phase, we need to learn a model to understand our embodiment. We achieve this by learning a world model. As an analogy to a language model, we define a world model as a probability distribution over sequences of observations. The model can be either unconditioned or conditioned on other factors such as previous observations or actions. For phase 1, the model needs to be the conditional distribution, i.e. \(p(o_{1:T}|a_{0:T-1})\), to model the effect of the actions. When given an observation sequence, the likelihood of this sequence under the model is referred to as evidence.

In this paper, we consider variational world models where the observation is governed by a Markovian hidden state. In the literature, this type of model is also referred to as a state-space model (SSM) [15; 16; 17; 18; 19; 20]. Such a variational world model involves four components, namely

encoder

\[z_{t}=f_{}(o_{t}),\] posterior

\[s_{t} q_{}(s_{t}|s_{t-1},a_{t-1},z_{t}),\] prior

\[s_{t} p_{}(s_{t}|s_{t-1},a_{t-1}),\] decoder

\[o_{t} p_{}(o_{t}|s_{t}).\]

\(f_{}(o_{t})\) is the encoder to extract the features from the observation; \(q_{}(s_{t}|s_{t-1},a_{t-1},z_{t})\) and \(p_{}(s_{t}|s_{t-1},a_{t-1})\) are the posterior and the prior of the latent state variable; while \(p_{}(o_{t}|s_{t})\) is the decoder that decodes the observation distribution from the state. \(\) and \(\) represent the parameters of the inference model and the generative model respectively.

Typically, a variational world model is trained by maximising the ELBO which is a lower bound of the log-likelihood, or evidence, of the observation sequence, i.e. \( p_{}(o_{1:T}|a_{0:T-1})\). Given a sequence of observations, actions, and states, the objective function can be computed as

\[J(o_{1:T},s_{0:T},a_{0:T-1}) =J_{}(o_{1:T},s_{0:T},a_{0:T-1})+J_{}(o_{1 :T},s_{0:T},a_{0:T-1}),\] (1) where

\[J_{}(o_{1:T},s_{0:T},a_{0:T-1}) =_{t=1}^{T} p_{}(o_{t}|s_{t}),\] (2) \[J_{}(o_{1:T},s_{0:T},a_{0:T-1}) =_{t=1}^{T}-D_{}[q_{}(s_{t}|s_{t-1},a_{t-1},f_ {}(o_{t}))||p_{}(s_{t}|s_{t-1},a_{t-1})].\] (3)The objective function is composed of two terms: the first term \(J_{ rec}\) is the likelihood of the observation under the inferred state, which is usually called the reconstruction loss; while the second term \(J_{ KL}\) is the KL divergence between the posterior and the prior distributions of the latent state. To compute the objective function, we use the re-parameterisation trick [21; 22] to autoregressively sample the inferred states from the observation and action sequence.

Combining all these, we formally define the optimisation problem for this phase as

\[^{*},^{*}=*{argmax}_{,}_{\{o_{1:T},a_{0:T-1}\} D_{ body},s_{0:T} q_{}}[J(o_{1:T},s_{0:T},a_{0:T-1 })].\] (4)

### Phase 2: Imitation Learning

In the second phase, we want to utilise the knowledge of the world model from the first phase to imitate the expert behaviour from the demonstration dataset \(D_{ demo}\) in which only sequences of observations but no actions are available. We derive our algorithm from two different perspectives.

**The Bayesian derivation** Since the actions are unknown in the demonstration, instead of modelling the conditional evidence in phase 1, we need to model the unconditional evidence, i.e. \( p_{}(o_{1:T})\). Thus, we also need to model the actions as latent variables together with the states. In this way, the reconstruction term \(J_{ rec}\) will stay the same as eq. (2), while the KL term will be defined on the joint distribution of states and actions, i.e.

\[J_{ KL}(o_{1:T},s_{0:T},a_{0:T-1})=_{t=1}^{T}-D_{ KL}[q_{,}( s_{t},a_{t-1}|s_{t-1},f_{}(o_{t}))||p_{,}(s_{t},a_{t-1}|s_{t-1})].\] (5)

If we choose the action inference model in the form of a policy, i.e. \(_{}(a_{t}|s_{t})\), and share it in both posterior and prior, then the new posterior and prior can be factorised as

\[q_{,}(s_{t},a_{t-1}|s_{t-1},f_{}(o_{t})) =_{}(a_{t-1}|s_{t-1})q_{}(s_{t}|s_{t-1},a_{t-1},f_{ }(o_{t}))\] (6) \[\;\;p_{,}(s_{t},a_{t-1}|s_{t-1}) =_{}(a_{t-1}|s_{t-1})p_{}(s_{t}|s_{t-1},a_{t-1})\] (7)

respectively. When we plug them into the eq. (5), the policy term cancels and we will get a similar optimisation problem with phase 1 as

\[^{*}=*{argmax}_{}_{o_{1:T} D_{ demo},\{ s_{0:T},a_{0:T-1}\} q_{^{*},}}[J(o_{1:T},s_{0:T},a_{0:T-1})].\] (8)

The main difference between eq. (4) and eq. (8) is where the action sequence is coming from. In phase 1, the action sequence is coming from the embodiment dataset, while in phase 2, it is sampled from the policy instead since it is not available in the demonstration dataset.

**The control derivation** From another perspective, we can view phase 2 as a control problem. One crucial observation is that, as shown in eq. (1), given a trained world model, we can evaluate the lower bound of the evidence of any observation sequence given an associated action sequence as the condition. In a deterministic environment where the inverse dynamics model is injective, the true action sequence that leads to the observation sequence is the most likely under the true model. In general, the true action sequence may not necessarily be the most likely under the model. This is, however, a potential benefit of our approach. We are mainly interested in mimicking the expert's demonstration and may be better able to do so with a different action sequence.

Thus, for each observation sequence that we get from the demonstration dataset, finding the missing action sequence can be considered as a trajectory-tracking problem and can be tackled by planning. To be specific, we can find the missing action sequence by solving the optimisation problem

\[a_{0:T-1}^{*}=*{argmax}_{a_{0:T-1}}_{o_{1:T} D_{  demo},s_{0:T} q_{^{*}}}[J(o_{1:T},s_{0:T},a_{0:T-1})].\] (9)

If we solve the above optimisation problem for every sequence in the demonstration dataset, the problem will be converted to a normal imitation learning problem and can be solved with standard techniques such as behavioural cloning. We can also view this as forming an implicit inverse dynamics model (IDM) by inverting a forward model w.r.t. the actions.

To make it more efficient, we use amortised inference. We directly define a policy \(_{}(a_{t}|s_{t})\) under the latent state of the world model. By composing the learnt world model and the policy, we can form a new generative model of the state sequence by the chain of \(s_{t} a_{t} s_{t+1} a_{t+1} s_{T}\). Then we will get the same optimisation problem as eq. (8).

To sum up, in AIME, we use the same objective function - the ELBO - in both phases with the only difference being the source of the action sequence. We provide the pseudo-code for the algorithm in Algorithm 1 with the colour highlighting the different origins of the actions between the two phases.

``` Data: Embodiment dataset \(D_{}\), Demonstration dataset \(D_{}\), Learning rate \(\) # Phase 1: Model Learning  Initialise world model parameters \(\) and \(\) whilemodel has not convergeddo\(\{_{1:T},a_{0:T-1}\} D_{body}\)\(s_{0} 0\) for\(t=1:T\)do\(s_{t} q_{}(s_{t}|s_{t-1},a_{t-1},f_{}(o_{t}))\) Compute objective function \(J\) from eq. (1) Update model parameters \(+_{}J\), \(+_{}J\) # Phase 2: Imitation Learning  Initialise policy parameters \(\) whilepolicy has not convergeddo\(o_{1:T} D_{demo}\)\(s_{0} 0\) for\(t=1:T\)do\(a_{t-1}_{}(a_{t-1}|s_{t-1})\)\(s_{t} q_{}(s_{t}|s_{t-1},a_{t-1},f_{}(o_{t}))\) Compute objective function \(J\) from eq. (1) Update policy parameters \(+_{}J\) ```

**Algorithm 1**AIME

## 4 Experiments

To test our method, we need multiple environments sharing an embodiment while posing different tasks. Therefore, we consider Walker and Cheetah embodiment from the DeepMind Control Suite (DMC Suite) . Officially, the Walker embodiment has three tasks: stand, walk and run. While the Cheetah embodiment only has one task, run, we add three new tasks, namely run backwards, flip and flip backwards, inspired by previous work . Following the common practice in the benchmark , we repeat every action two times when interacting with the environment. For both embodiments, the true state includes both the position and the velocity of each joint and the centre of mass of the body. In order to study the influence of different observation modalities, we consider three settings for each environment: _MDP_ uses the true state as the observation; _Visual_ uses images as the observation; _LPOMDP_ uses only the position part of the state as the observation, so that information-wise it is identical to the _Visual_ setting but the information is densely represented in a low-dimensional form.

To generate the embodiment and demonstration datasets, we train a Dreamer  agent in the Visual setting for each of the tasks for 1M environment steps. We take the replay buffer of these trained agents as the embodiment datasets \(D_{}\), which contain \(1000\) trajectories, and consider the converged policy as the expert to collect another \(1000\) trajectories as the demonstration dataset \(D_{}\). We only use \(100\) trajectories for the main experiments, and the remaining trajectories are used for an ablation study. The performance of the policy is measured by accumulated reward. The exact performance of the demonstration dataset can be found in Appendix D. Besides the above embodiment datasets, we also study two datasets generated by purely exploratory behaviour. First, we use a random policy that samples uniformly from the action space to collect \(1000\) trajectories, and we call this the _random_ dataset. Second, we train a Plan2Explore  agent for \(1000\) trajectories and label its replay buffer as the _p2e_ dataset. Moreover, for the Walker embodiment, we also merge all the above datasets except the _run_ dataset to form a _mix_ dataset. This resembles a practical setting where one possesses a lot of experience with one embodiment and uses all of it to train a single foundational world model.

### Benchmark results

We mainly compare our method with BCO(0) . BCO(0) first trains an IDM from the embodiment dataset and then used the trained IDM to label the demonstration dataset and then uses Behavioural Cloning (BC) to recover the policy. We do not compare with other methods since they either require further environment interactions [26; 27] or use a goal-conditional setting  which does not suit the locomotion tasks. More details about related works can be found in Section 5. The implementation details can be found in Appendix B.

The main results of our comparison are shown in Figure 2 and Figure 3. Overall, we can see that AIME largely outperforms BCO(0) in all the environment settings on Walker and on POMDP settings on Cheetah. AIME typically achieves the lowest performance on the Visual setting, but even that is comparable with BCO(0)-MDP which can access the true states. We attribute the good performance of AIME to two reasons. First, the world model has a better data utilisation rate than the IDM because the world model is trained to reconstruct whole observation sequences, while the IDM only takes

Figure 2: Performances on Walker. Each column indicates one task and its associated demonstration dataset, while each row indicates the embodiment datasets used to train the model. The title of each figure is named according to \(D_{} D_{}\). Numbers are computed by averaging among \(100\) trials and then normalised to the percentage of the expert’s performance. Error bars are showing one standard deviation. The last row and column are averaged over the corresponding task or dataset. The error bar is large for them due to aggregating performance distributed in a large range.

short clips of the sequence and only predicts the actions. Thus, the world model has less chance to overfit, learns better representations and provides better generalisation. Second, by maximising the evidence, our method strives to find an action sequence that leads to the same outcome, not to recover the true actions. For many systems, the dynamics are not fully invertible. For example, if a human applies force to the wall, since the wall does not move, one cannot tell how much force is applied by visual observation. The same situation applies to the Walker and Cheetah when certain joints are locked due to the singular pose. This same phenomenon is also discussed in .

We also find that, comparing with the Walker experiments, the performance on Cheetah is lower and the improvement offered by AIME is smaller. We think it is because the setup for Cheetah is much harder than Walker. Although the tasks sound similar from the names, e.g. flip and flip backward, due to the asymmetrical structure of the embodiment, the behaviour for solving the tasks can be quite different. The difference limits the amount of knowledge that can be transferred from the embodiment dataset to the demonstrations. Moreover, some tasks are built to be hard for imitation. For example, in the demonstration of the flip tasks, the cheetah is "flying" in the air and the actions taken there is not relevant for solving the tasks. That leaves only a few actions in the sequence that are actually essential for solving the task. We think this is more challenging for AIME since it needs to infer a sequence of actions, while BCO(0) is operating on point estimation. That is, when the first few actions cannot output reasonable actions to start the flip, then the later actions will create a very noisy

Figure 3: Performances on Cheetah. Each column indicates one task and its associated demonstration dataset, while each row indicates the embodiment datasets used to train the model. The title of each figure is named according to \(D_{} D_{}\). _runb_ and _flipb_ are short hands for _run backwards_ and _flip backwards_. Numbers are computed by averaging among \(100\) trials and then normalised to the percentage of the expert’s performance. Error bars are showing one standard deviation. The last row and column are averaged over the corresponding task or dataset. The error bar is large for them due to aggregating performance distributed in a large range.

gradient since none of them can explain the "flying". In general, poorly modelled regions of the world may lead to noisy gradients for the time steps before it. On the other hand, we can also find most variants achieve a good performance on the run backward demonstration dataset, which is mainly due to low expert performance (see Appendix D) for the task that makes imitation easy. Last but not least, since we follow the common practise for the benchmark , the Cheetah embodiment is operated on \(50\)Hz which is much higher than the \(20\)Hz used in Walker. Higher frequency of operation makes the effect of each individual action, i.e. change in the observation, more subtle and harder to distinguish, which poses an additional challenge for the algorithms.

**Influence of different datasets** As expected, for almost all the variants of methods, transferring within the same task is better than transferring between different tasks. In these settings, BCO(0)-MDP is comparable with AIME. However, AIME shines in cross-task transfer. Especially when transferring between run and walk tasks and transferring from stand to run on Walker, AIME outperforms the baselines by a large margin, which indicates the strong generalisability of a forward model over an inverse model. We also find that AIME makes substantially better use of exploratory data. On Walker, AIME largely outperforms baselines when using the p2e dataset as the embodiment dataset and outperforms most variants when using the random dataset as the embodiment dataset. Moreover, when transferring from the mix dataset, except for the MDP version, AIME outperforms other variants that train the world model on just any single individual task dataset of the mixed dataset. This showcases the scalability of a world model to be trained on a diverse set of experiences, which could be more valuable in real-world scenarios.

**Influence of observation modality** Compared with BCO(0), AIME is quite robust to the choice of observation modality. We can see a clear ladder pattern with BCO(0) when changing the setting from hard to easy, while for AIME the result is similar for each modality. However, we can still notice a small difference when comparing LPOMDP and Visual settings. Although these observations provide the same information, we find AIME in the LPOMDP setting performs better than in the Visual setting in most test cases. We attribute it to the fact that low-dimension signals have denser information and offer a smoother landscape in the evidence space than the pixels so that it can provide a more useful gradient to guide the action inference. Surprisingly, although having access to more information, AIME-MDP performs worse than AIME-LPOMDP on average. The biggest gaps happen when transferring from exploratory datasets, i.e. the p2e dataset on Walker and the random dataset on Cheetah. We conjecture this to the fact the world model is not trained well with the default hyper-parameters, but we defer further investigation to future work.

### Ablation studies

In this section, we conduct some ablation studies to investigate how AIME's performance is influenced by different components and design choices. We will mainly focus on using the _mix_ embodiment dataset and transfer to _run_ task, which represents a more realistic setting where we want to use experience from multiple tasks to transfer to a new task.

**Sample efficiency and scalability** To test these properties, we vary the number of demonstrations within \(\{1,2,5,10,20,50,100,200,500,1000\}\). We also include BC with the true action as an oracle baseline. The results are shown in Figure 4. BCO(0) struggles with low-data scenarios and typically needs at least \(10\) to \(20\) demonstrations to surpass the performance of a random policy. In contrast, AIME demonstrates continual improvement with as few as \(2\) trajectories. And surprisingly, thanks to the generalisation ability of the world model, AIME even outperforms oracle BC when the demonstrations are limited. These demonstrate the superior sample efficiency of the method. Moreover, the performance of AIME keeps increasing as more trajectories are provided beyond \(100\), which showcases the scalability of the method.

**Objective function** The objective function, i.e. ELBO, consists of two terms, the reconstruction term \(J_{}\) and the KL term \(J_{}\). To investigate the role that each term plays in AIME, we retrain two variants of AIME by removing either of the terms. As we can see from Figure 5, removing either term will negatively impact the results. When we compare the two variants, only using the KL term is better in settings with low-dimensional signals, while using only the reconstruction term yields a slightly better result for the high-dimensional image signal. But on all settings, the performance of using only the KL term is very close to the one that use both terms. This suggests that the latent state in the world model has already mostly captured the essential part of the environment. Although it is still worse than using both terms, it sheds some light on the potential of incorporating decoder-free models  into the AIME framework.

**Components** Compared with the BCO(0) baseline, AIME consists of two distinct modifications: one is to use an SSM to integrate sequences and train the policy upon its latent representation; the other is to form an implicit IDM via gradients rather than training an IDM explicitly. We design two baselines to investigate the two components. First, to remove the SSM, we train a forward dynamics model directly on the observations of the embodiment dataset and use that as an implicit IDM for imitation on the demonstration dataset. We term this variant IIDM. Second, we train a separate IDM upon the trained latent state of the world model and use that to guide the policy learning in phase 2. The detailed derivation of the IDM formulation can be found in Appendix C. Figure 5 clearly demonstrates the significance of the latent representation for performance. Without the latent representation, the results are severely compromised across all settings. However, when compared to BCO(0), the IIDM does provide assistance in the high-dimensional Visual setting, where training an IDM directly from the observation space can be extremely challenging. While having IDM on the latent representation leads to a better performance comparing with BCO(0), but it still performs worse than AIME, especially on the POMDP settings.

## 5 Related works

**Imitation learning from observations** Most previous works on imitation learning from only observation can be roughly categorised into two groups, one based on IDMs [25; 9; 30; 28] and one based on generative adversarial imitation learning (GAIL) [31; 26; 27]. The core component of the first group is to learn an IDM that maps a state transition pair to the action that caused the transition. [25; 9] use the IDM to label the expert's observation sequences, then solve the imitation learning problem with standard BC. [30; 28] extend the IDM to a goal-conditioned setting in which the IDM is trained to be conditioned on a future frame as the goal instead of only the next frame. During deployment, the task is communicated on the fly by the user in the form of key-frames as goals. The

Figure 4: Ablation of the number of demonstrations on \(mix run\) transfer on the Walker embodiment. The performance is shown as the normalised returns over \(3\) seeds and \(100\) trials for each seed. The shaded region represents one standard deviation.

Figure 5: Ablation studies on \(mix run\) transfer on the Walker embodiment. Numbers are computed by averaging among \(3\) seeds and \(100\) trials for each seed, and then normalised to the percentage of the expert’s performance. Error bars are showing one standard deviation.

setup mainly suits for the robot manipulation tasks in their paper since the user can easily specify the goals by doing the manipulation himself, but not suits for the locomotion tasks, in which it is not clear what a long-term goal of observation is and also not practical set the next observation as the goal and demonstrate that in a high frequency by the user. Different from these methods, our approach uses a forward model to capture the knowledge of the embodiment. In the second group of approaches, the core component is a discriminator that distinguishes the demonstrator's and the agent's observation trajectories. Then the discriminator serves as a reward function, and the agent's policy is trained by RL . As a drawback, in order to train this discriminator the agent has to constantly interact with the environment to produce negative samples. Different from these methods, our method does not require further interactions with the environment, enabling zero-shot imitation from the demonstration dataset. Besides the majority, there are also works [32; 33] don't strictly fit to the two groups.  also use forward model like us by learning a latent action policy and a forward dynamic based on the latent action. However, it still needs online environment interactions to calibrate the latent actions to the real actions.  is hybrid method that uses both of the components and focus on a setting that the demonstrations are coming from a different embodiment.

**Reusing learnt components in decision-making** Although transferring pre-trained models has become a dominant approach in natural language processing (NLP) [34; 35; 36] and has been getting more popular in computer vision (CV) [37; 36], reusing learnt components is less studied in the field of decision-making . Most existing works focus on transferring policies [38; 9; 7]. On the other hand, the world model, a type of powerful perception model, that is purely trained by self-supervised learning lies behind the recent progress of model-based reinforcement learning [39; 17; 19; 40; 41; 42; 43; 44]. However, the transferability of these world models is not well-studied.  learns a policy by using a pre-trained world model from exploration data and demonstrates superior zero-shot and few-shot abilities. We improve upon this direction by studying a different setting, i.e. imitation learning. In particular, we communicate the task to the model by observing the expert while  communicates the task by a ground truth reward function which is less accessible in a real-world setting.

## 6 Discussion & conclusion

In this paper, we present AIME, a model-based method for imitation from observations. The core of the method exploits the power of a pre-trained world model and inverses it w.r.t. action inputs by taking the gradients. On the Walker and Cheetah embodiments from the DMC Suite, we demonstrate superior performance compared to baselines, even when some baselines can access the true state. The results showcase the zero-shot ability of the learnt world model.

Although AIME performs well, there are still limitations. First, humans mostly observe others with vision. Although AIME works quite well in the _Visual_ setting, there is still a gap compared with the LPOMDP setting where the low-dimensional signals are observed. We attribute this to the fact that the loss surface of the pixel reconstruction loss may not be smooth enough to allow the gradient method to find an equally good solution. Second, in this paper, we only study the simplest setting where both the embodiment and sensor layout are fixed across tasks. On the other hand, humans observe others in a third-person perspective and can also imitate animals whose body is not even similar to humans'. Relaxing these assumptions will open up possibilities to transfer across different embodiments and even directly from human videos. Third, for some tasks, even humans cannot achieve zero-shot imitation by only watching others. This may be due to the task's complexity or completely unfamiliar skills. So, even with proper instruction, humans still need to practise in the environment and learn something new to solve some tasks. This motivates an online learning phase 3 as an extension to our framework. We defer these topics to future work.

We hope this paper demonstrates the great potential of transferring a learnt world model, incentivises more people to work in this direction and encourages researchers to also share their learnt world model to contribute to the community.