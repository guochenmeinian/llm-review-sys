# Minimum Description Length and Generalization Guarantees for Representation Learning

Milad Sefidgaran \({}^{}\), Abdellatif Zaidi \({}^{}\)\({}^{}\), Piotr Krasnowski\({}^{}\)

\({}^{}\) Paris Research Center, Huawei Technologies France

\({}^{}\) Universite Gustave Eiffel, France

{milad.sefidgaran2,piotr.g.krasnowski}@huawei.com, abdellatif.zaidi@univ-eiffel.fr

###### Abstract

A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees. For example, the information bottleneck method seeks a good generalization by finding a minimal description of the input that is maximally informative about the label variable, where minimality and informativeness are both measured by Shannon's mutual information.

In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the "Minimum Description Length" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the "multi-letter" relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior. In particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms. Our compressibility approach, which is information-theoretic in nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two essential ingredients: block-coding and lossy-compression. The latter allows our approach to subsume the so-called _geometrical compressibility_ as a special case. To the best knowledge of the authors, the established generalization bounds are the first of their kind for Information Bottleneck type encoders and representation learning. Finally, we partly exploit the theoretical results by introducing a new _data-dependent_ prior. Numerical simulations illustrate the advantages of well-chosen such priors over classical priors used in IB.

## 1 Introduction

A key performance indicator of stochastic learning algorithms is their capability to generalize, i.e., perform equally well on training and unseen data. However, designing learning algorithms with good generalization guarantees remains a major challenge. A popular approach involves learning an encoder part and a decoder part. The encoder aims at generating a suitable representation of the input (referred to as "latent variable") by extracting relevant features from the input data. The decoder aims at optimizing the performance of the learning task for the given training dataset, known as empirical risk minimization (ERM), based only on the learned latent variables. This approach is grounded on the idea that performing ERM on the latent variable (instead of the input itself) prevents overfitting.

**Information Bottleneck.** Several approaches have attempted to formalize the concept of a "good representation" [SST10, AFDM17, VDOV\({}^{+}\)17, DKSV20]. Perhaps, the most prominent is the _information bottleneck (IB)_ method which was first introduced in [TPB00] and then extended in several directions [SST10, AFDM17, AZ19, KTW19, Fis20, RGTS20, KASK22]. The IB approach was deemed useful to analyze deep neural networks [SZT17, ZEAS20, GP20, Gei21] and guide their training process. Forinstance, in supervised learning tasks the IB seeks representations that capture "minimum" information about the input data (shoots for good generalization power) while providing "maximum" information about the label (shoots for small empirical risk), where the amounts of captured and provided information are measured using Shannon mutual information. More precisely, denoting by \(Y\) the label variable and by \(X\) the input data, the IB latent variable \(U\) is chosen so as to maximize the mutual information \(I(U;Y)\) while minimizing the mutual information \(I(U;X)\). Equivalently, this can be formulated as maximizing the Lagrange cost

\[I(U;Y)- I(U;X),\]

where \( 0\) designates a Lagrange multiplier. Let \((X^{m},U^{m})\) denote a vector, or block, of \(m\) independent and identically distributed (i.i.d.) realizations \((X_{i},U_{i}),i=1,,m\), of discrete variables \((X,U) P_{X,U}\). Fix a distribution \(P_{}\) such \(P_{X,}=P_{X,U}\). Essentially by means of the rate-distortion theoretic _covering lemma_, it is easy to see that one can get a suitable description \(\) of \(X\) using only \(I(U;X)\) bits per symbol. Precisely, generating a codebook of roughly \(l_{m} 2^{mI(U;X)}\) vectors \(^{m}[j]^{m},j=1,,m\), all drawn i.i.d. according to \(P_{U}\), the covering lemma states that for large \(m\) there exists at least one index \(j\) for which the empirical distribution of \((X^{m},^{m}[j])\) is arbitrary close to \(P_{X,U}\) in some suitable sense. That is, the empirical distributions of \((X^{m},U^{m})\) and \((X^{m},^{m}[j])\) are close. Hence, an "equivalent" version \(^{m}\) of \(U^{m}\) can be described with roughly \(mI(U;X)\) bits. Intuitively, this makes a connection between the mutual information \(I(U;X)\) and the concept of _Minimal Description Length_ (MDL)  when one considers MDL of the latent variables, not that of model parameters . Moreover, it is now relatively well known that there exists a connection between the generalization error of a learning model and the MDL of the parameters of that model, see, e.g., . A notable work  has considered MDL of the predicted labels of a super-sample of training and test data.

**Critics to IB.** The aforementioned connections perhaps formed a belief that \(I(U;X)\) is closely related to the generalization performance of representation learning algorithms. This belief, however, is controversial and conflicting evidence has been reported . For instance, using the term \(I(U;X)\) as a regularizer has been criticized for four main reasons : **i.** The few existing upper bounds on the generalization error which involve (among other terms) the mutual information \(I(U;X)\) reported in  and the very recent and concurrent work  are not convincing. For example, the bound of  holds only when the alphabets of the input and latent spaces are finite; and, in that case, it states that for any \(>0\), with probability \(1-\) it holds that: \((S,W)+C _{}\), where \((S,W)\) is the generalization error of model \(W\), \(C_{}:=||/\), \(||\) is the size of the latent space and \(n\) is the size of the training dataset. For reasonable setups, however, the term \(C_{}\) dominates and their bound becomes vacuous . The bound reported in  suffers similar shortcomings - For further details on this, see Appendix B.2. **ii.** Experimental evidence shows dependence of the generalization error on the so-called _geometrical compression_ rather than on \(I(U;X)\). Geometrical compression occurs when the latent variables are concentrated around a limited number of clusters. See [1, Fig. 2] for a visual representation. **iii.**\(I(U;X)\) is invariant to bijection; and, as such, it does not favor learning algorithms/representations with simple decision boundaries and does not reflect the "structure" or "simplicity" of the encoder/decoder. Please refer to [1, Section 4.3] for a detailed discussion and examples. **iv.** Finally, for deterministic algorithms \(I(U;X)\) can take large or even infinite values, especially for continuous variables or high-dimensional data, hence limiting the usefulness of IB .

**MDL/Compressibility.** Several works have studied MDL/compressibility to establish generalization bounds. In this context, key is the "Occam's razor" principle  which, e.g., for binary classification tasks, states that if the labels of the training data \(S=\{Z_{1},,Z_{n}\}\), \(Z_{i}=(X_{i},Y_{i})\) predicted by a model \(W\) can be described by \(k n\) number of bits, then the learned model \(W\) has a good generalization performance. There exist three closely related lines of work that use different means to describe the dataset labels: **i.** The first describes the predicted labels via the hypothesis (parameter models). It includes the works of . Recently, it was shown in  that information-theoretic bounds , PAC-Bayes bounds  and intrinsic dimension-based approaches  also fall into this category. The proof uses so-called "fixed-size"  and "variable-size"  compressibility frameworks introduced therein. **ii.** The second formulates the minimal description of labels as follows: if a learning algorithm can predict all labels of the training data \(S\) by using only samples from a small subset of \(S\), then the learning algorithm is guaranteed to generalize well. This second approach was initiated by  and followed by others including . **iii.** The third, initiated by , deals directly with the compression of the predicted labels. This approach is shown to be related to the previous two lines, and also to PAC-Bayes and VC-dimension-based results. Moreover, it is the closest to the prob of establishing bounds on the generalization error of representation learning algorithms in terms of the compressibility of the latent variables. This approach is detailed in Appendix B.1.

In this work we aim at understanding the theory of representation learning through a rigorous investigation of the connection between the MDL (or compressibility) of the latent variable and the generalization performance of representation learning algorithms. In doing so, we first study the single-step prediction model of Fig. 0(a); and, then, we leverage the developed tools to study the two-step (encoder-decoder) prediction model of Fig. 0(b). We also examine the claimed relationship between MDL and \(I(U;X)\).

**Contributions.** Specifically, the main contributions of this work are as follows.

* For the prediction model of Fig. 0(a), inspired by  we establish an _information-theoretic_ framework that allows us to measure the compressibility (MDL) of the predicted labels in terms of a new object which is the KL-divergence of a vector of labels and any arbitrary symmetric prior. For a proper choice of prior, this new measure reduces to the sample-wise mutual information and hence inherits the associated properties. However, unlike the sample-wise mutual information, it can also reflect the structure/simplicity of the learning algorithm. Furthermore, by extending the framework to "lossy compressibility", this new measure does not become vacuous when one considers continuous variables instead of discrete labels. Moreover, it subsumes geometrical compressibility as a special case.
* We also establish both in-expectation and tail generalization bounds in terms of this compressibility measure of the predicted labels. In a simple case, the in-expectation bound can be cast into the form \[()}{n}}.\] The tail bound involves a similar expression. This generalization bound easily recovers the VC-dimension bound as a special case; and, hence, it also shows how the introduced compressibility measure depends on the complexity of the hypothesis class.
* Our results make a connection between the compressibility framework of  and the functional conditional mutual information (f-CMI) of , which itself is an extension of the CMI-framework developed by . In particular, this connection shows how the f-CMI framework can be leveraged to study the compressibility of the predicted labels.
* For the encoder-decoder prediction model of Fig. 0(b), the framework is extended non-trivially to the case in which instead of the compressibility of the predicted labels it is the compressibility of the latent variable which is considered. The results for this prediction model, which are our _main results_ in this paper as given in Section 3, are generalization bounds for the representation learning algorithms in terms of the complexity of the latent variable. The established in-expectation and tail bounds hold for _any_ decoder; and, for the \(K\)-class classification task for example, take the form \[2()+K+2}{n}}.\] These bounds appear to be the first of their kind for the studied representation learning setup. In part, their utility lies in that: i) they reflect the structure of the encoder class, ii) they do not become vacuous for deterministic encoders with continuous latent space and iii) they can explain the geometrical compressibility phenomenon. Thus, our framework reveals that the "joint" MDL of the latent space is more related to the encoder structure, rather than the mutual information \(I(U;X)\).
* Finally, our results suggest that _data-dependent_ priors can be used in place of the data-independent prior of the popular variational IB (VIB) method. We conduct experiments that illustrate the advantage brought up by proper choices of _data-dependent_ priors over VIB.

Our results also open up several future directions as discussed in Appendix C.4.

Figure 1: Considered learning frameworks.

**Notation.** Random variables, their realizations, and their alphabets are denoted respectively by upper-case letters, lower-case letters, and Calligraphy fonts, _e.g.,_\(X\), \(x\), and \(\). Their distributions and expectations are denoted by \(P_{X}\) and \([X]\). For ease of presentation, when \(\) is a discrete set, \(P_{X}\) is a _probability mass function_. Otherwise, \(P_{X}\) is a _probability density function_. A collection of \(n\) random variables \((X_{1},,X_{n})\) is denoted by \(X^{n}\) or \(\). We use the notation \(\{x_{i}\}_{i=1}^{m}\) to denote a sequence of \(m\) real or natural numbers. The set \(\{1,,K\}\), \(K\), is denoted by \([K]\). Finally, \(^{+}\) denotes the set of non-negative real numbers. Our results are expressed in terms of information-theoretic functions. For two distributions \(P\) and \(Q\), the Kullback-Leibler (KL) divergence is defined as \(D_{KL}(P||Q)_{P}[(P/Q)]\) if \(P Q\), and \(\) otherwise. The mutual information between two random variables \(X,Y P_{X,Y}\) with marginals \(P_{X}\) and \(P_{Y}\) is defined as \(I(X;Y) D_{KL}(P_{X,Y}|P_{X}P_{Y})\). The reader is referred to  for further information.

Our results in this paper will be expressed in terms of _symmetric_ conditional priors. The following definition formalizes three types of symmetry.

**Definition 1** (Symmetric Priors).: _For \(U,V P_{U,V}\), let \((U^{2n},V^{2n})\) be vectors composed of \(2n\) i.i.d. instances \((U_{i},V_{i}) P_{U,V}\), \(i[2n]\). For any permutation \([2n][2n]\), denote \(U_{}^{2n}:=(U_{(1)},,U_{(2n)})\) and \(V_{}^{2n}(V_{(1)},,V_{(2n)})\)._

* _[leftmargin=*]_
* _Type-I symmetry: Define type-I permutations as the set of permutations_ \([2n][2n]\) _having the property that for any_ \(i[n]\)_, the sets_ \(\{(i),(i+n)\}\) _and_ \(\{i,i+n\}\) _are equal. The conditional prior_ \((U^{2n}|V^{2n})\) _has type-I symmetry if_ \((U_{}^{2n}|V_{}^{2n})\) _is invariant to arbitrary type-I permutations. This definition first appeared in_ _[_1_]_ _(called "almost exchangeable prior" therein) and was used for the CMI framework in_ _[_1_]__._
* _Type-II symmetry: The conditional prior_ \((U^{2n}|V^{2n})\) _has type-II symmetry if_ \((U_{}^{2n}|V_{}^{2n})\) _is invariant to any arbitrary permutations_ \([2n][2n]\)_._
* _Type-III symmetry: For a random variable_ \(Z P_{Z|U^{2n},V^{2n}}\)_, the conditional prior_ \((U^{2n}|V^{2n},Z)\) _has type-III symmetry if_ \((U_{}^{2n}|V^{2n},Z)\) _is invariant to any permutation_ \([2n][2n]\) _having the property that_ \(V_{i}=V_{(i)}\) _for every_ \(i[2n]\)_._

_Throughout, if the underlying \((U^{2n},V^{2n})\) is clear from the context, for ease of the notation the corresponding sets of Type-I and Type-II priors will be denoted simply as \(_{i}\) and \(_{ii}\) respectively._

**Problem setup.** Unless indicated otherwise, we consider the \(K\)-class classification setup. Let \(Z=(X,Y)\) be some _input data_ taking value over the _input space_\(=\) according to an unknown distribution \(\). We call \(X\) the _features_ of the data, and \(Y\) its _label_, where \(=[K]\). We assume a _training dataset_\(S=\{Z_{1},,Z_{n}\}^{ n}=:P_{S}\), composed of \(n\) i.i.d. samples \(Z_{i}=(X_{i},Y_{i})\) of the input data, is available. We denote the features and labels of \(S\) by \( X^{n}^{ n}\) and \( Y^{n}^{ n}_{S^{}}\), respectively. We often use also a _ghost_ or _test_ dataset \(S^{}=\{Z^{}_{1},,Z^{}_{n}\}^{ n}:P_{S^{ }}\), where \(Z^{}_{i}=(X^{}_{i},Y^{}_{i})\). Similarly, we denote the features and labels of \(S^{}\) by \(^{} X^{ n}^{ n}_{X}\) and \(^{} Y^{ n}^{ n}_{Y}\), respectively.

## 2 Generalization bounds in terms of predicted label complexity

In this section, we formulate a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm. Our proposed framework can be seen as a suitable generalization of the framework of Blum and Langford , in which the generalization encompasses _lossy compression_ of the predicted labels and which exploits _block-coding_. However, unlike in the original framework based on a (compression) game between two agents Alice and Bob (see Appendix B.1 for details), this work adopts a rate-distortion theoretic perspective.

For ease of exposition, the results are presented for classification problems and the 0-1 loss function, but they can be extended trivially to any continuous \(\) and bounded loss function. Consider the setup in Fig. 0(a). Let \(^{n}\) be a possibly stochastic learning algorithm. That is, for a given \(S=(Z_{1},,Z_{n})^{n}\) the algorithm picks a hypothesis or model \(W=(S)\). Also, let the induced joint distribution over \(\) be denoted as \(P_{S,W}\); and the induced conditional distribution over \(\) given \(S\) be denoted as \(P_{W|S}\). For every input data \(z=(x,y)\), every choice of hypothesis \(w\) induces a conditional distribution \(P_{|X,W}(|x,w)\) on \(}=\). For convenience, we use the following hand notations:

\[P_{|X,W}^{ n}(}|,w)= _{i[n]}P_{|X,W}(_{i}|x_{i},w),\] \[P_{|X,W}^{ n}(}^{}|^{},w)= _{i[n]}P_{|X,W}(_{i}^{}|x_{i}^{},w),\] \[P_{|X,W}^{ n}(},}^{ }|,w^{})= _{i[n]}P_{|X,W}(_{i}|x_{i},w)P_{|X,W}(_{i}^{}|x_{i}^{},w).\]The quality of the prediction is measured by the loss function \(}\) given by

\[(z,w)_{ P_{|X,w}(|x,w)}[}_{\{y\}}],\] (1)

where \(\) stands for the indicator function. The associated empirical and population risks for this loss are defined as \(}(s,w)_{i[n]}(z_{i},w)\) and \((w)_{Z}[(Z,w)]\), respectively. Finally, the generalization error is defined as \((s,w)(w)-}(s,w)\).

### Compressibility framework

Now, we introduce briefly the joint compression of a _block_ of the predicted labels. Further details can be found in Appendix C.1. Consider \(m\) i.i.d. pairs of train and test datasets \(S_{j}(Z_{j,1},,Z_{j,n})\) and \(S^{}_{j}(Z^{}_{j,1},,Z^{}_{j,n})\), where \(Z_{j,i}=(X_{j,i},Y_{j,i})\) and \(Z^{}_{j,i}=(X^{}_{j,i},Y^{}_{j,i})\). Let \(S^{m}=(S_{1},,S_{m})\), \(S^{ m}=(S^{}_{1},,S^{}_{m})\) and \(W^{m}(W_{1},,W_{m})\), where \(W_{j} P_{W_{j}|S_{j}}\). Denote the predicted labels using model \(W_{j}\) for inputs \(X_{j,i}\) and \(X^{}_{j,i}\) as \(_{j,i}\) and \(^{}_{j,i}\), for \(j[m]\) and \(i[n]\). Moreover, let \(^{2n}_{j}^{2n}\) denote a rearrangement of the elements of \((S_{j},S^{}_{j})\) in a way that makes it indistinguishable whether a given sample \(z\) is from \(S_{j}\) or \(S^{}_{j}\). In the following two subsections, we investigate two such rearrangements of a given \((S,S^{})\) as \(^{2n}\). Then, for the collection \((S^{m},S^{ m})\), each pair \((S_{j},S^{}_{j})\) for \(j[m]\) is rearranged independently and the matrix of all rearrangements is denoted by \(^{2mn}\). Finally, given some \(^{2mn}\), let denote the rearranged versions of \((Y^{mn},Y^{ mn})\) and \((^{mn},^{ mn})\) respectively by \(^{2mn}\) and \(^{2mn}\).

Our approach is based on studying the _compressibility_ of the rearranged model-predicted labels \(}^{2mn}\), from an information-theoretic point of view. The rationale is as follows: since the (rearranged) predicted-labels vector \(}^{2mn}\) agrees mostly with the true labels \(^{2mn}\) on the dataset \(S^{m}\), then in accordance with "Occam's Razor" theorem  the model \(W\) is guaranteed to generalize well if \(}^{2mn}\) can be described using only a few bits (or nats). We leverage source coding arguments to measure the _compressibility_ of \(}^{2mn}\). In our block-coding rate-distortion theoretic framework, a compression rate \(R^{+}\) is said to be _achievable_ if there exists a compression codebook of size \( e^{mR}\) (fixed a priori) which _covers_ the space spanned by the model-predicted labels \(}^{2mn}\) with high probability. That is, if \(R\) is achievable then \(}^{2mn}\) can be described using \(R^{+}\) nats. Formally, \(R\) is achievable if there exists a sequence of label books \(\{}_{m}\}_{m}\), with \(}_{m}\{}[r],r[l_{m}]\} ^{2mn}\), \(l_{m}\), \(}[r]=(}_{1}[r],,}_{m}[r])\) and \(}_{j}[r]=(_{j,1}[r],,_{j,2n}[r])^{2n}\), such that: \(\), \(l_{m} e^{mR}\) and \(\), there exist a sequence \(\{_{m}\}_{m}\) for which \(_{m}_{m}=0\) such that with probability at least \((1-_{m})\) over the choices of \(S^{m},S^{ m}\) one can find at least one index \(r[l_{m}]\) whose associated \(}[r]\) equals \(}^{2mn}\).

As explained further in Appendix C.1.3, this _fixed-size_ compressibility framework, which is suitable to upper bound the expectation of the generalization error, can be extended to _variable-size_ compressibility in a way that is similar to  in order to upper bound the generalization error with high probability. We notice that, in essence, the core idea of the compressibility framework of Blum-Langford, which we recall in Appendix B.1, can be _seen_ as a _one-shot_ counterpart of our approach. As discussed in , in the one-shot case one deals with the "worst case" scenario; and this results in bounds that involve combinatorial terms . In contrast, our information-theoretic framework allows us to bound the compression rate \(R\) in terms of a simpler new quantity: the relative entropy of the joint conditional \(P(^{n},^{ m}|Y^{n},Y^{ n})\) and a (symmetric) conditional prior \(\) over \(}^{2n}\) given \(Y^{2n}\).

Our approach to establishing a bound on the compressibility of \(}^{2mn}\) in terms of information-theoretic measures starting from the combinatorial approach of  and by using block-coding essentially consists in a suitable combination of the following key proof steps: _(i)_ introduce and use the symmetries of Definition 1 to rearrange \((S^{m},S^{ m})\), _(ii)_ generate the codewords using symmetric priors and _(iii)_ analyze the minimum codebook size needed to "cover" reliably \(}^{2mn}\), essentially by use of the _covering lemma_. As shown in  and also our proofs, the last two ingredients can be merged via the Donsker-Varadhan's variational representation lemma. The application of this lemma is reminiscent of information-theoretic works on the generalization error such as .

### Generalization bounds using type-I symmetric priors

One way to rearrange indistinguishably \((S,S^{})\) as \(^{2n}\) is as follows: Let \(=(J_{1},,J_{n})\) be a vector of \(n\) i.i.d. Bernoulli\(()\) random variables \(J_{i}\{i,i+n\}\), \(i[n]\). For every \(i[n]\), let the random variable \(J^{c}_{i}\{i,i+n\}\) be defined such that \(J^{c}_{i}=i+n\) if \(J_{i}=i\) if \(J_{i}=i+n\). Also, let the vector \(^{c}=(J^{c}_{1},,J^{c}_{n})\). For \(i[n]\), we let the random variables \(_{J_{i}}\) and \(_{J^{c}_{i}}\) defined as \(_{J_{i}}=Z_{i}\) and \(_{J^{c}_{i}}=Z^{}_{i}\). Observe that the vector \(^{2n}=(_{1},,_{2n})\) is a \(\)-dependent random re-arrangement of the samples of the training and ghost datasets \(S\) and \(S^{}\). Without knowledge of \(\) every element of the vector \(^{2n}\) has equal likelihood to be from \(S\) or \(S^{}\). A similar construction was used in the context of the analysis of Rademacher complexity and the CMI of . We use Type-I symmetric priors, block coding and information-theoretic _covering_ arguments in order to analyse the space spanned by the random vector \(^{2n}\). This yields the generalization bound stated in the next theorem whose proof is deferred to Appendix E.2.

**Theorem 1**.:
1. _Let_ \(_{i}\) _be the set of type-I symmetric conditional priors on_ \((},}^{})\) _given_ \((,^{})\)_. Then,_ \(_{S,W}[(S,W)]\)_, with_ \[R_{_{i}}_{, ^{}}D_{KL}_{^{},,W} P_{|X,W}^{ 2n}(},}^{}| ,^{},W)=I (;^{2n}|Y^{2n})\] (2) _where_ \(,^{}_{Y}^{ 2n}\) _and_ \(^{},,W P_{^{}|^{ }}P_{,W|}\)_. Also the mutual information is calculated with respect to the joint distribution_ \(P_{,^{2n},Y^{2n}}=(1/2)^{ n}_{Y}^{  2n}P_{^{2n}_{^{2n}_{^{2n}_{^{2n}_{^{ 2n}_{^{2n}_{^{2n}_{^{2n}_{^{2n}_{^{2n}_{ ^{2n}}_{^{2n}}_{^{2n}}}}}}}}}}}}\) _and the latter term is defined as_ \(_{^{},,W}P_{|X,W}^{ 2n }(^{2n}_{^{2}_{^{2n}}}|,^{},W) \) _in which_ \(^{},,W P_{^{}|Y^{}_{^{2n}}}P_{,W|Y^{2n}_{^{2n}}}\)_._
2. _For any_ \(^{+}\) _and any conditional type-I symmetric prior_ \(\) _on_ \((},}^{})\) _given_ \((,,^{},^{})\)_, with probability at least_ \((1-)\) _over choices of_ \(S,S^{},W P_{S^{}}P_{S,W}\)_, it holds that_1__ 
A couple of remarks are in order. The part i of the result of Theorem 1 can be understood as being some form of the f-CMI of  in which the function \(f\) represents the predicted labels - in fact our result is slightly stronger comparatively, since instead of conditioning on \(Z^{2n}\) as in the f-CMI of  one here conditions only on \(Y^{2n}\). Incidentally, the result also unveils an appealing connection between an extension of the compressibility approach of  (in this extension one needs to consider Type-I symmetric priors) and CMI and f-CMI . However, for type-II and type-III symmetries, used in the coming sections, our framework goes beyond the CMI framework. The type-I priors have been also used in  to establish (fast-rate) tail bounds for the CMI framework. Due to the difference in the considered setups, their results are not directly comparable with ours. We also note that the above result (and some of the results in the rest of the paper) is kept for clarity and can be trivially improved, by i) moving \(_{,^{}}\) outside the square root, ii) by rewriting \([(s,w)]=_{i[n]}[(z _{i},w)]\), and applying the bound for each \([(z_{i},w)]\), similar to , iii) by extending the results in accordance with e-CMI framework , and iv) establishing tail bound on \((S,W)\), by noting that with probability at least \((1-)\), \(}(S^{},W)(W)-\) (similar to Theorem 5).

Next, observe that for any \(_{,^{}|Y,^{ }}_{1}(},}^{}| ,^{},,^{})\), where \(_{1}(},}^{}|, ^{},,^{})\) is an arbitrary type-I symmetric prior, and by using the Jensen's inequality, we have

\[)} _{S,S^{},W P_{S,W}P_{S^{}}}D_{KL} P_{|X,W}^{ 2n}(^{n},^{ n}|X^{n},X^{ n},W) \|_{1}.\] (4)

**Relation to Mutual Information.** A particular choice of the conditional prior \(_{1}\) in the RHS of (4) is \(Q^{ 2n}\) for some prior \(Q\) defined over \(\). For this special choice, the RHS of (4) is given by

\[n_{S,W}_{X_{X|S}}D_{KL}P_{| X,W}(|X,W)\|Q+n_{X^{},W_{X}P_{W}}D_{KL} P_{|X,W}(^{}|X^{},W)\|Q,\] (5)

where \(_{X|S}\) is the empirical distribution of \(X\) in the dataset \(S\). Moreover, by choosing \(Q\) as the marginal distribution of \(\) under \(P_{|X,W}P_{S,W}\), it is easy to see that the first term of the sum of the RHS of (5) coincides with \(n(X;)\) for that choice, where \((;)\) stands for the "empirical mutual information" as computed from the available samples. Thus, the contribution of this term to the bound on generalization error does not necessarily vanish as \(n\) (unless the empirical mutual information itself is small). In fact, as already observed in , there exist models which generalize well but have non-small mutual-information \((X;)\). This instantiates that mutual-information type bounds may fall short of explaining true generalization capability, an observation which was already made in . The bound on the generalization error of our Theorem 1, which is provably tighter (see (4)), then possibly remedies this issue.

In what follows we further investigate the relationship of the result of our Theorem 1, which is based on KL-divergence, to VC-dimension and mutual information type bounds on the generalization error.

**Relation to VC-dimension** Suppose that the VC-dimension of the hypothesis class is \(d\). Then, using the Sauer-Shelah lemma  we get that given any \(\) and \(^{}\) one can have at most \((2en/d)^{d}\) distinct labels. By letting the conditional prior \(_{1}\) be a uniform distribution over all such possible predictions, it is easily seen that the KL divergence term of our Theorem 1 is upper bounded by \(d(2en/d)\). This means that the result of our Theorem 1 recovers and possibly improves over the VC-dimension bound.

**Structure and "simplicity" of the learning algorithm.** Let us consider a simple example. Suppose that \(=\) and let \((0,1)\) be a parameter. Then, consider the set of deterministic classifiers \(w\) as follows: \(P_{|X,W}(|x,w)=1\) if (\(=1\) and \(x\)) _or_ (\(=0\) and \(x<\)); and \(P_{|X,W}(|x,w)=0\) otherwise. It is well known that the VC-dimension of this learning class is \(d=1\). Then, by recalling the aforementioned relation to the VC dimension, we obtain that our Theorem 1 yields a bound on the generalization error which is \(()\). In particular, it is easy to see that this bound vanishes as \(n\). This is in sharp contrast with the mutual information term \((X;)\) which does not necessarily vanish for large \(n\). For example, if the pair \((X,Y)\) is such that \(Y=1\) iff \(X^{}\) for some \(^{}(0,1)\), then the ratio between RHS of (4) and \(n\) converges (for large \(n\)) to a value which is at least \(H(Y)\) (because \((X;) I(X;Y)\) as \(n\) and, in this example, \(I(X;Y)=H(Y)\)).

The above example indicates that using mutual information as a regularizer for learning algorithms (as is the case in IB) may fail to find models that generalize well and have a comparatively simple structure. In fact, using a different approach, it was already observed in  and  that using the mutual information term as a complexity measure does not reflect the "structure" of the learning algorithm and considering it as a regularizer might not favor "simple" learning algorithms. This suggests that mutual information regularizer in the IB approach may be replaced by the KL divergence term of the RHS of (3) (when the latent variables are considered instead of predictions) which does reflect the complexity of the encoder's structure. As investigated and shown in Section 3, in addition to favoring encoders of simpler structure, our approach provides theoretical guarantees of the generalization error.

#### 2.2.1 Lossy compressiblity

The above approach and results can be extended to any bounded loss function and continuous variables \(Y\) and \(\) (see the next section where continuous latent variables are studied). In the regime of continuous variables, a standard result of rate-distortion theory stipulates that any _lossless_ encoding of \(^{n}\) and \(^{ n}\) may require an infinite number of bits, making the generalization bounds vacuous. This is precisely why Shannon's mutual information fails as a regularizer in deterministic learning algorithms with continuous alphabet variables. On the other hand, our approach can be easily extended to include the _lossy compression_ of the labels (see Appendix C.1.2; in the same spirit as done in  for the hypothesis compression. The following result states a lossy in-expectation bound and a lossy tail bound is reported in Appendix A.

**Theorem 2**.: _Let \(_{i}\) be the set of type-I symmetric conditional priors on \((},}^{})\) given \((,^{})\). Then, for any \(\), \(_{S,W}[(S,W)]\) is upper bounded by_

\[_{P_{|S}}_{_{i}} _{,^{}}D_{KL}_{ ^{},,}P_{|X,W}^{ 2n}(},}^{}|,^{},) }+,\]

_where \(,^{}_{Y}^{ 2n}\), \(^{},, P_{^{}|^{ }}P_{,|}\), and the first infimum is over all \(P_{|S}\) satisfying \(_{P_{S,W}P_{|S}}(S,W)-(S,)\)._

A proof of this theorem follows by an easy combination of the distortion criterion with the bound on \(_{S,}(S,)\) obtained by application of Theorem 1 to the compressed model \(\) (not \(W\)). This simple trick, which is related conceptually to _lossy source coding_, prevents the KL-divergence term from taking very large (infinite) values for continuous alphabet variables. Moreover, the lossy compressibility also offers an interpretation of the _geometrical compressibility_ concept that was observed to be related to the generalization performance in .

### Generalization bounds using type-II symmetric priors

In this section, we consider another way to rearrange indistinguishably \((S,S^{})\) as \(^{2n}\), which is inspired by . Let \(=\{T_{1},,T_{n}\}\), be a random set obtained by picking uniformly \(n\) indices from \(\{1,,2n\}\), without replacement. Note that in contrast to \(\) which had i.i.d. components, here the components of are dependent. let \(^{c}=\{1,,2n\}\) be the complement of \(\), having the elements \(^{c}=\{T_{1}^{c},,T_{n}^{c}\}\). Now, for each \(i[n]\), let \((_{T_{i}},_{T_{i}^{c}}^{2})=(Z_{i},Z_{i}^{})\). We use type-II symmetric prior to _cover_ such rearranged vectors. To state the result, first we need to define the function \(h_{D}(x;x^{})\) as follows:

\[h_{D}(x,x^{}) 2h_{b}}{2}-h_{b}( x)-h_{b}(x^{}),\] (5)

where \(h_{b}(x)-x_{2}(x)-(1-x)_{2}(1-x)\). Note that \(h_{D}(x,x^{})/2\) is equal to the Jensen-Shannon divergence between two binary Bernoulli distributions with parameters \(x\) and \(x^{}\). The reader is referred to Appendix A for the relation of this function with the combinatorial term appeared in . The function \(h_{D}(,)\) has the following interesting properties, proved in Appendix E.3.

**Lemma 1**.: \(\;(x,x^{})\)_, we have: **(i)** \(h_{D}(x,x^{})(x-x^{})^{2}\), **(ii)** \(h_{D}(x,0) x\), **(iii)** \(h_{D}(x,x^{})\) is increasing with respect to \(x\) in the range \([x^{},1]\), and **(iv)** \(h_{D}(x,x^{})\) is convex with respect to both inputs._

Now, we state the main result of this section.

**Theorem 3**.: _Let \(_{ii}\) be the set of type-II symmetric priors on \((},}^{})\) given \((,^{})\). Then, for \(n 10\),_

\[nh_{D}_{W}(W),_{S,W}}(S,W)\] \[_{_{ii}}_{, ^{}}D_{KL}_{^{}, ,W}P_{|X,W}^{ 2n}(},}^{ }|,^{},W) +(n)=I;^{2n}|Y^{2n}+(n),\]

_where \(,^{}_{}}^{ 2n}\) and \(^{},,W P_{^{}|^{} }P_{},W|}\). Also the mutual information is calculated with respect to the joint distribution \(P_{,^{2n},Y^{2n}}=P_{}_{}^{ 2n}P_{ ^{2n}_{^{2n}_{^{2n}}},^{2n}_{^{2n}}}|Y^{2n} _{^{2n}_{^{2n}}}\) and the latter term is defined as \(_{^{},,W}P_{|X,W}^{ 2n} (^{2n}_{},^{2n}_{^{c}}|,^ {},W)\) in which \(^{},,W P_{^{}|Y^{2n}_{^{ 2n}}}P_{,W|Y^{2n}_{^{2n}}}\)._

The proof of Theorem 3 is deferred to Appendix E.4. Also, a similar tail bound is provided in Appendix A.

Using the part (i) of Lemma 1, it can be seen that if the value of the KL divergence term is larger than \((n)\) then the bound of Theorem 3 is tighter than that Theorem 1. Also, using part (ii) of Lemma 1 it is seen that if the error on the training set is zero (a setting referred to as "realizable case" in ), our Theorem 3 yields a bound on the generalization error which is \((1/n)\).

## 3 Generalization bounds in terms of latent variable complexity

The generalization bounds of the previous section are particularly useful in the following sense: if the learning algorithm is "simple" enough to produce a low "relative entropy" sequence of labels for training and test sets, then the algorithm generalizes well. However, they have a downside that they cannot be used directly as they are in the optimization, since minimizing those bounds may result in solutions with large empirical risk. In this section, we extend the results of the previous section to settings in which the processing is split into two parts: an encoder part that produces a family of representations that have the property to generalize well (developed according to the guidelines of the previous section) and a decoder part that selects among that family one representation that minimizes the empirical risk. As such the goal of the encoder is to guarantee a small generalization error and that of the decoder is to guarantee a small empirical risk. This procedure, which is similar to the Information Bottleneck method, aims at finding a good balance between generalizing well to unseen data and minimizing the risk of the training data.

Let \(W=(W_{e},W_{d})\), where \(W_{e}\) and \(W_{d}\) are the hypotheses (or models) used by the encoder and the decoder, respectively, as illustrated in Fig. (b)b. Also, let \(U\) denote the output of the encoder, which will be referred to hereafter interchangeably as "representation" or "latent variable". The encoder produces the representation \(U\) according to the conditional \(P_{U|X,W_{e}}\). The decoder produces an estimate \(\) of the true label \(Y\) according to the conditional \(P_{|U,W_{d}}\). We consider a stochastic learning algorithm \(^{n}\) which picks a model \(W=(W_{e},W_{d})_{e}_{d}\) according to \(P_{W|S}\). Let a loss function \(\) be given. For a model \(w=(w_{e},w_{d})\) the quality of the prediction of \(\) is evaluated as

\[(z,w)_{ P_{|X,W}(|x,w)}[ _{\{y\}}]_{U P_{U|X,W_{e}}(U|x,w_{ e})}_{ P_{|U,W_{d}}(|U,w_{d})}_{\{y \}}.\]

Empirical risk, population risk, and generalization error are defined similarly to the previous section. According to the above motivation, we are interested in establishing a bound on the generalization error of the algorithm \(\) that depends only on the part \(W_{e}\) of the model \(W\). In accordance with our compressibility framework of Section 2 such bound would then depend on the complexity of the latent variable \(U\). However, here, the encoder part \(W_{e}\) and the decoder part \(W_{d}\) are both trained using the dataset \(S\). Thus, they may be statistically dependent in general and therefore the "rearrangement" ideas using type-I and type-II symmetries do not work here. To elaborate on this a bit more, recall that for example for type-I symmetry, to rearrange \((,})\) and \((^{},}^{})\) in an indistinguishable manner, we randomly shuffle the positions of the pairs \((Y_{i},_{i})\) and \((Y^{}_{i},^{}_{i})\). If we now consider using the type-I symmetry for the new setup, then shuffling of the pairs \((Y_{i},U_{i},_{i})\) and \((Y^{}_{i},U^{}_{i},^{}_{i})\), changes the dataset \(S\) and thus the distributions \(P_{W|S}\) and \(P_{|W_{d},U}\). Hence, covering the resulting rearranged sequence would unavoidably depend on both encoder and decoder parts. To overcome this issue, we use type-III symmetry, where we leave the train and ghost datasets untouched and randomly "swap" only those latent variables and their corresponding predictions, _i.e., \((U_{i},_{i})\)_ and \((U^{}_{i},^{}_{i})\), that are associated with the train and ghost samples with the same label, _i.e.,_ if \(Y_{i}=Y^{}_{j}\). The following result is the main theorem of this section and the paper derived using the type-III symmetric priors. A formal proof of this result can be found in Appendix E.5.

**Theorem 4** (Generalization Bound for Representation Learning Algorithms).: _Consider a \(K\)-classification learning task. Let \(\) be a type-III symmetric conditional prior over \(U^{2n}\) given \(X^{2n},Y^{2n}\) and \(_{e}_{e}\), namely, \((u_{(1)},,u_{(2n)})|(x_{1},,x_{2n}),(y_{1}, ,y_{2n}),_{e}\) remains the same for all permutations \([2n][2n]\) that preserves the label, i.e., \(y_{(i)}=y_{i}\) for \(i[2n]\). Then,_

\[_{S,W}[(S,W)] 2_{S,S^{ },_{e}}D_{KL}P_{U|X,_{e}}^{ 2n} ,^{}|,^{},_{e} +K+2}{n}}+,\]

_where \(S,S^{},_{e} P_{S,_{e}}P_{S^{}}\) and the infimum is over all Markov kernels \(P_{_{e}|S}\) such that for \(=(_{e},W_{d})\), \(_{P_{S,W}P_{_{e}|S}}[(S,W)-(S,)]\)._

The bound of Theorem 4 on the generalization error of representation learning, and the related IB-encoder, is to the best of our knowledge the first of its kind and significance. In fact, while few works have already investigated the generalization error of IB , for the special case of discrete variables, their bounds, which in part are expressed in terms of the empirical mutual information \((U;X)\), appear to be vacuous for most settings as already observed in  (see introduction and Appendix B.2 for more details). Furthermore, as also extensively discussed in , mutual information may not be a good indicator of the generalization error. Such aspects, including how mutual information fails to reflect "simplicity" or "structure" of the encoder, are discussed in more detail in the previous section. An alternate bound on the generalization error of representation learning appeared in . However, their bound only holds for encoders that find the _optimal_ representation in a sense defined therein.

Now, few remarks on the result of Theorem 4 are in order. First, note that the generalization bound of this theorem depends _only_ on the encoder and more precisely on the complexity of the latent space \(U\); and, so, this bound is valid for _any_ choice decoder.2 Next, similar to the bounds of the Theorems 1-5, the KL-divergence term of Theorem 4 explicitly takes into account the "structure" and "simplicity" of the encoder, and, therefore, it resolves one of the major issues of the IB method .

Moreover, the result of Theorem 4 suggests that the considered prior could depend on the data and model. This enables a larger class of choices for the prior. Examples include (i) Symmetric jointly Gaussian priors, (ii) priors that depend on the category (label), _i.e.,_ the prior for a category \(k[K]\) at each optimization iteration could possibly depend on some statistics of the latent variables of all training and test samples having label \(k\), and (iii) priors that steer latent variables toward some pre-defined "constellations" in the latent space, depending on their label. On this aspect, note that allowing the prior to depend on labels enables a connection with the "conditional information-bottleneck" of .

Another important property of our bound of Theorem 4 is that, unlike the empirical mutual information term of ,3 it does not become infinite for deterministic encoders with continuous input-output. Moreover, our approach which is based on _lossy compression_ provides an interpretation of the _geometric compression_ of  where latent variables are concentrated around some constellation points.4 We hasten to mention that "lossy compression" here should not be confused with approaches that add noise after the encoder. The main difference is that in those approaches the noisy representations are passed to the decoder; while here, the "noisy" representations are used only to estimate lossy compressibility. These "noisy" representations can be achieved by either adding small "noise" to the model parameters or the latent variable. Note that by increasing the noise level the \(\) term in Theorem 4 increases while the KL-divergence term potentially decreases. In practice, a suitable trade-off between the two effects can be found by treating the amount of added noise as a _hyper-parameter_ to optimize.

Finally, a similar tail bound on the generalization error has been established in Appendix A.

## 4 Experiments

In this section, we illustrate our results via some experiments. For more detail and other experiments the reader is referred to Appendix D.

Our main Theorems 4 and 7_suggest_ that for the representation learning setup of Fig. (b)b the generalization error is controlled essentially by the divergence term \(D_{KL}P_{U|X,W_{e}}^{ 2n}(,^{}| ,^{},,^{},W_{e}) \) where \(\) is a type-III symmetric prior. In a sense, this also means that, with a proper _data-dependent_ choice of the prior, the usage of the aforementioned divergence term as a regularizer possibly offers better generalization guarantees (relative, e.g., to the conventional data-independent prior of VIB). In what follows, we propose a new family of data-dependent priors which appear to better capture the "simplicity" or "structure" of the encoder. We also compare the associated accuracy with that offered by the fixed prior of VIB.

More precisely, in VIB the prior \(\) factorizes as a product of \(2n\) scalar standard Gaussian priors, _i.e._, \(=Q^{ 2n}\), where \(Q=(_{m},_{m})\), \(_{m}^{m}\) is the zero vector and \(_{m}\) is the \(m m\) identity matrix. In our lossless approach, which we here coin as Lossless Category-Dependent VIB (CDVIB), the prior \(\) still factorizes as a product of \(2n\) scalar Gaussian priors, _i.e._, \(=_{i[2n]}Q_{i}\), but with three major differences: **i.** Each \(Q_{i}\) can be chosen from a set of \(M K\) priors - \(M\) priors for each label. **ii.** Unlike VIB, each of \(M K\) priors can depend on some statistics of \((,^{})\) and also on the label of each sample, **iii.** Unlike VIB, where the prior is fixed, here \(\) is 'learned" during the training phase. To this end, the mean and variance of the scalar priors are updated after each training iteration using a moving average with some small coefficient, allowing the latent space to better adapt to the structure of the encoder and the data. Taking the moving average also has another role, which is to "partially" reproduce the effect of the "ghost data" (which comes from the test dataset that is usually unavailable during training). In lossy CDVIB, similar priors are considered but over "noisy" versions of the latent variables, _i.e., \((},^{})\)_. Note that while the noisy versions are considered for the regularizer, the decoder receives as input \((,^{})\).

We consider CIFAR10 [KH\({}^{+}\)09] image classification using a small CNN-based encoder and a linear decoder. The results shown in Fig. 2 indicate that the model trained using our priors achieves better (\( 2.5\%\)) performance in terms of both generalization error and population risk. This _suggests_ that our priors help to find a better _representation_ than the standard VIB prior.

Figure 2: Accuracy during test phase of our two-step prediction model trained using the standard VIB prior and our “lossless” CDVIB and “lossy” CDVIB priors computed for \(M=5\). The values are averaged over 5 runs. The graphs are displayed together with 95% bootstrap confidence intervals.

Acknowledgement

The authors would like to thank the anonymous reviewers for their many insightful comments and suggestions. In particular, by pointing out the connection between our results and the f-CMI literature.