# How to Scale Your EMA

Dan Busbridge\({}^{}\)  Jason Ramapuram\({}^{}\)  Pierre Ablin\({}^{}\)  Tatiana Likhomanenko\({}^{}\)

Eeshan Gunesh Dhekane  Xavier Suau  Russ Webb

###### Abstract

Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important machine learning tool is the model EMA, a functional copy of a target model, whose parameters move towards those of its target model according to an Exponential Moving Average (EMA) at a rate parameterized by a momentum hyperparameter. This model EMA can improve the robustness and generalization of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have not considered the optimization of the model EMA when performing scaling, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of a model EMA and demonstrate the rule's validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, a 6\(\) wall-clock time reduction under idealized hardware settings.

## 1 Introduction

With data and models becoming progressively larger (Chen et al., 2020; Kaplan et al., 2020; Bommasani et al., 2021; Srivastava et al., 2022), the ability to reduce training wall-clock time is a requirement for practical Machine Learning (ML) at scale. Optimizer scaling rules allow us to find faster learning procedures that produce similar results. For example, the _linear scaling rule_ for Stochastic Gradient Descent (SGD) (Krizhevsky, 2014; Goyal et al., 2017), states that the learning rate should be scaled linearly with the batch size. This optimizer scaling works _both ways_. Access to larger computational resources means one can train equivalent models in reduced wall-clock time. Alternatively, with access to limited computational resources, larger distributed computations can be replicated at increased wall-clock time.

Many ML algorithms rely on a _model EMA_, a functional copy of a _target model_2, whose parameters move towards those of its target model according to an Exponential Moving Average (EMA) (Definition 1.1) at a rate parameterized by a momentum hyperparameter \(\).

**Definition 1.1** (EMA Update).: _The EMA update for the model EMA parameters \(_{t}\) following target model parameters \(_{t}\) at iteration \(t\) with momentum \( 1-_{}\) is_

\[_{t+1}=\,_{t}+(1-)\,_{t}(1-_{})\,_{t}+ _{}\,_{t}.\] (1)

The _model EMA_ has a number of desirable properties: i) the model EMA inhabits wider minima than the target model, reducing overfitting and improving generalization (Ruppert, 1988; Polyak and Juditsky, 1992; Huang et al., 2017; Izmailov et al., 2018; He et al., 2022); ii) compared to the target model, the model EMA moves slowly, making it useful as a stabilizer for networks governing Bellman updates in reinforcement learning, (Lillicrap et al., 2016); and iii) the model EMA is relatively cheap to compute, whilst providing a valid model but _different_ to the target model. This third property has made the model EMA a common choice for the _teacher_ in many distillation setups, from semi-supervised learning (Tarvinen and Valpola, 2017; Sohn et al., 2020; Manohar et al., 2021; Higuchi et al., 2022), to Self-Supervised Learning (SSL) methods like Bootstrap Your Own Latent (BYOL) (Grill et al., 2020), DINO (Caron et al., 2021), and data2vec (Baevski et al., 2022, 2020).

Despite its significant role in optimization, a recipe for adapting the EMA Update (Definition 1.1) when changing batch size has, to the best of our knowledge, been absent. To address this, we derive an EMA Scaling Rule (Definition 1.2) which states how the EMA momentum \(\) hyperparameter _should_ be modified3.

**Definition 1.2** (EMA Scaling Rule).: _When computing the EMA update (Definition 1.1) of a model undergoing stochastic optimization with batch size \(= B\), use a momentum \(=^{}\) and scale other optimizers according to their own scaling rules._

In Definition 1.2, the momentum \(\), which is defined at batch size \(B\), typically corresponds to a "good hyperparameter choice", although this does not need to be the case in general. In this paper, we make the following contributions.

1. With the assumptions of Goyal et al. (2017), we derive an EMA Scaling Rule: the EMA update _momentum_ should be scaled _exponentially_ with the batch size (Definition 1.2).
2. To validate this EMA Scaling Rule theoretically, we propose Stochastic Differential Equation (SDE) approximations of optimization in the presence of a model EMA (Section 2.2). This model EMA contributes to the loss, covering semi-supervised learning and SSL. We prove that these approximations are first order weak approximations, and that our EMA Scaling Rule is correct in the SDE limit under realistic gradient assumptions (Corollary 2.1.1).
3. We empirically validate the EMA Scaling Rule in synthetic settings (Section 3.1) and real-world settings where the model EMA plays an increasingly significant role in optimization: i) where the model EMA is used during inference instead of the target model (Section 3.2); ii) pseudo-labeling, where the model EMA (_teacher_) follows the target model (_student_), and the _student_ is optimized on a mixture of a) labeled data and b) data without labels, whose pseudo-labels are produced by the _teacher_ (Section 3.3); and iii) self-supervised learning, which is the same as the semi-supervised case, except there is no labeled data (Section 3.4).
4. We observe that pseudo-labeling and SSL training dynamics during optimizer warm-up are not always able to be replicated at large batch sizes using _only_ the EMA Scaling Rule. We propose and verify practical methods to overcome this limitation, enabling us to scale to a batch size of 24,576 with BYOL Vision Transformers (ViTs), reducing wall-clock training by 6\(\) under idealized hardware scenarios while maintaining performance of the batch size 4096 baseline.

Finally, to aid practitioners looking to scale, in Appendix C we provide a _Scaling Toolbox_, which gives practical advice on how to scale systematically, collecting known scaling rules, and explaining how to think about the SDE perspective of optimization.

The EMA Scaling Rule

We begin with an informal discussion of scaling rules and motivate the existence of an exponential scaling rule for the momentum parameter controlling the update of the model EMA.

### Background and an informal discussion of scaling rules

Consider a model with parameters \(_{t}\) at iteration \(t\) updated with SGD (Definition 2.1).

**Definition 2.1** (SGD Update).: _The SGD update for a model with parameters \(_{t}\) at iteration \(t\) given a minibatch \(=\{x^{(b)} P_{}:b=1,2,,B\}\) of \(B=||\) samples with learning rate \(\) is_

\[_{t+1}=_{t}-_{x} _{}(x;_{t}),\] (2)

_where \(\) is the loss function, \(_{}(x;_{t})\) is the parameter gradient for the sample \(x\) at iteration \(t\), and the \(x\) are Independent and Identically Distributed (i.i.d.) from \(P_{}\)._

Iterating over a sequence of independent minibatches \(_{0},_{1},,_{-1}\) produces model parameters

\[_{t+}=_{t}-_{j=0}^{ -1}_{x_{j}}_{}(x;_{t+ j}).\] (3)

If gradients vary slowly \(_{}(x;_{t+j})_{} (x;_{t})\), \(j=0,,-1\), _one_ SGD step with \(=\) on a batch \(}=_{i}_{i}\) of size \(= B\) results in \(}_{t+1}_{t+k}\), yielding the SGD Scaling Rule (Definition 2.2).

**Definition 2.2** (SGD Scaling Rule).: _When running SGD (Definition 2.1) with batch size \(= B\), use a learning rate \(=\)(Krizhevsky, 2014; Goyal et al., 2017)._

For clarity in this work, we adopt the naming convention _[Algorithm Name] Scaling Rule_, which means all parameters of those algorithms are appropriately scaled from batch size \(B\) to \( B\).

As discussed in Goyal et al. (2017), although the assumption of slowly changing gradients is strong, if it is true, then \(_{t+k}}_{t+1}\)_only_ if \(=\). The validity of the SGD Scaling Rule has been formally studied. In particular, there was ambiguity regarding whether the scaling should be a square-root or linear (Krizhevsky, 2014). SDE approaches have resolved this ambiguity, and have been used to estimate the scaling \(\) when the SGD Scaling Rule is no longer guaranteed to hold (Li et al., 2021).

To address model parameter EMAs, we first restate the EMA Update (Definition 1.1).

**Definition 1.1** (EMA Update).: _The EMA update for the model EMA parameters \(_{t}\) following target model parameters \(_{t}\) at iteration \(t\) with momentum \( 1-_{}\) is_

\[_{t+1}=\,_{t}+(1-)\,_{t}(1-_{})\, _{t}+_{}\,_{t}.\] (1)

The model EMA parameters \(\) do not typically receive gradient information, we take the convention that \(\) is close to one, and the \(_{}\) subscript will be omitted where it is clear from the context.

Assuming again that gradients change slowly \(_{}(x;_{t+j},_{t+j})_{ }(x;_{t},_{t})}\), for gradient \(}\), iterating over \(\) independent minibatches produces model states (see Appendix E.1 for derivation)

\[_{t+}\\ _{t+}\\ }=1&0&-\\ (1-)&&0\\ 0&0&1^{}_{t}\\ _{t}\\ }=_{t}-\,\,}\\ ^{}\,_{t}+(1-^{})\,}+( _{})\\ }.\] (4)

The first row is the SGD Scaling Rule (Definition 2.2). The third row implements the _slowly changing gradients_ assumption for the first row. The second row is equivalent to a single EMA update (Definition 1.1) with momentum \(=^{}\); we can take a _single_ SGD update with batch size \(= B\) and learning rate \(=\), and a _single_ EMA update with momentum \(=^{}\), and we get \((}_{t+1},}_{t+1})(_{t+}, _{t+})\) up to terms \((_{})\). This yields the EMA Scaling Rule (Definition 1.2).

**Definition 1.2** (EMA Scaling Rule).: _When computing the EMA update (Definition 1.1) of a model undergoing stochastic optimization with batch size \(= B\), use a momentum \(=^{}\) and scale other optimizers according to their own scaling rules._

The EMA Scaling Rule was derived for SGD, and is extended to other optimizers in the following way. An optimizer scaling rule ensures \(}_{t+1}=_{t+}\), satisfying identification for the first row. Next, the zeroth order term in \(_{}\) in the second row in Equation 4 is optimizer-independent, and therefore unchanged. Finally, the first order terms in \(_{}\) in the second row, corresponding to the scaling rule error, are an EMA accumulation of target model \(\) updates under optimization, which is still \((_{})\), although its functional form may be different for different optimizers.

The above discussion is intended to give an intuition for why the EMA momentum should be scaled exponentially. As we have used the same slow-moving gradient assumption as the original SGD Scaling Rule, this may cast doubt on whether our rule is correct. To remove this ambiguity, we will follow Smith and Le (2018); Li et al. (2021); Malladi et al. (2022), and show that the EMA Scaling Rule (Definition 1.2) is correct in the SDE limit under more realistic gradient assumptions.

### The EMA Scaling Rule through the lens of stochastic differential equations

SDEs are a tool typically used to obtain scaling rules from first principles (Li et al., 2021; Malladi et al., 2022). In the following, we use SDEs to obtain strong theoretical guarantees for the EMA Scaling Rule found in Section 2.1. We consider the following discrete dynamics for EMA:

\[&_{k+1}=_{k}-\,_{k},\ \ \ _{k}= f(_{k},_{k})+\,_{k},\ \ _{k}_{}(_{k},_{k}),\\ &_{k+1}=\,_{k}+(1-)\,_{k},\] (5)

where \(>0\) is the noise scale, \(_{}(_{k},_{k})\) is the gradient noise distribution, assumed to be zero-mean and variance \((_{k},_{k})\) independent of \(\), and \( f(_{k},_{k})_{}f(_{k},_{k})\). We posit a dependency of the loss \(f\) on the EMA \(\) in order to cover semi-supervised (Section 3.3) and SSL (Section 3.4). The case of Polyak-Ruppert averaging (Section 3.2), is covered by letting \(f\) be independent of \(\).

We aim to obtain an SDE approximation of Equation 5 as \(\) goes to zero. The scaling rule for iterations of \(\) is well known (Li et al., 2021): we let \(_{0}=\). The analysis of Section 2.1 gives the scaling rule \(=\) and \(=^{}\). Linearizing this rule near \(=0\) gives \(=1-(1-)\), which is a linear relationship between \(1-\) and \(\). We therefore let \(_{0}=(1-)/\) and consider the SDE

\[& d_{t}=- f(_{t},Z_{t})\,dt+ _{0}\,(_{t},Z_{t})^{}\,dW_{t},\ \ \ W_{t}\ ,\\ & dZ_{t}=_{0}(_{t}-Z_{t})dt,\] (6)

where \(_{t}\) and \(Z_{t}\) are SDE variables relating to model and EMA parameters respectively. The SDE in Equation 6 approximates the discrete iterations of Equation 5 when the learning rate \(\) goes to zero. One way to see this is that an Euler-Maruyama discretization of the SDE with learning rate \(\) exactly recovers the discrete iterations. More formally, we have Theorem 2.1, which is in the same spirit as those found in Li et al. (2021); Malladi et al. (2022). In the theorem, \(G^{}\) is the set of functions with derivatives up to order \(\) that have at most polynomial growth (see Definition D.1).

**Theorem 2.1** (SDE for SGD + EMA; informal see Theorem D.1).: _Assume that \(f\) is continuously differentiable, with \(f G^{3}\). Let \(_{t},Z_{t}\) be solutions of Equation 6, and \(_{k},_{k}\) iterations of Equation 5 with \(^{} G^{2}\). Then, for any time horizon \(T>0\) and function \(g G^{2}\), there exists a constant \(c>0\) independent of \(\) such that_

\[_{k=0,\,,\, T/}|[g(_{ k},Z_{ k})]-[g(_{k},_{k})]| c.\] (7)

Theorem 2.1 formalizes the intuition that the SDE is an accurate approximation of the discrete iterations. In turn, it allows validating the scaling rule in the same spirit as in Malladi et al. (2022).

**Corollary 2.1.1** (Validity of the EMA Scaling Rule).: _Assume that \(f\) is continuously differentiable, with \(f G^{3}\) and \(^{} G^{2}\). Let \(^{(B)}_{k},^{(B)}_{k}\) be iterations of Equation 5 with batch size \(B\) and hyperparameters \(,\). Let \(^{( B)}_{k},^{( B)}_{k}\) be iterates with batch size \( B\), and \(\) determined by the SGD Scaling Rule (Definition 2.2) and \(\) determined by the EMA Scaling Rule (Definition 1.2). Then, for any time horizon \(T>0\) and function \(g G^{2}\), there exists a constant \(c>0\) independent of \(\) such that_

\[_{k=0,\,,\, T/}|[g(^{(  B)}_{ k/},^{( B)}_{ k/ })]-[g(^{(B)}_{k},^{(B)}_{k})]|  c.\] (8)Corollary 2.1.1 shows that two trajectories with different batch sizes are close in the limit of small learning rate, demonstrating the validity of Definition 1.2. A natural follow-up question is _what happens when an adaptive optimizer is used instead of SGD?_ Malladi et al. (2022) study this without an EMA and characterize how hyperparameters change with the noise scale. In particular, they show that under a high gradient noise hypothesis, there exists a limiting SDE. In Appendix D, we derive the limiting SDEs for RMSProp and Adam with an EMA. Although a formal proof of closeness between the iterations and these SDEs is beyond the scope of this work, these SDEs indicate that the EMA Scaling Rule holds for adaptive algorithms. We demonstrate this empirically in Section 3.

## 3 Experiments

Now that we have derived and shown the validity of the EMA Scaling Rule, we verify it empirically. The experiments validate the EMA Scaling Rule for a variety of uses of EMA and are ordered by increasing influence of the role of EMA on the optimization procedure (see Table 1). The baseline in all of our experiments is _without the EMA Scaling Rule_, which applies all known relevant scaling rules _except_ the EMA Scaling Rule, and represents previous best practice.

### Polyak-Ruppert averaging in a simple setting

At inference, it is typical to use a model EMA, known as Polyak-Ruppert Averaging (Definition 3.1).

**Definition 3.1** (Polyak-Ruppert Average).: _When optimizing model parameters \(\), compute their EMA \(\) (Definition 1.1). Use \(\) instead of \(\) at inference (Polyak & Juditsky, 1992; Ruppert, 1988)._

We begin by showing the EMA Scaling Rule is _required_ to match parameter trajectories in a simple setting. Consider the optimization of \(\) in a _noisy parabola_ whose loss \(()\) is parameterized by coefficients for curvature \(a>0\), scaled additive noise \(b 0\), and additive noise \(c 0\):

\[()=\,^{2},_{k+1}=_{k }-\,_{k},_{k}=a\,_{k}+_{k}, _{k}(0,_{k}^{2}+ }{}).\] (9)

The scaling factor \(\) in the covariance denominator implements gradient noise reduction as scaling (i.e. batch size) increases (Jastrzebski et al., 2017). Let \(\) be optimized with SGD (Definition 2.1) and \(\) be a Polyak-Ruppert average (Definition 3.1) for \(\) with momentum \(=1-\). At scaling \(=1\), we use \(_{B}=_{B}=10^{-4}\) and \(I_{B}=10^{4}\) iterations, to yield a total time \(T=I_{B}_{B}=1\). To keep gradients \((1)\) and gradient noise non-negligible, we take \(a=1\), \(b=0.5\), and \(c=0\).

First, we observe the effect of scaling on a single run (Figure 0(a)) by tracking the position of the model EMA. We see that at scaling \(=8\) or \(=256\), the runs using the EMA Scaling Rule match the baseline trajectory, whereas the runs using the baseline momentum do not, with a greater deviation induced by greater scaling \(\). Even at \(=8\), there is a significant difference between scaled and unscaled trajectories, despite the seemingly small numerical difference of their momenta4.

Second, we consider whether the EMA Scaling Rule is optimal. To do this, inspired by the SDE analysis (Section 2.2), we define the approximation error, \((,,g)\), of a test function \(g\) for a given

 p{113.8pt}}   Technique & Role of Model EMA \\  Polyak-Ruppert average, Sec. 3.2 & \(\) undergoes optimization and is tracked by \(\), which does not affect \(\). \(\) is an estimate of \(\) with a time horizon and variance determined by \(B\) and \(\). \\  Continuous pseudo-labeling, Sec. 3.3 & _Pre-Training_ is as above in Polyak-Ruppert Averaging. _After Pre-Training_, \(\) (_teacher_) produces targets for \(\) (_student_) from unlabeled data, which is combined with labeled data. The optimization endpoint is dependent on \(B\) and \(\). \\  Self-supervised Learning, Sec. 3.4 & As above in _After Pre-Training_, except there is no labeled data. The optimization endpoint is dependent on \(B\) and \(\). \\   

Table 1: The role of the model EMA \(\) in the optimization of \((,)\) given a target model \(\) for different techniques, ordered by increasing influence of the EMA model. All statements assume a momentum \(0<1\) and that the target model \(\) is subject to stochastic optimization at a batch size \(B\).

scaling \(\) using momentum \(\), and the value of the momentum \(^{*}(,g)\) that minimizes this error:

\[^{*}(,g)= *{arg\,min\,Err}(,,g),(,,g)_{k=0,,T/}|\,g(_{k})- \,g(_{k/}^{(,)})|.\] (10)

For scalings \(\{1,2,4,,1024\}\), we determine the optimal momentum \(^{*}\) and compare it to the EMA Scaling Rule (Figure 0(b), left). The scaling rule tracks the \(^{*}\) until \(=256\), when the \(^{*}\) become systematically higher. We see target model error increase at \(=256\) (Figure 0(b), right). As the target model error is EMA-independent, this indicates that the SGD Scaling Rule is breaking. At the lower scaling \(=64\), there is an inflection point in the EMA Scaling Rule approximation error, before the model error grows. This difference indicates the \(O(_{})\) terms of Equation 4 are beginning to influence the EMA update. Finally, these observations are true in \(D=100\) dimensions, (Appendix F.1), and we stress that _not_ changing the momentum at every scaling \(\) induces large approximation error, indicating there is merit to using the EMA Scaling Rule.

### Supervised learning on real data with Polyak-Ruppert averaging

We now turn to real-world classification where the target model \(\) optimizes a parametric log-likelihood \(_{} p(|;)\) with inputs and labels \((,)\) drawn from a joint distribution \(p(,)\).

Image ClassificationWe consider a variant of the original SGD Scaling Rule result (Goyal et al., 2017) and train a ResNetv2 (He et al., 2016) on ImageNet1k (Russakovsky et al., 2014) (Figure 2) using a three step learning rate schedule. The base momentum \(_{B}=0.9999\) at batch size 1024 was found by hyperparameter optimizing for EMA test performance, and we seek to achieve this optimized performance at different batch sizes. We _do not_ apply the EMA Scaling Rule on the Batch Normalization (Ioffe and Szegedy, 2015) statistics5. We observe that _without_ the EMA Scaling Rule, there is a significant drop in model EMA test performance, whereas _with_ the EMA Scaling Rule, we

Figure 1: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\(=1\), black dashed) to \(=8\) (left) and \(=256\) (right), with (\(=_{B}^{}\), blue) and without (\(=_{B}\), red) the EMA Scaling Rule. (b, left) The momentum according for different scaling rules and the empirically optimal \(^{*}\) (Equation 10). (b, right) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange).

Figure 2: _ResNetv2-50 Polyak-Ruppert averaging on ImageNet1k_ for different scalings \(\). The baseline model (\(=1\), black dashed) uses batch size 1024 and momentum \(_{B}=0.9999\), is scaled down to a batch size of 512 (left), and up to a batch size of 4096 (right) with (blue, \(=_{B}^{}\)) and without (red, \(=_{B}\)) the EMA Scaling Rule (Definition 1.2). Bands indicate the mean and standard deviation across three runs.

can approximate the baseline model EMA test top-1 performance across all batch sizes. We match baseline EMA statistics across the full trajectory batch size 2048, where the test EMA performance diverges. This is due to non-EMA test performance dropping for high \(\) (see Appendix F.2). We observe that model EMA top-1 is approximately 0.2% to 0.3% higher than the target model.

**Automatic Speech Recognition (ASR)** We train a transformer (Vaswani et al., 2017) using the Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Adam optimizer on the _train-clean-100_ subset (100h) of LibriSpeech (Panayotov et al., 2015) (for details see Appendix G). We apply the Adam Scaling Rule (Malladi et al. (2022), Definition C.3) and use dynamic batching (minibatch size \(\) sequence length = const = 290s, and \(s\) indicates audio duration in seconds).

_Without_ the EMA Scaling Rule, there is a significant difference in model EMA test Word Error Rate (WER) trajectories compared to the baseline, whereas _with_ the EMA Scaling Rule, trajectories match, as is shown in Figure 3. We note that compared to image classification, in ASR, the model EMA converges to similar final performance irrespective of use of the scaling rule. This convergence is due to the longer training time compared to the EMA horizon as discussed in Table 1 (see Appendix E.2 for a proof sketch). Although in this specific case one can achieve similar _final performance_ without the EMA Scaling Rule, it is _necessary_ to use the EMA Scaling Rule in order to replicate the full training trajectory, which gives _guarantees_ on properties like final performance (see Corollary 2.1.1). We also observe a growing gap between the baseline and EMA-scaled trajectories as we increase \(\). Inspecting the train loss and non-EMA test WER, which _do not_ depend on the EMA update (see Figure 14, Appendix G.1), indicates this is due to a breakdown of the Adam Scaling Rule. _In summary, evaluation on ASR shows that the EMA Scaling Rule holds in practice for sequential data with dynamic batch sizes, as well as when using adaptive optimization._

### Semi-supervised speech recognition via pseudo-labeling

We continue using the same ASR model and training pipeline of Section 3.2. However, we consider semi-supervised learning via continuous pseudo-labeling where labeled (_train-clean-100_, 100h) and unlabeled (the rest of LibriSpeech, 860h) data are given during training, and the model EMA is involved in the overall optimization (Likhomarenko et al., 2021, 2022; Manohar et al., 2021; Higuchi et al., 2022). We first pre-train a target model (_student_) on a limited labeled set for a short period (e.g. 20k steps of \(B=8 290s^{6}\)). Concurrently, the student updates a model EMA (_teacher_). After pre-training, we continue training the student with both labeled and unlabeled data, with the teacher first transcribing unlabeled data from the batch producing Pseudo-Labels (PLs). These PLs are treated by the student as ground-truth transcriptions, and standard supervised optimization is performed.

Compared to Polyak-Ruppert Averaging (Section 3.2), where the model EMA plays no role in the joint optimization, we observe that in PL it is _essential_ to employ the EMA Scaling Rule in order to match the model trajectories at scaled batch sizes. When the EMA Scaling Rule is not used, Figure 4 reveals a significant difference in PL quality trajectory, leading to a higher test WER.

For \(>2\), we found the Adam Scaling Rule does not perfectly match the reference trajectory in the pre-training phase. This results in a significantly different PL quality at the start of pseudo-labeling (20k steps of \(B=8 290s\)), which affects the training dynamics (Berrebbi et al., 2023). To

Figure 3: _Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100)_ with different scalings \(\). The baseline (\(=1\), black dashed) is trained with Adam and momentum \(_{B}=0.99995\) at a _dynamic batch size_\(B=8 290s\), which corresponds to a single train step on the \(x\)-axis. We investigate dynamic batch sizes down to \(B=2 290s\) (left) and up to \(B=32 290s\) (right), with (blue, \(=_{B}^{}\)), and without (red, \(=_{B}\)) the EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout.

alleviate the Adam Scaling Rule mismatch effect for \(>2\), we postpone the pseudo-labeling until pre-training on labeled data gives similar validation WER, see Appendix G. With this heuristic, we can match the baseline trajectory with the EMA Scaling Rule up to \(=8\) (Figure 4).

_In summary, (a) model EMA affects the optimization process of pseudo-labeling in ASR resulting in the necessity of EMA Scaling Rule to be applied while scaling the batch size; (b) an optimizer scaling rule breakdown results in the EMA Scaling Rule breakdown but this effect can be alleviated by longer pre-training on labeled data having similar PLs quality at the start across different scalings._

### Self-supervised image representation learning

Finally, we turn our attention to distillation based Self-Supervised Learning (SSL). where the model EMA is the _teacher_(Grill et al., 2020; Nizzumi et al., 2023; Caron et al., 2021; Oquab et al., 2023).

We will use BYOL (Grill et al. (2020), Definition 1.1)7 for our investigation into scaling as it is well-studied (Tian et al., 2021; Richemond et al., 2023), relatively simple to implement due to minimal hyper-parameters, and obtains competitive results (Grill et al., 2020; Koppula et al., 2022). Since BYOL learns through self-referential distillation, momentum plays a significant role in optimization. We analyze: i) a ResNet-18 (He et al., 2016) on CIFAR10 (Krizhevsky et al., 2014) (Figure 5) using SGD (Definition 2.1); and ii) a ViT-B/16 (Dosovitskiy et al., 2021) on ImageNet1k using AdamW (Loshchilov and Hutter, 2019). A recipe for BYOL using ViTs is provided in Appendix H.3.

**ResNet-18 on CIFAR-10** We begin with a ResNet-18 model and short training duration to enable quick iteration, and an SGD optimizer as it has as _known_ scaling rule. This allows us to probe the EMA Scaling Rule without potential confounders like poor gradient-based optimizer scaling8.

We observe that _without_ the EMA Scaling Rule, there is a drop in test top-1 linear probe (Definition H.3) performance compared to the baseline, whereas _with_ the EMA Scaling Rule, we closely match the baseline model until batch size 4096. We show that this result is consistent for a range of base learning rates \(_{B}\) and momenta \(_{B}\) in Appendix H.8. At batch size 8192, we see a performance gap between the scaled model using the EMA Scaling Rule and the baseline. We speculate that this is due to dynamics early in the BYOL training process that are challenging to replicate at larger batch sizes. To test, and potentially circumvent this, we introduce _Progressive Scaling_ (Definition 3.2).

**Definition 3.2** (Progressive Scaling, informal; see Appendix C.4).: _Given batch size \(B\) and hyperparameters at \(B\), slowly increase the batch size to the desired largest batch size during training. At any intermediate batch size \(= B\), all hyperparameters are scaled according to their scaling rules._

Figure 4: _Transformer pseudo-labeling on LibriSpeech with different scalings \(\). The baseline (\(=1\), black dashed) is trained with Adam at a dynamic batch size of \(8 290\) seconds, which corresponds to a single train step on the \(x\)-axis. The model EMA (teacher) is updated with momentum \(_{B}=0.9999\). We investigate dynamic batch sizes down to \(B=4 290s\) (left) and up to \(B=64 290s\) (right), with (blue, \(=_{B}^{}\)) and without (red, \(=_{B}\)) the EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. For \( 2\), we start pseudo-labeling after \(20/\) training steps; while for \(>2\), we start when pre-training WER matches the baseline WER._

We see that transitioning to the higher batch size _during_ the warmup period results in a model optimization trajectory that diverges from the baseline, whereas transitioning _after_ warmup results in matching final trajectories of the scaled and baseline models. In summary, _progressive scaling_ allows us to match BYOL dynamics at large batch sizes, provided we transition after the warmup period. This observation is consistent with our hypothesis regarding BYOL dynamics during warmup.

Vision Transformers on ImageNet1kProgressive Scaling coupled with the EMA Scaling Rule is required when scaling BYOL ViTs (Figure 6), enabling baseline loss tracking to a batch size of 24,576. Perfect scaling fails at batch size 32,768, consistent with observations in supervised learning (Goyal et al., 2017; Huo et al., 2021). Despite the breakdown, there is only a small drop in 1.6% probe performance when using the EMA Scaling Rule, compared to as 44.56% drop _without_ it. We also observe that it is sometimes possible to match test model performance using _only_ Progressive Scaling and _not_ the EMA Scaling Rule, although this still induces a training loss mismatch. We stress that such an approach is _not_ guaranteed to work and discuss when this approach succeeds and fails in Appendix H.6 and Figure 22.

Figure 5: _ResNet-18 BYOL on CIFAR10_ for different \(\). The baseline (\(=1\), black dashed) uses batch size 1024 and momentum \(_{B}=0.992\), and is scaled from batch size 2048 (left) to 8192 (third) with (blue, \(=_{B}^{}\)) and without (red, \(=_{B}\)) the EMA Scaling Rule. At \(=8\) we also run _progressive scaling_ (right), with transitions at 10 (green) and 30 (orange) epochs. Bands indicate mean and standard deviation across three runs.

Figure 6: _BYOL ViT-B/16 on ImageNet1k_ for different scalings \(\). The baseline model (\(=1\), black dashed) uses batch size 4096 and teacher momentum \(_{B}=0.99\), and is scaled from batch size 8192 (left) to 32768 (right) with progressive scaling and the EMA Scaling Rule (Definition 3.2) (orange, \(=_{B}^{}\)), with the EMA Scaling Rule but without progressive scaling (blue, \(=_{B}^{}\)), without the EMA Scaling Rule but with progressive scaling (purple, \(=_{B}\)), and without either (red, \(=_{B}\)). Progressive scaling transitions from the reference model at epoch 60. See Appendix H.6 for a discussion on BYOL progressive scaling.

At the transition point between batch sizes, an impulse perturbation1 is measured at the student, visible from the training loss. This is recovered from by the learning process, and the new model matches the reference batch size. This perturbation happens in both the AdamW and SGD settings, leading us to suspect this is due to the BYOL learning process, rather than an artifact of optimizer or momentum scaling. However, since this is not directly related to the EMA Scaling Rule proposed in this work, we defer this analysis to future investigation.

## 4 Related work

**Optimizer scaling rules from SDEs** The SDE perspective has uncovered optimizer scaling rules and allowed an understanding of their limitations. Smith and Le (2018) used SDEs to uncover the SGD Scaling Rule, while (Li et al., 2021) used SDEs to explain that rule's breakdown in terms of discretization error. The SDE analysis was extended to adaptive optimization by (Malladi et al., 2022), producing an Adam Scaling Rule (Definition C.3), indicating that along with the learning rate, the \(_{1,2}\) and \(\) parameters transform. The \(_{1,2}\) transformation is consistent with the EMA Scaling Rule in the SDE limit. Our work differs as it considers a model EMA that alters the objective.

**Varying the batch size during training**Smith et al. (2018) investigated the benefits of scheduling the batch size at a fixed learning rate as an alternative to scheduling the learning rate at a fixed batch size. These two are equivalent through the SGD Scaling Rule. The authors _do not_ scale the optimizer hyperparameters during this procedure, as they are intentionally replicating the training dynamics of a learning rate schedule. This is in contrast with _Progressive Scaling_ (Definition 3.2) which scales the hyperparameters to _maintain_ the optimization process at different levels of discretization.

**Large batch training of SSL distillation methods** SSL methods learn representations without labels, meaning they can take advantage of web-scale data. Large batch optimization is required to make use of this data in a reasonable amount of time. Grill et al. (2020) demonstrated algorithmic robustness when _reducing_ the batch size through gradient accumulation and EMA update skipping, which implements an approximation of our EMA Scaling Rule for \(<1\). Our work provides a recipe to scale down _and up_ in \(\). MoCo-v3 (Chen et al., 2021) enables contrastively distilled ViTs up to a batch size of 6144, where the model drops in performance. More recently, methods like DINO (Caron et al., 2020) present a worse scenario, and are unable to scale beyond batch size 1024 (Koppula et al., 2022). In contrast, our work presents practical tools to scale to large batch sizes in the presence of an EMA, enabling practical training of these SSL methods on large scale data.

## 5 Conclusion

We provide an EMA Scaling Rule: when changing the batch size by a factor of \(\), exponentiate the momentum of the EMA update to the power of \(\). This scaling rule should be applied in addition to optimizer scaling rules (for example, linearly scaling the SGD learning rate), and enables the scaling of methods which rely on EMA and are sensitive to the choice of EMA momentum.

We prove the validity of the EMA Scaling Rule by deriving first-order SDE approximations of discrete model optimization when a model EMA is present and can contribute to the model objective. We demonstrate empirical support for a variety of uses of EMA, ordered by increasing influence of the role of EMA on the optimization procedure: supervised model tracking (i.e. Polyak-Ruppert averaging) in speech and vision domains, pseudo-labeling in speech, and self-supervised image representation learning. In almost all scenarios, using the EMA Scaling Rule enables matching of training dynamics under batch size modification, whereas not using it results in significant differences in optimization trajectories. For example, we can scale the BYOL self-supervised method to a batch size of 24,576 without any performance loss _only_ when using the EMA Scaling Rule.

While learning rate scaling rules are relatively commonplace in ML, the role of EMA has been overlooked. With this work, we highlight the importance of scaling the EMA momentum, and hope that future works will use the EMA Scaling Rule to scale the EMA momentum correctly, in the same way that learning rates and other optimizer hyperparameters are scaled.

## 6 Acknowledgements

We thank Miguel Sarabia del Castillo, Adam Golinski, Pau Rodriguez Lopez, Skyler Seto, Amitis Shidani, Barry Theobald, Vimal Thilak, Floris Weers, Luca Zappella, and Shaungfei Zhai for their helpful feedback and critical discussions throughout the process of writing this paper; Okan Akalin, Hassan Babaie, Denise Hui, Mubarak Seyed Ibrahim, Li Li, Cindy Liu, Rajat Dhull, Evan Samanas, Guillaume Seguin, and the wider Apple infrastructure team for assistance with developing and running scalable, fault tolerant code; and Kaifeng Lyu and Abhishek Panigrahi for discussion and details regarding scaling rules for adaptive optimizers. Names are in alphabetical order by last name within group.