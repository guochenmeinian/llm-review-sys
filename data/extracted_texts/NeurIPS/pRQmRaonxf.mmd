# Transformers as Game Players:

Provable In-context Game-playing Capabilities of Pre-trained Models

 Chengshuai Shi

University of Virginia

cs7ync@virginia.edu

&Kun Yang

University of Virginia

ky9tc@virginia.edu

&Jing Yang

The Pennsylvania State University

yangjing@psu.edu

&Cong Shen

University of Virginia

cong@virginia.edu

###### Abstract

The in-context learning (ICL) capability of pre-trained models based on the transformer architecture has received growing interest in recent years. While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting. This work proposes to further explore the in-context learning capabilities of pre-trained transformer models in competitive multi-agent games, i.e., _in-context game-playing_ (ICGP). Focusing on the classical two-player zero-sum games, theoretical guarantees are provided to demonstrate that pre-trained transformers can provably learn to approximate Nash equilibrium in an in-context manner for both decentralized and centralized learning settings. As a key part of the proof, constructional results are established to demonstrate that the transformer architecture is sufficiently rich to realize celebrated multi-agent game-playing algorithms, in particular, decentralized V-learning and centralized VI-ULCB.

## 1 Introduction

Since proposed in Vaswani et al. , the transformer architecture has received significant interest. It has powered many recent breakthroughs in artificial intelligence [11; 12; 21; 23], including the extremely powerful large language models such as GPT  and Llama [55; 56]. One of the most striking observations from the research of these transformer-powered models is that they demonstrate remarkable _in-context learning_ (ICL) capabilities. In particular, after appropriate pre-training, the models can handle new tasks when prompted by a few descriptions or demonstrations without any parameter updates, e.g., Brown et al. , Chowdhery et al. , Liu et al. .

ICL is practically attractive as it provides strong generalization capabilities across different downstream tasks without requiring further training or a large amount of task-specific data. These appealing properties have motivated many empirical studies to better understand ICL [18; 26; 59]; see the survey by Dong et al.  for key findings and results. In addition to the empirical investigations, recent years have witnessed growing efforts in gaining deeper theoretical insights into ICL, e.g., Ahn et al. , Akyurek et al. , Bai et al. , Cheng et al. , Li et al. , Raventos et al. , Wu et al. , Xie et al. , Zhang et al. .

Among these empirical and theoretical studies, one emerging direction focuses on the capability of pre-trained transformer models to perform _in-context reinforcement learning_ (ICRL) [28; 34; 35; 50; 73]. In particular, the transformer is pre-trained with interaction data from diverse environments, modelingthe interaction as a sequential prediction task. During inference, the pre-trained transformer is prompted via the interaction trajectory in the current environment for it to select actions. The work by Lin et al.  provides some theoretical understanding of ICRL, including both a general pre-training guarantee and specific constructions of transformers to realize some well-known designs in multi-armed bandits and RL (especially, LinUCB , Thompson sampling , and UCB-VI ). Wang et al.  further provides understandings on the capability of transformers learning temporal difference (TD) methods  via an in-context fashion. A detailed literature review can be found in Sec. 6.

The insights from Lin et al. , Wang et al.  are largely confined to the single-agent scenario, i.e., a single-agent multi-armed bandit or Markov decision process (MDP). The power of RL, however, extends to the much broader multi-agent scenario, especially the multi-player competitive games such as GO , Starcraft , and Dota 2 . To provide a more comprehensive understanding of ICRL, this work targets further studying the _in-context game-playing_ (ICGP) capabilities of transformers in multi-agent competitive settings. To the best of our knowledge, _this is the first work providing theoretical analyses and empirical pieces of evidence on the ICGP capabilities of transformers._ The contributions of this work are further summarized as follows.

\(\) A general framework is proposed to model in-context game-playing via transformers, where we focus on the representative two-player zero-sum Markov games and target learning Nash equilibrium (NE). Compared with the single-agent scenario , the multi-agent setting considered in this work broadens the ICRL research scope while it is also more complicated due to its game-theoretic nature.

\(\) The challenging decentralized learning setting is first studied, where two distinct transformers are trained to learn NE, one for each player, without observing the opponent's actions. A general realizability-conditioned guarantee is first derived that characterizes the generalization error of the pre-trained transformers. Then, the capability of the transformer architecture is demonstrated by providing a concrete construction so that the famous V-learning algorithm  can be exactly realized. Lastly, a finite-sample upper bound on the approximation error of NE is proved to establish the ICGP capability of transformers. As a further implication, the result of realizing V-learning demonstrates the capability of pre-trained transformers to perform model-free RL designs, in addition to the model-based ones (e.g., UCB-VI  as studied in Lin et al. ).

\(\) To obtain a complete understanding, the centralized learning setting is also investigated, where one transformer is pre-trained to control both players' actions. A similar set of results is provided: a general pre-training guarantee, a constructional result to demonstrate realizability, and a finite-sample upper bound on the approximation error of NE. Distinctly, the transformer construction is presented as a specific parameterization to implement the renowned centralized VI-ULCB algorithm .

\(\) Furthermore, experiments are also performed to practically test the ICGP capabilities of the pre-trained transformers. The obtained results not only corroborate the derived theoretical claims, but also empirically motivate this and further studies on the interesting direction of pre-trained models in game-theoretic settings.

## 2 A Theoretical Framework for In-Context Game Playing

### The Basic Setup of Environments

To demonstrate the ICGP capability of transformers, we focus on one of the most basic game-theoretic settings: two-player zero-sum Markov games , while the discussions provided later conceivably extend to more general games. An illustration of the different settings (i.e., decentralized and centralized) considered in this work (with details explained later) is given in Fig. 1. The overall framework is introduced in the following, which extends Lin et al.  from the single-agent decision-making setting to the competitive multi-agent domain.

Considering a set of two-player zero-sum Markov games denoted as \(\). Each environment \(M\) shares the number of episodes \(G\), the number of steps \(H\) in each episode, the state space \(\) (with \(||=S\)), the action spaces \(\{,\}\) (with \(||=A\) and \(||=B\)), and the reward space \(\). Here \(\) and \(\) denote the action spaces of two players, respectively, which are referred to as the max-player and the min-player for convenience.

Each environment \(M=\{_{M}^{h-1},_{M}^{h}:h[H]\}\) has its transition model \(_{M}^{h}:( )\) and reward functions \(_{M}^{h}:()\), where \(_{M}^{0}()\) denotes the initial state distribution. Particularly, overall \(G\) episodes of \(H\) steps happen in each environment \(M\), with each episode starting at \(s^{g,1}_{M}^{0}()\). If the action pair \((a^{g,h},b^{g,h})\) is taken upon state \(s^{g,h}\) at step \(h\) in episode \(g\), the state is trained to \(s^{g,h+1}_{M}^{h}(|s^{g,h},a^{g,h},b^{g,h})\), and reward \(r^{g,h}_{M}^{h}(s^{g,h},a^{g,h},b^{g,h})\) (respectively, \(-r^{g,h}\)) is collected by the max-player (respectively, the min-player). For simplicity, we assume that the max-player rewards are bounded in \(\) and deterministic, i.e., for each \((s,a,b,h)\), there exists \(r\) such that \(_{M}^{g,h}(r|s,a,b)=1\). Also, the initial state \(s^{g,1}\) is assumed to be a fixed one \(s^{1}\), i.e., \(_{M}^{0}(s^{1})=1\).

We further leverage the notation \(T:=GH\), while using time \(t\) and episode-step pair \((g,h)\) in an interleaving manner with \(t:=(g-1)H+h\). The partial interaction trajectory up to time \(t\) is then denoted as \(D^{t}:=\{(s^{r},a^{r},b^{r},r^{r}):[t]\}\) and we use the abbreviated notation \(D:=D^{T}\). Individually, for the max-player, we denote her observed interaction trajectory up to time \(t\) by \(D_{+}^{t}:=\{(s^{r},a^{r},r^{r}):[t]\}\) and write \(D_{+}:=D_{+}^{T}\) for short. Similarly, for the min-player, we denote \(D_{-}^{t}:=\{(s^{r},b^{r},r^{r}):[t]\}\) and \(D_{-}:=D_{-}^{T}\).

### Game-playing Algorithms and Nash Equilibrium

A game-playing algorithm Alg can map a partial trajectory \(D^{t-1}\) and state \(s^{t}\) to a distribution over the actions, i.e., \((,|D^{t-1},s^{t})()\). If one algorithm Alg is decoupled for the two players (as in the later decentralized setting), we denote it as \(=(_{+},_{-})\), where \(_{+}(|D_{+}^{t-1},s^{t})()\) and \(_{-}(|D_{-}^{t-1},s^{t})()\). Given an environment \(M\) and an algorithm Alg, the distribution over a full trajectory \(D\) can be expressed as

\[_{M}^{}(D)=_{t[T]}_{M}^{t-1}( s^{t}|s^{t-1},a^{t-1},b^{t-1})(a^{t},b^{t}|D^{t-1},s^{t}) _{M}^{t}(r^{t}).\]

If further considering an environment prior distribution \(()\) such that \(M\), the joint distribution of \((M,D)\) is denoted as \(_{}^{}(D)\), where \(M()\) and \(D_{M}^{}()\).

For environment \(M\) and a game-playing algorithm \(\), we define its value function over one episode as \(V_{M}^{}(s^{1})=_{D^{H}_{M}^{}}[_{t[H]}r^{ t}]\). With the marginalized policies of \(\) denoted as \((,)\), we define their best responses as

\[_{}():=_{^{}}V_{M}^{,^{}}( s^{1}),\ \ _{}():=_{^{}}V_{M}^{^{},}(s^{1}),\]

whose corresponding values are

\[V_{M}^{,}(s^{1}):=V_{M}^{,_{}()}(s^{1}),\ \ V_{M}^{,}(s^{1}):=V_{M}^{_{ }(),}(s^{1}).\]

With the notion of best responses, the following classical definition of approximate Nash equilibrium (NE) can be introduced .

**Definition 2.1** (Approximate Nash equilibrium).: _A decoupled policy pair \((,)\) is an \(\)-approximate Nash equilibrium for environment \(M\) if \(V_{M}^{,}(s^{1})+ V_{M}^{,}(s ^{1}) V_{M}^{,}(s^{1})-\), i.e., the Nash equilibrium gap \(V_{M}^{,}(s^{1})-V_{M}^{,}(s^{1}) 2\)._

Figure 1: An overall view of the framework, where the in-context game-playing (ICGP) capabilities of transformers are studied in both decentralized and centralized learning settings. The orange arrows denote the supervised pre-training procedure and the blue arrows mark the inference procedure.

For each environment, our learning goal is to approximate its NE policy pair. In other words, we target outputting a policy pair that is -approximate NE with an error that is as small as possible, after interacting with the environment for an overall - rounds.

### The Transformer Architecture

With the basics of the game-playing environment and the learning target established, we now introduce the transformer architecture , which has demonstrated great potential in processing sequential inputs. First, for an input vector, we denote as the entry-wise ReLU activation function and as the softmax activation function, while using to refer to a non-specified activation function (i.e., both ReLU and softmax may be used). Then, the masked attention layer and the MLP layer can be defined as follows.

**Definition 2.2** (Masked Attention Layer).: _A masked attention layer with - heads is denoted as. On any input sequence, we have, where_

**Definition 2.3** (MLP Layer).: _An MLP layer with hidden dimension is denoted as,. On any input sequence, we have, where_

The combination of masked attention layers and MLP layers leads to the overall decoder-based transformer architecture studied in this work, as defined in the following.

**Definition 2.4** (Decoder-based Transformer).: _An L-layer decoder-based transformer, denoted as, is a composition of L masked attention layers, each followed by an MLP layer and a clip operation:, where is defined iteratively by taking and for,_

_where parameter consists of, and._

We further define the parameter class of transformers as, where the norm of a transformer is denoted as

**Other Notations.** The total variation distance between two algorithms upon is denoted as, also, the notation indicates that is a lower or equivalent order term compared with, i.e.,, hides poly-logarithmic terms in, and compactly denotes a polynomial term with respect to the input.

## 3 Decentralized Learning

First, we study the decentralized learning setting, i.e., each player takes actions following her own model independently without observing the opponent's actions, as it better captures the unique game-playing scenario considering in this work. This setting is aligned with the canonical study of normal-form games [20; 53], and has been extended to Markov games in recent years [32; 41; 51]. In the following, we start by introducing the basic setup of supervised pre-training and provide a general performance guarantee relying on a realizability assumption. Then, we provide a constructional result to demonstrate that the algorithms induced by transformers are rich enough to realize the celebrated V-learning algorithm . With these results, we finally establish that with V-learning providing training data, the pre-trained transformer can effectively approximate NE when interacting with different environments in an in-context fashion.

### Supervised Pre-training Results

#### 3.1.1 Basic Setups

**Training Dataset.** In the supervised pre-training, we use a context algorithm \(_{0}\) to collect the offline trajectories. For the decentralized setting, the context algorithm \(_{0}\) used for data collection is assumed to be consisted of two decoupled algorithms \((_{+,0},_{-,0})\) for the max- and min-players, respectively. With the context algorithm, we consider \(N\) i.i.d. offline trajectories \(\{_{i}:i[N]\}\) are collected, where \(_{i}:=D_{i} D^{}_{i}\) with

\[D_{i}:=\{(s^{t}_{i},a^{t}_{i},b^{t}_{i},r^{t}_{i}):t[T]\} _{}^{_{0}}();\] \[D^{}_{i}:=\{(a^{t}_{i,s},b^{t}_{i,s})_{0}( ,|D^{t-1}_{i},s):t[T],s\}.\]

It can be observed that \(D_{i}\) is the commonly considered offline interaction trajectory of \(_{0}\), while \(D^{}_{i}\) is the sampled actions of each state \(s\) at each step \(t\) with \(_{0}\). Compared with Lin et al. , \(D^{}_{i}\) is an augmented component. We first note that collecting \(D^{}_{i}\) is relatively easy in practical applications, as it only needs to additionally sample from the distribution \(_{0}(,|D^{t-1}_{i},s)\) for each \(s\) (i.e., no additional interactions with the environment). Moreover, the reason to incorporate such an augmentation is to provide additional diverse pre-training data due to the unique game-theoretic environment, with further discussions provided after the later Lemma 3.6. It has also been recognized previously [16; 72] that the data requirement for learning Markov games is typically much stronger than that for single-agent RL.

To facilitate the decentralized training, the overall dataset is further split into two parts: \(\{_{+,i}:i[N]\}\) and \(\{_{-,i}:i[N]\}\), where

\[_{+,i}:=D_{+,i} D^{}_{+,i},\;\;D_{+,i}:=\{(s ^{t}_{i},a^{t}_{i},r^{t}_{i}):t[T]\},\;\;D^{}_{+,i}:=\{a^{t}_{i,s}:t [T],s\};\] \[_{-,i}:=D_{-,i} D^{}_{-,i},\;\;D_{-,i}:=\{( s^{t}_{i},b^{t}_{i},r^{t}_{i}):t[T]\},\;\;D^{}_{-,i}:=\{b^{t}_{i,s}:t [T],s\}.\]

In other words, \(_{i,+}\) denotes the observations of the max-player, while \(_{i,-}\) those of the min-layer. Note that neither player can observe the opponent's actions.

**Algorithm Induced by Transformers.** Due to the decentralized nature, two embedding mappings of \(d_{+}\) and \(d_{-}\) dimensions are considered as \(_{+}:()^{d_ {+}}\) and \(_{-}:()^{d_ {-}}\), together with two transformers \(_{_{+}}\) and \(_{_{-}}\). Taking the max-player's transformer as representative, for trajectory \((D^{t-1}_{+},s^{t})\), let \(_{+}=_{+}(D^{t-1}_{+},s^{t})=[_{+}(s^{1}), _{+}(a^{1},r^{1}),,_{+}(s^{t})]\) be the input to \(_{_{+}}\), and the obtained output is \(}_{+}=_{_{+}}( {H}_{+})=[}_{+,1},}_{+,2}, ,}_{+,-2},}_{+,-1}]\), which has the same shape as \(_{+}\). Similarly, the mapping \(_{-}\) is used for the min-player's transformer \(_{_{-}}\) to embed trajectory \((D^{t-1}_{-},s^{t})\).

We further assume that two fixed linear extraction mappings, \(^{A d_{+}}\) and \(^{B d_{-}}\), are used to induce algorithms \(_{_{+}}\) and \(_{_{-}}\) over the action spaces \(\) and \(\) of the max- and min-players, respectively, as

\[_{_{+}}(|D^{t-1}_{+},s^{t}) =_{}(_{_{+}}(_{+}(D^{t-1}_{+},s^{t}))_{-1}),\] (1) \[_{_{-}}(|D^{t-1}_{-},s^{t}) =_{}(_{_{-}}(_{-}(D^{t-1}_{-},s^{t}))_{-1}),\]

where \(_{}\) denotes the projection to a probability simplex.

**Training Scheme.** We consider the standard supervised pre-training to maximize the log-likelihood of observing training datasets \(_{+}\) (resp., \(_{-}\)) over algorithms \(\{_{_{+}}:_{+}_{+}\}\) (resp., \(\{_{_{-}}:_{-}_{-}\}\)) with \(_{+}:=_{d_{+},L_{+},M_{+},d^{}_{+},F_{+}}\) (resp., \(_{-}:=_{d_{-},L_{-},M_{-},d^{}_{-},F_{-}}\)). In particular, the pre-training outputs \(}_{+}\) and \(}_{-}\) are determined as

\[}_{+}=_{_{+}_{+ }}_{i[N]}_{t[T]}_{s }(_{_{+}}(a^{t}_{i,s}|D^{t -1}_{+,i},s));\]\[}_{-}=_{_{-}_{-}} _{i[N]}_{t[T]}_{s}(_{_{-}}(b_{i,s}^{t}|D_{-,i}^{t-1},s)).\]

#### 3.1.2 Theoretical Guarantees

In this section, we provide a generalization guarantee of the algorithms \(_{}_{+}}\) and \(_{}_{-}}\) pre-trained following the scheme introduced above. First, the standard definition regarding the covering number and an assumption of approximate realizability are introduced to facilitate the analysis, which are also leveraged in Lin et al. .

**Definition 3.1** (Decentralized Covering Number).: _For a class of algorithms \(\{_{_{+}}:_{+}_{+}\}\), we say \(_{+}_{+}\) is a \(_{+}\)-cover of \(_{+}\), if \(_{+}\) is a finite set such that for any \(_{+}_{+}\), there exists \(}_{+}_{+}\) such that for all \(D_{+}^{t-1},s,t[T]\), it holds that_

\[\|_{}_{+}}(|D_{+}^{t-1},s)- _{_{+}}(|D_{+}^{t-1},s)\|_{}_ {+}.\]

_The covering number \(_{_{+}}(_{+})\) is the minimal cardinality of \(_{+}\) such that \(_{+}\) is a \(_{+}\)-cover of \(_{+}\). Similarly, we can define the \(_{-}\)-cover of \(_{-}\) and the covering number \(_{_{-}}(_{-})\)._

**Assumption 3.2** (Decentralized Approximate Realizability).: _There exist \(_{+}^{}_{+}\) and \(_{+,}>0\) such that for all \(t[T],s,a\), it holds that_

\[(_{D_{}^{_{0}}}[ _{+,0}(a|D_{+}^{t-1},s)}{_{_{+}^{ }}(a|D_{+}^{t-1},s)}])_{+,}.\]

_We also similarly assume \(_{-,}\)-approximate realizability of \(_{-,0}\) via \(_{_{-}^{}}\) with \(_{-}^{}_{-}\)._

Then, we can establish the following generalization guarantee on the TV distance between \((_{}_{+}},_{} _{-}})\) and \(_{0}=(_{0,+},_{0,-})\), capturing their similarities.

**Theorem 3.3** (Decentralized Pre-training Guarantee).: _Let \(}_{+}\) be the max-player's pre-training output defined in Sec. 3.1.1. Take \(_{_{+}}=_{_{+}}(1/N)\) as in Def. 3.1. Then, under Assumption 3.2, with probability at least \(1-\), it holds that1_

\[_{D_{}^{_{0}}}[ _{t[T],s}(_{+,0},_{ }_{+}}|D_{+}^{t-1},s)] TS}}+TS_{_{+}} TS/)}{N}}.\]

_A similar result holds for the min-players' pre-training output \(}_{-}\)._

Theorem 3.3 demonstrates that in expectation of the pre-training data distribution, i.e., \(_{}^{_{0}}(D)\), the TV distance between the pre-trained algorithm \(_{}_{+}}\) (resp, \(_{}_{-}}\)) and the context algorithm \(_{+,0}\) (resp, \(_{-0}\)) can be bounded via two terms: one from the approximate realizability, i.e., \(_{+,}\) (resp, \(_{-,}\)), and the other from the limited amount of pre-training trajectories, i.e., finite \(N\). While we can diminish the second term via a large pre-training dataset (i.e., sufficient pre-training games), the key question is whether the transformer structure is sufficiently expressive to realize the context algorithm, i.e., having a small \(_{+,}\), which we affirmatively answer via an example of realizing V-learning  in the next subsection.

### Realizing V-learning

To demonstrate the capability of transformers in the decentralized game-playing setting, we choose to prove that they can realize the renowned V-learning algorithm , the first design that breaks the curse of multiple agents in learning Markov games. Particularly, V-learning leverages techniques from adversarial bandits  to perform policy updates without observing the opponent's actions. The details of V-learning are provided in Appendix G.1, where its unique output rule is also elaborated.

In the following theorem, we demonstrate that a transformer can be constructed to exactly perform V-learning with a suitable parameterization. One additional Assumption G.2 on the existence of a transformer parameterized by the class of \(_{d,L_{D},M_{D},d_{D},F_{D}}\) to perform exact division is adopted for the convenience of the proof, while in Appendix G.2, we further demonstrate that the required division operation can be approximated to any arbitrary precision.

**Theorem 3.4**.: _With embedding mapping \(_{+}\) and extraction mapping \(\) defined in Appendix G.3, under Assumption G.2, there exists a transformer \(_{_{+}}\) with_

\[d HSA,\ \ L GHL_{D},\ \ _{l[L]}M^{(l)}  HS^{2}+HSA+M_{D},\] \[d^{}  G+A+d_{D},\ \ \|\| GH^{2}S+G^{3}+F_{D},\]

_which satisfies that for all \(D_{+}^{t-1},s,t[T]\), \(}_{_{+}}(|D_{+}^{t-1},s)= }_{}(|D_{+}^{t-1},s)\). A similar construction \(_{_{-}}\) exists for the min-player's transformer such that for all \(D_{-}^{t-1},s,t[T]\), \(}_{_{-}}(|D_{-}^{t-1},s)= }_{}(|D_{-}^{t-1},s)\)._

The proof of Theorem 3.4 (presented in Appendix G.3) is challenging because V-learning is a model-free design, while UCB-VI  studied in Lin et al.  and VI-ULCB  later presented in Sec. 4.2 are both model-based ones. We believe this result deepens our understanding of the capability of pre-trained transformers in decision-making, i.e., they can realize both model-based and model-free designs, showcasing their further potentials.

More specifically, with the embedded trajectory as the input, the model-based philosophy is natural for the masked attention mechanism, i.e., the value computation at each step is directly over all raw inputs in previous steps. Thus, the construction procedure is straightforward as (input\({}_{1}\), input\({}_{2}\),..., input\({}_{t}\)) \(\) (value\({}_{1}\), value\({}_{2}\),..., value\({}_{t}\)). However, the model-free designs are different, where value computation at one step requires previous values (instead of raw inputs). In other words, the construction procedure is a recursive one as (input\({}_{1}\), input\({}_{2}\),..., input\({}_{t}\)) \(\) (value\({}_{1}\), input\({}_{2}\),..., input\({}_{t}\)) \(\) (value\({}_{1}\), value\({}_{2}\),..., input\({}_{t}\)) \(\)... \(\) (value\({}_{1}\), value\({}_{2}\),..., value\({}_{t}\)), whose realization requires carefully crafted constructions.

### The Overall ICGP Capablity

Finally, built upon the obtained results, the following theorem demonstrates the ICGP capability of pre-trained transformers in the decentralized setting.

**Theorem 3.5**.: _Let \(_{+}\) and \(_{-}\) be the classes of transformers satisfying the requirements in Theorem 3.4 and \((}_{+,0},}_{-,0})\) both be V-learning. Let \((,)\) be the output policies via the output rule of V-learning. Denoting \(}_{}}=(}_{}_{+}},}_{ }_{-}})\) and \(_{}=_{_{+}}_{_{-}}\). Then, with probability at least \(1-\), it holds that_

\[_{D_{}^{}_{}} }[V_{M}^{,}(s^{1})-V_{M}^{,}(s^{1}) ]S(A B)(SABT)}{G}}+THS_{}/)}{N}}.\]

With the obtained upper bound on the approximation error of NE, Theorem 3.5 demonstrates the ICGP capability of pre-trained transformers as the algorithms \(}_{}_{+}}\) and \(}_{}_{-}}\) are fixed during interactions with varying inference games (i.e., no parameter updates). When prompted by the interaction trajectory in the current game, they are capable of deciding the future interaction strategy and finally provide policy pairs that are approximate NE. We further note that during both pre-training and inference, each player's transformer takes inputs of its own observed trajectories, but not the opponent's actions, which reflects the decentralized requirement. Moreover, the approximation error in Theorem 3.5 depends on \(A B\) instead of \(AB\) as in the later Theorem 4.2, evidencing the benefits of decentralized learning.

**Proof Sketch.** The proof of Theorem 3.5 (presented in Appendix H) rely on the following, decomposition, where \(_{0}[]\) and \(_{}}[]\) are with respect to \(_{}^{}_{0}}\) and \(_{}^{}_{}}}\), respectively:

\[_{}}[V_{M}^{, {}}(s^{1})-V_{M}^{,}(s^{1})] =_{0}[V_{M}^{,}(s^{1})-V_{M}^{ ,}(s^{1})]\] (2) \[+_{}}[V_{M}^{, {}}(s^{1})]-_{0}[V_{M}^{,}(s^{1}) ]+_{0}[V_{M}^{,}(s^{1})]-_{ }}[V_{M}^{,}(s^{1})],\]

It can be observed that the first decomposed term is the performance of the considered V-learning, which can be obtained following Jin et al.  as in Theorem G.1.

Then, the remaining terms concern the performance of the policy pair \((,)\) learned from \(_{0}\) and \(_{}}\)_against their own best responses_, respectively. This is drastically different from the consideration in Lin et al. , which only bounds the performance of the learned policies, i.e.,

\[_{0}[V_{M}^{,}(s^{1})]-_{ }}[V_{M}^{,}(s^{1})].\]

The involvement of best responses complicates the analysis. After careful treatments in Appendix H, we obtain the following lemma to characterize these terms.

**Lemma 3.6**.: _For any two decentralized algorithms \(_{}\) and \(_{}\), we denote their performed policies for episode \(g\) are \((_{}^{g},_{}^{g})\) and \((_{}^{g},_{}^{g})\), and their final output policies via the output rule of V-learning (see Appendix G.1) are \((_{},_{})\) and \((_{},_{})\). For \(\{_{},_{}\}\), it holds that_

\[_{}[V_{M}^{_{},}(s^{1 })]-_{}[V_{M}^{_{},}(s^{1})] H_{t[T],s}_{} [(_{}^{t},_{}^{t}|D_{+}^{t-1},s)]\] \[+ H_{t[T],s}_{ }[(_{}^{t},_{}^{t}|D_{-}^{t-1},s )],\]

_where \(_{}[]\) and \(_{}[]\) are with respect to \(_{}^{_{}}\) and \(_{}^{_{}}\). A similar result holds for \(\{_{},_{}\}\)._

With Lemma 3.6, we can incorporate Theorem 3.3 to upper bound the TV distance between \(_{0}\) and \(_{}}\), which together with Theorem 3.4 establish \(_{,+}=_{,-}=0\) in this case, leading to the desired performance guarantee in Theorem 3.5. We here further note that the effectiveness of Theorem 3.3 in capturing the bound in Lemma 3.6 over all \(s\) credits to the augmented dataset \(D^{}\), which provides diverse data of all \(s\).

## 4 Centralized Learning

In this section, we discuss the scenario of centralized learning, i.e., training one joint model to control both players' interactions. This is also known as the self-play setting [8; 9; 33; 40; 67]. Following a similar procedure as the decentralized discussions, we first provide supervised pre-training guarantees and then demonstrate that transformers are capable of realizing the renowned VI-ULCB algorithm . It is thus established that in a centralized learning setting, the pre-trained transformer can still effectively perform ICGP and approximate NE.

### Supervised Pre-training Results

The same training dataset \(\{_{i}:i[N]\}\) as in Section 3.1.1 is considered. As the centralized setting is studied here, no further split of the dataset is needed. Moreover, one \(d\)-dimensional mapping \(:() ^{d}\) can be designed to embed the trajectories, and the induced algorithm \(_{}(,|D^{t},s^{t})\) from the transformer \(_{}\) can be obtained via a fixed linear extraction mapping \(\) similarly as Eqn. (1). Finally, the MLE training is performed with \(:=_{d,L,M,d^{},F}\) as \(}=_{} {N}_{i[N]}_{t[T]}_{s} _{}(a_{i,s}^{t},b_{i,s}^{t}|D_{i}^{t-1},s)\).

Then, a generalization guarantee of \(_{}}\) can be provided similarly as Theorem 3.3, which is deferred to Theorem C.3. This centralized result also implies that the pre-trained centralized algorithm performs similarly as the context algorithm, with errors caused by the approximate realizability and the finite pre-training data.

### Realizing Vi-ULCB

The VI-ULCB algorithm  is one of the first provably efficient centralized learning designs for Markov games. It extends the key idea of using confidence bounds to incorporate uncertainties from stochastic bandits and MDPs [4; 6] to handle competitive environments, and has further inspired many extensions in Markov games [9; 30; 33; 40; 65]. As VI-ULCB is highly representative, we choose it as the example for realization in the centralized setting to demonstrate the capability of transformers.

To make VI-ULCB practically implementable, we adopt an approximate CCE solver powered by multiplicative weight update (MWU) in the place of its originally required general-sum NE solver (which is computationally demanding). This modification is demonstrated as provably efficient in later works [9; 40; 65]. Then, the following result illustrates that a transformer can be constructed to exactly perform the MWU-version of VI-ULCB.

**Theorem 4.1**.: _With embedding mapping \(\) and extraction mapping \(\) defined in Appendix D.2, there exists a transformer \(_{}\) with_

\[d HS^{2}AB, L GHS,_{l[L]}M^{(l )} HS^{2}AB,\] \[d^{} G^{2}HS^{2}AB,\| \| HS^{2}AB+G^{3}+GH,\]

_which satisfies that for all \(D^{t-1},s,t[T]\), \(_{}(,|D^{t-1},s)=_{ }(,|D^{t-1},s)\)._

One observation from the proof of Theorem 4.1 (presented in Appendix D.2) is that transformer layers can perform MWU so that an approximate CCE can be found, which is not reported in Lin et al.  and further demonstrates the in-context learning capability of transformers in playing normal-form games (since MWU is one of the most basic designs).

### The Overall ICGP Capability

With Theorem 4.1 showing VI-ULCB can be exactly realized (i.e., \(_{}=0\) in Assumption C.2), we can further prove an overall upper bound of the approximation error of NE by \(_{}}\) via the following theorem, demonstrating the ICGP capability of transformers.

**Theorem 4.2**.: _Let \(\) be the class of transformers satisfying the requirements in Theorem 4.1 and \(_{}}\) be VI-ULCB. For all \((g,h,s)[G][H]\), let \((^{g,h}(|s),^{g,h}(|s))\) be the marginalized policies of \(_{}}(,|D^{t-1},s)\). Then, with probability at least \(1-\), it holds that_

\[_{_{_{_{_{_{_{ _{_{_{_{_{}}}}}}}}}}} [V_{M}^{,}(s^{1})-V_{M}^{,}(s^{1}) ]S^{2}AB(SABT)}{G}}+THS_{})/)}{N}},\]

_where \(\) and \(\) are uniformly sampled as \(\{^{1},,^{G}\}\) and \(\{^{1},,^{G}\}\), with \(^{g}:=\{^{g,h}(|s):(h,s)[H]\}\) and \(^{g}:=\{^{g,h}(|s):(h,s)[H]\}\), and \(_{,}\) is with respect to the process of policy sampling._

This result demonstrates the ICGP capability of pre-trained transformers in the centralized setting, complementing the discussions in the decentralized results.

## 5 Empirical Experiments

Experiments are performed on two-player zero-sum normal-form games (\(H=1\)) and Markov games (\(H=2\)), with the decentralized EXP3  (which can be viewed as a one-step V-learning) and the centralized VI-ULCB being the context algorithms as demonstrations, respectively. Additional experimental setups and details can be found in Appendix J. It can be first observed from Fig. 2 that, the transformers pre-trained with \(N=20\) games performs better on the inference tasks than the ones pre-trained with \(N=10\) games. This observation empirically validates the theoretical result that more pre-training games benefit the final game-playing performance during inference (i.e., the \(\)-dependencies established in Theorems 3.5 and 4.2). Moreover, when the number of pre-training games is sufficient (i.e., \(N=20\) in Fig. 2), the obtained transformers can indeed learn to approximate NE in an in-context manner (i.e., having a gradually decaying NE gap), and also the obtained performance is similar to the context algorithm, i.e., EXP3 or VI-ULCB. These observations provide empirical pieces of evidence to support the ICGP capabilities of pre-trained transformers, motivating and validating the theoretical analyses performed in this work.

## 6 Related Works

**In-context Learning.** Since GPT-3  demonstrates the ICL capability of pre-trained transformers, growing attention has been paid to this direction. In particular, an emerging line of work targets providing a deeper understanding of the fundamental mechanism behind the ICL capability [3; 24; 29; 31; 37; 44; 59; 60; 64; 66; 68], where many interesting results have been obtained. In particular, transformers have been shown to be capable of performing in-context gradient descent so that varying optimization-based algorithms can be realized [2; 3; 7; 26; 59]. Also, Giannou et al.  demonstrates that looped transformers can emulate basic computing blocks, whose combinations can lead to complex operations.

This work is more focused on the in-context reinforcement learning (ICRL) capability of pre-trained transformers, as demonstrated in Grigsby et al. , Laskin et al. , Lee et al. , Wang et al. . The recent work by Lin et al.  initiates the theoretical investigation of this topic. In particular, Lin et al.  provides generalization guarantees after pre-training in the single-agent RL scenario, and further constructs transformers to realize provably efficient single-agent bandits and RL algorithms (in particular, LinUCB , Thompson sampling , UCB-VI ). This work extends Lin et al.  to the domain of competitive multi-agent RL by studying the in-context game-playing setting. A recent concurrent work  also touches upon the in-context game-playing capability of pre-trained transformers, while focusing on practical aspects and exploiting different opponents.

**Competitive Multi-agent RL.** The study of RL in the competitive multi-agent domain has a long and fruitful history [10; 47; 49; 58; 70]. In recent years, researchers have gained a deeper theoretical understanding of this topic. The centralized setting (also known as self-play) has been investigated in Bai and Jin , Bai et al. , Cui et al. , Huang et al. , Jin et al. , Liu et al. , Wang et al. , Xiong et al. , Zhang et al. , and this work focuses on the representative VI-ULCB design . On the other hand, decentralized learning is more challenging, and the major breakthrough is made by V-learning [9; 32; 41; 51], which is thus adopted as the target algorithm in this work.

## 7 Conclusions

This work investigated the in-context game-playing (ICGP) capabilities of pre-trained transformers, broadening the research scope of in-context RL from the single-agent scenario to the more challenging multi-agent competitive games. Focusing on the classical two-player zero-sum Markov games, a general learning framework was first introduced, laying down a solid ground for this and later studies. Through concrete theoretical results, this work further demonstrated that in both decentralized and centralized learning settings, properly pre-trained transformers are capable of approximating Nash equilibrium in an in-context manner. As a key part of the proof, concrete sets of parameterization were provided to demonstrate that the transformer architecture can realize two famous designs, decentralized V-learning and centralized VI-ULCB. Empirical experiments further validate the theoretical results (especially that pre-trained transformers can indeed approximate NE in an in-context manner) and motivate future studies on this under-explored research direction.

Figure 2: Comparisons of Nash equilibrium (NE) gaps over episodes in both decentralized and centralized learning scenarios, averaged over \(10\) inference games.