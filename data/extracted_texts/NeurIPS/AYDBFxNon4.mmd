# Linking In-context Learning in Transformers to

Human Episodic Memory

 Li Ji-An

Neurosciences Graduate Program

University of California, San Diego

jil095@ucsd.edu

&Corey Y Zhou

Department of Cognitive Science

University of California, San Diego

yiz329@ucsd.edu

&Marcus K. Benna

Department of Neurobiology

University of California, San Diego

mbenna@ucsd.edu

Equal contribution.

Corey Y Zhou

Department of Cognitive Science

University of California, San Diego

yiz329@ucsd.edu

Equal contribution.

Department of Cognitive Science

University of California, San Diego

ymcecho@ucsd.edu

&Marcelo G. Mattar

Department of Psychology

New York University

marcelo.mattar@nyu.edu

###### Abstract

Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.

## 1 Introduction

Neural networks often bear striking similarities to biological intelligence. For instance, convolutional networks trained on computer vision tasks can predict neuronal activities in the visual cortex [1; 2; 3; 4]. Recurrent neural networks trained on spatial navigation develop neural representations similar to the entorhinal cortex and hippocampus [5; 6], and those trained on reward-based tasks recapitulate biological decision-making behaviors . Feedforward networks trained on category learning exhibit human-like attentional bias . Identifying commonalities between artificial and biological intelligence offers unique insights into both model properties and the brain's cognitive functions.

In contrast to this long tradition of drawing parallels between AI models and biology, exploration of the biological relevance of the Transformer architecture -- originally proposed for natural language translation  -- remains limited, with many researchers in neuroscience, cognitive science, and deep learning viewing it as being fundamentally different from the brain. So far, Transformers have been linked to generalized Hopfield networks , neural activities in the language cortex [11; 12; 13],and hippocampus-cortical circuit representations . However, it remains unclear whether and how the _emergent_ behavior and mechanisms of _interacting_ attention heads relate to biological cognition.

This study bridges this gap by examining the parallels between attention heads in Transformer models and episodic memory in biological cognition. We focus on "induction heads", a particular type of attention head in Transformer models and a crucial component of _in-context learning_ (ICL) observed in LLMs . ICL enables LLMs to perform new tasks on the fly during test time, relying solely on the context provided in the input prompt, without the need for additional fine-tuning or task-specific training [16; 17]. We show that induction heads share several parallel properties with the contextual maintenance and retrieval (CMR) model, an influential model of human episodic memory during free recall. Understanding the mechanisms of ICL is important for developing better models capable of performing unseen tasks, as well as for AI safety research, as the models could be instructed to perform malicious activities after being deployed in real-world scenarios.

In sections below, we introduce the task in Section 2, Transformer and induction heads in Sections 3.1, 3.2, and the CMR model in Section 4.1. We show that induction heads and CMR are _mechanistically_ similar in Sections 3.3 and 4.2 and _behaviorally_ similar in Section 5.1. We further characterize the emergence of CMR-like behavior in Section 5.2 and its possible causal role in Section 5.3. Overall, our study provides evidence for a novel bridge between Transformer models and episodic memory.

## 2 Next-token prediction and memory recall

Transformer models in language modeling are often trained to predict the next token . ICL thus helps next-token prediction using information provided solely in the input prompt context. One way to evaluate a model's ICL is to run it on a sequence of repeated random tokens  (Fig. 1a). For example, consider the prompt "[A][B][C][D][A][B][C][D]". Assuming that no structure between these tokens has been learned, the first occurrence of each token cannot be predicted -- e.g., the first [C] cannot be predicted to follow the first [B]. At the second [B], however, a model with ICL should predict [C] to follow by retrieving the temporal association in the first part of the context.

Much like ICL in a Transformer model, human cognition is also known to perform associative retrieval when recalling episodic memories. Episodic retrieval is commonly studied using the free recall paradigm [18; 19] (Fig. 1b). In free recall, participants first study a list of words sequentially, and are then asked to freely recall the studied words in any order . Despite no requirements on recall order, humans often exhibit patterns of recall that reflect the temporal structure of the preceding study list. In particular, the retrieval of one word triggers the subsequent recall of other words studied close in time (temporal contiguity). Additionally, words studied _after_ the previously recalled word are

Figure 1: **Tasks and model architectures.****(a)** Next-token prediction task. The ICL of pre-trained LLMs is evaluated on a sequence of repeated random tokens (“...[A][B][C][D]...[A][B][C][D]...”; e.g., [A]=light, [B]=cat, [C]=table, [D]=water) by predicting the next token (e.g., “...[A][B][C][D]...[B]’\(\)’). **(b)** Human memory recall task. During the study phase, the subject is sequentially presented with a list of words to memorize. During the recall phase, the subject is required to recall the studied words in any order. **(c)** Transformer architecture, centering on the residual stream. The blue path is the residual stream of the current token, and the grey path represents the residual stream of a past token. \(H_{1}\) and \(H_{2}\) are attention heads. MLP is the multilayer perceptron. **(d)** Contextual maintenance and retrieval model. The word vector \(\) is retrieved from the context vector \(\) via \(^{}\) and the context vector is updated by the word vector via \(^{}\) (see main text for details).

retrieved with higher probability than words studied _before_ the previously recalled word, leading to a tendency of recalling words in the same temporal ordering of the study phase (forward asymmetry). These effects are typically quantified through the conditional response probability (CRP): given the most recently recalled stimulus with a serial position \(i\) during study, the CRP is the probability that the subsequently recalled stimulus comes from the serial position \(i\)+lag (see e.g., Fig. 4a).

## 3 Transformer models and induction heads

### Residual stream and interacting heads

The standard view of Transformers emphasizes the stacking of Transformer blocks. An alternative, mathematically equivalent view emphasizes the _residual stream_[21; 22]. Each token at position \(i\) in the input has its own residual stream \(z_{i}\) (with a dimension of \(d_{}\)) serving as a shared communication channel between model components at different layers (Fig. 1c; the residual stream is shown as a blue path), such as self-attention and multi-layered perceptrons (MLP). The initial residual stream \(z_{i}^{(0)}\) contains _token embeddings_\(TE\) (vectors that represent tokens in the semantic space) and _position embeddings_\(PE\) (vectors that encode positions of each input token). Each model component reads from the residual stream, performs a computation, and _additively_ writes into the residual stream. Specifically, attention heads at layer \(l\) read from all past \(z_{j}\) (with \(j i\)) and write into the current \(z_{i}\) as \(z_{i}^{(l)^{}} z_{i}^{(l-1)}+_{\;h}H^{(h)} (z_{i}^{(l-1)};\{z_{j}^{(l-1)}\}_{j i\})\), while MLP layers read from only the current \(z_{i}\) and write into \(z_{i}\) as \(z_{i}^{(l)} z_{i}^{(l)^{}}+(z_{i}^{(l-1)^{}})\). Readers unfamiliar with attention heads (e.g., attention scores and patterns) are referred to Appendix B. Other components like layer normalization are omitted for simplicity. At the final layer, the residual stream is passed through the unembedding layer to generate the logits (input to softmax) that predict the next token.

Components in different layers can interact with each other through the residual stream . As an important example, a first-layer head \(H_{1}\) may write its output into the residual stream, which is later read by a second-layer head \(H_{2}\) that writes its output to the residual stream for later layers to use.

### Induction heads and their attention patterns

Previous mechanistic interpretability studies identified a type of attention heads critical for ICL, known as _induction heads_[21; 15; 23; 24]. Induction heads are defined by their _match-then-copy_ behavior [15; 24]. They look back (_prefix matching_) over previous occurrences of the current input token (e.g., [B]), determine the subsequent token (e.g., [C] if the past context included the pair [B][C]), and increase the probability of the latter - that is, after finding a "match", it makes a "copy" as the predicted next token (...[B][C]...[B] \(\) [C]). To formalize this match-then-copy pattern, we use the induction-head matching score (between 0 and 1) to measure the prefix-matching behavior. We then use the copying score (between -1 and 1) to measure the copying behavior (see Appendix E). An induction head should have a large induction-head matching score and a positive copying score.

We first examined the induction behaviors of attention heads in the pre-trained GPT2-small model  using the TransformerLens library . To elicit induction behaviors, we constructed a prompt consisting of two repeats of a random-token sequence (see Section 2 and Appendix D). We recorded the attention scores of each head \(}\) (before softmax) and the attention patterns \(\) (after softmax) for each pair of previous and current token positions (for definitions see Appendix B). Several heads in GPT2-small had a high induction-head matching score (Fig. 2a), such as L5H1 (layer number 5, head number 1). In the first sequence repeat, this attention head attends mostly to the beginning-of-sequence token. In the second repeat, this head shows a clear "induction stripe" (Fig. 2b) where it mostly attends to the token that follows the current token in the first repeat.

We calculated attention scores as a function of relative position lags to further characterize the behavior of induction heads. This analysis is reminiscent of the CRP analysis on human recall data, as in Section 4.1. We found that induction heads' attention to earlier tokens follows a similar pattern as seen in human episodic recall (Fig. 2c, Fig. 5a-c), including temporal contiguity (e.g., the average attention score for \(|| 2\) is larger than for \(|| 4\)) and forward asymmetry (e.g., the average attention score for \(\!>0\) is larger than for \(\!<0\)).

### K-composition and Q-composition induction heads

The matching score and copying score describe the behavior of individual attention heads. However, they do not provide a mechanistic understanding of _how_ the induction head works internally. To gain insights into the internal mechanisms of induction heads, we focus here on smaller transformer models, acknowledging that individual attention heads of larger LLMs likely exhibit more sophisticated behavior. Prior work has discovered two kinds of induction mechanisms in two-layer attention-only Transformer models: K-composition and Q-composition (Fig. 3a-b, Tab. S1) [21; 24], characterizing how information from the first-layer head is composed to inform attention of the second-layer head. We provide an overview of both below. Our main focus in this paper is the comparison between Q-composition and CMR, but K-composition is provided as a point of comparison for readers familiar with mechanistic interpretability.

In K-composition (Fig. 3a), the first-layer "previous token" head uses the current token's position embedding, \(PE_{i}\), as the query, and a past token's position embedding \(PE_{j}\), as the key. When the match condition \(PE_{j}=PE_{i-1}\) is satisfied (meaning \(j\) is the previous position of \(i\)), the head writes the previous token's token embedding, \(TE_{j}\), as the value into the residual stream \(z_{i}\). The second-layer induction head uses the current token's \(TE_{k}\) as the query, and the previous token head's output \(TE_{j}\) at residual stream \(z_{i}\) as the key ("K-composition"). When the match condition \(TE_{j}=TE_{k}\) is satisfied, the head writes \(TE_{i}\) (at residual stream \(z_{i}\)) as the value into the residual stream \(z_{k}\), effectively increasing the logit for the token that occurred at position \(i\).

In Q-composition (Fig. 3b), the first-layer "duplicate token" head uses the current token's \(TE_{k}\) as the query, and a past token's \(TE_{j}\) as the key. When the match condition \(TE_{j}=TE_{k}\) is satisfied (meaning token \(k\) is a duplicate of token \(j\)), the head writes the token's \(PE_{j}\) as the value into the residual stream \(z_{k}\). The second-layer induction head uses the duplicate token head's output \(PE_{j}\) at residual stream \(z_{k}\) as the query ("Q-composition") and a past token's \(PE_{i}\) as the key. When the match condition \(PE_{j}=PE_{i-1}\) is satisfied, the head writes \(TE_{i}\) (at residual stream \(z_{i}\)) as the value into the residual stream \(z_{k}\), increasing the logit for the token that occurred at position \(i\).

In the following sections, we will reveal a novel connection between ICL and human episodic memory. We first introduce the CMR model of episodic memory, and then formally re-write it as a Q-composition induction head performing prefix matching, allowing us to link induction heads' attention biases to those known in human episodic memory.

## 4 Contextual maintenance and retrieval model (CMR)

### CMR in its original form

CMR, an influential model of human episodic memory, provides a general framework to model memory recall as associative retrieval. It leverages a distributed representation called _temporal context_, which acts as a dynamic cue for recalling subsequent information based on recently seen words . CMR explains the asymmetric contiguity bias in human free recall (see Fig. 4 and Fig. S1) and has been extended to more complex memory phenomena such as semantic  and emotional

Figure 2: **Induction heads in the GPT2-small model.****(a)** Several heads in GPT2 have a relatively large induction-head matching score. **(b)** The attention pattern of the L5H1 head, which has the largest induction-head matching score. The diagonal line (“induction stripe”) shows the attention from the destination token in the second repeat to the source token in the first repeat. **(c)** The attention scores of the L5H1 head averaged over all tokens in the designed prompt as a function of the relative position lag (similar to CRP). Error bars show the SEM across tokens.

 effects. We provide a pedagogical introduction to CMR in Appendix C for readers without a background in cognitive science or episodic memory. Below, we briefly list the essentials of CMR.

In CMR (Fig. 1d), each word token is represented by an embedding vector \(\) (e.g., one-hot; \(_{i}\) for the \(i\)-th word in a sequence). The core dynamic that drives both sequential encoding and retrieval is

\[_{i}=_{i-1}+_{i}^{},\] (1)

where \(_{i}\) is the temporal context at time step \(i\), and \(_{i}^{}\) is an input context associated with \(_{i}\). \(\) controls the degree of _temporal drift_ between time steps (\(_{}\) for encoding/study phase and \(_{}\) for decoding/retrieval phase) and \(\) is picked to ensure \(_{i}\) has unit norm. Specifically, during the encoding phase, \(_{i}^{}\) represents a _pre_-experimental context associated with the \(i\)-th word as \(_{i}^{}=_{}^{}_{i}\), where \(_{}^{}\) is a pre-fixed matrix. At each time step, a word-to-context (mapping \(\) to \(\)) memory matrix \(_{}^{}\) learns the association between \(_{i}\) and \(_{i-1}\) (i.e., \(_{}^{}\) is updated by \(_{i-1}_{i}^{T}\)). During the decoding (retrieval) phase, \(_{i}^{}\) is a mixture of pre-experimental (\(_{}^{}=_{}^{} _{i}\)) and experimental contexts (\(_{}^{}=_{}^{ }_{i}\)). The proportion of these two contexts is controlled by an additional parameter \(_{}\) as \(_{i}^{}=((1-_{})_{}^{}+_{}_{}^{}) _{i}\). The asymmetric contiguity bias arises from this slow evolution of temporal context: when \(0<<1\), \(_{i}\) passes through multiple time steps, causing nearby tokens to be associated with temporally adjacent contexts that are similar to each other (temporal contiguity), i.e., \(_{i},_{j}\) is large if \(|i-j|\) is small. Additionally, \(_{}^{}=_{}^{ }_{i}\) only enters the temporal context after time \(i\). Thus \(_{i,}^{}\) is associated with \(_{j}\)_only_ for \(j>i\) (asymmetry).

CMR also learns a second context-to-word (mapping \(\) back to \(\)) memory matrix \(^{}\) (updated by each \(_{i}_{i-1}^{T}\)). When an output is needed, CMR retrieves a mixed word embedding \(_{i}^{}=^{}_{i}\). If \(_{j}\) are one-hot encoded, we can simply treat \(_{i}^{}\) as a (unnormalized) probability distribution over the input tokens. Or, CMR can compute the inner product \(_{j},_{i}^{}\) for each cached word \(_{j}\) as input to softmax (with an inverse temperature \(^{-1}\)) to recall a word.

Intuitively, the temporal context resembles a moving spotlight with a fuzzy edge: it carries recency-weighted historical information that may be relevant to the present, where the degree of information degradation is controlled by \(\). Larger \(\)'s correspond to "sharper" CRPs with stronger forward asymmetry and stronger temporal clustering that are core features of human episodic memory. As a concrete example, consider \(n\) unique one-hot encoded words \(\{_{i}\}\). If \(_{}^{}=_{i=1}^{n}_{i} _{i}^{T}\) (i.e., the pre-experimental context associated with each word embedding is the word embedding itself) and \(_{}=0\), Eq. 1 at decoding is reduced to \(_{i}=_{i-1}+_{i}=_{j=1}^{i} ^{i-j}_{j}\), which is a linear combination of past word embeddings.

Figure 3: **Comparison of composition mechanisms of induction heads and CMR. All panels correspond to the optimal Q-K match condition (\(j=i-1\)). See the main text and Tab. S1 for details. (a) K-composition induction head. The first-layer head’s output serves as the _Key_ of the second-layer head. (b) Q-composition induction head. The first-layer head’s output serves as the _Query_ of the second-layer head. (c) CMR is similar to a Q-composition induction head, except that the context vector \(t_{j-1}\) is first updated by \(^{}\) into \(t_{j}\) at position \(j\), then directly used at position \(j+1\) (equal to \(i\) for the optimal match condition; shown by red lines).**

### CMR as an induction head

We now proceed to map CMR to a Q-composition-like head (see Fig. 3c and Tab. S1 for details).

To begin with, we note that the word \(_{i}\) seen at position \(i\) is the same as \(TE_{i}\), and the context vector \(_{i-1}\) (before update) at position \(i\) is functionally similar to \(PE_{i}\). It follows that the set \(\{_{i}\), \(_{i-1}\}\) is functionally similar to the residual stream \(z_{i}\), updated by the head outputs.

**CMR experimental word-context retrieval as first-layer self-attention**. The temporal context is updated by \(_{i}^{ IN}=((1-_{ FT})_{ pre}^{ FT}+ _{ FT}_{ exp,i}^{ FT})_{i}\) at decoding. The memory matrix \(_{ exp,i}^{ FT}\) acts as a first-layer duplicate token head, where the current word \(_{i}\) is the query, the past embeddings \(\{_{j}\}\) make up the keys, and the temporal contexts \(_{j-1}\) associated with each \(_{j}\) are values. This head effectively outputs "What's the position (context vector) at which I encountered the same token \(_{i}\)?"

**CMR pre-experimental word-context retrieval as MLP**. The pre-experimental context \(_{i}^{ IN}=_{ pre}^{ FT}_{i}\) (retrieved contextual information not present in the experiment) is the output of a linear fully-connected layer (functionally similar to MLP; not drawn).

**CMR evolution as residual stream updating**. The context vector is updated by \(_{k}=_{k-1}+_{k}^{ IN}\). Equivalently, the head \(^{ FT}\) updates the information from \(\{_{k}\), \(_{k-1}\}\) to \(\{_{k}\), \(_{k-1}\), \(_{k}\}\). At the position \(k\) during recall, the updated context \(_{k}\) contains \(_{k}^{ IN}\) (\(_{j}\)) (Fig. 3c).

**CMR context-word retrieval as second-layer self-attention**. The retrieved embedding is \(_{k}^{ IN}=^{ TF}_{k}\), where \(^{ TF}\) acts as a second-layer induction head, where the temporal context \(_{k}\) is the query, the past contexts \(\{_{i-1}\}\) make up the keys, and the embeddings \(_{i}\) associated with each \(_{i-1}\) are values. This effectively implements Q-composition , because \(_{k}\), as the _Query_, is affected by the output of the first-layer \(_{ exp}^{ FT}\) head.

**CMR word recall as unembedding**. The final retrieved word probability is determined by the inner product between the retrieved memory \(_{k}^{ IN}\) and each studied word \(_{j}\), similar to the unembedding layer generating the output logits from the residual stream.

**CMR learning as a causal linear attention head**. Both associative matrices of CMR are learned in a manner consistent with the formation of causal linear attention heads. Specifically, the word-to-context matrix is updated by \(_{ exp,i}^{ FT}=_{ exp,i-1}^{ FT}+_ {i-1}_{i}^{T}\) (with \(_{ exp,0}^{ FT}=\)), associating \(_{i}\) (key) and \(_{i-1}\) (value). It is equivalent to a causal linear attention head, because \(_{ exp}^{ FT}_{k}=(_{i<k}_{i-1}_{i}^{T})_{k}=_{i<k}_{i-1}(_{i}^{T} _{k})=_{i<k}(_{i},_{k})_ {i-1}\). Similarly, the context-to-word

Figure 4: **The conditional response probability (CRP) as a function of position lags in a human experiment and different parametrization of CMR.****(a)** CRP of participants (N=171) in the PEERS dataset, reproduced from . “Top 10%” refers to participants whose performance was in the top 10th percentile of the population when recall started from the beginning of the list. They have a sharper CRP with a larger forward asymmetry than other subjects. **(b)** Left, CMR with “sequential chaining” behavior (\(_{ enc}=_{ rec}=1,_{ FT}=0\)). The recall has exactly the same order as the study phase without skipping over any word. Right, CMR with moderate updating at both encoding and retrieval, resulting in human-like free recall behavior (\(_{ enc}=_{ rec}=0.7,_{ FT}=0\)). Recall is more likely than not to have the same order as during study and sometimes skips words. **(c)** Same as (b Right) except with \(_{ FT}=0.5\) (Left) and \(_{ FT}=1\) (Right). For more examples, see Fig. S1.

matrix, updated by \(_{i}^{}=_{i-1}^{}+_{i}_{i-1}^{T}\) (with \(_{i}^{}=\)), is equivalent to a causal linear attention head that associates \(_{i-1}\) (key) with \(_{i}\) (value).

To summarize, the CMR architecture resembles a two-layer transformer with a Q-composition linear induction head. It's worth noting that although we cast \(_{i}\) as the position embedding, unlike position embeddings that permit parallel processing in Transformer models, \(_{i}\) is recurrently updated in CMR (Eq. 1). It is possible that Transformer models might acquire induction heads with a similar circuit mechanism, where \(_{i}\) corresponds to autoregressively updated context information in the residual stream that serves as the input for downstream attention heads.

## 5 Experiments

### Quantifying the similarity between an induction head and CMR

We have shown that induction heads in pre-trained LLMs exhibit CMR-like attention biases (Fig. 2c, Fig. 5a-c; also see Fig. S2 for heads unlike CMR) and established the mechanistic similarity between induction heads and CMR (Fig. 3). To further quantify their behavioral similarity, we propose the metric _CMR distance_, defined as the mean squared error (MSE) between the head's average attention scores and its CMR-fitted scores (see Appendix E, Fig. 5a-d). In essence, we optimized the parameters \((_{},_{},_{},^{-1})\) for each head to obtain a set of CMR-fitted scores that minimizes MSE.

At the population level, heads with a large induction-head matching score and a positive copying score also have a smaller CMR distance (Fig. 5e), suggesting that the CMR distance captures meaningful behavior of these heads. Notably, certain heads that are not typically considered induction heads (e.g., peaking at lag=0) can be well captured by CMR (Fig. 5d).

Consistent with prior findings that induction heads were primarily observed in the intermediate layers of LLMs , we found that the majority of heads in the intermediate-to-late layers of GPT2-small have lower CMR distances (Fig. 6a, Fig. S3a; for the choice of threshold see Appendix E.5). We also observed similar phenomena in a different set of LLMs called Pythia (Fig. 6b), a family of models with shared architecture but different sizes, as well as three well-known models (Qwen-7B , Mistral-7B , Llama3-8B , Fig. 6c). We summarized these results in Fig. S3b.

Additionally, to contextualize these CMR distances, we included the Gaussian distance using Gaussian functions as a baseline (with the same number of parameters as CMR), since its bell shape captures the basic aspects of temporal contiguity and forward/backward asymmetry. We found that, across 12 different models (GPT2, Pythia models, Qwen-7B, Mistral-7B, Llama3-8B), CMR provides significantly better descriptions (lower distances) than the Gaussian function for the top induction heads (average CMR distance: 0.11 (top 20), 0.05 (top 50), 0.12 (top 100), 0.12 (top 200); average

Figure 5: **CMR distance provides meaningful descriptions for attention heads in GPT2. (a-c) Average attention scores and the CMR-fitted attention scores of example induction heads (with a non-zero induction-head matching score and positive copying score). (d) Average attention scores and the CMR-fitted attention scores of a duplicate token head  that is traditionally not considered an induction head but can be well-captured by the CMR. (e) (Top) CMR distance (measured by MSE) and the induction-head matching score for each head. (Bottom) Histogram of the CMR distance.**

Gaussian distance: 1.0 (top 20), 0.98 (top 50), 0.98 (top 100), 0.97 (top 200); all \(p<0.0001\)), again suggesting that CMR is well-poised to explain the properties of observed CRPs.

### CMR-like heads develop human-like temporal clustering over training

The Pythia models' different pretraining checkpoints allowed us to measure the emergence of CMR-like behavior. As the model's loss on the designed prompt decreases through training (Fig. 7a), the degree of temporal clustering increases, especially in layers where induction heads usually emerge. For instance, intermediate layers of Pythia-70m (e.g., L3, L4) show the strongest temporal clustering that persists over training (Fig. S4a-b). This, combined with an increasing inverse temperature (Fig. 7b), suggests that attention patterns become more deterministic over training, while shaped to mirror human-like asymmetric contiguity biases. In fact, human subjects with better free recall performance tend to exhibit stronger temporal clustering and a higher inverse temperature .

Figure 6: **CMR distances vary with relative layer positions in LLMs.****(a-b)** Percentage of heads with a CMR distance less than 0.5 in different layers. Also see Fig. S3c-d for the threshold of 0.1. **(a)** GPT2-small. **(b)** Pythia models across different model sizes (label indicates the number of model parameters). CMR distances are computed based on the last model checkpoint. **(c)** Qwen-7B, Mistral-7B, and Llama-8B models. Heads with lower CMR distances often emerge in the intermediate-to-late layers.

Figure 7: **Strong asymmetric contiguity bias arises as model performance improves.****(a)** Model loss on the designed prompt as a function of training time. Loss is recorded every 10 training checkpoints. **(b)** Average fitted inverse temperature increases in the intermediate layers of Pythia-70m as training progresses. Values are averaged across heads with CMR distance lower than 0.5 in each layer. **(c)** Comparison of fitted \(_{}\) and \(_{}\) in Pythia’s top CMR-like heads and in existing human studies. **(d)** CMR distance of top induction heads in Pythia models as a function of training time. Heads are selected based on the highest induction-head matching scores across all Pythia models (e.g., “top 20” corresponds to twenty heads with the highest induction-head matching scores). **(e)** Fitted CMR temporal drift parameters \(_{}\)(left), \(_{}\) (middle), \(_{}\) (right) as a function of training time in attention heads with the highest induction-head matching scores. **(f-g)** Same as c-d but for top CMR-like heads (e.g., “top 20” corresponds to those with the lowest CMR distances), demonstrating differences between top induction heads and top CMR-like heads. Shaded regions indicate standard error, except **(b)** which indicates the range (the scale factor \(^{-1}\) is non-negative).

For individual heads, those with higher induction-head matching scores (Fig. 6(d)) (or similarly with smaller CMR distances, see Fig. 6(f)) consistently exhibit greater temporal clustering (Fig. 6(e), f respectively), as the fitted \(\)'s (both \(_{}\) and \(_{}\)) were large. We also observed similar values of fitted \(\)s in Qwen-7B, Mistral-7B, and Llama3-8B (Fig. 4(b)). These fitted \(\)'s of these attention heads fall into a similar range as human recall data (Fig. 6(c)). We interpret this in light of a normative view of the human memory system: in humans, the asymmetric contiguity bias with a \(<1\) is not merely phenomenological; under the CMR framework, it gives rise to an optimal policy to maximize memory recall when encoding and retrieval are noisy . In effect, a large \(\) (but less than 1) in Eq. 1 provides meaningful associations beyond adjacent words to facilitate recall, such that even if the immediately following token is poorly encoded or the agent fails to decode it, information from close-by tokens encapsulated in the temporal context still allows the agent to continue decoding.

In addition, we observed an increase in \(_{}\) and \(_{}\) (Fig. 6(e), g) during the first 10 training checkpoints, when the model loss significantly drops. The training process leads to higher values of \(_{}\). Specifically, \(_{}\) values are higher than \(_{}\), highlighting the importance of temporal clustering during decoding for model performance. These results suggest that attention to temporally adjacent tokens with an asymmetric contiguity bias may support ICL in LLMs.

### CMR-like heads are causally relevant for ICL capability

While these CMR-like heads were identified using repeated random tokens, we ask whether they were also causally necessary for ICL in more naturalistic tasks. We thus performed an ablation study using natural language texts. Specifically, we ablated either the top 10% CMR-like heads (i.e., top 10% heads with the smallest CMR distances) or the same number of randomly selected heads in each model. We then computed the resultant ICL score on the sampled texts (2000 sequences, each with at least 512 tokens) from the processed version of Google's C4 dataset . The ICL score is defined as the loss of the 500th token in the context minus the loss of the 50th token in the context, averaged over dataset examples . Intuitively, a model with better in-context learning ability has a lower ICL score, as the 500th token is further into the context established from the beginning. We tested models with various model architectures and complexity, including GPT2, Pythia models, and Qwen-7B (Fig. 8). Most models showed a higher ICL score (worse ICL ability) if the top 10% CMR-like heads were ablated, compared to if the same number of randomly selected heads were ablated. This effect was particularly significant if the original model had a low ICL score (e.g., GPT2, Qwen-7B).

Our finding therefore suggests that CMR-like heads are not merely an epiphenomenon, but essential underlying LLM's ICL ability, and that the CMR distance is a meaningful metric to characterize individual heads in LLMs. Nonetheless, our result needs to be interpreted cautiously: First, a Hydra effect has been noted where ablation of heads causes other heads to compensate . Second, our analysis cannot confirm a direct causal role of the CMR-like behavior of these CMR-like heads in ICL, since it is possible that characteristics other than the episodic-memory features in these CMR-like heads might causally contribute to ICL. Finally, the ICL scores for the original Pythia models are closer to 0 (worse ICL performance) than other models', suggesting either weaker ICL ability of the Pythia series, or larger distributional differences between their training data and our evaluation texts.

## 6 Discussion

This study bridges LLMs and human episodic memory by comparing Transformer models' induction heads and CMR. We revealed mechanistic similarities between CMR and Q-composition induction

Figure 8: **CMR-like heads are causally relevant for ICL.** ICL scores are evaluated for intact models (Original), models with the top 10% CMR-like heads ablated (Top 10% ablated), and models with randomly selected heads ablated (Random ablated). Lower scores indicate better ICL abilities, with error bars showing SEM across sequences. \(***\): \(p<0.001\), \(**\): \(p<0.01\), \(*\): \(p<0.05\), n.s.: \(p 0.1\).

heads and identified CMR-like attention biases (i.e., asymmetric contiguity) in pre-trained LLMs. Notably, CMR-like heads emerge in LLMs' intermediate-to-late layers, evolve towards a state akin to human memory biases, and may play a causal role in ICL. These findings offer significant connections between the current generation of AI algorithms and a century of human memory research.

From a neuroscience and cognitive science perspective, CMR's link to induction heads might reveal normative principles of memory and hippocampal processing, echoing the role of the hippocampus in pattern prediction and completion . While CMR is a behavioral model, it also explains neural activity patterns: associative matrices that represent episodic memories may be instantiated in hippocampal synapses. The temporal context aligns with the hippocampus' recurrent nature, likely involving subregions including CA1, dentate gyrus , CA3 ), and cortical areas projecting to the hippocampus such as the entorhinal cortex .

Our results show strong connections to neural network models of episodic memory. For example, neural networks with attention  or recurrence  trained for free recall exhibit the same recall pattern as the optimal CMR.  showed that a neural network implementation of CMR can explain humans' flexible cognitive control. Our results also align with research connecting attention mechanisms in Transformers to the hippocampal formation . While prior work focused on emergent place and grid cells in Transformers, the hippocampal subfields involved are also postulated to represent CMR components . These results support our proposal that query-key-value attention mechanisms link to biological episodic retrieval, suggesting that CMR-like behavior emerges naturally in neural networks with proper objectives.

The link to induction heads could enable researchers to develop alternative mechanisms for episodic memory. For instance, K-composition and Q-composition induction circuits might serve as alternative models to CMR. K-composition and Q-composition require positional encoding, which we speculate could be implemented by grid-like cells with periodic activations tracking space and time . Further, the interactions between episodic memory and other advanced cognitive functions  might be understood based on more complex attention-head composition mechanisms (e.g., N-th order virtual attention head ).

From the perspective of LLM mechanistic interpretability, we offer a more detailed behavioral description and reinterpret induction heads through the lens of CMR and the asymmetric contiguity bias. First, CMR-like heads are not limited to induction heads - some attention heads are well captured by CMR despite not meeting the traditional induction head criterion (e.g., Fig. 5d). Second, we speculate that heads with low CMR distances and low induction-head matching scores may encode multiple future tokens observed in LLMs , capturing distant token information better than ideal induction heads. Third, we observed a scale difference between raw attention scores and those from human recall, as discussed in Appendix G.1. Lastly, though CMR relies on recurrently updated context vectors that are different from the K-composition and Q-composition mechanisms, we posit that deeper Transformer models may develop similar mechanisms via autoregressively updated information in the residual stream, a possibility yet to be explored. Overall, our results suggest a fuller view of ICL mechanisms in LLMs, where heads learn attention biases akin to human episodic memory, empowering next-token and future-token predictions.

These connections with CMR may shed light on intriguing features and functions in LLMs. For instance, the "lost in the middle" phenomenon may be related to these heads , as humans exhibit similar recall patterns . Understanding the connection could suggest strategies to mitigate the problem, e.g., adjusting study schedules based on the serial position . Second, CMR not only applies to individual words but also to clusters of items , suggesting these heads may process hierarchically organized text chunks . Third, episodic mechanisms captured by CMR are posited to support adaptive control  and flexible reinforcement learning , suggesting similar roles for these heads in more complex cognitive functions of LLMs. Finally, it was proposed that Transformers can be implemented in a biologically plausible way , and we outline an alternative proposal mapping Transformer models to the brain (Appendix G.2).

Our study has several limitations. First, while we use CMR to characterize these heads' behavior, it is unclear if CMR can serve as a mechanistic model in larger Transformer models. Second, it is unclear if our conclusions hold for untested Transformer models. Third, our causal analysis lacks the power to narrow down the causal role of specific CMR-like characteristics. In addition, while ICL score is a primary metric for measuring ICL ability , a systematic evaluation using specific datasets and tasks would allow for a stronger causal claim. Addressing these limitations is a key future direction.