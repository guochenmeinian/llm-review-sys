# The calculations required for CFVFP in one information set are:

Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play

Ju Qi\({}^{1,2,}\), Hei Falin\({}^{1,2}\), Feng Ting\({}^{1,2}\), Yi Dengbing\({}^{1,2}\), Fang Zhemei\({}^{1,2,}\), Luo Yunfeng\({}^{1,2}\)

1. School of Artificial Intelligence and Automation, Huazhong University of Science and Technology

2. National Key Laboratory of Science and Technology on Multispectral Information Processing

{juqi, heifalin, fenting, yidengbing, zmfang2018, yfluo}@hust.edu.cn

\({}^{}\) Corresponding author.

###### Abstract

Counterfactual Regret Minimization (CFR) and its variants are widely recognized as effective algorithms for solving extensive-form imperfect information games. Recently, many improvements have been focused on enhancing the convergence speed of the CFR algorithm. However, most of these variants are not applicable under Monte Carlo (MC) conditions, making them unsuitable for training in large-scale games. We introduce a new MC-based algorithm for solving extensive-form imperfect information games, called MCCFVFP (Monte Carlo Counterfactual Value-Based Fictitious Play). MCCFVFP combines CFR's counterfactual value calculations with fictitious play's best response strategy, leveraging the strengths of fictitious play to gain significant advantages in games with a high proportion of dominated strategies. Experimental results show that MCCFVFP achieved convergence speeds approximately 20%\(\)50% faster than the most advanced MCCFR variants in games like poker and other test games.

## 1 Introduction

Game theory investigates mathematical models of strategic interactions among rational agents, aiming to identify a Nash Equilibrium (NE) where no participant can gain by deviating from the established strategy. For complete information games, it is possible to segment the game into smaller sub-games and apply backward induction to determine the equilibrium. In contrast, in incomplete information games, the inability to directly apply sub-game payoffs in the backward induction algorithm significantly increases the complexity of finding an equilibrium.

The Counterfactual Regret Minimization (CFR) algorithm Zinkevich _et al._ (2007) is crucial for solving extensive-form games with incomplete information. CFR has numerous variants, such as CFR+ Tammelin (2014), Lazy CFR Zhou _et al._ (2018), PCFR Farina _et al._ (2021), DCFR Brown and Sandholm (2019), and PDCFR Xu _et al._ (2024). Nevertheless, these variants typically require a full traversal of the entire game tree, which is impractical for real-world applications. In contrast, games like no-limit Texas hold'em, DouDizhu, and Mahjong have \(10^{162}\) Moravik _et al._ (2017), \(10^{83}\)Zha _et al._ (2021), and \(10^{121}\)Li _et al._ (2020) information sets, respectively, making full traversal infeasible.

To address this, Lanctot et al. Lanctot _et al._ (2010) proposed Monte Carlo CFR (MCCFR), which reduces the number of game tree nodes visited in each iteration through sampling. This makes MCCFR the preferred algorithm for training large-scale games. However, most CFR variants cannot improve the convergence speed in the case of MCCFR. Only a few variants, such as Discount MCCFR Brown and Sandholm (2019) and VR-MCCFR Schmid _et al._ (2019), are currently suitable for MCCFR. Therefore, it is crucial to study the characteristics of large-scale games and how to leverage these characteristics to accelerate the convergence of MC-based game learning algorithms.

We believe that the Fictitious Play (FP) algorithm is highly suitable for addressing large-scale incomplete information games, due to the characteristics of using the best response (BR) strategy. FP was first introduced in Brown's 1951 article Berger (1907); Brown (1951). The treatise _The Theory of Learning in Games_Fudenberg and Levine (1998) consolidated prior research, setting a standardized framework for FP. Generalized Weakened Fictitious Play (GWFP) Leslie and Collins (2006) further established that under specific perturbations and errors, convergence to NE is attainable in a manner consistent with FP. Hendon et al. Hendon _et al._ (1996) expanded FP to the domain of extensive-form games. The development of the Full-Width Extensive-Form Fictitious Play (XFP) algorithm Heinrich _et al._ (2015), predicated on GWFP, has facilitated faster convergence in these games.

Our algorithm, Monte Carlo Counterfactual Value-Based Fictitious Play (MCCFVFP), optimizes FP in extensive-form games by incorporating counterfactual value into the BR strategy calculations. We have theoretically proven that MCCFVFP can converge to a NE, and experimentally demonstrated that in large-scale games, MCCFVFP fully leverages the high proportion of dominated strategies to achieve faster convergence speeds than MCCFR. Our code can be found at GitHub.

## 2 Notation and Preliminaries

### Game Theory

#### 2.1.1 Normal-Form Game

The normal-form game serves as the foundational model in Game Theory. Let \(=1,2,,i,\) represent the set of players, where each player \(i\) has a finite action set \(^{i}\). Player \(i\)'s strategy, denoted \(^{i}\), is a probability distribution over \(^{i}\) and is represented by a \((|^{i}|-1)\)-dimensional simplex. Here, \(||\) indicates the cardinality of the set, and \(^{i}(a^{})\) denotes the probability with which player \(i\) selects action \(a^{}\). Let \(^{i}\) denote the strategy set for player \(i\), such that \(^{i}^{i}\). A strategy profile, \(=_{i}^{i}\), represents the collection of strategies for all players, while \(^{-i}=(^{1},,^{i-1},^{i+1},)\) includes all strategies in \(\) except for that of player \(i\). The entire set of strategy profiles is denoted by \(=_{i}^{i}\), where \(\). The payoff function for player \(i\), defined as \(u^{i}:\), is finite. The notation \(u^{i}(^{i},^{-i})\) indicates the expected payoff to player \(i\) when they select the pure strategy \(^{i}\) and all other players follow the strategy profile \(^{-i}\). Finally, define the payoff interval of the game as \(L=_{,i}u^{i}()-_{,i }u^{i}()\).

#### 2.1.2 Extensive-Form Games

In extensive-form games, typically represented as game trees, the player set is \(=1,2,\). Each node \(s\) represents a possible state in the game, forming a set \(\), with terminal states represented by leaf nodes \(z\). At each state \(s\), the action set \((s)\) includes all possible actions available to a player or chance. The player function \(P: c\) assigns each state an acting party, with \(c\) indicating a chance event. Information sets \(I^{i}\) group states that player \(i\) cannot distinguish among, reflecting their uncertainty. The payoff function \(R:^{||}\) maps each terminal state to a payoff vector for the players. For each information set \(I^{i}\), the behavioral strategy \(^{i}(I)^{|(I)|}\) defines a probability distribution over available actions.

#### 2.1.3 Nash Equilibrium

The BR strategy of player \(i\) to their opponents' strategies \(^{-i}\) is

\[b^{i}(^{-i})=_{a^{i}^{i}}u^{i}(a^{i},^{-i}),\] (1)

although a mixed strategy may also become a BR strategy, it is much easier to find a pure BR strategy than a mixed BR strategy in engineering implementation, so the BR strategies discussed in this article are all pure BR strategies. If there are multiple BR strategies at the same time, one of them will be returned randomly. Define the exploitability \(^{i}\) of player \(i\) in strategy profile \(\) as:

\[^{i}=u^{i}(b^{i}(^{-i}),^{-i})-u^{i}(),\] (2)

and total exploitability as \(=_{i}^{i}\). A Nash equilibrium is a strategy profile \(\) such that \(=0\).

During iterations, if the strategy's exploitability satisfies \( T^{-}\), the convergence rate of the algorithm is \((T^{-})\). Additionally, the time complexity within one iteration is a critical factor affecting the convergence rate. We use \(()\) to describe the time complexity needed for one iteration.

#### 2.1.4 Dominated Strategy and Clear Games

In game theory, strategic dominance occurs when one action (or strategy) is better than another action (or strategy) for one player, regardless of the opponents' actions. A classic example is the _Prisoner's Dilemma_, where choosing to **testify** always yields a higher payoff than **staying silent**, regardless of the opponent's decision. Here, staying silent is a dominated strategy. Formally, For player \(i\), if there is a pure strategy \(a^{i*}\) and a strategy \(^{i*}^{i}\) satisfies:

\[u^{i}(a^{i*},^{-i}) u^{i}(^{i*},^{-i}),^{- i}^{-i},\] (3)

then the pure strategy \(a^{i*}\) is a dominated strategy of player \(i\). Rational players will invariably avoid choosing a dominated strategy, allowing the elimination of dominated strategy from the action set \(^{i}\) without impacting the game's NE Samuelson (1992).

In our analysis, the proportion of dominated strategies in a game is crucial, significantly impacting the convergence speed of various algorithms. We categorize games into two distinct types based on this factor. Games where the number of dominated strategies is less than \(|}\) are classified as **clear games**. This term captures a strategic landscape where the preferable choices are evident, owing to the relatively few dominated strategies. On the other hand, **tangled games** are defined by having a number of dominated strategies greater than \(|}\), reflecting a more complex and nuanced strategic environment with less obvious choices. The reason for defining these categories in this way can be found in Appendix B.3. Such classification aids in comprehending a game's nature and assessing the effectiveness of different algorithmic approaches.

### Regret Matching and Counterfactual Regret Minimization

In normal-form games, let \(_{t}^{i}\) be the strategy used by player \(i\) on round \(t\). The regret of player \(i\)'s action \(a^{i}\) at time \(T\) is:

\[R_{T}^{i}(a^{i})=_{t=1}^{T}u^{i}(a^{i},_{t}^{-i})-u^{i} (_{t}).\] (4)

The new strategy is produced by:

\[_{T+1}^{i}(a^{i})=^{i+}(a)}{_{a^{i}}R_{T}^{i+}(a)}&_{a^{i}}R_{T}^{i,+}(a)>0\\ ^{i}|}&\] (5)

where \(R_{T}^{i,+}(a)=(R_{T}^{i}(a),0)\). Since the probability of selecting action \(_{t+1}^{i}(a)\) is proportional to the non-negative regret value \(R_{T}^{i,+}(a)\) for this action, this algorithm is referred to as the regret matching algorithm. Define the average strategy as \(_{T}=_{t=1}^{T}_{t}\), which converges to the NE as \(T\), with a convergence rate of \((L|/T})\). In each iteration, RM requires a matrix multiplication operation, resulting in an iteration time complexity of \((||^{2})\).

CFR is a variant of RM specifically adapted for extensive-form games. The counterfactual value \(u(I,)\) is defined as the expected payoff assuming that information set \(I\) is reached and all players follow strategy \(\), with the exception that player \(i\) acts specifically to reach \(I\). For each action \(a^{i}(I)\), let \(|_{I a}\) represent a strategy profile identical to \(\) except that player \(i\) consistently chooses action \(a\) within information set \(I\). The counterfactual regret is then given by:

\[R_{T}^{i}(I,a)=_{t=1}^{T}_{_{t}}^{-i}(I)(u^{i}(I,_ {t}|_{I a})-u^{i}(I,_{t})),\] (6)

where \(_{_{t}}^{-i}(I)\) is the probability of information set \(I\) occurring if all players (including chance, except \(i\)) choose actions according to \(_{t}\). Let \(R_{T}^{i,+}(I,a)=(R_{T}^{i}(I,a),0)\), the strategy at time \(T+1\) is:

\[_{T+1}^{i}(I,a)=^{i+}(I,a)}{_{a^{i}}R_{T}^{i+}(I,a)}&_{a^{i}}R_{T}^{i,+}(I,a)>0\\ (I)|}&\] (7)The average strategy \(_{T}^{i}\) for an information set \(I\) on iteration \(T\) is:

\[_{T}^{i}(I)=^{T}_{_{t}}^{i}(I)_{t}^{i} (I)}{_{t=1}^{T}_{_{t}}^{i}(I)}.\] (8)

Eventually, \(_{T}\) will converge to NE with \(T\), the convergence rate of CFR is \((L|||/T})\) and the time complexity of one iteration of the CFR algorithm is \((||||)\).

There are many forms of sampling MCCFR, but the most common is external sampling MCCFR (ES-MCCFR) Brown (2020) because of its simplicity and powerful performance. In ES-MCCFR, some players are designated as traversers and others are samplers during an iteration. Traversers follow the CFR algorithm to update the regret and the average strategy of the experienced information set. On the rest of the sampler's nodes and the chance node, only one action is explored (sampled according to the player's strategy for that iteration on the information set), and the regret and the average strategy are not updated.

### Fictitious Play

FP assuming all players start with random strategy profile \(_{t=1}\), the strategy profile is updated following the iterative function:

\[_{t+1}=(1-_{t})_{t}+_{t} _{t+1},\] (9)

where in vanilla FP \(_{t}=1/(t+1)\), \(_{t+1}=b(_{t})\). The convergence rate of vanilla FP in a two-player zero-sum game may be \(()\)Daskalakis and Pan (2014), and the time complexity of one iteration of the vanilla FP algorithm is \((||^{2})\). To speed up iteration, define the average Q-value of player \(i\)'s action \(a^{i}\) at time \(T\) as:

\[_{T}^{i}(a^{i})=_{t=1}^{T}u^{i}(a^{i},_{t}^{- i}).\] (10)

If the game is a two-player game, or if \(u^{i}\) is an affine function (e.g., in a potential game), we have:

\[_{T}^{i}(a^{i}) =_{t=1}^{T}u^{i}(a^{i},_{t}^{-i}) =u(a^{i},_{t=1}^{T}_{t}^{-i})\] (11) \[=u^{i}(a^{i},_{t}^{-i}),\]

therefore, we can conclude that

\[b^{i}(^{-i})=_{a^{i}^{i}}_{T}^{i}(a^{i} ).\] (12)

In 2.1.3, it is noted that BR strategy \(_{t}^{i}\) in FP is a pure strategy. As a result, calculating \(u^{i}(a^{i},_{t}^{-i})\) involves simply selecting a row (or column) from the payoff matrix, eliminating the need for matrix multiplication as required in RM. Consequently, the time complexity of each iteration in the FP algorithm decreases to \((||)\), representing a significant improvement over the \((||^{2})\) complexity in RM.

## 3 Motivation of CFVFP

Currently, the mainstream approach for large-scale games is a combination of "pre-trained blueprint strategy + real-time search." For example, Pluribus initially employs the MCCFR algorithm to establish a blueprint strategy, which it applies in the early stages of the game, and then transitions to real-time search as the game progresses Brown and Sandholm (2019). Similarly, in the game of Go, AlphaGo primarily used reinforcement learning to train its policy and value networks, incorporating a limited number of MCTS rollouts during training. However, in actual matches against human experts, AlphaGo significantly increased the number of MCTS rollouts to enhance its real-time decision-making Silver _et al._ (2016, 2017). This phenomenon can be understood from two perspectives:1. The complexity of real-world games is so high that it is impossible for any single strategy (even with deep networks) to perfectly handle all game scenarios. Due to this extreme complexity, only sampling-based training methods are feasible. Many current improvements to CFR, such as CFR+ and Predictive CFR, are designed for full traversal of the game tree. While these methods accelerate convergence, they are unsuitable for sampling-based approaches because the inexact payoffs introduced by sampling significantly disrupt the convergence direction of each strategy update.
2. Training a blueprint strategy usually starts with sampling from the initial nodes, resulting in a higher probability of exploring early game nodes compared to leaf nodes. Consequently, while the blueprint strategy might be less effective in the later stages of the game, it often rapidly converges towards the optimal solution in the early stages due to thorough exploration.

After clarifying the specific ideas for training large-scale game AI. Next, we need to think about the characteristics of large-scale games and how to design corresponding MC-type solving algorithms based on these characteristics.

One prominent characteristic of large-scale games is that the majority of strategies are dominated. For instance, in chess and Go, top AI systems and human experts often opt for fixed openings. Similarly, professional poker players tend to fold most hands in the early rounds. Theoretically, DeepMind likens game strategies to a spinning top Czarnecki _et al._, implying that only a limited subset of strategies can be considered non-dominated. MCCFR experiments show that up to 96% of strategies can be pruned in certain games Lanctot _et al._. Likewise, the supplementary materials of Pluribus Brown and Sandholm [2019c] reveal that around 50% of information sets are never encountered during training (indicating that unplayed strategies are almost certainly dominated, with many traversed ones also being dominated). In other words, while it cannot be proven with absolute certainty, the choices of professional players and the experimental outcomes from current advanced AIs suggest that the larger the game, the higher the likelihood that it can be classified as a "clear game."

Additionally, our toy experiment clearly demonstrates that the FP algorithm is more effective for clear games compared to the RM algorithm 1. We attribute this advantage to the fact that FP employs the BR strategy rather than the RM strategy during iterations, which is why we aim to incorporate this feature into the iterations of CFR.

Figure 1: The figure compares the convergence rates of the RM and FP algorithms in a \(100 100\) random payoff matrix game generated from a \(N(0,1)\) distribution. In right figure, the convergence for a standard random payoff matrix is shown, while left figure illustrates the convergence in \(100 100\) random payoff matrix where the payoffs for actions 1 to 10 are uniformly increased by 5 (causing actions 11 to 100 to have a high probability of being dominated strategies). It can be observed that in this setting, the convergence rate of the FP is very close to that of RM. Considering that the complexity of one FP iteration is only \((||)\) compared to the complexity of RM, which is \((||^{2})\), in a clear game, the overall convergence rate of FP can actually surpass that of RM. Each scenario tested an average of 30 rounds. The shaded areas represent the 90% confidence intervals for these trials. The experiments in Appendix A can also confirm our view from another perspective.

CFVFP Method

### Counterfactual Value Fictitious Play Implementation

Our method can be easily adapted from CFR. First, there is no need to compute the counterfactual regret; instead, we define the counterfactual value:

\[Q_{t}^{i}(I,a)=Q_{t-1}^{i}(I,a)+_{_{t}}^{-i}(I)u^{i}(I,_{t}|_{ I a}).\] (13)

Second, the strategy in the next iteration is a BR strategy rather than RM strategy:

\[_{t+1}^{i}=_{a^{i}(I)}Q_{t}^{i}(I,a).\] (14)

The main distinction between \(Q_{t}^{i}\) and \(R_{t}^{i}\) is that \(Q_{t}^{i}\) omits the average payoff term \(u^{i}(I,_{t})\), as this term does not affect identifying the maximum value of \(Q_{t}^{i}\). This omission leads to a significant reduction in computation time. Since \(\) selects the action with the highest counterfactual value, the resulting strategy in each iteration is a pure strategy, similar to the FP algorithm. Thus, we refer to this algorithm as Counterfactual Value-Based Fictitious Play. Additionally, the time complexity of calculating the BR strategy is notably lower than that required for computing the RM strategy.

Finally, since the update \(_{t+1}^{i}\) is a pure strategy, \(_{_{t}}^{-i}(I)\) is either 0 or 1. This significantly increases the likelihood of triggering naive pruning. From equation 6, we see that if \(_{_{t}}^{-i}(I)=0\), it is unnecessary to enter the sub-game tree to calculate \(Q_{t}^{i}(I,a)\). Similarly, equation 8 shows that if \(_{_{t}}^{i}(I)=0\), it is unnecessary to update the average strategy \(_{t}^{i}(I)\). This pruning greatly enhances the algorithm's efficiency.

CFVFP employs a simple yet effective approach by using BR strategy instead of regret-matching strategy for next iteration. The convergence of CFVFP can be easily proven. First, FP's convergence to a NE classifies it as a regret minimizer Abernethy _et al._ (2011), which also implies that FP satisfies Blackwell approachability Blackwell (1956). The CFR framework shows that if an algorithm satisfies Blackwell approachability, its counterfactual regrets will converge to zero Zinkevich _et al._ (2007). Therefore, CFVFP inherently adheres to Blackwell approachability.

Moreover, we can directly begin with the definition of Blackwell approachability and prove that FP fulfills this criterion. The convergence rate is \((L||T^{-})\), and the detailed proof can be found in B.1. The pseudocode for the CFVFP algorithm is provided in Appendix C.1.

Since CFVFP follows Blackwell approachability, it is fully compatible with various CFR variants, including MCCFR, CFR+, and different averaging schemes Brown and Sandholm (2019), resulting in a range of CFVFP variants. Consequently, we conducted a series of experiments to examine the convergence rates of these CFVFP variants. As detailed in Appendix B, the vanilla-weighted MCCFVFP, a combination of MCCFR and CFVFP, shows the fastest convergence among the variants. The pseudocode for the MCCFVFP algorithm is provided in Appendix C.2.

### Theoretical Analysis of MCCFVFP Algorithm

We discuss the advantages of the MCCFVFP algorithm in terms of the computing resources required per information set and the algorithm's pruning efficiency.

The MCCFVFP algorithm is highly efficient in conserving computing resources. For example, if an information set has \(|(I)|=x\) possible actions, MCCFVFP only requires \(2x+1\) additions to process this information set, whereas MCCFR needs \(6x-2\) additions and \(3x\) multiplications to complete the same calculation. This means that, for a single information set, MCCFVFP requires only about \(2/9\) of the computational time compared to MCCFR. Considering that a Blueprint training typically traverses at least 1 billion nodes, MCCFVFP offers significant advantages over MCCFR in terms of engineering implementation. For a detailed proof, refer to Appendix E.1.

Additionally, CFVFP is highly efficient in game tree pruning. Pruning is a common optimization technique used in all tree search algorithms. For instance, the Alpha-Beta algorithm in complete-information extensive-form games is a pruned version of the Min-Max algorithm, and it was a key factor in Deep Blue's success Hsu (2002). Naive pruning is the simplest pruning method in CFR. In naive pruning, if no player has a probability of reaching the current state \(s\) (\( i,_{_{t}}^{-i}(s)=0\)), the entire subtree at that state can be pruned for the current iteration without affecting regret calculations.

By analyzing the frequency of different node types in the game tree, we theoretically prove that CFVFP can significantly reduce the number of nodes that need to be processed. Compared to CFR, which must traverse all \((||)\) nodes, CFVFP only needs to traverse \(([]{||})\) nodes. This is especially beneficial in multiplayer games, where it greatly improves the efficiency of algorithm iterations. For detailed proof, please refer to Appendix E.2.

## 5 Experiments

### Description of the Game and Experimental Settings

We employed various game models, such as Kuhn-extension poker, Leduc-extension poker, the princess and monster game Lanctot _et al._ (2010), and Texas Hold'em, to evaluate the performance of different algorithms. These games are widely used benchmarks for comparing algorithmic convergence rates. Kuhn-extension poker, an expanded version of the classic Kuhn poker Kuhn (1950), features an increased card count of \(x\), a broader range of betting actions with \(y\) options, and up to \(z\) raising opportunities. Similarly, Leduc-extension poker is an advanced version of the original Leduc poker Shi and Littman (2002), allowing for flexible scaling. The princess and monster game represents a classic pursuit-evasion problem. Texas Hold'em, one of the most popular poker games in the world, was also included in our analysis.

In Kuhn, Leduc, and princess and monster games, we use a random distribution as the initial strategy to initialize all algorithms. In each iteration, the strategies and regrets of all players are updated simultaneously. In MCCFR, when \(R_{t}^{i,}=0\), the next stage strategy is set to a random pure strategy. In addition, the settings in our comparison experiment are consistent with those in previous groundbreaking works, including MCCFR Lanctot _et al._ (2010), DCFR+ Brown and Sandholm (2019), and PCFR Farina _et al._ (2021). For engineering implementation, any node with a probability less than \(10^{-20}\) during iteration will be pruned. Our Texas Hold'em experiments employed a variant of the multivalued state technique Brown _et al._ (2018), with an experimental setup closely mirroring that of Pluribus Brown and Sandholm (2019). The Texas Hold'em experiments were run on a 32-core, 128GB memory server, while the other experiments were conducted on a single core. It is important to note that all times mentioned below have been converted to reflect execution on a single CPU core.

For a comprehensive exposition of these games and additional experimental findings, please refer to Appendix F. In Appendix G, we compare time, number of iterations, and nodes touched as an indicator. Finally, we use nodes touched and time as an indicator.

### Experimental results

As shown in the Figure 2, we can demonstrate the core findings of our paper:

* In small-scale problems like vanilla Kuhn, algorithms such as DCFR, PCFR, and CFR+ may outperform MCCFR and MCCVFFP. However, as the game scale increases, the convergence speed of sampling-based algorithms (MCCFR/MCCVFFP) gradually surpasses that of full-traversal algorithms (DCFR/PCFR/CFR+).
* In our experiments, MCCVFFP consistently converged faster than MCCFR. While theoretically, our algorithm might be less effective than MCCFR in tangled games, the acceleration in its implementation and the tendency of large-scale games to be clear games have led to MCCVFFP outperforming MCCFR in all tested scenarios.

Specifically, when using the number of nodes touched as a metric, MCCVFFP demonstrates a slightly faster convergence rate compared to ES-MCCFR. However, in terms of processing the same number of nodes, MCCVFFP's computation time is only about 2/9 of MCCFR's (the underlying reasons are discussed in Section E.1). As a result, when factoring in the time required for game simulation, MCCVFFP achieves approximately 50% time savings compared to ES-MCCFR for similar levels of exploitability in these games.

As previously noted, algorithms such as CFR+, PCFR, and DCFR are not well-suited for large-scale games due to their reliance on full traversal, which is impractical in complex settings like Texas Hold'em. Therefore, in our Texas Hold'em experiments, we focused on comparing the performance of our algorithm specifically against the traditional MCCFR method, excluding these full-traversal algorithms from testing.

In our experiments, we used 2, 3, and 6-player Texas Hold'em game setups, where each player started with 25 Big Blinds (BB) in chips. We assumed that all players would check from the start of the game through to the river stage, simulating a conservative gameplay scenario to examine strategic strengths under limited betting dynamics.

In the two-player Texas Hold'em game, convergence speed can be directly measured using exploitability. The results are shown in Table 1. The public cards in the experiment were 5d2s9d2c7c, and after abstracting the hands, there were 69 hand strength ranks. This sub-game contains approximately 89k information sets, with exploitability measured in BB/100. Compared to MCCFR, MCCFVFP achieved a 20-30% faster convergence speed within the same time frame in the two-player Texas Hold'em game.

Figure 2: Convergence rates in Kuhn-extension, Leduc-extension, and princess-and-monster games are shown. In the first two rows, time is measured in milliseconds (ms). The last two rows reflect the same running time but with the horizontal axis representing the number of nodes touched during iteration. All experiments tested over an average of 30 rounds. The shaded areas indicate 90% confidence intervals for the trials.

In multiplayer situations, the exploitability cannot be directly calculated. Therefore, we use the method of mutual battles to measure the strength of different algorithms. As shown in Table 2, in the competition 1, MCCFYFP AI is randomly set as player \(i\), and all other players except player \(i\) are set as MCCFR AI. Define \(r_{1}\) as the rewards of the MCCFYFP AI player at competition 1. The second setting is exactly the opposite. Player \(i\) is randomly set as MCCFR AI, and the remaining players are set as MCCFYFP AI. Define \(r_{2}\) as the rewards of the MCCFR AI player at competition 2. By comparing \(r_{1}\) and \(r_{2}\), we can roughly compare the convergence speeds of different algorithms.

In these scenarios, the MCCFYFP AI, trained for the same duration, significantly outperforms the MCCFR algorithm across all aspects. The 30 to 40 seconds training experiment is particularly important, as it closely mirrors the setup of the Pluribus experiment. In the 6-player Pluribus experiment, the AI trained with MCCFR gained an average of 3.2 BB/100 per game, with a standard error of 1.5 BB/100. In our experiment, using community cards KsQsJs3h2h, MCCFYFP gained an average of 0.932 BB/100 compared to MCCFR--a substantial improvement, especially considering Pluribus gained 3.2 BB/100 against human players.

## 6 Conclusion

This paper introduces a novel method for solving large-scale incomplete information zero-sum games: Monte Carlo Counterfactual Value-Based Fictitious Play (MCCFYFP). By implementing the BR strategy in place of the regret-matching strategy, MCCFYFP achieved convergence speeds approximately 20% to 50% faster than the most advanced MCCFR variants.

In future research, we aim to evaluate the scalability of our method and its compatibility with different CFR variants, including those incorporating deep networks Brown _et al._ (2019). Furthermore, given MCCFYFP's efficacy in clear games, we envision developing a _warm start_ algorithm. This approach would initially use MCCFYFP to eliminate dominated strategies and then switch to algorithms that can further accelerate convergence in the later stages of training.

    Number \\ of \\ players \\  } &  Public \\ Cards \\  } &  Number of \\ of hand \\ ranks \\  } &  Training \\ time (s) \\  } &  Number of \\ time (s) \\  } &  Training \\ MCCFYFP \\  } &  Number of \\ MCCFR \\  } &  Battle result (BB/100) \\  } \\    & & & & 4.3 & 3.3 & 2.7 & 1.390 & -1.440 & \(\) 0.021 \\  &  &  &  & 40.0 & 30.5 & 25.0 & 0.268 & -0.277 & \(\) 0.015 \\  & & & & 432.0 & 301.0 & 246.0 & 0.014 & -0.014 & \(\) 0.003 \\   &  &  &  &  & 4.2 & 3.3 & 2.9 & 0.858 & -0.922 & \(\) 0.026 \\  & & & & 40.0 & 29.9 & 25.6 & 0.182 & -0.201 & \(\) 0.015 \\  & & & & 410.6 & 287.0 & 244.0 & 0.013 & -0.020 & \(\) 0.004 \\   &  &  &  & 3.6 & 2.1 & 1.7 & 5.380 & -5.950 & \(\) 0.071 \\  & & & & 35.2 & 18.8 & 12.6 & 0.515 & -0.454 & \(\) 0.010 \\   & & & & 374.4 & 185.0 & 144.0 & 0.096 & -0.098 & \(\) 0.002 \\    &  &  &  & 3.4 & 2.0 & 1.6 & 7.930 & -8.540 & \(\)0.092 \\   & & & & 32.0 & 17.7 & 13.6 & 0.935 & -0.902 & \(\) 0.013 \\   & & & & 320.0 & 172.0 & 131.0 & 0.035 & -0.023 & \(\) 0.004 \\   

Table 2: The results of different AIs competing against each other in multiplayer games