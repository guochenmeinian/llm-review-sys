# Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors

Pengchong Hu  Zhizhong Han

Machine Perception Lab, Wayne State University, Detroit, USA

pchu@wayne.edu h312h@wayne.edu

###### Abstract

Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Please see our project page for code and data at https://machineperceptionlab.github.io/Attentive_DF_Prior/.

## 1 Introduction

3D reconstruction from multi-view images has been studied for decades. Traditional methods like Structure from Motion (SfM) , Multi-View Stereo (MVS) , and Simultaneous Localization and Mapping (SLAM)  estimate 3D structures as point clouds by maximizing multi-view color consistency. Current methods  mainly adopt data-driven strategies to learn depth estimation priors from large scale benchmarks using deep learning models. However, the reconstructed 3D point clouds lack geometry details, due to their discrete representation essence, which makes them not friendly to downstream applications.

More recent methods use implicit functions such as signed distance functions (SDFs)  or occupancy functions  as continuous representations of 3D shapes and scenes. Using volume rendering, we can learn neural implicit functions by comparing their 2D renderings with multi-view ground truth including color , depth  or normal  maps. Although the supervision of using depth images as rendering target can provide detailed structure information and guide importance sampling along rays , both the missing depth at holes and the unawareness of occluded structures make it hard to significantly improve the reconstruction accuracy. Hence, how to moreeffectively leverage depth supervision for geometry inference through volume rendering is still a challenge.

To overcome this challenge, we introduce to learn neural implicit through volume rendering with an attentive depth fusion prior. Our key idea is to provide neural networks the flexibility of choosing geometric clues, i.e., the geometry that has been learned and the Truncated Signed Distance Function (TSDF) fused from all available depth images, and combining them into neural implicit for volume rendering. We regard the TSDF as a prior sense of the scene, and enable neural networks to directly use it as a more accurate representation. The TSDF enables accessing the missing depth at holes on one depth image and the occluded structures that are invisible from the current view, which remedies the demerits of using depth ground truth to supervise rendering. To achieve this, we introduce an attention mechanism to allow neural networks to balance the contributions of currently learned geometry and the TSDF in the neural implicit, which leads the TSDF into an attentive depth fusion prior. Our method works with either known camera poses or camera tracking in the context of SLAM, where our prior could be either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene. We evaluate our performance on benchmarks containing synthetic and real-world scans, and report our superiority over the latest methods with known or estimated camera poses. Our contributions are listed below.

1. We present a novel volume rendering framework to learn neural implicit representations from RGBD images. We enable neural networks to use either currently learned geometry or the one from depth fusion in volume rendering, which leads to a novel attentive depth fusion prior for learning neural implicit functions inheriting the merits of both the TSDF and the inference.
2. We introduce a novel attention mechanism and a neural network architecture to learn attention weights for the attentive depth fusion prior in neural rendering with either known camera poses or camera pose tracking in SLAM.
3. We report the state-of-the-art performance in surface reconstruction and camera tracking on benchmarks containing synthetic and real-world scans.

## 2 Related Work

Learning 3D implicit functions using neural networks has made huge progress [61; 62; 46; 51; 88; 78; 73; 76; 16; 56; 34; 25; 11; 93; 42; 44; 9; 43; 4; 33; 5; 19; 26; 47; 20; 70; 12; 48; 55; 17; 27; 79; 35; 32; 21; 7; 58; 86; 52]. We can learn neural implicit functions from 3D ground truth [23; 8; 54; 45; 68; 39; 69], 3D point clouds [94; 41; 15; 1; 92; 2; 10] or multi-view images [46; 14; 51; 77; 88; 73; 76; 16]. We briefly review methods with multi-view supervision below.

### Multi-view Stereo

Classic multi-view stereo (MVS) [61; 62] employs multi-view photo consistency to estimate depth maps from multiple RGB images. They rely on matching key points on different views, and are limited by large viewpoint variations and complex illumination. Without color, space carving  is also an effective way of reconstructing 3D structure as voxel grids.

Recent methods employ data driven strategy to train neural networks to predict depth maps from either depth supervision  or multi-view photo consistency .

### Neural Implicit from Multi-view Supervision

Early works leverage various differentiable renderers [63; 38; 26; 89; 37; 80; 50; 36; 67] to render the learned implicit functions into images, so that we can measure the error between rendered images and ground truth images. These methods usually require masks to highlight the object, and use surface rendering to infer the geometry, which limits their applications in scenes. Similarly, DVR  and IDR  predict the radiance near surfaces.

With volume rendering, NeRF  and its variations [53; 47; 57; 60; 97; 3; 75; 6; 98; 67] model geometry and color together. They can generate plausible images from novel viewpoints, and do not need masks during rendering procedure. UNISURF  and NeuS  learn occupancy functions and SDFs by rendering them with colors using revised rendering equations. Following methodsimprove accuracy of implicit functions using additional priors or losses related to depth [88; 3; 97], normals [88; 76; 16], and multi-view consistency .

Depth images are also helpful to improve the inference accuracy. Depth information can guide sampling along rays  or provide rendering supervision [88; 3; 97; 96; 24; 31; 82; 98], which helps neural networks to estimate surfaces.

### Neural Implicit with SLAM

Given RGBD images, more recent methods [91; 81; 72; 59] employ neural implicit representations in SLAM. iMAP  shows that an MLP can serve as the only scene representation in a realtime SLAM system. NICE-SLAM  introduces a hierarchical scene representation to reconstruct large scenes with more details. NICER-SLAM  uses easy-to-obtain monocular geometric cues without requiring depth supervision. Co-SLAM  jointly uses coordinate and sparse parametric encodings to learn neural implicit functions. Segmentation priors [29; 18] show their potentials to improve the performance of SLAM systems. With segmentation priors, vMAP  represents each object in the scene as a neural implicit in a SLAM system.

Instead of using depth ground truth to only supervise rendering, which contains incomplete depth and unawareness of occlusion, we allow our neural network to directly use depth fusion priors as a part of neural implicit, and determine where and how to use depth priors along with learned geometry.

## 3 Method

**Overview.** We present an overview of our method in Fig. 1. Given RGBD images \(\{I_{j},D_{j}\}_{j=1}^{J}\) from \(J\) view angles, where \(I_{j}\) and \(D_{j}\) denote the RGB and depth images, available either all together or in a streaming way, we aim to infer the geometry of the scene as an occupancy function \(f\) which predicts the occupancy \(f()\) at arbitrary locations \(=(x,y,z)\). Our method works with camera poses \(\{M_{j}\}\) that are either known or estimated by our camera tracking method in the context of SLAM.

We learn the occupancy function through volume rendering using the RGBD images \(\{I_{j},D_{j}\}_{j=1}^{J}\) as supervision. We render \(f\) with a color function \(c\) which predicts an RGB color \(c()\) at locations \(\) into an RGB image \(I_{j}^{}\) and a depth image \(D_{j}^{}\), which are optimized to minimize their rendering errors to the supervision \(\{I_{j},D_{j}\}\).

We start from shooting rays \(\{V_{k}\}\) from current view \(I_{j}\), and sample queries \(\) along each ray \(V_{k}\). For each query \(\), we employ learnable feature grids \(G_{l}\), \(G_{h}\), and \(G_{c}\) covering the scene to interpolate its hierarchical geometry features \(_{l}\), \(_{h}\), and a color feature \(_{c}\) by trilinear interpolation. Each feature is further transformed into an occupancy probability or RGB color by their corresponding decoders \(f_{l}\), \(f_{h}\), and \(f_{c}\). For queries \(\) inside the bandwidth of a TSDF grid \(G_{s}\) fused from available depth images \(\{D_{i}\}\), we leverage its interpolation from \(G_{s}\) as a prior of coarse occupancy estimation. The prior is attentive by a neural function \(f_{a}\) which determines the occupancy function \(f()\) by combining currently learned geometry and the coarse estimation using the learned attention weights from \(f_{a}\).

Lastly, we use the occupancy function \(f()\) and the color function \(c()\) to render \(I^{}\) and \(D^{}\) through volume rendering. With a learned \(f\), we run the marching cubes  to reconstruct a surface.

Figure 1: Overview of our method.

**Depth Fusion and Bandwidth Awareness.** With known or estimated camera poses \(\{M_{j}\}\), we get a TSDF by fusing depth images \(\{D_{j}\}\) that are available either all together or in a streaming way. Our method can work with a TSDF that describes either a whole scene or just a part of the scene, and we will report the performance with different settings in ablation studies. We use the TSDF to provide a coarse occupancy estimation which removes the limit of the missing depth or the unawareness of occlusion on single depth supervision. The TSDF is a grid which predicts signed distances at any locations inside it through trilinear interpolation. The predicted signed distances are truncated with a threshold \(\) into a range of \([-1,1]\) which covers a band area on both sides of the surface in Fig. 1.

Since the TSDF predicts signed distances for queries within the bandwidth with higher confidence than the ones outside the bandwidth, we only use the depth fusion prior within the bandwidth. This leads to different ways of learning geometries, as shown in Fig. 1. Specifically, for queries \(\) within the bandwidth, we model the low-frequency surfaces using a low resolution feature grid \(G_{l}\), learn the high-frequency details as a complementation using a high resolution feature grid \(G_{h}\), and let our attention mechanism to determine how to use the depth fusion prior from the TSDF \(G_{s}\). Instead, we only use the low resolution feature grid \(G_{l}\) to model densities outside the bandwidth during volume rendering. Regarding color modeling, we use the feature grid \(G_{c}\) with the same size for interpolating color of queries over the scene.

**Feature Interpolation.** For queries \(\) sampled along rays, we use the trilinear interpolation to obtain its features \(_{l}\), \(_{h}\), and \(_{c}\) from learnable \(G_{l}\), \(G_{h}\), \(G_{c}\) and occupancy estimation \(o_{s}\) from \(G_{s}\). Both feature grids and the TSDF grid cover the whole scene and use different resolutions, where learnable feature vectors are associated with vertices on each feature grid. Signed distances at vertices on \(G_{s}\) may change if we incrementally fuse depth images in the context of SLAM.

**Occupancy Prediction Priors.** Similar to NICE-SLAM , we use pre-trained decoders \(f_{l}\) and \(f_{h}\) to predict low frequency occupancies \(o_{l}\) and high frequency ones \(o_{h}\) from the interpolated features \(_{l}\) and \(_{h}\), respectively. We use \(f_{l}\) and \(f_{h}\) as an MLP decoder in ConvONet  respectively, and minimize the binary cross-entropy loss to fit the ground truth. After pre-training, we fix the parameters in \(f_{l}\) and \(f_{h}\), and use them to predict occupancies below,

\[o_{l}=f_{l}(,_{l}),\ o_{h}=f_{h}(,_{h}),\ o_{lh}=o_{l }+o_{h},\] (1)

where we enhance \(f_{h}\) by concatenating \(f_{l}\) as \(f_{h}[f_{h},f_{l}]\) and denote \(o_{lh}\) as the occupancy predicted at query \(\) by the learned geometry.

**Color Predictions.** Similarly, we use the interpolated feature \(_{c}\) and an MLP decoder \(f_{c}\) to predict color at query \(\), i.e., \(c()=f_{c}(,_{c})\). We predict color to render RGB images for geometry inference or camera tracking in SLAM. The decoder is parameterized by parameters \(\) which are optimized with other learnable parameters in the feature grids.

**Attentive Depth Fusion Prior.** We introduce an attention mechanism to leverage the depth fusion prior. We use a deep neural network to learn attention weights to allow networks to determine how to use the prior in volume rendering.

As shown in Fig. 1, we interpolate a signed distance \(s[-1,1]\) at query \(\) from the TSDF \(G_{s}\) using trilinear interpolation, where \(G_{s}\) is fused from available depth images. We formulate this interpolation as \(s=f_{s}()[-1,1]\). We normalize \(s\) into an occupancy \(o_{s}\), and regard \(o_{s}\) as the occupancy predicted at query \(\) by the depth fusion prior.

For query \(\) within the bandwidth, our attention mechanism trains an MLP \(f_{a}\) to learn attention weights \(\) and \(\) to aggregate the occupancy \(o_{lh}\) predicted by the learned geometry and the occupancy \(o_{s}\) predicted by the depth fusion prior, which leads the TSDF to an attentive depth fusion prior. Hence, our attention mechanism can be formulated as,

\[[,]=f_{a}(o_{lh},o_{s}),+=1.\] (2)

We implement \(f_{a}\) using an MLP with 6 layers. The reason why we do not use a complex network like Transformer is that we want to justify the effectiveness of our idea without taking too much credit from complex neural networks. Regarding the design, we do not use coordinates or positional encoding as a part of input to avoid noisy artifacts. Moreover, we leverage a Softmax normalization layer to achieve \(+=1\), and we do not predict only one parameter \(\) and use \(1-\) as the second weight, which also degenerates the performance. We will justify these alternatives in experiments.

For query \(\) outside the bandwidth, we predict the occupancy using the feature \(_{l}\) interpolated from the low frequency grid \(G_{l}\) and the decoder \(f_{l}\) to describe the relatively simpler geometry. In summary, we eventually formulate our occupancy function \(f\) as a piecewise function below,

\[f()= o_{lh}+ o_{s},&f_{s}()( -1,1)\\ o_{l},&f_{s}()=1-1\] (3)

**Volume Rendering.** We render the color function \(c\) and occupancy function \(f\) into RGB \(I^{}\) and depth \(D^{}\) images to compare with the RGBD supervision \(\{I,D\}\).

With camera poses \(M_{j}\), we shoot a ray \(V_{k}\) from view \(I_{j}\). \(V_{k}\) starts from the camera origin \(\) and points a direction of \(\). We sample \(N\) points along the ray \(V_{k}\) using stratified sampling and uniformly sampling near the depth, where each point is sampled at \(_{n}=+d_{n}\) and \(d_{n}\) corresponds to the depth value of \(_{n}\) on the ray. Following UNISURF , we transform occupancies \(f(_{n})\) into weights \(w_{n}\) which is used for color and depth accumulation along the ray \(V_{k}\) in volume rendering,

\[w_{n}=f(_{n})_{n^{}=1}^{n-1}(1-f(_{n^{}})),\;{I( k)}^{}=_{n^{}=1}^{N}w_{n^{}} c(_{n^{}}), \;D(k)^{}=_{n^{}=1}^{N}w_{n^{}} d_{n^{}}.\] (4)

**Loss.** With known camera pose \(M_{j}\), we render the scene into the color and depth images at randomly sampled \(K\) pixels on the \(j\)-th view, and optimize parameters by minimizing the rendering errors,

\[L_{I}=_{j,k=1}^{J,K}||I_{j}(k)-I^{}_{j}(k)||_{1},L_{D}= _{j,k=1}^{J,K}||D_{j}(k)-D^{}_{j}(k)||_{1},_{,G_{l},G_{h},G_{e}}L_{D}+ L_{I}.\] (5)

**In the Context of SLAM.** We can jointly do camera tracking and learning neural implicit from streaming RGBD images. To achieve this, we regard the camera extrinsic matrix \(M_{j}\) as learnable parameters, and optimize them by minimizing our rendering errors. Here, we follow  to weight the depth rendering loss to decrease the importance at pixels with large depth variance along the ray,

\[_{M_{j}}_{j,k=1}^{J,K}_{j}(k))}||D _{j}(k)-D^{}_{j}(k)||_{1}+_{1}L_{I},\] (6)

where \(Var(D^{}_{j}(k))=_{n=1}^{N}w_{n}(D^{}_{j}(k)-d_{n})^{2}\). Moreover, with streaming RGBD images, we incrementally fuse the most current depth image into TSDF \(G_{s}\). Specifically, the incremental fusion includes a pre-fusion and an after-fusion stage. The pre-fusion aims to use a camera pose coarsely estimated by a traditional method to fuse a depth image onto a current TSDF to calculate a rendering error for a more accurate pose estimation at current frame. The after-fusion stage will refuse the depth image onto the current TSDF for camera tracking at the next frame. Please refer to our supplementary materials for more details.

We do tracking and mapping iteratively. For mapping procedure, we render \(E\) frames each time and back propagate rendering errors to update parameters. \(E\) frames include the current frame and \(E-1\) key frames that have overlaps with the current frame. For simplicity, we merely maintain a key frame list by adding incoming frames with an interval of \(50\) frames for fair comparisons.

**Details.** The optimization is performed at three stages, which makes optimization converge better. We first minimize \(L_{D}\) by optimizing low frequency feature grid \(G_{l}\), and then both low and high frequency feature grid \(G_{l}\) and \(G_{h}\), and finally minimize Eq 5 by optimizing \(G_{l}\), \(G_{h}\), and \(G_{c}\). Our bandwidth from the TSDF \(G_{s}\) covers \(5\) voxels on both sides of the surface. We shoot \(K=1000\) or \(5000\) rays for reconstruction or tracking from each view, and render \(E=5\) or \(10\) frames each time for fair comparison with other methods. We set \(=0.2\), \(_{1}=0.5\) in loss functions. We sample \(N=48\) points along each ray for rendering. More implementation details can be found in our supplementary materials.

## 4 Experiments and Analysis

### Experimental Setup

**Datasets.** We report evaluations on both synthetic datasets and real scans including Replica  and ScanNet . For fair comparisons, we report results on the same scenes from Replica and ScanNetas the latest methods. Specifically, we report comparisons on all \(8\) scenes in Replica. As for ScanNet, we report comparison on scene 50, 84, 580, and 616 used by MonoSDF , and also scene 59, 106, 169, and 207 used by NICE-SLAM . We mainly report average results over the dataset, please refer to our supplementary materials for results on each scene.

**Metrics.** With the learned occupancy function \(f\), we reconstruct the surface of a scene by extracting the zero level set of \(f\) using the marching cubes algorithm . Following previous studies [97; 22], we use depth L1 [cm], accuracy [cm], completion [cm], and completion ratio [!5cm\(\%\)] as metrics to evaluate reconstruction accuracy on Replica. Additionally, we report Chamfer distance (L1), precision, recall, and F-score with a threshold of \(0.05\) to evaluate reconstruction accuracy on ScanNet. To evaluate the accuracy in camera tracking, we use ATE RMSE  as a metric. We follow the same parameter setting in these metrics as .

### Evaluations

**Evaluations on Replica.** We use the same set of RGBD images as [66; 96]. We report evaluations in surface reconstruction and camera tracking in Tab. 1, Tab. 2 and Tab. 3, respectively.

We jointly optimize camera poses and learn geometry in the context of SLAM. The depth fusion prior \(G_{s}\) incrementally fuses depth images using estimated camera poses. We report the accuracy of reconstruction from \(G_{s}\) as "TSDF". Compared to this baseline, we can see that our method improves the reconstruction using the geometry learned through volume rendering and occupancy prediction priors. We visualize the advantage of learning geometry in Fig. 2. Note that the holes in TSDF are caused by the absence of RGBD scans. We can see that our neural implicit keeps the correct structure in TSDF and plausibly completes the missing structures in TSDF. We further visualize the attention weights \(\) learned for depth fusion in Fig. 3. We visualize the cross sections on \(4\) scenes, where \(\) learned in bandwidth are shown in color red. Generally, the neural implicit mostly focuses more on the depth fusion priors in areas where TSDF is incomplete. In the area where TSDF is complete, the network also pays some attention to the inferred occupancy because the occupancy interpolated from TSDF may not be accurate, especially on the most front of surfaces in Fig. 3.

Moreover, our method outperforms the latest implicit-based SLAM methods like NICE-SLAM  and NICER-SLAM . We present visual comparisons in Fig. 6, where our method produces more accurate and compact geometry. For method using GT camera poses like vMAP  and MonoSDF , we achieve much better performance as shown by "Ours*" in Tab. 1 and "Ours" in Tab. 2. Although we require the absolute dense depth maps, compared to MonoSDF  that can work with

    & COLMAP  & TSDF  & iMAP  & DI  & NICE  & Vux  & DROD  & NICER  & Ours & vMAP  & Ours* \\ 
**Depth.1**\(\) & - & 6.56 & 7.64 & 23.33 & 3.53 & - & - & - & **3.01** & - & **2.60** \\
**Acc. \(\)** & 8.69 & **1.56** & 6.95 & 19.40 & 2.85 & 2.67 & 5.50 & 3.65 & 2.77 & 3.20 & **2.59** \\
**Comp. \(\)** & 12.12 & 3.33 & 5.33 & 10.19 & 3.00 & 4.55 & 12.29 & 4.16 & **2.45** & 2.39 & **2.28** \\
**Ratio \(\)** & 67.62 & 87.61 & 66.60 & 72.96 & 89.33 & 86.59 & 63.62 & 79.37 & **92.79** & 92.59 & **93.38** \\   

Table 1: Reconstruction Comparisons on Replica.

Figure 3: Visualization of attention on depth fusion.

Figure 2: Merits of attentive depth fusion prior.

scaled depth maps, our method only can use the views in front of the current time step, where MonoSDF  can utilize all views during the whole training process.

In camera tracking, our results in Tab. 3 achieve the best in average. We compare the estimated trajectories on scene off-4 in Fig. 4 (c). The comparison shows that our attentive depth fusion prior can also improve the camera tracking performance through volume rendering.

**Evaluations on ScanNet.** We further evaluate our method on ScanNet. For surface reconstruction, we compare with the latest methods for learning neural implicit from multiview images. We report both their results and ours with GT camera poses, where we also fuse every \(10\) depth images into the TSDF \(G_{s}\) and render every \(10\) frames for fair comparisons. Numerical comparisons in Tab. 4 show that our method achieves the best performance in terms of all metrics, where we use the culling strategy introduced in MonoSDF  to clean the reconstructed mesh. We highlight our significant improvements in visual comparisons in Fig. 7. We see that our method can reconstruct sharper, more compact and detailed surfaces than other methods. We detail our comparisons on every scene with the top results reported by GO-Surf  and NICE-SLAM  in Tab. 5 and Tab. 6. The comparisons in Tab. 5 show that our method achieves higher reconstruction accuracy while GO-Surf produces more complete surfaces. We present visual comparisons with error maps in Fig. 5.

In camera tracking, we compare with the latest methods. We incrementally fuse the most current depth frames into TSDF \(G_{s}\) which is used for attentive depth fusion priors in volume rendering. Numerical comparisons in Tab. 7 show that our results are better on \(2\) out of \(4\) scenes and achieve the best in average. Tracking trajectory comparisons in Fig. 4 (a) and (b) also show our superiority.

    &  &  \\  & Normal C.\(\) & Chamfer-\(L_{1}\) & F-score \(\) & Normal C.\(\) & Chamfer-\(L_{1}\) & F-score \(\) \\  MonoSDF  & 90.56 & 4.26 & 76.42 & **91.80** & 3.59 & 85.67 \\  Ours & **90.69** & **2.43** & **92.47** & 91.05 & **2.73** & **90.52** \\   

Table 2: Reconstruction Comparison with MonoSDF on Replica.

Figure 4: Visual comparisons in camera tracking.

Figure 5: Visual comparison of error maps (Red: Large) in surface reconstructions on Replica.

Figure 6: Visual comparisons of error maps (Red: Large) in surface reconstructions on Replica.

    & rm-0 & rm-1 & rm-2 & off-0 & off-1 & off-2 & off-3 & off-4 & Avg. \\  NICE-SLAM  & 1.69 & 2.04 & 1.55 & **0.99** & **0.90** & **1.39** & 3.97 & 3.08 & 1.95 \\ NICER-SLAM  & **1.36** & 1.60 & **1.14** & 2.12 & 3.23 & 2.12 & **1.42** & 2.01 & 1.88 \\  Ours & 1.39 & **1.55** & 2.60 & 1.09 & 1.23 & 1.61 & 3.61 & **1.42** & **1.81** \\   

Table 3: Camera Tracking Comparisons (ATE RMSE) on Replica.

### Ablation Studies

We report ablation studies to justify the effectiveness of modules in our method on Replica. We use estimated camera poses in ablation studies.

**Effect of Depth Fusion.** We first explore the effect of different ways of depth fusion on the performance. According to whether we use GT camera poses or incremental fusion, we try \(4\) alternatives with \(G_{s}\) obtained by fusing all depth images at the very beginning using GT camera poses (Offline) or fusing the current depth image incrementally (Online), and using tracking to estimate camera poses (Tracking) or GT camera poses (GT) in Tab. 8. The comparisons show that our method can work well with either GT or estimated camera poses and fusing depth either all together or incrementally in a streaming way. Additional conclusion includes that GT camera poses in either depth fusion and rendering do improve the reconstruction accuracy, and the structure fused from more recent frames is more important than the whole structure fused from all depth images.

    & &  &  \\ Scene ID & 0050 & 0084 & 0580 & 0616 & Avg. & 0050 & 0084 & 0580 & 0616 & Avg. \\  Acc \(\) & **0.030** & **0.031** & **0.032** & **0.026** & **0.030** & **0.030** & 0.039 & 0.041 & **0.026** & 0.034 \\ Comp \(\) & 0.053 & 0.020 & **0.031** & 0.076 & 0.045 & **0.043** & **0.014** & 0.035 & **0.063** & **0.039** \\ Chamfer-\(L_{1}\) & 0.041 & **0.025** & **0.032** & 0.051 & **0.037** & **0.037** & 0.026 & 0.038 & **0.045** & **0.037** \\   

Table 6: Reconstruction Comparison with NICE-SLAM on ScanNet.

    & Acc \(\) & Comp \(\) & Chamfer-\(L_{1}\) & Prec \(\) & Recall \(\) & F-score \(\) \\  COLMAP  & 0.047 & 0.235 & 0.141 & 0.711 & 0.441 & 0.537 \\ UNISURF  & 0.554 & 0.164 & 0.359 & 0.212 & 0.362 & 0.267 \\ NeuS  & 0.179 & 0.208 & 0.194 & 0.313 & 0.275 & 0.291 \\ VolSDF  & 0.414 & 0.120 & 0.267 & 0.321 & 0.394 & 0.346 \\ Manhattan-SDF  & 0.072 & 0.068 & 0.070 & 0.621 & 0.586 & 0.602 \\ NeuRIS  & 0.050 & 0.049 & 0.050 & 0.717 & 0.669 & 0.692 \\ MonoSDF  & 0.035 & 0.048 & 0.042 & 0.799 & 0.681 & 0.733 \\  Ours & **0.034** & **0.039** & **0.037** & **0.913** & **0.894** & **0.902** \\   

Table 4: Reconstruction Comparisons on ScanNet.

    & &  &  \\ Scene ID & 0050 & 0084 & 0580 & 0616 & Avg. & 0050 & 0084 & 0580 & 0616 & Avg. \\  Acc \(\) & 0.056 & 0.073 & 0.057 & **0.026** & 0.053 & **0.030** & **0.039** & **0.041** & **0.026** & **0.034** \\ Comp \(\) & **0.024** & 0.017 & **0.024** & **0.023** & **0.022** & 0.043 & **0.014** & 0.035 & 0.063 & 0.039 \\ Chamfer-\(L_{1}\) & 0.040 & 0.045 & 0.040 & **0.025** & 0.038 & **0.037** & **0.026** & **0.038** & 0.045 & **0.037** \\   

Table 5: Reconstruction Comparison with GO-Surf on ScanNet.

Figure 7: Visual comparisons in surface reconstructions on ScanNet.

**Effect of Attention.** We explore the effect of attention on the performance in Tab. 9. First of all, we remove the attention mechanism, and observe a severe degeneration in accuracy, which indicates that the attention plays a big role in directly applying depth fusion priors in neural implicit. Then, we try to apply attention different features including low frequency \(_{l}\), high frequency \(_{h}\), and both of them. The comparisons show that adding details from depth fusion priors onto low frequency surfaces does not improve the performance. More analysis on attention mechanism can be found in Fig. 9.

**Attention Alternatives.** Our preliminary results show that using softmax normalization layer achieves better performance than other alternatives. Since we merely need two attention weights \(\) and \(\) to combine the learned geometry and the depth fusion prior, one alternative is to use a sigmoid function to predict one attention weight and use its difference to 1 as another attention weight. However, sigmoid can not effectively take advantage of the depth fusion prior. We also try to use coordinates as an input, which pushes the network to learn spatial sensitive attentions. While the reconstructed surfaces turn out to be noisy in Fig. 8. This indicates that our attention learned a general attentive pattern for all locations in the scene. We also report the numerical comparisons in Tab. 10. Please refer to our supplementary materials for more results related to attention alternatives.

**Effect of Bandwidth.** We further study the effect of bandwidth on the performance in Tab. 11. We try to use attentive depth fusion priors everywhere in the scene with no bandwidth. The degenerated results indicate that the truncated area outside the bandwidth does not provide useful structures to improve the performance. Moreover, wider bandwidth may cause more artifacts caused by the calculation of the TSDF \(G_{s}\) while narrower bandwidth brings more incomplete structures, neither of which improves the performance. Instead of using low frequency geometry outside the bandwidth, we also try to use high frequency surfaces. We can see that low frequency geometry is more suitable to describe the scene outside the bandwidth.

    & Coordinates & Sigmoid & Softmax (Ours) \\ 
**DepthL1 \(\)** & 1.81 & 1.96 & **1.44** \\
**Acc. \(\)** & 2.66 & 2.86 & **2.54** \\
**Comp. \(\)** & 2.74 & 2.66 & **2.41** \\
**Ratio \(\)** & 93.13 & **93.27** & 93.22 \\   

Table 10: Ablation Studies on Attention Alternatives.

   Scene ID & 0059 & 0106 & 0169 & 0207 & Avg. \\  iMAP  & 32.06 & 17.50 & 70.51 & 11.91 & 33.00 \\ DI  & 128.00 & 18.50 & 75.80 & 100.19 & 80.62 \\ NICE  & 12.25 & 8.09 & 10.28 & **5.59** & 9.05 \\ CO  & 12.29 & 9.57 & **6.62** & 7.13 & **8.90** \\  Ours & **10.50** & **7.48** & 9.31 & 5.67 & **8.24** \\   

Table 7: Camera Tracking Comparisons (ATE RMSE) on ScanNet.

    &  &  &  &  \\ 
**DepthL1 \(\)** & 1.86 & 2.75 & 2.96 & **1.44** \\
**Acc. \(\)** & 2.69 & 2.96 & 3.85 & **2.54** \\
**Comp. \(\)** & 2.81 & 3.14 & 2.91 & **2.41** \\
**Ratio \(\)** & 91.46 & 89.73 & **93.63** & 93.22 \\   

Table 9: Ablation Studies on Attention.

Figure 8: Visual comparison of error maps with different attention alternatives (Red: Large).

    &  &  &  &  \\ 
**DepthL1 \(\)** & 1.86 & 2.75 & 2.96 & **1.44** \\
**Acc. \(\)** & 2.69 & 2.96 & 3.85 & **2.54** \\
**Comp. \(\)** & 2.81 & 3.14 & 2.91 & **2.41** \\
**Ratio \(\)** & 91.46 & 89.73 & **93.63** & 93.22 \\   

Table 9: Ablation Studies on Attention.

**Effect of Prediction Priors.** Prediction priors from decoders \(f_{l}\) and \(f_{h}\) are also important for accuracy improvements. Instead of using fixed parameters in these decoders, we try to train \(f_{l}\) (Fix \(f_{h}\)) or \(f_{h}\) (Fix \(f_{l}\)) with other parameters. Numerical comparisons in Tab. 12 show that training \(f_{l}\) or \(f_{h}\) cannot improve the performance.

**More Analysis on How Attention Works.** Additionally, we do a visual analysis on how attention works in Fig. 9. We sample points on the GT mesh, and get the attention weights in Fig. 9 (a). At each point, we show its distance to the mesh from the TSDF in Fig. 9 (b) and the mesh from the inferred occupancy in Fig. 9 (c), respectively. Fig. 9 (d) indicates where the former is smaller than the latter. The high correlation between Fig. 9 (a) and Fig. 9 (d) indicates that the attention network focuses more on the occupancy producing smaller errors to the GT surface. Instead, the red in Fig. 9 (e) indicates where the interpolated occupancy is larger than the inferred occupancy is not correlated to the one in Fig. 9 (a). The uncorrelation indicates that the attention network does not always focus on the larger occupancy input but the one with smaller errors, even without reconstructing surfaces.

## 5 Conclusion

We propose to learn neural implicit through volume rendering with attentive depth fusion priors. Our novel prior alleviates the incomplete depth at holes and the unawareness of occluded structures when using depth images as supervision in volume rendering. We also effectively enable neural networks to determine how much depth fusion prior can be directly used in the neural implicit. Our method can work well with depth fusion from either all depth images together or the ones available in a streaming way, using either known or estimated camera poses. To this end, our novel attention successfully learns how to combine the learned geometry and the depth fusion prior into the neural implicit for more accurate geometry representations. The ablation studies justify the effectiveness of our modules and training strategy. Our experiments on benchmarks with synthetic and real scans show that our method learns more accurate geometry and camera poses than the latest neural implicit methods.