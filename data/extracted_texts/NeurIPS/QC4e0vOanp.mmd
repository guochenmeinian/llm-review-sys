# Leveraging partial stragglers within gradient coding

Aditya Ramamoorthy Ruoyu Meng Vrinda S. Girimaji

Department of Electrical and Computer Engineering

Iowa State University

Ames, IA 50010.

{adityar, rmeng, vrindasg}@iastate.edu

###### Abstract

Within distributed learning, workers typically compute gradients on their assigned dataset chunks and send them to the parameter server (PS), which aggregates them to compute either an exact or approximate version of \( L\) (gradient of the loss function \(L\)). However, in large-scale clusters, many workers are slower than their promised speed or even failure-prone. A gradient coding solution introduces redundancy within the assignment of chunks to the workers and uses coding theoretic ideas to allow the PS to recover \( L\) (exactly or approximately), even in the presence of stragglers. Unfortunately, most existing gradient coding protocols are inefficient from a computation perspective as they coarsely classify workers as operational or failed; the potentially valuable work performed by slow workers (partial stragglers) is ignored. In this work, we present novel gradient coding protocols that judiciously leverage the work performed by partial stragglers. Our protocols are efficient from a computation and communication perspective and numerically stable. For an important class of chunk assignments, we present efficient algorithms for optimizing the relative ordering of chunks within the workers; this ordering affects the overall execution time. For exact gradient reconstruction, our protocol is around \(2\) faster than the original class of protocols and for approximate gradient reconstruction, the mean-squared-error of our reconstructed gradient is several orders of magnitude better.

## 1 Introduction

Large scale distributed learning is the workhorse of modern day machine learning (ML) algorithms. The sheer size of the data and the corresponding computation needs, necessitate the usage of huge clusters for the purpose of parameter fitting in most ML problems of practical interest: deep learning , low-rank matrix completion  etc. A typical scenario consists of a dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\) of \(\) data points, where \(_{i}\)'s and \(y_{i}\)'s are the features and labels respectively. We wish to minimize a loss function \(L=}_{i=1}^{}l(_{i},y_{i},)\) with respect to \(^{d}\) (\(\): parameter vector, \(l\): prediction error). When \(\) is large, we can perform the learning task in a distributed manner .

**Background:** We partition \(\) into \(N\) equal-sized chunks denoted \(_{i},i[N]\) (\([n]\) denotes the set \(\{1,,n\}\)), where a chunk is a subset of the data points and distinct chunks are disjoint. Within each chunk, the assignment of the data points to the workers is identical. Suppose that there are \(m\) workers \(W_{1},,W_{m}\) and a parameter server (PS). We distribute the chunks to the different workers and let them compute the gradients on the data points assigned to them. The PS coordinates the training by aggregating the (partial) gradients from the workers and transmitting an updated parameter vector to the workers at each iteration. In the "baseline" scheme, \(N=m\), \(W_{j}\) is assigned \(_{j}\) and it computes \(_{i_{j}} l(_{i},y_{i},_{t})\) (a vector of length-\(d\)) and sends it to the PS. Using these, the PS computes the desired gradient \( L=}_{i=1}^{} l(_{i},y_{ i},_{t})\) and the updated parameter \(_{t+1}\)thereafter. Unfortunately, in many large scale clusters, workers are often slower than their promised speed or even prone to failure. This is especially true in cloud platforms, where the load often fluctuates depending on system load and spot instance pricing  explicitly builds in the possibility of job preemption. To address these issues, gradient coding (GC) introduced in  incorporates redundancy within the assignment of chunks to the workers. Once a given worker calculates the gradient on all its assigned chunks, it computes a specified linear combination of these gradients and sends it to the PS. An exact gradient coding solution allows the PS to exactly recover \( L\) even in the presence of limited node failures.

Let \(A^{N m}\) be a matrix such that \(A_{i,j} 0\) if and only if \(_{i}\) is assigned to \(W_{j}\); henceforth, we call this the assignment matrix. Let \(nnz(v)\) denote the number of non-zero entries in a vector \(v\). Let \(_{i}\) and \(_{j}\) denote \(nnz(A(i,:)\) and \(nnz(A(:,j))\) (using MATLAB notation). These correspond respectively to the number of times \(_{i}\) appears in the cluster (replication factor) and the number of chunks assigned to \(W_{j}\) (load factor). We assume that at most \(s\) workers out of \(m\) are stragglers. Let \(g_{_{j}}=_{i_{j}} l(_{i},y_{i}, _{t})\), so that \( L=_{j=1}^{N}g_{_{j}}\). At iteration \(t\), worker \(W_{j}\) calculates \(g_{_{i}}\) for all \(i(A(:,j))\) (non-zero entries in \(A(:,j)\)) and linearly combines them to obtain \(g_{j}=_{i=1}^{N}A_{i,j}g_{_{i}}\). It subsequently transmits \(g_{j}\) to the PS. For decoding \( L\), the PS picks a decoding vector \(r\) which is such that \(r_{j}=0\) if \(W_{j}\) has not transmitted \(g_{j}\) (we say that \(W_{j}\) is straggling in this case). It subsequently calculates

\[_{j=1}^{m}r_{j}g_{j}=_{i=1}^{N}(_{j=1}^{m}A_{i,j}r_{j}) g_{_{i}}.\] (1)

**Related Work:** Under the original GC model , for _exact gradient coding_, we want that \(Ar=\) (the all-ones vector) under any choice of at most \(s\) stragglers. This means that the PS can perform a full gradient update. For exact GC, we need \(_{i} s+1\) for all \(i[N]\). This setting has been studied extensively . _Approximate gradient coding_ considers the scenario where the full gradient is either not required (e.g., SGD  works with an inexact gradient) or too costly to work with because of the high replication factor needed. In this setting, we want to design \(A\) such that \(||Ar-||_{2}\) (\(_{2}\)-norm) is small over all possibilities for the straggling workers. Prior work demonstrates constructions from expander graphs , sparse random graphs  and , block designs  and the like. Within distributed training, a significant time cost is associated with the transmission of the computed gradients (vectors of length-\(d\)) by the workers to the PS , e.g., deep learning usually operates in the highly over-parameterized regime  (\(d\)). For exact GC, if \(_{i} s+\) for \(i[N]\), then the dimension of the transmitted vectors from the workers can be lowered to \(d/\), thus saving on communication time. This is referred to as _communication-efficient GC_ and allows us to trade-off communication for computation. However, both  and  use polynomial interpolation in their solution. This leads to significant numerical instability  to the extent that their solution is essentially unusable for systems with twenty or more workers. This point is also acknowledged within the papers: Section V of  and Section II of . Some work considering these issues appears in  under restrictive parameter assumptions.

The usage of the partial work performed by stragglers has been considered  only within exact GC (i.e., approximate GC has not been considered); some of these apply in the communication-efficient setting. However, these approaches use multiple messages from the workers to the PS and thus incur a higher communication cost per iteration.

**Motivation:** Consider Fig. 1a where the edge labels indicate the encoded gradients. Note that under ideal operating conditions when each worker operates at the same speed, the overall gradient can be computed as long as each \(W_{i}\) processes its first chunk \(_{i}\) for \(i=1,,3\). Thus, it is quite wasteful for the workers to continue processing their second assigned chunk as specified in the original GC scheme; it would double the computation time. In addition, the original GC formulation ignores partial work by slow but not failed workers; in the sequel, we refer to these as "partial stragglers". For instance, in Fig. 1a, we consider a scenario, where \(W_{1}\) is slow and \(W_{3}\) is failed. The state of the computation is such that there is enough information for the PS to obtain the full gradient. However, under the original GC model, \(W_{1}\) will either wait until it processes \(_{2}\) before generating the encoded gradient to be sent to the PS, or the PS will treat \(W_{1}\) as failed and will have to settle for an approximate gradient.

Gradient coding can be viewed as an application of coding-theoretic techniques  to distributed learning; it allows the PS to be resilient to failures with very little coordination in the cluster. We note here that within classical coding theory  most constructions of erasure codes do not consider feedback from the receiver to the sender since such feedback may be expensive or noisy when the sender and receiver are at remote locations or not even necessary (, Chap. 7). However, feedback is quite easy to implement in the distributed learning setup.

**Main Contributions:** We present a new GC protocol that exploits a small amount of additional interactive communication to/from the PS and the workers. It greatly improves the computation-efficiency and communication-efficiency of GC while continuing to allow for efficient coordination within the cluster. Specifically, our protocol efficiently leverages the chunks processed by partial stragglers. Prior work that potentially applies in our setting suffers from the serious problem of numerical instability. In contrast, our protocol is provably numerically stable.

To our best knowledge, despite the importance of communication-efficiency, there are hardly any schemes for communication-efficient approximate GC. Our protocol provides an elegant way to address this problem, which in addition allows the PS to obtain an accurate estimate of the mean-square-error of the reconstructed gradient at any given point in the computation.

Prior work in the GC area ignores the relative ordering of the chunks within the workers. As our protocol leverages partial work performed by the workers, the relative ordering of the chunks within the workers is an important factor in the performance of the algorithm. For a large class of assignment matrices, we present an efficient polynomial-time algorithm that returns the optimal ordering of the chunks within workers.

## 2 Gradient Coding for partial stragglers

Our GC protocol operates under the following assumptions: (i) the workers know the assignment matrix and the ordering of the chunks within all the workers, (ii) at regular intervals the workers keep communicating to the PS, the number of chunks they have processed, and (iii) the PS wants the workers to communicate vectors of length \(d/\) for integer \( 1\).

We now provide a top-level description of our protocol; a formal statement appears in Algorithm 1. At the beginning of the training process, the PS generates a random \( m\) matrix \(\) with i.i.d. entries from a standard normal distribution, \(N(0,1)\) and shares it with all the workers. The workers keep reporting the number of chunks that they have processed in an iteration. The PS keeps monitoring these counts, and at a certain point, it broadcasts to all the workers an "encode-and-transmit" signal and an integer vector \(\) of length-\(m\) that specifies the number of chunks that have been processed by each node. This shares the global system state amongst the workers. For exact GC, the PS sends the encode-and-transmit signal when at least \(\) copies of each \(_{i}\) have been processed across the cluster. For approximate GC, the PS can send the signal even earlier.

Figure 1: Green/red means that the worker did/didnâ€™t process a chunk. (a) System with \(N=m=3\). Each worker is assigned two chunks that they process in a top-to-bottom order. \(W_{3}\) is failed and \(W_{1}\) is slow. (b) An arbitrary assignment of chunks to the workers (example also appears in ).

Following this, the workers need to decide their own encoding coefficients for the gradients that they have computed. Each gradient \(g_{_{i}}\) is block-partitioned into \(\) disjoint parts \(g_{_{i}}[k],k=0,,-1\). Each worker forms a matrix \(\) of dimension \(m N\) that consists of indeterminates that specify the encoding coefficients of all the workers (see example in the upcoming Section 2.2). Matrix \(\) consists of \(N\) block-columns, where each block-column itself consists of \(\) columns of length-\(m\), i.e., \(=[^{(1)}^{(2)}^{(N)}]\). The \(j\)-th block column, \(^{(j)}\) is associated with \(g_{_{j}}[k],k=0,,-1\). Based on the global state vector \(\), all workers know whether a chunk \(_{j},j[N]\) has been processed by worker \(W_{i},i[m]\). If \(_{j}\) has been processed by \(W_{i}\), then the \(i\)-th row of \(^{(j)}\) is populated with indeterminates, otherwise these entries are set to zero. Once the indeterminates are found (see Algorithm 1 for a description), worker \(W_{}\) encodes its gradients as

\[g_{}=_{i=1}^{N}_{j=0}^{-1}^{(i)}_{,j}g_{ _{i}}[j].\] (2)

Let \(}\) denote the \(i\)-th canonical basis vector of length-\(\), i.e., it contains a \(1\) at location \(i\{0,,-1\}\) and zeros elsewhere. We denote

\[z_{i}=_{N}}=[}^{ }^{}}\ \ }^{}}^{N}}]^{T}.\] (3)

where \(\) denotes the Kronecker product and \(_{N}\) denotes the all-ones vector of length \(N\). Recall that in the exact GC scenario, the PS wants to obtain \(_{i=1}^{N}g_{_{i}}[k]\) for \(k=0,,-1\). This is equivalent to requiring that

\[z_{i}^{T}(),i=0,,-1.\]

Our protocol is such that each \(W_{i}\) can independently calculate their encoding coefficients, such that collectively all the workers agree on the same matrix \(\) with the same values assigned to all the indeterminates. Towards this end, let \(}_{j}\) denote the submatrix of \(\) that is relevant to \(W_{j}\), i.e., the block-columns in which the processed chunks of \(W_{j}\) participate. For instance, suppose that \(W_{j}\) has processed \(_{j}_{j}\) chunks \(_{i_{1}},,_{i_{_{j}}}\) where \(1 i_{1}<i_{2}<<i_{_{j}} N\). Then,

\[}_{j}=[^{(i_{1})}\ ^{(i_{2})}\ \ ^{(i_{_{j}})}].\] (4)

\(W_{j}\) then solves a minimum-\(_{2}\)-norm least-squares solution to determine its encoding coefficients (see (5) in Algorithm 1). This ensures the solution is unique  even if the corresponding problem is under-determined. Thus, the workers automatically agree on the same solution.

```
0:\( m\) matrix \(\), \(m\)-length state vector \(\), \(_{i}\) the number of chunks processed by \(W_{i}\).
0: Encoding coefficients for the \(i\)-th worker \(_{i}\).
1:\(W_{i}\) forms the indeterminate matrix \(m N\) matrix \(\) based on \(\).
2:\(W_{i}\) extracts the columns of \(\) that correspond to its processed chunks. This matrix is called \(}_{i}\) (_cf._ (4)).
3:Worker \(W_{i}\) solves the following minimum-\(_{2}\)-norm least-squares problem, where the variables are the indeterminates in \(}_{i}\). \[\|_{_{i}}^{T} I_{}-} _{i}\|_{2}.\] (5)
4:Set \(_{i}=(i,:)\). ```

**Algorithm 1** Find-Encoding-Coeff

**Remark 1**.: _If \(=1\), then it is possible to arrive at a protocol whereby the workers agree to transmit appropriately weighted partial sums of their gradients, so that the PS can exactly/approximately recover the sum. However, in the communication-efficient setting when \(>1\), the encoding is no longer straightforward. Thus, \(>1\) is the main scenario we consider in what follows._

**Remark 2**.: _We discussed that each worker can form the \(m N\) matrix of indeterminates \(\) for the sake of ease of explanation. In fact, \(W_{j}\) only works with \(}_{j}\), which is of size at most \(m\)._

**Remark 3**.: _The additional communication assumed in our protocol is only \(O(m)\) as against the parameter vector of length-\(d\) and \(O(m) d\). Thus, the additional communication cost of our algorithm is minimal._

### Analysis of Algorithm 1

It is evident from our description and Algorithm 1 that upon receiving the encode-and-transmit signal and the vector \(\), each worker can independently create the matrix of indeterminates \(\).

**Exact GC analysis:** Suppose that each \(_{i},i[N]\) has been processed at least \(\) times across the cluster, and suppose that \(W_{i}\) has processed \(_{j}\). This means that there is a block-column \(^{(j)}\) such that each column within \(^{(j)}\) has at least \(\) indeterminates that need to be assigned values. For one column within \(^{(j)}\), \(W_{i}\) will solve a system of equations that is specified by \(\) submatrix of \(\) denoted \(X\) (an example appears in Section 2.2). Note that the columns of \(X\) will be linearly independent with high probability owing to the choice of \(\) and since \(\), a solution is guaranteed to exist. Let \(_{2}(M)\) denote the condition number of matrix \(M\). For a random \(\) matrix with i.i.d. \(N(0,1)\) entries, it is known that \((_{2}(X)) O()\). Thus, each such system of equations is well-conditioned with very high probability. In the under-determined case when \(>\), each worker will still agree on the same values of the corresponding indeterminates since we enforce that we work with the (unique) minimum \(_{2}\)-norm solution; no additional communication between the workers is required.

**Approximate GC analysis:** It is possible that the PS sends the encode-and-transmit vector when some \(_{i}\) has been processed \(-1\) times. In this case, the corresponding step for \(^{(i)}\) in (5) will be an over-determined least-squares procedure, which implies that there will be a non-zero error associated with it. Let \(X\) be the relevant \(\) submatrix of \(\) where we have \(<\) now, and recall that all entries of \(X\) are i.i.d. \(N(0,1)\) random variables. The squared error corresponding to \(^{(i)}\) can be expressed as \(_{i=0}^{-1}||XX^{}_{i}-_{i}||_{2}^{2}\) (\(X^{}\) denotes the pseudo-inverse ). Let \(X=USV^{T}\) denote the SVD of \(X\), where \(U\) and \(V\) are orthogonal matrices of dimension \(\) and \(\) respectively and \(S=[D 0]^{T}\) where \(D\) is a \(\) matrix with positive entries on the diagonal, and \(0\) represents a \((-)\) matrix of zeros. Then, \(X^{}=V[D^{-1} 0]U^{T}\). It is well known (, Remark 5.2.8) that for a matrix with i.i.d. \(N(0,1)\) entries, the singular vectors are uniformly distributed on the unit-sphere. Therefore, the expected squared error becomes

\[[||XX^{}_{i}-_{i}||_{2}^{2}]=[||_ {j=+1}^{}u_{j}u_{j}^{T}_{i}||_{2}^{2}]=_{j=+1}^{ }[(u_{j}^{T}_{i})^{2}]=.\] (6)

The last step above follows since each \(u_{j}\) is uniformly distributed over the sphere of dimension \(\). Therefore, we have the \([u_{j,0}^{2}]=1/\) since \(||u_{j}||_{2}^{2}=1\) and each \(u_{j,k},k=0,,-1\) is identically distributed. We conclude that if \(_{i}\) appears \(_{i}-1\) times, then its error contribution is \(-_{i}\) and the overall error is therefore \(_{i=1}^{N}(0,-_{i})\).

**Complexity analysis:** The time complexity of each least-squares problem is \(O(^{2})\) and the \(i\)-th worker solves at most \(_{i}\) of them independently and in parallel. The marginal cost of this calculation as against the calculation of the actual gradients will be very small in most practical settings.

### Illustrative Example

Consider Fig. 0(b) (from ) where the dataset consists of chunks \(_{1},,_{5}\) and are assigned in a non-uniform fashion to workers \(W_{1},,W_{5}\). Suppose that the PS wants the exact gradient with \(=2\). As two copies of each chunk have been processed, the PS broadcasts the encode-and-transmit signal and the vector \(=[5\;2\;0\;2\;3]\) to all the workers which indicates, e.g., that \(W_{1}\) has processed all its chunks, \(W_{3}\) is failed etc. The matrix \(\) of indeterminates for this example and the corresponding \(^{(i)}\)'s, turn out to be

\[ =[^{(1)}|^{(2)}|^{(3)}|^{ (4)}|^{(5)}]\] (7) \[=a_{1}&a_{2}&a_{3}&a_{4}&a_{5}&a_{6}&a_{7}&a_{8}& a_{9}&a_{10}\\ b_{1}&b_{2}&b_{3}&b_{4}&0&0&0&0&0&0&0\\ 0&0&0&0&0&0&0&0&0&0\\ 0&0&c_{3}&c_{4}&c_{5}&c_{6}&0&0&0&0&0\\ d_{1}&d_{2}&0&0&0&0&d_{7}&d_{8}&|d_{9}&d_{10}.\] (8)

In this example, \(W_{5}\) has processed \(_{1},_{4},_{5}\) so that \(_{5}=[^{(1)}\;^{(4)}\;^{(5)}]\). Note that the PS requires the vectors \([_{0}^{T}\;_{0}^{T}\;_{0}^{T}\;_{0}^{T}\;_{0}^{T}]\) and \([_{1}^{T}\;_{1}^{T}\;_{1}^{T}\;_{1}^{Tcan recover \(_{i=1}^{5}g_{_{i}}[k],k=0,1\) from the encoded gradients. Towards this end, e.g., \(W_{5}\) solves (5) in Algorithm 1 with the matrix \(_{5}\), i.e.,

\[||[_{2}|_{2}|_{2}]-[^{(1 )}\ ^{(4)}\ ^{(5)}]||_{2}\] (9)

where the decision variables are \(a_{1},a_{2},a_{7},a_{8},a_{9},a_{10}\), \(b_{1},b_{2}\) and \(d_{1},d_{2},d_{7},d_{8},d_{9},d_{10}\). For instance, corresponding to the first column of \(^{(1)}\), \(W_{5}\)'s problem becomes determining the minimum \(_{2}\)-norm solution of the under-determined least-squares problem

\[||r_{01}&r_{02}&r_{05}\\ r_{11}&r_{12}&r_{15}a_{1}\\ b_{1}\\ d_{1}-1\\ 0||_{2}.\] (10)

We emphasize that the same minimization will be independently performed at workers \(W_{1}\) and \(W_{2}\) as well. After each \(W_{j}\) determines its encoding coefficients and transmits \(g_{j}\) for \(j=1,,5\), the PS can easily recover \(_{j=1}^{5}g_{_{j}}[k]=_{i=1}^{5}r_{kj}g_{j}\) for \(k=0,1\).

Algorithm 1 works as is, even in the case when the PS is interested in an approximate gradient. For instance, suppose that the PS sends the encode-and-transmit signal when the vector \(=[4\ 2\ 0\ 2\ 3]\), so that only one copy of \(_{5}\) has been processed in the cluster. The corresponding encoding coefficients can still be computed using Algorithm 1. The only difference will be that the relevant indeterminate matrix becomes

\[^{}=a_{1}^{}&a_{2}^{}&a_{3}^{ }&a_{4}^{}&a_{5}^{}&a_{6}^{}&a_{7}^{}&a_{8}^{ }&0&0\\ b_{1}^{}&b_{2}^{}&b_{3}^{}&b_{4}^{}&0&0&0&0&0&0\\ 0&0&0&0&0&0&0&0&0&0\\ 0&0&c_{3}^{}&c_{4}^{}&c_{5}^{}&c_{6}^{}&0&0&0&0\\ d_{1}^{}&d_{2}^{}&0&0&0&0&d_{7}^{}&d_{8}^{}&d_{9}^{ }&d_{10}^{}.\] (11)

This means, e.g., when \(W_{5}\) is trying to find \(d_{9}^{}\), then it will be solving an over-determined least-squares problem: \(||r_{05}\\ r_{05}^{}d_{9}^{}-1\\ 0||_{2}\). It is evident, that all the workers can still agree on the same solution.

## 3 Chunk Ordering Algorithm

Note that the assignment matrix only specifies the assignment of chunks to workers but says nothing about the relative order of chunks within a worker. When taking into account partial stragglers, the relative ordering of the \(_{i}\)'s within a worker is crucial. Therefore, an important question when leveraging partial stragglers within GC is one of how to determine this chunk ordering within workers for a given assignment matrix. This will in general depend on models of worker speeds. However, it is a difficult task to obtain accurate models on the worker speeds within a cloud cluster as conditions can change dynamically.

We work instead with a combinatorial metric that depends on the total number of chunks that the cluster has to process in the worst case, such that at least a single copy of each \(_{i}\) is processed; this was also used in [35; 36] in a coded matrix computation context. This metric minimizes the worst case number of chunks that the cluster needs to process in the case when \(=1\). In particular, for a given \(_{i}\), let \(Q_{i}\) denote the maximum number of chunks that can be processed by the cluster such that none of the copies of \(_{i}\) are processed (see Figs. 1(a) and 1(b)) and let \(Q_{}=_{i=1,,N}Q_{i}\). Thus, \(1+Q_{}\) is a metric that quantifies the worst-case number of chunks that need to be processed

Figure 2: Two different relative orderings of the chunks within workers for the same assignment matrix. Individual figures show the calculation of \(Q_{5}\). Similarly, other \(Q_{i}\) values can be computed. \(Q_{}=Q_{5}\) for both assignments. Thus, (a) \(Q_{}=10\). (b) \(Q_{}=9\).

before at least a single copy of every \(_{i}\) is guaranteed to be processed. Here, the worst-case is over the speeds of the different workers. We say that an assignment is optimal if it achieves the lowest possible \(Q_{}\) for a given assignment matrix. Indeed, Figs. 1(a) and 1(b) show two different orderings for the same assignment matrix with different values of \(Q_{}\).

From the point of view of leveraging partial stragglers, for exact GC it can be shown that the ordering imposed by the cyclic assignment scheme (in ) is optimal ((_cf._ Remark 1 in work )) for this \(Q_{}\) metric. However, for approximate GC, when the number of stragglers can be much higher, other assignments, e.g., those based on regular graphs perform better . In particular, in these cases, the assignment matrix \(A\) is chosen as the adjacency matrix of the relevant graphs and is such that \(N=m\) and \(_{i}=_{i}=\) for \(i[m]\). For such assignment matrices, Algorithm 2 presents an optimal algorithm for determining the ordering of the chunks within each worker in \(Q_{}\)-metric. Corresponding to the assignment matrix \(A\), we can associate an ordering matrix \(O\) which is such that \(O_{i,j}=0\) if \(A_{i,j}=0\) and \(O_{i,j}[]\) otherwise. Thus, if \(O_{i,j}= 0\), it means that \(_{i}\) is the \(\)-th chunk in \(W_{j}\), e.g., \(=1\) implies that the chunk is at the top and \(=\) means that it is at the bottom. Let \(Q_{i}(O)\) denote the maximum number of chunks that can be processed across the cluster, such that no copy of \(_{i}\) has been processed. Then, upon inspection, we can see that

\[Q_{i}(O)=_{j[m]}_{\{O_{i,j} 0\}}(O_{i,j}-1)+(m-) =_{j[m]}_{\{O_{i,j} 0\}}O_{i,j}+(m--1),\]

where the notation \(_{}\) denotes the indicator of \(Y\). Next, let \(Q_{}(O)=_{i[m]}Q_{i}(O)\). Thus, given an assignment matrix \(A\), our goal is to find an ordering matrix, such that \(Q_{}(O)\) is minimized. As \((m--1)\) is a constant, this optimization is equivalent to the following min-max problem.

\[}_{i[m]}_{j[m]}O_{i,j}.\] (12)

Each column of \(O\) has exactly \(\) non-zero entries, with each value in \([]\) appearing exactly once. Counting the sum of the entries in \(O\) two different ways yields

\[m=_{i[m],j[m]}O_{i,j} mQ_{}(O), { so that }Q_{}(O).\]

It turns out that this bound is in fact achievable. Let \(=^{(1)}\) (defined in Algorithm 2). Define \(()=(V,E)\) as a bipartite graph on vertex set \(V=X Y\) such that \(|X|=|Y|=m,X Y=\) and \((v)=\) for all \(v X Y\). For \(u X,v Y\), we have that \((u,v) E\) if and only if \(_{u,v}=1\). Thus, \(\) is in one-to-one correspondence with \(()\). Algorithm 2 decomposes \(()\) into a collection of disjoint perfect matchings  which are then assigned values in \([]\). This gives us the required ordering.

```
1:Assignment matrix \(A\) such that \(N=m\) and \(_{i}=_{i}=\) for all \(i[m]\)
2:An optimal ordering matrix \(O\).
3:Let \(^{(1)}\{0,1\}^{m m}\) such that \(^{(1)}_{i,j}=1& 0$}\\ 0&.\) and \(^{(1)}=\).
4:for\(i\) ranges from \(1\) to \(\)do
5: Apply Claim 1 with \(^{(i)}\) and \(^{(i)}\) and solve the max-bipartite matching problem in Claim 1. Let the permutation matrix be \(P_{i}\).
6: Let \(^{(i+1)}:=^{(i)}-P_{i}\) and \(^{(i+1)}=^{(i)}-1\).
7:endfor
8: Set \(O=_{i[]}iP_{i}\). ```

**Algorithm 2** Chunk-Ordering

**Claim 1**.: _Let \(\{0,1\}^{m m}\) be such that both \(i\)-th row sum and \(j\)-th column sum of \(\) are \(\) for all \(i,j[m]\). Then there exists a permutation matrix \(P\{0,1\}^{m m}\) such that \(-P\{0,1\}^{m m}\) and both \(i\)-th row sum and \(j\)-th column sum of \(-P\) are \(-1\) for \(i,j[m]\)._

Proof.: We claim that \(()\) has a \(X\)-perfect matching. Let \(S X\) be arbitrary and \(N(S) Y\) denote its neighborhood. Let \(\) denote the average degree of the vertices in \(N(S)\) in the subgraphinduced by \(S N(S)\). Then, we have

\[|S|=|N(S)||N(S)|,|S||N(S)|.\]

The first inequality above follows because the degree of each node in \(N(S)\) in \(()\) is \(\). Thus, Hall's condition  holds and the claim follows. Since \(|X|=|Y|\), this is actually a perfect matching \(M\). Then, this perfect matching \(M\) gives the desired \(P\): \(P_{u,v}=1\) if \((u,v) M\) and \(P_{u,v}=0\) if \((u,v) M\). Removing the matching \(M\) from \(()\) results in a bi-regular bipartite graph with degree \(-1\) which corresponds to \(-P\). 

**Remark 4**.: _The proposed algorithm above finds the optimal ordering when considering the case of \(=1\) with \(N=m\) (number of chunks equal to number of workers); it only applies when \(N=m\). While this algorithm is expected to have better performance than a randomly chosen ordering in the case of higher \(\), we do not have an optimal construction in this case._

**Algorithm 2 Analysis:** The algorithm gives \(\) permutation matrices \(\{P_{i}\}_{i[]}\) such that \(=_{i=1}^{}P_{i}\), i.e., the set of non-zero entries of the \(P_{i}\)'s are disjoint. Since \(O=_{i[]}i P_{i}\), this implies that each column has exactly \(\) non-zero entries such that these entries consists of elements of \([]\). Therefore, \(O\) is an ordering matrix. In addition, each row has \(\) non-zero elements from \([]\), so that all row sums and \(Q_{}(O)\) equal \(\). A maximum matching can be in time \(O(m^{2})\) by converting it to a max-flow problem , so our overall complexity is \(O(m^{2}^{2})\). The complexity can potentially be reduced further by using more efficient max-flow algorithms.

## 4 Numerical Experiments and Comparisons

Our proposed protocol in Section 2, utilizes additional communication between the PS and the workers. We note here that ideas in  that are based on Lagrange interpolation can potentially be adapted to arrive at an exact GC protocol within our setting. However, the numerical instability of Lagrange interpolation is a serious issue with this solution, since it can be shown that the degree of the polynomial to be interpolated is at least \(m-\). Even for values such as \(=2\) and \(m=22,27,32\), the error in Lagrange interpolation is too high for the technique to be useful (see Appendix A).

In what follows, we present comparisons with the original GC protocol via a simulation of the distributed system for both the exact and approximate GC scenarios. All software code for recreating these results can be found at . These simulations were performed within a MacBook Pro (M1 chip, 16 GB RAM). In both scenarios, we simulated node failures and slow-downs. We generated a random vector of length-\(m\) where \(\) workers uniformly at random are chosen to be failed (\(\) is chosen based on the scenario). For the other workers, the amount of time taken to process a chunk is

Figure 3: (a) Mean-squared error (MSE) vs. \(T\) for an approximate GC scenario. Blue curves: proposed protocol with \(=1,2,3\), purple curves: corresponding MSE estimates, and red curve: original GC protocol with \(=1\). Error bars correspond to one standard deviation. (b) Completion time vs. \(\) for exact GC scenario with two different assignment matrices. Blue curves: proposed protocol, green curves: original GC protocol. Error bars correspond to one standard deviation.

chosen i.i.d. from an exponential distribution with parameter \(=1\). The entries of the matrix \(\) are chosen i.i.d. from the standard normal \(N(0,1)\).

**Approximate GC simulation:** We considered two different random regular graphs, \(G_{i},i=1,2\) with sizes \(m=200,300\) and degree \(=8\). These graphs have second-largest absolute value of eigenvalues: 5.14 and 5.23, i.e., less than \(2\) so they can be considered as Ramanujan graphs  (these were used in ). Their adjacency matrices were used as the assignment matrix. The number of failures \(\) was set to \(-1\).

For the original GC protocol with arbitrary assignment matrices,  provides a communication-efficient approximate GC method that relies on rational interpolation. However, even for \(m=100\), these will result in very complex basis functions. Furthermore, there are no numerical results in  (or posted software) and the approximation guarantees depend on the form of the function to be interpolated, i.e., the guarantees cannot be expressed in terms of the system parameters. Thus, in our comparison, we only consider the original GC protocol with \(=1\).

For the original GC protocol, we compute the least squares solution \(\) for minimizing \(\|Ar-\|_{2}^{2}\). Here, \(\) such that \(_{i}=0\) if \(W_{i}\) has not completed processing all its chunks at time \(T\). For our proposed protocol, we leverage partial work completed by \(T\), as described in Section 2. The overall error is computed as the sum of the errors for the least-squares solution for each \(z_{i}^{T},i=0,,-1\). Each data point on the curves was chosen by performing 1000 simulations of failures and slow-downs. We have also plotted the expected value of the error, derived in (6).

Fig. (a)a shows the mean-squared-error (MSE) comparing the original GC approach with \(=1\) and our partial GC approach for \(=1,2,3\) for graph \(G_{1}\) (see Appendix B for \(G_{2}\) results). As can be observed, the MSE for our approach is several orders of magnitude lower with increasing \(T\). Crucially, our estimate (6) closely tracks the behavior of the error of our method. Thus, it can easily be used by the PS as a way to decide when to send the encode-and-transmit message. We emphasize that our approach, even with \( 2\) actually has a lower MSE than the original approach (that operates with \(=1\)). Note that with \( 2\), we will enjoy a lower communication time in a real-world distributed cluster. However, as we are working with a simulation of the distributed system, at this time we do not have precise figures for the reduction in communication time on actual clusters.

In Fig. 6 in Appendix B, we compare the performance of our approach under the optimal ordering (_cf._ Section 3) and an appropriately chosen random ordering. The random ordering is picked as follows. We generated 100 independent random orderings and selected the one with the best (smallest) \(Q_{}(O)\). Our optimal ordering, which can be found efficiently, clearly has a better performance.

**Exact GC simulation:** We considered (i) the cyclic assignment  with \(N=m=200\) and \(=8\), and (ii) the assignment based on graph \(G_{1}\) discussed above. The number of failures \(\) in the simulations is set to \(-\) so that exact gradient reconstruction is possible. For both approaches, we determined the time \(T\) such that each chunk is processed at least \(\) times across the cluster (for original GC we only consider workers that have processed all their chunks). These values were averaged over 1000 runs for each value of \(\). In Fig. (b)b we clearly observe that the exact gradient can be computed using our method is approximately half the time as compared to the original GC protocol. We note that the average completion time for the original GC protocol (\(G_{1}\)-based assignment) for \(=1\) is about \(T=6\) time units. However, the MSE for the original GC protocol in the approximate scenario continues to be high at \(T=24\). The reason is that there are \(-1\) failures that are introduced in the simulation. The approximate GC recovery operates by solving a least-squares problem for the fixed \(G_{1}\)-based assignment. Thus, the MSE does not necessarily drop to zero even if one copy of each chunk has been processed in the cluster. However, there are exact recovery algorithms that one can use in this case. We note here that when \( 2\), the known techniques for exact recovery for the original GC protocol are based on Lagrange interpolation and are numerically unstable. Thus, the gradient recovered using the original approach will in general not be useful.

Fig. 7 in Appendix B compares the completion times of random vs. optimal ordering; the optimal ordering is clearly better.

## 5 Limitations of our work

Our chunk ordering algorithm (Section 3) is optimal only in the case when the assignment matrix has \(N=m\) and \(_{i}=_{i}=\) for \(i[m]\). While several well known gradient coding schemes,especially those from regular graphs, satisfy this property, there are others that do not. Our results in Section 4 with \( 2\) are simulations of the actual distributed cluster and indicate lower MSE than the original GC protocol. However, we do not have actual cloud platform statistics on the reduction in communication time within our method. We do however expect this reduction to be quite significant.

## 6 Conclusions

We presented a novel gradient coding protocol that leverages the work performed by partial stragglers. Our protocol is simultaneously computation-efficient and communication-efficient, and numerically stable. Furthermore, we present rigorous analyses of the protocol's correctness and performance guarantees. Our protocol is one of the first to provide a satisfactory solution to the problem of communication-efficient, approximate gradient coding for general assignment matrices.