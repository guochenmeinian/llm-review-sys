# Decomposed Prompt Decision Transformer for

Efficient Unseen Task Generalization

 Hongling Zheng\({}^{1}\)  Li Shen\({}^{2}\)  Yong Luo\({}^{1}\)  Tongliang Liu\({}^{3}\)  Jialie Shen\({}^{4}\)  Dacheng Tao\({}^{5}\)

\({}^{1}\)Wuhan University \({}^{2}\)Shenzhen Campus of Sun Yat-sen University \({}^{3}\)The University of Sydney

\({}^{4}\)City, University of London \({}^{5}\)Nanyang Technological University

{hlzheng, luoyong}@whu.edu.cn {mathshenli, jialie, dacheng.tao}@gmail.com

tongliang.liu@sydney.edu.au

Corresponding authors.

###### Abstract

Multi-task offline reinforcement learning aims to develop a unified policy for diverse tasks without requiring real-time interaction with the environment. Recent work explores sequence modeling, leveraging the scalability of the transformer architecture as a foundation for multi-task learning. Given the variations in task content and complexity, formulating policies becomes a challenging endeavor, requiring careful parameter sharing and adept management of conflicting gradients to extract rich cross-task knowledge from multiple tasks and transfer it to unseen tasks. In this paper, we propose the Decomposed Prompt Decision Transformer (DPDT) that adopts a two-stage paradigm to efficiently learn prompts for unseen tasks in a parameter-efficient manner. We incorporate parameters from pre-trained language models (PLMs) to initialize DPDT, thereby providing rich prior knowledge encoded in language models. During the **decomposed prompt tuning phase**, we learn both cross-task and task-specific prompts on training tasks to achieve prompt decomposition. In the **test time adaptation phase**, the cross-task prompt, serving as a good initialization, were **further** optimized on unseen tasks through test time adaptation, enhancing the model's performance on these tasks. Empirical evaluation on a series of Meta-RL benchmarks demonstrates the superiority of our approach. The project is available at https://github.com/ruthless-man/DPDT.

## 1 Introduction

The purpose of offline reinforcement learning (Offline RL)  is to develop a reward-maximizing RL strategy using offline data. This approach is of highly valuable in real-world scenarios where online data collection is expensive, time-consuming, or impractical. Existing offline RL algorithms typically perform well in single task but often struggle for multiple tasks with similar conditions and objectives, as they lack the ability to separate common knowledge from conflicting task gradients. In contrast, humans can leverage knowledge from existing tasks to excel in new ones, which has led to increasing research interest in multi-task reinforcement learning (MTRL) [2; 3; 4]. The goal of MTRL is to develop a universal strategy applicable to tasks with certain similarities, thereby enhancing adaptability and performance in multi-task environments.

Decision transformer (DT)  and Prompt-DT  introduce the transformer architecture to the field of RL, demonstrating the powerful data modeling capability of sequence offline RL. Additionally, they provide possibilities for integrating advancements  from language modeling into MTRL methodologies. Prompt-tuning DT  uses a gradient-free method to introduce prompts, retaining context-specific information and catering to specific preferences. These existing sequence-based offline RL methods are primarily trained and tested on the same task or perform fine-tuning using asmall portion of labeled test data . As a result, these algorithms often perform poorly when faced with testing tasks that are unseen and unlabeled in a Meta-RL setting .

Leveraging MTRL to extract general knowledge offers a promising approach for facilitating cross-task knowledge transfer in Meta-RL scenarios. However, as the number of tasks increases, gradient conflicts become more pronounced, hindering MTRL performance due to unregulated parameter sharing. Additionally, while transformer architectures can capture extensive relationships in offline sequential data, their data-hungry nature means that insufficient training data for RL tasks significantly diminishes model performance. A feasible solution might be to enhance the model's prior knowledge.

To remedy these drawbacks, we propose a prompt-based MTRL method named Decomposed Prompt Decision Transformer (DPDT), inspired by some works in natural language processing where knowledge transfer in multi-task learning scenarios is achieved through prompting strategies [11; 12]. We first employ pre-trained parameters from GPT to initialize a DPDT architecture. Incorporating the parameters of PLMs is motivated by several studies in RL [13; 14; 15]. Leveraging the rich prior knowledge encoded in Pre-trained Language Models (PLMs) effectively addresses the data hunger challenge of transformer architectures, providing ample semantic information for reinforcement learning tasks. Then, we design a two-stage training and testing framework for DPDT. (1)

**Decomposed prompt tuning phase:** At this stage, we use prompt decomposition to avoid gradient conflicts between different tasks and to extract common knowledge. Specifically, we decompose the task prompt for each task into a cross-task prompt and a task-specific prompt. The cross-task prompt remains consistent across all training tasks, while the task-specific prompt is tailored to each task's unique characteristics. By isolating the cross-task prompt from the task-specific prompts, the model ensures that updates related to general knowledge do not conflict with those related to specific tasks. Compared to , our structured decomposition enables more regulated and harmonious parameter updates, thereby enhancing parameter efficiency and facilitating the extraction of general knowledge more effectively. (2) **Test time adaptation phase:** The cross-task prompt, serving as a strong initialization, is further optimized on unlabeled unseen tasks by incorporating the Test Time Adaptation (TTA)  to our model. TTA dynamically adjusts the cross-task prompts during the testing phase based on task characteristics, enhancing the model's adaptability to unseen tasks features.

Our DPDT has been empirically validated in the Meta-RL setting, and the results demonstrate its superiority compared with many recent and competitive counterparts [5; 6; 17]. Furthermore, we conducted ablation experiments covering aspects such as prompt length, scalability, and model variants to establish the superiority of the model. The main contributions of this work are as follows:

* We reconsider the problem of knowledge extraction in MTRL and propose the method of Decomposed Prompt Decision Transformer.
* We propose a two-stage paradigm, which includes decomposing the task prompts for training tasks into cross-task prompts and task-specific prompts, and aligning the cross-task prompts in unlabeled unseen tasks.
* We demonstrate the effectiveness of DPDT through intensive experiments on a broad spectrum of benchmarks, highlighting its competitive performance in Meta-RL scenarios.

## 2 Related works

In this section, we summarize the most related works as two-fold: offline RL and multitask RL.

**Offline RL.** In contrast to traditional RL methods [18; 19], offline RL focuses on training models and performing trial-and-error using offline data without environmental interaction to arrive at appropriate strategies. These methods primarily address the issue of out-of-distribution (OOD) through strategies such as constraining the learning policies  or bounding the overestimated policy values . The integration of transformer architecture in sequence modeling has emerged as a prominent approach for addressing offline RL tasks [22; 23], further demonstrating the advantages of data-driven policy learning. Decision Transformer (DT)  involves encapsulating rewards, states, and actions into triples and training them using autoregressive supervision on offline data. Owing to the transformer's proficiency in capturing and fitting long time series features, it has achieved remarkable results in various offline RL tasks. HDT  generalizes new tasks by designing an adaptation module initialized by a hypernetwork. To alleviate the tuning burden while preserving performance, prompt tuning [24; 25] in NLP focuses on optimizing only the input parameters while keeping the majority of the PLMs parameters frozen. However, integrating prompt tuning into RL field poses a challenge, as RL prompts lack semantic information and are challenging to optimize. While Prompt-DT  selects pre-defined expert trajectories combined with inputs to guide model training, it primarily relies on the quality of these trajectories for improvement, rather than enhancing prompts directly. On the other hand, Prompt-tuning DT  stands out for introducing prompt-tuning techniques using a gradient-free approach, with the goal of preserving environment-specific details and accommodating specific preferences.

**Multitask RL.** Multi-Task Reinforcement Learning (MTRL) [26; 27; 28] aims to address multiple similar reinforcement learning tasks using a unified model. A straightforward approach involves developing a task-conditional multi-task model, akin to those utilized in goal-conditional RL  and visual-language grounding . While this method has demonstrated success in certain scenarios, it often encounters challenges stemming from negative interference among tasks. PaCo  delves into a compositional structure within the parameter space, distinguishing between task-agnostic and task-specific components. This approach significantly enhances the efficiency and robustness of the MTRL process, leading to more effective training outcomes. Building upon the MTRL paradigm, multi-task prompt  aims to acquire transferable, cross-task prompts from multiple tasks, guiding outputs for unseen downstream tasks. Several studies have approached multi-task prompt design from the perspective of prompt decomposition, yielding notable results across various tasks [33; 34].

The most relevant work to ours is Prompt-DT , which utilizes carefully selected prompts for Meta-RL tasks training. Our approach differs in (1) employing trainable prompt decomposition to avoid gradient conflicts among multiple tasks, (2) further optimizing general prompts with TTA without using any test data labels, and (3) extending the model architecture by incorporating PLMs for initialization. To the best of our knowledge, we are the first to implement multi-task prompt tuning based on PLMs parameters in the reinforcement learning domain.

## 3 Preliminary

In this section, we provide several concepts and terminologies that will be used in this work.

### Prompt Decision Transformer

Prompt-DT  examines the beneficial effects of incorporating trajectory prompts on the DT  in few-shot scenarios. Specifically, the form of trajectory prompts is the same as that of training trajectories, consisting of triplets made up of state \(s^{*}\), action \(a^{*}\), and return-to-go \(^{*}\). However, these trajectory prompts are significantly shorter than the training trajectories. During training, for each task \(i\), the trajectory prompt \(^{*}_{i,K^{*}}\) is concatenated with the corresponding training trajectory \(_{i,K}\) to form \(^{input}_{i}=(^{*}_{i,K^{*}},_{i,K})\), which is then inputted into the model \(f\) for training. The concrete form of \(^{*}_{i,K^{*}}\) and \(_{i,K}\) are as follows:

\[^{}_{i,K^{*}}=(^{}_{1},s^{}_{1},a^{}_{1}, ,^{}_{K^{*}},s^{}_{K^{*}},a^{}_{K^{*}}), _{i,K}=(_{1},s_{1},a_{1},,_{K},s_{K},a_{K})\] (1)

where \(K^{*}\) represents the number of environment steps stored in the prompt, and \(K\) is the nearest steps of the training trajectory. The prediction head associated with the state token \(s\) is designed to predict the corresponding action \(a\). It is noteworthy that during training, the model does not predict the actions of the trajectory prompts. The loss function is formulated as follows: \(a_{i,m}\) denotes the actual action at the \(m\)-th timestep of the \(i\)-th task, while \(_{i,m-1}\) encompasses all data up to and including the \((m-1)^{th}\) timestep in the training trajectory of the \(i^{th}\) task.

\[L_{m}=_{^{input}_{i}_{i}}[_{ m=1}^{K}(a_{i,m}-f(^{}_{i,K^{*}},_{i,m-1}))^{2}]\] (2)

### Test-Time Adaptation

Test time adaptation (TTA)  aims to minimize the gap between training data and testing data distribution during the testing phase. Test-time prompt tuning (TPT)  leverages the extensive knowledge in transformer architecture to enhance its generalization capabilities in zero-shot scenarios.

During the inference phase, multiple randomly augmented views are created from the provided testing sample \(X_{test}\). Predictions with entropy below a specified threshold are retained, while other views are filtered out using a confidence selection criterion. The average entropy of the filtered predictions is then used to unsupervisedly update the prompts \(p\). Some methods [36; 37; 38], considering the data instability caused by augmentation techniques, use alignment of feature values in attention layers or feedforward layers to achieve test-time adaptation. Our method falls into this category.

## 4 Decomposed Prompt Decision Transformer

In this section, we provide a comprehensive description of the proposed DPDT. Task prompts and training data are combined to calculate the loss through action prediction, which is then used to optimize the cross-task and task-specific prompts in a backward pass. Once optimized, the cross-task prompts serve as a good initialization for use in unseen tasks during test-time adaptation. The objective of decomposed prompt tuning is to learn cross-task prompts and task-specific prompts through prompt decomposition. Test-time adaptation provides an alignment approach for the cross-task prompts. Below, we will describe each module of DPDT in detail.

### Decomposed Prompt Tuning

**Initialization.** Given that Transformers are data-intensive and require pre-training on substantial datasets to achieve satisfactory performance, integrating PLMs from the same architectural family into offline RL is a natural progression. Some existing work has already explored this avenue [13; 39; 40]. Taking inspiration from this, we employ GPT2-SMALL  to initialize our DPDT and maintain these parameters frozen throughout training. It is worth noting that, incorporating PLMs into RL is not predicated on the direct applicability of language data to RL tasks. Instead, the advantage lies in leveraging the deep, nuanced representations acquired by PLMs from a variety of datasets. These representations encode a broad spectrum of patterns, relationships, and contexts that can transcend purely linguistic tasks. This taps into the reasoning and few-shot capabilities of language models, addressing challenging scenarios like data scarcity and sparse rewards.

Figure 1: The architecture diagram of DPDT. For simplicity and clarity, the figure only displays the decomposition and integration process during training on a single task. Snowflake icons represent the frozen parts of the model that are not subject to training updates, while flame icons indicate components of the model that remain trainable. **Left: Decomposed Prompt Tuning.** The prompt decomposition is trained on the entire dataset with the assistance of the teacher prompt \(p_{k}^{teacher}\). \(p_{k}^{*}\) is then combined with the training samples that include \(K\) steps, inputted into DPDT, and outputs the corresponding action \(a\). **Right: Test Time Adaptation.** When test samples are fed into the model, we calculate the mean and variance of all samples at each layer and compute the loss by comparing them with the mean and variance of the corresponding layer from the training samples. The losses from all layers are summed to obtain the alignment loss, denoted as \(L_{align}\).

**Prompt Decomposition.** Given a set of training tasks \(S=\{S_{1},S_{2},,S_{n}\}\), our objective is to learn a general prompt \(P_{c}\) that encapsulates common knowledge shared across all tasks in \(S\) and can efficiently adapt to unseen tasks. Extracting general task information from tasks with different distributions is often challenging, as gradient conflicts between tasks can lead to suboptimal convergence of information. We adopt prompt decomposition approach to address this issue. As shown in Figure 1, let \(P_{c}^{l s}\) and \(P_{k}^{l s}\). The task prompt \(P_{k}^{*}\) for the \(k\)-th task is obtained by taking the element-wise product of \(P_{c}\) and \(P_{k}\). The goal of prompt decomposition is to enable efficient knowledge sharing across \(S\), while still allowing each task to maintain its own parameters to encode task-specific knowledge. The \(P_{c}\) aims to acquire general knowledge from \(S\), while the task-specific prompt \(P_{k}\) allows task \(k\) to retain its unique knowledge. The task prompts parameterization of the \(k\)-th training task is expressed as:

\[P_{k}^{*}=P_{c} P_{k}=P_{c}(v_{k} u_{k}).\] (3)

**Input**: Training task set \(S\), Offline datasets \(_{}\), Batch size \(M\), Learning rate \(\), training iterations \(N\), teacher task prompts \(p_{k}^{ teacher}\).

**Initialize**: Initialize a 12-layer, 12-head DPDT \(\) using GPT2-SMALL, randomly initialize cross-task prompts \(P_{c}\) and low-rank vectors \(v_{k},u_{k}\).

**for**\(t=1\) to \(N\)**do**

**for**\(k\) in \(S\)**do**

 Select a trajectory \(\) that contains \(M\) samples in task \(k\).

 Calculate \(P_{k}^{*}\) by Equation 3.

 Calculate \(L_{MSE}\) and \(L_{dis}\) according to Equations 4 and 5.

 Computed loss function by Equation 6.

\(-_{}_{Total}\).

**end for**

**end for**

**Algorithm 1** Decomposed Prompt Tuning

In the specific implementation process, inspired by LORA , we further decompose each task-specific prompt into two low-rank vectors \(v_{k}^{l r}\) and \(u_{k}^{l s}\) using a low-rank method. We obtain \(P_{k}\) through vector multiplication. Here \(l\) represents the prompt length, \(r\) represents the hidden layer dimension, and \(s\) represents the prompt dimension. The hyperparameter \(r\) is a manually specified low-rank parameter. Its introduction is crucial for designing prompts for all tasks in the dataset, significantly maintaining model superiority while reducing computational load. We use standard normal distribution to initialize \(P_{c}\), \(u_{k}\) and \(v_{k}\). Given the DPDT \(\), the mean squared loss between the model's predicted actions and the true actions is calculated as:

\[_{MSE}=(a-(P_{k}^{},))^{2}\] (4)

**Prompt distillation.** Due to the lack of explicit constraints, directly implementing prompt decomposition on the multitask dataset \(S\) may lead to an overlap in the information learned by \(P_{c}\) and \(P_{k}\), potentially undermining their ability to capture distinct intended details. We employed knowledge distillation techniques to compel the cross-task and task-specific prompts to learn their respective information. We obtained teacher task prompts \(p_{k}^{ teacher}\) for each task \(k\) by using traditional prompt-tuning methods individually. During training, the mean squared error is calculated directly between \(p_{k}^{ teacher}\) and \(p_{k}^{}\):

\[_{dis}=_{k||}|p_{k}^{ teacher}-p_{k}^{}|^{2}\] (5)

The total loss function for training task prompts for obtaining a cross-task prompt to be transferred to the target side is then:

\[_{Total}=_{MSE}+_{dis}\] (6)

where \(\) is a weight to balance the impact of distillation loss terms. In our experiments, we set \(\) to 0.5. The overall summary of the multitask training algorithm is presented in Algorithm 1.

``` Input: Training task set \(S\), Offline datasets \(_{}\), Batch size \(M\), Learning rate \(\), training iterations \(N\), teacher task prompts \(p_{k}^{ teacher}\). Initialize: Initialize a 12-layer, 12-head DPDT \(\) using GPT2-SMALL, randomly initialize cross-task prompts \(P_{c}\) and low-rank vectors \(v_{k},u_{k}\). for\(t=1\) to \(N\)do for\(k\) in \(S\)do  Select a trajectory \(\) that contains \(M\) samples in task \(k\).  Calculate \(P_{k}^{}\) by Equation 3.  Calculate \(L_{MSE}\) and \(L_{dis}\) according to Equations 4 and 5.  Computed loss function by Equation 6. \(-_{}_{Total}\). endfor endfor ```

**Algorithm 2** Productive Training

### Test Time Adaptation

During the test time adaptation (TTA) phase, we address distribution bias by aligning the distribution of unlabeled test samples with the training samples. For each test task \(t\) in the test task set \(T\), we randomly select a subset \(X\) of unlabeled test samples, combine them with the cross-task prompts \(P_{c}\) and input them into the model.

Here, we introduce the data collection method for \(X\). The model's testing phase usually occurs in a simulated environment where we predefine our expected reward values \(\). The environmentprovides the initial state \(s\) of the environment, consistent with the settings during inference in prompt DT methods. However, unlike in training tasks where ground-truth labels exist, for action \(a_{1}\), we assign a value sampled randomly from the action space (which is typically consistent between training and testing tasks). We feed this sequence of Markov chains into the environment, obtaining rewards and the next environment states iteratively, assigning a randomly sampled value to action \(a_{2}\) in subsequent iterations. This process is repeated \(|X|\) times, resulting in data of the form \((0,s_{0},a_{0},1,s_{1},a_{1},,_{|N|},s_{|N|},a_{|N|})\).

In each layer of the model, we calculate the alignment loss based on the means and variances of the training and test samples. Our goal is to update the \(P_{c}\) for the given test task through this alignment loss. For each test task \(t\), we denote the distribution of the test samples as \(\) and the distribution of the training samples as \(\). Specifically, we calculate the aligned token mean and variance via:

\[_{l}()=_{i=1}^{|X|}H_{l,i},_{l}^{2}( )=_{i=1}^{|X|}[H_{l,i}-_{l}() ]^{2}\] (7)

Here, \(H_{l,i}\) represents the state of the \(i^{th}\) sample at the \(l^{th}\) hidden layer, while \(_{l}()\) and \(_{l}^{2}()\) denote the mean and variance of all test samples at the \(l^{th}\) hidden layer, respectively. Similarly, for each hidden layer of the model, we pre-calculate the statistical measures of mean \(_{l}()\) and variance \(_{l}^{2}()\) of the training samples, which are uniformly sampled across all tasks in the training set, in an offline setting to reduce parallel computing costs, since both training samples and labels are accessible. The formula for calculating the alignment loss function is as follows:

\[L_{}=_{l=1}^{L}(\|_{l}()-_{l }()\|_{1}+\|_{l}^{2}()-_{l}^{2}()\|_{1}).\] (8)

The test time adaptation phase process is illustrated in Algorithm 2.

```
0: Test samples set \(X\), Cross-task prompts \(P_{c}\), \(_{l}()\), \(_{l}^{2}()\), The number of layers \(L\).
1:for\(l=1\) to \(L\)do
2:for\(i\) in \(X\)do
3: Calculate \(H_{l,i}\) obtained by inputting the concatenation of \(P_{c}\) and \(i\) into DPDT.
4:endfor
5:endfor
6:for\(l=1\) to \(L\)do
7: Compute \(_{l}()\) and \(_{l}^{2}()\) by Equation 7.
8:endfor
9: Compute token distribution alignment loss by Equation 8.
10: Optimize \(L_{}\) to update \(P_{c}\). ```

**Algorithm 2** Test Time Adaptation

## 5 Experiment

In this section, we present an extensive evaluation of our proposed DPDT using widely recognized benchmarks. Additionally, we conduct empirical ablation studies to dissect and understand the individual contributions of the core components of our methodology.

### Environments and Baselines

**Environments.** To ensure a fair comparison with existing multi-task offline reinforcement learning algorithms, we conducted verification of DPDT using the MuJoCo  and MetaWorld  benchmarks, which serve as standard tasks in the domain of sequence offline RL, offering sufficient diversity and representing common challenges in classical RL, such as sparse rewards, complex state spaces, and precise control of robotic systems. Our experiments on the Cheetah-dir, Cheetah-vel, and Ant-dir environments in the MuJoCo benchmark meticulously adhere to the datasets and methodologies outlined in Prompt-DT. These tasks penalize agents for using excessive control signals. In the MetaWorld benchmark, we used the ML10, ML45, MT10 and MT50 environments for Meta-RL. A detailed description of the datasets and the division of training and test tasks is provided in Appendix A.

**Baselines.** We compared DPDT to the following offline RL baselines: (1) **Multi-task Behaviour Cloning (MT-BC) **: MT-BC optimizes multi-task learning by exclusively simulating trajectories from the original dataset, dispensing with the need for prompts and reward-to-go tokens. This approach emphasizes the utilization of intrinsic task-specific information, adopting a behavior cloningstrategy to streamline the learning process. (2) **Multi-Task Decision Transformer (MT-ORL) **: We train a decision transformer to learn multiple tasks from the training set. To construct the MT-DT, we exclude prompt augmentation present in DPDT, while retaining the rest of the training process identical to that of DPDT. (3) **Soft-Prompt **: Soft-prompt is trained using a universal prompt across all tasks. (4) **Hyper-decision transformer (HDT) **: HDT efficiently adapts DT to new tasks by augmenting them with an adaptation module, whose parameters are initialized by a hyper-network, enabling quick and efficient adaptation with minimal data. (5) **Prompt-DT **: Prompt-DT builds on DT, leveraging trajectory prompts and reward-to-go for multi-task learning and generalization to unseen tasks.

**Implementation details.** All experiments were carried out on a server with 8 NVIDIA 3090 GPUs, each with 24GB of memory, using PyTorch  and Hugging Face Transformers libraries . The experimental hyperparameter configurations are shown in Appendix B. The computer resources utilized by all methods are shown in Table 12.

### Main Results and Analysis

**Zero-shot Generalization.** In Table 1, we compare the zero-shot generalization ability of DPDT and the baselines to investigate the overall performance of DPDT. For evaluation, we use the average episode cumulative returns in the test task set as the evaluation metric. Additionally, we introduce a variant of DPDT that does not use GPT-SMALL parameters for initialization, referred to as DPDT-WP (DPDT-Without Pretrained). Soft prompts adapt to tasks in a parameter-efficient way. However, training a universal prompt across multiple tasks suffers from significant gradient interference, as demonstrated by the experimental results. The prompt-DT performs well in few-shot scenarios due to its utilization of test data for fine-tuning. However, in downstream tasks where data is scarce, the prompt fails to provide sufficient task-specific guidance to the model, resulting in suboptimal performance. Importantly, our proposed DPDT exhibits significant performance improvements over fine-tuning and prompt-based methods. This vividly demonstrates the distinct advantages offered by our innovative multitask training techniques. It is worth noting that without initialization with PLM, the performance of DPDT-WP is inferior to most methods. We believe this is mainly due to the model lacking sufficient prior knowledge for effective multi-task prompt tuning, resulting in suboptimal performance of DPDT. In Figure 2, we illustrate the accumulated returns curves of DPDT and other baselines across the Cheetah-vel, MW ML45, and MW MT50 environments. Additional curves for other environments can be found in Figure 4. We also conducted experiments on Soft-Prompt and Prompt-DT combined with TTA (shown in Figure 11). Soft-Prompt-TTA showed performance improvements across all tasks, whereas Prompt-DT-TTA experienced performance declines in some tasks. The main reason for this is that Prompt-DT relies on high-quality trajectory data for prompts during testing, and applying TTA on unlabeled data may have adversely affected prompt optimization.

**Few-shot Generalization.** We explored the performance of DPDT in few-shot scenarios and further investigated whether the prompt decomposition mechanism successfully isolated general knowledge. In this scenario, DPDT does not use TTA to align cross-task prompts \(P_{c}\). Instead, \(P_{c}\) is fine-tuned

    & **MT-BC ** & **MT-ORL ** & **Soft-Prompt ** & **HDT ** & **Prompt-DT ** & **DPDT-WP** & **DPDT** \\  Trainable Params & 125.5M & 125.5M & 3.94M & 12.94M & 125.5M & 1.42M & 1.42M \\ Percentage & 100\% & 100\% & 3\% & 10.31\% & 100\% & 1.14\% & 1.14\% \\ 
**Cheeta-dir** & -24.714\({}_{141.04}\) & -86.924\({}_{241.51}\) & -4.214\({}_{5.51}\) & -45.324\({}_{213.22}\) & -7.924\({}_{29.97}\) & 11.73\({}_{21.8}\) & **50.324\({}_{211.67}\)** \\
**Cheeta-vd** & -201.66\({}_{24.02}\) & -148.24\({}_{22.18}\) & -171.23\({}_{20.58}\) & -162.75\({}_{220.50}\) & -192.38\({}_{211.81}\) & -143.14\({}_{212.40}\) & **-139.88\({}_{218.65}\)** \\
**Ant-dir** & 131.89\({}_{21.96}\) & 109.21\({}_{21.96}\) & 119.45\({}_{41.2}\) & 115.43\({}_{210.22}\) & **123.46\({}_{49.70}\)** & 101.49\({}_{217.14}\) & 121.84\({}_{501}\) \\
**MW ML10** & 256.77\({}_{11.59}\) & 343.16\({}_{19.64}\) & 246.42\({}_{23.60}\) & 292.14\({}_{48.21}\) & 317.31\({}_{214.96}\) & 204.88\({}_{228.88}\) & **371.01\({}_{210.94}\)** \\
**MW ML45** & 287.37\({}_{31.73}\) & 266.74\({}_{23.81}\) & 91.97\({}_{21.41}\) & 274.88\({}_{289.71}\) & 294.55\({}_{287.81}\) & 300.74\({}_{215.74}\) & **347.21\({}_{211.82}\)** \\
**MW MT 10** & 547.83\({}_{21.01}\) & 1064.58\({}_{21.78}\) & 201.23\({}_{22.71}\) & 964.57\({}_{215.34}\) & 1087.54\({}_{17.170}\) & 1015.91\({}_{20.74}\) & **1317.52\({}_{23.82}\)** \\
**MW MT 50** & 582.80\({}_{41.48}\) & 929.74\({}_{22.21}\) & 400.71\({}_{23.60}\) & 820.45\({}_{227.19}\) & 994.63\({}_{35.99}\) & 1131.01\({}_{11.17}\) & **1559.94\({}_{24.20}\)** \\ 
**Average** & 225.76 & 354.04 & 130.62 & 309.79 & 837.66 & **518.28** \\   

Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).

directly on a small number of labeled test samples through a self-supervised paradigm. Specifically, we randomly selected only one trajectory from the test dataset for fine-tuning. Other methods adhered to the same amount of fine-tuning data. Furthermore, in few-shot scenarios, we fully fine-tuned DPDT on the complete training and testing data, denoted as DPDT-F. The performance of the DPDT-F method represents the upper bound of all model performances in the current environment. Table 2 shows that the DPDT method, even after fine-tuning, still significantly outperforms or matches the baseline algorithms, demonstrating the effectiveness of prompt decomposition in few-shot environments. Moreover, in some datasets, DPDT approaches the performance of fully fine-tuned models on the test set using only 1.14% of the parameters, as observed in the cheetah-vel environment. It is worth noting that in the Ant-dir environment, the performance of DPDT is slightly inferior to that of Prompt-DT in both zero-shot and few-shot settings. We believe the primary reason for this is the significant domain difference between the language model and environment.

### Further Analysis

In the ablation studies, we conducted research on the components of DPDT as well as the impact of prompt length and model parameters on convergence speed and performance.

**Impact of model components.** As shown in Table 3, the impact of prompt decomposition was evaluated. Compared to the soft-prompt method in the unseen task settings (first row), substituting it with decomposed prompts \(P_{k}\) and \(P_{c}\) without distillation (third row) resulted in performance improvements across all three tasks. This ablation highlights the significance of the prompt decomposition strategy in DPDT, demonstrating that the shared component adeptly captures the diverse cross-task knowledge essential for enhancing target downstream tasks.

To evaluate the impact of prompt distillation, we trained a standard prompt shared across all training tasks using the same training loss as DPDT. The teacher prompts for each task remained consistent in DPDT. Compared to the basic baseline (first row), incorporating prompt distillation (second row)

    & **MT-ORL ** & **Soft-Prompt ** & **HDT ** & **Prompt-DT ** & **DPDT-WP** & **DPDT** & **DPDT-F** \\  Trainable Params & 125.5M & 3.94 M & 12.94 M & 125.5M & 1.42M & 1.42M & 125.5M \\ Percentage & 100\% & 3\% & 10.31\% & 100\% & 1.14\% & 1.14\% & 100\% \\ 
**Cheetah-dir** & -46.22\({}_{24.34}\) & 940.24\({}_{11.08}\) & 875.23\({}_{24.24}\) & 934.78\({}_{53.53}\) & 946.81\({}_{417.24}\) & **955.17\({}_{48.03}\)** & 1037.85\({}_{55.08}\) \\
**Cheetah-vel** & -146.64\({}_{21.22}\) & -41.81\({}_{21.20}\) & -63.81\({}_{46.30}\) & -37.80\({}_{42.09}\) & -48.07\({}_{41.85}\) & **-30.73\({}_{41.88}\)** & -29.85\({}_{55.66}\) \\
**Ant-dir** & 110.51\({}_{21.2}\) & 379.10\({}_{41.75}\) & 361.49\({}_{53.53}\) & **411.96\({}_{53.28}\)** & 308.10\({}_{52.22}\) & 384.29\({}_{41.01}\) & 400.01\({}_{59.79}\) \\
**MW ML10** & 421.22\({}_{21.20}\) & 379.82\({}_{21.76}\) & 467.81\({}_{21.07}\) & 315.07\({}_{46.17}\) & 485.27\({}_{48.191}\) & **535.52\({}_{41.79}\)** & 67.02\({}_{43.88}\) \\
**MW ML45** & 264.14\({}_{49.67}\) & 448.72\({}_{21.18}\) & 477.19\({}_{22.16}\) & 473.34\({}_{41.21}\) & 519.28 \({}_{47.22}\) & **579.09\({}_{50.04}\)** & 600.44\({}_{41.74}\) \\
**Average** & 120.60 & 421.204 & 423.56 & 419.47 & 442.27 & **484.66** & 535.74 \\   

Table 2: Results for Meta-RL control tasks (few-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).

Figure 2: Episodic accumulated returns in three tasks of MTBC, MT-ORL, Soft-Prompt, HDT, Prompt-DT, DPDT-WP and DPDT. Each method is restricted to 1500 rounds of runs in each environment.

resulted in a modest improvement in average performance across the three tasks. This emphasizes that prompt distillation from separately trained source prompts is an effective strategy for acquiring high-quality decomposable prompts.

To compare the impact of using TTA on model performance, we examined the results in the fourth and fifth rows. We found that the use of TTA affects the model's final performance. The reason is intuitive: cross-task prompts provide a good initialization environment for TTA, but relying solely on the general information from cross-task prompts is insufficient for the model to perform well on unseen tasks. Incorporating TTA allows the model to adapt to the specific nuances of each task during testing, resulting in substantial performance gains.

**Impact of prompt length.** We examined the influence of prompt length on the performance of DPDT by investigating five distinct prompt lengths (3, 6, 30, 60, 90). Specifically, we explored the effect of prompt length variation on the convergence behavior and generalization capability of the model. It is widely recognized that prompt lengths that are excessively short may impede model convergence, while prompt lengths that are overly long can result in slow convergence rates and potential overfitting. Ablation experiments revealed that a prompt length of 30 is optimal. Further increasing the prompt length to 60 or 90, however, results in minor performance fluctuations but increases the convergence time. Therefore, we used a prompt length of 30 for all our experiments.

**Impact of model size.** We explored the performance of DPDT under three model size configurations. The (3,1,128) configuration uses the pretrained model provided in the original Prompt-DT  to initialize DPDT, while the (24,16,768) configuration employs GPT-MIDDLE. Table 4 shows that the size of the pretrained model parameters is correlated with the performance improvement of DPDT. When the model size is expanded to a certain extent (12,12,768), efficient parameter fine-tuning can extract adequate prior knowledge for downstream tasks. However, as the model complexity increases, such as when reaching the size of GPT-MIDDLE, the model exhibits performance improvement on some tasks (Ant-dir) but a decrease in performance on others. This phenomenon can be attributed to the significant gap between the size of the dataset and the complexity of the model. Fine-tuning the model in such scenarios may encounter challenges in appropriately converging for reinforcement learning tasks, potentially leading to overfitting.

**Impact of data quality.** The quality of data used for fine-tuning cross prompts does indeed affect the final model performance. We conducted experiments focusing on data quality. In the Cheetah-vel and ML45 environments, we differentiated the quality of datasets into expert, medium, random, and mixed datasets. Each dataset consists of 200 time steps, which aligns with the setup for few-shot scenarios relative to the size of the training set. As shown in Table 5, we fou

   Model size & Cheetah-vel & Ant-dir & MW ML45 & MW MT50 \\  (3,1,128) & -164.88 & 129.34 & 288.14 & 749.18 \\ (12,12,768) & **-139.88** & 121.84 & **347.21** & **1559.94** \\ (24,16,768) & -210.35 & **165.99** & 292.48 & 1527.34 \\   

Table 4: Ablation: The impact of model size. The elements of the triplet represent, in order, the number of transformer blocks, the count of attention heads, and the size of the hidden layers.

Figure 3: Ablation: The effect of prompt length on DPDT’s zero-shot generalization ability.

   Decomposition & Distillation & TTA & Cheetah-vel & MW ML45 & MW MT50 \\  ✗ & ✗ & ✗ & -171.23 & 91.97 & 400.71 \\ ✗ & ✗ & ✗ & -163.05 & 108.01 & 709.81 \\ ✗ & ✗ & ✓ & -160.10 & 273.99 & 1137.39 \\ ✗ & ✗ & ✗ & -167.80 & 149.21 & 824.07 \\ ✗ & ✗ & ✓ & **-139.88** & **347.21** & **1559.94** \\   

Table 3: Ablation: The impact of prompt decomposition, prompt distillation and test time adaptation.

using expert datasets perform the best, which aligns with our intuition. Additionally, the performance of models fine-tuned on mixed datasets is close to that on expert datasets, suggesting implicitly that the DPDT method can extract information from suboptimal datasets to ensure model performance.

**Impact of adaptation method.** In addition to only utilizing cross-task prompts \(P_{c}\) for TTA in zero-shot scenarios, we also investigated (1) combining cross-task prompts \(P_{c}\) with the average of all task-specific prompts \(P_{k}\) from the training set for TTA, (2) freezing the cross-task prompts \(P_{c}\), we initialized a new task-specific prompt combined with the cross-task prompts for TTA and (3) randomly selecting one \(P_{k}\) from a training task and combining it with \(P_{c}\) for TTA. However, we found that all of these initialization methods resulted in suboptimal outcomes, shown in Table 10.

**Impact of learning rate in prompt decomposition.** As shown in Table 6, we present experimental results on the ML45 dataset where different learning rates were applied to \(P_{c}\) and \(P_{k}\) in prompt decomposition. We observed optimal performance when both had the same learning rate. We speculate that this occurs because, over training iterations, both prompts converge to their optimal values, and differing learning rates disrupt their joint convergence, leading to poorer performance under similar runtime conditions.

**Impact of low-rank parameter \(r\).** Table 7 presents the results of our ablation experiments focusing on the low-rank parameter \(r\) of prompt decomposition for Cheeta-vel and MW ML45. DPDT is relatively insensitive to selecting hyperparameters, a potential advantage of our work. As observed, the model's performance varies with different values of \(r\). These findings suggest that the performance of DPDT remains stable across different values of \(r\). This characteristic can be advantageous, as it allows users to implement the model without extensive tuning, streamlining the deployment process while still achieving competitive results across diverse tasks.

## 6 Conclusion, Limitation and Broader Impact

We have introduced a novel approach, the Decomposed Prompt Decision Transformer (DPDT), aimed at efficient generalization to unseen tasks. Through the utilization of PLMs for parameter initialization and the implementation of parameter-efficient multi-task prompt tuning techniques, we have successfully extracted cross-task general knowledge and further fine-tuned it on previously unseen tasks. Our experiments across various Meta-RL environments demonstrated the effectiveness of our components, achieving superior performance with significantly fewer task-specific parameters compared to fully fine-tuned methods. This approach offers a robust framework for future research to further explore and optimize multi-task learning and generalization capabilities.

**Limitation.** Currently, our work primarily involves using large language models to initialize DPDT. While we've utilized parameter-efficient techniques to fine-tune the model and mitigate inter-domain variances, focusing on optimizing these differences could potentially enhance model performance.

**Broader Impact.** Overall, the application of PEFT methods based on PLMs can help users obtain high-quality reinforcement learning decision models at minimal cost. However, this approach may lead to the misuse of language models when users are unaware of the inter-domain differences, potentially resulting in unforeseen negative outcomes in the RL decision-making process.