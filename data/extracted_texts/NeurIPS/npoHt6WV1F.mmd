# NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes

NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes

 Hao-Lun Sun\({}^{1}\), Lei Hsiung\({}^{2}\), Nandhini Chandramoorthy\({}^{3}\), Pin-Yu Chen\({}^{3}\), Tsung-Yi Ho\({}^{4}\)

\({}^{1}\) National Tsing Hua University \({}^{2}\) Dartmouth College \({}^{3}\) IBM Research

\({}^{4}\) The Chinese University of Hong Kong

s109062594@m109.nthu.edu

lei.hsiung.gr@dartmouth.edu

{pin-yu.chen, nandhini.chandramoorthy}@ibm.com

tyho@cse.cuhk.edu.hk

###### Abstract

Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains problematically high. An effective strategy for reducing such consumption is supply-voltage reduction, but if done too aggressively, it can lead to accuracy degradation. This is due to random bit-flips in static random access memory (SRAM), where model parameters are stored. To address this challenge, we have developed NeuralFuse, a novel add-on module that handles the energy-accuracy tradeoff in low-voltage regimes by learning input transformations and using them to generate error-resistant data representations, thereby protecting DNN accuracy in both nominal and low-voltage scenarios. As well as being easy to implement, NeuralFuse can be readily applied to DNNs with limited access, such cloud-based APIs that are accessed remotely or non-configurable hardware. Our experimental results demonstrate that, at a 1% bit-error rate, NeuralFuse can reduce SRAM access energy by up to 24% while recovering accuracy by up to 57%. To the best of our knowledge, this is the first approach to addressing low-voltage-induced bit errors that requires no model retraining.

+
Footnote †: Code: https://github.com/IBM/NeuralFuse

+
Footnote †: Code: https://github.com/IBM/NeuralFuse

## 1 Introduction

Energy-efficient computing is of primary importance to the effective deployment of deep neural networks (DNNs), particularly in edge devices and in on-chip AI systems. Increasing DNN computation's energy efficiency and lowering its carbon footprint require iterative efforts from both chip designers and algorithm developers. Processors with specialized hardware accelerators for AI computing, capable of providing orders of magnitude better performance and energy efficiency for AI computation, are now ubiquitous. However, alongside reduced precision/quantization and architectural optimizations, endowing such systems with the capacity for low-voltage operation is a powerful lever for reducing their power consumption.

The computer engineering literature contains ample evidence of the effects of undervolting and low-voltage operation on accelerator memories that store weights and activations during computation. Aggressive scaling-down of static random access memory's (SRAM's) supply voltage to below the rated value saves power, thanks to the quadratic dependence of dynamic power on voltage. Crucially,however, it also leads to an exponential increase in bit failures. Memory bit flips cause errors in the stored weight and activation values (Chandramoorthy et al., 2019; Ganapathy et al., 2017), leading to catastrophic accuracy loss.

A recent wave of research has proposed numerous techniques for allowing low-voltage operation of DNN accelerators while preserving their accuracy. Most of these have been either hardware-based error-mitigation techniques or error-aware robust training of DNN models. On-chip error mitigation methods have significant performance and power overheads (Chandramoorthy et al., 2019; Reagen et al., 2016). On the other hand, some have proposed to generate models that are robust to bit errors via a specific learning algorithm (Kim et al., 2018; Koppula et al., 2019; Stutz et al., 2021), thereby eliminating the need for on-chip error mitigation. However, error-aware robust training to find the optimal set of robust parameters for each model is time- and energy-intensive and may not be possible in all access-limited settings.

In this paper, therefore, we propose a novel model-agnostic approach: _NeuralFuse_. This proof-of-concept machine-learning module offers trainable input transformation parameterized by a relatively small DNN; and, by enhancing input's robustness, it mitigates bit errors caused by very low-voltage operation, thus serving the wider goal of more accurate inferencing. The pipeline of NeuralFuse is illustrated in Figure 1. To protect the deployed models from making wrong predictions under low-power conditions, NeuralFuse accepts scenarios under access-limited neural networks (e.g., non-configurable hardware or cloud-based APIs). Specifically, we consider two access-limited scenarios that are common in the real world: 1) _relaxed access_, in which 'black box' model details are unknown, but backpropagation through those models is possible; and 2) _restricted access_, in which the model details are unknown and backpropagation is disallowed. To enable it to deal with relaxed access, we trained NeuralFuse via backpropagation, and for restricted-access cases, we trained it on a white-box surrogate model. To the best of our knowledge, this is the first study that leverages a learning-based method to address random bit errors as a means of recovering accuracy in low-voltage and access-limited settings.

We summarize our **main contributions** as follows:

* We propose _NeuralFuse_, a novel learning-based input-transformation module aimed at enhancing the accuracy of DNNs that are subject to random bit errors caused by very low voltage operation. NeuralFuse is model-agnostic, i.e., operates on a plug-and-play basis at the data-input stage and does not require any re-training of deployed DNN models.
* We explore two practical limited-access scenarios for neural-network inference: relaxed access and restricted access. In the former setting, we use gradient-based methods for module training. In the latter one, we use a white-box surrogate model for training, which is highly transferable to other types of DNN architecture.
* We report the results of an extensive program of experiments with various combinations of DNN models (ResNet18, ResNet50, VGG11, VGG16, and VGG19), datasets (CIFAR-10, CIFAR-100,

Figure 1: (a) At inference, NeuralFuse transforms input samples \(\) into robust data representations. The _nominal_ voltage allows models to work as expected, whereas at _low voltage_, one would encounter bit errors (e.g., \(1\%\)) that cause incorrect inferences. The percentages reflect the accuracy of a CIFAR-10 pre-trained ResNet18 with and without NeuralFuse in both those voltage cases. (b) On the same base model (ResNet18), we illustrate the energy/accuracy tradeoff of six NeuralFuse implementations. The x-axis represents the percentage reduction in dynamic-memory access energy at low-voltage settings (base model protected by NeuralFuse), as compared to the bit-error-free (nominal) voltage. The y-axis represents the perturbed accuracy (evaluated at low voltage) with a \(1\%\) bit-error rate.

GTSRB, and ImageNet-10), and NeuralFuse implementations of different architectures and sizes. These show that NeuralFuse can consistently increase the perturbed accuracy (accuracy evaluated under random bit errors in weights) by up to \(57\%\), while simultaneously saving up to \(24\%\) of the energy normally required for SRAM access, based on our realistic characterization of bit-cell failures for a given memory array in a low-voltage regime inducing a \(0.5\%/1\%\) bit-error rate.
* We demonstrate NeuralFuse's transferability (i.e., adaptability to unseen base models), versatility (i.e., ability to recover low-precision quantization loss), and competitiveness (i.e., state-of-the-art performance) in various scenarios, establishing it as a promising proof-of-concept for energy-efficient, resilient DNN inference.

## 2 Related Work and Background

Software-based Energy-saving Strategies.Various recent studies have proposed software-based methods of reducing computing's energy consumption. For instance, quantization techniques have been reported to reduce the precision required for storing model weights, and thus to decrease total memory storage (Gong et al., 2014; Rastegari et al., 2016; Wu et al., 2016). On the other hand, Yang et al. (2017) - who proposed energy-aware pruning on each layer and fine-tuning of weights to maximize final accuracy - suggested several ways to reduce DNNs' energy consumption. For example, they devised the ECC framework, which compresses DNN models to meet a given energy constraint (Yang et al., 2019), and a method of compressing such models via joint pruning and quantization (Yang et al., 2020). It is also feasible, during DNN training, to treat energy constraints as an optimization problem and thereby reduce energy consumption while maximizing training accuracy (Yang et al., 2019). However, unlike ours, all these methods imply changing either model architectures or model weights.

Hardware-based Energy-saving Strategies.Prior studies have also explored ways of improving energy efficiency via specially designed hardware. Several of them have focused on the undervolting of DNN accelerators and proposed methods to maintain accuracy in the presence of bit errors. For instance, Reagen et al. (2016) proposed an SRAM fault-mitigation technique that rounds faulty weights to zero to avoid degradation of prediction accuracy. Srinivasan et al. (2016) recommended storing sensitive MSBs (most significant bits) in robust SRAM cells to preserve accuracy. Chandramoorthy et al. (2019) proposed dynamic supply-voltage boosting to improve the resilience of memory-access operations; and the learning-based approach proposed by Stutz et al. (2021) aims to find models that are robust to bit errors. The latter paper discusses several techniques for improving such robustness, notably quantization, weight-clipping, random bit-error training, and adversarial bit-error training. Its authors concluded from their experiments that a combination of quantization, weight-clipping, and adversarial bit-error training will yield excellent performance. However, they also admitted that the relevant training process was sensitive to hyperparameter settings, and hence, it might come with a challenging training procedure.

However, we suggest that all the methods mentioned above are difficult to implement and/or unsuitable for use in real-world access-limited settings. For example, the weights of DNN models packed on embedded systems may not be configurable or updatable, making model retraining (e.g., Stutz et al. (2021)) non-viable in that scenario. Moreover, DNN training is already a tedious and time-consuming task, so adding error-aware training to it may further increase its complexity and, in particular, make hyperparameter searches more challenging. Ozdenizci and Legenstein (2022) also reported that error-aware training was ineffective for large DNNs with millions of bits. NeuralFuse obviates the need for model retraining via an add-on trainable input-transformation function parameterized by a relatively small secondary DNN.

SRAM Bit Errors in DNNs.Low voltage-induced memory bit-cell failures can cause bit-flips from 0 to 1 and vice versa. In practice, SRAM bit errors increase exponentially when the supply voltage is scaled below \(V_{min}\), i.e., the minimum voltage required to avoid them. This phenomenon has been studied extensively in the prior literature, including work by Chandramoorthy et al. (2019) and Ganapathy et al. (2017). The specific increases in bit errors as voltage scales down, in the case of an SRAM array of \(512 64\) bits with a 14nm technology node, is illustrated in Figure 2. The corresponding dynamic energy per SRAM read access, measured at each voltage at a constant frequency, is shown on the right-hand side of the figure. In this example, accessing the SRAM at 0.83\(V_{min}\) leads to a \(1\%\) bit-error rate, and at the same time, dynamic energy per access is reduced by approximately \(30\%\). This can lead to DNNs making inaccurate inferences, particularly when bit-flips occur at the MSBs. However, improving robustness to bit errors can allow us to lower \(V_{min}\) and exploit the resulting energy savings.

It has been observed that bit-cell failures for a given memory array are randomly distributed and independent of each other. That is, the spatial distribution of bit-flips can be assumed to be random, as it generally differs from one array to another, within as well as between chips. Below, following Chandramoorthy et al. (2019), we model bit errors in a memory array of a given size by generating a random distribution of such errors with equal likelihood of 0-to-1 and 1-to-0 bit-flipping. More specifically, we assume that the model weights are quantized to 8-bit precision (i.e., from 32-bit floats to 8-bit integers), and generate perturbed models by injecting our randomly distributed bit errors into the two's complement representation of weights. For more implementation details, please refer to Section 4.1.

## 3 NeuralFuse: Framework and Algorithms

### Error-Resistant Input Transformation

As illustrated in Figure 1, we propose a novel trainable input-transformation module, NeuralFuse, parametrized by a relatively small DNN, to mitigate the accuracy-energy tradeoff for model inference and thus overcome the drawback of performance degradation in low-voltage regimes. A specially designed loss function and training scheme are used to derive NeuralFuse and apply it to the input data such that the transformed inputs will become robust to low voltage-induced bit errors.

Consider the input \(\) sampled from the data distribution \(\) and a set of models \(_{p}\) with \(p\%\) random bit errors on weights (i.e., _perturbed_ models). When it is not manifesting any bit errors (i.e., at normal-voltage settings), the perturbed model operates as a nominal deterministic one, denoted by \(M_{0}\). NeuralFuse aims to ensure that a model \(M_{p}_{p}\) can make correct inferences on the transformed inputs while also delivering consistent results in its \(M_{0}\) state. To adapt to various data characteristics, NeuralFuse - designated as \(\) in Eq. (1), below - is designed to be input-aware. This characteristic can be formally defined as

\[()=_{[-1,1]}+( ),\] (1)

where \(()\) is a "generator" (i.e., an input-transformation function) that can generate a perturbation based on input \(\). As transformed by NeuralFuse, i.e., as \(()\), that input is passed to the deployed model (\(M_{0}\) or \(M_{p}\)) for final inference. Without loss of generality, we assume the transformed input lies within a scaled input range \(()[-1,1]^{d}\), where \(d\) is the (flattened) dimension of \(\).

### Training Objective and Optimizer

To train our generator \(()\), which ought to be able to ensure the correctness of both the perturbed model \(M_{p}\) and the clean model \(M_{0}\), we parameterized it with a neural network and apply our training objective function

\[_{_{}}_{M_{0}}(y|( ;_{}))+_{M_{p} _{p}}[_{M_{p}}(y|(;_{}))],\] (2)

Figure 2: The bit-error rates (left) and dynamic energy per memory access versus voltage for static random access memory arrays (right) as reported by Chandramoorthy et al. (2019). The x-axis shows voltages normalized with respect to the minimum bit error-free voltage (\(V_{min}\)).

where \(_{}\) is the set of trainable parameters for \(\); \(y\) is the ground-truth label of \(\); \(_{M}\) denotes the likelihood of \(y\) as computed by a model \(M\) being given a transformed input \((;_{})\); \(_{p}\) is the distribution of the perturbed models inherited from the clean model \(M_{0}\), under a \(p\%\) random bit-error rate; and \(\) is a hyperparameter that balances the importance of the nominal and perturbed models.

The training objective function can be readily converted to a loss function (\(\)) that evaluates cross-entropy between the ground-truth label \(y\) and the prediction \(_{M}(y|(;_{})\). That is, the total loss function can be calculated as

\[_{}=_{M_{0}}+_{ _{p}}.\] (3)

In particular, optimizing the loss function requires evaluation of the impact of the loss term \(_{_{p}}\) on randomly perturbed models. Our training process is inspired by expectation over transformation (EOT) attacks (Athalye et al., 2018), which aim to produce robust adversarial examples that are simultaneously adversarial over the entire transformation distribution. Based on that idea, we propose a new optimizer for solving Eq. (3), which we call expectation over perturbed models (EOPM). EOPM-trained generators can generate error-resistant input transformations and mitigate inherent bit errors. However, it would be computationally impossible to enumerate all possible perturbed models with random bit errors, and the number of realizations for perturbed models is constrained by the memory size of the GPUs used for training. In practice, therefore, we only use \(N\) perturbed models per iteration to calculate empirical average loss, i.e.,

\[_{_{p}}_{M_{p_{1}}}++ _{M_{p_{N}}}}{N},\] (4)

where \(N\) is the number of perturbed models \(\{M_{p_{1}},,M_{p_{N}}\}\) that are simulated to calculate the loss caused by random bit errors. Therefore, the gradient used to update the generator can be calculated as follows:

\[_{}}{_{}}= _{M_{0}}}{_{}}+(_{M_{p_{1}}}}{_{ }}++_{M_{p_{N}}}}{_{}}).\] (5)

Through our implementation, we found that stable performance could be delivered when \(N=10\), and that there was little to be gained by using a larger value. The results of our ablation study for different values of \(N\) can be found in Appendix E.

### Training Algorithm

Algorithm 1 in Appendix A summarizes NeuralFuse's training steps. Briefly, this involves splitting the training data \(\) into \(B\) mini-batches for training the generator in each epoch. For each mini-batch, we first feed these data into \(()\) to obtain the transformed inputs. Also, we simulate \(N\) perturbed models using a \(p\%\) random bit-error rate, denoted by \(M_{p_{1}},,M_{p_{N}}\), from \(_{p}\). Then, the transformed inputs are fed into those \(N\) perturbed models as well as into the clean model \(_{0}\), and their respective losses and gradients are calculated. Finally, NeuralFuse parameters \(_{}\) are updated based on the gradient obtained by EOPM.

## 4 Experiments

### Experiment Setups

**Datasets.** We evaluate NeuralFuse on four different datasets: CIFAR-10 (Krizhevsky and Hinton, 2009), CIFAR-100 (Krizhevsky and Hinton, 2009), the German Traffic Sign Recognition Benchmark (GTSRB) (Stallkamp et al., 2012), and ImageNet-10 (Deng et al., 2009). CIFAR-10 consists of 10 classes, with 50,000 training images and 10,000 testing images in total. Similarly, CIFAR-100 consists of 100 classes, with 500 training images and 100 testing images in each. The GTSRB contains 43 classes with a total of 39,209 training images and 12,630 testing images. Similar to CIFAR-10 and CIFAR-100, we resize GTSRB into 32\(\)32\(\)3 in our experiment. For ImageNet-10, we chose the same ten categories as Huang et al. (2022), in which there are 13,000 training images and 500 test images cropped into 224\(\)224\(\)3. Due to space limitations, our CIFAR-100 results are presented in Appendices F and G.

**Base Models.** We selected several common architectures for our base models: ResNet18, ResNet50 (He et al., 2016), VGG11, VGG16, and VGG19 (Simonyan and Zisserman, 2015). To replicate the deployment of models on chips, all our based models were given quantization-aware training that followed Stutz et al. (2021).

NeuralFuse Generators.The architecture of the NeuralFuse generator (\(\)) ) is based on an encoder-decoder structure. We designed and compared three types of generators, namely convolution-based, deconvolution-based, and UNet-based. We also considered large(L)/small(S) network sizes for each type. Further details can be found below and in Appendix B.

* **Convolution-based (Conv).** Conv uses convolution with MaxPool layers for its encoder and _convolution with UpSample layers_ for its decoder. This architecture has previously been shown to be efficient and effective at generating _input-aware_ backdoor triggers (Nguyen and Tran, 2020).
* **Deconvolution-based (DeConv).** DeConv uses convolution with MaxPool layers for its encoder and _deconvolution layers_ for its decoder. We expected this modification both to enhance its performance and to reduce its energy consumption.
* **UNet-based (UNet).** UNet uses convolution with MaxPool layers for its encoder, and deconvolution layers for its decoder. UNet is known for its robust performance in image segmentation (Ronneberger et al., 2015).

Energy-consumption Calculation.The energy consumption reported in Figure 1 is based on the product of the total number of SRAM memory accesses in a systolic array-based convolution neural network (CNN) accelerator and the dynamic energy per read access at a given voltage. Research by Chen et al. (2016) previously showed that energy consumption by SRAM buffers and arrays accounts for a high proportion of total system energy consumption. We assume that there are no bit errors on NeuralFuse, given its status as an add-on data preprocessing module whose functions could also be performed by a general-purpose core. In this work we assume it is implemented on the accelerator equipped with dynamic voltage scaling and therefore NeuralFuse computation is performed at nominal error-free voltage. We report a reduction in overall weight-memory energy consumption (i.e., NeuralFuse + Base Model under a \(p\%\) bit-error rate) with respect to the unprotected base model in the regular-voltage mode (i.e., \(0\%\) bit-error rate and without NeuralFuse).

To quantify memory accesses, we used the SCALE-SIM simulator (Samajdar et al., 2020), and our chosen configuration simulated an output-stationary dataflow and a 32\(\)32 systolic array with 256KB of weight memory. We collected data on the dynamic energy per read access of the SRAM both at \(V_{min}\) and at the voltage corresponding to a \(1\%\) bit-error rate (\(V_{ber} 0.83V_{min}\)) from Cadence ADE Spectre simulations, both at the same clock frequency.

Relaxed and Restricted Access Settings.In the first of our experiments' two scenarios, relaxed access, the base-model information was not entirely transparent, but allowed us to obtain gradients from the black-box model through backpropagation. Therefore, this scenario allowed direct training of NeuralFuse with the base model using EOPM. In the restricted-access scenario, on the other hand, only the inference function was allowed for the base model, and we therefore trained NeuralFuse using a white-box surrogate base model and then transfering the generator to the access-restricted model.

Computing Resources.Our experiments were performed using eight Nvidia Tesla V100 GPUs and implemented with PyTorch. NeuralFuse was found to generally take 150 epochs to converge, and its training time was similar to that of the base model it incorporated. On both the CIFAR-10 and CIFAR-100 datasets, average training times were 17 hours (ResNet18), 50 hours (ResNet50), 9 hours (VGG11), 13 hours (VGG16), and 15 hours (VGG19). For GTSRB, the average training times were 9 hours (ResNet18), 27 hours (ResNet50), 5 hours (VGG11), 7 hours (VGG16), and 8 hours (VGG19); and for ImageNet-10, the average training times were 32 hours (ResNet18), 54 hours (ResNet50), 50 hours (VGG11), 90 hours (VGG16), and 102 hours (VGG19).

### Performance Evaluation, Relaxed-access Scenario

Our experimental results pertaining to the relaxed-access scenario are shown in Figure 3. The bit-error rate (BER) due to low voltage was \(1\%\) in the cases of CIFAR-10 and GTSRB, and \(0.5\%\) for ImageNet-10. The BER of ImageNet-10 was lower than that of the other two because, being pre-trained, it has more parameters than either of them. For each experiment, we sampled and evaluated \(N=10\) perturbed models (independent from training), and below, we report the means and standard deviations of their respective accuracies. Below, clean accuracy (CA) refers to a model's accuracy measured at nominal voltage, and perturbed accuracy (PA) to its accuracy measured at low voltage.

In the cases of CIFAR-10 and the GTSRB, we observed that large generators like ConvL and UNetL recovered PA considerably, i.e., in the range of \(41\%\) to \(63\%\) on ResNet18, VGG11, VGG16,and VGG19. ResNet50's recovery percentage was slightly worse than those of the other base models, but it nevertheless attained up to \(51\%\) recovery on the GTSRB. On the other hand, the recovery percentages achieved when we used small generators like DeConvS were worse than those of their larger counterparts. This could be explained by larger-sized networks' better ability to learn error-resistant generators (though perhaps at the cost of higher energy consumption). In the case of ImageNet-10, using larger generators also yielded better PA performance recovery, further demonstrating NeuralFuse's ability to work well with large input sizes and varied datasets.

### Performance Evaluation, Restricted-access Scenario (Transferability)

The experimental results of our restricted-access scenario are shown in Table 1. We adopted ResNet18 and VGG19 as our white-box surrogate source models for training the generators under a \(1.5\%\) bit-error rate. We chose ConvL and UNetL as our generators because they performed best out of the six we tested (see Figure 3).

From Table 1, we can see that transferring from a larger BER such as \(1.5\%\) can endow a smaller one (e.g., \(1\%\)) with strong resilience. The table also makes it clear that using VGG19 as a surrogate model with UNet-based generators like UNetL can yield better recovery performance than other model/generator combinations. On the other hand, we observed in some cases that if we transferred between source and target models of the same type (but with different BERs for training and testing), performance results could exceed those we had obtained during the original relaxed-access scenario. For instance, when we transferred VGG19 with UNetL under a \(1.5\%\) BER to VGG19 or VGG11 under a \(0.5\%\) BER, the resulting accuracies were \(85.86\%\) (as against \(84.99\%\) for original VGG19) and \(84.81\%\) (as against \(82.42\%\) for original VGG11). We conjecture that generators trained on relatively large BERs can cover the error patterns of smaller BERs, and even help improve the latter's generalization. These findings indicate the considerable promise of access-limited base models in low-voltage settings to recover accuracy.

### Energy/Accuracy Tradeoff

We report total dynamic energy consumption as the total number of SRAM-access events multiplied by the dynamic energy of a single such event. Specifically, we used SCALE-SIM to calculate total

Figure 3: Relaxed-access scenario test accuracies (\(\%\)) of various pre-trained models with and without NeuralFuse, compared at nominal voltage (\(0\%\) bit-error rate) or low voltage (with specified bit-error rates). The results demonstrate that NeuralFuse consistently recovered perturbation accuracy.

weight-memory access (TWMA), specifics of which can be found in Appendix C's Table 6. In Table 2, below, we report the percentages of energy saved (ES) at voltages that yield a \(1\%\) bit-error rate for various base-model and generator combinations. The formula for computing ES is

\[=_{NN}-(_{LM}+_{  4N})}{_{NV}} 100\%,\] (6)

where \(NV\) denotes nominal voltage regime, and \(LV\), a low-voltage one.

Our results indicate that when ResNet18 is utilized as a base model, NeuralFuse can recover model accuracy by \(20-49\%\) while reducing energy use by \(19-29\%\). In Appendix C, we provide more results on the tradeoff between energy and accuracy of different NeuralFuse and base-model combinations. Overall, it would appear that using NeuralFuse can effectively restore model accuracy when SRAM encounters low-voltage-induced random bit errors.

Runtime and Latency.On the other hand, runtime and its corresponding energy consumption may also affect overall energy savings. For instance, previous research has shown that multiply-and-accumulate (MAC) operations account for more than 99% of all operations in state-of-the-art CNNs, dominating processing runtime and energy consumption alike Yang et al. (2017). Therefore, we also report the results of our MAC-based energy-consumption estimation in Appendix C, and of our latency analysis in Appendix D. Here, it should also be noted that an additional latency overhead is an inevitable tradeoff for reducing energy consumption in our scenarios. Although neither runtime nor latency is a major focus of this paper, future researchers could usefully design a lighter-weight version of the NeuralFuse module, or apply model-compression techniques to it, to reduce these two factors.

### Model Size and NeuralFuse Efficiency

To arrive at a full performance characterization of NeuralFuse, we analyzed the relationship between the final recovery achieved by each base model in combination with generators of varying parameter counts. For this purpose, we defined _efficiency ratio_ as the recovery percentage in PA divided by NeuralFuse's parameter count. Table 3 compares the efficiency ratios of all NeuralFuse generators

   &  &  &  &  &  &  \\  & & & & & &  &  &  &  &  &  \\   & ResNet18 & 1\% & 92.6 & \(38.9 12.4\) & 89.8 & \(89.0 0.5\) & 50.1 &  & \(85.2 0.5\) & 46.3 \\  & & 0.5\% & & \(70.1 11.6\) & 89.6 \( 0.2\) & 19.5 & & & 85.7 \(\) 0.2 & 15.6 \\   & ResNet50 & 1\% & 92.6 & \(61.1 9.4\) & 89.2 & 36.1 \(\) 18 & 10.0 & 84.4 & \(38.9 16\) & 12.8 \\  & & 0.5\% & & \(61.0 10.3\) & 74.1 \(\) 10 & 13.1 & & & 72.7 \(\) 4.6 & 11.7 \\   & VGG11 & 1\% & 88.4 & \(42.2 11.6\) & 86.3 & 59.2 \(\) 10 & 17.0 & 82.3 & 69.8 \(\) 7.5 & 27.6 \\  & & 0.5\% & & \(63.6 9.3\) & 78.9 \(\) 49 & 15.3 & & & 77.0 \(\) 4.0 & 13.4 \\   & VGG16 & 1\% & 90.3 & \(35.7 7.9\) & 89.4 & 62.2 \(\) 18 & 26.5 & 84.7 & 68.9 \(\) 14 & 33.2 \\  & & 0.5\% & & \(66.6 8.1\) & 81.8 & 83.4 \(\) 5.5 & 16.8 & & 80.5 \(\) 5.9 & 13.9 \\   & VGG19 & 1\% & 90.5 & \(36.0 12.0\) & 89.8 & 49.9 \(\) 23 & 13.9 & & & 55.1 \(\) 17 & 19.1 \\  & & 0.5\% & & \(64.2 12.4\) & 89.8 & 81.8 \(\) 8.5 & 17.6 & & & 78.5 \(\) 6.8 & 14.3 \\   & ResNet18 & 1\% & 92.6 & \(38.9 12.4\) & 88.9 & 62.6 \(\) 13 & 23.7 & & 72.3 \(\) 11 & 33.4 \\  & & 0.5\% & & \(70.1 11.6\) & 88.4 \(\) 7.2 & 14.1 & & & 82.1 \(\) 2.2 & 12.0 \\   & ResNet50 & 1\% & 92.6 & \(26.1 9.4\) & 88.8 & 37.9 \(\) 18 & 11.8 & & & 46.7 \(\) 17 & 20.6 \\  & & 0.5\% & & \(61.0 10.3\) & 88.8 & 76.6 \(\) 7.8 & 15.6 & & & 78.3 \(\) 3.7 & 17.3 \\   & VGG11 & 1\% & 88.4 & \(42.2 11.6\) & 89.8 & 76.0 \(\) 6.1 & 33.8 \(\) 6.0 & & & 81.9 \(\) 3.9 & 39.7 \\  & & 0.5\% & & \(63.6 9.3\) & 88.9 & 85.9 \(\) 2.6 & 22.3 & & & 85.5 & 84.8 \(\) 0.5 & 21.2 \\   & VGG16 & 1\% & 90.3 & \(35.7 7.9\) & 89.0 & 76.5 \(\) 9.0 & 40.8 & & & 79.2 \(\) 7.5 & 43.5 \\  & & 0.5\% & & \(66.6 8.1\) & 87.7 \(\) 7.0 & 21.1 & & & 84.7 \(\) 0.9 & 18.1 \\   & VGG19 & 1\% & 90.5 & \(36.0 12.0\) & 89.1 & 80.2 \(\) 12 & 44.2 & & & 86.3 \(\) 1.2 & 48.3 \(\) 1.2 & 48.3 \\   & VGG19 & 0.5\% & & \(64.2 12.4\) & 89.1 & 88.8 \(\) 0.4 & 24.6 & & & 85.9 \(\) 0.3 & 21.7 \\   _Note._ SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target model; CA (\(\%\)): clean accuracy; PA (\(\%\)): perturbed accuracy; NP: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA

Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with \(1.5\%\) bit-error rate

   & ConvL & ConvS & DeConvL & DeConvS & UNetL & UNetS \\  ResNet18 & 19.0 & 29.1 & 21.2 & 27.5 & 24.0 & 28.9 \\ ResNet50 & 25.4 & 29.9 & 26.4 & 29.2 & 27.7 & 29.9 \\ VGG11 & 6.6 & 27.5 & 11.2 & 24.1 & 17.1 & 27.2 \\ VGG16 & 17.1 & 28.9 & 19.7 & 27.0 & 23.0 & 28.7 \\ VGG19 & 20.3 & 29.7 & 22.3 & 27.8 & 24.8 & 29.1 \\  

Table 2: Energy saving (\(\%\)) by NeuralFuse for 30 combinations of base models and generators.

trained on CIFAR-10. Those results show that UNet-based generators had better efficiency per million parameters than either convolution-based or deconvolution-based ones.

### NeuralFuse's Robustness to Reduced-precision Quantization

Lastly, we also explored NeuralFuse's robustness to low-precision quantization on model weights. Uniform quantization is the usual method for quantizing model weights (Gholami et al., 2022). However, it is possible for it to cause an accuracy drop due to lack of precision. Given our aim of offering protection from bit errors, we hoped to understand whether NeuralFuse could also recover model-accuracy drops caused by this phenomenon. We therefore uniformly quantized the model weights to a lower bit precision and measured the resulting accuracy. Specifically, we applied symmetric uniform quantization to our base models with various numbers of bits to induce precision loss, and defined the quantized weight \(}\) (integer) as \(}=}{}\), where \(\) denotes the original model weight (full precision), \(s=|}{2^{b-1}-1}\) is the quantization scale parameter, and \(b\) is the precision (number of bits) used to quantize the models. Bit errors induced by low voltage operation as previously described, are also applied to low precision weights.

We used the GTSRB pre-trained ResNet18 as our example in an evaluation of two NeuralFuse generators, i.e., ConvL and UNetL trained with \(0.5\%\) BER, and varied precision \(b\) from 8 bits to 2 bits (integer). The results, shown in Figure 4, indicated that when \(b>3\) bits, NeuralFuse could effectively recover accuracy in both the low-voltage and low-precision scenarios. When \(b=3\), while NeuralFuse could still handle the bit-error-free model (Fig. 4 top), it exhibited a limited ability to recover the random bit-error case (Fig. 4 bottom). We find these results encouraging, insofar as NeuralFuse was only trained on random bit errors, yet demonstrated high accuracy in dealing with unseen bit-quantization errors. Further experimental results derived from other base models and datasets can be found in Appendix H.

### Extended Analysis

Here, we would like to highlight some key findings from the additional results in the Appendices. In Appendix E, we compare NeuralFuse against a simple baseline of learning a universal input perturbation. We found that such baseline performed much worse than NeuralFuse at that task, validating the necessity of adopting input-aware transformation if the goal is to learn error-resistant data representations in low-voltage scenarios. In Appendix G, we report that ensemble training of white-box surrogate base models could further improve the transferability of NeuralFuse in restricted-access scenarios. Appendices K and L present visualization results of NeuralFuse's data embeddings and transformed inputs. In Appendix J, we show that NeuralFuse can further recover the accuracy of a base model trained with adversarial weight perturbation in a low-voltage setting.

## 5 Conclusion

In this paper, we have proposed NeuralFuse, the first non-intrusive post hoc module that protects model inference against bit errors induced by low voltage. NeuralFuse is particularly well suited to

Figure 4: Reduced-precision accuracy

    Base \\ Model \\  &  BER \\ ConvL \\  &   \\ DeConvL \\  &   \\   ResNet18 \\  &  1\% \\ 0.5\% \\  &  67.5 \\ 24.7 \\  &  182 \\ 73.3 \\  &  76.6 \\ 30.7 \\  &  190.7 \\ 62.5 \\  &  94.5 \\ 33.6 \\  & 
 245.9 \\ 88.3 \\  \\   & 19 & 37.4 & 75.1 & 57.4 & 102.7 & 102.3 & 248.4 \\  & 0.5\% & 35.2 & 108.7 & 40.4 & 92.5 & 47.4 & 124.6 \\   & 1\% & 62.3 & 212.9 & 69.5 & 165.8 & 92.0 & 251.7 \\  & 0.5\% & 32.3 & 96.3 & 35.8 & 77.2 & 38.9 & 100.7 \\   & 1\% & 69.6 & 211.2 & 76.9 & 196.5 & 98.8 & 292.9 \\  & 0.5\% & 30.3 & 98.1 & 33.0 & 75.3 & 40.6 & 113 \\   & 1\% & 57.6 & 147.5 & 65.5 & 141.6 & 95.4 & 250.8 \\  & 0.5\% & 33.0 & 91.0 & 37.5 & 70.2 & 43.1 & 106.4 \\  

Table 3: The efficiency ratio for all NeuralFuse generators.

practical machine-deployment cases in which access to the base model is limited or relaxed. The design of NeuralFuse includes a novel loss function and a new optimizer, EOPM, that enable it to handle simulated randomness in perturbed models. Our comprehensive experimental results and analysis show that NeuralFuse can recover test accuracy by up to \(57\%\) while simultaneously enjoying an up to \(24\%\) reduction in memory-access energy requirements. Moreover, NeuralFuse demonstrates high transferability to access-constrained models and high versatility, e.g., robustness to low-precision quantization. In short, NeuralFuse is a notable advancement in mitigating neural-network inference's energy/accuracy tradeoff in low-voltage regimes, and points the way to greener future AI technology. Our future work will include extending this study to other neural-network architectures and modalities, such as transformer-based language models.

**Limitations.** We acknowledge the challenge of achieving significant power savings without accuracy loss and view NeuralFuse as a foundational, proof-of-concept step toward this goal. Future research could enhance this approach by optimizing the pre-processing module to adapt to specific error characteristics of low-voltage SRAM or by integrating lightweight hardware modifications to further improve the energy-accuracy trade-off.

**Broader Impacts.** We see no ethical or immediate negative societal consequence of our work, and it holds the potential for positive social impacts, from environmental benefits to improved access to technology and enhanced safety in critical applications.