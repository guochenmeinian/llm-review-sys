# Rethinking the Role of Token Retrieval in Multi-Vector Retrieval

Jinhyuk Lee

Correspondence: jinhyuklee@google.com

Zhuyun Dai

Sai Meher Karthik Duddu

Tao Lei

Iftekhar Naim

Ming-Wei Chang

Vincent Y. Zhao

Google DeepMind

###### Abstract

Multi-vector retrieval models such as ColBERT  allow token-level interactions between queries and documents, and hence achieve state of the art on many information retrieval benchmarks. However, their non-linear scoring function cannot be scaled to millions of documents, necessitating a three-stage process for inference: retrieving initial candidates via token retrieval, accessing all token vectors, and scoring the initial candidate documents. The non-linear scoring function is applied over all token vectors of each candidate document, making the inference process complicated and slow. In this paper, we aim to simplify the multi-vector retrieval by rethinking the role of token retrieval. We present XTR, Conte**X**tualized **T**oken **R**etriever, which introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens first. The improvement to token retrieval allows XTR to rank candidates only using the retrieved tokens rather than all tokens in the document, and enables a newly designed scoring stage that is two-to-three orders of magnitude cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the state-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis confirms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval stage compared to ColBERT.

## 1 Introduction

The performance of a dense retrieval model is largely affected by how it defines expressive representations over queries and documents, and whether it can efficiently retrieve and score a document using these vector representations. For example, dual encoder models  encode queries and documents into single vectors and compute query-document similarities using dot products. While these models are very efficient for retrieval, their expressivity is limited due to the absence of token-level modeling for scoring. In contrast, multi-vector models such as ColBERT  are directly designed to capture token-level interactions. By utilizing a (non-linear) scoring function over all query and document token representations, multi-vector models enjoy much better model expressivity and often achieve superior results across various benchmarks .

The enhanced model expressivity, however, comes at a great cost of inference complexity. Unlike the case in dual encoders, the non-linear scoring function in multi-vector retrieval models prohibits the use of efficient Maximum Inner Product Search (MIPS)  for finding the maximum scoring documents. As a result, models such as ColBERT adopt an intricate and resource-intensive inference pipeline, which typically consistsof three stages: 1) _token retrieval:_ using each query token to retrieve document tokens, with their source documents becoming candidates; 2) _gathering:_ collecting all the token embeddings from each candidate document, including those that are not retrieved in the first stage (most document tokens are not retrieved); and 3) _scoring:_ ranking candidates using a non-linear function based on all the token embeddings per document.

This procedure leads to two major issues. First, compared to the token retrieval stage, gathering all document token embeddings and re-scoring the documents can introduce orders of magnitude additional data loading and floating operation cost, making multi-vector models extremely expensive to deploy. Secondly, while the candidate documents are decided in the token retrieval stage, previous training objectives are designed for the scoring stage. This creates a significant training-inference gap causing multi-vector models achieve sub-optimal (and often poor) recall performance. Clearly, the three-stage pipeline has largely limited the potential of multi-vector models, raising an interesting research question - _can the token retrieval stage alone be sufficient for great performance?_

We present XTR, Cont**X**xtualized **T**oken **R**etriever: a simplified and efficient method for multi-vector retrieval, through re-thinking the role of token retrieval. The key insight of XTR is that the token retrieval in multi-vector models should be trained to retrieve the most salient and informative document tokens, so that the score between a query and document can be computed using only the retrieved information, just like how single-vector retrieval models work. By doing so, the gathering step can be completely eliminated, and the cost of scoring is significantly reduced as only a fraction of the tokens need to be considered and the dot products from the token retrieval can be reused. To improve the quality of the token retrieval, XTR proposes a novel, yet simple, training objective, which dramatically improves retrieval accuracy, doubling the chances of a gold token being retrieved in the top-\(k\) results. Furthermore, despite the improved token retrieval, some relevant tokens may still be missed (i.e., not retrieved). To address this issue, we propose a simple method, called missing similarity imputation, which accounts for the contribution of the missing tokens to the overall score.

XTR streamlines the inference process, bringing it closer to the straightforward procedure of dual encoders, while maintaining and enhancing the expressive scoring function of multi-vector retrieval models. On the BEIR (Thakur et al., 2021) and LoTTE (Santhanam et al., 2022) benchmarks, XTR attains state-of-the-art performance, requiring neither distillation nor hard negative mining. Notably, our model surpasses state-of-the-art dual-encoder GTR (Ni et al., 2021) by 3.6 nDCG@10 on BEIR without any additional training data. On the EntityQuestions benchmark (Sciavolino et al., 2021), XTR outperforms the previous state-of-the-art by 4.1 points on top-20 retrieval accuracy. XTR also does not require any secondary pre-training for retrieval and greatly outperforms mContriever (Izacard et al., 2022) on MIRACL, which contains multilingual retrieval tasks in 18 languages (Zhang et al., 2022). Our analysis supports that XTR indeed benefits from retrieving more contextualized tokens in relevant contexts, while making the scoring stage two-to-three orders of magnitude cheaper.

## 2 Background

### Multi-vector Retrieval

Single-vector retrieval models, also known as dual encoders, encode an input text sequence as a single dense embedding and define the similarity of a query and a document based on the dot product (Lee et al., 2019; Karpukhin et al., 2020). Multi-vector retrieval models, on the other hand, make use of multiple dense embeddings for each query and document, typically leveraging all contextualized word representations of the input to gain improved model expressivity.

Consider a query \(Q=\{_{i}\}_{i=1}^{n}\) and a document \(D=\{_{j}\}_{j=1}^{m}\) where \(_{i}\) and \(_{j}\) denote the \(d\)-dimensional query token vector and the document token vector, respectively. Multi-vector retrieval models compute the query-document similarity as follows: \(f(Q,D)=_{i=1}^{n}_{j=1}^{m}_{ij}_{ij}\) where \(_{ij}=_{i}^{}_{j}\) and \(\{0,1\}^{n m}\) denotes the alignment matrix with \(_{ij}\) being the token-level alignment between the query token vector \(_{i}\) and the document token vector \(_{j}\). The sum-of-max operator of ColBERT (Khattab and Zaharia, 2020) sets \(_{ij}=_{[j_{j}(_{ij})]}\) where the argmax operator is over \(1 j^{} m\) (i.e., tokens from a single document \(D\)) and \(_{[]}\) is an indicator function. Then, \(f_{}(Q,D)\) is defined as follows:

\[f_{}(Q,D)=_{i=1}^{n}_{j=1}^{m}_{ij} _{ij}=_{i=1}^{n}_{1 j m}_{i}^{ }_{j}.\] (1)Here, we include the normalizer \(n\), which was not included in the original sum-of-max, as it stabilizes training while not affecting the ranking during inference. After computing the query-document similarity, multi-vector retrieval models are typically trained with a cross-entropy loss over in-batch negatives . Specifically, given a positive document \(D^{+}\) for \(Q\) and a set of mini-batch documents \(D_{1:B}\) = \([D_{1},,D_{B}]\) where \(D^{+}\)\(\)\(D_{1:B}\), they minimize the cross-entropy loss defined as: \(_{}\) = \(-)}{_{b=1}^{B} f(Q,D_{b})}\).

### Three-stage inference of Multi-vector Retrieval

Unlike dual encoder models, finding the maximum scoring document--the document that maximizes eq. (1)--cannot be directly handled by MIPS as the scoring function uses a non-linear, sum-of-max operation. Instead, a multi-vector retrieval model typically takes the following steps for the inference.

1) **Token Retrieval**: for each of the \(n\) query token vectors, it first retrieves \(k^{t}\) document token vectors, which is simply used to form _initial candidate document set_ by taking the union of source documents of retrieved tokens. The total number of candidate documents is up to \(nk^{t}\) if each token is coming from a unique document.2 **Gathering**: since the scoring function eq. (1) requires the computation over all document tokens, multi-vector models need to load all of the token vectors of the candidate documents. To optimize the loading process, a RAM-based index is often employed. 3) **Scoring**: to provide final ranks of candidate documents, multi-vector models score all the candidate documents with eq. (1). This stage is also called _refinement_. Note that the training of typical multi-vector models only takes care of the scoring stage with mini-batch documents. Finally, top-\(k\) documents are returned based on the computed scores. The three-stage inference is illustrated in the top of Figure 1.

## 3 XTR: Contextualized Token Retriever

Unlike existing multi-vector models that follow the retrieve-gather-score stages, XTR directly scores documents utilizing the tokens retrieved from the token retrieval stage. In this section, we start by showing why the existing cross entropy loss with the sum-of-max scoring function would fail on the first-stage token retrieval. Then, we introduce simple but important modifications for XTR.

Figure 1: Overview of XTR. ColBERT has the three-stage inference combining (a) the token retrieval, (b) the gathering and (c) the scoring stages (§2.2). XTR leverages the token retrieval for both training and inference. XTR efficiently obtains the score of each candidate document by applying \(f_{}\) (or \(f_{}\)) on the retrieved tokens, completely removing the gathering stage (§3.2).

[MISSING_PAGE_FAIL:4]

Here, top-\(k^{l}(})\) is a union of top-\(k^{l}\) document tokens (from the entire corpus) based on the inner product scores with each query vector (i.e., \(^{}\)). Given the \(n\) query token vectors, there are \(C\) (\( nk^{l}\)) candidate documents. Previous methods load the entire token vectors of each document and compute eq.1 for every query and candidate document pair, which takes \((n^{2}k^{l}d)\) computation per query (\(\) = average document length). Instead, we propose to score the documents _solely using the retrieved token similarity_. This significantly reduces the computational cost for the scoring stage since re-using the token retrieval scores removes computing redundant inner products and unnecessary (non-max) inner products. Furthermore, the expensive gathering stage (which requires loading all the document token vectors for computing eq.1) can be removed completely. Unlike previous work (Macdonald and Tonellotto, 2021) that leverages token retrieval to sort first-stage candidate documents before the scoring stage, we aim to directly provide the final scores of documents.

Missing similarity imputationDuring inference, we retrieve \(k^{l}\) document tokens for each of \(n\) query tokens. Assume that each document token belongs to a unique document, providing \(C\) = \(nk^{l}\) candidate documents in total. This leaves us with a single token similarity to score each document in the absence of the gathering stage. However, during training--either with eq.1 or eq.2--each positive document has up to \(n\) (max) token similarities to average, which mostly converges to \(n\) as training proceeds. Hence, during inference, we impute the _missing similarity_ for each query token treating each of candidate documents as if it were positive with \(n\) token similarities.

For every candidate document \(\), we first define the following scoring function for the inference:

\[f_{^{l}}(Q,)=_{i=1}^{n}_{1 j m} }_{ij}_{i}^{}_{j}+(1-}_{ij})m_{i}.\] (4)

This is similar to eq.2, but introduces \(m_{i}\), which estimates the missing similarity for each \(q_{i}\). \(}\) is defined similar to the one described in eq.2 except that it uses \(k^{l}\) for the top-\(k\) operator. Each \(q_{i}\) would take the missing similarity \(m_{i}\) as the maximum value if \(}_{i*}\) = 0 and \(m_{i} 0\). Importantly, \(f_{^{l}}\) removes the need of recomputing any \(_{i}^{}_{j}\) since when \(}_{ij}=1\) we already know the retrieval score from the token retrieval stage, and when \(}_{ij}=0\) we simply don't need to compute it as \(}_{ij}_{i}^{}_{j}=0\). Note that when every \(}_{ij}=1\), the equation becomes the sum-of-max operator. On the other hand, when no document tokens of \(\) were retrieved for \(q_{i}\) (i.e., \(}_{i*}=0\)), we fall back to the imputed score \(m_{i}\), which provides an approximated sum-of-max result.

    & **Scoring** & **Estimated FLOPs/query** & **Setting** \\  \(f_{}\) & \(n^{2}k^{l}(2d++1)\) & \(0.36 10^{9}\) & \(M\) = 3 \( 10^{9}\), \(n\) = 16, \(d\) = 128, \\ \(f_{^{l}}\) & \(n^{2}k^{l}(+1)\) & \(0.09 10^{6}\) & \(k^{l}\) = 100, \(\) = 55, \(\) = 2.5 \\   

Table 1: FLOPs comparison of ColBERT and XTR for the scoring stage. XTR only adds minimal complexity for scoring each candidate document. The setting is derived from MS MARCO.

Figure 3: Comparison of \(f_{}\) in eq.1 and \(f_{^{l}}\) in eq.4. Assume that \(D_{a}\) and \(D_{b}\) were selected as initial candidate documents from the token retrieval stage. \(f_{}\) loads all token vectors of \(D_{a}\) and \(D_{b}\) and exhaustively recomputes pairwise token similarity to obtain the max values (**red boxes**). On the other hand, \(f_{^{l}}\) does not load any token vectors and reuses retrieval scores from the first-stage token retrieval. Assume that, with the top-2 token retrieval results, the first query token retrieved each max score of \(D_{a}\) and \(D_{b}\), but the second query token retrieved two tokens only from \(D_{a}\) but not \(D_{b}\). We impute the missing similarity \(m\) for \(D_{b}\) (denoted as yellow dashed box) by finding its upper bound using the top-2 score (denoted as \(s_{2}\)) of the second query token (i.e., \(m s_{2} s_{1}\)).

In fact, we can find the upper bound of the missing similarity. For every token retrieval with \(_{i}\), the missing similarity of the query token for \(\) will be upper bounded by its last top-\(k^{}\) score. Specifically, for each query token \(q_{i}\), we have the following top-\(k^{}\) token similarity during inference: \(_{i}^{}_{\{1\}},,_{i}^{} _{\{k^{}\}}\). Here, each \(_{(*)}\) could come from a different document. Since the missing similarity would have a score less than equal to the score of the last retrieved token, we know that \(m_{i}_{i}^{}_{\{k^{}\}}\). With a larger \(k^{}\), the upper bound becomes tighter. In our experiments, we show that simply choosing \(m_{i}\) = \(_{i}^{}_{\{k^{}\}}\) works well especially when a model is trained with \(f_{}\).5 While we also tried more complicated imputation methods based on regression, our method was competitive enough despite its simplicity. The imputation process is illustrated in Figure 3.

Table 1 shows the estimated FLOPs of ColBERT and XTR (see Appendix B for more details). Due to the differences in hardware and infrastructure, we mainly compared the theoretical FLOPs. XTR reduces the FLOPs at the scoring stage by 4000\(\) making multi-vector retrieval more efficient.

## 4 Experiments

Experimental SettingFollowing Ni et al. (2021), we fine-tune XTR on MS MARCO with a fixed set of hard negatives from RocketQA (Qu et al., 2021). Then, we test XTR on MS MARCO (MS; in-domain) and zero-shot IR datasets. For the zero-shot evaluation, we use 13 datasets from BEIR (Thakur et al., 2021) (see Appendix C for acronyms), 12 datasets from LoTTE (Santhanam et al., 2022), and 4 datasets on open-domain QA passage retrieval (EQ: EntityQuestions (Sciavolino et al., 2021), NQ, TQA: TriviaQA, SQD: SQuAD). We also train multilingual XTR (mXTR) and evaluate it on MIRACL (Zhang et al., 2022), which contains retrieval tasks in 18 languages. The performance gap between T5-ColBERT (Qian et al., 2022) and XTR shows the improvement with our methods on a multi-vector retrieval model. For implementation details and baselines, see Appendix C. For the relationship between hyperparameters (e.g., \(k_{}\) and \(k^{}\)), see SS5.3.

    & MS & AR & TO & FE & CF & SF & CV & NF & NQ & HQ & FQ & SD & DB & QU & Avg. \\   \\  GenQ & 40.8 & 49.3 & 18.2 & 66.9 & 17.5 & 64.4 & 61.9 & 31.9 & 35.8 & 53.4 & 30.8 & 14.3 & 32.8 & 83.0 & 43.1 \\ _{}\)} & - & 58.8 & 25.6 & 76.2 & 23.5 & 63.8 & 70.2 & 33.7 & 45.6 & 61.7 & 43.0 & 18.3 & 34.4 & 87.5 & 49.4 \\   \\  BM25 & 22.8 & 31.5 & 36.7 & 75.3 & 21.3 & 66.5 & 63.6 & 32.5 & 32.9 & 60.3 & 23.6 & 15.8 & 31.3 & 78.9 & 44.0 \\ ColBERT & 40.1 & 23.3 & 20.2 & 77.1 & 18.4 & 67.1 & 67.7 & 30.5 & 52.4 & 59.3 & 31.7 & 14.5 & 39.2 & 85.4 & 45.1 \\ _{}\)} & 42.0 & 51.1 & 21.5 & 66.0 & 24.1 & 60.0 & 53.9 & 30.8 & 49.5 & 53.5 & 34.9 & 14.9 & 39.2 & 88.1 & 45.2 \\ _{}\)} & 45.6 & 28.8 & 31.1 & 72.4 & 18.1 & 70.4 & 68.3 & 34.0 & 52.2 & 61.7 & 33.4 & 14.1 & 41.6 & 82.3 & 46.8 \\ _{}\)} & 45.0 & 40.7 & 31.3 & 73.7 & 20.7 & 71.0 & 73.6 & 34.0 & 53.0 & 64.7 & 34.7 & 14.5 & 40.9 & 86.1 & 49.1 \\ ,\)} & 43.3 & 47.9 & 27.2 & 78.6 & 23.5 & 69.3 & 71.0 & 33.4 & 52.1 & 68.4 & 33.6 & 15.8 & 43.5 & 83.8 & 49.9 \\ _{}\)} & - & 46.3 & 26.3 & 78.5 & 17.6 & 69.3 & 73.8 & 33.8 & 56.2 & 66.7 & 35.6 & 15.4 & 44.6 & 85.2 & 49.9 \\  _{}\)} & 44.2 & 54.0 & 23.3 & 74.0 & 26.7 & 66.2 & 50.1 & 34.2 & 56.8 & 59.9 & 46.7 & 16.1 & 40.8 & 89.2 & 49.1 \\ _{}\)} & 47.3 & 33.8 & 31.0 & 74.2 & 19.7 & 73.1 & 75.8 & 35.2 & 60.5 & 65.2 & 43.5 & 17.1 & 45.0 & 86.0 & 50.8 \\ _{}\)} & 46.6 & 44.2 & 30.9 & 77.0 & 24.5 & 74.3 & 78.9 & 35.3 & 60.9 & 66.2 & 43.8 & 17.1 & 44.3 & 88.1 & **52.7** \\   }\)} &  \\   & Writing & Rec. & Sci. & Tech. & Life. & Pooled & Writing & Rec. & Sci. & Tech. & Life. & Pooled \\  BM25 & 60.3 & 56.5 & 32.7 & 41.8 & 63.8 & 48.3 & 64.0 & 55.4 & 37.1 & 39.4 & 60.6 & 47.2 \\ ColBERT & 74.7 & 68.5 & 53.6 & 61.9 & 80.2 & 67.3 & 71.0 & 65.6 & 41.8 & 48.5 & 73.0 & 58.2 \\ _{}\)} & 74.1 & 65.7 & 49.8 & 58.1 & 82.0 & 65.0 & 69.2 & 62.0 & 33.7 & 47.6 & 72.2 & 54.9 \\ _{}\)} & 77.0 & 69.4 & 54.9 & 63.2 & 82.1 & 69.0 & 73.9 & 68.7 & 42.2 & 51.9 & 74.4 & 60.1 \\  ,\)} & 77.1 & 69.0 & 55.4 & 62.4 & 82.3 & 68.9 & 73.0 & 67.1 & 43.7 & 50.8 & 74.0 & 60.1 \\ _{}\)} & 80.1 & 72.3 & 56.7 & 66.1 & 84.7 & 71.6 & 76.3 & 70.8 & 46.1 & 53.6 & 76.9 & 63.4 \\  _{}\)} & **83.9** & 78.0 & 60.0 & 69.5 & 87.4 & 76.0 & 79.5 & 73.5 & 43.1 & 62.6 & 81.9 & 66.9 \\ _{}\)} & 83.3 & **79.3** & **60.

[MISSING_PAGE_FAIL:7]

## 5 Analysis

### Towards Better Token Retrieval

Gold token retrievalIf the tokens of gold documents are not retrieved at all, multi-vector retrieval models would fail to retrieve the gold documents. Hence, a better token retrieval would contain these _gold tokens_ more often in their top results. In Figure 4 (top), we show the probability of a token at the rank \(k\) coming from the gold documents of a query. To compute the probability for the rank \(k\), we simply count the number of an event where a token at rank \(k\) belongs to the gold document and divide it by the number of tokens at rank \(k\). While this is measuring the precision of the token retrieval, we observed a similar trend for the recall of gold tokens. Compared to T5-ColBERT, XTR retrieves gold tokens with higher probability, even on MS MARCO. This shows that the training objective of XTR encourages it to retrieve tokens from more relevant context.

Lexical token retrievalIn Figure 4 (bottom), we show the probability of a token at the rank \(k\) being the same as its query token (e.g., 'insulin' retrieving 'insulin's). T5-ColBERT has very high probability of retrieving the same token across different ranks and datasets. However, it is unclear to what extent the token retrieval stage should behave as sparse retrieval, as it might suffer from the vocabulary mismatch problem. XTR effectively lowers the reliance on the lexical matching while preserving a good amount of lexical precision so that it would achieve a high retrieval accuracy on the entity-centric dataset (SS4.2). In fact, Table 6 in Appendix shows that having lower lexical matching doesn't necessarily mean a lower retrieval quality, but often means better contextualization.

### Efficient Scoring

In Table 5, we show how we can employ the efficient scoring function \(f_{^{}}\) in XTR with minimal performance losses. We apply \(f_{^{}}\) on both T5-ColBERT and XTR, and show their performances on MS MARCO. With T5-ColBERT, even if we use the top-\(k^{l}\) score for the imputation, the performance

Figure 4: (top) Gold token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank \(k\) coming from the gold document. (bottom) Lexical token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank \(k\) being lexically identical to its query token.

is much worse than the original sum-of-max scoring. With XTR, the performance greatly improves as it has better token retrieval. Figure 5 shows how Recall@100 improves with larger \(k^{t}\)'s as it provides more exact upper bound for the missing similarity imputation. Table D.2 shows that even if we use smaller \(k^{t}\), XTR still maintains high performances on BEIR.

### Relationship between Hyperparameters

\(}}\) vs. \(}\)In Figure 6, we show MRR@10 of XTR trained with different \(k_{}\) and evaluated with different \(k^{t}\) on the MS MARCO development set. While all variants of XTR prefer larger \(k^{t}\), ones trained with smaller \(k_{}\) show higher performances than others under small \(k^{t}\) settings. XTR with larger \(k_{}\) exhibits better performances than ones with smaller \(k_{}\) as \(k^{t}\) becomes larger.

Training batch size vs. \(}}\)In Figure 7, we show the relationship between the training batch size and \(k_{}\) during training XTR. In this experiment, we use \(k^{t}=40,000\). While it is evident that XTR mostly favors large training batch sizes, the optimal top-\(k_{}\) can be different for different datasets. While most datasets including MS MARCO favored a large enough \(k_{}\), ArguAna prefers smaller \(k_{}\). We hypothesize that this is due to the longer query length in ArguAna, which makes multi-vector models fall short compared to dual-encoders (see GTR vs. T5-ColBERT in Table 2).

### Qualitative Analysis

Table 6 shows a prediction sample from MS MARCO. For T5-ColBERT, all of the top retrieved tokens are exact lexical matches. Surprisingly, none of the retrieved passages are about the query, demonstrating T5-ColBERT's failure to retrieve tokens from the correct context. In contrast, XTR retrieves fewer exact lexically matching tokens, but the contexts of the retrieved tokens are much more related to the query. This example explains the lower lexical token retrieval probability of XTR compared to T5-ColBERT in Figure 4 (bottom), but higher gold token retrieval performance in Figure 4 (top). For more qualitative examples, please see Appendix E.

## 6 Related Work

One of the main limitations of dense retrieval models is that encoding the query and document into a single vector constrains the representational power of the models. Polyencoder (Humeau et al., 2020), MEBERT (Luan et al., 2021), and MVR (Zhang et al., 2022) propose to use multiple embeddings, instead of one, to represent the query or the document. A more recent approach is token-level multi-vector retrieval, which stores and retrieves with every token embedding. ColBERT (Khattab and Zaharia, 2020) is probably the most renowned model in this family. ALIGNER (i.e. T5-ColBERT) (Qian et al., 2022) extends ColBERT by scaling up the backbone langauge model and studying various strategies for aggregating the token-level alignment scores. These token-level retrieval models show strong effectiveness and out-of-domain generalization ability.

Efforts for reducing serving costs of multi-vector models have been mostly focused on the token-level retrieval stage. COIL (Gao et al., 2021) accelerates token-level retrieval by confining retrieval within exact match tokens, sharing the spirit of classic inverted indexing. CITADEL (Li et al., 2022) relaxes COIL with a lexical routing mechanism where a query token vector only retrieves from a subset of

Figure 6: MRR@10 of XTR with different \(k_{}\) and \(k^{t}\). For T5-ColBERT, we also use \(f_{^{t}}\) with the top-\(k^{t}\) score imputation method for the inference.

Figure 7: Effect of training XTR with different batch sizes and \(k_{}\). For each point of the graph, we train XTRbase with the specified training batch size (128, 256, 320) and \(k_{}\) (32, 64, 128, 256) and evaluate on each dataset (MS MARCO and ArguAna). nDCG@10 of each model is reported.

document token vectors routed to the same key. PLAID  optimizes the speed of ColBERT by pruning weaker candidates in the earlier stages of retrieval and using better vector quantization. ColBERT-v2  further adopts residual representations with cluster centroids to improve the efficiency of ColBERT. On the other hand, how to accelerate the scoring stage remains under-explored. To the best of our knowledge, XTR is the first work to simplify the scoring stage and remove the gathering stage in multi-vector retrieval.

## 7 Conclusion

Multi-vector retrieval leverages query and document token representations for effective information retrieval. In this paper, we propose XTR that simplifies the existing three-stage inference of multi-vector models by improving the initial token retrieval stage. Specifically, XTR scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. As a result, XTR achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the FLOPs of the scoring stage. We further show that our objective function indeed encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.

## Limitations

In most of our experiments, XTR was trained on MS MARCO, a large-scale retrieval dataset in English. While our experiments were conducted in a fair setting where most baseline models also utilize MS MARCO, future use cases might need to remove its dependency on MS MARCO due to the license or language-specific issue. We believe that LLM-based retrieval dataset generation  would be able to mitigate the problem in the future.

    \\  Rank & Token & Context of Token & Relevance \\ 
1 & usual & routine passport services: the usual waiting time in logan to get your passport is four (4) to eight (8) weeks for routine applications. & No \\
2 & usual & the usual pay days are the 1st and 16th of each month. for annual educational paraprofessionals there is no payroll lag. & No \\
5 & usual & the usual part xiii tax rate is 25\% (unless a tax treaty between canada and your home country reduces the rate). & No \\
50 & usual & this is where one can challenge the judgment debtor’s claim. one option & No \\  & & creditors have is to try and make a deal with the debtor to take less than & 25\% (the usual amount of a wage levy). & No \\
100 & usual & the usual maximum inventory is 1 talisman, 26 elemental runes, and 26 pure essence. the ingredients must be brought to an opposing altar... & No \\  & & from the runes being crafted. & \\    \\  Rank & Token & Context of Token & Relevance \\ 
1 & usual & store manager. 1 salary: the usual salary a store manager receives can & No \\  & & be anywhere around $52,000 to $115,000 annually. & No \\
2 & usual & 1 salary: the usual salary a store manager receives can be anywhere & No \\  & & around $52,000 to $115,000 annually. 2 bonuses: public provide bonuses & No \\  & & that could reach up to $40,000. & \\
5 & average & average salaries for michaels stores stock associate: $9. michaels stores & \\  & & hourly pay trends based on salaries posted anonymously by michaels & \\
50 & v & i think the avg starting pay is closer to 30k for asst mgr trainees. it is an & No \\  & & hourly position until you are fully trained (40 hours per week). & No \\
100 & average & average macros salaries. the average salary for macros jobs is $32,000. average macros salaries can vary greatly due to company, location, industry, experience and benefits. & No \\   

Table 6: Token retrieval example from MS MARCO. Among the top 100 retrieved tokens, \(100\%\) of T5-ColBERT tokens are lexically identical as the query token usual while only \(8\%\) of XTR tokens are lexically identical. XTR retrieves the relevant passage by retrieving average for usual.