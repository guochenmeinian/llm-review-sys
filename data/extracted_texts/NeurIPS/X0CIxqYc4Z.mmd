# Risk-Averse Model Uncertainty for

Distributionally Robust Safe Reinforcement Learning

 James Queeney

Division of Systems Engineering

Boston University

jqueeney@bu.edu

&Mouhacine Benosman

Mitsubishi Electric Research Laboratories

benosman@merl.com

Work partly done during an internship at Mitsubishi Electric Research Laboratories.

###### Abstract

Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments.

## 1 Introduction

In many real-world decision making applications, it is important to satisfy safety requirements while achieving a desired goal. In addition, real-world environments often involve uncertain or changing conditions. Therefore, in order to reliably deploy data-driven decision making methods such as deep reinforcement learning (RL) in these settings, they must deliver _robust performance and safety_ even in the presence of uncertainty. Recently, techniques have been developed to handle safety constraints within the deep RL framework , but these safe RL algorithms only focus on performance and safety in the training environment. They do not consider uncertainty about the true environment at deployment time due to unknown disturbances or irreducible modeling errors, which we refer to as _model uncertainty_. In this work, we introduce a framework that incorporates model uncertainty into safe RL. In order for our framework to be useful, we emphasize the importance of (i) an efficient deep RL implementation during training and (ii) robustness guarantees on performance and safety upon deployment.

Existing robust RL methods address the issue of model uncertainty, but they can be difficult to implement and are not always suitable in real-world decision making settings. Robust RL focuses on worst-case environments in an uncertainty set, which requires solving complex minimax optimization problems throughout training. This is typically approximated in a deep RL setting through direct interventions with a learned adversary , or through the use of parametric uncertainty with multiple simulated training environments . However, we do not always have access to fast, high-fidelity simulators for training . In these cases, we must be able to incorporate robustness to model uncertainty without relying on multiple training environments or potentially dangerous adversarial interventions, as real-world data collection may be necessary.

A more informative way to represent model uncertainty is to instead consider a distribution over potential environments. Domain randomization  collects training data from a range of environments by randomizing across parameter values in a simulator, and optimizes for average performance. This approach to model uncertainty avoids minimax formulations and works well in practice , but lacks robustness guarantees. In addition, domain randomization focuses on parametric uncertainty, which still requires detailed simulator access and domain knowledge to define the training distribution.

In this work, we introduce a general approach to safe RL in the presence of model uncertainty that addresses the main shortcomings of existing methods. In particular, we consider a distribution over potential environments, and apply a _risk-averse perspective towards model uncertainty_. Through the use of coherent distortion risk measures, this leads to a safe RL framework with robustness guarantees that does not involve difficult minimax formulations. Using this framework, we show how we can learn safe policies that are robust to model uncertainty, without the need for detailed simulator access or adversarial interventions during training. Our main contributions are as follows:

1. We reformulate the safe RL problem to incorporate a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures, and we introduce the corresponding Bellman operators.
2. From a theoretical standpoint, we provide robustness guarantees for our framework by showing it is equivalent to a specific class of distributionally robust safe RL problems.
3. We propose an efficient deep RL implementation that avoids the difficult minimax formulation present in robust RL and only uses data collected from a single training environment.
4. We demonstrate the robust performance and safety of our framework through experiments on continuous control tasks with safety constraints in the Real-World RL Suite [18; 19].

## 2 Preliminaries

Safe reinforcement learningIn this work, we consider RL in the presence of safety constraints. We model this sequential decision making problem as an infinite-horizon, discounted Constrained Markov Decision Process (CMDP)  defined by the tuple \((,,p,r,c,d_{0},)\), where \(\) is the set of states, \(\) is the set of actions, \(p: P()\) is the transition model where \(P()\) represents the space of probability measures over \(\), \(r,c:\) are the reward function and cost function used to define the objective and constraint, respectively, \(d_{0} P()\) is the initial state distribution, and \(\) is the discount rate. We focus on the setting with a single constraint, but all results can be extended to the case of multiple constraints.

We model the agent's decisions as a stationary policy \(: P()\). For a given CMDP and policy \(\), we write the expected total discounted rewards and costs as \(J_{p,r}()=_{(,p)}[_{t=0}^{}^{t}r (s_{t},a_{t})]\) and \(J_{p,c}()=_{(,p)}[_{t=0}^{}^{t}c (s_{t},a_{t})]\), respectively, where \((,p)\) represents a trajectory sampled according to \(s_{0} d_{0}\), \(a_{t}(\, s_{t})\), and \(s_{t+1} p(\, s_{t},a_{t})\). The goal of safe RL is to find a policy \(\) that maximizes the constrained optimization problem

\[_{}\ J_{p,r}() J_{p,c}() B,\] (1)

where \(B\) is a safety budget on expected total discounted costs.

We write the corresponding state-action value functions (i.e., Q functions) for a given transition model \(p\) and policy \(\) as \(Q_{p,r}^{}(s,a)\) and \(Q_{p,c}^{}(s,a)\), respectively. Off-policy optimization techniques [28; 55] find a policy that maximizes (1) by solving at each iteration the related optimization problem

\[_{}\ }{}[}[Q_{p,r}^{_{k}}(s,a)]] }{}[}[Q_{p,c}^{_{k}}(s,a)]]  B,\] (2)

where \(_{k}\) is the current policy and \(\) is a replay buffer containing data collected in the training environment. Note that \(Q_{p,r}^{}(s,a)\) and \(Q_{p,c}^{}(s,a)\) are the respective fixed points of the Bellman operators

\[_{p,r}^{}Q(s,a) :=r(s,a)+ p_{s,a}}{}[ (\, s^{})}{}[Q(s^{ },a^{})]],\] \[_{p,c}^{}Q(s,a) :=c(s,a)+ p_{s,a}}{} [(\, s^{})}{}[Q(s ^{},a^{})]].\]Model uncertainty in reinforcement learningRather than focusing on a single CMDP with transition model \(p\), we incorporate uncertainty about the transition model by considering a distribution \(\) over models. We focus on distributions of the form \(=_{(s,a)}_{s,a}\), where \(_{s,a}\) represents a distribution over transition models \(p_{s,a}=p(\, s,a) P()\) at a given state-action pair and \(\) is the product over all \(_{s,a}\). This is known as rectangularity, and is a common assumption in the literature [11; 15; 16; 53; 56]. Note that \(_{s,a} P()\), where we write \(=P()\) to denote model space. Compared to robust RL methods that apply uncertainty sets over transition models, the use of a distribution \(\) over transition models is a more informative way to represent model uncertainty that does not require solving for worst-case environments (i.e., _does not introduce a minimax formulation_).

In order to incorporate robustness to the choice of \(\), distributionally robust MDPs [53; 56] consider an ambiguity set \(=_{(s,a)}_{s,a}\) of distributions over transition models, where \(_{s,a}_{s,a} P()\). The goal of distributionally robust RL is to optimize the worst-case average performance across all distributions contained in \(\). In this work, we will show that a risk-averse perspective towards model uncertainty defined by \(\) is equivalent to distributionally robust RL for appropriate choices of ambiguity sets in the objective and constraint of a CMDP. However, our use of risk measures _avoids the need to solve for worst-case distributions in \(\) throughout training_.

Risk measuresConsider the probability space \((,,_{s,a})\), where \(\) is a \(\)-algebra on \(\) and \(_{s,a} P()\) defines a probability measure over \(\). Let \(\) be a space of random variables defined on this probability space, and let \(^{*}\) be its corresponding dual space. A real-valued risk measure \(:\) summarizes a random variable as a value on the real line. In this section, we consider cost random variables \(Z\) where a lower value of \((Z)\) is better. We can define a corresponding risk measure \(^{+}\) for reward random variables through an appropriate change in sign, where \(^{+}(Z)=-(-Z)\). Risk-sensitive methods typically focus on classes of risk measures with desirable properties , such as coherent risk measures  and distortion risk measures [17; 50].

**Definition 1** (Coherent risk measure).: _A risk measure \(\) is a coherent risk measure if it satisfies monotonicity, translation invariance, positive homogeneity, and convexity._

**Definition 2** (Distortion risk measure).: _Let \(g:\) be a non-decreasing, left-continuous function with \(g(0)=0\) and \(g(1)=1\). A distortion risk measure with respect to \(g\) is defined as_

\[(Z)=_{0}^{1}F_{Z}^{-1}(u)d(u),\]

_where \(F_{Z}^{-1}\) is the inverse cumulative distribution function of \(Z\) and \((u)=1-g(1-u)\)._

A distortion risk measure is coherent if and only if \(g\) is concave . In this work, we focus on the class of _coherent distortion risk measures_. We will leverage properties of coherent risk measures to provide robustness guarantees for our framework, and we will leverage properties of distortion risk measures to propose an efficient, model-free implementation that does not involve minimax optimization. See the Appendix for additional details on the properties of coherent distortion risk measures. Many commonly used risk measures belong to this class, including expectation, conditional value-at-risk (CVaR), and the Wang transform  for \( 0\) which is defined by the distortion function \(g_{}(u)=(^{-1}(u)+)\), where \(\) is the standard Normal cumulative distribution function.

## 3 Related work

Safe reinforcement learningThe CMDP framework is the most popular approach to safety in RL, and several deep RL algorithms have been developed to solve the constrained optimization problem in (1). These include primal-dual methods that consider the Lagrangian relaxation of (1) [41; 44; 47], algorithms that compute closed-form solutions to related or approximate versions of (1) [3; 28], and direct methods for constraint satisfaction such as the use of barriers  or immediate switching between the objective and constraint . All of these approaches are designed to satisfy expected cost constraints for a single CMDP observed during training. In our work, on the other hand, we consider a distribution over possible transition models.

Uncertainty in reinforcement learningOur work focuses on irreducible uncertainty about the true environment at deployment time, which we refer to as _model uncertainty_ and represent using a distribution \(\) over transition models. The most popular approach that incorporates model uncertainty in this way is domain randomization [37; 48], which randomizes across parameter values in a simulator and trains a policy to maximize average performance over this training distribution. This represents a risk-neutral attitude towards model uncertainty, which has been referred to as a soft-robust approach . Distributionally robust MDPs incorporate robustness to the choice of \(\) by instead considering a set of distributions [11; 15; 53; 56], but application of this distributionally robust framework has remained limited in deep RL as it leads to a difficult minimax formulation that requires solving for worst-case distributions over transition models.

Robust RL represents an alternative approach to model uncertainty that considers uncertainty sets of transition models [21; 34]. A major drawback of robust RL is the need to calculate worst-case environments during training, which is typically approximated through the use of parametric uncertainty with multiple training environments [31; 32; 39] or a trained adversary that directly intervenes during trajectory rollouts [38; 46; 49]. Unlike these methods, we propose a robust approach to model uncertainty based on a distribution \(\) over models, which does not require access to a range of simulated training environments, does not impact data collection during training, and does not involve minimax optimization problems.

In contrast to irreducible model uncertainty, _epistemic uncertainty_ captures estimation error that can be reduced during training through data collection. Epistemic uncertainty has been considered in the estimation of Q functions [9; 35; 36] and learned transition models [7; 13; 22; 25; 40], and has been applied to promote both exploration and safety in a fixed MDP. Finally, risk-sensitive methods typically focus on the _aleatoric uncertainty_ in RL, which refers to the range of stochastic outcomes within a single MDP. Rather than considering the standard expected value objective, they learn risk-sensitive policies over this distribution of possible outcomes in a fixed MDP [12; 24; 43; 45]. Distributional RL  trains critics that estimate the full distribution of future returns due to aleatoric uncertainty, and risk measures can be applied to these distributional critics for risk-sensitive learning [14; 29]. We also consider the use of risk measures in our work, but different from standard risk-sensitive RL methods we apply a risk measure over _model uncertainty_ instead of aleatoric uncertainty.

## 4 Risk-averse model uncertainty for safe reinforcement learning

The standard safe RL problem in (1) focuses on performance and safety in a single environment with fixed transition model \(p\). In this work, however, we are interested in a distribution of possible transition models \(p\) rather than a fixed transition model. The distribution \(\) provides a natural way to capture our uncertainty about the unknown transition model at deployment time. Next, we must incorporate this model uncertainty into our problem formulation. Prior methods have done this by applying the expectation operator over \(_{s,a}\) at every transition . Instead, we adopt a risk-averse view towards model uncertainty in order to learn policies with robust performance and safety. We accomplish this by applying a coherent distortion risk measure \(\)_with respect to model uncertainty_ at every transition.

We consider the risk-averse model uncertainty (RAMU) safe RL problem

\[_{}\ J_{^{+},r}() J_{,c}() B,\] (3)

where we use \(^{+}\) and \(\) to account for reward and cost random variables, respectively, and we apply these coherent distortion risk measures over \(p_{s,a}_{s,a}\) at every transition to define

\[J_{^{+},r}() :=}_{s d_{0}}[}_{a (|s)}[r(s,a)+_{p_{s,a}_{s,a}}^{+} (}_{s^{} p_{s,a}}[}_{a ^{}(|s^{})}[r(s^{},a^{})+ ]])],\] \[J_{,c}() :=}_{s d_{0}}[}_{a (|s)}[c(s,a)+_{p_{s,a}_{s,a}}( }_{s^{} p_{s,a}}[}_{a^{ }(|s^{})}[c(s^{},a^{})+] ])]].\]

The notation \(^{+}_{p_{s,a}_{s,a}}(\,\,)\) and \(_{p_{s,a}_{s,a}}(\,\,)\) emphasize that the stochasticity of the random variables are with respect to the transition models sampled from \(_{s,a}\). Note that we still apply expectations over the aleatoric uncertainty of the CMDP (i.e., the randomness associated with a stochastic transition model and stochastic policy), while being risk-averse with respect to model uncertainty. Because we are interested in learning policies that achieve robust performance _and_ robust safety at deployment time, we apply this risk-averse perspective to model uncertainty in both the objective and constraint of (3).

We write the corresponding RAMU reward and cost Q functions as \(Q^{}_{^{+},r}(s,a)\) and \(Q^{}_{,c}(s,a)\), respectively. Similar to the standard safe RL setting, we can apply off-policy techniques to solve the RAMU safe RL problem in (3) by iteratively optimizing

\[_{}\ }_{s}[}_{a (|s)}[Q^{_{k}}_{^{+},r}(s,a)]]\ \ \ \ \ }_{s}[ }_{a(|s)}[Q^{_{k}}_{,c}(s,a)] ] B.\] (4)

Therefore, we have replaced the standard Q functions for a fixed transition model \(p\) in (2) with our RAMU Q functions in (4).

We can write the RAMU Q functions recursively as

\[Q^{}_{^{+},r}(s,a) =r(s,a)+_{p_{s,a}_{s,a}}^{+}( }_{s^{} p_{s,a}}[}_{a^{ }(|s^{})}[Q^{}_{^{+},r}(s^{},a^{ })]]),\] \[Q^{}_{,c}(s,a) =c(s,a)+_{p_{s,a}_{s,a}}(}_{s^{} p_{s,a}}[}_{a^{} (|s^{})}[Q^{}_{,c}(s^{},a^{})] ]).\]

These recursive definitions motivate corresponding RAMU Bellman operators.

**Definition 3** (RAMU Bellman operators).: _For a given policy \(\), the RAMU Bellman operators are defined as_

\[^{}_{^{+},r}Q(s,a) :=r(s,a)+_{p_{s,a}_{s,a}}^{+}( }_{s^{} p_{s,a}}[}_{a^{ }(|s^{})}[Q(s^{},a^{})]] ),\] \[^{}_{,c}Q(s,a) :=c(s,a)+_{p_{s,a}_{s,a}}( }_{s^{} p_{s,a}}[}_{a^{ }(|s^{})}[Q(s^{},a^{})]] ).\]

Note that the RAMU Bellman operators can also be interpreted as applying a coherent distortion risk measure over standard Bellman targets, which are random variables with respect to the transition model \(p_{s,a}_{s,a}\) for a given state-action pair.

**Lemma 1**.: _The RAMU Bellman operators can be written in terms of standard Bellman operators as_

\[^{}_{^{+},r}Q(s,a)=_{p_{s,a}_{s,a}}^{+} (^{}_{p,r}Q(s,a)),^{}_{,c}Q(s,a)=_{p_{s,a}_{s,a}}(^{}_{p,c}Q(s,a) ).\] (5)

Proof.: The results follow from the definitions of \(^{}_{p,r}\) and \(^{}_{p,c}\), along with the translation invariance and positive homogeneity of coherent distortion risk measures. See the Appendix for details. 

In the next section, we show that \(^{}_{^{+},r}\) and \(^{}_{,c}\) are contraction operators, so we can apply standard temporal difference learning techniques to learn the RAMU Q functions \(Q^{}_{^{+},r}(s,a)\) and \(Q^{}_{,c}(s,a)\) that are needed for our RAMU policy update in (4).

## 5 Robustness guarantees

Intuitively, our risk-averse perspective places more emphasis on potential transition models that result in higher costs or lower rewards under the current policy, which should result in learning safe policies that are robust to model uncertainty. Next, we formalize the robustness guarantees of our RAMU framework by showing it is equivalent to a distributionally robust safe RL problem for appropriate choices of ambiguity sets.

**Theorem 1**.: _The RAMU safe RL problem in (3) is equivalent to the distributionally robust safe RL problem_

\[_{}\ _{^{+}}}_{p}[J_{p,r} ()]\ \ \ \ \ \ _{}}_{p}[J_{p,c}()] B\] (6)

_with ambiguity sets \(^{+}=_{(s,a)} ^{+}_{s,a}\) and \(=_{(s,a)}_{s,a}\), where_

\[^{+}_{s,a},\ _{s,a}\{_{s,a} P( )_{s,a}=_{s,a}_{s,a},\ _{s,a}^{*}\}\]

_are sets of feasible reweightings of \(_{s,a}\) with \(_{s,a}\) that depend on the choice of \(^{+}\) and \(\), respectively._Proof.: Using duality results for coherent risk measures , we show that the RAMU Bellman operators \(^{}_{^{+},r}\) and \(^{}_{,c}\) are equivalent to distributionally robust Bellman operators [53; 56] with ambiguity sets of distributions \(^{+}\) and \(\), respectively. The RAMU Q functions are the respective fixed points of these Bellman operators, so they can be written as distributionally robust Q functions. Finally, by averaging over initial states and actions, we see that (3) is equivalent to (6). See the Appendix for details. 

Theorem 1 shows that the application of \(^{+}\) and \(\) at every timestep are equivalent to solving distributionally robust optimization problems over the ambiguity sets of distributions \(^{+}\) and \(\), respectively. This can be interpreted as adversarially reweighting \(_{s,a}\) with \(_{s,a}\) at every state-action pair. Note that worst-case distributions appear in both the objective and constraint of (6), so any policy trained with our RAMU framework is guaranteed to deliver robust performance _and_ robust safety. The level of robustness depends on the choice of \(^{+}\) and \(\), which determine the structure and size of the corresponding ambiguity sets based on their dual representations .

In addition, because (3) is equivalent to a distributionally robust safe RL problem according to Theorem 1, we can leverage existing results for distributionally robust MDPs [53; 56] to show that \(^{}_{^{+},r}\) and \(^{}_{,c}\) are contraction operators.

**Corollary 1**.: _The RAMU Bellman operators \(^{}_{^{+},r}\) and \(^{}_{,c}\) are \(\)-contractions in the sup-norm._

Proof.: Apply results from Xu and Mannor  and Yu and Xu . See the Appendix for details. 

Therefore, we have that \(Q^{}_{^{+},r}(s,a)\) and \(Q^{}_{,c}(s,a)\) can be interpreted as distributionally robust Q functions by Theorem 1, and we can apply standard temporal difference methods to learn these RAMU Q functions as a result of Corollary 1. Importantly, Theorem 1 demonstrates the robustness properties of our RAMU framework, _but it is not used to implement our approach_. Directly implementing (6) would require solving for adversarial distributions over transition models throughout training. Instead, our framework provides the same robustness, but the use of risk measures leads to an efficient deep RL implementation as we describe in the following section.

## 6 Model-free implementation with a single training environment

The RAMU policy update in (4) takes the same form as the standard safe RL update in (2), except for the use of \(Q^{}_{^{+},r}(s,a)\) and \(Q^{}_{,c}(s,a)\). Because our RAMU Bellman operators are contractions, we can learn these RAMU Q functions by applying standard temporal difference loss functions that are used throughout deep RL. In particular, we consider parameterized critics \(Q_{_{r}}\) and \(Q_{_{c}}\), and we optimize their parameters during training to minimize the loss functions

\[^{+}(_{r}) =}_{(s,a)}[(Q_{ _{r}}(s,a)-}^{}_{^{+},r}_{_{r}}(s,a) )^{2}],\] \[(_{c}) =}_{(s,a)}[(Q_{ _{c}}(s,a)-}^{}_{,c}_{_{c}}(s,a))^{ 2}],\]

where \(}^{}_{^{+},r}\) and \(}^{}_{,c}\) represent sample-based estimates of the RAMU Bellman operators applied to target Q functions denoted by \(\). Therefore, we must be able to efficiently estimate the RAMU Bellman targets, which involve calculating coherent distortion risk measures that depend on the distribution \(_{s,a}\).

Sample-based estimation of risk measuresUsing the formulation of our RAMU Bellman operators from Lemma 1, we can leverage properties of distortion risk measures to efficiently estimate the results in (5) using sample-based weighted averages of standard Bellman targets. For \(n\) transition models \(p^{(i)}_{s,a}\), \(i=1,,n\), sampled independently from \(_{s,a}\) and sorted according to their corresponding Bellman targets, consider the weights

\[w^{(i)}_{}=g()-g(),\]here \(g\) defines the distortion risk measure \(\) according to Definition 2. See Figure 1 for the distortion functions and weights associated with the risk measures used in our experiments. Then, from Jones and Zitikis  we have that

\[_{i=1}^{n}w_{^{+}}^{(i)}_{p^{(i)},r}^{}Q(s,a),_{i= 1}^{n}w_{}^{(i)}_{p^{(i)},c}^{}Q(s,a),\]

are consistent estimators of the results in (5), where \(_{p^{(i)},r}^{}Q(s,a)\) are sorted in ascending order and \(_{p^{(i)},c}^{}Q(s,a)\) are sorted in descending order. Finally, we can replace \(_{p^{(i)},r}^{}Q(s,a)\) and \(_{p^{(i)},c}^{}Q(s,a)\) with the standard unbiased sample-based estimates

\[}_{p^{(i)},r}^{}Q(s,a)=r(s,a)+ Q(s^{},a^{ }),}_{p^{(i)},c}^{}Q(s,a)=c(s,a)+ Q(s^{ },a^{}),\]

where \(s^{} p_{s,a}^{(i)}\) and \(a^{}(\, s^{})\). This leads to the sample-based estimates

\[}_{^{+},r}^{}Q(s,a)=_{i=1}^{n}w_{^{+}}^{(i)} }_{p^{(i)},r}^{}Q(s,a),}_{,c}^{ }Q(s,a)=_{i=1}^{n}w_{}^{(i)}}_{p^{(i)},c}^{}Q(s,a),\] (7)

which we use to train our RAMU Q functions. Note that the estimates in (7) can be computed very efficiently, which is a major benefit of our RAMU framework compared to robust RL methods. Next, we describe how we can sample models \(p_{s,a}^{(i)}\), \(i=1,,n\), from \(_{s,a}\), and generate state transitions from these models to use in the calculation of our sample-based Bellman targets in (7).

Generative distribution of transition modelsNote that our RAMU framework can be applied using any choice of distribution \(\), provided we can sample transition models \(p_{s,a}^{(i)}_{s,a}\) and corresponding next states \(s^{} p_{s,a}^{(i)}\). In this work, we define the distribution \(\) over perturbed versions of a single training environment \(p^{}\), and we propose a generative approach to sampling transition models and corresponding next states that only requires data collected from \(p^{}\). By doing so, our RAMU framework achieves robust performance and safety with minimal assumptions on the training process, and can even be applied to settings that require real-world data collection for training.

We consider a latent variable \(x X\), and we define a transition model \(p_{s,a}(x)\) for every \(x X\) that shifts the probability of \(s^{}\) under \(p_{s,a}^{}\) according to a perturbation function \(f_{x}:\). This perturbation function takes as input a state transition \((s,s^{})\), and outputs a perturbed next state \(^{}\) that depends on the latent variable \(x X\). Therefore, a distribution over latent space implicitly defines a distribution \(_{s,a}\) over perturbed versions of \(p_{s,a}^{}\). In order to obtain the next state samples needed to compute the Bellman target estimates in (7), we sample latent variables \(x X\) and apply \(f_{x}\) to the state transition observed in the training environment. We have that \(s^{} p_{s,a}^{}\) for data collected in the training environment, so \(^{}=f_{x}(s,s^{})\) represents the corresponding sample from the perturbed transition model \(p_{s,a}(x)\).

In our experiments, we consider a simple implementation for the common case where \(=^{d}\). We use uniformly distributed latent variables \(x U([-2,2]^{d})\), and we define the perturbation function as

\[f_{x}(s,s^{})=s+(s^{}-s)(1+x),\]

Figure 1: Coherent distortion risk measures used in RAMU experiments. Left: Distortion function \(g\). Right: Weights for sample-based estimates in (7) when \(n=5\).

where all operations are performed per-coordinate. Therefore, the latent variable \(x U([-2,2]^{d})\) can be interpreted as the percentage change in each dimension of a state transition observed in the training environment, where the average magnitude of the percentage change is \(\). The hyperparameter \(\) determines the distribution \(_{s,a}\) over transition models, where a larger value of \(\) leads to transition models that vary more significantly from the training environment. The structure of \(f_{x}\) provides an intuitive, scale-invariant meaning for the hyperparameter \(\), which makes it easy to tune in practice. This choice of distribution \(_{s,a}\) captures general uncertainty in the training environment, without requiring specific domain knowledge of potential disturbances.

AlgorithmWe summarize the implementation of our RAMU framework in Algorithm 1. Given data collected in a single training environment, we can efficiently calculate the sample-based RAMU Bellman targets in (7) by (i) sampling from a latent variable \(x X\), (ii) computing the corresponding next state samples \(f_{x}(s,s^{})\), and (iii) sorting the standard Bellman estimates that correspond to these sampled transition models. Given the sample-based RAMU Bellman targets, updates of the critics and policy have the same form as in standard deep safe RL algorithms. Therefore, _our RAMU framework can be easily combined with many popular safe RL algorithms to incorporate model uncertainty with robustness guarantees_, using only a minor change to the estimation of Bellman targets that is efficient to implement in practice.

## 7 Experiments

In order to evaluate the performance and safety of our RAMU framework, we conduct experiments on 5 continuous control tasks with safety constraints from the Real-World RL Suite [18; 19]: Cartpole Swingup, Walker Walk, Walker Run, Quadruped Walk, and Quadruped Run. Each task has a horizon length of \(1{,}000\) with \(r(s,a)\) and \(c(s,a)\{0,1\}\), and we consider a safety budget of \(B=100\). Unless noted otherwise, we train these tasks on a single training environment for 1 million steps across 5 random seeds, and we evaluate performance of the learned policies across a range of perturbed test environments via 10 trajectory rollouts. See the Appendix for information on the safety constraints and environment perturbations that we consider.

Our RAMU framework can be combined with several choices of safe RL algorithms. We consider the safe RL algorithm Constraint-Rectified Policy Optimization (CRPO) , and we use Maximum a Posteriori Policy Optimization (MPO)  as the unconstrained policy optimization algorithm in CRPO. For a fair comparison, we apply this choice of safe RL policy update in every method we consider in our experiments. We use a multivariate Gaussian policy with learned mean and diagonal covariance at each state, along with separate reward and cost critics. We parameterize our policy and critics using neural networks. See the Appendix for implementation details.1

We summarize the performance and safety of our RAMU framework in Table 1 and Figure 2, compared to several baseline algorithms that we discuss next. We include detailed experimental results across all perturbed test environments in the Appendix. We apply our RAMU framework using the Wang transform with \(=0.75\) as the risk measure in both the objective and constraint. In order to understand the impact of being risk-averse to model uncertainty, we also consider the risk-neutral special case of our framework where expectations are applied to the objective and constraint. For our RAMU results in Table 1 and Figure 2, we specify the risk measure in parentheses. Finally, we consider \(n=5\) samples of transition models with latent variable hyperparameter \(=0.10\) in order to calculate Bellman targets in our RAMU framework.

Comparison to safe reinforcement learningFirst, we analyze the impact of our RAMU framework compared to standard safe RL. In both cases, we train policies using data collected from a single training environment, so the only difference comes from our use of risk-averse model uncertainty to learn RAMU Q functions. By evaluating the learned policies in perturbed test environments different from the training environment, we see that our RAMU framework provides robustness in terms of both total rewards and safety. In particular, the risk-averse implementation of our algorithm leads to safety constraint satisfaction in 80% of test environments, compared to only 51% with standard safe RL. In addition, this implementation results in higher total rewards (1.08x) and lower total costs (0.51x), on average. We see in Table 1 that the use of expectations over model uncertainty (i.e., a risk-neutral approach) also improves robustness in both the objective and constraint, on average, compared to standard safe RL. However, we further improve upon the benefits observed in the risk-neutral case by instead applying a risk-averse perspective.

Comparison to domain randomizationNext, we compare our RAMU framework to domain randomization, a popular approach that also represents model uncertainty using a distribution \(\) over models. Note that domain randomization considers parametric uncertainty and has the benefit of training on a range of simulated environments, while our method only collects data from a single training environment. In order to evaluate the importance of domain knowledge for defining the training distribution in domain randomization, we consider two different cases: an in-distribution

    & & ^{}\)} & ^{*}\)} \\  Algorithm & \% Safe\({}^{}\) & Reward & Cost & Adversary & Simulator \\  Safe RL & 51\% & 1.00 & 1.00 & No & No \\
**RAMU (Wang 0.75)** & **80\%** & **1.08** & **0.51** & **No** & **No** \\ RAMU (Expectation) & 74\% & 1.05 & 0.67 & No & No \\ Domain Randomization & 76\% & 1.14 & 0.72 & No & Yes \\ Domain Randomization (OOD) & 55\% & 1.02 & 1.02 & No & Yes \\ Adversarial RL & 82\% & 1.05 & 0.48 & Yes & No \\   

* Percentage of policies that satisfy the safety constraint across all tasks and test environments.
* Normalized relative to the average performance of standard safe RL for each task and test environment.
* Denotes need for adversary or simulator during data collection (i.e., trajectory rollouts) for training.

Table 1: Aggregate performance summary

Figure 2: Performance summary by task, aggregated across perturbed test environments. Performance of adversarial RL is evaluated without adversarial interventions. Top: Total rewards averaged across test environments. Bottom: Percentage of policies across test environments that satisfy the safety constraint.

version that trains on a subset of the perturbed test environments, and an out-of-distribution (OOD) version that randomizes over a different perturbation parameter than the one varied at test time.

The results in Table 1 and Figure 2 show the importance of domain knowledge: in-distribution domain randomization leads to improved robustness compared to standard safe RL and the highest normalized average rewards (1.14x), while the out-of-distribution version provides little benefit. In both cases, however, domain randomization achieves lower levels of safety, on average, than our risk-averse formulation. In fact, we see in Figure 2 that the safety constraint satisfaction of our risk-averse formulation is at least as strong as both versions of domain randomization in 4 out of 5 tasks, _despite only training on a single environment with no specific knowledge about the disturbances at test time_. This demonstrates the key benefit of our risk-averse approach to model uncertainty.

Comparison to adversarial reinforcement learningFinally, we compare our approach to adversarial RL using the action-robust PR-MDP framework , which randomly applies worst-case actions a percentage of the time during data collection. Although adversarial RL only collects data from a single training environment, it requires potentially dangerous adversarial interventions during training in order to provide robustness at test time. In order to apply this method to the safe RL setting, we train an adversary to maximize costs and consider a 5% probability of intervention during training. The performance of adversarial RL is typically evaluated without adversarial interventions, which requires a clear distinction between training and testing.

We see in Figure 2 that adversarial RL learns policies that achieve robust safety constraint satisfaction at test time in the Quadruped tasks. Our risk-averse formulation, on the other hand, achieves higher levels of safety in the remaining 3 out of 5 tasks, and similar levels of safety on average. Unlike adversarial RL, our RAMU framework achieves robust safety in a way that (i) does not alter the data collection process, (ii) does not require training an adversary in a minimax formulation, and (iii) does not require different implementations during training and testing. In addition, our use of a distribution over models represents a less conservative approach than adversarial RL, resulting in higher normalized average rewards as shown in Table 1.

## 8 Conclusion

We have presented a framework for safe RL in the presence of model uncertainty, an important setting for many real-world decision making applications. Compared to existing approaches to model uncertainty in deep RL, our formulation applies a risk-averse perspective through the use of coherent distortion risk measures. We show that this results in robustness guarantees, while still leading to an efficient deep RL implementation that does not involve minimax optimization problems. Importantly, our method only requires data collected from a single training environment, so it can be applied to real-world domains where high-fidelity simulators are not readily available or are computationally expensive. Therefore, our framework represents an attractive approach to safe decision making under model uncertainty that can be deployed across a range of applications.

Prior to potential deployment, it is important to understand the limitations of our proposed methodology. The robustness and safety of our RAMU framework depend on the user-defined choices of model distribution \(\) and risk measure \(\). The distribution \(\) defines the uncertainty over transition models, and the risk measure \(\) defines the level of robustness to this choice of \(\). In addition, our approach only considers robustness with respect to model uncertainty and safety as defined by expected total cost constraints. It would be interesting to extend our techniques to address other forms of uncertainty and other definitions of safety, including epistemic uncertainty in model-based RL, observational uncertainty, and safety-critical formulations based on sets of unsafe states.