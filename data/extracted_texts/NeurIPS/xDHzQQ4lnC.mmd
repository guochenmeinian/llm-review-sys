# Probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs

Probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs

 Dominik Straub

Centre for Cognitive Science

Technische Universitat Darmstadt

dominik.straub@tu-darmstadt.de &Matthias Schultheis

Centre for Cognitive Science

Technische Universitat Darmstadt

matthias.schultheis@tu-darmstadt.de &Heinz Koeppl

Centre for Cognitive Science

Technische Universitat Darmstadt

heinz.koeppl@tu-darmstadt.de &Constantin A. Rothkopf

Centre for Cognitive Science

Technische Universitat Darmstadt

constantin.rothkopf@tu-darmstadt.de

equal contribution

###### Abstract

Inverse optimal control can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, is limited to fully observable or linear systems, or requires the action signals to be known. Here, we introduce a probabilistic approach to inverse optimal control for partially observable stochastic non-linear systems with unobserved action signals, which unifies previous approaches to inverse optimal control with maximum causal entropy formulations. Using an explicit model of the noise characteristics of the sensory and motor systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood function for the model parameters, which can be computed within a single forward pass. We present quantitative evaluations on stochastic and partially observable versions of two classic control tasks and two human behavioral tasks. Importantly, we show that our method can disentangle perceptual factors and behavioral costs despite the fact that epistemic and pragmatic actions are intertwined in sequential decision-making under uncertainty, such as in active sensing and active learning. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience.

## 1 Introduction

Inverse optimal control (IOC) is the problem of inferring an agent's cost function and other properties of their internal model from behavior. While IOC has been a fundamental task in artificial intelligence and machine learning, particularly reinforcement learning (RL) and robotics, it has widespread applicability in several scientific fields including behavioral economics, psychology, and neuroscience. For example, in cognitive science and sensorimotor neuroscience, optimal control models have explained key properties of behavior, such as speed-accuracy trade-offs  or the minimum intervention principle . But, while researchers usually build an optimal control model and compare its predictions to behavior, certain parameters of the agent's internal processes are typically unknown. For example, an agent might have uncertainty about their perception or experience intrinsic costs of behavior. These parameters are different between individuals and inferring them from observed behavior can help to understand internal tradeoffs between behavioral goals, perceptual and cognitive processes, and predict behavior under novel conditions. Applying IOC in these domains poses several challenges that make most previous methods not viable.

First, most IOC methods assume the agent's control signals to be known. This assumption, while convenient in simulations or robotics, where the control signals may be easily quantified, does not hold in many other real-world applications. In transfer learning or behavioral experiments, the control signals are internal quantities of an animal or human, e.g., neural activity or muscle activations, and are therefore not straightforwardly observable. Thus, we consider the scenario where a researcher has observations of the system's state only, i.e., measurements of behavior.

Second, most IOC methods model the variability of the agent using a stochastic policy in a maximum causal entropy formulation [MCE; 3]. Behavioral variability in biological systems, however, is known to arise from multiple distinct sources . There is noise in the sensory system, which makes the state of the world partially observable, and in the motor system. In sensorimotor neuroscience, the uncertainty in the sensory and motor systems can be characterized quantitatively by formulating accurate models, which are helpful to understand behavioral variability .

Third, many IOC methods are based on matching feature expectations of the cost function between the model and data , and are thus not easily adapted to infer other model parameters. In a behavioral experiment, however, researchers are often interested in inferring the noise characteristics of the sensorimotor system or other properties of the agent's internal model besides the cost function .

Fourth, while the theory of linear-quadratic-Gaussian (LQG) control  is suited to deal with the issues above, many real-world systems are not captured by the LQG assumptions. First, the dynamics may be non-linear, e.g., in robotics and motor control when controlling joint angles in a kinematic chain. Second, the variability of the system may not be captured by normal distributions, e.g., in sensorimotor control, where the standard deviation of sensory and control signals scales with their means. While iterative methods for solving the optimal control problem such as iterative variants of LQG  exist, here we consider the corresponding inverse problem.

To address these issues, we adopt a probabilistic perspective. We distinguish between the control problem faced by the agent and the IOC problem faced by the researcher. From the agent's perspective, the problem consists of acting in a partially observable Markov decision process (POMDP; Fig. 1 A). We consider the setting of continuous states and controls, stochastic non-linear dynamics, partial observations, and finite horizon. For this setting, there are efficient approximately optimal solutions to the estimation and control problem (see Section 2). The researcher, on the other hand, is interested in inferring properties of the agent's model and cost function. The IOC problem from their perspective can be formulated using a probabilistic graphical model (Fig. 1 B), in which the state of the system is observed, while variables internal to the agent are latent.

Here, we unify MCE models, which are agnostic regarding the probabilistic structure causing the observed stochasticity of the agent's policy, with IOC methods that involve an explicit observation model. First, we define the IOC problem where we allow for both: We employ an explicit observation model, but also allow the agent to have additional stochasticity through an MCE policy. Second, we provide a solution to the IOC problem in this setting by approximate filtering of the agent's state estimate via local linearization, which allows marginalizing over these latent variables and deriving an approximate likelihood function for observed trajectories given parameters (Section 3). By maximizing the approximate likelihood function, an estimate of the optimal parameters can be determined. Third, we evaluate our proposed method on two classic control tasks, pendulum and cart pole, and on two human behavioral tasks, navigation and a manual reaching (Section 4.2 - Section 4.3). Fourth, we show that our approach allows disentangling the influences of perceptual uncertainty and behavioral costs on information-seeking behavior in the light-dark domain (Section 4.4).

Figure 1: **A** Decision network from the agent’s perspective [notation from 9]. At each time step, the agent receives a noisy observation \(_{t}\) of the state \(_{t}\), performs a control \(_{t}\), and incurs a cost \(c_{t}\). **B** Probabilistic graphical model from the researcher’s perspective, who observes a trajectory \(_{1:T}\) of an agent. Quantities internal to the agent, i.e. observations \(_{t}\), beliefs \(_{t}\) and control signals \(_{t}\), are not directly observed.

#### Related work

Inferring costs or utilities from behavior has been of interest for a long time in several scientific fields, such as behavioral economics, psychology, and neuroscience [10; 11; 12]. More specific to the problem formulation adopted here, estimating objective functions in the field of control was first investigated by Kalman  in the context of deterministic linear systems with quadratic costs. More recent formulations were developed first for discrete state and control spaces under the term inverse reinforcement learning [IRL; 14; 15], including formulations allowing for stochasticity in action selection . In this line, the maximum entropy [ME; 17] and MCE formulation  gave rise to many new methods, e.g., for non-linear continuous systems via linearization  or importance sampling  for fully observable deterministic systems.

IOC methods for stochastic systems have been developed in the setting of affine control dynamics [20; 21]. Arbitrary non-linear stochastic dynamics in the infinite horizon setting have been approached using model-free deep MCE IRL [22; 23]. The latter approaches, however, do not yield interpretable representations, as the cost function is represented by a neural network. Further, past methods based on MCE are limited to estimating cost functions and cannot be used to infer other latent quantities, such as noises or subjective beliefs. The partially observable setting for IOC has previously been addressed for discrete state-action spaces  and continuous states with discrete actions . Schmitt et al.  addressed systems with linear dynamics and continuous controls for a linear switching observation model. Other work has considered partial observability from the researcher's perspective, e.g., through occlusions [27; 28]. There are some IOC methods which are applicable to partially observable stochastic systems: In our previous work  we regarded LQG systems, while the work of Chen and Ziebart  can be used to estimate cost functions that depend on the state only. Non-linear dynamics in the infinite-horizon setting and the joint estimation of model parameters have been approached by Kwon et al.  by training a policy network as a function of the whole parameter space. This work, however, also assumes the control signals to be given and a stationary, deterministic policy.

Applications of IOC methods range from human locomotion  over spatial navigation , table tennis , to attention switching , and target tracking . Other work has been aimed at inferring other properties of control tasks, e.g., the dynamics model , learning rules , or discount functions . Several subfields of robotics including imitation and apprenticeship learning  as well as transfer learning  have also employed IOC.

## 2 Background

Before we introduce our probabilistic approach to inverse optimal control, we give an overview of the control and filtering problems faced by the agent and algorithms that can be used to solve it. For a summary of our notation in this paper, see Appendix A.

### Partially observable Markov decision processes (POMDPs)

We consider a special case of POMDPs [41; 42], a discrete-time stochastic non-linear dynamical system (Fig. 1 A) with states \(_{t}^{n}\) following the dynamics equation \(_{t+1}=f(_{t},_{t},_{t})\), where \(f\) is the dynamics function, \(_{t}^{u}\) are the controls and \(_{t}(0,I)\) is \(v\)-dimensional Gaussian noise. We assume that the agent has only partial observations \(_{t}^{m}\), following \(_{t}=h(_{t},_{t})\), with \(h\) the stochastic observation function and \(_{t}(0,I)\)\(w\)-dimensional Gaussian noise. While \(_{t}\) and \(_{t}\) are standard normal random variables, the system can incorporate general control- and state-dependent noises through non-linear transformations within the dynamics function \(f\) and observation function \(h\). The agent's goal is to minimize the expected cost over a time horizon \(T\), defined by

\[J=[c_{T}(_{T})+_{t=1}^{T-1}c_{t}(_{t},_{ t})],\]

consisting of a final state cost \(c_{T}(_{T})\) and a cost at each time step \(c_{t}(_{t},_{t})\).

### Iterative linear-quadratic Gaussian (iLQG)

The control problem from Section 2.1 can be solved approximately using iLQG [8; 43]. This method iteratively linearizes the dynamics and quadratizes the costs around a nominal trajectory, \(\{}_{i},}_{i}\}_{i=1,,T}\), with \(}_{i}^{n},}_{i}^{n}\), and computes the optimal linear control law, \(_{t}=_{t}(_{t})=L_{t}(_{t}-}_{t})+_{t }+}_{1:T}\) for the approximated system. The quantities \(L_{t}\) and \(_{t}\) are the control gain and offset, respectively, and determined through a backward pass for the current reference trajectory. In the following iteration, the determined optimal control law is used to generate a new reference trajectory and the process is repeated until the controller converges.

### Maximum causal entropy (MCE) reinforcement learning

The goal of MCE RL is to minimize the expected cost as in Section 2.2, while maximizing the conditional entropy of the stochastic policy \(_{t}(_{t}_{t})\), i.e., to minimize \([J(_{1:T},_{1:T})-_{t=1}^{T-1}(_{t}( _{t}_{t}))]\). This formulation has been used to treat RL as probabilistic inference [44; 45; 46] and model the stochasticity of the agent in IRL [17; 3]. The objective of IRL is to maximize the likelihood of states and controls \(\{_{t},_{t}\}_{t=1,,N}\), induced by the maximum entropy policy. The resulting optimal policy is given by the distribution \(_{t}(_{t}_{t})=(Q_{t}(_{t},_{t})-V_{t}( {x}_{t}))\), where \(Q_{t}\) is the soft Q-function and \(V_{t}\) the normalization . For general dynamics and reward functions, it is not feasible to compute the soft Q-function exactly. Approximate solutions have been derived using linearization , importance sampling , or deep function approximation . For linear dynamics and quadratic costs, the optimal policy is a Gaussian distribution \(_{t}(_{t}_{t})=(_{t};L_{t}_{t},-H_{t }^{-1})\), where \(L_{t}\) and \(H_{t}\) result from the optimal LQG controller . More detailed formulas are provided in Appendix B.

### Extended Kalman filter (EKF)

Given the system defined in Section 2.1, the optimal filtering problem is to compute a belief distribution of the current state given past observations, i.e., \(p(_{t}_{1:t-1})\). For linear-Gaussian systems, the solution is given in closed form and known as the Kalman filter . In case of non-linear systems as in Section 2.1, a Gaussian approximation to the optimal belief can be computed using the extended Kalman filter (EKF) via \(_{t+1}=f(_{t},_{t},0)+K_{t}(_{t}-h( _{t},0))\), where \(_{t}^{n}\) denotes the mean of the Gaussian belief \(p(_{t}_{1},,_{t-1})\). The matrix \(K_{t}\) denotes the Kalman gain for time \(t\) and is computed by applying the Kalman filter to the system locally-linearized around the nominal trajectory obtained by the approximate optimal control law of iLQG (Section 2.2).

## 3 Probabilistic inverse optimal control

We consider an agent acting in a partially observable Markov decision process (POMDP) as introduced in Section 2.1. We assume that the agent acts at time \(t\) based on their belief \(_{t}\) about the state of the system \(_{t}\), which evolves according to \(_{t+1}=_{t}(_{t},_{t},_{t})\). While the belief of the agent is defined commonly as a distribution over the true state, here we model \(_{t}\) as a finite-dimensional summary statistics of the distribution, i.e., \(_{t}^{b}\). The function \(_{t}:^{b}^{u}^{m}^ {b}\) is called belief dynamics. We further assume that the agent follows a time-dependent policy \(_{t}:^{b}^{j}^{u}\), i.e., \(_{t}=_{t}(_{t},_{t})\), which can be stochastic with \(_{t}(0,I)\).

In the inverse optimal control problem, the goal is to estimate parameters \(^{p}\) of the agent's optimal control problem given the model and trajectory data. These parameters can include properties of the agent's cost function, the sensory and control systems of the agent, or the system's dynamics. More precisely, we assume that we are given a parametric form of the system dynamics, belief dynamics, cost function, and initial belief of the agent, which all might depend on the unknown parameters that we aim to infer. Further we assume a set of state trajectories. Importantly, we do not assume knowledge of the agent's belief. We follow a probabilistic approach to inverse optimal control, i.e., we consider the likelihood function

\[p(_{1:T})=p(_{1})_{t=1}^{T-1}p( _{t+1}_{1:t},),\] (1)describing the probability of the observed trajectory data \(_{1:T}:=\{_{1},,_{T}\}\) given the parameters. For a set of trajectories, we assume them to be independent given the parameters, so that the likelihood factorizes into single trajectory likelihoods of the form in Eq. (1). In this equation, generally, each state \(_{t+1}\) depends on all previous states \(_{1},,_{t}\), because the agent's internal noisy observations and control signals are not accessible to the researcher (Fig. 1 B). Therefore, the Markov property does not hold from the researcher's perspective, rendering computation of the likelihood function intractable. To deal with this problem, we employ two key insights: First, the joint dynamical system of the states and the agent's belief is Markovian . Second, by keeping track of the distribution over the agent's belief, i.e., by performing belief tracking , we can iteratively compute the individual factors of the likelihood function in Eq. (1). In our IOC method, the goal is to maximize the likelihood w.r.t. the parameters \(\). To do so, we use gradient-based optimization with automatic differentiation to differentiate through the likelihood for computing the optimal parameters (Algorithm 1). An implementation of our algorithm is publicly available2.

We first introduce a general formulation of the IOC likelihood involving marginalization over the agent's internal beliefs in Section 3.1. Then, we show how to make the computations tractable by local linearization in Section 3.2. In Section 3.3, we provide details for suitable linearization points, which enables us to evaluate the approximate likelihood within a single forward pass.

### Likelihood formulation

We start by defining a joint dynamical system of states and beliefs , in which each depends only on the state and belief at the previous time step and the noises. For that, we insert the policy into the dynamics and the policy and observation function into the belief dynamics, yielding the equation

\[_{t+1}\\ _{t+1}=f(_{t},_{t}(_{t},_{t}),_{t})\\ _{t}(_{t},_{t}(_{t},_{t}),h(_{t},_{t})) =:g(_{t},_{t},_{t},_{t},_{t}).\] (2)

For given values of \(_{t}\) and \(_{t}\), this equation defines the distribution \(p(_{t+1},_{t+1}_{t},_{t})\), as \(_{t},_{t},_{t}\) are independent of \(_{t+1}\) and \(_{t+1}\). Importantly, with this formulation, the control signals are only implicitly regarded through the policy, as we assume them to be latent for the researcher. In Section 3.2 we will introduce an approximation via linearization, which leads to a closed-form expression for \(p(_{t+1},_{t+1}_{t},_{t})\).

One can use this Markovian joint dynamical system to compute the likelihood factors for each time step . To this end, we first rewrite the individual likelihood terms \(p(_{t+1}_{1:t})\) of Eq. (1) by marginalizing over the agent's belief at each time step, i.e.,

\[p(_{t+1}_{1:t})= p(_{t+1},_{t+1}_{ 1:t})\,_{t+1}.\] (3)

As the belief is an internal quantity of the agent and thus not observable to the researcher, we keep track of its distribution, \(p(_{t}_{1:t})\). For this, we rewrite

\[p(_{t+1},_{t+1}_{1:t})= p(_{t+1},_{t+1} _{t},_{t})\,p(_{t}_{1:t})\,_{ t},\] (4)

where we have exploited the fact that the joint dynamical system of states and beliefs is Markovian. The distribution \(p(_{t}_{1:t})\) acts as a summary of the past states and can be computed by conditioning on the current state, i.e.,

\[p(_{t}_{1:t})=_{t},_{t}_{1:t-1}) }{p(_{t}_{1:t-1})}.\] (5)

After determining \(p(_{t}_{1:t})\), we can propagate it through the joint dynamical system to arrive at the distribution \(p(_{t+1},_{t+1}_{1:t})\). To obtain the belief distribution of the following time step, \(p(_{t+1}_{1:t+1})\), we condition on the observed state \(_{t+1}\). To obtain the likelihood contribution, on the other hand, we marginalize out \(_{t+1}\). To summarize, starting with an initialization \(p(_{0})\), we can compute the individual terms \(p(_{t+1}|_{1:t})\) of the likelihood by executing Algorithm 1 (Appendix C).

### Tractable likelihood via local linearization

While the marginalization and propagating operations in the previous section can be done in closed form for linear-Gaussian systems, this is no longer feasible for non-linear systems. Therefore, we follow the approach of local linearization used in iLQG (Section 2.2) and the EKF (Section 2.4). For the belief statistics, we consider the mean of the agent's belief, i.e., \(_{t}=[_{t}_{1},,_{t-1}]\) and initialize the distribution for the first time step as a Gaussian, \(p(_{1})=(_{1}^{(b)},_{1}^{(b)})\). We then approximate the distribution \(p(_{t+1},_{t+1}_{t},_{t})\) as a Gaussian by applying a first-order Taylor expansion of \(g\).

To obtain a closed-form expression for \(g\), which we can linearize, we model the agent's policy using iLQG (Section 2.2) and the belief dynamics using the EKF (Section 2.4). This choice leads to an affine control and belief given \(_{t}\), making linearization of \(p(_{t+1},_{t+1}_{t},_{t})\) straightforward. To allow for additional stochasticity in the agent's policy, we use the MCE formulation (Section 2.3). For linearized dynamics, the MCE policy is given by a Gaussian distribution, so that \(_{t}(_{t},_{t})=L_{t}(_{t}-}_{1:T })+_{t}+}_{1:T}-_{t}_{t}\), with \(_{t}\) the Cholesky decomposition of \(H_{t}\), and can be marginalized out in closed form.

The approximations we have introduced allow us to solve the integral in Eq. (4) in closed form by applying standard equations for linear transformations of Gaussians, resulting in

\[p(_{t+1},_{t+1}_{1:t})(_{ t},_{t}),\] (6)

with \(_{t}=g(_{t},_{t}^{(b)},0,0,0)\) and \(_{t}=_{}_{t}^{(b)}_{}^{T}+ J_{}_{}^{T}+_{}_{ }^{T}+_{}_{}^{T}\), where \(_{}\) denotes the Jacobian of \(g\) w.r.t. \(\), evaluated at \((_{t},_{t}^{(b)},0,0,0)\). Under this Gaussian approximation, both remaining operations of Algorithm 1 (Appendix C) can also be performed in closed form. A more detailed derivation and representation of these formulas can be found in Appendix D. If the agent has full observations of the system's state, the inverse optimal control problem is simplified significantly (see Appendix E). Details about the implementation are provided in Appendix F.

### Data-based linearization

The forward optimal control problem is commonly solved by starting with a randomly initialized nominal trajectory and iterating between computing the locally optimal control law and linearization until convergence. To compute the likelihood in the inverse problem, we can take a more efficient approach by linearizing directly around the given trajectory \(_{1:T}\). We then need to perform only one backward pass to compute an approximately optimal control law given the current parameters, and a forward pass to compute an approximately optimal filter. This, in particular, allows efficient computation of the gradient of the likelihood function for the optimization procedure. As we assume the controls to be unobservable, but they are needed for the linearization, we compute estimates of the controls by minimizing the squared difference of the noiseless predicted states and the actual states (see Appendix G). Note that these estimated controls are only used for the linearization, but are not used as observed controls in the IOC likelihood itself. In the case where the full state is not observable, we cannot linearize around the trajectory. For these cases, we propose two approaches to compute gradients based on implicit differentiation and differentiating only through the last iteration. As this setting is not the main focus of this paper, details of these approaches are provided in Appendix H.

## 4 Experiments

We evaluated our method on two classic control tasks, i.e., Pendulum and Cart Pole, and two human behavioral tasks, manual reaching and navigation. To evaluate the accuracy of the parameter estimates obtained by our method and to compare it against a baseline, we computed absolute relative errors per parameter, i.e., \(|(-)/|\). This metric makes averages across parameters on different scales more interpretable compared to other metrics such as root mean squared errors. For each task, we simulated 100 sets of parameters from a uniform distribution in logarithmic space. For each set of parameters, we simulated 50 trajectories. We then maximized the log likelihood using gradient-based optimization with automatic differentiation [L-BFGS algorithm; 51]. See Appendix I for a summary of the hyperparameters of our experiments.

All tasks we consider have four free parameters: control cost \(c_{a}\), cost of final velocity \(c_{v}\), motor noise \(_{m}\), and observation noise \(_{o}\) of the agent. In the fully observable case, we leave out the observation noise parameter and only infer the three remaining parameters. For concrete definitions of the parameters in each specific task, see Appendix J.

### Baseline method

For a comparison to previously proposed methods, we applied a baseline method based on the maximum causal entropy (MCE) approach . As for this approach, control signals of the observed trajectories are required, we use the estimates of the controls that we determine in our proposed method for the data-based linearization (Section 3.3). Note that the baseline, representative for applicable past IOC methods, does not have an explicit model of partial observability. Further note that past methods based on MCE are limited to estimating cost functions, so that parameters such as the agent's noise cannot be inferred. For the specific MCE linearization-based baseline we consider, it is actually straightforward to maximize the likelihood with respect to the noise parameters, which enables us to evaluate noise estimates. To show that this approach constitutes a suitable baseline, in

Figure 2: **IOC for non-linear reaching.****A** Simulated trajectories for eight targets. Increasing the control cost and the motor noise affects the trajectories, since reaching the target becomes less important and variability increases. **B** IOC log likelihood for control cost \(c_{a}\) and motor noise \(_{m}\). The maximum likelihood estimate (pink cross) is close to the ground truth parameters (black dot). **C** Simulated trajectories using the MLEs from **B**. The simulations are visually indistinguishable from the ground truth data. **D** True parameters plotted against maximum likelihood estimates. Top row: our method, bottom row: MCE baseline. The columns contain the four different model parameters (control cost \(c_{a}\), velocity cost \(c_{v}\), motor noise \(_{m}\), observation noise \(_{o}\)).

Appendix K.3, we provide results for the case where the true control signals are known and there is no partial observability. More details of the baseline are provided in Appendix B.

### Evaluation on manual reaching task

We evaluate the method on a reaching task with a non-linear two-joint biomechanical arm model, which has been applied to reaching movements in the sensorimotor neuroscience literature (e.g., 52, 53). The agent's goal is to move its arm towards a target at position \(^{}\) by controlling the torque to the two joints. This objective is expressed as a non-quadratic cost function of the joint angles,

\[J=\|_{T}-^{}\|^{2}+c_{v}\|}_{T}\|^{2}+c_{a}_{t=1}^{T-1}\|_{t}\| ^{2},\] (7)

since the final position and velocity of the hand \(_{T},}_{T}\) are non-linear functions of the joint angles. See Appendix J.1 for details.

We use a fully observable  and a partially observable version of the task . Fig. 2 A shows simulations from the model with two different parameter settings. Evaluating the likelihood function for a grid of two of the parameters (Fig. 2 B) confirms that it has its maxima close to the true parameter values. Simulated data using the maximum likelihood estimates look indistinguishable from the ground truth data (Fig. 2 C).

In Fig. 2 D, we show maximum likelihood estimates and true values for repeated runs with different random parameter settings. The parameter estimates of our method closely align with the true parameter values, showing that we can successfully recover the parameters from data. The baseline method, in contrast, shows considerably worse performance, in particular for estimating noises due to the lacking explicit representation of partial observability. Importantly, even when the true control signals are provided, the noise parameter estimates of the baseline are not well estimated (Appendix K.3). Estimates for the fully observable case are provided in Appendix K.2. The median absolute relative errors of our method were 0.11, while they were 0.93 for the baseline. The influence of missing control signals and of the lack of an explicit observation model in the baseline can be observed by comparing the results to the fully observable case and the case of given control signals in Appendix K.2 and Appendix K.3.

### Quantitative evaluation on other tasks

To show that our method works for a range of different tasks, we evaluated it on the three other tasks (navigation, pendulum and cart pole). In the navigation task, we consider an agent navigating to a target under non-linear dynamics while receiving noisy observations from a non-linear observation model. To reach the target, the agent can control the angular velocity of their heading direction and the acceleration with which they move forward. The agent observes noisy versions of the distance to the target and the target's bearing angle. We provide more details about the experiment in Appendix J.2. Maximum likelihood parameter estimates for the navigation task are shown for the partially observable case in Fig. S6 and for the fully observable case in Fig. S10. As for the reaching task, our method provides parameter estimates close to the true ones, while the estimates of the baseline deviate for many trials. Median absolute relative errors of our method were 0.31, while they were 1.99 for the baseline (Fig. 3).

The two classic control tasks (Pendulum and Cart Pole) are based on the implementations in the gym library . Because these tasks are neither stochastic nor partially observable in their standard formulations, we introduce noise on the dynamics and turn them into partially observable problems by defining a stochastic observation function (see Appendix J.3). In Appendix K, we show the parameter estimates for the Pendulum (Fig. S4) and for the Cart Pole (Fig. S5) for the partially observable case, while Fig. S8 and Fig. S9 show the fully observable case, respectively. One can observe that the results are qualitatively similar to the ones in the reaching and navigation tasks, showing that our method provides accurate estimates of the parameters. Median absolute relative errors of our method were 0.12 and 0.41, while for the baseline they were 2.21 and 3.82 (Fig. 3).

### Information-seeking behavior in the light-dark domain

Finally, we investigate the ability of our method to disentangle sources of information-seeking behavior. In the light-dark domain , an agent moves in a 2D space and receives noisy measurements of its position, whose standard deviation depends on the horizontal distance from a light source:

\[_{t}=_{t}+|x_{t,1}-5|\ _{t},\] (8)

where \(\) governs the amount of perceptual uncertainty. This task is a common test for information-seeking behavior, because it requires the agent to move towards the light source and at the same time away from the target to reduce the uncertainty about its position relative to the target. When this uncertainty has been reduced, the agent can approach the target (see Appendix J.4 for details). The agent's goal is to reach the target's position \(\) at the final time step, while minimizing control effort \(_{i}^{2}\):

\[J=_{T}-)^{2}}_{}+^{T-1}_{t}^{2}+c(x_{t,1}-5)^{2}}_{ {running cost}}.\] (9)

Different from the original problem formulation, we consider the case in which the agent may have an additional inherent desire to be close to the light, parameterized by \(c\). We recover the original cost function  for \(c=0\), but we can represent agents that seek light more than necessary to reduce uncertainty for reaching the goal state with \(c>0\). Accordingly, both the reduction of perceptual uncertainty and state-dependent running cost could encourage the agent to move towards the light source before approaching the target. For an external observer, e.g., an experimenter, observing an agent moving towards the light might not reveal the different potential sources for this behavior. We now ask if it is possible to disentangle these two factors using our proposed IOC algorithm.

We simulated 100 trajectories of an iLQG agent [partially observable version, 43] with no inherent desire to be near the light source (\(c=0\)) and some perceptual uncertainty (\(=0.2\)) depending on the distance to the light source. The agent first moves towards the light source and then reaches the target (Fig. 4 A). We inferred the parameters \(\), \(\), and \(c\) using our method and the baseline. Both methods recover the target position. Our method in addition infers values close to the true \(c\), and \(\). It therefore correctly attributes the information-seeking behavior of the agent to the perceptual uncertainty (Fig. 4 B). The baseline method, however, does not infer the correct

Figure 4: **Light-dark domain.** Cost maps and perceptual uncertainty map plotted with true parameters (**A**) and inferred parameters using our method (**B**) and baseline (**C**). Mean trajectories with start point (circle) and target (cross) are shown in orange.

Figure 3: **Evaluation across tasks.** Absolute relative errors (log scale) for different tasks. Our method consistently outperforms the MCE baseline.

perceptual uncertainty. It instead attributes the agent's information-seeking behavior to an inherent desire to be in the right part of the room in the running cost (Fig. 4 C). This highlights the importance for IOC in partially observable domains to probabilistically take the agent's belief into account, if one is interested in inferring the correct cognitive mechanisms. For a quantitative evaluation of the maximum likelihood estimates in the light-dark domain, see Appendix K.4.

## 5 Conclusion

In this paper, we introduced a new IOC method for partially observable systems with stochastic non-linear dynamics and missing control signals. Using a probabilistic approach, we formulate the IOC problem as maximizing the likelihood of the observed states given the parameters. As the exact evaluation of the likelihood for a general non-linear model is intractable, we developed an efficient approximate likelihood by linearizing the system locally around the given trajectories, as in popular approaches such as the EKF or iLQG. By maintaining a distribution that tracks the agent's belief, an approximate likelihood can be evaluated in closed form within a single forward pass and efficiently optimized. Our proposed formulation is able to incorporate multiple sources of the stochasticity of the agent, reconciling the theory of past MCE IOC algorithms (e.g.,  and approaches where the agent's stochasticity stems from an explicit stochastic observation and control model .

We evaluated our method on two stochastic variants of classic control tasks, pendulum and cart pole, and on two human behavioral tasks, a reaching and a navigation task. In comparison to an MCE baseline, we have found our method to achieve lower estimation errors across all tasks. Further, it successfully inferred noise parameters of the system, which was not possible with the baseline. In the light-dark domain, we were able to infer costs and perceptual uncertainty parameters, two different causes of apparent information-seeking behavior that could lead to qualitatively similar trajectories. This means that our method is a first step towards distinguishing pragmatic from epistemic controls. To further investigate this, it would be fruitful to examine belief-space planning methods that explicitly include the agent's belief covariance in the policy .

The limitations of our method are mainly due to the linearization of the dynamical system and the Gaussian approximations involved in the belief tracking formulation of the likelihood function. In more complex scenarios with non-Gaussian belief distributions, e.g., multimodal beliefs, the method will likely produce inaccurate results. This problem could be addressed by replacing the closed-form Gaussian belief by particle-based methods . Further, we focused on tasks which could be solved well by control methods based on linearization and Gaussian approximation (iLQG and EKF), motivated by their popularity in applications in cognitive science and neuroscience. Forward problems that cannot be solved using iLQG are probably not directly solvable using our inverse method. While, in principle, our method is also applicable to other forward control methods that compute differentiable policies, it is an empirical question whether linearizing these policies leads to accurate approximate likelihoods and parameter estimates. A further limitation of our method is that it requires parametric models of the dynamics and noise structure. While missing parameters can be determined using our method, in the case of completely unknown dynamics a model-free approach to IOC would be more suitable. Lastly, while we have shown that inference is feasible, the results probably do not scale to high-dimensional parameter spaces. One reason for this is that optimization in a high-dimensional non-linear space can potentially get stuck in local minima. This problem could be relieved by using more advanced optimization methods. A further, more fundamental, concern with higher-dimensional parameter spaces is that identifiability issues and ambiguous solutions arise. However, our probabilistic approach with a closed-form likelihood opens up the possibility of using Bayesian methods to investigate the identifiability of model parameters . For future work exploring the relationship to methods for learning world models in POMDPs might also be a fruitful direction [59; 60].

Our method provides a tool for researchers, e.g., in sensorimotor domains, to model sequential behavior by inferring an agent's subjective costs and internal uncertainties. This will enable answering novel scientific questions about how these quantities are affected by different experimental conditions, how they deviate from intended task goals and provided task instructions, or how they vary between individuals. This is particularly relevant to a computational understanding of naturalistic behavior [61; 62; 63], for which subjective utilities are mostly unknown.