# Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel

Valerii Likhosherstov

University of Cambridge

v.lihosherstov@gmail.com &Krzysztof Choromanski

Google DeepMind &

Columbia University

kchoro@google.com &Avinava Dubey

Google Research

&Frederick Liu

Google Research &Tamas Sarlos

Google Research &Adrian Weller

University of Cambridge &

The Alan Turing Institute

 Equal contributionWork done while at University of Cambridge. The author is at Waymo now.

###### Abstract

The problem of efficient approximation of a linear operator induced by the Gaussian or softmax kernel is often addressed using _random features_ (RFs) which yield an unbiased approximation of the operator's result. Such operators emerge in important applications ranging from kernel methods to efficient Transformers. We propose parameterized, positive, non-trigonometric RFs which approximate Gaussian and softmax-kernels. In contrast to traditional RF approximations, parameters of these new methods can be optimized to reduce the variance of the approximation, and the optimum can be expressed in closed form. We show that our methods lead to variance reduction in practice (\(e^{10}\)-times smaller variance and beyond) and outperform previous methods in a kernel regression task. Using our proposed mechanism, we also present FAVOR#, a method for self-attention approximation in Transformers. We show that FAVOR# outperforms other random feature methods in speech modelling and natural language processing.

## 1 Introduction

Random feature decomposition is an important technique for the linearization of nonlinear kernel functions with theoretical guarantees such as unbiasedness and concentration around the true kernel value. Linearization allows a significant reduction in computations from quadratic to linear complexity in the size of the operator induced by the kernel. The technique emerged under the name of _random kitchen sinks_ (RKS) introduced in  and was used in many applications such as kernel SVM , dimensionality reduction , neural networks , function-to-function regression , kernel regression , nonparametric adaptive control , differentially-private ML algorithms , operator-valued kernels  and semigroup kernels . An in-depth theoretical analysis of random features was performed by .

An exciting recent application of random features is in the area of scalable Transformer networks , where the self-attention matrix is approximated as a low-rank matrix when the sequence is long. However, the RKS family of methods relies on the Fourier transform, resulting in \(\) and \(\) types of random features, which were shown to be unsuitable for application in Transformers due to negative values in the low-rank matrix.  proposed a solution in the form of positive-valued random features relying on the exponential function (_positive random features_, _PosRFs_), yieldinga method they called _Fast Attention Via Orthogonal positive Random features (FAVOR++)_ for self-attention approximation. This solution was improved by  by means of a careful choice of the linear combination parameters under the exponent, and the so-called _homogeneity heuristic_, which allows a choice of one set of parameters for all approximated values. The resulting random features were called _generalized exponential random features (GERFs)_, and the corresponding self-attention approximation method was termed _FAVOR++_.

**Contributions:** In this paper, we make a leap forward in the design of positive-valued random features by proposing _dense exponential random features (DERFs)_ which contain both PosRFs and GERFs as special cases. Instead of scalar parameters as in GERFs, DERFs rely on matrix parameters and dense quadratic forms inside the exponent. We show how to select parameters of the new random features efficiently without harming the overall subquadratic complexity.

More technically, our contributions are as follows:

1. We show that the homogeneity heuristic of  may in fact be viewed not as a heuristic, but a closed-form optimum of the _shifted log-variance objective_.

2. We introduce DERFs and three special instantiations: _asymmetric_ DERFs (_ADERFs_), _symmetric_ DERFs (_SDERFs_), and _simplified_ ADERFs (_SADERFs_). All these instantiations contain GERFs as a special case (Figure 1, left). For each instantiation we prove that a closed-form optimum of the shifted log-variance objective can be found efficiently.

3. We show that our new variants result in lower variance than GERFs and other previous methods in practice (e.g. up to \(e^{10}\) times variance improvement as in Figure 1, right). Further, we show that DERFs outperform other random features in kernel regression and Transformer setups (speech modelling and natural language processing). We refer to the DERF-based self-attention approximation method as _FAVOR#_.

## 2 Prerequisites

### Scaled softmax kernel and random features

By the _scaled softmax kernel_\(K^{()}:^{d}^{d}(0,+)\), where \(\), we denote a mapping defined as \(K^{()}(,)=(\|\|^{2}+^ {}+\|\|^{2})\) for all \(,^{d}\) where \(\|\|\) is an \(L_{2}\)-norm. Two important special cases of the scaled softmax kernel are 1) the _Gaussian kernel_\(K^{(-1/2)}(,)=(-\|-\|^{2}/2)\) and 2) the _softmax kernel_\(K^{(0)}(,)=(^{})\). For two sets of vectors \(=\{^{(i)}^{d}\}_{i=1}^{L}\) and \(=\{^{(j)}^{d}\}_{j=1}^{L}\), by \((,)^{L L}\) we denote a matrix where \(^{()}(,)_{i,j}=K^{()}( ^{(i)},^{(j)})\) for all \(1 i,j L\).

In this paper, we will be interested in the problem of computing \(^{()}(,)\) where \(\), \(\) and a matrix \(^{L n}\) are provided as an input. A naive solution requires \(O(L^{2}(d+n))\) computations for constructing \(^{()}(,)\) (\(O(L^{2}d)\)) and computing the matrix multiplication \(^{()}(,)\) (\(O(L^{2}n)\)). Instead, we will use an efficient Monte-Carlo approximation of \(^{()}(,)\) using the following notion of _random features (RFs) for the scaled softmax kernel_:

Figure 1: **(left) Venn diagram of the new types of random features (green) we propose. (right) Logarithm of the relative variance of different random feature maps on pairs of vectors sampled from CIFAR10 and MNIST/CIFAR10. A new random feature map SDERF results in a consistent variance reduction of the previous best method GERF, up to \( e^{10}\) and \( e^{5}\) times. Figure 2 extends this plot.**

**Definition 2.1**.: By random features for the scaled softmax kernel \(K^{()}\), \(\), we denote a triple \(=,f^{(1)},f^{(2)}\) where \(\) is a probability distribution over random objects \(\) and \(f^{(i)}:^{d}\), \(i\{1,2\}\), are such mappings that, for all \(,^{d}\),

\[K^{()}(,)=_{}[f^{(1)}(,)f^{(2)}(,)].\] (1)

The decomposition of type (1) can be used for an efficient unbiased approximation of \(^{()}(,)\). Let \(^{(1)},,^{(M)}\) be i.i.d. samples from \(\). Define matrices \(,^{L M}\) where for all \(1 i,j L\),

\[_{i,:}=M^{-1/2}(f^{(1)}(^{(m)},^{(i)} ))_{m=1}^{M},_{j,:}=M^{-1/2}(f^{(2)}(^{(m)},^{(j)}))_{m=1}^{M},\] (2)

where \(_{i,:},_{j,:}^{d}\) are _column vectors_ corresponding to the rows of \(,\). Then according to (1), \(}=^{}\) is an unbiased _Monte Carlo (MC)_ approximation of \(^{()}(,)\) on \(M\) samples. The variance \(\,}_{i,j}=M^{-1}_{}f^{(1)}( ,^{(i)})f^{(2)}(, ^{(j)})\) of this approximation is inversely proportional to \(M\), hence \(M\) is a tradeoff parameter between computations and precision. Now, \(}\) is an unbiased approximation of \(^{()}(,)\) but \(}=^{}\) is a rank-\(M\) matrix, hence computing \(}\) has \(O(LMn)\) complexity. Assuming that sampling each \(^{(m)}\) and computing \(f^{()}(,)\) are \(O(d)\) operations, which is usually the case, precomputing \(\) and \(\) takes \(O(LMd)\) computations, resulting in a total \(O(LM(d+n))\) computational complexity. By choosing \(M L\), we obtain a significant reduction in computations compared to the exact variant: \(O(LM(d+n)) O(L^{2}(d+n))\).

Operations of type \(^{()}(,)\), especially for the Gaussian kernel \(=-1/2\), emerge in kernel SVM , kernel regression [33; 47] and in physics in the form of the Gauss transform . Another important application is in the area of efficient Transformers and is discussed in the next section .

### Random features for efficient Transformers

RFs found a prominent application in the area of efficient long-sequence Transformers . Transformers rely on a self-attention block for propagating information between elements of the sequence. If the sequence length is \(L\) and input matrices are denoted as \(,,^{L d}\) (_queries_, _keys_ and _values_), then self-attention outputs the following matrix:

\[=(^{(0)}(,) _{L})^{-1}^{(0)}(,)^{ L d},\] (3)

where \(_{L}^{L}\) is a vector of all ones, \(()\) returns a diagonal \((L L)\)-sized matrix with the argument on the diagonal, \(=\{^{(i)}=d^{-1/4}_{i,:}^{d}\}\), and \(=\{^{(j)}=d^{-1/4}_{j,:}^{d}\}\). Hence, substitution of \(}\) instead of \(^{(0)}(,)\) in (3) reduces computational complexity from \(O(L^{2}d)\) to \(O(LMd)\) (\(n=d+1\)). \((^{(0)}(,)_{L})^{-1} ^{(0)}(,)\) is the result of a _softmax_ operation performed on rows of \(d^{-1/2}^{}\).

### Existing random features for the softmax kernel

Representation (1) is not unique and different RFs can be proposed for a single \(K^{()}\). Note that if \(,f^{(1)},f^{(2)}\) are RFs for \(K^{(0)}\), then \(,^{(1)},^{(2)}\) are RFs for \(K^{()}\) for \(\) where \(^{(k)}(,)=(\|\|^{2 })f^{(k)}(,)\). Hereafter we focus on the softmax kernel \(K^{(0)}\) without loss of generality.

 proposed to use _trigonometric random features (TrigRFs)_ from  for efficient Transformers:

\[_{}=^{d+1},_{}= ([0,2])(_{d},_{d})^{d},  f^{(1)}_{}((,}), )=\] (4) \[=(\|\|^{2}/2)(}^{}+), f^{(2)}_{}((, }),)=(\|\|^{2}/2) (-}^{}+),\] (5)

where \(=(,})\), \(()\) denotes a uniform distribution on the argument set, \((_{d},_{d})\) is a multivariate Gaussian distribution with mean \(_{d}\) (vector of \(d\) zeros) and covariance matrix \(_{d}\) (identity matrix of size \(d d\)).

The next iteration of efficient attention approximators  observed a problem with TrigRFs (4-5). The _attention matrix_\((^{(0)}(,)_{L})^{-1} ^{(0)}(,)\) from (3) is _right stochastic_ meaning that itsentries are nonnegative and each row sums to \(1\) due to the normalizing term \((^{(0)}(,)_{L})^{-1}\). However, since \(f^{()}_{}\) can be arbitrary real numbers, \(,\) (2) and, therefore, \(}\) can take negative values. Hence, \((}_{L})^{-1}}\) is not right stochastic in general and entries of \(}_{L}\) can take very small and/or negative values resulting in unstable behaviour when inverting \((}_{L})^{-1}\).  therefore proposed a new type of _positive random features (PosRFs)_ which have the form:

\[_{}=^{d},_{}= (0,1)^{d}, f^{(1)}_{}(, )=f^{(2)}_{}(,)= (^{}-\|\|^{2}/2).\]

It is clear that such \(f^{()}_{}\) only take strictly positive values resulting in the right stochastic \((}_{L})^{-1}}\) and a stable Transformer training procedure.

 extended PosRFs, proposing _generalized exponential random features (GERFs)1_ for \(K^{(0)}\):

\[_{}=^{d},\;_{}= (0,1)^{d},\;f^{(1)}_{}(, )\!=\!f^{(2)}_{}(,\! )\!=\!D(A\|\|^{2}\!+\!B^{}\!+\!C\|\|^{2}/2)\] (6)

where \(A,B,C,D\) are real numbers2 satisfying: \(1-8A>0\), \(B=\), \(C=-0.5\), \(D=(1-4A)^{d/4}\).  express \(B,C,D\) through \(A\) and find a closed-form equation for the variance of (1):

\[_{_{}}f^{(1)}_{}( ,)f^{(2)}_{}(,)\!=\!e^{_{}(A,, )}\!-\!K^{(0)}(,)^{2},\]

\[_{}(A,,)=d(})+\|+\|^{2}-\| \|^{2}-\|\|^{2}.\] (7)

The minimum variance corresponds to the minimum \((A,,)\) since \(K^{(0)}(,)^{2}\) does not depend on \(A\). Since \((A,,)\) is defined for a single pair of \(,\) and not for sets \(,\),  propose a _homogeneity heuristic_ when they replace \(\|+\|^{2}\), \(\|\|^{2}\), \(\|\|^{2}\) in (7) with averages over \(,\): \(L^{-2}_{i,j}\|^{(i)}+^{(j)}\|^{2}\), \(L^{-1}_{i}\|^{(i)}\|^{2}\) and \(L^{-1}_{j}\|^{(j)}\|^{2}\) respectively. This heuristic is based on the assumption that \(\{^{(i)}\}\) and \(\{^{(j)}\}\) are homogeneous and their statistics are tightly concentrated around the mean. After this, the minimum of (7) with respect to \(A\) can be found in closed form.

## 3 Dense-exponential random features (DERFs)

We prove that the homogeneity heuristic (Section 2.3) corresponds to a certain minimization problem. Then, we present DERFs which generalize GERFs and provide a tighter solution of that problem.

### The objective minimized by GERFs

Our first contribution is showing that the homogeneity heuristic adopted in GERFs is actually an analytic solution of a certain optimization problem. Define

\[}(;,,) =L^{-2}_{1 i,j L}(_{}[f^{(1)}( ,^{(i)})f^{(2)}(,^ {(j)})]+K^{(0)}(^{(i)},^{(j)})^{2}),\] (8)

where \(=,f^{(1)},f^{(2)}\) are RFs for the kernel \(K^{(0)}\) and \(\) are their parameters appearing implicitly in \(,f^{(1)},f^{(2)}\). (8) is a mean log-variance shifted by \(K^{(0)}(^{(i)},^{(j)})^{2}\). The best possible value of (8) is \( K^{(0)}(^{(i)},^{(j)})\) which corresponds to all variances \(f^{(1)}(,^{(i)})f^{(2)}( ,^{(j)})\) being zero, meaning that RFs provide exact kernel estimation. Hence, minimization of (8) leads to more precise estimators on \(,\). We call the loss function \(}(;,,)\) the _shifted log-variance_ objective. Since \(\) is concave loss \(}\) is conceptually similar to relative standard deviation: the smaller \(K^{(0)}(^{(i)},^{(j)})\), the higher \(}\) is for the same amount of variance \(f^{(1)}(,^{(i)})f^{(2)}( ,^{(j)})\).

If \(_{}=_{},f^{(1)}_{ },f^{(2)}_{}\) are taken in (8), then \(_{}=\{A,B,C,D\}\) and \(}(_{};, ,_{})=L^{-2}_{i,j}_{ }(A;^{(i)},^{(j)})\). Using (7), we get: \(}(_{};, ,_{})=\)

\[d(})+} _{i,j}\|^{(i)}+^{(j)}\|^{2}\!-\!_{i} \|^{(i)}\|^{2}\!-\!_{j}\|^{(j)}\|^{2}.\]That is, \((A;,)\) coincides with (7) when \(\|+\|^{2}\), \(\|\|^{2}\), \(\|\|^{2}\) are replaced by their average statistics computed on \(,\). Hence, the homogeneity heuristic (Section 2.3) is nothing but minimization of (8). While in general it's unclear how to find a closed-form optimum of \(}\) or \((})\), the global minimum of (8) is feasible and can be computed in \(O(1)\) time. Further, in the case of GERF, the values minimizing (8) lead to very good results in large-scale applications of efficient Transformers as shown by  without knowing about (8) objective. In the next section we present extensions of GERFs which aim to minimize (8) in closed form.

## 4 Towards DERFs

Dense-exponential random features (DERFs) are an extension of GERFs where scalars \(A,B,C\) are replaced with dense matrices. DERFs may be viewed as a generalization that contain the previously introduced classes as special cases. We define DERFs as follows: \(_{}=^{d}\), \(_{}=(0,1)^{d}\) and for \(k\{1,2\}\):

\[f_{}^{(k)}(,)\!=\!D(^{}+^{} ^{(k)}+^{}^{(k)}),\]

where \(^{(k)},^{(k)}^{d d}\), \(D\), \(_{d}\) (a set of \(d d\) real symmetric matrices). Clearly, GERFs with parameters \(A,B,C,D\) can be expressed via DERFs with parameters \(=A_{d}\), \(^{(1)}=^{(2)}=B_{d}\), \(^{(1)}=^{(2)}=C_{d}\), \(D\) is unchanged. Our first theoretical result is giving the conditions when \(_{}=_{},f_{}^{(1)},f_{ }^{(2)}\) are valid RFs:

**Theorem 4.1**.: _Let the following conditions hold: \(8_{d}\), \((^{(1)})^{}(_{d}-4)^{-1}^{(2)}= _{d}\), \(^{(k)}=-(^{(k)})^{}(_{d}-4 )^{-1}^{(k)}\), \(D=(_{d}-4)^{1/4}\) where \(k\{1,2\}\). Then \(_{}\) are RFs for \(K^{(0)}\) and for all \(,^{d}\): \(_{_{}}f_{}^{(1)}(, )f_{}^{(2)}(,)=\)_

\[D^{4}(_{d}-8)^{-1/2}\!(2^{}(^{(1)}+(^{(1)})^{}(_{d}-8 )^{-1}^{(1)}).\] \[.+2^{}(^{(2)}\!+\!(^ {(2)})^{}(_{d}\!-\!8)^{-1}^{(2)}) \!+\!4^{}(^{(1)})^{}(_{d}\!- \!8)^{-1}^{(2)}\!)\!-\!K^{(0)}( \!,\!)^{2}.\] (9)

Our ultimate goal is to find optimal parameters \(,^{(k)},^{(k)}\) and \(D\) minimizing the variance of the low-rank approximation of \(^{(0)}(,)\) where sets \(,\) are provided. Our first observation is that we can assume that \(_{d}\) (a set of \(d d\) real diagonal matrices). Indeed, any symmetric \(\) can be expressed as \(}^{}\) where \(_{d}\) (a set of orthogonal matrices \(\{^{d d}\,|\,^{}=_{d}\}\)) and \(}_{d}\). Let \((_{d},_{d})\). Then, for any \(^{d}\), \(k\{1,2\}\),

\[f_{}^{(k)}(,\!)\!=\!D( ^{}}} ^{}\!+\!^{}^{ (k)}\!+\!^{}^{(k)})=\] \[=D(}^{} }}+}^{} }^{(k)}+^{}^{(k)} )=_{}^{(k)}(}, ),\]

where \(}^{(k)}=^{}^{(k)}\), \(}=^{}(_{d},_{d})\) since the distribution \((_{d},_{d})\) is _isometric_, i.e. rotation-invariant and \(_{}^{(k)}\) are DERFs with parameters \(},}^{(k)}\), \(^{(k)}\), \(D\). We conclude that with any \(\), \(f_{}^{(k)}(,)\) can be expressed as DERFs \(_{}^{(k)}\) with \(}_{d}\). Hence, hereafter we only consider \(_{d}\) without loss of generality.

Since \(^{(k)},^{(k)}\) are dense matrices in general, evaluation of \(f_{}^{(k)}(,)\) takes \(O(d^{2})\) time which is bigger than the \(O(d)\) complexity for TrigRFs, PosRFs and GERFs. However, \(\) and \(\) matrices (2) can be still computed in a time subquadratic in \(L\). For that, precompute \((^{(k)})^{}^{(m)}\), \(^{(1)}^{(i)}\), \(^{(2)}^{(j)}\) for all \(k\{1,2\}\), \(1 m M\), \(1 i,j L\) in \(O((M+L)d^{2})\) time. Then, computing \(f_{}^{(1)}(^{(m)},^{(i)})\), \(f_{}^{(2)}(^{(m)},^{(j)})\) for all \(1 i,j L\), \(1 m M\) takes \(O(LMd)\) operations. The complexity of constructing (2) then is \(O(L(Md+d^{2})+Md^{2})\) which is still subquadratic in \(L\).

Our goal is to minimize \(}(_{};,, _{})\) for \(_{}=\{,^{(1)},^{(2)}, ^{(1)},^{(2)},D\}\). However, we find that even for a single pair of \(,\) it's unclear how to minimize the variance (9) in closed form. Hence, below we consider special cases where an analytic solution is feasible.

### Asymmetric dense-exponential random features

Define RFs \(_{}=_{},f^{(1)}_{},f^{(2) }_{}\) in the same way as \(_{}\) with the only difference that \(=A_{d}\) where \(A\). We refer to these RFs as _asymmetric dense-exponential RFs (ADERFs)_ since \(f^{(1)}_{} f^{(2)}_{}\) in general. The only additional restriction of ADERFs compared to DERFs is that all diagonal entries of \(_{d}\) are the same. The parameters of \(_{}\) are \(_{}=\{A,^{(1)},^{(2)}, ^{(1)},^{(2)},D\}\). By \(_{}\) denote a set of all possible \(_{}\)'s resulting in correct RFs for the kernel \(K^{(0)}\), i.e. satisfying Theorem 4.1 with \(=A_{d}\). The following result gives an analytic formula for a minimum of \(}(_{};, ,_{})\). In the theorem, we use notions of SVD and eigendecomposition of a symmetric matrix  (all proofs are in the Appendix).

**Theorem 4.2**.: _Let \(=\{^{(i)}^{d}\}_{i=1}^{L}\), \(=\{^{(j)}^{d}\}_{j=1}^{L}\). Let \(^{(1)}=_{i=1}^{L}^{(i)}(^{(i)})^ {}\), \(^{(2)}=_{j=1}^{L}^{(j)}(^{(j)})^ {}\). Suppose that \(^{(1)},^{(2)}_{d}\) are nonsingular (implying that \(L d\)). Define \(^{(3)}=d^{-1}L^{-2}(_{i=1}^{L}^{(i)})^{}( _{j=1}^{L}^{(j)})\). For \(k\{1,2\}\), let \(^{(k)}=^{(k)}^{(k)}(^{(k)})^{}\) be eigendecomposition of a symmetric \(^{(k)}\) where \(^{(k)}_{d}\). \(^{(k)}_{d}\) has strictly positive diagonal values since \(^{(k)} 0\) by definition and \(^{(k)}\) is nonsingular. Let \(^{}\) be SVD decomposition of \((^{(1)})^{}(^{(1)})^{}^{(2 )}(^{(2)})^{}\) where \(,^{d}\), \(_{d}\) has nonnegative diagonal entries._

_One of the solutions \(_{}^{*}=\{A,^{(1)},^{(2)}, ^{(1)},\ ^{(2)},D\}\) of \(_{_{}_{}}}(_{};,, _{})\) is as follows. Set \(=2d^{-1}_{l=1}^{d}_{l,l}+2^{(3)}\) and, for \(k\{1,2\}\),_

\[A=(1-2-+8}),\ \ \ ^{(1)}=^{1/2}^{}(^{(1)})^{-1/2}(^{(1)})^{},\]

\[^{(2)}=^{-1/2}^{}(^{(1)})^{1/2}(^{(1)})^{},\ \ \ ^{(k)}=-(^{(k)})^{}^{(k)},\ \ \ D=(1-4A)^{d/4}.\]

_Further, we have: \(}(_{}^{*};, ,_{})=\)_

\[d(1\!-\!4A)-(1\!-\!8A)+2(1-8A)^{-1}(d^{-1}_ {l=1}^{d}_{l,l}+^{(3)})+2^{(3)}.\] (10)

Theorem 4.2 implies an algorithm for finding \(_{}^{*}\) efficiently. Namely, compute \(^{(k)}\), \(k\{1,2\}\) (\(O(Ld^{2})\) time) and \(^{(3)}\) (\(O(Ld)\) time). Then, perform matrix decompositions to obtain \(^{(k)},^{(k)}\), \(k\{1,2\}\), and \(,\), \(\) in \(O(d^{3})\) time. After that, \(A,^{(1)},^{(2)},^{(1)},^{(2)},D\) can be all evaluated in \(O(d^{3})\) time using formulae from Theorem 4.2. The total time complexity of the approximation scheme is therefore \(O(L(Md+d^{2})+Md^{2}+d^{3})\) which is subquadratic in \(L\) as required.

Figure 2: Log of the relative variance of new and existing RF mechanisms, mean value over multiple samples. \(0.1 1\).

### Symmetric dense-exponential random features

Define \(_{}=_{},f^{(1)}_{},f^{(2) }_{}\) in the same way as \(_{}\) with the only difference that \(^{(1)}=^{(2)}=\). From the conditions in Theorem 4.1 it follows immediately that also \(^{(1)}=^{(2)}=\). Hence, \(f^{(1)}_{}=f^{(2)}_{}\) and we refer to these RFs as _symmetric dense-exponential RFs (SDERFs)_. The parameters of \(_{}\) are \(_{}=\{,,,D\}\). By \(_{}\) denote a set of all possible \(_{}\)'s resulting in correct RFs for the kernel \(K^{(0)}\), i.e. satisfying conditions from Theorem 4.1 with \(^{(k)}=\), \(^{(k)}=\), \(k\{1,2\}\). The following theorem gives an analytic solution for a global minimum of \(}(_{};, ,_{})\).

**Theorem 4.3**.: _Let \(=\{^{(i)}^{d}\}_{i=1}^{L}\), \(=\{^{(j)}^{d}\}_{j=1}^{L}\) and let \(^{(1)}\), \(^{(2)}\) be defined as in Theorem 4.2 (but without the restriction of nonsingularity) and define \(^{(4)}=_{i=1}^{L}^{(i)}^ {d}\), \(^{(5)}=_{j=1}^{L}^{(j)}^ {d}\). Further, let \(^{(3)}^{(3)}(^{(3)})^{}\) be eigendecomposition of a symmetric positive semidefinite matrix \(^{(1)}+^{(4)}(^{(5)})^{}+ ^{(5)}(^{(4)})^{}+^{(2)}\) where \(^{(3)}_{d}\) and \(^{(3)}_{d}\) with nonnegative diagonal entries. Let the entries on the diagonal of \(^{(3)}\) be sorted in the non-ascending order._

_One of the solutions \(^{*}_{}=\{,,,D\}\) of \(_{_{}_{}}}(_{};,, _{})\) is as follows._

\(_{d}\)_, for all \(1 l d\): \(_{l,l}=(1-2^{(3)}_{l,l}-^{(3)}_{l,l}+1)^{2}+8^{( 3)}_{l,l}})\), \(=(_{d}-4)^{1/2}(^{(3)})^{}\), \(=-_{d}\), \(D=(_{d}-4)^{1/4}\). Further, we have: \(}(_{};, ,_{})=\)_

\[_{l=1}^{d}\!\!(\!(1\!-\!4_{l,l})\!-\!(1 \!-\!8_{l,l})\!+\!(1\!+\!(1\!-\!8_{l,l})^{-1}) ^{(3)}_{l,l}\!)\!-\!\!_{i=1}^{L}\!\!\| ^{(i)}\|^{2}-\!_{j=1}^{L}\!\!\|^{(j)}\|^{2}\] (11)

Again, Theorem 4.3 implies an algorithm for finding \(^{*}_{}\) in a time subquadratic in \(L\). That is, we can compute \(^{(1)},^{(2)}\), \(^{(3)}\), \(^{(4)}\), \(^{(5)}\) in \(O(Ld^{2})\) total time. Then, perform an eigendecomposition to obtain \(^{(3)},^{(3)}\) in \(O(d^{3})\) time. After that, \(,,,D\) can be computed in \(O(d^{3})\) time using formulae from Theorem 4.3. The total time complexity of the approximation scheme is the same as for ADERFs: \(O(L(Md+d^{2})+Md^{2}+d^{3})\) or \(O(L(Md+d^{2})+Md^{2})\) if we assume that \(L d\).

### Simplified ADERFs

While having a compact and closed-form expression, both ADERFs and SDERFs rely on eigendecomposition and SVD decompositions: operations for which implementation has not yet matured in popular deep learning libraries with GPU and TPU support. For this reason, we propose _simplified ADERFs (SADERFs)_\(_{}=_{},f^{(1)}_{},f^{(2) }_{}\) which extend GERFs but require only basic unary operations. SADERFs are defined via GERs as follows: \(_{}=^{d}\), \(_{}=(0,1)^{d}\), \(f^{(1)}_{}(,)=f^{(1)}_{}( ,)\), \(f^{(2)}_{}(,)=f^{(2)}_{}( ,^{-1})\) where \(_{d}\) is a diagonal matrix with nonzero diagonal entries. First of all, \(_{}\) are valid random features for the softmax kernel \(K^{(0)}\) since \(_{_{}}[f^{(1)}_{}(, )f^{(2)}_{}(,)]=_ {_{}}[f^{(1)}_{}(, )f^{(2)}_{}(,^{-1} )]=K^{(0)}(,^{-1})=K^{( 0)}(,)\), where we use \(K^{(0)}(,)=(^{})\) by the definition.

We find \(\) by optimizing the objective (8) for \(_{}\), the form of which is easily deduced from \(}(_{};,, _{})\):

\[}(_{};, ,_{})-d(} )\!=\!}_{i,j}\| ^{(i)}+^{-1}^{(j)}\|^{2}\] \[-_{i}\!(\!\|^{(i)}\|^{2}\!+\! \|^{-1}^{(j)}\|^{2})\!=\!(1\!-\!8A)}_{i,j}\! \|^{(i)}\!+\!^{-1}^{(j)}\|^{2}\!+ \!}_{i,(^{(i)})}\!\!^{(j)},\] (12)

where we move a term not depending on \(\) to the left-hand side. Since \(1-8A>0\), we conclude that minimizing (12) is equivalent to minimizing \(_{i,j}\|^{(i)}+^{-1}^{(j)}\|^ {2}\!=\)

\[_{l} (13) reduces to independent optimization problems with respect to \(_{l,l}\), \(1 l d\). Each problem is convex and the solution is found trivially by setting the derivative to zero: for all \(1 l d:_{l,l}^{*}=(_{j}(_{l}^{(j)})^{2}/_{i}( _{l}^{(i)})^{2})^{1/4}\). The solution can be computed in \(O(dL)\) time, after which the parameters of \(f_{}^{(1)},f_{}^{(2)}\) can be found efficiently as described in Section 2.3.

It is easy to see that \(_{}\) are a special case of ADERFs (Section 4.1) which explains their name. Furthermore, the case \(=_{d}\) reduces \(_{}\) to \(_{}\), hence the latter is a special case of the former. Figure 1 (left) illustrates all the new types of random features in a Venn diagram.

## 5 Experiments

In this section, we evaluate DERFs experimentally in various machine learning applications. More details about each experiment can be found in Appendix B.

### Variance comparison

We follow the variance comparison setup from : we sample pairs of vectors \(,\) and compute relative variances of the approximation \(^{(0)}(,)/K^{(0)}(, )\) where \(^{(0)}\) denotes the RF approximation and \(^{(0)}(,)\) is evaluated via (9). We set \(d=64\) as in  and take 6 different regimes for sampling \(,\): normal where \(,\) are drawn from \((_{d},^{2}_{d})\), sphere where \(,\) are drawn uniformly on a sphere \(^{d-1}\), heterogen where \(,\) are drawn from different distributions \((_{d},^{2}_{d})\) and \((_{d},^{2}_{d})\). mnist and cifar10 are where \(,\) are random images from MNIST  or CIFAR10 , resized to \(8 8\), scaled by \(>0\) and flattened. Finally, mnist/cifar10 is a regime where \(\) is drawn as in mnist and \(\) is drawn as in cifar10.

We do not report SADERFs since they're a special case of ADERFs (Figure 2). SDERFs outperform or are on par with other methods in all setups - about \(e^{5}\) times better than GERFs in heterogen, mnist and mnist/cifar10 and about \(e^{10}\) times better in cifar10. Further, ADERFs outperform GERFs by around \(e^{3}\) times in mnist/cifar10 where \(\) and \(\) are drawn "asymmetrically".

Figure 3: Kernel classification, test accuracy (%). The last plot shows average curves over 8 benchmarks. We observe that our proposed method SDERF, shows the best accuracy across most of the (benchmark, \(M\)) pairs and also shows the best average performance.

### Kernel classification

In this experiment, we compare accuracy of different RF methods in kernel classification on 8 benchmarks from UCI , following the setup of . Kernel regression [33; 47] is applied for predicting class probabilities. Training objects are denoted as \(^{(1)},,^{(L)}^{d}\) and their one-hot labels as \(^{(1)},,^{(L)}^{n}\). During testing, the goal is to predict the class of a new object \(^{*}\) as \(*{argmax}_{1 l n}^{*}\) where \(^{*}=_{i=1}^{L}K^{(-0.5)}(^{*}, ^{(i)})^{(i)}\) and \(>0\) is tuned on the validation set. With \(O(nLM)\) preprocessing, RFs are used to find an unbiased approximation of \(^{*}\) in \(O(nM)\) instead of \(O(nL)\) exact computation. For each benchmark, we range the values of \(M\) from \(2^{4}\) to \(2^{7}\) (Figure 3). We observe that SDERF, which is proposed in this paper, shows the best accuracy across most of the (benchmark, \(M\)) pairs and also shows the best average performance.

### DERFs for long-sequence Transformers

In this section, we evaluate DERFs for self-attention approximation in Performer-Transformer training setups . We refer to the DERF-based self-attention approximation method as _FAVOR#_.

#### 5.3.1 Speech modelling

In our first set of experiments, we focus on speech models. We train Performer-encoders and test them on the _LibriSpeech_ corpus , commonly used for benchmarking speech models. We considered two Transformers architectures/training setups: **(a)** Conformer-Transducer  trained in a regular way (\(\)) as well as: **(b)** the Noisy Student Training (\(\)) variant introduced in . We compare "performized" variants of these architectures, applying FAVOR# with SDERF (since it worked best in the previous setups) as well as FAVOR++ .

In the first setting, we see that FAVOR# consistently outperforms FAVOR++ for smaller \(m\) (where reduced variance of the softmax-kernel estimation is more critical) and both achieve similar scores for larger \(m\). In the NST-experiment, we focused on the smaller \(m\) variant, where FAVOR# again beats FAVOR++. All results are presented in Fig. 4.

#### 5.3.2 Natural language processing

The General Language Understanding Evaluation (GLUE) benchmark  consists of 8 different natural language understanding tasks with the sequence length ranging from 32 to 128. We use this to test the performance of different low rank attention methods on NLP tasks. We used the same training parameters as mentioned in  (see Appendix B.4.2 for details). We warm start all low-rank

Figure 4: Comparison of FAVOR# using SDRF with FAVOR++ Performer for regular Conformer-Transducer training with \(m\) random features (TRANS-\(m\)) as well as the Noisy Student Training variant with \(m\) random features (NST-\(m\)) on the _LibriSpeech_ corpus. We report commonly used normalized word error rate (NWER) metric.

Transformers with a pre-trained BERT-base model checkpoint , thus contrasting how well the low rank methods approximate the softmax kernel.

We compared FAVOR++ , FAVOR+ , ELU , ReLU  variants of the Performers  and sparse attention [37; 4] against the FAVOR# variant and report the results in Table 1. We couldn't use SDERF in this setup because eigendecomposition led to errors on TPUs due to a different implementation compared to the speech modelling experiment. For this reason, we used SADERF which doesn't require any matrix decompositions. On most tasks we find that FAVOR# is the best or second best performing variant showcasing its effectiveness in modelling the softmax kernel for Transformers.

## 6 Conclusion

We proposed an extension of generalized exponential random features (GERFs) for the Gaussian and softmax kernels: dense-exponential random features (DERFs). DERFs employ matrix parameters and are more flexible than GERFs. We evaluated DERFs in several applications such as kernel regression and two Transformers training setups, demonstrating downstream performance benefits.

**Limitations & broader impact.** Optimizing matrix parameters in the most general formulation of DERFs (Theorem 4.1) could lead to further variance reductions. It remains an open question how to find a closed form solution of the optimum which we leave to future work. Our work is primarily theoretical but has broad applications. A prominent application is efficient Transformers which should be used responsibly due to their potential for misuse and significant societal impact, and carbon footprint [48; 5; 10].

## 7 Acknowledgements

V. L. acknowledges support from the Cambridge Trust and DeepMind. V. L. was part-time employed by Google while a PhD student. A.W. acknowledges support from a Turing AI Fellowship under EPSRC grant EP/V025279/1, The Alan Turing Institute, and the Leverhulme Trust via CFI.