# StepbaQ: Stepping backward as Correction for

Quantized Diffusion Models

 Yi-Chung Chen

MediaTek, Purdue University

chen5262@purdue.edu

&Zhi-Kai Huang

MediaTek

brent5481@gmail.com

&Jing-Ren Chen

MediaTek

jingren.chen@mediatek.com

This work was conducted during Yi-Chung's time at MediaTek.

###### Abstract

Quantization of diffusion models has attracted considerable attention due to its potential to enable various applications on resource-constrained mobile devices. However, given the cumulative nature of quantization errors in quantized diffusion models, overall performance may still decline even with efforts to minimize quantization error at each sampling step. Recent studies have proposed several methods to address accumulated quantization error, yet these solutions often suffer from limited applicability due to their underlying assumptions or only partially resolve the issue due to an incomplete understanding. In this work, we introduce a novel perspective by conceptualizing quantization error as a "stepback" in the denoising process. We investigate how the accumulation of quantization error can distort the sampling trajectory, resulting in a notable decrease in model performance. To address this challenge, we introduce StepbaQ, a method that calibrates the sampling trajectory and counteracts the adverse effects of accumulated quantization error through a sampling step correction mechanism. Notably, StepbaQ relies solely on statistics of quantization error derived from a small calibration dataset, highlighting its strong applicability. Our experimental results demonstrate that StepbaQ can serve as a plug-and-play technique to enhance the performance of diffusion models quantized by off-the-shelf tools without modifying the quantization settings. For example, StepbaQ significantly improves the performance of the quantized SD v1.5 model by 7.30 in terms of FID on SDprompts dataset under the common W8A8 setting, and it enhances the performance of the quantized SDXL-Turbo model by 17.31 in terms of FID on SDprompts dataset under the challenging W4A8 setting.

## 1 Introduction

Diffusion models  have demonstrated its power of generating high quality samples in a wide variety of applications including super resolution , image enhancement , image inpainting , image editing  and image-to-image translation . In contrast to GAN  and VAE , which are prone to issues such as mode and posterior collapse, diffusion models consistently produce diverse, high-quality samples, thus emerging as the predominant technique in the field of image generation. However, the deployment of diffusion models on computationally constrained devices, such as smartphones, is impeded by their extensive computational demands, which stem from the complex network structures and the multitude of denoising steps required during the sampling phase. To enhance the computational efficiency of diffusion models, a body of researchhas concentrated on reducing the number of sampling steps . Concurrently, another strand of investigation has pursued model compression strategies, such as quantization  and pruning , to diminish the computational resources necessitated. In this paper, we focus on improving the performance of model quantization.

Model quantization  stands as one of the most popular model compression techniques. This technique transits model parameters and activations from a high bit-width floating-point format to a more compact low bit-width representation, thereby facilitating a substantial acceleration of model inference with a tolerable drop in performance. However, quantizing diffusion models leads to greater performance degradation compared to standard deep learning models due to two challenges: **i) Activation range varies across timesteps.** Diffusion models generate samples from a normal distribution through a multi-step denoising process, wherein noise levels are gradually reduced following a predefined schedule. This mechanism results in a significant disparity in the activation range across different timesteps, as noted in studies by Shang et al.  and Li et al. . Such variability poses a challenge in establishing an appropriate quantization range. Opting for a broader interval may mitigate clipping errors at the expense of increased rounding errors and vice versa. Prior research by Shang et al.  and Li et al.  have proposed sampling strategies to select representative calibration samples over multiple timesteps. To further reduce quantization errors, So et al. , He et al. , and Yang et al.  relax the quantization constraints, adopting techniques such as timestep-specific quantization parameters and mixed precision quantization. However, since these methods do not consider the accumulation of quantization errors, they only achieve mediocre performance. **ii) Quantization error accumulates over time.** The iterative nature of the denoising process in diffusion models means that quantization errors from each step can accumulate. Although errors at individual sampling steps may appear negligible, the cumulative effect can significantly impair the final sample quality. To mitigate this error propagation, Huang et al.  have underscored the significance of temporal features and introduced a temporal information-aware reconstruction method to prevent deviations from the intended sampling trajectory caused by temporal information mismatch. Conversely, He et al.  have presented PTQD, which employs variance schedule calibration to integrate the quantization error at each step into the Gaussian diffusion noise of stochastic samplers, thereby addressing the accumulation issue. However, the approach proposed by Huang et al.  only resolves the disturbance of temporal features without considering the distribution shift in the latent space. On the other hand, the strategy of error absorption introduced by He et al.  is limited to stochastic samplers, thereby excluding its application to deterministic samplers, such as DDIM .

In this work, our goal is to devise a general approach capable of addressing the issue of error accumulation and enhancing the performance of the quantized diffusion model. We investigate the mechanisms by which accumulated errors can detrimentally impact overall model performance. Our research reveals that quantization error can alter the distribution of the latent variables. This shift in the distribution can lead to a divergence from the original sampling trajectory, ultimately causing a decline in model performance. Intriguingly, we show that with the assumption of quantization error following Gaussian distribution, the distribution shift in the latent space caused by quantization error can be interpreted as a "stepback" in the denoising process.

Building upon this insight, we introduce StepbaQ, a novel method that uses a sampling step correction technique to minimize the deviation from the original sampling trajectory. StepbaQ analyzes quantization errors to measure the extent of the distribution shifts and subsequently determine the "corrected sampling steps." We demonstrate that with appropriate modifications based on the corrected sampling steps, StepbaQ can effectively correct the sampling trajectory and mitigate the accumulation of quantization error, thereby significantly enhancing the quality of the generated results. It is worth noting that StepbaQ only necessitates the statistics of quantization error derived from a small calibration dataset. Consequently, it can be seamlessly integrated with existing quantization frameworks as a plug-and-play solution, enhancing the performance of diffusion models quantized by off-the-shelf tools without modifying their quantization settings.

The contributions of this paper are summarized as follows:

* We introduce a novel perspective that interprets the distribution shift in the latent space, caused by quantization error, as a "stepback" in the denoising process. We demonstrate that such a temporal shift can alter the sampling trajectory and adversely affect the generated results.

* We propose StepbaQ, a general strategy designed to enhance the performance of quantized diffusion models. This method employs a sampling step correction technique to realign the sampling trajectory and eliminate the accumulation of quantization error.
* Extensive experiments show that StepbaQ can serve as a plug-and-play technique, significantly improving the performance of diffusion models quantized by off-the-shelf tools, and achieve the state-of-the-art performance for quantized diffusion models.

## 2 Related Works

### Efficient Diffusion Models

Although diffusion models have powerful generative capabilities, slow inference speeds severely limit their practical application. Various approaches have been developed to improve the efficiency of diffusion models. Rombach et al.  speeds up each denoising step by transferring the denoising process to the latent space; Song et al.  modifies the original diffusion process with a non-Markovian formulation, enabling fewer sampling steps and deterministic sampling. Lu et al.  further reduce the required number of steps by introducing an exact solution formulation for diffusion ODEs. Luo et al.  applies a consistency model to the latent space, enabling fast sampling with just a few steps or potentially a single step. Sauer et al.  leverages adversarial training on the diffusion model, enabling the network to generate images in one step, just like GAN . While the methods above can effectively speed up the inference process of diffusion models, deploying these models on devices with limited computational resources necessitates reduced computation and memory usage, which can be achieved via model quantization.

### Diffusion Model Quantization

Model quantization is a technique that effectively reduces memory and computation costs required for deep learning models. It can be divided into two categories: Quantization-Aware Training (QAT) [13; 17; 27; 26], which mitigates performance drop after quantization through model fine-tuning at the expense of substantial computational cost, and Post-Training Quantization (PTQ) [42; 34; 25; 55; 15; 43; 10], which requires only a small calibration dataset and therefore is ideal for large models like diffusion models. While prior works have achieved considerable progress, directly applying conventional quantization methods on diffusion models does not yield good results due to challenges such as varying activation range across steps and error accumulation. Shang et al.  and Li et al.  propose sampling strategies for selecting representative calibration samples across steps to alleviate the issue of varying activation ranges. Some methods relax the quantization constraints and use finer granularity to enhance quantization results. For example, Yang et al.  leverage SQNR as a metric to find sensitive blocks and quantize them with higher precision. Li et al. , So et al. , and He et al.  employ distinct sets of quantization parameters at various timesteps to address the drastic changes in activation ranges. While effective, these strategies incur additional computational overhead, which reduces their practicality. Moreover, even when efforts are made to minimize quantization error at each sampling step, the overall performance still suffers due to error accumulation. To address the issue of error accumulation, Huang et al.  propose temporal information aware reconstruction, which alleviates temporal feature disturbance, to prevent deviation from the original sampling trajectory. On the other hand, Tang et al.  introduces a progressive calibration strategy to minimize accumulation error. Aside from the approaches above, the method PTQD, proposed by He et al. , is closely related to our work. Both methods utilize the statistics of quantization error to enhance the performance of quantized diffusion models. They can be considered complementary techniques integrated with existing quantization approaches. PTQD decomposes the quantization error into correlated and uncorrelated parts. The correlated part is eliminated by simple division, while the uncorrelated part is absorbed into the scheduled Gaussian noise. Though this strategy is effective, its practicality is restricted. The design of the error absorption mechanism is only compatible with stochastic samplers, such as DDPM , which narrows its applicability. In contrast, our proposed StepbaQ, utilizing a sampling step correction technique, is applicable to both deterministic and stochastic samplers.

## 3 Preliminary

Diffusion models [23; 49] are latent variable models characterized by a forward process, wherein Gaussian noise of a predetermined magnitude is incrementally applied to the original data \(x_{0}\). The goal is to learn a reverse process that gradually removes noise from latent to generate high-quality images. The equations of the posterior and Gaussian transitions are defined as follows:

\[q(x_{1:M}|x_{0})_{t=1}^{M}q(x_{t}|x_{t-1}), q(x_{t}|x_{t-1 })(}{_{t-1}}}x_{t-1},(1- {_{t}}{_{t-1}}))\] (1)

Here, \([_{0},,_{M}]\) represents a descending sequence of hyperparameters that schedule the noise level, as described in . Through the reparameterization technique , we can express \(x_{t}\) as:

\[x_{t}=}x_{0}+}, (,)\] (2)

The aim of a diffusion model is to learn a parameterized distribution \(p_{}(x_{0})\) that approximates \(q(x_{0})= p_{}(x_{0:M})dx_{1:M}\) and is easy to sample from. The parameters \(\) are trained by optimizing a variational lower bound to accurately predict the added noise, as outlined in the following objective:

\[\|-_{}(}x_{0}+} ,t)\|^{2}\] (3)

While the forward process has \(M\) steps, the denoising process can employ a condensed sampling path by choosing a sub-sequence of length \(N\) from the set \([1,,M]\) to expedite the sampling process. DDIM  presents a generalized formula to generate a sample \(x_{t-1}\) from a sample \(x_{t}\) via:

\[x_{t-1}=}(-}_{}(x _{t},t)}{}})+-_{t}^{2}}_{ }(x_{t},t)+_{t}_{t},_{t}( ,)\] (4)

Varying the choices of parameter \(_{t}\) yields different denoising processes. Specifically, setting \(_{t}=}{1-_{t}}}} {_{t-1}}}\) results in DDPM , whereas a \(_{t}\) of \(0\) corresponds to DDIM .

In previous literature, the notation \(t[1,...,N]\) has been conventionally used to index the sampling steps in the denoising process. To distinguish between the original sampling steps and the one that StepbaQ has modified, we employ the notation \([t_{1},...,t_{N}]\) to represent the original sampling steps, while \([_{1},...,_{N}]\) is used to denote the steps post-correction.

## 4 Method

In this section, we first present a novel perspective that interprets the distribution shift in the latent space, resulting from quantization errors, as a "stepback" along the sampling trajectory. We then explain how such temporal information affects the sampling trajectory. Finally, we introduce StepbaQ,

Figure 1: Overview of the denoising process of StepbaQ and existing methods. Figure (a) shows the original denoising process. Figure (b) demonstrates the negative impact of quantization error without changing the step size, leading to significant accumulation error. Figure (c), on the other hand, illustrates how StepbaQ treats the quantization error as a stepback in the denoising process and adopts corrected steps with a larger step size to eliminate cumulative quantization error.

a method that calibrates the sampling trajectory and counteracts the negative effects of accumulated quantization error through a sampling step correction mechanism. We demonstrate that StepbaQ can effectively mitigate the accumulation of quantization errors, thereby diminishing the total error. Note that while the derivations discussed herein are based on the denoising process of DDIM  as in Eq.4, the proposed StepbaQ is adaptable to other samplers with appropriate modifications.

### Quantization Error as Stepback

Following PTQD , we assume the quantization error follows Gaussian distribution. Then the conditional probability of the quantized latent variable \(_{t_{i}}\) can be formulated as follows:

\[p(_{t_{i}}|x_{t_{i}})=(x_{t_{i}},_{i}^{2}),_{t_{i}}=x_{t_{i}}+_{i},_{i}(_{i}, _{i}^{2})\] (5)

where \(_{i}\) denotes the quantization error with \(_{i}\) and \(_{i}^{2}\) representing its mean and variance. As the error mean can be readily rectified by subtracting the error mean as described in , here we assume \(_{i}=\). Recalling the Gaussian transition from step \(t_{i}\) to step \(_{i}\) with \(_{i} t_{i}\) as mentioned in Eq. 1:

\[q(x_{_{i}}|x_{t_{i}})=(}}{_{ t_{i}}}}x_{t_{i}},(1-}}{_{t_{i}}}))\] (6)

Scaling the latent variable \(x_{_{i}}\) by \(}}{_{_{i}}}}\), we obtain:

\[q(}}{_{_{i}}}}x_{_{i}}|x_{t_{i}})= (x_{t_{i}},(}}{_{_{i}}}-1))\] (7)

It is observed that the scaled latent variable \(}}{_{_{i}}}}x_{_{i}}\) shares the same mean as the quantized latent variable \(_{t_{i}}\) as shown in Eq. 5. By selecting a step \(_{i}\) such that \(}}{_{_{i}}}-1=_{i}^{2}\), or equally \(_{_{i}}=}}{1+_{i}^{2}}\), we align the variance of the scaled latent variable with the variance of \(_{t_{i}}\). This alignment allows us to state that the scaled latent variable is statistically equivalent to the quantized latent variable. Consequently, we can consider \(_{t_{i}}\) as being sampled from the distribution at \(_{i}\)-th step and scaled by a factor \(}}{_{_{i}}}}\). In practice, given that discrete samplers operate with a finite set of sequence \(\), \(_{i}\) can be approximated by solving the following equation (See Appendix B for further discussion):

\[_{i}=_{j}_{j}-}}{1+_{i}^{2}} , j[1,,M]\] (8)

Note that \(_{i} t_{i}\) due to the descending property of \(\). This novel perspective to treating the quantized latent as if sampled from the distribution at \(_{i}\)-th step implies that a temporal shift, or "stepback," occurs in the latent space. This insight enables us to further explore the impact of quantization errors.

### Error Arising from Temporal Shift

Based on the finding that a temporal shift occurs in the latent space, we will now demonstrate how such a shift can contribute to error accumulation, resulting in a decline in overall performance.

**Inaccurate Noise Prediction.** Throughout the sampling process, the noise prediction network relies on the temporal feature at the \(t_{i}\)-th step as a conditional input, which informs the network about the noise level of the latent variable. Given that the noise level of the quantized latent \(_{t_{i}}\) is increased by quantization error, utilizing the temporal feature at step \(t_{i}\) as a condition could misguide the noise prediction network regarding the actual noise level. This discrepancy has the potential to yield inaccurate noise predictions. An additional concern arises from the distribution shift in the latent space. The noise prediction network is trained with the scheduled latent distributions, as described in Eq. 2. However, quantization error changes the latent variable's mean and variance. Without appropriate adjustment, the distribution of the latent variable \(_{t_{i}}\) deviates from the expected value range. This deviation in the input distribution can also lead to inaccurate noise predictions, ultimately reducing overall performance.

**Trajectory Deviation.** For DDIM, the first term within Eq. 4 indicates the predicted \(x_{0}\). The expression \(x_{t_{i}}-}_{}(x_{t_{i}},t_{i})}\) can be interpreted as advancing from the latent variable \(x_{t_{i}}\) toward the predicted latent variable \(x_{0}\) along the direction \(-_{}(x_{t_{i}},t_{i})\), with a step size of \(}}\).

As the quantized latent variable \(_{t_{i}}\) possesses a higher noise level than \(x_{t_{i}}\), implying that the gap between \(x_{0}\) and \(_{t_{i}}\) is more pronounced, a larger step is required to obtain the expected result. However, previous studies ignore this discrepancy and continue utilizing \(_{t_{i}}\) during the denoising process. Fig. 1(b) illustrates that adhering to the original step size, without accounting for the quantization error, can lead to a significant accumulation of overall error.

### StepbaQ

To tackle the challenges mentioned above, we introduce StepbaQ. By employing the corrected step as obtained by Eq. 8, we demonstrate that StepbaQ can significantly reduce the overall accumulated quantization error through a sequence of simple yet effective modifications.

**Temporal Information Alignment.** The quantized latent variable \(_{t_{i}}\) is characterized by an increased noise level in comparison to its original counterpart \(x_{t_{i}}\). Therefore, the temporal information input must precisely reflect the increased noise levels to aid the noise prediction network make accurate predictions. Replacing the original input \(t_{i}\) with the corrected sampling step \(_{i}\) gives the network a more accurate noise level indication.

**Latent Adjustment.** To approximate the quantized latent \(_{t_{i}}\) as though it were sampled from the distribution at the \(_{i}\)-th step, adjustments are necessary to align their distributions. We subtract the channel-wise mean of quantization error from the quantized latent variable, as suggested in PTQD , and rescale it by a factor \(}}{_{t_{i}}}}\). These adjustments effectively bridge the distribution shift and improve the noise prediction results.

**Step Size Adaptation.** Nevertheless, improving the noise prediction results is not enough. As depicted in Fig.2, the quantization errors increase the variance of quantized latent variables and decrease the signal-to-noise ratio (SNR), defined as \(SNR=}{^{2}}\), which enlarges the distance between \(_{t_{i}}\) and \(x_{0}\). Therefore, the step size should be extended to consider the increased distance. This extension can be realized by substituting the original step size \(}}\) with \(}}\), which is the requisite step size for progressing towards \(x_{0}\) from the latent variable of \(_{i}\)-th step. The adjusted step size is longer than or equal to the original one, owing to the relationship \(_{_{i}}_{t_{i}}\). Fig. 1(c) illustrates that the cumulative error can be markedly diminished by adapting the step size.

**Accounting for Error Accumulation.** Since discrete samplers only involve finite sampling steps, the modifications mentioned above only take effect when quantization errors are substantial enough to result in a corrected step \(_{i}\) larger than \(t_{i}\). No correction will be made if the quantization error does not meet this threshold. Nonetheless, even minor quantization errors can accumulate over multiple steps, imperceptibly altering the latent distribution and degrading the quality of the generated output.

To address this issue, instead of measuring quantization errors at each step independently, StepbaQ measures quantization errors across multiple steps to obtain more accurate statistics for correction.

Algo. 1 shows the overall correction process of StepbaQ. Given floating-point and quantized noise prediction networks \(_{}\) and \(_{}\), StepbaQ calculates the corrected step based on statistics of quantization error. If the error is not substantial enough, no correction is made, allowing the error to accumulate until it becomes sufficiently large for correction. For simplicity, we ignore the iterations over the whole calibration set during the measurement of quantization error. The process yields a sequence of corrected steps \([_{1},,_{N}]\) and a corresponding sequence of error means \([_{1},,_{N-1}]\), which are subsequently applied during the inference phase. The inference phase is identical to Algo. 1 except that lines 5-8 are skipped. As StepbaQ precludes the accumulation of error, the final output is influenced only by the quantization error present in the last sampling step. This error is significantly less than the original accumulated error without corrections.

## 5 Experiment

We demonstrate the effectiveness of the proposed StepbaQ in enhancing the performance of quantized diffusion models for text-guided image generation. Experiments utilizing an off-the-shelf quantization tool show that StepbaQ can improve the performance of an existing quantized diffusion model without altering its quantization configuration. We also provide comparative results to emphasize StepbaQ's superiority over existing approaches. For qualitative results, please refer to Appendix D.

**Datasets and Evaluation Metrics.** Our experiments are conducted on two datasets: MS-COCO  and Stable-Diffusion-Prompts  (SDprompts), each with 5,000 samples for evaluation. We employ the Frechet Inception Distance  (FID) as our evaluation metric to assess the quality of images generated by the quantized diffusion models. Notably, as our objective is to minimize the performance discrepancy between the floating-point model and the quantized model, we follow the setting in  and utilize the images produced by the floating-point model as the reference for FID evaluation. This approach offers a more precise indication of the performance gap resulting from quantization. In addition, to evaluate the alignment between text and generated images, we also present the CLIP score , employing ViT-L/14  as the backbone.

**Implementation Details.** In implementing StepbaQ, we utilize a compact dataset of 128 samples for correction. Empirical observations indicate that this sample size is adequate for gathering the necessary statistics of quantization error, while expanding the sample size to 1024 does not yield a discernible enhancement in performance. The correction can be fast since only a small calibration dataset is required. For example, The correction process for the 20-step Stable-Diffusion v1.5  model can be completed in approximately 20 minutes on a single A6000 GPU.

### Improvement upon Off-the-Shelf Tool

To facilitate efficient on-device inference of deep learning models, the industry has proposed toolkits such as NeuroPilot , OpenVINO , SNPE , and TensorRT . These tools are tailored to different platforms and support various quantization settings and algorithms. In this section, we utilize the post-training quantization tool in NeuroPilot and demonstrate that StepbaQ significantly improves the performance of the given quantized diffusion model without modifying the original quantization settings. This underscores StepbaQ's effectiveness in seamlessly enhancing model performance within current quantization frameworks. Considering that PTQD depends solely on the statistics of quantization error, similar to our approach, we also include a comparison with PTQD.

**Quantization Settings.** Following previous works, we focus on quantizing the noise prediction network since it is the primary computation workload. Other parts of the diffusion pipelines remain at a floating-point precision. Notation W\(x\)A\(y\) indicates the weights are quantized to \(x\)-bit while the activations are quantized to \(y\)-bit. We test two bit-width settings, W8A8 and W4A8, using the default symmetric/asymmetric settings for weights and activations, respectively. The calibration set comprises 270 samples, which are uniformly selected from each step. NeuroPilot facilitates quantization of most operators within the noise prediction network, e.g., convolution, linear layer, batch matrix multiplication, addition, multiplication, softmax, and SiLU. This scenario presents a more difficult challenge than the one adopted by Q-Diffusion, which only accounts for the quantization of selected operators such as convolution and linear layer. Hence, our setting more rigorously tests StepbaQ's robustness under comprehensive and demanding conditions. Note that for SDXL-Turbo, due to its wide activation value ranges, the input prompt embedding is quantized to 16-bit instead to prevent loss of conceptual information. The consuming linear layers take 16-bit prompt embedding and output 8-bit activations; the weights are still quantized to 4-bit/8-bit as described.

**Stable-Diffusion v1.5 .** We perform experiments on Stable-Diffusion v1.5, utilizing a DDIM sampler with 20 sampling steps. We opt for DDIM over the default PNDM sampler as our empirical findings suggest that DDIM yields better results in a 20-step sampling setting. The results are provided in Table 1, which indicate that our proposed correction technique enhances performance in terms of FID and CLIP-score compared to simple post-training quantization. Specifically, for the W8A8 setting, StepbaQ markedly improves FID by 4.35 on the MS-COCO dataset and 7.30 on the SDprompts dataset. Meanwhile, PTQD, constrained by its limited applicability, achieves only modest improvements of 0.33 and 0.16 in FID on the MS-COCO and SDprompts datasets, respectively. For the W4A8 setting, the impact of quantization error is profoundly detrimental, as evidenced by the significant deterioration in both FID and CLIP-score. In this scenario, StepbaQ dramatically reduces the FID by 26.08 and 33.60 on the MS-COCO and SDprompts datasets, respectively. These results underscore the efficacy of StepbaQ, demonstrating its capability to substantially mitigate the adverse effects of quantization error, even under challenging scenarios.

**SDXL-Turbo .** To explore whether StepbaQ is compatible with few-step approaches, we conduct experiments on SDXL-Turbo with 4 sampling steps, employing the default EulerAncesstralDiscreteScheduler (Euler-a)  (See Appendix C for implementation details). Experimental results are presented in Table 1. Interestingly, SDXL-Turbo exhibits greater resilience to quantization errors than SD v1.5. As Euler-a is a stochastic sampler, the error absorption strategy proposed by PTQD is applicable in this setting. The results show that both PTQD and StepbaQ significantly improve the

   &  &  &  \\   & &  &  &  &  \\  & & FID\(\) & CLIP\(\) & FID\(\) & CLIP\(\) & FID\(\) & CLIP\(\) & FID\(\) & CLIP\(\) \\  Naive PTQ & 8/8 & 16.69 & 26.93 & 23.66 & 28.16 & 10.48 & 26.93 & 10.12 & 28.05 \\ PTQD & 8/8 & 16.36 & 26.93 & 23.50 & 28.49 & 9.58 & **27.20** & 10.67 & 28.20 \\ StepbaQ & 8/8 & **12.34** & **27.01** & **16.36** & **28.69** & **9.53** & 27.08 & **9.72** & **28.64** \\  Naive PTQ & 4/8 & 95.51 & 23.27 & 134.91 & 19.95 & 44.76 & 26.19 & 45.45 & 28.12 \\ PTQD & 4/8 & 94.55 & 23.47 & 134.62 & 19.77 & 27.77 & 26.70 & 32.38 & 27.98 \\ StepbaQ & 4/8 & **69.43** & **24.12** & **101.31** & **21.71** & **23.92** & **26.74** & **28.14** & **28.46** \\  

Table 1: Quantization results of SD v1.5 and SDXL-Turbo on MS-COCO and SDprompts.

quality of the generated images under the challenging W4A8 condition, with StepbaQ outperforming PTQD. StepbaQ's improvements, which reduce the FID from 44.76 to 23.92 and from 45.45 to 28.14, effectively eliminate the visual artifacts associated with quantization errors, as shown in Fig. 7.

**Ablation Study.** To assess the contribution of each component of StepbaQ, we perform an ablation study on Stable-Diffusion v1.5 under the W8A8 setting. Table 2 details the results of the ablation study, examining individual components such as Temporal Information Alignment (TIA), Latent Adjustment (LA), Step Size Adaptation (SSA), and Error Accumulation (ACC). The results indicate that mere adjustments to the temporal embedding yield only marginal improvements. In contrast, simultaneous corrections to both temporal embedding and latent variables lead to more substantial enhancements. The result also underscores the critical role of the step size adaptation. Correcting the step size results in significant FID improvements of 2.27 and 2.99 on the MS-COCO and SDprompts datasets, respectively. Finally, accounting for error accumulation enables the model to achieve further FID reductions of 1.20 and 3.22 on the MS-COCO and SDprompts datasets, respectively. This highlights the value of considering error accumulation in improving model performance. Fig. 3 shows the magnitude of stepback for SD v1.5 on the SDprompts dataset under the W8A8 setting. Without considering error accumulation, steps with minor quantization errors are ignored, and therefore, corrections are not applied until the final six sampling steps. In contrast, StepbaQ, accounting for the error accumulation across multiple steps, corrects the sampling steps more frequently. Notably, both configurations exhibit a greater magnitude of stepback during the final stages of the process. This observation implies that the latter sampling steps are particularly susceptible to quantization errors, confirmed by the steep SNR curve as depicted in Fig. 2.

### Comprehensive Comparative Results

To demonstrate the superiority of StepbaQ over existing approaches, we conduct a comparative experiment against Q-diffusion , TFMQ-DM , and PTQD . We select Q-diffusion as the baseline and integrate StepbaQ and PTQD with it. The experiment is implemented based on the codebase of Q-diffusion 2, adopting the exact quantization setting for fair comparison. It is important to note that we deactivate the timestep-specific quantization parameters utilized in TFMQ-DM. While the technique can potentially enhance the results, it can be regarded as an orthogonal strategy that alters the quantization setting by introducing control of finer granularity.

**Stable-Diffusion v1.4 .** Table 3 provides the results of Stable-Diffusion v1.4 using DDIM sampler with 20 sampling steps. Results show that StepbaQ surpasses previous works in terms of FID and CLIP-score across all tested scenarios. Observations indicate that PTQD generally produces results that are inferior to the Q-Diffusion baseline. Further examination revealed that the performance drop is attributable to their strategy of correlated noise elimination, which depends on the correlation between the model's output and the quantization error. However, the observed \(R^{2}\) values are relatively low, indicating a weak correlation. Consequently, the estimated \(k\) value utilized for correlated noise elimination is unreliable and may inadvertently compromise the model's performance. On the other hand, TFMQ-DM outperforms Q-Diffusion only under the W8A8 setting through their temporal information aware reconstruction. This limitation stems from their oversight of the distribution shift in the latent space, which is a more critical issue for quantized diffusion models.

   &  &  &  &  &  \\  & & & & FID\(\) & CLIP\(\) & FID\(\) & CLIP\(\) \\   & & & 16.69 & 26.93 & 23.66 & 28.16 \\  & & & 16.25 & **27.01** & 23.21 & 28.24 \\   & \(\) & & 15.81 & 26.87 & 22.57 & 28.53 \\   & \(\) & \(\) & & 13.54 & 26.82 & 19.58 & 28.61 \\   & \(\) & \(\) & \(\) & **12.34** & **27.01** & **16.36** & **28.69** \\  

Table 2: Ablation Study of SD v1.5 on MS-COCO and SDprompts under W8A8 setting.

   &  &  &  &  &  \\  & & & FID\(\) & CLIP\(\) & FID\(\) & CLIP\(\) \\   & & & 16.69 & 26.93 & 23.66 & 28.16 \\  & & & 16.25 & **27.01** & 23.21 & 28.24 \\   & \(\) & & 15.81 & 26.87 & 22.57 & 28.53 \\   & \(\) & \(\) & & 13.54 & 26.82 & 19.58 & 28.61 \\   & \(\) & \(\) & \(\) & **12.34** & **27.01** & **16.36** & **28.69** \\  

Table 3: Quantization results of SD v1.4 on MS-COCO and SDprompts.

Conclusion

In this work, we introduce a novel aspect that considers the temporal shift in the latent space caused by the quantization error as a "stepback" in the denoising process. We present how this temporal shift would lead to deviation from the scheduled sampling trajectory and harm the performance of quantized diffusion models. To address this issue, we propose StepbaQ, which corrects the sampling steps to calibrate the sampling trajectory and alleviate quantization error accumulation. Our experimental results demonstrate that StepbaQ can serve as a plug-and-play technique, enhancing the performance of a given quantized diffusion model without modifying its quantization settings. The comparative result also demonstrates the superiority of StepbaQ over existing methods. Notably, StepbaQ requires only statistics of quantization error derived from a small calibration dataset and can be applied to both stochastic and deterministic samplers, demonstrating its applicability. However, due to the design of the stepback mechanism, StepbaQ does not apply to single-step scenarios, such as 1-step SDXL-Turbo. How to prevent performance drop of quantized diffusion model under a single-step scenario is an area that requires further exploration in future research.

## Broader Impact.

StepbaQ significantly improves the performance of quantized diffusion models, thereby enabling users to generate high-quality results even on computationally limited edge devices. While this advancement broadens the accessibility of diffusion models to a broader audience, it also increases the risk of these models being leveraged for malicious purposes.