# CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching

Dongzhi Jiang\({}^{1}\), Guanglu Song\({}^{2}\), Xiaoshi Wu\({}^{1}\), Renrui Zhang\({}^{1,3}\), Dazhong Shen\({}^{3}\),

Zhuofan Zong\({}^{1,2}\), Yu Liu\({}^{2}\), Hongsheng Li\({}^{1,3,4}\)\({}^{,}\)

\({}^{1}\)CUHK MMLab, \({}^{2}\)SenseTime Research, \({}^{3}\)Shanghai AI Laboratory, \({}^{4}\)CPII under InnoHK

{dzjiang, wuxiaoshi, zhangrenruij}@link.cuhk.edu.hk songguanglu@sensetime.com

{dzah.shen, zongzhuofan, liuyuisanail}@gmail.com hsli@ee.cuhk.edu.hk

###### Abstract

Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. We break down the problem into two causes: concept ignorance and concept mismapping. To tackle the two challenges, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with the image-to-text concept matching mechanism. Firstly, we introduce a novel image-to-text concept activation module to guide the diffusion model in revisiting ignored concepts. Additionally, an attribute concentration module is proposed to map the text conditions of each entity to its corresponding image area correctly. Extensive experimental evaluations, conducted across three distinct text-to-image alignment benchmarks, demonstrate the superior efficacy of our proposed method, CoMat-SDXL, over the baseline model, SDXL . We also show that our method enhances general condition utilization capability and generalizes to the long and complex prompt despite not specifically training on it. The code is available at https://github.com/CaraJ7/CoMat.

Figure 1: Current text-to-image diffusion model still struggles to produce images well-aligned with text prompts, as shown in the generated images of SDXL . Our proposed method, CoMat, significantly enhances the baseline model on text condition following, demonstrating superior capability in text-image alignment. All the pairs are generated with the same random seed.

## 1 Introduction

The area of text-to-image generation has witnessed considerable progress with the introduction of diffusion models  recently. These models have demonstrated remarkable performance in creating high-fidelity and diverse images based on textual prompts. However, it still remains challenging for these models to faithfully align with the prompts, especially for the complex ones. For example, as shown in Fig. 1, current state-of-the-art open-sourced model SDXL  fails to generate entities or attributes mentioned in the prompts, e.g., _the feathers made of lace_ and _dwarfs_ in the top row. Additionally, it fails to understand the relationship in the prompt. In the middle row of Fig. 1, it mistakenly generates a _Victorian gentleman_ and a _quilt_ with a river on it.

We break down this misalignment problem into two causes: concept ignorance and concept mismapping. The concept ignorance problem is caused by the diffusion model's omission of certain concepts in the text prompt. Even though the concept token is activated, the diffusion model often fails to map it to the correct area in the image, which is termed the concept mismapping problem. Actually, the misalignment originally stems from the training paradigm of the text-to-image diffusion models: Given the text condition \(c\) and the paired image \(x\), the training process aims to learn the conditional distribution \(p(x|c)\). However, the text condition only serves as additional information for the denoising loss. Without explicit guidance in learning each concept in the text, the diffusion model could easily fail to understand the concepts in the prompt correctly.

Recently, to alleviate the misalignment, various works have proposed to incorporate linguistics prior  to heuristically address the concept omission or concept mismapping problem. However, a specific design is required for each type of misalignment problem. Other works use the Large Language Model (LLM)  to split the prompt into single entities and generate each of them. Although this method promotes the congruence between the image's global structure and the text prompt, it still suffers from local misalignment of the single entity. Hence, we ask the question: _Is there a universal solution to address various global and local misalignment problems?_

In this work, we propose CoMat, an end-to-end fine-tuning strategy to enhance the prompt understanding and following by a novel image-to-text matching mechanism. **Concept Activation** module is proposed to address the concept ignorance problem. Given the generated image \(\) conditioning on the prompt \(c\), we seek to model and maximize the posterior probability \(p(c|)\) using a pre-trained image-to-text model. In contrast to regarding the textual prompt merely as a condition, as performed in the pre-training phase of the diffusion model, our approach incorporates the condition as a supervisory signal during the training process. Thanks to the proficiency of the image-to-text model in concept matching, whenever a particular concept is absent from the generated image, the diffusion model is steered to incorporate it within the image generation process. The guidance forces the diffusion model to revisit the ignored conditions and attend more to them. As an illustrative example shown in Fig. 2, the ignored concept in the image (e.g., the gown) possesses low attention activation values. After applying our method, we observe increased attention activation of each key concept,

Figure 2: **Visualization of token activation and attention map**. We compare the tokens’ attention activation value and attention map before and after applying our methods. Our method improves token activation and encourages the missing concept ‘gown’ to appear. Furthermore, the attention map of the attribute token ‘red’ better aligns with its region in the image.

contributing to the aligned image. In addition, considering the catastrophic forgetting issue arising from the new optimization objective, we also introduce a novel fidelity preservation module and mixed latent strategy to preserve the generation capability of the diffusion model. As for the concept mismapping problem, we find it especially prevails among the attributes of the objects. Hence, the **Attribute Concentration** module is introduced to promote both positive and negative mapping. We match the concept of attribute tokens in the text prompt to the generated image, with the insight that the attribute tokens should only be activated within its entity's area. Since the concept is a general term for a variety of features, our method can address both global structures and local details.

As an end-to-end method, no extra overhead is introduced during inference. We also show that our method is composable with methods leveraging external knowledge. Our contributions are summarized as follows:

* We propose CoMat, a text-to-image diffusion model fine-tuning strategy to effectively enhance the condition utilization capability by explicitly addressing the condition ignorance and incorrect condition mapping problem.
* We introduce the concept activation module equipped with fidelity preservation and mixed latent strategy to facilitate concept generation and attribute concentration module to foster correct concept mapping from text to image.
* Extensive quantitative and qualitative comparisons with baseline models indicate that our method significantly improves the text-image alignment in various scenarios, including object existence, attribute binding, relationship, and complex prompts.

## 2 Related Work

Recently, text-to-image diffusion models [56; 51; 65; 66] have become extremely trending, but they have also brought many new challenges [28; 61]. Among them, the text-to-image alignment problem has gained much attention. The problem is defined as the incoherence between the prompts and the generated images, which involves multiple aspects including existence, attribute binding, relationship, etc. Recent methods address the problem mainly in three ways.

Attention-based methods [6; 53; 47; 70; 2; 40] aim to modify or add restrictions on the attention map in the attention module in the UNet. This type of method often requires a heuristic design for each misalignment problem.

Planning-based methods first obtain the image layouts, either from the input of the user [39; 11; 32; 74; 15] or the generation of the Large Language Models (LLM) [48; 77; 69], and then produce aligned images conditioned on the layout. In addition, a few works propose to further refine the image with other vision expert models [55; 72; 71; 77]. Although such method splits a compositional prompt into single objects, it does not resolve the inaccuracy of the downstream diffusion model and still suffers from incorrect attribute binding problems. Besides, it exerts nonnegligible costs during inference.

Moreover, some works aim to enhance the alignment using feedback from image understanding models. [28; 62] fine-tune the diffusion model with well-aligned generated images chosen by the VQA model  to strategically bias the generation distribution. Other works propose to optimize the diffusion models in an online manner. [17; 4] introduce RL fine-tuning for generic rewards. As for differentiable reward, [14; 75; 73] propose to backpropagate the reward function gradient through the denoising process. Other works like  enhance the prompt encoding to foster better alignment. Similar to our work,  also proposes to leverage image captioning models. We discuss the difference between their method with ours in Appendix C.

## 3 Preliminaries

We implement our method on the leading text-to-image diffusion model, Stable Diffusion , which belongs to the family of latent diffusion models (LDM). In the training process, a normally distributed noise \(\) is added to the original latent code \(z_{0}\) with a variable extent based on a timestep \(t\) sampling from \(\{1,...,T\}\). Then, a denoising function \(_{}\), parameterized by a UNet backbone, is trained to predict the noise added to \(z_{0}\) with the text prompt \(\) and the current latent \(z_{t}\) as the input. Specifically, the text prompt is first encoded by the CLIP  text encoder \(W\), then incorporated into the denoisingfunction \(_{}\) by the cross-attention mechanism. Concretely, for each cross-attention layer, the latent and text embedding is linearly projected to query \(Q\) and key \(K\), respectively. The cross-attention map \(A^{(i)}^{h w l}\) is calculated as \(A^{(i)}=(^{(i)}(K^{(i)})^{T}}{})\), where \(i\) is the index of head. \(h\) and \(w\) are the resolution of the latent, \(l\) is the token length for the text embedding, and \(d\) is the feature dimension. \(A^{n}_{i,j}\) denotes the attention score of the token index \(n\) at the position \((i,j)\). The denoising loss in diffusion models' training is formally expressed as:

\[_{}=_{z_{0},t,p,(0,I)} [\|-_{}(z_{t},t,W())\|^{2} ].\] (1)

For inference, one draws a noise sample \(z_{T}(0,I)\), and then iteratively uses \(_{}\) to estimate the noise and compute the next latent sample.

## 4 Method

The overall framework of our method is shown in Fig. 4. In Section 4.1, we first illustrate the concept activation module. Following this, we detail how we maintain the generation capability of the diffusion model by the fidelity preservation module and mixed latent strategy. Subsequently, in Section 4.2, we introduce the attribute concentration module for promoting attribute binding, and then we integrate the two components for joint learning.

### Concept Activation

As noted in Section 1, the diffusion model occasionally exhibits little attention on certain concepts, and the corresponding concept is therefore missing in the image, which we termed as the condition

Figure 3: We showcase the results of our CoMat-SDXL compared with other state-of-the-art models. CoMat-SDXL consistently generates more faithful images.

ignorance problem. To address this, our key insight is to add supervision on the generated image to detect the missing concepts. We achieve this by leveraging the image understanding ability of an image-to-text model, which can accurately identify concepts not present in the generated image based on the given text prompt. With the image-to-text model's supervision, the diffusion model is compelled to revisit text tokens to search for ignored condition information and assign more significance to the previously overlooked text concepts for better text-image alignment. Concretely, given a prompt \(\) with word tokens \(\{w_{1},w_{2},,w_{L}\}\), we first generate an image \(\) with the denoising function \(_{}\) after \(T\) denoising steps. Then, a frozen image-to-text model \(\) is used to score the alignment between the prompt and the image in the form of log-likelihood. The scoring capability of \(\) comes with the image-to-text models' training nature. These models are trained for the image captioning task with the negative loglikelihood loss, i.e., the model needs to maximize the probability of generating the caption given the corresponding image. Therefore, whenever the generated image does not align with the text prompt, the model will output a low log-likelihood. Our training objective aims to minimize the negative of the log-likelihood, denoted as \(_{i2t}\):

\[_{i2t}=-(p_{}(|(; _{})))=-_{i=1}^{L}(p_{}(w_{i}|,w_ {1:i-1})).\] (2)

Besides, it is also important to note that the concepts in the image include a broad field. This method provides a universal solution to various misalignment problems like object existence, complex relationships, etc. Finally, to conduct the gradient update through the whole iterative denoising process, we follow  to fine-tune the denoising network \(_{}\), which ensures the training effectiveness and efficiency by simply stopping the gradient of the denoising network input.

However, since this fine-tuning process is purely piloted by the knowledge from the image-to-text model, the diffusion model could quickly overfit to the image-to-text model, lose its original capability, and produce deteriorated images, as shown in Fig. 7. To address this hacking issue, we introduce a novel fidelity preservation module and a mixed latent training strategy to preserve the generation ability of the diffusion model and guide the learning process.

**Fidelity Preservation**. We propose a novel adversarial loss that uses a discriminator to differentiate between images generated by pre-trained and fine-tuned diffusion models. Instead of using real-world images as the real data input for the discriminator, we use images generated by the original pre-trained diffusion model. This choice is based on the significant gap that still exists between the images generated by the original diffusion model and real-world images. Simply aligning the distribution of images generated by the fine-tuned diffusion model with that of real-world images would pose an undesired challenge for the learning process. For the discriminator \(_{}\), we initialize it with the pre-trained UNet in the Stable Diffusion model. The choice is motivated by the fact that the

Figure 4: **Overview of CoMat**. The text-to-image diffusion model (T2I-Model) first generates an image according to the text prompt. Then the image is sent to the concept activation module and attribute concentration module to compute the loss for fine-tuning the online T2I-Model.

pre-trained UNet shares similar knowledge with the online training model and fits well with the input domain. In our practice, this also enables the adversarial loss to be directly calculated in the latent space instead of the image space. Concretely, given a single text prompt, we employ the original diffusion model and the online training model to respectively generate image latent \(_{0}\) and \(_{0}^{}\). The adversarial loss is then computed as follows:

\[_{adv}=(_{}(_{0}))+ (1-_{}(_{0}^{})).\] (3)

We aim to fine-tune the online model to minimize this adversarial loss, while concurrently training the discriminator to maximize it.

**Mixed Latent Strategy.** Besides, we inject information from real-world images to guide the learning process. Specifically, in addition to the latents starting from pure noise (marked as black in Fig. 4), we obtain the noisy real latents by adding noise on a real-world image at a random timestep \(\) (marked as 'Noisy GT'). We jointly denoise these two types of latents and calculate the loss given by the image-to-text model. The intuition is that, since the noisy real latent is a perturbed version of the real-world image, which is well aligned with its prompt, this provides a shortcut for the diffusion model to directly reconstruct the original image. This guidance can not only smooth the optimization process, but also prohibits the gradient from simply hacking the image-to-text model and encourages the diffusion model to generate an image both aligned with the prompt and of high fidelity. More illustration is included in Appendix A.

### Attribute Concentration

Except for paying enough attention to the concept, the diffusion model must also map the concepts correctly on the image. As we dive into the generation process by visualizing the token attention activation map, we find that, for the attribute token, even though it is activated, it fails to attend to the correct area in the image and still causes the misalignment, e.g., 'yellow' in Fig. 5. Hence, we introduce the attribute concentration module to encourage the positive and discourage the negative concept mapping of attributes.

Specifically, we first extract all the entities \(\{e_{1},..,e_{N}\}\) in the prompts. An entity can be defined as a tuple of a noun \(n_{i}\) and its attributes \(a_{i}\), i.e., \(e_{i}=(n_{i},a_{i})\), where both \(n_{i}\) and \(a_{i}\) are the sets of one or multiple tokens. We employ spaCy's transformer-based dependency parser  to parse the prompt to find all entity nouns, and then collect all attributes for each noun. A predefined set of nouns is established for filtering, including nouns that are abstract (e.g., scene, atmosphere, language), difficult to identify their area (e.g., sunlight, noise, place), or describe the background (e.g., morning, bathroom, party). Given all the selected nouns, we use them to prompt an open vocabulary segmentation model, Grounded-SAM , to find their corresponding regions as a binary mask \(\{M^{1},...,M^{N}\}\). It is worth emphasizing that, to guarantee the segmentation accuracy, we only use the nouns of entities, excluding their associated attributes, as prompts for segmentation, considering the diffusion model could likely ignore the attribute or assign a wrong one to the object. Taking the'suitcase' object in Fig 5 as an example, the model ignored the 'purple' attribute. Consequently, if the prompt 'purple suitcase' is given to the segmentor, it will fail to identify the entity's region. These inaccuracies can lead to a cascade of errors in the following process.

We add supervision to promote the diffusion model to map the entity tokens to the positive area, i.e., the entity area, and not to attend to the negative area, i.e., the other area:

\[_{} = -_{i=1}^{N}_{M^{1}_{u,v}=1}(_{ k n_{i} a_{i}}(_{u,v}}{_{x,y}A^{k}_{x,y}})+  a_{i}}A^{k}_{u,v})}{|A|}),\] (4) \[_{} = -_{i=1}^{N}_{M^{1}_{u,v}=0}(_{ k n_{i} a_{i}}(_{u,v}}{_{x,y}A^{k}_{x,y}})+  a_{i}}A^{k}_{u,v})}{|A|}}_{ }),\] (5)

where \(|A|\) is the number of pixels on the attention map, \(\) and \(\) are two scaling factors, and \(M^{i}\) should be resized to the resolution for each attention map \(A\). The loss function covers the level of regions and pixels. Take the \(_{}\) for example. We restrict the attention of each entity tokens \(e_{i}\) only activated inside the positive region by the region-level loss. We further restrict each pixel in the positive region to only attend to entity tokens by the pixel-level loss. We take into account the scenario where certain objects in the prompt do not appear in the generated image due to misalignment. In this case, the negative loss of pixels is still valid. When the mask is entirely zero, it signifies that none of the pixels should attend to the missing entity tokens in the current image.

Finally, we combine the image-to-text model loss, adversarial loss and attribute concentration loss to build up our training objectives for the online diffusion model as follows:

\[=_{}+_{}+_{ }+_{},\] (6)

where \(\) are scaling factors to balance the loss. We provide the pseudocode for the loss computation process in Algorithm 1.

## 5 Experiment

### Experimental Setup

**Base Model Settings.** We mainly implement our method on SDXL  for all experiments, and we also evaluate our method on Stable Diffusion v1.5  (SD1.5). For the caption

    &  &  &  \\    & **Color**\(\) & **Shape\(\)** & **Texture\(\)** & **Spatial\(\)** & **Non-Spatial\(\)** & \\  StructureDiffusion  & 0.4990 & 0.4218 & 0.4900 & 0.1386 & 0.3111 & 0.3355 \\ Composable Diffusion  & 0.4063 & 0.3299 & 0.3645 & 0.0800 & 0.2980 & 0.2898 \\ Attend-and-Excite  & 0.6400 & 0.4517 & 0.5963 & 0.1455 & 0.3109 & 0.3401 \\ TokenCompose  & 0.5055 & 0.4852 & 0.5881 & 0.1815 & 0.3173 & 0.2937 \\ PixArt-\(\) & 0.6690 & 0.4927 & 0.6477 & 0.2064 & 0.3197 & 0.3433 \\ Playground-v2  & 0.6208 & 0.5087 & 0.6125 & 0.2372 & 0.3098 & 0.3613 \\  SD1.5  & 0.3758 & 0.3713 & 0.4186 & 0.1165 & 0.3112 & 0.3047 \\
**CoMat-SDL5 (Ours)** & 0.6734 & 0.5064 & 0.6243 & 0.2073 & 0.3166 & 0.3575 \\  & _(+0.2976) & _(+0.1351) & _(+0.2057) & _(+0.0908) & _(+0.0054) & _(+0.0528)_ \\  SDXL  & 0.5879 & 0.4687 & 0.5299 & 0.2131 & 0.3119 & 0.2327 \\
**CoMat-SDXL (Ours)** & 0.7827 & 0.5329 & 0.6468 & 0.2428 & 0.3187 & 0.3680 \\  & _(+0.1948) & _(+0.0642) & _(+0.1169) & _(+0.0297) & _(+0.0068)_ & _(+0.0443)_ \\   

Table 1: T2I-CompBench result. The best score is in blue, with the second-best score in green.

Figure 5: **Overview of Attribute Concentration**. Given a prompt, we first generate an image and record the cross-attention map for each token. We then identify regions of each entities in the prompt using the segmentation model. Finally, we optimize for the consistency between the entity attention map and its respective area in the image by encouraging positive and discouraging negative mapping.

BLIP  fine-tuned on COCO  image-caption data. We adopt the pre-trained UNet of SD1.5 as the discriminator in the fidelity preservation module. More training details are in Appendix E.1.

**Dataset.** Since the prompt to the diffusion model needs to be challenging enough to lead to missing concepts, we directly utilize the training data or text prompts provided in existing text-to-image alignment benchmarks. Specifically, the training data includes the training set provided in T2I-CompBench , all the data from HRS-Bench , and 5,000 prompts randomly chosen from ABC-6K . Altogether, these amount to around 20,000 text prompts. Note that the training set composition can be freely adjusted according to the ability targeted to improve. The text-image pairs used in the mixed latent strategy are from the training set of COCO .

**Benchmarks.** We evaluate our method on three text-image alignment benchmarks and follow their default settings. T2I-CompBench  comprises 6,000 compositional text prompts evaluating 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). TIFA  uses pre-generated question-answer pairs and a VQA model to evaluate the generation results with 4,000 diverse text prompts and 25,000 questions across 12 categories. DPG-Bench  composes 1065 dense prompts with an average token length of 83.91. The prompt depicts a much more complex scenario with diverse objects and adjectives.

### Quantitative Results

We compare our methods with our baseline models: SD1.5 and SDXL, and two state-of-the-art open-sourced text-to-image models: PixArt-\(\) and Playground-v2 .

**T2I-CompBench**. The evaluation result is shown in Table 1. Note that we cannot reproduce results reported in some relevant works [7; 28] due to the evolution of the evaluation code. All our shown results are based on the latest code released in GitHub1. We observe significant gains in all six sub-categories compared with our baseline models. With our methods, SD1.5 can even achieve better or comparable results compared with PixArt-\(\) and Playground-v2. Our CoMat-SDXL demonstrates the best performance regarding attribute binding, spatial relationships, and complex compositions.

**TIFA.** We show the results in TIFA in Table 2. Our CoMat-SDXL achieves the best performance with an improvement of 1.6 scores compared to SDXL. Besides, CoMat significantly enhances SD1.5 by 7.4 scores, which largely surpasses PixArt-\(\).

**DPG-Bench.** The results in DPG-Bench is shown in Table 2. Although we do not train our model on dense prompts and can only accept 77 tokens, similar to Stable Diffusion, our method successfully generalizes to this more complex scenario and brings significant improvement to the baseline model.

### Qualitative Results

Fig. 3 presents a side-by-side comparison between CoMat-SDXL and other state-of-the-art diffusion models. We observe these models exhibit inferior condition utilization ability compared with CoMat-SDXL. Prompts in Fig. 3 all possess concepts that are contradictory to real-world phenomena. All the three compared models stick to the original bias and choose to ignore the unrealistic content (e.g., waterfall cascading from a teapot, transparent violin, robot penguin, and waterfall of liquid gold), which causes misalignment. However, by training to faithfully align with the conditions in the prompt, CoMat-SDXL follows the unrealistic conditions and provides well-aligned images. The user study result and more visualization result is detailed in Appendix B.1 and F.2.

  
**Model** & **TIFA\(\)** & **DPG\(\)** \\  PixArt-\(\) & 82.9 & 71.11 \\ Playground-v2  & 86.2 & 74.54 \\  SD1.5  & 78.4 & 63.18 \\
**CoMat-SD1.5 (Ours)** & 85.8 & 73.32 \\
**(_ref._4_)** & _(_ref._4_)_ & _(_ref._10_)_ \\  SDXL  & 85.9 & 74.65 \\
**CoMat-SDXL (Ours)** & **87.5** & **77.13** \\ _(_ref._6_)_ & _(_ref._2_)_ & _(_ref._2_)_ \\   

Table 2: TIFA and DPG-Bench results.

  
**Model** & \(_{}\) & \(_{}\) input & ML & **FID-10K\(\)** \\  SD1.5  & N/A & N/A & N/A & 16.69 \\ CoMat-SD1.5 & ✗ & N/A & ✗ & 19.02 \\ CoMat-SD1.5 & UNet  & real-world latent & ✗ & 17.99 \\ CoMat-SD1.5 & UNet  & generated latent & ✗ & 16.69 \\ CoMat-SD1.5 & DINO  & generated image & ✗ & 23.86 \\ CoMat-SD1.5 & UNet  & generated latent & ✗ & **15.43** \\   

Table 3: FID-10K result.

### Ablation Study

**Effectiveness of Concept Activation and Attribute Concentration.** In Table 4, we show the T2I-CompBench result aiming to identify the effectiveness of the concept activation and attribute concentration modules. We find that the concept activation module accounts for major gains to the baseline model. On top of that, the attribute concentration module brings further improvement to all six sub-categories in T2I-CompBench. We show the qualitative effectiveness in Fig. 6.

**Design of Fidelity Preservation and Mixed Latent.** We examine the photorealism of generated images to evaluate the generation capability. We calculate the FID  score using 10K data from the COCO validation set. As shown in Table 3 and Fig. 7, without any preservation method, the diffusion model only tries to hack the image-to-text model and loses its original generation ability with an increase of FID score from 16.69 to 19.02. Besides, inputting the latent generated by the original diffusion model performs better than the latent of real-world images. As for the discriminator architecture, the UNet is superior to a pre-trained DINO  which even interferes the training process. Finally, the Mixed Latent (ML) strategy further enhances the generated image quality.

    &  &  &  &  &  \\    & & & **Color \(\)** & **Shape\(\)** & **Texture\(\)** & **Spatial\(\)** & **Non-Spatial\(\)** \\  SDXL & & & 0.5879 & 0.4687 & 0.5299 & 0.2131 & 0.3119 & 0.3237 \\ SDXL & ✓ & & 0.7455 & 0.5043 & 0.6252 & 0.2321 & 0.3171 & 0.3660 \\ SDXL & ✓ & ✓ & **0.7827** & **0.5329** & **0.6468** & **0.2428** & **0.3187** & **0.3680** \\   

Table 4: Impact of concept activation and attribute concentration. ‘CA’ and ‘AC’ denote concept activation and attribute concentration respectively.

    &  &  &  \\    & & **Color \(\)** & **Shape\(\)** & **Texture\(\)** & **Spatial\(\)** & **Non-Spatial\(\)** \\  BLIP  & & **0.7827** & **0.5329** & **0.6468** & **0.2428** & **0.3187** & **0.3680** \\ GIT  & & 0.6916 & 0.5146 & 0.5971 & 0.2404 & 0.3149 & 0.3413 \\ LLaVA  & & 0.6338 & 0.4722 & 0.5518 & 0.1963 & 0.3117 & 0.3286 \\ N/A & & 0.5879 & 0.4687 & 0.5299 & 0.2131 & 0.3119 & 0.3237 \\   

Table 5: The impact of different image-to-text models.

Figure 6: Visualization of the effectiveness of the proposed modules. CA contributes to the existence of objects mentioned in the prompts. AC further guides the attention of the attributes to focus on their corresponding objects.

Different Image-to-text Models.We show the T2I-CompBench results with different image captioning models in Table 5. We find that all three image-to-text models can boost the performance of the diffusion model with our framework, where BLIP achieves the best performance. We provide more analysis on the choice of the image-to-text models in Appendix B.3.

### Robustness Analysis

We test the robustness of our method by the method proposed in , which introduces an automated way to discover prompts that induce misalignment in Stable Diffusion models. We evaluate this attack method on SD1.5 and CoMat-SD1.5 using both short and long prompts.

For 1,000 ImageNet-1K classes, we generate 20 samples per class using the attack method and measure the success rate - defined as the proportion of generated images that could be mistakenly classified by a visual classifier. Table 6 shows that CoMat-SD1.5 exhibits lower attack success rates for both prompt lengths, demonstrating enhanced alignment robustness compared to the base model.

## 6 Limitations

How to more effectively incorporate Multimodal Large Language Models (MLLMs) into text-to-image diffusion models by our proposed method requires more exploration. MLLM possesses state-of-the-art image-text understanding capability in addition to image captioning. We will focus on leveraging MLLMs to provide finer-grained guidance to the diffusion model in our future work. In addition, the attribute concentration module cannot assign attributes to multiple same-name objects, such as an Asian girl with an Indian girl, the segmentation model cannot differentiate two girls and therefore cannot assign attributes. As for the training cost, since our method needs the diffusion model to perform the whole inference process, the training time is extended. Our future direction will be to accelerate the training process.

## 7 Conclusion

In this paper, we propose CoMat, an end-to-end diffusion model fine-tuning strategy equipped with image-to-text concept matching. We identify the two causes of the misalignment problem and propose two key components to explicitly address them. The concept activation module leverages an image-to-text model to supervise the generated image and find out the ignored condition information. It also integrates the fidelity preservation module and mixed latent strategy to maintain the generation capability. Besides, we introduce the attribute concentration module to address the attribute mismapping issue. Through extensive experiments, we have demonstrated that CoMat largely outperforms its baseline model and even surpasses commercial products in multiple aspects. We hope our work can inspire future work on the cause of the misalignment and the solution to it.

  
**Model** & **Short prompt\(\)** & **Long prompt\(\)** \\  SD1.5  & 49.1\% & 51.1\% \\ CoMat-SD1.5 & 46.8\% & 50.4\% \\   

Table 6: Success rate of prompt attack.

Figure 7: Visualization result of the effectiveness of the Fidelity Preservation module (FP) and Mixed Latent (ML) strategy.