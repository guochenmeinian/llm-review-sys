# Empowering Visible-Infrared Person Re-Identification with Large Foundation Models

Zhangyi Hu\({}^{1}\)   Bin Yang\({}^{1}\)   Mang Ye\({}^{1}\)

\({}^{1}\)National Engineering Research Center for Multimedia Software,

School of Computer Science, Wuhan University, Wuhan, China.

{zhangyi_hu,yangbin_cv,yemang}@whu.edu.cn

https://github.com/WHU-HZY/TVI-LFM

Equal Contribution.Corresponding Author

###### Abstract

Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal retrieval task due to significant modality differences, primarily resulting from the absence of color information in the infrared modality. The development of large foundation models like Large Language Models (LLMs) and Vision Language Models (VLMs) motivates us to explore a feasible solution to empower VI-ReID with off-the-shelf large foundation models. To this end, we propose a novel Text-enhanced VI-ReID framework driven by Large Foundation Models (TVI-LFM). The core idea is to enrich the representation of the infrared modality with textual descriptions automatically generated by VLMs. Specifically, we incorporate a pre-trained VLM to extract textual features from texts generated by VLM and augmented by LLM, and incrementally fine-tune the text encoder to minimize the domain gap between generated texts and original visual modalities. Meanwhile, to enhance the infrared modality with extracted textual representations, we leverage modality alignment capabilities of VLMs and VLM-generated feature-level filters. This enables the text model to learn complementary features from the infrared modality, ensuring the semantic structural consistency between the fusion modality and the visible modality. Furthermore, we introduce modality joint learning to align features across all modalities, ensuring that textual features maintain stable semantic representation of overall pedestrian appearance during complementary information learning. Additionally, a modality ensemble retrieval strategy is proposed to leverage complementary strengths of each query modality to improve retrieval effectiveness and robustness. Extensive experiments on three expanded VI-ReID datasets demonstrate that our method significantly improves the retrieval performance, paving the way for the utilization of large foundation models in downstream multi-modal retrieval tasks.

## 1 Introduction

Person Re-Identification (ReID) aims to retrieve images of the same identity across different cameras, which is an important task for intelligent surveillance and urban security . Although RGB-based methods  have shown promising results during the daytime, their performance significantly declines at night, as RGB cameras fail to capture sufficient information about individuals in low-light conditions. Infrared cameras can capture pedestrian appearances in dark environments. Therefore, Visible-Infrared Person Re-Identification (VI-ReID) is proposed to match images of individuals captured by visible and infrared cameras, enabling 24-hour surveillance. However, the absence of crucial information, such as color, in infrared images results in significant differences between infrared and visible modalities, posing a major challenge for VI-ReID.

Most existing VI-ReID methods, including supervised methods [59; 57; 20; 56; 28; 58; 11; 53; 60], semi-supervised [39; 45] methods and unsupervised methods [51; 38; 37; 49; 50; 52; 48; 61], primarily focus on mining modality-shared features, while paying less attention to compensating for information absence in the infrared modality, which limits further improvements in cross-modal retrieval performance.

In real-world scenarios, based on the visible modality, human descriptions can provide rich information, such as color, serving as vital auxiliary clues. However, existing methods that utilize auxiliary text  to enhance the infrared modality, as shown in Fig. 1, heavily rely on human annotation to collect fixed text descriptions, resulting in significant time and labor costs. Moreover, they depend on prior knowledge, such as pre-defined color vocabularies or hyper-parameters, to design complex loss functions and modules with additional parameters for modality alignment and fusion. This reliance increases sensitivity to data variations and reduces the effectiveness of the fusion process.

Recent advancements in large foundation models , particularly LLMs and VLMs, demonstrate significant potential for multi-modal retrieval tasks. This motivates us to explore a feasible solution to complement missing vital information with off-the-shelf large foundation models. To this end, we propose a Text-enhanced VI-ReID framework driven by Large Foundation Models (TVI-LFM) which comprises Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). The core idea is to enrich infrared representations with generated text, which is a cross-modality retrieval approach bolstered by heterogeneous text descriptions. Specifically, the MSC employs an LLM to augment VLM-generated texts, creating dynamic descriptions of visible and infrared images, reducing labor and time costs, while enhancing the model's robustness against text variations. Then, IFS incorporates a pre-trained VLM to extract features from text generated by MSC, and incrementally fine-tunes the text encoder to minimize the domain gap between the generated texts and the original visual modalities. To enhance the infrared modality with extracted textual representations, IFS leverages the modality alignment capabilities of VLMs and VLM-generated feature-level filters to create a fusion modality. This enables the text model to learn complementary features from the infrared modality, ensuring semantic structural consistency between the fusion modality and the visible modality. Furthermore, IFS introduces modality joint learning to align features across all modalities, ensuring that textual features maintain a stable semantic representation of the overall pedestrian appearance during complementary information learning. Additionally, MER is introduced to leverage complementary strengths of each query modality, forming ensemble queries to further improve retrieval performance. Finally, by integrating the above three modules for dynamic text generation, semantic alignment, and the integration of complementary queries, our method effectively addresses the information absence in the infrared modality, significantly improving cross-modal retrieval performance.

The main contributions can be summarized as follows:

Figure 1: Illustration of our idea. Existing methods rely on manual annotations, complex architectures and prior-knowledge-based optimization to enrich infrared modality, leading to significant time and labor cost, additional parameters and data sensitivity. In contrast, our method employs VLMs and LLMs to automatically generate dynamic text, enhancing the robustness against text variation. Additionally, we fine-tune a pre-trained VLM through aligning features across all modalities, enabling the framework to create fusion features semantically consistent with visible modality in a parameter-free manner.

* We design a Text-enhanced VI-ReID framework driven by Large Foundation Models (TVI-LFM). It enriches infrared representations with generated textual descriptions, effectively mitigating the absence of critical information, e.g. color, in the infrared modality and significantly improving the performance of cross-modal retrieval.
* We propose the IFS that fine-tunes a pre-trained VLM to align generated texts with original images. It creates a fusion modality to learn complementary information from the infrared modality and jointly aligns features across all modalities. This ensures a stable semantic consistency of the text, the fusion, and the visible modality during complementary information learning.
* We propose the MER that leverages the complementary strengths of all query modalities, forming ensemble queries to further improve the performance of cross-modality retrieval bolstered by heterogeneous text descriptions.
* We introduce three extended VI-ReID datasets with VLM-generated textual descriptions for every image. Extensive experiments on these expanded datasets demonstrate the competitive performance of our TVI-LFM framework, paving the way for the utilization of large foundation models in downstream multi-modal retrieval tasks.

## 2 Related Work

### Visible-Infrared Person Re-Identification

VI-ReID aims to retrieve images across visible and infrared modalities, but it suffers from the absence of critical information, such as color, in infrared modality. Previous methods [59; 4; 64; 27; 57] primarily focus on mining modality-shared information and optimizing features extracted by CNNs or Transformers  but pay less attention to compensate for the missing vital information in the infrared modality, which limits further improvements in retrieval performance. Some methods [9; 5; 1] explore auxiliary information compensation. Specifically,  uses coarse descriptions as textual identity labels, while  and  integrate attribute embeddings with visual features. These methods heavily rely on handcrafted annotations for each identity. Furthermore and require prior knowledge, such as hyper-parameters and pre-defined color vocabulary, to design complex modules and loss functions. This results in additional parameters and increased sensitivity to variations in auxiliary data. In contrast, we propose a framework that automatically generates dynamic textual descriptions for VI-ReID datasets and fine-tunes a pre-trained VLM to align the generated texts with the original images. By leveraging modality alignment capabilities and feature-level filters of the VLM, our framework creates a fusion modality that enables the text model to learn complementary features from the infrared modality, while maintaining semantic consistency with the visible modality.

### Large Foundation Model

Large foundation models , pre-trained on extensive datasets, have shown great potential in downstream tasks. Recent advancements in VLMs like [44; 24; 34; 7; 26] and LLMs such as [35; 3; 67; 41; 18], demonstrate remarkable data generation and text-visual alignment capabilities. For instance, BLIP  excels at generating textual captions from images, and can be fine-tuned to accommodate various image styles such as infrared images. Vicuna , pre-trained on extensive text data, is great at customized text generation and understanding with prompts. CLIP 's pre-training on large-scale image-text pairs enables its basic capability to align text-image semantics. It can also be fine-tuned on downstream cross-modal retrieval tasks . Our approach integrates generative VLMs and LLMs for automatic text generation and dynamic augmentation. And it incorporates a pre-trained VLM into the VI-ReID system to extract features from texts and utilize them to enrich infrared representations.

### Multi-modal Analogical Reasoning in VI-ReID

As demonstrated in , language analogical reasoning in the language embedding space can be represented using vector arithmetic. For example, the analogy "_man is to woman as king is to \(?\)_" can be solved by finding the word whose embedding vector is closest to:

\[}_{^{*}}-}_{^{*}}+}_{ ^{*}}.\] (1)

Subsequently,  investigates an identical regularity manifested in the multi-modal vector space. For instance, the relationship can be expressed as:

\[}_{}-}_{^{*}}+}_{^{*}}}_{}.\] (2)

We establish this property by aligning features across all modalities, subsequently mapping them into a unified embedding space. This enables us to create fusion features in a parameter-free manner.

## 3 Proposed Method

**Task Setting.** Humans can provide textual descriptions based on visible modality. Containing critical information, such as colors, they can serve as auxiliary clues to identify individuals. Therefore, we propose the cross-modal retrieval bolstered by heterogeneous text descriptions. Let \(=\{_{rgb}^{I},_{ir}^{I},\}\) denote the sample set, where \(_{rgb}^{I}=\{V_{rgb}^{i}\}_{i=1}^{N_{rgb}}\}_{i=1}^{N_{rgb}}\) denotes \(N_{rgb}\) visible images, \(_{ir}^{I}=\{V_{ir}^{i}\}_{i=1}^{N_{ir}}\) represents \(N_{ir}\) infrared images, and \(=\{y_{i}\}_{i=1}^{N_{id}}\) denotes the label set. During inference, the \(i\)-th query sample \(q_{i}\) consists of an infrared image \(V_{ir}^{i}\), a text description \(T_{ir}^{i}\) generated from \(V_{ir}^{i}\), and a randomly selected text \(T_{rgb}^{i}\) generated from visible images of the same identity \(y_{i}\), represented as \(q_{i}=\{V_{ir}^{i},T_{ir}^{i},T_{rgb}^{i}\}\). The gallery contains visible images represented as \(}=\{V_{rgb}^{i}\}_{i=1}^{N_{rgb}}\). Finally, compute similarity ranking lists between each \(q_{i}\) and \( g_{j}}\) as results.

**Overview.** Detailed in Fig. 2, TVI-LFM contains MSC, IFS and MER. MSC employs two fine-tuned Blips  to automatically generate textual descriptions from visible and infrared images, and utilizes LLM for augmentation. IFS trains a VI-ReID backbone to extract vision features, and incrementally fine-tunes a pre-trained CLIP  to align generated texts with original images. Then, it creates a fusion modality to learn complementary features from the infrared modality and jointly aligns features across all modalities. It ensures a stable semantic consistency of the text, the fusion and the visible modality during complementary information learning. Additionally, MER leverages the strengths of each query modality, forming ensemble queries for more accurate retrieval.

### Modal-Specific Caption (MSC)

MSC utilizes fine-tuned VLMs to automatically generate text from visible and infrared images and employs an off-the-shelf LLM for textual augmentation, consequently creating dynamic descriptions for VI-ReID datasets. This module reduces the time and labor costs of manual annotations while enhancing the system's robustness against the variations in auxiliary text.

**VLM based Textual Generation.** Currently, there are no publicly available large-scale VI-ReID datasets with image-level text annotations. Thus, to reduce labor and time costs, we fine-tune two VLMs to automatically generate text descriptions with critical information, such as color, for every visible and infrared image. Consequently, we construct three expanded datasets: Tri-SYSU-MM01, Tri-LLCM, and Tri-RegDB, each derived from the original datasets [46; 31; 40] respectively.

\(\)**1) RGB Captioner**: First, train a Blip  on a large-scale pedestrian image-text dataset  as the RGB captioner \(G_{rgb}\), to generate text descriptions for each visible image \(V_{rgb}^{i}_{rgb}^{I}\).

\(\)**2) IR Captioner**: Then, randomly select visible and infrared images pairs in SYSU-MM01's training split for every identity, then apply the RGB captioner in step 1 to generate textual descriptions for

Figure 2: Illustration of our TVI-LFM, including Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). MSC utilizes fine-tuned VLMs as modal-specific captioners and employs an LLM for augmentation. IFS fine-tunes a pre-trained VLM to create fusion features semantically consistent with visible features. MER leverages the strengths of all query modalities, forming ensemble queries to improve the retrieval performance.

every visible image in these pairs. Then, remove color-related terms from these generated texts by regular expression filters, and build infrared-filtered text pairs dataset with filtered text descriptions and corresponding infrared images in the same expanded visible-infrared image pairs. Finally we fine-tune the Blip  in step 1 again on the Infrared-Text (filtered) dataset as the IR Captioner \(G_{ir}\), which is able to generate text descriptions without color for each infrared image \(V^{i}_{ir}^{I}_{ir}\).

\(\) **3) Text Expanding**: Finally, utilize the two refined modality-specific captioners in previous steps to generate text descriptions \(T^{i}_{rgb}=G_{rgb}(V^{i}_{rgb})\) and \(T^{i}_{ir}=G_{rgb}(V^{i}_{rgb})\) from original images.

The statistics and samples visualization of the expanded datasets Tri-LLCM, Tri-RegDB and Tri-SYSU-MM01 are shown in Appendix A. Through this expansion process, the framework can automatically generate text descriptions for VI-ReID datasets without large-scale manual annotations.

**LLM based Textual Augmentation.** To ensure that the framework can extract robust representations from generated descriptions against text variations while preserving original semantics of sentences, we propose the LLM-based textual augmentation module applied during the training stage. This module regenerates diverse descriptions by rephrasing the original text for the same target. In detail, given an original description, the module employs an LLM to rephrase it, producing an augmented textual description. The LLM is guided by the prompt _"Rephrase the person's description above using similar words. Answer:"_. The augmentation is applied as follows:

\[T^{i*}_{m}=llm(T^{i}_{m} prompt),&p\\ &T^{i}_{m},&1-p,\;m\{rgb,ir\},\] (3)

where \(T^{i}_{m}\) denotes the text descriptions generated from visible or infrared images, \(T^{i*}_{m}\) represents the augmented text \(T^{i}_{m}\), and \(p=0.5\) reflects that each description variant is equally probable. Benefiting from the powerful prompt-driven text generation capability of LLM, this approach diversifies the textual descriptions while maintaining their original meanings. This forces the model to focus on the core person appearance information, thus enhancing the robustness of our system against text variation. Moreover, we can also apply this augmentation method directly on existing frameworks involving text data processing, without changing the original structure.

### Incremental Fine-tuning Strategy (IFS)

IFS incrementally fine-tunes a CLIP  based on the frozen visual features extracted by a trained VI-ReID backbone, to minimize the domain gap between the generated texts and original visual modalities, namely, to align the complementary feature across vision and text, thereby creating fusion features semantically consistent with visible modality. The detailed steps are as follow:

**Features Extraction.** During the training stage, each batch contains \(N_{B}\) selected **quads** as inputs, denoted as \(\{(V^{i}_{ir},V^{i}_{rgb},T^{i*}_{ir},T^{i*}_{rgb})\}_{i=1}^{N_{B}}\). Here, \(T^{i*}_{ir}\) and \(T^{i*}_{rgb}\) are augmented texts generated from the \(i\)-th image pair \(V^{i}_{ir}\) and \(V^{i}_{rgb}\), which is randomly selected belonging to identity \(y_{i}\). IFS first utilizes the Channel Augmentation (CA)  strategy to train a dual-stream ResNet-50  as the VI-ReID backbone, detailed in Appendix B. Then utilize the trained backbone to extract infrared features \(^{I(i)}_{ir}\) and visible features \(^{I(i)}_{rgb}\), denoted as:

\[^{I(i)}_{ir}=F^{S}_{_{3}}(F^{IR}_{_{1}}(V^{i}_{ir})) ^{I(i)}_{rgb}=F^{S}_{_{3}}(F^{RGB}_{_{2}}(V ^{i}_{rgb})),\] (4)

where \(F^{RGB}_{_{1}}\) and \(F^{IR}_{_{3}}\) denote the visible/infrared specific layers respectively, \(F^{S}_{_{3}}\) denotes the shared layers of the visual backbone. Meanwhile, IFS incorporates a CLIP \(F^{VLM}_{_{0}}\) to extract the "text features" \(^{T(i)}_{rgb}\) from visible images texts and "filter features" \(^{T(i)}_{ir}\) from infrared images texts:

\[^{T(i)}_{rgb}=F^{VLM}_{_{0}}(T^{i*}_{rgb}) ^{T(i)}_{ir}=F^{VLM}_{_{0}}(T^{i*}_{ir}).\] (5)

**Semantic Filtered Fusion (SFF).** Meanwhile, to enhance the infrared modality with the generated texts, we propose the SFF module that leverages the text-visual alignment capability of VLM and VLM-generated filter features to create fusion features.

Benefiting from the large-scale pre-training on image-text pairs, the CLIP  possesses powerful text-visual alignment capability, ensuring that the features extracted from the generated texts can be aligned with the features of original images. Therefore, the text feature \(^{(i)}_{rgb}\) can work as an alternative for the visible feature \(^{I(i)}_{rgb}\). Similarly, the filter feature \(^{T(i)}_{ir}\) can work as an alternative for the infrared feature \(_{ir}^{I(i)}\). Next, we formulate the complementary features for the infrared modality by decomposing the visible feature \(_{rgb}^{I(i)}\) and the text feature \(_{rgb}^{T(i)}\) as:

\[_{rgb}^{I(i)}=_{ir}^{I(i)}+_{comp}^{I(i)} _{rgb}^{T(i)}=_{ir}^{T(i)}+_{comp}^{T(i)},\] (6)

where \(_{comp}^{I}\) denotes the visual complementary feature for the infrared modality. Similarly, \(_{comp}^{T}\) denotes the textual complementary feature for the infrared modality. Finally, with Eq. (6), and the alignment of features of generated texts and original images, benefiting from the pre-trained CLIP, the representation of fusion features \(^{F}\) with the same semantic structure as the visible modality can be derived, represented as:

\[_{rgb}^{I(i)} =_{ir}^{I(i)}+_{comp}^{I(i)}\] (7) \[=_{ir}^{I(i)}+(_{rgb}^{I(i)}-_{ir}^{I(i)})\] \[_{ir}^{I(i)}+(_{rgb}^{T(i)}-_{ir}^{T(i)}).\] \[=_{ir}^{I(i)}+_{comp}^{T(i)}\] \[^{F(i)}\]

By leveraging the powerful text-visual alignment capability of CLIP and VLM-generated filter features, SFF approximates \(_{rgb}^{I(i)}-_{ir}^{I(i)}\) with \(_{rgb}^{T(i)}-_{ir}^{T(i)}\). Thus, the framework can create fusion features by adding this textual complementary features to the infrared features, as shown in Fig.3. This allows the text model to learn complementary features from the infrared modality, while ensuring semantic structural consistency between the fusion modality and the visible modality.

**Modality Joint Learning (MJL)**. Furthermore, MJL is proposed to optimize the pre-trained VLM. It incrementally fine-tunes a CLIP text encoder \(F_{_{0}}^{VLM}\) based on the infrared and visible features extracted from the frozen VI-ReID backbone with parameters \(_{1},_{2}\) and \(_{3}\), thus further aligning the fusion modality and the visible modality. This training strategy and its effectiveness in avoiding conflicts during the visual and textual part representation learning are discussed in the Appendix E.

This method utilizes a classic ReID loss for fine-tuning, which eliminates the prior knowledge reliance, such as hyper-parameters and predefined vocabulary, during optimization. The loss consists of a cross-entropy loss \(L_{id}\) and a weighted regularized triplet loss \(L_{wrt}\), denoted as:

\[L_{total}=L_{id}(B^{*};_{0})+L_{wrt}(B^{*};_{0}),\;B^{*}=\{( _{rgb}^{T(i)},_{rgb}^{I(i)},_{ir}^{I(i)},^{F(i)},y_{i})\}_ {i=1}^{N_{B}},\] (8)

where \(_{0}\) denotes the **trainable** parameters of VLM, each batch \(B^{*}\) contains the **frozen** visible feature \(_{rgb}^{I(i)}\), the **frozen** infrared feature \(_{ir}^{I(i)}\), the **trainable** text feature \(_{rgb}^{T(i)}\), the **trainable** fusion feature \(^{F(i)}\) and identity label \(y_{i}\). During optimization, MJL pulls all these features belonging to the same identity \(y_{i}\) together while pushing them away from whom belonging to \( y_{j}\{y|y y_{i}\}\), aligning the semantics across all modalities. Since we solely optimize the \(F_{_{0}}^{VLM}\), this process can be considered as aligning the text features and the fusion features with visible features, under the supervision of an identity label \(y_{i}\) and the regularization from infrared features.

By aligning the textual complementary features for infrared modality \(_{comp}^{T(i)}\) integrated in the fusion features with the corresponding part \(_{comp}^{I(i)}\) in the visible features, MJL effectively reduces the discrepancy between fusion and visible modality. Additionally, it ensures the overall semantic consistency of text features \(_{rgb}^{T(i)}\) with visible features \(_{rgb}^{I(i)}\) during complementary information learning, which enables the following MER to form effective ensemble queries.

Figure 3: The Visualization of SFF. With the aligned features of generated texts and original images, SFF creates fusion features semantically consistent with visible modality by arithmetically adding the textual complementary information for infrared modality to the infrared features.

Thereby, considering the established two alignments above while taking into account the feature decomposition in Eq. (6), the alignment between infrared features \(_{ir}^{I(i)}\) and filter features \(_{ir}^{T(i)}\) can be derived and represented as:

\[_{ir}^{I(i)} =_{rgb}^{I(i)}-_{comp}^{I(i)}\] \[_{rgb}^{T(i)}-_{comp}^{T(i)}.\] (9) \[=_{ir}^{T(i)}\]

Consequently, by establishing the alignment between generated texts with original images, we successfully minimize the domain gap between the generated texts and original visual modalities, namely, align each part of the visual and textual complementary features, thereby enabling the alignment between fusion features and the visible features, leading to improved retrieval accuracy.

According to Eq. (6), \(_{rgb}^{T(i)}\) can be decomposed as trainable \(_{ir}^{T(i)}\) and \(_{comp}^{T(i)}\), while \(^{F(i)}\) can be decomposed as frozen \(_{ir}^{I(i)}\) and trainable \(_{comp}^{T(i)}\). Considering that frozen infrared features \(_{ir}^{I(i)}\) also participate in aligning with the text features \(_{rgb}^{T(i)}\) and the fusion features \(^{F(i)}\), it can be regarded as a regularization to constrain the complexity of textual complementary features \(_{comp}^{T(i)}\) while aligning with filter features \(_{ir}^{T(i)}\), enabling the framework to construct better fusion features.

### Modality Ensemble Retrieval (MER)

MER leverages the complementary modalities strengths and rich semantics in query representations mined from IFS to create an ensemble query feature \(_{q}^{(i)}\) for more accurate retrieval, represented as:

\[_{q}^{(i)}=(_{ir}^{I(i)}+_{rgb}^{T(i)}+^{F(i)})/3,\] (10)

where _fusion feature_\(^{F(i)}\) provides a combined feature with semantic structure the same as the visible features, serving as the primary matching modality. _Infrared feature_\(_{ir}^{I(i)}\) provides contiguous visual semantics. Their similarity with visible images can serve as a supplementary reference for texture and shape information. _Text feature_\(_{rgb}^{T(i)}\), aligned with visible features in MJL, provides descriptive information such as the color and the pattern of the clothes. The similarity between text features and visible features serves as a reference for these key matching features. Therefore, when encountering challenging scenarios that are hard to distinguish a person's clothing or shape but distinguishing color is feasible, or vice versa, the _ensemble query feature_\(_{q}^{(i)}\) can leverage the features of two modalities in addition to the primary matching contribution from fusion modality to explore their similarities in visual texture and key attributes respectively. Thus, the similarity score \(s_{ij}\) between \(i\)-th query sample and \(j\)-th gallery sample is defined as \(_{q}^{(i)}_{rgb}^{I(j)}\). Thus, the strengths of all query modalities can be integrated into the final similarity score, consequently enhancing the accuracy against hard cases of retrieval. In fact, by plugging Eq. (10) into \(_{q}^{(i)}_{rgb}^{I(j)}\), an equivalent definition for the similarity between two high-dimension features is derived, represented as:

\[s_{ij} =(_{ir}^{I(i)}+_{rgb}^{T(i)}+^{F(i)})_{rgb}^{I(j)}/3\] (11) \[=concat[_{ir}^{I(i)},_{rgb}^{T(i)},^{F(i)}]  concat[_{rgb}^{I(j)},_{rgb}^{I(j)},_{rgb}^{I(j)}]/3.\]

It is equivalent to increasing the feature dimension for retrieval but use ensemble features with less dimensions and computational cost. Due to the larger distances between classes in higher dimensional feature space, the models can distinguish features of different identities in the ensemble feature space more easily, therefore utilizing ensemble queries can further improve the retrieval performance.

## 4 Experiments

### Experimental Settings

**Datasets.** We evaluate our framework on Tri-SYSU-MM01, Tri-RegDB, and Tri-LLCM. These datasets with text descriptions for each visible and infrared image are expanded from the original VI-ReID datasets [46; 65; 31], utilizing fine-tuned Blip  as captioner. The splits of the training set and testing set for each dataset are available in Appendix D.

**Evaluation Protocols.** In line with established VI-ReID settings [59; 57], we assess the performance of the infrared query and the fusion query using Rank-k matching accuracy, mean Average Precision(mAP), and mean Inverse Negative Penalty (mINP)  within our TVI-LFM framework. To get stable performance on Tri-SYSU-MM01 and Tri-LLCM, we evaluate our model 10 times with random splits of the gallery set; as for Tri-RegDB, we evaluate our model on 10 trials with different train/test splits and report the average performance on each dataset.

**Implementation Overview.** We utilize a dual-stream ResNet-50  pre-trained on ImageNet  as the visual backbone and a transformer in CLIP  as the textual encoder. Training involves visible and infrared images alongside text descriptions generated by two modality-specialized fine-tuned Blip  models. All text descriptions are augmented by vicuna-7b  with a random rephrasing strategy. Incremental fine-tuning is applied by fixing the visual parameters while tuning the textual part of the framework. All details are described in Appendix B.

### Ablation Study

To thoroughly evaluate the effect of each component of our proposed method, we conduct comprehensive ablation studies on Tri-LLCM and Tri-SYSU-MM01. These studies involve gradually adding the proposed modules to our baseline, systematically removing specific modules from our framework and assessing their impact on performance. The overall experimental setup remained consistent, with only the module under evaluation being modified.

**Effect of Semantic Filtered Fusion.** By leveraging text-visual modality alignment capability of VLM and VLM-generated filter features, SFF tuses textual complementary information with infrared features to create fusion features semantically consistent with visible modality. Compared to the baseline, the method obtains a 4.48% Rank-1 improvement in Tri-SYSU-MM01 and a 2.10% Rank-1 improvement in Tri-LLCM, as shown in Table 1. The results demonstrate that the module effectively integrates information from different modalities.

**Effect of Modality Joint Learning.** In cooperation with SFF, MJL aligns features across all modalities to minimize the domain gap between the generated texts and original visual modalities, thereby mitigating the fusion-visible modality difference. Based on the experimental results in Table 1 and compared to the baseline with SFF, adding MJL gains a significant enhancement of 6.97% Rank-1 improvement, 6.67% mAP improvement, and 7.96% mINP improvement in Tri-SYSU-MM01, and 2.03% Rank-1 improvement, 2.63% mAP improvement, and 2.71% mINP improvement in Tri-LLCM. This result demonstrates the effectiveness of MJL, which greatly minimizes the modality gap.

**Effect of Modality Ensemble Retrieval.** MER leverages the complementary advantages of different query modalities to construct ensemble query features thereby improving the retrieval accuracy. The results demonstrate its effectiveness, in Table 1, incorporating MER provides an additional improvement of 0.91% in Rank-1, 0.92% in mAP, and 1.11% in mINP in the Tri-SYSU-MM01 dataset over the baseline with MJL+SFF. Similarly, on the Tri-LLCM dataset, MER achieves 0.33% Rank-1 improvement, 0.29% mAP improvement, and 0.27% mINP improvement.

**Effect of LLM based Textual Augmentation.** To extract robust text representations against text variation, we implement LLM based augmentation module by randomly rephrasing the generated descriptions for dynamic expression. As shown in Table 1, incorporating it further improves the overall performance and robustness against text variation. It also works well with other modules, achieving 84.90% Rank-1 and 58.19% Rank-1 in Tri-SYSU-MM01 and Tri-LLCM respectively.

### Comparison with the State-of-the-art Methods

We present a comprehensive comparison of TVI-LFM against state-of-the-art methods on different datasets as outlined in Table 2 and Table 3. Our evaluation includes a variety of metrics: Rank-1

    &  &  &  \\   B & SFF & MJL & LLM & MER & R1 & mAP & mINP & R1 & mAP & mINP \\  ✓ & & & & & & 72.52 & 69.15 & 55.93 & 52.63 & 58.82 & 55.43 \\ ✓ & ✓ & & & & & 77.00 & 73.73 & 61.50 & 54.73 & 60.95 & 57.64 \\ ✓ & ✓ & ✓ & & & 83.97 & 80.40 & 69.46 & 56.76 & 63.58 & 60.35 \\ ✓ & ✓ & ✓ & ✓ & & 84.17 & 80.72 & 70.02 & 57.13 & 64.06 & 60.72 \\ ✓ & ✓ & ✓ & & ✓ & 84.88 & 81.32 & 70.57 & 57.09 & 63.87 & 60.62 \\ ✓ & ✓ & ✓ & ✓ & ✓ & **84.90** & **81.47** & **70.85** & **58.19** & **65.08** & **61.83** \\   

Table 1: Ablation study on fusion query (\(\)) about each component on the performance of **Tri-SYSU-MM01** and **Tri-LLCM** datasets. **Rank-1** (%), **mAP**(%), and **mINP**(%) are reported.

(R-1), mean Average Precision (mAP), and mean Inverse Negative Penalty (mINP) . For fair comparison, we re-run YYDS on the proposed expanded datasets with the same image size: 288\(\)144.

**Performance on Tri-SYSU-MM01 Dataset** As shown in Table 2, with the enhancement of generated text, TVI-LFM greatly improves the performance of the VI-ReID backbone and outperforms all previous methods under 'All Search' and 'Indoor Search' conditions. Specifically, TVI-LFM achieves significant improvements in Rank-1, reaching 84.90% and 89.06% respectively, compared to the next best result of 77.78% by PartMix  in All Search and 83.20% by SAAI  in Indoor Search. Furthermore, in terms of mAP, TVI-LFM posts scores of 81.47% and 90.78%, which are substantial increases from the previous high scores of 77.03% and 88.01%, respectively.

**Performance on Tri-RegDB and Tri-LLCM Dataset** Table 3 outlines our method's performance on the two datasets. In the Tri-RegDB dataset, TVI-LFM obtains a Rank-1 of 91.38% and an mAP of 85.92%, higher than the prior top scores of 90.95% in Rank-1 and 84.22% in mAP by YYDS. In the Tri-LLCM dataset, our method leads with a Rank-1 of 58.19% and an mAP of 65.08%, surpassing the prior top scores of 58.13% in Rank-1 and 64.91% in mAP, both held by YYDS.

### Visualization

**Feature Distribution Visualization.** To explore the reason why our method is effective, we utilize t-SNE  2D feature space and visualize cosine distances of the intra-class and inter-class features on Tri-SYSU-MM01 dataset. From the (a) to (d) in Fig. 4, the t-SNE feature distribution shows that our method greatly enhances the ability of distinguishing features from different identities with text and reduces extreme outliers of the same identity and samples with too large cross-modal discrepancy. For the feature distance distribution shown in Fig. 4 (e-h), which corresponds to the 2D t-SNE  feature distribution, the inter-class and intra-class distance distributions are increasingly well separated, particularly noting that the excessive intra-class distance is also significantly reduced.

    &  &  &  &  \\   & & & R-1 & mAP & mINP & R-1 & mAP & mINP \\   Zero-Padding  & ICCV-17 & & 14.80 & 15.95 & - & 20.58 & 26.92 & - \\ HCML  & AAAI-18 & & 14.32 & 16.16 & - & 24.52 & 30.08 & - \\ cmGAN  & IJCAI-18 & & 26.97 & 27.80 & - & 31.63 & 42.19 & - \\ AlignGAN  & ICCV-19 & & 42.40 & 40.70 & - & 45.90 & 54.30 & - \\ AGW  & TPAMI-21 & & 47.50 & 47.65 & 35.30 & 54.17 & 62.97 & 59.23 \\ DDAG  & ECCV-20 & & 54.75 & 53.02 & 39.62 & 61.02 & 67.98 & 62.61 \\ CM-NAS  & ICCV-21 & \(I R\) & 61.99 & 60.02 & - & 67.01 & 72.95 & - \\ DART  & CVPR-22 & & 68.7 & 66.3 & - & 82.0 & 73.8 & - \\ CAJ  & ICCV-21 & & 69.88 & 66.89 & 53.61 & 76.26 & 80.37 & 76.79 \\ DEEN  & CVPR-23 & & 74.70 & 71.80 & - & 80.30 & 83.30 & - \\ SAAI  & ICCV-23 & & 75.90 & 77.03 & - & 83.20 & 88.01 & - \\ MSCLNet  & ECCV-22 & & 76.99 & 71.64 & - & 78.49 & 81.17 & - \\ SGIEL  & CVPR-23 & & 77.12 & 72.33 & - & 82.07 & 82.95 & - \\ PartMix  & CVPR-23 & & 77.78 & 74.62 & - & 81.52 & 84.38 & - \\  YYDS  & Arixu-24 & \(I+T R\) & 74.60 & 70.35 & 56.01 & 81.35 & 83.64 & 79.56 \\  VI-ReID Backbone & - & \(I R\) & 69.89 & 66.74 & 53.34 & 76.91 & 80.64 & 76.70 \\
**TVI-LFM** & - & \(I+T R\) & **84.90** & **81.47** & **70.85** & **89.06** & **90.78** & **88.39** \\   

Table 2: Comparison with the state-of-the-art methods on the proposed Tri-SYSU-MM01.

    &  &  &  &  \\   & & & R-1 & mAP & mINP & R-1 & mAP & mINP \\   DDAG  & ECCV-20 & & 68.06 & 61.80 & 48.62 & 40.3 & 48.4 & - \\ AGW  & TPAMI-21 & & 70.49 & 65.90 & 51.24 & 43.6 & 51.8 & - \\ CAJ  & ICCV-21 & \(I R\) & 84.8 & 77.8 & 61.56 & 48.8 & 56.6 & - \\ DART  & CVPR-22 & & 82.0 & 73.8 & - & 52.2 & 59.8 & - \\ MMN  & MM-21 & & 87.5 & 80.5 & - & 52.5 & 58.9 & - \\ DEEN  & CVPR-23 & & 89.5 & 83.4 & - & 54.9 & 62.9 & - \\  YYDS  & Arixu-24 & \(I+T R\) & 90.95 & 84.22 & 70.12 & 58.13 & 64.91 & 61.77 \\  VI-ReID Backbone & - & \(I R\) & 89.51 & 83.51 & 69.65 & 53.53 & 59.77 & 56.40 \\
**TVI-LFM** & - & \(I+T R\) & **91.38** & **85.92** & **72.73** & **58.19** & **65.08** & **61.83** \\   

Table 3: Comparison with the state-of-the-art methods on the proposed Tri-RegDB and Tri-LLCM.

## 5 Conclusion

To alleviate the absence of detailed color information in the infrared modality, this paper presents a VI-ReID framework driven by Large Foundation Models (TVI-LFM) to enrich the infrared representation with VLM-generated textual descriptions, which is a cross-modality retrieval approach bolstered by heterogeneous text descriptions. To enhance the infrared modality with text, MSC utilizes one off-the-shelf LLM to augment VLM-generated text descriptions. Then, IFS incorporates a pre-trained VLM to extract features from generated texts, and incrementally fine-tunes the text encoder to align generated texts and original visual modalities. To enhance the infrared modality with extracted text features, IFS leverages modality alignment capabilities of VLMs and VLM-generated feature-level filters to create fusion modality. This enables the text model to learn complementary features from the infrared modality, ensuring semantic structural consistency between the fusion modality and the visible modality. Furthermore, IFS introduces modality joint learning to align features of all modalities, maintaining a stable semantic representation of the overall pedestrian appearances for text features during complementary information learning. Additionally, MER leverages complementary strengths of query modalities, forming ensemble queries to further improve retrieval performance. Extensive experiments on three expanded VI-ReID datasets demonstrate the competitive performance of TVI-LFM, paving the way for the utilization of large foundation models in downstream multi-modal retrieval tasks.

**Limitations and future research.** While the proposed TVI-LFM shows promising performance, the retrieval accuracy hinges on text quality, and the performance on hard datasets, such as LLCM, still have rooms for improvement. High-quality text enhances retrieval accuracy by improving text-vision correspondences during training and providing precise information for infrared compensation during inference. Therefore, for future improvements, _1) more advanced generative models_; _2) image augmentations during generator fine-tuning_; _3) progressive generation strategies focusing on fine-grained attributes_, could be introduced to enhance text quality, thereby improving the accuracy of cross-modality retrieval bolstered by heterogeneous textual descriptions.

**Acknowledgements.** This work is partially supported by National Natural Science Foundation of China under Grant (62176188, 62361166629, 62225113), Key Research and Development Project of Hubei Province (2022BAD175), and Postdoctoral Fellowship Program of China Postdoctoral Science Foundation (GZC20241268). The numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of Wuhan University.

Figure 4: First row (a-d) shows the t-SNE feature distribution of the 20 randomly selected identities, triangles means infrared features (w/wo textual enhancement), circles means visible features. Different colors indicate different identities. Figures in the second row (e-h) represent the intra-class (blue) and inter-class (green) distances of infrared features (w/wo textual fusion) and visible features.