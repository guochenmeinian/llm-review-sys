# A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions

Hoonhee Cho KAIST

gnsgnsgml@kaist.ac.kr

&Taewoo Kim1 KAIST

intelpro@kaist.ac.kr

&Yuhwan Jeong KAIST

jeongyh98@kaist.ac.kr

&Kuk-Jin Yoon KAIST

kjyoon@kaist.ac.kr

Equal contribution.

###### Abstract

Multi-person pose estimation and tracking have been actively researched by the computer vision community due to their practical applicability. However, existing human pose estimation and tracking datasets have only been successful in typical scenarios, such as those without motion blur or with well-lit conditions. These RGB-based datasets are limited to learning under extreme motion blur situations or poor lighting conditions, making them inherently vulnerable to such scenarios. As a promising solution, bio-inspired event cameras exhibit robustness in extreme scenarios due to their high dynamic range and micro-second level temporal resolution. Therefore, in this paper, we introduce a new hybrid dataset encompassing both RGB and event data for human pose estimation and tracking in two extreme scenarios: low-light and motion blur environments. The proposed Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPT-XC) dataset covers cases of motion blur caused by dynamic objects and low-light conditions individually as well as both simultaneously. With EHPT-XC, we aim to inspire researchers to tackle pose estimation and tracking in extreme conditions by leveraging the advantageous of the event camera. Project pages are available at https://github.com/Chooonhee/EHPT-XC.

## 1 Introduction

Human pose estimation and tracking involve the identification and monitoring of human body parts or significant joints. This task holds paramount importance in comprehending human activities and analyzing movements across various domains, including rehabilitation, sports, augmented/virtual reality, autonomous driving. Consequently, numerous datasets [10; 33; 11; 20; 30] have been dedicated to studying human pose estimation and tracking for various applications. However, despite the dynamic nature of human activity, most datasets assume that the subjects are well-groomed in terms of motion and lighting conditions. Considering the reality of perceiving human motion, most individuals move in a dynamic fashion. Moreover, human activity occurs across various times of the day, exposing individuals to diverse lighting environments. This diversity is directly reflected in the cameras acquiring the data. Ultimately, AI models should strive to predict human body movements even in such varied environments.

To address this, we acquired human pose estimation and tracking dataset that tackle two extreme cases that can occasionally arise when capturing images with cameras. Firstly, we obtained dataincluding motion blur that may occur due to the movement of the subject or camera during the exposure time. While videos from professional sports, captured by experts using high-end equipment like gimbals, tend to be clean, videos taken by average users often exhibit blur caused by moving subjects. Additionally, when the camera attempts to track human objects, motion blur caused by the camera may also occur. Therefore, solving human pose estimation and tracking within motion blur is crucial. Secondly, we acquired data captured in low-light environments to perform human pose estimation and tracking under poor lighting conditions. The ability to analyze human actions at low-light condition has been of continuous interest [22; 41; 17], prompting us to capture data in conditions where very little light is present, allowing for scenarios people are barely visible. These two extreme cases are closely related, as increasing the exposure time of the camera to compensate for the low intensity in low-light environments can exacerbate motion blur. Addressing both cases simultaneously is highly practical and reasonable, as they often occur together and solving them together offers a comprehensive solution.

Performing accurate human pose estimation and tracking in these extreme conditions poses a significant challenge. Especially when both situations are present, relying solely on standard cameras may not provide sufficient information. Therefore, we augmented standard cameras with a auxiliary sensor called an event camera [12; 44], also known as a neuromorphic camera. Event cameras mimic the human eye by providing pixel-wise changes asynchronously in an on/off manner. These cameras possess high dynamic range and high temporal resolution, ensuring sufficient data even in low-light conditions and being immune to motion blur. Therefore, to tackle extreme conditions, we introduce the Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPT-XC) dataset and benchmark. To build the dataset, we constructed a multi-camera system to acquire high-resolution RGB-Event paired data, enabling us to freely adjust camera settings for extreme conditions. Additionally, we captured diverse motions and scenarios through experimental participants and manually labeled the acquired data accordingly. The contributions and unique aspects of EHPT-XC datasets are as follows:

* **Multi-human Pose Dataset with Neuromorphic Cameras.** While existing datasets utilizing RGB frames can be collected from various sources such as publicly available human-centric datasets and data shared on platforms like YouTube and the web, data collection using event cameras presents a challenge, particularly for human-centric datasets, as there are no readily available sources. We gathered human data directly using a multi-camera system, making EHPT-XC the _first multi-human pose dataset utilizing neuromorphic cameras_. Moreover, EHPT-XC provides track IDs, enabling its utilization for multi-object tracking. While datasets for event-based single-object tracking [48; 38] exist, EHPT-XC stands as the _pioneering dataset for multi-object tracking using event cameras_.
* **Real-captured Data in Extreme Conditions.** The EHPT-XC dataset aims to address scenarios characterized by low-light conditions and significant motion blur. The absence of multi-human pose estimation and tracking datasets in such conditions stems from the difficulty in directly acquiring data and annotating it due to the challenges posed by degradation conditions. To tackle this, we developed a multi-camera system consisting of a triplet camera configuration, where two cameras capture RGB frames and the remaining one comprises an event camera. One of the RGB cameras was configured to acquire data in low-light environments and/or motion blur by adjusting its settings, while the other was set to capture sharp image under normal lighting conditions.
* **Indoor/outdoor Environments and Various Scenarios.** The EHPT-XC dataset comprises data not only in general scenes but also in dynamic scenarios such as sports, encompassing a variety of indoor and outdoor environments. This further emphasizes the motivation behind these low-light and motion blur conditions. Additionally, we varied the number of people appearing in each sequence to enhance the versatility of the dataset.

## 2 Related Works

### Human Pose Estimation and Tracking Datasets in Low-light Conditions.

Several datasets [45; 23; 21; 40] have been proposed to facilitate perception in low-light environments. However, datasets related to human subjects are challenging to acquire and are not actively publicized due to privacy issues. As shown in Table 1, there are only a few datasets related to human pose estimation or tracking in low-light conditions, including ExLPose , WHPD , and M\({}^{3}\)FD . Acquiring data in such extreme conditions exacerbates the difficulty of the annotation process, resulting in an insufficient number of total images and annotations. For example, the MPII

[MISSING_PAGE_FAIL:3]

## 3 Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPT-XC) dataset

EHPT-XC encompasses RGB video frames from 158 diverse sequences, along with pixel-wise aligned and temporally synchronized event streams, and annotations containing 38K 2D keypoints and bounding boxes with track IDs. To ensure accurate annotation in extreme low-light and motion blur environments, we designed a triplet camera system. This system enabled the simultaneous acquisition of RGB frames degraded by low-light and/or blur alongside sharp RGB frames.

### Triplet Camera System

To address the difficulty in marking accurate keypoints of human joints in images with strong motion blur, where object boundaries and structures are hard to distinguish, as well as the challenge of annotating in low-light environments where people are barely visible, we devise an approach using an additional RGB camera serving as a reference for annotation alongside RGB and event pair cameras. This additional RGB camera captures sharp and well-lit images, unlike the one recording blurred and/or low-light images, and is used solely for annotation process.

**Camera system with beam splitters.** One of the issues when using multiple cameras to capture the same scene is that each camera has different camera coordinates and image planes, making pixel-wise alignment challenging. To address this, we utilize an optical device called a beam splitter to divide incoming light into two identical beams, allowing two cameras to capture the same scene. Specifically, as shown in Fig. 1, two RGB cameras (BFS-U3-16S2C-CS) and one event camera (Prophesee EVK4) are aligned two 50/50 mirror beam splitters, resulting in a minimal baseline. Our camera system aligns the axes of three cameras but there is still geometric misalignment due to the remaining baseline. To deal with this, we correct the residual mismatching using homography-based geometric alignment.

**Camera synchronization.** Even if multiple cameras are geometrically aligned, another issue arises from the fact that they can capture data at different time instances. To address this, precise time synchronization among the multi-camera setup is required, and we accomplished this by designing a micro-controller (ATmega) to serve as an external trigger for hardware-level synchronization of three different cameras. The event and two RGB cameras are connected to the micro-controller via a trigger cable, ensuring simultaneous transmission of signals. Recording software is then created using the provided C++ SDK of each camera product to control them by receiving signals from the microcontroller. Each camera synchronizes with the falling and rising edges of the trigger signals, allowing for control of the RGB camera's exposure time with synchronized signals. By using this

Figure 1: Our triplet camera system comprises two RGB cameras and one event camera, all aligned using two beam splitters. One RGB camera is set to capture sharp frames with short exposure time in a well-lit environment exclusively for annotation purposes, while the other RGB camera is configured to capture low-light images through aperture adjustments or the use of ND filters or extend exposure time for acquiring blurred frames. The event camera was adjusted to ensure the exact same amount of light enters as with the low-light camera by applying the same ND filter and aperture settings.

synchronization methods, we adjusted the exposure time: one camera used a short exposure time to capture a sharp image, while the other camera was set to an exposure time 16 times longer than that of the sharp image, resulting in both a blurred image and its corresponding sharp image.

### Data Collection

To conduct our human subject study, we locally recruited participants and obtained signed consent forms and privacy agreements regarding the data distribution reviewed by the institution before the experiment. Additionally, prior to data acquisition, we reiterated the research objectives, procedures, potential risks, and data distribution to participants, informing them of their ability to withdraw from the experiment at any point. A total of 61 male and 21 female participants agreed to participate in the experiment by signing the consent form. Natural and realistic scenarios were preselected and presented to the participants, who then performed these scenarios as instructed. We acquired data corresponding to these scenarios during the participants' performance. Due to the acquisition of data in diverse lighting conditions such as indoor and outdoor environments, we did not employ fixed camera settings. Especially for low-light data, we adjusted the aperture of the camera lens and the exposure time to reduce the incoming light in the camera shutters and adjusted the gain accordingly.

**Low-light/well-lit and indoor/outdoor distributions.** The EHPT-XC aims to capture humans in various environments, including challenging low-light conditions, which can pose difficulties for pose estimation. As depicted in Fig. 3, we have split the dataset to ensure a balanced distribution of indoor/outdoor and low-light/well-lit environments between the train and test sets. Specifically, we have acquired data in such a way that low-light environments constitute a substantial portion of the overall dataset.

### Data Annotation

For annotation, we utilized well-lit and sharp images as references, which were precisely geometrically and temporally aligned with the blurred and/or low-light condition images. As depicted in Fig. 2,

Figure 3: Scene distribution over light conditions (low-light/well-lit) and indoor/outdoor environments.

Figure 2: Visualization of sample data with pose annotations. The 1st row shows degraded RGB frames affected by motion blur and/or low-light conditions. The 2nd row displays event data captured in the same environment. The 3rd row consists of reference RGB frames for annotation, time-synchronized with the 1st row data and precisely aligned pixel-wise through a beam-splitter.

estimating accurate key points in degraded images proved to be extremely challenging for annotators, whereas it was straightforward in the reference well-lit and sharp images. For the high-quality annotation of the EHPT-XC dataset, we engaged five annotators with ample experience in the field of computer vision. Each annotator performed annotations for different sequences, and through a cross-checking process, we enhanced the overall quality of the annotations. Following the previous labeling format [19; 37], a total of 15,800 images were labeled with 14 human skeletal keypoints.

### Data Statistics

**Intensity distribution.** As shown in Fig. 4 (a), we calculate the average intensity of RGB frames in each scene. In fact, low-light environments have mean intensities distributed below 40, with occasional instances in indoor settings where sunlight is absent, resulting in intensities close to 40. However, upon actual inspection, these environments still provide sufficient visibility about humans. Hence, in Fig. 3, they are classified as well-lit. Low-light environments were categorized only when filters or aperture adjustments were deliberately applied to decrease the intensity. Furthermore, it can be observed that aside from the division into low-light and well-lit categories, the EHPT-XC dataset also exhibits a uniform distribution of overall intensity.

**Motion distribution.** To analyze the motion distribution of the dataset, we calculate pixel displacement for each keypoint between adjacent frames. As shown in Fig. 4 (b), we compare the motion distribution with the recent multi-human pose estimation and tracking dataset, PoseTrack21 . We measure the distribution for the 14 keypoints among the 17 keypoints in PoseTrack21 for comparison.

We consider the motion distribution of the dataset, and the motion distribution of the dataset is shown in Fig. 5. We compare the motion distribution with the recent multi-human pose estimation and tracking dataset, PoseTrack21 . We measure the distribution for the 14 keypoints among the 17 keypoints in PoseTrack21 for comparison.

Figure 4: Statistics of EHPT-XC dataset. (a) The distribution of average channel intensity for each sequence. (b) The distribution of motion distances for each keypoint category. We compare the distribution with a recent pose estimation and tracking dataset .

Figure 5: Motion magnitude distribution.

between two adjacent sharp frames. Blur can occur not only due to the movement of the target object (_i_.\(e\). humans), but also due to the ego motion of the camera. Therefore, to calculate blur intensity, we need to obtain the displacement of each pixel between sharp frames rather than just the keypoint displacement. To achieve this, we apply a pre-trained optical flow network  to sharp and well-lit reference images, calculating pixel displacement for all pixels. Figure 5 illustrates the motion magnitude of each image, calculated by averaging the displacement of all pixels. In the EHPT-XC dataset, it can be observed that the blur intensity is uniformly distributed across the entire dataset.

## 4 Baseline for multi-modal fusion

Various approaches [29; 49] have been proposed for fusing RGB and event modalities, and how the fusion between different modalities is performed significantly impacts the performance of the end task. As shown in Fig. 6, to obtain high-quality representations even from degraded inputs, we split each event into two segments at its midpoint of the exposure time and then fused them with RGB data. Given an event voxel, \(E\), we split the voxel into two parts, \(E^{0}\) and \(E^{1}\). Then, through separate convolution-based encoders, we extract features \((E^{0})\) and \((E^{1})\). The two event features and one RGB feature are fed into the proposed temporally-aggregated multi-modal fusion module, where they undergo a process of being merged into a single representation. We apply this aggregation process twice consecutively, resulting in a well-fused feature even with degraded information.

**Temporally-aggregated Multi-modal Fusion.** As shown in Fig. 7, all features are concatenated and merged into a single query through a \(1 1\) convolution. The merged feature then undergoes a self-attention operation via an attention block. As shown in the right of Fig. 7, in the attention block, we generate query, key, and value features, \(=W_{Q}()\), \(=W_{K}()\), \(=W_{V}()\), where \(W_{\{\}}\) is \(1 1\) convolution with a layer normalization. Utilizing these \(\), \(\), and \(\), we compute the attended feature as follows:

\[(,,)=(^{T}}{})\] (1)

where \(\) is the scaling factor of the attention matrix. To reduce computational costs, we calculate the cross-covariance matrix of the attention following the method in . As shown in the figure above, while self-attention is being performed, we also apply cross-attention to two event voxels, split along the temporal axis, to better fuse the modalities. All outputs are then passed through concatenation, a \(1 1\) convolution block, and MLP layers to generate the final feature representation.

Figure 6: A baseline approach for multi-modal fusion. To achieve effective multi-modal fusion, we propose splitting the event stream into two halves centered around the midpoint in time, enabling an effective fusion with the image. Enc-E shares weights for both \(E^{0}\) and \(E^{1}\), and the two temporally-aggregated multi-modal fusion modules also share weights with each other.

## 5 Evaluation and Benchmarks

### Multi-Person Pose Estimation

**Metrics.** Our evaluation strategy follows the well-established MSCOCO  and CrowdPose  metrics. We assess performance using average precision (AP) and average recall (AR). The Object Keypoint Similarity (OKS) serves a similar purpose to Intersection over Union (IoU) in the context of adopting Average Precision (AP) and Average Recall (AR) for keypoint detection. Our primary metrics are mAP and mAR, which are computed by averaging over multiple OKS values (.50: 05: 95).

**Benchmark.** We train and evaluate all methods using our images, event data, and annotations. We evaluate three main methodologies: one that utilizes only the RGB modality, another that relies solely on the event modality, and a third that integrates both RGB and event data in a multi-modality framework. To leverage the event modality, we utilized the widely used event representation, the voxel grid , setting the bin size to 10. For each modality, we adopt the same methods, but for the multi-modality (RGB+Event) approach, we apply three fusion methods: 1) Concatenation of input modalities. 2) Existing fusion method  between RGB and events. 3) Our newly proposed base fusion method (Fig. 6). We evaluate several recent state-of-the-art (SOTA) methods for multi-person pose estimation models. Specifically, we evaluate three recent bottom-up models: DEKR , CID , and HigherHRNet . We remove redundant poses from the stitched annotation set by employing non-maximum suppression (NMS) on the predicted bounding boxes.

Table 2 presents the results of pose estimation, categorized by modality and method. Among methods, DEKR  achieves the best performance in terms of mAP metric, while HigherHRNet  achieves the best performance in terms of mAR metric. When examining the resu

   Modality & Method & mAP@0.5:0.95 & mAP@0.5 & mAP@0.75 & mAR@0.5:0.95 & mAR@0.5 & mAR@0.75 \\   & HigherHRNet  & 22.7 & 31.8 & 23.3 & 67.0 & 85.8 & 70.0 \\  & DEKR  & 25.1 & 34.8 & 26.5 & 63.8 & 85.5 & 66.9 \\  & CID  & 24.0 & 33.1 & 24.5 & 65.9 & 87.7 & 67.9 \\   & HigherHRNet  & 32.1 & 37.8 & 33.6 & 83.8 & 95.5 & 86.8 \\  & DEKR  & 33.1 & 39.4 & 34.6 & 84.0 & 96.6 & 87.6 \\  & CID  & 31.1 & 38.1 & 32.6 & 85.6 & 97.0 & 88.8 \\   & HigherHRNet  & 33.8 & 39.0 & 34.3 & 84.4 & 94.6 & 85.9 \\  & DEKR  & 36.9 & 41.0 & 37.2 & 87.7 & 95.8 & **88.9** \\  & CID  & 33.7 & 39.3 & 34.5 & **88.0** & **98.2** & **90.2** \\   & HigherHRNet  & **34.3** & **39.7** & **34.8** & **86.0** & **97.0** & **87.6** \\  & DEKR  & **37.3** & **42.0** & **37.5** & **87.8** & **97.1** & **88.9** \\   & CID  & **34.5** & **40.9** & **36.0** & 84.7 & 98.0 & 88.4 \\   

Table 2: Multi-person pose estimation baselines evaluated on the EHPT-XC dataset.

Figure 7: The proposed temporally-aggregated multi-modal fusion. First, we combine the three features and perform self-attention. Then, we apply cross-attention between each event and the combined features. Finally, we aggregate the results using an MLP layer.

approaches combining RGB and event data, it's evident that performance improves across all metrics compared to using only the RGB approach. One particularly interesting observation is the significant improvement in overall performance, especially in the mAR metric, when using multi-modality approaches. This improvement suggests that the results obtained solely from RGB often fail to accurately estimate the positions of human keypoints due to motion blur and low-light conditions. In contrast, when RGB and event data are used together, even in scenarios with motion blur and low-light conditions, the positions of human joint keypoints are estimated more accurately.

Figure 8 provides a clearer representation of the performance difference between the multi-modality approaches using event data and the RGB-only method. Specifically, examining the 2nd result of the DEKR experiment, the scene presents an extremely challenging situation due to both low-light conditions and significant motion blur. Consequently, RGB-based methods tend to misinterpret the scene often leading to numerous false positive predictions where multiple humans are incorrectly identified. On the other hand, when incorporating event data, such scene misinterpretations are reduced, and keypoints are predicted accurately at the human positions. Similarly, in various challenging conditions such as severe blurring and low-light environments, multi-modal approaches demonstrate superior performance.

### Multi-Person Pose Tracking

**Metrics.** To evaluate the pose tracking results for multiple persons, we used common evaluation metrics frequently employed in multi-object tracking, namely MOTA, IDF1, FP, IDSW, and FN. For MOTA, it provides an overall evaluation of FP, FN, and IDSW metrics, making it the most important performance indicator in multi-object tracking. Additionally, IDF1 is a metric used to assess the performance of multi-object tracking systems, focusing on the accuracy of identity matching for objects over time. It evaluates how effectively the tracker maintains the correct identities of objects throughout the tracking sequence. FP are incorrect identifications of non-existent objects, IDSW are errors where the system changes the identity of a tracked object, and FN are failures to detect existing objects. We consider MOTA as the primary metric, as done in previous benchmarks .

**Benchmark.** We evaluate recent state-of-the-art methodologies, ByteTrack , Unitrack , and OC-SORT . These methods fall under the category of tracking-by-detection, using object detection results estimated through pose estimation methods to perform multi-object tracking. For

Figure 8: Qualitative results of multi-person pose estimation on the EHPT-XC dataset. The 1st and 2nd rows represent the results of DEKR , while the 3rd and 4th rows depict the results of HigherHRNet . The ground-truth keypoints are visualized on a sharp and clean reference image used for annotations. The 2nd row presents low-light conditions with significant level of motion blur.

pose estimation, we adopt DEKR , which demonstrated satisfactory performance in both RGB and RGB+event modality approaches. To utilize both RGB and event data together, we adopted the concatenation method.

Table 3 presents the results of multi-person pose tracking, applying various tracking methods for RGB and RGB+event modalities. OC-SORT was proposed to handle occlusions by associating over long time steps. However, we believe that the relatively poor performance of OC-SORT is attributed to the significant motion displacement in the EHPT-XC dataset, where distant frames often fail to provide valuable information for current frame tracking. ByteTrack demonstrates robust operation by incorporating even low-confidence predicted bounding boxes into tracklets, thus addressing common challenges in extreme environments. When comparing the results across modalities, an interesting observation is that in extreme cases with low-light and motion blur, using only RGB data leads to relatively low false positive (FP), but significantly higher false negative (FN). When only using RGB data, objects are frequently overlooked or undetected, resulting in a higher false negative rate. On the other hand, integrating event data reduces the occurrence of missed detection, although it may introduce problems with false positives. Even considering such cases, it's evident that incorporating event data leads to a significant improvement in the overall performance of tracking, as assessed by the MOTA metric.

## 6 Conclusion and Future Work

In this paper, we establish Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPT-XC) dataset, which is the first multi-human pose and tracking dataset with real captured extreme motion blur and low-light conditions with the real events and frames. To leverage the benefits of event cameras, which are well-suited for such extreme environments, we customize a triplet camera system to acquire multi-modal data. Thanks to the triplet camera system, we were able to acquire clean and sharp RGB frames that are pixel-wise aligned and time-synchronized, enabling precise annotation. We benchmark recent state-of-the-art multi-person pose estimation and tracking methods on the EHPT-XC dataset, showcasing the advantages, particularly in extreme environments, of utilizing the event modality. We expect that EHPT-XC will pave the way for further exploration in understanding human actions in extreme scenarios.

**Limitation and future work.** In this work, we propose a simple multi-modal fusion baselines. However, for tasks such as human pose estimation, more sophisticated modality fusion methods may be more effective. We plan to explore these fusion methods in future work.

**Social impact.** The proposed EHPT-XC dataset aims to estimate human pose even in RGB images degraded by motion blur and/or low-light conditions, and its main applications are targeted at rehabilitation and sports. In particular, since such degraded images do not contain personal identification information, inferring poses from these images can reduce emerging privacy issues. Moreover, the event camera we adopted as part of our multi-modal approach is well-known for its privacy-preserving characteristics, making it highly suitable for this purpose. However, we are aware that it can be misused for its intended purpose (_e.g._, pedestrian surveillance at night), and in case of misuse, we reserve our right to withdraw permission for users to use the dataset at any point.