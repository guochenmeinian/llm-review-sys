# Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems

Benjamin Coleman

Google DeepMind

colemanben@google.com

&Wang-Cheng Kang

Google DeepMind

wckang@google.com

&Matthew Fahrbach

Google Research

fahrbach@google.com

&Ruoxi Wang

Google DeepMind

ruoxi@google.com

&Lichan Hong

Google DeepMind

lichan@google.com

&Ed H. Chi

Google DeepMind

edchi@google.com

&Derek Zhiyuan Cheng

Google DeepMind

zcheng@google.com

Equal contribution.

###### Abstract

Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a \(d\)-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, _Feature Multiplexing_, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multiplexed representations lead to Pareto-optimal parameter-accuracy tradeoffs for three public benchmark datasets. Further, we propose a highly practical approach called _Unified Embedding_ with three major benefits: simplified feature configuration, strong adaptation to dynamic data distributions, and compatibility with modern hardware. Unified embedding gives significant improvements in offline and online metrics compared to highly competitive baselines across five web-scale search, ads, and recommender systems, where it serves billions of users across the world in industry-leading products.

## 1 Introduction

There have been many new and exciting breakthroughs across academia and industry towards large-scale models recently. The advent of larger datasets coupled with increasingly powerful accelerators has enabled machine learning (ML) researchers and practitioners to significantly scale up models. This scale has yielded surprising emergent capabilities, such as human-like conversation and reasoning (Bubeck et al., 2023; Wei et al., 2022). Interestingly, large-scale model architectures are headed towards two extreme scenarios. In natural language understanding, speech, and computer vision, the transformer (Vaswani et al., 2017) dominates the Pareto frontier. Most transformer parameters arelocated in the hidden layers, with very small embedding tables (e.g., ~0.63B embedding parameters in the 175B GPT-3 (Brown et al., 2020)). On the other hand, search, ads, and recommendation (SAR) systems require staggering scale for state-of-the-art performance (e.g., 12T parameters in Mudigere et al. (2022)), but here, most of the parameters are in the embedding tables. Typical downstream hidden layers are 3 to 4 orders of magnitude smaller than the embedding tables. Recent work blurs the boundary between these extremes--large transformers started adopting bigger embedding tables for new types of tokens, and new SAR models leverage deeper and ever more-complicated networks. Therefore, embedding learning is a core technique that we only expect to become more critical and relevant to large-scale models in the future.

Motivated by web-scale ML for SAR systems, we study the problem of learning embeddings for categorical (sparse) features.1 The weekend golf warriors in the ML community may appreciate our analogy between embeddings and the golf club grip (Hogan and Wind, 1985). In golf, the quality of your grip (feature embeddings) dictates the quality of your swing (model performance). Feature embeddings are the only point of contact between the data and the model, so it is critical to learn high-quality representations.

Feature embedding learning has been a highly active research area for nearly three decades (Rumelhart et al., 1986; Bengio et al., 2000; Mikolov et al., 2013; Papyan et al., 2020). In the linear model era, sparse features were represented as one-hot vectors. Linear sketches for dimension reduction became a popular way to avoid learning coefficients for each coordinate (Weinberger et al., 2009). In deep learning models, sparse features are not sketched but rather represented directly as embeddings. Learned feature embeddings carry a significant amount of semantic information, providing avenues to meaningful model interactions in the high-dimensional latent space.

In practiceThe simplest transformation is to represent each feature value by a row in an \(N d\) matrix (i.e., an _embedding table_). This approach is sufficient for many language tasks, but SAR features often have a massive vocabulary (e.g., tens of billions of product IDs in commerce) and highly dynamic power-law distributions. It is impractical to store and serve the full embedding table.

The _"hashing trick"_(Moody, 1988; Weinberger et al., 2009) provides a workaround by randomly assigning feature values to one of \(M\) rows using a hash function. _Hash embeddings_(**?**) are a variation on the hashing trick where each feature value is assigned to multiple rows in the table. The final embedding is a weighted sum of rows, similar to a learned form of two-choice hashing (Mitzenmacher, 2001). _Compositional embeddings_(Shi et al., 2020) look up the feature in several independent tables and construct the final embedding from the components by concatenation, addition, or element-wise product. _HashedNet_ embeddings (Chen et al., 2015) independently look up each dimension of the embedding in a flattened parameter space. _ROBE embeddings_(Desai et al., 2022) look up chunks, instead of single dimensions, to improve cache efficiency. Tsang and Ahle (2022) learn a sparse matrix to describe the row/dimension lookups, and _deep hash embeddings_(Kang et al., 2021) use a neural network to directly output the embedding.

In theoryThe analysis of embedding algorithms is heavily influenced by linear models and the work of Weinberger et al. (2009) on the hashing trick. This line of reasoning starts with the assumption that all inputs have a "true" representation in a high-dimensional space that preserves all of the properties needed for learning (e.g., the one-hot encoding). The next step is to perform linear dimension reduction to compress the ground-truth representation into a smaller embedding, while approximately preserving inner products and norms. The final step is to extend the inner product argument to models that only interact with their inputs through inner products and norms (e.g., kernel methods).

In this framework, a key opportunity for improving performance is to reduce the dimension-reduction error. Early methods were based on the Johnson-Lindenstrauss lemma and sparse linear projections (Achlioptas, 2003; Li et al., 2006; Weinberger et al., 2009). Since then, a substantial effort has gone into improving the hash functions (Dahlgaard et al., 2017) and tightening the theoretical analysis of feature hashing (Dasgupta and Gupta, 2003; Freksen et al., 2018). Learned embeddings are now state-of-the-art, but the focus on dimension reduction remains: most recent algorithms are motivated by dimension reduction arguments. Previous attempts to understand hash embeddings do not study the full give-and-take relationship between feature encoding and the downstream learning problem. In this work, we carry the analysis forward to better understand learned embeddings.

ContributionsWe present an embedding learning framework called _Feature Multiplexing_, where multiple features share one representation space. Our theoretical analysis and experimental studies on public benchmark datasets show that Feature Multiplexing is theoretically sound and generalizable to many popular embedding methods. We also propose a highly-efficient multiplexed approach called _Unified Embedding_, which is battle-tested and serves billions of users in industry-leading products via various web-scale search, ads, and recommendation models.

1. **Feature multiplexing**: We propose the Feature Multiplexing framework, where a single representation space (embedding table) is used to support different sparse features. All of the SOTA benchmark embedding algorithms in our experiments show better results when adopting Feature Multiplexing. Furthermore, the Unified Embedding approach (i.e., multiplexing + hashing trick) outperforms many SOTA embedding methods that are noticeably more complicated to implement and impractical for low-latency serving systems on modern day hardware.
2. **Analysis beyond collision counts**: We analyze hash embeddings in the standard framework as well as in a supervised learning setting to capture interactions between feature embedding and downstream learning. Our analysis gives insights and testable predictions about unified embeddings that cannot be obtained by dimension reduction arguments alone.
3. **Battle-tested approach and industrial experiments at scale**: Multi-size Unified Embedding has been launched in over a dozen web-scale SAR systems, significantly improving user engagement metrics and key business metrics. In addition, Unified Embedding presents major practical benefits: simplified feature configuration, strong adaptation to dynamic data distributions, and compatibility with ML accelerators. In Section 5.2, we share our insights, challenges, and practical lessons on adopting Unified Embedding.

## 2 Preliminaries

SAR applications often present supervised learning problems over categorical (sparse) features that represent properties of users, queries, and items. Each categorical feature consists of one or more _feature values_\(v\) drawn from the _vocabulary_ of the feature \(V\). For a concrete example, consider the classic click-through rate (CTR) prediction task for ads systems, where we want to predict the probability of a user clicking on an advertisement. The "ad_id" feature may take on hundreds of billions of possible values--one for each unique ad served by the platform--while "site_id" describes the website where that ad was shown (a.k.a., the publisher).

We embed the values of each feature into a space that is more suitable for modeling, e.g., \(^{d}\) or the unit \((d-1)\)-sphere. Given a vocabulary of \(N\) tokens \(V=\{v_{1},v_{2},,v_{N}\}\), we learn a transformation that maps \(v V\) to an embedding vector \(^{d}\). After repeating this process for each categorical feature, we get a set of embeddings that are then concatenated and fed as input to a neural network (Covington et al., 2016; Anil et al., 2022).

Problem statementWe are given a dataset \(D=\{(_{1},y_{1}),(_{2},y_{2}),,(_{|D|},y_{|D|})\}\) of examples with labels. The examples consist of values from \(T\) different categorical features with vocabularies \(\{V_{1},V_{2},,V_{T}\}\), where feature \(t\) has vocabulary \(V_{t}\). Unless otherwise stated, we do not consider multivalent features and instead assume that each example \(\) has one value for each categorical feature, i.e., \(=[v_{1},v_{2},,v_{T}]\) where \(v_{i} V_{i}\). However, this is for notational convenience only--the techniques in this paper extend to missing and multivalent feature values well.

Formally, the embedding table is a matrix \(^{M d}\) and the embedding function \(g(;)\) transforms the categorical feature values into embeddings. We let \(h(v):V[M]\) be a 2-universal hash function (Vadhan et al., 2012) that assigns a feature value to a row index of the embedding table. We also consider a model function \(f(;)\) that transforms the concatenated embeddings into a model prediction, which is penalized by a loss \(\). Using this decomposition, we define the joint feature learning problem as follows:

\[*{arg\;min}_{,}\;_{D}(,),_{D}(,)= _{(,y) D}(f(g(;);),y).\] (1)

We use independent hash functions \(h_{t}(v)\) for each feature \(t[T]\) so that tokens appearing in multiple vocabularies have the chance to get mapped to different locations, depending on the feature. We use the notation \(_{m}\) to denote the \(m\)-th row of \(\), which means \(_{h(u)}\) is the embedding of \(u\). Lastly, we let \(_{u,v}\) be the indicator variable that denotes a hash collision between \(u\) and \(v\), i.e., \(h(u)=h(v)\).

## 3 Feature Multiplexing: The Inter-Feature Hashing Trick

It is standard practice to use a different hash table for each categorical feature. There are a few approaches that deviate from this common practice--for example, HashedNet and ROBE-Z models use parameter sharing across all features, depending on the implementation. However, conventional wisdom suggests that each categorical vocabulary benefits from having an independent representation, so shared representations are not well-studied.

Parameter reuseWe present a _Feature Multiplexing_ framework that only uses one embedding table to represent all of the feature vocabularies. In contrast, typical models in SAR systems have hundreds of embedding tables, each representing one input feature. Multiplexed embeddings allow models to reuse parameters and lookup operations, improving efficiency in terms of both space and time. This is critical for models that must fit into limited GPU/TPU memory or satisfy latency and serving cost constraints. In principle, any feature embedding scheme (e.g., hashing trick, multihash, compositional, or ROBE-Z) can be used as the shared representation for feature multiplexing. In Section5, we experiment with multiplexed versions of six different embedding algorithms. Here, we focus on a multi-size (multi-probe) version of the multiplexed feature hashing trick (Figure1), which we call _Unified Embedding_. Unified Embedding presents unique practical advantages that we describe in Section5.2.

Tunable embedding widthBy using a unified table, we make the architectural assumption that all features share the same embedding dimension. However, this is often sub-optimal in practice since independent tuning of individual embedding dimensions can significantly improve model performance. For use-cases where different features require different embedding dimensions, we concatenate the results of a variable number of table lookups to obtain the final embedding. While this limits the output dimension to multiples of the table dimension, this is not a limiting constraint (and is needed by other methods such as ROBE-Z and product quantization).

Shared vocabularyIt is common for categorical feature vocabularies to share tokens. For example, two features that represent character bigrams of different text fields (e.g., query and document) might have substantial overlap. Our default approach is to use a different hash function for each feature by using a different hash seed for each feature. However, in cases where many semantically-similar features are present, the performance may slightly improve by using the same hash function.

## 4 Analysis of Feature Multiplexing

In this section, we present theoretical arguments about feature multiplexing applied to the "multiplexed feature hashing trick" setting. We start with a brief analysis in the dimension-reduction framework, suggesting that multiplexing can optimally load balance feature vocabularies across buckets in the table. This classic framework, however, cannot be used to distinguish between collisions that happen within the vocabulary of a feature (i.e., _intra-feature_) and those occurring across features (i.e., _inter-feature_).

Figure 1: Embedding methods for two categorical features. We highlight the lookup process for the first value \(v_{1}\) of each feature. Hash tables randomly share representations within each feature, while Unified Embedding shares representations across features. To implement Unified Embedding with different dimensions (multi-size or variable-length), we perform multiple lookups and concatenate the results.

To understand these interactions, we also need to consider the learning task and model training. Thus, we analyze the gradient updates applied to each bucket under a simplified but representative model--binary logistic regression with learnable feature embeddings for CTR prediction. Our analysis demonstrates a novel relationship between the embedding parameters and model parameters during gradient descent. Specifically, we find that the effect of inter-feature collisions can be mitigated if the model projects different features using orthogonal weight vectors. In Section 4.2, we empirically verify this by training models on public benchmark on click-through prediction (Criteo) and finding that the weight vectors do, in fact, orthogonalize. We defer the proofs and detailed derivations in this section to Appendix A.

### Parameter Efficiency in The Dimension-Reduction Framework

In the feature hashing problem, we are given two vectors \(,^{N}\) that we project using a linear map \(:^{N}^{M}\) given by the sparse \(\{ 1\}\) matrix in Weinberger et al. (2009). The goal is for \(\) to minimally distort inner product, i.e., \((),(), \). Since the estimation error directly depends on the variance, it is sufficient to compare hashing schemes based on the moments of their inner product estimators. We examine the variance of the hashing trick and multiplexing trick.

To analyze multiple features at once in this framework, we consider the concatenations \(=[_{1},_{2}]\) and \(=[_{1},_{2}]\). The components \(_{1}\{0,1\}^{N_{1}}\) and \(_{2}\{0,1\}^{N_{2}}\) are the one-hot encodings (or bag-of-words) for the two vocabularies \(V_{1}\) and \(V_{2}\). The standard hashing trick approximates \(,\) by independently projecting \(_{1}\) to dimension \(M_{1}\) and \(_{2}\) to dimension \(M_{2}\). The multiplexed version uses a single projection, but into dimension \(M_{1}+M_{2}\). Now, we formalize the hashing trick of Weinberger et al. (2009), but with slightly revised notation. Given a set of tokens from the vocabulary, the embedding is the signed sum of value counts within each hash bucket.

**Definition 4.1**.: Let \(h:V\{1,2,,M\}\) be a 2-universal hash function and \(:V\{-1,+1\}\) be a sign hash function. The function \(_{h,}:2^{V}^{M}\) is defined as \(_{h,}(W)=_{w W}(w)_{h(w)}\), where \(_{i}\) is the \(i\)-th unit basis vector in \(^{M}\).

The following result compares multiplexed and standard embeddings in the dimension reduction framework. Note that the randomness is over the choice of hash functions \(h(v)\) and \((v)\). This result also directly extends to concatenating \(T\) feature embeddings.

**Proposition 4.2**.: _For any \(_{1},_{1}\{0,1\}^{N_{1}}\) and \(_{2},_{2}\{0,1\}^{N_{2}}\), let \(=[_{1},_{2}]\), \(=[_{1},_{2}]\) denote their concatenations. Let \(_{U}\), \(_{H}\), \(_{U}^{2}\), and \(_{H}^{2}\) be the mean and variance of \(_{h,}(),_{h,}()\) for multiplexed and hash encodings, respectively. Then, \(_{U}=_{H}=,\) and_

\[_{U}^{2} =\|_{2}^{2}\|\|_{2}^{2}+ ,^{2}-2,}{M_{1}+ M_{2}},\] \[_{H}^{2} =\|_{2}^{2}\|\|_{1}^{2}+ _{1},_{1}^{2}-2_{1},_{1} }{M_{1}}+_{2}\|_{2}^{2}\|_{2}\|_{2}^{2}+ _{2},_{2}^{2}-2_{2}, _{2}}{M_{2}}.\]

ObservationsProposition 4.2 shows that multiplexed embeddings can do a good job balancing hash collisions across the parameter space. Consider the problem of estimating the inner product \(,\). Suppose that \(\|_{1}\|_{2}^{2}=\|_{1}\|_{2}^{2}=k_{1}\) and \(\|_{2}\|_{2}^{2}=\|_{2}\|_{2}^{2}=k_{2}\), i.e., features 1 and 2 are multivalent with \(k_{1}\) and \(k_{2}\) values, and assume \(\) and \(\) are orthogonal, i.e., \(,=0\). Proposition 4.2 tells us that \(_{U}^{2}=(k_{1}+k_{2})^{2}/(M_{1}+M_{2})\) and \(_{H}^{2}=k_{1}^{2}/M_{1}+k_{2}^{2}/M_{2}\). The difference \(_{H}^{2}-_{U}^{2}\) factors as \((/M_{2}}k_{1}-/M_{1}}k_{2})^{2}/(M_{1}+M_{2})\), which is non-negative, implying \(_{U}^{2}_{H}^{2}\).

### Trajectories of Embeddings During SGD for Single-Layer Neural Networks

Intuition suggests that inter-feature collisions are less problematic than intra-feature collisions. For example, if \(x V_{1}\) collides with \(y V_{2}\) but no other values collide across vocabularies, then the embeddings from other tokens in \(V_{1}\) and \(V_{2}\) are free to rotate around the shared embedding for \(x,y\) to preserve their optimal similarity relationships.

We analyze a logistic regression model with trainable embeddings for binary classification (i.e., a single-layer neural network with hashed one-hot encodings as input). This corresponds to Eq. (1) where \(f(;)\) is the sigmoid function \(_{}()=1/(1+(-, ))\), \(\) is the binary cross-entropy loss, and \(y\{0,1\}\) (e.g., click or non-click). We concatenate the embeddings of each feature, which means the input to \(f(;)\) is \(=g(;)=[_{h_{1}(_{1})},_{h_{2 }(_{2})},,_{h_{T}(_{T})}]\), where \(_{h_{1}(_{t})}\) is the embedding for the \(t\)-th feature value in example \(\). We write the logistic regression weights as \(=[_{1},_{2},,_{T}]\) so that embedding \(_{h_{t}(_{t})}\) for feature \(t\) is projected via \(_{t}^{M}\). We illustrate this simple but representative model architecture in Figure 2.

The following analysis holds in general for \(T\) features, but we consider the problem for two categorical features with vocabularies \(V_{1}\) and \(V_{2}\) to simplify the presentation. We write the logistic regression objective as follows in Eq. (2), using \(C_{u,v,1}\) and \(C_{u,v,0}\) to denote the number of examples for which \(=[u,v]\) and \(y=1\) and \(y=0\), respectively. We present a full derivation in Appendix A.2.

\[_{D}(,)=-_{u V_{1}}_{v  V_{2}}C_{u,v,0}(^{}_{u,v})-(C_{u,v,0}+C_ {u,v,1})(1+(^{}_{u,v}))\] (2)

To understand the effect of hash collisions on training dynamics, we take the gradient with respect to \(_{h_{1}(u)}\), i.e., the embedding parameters that represent \(u V_{1}\). By studying the gradients for collisionless, hash, and multiplexed embeddings, we quantify the effects of intra-feature and inter-feature collisions. Interestingly, all three gradients can be written using the following three terms:

\[_{_{h_{1}(u)}}_{D}(,) =_{1}_{v V_{2}}C_{u,v,0}-(C_{u,v,0}+C_{u,v,1}) _{}(_{u,v})\] (3) \[+_{1}_{w V_{1}\\ w u}_{u,w}_{v V_{2}}C_{w,v,0}-(C_{w,v,0}+C _{w,v,1})_{}(_{u,v})\] (4) \[+_{2}_{v V_{2}}_{u,v}_{w  V_{1}}C_{w,v,0}-(C_{w,v,0}+C_{w,v,1})_{}(_{w,v }).\] (5)

For collisionless embeddings, component (3) is the full gradient. For hash embeddings, the gradient is the sum of components (3) and (4). The multiplexed embedding gradient is the sum of all three terms.

InsightsOur core observation is that embedding gradients can be decomposed into a collisionless (true) component (3), an intra-feature component (4), and an inter-feature component (5). The components from hash collisions act as a gradient bias. The intra-feature component is in the same direction as the true component, implying that intra-feature collisions are not resolvable by the model. This agrees with intuition since intra-feature hash collisions effectively act as value merging.

Inter-feature collisions, however, push the gradient in the direction of \(_{2}\), which creates an opportunity for model training to mitigate their effect. To see this, consider a situation where \(_{1}\) and \(_{2}\) are initialized to be orthogonal and do not change direction during training.2 During SGD, the embedding \(_{h_{1}(u)}\) is a linear combination of gradients over the training steps, which means \(_{h_{1}(u)}\) can be decomposed into a true and intra-feature component, in the \(_{1}\) direction, and an inter-feature component in the \(_{2}\) direction. Since \(_{1},_{2}=0\), the projection of \(_{1}^{}_{h_{1}(u)}\) effectively _eliminates the inter-feature component_.

Figure 2: Single-layer neural embedding model with per-feature weights \(_{t}\) (left). Mean embedding \(^{2}\)-norm (middle) and mean angle between all pairs of weight vectors \(_{t_{1}},_{t_{2}}\) (right) as a function of table size for Criteo across all 26 categorical features. Note that the horizontal axes are in log scale.

Empirical studyThese observations allow us to form two testable hypotheses. First, we expect the projection vectors \([_{1},_{2},,_{T}]\) to orthogonalize since this minimizes the effect of inter-feature collisions (and improves the loss). The orthogonalization effect should be stronger for embedding tables with fewer hash buckets since that is when inter-feature collisions are most prevalent. Second, intra-feature and inter-feature gradient contributions add approximately \(O(N/M)\) for each of their dimensions to the collisionless gradient since \([_{u,v}]=1/M\). Therefore, we expect the squared embedding norms to scale roughly as \(O(N/M)\).

To investigate these hypotheses, we train a single-layer neural network on the categorical features of the Criteo click-through prediction task. We initialize \(=[_{1},_{2},,_{T}]\) with each \(_{t}\) component in the same direction (i.e., the worst-case) to see if multiplexing encourages the weights to orthogonalize. We plot the results in Figure 2, which support our hypotheses.

### Why Can Features Share a Single Table?

Returning to our parameter efficiency argument of Section 4.1, we can finally explain the full benefits of feature multiplexing. The dimension reduction argument shows that, given a parameter budget, the multiplexed embedding has the same overall number of hash collisions as a well-tuned hash embedding. Our gradient analysis, however, shows that not all collisions are equally problematic. Inter-feature collisions can be mitigated by a single-layer neural network because different features are processed by different model parameters. While our theoretical results are limited to single-layer neural networks, we expect deeper and more complicated network architectures to exhibit analogs of weight orthogonalization due to their relative overparameterization. However, we believe this is still compelling evidence that shared embedding tables work by load balancing unrecoverable collisions across a larger parameter space and effectively use one massive table for each feature.

## 5 Experiments

Finally, we present experimental results on three public benchmark datasets, followed by anonymized, aggregate results from our industrial deployments across multiple web-scale systems. We give more details in Appendix B, including full parameter-accuracy tradeoffs, additional experiments, and the neural network architectures used.

### Experiments with Public Benchmark Datasets

DatasetsCriteo is an online advertisement dataset with ~45 million examples (7 days of data). Each example corresponds to a user interaction (click/no-click decision) and has 39 corresponding features. 26 of these features are categorical, and 13 are real-valued continuous variables. We only embed the categorical features. Avazu is a similar advertisement click dataset but with ~36 million examples (11 days of data). Each example here contains 23 categorical features and no continuous features. Movielens is a traditional user-item collaborative filtering problem, which we convert to a binary prediction task by assigning "1"s to all examples with rating \( 3\), and "0"s to the rest. For more details, see the benchmarks in Zhu et al. (2022) for Criteo and Avazu and the review by Harper and Konstan (2015) for Movielens. For Criteo and Movielens, we apply the same continuous feature transformations, train-test split, and other preprocessing steps in Wang et al. (2021). For Avazu, we use the train-test split and preprocessing steps in Song et al. (2019).

Experiment designTo find the Pareto frontier of the memory-AUC curve (Figure 3), we train a neural network with different embedding representation methods at 16 logarithmically-spaced memory budgets. Models can overfit on the second epoch, but compressed embeddings sometimes require multiple epochs. To showcase the best performance, we follow the guidelines of Tsang and Ahle (2022) and report the test performance of the best model found over three epochs. As baselines, we implement collisionless embeddings, the feature hashing trick, hash embeddings, HashedNets, ROBE-Z, compositional embeddings with element-wise product (QR), and compositional embeddings with concatenation (i.e., product quantization (PQ)).

For the multiplexed version of an existing embedding scheme, we use a single instance of the scheme to index all of the features. For example, we look up all feature values in a single multihash table, rather than using a separately-tuned multihash table for each feature. This is equivalent to salting the vocabulary of each feature with the feature ID and merging the salted vocabularies into a single massive vocabulary (which is then embedded as usual). We apply this procedure to each baseline to obtain a new multiplexed algorithm, reported below the horizontal bar in Table 1.

Implementation and hyperparametersAll methods are implemented in Tensorflow 2.0 using the TensorFlow Recommenders (TFRS) framework.3 For a fair comparison, all models are identical except for the embedding component. Following other studies (Naumov et al., 2019; Wang et al., 2021), we use the same embedding dimension for all features (\(d\{39,32,30\}\) for Criteo, Avazu, and Movielens, respectively). This is followed by a stack of 1-2 DCN layers and 1-2 DNN feed-forward layers--we provide a full description of the architecture in Appendix B. We run a grid search to tune all embedding algorithm hyperparameters and conduct five runs per combination. When allocating embedding parameters to features, we size tables based on the cardinality of their vocabulary (e.g., a feature with 10% of the total vocabulary receives 10% of the memory budget).

ResultsTable 1 shows the results for the academic datasets. We give three points on the parameter-accuracy tradeoff and highlight the Pareto-optimal choices, but present the full tradeoff (including standard deviations across runs) in Appendix B. Figure 3 shows the Pareto frontier of the parameter-accuracy tradeoff for all multiplexed and non-multiplexed methods. We observe that feature multiplexing is Pareto-optimal. Interestingly, the multiplexed hashing trick outperforms several embedding techniques (e.g., ROBE-Z) that were previously SOTA.

Collisionless embeddings are included in our benchmark evaluation only to provide an optimistic headroom reference point. Collisionless tables are often impossible to deploy in industrial recommendation systems since feature vocabularies can be massive and change dynamically due to churn. Multiplexing provides a significant improvement to methods that are fast, implementable, and orders of magnitude cheaper to train and serve than collisionless embeddings.

For significance tests comparing multiplexed methods to their corresponding non-multiplexed baseline (rather than the strongest baseline), see Appendix B. Appendix B also contains additional details about Criteo and Avazu, including an explanation for the collisionless-hash embedding performance gap on Criteo.

### Industrial Deployments of Unified Embedding in Web-Scale Systems

Among the proposed approaches, Unified Embedding (multi-probe hashing trick + multiplexing) is uniquely positioned to address the infrastructure and latency constraints associated with training and serving web-scale systems. We adopt Unified Embedding in practice, using variable row lookups for each feature (typically 1-6) with concatenation to achieve the desired embedding width for our features. It should be noted that this process yields a similar algorithm to Multiplex PQ--one of the top performers from Table 1. We discuss how to adopt Unified Embedding in production SAR systems, where it has already served billions of users through launches in over a dozen models. We also present an experimental study of Unified Embedding in offline and online A/B experiments.

    &  &  &  \\ Method & 25MB & 12.5MB & 2.5MB & 32.4MB & 3.24MB & 324kB & 1.6MB & 791kB & 158kB \\  Collisionless & 80.70 & – & – & 77.35 & – & – & 88.72 & – & – \\  Hashing Trick & 80.21 & 79.98 & 79.44 & 77.24 & 76.71 & 75.10 & 85.37 & 83.00 & 77.07 \\ Hash Embedding & 79.82 & 80.34 & 80.40 & 77.29 & 76.89 & 76.86 & 88.28 & 87.71 & 86.05 \\ HashedNet & 80.54 & 80.50 & 80.42 & 77.30 & 77.19 & 76.89 & 87.92 & 87.92 & 87.31 \\ ROBE-Z Embedding & 80.55 & 80.52 & 80.41 & 77.32 & 77.18 & 76.88 & 88.05 & 87.92 & 87.40 \\ PQ Embedding & 80.29 & 80.25 & 79.82 & **77.39** & 77.11 & 76.48 & 88.49 & 88.38 & 87.32 \\ QR Embedding & 80.29 & 80.21 & 79.74 & 76.98 & 76.61 & 75.86 & 88.49 & 88.63 & 88.22 \\  Multiplex Hash Trick & 80.43 & 80.47 & 80.49\({}^{**}\) & 77.35 & 77.18 & 76.86 & 87.74 & 86.93 & 82.00 \\ Multiplex HashEmb & 80.52 & 80.42 & 80.31 & 77.28 & 77.16 & 76.82 & 88.79\({}^{**}\) & **88.74\({}^{*}\)** & 87.66 \\ Multiplex HashedNet & 80.53 & 80.55 & 80.48\({}^{**}\) & 77.30 & 77.23\({}^{*}\) & 76.94\({}^{**}\) & 87.95 & 87.93 & 87.57 \\ Multiplex ROBE-Z & **80.57** & **80.56** & 80.50\({}^{**}\) & 77.32 & 77.24\({}^{**}\) & 76.94\({}^{**}\) & 88.00 & 88.03 & 87.62 \\ Multiplex PQ & 80.43 & 80.49 & **80.54\({}^{**}\)** & **77.39** & **77.29\({}^{**}\)** & **76.96\({}^{**}\)** & 88.81\({}^{**}\) & 88.69\({}^{*}\) & **88.09** \\ Multiplex QR & 80.22 & 80.28 & 80.48\({}^{**}\) & 77.29 & 77.16 & 76.90 & **88.83\({}^{**}\) & 88.53 & **88.27\({}^{**}\)** \\   

Table 1: AUC of embedding methods on click-through datasets. We give results for several points on the parameter-accuracy tradeoff, listed as sub-headings under the dataset that describe the total embedding table size. Our multiplexed methods (listed below the bar) outperform the strongest baseline at 0.01\({}^{**}\) and 0.05\({}^{*}\) significance levels of Welch’s \(t\)-test.

Experiment and production setupTo test the robustness and generalization capability of Unified Embedding, we conduct offline and online experiments with five key models from different SAR domains, including commerce, apps, and short-form videos. These systems represent a diverse set of production model architectures powered by state-of-the-art techniques, including two-tower retrieval models with LogQ correction (Yi et al., 2019), DCN-V2 (Wang et al., 2021), and MMOE (Zhao et al., 2019). These models handle a wide range of prediction tasks, including candidate retrieval, click-through rate (pCTR), conversion rate (pCVR) prediction (Chapelle, 2014), and multi-task user engagement modeling.

All of these production models are continuously trained and updated in an online streaming fashion, with the embedding tables being the dominant component (often >99%) in terms of model parameters (Anil et al., 2022; Fahrbach et al., 2023). Many of the baseline embedding methods are simple but quite strong (e.g., feature hashing and multiple hashing), and have been tuned with AutoML and human heuristics (Anil et al., 2022). In fact, these baseline embedding learning methods have roughly stayed the same for the past five years, with various efforts failing to improve them through novel techniques.

Practical benefitsUnified Embedding tables offer solutions to several challenging and practical issues in large-scale embedding learning systems.

* **Simple configuration**: With standard embedding methods, we need to independently tune the table size and dimension for each feature. Hyperparameter tuning is an arduous process due to the large number of categorical features used in production (hundreds if not thousands). Unified Embedding tables are far simpler to configure: the total table size is calculated based on the available memory budget, and we only need to tune the dimension on a per-feature basis--easily accomplished via AutoML (Bender et al., 2020) or attention-based search methods (Yasuda et al., 2023; Axiotis and Yasuda, 2023). This yields roughly a 50% reduction in the number of hyperparameters.
* **Adapt to dynamic feature distributions**: In practice, the vocabulary size of each feature changes over time (new IDs enter the system on a daily basis, while stale items gradually vanish). For example, in a short-form video platform, we expect a highly dynamic video ID vocabulary with severe churn. In e-commerce, a significant number of new products are added during the holiday season compared to off-season. Thus, a fixed number of buckets for a given table can often lead to sub-optimal performance. Unified Embedding tables offer a large parameter space that is shared across all features, which can better accommodate fluctuations in feature distributions.
* **Hardware compatibility**: Machine learning accelerators and frameworks have co-evolved with algorithms to support common use-cases, and the "row embedding lookup" approach has been the standard technique for the past decade. While Unified Embedding tables are well-supported by the latest TPUs (Jouppi et al., 2023) and GPUs (Wei et al., 2022; Mudigere et al., 2022), many of the recent and novel embedding methods require memory access patterns that are not as compatible with ML accelerators.

Figure 3: Pareto frontier of multiplexed and original (non-multiplexed) methods. Top-left is better.

Offline and online resultsTable 2 displays the results of our offline and online experiments for Unified Embedding. It was applied to production models with vocabulary sizes spanning from 10M to 10B. We observe substantial improvements across all models. Note that the offline metrics and the key business metrics for different models can be different, and the gains are not directly comparable. For offline metrics, the models are evaluated using AUC (higher is better), Recall@1 (higher is better), and RMSE (lower is better). From these experiments, we found that feature multiplexing provides greater benefits when (i) the vocabulary sizes for features are larger, and (ii) the vocabularies are more dynamic (e.g., suffers from a high churn rate). For instance, an overwhelming number of new short-form videos are produced daily, creating a large and dynamic vocabulary.

## 6 Conclusion

In this work, we propose a highly generalizable _Feature Multiplexing_ framework that allows many different features to share the same embedding space. One of our crucial observations is that models can learn to mitigate the effects of inter-feature collisions, enabling a single embedding vector to represent feature values from different vocabularies. Multiplexed versions of SOTA embedding methods provide a Pareto-optimal tradeoff, and _Unified Embedding_ proves to be a particularly simple yet effective approach. We demonstrate that a Unified Embedding table performs significantly better in both offline and online experiments compared to highly-competitive baselines across multiple web-scale ML systems. We believe the interplay between feature multiplexing, dynamic vocabularies, power-law feature distributions, and DNNs provides interesting and important opportunities for future work.

LimitationsWhile we expect deeper and more complicated network architectures to exhibit similar behavior, our theoretical analysis is limited to single-layer neural networks. The Pareto frontier from Section 5.1 is based on models that lag behind the current SOTA (we use 1-2 DCN layers + 1-2 DNN layers), though we provide evidence that feature multiplexing is also effective for SOTA models in Table 2. Unified embeddings impose limitations on the overall model architecture, and all embedding dimensions must share a common multiple. Because of the latency involved with looking up more than 6 components, we are often limited to 5-6 discrete choices of embedding width. Finally, unified embeddings sacrifice the ability to aggressively tune hyperparameters on a per-feature basis in exchange for flexibility, ease-of-use, and simplified feature engineering.