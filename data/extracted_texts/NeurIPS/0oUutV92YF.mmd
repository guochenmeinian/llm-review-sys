# ProtGO: Function-Guided Protein Modeling for Unified Representation Learning

Bozhen Hu\({}^{1,2^{*}}\), Cheng Tan\({}^{2^{*}}\), Yongjie Xu\({}^{2}\), Zhangyang Gao\({}^{2}\), Jun Xia\({}^{2}\),

Lirong Wu\({}^{2}\), Stan Z. Li\({}^{2}\)

\({}^{1}\)Zhejiang University \({}^{2}\)Westlake University

{hubozhen, tancheng, stan.zq.li}@westlake.edu.cn

Equal contribution. \(\)Correspondence: {stan.zq.li}@westlake.edu.cn

###### Abstract

Protein representation learning is indispensable for various downstream applications of artificial intelligence for bio-medicine research, such as drug design and function prediction. However, achieving effective representation learning for proteins poses challenges due to the diversity of data modalities involved, including sequence, structure, and function annotations. Despite the impressive capabilities of large language models in biomedical text modelling, there remains a pressing need for a framework that seamlessly integrates these diverse modalities, particularly focusing on the three critical aspects of protein information: sequence, structure, and function. Moreover, addressing the inherent data scale differences among these modalities is essential. To tackle these challenges, we introduce ProtGO, a unified model that harnesses a teacher network equipped with a customized graph neural network (GNN) and a Gene Ontology (GO) encoder to learn hybrid embeddings. Notably, our approach eliminates the need for additional functions as input for the student network, which shares the same GNN module. Importantly, we utilize a domain adaptation method to facilitate distribution approximation for guiding the training of the teacher-student framework. This approach leverages distributions learned from latent representations to avoid the alignment of individual samples. Benchmark experiments highlight that ProtGO significantly outperforms state-of-the-art baselines, clearly demonstrating the advantages of the proposed unified framework.

## 1 Introduction

Proteins constitute the fundamental structural and functional components within cells and organisms and serve as indispensable biomolecules thereof. These biomolecules are composed of linear sequences of amino acids, linked together by peptide bonds, intricately folding into complex three-dimensional (3D) structures . Recent groundbreaking advancements, exemplified by AlphaFold models [2; 3], have revolutionized protein structure prediction, leveraging artificial intelligence techniques with unprecedented accuracy. Consequently, a significant scientific challenge emerges: unravelling the intricate relationships among a protein's sequence, structure, and function -- an endeavour crucial for understanding disease mechanisms . Protein science primarily encompasses three core types of information: sequence, structure, and function, as illustrated in Figure 1. There, Gene Ontology (GO) annotations offer a standardized framework for delineating gene and protein functions, covering molecular functions, cellular components, and biological processes. These annotations furnish a comprehensive grasp of protein functionality across multiple dimensions andare the purpose of a protein study. Designed to be species-agnostic, the GO vocabulary spans a diverse array of biological functions, rendering it applicable to various biological contexts and organisms1.

Protein representation learning emerges as a dynamic research domain, aiming to uncover underlying patterns within raw protein data, providing invaluable insights applicable across diverse downstream tasks . Recently, protein language models (LMs) have emerged as powerful tools for processing protein sequences, showcasing their capability to learn the certain 'grammar of life' from vast collections of protein sequences . Models such as ProtTrans , the ESM series [6; 8; 9; 10], and xTrimoPGLM  leverage transformer architectures and attention mechanisms to autonomously uncover intrinsic patterns, undergoing self-supervised pre-training on extensive datasets. In contrast to sequences, protein structures entail continuous 3D coordinate data , necessitating distinct modelling strategies. Graph neural network (GNN)-based models [13; 14] have been developed and adapted to represent protein 3D structures [15; 16]. For instance, GearNet  encodes both sequential and spatial features by facilitating message passing between nodes and edges across multiple types of protein graphs.

While protein LMs and GNNs have demonstrated impressive performance across various protein-related tasks, such as predicting protein mutational stability and Enzyme Commission (EC) numbers , most of these methods overlook the utilization of functional information [6; 7]. However, integrating functional annotations is crucial for enhancing model capabilities and unveiling the intrinsic relationships between protein sequences and functions [18; 19]. Recent research explores token-level protein knowledge through pre-training on biomedical texts, which contain sequential and biological information [19; 20]. Nonetheless, the disparity between the vast number of protein sequences and the limited availability of structures and annotations presents a significant challenge . For instance, UniParc contains over 500 million (M) sequences , whereas the Protein Data Bank (PDB) houses only about 190 thousand (K) structures , with approximately 5M triplets in ProteinKG25 , comprising around 600K proteins and 50K attribute terms. This discrepancy in scale hampers the translation of the success achieved in sequence modelling into structure and function modelling. Consequently, enhancing the integration and fusion of information from these three modalities poses a crucial and formidable challenge in protein science.

Considering the disparity in data categories and scales between protein sequences, structures, and functions, we introduce ProtGO, a comprehensive multimodal framework for protein representation learning. ProtGO leverages a teacher model to glean insights from triplets comprising sequences, structures, and functions, distilling this knowledge to guide the training of the student model. However, given that functional annotations are lacking for the vast majority of sequenced proteins [25; 26], such information may not always be available for the teacher model in downstream tasks. Consequently, we opt to train a sequence-structure student model, which can be readily applied to various downstream tasks, with the teacher model serving solely to provide functional knowledge to the student model. To facilitate knowledge transfer from teacher to student, we employ domain adaptation techniques to align the distributions of latent spaces between the teacher and student models. Specifically, we minimize the Kullback-Leibler (KL) divergence to mitigate distribution discrepancies between the teacher and student domains.

The contributions of this paper can be summarized: **1)** We propose ProtGO to integrate multimodal information about proteins, encompassing sequence, structure, and functions. The teacher-student framework enables the learning of unified representations suitable for diverse downstream tasks where a protein model is needed. **2)** We pioneer the adaptation of knowledge distillation methods to connect protein teacher-student networks, infusing functional information into student representations through distribution approximation and domain adaptation. **3)** We validate ProtGO by outperforming prevailing protein representation methods across a range of tasks, including protein fold prediction, enzyme reaction classification, GO term prediction, and EC number prediction.

Figure 1: Protein sequence, structure, and function.

Related Work

### Representation Learning for Protein

Unimodal Protein Representation Learning.There are two primary categories of methods in unimodal protein representation learning: sequence-based and structure-based approaches. Sequence-based methods aim to derive representations directly from amino acid sequences , with notable efforts focused on enhancing model sizes or scaling datasets [27; 28; 29; 30; 7; 10]. For instance, Chen et al. proposed xTrimoPGLM, a unified model capable of learning from protein sequences to address both protein understanding and generation tasks concurrently, boosting a staggering 100 billion (B) parameters and 1 trillion (T) training tokens. In addition to sequence-based approaches, structure-based encoders have emerged to leverage the 3D structural information of proteins. For example, IEConv  accommodates the inherent inductive bias in protein structure modelling by introducing a graph convolution layer that integrates intrinsic and extrinsic distances between nodes. ProNet  provides geometric representations across multiple levels of structure granularity.

Multimodal Protein Representation Learning.In the realm of protein multimodality learning, methodologies such as GearNet  have been devised to concurrently exploit both sequence and structural information. GearNet represents sequential and geometric features as graph node and edge features, employing a message-passing mechanism to encode them [31; 33]. CDConv  introduces continuous-discrete convolution to model sequential and geometric features. Acknowledging the SE(3)-equivariant properties in protein structures, equivariant and invariant features are tailored as model inputs [33; 34]. Furthermore, the integration of factual biological knowledge has demonstrated enhancements in pre-trained LMs on protein sequences . KeAP  and ProtST  train biomedical LMs using masked language modelling  as the pretext task. Particularly noteworthy, MASSA  initially derives sequence-structure embeddings from existing pre-trained models [10; 33], subsequently aligning them globally with GO embeddings using five pre-training objectives.

### Knowledge Distillation

Knowledge distillation involves the process of transferring knowledge from a larger teacher model to a smaller student model [36; 37]. Significant advancements have been made in graph-based knowledge distillation, leading to the development of various methodologies [38; 39]. RDD  mandates the student model to faithfully replicate the complete node embeddings of the teacher, ensuring the transfer of more informative knowledge. Another notable approach, GraphAKD , employs adversarial learning to distill node representations from the teacher to the student. This method effectively distills knowledge from both local and global perspectives, exhibiting superior performance compared to earlier graph distillation techniques .

### Domain Adaptation

Domain adaptation aims to develop a model from labelled data in a source domain that can be effectively applied to a target domain by minimizing dissimilarities between their distributions [43; 44; 45]. Methods for distribution alignment focus on reducing disparities in both marginal and conditional representation distributions between the source and target domains [46; 47]. Adversarial learning techniques have demonstrated remarkable effectiveness in mitigating the discrepancy between source and target domains [48; 49; 50]. Semi-supervised domain adaptation strategies aim to minimize the source-target gap with limited labelled target data [51; 52; 53; 54]. In our work, we apply domain adaptation methods to align the distributions of representations learned by teacher and student networks to avoid dependence on individual samples.

## 3 Method

### Preliminaries

Here, we provide definitions and relevant notations for the problem, and background knowledge of the local coordinate system (LCS).

Problem Statement.Mathematically, we represent a protein graph as \(G=(,,X,E)\), where \(=\{v_{i}\}_{i=1,,n}\) and \(=\{_{ij}\}_{i,j=1,,n}\) denote the vertex and edge sets of \(n\) residues, respectively. We denote the position of a residue by the coordinate of \(_{}\), the collection by the position matrix is denoted as \(=\{P_{i}\}_{i=1,,n}\), where \(P_{i}^{3 1}\). The node and edge feature matrices are \(X=[_{i}]_{i=1,,n}\) and \(E=[_{ij}]_{i,j=1,,n}\), the feature vectors of node and edge are \(_{i}^{d_{1}}\) and \(_{ij}^{d_{2}}\), \(d_{1}\) and \(d_{2}\) are the initial feature dimensions. The set of \(k\) GO annotations for proteins is denoted as \(A=\{A_{i}\}_{i=1,,k}\), where \(A_{i}\{0,1\}\) is the indicator for annotation \(i\). The goal of protein graph representation learning is to find a low-dimensional embedding \(z\) for each protein.

There is a source domain \(S\) for the teacher model with the data distribution \(p_{S}(z_{S}|G_{S},A)\) in the latent space, and there is also a target domain \(T\) for the student model with the data distribution \(p_{T}(z_{T}|G_{T})\) in the latent space. \(z_{S},z_{T}\) are latent embeddings from the teacher and student networks for protein graphs \(G_{S}\) and \(G_{T}\).

Local Coordinate System.In order to avoid the usage of complicated SE(3)-equivariant models, the invariant and locally informative features are developed from the LCS , which is defined as:

\[_{i}=[_{i}_{i}_{i}_{i}]\] (1)

where \(_{i}=-P_{i-1}}{\|P_{i}-P_{i-1}\|},_{i}=_{i} -_{i+1}}{\|_{i}-_{i+1}\|},_{i}=_{i} _{i+1}}{\|_{i}_{i+1}\|}\), \(b_{i}\) is the negative bisector of the angle between the rays (\(P_{i-1}-P_{i}\)) and (\(P_{i+1}-P_{i}\)).

\[_{ij}=(\|P_{i}-P_{j}\|,_{i}^{T} -P_{j}}{\|P_{i}-P_{j}\|},_{i}^{T}_{j})\] (2)

Note that the edge feature vector \(_{ij}\) is the concatenation of the geometric features for protein 3D structures, including distance, direction, and orientation, where \(\|\|\) denotes the \(l^{2}\)-norm.

### Overall Framework

The overall framework of ProtGO is illustrated in Figure 2. It consists of two branches that train a teacher model and a student model via iterative knowledge distillation. Compared to the student, the teacher has an additional annotation encoder module comprised of several fully connected layers. This transforms GO annotations into functional embeddings, combined with sequence-structure embeddings from the GNNs to form the final knowledge-enhanced embeddings \(z_{S}\). Previous works have successfully utilized label-augmented techniques to enhance model training [56; 57]. This technique involves encoding labels and combining them with node attributes through concatenation or summation. By doing so, it improves feature representation and enables the model to effectively utilize valuable information from labels. Importantly, instead of directly minimizing the distances

Figure 2: The overall framework of ProtGO consists of two branches: a teacher model in the source domain and a student model in the target domain, connected by a knowledge distillation loss.

between sample-dependent embeddings, denoted as \(z_{S}\) and \(z_{T}\), we introduce a sample-independent method. This is accomplished by aligning the latent space of the student with that of the teacher, achieved through the approximation of distributions of embeddings from both networks. This distribution alignment approach avoids dependence on individual sample inputs. It is noteworthy that our primary objective is to derive comprehensive embeddings for the student model, with less emphasis on the training specifics of the teacher model. Consequently, the teacher model can be trained on multiple datasets, whereas the student does not need to have access to the same datasets when data modalities are not available.

Protein Graph Message Passing.A protein sequence consists of \(n\) residues, which are deemed as graph nodes. We concatenate the one-hot encoding of residue types with the physicochemical properties of each residue, namely, a steric parameter, hydrophobicity, volume, polarizability, isoelectric point, helix probability, and sheet probability , which are used as the graph node features \(_{i}\). These node features capture meaningful biochemical characteristics, enabling the model to learn which residues tend to be buried, exposed, tightly packed, etc. We define the sequential distance, \(l_{ij}=\|i-j\|\), and spatial distance \(d_{ij}=\|P_{i}-P_{j}\|\), where \(P_{i}\) is the 3D coordinate of the \(_{}\) atom of the \(i\)-th residue. An edge \(_{ij}\) exists if:

\[l_{ij}<l_{s} d_{ij}<r_{s}\] (3)

where \(l_{s},r_{s}\) are predefined radius thresholds, \(_{ij}\) consists of geometric features of the protein structure, defined in Eq. 2. We convolve node and edge features from sequence and structure simultaneously and formulate the message passing mechanism as:

\[_{i}^{(0)} =((_{i})),\] (4) \[_{i}^{(l)} =((_{v_{j}(v_{i})}W_{ij }_{j}^{(l-1)}),\] \[_{i}^{(l)} =_{i}^{(l)}+((_{i}^{(l)}))\]

This mechanism (as shown in Eq. 4) can fuse and update the node and edge features, which includes aggregation and update functions, where \(()\), \(()\), \(()\) represent fully connected, batch normalization, and dropout layers, \(()\) is the activation function LeakyReLU and \(W\) is the learnable convolutional kernel. \((v_{i})\) refers to the neighbors of node \(v_{i}\), and \(_{i}^{(l)}\) is the representation of node \(v_{i}\) in the \(l\)-th message passing layer. The node and edge features are processed together in Eq. 4. After message passing operations, a sequence pooling layer is applied to reduce the sequence length, providing a simple but effective way to aggregate key patterns. After average pooling, the residue number is halved; we expand the radius \(r_{s}\) to \(2r_{s}\) to update the edge conditions and perform the message passing and pooling operations again. These operations can make the GNNs cover more distant nodes gradually. The teacher and student models share the same GNN architecture to process protein sequences and structures. Finally, a global pooling layer is applied to obtain the graph-level protein embeddings, denoted as \(h_{S}\) and \(z_{T}\) for the teacher and student. Detailed model descriptions are presented in Appendix E.1.

Protein Domain Adaption.As shown in Figure 2, the teacher model consists of GNNs and an auxiliary annotation encoder, which is a multi-layer perceptron (MLP) that provides function-friendly protein representations. The annotations associated with \(G_{S}\) serve as the input for the annotation encoder, resulting in the extraction of feature vector \(h_{A}\). Therefore, we can combine \(h_{A}\) and the graph-level protein embeddings \(h_{S}\) learned from \(G_{S}\) together:

\[h_{A} =(A)\] (5) \[z_{S} =h_{A}+ h_{S}\]

where \(\) is a hyper-parameter, controlling the balance between the contribution of the annotation embeddings \(h_{A}\) and the protein embeddings \(h_{S}\) in the combined representations.

As depicted in Figure 2, the generated protein embeddings \(z_{S}\) contain sequence, structure, and function information, guiding the training of the student model. Since knowledge-enhanced embeddings \(z_{S}\) are intended to be aligned with \(z_{T}\), we obtain \(z_{S}\) from the entire protein and GO term datasets to avoid dependence on individuals. Then, we calculate the distributions of \(z_{S}\) and \(z_{T}\) to better capture the inherent uncertainty in the teacher's and student's latent spaces, in which the real distributions are sample-independent. The minibatch is adopted to approximate the quantities \(p_{S}(z_{S})\) and \(p_{T}(z_{T})\):

\[ p_{S}(z_{S})&=_{p_{S}(G_{S}, A)}[p(z_{S}|G_{S},A)]\\ &}_{i=1}^{B_{S}}p_{S}(z_{S}|G_{S}^{(i)},A^{(i)})\\ p_{T}(z_{T})&=_{p_{T}(G_{T})}[p_{T}(z_{ T}|G_{T})]\\ &}_{i=1}^{B_{S}}p_{T}(z_{T}|G_{T}^{(i)} )\] (6)

where \(B_{S}\) is the batch size. A Gaussian distribution \(\) is assumed for protein embeddings, which exhibit smoothness and symmetry properties that can reasonably mimic the expected continuity and unimodality of the embeddings aggregated over many residues. We employ the reparameterization trick  to sample the protein embeddings.

\[p_{S}(z_{S})=(_{S},_{S}^{2}); p_{T}(z_{T})=(_{T},_{T}^{2})\] (7)

where \(_{S},_{S}^{2}\) and \(_{T},_{T}^{2}\) are the mean and variance values of the embeddings for the teacher and student models, providing a summary of the distribution using first- and second-order statistics.

Proposition 2 in Appendix F shows that the conditional misalignment in the representation space is bounded by the conditional misalignment in the input space. We have:

\[_{}^{*}\,_{}\,+ {}C\] (8)

\[C=[p_{S}(z) p_{T}(z)]+_{ p_{S}(G)}[[p_{S}(y|G) p_{T}(y|G)] ]}\] (9)

where \(_{}^{*}\) is the ideal target domain loss, and \(_{}\) is the teacher's supervised loss, \(M\) is a bound, see Appendix F. \(_{p_{S}(G)}[[p_{S}(y|G) p_{T}( y|G)]]\) is often small and fixed (not dependent on the representation \(z\), and \(y\) is the function label). To reduce the generalization bound, we can focus on optimizing the marginal misalignment with a hyper-parameter \(\):

\[_{}+([p_{S}(z) p _{T}(z)])\] (10)

Eq. 10 can be used in an unsupervised way for the student to predict functions, which is near the ideal target domain loss. For the proposed framework, ProtGO (shown in Figure 2), we use the \(_{}\) to train the teacher model. When the student model has task labels, we adopt a hybrid loss \(\) to train the student model, where the \(_{kd}=[p_{S}(z)|p_{T}(z)]\) is to optimize the marginal misalignment between teacher and student models. Therefore, the final loss \(\) with a hyper-parameter \(\) for the student model is formulated as:

\[=_{}+_{kd}\] (11)

The objective function of the teacher model \(_{}\) is the cross entropy loss for protein graph classification. The hybrid loss for the student model has a cross entropy loss \(_{}\) for classification and a regularization loss \(_{kd}\) for domain-adapted knowledge distillation.

## 4 Experiments

### Training Details

The proposed multimodal knowledge distillation framework, ProtGO, undergoes an easy training process. A dataset comprising approximately 30,000 proteins, each associated with 2,752 GO annotations from the GO dataset, is utilized without further categorization into biological process (BP), molecular function (MF), and cellular component (CC) classes . These classes serve as input to the annotation encoder of the teacher model, yielding an overall \(_{}\) of 0.489 for the teacher model. Subsequently, the student model is trained. The optimization is performed using the Adam optimizer through the PyTorch library, and the performance metrics are computed as mean values over three initializations. Further details regarding experimental settings are available in Appendix E.2.

### Tasks and Baselines

Following the tasks in IEConv  and CDConv , we evaluate ProtGO on four protein tasks: protein fold classification, enzyme reaction classification, GO term prediction, and EC number prediction. Detailed task descriptions are presented in Appendix B. Dataset statistics are shown in Table 4 in the appendix.

Baselines.The proposed method is compared with existing protein representation learning methods, which are classified into three categories based on their inputs, which could be a sequence, 3D structure, or both sequence and structure. 1) Sequence-based encoders, including CNN , ResNet , LSTM  and Transformer . 2) Structure-based methods (GCN , GAT , 3DCNN_MQA  3) Sequence-structure based models, e.g., GVP , CRL , ProNet , GearNet , CDConv , etc. GearNet-IEConv and GearNetEdge-IEConv  add the IEConv  layer on GearNet.

### Results of Fold and Enzyme Reaction Classification

Table 1 presents a performance comparison for protein fold and enzyme reaction prediction across various methods, with results reported as average values. As depicted in the table, the proposed ProtGO consistently achieves the highest performance across all four test sets for both fold and reaction prediction tasks. The superiority of sequence-structure-based methods over sequence- or structure-only approaches is evident, underscoring the advantages of jointly modelling sequence and structure information. ProtGO achieves a remarkable improvement in accuracy of over \(6.7\%\) compared to prior techniques on the Fold test set, highlighting its efficacy in learning mappings between sequence, structure, and function. Furthermore, despite both CDConv and ProtGO employing sequence-structure convolution architectures, ProtGO demonstrates superior performance over the CDConv model. This observation suggests that the teacher-student training paradigm adopted in ProtGO facilitates the acquisition of enhanced protein embeddings by the student model.

    &  &  & Enzyme \\   & &  \\   & CNN  & 11.3 & 13.4 & 53.4 & 51.7 \\  & ResNet  & 10.1 & 7.21 & 23.5 & 24.1 \\  & LSTM  & 6.41 & 4.33 & 18.1 & 11.0 \\  & Transformer  & 9.22 & 8.81 & 40.4 & 26.6 \\   & GCN  & 16.8 & 21.3 & 82.8 & 67.3 \\  & GAT  & 12.4 & 16.5 & 72.7 & 55.6 \\  & 3DCNN\_MQA  & 31.6 & 45.4 & 92.5 & 72.2 \\   & GraphQA  & 23.7 & 32.5 & 84.4 & 60.8 \\  & GVP  & 16.0 & 22.5 & 83.8 & 65.5 \\  & ProNet-Amino Acid  & 51.5 & 69.9 & 99.0 & 86.0 \\  & ProNet-Backbone  & 52.7 & 70.3 & 99.3 & 86.4 \\  & ProNet-All-Atom  & 52.1 & 69.0 & 99.0 & 85.6 \\  & CRL  & 47.6 & 70.2 & 99.2 & 87.2 \\  & GearNet  & 28.4 & 42.6 & 95.3 & 79.4 \\  & GearNet-IEConv  & 42.3 & 64.1 & 99.1 & 83.7 \\  & GearNet-Edge  & 44.0 & 66.7 & 99.1 & 86.6 \\  & GearNet-Edge-IEConv  & 48.3 & 70.3 & 99.5 & 85.3 \\  & CDConv  & 56.7 & 77.7 & 99.6 & 88.5 \\  & ProtGO (Student) & **60.5** & **79.4** & **99.8** & **89.4** \\   

Table 1: Accuracy (\(\%\)) of fold classification and enzyme reaction classification. The best results are shown in bold.

### Results of GO Term and EC Number Prediction

Following the protocol in GearNet , the test sets for GO term and EC number prediction only contain PDB chains with less than \(95\%\) sequence identity to the training set, ensuring rigorous evaluation. The student model conducts the experiments, and the teacher model's annotations are not classified into these classes, avoiding data leakage. Table 2 shows comparative results between different protein modeling methods on these tasks, with performance measured by \(_{}\), which balances precision and recall, working well even if positive and negative classes are imbalanced. The mean values of three independent runs are reported. ProtGO achieves the highest \(_{}\) across all test sets for both GO and EC prediction, outperforming other approaches. This demonstrates ProtGO's strong capabilities for predicting protein functions and activities. Overall, the consistent improvements verify the benefits of injecting functional information into sequence-structure models, as done in ProtGO's teacher-student framework. The results cement ProtGO's effectiveness using knowledge distillation techniques.

### Ablation Study

Table 3 presents ablation studies of the proposed ProtGO model on the four downstream tasks. We examine the impact of removing the teacher model, which means removing the \(_{kd}\). We also remove the annotation encoder in the teacher, which means that we incorporate function information into the loss function for the teacher models. As shown in Table 3, removing the teacher model altogether

   Modality (Input) & Method & GO-BP & GO-MF & GO-CC & EC \\   & CNN  & 0.244 & 0.354 & 0.287 & 0.545 \\  & ResNet  & 0.280 & 0.405 & 0.304 & 0.605 \\  & LSTM  & 0.225 & 0.321 & 0.283 & 0.425 \\  & Transformer  & 0.264 & 0.211 & 0.405 & 0.238 \\   & GCN  & 0.252 & 0.195 & 0.329 & 0.320 \\  & GAT  & 0.284 & 0.317 & 0.385 & 0.368 \\  & 3DCNN\_MQA  & 0.240 & 0.147 & 0.305 & 0.077 \\   & GraphQA  & 0.308 & 0.329 & 0.413 & 0.509 \\  & GVP  & 0.326 & 0.426 & 0.420 & 0.489 \\  & CRL  & 0.421 & 0.624 & 0.431 & - \\   & GearNet  & 0.356 & 0.503 & 0.414 & 0.730 \\   & GearNet-IEConv  & 0.381 & 0.563 & 0.422 & 0.800 \\   & GearNet-Edge  & 0.403 & 0.580 & 0.450 & 0.810 \\   & GearNet-Edge-IEConv  & 0.400 & 0.581 & 0.430 & 0.810 \\   & CDConv  & 0.453 & 0.654 & 0.479 & 0.820 \\   & ProtGO (Student) & **0.464** & **0.667** & **0.492** & **0.857** \\   

Table 2: \(_{}\) of GO term prediction and EC number prediction. The best results are shown in bold.

Figure 3: The KL training loss curves (a), (c) and test performance (b), (d) on the tasks of fold classification and EC number prediction. The red curve denotes that \(_{}\) conducts its function, while the green curve denotes we calculated the value of \(_{}\), but it is not involved in the process of the gradient backpropagation (BP).

(w/o Teacher) leads to substantial performance drops across all tasks compared to the full ProtGO. This shows the teacher's knowledge distillation provides useful signals for the student model. Besides, removing the annotation encoder in the teacher (w/o AE-T) also degrades performance, though less severely. Despite being a label-augmented strategy, the annotation encoder exhibits minimal influence, indicating low sensitivity and limited impact on test performance. Our student model is specifically designed to process protein sequences and structures as inputs, enabling it to function independently without the need for guidance from the teacher model.

Figure 3 illustrates the comparisons of the knowledge distillation loss \(_{kd}\) with and without involvement in backpropagation during training. When \(_{kd}\) is excluded from the gradient backpropagation process, it exhibits a decrease alongside the classification loss \(_{}\). However, its value remains substantially higher compared to when \(_{kd}\) is included in the training process. Similar observations are presented for the accuracy and \(_{}\) on the fold classification and EC number prediction. The notable disparity observed between the distillation loss and the test performance with and without involvement in backpropagation suggests that the KL loss does indeed play a significant role in guiding the student model's learning process. Its presence influences the model's performance and convergence.

We compare the performance of the teacher and the student on the tasks of GO term prediction. From the provided Figure 4, it is evident that incorporating functional information as the input of the annotation encoder significantly enhances performance, particularly for MF and CC term prediction. These two classes have fewer categories and are more accessible, resulting in higher scores.

## 5 Conclusion

In this paper, we proposed ProtGO, a multimodal protein representation learning framework integrating information from protein sequences, structures, and function annotations. While the teacher network requires extra information about function annotations as input, such is not always available for the student model. The student model amends the problem by mimicking the behavior or predictions of the teacher model. Our main focus is to obtain comprehensive embeddings for the student model, whereas the complete training of the teacher model is not our primary concern. We estimate the latent embedding distributions for the teacher-student model and learn annotation-enriched student representations by distribution approximation. Compared to the mainstream protein representation learning techniques, ProtGO achieves superior performances in predicting protein families, reactions, GO terms, and EC numbers. These consistent improvements across benchmarks highlight the advantages of this approach for informative protein representation learning. A limitation is that this base model is not pre-trained on large-scale datasets. One way for improvement is to integrate ProtGO with other pre-training strategies (refer to Appendix G). This would need to manage the computational resources required for training and inference and ensure compatibility between different model architectures and training objectives.

    &  & Enzyme &  &  \\    & Fold & Superfamily & Family & & & & BP & MF & CC \\  ProtGO & 60.5 & 79.4 & 99.8 & 89.4 & 0.464 & 0.667 & 0.492 & 0.857 \\  w/o AE-T & 60.4 & 79.1 & 99.7 & 88.9 & 0.454 & 0.664 & 0.490 & 0.854 \\ w/o Teacher & 57.8 & 78.7 & 99.6 & 88.6 & 0.458 & 0.660 & 0.484 & 0.851 \\   

Table 3: Ablation experiments of our proposed method. w/o AE-T denotes without the annotation encoder in the teacher model. w/o teacher means without the teacher model and directly using the student model, which also means without \(_{kd}\).

Figure 4: Performance comparisons of the teacher and student of ProtGO on GO term prediction.