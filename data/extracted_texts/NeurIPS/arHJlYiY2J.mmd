# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

In this paper, we introduce CVD, a plug-and-play module capable of generating videos with different camera trajectories sharing the same underlying content and motion of a scene. CVD is designed on a collaborative diffusion process that generates consistent pairs of videos with individually controllable camera trajectories. Consistency between corresponding frames of a video is enabled using epipolar attention, introduced by a learnable _cross-view synchronization module_. To effectively train this module, we propose a new pseudo-epipolar line sampling scheme to enrich the epipolar geometry attention. Due to the shortage of large-scale training data for 3D dynamic scenes, we propose a _hybrid training_ scheme where multi-view static data from RealEstate10k  and monocular dynamic data from WebVid10M  are utilized to learn camera control and motion, respectively. To our knowledge, CVD is the first approach to generate multiple videos with consistent content and dynamics while providing camera control. Through extensive experiments, we demonstrate that CVD ensures strong geometric and semantic consistencies, significantly outperforming relevant baselines. We summarize our contributions as follows:

* To our knowledge, our CVD is the first video diffusion model that generates multi-view consistent videos with camera control;
* We introduce a novel module called the _Cross-Video Synchronization Module_, designed to align features across diverse input videos for enhanced consistency;
* We propose a new collaborative inference algorithm to extend our video model trained on video pairs to arbitrary numbers of video generation;
* Our model demonstrates superior performance in generating multi-view videos with consistent content and motion, surpassing all baseline methods by a significant margin.

## 2 Related Work

Video Diffusion Models.Recent efforts in training large-scale video diffusion models have enabled high-quality video generation [15; 6; 23; 21; 17; 7; 12; 50]. Video Diffusion Model , utilizes a 3D UNet to learn from images and videos jointly. With the promising image quality obtained by text-to-image (T2I) generation models, such as StableDiffusion , many recent efforts focus on extending pretrained T2I models by learning a temporal module. Align-your-latents  proposes to inflate the T2I model with 3D convolutions and factorized space-temporal blocks to learn video dynamics. Similarly, AnimateDiff  builds upon StableDiffusion , adding a temporal module after each fixed spatial layer to achieve plug-and-play capabilities that allow users to perform personalized animation without any finetuning. Pyoco  proposes a temporally coherent noise strategy to effectively model temporal dynamics. More recently, SORA  shows a great step towards photo-realistic long video generation by utilizing space-time diffusion with a transformer architecture.

Controllable Video Generation.The ambiguity of textual conditions often results in weak control for text-to-video models (T2V). To provide precise guidance, some approaches utilize additional conditioning signals such as depth, skeleton, and flow to control the generated videos [12; 59; 26; 48; 27; 8]. Recent efforts like SparseCtrl  and SVD incorporate images as control signals for video generation. To further control motions and camera views in the output video, DragNUWA  and MotionCtrl  inject motion and camera trajectories into the conditioning branch, where the former uses a relaxed version of optical flow as stroke-like interactive instruction, and the later directly concatenate camera parameters as additional features. CameraCtrl  proposes to over-parameterize the camera parameters using Plucker Embeddings  and achieves more accurate camera conditioning. Alternatively, AnimateDiff  trains camera-trajectory LoRAs  to achieve viewpoint movement conditioning, while MotionDirector  also utilizes LoRAs  but to overfit to specific appearances and motions to gain their decoupling.

Multi-View Image Generation.Due to the lack of high-quality scene-level 3D datasets, a line of research focuses on generating coherent multi-view images. Zero123  learns to generate novel-view images from pose conditions, and subsequent works extend it to multi-view diffusion [11; 36; 37; 49; 54; 55; 62; 31] for better view consistency. However, these methods are only restricted to objects and consistently fail to generate high-quality large-scale 3D scenes. MultiDiffusion  and DiffCollage  facilitates 360-degree scene image generation, while SceneScape  generates zooming-out views by warping and inpainting using diffusion models. Similarly, Text2Room  generates multi-view images of a room, where the images can be projected via depths to get a coherent room mesh. DiffDreamer  follows the setups in Infinite-Nature [34; 33] and iteratively performs projection and refinement using a conditional diffusion model. A recent work, PoseGuided-Diffusion , performs novel view synthesis from a single image by training and adding an epipolar line bias to its attention masks on multi-view datasets with camera poses provided (RealEstate10k ). However, this method by construction does not generalize to in-the-wild or dynamic scenes, as its prior is solely learned from well-defined static indoor data.

A comprehensive survey of recent advances in diffusion models for visual computing is provided by Po et al. .

## 3 Collaborative Video Generation

Conventionally, video diffusion models (VDMs) aim to generate videos from randomly sampled Gaussian noise with multiple denoising steps, given conditions such as text prompts, frames, or camera poses. Specifically, let \(_{0} q_{}()\) be a data point sampled from the data distribution; the forward diffusion process continuously adds noises to \(_{0}\) to get a series of \(_{t},t 1,...,T\) until it becomes Gaussian noise. Using the reparameterization trick from Ho et al., the distribution of \(_{t}\) can be represented as \(q(_{t}_{0})=(_{t};\ _{t}} _{0},(1-_{t})I)\), where \(_{t}(0,1]\) are the noise scheduling parameters, which are monotonously increasing, and \(_{T}=1\). The video diffusion model, typically denoted as \(p_{}(_{t-1}|_{t})\), is a model parameterized by \(\) that is trained to estimate the backward distribution \(q(_{t-1}|_{t},_{0})\). According to Ho et al. , the optimization of \(p_{}(_{t-1}|_{t})\) results in minimizing the following loss function:

\[=_{,_{0},t,c}\|-_{}(_{t},t,c)\|^{2}, \]

where \(_{t}=_{t}}_{0}+(1-_{t})\) is the noisy video feature generated from \(_{0}\) and a random sampled Gaussian noise \(\), \(_{}(_{t},t)\) is the noise prediction of the VDM, and \(c\) is the video condition. During inference time, one can start from a normalized Gaussian noise \(_{T}(0,I)\) and apply the noise prediction model \(_{}(_{t},t)\) multiple times to denoise it until \(_{0}\).

In contrast to conventional video diffusion models, CVD seeks to find an arbitrary number of videos \(^{i},i\) that comply with the unknown data distribution \(q_{}(^{1,,M})\) given separate conditions \(c^{1,,M}\). Similarly, the CVD model can be represented as \(p_{}(^{1,,M}|c^{1,,M})\). An example includes multi-view videos synchronously captured from the same dynamic 3D scene. Similarly, the loss function for a collaborative video diffusion model is defined as:

\[_{}=_{^{1,,M},^{1,,M}_{0}, t,c}\|^{1,,M}-_{}(^{1,,M}_{t},t,c^{1, ,M})\|^{2}. \]

In practice, however, the scarcity of large-scale multi-view video data prevents us from directly training a model for an arbitrary quantity of videos. Therefore, we build our training dataset of consistent video pairs (i.e., \(M=2\)) from existing monocular video datasets, and train the diffusion model to generate pairs of videos sharing the same underlying contents and motions (see details in Secs. 4.1 and 4.2). Our model is designed to accommodate any number of input video features, however, and we develop an inference algorithm to generate an arbitrary number of videos from our pre-trained pairwise CVD model (see Sec. 4.3).

## 4 Collaborative Video Diffusion with Camera Control

We seek to build a diffusion model that takes a text prompt \(y\) and a set of camera trajectories \(cam^{1,,M}\) and generates the same number of collaborative videos \(^{1,,M}\). To ease the generation of consistent videos, in this work we train our model with video pairs (\(M=2\)), we make the assumption that the videos are synchronized (i.e., corresponding frames are captured simultaneously), and set the first pose of every trajectory to be identical, forcing the first frame of all videos to be the same.

Inspired by [18; 17], our model is designed as an extension of the camera-controlled video model CameraCtrl . As shown in Fig. 2, our model takes two (or more) noisy video feature inputs and generates the noise prediction in a single pass. The video features pass through the pretrained weights of CameraCtrl and are synchronized in our proposed _Cross-View Synchronization Modules_ (Sec. 4.1). The model is trained with two different datasets: RealEstate10K , which consists of camera-calibrated video on mostly static scenes, and WebVid10M , which contains generic videos without poses. This leads to our two-phase training strategy introduced in Sec. 4.2. The learned model can infer arbitrary numbers of videos using our proposed inference algorithm, which will be described in Sec. 4.3.

### Cross-View Synchronization Module

State-of-the-art VDMs commonly incorporate various types of attention mechanisms defined on the spatial and temporal dimension: works such as AnimateDiff , SVD , LVDM  disentangles space and time and applies separate attention layers; the very recent breakthrough SORA  processes both dimensions jointly on its 3D spatial-temporal attention modules. Whilst the operations defined on the spatial and temporal dimensions bring a strong correlation between different pixels of different frames, capturing the context between different videos requires a new operation: cross-video attention.

Figure 2: **Architecture of collaborative video diffusion.**_Left_: The model takes two (or more) noisy video features and camera trajectories as input and generates the noise prediction for both videos. Note that the image autoencoder of Stable Diffusion is omitted here; _Right_: Our _Cross-View Synchronization Module_ takes the same frames from the two videos along with the corresponding fundamental matrix as input, and applies a masked cross-view attention between the frames.

Thankfully, prior works [10; 8] have shown that the extended attention technique, i.e., concatenating the key and values from different views together, is evidently efficient for preserving identical semantic information across videos. However, it refrains from preserving the structure consistency among them, leading to totally different scenes in terms of geometry. Thus, inspired by , we introduce the _Cross-View Synchronization Module_ based on the epipolar geometry to shed light on the structure relationship between cross-video frames during the generation process, aligning the videos towards the same geometry.

Fig. 2 demonstrates the design of our cross-view module for two videos. Taking a pair of feature sequences \(^{1}_{1...,N},^{2}_{1...,N}\) of \(N\) frames as input, our module applies a cross-video attention between the same frames from the two videos. Specifically, we define our module as:

\[out^{1}_{k}=((_{Q}^{1}_{k},_{K}^{2}_{k},_{V}^{2}_{k},^{1,2}_{k })), k, \]

\[^{1,2}_{k}(_{1},_{2})=(^{ 2}_{2}^{1\!2}_{k}_{1}<_{}) \]

where \(k\) is the frame index, \(_{Q},_{K},_{V}\) are the query, key, value mapping matrices, \(\) is the attention mask, \((Q,K,V,)\) is the attention operator introduced from the Transformer , \(\) is the feed-forward function and \(^{1\!2}_{k}\) is the fundamental matrix between \(cam^{1}_{k}\) and \(cam^{2}_{k}\). The attention mask \(\) between any two pixels \(_{1},_{2}\) is determined by the epipolar distance between\(_{1}\) and \(_{2}\), i.e. the shortest distance between \(_{1}\) and the epipolar line of \(_{2}\) in \(_{1}\)'s frame, which is set to \(1\) if the epipolar distance is smaller than a given threshold \(_{}\) (set to \(3\) in all of our experiments) and vise versa. The outputs of these modules are used as residual connections with corresponding original inputs to ensure no loss of originally learned signals. The key insight of this module is as the two videos are assumed to be synchronized to each other, the same frame from the two videos is supposed to share the same underlying geometry and hence can be correlated by their epipolar geometry defined by the given camera poses. For the first frames where the camera poses are set to be identical since the fundamental matrix is undefined here, we generate pseudo epipolar lines for each pixel with random slopes that go through the pixels themselves. In the scenario where multi-view datasets are available, the modules can be further adapted to more videos by extending the cross-view attention from 1-to-1 to 1-to-many. Our study shows that epipolar-based attention remarkably increases the geometry integrity of the generated video pairs.

### Hybrid Training Strategy from Two Datasets

Considering the fact that there is no available large-scale real-world dataset for video pairs, we opt to make use of the two popular monocular datasets, RealEstate10K  and WebVid10M , to develop a hybrid training strategy for video pair generation models.

**RealEstate10K with Video Folding.** The first phase of the training involves RealEstate10K , a dataset consisting of video clips capturing mostly static indoor scenes and corresponding camera poses. We sample video pairs by simply sampling subsequences of \(2N-1\) frames from a video in the dataset, then cutting them from the middle and reversing their first parts to form synchronized video pairs. In other words, the subsequences are folded into two video clips sharing the same starting frame.

**WebVid10M with Homography Augmentation.** While RealEstate10K  provides a decent geometry prior, training our model only on this dataset is not ideal since it does not provide any knowledge regarding dynamics and only contains indoor scenes. On the other hand, WebVid10M, a large-scale video dataset, consists of all kinds of videos and can be used as a good supplement to RealEstate10K. To extract video pairs, we clone the videos in the dataset and then apply random homography transformations to the clones. Nonetheless, The WebVid10M dataset contains no camera information, making it unsuitable for camera-conditioned model training. To address this problem,

Figure 3: **Two-Phase Hybrid Training.** We use different data processing schemes to handle the two datasets (_Top_) and apply separate model structures to train in corresponding phases (_Bottom_).

we propose a two-phase training strategy to adapt both datasets (with or without camera poses) for the same model.

Two-Phase Training.As previously mentioned, our model is built upon the existing camera-controlled VDM CameraCtrl . It is an extended version of AnimateDiff  that adds a pose encoder and several pose feature injectors for the temporal attention layers to the original model. Both AnimateDiff  and CameraCtrl  are based on Stable Diffusion . This implies that they incorporate the same latent space domain, and thus, it is feasible to train a module that can be universally adapted. Therefore, our training procedure follows a two-phase scheme Fig. 3 shows. Specifically, we build a hybrid dataset that combines the data from both sources. Then in each training iteration, if the training data is from RealEstate10K, we use CameraCtrl with LoRA fine-tuned on RealEstate10K as the backbone and applying the ground truth epipolar geometry in the cross-video module. Otherwise, we use AnimateDiff with LoRA fine-tuned on WebVid10M as the backbone, and apply the pseudo epipolar geometry (the same strategy used for the first frames in RealEstate10K dataset) in the cross-video module. The two training phases are applied alternatively to the same instance of CVSM in a single training procedure. Experiments show that the hybrid training strategy greatly helps the model generate videos with synchronized motions and great geometry consistency.

### Towards More Videos

With the CVD trained on video pairs, during inference, we can generate multiple videos (for example, \(M\) videos where \(M>2\)) that share consistent content and motions. To achieve that, we start from \(M\) individual gaussian noise maps and denoise them in multiple steps. At each denoising step \(t\), we select \(P\) feature pairs \(=\{_{t}^{t_{1},j_{1}},_{t}^{t_{2},j_{2}},...,_{t }^{t_{P},j_{P}}|\ i_{1,...,P},j_{1,...,P} 1,...,M\}\) among all \(M\) video features. We then use the trained network to predict the noise of each feature pair, and averaging them w.r.t. each video feature. That is, the output noise for the \(i\)th video feature is defined as: \(_{out}(_{t}^{i})=_{^{i,j}}( _{b}^{i,j}(_{t}^{i,j},t,cam^{i,j}))\), where \(_{b}^{i}(_{t}^{i,j},t,cam^{i,j})\) is the noise prediction for \(_{t}^{i}\) given the video pair input \(_{t}^{i,j}\). For pair selection, we propose the following strategies:

* _Exhaustive Strategy_: Select all \(M(M-1)/2\) pairs.
* _Partitioning Strategy_: Randomly divide \(M\) noisy video inputs into \(\) pairs.
* _Multi-Partitioning Strategy_: Repeat the Partitioning Strategy multiple times and combine all selected pairs.

The exhaustive strategy has a higher computational complexity of \(O(M^{2})\) compared to the partitioning one (\(O(M)\)) but covers every pair among \(M\) videos and thus can produce more consistent videos. The multi-partitioning strategy, on the other hand, is a trade-off between the two strategies. We also embrace the recurrent denoising method introduced by Bansal et al.  that does multiple recurrent iterations on each denoising timestep. We provide the pseudo-code of our inference algorithm and detailed mathematical analysis in our supplementary.

## 5 Experiments

### Qualitative Results

#### 5.1.1 Comparison with Baselines

Qualitative comparisons are shown in Fig. 4. Following our quantitative comparisons in Sec. 5.2, we compare against CameraCtrl  and its combination with SparseCtrl , MotionCtrl  and its combination with SVD . The results indicate our method's superiority in aligning the content within the videos, including dynamic content such as lightning, waves, etc. More qualitative results are provided in our supplemental material and video.

#### 5.1.2 Additional results for arbitrary views generation

We also show the results of arbitrary view generation shown in Fig. 5. Using the algorithm introduced in Sec. 4.3, our model can generate groups of different camera-conditioned videos that share the same contents, structure, and motion. Please refer to our supplementary video for animated results.

### Quantitative Results

We compare our model with two state-of-the-art camera-controlled video diffusion models for quantitative evaluation: CameraCtrl  and MotionCtrl . Both of the two baselines are trained on the RealEstate10K  for camera-controlled video generation. We conduct the following experiments to test the geometric consistency, semantic consistency, and video fidelity of all models:

Per-video geometric consistency on estate scenes.Following CameraCtrl , we first test the geometry consistency across the frames in the video generated from our model, using the camera trajectories and text prompts from RealEstate10K  (which mostly consists of static scenes). Specifically, we first generate 1000 videos from randomly sampled camera trajectory pairs (two camera trajectories with the same starting transformation) and text captions. All baselines generate one video at a time; our model generates two videos simultaneously. For each generated video, we apply the state-of-the-art image matching algorithm SuperGlue  to extract the correspondences between its first frame and following frames and estimate their relative camera poses using the RANSAC  algorithm. To evaluate the quality of correspondences and estimated camera poses, we adopt the same protocol from SuperGlue , which 1) evaluates the poses by the angle error of their rotation and translation and 2) evaluates the matched correspondences by their epipolar error (i.e., the distance to the ground truth epipolar line). The results are shown in Tab. 1, where our model significantly outperforms all baselines. More details are provided in our supplementary materials.

Cross-video geometric consistency on generic scenes.Aside from evaluating the consistency between frames in the same video, we also test our model's ability to preserve the geometry information across different videos. To do that, we randomly sample 500 video pairs (1000 videos in total) using camera trajectory pairs from RealEstate10K  and text prompts from WebVid10M's captions . To the best of our knowledge, there is no available large video diffusion model that is designed to generate multi-view consistent videos for generic scenes. Hence, we modify the CameraCtrl  and MotionCtrl  to generate video pairs as baselines. Here, we use the text-to-video version of each

    &  & Rot. AUC \(\) & Trans. AUC \(\) & Prec. \(\) & M-S. \(\) \\  & & (\(5^{}/10^{}/20^{}\)) & (\(5^{}/10^{}/20^{}\)) & & Prec. \(\) & M-S. \(\) \\   & Reference & 61.4 / 77.2 / 87.8 & 6.9 / 17.5 / 41.0 & 60.2 & 36.5 \\  & CameraCtrl  & 34.8 / 55.2 / 72.4 & 2.3 / 6.6 / 17.0 & 50.8 & 27.3 \\  & MotionCtrl  & 49.0 / 68.0 / 81.2 & 3.4 / 10.2 / 25.0 & 64.6 & 38.9 \\  & Ours & **55.5** / **71.8** / **83.3** & **5.6** / **15.9** / **33.2** & **76.9** & **42.3** \\   & CameraCtrl +SparseCtrl  & 6.2 / 14.3 / 25.8 & 0.5 / 1.7 / 4.7 & 16.5 & 5.4 \\  & MotionCtrl +SVD  & 12.2 / 28.2 / 48.0 & 1.2 / 4.9 / 13.5 & 23.5 & 12.8 \\   & Ours & **25.2** / **40.7** / **57.5** & **3.7** / **9.6** / **19.9** & **51.0** & **23.5** \\   

Table 1: **Quantitative Results on Geometry Consistency.** Following SuperGlue , we report the area under the cumulative error curve (AUC) of the predicted camera rotation and translation under certain thresholds (\(5^{},10^{},20^{}\)), and the precision (P) and matching score (MS) of the SuperGlue correspondences. We feed the models with prompts from RealEstate10K  (RE10K) and WebVid10M  (WV10M) in two experiments separately. For RealEstate10K scenes, we also run SuperGlue on the original RealEstate10K  frames as reference. Our model achieves the highest scores on all metrics compared to baselines.

    &  &  \\  & CLIP-T \(\) & CLIP-F \(\) & FID \(\) & KID \(\) & FVD \(\) \\  MotionCtrl +SVD  & - & 0.81 & - & - & - \\ CameraCtrl  & 0.28 & 0.79 & **32.10** & 0.79 & **277** \\ AnimateDiff +SparseCtrl  & 0.29 & 0.86 & 51.97 & 1.86 & 327 \\ CameraCtrl +SparseCtrl  & 0.29 & 0.85 & 61.68 & 2.47 & 430 \\ Ours & **0.30** & **0.93** & 32.90 & **0.61** & 285 \\   

Table 2: **Quantitative Results for semantic & fidelity metrics.** The semantic metrics are evaluated on WebVid10M  and the fidelity metrics are performed on RealEstate10k . As shown in the table, our method is better than or on par with all prior work regarding semantic matching with the prompt, cross-video consistency, and frame fidelity.

model to generate a reference video first, then take its first frame as the input of their image-to-video version (i.e., their combination with SparseCtrl  and SVD ) to derive the second video. We use the same metrics as in the first experiment but instead evaluate between the corresponding frames from the two videos. Results are shown in Tab. 1, where our model greatly outperforms all baselines.

Semantic and fidelity evaluations.Following the standard practice of prior works , we report CLIP  embedding similarity between **1)** each frame of the output video and the corresponding input prompt and **2)** pairs of frames across video pairs. The former metric, denoted as CLIP-T, is to show that our model does not destroy the appearance/content prior of our base model, and the latter, denoted as CLIP-F, is aimed to show that the cross-view module can improve the semantic and structural consistency between the generated video pair. For these purposes, we randomly sample 1000 videos using camera trajectory pairs from RealEstate10K, along with text

Figure 4: **Qualitative comparison. Our method maintains consistency across videos for static and dynamic scenes, while no prior work can generate synchronized different realizations. * Despite our best efforts, we are incapable of getting MotionCtrl +SVD  to generate any motion beyond the simplest static camera zooming in-and-out. Please refer to our supplemental video for illustration.**

captions from WebVid10M (2000 videos generated in total). To further demonstrate our method's ability to maintain high-fidelity generation contents, we report FID  and KID \( 100\) using the implementation , and FVD . We do not compare against models that do not share the same base model as us for FID , KID  and FVD , since these metrics are strongly influenced by the abilities of the base models. Following prior work , we evaluate these two metrics on RealEstate10k  because of the strong undesired bias, e.g., watermarks, on WebVid10M . As shown in Tab. 2, our model surpasses all baselines for the CLIP -based metrics. This proves our model's ability to synthesize collaborative videos that share a scene while maintaining and improving fidelity according to the prompt. Our model also performs better than or on par with all prior works on fidelity metrics, which indicates robustness to the appearances and content priors learned by our base models.

### Ablation Study

We perform a thorough ablation study in Tab. 3 to verify our design choices, where the variants are: **1)** No epipolar line constraints (_Ours w/o Epi_), where we perform a normal self-attention instead of epipolar attention in our _Cross-View Synchronization Module_; **2)** No mixed training (_Ours RE10K only_), where we follow the setups in CameraCtrl  and train the model only on RealEstate10k ; **3)** No homography augmentation (_Ours w/o HG_), where we switch off the homography transformations applied to WebVid10M  videos during training; and **4)** using only 1 _Cross-View Synchronization Module_ instead of 2 (_Ours 1 Layer_). The ablation study indicates that while we can get semantically consistent outputs without epipolar constraints, they are essential to gain geometrical consistency. We also observe that the mixed training strategy and homography augmentation greatly improve all metrics, including semantic consistency, further verifying their purpose of closing the gap between static training scenes and desired dynamic outputs. We believe there are two reasons why our full model outperforms the model trained on RealEstate10K . The first reason is our epipolar attention design. In the WebVid10M  training stage, while there are no camera poses available, we use pseudo-gt epipolar lines (i.e. lines calculated from homography matrix H. The line of pixel x in the warped frame goes through the pixel Hx) to describe the spatial relationship between video frames. This enhances the model's ability to generate videos that satisfy the given line conditions. Hence, in a camera-control setting, the full model is more constrained to the epipolar lines and generates videos that align better with the camera poses. Secondly, since RealEstate10K  mostly consists of static indoor scenes, models trained on RealEstate10K  may suffer from data bias and may not perform well on general scenes, thus resulting in poor evaluation performance in this experiment.

## 6 Discussion

We introduce CVD, a novel framework facilitating collaborative video generation. It ensures seamless information exchange between video instances, synchronizing content and dynamics. Additionally, CVD offers camera customization for comprehensive scene capture with multiple cameras. The core innovation of CVD is its utilization of epipolar geometry, derived from reconstruction pipelines, as a constraint. This geometric framework fine-tunes a pre-trained video diffusion model. The training process is enhanced by integrating dynamic, single-view, in-the-wild videos to maintain a diverse range of motion patterns. During inference, CVD employs a multi-view sampling strategy to facilitate efficient information sharing across videos, resulting in a "collaborative diffusion" effect for unified video output. To our knowledge, CVD represents the first approach to tackle the complexities of

    & Rot. AUC & Trans. AUC & Semantic Consistency \\  & (@5\({}^{}\)/10\({}^{}\)/20\({}^{}\)) & (@5\({}^{}\)/10\({}^{}\)/20\({}^{}\)) & CLIP-T \(\) & CLIP-F \(\) \\  Ours w/o Epi & 16.8 / 31.8 / 49.1 & 1.5 / 5.4 / 13.7 & 0.30 & 0.91 \\ Ours RE10K only & 17.9 / 29.8 / 43.3 & 1.7 / 5.3 / 13.2 & 0.29 & 0.90 \\ Ours w/o HG & 22.0 / 35.5 / 50.5 & 2.3 / 6.1 / 14.5 & 0.29 & 0.92 \\ Ours I Layer & 22.7 / 37.8 / 54.3 & 3.1 / 8.5 / 19.2 & 0.29 & 0.92 \\ Ours & **25.2 / 40.7 / 57.5** & **3.7 / 9.6 / 19.9** & **0.30** & **0.93** \\   

Table 3: **Ablation Study** conducted on generic scenes (prompts from WebVid10M ), where we deactivate each of our introduced modules. Results indicate that our full pipeline outperforms the ablation settings for both geometric and semantic consistencies.

multi-view or multi-trajectory video synthesis. It significantly advances beyond existing multi-view image generation technologies, such as Zero123 , by also ensuring consistent dynamics across all videos produced. This breakthrough marks a critical development in video synthesis, promising new capabilities and applications.

### Limitations

CVD faces certain limitations. Primarily, the effectiveness of CVD is inherently linked to the performance of its base models, AnimateDiff  and CameraCrtl . While CVD strives to facilitate robust information exchange across videos, it does not inherently solve the challenge of internal consistency within individual videos. As a result, issues such as uncanny shape shifting and dynamic inconsistencies that are presented in the base models may persist, affecting the overall consistency across the video outputs. Additionally, it cannot synthesize videos in real time, owing to the computationally intensive nature of diffusion models. Nevertheless, the field of diffusion model optimization is rapidly evolving, and forthcoming advancements are likely to enhance the efficiency of CVD significantly.

### Broader Impacts

Our approach represents a significant advancement in multi-camera video synthesis, with wide-ranging implications for industries such as filmmaking and content creation. However, we are mindful of the potential misuse, particularly in creating deceptive content like deepfakes. We categorically oppose the exploitation of our methodology for any purposes that infringe upon ethical standards or privacy rights. To counteract the risks associated with such misuse, we advocate for the continuous development and improvement of deepfake detection technologies.

AcknowledgementThis project was partly supported by Google and Samsung.

Figure 5: **Multi-view Video Generation.**_Left_: The cameras move towards 4 directions, while all cameras are looking at the same 3D point; _Right_: The trajectories are interpolated from one trajectory (_1st Row_) to another (_4th Row_).