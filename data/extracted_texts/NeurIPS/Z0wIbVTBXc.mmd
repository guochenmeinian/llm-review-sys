# Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling

Grigory Bartosh

University of Amsterdam

g.bartosh@uva.nl

&Dmitry Vetrov

Constructor University, Bremen

dvetrov@constructor.university

&Christian A. Naesseth

University of Amsterdam

c.a.naesseth@uva.nl

###### Abstract

Conventional diffusion models often rely on a fixed forward process, which implicitly defines complex marginal distributions over latent variables. This can often complicate the reverse process' task in learning generative trajectories, and results in costly inference for diffusion models. To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the standard linear Gaussian. We also propose a novel parameterization technique for learning the forward process. Our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood. Experimental results demonstrate NFDM's strong performance, evidenced by state-of-the-art likelihoods across a range of image generation tasks. Furthermore, we investigate NFDM's capacity for learning generative dynamics with specific characteristics, such as deterministic straight lines trajectories, and demonstrate how the framework can be adopted for learning bridges between two distributions. The results underscores NFDM's versatility and its potential for a wide range of applications.

## 1 Introduction

Diffusion models  are a class of generative models constructed by two key processes: the forward process and the reverse process. The forward process gradually corrupts the data distribution, transforming it from its original form to a noised state. The reverse process learns to invert corruptions of the forward process and restore the data distribution. This way, the model learns to generate data from pure noise. Diffusion models have demonstrated remarkable results in various domains . Nevertheless, most existing diffusion models fix the forward process to be predefined, usually linear, Gaussian which makes it unable to adapt to the task at hand or simplify the target for the reverse process. At the same time there is a growing body of work that demonstrates how modifications of the forward process improve performance in terms of generation quality , likelihood estimation  or sampling speed .

In this paper, we present Neural Flow Diffusion Models (NFDM), a framework that allows for the pre-specification and learning of complex latent variable distributions defined by the forward process. Unlike conventional diffusion models , which rely on a conditional Gaussian forward process, NFDM can accommodate any continuous (and learnable) distribution that can be expressed as an invertible mapping applied to noise. We also derive, and leverage, a new end-to-end simulation-free optimization procedure for NFDM, that minimizes a variational upper bound on the negative log-likelihood (NLL).

Furthermore, we propose an efficient neural network-based parameterization for the forward process, enabling it to adapt to the reverse process during training and simplify the learning of the data distribution. To demonstrate NFDM's capabilities with a learnable forward process we provideexperimental results on CIFAR-10, ImageNet 32 and 64, attaining state-of-the-art NLL results, which is crucial for many applications such as data compression [22; 71], anomaly detection [7; 13] and out-of-distribution detection [51; 70].

Then, leveraging the flexibility of NFDM, we demonstrate how this framework can be applied to learn bridges between two distributions using the AFHQ dataset. Finally, we explore training with constraints on the reverse process to learn generative dynamics with specific properties. As a case study, we discuss curvature and obstacle avoidance penalties on the deterministic generative trajectories. Our empirical results indicate improved computational efficiency compared to baselines on CIFAR-10, downsampled ImageNet, and synthetic data.

We summarize our contributions as follows:

1. We introduce Neural Flow Diffusion Models (NFDM), improving diffusion modelling through a learnable forward process.
2. We develop an end-to-end optimization procedure that minimizes an upper bound on the negative log-likelihood in a simulation-free manner.
3. We demonstrate state-of-the-art log-likelihood results on CIFAR-10, ImageNet 32 and 64.
4. We show how NFDM can be used in learning bridges and generative processes with specific properties, such as dynamics with straight line trajectories, leading to significantly faster sampling speeds and enhanced generation quality with fewer sampling steps.

## 2 Background

Diffusion models are generative latent variable models consisting of two processes: the forward and the reverse (or generative) process. The forward process is a dynamic process that takes a data point \( q(),^{D}\), and perturbs it over time by injecting noise. This generates a trajectory of latent variables \(\{(t)\}_{t}\), conditional on the data \(\), where \(\) is a fixed time horizon and \(_{t}=(t)^{D}\). The (conditional) distribution can be described by an initial distribution \(q(_{0}|)\) and a Stochastic Differential Equation (SDE) with a linear drift term \(^{F}(_{t},t):^{D}^ {D}\), scalar variance \(g(t):_{+}\), and a standard Wiener process \(\):

\[d_{t}=^{F}(_{t},t)dt+g(t)d.\] (1)

Due to the linearity of \(^{F}\), we can reconstruct the conditional marginal distribution \(q(_{t}|)=(_{t};_{t},_{t}^{2}I)\). Typically, the conditional distributions evolve from some low variance distribution \(q(_{0}|)(_{0}-)\) to a unit Gaussian \(q(_{1}|)(_{1};0,I)\). This forward process is then reversed by starting from the prior \(_{1}(_{1};0,I)\), and following the reverse SDE :

\[d_{t}=^{B}(_{t},t)dt+g(t)d}, ^{B}(_{t},t)=^{F}(_{ t},t)-g^{2}(t)_{_{t}} q(_{t}).\] (2)

Here, \(}\) denotes a standard Wiener process where time flows backwards. Diffusion models approximate this reverse process by learning \(_{_{t}} q(_{t})\), known as the score function, through a \(_{t}\)-weighted denoising score matching loss:

\[*{}_{u(t)q(,_{t})}[_{ t}s_{}(_{t},t)-_{_{t}} q( _{t}|)_{2}^{2}],\] (3)

where \(u(t)\) represents a uniform distribution over the interval \(\), and \(s_{}:^{D}^{D}\) is a learnable approximation. With a learned score function \(s_{}(_{t},t)\), one can generate a sample from the reverse process by first sampling from the prior \(_{1}(_{1};0,I)\), and then simulating the reverse SDE, resulting in a sample \(_{0} p_{}(_{0}) q(_{0}) q ()\):

\[d_{t}=^{F}(_{t},t)-g^{2}(t)s_{}( _{t},t)dt+g(t)d}.\] (4)

Diffusion models possess several important properties. For example, for a specific \(_{t}\), the objective (eq. (3)) can be reformulated  as an Evidence Lower Bound (ELBO) on the model's likelihood. Furthermore, the minimization of denoising score matching (eq. (3)) is a simulation-free procedure. This means that simulating either the forward or reverse processes through its SDE is not necessary for sampling \(_{t}\), nor is it necessary for estimating the gradient of the loss function. Instead, we can directly sample \(_{t} q(_{t}|)\). The simulation-free nature of this approach is a crucial aspect for efficient optimization.

Another notable property is the existence of an Ordinary Differential Equation (ODE) corresponding to the same marginal densities \(q(_{t})\) as the SDE (eq. (1)):

\[d_{t} =f(_{t},t)dt,\] (5) \[f(_{t},t) =^{F}(_{t},t)-(t)}{2}_{ _{t}} q_{}(_{t}).\] (6)

This implies that we can sample from diffusion models deterministically, allowing the use of off-the-shelf numerical ODE solvers for sampling, which may improve the sampling speed compared to stochastic sampling that requires simulating an SDE. Additionally, deterministic sampling enables us to compute densities by treating the model as a continuous normalizing flow, as detailed in [6; 17].

## 3 Neural Flow Diffusion Models

In diffusion models the forward process defines stochastic conditional trajectories \(\{(t)\}_{t}\) and the reverse process tries to match the marginal distribution of trajectories. This construction can be viewed as a specific type of hierarchical Variational Autoencoders (VAEs) [30; 45]. However, in conventional diffusion models the latent variables are inferred through a pre-specified linear combination of the data point and Gaussian noise. This formulation limits diffusion models in terms of the flexibility of their latent space, and makes learning of the reverse process more challenging. To address this limitation, we propose a generalized form of the forward process that enables the definition and learning of a broad range of distributions in the latent space. From a practical perspective, a more flexible forward process can simplify the task of learning the reverse process. From a theoretical perspective, learning of the forward process is analogous to learning the variational distribution in a hierarchical VAE, which gives a tighter bound on model's NLL (see an extended discussion in Appendix B.1).

In this section, we introduce Neural Flow Diffusion Models (NFDM) - a framework that generalizes conventional diffusion models. The key idea in NFDM is to define the forward process' conditional SDE implicitly via a learnable transformation \(F_{}(,t,)\) that defines the marginal distributions. This lets the user define a broad range of continuous time- and data-dependent forward processes, that the reverse process will learn to invert. Importantly, NFDM retains crucial properties of conventional diffusion models, like likelihood-based and simulation-free training. Previous diffusion models emerge as special cases when the data transformation is linear, time-independent, and/or additive Gaussian.

### Forward Process

We approach the forward process constructively. The ultimate goal is a learnable distribution over trajectories, \(\{(t)\}_{t}\) given \(\), realized by a conditional SDE constructed in three steps.

First, we (implicitly) define the conditional marginal distributions \(q_{}(_{t}|)\) for \(t\) using \(F_{}(,t,)\). Then, we introduce the corresponding conditional ODE that together with an initial distribution \(q_{}(_{0}|)\) matches the conditional marginal distribution \(q_{}(_{t}|)\). Finally, we define a conditional SDE that defines a distribution over trajectories \(\{(t)\}_{t}\), with marginal distributions \(q_{}(_{t}|)\) for each \(t\).

**Forward Marginal Distribution**. We characterize the marginal distribution \(q_{}(_{t}|)\) of the forward process trough a function that transforms a noise sample \(\) into \(_{t}\), conditional on the time step \(t\) and data point \(\):

\[_{t}=F_{}(,t,),\] (7)

where \(F_{}:^{D}^{D}^{D}\) and \( q()=(;0,I)\). This defines the conditional distribution of latent variables \(q_{}(_{t}|)\). Additionally, eq. (7) facilitates direct and efficient sampling from \(q_{}(_{t}|)\) through \(F_{}\) in eq. (7).

**Conditional ODE**. We assume that \(F_{}\) is differentiable with respect to \(\) and \(t\), invertible with respect to \(\). Further, we assume that fixing specific values of \(\) and \(\) and varying \(t\) from \(0\) to \(1\) results in a smooth trajectory from \(_{0}\) to \(_{1}\). Differentiating these trajectories over time yields a velocity field corresponding to the conditional distribution \(q_{}(_{t}|)\), thereby defining a conditional ODE:

\[d_{t}=f_{}(_{t},t,)dt,  f_{}(_{t},t,)=. (,t,)}{ t}|_{=F_{}^{-1}( _{t},t,)}.\] (8)

Therefore, if we sample \(_{0} q_{}(_{0}|)\) and solve eq. (8) until time \(t\), we have \(_{t} q_{}(_{t}|)\).

The time derivative of \(F_{}\) may be calculated efficiently using automatic differentiation tools like PyTorch  or JAX  (see details in Appendix B.2).

**Conditional SDE**. The function \(F_{}\) and the distribution of the noise \(q()\) together defines \(q_{}(_{t}|)\). To completely define the distribution of trajectories \(\{(t)\}_{t}\), we introduce a conditional SDE that starts from sample \(_{0}\) and runs forward in time.

With access to both the ODE and score function \(_{_{t}} q_{}(_{t}|)\), an SDE  with marginal distributions \(q_{}(_{t}|)\), is:

\[d_{t}=_{}^{F}(_{t},t,)dt+g_{ }(t)d,\] (9)

\[_{}^{F}(_{t},t,)=f_{}(_{ t},t,)+^{2}(t)}{2}_{_{t}} q_{ }(_{t}|).\]

Here, \(g_{}:_{+}\) is a scalar function, and \(\) represents a standard Wiener process. Note that \(g_{}\) only influences the distribution of trajectories \(\{(t)\}_{t}\). The marginal distributions are the same, \(q_{}(_{t}|)\), for any choice of \(g_{}\).

The SDE in eq. (9) requires access to the conditional score function \(_{_{t}} q_{}(_{t}|)\), which in the general case can be computationally costly. However, for \(F_{}\) that allow efficient evaluation of the log-determinant of the Jacobian matrix, the score function can be calculated efficiently. Examples of such \(F_{}\) are functions linear with respect to \(\) or RealNVP style architectures [14; 29]. The calculation of this score function is further discussed in Appendix B.3.

### Reverse (Generative) Process

To define the reverse (generative) process, we specify a reverse SDE that starts from \(_{1} p(_{1})\) and runs backwards in time. To do so we first introduce a conditional reverse SDE that reverses the conditional forward SDE (eq. (9)). Following , we define:

\[d_{t} =_{}^{B}(_{t},t,)dt+g_{ }(t)d},\] (10) \[_{}^{B}(_{t},t,) =f_{}(_{t},t,)-^{2}(t) }{2}_{_{t}} q_{}(_{t}|).\]

Secondly, leveraging this reverse SDE (see eq. (10)) we incorporate a prediction of \(\):

\[d_{t}=_{,}(_{t},t)dt+g_{}(t)d },_{,}(_{ t},t)=_{}^{B}_{t},t,}_{ }(_{t},t),\] (11)

and \(}_{}:^{D}^{D}\) is a function that predicts the data point \(\). This SDE defines the dynamics of the generative trajectories \(\{(t)\}_{t}\).

To fully specify the reverse process, it is also necessary to define a prior distribution \(p(_{1})\) and a reconstruction distribution \(p(|_{0})\). In all of our experiments, we set the prior \(p(_{1})\) to be a unit Gaussian distribution \((_{1};0,I)\) and let \(p(|_{0})\) be a Gaussian distribution with a small variance \((;_{0},^{2}I)\), where \(^{2}=10^{-4}\).

The above parameterization of the reverse process is not the only possibility. However, it is a convenient choice as it allows for the definition of the reverse process simply through the prediction of \(\), similar to conventional diffusion models . We leave exploration of alternate parameterizations for future work.

### Optimization and Sampling

With \(F_{}\) in eq. (7) parameterized such that \(q_{}(_{0}|)(-_{0})\) and \(q_{}(_{1}|) p(_{1})\), we propose to optimize the forward and reverse processes of NFDM jointly, minimizing the following objective:

\[=_{u(t)q()q_{}(_{t}|)}[^{2}(t)}_{}^{B}(_{t},t,)-_{,}(_{t},t)_{2}^{2} ].\] (12)

Since the forward process is parameterized by \(\), we need to optimize the objective (eq. (12)) with respect to both \(\) and \(\) jointly. We discuss parameterization of the forward process in Appendix B.5.

As we demonstrate in Appendix A.1 the objective \(\) shares similarities with the standard diffusion model objective , and it provides a variational bound on the model's \(\)-likelihood \( p_{,}()\). The objective \(\) also exhibits strong connections with both the Flow Matching  and Score Matching  objectives. We explore these connections in Appendix E.4, where we also discuss the role of \(g_{}\) in detail. It is important to note that despite the parameterization of the reverse process through the prediction of \(\), the objective \(\) optimizes the generative dynamics and does not necessarily lead to accurate predictions of \(\).

A key characteristic of the NFDM objective is its compatibility with the simulation-free paradigm, which is critical for efficient optimization. We summarize the training procedure in Algorithm 1.

``` \(F_{}\), \(g_{}\), \(}_{}\), \(T\) - number of steps \( t=\), \(_{1} p(_{1})\) for\(t=1,,,\)do \(}(0,I)\) \(_{t- t}=_{t}-_{,}(_ {t},t) t+g_{}(t)}\) endfor \( p(|_{0})\) ```

**Algorithm 2** Stochastic Sampling from NFDM

To sample from the trained reverse process we can simulate the SDE, as defined in Section 3.2. This procedure is summarized in Algorithm 2. Additionally, during the sampling process, we can adjust the level of stochasticity by modifying \(g_{}(t)\) (eq. (9)). It is important to note that changes to \(g_{}(t)\) also influence \(_{}^{F}\) (eq. (9)) and \(_{,}\) (eq. (11)). In the extreme case where \(g_{}(t) 0\), the reverse process becomes deterministic, allowing us to utilize off-the-shelf numerical ODE solvers and to estimate densities [6; 17]. We provide an extended sampling discussion in Appendix B.4.

## 4 Neural Flow Bridge Models

In this section we discuss a simple modification to the NFDM framework that enables us to learn bridges between two data distributions, \(q(_{0})\) and \(q(_{1})\), a modification we refer to as Neural Flow Bridge Models (NFBM).

In the context of bridges, we consider a joint data distribution \(q(_{0},_{1})\) (which may be factorized as \(q(_{0})q(_{1})\) if paired data is unavailable). Our goal is to learn a generative process that starts from \(_{1}\) and generates \(_{0}\) such that \(q(_{0}) p_{}(_{0})= q(_{1})p_{ }(_{0}|_{1})d_{1}\). To turn NFDM into NFBM, we modify both the forward and reverse processes. In NFDM, the forward process is defined by two functions: \(F_{}(,t,)\) (see eq. (7)) and \(g_{t}\) (see eq. (9)). For NFBM, we let \(F_{}\) depend on both \(_{0}\) and \(_{1}\), thereby conditioning the entire forward process on these data points:

\[_{t}=F_{}(,t,_{0},_{1}),\] (13)

Similar to the discussion in Section 3.2, for the reverse process of NFBM, we predict the data points \(_{0}\) and \(_{1}\) from \(_{t}\) and \(t\), and substitute them into the conditional reverse time SDE (eq. (10)):

\[d_{t}=_{,}(_{t},t)dt+g_{}(t)d },_{,}(_{t },t)=_{}^{B}_{t},t,}_{}^ {0,1}(_{t},t).\] (14)Here, the function \(}_{}^{0,1}(_{t},t)\) returns predictions of both \(_{0}\) and \(_{1}\). Alternatively, the data point \(_{1}\) can be reused as conditioning for intermediate steps of the reverse process instead of predicting both points, a strategy further detailed in Appendix C.2.

When \(F_{}\) in eq. (13) is parameterized such that \(q_{}(_{0}|_{0},_{1})( -_{0})\) and \(q_{}(_{1}|_{0},_{1}) p(_{1}|_{1})\) (see Appendix C.1 for parameterization details of the NFBM forward process), we propose training NFBM by minimizing the following objective:

\[=_{u(t)q(_{0},_{1})q_{}( _{t}|_{0},_{1})}[^{2} (t)}_{}^{B}(_{t},t,_{0},_{1})-_{,}(_{t},t)_{2}^{2}].\] (15)

The derivation of this objective is provided in Appendix A.2. This objective shares key properties with the NFDM objective, it provides a variational bound on the model's log-likelihood \( p_{,}(_{0})\), and is compatible with the simulation-free paradigm. Furthermore, it allows for sampling with various levels of stochasticity (by adjusting \(g_{}(t)\)) including deterministic sampling (when \(g_{}(t) 0\)).

Thus, the NFDM framework not only enables the construction of generative models, but also facilitates learning bridge models between two data distributions.

## 5 Restricted NFDM

We introduce NFDM as a powerful framework that enables learning of the forward process. However, there is in general an infinite number of forward and reverse processes that correspond to each other. In this section we discuss how the flexibility of NFDM (and NFBM) allows learning generative dynamic with user-specified beneficial properties.

Suppose our objective is to learn straight generative ODE trajectories, which can be highly beneficial, as it enables generation with far fewer steps in the ODE solver. One approach is to introduce penalties on the curvature of the reverse process's trajectories and let the learnable forward process to adapt to align with the reverse process. As a result, we would obtain a generative process that not only corresponds to the forward process, ensuring accurate data generation, but also features the desired property of straight trajectories. We discuss NFDM with restrictions in more details in Appendix D.1.

We propose learning a model with curvature penalty as suggested by :

\[_{}=+_{}, _{}=_{u(t)q_{}( ,_{t})}\|_{,}(_ {t},t)}{dt}\|_{2}^{2}.\] (16)

We refer to this variant as NFDM-OT1. \(_{}\) is an additional curvature loss that penalizes the second time derivative of the generative ODE trajectories. \(\) is estimated as in Section 3.3, whereas when calculating \(_{}\) we set \(g_{}(t) 0\). \(_{} 0\) ensures that the generative trajectories are straight lines. In our experiments, we set \(=10^{-2}\). Empirical evidence suggests that higher values of \(\) lead to slower convergence of the model. The specifics of the curvature loss are elaborated upon in more detail in Appendix D.2.

We would like to emphasise that the purpose of this section is to provide an example of how the NFDM framework allows learning dynamics with specific properties. Not to propose the best way for learning generative straight-line generative dynamics. In addition, we note that conventional diffusion models are incapable of handling such penalization strategies. In diffusion models with a fixed forward process, the target for the reverse process is predetermined and the corresponding ODE trajectories are highly curved. Hence, imposing constraints on the reverse process, such as trajectory straightness, would lead to a mismatch with the forward process and, consequently, an inability to generate samples with high data fidelity.

## 6 Experiments

We first showcase results demonstrating that NFDM consistently achieves better likelihood compared to baselines, obtaining state-of-the-art diffusion modeling results on the CIFAR-10  and downsampled ImageNet [11; 67] datasets. Then, we explore the NFDM-OT modification, which penalizesthe curvature of the deterministic generative trajectories. The NFDM-OT reduces trajectory curvature, significantly lowering the number of generative steps required for sampling. Finally, we demonstrate the NFBM modification, learning bridges on the AFHQ  dataset and several synthetic examples.

We report the NLL in Bits Per Dimension (BPD) and sample quality measured by the Frechet Inception Distance (FID) score . The NLL is calculated by integrating the ODEs using the RK45 solver , with all NLL metrics computed on test data. For FID, we provide the average over 50k generated images.

Unless otherwise stated, we parameterized the forward process of NFDM and NFBM such that \(q_{}(_{t}|)\) is a Gaussian with learnable mean and covariance (see details in Appendices B.5 and C.1). For a detailed description of parameterizations and other experimental details, please refer to Appendix F.

The primary aim of the experiments with NFDM-OT and NFBM is not to introduce a novel model that surpasses others in few-step generation or learning bridges, but rather to showcase the ability of the NFDM framework to learn generative dynamics with specific properties. The straightness of the trajectories is just one example of such a property. We leave it for future research to explore different parameterizations and modified objectives for NFDM that may yield even better results.

Please refer to appendix G.3 for generated samples. The code is available at https://github.com/GrigoryBartosh/neural_diffusion.

### Likelihood Estimation

For the first experiments, in addition to Gaussian we also provide results with non-Gaussian parameterization of the forward process (see Appendix B.5). Table 1 summarizes the NLL results on the CIFAR-10 and two downsampled ImageNet datasets. Notably, NFDM outperforms diffusion-based baselines on all three datasets with both parameterizations, achieving state-of-the-art performance. The improved performance due to NFDM is a natural progression for the following reasons.

   Model & **CIFAR10** & **ImageNet 32** & **ImageNet 64** \\  DDPM  & \(3.69\) & & \\ Score SDE  & \(2.99\) & & \\ Improved DDPM  & \(2.94\) & & \(3.54\) \\ VDM  & \(2.65\) & \(3.72\) & \(3.40\) \\ Score Flow  & \(2.83\) & \(3.76\) & \\ Flow Matching  & \(2.99\) & \(3.53\) & \(3.31\) \\ Stochastic Interp.  & \(2.99\) & \(3.48\) & \\ i-DODE  & \(2.56\) & \(3.43\) & \\ NDM  & \(2.70\) & \(3.55\) & \(3.35\) \\ MuLAN  & \(2.55\) & \(3.67\) & \\ NFDM (**Gaussian**\(q_{}(_{t}|)\)) & \(2.49\) & \(3.36\) & \(\) \\ NFDM (**non-Gaussian**\(q_{}(_{t}|)\)) & \(\) & \(\) & \(\) \\   

Table 1: Comparison of NFDM results with baselines on density estimation tasks. We present results in terms of BPD, lower is better. NFDM achieves state-of-the-art results across all three benchmark tasks.

Figure 1: Comparison of trajectories between the data distribution (on the left) and the prior distribution (on the right), as learned by conventional diffusion and NFDM-OT.

First, it is well-established [59; 76] that diffusion models exhibit improved likelihood estimation when trained with the full ELBO objective. The objective \(\) (eq. (12)) used for training the NFDM is also a variational bound on the likelihood.

Second, diffusion models can be seen as hierarchical VAEs. From this perspective, most baselines in Table 1 resemble VAEs with either fixed or constrained variational distributions. In contrast, the NFDM extends beyond these baselines by providing a more flexible variational distribution. That is true even for the Gaussian parameterization with a mean and covariance that is non-linear in \(\) and \(t\). This flexibility allows the NFDM to better conform to the reverse process, consequently enhancing likelihood estimation.

### Straight Trajectories

We next evaluate NFDM-OT, designed to penalize the deterministic generative trajectory curvature. First, we compare NFDM-OT with a conventional continuous-time diffusion model. Figure 0(a) illustrates deterministic trajectories between a two-dimensional data distribution and a unit Gaussian distribution learnt by a conventional diffusion model . Figure 0(b) depicts trajectories learnt by NFDM-OT. Conventional diffusion, being constrained in its forward process, learns highly curved trajectories, whereas NFDM-OT successfully learns straight generative trajectories as desired. In Appendix G.1 we provide some additional result, demonstrating the importance of learnable forward process for NFDM-OT.

Then, we present results of NFDM-OT on image datasets. Table 2 reports the FID scores for \(2\), \(4\), and \(12\) Number of Function Evaluations (NFE) with respect to the function \(_{,}\) (eq. (11)). In this experiment, we employ Euler's method for sampling integration. For the specified NFEs, NFDM-OT demonstrates superior sample quality compared to other approaches with similar NFE values. Specifically, NFDM-OT outperforms approaches specifically designed to minimize the curvature of generative trajectories [44; 32].

Importantly, NFDM-OT is trained with an ELBO-based objective (eq. (12)), which is known to yield higher FID scores for diffusion models [59; 76]. In contrast, some of the approaches listed in Table 2 are trained with different objectives, leading to improved FID scores. Even so, for comparable NFE values NFDM-OT still achieves superior results. We provide additional results of NFDM-OT in Appendix G.2.

    &  &  &  \\ Model & NFE \(\) & FID \(\) & NFE \(\) & FID \(\) & NFE \(\) & FID \(\) \\  DDPM (\(_{}\))  & \(1000\) & \(3.17\) & & & & \\ DDPM (ELBO)  & \(1000\) & \(13.51\) & & & & \\ Flow Matching  & \(142\) & \(6.35\) & \(122\) & \(5.02\) & \(138\) & \(14.14\) \\  DDIM  & \(10\) & \(13.36\) & & & & \\ DPM Solver  & \(12\) & \(5.28\) & & & & \\  & \(24\) & \(2.75\) & & & & \\  Trajectory Curvature Minimization  & \(5\) & \(18.74\) & & & & \\ Multisample Flow Matching  & & & \(4\) & \(17.28\) & \(4\) & \(38.45\) \\  & \(12\) & \(7.18\) & \(12\) & \(17.6\) & \(12\) & \(17.6\) \\  NFDM-OT (**this paper**) & \(2\) & \(12.44\) & \(2\) & \(9.83\) & \(2\) & \(27.70\) \\  & \(4\) & \(7.76\) & \(4\) & \(6.13\) & \(4\) & \(17.28\) \\  & \(12\) & \(5.20\) & \(12\) & \(4.11\) & \(12\) & \(11.58\) \\   

Table 2: Summary of FID results for few-step generation. The table is divided into three sections, based on different types of methods: those that do not minimize curvature, solvers for pretrained models, and models that specifically aim to minimize curvature. For the DDPM, we include results corresponding to two distinct objectives: the full ELBO-based objective and a simplified objective (\(_{}\)). NFDM-OT outperforms baselines with comparable NFE values.

### Bridges

We consider the task of translating images of dogs into images of cats from the downsampled AFHQ dataset. Table 3 reports the FID scores on images sampled with \(1000\) steps. Notably, NFBM outperforms the baselines, demonstrating the effectiveness of our framework for learning bridges.

Finally, we demonstrate that the framework flexibility also allows for learning dynamics with specific properties in the case of NFBM. To illustrate this, we adopted the strategy discussed in Section 5 and learn NFBM with an additional penalty to avoid obstacles (see details in Appendix F.2). Figure 2 visualizes the learned stochastic trajectories. It is evident that NFBM has efficiently learned to translate distributions while avoiding obstacles.

## 7 Related Work

Diffusion models, originally proposed by , have evolved significantly through subsequent developments [60; 20]. These advancements have resulted in remarkable generative quality for high-dimensional data distributions [12; 48]. Nevertheless, conventional diffusion models, typically relying on a linear Gaussian forward process, may not optimally fit some data distributions. To address this, alternative forward processes have been explored, such as alternative linear transformations [16; 55], combining blurring with Gaussian noise injection [46; 9; 23], diffusion in the wavelet spectrum , and forward processes based on the exponential family . These models are limited by their fixed forward processes and may be seen as specific instances of NFDM.

Some studies have focused on making the forward process learnable. Approaches include a learnable noise injection schedule [28; 39; 49] and learning data transformations like time-independent transformations based on VAEs [66; 47] and normalizing flows , or time-dependent transformations [18; 55; 40; 4]. These methods can also be considered special cases of NFDM with specific transformations \(F_{}\) (eq. (7)).

To improve the sampling efficiency, in orthogonal line of works alternative sampling methods have been studied [62; 35; 38; 53]. Additionally, distillation techniques have been applied [50; 58; 75; 72] to enhance sampling speed, albeit at the cost of training additional models. Most of these approches are compatible with NFDM and may be combined for further gains.

Another line of works proposed technics for learning straighter generative trajectories. However, these approaches requires distillation [37; 36], solving unstable min-max problem , estimating an optimal coupling over minibatches [44; 64] of the entire dataset, which, for large datasets, may become uninformative, or can be viewed as specific instances [32; 52] of NFDM.

The connections between NFDM and some related works is discussed further in Appendix E.

## 8 Conclusion and Limitations

In this paper we introduced NFDM, a novel simulation-free framework for improved diffusion modeling through a learnable forward processes. NFDM outperforms baseline diffusion models on

Figure 2: Trajectories of NFBM between two-dimensional data distributions, trained with an additional penalty to avoid obstacles.

standard benchmarks, showcasing its effectiveness. However, NFDM is not just another model, but rather it is a versatile framework that facilitates the predefined specification and learning of a forward process. Similarly, the penalty on the curvature of deterministic trajectories discussed in Section 5 represents just one specific example of the restrictions that NFDM can accommodate.

Nonetheless, the advantages of NFDM come with certain trade-offs. Once the forward process is parameterized using a neural network, this leads to approximately 2.2 times longer optimization iteration of NFDM compared to conventional diffusion models. Additionally, as discussed in Section 3.1, NFDM is limited to forward process parameterisations with functions that match specific conditions.

Despite these challenges, we are optimistic about the potential of NFDM. NFDM opens up significant prospects for exploring new generative dynamics. We believe that with alternative parameterizations, modifications of the objective, and the integration of orthogonal approaches, NFDM has the potential to achieve even better results across a range of data modalities.