# The Target-Charging Technique

for Privacy Analysis across Interactive Computations

 Edith Cohen

Google Research and Tel Aviv University

edith@cohenwang.com &Xin Lyu

UC Berkeley

lyuxin1999@gmail.com

###### Abstract

We propose the _Target Charging Technique_ (TCT), a unified privacy analysis framework for interactive settings where a sensitive dataset is accessed multiple times using differentially private algorithms. Unlike traditional composition, where privacy guarantees deteriorate quickly with the number of accesses, TCT allows computations that don't hit a specified _target_, often the vast majority, to be essentially free (while incurring instead a small overhead on those that do hit their targets). TCT generalizes tools such as the sparse vector technique and top-\(k\) selection from private candidates and extends their remarkable privacy enhancement benefits from noisy Lipschitz functions to general private algorithms.

## 1 Introduction

In many practical settings of data analysis and optimization, the dataset \(D\) is accessed multiple times interactively via different algorithms \((_{i})\), so that \(_{i}\) depends on the transcript of prior responses \((_{j}(D))_{j<i}\). When each \(_{i}\) is privacy-preserving, we are interested in tight end-to-end privacy analysis. We consider the standard statistical framework of differential privacy introduced in . _Composition_ theorems  are a generic way to do that and achieve overall privacy cost that scales linearly or (via "advanced" composition) with square-root dependence in the number of private computations. We aim for a broad understanding of scenarios where the overall privacy bounds can be lowered significantly via the following paradigm: Each computation is specified by a private algorithm \(_{i}\) together with a _target_\(_{i}\), that is a subset of its potential outputs. The total privacy cost depends only on computations where the output hits its target, that is \(_{i}(D)_{i}\). This paradigm is suitable and can be highly beneficial when (i) the specified targets are a good proxy for the actual privacy exposure and (ii) we expect the majority of computations to not hit their target, and thus essentially be "free" in terms of privacy cost.

The Sparse Vector Technique (SVT)  is a classic special case. SVT is designed for computations that have the form of approximate threshold tests applied to Lipschitz functions. Concretely, each such AboveThreshold test is specified by a \(1\)-Lipschitz function \(f\) and a threshold value \(t\) and we wish to test whether \(f(D) t\). The textbook SVT algorithm compares a noisy value with a noisy threshold (independent Laplace noise for the values and threshold noise that can be updated only after positive responses). Remarkably, the overall privacy cost depends only on positive responses: Roughly, composition is applied to twice the number of positive responses instead of to the total number of computations. In our terminology, the target of each test is a positive response. SVT privacy analysis benefits when the majority of AboveThreshold test results are negative (and hence "free"). This makes SVT a key ingredient in a range of methods : private multiplicative weights , Propose-Test-Release , fine privacy analysis via distance-to-stability , model-agnostic private learning ,and designing streaming algorithms that are robust to adaptive inputs .

We aim to extend such target-hits privacy analysis to interactive applications of _general_ private algorithms (that is, algorithms that provide privacy guarantees but have no other assumptions): private tests, where we would hope to incur privacy cost only for positive responses, and private algorithms that return more complex outputs, e.g., vector average, cluster centers, a sanitized dataset, or a trained ML model, where the goal is to incur privacy cost only when the output satisfies some criteria. Textbook SVT, however, is less amenable to such extensions: First, SVT departs from the natural paradigm of applying private algorithms to the dataset and reporting the output. A natural implementation of private AboveThreshold tests would add Laplace noise to the value and compare with the threshold. Instead, SVT takes as input the Lipschitz output of the non-private algorithms with threshold value and the privacy treatment is integrated (added noise both to values and threshold). The overall utility and privacy of the complete interaction are analyzed with respect to the non-private values, which is not suitable when the algorithms are already private. Moreover, the technique of using a hidden shared threshold noise across multiple AboveThreshold tests is specific for Lipschitz functions, introduces dependencies between responses, and more critically, results in separate privacy costs for reporting noisy values (that is often required by analytics tasks ).

Consider private tests. The natural paradigm is to sequentially choose a test, apply it, and report the result. The hope is to incur privacy loss only on positive responses. Private testing was considered in prior works [21; 7] but in ways that departed from this paradigm:  processed the private tests so that a positive answer is returned only when the probability \(p\) of a positive response by the private test is very close to 1. This seems unsatisfactory: If the design goal of the private testing algorithm was to report only very high probabilities, then this could have been more efficiently integrated into the design, and if otherwise, then we miss out on acceptable positive responses with moderately high probabilities (e.g. 95%).

Consider now _Top-\(k\) selection_, which is a basic subroutine in data analysis, where input algorithms \((_{i})_{i[m]}\) (aka candidates) that return results with quality scores are provided in a _batch_ (i.e., non interactively). The selection returns the \(k\) candidates with highest quality scores on our dataset. The respective private construct, where the data is sensitive and the algorithms are private, had been intensely studied [23; 17; 32]. The top-\(k\) candidates can be viewed as target-hits and we might hope for privacy cost that is close to a composition over \(k\) private computations, instead of over \(m k\). The natural approach for top-\(k\) is _one-shot_ (Algorithm 3), where each algorithm is applied once and the responses with top-\(k\) scores are reported. Prior works on private selection that achieve this analysis goal include those [9; 29] that use the natural one-shot selection but are tailored to Lipschitz functions (apply the Exponential Mechanism  or the Report-Noise-Max paradigm ) and works [21; 28; 7] that do apply with general private algorithms but significantly depart from the natural one-shot approach: They make a randomized number of computations that is generally much larger than \(m\), with each \(_{i}\) invoked multiple times or none. The interpretation of the selection deviates from top-\(1\) and does not naturally extend to top-\(k\). We seek privacy analysis that applies to one-shot top-\(k\) selection with candidates that are general private algorithms.

The natural interactive paradigm and one-shot selection are simple, interpretable, and general. The departures made in prior works were made for a reason: Simple arguments (that apply with both top-1 one-shot private selection  and AboveThreshold tests) seem to preclude efficient target-charging privacy analysis: With pure-DP, if we perform \(m\) computations that are \(\)-DP (that is, \(m\) candidates or \(m\) tests), then the privacy parameter value for a pure DP bound is \((m)\). With approximate-DP, and even a single "hit," the parameter values are \((((1/)),)\). The latter suggests a daunting overhead of \(O((1/))\) instead of \(O(1)\) per "hit." We circumvent the mentioned limitations by taking approximate DP to be a reasonable relaxation and additionally, aim for application regimes where many private computations are performed on the same dataset and we expect multiple, say \(((1/))\), "target hits" (e.g. positive tests and sum of the \(k\)-values of selections). With these relaxations in place, we seek a unified target-charging analysis (e.g. privacy charge that corresponds to \(O(1)\) calls per "target hit") that applies with the natural paradigm across interactive calls and top-\(k\) selections.

## 2 Overview of Contributions

We overview our contributions (proofs and details are provided in the Appendix). We introduce the _Target-Charging Technique (TCT)_ for privacy analysis over interactive private computations (see Algorithm 1). Each computation performed on the sensitive dataset \(D\) is specified by a pair of private algorithm \(_{i}\) and _target_\(_{i}\). The interaction is halted after a pre-specified number \(\) of computations satisfy \(_{i}(D)_{i}\). We define targets as follows:

**Definition 2.1** (\(q\)-Target).: _Let \(:X^{n}\) be a randomized algorithm. For \(q(0,1]\) and \(>0\), we say that a subset \(\) of outcomes is a \(q\)-Target of \(\), if the following holds: For any pair \(D^{0}\) and \(D^{1}\) of neighboring data sets, there exist \(p\), and three distributions \(\), \(^{0}\) and \(^{1}\) such that_

1. _The distributions_ \((D^{0})\) _and_ \((D^{1})\) _can be written as the following mixtures:_ \[(D^{0})  p+(1-p)^{0},\] \[(D^{1})  p+(1-p)^{1}.\]
2. \(^{0},^{1}\) _are_ \((,0)\)_-indistinguishable,_
3. \(([^{0}],[^{1}]) q\)_._

The effectiveness of a target as a proxy of the actual privacy cost is measured by its \(q\)-value where \(q(0,1]\). We interpret \(1/q\) as the _overhead factor_ of the actual privacy exposure per target hit, that is, the number of private accesses that correspond to a single target hit. Note that an algorithm with a \(q\)-target for \(>0\) must be \((,0)\)-DP and that any \((,0)\)-DP algorithm has a \(1\)-target, as the set of all outcomes \(=\) is a \(1\)-target (and hence also a \(q\)-target for any \(q 1\)). More helpful targets are "smaller" (so that we are less likely to be charged) with a larger \(q\) (so that the overhead per charge is smaller). We establish the following privacy bounds.

**Lemma 2.2** (simplified meta privacy cost of target-charging).: _The privacy parameters of Algorithm 1 are \((^{},)\) where \(^{}\) and \(=e^{-()}\)._

_Alternatively, we obtain parameter values \((^{},^{})=(f_{}(r,),f_{ }(r,)+e^{-()})\) where \(r/q\) and \((f_{}(r,),f_{}(r,))\) are privacy parameter values for advanced composition  of \(r\)\(\)-DP computations._

Proof details for a more general statement that also applies with approximate DP algorithms are provided in Section B (in which case the \(\) parameters of all calls simply add up). The idea is simple: We compare the execution of Algorithm 4 on two neighboring data sets \(D^{0},D^{1}\). Given a request \((,)\), let \(p\), \(\), \(\), \(^{0},^{1}\) be the decomposition of \(\) w.r.t. \(D^{0},D^{1}\) given by Definition 2.1. Then, running \(\) on \(D^{0},D^{1}\) can be implemented in the following equivalent way: we flip a \(p\)-biased coin. With probability \(p\), the algorithm samples from \(\) and returns the result, without accessing \(D^{0},D^{1}\) at all (!). Otherwise, the algorithm needs to sample from \(^{0}\) or \(^{1}\), depending on whether the private data is \(D^{0}\) or \(D^{1}\). However, by Property 3 in Definition 2.1, there is a probability of at least \(q\) that Algorithm 1 will "notice" the privacy-leaking computation by observing a result in the target set \(\). If this indeed happens, the algorithm increments the counter. On average, each counter increment corresponds to \(\) accesses to the private data. Therefore we use the number of target hits (multiplied by \(1/q\)) as a proxy for the actual privacy leak. Finally, we apply concentration inequalities to obtain high confidence bounds on the probability that the actual number of accesses significantly exceeds its expectation of \(/q\). The multiplicative error decreases when the number \(\) of target hits is larger. In the regime \(>(1/)\), we amortize the mentioned \(O((1/))\) overhead of the natural paradigm so that each target hit results in privacy cost equivalent to \(O(1/q)\) calls. In the regime of very few target hits (e.g., few private tests or private selections), we still have to effectively "pay" for the larger \(=((1/))\), but TCT still has some advantages over alternative approaches, due to its use of the natural paradigm and its applicability with general private algorithms.

TCT is simple but turns out to be surprisingly powerful due to natural targets with low overhead. We present an expansive toolkit that is built on top of TCT and describe application scenarios.

### NotPrior targets

A NotPrior target of an \(\)-DP algorithm is specified by any outcome of our choice (the "prior") that we denote by \(\). The NotPrior target is the set of all outcomes except \(\). Surprisingly perhaps, this is an effective target (See Section C for the proof that applies also with approximate-DP):

**Lemma 2.3** (Property of a NotPrior target).: _Let \(:X\{\}\), where \(\), be an \(\)-DP algorithm. Then the set of outcomes \(\) constitutes an \(+1}\)-target for \(\)._Note that for small \(\), we have \(q\) approaching \(1/2\) and thus the overhead factor is close to \(2\). The TCT privacy analysis is beneficial over plain composition when the majority of all outcomes in our interaction match their prior \(\). We describe application scenarios for NotPrior targets. For most of these scenarios, TCT is the only method we are aware of that provides the stated privacy guarantees in the general context.

Private testingA private test is a private algorithm with a Boolean output. By specifying our prior to be a negative response, we obtain (for small \(\)) an overhead of \(2\) for positive responses, which matches SVT. TCT is the only method we are aware of that provides SVT-like guarantees with general private tests.

Pay-only-for-changeWhen we have a prior on the result of each computation and expect the results of most computations to agree with their respective prior, we set \(\) to be our prior. We report all results but pay only for those that disagree with the prior. We describe some use cases where paying only for change can be very beneficial (i) the priors are results of the same computations on an older dataset, so they are likely to remain the same (ii) In streaming or dynamic graph algorithms, the input is a sequence of updates where typically the number of changes to the output is much smaller than the number of updates. Differential privacy was used to obtain algorithms that are robust to adaptive inputs  by private aggregation of non-robust copies. The pay-only-for-change allows for number of changes to output (instead of the much larger number of updates) that is quadratic in the number of copies. Our result enables such gain with any private aggregation algorithm (that is not necessarily in the form of AboveThreshold tests).

### Conditional Release

We have a private algorithm \(:X\) but are interested in the output \((D)\) only when a certain condition holds (i.e., when the output is in \(\)). The condition may depend on the interaction transcript thus far (depend on prior computations and outputs). We expect most computations not to meet their release conditions and want to be "charged" only for the ones that do. Recall that with differential privacy, not reporting a result also leaks information on the dataset, so this is not straightforward. We define \(A_{}:=(,)\) as the operation that inputs a dataset \(D\), computes \(y(D)\). If \(y\), then publish \(y\) and otherwise publish \(\). We show that this operation can be analysed in TCT as a call with the algorithm and NotPrior target pair \((_{},)\), that is, a target hit occurs if and only if \(y\):

**Lemma 2.4** (ConditionalRelease privacy analysis).: \(_{}\) _satisfies the privacy parameters of \(\) and \(\) is a_ NotPrior _target of \(_{}\)._

Proof.: \(_{}\) processes the output of the private algorithm \(\) and thus from post processing property is also private with the same privacy parameter values. Now note that \(\) is a NotPrior target of \(\), with respect to prior \(\). 

We describe some example use-cases:

(i) Private learning of models from the data (clustering, regression, average, ML model) but we are interested in the result only when its quality is sufficient, say above a specified threshold, or when some other conditions hold.

(ii) Greedy coverage or representative selection type applications, where we incur privacy cost only for selected items. To do so, we condition the release on the "coverage" of past responses. For example, when greedily selecting a subset of features that are most relevant or a subset of centers that bring most value.

(iii) Approximate AboveThreshold tests on Lipschitz functions, with release of above-threshold noisy values: As mentioned, SVT incurs additional privacy cost for the reporting whereas TCT (using ConditionalRelease) does not, so TCT benefits in the regime of sufficiently many target hits.

(iv) AboveThreshold tests with sketch-based approximate distinct counts: Distinct counting sketches  meet the privacy requirement by the built-in sketch randomness . We apply ConditionalRelease and set \(\) to be above threshold values. In comparison, despite the function (distinct count) being 1-Lipschitz, the use of SVT for this task incurs higher overheads in utility (approximation quality) and privacy: Even for the goal of just testing, a direct use of SVT treats the approximate value as the non-private input, which reduces accuracy due to the additional added noise. Treating the reported value as a noisy Lipschitz still incurs accuracy loss due to the threshold noise, threshold noise introduces bias, and analysis is complicated by the response not following a particular noise distribution. For releasing values, SVT as a separate distinct-count sketch is needed to obtain an independent noisy value , which increases both storage and privacy costs.

### Conditional Release with Revisions

We present an extension of Conditional Release that allows for followup _revisions_ of the target. The initial ConditionalRelease and the followup ReviseCR calls are described in Algorithm 2. The ConditionalRelease call specifies a computation identifier \(h\) for later reference, an algorithm and a target pair \((,)\). It draws \(r_{h}(D)\) and internally stores \(r_{h}\) and a current target \(_{h}\). When \(r_{h}\) then \(r_{h}\) is published and a charge is made. Otherwise, \(\) is published. Each (followup) ReviseCR call specifies an identifier \(h\) and a disjoint extension \(^{}\) to its current target \(_{h}\). If \(r_{h}^{}\), then \(r_{h}\) is published and a charge is made. Otherwise, \(\) is published. The stored current target for computation \(h\) is augmented to include \(^{}\). Note that a target hit occurs at most once in a sequence of (initial and followup revise) calls and if and only if the result of the initial computation \(r_{h}\) is in the final target \(_{h}\).

```
// Initial Conditional Release call: Analysed in TCT as a \((,)\)-DP algorithm \(\)\(\) and NotPrior target \(\) FunctionConditionalRelease(\(h,,\)):// unique identifier \(h\), an \((,)\)-DP algorithm \(\), \(\) \(_{h}\) // Current target for computation \(h\) TCT Charge for \(\) // If \(>0\), see Section B \(r_{h}(D)\) // Result for computation \(h\) if\(r_{h}_{h}\)then // publish and charge only if outcome is in \(_{h}\) Publish \(r_{h}\) TCT Charge for a NotPrior target hit of an \(\)-DP algorithm else  Publish \(\)
```

**Algorithm 2**Conditional Release and Revise Calls

We show the following (Proof provided in Section D):

**Lemma 2.5** (Privacy analysis for Algorithm 2).: _Each_ ReviseCR _call can be analysed in TCT as a call to a \(2\)-DP algorithm with a_ NotPrior _target \(^{}\)._

Thus, the privacy cost of conditional release followed by a sequence of revise calls is within a factor of 2 (due to the doubled privacy parameter on revise calls) of a single ConditionalRelease call made with the final target. The revisions extension of conditional release facilitates our results for private selection, which are highlighted next.

### Private Top-\(k\) Selection

Consider the nature one-shot top-\(k\) selection procedure as shown in Algorithm 3: We call each algorithm once and report the \(k\) responses with the highest quality scores. We establish the following:

**Lemma 2.6** (Privacy of One-Shot Top-\(k\) Selection).: _Consider one-shot top-\(k\) selection (Algorithm 3) on a dataset \(D\) where \(\{_{i}\}\) are \((,_{i})\)-DP. This selection can be simulated exactly in TCT by a sequence of calls to \((2,)\)-DP algorithms with_ NotPrior _targets that has \(k\) target hits._

_As a corollary, assuming \(<1\), Algorithm 3 is \((O(),2^{-(k)}++_{i}_{i})\)-DP for every \(>0\)._

To the best of our knowledge, our result is the first such bound for one-shot selection from general private candidates. For the case when the only computation performed on \(D\) is a single top-\(1\) selection, we match the "bad example" in  (see Theorem J.1). In the regime where \(k>(1/)\) our bounds generalize those specific to Lipschitz functions in  (see Section J). Moreover, Lemma 2.6 allows for a unified privacy analysis of interactive computations that are interleaved with one-shot selections. We obtain \(O(1)\) overhead per target hit when there are \(((1/))\) hits in total.

```
0: A dataset \(D\). Candidate algorithms \(_{1},,_{m}\). Parameter \(k m\). \(S\)for\(i=1,,m\)do \((y_{i},s_{i})_{i}(D)\)\(S S\{(i,y_{i},s_{i})\}\)return\(L\) the top-\(k\) triplets from \(S\), by decreasing \(s_{i}\)
```

**Algorithm 3**One-Shot Top-\(k\) Selection

The proofs of Lemma 2.6 and implications to selection tasks are provided in Section J. The proof utilizes Conditional Release with revisions (Section 2.3).

#### 2.4.1 Selection using Conditional Release

We analyze private selection procedures using conditional release (see Section J for details). First note that ConditionalRelease calls (without revising) suffice for _one-shot above-threshold_ selection (release all results with a quality score that exceeds a pre-specified threshold \(t\)), with target hits only on what was released: We simply specify the release condition to be \(s_{i}>t\). What is missing in order to implement one-shot top-\(k\) selection is an ability to find the "right" threshold (a value \(t\) so that exactly \(k\) candidates have quality scores above \(t\)), while incurring only \(k\) target hits. The revise calls provide the functionality of lowering the threshold of previous conditional release calls (lowering the threshold amounts to augmenting the target). This functionality allows us to simulate a sweep of the \(m\) results of the batch in the order of decreasing quality scores. We can stop the sweep when a certain condition is met (the condition must be based on the prefix of the ordered sequence that we viewed so far) and we incur target hits only for the prefix. To simulate a sweep, we run a high threshold conditional release of all \(m\) candidates and then incrementally lower the threshold using sets of \(m\) revise calls (one call per candidate). The released results are in decreasing order of quality scores. To prove Lemma 2.6 we observe that the one-shot top-\(k\) selection (Algorithm 3) is simulated exactly by such a sweep that halts after \(k\) scores are released (the sweep is only used for analysis).

As mentioned, with this approach we can apply _any stopping condition that depends on the prefix_. This allows us to use data-dependent selection criteria. One natural such criteria (instead of usinga rigid value of \(k\)) is to choose \(k\) when there is a large gap in the quality scores, that the \((k+1)\)st quality score is much lower than the \(k\)th score . This criterion can be implemented using a one-shot algorithm and analyzed in the same way using an equivalent sweep. Data-dependent criteria are also commonly used in applications such as clustering (choose "the right" number of clusters according to gap in clustering cost) and greedy selection of representatives.

### Best of multiple targets

_Multi-target_ charging is a simple but useful extension of Algorithm 1 (that is "single target"). With \(k\)-TCT, queries have the form \(,(_{i})_{i[k]}\) where \(_{i}\) for \(i[k]\) are \(q\)-targets (we allow targets to overlap). The algorithm maintains \(k\) counters \((C_{i})_{i[k]}\). For each query, for each \(i\), we increment \(C_{i}\) if \(r_{i}\). We halt when \(_{i}C_{i}=\).

The multi-target extension allows us to flexibly reduce the total privacy cost to that of the "best" among \(k\) target indices _in retrospect_ (the one that is hit the least number of times). Interestingly, this extension is almost free in terms of privacy cost: The number of targets \(k\) only multiplies the \(\) privacy parameter (see Section B.1 for details).

### BetweenThresholds in TCT

The BetweenThresholds classifier refines the AboveThreshold test: The goal is to report if the noisy Lipschitz value is below, between, or above two thresholds \(t_{l}<t_{r}\). We aim for privacy loss that only depends on between outcomes. An SVT-based BetweenThresholds was proposed by  (with noisy value and thresholds). Their analysis required the gap size to satisfy \(t_{r}-t_{l}(12/)((10/)+(1/)+1)\).

We consider the "natural" BetweenThresholds classifier that compares the Lipschitz value with added \((1/)\) noise to the two threshold values and reports the result. This is \(\)-DP and we show (see Section G) that the between outcome is a target with \(q(1-e^{-(t_{r}-t_{l})})+1}\). This \(q\) value is at most that of a NotPrior target (\(+1}\)) and degrades smoothly as the gap decreases. Importantly, there is almost no degradation for fairly small gaps: When \(t_{r}-t_{l} 2/\) (resp. \(3/\)) the \(q\)-value is \(+1}\) (resp \(+1}\)).

One benefit of TCT BetweenThresholds is that it applies with much smaller gaps \(t_{r}-t_{l}\) compared with , also asymptotically. Another benefit that holds even for large gaps (where the SVT variant is applicable) is that the natural algorithm requires lower privacy noise for a given accuracy. In Section H we demonstrate this improvement numerically in that we can answer \( 6\)-\( 95\) fold more queries for the same accuracy requirements compared to . This bring BetweenThresholds into the practical regime.

We can compare an AboveThreshold test with a threshold \(t\) with a BetweenThresholds classifier with \(t_{l}=t-1/\) and \(t_{r}=t+1/\). Surprisingly perhaps, despite BetweenThresholds being _more informative_ than AboveThreshold, as it provides more granular information on the value, its privacy cost is _lower_, and is much lower when values are either well above or well below the thresholds. Somehow, the addition of the third between outcome to the test tightened the privacy analysis! A natural question is whether we can extend this benefit more generally - inject a "boundary outcome" as a target when our private algorithm does not have one, then tighten the privacy analysis when queries are "far" from the boundary. We introduce next a method that achieves this goal.

### The Boundary Wrapper Method

When the algorithm is a tester or a classifier, the result is most meaningful when one outcome dominates the distribution \((D)\). Moreover, when performing a sequence of tests or classification tasks we might expect most queries to have high confidence labels (e.g., ). Our hope then is to incur privacy cost that depends only on the "uncertainty," captured by the probability of non-dominant outcomes. When we have for each computation a good prior on which outcome is most likely, this goal can be achieved via NotPrior targets (Section 2.1). When we expect the whole sequence to be dominated by one type of outcome, even when we don't know which one it is, this goal can be achieved via NotPrior with multiple targets (Section 2.5). But these approaches do not apply when a dominant outcome exists in most computations but we have no handle on it.

For a private test \(\), can we choose a moving target _per computation_ to be the value with the smaller probability \(_{b\{0,1\}}[(D)=b]\)? More generally, with a private classifier, can we somehow choose the target to be all outcomes except for the most likely one? Our _boundary wrapper_, described in Algorithm 4, achieves that goal. The privacy wrapper \(\) takes any private algorithm \(\), such as a tester or a classifier, and wraps it to obtain algorithm \(()\). The wrapped algorithm has its outcome set augmented to include one _boundary_ outcome \(\) that is designed to be a \(q\)-target. The wrapper returns \(\) with some probability that depends on the distribution of \((D)\) and otherwise returns a sample from \((D)\) (that is, the output we would get when directly applying \(\) to \(D\)). We then analyse the wrapped algorithm in TCT.

Note that the probability of the wrapper \(\) returning \(\) is at most \(1/3\) and is roughly proportional to the probability of sampling an outcome other than the most likely from \((D)\). When there is no dominant outcome the \(\) probability tops at \(1/3\). Also note that a dominant outcome (has probability \(p[1/2,1]\) in \((D)\)) has probability \(p/(2-p)\) to be reported. This is at least \(1/3\) when \(p=1/2\) and is close to \(1\) when \(p\) is close to \(1\). For the special case of \(\) being a private test, there is always a dominant outcome.

A wrapped AboveThreshold test provides the benefit of BetweenThresholds discussed in Section 2.6 where we do not pay privacy cost for values that are far from the threshold (on either side). This is achieved mechanically without the need to explicitly introduce two thresholds around the given one and defining a different algorithm.

```
0: Dataset \(D=\{x_{1},,x_{n}\} X^{n}\), a private algorithm \(\) \(r^{*}_{}[(D)=r]\)// The most likely outcome of \((D)\) \((D) 1-[(D)=r^{*}]\)// Probability that \(\) does not return the most likely outcome \(c(\{,\})\)// Coin toss for boundary if\(c=1\)thenReturn\(\) else Return\((D)\)// return boundary or value
```

**Algorithm 4**Boundary Wrapper

We show (proofs provided in Section E) that the wrapped algorithm is nearly as private as its baseline:

**Lemma 2.7** (Privacy of a wrapped algorithm).: _If \(\) is \(\)-DP then Algorithm 4 applied to \(\) is \(t()\)-DP where \(t()\)._

**Lemma 2.8** (\(q\)-value of the boundary target).: _The outcome \(\) of a boundary wrapper (Algorithm 4) of an \(\)-DP algorithm is a \(-1}{2(e^{t+t()}-1)}\)-target._

For small \(\) we obtain \(q t()/(2(+t())\). Substituting \(t()=\) we obtain \(q\). Since the target \(\) has probability at most \(1/3\), this is a small loss of efficiency (\(1/6\) factor overhead) compared with composition in the worst case when there are no dominant outcomes.

The boundary wrapper yields light-weight privacy analysis that pays only for the "uncertainty" of the response distribution \((D)\) and can be an alternative to more complex approaches based on smooth sensitivity (the stability of \((D)\) to changes in \(D\)) . Note that the boundary-wrapper method assumes availability of the probability of the most dominant outcome in the distribution \((D)\), when it is large enough. The probability can always be computed without incurring privacy costs (only computation cost) and is readily available with the Exponential Mechanism  or when applying known noise distributions for AboveThreshold, BetweenThresholds, and Report-Noise-Max . In Section F we propose a boundary-wrapper that only uses sampling access to \((D)\).

#### 2.7.1 Applications to Private Learning using Non-privacy-preserving Models

Methods that achieve private learning through training non-private models include Private Aggregation of Teacher Ensembles (PATE)  and Model-Agnostic private learning . The private dataset \(D\) is partitioned into \(k\) parts \(D=D_{1} D_{k}\) and a model is trained (non-privately) on each part. For multi-class classification with \(c\) labels, the trained models can be viewed as functions \(\{f_{i}:[c]\}_{i[k]}\). Note that changing one sample in \(D\) can only change the training set of one of the models. To privately label an example \(x\) drawn from a public distribution, we compute the predictions of all the models \(\{f_{i}(x)\}_{i[k]}\) and consider the counts \(n_{j}=_{i[k]}\{f_{i}(x)=j\}\) (the number of models that gave label \(j\) to example \(x\)) for \(j[c]\). We then privately aggregate to obtain a private label, for example using the Exponential Mechanism  or Report-Noisy-Max . This setup is used to process queries (label examples) until the privacy budget is exceeded. In PATE, the new privately-labeled examples are used to train a new _student_ model (and \(\{f_{i}\}\) are called _teacher_ models). In these applications we seek tight privacy analysis. Composition over all queries - for \(O(1)\) privacy, only allows for \(O(k^{2})\) queries. We aim to replace this with \(O(k^{2})\) "target hits." These works used a combination of methods including SVT, smooth sensitivity, distance-to-instability, and propose-test-release . The TCT toolkit can streamline the analysis:

(i) It was noted in  that when the teacher models are sufficiently accurate, we can expect that \(n_{j} k/2\) on the ground truth label \(j\) on most queries. High-agreement examples are also more useful for training the student model. Moreover, agreement implies stability and lower privacy cost (when accounted through the mentioned methods) is lower. Instead, to gain from this stability, we can apply the boundary wrapper (Algorithm 4) on top of the Exponential Mechanism. Then use \(\) as our target. Agreement queries, where \(_{j}n_{j} k/2\) (or more finely, when \(h=_{j}n_{j}\) and \(n_{h}_{j[k]\{h\}}n_{j}\)) are very unlikely to be target hits.

(ii) If we expect most queries to be either high agreement \(_{j}n_{j} k/2\) or no agreement \(_{j}n_{j} k/2\) and would like to avoid privacy charges also with no agreement, we can apply AboveThreshold test to \(_{j}n_{j}\). If above, we apply the exponential mechanism. Otherwise, we report "Low." The wrapper applied to the combined algorithm returns a label in \([c]\), "Low," or \(\). Note that "Low" is a dominant outcome with no-agreement queries (where the actual label is not useful anyway) and a class label in \([c]\) is a dominant outcome with high agreement. We therefore incur privacy loss only on weak agreements.

### SVT with individual privacy charging

Our TCT privacy analysis simplifies and improves the analysis of SVT with individual privacy charging, introduced by Kaplan et al . The input is a dataset \(D^{n}\) and an online sequence of linear queries that are specified by predicate and threshold value pairs \((f_{i},T_{i})\). For each query, the algorithms reports noisy AboveThreshold test results \(_{x D}f_{i}(x) T\). Compared with the standard SVT, which halts after reporting \(\) positive responses, SVT with individual charging maintains a separate budget counter \(C_{x}\) for each item \(x\). For each query with a positive response, the algorithm only charges items that _contribute_ to this query (namely, all the \(x\)'s such that \(f_{i}(x)=1\)). Once an item \(x\) contributes to \(\) hits (that is, \(C_{x}=\)), it is removed from the data set. This finer privacy charging improves utility with the same privacy budget, as demonstrated by several recent works . We establish the following:

**Theorem 2.9** (Privacy of Algorithm 5).: _Assume \(<1\). Algorithm 5 is \((O(),2^{-()}+)\)-DP for every \((0,1)\)._

Compared with prior work : Algorithm 5 simply adds Laplace noise to obtain \(_{i}=(_{x D}f_{i}(x))+(1/)\) and then tests whether \(_{i} T\), whereas  adds two independent Laplace noises and publishes the approximate sum \(_{i}\) for "Above-Threshold" without incurring additional privacy loss. Our analysis is much simpler (few lines instead of several pages) and significantly tighter, also asymptotically: For the privacy bound in the statement of Theorem 5, our additive error is lower by a \((1/)\) factor. Importantly, our improvement aligns the bounds of SVT with individual privacy charging with those of standard SVT, bringing the former into the practical regime. See Section I for details.

ConclusionWe introduced the Target Charging Technique (TCT), a versatile unified privacy analysis framework that is particularly suitable when a sensitive dataset is accessed multiple times via differentially private algorithms. We provide an expansive toolkit and demonstrate significant improvement over prior work for basic tasks such as private testing and one-shot selection, describe use cases, and list challenges for followup works. TCT is simple with low overhead and we hope will be adopted in practice.

AcknowledgementEdith Cohen is partially supported by Israel Science Foundation (grant no. 1156/23)