# A-NeSI: A Scalable Approximate Method

for Probabilistic Neurosymbolic Inference

 Emile van Krieken

University of Edinburgh

Vrije Universiteit Amsterdam

Emile.van.Krieken@ed.ac.uk

&Thiviyan Thanapalasingam

University of Amsterdam

Jakub M. Tomczak

Eindhoven University of Technology

&Frank van Harmelen, Annette ten Teije

Vrije Universiteit Amsterdam

###### Abstract

We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce _Approximate Neurosymbolic Inference_ (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance.

## 1 Introduction

Recent work in neurosymbolic learning combines neural perception with symbolic reasoning , using symbolic knowledge to constrain the neural network , to learn perception from weak supervision signals , and to improve data efficiency . Many neurosymbolic methods use a differentiable logic such as fuzzy logics  or probabilistic logics . We call the latter _Probabilistic Neurosymbolic Learning (PNL)_ methods. PNL methods add probabilities over discrete truth values to maintain all logical equivalences from classical logic, unlike fuzzy logics . However, performing inference requires solving the _weighted model counting (WMC)_ problem, which is computationally exponential , significantly limiting the kind of tasks that PNL can solve.

We study how to scale PNL to exponentially complex tasks using deep generative modelling . Our method called _Approximate Neurosymbolic Inference_ (A-NeSI), introduces two neural networks that perform approximate inference over the WMC problem. The _prediction model_ predicts the output of the system, while the _explanation model_ computes which worlds (i.e. which truth assignment to a set of logical symbols) best explain a prediction. We use a novel training algorithm to fit both models with data generated using background knowledge: A-NeSI samples symbol probabilities from a prior and uses the symbolic background knowledge to compute likely outputs given these probabilities. We train both models on these samples. See Figure 1 for an overview.

A-NeSI combines all benefits of neurosymbolic learning with scalability. Our experiments on the Multi-digit MNISTAdd problem  show that, unlike other approaches, A-NeSI scales almostlinearly in the number of digits and solves MNISTAdd problems with up to 15 digits while maintaining the predictive performance of exact inference. Furthermore, it can produce explanations of predictions and guarantee the satisfaction of logical constraints using a novel symbolic pruner.

The paper is organized as follows. In Section 3, we introduce A-NeSI. Section 3.1 presents scalable neural networks for approximate inference. Section 3.2 outlines a novel training algorithm using data generated by the background knowledge. Section 3.2.4 extends A-NeSI to include an explanation model. Section 3.3 extends A-NeSI to guarantee the satisfaction of logical formulas. In Section 4, we perform experiments on three Neurosymbolic tasks that require perception and reasoning. Our experiments on Multi-digit MNISTAdd show that A-NeSI learns to predict sums of two numbers with 15 handwritten digits, up from 4 in competing systems. Similarly, A-NeSI can classify Sudoku puzzles in \(9 9\) grids, up from 4, and find the shortest path in \(30 30\) grids, up from 12.

## 2 Problem setup

First, we introduce our inference problem. We will use the MNISTAdd task from  as a running example. In this problem, we must learn to recognize the sum of two MNIST digits using only the sum as a training label. Importantly, we do not provide the labels of the individual MNIST digits.

### Problem components

We introduce four sets representing the spaces of the variables of interest.

1. \(X\) is an input space. In MNISTAdd, this is the pair of MNIST digits \(=,,\).
2. \(W\) is a structured space of \(k_{W}\) different discrete variables \(w_{i}\), each with their own domain. Its elements \( W=W_{1}... W_{k_{W}}\) are _worlds_ or _concepts_ of some \( X\). For \(,,\), the correct world is \(=(5,8)\{0,...,9\}^{2}\).
3. \(Y\) is a structured space of \(k_{Y}\) discrete variables \(y_{i}\). Elements \( Y=Y_{1}... Y_{k_{Y}}\) represent the output of the neurosymbolic system. Given world \(=(5,8)\), the sum is \(13\). We decompose the sum into individual digits, so \(=(1,3)\{0,1\}\{0,...,9\}\).
4. \(^{|W_{1}|}...^{|W_{k_{W}}|}\), where each \(^{|W_{i}|}\) is the probability simplex over the options of the variable \(w_{i}\), is a _belief_. \(\) assigns probabilities to different worlds with \(p(|)=_{i=1}^{k_{W}}_{i,w_{i}}\). That is, \(\) is a parameter for an independent categorical distribution over the \(k_{W}\) choices.

We assume access to some _symbolic reasoning function_\(c:W Y\) that deterministically computes the (structured) output \(\) for any world \(\). This function captures our background knowledge of the problem and we do not impose any constraints on its form. For MNISTAdd, \(c\) takes the two digits \((5,8)\), sums them, and decomposes the sum by digit to form \((1,3)\).

### Weighted Model Counting

Together, these components form the _Weighted Model Counting (WMC)_ problem :

\[p(|)=_{p(|)}[_ {c()=}]\] (1)

Figure 1: Overview of A-NeSI. The forward process samples a belief \(\) from a prior, then chooses a world \(\) for that belief. The symbolic function \(c\) computes its output \(\). The inference model uses the perception model \(f_{}\) to find a belief \(\), then uses the prediction model \(q_{}\) to find the most likely output \(\) for that belief. If we also use the optional explanation model, then \(q_{}\) explains the output.

The WMC counts the _possible_ worlds \(^{1}\) for which \(c\) produces the output \(\), and weights each possible world \(\) with \(p(|)\). In PNL, we want to train a perception model \(f_{}\) to compute a belief \(=f_{}(x)\) for an input \( X\) in which the correct world is likely. Note that _possible_ worlds are not necessarily _correct_ worlds: \(=(4,9)\) also sums to 13, but is not a symbolic representation of \(=(+k_{Y}\), instead of exponential in \(k_{W}\) for exact inference. During testing, we use a beam search to find the most likely prediction from \(q_{,}(,|)\). If the method successfully trained the perception model, then its entropy is low and a beam search should easily find the most likely prediction .

There are no restrictions on the architecture of the inference model \(q_{}\). Any neural network with appropriate inductive biases and parameter sharing to speed up training can be chosen. For instance, CNNs over grids of variables, graph neural networks [27; 48] for reasoning problems on graphs, or transformers for sets or sequences .

We use the prediction model to train the perception model \(f_{}\) given a dataset \(\) of tuples \((,)\). Our novel loss function trains the perception model by backpropagating through the prediction model:

\[_{Perc}(,)=- q_{}(| =f_{}()),,\] (4)

The gradients of this loss are biased due to the error in the approximation \(q_{}\), but it has no variance outside of sampling from the training dataset.

### Training the inference model

We define two variations of our method. The _prediction-only_ variant (Section 3.2.1) uses only the prediction model \(q_{}(|)\), while the _explainable_ variant (Section 3.2.4) also uses the explanation model \(q_{}(|,)\).

Efficient training of the inference model relies on two design decisions. The first is a descriptive factorization of the output space \(Y\), which we discuss in Section 3.2.2. The second is using an informative _belief prior_\(p()\), which we discuss in Section 3.2.3.

We first define the forward process that uses the symbolic function \(c\) to generate training data:

\[p(,|)=p(|)p(| ,)=p(|)_{c()= }\] (5)

We take some example world \(\) and deterministically compute the output of the symbolic function \(c()\). Then, we compute whether the output \(c()\) equals \(\). Therefore, \(p(,|)\) is 0 if \(c()\) (that is, \(\) is not a possible world of \(\)).

The belief prior \(p()\) allows us to generate beliefs \(\) for the forward process. That is, we generate training data for the inference model using

\[p(,)=p()p(|)\] (6) \[, p(,), =c().\]

The belief prior allows us to train the inference model with synthetic data: The prior and the forward process define everything \(q_{}\) needs to learn. Note that the sampling process \(, p(,)\) is fast and parallelisable. It involves sampling from a Dirichlet distribution, and then sampling from a categorical distribution for each dimension of \(\) based on the sampled parameters \(\).

#### 3.2.1 Training the prediction-only variant

In the prediction-only variant, we only train the prediction model \(q_{}(|)\). We use the samples generated by the process in Equation 6. We minimize the expected cross entropy between \(p(|)\)

Figure 2: The training loop of A-NeSI.

and \(q_{}(|)\) over the prior \(p()\):

\[_{Pred}()=- q_{}(c()|), , p(,)\] (7)

See A for a derivation. In the loss function defined in Equation 7, we estimate the expected cross entropy using samples from \(p(,)\). We use the sampled world \(\) to compute the output \(=c()\) and increase its probability under \(q_{}\). Importantly, we do not need to use any data to evaluate this loss function. We give pseudocode for the full training loop in Figure 2.

#### 3.2.2 Output space factorization

The factorization of the output space \(Y\) introduced in Section 2 is one of the key ideas that allow efficient learning in A-NeSI. We will illustrate this with MNISTAdd. As the number of digits \(N\) increases, the number of possible outputs (i.e., sums) is \(2 10^{N}-1\). Without factorization, we would need an exponentially large output layer. We solve this by predicting the individual digits of the output so that we need only \(N 10+2\) outputs similar to . Furthermore, recognizing a single digit of the sum is easier than recognizing the entire sum: for its rightmost digit, only the rightmost digits of the input are relevant.

Choosing the right factorization is crucial when applying A-NeSI. A general approach is to take the CNF of the symbolic function and predict each clause's truth value. However, this requires grounding the formula, which can be exponential. Another option is to predict for what objects a universally quantified formula holds, which would be linear in the number of objects.

#### 3.2.3 Belief prior design

How should we choose the \(\)s for which we train \(q_{,}\)? A naive method would use the perception model \(f_{}\), sample some training data \(_{1},...,_{k}\) and train the inference model over \(_{1}=f_{}(_{1}),...,_{k}=f_{}(_{k})\). However, this means the inference model is only trained on those \(\) occurring in the training data. Again, consider the Multi-digit MNISTAdd problem. For \(N=15\), we have a training set of 2000 sums, while there are \(2 10^{15}-1\) possible sums. By simulating many beliefs, the inference model sees a much richer set of inputs and outputs, allowing it to generalize.

A better approach is to fit a Dirichlet prior \(p()\) on \(_{1},...,_{k}\) that covers all possible combinations of numbers. We choose a Dirichlet prior since it is conjugate to the discrete distributions. For details, see Appendix F. During hyperparameter tuning, we found that the prior needs to be high entropy to prevent the inference model from ignoring the inputs \(\). Therefore, we regularize the prior with an additional term encouraging high-entropy Dirichlet distributions.

#### 3.2.4 Training the explainable variant

The explainable variant uses both the prediction model \(q_{}(|)\) and the optional explanation model \(q_{}(|,)\). When training the explainable variant, we use the idea that both factorizations of the joint should have the same probability mass, that is, \(p(,|)=q_{,}(,|)\). To this end, we use a novel _joint matching_ loss inspired by the theory of GFlowNets [9; 10], in particular, the trajectory balance loss introduced by  which is related to variational inference . For an in-depth discussion, see Appendix E. The joint matching loss is essentially a regression of \(q_{,}\) onto the true joint \(p\) that we compute in closed form:

\[_{Expl}(,)=(,} (,c()|)}{p(|)})^{2}, , p(,)\] (8)

Here we use that \(p(,c()|)=p(|)\) since \(c()\) is deterministic. Like when training the prediction-only variant, we sample a random belief \(\) and world \(\) and compute the output \(\). Then we minimize the loss function to _match_ the joints \(p\) and \(q_{,}\). We further motivate the use of this loss in Appendix D. Instead of a classification loss like cross-entropy, the joint matching loss ensures \(q_{,}\) does not become overly confident in a single prediction and allows spreading probability mass easier.

### Symbolic pruner

An attractive option is to use symbolic knowledge to ensure the inference model only generates valid outputs. We can compute each factor \(q_{}(w_{i}|,_{1:i-1},)\) (both for world and output variables)

[MISSING_PAGE_FAIL:6]

neural prediction), it is much more accurate already for \(N=2\). Therefore, A-NeSI's training loop allows training a prediction network with high accuracy on large-scale problems.

Comparing the different A-NeSI variants, we see the prediction-only, explainable and pruned variants perform quite similarly, with significant differences only appearing at \(N=15\) where the explainable and pruning variants outperform the predict-only model, especially on neural prediction. However, when removing the prior \(p()\), the performance degrades quickly. The prediction model sees much fewer beliefs \(\) than when sampling from a (high-entropy) prior \(p()\). A second and more subtle reason is that at the beginning of training, all the beliefs \(=f_{}()\) will be uniform because the perception model is not yet trained. Then, the prediction model learns to ignore the input belief \(\).

Figure 4 shows the runtime for inference on a single input \(\). Inference in DeepProbLog (corresponding to exact inference) increases with a factor 100 as \(N\) increases, and DPLA* (another approximation) is not far behind. Inference in DeepStochLog, which uses different semantics, is efficient due to caching but requires a grounding step that is exponential both in time and memory. We could not ground beyond \(N=4\) because of memory issues. A-NeSI avoids having to perform grounding altogether: it scales slightly slower than linear. Furthermore, it is much faster in practice as parallelizing the computation of multiple queries on GPUs is trivial.

   & N=1 & N=2 & N=4 & N=15 \\   &  \\ LTN & 80.54 \(\) 23.33 & 77.54 \(\) 35.55 & T/O & T/O \\ DeepProbLog & 97.20 \(\) 0.50 & 95.20 \(\) 1.70 & T/O & T/O \\ DPLA* & 88.90 \(\) 14.80 & 83.60 \(\) 23.70 & T/O & T/O \\ DeepStochLog & **97.90 \(\) 0.10** & **96.40 \(\) 0.10** & **92.70 \(\) 0.60** & T/O \\ Embed2Sym & 97.62 \(\) 0.29 & 93.81 \(\) 1.37 & 91.65 \(\) 0.57 & 60.46 \(\) 20.36 \\ A-NeSI (predict) & 97.66 \(\) 0.21 & 95.96 \(\) 0.38 & 92.56 \(\) 0.79 & 75.90 \(\) 2.21 \\ A-NeSI (explain) & 97.37 \(\) 0.32 & 96.04 \(\) 0.46 & 92.11 \(\) 1.06 & **76.84 \(\) 2.82** \\ A-NeSI (pruning) & 97.57 \(\) 0.27 & 95.82 \(\) 0.33 & 92.40 \(\) 0.68 & 76.46 \(\) 1.39 \\ A-NeSI (no prior) & 76.54 \(\) 27.38 & 95.67 \(\) 0.53 & 44.58 \(\) 38.34 & 0.03 \(\) 0.09 \\   &  \\ Embed2Sym & 97.34 \(\) 0.19 & 84.35 \(\) 6.16 & 0.81 \(\) 0.12 & 0.00 \\ A-NeSI (predict) & **97.66 \(\) 0.21** & 95.95 \(\) 0.38 & **92.48 \(\) 0.76** & 54.66 \(\) 1.87 \\ A-NeSI (explain) & 97.37 \(\) 0.32 & **96.05 \(\) 0.47** & 92.14 \(\) 1.05 & **61.77 \(\) 2.37** \\ A-NeSI (pruning) & 97.57 \(\) 0.27 & 95.82 \(\) 0.33 & 92.38 \(\) 0.64 & 59.88 \(\) 2.95 \\ A-NeSI (no prior) & 76.54 \(\) 27.01 & 95.28 \(\) 0.62 & 40.76 \(\) 34.29 & 0.00 \(\) 0.00 \\  Reference & 98.01 & 96.06 & 92.27 & 73.97 \\  

Table 1: Test accuracy of predicting the correct sum on the Multi-digit MNISTAdd task. “T/O” (timeout) represent computational timeouts. Reference accuracy approximates the accuracy of an MNIST predictor with \(0.99\) accuracy using \(0.99^{2N}\). Bold numbers signify the highest average accuracy for some \(N\) within the prediction categories. _predict_ is the prediction-only variant, _explain_ is the explainable variant, _pruning_ adds symbolic pruning (see Appendix H), and _no prior_ is the prediction-only variant _without_ the prior \(p()\).

Figure 4: Inference time for a single input \(\) for different nr. of digits. The left plot uses a log scale, and the right plot a linear scale. DSL stands for DeepStochLog. We use the GM variant of DPLA*.

### Visual Sudoku Puzzle Classification

In this task, the goal is to recognize whether an \(N N\) grid of MNIST digits is a Sudoku. We have 100 examples of correct and incorrect grids from . We use the same MNIST classifier as in the previous section. We treat \(\) as a grid of digits in \(\{1,,N\}\) and designed an easy-to-learn representation of the label. For sudokus, all pairs of digits \(_{i},_{j}\) in a row, column, and block must differ. For each such pair, a dimension in \(\) is 1 if different and 0 otherwise, and the symbolic function \(c\) computes these interactions from \(\). The prediction model is a single MLP that takes the probabilities for the digits \(_{i}\) and \(_{j}\) and predicts the probability that those represent different digits. For additional details, see Appendix I.2.

Table 2 shows the accuracy of classifying Sudoku puzzles. A-NeSI is the only method to perform better than random on \(9 9\) sudokus. Exact inference cannot scale to \(9 9\) sudokus, while we were able to run A-NeSI for 3000 epochs in 38 minutes on a single NVIDIA RTX A4000.

### Warcraft Visual Path Planning

The Warcraft Visual Path Planning task  is to predict a shortest path from the top left corner of an \(N N\) grid to the bottom right, given an image from the Warcraft game (Figure 5). The perception model has to learn the cost of each tile of the Warcraft image. We use a small CNN that takes the tile \(i,j\) (a \(3 8 8\) image) and outputs a distribution over 5 possible costs. The symbolic function \(c()\) is Dijkstra's algorithm and returns a shortest path \(\), which we encode as a sequence of the 8 (inter)-cardinal directions starting from the top-left corner. We use a ResNet18  as the prediction model, which we train to predict the next direction given beliefs of the tile costs \(\) and the current location. We pretrain the prediction model on a fixed prior and train the perception model with a frozen prediction model. See Appendix I.3 for additional details.

Table 3 presents the accuracy of predicting a shortest path. A-NeSI is competitive on \(12 12\) grids but struggles on \(30 30\) grids. We believe this is because the prediction model is not accurate enough, resulting in gradients with too high bias. Still, A-NeSI finds short paths, is far better than a pure neural network, and can scale to \(30 30\) grids, unlike SPL , which uses exact inference. Since both SPL and I-MLE  have significantly different setups (see Appendix I.3.2), we added experiments using REINFORCE with the leave-one-out baseline (RLOO, ) that we implemented with the Storchastic PyTorch library . We find that RLOO has high variance in its performance. Since RLOO is unbiased with high variance and A-NeSI is biased with no variance, we also tried running A-NeSI and RLOO simultaneously. Interestingly, this is the best-performing method on the \(12 12\) grid, and has competitive performance on \(30 30\) grids, albeit with high variance (6 out 10 runs get to an accuracy between 93.3% and 98.5%, while the other runs are stuck around 25% accuracy).

  & N=4 & N=9 \\  CNN & 51.50 \(\) 3.34 & 51.20 \(\) 2.20 \\ Exact inference & 86.70 \(\) 0.50 & T/O \\ NeuPSL & **89.7**\(\) **2.20** & 51.50 \(\) 1.37 \\ A-NeSI (symbolic prediction) & **89.70**\(\) **2.08** & **62.15**\(\) **2.08** \\ A-NeSI (neural prediction) & **89.80**\(\) **2.11** & **62.25**\(\) **2.20** \\ 

Table 2: Test accuracy of predicting whether a grid of numbers is a Sudoku. For A-NeSI, we used the prediction-only variant.

Figure 5: Example of the Warcraft task.

## 5 Related work

A-NeSI can approximate multiple PNL methods . DeepProbLog  performs symbolic reasoning by representing \(\) as ground facts in a Prolog program. It enumerates all possible proofs of a query \(\) and weights each proof by \(p(|)\). NeurASP  is a PNL framework closely related to DeepProbLog, but is based on Answer Set Programming . Some methods consider constrained structured output prediction . In Appendix B, we discuss extending A-NeSI to this setting. Semantic Loss  improves learning with a loss function but does not guarantee that formulas are satisfied at test time. Like A-NeSI, Semantic Probabilistic Layers  solves this with a layer that performs constrained prediction. These approaches perform exact inference using probabilistic circuits (PCs) . Other methods perform approximate inference by only considering the top-k proofs in PCs [36; 23]. However, finding those proofs is hard, especially when beliefs have high entropy, and limiting to top-k significantly reduces performance. Other work considers MCMC approximations . Using neural networks for approximate inference ensures computation time is constant and independent of the entropy of \(\) or long MCMC chains.

Other neurosymbolic methods use fuzzy logics [5; 17; 15; 18], which are faster than PNL with exact inference. Although traversing ground formulas is linear time, the grounding is itself often exponential , so the scalability of fuzzy logics often fails to deliver. A-NeSI is polynomial in the number of ground atoms and does not traverse the ground formula. Furthermore, background knowledge is often not fuzzy [54; 20], and fuzzy semantics does not preserve classical equivalence.

A-NeSI performs gradient estimation of the WMC problem. We can extend our method to biased but zero-variance gradient estimation by learning a distribution over function outputs (see Appendix C). Many recent works consider continuous relaxations of discrete computation to make them differentiable [42; 24] but require many tricks to be computationally feasible. Other methods compute MAP states to compute the gradients [41; 11; 47] but are restricted to integer linear programs. The score function (or 'REINFORCE') gives unbiased yet high-variance gradient estimates [40; 53]. Variance reduction techniques, such as memory augmentation  and leave-one-out baselines , exist to reduce this variance.

## 6 Conclusion, Discussion and Limitations

We introduced A-NeSI, a scalable approximate method for probabilistic neurosymbolic learning. We demonstrated that A-NeSI scales to combinatorially challenging tasks without losing accuracy. A-NeSI can be extended to include explanations and hard constraints without loss of performance.

However, there is no 'free lunch': when is A-NeSI a good approximation? We discuss three aspects of learning tasks that could make it difficult to learn a strong and efficient inference model.

* **Dependencies of variables.** When variables in world \(\) are highly dependent, finding an informative prior is hard. We suggest then using a prior that can incorporate dependencies such as a normalizing flow [45; 14] or deep generative models  over a Dirichlet distribution.
* **Structure in symbolic reasoning function.** We studied reasoning tasks with a relatively simple structure. Learning the inference model will be more difficult when the symbolic function \(c\) is less structured. Studying the relation between structure and learnability is interesting future work.
* **Problem size.** A-NeSI did not perfectly train the prediction model in more challenging problems, which is evident from the divergence in performance between symbolic and neural prediction for 15 digits in Table 1. We expect its required parameter size and training time to increase with the problem size.

Promising future avenues are studying if the explanation model produces helpful explanations , extensions to continuous random variables  (see Appendix C for an example), and extensions to unnormalized distributions such as Markov Logic Networks, as well as (semi-) automated A-NeSI solutions for neurosymbolic programming languages like DeepProbLog .