# Continuous-time Analysis of Anchor Acceleration

Jaewook J. Suh

Seoul National University

jacksuhkr@snu.ac.kr &Jisun Park

Seoul National University

colleenp0515@snu.ac.kr &Ernest K. Ryu

Seoul National University

ernestryu@snu.ac.kr

###### Abstract

Recently, the anchor acceleration, an acceleration mechanism distinct from Nesterov's, has been discovered for minimax optimization and fixed-point problems, but its mechanism is not understood well, much less so than Nesterov acceleration. In this work, we analyze continuous-time models of anchor acceleration. We provide tight, unified analyses for characterizing the convergence rate as a function of the anchor coefficient \((t)\), thereby providing insight into the anchor acceleration mechanism and its accelerated \((1/k^{2})\)-convergence rate. Finally, we present an adaptive method inspired by the continuous-time analyses and establish its effectiveness through theoretical analyses and experiments.

## 1 Introduction

Nesterov acceleration  is foundational to first-order optimization theory, but the mechanism and its convergence proof are not transparent. One approach to better understand the mechanism is the continuous-time analysis: derive an ODE model of the discrete-time algorithm and analyze the continuous-time dynamics . This approach provides insight into the accelerated dynamics and has led to a series of follow-up work .

Recently, a new acceleration mechanism, distinct from Nesterov's, has been discovered. This _anchor acceleration_ for minimax optimization and fixed-point problems  has been an intense subject of study, but its mechanism is understood much less than Nesterov acceleration. The various analytic techniques developed to understand Nesterov acceleration, including continuous-time analyses, have only been applied in a very limited manner .

Contribution.In this work, we present continuous-time analyses of anchor acceleration. The continuous-time model is the differential inclusion

\[-(X)-(t)(X-X_{0})\]

with initial condition \(X(0)=X_{0}\), maximal monotone operator \(\), and scalar-valued function \((t)\). The case \((t)=\) corresponds to the prior anchor-accelerated methods APPM , EAG , and FEG .

We first establish that the differential inclusion is well-posed, despite the anchor coefficient \((t)\) blowing up at \(t=0\). We then provide tight, unified analyses for characterizing the convergence rate as a function of the anchor coefficient \((t)\). This is the first formal and rigorous treatment of this anchored dynamics, and it provides insight into the anchor acceleration mechanism and its accelerated \((1/k^{2})\)-convergence rate. Finally, we present an adaptive method inspired by the continuous-time analyses and establish its effectiveness through theoretical analyses and experiments.

### Preliminaries and notation

We provide the organization for prior works in Appendix A. Here, we review standard definitions and set up the notation.

Monotone and set-valued operators.We follow the standard definitions of Bauschke and Combettes , Ryu and Yin . For the underlying space, consider \(^{n}\) with standard inner product \(,\) and norm \(\|\|\). Define domain of \(\) as \(=\{x^{n}x\}\). We say \(\) is an operator on \(^{n}\) and write \(^{n}^{n}\) if \(\) maps a point in \(^{n}\) to a subset of \(^{n}\). We say \(^{n}^{n}\) is monotone if

\[x-y,x-y 0, x,y ^{n},\]

where the notation means that \( u-v,x-y 0\) for all \(ux\) and \(vy\). For \((0,)\), say \(^{n}^{n}\) is \(\)-strongly monotone if

\[x-y,x-y\|x-y\|^{2}, x,y ^{n}.\]

Write \(=\{(x,u) ux\}\) for the graph of \(\). An operator \(\) is maximally monotone if there is no other monotone \(\) such that \(\) properly, and is maximally \(\)-strongly monotone if there is no other \(\)-strongly monotone \(\) such that \(\) properly.

For \(L(0,)\), single-valued operator \(^{n}^{n}\) is \(L\)-Lipschitz if

\[\|x-y\| L\|x-y\|, x,y ^{n}.\]

Write \(_{}=(+)^{-1}\) for the resolvent of \(\), while \(^{n}^{n}\) is the identity operator. When \(\) is maximally monotone, it is well known that \(_{}\) is single-valued with \(_{}=^{n}\).

We say \(x_{}^{n}\) is a zero of \(\) if \(0x_{}\). We say \(y_{}\) is a fixed-point of \(\) if \(_{}=y_{}\). Write \(\) for the set of all zeros of \(\) and \(\) for the set of all fixed-points of \(\).

Monotonicity with continuous curves.We say an operator is differentiable if it is single-valued, continuous, and differentiable as a function. If a differentiable operator \(\) is monotone and \(X[0,)^{n}\) is a differentiable curve, then taking limit \(h 0\) of

\[}(X(t+h))-(X(t)),X(t+h)-X(t)  0\]

leads to

\[(X(t)),(t) 0.\] (1)

Similarly if \(\) is furthermore \(\)-strongly monotone, then

\[(X(t)),(t)\| (t)\|^{2}.\] (2)

## 2 Derivation of differential inclusion model of anchor acceleration

### Anchor ODE

Suppose \(^{n}^{n}\) is a maximal monotone operator and \(:(0,)[0,)\) is a twice differentiable function. Consider differential inclusion

\[(t)-(X(t))-(t)(X(t)-X_{0})\] (3)

with initial condition \(X(0)=X_{0}()\). We refer to this as the _anchor ODE_. 1 We say \(X[0,)^{n}\) is a solution, if it is absolutely continuous and satisfies (3) for \(t(0,)\) almost everywhere.

Denote \(S\) as the subset of \([0,)\) on which \(X\) satisfies the differential inclusion. Define

\[}(X(t))=-(t)-(t)(X(t)-X_{0})\]

for \(t S\). Since \(}(X(t))(X(t))\) for \(t S\), we say \(}\) is a _selection_ of \(\) for \(t S\). If \(\|}(X(t))\|\) is bounded on all bounded subsets of \(S\), then we can extend \(}\) to \([0,)\) while retaining certain favorable properties. We discuss the technical details of this extension in Appendix E.1. The statements of Section 3 are stated with this extension.

### Derivation from discrete methods

We now show that the following instance of the anchor ODE

\[(t)=-(X(t))-(X(t)-X_{0}),\] (4)

where \(X(0)=X_{0}\) is the initial condition and \(^{n}^{n}\) is a continuous operator, is a continuous-time model of APPM , EAG , and FEG , which are accelerated methods for monotone inclusion and minimax problems.

Consider APPM with operator \(h\)

\[x^{k} =_{h}y^{k-1}\] \[y^{k} =(2x^{k}-y^{k-1})+y^{0}\] (5)

with initial condition \(y^{0}=x^{0}\). Assume \(h>0\) and \(^{n}^{n}\) is a continuous monotone operator. Using \(y^{k-1}=x^{k}+hx^{k}\) obtained from the first line, substituting \(y^{k}\) and \(y^{k-1}\) in the second line we get,

\[x^{k+1}+hx^{k+1}=(x^{k}-hx^{k})+ x^{0}.\]

Then reorganizing and dividing both sides by \(h\), we have

\[-x^{k}}{h}=-x^{k+1}-x^{k}- {1}{h(k+1)}(x^{k}-x^{0}).\]

Identifying \(x^{0}=X_{0}\), \(2hk=t\), and \(x^{k}=X(t)\), we have \(=1-=1+(h)\) and so

\[2(t)+(h)=-(X(t+2h))-(1+(h))(X(t))-(h)} (X(t)-X_{0}).\]

Taking limit \(h 0^{+}\) and dividing both sides by \(2\), we get the anchor ODE (4). The correspondence with EAG and FEG are provided in Appendix D.4.

The following theorem establishes a rigorous correspondence between APPM and the anchor ODE for general maximal monotone operators.

**Theorem 2.1**.: _Let \(\) be a (possibly set-valued) maximal monotone operator and assume \(\). Let \(x^{k}\) be the sequence generated by APPM (5) and \(X\) be the solution of the differential inclusion (3) with \((t)=\). For all fixed \(T>0\),_

\[_{h 0+}_{0 k}\|x^{k}-X(2kh)\|=0.\]

We provide the proof in Appendix D.2.

### Existence of the solution for \((t)=}\)

To get further insight into the anchor acceleration, we generalize anchor coefficient to \((t)=}\) for \(p,>0\). We first establish the uniqueness and existence of the solution.

**Theorem 2.2**.: _Consider (3) with \((t)=}\), i.e._

\[(t)-(X(t))-}(X(t)-X_{0}).\] (6)

_for \(p,>0\). Then solution of (6) uniquely exists._

We provide the proof in Appendix B.

### Additional properties of anchor ODE

We state a regularity lemma of the differential inclusion (3), which we believe may be of independent interest. In particular, we use this result several times throughout our various proofs.

**Lemma 2.3**.: _Let \(X()\) and \(Y()\) are solutions of the differential inclusion (3) respectively with initial values and anchors \(X_{0}\) and \(Y_{0}\). Then for all \(t[0,)\),_

\[\|X(t)-Y(t)\|\|X_{0}-Y_{0}\|.\]

We provide the proof in Appendix B.1.

Boundedness of trajectories is an immediate corollary of Lemma 2.3. Specifically, suppose \(X()\) is the solution of differential inclusion (3) with initial value \(X_{0}\). Then for all \(X_{}\) and \(t[0,)\),

\[\|X(t)-X_{}\|\|X_{0}-X_{}\|.\]

This follows from setting \(Y_{0}=X_{}\) in Lemma 2.3.

## 3 Convergence analysis

We now analyze the convergence rate of \(\|}(X(t))\|^{2}\) for the anchor ODE (3) with \((t)=}\) and \(,p>0\). The results are organized in Table 1.

Let \(\) be the anchor coefficient function of (3). Define \(C[0,)\) as \(C(t)=e^{_{}^{t}(s)ds}\) for some \(v[0,]\). Note that \(=C\) and \(C\) is unique up to scalar multiple. We call \(((t))\) the _vanishing speed_ and \(()\) the _contracting speed_, and we describe their trade-off in the following.

Loosely speaking, the _contracting speed_ describes how fast the anchor term alone contracts the dynamical system. Consider \((t)=-(t)(X(t)-a)\) for \(a^{n}\), a system only with the anchor. Then, \(X(t)=(X(0)-a)+a\) is the solution, so the flow contracts towards the anchor \(a\) with rate \(\). Intuitively speaking, this contracting behavior leads to stability and convergence. On the other hand, the anchor must eventually vanish, since our goal is to converge to an element in \(\), not the anchor. Thus the _vanishing speed_ must be fast enough to not slow down the convergence of the flow to \(\).

This observation is captured in Figure 1. Consider a monotone linear operator \(=(0&1\\ -1&0)\) on \(^{2}\) and \((t)=}\) with \(=1\) and \(p>0\). Note if there is no anchor, the ODE reduces to \(=-(X)\) which do not converge [31, Chapter 8.2]. Figure 1 shows that with \(p>1\), the anchor vanished too early before the flow is contracted enough to result in converging flow. With \(p<1\), the flow does converge but the anchor vanished too late, slowing down the convergence. With \(p=1\), the convergence is fastest.

The following theorem formalizes this insight and produces the results of Table 1.

   Case & \(p=1\), \\ \( 1\) \\ \) & \(p=1\), & \(p<1\) & \(p>1\) \\  \(\|}(X(t))\|^{2}\) & \((})\) & \((})\) & \((})\) & \((1)\) \\   

Table 1: Convergence rates of Theorem 3.1.

**Theorem 3.1**.: _Suppose \(\) is a maximal monotone operator with \(\). Consider (3) with \((t)=}\). Let \(}(X(t))\) be the selection of \((X(t))\) as in Section 2.1. Then,_

\[\|}(X(t))\|^{2}=( })+((t)^{2})+((t )).\]

Note that

\[C(t)=t^{}&p=1\\ e^{t^{1-p}}&p 1.\]

We expect the convergence rate of Theorem 3.1 to be optimized when the terms are balanced. When \((t)=\),

\[}=^{t}ds})^{2}}=}=(t)^{2}=-(t)\]

and all three terms are balanced. Indeed, the choice \((t)=\) corresponds to the optimal discrete-time choice \(\) of APPM or other accelerated methods.

### Proof outline of Theorem 3.1

The proof of Theorem 3.1 follows from Lemma 3.4, which we will introduce later in this section. To derive Lemma 3.4, we introduce a conservation law.

**Proposition 3.2**.: _Suppose \(}\) is Lipschitz continuous and monotone. For \(t_{0}>0\), define \(E:(0,)\) as_

\[E =}{2}\|}(X(t))\| ^{2}+2(t)}(X(t)),X(t)-X_{0} +((t)^{2}+(t))\|X(t)-X_{0}\| ^{2}\] \[-_{t_{0}}^{t}((s)}{2})\|X(s)-X_{0}\|^{2}ds+_{t_{0}}^{t}C(s)^{2} }(X(s)),(s) ds.\]

_Then \(E\) is a constant function._

The proof of Proposition 3.2 uses dilated coordinate \(W(t)=C(t)(X(t)-X_{0})\) to derive its conservation law in the style of Suh et al. . We provide the details in Appendix E.2.

Recall from (1) that \(}(X(s)),(s) 0\), the integrand of the last term of \(E\) is nonnegative. This motivates us to define

\[V(t)=E-_{t_{0}}^{t}C(s)^{2}}(X(s) ),(s) ds\]

as our Lyapunov function.

**Corollary 3.3**.: _Let \(\) be maximal monotone and \((t)=}\) with \(p>0\), \(>0\). Let \(}(X(t))\) be the selection of \((X(t))\) as in Section 2.1. For \(t_{0} 0\), define \(V:[0,)\) as_

\[V(t) =}{2}\|}(X(t))\| ^{2}+2(t)}(X(t)),X(t)-X_{0}+ ((t)^{2}+(t))\|X(t)-X_{0}\|^{2}\] \[-_{t_{0}}^{t}((s)}{2})\|X(s)-X_{0}\|^{2}ds.\]

_for \(t>0\) and \(V(0)=_{t 0+}V(t)\). Then \(V(t) V(0)\) holds for \(t 0\)._

A technical detail is that all terms involving \(}(X(s))\) have been excluded in the definition of \(V\) and this is what allows \(\) to not be Lipschitz continuous. We provide the details in Appendix E.3.

**Lemma 3.4**.: _Consider the setup of Corollary 3.3. Assume \(\). Then for \(t>0\) and \(X_{}\),_

\[\|}(X(t))\|^{2}  4(t)^{2}\|X_{0}-X_{}\|^{2}+}-2((t)^{2}+(t))\|X(t)-X_{0}\|^{2}\] \[+}_{t_{0}}^{t}(C(s)^{2 }(s))\|X(s)-X_{0}\|^{2}ds.\] (7)Proof outline of Lemma 3.4.: Define

\[(t)=\|}(X(t))\|^{2}+2(t)}(X(t)),X(t)-X_{0}.\]

Then, from monotonicity of \(}\) and Young's inequality,

\[(t) \|}(X(t))\|^{2}+2(t) }(X(t)),X_{}-X_{0}\] \[\|}(X(t))\|^{2}-2\| }(X(t))\|^{2}+\|(t)(X_{} -X_{0})\|^{2}\] \[=\|}(X(t))\|^{2}-2(t )^{2}\|X_{0}-X_{}\|^{2}.\] (8)

By Corollary 3.3, \(}-}+(t)(t)\) for \(t>0\). Applying (8) and organizing, we can get the desired result. The details are provided in Appendix E.4. 

Proof outline of Theorem 3.1.: It remains to show that last integral term of Lemma 3.4 is \((})+((t)^{2} )+((t))\). The details are provided in Appendix E.5. 

Before we end this section, we observe how our analysis simplifies in the special case \((t)=\). In this case,

\[V(t)=}{2}\|}(X(t))\|^{2}+t }(X(t)),X(t)-X_{0},\]

and this corresponds to the Lyapunov function of [59, Section 4] for the case \(=1\). As \(V(0)=0\), the conclusion of Lemma 3.4 becomes

\[\|}(X(t))\|^{2}}\|X_{0}-X_ {}\|^{2}=(}),\]

which to the best rate in Table 1.

### Point convergence

APPM is an instance of the Halpern method [54, Lemma 3.1], which iterates converge to the element in \(\) closest to \(X_{0}\). The anchor ODE also exhibits this behavior.

**Theorem 3.5**.: _Let \(\) be a maximal monotone operator with \(\) and \(X\) be the solution of (3). If \(_{t}\|}(X(t))\|=0\) and \(_{t}1/C(t)=0\), then, as \(t\),_

\[X(t)*{argmin}_{z}\|z-X_{0}\|.\]

We provide the proof in Appendix E.6.

## 4 Tightness of analysis

In this section, we show that the convergence rates of Table 1 are actually tight by considering the dynamics under the explicit example \(=(0&1\\ -1&0)\). Throughout this section, we denote \(\) as \(A\) when when the operator is linear.

### Explicit solution for linear \(A\)

**Lemma 4.1**.: _Let \(A^{n}^{n}\) be a linear operator and let \((t)=\). The series_

\[X(t)=_{n=0}^{}}{(n++1)}(+1)X_ {0},\]

_where \(\) denotes the gamma function, is the solution for (3) with \(=A\)._Note that when \(=0\), this is the series definition of the matrix exponential and \(X(t)=e^{-tA}\). The solution also has an integral form, which extends to general \((t)\).

**Lemma 4.2**.: _Suppose \(A^{n}^{n}\) is a monotone linear operator. Then_

\[X(t)=}{C(t)}(_{0}^{t}e^{sA}C(s)(s)ds+C(0)I)X_{0}\] (9)

_is the solution for (3) with \(=A\)._

See Appendix F.1.1 and Appendix F.1.2 for details.

### The rates in Table 1 are tight

First, we consider \(p>1\) for \((t)=}\).

**Theorem 4.3**.: _Suppose \(_{t} 0\), i.e., suppose \((t) L^{1}[t_{0},)\) for some \(t_{0}>0\). Then there exists an operator \(\) such that_

\[_{t}\|}(X(t))\| 0,\]

_where \(X\) is the solution of (3)._

Note that \(} L^{1}[t_{0},)\) when \(p>1\). The proof of Theorem 4.3 considers \(=2(0&1\\ -1&0)\) for \(\) and uses the Fourier inversion formula. See Appendix F.2 for details.

Next, we consider \((t)=}\) for cases other than \(p>1\).

**Theorem 4.4**.: _Let \(A=(0&1\\ -1&0)\), \((t)=}\), \(0<p 1\), and \(>0\). Let \(X\) be the solution given by (9) and \(X_{0} 0\). Let_

\[r(t)=t^{2}&p=1, 1\\ t^{2}&p=1,<1\\ t^{2p}&0<p<1.,\]

_Then,_

\[_{t}r(t)\|A(X(t))\|^{2} 0.\]

We provide the proof in Appendix F.3.

## 5 Discretized algorithms

In this section, we provide discrete-time convergence results that match the continuous-time rate of Section 3.

**Theorem 5.1**.: _Suppose \(\) be a maximal monotone operator, \(p>0\), and \(>0\). Consider_

\[x^{k} =_{}y^{k-1}\] \[y^{k} =}{k^{p}+}(2x^{k}-y^{k-1})++ }x^{0}\]

_for \(k=1,2,\), with initial condition \(y^{0}=x^{0}^{n}\). Let \(}x^{k}=y^{k-1}-x^{k}\) for \(k=1,2,\). Then this method exhibits the rates of convergence in Table 2._

Note that the method of Theorem 5.1 reduces to APPM when \(=1\), \(p=1\).

   Case & \(p=1,\\  1\) & \(p=1,\\ <1\) & \(p<1\) & \(p>1\) \\  \(\|}(x^{k})\|^{2}\) & \((})\) & \((})\) & \((})\) & \((1)\) \\   

Table 2: Rates for the discrete-time method of Theorem 5.1.

Proof outline of Theorem 5.1.: The general strategy is to find discretized counterparts of corresponding continuous-time analyses. However, directly discretizing the conservation law of Proposition 3.2 was difficult due to technical reasons. Instead, we obtain differently scaled but equivalent conservation laws using dilated coordinates and then performed the discretization. The specific dilated coordinates, inspired by , are \(W_{1}(t)=X(t)-X_{0}\) for \(p>1\), \(W_{2}(t)=t^{p}(X(t)-X_{0})\) for \(0<p<1\), \(W_{3}(t)=t(X(t)-X_{0})\) for \(p=1\), \( 1\) and \(W_{4}(t)=t^{}(X(t)-X_{0})\) for \(p=1\), \(0<<1\).

In the discrete-time analyses, the behavior of the leading-order terms is predictable as they match the continuous-time counterpart. The difficult part is, however, controlling the higher-order terms that were not present in the continuous-time analyses. Through our detailed analyses, we bound such higher-order terms and show that they do not affect the convergence rate in the end. We provide the details in Appendix G.3. 

## 6 Convergence analysis under strong monotonicity

In this section, we analyze the dynamics of the anchor ODE (3) for \(\)-strongly monotone \(\). When \((t)=\) and \(=(&0\\ 0&)\), Lemma 4.1 tells us that \((X(t))=(I-e^{-tA})X_{0}\) and therefore that \(\|(X(t))\|^{2}=(})\), which is a slow rate for the strongly monotone setup. On the other hand, we will see that \((t)=-1}\) is a better choice leading to a faster rate in this setup.

Our analysis of this section is also based on a conservation law, but we use a slightly modified version to exploit strong monotonicity.

**Proposition 6.1**.: _Suppose \(}\) is monotone and Lipschitz continuous. Let \(X\) be the solution of (3) and let \(R:[0,)(0,)\) be a differentiable function. For \(t_{0}>0\), define \(E:(0,)\) as_

\[E=R(t)^{2}}{2}\|}(X( t))\|^{2}+2(t)}(X(t)),X(t)\!-\!X_{0} +((t)^{2}+(t))\|X(t)-X_{0}\| ^{2}\] \[-\!_{t_{0}}^{t}\!R(s)^{2} (s)}{2}\!\|X(t)\!-\!X_{0}\|^{2}ds+\!\!_{t_{0 }}^{t}\!\!C(s)^{2}R(s)^{2}\!\!(\!\!}(X(s)),(s)\!\!-\!(s)}{R(s)}\! \|(s)\|^{2}\!\!)\!ds.\]

_Then \(E\) is a constant function for \(t[0,)\)._

Proposition 6.1 generalizes Proposition 3.2, since it corresponds to the special case with \(R(t) 1\).

Recall from (2), when \(\) is \(\)-strongly monotone we have

\[(X(t)),(t)-\| (t)\|^{2} 0.\]

This motivates the choice \(R(t)=e^{ t}\), since \((s)}{R(s)}=\). From calculation provided in Appendix H.2, the choice \((t)=-1}\) makes \((R(s)^{2}(s)}{2})=0\). Plugging these choices into Proposition 6.1 and following arguments of Section 3, we arrive at the following theorem.

**Theorem 6.2**.: _Let \(\) be a \(\)-strongly maximal monotone operator with \(>0\) and assume \(\). Let \(X\) be a solution of the differential inclusion (3) with \((t)=-1}\), i.e. for almost all \(t\),_

\[-(X)--1}(X-X_{0}).\] (10)

_Let \(}(X(t))\) be the selection of \((X(t))\) as in Section 2.1. Define \(V:[0,)\) as_

\[V(t)=-e^{- t})^{2}}{2}\|}(X(t))\| ^{2}+2(1-e^{-2 t})\!(}(X(t )),X(t)-X_{0}-\|X(t)-X_{0}\|^{2}).\]

_Then \(V(t) V(0)\) holds for \(t 0\). Furthermore for \(X_{}\),_

\[\|}(X(t))\|^{2}(-1})^{2} \|X_{0}-X_{}\|^{2}=(}).\]In Appendix H.2.3, we show that (10) is a continuous-time model for OS-PPM of Park and Ryu . In Appendix C, we show the existence and uniqueness of the solution.

Since \((t)=-1} L^{1}[t_{0},)\) for any \(t_{0}>0\), Theorem 4.3 implies that \(}(X(t)) 0\) when \(\) is merely monotone. This tells us that the optimal choice of \((t)\) for should depend on the properties of \(\). In the following section, we describe how \((t)\) can be chosen to adapt to the operator's properties.

## 7 Adaptive anchor acceleration and experiments

In this section, we present an adaptive method for choosing the anchor coefficient \(\), and we theoretically and experimentally show that this choice allows the dynamics to adapt to the operator's properties. It achieves the optimal \((1/k^{2})\)-convergence rate when \(\) is monotone and an exponential convergence rate when \(\) is furthermore \(\)-strongly monotone and Lipschitz continuous.

**Theorem 7.1**.: _Suppose \(}\) is Lipschitz continuous and monotone. Consider the anchor ODE_

\[=-}(X)+}(X)\|^{2}}{2}(X),X-X_{0} }}_{=-(t)}(X-X_{0})\] (11)

_with initial condition \(X(0)=X_{0}\) and \(\|}(X_{0})\| 0\). Suppose the solution exists and \(\) is continuous at \(t=0\). Moreover, suppose \((0,)\) is well-defined, i.e., no division by zero occurs in the definition of \((t)\). Then for \(t>0\) and \(X_{}}\), we have \((t)>0\) and_

\[\|}(X(t))\|^{2}  4(t)^{2}\|X_{0}-X_{}\|^{2}\] \[(t)^{2} }.\]

_If \(}\) is furthermore \(\)-strongly monotone, then for \(t>0\),_

\[(t)^{2}(-1})^{2}.\]

We provide the proof in Appendix I.1. Note that anchor coefficient (11) is chosen so that

\[(t)=\|}(X(t))\|^{2}+2(t) }(X(t)),X(t)-X_{0}=0.\]

So left-hand side of (8) is zero and a \(((t)^{2})\) convergence rate is immediate. An analogous discrete-time result is shown in the following theorem.

**Theorem 7.2**.: _Let \(\) be a maximal monotone operator. Let \(x^{0}=y^{0}^{n}\). Consider_

\[x^{k} =_{}y^{k-1}\] \[y^{k} =(1-_{k})(2x^{k}-y^{k-1})+_{k}x^{0}\]

_with_

\[_{k}=}x^{k}\|^{2}}{- }x^{k},\,x^{k}-x^{0}+\|}x^{k} \|^{2}}&\|}x^{k}\|^{2} 0\\ 0&\|}x^{k}\|^{2}=0,\]

_for \(k=1,2,\), where \(}x^{k}=y^{k-1}-x^{k}\)._

_Then \(_{k} 0\) and_

\[\|}x^{k+1}\|^{2} _{k}^{2}\|x^{0}-x^{}\|^{2}\] \[_{k}^{2} }\]

_for \(k=1,2,\) and \(x^{}\)._

_If \(\) is furthermore \(\)-strongly monotone and \(L\)-Lipschitz continuous, then for \(k=1,2,\),_

\[_{k}^{2}()}{(1+/(1+L^{2}))^{k}-1 +/(1+L^{2})})^{2}.\]For the monotone setup, the rate \(\|}x^{k+1}\|^{2}}\|x^{0}-x ^{}\|^{2}\) matches the exact optimal rate of APPM . In the limit \( 0\), the result for the \(\)-strongly monotone case reduces to the result for the monotone case. We provide the proof and details of Theorem 7.2 in Appendix I.3.

The method of Theorem 7.2 is a discrete-time counterpart of the ODE of (11). The extra term \(\|}x^{k}\|^{2}\) of the denominator in the definition of \(_{k}\) vanishes in the continuous-time limit. We provide further details in Appendix I.2. Analogous to the continuous-time case, a key property of the discrete-time adaptive method is that the counterpart of \((t)\) is kept nonpositive. In the proof of Lemma I.3, the fact \(_{k}<1\) plays the key role in proving this property. The extra term \(\|}x^{k}\|^{2}\) in the denominator and the fact that \(}x^{k},\,x^{k}-x^{0}<0\) when \(\|}x^{k}\|^{2} 0\) leads to \(_{k}<1\).

### Experiment details

We now show an experiment with the method of Theorem 7.2 applied to a decentralized compressed sensing problem Shi et al. . We assume that we have the measurement \(b_{i}=A_{(i)}x+e_{i}\), where \(A_{(i)}\) is a measurement matrix available for each local agent \(i\), \(x\) is an unknown shared signal we hope to recover, and \(e_{i}\) is an error in measurement. We solve this problem in a decentralized manner in which the local agents keep their measurements private and only communicate with their neighbors.

As in Shi et al. , we formulate the problem into an unconstrained \(_{1}\)-regularized least squares problem

\[^{d}}{} _{i=1}^{n}\{\|A_{(i)}x-b_{i}\|^{2 }+\|x\|_{1}\},\]

and apply PG-EXTRA. We compare vanilla PG-EXTRA with the various anchored versions of PG-EXTRA with \(_{k}\) as in Theorem 5.1 and Theorem 7.2. We show the results in Figure 2. Further details of the experiment are provided in Appendix J.

## 8 Conclusion

This work introduces a continuous-time model of anchor acceleration, the anchor ODE \(-(X)-(t)(X-X_{0})\). We characterize the convergence rate as a function of \((t)\) and thereby obtain insight into the anchor acceleration mechanism. Finally, inspired by the continuous-time analyses, we present an adaptive method and establish its effectiveness through theoretical analyses and experiments.

Prior work analyzing continuous-time models of Nesterov acceleration had inspired various follow-up research, such as analyses based on Lagrangian and Hamiltonian mechanics , high-resolution ODE model , and continuized framework . Carrying out similar analyses for the anchor ODE are interesting directions of future work.

Figure 2: (Left) Network graph. (Right) Squared \(M\)-norm \(\|}x^{k}\|_{M}^{2}\) vs. \(k\). Halpern corresponds to the method in Theorem 5.1, we use \(p=1.5\) and \(=2.0\).