# Sensitivity in Translation Averaging

Lalit Manam

Indian Institute of Science

Bengaluru, India - 560012

lalitmanam@iisc.ac.in&Venu Madhav Govindu

Indian Institute of Science

Bengaluru, India - 560012

venug@iisc.ac.in

###### Abstract

In 3D computer vision, translation averaging solves for absolute translations given a set of pairwise relative translation directions. While there has been much work on robustness to outliers and studies on the uniqueness of the solution, this paper deals with a distinctly different problem of sensitivity in translation averaging under uncertainty. We first analyze sensitivity in estimating scales corresponding to relative directions under small perturbations of the relative directions. Then, we formally define the conditioning of the translation averaging problem, which assesses the reliability of estimated translations based solely on the input directions. We give a sufficient criterion to ensure that the problem is well-conditioned. Subsequently, we provide an efficient algorithm to identify and remove combinations of directions which make the problem ill-conditioned while ensuring uniqueness of the solution. We demonstrate the utility of such analysis in global structure-from-motion pipelines for obtaining 3D reconstructions, which reveals the benefits of filtering the ill-conditioned set of directions in translation averaging in terms of reduced translation errors, a higher number of 3D points triangulated and faster convergence of bundle adjustment.

## 1 Introduction

The goal of the translation averaging problem is to recover absolute translations given a redundant set of pairwise relative translation directions. This problem has been relatively less studied compared to the case when pairwise displacements are available . These methods belong to the category of averaging or map synchronization , which can be modelled as a network with known pairwise relations between nodes and unknown node values to be estimated. For our specific problem of translation averaging, \(=(,)\) represents a network, with \(N\) nodes, \(\), denoting the absolute translations \(_{i}^{3},i\), and \(M\) edges, \(\), denoting the pairwise relative direction measurements between the nodes \(_{ij}^{2},(i,j)\). Here, all the relative directions \(_{ij}\)'s are in the same coordinate frame. Ideally, \(_{ij}\) should be the unit vector in the direction of \(_{j}-_{i}\). Since the measurements made are directions, these networks are also called bearing-based networks. This problem finds a place in global methods  for solving Structure-from-Motion (SfM)  in 3D computer vision, where a network of cameras is present with relative translation directions measured between the cameras.

Solving translation averaging is a challenging problem since it requires estimating translation scales in a context of dissimilarity in the input (directions) and output (absolute translations). Due to this dissimilarity, the solutions are defined upto a global scale and a choice of origin. The well-posedness of the bearing-based network problems has been studied under the lens of parallel rigidity theory . This theory determines whether a given set of input directions will have a unique solution (upto a global scale and an origin) for absolute translations.

**Distinction from parallel rigidity and outlier detection:** This paper takes the first step to deal with sensitivity in translation averaging under uncertainty, i.e. determining under what configuration ofinput directions the solution is reliable. This can be seen as a perturbation analysis of the translation averaging solution with respect to small changes in the input directions. Although there have been empirical studies on small-scale datasets (3 to 20 cameras) on the effect of perturbing input directions [48; 40; 32], to the best of our knowledge, analysis on large-scale networks, both theoretical and empirical, has not been done for bearing-based networks. This analysis is independent of parallel rigidity  since parallel rigidity deals with uniqueness of the solution, while we deal with change in solution for small input perturbations. Parallel rigidity can be equivalently described with the algebraic rank of a specific matrix , which is similar to the analysis of uniqueness of a solution given a matrix obtained from a linear system of equations. Sensitivity analysis is similar in spirit to the conditioning of a matrix while solving a linear system of equations (\(Ax=b\) problem) where the reliability of a solution is studied. Sensitivity is also different from outlier detection in the data because it only deals with small perturbations of the given input directions and not with the noise/outlier levels. The issue of sensitivity remains relevant even without outliers.

**Our contributions:** We first analyze the sensitivity in estimating edge scales in translation averaging by small perturbations to input directions on the smallest solvable bearing-based network. Then, we characterize the conditioning of the translation averaging problem based only on input directions, without any perturbation, for the smallest solvable network i.e. \(3\) edges between all \(3\) nodes, and extend it to a general network. We provide a sufficient condition for the network to be well-conditioned. We propose an efficient algorithm to identify ill-conditioned parts of the network, filter them and extract the maximal parallel rigid graph without explicit computation that checks for parallel rigidity. Finally, we show the usefulness of such a filter in the context of SfM with improved translation accuracy, more 3D points triangulated and faster convergence of bundle adjustment after removing ill-conditioned parts of the network.

## 2 Literature Review

In this section, we briefly discuss the relevant literature on translation averaging and parallel rigidity.

**Translation averaging in SfM**: In SfM, the input data contains relative translation directions that are not aligned to a common reference frame. To get the input directions in a common global frame, absolute rotations for each node are estimated using rotation averaging methods [27; 10; 11; 21; 47]. Many translation averaging methods have been proposed over the last two decades. Govindu  minimized the cross-product between the input directions and directions obtained from absolute translations. Jiang _et al._ considered triplets of nodes and used the constraints of a triangle to formulate the problem. Wilson _et al._ minimized the deviation between the input directions and directions estimated from absolute translations. Tron _et al._ minimized the squared relative displacements and solved it in a distributed manner. Ozyesil _et al._ proposed the Least Unsquared Deviations (LUD) method, extending , with \(L_{1}\) loss for robustness which made the problem as a convex program. Arrigoni _et al._ minimized the squared error of the orthogonal projection of the estimated relative translations onto input directions. Goldstein _et al._ also minimized the orthogonal projection using ADMM but used an \(L_{1}\) loss for robustness. Zhuang _et al._ relaxed the cost in  by comparing estimated relative translations to that of the observed directions and called it Bilinear Angle-based Translation Averaging (BATA). Other methods include using two-view and three-view geometry  of the cameras to set up the problem [1; 40], estimating edge scales through cycles in a network before solving for absolute translations [3; 4] or through point correspondence constraints [13; 14], iteratively refining input directions , averaging matrices obtained from two view geometry [33; 34], and exploiting the structure of the matrix generated from pairwise displacements .

**Parallel rigidity**: Several works about parallel rigidity are present in the literature arising from different communities: computer vision [42; 3; 2], robotics , computer-aided design  and decision control [20; 19; 53; 56]. The node-based formulation is the classical way to approach parallel rigidity, which deals with absolute translations (also called point formation) [45; 20; 19]. The edge-based formulation is a more recent approach which reasons about parallel rigidity based on edge lengths in terms of the cycles in the network [35; 3; 53]. Readers are referred to  for an excellent survey on parallel rigidity.

Sensitivity in Scale Estimation from Directions

In this section, we study the sensitivity in estimating edge scales in translation averaging. At first, we define the notion of a consistent set of directions for a network \(\), which will be useful for further discussion.

**Definition 1** (Consistent Directions).: _A set of relative directions, \(_{ij},(i,j)\) in a bearing-based network \(\), are said to be consistent if there exist absolute translations, \(_{i},i\), such that \(_{j}-_{i}}{\|_{j}-_{i}\|}= _{ij}\)._

We consider the smallest possible bearing-based network which is solvable, i.e. a network \(_{}\) of 3 nodes, \(_{}=\{1,2,3\}\), with all possible edges, \(_{}=\{(1,2),(2,3),(3,1)\}\). When these edges are consistent, they form a triangle. Let \(\) be the matrix containing relative directions \(_{ij},(i,j)_{}\) in its columns and \(\) be the vector containing edge scales \(s_{ij}\). Then, the least squares problem to estimate the scales can be written as

\[_{}\|\|^{2}\|\|^{2}=1, \]

where the unit norm constraint on \(\) is used to fix the global scale. The solution to the problem in Eqn. 1 is given by the eigenvector corresponding to the smallest eigenvalue of \(^{}\).

To analyze the sensitivity of the estimated scales, we perturb each direction in \(_{}\) by a small 3D rotation \(_{ij}(3)\), thereby ensuring that the perturbed vectors always lie on the unit sphere. Let \(_{ij}^{2}\) and \(_{ij}>0\) be the rotation axis and angle for \(_{ij}\), respectively. Then, the small rotation can be approximated to a first order, using Rodrigues' rotation formula, as \(_{ij}+_{ij}[_{ij}]_{}\), where \([_{ij}]_{}^{3 3}\) is a matrix such that \([_{ij}]_{}=_{ij}\) for any \(^{3}\), and \(\) is the \(3 3\) identity matrix. For small perturbations to \(_{ij}\) by \(_{ij}\), the following theorem holds:

**Theorem 1**.: _For a set of consistent directions, \(_{ij},(i,j)_{}\), in \(_{}\), the absolute change in any eigenvalue of \(^{}\), denoted as \(||\), when the directions \(_{ij}\) are perturbed by small rotations \(_{ij}\), with \(_{ij}\) and \(_{ij}>0\) being the rotation axis and angle, is bounded by_

\[|| _{(k,i,j) TI()}_{ij}\|_{ki}^{_{ij}}\|\|_{kj}^{_{ij}} \|^{_{ij}}|}{^ {2}_{(k,i),(k,j)}}.\] \[[(1+(_{ik}^{T}_{jk})^{2}) ((_{ij}^{T}_{ik})^{2}+(_{ij}^{T}_ {jk})^{2})-4_{ik}^{T}_{jk}_{ij} ^{T}_{ik}_{ij}^{T}_{jk}]^{ }, \]

_where \(TI()=\{(1,2,3),(2,3,1),(3,1,2)\}\), \(^{_{ij}}\) is the component of \(\) orthogonal to \(_{ij}\), \(_{(k,i),(k,j)}^{_{ij}}\) is the angle between \(_{ki}^{_{ij}}\) and \(_{kj}^{_{ij}}\), and \(_{(k,i),(k,j)}\) is the angle between \(_{ki}\) and \(_{kj}\)._

The proof of Thm. 1 uses eigenvalue perturbation theory . Please refer to the appendix for the proof. From Thm. 1, it can be observed that the bound for absolute change in any eigenvalue is inversely dependent on the square of the sine of angles of the triangle formed by the unperturbed directions. Since the scale estimate, \(\), is the eigenvector corresponding to the smallest eigenvalue of \(^{}\), Thm. 1 reveals that scales are sensitive to small perturbation of directions when at least one angle in the triangle is small. The term in the square root in Eqn. 2 is bounded due to the dot product of unit norm vectors and is strictly greater than zero since the unperturbed directions are consistent. We note that the bound is also directly proportional to the sine of the angle between the directions projected onto the orthogonal space of the rotation axis. This suggests that the effect of perturbation is maximum when the rotation axis is orthogonal to the directions. This is the same as the case in Cor. 1 when the perturbed directions are consistent.

**Corollary 1**.: _For a set of consistent directions, \(_{ij},(i,j)_{}\), in \(_{}\), the absolute change in any eigenvalue of \(^{}\), denoted as \(||\), when the directions \(_{ij}\) are perturbed by small rotations \(_{ij}\), with \(_{ij}\) and \(_{ij}>0\) being the rotation axis and angle, and \(_{ij}\) being orthogonal to \(_{ki}\) and \(_{kj}\) for all \((i,j,k) TI()\), is bounded by_

\[|| _{(k,i,j) TI()}_{ij}|}.\] \[[(1+(_{ik}^{T}_{jk})^{2}) ((_{ij}^{T}_{ik})^{2}+(_{ij}^{T}_ {jk})^{2})-4_{ik}^{T}_{jk}_{ij}^ {T}_{ik}_{ij}^{T}_{jk}]^{}, \]_where \(TI()=\{(1,2,3),(2,3,1),(3,1,2)\}\), and \(_{(k,i),(k,j)}\) is the angle between \(_{ki}\) and \(_{kj}\)._

Cor. 1 reveals that the scale estimates are unstable with small angles, even in the case when the perturbed directions are consistent. We call triangles with at least one small angle as **skewed** triangles. In Fig. 1, we show conditioning of the triangle under different scenarios, with green and red depicting the unperturbed and perturbed directions, respectively. For a well-conditioned triangle, a small change in direction leads to a small change in the absolute translation. But for an ill-conditioned triangle (skewed triangle), a small change in the direction leads to a large change in the absolute translation. Fig. 0(a) shows a well-conditioned triangle, Fig. 0(b) shows a triangle with one small angle due to which it is **ill-conditioned (Type I)** and Fig. 0(c) shows a triangle with two small angles making it **ill-conditioned (Type II)**. Such skewed triangles are known to be problematic in different fields. In numerical analysis [25; 8], the skewed triangles are termed as 'needle-like' triangles, and measuring their area and angles have numerical issues. In computational topology and computer graphics [17; 38], skewed triangles (also called sliver triangles) are often avoided, as in Delaunay triangulation, or are filtered out. We note that none of them deal with directions as input. Our aim is to quantify the conditioning of the triangle and the translation averaging problem based solely on the input directions, without actual perturbation, and filter out skewed triangles, which are discussed in subsequent sections.

## 4 Conditioning of Translation Averaging

In this section, we first analyze the conditioning of the bearing-based network with \(3\) nodes and \(3\) edges, i.e. \(_{}\) (as defined in Sec. 3). We define an _angle matrix_, \(_{_{}}^{3 3}\), as follows:

\[_{_{}}=_{(1,2),(1,2)}&_{(2,1 ),(2,3)}&_{(1,2),(1,3)}\\ _{(2,3),(2,1)}&_{(2,3),(2,3)}&_{(3,2),(3,1)}\\ _{(1,3),(1,2)}&_{(3,1),(3,2)}&_{(3,1),(3,1)}= 0&_{(2,1),(2,3)}&_{(1,2),(1,3)}\\ _{(2,1),(2,3)}&0&_{(3,2),(3,1)}\\ _{(1,2),(1,3)}&_{(3,2),(3,1)}&0, \]

where \(_{(k,i),(k,j)}\) is the angle between \(_{ki}\) and \(_{kj}\) (as defined in Thm. 1). The rows and columns of \(_{_{}}\) signify edges in \(_{}\) and its entries are the angular differences between directions with angles being measured within the triangle using the common node between two edges as the reference point for measuring directions. Since \(_{(k,i),(k,j)}=_{(k,j),(k,i)}\), \(_{_{}}\) is symmetric, and since \(_{(k,i),(k,i)}=0\), \(_{_{}}\) has zero diagonal entries. We quantify conditioning of the translation averaging problem on \(_{}\) as conditioning of the matrix \(_{_{}}\) based on the following theorem:

**Theorem 2**.: _Consider the bearing based-network of \(3\) nodes and \(3\) edges, \(_{}=(_{},_{})\), and the corresponding angle matrix \(_{_{}}\). The conditioning of the matrix \(_{_{}}\) signifies the skewness of the triangle formed using the directions in \(_{}\)._

The proof of Thm. 2 involves proving \(_{_{}}\) being non-singular for non-zero angles using its determinant and checking for the closeness of columns in relation to the angle values. We choose this approach instead of checking determinant since the later is not a good measure of closeness of a matrix to singularity . Please refer to the appendix for the complete proof. Thm. 2 reveals that the conditioning of \(_{}\) can be characterized by the condition number of matrix \(_{_{}}\).

Figure 1: Conditioning of triangles: analyzing change in output with small perturbation in input direction (green: given direction; red: perturbed direction; change in output: from A to Aâ€™).

We generalize the angle matrix and Thm. 2 for any bearing-based network \(\). We assume all edges in \(\) are a part of at least one **triplet** (a triplet consists of 3 nodes with all possible edges between them). We use the terms **triplet** and **triangle** interchangeably since a triplet forms a triangle for our specific problem. If this assumption is not satisfied, we remove the edges in \(\), which are not a part of any triplet and consider its maximum connected component. This will enable us to use \(_{}\) as the basic building block of \(\) and thus characterizing the conditioning of \(\).

We now expand the angle matrix to a general bearing-based network \(\) in which every edge is a part of at least one triplet. We denote the angle matrix for the general network as \(_{}^{M M}\) (\(M\) is the number of edges in \(\)), where the rows and columns signify edges in \(\), and its entries, \(a^{ij,kl}_{}\), corresponding to the row for the edge \((i,j)\) and the column for the edge \((k,l)\), are defined as follows:

\[a^{ij,kl}_{}=_{(c,c^{}_{1}),(c,c^{}_{2 })}&\\ &\{k,l\}$ and $c^{}_{1},c^{}_{2}\{i,j \}\{k,l\}\{c\}$;}\\ 0& \]

In \(_{}\), we measure the angles between the directions formed by triplet. In such cases, to ensure that the directions are measured within the triangle, similar to \(_{_{}}\), the common node, \(c\), is taken as reference and the directions are measured from \(c\) to other nodes \(c^{}_{1}\) and \(c^{}_{2}\). From Eqn. 5, it is clear that the diagonal entries are zero (\(_{(k,i),(k,i)}=0\)) and the matrix is symmetric (\(_{(k,i),(k,j)}=_{(k,j),(k,i)}\)). We note that \(_{}\) is not the same as that of the distance matrix used in Multi-Dimensional Scaling (MDS) [7; 9]. In instances when a node appears in more than one triplet, angles between the edges belonging to the different triplets are not computed, but in MDS, such angles would also be considered. This restricts the angle matrix \(_{}\) to contain information only about triangles and not of other structures. Such a construction helps to extend the findings from \(_{_{}}\) to \(_{}\). We define the **conditioning of the translation averaging problem** as the condition number of angle matrix \(_{}\). The following theorem states a sufficiency condition for a well-conditioned angle matrix:

**Theorem 3**.: _Consider a bearing-based network \(\), with all edges contributing to triplets. The angle matrix \(_{}\), corresponding to \(\), is well conditioned if the minimum angle (or equivalently all the angles) in all the triangles formed by the triplets are sufficiently large._

The proof for 3 is similar to the proof for Thm. 2. Here, we check for different combinations of columns based on the structure of \(\) and check for their closeness. Please refer to the appendix for the proof.

We note that we do not need \(\) to be parallel rigid for the angle matrix \(_{}\) to be well conditioned, implying that sensitivity and parallel rigidity are different aspects of the bearing-based network. However, we need \(\) to be parallel rigid to ensure a unique solution. In general, computing the maximal parallel rigid component is expensive. Since we remove edges in \(\) not contributing to triplets, this can affect its parallel rigidity. We avoid such expensive computation since we deal with triplets. We construct a **triplet network**\(_{T}=(_{T},_{T})\), as done in , where nodes \(_{T}\) denote a triplet in \(\) and edges \(_{T}\) connect the nodes if an edge is common between the triplets in \(\). By construction, it can be seen that disconnected components in \(\) will be disconnected in \(_{T}\) since there cannot be shared edges among disconnected components. The following theorem enables extracting the maximal parallel rigid graph when \(\) contains all edges contributing to triplets:

**Theorem 4**.: _Given a bearing-based network \(\), with all edges contributing to triplets forming triangles, and its corresponding triplet network \(_{T}\), the maximal parallel rigid component of \(\) can be determined by the edges in \(\) contributing to the largest connected component of \(_{T}\)._

Please refer to the appendix for proof of Thm. 4. Based on Thms. 3 and 4, we develop an algorithm to identify and remove skewed triangles from the translation averaging problem while ensuring that the network is parallel rigid, which is discussed in the next section.

## 5 Proposed Method

In this section, we show how to efficiently identify skewed triangles in \(\). First, we get the list of nodes and edges contributing to each triplet in \(\), for which efficient implementations exist [52; 26]. The brute force way to identify skewed triangles is to compute the angles in each triangle using the relative directions \(_{ij}\) and mark triangles with the minimum angle less than a threshold. The number of triplets depends on the sparsity of the network \(\), which, in general, is large. This makes the brute force method time-consuming. In contrast with the brute force approach, we use vectorized operations to construct the angle matrix \(_{}\) and filter skewed triangles, making it time efficient. It is observed that the vectorized version is \( 100\) times faster than the parallelized version (with 20 threads) of the brute force method for \( 10^{5}\) triplets coming from \( 10^{4}\) edges, and this gain increases significantly with an increase in the number of triplets and edges. A time comparison between the brute force method and our method is given in the appendix. In real-world data, the large-scale bearing-based networks are sparse, making \(_{}\) sparse. We present our method for sparse networks in the following steps and for dense networks in the appendix.

**Step 1:** The first step is to identify which entries are non-zero in the angle matrix \(_{}\). Since we are considering only triplets, we require a matrix that captures which edges are part of a triplet. In computational topology , higher order relationships in \(=(,)\) (other than node-to-node relationship via edges) are studied using simplices. A \(k\)-simplex is a subset of the vertex set \(\) with \((k+1)\) elements. A finite collection of simplices, such as nodes (0-simplices), edges (1-simplices), and triangles (2-simplices), is called a simplicial complex. The boundary matrix \(_{k}\) of a simplicial complex encodes which \((k-1)\)-simplex contributes to \(k\)-simplex. In our case, we need the relationship between \(1\)-simplices (edges) and \(2\)-simplices (triangles). The \((i,j)^{th}\) element of the boundary matrix \(_{2}^{M W}\), with \(M\) edges and \(W\) triplets, is given as:

\[b_{2}^{ij}=1&$ edge contributes to $j^{th}$ triangle};\\ 0&. \]

It is easy to compute \(_{2}\) since we have the list of edges contributing to each triplet. The row and column indices of non-zero elements in \(_{2}_{2}^{T}\) give us the edge pairs participating in triplets. Only these elements in \(_{}\) are to be computed (except diagonals in \(_{2}_{2}^{T}\) which are non-zero). The corresponding dot products between the edges are computed in a vectorized fashion and stored.

**Step 2:** We need to identify the correct signs for the dot products to ensure that the dot products reflect the cosine of the angles of the triangles. Since the dot products are \(_{(i,j),(k,l)}\) for edges \((i,j),(k,l)\) in triplets, there will be one common node \(c\{i,j\}\{k,l\}\). The directions \(_{ij}\) and \(_{kl}\) should be measured with reference to the common node to get the angle of the triangle that includes those edges. We need the dot product between \(_{cc_{1}^{}}\) and \(_{cc_{2}^{}}\), where \(c_{1}^{}\{i,j\}\{c\}\) and \(c_{2}^{}\{k,l\}\{c\}\). Using \(_{cc_{1}^{}}^{T}_{cc_{2}^{}}\) = \(_{c_{1}^{}c_{2}}^{T}_{c_{2}^{}c}\) and \(_{ij}=-_{ji}\), we premultiply each dot product by \(m^{ij,kl}\), where \(m^{ij,kl}\) is determined as follows:

\[m^{ij,kl}=1&;\\ -1&;\\ 0&. \]

Since edges \((i,j),(k,l)\) are part of a triplet, the third case in Eqn. 7 will not arise, but it will be helpful in the dense network case. We premultiply \(m^{ij,kl}\) to the corresponding dot products. We take the inverse cosine of the sign-corrected dot products to get the angles and allocate \(_{}\).

**Step 3:** It is easy to extract angles from \(_{}\) since edges corresponding to a triplet are known and \(_{}\) provides an easy way to index edges based on its rows and columns. Let \(Trp\) be the set of triplets in \(\). We check if the angles of the triangles are greater than a minimum threshold and remove the triplets having skewed triangles. This creates two sets of triplets \(Trp_{Ret}\) and \(Trp_{Rem}\) denoting retained and removed triplets, both being mutually exclusive. Let the edges contributing to the two triplet sets be \(_{ret}\) and \(_{rem}\), respectively, such that \(=_{ret}_{rem}\). We note that \(_{ret}_{rem}\) is not necessarily empty. For a parallel rigid \(\), the two edge sets are not mutually exclusive since there will be common edges between the triplets of \(_{ret}\) and \(_{rem}\), as a consequence of Thm. 4. Let the filtered network be denoted as \(}_{F}=(}_{F},}_{F})\). There are two ways to remove edges: one is to remove edges **aggressively** such that \(}_{F}=_{rem}\) and the other way is **non-aggressive** i.e. \(}_{F}=_{ret}\). Since all operations are vectorized, this makes the whole process of extracting the angles and removal of skewed triangles time efficient. Next, we construct the triplet network \(_{T}\) (as discussed in Sec. 4) from \(}_{F}\) and extract the largest connected component of \(_{T}\), to ensure that the network is connected and is parallel rigid. The edges of \(}_{F}\) contributing to the largest connected component of \(_{T}\) gives the final network \(_{F}=(_{F},_{F})\). The whole process is summarized in Algo 1. In our experiments, the network \(\) is sparse, and we choose the non-aggressive way of removing the edges, which ensuresthat the connected component of \(_{F}\) is large. We note that with non-aggressive pruning, we still ensure that the nodes are estimated reliably since the nodes are also a part of atleast one triplet, which belong to the set of non-skewed triangles.

## 6 Experiments

We consider SfM datasets provided in 1DSfM  for the experiments. It provides the relative motions and a reference reconstruction using Bundler [49; 50]. Since Bundler was published more than ten years ago, we use COLMAP  to generate the pairwise relative rotations and translations and use COLMAP's solution as the ground truth to get a better reconstruction. We take COLMAP solution and align it to the solution provided in 1DSfM to get absolute translations in meters. We compute absolute rotations using  and use them to align the relative translation directions into a global coordinate frame. The output of this process gives us a bearing-based network. Then, we extract triplets from the network and ensure parallel rigidity. Since the filtering process is independent of the cost function to solve the problem, we consider two representative cost functions. Revised LUD (an improvement on , provided by ), compares relative displacements, and BATA , compares relative directions. Please refer to appendix for the problem formulations. Our code is implemented in MATLAB. All experiments are performed on a PC with Intel Xeon Silver 4210 processor with 128 GB RAM. Finally, in each table, **Mean-ATE** and **RMS-ATE** denote the **mean** and **RMS** absolute translation errors, respectively, **w/o** and **w/ filter** denotes the network **before** and **after removing** skewed triangles and **bold** entries denote better performance between the solutions of the two networks.

### Analysis of Real Data

In this subsection, we first provide an illustrative example using COLMAP  reconstruction and then analyze the real data. Fig. 2 shows an illustrative example of the existence of such skewed triangles in the SfM problem. For the Alamo dataset, most images capture the front part of the museum, and thus, the cameras are densely connected in the network. The blue triangle depicts a triplet which is Type-I ill-conditioned triangle and the green triangle shows Type-II ill-conditioned triangle.

Now, we analyze the real data to understand the frequency of occurrence of skewed triangles and their difference with outlier data. For the analysis, we compute the errors in relative directions with respect to ground truth. To check whether atleast one outlier is present in the triplet, we check the maximum error of relative directions in a triplet. Since we identify skewed triangles with the minimum angle in each triplet, we compare the maximum error of relative directions with the minimum angle between directions for each triplet. In Fig. 3, we show scatter plots between the two compared quantities for all the triplets for three datasets with different edge densities. At first, we observe that there are a considerable number of skewed triangles in the network since there are many points in the scatter plot which are close to \(0^{}\) in the x-axis (representing the minimum angle in triplets). Next, we can also see that the minimum angle between edges in the triplets is independent of the maximum error in the triplets, implying that the presence of an outlier and the skewness of a triangle have no relation. Also, it can be seen that triplets with minimum angle \(>60^{}\) are clear outliers and all of them have a high angular error with respect to ground truth. Based on this observation, we first provide results on outlier-free data, and then we show the results on the real data with outliers in the next subsections.

### Outlier-Free Data

In Table 1, we list the number of nodes and edges removed due to the removal of skewed triangles and check the absolute translation errors obtained using BATA. It can be seen with the removal of a small number of nodes and edges, the mean and RMS errors of the absolute translations decrease consistently (see appendix for the number of nodes and edges of the networks). We also check the errors of the nodes in the unfiltered network, which were removed due to the removal of skewed triangles. It can be seen that the mean and RMS errors of the removed nodes are high compared to the overall errors implying that absolute translations at these nodes are not well estimated. Absolute translation error obtained using Revised LUD is provided in the appendix, which also shows that removal of skewed triangles leads to improved translation estimates. This shows the impact of skewed triangles on the translation averaging and the benefits of removing them.

Figure 3: Scatter plots of maximum error of relative directions in each triplet (with respect to ground truth) vs minimum angle between edges in each triplet on datasets from . \(ed\) denotes edge density.

Figure 2: Reference reconstruction of Alamo  using COLMAP  for displaying skewed triangles. Blue triangle: Ill-conditioned triangle (Type-I), Green triangle: Ill-conditioned triangle (Type-II).

[MISSING_PAGE_FAIL:9]

Theia  with the absolute translation solutions obtained in Table 3. Theia removes the triangulated 3D points which have reprojection errors greater than \(15\) pixels. It can be seen from Table 4 that more 3D points are triangulated in the filtered networks for most of the datasets. We note that filtered networks have lesser nodes than unfiltered networks, which implies removal of nodes coming from skewed triangles leads to better conditioning for 3D point triangulation. We also see that the bundle adjustment converges faster for the filtered networks indicating absolute translations are more stable in the filtered networks compared to unfiltered networks.

**Limitation:** Our sensitivity analysis of translation averaging is based on the triplets in the network. Although we do not lose much in terms of 3D reconstruction in SfM, further work is required to understand the sensitivity of a general bearing-based network.

## 7 Conclusion

This paper deals with sensitivity in translation averaging under input uncertainty. We study sensitivity in estimating edge scales in bearing-based networks which suggests skewed triangles are unstable. We define the conditioning of the translation averaging problem and provide a sufficient criterion to ensure that the problem is well-conditioned. Then, we propose an efficient algorithm to remove skewed triangles from the network while ensuring parallel rigidity. We demonstrate the effectiveness of our filtering scheme using structure-from-motion data without and with outliers leading to better absolute translation estimates, more 3D points triangulated and faster convergence of bundle adjustment for filtered networks.

   Dataset & & ALM & ELS & GMM & MDR & MND & NYC & ND \\  \)} & w/o filter & 142 & 57 & 101 & 46 & 115 & 77 & **376** \\  & w/ filter & **144** & **58** & **104** & **49** & **118** & **79** & 375 \\  \)} & w/o filter & 73 & 40 & 31 & 66 & 59 & 100 & 34 \\  & w/ filter & **54** & **33** & **21** & 83 & **37** & **74** & **22** \\  \)} & PDP & PIC & ROF & TOL & USQ & VNC & YKM \\  & w/o filter & 91 & 231 & **196** & 107 & **57** & 219 & 175 \\  & w/ filter & **94** & **234** & **196** & **110** & 56 & **222** & **168** \\  \)} & w/o filter & **34** & 53 & 31 & 89 & 122 & 52 & 58 \\  & w/ filter & 44 & **47** & **21** & **85** & **52** & **25** & **33** \\   

Table 4: No. of points triangulated (\(P_{tri} 10^{3}\)) and bundle adjustment iterations (\(BA_{iters}\)) using the solutions obtained by BATA for unfiltered and filtered networks on 1DSfM  datasets.

   Dataset &  &  &  & \(t_{BATA}\) (sec) \\   & w/o filter & w/ filter & w/o filter & w/ filter & Mean & RMS & \\  ALM & 4.7 & **4.5** & 11.1 & **10.5** & 22.9 & 39.5 & 17 \\ ELS & 23.2 & **22.1** & **50.7** & 51.8 & 98.6 & 118.1 & 7 \\ GMM & 50.6 & **40.9** & **77.8** & **61.1** & 149.7 & 190.8 & 14 \\ MDR & 13.9 & **12.7** & 29.4 & **26.1** & 54.2 & 61.2 & 5 \\ MND & 4.4 & **4.3** & 10.0 & **9.9** & 31.2 & 34.1 & 17 \\ NYC & 6.5 & **5.2** & 15.8 & **12.6** & 28.7 & 38.1 & 7 \\ ND & **3.3** & **3.3** & **6.4** & **6.4** & 6.9 & 7.0 & 71 \\ PDP & **8.0** & **8.0** & **13.0** & 13.1 & 15.0 & 17.3 & 16 \\ PIC & 5.3 & **5.1** & 11.0 & **10.8** & 20.8 & 29.2 & 68 \\ ROF & 12.8 & **10.4** & 27.0 & **19.6** & 65.4 & 100.5 & 24 \\ TOL & 15.6 & **14.5** & 32.2 & **30.3** & 63.7 & 90.3 & 10 \\ TFG & 20.3 & **14.5** & 62.6 & **31.8** & 65.0 & 130.8 & 287 \\ USQ & 14.5 & **10.6** & 24.8 & **18.1** & 34.5 & 49.9 & 14 \\ VNC & 10.2 & **10.2** & **17.8** & 18.4 & 21.4 & 27.5 & 28 \\ YKM & 20.4 & **19.3** & 29.5 & **28.3** & 44.0 & 51.4 & 13 \\   

Table 3: Absolute translations errors (in meters) on 1DsfM  datasets using BATA . Removed Node Errors: Errors of removed nodes in the unfiltered network, \(t_{BATA}\): time taken by BATA.