# Pedestrian-Centric 3D Pre-collision Pose and Shape Estimation from Dashcam Perspective

Meijun Wang\({}^{1}\), Yu Meng\({}^{1}\), Zhongwei Qiu\({}^{2}\), Chao Zheng\({}^{1}\), Yan Xu\({}^{1}\), Xiaorui Peng\({}^{1}\), Jian Gao\({}^{3}\)

\({}^{1}\)University of Science and Technology Beijing

\({}^{2}\)Alibaba DAMO Academy

\({}^{3}\)Northwest University

drvmj@xs.ustb.edu.cn, myu@ustb.edu.cn, qizhongwei.qzw@alibaba-inc.com

{miniflash, b20160225, pxrw}@xs.ustb.edu.cn, gaojian1@stumail.nwu.edu.cn

Corresponding author.

###### Abstract

Pedestrian pre-collision pose is one of the key factors to determine the degree of pedestrian-vehicle injury in collision. Human pose estimation algorithm is an effective method to estimate pedestrian emergency pose from accident video. However, the pose estimation model trained by the existing daily human pose datasets has poor robustness under specific poses such as pedestrian pre-collision pose, and it is difficult to obtain human pose datasets in the wild scenes, especially lacking scarce data such as pedestrian pre-collision pose in traffic scenes. In this paper, we collect pedestrian-vehicle collision pose from the dashcam perspective of dashcam and construct the first Pedestrian-Vehicle Collision Pose dataset (PVCP) in a semi-automatic way, including 40k+ accident frames and 20K+ pedestrian pre-collision pose annotation (2D, 3D, Mesh). Further, we construct a Pedestrian Pre-collision Pose Estimation Network (PPSENet) to estimate the collision pose and shape sequence of pedestrians from pedestrian-vehicle accident videos. The PPSENet first estimates the 2D pose from the image (Image to Pose, ITP) and then lifts the 2D pose to 3D mesh (Pose to Mesh, PTM). Due to the small size of the dataset, we introduce a pre-training model that learns the human pose prior on a large number of pose datasets, and use iterative regression to estimate the pre-collision pose and shape of pedestrians. Further, we classify the pre-collision pose sequence and introduce pose class loss, which achieves the best accuracy compared with the existing relevant _state-of-the-art_ methods. Code and data are available for research at https://github.com/wmj142326/PVCP.

## 1 Introduction

Pedestrian pre-collision pose refers to the emergency actions pedestrians take when facing potential hazards before collision accidents, affecting both the severity of pedestrian injuries and subsequent injury assessments [(1)]. Using collision simulation software to reconstruct pedestrian-vehicle accidents is a popular and effective method for analyzing pedestrian injuries [(2; 3; 4; 5)]. However, the current input for initial pose still relies on predefined gait sequence templates [(6; 2; 3; 4; 5)] or manual measurement of pose angles from accident images, the former cannot represent the posture of pedestrians in real accidents, and the latter is inefficient. Computer vision-based pedestrian pose estimation methods can directly estimate pose information such as joint positions or limb angles from images in real-time [(7; 8; 9; 10; 11; 12; 13)]. Existing pose estimation methods are trained and applied on multiple datasets for various scenarios, adapting to different downstream tasks. However, unlike common poses, pedestrian pre-collision poses in traffic scenes are specific, with differences in spatialtemporal characteristics and challenges such as dynamic backgrounds, sudden scene changes, and occlusions of lower limbs [14; 15]. Directly applying existing algorithms to pedestrian pre-collision pose estimation in traffic scenarios does not achieve perfect adaptation effects.

Training a proprietary network with specific datasets can effectively improve the pose estimation performance of the network in that scenario. Precise 2D pose annotations can be obtained through time and manpower-intensive efforts, indoor motion capture (Mocap) systems [16; 17] utilize markers and sensors to acquire high-quality 3D motion data. However, acquiring ground truth (GT) 3D joint positions in-the-wild is nearly impossible . Existing in-the-wild datasets either do not contain human pose [18; 19] labels or only include limited movements of daily activities [20; 21; 14]. Furthermore, training a model using a large amount of data is costly, and video data of pedestrian-vehicle collisions belongs to small sample scarce data in traffic scenes, making dataset collection difficult. Dashcams or public surveillance devices are the only sources of data [22; 23; 24; 15], further constrains the approaches to dataset creation, thereby enhancing the difficulty and complexity of producing such datasets.

In this work, we constructed a Pedestrian-Vehicle Pre-collision Pose (PVCP) dataset and proposed a simple framework for Pedestrian Pre-collision Pose and Shape Estimation (PPSENet) in collision accident videos. We collected dashcam videos and used existing pose estimation algorithms to obtain rough 2D keypoints and 3D mesh initialization results, followed by manual correction using specialized annotation tools. Specifically, we designed an SMPL annotation tool  to align the initial results with image contours, resulting in approximately 40K+ frames of accident images and 20K+ instances of pedestrian emergency poses with both 2D and 3D annotations. Our PPSENet estimates the 2D pose from images (Image to Pose, ITP) and lifts the 2D pose to the 3D mesh (Pose to Mesh, PTM). We used a pre-trained model  to capture prior knowledge of human actions and employed iterative regression [26; 11; 27] to estimate pedestrian pre-collision poses and shapes. Additionally, we classified emergency poses and introduced pose class loss, achieving superior accuracy compared to existing methods.

The main contributions of this paper are summarized below:

* We constructed a pedestrian pre-collision pose dataset, PVCP, by collecting dashcam videos of pedestrian-vehicle collisions. Through algorithm initialization and manual annotation, we obtained rich pose representation annotations, including 2D, 3D keypoints and SMPL mesh.
* We propose a two-stage pedestrian pre-collision pose and shape estimation network, PPSENet, which first estimates the 2D pose from the image and then lifts the 2D pose to the 3D pose. A pretrained encoder with pose estimation and an iterative regression decoder are combined, and introduce a collision pose class loss.
* Our framework achieved promising results on the PVCP dataset, outperforming other methods of human pose estimation. This provides both data and algorithmic support for pedestrian pre-collision pose estimation and active safety protection for pedestrians.

## 2 Related Work

**Pedestrian Pre-collision Pose.** Pedestrian pre-collision pose is crucial for studying collision damage, as the initial posture at the time of impact directly affects the severity and nature of the injuries [28; 6; 29; 30; 3; 31]. Early studies estimated collision poses by collecting post-accident data from pedestrians and vehicles . Cadaver tests  became effective for biomechanical damage studies but are limited by ethics, sample size, and high costs. Currently, collision simulation software is the most convenient and effective method to assess damage under various poses [2; 3; 4; 5]. However, initial collision poses are often fixed templates or simple categories [6; 2; 3; 4; 5], differing significantly from real pre-collision poses. One method to obtain pre-collision poses is using motion capture in virtual environments with volunteers , but this is limited by device constraints and lack of real danger. Another method captures collision sequences from real accident videos, manually measuring posture angles or adjusting dummies to match real collision poses . This method is closer to real accident scenarios but is labor-intensive, time-consuming, and lacks standardized testing, limiting its use to single accident reconstructions. With advances in deep learning and computer vision, some research has employed human pose estimation algorithms to automatically extract collision poses from accident images , providing a new approach to acquiring pre-collision posesin real accidents. Rapid and accurate acquisition of pedestrian pre-collision poses supports research on collision damage and active safety protection.

**Human Pose Estimation.** Human Pose Estimation (HPE) is a fundamental task of computer vision, which aims to obtain human pose information such as joint positions and angle from images and videos [(35)]. It can be simply classified into 2D human pose estimation and 3D human pose estimation. 2D HPE regresses pixel coordinates \((x,y)\) of joints, while 3D HPE includes depth to obtain three-dimensional coordinates (\(x,y,z\)) [(7; 8; 36)]. Though 3D coordinates can be regressed directly from images [(37; 38; 39; 40; 41)], using 2D pose as intermediate supervision before lifting to 3D often achieves higher accuracy [(9; 42)]. Additionally, the SMPL (Skinned Multi-Person Linear Model) [(43)] has gained popularity for providing pose and morphological information, along with prior knowledge of body structure, avoiding issues with limb length changes [(44; 11; 12; 27)]. This rotation-based model is particularly useful in biomechanical research [(15; 45)], which benefits the study of pedestrian emergency poses. In our research, we used 2D-to-3D lifting to estimate the pre-collision pose and shape of pedestrians from real accident videos.

**Accident and Pedestrian Datasets.** Collecting 3D pose datasets in complex traffic scenes poses challenges due to the dynamic environments and uncertain pedestrian poses. While existing large-scale datasets focus on 2D poses[(20; 51; 52)], 3D pose datasets are often confined to indoor settings using Motion Capture (Mocap) systems[(16; 17)] or estimated via Inertial Measurement Units (IMUs) for outdoor poses[(21)]. Models trained on indoor datasets do not adapt well to other in-the-wild tasks. Advanced pose estimation methods can generate pseudo-datasets to construct 3D pose datasets in the wild. Although pseudo-3D labels from semi-automatic[(44; 53)] or fully-automated methods[(54; 55)] are less accurate than Mocap data and may contain noise, they significantly improve regression-based methods[(56)]. Using a semi-automatic method, we collected dashcam videos of collisions to create a pedestrian-vehicle collision pose dataset, offering rich annotation information and contributing to pedestrian protection tasks. Table 1 highlights its advantages over other datasets.

## 3 PVCP Dataset

### Data Collection

Dashcams or public surveillance devices are the main sources of crash data [(22; 23; 24; 15)]. Dashcam views dominate vision-based Traffic Accident Anticipation (TAA) datasets due to the high potential for collision avoidance through vehicle control [(57)]. Our PVCP dataset are all derived from the vehicular perspective of dashcam, and videos are sourced from two primary origins. Similar to previous works [(47; 22; 23; 24; 15)], we collected videos of pedestrian and vehicle collisions from online platforms such as YouTube, using 'pedestrian-vehicle collision' as a keyword. In addition, a small number of videos are derived from existing open-source traffic datasets [(47; 22; 23)], which were primarily developed for tasks related to driver attention and the prediction of sudden accidents. All of the collected videos were reduced to individual accident footage, recording a complete pedestrian

  
**Type** & **Dataset** & **Year** & **Perpective** & **Background** & **Detection** & **Track** & **Depth** & **Fuse** & **Shape** & **Class** & **Frame** \\   & DAD(22) & 2016 & V & D & ✓(2D Block) & ✓ & ✗ & ✗ & ✗ & ✗ & \(>\)62k \\  & StandwalkTech(46) & 2017 & M & S & ✓(Mask) & ✓ & ✗ & ✗ & ✗ & ✗ & \(>\)300k \\  & A3D(23) & 2019 & V & D & ✓(2D Block) & ✓ & ✗ & ✗ & ✗ & ✗ & \(>\)128k \\  & DADA(47) & 2019 & V & D & ✓(2D Block) & ✗ & ✗ & ✗ & ✗ & ✗ & \(>\)656k \\  & CCD(24) & 2020 & V & D & ✓(2D Block) & ✓ & ✗ & ✗ & ✗ & ✗ & \(>\)75\% \\   & KITTI(19) & 2012 & V & D & ✓(3D Block) & ✓ & ✓ & ✗ & ✗ & ✗ & \(>\)30k \\  & Cityscapes(48) & 2015 & V & D & ✓(3D Block) & ✗ & ✗ & ✗ & ✗ & ✗ & \(>\)5k \\  & Cityscapes(49) & 2016 & V & D & ✓(2D Block) & ✗ & ✗ & ✗ & ✗ & ✗ & \(>\)5k \\  & MOT(50) & 2012-2017 & VM & DS & ✓(2D Block) & ✓ & ✗ & ✗ & ✗ & \(>\) & \(-\) \\  & Nuscen(18) & 2019 & V & D & ✓(3D Block) & ✓ & ✓ & ✗ & ✗ & \(>\)35k \\   & MSCCO(202) & 2014-2017 & Daily scene & S & ✓(2D Block) & ✗ & ✗ & ✓(2D) & ✗ & ✗ & \(>\)1000k \\  & Human3.6X(16) & 2014 & M & S & ✓(2D Block) & ✓ & ✓ & ✓(2D) & ✗ & ✗ & \(>\)300k \\  & PW3D(21) & 2018 & hand-held cameras & D & ✗ & ✓ & ✗ & ✓(3D) & ✗ & ✗ & \(>\)50k \\  & Accident Video(15) & 2020 & V/M & DS & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & \(-\) \\  & PolyKit(4) & 2018 & M & S & ✓(Mak) & ✓ & ✓ & ✓(2D) & ✓ & \(>\)10k \\ 
**Ours** & **PVCP** & **2024** & **V(Dashcam)** & **DS** & ✓(2D Block)** & ✓ & ✓ & ✓(**2D30)** & ✓ & ✓ & **>40k** \\   

Table 1: Comparison of datasets on _Accident Warning_, _Traffic Scene_ and _Pedestrian Pose_. ‘V’ represents the vehicle perspective, ‘M’ represents the monitoring perspective, ‘D’ represents a dynamic background and ‘S’ represents a static background.

and vehicle collision, resulting in 209 pedestrian-vehicle accident videos, totaling 42,511 frames of images and about 19,533 pre-collision poses.

### Data Annotations

For the application of pedestrian-vehicle collision accident reconstruction and vehicle active protection in traffic collision scenarios, PVCP provides a rich pedestrian pose representation, including pedestrian clipping images, pedestrian Bounding Box, track id, 2D and 3D keypoints and SMPL mesh label. The entire annotation pipeline is shown in Figure 1(a).

**Pedestrian Pre-collision Pose.** Manually annotating human keypoints is tedious and labor-intensive. We address this by initializing the annotation process with pose estimation models and refining the results manually. First, we identify collision-involved pedestrians in each video using a tracking network (59) and manual filtering, followed by pose annotation. We use a 15 keypoint representation similar to the JHMDB dataset (60) for efficient pose depiction. ViTPose (10) provides rough 2D pose annotations, which we manually adjust for accuracy. For occluded limbs, we estimate positions to complete the 2D skeleton, excluding body parts beyond the frame. Annotating 3D human poses in-the-wild remains challenging with images as the only source. Unlike Pseudo-GT annotators (61; 62), we use SPIN (11) to initialize predictions from cropped images, then refine the SMPL model parameters \(^{24 3}\) and \(^{10}\) using our specially designed SMPL annotation tool (25) for better pixel alignment. This yields mesh pose annotations \((,)^{6890 3}\) for pre-collision pedestrians. Finally, we apply the pre-defined joint regression matrix \(^{J 6890}\) (44) to obtain 3D keypoints \(X_{3D}^{J 3}\) from \(((,))\), where \(J=17\) (16), as shown in Figure 1(a).

**Pedestrian Motion Class.** Throughout the course of a pedestrian-vehicle collision event, pedestrians often undergo a series of rapid evasive action changes in a short period. Effectively and accurately distinguishing and predicting these imminent changes is crucial for the proactive safety features of vehicle driving systems. Through the observation and analysis of all collected accident videos, as shown in Figure 1(b), we categorize pedestrian behaviors in collision sequences into four types:

_Normal pose_: Represents the pedestrian's pose under normal, non-emergency conditions. This includes upright body pose and natural stances, reflecting the general behavior of pedestrians when not faced with emergencies.

_Run pose:_ Characterized by the pedestrian's body leaning forward with rapid alternation of arms and legs, this pose is an active measure to prevent vehicle collisions. It reflects a preemptive action to swiftly move away from potential threats, serving as a strategic pose to avoid accidents.

_Avoid pose:_ This pose is adopted by pedestrians upon detecting an imminent collision or other emergency situations. It includes potential actions such as jumping, quickly turning around, and dodging, reflecting the emergency response of pedestrians when recognizing potential danger.

Figure 1: (a) PVCP dataset annotation pipeline. (b) Pose class definition. Different colors are used to represent different pose types. (c) Dataset attribute distributions. Utilizing UMAP(58), the pose parameters \(^{N 72}\), and shape parameters shape \(^{N 10}\) are reduced to a two-dimensional \(^{N 2}\) embedding space (no unit), with coordinates along the \(x\) and \(y\)-axes, respectively.

_Collision pose:_ Represents the pose of a pedestrian post-collision with a vehicle. It encompasses possible actions such as losing balance, falling, and sustaining injuries, reflecting the change in the physical state of pedestrians after a collision.

The sequences of two behavior annotations are illustrated in Figure 1(b). It is important to note that a complete collision incident does not necessarily encompass all four types of behavior, nor is there a fixed sequence. Because some accidents do not result in a final collision due to timely measures taken, and some pedestrians may not even be aware of the approaching vehicle. To distinguish between the four types, we employ a four-digit one-hot encoding for pose annotation. For some critical changes in pose, we use three annotators and take the majority's annotation result as the final Ground Truth.

### Dataset attribute distributions

PVCP dataset contains the pose sequences during pedestrian collision. We visualized the pose and shape distribution of SMPL labels in PVCP dataset and compared them with those in commonly used and influential datasets in human pose estimation, including an indoor MoCap pose dataset Human3.6M (16), a in-the-wild dataset PW3D (21), a pseudo-3D labels dataset MSCOCO (20) and a traffic scenario pedestrian dataset PedX (14) are shown in the Figure 1(c). The distribution of pose parameters in our dataset is comparable to PW3D and significantly larger than that of everyday poses in MSCOCO. Our dataset contains over 200 individual collision pedestrians, and its shape parameter distribution is more extensive than other datasets that use a limited number of actors, such as 5 actors in the Human3.6M dataset. The most similar to PVCP is PedX, but its distribution is sparse due to its small number of poses. To sum up, our PVCP has good generalization in pose and shape parameters, representing pedestrian pose under a variety of collision conditions. Compared to existing pose datasets, PVCP demonstrates significant differences and advantages in terms of scene specificity, action space variation, and temporal continuity.

## 4 Network Architecture

In this section we present a pipeline for pedestrian Pre-collision Pose and Shape Estimation (PPSE), as shown in Figure 2, adopts a top-down two-stage strategy. Instead of estimating the pose of all pedestrians in the whole images, we pre-select the Bbox of collision pedestrians obtained by the detect and track networks (59). The crop of a single pedestrian was input into the Image to Pose (ITP) network to extract the image features of the collision pedestrian and estimate the pedestrian's 2D pose. Then, the 2D pose was lifted to 3D mesh through the Pose to Mesh (PTM) network.

### Pedestrian Pre-collision Pose and Shape Estimation

**Image to Pose**. Estimating 2D human pose from images is a basic and mature task, and many works have achieved very effective results in different datasets. We take image frame \(I\) and corresponding

Figure 2: The overview of the PPSENet. It consists of two stages: image to 2D pose (ITP) and 2D pose to 3D Mesh (PTM).

\(Bbox\) as input, simply use ResNet50 (63) as the backbone of feature extraction, and use a transposed convolution and a heat map regression head \(\) as the 2D pose estimation network, which is also the classic paradigm of 2D pose estimation (8).

**Pose to Mesh**. PTM is a network architecture for 2D-to-3D lifting, we take the 2D pose sequence \(P_{2D}^{t}^{T J C_{in}}\)as input. First, a Dual-stream Spatio-temporal Transformer (DSTformer) is used as an encoder to extract the spatio-temporal features of the pose sequence \(F_{m}^{T J C_{f}}\). Then we designed an iterative regression decoder to obtain pose parameters and shape parameters respectively. At the same time, we added an additional regression head of pose class to coordinate with pose class loss to further improve the precision of pose regression. Where \(T\) represents the length of the pose sequence and \(J\) represents the number of pedestrian joints. \(J=17\) takes the Human3.6M dataset (16) joint format and we generate additional nodes from \(P_{2d}^{15 2}\).

Inspired (12), PTM uses the backbone of an pre-trained model, MotionBERT (12), and combines PVCP dataset to estimate pedestrian pre-collision pose in collision scenarios. MotionBERT is a pre-training model of human motion representations. Firstly, it learns the prior knowledge of human motion poses under the training of a large number of datasets, which is suitable for the further improvement of small datasets such as PVCP and special pose types such as pre-collision poses. Secondly, its training mode simulates the detection results by randomly masking and adding noise to 2D pose sequences. In the same way, we train the situation of vehicles shielding pedestrian's lower limbs in collision environment. This encoder stacks spatial and temporal Multi-Head Self-Attention (MHSA) blocks in different orders to form two parallel computation branches:

\[F^{i}=_{ST}^{i}_{1}^{i}(_{1}^{i}(F^{i-1}))+ _{TS}^{i}_{2}^{i}(_{2}^{i}(F^{i-1}))\] (1)

\[_{ST}^{i},_{TS}^{i}=softmax(_{f}(_{1}^{i}( _{1}^{i}(F^{i-1}))_{2}^{i}(_{2}^{i}(F^ {i-1}))))\] (2)

Where \(i\) and \(M\) represents network depth. \(S_{i}\) and \(T_{i}\) represent Spatial MHSA and temporal MHSA of different depth layers, respectively. Adaptive fusion weights \(_{ST},_{TS}\) fuses the output features of the two branches using adaptive weights predicted by an attention regressor. \(_{f}\) denotes linear layer, \(\) denotes concatenation.

We divide the fusion feature \(F_{m}^{T J C_{f}}\) obtained by DSTFormer into three branches using different linear layers. That is, pose feature \(F_{}^{T C_{h}}\), shape features \(F_{}^{T C_{h}}\) and class features \(F_{c}{}^{T C_{h}}\). Then the three features are predicted by three different heads, outputting the pose parameter \(^{T 24 6}\), shape parameter \(^{10}\) and class probability \(c^{T 4}\). Here, we use the 6D rotation representation to converge the pose parameter more quickly (64). Further, we adopted the idea of iterative regression to add the fusion features of pose and form with the predicted results and iteratively output:

\[^{k}=W_{}^{k}(F_{})+^{k-1};^{k}=W_{}^{k }(F_{})+^{k-1}; c=softmax(W_{c}(F_{c}))\] (3)

Where \(k\) and \(N\) represents the number of iterations. \(W_{}\), \(W_{}\) and \(W_{c}\) are three linear transformations heads, \(\) denotes concatenation, \(+\) denotes add.

### Loss Function

The loss function of ITP, defined as the Mean Squared Error (MSE), is applied for comparing the predicted heatmaps \(\) and the ground truth heatmaps \(H\). The heatmap for joint \(k\) is generated by applying a 2D Gaussian centered on the \(k_{th}\) joint's location.

\[_{ITP}=\|-H\|_{2}\] (4)

The PTM loss function consists of three parts: SMPL loss, motion loss, and the pose class loss introduced by us. The final loss function is calculated as:

\[_{PTM}=_{SMPL}+_{Motion}+_{Class}\] (5)

**SMPL loss:** The loss function of SMPL-based 3D human mesh usually consists of three parts:

\[_{SMPL}=_{}_{}+_{} _{}+_{n}_{norm}\] (6)Where \(_{}=\|-\|_{1}\), \(_{}=\|-\|_{1}\), \(_{norm}=\|\|_{2}+\|\|_{2}\) represents pose loss, shape loss and normalization loss respectively.

**Motion Loss:** The human body is a complex rigid structure interconnected between joints, and the continuous frame sequence of human body movements possesses certain temporal characteristics. Therefore, introducing motion loss as

\[_{Motion}=_{k}_{kp3D}+_{v}_{v}\] (7)

Where \(_{kp3D}=\|-X\|_{1}\) represents the loss of 3D keypoints. \(_{v}=\|-V\|_{1}\) represents speed loss, and \(V=X_{t+1}-X_{t}\), \(=_{t+1}-_{t}\).

**Pose class loss:** The pedestrian pose of the collision sequence has obvious categories, namely normal pose, running pose, avoiding pose and collision pose as described in Sec. 3.2, the class loss of pose is defined as follows:

\[_{Class}=_{c}_{}}(,C)\] (8)

where \(_{}}(,C)\) represents the cross entropy loss between the predicted pose class and the GT pose class. \(_{},_{},_{n},_{k},_{v},_{c}\) are the constants of the balance weight loss.

## 5 Experimental and Results

### Dataset, Evaluation Metric and Implementation Details

**Dataset.** The PVCP dataset consists of over 20K+ pedestrian pre-collision poses, with 19,533 poses annotated with category labels. Subsequently, we selected 164 video sequences as the trainset and 45 video sequences as the testset. To ensure the effectiveness of the pose sequences, only poses with the number of keypoints \(N_{kpt} 10\) were selected, resulting in 15,458 poses for training (_Normal_:7,912; _Run_:5,044; _Avoid_:2,289; _Collision_:213) and 5,503 poses for testing (_Normal_:3,383; _Run_:1,431; _Avoid_:631; _Collision_:58). The entire training process only utilized the PVCP dataset, while the pre-training model weights were obtained from MotionBERT (12) trained on the AMASS (17), Human3.6M (16), PW3D (21), and MSCOCO (20) datasets.

**Evaluation Metric.** We evaluated the estimation of 3D human pose and shape using the following metrics: MPJPE(\(mm,\)), PA-MPJPE(\(mm,\)), MPVE(\(mm,\)), PA-MPVE(\(mm,\)). Further, we test the errors of 14 keypoints (\(X\_14j\)) shared by 2D and 3D pose representations (21) and 17 keypoints (\(X\_17j\)) represented only by 3D pose representations (16) respectively.

**Implementation Details.** PyTorch (65) was used for the entire experimental environment, four NVIDIA RTX 2080Ti GPUs for all training, and batchsize was uniformly set to 32. In the training stage, we only use the images from the PVCP trainset and the corresponding 2D ground truth keypoints as the input of the two models. We first train the ITP network by loading a pre-trained model of the MPII dataset (51) and training 40 epoches. For PTM, we use sequence length \(T=16\) and train 100 epochs in about two hours. In the test stage, in addition to the 2D ground truth keypoints of the testset, we also take the image of the testset as input to the ITP model, and then take the estimated 2D keypoint results as input to the PTM model. The effects of the same PTM training model with two different inputs are compared.

### Effects of Dataset and Pose priors

We first evaluate the effects of PVCP dataset on improving the pedestrians pre-collision pose estimation. Compared to large-scale human pose datasets, our PVCP dataset is not numerically dominant, so we use a pre-trained model that learns human pose priors and fine-tune it based on that. We ran tests on the original MotionBERT (12) to compare scratch training and the PVCP trainset with together pre-trained models. We take the detected 2D pose sequence (2D Det) as input, and compare the errors of four pre-collision pose (Normal, Run, Avoid, Collision) and all pose (All). As shown in the Table 2, the results of only-pretrain model or only-PVCP trainset are relatively poor. Due to the difference between the pre-collision pose and the daily pose, the error of using only-pretrain model (\(MPVE_{det}=335.11mm\)) is even worse than that of only-PVCP dataset (\(MPVE_{det}=315.64mm\)). When the pre-trained model and the PVCP dataset are trained together, The minimum error (\(MPVE_{det}=282.50mm\)) is obtained.

When 2D ground truth pose sequence (2D GT) is used as the input, the result without using the pre-trained model is relatively poor (\(MPVE_{gt}=300.04mm\)), and the error after using only-pretrain model is significantly decreased (\(MPVE_{gt}=165.90mm\)), because the number of keypoints (\(\)10) of the 2D GT pose sequences are relatively complete compared with the 2D Det pose sequences, its input is not affected by lighting conditions, background appearance, clothing, and weather conditions. Similarly, when the pre-trained model and the PVCP dataset are trained together, The minimum error (\(MPVE_{gt}=145.77mm\)) is obtained. This shows that our PVCP dataset has different features from ordinary pose, and the pose prior of the pre-trained model can effectively promote the precision of the pre-collision pose.

### Ablation Study

In the stage of PTM, we added an iterative decode, which does not directly predict the output once but gradually approximates the optimal solution with multiple iterations. At the same time, an pose classification regression head is added to use class loss as supervision. As shown in Table 3, we compared the impact of different components on network performance, loaded the pre-trained model each time, and set the optimal number of iterations to 3 (as shown in Table 4).

 
**Input** & **Train Set** & **testset** & **Pose class** & **MPVE** & **PAMPVE** & **MPJPE\_14j** & **PAMPJPE\_14j** & **MPJPE\_17j** & **PAMPJPE\_17j** \\   &  & Normal & 315.94 & 160.25 & 272.18 & 130.72 & 246.42 & 121.30 \\  & & & Run & 318.29 & 189.84 & 274.78 & 160.35 & 246.95 & 145.07 \\  & & & Avoid & 305.01 & 159.19 & 260.31 & 121.42 & 232.56 & 113.21 \\  & & & Collision & 347.53 & 171.82 & 311.88 & 145.64 & 281.35 & 139.46 \\   & & & All & 315.64 & 168.11 & 271.91 & 137.75 & 248.35 & 1269.92 \\   & & & Normal & 347.10 & 190.17 & 312.21 & 154.85 & 285.62 & 145.55 \\  & & & Run & 309.19 & 183.27 & 277.01 & 152.19 & 251.53 & 141.11 \\  & & & Avoid & 330.18 & 189.69 & 293.76 & 155.54 & 264.89 & 144.38 \\  & & & Collision & 344.14 & 164.32 & 301.52 & 133.26 & 275.28 & 128.19 \\   & & & All & 335.11 & 188.09 & 300.80 & 154.06 & 274.27 & 1484.12 \\   & & & Normal & 294.73 & 170.10 & 253.80 & 137.39 & 232.74 & 128.24 \\  & & & Run & 253.16 & 149.99 & 219.06 & 124.01 & 200.19 & 115.27 \\  & & & Vapor & 286.85 & 159.69 & 246.94 & 124.86 & 222.02 & 114.96 \\  & & & Collision & 250.58 & 161.25 & 222.47 & 127.37 & 200.38 & 120.47 \\   & & & All & **282.50** & **163.58** & **243.59** & **132.43** & **222.70** & **123.33** \\    &  & Normal & 304.65 & 167.56 & 260.68 & 138.49 & 233.70 & 126.83 \\  & & & Run & 296.75 & 192.00 & 254.58 & 163.80 & 226.49 & 146.66 \\   & & & Avoid & 277.51 & 157.48 & 234.30 & 123.02 & 206.55 & 113.44 \\  & & & Collision & 354.76 & 178.22 & 319.56 & 154.95 & 287.38 & 146.83 \\  & & & All & 300.04 & 173.09 & 256.69 & 143.73 & 229.30 & 130.85 \\   & & & Normal & 175.24 & 111.72 & 152.10 & 87.68 & 138.87 & 82.11 \\  & & & Run & 153.45 & 107.26 & 131.93 & 84.02 & 118.99 & 77.76 \\  & & & Avoid & 143.32 & 95.61 & 122.91 & 73.45 & 111.33 & 68.89 \\  & & Collision & 151.18 & 91.20 & 133.60 & 77.66 & 124.71 & 71.06 \\  & & & All & **165.90** & 108.48 & 143.52 & 85.14 & 130.56 & 79.48 \\   & & & Normal & 156.06 & 103.16 & 132.74 & 80.59 & 120.35 & 74.92 \\  & & & Run & 129.49 & 89.31 & 109.93 & 70.91 & 100.19 & 65.70 \\  & + & PVCP & Avoid & 127.04 & 85.36 & 108.30 & 65.35 & 96.74 & 60.44 \\ PVCP & & & Collision & 135.89 & 89.71 & 127.11 & 70.86 & 112.50 & 64.94 \\   & & & All & **145.77** & **97.50** & **124.04** & **76.34** & **112.43** & **70.87** \\  

Table 2: Effects of Dataset and Pre-training. Top use detected 2D pose sequences. Bottom use GT 2D pose sequences.

 
**Input** & **Pretrain** & **Iter** & **Class Loss** & **Pose class** & **MPVE** & **PAMPVE** & **MPJPE\_14j** & **PAMPJPE\_14j** & **MPJPE\_17j** & **PAMPJPE\_17j** \\   & ✓ & & & All & 282.50 & 163.58 & 243.59 & 132.43 & 222.70 & 123.33 \\  & ✓ & 3 & & All & 266.20 & 146.88 & 225.38 & 116.99 & 204.98 & 108.63 \\  & ✓ & & ✓ & All & 259.05 & **143.52** & 220.39 & 115.47 & 200.16 & 107.03 \\  & ✓ & 3 & ✓ & All & **257.75** & 144.19 & **218.61** & **114.50** & **198.16** & **105.86** \\   & ✓ & & & All & 145.77 & 97.50 & 124.04 & 76.34 & 112.43 & 70.87 \\  & ✓ & 3 & & All & 145.75 & 96.69 & 123.16 & 75.13 & 111.90 & 69.89 \\   & ✓ & & ✓ & All & 141.28 & **97.78** & 120.16 & **72.43** & 108.90 & **67.58** \\   & ✓ & 3 & ✓ & All & **140.43** & 96.43 & **118.80** & 75.13 & **107.47** & 69.56 \\  

Table 3: Component of system. Top use detected 2D pose sequences.

Due to the difference between 2D Det pose sequences and 2D GT pose sequences in the number and correct position of keypoints, when only iterative decode is used, the input error of 2D Det decreases significantly (\(MPVE_{det}=282.50mm 266.20mm\)), while that of 2D GT decreases slightly (\(MPVE_{gt}=145.77mm 145.75mm\)), which may be because multiple iterations improve the pose regression ability of incomplete pose. When only pose class loss is added, the error reduction space of the 2D GT input is significantly stronger (\(MPVE_{gt}=145.75mm 141.28mm\)) than that of the 2D Det input (\(MPVE_{det}=266.20mm 259.05mm\)), possibly because the pose that is complete and correctly positioned at the keypoints has a stronger correlation with the pose class label. Under the combined action of iterative regression decoder and loss function, both 2D pose sequence inputs achieve the minimum error (\(MPVE_{det}=257.75mm,MPVE_{gt}=140.43mm\)).

### Comparison with the state-of-the-art

**Quantitative comparison.** Similar to (69), Table 5 reports results for multiple baselines on the PVCP testset using the evaluation metrics described in Sec. 5.1. We compare the classic baseline methods of two paradigms: the one-stage method, which involves direct regression from image to mesh, and the two-stage method, which involves regression from 2D pose to 3D mesh. Following (66; 67), in the one-stage method, we used ResNet-50 (63) to extract the feature \(f^{i}^{2048}\) of the collision pedestrian clip-off image in each frame. Following (68), We use DarkPose (70) to extract 2D poses in COCO format (20). For MotionBERT (12) and our method, 2D pose is converted from 15 keypoints of JHMDB (60) to the corresponding 17 keypoints of Human3.6 (16) as input. Because the PVCP dataset contains only pose and shape annotations, there is a lack of spatial arrangement in the 3D scene. Therefore, the error of MPVE and MPJPE is large, but in PAMPVE and PAMPJPE, our method achieves the best accuracy. In addition, in the one-stage method, the effect of PARE (67) is relatively excellent, because PARE has optimized the occlusion of pedestrians and is well adapted

  
**Iter** & **Pose class** & **MPVE** & **PAMPVE** & **MPJPE\_14j** & **PAMPJPE\_14j** & **MPJPE\_17j** & **PAMPJPE\_17j** \\ 
2 & All & 141.95 & 97.43 & 120.04 & 75.45 & 108.63 & 69.85 \\
3 & All & 140.43 & **96.43** & 118.80 & **75.13** & 107.47 & **69.56** \\
4 & All & **139.96** & 96.92 & **118.46** & 75.19 & **107.16** & 69.62 \\
5 & All & 140.01 & 97.10 & 118.54 & 75.40 & 107.27 & 69.83 \\
6 & All & 140.41 & 97.42 & 118.89 & 75.70 & 107.68 & 70.14 \\   

Table 4: Comparison of 2D GT input in different iterations number.

  
**Paradigm** & **Method** & **Pose class** & **MPVE** & **PAMPVE** & **MPJPE\_14j** & **PAMPJPE\_14j** & **MPJPE\_17j** & **PAMPJPE\_17j** \\   & ^{}\)VIBE(66)} & Normal & 856.87 & 234.47 & 731.90 & 217.35 & – & – \\  & & Run & 856.10 & 232.67 & 732.33 & 226.45 & – & – \\  & & Avoid & 777.92 & 227.16 & 664.25 & 216.72 & – & – \\  & & Collision & 950.47 & 212.21 & 869.86 & 202.01 & – & – \\  & & All & **489.09** & 233.08 & **275.92** & **219.55** & – & – \\   & & Normal & 225.99 & 147.04 & 193.62 & 114.35 & – & – \\  & & Run & 235.99 & 180.98 & 193.04 & 137.08 & – & – \\  & & Apollo & 210.02 & 143.88 & 176.76 & 109.10 & – & – \\  & & Collision & 247.18 & 167.62 & 225.96 & 132.89 & – & – \\  & & All & **226.98** & 155.72 & **191.97** & **119.85** & – & – \\   & ^{}\)PoseMesh(68)} & Normal & 247.24 & 148.87 & 222.34 & 122.42 & – & – \\  & & Run & 255.26 & 181.16 & 227.33 & 145.14 & – & – \\  & & Avoid & 217.97 & 141.43 & 191.38 & 112.35 & – & – \\  & & Collision & 231.65 & 174.44 & 210.44 & 145.54 & – & – \\  & & All & **245.88** & 156.69 & **218.71** & 127.41 & – & – \\   & & Normal & 294.73 & 170.10 & 253.80 & 137.39 & 232.74 & 128.24 \\  & & & 253.16 & 149.99 & 219.06 & 124.01 & 200.19 & 115.27 \\  & & Avoid & 286.85 & 195.99 & 246.94 & 124.86 & 222.02 & 114.96 \\  & & Collision & 250.58 & 161.25 & 222.47 & 127.37 & 200.38 & 120.47 \\  & & All & **282.80** & **163.85** & **243.59** & **132.43** & 222.70 & 123.33 \\   & & Normal & 272.79 & 149.02 & 230.49 & 117.47 & 209.99 & 109.04 \\  & & Run & 226.22 & 133.45 & 193.75 & 109.50 & 174.47 & 100.73 \\   & & Avoid & 251.60 & 143.52 & 212.75 & 109.75 & 190.00 & 100.09 \\  & & Collision & 217.68 & 134.95 & 201.15 & 113.10 & 174.57 & 105.94 \\   & & All & **257.75** & **144.49** & **218.61** & **114.50** & **198.16** & **108.86** \\   

Table 5: Comparison of state-of-the-art methods on the PVCP testset. \({}^{}\) denotes that the training weights provided by the official are used, and * denotes the model weights trained together with the PVCP trainset.

to the real situation of the occlusion of pedestrians' lower limbs in the collision scene. In addition, the two-stage method needs to detect the 2D pose first, and the missing keypoints detection caused by occlusion will continue the error to the subsequent Mesh regression stage. However, in the case of single-class pre-collision poses (_Run, Avoid, Collision_), our method still has great advantages.

**Qualitative comparison.** A qualitative comparison of different methods is shown in Figure 3. Ignoring the global orientation and position, we manually adjust the Mesh output of different methods to the outline of the pedestrian in the picture, focusing on comparing the pre-collision pose itself. It can be seen that our method is most close to the real pedestrian pose in the image, and at the same time close to the estimated 2D pose, especially for the most complex pedestrian upper limb (row 4).

## 6 Conclusion

In this work, we construct the first Pedestrian-Vehicle Collision Pose (PVCP) dataset from the perspective of dashcams, which contains a variety of pedestrian pose representation annotation. At the same time, we propose a framework called PPSENet for the estimation of pedestrian pre-collision pose and shape. Specifically, a two-stage method is adopted: first, the pedestrian's 2D pose is estimated from the image, and then the pedestrian's 3D mesh is estimated from the 2D pose. Furthermore, we adopt the pose prior of the pre-trained model and the idea of iterative regression, introducing the pose class loss to achieve the minimum error on the PVCP testset, effectively estimating the pedestrian pre-collision pose in traffic collision scenarios. We hope that this work will offer new insights into human pose estimation and active pedestrian safety protection.

**Limitations.** Due to the difficulty of collecting the dataset, the dataset is small in size and lacks real camera parameters, vehicle speed information, global position and direction of pedestrians. Additionally, our task involves two-stage of pose estimation, and the final pose error is largely influenced by the 2D pose estimation results from the first stage. Our method is not real-time at present, because our input is Image and pre-selected Bbox sequence of collision pedestrian targets.

**Future Work.** From the comparison with state-of-the-art (SOTA) methods, it is evident that methods like PARE , which directly estimate 3D meshes of pedestrians from images, can also achieve good results. Future work can focus on new improvements in one-stage methods. Furthermore, we hope that the introduction of more modal information can further improve the accuracy of the estimation. The purpose of our work is to provide pose data support for the study of vehicle active and passive protection system, so as to facilitate subsequent accident reconstruction, pedestrian injury assessment and vehicle structural design.

Figure 3: Qualitative comparison. Left: Comparison with SOTA methods in PVCP testset. VIBE  and PARE  take images as input, P2M (Pose2Mesh)  and MotionBERT take detected 2D pose as input. Right: Output examples of our method in PVCP testset.