# Guoyang Zeng\({}^{2}\), Zhiyuan Liu\({}^{1}\)\({}^{,}\)\({}^{3}\), Maosong Sun\({}^{\dagger}\)\({}^{1}\)\({}^{,}\)\({}^{3}\)

H3T: Efficient Integration of Memory Optimization and Parallelism for High-Throughput Transformer Training

Yuzhong Wang\({}^{*}\)\({}^{1}\), Xu Han\({}^{*}\)\({}^{}\)\({}^{1}\), Weilin Zhao\({}^{*}\)\({}^{1}\)

**Guoyang Zeng\({}^{2}\), Zhiyuan Liu\({}^{1}\)\({}^{,}\)\({}^{3}\), Maosong Sun\({}^{}\)\({}^{1}\)\({}^{,}\)\({}^{3}\)**

\({}^{1}\) NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China

\({}^{2}\) ModelBest Inc., Beijing, China

\({}^{3}\) Collaborative Innovation Center for Language Ability, Jiangsu Normal University, China

{yz-wang21,zwl23}@mails.tsinghua.edu.cn

zengguoyang@modelbest.cn {hanxu2022,liuzy,sms}@tsinghua.edu.cn

Indicates equal contribution.

###### Abstract

In recent years, big models based on Transformers have achieved state-of-the-art performance on many artificial intelligence (AI) tasks. Despite the success of these Transformer-based models, their huge parameter size poses a serious challenge to their training, both from the storage and computation perspectives. To this end, memory optimization (e.g., rematerialization and offloading) and parallelism (e.g., data parallelism and model parallelism) are widely explored to make training Transformers more efficient. In this paper, we propose a framework to automatically find an efficient integration of memory optimization and parallelism for **H**igh-**T**hroughput **T**ransformer **T**raining (named **H3T**), which is rarely considered by existing efforts for training big Transformer-based models. Specifically, we design search algorithms to combine appropriate memory optimization strategies and parallelism schemes to achieve a balance between memory overhead and training efficiency. We implement H3T based on an open-source toolkit BMTrain and then use H3T to train the Transformers of different sizes to evaluate the efficiency of H3T. The experimental results show that H3T outperforms the most popular deep learning (DL) toolkit Megatron-DeepSpeed by \(1.2 4.3\) training speed while reducing \(34.6\% 80.5\%\) of memory overhead. Moreover, H3T can use only 64 NVIDIA A100 GPUs to train GPT-3-175B, which is very difficult for existing DL toolkits. The source code is available at https://github.com/OpenBMB/BMTrain/tree/h3t.

## 1 Introduction

In recent years, the emergence of Transformers  has significantly advanced the development of the AI field. Owing to the strong abilities of sequence modeling and transduction brought by the attention mechanisms, Transformer-based models have achieved state-of-the-art performance on many tasks and have become the foundation architecture for various AI directions, such as natural language processing , computer vision , and multimodal processing .

Despite the outstanding performance, with the trend of continuously growing model size, the computation and storage costs become a severe challenge for training most big Transformer-based models. For example, the popular big model GPT-3  has 175 billion parameters, and all these parameters require at least \(700\) GB of memory to store in float32 format, not to mention the forward activations,backward gradients, and optimizer states. Even ignoring the memory overhead, according to the existing analysis , it will take about 288 years to train GPT-3 with a single NVIDIA V100 GPU.

To tackle these issues, researchers explore various memory optimization strategies and parallelism techniques. Memory optimization strategies such as offloading [42; 6; 16; 2; 41; 37; 45] and rematerialization [12; 5; 20; 18; 19] aim to reduce the memory overhead of one (or each) single GPU. Parallelism techniques such as data parallelism [53; 30; 11] and parameter parallelism [8; 17; 27; 51; 36] mainly focus on improving both training efficiency and memory utilization across multiple GPUs. Some hybrid memory optimization strategies [49; 31; 3; 15] and automatic parallelism techniques [52; 46; 26] have also been proposed to further improve the storage and computation. Although promising results have been achieved on both tracks of memory optimization and parallelism, respectively, very limited efforts have been devoted to combining them both in an easy and efficient way.

In this paper, we propose a framework to automatically find an efficient integration of memory optimization and parallelism for **H**igh-**T**rroughput **T**ransformer **T**raining (named **H3T**), which is rarely considered by existing efforts for training big models. In H3T, we formalize memory optimization strategies (including offloading and rematerialization) and parallelism techniques (including data parallelism, parameter parallelism, and zero redundancy optimizer (ZeRO)) into multiple optimization switches. We then arrange an automatic solver that selects appropriate switches at each Transformer layer, while the core purpose of the solver is to achieve better training efficiency under the memory constraints. Since each transformer layer has a set of optimization switches independent of the other layers, the giant search space poses a challenge for the solver. Toward this end, we introduce greedy and dynamic programming (DP) algorithms to the searching process, enabling the solver to consider both search performance and search efficiency.

In experiments, our simulation studies show that H3T with our designed solvers outperforms traditional manual optimization without H3T. We also implement H3T based on BMTrain, an open-source toolkit that accelerates the training of big Transformers with various parallelism techniques. We conduct actual-running experiments to compare H3T and Megatron-DeepSpeed , one of the most popular distributed learning toolkits for DL. The results show H3T can train big Transformers \(1.2 4.3\) faster than Megatron-DeepSpeed while reducing the memory usage by \(34.6\% 80.5\%\). Moreover, supported by our implementation of H3T, it is possible to train GPT-3 with 175 billion parameters by using 64 NVIDIA A100 GPUs.

We summarize our contributions as follows: (1) We design H3T, which is, to our best knowledge, the first to automatically integrate memory optimization and parallelism for training big Transformers. (2) We conduct adequate experiments that show H3T outperforms conventional manual optimization as well as the most popular DL toolkit Megatron-DeepSpeed. (3) Supported by our implementation of H3T, we can train GPT-3 with 175 billion parameters on 64 NVIDIA A100 GPUs.

## 2 Background and Related Work

### Transformer-based Model

Transformer  is a widely-used deep neural network architecture. Due to the powerful capability of modeling data, the Transformer architecture is used to build various pre-trained models. These pre-trained models can capture rich knowledge from massive unlabeled data and then transfer the captured knowledge to handle various complex AI tasks. Transformer-based pre-trained models have become one of the cornerstones of recent AI developments, especially in the NLP domain. Currently, Transformer-based models achieve state-of-the-art performance on almost all NLP tasks [9; 21; 35; 33; 34; 4]. Inspired by the success of these models in the NLP domain, Transformers are also explored in other important AI domains like CV [10; 23; 22] and Multi-Modal (MM) [32; 39; 38; 29].

Despite the remarkable achievements of Transformers, it is a big challenge to train big Transformer-based models. On the one hand, the parameter size of popular models is increasing in an explosive way and has already exceeded Moore's Law. Take OpenAI's GPT as an example: the parameter sizes of GPT-1 , GPT-2 , and GPT-3  released in 2018, 2019, and 2020 are 117 million, 1.5 billion, and 175 billion, respectively. On the other hand, Transformers contain heavy dense algebraic operations, such as matrix multiplication, making the time and memory complexities of these operations much higher than the parameter size. The combination of rapid parameter growth and huge computational overhead challenges the DL community.

### Memory Optimization

**Rematerialization**. Rematerialization, also known as checkpointing, is the process of discarding intermediate results within the GPU and recomputing these discarded results when needed. Rematerialization is first proposed by Grimm et al.  and first implemented by Griewank et al. . Chen et al.  reduce the memory complexity of the conventional rematerialization method to \(()\) without changing the computational time complexity, which is a milestone that makes rematerialization known to researchers and implemented in many DL toolkits. However, Chen's work is based on artificial rules and cannot precisely balance memory overhead and training efficiency for different models and environments, limiting its performance and scalability. Many researchers devote their efforts to developing automatic rematerialization methods to tackle this issue. Dynamic programming (DP) [14; 20] and mixed-integer linear programming (MILP)  are both adopted to find better rematerialization schemes by automatically arranging which neural network layers need to be rematerialized to reduce the memory overhead while minimizing the efficiency loss.

**Offloading**. Offloading aims to transfer appropriate tensors from GPUs to CPUs and prefetch them back when they are required for computation. Preliminary offloading methods [42; 6; 44] mainly focus on using empirical rules to select the tensors for offloading. However, since offloading introduces complicated communication overhead for data transferring, it is not easy to empirically find the global optimal solution. For this reason, researchers attempt to offload tensors more intelligently. Huang et al.  perform tensor offloading based on a custom-designed genetic algorithm, while Beaumont et al.  use a DP algorithm. Besides the above offloading methods that mainly focus on activation tensors and convolutional neural networks (CNNs), some other researchers [41; 45] also explore offloading parameters and optimizer states, which can effectively reduce GPU memory usage for other types of model architectures like Transformers.

**Hybrid Memory Optimization**. Rematerialization saves memory by bringing additional tensor computation, while offloading saves memory by introducing extra tensor movement. These two optimization strategies can coexist, so integrating them and saving more memory is a natural idea. Early works [49; 31] attempt to intelligently combine them using heuristics and achieve remarkable performance. Afterward, some scholars explore using better algorithms, such as DP  and MILP , to model the integration of rematerialization and offloading and thus to find better hybrid optimization schemes. Although these hybrid methods have achieved promising results, existing works ignore incorporating them with parallelism techniques that are crucial for training big models. In view of this, we explore finding an efficient integration of memory optimization strategies and parallelism techniques for training big Transformers.

### Parallelism

**Data Parallelism**. Data parallelism [53; 30; 11] is a typical approach for distributed DL. Its main idea is to divide the batch of training data into several mini-batches and process them on different devices (GPUs) in parallel. To synchronize the model parameters distributed on multiple devices, the gradients of parameters must be averaged among all devices before updating parameters in the parameter optimizer. Based on data parallelism, zero redundancy optimizer (ZeRO)  is proposed to partition the optimizer states and parameter gradients, which can significantly reduce the memory overhead of data parallelism without changing the computational complexity.

**Model Parallelism**. In recent years, the parameter size of DL models is continuously growing, and the conventional training scheme that allocates the entire model on all devices can easily run out of memory. To tackle this issue, researchers explore various model parallelism methods that distribute model parameters over multiple devices. There are generally two branches of model parallelism. One is pipeline parallelism [8; 17; 27; 51], which partitions model parameters by layer level and lets devices be responsible for the computation of different layers. The other is parameter parallelism [28; 40], which partitions parameter matrices within a layer to enable distributed computing in multiple devices. Like data parallelism, model parallelism can also be incorporated into ZeRO by partitioning the model at the tensor level and gathering the required parameters before computation. Most recent distributed DL toolkits for training big Transformers, such as BMTrain and Megatron-DeepSpeed, are designed based on data parallelism, model parallelism, and ZeRO.

**Automatic Parallelism**. Although many efforts have attempted to combine model parallelism and data parallelism to improve efficiency further, finding the optimal combination based on empiricalrules is not easy. To this end, some scholars explore automatic parallelism methods. Galvatron  uses a decision tree and a DP algorithm to find a hybrid parallelism solution among multiple parallelism dimensions. Alpa  and Unity  focus on finding the parallelism scheme at the computational graph level and use DP algorithms to search for the scheme based on their defined parallel operators. However, most existing works of automatic parallelism pay more attention to execution efficiency and rarely involve memory optimizations, which makes the memory overhead suboptimal for training big models. Therefore, we introduce memory optimization strategies to the existing parallelism schemes for better results.

## 3 H3T Framework

### Formalization of Training Transformers

We first formalize the process of training Transformers. Generally, training DL models involves three stages: forward propagation, backward propagation, and parameter optimization. As a typical sequential model, Transformers can be formalized as an ordered sequence of neural layers, with each layer only connected to adjacent ones. So we go deep into the layer aspect and define \(_{i},_{i},_{i}\) as the three stages at the \(i\)-th layer, respectively.

In the forward propagation, \(_{i}\) takes \(_{i}\) and \(_{i}\) to calculate \(_{i+1}\), where \(_{i}\) and \(_{i+1}\) are the hidden states, and \(_{i}\) is the parameters of the \(i\)-th layer. Along with generating some intermediate activations \(_{i}\), the forward propagation at the \(i\)-th layer can be formalized as

\[_{i+1},_{i}=_{i}(_{i},_{i}).\] (1)

The final output \(_{N+1}\) is used to calculate the loss function and the initial gradient \(_{N+1}\). Then, in the backward propagation, \(_{i}\) is to calculate the gradient \(_{i}\) and \(_{i}\), by using \(_{i+1}\), \(_{i}\), \(_{i}\), and the intermediate activations \(_{i}\) generated by \(_{i}\), which can be given as

\[_{i},_{i}=_{i}(_ {i},_{i},_{i},_{i+1}).\] (2)

After the backward propagation, \(_{i}\) is to use some parameter optimization functions to update the parameters \(_{i}\) according to the gradients \(_{i}\).

In each training step, the conventional training way is first to run \(_{i}\) from \(i=1\) to \(N\), then run \(_{i}\) in reverse order, and finally use the parameter optimizer to update parameters according to gradients. For H3T, the training step is similar to the conventional one, but it is allowed to adjust the execution order or add some additional operations under the premise that the gradients are calculated correctly and the parameter optimization operations are performed stably, which can bring us much flexibility and convenience to integrate memory optimization and parallelism.

### Optimization Switches

#### 3.2.1 Activation-aspect Optimization Switches

For the sequential model, activations involve hidden states \(_{i}\) and intermediate activations \(_{i}\). They play important roles in model training and meanwhile consume a lot of GPU memory, especially intermediate activations . As mentioned in the background, scholars have developed two major approaches, rematerialization and offloading, to reduce the memory overhead caused by activations.

Considering the properties of hidden states and intermediate activations, we adopt rematerialization to optimize the memory of intermediate activations while using offloading to save the memory of

Figure 1: The overall architecture of H3T.

hidden states. This setting is also applied by some existing work of hybrid optimization . The two optimization switches are illustrated in Figure 2.

**Rematerializing intermediate activations** is to evict \(_{i}\) after forward steps and recompute \(_{i}\) before backward steps. To differentiate, we define \(_{i}^{*}\) as the forward computation without saving \(_{i}\). As Figure 2 shows, rematerialization for the \(i\)-th layer replaces \(_{i}\) with \(_{i}^{*}\) in the forward stage, and insert an extra \(_{i}\) before \(_{i}\) in the backward stage.

**Offloading hidden states** is to offload \(_{i}\) and prefetch them before the backward propagation steps. Different from intermediate activations, if we similarly try to evict and recompute hidden states, the training process will suffer from excessive recomputations, and the performance will also be affected to a great extent. To this end, offloading hidden states can be more efficient.

These two optimizations can significantly reduce the memory cost of activations. Specifically, switching on both of them for every layer will reduce the resident activation memory to \(0\).

#### 3.2.2 Parameter-aspect Optimization Switches

Traditional training process store all \(_{i}\) and \(_{i}\) on GPUs. As described above, \(_{i}\) and \(_{i}\) are only used for \(_{i}\), \(_{i}\), and \(_{i}\). Therefore, it is a straightforward idea to reduce this part of memory overhead by offloading optimizer states, parameters, and gradients to CPU RAM, as shown in Figure 2(a). Generally, the number of gradients equals the number of parameters, and for most common optimizers, the number of optimizer states equals or even exceeds the number of trainable parameters. Therefore, offloading them is an efficient way to reduce the GPU memory overhead. On this basis, we further implement a high-performance CPU optimizer3 that runs \(_{i}\) on CPUs.

Moreover, we employ parameter parallelism for H3T. Specifically, we store partitioned parameters \(_{i}^{(j)}\) on each GPU. Before \(_{i}\), we use an all gather operation to gather them from all GPUs to get \(_{i}\) for computation. After \(_{i}\), we evict the gathered \(_{i}\) and use a reduce scatter operation to average and scatter the \(_{i}\) to each GPU.

In this way, the parameters and gradients form a closed loop, as shown in Figure 2(a). Due to the holistic nature and outstanding memory-saving performance, we globally turn on the above offloading optimizations and parallelism in this paper.

In addition to the above global optimizations, we have two advanced parameter-aspect switches.

**Parameter partition** (Figure 2(b)) is to release gathered parameters after using them and gather them before their next use. Parameter partition mainly focuses on saving the wasted memory between the forward and backward steps of each layer, which is an issue overlooked by model parallelism.

**Lazy-prefetching** (Figure 2(c)) is to evict the partitioned parameters after every reduce scatter operation and lazily prefetch them back before every all gather operation. Lazy-prefetching aims to eliminate the memory cost of resident partitioned parameters.

Both parameter partition and lazy-prefetching can avoid the long-term persistence of parameters during the forward-to-backward period, thus effectively saving the memory of parameters. Specifically, turning on both optional switches for every layer will reduce the resident parameter memory cost to \(0\).

Figure 2: Activation-aspect switches.

#### 3.2.3 Implementation: Concurrent Streams and Execution Switches

We implement the above optimizations with three concurrent streams for H3T. Specifically, we have (1) A calculation stream that takes charge of forward, backward, and parameter optimization. (2) An NVLink4 stream that covers the GPU-GPU communication operators, i.e., all gather and reduce scatter. (3) A PCI-E stream that covers the GPU-CPU communication operators, i.e., offloading and prefetching. Following most previous works , asynchronous communication and computation can overlap different parts of the time overhead, thus minimizing the efficiency loss. As illustrated in Figure 4, the main idea of our implementation is to overlap the post-process (scattering and offloading) of the previous layer and the preparation (gathering and prefetching) of the next layer with the computation of the current layer, and we call these overlapped operations a "segment".

Furthermore, we design three execution switches for H3T. **Economical forward** and **economical backward** prioritize the scattering and offloading of the previous layer in order to save more memory. In contrast, non-economical modes prioritize gathering and prefetching of the next layer, which may reduce the delay in the computation of the next layer. **Segment synchronization** is to force all streams to be synchronized between two segments. More details about the implementation and the three switches are introduced in the appendix.

Figure 4: An illustration of backward segment \(i\). Without loss of generality, we suppose the switches of layer \(i-1\) and \(i+1\) are all switched on.

Figure 3: Parameter-aspect switches.

### Optimization Solver

The optimizations save memory while bringing performance losses. To tackle this, H3T involves a solver to decide the appropriate switches to achieve the trade-off between memory and speed.

#### 3.3.1 Problem Definition

We first state the problem definition for the solver. We define \(=(s_{1},s_{2},,s_{N})\) as an optimization sequence for the model, where \(s_{i} S\) represents the optimization scheme for the \(i\)-th layer, while \(S\) is the set of all combinations of optimization switches for one single layer. In this paper, \(S\) involves all combinations of the \(7\) layer-level switches described above, so we have \(|S|=2^{7}=128\).

We define \((),()\) as the runtime (per step) and the memory overhead (per device) given the model and the optimization sequence \(\), respectively. Then, the goal of the solver is to find an optimization sequence \(\) with lower \(()\) under the memory constraint \(M\) (per device):

\[(),()  M.\] (3)

A key problem is how to precisely estimate \(()\) and \(()\). Towards this goal, we introduce a profiler that collects the runtime and memory overhead of each operation of each layer at the first several training steps. Then we implement a model simulator to simulate the training process of each layer based on the model architecture and implementation, thus helping us estimate \((),()\). Besides, the model simulator can also help the solver evaluate any solutions or sub-solutions when exploring different optimization schemes.

How to pick appropriate switches is another big challenge. The optimization of different layers can be in any combination, so the number of global solutions can be up to \(|S|^{N}\). It is not easy to find a good solution from such exponential search space with the memory constraint and toward the goal of minimizing the training runtime. Toward this goal, we design three search algorithms in this paper.

#### 3.3.2 Search Algorithms

**Random solver**. Brute force does not work for our task because of the exponential search space, so we design a simple method based on random sampling to find a suboptimal solution more efficiently. Specifically, we randomly sample several optimization sequences from \(S^{N}\) and pick the best one.

**Greedy solver**. The random solver is simple but not effective for large-scale models. The main reason is that the search space grows exponentially with \(N\), but the sampling time is constant, which means the probability we hit a good solution becomes much lower with the growth of the model size. To address these issues, we design a greedy solver to search for better solutions. The core idea of our greedy solver is first assuming all optimization switches are turned on, then greedily turning off some switches to increase memory utilization and improve training efficiency. Due to the space limitation, detailed descriptions of the greedy solver are in the appendix.

**Dynamic Programming (DP) Solver**. Our task strives for better runtime under the memory constraint, similar to the knapsack problem  that pursues higher value under the weight constraint. Inspired by the classic dynamic programming (DP) algorithm for the knapsack problem , we design a search algorithm based on DP to solve our task better. Specifically, our DP algorithm considers the optimal sub-solutions for specified prefixes and memory limits. When transferring, we need to enumerate the next layer's switches and estimate the next segment's running status.

Formally, the DP state can be represented by the following variables: (1) \(i\): The prefix length, which means this state is considering segment \(1 i\) (including forward segment \(1 i\) and backward segment \(i 1\), similarly hereinafter). (2) \(_{}\): The memory usage after forward segment \(1 i\).

(The memory usage before backward segment \(i 1\) is essentially equal and can be easily calculated.) (3) \(}\): The peak memory usage during segment \(1 i\). (We have to record \(}\) because the resident memory may increase due to non-lazy-prefetching, leading to an increase in the peak memory from the previous segments.) (4) \(s_{i},s_{i+1}\): The switches of layer \(i\) and \(i+1\), which is used to simulate segment \(i+1\).

We use \((i,},},s_{i},s_{i+1})\) to represent the optimal sub-solution of the state. Then we can enumerate \(s_{i+2}\) to transfer to the next state \((i+1,_{curr}},_{peak}},s_{i+1},s_{i+2})\). During the transfer, we need to: (1) Simulate the forward segment \(i+1\) and backward segment \(i+1\) based on \(s_{i},s_{i+1},s_{i+2}\). (2) Calculate \(_{curr}},_{peak}}\) and the total runtime based on \(},}\), the previous runtime, and the simulation result. (3) Update \((i+1,_{curr}},_{peak}},s_ {i+1},s_{i+2})\) if the peak memory \(_{peak}}\) does not exceed \(M\) and the runtime is shorter than the existing solution's.

There are two noteworthy details for our DP algorithm. First, considering the overlapping sub-problems condition, we must ensure the state is finite, so we discretize the memory limit into \(m+1\) values that are evenly distributed in \([0,M]\). Second, following the condition of optimal substructure, the problem for each state must be reducible to the corresponding sub-problem for the other states. In this paper, only cross-segment communications violate this condition, so we force segment synchronization switched on and remove it from the switch set (thus \(|S|\) reduce to \(64\)). These two assumptions may affect the result but make our algorithm design much simpler and more efficient. We leave the design of the more comprehensive DP algorithm as our future work.

In addition to the basic version of the DP algorithm, we have two prunings to improve its efficiency. Please refer to the appendix for them along with the pseudo-code and time complexity analysis.

## 4 Experiments

### Experimental Settings

**Models**. We use BERT, one of the most common Transformer-based language models, as an example for our experiments. We customize different BERT sizes for our experiments, including 1.8B, 6B, 13B, and 100B. Traditional small models, such as BERT-base (0.1B) or BERT-large (0.3B), are not considered in this paper because they can be easily trained using conventional approaches.

**Environments**. We test the performance of H3T in three different experimental environments: (1) 1 node of 8 \(\) NVIDIA GeForce RTX 2080 Ti 10 GB (8 \(\) 2080Ti); (2) 1 node of 8 \(\) NVIDIA A100-SXM4-40GB with NVLink 3.0 (8 \(\) A100); and (3) 8 nodes of 8 \(\) A100 (64 \(\) A100).

**Implementations**. We implement H3T based on BMTrain, an open-source toolkit that accelerates the training of big Transformers with various parallelism techniques.

**Baselines**. Except for H3T with random solver and greedy solver, we introduce manual optimization strategy without H3T and another toolkit Megatron-DeepSpeed for comparison. (1) In the simulation study, we test the performance of manually enumerating the combination of global optimizations and picking the best one. In other words, we simulate arbitrarily and globally turning on or off all the switches and picking the best-performing solution that does not exceed the memory limit. (2) In the actual-running experiment, we attempt to turn on all optimization switches to achieve the lowest memory overhead since testing all manual optimization schemes is time-consuming and resource-intensive. (3) We introduce one of the most popular distributed deep learning toolkits, Megatron-DeepSpeed , for comparison in our actual-running experiment. For a fair comparison, we test Megatron-DeepSpeed with ZeRO-2 and ZeRO-3 respectively (ZeRO-2 distributes optimizer states and gradients; ZeRO-3 distributes optimizer states, gradients, and parameters ). Please refer to the appendix and the code for detailed configurations of Megatron-DeepSpeed and H3T in the actual-running experiment.

We do not test POFO , which also uses DP to solve the memory optimization problem for big model training. We consider it unfair to compare POFO with H3T because POFO does not support any parameter-aspect optimization, such as parameter offloading and parallelism. Considering the large model size, it is almost impossible for a vanilla POFO to train the large-scale language models in this paper. If we arbitrarily implement parameter optimizations for POFO, additional communication will affect the well-designed offloading solution of POFO and seriously hurt its performance.

**Other settings**. Please refer to the appendix for other detailed experimental settings, including detailed model size, hardware configurations, training settings, etc.

### Simulation Studies

In this section, we report our simulation studies to evaluate the ideal performance of H3T. We profile the models in different environments and use the model simulator to evaluate the performance of different solvers and the manual strategy. The result is shown in Figure 6.

The result shows that our DP solver outperforms the other three baseline approaches in most cases. The greedy solver also performs well and is usually close to DP. Both DP and greedy solver have the flexibility to adjust the optimization solution based on variant memory constraints. This conclusion is applicable in all three environments, showing H3T has good adaptability to GPUs with different numbers, performance, and connections. In contrast, the random solver is less stable and less effective. Although the performance gap is not significant when memory is sufficient, the random solver, more often than not, struggles even to find an available solution.

Compared to three automatic solvers, manual optimization without H3T is often the worst, especially when memory is sufficient. This indicates that our layer-level automatic solver is able to precisely decide the optimization scheme for large-scale Transformer-based models given the memory constraint. In contrast, the traditional manual strategy performs worse and is heavy for users as well.

However, the above rules do not always stand. For BERT-1.8B with a batch size of 128 on 8\(\)2080Ti, the greedy solver underperforms the random solver. For BERT-6B on 8\(\)2080Ti, the DP solver continuously underperforms the greedy solver and even underperforms the manual strategy in some cases. This is because the local optimality may let the greedy algorithm fall into a suboptimum, and the segment synchronization assumption and the memory discretization may influence the DP algorithm. We argue that it is normal for such problems to occur in a few cases.

### Actual-running Experiments

We conduct our actual-running experiment in the three environments, and the result is shown in Table 1. The results demonstrate our great advantage over Megatron-DeepSpeed. Compared to the Megatron-DeepSpeed with ZeRO-2 (ZeRO-3), H3T improves the training speed by \(1.2 1.8\) (\(1.9 4.3\)) in our experiments. Comparing the results of BERT-6B and BERT-13B, both with the batch size of \(128\) and both on 8\(\)A100, we find that H3T outperforms Megatron-DeepSpeed more significantly for larger model size, which means H3T is not only faster but also more adapted to bigger models.

Figure 6: The results of the simulation study.

Focusing on the results of BMTrain, we find that H3T with DP solver and greedy solver both outperform BMTrain without H3T by up to \(1.2\) speed up when memory is sufficient. In contrast, when memory is tight, the two automatic solvers can also enable more memory optimization and trade time performance for keeping model training. This result is consistent with the findings from the simulation study, which further verifies the flexibility of our automatic solvers. Also, similar to the simulations, the performance of DP and greedy are close, but DP still outperforms greedy a little in most cases. Besides, the random solver fails to solve for all of the settings, so we do not report its results in the table.

The memory cost of our implementation is also better than Megatron-DeepSpeed. From Table 1, we find Megatron-DeepSpeed often runs out of memory for larger models and batch sizes, which are better supported by H3T. Hence, users can train larger models with more limited hardware, e.g., train BERT-6B on 8\(\)2080Ti. Not only that, **H3T can train GPT-3-175B  on 64\(\)A100 with a batch size of 512 and using only about 11 GB of memory per GPU. This is really an exciting result because we would never have dreamed that a 175B-parameter model could be trained with such limited hardware, not to mention with such low GPU memory overhead.** We conduct a comprehensive memory test in the appendix and find our implementation can save \(34.6\% 80.5\%\) GPU memory compared with Megatron-DeepSpeed under various settings.

Besides, we verify the correctness of our implementation. (1) We fix the input data and check whether the output loss equals the release version of BMTrain during the whole training process, which justifies our incremental development based on BMTrain. (2) We do an end-to-end training experiment on \(4\) actual tasks of SuperGLUE . The results show that H3T can achieve comparable results with PyTorch, which confirms the soundness of the whole system. The detailed experimental results on SuperGLUE are in the appendix.

We conduct three extra experiments in this paper. Due to the space limitation, we briefly introduce them here and elaborate on the details in the appendix. (1) We conduct an energy test and find that H3T is more energy-efficient than Megatron-DeepSpeed for training big Transformer-based models. (2) We test the efficiency of the solver to confirm it is not too slow. (3) We conduct a case study to show the automatically generated optimization scheme makes sense.

## 5 Conclusion

In this paper, we propose a novel framework for automatic integration of memory optimization and parallelism for **H**igh-**T**hroughput **T**ransformer **T**raining, named **H3T**. Despite the challenge of large search space, H3T significantly outperforms some manual strategies and one of the most popular DL toolkits, Megatron-DeepSpeed, in our experiments. More excitingly, H3T can use only 64 NVIDIA A100 GPUs to train Transformer-based models with more than 100 billion parameters like GPT-3-175B. We hope that the release of H3T will lower the barrier to training big Transformer-based models and promote the development of the AI community.