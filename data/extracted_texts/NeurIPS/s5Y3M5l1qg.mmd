# Carefully Blending Adversarial Training and Purification Improves Adversarial Robustness

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In this work, we propose a novel adversarial defence mechanism for image classification - Carso - blending the paradigms of _adversarial training_ and _adversarial purification_ in a synergistic robustness-enhancing way. The method builds upon an adversarially-trained classifier, and learns to map its _internal representation_ associated with a potentially perturbed input onto a distribution of tentative _clean_ reconstructions. Multiple samples from such distribution are classified by the same adversarially-trained model, and an aggregation of its outputs finally constitutes the _robust prediction_ of interest. Experimental evaluation by a well-established benchmark of strong adaptive attacks, across different image datasets, shows that Carso is able to defend itself against adaptive _end-to-end white-box_ attacks devised for stochastic defences. Paying a modest _clean_ accuracy toll, our method improves by a significant margin the _state-of-the-art_ for CIFAR-10, CIFAR-100, and TinyImageNet-200 \(_{}\) robust classification accuracy against AutoAttack.

## 1 Introduction

Vulnerability to adversarial attacks [8; 57] - _i.e._ the presence of inputs, usually crafted on purpose, capable of catastrophically altering the behaviour of high-dimensional models  - constitutes a major hurdle towards ensuring the compliance of deep learning systems with the behaviour expected by modellers and users, and their adoption in safety-critical scenarios or tightly-regulated environments. This is particularly true for adversarially-_perturbed_ inputs, where a norm-constrained perturbation - often hardly detectable by human inspection [48; 5] - is added to an otherwise legitimate input, with the intention of eliciting an anomalous response .

Given the widespread nature of the issue , and the serious concerns raised about the safety and reliability of data-learnt models in the lack of an appropriate mitigation , adversarial attacks have been extensively studied. Yet, obtaining generally robust machine learning (_ML_) systems remains a longstanding issue, and a major open challenge.

Research in the field has been driven by two opposing, yet complementary, efforts. On the one hand, the study of _failure modes_ in existing models and defences, with the goal of understanding their origin and developing stronger attacks with varying degrees of knowledge and control over the target system [57; 21; 44; 60]. On the other hand, the construction of increasingly capable defence mechanisms. Although alternatives have been explored [15; 59; 11; 68], most of the latter is based on adequately leveraging _adversarial training_[21; 42; 58; 49; 23; 31; 54; 62; 18; 47], _i.e._ training a _ML_ model on a dataset composed of (or enriched with) adversarially-perturbed inputs associated with their correct, _pre-perturbation_ labels. In fact, adversarial training has been the only technique capable of consistently providing an acceptable level of defence , while still incrementally improving up to the current _state-of-the-art_[18; 47].

Another defensive approach is that of _adversarial purification_[53; 66], where a generative model is used - similarly to denoising - to recover a perturbation-free version of the input before classification is performed. Nonetheless, such attempts have generally fallen short of expectations due to inherent limitations of the generative models used in early attempts , or due to decreases in robust accuracy1 when attacked _end-to-end_ - resulting in subpar robustness if the defensive structure is known to the adversary . More recently, the rise of diffusion-based generative models  and their use for purification have enabled more successful results of this kind [45; 13] - although at the cost of much longer training and inference times, and a much brittle robustness evaluation [13; 38].

In this work, we design a novel adversarial defence for supervised image classification, dubbed Carso (_i.e._, Counter-Adversarial Recall of Synthetic Observations). The approach relies on an adversarially-trained classifier (called hereinafter simply _the classifier_), endowed with a stochastic generative model (called hereinafter _the purifier_). Upon classification of a potentially-perturbed input, the latter learns to generate - from the tensor2 of (pre)activations registered at neuron level in the former - samples from a distribution of plausible, perturbation-free reconstructions. At inference time, some of these samples are classified by the very same _classifier_, and the original input is robustly labelled by aggregating its many outputs. This method - to the best of our knowledge the first attempt to organically merge the _adversarial training_ and _purification_ paradigms - avoids the vulnerability pitfalls typical of the mere stacking of a purifier and a classifier , while still being able to take advantage of independent incremental improvements to adversarial training or generative modelling.

An empirical assessment3 of the defence in the _white-box_ setting is provided, using a _conditional_[56; 64]_variational autoencoder_[32; 50] as the purifier and existing _state-of-the-art_ adversarially pre-trained models as classifiers. Such choices are meant to give existing approaches - and the _adversary_ attacking our architecture _end-to-end_ as part of the assessment - the strongest advantage possible. Yet, in all scenarios considered, Carso improves significantly the robustness of the pre-trained classifier - even against attacks specifically devised to fool stochastic defences like ours. Remarkably, with a modest _clean_ accuracy toll, our method improves by a significant margin the current _state-of-the-art_ for Cifar-10 , Cifar-100 , and TinyImageNet-200 \(_{}\) robust classification accuracy against AutoAttack.

In summary, the paper makes the following contributions:

* The description of Carso, a novel adversarial defence method synergistically blending _adversarial training_ and _adversarial purification_;
* but applicable to more general scenarios as well;
* showing higher robust accuracy _w.r.t._ to existing _state-of-the-art_ adversarial training and purification approaches.

The rest of the manuscript is structured as follows. In section 2 we provide an overview of selected contributions in the fields of _adversarial training_ and _purification-based_ defences - with focus on image classification. In section 3, a deeper analysis is given of two integral parts of our experimental assessment: Pgd adversarial training and conditional variational autoencoders. Section 4 is devoted to the intuition behind Carso, its architectural description, and the relevant technical details that allow it to work. Section 5 contains details about the experimental setup, results, comments, and limitations. Section 6 concludes the paper and outlines directions of future development.

## 2 Related work

Adversarial training as a defenceThe idea of training a model on adversarially-generated examples as a way to make it more robust can be traced back to the very beginning of research in the area.

The seminal  proposes to perform training on a mixed collection of _clean_ and adversarial data, generated beforehand.

The introduction of Fgsm enables the efficient generation of adversarial examples along the training, with a single normalised gradient step. Its iterative counterpart Pgd - discussed in section 3 and Appendix A - significantly improves the effectiveness of adversarial examples produced, making it still the _de facto_ standard for the synthesis of adversarial training inputs . Further incremental improvements have also been developed, some focused specifically on robustness assessment (_e.g._ adaptive-stepsize variants, as in ).

The most recent adversarial training protocols further rely on synthetic data to increase the numerosity of training datapoints , and adopt adjusted loss functions to balance robustness and accuracy  or generally foster the learning process . The entire model architecture may also be tuned specifically for the sake of robustness enhancement . At least some of such ingredients are often required to reach the current _state-of-the-art_ in robust accuracy via adversarial training.

Purification as a defenceAmongst the first attempts of _purification-based_ adversarial defence,  investigates the use of denoising autoencoders  to recover examples free from adversarial perturbations. Despite its effectiveness in the denoising task, the method may indeed _increase_ the vulnerability of the system when attacks are generated against it _end-to-end_. The contextually proposed improvement adds a smoothness penalty to the reconstruction loss, partially mitigating such downside . Similar in spirit,  tackles the issue by computing the reconstruction loss between the last-layers representations of the frozen-weights attacked classifier, respectively receiving, as input, the _clean_ and the tentatively _denoised_ example.

In , _Generative Adversarial Networks_ (GANs)  learnt on _clean_ data are used at inference time to find a plausible synthetic example - close to the perturbed input - belonging to the unperturbed data manifold. Despite encouraging results, the delicate training process of GANs and the existence of known failure modes  limit the applicability of the method. More recently, a similar approach  employing _energy-based models_ suffered from poor sample quality .

Purification approaches based on (conditional) variational autoencoders include  and . Very recently, a technique combining variational manifold learning with a test-time iterative purification procedure has also been proposed .

Finally, already-mentioned techniques relying on _score-_ and _diffusion-_ based  models have also been developed, with generally favourable results - often balanced in practice by longer training and inference times, and a much more fragile robustness assessment .

## 3 Preliminaries

Pgd adversarial trainingThe task of finding model parameters robust to adversarial perturbations is framed by  as a _min-max_ optimisation problem seeking to minimise _adversarial risk_. The inner optimisation (_i.e._, the generation of worst-case adversarial examples) is solved by an iterative algorithm - _Projected Gradient Descent_ - interleaving gradient ascent steps in input space with the eventual projection on the shell of an \(\)-ball centred around an input datapoint, thus imposing a perturbation strength constraint.

In this manuscript, we will use the shorthand notation \(_{p}\) to denote \(_{p}\) norm-bound perturbations of maximum magnitude \(\).

The formal details of such method are provided in Appendix A.

(Conditional) variational autoencodersVariational autoencoders (_VAEs_)  allow the learning from data of approximate generative latent-variable models of the form \(p(,)=p(\,|\,)p()\), whose likelihood and posterior are approximately parameterised by deep artificial neural networks (_ANN_). The problem is cast as the maximisation of a variational lower bound.

In practice, optimisation is performed iteratively - on a loss function given by the linear mixture of data-reconstruction loss and empirical _KL_ divergence _w.r.t._ a chosen prior, computed on mini-batches of data.

_Conditional_ Variational Autoencoders [56; 64] extend _VAE_s by attaching a _conditioning tensor_\(\) - expressing specific characteristics of each example - to both \(\) and \(\) during training. This allows the learning of a decoder model capable of conditional data generation.

Further details on the functioning of such models are given in Appendix B.

## 4 Structure of Carso

The core ideas informing the design of our method are driven more by _first principles_ rather than arising from specific contingent requirements. This section discusses such ideas, the architectural details of Carso, and a group of technical aspects fundamental to its training and inference processes.

### Architectural overview and principle of operation

From an architectural point of view, Carso is essentially composed of two _ANN_ models - a _classifier_ and a _purifier_ - operating in close synergy. The former is trained on a given classification task, whose inputs might be adversarially corrupted at inference time. The latter learns to generate samples from a distribution of potential input reconstructions, tentatively free from adversarial perturbations. Crucially, the _purifier_ has only access to the internal representation of the _classifier_ - and not even directly to the perturbed input - to perform its task.

During inference, for each input, the internal representation of the _classifier_ is used by the _purifier_ to synthesise a collection of tentatively unperturbed input reconstructions. Those are classified by the same _classifier_, and the resulting outputs are aggregated into a final _robust prediction_.

There are no specific requirements for the classifier, whose training is completely independent of the use of the model as part of Carso. However, training it adversarially improves significantly the _clean_ accuracy of the overall system, allowing it to benefit from established adversarial training techniques.

The purifier is also independent of specific architectural choices, provided it is capable of stochastic conditional data generation at inference time, with the internal representation of the classifier used as the conditioning set.

In the rest of the paper, we employ a _state-of-the-art_ adversarially pre-trained WideResNet model as the classifier, and a purpose-built _conditional variational autoencoder_ as the purifier, the latter operating decoder-only during inference. Such choice was driven by the deliberate intention to assess the adversarial robustness of our method in its worst-case scenario against a _white-box_ attacker, and with the least advantage compared to existing approaches based solely on adversarial training.

In fact, the decoder of a conditional VAE allows for exact algorithmic differentiability _w.r.t._ its conditioning set, thus averting the need for backward-pass approximation  in generating _end-to-end_ adversarial attacks against the entire system, and preventing (un)intentional robustness by gradient obfuscation . The same cannot be said  for more capable and modern purification models, such as those based _e.g._ on diffusive processes, whose robustness assessment is still in the process of being understood .

A downside of such choice is represented by the reduced effectiveness of the decoder in the synthesis of complex data, due to well-known model limitations. In fact, we experimentally observe a modest increase in reconstruction cost for non-perturbed inputs, which in turn may limit the _clean_ accuracy of the entire system. Nevertheless, we defend the need for a fair and transparent robustness evaluation, such as the one provided by the use of a VAE-based purifier, in the evaluation of any novel architecture-agnostic adversarial defence technique.

A diagram of the whole architecture is shown in Figure 1, and its detailed principles of operation are recapped below.

TrainingAt training time, adversarially-perturbed examples are generated against the _classifier_, and fed to it. The tensors containing the _classifier_ (pre)activations across the network are then extracted. Finally, the conditional _VAE_ serving as the _purifier_ is trained on perturbation-free input reconstruction, conditional on the corresponding previously extracted internal representations, and using pre-perturbation examples as targets.

Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.

InferenceThe example requiring classification is fed to the _classifier_. Its corresponding internal representation is extracted and used to condition the generative process described by the decoder of the _VAE_. Stochastic latent variables are repeatedly sampled from the original priors, which are given by an _i.i.d._ multivariate Standard Normal distribution. Each element in the resulting set of reconstructed inputs is classified by the same _classifier_, and the individually predicted class logits are aggregated. The result of such aggregation constitutes the robust prediction of the input class.

Remarkably, the only link between the initial potentially-perturbed input and the resulting purified reconstructions (and thus the predicted class) is through the _internal representation_ of the classifier, which serves as a _featurisation_ of the original input. The whole process is exactly differentiable _end-to-end_, and the only potential hurdle to the generation of adversarial attacks against the entire system is the stochastic nature of the decoding - which is easily tackled by _Expectation over Transformation_.

### A first-principles justification

If we consider a trained _ANN_ classifier, subject to a successful adversarial attack by means of a slightly perturbed example, we observe that - both in terms of \(_{p}\) magnitude and human perception - a small variation on the input side of the network is amplified to a significant amount on the output side, thanks to the layerwise processing by the model. Given the deterministic nature of such processing at inference time, we speculate that the _trace_ obtained by sequentially collecting the (pre)activation values within the network, along the forward pass, constitutes a richer characterisation of such an amplification process compared to the knowledge of the input alone. Indeed, as we do, it is possible to learn a direct mapping from such featurisation of the input, to a distribution of possible perturbation-free input reconstructions - taking advantage of such characterisation.

### Hierarchical input and internal representation encoding

Training a conditional VAE requires  that the conditioning set \(\) is concatenated to the input \(\) before encoding occurs, and to the sample of latent variables \(\) right before decoding. The same is

Figure 1: Schematic representation of the Carso architecture used in the experimental phase of this work. The subnetwork bordered by the red dashed line is used only during the training of the _purifier_. The subnetwork bordered by the blue dashed line is re-evaluated on different random samples \(_{i}\) and the resulting individual \(_{i}\) are aggregated into \(_{}\). The _classifier_\(f(;)\) is always kept frozen; the remaining network is trained on \(_{}(,})\). More precise details on the functioning of the networks are provided in subsection 4.1.

also true, with the suitable adjustments, for any conditional generative approach where the target and the conditioning set must be processed jointly.

In order to ensure the usability and scalability of Carso across the widest range of input data and classifier models, we propose to perform such processing in a hierarchical and partially disjoint fashion between the input and the conditioning set. In principle, the encoding of \(\) and \(\) can be performed by two different and independent subnetworks, until some form of joint processing must occur. This allows to retain the overall architectural structure of the purifier, while having finer-grained control over the inductive biases  deemed the most suitable for the respective variables.

In the experimental phase of our work, we encode the two variables independently. The input is compressed by a multilayer convolutional neural network (CNN). The internal representation - which in our case is composed of differently sized multi-channel _images_ - is processed _layer by layer_ by independent multilayer CNNs (responsible for encoding local information), whose flattened outputs are finally concatenated and compressed by a fully-connected layer (modelling inter-layer correlations in the representation). The resulting compressed input and conditioning set are then further concatenated and jointly encoded by a fully-connected network (FCN).

In order to use the VAE decoder at inference time, the entire compression machinery for the conditioning set must be preserved after training, and used to encode the internal representations extracted. The equivalent input encoder may be discarded instead.

### Adversarially-balanced batches

Training the purifier in representation-conditional input reconstruction requires having access to adversarially-perturbed examples generated against the classifier, and to the corresponding clean data. Specifically, we use as input a mixture of _clean_ and adversarially _perturbed_ examples, and the clean input as the target.

Within each epoch, the _training set_ of interest is shuffled , and only a fixed fraction of each resulting batch is adversarially perturbed. Calling \(\) the maximum \(_{p}\) perturbation norm bound for the threat model against which the _classifier_ was adversarially pre-trained, the portion of perturbed examples is generated by an even split of Fgsm\({}_{}{{2}}}\), Pgd\({}_{}{{2}}}\), Fgsm\({}_{}\), and Pgd\({}_{}\) attacks.

Any smaller subset of attack types and strengths, or a detailedly unbalanced batch composition, always experimentally results in a worse performing purification model. More details justifying such choice are provided in Appendix C.

### Robust aggregation strategy

At inference time, many different input reconstructions are classified by the _classifier_, and the respective outputs concur to the settlement of a _robust prediction_.

Calling \(l_{i}^{}\) the output logit associated with class \(i\{1,,C\}\) in the prediction by the classifier on sample \(\{1,,N\}\), we adopt the following aggregation strategy:

\[P_{i}:=_{=1}^{N}e^{e^{l_{i}^{}}}\]

with \(P_{i}\) being the aggregated probability of membership in class \(i\), \(Z\) a normalisation constant such that \(_{i=1}^{C}P_{i}=1\), and \(e\) Euler's number.

Such choice produces a _robust prediction_ much harder to take over in the event that an adversary selectively targets a specific input reconstruction. A heuristic justification for this property is given in Appendix D.

## 5 Experimental assessment

Experimental evaluation of our method is carried out in terms of _robust_ and _clean_ image classification accuracy within three different scenarios (\(a\), \(b\) and \(c\)), determined by the specific classification task.

The _white-box_ threat model with a fixed \(_{}\) norm bound is assumed throughout, as it generally constitutes the most demanding setup for adversarial defences.

### Setup

DataThe Cifar-10  dataset is used in _scenario (a)_, the Cifar-100  dataset is used in _scenario (b)_, whereas the TinyImageNet-200  dataset is used in _scenario (c)_.

ArchitecturesA WideResNet-28-10 model is used as the _classifier_, adversarially pre-trained on the respective dataset - the only difference between scenarios being the number of output logits: \(10\) in _scenario (a)_, \(100\) in _scenario (b)_, and \(200\) in _scenario (c)_.

The purifier is composed of a conditional VAE, processing inputs and internal representations in a partially disjoint fashion, as explained in subsection 4.3. The input is compressed by a two-layer CNN; the internal representation is instead processed layerwise by independent CNNs (three-layered in _scenarios (a)_ and _(b)_, four-layered in _scenario (c)_) whose outputs are then concatenated and compressed by a fully-connected layer. A final two-layer FCN jointly encodes the compressed input and conditioning set, after the concatenation of the two. A six-layer deconvolutional network is used as the decoder.

More precise details on all architectures are given in Appendix E.

Outer minimisationIn _scenarios (a)_ and _(b)_, the _classifier_ is trained according to ; in _scenario (c)_, according to . _Classifiers_ were always acquired as pre-trained models, using publicly available weights provided by the respective authors.

The _purifier_ is trained on the _VAE_ loss, using _summed pixel-wise channel-wise_ binary cross-entropy as the reconstruction cost. Optimisation is performed by Radam+Lookahead with a learning rate schedule that presents a linear warm-up, a plateau phase, and a linear annealing . To promote the learning of meaningful reconstructions during the initial phases of training, the _KL divergence_ term in the VAE loss is suppressed for an initial number of epochs. Afterwards, it is linearly modulated up to its actual value, during a fixed number of epochs (\(\)_increase_) . The initial and final epochs of such modulation are reported in Table 14.

Additional scenario-specific details are provided in Appendix E.

Inner minimisation\(_{}=}{{255}}\) is set as the perturbation norm bound.

Adversarial examples against the _purifier_ are obtained, as explained in subsection 4.4, by Fgsm\({}_{}{{2}}}\), Pgd\({}_{}{{2}}}\), Fgsm\({}_{}\), and Pgd\({}_{}\), in a _class-untargeted_ fashion on the cross-entropy loss. In the case of Pgd, gradient ascent with a step size of \(=0.01\) is used.

The complete details and hyperparameters of the attacks are described in Appendix E.

EvaluationIn each scenario, we report the _clean_ and _robust_ test-set accuracy - the latter by means of AutoAttack - of the _classifier_ and the corresponding Carso architecture.

For the _classifier_ alone, the _standard_ version of AutoAttack (_AA_) is used: _i.e._, the worst-case accuracy on a mixture of AutoPgd on the cross-entropy loss  with \(100\) steps, AutoPgd on the _difference of logits ratio_ loss  with \(100\) steps, Fab with \(100\) steps, and the _black-box_ Square attack  with \(5000\) queries.

In the evaluation of the Carso architecture, the number of reconstructed samples per input is set to 8, the logits are aggregated as explained in subsection 4.5, and the output class is finally selected as the \(\) of the aggregation. Due to the stochastic nature of the _purifier_, robust accuracy is assessed by a version of AutoAttack suitable for stochastic defences (_randAA_) - composed of AutoPgd on the cross-entropy and _difference of logits ratio_ losses, across \(20\)_Expectation over Transformation_ (EoT)  iterations with \(100\) gradient ascent steps each.

Computational infrastructureAll experiments were performed on an _NVIDIA DGX A100_ system. Training in _scenarios (a)_ and _(c)_ was run on 8 _NVIDIA A100_ GPUs with 40 GB of dedicated memory each; in _scenario (b)_ 4 of such devices were used. Elapsed real training time for the purifier in all scenarios is reported in Table 1.

### Results and discussion

An analysis of the experimental results is provided in the subsection that follows, whereas their systematic exposition is given in Table 2.

Scenario (a)Comparing the robust accuracy of the _classifier_ model used in _scenario (a)_ with that resulting from the inclusion of the same model in the Carso architecture, we observe a \(+8.4\%\) increase. This is counterbalanced by a \(-5.6\%\) clean accuracy toll. The same version of Carso further provides a \(+5.03\) robustness increase _w.r.t._ the current best AT-trained model  that employs a \( 3\) larger RaWideResNet-70-16 model.

In addition, our method provides a remarkable \(+9.72\%\) increase in robust accuracy _w.r.t._ to the best adversarial purification approach , a diffusion-based purifier. However, the comparison is not as straightforward. In fact, the paper  reports a robust accuracy of \(78.12\%\) using AutoAttack on the gradients obtained via the adjoint method . As noted in , such evaluation (which uses the version of AutoAttack that is unsuitable for stochastic defences) leads to a large overestimation of the robustness of diffusive purifiers. As suggested in , the authors of  re-evaluate the robust accuracy according to a more suitable pipeline (Pgd+EoT, whose hyperparameters are shown in Table 12), obtaining a much lower robust accuracy of \(66.41\%\). Consequently, we repeat the same evaluation for Carso and compare the worst-case robustness amongst the two. In line with typical AT methods, and unlike diffusive purification, the robustness of Carso assessed by means of _randAA_ is still lower _w.r.t._ than achieved by Pgd+EoT.

Scenario (b)Moving to _scenario (b)_, Carso achieves a robust accuracy increase of \(+27.47\%\)_w.r.t._ the _classifier_ alone , balanced by a \(5.79\%\) decrease in clean accuracy. Our approach also improves upon the robust accuracy of the best AT-trained model  (WideResNet-70-16) by \(23.98\%\). In the absence of a reliable robustness evaluation by means of Pgd+EoT for the best purification-based method , we still obtain a \(+20.25\%\) increase in robust accuracy upon its (largely overestimated) AA result.

Scenario (c)In _scenario (c)_, Carso improves upon the _classifier_ alone  (which is also the best AT-based approach for TinyImageNet-200) by \(+22.26\%\). A significant clean accuracy toll is

   _Scenario_ & _(a)_ & _(b)_ & _(c)_ \\  _Etapsed real training time_ & \(159\) & \(138\) & \(213\) \\   

Table 1: Elapsed real running time for training the _purifier_ in the different scenarios considered.

    &  &  &  &  &  &  &  Best P/AA \\ (Pgd+EoT) \\  } \\  (a) & & _Cifar-10_ & _0.9216_ & _0.8686_ & 0.6773 &  **0.7613** \\ (0.7689) \\  & 0.7107 & 
 0.7812 \\ (0.6641) \\  \\  (b) & & _Cifar-100_ & _0.7385_ & _0.6806_ & 0.3918 & **0.6665** & 0.4267 & 0.4609 \\  (c) & & _TinyImageNet-200_ & _0.6519_ & _0.5632_ & 0.3130 & **0.5356** & 0.3130 & \\   

Table 2: Clean (results in _italic_) and adversarial (results in upright) accuracy for the different models and datasets used in the respective scenarios. The following abbreviations are used: Seen: scenario considered; AT/Cl: clean accuracy for the adversarially-pretrained model used as the _classifier_, when considered alone; C/Cl: clean accuracy for the Carso architecture; AT/AA: robust accuracy (by the means of AutoAttack) for the adversarially-pretrained model used as the _classifier_, when considered alone; C/randAA: robust accuracy for the Carso architecture, when attacked _end-to-end_ by AutoAttack for randomised defences; Best AT/AA: best robust accuracy result for the respective dataset (by the means of AutoAttack), obtained by adversarial training alone (any model); Best P/AA: best robust accuracy result for the respective dataset (by the means of AutoAttack), obtained by adversarial purification (any model). Robust accuracies in round brackets are obtained using the Pgd+EoT  pipeline, developed for diffusion-based purifiers. The best clean and robust accuracies per dataset are shown in **bold**. The clean accuracies for the models referred to in the Best columns are shown in Table 15 (in Appendix F).

imposed by the relative complexity of the dataset, _i.e._\(-8.87\%\). In this setting, we lack any additional purification-based methods.

Assessing the impact of _gradient obfuscation_ Although the architecture of Carso is algorithmically differentiable _end-to-end_ - and the integrated diagnostics of the _randAA_ routines raised no warnings during the assessment - we additionally guard against the eventual gradient obfuscation  induced by our method by repeating the evaluation at \(_{}=0.95\), verifying that the resulting robust accuracy stays below random chance . Results are shown in Table 3.

### Limitations and open problems

In line with recent research aiming at the development of robust defences against multiple perturbations [20; 35], our method determines a decrease in _clean_ accuracy _w.r.t._ the original model on which it is built upon - especially in _scenario (c)_ as the complexity of the dataset increases. This phenomenon is partly dependent on the choice of a VAE as the generative purification model, a requirement for the fairest evaluation possible in terms of robustness.

Yet, the issue remains open: is it possible to devise a Carso-like architecture capable of the same - if not better - robust behaviour, which is also competitively accurate on clean inputs? Potential avenues for future research may involve the development of Carso-like architectures in which representation-conditional data generation is obtained by means of diffusion or score-based models. Alternatively, incremental developments aimed at improving the cross-talk between the purifier and the final classifier may be pursued.

Lastly, the scalability of Carso could be strongly improved by determining whether the internal representation used in conditional data generation may be restricted to a smaller subset of layers, while still maintaining the general robustness of the method.

## 6 Conclusion

In this work, we presented a novel adversarial defence mechanism tightly integrating input purification, and classification by an adversarially-trained model - in the form of representation-conditional data purification. Our method is able to improve upon the current _state-of-the-art_ in Cifar-10, Cifar-100, and TinyImageNet\(_{}\) robust classification, _w.r.t._ both _adversarial training_ and _purification_ approaches alone.

Such results suggest a new synergistic strategy to achieve adversarial robustness in visual tasks and motivate future research on the application of the same design principles to different models and types of data.

   _Scenario_ & _(a)_ & _(b)_ & _(c)_ \\  \(_{}=0.95\) _acc._ & 0.047 & 0.010 & 0.0 \\   

Table 3: Robust classification accuracy against AutoAttack, for \(_{}=0.95\), as a way to assess the (lack of) impact of _gradient obfuscation_ on robust accuracy evaluation.