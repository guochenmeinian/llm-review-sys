# Opponent Modeling based on Subgoal Inference

Xiaopeng Yu  Jiechuan Jiang  Zongqing Lu

School of Computer Science, Peking University

Correspondence to EZ zongqing.lu@pku.edu.cn

###### Abstract

When an agent is in a multi-agent environment, it may face previously unseen opponents, and it is a challenge to cooperate with other agents to accomplish the task together or to maximize its own rewards. Most opponent modeling methods deal with the non-stationarity caused by unknown opponent policies via predicting the opponent's actions. However, focusing on the opponent's action is shortsighted, which also constrains the adaptability to unknown opponents in complex tasks. In this paper, we propose _opponent modeling based on subgoal inference_, which infers the opponent's subgoals through historical trajectories. As subgoals are likely to be shared by different opponent policies, predicting subgoals can yield better generalization to unknown opponents. Additionally, we design two subgoal selection modes for cooperative games and general-sum games respectively. Empirically, we show that our method achieves more effective adaptation than existing methods in a variety of tasks.

## 1 Introduction

Autonomous agents are systems capable of making decisions and acting independently in their environment, often operating without direct human intervention . These agents can either cooperate with or compete against each other, depending on the context. In cooperative scenarios, many multi-agent reinforcement learning (MARL) methods  aim to bridge the information gap between agents  by training agents in a centralized manner, called centralized training with decentralized execution, enabling agents to work together seamlessly to accomplish cooperative tasks. Alternatively, fully decentralized methods seek to break free from the constraints of centralized training, allowing agents to reach collaboration in a simpler and decentralized manner. In competitive scenarios, NFSP , PSRO , and DeepNash  employ self-play to train agents for equilibrium strategies, allowing agents to adapt and improve their policy. By considering how the agent affects the expected learning progress of other agents, LOLA  and COLA  apply opponent shaping to this setting. Overall, these methods focus on training agents in a way that accounts for their interactions, resulting in a set of policies that enable effective collaboration or competition within a group of agents.

While the above methods emphasizes the collective behavior of agents, it is also crucial to consider the role of individual agents, particularly self-interested agents, in these multi-agent environments. A self-interested agent  operates with the primary goal of maximizing its own benefits,

Figure 1: Infer the goal of otherseven when interacting with other agents. When the objectives of a self-interested agent align with those of the team, this scenario falls under ad-hoc teamwork [21; 8; 39]; however, in more general cases, these interactions are framed as noncooperative games [34; 23; 45]. A key technique for self-interested agents in such settings is opponent modeling[24; 3], which enables them to analyze and predict the actions, goals, and beliefs of other agents. By modeling the intentions and policies of other agents, the training process of the agent might be stabilized . Many studies rely on predicting the actions [14; 16; 13; 25; 26], goals [32; 31], and returns  of opponents during training. Then, the autonomous agent can adapt to different or unseen opponents by using the predictions or representations that are produced by the relevant modules.

Although a lot of the existing methods concentrate on modeling the opponent's actions, we argue that such an approach is short-sighted, pedantical, and highly complex. Generally, modeling an opponent's actions just predicts what it will do at the next step. Intuitively, it is more beneficial for the agent to make decisions if it knows the situation of the opponent several steps ahead. Predicting the actions over a few steps has high uncertainty. For example, to reach the goal point of \((2,2)\), an opponent moves from \((0,0)\) following the action sequence \(<,,,>\) by four steps (Cartesian coordinates). But, there are also 5 other action sequences, _i.e._, \(<,,,>,<,,, >,<,,>>,<,,, >>,<,,>>\), that can lead to the same goal. Obviously, the complexity and uncertainty of predicting the action sequence are much higher than the goal itself. Other methods that claim to predict the opponent's goal [31; 32], but without explicitly making a connection to the opponent's goal or just predicting the goal at the next step, are essentially as shortsighted as modeling actions.

Inspired by the fact that humans can predict the opponent's goal by observing the opponent's actions for several steps as illustrated in Figure 1, in this paper, we propose _Opponent Modeling based on subGoals inference_ (**OMG**), which uses variational inference to predict the opponent's future subgoals from historical trajectories. The trajectory of an opponent's policy consists of a set of subgoals, and the trajectories of different policies may contain the same subgoals. This combinatorial property of the subgoals facilitates the generalization of the agent to unseen opponents' policies. Moreover, we design two manners for selecting subgoals, which are applied to cooperative games and general sum games, respectively. Empirically, OMG outperforms existing opponent modeling methods in a variety of multi-agent environments, demonstrating the superiority of inferring subgoals over predicting actions.

## 2 Related Work

**Opponent modeling.** Opponent modeling plays a crucial role in enhancing the robustness and stability of reinforcement learning . Given the presence of diverse opponent policies in multi-agent environments, the autonomous agent faces a significant challenge in learning resilient policies. When an agent perceives an opponent as part of the environment, the resulting environment becomes inherently unstable and intricate. To address this challenge, one straightforward method involves equipping the agent with the ability to incorporate information about its opponent, including aspects like the opponent's behavior, goals, and beliefs , _i.e.,_ opponent modeling. It gives the agent a deeper insight and prediction ability about the opponent's policy. Thus, the autonomous agent views the environment as less unstable and can simply use single-agent reinforcement learning methods.

A common approach to modeling the policy of an opponent is predicting the opponent's actions. DRON  and DPIQN  extend DQN  by adding another network that estimates the opponents' actions from the observations. The DQN uses the hidden layer of this network to improve its policy. Variational auto-encoders can also be used to model the opponent's policy , which results in probabilistic representations instead of fixed vectors. PR2  and TP-MCTS  combine the idea of recursive reasoning, nested form as "the agent believes [that the opponent believes (that the agent believes...)]", based on modeling the action of the opponent. Some works focus on modeling beliefs.  combined the sequential and hierarchical variational auto-encoders to construct a belief inference model using meta-learning, for belief inference.  introduced landmarks into the behavior model and improved the model by the action sequence of the opponents, so as to recognize and compare the opponent's intention.

Another key aspect of opponent modeling is to infer the opponent's goal.  formulated the goal recognition as a Markov decision process (MDP) and calculated the posterior probability of the goal by Bayes' rule based on a prior goal library. ToMnet  aims to give the agent a human-like Theory of Mind. It uses three networks to infer the agent's goal and action from previous and present information. SOM  implements the Theory of Mind with a goal library from a different perspective. SOM uses its own policy, the opponent's observation, and the opponent's action to work backward to learn the opponent's goal distribution by gradient ascent. These methods either require a prior goal library or infer implicit "goals" that are not supervised by ground truth goals.

In some scenarios, opponents may continuously learn during interaction. Meta-MAPG  combines Meta-PG  and LOLA , and focuses on the problem of the non-stationary environment caused by the continuous learning of opponents. MBOM  simultaneously targets a variety of adversaries, fixed policy, or continuous learning, by modeling the possible policies that an opponent may form, combined with Bayesian inference to generate an opponent's imagined policy. GSCU  chooses online between a real-time greedy strategy and a fixed conservative strategy through Bayesian belief in competitive environments. Unlike these methods, in this paper, we consider the most common setting where opponents have unseen, diverse, but fixed policies during test.

**Goal-conditioned RL.** Goal-conditioned reinforcement learning is an extension of the single-agent algorithm. Most works focus on learning a goal-conditioned policy, where the goals are usually predefined [30; 52]. Some works consider acquiring subgoals automatically to accelerate learning.  proposed a method that uses expert trajectories to generate subgoals, while  proposed to incorporate imaginary subgoals into policy learning to facilitate learning complex tasks, where subgoals are measured by value functions. Unlike existing goal-conditioned RL methods, we aim to infer the subgoal of the opponent and condition the agent policy on the inferred subgoal.

## 3 Preliminaries

In general, we consider an \(n\)-agent stochastic game \(=(,^{1},,^{n},, ^{1},,^{n},)\), where \(\) is the state space, \(^{i}\) is the action space of agent \(i[1,,n]\), \(=_{i=1}^{n}^{i}\) is the joint action space of agents, \(:\) is a transition function, \(^{i}:\) is the reward function of agent \(i\), and \([0,1)\) is the discount factor. The policy of agent \(i\) is \(^{i}\), and the joint policy of other agents is \(^{-i}(a^{-i}|s)=_{j i}^{j}(a^{j}|s)\), where \(a^{-i}\) is the joint action except agent

Figure 2: Diagram of OMG. During the interaction phase, OMG infers the subgoal from the historical trajectory. The CVAE \(\) acts as an inference model, deducing the opponent subgoal denoted as \(\). The inferred subgoal serves as input for the policy model \(Q(s,,a)\). In the update phase, OMG examines the entire trajectory in a hindsight manner to select subgoals \(\) as priors for training the inference model. The subgoal selector employs a value-based heuristic to choose a state from the next few steps and then encodes it into a subgoal using the pre-trained VAE \(\).

\(i\). All agents interact with the environment simultaneously without communication. The historical trajectory is available, _i.e.,_ for agent \(i\) at timestep \(t\), \(_{t}=\{s_{0},a_{0}^{i},a_{0}^{-i},,s_{t-1},a_{t-1}^{i},a_{t-1}^{-i}\}\) is observable. The goal of the agent \(i\) is to maximize its expected cumulative discount rewards:

\[}_{s_{t+1}( |s_{t},a_{t}^{i},a_{t}^{-i}),\\ a^{i}(|s_{t}),a_{t}^{-i}^{-i}(|s_{t})} [_{t=0}^{}^{t}^{i}(s_{t},a_{t}^{i},a_{t}^{-i})].\] (1)

For convenience, the learning agent treats all other agents as a joint opponent with the joint action \(a^{-i}^{-i}(|s)\) and reward \(r^{-i}\). The action and reward of the learning agent are respectively denoted as \(a(|s)\) and \(r\) for notation simplicity.

If an agent treats other agents as part of the environment and ignores the non-stationarity posed by the change of other agents' policies as independent Q-learning [43; 44]. Its Q-function \(\) is updated by:

\[(s_{t},a_{t})=_{(s_{t+1}|s_{t},a^{-i},a)}[r+ _{a}(s_{t+1},a)].\] (2)

Opponent modeling typically predicts the actions of other agents to address the non-stationary problem. The opponent model uses historical trajectory as input to predict \(^{-i}(|)\), where \(^{-i}\) is the estimate of \(a^{-i}\). Then, its Q-function is updated as:

\[(s_{t},_{t}^{-i},a_{t})=_{(s_{t+1}|s _{t},a^{-i},a)}[r+_{a}(s_{t+1},_{t+1}^{-i},a)].\] (3)

Note that we cast our discussion here to Q-learning. All can be similarly applied to other RL methods, such as PPO .

## 4 Method

In this section, we present our method, opponent modeling based on subgoals inference (OMG). First, we discuss learning policies with the opponent's subgoals, compared to learning based on the opponent's actions. Then, we introduce our opponent model that infers the opponent's subgoals using a value-based heuristic.

### Policy Learning with Opponent's Subgoals

In Equation (3), the traditional opponent modeling with the opponent's actions is introduced. Here, we introduce policy learning with the opponent's subgoals.

_The opponent's subgoals offer a more structured representation compared to individual actions._ Subgoals represent feature embeddings of future states that the opponent aims to achieve based on its policy. Although diverse action sequences can lead to the same state, focusing on subgoals provides a higher-level understanding of the opponent's long-term intentions. Instead of gaining new information, subgoal modeling reinterprets observed data to emphasize long-term objectives, reducing variability and improving learning efficiency . By concentrating on the opponent's desired states rather than individual actions, the agent can achieve more stable and effective policy learning.

The opponent's subgoal distribution is determined by the opponent's action sequence, _i.e.,_ the opponent's policy, but the subgoal space is still the representation of the state space. Here we decouple the subgoal from the opponent's policy and just consider decision-making problems conditioned on the opponent's subgoal. Formally, we transform the original stochastic game \(\) into a state-augmented MDP, defined by \(_{}=(,,^{i},,^{i},)\), where \(\) is the subgoal space. \(\) is a representation of future states the opponent may go, \(||||\).

The state-augmented MDP's state space \(\) extends to the MDP with state-subgoal pairs \((,)\). Therefore, the agent's Q-function based on the opponent's subgoal is updated as:

\[(s_{t},g_{t},a_{t})=_{(s_{t+1}|s_{t},a^{-i},a) }[r+_{a}(s_{t+1},g_{t},a)].\] (4)

Here the pair \((s_{t+1},g_{t})\) is used instead of \((s_{t+1},g_{t+1})\), as we assume that the next state of \((s_{t},g_{t})\) follows the same goal. In the framework of OMG, \(g_{t}\) and \(g_{t+1}\) will reach the same state at the end of the episode.

_Q-values with opponent's subgoals are just as effective as with opponent's actions_. We carry out an experiment in an \(11 11\) gridworld with two agents, as detailed in Figure 3. After convergence, the Q-value increases as the agent gets closer to the rewarding grid, indicating a meaningful Q-value with the opponent's subgoal, as shown in Figure 3(b).

_Effective opponent's subgoals enhance policy learning_. The Q-value using the opponent's action learns slower than the Q-value with the opponent's subgoal in Figure 3(a), resulting from the tuple \((s,a^{-i},a)\) is more numerous than \((s,g,a)\) in the Q-table. When there are fewer \((s,g,a)\) than \((s,a^{-i},a)\), the method using \((s,g,a)\) naturally holds the advantage of faster learning than the method of \((s,a^{-i},a)\). The quantity of \((s,g,a)\) is contingent upon the goal selection, and we present an analysis of the quantitative relationship between pair \((s,g)\) and \((s,a^{-i})\), see Appendix A. In short, the number of \((s,g)\) is significantly smaller than that of \((s,a^{-i})\) in our method. Next, we explain how the opponent model secures these benefits.

### Opponent Modeling Based on Subgoal Inference

Our opponent modeling consists of two components: _subgoal inference model_ and _subgoal selector_. The subgoal inference model employs the historical trajectory to predict the opponent's subgoal, serving as input for the policy during the interaction phase. The subgoal selector scrutinizes the entire historical trajectory using a value-based heuristic to choose the appropriate subgoal for training the inference model during the update phase.

**Subgoal inference model.** The subgoal \(g\) represents a feature embedding of a future state. Specifically, for a trajectory \(\{s_{0},a_{0},a_{0}^{-i},,s_{t},a_{t},a_{t}^{-i},,s_{T}\}\), the state corresponding to subgoal \(g_{t}\) at \(s_{t}\) is one of future states \(_{t}=\{s_{t+1},s_{t+2},,s_{T}\}\), denoted as \(s_{t}^{g}\) and determined by the subgoal selector.

The objective of the subgoal inference model is to infer \(g_{t}\) from the historical trajectory \(_{t}=\{s_{0},a_{0},a_{0}^{-i},,s_{t-1},a_{t-1},a_{t-1}^{-i}\}\). This aligns with the intuitive hypothesis that the opponent's intention can often be inferred after just a few initial actions.

Here, we introduce variational inference and employ a conditional variational auto-encoder (CVAE) as the subgoal inference model. In this model, we represent the subgoal posterior probability as \(q_{}(_{t}|_{t},s_{t})\) and the likelihood estimate as \(p_{}(s_{t}|_{t},_{t})\) with \(\) and \(\) respectively denoting the network parameters. The subgoal prior model, denoted as \(p_{}\), is a pre-trained variational autoencoder (VAE) using the states previously collected in the environment, and produces the subgoal prior \(p_{}(_{t}|s_{t}^{g})\) given the subgoal state \(s_{t}^{g}\) chosen by the subgoal selector.

Further details about the network architecture are provided in Figure 2. The optimization objective of the subgoal inference model is:

\[<,>=*{arg\,max}_{,}_{q _{}(_{t}|_{t},s_{t})} p_{}(s_{t}|_{t}, _{t})-q_{}(_{t}|_{t},s_{t}) \|p_{}(_{t}|s_{t}^{g}).\] (5)

Figure 3: Learned Q-values using tabular Q-learning in an \(11 11\) gridworld. The agent and the opponent start from the \(S_{1}\) and \(S_{2}\), respectively. The two rewarding grids are \(D_{1}\) and \(D_{2}\), and the reward will only be given to the agent who arrives first. The opponent executes one of policies \(_{1}^{-i}\) and \(_{2}^{-i}\), which target \(D_{1}\) and \(D_{2}\), respectively. The \(g\) and \(a^{-i}\) are obtained from an oracle.

where the term \(p_{}(|s^{g})\) in the KL divergence accounts for the prior distribution and is pre-trained. The purpose of including the KL divergence term is to prevent collapse of the inference model.

**Subgoal selector.** The primary objective of the subgoal selector is to choose the appropriate future state of the subgoal state \(s_{t}^{g}\) from \(_{t}\) as input to the prior model. The choice of the subgoal state plays a significant role in shaping the agent's behavior and leaning towards either optimism or conservatism. This is especially critical when dealing with cooperative games and general-sum games, where the dynamics of interactions are complex and multifaceted. In these contexts, we provide two distinct manners for the subgoal selection:

\[_{t} =*{arg\,max}_{s_{i}_{t}^{H}}_{g p_{}(|s_{i})}V(s_{t},g)\] (6) \[_{t} =*{arg\,min}_{s_{i}_{t}^{H}}_{g p_{}(|s_{i})}V(s_{t},g),\] (7)

where \(V(s,g)=_{a}Q(s,g,a)\), \(_{t}^{H}\) is the set of future states \(\{s_{t+1},,s_{t+H}\}\). As discussed in Section 4.1, the quantity of \((s,g)\) pairs is crucial. Selecting candidate subgoal states is pivotal in this regard. Thus, we use states within the next \(H\) timesteps instead of all future states. The choice of \(H\) gives a tradeoff between the agent's generalization to diverse opponents induced by the fact that the subgoals of different trajectory fragments have combinatorial properties and the learning difficulty incurred by the increased opponent subgoals.

As indicated in Equation (6), we pinpoint the subgoal within an \(H\)-horizon that maximizes the V-value. The agent incorporates this to optimize the Q-function, thus adopting an optimistic strategy akin to the maximax strategy , which applies to cooperative games. Conversely, if we choose the subgoal as in Equation (7), it corresponds to the subgoal yielding the lowest value. The agent then employs this for learning Q-function, leading to a conservative strategy similar to the minimax strategy, which is commonly used in general-sum games.

The subgoal selector and the subgoal inference model as a whole constitute our opponent modeling module. During the interaction phase, the subgoal inference model is used to get the inferred subgoal \(\), which is combined with the state as the input to the Q-network. During the update phase, the prior subgoal \(\) generated by the subgoal selector is provided to the inference model for training. The subgoal inference model is unstable at the beginning, which may disturb the updating of the Q-network. Therefore, we use the following combination of the prior subgoal \(\) and the inferred subgoal \(\) as the input of Q-network,

\[g_{t}=_{t}(>)+_{t}( ), U,\] (8)

where \(\) is a hyperparameter that decreases to zero over training. We will further empirically study this in Section 5.4.

For completeness, the full procedure of OMG is given in Algorithm 1.

## 5 Experiments

First, we evaluate OMG's training performance in two environments (discrete and continuous state spaces) and then test its generalization to opponents with unseen policies in a more complex environment. In all the experiments, the baselines have the same neural network architectures as OMG. All the methods are trained for five runs with different random seeds, and results are presented using mean and standard deviation. More details about experimental settings and hyperparameters are available in Appendix B. To ensure reproducibility, we include the code in the supplementary material and will make it open-source upon acceptance.

We experiment in the following three multi-agent environments. Foraging [2; 4] is an \(8 8\) gridworld where the agent aims to collect foods. Predator-Prey  is a three-against-one scenario with continuous space where the agent collaborates with predators to capture prey. SMAC  is a high-dimensional environment for collaborative multi-agent reinforcement learning based on StarCraft II, where the agent cooperates with a set of opponents with unknown policies to accomplish tasks.

### Baselines

In the experiments, we implement two variants of OMG, OMG-optimistic and OMG-conservative, based on the subgoal selection manners in Equation (6) and Equation (7), respectively. OMG compared with the following methods:

* Naive OM  uses observation to directly model the opponent's policy, which assists the agent in decision-making by predicting the opponent's actions.
* LIAM  uses the observations and actions of the opponent with an encoder-decoder architecture, and the model learns to extract representations about the opponent, conditioned only on the local observations of the controlled agent.
* D3QN & PPO & IQL [46; 36; 43] are classical RL algorithms without opponent modeling.

We use D3QN, PPO, and IQL as the backbone algorithms in Foraging, Predator-Prey, and SMAC, respectively, to reproduce the performance of baselines. The versions of OMG that are based on D3QN and IQL incorporate "dueling" and "double" tricks over Algorithm 1. For OMG based on PPO, please refer to Appendix F for details.

### Performance of Training

We evaluate the performance of OMG on Foraging and Predator-Prey, and the results are shown in Figure 4(a) and Figure 4(b), respectively. In the foraging environment, our method attains similar scores to the baseline methods, and both the agent and the opponent achieve comparable scores.

Figure 4: (a) Performance in Foraging. The red bar shows the total score obtained by the agent. The blue bar illustrates the number of steps in each episode. The results show that OMG can converge to the same score as the baselines but end the episode in fewer steps because it predicts the opponent’s goal. (b) Performance in Predator-Prey. The results show the score obtained by the agent as a predator with two other uncontrolled predators, and OMG outperforms the baselines.

OMG has a shorter episode length compared to other methods because OMG can predict the subgoal that the opponent is heading to and thus avoid wasting steps in the same direction. In addition, the results show that OMG-conservative is more suitable than OMG-optimistic in this scenario since this is a general-sum game. The baselines based on action modeling, LIAM and Naive OM, demonstrate comparable performance, whereas D3QN without opponent modeling, exhibits subpar results. In the predator-prey environment, the agent acts as the predator and collaborates with the other two uncontrolled predators to catch the prey. The results in Figure 4(b) show that OMG obviously outperforms action modeling methods, which demonstrates that OMG can also work efficiently in continuous state space. PPO without opponent modeling can hardly improve performance in training due to the non-stationarity caused by opponents. OMG-optimistic slightly performs better than OMG-conservative because OMG-optimistic is suitable for the cooperative game.

### Generalization to Unknown Opponents

We evaluate the generalization of OMG in a more complex multi-agent environment, SMAC, which enables the opponents to exhibit more diverse policies. The experimental results of _8m_, _3s_vs_5z_ and _2c_vs_64zg_ are shown in Figure 5. Without opponent modeling, IQL struggles to adapt to various unknown opponents, resulting in poor performance, especially when the opponents are _non-homologue_. This underscores the effectiveness of opponent modeling in autonomous agent tasks. LIAM and Naive OM, the action modeling methods, contribute to the team's improved win rate to some extent. The mediocre performance of OMG-conservative is attributed to its overly cautious subgoal selection. OMG-conservative is on par with IQL, which is consistent with the "conservative". OMG-optimistic surpasses the baseline methods, indicating that OMG-optimistic can generalize well to unknown collaborators through positive subgoal selection.

### Ablation Study

The ablation study is conducted for the network structure of the inference model, subgoal selection, and hyperparameter horizon \(H\). OMG uses CVAE as the inference model. Here, we instead employ supervised learning to train an inference model using the subgoal selector's output \(_{t}\), obtained from either Equation (6) or Equation (7). This model is referred to as OMG-supervised. The results in the foraging environment are presented in Figure 6(a). The results indicate that OMG-optimistic and OMG-conservative outperform their counterparts, which is attributed to the enhanced adaptability of variational inference to the uncertainty in the opponent's policy. Dealing with multiple opponents employing distinct policies poses a challenge for supervised learning, as establishing a mapping relationship between historical trajectories and subgoals becomes intricate.

Figure 5: Test performance of cooperation with unseen opponents in _8m_ (a), _3s_vs_5z_ (b) and _2c_vs_64zg_ (c) maps of SMAC. The X-axis represents the opponent’s policies, and “homologue” refers to the policy learned by the same algorithm, while “non-homologue” represents different ones; e.g. 7 homologue refers to 7 opponents from 8 agents trained by the same algorithm (QMIX, VDN or IQL) on the 8m, and 7 non-homologue involves 7 opponents from different runs of those algorithms. The results show that OMG-optimistic outperforms all baselines. The results are averaged over collaborating with 30 opponents of different policies, with 95% confidence intervals.

In the OMG, the subgoal is selected by choosing the state within the future \(H\) steps that either maximizes or minimizes the value function \(V(s,g)\). To explore the impact of different subgoal selection strategies, we introduced three alternatives: random selection within the \(H\) steps (OMG-random), selecting the first step as the subgoal (OMG-1s), and selecting the third step as the subgoal (OMG-3s). The results, presented in Figure 6(b), suggest that the choice of subgoal selection strategy significantly affects performance, with OMG's strategy leading to more effective training compared to the alternatives. We also observe that the subgoal often remains constant over consecutive time steps for OMG-supervised. Further details can be found in Appendix D.

We further investigate our design choice on the subgoal selection for the policy. During the policy update, Equation (8) (_i.e._, \(g\)) is utilized. As \(p_{}\) is pre-trained and fixed during the update phase, \(\) remains stable. On the other hand, \(\), which represents the inferred subgoal when executing the policy, also stabilizes as the training steps increase. Thus, we choose a gradual transition of \(g\) from \(\) to \(\), which should help avoid instability during the training of the subgoal inference model. Here we perform the experiments in the foraging environment with different subgoal inputs for the policy, _i.e._, \(g,,\). As shown in Figure 7(a) and Figure 7(b), OMG with \(g\) indeed shows faster and better convergence.

The parameter \(H\) denotes the horizon of the subgoal selector. The ablation experiment results are shown in Figure 7(c) and Figure 7(d). It is observed that an appropriate horizon value is neither excessively high nor excessively low. When \(H=1\), it is essentially equivalent to combining with QSS  and opponent modeling, which can be interpreted as another way of action modeling. However, if \(H\) is set too large, such as \(H=10\), the agent may skip important states in the trajectory, leading to a degradation in performance. Therefore, selecting an appropriate value for \(H\) is crucial in achieving satisfactory results.

### Inferred Subgoal Analysis

We analyze the predictive performance of the opponent model. In Figure 8(a), we plot the ratio of that an opponent's future trajectory passes through the opponent's subgoal state, termed subgoal hit ratio. The subgoal state is reconstructed by the inferred subgoal \(\) using the decoder of the subgoal prior model. The subgoal hit rate gradually improves during training, which indicates that

Figure 6: Ablation study in Foraging. In (a), methods on the X-axis labeled with “supv” indicate that the inference model uses an MLP instead of a CVAE. In (b) OMG-random, OMG-1s, and OMG-3s represent subgoals selected from the opponent’s future states: randomly, at the next step, and at the third step, respectively.

Figure 7: Ablation study of OMG in Foraging. (a) and (b) compares OMGs with different subgoal inputs for policy learning. (c) and (d) show ablation study for the hyperparameter horizon \(H\).

the subgoal-based opponent modeling is able to predict the future state of the opponent. OMG tends to predict the opponent's future state several steps ahead as the subgoal, rather than focusing solely on the next step. This kind of prediction requires validation over multiple steps, and the agent policy conditioned on the predicted subgoal may also influence the behavior of the opponent. These make it challenging to verify the predicted subgoal. Consequently, the overall hit ratio remains at a moderate level at the end of training. There is a small gap between the subgoal hit rates of OMG-conservative and OMG-optimistic, which leads to a longer episode length for OMG-optimistic than OMG-conservative, as illustrated in Figure 8(b). The root cause lies in the differences in the subgoal selection between OMG-conservative and OMG-optimistic. More details can be found in Appendix E.

## 6 Conclusion and Limitation

In this paper, we introduce OMG, a novel method for opponent modeling based on subgoal inference. OMG is a simple and efficient opponent modeling method and can be combined with various RL algorithms. Unlike most opponent modeling methods, which primarily focus on predicting the opponent's actions, OMG focuses on modeling the opponent's subgoals. Specifically, it leverages the value function of the policy to guide the selection of subgoals, which yields two variants of OMG for cooperative and general-sum games, respectively. Empirical results demonstrate the remarkable performance achieved by OMG, as compared to baselines based on action modeling, and that OMG exhibits better generalization when cooperating with opponents with unknown policies. We analyze the subgoals obtained by the inference model, and the results show they closely correlate with the opponent's trajectory. The limitation of OMG is it cannot handle open multi-agent systems where agents may enter and leave during the interaction. This is left for future work.

#### Acknowledgments

This work was supported by NSFC under Grant 62450001 and 62476008. The authors would like to thank the anonymous reviewers for their valuable comments and advice.