# DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions

Haochen Wang\({}^{1,3}\)  Junsong Fan\({}^{1,4}\)  Yuxi Wang\({}^{1,4}\)  Kaiyou Song\({}^{2}\)

**Tong Wang\({}^{2}\)  Zhaoxiang Zhang\({}^{1,3,4}\)**

\({}^{1}\)Center for Research on Intelligent Perception and Computing (CRIPAC),

State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS),

Institute of Automation, Chinese Academy of Sciences (CASIA)

\({}^{2}\)Megvii Technology \({}^{3}\)University of Chinese Academy of Sciences (UCAS)

\({}^{4}\)Centre for Artificial Intelligence and Robotics, HKISI_CAS

{wanghaochen2022, junsong.fan, zhaoxiang.zhang}@ia.ac.cn

yuxiwang93@gmail.com {songkaiyou, wangtong}@megvii.com

Corresponding author.

###### Abstract

As it is empirically observed that Vision Transformers (ViTs) are quite insensitive to the order of input tokens, the need for an appropriate self-supervised pretext task that enhances the location awareness of ViTs is becoming evident. To address this, we present DropPos, a novel pretext task designed to _reconstruct **Dropped Positions**_. The formulation of DropPos is simple: we first drop a large random subset of positional embeddings and then the model _classifies the actual position_ for each non-overlapping patch among all possible positions _solely_ based on their visual appearance. To avoid trivial solutions, we increase the difficulty of this task by keeping only a _subset_ of patches visible. Additionally, considering there may be different patches with similar visual appearances, we propose position smoothing and attentive reconstruction strategies to relax this classification problem, since it is _not_ necessary to reconstruct their _exact_ positions in these cases. Empirical evaluations of DropPos show strong capabilities. DropPos outperforms supervised pre-training and achieves competitive results compared with state-of-the-art self-supervised alternatives on a wide range of downstream benchmarks. This suggests that explicitly encouraging spatial reasoning abilities, as DropPos does, indeed contributes to the improved location awareness of ViTs. The code is publicly available at https://github.com/Haochen-Wang409/DropPos.

## 1 Introduction

Learning extensible visual representations without any human annotations, known as self-supervised learning (SSL), has become a research hotspot in the field of computer vision  since it enables efficient transfer to various downstream benchmarks. To achieve this goal, researchers carefully design visual pretext tasks to produce appropriate supervision using _only_ images . Two popular approaches among these are contrastive learning (CL)  and masked image modeling (MIM) , both of which have demonstrated promising scaling behavior of vision models, particularly Vision Transformers (ViTs) . Despite the success of these methods, ViTs are found to be relatively _insensitive_ to the order of input tokens , leading to the hypothesis that they tend to model the relationship between a set of _unordered_ input tokens. Therefore, a natural question arises: beyond the current CL and MIM paradigms, _whether a pretext task that explicitly enhances the positional awareness of ViTs can further improve their representation learning abilities?_To answer this question, we begin by revisiting the forward procedure of ViTs. A sequence of positional embeddings (PEs)  is added to patch embeddings to preserve position information. Intuitively, simply _discarding_ these PEs and requesting the model to reconstruct the position for each patch naturally becomes a qualified location-aware pretext task. By forcing the model _only_ to utilize visual appearance for position reconstruction, the model is supposed to learn the shape relationships and layouts of image contents, thereby improving the ability to encode spatial visual content. However, despite the simplicity and intuitiveness of this idea, position-related methods such as  have fallen far behind. We identify several difficulties in designing this type of paradigm:

_(i)_ Discarding all PEs brings _discrepancies between pre-training and fine-tuning_ because the model has _never_ been exposed to any PEs during pre-training. _(ii)_ Strengths of ViTs in modeling long-range dependencies may cause them to solve the task superficially, leading to trivial solutions that _fail to learn highly semantic representations by solving this simple task. _(iii)_ Patches with similar visual appearances may result in confusing reconstruction targets, making it crucial to _decide which position to reconstruct precisely._

Driven by this analysis, we present a novel, straightforward, and highly effective pretext task for self-supervised visual representation learning: _reconstructing Dropped Positions_ (DropPos). DropPos tackles the above issues systematically: _(i)_ To address discrepancies, simply _dropping_ a large subset of PEs instead of discarding all of them works effectively. _(ii)_ To avoid trivial solutions, we only keep a _subset_ of patches visible during pre-training, forcing ViTs to reconstruct the position of each visible patch with only _partial_ inputs. _(iii)_ To prevent ViTs from being overwhelmed by this particular task, we propose to relax this problem using position smoothing and attentive reconstruction strategies.

DropPos can be easily formulated into a simple patch-wise classification problem. Here is how it works: First, input patches are randomly masked and a _large_ subset of PEs corresponding to these visible patches is dropped. Then, ViTs are tasked with classifying the positions of visible patches among all possible candidates, relying on _partial_ observations and a few anchors (_i.e._, patches with unmasked PEs). Fig. 1 illustrates two benefits of DropPos: _(i)_ requiring less prior knowledge of data augmentation techniques compared to CL, and _(ii)_ eliminating the need for a careful selection of target representation  and mask strategy  as MIM does.

We conducted extensive experiments to evaluate the performance of DropPos in comparison to state-of-the-art alternatives across various downstream tasks. With only 800 epochs of pre-training, DropPos achieves 84.2% top-1 accuracy on ImageNet-1K  validation set using ViT-B/16 , outperforming MAE  pre-trained with 1600 epochs by +0.6%. Moreover, on COCO  object detection/segmentation, and ADE20k  semantic segmentation benchmarks, which requires more abilities of spatial reasoning, DropPos surpasses MAE _consistently_.

## 2 Related work

**Self-supervised learning.** The key challenge of self-supervised learning is the design of pretext tasks , how to produce appropriate supervision signals using _only_ images. Among them, contrastive learning  has been popular recently, which seeks to maximize agreement between different views of the same images. Nevertheless, the

Figure 1: **Comparison between different pretext tasks.****(a)** Contrastive learning aims to maximize the agreement between different views of one image. **(b)** Masked image modeling predicts specific contents of masked patches. **(c)** DropPos reconstructs positions _solely_ based on visual appearance.

performance of contrastive-related methods highly depends on carefully-designed data augmentation techniques . Another mainstream is masked image modeling , which involves predicting _specific contents_ masked patches . Unlike these methods, DropPos offers a novel alternative for self-supervised ViTs: reconstructing dropped positions based on _only_ visual clues. In this way, the model is urged to learn the spatial arrangement of local image patches, contributing to better localization abilities, which is important for spatial reasoning recognition tasks, _e.g._, segmentation  and video understanding .

**Position prediction in self-supervised learning.** Doersch _et al._ first split an image into 3\(\)3 grids and train a network to predict the _relative position_ of paired patches from the same image. This problem is formulated as an 8-way classification. Noroozi and Favaro  extended this approach to solve "jigsaw puzzles" by urging the model to _predict the order_ of _shuffled_ non-overlapping crops. These approaches were originally developed using ConvNets  and thus lacked the ability to learn long-range dependencies, which is crucial for position prediction. Very recently, Zhai _et al._ revisited this type of pretext task in the scope of Transformers  by pre-training ViTs  to predict patch positions _solely_ based on their visual appearance, while discarding positional embeddings. However, these pre-trained models have _never_ been exposed to positional embeddings, leading to discrepancies between pre-training and fine-tuning, and thus their performances are significantly inferior to state-of-the-art alternatives . Sameni _et al._ come up with an auxiliary position-related objective and combine it with the popular contrastive learning paradigm. Caron _et al._ extended this idea by predicting the relative location of a _query_ (_i.e._, local crop) to the corresponding _reference_ (_i.e._, global crop). However, _only_ examined the effectiveness on segmentation benchmarks . In contrast, DropPos incorporates a novel dropped position reconstruction objective and thus achieves competitive performances on a variety of downstream tasks, including classfication , detection , and segmentation .

**Positional embeddings in Vision Transformers.** In the standard forward procedure of ViTs , learnable positional embeddings (PEs) are added with patch embeddings. Improving PEs and introducing more inductive bias into ViTs has become an active research area . This is because some research shows that ViTs are surprisingly robust against permutations of the order of input patches. For instance, Chen _et al._ demonstrate that the model performs well in classification even with _no_ position embedding, suggesting that ViTs have _not_ fully leveraged the positional information. Additionally, Naseer _et al._ shows that ViTs are much _less susceptible_ to patch shuffling perturbations than ConvNets. Therefore, we propose DropPos, a novel self-supervised pretext task that explicitly makes ViTs location-aware and promotes the emergence of spatial features.

## 3 Method

**Preliminary: Vision Transformers.** Given an image \(^{H W C}\), it is first reshaped into a sequence of patches \(_{p}^{N(P^{2}C)}\), where \((H,W)\) indicates the spatial resolution, \(C\) is the number of channels, \(P\) is the patch size, and \(N=HW/P^{2}\) is the number of patches. A linear projection is then applied to \(_{p}\), mapping it to \(D\) dimensions to get patch embeddings \(^{N D}\). Also, a \(\) token \(_{}^{D}\) is used to aggregate the information. Position embeddings \(^{(N+1) D}\) are added to the patch embeddings to retain positional information. Then, \(=[_{};]\) is the input of transformer blocks  which consists of a stack of multi-head self-attention mechanisms, where \(\) denotes element-wise plus. In particular, \(_{0}\) denotes the position embeddings of the \(\) token, which will never be dropped.

### DropPos

Fig. 2 illustrates our proposed DropPos. We first randomly generate a binary patch mask \(\{0,1\}^{N}\), where 1 means visible and 0 means masked, respectively. Note that the \(\) token, _i.e._, \(_{0}\), is _always_ visible in our setting, and thus the length of \(\) is \(N\) instead of \(N+1\) for simplicity. Given a pre-defined mask ratio \((0,1)\), \(\) is supposed to subject to

\[_{i=0}^{N-1}_{i}=(1-)N,\] (1)

which means there are \( N\) masked patches in total. It is worth noticing that we use patch masks to increase the difficulty of the pretext task, which is quite different from building a pretext task to recover input images in MIM . Then, we denote \(_{}=(,)^{(1- )N D}\) to be the visible tokens, and position embeddings (PEs) for these unmasked tokens are randomly dropped, where \((,)\) means only those unmasked patches (_i.e._, \(_{i}=1\)) are reserved. To address possible discrepancies, we aim to _drop_ a subset of PEs of these visible patches instead of simply discarding all of them. Thus, we randomly generate a binary positional mask \(_{}\{0,1\}^{(1-)N}\). Given a pre-defined position mask ratio \(_{}(0,1)\), \(_{}\) is supposed to subject to

\[_{i=0}^{(1-)N-1}_{}^{i}=(1-_{})(1-)N,\] (2)

which indicates that the model is supposed to reconstruct \(_{}(1-)N\) dropped positions based on remaining \((1-_{})(1-)N\) anchor patches and the [CLS] token. PEs for visible patches \(_{}=(_{1:N},)\) are gathered first. Then, we aim to obtain the final PEs \(^{}^{(1-)N D}\) by replacing those dropped positions with a learnable [MASK] token:

\[_{i}^{}=_{0},&i=0,\\ _{}^{i-1},&_{}^{i-1}=1,\\ _{},&,\] (3)

where \(i=0,1,,(1-)N\) is the patch index, and \(_{}^{D}\) is the [MASK] token. Finally, the input of the Transformer encoder is \(^{}=[_{};_{}] ^{}^{((1-)N+1] D}\).

**Implementation.** Pre-training ViTs using our DropPos can be implemented efficiently and requires few specialized operations. The process involves three stages prior to forwarding features to the encoder. First, we randomly mask a subset of embedded patch tokens. Following MAE , the list of tokens is shuffled randomly, and the last portion of the list is removed. Next, positional embeddings of those unmasked patches are randomly dropped using the _same_ masking operation. Then, positional mask tokens are introduced to obtain the final positional embeddings, which are added to the patch tokens finally and encoded by ViTs followed by a lightweight decoder. A simple classification objective is applied to those unmasked patches introduced as follows.

```
#x:embeddedpatchtokens
#gamma_pos:patch/positionmaskratio
#mask_token:learnable[MASK]token
#patchmasking,[CLS]tokeniskeptvisible
#mask_vis_ =masking(x,gamma)
#x_vis_ = gather(x,ids_vis)
#positiondroppingforallpatches
#positiondrops_embed-#gather(pos_embed,ids_vis)
#ids_reps_ = masking(pos_embed,gamma_pos)
#embed=gather(pos_embed,ids_keep)
#concatwithmasktokens
#concatwithmask tokens
#pos_embed-#cat([pos_embed,mask_token])
#retstore
#edge_embed-#gather(pos_embed,ids_res)
#addtopatchtokensforforwardpass returnx_vis+pos_embed ```

**Algorithm 1** Pseudo-Code of DropPos.

Figure 2: **Illustration of DropPos. We first mask a large random subset of input images. Then, positional embeddings of visible patches are randomly dropped and [MASK] tokens are introduced. A lightweight projector is adopted to _reconstruct_ those dropped positions. A simple patch-wise classification objective is adopted and \(\) position targets do not contribute to training.**

### Pre-training DropPos

DropPos is pre-trained by reconstructing dropped positions. Concretely, as illustrated in Fig. 2 and presented in Sec. 3.1, an ViT \(f_{}\) parameterized by \(\), takes \(^{}\) as input. A lightweight decoder \(g_{}\) parameterized by \(\) then projects \(f_{}(^{})\) to patch-wise position predicitons \(=g_{}(f_{}(^{}))^{(1-) N N}\). We omit the [CLS] token here for simplicity. For each _visible_ patch \(i\{0,1,,(1-)N-1\}\), \((_{i})^{N}\) represents the probability density function of the predicted positions. Next, we have to produce appropriate ground truths, _i.e._, the actual position for each visible patch. To this end, we gather positions via \(=([0,1,,N-1],)^{(1- )N}\), where vector \([0,1,,N-1]\) is naturally the actual position for all patches before masking (w/o the [CLS] token). Overall, the objective is simply the vanilla cross-entropy loss given true positions \(_{ij}\):

\[=-_{i=0}^{(1-)N-1}_{j=0}^{N-1}(1-_{}^{i})(_{ij})[_{ij})}{_{k=0}^{(1-)N-1}(_{ik})}].\] (4)

_Only_ patches with dropped positions, _i.e._, \(\) in Fig. 2, contribute to this objective, which is reflected by the term \((1-_{}^{i})\) in Eq. (4). Next, several techniques are proposed to prevent ViTs from being overwhelmed by this particular task, making them pay more attention to model spatial dependencies instead of simply reconstructing positions.

**Position smoothing.** Considering that different categories (_i.e._, positions) are _not_ completely independent under this setting, the classification problem in Eq. (4) is relaxed when the model has predictions _close_ to the actual position. Specifically, a weight matrix \((0,1)^{N N}\) is defined to measure similarities between different patches:

\[w(i,j)=(-(i,j)}{^{2}}),\] (5)

where \((i,j)\) means the Euclidean distance between patch \(i\) and \(j\), and \(\) is a hyper-parameter. The matrix then should be normalized by \(w^{*}(i,j)=w(i,j)/(_{k=0}^{N-1}w(i,k))\). Here, \(_{i}^{*}^{N}\) indicates the smoothed ground truth when the actual position is \(i\). Overall, the smoothed objective is:

\[_{}=-_{i=0}^{(1-)N-1}_{j=0}^{N-1}(1- _{}^{i}) w^{*}(_{ij},j)[ _{ij})}{_{k=0}^{(1-)N-1}(_{ik})} ].\] (6)

Furthermore, we relax the problem in Eq. (6) by setting a large \(\) at the early stage, and we gradually _decay_ the value of \(\) to produce a challenging pretext task. A simple linearly decay is performed:

\[_{t}=(_{T}-_{0})+_{0},\] (7)

where \(t\) and \(T\) denote the iteration and the number of total steps, respectively. \(_{0}\) and \(_{T}\) are the initial value and final value of \(\), respectively. We set \(_{0}=1\) and \(_{T}=0\) by default.

**Attentive reconstruction.** Since there may be different patches that share similar visual appearance (_e.g._, two blue patches that represent the sky), it is _not_ necessary to reconstruct their _exact_ positions. Simply swapping these patches still maintains reasonable visual coherence. Therefore, we leverage the feature similarities between the [CLS] token and patch tokens to be an extra weight of Eq. (6):

\[_{}^{}=-_{i=0}^{(1-)N-1} _{j=0}^{N-1}(1-_{}^{i}) A_{_{ij}}  w^{*}(_{ij},j)[_{ij})}{ _{k=0}^{(1-)N-1}(_{ik})}],\] (8)

where \(^{N}\) denotes the similarity matrix. Concretely, \(A_{i}\) means the affinity between the [CLS] token and patch token \(i\). Let \(_{}^{D}\) and \(^{N D}\) be the features output by the encoder \(f_{}\), and thus the affinity \(A_{i}\) is computed by

\[_{i}=_{},_{i})/) }{_{j=0}^{N-1}((_{},_{j})/)},\] (9)

where \(\) indicates the temperature parameter and is set to 0.1 by default.

## 4 Experiments

**Pre-training.** We perform self-supervised pre-training on the ImageNet-1K  training set with a resolution of 224\(\)224. By default, we take ViT-B/16  as the backbone and perform 200 epochs of pre-training. The decoder is a stack of Transformer  and has a depth of 2 and a width of 512. Patch mask ratio \(\) and position mask ratio \(_{}\) are both 0.75 by default. Our implementation is based on HPM . Details can be found in _Supplementary Material_.

**Evaluation.** We perform supervised training to evaluate our DropPos with end-to-end fine-tuning on ImageNet-1K  for classification. By default, 100 epochs of fine-tuning are performed following common practices  for ViT-B/16 . We report top-1 validation accuracy of a single 224\(\)224 crop. As for COCO , following previous methods , we take Mask R-CNN  with FPN  as the detector. We perform end-to-end fine-tuning on COCO  for 1\(\) schedule with 1024\(\)1024 resolution. We report AP\({}^{}\) for object detection and AP\({}^{}\) for instance segmentation, respectively. Our implementation is based on detectron2  and ViTDet . For ADE20k , following previous methods , we take UperNet  as the decoder and perform end-to-end fine-tuning on ADE20k  with 512\(\)512 resolution for 80k iterations. We take mIoU  as the main evaluation metric. Our implementation is based on mmsegmentation .

### Ablation studies

In Tabs. 1 to 4, we take the ViT-B/16  pre-trained with 200 epochs on ImageNet-1K  as the backbone. We highlight the default settings of our DropPos. Concretely, if not specified, the default setting is \(=0.75\), \(_{}=0.75\), \(_{0}=1\), \(_{T}=0\), and \(=0.1\). By default, 100 epochs of fine-tuning on ImageNet-1K , 1\(\) schedule fine-tuning on COCO , and 80k iterations fine-tuning on ADE20k  are performed.

To evaluate the performance on the pretext task, _i.e._, reconstructing dropped positions, we also report the _averaged_ top-1 accuracy of position predictions on the ImageNet-1K _validation_ set in Tabs. 1 to 4. We vary \(\{0,0.25,0.5,0.75\}\) and \(_{}\{0.25,0.5,0.75,0.95\}\) when measuring the position prediction accuracy, and average the position accuracy among 16 different cases.

**Main properties.** Illustrated by Tabs. 1 to 4, we find sufficient evidence to support the three difficulties claimed in Sec. 1. _(i)_ In Tab. 1, _ViTs fail to learn highly semantic representations by simply solving the position reconstruction task_, because the performances of \(=0\) significantly lag behind. _(ii)_ _Discrepancies between pre-training and fine-tuning_ are revealed in Tab. 2 by setting

    &  &  &  &  \\   & fine-tune & position & \(^{}_{50}\) & \(^{}_{55}\) & \(^{}_{55}\) & \(^{}_{50}\) & \(^{}_{55}\) & \(_{55}\) & **mIoU** & aAcc \\ 
0.00 & 81.94 & 59.79 & **34.10** & 52.80 & 37.17 & **31.24** & 50.27 & 33.03 & **31.66** & 75.86 \\
0.25 & 82.02 & 79.62 & 37.05 & 56.23 & 40.18 & **33.60** & 53.51 & 35.84 & **32.80** & 76.92 \\
0.50 & 82.78 & 87.27 & **38.45** & 57.83 & 41.87 & **34.88** & 55.14 & 37.50 & **36.33** & 78.25 \\
0.75 & **82.96** & 87.83 & **42.14** & 61.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
0.90 & 82.81 & 65.12 & **41.06** & 60.92 & 44.57 & **37.04** & 58.23 & 39.62 & **40.30** & 80.09 \\   

Table 1: **Ablation study of patch mask ratio \(\). Note that “\(=0\)” means that the whole image is visible. We report the _averaged_ top-1 accuracy of position predictions.**

   }\)} &  &  &  &  \\   & fine-tune & position & \(^{}_{50}\) & \(^{}_{55}\) & \(^{}_{55}\) & \(^{}_{50}\) & \(^{}_{55}\) & **mIoU** & aAcc \\ 
0.25 & 82.71 & 65.19 & **37.98** & 57.65 & 41.39 & **34.61** & 54.79 & 36.93 & **36.15** & 78.16 \\
0.50 & 82.86 & 86.70 & **39.23** & 58.77 & 42.99 & **35.60** & 56.23 & 37.89 & **37.82** & 78.60 \\
0.75 & **82.96** & 87.83 & **42.14** & 61.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
1.00 & 82.66 & 19.44 & **41.48** & 61.50 & 45.18 & **37.41** & 58.81 & 39.92 & **39.98** & 80.19 \\   

Table 2: **Ablation study of position patch mask ratio \(_{}\). Note that “\(_{}=1\)” means that we do not provide any reference patch, _i.e._, all visible patches are randomly shuffled. We report the _averaged_ top-1 accuracy of position predictions.**\(_{}=1\), where we observe performance deterioration. _(iii) Deciding which position to reconstruct precisely is crucial_ because by setting \(=0\) in Tab. 3 and \(=\) in Tab. 4, ViTs have better position predictions task but perform worse on downstream tasks.

**Performance on the position reconstruction task.** In general, a _positive correlation_ is observed between the accuracy of the downstream task and the averaged accuracy of predicted positions, indicating that the averaged accuracy of position predictions is a reliable indicator of how well the model is trained. However, there are some exceptions, _i.e._, \(=0\) in Tab. 3 and \(=\) in Tab. 4, where the model reconstructs more precise positions but performs worse on downstream benchmarks, indicating that _deciding which position to reconstruct precisely is crucial_.

Please refer to _Supplementary Material_ for the detailed performance of the position reconstruction task, where we provide more evidence to support the three difficulties proposed in Sec. 1.

**Patch mask ratio \(\).** We vary the patch mask ratio \(\) from 0 to 0.75 in Tab. 1. When \(=0\), the entire image is visible during pre-training, resulting in nearly _perfect_ position predictions and making the task too simple for self-supervised learning (see _Supplementary Material_ for details). However, the _averaged_ accuracy of position predictions is quite low since the accuracy deteriorates quickly as we enlarge \(\) during evaluation. Adopting a larger \(\) for pre-training leads to better performance, especially on detection and segmentation benchmarks. Therefore, we can conclude that _patch masking is essential in DropPos to prevent trivial solutions_. However, deterioration is observed when \(=0.9\). This is because the model may be under-fitted using this extremely difficult pretext task.

**Position mask ratio \(_{}\).** We study the effectiveness of the position mask ratio \(_{}\) in Tab. 2. DropPos appears to be more _robust_ against different \(_{}\) than \(\). However, an appropriate \(_{}\) is still necessary. A small \(_{}\) leads to trivial solutions even if we have a large \(\), while an extremely large \(_{}\), _e.g._, \(_{}=1\), results in _discrepencies between pre-training and fine-tuning_.

**Position smoothing.** By setting different values of \(_{0}\) and \(_{T}\), we study the effectiveness of position smoothing. By setting \(=0\), we remove the position smoothing strategy, and the model tends to be overwhelmed by this position reconstruction task, leading to _high accuracy in position prediction but poor performance on downstream tasks_. However, when \(\) is too large, the smoothed positions become noisy, contributing to _poor_ performance on _both_ downstream tasks and position reconstruction. Additionally, equipped with a linear decay schedule of \(\), DropPos is gradually guided, bringing improvements on _both_ downstream tasks and position reconstruction.

**Attentive reconstruction.** Attentive reconstruction is studied in Tab. 4 by setting different values of temperature \(\), where \(=\) means no attentive reconstruction. Without attentive reconstruction, DropPos is able to obtain better position predictions but performs worse on downstream tasks. This is because we do _not_ have to reconstruct the _exact_ positions when _different patches share similar visual appearances_. This phenomenon can be mitigated by setting an appropriate \(\). A large \(\) leads

    &  &  &  &  \\   & fine-tune & position & \(}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & mIoU & aAcc \\  \(\) & 82.84 & 88.66 & **40.56** & 60.48 & 44.15 & **36.78** & 57.64 & 39.46 & **38.38** & 79.36 \\
0.1 & **82.96** & 87.83 & **42.14** & 61.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
0.5 & 82.86 & 87.78 & **40.73** & 60.33 & 44.77 & **36.71** & 57.72 & 39.38 & **38.80** & 79.62 \\   

Table 4: **Ablation study of attentive reconstruction. “\(=\)” indicates no attentive reconstruction. We report the _averaged_ top-1 accuracy of position predictions.**

    &  &  &  &  \\   & fine-tune & position & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & mIoU & aAcc \\ 
0 & 82.85 & 88.81 & **40.32** & 59.89 & 43.90 & **36.45** & 57.09 & 38.88 & **38.32** & 79.22 \\
1 & 82.91 & 87.13 & **40.19** & 59.76 & 43.81 & **36.30** & 57.06 & 38.87 & **38.60** & 79.17 \\
2 & 82.79 & 69.48 & **40.16** & 59.94 & 43.92 & **36.32** & 57.32 & 38.88 & **36.69** & 79.30 \\
1 \(\) 0 & **82.96** & 87.83 & **42.14** & 611.99 & 46.38 & **37.93** & 59.23 & 40.76 & **40.68** & 80.14 \\
2 \(\) 0 & 82.88 & 87.65 & **40.46** & 60.03 & 44.21 & **36.53** & 57.20 & 39.30 & **38.79** & 79.43 \\   

Table 3: **Ablation study of \(\). “\(=0\)” means no position smoothing. “\(\)” denotes a linear decay schedule is performed on \(\). We report the _averaged_ top-1 accuracy of position predictions.**to noisy position ground truths, leading to _poor_ performance on _both_ downstream tasks and position reconstruction, which is the same as a large \(\).

### Comparisons with previous results

We compare our proposed DropPos with the supervised pre-training baseline and a wide range of self-supervised methods, including _(i)_ contrastive learning methods , _(ii)_ masked image modeling methods , and _(iii)_ their combinations . Effective pre-training epoch is used for fair comparison following  since it accounts for the _actual_ trained images/views. The detailed definition can be found in _Supplementary Material_. All methods are pre-trained with the same resolution, _i.e._, 224\(\)224 on ImageNet-1K .

**ImageNet-1K classification.** We compare our DropPos with state-of-the-art alternatives on the ImageNet-1K  classification benchmark in Tab. 5. Notably, with only 200 epochs pre-training,

    &  &  &  &  &  \\   & & & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) & mIoU & aAcc \\  MAE\({}^{}\) & [CVPR’22] & 200 & 40.1 & 60.5 & 44.1 & **36.4** & 57.8 & 39.3 & 40.5 & 80.1 \\ DropPos & [Ours] & 200 & **42.1** & 62.0 & 46.4 & **37.9** & 59.2 & 40.8 & **40.7** & 80.1 \\  MoCo v3\({}^{}\) & [ICCV’21] & 600 & 43.7 & 65.7 & 47.7 & **39.1** & 62.0 & 41.8 & **44.7** & 81.5 \\ MAE\({}^{}\) & [CVPR’22] & 1600 & 47.3 & 68.2 & 52.5 & **42.4** & 65.3 & 45.6 & **47.0** & 82.7 \\ BootMAE\({}^{}\) & [ECCV’22] & 800 & 47.3 & 67.9 & 52.1 & **42.3** & 65.0 & 45.8 & **47.3** & 83.0 \\ SemMAE\({}^{}\) & [NeuIPS’22] & 800 & 45.6 & 66.2 & 55.2 & **40.9** & 63.3 & 44.4 & **44.9** & 82.0 \\ LocalMIA\({}^{}\) & [CVPR’23] & 1600 & 47.4 & 67.7 & 52.2 & **42.2** & 64.8 & 45.5 & **47.1** & 83.1 \\ DropPos & [Ours] & 800 & **47.7** & 68.3 & 52.8 & **42.6** & 65.3 & 46.2 & **47.8** & 82.8 \\   

Table 6: **Comparison with previous methods on downstream tasks.** All methods take the ViT-B/16  as the backbone and utilize Mask R-CNN  on COCO  object detection and instance segmentation, and UperNet  on ADE20k  semantic segmentation, respectively. \(\) means the result is borrowed from . \(\) indicates our implementation, including pre-training and supervised fine-tuning, while \(\) represents we reproduce fine-tuning using the official pre-trained backbone. We perform 1\(\) schedule of fine-tuning on COCO using ViTDet , and 80k iterations of fine-tuning on ADE20k using mmsegmentation .

   method & venue & eff. ep. & ViT-B & ViT-L \\  supervised & & - & 80.9\({}^{}\) & 82.6\({}^{}\) \\ MAE  & [CVPR’22] & 200 & 82.2\({}^{}\) & 83.3\({}^{}\) \\ DropPos & [Ours] & 200 & **83.0** & **83.7** \\   \\ DINO\({}^{}\) & [ICCV’21] & 1600 & 82.8 & - \\ MoCo v3\({}^{}\) & [ICCV’21] & 600 & 83.2 & 84.1 \\   \\ BEI\({}^{}\) & [ICLR’22] & 800 & 83.2 & 85.2 \\ MAE\({}^{}\) & [CVPR’22] & 1600 & 83.6 & **85.9** \\ SimMIM  & [CVPR’22] & 800 & 83.8 & - \\ SemMAE  & [NeuIPS’22] & 800 & 83.4 & - \\ LocalMIM  & [CVPR’23] & 1600 & 84.0 & - \\ HPM  & [CVPR’23] & 800 & **84.2** & 85.8 \\   \\ iBOT  & [ICLR’22] & 1600 & 84.0 & - \\ BootMAE  & [ECCV’22] & 800 & **84.2** & **85.9** \\   \\ DropPos & [Ours] & 800 & **84.2** & 85.8 \\   

Table 5: **Comparison with previous methods on ImageNet-1K classification.** All methods are evaluated by fine-tuning. The resolution of images is fixed to 224\(\)224. \(\) means our implementation. \(\) means the result is borrowed from .

DropPos achieves 83.0% and 83.7% using ViT-B/16 and ViT-L/16 as the backbone, surpassing the supervised baseline by +1.1%, and MAE  by +0.8% and +0.4% respectively. This empirical evidence demonstrates that _enhancing the location awareness of ViTs indeed brings better visual representations_. Furthermore, with 800 epochs of pre-training, DropPos manages to achieve competitive results compared with the state-of-the-art. Specifically, it achieves 84.2% and 85.8% using ViT-B/16 and ViT-L/16, respectively. Strikingly, DropPos reaches competitive results with BootMAE , which combines contrastive learning and masked image modeling.

**COCO object detection and segmentation.** We fine-tune Mask R-CNN  on COCO  with 1\(\) schedule, _i.e._, 12 epochs, using the configuration of ViTDet . We take ViT-B/16  as the backbone for all entries in Tab. 6. We regard AP\({}^{}\) and AP\({}^{}\) as the main metric for object detection and instance segmentation, respectively. For further comparison, we additionally report AP\({}^{}_{50}\) and AP\({}^{}_{75}\) for object detection, and AP\({}^{}_{50}\) and AP\({}^{}_{75}\) for instance segmentation. With only 200 epochs of pre-training, DropPos achieves 42.1% AP\({}^{}\) and 37.9% AP\({}^{}\), outperforming MAE  by +2.0% and +1.5%, respectively. Note that these improvements appear to be more significant than those on the classification benchmark shown in Tab. 5, indicating that _DropPos indeed contributes to better spatial reasoning abilities of ViTs_. With 800 epochs of pre-training, DropPos achieves 47.7% AP\({}^{}\) and 42.6% AP\({}^{}\), surpassing MAE  by +0.4% and +0.2%, respectively.

**ADE20k semantic segmentation.** We fine-tune UperNet  on ADE20k  with 80k iterations. We take ViT-B/16  as the backbone for all entries in Tab. 6 and search for the optimal learning rate for each entry. We regard mIoU as the main metric for semantic segmentation. We additionally report aAcc for further comparison. With only 200 epochs of pre-training, DropPos achieves 40.7% mIoU, outperforming MAE  by +0.2%. With 800 epochs of pre-training, DropPos achieves 47.8% mIoU, surpassing MAE  by +0.8%.

**Qualitative results.** We load DropPos with 200 epochs of pre-training and provide qualitative results on this position reconstruction task in Fig. 3. DropPos manages to reconstruct the exact positions of most patches _even in extreme circumstances_, _i.e._, \(=0.75\) and \(_{}=0.95\). This suggests that DropPos, as a self-supervised pretext task, _indeed makes ViTs location-aware_.

### Analysis

To explore whether the improved position sensitivity results in better feature representation and benefits to downstream tasks or not, we propose a metric to evaluate the position sensitivity.

    &  & Position prediction & ImageNet-1K \\  & & Pre-trained & Fine-tuned & Top-1 \\  MoCo v3  & [ICCV’21] & 43.2 & 78.0 \( 34.8\) & 83.2 \\ MAE  & [CVPR’22] & 63.1 & 82.5 \( 19.4\) & 83.6 \\ DropPos & [ours] & **77.3** & **88.0**\( 10.7\) & **84.2** \\   

Table 7: An in-depth analysis about **whether the improved position sensitivity benefits downstream tasks or not**. We freeze the backbone and train an extra linear patch classification head. 75% of position embeddings are randomly masked during linear probing. We can conclude that _image classification indeed needs better location awareness_ and thus designing a position prediction pretext task is crucial and worth studying.

Figure 3: **Qualitative results of position reconstruction.** We evaluate position predictions with _different_\(\) but fix \(_{}=0.95\). Black patches are _masked_ during inference. The positions of those white patches are wrongly predicted, while the remaining patches are predicted correctly. For each tuple, we show results under **(a)**\(=0\), **(b)**\(=0.25\), **(c)**\(=0.5\), **(d)**\(=0.75\), and **(e)** the original image. DropPos manages to reconstruct _precise_ positions.

Specifically, we freeze the backbone and train an extra linear position prediction head using the vanilla cross-entropy loss. Top-1 accuracies of position predictions _before and after_ fine-tuning are reported in Tab. 7, and 75% of position embeddings are randomly masked during training. Higher values mean the model is better at modeling the position relationship. The top-1 accuracy on the ImageNet-1K  validation set after fine-tuning is also reported.

As shown in Tab. 7, the backbone performs better in position prediction after fine-tuning, indicating that _image classification indeed needs strong abilities in modeling spatial relationships_. It means that _better position sensitivity corresponds to better performances on downstream tasks_. This evidence suggests that our motivation, _i.e._, enhancing the location awareness of ViTs, is reasonable, and the topic is worth studying. By designing a position prediction pretext task, the backbone pre-trained by DropPos has better position modeling abilities, performing better on a variety of downstream tasks.

## 5 Conclusion

In this paper, we present DropPos, a novel, simple, and effective self-supervised pretext task that enhances the location awareness of ViTs. By masking patch tokens first and dropping positional embeddings next, ViTs are requesting to classify the position of each visible patch among all candidates based on partial visual appearance and a few anchor patches. In this way, we manage to avoid _(i) discrepancies_ between pre-training and fine-tuning, _(ii) trivial solutions_, and _(iii)_ reconstructing precise positions when _unnecessary_, resulting in improved spatial reasoning and understanding abilities of ViTs. Experiments across various benchmarks demonstrate the efficacy of DropPos, where DropPos _consistently_ achieves competitive results compared with previous methods.

**Discussion.** Due to limited computational resources, we do not use DropPos to pre-train larger ViTs, _e.g._, ViT-H/14. Despite these limitations, a growing need to design a pretext task that effectively uncovers the representation learning capabilities of vision models is becoming evident. We hope our DropPos will inspire future work. Additionally, how to enhance location awareness of vision models for spatial reasoning tasks in a supervised manner is also valuable to study.