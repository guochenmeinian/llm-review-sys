# Beyond Concept Bottleneck Models:

How to Make Black Boxes Intervenable?

Sonia Laguna, Ricards Marcinkevics, Moritz Vandenhirtz, Julia E. Vogt

Department of Computer Science, ETH Zurich, Switzerland

Equal contribution. Correspondence to sonia.lagunacillero@inf.ethz.ch

###### Abstract

Recently, interpretable machine learning has re-explored concept bottleneck models (CBM). An advantage of this model class is the user's ability to intervene on predicted concept values, affecting the downstream output. In this work, we introduce a method to perform such concept-based interventions on _pretrained_ neural networks, which are not interpretable by design, only given a small validation set with concept labels. Furthermore, we formalise the notion of _intervenability_ as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black boxes. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We focus on backbone architectures of varying complexity, from simple, fully connected neural nets to Stable Diffusion. We demonstrate that the proposed fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of our techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes are more intervenable than CBMs. Lastly, we establish that our methods are still effective under vision-language-model-based concept annotations, alleviating the need for a human-annotated validation set.

## 1 Introduction

Interpretable and explainable machine learning (Doshi-Velez & Kim, 2017; Molnar, 2022) have seen a renewed interest in concept-based predictive models and approaches to post hoc explanation, such as concept bottlenecks (Lampert et al., 2009; Kumar et al., 2009; Koh et al., 2020), contextual semantic interpretable bottlenecks (Marcos et al., 2020), concept whitening layers (Chen et al., 2020), and concept activation vectors (B. Kim et al., 2018). Moving beyond interpretations defined in the high-dimensional, unwieldy input space, these techniques relate the model's inputs and outputs via additional high-level human-understandable attributes, also referred to as _concepts_. Typically, neural network models are supervised to predict these attributes in a dedicated bottleneck layer, or post hoc explanations are derived to measure the model's sensitivity to concept variables.

This work focuses specifically on the concept bottleneck models, as revisited by Koh et al. (2020). In brief, a CBM is a neural network consisting of successive concept and target prediction modules, where the final output depends on the input solely through the predicted concepts. Such models are trained on labelled data, in addition, annotated by attributes. At inference time, a human user may interact with the CBM by editing the predicted concept values, which, as a result, affects the downstream target prediction. This act of model editing is known as an _intervention_. The user's ability to intervene is a compelling advantage of CBMs over other interpretable model classes, in that the former allows for human-model interaction.

In contrast to previous works (Yuksekgonul et al., 2023; Oikarinen et al., 2023), we focus on _instance-specific_ interventions, _i.e._ performed individually for each data point. To this end, we explore two questions: **(i)**_Given a small validation set with concept labels, how can we perform instance-specific interventions directly on a pretrained black-box model?_ **(ii)**_How can we fine-tune the black-box model to improve the effectiveness of interventions performed on it?_

Such instance-specific interventions can be relevant in high-stakes decisions. Our specific motivation is healthcare. For instance, consider computer-aided diagnosis, where a doctor may make decisions assisted by a predictive model. In this setting, the doctor handles patients on a _case-by-case_ basis and may benefit from instance-specific interactions with the black box. While, in principle, a specialist may just override predictions, in many cases, concept and target variables are linked via nonlinear relationships potentially unknown to the user. Figure 1 contains a simplified, intuitive example of an instance-specific concept-based intervention for natural images. Additional and more comprehensive examples can be found in Appendix A.

ContributionsThis work contributes to the research on concept bottleneck models and concept-based explanations. **(1)** We devise a simple procedure that, given a set of concepts and a small labelled validation set, allows performing concept-based instance-specific interventions (Figure 1) on a pretrained black-box neural network by editing its activations at an intermediate layer. Notably, concept labels are not required in the large _training set_, and the network's architecture does not need to be adjusted. **(2)** We formalise _intervenability_ as a measure of the effectiveness of interventions performed on the model. Utilising intervenability as a loss, we introduce a novel fine-tuning procedure. This fine-tuning strategy is designed to improve the effectiveness of concept-based interventions. It preserves the original model's architecture and representations to be used in downstream tasks. **(3)** We evaluate the proposed procedures alongside several baselines on the synthetic tabular, natural image, and medical imaging data. We demonstrate that in practice, for studied classification problems, we can improve the predictive performance of pretrained black-box models via concept-based interventions. We investigate fully connected and more complex backbone architectures. We show that the effectiveness of interventions improves considerably when explicitly fine-tuning for intervenability. Lastly, we observe that our methods are successful in datasets where concept labels are acquired using vision-language models (VLM), alleviating the need for a human annotation.

## 2 Related Work

The use of high-level attributes in predictive models has been well-explored in computer vision (Lampert et al., 2009; Kumar et al., 2009). Recent efforts have focused on explicitly incorporating concepts in neural networks (Koh et al., 2020; Marcos et al., 2020), producing high-level post hoc explanations by quantifying the network's sensitivity to the attributes (B. Kim et al., 2018), probing (Alain and Bengio, 2016; Belinkov, 2022) and de-correlating and aligning the network's latent space with concept variables (Chen et al., 2020). Other works (Xie et al., 2020) have studied the use of auxiliary external attributes in out-of-distribution settings. To alleviate the assumption of being given interpretable concepts, some have explored concept discovery prior to post hoc explanation (Ghorbani et al., 2019; Yeh et al., 2020). Another relevant line of work investigated concept-based counterfactual explanations (CCE) (Abid et al., 2022; S. Kim et al., 2023).

Concept bottleneck models (Koh et al., 2020) have sparked a renewed interest in concept-based classification methods. Many related works have described the inherent limitations of this model class and attempted to address them (Margeloiu et al., 2021; Mahinpei et al., 2021; Marconato et al.,

Figure 1: A simplified, intuitive example: an image of a grizly bear is wrongly identified as an outer. Our method allows performing a concept-based intervention and flip the predicted class. In order of appearance from left to right and top to bottom, the depicted concepts and classes are “fierce”, “timid”, “muscle”, “walks”, “otter”, and “grizly bear”.

2022; Havasi et al., 2022; Sawada and Nakamura, 2022; Marcinkevics et al., 2024). Another line of research has investigated modelling uncertainty and probabilistic extensions of the CBMs (Collins et al., 2023; E. Kim et al., 2023). Most related to the current paper are the techniques for converting pretrained black-box neural networks into CBMs post hoc (Yuksekgonul et al., 2023; Oikarinen et al., 2023) by keeping the network's backbone and projecting its activations into the concept space. Additionally, these works explore automated concept discovery using VLMs.

As mentioned, CBMs allow for concept-based instance-specific interventions. Several follow-up works have studied interventions in further detail. Chauhan et al. (2023) and Sheth et al. (2022) introduce adaptive intervention policies to further improve the predictive performance of the CBMs at the test time. In a similar vein, Steinmann et al. (2023) propose learning to detect mistakes in the predicted concepts and, thus, learning intervention strategies. Shin et al. (2023) empirically investigate different intervention procedures across various settings.

## 3 Methods

In this section, we define a measure for the effectiveness of concept-based interventions and present a technique for intervening on black-box neural networks. Furthermore, we propose a fine-tuning procedure to improve the effectiveness of such interventions. Additional remarks beyond the current scope are included in Appendix C.

In the remainder of this paper, we will adhere to the following notation. Let \(\), \(y\), and \(\) be the covariates, targets, and concepts. A CBM \(f_{}\), parameterised by \(\), is given by \(f_{}()=g_{}(h_{}( {x}))\), where \(h_{}:\) maps inputs to predicted concepts, _i.e._\(}=h_{}()\), and \(g_{}:\) predicts the target based on \(}\), _i.e._\(=g_{}(})\). CBMs are trained on data points \((,,y)\) and are supervised by the concept and target prediction losses. At test time, if the user chooses to replace \(}\) with another \(^{}\), _i.e._ intervene, the final prediction is given by \(^{}=g_{}(^{})\).

Next to CBMs, we will consider a black-box neural network \(f_{}:\) parameterised by \(\) and a slice \( g_{},h_{}\)(Leino et al., 2018), defining a layer, s.t. \(f_{}()=g_{}(h_{}( {x}))\). We will assume that the black box has been trained end-to-end on the labelled data \(\{(_{i},y_{i})\}_{i}\). Lastly, for all techniques, we will assume being given a small _validation_ set \(\{(_{i},_{i},y_{i})\}_{i}\).

### Intervening on Black-box Models

Given a black-box model \(f_{}\) and a data point \((,y)\), a human user might desire to influence the prediction \(=f_{}()\) made by the model via high-level and understandable concept values \(^{}\), _e.g._ think of a doctor trying to interact with a chest X-ray classifier (\(f_{}\)) by annotating their findings (\(^{}\)) in a radiograph (\(\)), where findings correspond to the clinical concepts, such as the presence of edema or fracture. To facilitate such interactions, we propose a simple recipe for concept-based instance-specific interventions (detailed in Figure 2) that can be applied to _any_ black-box neural network model. Intuitively, using the given validation data and concept values, our procedure edits the network's representations \(=h_{}()\), where \(\), to align more closely with \(^{}\) and, thus, affects the downstream prediction. Below, we explain this procedure step-by-step. Pseudocode implementation can be found as part of Algorithm B.1 in Appendix B.

Figure 2: Three steps of the intervention procedure. (i) A probe \(q_{}\) is trained to predict the concepts \(\) from the activation vector \(\). (ii) The representations are edited according to Equation 1. (iii) The final prediction is updated to \(^{}\) based on the edited representations \(^{}\).

Step 1: ProbingTo align the network's activation vectors with concepts, the preliminary step is to train a probing function (Alain and Bengio, 2016; B. Kim et al., 2018; Belinkov, 2022), or a _probe_ for short, mapping the intermediate representations to concepts. Namely, using the given annotated validation data \(\{(_{i},_{i},y_{i})\}_{i}\), we train a multivariate probe \(q_{}\) to predict the concepts \(_{i}\) from the representations \(_{i}=h_{}(_{i})\): \(_{}_{i}^{}(q_{}(_{i} ),_{i})\), where \(^{}\) is the concept prediction loss. Note that, herein, an essential design choice explored in our experiments is the (non)linearity of the probe. Consequently, the probing function can be used to interpret the activations in the intermediate layer and edit them.

Step 2: Editing RepresentationsRecall that we are given a data point \((,y)\) and concept values \(^{}\) for which an intervention needs to be performed. Note that this \(^{}\) could correspond to the ground-truth concept values or reflect the beliefs of the human subject intervening on the model. Intuitively, we seek an activation vector \(^{}\), which is similar to \(=h_{}()\) and consistent with \(^{}\) according to the previously learnt probing function \(q_{}\): \(_{^{}}\ d(,^{}),\ \ q_{} (^{})=^{}\), where \(d\) is an appropriate distance function applied to the activation vectors from the intermediate layer. Throughout main experiments (Section 4), we utilise the Euclidean distance, which is frequently applied to neural network representations, _e.g._ see works by Moradi Fard et al. (2020) and Jia et al. (2021). In Appendix F.8, we additionally explore the cosine distance. Instead of the constrained problem above, we resort to minimising a relaxed objective:

\[_{^{}}\ ^{}(q_{} (^{}),^{})+d(,^{ }),\] (1)

where, similarly to the counterfactual explanations (Wachter et al., 2017; Mothilal et al., 2020), hyperparameter \(>0\) controls the tradeoff between the intervention's validity, _i.e._ the "consistency" of \(^{}\) with the given concept values \(^{}\) according to the probe, and proximity to the original activation vector \(\). In practice, we optimise \(^{}\) for batched interventions using Adam (Kingma and Ba, 2015). Appendix F.2 explores the effect of \(\) on the post-intervention distribution of representations.

Step 3: Updating OutputThe edited \(^{}\) can be consequently fed into \(g_{}\) to compute the updated output \(^{}=g_{}(^{})\), which could be then returned and displayed to the human subject. For example, if \(^{}\) are the ground-truth concept values, we would ideally expect a decrease in the prediction error for the given data point \((,y)\).

### What is Intervenability?

Concept bottlenecks (Koh et al., 2020) and their extensions are often evaluated empirically by plotting test-set performance or error attained after intervening on concept subsets of varying sizes. Ideally, the model's test-set performance should improve when given more ground-truth attribute values. Below, we formalise this notion of intervention effectiveness, referred to as _intervenability_, for the concept bottleneck and black-box models.

For a trained CBM \(f_{}()=g_{}(h_{}( {x}))=g_{}(})\), where \(}\) are the predicted concept values, we define the intervenability as follows:

\[,,y)}{} ^{}}{}^{y} }()}_{=g_{}( })},y-^{y}} (^{})}_{^{}},y,\] (2)

where \(\) is the joint distribution over the covariates \(\), concepts \(\), and targets \(y\), \(^{y}\) is the target prediction loss, _e.g._ the mean squared error (MSE) or cross-entropy (CE), and \(\) denotes a distribution over edited concept values \(^{}\). Observe that Equation 2 generalises the standard evaluation strategy of intervening on a random concept subset and setting it to the ground-truth values, as proposed in the original work by Koh et al. (2020). Here, the effectiveness of interventions is quantified by the gap between the regular prediction loss and the loss attained after the intervention: the larger the gap between these values, the stronger the effect interventions have. The intervenability measure is loosely related to permutation-based variable importance and model reliance (Fisher et al., 2019). We provide a discussion of this relationship in Appendix C.

Note that the definition in Equation 2 can also accommodate more sophisticated intervention strategies, for example, similar to those studied by Shin et al. (2023) and Sheth et al. (2022). An intervention strategy can be specified via the distribution \(\), which can be conditioned on \(\), \(}\), \(\), \(\)or even \(y\): \((^{}|,},},},y)\). The set of conditioning variables may vary across application scenarios. For brevity, we will use \(\) as a shorthand notation for this distribution. Lastly, notice that, in practice, when performing human- or application-grounded evaluation (Doshi-Velez & Kim, 2017), sampling from \(\) may be replaced with the interventions by a human. Algorithms E.1 and E.2 provide concrete examples of the strategies utilised in our experiments.

Leveraging the intervention procedure described in Section 3.1, analogous to Equation 2, the inter-venability for a black-box neural network \(f_{}\) at the intermediate layer given by \( g_{},h_{}\) is

\[&_{(,e,y) ,\,^{}}[^{y}( f_{}(),y)-^{y}(g_{ }(^{}),y)],\\ &\,\,^{}_{ }}^{}(q_{ {}}(}),^{})+d (,}).\] (3)

Recall that \(q_{}\) is the probe trained to predict \(\) based on the activations \(h_{}()\) (step 1, Section 3.1). Furthermore, in the first line of Equation 3, edited representations \(^{}\) are a function of \(^{}\), as defined by the second line, which corresponds to step 2 of the intervention procedure (Equation 1).

### Fine-tuning for Intervenability

Since the intervenability measure defined in Equation 3 is differentiable, a neural network can be fine-tuned by explicitly maximising it using, for example, mini-batch gradient descent. We expect fine-tuning for intervenability to reinforce the model's reliance on the high-level attributes and have a regularising effect. In this section, we provide a detailed description of the fine-tuning procedure (Algorithm B.1, Appendix B), and, afterwards, we demonstrate its practical utility empirically.

Naively optimising intervenability from Equation 3 may decrease the predictive performance. Therefore, to fine-tune an already trained black-box model \(f_{}\), we combine the intervenability term with the target prediction loss, which amounts to the following optimisation problem:

\[_{,^{}}_{( ,,y),\,^{ }}^{y}g_{}( ^{}),y,\;\,\, {z}^{}_{}}^{ }(q_{}(}), ^{})+d(,} ).\] (4)

Notably, Equation 4 can be generalised by introducing a weight for the intervenability term:

\[&_{,, ^{}}_{(,,y ),\,^{}}(1- )^{y}g_{}(h_{} ()),y+^{y}g_{ }(^{}),y,\\ &\,\,^{}_{ }}^{}(q_{ }(}),^{} )+d(,}),\] (5)

where \((0,1]\) is the aforementioned weight. Note that for \(=1\), the optimisation simplifies to Equation 4. For simplicity, we treat the probe's parameters \(\) as fixed. However, since the outer optimisation problem is defined w.r.t. \(\), ideally, the probe would need to be optimised as the third, inner-most level. By contrast, in the simplified setting under \(=1\) (Equation 4), the parameters of \(h_{}\) do not need to be optimised, and, hence, the probing function can be left fixed, as activations \(\) are not affected by the fine-tuning. We consider this case to (i) computationally simplify the problem, avoiding trilevel optimisation, and (ii) keep the network's representations unchanged after fine-tuning for purposes of transfer learning for other downstream tasks. In practice, fine-tuning is performed by intervening on batches of data points. Since interventions can be executed online using a GPU (within seconds), our approach is computationally feasible.

## 4 Experimental Setup

DatasetsWe evaluate the proposed methods on synthetic and real-world benchmarks summarised in Table D.1 (Appendix D). Across all experiments, fine-tuning has been performed exclusively on the validation data, and evaluation--on the test set. Further details can be found in Appendix D.

For controlled experiments, we have adapted the nonlinear **synthetic** tabular dataset from Marcinkevics et al. (2024). We consider two data-generating mechanisms shown in Figure D.1 (Appendix D.1): _bottleneck_, and _incomplete_. The first scenario directly matches the inference graph of the vanilla CBM. The _incomplete_ is a scenario with incomplete concepts, where \(\) does not fully explain the association between \(\) and \(y\), with unexplained variance modelled via a residual connection.

Another benchmark we consider is the **Animals with Attributes 2 (AwA2)** natural image dataset (Lampert et al., 2009; Xian et al., 2019). It includes animal images accompanied by 85 binary attributes and species labels. To further corroborate our findings, we perform experiments on the Caltech-UCSD Birds-200-2011 (**CUB**) dataset (Wah et al., 2011) (Appendix D.3), adapted for the CBM setting as described by Koh et al. (2020). We report the CUB results in Appendix F.4.

To investigate settings _without_ human-annotated concept values, we evaluate our method on **CIFAR-10**(Krizhevsky et al., 2009) and the large-scale **ImageNet**(Russakovsky et al., 2015) natural image datasets. Following the previous literature (Oikarinen et al., 2023), we use concepts generated by GPT-3. Concept labels are produced based on CLIP (Radford et al., 2021) similarities between each image and verbal descriptions. We utilise 143 attributes for CIFAR-10 and 100 for ImageNet. ImageNet results are reported in Appendix F.5.

Finally, we explore a practical setting of chest radiograph classification. Namely, we test the techniques on public **MIMIC-CXR**(Johnson et al., 2019) and **CheXpert**(Irvin et al., 2019) datasets from the Beth Israel Deaconess Medical Center, Boston, MA, and Stanford Hospital. Both datasets have 14 binary attributes extracted from radiologist reports. In our analysis, the _Finding/No Finding_ attribute is the target variable, and the remaining labels are the concepts, similar to Chauhan et al. (2023). For simplicity, we retain a single X-ray per patient, excluding data with uncertain labels. The results on CheXpert are similar to those on MIMIC-CXR and can be found in Appendix F.6.

Baselines & MethodsBelow, we briefly outline the neural network models and fine-tuning techniques compared. All methods were implemented using PyTorch (v 1.12.1) (Paszke et al., 2019). Appendix E provides additional details. The code is available in a repository at https://github.com/sonialagunac/Beyond-CBM.

Firstly, we train a standard neural network (**Black box**) without concept knowledge, _i.e._ on the dataset of tuples \(\{(_{i},y_{i})\}_{i}\). We utilise our technique for intervening post hoc by training a probe to predict concepts and editing the network's activations (Equation 1, Section 3.1). All experiments reported in Section 5 use a linear probe, while the nonlinearity is explored in Appendix F. As an interpretable baseline, we consider the vanilla concept bottleneck model (**CBM**) by Koh et al. (2020). Across all experiments, we restrict ourselves to the joint bottleneck version, which minimises the weighted sum of the target and concept prediction losses: \(_{,}_{(,,y)}[^{y}(f_{}(),y)+ ^{}(h_{}(),)]\), where \(>0\) is a hyperparameter controlling the tradeoff between the two loss terms. Finally, as the primary method of interest, we apply our fine-tuning for intervenability technique (**Fine-tuned, I**; Equation 4, Section 3.3) on the annotated validation set \(\{(_{i},_{i},y_{i})\}_{i}\).

In addition, as a common-sense baseline, we fine-tune the black box by training a probe to predict the concepts from intermediate representations (**Fine-tuned, MT**). This amounts to multitask (MT) learning with hard weight sharing (Ruder, 2017). Specifically, the model is fine-tuned by minimising the following MT loss: \(_{,,}_{(,,y) }[^{y}(f_{}(),y )+^{}(q_{}(h_{}( )),)]\). Interventions on this model are performed using the three-step approach introduced in Section 3.1.

As another baseline, we fine-tune the black box by appending concepts to the network's activations (**Fine-tuned, A**). At test time, unknown concept values are set to \(0.5\). To prevent overfitting and handle missingness, randomly chosen concept variables are masked during training. The objective is given by \(_{}}_{(,,y)}[^{y}(_{}}([h_{} (),]),y)]\), where \(\) takes as input concatenated activation and concept vectors. Note that, for this baseline, the parameters \(\) remain fixed during fine-tuning.

Last but not least, as a strong baseline resembling the approaches by Yuksekgonul et al. (2023) and Oikarinen et al. (2023), we train a CBM post hoc (**Post hoc CBM**) using _sequential_ optimisation. Our implementation follows the original methods by Yuksekgonul et al. (2023) and Oikarinen et al. (2023), while adjusting some design choices to make the techniques more readily comparable. The optimisation comprises two steps: (i) \(}=_{}_{(,,y) }[^{e}(q_{}(h_{}( )),)]\), (ii) \(_{}_{(,,y)}[ ^{y}(g_{}(q_{}(h_{}())),y)]\). Additionally, we explore the impact of residual modelling (Yuksekgonul et al., 2023) in Appendix F.9. The architectures of individual modules were kept as similar as possible for a fair comparison across all techniques.

EvaluationTo compare the methods, we conduct interventions and analyse model performance under varying concept subset sizes. We report the areas under the receiver operating characteristic (AUROC) and precision-recall curves (AUPR) (Davis and Goadrich, 2006) since these performance measures provide a well-rounded summary over varying cutoff points and it might be challenging to choose a single cutoff in high-stakes decision areas. We utilise the Brier score (Brier, 1950) to gauge the accuracy of probabilistic predictions and, in addition, evaluate calibration.

## 5 Results

Results on Synthetic DataFigures 3(f) and 4(a) show intervention results obtained across ten independent simulations under the two generative mechanisms (Figure D.1, Appendix D.1) on the synthetic tabular data. We observe that, in principle, the proposed intervention procedure can improve the predictive performance of a black-box neural network. However, in the bottleneck scenario, interventions are considerably more effective in CBMs than in untuned black-box classifiers since the underlying generative process directly matches the CBM's architecture. Models explicitly fine-tuned for intervenability (Fine-tuned, I) significantly improve over the original classifier, achieving intervention curves comparable to those of the CBM.

Importantly, under an _incomplete_ concept set, black-box classifiers are superior to the ante hoc CBM because not all concepts relevant to the target prediction are given. Moreover, fine-tuning for intervenability improves intervention effectiveness while maintaining the performance gap. This experiment suggests the superiority of our method in settings where the concept set does not capture all label-relevant information. Other fine-tuning strategies (Fine-tuned, MT and Fine-tuned, A) are either less effective or harmful, leading to a lower increase in AUROC and AUPR than attained by the untuned black box. Lastly, CBMs trained post hoc perform well in the simple _bottleneck_ scenario, being only slightly less intervenable than Fine-tuned, I. However, for the _incomplete_ setting, interventions hurt the performance of the post hoc CBM. This behaviour may be related to the leakage (Havasi et al., 2022) and is not mitigated by residual modelling explored in Appendix F.9.

To study the influence of the validation set size (\(N_{}\)) on probing and fine-tuning, we perform ablations under the _bottleneck_ scenario (Figure 3). For a fair comparison w.r.t. sample efficiency, here, we train a CBM on the dataset of the _same_ size. While the effectiveness of interventions on Fine-tuned, I is slightly hampered by smaller validation sets, the decrease is moderate. We observe impactful interventions with validation set sizes as small as 0.5% of the original one (Figure 3(a)). Across all settings, our method remains superior to baselines. Importantly, our fine-tuning approach has a

Figure 3: Intervention results w.r.t. target AUROC on the synthetic _bottleneck_ data. We explore the performance under varying validation set sizes (\(N_{}\)). Percentages correspond to the fractions of the _original_ validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (**CBM full**). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.

better sample efficiency than post hoc CBM, which exhibits a considerable dropoff in intervention effectiveness. Likewise, the performance of the CBMs decreases, suggesting their limited utility under smaller dataset sizes. Analogous results w.r.t. AUPR are reported in Appendix F.1.

In Table 1, we report the test-set performance of the models _without_ interventions (under the _bottleneck_ mechanism). For the concept prediction, CBM outperforms black-box models, even after fine-tuning with the MT loss. However, without interventions, all models attain comparable AUROCs and AUPRs at the target prediction. Interestingly, Fine-tuned, I results in better-calibrated probabilistic predictions with lower Brier scores than those made by the original black box and after applying other fine-tuning strategies. As evidenced by Figure F.9(a) (Appendix F.7), fine-tuning has a regularising effect, reducing the false overconfidence observed in neural networks (Guo et al., 2017).

In the supplementary material, we report several additional findings. Figure F.2 (Appendix F.1) contains further ablations for the intervention procedure on the influence of the hyperparameters, intervention strategies, and probe. In addition, Appendix F.2 explores the effect of interventions on the distribution of representations. Lastly, in Appendix F.10, we show that the performance of the CBM on this dataset is not sensitive to the choice of the optimisation procedure (Koh et al., 2020).

Results on AwA2Additionally, we explore the AwA2 dataset in Figure 4(b). This is a simple classification benchmark with class-wide concepts helpful for predicting the target. Hence, CBMs trained ante and post hoc are highly performant and intervenable. Nevertheless, untuned black-box models also benefit from concept-based interventions. In agreement with our findings on the synthetic dataset and in contrast to the other fine-tuning methods, ours enhances the performance of black-box models. Notably, black boxes fine-tuned for intervenability even surpass CBMs. Overall, the simplicity of this dataset leads to the generally high AUROCs and AUPRs across all methods.

To further investigate the impact of hyperparameters on the interventions, we have performed ablation studies on untuned black boxes. These results are reported in Figures F.4(a)-(c), Appendix F.3. In brief, we observe that interventions are effective across all values of the \(\)-parameter from Equation 3 (Figure F.5(a)). Expectedly, higher values yield a steeper increase in AUROC and AUPR. Figure F.4(b) compares two intervention strategies: randomly selecting concepts (random) and prioritising the most uncertain ones (uncertainty) (Shin et al., 2023) to intervene on (Algorithms E.1 and E.2, Appendix E). The strategy has an impact on the performance increase, with the uncertainty-based approach yielding a steeper improvement. Finally, Figure F.4(c) compares linear and nonlinear probes. Here, intervening via a nonlinear function leads to a higher performance increase.

To show the efficacy of our methods across different backbone architectures, in Appendix F.3, we also explore AwA2 with the Inception (Szegedy et al., 2015) backbone (note that Figure 4(b) reports the results on the ResNet-18 (He et al., 2016)). Finally, Table 1 contains evaluation metrics at test time without interventions for target and concept prediction. We observe comparable performance across methods, which are all successful due to the large dataset size and the task simplicity.

Figure 4: Intervention results on the (a) synthetic _incomplete_, (b) AwA2, (c) CIFAR-10, and (d) MIMIC-CXR datasets w.r.t. target AUROC (_top_) and AUPR (_bottom_) across ten seeds.

[MISSING_PAGE_FAIL:9]

common-sense fine-tuning baselines that perform worse than the proposed method, highlighting the need for the explicit maximisation of intervenability.

The utility of our method is highlighted empirically on synthetic tabular and natural image data. We show that, given a _small_ annotated validation set, black-box models trained without explicit concept knowledge are intervenable. Moreover, our fine-tuning method improves the effectiveness of the interventions, with overall better results than alternative techniques. In addition, our approach is effective in scenarios where the concept labels are generated using VLMs. Thus, we can alleviate the need for costly human annotation while maintaining improved intervention effectiveness. Lastly, we apply the techniques in a more realistic setting of chest X-ray classification, where black boxes are not directly intervenable. The proposed fine-tuning procedure alleviates this limitation, while the other post hoc techniques are ineffective or even harmful.

Limitations & Future WorkOur work opens many avenues for future research and improvements. Firstly, the variant of the fine-tuning procedure considered in this paper does not affect the neural network's representations. However, it would be interesting to investigate the more general formulation wherein all model and probe parameters are fine-tuned end-to-end. According to our empirical findings, the choice of intervention strategy, hyperparameters, and probing function can influence the effectiveness of interventions. A deeper experimental investigation of these aspects is warranted. Furthermore, we only considered a single fixed intervention strategy throughout fine-tuning, whereas further improvement could come from learning an optimal strategy alongside fine-tuned weights. Beyond the current setting, we would like to apply our intervenability measure to evaluate and compare other large pretrained discriminative and also generative models.