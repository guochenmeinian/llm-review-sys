# Self-supervised video pretraining yields robust and more human-aligned visual representations

Nikhil Parthasarathy &S. M. Ali Eslami &Joao Carreira &Olivier J. Henaff

Google DeepMind

Current affiliation: NYU Center for Neural Science, work done while interning at DeepMind.

###### Abstract

Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformations than image-, video-, and adversarially-trained ones. Finally, VITO's predictions are strongly aligned with human judgements, surpassing models that were specifically trained for that purpose. Together, these results suggest that video pretraining could be a simple way of learning unified, robust, and human-aligned representations of the visual world.

## 1 Introduction

With the explosion of recent AI breakthroughs, humans now interact with and depend on the outputs of these models at an unprecedented rate. It is therefore increasingly important that these models be aligned with human abilities, judgements, and preferences. In the context of computer vision systems, human alignment can be quantified with accurate generalization across a wide range of tasks , robustness to various input deformations , and consistency with human perceptual judgements . While each of these challenges has been tackled separately, progress along one axis has often come at the expense of the others. For example, gains in robustness  or temporal understanding  have thus far come at the cost of spatial understanding, and scaling the model and dataset size, while improving task-generality and robustness , can be detrimental for their consistency with human perception .

In this work we question this trend, and ask whether improvements to all aspects of human alignment can be made with the appropriate pretraining methodology. Specifically, humans and animals have long been thought to learn from the dynamic evolution of natural scenes  and we hypothesize that artificial visual systems will be more aligned by appropriately leveraging natural video pretraining. In particular, while many current self-supervised methods  learn representations that are invariant to synthetic augmentations that capture important image priors such as scale-, color-, and translation-invariance, these represent a small part of the complex (and signal-rich) changes in pose, viewpoint, and motion that are captured from natural videos. Predicting the evolution of videos is also a natural means of learning intuitive physics and model-based reasoning .

Practically, we develop a self-supervised contrastive framework which learns to locate the most stable and distinctive elements in temporally displaced video frames, and maximizes their invariance. Secondly, we find the statistics of standard video datasets to have a detrimental effect on the quality of the resulting representations, as measured by their performance on canonical scene understanding tasks. We therefore introduce a simple, yet powerful video curation procedure--VideoNet--which aligns their class distribution with that of ImageNet, and which redresses the imbalance between image and video learning. In concert, this paradigm constitues a new methodology for distilling the knowledge of **v**ideos **into** visual representations: VITO.

VITO yields task-general representations that perform well across both spatial and temporal understanding tasks. Particularly, VITO shows large gains over prior video pretraining efforts in scene understanding tasks, while achieving similarly large performance gains over image pretraining on video understanding tasks. Furthermore, VITO significantly outperforms the default ImageNet pretraining as well as adversarial pretraining on image classification tasks subject to natural distribution shifts. Finally, we find that even without a significant expansion in model size, VITO is not only task-general and robust in performance, but also quantitatively captures multiple aspects of human perceptual judgements, surpassing models specifically trained for that purpose.

## 2 Related work

**Learning general visual representations from videos.** Many prior works have considered self-supervised representation learning for capturing spatio-temporal invariances, beginning with methods that leveraged temporal coherence, optical flow, and object tracking . More recently, many successful approaches have leveraged contrastive learning, masked autoencoding, and other self-supervised pretext tasks to learn strong video representations . However, most of these methods employ specialized video architectures and only transfer to video-based tasks such as action recognition and motion segmentation.

Yet natural motion-induced deformations are powerful learning signals that should allow for learning better _image_ representations as well. Indeed, human infants can form complex understanding of objects and shape within months, specifically driven by their observations of how they move . Given this inspiration, some works have demonstrated that self-supervised contrastive learning in videos can lead to aspects of efficient human learning and robust recognition . In computer vision, cycle-consistency  and optical flow  have been used to learn correspondences between temporally ordered image patches. The most similar works to ours utilize video-based contrastive learning  to improve performance on temporal understanding tasks, however they do so at the cost of spatial scene understanding.

**Robustness to distribution shifts.** As standard benchmarks have been progressively saturated , the community has turned to measuring robustness to adversarial attacks , corruptions , and out-of-distribution datasets . We focus on a subset of these benchmarks that are as "natural" as possible, to evaluate generalization with respect to shifts that are most likely to appear in the real world. While there have been many efforts to specifically encourage regularize models for these kinds of robustness , we instead investigate the complementary question of whether image and video pretraining differ in this respect.

**Human-aligned representations.** Most recent progress in achieving more behaviorally-matched representations has been by scaling existing approaches. Indeed, recent examples  show that as data and model sizes grow by orders of magnitude, generality and robustness of representations tend to emerge. Moreover some aspects of human perception such as an increased shape-bias and consistency with human perceptual behavior  can be captured reasonably well by certain large models. However this scaling property tends to be brittle, with some large-scale models displaying significantly worse consistency with human perception . Additionally, more recent work on alignment has found that scaling and architecture are not as important for alignment on specific benchmarks, in comparison to the training dataset and objective function . Therefore, while scaling may continue to lead to task-performance gains, it is unclear whether only scaling image-based pretraining will close the gap with general human behavior. We therefore explore the complementary and potentially synergistic question of whether video pretraining can improve the task-generality, robustness, and behavioral similarity of learned visual representations.

## 3 Method

We pretrain image representations using video datasets, then transfer them to a range of downstream tasks that test image, video, and robust understanding. We adopt the ResNet-50 architecture for our initial exploration, then validate our results with Swin transformers (see Sec. B.4).

### Self-supervised pretraining

Our method for distilling **v**ideos **into** image representations, **VITO**, builds robust visual representations by learning to track stable and distinctive content in videos while they evolve over time.

**Natural video pipeline.** The key to our method is to distill the natural transformations present in videos into image-based representations. Given a video-clip, we sample frames according to a distribution \(\) and further transform each frame with image-based augmentations:

\[^{1}_{1}(_{1})^{2}_{2}( _{2})_{1},_{2}(\{_{t}\}_{t=1, ,T})\] (1)

where the distribution \(\) samples frames uniformly from a video clip of length \(T=2.56s\) and the image transformations \(_{l}\) include random cropping, flipping, blurring, and point-wise color transformations , see appendices A.1 and B.2, and Figure B.3 for an ablation.

We note that video frames (or even uncurated image data) typically differ from the statistics of (object centered) ImageNet images, with more variable viewpoints and a larger field-of-view that can cover multiple objects in complex scenes. As a result, the aggressive random cropping from  (whose smallest crops cover only \(8\%\) of the original image) can result in "positive" pairs with very different semantic content (e.g. entirely different objects). We therefore suggest and empirically validate that larger crop sizes (e.g. increasing the minimum crop size to 40%) are beneficial when learning from real-world video frames (see Figure B.2).

**Multi-scale contrastive attention pooling.** Standard contrastive frameworks use global average pooling of hidden vectors to obtain a single representation of each view. It has been shown that using dense contrastive losses can lead to significant improvements [62; 63; 64; 65], but these methods require establishing correspondences across views. Whereas correspondences can easily be obtained from static images, when temporal deformations are introduced they require some form of object or point tracking . Furthermore, with the larger field-of-view of video frames, correspondence learning becomes an increasingly difficult task. In this work, we propose a more general, adaptive method for learning correspondences at multiple scales. Our method learns what features should be attended to in order to solve the contrastive learning problem across temporally displaced views.

As shown in Figure 1, given a view \(^{l}\) the feature extractor outputs a spatial map of feature vectors \(_{}^{l,s}^{h w c}\) at a given scale \(s\), where different scales correspond to the outputs of different blocks

Figure 1: Learning to attend to related video content. Each augmented frame is encoded by the network \(f\) as a spatial array of hidden vectors. The attention module \(a\) takes as input features from one view and produces a mask that isolates features that are likely to be predictive of the other, temporally-displaced view. The attention-gated features are pooled accordingly, and both the feature extractor and attention module are trained to satisfy the contrastive objective. Subscripts \(\) and \(\) refer to online and target (EMA) networks respectively.

of a ResNet for example. At each scale, we introduce a 2-layer attention MLP \(a_{}^{s}\) which outputs a mask \(^{l,s}=(a_{}(_{}^{l,s}))\) that we use to spatially weight and pool hidden vectors:

\[}_{}^{l,s}=_{i,j}^{l,s}[i,j]\ _{}^{l,s}[i,j]\] (2)

which we we concatenate and transform with the two-layer MLP projector: \(_{}^{l}=g_{}(}_{}^{l})\) where \(}_{}^{l}=[}_{}^{l,s},\ s 1...S]\). In our experiments, we find that for the canonical ResNet-50 architecture, attending over the outputs of the last two ResNet blocks (i.e. \(S=2\)) is optimal given our evaluations. These hidden vectors are then transformed with a standard two-layer MLP \(g_{}\), yielding projections \(_{}^{l}=g_{}(}_{}^{l})\). We enforce invariance across views using the standard InfoNCE loss , encoding targets with slowly-varying _target_ networks \(f_{}\) and \(g_{}\) that are exponential moving averages of the online network 

\[^{ij}(;)=-_{}^{i} _{}^{j})}{(_{}^{i}_{}^{j})+_{ n}(_{}^{i}_{}^{n})}.\] (3)

\(\{_{}^{n}\}_{n}\) are _negative_ features computed from frames from other videos in the batch. The final, multi-view loss is evaluated for all pairs \((;)=_{i j}^{ij}(;)\).

### Addressing dataset domain mismatch

We began investigating the potential for learning general representations from videos, using standard datasets including Kinetics, AudioSet, and YouTube-8M. However, Kinetics is quite small and is limited in scope to human actions. On the other-hand, AudioSet and YouTube-8M are noisy and have very imbalanced class distributions. Additionally, prior work has shown that even self-supervised methods are quite sensitive to the pretraining distribution . Yet over the last decade, it has been shown that ImageNet can be used for learning image representations that transfer well to many downstream tasks. As a result, we hypothesized that collecting a minimally-curated video dataset matched to the rough properties of ImageNet would be beneficial for learning a more general visual model from videos.

To test of this hypothesis, we developed a data curation pipeline--_VideoNet_--to filter online videos such that our training data more closely matches the distribution of ImageNet categories. For each of the 1,000 ImageNet categories, we retrieved 5,000 video clips whose title included the category's name or a synonym. We then filtered these videos by applying an image classifier (pretrained ResNet-50 on ImageNet) to verify that the videos contained the intended object category. We classified the first 100 frames of each video and discarded videos for which the query category was not equal to the ResNet's top-1 prediction for any of the frames. We also discarded videos of less than \(10s\) in length.

While the VideoNet procedure is close in conceptualization to the method used to create the R2V2 dataset proposed by Gordon et al. , it differs in a few ways. First, we utilize full video clips that allow us to uniformly sample frames at any time point rather than the fixed sampling of frames that are \(5s\) apart in R2V2. Second, by using the ImageNet classifier to filter videos, we can reduce mismatch with the ImageNet distribution that can arise from incorrect tagging and noisy labeling of online videos. This is verified by the fact that only 1.18M of the 5M retrieved videos met our filtering criteria. We also note that the use of classification-based filtering is just one method of curation. While we demonstrate in Sec. 4.3, that this curation does provide large benefits in the context of video pre-training compared with existing datasets, there is still great potential to make improvements by utilizing larger target datasets (such as ImageNet-22K) and utilizing alternative curation strategies such as the nearest-neighbor retrieval proposed by  in creating the LVD-142M image dataset.

## 4 Results

Humans are able to solve a range of visual tasks that require complex spatial and temporal reasoning, including generalizing to noisy or out-of-distribution (OOD) scenarios. Therefore, we first benchmark VITO against image and video pretrained models on a variety of tasks to demonstrate sufficient generality and robustness in task performance. We then assess whether VITO not only captures these task-based properties, but also displays strong quantitative alignment with human behavior.

### VITO generalizes across diverse visual tasks

We present in Table 1 the transfer performance of VITO compared to strong supervised and self-supervised baselines on dense scene understanding (semantic segmentation and object detection), video understanding (video segmentation and action recognition), and out-of-distribution (OOD) object recognition. On every benchmark, VITO either outperforms or is competitive with the best baselines _for that specific task_.

**Scene understanding.** We first note that VITO provides large gains over all prior video pretraining methods on scene understanding and robust object recognition. We further validate these comparisons on three additional benchmarks and find that VITO strongly outperforms the prior work across all 5 datasets (PASCAL/ADE20K/COCO/LVIS/IN-1K, see Table B.3). For example, VITO improves over VIVI  by 2-10%, highlighting the importance of data curation and our contrastive formulation. VITO improves over VINCE  by 1-12%, highlighting the importance of fine-grained temporal deformations. Finally, VITO improves even over MMV  by 2-15%, despite their use of large-scale text supervision, highlighting the relevance of video-only learning.

Compared with the best supervised and self-supervised image-pretraining methods VITO achieves competitive performance on these same benchmarks (Table 1 and Table B.3). To our knowledge, VITO is the first video pretrained method to close the gap with ImageNet pretraining on large-scale scene understanding benchmarks such as these.

**Video understanding.** We next ask whether this increased spatial understanding come at the cost of traditional benefits of video pretraining on video tasks. We find that this is not the case, evaluating on DAVIS segmentation and UCF-101 action recognition. On DAVIS, which tests the ability to segment an object over its dynamic temporal evolution, VITO features capture fine-grained temporal deformations of objects far better than ImageNet pretraining methods, as well as the best video pretraining methods (See Table B.4 for additional comparisons). On UCF-101, which tests the ability to classify global spatio-temporal features, we find that a simple average pooling of VITO frame representations again outperforms all image pretraining and prior frame-based video pretraining significantly. VITO even outperforms a number of recent methods that use specialized video architectures (See Table B.5). While VITO under-performs relative to the best video models, we note that these methods either cannot be tested or under-perform on spatial understanding. Additionally, as shown in Table B.5 and Sec. A.5, simple learned temporal pooling strategies on top of VITO representations further close the gap with the best video architectures.

  &  &  &  \\  &  & ADE20K & COCO & DAVIS & UCF101 & IN-A & IN-Vid \\  & & (mIoU) & (mAP) & (\(\) & (top-1) & (top-1) & (pm0/ \\  & & & mean) & & pm10) \\  Random & - & 27.9 & 39.0 & - & - & - & - \\  \\ Supervised & ImageNet & 33.5 & 44.2 & 66.1 & 83.4 & 2.2 & 67.7/52.4 \\ BYOL  & ImageNet & 38.8 & 43.7 & 66.6 & 85.6 & - & - \\ MoCLR  & ImageNet & 39.2 & 43.9 & 65.5 & 85.5 & 3.7 & 64.7/50.0 \\ DINO  & ImageNet & 39.0 & **44.3** & 65.3 & 85.4 & 5.0 & 65.2/52.0 \\  \\ Stylized-IN  & SIN+IN & - & - & - & 83.3 & 2.0 & 68.4/51.7 \\ L2-Robust  & ImageNet & - & - & - & 83.7 & 2.1 & 65.2/51.6 \\  \\ VIVI  & YT8M & 34.2 & 41.3 & - & - & 0.5 & 57.9/36.5 \\ MMV-VA  & AS + HT & 32.5 & 41.3 & - & - & - & - \\ VINCE  & R2V2 & 35.7 & 42.4 & 66.1 & - & - & - \\ VFS  & K400 & 31.4 & 41.6 & 67.8 & - & - & - \\ CycleCon  & R2V2 & 35.6 & 42.8 & - & 82.8 & 0.4 & 50.4/30.1 \\ VITO & VideoNet & **39.4** & 44.0 & **68.2** & **87.4** & **5.4** & **70.6/57.2** \\ 

Table 1: VITO representations generalize to a variety of tasks in both image and video modalities, surpassing models specialized for each task. For external models, we finetune publicly available checkpoints.

**Object recognition under distribution shifts.** A key feature of human perception is being able to generalize under distribution shifts away from the training data. The standard ImageNet benchmark does not test this, as the validation set is drawn from a similar distribution as the train set. We hypothesize that while ImageNet pretraining can lead to strong performance in-distribution, pretraining on videos can endow models with better generalization capabilities.

We thus evaluate on a suite of benchmarks designed to test distributional robustness . To test recognition under _natural_ shifts we evaluate on the ImageNet-Vid-Robust and ImageNet-A benchmarks (Table 1). ImageNet-Vid-Robust tests generalization of image classifiers to natural deformations over time. The anchor frame is identified as the cleanest frame capturing the object, and as time evolves, recognition becomes more difficult. We see that VITO surpasses all models on the anchor frame accuracy (+3% relative to supervised ImageNet training for _pm0_), but more importantly, the accuracy gap grows for the largest temporal displacement (+5% for _pm10_). ImageNet-A on the other hand contains ImageNet-like images that systematically fool ImageNet classifiers (i.e. 'natural adversarial examples'). On this dataset, while performance is very low across all models, VITO again shows more robustness. For additional comparison, we also evaluate two models (SIN-IN and L2-Robust (\(=1\))) which are models trained specifically for robustness (to shape-bias and adversarial attacks respectively). While SIN-IN yields modest improvements on ImageNet-Vid-Robust, neither method approaches the gains in robustness afforded by VITO.

Finally, we evaluate robustness on the ImageNet-3DCC dataset, which contains naturalistic and synthetic corruptions applied to clean images from the ImageNet validation set . To test robustness to conditions of real-world deployment, we choose the subset of corruptions designed with 3D models to be consistent with scene geometry. These include things like fog, near/far focus, motion blur, etc. and have 5 different severity levels per image. In Fig. 4.1 (Left), we plot the difference in accuracy between clean (ImageNet val) and corrupted accuracy across severity levels. This "\(\)-accuracy" provides a measure of how robust a model is as distortion levels increase. We see that across all corruption strengths, VITO shows increased robustness compared to supervised and self-supervised (MoCLR, DINO) ImageNet pre-trained models. The robustness gap grows significantly at the highest corruption levels, demonstrating the generality of this effect (+10% relative to supervised ImageNet training). While the robust training methods (SIN+1N1K and L2-Robust) outperform supervised and MoCLR models, VITO remains significantly more robust, demonstrating that learning from video deformations may endow a more general form of robustness than that provided by either style-transfer or adversarial images.

To quantify further the specific impact of individual components of VITO on robust recognition, we show the same plot (Fig. 4.1 (Right)), now with the ablations described in Sec 4.3. We find that all components of our method and architecture are necessary for best robustness, but in particular there is a striking split between models trained with only spatial deformations (VITO (T=0), MoCLR ImageNet, MoCLR VideoNet) and those trained with video deformations. We find that the models that learn only from image-level spatial deformations suffer significantly in robustness against all of the models that learn from video deformations.

Figure 2: ImageNet-3DCC validation accuracy for different levels of corruption severity. (Left): Comparisons with prior work including methods specifically designed to enhance robustness (SIN+IN1K and L2-Robust). (Right): comparisons with ablations of the VITO method/model.

### Measuring explicit human-alignment

Given that VITO representations display strong generalization across many tasks and robustness to distribution shifts, two signatures of human perceptual intelligence, we now directly ask whether they align with human perceptual representations.

**Visual saliency via contrastive attention.** We start by comparing VITO's learned attention masks to human saliency data from the ClickMe dataset , as well as saliency maps obtained from a collection of ResNet-50 models. For the supervised and MoCLR ResNets we use standard gradient-based saliency as in . Since our model contains two attention maps at two scales of the ResNet, we upsample both maps to the image size and simply average them to obtain a single map. We compare our attention maps additionally to those obtained from the modified CLIP ResNet , which also utilizes attention-pooling in the final layer but is trained for image-language alignment (the canonical approach for training state-of-the-art visual language models). Because the CLIP pooling uses multi-head attention, we upsample these maps and average them across heads. Finally, we also compare to the gradient-based saliency maps from a "harmonized" model explicitly trained to align with human saliency ().

Qualitatively, VITO saliency maps appear significantly more aligned with human perception maps than the supervised and CLIP ResNets (Figure 3). Surprisingly, VITO appears more aligned than the Harmonized saliency maps across the 4 examples. Quantitatively (using Spearman rank correlation) VITO outperforms the supervised, MoCLR, and CLIP models by a large margin, and even surpasses the Harmonized model which has been specifically trained for this purpose (Table 2).

This result suggests that as opposed to image-based objectives or image-language alignment, human perception of feature importance across the visual scene can be better explained as a consequence of learning what to attend to in the context of self-supervised video-based learning. We hypothesize that these attention masks could underlie the formation of high-level concepts via "semantic binding", which we investigate in Figure B.1 and Section B.1.

**Human error consistency in shape-biased tasks**. Based on this result relating to object saliency, we hypothesize that VITO may be capturing global object shape features better than traditional deep networks which have been shown to heavily rely on textural cues for classification .

To evaluate this quantitatively, we used a subset of the dataset proposed in  to test both the accuracy and consistency with human judgments of model classifications of stimuli that require

 Method &  Trained for \\ alignment \\  & 
 Human \\ Alignment \\  \\  MoCLR  & ✗ & 21.4 \\ Supervised & ✗ & 34.4 \\ CLIP  & ✗ & 41.8 \\ Harmonized  & ✓ & 45.5 \\ VITO & ✗ & **47.7** \\ 

Table 2: Quantitative comparison between gradient-based saliency maps (from Supervised, MoCLR, CLIP-RN50 (attention-map), and Harmonized networks), VITO attention weights, with human saliency maps using a correlation based alignment score from 

Figure 3: Example human saliency maps from the ClickMe dataset  and ResNet-50 models. Gradient-based saliency is shown for Supervised and Harmonized . Attention maps are shown for CLIP and VITO model. We use multi-head attention pool weights for CLIP and average of weights from last 2 attention pooling scales in VITO.

shape-cues for effective discrimination (Table 3). Specifically, these stimuli are categorized into 4 groups: edge drawings, cue-conflict / stylized (mixing of shapes with contradictory textures through style-transfer), variable low-pass filtering (to remove high-frequency local content), uniform noise (corrupts local texture features). Based on the original methodology proposed in , we report the accuracy difference (from human accuracy), the raw consistency with human judgments, and coiled error consistency (method from ).

We compare to supervised and MoCLR ResNets, the robust training methods cited earlier, as well as CLIP . We also compare to various video pre-training methods cited earlier and another (R3M ), which has specifically shown to have human- and neurally-aligned representations of dynamic, object-centric scenes . For all networks, we train linear classifiers on the ImageNet validation set and evaluate on the modified shape-biased stimuli. Compared with all other comparable image pretrained models, VITO achieves stronger robustness to shape-biasing transformations (lower accuracy difference relative to original images). Furthermore, VITO makes predictions more consistent with human judgements in terms of per-trial classification behavior. This is particularly surprising as VITO even outperforms the adversarially-trained robust model without requiring any explicit robust training procedure. Moreover, this improvement is not captured by prior video pretraining efforts (which are in fact far worse than the image pretraining methods). The R3M model, in particular, performs surprisingly poorly. Because the images used to collect the human judgments are modified versions of those from the ImageNet validation set, we hypothesize that this performance can be attributed to the poor transfer of the Ego4D datasets to the diverse classes present in ImageNet (contrarily to VideoNet). Indeed, the R3M model only achieves 13% accuracy on the clean ImageNet validation set (see Table B.3). Finally, we note that VITO does underperform CLIP on this benchmark; however, this comparison is not truly fair as CLIP is trained with explicit human supervision via large-scale image-language mappings. In fact, we believe that our method can be augmented with similar language supervision to improve human alignment even further.

In summary, VITO captures aspects of how humans process shape-information that cannot be captured by other strong visual models. Understanding more about this effect and what aspects of learning from videos lead to this remain interesting opportunities for future work.

### Ablations

To understand more about how the components of VITO training contribute to its performance, we vary the different aspects of our paradigm in isolation: our method for data curation (VideoNet), multi-scale attention pooling, and details of the input data (spatial crop size and the temporal sampling scheme). We explore some ablations in detail on an example benchmark (PASCAL segmentation), but also evaluate ablations across many of the benchmarks used in this work. Finally, we provide a brief exploration demonstrating that our method scales well to larger architectures.

   Method & accuracy diff. \(\) & obs. consistency \(\) & coiled error consistency \(\) \\    \\ DINO  & 0.236 & 0.504 & 0.291 \\ Supervised & 0.215 & 0.511 & 0.329 \\ SIN+IN1K  & 0.203 & 0.527 & 0.330 \\ MoCLR  & 0.190 & 0.536 & 0.335 \\ L2-Robust  & 0.178 & 0.544 & 0.389 \\ CLIP  & 0.108 & 0.612 & 0.482 \\   \\ R3M  & 0.392 & 0.359 & 0.054 \\ CycleCon  & 0.237 & 0.484 & 0.258 \\ VINCE  & 0.210 & 0.501 & 0.269 \\ VITO & **0.157** & **0.564** & **0.422** \\   

Table 3: Accuracy difference and consistency with human judgments on stimuli that are biased to requiring global-shape understanding (instead of texture) for recognition/discrimination. VITO surpasses all comparable trained models (both image and video pretraining) in all benchmarks, including those that are trained specifically to be robust (SIN+IN1K, and L2-robust). We underperform the CLIP model; however, we note that CLIP is trained with an order of magnitude more images (400M) and explicit human-language supervision.

**Effect of pretraining data.** To demonstrate the effect of the pretraining data distribution on transfer performance, we pretrain a baseline MoCLR model (using 2 views) on a variety of image and video datasets, where we initially treat video datasets as collections of individual frames. We train each model for 300 ImageNet-equivalent epochs, referred to hereafter as "epochs" (i.e. 1 epoch = learning from 1.28M examples, irrespective of the dataset), such that each model benefits from the same amount of computation. Figure 4 (left) shows their transfer performance on PASCAL semantic segmentation. As expected, ImageNet pretraining works very well, but pretraining on standard video datasets results in a substantial drop in performance (e.g. \(-6.8\%\) or \(-5\%\) mIoU from pretraining on Kinetics700 or AudioSet). This performance gap between video and image pretraining can be attributed to a combination of increased complexity and field-of-view of video frames and domain mismatch between the dataset categories (Figure 4, right). Consistent with this, training on JFT , an uncurated dataset with a heavy-tailed class distribution, also results in a loss in performance. Notably, this is despite the much larger size of JFT (300M images). We find that applying the same baseline pretraining to frames from our curated video dataset performs better than existing large-scale video datasets like Audioset (\(+1.6\%\) mIoU), but still underperforms image pretraining on JFT and ImageNet (Figure 4). This demonstrates the importance of aligning the distribution of video frames with that of common image datasets. We therefore use VideoNet as our primary pretraining dataset for the rest of the study. In Sec B.3 we disentangle the power of our method and dataset by confirming that each independently have strong effects: MoCLR trained on VideoNet, and VITO trained on standard datasets (Audioset or Y78M) also outperform all prior work (including models trained on much larger image datasets like JFT-300M).

**Multi-scale attention pooling.** We decompose the proposed multi-scale contrastive attention pooling to isolate the effects of multi-scale learning from those of attention pooling (Figure B.2, right). While we find only modest gains from adding attention pooling to a single-scale version of the model (\(+0.2\%\) mIoU), we find that the 2-scale model (without attention pooling) improves over the single scale model more robustly (\(+0.6\%\) mIoU). Interestingly, we find that the combination of the 2-scale model with attention pooling has a synergistic effect (\(+1\%\) mIoU over the single-scale attention model), highlighting the importance of handling the variability in scales present in natural videos.

**Spatial and temporal augmentation parameters.** We first validate in Figure B.2 (left) our hypothesis that increasing the minimum crop-scale in the random-resized crop operation during training leads to models that generalize better to fine-grained tasks like semantic segmentation. Specifically, we find that a minimum crop scale of 0.4 (as opposed to the traditional 0.08) results in the best transfer performance (\(+1.7\%\) mIoU). Note that this conclusion differs slightly from that of  who find more aggressive cropping to be beneficial for action recognition.

Next, to study the effect of different temporal sampling schemes, for each training example, we sample 3 views using marginal sampling of each frame from the video clip of length \(T=2.56\) seconds. This length determines the distribution of time differences between any pair of frames, and thus the time-scale over which the contrastive model learns invariances. We verify our choice by varying the total length of clips. While going to longer time-scales \(T=3.2s\) does not hurt performance much, we find a significant improvement over using shorter clips (e.g. \(T=1.28s\)

Figure 4: Impact of pretraining data’s spatial content on representation quality. Left: transfer performance of models pretrained on single frames from image datasets (grey bars) or individual videos (blue bars). Right: example frames from different video and image datasets.

\(+1.0\%\) mIoU; Figure 2, center). This suggests that invariance to the rich temporal deformations present in video clips is indeed a beneficial criterion for learning fine-grained spatial representations.

**Comprehensive ablation summary.** In Table 4, we extend the above ablation studies to a more comprehensive benchmark set. In addition to the PASCAL segmentation task, we evaluate the key ablated models on video understanding (UCF101), OOD recognition (IN-A/IN-Vid) and human alignment on the shape-bias tasks specified in Sec 4.2. We confirm that all of the major methodological components (VideoNet dataset, multi-scale attention pooling, and using temporal deformations) work in concert, and are required for best performance across all tasks. Notably, we see a particularly striking dichotomy between models trained with and without temporal deformations on human error-consistency. Specifically, models trained without temporal deformations (MoCLR and VITO (T=0)) have a significant drop in human error-consistency relative to all other models trained with temporal deformations, highlighting the importance of learning these kinds of invariances.

**Scaling model architectures.** We briefly demonstrate that VITO scales to more recent larger architectures. Specifically, we show preliminary results that VITO achieves highly competitive performance on four scene understanding benchmarks using the Swin-S transformer architecture . In Sec. B.4, we show that performance improves dramatically over the ResNet-50 architecture and is competitive with a strong, specialized ImageNet pretrained baseline for fine-grained scene understanding (DetCon ).

## 5 Discussion

**Summary.** We propose VITO, a simple method for distilling videos into visual representations. The key features of our method include improved dataset curation, adapting augmentation pipelines to appropriately handle video frames, and using attention-guided contrastive learning. With these components, VITO surpasses both prior video pretraining in spatial understanding, and image pretraining on temporal understanding and robustness. In addition to these hallmarks of human perception, VITO explicitly aligns with aspects of human saliency and image recognition behavior that are not captured by other high-performance representation learning techniques. In sum, despite the many successes in video representation learning, our results suggest that there is a great untapped potential in video pretraining as a paradigm for learning general, human-aligned visual representations.

**Limitations and Future Work.** We believe this work can be a foundation for future video pretraining efforts, as our approach is powerful, yet simple and extensible. However, we recognize that this demonstration is mostly limited to a single contrastive learning framework and ResNet-50 architecture. We leave for future work, the validation and exploration of similar analyses with larger models and other self-supervised training objectives (such as MAEs and self-distillations methods like DINO). Additionally, while we have shown the benefits of a surprisingly simple attention module for learning correspondences in video data, there are more powerful attentional architectures we can leverage along with scaling dataset size as in . We have started these experiments with our exploration of Swin transformer architectures.

 Pretraining & Dataset & PASCAL & UCF101 & IN-A & IN-Vid & Human \\  & & (mIoU) & (top-1) & (top-1) & (pm0/pm10) & error consistency \\  MoCLR & VideoNet & 72.8 & 83.0 & 2.3 & 55.5/40.5 & 0.224 \\ VITO 1scale (w/o attn) & VideoNet & 75.2 & 85.5 & 3.9 & 67.3/55.5 & 0.359 \\ VITO 1scale (attn) & VideoNet & 75.4 & 85.7 & 3.5 & 65.6/52.9 & 0.368 \\ VITO 2scale (w/o attn) & VideoNet & 75.8 & 86.2 & 4.2 & 67.4/54.9 & 0.390 \\ VITO (T=0) & VideoNet & 74.8 & 83.2 & 3.9 & 63.9/49.5 & 0.323 \\ VITO & AudioSet & 73.8 & 84.8 & 3.4 & 55.7/42.4 & 0.401 \\ VITO & VideoNet & **76.3** & **87.4** & **5.4** & **70.6/57.2** & **0.422** \\ 

Table 4: Summary of ablation models on key evaluations covering image understanding, video understanding, and human alignment on ood object recognition. In summary, it is clear that all components (pretraining data, temporal deformations, and the multi-scale attention pooling) are required for best performance across all tasks.