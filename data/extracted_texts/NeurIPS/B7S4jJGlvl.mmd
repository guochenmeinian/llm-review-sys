# Symbolic Regression with a Learned Concept Library

Arya Graylei

UT Austin, Foundry Technologies

&Atharva Sehgal1

UT Austin

&Omar Costilla-Reyes

MIT

&Miles Cranmer

University of Cambridge

&Swarat Chaudhuri

UT Austin

Equal contribution; Correspondence to atharvas@utexas.edu

###### Abstract

We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve abstract concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LaSR can be used to discover a new and powerful scaling law for LLMs.

## 1 Introduction

Symbolic regression (SR)  is the task of finding succinct programmatic hypotheses -- written in a flexible, domain-specific programming language -- that best explain a dataset. Initially proposed in the 1970s, SR has recently emerged as a prominent approach to automated scientific discovery, with applications in domains from astrophysics  to chemistry  to medicine .

Computational complexity is a fundamental challenge in SR, as the space of hypotheses that an SR algorithm must search is discrete and exponential. Previous work has approached this challenge using methods like genetic programming , neural-guided search , deep reinforcement learning  and hybrid algorithms . However, new tools to enhance the scalability of SR remain a critical need for applications in SR and scientific discovery.

In this paper, we show that _abstraction_ and _knowledge-directed discovery_ can be powerful principles in building such scaling tools in SR. State-of-the-art genetic algorithms for SR  evolve pools of candidate hypotheses using random mutation and crossover operations. By contrast, a human scientist does not just randomly mutate their explanations of data. Instead, they synthesize background knowledge and empirical observations into abstract concepts, then use these concepts to derive new explanations. We show that zero-shot queries to large language models (LLMs) can be used to implement such a discovery process on top of a standard SR algorithm.

Concretely, we present a new method for symbolic regression, called LaSR, that discovers a library of abstract, reusable and interpretable textual _concepts_ and uses it to accelerate SR. LaSR alternates between three phases: (i) _concept-directed hypothesis evolution_, where standard genetic operationsover hypotheses are interleaved with LLM-guided mutation and crossover operations conditioned on known library concepts; (ii) the LLM-based _abstraction_ of patterns in known high-performing hypotheses into new concepts; and (iii) the LLM-directed _evolution of concepts_ into more succinct and general forms. Together, these three steps form an open-ended alternating maximization loop that combines evolutionary exploration with the exploitation of the LLM's background knowledge and in-context learning ability.

We experimentally compare LaSR on Feynman Equations  -- a popular SR benchmark in which the goal is to discover 100 equations from the Feynman Lectures in Physics -- against several state-of-the-art genetic and deep learning approaches. LaSR can discover 66 of the 100 target equations, while the best existing approach can solve 59. To address the concern that LaSR's performance could be attributed to test set leakage, we compare LaSR with a state-of-the-art genetic approach on a suite of synthetic benchmarks. We show that LaSR substantially outperforms the baseline. Finally, we support the contribution of LaSR to the research community by evaluating the methodology in a case study where the goal is to find new scaling laws for LLMs.

In summary, the contributions of this paper are as follows:

* We pose the problem of discovering an open-ended, reusable concept library that can accelerate solutions to downstream SR tasks.
* We present LaSR, a method for combining zero-shot LLM queries and standard evolutionary operations to simultaneously induce a concept library and high-performing hypotheses. LaSR's strategy of using LLMs to accelerate evolutionary algorithms may have future applications in settings beyond SR.
* We offer promising experimental results, including a demonstration that LaSR outperforms state-of-the-art algorithms in standard SR tasks and synthetic domains, as well as a case study that uses LaSR to discover a novel LLM scaling law.

Figure 1: An overview of LaSR. LaSR iteratively refines a library of interpretable textual concepts which are used to bias the search for hypotheses for scientific discovery tasks. This involves three distinct phases: (**Top**) finding optimal hypotheses within a concept-directed hypothesis evolution, (**Right**) leveraging the optimal hypotheses to find new concept abstractions, and (**Left**) iterating on learned concepts to discover new concepts to accelerate hypothesis evolution. LaSR introduces an orthogonal direction of improvement over current symbolic regression algorithms  (in gray).

Problem Formulation

Symbolic Regression.We formulate symbolic regression (SR) as a program synthesis  problem. The inputs to this problem include a language \(\) of programmatic hypotheses and a dataset \(:=\{(_{i},_{i})\}_{i=1}^{N}\) of input-output examples. The syntax of \(\) is described by a _context-free grammar_. The grammar allows each hypothesis \(\) to be represented using a set of mathematical operators (e.g., addition, multiplication, trigonometric functions) that facilitate the composition of simpler hypotheses into more complex ones. We abstractly define the _fitness_ of a hypothesis \(\) as the likelihood \(p_{}()\) that it generates \(\).

In order to prevent finding non-useful solutions, we impose a _prior probability distribution_\(p_{}()\) over hypotheses \(\) that penalizes syntactically complex hypotheses. We now pose SR as the task of finding a hypothesis \(^{}\) that maximizes the fitness while minimizing syntactic complexity. The problem can be expressed as a maximum a posteriori (MAP) estimation problem :

\[^{}=_{}p_{}(|)=_{} }(|)}_{} }()}_{}\] (1)

Recent work leverages large language models (LLMs) for program synthesis [8; 19; 32]. Large language models (LLMs) approach program synthesis as a token prediction problem, directly approximating the likelihood of programs by training on internet-scale datasets. That is,

\[p_{}(|) p_{}( ,()),\] (2)

where \(\) and \(\) are, respectively, textual representations of \(\) and a specification of the syntax of \(\), and the _task description_\(()\) is a few-shot serialization of a subset of the examples in \(\).

Symbolic Regression with Latent Concept Libraries.Classical symbolic regression typically assumes no prior knowledge or intuition about the problem. In contrast, human scientific discovery often leverages empirical patterns  and intuitions derived from previously observed data. For example, recognizing a 'power law relationship between variables' has led to the formulation of fundamental empirical laws across various fields, such as the Arrhenius equation in Chemistry, the Rydberg formula in Physics, Zipf's law in Linguistics, and Moore's law in Computer Science.

We model such empirical patterns as natural-language _concepts_ drawn from a latent _concept library_\(\). We frame the relationship between the concept library and programs as a Hierarchical Bayesian model consisting of: (i) a _prior_\(p()\) representing the natural distribution over concept libraries; (ii) a model \(p_{}()\) that quantifies the likelihood of various hypotheses for a given concept library \(\); and (iii) the previously mentioned fitness function \(p_{}()\) for programs \(\). We assume that the distributions \(p()\) and \(p_{}()\) can be approximated using LLMs. That is, we can prompt an LLM to generate interesting concepts, and we can prompt an LLM with a set of concepts to generate token-sequence representations of hypotheses that adhere to the concepts. Now we state the problem of _symbolic regression with latent concept learning_ as one of simultaneously inducing an optimal concept library and an optimal programmatic hypothesis:

\[_{,}p(,|)=_{, }|)}_{} )}_{})}_ {}\] (3)

## 3 Method

LaSR performs a two-stage evolution over natural-language concepts and programmatic hypotheses. The two stages follow an alternating maximization strategy shown in Figure 1: (1) _Hypothesis evolution_: We fix the set of concepts and focus on maximizing the hypotheses' fitness to the dataset, and (2) _Concept abstraction and evolution_: We leverage the best hypotheses found to induce a new library of concepts.

In the rest of this section, we first describe PySR, the SR algorithm  that LaSR extends. Next, we show how to modify this algorithm into one guided by natural-language concepts. Finally, we show how these concepts can be naturally extracted and evolved into new concepts. The full LaSR algorithm is presented in Algorithm 1 and visualized in Figure 2. LaSR is built in Julia with an additional Python interface2 and uses an open-source, optimized framework for LLM inference .

Base Algorithm: PySR.LaSR builds on PySR , a scalable, parallelizable genetic search algorithm for SR. The search in PySR maintains multiple populations \(\{_{1},,_{k}\}\) of hypotheses, with each hypothesis represented as an expression tree. In its _initialization_ step, captured by a procedure InitializePopulations, PySR creates a new expression at random to insert into a population. After running this step, PySR runs a genetic search, encapsulated in a procedure SRCycle, which evolve these populations in parallel, simplifies and optimizes the constants of the resulting hypotheses, and then migrates top-performing hypotheses between populations.

Like other evolutionary algorithms, the search in PySR uses symbolic _mutation_ and _crossover_ operations. The mutation step is broken into many categories, each with distinct weighting, to either mutate a constant, mutate an operator, add a node (append, prepend, insert), delete a subtree of an expression tree, simplify the tree, initialize a new tree, or do nothing. One of these operations is randomly selected at each call to a mutation request, and each operation executes itself at random but within user-provided constraints. For example, deleting a subtree is done by choosing a random node to replace with a randomly-generated leaf node such as a feature or constant. The crossover step involves swapping random subtrees of two expressions in a population.

LLM-guided Hypothesis Evolution.LaSR speeds up PySR by injecting natural language priors into its search procedure. To do this, we modify the InitializePopulations procedure to use an LLM-augmented initialization operation, and the SRCycle routine to use LLM-augmented versions of its symbolic mutation and crossover operations. The altered procedures are named Llmit, LlmMutate, and LlmCrossover, respectively. These operations do not _replace_ their standard genetic counterparts. Instead, we introduce a hyperparameter \(p\) that, with a fixed probability, substitutes the standard genetic operation with the LLM-based operation. This enables "doping" each population with a program that respects the language priors, while ensuring that we do not bottleneck the local exploration of the search space.

The LLM-guided operations follow the same base format: they sample multiple concepts from the concept library, concatenate these concepts with the task-specific variable names and language operations, and append a specialized prompt for each task. We employ zero-shot prompts (see Appendix A.2 for more details) to avoid sampling biases. In further detail:

* LlmInit: The LlmInit function takes an initial set of concepts and uses them to initialize the populations for the evolutionary search step. The initial set of concepts can either be instantiated from an optional set of user-provided "hints" or generated by the LLM.
* LlmMutate: For mutation within a population, we sample a set of \(l\) concepts from the concept library \(C\), and construct a prompt that uses this set of concepts to mutate an expression \(_{i}\) into \(_{j}\). The prompt to the LLM takes inspiration from the standard genetic mutation operation, and asks it to mutate the expression given the concepts sampled from the library.

Figure 2: A single step of LaSR. LaSR induces multiple hypothesis populations that are evolved using a scalable evolutionary algorithm. Concept guidance is provided by randomly replacing symbolic operations with concept-directed LLM operations with probability \(p\). After each iteration, the top-performing programs are summarized into natural language concepts, which are evolved to form new concepts that are sampled to guide the search in the next iteration.

* LlmCrossover: The LlmCrossover function also samples a set of \(l\) concepts from the concept library along with two hypotheses \(_{i}\) and \(_{j}\) to construct a new expression \(_{k}\), which reuses sub-expression trees from the two hypotheses while respecting the sampled concepts. Our implementation is inspired by prior work  -- see Figure 4.

Concept Abstraction.After each iteration of symbolic regression, we use a function ExtractParetoFrontier to collect: (i) the hypotheses, across all populations, that are Pareto-optimal with respect to the criteria of syntactic simplicity and dataset loss; (ii) the hypotheses with the worst loss across all populations. The resulting set of hypotheses \(=\{_{1}^{*},_{a}^{*}_{1}^{-},_{b}^{-}\}\) captures the trends that were most helpful and most detrimental to performance during hypothesis search. Now we use the ConceptAbstraction function, which uses a zero-shot prompt to extract a natural language concept \(c^{*}\) that summarizes the positive trends while eschewing negative trends. This concept is subsequently added to the concept library. The prompt for the function is presented in Figure 5.

Concept Evolution.Each concept in \(\) represents trends that were useful at a previous state in the search process. After adding new concepts into the library, we use a function ConceptEvolution to evolve the library to include new ideas that logically follow from the ideas in the current library. The implementation of this function follows that of the LlmCrossover operation in that we are using multiple concepts as a reference to generate new ones, with the key distinction that, unlike in the LlmCrossover operation, the fitness of each generated concept here is difficult to quantify. Thus, we include all the generated responses in the concept library. While these concepts may sometimes be inaccurate, they increase the evolutionary algorithm's exploration ability.

## 4 Experiments

We demonstrate the effectiveness of LaSR on multiple tasks integral to scientific discovery. First, we evaluate LaSR's performance on the Feynman Equation dataset, a widely adopted scientific discovery benchmark, under a variety of ablations and additional priors. Second, we measure the effect of data leakage by evaluating LaSR's performance on a procedurally generated synthetic dataset of challenging equations. Finally, we conduct a case study using LASR to discover LLM scaling laws with data from the BIG-Bench evaluation suite .

LaSR's main focus is to serve as a practical toolkit for scientists. Therefore, our evaluation primarily targets slightly noisy environments, using exact solution rate to gauge performance rather than statistical similarity measures like correlation \(R^{2}\), which are less relevant to scientific discovery applications. Additional experiments characterizing the practical behavior of LaSR are in Appendix A.5. Information regarding compute usage is in Appendix A.3.1.

### Comparison against baselines in the Feynman Equation Dataset

**Dataset**: The Feynman Equations dataset is a well established benchmark for Symbolic Regression algorithms . This dataset consists of 100 physics equations extracted from the Feynman lectures on Physics. We compare against performance reported on SRBench : a continuously updated public benchmark of SR methods on many datasets. Specifically, we compare against GPlearn, AFP, AFP-FE, DSR, uDSR, PySR, and AI Feynman [42; 47; 49; 28; 38]. Within this subset PySR represents an ablation of our model without the LLM genetic operations and the concept evolution (Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental errors common in scientific discovery domains. More details are presented in Appendix A.4.1.

**Setup**: We instantiate LASR using gpt-3.5-turbo-0125  as the backbone LLM and calling it with \(p=0.01\) for \(40\) iterations, and compare our results with PySR which uses the same default hyperparameters. For the other baselines, we use the numbers reported in SRBench with one exception being uDSR , for which we couldn't find any benchmarking numbers. For this method, we derive the exact solve rate from .

**Results**: We showcase results in Table 1. We draw three observations from this experiment. First, LASR achieves a higher exact solve rate than all other baselines. Second, both PySR and LASR outperform the other baselines by a wide margin, indicating that scalable and efficient synthesis is imperative to practical scientific discovery algorithms. Finally, and most notably, a subset of the equations LASR finds could not be derived with any of the previous methods.

### Cascading Experiments

LaSR's performance is inherently bottlenecked by the reasoning capabilities of the backbone LLMs and the frequency of their invocation in each iteration. To evaluate the effect of the backbone LLM on LASR's performance, we instantiate a model cascade over two of LASR's hyperparameters: the backbone model (llama3-8b, gpt-3.5-turbo-0125) and the probability \(p\) with which we call that model in the evolution step (\(p\) = [1%, 5%, 10%]).

**Setup**: Our cascade operates as a tournament. We start LASR with the configuration that provides the least language guidance (llama3-8b at \(p=1\%\)) and progressively increase the value of \(p\) and then the backbone model. Each subsequent model is only evaluated on the problems that the previous model could not solve. We compare this against PySR's performance on the Feynman equation dataset. To ensure a fair comparison, we cascade PySR using the same procedure but find it does not solve any additional equations. For this experiment, we tag each equation with a qualitative rating comparing the equation to the ground truth form (Exact Solve, Almost Solve, Close, and Not Close). An in-depth discussion on this metric is presented in Section A.7.

**Results**: Our results are presented in 2. We draw two key observations from these results. First, LASR outperforms PySR even with minimal concept guidance (llama3-8b at \(p=1\%\)). Second,

    &  &  & LASR (GPT-3.5) \\   & & \(p=1\%\) & \(p=5\%\) & \(p=10\%\) & \(p=1\%\) \\  Exact Solve & 59/100 & 67/100 & 69/100 & 71/100 & 72/100 \\ Almost Solve & 7/100 & 5/100 & 6/100 & 2/100 & 3/100 \\ Close & 16/100 & 9/100 & 12/100 & 12/100 & 10/100 \\ Not Close & 18/100 & 19/100 & 13/100 & 16/100 & 15/100 \\   

Table 2: Evaluation results on Feynman dataset by cascading LASRâ€™s LLM backbone (llama3-8b, gpt-3.5-turbo) and changing the probability of calling the model (\(p\) = [0.01, 0.05, 0.10]) in the order of increasing concept guidance. LASR outperforms PySR even with minimal concept guidance using an open-source LLM.

    &  &  &  & uDSR &  & PySR &  \\    & & & & & & & \\ 
20/100 & 24/100 & 26/100 & 23/100 & 40/100 & 38/100 & 59/100 & **72/100** \\   

Table 1: Results on 100 Feynman equations from . We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR.

increasing the backbone model size and the mixture probability substantially enhances LaSR's performance, indicating that as the language reasoning capabilities of LLMs improve, so will our performance.

### Ablation Experiments

We conduct ablations on the use of Concept Evolution (skip phase three from Figure 1), Concept Library (skip phase two and three), variable names, and user hints. Figure 3 shows how these ablations affect performance over 40 iterations. We designate an equation as "solved" if, after \(N\) iterations, the MSE of our predicted equation is less than \(10^{-11}\). This metric differs from 'Exact Solved' as defined in the prior experiments: an equation can be 'exactly solved' yet have an MSE higher than \(10^{-11}\) due to the noise floor in the target variables, and an equation can have low loss but not be an exact match. We observe from the results that: (1) Removing variable names results in a substantial performance drop, as we lose semantic meaning provided by variables (for instance, observing \(\) could suggest employing trigonometric functions on \(\)). (2) Learning a concept library enables faster convergence to solutions. Without the concept library, task convergence is slower, and widens under higher concept guidance conditions (\(p>0.1\%\)).

### Qualitative Analysis and User Hints

The concept library provides an interpretable window into our evolutionary search process. To showcase the concepts learned by LaSR, we take a sample equation from the Feynman dataset, the electric field of a dipole \(E_{f}=}{4 e^{3}}\) and comment on the libraries learned at various intervals. We see rudimentary concepts emerge in the second iteration:

_"The presence of basic trigonometric functions like sin in the good expressions contributes to their quality, indicating a connection to physical concepts such as waveforms or periodic phenomena."_

And, in subsequent iterations, the concepts become even more refined:

_"The good mathematical expressions exhibit a balance between mathematical operations such as multiplication, division, and trigonometric functions, which are known to have physical interpretations and relevance in various scientific phenomena."_

This iterative refinement helps LaSR consistently maintain high-quality concepts, allowing it to converge to an exact match within 40 iterations. By contrast, PySR and the concept library ablations fail to converge on an exact match solution, returning equations that -- while low-loss -- involve many extra terms absent from the ground truth. This reinforces our hypothesis that injecting semantic meaning into the search process not only improves search efficiency, but also regularizes against

Figure 3: Evaluation results for ablations/extensions of LaSR. (**Left**): We ablate three components of LaSR: Concept Evolution, Concept Library, and variable names and evaluate their MSE solve rate performance on the Feynman dataset over 40 iterations. We find that each component contributes to accelerating search at different stages in the search process. (**Right**): We extend LaSR by providing an initial concept library \(_{0}\) in the form of user provided hints. We find that natural language hints significantly increases the speed of solving equations.

complex equations -- as the LLM-generated concepts help filter out irrelevant terms. A deeper qualitative analysis is in Appendix A.8.

**Extending LaSR with Hints**: A benefit of LaSR is that its search can be initialized with a set of user-specified, natural-language "hints." To evaluate this capability, we generate hints for each equation based on variations of the chapter title of the Feynman lecture that the equation belongs to. We intentionally keep the hints vague to see if knowledge about just the general field is sufficient in improving LaSR's performance. We showcase results in Figure 3. We observe a noticeable boost in performance from injecting these hints, even for our weakest performing model, indicating that even minimal user input can substantially enhance LaSR's effectiveness in discovering equations.

### Data Leakage Validation

An important consideration in using LLMs for existing SR problems is the possibility that the LLM was exposed to the hold-out problems in the validation set, presenting an unfair advantage to LLMs trained on massive datasets. Intuitively, LaSR generates its own concepts which are conditioned on suboptimal programs, which are unlikely to be within the LLM's memorized responses. To validate this, we generate a dataset of 41 synthetic equations that are engineered to deviate from common physical and mathematical structures and have arbitrary variables. For example, one such equation is \(y=+0.536}{x_{2}e^{x_{1}}( x_{2}-x_{2}e^{}x_{1})}\). We find that PySR struggles to solve equations with these characteristics (given 400 iterations). Hence, solving such equations hinges on the language guidance components.

We run LaSR with Llama3-8B at 0.1%. We then compare our synthesized program's test set \(R^{2}\) with that of PySR's. We justify using correlation instead of exact-match as we are not motivated by the application of results for scientific discovery in this experiment. Our results are summarized in Table 3 and show that LaSR's concept-guided synthesis still provides a considerable performance boost compared to PySR - demonstrating that LaSR can outperform PySR even when data leaking is not possible.

### Using LaSR to discover LLM Scaling Laws

So far, we have demonstrated that LaSR can discover equations that are practical but already known (Feynman Dataset) and equations that are novel but aren't practical (Synthetic Dataset). To investigate LaSR's utility in finding novel and practical empirical trends, we investigate whether LaSR can discover novel LLM scaling laws on the BigBench dataset . More details on this experiment are presented in Section A.6.

Traditionally, to identify an LLM scaling law, practitioners must first manually posit a "skeleton equation" with a fixed set of known variables and unknown free parameters, and then optimize the unknown parameters based on a dataset of model hyperparameters and resulting dataset fitness [23; 1; 5]. Instead of starting with a predefined equation, we use LaSR to discover the skeleton equation that best fits various subsets of the BigBench dataset.

**Setup.** BigBench contains 204 tasks with scored responses from 55 language models trained with different hyperparameters. We evaluate on the subset of tasks where the preferred metric is 'Multiple choice grade' (53,812 samples). Our goal is to find the equation that best predicts the test score given the model hyperparameters and the dataset hyperparameters. We run LaSR with 3840 populations of 200 candidates each for 7 hours (overnight). The runtime of LaSR is comparable to other SR algorithms for this experiment as the slowest operation isn't generating candidate equations but rather optimizing and evaluating candidate equations.

**Results.** LaSR discovers the following scaling law on the subset of BigBench:

\[=}{B}^{ }}+E\] (4)

   PySR & LaSR \\  & (Llama3-8B, \(0.1\%\)) \\ 
0.070 & **0.913** \\   

Table 3: Evaluation results of data leakage. We present the test set \(R^{2}\) of PySR and of LaSR on a synthetic symbolic regression dataset. Higher \(R^{2}\) is better.

where score is the MCQ grade, train_steps is the number of training steps for the model, and #shots is the number of in-context examples provided during inference. The fitted free parameters are presented in Equation 5.

_Qualitative Evaluation:_ Equation 4 describes an empirical relationship between training hyperparameters (training steps) and inference hyperparameters (number of shots). It asserts that increasing the number of shots exponentially increases the model's performance for low-resource models, while having diminishing gains as the number of training steps of the model increase. This observation is consistent with work in scaling test-time compute .

As the output artifacts of LaSR are interpretable, we can integrate this empirical relationship between training steps and number of shots into known scaling laws. Specifically, we can augment the chinchilla scaling law as follows:

\[ = )^{}}+)^{}}+E\] (Chinchilla ) \[ = )^{}}+ )^{}}+E\] (Modified Chinchilla)

_Quantitative Evaluation_: We fit the free parameters of each equation on the training set (\(43,049\) samples) and measure the MSE loss between the actual grade and the predicted grade on the validation set (\(10,763\) samples). The results are presented in Table 4. We find that the Equation 4's performance, as well as modified Chinchilla's performance, is competitive with that of Chinchilla's in predicting the MCQ grade. However, the horizontal line \(=E\) demonstrates acceptable performance as well. We believe increasing the scale of these preliminary experiments (with richer datasets or longer search horizon) will lead to additional empirical findings.

## 5 Related Work

Symbolic Regression.The field of SR started in the 1970s [18; 29] and has recently become a prominent approach to AI-for-science [33; 34; 40]. Two algorithmic themes here are:

_Non-parametric Algorithms_: Most work on SR focuses on improving search efficiency using heuristics or parallelization. Specifically, PySR  builds a multi-population evolutionary algorithm that incorporates various preexisting heuristics , and introduces novel ones such as simulated annealing, an evolve-simplify-optimize loop, and an adaptive parsimony metric. PySR has been successfully applied to study problems in domains such as cosmology , international economics , and climate modeling . LaSR extends PySR to enable the discovery of latent concepts.

_Parametric Algorithms_: Recent work in SR and program synthesis has often used neural networks to accelerate search [43; 40; 38; 28; 34; 13; 35]. The interplay between the neural and the symbolic components in these works can be abstracted into two categories: (1) leveraging LLMs to induce program scaffolds [34; 40; 35], and (2) learning a neural policy to accelerate search [38; 28; 43; 13]. We highlight two methods from the first category: Funsearch  and LLM-SR . Funsearch  uses a pretrained LLM to implement a mutation operator on a database of executable programs under a fixed specification to find super-optimized programs in extremal combinatorics. LASR is a generalization of FunSearch: while FunSearch conditions program generation on a static "specification" (analogous to our concept library), we discover the concept library in the course of the

  
**Scaling Law Skeleton** & **MSE Loss** & **Free Parameters** \\  Equation 4 & \(0.03598 0.00265\) & 3 \\ Chinchilla  & \(0.03610 0.00268\) & 5 \\ Modified Chinchilla & \(\) & 5 \\ Residual Term Only & \(0.09324 0.01992\) & 1 \\   

Table 4: Preliminary results on evaluating different LLM scaling laws. We measure MSE loss on a held out subset of BigBench . The equation discovered with LASR performs as well as the Chinchilla equation  on BigBench while using less free parameters. The residual term skeleton equation \((=E)\) also performs well.

algorithm. We do not compare against FunSearch due to resource constraints. As for LLM-SR , it leverages a pretrained LLM for generating program sketches . The sketch parameters are optimized and cached in a database, which is in turn used to generate new sketches. Our work is an orthogonal direction of improvement. It is technically possible to "plug" the LLM-SR framework (or other LLM-based search algorithms ) into LASR and use our generated concepts to guide the lower-level search component.

The second category includes methods like DSR , which, just like LASR, frame SR as a sequence modeling problem. However, the search in LASR leverages a learned concept library and the language and code biases in LLMs, instead of relying on amortization alone.

Program Synthesis with Foundation Models.Recent work in program synthesis models program generation as a sequence prediction problem. Under this paradigm, the DSL and the input-output specification is serialized in the prompt and a code-generation foundation model [31; 7; 4] is leveraged to autoregressively generate candidate programs. This approach has been impactful in many areas including spreadsheet formula prediction [13; 8], competitive programming , and visual programming [48; 21; 9]. LASR is similar to work in this area in that the LLM Mutate, LLM Crossover, and LLM Initialization functions all follow the sequence prediction paradigm to synthesize mathematical equations, relying on guidance from the concept library.

Program Synthesis with Library Learning.Deploying classical program synthesizers in a new domain often necessitate hand-engineering DSLs to enable scalable synthesis. This severely limits the generality and practicality of such methods. An emerging direction of research - called library learning - attempts to learn the DSL and the programs simultaneously [15; 3; 19; 27; 53; 14; 44; 54]. This is typically framed as a hierarchical Bayesian optimization problem over the space of programs and the space of library functions that generate those programs. Notably,  uses LLM guidance to assist in program induction and in auto-documenting learned library modules and  considers learning programs under a latent distribution over the space of natural language and the space of the DSL. LASR shares a similar problem formulation to these works, but optimizes over the space of programs and over the space of natural language descriptions of these programs.

## 6 Conclusion

We have presented LASR, a framework that uses zero-shot queries to an LLM to induce abstract, reusable concepts that can be used to accelerate SR. We have shown that LASR outperforms state-of-the-art approaches on the standard Feynman equation task. We have also used the algorithm to discover a novel scaling law for LLMs.

A key benefit of LASR is that its capabilities are ultimately bottlenecked by those of the underlying LLM. LLMs are rapidly gaining capability and getting cheaper, and future versions of LASR should be able to tap into this progress.

Many directions of research remain open. First, our strategy of accelerating evolutionary search with LLM-based concept induction may be applicable beyond the SR setting. Future research should explore such applications. Second, while our approach here was entirely based on in-context learning, it is worth exploring if finetuning improves the performance of the LLM. Finally, we evaluated the learned concept library exclusively on the downstream SR task. However, the library may also be valuable in other tasks such as clustering or explanation synthesis. Exploring these other tasks is an attractive topic for future work.

Limitations.The current instantiation of LASR has several limitations. First, it cannot guarantee that the concepts it learns are correct or insightful. Even a concept that leads to strong performance in downstream SR tasks may do so because of quirks of the model and data, and end up misleading scientists using the method in a discovery process. Also, we do not currently have a way to ensure that the learned concepts are mutually consistent. Finally, our evaluation here was constrained by our compute budgets for LLMs and search. Whether the trends we see generalize to higher-compute regimes remains to be seen.

**Acknowledgements:** This research was partially supported by the NSF Expeditions in Computing Award #CCF-1918651, the NSF National AI Institute for Foundations of Machine Learning (IFML), and ARO award #W911NF-21-1-0009. We thank Foundry Technologies for providing substantial computational resources for our experiments.