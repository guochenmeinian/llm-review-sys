# Quantum Equilibrium Propagation:

Gradient-Descent Training of Quantum Systems

Benjamin Scellier

Rain AI

benjamin@rain.ai

###### Abstract

Equilibrium propagation (EP) is a training framework for physical systems that minimize an energy function. EP uses the system's intrinsic physics during both inference and training, making it a candidate for the development of energy-efficient processors for machine learning. EP has been studied in various classical physical systems, including classical Ising networks and elastic networks. We present a version of EP for quantum systems, where the energy function is the Hamiltonian's expectation value, whose minimum is reached at the ground state. As examples, we study the settings of the transverse-field Ising network and the quantum harmonic oscillator network - quantum analogues of the network models studied within classical EP.

## 1 Introduction

Machine learning (ML) is currently powered by classical digital computing. Meanwhile, fundamental research explores alternative computing paradigms to enhance ML capabilities. Quantum computing leverages the principles of quantum mechanics to encode and process information in ways that classical computers cannot, potentially handling exponentially larger amounts of information. In contrast, neuromorphic computing, taking inspiration from the brain's energy efficiency, aims to leverage analog physics and compute-in-memory platforms to significantly reduce the cost of inference and training in ML . The field of 'physical learning' aims to unite these efforts by exploring the inherent physics of any physical system (whether classical or quantum) for computation, without necessarily mimicking neurons and synapses  - see Momeni et al.  for a very recent review.

Over the past decades, progress in ML research has been driven by the effectiveness of frameworks for optimizing cost functions based on the backpropagation (BP) algorithm. One challenge for the field of physical learning has been the search for frameworks that are as effective as BP, while adhering to local computation and being robust to analog noise, which are essential for efficient implementations on analog compute-in-memory platforms. In recent years, several gradient-descent training frameworks for physical systems have been proposed. For instance, Lopez-Pastor and Marquardt  introduced a framework applicable to arbitrary time-reversal invariant Hamiltonian systems, and Wanjura and Marquardt  developed a method for extracting weight gradients in optical systems based on linear wave scattering. The present paper focuses on the training framework known as equilibrium propagation.

Equilibrium propagation (EP)  is a training framework for energy-based systems, that is systems whose physics drives their state towards the minimum of an energy function (equilibrium or steady state). EP extracts the gradients of the cost function using two equilibrium states corresponding to different boundary conditions, which are then used to locally adjust the trainable weights. EP requires only knowledge of trainable interactions, and thus is applicable in partially unknown systems too. EP has been applied to various systems, including continuousHopfield networks (Scellier and Bengio, 2017), resistor networks (Kendall et al., 2020), elastic and flow networks (Stern et al., 2021), spiking networks (Martin et al., 2021), weakly connected oscillatory networks (Zoppo et al., 2022), the (classical) Ising model (Laydevant et al., 2024), and coupled phase oscillators (Wang et al., 2024). More generally, EP applies in systems obeying variational principles (Scellier, 2021). Recent experimental demonstrations have shown the applicability of a variant of EP called 'Coupled Learning' on hardware: Dillavou et al. (2022, 2024) built two generations of self-learning resistor networks, and Altman et al. (2024) built a self-learning elastic network. Additionally, (Yi et al., 2023) used another variant of EP in a memristor crossbar array, and Laydevant et al. (2024) used EP on D-wave to train a classical Ising machine (where they used quantum annealing to reach the ground state). Simulations have further underscored the potential of EP for ML applications: in particular, Laborieux and Zenke (2022) trained an energy-based convolutional network to classify a downsampled version of the ImageNet dataset. More broadly, (Zucchet and Sacramento, 2022) have highlighted EP's general applicability to any bilevel optimization problem (beyond physical, energy-based learning), including meta-learning (Zucchet et al., 2022).

In this work, we derive a quantum version of EP. In Quantum Equilibrium Propagation (QEP), the system is brought to the ground state of its Hamiltonian, parameterized by real-valued trainable weights, to produce a prediction. The algorithm performs gradient descent on the expectation value of an observable, which serves as the cost function to optimize. The central ingredient for translating from EP to QEP is the variational principle of quantum mechanics: the Hamiltonian's expectation value is minimized (more generally, extremized) at the Hamiltonian's ground state (more generally, eigenstates). Thus, in QEP, the Hamiltonian's expectation value represents the classical EP energy function, and eigenstates represent equilibrium states. Similar to EP, QEP only requires knowledge of trainable interactions, and has a local learning rule, which might be useful for the development of specialized quantum hardware with reduced classical overhead, where measurements of the weight gradients and adjustments of the trainable weights would be performed locally. To illustrate QEP, we study the settings of the transverse-field Ising network and the quantum harmonic oscillator network - quantum analogues of the Ising model and elastic network.

In parallel with this work, two other strongly related manuscripts exploring quantum extensions of EP have recently been published on Arxiv (Massar and Mognetti, 2024; Wanjura and Marquardt, 2024). Massar and Mognetti (2024) also studied EP in thermal systems, where the system settles into the minimum of the free energy functional, and demonstrated how weight gradients can be extracted solely from thermal fluctuations, while Wanjura and Marquardt (2024) established a connection between EP and Onsager reciprocity.

## 2 Quantum Equilibrium Propagation

We present Quantum Equilibrium Propagation (QEP), an extension of EP for quantum systems. For a brief presentation of classical EP, see Appendix A. A primer on the necessary concepts of quantum mechanics is provided in Appendix B.

We consider a quantum system, serving as a 'learning machine', with a Hamiltonian \((w,x)\), parameterized by trainable weights \(w=(w_{1},w_{2},,w_{M})\) and an input \(x=(x_{1},x_{2},,x_{P})\). The system's ground state, represented as \(|(w,x)\), satisfies:

\[(w,x)|(w,x)=E(w,x)|(w,x)\] (1)

where \(E(w,x)\) denotes the ground state energy. This ground state serves to encode a prediction on a target output \(y=(y_{1},y_{2},,y_{K})\) corresponding to the supplied input \(x\). We also introduce a 'cost operator' \((y)\) parameterized by \(y\), whose expectation value in the state \(|(w,x)\),

\[(y)_{(w,x)}=(w,x)|(y)| (w,x),\] (2)

serves as the cost function. The goal is to adjust the trainable weights of the Hamiltonian to minimize this cost function. Assuming that the cost operator can be integrated in the system as an interaction Hamiltonian (interaction between the system's state \(|\) and target output \(y\)), we form the 'total Hamiltonian':

\[^{}=(w,x)+(y),\] (3)

where \(\) controls the strength of the cost interaction.

Finally, we partition the trainable weights into groups, such that the Hamiltonian derivatives \(^{}}{ w_{x}}\) and \(^{}}{ w_{x}}\) commute for any pair \(w_{j}\) and \(w_{k}\) within the same group. Given an input-output pair \((x,y)\) from training data, QEP proceeds as follows.

1. Set \(=0\) and reach the ground state \(|^{0}_{}=|(w,x)\) of \(^{0}\) (with ground state energy \(E^{0}_{}=E(w,x)\)), characterized by \[^{0}|^{0}_{}=E^{0}_{}|^{0}_{}.\] (4) Measure the Hamiltonian derivatives \(^{0}}{ w_{x}}\) for all trainable weights of a given group. Repeat this step for the other groups of commuting trainable weights. Repeat \(T\) times and denote the measurement outcomes as \(h^{(1)}_{k}(0),h^{(2)}_{k}(0),,h^{(T)}_{k}(0)\), for each \(w_{k}\).
2. Set \(>0\) and repeat the same step as above: reach the ground state \(|^{}_{}\) of \(^{}\), with ground state energy \(E^{}_{}\), characterized by \[^{}|^{}_{}=E^{}_{}|^{ }_{}.\] (5) For each Hamiltonian derivative \(^{}}{ w_{k}}\), denote the \(T\) measurement outcomes as \(h^{(1)}_{k}(),h^{(2)}_{k}(), h^{(T)}_{k}()\).
3. Update the trainable weights \(w_{1},w_{2},,w_{M}\) as \[ w_{k}=[_{t=1}^{T}h^{(t)}_{k}(0)- _{t=1}^{T}h^{(t)}_{k}()],\] (6) where \(>0\) is a learning rate.

The central result of QEP is that the learning rule of Eq. (6) approximates one step of gradient descent on the expectation value of the cost operator, as stated in the following result.

**Theorem 1**.: _The gradient of the cost function can be approximated as_

\[_{w}(w,x)|(y)|(w,x) =.|_{=0}^{}_{ }|^{}}{ w}|^{}_{}\] (7) \[[^{}_{}|^{}}{ w}|^{}_{}- ^{0}_{}|^{0}}{ w}|^{0}_{}]\] (8) \[[_{t=1}^{T}h^{(t)}( )-_{t=1}^{T}h^{(t)}(0)].\] (9)

Theorem 1 follows from the classical EP formula (Theorem 2 in Appendix A) and the variational principle of quantum mechanics (Lemma 3 in Appendix B). First, we comment on the differences between EP and QEP, and then we discuss the properties of EP that transfer to QEP.

Compared to the classical setting (Appendix A), the characteristics of quantum measurements affect the training procedure in several ways. First, QEP involves two levels of approximation in the estimate of the gradient of the cost function. In addition to the finite difference used to approximate the derivative \(\) at \(=0\), a second level of approximation is due to the probabilistic nature of quantum measurements: since a quantum measurement only gives an unbiased estimate of the expectation value, multiple measurements of the Hamiltonian derivatives \(}{ w_{k}}\) are required to get better estimates of the weight gradients. Second, since the state of the system generally changes upon measurement of a Hamiltonian derivative, the system must be reset to its ground state after each measurement. Third, the Hamiltonian derivatives \(}{ w_{j}}\) and \(}{ w_{k}}\) cannot generally be measured simultaneously unless they commute - we will see, however, in Section 3 that the trainable weights can typically be partitioned into \(p\) groups of commuting Hamiltonian derivatives, with small \(p\) (\(p=1\) or \(p=2\) in our examples).

QEP also inherits from key features of classical EP. Suppose the total Hamiltonian of the system can be expressed as the sum of Hamiltonians corresponding to individual interactions or contributions, i.e.

\[=_{}+_{1}+_{2}+ +_{M}\] (10)

where \(_{k}\) is the Hamiltonian of an interaction parameterized by \(w_{k}\) (for \(1 k M\)), and \(_{}\) does not depend on any trainable weight. Then the Hamiltonian derivatives simplify to \(}{ w_{k}}=_{k}}{  w_{k}}\). If the trainable weight \(w_{k}\) is stored close to where the observable \(_{k}}{ w_{k}}\) is measured, the learning rule for \(w_{k}\) is local. Moreover, the system's Hamiltonian need not be fully known: in particular, no knowledge of \(_{}\) is required. Finally, similar to the classical setting where the equilibrium state need not be a minimum but only a critical point (stationary state) of the energy function, QEP only requires reaching an eigenstate of the Hamiltonian, not necessarily the ground state. One condition for Eq. (8) to hold, however, is that the nudge eigenstate \(|^{}_{}\) must be obtained as a smooth deformation (adiabatic transformation) of the free eigenstate \(|^{0}_{}\) when varying the nudging parameter from \(0\) to \( 0\). It remains to be seen whether this condition is necessary or can be further relaxed in practice.

## 3 Examples of Quantum Systems Compatible with QEP

Next, we present for illustration the setting of the transverse-field Ising model and quantum harmonic oscillator network.

### Quantum Ising Model

As a first example, we consider a quantum version of the classical Ising model studied in classical EP (Appendix A.1). While a classical Ising network of \(N\) classical spins can be in either of \(2^{N}\) possible configurations, a quantum Ising network of \(N\) spins exists in a superposition of these \(2^{N}\) configurations (i.e. a linear combination with coefficients in \(\)). Hence the difference between the classical and the quantum settings: while the state of the classical model is described by a \(N\)-dimensional binary-valued vector, the quantum model's state is represented by a vector in a \(2^{N}\)-dimensional complex vector space. We denote the \(d=2^{N}\) basis states as \(|_{1}_{2}_{N}\) with \(_{k}\{,\}\) for each \(k\{1,2,,N\}\), e.g. \(|\), \(|\) and similarly for the other \(2^{N}-2\) basis states.

Similar to the classical Ising energy function, the Hamiltonian of a quantum Ising network has couplings between spins \(J_{jk}\) and bias fields \(h_{k}\) applied to individual spins, which we view as trainable weights. For example, the Hamiltonian of the transverse-field Ising model is given by:

\[_{}=-_{1 j<k N}J_{jk}_{j} _{k}-_{k=1}^{N}h_{k}_{k},\] (11)

where \(_{k}\) and \(_{k}\) are the Pauli operators, defined as follows. The Pauli \(_{k}\) operator acts as a phase-flip operator on the \(k\)-th spin, according to:

\[_{k}\;|_{1}_{k-1}_{k+1 }_{N}=+|_{1}_{k-1}_{k+1} _{N},\] (12) \[_{k}\;|_{1}_{k-1}_{k +1}_{N}=-|_{1}_{k-1}_{k+1 }_{N}.\] (13)

The Pauli \(_{k}\) operator acts as a bit-flip operator on the \(k\)-th spin, according to:

\[_{k}\;|_{1}_{k-1}_{k+1 }_{N}=|_{1}_{k-1}_{k+1} _{N},\] (14) \[_{k}\;|_{1}_{k-1}_{k +1}_{N}=|_{1}_{k-1}_{k+1} _{N}.\] (15)

In this setting, the gradients of the Ising Hamiltonian with respect to the trainable weights, required in the learning rule of Eq. (6), are given by

\[_{}}{ J_{jk}}=-_{j} _{k},_{}}{ h_ {k}}=-_{k}.\] (16)Importantly, the Pauli \(_{k}\) operators (\(1 k N\)) commute with one another, allowing for simultaneous measurements during training. Similarly, the Pauli \(_{k}\) operators (\(1 k N\)) commute and can be measured simultaneously. However, \(_{j}\) and \(_{k}\) do not commute. In this example, the trainable weights can be partitioned into two groups of commuting Hamiltonian derivatives.

### Quantum Harmonic Oscillator Network

As a second example, we consider the quantum harmonic oscillator network, a quantum analogue of the elastic network model studied in classical EP (Appendix A.2). It consists of \(N\) quantum particles interacting via harmonic potentials. For clarity, we assume that the particles have one-dimensional (rather than three-dimensional) positions. Whereas the state of a classical mass-spring network is represented by the \(N\)-dimensional vector of positions of the particles or masses \((r_{1},r_{2},,r_{N})^{N}\), the state of the quantum network is a superposition of all these configurations. In this setting, the state vector is a function \(:^{N}\) (the wave function), that assigns a complex number \((r_{1},r_{2},,r_{N})\) to each possible configuration \((r_{1},r_{2},,r_{N})\). The corresponding Hilbert space is infinite-dimensional.

The position and momentum operators of the \(i\)-th particle, denoted as \(_{i}\) and \(_{i}\), are defined by their action on the wavefunction as follows:

\[(_{i})(r_{1},r_{2},,r_{N}) =r_{i}(r_{1},r_{2},,r_{N}),\] (17) \[(_{i})(r_{1},r_{2},,r_{N}) =-}(r_{1},r_ {2},,r_{N}),\] (18)

where \(\) is the imaginary unit (\(^{2}=-1\)) and \(\) is the reduced Planck constant. We denote as \(_{i}^{2}}{2m_{i}}\) the kinetic energy operator of the \(i\)-th particle, where \(m_{i}\) is the mass, and as \(V(_{i}-_{j})\) the interaction potential between the \(i\)-th and \(j\)-th particles. Assuming that some of these interaction potentials are harmonic potential operator, \(V(_{i}-_{j})=k_{ij}( {r}_{i}-_{j})^{2}\), where \(k_{ij}\) is the spring constant, the Hamiltonian of the system is given by:

\[_{}=_{i=1}^{N}_{i}^{2}}{2m_{i}}+ _{(i,j)}V(_{i}-_{j})+_{(i,j)}k_{ij}(_{i}-_{j})^{2},\] (19)

where the \(k_{ij}\)'s are viewed as trainable weights.

We view this system as a 'learning machine' as follows. A subset of the particles represent 'input particles' whose positions are fixed to (classical) input values. Another subset of the particles represent 'output particles' whose position operators \(_{i}^{}\) are used as output observables, and whose measurement outcomes must match target outputs \(y_{i}\). We use the squared error cost operator

\[(y)=_{i=1}^{K}(_{i}^{}-y_{i} )^{2},\] (20)

where \(K\) is the number of output particles and \(\) is the identity operator (\(|=|\) for any \(|\)). The expectation value of this cost operator is non-negative, and it is zero if and only if the state is an eigenstate of \(_{i}^{}\) with eigenvalue \(y_{i}\), i.e. if and only if a measurement of \(_{i}^{}\) gives outcome \(y_{i}\) with certainty. The term \((_{i}^{}-y_{i})^{2}\) represents a harmonic potential around \(y_{i}\), where the \(i\)-th output particle experiences a restoring force that pulls it toward \(y_{i}\). To implement the nudging Hamiltonian \((y)\) corresponding to this cost operator, we use \(K\) springs with spring constants \(k_{i}^{}=\), connecting the \(K\) output particles to \(K\) 'target particles' placed at positions \(y_{1},,y_{K}\).

Finally, the QEP learning rule requires the partial derivatives of the Hamiltonian with respect to the spring constants. These are given by:

\[_{}}{ k_{ij}}=( _{i}-_{j})^{2}.\] (21)

Since the \(_{i}\) operators commute with one another, all Hamiltonian derivatives can be measured simultaneously to obtain the gradients of the cost function. This example also illustrates that the details of untrainable interactions need not be known: specifically, we do not require analytical knowledge of the Hamiltonian term

\[_{}=_{i=1}^{N}_{i}^{2}}{2m_{i} }+_{(i,j)}V(_{i}-_{j}).\] (22)

## 4 Discussion

Equilibrium Propagation (EP) has been studied in various classical physical systems, including classical Ising networks and elastic networks. We have derived Quantum Equilibrium Propagation (QEP), a quantum extension of EP, and have illustrated it in quantum versions of these network models. The key conceptual bridge between EP and QEP is the variational principle of quantum mechanics, which states that the ground state of a Hamiltonian minimizes its expectation value (or more generally, its eigenstates extremize its expectation value).

QEP inherits from key features of EP. Notably, it is partially agnostic to the system's Hamiltonian, requiring only analytical knowledge of trainable interactions. Additionally, QEP employs a local learning rule. These features suggest potential benefits for developing specialized quantum computing devices that are tolerant to device variations, and where the trainable weights would be located near the location where the Hamiltonian derivatives are measured (thereby reducing the classical computational overhead). QEP thus fundamentally departs from hybrid quantum-classical approaches, where a classical computer optimizes the parameters of a parameterized quantum circuit, typically necessitating full knowledge of the circuit (Cerezo et al., 2021). QEP is also significantly more efficient than methods that estimate partial derivatives of the loss function sequentially by perturbing each trainable weight individually (Schuld et al., 2019).

Despite these potential advantages, QEP also comes with its requirements and challenges. First, similar to classical EP, the cost operator \((y)\) must be implementable as an interaction Hamiltonian, with its interaction strength controllable through the nudging parameter \(\)(see, however, Wanjura and Marquardt (2024), where a more generic linearized nudging method is employed). Second, QEP relies on equilibration steps. While reaching the ground state of a complex Hamiltonian remains a challenging problem, QEP only necessitates reaching an _eigenstate_, which provides significantly more flexibility and may mitigate some of the practical difficulties associated with full ground-state preparation.