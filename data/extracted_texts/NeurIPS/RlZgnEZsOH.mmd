# HuRef: HUman-REadable Fingerprint

for Large Language Models

 Boyi Zeng\({}^{1}\), Lizheng Wang\({}^{2}\), Yuncong Hu\({}^{2}\), Yi Xu\({}^{2}\)

**Chenghu Zhou\({}^{3}\), Xinbing Wang\({}^{2}\), Yu Yu\({}^{2}\), Zhouhan Lin\({}^{1}\)**

\({}^{1}\)LUMIA Lab, Shanghai Jiao Tong University

\({}^{2}\)Shanghai Jiao Tong University, \({}^{3}\)Chinese Academy of Sciences

boyizeng@sjtu.edu.cn \({}^{}\)lin.zhouhan@gmail.com

Zhouhan Lin is the corresponding author.The code is available at https://github.com/LUMIA-Group/HuRef.

###### Abstract

Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without interfering with training or exposing model parameters to the public. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, with negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model. Due to the potential risk of information leakage, we cannot publish invariant terms directly. Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image. In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners. To ensure the published fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP). Experimental results across various LLMs demonstrate the effectiveness of our method.1

## 1 Introduction

Large language models (LLMs) have become the foundation models in many scenarios of artificial intelligence. As training an LLM from scratch consumes a huge amount of computation and data resources and the trained LLM needs to be carefully protected from malicious use, the parameters of the LLMs become a crucial property to protect, for both commercial and ethical reasons. As a result, many of the LLMs are open-sourced with carefully designed licenses to reject commercial use (Touvron et al., 2023; Taylor et al., 2022) or requiring an apply-and-approval process (Touvron et al., 2023; Zhang et al., 2022; Penedo et al., 2023; BaiChuan-Inc, 2023; Team, 2023; Zheng et al., 2023), let alone some LLMs are not open-sourced entirely (OpenAI, 2022; GPT-4, 2023; Brown et al., 2020; Wu et al., 2023; Chowdhery et al., 2022; Hoffmann et al., 2022).

At the core of protecting LLMs from unauthorized use is to identify the base model of a given LLM. However, different from other forms of property such as software or images, protecting LLMs is a novel problem with unique challenges. First, the base model usually needs to be fine-tuned or even continued pretraining to be applied to downstream tasks, resulting in parameter updates that make the resulting model different from the original base model, which makes it disputable to identify the base model. Second, many of the popular LLMs are not releasing their parameters, leaving the identification in a black-box setting. Third, different from previous smaller-scale neural networks that are only trained for specific tasks, LLMs are usually targeted for enormous forms of tasks that are not yet defined during pretraining. This has made the watermarking methods for traditional neural networks (Adi et al., 2018; Xiang et al., 2021; Yadollahi et al., 2021) not suited in this case, especially under extensive subsequent training steps.

In this work, we propose a novel way to overcome the aforementioned challenges by proposing a method that reads part of the model parameters and computes a fingerprint for each LLM without interfering with training or exposing model parameters to the public. The appearance of the fingerprint is closely dependent on the base model, and invariant to almost all subsequent training steps, including supervised fine-tuning (SFT), reinforcement learning with human feedback (RLHF), or even continue-pretraining with augmented vocabulary in a new language.

The fingerprint is based on our observation that the vector direction of LLM parameters remains stable against various subsequent training steps after the model has converged during pretraining. This makes it a good indicator for base model identification. Empirically, the sufficiency of this correlation is elaborated in Section3.1.1, while its necessity is presented in Section3.1.2.

Further, despite its stability towards training, the vector direction of the model parameter is vulnerable to some simple direct weight rearrangements that could significantly change the direction of parameter vectors without affecting the model's performance. We construct three invariant terms that are robust to these weight rearrangements by systematically analyzing possible rearrangements and leveraging the Transformer structure. This is elaborated in Section3.2.

Moreover, we generate human-readable fingerprints by mapping the invariant terms into a Gaussian random vector through an encoder and then mapping the Gaussian vector to a natural image through an off-the-shelf image generation model, such as StyleGAN2 (Karras et al., 2020). This generation offers a dual benefit of mitigating information leakage and making our fingerprints straightforward to decipher. To ensure the published fingerprints are honestly generated, we also introduced Zero-Knowledge Proof (ZKP). This is elaborated in Section4.

With this fingerprinting approach, we can sketch an outline for protecting LLMs in Figure1.

## 2 Related Works

There are two primary categories of related approaches.

Figure 1: An illustrative framework for LLM protection with fingerprints. The LLM manufacturers compute invariant terms internally and feed them into the fingerprinting model (FPM2) to generate a fingerprint image. This image is then released to the public along with zero-knowledge proofs (\(_{1}\)), allowing for intuitive identification of shared base models through the fingerprint images. We also provide a limited one-to-one quantitative comparison scheme (ICS & \(_{2}\)) as a complement. Zero-Knowledge Proof guarantees the reliability of the fingerprints and comparison results, without interfering with LLM training or revealing model parameters to the public.

[MISSING_PAGE_FAIL:3]

On the other hand, the models that are trained independently appear to be completely different in parameter vector direction, showing almost zero cosine similarity with the LLaMA-7B model.

These observations indicate that a high cosine similarity between the two models highly suggests that they share the same base model, and vice versa.

#### 3.1.2 Necessity

From the necessity perspective, we want to verify if the base model's ability can still be preserved when the cosine similarity is intentionally suppressed in subsequent training steps. To this end, we inherit the LLaMA-7B base model and interfere with the Alpaca's SFT process by augmenting the original SFT loss with an extra term that minimizes the absolute value of cosine similarity. i.e. \(L_{A}=_{A},_{base}|}{| _{A}||_{base}|}\). Here \(_{A},_{base}\) stand for the parameter vector of the model being tuned and that of the base model, respectively.

Figure 2 presents the average zero-shot performance on a set of standard benchmarks when \(L_{A}\)(PCS) is at different values. The benchmarks include BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e, ARC-c (Clark et al., 2018), RACE (Lai et al., 2017) and MMLU (Hendrycks et al., 2020). (c.f. Appendix Table 5 for a detailed breakdown of performances on each task.) We can see that despite the original training loss is still present, the model quickly deteriorates to random guesses as the cosine similarity detaches away from that of the base model.

These observations indicate that it is fairly hard for the model to preserve the base model's performance without keeping a high cosine similarity to it.

### Deriving the Invariant Terms

Although the vector direction of model parameters is shown to closely stick to its base model, directly comparing the vector direction through cosine similarity requires both models to reveal their parameters, which is unacceptable in many cases. In addition, apart from training, parameter vector direction is vulnerable to some attacks that directly rearrange the model weights. For example, since the hidden units in a model layer are permutation-invariant, one can easily alter the parameter vector direction by randomly permuting the hidden units along with the weights wired to the units.

These attacks are invisible to discover since they could easily break the cosine similarity but neither change the model structure nor affect the model performance.

In this subsection, we are going to first systematically analyze and formalize possible weight rearrangements by leveraging the structure constraints of the Transformer, and then derive three terms that are invariant under these rearrangements, even when they are combined. Let's first consider the Transformer layer as depicted in Figure 3. Formally, the layer conducts the following computation:

\[_{Attn}^{{}^{}}=(_{n}_{Q}( _{n}_{K})^{T}}{})_{n}_{V}_{O}\] (1)

\[_{n+1}^{{}^{}}=(_{Attn}_{1}+_{1} )_{2}+_{2}\] (2)

where \(_{n}^{l d}\) is the hidden state of the \(n\)-th layer, with \(l,d\) being sequence length and model dimensions, respectively. \(_{Attn}^{{}^{}}\) is the self-attention output. To reduce clutter, we omit equations related to residual connection and LayerNorm, but denote the variables right before it with an apostrophe. The \(\)'s and \(\)'s are weights and biases.

Note that the first layer reads the word embedding, i.e., \(_{0}=^{l d}\), and the final output distribution \(^{l v}\) is given by

\[=(_{N})\] (3)

where \(v\) is the vocabulary size, \(N\) is the total number of layers, and \(^{d v}\) is the parameter matrix in the softmax layer, which is sometimes tied with the word embedding matrix at the input.

#### 3.2.1 Forms of Weight Rearrangement Attacks

Putting Equation 1 - Equation 3 together, we can systematically analyze how the parameter vector direction can be attacked through direct weight rearrangements. There are totally 3 forms of attacks that could camouflage the model without changing its architecture or affecting its output.

**1. Linear mapping attack on \(_{Q},_{K}\) and \(_{V},_{O}\).** Consider Equation 1, one can transform \(_{Q}\) and \(_{K}\) symmetrically so that the product \(_{Q}_{K}^{T}\) remains unchanged but both weights are significantly modified. This will alter the parameter vector direction significantly. Formally, for any invertible matrix \(_{1}\), let

\[_{Q}}=_{Q}_{1},_{K}}=_{K}_{1}^{-1}\] (4)

and substitute them respectively into the model, one can camouflage it as if it's a brand new model, without sacrificing any of the base model's performance. The same holds for \(_{V},_{O}\) as well.

**2. Permutation attack on \(_{1},_{1},_{2}\).** Consider Equation 2, since it consists of two fully connected layers, one can randomly permute the hidden states in the middle layer without changing its output. Formally, let \(_{FFN}\) be an arbitrary permutation matrix, one can camouflage the model without sacrificing its performance by substituting the following three matrices accordingly

\[_{1}}=_{1}_{FFN},_{2}}=_{FFN }^{-1}_{2},_{1}}=_{1}_{FFN}\] (5)

**3. Permutation attack on word embeddings.** In a similar spirit, one can permute the dimensions in the word embedding matrix as well, although it would require all remaining parameters to be permuted accordingly. Formally, let \(_{E}\) be an arbitrary permutation matrix that permutes the dimensions in \(\) through \(}=_{E}\), due to the existence of the residual connections, the output of all layers have to be permuted in the same way, i.e., \(_{n}}=_{n}_{E}\). Note that it's not necessarily the case in the former two types of attacks. This permutation has to be canceled out at the final softmax layer (Equation 3), by permuting the dimensions in \(\) accordingly, i.e. \(}=_{E}^{-1}\). Specifically, all remaining parameters have to be permuted in the following way:

\[&_{Q}}=_{E}^{-1}_{Q}, _{K}}=_{E}^{-1}_{K},_{V}}=_{E} ^{-1}_{V},_{O}}=_{O}_{E}\\ &_{1}}=_{E}^{-1}_{1},_ {2}}=_{2}_{E},_{2}}=_{2}_{E} \] (6)

Moreover, putting everything together, one can combine all the aforementioned three types of attacks altogether. Formally, the parameters can be camouflaged as:

\[&_{Q}}=_{E}^{-1}_{Q}_{1},_{K}}=_{E}^{-1}_{K}_{1}^{-T},_{V}}=_{E}^{-1}_{V}_{2},_{O}}= _{2}^{-1}_{O}_{E}\\ &_{1}}=_{E}^{-1}_{1}_{FFN}, _{1}}=_{1}_{FFN},_{2}}=_{FFN}^{-1}_{2}_{E},_{2}}=_{2}_{E} \\ &}=_{E},}=_{E}^{- 1}\] (7)

Note that for permutation matrix we have \(^{-1}=^{T}\). This includes all possible attacks that 1) do not change the model architecture, and 2) do not affect the model's output.

#### 3.2.2 The Invariant Terms to These Attacks

To find the invariant terms under all these attacks, we need to combine terms in Equation 7 to get the invariant term that nicely cancels out all extra camouflaging matrices. To this end, we construct 3 invariant terms:

\[_{a}=}_{Q}_{K}^{T}}^{T},_{b} =}_{V}_{O}}^{T},_{f}=} _{1}_{2}}^{T}\] (8)

Note that for \(}\) in these terms, we don't include all tokens from a vocabulary or tokens in a specific sentence; instead, we select a subset of tokens. There are two problems if we directly use all tokens' embeddings \(\). First, using the whole embedding matrix will make the terms unnecessarily large and of variable size between different models. Second, more importantly, since it is common to inherit a base model with an augmented vocabulary, i.e., to append a set of new tokens at the end of the original vocabulary, the invariant terms would have different sizes and be incomparable. Third, if we designate specific tokens instead, the selected tokens may not always exist in all LLMs being tested. Consequently, we carefully choose the tokens to be included in \(}\), by following these steps:

1. Select a sufficiently big corpus as a standard verifying corpus.

2. Tokenize the corpus with the LLM's vocabulary and sort all tokens according to their frequency.
3. Delete all tokens in the vocabulary that don't show up in the corpus.
4. Among the remaining tokens, select the least frequent \(K\) tokens as the tokens to be included in \(}\).

Here, using a standard corpus ensures that the resulting tokenization will be identical if a certain model's vocabulary is a subset of another; the sufficiently large corpus stabilizes the frequencies of tokens in the vocabulary and provides enough chance for as many tokens as possible to show up. Deleting zero-shot tokens automatically sweeps off augmented tokens. Selecting the rarest tokens minimizes potential affections brought by parameter updates in subsequent training processes. A properly large \(K\) will ensure a large enough set of tokens is included, making the resulting invariant terms more generally representative. More importantly, it will make all the invariant terms have the same size across all LLMs, regardless of their original sizes, i.e., \(_{a},_{b},_{f}^{K K}\), regardless of the index of the layer or LLM sizes.

As a result, we can tile up them to form a 3D input tensor \(^{K K C}\), where \(C\) is the channel dimension. If we are using all layers, \(C=3N\). Again, in order to make \(\) the same size across all models, we only involve the last \(r\) layers in the LLM3. We show the cosine similarity between the invariant terms in Table 1, they still preserve a high correlation to the base model.

## 4 Mapping the Invariant Terms to Image and Publish it through ZKP

Although invariant terms serve as robust and effective representations for LLMs, we cannot directly publish them due to the potential risk of leaking hidden information, including the size, statistical features, and distribution of parameters. Therefore, we further process invariant terms by mapping them into an image through the fingerprinting model and then publish the image fingerprint instead. This approach helps mitigate the risk of leakage while providing a human-readable fingerprint.

The fingerprinting model consists of a neural network encoder and an off-the-shelf image generator as depicted in Figure 4. In principle, the encoder takes as input the invariant terms of a certain model, tile them together, and deterministically maps them to a vector that appears to be filled with Gaussian variables. The subsequent image generator reads this vector and maps it to a natural image. Importantly, throughout the process, the locality of the inputs has to be preserved from end to end. i.e., similar invariable terms should result in similar Gaussian variables and finally similar images.

Figure 4: The training and inference of our fingerprinting model.

### Training

The encoder is the only component that needs to be trained in our fingerprinting model. Note that we don't need to use any real LLM weights for training the encoder (all the training data are synthesized by randomly sampled matrices), as it only needs to learn a locality-preserving mapping between the input tensor and the output Gaussian vector. We adopt contrastive learning to learn locality-preserving mapping. To render the output vector to be Gaussian, we adopt the standard GAN (Karras et al., 2019) training scheme. (c.f. Appendix B for details of data synthesis and the whole training process.)

### Inference

In the inference stage, the encoder takes the invariant terms from real LLMs and outputs \(\). One image generator converts \(\) into a natural image. In principle, any image generator that takes a Gaussian input and has the locality-preserving property would fit here. By visually representing the invariant terms as fingerprints, we can easily identify base models based on their fingerprint images, enabling reliable tracking of model origins. In this paper, we employ the StyleGAN2 generator pretrained on the AFHQ (Choi et al., 2020) dog dataset to generate natural images, we detailed it in Appendix E.

### Zero-knowledge Proof for Fingerprints

In our black-box setting, users are unable to access the model parameters, which presents a significant challenge in ensuring the fingerprint is genuinely derived from the claimed LLM parameters. To address this issue, we employ zero-knowledge proof, a cryptographic technique that allows the prover to convince the verifier that a statement is true without revealing any information beyond the statement's validity (Ben-Sasson et al., 2013; Goldwasser et al., 2019; Chiesa et al., 2020).

The manufacturer generates a publicly verifiable zero-knowledge proof along with computing the fingerprint, ensuring two critical aspects: (1) the input parameters indeed originate from the specific LLM the manufacturer claims, thereby safeguarding against substitution attack. (c.f. Appendix D for detailed discussion of substitution attack.) (2) the human-readable fingerprint is calculated correctly, confirming the fingerprint is genuinely derived from the LLM parameters. The detailed zero-knowledge proof generation process is as follows:

1. Select a random number \(t\), commit to LLM parameters (which we denote by \(model\)) and input \(}\), \((model,},t)=\). The commitment \(\) is public and does not reveal any information about the model.
2. While calculating fingerprint, generate a ZK proof \(_{1}\) prove that the manufacturer knows \(model,},t\) s.t. 1. \(model\) is the claimed LLM parameters and \(}\) satisfy \((model,},t)=\); 2. The last two layers parameters \(_{Q},_{K},_{V},_{O},_{1},_{2}\) in \(model\) and input \(}\) satisfy \[_{a}=}_{Q}_{K}^{T}}^{T},_{ b}=}_{V}_{O}}^{T},_{f}=} _{1}_{2}}^{T}\] (9)
3. The output human-readable fingerprint is indeed calculated from the invariant terms above.

As above( item 1. and item 2.a), the manufacturers commit to the claimed model and publish the commitment first, which is a conventional approach to ensure the parameters are not altered during the proof generation. All subsequent proof and inference processes will be carried out with this commitment, and anyone can verify if the model parameters used match those sealed within the commitment. The steps item 2.b and item 2.c are to ensure that the invariant terms and fingerprint are correctly calculated. Anyone who gets proof \(_{1}\) and the commitment \(\) can verify that the fingerprint is calculated based on LLM.

Moreover, we also provide a limited quantitative comparison scheme, which supports one-to-one comparison with open-source models. The manufacturers calculate the cosine similarity of invariant terms and give the zero-knowledge proof \(_{2}\) of this calculation process. Anyone who gets the proof \(_{2}\) and the open-source model can verify the cosine similarity without learning the private model.

## 5 Experiments

Our experiment is twofold. First, we validated the effectiveness and robustness of invariant terms for identifying the base model. Second, we generated fingerprints based on invariant terms for 80 LLMs and quantitatively assessed their discrimination ability through a human subject study.

### Effectiveness and Robustness of Invariant Terms

In this subsection, we validate the effectiveness and robustness of invariant terms in identifying base model through four key experiments. First, we compute the **In**variant Terms' **C**osine **S**imilarity (ICS) between 8 widely used open-sourced LLM base models and their offspring models (including heavily continue-pretrained models), verifying its robustness against subsequent training processes. Second, we conduct extensive experiments on additional open-sourced LLMs, showcasing low ICS between 28 independently trained models. Third, we gather 51 offspring models and calculate the accuracy of correctly identifying the base model. Finally, we compare our methods with two latest baselines.

#### 5.1.1 High ICS between Base LLMs and Their Offspring Models

First, we perform experiments on 7 commonly used open-sourced LLMs, ranging in size from 7B to 40B. The 7 base models considered are Falcon-40B (Almazrouei et al., 2023), MPT-30B (Lin et al., 2022), LLaMA2-7B, 13B, Qwen-7B (Bai et al., 2023), Internlm-7B and Baichuan-13B. For each of these base models, we collect 2 popular offspring models. We extract the invariant terms for all these models and calculate the ICS for each offspring model w.r.t. its base model (Table 2). Remarkably, all offspring models exhibit very high ICS, with an **average ICS** of \(\).

Second, we leverage the LLaMA-7B base model as a testing ground to assess the robustness of invariant terms under diverse subsequent training processes. We include 10 offspring models detailed in Section 3.1.1 and add Beaver, Guanaco (Dettmers et al., 2023), and BiLLa (Li, 2023) to the collection. See Appendix Table 10 for detailed descriptions. We extract invariant terms following the previous settings and compute the cosine similarity of the invariant terms (ICS) between each pair of models. Despite undergoing various training paradigms, such as RLHF, SFT, modality extension, and continued pretraining in a new language, we observe a high degree of similarity (Table 3), with an **average ICS** of \(\).

  
**ICS** &  &  &  &  &  &  &  \\  Offspring1 & 99.61 & 99.50 & 99.99 & 99.47 & 98.98 & 99.76 & 99.28 & \\ Offspring2 & 99.69 & 99.49 & 99.99 & 99.41 & 99.71 & 99.98 & 99.02 & \\   

Table 2: The ICS between offspring models and their corresponding base model.

  
**ICS** &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  LLaMA & 100.00 & 99.20 & 99.95 & 99.86 & 99.42 & 99.89 & 99.60 & 99.60 & 99.60 & 91.35 & 99.63 & 93.57 & 99.97 & 92.62 & 82.56 \\ MiGPT & 99.20 & 100.00 & 99.17 & 99.10 & 99.10 & 99.15 & 98.83 & 98.82 & 90.65 & 99.00 & 92.84 & 99.19 & 91.93 & 82.24 \\ Alpaca & 99.95 & 99.17 & 100.00 & 99.82 & 99.38 & 99.85 & 99.55 & 99.57 & 91.31 & 99.59 & 93.53 & 99.97 & 92.59 & 82.52 \\ Malpaca & 99.86 & 99.10 & 99.82 & 100.00 & 99.31 & 99.76 & 99.46 & 99.47 & 91.23 & 99.51 & 93.45 & 99.84 & 92.50 & 82.51 \\ Vienna & 99.42 & 99.10 & 99.38 & 99.31 & 100.00 & 99.35 & 99.05 & 99.04 & 90.84 & 99.15 & 93.04 & 99.41 & 92.14 & 82.28 \\ Wizard & 99.89 & 99.15 & 99.85 & 99.76 & 99.35 & 100.00 & 99.50 & 99.50 & 91.25 & 99.56 & 93.47 & 99.87 & 92.52 & 82.57 \\ Baire & 99.60 & 98.83 & 99.55 & 99.46 & 99.05 & 99.00 & 100.00 & 99.23 & 100.00 & 99.92 & 93.19 & 99.57 & 92.25 & 82.25 \\ AlpacaL & 99.60 & 99.82 & 99.57 & 99.47 & 99.04 & 99.05 & 99.23 & 100.00 & 99.99 & 92.49 & 93.21 & 99.59 & 92.31 & 82.30 \\ Calpaca & 91.35 & 90.65 & 91.31 & 91.23 & 90.84 & 91.25 & 90.97 & 90.99 & 100.00 & 91.04 & 97.44 & 91.33 & 85.19 & 75.60 \\ Koala & 99.63 & 99.00 & 99.59 & 99.51 & 99.15 & 99.56 & 99.25 & 99.24 & 910.00 & 100.00 & 93.23 & 99.61 & 92.27 & 82.34 \\ CLaMA & 93.57 & 92.84 & 93.53 & 93.45 & 93.49 & 93.47 & 93.19 & 93.91 & 97.44 & 93.23 & 100.00 & 93.55 & 86.00 & 77.41 \\ Beaver & 99.97 & 99.19 & 99.97 & 99.84 & 99.41 & 98.97 & 99.57 & 99.59 & 91.33 & 99.61 & 93.55 & 100.00 & 92.60 & 82.57 \\ Guano & 92.62 & 91.93 & 92.59 & 92.50 & 92.14 & 92.52 & 92.25 & 92.31 & 85.19 & 92.27 & 86.80 & 92.60 & 100.00 & 77.17 \\ BiLLa & 82.56 & 82.24 & 82.52 & 82.51 & 82.28 & 82.57 & 82.25 & 82.30 & 75.60 & 82.34 & 77.41 & 82.57 & 77.17

#### 5.1.2 Low ICS between 28 Independently Trained LLMs

Besides the aforementioned base models, we assemble a comprehensive collection of 28 open-sourced LLMs, ranging in size from 774M (GPT2-Large) to 180B (Falcon-180B). Please refer to Appendix F for details. We extract invariant terms and calculate ICS between each pair of models. Notably, the similarities between different models were consistently low, with an **average ICS** of \(\), affirming the effectiveness of invariant terms. (c.f. Appendix Table 7 for detailed ICSs.)

#### 5.1.3 Accuracy in Identify 51 Offspring Models' Base Model

To assess the effectiveness of our method, we gathered 51 offspring models derived from 18 distinct base models. (c.f. Table 10 for detailed list and description.) Calculating the ICS between each offspring model and the 18 base models, we predicted the base model with the highest ICS. Comparing these predictions with the ground truth, our method accurately identified the base models for all 51 offspring LLMs, achieving \(\)**accuracy**.

#### 5.1.4 Comparing to Latest Fingerprinting Methods

There are few fingerprinting methods designed for LLMs. Trap (Gubri et al., 2024) optimizes adversarial suffixes to elicit specific responses, while IF (Xu et al., 2024) fintuned LLMs to make them generate predefined answers. We tested the Fingerprint Success Rate (FSR) (Gu et al., 2022) of these methods on LLaMA's offspring models. Our method demonstrates superior performance (Table 4), even when compared to the white-box method IF\({}_{}\). (More illustrations in Appendix L.)

### Discrimination Ability of Human-readable Fingerprints

Based on previous invariant terms, we employ the fingerprinting model illustrated in Section 4 to generate and publish human-readable fingerprints for previously mentioned LLMs. In Figure 5, there are fingerprints of the 7 independently trained LLMs and their offspring models (Section 5.1.1).

  
**FSR** & MiGPT & Alpaca & MAplaca & Vicuna & Wizard & Baire & Alpaca & CAplaca & Kola & CLaMA & Beaver & Guanaco & BiLa & Avg. \\  Trap & 0 & 8.04 & 24.14 & 2.30 & 17.24 & 44.83 & 39.08 & 1.15 & 0 & 0 & 8.05 & 10.34 & 0 & **11.94** \\ \(^{1}_{}\) & 0 & 10 & 0 & 0 & 0 & 40 & 10 & 0 & 0 & 0 & 10 & 0 & 0 & **5.38** \\ \(^{2}_{}\) & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 20 & 100 & 50 & 100 & 30 & 100 & **84.62** \\ Ours & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & **100.00** \\   

Table 4: Different methods’ FSR on various LLaMA’s offspring models. \(^{1}_{}\) and \(^{2}_{}\) represent two different experimental settings of IF, with the former using all parameters and the latter only using the embedding parameter. Abbreviations are consistent with Table 3.

Figure 5: Fingerprints of 7 different base models (in the first row) and their corresponding offspring models (the lower two rows) are presented. The base model’s name is omitted in the offspring models.

Notably, for all the offspring models, their fingerprints closely resemble those of their base models. On the other hand, LLMs based on different models yield highly distinctive fingerprints, encompassing various appearances and breeds of dogs. Due to space limit, the fingerprints of LLaMA family models, the rest offspring models, and the 28 independently trained LLMs are listed in Appendix H.

Furthermore, we conducted a human subject study and yielded a \(\)**accuracy** rate (c.f. Appendix I for details), quantitatively demonstrate the discrimination ability of our generated fingerprints. Although using human-readable fingerprints introduces minor losses, manufacturers can provide one-to-one comparison results with proof to make up for this loss of misjudgment.

Except for the aforementioned experiments, we independently train LLMs on a smaller scale to provide further validation for our method. (c.f. Appendix G)

## 6 Conclusion

In this paper, we introduce a novel approach that generates a human-readable fingerprint for LLM. Owing to Zero-Knowledge Proof, all fingerprinting steps are internally conducted by the LLMs owners. Our method is actually a black-box method as only the image fingerprint and corresponding proof need to be released. There is no exposure of model weights or information leakage to the public throughout the entire process. Furthermore, we detailed our works' limitations in Appendix K.

## 7 Acknowledgements

We would like to thank Shiyu Liang, Siyuan Huang, and the anonymous reviewers for helpful discussions and feedback. This work was sponsored by the National Key Research and Development Program of China (No. 2023ZD0121402) and National Natural Science Foundation of China (NSFC) grant (No.62106143).