# CycleNet: Rethinking Cycle Consistency in

Text-Guided Diffusion for Image Manipulation

 Sihan Xu\({}^{1}\) Ziqiao Ma\({}^{1}\) Yidong Huang\({}^{1}\) Honglak Lee\({}^{1,2}\) Joyce Chai\({}^{1}\)

\({}^{1}\)University of Michigan, \({}^{2}\)LG AI Research

{sihanxu,marstin,owenhji,honglak,chaijy}@umich.edu

Equal contribution.

###### Abstract

Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces CycleNet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate CycleNet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that CycleNet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. CycleNet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train.

## 1 Introduction

Recently, pre-trained diffusion models (DMs)  have enabled an unprecedented breakthrough in image synthesis tasks. Compared to GANs  and VAEs , DMs exhibit superior stability and quality in image generation, as well as the capability to scale up to open-world multi-modal data. As such, pre-trained DMs have been applied to image-to-image (I2I) translation, which is to acquire a mapping between images from two distinct domains, e.g., different scenes, different

Figure 1: A high-resolution example of CycleNet for diffusion-based image-to-image translation compared to other diffusion-based methods. CycleNet produces high-quality translations with satisfactory consistency. The areas in the boxes are enlarged for detailed comparisons.

objects, and different object states. For such translations, text-guided diffusion models typically require mask layers [32; 2; 7; 1] or attention control [10; 30; 25; 35]. However, the quality of masks and attention maps can be unpredictable in complex scenes, leading to semantic and structural changes that are undesirable. Recently, researchers have explored using additional image-conditioning to perform paired I2I translations with the help of a side network  or an adapter . Still, it remains an open challenge to adapt pre-trained DMs in _unpaired_ I2I translation with a _consistency_ guarantee.

We emphasize that _consistency_, a desirable property in image manipulation, is particularly important in unpaired I2I scenarios where there is no guaranteed correspondence between images in the source and target domains. Various applications of DMs, including video prediction and infilling , imagination-augmented language understanding , robotic manipulation [18; 8] and world models , would rely on strong consistency across the source and generated images.

To enable unpaired I2I translation using pre-trained DMs with satisfactory consistency, this paper introduces CycleNet, which allows DMs to translate a source image by conditioning on the input image and text prompts. More specifically, we adopt ControlNet  with pre-trained Stable Diffusion (SD)  as the latent DM backbone. Motivated by cycle consistency in GAN-based methods , CycleNet leverages consistency regularization over the image translation cycle. As illustrated in Figure 2, the image translation cycle includes a forward translation from \(x_{0}\) to \(_{0}\) and a backward translation to \(_{0}\). The key idea of our method is to ensure that when conditioned on an image \(c_{}\) that falls into the target domain specified by \(c_{}\), the DM should be able to reproduce this image condition through the reverse process.

We validate CycleNet on I2I translation tasks of different granularities. Besides the scene and object level tasks introduced by Zhu et al. , we additionally contribute ManiCups, a multi-domain I2I translation dataset for manipulating physical state changes of objects. ManiCups contains 6k images of empty cups and cups of coffee, juice, milk, and water, collected from human-annotated bounding boxes. The empirical results demonstrate that compared to previous approaches, CycleNet is superior in translation faithfulness, cycle consistency, and image quality. Our approach is also computationally friendly, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Further analysis shows that CycleNet is a robust zero-shot I2I translator, which can generate faithful and high-quality images for out-of-domain distributions with a simple change of the textual prompt. This opens up possibilities to develop consistent diffusion-based image manipulation models with image conditioning and free-form language instructions.

## 2 Preliminaries

We start by introducing a set of notations to characterize image-to-image translation with DMs.

Diffusion ModelsDiffusion models progressively add Gaussian noise to a source image \(z_{0} q(z_{0})\) through a forward diffusion process and subsequently reverse the process to restore the original image. Given a variance schedule \(_{1},,_{T}\), the forward process is constrained to a Markov chain \(q(z_{t}|z_{t-1}):=(z_{t};}z_{t-1},_{t})\), in which \(z_{1:T}\) are latent variables with dimensions matching \(z_{0}\). The reverse process \(p_{}(z_{0:T})\) is as well Markovian, with learned Gaussian transitions that begin at \(z_{T}(0,)\). Ho et al.  noted that the forward process allows the sampling of \(z_{t}\) at any time step \(t\) using a closed-form sampling function (Eq. 1).

\[z_{t}=S(z_{0},,t):=_{t}}z_{0}+_{t}},\;(0,)t[1,T]\] (1)

in which \(_{t}:=1-_{t}\) and \(_{t}:=_{s=1}^{t}_{s}\). Thus, the reverse process can be carried out with a UNet-based network \(_{}\) that predicts the noise \(\). By dropping time-dependent variances, the model can be trained according to the objective in Eq. 2.

\[_{}_{z_{0},,t}\;||-_{ }(z_{t},t)||_{2}^{2}\] (2)

Eq. 2 implies that in principle, one could estimate the original source image \(z_{0}\) given a noised latent \(z_{t}\) at any time \(t\). The reconstructed \(_{0}\) can be calculated with the generation function:

\[_{0}=G(z_{t},t):=z_{t}-_{t}}_{ }(z_{t},t)/_{t}}\] (3)

For simplicity, we drop the temporal conditioning \(t\) in the following paragraphs.

Conditioning in Latent Diffusion ModelsLatent diffusion models (LDMs) like Stable Diffusion  can model conditional distributions \(p_{}(z_{0}|c)\) over condition \(c\), e.g., by augmenting the UNet backbone with a condition-specific encoder using cross-attention mechanism . Using textual prompts is the most common approach for enabling conditional image manipulation with LDMs. With a textual prompt \(c_{z}\) as conditioning, LDMs strive to learn a mapping from a latent noised sample \(z_{t}\) to an output image \(z_{0}\), which falls into a domain \(\) that is specified by the conditioning prompt. To enable more flexible and robust conditioning in diffusion-based image manipulation, especially a mixture of text and image conditioning, recent work obtained further control over the reverse process with a side network  or an adapter . We denote such conditional denoising autoencoder as \(_{}(z_{t},c_{},c_{})\), where \(c_{}\) is the image condition and the text condition \(c_{}\). Eq. 3 can thus be rewritten as:

\[_{0}=G(z_{t},c_{},c_{}):=z_{t}-_{t}}_{}(z_{t},c_{},c_{}) /_{t}}\] (4)

The text condition \(c_{}\) contains a pair of conditional and unconditional prompts \(\{c^{+},c^{-}\}\). A conditional prompt \(c^{+}\) guides the diffusion process towards the images that are associated with it, whereas a negative prompt \(c^{-}\) drives the diffusion process away from those images.

Consistency Regularization for Unpaired Image-to-Image TranslationThe goal of unpaired image-to-image (I2I) translation is to learn a mapping between two domains \(^{d}\) and \(^{d}\) with unpaired training samples \(\{x_{i}\}\) for \(i=1,,N\), where \(x_{i}\) belongs to \(X\), and \(\{y_{j}\}\) for \(j=1,,M\), where \(y_{j}\). In traditional GAN-based translation frameworks, the task typically requires two mappings \(G:\) and \(F:\). **Cycle consistency** enforces transitivity between forward and backward translation functions by regularizing pairs of samples, which is crucial in I2I translation, particularly in unpaired settings where no explicit correspondence between images in source and target domains is guaranteed [55; 27; 50; 44]. To ensure cycle consistency, CycleGAN  explicitly regularizes the translation cycle, bringing \(F(G(x))\) back to the original image \(x\), and vice versa for \(y\). Motivated by consistency regularization, we seek to enable consistent unpaired I2I translation with LDMs. Without introducing domain-specific generative models, we use one single denoising network \(_{}\) for translation by conditioning it on text and image prompts.

## 3 Method

In the following, we discuss only the translation from domain \(\) to \(\) due to the symmetry of the backward translation. Our goal, at inference time, is to enable LDMs to translate a source image \(x_{0}\) by using it as the image condition \(c_{}=x_{0}\), and then denoise the noised latent \(y_{t}\) to \(y_{t-1}\) with text prompts \(c_{}=c_{y}\). To learn such a translation model \(_{}(y_{t},c_{y},x_{0})\), we consider two types of training objectives. In the following sections, we describe the **cycle consistency regularization** to ensure cycle consistency so that the structures and unrelated semantics are preserved in the generated images, and the **self regularization** to match the distribution of generated images with the target

Figure 2: The image translation cycle includes a forward translation from \(x_{0}\) to \(_{0}\) and a backward translation to \(_{0}\). The key idea of our method is to ensure that when conditioned on an image \(c_{}\) that falls into the target domain specified by \(c_{}\), the LDM should reproduce this image condition through the reverse process. The dashed lines indicate the regularization in the loss functions.

domain, As illustrated in Figure 2, the image translation cycle includes a forward translation from a source image \(x_{0}\) to \(_{0}\), followed by a backward translation to the reconstructed source image \(_{0}\).

### Cycle Consistency Regularization

We assume a likelihood function \(P(z_{0},c_{})\) that the image \(z_{0}\) falls into the data distribution specified by the text condition \(c_{}\). We consider a generalized case of cycle consistency given the conditioning mechanism in LDMs. If \(P(c_{},c_{})\) is close to 1, i.e., the image condition \(c_{}\) falls exactly into the data distribution described by the text condition \(c_{}\), we should expect that \(G(z_{t},c_{},c_{},c_{})=c_{}\) for any noised latent \(z_{t}\). With the translation cycle in Figure 2, the goal is to optimize (1) \(_{x x}=_{x_{0},_{x}}\)\(||x_{0}-G(x_{t},c_{x},x_{0})||_{2}^{2}\); (2) \(_{y y}=_{x_{0},_{x},_{y}}\)\(||_{0}-G(y_{t},c_{y},_{0})||_{2}^{2}\); (3) \(_{x y x}=_{x_{0},_{x},_{y}}\)\(||x_{0}-G(y_{t},c_{x},x_{0})||_{2}^{2}\); and (4) \(_{x y y}=_{x_{0},_{x}}\)\(||_{0}-G(x_{t},c_{y},_{0})||_{2}^{2}\).

**Proposition 1** (Cycle Consistency Regularization).: _With the translation cycle in Figure 2, a set of consistency losses is given by dropping time-dependent variances:_

\[_{x x} =_{x_{0},_{x}}\)\(||_{}(x_{t},c_{x},x_{0})-_{x}||_{2}^{2}\) (5) \[_{y y} =_{x_{0},x_{},_{y}}\)\(||_{}(y_{t},c_{y},_{0})-_{y}||_{2}^{2}\) (6) \[_{x y x} =_{x_{0},_{x},_{y}}\)\(||_{}(y_{t},c_{x},x_{0})+_{}(x_{t},c_{y},x_{0})- _{x}-_{y}||_{2}^{2}\) (7) \[_{x y y} =_{x_{0},_{x}}\)\(||_{}(x_{t},c_{y},x_{0})-_{}(x_{t},c_{y},_{0}) ||_{2}^{2}\) (8)

We leave the proof in Section A.2. Proposition 1 states that pixel-level consistency can be acquired by regularizing the conditional denoising autoencoder \(_{}\). Specifically, the **reconstruction loss**\(_{x x}\) and \(_{y y}\) ensures that CycleNet can function as a LDM to reverse an image similar to Eq. 2. The cycle **consistency loss**\(_{x y x}\) serves as the transitivity regularization, which ensures that the forward and backward translations can reconstruct the original image \(x_{0}\). The **invariance loss**\(_{x y y}\) requires that the target image domain stays invariant under forward translation, i.e., given a forward translation from \(x_{t}\) to \(_{0}\) conditioned on \(x_{0}\), repeating the translation conditioned on \(_{0}\) would reproduce \(_{0}\).

### Self Regularization

In the previous section, while \(x_{0}\) is naturally sampled from domain \(\), we need to ensure that the generated images fall in the target domain \(\), i.e., the translation leads to \(G(x_{t},c_{y},x_{0})\). Our goal is therefore to maximize \(P(_{0},c_{y})\), or equivalently to minimize

\[_{}=-_{x_{0},_{x}}PG S(x_{0},),c_{y},x_{0},c_{y}\] (9)

**Assumption 1** (Domain Smoothness).: _For any text condition, \(P(,c_{})\) is \(L\)-Lipschitz._

\[\ L<,\ |P(z_{0}^{1},c_{})-P(z_{0}^{2},c_{} )| L||z_{0}^{1}-z_{0}^{2}||_{2}\] (10)

**Proposition 2** (Self Regularization).: _Let \(_{}^{*}\) denote the denoising autoencoder of the pre-trained text-guided LDM backbone. Let \(x_{t}=S(x_{0},_{x})\) be a noised latent. A self-supervised upper bound of \(_{}\) is given by:_

\[_{}=_{x_{0},_{x}}[L_{t}}{_{t}}}||_{}(x_{t},c_{y},x_{0})-_{}^{*}(x_{t},c_{y})||_{2}]+\] (11)

Lipschitz assumptions have been widely adopted in diffusion methods [53; 48]. Assumption 1 hypothesizes that similar images share similar domain distributions. A self-supervised upper bound \(_{}\) can be obtained in Proposition 2, which intuitively states that if the output of the conditional translation model does not deviate far from the pre-trained LDM backbone, the outcome image should still fall in the same domain specified by the textual prompt. We leave the proof in Section A.3.

### CycleNet

In practice, \(_{}\) can be minimized from the beginning of training by using a ControlNet  with pre-trained Stable Diffusion (SD)  as the LDM backbone, which is confirmed through preliminary experiments. As shown in Figure 2, the model keeps the SD encoder frozen and makes a trainable copy in the side network. Additional zero convolution layers are introduced to encode the image condition and control the SD decoder. These zero convolution layers are 1D convolutions whose initial weights and biases vanish and can gradually acquire the optimized parameters from zero. Since the zero convolution layers keep the SD encoder features untouched, \(_{}\) is minimal at the beginning of the training, and the training process is essentially fine-tuning a pre-trained LDM with a side network.

The text condition \(c_{}=\{c^{+},c^{-}\}\) contains a pair of conditional and unconditional prompts. We keep the conditional prompt in the frozen SD encoder and the unconditional prompt in the ControlNet, so that the LDM backbone focuses on the translation and the side network looks for the semantics that needs modification. For example, to translate an image of summer to winter, we rely on a conditional prompt \(l_{x}=\) and unconditional prompt \(l_{y}=\). Specifically, we use CLIP  encoder to encode the language prompts \(l_{x}\) and \(l_{y}\) such that \(c_{x}=\{(l_{x}),(l_{y})\}\) and \(c_{y}=\{(l_{y}),(l_{x})\}\).

We also note that \(_{y y}\) can be omitted, as \(_{x x}\) can serve the same purpose in the symmetry of the translation cycle from \(\) to \(\), and early experiments confirmed that dropping this term lead to significantly faster convergence. The simplified objective is thus given by:

\[_{x}=_{1}_{x x}+_{2}_{x  y y}+_{3}_{x y x}\] (12)

Consider both translation cycle from \(\), the complete training objective of CycleNet is:

\[_{}=_{x}+_{y}\] (13)

The pseudocode for training is given in Algo. 1.

### FastCycleNet

Similar to previous cycle-consistent GAN-based models for unpaired I2I translation, there is a trade-off between the image translation quality and cycle consistency. Also, the cycle consistency loss \(_{x y x}\) requires deeper gradient descent, and therefore more computation expenses during training (Table 6). In order to speed up the training process in this situation, one may consider further removing \(_{x y x}\) from the training objective, and name this variation FastCycleNet. Through experiments, FastCycleNet can achieve satisfying consistency and competitive translation quality, as shown in Table 1. Different variations of models can be chosen depending on the practical needs.

## 4 Experiments

### Benchmarks

Scene/Object-Level ManipulationWe validate CycleNet on I2I translation tasks of different granularities. We first consider the benchmarks used in CycleGAN by Zhu et al. , which contains:

* [leftmargin=*]
* (Scene Level) Yosemite summer\(\)winter: We use around 2k images of summer and winter Yosemite, with default prompts "summer" and "winter";
* (Object Level) horse\(\)zebra: We use around 2.5k images of horses and zebras from the dataset with default prompts "horse" and "zebra";
* (Object Level) apple\(\)orange: We use around 2k apple and orange images with default prompts of "apple" and "orange".

State Level ManipulationAdditionally, we introduce ManiCups1, a dataset of state-level image manipulation that tasks models to manipulate cups by filling or emptying liquid to/from containers, formulated as a multi-domain I2I translation dataset for object state changes:

* (State Level) ManiCups: We use around 5.7k images of empty cups and cups of coffee, juice, milk, and water for training. The default prompts are set as "empty cup" and "cup of <liquid>". The task is to either empty a full cup or fill an empty cup with liquid as prompted.

ManiCups is curated from human-annotated bounding boxes in publicly available datasets and Bing Image Search (under Share license for training and Modify for test set). We describe our three-stage data collection pipeline. In the **image collection** stage, we gather raw images of interest from MSCOCO , Open Images , as well as Bing Image Search API. In the **image extraction** stage, we extract regions of interest from the candidate images and resize them to a standardized size. Specifically, for subsets obtained from MSCOCO and Open Images, we extract the bounding boxes with labels of interest. All bounding boxes with an initial size less than 128\(\)128 are discarded, and the remaining boxes are extended to squares and resized to a standardized size of 512\(\)512 pixels. After this step, we obtained approximately 20k extracted and resized candidate images. We then control the data quality through a **filtering and labeling** stage. Our filtering process first discards replicated images using the L2 distance metric and remove images containing human faces, as well as cups with a front-facing perspective with a CLIP processor. Our labeling process starts with an automatic annotation with a CLIP classifier. To ensure the accuracy of the dataset, three human annotators thoroughly review the collected images, verifying that the images portray a top-down view of a container and assigning the appropriate labels to the respective domains. The resulting ManiCups dataset contains 5 domains, including 3 abundant domains (empty, coffee, juice) with more than 1K images in each category and 2 low-resource domains (water, milk) with less than 1K images to facilitate research and analysis in data-efficient learning.

To our knowledge, ManiCups is one of the first datasets targeted to the physical state changes of objects, other than stylistic transfers or type changes of objects. The ability to generate consistent state changes based on manipulation is fundamental for future coherent video prediction  as well as understanding and planning for physical agents [49; 18; 8; 46]. For additional details on data collection, processing, and statistics, please refer to Appendix B.

### Experiment Setup

BaselinesWe compare our proposed models FastCycleNet and CycleNet to state-of-the-art methods for unpaired or zero-shot image-to-image translation.

* GAN-based methods: CycleGAN  and CUT ;
* Mask-based diffusion methods: Direct inpainting with CLIPSeg  and Text2LIVE ;
* Mask-free diffusion methods: ControlNet with Canny Edge , ILVR , EGSDE , SDEdit , Pix2Pix-Zero , MasaCtrl , CycleDiffusion , and Prompt2Prompt  with null-text inversion .

TrainingWe train our model with a batch size of 4 on only one single A40 GPU.2 Additional details on the implementations are available in Appendix C.

SamplingAs shown in Figure 3, CycleNet has a good efficiency at inference time and more sampling steps lead to better translation quality. We initialize the sampling process with the latent noised input image \(z_{t}\), collected using Equation 1. Following , a standard 50-step sampling is applied at inference time with \(t=100\) for fair comparison.

### Qualitative Evaluation

We present qualitative results comparing various image translation models. Due to the space limit, additional examples will be available in Appendix E. In Figure 4, we present the two unpaired translation tasks: summer\(\)winter, and horse\(\)zebra. To demonstrate the image quality, translation quality, and consistency compared to the original images, we provide a full image for each test case

Figure 3: Step skipping during sampling. The source image is from MSCOCO .

and enlarge the boxed areas for detailed comparisons. As presented with the qualitative examples, our methods are able to perform image manipulation with high quality like the other diffusion-based methods, while preserving the structures and unrelated semantics.

In Figure 5, we present qualitative results for filling and emptying a cup: coffee\(\)empty and empty\(\)jucie. As demonstrated, image editing tasks that require physical state changes pose a significant challenge to baselines, which struggle with the translation itself and/or maintaining strong consistency. CycleNet, again, is able to generate faithful and realistic images that reflect the physical state changes.

### Quantitative Evaluation

We further use three types of evaluation metrics respectively to assess the quality of the generated image, the quality of translation, and the consistency of the images. For a detailed explanation of these evaluation metrics, we refer to Appendix C.4.

* **Image Quality**. To evaluate the quality of images, we employ two metrics: The naive Frechet Inception Distance (FID)  and FID\({}_{}\) with CLIP ;

  
**Tasks** &  &  \\ 
**Metrics** & **FID\({}_{}\)** & **FID\({}_{}\)** & **CLIP\({}_{}\)** & **PSNR\({}^{}\)** & **SSMM\({}^{}\)** & **L2\({}^{}\)** & **FID\({}_{}\)** & **FID\({}_{}\)** & **CLIP\({}^{}\)** & **LIP\({}_{}\)** & **PSNR\({}^{}\)** & **SSMM\({}^{}\)** & **L2\({}^{}\)** \\    & 133.16 & 18.55 & 22.07 & 0.20 & 16.27 & 0.39 & 36.71 & 71.88 & 27.69 & 28.07 & 0.25 & 18.53 & 0.67 & 1.39 \\  & 180.09 & 23.45 & 24.21 & 0.19 & 20.05 & 0.71 & 1.15 & 45.50 & 21.00 & 29.15 & 0.46 & 13.71 & 0.35 & 2.44 \\  \\    & 246.55 & 79.70 & 21.85 & 0.27 & 12.61 & 0.19 & 28.38 & 18.763 & 40.03 & 26.52 & 0.30 & 15.45 & 0.41 & 2.31 \\ _{}\)**} & 100.63 & 25.29 & 26.03 & 0.22 & 16.51 & 0.67 & 17.42 & 28.21 & 24.26 & 0.51 & 0.14 & 21.05 & 0.81 & 1.03 \\  \\    & 338.24 & 83.26 & 21.77 & 0.59 & 6.65 & 0.09 & 11.30 & 39.71 & 77.16 & 23.88 & 0.66 & 7.37 & 0.07 & 3.89 \\  & 105.93 & 372.24 & 22.91 & 0.59 & 10.66 & 0.16 & 3.62 & 148.45 & 40.80 & 25.95 & 0.57 & 10.24 & 0.17 & 3.57 \\  & 113.00 & 28.74 & 22.96 & 0.44 & 17.68 & 0.27 & 15.93 & 79.64 & 27.79 & 27.31 & 0.41 & 18.05 & 0.29 & 1.44 \\  & 330.98 & 79.70 & 21.88 & 0.57 & 12.63 & 0.19 & 2.83 & 98.96 & 83.32 & 24.17 & 0.66 & 9.75 & 0.11 & 4.01 \\  & 310.13 & 81.54 & 22.03 & 0.57 & 14.31 & 0.52 & 5.08 & 37.74 & 86.21 & 26.32 & 0.67 & 11.88 & 0.19 & 3.85 \\  & 106.91 & 52.38 & 20.79 & 0.36 & 16.22 & 0.36 & 0.37 & 37.31 & 68.31 & 23.15 & 0.40 & 16.31 & 0.37 & 1.83 \\  & 460.00 & 41.22 & 23.31 & 0.37 & 16.84 & 0.39 & 1.73 & 25.04 & 48.93 & 25.91 & 0.36 & 17.28 & 0.41 & 1.68 \\  & 243.98 & 62.96 & 23.26 & **0.44** & **15.06** & 0.31 & 2.20 & 347.27 & **66.80** & 25.04 & 0.57 & 11.51 & 0.21 & 3.46 \\ \) into one that belongs to the target domain \(\), given only unpaired images from each domain. Several GAN-based methods [55; 50; 27] were proposed to address this problem. In recent years, DPMs have demonstrated their superior ability to synthesize high-quality images, with several applications in I2I translation [40; 5]. With the availability of pre-trained DMs, SDEdit  changes the starting point of generation by using a noisy source image that preserves the overall structure. EGSDE  combines the merit of ILVR and SDEdit by introducing a pre-trained energy function on both domains to guide the denoising process. While these methods result in leading performance on multiple benchmarks, it remains an open challenge to incorporate pre-trained DMs for high-quality image generation, and at the same time, to ensure translation consistency.

### Cycle Consistency in Image Translation

The idea of cycle consistency is to regularize pairs of samples by ensuring transitivity between the forward and backward translation functions [41; 33; 17]. In unpaired I2I translation where explicit correspondence between source and target domain images is not guaranteed, cycle consistency plays a crucial role [55; 21; 27]. Several efforts were made to ensure cycle consistency in diffusion-based I2I translation. UNIT-DDPM  made an initial attempt in the unpaired I2I setting, training two DPMs and two translation functions from scratch. Cycle consistency losses are introduced in the translation functions during training to regularize the reverse processes. At inference time, the image generation does not depend on the translation functions, but only on the two DPMs in an iterative manner, leading to sub-optimal performance. Su et al.  proposed the DDIB framework that exact cycle consistency is possible assuming zero discretization error, which does not enforce any cycle consistency constraint itself. Cycle Diffusion  proposes a zero-shot approach for image translation based on Su et al. 's observation that a certain level of consistency could emerge from DMs, and there is no explicit treatment to encourage cycle consistency. To the best of our knowledge, CycleNet is the first to guarantee cycle consistency in unpaired image-to-image translation using pre-trained diffusion models, with a simple trainable network and competitive performance.

## 7 Conclusion

The paper introduces CycleNet that incorporates the concept of cycle consistency into text-guided latent diffusion models to regularize the image translation tasks. CycleNet is a practical framework for low-resource applications where only limited data and computational power are available. Through extensive experiments on unpaired I2I translation tasks at scene, object, and state levels, our empirical studies show that CycleNet is promising in consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt.

Future WorkThis paper is primarily concerned with the unpaired I2I setting, which utilizes images from unpaired domains during training for domain-specific applications. Although CycleNet demonstrates robust out-of-domain generalization, enabling strong zero-shot I2I translation capabilities is not our focus here. We leave it to our future work to explore diffusion-based image manipulation with image conditioning and free-form language instructions, particularly in zero-shot settings.