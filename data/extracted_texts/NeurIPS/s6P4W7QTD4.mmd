# The Probe Paradigm: A Theoretical Foundation for Explaining Generative Models

Amit Kiran Rege

Department of Computer Science

University of Colorado Boulder

Boulder, CO, USA

amit.rege@colorado.edu

###### Abstract

To understand internal representations in generative models, there has been a long line of research of using _probes_ i.e. shallow binary classifiers trained on the model's representations to indicate the presence/absence of human-interpretable _concepts_. While the focus of much of this work has been empirical, it is important to establish rigorous guarantees on the use of such methods to understand its limitations. To this end, we introduce a formal framework to theoretically study explainability in generative models using probes. We discuss the applicability of our framework to number of practical models and then, using our framework, we are able to establish theoretical results on sample complexity and the limitations of probing in high-dimensional spaces. Then, we prove results highlighting significant limitations in probing strategies in the worst case. Our findings underscore the importance of cautious interpretation of probing results and imply that comprehensive auditing of complex generative models might be hard even with white box access to internal representations.

## 1 Introduction

Generative models have achieved remarkable success in generating high-quality data across various domains, such as text, images, and audio. Models like large language models (LLMs) (Radford et al., 2019; Brown et al., 2020), diffusion models (Ho et al., 2020), and generative adversarial networks (GANs) (Goodfellow et al., 2014) have demonstrated impressive capabilities in tasks ranging from text completion to realistic image synthesis. Despite these advancements, understanding the internal mechanisms and representations learned by these models remains a significant challenge.

Probing methods have emerged as a valuable approach to interpret the hidden representations of neural networks (Alain and Bengio, 2018; Belinkov, 2022). By training simple classifiers, known as _probes_, researchers can detect the presence of human-understandable concepts within the hidden layers of a model. These concepts can range from syntactic structures in language models to visual features in image generation models.

However, probing high-dimensional representations introduces challenges related to overfitting and the expressiveness of the probes. Expressive probes, such as multi-layer perceptrons (MLPs), can perfectly fit training data with random labels, leading to misleading conclusions about the model's internal representations (Zhang et al., 2017). Conversely, simpler probes may fail to capture complex, non-linear relationships present in the data (internal representations in our case). Thus, it is necessary to strike a balance in order to use probes to understand a model's internal reasoning process.

On the other hand, there has been a flurry of work on explainability methods in the supervised, classification setting (Ribeiro et al., 2016; Lundberg and Lee, 2017). These methods, however, have been shown to be brittle (Alvarez-Melis and Jaakkola, 2018) so there has been some recent work on trying to provide _formal_ guarantees for explanations (Yadav et al., 2023; Dasgupta et al., 2022; Bhattacharjee and von Luxburg, 2024). While probing techniques have been used in a wide variety of generative models empirically (Belinkov, 2022; Hewitt and Liang, 2019), a formal framework for their analysis is lacking.

In this paper, we initiate the study of theoretical properties of explainability in generative models using a general, model-agnostic formal framework using probes. Our framework formalizes the use of probes -- auxiliary models trained to detect specific human-understandable concepts detected within the model's hidden representations. By maintaining generality, we aim to encompass a wide range of real-world probing-based explainability methods while providing a solid foundation for theoretical analysis.

To this end, we introduce the first formal framework utilizing probes in generative models, providing a rigorous foundation for explanations. Our key contributions are:

* A first formal framework to theoretically study the explainability of generative models through the lens of probing.
* Demonstration of our framework's versatility through application to diverse generative models, including language models, diffusion models, and GANs.
* Theoretical results on sample complexity for concept detection with linear probes and fundamental limitations in high-dimensional spaces.
* Theoretical validation of empirical findings from prior work about the overfitting risks of expressive probes in high-dimensional settings.
* Novel results revealing inherent limitations of probing internal representations, including challenges with fine-grained concepts and nonlinear concept detection.

Through this work, we aim to advance the theoretical understanding of explainability in generative models and offer a foundation for further research into offering rigorous guarantees for auditing and interpretability in such systems.

## 2 Formal Framework

In this section, we present our formal framework to understand concepts learned in generative models using probes.

### Definitions and Notation

We assume generative models have a layered architecture where internal representations can be extracted at each layer. When we say "internal representations" in this paper, we take these to mean dense layer outputs, attention head outputs, context embeddings, or any other intermediate computation. This allows our framework to be extremely general.

**Definition 1** (Generative Model).: _Let \(M\) be a generative model with \(L\) layers, denoted as \(\{l_{1},l_{2},,l_{L}\}\). The model maps inputs \(x\) to outputs \(y\), such that \(y=M(x)\)._

**Definition 2** (Hidden Representations).: _For an input \(x\), the hidden representation at layer \(l\) is denoted by \(h_{l}(x)_{l}\), where \(_{l}^{d_{l}}\) is the hidden space associated with layer \(l\) of dimensionality \(d_{l}\)._

A common strategy to understand model internals is to train simple, shallow classifiers (probes) on these hidden representations to detect if a particular concept, such as "face" in visual generative models or "truthfulness" in language models, is present in the model's internal activations. Specifically, we define each concept at a layer \(l\) of the model and train a binary classifier \(p_{c,l}\) to tell us if the concept \(c\) is present in the representation at that layer. Note that the choice of concept to test is inherently subjective and not amenable to formalization.

**Definition 3** (Concepts in Representation Space).: _A concept \(c\) is a human-understandable property we wish to detect in the model's hidden representations. For each layer \(l\{1,2,,L\}\), we define a probe \(p_{c,l}\) that outputs the probability that concept \(c\) is present in the hidden representation \(h_{l}(x)\)._

Thus, the concept \(c\) is detected at layer \(l\) based on the probe \(p_{c,l}(h_{l}(x))\), which returns a probability value in the range \(\) indicating the confidence that concept \(c\) is present in the hidden representation \(h_{l}(x)\).

**Definition 4** (Probes).: _A probe \(p_{c,l}\) is a probabilistic binary classifier associated with concept \(c C\) and layer \(l\{1,2,,L\}\). It outputs a probability score reflecting the likelihood that concept \(c\) is present in the hidden representation \(h_{l}(x)\)._

_The probe is a function \(p_{c,l}:_{l}\), where:_

\[p_{c,l}(h_{l}(x))=(h_{l}(x)).\]

_We assume probes belong to some pre-specified hypothesis class \(\)._

The probe is typically a linear classifier, such as a logistic regression, which outputs the probability of concept detection. The decision boundary for the concept \(c\) is defined by a linear hyperplane in the representation space \(_{l}\).

Typically, probes are trained by creating a dataset of \(\{h_{l}(x_{i}),y_{i}\}\) where \(y_{i}\) indicates the presence/absence of the concept being tested using a gold-standard annotated dataset. The exact procedure to do this depends on the modality of the data and architecture of the model (among other variables). In this paper, to remain model-agnostic, we will not concern ourselves with details involved in training such probes - we assume that such a dataset can be created and a probe can be trained.

**Definition 5** (Thresholded Concept Detection).: _To convert the probabilistic output of the probe into a binary decision, we introduce a threshold \(\). The concept \(c\) is detected at layer \(l\) if:_

\[p_{c,l}(h_{l}(x))>.\]

If no threshold is explicitly chosen, a default value of \(=0.5\) may be used to convert the probability into a binary decision.

### Examples of Generative Models within Our Framework

To illustrate the applicability of our framework, we provide examples of how different generative models fit within our formalization, mapping each component accordingly. These are meant to be fairly general to show the large scope of our framework but by no means are they exhaustive. We provide additional examples in Appendix B.

#### 2.2.1 Large Language Models (LLMs)

* **Generative Model:** The generative model \(M\) is a large language model, such as GPT-3 (Brown et al., 2020) or BERT (Devlin et al., 2018), which generates text sequences given an input prompt.
* **Hidden Representations:** For an input sequence \(x\), the hidden representation at layer \(l\) is denoted by \(h_{l}(x)_{l}^{d_{l}}\), where \(d_{l}\) is the dimensionality of the hidden states at that layer. These representations could be the activations after self-attention and feed-forward layers.
* **Concepts:** Concepts \(c\) in the context of language models could include syntactic properties (e.g., part-of-speech tags), semantic roles (e.g., agent, patient), or stylistic attributes (e.g., formality level).
* **Probes:** Probes \(p_{c,l}\) are classifiers trained to detect the presence of concept \(c\) in the hidden representation \(h_{l}(x)\). For example, a probe could be trained to predict the part-of-speech tag of a word based on its hidden representation at layer \(l\).

#### 2.2.2 Diffusion Models

* **Generative Model** The generative model \(M\) is a diffusion model used for image generation, such as Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020), which generate images through an iterative denoising process.
* **Hidden Representations:** For an initial noise vector \(x\), the hidden representation at diffusion step \(l\) is \(h_{l}(x)_{l}^{d_{l}}\). These representations correspond to the intermediate noisy images or latent variables at each step of the diffusion process.

* **Concepts:** Concepts \(c\) could include the presence of specific objects (e.g., "cat", "car"), styles (e.g., "impressionist", "photorealistic"), or attributes (e.g., color schemes, textures) in the images being generated.
* **Probes:** Probes \(p_{c,l}\) are trained to detect whether a concept \(c\) is present in the hidden representation \(h_{l}(x)\) at diffusion step \(l\). For example, a probe could predict whether the image at a certain diffusion step contains a particular object or style.

### Mapping Components to Our Framework

In all these examples, we can map the components of our framework as follows:

* **Generative Model (\(M\))**: The model responsible for generating data, such as text or images.
* **Hidden Representations (\(h_{l}(x)\))**: Intermediate activations or latent variables at different layers or steps within the model.
* **Concepts (\(c\))**: Human-understandable properties or attributes that we aim to detect within the model's representations.
* **Probes (\(p_{c,l}\))**: Classifiers trained to identify the presence of concepts within the hidden representations at specific layers or steps.

Our framework provides a unified approach to analyze and interpret various generative models by examining how concepts are represented internally at different levels of abstraction.

### Basic Results

Typically, a practitioner will decide the concept they want to test for and then provide the model with a ground truth dataset to produce "good" internal representations. These are then recorded and used to train linear probes across all the layers in the model. If a concept cannot be detected at a particular layer, the probe will be as good as random guessing. In practice, it is not known which layer a particular concept exists at apriori, thus, probes are trained to detect a concept on all layers and the probe with the highest accuracy is chosen to study the concept of interest.

We begin our theoretical anlysis by first studying the number of samples required to learn a concept with high accuracy using this strategy (across all layers). All proofs (unless specified) are in Appendix A.

**Proposition 1** (Sample Complexity for Multi-Layer Concept Detection).: _Let \(M\) be a generative model with \(L\) layers, \(c\) a concept to detect, and \(_{l}^{d_{l}}\) the hidden space for layer \(l\). Using linear probes \(p_{c,l}\) to detect concept \(c\) at each layer independently, and selecting the best performing probe based on a validation set, the total sample complexity to achieve an error of at most \(\) with probability at least \(1-\) is:_

\[n=O(}(d_{}+) ),\]

_where \(d_{}=_{l\{1,2,,L\}}d_{l}\) is the maximum dimensionality across all layers._

The theorem is a straightforward application of standard PAC learning results combined over all layers in the model.

Next, we attempt to understand the limitations of using linear probes in our framework. To do this, we examine the capacity of the representation space to encode independent concepts.

**Definition 6** (Independent Concepts).: _Two concepts \(c_{1}\) and \(c_{2}\) are considered independent in a representation space \(^{d}\) if their corresponding linear probes \(p_{1}\) and \(p_{2}\) have weight vectors \(w_{1}\) and \(w_{2}\) that are linearly independent._

This definition captures the idea that independent concepts should be detectable by probes that focus on different aspects of the representation space.

**Proposition 2** (Maximum Independent Concepts for Linear Probes).: _Let \(^{d}\) be the hidden space for a layer in a generative model. The maximum number of mutually independent concepts that can be represented by linear probes is \(d\)._Proof.: A linear probe \(p\) for a concept \(c\) in the hidden space \(\) is represented as \(p(h)=(w^{T}h+b)\), where \(w^{d}\) is the weight vector and \(b\) is a scalar bias. The decision boundary of this probe is the hyperplane \(w^{T}h+b=0\).

The maximum number of linearly independent weight vectors \(w\) in \(^{d}\) is \(d\). This directly corresponds to the maximum number of linearly independent hyperplanes that can be defined in \(d\)-dimensional space, which in turn represents the maximum number of independent concepts detectable by linear probes. 

This theorem has several implications for our framework. It establishes an upper bound on the number of independent concepts detectable using linear probes in a \(d\)-dimensional hidden space, informing the expressiveness of our representation space and the limitations of our probing approach. When trying to look for multiple concepts within a given representation space, we must consider that beyond \(d\) concepts, some level of dependence or overlap is guaranteed. This result also guides the required dimensionality of the hidden space for representing a given number of independent concepts i.e. a larger representation space potentially allows for more non-overlapping concepts to be present.

Such limitations in expressivity have led to some prior work considering using more expressive probes such as MLPs (Hewitt & Liang, 2019; Belinkov, 2022). The idea is that certain complex concepts will only show up non-linearly in the representation space. However, as we prove in the next section, this can cause the issue of now having to decipher whether the probe is actually showing us something meaningful about the representation space or just very good at the getting a high accuracy. Besides, the entire point of using probes was to have explainability - if we use MLPs, one has to now presumably explain the MLPs themselves as well.

### Limitations of Probing in High Dimensions

We now present a result that formalizes the phenomenon of probes fitting random labels in high-dimensional spaces. We show that given sufficiently expressive probes (such as MLPs), a practitioner examining a model can be mislead into thinking that the model encodes a concept when it does not. The proof goes through via standard concentration of measure arguments.

**Theorem 1** (Limitations of Probing in High Dimensions).: _Let \(M\) be a generative model with \(L\) layers, and let \(h_{l}(x)^{d}\) be the hidden representation at layer \(l\). Let \(\) be a set of concepts, and \(\) be a set of probes including sufficiently expressive neural networks. Assume that \(h_{l}(x)\) is drawn from a standard normal distribution \((0,I_{d})\)._

_For any \(>0\) and \(>0\), there exists a dimension \(D=O(})\) such that for all \(d>D\), the following holds with probability at least \(1-\) over the choice of a random concept \(c\) (i.e., a random labeling):_

1. _There exists a probe_ \(p\) _that achieves perfect training accuracy on_ \(n=(d)\) _samples: PERF_\({}_{}(p,h_{l},c)=1\)_._
2. _The expected test accuracy of_ \(p\) _is PERF_\({}_{}(p,h_{l},c) 0.5+\)_._
3. _The mutual information_ \(I(c;h_{l})\)_._

This result demonstrates that in high-dimensional spaces, expressive probes can perfectly fit random labels on training data, yet fail to generalize. The representations do not meaningfully encode the concept, as evidenced by the negligible mutual information. Thus, it underscores the importance of cautiously interpreting high probe accuracies as it could be a result of overfitting in a high-dimensional space.

Indeed, several works (Hewitt & Liang, 2019) have in the past demonstrated spurious correlations found using MLP probes and have suggested the use of regularization or training "control" probes acting as baselines trained on random labels. While these are valuable, Theorem 1 shows they might not be enough, as both real concepts and random controls can be "detected" in high-dimensional spaces. This finding has been validated in prior work where the difference in accuracy of probes trained on actual representations and random ones is small for larger, more expressive probes (Hewitt & Liang, 2019).

Thus, we are faced with a trade-off - Linear probes have lower risk of overfitting but may miss complex concept encodings while MLP probes can detect more complex relationships but are at higher risk of fitting noise.

## 3 Limitations of Probing Internal Representations

Building on our framework and initial results, we now turn to a deeper investigation of the fundamental limitations of probing techniques. While the previous section established the basic machinery for understanding probes and demonstrated some practical challenges in high-dimensional spaces, this section reveals additional, concrete theoretical barriers to concept verification in generative models. We present a series of impossibility results that highlight inherent limitations in our ability to verify certain types of concepts, regardless of the probing strategy employed.

Our analysis focuses on three key aspects: the verification of fine-grained concepts, the limitations of linear probes for nonlinear concepts, and the fundamental impossibility of universal concept verification. These results complement our earlier findings by showing that even with ideal implementation of our framework, certain conceptual properties of generative models may remain fundamentally unverifiable through probing.

### Impossibility of Verifying Fine-Grained Concepts

We begin by demonstrating that for high-dimensional internal representations, it is impossible to verify the presence of all "fine-grained" concepts using a finite number of samples.

Fine-grained concepts, as defined in our work, represent properties that depend on precise patterns in the representation space. Technically, these concepts partition the d-dimensional space into \(2^{d}\) orthants, with each concept corresponding to a specific sign pattern across all dimensions.

Intuitively, one can think of fine-grained concepts as very specific properties that require examining all dimensions of the representation space simultaneously. For example, in a language model, a fine-grained concept might be "formal technical writing with positive sentiment about mathematics" - a property that requires many subtle features to align in specific ways. In an image model, it might be "portrait photo with soft lighting from the left and a slight smile" - again requiring precise combinations of many features.

The key insight is that these concepts are "fine-grained" because they require exact matches across many dimensions, making them fundamentally hard to verify with any finite sampling strategy. This matches real-world intuitions about why it's hard to verify that generative models consistently capture very specific combinations of properties. We defer a technical definition to the proof of the following theorem in Appendix A.

**Theorem 2** (Fine-Grained Concept Verification).: _Let \(G:\) be a generative model with a d-dimensional internal representation function \(h:^{d}\). There exists a family of "fine-grained" concepts \(\) such that for any probe \(P\) using less than \(O(2^{d})\) samples, there exist two generative models \(G_{1}\) and \(G_{2}\) that are indistinguishable by \(P\), but differ in whether they express a concept \(c\)._

The key insight here is that in high-dimensional spaces, there are exponentially many "regions" (orthants in this case) where a concept could be present. With a limited number of samples, a probe is bound to miss some of these regions, allowing for the construction of models that differ in these unobserved areas.

This result suggests that for models with high-dimensional internal representations, it's practically impossible to verify all fine-grained concepts using a finite number of samples. In practice, probes might miss important but rare concepts in the representation space, and two seemingly identical models (from the probe's perspective) might have significantly different behaviors in certain regions - this can make auditing properties of such models hard (Bhattacharjee & von Luxburg, 2024).

### Limitations of Linear Probes for Nonlinear Concepts

While our earlier results focused on linear probes for linearly separable concepts, we now explore their limitations when dealing with nonlinear concepts.

**Theorem 3** (Limitations of Linear Probes).: _Let \(G:\) be a generative model with a \(d\)-dimensional internal representation function \(h:^{d}\). Assume the components of \(h(x)\) are independent and symmetrically distributed around zero. There exists a family of nonlinear concepts \(_{k}\) of complexity \(k\) defined over \(^{d}\), such that any linear probe \(P\) requires \(O(}}{^{2}})\) samples to reliably detect concepts from \(_{k}\) within an error of \(\) with probability at least \(1-\). Moreover, even with an infinite number of samples, a linear probe cannot perfectly capture these nonlinear concepts._

This theorem reveals a duality in linear probes: efficiency in sampling versus limitations in expressiveness. While they can scale to high-dimensional spaces relatively efficiently, their inability to capture nonlinear relationships persists regardless of sample size. This has significant implications for probing strategies in deep learning, highlighting the need to balance the simplicity and interpretability of linear probes against the potential necessity for more expressive, nonlinear probing techniques when investigating complex concepts.

### Impossibility of Universal Concept Verification

We now extend our analysis beyond specific probe types to consider the limitations of any fixed probing strategy.

**Theorem 4** (Universal Concept Verification).: _Let \(G:\) be a generative model with a \(d\)-dimensional internal representation function \(h:\), where \(^{d}\) is compact. For any fixed probing strategy \(P\) with sample size \(m\), there exists a concept \(c:\{0,1\}\) that is strongly present in the model's internal representations but cannot be reliably detected by \(P\)._

This result leverages the fact that any fixed probing strategy, with its limited sampling, can only distinguish among a finite number of concepts, while the space of possible concepts is much larger. It demonstrates the existence of strongly present yet undetectable concepts, highlighting fundamental limitations in our ability to comprehensively verify the internal representations of complex models using any fixed probing strategy.

This result is reminiscent of the "No Free Lunch" theorems in optimization and machine learning. It suggests that no single probing strategy can be universally effective, echoing limitations found in other areas of information theory and statistical learning.

### Discussion

The apparent tension between our earlier sample complexity guarantees and later impossibility results reveals important insights about probing methodology. While Proposition 1 establishes polynomial sample complexity for concept detection, Theorems 2-4 demonstrate fundamental limitations. This seeming contradiction can be reconciled by examining their different contexts and assumptions.

First, these results address fundamentally different concept classes. Our early sample complexity bounds focus on linearly separable concepts, forming a restricted but learnable subset of all possible concepts. In contrast, our later theorems deal with increasingly complex concept spaces, from fine-grained patterns to nonlinear relationships, culminating in the full space of possible concepts. This progression demonstrates how sample complexity requirements escalate dramatically with concept complexity.

The results also highlight a crucial trade-off in probe expressiveness. Linear probes offer reliable learning guarantees but limited detection capability, while more expressive probes can capture complex concepts but face fundamental limitations in high-dimensional spaces. Together, these findings define the boundaries of what is and isn't possible with probing strategies.

These theoretical results suggest that while probing can provide valuable insights about certain concepts in generative models, it cannot offer complete guarantees about all possible concepts a model might encode. This inherent limitation points to the importance of considering probing as one toolwithin a broader framework of interpretability methods. Future work may explore adaptive or iterative probing methods, the incorporation of domain-specific knowledge, and the complementary use of other analysis techniques such as adversarial testing or causal interventions.

## 4 Related Works

Interpreting deep learning models has been a significant research focus, with probes or diagnostic classifiers being a common approach. (Alain and Bengio, 2018) introduced using linear classifiers to probe hidden layers. In NLP, (Conneau et al., 2018) and (Tenney et al., 2019) applied probing to models like BERT, revealing layered learning of linguistic features. For computer vision, (Zeiler and Fergus, 2014) and (Bau et al., 2017) developed methods to visualize and quantify interpretability of convolutional networks.

However, limitations of probes have been debated. (Zhang et al., 2017) showed networks could fit random labels, raising overfitting concerns. (Hewitt and Liang, 2019) argued high probe performance doesn't necessarily imply structural alignment with probed concepts. (Voita and Titov, 2020) and (Pimentel et al., 2020) cautioned against overinterpretation of probing results.

Our work extends this research by providing a formal framework al (Dasgupta et al., 2022; Yadav et al., 2023) that defines probe use and rigorously analyzes their limitations, offering theoretical foundations for understanding when and why probes might fail to reveal meaningful insights.

## 5 Conclusion

We have presented a formal framework for explainability in generative models using probes, establishing a structured approach to analyze concept encoding within models. Our theoretical results highlight significant limitations in probing strategies, particularly in high-dimensional spaces and with nonlinear concepts. We showed that while linear probes are efficient and interpretable, they may fail to detect complex, nonlinear relationships, while more expressive probes can overfit to random labels.

Our findings suggest cautious application and interpretation of probing results. Additionally, there has been some recent work which argues for providing white box access to auditors for checking properties of ML models (Bhattacharjee and von Luxburg, 2024; Casper et al., 2024). Our work adds to that chorus while also suggesting that even white box access (at least via probing) might not be enough in generative settings - there is a need to clarify what type of access is needed for efficient and rigorous audits.

Our work also suggests the use of complementary methods, such as causal interventions or alternative interpretability techniques, for a more comprehensive understanding of generative models. Future work may explore adaptive probing strategies or develop methodologies balancing probe expressiveness with reliability to address the challenges identified in our analysis.