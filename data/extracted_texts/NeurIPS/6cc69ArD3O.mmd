# Globally injective and bijective neural operators

Takashi Furuya\({}^{1}\)  Michael Puthawala\({}^{2}\)  Matti Lassas\({}^{3}\)  Maarten V. de Hoop\({}^{4}\)

\({}^{1}\)Shimane University, takashi.furuya0101@gmail.com

\({}^{2}\)South Dakota State University, Michael.Puthawala@sdstate.edu

\({}^{3}\)University of Helsinki, matti.lassas@helsinki.fi

\({}^{4}\)Rice University, mdehoop@rice.edu

###### Abstract

Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank setting, is subtler in the infinite-rank setting and is proven using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not 'lost' in the transcription from analytical operators to their finite-rank implementation with networks. Finally, we conclude with an increase in abstraction and consider general conditions when subnetworks, which may have many layers, are injective and surjective and provide an exact inversion from a 'linearization.' This section uses general arguments from Fredholm theory and Leray-Schauder degree theory for non-linear integral equations to analyze the mapping properties of neural operators in function spaces. These results apply to subnetworks formed from the layers considered in this work, under natural conditions. We believe that our work has applications in Bayesian uncertainty quantification where injectivity enables likelihood estimation and in inverse problems where surjectivity and injectivity corresponds to existence and uniqueness of the solutions, respectively.

## 1 Introduction

In this work, we produce results at the intersection of two fields: neural operators (NO), and injective and bijective networks. Neural operators  are neural networks that take a infinite dimensional perspective on approximation by directly learning an operator between Sobolev spaces. Injectivity and bijectivity on the other hand are fundamental properties of networks that enable likelihood estimation by the change of variables formula, are critical in applications to inverse problems, and are useful properties for downstream applications.

The key contribution of our work is the translation of fundamental notions from the finite-rank setting to the infinite-rank setting. By the 'infinite-dimension setting' we refer to the case when the object of approximation is a mapping between Sobolev spaces. This task, although straight-forward on first inspection, often requires dramatically different arguments and proofs as the topology, analysis and notion of noise are much simpler in the finite-rank case as compared to the infinite-rank case. We see our work as laying the groundwork for the application of neural operators to generative modelsin function spaces. In the context of operator extensions of traditional VAEs (Kingma and Welling, 2013), injectivity of a decoder forces distinct latent codes to correspond to distinct outputs.

Our work draws parallels between neural operators and pseudodifferential operators Taylor (1981), a class that contains many inverses of linear partial differential operators and integral operators. The connection to pseudodifferential operators provided an algebraic perspective to linear PDE Kohn and Nirenberg (1965). An important fact in the analysis of pseudodifferential operators, is that the inverses of certain operators, e.g. elliptic pseudodifferential operators, are themselves pseudodifferential operators. By proving an analogous result in section 4.2, that the inverse of invertible NO are themselves given by NO, we draw an important and profound connection between (non)linear partial differential equations and NO.

We also believe that our methods have applications to the solution of inverse problems with neural networks. The desire to use injective neural networks is one of the primary motivations for this work. These infinite dimensional models can then be approximated by a finite dimensional model without losing discretization invariance, see Stuart (2010). Crucially, discretization must be done at the last possible moment,' or else performance degrades as the discretization becomes finer, see Lassas and Siltanen (2004) and also Saksman et al. (2009). By formulating machine learning problems in infinite dimensional function spaces and then approximating these methods using finite dimensional subspaces, we avoid bespoke ad-hoc methods and instead obtain methods that apply to any discretization.

More details on our motivations for and applications of injectivity & bijectivity of neural operators are given in Appendix A.

### Our Contribution

In this paper, we give a rigorous framework for the analysis of the injectivity and bijectivity of neural operators. Our contributions are as follows:

1. We show an equivalent condition for the layerwise injectivity and bijectivity for linear neural operators in the case of ReLU and bijective activation functions (Section 2). In the particular ReLU case, the equivalent condition is characterized by a directed spanning set (Definition 2).
2. We prove that injective linear neural operators are universal approximators, and that their implementation by finite rank approximation is still injective (Section 3). We note that universal approximation theorem (Theorem 1) in the infinite dimensional case does not require an increase in dimension, which deviate from the finite dimensional case Puthawala et al. (2022a, Thm. 15).
3. We zoom out and perform a more abstract global analysis in the case when the input and output dimensions are the same. In this section we 'coarsen' the notion of layer, and provide a sufficient condition for the surjectivity and bijectivity of nonlinear integral neural operators _with nonlinear kernels_. This application arises naturally in the context of subnetworks and transformers. We construct their inverses in the bijective case (Section 4).

### Related Works

In the finite-rank setting, injective networks have been well-studied, and shown to be of theoretical and practical interest. See Gomez et al. (2017); Kratsios and Bilkopytov (2020); Teshima et al. (2020); Ishikawa et al. (2022); Puthawala et al. (2022a) for general references establishing the usefulness of injectivity or any of the works on flow networks for the utility of injectivity and bijectivity for downstream applications, (Dinh et al., 2016; Siahkoohi et al., 2020; Chen et al., 2019; Dinh et al., 2014; Kingma et al., 2016), but their study in the infinite-rank setting is comparatively underdeveloped. These works, and others, establish injectivity in the finite-rank setting as a property of theoretical and practical interest. Our work extends Puthawala et al. (2022a) to the infinite-dimensional setting as applied to neural operators, which themselves are a generalization of multilayer perceptrons (MLPs) to function spaces. Moreover, our work includes not only injectivity, but also surjectivity in the non-ReLU activation case, which Puthawala et al. (2022a) has not focused on.

Examples of works in these setting include neural operators Kovachki et al. (2021, 2021); LeepONet Lu et al. (2019); Lanthaler et al. (2022), and PCA-Net Bhattacharya et al. (2021); De Hoop et al. (2022). The authors of Alberti et al. (2022) recently proposed continuous generative neural networks (CGNNs), which are convolution-type architectures for generating \(L^{2}()\)-functions, and provided the sufficient condition for the global injectivity of their network. Their approach is the wavelet basis expansion, whereas our work relies on an independent choice of basis expansion.

### Networks considered and notation

Let \(D^{d}\) be an open and connected domain, and \(L^{2}(D;^{h})\) be the \(L^{2}\) space of \(^{h}\)-value function on \(D\) given by \(L^{2}(D;^{h})(D;)  L^{2}(D;)}_{h}=L^{2}(D)^{h}\).

**Definition 1** (Integral and pointwise neural operators).: _We define an integral neural operator \(G:L^{2}(D)^{d_{in}} L^{2}(D)^{d_{out}}\) and layers \(_{}:L^{2}(D)^{d_{+1}} L^{2}(D)^{d_{+1}}\) by_

\[G:=T_{L+1}_{L}_{1} T_{0},( _{}v)(x)(T_{}(v)(x)+b_{}(x)),\]

\[T_{}(v)(x)=W_{}(x)u(x)+_{D}k_{}(x,y)u(y), x D,\]

_where \(:\) is a non-linear activation operating element-wise, and \(k_{}(x,y) L^{2}(D D;^{d_{+1} d_{}})\) are integral kernels, and \(W_{} C(;^{d_{+1} d_{}})\) are pointwise multiplications with matrices, and \(b_{} L^{2}(D)^{d_{+1}}\) are bias functions (\(=1,...,L\)). Here, \(T_{0}:L^{2}(D)^{d_{in}} L^{2}(D)^{d_{1}}\) and \(T_{L+1}:L^{2}(D)^{d_{L+1}} L^{2}(D)^{d_{out}}\) are mappings (lifting operator) from the input space to the feature space and mappings (projection operator) from the feature spaces to the output space, respectively._

The layers \(T_{0}\) and \(T_{L+1}\) play a special role in the neural operators. They are local linear operators and serve to lift and project the input data from and to finite-dimensional space respectively. These layers may be 'absorbed' into the layers \(_{1}\) and \(_{L}\) without loss of generality (under some technical conditions), but are not in this text to maintain consistency with prior work. Prior work assumes that \(d_{in}<d_{1}\), we only assume that \(d_{in} d_{1}\) for lifting operator \(T_{0}:L^{2}(D)^{d_{in}} L^{2}(D)^{d_{1}}\). This would seemingly play an important role in the context of injectivity or universality, but we find that our analysis does not require that \(d_{in}<d_{1}\) at all. In fact, as elaborated in Section 3.2, we may take \(d_{in}=d_{}=d_{out}\) for \(=1,,L\) and our analysis is the same.

## 2 Injective linear neural operator layers with ReLU and bijective activations

In this section we present sharp conditions under which a layer of a neural operator with \(\) activation is injective. The Directed Spanning Set (DSS) condition, described by Def. 2 is a generalization of the finite-dimensional DSS (Puthawala et al., 2022) which guarantees layerwise injectivity of \(\) layers. Extending this condition from finite to infinite dimensions is not automatic. The finite-dimensional DSS will hold with high probability for random weight matrices if they are expansive enough (Puthawala et al., 2022, Theorem7). However, the infinite-dimensional DSS is much more restrictive than the finite dimensional setting. We then present a less restrictive condition that is met when the activation function is bijective, e.g. a leaky-ReLU activation is used.

Although it may appear that the end-to-end result is strictly stronger than the layerwise result, this is not the case. The layerwise result is an exact characterization, whereas the end-to-end result is sufficient for injectivity, but not necessary. The layerwise analysis is also constructive, and so gives a rough guide for the construction of injective networks, whereas the global analysis is less so. Finally, the layerwise condition has different applications, such as network of stochastic depth, see e.g. Huang et al. (2016); Benitez et al. (2023). End-to-end injectivity by enforcing layerwise injectivity is straightforward, whereas deriving a sufficient condition for networks of any depth is more daunting.

We denote by

\[(Tv+b)(x):=(T_{1}v(x)+b_{1}(x))\\ \\ (T_{m}v(x)+b_{m}(x)), x D,\;v L^{2}(D)^{n}\]where \(:\) is a non-linear activation function, \(T(L^{2}(D)^{n},L^{2}(D)^{m})\), and \(b L^{2}(D)^{m}\), where \((L^{2}(D)^{n},L^{2}(D)^{m})\) is the space of linear bounded operators from \(L^{2}(D)^{n}\) to \(L^{2}(D)^{m}\). The aim of this section is to characterize the injectivity condition for the operator \(v(Tv+b)\) mapping from \(L^{2}(D)^{n}\) to \(L^{2}(D)^{m}\), which corresponds to layer operators \(_{}\). Here, \(T:L^{2}(D)^{n} L^{2}(D)^{m}\) is linear.

### ReLU activation

Let \(:\) be ReLU activation, defined by \((s)=\{0,s\}\). With this activation function, we introduce a definition which we will find sharply characterizes layerwise injectivity.

**Definition 2** (Directed Spanning Set).: _We say that the operator \(T+b\) has a directed spanning set (DSS) with respect to \(v L^{2}(D)^{n}\) if_

\[(T_{S(v,T+b)}) X(v,T+b)=\{0\},\] (2.1)

_where \(T|_{S(v,T+b)}(v)=(T_{i}v)_{i S(v,T+b)}\) and_

\[S(v,T+b):=\{i[m]\;\;T_{i}v+b_{i}>0\;in\;D\},\] (2.2)

\[X(v,T+b):=\{u L^{2}(D)^{n}|i S (v,T+b)x D,\\ (i)\;T_{i}v(x)+b_{i}(x) T_{i}u(x)T_{i}v(x)+b_{i}(x) 0,\\ (ii)\;T_{i}u(x)=0T_{i}v(x)+b_{i}(x)>0\}.\] (2.3)

The name directed spanning set arises from the \((T|_{S(v,T+b)})\) term of (2.1). The indices of \(S(v,T+b)\) are those that are directed (positive) in the direction of \(v\). If \(T\) restricted to these indices together span \(L^{2}(D)^{n}\), then \((T|_{S(v,T+b)})\) is \(\{0\}\), and the condition is automatically satisfied. Hence, the DSS condition measures the extent to which the set of indices, which are directed w.r.t. \(v\), form a span of the input space.

**Proposition 1**.: _Let \(T(L^{2}(D)^{n},L^{2}(D)^{m})\) and \(b L^{2}(D)^{m}\). Then, the operator \((T+b):L^{2}(D)^{n} L^{2}(D)^{m}\) is injective if and only if \(T+b\) has a DSS with respect to every \(v L^{2}(D)^{n}\) in the sense of Definition 2._

See Section B for the proof. Puthawala et al. (2022) has provided the equivalent condition for the injectivity of ReLU operator in the case of the Euclidean space. However, proving analogous results for operators in function spaces require different techniques. Note that because Def. 2 is a sharp characterization of injectivity, it can not be simplified in any significant way. The condition restrictive Def. 2 is, therefore, difficult to relax while maintaining generality. This is because for each function \(v\), multiple components of the function \(Tv+b\) are strictly positive in the entire domain \(D\), and cardinality \(|S(v;T+b)|\) of \(S(v;T+b)\) is larger than \(n\). This observation prompts us to consider the use of bijective activation functions instead of ReLU, such as leaky ReLU function, defined by \(_{a}(s):=(s)-a\;(-s)\) where \(a>0\).

### Bijective activation

If \(\) is injective, then injectivity of \((T+b):L^{2}(D)^{n} L^{2}(D)^{m}\) is equivalent to the injectivity of \(T\). Therefore, we consider the bijectivity in the case of \(n=m\). As mentioned in Section 1.3, an significant example is \(T=W+K\), where \(W^{n n}\) is injective and \(K:L^{2}(D)^{n} L^{2}(D)^{n}\) is a linear integral operator with a smooth kernel. This can be generalized to Fredholm operators (see e.g., Jeribi (2015, Section 2.1.4)), which encompasses the property for identity plus a compact operator. It is well known that a Fredholm operator is bijective if and only if it is injective and its Fredholm index is zero. We summarize the above observation as follows:

**Proposition 2**.: _Let \(:\) be bijective, and let \(T:L^{2}(D)^{n} L^{2}(D)^{m}\) and \(b L^{2}(D)^{m}\). Then, \((T+b):L^{2}(D)^{n} L^{2}(D)^{m}\) is injective if and only if \(T:L^{2}(D)^{n} L^{2}(D)^{m}\) is injective. Furthermore, if \(n=m\) and \(T(L^{2}(D)^{n},L^{2}(D)^{n})\) is the linear Fredholm operator, then, \((T+b):L^{2}(D)^{n} L^{2}(D)^{n}\) is bijective if and only if \(T:L^{2}(D)^{n} L^{2}(D)^{n}\) is injective with index zero._

We believe that this characterization of layerwise injectivity is considerably less restrictive than Def. 2, and the characterization of bijectivity in terms of Fredholm theory will be particularly useful in establishing operator generalization of flow networks.

Global analysis of injectivity and finite-rank implementation

In this section we consider global properties of the injective and bijective networks that constructed in Section 2. First we construct end-to-end injective networks that are not layerwise injective. By doing this, we may avoid the dimension increasing requirement that would be necessary from a layerwise analysis. Next we show that injective neural operators are universal approximators of continuous functions. Although the punchline resembles that of [Puthawala et al., 2022a, Theorem 15], which relied on Whitney's embedding theorem, the arguments are quite different. The finite-rank case has dimensionality restrictions, as required by degree theory, whereas our infinite-rank result does not. Finally, because all implementations of neural operators are ultimately finite-dimensional, we present a theorem that gives conditions under which finite-rank approximations to injective neural operators are also injective.

### Global analysis

By using the characterization of layerwise injectivity discussed in Section 2, we can compose injective layers to form \(_{L}_{1} T_{0}\), a injective network. Layerwise injectivity, however, prevents us from getting injectivity of \(T_{L+1}_{L}_{1} T_{0}\) by a layerwise analysis if \(d_{L+1}>d_{out}\), as is common in application [Kovachki et al., 2021b, Pg. 9]. In this section, we consider global analysis and show that \(T_{L+1}_{L}_{1} T_{0}\), nevertheless, remains injective. This is summarized in the following lemma.

**Lemma 1**.: _Let \(\) with \(<m\), and let the operator \(T:L^{2}(D)^{n} L^{2}(D)^{m}\) be injective. Assume that there exists an orthogonal sequence \(\{_{k}\}_{k}\) in \(L^{2}(D)\) and a subspace \(S\) in \(L^{2}(D)\) such that \((_{1}T) S\) and_

\[\{_{k}\}_{k} S=\{0\}.\] (3.1)

_where \(_{1}:L^{2}(D)^{m} L^{2}(D)\) is the restriction operator defined in (C.1). Then, there exists a linear bounded operator \(B(L^{2}(D)^{m},L^{2}(D)^{})\) such that \(B T:L^{2}(D)^{n} L^{2}(D)^{}\) is injective._

See Section C.1 in Appendix C for the proof. \(T\) and \(B\) correspond to \(_{L}_{1} T_{0}\) (from lifting to \(L\)-th layer) and \(T_{L+1}\) (projection), respectively. The assumption (3.1) on the span of \(_{k}\) encodes a subspace distinct from the range of \(T\). In Remark 3 of Appendix C, we provide an example that satisfies the assumption (3.1). Moreover, in Remark 4 of Appendix C, we show the exact construction of the operator \(B\) by employing projections onto the closed subspace, using the orthogonal sequence \(\{_{k}\}_{k}\). This construction is given by the combination of "Pairs of projections" discussed in Kato (2013, Section I.4.6) with the idea presented in [Puthawala et al., 2022b, Lemma 29].

### Universal approximation

We now show that the injective networks that we consider in this work are universal approximators. We define the set of integral neural operators with \(L^{2}\)-integral kernels by

\[_{L}(;D,d_{in},d_{out}):=G:L^{2}(D)^{d_{ in}} L^{2}(D)^{d_{out}}:\] \[G=K_{L+1}(K_{L}+b_{L})(K_{2}+b_{2}) (K_{1}+b_{1})(K_{0}+b_{0}),\] \[K_{}(L^{2}(D)^{d_{}},L^{2}(D)^{d_{+1}} ),\;K_{}:f_{D}k_{}(,y)f(y)dy_{D},\;k_{} L ^{2}(D D;^{d_{+1} d_{}}),\] \[b_{} L^{2}(D;^{d_{+1}}),\;d_{} ,\;d_{0}=d_{in},\;d_{L+2}=d_{out},\;=0,...,L+2},\] (3.2)

and

\[_{L}^{inj}(;D,d_{in},d_{out}):=\{G_{L}(; D,d_{in},d_{out}):\}.\]

The following theorem shows that \(L^{2}\)-injective neural operators are universal approximators of continuous operators.

**Theorem 1**.: _Let \(D^{d}\) be a Lipschitz bounded domain, and \(G^{+}:L^{2}(D)^{d_{in}} L^{2}(D)^{d_{out}}\) be continuous such that for all \(R>0\) there is \(M>0\) so that_

\[G^{+}(a)_{L^{2}(D)^{d_{out}}} M,\; a L^{2}(D)^{ d_{in}},\;\|a\|_{L^{2}(D)^{d_{in}}} R,\] (3.3)_We assume that either (i) \(_{0}^{L}\) is injective, or (ii) \(=\). Then, for any compact set \(K L^{2}(D)^{d_{in}}\), \((0,1)\), there exists \(L\) and \(G_{L}^{inj}(;D,d_{in},d_{out})\) such that_

\[_{a K}\|G^{+}(a)-G(a)\|_{L^{2}(D)^{d_{out}}}.\]

See Section C.3 in Appendix C for the proof. For the definitions of \(_{0}^{L}\) and \(\), see Definition 3 in Appendix C. For example, ReLU and Leaky ReLU functions belong to \(_{0}^{L}\) (see Remark 5 (i)).

We briefly remark on the proof of Theorem 1 emphasizing how its proof differs from a straightforward extension of the finite-rank case. In the proof we first employ the standard universal approximation theorem for neural operators ((Kovachki et al., 2021b, Theorem 11)). We denote the approximation of \(G^{+}\) by \(\), and define the graph of \(\) as \(H:L^{2}(D)^{d_{in}} L^{2}(D)^{d_{in}} L^{2}(D)^{d_{out}}\). That is \(H(u)=(u,(u))\). Next, utilizing Lemma 1, we construct the projection \(Q\) such that \(Q H\) becomes an injective approximator of \(G^{+}\) and belongs to \(_{L}(;D,d_{in},d_{out})\). The proof for universal approximation theorem is constructive. If, in the future, efficient approximation bounds for neural operators are given, such bounds can likely be used directly in our universality proof to generate corresponding efficient approximation bounds for injective neural operators.

This approach resembles the approach in the finite-rank space Puthawala et al. (2022a, Theorem 15), but unlike that theorem we don't have any dimensionality restrictions. More specifically, in the case of Euclidean spaces \(^{d}\), Puthawala et al. (2022a, Theorem 15) requires that \(2d_{in}+1 d_{out}\) before all continuous functions \(G^{+}:^{d_{in}}^{d_{out}}\) can be uniformly approximated in compact sets by injective neural networks. When \(d_{in}=d_{out}=1\) this result is not true, as is shown in Remark 5 (iii) in Appendix C using topological degree theory (Cho and Chen, 2006). In contrast, Theorem 1 does not assume any conditions on \(d_{in}\) and \(d_{out}\). Therefore, we can conclude that infinite dimensional case yields better approximation results than the finite dimensional case.

This surprising improvement in restrictions in infinite-dimensions can be elucidated by an analogy to Hilbert's hotel paradox, see (Burger and Starbird, 2004, Sec 3.2). In this analogy, the orthonormal bases \(\{_{k}\}_{k}\) and \(_{j,k}(x)=(_{ij}_{k}(x))_{i=1}^{d}\) play the part of guests in the hotel with \(\) floor, each of which as \(d\) rooms. A key step in the proof of Theorem 1 is that there is a linear isomorphism \(S:L^{2}(D)^{d} L^{2}(D)\) (i.e., a rearrangement of guests) which maps \(_{j,k}\) to \(_{b(j,k)}\), where \(b:[d]\) is a bijection.

### Injectivity-preserving transfer to Euclidean spaces via finite-rank approximation

In the previous section, we have discussed injective integral neural operators. The conditions are given in terms of integral kernels, but such kernels may not actually be implementable with finite width and depth networks, which have a finite representational power. A natural question to ask, therefore, is how should these formal integral kernels be implemented on actual finite rank networks, the so-called implementable neural operators? In this section we discuss this question.

We consider linear integral operators \(K_{}\) with \(L^{2}\)-integral kernels \(k_{}(x,y)\). Let \(\{_{k}\}_{k}\) be an orthonormal basis in \(L^{2}(D)\). Since \(\{_{k}(y)_{p}(x)\}_{k,p}\) is an orthonormal basis of \(L^{2}(D D)\), integral kernels \(k_{} L^{2}(D D;^{d_{+1} d_{}})\) in integral operators \(K_{}(L^{2}(D)^{d_{}},L^{2}(D)^{d_{+1}})\) has the expansion

\[k_{}(x,y)=_{k,p}C_{k,p}^{()}_{k}(y)_{ p}(x),\]

where \(C_{k,p}^{()}^{d_{+1} d_{}}\) whose \((i,j)\)-th component \(c_{k,p,ij}^{()}\) is given by

\[c_{k,p,ij}^{()}=(k_{,ij},_{k}_{p})_{L^{2}(D D)},\]

Here, we denote \((u,_{k})^{d_{}}\) by \((u,_{k})=((u_{1},_{k})_{L^{2}(D)},...,(u_{d_{}}, _{k})_{L^{2}(D)}).\) By truncating by \(N\) finite sums, we approximate \(L^{2}\)-integral operators \(K_{}(L^{2}(D)^{d_{}},L^{2}(D)^{d_{+1}})\) by finite rank operator \(K_{,N}\) with rank \(N\), having the form

\[K_{,N}u(x)=_{k,p[N]}C_{k,p}^{()}(u,_{k})_{p}(x), \;u L^{2}(D)^{d_{}}.\]The choice of orthonormal basis \(\{_{k}\}_{k}\) is a hyperparameter. If we choose \(\{_{k}\}_{k}\) as Fourier basis and wavelet basis, then network architectures correspond to Fourier Neural Operators (FNOs) [Li et al., 2020b] and Wavelet Neural Operators (WNOs) [Tripura and Chakraborty, 2023], respectively. We show that Propositions 1, 2 (characterization of layerwise injectivity), and Lemma 1 (global injectivity) all have natural analogues for finite rank operator \(K_{,N}\) in Proposition 5 and Lemma 2 in Appendix D. These conditions applies out-of-the-box to both FNOs and WNOs.

Lemma 2 and Remark 3 in the appendix D give a'recipe' to construct the projection \(B\) such that the composition \(B T\) (interpreted as augmenting finite rank neural operator \(T\) with one layer \(B\)) is injective. The projection \(B\) is constructed by using an orthogonal sequence \(\{_{k}\}_{k}\) subject to the condition (3.1), which does not over-leap the range of \(T\). This condition is automatically satisfied for any orthogonal base \(\{_{k}\}_{k}\). This could yield practical implications in guiding the choice of the orthogonal basis \(\{_{k}\}_{k}\) for the neural operator's design.

We also show the universal approximation in the case of finite rank approximation. We denote \(_{L,N}(;D,d_{in},d_{out})\) by the set of integral neural operators with \(N\) rank, that is the set (3.2) replacing \(L^{2}\)-integral kernel operators \(K_{}\) with finite rank operators \(K_{,N}\) with rank \(N\) (see Definition 4). We define by

\[_{L,N}^{inj}(;D,d_{in},d_{out}):=\{G_{N}_{L,N}( ;D,d_{in},d_{out}):G_{N}\}.\]

**Theorem 2**.: _Let \(D^{d}\) be a Lipschitz bounded domain, and \(N\), and \(G^{+}:L^{2}(D)^{d_{in}} L^{2}(D)^{d_{out}}\) be continuous with boundedness as in (3.3). Assume that the non-linear activation function \(\) is either ReLU or Leaky ReLU. Then, for any compact set \(K L^{2}(D)^{d_{in}}(\{_{k}\}_{k N})^{d_{in}}\), \((0,1)\), there exists \(L\), \(N^{}\) with_

\[N^{}d_{out} 2Nd_{in}+1,\] (3.4)

_and \(G_{N^{}}_{L,N^{}}^{inj}(;D,d_{in},d_{out})\) such that_

\[_{a K}\|G^{+}(a)-G_{N^{}}(a)\|_{L^{2}(D)^{d_{out}}} .\]

See Section D.4 in Appendix D for the proof. In the proof, we make use of Puthawala et al. [2022a, Lemma 29], which gives rise to the assumption (3.4). We do not require any condition on \(d_{in}\) and \(d_{out}\) as well as Theorem 1.

**Remark 1**.: _Observe that in our finite-rank approximation result, we only require that the target function G+ is continuous and bounded, but not smooth. This differs from prior work that requires smoothness of the function to be approximated._

### Limitations

We assume square matrices for the bijectivity and construction of inversions. Weight matrices in actual neural operators are not necessarily square. Lifting and projection are local operators which map the inputs into a higher-dimensional feature space and project back the feature output to the output space. We also haven't addressed possible aliasing effects of our injective operators. We will relax the square assumption and investigate the aliasing of injective operators in future work.

## 4 Subnetworks & nonlinear integral operators: bijectivity and inversion

So far our analysis of injectivity has been restricted to the case where the only source of nonlinearity are the activation functions. In this section we consider a weaker and more abstract problem where nonlinearities can also arise from the integral kernel with surjective activation function, such as leaky \(\). Specifically, we consider layers of the form

\[F_{1}(u)=Wu+K(u),\] (4.1)

where \(W(L^{2}(D)^{n},L^{2}(D)^{n})\) is a linear bounded bijective operator, and \(K:L^{2}(D)^{n} L^{2}(D)^{n}\) is a non-linear operator. This arises in the non-linear neural operator construction by Kovachki et al. [2021b] or in Ong et al.  to improve performance of integral autoencoders. In this construction, each layer \(_{}\) is written as

\[x D,(_{}v)(x)=(W_{}v(x)+K_{}(v)(x)), K _{}(u)(x)=_{D}k_{}(x,y,u(x),u(y))u(y)dy,\]where \(W_{}^{d_{+1} d_{}}\) independent of \(x\), and \(K_{}:L^{2}(D)^{d_{}} L^{2}(D)^{d_{+1}}\) is the non-linear integral operator.

This relaxing of assumptions is motivated by a desire to obtain theoretical results for both subnetworks and operator transformers. By subnetworks, we mean compositions of layers within a network. This includes, for example, the encoder or decoder block of a traditional VAE. By neural operator we mean operator generalizations of finite-rank transformers, which can be modeled by letting \(K\) be an integral transformation with nonlinear kernel \(k\) of the form

\[k(x,y,v(x),v(y)) Av(x),Bv(y),\]

where \(A\) and \(B\) are matrices of free parameters, and the (integral) softmax is taken over \(x\). This specific choice of \(k\) can be understood as a natural generalization of the attention mechanism in transformers, see (Kovachki et al., 2021b, Sec. 5.2) for further details.

### Surjectivity and bijectivity

Critical to our analysis is the notion of coercivity. Apart from being a useful theoretical tool, layerwise coercivity of neural networks is a useful property in imaging applications, see e.g.Li et al. (2020)Recall from Showalter (2010, Sec 2, Chap VII) that a non-linear operator \(K:L^{2}(D)^{n} L^{2}(D)^{n}\) is coercive if

\[_{\|u\|_{L^{2}(D)^{n}}} K(u),(D)^{n}} }_{L^{2}(D)^{n}}=.\] (4.2)

**Proposition 3**.: _Let \(:\) be surjective and \(W:L^{2}(D)^{n} L^{2}(D)^{n}\) be linear bounded bijective (then the inverse \(W^{-1}\) is bounded linear), and let \(K:L^{2}(D)^{n} L^{2}(D)^{n}\) be a continuous and compact mapping. Moreover, assume that the map \(u u+W^{-1}K(u)\) is coercive with some \(0<<1\). Then, the operator \( F_{1}\) is surjective._

See Section E.1 in Appendix E for the proof. An example \(K\) satisfying the coercivity (4.2) condition is given here.

**Example 1**.: _We simply consider the case of \(n=1\), and \(D^{d}\) is a bounded interval. We consider the non-linear integral operator_

\[K(u)(x):=_{D}k(x,y,u(x))u(y)dy,\;x D.\]

_The operator \(u u+W^{-1}K(u)\) with some \(0<<1\) is coercive when the non-linear integral kernel \(k(x,y,t)\) satisfies certain boundedness conditions. In Examples 2 and 3 in Appendix E, we show that these conditions are met by kernels \(k(x,y,t)\) of the form_

\[k(x,y,t)=_{j=1}^{J}c_{j}(x,y)(a_{j}(x,y)t+b_{j}(x,y)),\]

_where \(:\) is the sigmoid function \(_{s}:\), and \(a,b,c C()\) or by a wavelet activation function \(_{wire}:\), see Saragadam et al. (2023), where \(_{wire}(t)=(e^{i t}e^{-t^{2}})\) and \(a,b,c C()\), and \(a_{j}(x,y) 0\)._

In the proof of Proposition 3, we utilize the Leray-Schauder fix point theorem. By employing the Banach fix point theorem under a contraction mapping condition (4.3), we can obtain bijectivity as follows:

**Proposition 4**.: _Let \(:\) be bijective. Let \(W:L^{2}(D)^{n} L^{2}(D)^{n}\) be bounded linear bijective, and let \(K:L^{2}(D)^{n} L^{2}(D)^{n}\). If \(W^{-1}K:L^{2}(D)^{n} L^{2}(D)^{n}\) is a contraction mapping, that is, there exists \((0,1)\) such that_

\[\|W^{-1}K(u)-W^{-1}K(v)\|\|u-v\|,\;u,v L^{2} (D)^{n},\] (4.3)

_then, the operator \( F_{1}\) is bijective._

See Section E.3 in Appendix E for the proof. We note that if \(K\) is compact linear, then assumption (4.3) implies that \(W+K\) is an injective Fredholm operator with index zero, which is equivalent to \((W+K)\) being bijective as observed in Proposition 2. That is, Proposition 4 requires stronger assumptions when applied to the linear case.

Assumption (4.3) implies the the injectivity of \(K\). An interesting example of injective operators arises when \(K\) are Volterra operators. When \(D^{d}\) is bounded and \(K(u)=_{D}k(x,y,u(y))u(y)dy\), where we denote \(x=(x_{1},,x_{d})\) and \(y=(y_{1},,y_{d})\), we recall that \(K\) is a Volterra operator if \(k(x,y,t) 0\) implies \(y_{j} x_{j}\) for all \(j=1,2,,d\). A well known fact, as discussed in Example 4 in Appendix E, is that if \(K(u)\) is a Volterra operator whose kernel \(k(x,y,t) C(^{n})\) is bounded and uniformly Lipschitz in \(t\)-variable then \(F:u u+K(u)\) is injective.

**Remark 2**.: _A similar analysis (illustrating coercivity) shows that operators of the following two forms are bijective._

1. _Operators of the form_ \(F(U) u+K(u)\) _where_ \(K(u)_{D}a(x,y,u(x),u(y))dy\) _where_ \(a(x,y,s_{1},s_{2})\) _is continuous and is such that_ \( R>0\)_,_ \(c_{1}<\) _so that for all_ \(|(s_{1},s_{2})|>R\)_,_ \((s_{1})(s_{2})a(x,y,s_{1},s_{2})-c_{1}\)_._
2. _Layers of the form_ \((_{}v)(x)_{1}(Wu+_{2}(K(u)))\) _where_ \(_{1}\) _is bijective and_ \(_{2}\) _bounded._

_Finally we remark that coercivity is bounded preserved by perturbations in a bounded domain. This makes it possible to study non-linear and non-positive perturbations of physical models. For example, in quantum mechanics when a non-negative energy potential \(||^{4}\) is replaced by a Mexican hat potential \(-C||^{2}+||^{4}\), as occurs in the study of magnetization, superconductors and the Higgs field._

### Construction of the inverse of a non-linear integral neural operator

The preceding sections clarified sufficient conditions for surjectivity and bijectivity of the non-linear operator \(F_{1}\). We now consider how to construct the inverse of \(F_{1}\) in a compact set \(\). We find that constructing inverses is possible in a wide variety of settings and, moreover, that the inverses themselves can be given by neural operators. The proof that neural operators may be inverted with other neural operators provides a theoretical justification for Integral Auto Encoder networks  where an infinite encoder/decoder pair play a role parallel to those of encoder/decoders in finite-dimensional VAEs . This section proves that the decoder half of a IAE-net is provably able to inverse the encoder half. Our analysis also shows that injective differential operators (as arise in PDEs) and integral operator encoders form a formal algebra under operator composition. We prove this in the rest of this section, but first we summarize the main three steps of the proof.

First, by using the Banach fixed point theorem and invertibility of derivatives of \(F_{1}\) we show that, locally, \(F_{1}\) may be inverted by an iteration of a contractive operator near \(g_{j}=F_{1}(v_{j})\). This makes local inversion simple in balls which cover the set \(\). Second, we construct partition of unity functions \(_{j}\) that masks the support of each covering ball and allows us to construct one global inverse that simply passes through to the local inverse on the appropriate ball. Third and finally, we show that each function used in both of the above steps are representable using neural operators with distributional kernels.

As a simple case, let us first consider the case when \(n=1\), and \(D\) is a bounded interval, and the operator \(F_{1}\) of the form

\[F_{1}(u)(x)=W(x)u(x)+_{D}k(x,y,u(y))u(y)dy,\]

where \(W C^{1}()\) satisfies \(0<c_{1} W(x) c_{2}\) and the function \((x,y,s) k(x,y,s)\) is in \(C^{3}()\) and in \(\) its three derivatives and the derivatives of \(W\) are uniformly bounded by \(c_{0}\), that is,

\[\|k\|_{C^{3}()} c _{0},\|W\|_{C^{1}()} c_{0}.\] (4.4)

The condition (4.4) implies that \(F_{1}:H^{1}(D) H^{1}(D)\), contains locally Lipschitz smooth functions. Furthermore, \(F_{1}:H^{1}(D) H^{1}(D)\) is Frechet differentiable at \(u_{0} C()\), and we denote Frechetderivative of \(F_{1}\) at \(u_{0}\) by \(A_{u_{0}}\), which can be written as the integral operator (F.2). We will assume that for all \(u_{0} C()\), the integral operator

\[A_{u_{0}}:H^{1}(D) H^{1}(D).\] (4.5)

This happens for example when \(K(u)\) is a Volterra operator, see Examples 4 and 5. As the integral operators \(A_{u_{0}}\) are Fredholm operators having index zero, this implies that the operators (4.5) are bijective. The inverse operator \(A_{u_{0}}^{-1}:H^{1}(D) H^{1}(D)\) can be written by the integral operator (F.5).

We will consider the inverse function of the map \(F_{1}\) in \(_{a}(_{C^{1,}()}(0,R)) =\{_{a} g C():\|g\|_{C^{1,}()} R\}\), which is a set of the image of Holder spaces \(C^{n,}()\) through (leaky) ReLU-type functions \(_{a}(s)=(s)-a(-s)\) with \(a 0\). We note that \(\) is a compact subset the Sobolev space \(H^{1}(D)\), and we use the notations \(B_{}(g,R)\) and \(_{}(g,R)\) as open and closed balls with radius \(R>0\) at center \(g\) in Banach space \(\).

To this end, we will cover the set \(\) with small balls \(B_{H^{1}(D)}(g_{j},_{0})\), \(j=1,2,,J\) of \(H^{1}(D)\), centered at \(g_{j}=F_{1}(v_{j})\), where \(v_{j} H^{1}(D)\). As considered in detail in Appendix F, when \(g\) is sufficiently near to the function \(g_{j}\) in \(H^{1}(D)\), the inverse map of \(F_{1}\) can be written as a limit \((F_{1}^{-1}(g),g)=_{m}_{j}^{ m}(v_{j},g)\) in \(H^{1}(D)^{2}\), where

\[_{j}(u\\ g)(u-A_{v_{j}}^{-1}(F_{1}(u) -F_{1}(v_{j}))+A_{v_{j}}^{-1}(g-g_{j})\\ g),\]

that is, near \(g_{j}\) we can approximate \(F_{1}^{-1}\) as a composition \(_{j}^{ m}\) of \(2m\) layers of neural operators.

To glue the local inverse maps together, we use a partition of unity \(_{}\), \(\) in the function space \(\), where \(^{_{0}}\) is a finite index set. The function \(_{}\) are given by neural operators

\[_{}(v,w)=_{1}_{,1}_{,2} _{,_{0}}(v,w),_{, }(v,w)=(F_{y_{},s(,),_{1}}(v,w),w),\]

where some \(_{1}>0\), \(s(,)\) are some suitable values near \(g_{j()}(y_{})\), some \(y_{} D\) (\(=1,...,_{0}\)), and \(_{1}\) is the map \(_{1}(v,w)=v\) that maps a pair \((v,w)\) to the first function \(v\). Here, \(F_{z,s,h}(v,w)\) are integral neural operators with distributional kernels \(F_{z,s,h}(v,w)(x)=_{D}k_{z,s,h}(x,y,v(x),w(y))dy\), where \(k_{z,s,h}(x,y,v(x),w(y))=v(x)_{[s-h,s+h)}(w(y) )(y-z)\), and \(_{A}\) is a indicator function of a set \(A\) and \(y(y-z)\) is the Dirac delta distribution at the point \(z D\). Using these, we can write the inverse of \(F_{1}\) at \(g\) as

\[F_{1}^{-1}(g)=_{m}_{}_{} _{j()}^{ m}(v_{j()}\\ g)H^{1}(D)\] (4.6)

where \(j()\{1,2,,J\}\).

This result is summarized in following theorem which is proven in Appendix F.

**Theorem 3**.: _Assume that \(F_{1}\) satisfies the above assumptions (4.4) and (4.5) and that \(F_{1}:H^{1}(D) H^{1}(D)\) is a bijection. Let \(_{a}(_{C^{1,}()}(0,R))\) be a compact subset of the Sobolev space \(H^{1}(D)\), where \(>0\) and \(a 0\). Then the inverse of \(F_{1}:H^{1}(D) H^{1}(D)\) in \(\) can written as a limit (4.6) that is, as a limit of integral neural operators._

## 5 Discussion and Conclusion

In this paper, we provided a theoretical analysis of injectivity and bijectivity for neural operators. In the future, we will further develop applications of our theory, particularly in the areas of generative models and inverse problems and integral autoencoders. We gave a rigorous framework for the analysis of the injectivity and bijectivity of neural operators including when either the \(\) activation or bijective activation functions are used. We further proved that injective neural operators are universal approximators and their finite-rank implementation are still injective. Finally, we ended by considering the 'coarser' problem of non-linear integral operators, as arises in subnetworks, operator transformers and integral autoencoders.