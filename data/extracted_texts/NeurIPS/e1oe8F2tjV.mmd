# Multinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions

Kai Tan

Department of Statistics

Rutgers University

Piscataway, NJ 08854

kai.tan@rutgers.edu

&Pierre C. Bellec

Department of Statistics

Rutgers University

Piscataway, NJ 08854

pierre.bellec@rutgers.edu

###### Abstract

This paper investigates the asymptotic distribution of the maximum-likelihood estimate (MLE) in multinomial logistic models in the high-dimensional regime where dimension and sample size are of the same order. While classical large-sample theory provides asymptotic normality of the MLE under certain conditions, such classical results are expected to fail in high-dimensions as documented for the binary logistic case in the seminal work of Sur and Candes (2019). We address this issue in classification problems with 3 or more classes, by developing asymptotic normality and asymptotic chi-square results for the multinomial logistic MLE (also known as cross-entropy minimizer) on null covariates. Our theory leads to a new methodology to test the significance of a given feature. Extensive simulation studies on synthetic data corroborate these asymptotic results and confirm the validity of proposed p-values for testing the significance of a given feature.

## 1 Introduction

Multinomial logistic modeling has become a cornerstone of classification problems in machine learning, as witnessed by the omnipresence of both the cross-entropy loss (multinomial logistic loss) and the softmax function (gradient of the multinomial logistic loss) in both applied and theoretical machine learning. We refer to Cramer (2002) for an account of the history and early developments of logistic modeling.

Throughout, we consider a classification problem with \(K+1\) possible labels where \(K\) is a fixed constant. This paper tackles asymptotic distributions of multinomial logistic estimates (or cross-entropy minimizers) in generalized linear models with moderately high-dimensions, where sample size \(n\) and dimension \(p\) have the same order, for instance \(n,p+\) simultaneously while the ratio \(p/n\) converges to a finite constant. Throughout the paper, let \([n]=\{1,2,,n\}\) for all \(n\), and \(I\{\}\) be the 0-1 valued indicator function, equal to 1 if statement is true and 0 otherwise (e.g., \(I\{y_{i}=1\}\) in the next paragraph equals 1 if \(y_{i}=1\) holds and 0 otherwise).

The case of binary logistic regression.Let \((t)=(1+e^{t})\) be the logistic loss and \(^{}(t)=1/(1+e^{-t})\) be its derivative, often referred to as the sigmoid function. In the current moderately-high dimensional regime where \(n,p+\) with \(p/n>0\) for some constant \(\), recent works (Candes and Sur, 2020, Sur and Candes, 2019, Zhao et al., 2022) provide a detailed theoretical understanding of the behavior of the logistic Maximum Likelihood Estimate (MLE) in binary logistic regression models. Observing independent observations \((x_{i},y_{i})_{i[n]}\) from a logistic model defined as \((y_{i}=1|x_{i})=^{}(x_{i}^{T})\) where \(x_{i} N(,n^{-1}I_{p})\), and \(_{n}\|\|^{2}/n=^{2}\) for a constant \(\) for the limiting squared norm of the unknown regression vector \(\). These works prove that the behavior of the MLE \(=_{b^{p}}_{i=1}^{n}(x_{i}^{T}b)-I\{y_ {i}=1\}x_{i}^{T}b\) is summarized by the solution\((_{*},*,_{*})\) of the system of three equations

\[^{2}&=}[2^{}( Z_{ 1})(^{}(_{}(- Z_{1}+  Z_{2})))^{2}]\\ 0&=[^{}( Z_{1})^{}(_{}(- Z_{1}+ Z_{2}))]\\ 1-&=[2^{}( Z_{1})/1+^{ }(_{}(- Z_{1}+ Z_{2}))],\] (1.1)

where \((Z_{1},Z_{2})\) are i.i.d. \(N(0,1)\) random variables and the proximal operator is defined as \(_{}(z)=_{t} (t)+(t-z)^{2}/2}\). The system (1.1) characterize, among others, the following behavior of the MLE \(\): for almost any \((,)\), the system admits a solution if and only if \(\) exists with probability approaching one and in this case, \(\|\|^{2}/n\) and \(\|-\|^{2}/n\) both have finite limits that may be expressed as simple functions of \((_{*},_{*},_{*})\), and for any feature \(j[p]\) such that \(_{j}=0\) (i.e., \(j\) is a null covariate), the \(j\)-th coordinate of the MLE satisfies

\[_{j}^{}}N(0, _{*}^{2}).\]

The proofs in Sur and Candes (2019) are based on approximate message passing (AMP) techniques; we refer to Berthier et al. (2020); Feng et al. (2022); Gerbelot and Berthier (2021) and the references therein for recent surveys and general results. More recently, Zhao et al. (2022) extended the result of Sur and Candes (2019) from isotropic design to Gaussian covariates with an arbitrary covariance structure: if now \(x_{i} N(,)\) for some positive definite \(\) and \(_{n,p+}^{T}=\), null covariates \(j[p]\) (in the sense that \(y_{i}\) is independent of \(x_{ij}\) given \((x_{ik})_{k[p]\{j\}}\)) of the MLE satisfy

\[(n/_{jj})^{1/2}_{j}^{ }}N(0,_{*}^{2}),\] (1.2)

where \(_{*}\) is the same solution of (1.1) and \(=^{-1}\). Zhao et al. (2022) also obtained asymptotic normality results for non-null covariates, that is, features \(j[p]\) such that \(_{j} 0\). The previous displays can be used to test the null hypothesis \(H_{0}:y_{i}\) is independent of \(x_{ij}\) given \((x_{ik})_{k[p]\{j\}}\) and develop the corresponding p-values if \(_{*}\) is known; in this binary logistic regression model the ProbeFrontier (Sur and Candes, 2019) and SLOE Yadlowsky et al. (2021) give means to estimate the solutions \((_{*},_{*},_{*})\) of system (1.1) without the knowledge of \(\). Mai et al. (2019) studied the performance of Ridge regularized binary logistic regression in mixture models. Salehi et al. (2019) extended Sur and Candes (2019) to separable penalty functions. Bellec (2022) derived asymptotic normality results similar to (1.2) in single-index models including binary logistic regression without resorting to the system (1.1), showing that for a null covariate \(j[p]\) in the unregularized case that

\[(n/_{jj})^{1/2}(/)_{j}^{}}N(0,1)\] (1.3)

where \(=_{i=1}^{n}^{}(x_{i}^{T})- ^{}(x_{i}^{T})^{2}x_{i}^{T}[_{l=1}^{n}x_{l}^{ }(x_{l}^{T})x_{l}^{T}]^{-1}x_{i}\) is scalar and so is \(^{2}=_{i=1}^{n}(I\{y_{i}=1\}-^{}(x_{i}^{T}))^{2}\). In summary, in this high dimensional binary logistic model,

* The phase transition from Candes and Sur (2020) splits the \((,)\) plane into two connected components: in one component the MLE does not exist with high probability, in the other component the MLE exists and \(\|^{1/2}\|^{2}\) is bounded with high probability (boundedness is a consequence of the fact that \(\|^{1/2}\|^{2}\) or \(\|^{1/2}(-)\|^{2}\) admit finite limits);
* In the component of the \((,)\) plane where the MLE exists, for any null covariate \(j[p]\), the asymptotic normality results (1.2)-(1.3) holds.

Multiclass classification.The goal of this paper is to develop a theory for the asymptotic normality of the multinomial logistic regression MLE (or cross-entropy minimizer) on null covariates when the number of classes, \(K+1\), is greater than 2 and \(n,p\) are of the same order. In other words, we aim to generalize results such as (1.2) or (1.3) for three or more classes. Classification datasets with 3 or more classes are ubiquitous in machine learning (MNIST, CIFAR to name a few), which calls for such multiclass generalizations. In Gaussian mixtures and logistic models, Thrampoulidis et al. (2020) derived characterizations of the performance of of least-squares and class-averaging estimators, excluding cross-entropy minimizers or minimizers of non-linear losses. Loureiro et al. (2021) extended Sur and Candes (2019); Zhao et al. (2022); Salehi et al. (2019) to multiclass classification problems in a Gaussian mixture model, and obtained the fixed-point equations that characterize the performance and empirical distribution of the minimizer of the cross-entropy loss plus a convex regularizer. In the same vein as Loureiro et al. (2021); Cornacchia et al. (2022) studied the limiting fixed-point equations in a multiclass teacher-student learning model where labels are generated by a noiseless channel with response \(_{k\{1,...,K\}}x_{i}^{T}_{k}\) where \(_{k}^{p}\) is unknown for each class \(k\). These two aforementioned works assume a multiclass Gaussian mixture model, which is different than the normality assumption for \(x_{i}\) used in the present paper. More importantly, these results cannot be readily used for the purpose testing significant covariates (cf. (1.10) below) since solving the fixed-point equations require the knowledge of several unknown parameters, including the limiting spectrum of the mixture covariances and empirical distributions of the mixture means (cf. for instance Corollary 3 in Loureiro et al. (2021)). In the following sections, we fill this gap with a new methodology to test the significance of covariates. This is made possible by developing new asymptotic normality results for cross-entropy minimizers that generalize (1.3), without relying on the low-dimensional fixed-point equations.

Notation.Throughout, \(I_{p}^{p p}\) is the identity matrix, for a matrix \(A^{m n}\), \(A^{T}\) denotes the transpose of \(A\), \(A^{}\) denotes the Moore-Penrose inverse of \(A\). If \(A\) is psd, \(A^{1/2}\) denotes the unique symmetric square root, i.e., the unique positive semi-definite matrix such that \((A^{1/2})^{2}=A\). The symbol \(\) denotes the Kronecker product of matrices. Given two matrices \(A^{n k},B^{n q}\) with the same number or rows, \((A,B)^{n(k+q)}\) is the matrix obtained by stacking the columns of \(A\) and \(B\) horizontally. If \(v^{n}\) is a column vector with dimension equal to the number of rows in \(A\), we construct \((A,v)^{n(k+1)}\) similarly. We use \(_{n}\) and \(_{n}\) to denote the all-zeros vector and all-ones vector in \(^{n}\), respectively; we do not bold vectors and matrices other than \(_{n}\) and \(_{n}\). We may omit the subscript giving the dimension if clear from context; e.g., in \(I_{K+1}-}{K+1}\) the vector \(\) is in \(^{K+1}\). The Kronecker product between two matrices is denoted by \(\) and \((M)^{nd}\) is the vectorization operator applied to a matrix \(M^{n d}\). For an integer \(K 2\) and \((0,1)\), the quantile \(_{K}^{2}()\) is the unique real number satisfying \((W>_{K}^{2}())=\) where \(W\) has a chi-square distribution with \(K\) degrees of freedom. The symbols \(}}}{{}}\) and \(}}}{{}}\) denote convergence in distribution and in probability.

Throughout, _classical asymptotic regime_ refers to the scenario where the feature dimension \(p\) is fixed and the sample size \(n\) goes to infinity. In contrast, the term _high-dimensional regime_ refers to the situation where \(n\) and \(p\) both tend to infinity with the ratio \(p/n\) converging to a limit smaller than 1.

### Multinomial logistic regression

Consider a multinomial logistic regression model with \(K+1\) classes. We have \(n\) i.i.d. data samples \(\{(x_{i},_{i})\}_{i=1}^{n}\), where \(x_{i}^{p}\) is the feature vector and \(_{i}=(y_{i1},...,_{i(K+1)})^{T}^{K+1}\) is the response. Each response \(_{i}\) is the one-hot encoding of a single label, i.e., \(_{i}\{0,1\}^{K+1}\) with \(_{k=1}^{K+1}_{ik}=1\) such that \(_{ik}=1\) if and only if the label for \(i\)-th observation is \(k\). A commonly used generative model for \(_{i}\) is the multinomial regression model, namely

\[(_{ik}=1|x_{i})=^{T}^{*}e_{k})}{ _{k^{}=1}^{K+1}(x_{i}^{T}^{*}e_{k^{}})}, k \{1,2,,K+1\}\] (1.4)

where \(^{*}^{p(K+1)}\) is an unknown logistic model parameter and \(e_{k}^{K+1},e_{k^{}}^{K+1}\) are the \(k\)-th and \(k^{}\)-th canonical basis vectors. The MLE for \(^{*}\) in the model (1.4) is any solution that minimizes the cross-entropy loss,

\[}_{^{p(K+1)}}_{i=1} ^{n}_{i}(^{T}x_{i}),\] (1.5)

where \(_{i}:^{K+1}\) is defined as \(_{i}()=-_{k=1}^{K+1}_{ik}_{k}+ _{k^{}=1}^{K+1}(_{k^{}})\). If the solution set in (1.5) is non-empty, we define for each observation \(i[n]\) the vector of predicted probabilities \(}_{i}=(}_{i1},...,}_{i(K+1)})^{T}\) with

\[}_{ik}}}}{{}} (}_{ik}=1)=^{T}}e_{k})}{ _{k^{}=1}^{K+1}(x_{i}^{T}}e_{k^{}})} $}.\] (1.6)

Our results will utilize the gradient and Hessian of \(_{i}\) evaluated at \(}^{T}x_{i}\), denoted by

\[_{i}}}}{{}} _{i}(}^{T}x_{i})=-_{i}+}_{i}, _{i}}}}{{}} ^{2}_{i}(}^{T}x_{i})=(}_{i} )-}_{i}}_{i}^{T}.\] (1.7)The quantities \((},}_{i},_{i},_{i})\) can be readily computed from the data \(\{(x_{i},_{i})\}_{i=1}^{n}\). To be specific, the MLE \(}\) in (1.5) can be obtained by invoking a multinomial regression solver (e.g., sklearn.linear_model.LogisticRegression from Pedregosa et al. (2011)), and the quantities \(}_{i},_{i},_{i}\) can be further computed from eqs. (1.6) and (1.7) by a few matrix multiplications and application of the softmax function.

Log-odds model and reference class.The matrix \(^{*}\) in (1.4) is not identifiable since the conditional distribution of \(_{i}|x_{i}\) in the model (1.4) remains unchanged if we replace columns of \(^{*}\) by \(^{*}-b_{K+1}^{T}\) for any \(b^{p}\). In order to obtain an identifiable model, a classical and natural remedy is to model the log-odds, here with the class \(K+1\) as the reference class:

\[(_{ik}=1|x_{i})}{(_{i(K+1)} =1|x_{i})}=x_{i}^{T}A^{*}e_{k}, k[K]\] (1.8)

where \(e_{k}\) is the \(k\)-th canonical basis vector of \(^{K}\), and \(A^{*}^{p K}\) is the unknown parameter. The matrix \(A^{*}^{p K}\) in log-odds model (1.8) is related to \(^{*}^{p(K+1)}\) in the model (1.4) by \(A^{*}=^{*}(I_{K},-_{K})^{T}\). This log-odds model has two benefits: First it is identifiable since the unknown matrix \(A^{*}\) is uniquely defined. Second, the matrix \(A^{*}\) tends itself well to interpretation as its \(k\)-th column represents the contrast coefficient between class \(k\) and the reference class \(K+1\).

The MLE \(\) of \(A^{*}\) in (1.8) is \(=_{A^{p K}}_{i=1}^{n}_{i}((A, _{p})^{T}x_{i})\). If the solution set in (1.5) is non-empty, \(\) is related to any solution \(}\) in (1.5) by \(=}(I_{K},-_{K})^{T}\). Equivalently,

\[_{jk}=}_{jk}-}_{j(K+1)}\] (1.9)

for each \(j[p]\) and \(k[K]\). If there are three classes (i.e. \(K+1=3\)), this parametrization allows us to draw scatter plots of realizations of \(e_{j}^{T}=(_{j1},_{j2})\) as in Figure 1.

### Hypothesis testing for the \(j\)-th feature and classical asymptotic normality for MLE

Hypothesis testing for the \(j\)-th feature.Our goal is to develop a methodology to test the significance of the \(j\)-th feature. Specifically, for a desired confidence level \((1-)(0,1)\) (say, \(1-=0.95\)) and a given feature \(j[p]\) of interest, our goal is to test

\[H_{0}:_{i}\]

 is conditionally independent of

\[x_{ij}\]

 given

\[(x_{ij^{}})_{j^{}[p]\{j\}}\]

. (1.10)

Namely, we want to test whether the \(j\)-th variable is independent from the response given all other explanatory variables \((x_{ij^{}},j^{}[p]\{j\})\). Assuming normally distributed \(x_{i}\) and a multinomial model as in (1.4) or (1.8), it is equivalent to test

\[H_{0}:e_{j}^{T}A^{*}=_{K}^{T} H_{1}:e_{j}^{T}A^{*} _{K}^{T},\] (1.11)

where \(e_{j}^{p}\) is the \(j\)-th canonical basis vector.

If the MLE \(}\) in (1.5) exists in the sense that the solution set in (1.5) is nonempty, the conjecture that rejecting \(H_{0}\) when \(e_{j}^{T}}\) is far from \(_{K+1}\) is a reasonable starting point. The important question, then, is to determine a quantitative statement for the informal "far from \(_{K+1}\)", similarly to (1.2) or (1.3) in binary logistic regression.

Classical theory with \(p\) fixed.If \(p\) is fixed and \(n\) in model (1.8), classical maximum likelihood theory (Van der Vaart, 1998, Chapter 5) provides the asymptotic distribution of the MLE \(\), which can be further used to test (1.11). Briefly, if \(x\) has the same distribution as any \(x_{i}\), the MLE \(\) in the multinomial logistic model is asymptotically normal with

\[(()-(A^{*}))^{}}N(,^{-1 })=[(xx^{T})((^{*})- ^{*}^{*T})]\]

is the Fisher information matrix evaluated at the true parameter \(A^{*}\), \(()\) is the usual vectorization operator, and \(^{*}^{K}\) has random entries \(_{k}^{*}=(x^{T}A^{*}e_{k})(1+_{k^{}=1}^{K}(x^{T}A ^{*}e_{k^{}}))\) for each \(k[K]\). In particular, under \(H_{0}:e_{j}^{T}A^{*}=_{K}^{T}\),

\[^{T}e_{j}^{}}N( ,S_{j})\] (1.12)where \(S_{j}=(e_{j}^{T} I_{K})^{-1}(e_{j} I_{K})=e_{j}^{T}( (x))^{-1}e_{j}[\ ((^{*})-^{*}^{*T})]^{-1}\). When (1.12) holds, by the delta method we also have \(S_{j}^{-1/2}^{T}e_{j}}{}N( ,I_{K})\) and

\[n\|S_{j}^{-1/2}^{T}e_{j}\|^{2}}{} _{K}^{2}.\] (1.13)

where the limiting distribution is chi-square with \(K\) degrees of freedom. This further suggests the size \(\) test that rejects \(H_{0}\) when \(T_{n}^{j}(X,Y)>_{K}^{2}()\), where \(T_{i}^{j}(X,Y)=n\|S_{j}^{-1/2}^{T}e_{j}\|^{2}\) is the test statistic. If (1.13) holds, this test is guaranteed to have a type I error converging to \(\). The p-value of this test is given by

\[_{T_{n}^{k}(X,Y)}^{+}f_{_{K}^{2}}(t)dt,\] (1.14)

where \(f_{_{K}^{2}}()\) is the density of the chi-square distribution with \(K\) degrees of freedom.

As discussed in the introduction, Sur and Candes (2019) showed that in binary logistic regression, classical normality results for the MLE such as (1.12) fail in the high-dimensional regime because the variance in (1.12) underestimates the variability of the MLE even for null covariates; see also the discussion surrounding (1.2). Our goal is to develop, for classification problems with \(K+1 3\) classes, a theory that correctly characterize the asymptotic distribution of \(^{T}e_{j}\) for a null covariate \(j[p]\) in the high-dimensional regime.

We present first some motivating simulations that demonstrate the failure of classical normal approximation (1.12) in finite samples. These simulations are conducted for various configurations of \((n,p)\) with \(K+1=3\) classes. We fix the true parameter \(A^{*}\) and obtain 1000 realizations of \((_{j1},_{j2})\) by independently resampling the data \(\{(x_{i},_{i})\}_{i=1}^{n}\) 1000 times. If the result (1.12) holds, then \((^{T}e_{j}_{}^{j}) 1-\), where \(_{}^{j}=\{u^{K}:\|S_{j}^{-1/2}u\|_{K}^{2 }()\}\). Figure 1 displays scatter plots of \((_{j1},_{j2})\) along with the boundary of 95% confidence set \(_{}^{j}\) with \(=0.05\). We observe that, across the three different configurations of \((n,p)\), the 95% confidence sets from our theory (Theorem 2.2 presented in next section) cover around 95% of the realizations, while the set \(_{}^{j}\) from classical theory only covers approximately \(30\%\) of the points, which is significantly lower than the desired coverage rate of \(95\%\). Intuitively and by analogy with results in binary classification (Sur and Candes, 2019), this is because the classical theory (1.12) underestimates the variation of the MLE in the high-dimensional regime. Motivated by this failure of classical MLE theory and the results in binary classification (Sur and Candes, 2019, among others), the goal of this paper is to develop a theory for multinomial logistic regression that achieves the following objectives:

Figure 1: Scatter plot of pairs \((_{j1},_{j2})\) with \(K=2\) over 1000 repetitions. The blue ellipsoid is the boundary of the \(95\%\) confidence set for \(^{T}e_{j}\) under \(H_{0}\) from the classical MLE theory (1.12)-(1.13) based on the Fisher information, the dashed red ellipsoids are the boundaries of the \(95\%\) confidence set for \(^{T}e_{j}\) under \(H_{0}\) from this paper (cf. (2.3) below). Each of the 1000 repetition gives a slightly different dashed ellipsoid. The solid red ellipsoid is the average of these 1000 dashed ellipsoids. Each row of \(X\) is i.i.d. sampled from \(N(,)\) with \(=(0.5^{|i-j|})_{p p}\). The first \( p/4\) rows of \(A^{*}\) are i.i.d. sampled from \(N(,I_{K})\) while other rows are set to zeros. We further normalize \(A^{*}\) such that \(A^{*T} A^{*}=I_{K}\). The last coordinate \(j=p\) is used as the null coordinate.

* Establish asymptotic normality of the multinomial MLE \(^{T}e_{j}\) for null covariates as \(n,p+\) simultaneously with a finite limit for \(n/p\).
* Develop a valid methodology for hypothesis testing of (1.10) in this regime, i.e., testing for the presence of an effect of a feature \(j[p]\) on the multiclass response.

The contribution of this paper is two-fold: (i) For a null covariate \(j[p]\), we establish asymptotic normality results for \(^{T}e_{j}\) that are valid in the high-dimensional regime where \(n\) and \(p\) have the same order; (ii) we propose a user-friendly test for assessing the significance of a feature in multiclass classification problems.

Main result: asymptotic normality of \(}^{T}e_{j}\) and \(^{T}e_{j}\) on null covariates

In this section, we present the main theoretical results of our work and discuss their significance. We work under the following assumptions.

**Assumption 2.1**.: For constants \(>1\), assume that \(n,p\) with \(p/n^{-1}\), and that the design matrix \(X^{n p}\) has \(n\) i.i.d. rows \((x_{i})_{i[n]} N(,)\) for some invertible \(^{p p}\). The observations \((x_{i},_{i})_{i[n]}\) are i.i.d. and each \(_{i}\) is of the form \(_{i}=f(U_{i},x_{i}^{T}^{*})\) for some deterministic function \(f\), deterministic matrix \(^{*}^{p(K+1)}\) such that \(^{*}_{K+1}=_{p}\), and latent random variable \(U_{i}\) independent of \(x_{i}\).

**Assumption 2.2** (One-hot encoding).: The response matrix \(Y\) is in \(^{n(K+1)}\). Its \(i\)-th row \(_{i}\) is a one-hot encoded vector, that is, valued in \(\{0,1\}^{K+1}\) with \(_{k=1}^{K+1}_{ik}=1\) for each \(i[n]\).

The model \(_{i}=f(U_{i},x_{i}^{T}^{*})\) for some deterministic \(f\) and \(^{*}\) and latent random variable \(U_{i}\) in Assumption 2.1 is more general than a specific generative model such as the multinomial logistic conditional probabilities in (1.4), as broad choices for \(f\) are allowed. In words, the model \(_{i}=f(U_{i},x_{i}^{T}^{*})\) with \(^{*}_{K+1}=0_{p}\) means that \(_{i}\) only depends on \(x_{i}\) through a \(K\) dimensional projection of \(x_{i}\) (the projection on the row-space of \(^{*}\)). The assumption \(p/n^{-1}\) is more general than assuming a fixed limit for the ratio \(p/n\); this allows us to cover low-dimensional settings satisfying \(p/n 0\) as well.

The following assumption requires the labels to be "balanced": we observe each class at least \( n\) times for some constant \(>0\). If \((_{i})_{i[n]}\) are i.i.d. as in Assumption 2.1 with distribution independent of \(n,p\), by the law of large numbers this assumption is equivalent to \(_{k[K+1]}(_{ik}=1)>0\).

**Assumption 2.3**.: There exits a constant \((0,]\), such that for each \(k[K+1]\), with probability approaching one at least \( n\) observations \(i[n]\) are such that \(_{ik}=1\). In other words, \((_{i=1}^{n}I(_{ik}=1) n) 1\) for each \(k[K+1]\).

As discussed in item list (i) on page 2, in binary logistic regression, Candes and Sur (2020), Sur and Candes (2019) show that the plane \((,\|^{1/2}^{*}\|)\) is split by a smooth curve into two connected open components: in one component the MLE does not exist with high probability, while in the other component, with high probability the MLE exists and is bounded in the sense that \(\|^{1/2}\|^{2}<^{}\) or equivalently \(\|X\|^{2}<\) for constants \(,^{}\) independent of \(n,p\). The next assumption requires the typical situation of the latter component, in the current multiclass setting: \(}\) in (1.5) exists in the sense that the minimization problem has solutions, and at least one solution is bounded.

**Assumption 2.4**.: Assume \((}\) exists and \(\|X}(I_{K+1}-^{T}}{K+1})\|_{F}^{2} n ) 1\) as \(n,p+\) for some large enough constant \(\).

Note that the validity of Assumption 2.4 can be assessed using the data at hand; if a multinomial regression solver (e.g. sklearn.linear_model.LogisticRegression) converges1 and \(\|X}(I_{K+1}-^{T}}{K+1})\|_ {F}^{2}\) is no larger than a predetermined large constant \(\), then we know Assumption 2.4 holds. Otherwise the algorithm does not converge or produces an unbounded estimate: we know Assumption 2.4 fails to hold and we need collect more data.

Our first main result, Theorem 2.1, provides the asymptotic distribution of \(}^{T}e_{j}\) where \(j[p]\) is a null covariate, where \(}\) is any minimizer \(}\) of (1.5). Throughout, we denote by \(\) the precision matrix defined as \(=^{-1}\).

**Theorem 2.1**.: _Let Assumptions 2.1 to 2.4 be fulfilled. Then for any \(j[p]\) such that \(H_{0}\) in (1.10) holds, and any minimizer \(}\) of (1.5), we have_

\[}}}_{} _{i=1}^{n}_{i}_{i}^{T} ^{1/2}}_{^{(K+1)(K+1)}}^{}\,\, _{i=1}^{n}_{i}\,\,}^{T}e_{j} }}{{}}\!\!N, -^{T}}{K+1}}_{\, ^{(K+1)(K+1)}},\] (2.1)

_where \(_{i}=-_{i}+}_{i}\) as in (1.7) and \(_{i}=_{i}-(_{i} x_{i}^{T})[_{l=1}^{n} _{l}(x_{i}x_{l}^{T})]^{}(_{i} x_{i})\)._

The proof of Theorem 2.1 is given in Supplementary Section S3. Theorem 2.1 establishes that under \(H_{0}\), \(}^{T}e_{j}\) converges to a singular multivariate Gaussian distribution in \(^{K+1}\). In (2.1), the two matrices \(_{i=1}^{n}(_{i}-}_{i})(_{i} -}_{i})^{T}\) and \(_{i=1}^{n}_{i}\) are symmetric with kernel being the linear span of \(_{K+1}\), and similarly, if a solution exists, we may replace \(}\) by \(}(I_{K+1}-^{T}}{K+1})\) which is also solution in (1.5). In this case, all matrix-matrix and matrix-vector multiplications, matrix square root and pseudo-inverse in (2.1) happen with row-space and column space contained in the orthogonal component of \(_{K+1}\), so that the limiting Gaussian distribution in \(^{K+1}\) is also supported on this \(K\)-dimensional subspace.

Since the distribution of the left-hand side of (2.1) is asymptotically pivotal for all null covariates \(j[p]\), Theorem 2.1 opens the door of statistical inference for multinomial logistic regression in high-dimensional settings. By construction, the multinomial logistic estimate \(^{p K}\) in (1.9) ensures \((,_{p})\) is a minimizer of (1.5). Therefore, we can deduce the following theorem from Theorem 2.1.

**Theorem 2.2**.: _Define the matrix \(R=(I_{K},_{K})^{T}^{(K+1) K}\) using block matrix notation. Let Assumptions 2.1 to 2.4 be fulfilled. For \(\) in (1.9) and any \(j[p]\) such that \(H_{0}\) in (1.10) holds,_

\[I_{K}+_{K}_{K}^{T}}{ }R^{T}}_{\,\,^{K(K+1)}}}}}_{} _{i=1}^{n}_{i}_{i}^{T}^{1/2}}_{ \,\,^{(K+1)(K+1)}}^{}\,\,_ {i=1}^{n}_{i}R\,\,^{T}e_{j}}}{{}}\!\!N(_{K},I_{K})\] (2.2)

_where \(_{i}\) is defined in (1.7) and \(_{i}\) is defined in Theorem 2.1. Furthermore, for the same \(j[p]\),_

\[_{n}^{j}(X,Y)\!f}}{{}} }_{i=1}^{n}_{i}_{i}^{T}^{1/2}^{}_{ i=1}^{n}_{i}R^{T}e_{j}^{2}\,\,\,\,_{n}^{j}(X,Y)}}{{ }}\!_{K}^{2}.\] (2.3)

_Note that Equations (2.2) and (2.3) is stated using \(_{jj}=e_{j}^{T}^{-1}e_{j}\). When \(\) is unknown, the quantity \(_{jj}\) in above results can be replaced by its consistent estimate \(_{jj}\) defined in (2.5), and the convergence in distribution results still hold._

Theorem 2.2 is proved in Supplementary Section S4. To the best of our knowledge, Theorem 2.2 is the first result that characterizes the distribution of null MLE coordinate \(^{T}e_{j}\) in high-dimensional multinomial logistic regression with 3 or more classes. It is worth mentioning that the quantities \((_{i},_{i},)\) used in Theorem 2.2 can be readily computed from the data \((X,Y)\). Therefore, Theorem 2.2 lets us test the significance of a specific feature: for testing \(H_{0}\), this theorem suggests the test statistic \(_{n}^{j}(X,Y)\) in (2.3) and the rejection region \(_{}^{j}\!f}}{{}} (X,Y):_{n}^{j}(X,Y)_{K}^{2}()}\). Under the null hypothesis \(H_{0}\) in (1.10), Theorem 2.2 guarantees \((X,Y)_{}^{j}\). In other words, the test that rejects \(H_{0}\) if \((X,Y)_{}^{j}\) has type I error converging to \(\). The p-value of this test is

\[=_{_{n}^{j}(X,Y)}^{+}f_{_{K}^{2}}(t)dt,\] (2.4)

where \(f_{_{K}^{2}}()\) is the density of the chi-square distribution with \(K\) degrees of freedom.

**Unknownon \(_{jj}=e_{j}^{T}^{-1}e_{j}\).** If \(\) is unknown, we describe a consistent estimate of the quantity \(_{jj}\) appearing in (2.1), (2.2), and (2.3). Under the Gaussian Assumption 2.1, the quantity \(_{jj}\) is the reciprocal of the conditional variance \((x_{ij}|x_{i,-j})\), which is also the noise variance in the linear model of regressing \(Xe_{j}\) onto \(X_{-j}\) (the submatrix of \(X\) excluding the \(j\)-th column). According to standard results in linear models, we have \(_{jj}\|[I_{n}-X_{-j}(X_{-j}^{T}X_{-j})^{-1}X_{-j}^{T}]Xe_{j}\|^{2} _{n-p+1}^{2}\). Since \(_{n-p+1}^{2}/(n-p+1) 1\) almost surely by the strong law of large numbers,

\[_{jj}=(n-p+1)\|[I_{n}-X_{-j}(X_{-j}^{T}X_{-j})^{-1}X_{-j}^{T }]Xe_{j}\|^{2}\] (2.5)

is a consistent estimator of \(_{jj}\). Therefore, the previous asymptotic results in Theorems 2.1 and 2.2 still hold by Slutsky's theorem if we replace \(_{jj}\) by the estimate \(_{jj}\) in (2.5).

## 3 Numerical experiments

This section presents simulations and a real data analysis to examine finite sample properties of the above results and methods. The source code for generating all of the experimental results in this paper can be found in the supplementary material.

**Simulation settings.** We set \(p=1000\) and consider different combinations of \((n,K)\). The covariance matrix \(\) is specified to be the correlation matrix of an AR(1) model with parameter \(=0.5\), that is, \(=(0.5^{|i-j|})_{p p}\). We generate the regression coefficients \(A^{*}^{p K}\) once and for all as follows: sample \(A_{0}^{p K}\) with first \( p/4\) rows being i.i.d. \(N(,I_{K})\), and set the remaining rows to 0. We then scale the coefficients by defining \(A^{*}=A_{0}(A_{0}^{T} A_{0})^{-1/2}\) so that \(A^{*T} A^{*}=I_{K}\). With this construction, the \(p\)-th variable is always a null covariate, and we use this null coordinate \(j=p\) to demonstrate the effectiveness of our theoretical results presented in Theorem 2.2 and the suggested test for testing \(H_{0}\) as described in (1.10). Using the above settings, we generate the design matrix \(X^{n p}\) from \(N(0,)\), and then simulate the labels from a multinomial logistic model as given in (1.8), using the coefficients \(A^{*}^{p K}\). For each simulation setting, we perform 5,000 repetitions.

**Assessment of \(^{2}\) approximations.** To assess the \(^{2}\) approximation (2.3) from this paper and that of the classical theory (1.13), we compute the two \(^{2}_{K}\) test statistics for each sample \((x_{i},y_{i})_{i=1}^{n}\). Figure 2 shows the empirical quantiles of the two statistics versus the \(^{2}_{K}\) distribution quantiles. The results demonstrate that the quantiles (in blue) from our high-dimensional theory closely match the 45-degree line (in red), whereas the quantiles (in orange) from the classical theory significantly deviate from the 45-degree line. These findings highlight the accuracy of our proposed \(^{2}\) approximation (2.3) over the classical result (1.13) when \(p\) is not sufficiently small compared to \(n\).

**Uniformity of null p-values.** Recall that the p-value from the classical test (1.13) is given by (1.14), while the p-value from this paper taking into account high-dimensionality is given by (2.4). Figure 3 displays the histograms of these two sets of p-values out of 5000 repetitions. The results in Figure 3 show that the p-values obtained from the classical test deviate significantly from the uniform distribution, with a severe inflation in the lower tail. This indicates that the classical test tends to produce large type I errors due to the excess of p-values close to 0. In contrast, the p-values proposed in this paper exhibit a uniform distribution, further confirming the effectiveness and applicability of the theory in Theorem 2.2 for controlling type I error when testing for null covariates with (1.10).

Figure 2: Q-Q plots of the test statistic in the left-hand side of (1.13) (in orange) and in the left-hand side of (2.3) (in blue) for different \((n,K)\) and \(p=1000\).

**Unknown \(_{jj}\).** In the situation where the covariance matrix \(\) is unknown, we can estimate the diagonal element \(_{jj}=e_{j}^{T}^{-1}e_{j}\) by \(_{jj}\) defined in (2.5). To evaluate the accuracy of the normal and chi-square approximations and the associated test with \(_{jj}\) replaced by \(_{jj}\), we conduct simulations similar to those in Figures 2 and 3, but we replace \(_{jj}\) with its estimate \(_{jj}\). The results are presented in Figure S1. The plots are visually indistinguishable from the plots using \(_{jj}\). These confirm that the chi-square approximation and the associated test using \(_{jj}\) are accurate.

**Non-Gaussian covariates and unknown \(_{jj}\).** Although our theory assumes Gaussian covariates, we expect that the same results hold for other distributions with sufficiently light tails. To illustrate this point, we consider the following two types of non-Gaussian covariates: (i) The design matrix \(X\) has i.i.d. Rademacher entries, i.e., \((x_{ij}= 1)=\), (ii) Each \(x_{ij}\) takes on values 0, 1 and 2 with respectively probabilities \(a_{j}^{2},2a_{j}(1-a_{j})\), and \((1-a_{j})^{2}\), where \(a_{j}\) varies in \([0.25,0.75]\). Each columns of \(X\) are then centered and normalized to have 0 mean and unit variance. This generation of non-Gaussian covariates is adopted from single-nucleotide poly-morphisms (SNPs) example in Sur and Candes (2019). For these two types of non-Gaussian covariates, we further rescale the feature vectors to ensure that \(x_{i}\) has the same covariance as in the Gaussian case at the beginning of Section 3, that is \(=(0.5^{[i-j]})_{p p}\). We present the Q-Q plots in Figure S2 using the same settings as in Figure S1, with the only difference being that the covariates in Figure S2 are non-Gaussian distributed. The Q-Q plots of \(_{n}^{j}(X,Y)\) in (2.3) plotted in Figure S2 still closely match the diagonal line. These empirical successes suggest that the normal and \(_{K}^{2}\) approximations (2.1)-(2.3) apply to a wider range of covariate distributions beyond normally distributed data.

**Real data example.** We conduct a real data analysis by applying the proposed test to heart disease data from the UCI Machine Learning Repository (link: http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data). After standard data-cleaning processes, the dataset has 297 instances with 13 features, including age, sex, and other attributes. The response variable was transformed into 3 classes (0, 1, and 2) after converting the labels 3 and 4 to 2. To demonstrate the validity of the proposed significance test, we generate a noise variable from a standard normal distribution, resulting in a dataset with 297 instances and 14 variables. We test the significance of this noise variable using both the proposed chi-square test and the classical test, repeating the experiment 10,000 times. The type I error of our proposed test is 0.0508, aligning well with the desired type I error of 0.05. In contrast, the classical test exhibits a type I error of 0.0734, significantly exceeding the desired rate of 0.05. These results confirm the validity of our proposed test on real data corrupted with fake covariate, while the classical test concludes that the fake covariate is significant in more than 7% of experiments, leading to false discoveries exceeding the desired 0.05 type I error.

## 4 Discussion and future work

Multinomial logistic regression estimates and their p-values are ubiquitous throughout the sciences for analyzing the significance of explanatory variables on multiclass responses. Following the seminal work of Sur and Candes (2019) in binary logistic regression, this paper develops the first valid tests and p-values for multinomial logistic estimates when \(p\) and \(n\) are of the same order. For 3 or more

Figure 3: Histogram for p-values of the classical test (1.14) (in orange) and of the proposed test (2.4) (in blue) under \(H_{0}\) in simulated data with different \((n,K)\) and \(p=1000\).

classes, this methodology and the corresponding asymptotic normality results in Theorems 2.1 and 2.2 are novel and provide new understanding of multinomial logistic estimates (also known as cross-entropy minimizers) in high-dimensions. We expect similar asymptotic normality and chi-square results to be within reach for loss functions different than the cross-entropy or a different model for the response \(_{i}\); for instance Section S1 provides an extension to the \(q\)-repeated measurements model, where \(q\) responses are observed for each feature vector \(x_{i}\).

Let us point a few follow-up research directions that we leave open for future work. A first open problem regards extensions of our methodology to confidence sets for \(_{j}^{T}^{*}\) when \(H_{0}\) in (1.10) is violated for the \(j\)-th covariate. This would require more stringent assumptions on the generative model than Assumption 2.1 as \(^{*}\) there is not identifiable (e.g., modification of both \(^{*}\) and \(f(,)\) in Assumption 2.1 is possible without changing \(_{i}\)). A second open problem is to relate this paper's theory to the fixed-point equations and limiting Gaussian model obtained in multiclass models, e.g., Loureiro et al. (2021). While it may be straightforward to obtain the limit of \(_{i=1}^{n}_{i}_{i}^{T}\) and of the empirical distribution of the rows of \(\) in this context (e.g., using Corollary 3 in Loureiro et al. (2021)), the relationship between the fixed-point equations and the matrix \(_{i=1}^{n}_{i}\) appearing in (2.1) is unclear and not explained by typical results from this literature. A third open problem is to characterize the exact phase transition below which the multinomial logistic MLE exists and is bounded with high-probability (Assumption 2.4); while this is settled for two classes (Candes and Sur, 2020) and preliminary results are available for 3 or more classes (Loureiro et al., 2021; Kini and Thrampoulidis, 2021), a complete understanding of this phase transition is currently lacking. A last interesting open problem is to prove that our theory extend to non-Gaussian data, as observed in simulations. This challenging problem is often referred to as "universality" and has received intense attention recently (Gerace et al., 2022; Han and Shen, 2022; Montanari and Saeed, 2022; Pesce et al., 2023; Dandi et al., 2023), showing that in several settings of interest (although none exactly matching the one considered in the present paper), the asymptotic behavior of the minimizers is unchanged if the distribution of the covariates is modified from normal to another distribution with the same covariance.