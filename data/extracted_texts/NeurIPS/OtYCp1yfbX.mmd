# Improved Guarantees for Fully Dynamic \(k\)-Center Clustering with Outliers in General Metric Spaces

Improved Guarantees for Fully Dynamic \(k\)-Center Clustering with Outliers in General Metric Spaces

 Leyla Biabani

Eindhoven University of Technology

Eindhoven, The Netherlands

l.biabani@tue.nl

&Annika Hennes

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

annika.hennes@hhu.de

Denise La Gordt Dillie

Eindhoven University of Technology

Eindhoven, The Netherlands

lagordtdilliedenise@gmail.com

&Morteza Monemizadeh

Eindhoven University of Technology

Eindhoven, The Netherlands

m.monemizadeh@tue.nl

Melanie Schmidt

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

mschmidt@hhu.de

###### Abstract

The metric \(k\)-center clustering problem with \(z\) outliers, also known as \((k,z)\)-center clustering, involves clustering a given point set \(P\) in a metric space \((M,d)\) using at most \(k\) balls, minimizing the maximum ball radius while excluding up to \(z\) points from the clustering. This problem holds fundamental significance in various domains, such as machine learning, data mining, and database systems.

This paper addresses the fully dynamic version of the problem, where the point set undergoes continuous updates (insertions and deletions) over time. The objective is to maintain an approximate \((k,z)\)-center clustering with efficient update times. We propose a novel fully dynamic algorithm that maintains a \((4+)\)-approximate solution to the \((k,z)\)-center clustering problem that covers all but at most \((1+)z\) points at any time in the sequence with probability \(1-k/^{( k)}\). The algorithm achieves an expected amortized update time of \((^{-3}k^{6}(k)())\), and is applicable to general metric spaces. Our dynamic algorithm presents a significant improvement over the recent dynamic \((14+)\)-approximation algorithm by Chan, Lattanzi, Sozio, and Wang  for this problem.

## 1 Introduction

Clustering problems and algorithms play an important role across a multitude of fields, helping researchers and practitioners in the analysis of data and identification of patterns. These techniques find extensive application in diverse domains, including machine learning, where they help in categorizing and understanding complex datasets. In data mining, clustering methods are utilized to uncover hidden structures and relationships within large datasets, facilitating better decision-making and insight generation. Moreover, in image and signal processing, clustering algorithms assist in segmenting and classifying data, enabling tasks such as image recognition and signal denoising.

In bioinformatics, clustering techniques are essential for organizing biological data and identifying patterns in genetic sequences, protein structures, and gene expression profiles. Similarly, in anomaly detection, clustering methods are employed to identify unusual or unexpected patterns in data, which may indicate potential anomalies or security breaches. Furthermore, in social network analysis, clustering algorithms help in understanding the structure and dynamics of social networks by identifying communities and influential nodes.

The \(k\)-center problem is known as one of the fundamental clustering problems. Given a set of points \(P\) in a metric space and a number \(k\), the aim of the \(k\)-center problem is to find \(k\) centers such that the maximum distance between any point and its closest center is minimized. This can also be equivalently formulated as finding a minimum radius \(r\) and centers \(c_{1},,c_{k}\) such that the balls \(_{i=1}^{k}_{P}(c_{i},r)\) cover the point set. The \(k\)-center problem can be \(2\)-approximated, and this is the best possible approximation guarantee . In the last decade, the focus has shifted to analyzing the problem under various complications that arise in applications.

One line of research is to study the \(k\)-center problem (and other clustering problems) in different computational models like _streaming_ or for _dynamic_ point sets. In the first case, points arrive sequentially, and only a summary can be stored in memory. In the second case, the point set is maintained by insertion queries and deletion queries for single points, and algorithms have to update their solution after any such query.

Another line of research is to study clustering under constraints. For example, _capacitated_ clustering is very popular, i.e., limiting the number of points per cluster. However, lower bounds on cluster sizes have also been studied in the context of anonymity, and newer works have also considered constraints that model societal concerns like fair or diverse composition of clusters. In this paper, we study clustering _with outliers_. Formulated as a constraint, the \(k\)-center problem with outliers allows \(k+z\) centers, but \(z\) of these have to be singletons, meaning no point may be assigned to them. We can intuitively formulate it with balls as follows: The \(k\)-center problem with outliers asks to find a minimum \(r\) and \(k\) centers \(c_{1},,c_{k}\) such that the balls \(_{i=1}^{k}_{P}(c_{i},r)\) cover all but \(z\) points.

Solving clustering problems in the presence of outliers is a crucial task due to the common occurrence of measurement errors or other sources of significant deviation from the rest of the data in real-world datasets. Ignoring outliers can severely distort the results of clustering algorithms, leading to inaccurate groupings. To address this challenge, a common approach is to solve a clustering problem while excluding up to \(z\) data points considered as outliers.

The first approximation algorithm for the \(k\)-center problem with outliers is due to Charikar et al. . The challenge when designing approximation algorithms in the presence of outliers is that one needs to show that _enough_ points are covered by balls of bounded sizes around the approximate centers. It is not necessary to identify the outliers of an optimal solution _exactly_ as long as the number of uncovered points remains small enough. Due to this,  and follow-up papers use _charging_ arguments. Points covered by the solution of the algorithm are mapped to points in optimum clusters, and it is then shown that the number of uncharged points is small enough.

We explore the \(k\)-center clustering problem with \(z\) outliers within the fully dynamic model, where the point set experiences continuous updates through insertions and deletions over time. Quite some work on this problem has been done for inputs from metric spaces with _bounded doubling dimension_. This setting allows for geometric data structures that allow easier navigation through the data set and also bounding of the number of changes that can occur due to a query by dimension-dependent volume arguments.

We study the general metric setting. General metric spaces represent a key objective for clustering algorithms due to their broad range of distance functions, ensuring applicability to any data type. Chan et al. recently showed that in general metric spaces, any dynamic \((1)\)-approximation algorithm for \(k\)-center clustering excluding at most \(z\) outliers has an amortized update time of \((z)\). For real-world applications, the fraction of outliers of the data could be arbitrarily large. Therefore, we allow for \((1+)z\) outliers to be excluded to avoid the dependence on \(z\) of the update time.

The only previous work in this setting is the algorithm by Chan et al. , which returns a solution with at most \((1+)z\) outliers with probability at least \(1-\), for \(0<\) and \(>0\). It works by maintaining a clustering with \(t:=k_{1+}\) clusters of radius \(2r\) and using this to derive a final clustering with \(k\) clusters and radius \(14r\). They maintain such a clustering for all \(r:=\{(1+)^{i}:d_{}(1+)^{i}(1+) d_{ },i\}\), with \(d_{}\) and \(d_{}\) the minimum and maximum distances, respectively, between any two points ever inserted. It is then shown that there exists an instance \(r\) that will approximate the optimal radius to within a factor \((14+)\)while allowing for \((1+)z\) outliers, with probability at least \(1-\). The amortized time per update is \((||}{^{2}}^{2})\), with \(||=(}}{d_{}})/\). The total memory requirement is \((|||n|)\) where \(n\) is the number of points in the current set.

### Our contribution

We introduce a novel fully dynamic \((4+)\)-approximation algorithm designed to maintain a \(k\)-center clustering while allowing for at most \((1+)z\) outliers at any time in the sequence.

The expected amortized update time is \((^{-3}k^{6}(k)())\) per operation (insertion or deletion). This is independent of \(z\) and of the number of points currently present in the sequence. This characteristic makes the algorithm applicable to real-world scenarios.

Notice that our data structure is continually storing an actual solution, so we can produce this solution at any time and do not need to specify additional query times (like, for example [9; 2; 18]).

Our main technical contribution is a novel combination of the sampling-based level data structure by Chan et al.  and the original greedy strategy by Charikar et al.  that enables us to achieve a much improved approximation guarantee compared to .

**Theorem 1.1**.: _Let \((M,d)\) be a metric space and \(>0\) be an accuracy parameter. The spread ratio \(=}}{d_{}}\) of all points ever inserted is assumed to be bounded. There exists a randomized fully dynamic algorithm that maintains a \(k\)-center solution that allows up to \((1+)z\) many outliers on the current set of points. At every point in time, the current clustering is a \((4+)\)-approximation to an optimal solution for the \((k,z)\)-center problem with high probability. Upon insertion or deletion of a point, the data structure is updated in amortized update time \((^{-3}k^{6}(k)())\)._

To achieve this, we make use of a data structure that is described in Section 2.1 and maintained by the respective algorithms handling updates to the point set. The algorithm handling insertions is specified in Section 2.2, and the deletion algorithm is described in Section 2.3. Similar to the approach by Chan et al. , we maintain a hierarchical structure consisting of levels, each containing one cluster. Unlike their work, however, we only maintain \( k\) levels. For the correct radius guess, the union of these levels directly gives the desired \((4+)\)-approximation. If a level violates certain properties, this and all higher levels are reclustered. As opposed to , we do not need to recluster every time a center gets deleted but only when a cluster does not contain enough points anymore. The algorithm handling the reclustering is described in Section 2.4.

Our analysis of the approximation ratio follows a similar argument as the proof of the 3-approximation for static \(k\)-center with \(z\) outliers as given by Charikar et al.  and works by charging points from chosen clusters to points in optimal clusters. We follow the same iterative charging method but extend their approach by maintaining a set of artificial outliers in order to adjust the argument to our algorithm specifically.

Note that we assume that a center can be any point from the underlying metric space, if the point was present in the data set once. Requiring that centers can only be placed at currently active points, we can achieve a 6-approximation. We formalize this in Lemma G.1.

### Further Related work

The \(k\)-center problem is very well understood; two \(2\)-approximation algorithms for it are known [13; 15] and a matching lower bound is also known . The \(k\)-center problem with outliers was first studied and \(3\)-approximated in . In 2016,  gave a \(2\)-approximation for this problem, but the algorithm is rather complex and less amenable to practical implementation. Charikar et al.  developed an elegant algorithm to maintain an \(8\)-approximation for the vanilla \(k\)-center problem in the streaming model. Streaming can be seen as a dynamic insertion-only model. McCutchen and Khuller  improved this algorithm to a \((2+)\)-approximation and also extended it to a \((4+)\)-approximation for \(k\)-center with outliers. The \(k\)-center problem with outliers has also been studied in the sliding window model [10; 18] that also bears some similarity with our setting. For bounded doubling dimension,  gave a \((3+)\)-approximation, but the algorithm for the general metric case is only stated as a \((1)\)-approximation.

The fully dynamic \(k\)-center problem without outliers in general metrics was studied by [1; 6; 11], and the best-known result is a \((2+)\)-approximation with an amortized update time of \((k(n,))\) by Bateni et al. . The same paper also shows that any algorithm that provably satisfies a non-trivial approximation guarantee needs \((nk)\) queries to the distance function, i.e., the amortized update time is in \((k)\), making their algorithm close to tight. The fully dynamic \(k\)-center problem without outliers was also studied for metrics with bounded doubling dimension [14; 19; 12] and in Euclidean space .

### Preliminaries

In the preliminaries section, we provide an introduction to the \((k,z)\)-center problem and discuss the dynamic model used in our paper.

We study the \(k\)-center clustering problem with \(z\) outliers, formally defined as follows.

**Definition 1.2** (\((k,z)\)-center clustering).: Let \(P\) be a point set in a metric space \((M,d)\) and let \(k,z\) be two parameters. The goal is to compute a set \(C M\) of size at most \(k\), such that the maximum distance of all but at most \(z\) points to their nearest \(c C\) is minimized. That is, find \(C\) such that \(_{Z P,|Z| z}_{x P Z}_{c C}d(x,c)\), with \(|C| k\).

We define \(d_{}\) and \(d_{}\) as the minimum and maximum distances, respectively, between any two points ever inserted. The ratio \(=}}{d_{}}\) is referred to as the spread ratio, and is assumed to be bounded. Let \(r_{}\) be the optimal radius for the \((k,z)\)-center clustering problem of a point set \(P\). We also define the ball \(_{P}(c,r)=\{p P:d(c,p) r\}\) to be the set of points in \(P\) that are within distance \(r\) from \(c\). If \(P\) is clear from the context, we may drop \(P\) from the definition \(_{P}(c,r)\). For a non-negative integer \(m\), we denote \(\{1,,m\}\) by \([m]\).

In the version of the \((k,z)\)-center clustering problem that we consider, we allow \((1+)z\) outliers instead of strictly \(z\). Furthermore, we require centers of clusters to be in the metric space \((M,d)\), but they need not be currently present in \(P\). This is referred to as the non-discrete version of the problem. Lemma G.1 in the Appendix shows how our data structure can also support the discrete version of the problem and provides a \(6\)-approximation solution for it.

Fully dynamic model.We consider the \((k,z)\)-center clustering problem in the _fully dynamic model_ against an _adaptive adversary_. In this model, we start with an empty point set, \(P=\), and process a sequence of operations determined by the adversary. We assume the adversary does not know the random bits chosen by our algorithm; however, it can observe the algorithm's output and adapt its responses in real time (unlike an _oblivious adversary_, which fixes a sequence of operations in advance). Each operation can be either an insertion, where a point from the metric space \((M,d)\) is added to \(P\), or a deletion, where a point currently in \(P\) is removed. We assume only points currently in \(P\) may be deleted. Let \(P^{t}\) represent the point set \(P\) after \(t\) operations. In other words, \(P^{t}\) consists of all points in \((M,d)\) that have been inserted but not deleted after \(t\) operations.

## 2 Algorithm

To explain the main ideas of our algorithm, we start by describing an iterative offline algorithm, which gives a \((4+)\) approximation for the \((k,z)\)-center clustering problem.

Offline algorithm.We start with a point set \(P_{1}=P\), \(>0\) and parameters \(k,z,r\). Here, \(r\) is a fixed guess for the optimal radius. For each iteration \(i\), we sample a set of \(S_{i} P_{i}\) of \(|S_{i}|=^{-1}k^{2} k\) points with replacement, uniformly at random. Here, \( 6\) is a constant, where \(> 1\) are constants. We find a point \(c_{i} S\), such that \((c_{i},2r)\) covers at least \(\) points from \(P_{i}\), where \(\) is some threshold to be defined later. We then create a new cluster \(C_{i}=_{P_{i}}(c_{i},4r)\), let \(P_{i+1}=P_{i} C_{i}\), and continue to the next iteration. We stop once \(k\) clusters have been created or \(P_{i}=\). Let \(\) be a random variable representing the number of iterations we complete, where \(\) can be at most \(k\). After we have done \(\) iterations we have computed clusters \(C_{1},C_{2},...,C_{}\) with corresponding centers \(c_{1},c_{2},...,c_{}\). The points that are not covered by the union of these clusters will be the set of outliers that our algorithm reports. If the set of outliers is at most \((1+)z\) points, we can report a solution for the \((k,z)\)-center clustering problem. This offline algorithm will be used as a sub-routine for the fully dynamic algorithm, which will be explained in Section 2.4. The definition of \(\) and the pseudocode of the algorithm will also be given in that section.

Guesses for unknown \(r_{}\).Since the optimal \(r\) is usually not known, we can run the algorithm for all \(r=\{(1+)^{i}:d_{}(1+)^{i}(1+ ) d_{},i\}\). We then choose the smallest \(r\) such that all but at most \((1+)z\) points are covered. We will show that this gives a \((4+)\)-approximation.

Leveling.We can visualize the output of this algorithm as (at most) \(+1\) levels, where the first \( k\) levels each represent a cluster, and the last level represents the outliers (See Figure 1). More precisely, level \(i\) represents cluster \(C_{i}\) with center \(c_{i}\), which was created in the \(i^{th}\) iteration. Note that the construction of level \(i\) only considers the points in \(P_{i}\), which does not include points in the lower levels \(\{1,2,...,i-1\}\).

The main idea for the fully dynamic algorithm is to maintain each level \(i\) for \(i[+1]\) under an arbitrary number of insertions and deletions by updating the cluster \(C_{i}\) when necessary. In the rest of the paper, we use subscript \(i\) to refer to the level and superscript \(t\) to refer to the time. For example, \(P_{i}^{t}\) refers to the points present in level \(i\) at time \(t\), and \(r_{}^{t}\) refers to the optimal radius at time \(t\). If the level or time is clear from the context, these may be omitted. For instance, if we define a situation at time \(t^{}\), we will not repeat the superscript \(t^{}\) on every term.

### Data structure and invariants

We will construct a clustering \(C_{1},,C_{}\) of the current set of points \(P^{t}\) that we store in the data structure \(_{r}\) introduced by . If \(t\) is clear from the context, we will just write \(P\) for \(P^{t}\). As we do not know the current optimal radius, we will maintain one data structure \(_{r}\) for every choice of \(r=\{(1+)^{i}:d_{}(1+)^{i} d_{ },i\}\). Every data structure consists of up to \( k\) cluster-levels, with each level \(i\) containing a cluster \(C_{i}\). In level \(i\), we keep track of the set of the points \(P_{i}\) currently not covered, i. e., \(P_{i}=P_{j<i}C_{j}\). Level \(+1\) contains the outliers, i.e., \(_{r}=P_{i}C_{i}\). We denote \(n_{i}=|P_{i}|\). By \((p,r)\) we denote the ball with radius \(r\) centered at \(p\), and \(_{A}(p,r)=(p,r) A\) for \(A P\).

Let \( 1\). The data structure \(_{r}\) consists of the following components:

1. A list \(_{r}=\{c_{1},c_{2},...,c_{}\}\) of \( k\) centers.
2. A list \(_{r}=\{C_{1},C_{2},...,C_{}\}\) of clusters with \(C_{i}=_{P_{i}}(c_{i},4r)\).
3. A set \(_{r}=P_{i[]}C_{i}\) of outliers such that for all \(i[]\) and \(x_{r}\), \(d(x,c_{i})>4r\).

Let \( 1\) be a constant. We have the following invariants for \(_{r}\).

1. **Level invariant:** for all \(i\), \(P_{i+1}=P_{i} C_{i}\), with \(C_{i}=_{P_{i}}(c_{i},4r)\).
2. **Dense invariant:** for all \(c_{i}_{r},|_{P_{i}}(c_{i},2r)|z+1, -z}{k-i+1}-\).

Figure 1: The \(\) levels constructed by our offline algorithm.

### Insertion

When a new point \(p\) is inserted at time \(t\), Procedure 3 in the appendix is executed. As input, we have \(_{r}\) at time \(t\), \(p(M,d)\), \(>0\) and \(z,k\). First, we check whether \(p\) is inside one of the existing clusters, in which case we can add \(p\) to such a cluster. Otherwise, we add \(p\) to \(_{r}\). Next, we check if the dense invariant is maintained after the insertion of the new point. Generally, the dense invariant can be broken in two ways:

* If the insertion of a new point results in more than \((1+)z\) outliers, then Lemma 3.7 shows that the dense invariant is no longer valid at some level \(i\).
* If the addition of a point \(p\) to a cluster \(C_{i}\) causes the dense invariant to be violated at some lower level \(j<i\), that occurs because \(n_{j}^{t+1}=n_{j}^{t}+1\).

If the dense invariant is broken for some level \(i\), we recluster levels \(i,...,\) with \( k\) by invoking Procedure 5 as a sub-routine. Observe that if there are multiple levels \(i\) where the dense invariant is broken, we choose the lowest one.

### Deletion

For the deletion of a point \(p\) at time \(t\), Procedure 4 in the appendix is executed. The input is \(_{r}\) at time \(t\), \(p(M,d)\), \(>0\) and \(z,k\). First we check if \(p\) is an outlier, in which case we remove \(p\) from \(_{r}\). If \(p\) is either a center or a point in a cluster, we find cluster \(C_{i}\) which contains \(p\). If cluster \(C_{i}=(c_{i},4r)\) contains at least \((z+1,|-z}{k-i+1}-)\) points, we do not re-cluster and simply remove \(p\) from \(C_{i}\). Note that the underlying point set exists in the metric space \((M,d)\). If the center \(c_{i}\) of a cluster \(C_{i}\) is deleted, provided that the dense invariant remains valid, we can continue to utilize \(c_{i}\) as the center of cluster \(C_{i}\), given our knowledge that the point \(c_{i}\) is located within the metric space \((M,d)\). In Lemma G.1, we explain how we can still obtain a 6-approximation if centers have to come from the current point set. If \(|C_{i}|<(z+1,|-z}{k-i+1}-)\) after the deletion of \(p\), then it also follows that \(|(c_{i},2r) P_{i}|<(z+1,|-z}{k-i+1}-)\), i.e., the dense invariant is violated on this level. In this case, we want to redistribute the points in \(P_{i}\) such that the levels \(i,,\) fulfill the dense invariant. Deletion of a point in level \(i\) does not violate the invariants at levels \(1,,i-1\). We re-cluster the points in \((_{i j k}C_{j})_{r}\) using Procedure 5. We finish by updating \(_{r}\).

### Clustering sub-routine

The clustering sub-routine is the offline algorithm that was described at the beginning of Section 2. The pseudocode of this sub-routine is shown in Procedure 5. We use it to iteratively build the levels of data structure \(_{r}\). Two cases are distinguished based on whether \(z\) is small or large compared to the number of points in level \(i\). In line 4, \(\) is a constant with \( 6\), where \(>\), and \(\) is the constant used in the dense invariant. The threshold \(\) that was introduced in Section 2 is different depending on the size of \(z\) compared to \(n_{i}\). If \(z+1-z}{4(k-i+1)}\), then \(=-z}{2(k-i+1)}\) and if \(z+1>-z}{4(k-i+1)}\), then \(=-z}{k-i+1}-\). If we find multiple points \(p^{*}\) in line 6 or 12 we choose one arbitrarily.

### All aspects combined

The first step will be to initialize all \(_{r}\) such that \(_{r},_{r},_{r}=\). Then, the algorithm waits for an insertion or deletion operation. If a point is inserted, Procedure 3 is executed, and if a point is deleted, Procedure 4 is executed. The algorithm is ran simultaneously for all \(r=\{(1+)^{i}:d_{}(1+)^{i}(1+ ) d_{},i\}\).

In the next sections, it will be shown that a \((4+)\)-approximation is given by the clustering \(_{r}\) with \(r\) the smallest \(r\) for which \(|_{r}|(1+)z\).

## 3 Analysis

We begin our analysis of the dynamic algorithm by introducing the concept of a dense cluster and specifying the criteria for a valid solution.

**Definition 3.1** (Dense cluster).: A cluster \(_{P_{i}}(c_{i},4r)\) is dense with respect to point set \(P_{i}\) if it satisfies the dense invariant. That is, \(|_{P_{i}}(c_{i},2r)|(z+1,-z}{k-i+1}-)\).

Observe that for the dense invariant, we consider the ball \(_{P_{i}}(c_{i},2r)\) with a radius of \(2r\). However, a cluster (as seen in line 18 of Procedure5) corresponds to the points within the ball \(_{P_{i}}(c_{i},4r)\), which has a radius of \(4r\).

**Definition 3.2** (Valid solution).: A solution \(_{P_{i}}(c_{i},4r)_{P_{i}}(c_{},4r)\) with \( k\) for point set \(P_{1}\) is valid if it covers all but at most \((1+)z\) points from \(P_{1}\). We refer to each cluster \(_{P_{j}}(c_{j},4r)\) for \(j\{i,,\}\) of a valid solution as a valid cluster.

**Lemma 3.3** (Running time offline algorithm).: _The running time of the offline algorithm, shown in Procedure 5, is \((n^{-1}k^{3}(k))\), where \(n=|P|\)._

Proof.: In each iteration of the while-loop, we need to compute \(|(p,2r)|\) for all \(p S_{i}\). Since \(|S_{i}|=(^{-1}k^{2} k)\), this takes \((n^{-1}k^{2} k)\) time. There can be at most \(k\) iterations of the while-loop and hence the total running time is \((n^{-1}k^{3} k)\). 

### Maintaining invariants

In Supplementary Section B, we prove the following three lemmas. Lemmas 3.4 and 3.5 show that the dense and level invariants are maintained during the insertion or deletion of a point, respectively. Additionally, Lemma 3.6 shows that both invariants are maintained when invoking Procedure 5 as a subroutine upon the insertion or deletion of an arbitrary point with high probability if \(r r_{}\). The case where \(r<r_{}\) is considered in Lemmas E.1 and E.2.

**Lemma 3.4** (Procedure 3 maintains invariants).: _Assume that at time \(t\), we have point set \(P^{t}\), data structure \(_{r}=(_{r},_{r},_{r})\). We assume that the level and dense invariants hold at time \(t\) and \(r r_{}^{t+1}\). At the start of time \(t+1\), we insert point \(p\) using Procedure 3. After the insertion, the level and dense invariants still hold with probability 1 if Procedure 5 was not called and with the probability of at least \(1-}\), where \( 1\) if Procedure 5 was not called._

**Lemma 3.5** (Procedure 4 maintains invariants).: _Assume that at time \(t\), we have point set \(P^{t}\), instance \(_{r}=(_{r},_{r},_{r})\), parameters \(k,z\) and \(>0\). We assume that the level and dense invariants hold at time \(t\) and \(r r_{}^{t+1}\). At the start of time \(t+1\), we delete an arbitrary point \(p\) using Procedure 4. After the deletion, the level and dense invariants hold with probability 1 if Procedure 5 was not called, and with probability \(1-}\) with \( 1\) if Procedure 5 was called._

**Lemma 3.6** (Procedure 5 maintains invariants with high probability).: _Suppose the level and dense invariants hold for all levels \(j<i\) and we call Procedure 5 on \(P_{i}\) as the result of an insertion or deletion. Let \( k\) be a random variable representing the number of levels we have after completing Procedure 5. If \(r r_{}\), Procedure 5 maintains the level and dense invariants for all levels \(j\) with \(i j\) with probability at least \(1-}\), with \( 1\)._

### Approximation guarantee

Next, we prove that if the invariants hold, our data structure \(_{r}=(_{r},_{r},_{r})\) contains a valid solution to the \(k\)-center with \(z\) outliers problem. Furthermore, if \(r<(1+)r_{}\), \(_{r}\) provides a \((4+)\)-approximation. Without loss of generality, for simplicity we assume that \(_{r}\) contains \(k\) levels. The proof of Lemma 3.7 follows the structure of the proof given in . This proof uses the greediness of the algorithm to argue that the balls in the solution cover enough points to charge to. Here, we have to modify the proof to work with the dense invariant.

**Lemma 3.7** (Approximation guarantee).: _Let \(P\) be the current point set. Assume that the level invariant and the dense invariant hold for all levels \(i k\). Then we have the following guarantees:_

1. _Valid solution: If_ \(r_{} r\)_, then_ \((c_{1},4r)(c_{k},4r)\) _covers all but at most_ \((1+)z\) _outliers in_ \(P\)_._
2. _Approximate solution: If_ \(r_{} r<(1+)r_{}\)_, then_ \((c_{1},4r)(c_{k},4r)\) _gives a_ \((4+)\)_-approximation for the_ \(k\)_-center with_ \(z\) _outliers problem on_ \(P\)Proof.: Recall that the level invariant states that for all \(i\), we have \(P_{i+1}=P_{i}(c_{i},4r)\) and the dense invariant states that for all \(i\), we have that \(|(c_{i},2r) P_{i}|(z+1,-z}{k-i+1}-)\).

Assume that in the optimal solution, we have balls \(O_{1},,O_{k}\) with radius \( r_{}\). The union of these balls covers all but \(z\) points in \(P\). In order to prove Part 1, we aim to charge all but at most \(z\) points of the optimal solution to points in our solution \((c_{1},4r)(c_{k},4r)\). We prove this by induction, and Part 2 will then follow easily. In order to construct the charging argument, we need to argue that there are enough points in our solution to charge the points in the optimal balls. To this end, we construct modified optimal balls \(O^{}_{1},,O^{}_{k}\), where \(O^{}_{i} O_{i}\) for every \(i k\).

For the base case, we have \(O^{}_{1}=O_{1},,O^{}_{k}=O_{k}\). We will show that we can order the modified balls in such a way, that at the end of time step \(i\), all but at most \( i\) points from the first \(i\) modified balls are charged to distinct points in \((c_{1},4r)(c_{i},4r)\). This will allow us to prove that our solution covers at least as many points as the optimal solution.

Assume that all but at most \((i-1)\) points in the first \(i-1\) modified balls \(O^{}_{1} O^{}_{2} O^{}_{i-1}\) have been charged to distinct points in \((c_{1},4r)(c_{i-1},4r)\) and consider iteration \(i\). We distinguish two cases, namely if \((c_{1},2r)(c_{i},2r)\) intersects one of the remaining modified balls, or if it does not. The charging argument for each case proceeds as follows:

**Case 1.**  Case 1 is when \((c_{1},2r)(c_{i},2r)\) intersects a remaining modified ball, call this ball \(O^{}_{i}\). Note that \(O^{}_{i}\) will be covered entirely by \((c_{1},4r)(c_{i},4r)\), since \(r r_{}\). Hence, we charge the points of \(O^{}_{i}\) to themselves and mark these points as covered. (See case 1 in Figure 2.) We call this charging _rule_\(I\). Since the modified balls are disjoint 1, any point can be charged only once (to itself) by this rule. Next, we update \(O^{}_{1},O^{}_{2},,O^{}_{k}\).

We maintain two sets of credit points that we save and may use for future charging purposes. First, the set \(^{c}_{i}=(c_{i},4r)(O^{}_{1}  O^{}_{k})\) which is the set of points covered by \((c_{i},4r)\) that are not covered by \(O^{}_{1} O^{}_{2} O^{}_{k}\). We let \(z^{c}_{i}=|^{c}_{i}|\) be the number of such points. In Figure 2, points \(p,q\) are such points. Observe that no point is charged to points in \(Z^{c}_{i}\), allowing us to use them later. We refer to these points as _credit points_.

There may also be previous modified balls \(O^{}_{j}\), with \(j<i\) that were considered in case 2 and are still present in \(P_{i}\). More specifically, let \(Z^{d}_{i}\) be the set of (there may exist) points in \(O^{}_{j}(c_{j},4r)\) that have been charged to distinct points in \((c_{j},4r)\) (See the discussion of case 2, below). For example, points \(a,b,c\) in case 2 of Figure 2. Let \(z^{d}_{i}=|Z^{d}_{i}|\) be the number of such points. Since no points

Figure 2: Visualization of _charging rules_ I and II. For case 1, we see that \((c_{i},2r)\) and \(O_{i}\) intersect. As a result, \((c_{i},4r)\) covers all points in \(O_{i}\). Points \(p,q\) are in set \(Z^{c}_{i}\). For case 2, the balls \((c_{1},2r)\) and \((c_{2},2r)\) do not intersect the optimal cluster \(O^{}_{i}\). The crossed points in \(O^{}_{2}\) are charged to black squared in \((c_{2},2r)\). The circle points in \(O^{}_{2}\) are not charged to any point. Points \(a,b,c\) are in \(Z^{d}_{i}\).

are charged to points in \(Z^{d}_{i}\), we save them as _credit points_ for future charging purposes. We now update \(O^{}_{1},O^{}_{2},,O^{}_{k}\) as follows: \(O^{}_{1},,O^{}_{i}\) stay the same and we define \(z^{c}_{i}+z^{d}_{i}\) artificial outliers in \((O^{}_{i+1} O^{}_{k}) P_{i+1}\).

Case 2.Case 2 occurs if \((c_{1},2r)(c_{i},2r)\) does not intersect any of the remaining modified balls. See Figure 2. Let \(O^{}_{i}\) be one of the remaining modified balls covering \(-z}{k-i+1}\) points. Note that for finding \(O^{}_{i}\) we do not count points that have been defined as artificial outliers, since these artificial outliers are already covered by balls of radius \(4r\) in previous levels and we do not cover them by the remaining modified balls. We prove in Lemma 3.8 that such a ball \(O^{}_{i}\) exists.

Using the assumption of this lemma that the dense invariant holds, we show in Lemma 3.9 that \(|(c_{i},2r)|-z}{k-i+1}-\). In this way, we find an upper bound for the number of points in \(O^{}_{i}\) and a lower bound for the number of points in \((c_{i},2r)\). Now, we charge all but at most \(\) points of \(O^{}_{i}\) to the points of \((c_{i},2r)\) and mark these points as charged. We call this charging _rule II_. Note the difference between charging argument in Case 1 and 2. In Case 1, we charge the points in \(O^{}_{i}\) to themselves, but in Case 2, we charge them to points covered by \((c_{i},2r)\). Recall that \((c_{i},2r)\) and \(O^{}_{i}\) are disjoint.

Observe that points in balls \(O^{}_{i+1},,O^{}_{k}\) will not be charged to points in \((c_{i},2r)\). Indeed, points in balls \(O^{}_{i+1},,O^{}_{k}\) are charged to the balls that our algorithms find either using rule I or rule II. However, this will not happen based on rule I since \((c_{i},2r)\) is disjoint from all balls \(O^{}_{i+1},,O^{}_{k}\). Moreover, rule II cannot also be applied since among balls \(O^{}_{i},O^{}_{i+1},,O^{}_{k}\), we already charged the points in \(O^{}_{i}\) to \((c_{i},2r)\) and points in \(O^{}_{i+1},,O^{}_{k}\) will be charged to points in balls \((c_{i+1},2r),,(c_{},2r)\).

Next, we consider how we update \(O^{}_{1},O^{}_{2},,O^{}_{k}\). Similar to case 1, we define the size of two sets of credits points. First, the variable \(z^{c}_{i}=|(c_{i},4r)(O^{}_{1} O^{ }_{k})|\) that corresponds to the number of points covered by \((c_{i},4r)\) that are not covered by \(O^{}_{1} O^{}_{2} O^{}_{k}\). There are \(z^{c}_{i}-|O^{}_{i}|\) points from \((c_{i},4r)\) that are free (i.e., no point is charged to) and can still be charged. We consider these points as credits that we save and may use for future charging purposes.

There may also be previous modified balls \(O^{}_{j}\), with \(j<i\) that were considered in case 2 and are still present in \(P_{i}\). More specifically, let \(Z^{d}_{i}\) be the set of (there may exist) points in \(O^{}_{j}(c_{j},4r)\) that have been charged to distinct points in \((c_{j},4r)\). For example, points \(a,b,c\) in case 2 of Figure 2. Let \(z^{d}_{i}=|Z^{d}_{i}|\) be the number of such points. Since no points are charged to points in \(Z^{d}_{i}\), we save them as _credit points_ for future charging purposes. We now update \(O^{}_{1},O^{}_{2},,O^{}_{k}\) as follows: \(O^{}_{1},O^{}_{2},,O^{}_{i}\) stays the same and we define \((z^{c}_{i}-|O^{}_{i}|)+z^{d}_{i}\) artificial outliers in \((O^{}_{i+1} O^{}_{k}) P_{i+1}\).

Now, for both cases, we apply charging rule I to any points in the remaining modified balls \(O^{}_{i+1} O^{}_{k}\) that are inside \((c_{i},4r)\). These points are then marked as covered. See Figure 3 for an example. Note that these points will not be in \(P_{i+1}=P_{i}(c_{i},4r)\). We assumed that \(O_{1} O_{2} O_{k}\) covers all but at most \(z\) points. After the charging has taken place and the modified clustering \(O^{}_{1} O^{}_{2} O^{}_{k}\) has been constructed, we have that \(|O^{}_{1} O^{}_{2} O^{}_{k}|=n-z-_{j 1}(z^{c}_{j}+z^{d}_{j})-_{j2}(z^{c}_{j}-|O^{}_{j}|+z^{d}_{j})\).

Note that \(|O^{}_{j}|\) refers to the number of points covered by the modified ball \(O^{}_{j}\) at the time of iteration \(j\). Then, by the way we have charged \(O^{}_{1} O^{}_{k}\) to our solution \((c_{1},4r)(c_{k},4r)\), we obtain \(|(c_{1},4r)(c_{k},4r)||O^{}_{1}  O^{}_{2} O^{}_{k}|-+ _{j1}(z^{c}_{j}+z^{d}_{j})\)

\(+_{j2}(z^{c}_{j}-|O^{}_{j}|+z^{d}_{j})=n-(1+)z\). This concludes the proof of Part 2 of the lemma. It follows easily that when \(r_{} r<(1+)r_{}\), Part 2 also holds, which completes the proof of this lemma. 

**Lemma 3.8**.: _Let \(i\) be an iteration of the charging argument above such that we are in case 2. This means that \((c_{1},2r)(c_{i},2r)\) does not intersect any of the remaining modified balls. Then, there must be a remaining modified ball covering \(-z}{k-i+1}\) points._

For the proof of Lemma 3.8, see Supplementary Section C.

**Lemma 3.9** (Coverage of \(B(c_{i},2r)\)).: _When we are in case 2 of the charging argument for some iteration \(i\), we must have that \(|(c_{i},2r)|-z}{k-i+1}-\)._

For the proof of Lemma 3.9, see Supplementary Section C.

### Duration of dense clusters

After running Procedure 5 as a subroutine, we know that each cluster \(C_{i}\) covers at least \(\) points. More specifically, for each level \(i\), if \(z+1-z}{4(k-i+1)}\), then \(|(c_{i},2r)|-z}{2(k-i+1)}\) and if \(z+1>-z}{4(k-i+1)}\), then \(|(c_{i},2r)|-z}{k-i+1}-\). The following two lemmas show that when \(C_{i}\) satisfies one of these two constraints, it will be dense for a significant number of arbitrary operations. This will lead to the amortized update time being independent of \(n\). The remaining lemmas are split into two cases, corresponding to the two cases in Procedure 5.

**Lemma 3.10** (Duration of dense cluster for \(z\) is small).: _Assume that we are currently at time \(t\). Let us consider a level \(i\) in which \(z+1^{t}-z}{4(k-i+1)}\). Let \(p=arg\,max_{p^{} S_{i}}|_{P_{i}}(p^{},2r)|\), and \(_{}=_{P_{i}}(p,2r)\). Assume that \(^{t}-z}{2(k-i+1)}|_{}|\). Then, we can add \(_{P_{i}}(p,4r)\) as a cluster in our solution, and this cluster will be dense until time \(t^{}=t+t^{*}\), with \(t^{*}^{t}-z}{4(k-i+1)}\)._

For the proof of Lemma 3.10, see Supplementary Section D.

**Lemma 3.11** (Duration of dense cluster for \(z\) is large).: _Assume that we are currently at time \(t\). Let \(z+1>^{t}-z}{4(k-i+1)}\) for some level \(i\). Let \(p=arg\,max_{p^{} S_{i}}|_{P_{i}}(p^{},2r)|\), and \(_{}=_{P_{i}}(p,2r)\). Assume that \(^{t}-z}{k-i+1}-|_{}|\). Then, we can add \(_{P_{i}}(p,4r)\) as a cluster in our solution, and this cluster will be dense until time \(t^{}=t+t^{*}\), with \(t^{*}=()\)._

For the proof of Lemma 3.11, see Supplementary Section D.

### Small radius guesses

It has not yet been considered what will happen if Procedure 5 fails on some level \(i\). That is, when there exists no \(p S_{i}\) such that \(_{P_{i}}(p,2r)\) covers sufficiently many points. Lemmas E.1 and E.2 show that if Procedure 5 fails at some level, then with high probability the guess of the optimal radius for that specific instance is too small (\(r<r_{}\)). In this case, we postpone the construction of that level to a time \(t+t^{*}\), referred to as \(t^{}_{r}\) in the algorithm.

### Computing update time

Finally, we prove the update time of our algorithm. Lemma 3.4, Lemma 3.5 and Lemma 3.6 prove that the algorithms maintain the invariants with high probability. Together with Lemma 3.7 and Lemma F.1 this implies Theorem 1.1.

### Robustness to adversarial inputs

Our dynamic algorithm is robust against an adaptive adversary. Specifically, we do not assume that the adversary has predetermined the sequence of updates in advance, as with an oblivious adversary. Instead, the adversary can query the insertion and deletion updates in an online manner, with knowledge of our solution.

We only use randomization to generate the sample set \(S_{i}\) in Procedure 5, which is then used to construct the levels. This randomness affects only the probability of failure in this procedure. After the insertion or deletion updates, we do not need to reconstruct a level until it no longer satisfies the invariants. The guarantees we provide for how long a level can remain valid are all in the worst case; therefore, they hold even when an adaptive adversary chooses the insertion and deletion operations online.