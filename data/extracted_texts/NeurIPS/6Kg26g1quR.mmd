# RODICE: Offline Return on Investment Maximization for Efficient Decision Making

Woosung Kim\({}^{1*}\)  Hayeong Lee\({}^{1*}\)  Jongmin Lee\({}^{2}\)  Byung-Jun Lee\({}^{1,3}\)

\({}^{1}\)Korea University \({}^{2}\) UC Berkeley \({}^{3}\)Gauss Labs Inc.

{wsk208,hayeong_lee,byungjunlee}@korea.ac.kr

jongmin.lee@berkeley.edu

\({}^{*}\)Equal contribution \({}^{}\)Corresponding authors

###### Abstract

In this paper, we propose a novel policy optimization framework that maximizes Return on Investment (ROI) of a policy using a fixed dataset within a Markov Decision Process (MDP) equipped with a cost function. ROI, defined as the ratio between the return and the accumulated cost of a policy, serves as a measure of the efficiency of the policy. Despite the importance of maximizing ROI in various applications, it remains a challenging problem due to its nature as a ratio of two long-term values: return and accumulated cost. To address this, we formulate the ROI maximizing reinforcement learning problem as linear fractional programming. We then incorporate the stationary distribution correction (DICE) framework to develop a practical offline ROI maximization algorithm. Our proposed algorithm, RODICE, yields an efficient policy that offers a superior trade-off between return and accumulated cost compared to policies trained using existing frameworks.

## 1 Introduction

In economics, Return on Investment (ROI) is a financial metric used to evaluate the profitability of an investment relative to its cost. The concept of ROI originates from the work of  and is widely regarded as a valuable metric by the majority of marketing managers . ROI is calculated by dividing the profit generated from the investment by the cost of the investment, and it is a key indicator for evaluating the efficiency and effectiveness of various economic decisions and strategies . Maximizing ROI is crucial as a decision with a high ROI yields a higher return at a relatively lower cost.

While ROI is a compelling metric for optimization, ROI maximization for decision making has not been actively explored within the field of machine learning. Previous studies on ROI maximization  have focused on multi-armed bandit scenarios, where reward and cost are immediately accessible following a decision. When it comes to sequential decision-making setups like the Markov Decision Process (MDP) that additionally considers a cost function, optimizing a sequence of decisions or a policy to maximize the ratio between return and accumulated cost becomes far more challenging and remains mostly unexplored. This optimization contains two major difficulties: the fractional relationship between the return and accumulated cost of a policy, and the fact that both are long-term quantities collected after a sequence of decisions or actions of the agent. The issues make ROI maximization via existing policy optimization techniques not straightforward, as it includes computing the gradient of the ratio between the two expected values.

For effective optimization of ROI, we refer to the dual formulation of V-LP , which represents policy optimization in reinforcement learning (RL) as linear programming in terms of the stationary distribution. The stationary distribution of a policy is the discounted sum of probabilities of thestate-action pairs visited by the agent following the policy. Leveraging the property of stationary distribution, which allows for easy estimation of both return and accumulated cost, we derive ROI-LP, linear programming problem for ROI maximization. In this paper, we focus specifically on the offline setting and aim to derive an offline ROI maximization algorithm. The offline setting, which involves optimizing a policy using a fixed dataset of pre-collected experiences without further environmental interactions (e.g., offline RL), has been actively studied recently due to its compelling use cases [2; 7; 9; 10; 11; 20]. Building on the derived ROI-LP, we incorporate a convex regularization to address the distribution shift inherent in offline learning. This leads to our proposed algorithm, ROIDICE, which achieves offline ROI maximization via stationary distribution correction estimation.

Through a series of diverse experiments, we compare ROIDICE to algorithms from other offline policy optimization frameworks, including offline RL and offline constrained RL. We demonstrate that maximizing the ROI of a policy leads to a different, more efficient behavior compared to frameworks that focus solely on maximizing policy return. In summary, our contributions are threefold:

* Formulation of ROI maximization framework in an MDP with a cost function.
* Derivation of ROIDICE, an offline ROI maximization algorithm that optimizes the ROI of a policy using a fixed dataset of pre-collected experiences.
* Comparison with existing offline policy optimization frameworks, showing that ROIDICE optimizes ROI and improves policy efficiency across various domains.

## 2 Backgrounds

Markov Decision Process with a cost functionWe consider an infinite-horizon discounted Markov Decision Process (MDP) with a cost function, represented as a tuple \(= S,A,T,r,p_{0},\). Here, \(S\) is the set of states, \(A\) is the set of actions, \(T(s^{}|s,a)\) denotes the transition probability from state-action pair \((s,a)\) to the next state \(s^{}\), and \(r(s,a)\) is the reward function for state-action pair \((s,a)\). \(p_{0}(s)\) is the initial state distribution, and \(\) is the discount factor. Additionally, we introduce a cost function \(c(s,a)>0\) for all state-action pairs \((s,a)\).

An agent begins at an initial state following \(p_{0}(s)\) and repeats the process of collecting reward \(r(s,a)\) and incurring cost \(c(s,a)\) by taking action \(a\) in state \(s\) and transitioning to the next state \(s^{}\). The policy \((a|s)\) represents the distribution of actions to be taken by the agent in state \(s\). The performance of the policy \(\) can be evaluated with the expected return \(R_{}=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})]\) and the expected accumulated cost \(C_{}=_{}[_{t=0}^{}^{t}c(s_{t},a_{t})]\), which are the expected cumulative sums of discounted rewards and costs over rollouts of the policy \(\).

The main goal of reinforcement learning (RL) is to optimize policy \(\) to maximize expected return \(R_{}\) collected by the agent. Constrained RL optimizes the policy to maximize its expected return \(R_{}\) while constraining its expected accumulated cost \(C_{}\) to be lower than a given threshold \(C_{}\).

Stationary Distribution and Linear Programming (LP) formulation of RLThe stationary distribution of policy \(d_{}\) represents the discounted sum of probabilities of the state-action pair \((s,a)\) visited by an agent following policy \(\), i.e., \(d_{}(s,a):=(1-)_{t=0}^{}^{t}(s_{t}=s,a_{t}=a)\). Using the stationary distribution, \(R_{}\) and \(C_{}\) can be expressed as linear combinations of the stationary distribution and the reward and the cost respectively: \(R_{}=_{s,a}d_{}(s,a)r(s,a)\) and \(C_{}=_{s,a}d_{}(s,a)c(s,a)\). This characteristic allows the RL problem to be formulated as a linear programming (LP), known as the dual of V-LP in .

\[_{d 0} _{s,a}d(s,a)r(s,a)\] (1) s.t. \[(_{*}d)(s)=(1-)p_{0}(s)+(_{*}d )(s)\; s\] (2)

where \((_{*}d)(s)=_{a}d(s,a)\) and \((_{*}d)(s)=_{,}T(s|,)d(, {a})\).

Constraint (2), known as Bellman flow constraint, is a requirement that any stationary distribution must satisfy. The condition can be interpreted as ensuring that the probability of state-action pairs leaving state \(s\) matches the total probability of state-action pairs entering \(s\), plus the probability of state \(s\) initiating a trajectory. Once the optimal stationary distribution \(d^{*}\) is obtained, optimal policy \(^{*}\) can be extracted from it by \(^{*}(a|s)=d^{*}(s,a)/_{a^{}}d^{*}(s,a^{})\; s,a\).

Offline RL and DICE-RL FrameworkIn this paper, we assume an offline policy optimization setting where interaction between the agent and environment is not allowed. Instead, the agent is provided with a fixed dataset of experiences \(D=\{(s_{i},a_{i},r_{i},c_{i},s^{}_{i})\}_{i=1}^{N}\), consisting of \(N\) transition samples, to optimize its policy. A unique challenge in offline RL is balancing between policy optimization and distribution shift, as it is not possible to gather samples from the optimized policy.

In , regularized policy optimization is proposed to penalize return maximization (1) with \(f\)-divergence between stationary distributions of the trained policy \(d\) and the behavior policy \(d_{D}\), defined as \(D_{f}(d||d_{D}):=_{s,a}d_{D}(s,a)f(d(s,a)/d_{D}(s,a))\). The convexity of \(f\) leads to a convex optimization problem that balances return maximization and distribution shift:

\[_{d 0} _{s,a}d(s,a)r(s,a)-_{s,a}d_{D}(s,a)f((s,a)})\] s.t. \[(_{*}d)(s)=(1-)p_{0}(s)+(_{*}d )(s)\; s\]

where \(\) is a hyper-parameter that controls the trade-off. In the constrained setting studied in , a cost constraint \(_{s,a}d(s,a)c(s,a) C_{}\) is added to the problem.

Previous studies have leveraged the characteristics of the above convex optimization problem to develop practical offline RL algorithms. They follow a similar approach by applying a Lagrangian multiplier \((s)\) to the Bellman flow constraint (2), reformulating the Lagrangian dual of the problem into the main loss function of the algorithms. We refer to this family of algorithms as the DICE-RL framework. Detailed explanations of the loss functions and the full derivations of related DICE-RL algorithms (OptiDICE  and COptiDICE ) are provided in Appendix A.

Linear-fractional ProgrammingLinear-fractional programming is defined as maximizing a ratio of two affine functions over a polyhedron. Its standard form is given by:

\[_{}\;f():=^{T}+}{ ^{T}+}\; ,=\] (3)

The domain of \(f\) is restricted to ensure the denominator remains positive, i.e., \(\{|^{T}+>0\}\), as a zero denominator would result in an infeasible solution.

Assuming the region satisfying the constraints is non-empty and bounded, any linear-fractional programming can be transformed into equivalent linear programming using the Charnes-Cooper transformation . The main idea of the transformation is to replace variable \(\) with two new variables \(t 0\) and \(\), such that \(t(^{T}+)=1\) and \(=t\). This change of variables eliminates the denominator of \(f()\), resulting in the following linear programming problem:

\[_{,t 0}\;^{T}+ t\;  t,=t,^{T}+ t=1.\]

## 3 ROI Maximization in Linear Programming form (ROI-LP)

In this section, we start with a return on investment (ROI) maximization problem by substituting the objective of the dual of V-LP from return \(R_{}\) to ROI. This results in a linear-fractional programming problem, as the objective is the ratio of two linear functions representing return and accumulated cost. We then apply the Charnes-Cooper transformation to derive an equivalent linear programming (LP), ROI-LP.

ROI Maximization ProblemWe begin by defining the ROI of policy \(\) in a Markov Decision Process (MDP). The ROI of policy \(\) is the ratio between the return \(R_{}\) and its accumulated cost \(R_{}\), which can be expressed in terms of its stationary distribution \(d_{}\): \(()=}{C_{}}=d_{}(s,a)r(s,a)}{ _{s,a}d_{}(s,a)c(s,a)}\). Substituting the objective of the dual of V-LP (1) with ROI, we get our ROI maximization problem, which adheres to the standard form of linear-fractional programming in (3).

\[_{d 0} d(s,a)r(s,a)}{_{s,a}d(s,a)c(s,a)}\;(_{*}d)(s)=(1-)p_{0}(s)+(_{*}d)(s)\;  s.\] (4)Roi-LpWe apply the Charnes-Cooper transformation to reformulate the ROI maximization problem (4) into an equivalent linear programming problem, ROI-LP. This involves introducing a non-negative variable \(t 0\) and replacing \(d(s,a)\) with \(d^{}(s,a)\) and \(t\), which are defined as:

\[d^{}(s,a)=d(s,a)c(s,a)}d(s,a), t= d(s,a)c(s,a)}\] (5)

and thus \(d(s,a)=d^{}(s,a)/t\). Substituting \(d\), ROI-LP, the linear programming formulations for maximizing ROI, is given by:

\[_{d^{} 0,t 0} _{s,a}d^{}(s,a)r(s,a)\] s.t. \[(_{s}d^{})(s)=t(1-)p_{0}(s)+( _{s}d^{})(s)\; s\] \[_{s,a}d^{}(s,a)c(s,a)=1\] (6)

where the equality constraint (6) is newly introduced to ensure that (5) is satisfied. Once the optimization on ROI-LP is complete, optimal stationary distribution \(d^{*}(s,a)\) is obtained by dividing \(d^{*}(s,a)\) with \(t^{*}\).

_Remark_.: When \(c(s,a)\) is a fixed constant \(C\) for all states and actions, the optimal solution of ROI-LP becomes equivalent to that of the dual of V-LP. In this case, the optimal \(t\) becomes \(\) as per (5), since any stationary distribution satisfies \(_{s,a}d(s,a)=1\). Consequently, the optimal \(d^{*}\) becomes \(d^{*}\) from the dual of V-LP scaled by \(t\). This special case illustrates that ROI maximization simplifies return maximization when the accumulated cost is constant, regardless of the policy.

## 4 Offline ROI maximization

In this section, we derive an offline ROI maximization algorithm that extends ROI-LP to also include the trade-off for distribution shifts. We formulate a convex optimization problem for offline ROI maximization and use it to derive ROIIDICE, the first offline algorithm to optimize policy efficiency in terms of ROI given a fixed dataset.

### Regularized ROI Maximization Framework

To formulate an offline ROI maximization problem, we add a convex regularization to ROI-LP representing a distribution shift. The approach is similar to the DICE-RL framework , which incorporates an \(f\)-divergence between the stationary distribution of trained policy \(d\) and behavior policy \(d_{D}\) into the dual of V-LP. Our regularized ROI maximization framework is given by:

\[_{d^{} 0,t 0} _{s,a}d^{}(s,a)r(s,a)-_{s,a}d_{D}(s,a)f( (s,a)}{d_{D}(s,a)},t)\] s.t. \[(_{s}d^{})(s)=t(1-)p_{0}(s)+( _{s}d^{})(s)\; s\] (7) \[_{s,a}d^{}(s,a)c(s,a)=1\] (8)

where the design of \(f(x,t)\) will be provided later in this subsection. We denote \(D_{f,t}(d^{},d_{D})=_{s,a}d_{D}(s,a)f((s,a)}{d_{D}( s,a)},t)\).

The primary difference between our proposed problem and DICE-RL framework lies in the convex regularization. The \(f\)-divergence \(D_{f}(d||d_{D})\) from DICE-RL framework is not directly applicable to \(d^{}(s,a)\), as \(d^{}(s,a)\) is not a valid probability distribution; it is derived by scaling the stationary distribution \(d(s,a)\) by \(t\). However, naively incorporating \(D_{f}(}{t}||d_{D})\) results in the loss of the convexity, as \(f()\) of \(f\)-divergence is generally not convex with respect to \(t\). To this end, we design a new convex regularization that measures the amount of distribution shift while not breaking the convexity of the problem. We begin by outlining three conditions that the regularization should satisfy.

1. \(f(x,t)\) should be convex in \(x\) given \(t>0\) and convex in \(t\) given \(x>0\).

2. \(f(x,t)\) should be zero when \(x=t\).
3. \(f(x,t)\) should be well-defined on \(x>0\) and \(t>0\).

The first condition is that the problem must maintain convexity with respect to both \(x\) and \(t\). The second condition is that it must effectively regularize the distribution shift. Specifically, \(D_{f,t}(d^{},d_{D})\) should be minimized to zero when the stationary distribution \(d(s,a)=d^{}(s,a)/t\) matches \(d_{D}(s,a)\). The last condition is to avoid infeasible optimization scenarios, e.g., negative values inside a \(\) function. We introduce two convex functions as examples.

\[f_{1}(x,t)=(x-t)^{2}, f_{2}(x,t)=(x-t )^{2}&x t\\ ()-+1&0<x<t\]

From \(f(x)=(x-1)^{2}\) of \(^{2}\)-divergence, we can obtain the first function \(f_{1}(x,t)\) by shifting it by \(t-1\). We introduce a softened variant of \(f_{1}\), denoted as \(f_{2}\), where a scaled version of the function \(f(x)=x x-x+1\) (arising from the KL divergence) is seamlessly combined with the left segment of \(f_{1}\). Although \(f_{2}(x,t)\) is not entirely convex with respect to \(t\), it serves as a valid regularization term, as \(f_{2}(x,t)=0\) holds exclusively when \(x=t\).

_Remark_.: A unique characteristic of the proposed regularization \(D_{f,t}(d^{},d_{D})\) is that its magnitude is affected not only by the degree of distribution shift but also by the trained policy's accumulated cost. We demonstrate this by comparing the magnitude of \(D_{f,t}(d^{},d_{D})\) and \(f\)-divergence \(D_{f}(}{t}\|d_{D})\) that corresponds to typical regularizer of DICE-RL framework. In the case of \(^{2}\)-divergence, \(D_{f,t}(d^{},d_{D})=t^{2}D_{f}(}{t}\|d_{D})\) as \(D_{f}(}{t}\|d_{D})=_{s,a}d_{D}(s,a)}( {d^{}(s,a)}{d_{D}(s,a)}-t)^{2}\). Since \(t\) represents the inverse of the expected accumulated cost (5), the strength of the regularization with \(D_{f,t}(d^{},d_{D})\) becomes stronger than the actual \(f\)-divergence when the expected accumulated cost of the trained policy is less than 1.

### Roidice

We are now prepared to derive ROIDICE, an offline algorithm for ROI maximization based on the Regularized ROI maximization framework. To enforce the constraints (7) and (8), we introduce Lagrangian multiplier \(^{|S|}\) and \(\):

\[_{w 0,t 0}_{,}\ L(,,w,t):=_{s,a}w(s,a)d_{D} (s,a)r(s,a)-_{s,a}d_{D}(s,a)f(w(s,a),t)\] (9)

\[+_{s}(s)(t(1-)p_{0}(s)+(_{s}d^{})(s) -(_{s}d^{})(s))+(1-_{s,a}w(s,a)d_{D}(s,a)c( s,a)),\]

where \(w(s,a)=d^{}(s,a)/d_{D}(s,a)\). We follow a similar derivation process as outlined by  and obtain a closed-form solution of \(w^{*}_{,,t}(s,a)\):

\[w^{*}_{,,t}(s,a)=(0,(f^{})^{-1}((s,a) - c(s,a)}{},t)) s,a.\] (10)

where \(e_{}(s,a)=r(s,a)+_{s^{}}T(s^{}|s,a)(s^{})- (s)\) and \((f^{})^{-1}(y,t)\) is the inverse function of \(f(x,t)\) with respect to \(x\). Loss functions of \(,\) and \(t\) are obtained by substituting \(w^{*}_{,,t}(s,a)\) into \(L(,,w,t)\):

\[_{}_{s p_{0}}[t(1-)(s)]+ _{(s,a) d_{D}}[w^{*}_{,,t}(s,a)(e_{}(s,a)- c( s,a))- f(w^{*}_{,,t}(s,a),t)],\] \[_{}_{(s,a) d_{D}}[w^{*}_{,,t}(s,a )(e_{}(s,a)- c(s,a))- f(w^{*}_{,,t}(s,a),t)]+,\] \[_{t 0}_{s p_{0}}[t(1-)(s) ]-_{(s,a) d_{D}}[ f(w^{*}_{,,t}(s,a),t) ].\]

After the minimization on \(,\) and \(t\) is complete, we apply policy extraction techniques from  to obtain the policy that generates optimal stationary distribution \(d^{*}(s,a)=w^{*}_{,,t}(s,a)d_{D}(s,a)\). In this paper, we adopt weighted behavior cloning method, whose loss function is given as,

\[_{}_{(s,a) d_{D}}[w^{*}_{,,t}(s,a )_{}(a|s)]\] (11)

The more detailed derivation and algorithm of ROIDICE can be found in Appendix B.

## 5 Experiments

In this section, we demonstrate that the offline policy obtained from ROIDICE achieves a superior trade-off between return and accumulated cost compared to other RL approaches such as offline RL and constrained offline RL, in both finite and continuous domains.

### Random Finite MDP Experiments

We test ROIDICE in a Random MDP setting similar to . A tabular MDP with \(|S|=50\) and \(|A|=4\) is randomly generated and a behavior policy that achieves 90\(\%\) of the optimal return is used to gather fixed datasets with \(\{10,20,50,100,1000,2000\}\) trajectories. We estimate the empirical transition probability \((s^{}|s,a)\) with the given samples and use it to run a tabular version of ROIDICE. Details of the experiments are provided in the Appendix C. We compare ROIDICE with DICE-RL on other offline policy optimization schemes, OptiDICE  for offline RL and COptiDICE  for offline constrained RL.

vs. Offline RLIn Figure 0(a), we compare the performance of ROIDICE with OptiDICE. While OptiDICE achieves the highest return, it incurs a higher accumulated cost than ROIDICE. This is expected, as RL does not account for the accumulated cost it incurs. In contrast, ROIDICE optimizes the policy's ROI, resulting in a highly efficient policy that delivers a substantial return at a relatively lower accumulated cost.

vs. Offline Constrained RLIn Figure 0(b), we compare the performance of ROIDICE with COptiDICE. Offline constrained RL aims to maximize return while keeping the accumulated cost below a specified threshold \(C_{} C_{}\). As demonstrated in Appendix A, COptiDICE optimizes its policy using a penalized reward function \(r(s,a)-_{c}c(s,a)\), where \(_{c}\) is the Lagrange multiplier for the cost constraint. To ensure satisfaction of the cost constraint, \(_{c}\) is updated to increase when the constraint is violated. To set the cost thresholds for our experiment, we obtain the accumulated cost of the ROIDICE policy on the Maximum Likelihood Estimate (MLE) MDP and multiply it with \([0.7,0.9,1.0,1.1,1.3]\). The left side of the figure shows the true ROI performance of the agents, while the right side shows the ROI performance on the MLE MDP, representing the performance agents expect. The cost thresholds from the MLE MDP are based on the offline assumption which excludes the interaction with the true MDP.

With a sufficient number of trajectories in the offline dataset, COptiDICE with a cost multiplier of 1.0, which corresponds to the same cost budget as the optimized ROIDICE policy, achieves the best ROI among the tested multipliers. Both increasing or decreasing the cost threshold result in reduced

Figure 1: Comparison of ROIDICE with other offline algorithms. We average the scores and obtain \( 2\) standard error using 1000 seeds. \(N\) denotes the number of trajectories within the dataset.

policy efficiency, as constrained RL agents typically use the entire available budget to maximize return. When the number of trajectories is insufficient, ROIDICE is rarely outrun by COptiDICE as the large discrepancy between True MDP and MLE MDP causes ROIDICE to be optimized on the erroneous MLE MDPs.

While it is theoretically possible to identify a policy with optimal ROI using a constrained RL algorithm, doing so requires multiple runs with different cost thresholds to determine the best one. This process can become computationally intractable and less accurate as the complexity of the problem increases. In contrast, a single run of ROIDICE is sufficient to find a policy with high ROI without the need to explicitly search for the right cost threshold.

### ROI Maximization in Continuous Domains

We evaluate our algorithm's performance across diverse domains using datasets from D4RL  (CC BY 4.0) and NeoRL  (CC BY 4.0). From the D4RL benchmark, we select three locomotion tasks (Hopper, Halfcheetah, Walker2D), and from NeoRL, we include a stock trading task (FinRL). To assess ROIDICE's capability to maximize Return on Investment (ROI), we conduct comparative analyses with various unconstrained or constrained offline RL algorithms. We utilize offline datasets from D4RL and NeoRL, which are collected using data collection policies that do not consider cost. Similar to the random MDP experiments, we include OptiDICE and COptiDICE for comparison. Additionally, we incorporate the Constrained Decision Transformer (CDT, ) to represent a recent approach that leverages the power of generative models on RL.

#### 5.2.1 Environment and Offline Dataset

In the three locomotion environments, observations include the positional values and velocities of various body parts, with actions corresponding to the torques exerted between links. The common reward function used in recent benchmarks consists of three components: incentive for forward running speed, incentive for maintaining a healthy state, and penalty for applying torques on joints. In this experiment, we use the control penalty as the cost function, aiming to train the agent to strive for efficient energy consumption from an ROI perspective. We used only forward running speed as our reward function.

FinRL, the time-driven stock trading simulator, enables us to replicate live stock markets using real market data. This allows us to assess our algorithm's proficiency in both resource management and decision-making amidst uncertainty. Observations include balances, daily stock amounts, closing

   Task & ROIDICE & OptiDICE &  &  \\  & & 50th & 80th & 50th & 80th \\  H-m & **9.21\(\)0.49** & 3.58\(\)0.03 & 7.37\(\)0.19 & 7.21\(\)0.15 & 3.04\(\)0.2 & 3.0\(\)0.16 \\  H-m-e & **8.29\(\)0.18** & 4.93\(\)0.36 & 7.6\(\)0.44 & 7.88\(\)0.21 & 3.71\(\)0.15 & 3.73\(\)0.16 \\  H-e & **8.51\(\)0.25** & 5.23\(\)0.04 & 7.9\(\)0.14 & 7.99\(\)0.18 & -0.1\(\)0.17 & -0.17\(\)0.23 \\  W-m & **3.06\(\)0.14** & 2.34\(\)0.15 & 2.66\(\)0.63 & 2.92\(\)0.48 & 0.6\(\)0.4 & 0.82\(\)0.31 \\  W-m-e & **4.21\(\)0.78** & 4.03\(\)0.06 & 3.45\(\)0.57 & 3.3\(\)0.49 & 0.91\(\)0.49 & 0.77\(\)0.35 \\  W-e & **5.01\(\)0.42** & 4.59\(\)0.02 & 4.25\(\)0.12 & 4.2\(\)0.12 & -0.01\(\)0.2 & -0.01\(\)0.2 \\  HC-m & **8.48\(\)0.19** & 6.18\(\)0.05 & 8.04\(\)0.4 & 7.98\(\)0.32 & -0.03\(\)0.01 & -0.01\(\)0.01 \\  HC-m-e & **10.17\(\)0.4** & 9.04\(\)1.03 & 8.31\(\)0.4 & 8.06\(\)0.47 & 6.12\(\)0.78 & 6.02\(\)0.08 \\  HC-e & 12.69\(\)0.09 & **12.74\(\)0.13** & 7.42\(\)0.86 & 7.91\(\)0.39 & 12.67\(\)0.07 & 12.66\(\)0.06 \\  F-M & **43.2\(\)13.13** & 40.3\(\)10.29 & 24.4\(\)8.5 & 16.3\(\)5.84 & 20.5\(\)12.63 & 20.5\(\)12.63 \\  F-H & **47.2\(\)8.67** & 41.8\(\)7.23 & 27.3\(\)6.41 & 26.4\(\)4.31 & 46.5\(\)8.97 & 46.5\(\)8.98 \\   

Table 1: ROI of ROIDICE compared with offline RL and offline constrained RL algorithms. We average each score and get \( 2\) standard error with 5 seeds across 10 episodes. The task name is succinctly stated: Hopper (H), Walker2D (W), Halfcheetah (HC), and Finance (F).

prices, and various technical indicators for each of the 30 stocks in the trading pool. Agents use their balances to trade these stocks. We define a cost function based on trading volume and stock price, imposing a fee on each transaction. This requires the agent to strategically optimize its transactions, considering both stock price fluctuations and associated fees. For more details on each environment and experimental setting, see Appendix E.

#### 5.2.2 Results

Table 1 presents our results for all four tasks. Each algorithm is trained for 100k steps and evaluated for a maximum of 1000 steps with 5 seeds across 10 episodes. The data quality levels are denoted as follows: for locomotion tasks, medium (m), medium-expert (m-e), and expert (e); for the financial task, medium (M), and high (H). Complete experimental results are provided in Appendix H.

ROIDICE exhibits superior ROI performance compared to other algorithms across most tasks, particularly excelling in the Hopper environment where there is significant potential for ROI improvement. OptiDICE also demonstrates high ROI in environments other than Hopper due to the high ROI of the offline datasets themselves for Walker2D and Halfcheetah (see Figure 2 for dataset ROIs). However, compared to OptiDICE, ROIDICE shows a greater return with lower accumulated cost in a significant number of tasks (see Appendix H).

The constrained offline RL algorithms are evaluated under two cost thresholds, specifically using the \(80\)th and \(50\)th percentiles of the accumulated costs from the offline dataset to set the cost conditions for COptiDICE (discounted with \(=0.99\)) and CDT (undiscounted). This constraint setting ensures that the constrained offline RL algorithms make in-distribution decisions regarding cost constraints. In some domains, particularly where costs can be lowered with a slight loss in return, COptiDICE demonstrates a higher ROI compared to OptiDICE, but without fine-grained tuning of cost constraints, it does not reach the performance of ROIDICE. Conversely, CDT tends to overfit under the current experiment settings when the dataset lacks diversity. In most cases, CDT either fails to optimize a policy (resulting in negative ROI), or optimizes a policy with a very high return but also a very high accumulated cost (resulting in a small ROI).

Figure 2 illustrates how ROIDICE effectively learns from diverse behaviors in the dataset. We compare the ROI of ROIDICE with that of Behavior Cloning (BC) that clones behaviors only from high ROI trajectories. We compute the ROI of trajectories in the dataset and select the top 20%, 50%, and 80% ROI trajectories to train BC\(n\%\) agents. We can observe that in a significant number of experiment settings, ROIDICE outperforms BC\(n\%\) agents. This demonstrates ROIDICE's capability

Figure 2: ROI Comparison of ROIDICE and Dataset with varying dataset qualities. We average the each scores and get \( 2\) standard error with 5 seeds across 10 episodes. BC\(n\)% refers to behavior cloning utilizing the top \(n\)% of the offline dataset, ranked by ROI.

to stitch together various segments from different trajectories to optimize a policy that behaves more efficiently than any other trajectory in the dataset.

Compared to locomotion tasks, the financial task adds another challenge of high stochasticity. Despite this, ROIDICE demonstrates strong performance on the financial task with both medium and high-quality datasets, as shown in Table 1. OptiDICE, although achieving high returns, shows relatively low ROI performance due to high transaction fees (cost). The high ROI observed from CDT is actually coincidental, as CDT fails to meet the given cost constraint due to overfitting; its return maximization policy accidentally results in an efficient policy with high ROI.

Qualitative Behavior ExamplesFigure 3 illustrates the different behaviors between ROIDICE and OptiDICE in the Hopper task at the same time steps, selected to clearly represent these differences. Recall that in the locomotion task, the cost corresponds to the amount of torque applied, i.e., the energy consumed forcing it to move forward. OptiDICE focuses solely on maximizing return, resulting in the agent making large, far-jumping movements, even applying torque to the joint after the jump, which is inefficient for obtaining additional rewards. In contrast, ROIDICE finds the optimal behavior to increase return while reducing energy consumption. Consequently, the agent attempts to hop far but avoids inefficient actions that decrease ROI.

## 6 Conclusion

In this paper, we propose a novel policy optimization framework designed to maximize the Return on Investment (ROI) of the policy, the ratio between the return and the accumulated cost. Maximizing ROI is particularly relevant in practical scenarios: the fuel efficiency (km/L) of autonomous vehicles, managing the trade-off between decision quality and planning time in meta-controllers for real-time decision-making, or balancing generation quality and inference time in large language models (LLMs). The ROI maximization is approached through linear-fractional programming that incorporates the stationary distribution of the policy. We apply the Charnes-Cooper transformation to convert the problem into equivalent linear programming, ROI-LP. Motivated by the DICE RL frameworks, we extend ROI-LP to develop our offline policy optimization algorithm, ROIDICE, by incorporating convex regularization designed to properly regularize the distribution shift. We show that ROIDICE yields a policy with better efficiency than policies from the existing RL-based optimization methods by considering the trade-off between return and accumulated cost.

## 7 Limitations

While our work is innovative in optimizing the ROI of a policy, it functions within the constraints of an offline setting where direct interaction with the environment is not feasible. Consequently, the ultimate performance of the policy optimized with ROIDICE is contingent upon the quality of the provided dataset. Since ROIDICE takes into account both returns and accumulated costs, it is sensitive to the design of the reward and cost functions. For instance, the agent may behave similarly

Figure 3: Visualization of ROIDICE and OptiDICE in Hopper environment.

to one trained by an unconstrained RL when the cost function approximates a constant. Conversely, the agent may exhibit behaviors aimed at minimizing costs excessively, akin to laziness, when the cost function varies significantly.