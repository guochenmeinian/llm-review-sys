# Learning-to-Cache:

Accelerating Diffusion Transformer via Layer Caching

Xinyin Ma\({}^{1}\) Gongfan Fang\({}^{1}\) Michael Bi Mi\({}^{2}\) Xinchao Wang\({}^{1}\)

National University of Singapore\({}^{1}\) Huawei Technologies Ltd.\({}^{2}\)

maxinyin@u.nus.edu, xinchao@nus.edu.sg

Corresponding author

###### Abstract

Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks. The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters. In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID. To achieve this, we introduce a novel scheme, named **L**earning-to-**C**ache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers. Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching. To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective. An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph. Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed.

Figure 1: (a) Generate 512\(\)512 images using DiT-XL/2, sampled by DDIM with 50 NFEs. (b) Generate 256\(\)256 images using U-ViT-H/2, sampled by DPM-Solver-2 with 50 NFEs.

Introduction

In recent years, diffusion models [60; 59; 19] have achieved remarkable performance as powerful generative models for image generation [49; 10]. Among the various backbone designs for diffusion models, transformers  have emerged as a strong contender, showing its exceptional capabilities not only in synthesizing high-fidelity images [46; 3] but also in video generation [38; 7; 4], text-to-speech synthesis [33; 21; 61] and 3D generation [41; 5]. The diffusion transformer, while benefiting greatly from the great property of scalability of the transformer architecture, however, also brings about significant challenges in efficiency, including high deployment costs and slow inference speed.

Since the cost of sampling increases proportionally with the number of timesteps and the model size per timestep, naturally, current methods for increasing the sampling efficiency entail two branches: reducing the sampling steps[57; 19; 34; 2] or reducing the inference cost per step [16; 67]. The methods to reduce the number of timesteps include distilling the trajectory into fewer steps [52; 58; 39], discretizing the reverse-time SDE or the probability flow ODE [57; 73; 37]. Methods in another branch are mainly about compressing the model size [25; 30] or using a low-precision data format [18; 53]. A new method in the dynamic inference of diffusion is a special cache mechanism in the denoising process [40; 64]. These methods leverage the high similarity between the two steps and the special property of U-Net to cache some of the computations, which would be directly used in the next step. Some other dynamic inference methods employ a spectrum of diffusion models and allocate different networks for different steps [65; 44].

Previous approaches, especially those aimed at reducing model size, have predominantly targeted the compression of the U-Net architecture . Our objective is to explore a paradigm for inference acceleration that is more suitable for transformer-based diffusion models. Unlike other architectures, transformers are distinctively composed of several layers with consistent structure. Based on this property, previous compression work on transformers mainly focuses on layer pruning  and random layer dropping [14; 48], as optimizing at the layer level tends to achieve higher speedup ratios compared to width pruning [24; 15; 8]. However, for diffusion transformers, we observed that dropping layers without retraining is not feasible. Removing even a few layers significantly degrades image quality (see Section 4.3). This observation highlights that the redundancy among layers at varying depths is not evident in DiT. Therefore, we consider another perspective of redundancy: the redundancy across layers situated at the same depths but occurring at different timesteps.

Motivated by cache-based methods [40; 64; 28], we aim to explore the existence and limitations of layer redundancy between timesteps within the diffusion transformer. A straightforward approach involves an exhaustive search where each layer is either cached or not, resulting in an exponentially growing search space with the depth of the layers. Additionally, heuristic-based layer selection cannot adequately address the mutual dependencies between layers. To overcome these challenges, we designed a framework that makes the problem of layer selection differentiable. Specifically, we interpolate predictions between two adjacent steps. This interpolation spans two extremes: a fast configuration where all layers are cached at the expense of image quality, and a slow configuration where all layers are retained, achieving optimal performance. We then search this interpolation space to identify an optimal caching scheme, optimizing a specialized router. This router is time-dependent but input-invariant, allowing the creation of a static computation graph for inference. We train this router by formulating an optimization problem that does not require updating model parameters, making it both cost-effective and easy to optimize.

Our results indicate that different percentages of layers can be cached in DiT  and U-ViT . Notably, for U-ViT-H/2 on ImageNet, approximately 93.68% of layers are cacheable in the cache step, whereas for DiT-XL/2, the cacheable ratio is 47.43%, both with an almost negligible performance loss (\(\)FID < 0.01). By comparison, with the same acceleration ratio, a sampler with fewer steps would compromise image quality. Our method L2C can significantly outperform the fast sampler, as well as previous cache-based methods. Additionally, we observed distinct sparsity patterns for layers between these two models, suggesting significant behavioral variations between different architecture designs for diffusion transformers.

In summary, our contribution is the proposal of a novel acceleration method, learning-to-cache (L2C), specifically for diffusion transformers. We convert the non-differentiable layer selection problem into a differentiable optimization problem by interpolation, facilitating the learning of layer caching. Our results demonstrate that a large proportion of layers in the diffusion transformer can be cached without compromising performance. Furthermore, our approach significantly outperforms samplers with fewer steps and other cache-based methods. The code is available at https://github.com/horseee/learning-to-cache

## 2 Related Work

Transformers in Diffusion Models. Diffusion models have demonstrated broad applicability across various domains[13; 4; 69]. Transformer  is applied in diffusion models as an alternative to UNet. GenViT integrates the ViT architecture into DDPM. U-ViT  employs the long skip connections between shallow and deep layers. DiT  shows the scalability of diffusion transformers and is further used as a general architecture for text-to-video generation [4; 38], speech synthesis  and 3D generation .

Acceleration of Diffusion Models.Generating images by diffusion models requires several rounds of model evaluation which is time-expensive. Some works focus on reducing the number of sampling steps in a training-free manner. DDIM extends the original DDPM to non-Markovian cases. DPM-Solver[36; 37] further approximates the solution of diffusion ODES by the exponential integrators. EDM finds that the Heun'2 2nd order method provides an excellent tradeoff between truncation error and NFE. More works try to solve either SDEs[60; 22; 11] or ODEs[34; 73; 72] in a more accurate and fast way. Other training-based methods [52; 31] distill and half the sampling steps. [58; 39] learns to map any point on the ODE trajectory to its origin. Another line of work reduces the workload per step. The model per step is compressed by reducing the parameter size [16; 6; 71; 63], using reduced precision [29; 18] and re-design the structure of the diffusion model [67; 30; 75; 25; 35]. In addition to static model inference, dynamic model inference has also been extensively explored within diffusion models, which employs different models for inference at varying steps. [32; 45] switch between different sizes of models in a model zoo.  designs a time-dependent exit schedule to skip a subset of parameters. Other works focus on denoising diffusion models in parallel, either through iterative optimization or image splitting. In addition to inference acceleration, some works also show how to train a diffusion model more efficiently by employing different training paradigm [17; 76] or from the data perspective .

Cache in Diffusion ModelsCache  is used in computer systems to hold temporarily those portions of contents in the main memory which is believed to be used in a short time. Recently, [40; 64; 1] explores the cache mechanism in diffusion models. Based on the observations that the similarities of high-level features  is typically very high in consecutive steps, they propose to reuse the feature maps. By utilizing the computation flow of U-Net,  reuse the high-level features while updating the low-level features. [64; 28] further discovers the better position in U-Net to be cached.  proposes to reuse the attention map. [64; 56; 40] adjust the lifetime for each caching features and  further scales and shifts the reused features.  finds the cross-attention is redundant in the fidelity-improving stage and can be cached.  hashes and caches the images rendered from camera positions and diffusion timesteps to improve the efficiency of 3D generative modeling.

## 3 Method

### Preliminary

The forward diffusion process starts at the starting point \(_{0}\), where \(_{0}\) is sampled from the data distribution \(q(_{0})\) to be learned. \(_{0}\) is degenerated with gradually added Gaussian noise, with:

\[_{t} q(_{t}|_{0})=(_{t};_{ t}_{0},_{t}^{2})\] (1)

where \(_{t}\) and \(_{t}\) is the noise coefficient. We can quickly sample \(x_{t}\) at arbitrary timestep by reparameterization trick. And for the reverse process, given two timesteps \(s\) and \(t\), where \(s>0\) and \(t<s\), \(x_{t}\) is calculated as:

\[_{t}=}{_{s}}_{s}-_{t}_{_{ s}}^{_{t}}e^{-}}_{}(_{t_{ }()},t_{}())\] (2)

where \(_{t}=(_{t}/_{t})\). \(t_{}()\) is the inverse function of \(_{t}\) that satisfies \(t_{}(_{t})=t\). \(_{}()\) often represents the learned model, which, in our case, is the diffusion transformer. Previous methodsshow that this integral term can be approximated by adopting Taylor expansion at \(_{s}\), adopting the first-order  or higher-order approximation of this . Take the first-order one as an example, the update of \(_{t}\) would be:

\[_{t}=}{_{s}}_{s}-_{t}(e^{_{ t}-_{s}}-1)_{}(_{s},s)\] (3)

### Approximating \(_{}()\)with a lightweight substitute

The question falls into how to efficiently calculate the term \(_{_{s}}^{_{t}}e^{-}}_{} (_{t_{}()},t_{}())\). Our core idea is that we want to keep more updates between \(s\) and \(t\) while the overall inference time would not increase too much. Suppose that we have three timesteps: \(s\) and \(t\) and one step \(m\) between \(s\) and \(t\), the calculation of \(_{t}\), in the case of Eq.3, would become:

\[_{t}=}{_{m}}_{m}-_{t}(e^{ _{t}-_{m}}-1)_{}(_{m},m), _{m}=}{_{s}}_{s}-_{m} (e^{_{m}-_{s}}-1)_{}(_ {s},s)\] (4)

If we directly set \(_{}(_{m},m)=_{}( _{s},s)\), it would be equivalent to the results in Equation 3 if we take a step directly from \(s\) to \(t\) (see the derivation in Appendix A.1). This approach results in faster computation, as it eliminates the need to compute \(_{}(_{m},m)\); however, it compromises the quality of the resulting image. In contrast, another time-consuming but optimal way is to calculate \(_{}(_{m},m)\) as usual, which necessitates a full model evaluation but yields superior image quality.

Recognizing that \(_{}(_{s},s)\) represents a rapid yet suboptimal solution and \(_{}(_{m},m)\) represents a slower but optimal solution when calculating \(_{t}\), we want to find a model \(}(_{m},m)\), which is the interpolation of these two models. We first define the interpolation as follows:

\[}_{}(_{m},m;)=(_{}(_{s},s),_{}( _{m},m),)\] (5)

where \(}_{}(_{m},m)\) is controlled by a set of variables \(\), functioning as a slider that can smoothly transition between the two endpoints \(_{}(_{s},s)\) and \(_{}(_{m},m)\). \(}_{}(_{m},m)\) needs to meet two criteria: it should approximate the output of \(_{}(_{m},m)\) and be faster for inference compared to \(_{}(_{m},m)\). By creating the interpolation \(\), we generate large collection of models, allowing us to search within this set to find if there exists an \(}_{}(_{m},m)\) that satisfies our requirements.

### Caching the Layer: A Feasible Choice for the Interpolation \(\)

In this section, we specifically define an interpolation \(\) and explore the possibility of the existence of \(}_{}(_{m},m)\) within it. Given the transformer architecture, we propose an interpolation schema

Figure 2: Illustration of Learning-to-Cache. When a layer is activated, the calculation proceeds as usual. In contrast, when a layer is disabled, the computation of the non-residual path is bypassed, and the results from the previous step are utilized instead. The router \(\) smoothly controls the transition between two endpoints \(_{}(_{s},s)\) and \(_{}(_{m},m)\).

by leveraging the layers of the transformer model. Here we take the computation of DiT as an illustrative example. The transformer model can be decomposed into a sequence of basic layers \(L_{i}(h,t)_{i=1}^{D}\), where \(L_{i}(h,t)=h+g(t)*f_{i}(h,t)\), consisting of a residual connection. Here, \(h\) is the input feature, and \(D\) denotes the depth of the model. \(t\) is the time condition. \(f_{i}(h,t)\) can represent either a multi-head self-attention (MHSA) block or a pointwise feedforward block, and \(g(t)\) is a time-conditioned scalar. We omit the condition \(t\) in \(f_{i}(h,t)\) for simplicity. Then we can construct a linear interpolation within the layers, and this interpolation of layer satisfies the model interpolation \(\) (See Appendix A.2):

\[_{i}(h_{i}^{m},m;_{i},_{i})=h_{i}^{m}-(1-_{i}) (h_{i}^{m}-h_{i}^{s})+g(m)(_{i} f(h_{i}^{m})+(1-_{i}) f (h_{i}^{s}))\] (6)

where \(h_{i}^{s}\) and \(h_{i}^{m}\) is the input to the block \(L_{i}\) at timestep \(s\) and \(m\) respectively. \(_{i}\) is a coefficient in layer \(i\) to control the proximity to \(f(h_{i}^{m})\) or \(f(x_{i}^{s})\) and \(_{i}\) is to used as an control for the input. Both of these variables are constrained within the range \(\).

This interpolation provides a special mechanism for inference. If \(_{i}\) in layer \(i\) is set to 0, the output can be directly taken from the layer in the previous timestep, allowing the computation cost in this layer to be skipped. Non-zero \(_{i}\) would trigger the original computation of layer \(i\). A discretized \(_{i}\) can be seen as a router, which selects the layers to be activated or disabled. And for \(_{i}\), it can be set to any value since there is almost no computation cost for a combination of \(h_{i}^{m}\) and \(h_{i}^{s}\) and we choose \(_{i}=0\). By setting more \(_{i}\) in different layers to 0, the acceleration ratio can be cumulative. Therefore, we can calculate the total computational cost based on the number of non-zero \(_{i}\), and our goal \(}_{}(_{m},m)\) can be interpreted as finding as many zeros in \(\{_{i}\}_{i=1}^{D}\) as possible with the minimal approximation error between \(}_{}(_{m},m)\) and \(_{}(_{m},m)\).

One key observation.One greedy way for finding the \(_{i}\) in each layer is taking the approximation error of each layer into account:

\[E=||()-L()||_{2}^{2}=(1-_{i})|g(m)|||f(h_{i }^{m})-f(h_{i}^{s})||_{2}^{2}\] (7)

and taking \(_{i}\) in those layer with smallest \(|g(m)|||f(h_{i}^{m})-f(h_{i}^{s})||_{2}^{2}\) to be 0. In Figure 3, we analyze \(||f(h_{i}^{m})-f(h_{i}^{s})||_{2}^{2}\) in two types of models: DiT and U-ViT. We find that performance varies significantly across different timesteps, even at the same layer. Particularly in the DiT model, the error is markedly higher in the later steps compared to the early denoising steps. Additionally, the performance of multi-head self-attention differs substantially from that of feedforward layers. Based on this, we assign each timestep with its own \(\{_{i}\}_{i=1}^{D}\). Thus, \(\) becomes time-variant, where \(=\{_{ij} i=1,2,,T;j=1,2,,D\}\) and \(T\) is the total denoising steps.

In addition, we directly use this metric as the criterion for \(_{ij}\) and employ it during inference. From the experimental results in 4, we observe that it cannot effectively handle a combination of layers. This limitation arises because the approximation error for each layer is influenced by changes in the preceding layer. However, exhaustively evaluating all possible configurations is impractical, as the number of trials increases exponentially with the depth of the model.

### Learning to Cache

To address this, we propose the following method: Learning to Cache. Recall that our goal is to find a \(}_{}(_{m},m)\) that is (1) as close as \(_{}(_{m},m)\) and (2) with minimal computation cost. We can reformulate this as an optimization problem as:

\[_{}||(_{m},m;)-( {x}_{m},m)||_{2}^{2}\;_{i=1}^{D}_{_{ij}1} C\] (8)

Figure 3: Approximation Error for DiT and U-ViT in different timesteps and different layers

where \(C\) is the constraint for the total cost. \(_{_{ij},1}\) is the Kronecker delta function, which is 1 if \(_{ij}=1\). Though \(_{ij}\) in the final solution needs to be discrete, \(_{ij}\) is designed to be continuous to make the computation differentiable when optimized. And when inference, a threshold \(\) would be set to discretize the \(_{ij}\) to be either 0 or 1, where \(_{ij}\) turned to become a router. The only trained variables in our algorithm are \(\). Thus, the parameters in the diffusion model would remain unchanged. With the help of Lagrange duality to transform the optimization problem into an unconstrained one, the loss would be:

\[(},,_{m},m;)=||(_{m},m;)-(_{m},m)||_{ 2}^{2}+_{i=1}^{D}_{ij}\] (9)

where \(\) is the Lagrangian multiplier that governs the regularization. We show the algorithm for training and inference in Algorithm 1 and 2. To ensure \(\) remains within the range \(\), a sigmoid operation is performed before \(\) is passed into the model. In these algorithms, we adopt layer caching every two steps, representing that only half of the steps would inference in a faster speed. For simplicity, the image encoder and decoder are omitted.

```
1:Input: Data distribution \(p(_{0})\), diffusion model \(_{}()\), learning rate \(\), ODE solver \(()\), total steps \(T\) and the step schedule \(\{t_{i}\}_{i=1}^{T}\) in \(()\)
2:\((0,1)\)
3:repeat
4:\(_{0} p(_{0})\), \(n[1,T/2]\) // Step \(s\) for calculating states for caching
5:\(s t_{n*2}\)
6:\(_{s}(_{s};_{s}_{0},_{1}^{ 2})\)
7:\(_{s}(_{s},s)\) and cache \(\{f()\}_{i=1}^{D}\)
8:// Step \(m\) for using cached states
9:\(m t_{n*2-1}\)
10:\(_{m}(_{s},s,m)\)
11:\(_{m}(_{m})\)
12:// Optimize
13: Calculate \((_{m},m;_{m})\) by Eq.6
14:\(||(x_{m},m)-_{}(_{m}, m)||_{2}^{2}+_{m}\)
15:\(_{m}_{m}-_{_{m}}\)
16:until converged ```

**Algorithm 1** Training

## 4 Experiments

### Experimental Setup

Models and Datasets.We explore our methods on two commonly used transformer architectures in diffusion models: DiT  and U-ViT . Specifically, we use DiT-XL/2 (256\(\)256), DiT-XL/2 (512\(\)512), DiT-L/2 and U-ViT-H/2. Except for DiT-L/2, we use the officially released models. We trained a DiT-L/2 for one million steps, which is used to investigate if layer redundancy exists in smaller models that may not be fully converged. Most of the results are presented under the resolution 256\(\)256 and we also show the results on models that generate high resolution 512\(\)512 images.

Implementations.Since the parameters of the diffusion model would not be updated, the only parameters that require optimization are \(\), resulting in a very limited number of variables. For example, for DiT-XL-2 with 20 denoising steps, the number of trainable variables is 560. We take the training set of ImageNet to train \(\) for 1 epoch. The learning rate is set to 0.01 and AdamW optimizer is used to optimize \(\). The training is conducted upon 8 A5000 GPUs with a global batch size equal to 64. To train with classifier-free guidance, we randomly drop some labels and assign a null token to the label. The dropping rates for labels follow the original training pipeline.

Evaluation.We tested our method upon two samplers, DDIM and DPM-Solver, with sampling steps from 10 to 50. For the DiT model, we use the DDIM sampler. And for U-ViT, we use the DPM-Solver-2. All the experiments here use classifier-free guidance. To evaluate the image quality, 50k images are generated per trial. We measure the image quality with Frechet Inception Distance(FID), sFID, Inception Score, Precision and Recall. Besides, we reported the total MACs and the latency to make a comparison of the acceleration ratio. The MACs is evaluated using pytorch-OpCounter2, and the latency is tested when generating a batch of images(8 images) with classifier-free guidance on a single A5000, which we conducted five tests and took the average.

### Main Results

We present the results of DiT in Tables 1 and 2, comparing our algorithms with samplers of comparable inference speed. Our method requires more denoising steps, but each step takes less average time. In contrast, samplers require fewer steps, but each step takes more time. Our experiments demonstrate that our methods significantly outperform DDIM and DPM-Solver. For instance, with the 20-step DDIM on DiT-XL/2, our method achieves an FID of 3.46, nearly identical to the unaccelerated one. In comparison, the DDIM achieves an FID of 4.68. When generating high-resolution images, sampling with fewer steps, or using a relatively smaller model, our method still outperforms baselines.

   Methods & NFE & MACs (T) & Latency(s) & Speedup & FID\(\) & NFE & MACs & Latency & Speedup & FID\(\) \\   \\  DDPM & 250 & 28.61 & 36.55 & - & 280.1 & 2.27 & 4.54 & 82.73 & 57.95 \\ DDIM & 250 & 28.61 & 36.45 & - & 243.4 & 2.14 & 4.55 & 80.70 & 60.57 \\  DDIM & 50 & 5.72 & 7.25 & 1.00\(\) & 238.6 & 2.26 & 4.29 & 80.16 & 59.89 \\ DDIM & 40 & 4.57 & 5.82 & 1.24\(\) & 239.8 & 2.39 & 4.28 & 80.36 & **59.13** \\ Ours & 50 & 4.36 & 5.57 & 1.30\(\) & **244.1** & **2.27** & **4.23** & **80.94** & 58.76 \\  DDIM & 20 & 2.29 & 2.87 & 1.00\(\) & 223.5 & 3.48 & 4.89 & 78.76 & 57.07 \\ DDIM & 16 & 1.83 & 2.30 & 1.25\(\) & 210.9 & 4.68 & 5.71 & 76.78 & **56.20** \\ Ours & 20 & 1.78 & 2.26 & 1.27\(\) & **227.0** & **3.46** & **4.64** & **79.15** & 55.62 \\  DDIM & 10 & 1.14 & 1.43 & 1.00\(\) & 158.3 & 12.38 & 11.22 & 66.78 & 52.82 \\ DDIM & 9 & 1.03 & 1.29 & 1.11\(\) & 140.9 & 16.57 & 14.21 & 62.28 & 49.98 \\ Ours & 10 & 1.04 & 1.30 & 1.10\(\) & **156.3** & **12.79** & **10.42** & **66.21** & **52.15** \\   \\  DDIM & 50 & 22.85 & 37.73 & 1.00\(\) & 204.1 & 3.28 & 4.50 & 83.33 & 54.80 \\ DDIM & 30 & 13.71 & 22.51 & 1.68\(\) & 198.3 & 3.85 & **4.92** & **83.01** & **56.00** \\ Ours & 50 & 14.19 & 22.57 & 1.67\(\) & **202.1** & **3.69** & 5.03 & 82.90 & 54.60 \\   \\  DDIM & 50 & 3.88 & 5.06 & 1.00\(\) & 167.6 & 4.82 & 4.40 & 78.72 & 54.66 \\ DDIM & 40 & 3.10 & 4.06 & 1.25\(\) & 168.2 & 4.99 & 4.43 & **79.01** & 54.71 \\ Ours & 50 & 2.95 & 4.01 & 1.26\(\) & **168.3** & **4.82** & **4.41** & 78.97 & **54.73** \\  DDIM & 20 & 1.55 & 2.01 & 1.00\(\) & 160.16 & 6.45 & 5.26 & 77.13 & 53.65 \\ DDIM & 16 & 1.24 & 1.63 & 1.23\(\) & 151.70 & 7.91 & 6.24 & 75.93 & 51.71 \\ Ours & 20 & 1.20 & 1.60 & 1.26\(\) & **160.53** & **6.55** & **5.08** & **77.47** & **52.22** \\   

Table 1: Accelerating image generation on ImageNet for the DiT model family.

   Methods & NFE & MACs & Latency & Speedup & FID\(\) & NFE & MACs & Latency & Speedup & FID\(\) \\  DPM-Solver & 50 & 6.44 & 19.37 & 1.00\(\) & 2.3728 & 20 & 2.58 & 7.69 & 1.00\(\) & 2.5739 \\ DPM-Solver & 30 & 3.86 & 11.55 & 1.68\(\) & 2.4644 & 16 & 2.06 & 6.08 & 1.26\(\) & 2.7005 \\ Ours & 50 & 3.79 & 11.16 & 1.74\(\) & **2.3625** & 20 & 1.92 & 5.64 & 1.35\(\) & **2.5809** \\   

Table 2: Results with U-ViT-H/2 on ImageNet dataset. The resolution here is 256\(\)256. We adopt the DPM-Solver-2, which has 2 function evaluations per step. The total NFE (instead of steps) is reported below. Guidance strength is set to 0.4.

However, we observe that achieving nearly lossless compression under these conditions is challenging. We argue that this difficulty arises because layer redundancy is less apparent in these scenarios.

Quality-Latency Tradeoff.We show the trade-off curve between FID and Latency in Figure 4. These figures offer a more comprehensive comparison with two types of baselines: (1) **Heuristic Methods for Selecting Layers**. We designed several methods for selecting layers to cache, including rule-based approaches such as caching from top to bottom or from bottom to top, randomly selecting layers, and metric-based selection as described in Eq.7. We found that when the dependency between layers must be considered, they fail to select the optimal layers, leading to a degradation in image quality. In contrast, our method consistently achieves improved quality across various acceleration ratios. (2) **Sampler with fewer steps**. Our method significantly outperforms DDIM and DPM-Solver, as evidenced by the detailed comparison provided.

Maximum Cacheable Layers for diffusion transformer.From the trade-off curve, we found that there exists an upper limit for the number of cacheable layers. Below this limit, image quality remains almost unaffected, as indicated by a FID degradation of less than 0.01. This limit is detailed in Table 4. Notably, caching does not occur at every step: step \(s\) involves full model inference, while only step \(m\) caches layers. With a significant proportion of layers can be cached and the computation of these layers to be saved, notable differences emerge between the U-ViT and DiT models. For instance, in U-ViT, up to 94% of layers can be discarded for the cache step during the denoising process, whereas this proportion is considerably lower for DiT. Furthermore, we observed that the cacheable ratios for FFN and MHSA vary.

Comparison with other cache-based methodsWe also compared our method with other cache-based methods. Notably, previous cache-based methods are strongly coupled to the U-Net structure and cannot be applied to models without the U-structure, such as DiT. To ensure a fair comparison, we selected U-ViT, which incorporates both the U-structure and transformers, to implement these methods as baselines alongside our method. Table 3 presents the comparison results. The findings demonstrate that our method achieves better quality than the baselines.

   Model &  &  \\  NFE & 50 & 20 & 50 & 20 \\  Remove Ratio & 47.43\% & 44.29\% & 93.68\% & 63.79\% \\ FFN Remove Ratio & 47.85\% & 44.64\% & 94.11\% & 60.54\% \\ MHSA Remove Ratio & 47.00\% & 43.93\% & 93.25\% & 67.05\% \\   

Table 4: Maximum cacheable layers for DiT and U-ViT with different steps.

Figure 4: Speed-Quality Tradeoff for DiT-XL/2 and U-ViT-H/2 with 20 denosing steps as the basis. The dashed line indicates the performance without applying inference acceleration.

   Methods & NFE & Latency & Speedup & FID\(\) \\  DPM-Solver & 20 & 7.69 & 1.00\% & 2.57 \\  DeepCache & 20 & 4.68 & 1.64\% & 2.70 \\ Ours & 20 & 4.62 & 1.67\% & **2.64** \\  Faster Diffusono & 20 & 5.95 & 1.29\% & 2.82 \\ Ours & 20 & 5.93 & 1.30\% & **2.57** \\   

Table 3: Comparison with other cache-based method on U-ViT

### Analysis

The Learned Pattern of \(\)We present the learned pattern in Figure 5. The two different architectures produce distinct patterns. For U-ViT, the entire middle section is almost entirely cacheable, allowing it to be replaced with the results from the previous step's calculations. However, the computations at both ends of the model are crucial and cannot be discarded. This observation explains why DeepCache outperforms faster-diffusion on U-ViT, as the learned patterns resemble the manually designed approach of DeepCache. However, this phenomenon is not clearly observed in DiT-XL. Additionally, we found a consistent tendency across models to retain more computation in the later stages while discarding calculations in the earlier stages. This observation aligns with our findings in Figure 3. When comparing the impact of different steps within the same layer, removing parts with smaller timestep has a greater effect on the changes in the output.

Comparison between Layer Cache and Layer DropoutLayer dropout involves directly removing \(f_{i}()\), retaining only the computation in the skip path. We compare our method with layer dropout, where the layers are either randomly dropped or optimized using our algorithm (named Learning-to-Drop). The results, presented in Table 5, indicate that layer caching significantly outperforms layer dropout. Interestingly, when we learn the layers to be dropped, the models still produce acceptable images, although the quality is not as high. Illustrative examples are provided in Appendix B.2.

Choice of thresholdWe investigated the effect of different thresholds on the image quality. Results are shown in Figure 6, where the model here is trained with six different \(\) (corresponding to 6 points on one curve). We show the effect of different \(\) in Appendix B.3. Our results reveal that for higher acceleration ratios, a larger threshold improves image quality. Conversely, for lower acceleration ratios, a smaller threshold is more effective. These also findings suggest that ranking layers by importance is not a reliable approach, since the selection of layers does not follow a strict sequential order. Otherwise, one threshold would win all.

Figure 5: Learned Router \(\) for DiT-XL/2 (Top) and U-ViT-H/2 (Bottom). Different caching patterns are observed in different types of diffusion transformers.

Figure 6: Effect of threshold \(\).

   Methods & Remove Ratio & Latency(s) & Speedup & IS\(\) & FID\(\) & sFID\(\) & Precision\(\) & Recall\(\) \\  Random Drop & 170/560 & 2.439 & 1.18\(\) & 3.36 & 277.42 & 171.83 & 1.23 & 0.24 \\ Learning-to-Drop & 179/560 & 2.421 & 1.19\(\) & 113.93 & 17.35 & 28.46 & 60.25 & 52.68 \\ Learning-to-Cache & 176/560 & 2.438 & 1.18\(\) & 226.13 & 3.47 & 4.58 & 79.19 & 56.47 \\   

Table 5: Comparison with layer dropout. The removal ratio corresponds to the percentage of sub-layers being removed, including both MHSA and MLP blocks, for a total of 28 layers and 10 steps.

Limitation

The primary limitation of this work arises from its dependence on the trained diffusion models. For instance, when applied to DiT-XL/2 at a resolution of 512, our method encounters a slight drop in FID. Although it still surpasses the baseline, this indicates that the lossless caching of the layers does not uniformly exist across all models. It highlights significant variations between different models, and thus our method is strongly dependent on the structure design of the trained diffusion models. Another limitation of our method is that the acceleration is capped at 2\(\) because every two steps consist of one full model inference step and one cheaper step. This inherently restricts the maximum achievable acceleration ratio. However, we believe that this approach can be expanded to more than two steps, potentially improving the overall efficiency.

## 6 Conclusion

In this paper, we propose a novel acceleration method for diffusion transformers. By interpolating between the computationally inexpensive solution but suboptimal model, and the optimal solution but expensive model, we find there exist some models which would infer much faster and also produce high-fidelity images. To find this we train the router which is continuous when training and would be discretized when inference. Experiments show that our method largely outperforms baselines such as DDIM, DPM-Solver and other cache-based methods.