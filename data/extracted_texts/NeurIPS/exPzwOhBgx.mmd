# Learning Dictionary for Visual Attention

Yingjie Liu\({}^{1}\)   Xuan Liu\({}^{2}\)   Hui Yu\({}^{2}\)   Xuan Tang\({}^{1}\)   Xian Wei\({}^{1}\)

\({}^{1}\)East China Normal University

\({}^{2}\)FJIRSM, Chinese Academy of Sciences

Corresponding author: xwei@sei.ecnu.edu.cn

###### Abstract

Recently, the attention mechanism has shown outstanding competence in capturing global structure information and long-range relationships within data, thus enhancing the performance of deep vision models on various computer vision tasks. In this work, we propose a novel alternative dictionary learning-based attention (_Dic-Attn_) module, which models this issue as a decomposition and reconstruction problem with the sparsity prior, inspired by sparse coding in the human visual perception system. The proposed _Dic-Attn_ module decomposes the input into a dictionary and corresponding sparse representations, allowing for the disentanglement of underlying nonlinear structural information in visual data and the reconstruction of an attention embedding. By applying transformation operations in the spatial and channel domains, the module dynamically selects the dictionary's atoms and sparse representations. Finally, the updated dictionary and sparse representations capture the global contextual information and reconstruct the attention maps. The proposed _Dic-Attn_ module is designed with plug-and-play compatibility and facilitates integration into deep attention encoders. Our approach offers an intuitive and elegant means to exploit the discriminative information from data, promoting visual attention construction. Extensive experimental results on various computer vision tasks, e.g., image and point cloud classification, validate that our method achieves promising performance, and shows a strong competitive comparison with state-of-the-art attention methods.

## 1 Introduction

Visual perception plays a critical role in obtaining external information. However, processing massive visual information in the real world is challenging for both human brains and computers. Modern psychology and cognitive neuroscience find that the visual attention mechanism is one of the essential keys to a human's good cognitive ability , enabling selective and sparse activation of neurons in response to input. In the field of computer vision (CV), various attention methods have also been developed, with successful applications in lots of Convolutional Neural Networks and Transformers, revealing great potential in visual tasks . CV attention methods utilize various calculation functions to generate attention maps, but they can be seen as a general form (refer to Sec. 3.1) . Researchers have recognized several advantages of the attention mechanism, e.g., capturing global long-term information and extracting hierarchical data structures . Moreover, recent research has also highlighted the potential of leveraging sparsity to develop attention algorithms . However, there are still pending questions regarding the efficiency and effectiveness of current attention methods when dealing with regular image features and non-Euclidean point cloud data . Considering the significant memory costs and time complexity  associated with current attention methods, it is crucial to explore possibilities of optimizing their internal operations or re-evaluating the current form, especially in the view of sparsity.

In this context, recalling the previous works and considering the original motivation behind introducing attention methods remind us of the dictionary learning/sparse coding algorithm, which holds importance but is often overlooked in present discussions. The dictionary learning algorithm decomposes visual data into a dictionary basis and their corresponding sparse representations/codes. Under different conditions, the sparsity of sparse code can be adapted: code with higher sparsity (more zero elements) mostly corresponds to features of smaller scale, higher roughness, or higher hierarchy. Furthermore, this process echoes the nonlinear effects in complex cells and the sparse coding strategies for sensory information in the primary visual cortex and advanced visual region . It has been observed that the structure in a sparse domain could make the hidden patterns more prominent and easier to be captured . All these motivate us to develop an alternative novel attention form, i.e. **Dictionary learning-based Attention** (_Dic-Attn_), that offers a visual focus for deep neural networks (DNNs). The _Dic-Attn_ module can be conveniently used to replace existing attention modules, and compatibly explain their working principles.

The _Dic-Attn_ module employs two selection matrices to exploit channel and spatial attention from dictionary atoms. By selecting the appropriate basis and re-locating the coordinates, we can uncover hidden visual attention information by reconstructing the attention map using the updated basis (dictionary) and coordinates (corresponding sparse representations). Note that, the sparse representations in our proposed module originate from the non-linear decomposition of visual data and are subsequently reconstructed into the attention matrix. The sparsity property of the attention matrix is induced naturally by the sparse prior  and is related to the visual attention area within the images. The main contributions of this paper are summarized as follows:

1. We propose a novel visual attention module, namely, the **Dictionary learning-based Attention** (_Dic-Attn_) module, which is based on dictionary learning and provides better interpretability of providing contextual information.
2. We demonstrate that the proposed module has great potential in disentangling non-linear structural information by employing a masked dictionary and transformed sparse representations. We construct a deep attention stage composed of the proposed modules. Each module at different depths, ranging from shallow to deep, dynamically updates dictionaries and sparse representations through spatial and channel transformations, step-wise-ly reconstructing the final attention map.
3. We apply the proposed module to various visual models since it is designed to be plug-and-play, achieving better performance than various existing attention modules on image classification, semantic segmentation, and point cloud classification, with fewer parameters and faster computation.

Figure 1: The pipeline of our proposed _Dic-Attn_ block. First, input features \(}\) into the _Dic-Attn_ block. Within the _Dic-Attn_ block, the inner _Dic-Attn_ module decomposes the normalized input \(\) into a dictionary and corresponding sparse representations, which will be transformed in the spatial and channel domains. The _Dic-Attn_ module can dynamically adjust the weights to promote the visual focus on influential elements. This process is both data and task-driven, thereby enhancing the effectiveness of the attention mechanism. Finally, the output of the _Dic-Attn_ block is obtained by summing the input with the **Attention Output**. The _Dic-Attn_ block can be stacked layer by layer to form a deep attention model, enabling capturing and integrating visual attention hierarchically across multiple levels.

Related Work

Attention Mechanism in Computer Vision Tasks.CV Attention methods simulate the phenomenon of the human visual system, where selective attention helps neural system analyze and understand complex scenes more efficiently. Recurrent Model  is a pioneering work that develops the attention mechanism in DNNs. Later, various attention methods are proposed. Some of them try to improve the pre-processing method of the input images [8; 54], while others modify the way of obtaining the attention matrix, resulting in a series of variants, e.g., spatial transformer network (STN) , self-attention (SA) , squeeze-and-excitation network (SENet) , dual attention (DA) module , double attention (A\({}^{2}\)) module , hamburger (HAM) module , etc. [21; 10; 48; 25; 39]. The SA module is proposed in  and first introduced to computer vision in . It achieved great success in various vision tasks and made the Vision Transformer (ViT) successful in academia.  constructs the long-term global context learning as a low-rank recovery problem and proposes the matrix decomposition (MD) module, focusing only on the low-rank property prior assumption of feature representations. It may have limitations in fully capturing the complexity and richness of global structure information in the data. Most recent related work  was inspired by the two prominent features of the human visual attention mechanism, i.e., recurrency and sparsity. The so-called recurrency attribute in  naturally comes from the ordinary differential equation description of the sparse coding update process, which is actually inside the attention module and the sparse representations require a multi-step update. It also lacks the analysis of attention modules with different depths. Moreover, the residual connections between multi-layer attention modules still remain under explanation.  measures sparsity as the percentage of nonzero entries in the activation maps of each layer. It finds that sparsity emerges in the parameters of all layers of the Transformer. However, it does not explicitly state whether the output of the attention module exhibits sparsity or not. Some studies [37; 51; 17; 33] have tried to post-hoc-interpret these developed attention methods, and utilize white-box approaches to do the analysis [13; 41; 58; 31].

Dictionary Learning in Computer Vision Tasks.Sparsity is an important and noticeable prior in the field of computer vision, e.g., the statistically redundant property of natural images is served for dictionary learning. Hence, image sample \(\) is reconstructed with the dictionary \(\) and its corresponding sparse code \(\), i.e., samples can be decomposed into a dictionary and corresponding sparse representations. The dictionary \(\) captures representative and critical features from the inputs and reveals the interference of insignificant information. Dictionary learning has been extensively studied and verified to be an outstanding approach for exploiting the underlying sparse structures hidden in images [29; 52; 50; 55; 60], efficiently transferring and expressing information from high-dimensional image data into sparse space. It plays an important role in various computer vision tasks, e.g., image reconstruction [1; 29; 3; 38; 64], image classification[52; 47; 55; 24; 60], visual tracking  and others.. Note that, compared with non-negative matrix factorization and vector quantization applied in MD module  mentioned above, the dictionary learning algorithm performs better in the case of natural images . Technical details about how we learn the dictionary is shown in Sec. 3.2.

## 3 Method

In this section, we present the proposed dictionary learning-based visual attention (Dic-Attn) method. First, we give a brief review and summary of the CV attention form and dictionary learning. Then, we describe the main idea of the _Dic-Attn_ module to obtain visual attention. Finally, we analyze the core concept behind the _Dic-Attn_ module. The overall structure of the proposed _Dic-Attn_ method is illustrated in Fig. 1.

### Attention Form

Generally speaking, we can summarize a CV attention form [11; 41].

\[,\] \[f().\] \[g(f(),).\]The input features \(\) are firstly pre-processed. Subsequently, the attention module obtains an attention map of the input. Finally, \(g()\) is introduced to further connect attention map and input features, e.g., the normalization operation, drop operation, skip-connection, etc.. All these constitute an **Attention Block**. Among them, the function \(f():\) in the attention module is the most critical mapping. Attention blocks can be added to the middle of the network, or form a feed-forward network. For example, in the ViT encoder with depth \(N\), there will be \(N\) attention blocks. Take ViT as an example, suppose there is a feature tensor \(\) with shape \(b n h w\), where \(n\) denotes the number of channels. The feature tensor is vectorized in the spatial domain and the last two dimensions are transposed. Hence, the attention block takes a sequence of image features as input, i.e., \(^{b s n}\), where \(s=h w\). The Self-Attention module is introduced and subsequently outputs a spatial attention map, essentially operating in the spatial domain. The formulation of the standard Self-Attention module in ViT  is as follows:

\[[,,]=[} ,},}],\\ =softmax(}(^{T})), \\ =, \]

where \(^{b s n}\) is the input, \(^{b s n}\), \(^{b s n}\) and \(^{b s n}\) denote the query matrix, the key matrix, and the value matrix, respectively. They are obtained by full-connected layers. \(},}\) and \(}\) are all square matrices with the shape \(n n\). \(\) represents the attention matrix. Finally, through the residual structure, i.e. skip-connection, attention blocks have the final output result, i.e., **Attention Output**\(+\)\(\).

The implicit variables or the spatial attention map results of these attention modules can be visualized, e.g., traditional SA blocks in ViT , and External-Attention (EA) blocks . These works indicate that the attention module focuses on the discriminative regions of each image. Recalling the CV attention form we summarized at the beginning of this section, we can find that \(f()\) is the most important function when obtaining visual attention in DNNs. In this paper, \(f()\) contains the process of dictionary learning, the transformation of both learned dictionary & corresponding sparse representations, as well as reconstruction, which is to be introduced in Sec. 3.2.

### Dictionary Learning

Formally, given data \(:=[x_{1},...,x_{n}]^{n s}\), the standard dictionary learning algorithm is to find a dictionary \(\) and the corresponding sparse code \(\). The dictionary \(=[_{1},...,_{k}]\) has a shape of \(n k\), the sparse code \(=[_{1},...,_{k}] ^{k s}\) selects atoms from the dictionary to reconstruct the input \(\). The objective function of dictionary learning can be formulated as follows:

\[*{argmin}_{,_{i}}\ h()+_{i=1}^{s}\|x_{i}- _{i}\|_{2}^{2}+g(_{i}),\\ \ (n,k),_{i} ^{k}, \]

where \(h()=_{i j}\|_{i}^{T}_{j} \|_{F}^{2}\) denotes the regularization term of the dictionary. It leads to mutual incoherence between pair atoms in the dictionary. Besides, \((n,k):=\{_{*}^{n k }:diag(^{T})=_{k}\}\). Namely, the set of dictionaries is a product manifold of \(s\) times the \((k-1)\)-dimensional unit sphere, i.e., \((n,k)\) restrict all atoms \(_{i}^{k}\) to have unit norm. \(g(_{i})\) usually utilizes norm constraint to control the sparsity of the sparse code \(_{i}\). The \(l_{1}\) norm regularization is a popular way to replace \(l_{0}\)-norm by \(l_{1}\)-norm convex relaxation, known as the Lasso  regression. It has numerous variants, e.g., elastic net. Elastic net inherits the stability of Ridge and the sparsity brought by Lasso, for combining both regularization terms of \(l_{1}\) norm and \(l_{2}\) norm. Hence in this paper, we have \(g(_{i})=\|_{i}\|_{1}+\|_{i}\|_{2}^{2}\). \(\) and \(\) are regularization weights.

Eq. (2) can be regarded as an iterative optimization problem, where dictionary \(\) and sparse representation alternately updates. Each sparse code is an implicit function about the temporarily fixed dictionary \(\). With Eq. (2), the incoherent assumption of dictionary atoms and the sparse prior \(g(_{i})\), the close solution of sparse representation can be expressed as follows:

\[_{i}^{*}=(_{}^{T}_{}+ )^{-1}(_{}^{T}x_{i}+_{}), \]where \(:=\{i\{1,...,s\}|_{ij}^{*} 0\}\) denotes the set of indexes of the non-zero entries of the solution \(_{i}^{*}=[_{i1}^{*},..,_{is}^{*}]\). \(_{}\) is the subset of the dictionary in which the indexes of atoms fall into \(\). \(||\) denotes the sparsity of \(_{i}^{*}\), \(_{}\{ 1\}^{||}\) carries the signs of \(_{i}^{*}\). Technically, it is easy to realize the calculation of the above polynomial formula and obtain the sparse representations.

Constructing or learning a dictionary is also crucial for achieving the goal of highlighting significant objects while suppressing distracting or noisy elements. In the early stage, the dictionary is constructed by Cosine functions, Fourier, Wavelets, Contourlets, Gabor, a set of complete bases, etc. These approaches were widely employed in signal processing due to their mathematical simplicity. However, such a fixed dictionary is manually designed under some mathematical constraints and is not flexible enough to represent complex natural image structures. Recently, researchers have turned to directly initializing and learning the dictionary from image data, which can also improve the sparsity of \(\) and the performance of downstream tasks . The dictionary contains prototype underlying features of vision data, based on which each sample can find its unique optimal sparse solution.

### The Proposed _Dic-Attn_ for Visual Attention

#### 3.3.1 _Dic-Attn_ Method

The _Dic-Attn_ module first decomposes the input into a dictionary and sparse representation. Then its primary objective is to provide visual attention to the model by reconstructing the attention map with residuals. Its process is as follows,

\[ =, \] \[ =_{},\] \[ =softmax(_{scale}(_{})),\] \[=,\]

where \(=[_{1}^{T},_{2}^{T},...,_{b}^{T}] ^{b s n}\). Each atom \(_{i}^{n}\) in the dictionary \(^{n k}\) will be re-weighted by \(_{}^{k}\). The elements of \(_{}^{k}\) are trainable mask weights for dictionary atoms. \(_{}^{k k}\) denotes the selection mask matrix for sparse representations \(=[_{1},...,_{b}]^{b k s}\). \(_{scale}\) denotes the scaling matrix. Note that \(k= n\), \(\) is the hyper-parameter.

Firstly, the dictionary is learned on the entire training-dataset, aiming to be adaptive to all inputs \(\). It contains diverse and important features (in atoms) of the input, so it can solve the sparse coding problem and obtain the corresponding code \(\). We have:

\[=[_{1}^{c}_{2}^{c} _{k}^{c}]=[_{1}^{r}_{2}^{r} _{n}^{r}]^{T},\]

therefore,

\[_{m}=[_{1}_{2}_{ s}]=[_{1}_{2}_{k} ]^{T},\]

\[_{m}=_{11}&_{12}&&_{1n}\\ _{21}&_{22}&&_{2n}\\ &&&\\ _{s1}&_{s2}&&_{sn}= _{1}^{r}_{1}&_{1}^{r}_{2}&&_{1}^{r}_{n}\\ _{2}^{r}_{1}&_{2}^{r}_{2}&&_{2}^{r}_{n}\\ &&&\\ _{s}^{r}_{1}&_{s}^{r}}\) impose weights on different channels \(_{i}_{i}\) in the reconstruction process. In view of sparse representations/codes, the _Dic-Attn_ module employs sparse prior and exploits the intrinsic underlying structure of features. The sparsity involved in the attention module arises spontaneously from the statistically redundant natural images. The sparse representation serves as the coordinates of the dictionary. These sparse representations indicate the importance or relevance of each atom in the dictionary for reconstructing the input features. Importantly, the attention similarity between different tokens or grid features in \(\) is inherently encoded within the dictionary and the sparse representation. \(}\) modifies individual elements and \(}\) applies unified weights within each column of \(\), respectively. These transformation parameters are driven by the task objective function and are updated through the backpropagation process.

```
Input:\(^{b s n}\), depth \(N\), Parameter:\((n,k),}^{k k}, }^{k},}\) Latent Variables:\(^{k s}\), \(f(})\), \(\) Output:Attention Output # Initialize \((n,k)\) by randomly sampling from \((0,1)\) and atom-wise normalization; # Initialize \(}\), \(}\), \(}\) with reference to kaiming-init method ; # forward ## Update \(\) by Eq. (3); ## Obtain Attention Output by Eq. (4); ## backward ## Update \(,}\), \(}\), \(}\) by minimizing loss function of the network ;
```

**Algorithm 1**Dictionary Learning-Based Attention Module

Algorithm 1 summarizes the algorithm flow of the _Dic-Attn_ module. Furthermore, it is worth noting that constructing a deep _Dic-Attn_ vision model is feasible. All these transformation matrices are trained in a back-forward manner and serve as a crucial link in connecting the dictionary, sparse representations, and reconstructed attention maps. To enhance the stability and effectiveness of the _Dic-Attn_ module, it is recommended to include a normalization step before the attention module as . Additionally, a residual connection between the input \(\) and the **Attention Output** facilitates the flow of information. This results in the formation of the _Dic-Attn_ block, as illustrated in Fig. 1.

In summary, the _Dic-Attn_ module utilizes a data-initialized and task-driven dictionary strategy, which captures global information and serves as a comprehensive resource containing essential data constituents. The process of reconstructing attention maps fully capitalizes on the dictionary learning sparse coding approach, the ability to extract both channel and spatial information from nonlinear features. The _Dic-Attn_ module has a simpler structure with fewer parameters and elegant internal operations while achieving higher performance. Moreover, the _Dic-Attn_ module is a convenient plug-and-play attention module that can be easily integrated into various vision networks.

#### 3.3.2 General _Dic-Attn_-Based Vision Transformer

Herein, we give a guide on how to build a _Dic-Attn_-based Vision Transformer and analyze the role of the proposed _Dic-Attn_ method. The model architecture can be divided into three main parts: the preprocessing stage, the deep attention stage, and the post-processing stage for downstream tasks.

Firstly, multi-layer convolution and linear operations are employed to extract primary visual features from the natural images. Then, multiple concatenated _Dic-Attn_ blocks form the deep attention stage, generating the final attention map step by step. To conveniently introduce this process, we further summarize the _Dic-Attn_ form as follows

\[(},()), \]

where \(()\) denotes non-linear scaling function. \(}\) is the learned dictionary with re-weighted atoms. \(()\) can be treated as a response prediction model . We assume that the input of \((l+1)\)-layer attention block is **Attention Output\({}_{l}+_{l}\)**, i.e., the output of the previous layer's attention block. Learning-based dictionary \(\) of each _Dic-Attn_ module groups significant features of different objectsin the input, and the most salient objects are preserved while those irrelevant are suppressed by further selecting sparse representations. The output of each layer has a common intersection as the input \(_{1}\) of the first layer, which is a task-independent bias for dictionary learning in _Dic-Attn_ modules of any depth. Moreover, the output can be further considered as consisting of two components, one of which is the \(l\)-layer module's attention area, and the other component represents the ignored part. The first component will affect deep attention modules layer by layer, and adjust the task-driven selection matrix through backpropagation. Hence, our proposed _Dic-Attn_ module of shallow depth can bring selective biases with a task-independent impact on deeper attention modules.

The proposed _Dic-Attn_ method, based on feature decomposition and sparse prior, offers a white-box novel strategy for obtaining visual attention, which may be able to be applied to scenarios with more stringent security requirements. Generally, in the visual perceptual system, selective attention is primarily driven by strong stimulus signals and attention to specific regions. Selective attention can be understood from three main aspects: bottom-up voluntary attention, involuntary attention, and "selection history" attention. Involuntary attention, similar to the task-driven model in computer vision, is obtained through top-down rewards. "selection history" refers to the influence of previously attended regions or objects on future attention allocation. Meanwhile, the selective lingering biases have a task-independent impact on the subsequent attention priority. In biology, it means that the nerve has better plasticity. While in the deep neural network, the structure of the multi-layer attention module is not just capable to acquire features with higher semantics. Shallow-depth attention modules capture data prototype features and affect deeper attention modules. It will make models with greater capacity, better generalization, and robustness. The multi-layer attention module structure in the _Dic-Attn_ module goes beyond acquiring features with higher semantics. Shallow-depth attention modules capture data prototype features and influence deeper attention modules, resulting in models with increased capacity, better generalization, and improved robustness.

In summary, we demonstrate the effectiveness of the _Dic-Attn_ module in capturing visual attention, enhancing visual model performance. We also provide the interpretability for deeper insights into the attention mechanism.

## 4 Experiments

In this section, we evaluate the performance of proposed _Dic-Attn_ on 6 benchmarks. Firstly, we conduct ablation experiments on CIFAR-10 dataset, as described in Sec. 4.1. Subsequently, we compare the computational costs. In Sec. 4.2, we evaluate the performance of the _Dic-Attn_ module across various visual tasks, such as point cloud classification, image segmentation, and image classification. We compare the accuracy and robustness of the proposed method with several notable attention modules. More experiments and the code are available in the Supplementary Material.

### Ablation Study and Computational Costs

We explore the influence of the hyper-parameters, i.e., the second dimension \(k\) of the dictionary and the number \(l\) of attention blocks (depth of encoder). Fig. 2 illustrates the significance of selecting appropriate values for \(l\) and \(k\). A dictionary with higher dimension \(k\) creates a comprehensive feature space for images, which enhances the performance of the Vision Transformer. As the depth \(l\) increases, the accuracy initially improves but starts to decline after reaching seven in CIFAR-10. The phenomenon of rising first can be attributed to the fact that deeper networks have the capability to capture more intricate relationships between inputs and outputs. However, they are also prone to

Figure 2: The _Dic-Attn_ module ablation experiments (with ViT backbone) on CIFAR-10 dataset.

overfitting, particularly when the training dataset is relatively small, such as in the case of training from scratch using the CIFAR-10 dataset. Therefore, it is important to carefully consider the trade-offs between depth and complexity when designing a neural network, rather than assuming that deeper is always better.

We evaluate the performance of the _Dic-Attn_ module, and compare it with the SA module and notable variants, as presented in Tab. 1. The results show that the proposed _Dic-Attn_ module is more lightweight and has a lower computational burden. The total number of parameters (Params.) of the SA module is related to \(n\), which is equal to \(3n^{2}\). For our proposed _Dic-Attn_ module, the Params. is counted as \(k(n+k+1)\), where Params. of \(\) is \(nk\), Params. of \(}\) is \(k^{2}\) and Params. of \(W_{}\) is \(k\). Since \(k= n\), it can be inferred that \(k(n+k+1)\) is always less than \(n^{3}\) when \((0,_{1})(_{2},+)\). \(_{1}\) and \(_{2}\) are solutions of the quadratic equation \(n^{2}+(n+1)-n^{2}\) with variable \(\). Note that, it always has real number solutions.

### Computer Vision Tasks

We evaluate the performances of our proposed _Dic-Attn_ module on public image datasets and point cloud datasets, including CIFAR-10, CIFAR-100, ModelNet40 , ScanObjectNN , and ADE20K . Note that, to mitigate the impact of various complex factors that may arise from pre-training and fine-tuning processes, **all models in this paper were trained from scratch, without pre-training, and adopted the same data augmentation method.** Additionally, **unless explicitly stated, the hyperparameter settings remained consistent across all experiments.** More implementation settings are described in detail in the Supplementary Material.

#### 4.2.1 Image Classification

**CIFAR-10** dataset is composed of \(10\) categories \(60000\) color images with the size of \(32 32\). **CIFAR-100** dataset has 100 classes. Both of CIFAR-10 and CIFAR-100 contain \(50000\) training images and \(10000\) test images. The number of images in each category is equal, so \(6000\) images per class in CIFAR-10 and \(600\) in CIFAR-100, respectively.

The classification results on image datasets are shown in Tab. 2. The image classification accuracy of ViT can be improved from \(0.64\%\) to \(2.12\%\) by changing the SA module to _Dic-Attn_ module.

In Tab. 3, we report the model performance on adversarial robustness under the Fast Gradient Sign Method (FGSM)  and Projected Gradient Descent (PGD) . We set the FGSM attack with the perturbation magnitude \(=\). For PGD, we set \(=\), iteration numbers \(t=10\), and step size \(=\). We evaluate the model performance by using Attack Accuracy, including the FGSM accuracy and PGD accuracy on classifying images corrupted by adversarial attack FGSM and PGD, respectively. Note that we do not apply adversarial training here, but only use clean samples to train RVT networks . In this case, the proposed _Dic-Attn_ module is still competitive.

#### 4.2.2 Point Cloud Classification

**ModelNet40** and **ScanObjectNN** datasets are widely used benchmarks for point cloud analysis. ModelNet40 dataset comprises \(12311\) CAD-generated meshes categorized into 40 classes. Among them, \(9843\) meshes are used for training, while the remaining \(2468\) meshes are reserved for testing. ScanObjectNN is a real-world dataset, that contains \(15000\) objects, divided into \(15\) and \(2902\) unique object instances. For all experiments conducted, only the coordinates \(xyz\) of objects are utilized. In the case of ModelNet40, all baselines are tested with \(1024\) input points.

  
**Methods/Indicators** & **Params. (M)** & **MACs. (G)** & **GPU Train / Inference Time (ms)** \\ 
**SA** & 1.00 & 292.00 & 242.00 / 82.20 \\
**DA** & 4.82 & 79.50 & 72.60 / 64.40 \\
**A\({}^{2}\)** & 1.01 & 25.70 & 22.90 / 8.00 \\
**ACF** & 0.75 & 79.5 & 71.00 / 22.60 \\
**HAM** & 0.50 & 17.60 & 15.60 / 7.70 \\
**Dic-Attn** (Ours) & 0.60 & 20.00 & 78.00 / 24.70 \\   

* **Input: tensor with shape \(1 512 128 128\).**

Table 1: Comparison of the efficiency of various attention modules. **Indicators:** Number of parameters (Params.), multiply-accumulates operations (MACs) and training/inference time cost compared to Self-Attention and its variants.

The pre-processing of cloud points is performed according to . Note that the point cloud data is encoded as a fixed-shape feature tensor to input the attention module, which preserves and incorporates the original point clouds' position correlation, etc. The classification results on point cloud datasets are shown in Tab. 4. It is evident that our proposed _Dic-Attn_ module significantly enhances the classification performance of the Point Cloud Transformer (PCT) model . The average point cloud classification results achieved with our proposed module are highly competitive and demonstrate notable improvement over existing approaches. Our proposed _Dic-Attn_ module is able to compute an attention map for the point cloud Transformer. In addition, all these indicate that our proposed _Dic-Attn_ module has the advantage of processing complicated data and has the potential to measure the topological attention relationships of the underlying geometric structure.

#### 4.2.3 Image Segmentation

**ADE20K** dataset  contains a total of \(150\) semantic categories, and can be used for scene perception, parsing, segmentation, multi-object recognition, and semantic understanding, which is one of the most challenging semantic segmentation datasets. The training set contains \(20210\) images. There are about \(2000\) and \(3352\) images in the validation and test set, respectively.

The performance of the _Dic-Attn_ module is verified in the semantic segmentation task with the backbone Segmenter , which is a competitive and representative transformer-based semantic segmentation model. We train Segmenter (SA) and Segmenter (Dic-Attn) from scratch on ADE20K dataset. Note that, Segmenter (Dic-Attn) denotes the proposed _Dic-Attn_ module replacing the original SA module in the Segmenter backbone. The mean Intersection over Union (mIoU) and the mean Pixel Accuracy (mPA) of Segmenter (Dic-Attn) are \(1.00\%\) and \(1.25\%\) higher than that of Segmenter (SA), respectively.

Furthermore, the two image examples and their corresponding segmentation attention map results, as shown in Fig. 3, intuitively indicate that our proposed _Dic-Attn_ module highly promotes the ability of the segmentation model to distinguish objects. The learned attention maps are also more similar to human visual attention, which focus on referred objects, e.g., building, sky, door, cabinet, etc. The learned dictionary in the _Dic-Attn_ module contains more sufficient features, including the underlying nonlinear structure. All results above indicate that by fully extracting attention information in both the channel domain and spatial domain of feature space, the attention maps for different labeled objects can be reconstructed more accurately.

   Model &  \\  (From Scrach) & ModelNet40 & ScanObjectNN \\  PointNet++  & 91.90 & 84.30 \\ PointCNN  & 91.70 & 85.50 \\ PCT & 92.95 & 80.60 \\ Point-BERT  & 93.19 & 88.10 \\ Point-MAE  & 93.80 & 88.30 \\  PCT (Dic-Attn) & 94.60 & 88.96 \\ Point-MAE (Dic-Attn) & 94.46 & 89.41 \\   

Table 4: Point Cloud Classification task _Top1_-Accuracy (%). The number of points is set to \(1024\).

   Model &  &  \\  (From Scrach) & & & \\  (From Scrach) & & & \\  RVT (SA) & 0.20 & 0.10 & 64.80 & 8.29 \\ RVT (VARS-D) & 10.58 & 3.80 & 62.20 & 7.68 \\  RVT (Dic-Attn) & 10.00 & 4.30 & 55.93 & 7.48 \\   

Table 3: Robustness Evaluation Results on CIFAR-100 dataset. Comparison of our approach with backbones and other attention baseline methods, against various adversarial attacks.

Figure 3: The comparison of attention maps generated by SA module and _Dic-Attn_ module (with backbone Segmenter, experiment on ADE20K dataset).

Conclusions

In this work, a novel dictionary learning-based attention module, namely _Dic-Attn_, was proposed. _Dic-Attn_ takes the advantage of dictionary learning mechanism, enabling effective exploration of the underlying nonlinear structure information. By dynamically selecting learned dictionary atoms and sparse representations through spatial and channel transformations, multi-layer _Dic-Attn_ modules achieve the accurate reconstruction of attention maps and allow for the extraction of features at different semantic levels, enhancing the comprehensive representation of visual information. Experimental results in various visual tasks and vision models showed that the _Dic-Attn_ module can seamlessly integrate into various vision networks, delivering competitive performance comparable to state-of-the-art methods. The _Dic-Attn_ module is also a promising solution with competitive performance, compatibility, and computational efficiency. Further research will be devoted to the robustness of the module and its applications in more tasks, e.g., temporal prediction tasks in scientific and engineering fields. Conducting further investigation and validation of the diverse levels of semantics exhibited by dictionaries learned at different depths in natural language processing scenarios would also be valuable.