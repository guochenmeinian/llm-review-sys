# Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion

Taehyun Cho1, Seungyub Han1,3, Heesoo Lee1, Kyungjae Lee2, Jungwoo Lee1*

1 Seoul National University, 2 Chung-Ang University, 3 Hodoo AI Labs

{talium, seungyubhan, algol7240, junglee}@snu.ac.kr

{kyungjae.lee}@ai.cau.ac.kr

###### Abstract

Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.

## 1 Introduction

Distributional reinforcement learning (DistRL) learns the stochasticity of returns in the reinforcement learning environments and has shown remarkable performance in numerous benchmark tasks. DistRL agents model the approximated distribution of returns, where the mean value implies the conventional Q-value [2; 6; 13; 25] and provides more statistical information (e.g., mode, median, variance) for control. Precisely, DistRL aims to capture _intrinsic (aleatoric)_ uncertainty which is an inherent and irreducible randomness in the environment. Such learned uncertainty gives rise to the notion of risk-sensitivity, and several distributional reinforcement learning algorithms distort the learned distribution to create a risk-averse or risk-seeking decision making [8; 12].

Despite the richness of risk-sensitive information from return distribution, only a few DistRL methods [11; 21; 26; 35; 42] have tried to employ its benefits for exploration strategies which is essential in deep RL to find an optimal behavior within a few trials. The main reason is that the exploration strategies so far is based on _parametric (epistemic)_ uncertainty which arise from insufficient or

Figure 1: Illustrative example of why a biased risk criterion (naive optimism) can degrade performance. Suppose two actions have similar expected returns, but different variances (intrinsic uncertainty). (**Left**) If an agent does not specify the risk criterion at the moment, the probability of selecting each action should be similar. **(Right)** As OFU principle encourages to decide uncertain behaviors, the empirical variance from quantiles was used as an estimate of uncertainty. [19; 21; 23]. However, optimistic decision based on empirical variance inevitably leads a risk-seeking behavior, which causes biased action selection.

inaccurate data. In particular, _Optimism in the face of uncertainty_ (OFU) is one of the fundamental exploration principles that employs parametric uncertainty to promote exploring less understood behaviors and to construct confidence set. In bandit or tabular MDP settings, OFU-based algorithms select an action with the highest upper-confidence bound (UCB) of parametric uncertainty which can be considered as the optimistic decision at the moment [5; 10].

However, in deep RL, it is hard to trivially estimate the parametric uncertainty accurately due to the black-box nature of neural networks and high-dimensionality of state-action space. Without further computational task, the estimated variance from distribution is extracted as a mixture of two types of uncertainty, making it difficult to decompose either component. For example, DLTV  was proposed as a distribution-based OFU exploration that decays bonus rate to suppress the effect of intrinsic uncertainty, which unintentionally induces a risk-seeking policy. Although DLTV is the first attempt to introduce OFU in distRL, we found that consistent optimism on the uncertainty of the estimated distribution still leads to biased exploration. We will refer to this side-effect as _one-sided tendency on risk_, where selecting an action based on a fixed risk criterion degrades learning performance. In Section 4, we will demonstrate the one-sided tendency on risk through a toy experiment and show that our proposed randomized approach is effective to avoid this side effect.

In this paper, we introduce _Perturbed Distributional Bellman Optimality Operator (PDBOO)_ to address the issue of biased exploration caused by a one-sided tendency on risk in action selection. We define the distributional perturbation on return distribution to re-evaluate the estimate of return by distorting the learned distribution with perturbation weight. To facilitate deep RL algorihm, we present _Perturbed Quantile Regression (PQR)_ and test in Atari 55 games comparing with other distributional RL algorithms that have been verified for reproducibility by official platforms [4; 29].

In summary, our contributions are as follows.

* A randomized approach called perturbed quantile regression (PQR) is proposed without sacrificing the original (risk-neutral) optimality and improves over naive optimistic strategies.
* A sufficient condition for convergence of the proposed Bellman operator is provided without satisfying the conventional contraction property.

## 2 Backgrounds & Related works

### Distributional RL

We consider a Markov decision process (MDP) which is defined as a tuple \((,,P,R,)\) where \(\) is a finite state space, \(\) is a finite action space, \(P:\) is the transition probability, \(R\) is the random variable of rewards in \([-R_{},R_{}]\), and \([0,1)\) is the discount factor. We define a stochastic policy \((|s)\) which is a conditional distribution over \(\) given state \(s\). For a fixed policy \(\), we denote \(Z^{}(s,a)\) as a random variable of return distribution of state-action pair \((s,a)\) following the policy \(\). We attain \(Z^{}(s,a)=_{t=0}^{}^{t}R(S_{t},A_{t})\), where \(S_{t+1} P(|S_{t},A_{t}),\ A_{t}(|S_{t})\) and \(S_{0}=s,\ A_{0}=a\). Then, we define an action-value function as \(Q^{}(s,a)=[Z^{}(s,a)]\) in \([-V_{},V_{}]\) where \(V_{}=R_{}/(1-)\). For regularity, we further notice that the space of

Figure 2: An illustrative example of proposed algorithm (PQR). Each distribution represents the empirical PDF of return. PQR benefits from excluding inferior actions and promoting unbiased selection with regards to high intrinsic uncertainty through randomized risk criterion.

action-value distributions \(\) has the first moment bounded by \(V_{}\):

\[=\{Z:()|\,[|Z(s,a)|] V_{},(s,a)}\,.\]

In distributional RL, the return distribution for the fixed \(\) can be computed via dynamic programming with the distributional Bellman operator defined as,

\[^{}Z(s,a)}{{=}}R(s,a)+ Z(S^{ },A^{}), S^{} P(|s,a),\ A^{}( |S^{})\]

where \(}{{=}}\) denotes that both random variables share the same probability distribution. We can compute the optimal return distribution by using the distributional Bellman optimality operator defined as,

\[\,Z(s,a)}{{=}}R(s,a)+ Z(S^{},a^{*}), S^{} P(|s,a),\ a^{*}=*{argmax}_{a^{ }}\ _{Z}[Z(S^{},a^{})].\]

Bellemare et al.  have shown that \(^{}\) is a contraction in a maximal form of the Wasserstein metric but \(\) is not a contraction in any metric. Combining with the expectation operator, \(\) is a contraction so that we can guarantee that the expectation of \(Z\) converges to the optimal state-action value. Another notable difference is that the convergence of a return distribution is not generally guaranteed to be unique, unless there is a total ordering \(\) on the set of greedy policies.

### Exploration on Distributional RL

To combine with deep RL, a parametric distribution \(Z_{}\) is used to learn a return distribution. Dabney et al.  have employed a quantile regression to approximate the full distribution by letting \(Z_{}(s,a)=_{i=1}^{N}_{_{i}(s,a)}\) where \(\) represents the locations of a mixture of \(N\) Dirac delta functions. Each \(_{i}\) represents the value where the cumulative probability is \(_{i}=\). By using the quantile representation with the distributional Bellman optimality operator, the problem can be formulated as a minimization problem as,

\[=_{^{}}D(Z_{^{}}(s_{t},a_{t}), \,Z_{^{-}}(s_{t},a_{t}))=_{^{}} _{i,j=1}^{N}_{i}}^{}(r_{t}+_{j}^{-} (s_{t+1},a^{})-_{i}^{}(s_{t},a_{t}))}{N}\]

where \((s_{t},a_{t},r_{t},s_{t+1})\) is a given transition pair, \(_{i}=+_{i}}{2}\), \(a^{}:=*{argmax}_{a^{}}\ _{Z}[Z_{}(s_{t+1},a^{ })]\), \(_{_{i}}^{}(x):=|_{i}-_{\{x<0\}}|_{}(x)\), and \(_{}(x):=x^{2}/2\) for \(|x|\) and \(_{}(x):=(|x|-)\), otherwise.

Based on the quantile regression, Dabney et al.  have proposed a quantile regression deep Q network (QR-DQN) that shows better empirical performance than the categorical approach , since the quantile regression does not restrict the bounds for return.

As deep RL typically did, QR-DQN adjusts \(\)-greedy schedule, which selects the greedy action with probability \(1-\) and otherwise selects random available actions uniformly. The majority of QR-DQN variants [12; 38] rely on the same exploration method. However, such approaches do not put aside inferior actions from the selection list and thus suffers from a loss . Hence, designing a schedule to select a statistically plausible action is crucial for efficient exploration.

In recent studies, Mavrin et al.  modifies the criterion of action selection for efficient exploration based on optimism in the face of uncertainty. Using left truncated variance as a bonus term and decaying ratio \(c_{t}\) to suppress the intrinsic uncertainty, DLTV was proposed as an uncertainty-based exploration in DistRL without using \(\)-greedy schedule. The criterion of DLTV is described as:

\[a^{*}=*{argmax}_{a^{}}(_{P}[Z(s^{},a^{ })]+c_{t}^{2}(s^{},a^{})}),\ \ c_{t}=c},\ _{+}^{2}=_{i= }^{N}(_{}-_{i})^{2},\]

where \(_{i}\)'s are the values of quantile level \(_{i}\).

### Risk in Distributional RL

Instead of an expected value, risk-sensitive RL is to maximize a pre-defined risk measure such as Mean-Variance , Value-at-Risk (VaR) , or Conditional Value-at-Risk (CVaR) [30; 31] and results in different classes of optimal policy. Especially, Dabney et al.  interprets risk measuresas the expected utility function of the return, i.e., \(_{Z}[U(Z(s,a))]\). If the utility function \(U\) is linear, the policy obtained under such risk measure is called _risk-neutral_. If \(U\) is concave or convex, the resulting policy is termed as _risk-averse_ or _risk-seeking_, respectively. In general, a _distortion risk measure_ is a generalized expression of risk measure which is generated from the distortion function.

**Definition 2.1**.: Let \(h:\) be a **distortion function** such that \(h(0)=0,h(1)=1\) and non-decreasing. Given a probability space \((,,)\) and a random variable \(Z:\), a **distortion risk measure**\(_{h}\) corresponding to a distortion function \(h\) is defined by:

\[_{h}(Z)^{h()}[Z]=_{-}^{}z (h F_{Z})(z)dz,\]

where \(F_{Z}\) is the cumulative distribution function of \(Z\).

In fact, non-decreasing property of \(h\) makes it possible to distort the distribution of \(Z\) while satisfying the fundamental property of CDF. Note that the concavity or the convexity of distortion function also implies risk-averse or seeking behavior, respectively. Dhaene et al.  showed that any distorted expectation can be expressed as weighted averages of quantiles. In other words, generating a distortion risk measure is equivalent to choosing a reweighting distribution.

Fortunately, DistRL has a suitable configuration for risk-sensitive decision making by using distortion risk measure. Chow et al.  and Stanko and Macek  considered risk-sensitive RL with a CVaR objective for robust decision making. Dabney et al.  expanded the class of policies on arbitrary distortion risk measures and investigated the effects of a distinct distortion risk measures by changing the sampling distribution for quantile targets \(\). Zhang and Yao  have suggested QUOTA which derives different policies corresponding to different risk levels and considers them as options. Moskovitz et al.  have proposed TOP-TD3, an ensemble technique of distributional critics that balances between optimism and pessimism for continuous control.

## 3 Perturbation in Distributional RL

### Perturbed Distributional Bellman Optimality Operator

To choose statistically plausible actions which may be maximal for certain risk criterion, we will generate a distortion risk measure involved in a pre-defined constraint set, called an _ambiguity set_. The ambiguity set, originated from distributionally robust optimization (DRO) literature, is a family of distribution characterized by a certain statistical distance such as \(\)_-divergence_ or _Wasserstein distance_. In this paper, we will examine the ambiguity set defined by the discrepancy between distortion risk measure and expectation. We say the sampled reweighting distribution \(\) as _(distributional) perturbation_ and define it as follows:

**Definition 3.1**.: (Perturbation Gap, Ambiguity Set) Given a probability space \((,,)\), let \(X:\) be a random variable and \(=:0(w)<,\ _{w}(w)(dw)=1}\) be a set of probability density functions. For a given constraint set \(\), we say \(\) as a **(distributional) perturbation** from \(\) and denote the \(-\)weighted expectation of \(X\) as follows:

\[_{}[X]_{w}X(w)(w)(dw),\]

which can be interpreted as the expectation of \(X\) under some probability measure \(\), where \(=d/d\) is the Radon-Nikodym derivative of \(\) with respect to \(\). We further define \(d(X;)=|[X]-_{}[X]|\) as **perturbation gap** of \(X\) with respect to \(\). Then, for a given constant \( 0\), the **ambiguity set** with the bound \(\) is defined as

\[_{}(X)=:d(X;)}.\]

For brevity, we omit the input \(w\) from a random variable unless confusing. Since \(\) is a probability density function, \(_{}[X]\) is an induced risk measure with respect to a reference measure \(\). Intuitively, \((w)\) can be viewed as a distortion to generate a different probability measure and vary the risk tendency. The aspect of using distortion risk measures looks similar to IQN . However, instead of changing the sampling distribution of quantile level \(\) implicitly, we reweight each quantile from the ambiguity set. This allows us to control the maximum allowable distortion with bound \(\), whereas the risk measure in IQN does not change throughout learning. In Section 3.3, we suggest a practical method to construct the ambiguity set.

Now, we characterize _perturbed distributional Bellman optimality operator_ (PDBOO) \(_{\,}\) for a fixed perturbation \(_{}(Z)\) written as below:

\[_{\,}Z(s,a)R(s,a)+ Z(S^{},a^{*}() ),\ \ \ S^{} P(|s,a),\ a^{*}()=*{ argmax}_{a^{}}\ _{,P}[Z(s^{},a^{})].\]

Notice that \( 1\) corresponds to a base expectation, i.e., \(_{,P}=_{P}\), which recovers the standard distributional Bellman optimality operator \(\). Specifically, PDBOO perturbs the estimated distribution only to select the optimal behavior, while the target is updated with the original (unperturbed) return distribution.

If we consider the time-varying bound of ambiguity set, scheduling \(_{t}\) is a key ingredient to determine whether PDBOO will efficiently explore or converge. Intuitively, if an agent continues to sample the distortion risk measure from a fixed ambiguity set with a constant \(\), there is a possibility of selecting sub-optimal actions after sufficient exploration, which may not guarantee eventual convergence. Hence, scheduling a constraint of ambiguity set properly at each action selection is crucial to guarantee convergence.

Based on the quantile model \(Z_{}\), our work can be summarized into two parts. First, we aim to minimize the expected discrepancy between \(Z_{}\) and \(_{\,}Z_{^{-}}\) where \(\) is sampled from ambiguity set \(_{}\). To clarify notation, we write \(_{}[]\) as a \(-\)weighted expectation and \(_{(_{})}[]\) as an expectation with respect to \(\) which is sampled from \(_{}\). Then, our goal is to minimize the perturbed distributional Bellman objective with sampling procedure \(\):

\[_{^{}}\ _{_{t}(_{ _{t}})}[D(Z_{^{}}(s,a),_{\,_{t}}Z_{^{-} }(s,a))]\] (1)

where we use the Huber quantile loss as a discrepancy on \(Z_{^{}}\) and \(_{\,}Z_{^{-}}\) at timestep \(t\). In typical risk-sensitive RL or distributionally robust RL, the Bellman optimality equation is reformulated for a pre-defined risk measure [8; 33; 39]. In contrast, PDBOO has a significant distinction in that it performs dynamic programming that adheres to the risk-neutral optimal policy while randomizing the risk criterion at every step. By using min-expectation instead of min-max operator, we suggest unbiased exploration that can avoid leading to overly pessimistic policies. Furthermore, considering a sequence \(_{t}\) which converges to 1 in probability, we derive a sufficient condition of \(_{t}\) that the expectation of any composition of the operators \(_{\,_{n:1}}_{\,_{n }}_{\,_{n-1}}_{\,_{1}}\) has the same unique fixed point as the standard. These results are remarkable that we can apply the diverse variations of distributional Bellman operators for learning.

### Convergence of the perturbed distributional Bellman optimality operator

Unlike conventional convergence proofs, PDBOO is time-varying and not a contraction, so it covers a wider class of Bellman operators than before. Since the infinite composition of time-varying Bellman operators does not necessarily converge or have the same unique fixed point, we provide the sufficient condition in this section. We denote the iteration as \(Z^{(n+1)}_{\,_{n+1}}Z^{(n)}\), \(Z^{(0)}=Z\) for each timestep \(n>0\), and the intersection of ambiguity set as \(}_{_{n}}(Z^{(n-1)})_{s,a}_{ _{n}}(Z^{(n-1)}(s,a))\).

**Assumption 3.2**.: Suppose that \(_{n=1}^{}_{n}<\) and \(_{n}\) is uniformly bounded.

**Theorem 3.3**.: _(Weaker Contraction Property) Let \(_{n}\) be sampled from \(}_{_{n}}(Z^{(n-1)})\) for every iteration. If Assumption 3.2 holds, then the expectation of any composition of operators \(_{\,_{n:1}}\) converges, i.e., \(_{\,_{n:1}}[Z][Z^{*}]\). Moreover, the following bound holds,_

\[_{s,a}\ [Z^{(n)}(s,a)]-[Z^{*}(s,a)] _{k=n}^{}(2^{k-1}V_{}+2_{i=1}^{k} ^{i}(_{k+2-i}+_{k+1-i})).\]

Practically, satisfying Assumption 3.2 is not strict to characterize the landscape of scheduling. Theorem 3.3 states that even without satisfying \(\)-contraction property, we can show that \([Z^{*}]\) is the fixed point for the operator \(_{\,_{n:1}}\). However, \([Z^{*}]\) is not yet guaranteed to be "unique" fixed point for any \(Z\). Nevertheless, we can show that \([Z^{*}]\) is, in fact, the solution of the standard Bellman optimality equation, which is already known to have a unique solution.

**Theorem 3.4**.: _If Assumption 3.2 holds, \([Z^{*}]\) is the unique fixed point of Bellman optimality equation for any \(Z\)._

As a result, PDBOO generally achieves the unique fixed point of the standard Bellman operator. Unlike previous distribution-based or risk-sensitive approaches, PDBOO has the theoretical compatibility to obtain a risk-neutral optimal policy even if the risk measure is randomly sampled during training procedure. For proof, see Appendix A.

### Practical Algorithm with Distributional Perturbation

In this section, we propose a **perturbed quantile regression (PQR)** that is a practical algorithm for distributional reinforcement learning. Our quantile model is updated by minimizing the objective function (1) induced by PDBOO. Since we employ a quantile model, sampling a reweight function \(\) can be reduced into sampling an \(N\)-dimensional weight vector \(:=[_{1},,_{N}]\) where \(_{i=1}^{N}_{i}=N\) and \(_{i} 0\) for all \(i\{1,,N\}\). Based on the QR-DQN setup, note that the condition \(_{w}(w)(dw)=1\) turns into \(_{i=1}^{N}_{i}=1\), since the quantile level is set as \(_{i}=\).

A key issue is how to construct an ambiguity set with bound \(_{t}\) and then sample \(\). A natural class of distribution for practical use is the _symmetric Dirichlet distribution_ with concentration \(\), which represents distribution over distributions. (i.e. \(()\).) We sample a random vector, \(()\), and define the reweight distribution as \(:=^{N}+(N-^{N})\). From the construction of \(\), we have \(1-_{i} 1+(N-1)\) for all \(i\) and it follows that \(|1-_{i}|(N-1)\). By controlling \(\), we can bound the deviation of \(_{i}\) from \(1\) and bound the perturbation gap as

\[_{s,a}|[Z(s,a)]-_{}[Z(s,a)]|=_{s, a}|_{w}Z(w;s,a)(1-(w))(dw)|\] \[_{w} 1-(w)_{s,a}[  Z(s,a)]_{w} 1-(w) V_{} (N-1)V_{}.\]

Hence, letting \(}}\) is sufficient to obtain \(d(Z;)\) in the quantile setting. We set \(=0.05^{N}\) to generate a constructive perturbation \(_{n}\) which gap is close to the bound \(_{n}\). For Assumption 3.2, our default schedule is set as \(_{t}=_{0}t^{-(1+)}\) where \(=0.001\).

## 4 Experiments

Our experiments aim to investigate the following questions.

1. Does randomizing risk criterion successfully escape from the biased exploration in stochastic environments?
2. Can PQR accurately estimate a return distribution?
3. Can a perturbation-based exploration perform successfully as a behavior policy for the full Atari benchmark?

### Learning on Stochastic Environments with High Intrinsic Uncertainty

For intuitive comparison between optimism and randomized criterion, we design **p-DLTV**, a perturbed variant of DLTV, where coefficient \(c_{t}\) is multiplied by a normal distribution \((0,1^{2})\). Every experimental setup, pseudocodes, and implementation details can be found in Appendix C.

N-Chain with high intrinsic uncertainty.We extend N-Chain environment  with stochastic reward to evaluate action selection methods. A schematic diagram of the stochastic N-Chain environment is depicted in Figure 3. The reward is only given in the leftmost and rightmost states and the game terminates when one of the reward states is reached. We set the leftmost reward as \((10,0.1^{2})\) and the rightmost reward as \((5,0.1^{2})+(13,0.1^{2})\) which has a lower mean as \(9\) but higher variance. The agent always starts from the middle state \(s_{2}\) and should move toward the leftmost state \(s_{0}\) to achieve the greatest expected return. For each state, the agent can take one of six available actions: left, right, and 4 no-op actions. The optimal policy with respect to mean is to move left twice from the start. We set the discount factor \(=0.9\) and the coefficient \(c=50\).

Despite the simple configuration, the possibility to obtain higher reward in suboptimal state than the optimal state makes it difficult for an agent to determine which policy is optimal until it experiences enough to discern the characteristics of each distribution. Thus, the goal of our toy experiment is to evaluate how rapidly each algorithm could find a risk-neutral optimal policy. The results of varying the size of variance are reported in Appendix D.1.

Analysis of Experimental Results.As we design the mean of each return is intended to be similar, examining the learning behavior of the empirical return distribution for each algorithm can provide fruitful insights. Figure 4 shows the empirical PDF of return distribution by using Gaussian kernel density estimation. In Figure 4(b), DLTV fails to estimate the true optimal return distribution. While the return of \((s_{2},)\) (red line) is correctly estimated toward the ground truth, \((s_{2},)\) (blue line) does not capture the shape and mean due to the lack of experience. At 20K

Figure 4: Empirical return distribution plot in N-Chain environment. The ground truth of each distribution is \(^{2}(10,0.1^{2})\) and \(^{2}[(5,0.1^{2})+(13,0.1^{2 })]\). Each dot represents an indicator for choosing action. Since QR-DQN does not depend on other criterion, the dots are omitted.

Figure 5: Total count of performing true optimal action. The oracle (dashed line) is to perform the optimal action from start to end.

Figure 3: Illustration of the N-Chain environment  with high uncertainty starting from state \(s_{2}\). To emphasize the intrinsic uncertainty, the reward of state \(s_{4}\) was set as a mixture model composed of two Gaussian distributions. Blue arrows indicate the risk-neutral optimal policy in this MDPs.

timestep, the agent begins to see other actions, but the monotonic scheduling already makes the decision like exploitation. Hence, decaying schedule of optimism is not a way to solve the underlying problem. Notably, p-DLTV made a much better estimate than DLTV only by changing from optimism to a randomized scheme. In comparison, PQR estimates the ground truth much better than other baselines with much closer mean and standard-deviation.

Figure 5 shows the number of timesteps when the optimal policy was actually performed to see the interference of biased criterion. Since the optimal policy consists of the same index \(a_{1}\), we plot the total count of performing the optimal action with 10 seeds. From the slope of each line, it is observed that DLTV selects the suboptimal action even if the optimal policy was initially performed. In contrast, p-DLTV avoids getting stuck by randomizing criterion and eventually finds the true optimal policy. The experimental results demonstrate that randomizing the criterion is a simple but effective way for exploration on training process.

Hyperparameter Sensitivity.In Figure 6, we compute the 2-Wasserstein distance from the ground truth return distribution \((10^{2},(0.1^{2})^{2})\). Except for QR-DQN, each initial hyperparameter \(\{c,_{0}\}\) was implemented with grid search on \(\) in 5 different seeds. As the hyperparameter decreases, each agent is likely to behave as exploitation. One interesting aspect is that, while it may be difficult for DLTV and p-DLTV to balance the scale between the return and bonus term, PQR shows robust performance to the initially hyperparameter. This is because the distorted return is bounded by the support of return distribution, so that PQR implicitly tunes the scale of exploration. In practice, we set \(_{0}\) to be sufficiently large. See Table2 in Appendix C.1.

### Full Atari Results

We compare our algorithm to various DistRL baselines, which have demonstrated good performance on RL benchmarks. In Table 1, we evaluated 55 Atari results, averaging over 5 different seeds at 50M frames. We compared with the published score of QR-DQN , IQN , and Rainbow  via the report of DQN-Zoo  and Dopamine  benchmark for reliability. This comparison is particularly noteworthy since our proposed method only applys perturbation-based exploration strategy and outperforms advanced variants of QR-DQN. 2

 
**SQN Performance** & **Mean** & **Median** & \(>\)**human** & \(>\)**DQN** \\ 
**DQN-zoo (no-ops)** & \(314\%\) & \(55\%\) & 18 & 0 \\
**DQN-dopamine (sticky)** & \(401\%\) & \(51\%\) & 15 & 0 \\ 
**QR-DQN-zoo (no-ops)** & \(559\%\) & \(118\%\) & 29 & 47 \\
**QR-DQN-dopamine (sticky)** & \(562\%\) & \(93\%\) & 27 & 46 \\ 
**IQN-zoo (no-ops)** & \(902\%\) & \(131\%\) & 21 & 50 \\
**IQN-dopamine (sticky)** & \(940\%\) & \(124\%\) & 32 & 51 \\
**RAINBOW-zoo (no-ops)** & \(1160\%\) & \(154\%\) & 37 & 52 \\
**RAINBOW-dopamine (sticky)** & \(965\%\) & \(123\%\) & 35 & 53 \\
**PRQ-zoo (no-ops)** & \(112\%\) & \(124\%\) & 33 & 53 \\
**PRQ-dopamine (sticky)** & \(962\%\) & \(123\%\) & 35 & 51 \\  

Table 1: Mean and median of best scores across 55 Atari games, measured as percentages of human baseline. Reference values are from Quan and Ostrovski  and Castro et al. .

Figure 6: 2-Wasserstein distance between the empirical return distribution and the ground truth \((8.1,0.081^{2})\). We use QR-DQN with a fixed setting of \(\)-greedy as a reference baseline, because the hyperparameter of \(\)-greedy is not related to the scale of Q-values.

No-ops Protocol.First, we follow the evaluation protocol of [1; 22] on full set of Atari games implemented in OpenAI's Gym . Even if it is well known that the _no-ops_ protocol does not provide enough stochasticity to avoid memorization, intrinsic uncertainty still exists due to the _random frame skipping_. While PQR cannot enjoy the environmental stochasticity by the deterministic dynamics of Atari games, PQR achieved 562% performance gain in the mean of human-normalized score over QR-DQN, which is comparable results to Rainbow. From the raw scores of 55 games, PQR wins 39 games against QR-DQN and 34 games against IQN.

Sticky actions protocol.To prevent the deterministic dynamics of Atari games, Machado et al.  proposes injecting stochasticity scheme, called _sticky actions_, by forcing to repeat the previous action with probability \(p=0.25\). Sticky actions protocol prevents agents from relying on memorization and allows robust evaluation. In Figure 7, PQR shows steeper learning curves, even without any support of advanced schemes, such as \(n\)-step updates for Rainbow or IQN. In particular, PQR dramatically improves over IQN and Rainbow in Assault, Battlezone, Beamrider, Berzerk and Bowling. In Table 1, PQR shows robust median score against the injected stochasticity.

It should be noted that IQN benefits from the generalized form of distributional outputs, which reduces the approximation error from the number of quantiles output. Compare to IQN, PQR does not rely on prior distortion risk measure such as CVaR , Wang  or CPW , but instead randomly samples the risk measure and evaluates it with a risk-neutral criterion. Another notable difference is that PQR shows the better or competitive performance solely through its **exploration strategies**, compared to \(\)-greedy baselines, such as QR-DQN, IQN, and especially Rainbow. Note that Rainbow enjoys a combination of several orthogonal improvements such as double Q-learning, prioritized replay, dueling networks, and \(n\)-step updates.

## 5 Related Works

Randomized or perturbation-based exploration has been focused due to its strong empirical performance and simplicity. In tabular RL, Osband et al.  proposed randomized least-squares value iteration (RLSVI) using random perturbations for statistically and computationally efficient exploration. Ishfaq et al.  leveraged the idea into optimistic reward sampling by perturbing rewards and regularizers. However, existing perturbation-based methods requires tuning of the hyperparameter for the variance of injected Gaussian noise and depend on well-crafted feature vectors in advance. On the other hand, PDBOO does not rely on the scale of rewards or uncertainties due to the built-in scaling mechanism of risk measures. Additionally, we successfully extend PQR to deep RL scenarios in distributional lens, where feature vectors are not provided, but learned during training.

## 6 Conclusions

In this paper, we proposed a general framework of perturbation in distributional RL which is based on the characteristics of a return distribution. Without resorting to a pre-defined risk criterion,

Figure 7: Evaluation curves on 8 Atari games with 3 random seeds for 50 million frames following _sticky actions_ protocol . Reference values are from Castro et al. .

we revealed and resolved the underlying problem where one-sided tendency on risk can lead to biased action selection under the stochastic environment. To our best knowledge, this paper is the first attempt to integrate risk-sensitivity and exploration by using time-varying Bellman objective with theoretical analysis. In order to validate the effectiveness of PQR, we evaluate on various environments including 55 Atari games with several distributional RL baselines. Without separating the two uncertainties, the results show that perturbing the risk criterion is an effective approach to resolve the biased exploration. We believe that PQR can be combined with other distributional RL or risk-sensitive algorithms as a perturbation-based exploration method without sacrificing their original objectives.

## 7 Acknowledgements

This work was supported in part by National Research Foundation of Korea (NRF, 2021R1A2C2014504(20%) and 2021M3F3A2A02037893(20%)), in part by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Ministry of Science and ICT (MSIT) (2021-0-00106(15%), 2021-0-02068(15%), 2021-0-01080(15%), and 2021-0-01341(15%)), and in part by AI Graduate School Program, CAU, INMAC and BK21 FOUR program.