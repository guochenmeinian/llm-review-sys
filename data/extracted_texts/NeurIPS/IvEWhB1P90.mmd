# It begins with a boundary: A geometric view on probabilistically robust learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Although deep neural networks have achieved super-human performance on many classification tasks, they often exhibit a worrying lack of robustness towards adversarially generated examples. Thus, considerable effort has been invested into reformulating Empirical Risk Minimization (ERM) into an adversarially robust framework. Recently, attention has shifted towards approaches which interpolate between the robustness offered by adversarial training and the higher clean accuracy and faster training times of ERM. In this paper, we take a fresh and geometric view on one such method--Probabilistically Robust Learning (PRL) (Robey et al., 2022). We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. We prove existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced perimeters.

## 1 Introduction

The fragility of DNN-based classifiers in the face of adversarial examples (Goodfellow et al., 2014; Chen et al., 2017; Qin et al., 2019; Cai et al., 2021) and distributional shifts (Quinonero Candela et al., 2008; Hendrycks et al., 2021) is by now nearly as familiar as their successes. In light of this, a multitude of works (see Section 1.4) propose replacing standard Empirical Risk Minimization (ERM) (Vapnik, 1999) with a more robust alternative (see, e.g., Madry et al. (2017)). Unfortunately there is no free lunch: robust classifiers frequently exhibit degraded performance on clean data and significantly longer training times (Tsipras et al., 2018). Consequently, identifying frameworks which balance performance and robustness is of pressing interest to the Machine Learning (ML) community, and over the past several years many such frameworks have been proposed (Zhang et al., 2019; Wang et al., 2020; Robey et al., 2022). Moreover, it is crucial that the mechanism by which such frameworks balance these competing aims be understood.

Beginning with the Probabilistically Robust Learning (PRL) of Robey et al. (2022) we analyze such frameworks geometrically. This perspective reveals a subtle, paradoxical aspect of PRL: sometimes the adversary modeled by this framework corrects, instead of exploits, the learner! Fortunately, the geometric perspective we propose suggests a natural remedy which leads to an interpretation of the corrected PRL as regularized ERM where a certain nonlocal notion of length (or perimeter) of the decision boundary acts as a regularizer. We exemplify this correction in Figure 1. The interpretation of PRL as perimeter-regularized ERM leads us to further generalizations, and we provide a novel view of the Conditional Value at Risk (CVaR) relaxation of PRL proposed by Robey et al. (2022).

### From empirical risk minimization to robustness

Given an input space \(\), an output space \(\), a probability measure \(()\), a loss function \(:\), and a hypothesis class \(\), the standard risk minimization problem is

\[_{h}_{(x,y)}[(h(x),y)].\] (1)

For training classifiers which are robust against adversarial attacks Goodfellow et al. (2014); Madry et al. (2017) suggested adversarial training:

\[_{h}_{(x,y)}[_{x^{} B_{ }(x)}(h(x^{}),y)].\] (2)

Here \(\) is assumed to have the structure of a metric space and \(B_{}(x)\) for \( 0\) denotes the (open or closed) ball of radius \(\) around \(x\).

The recent work by Robey et al. (2022) offered an alternative to adversarial training in order to reduce the (in general) large trade-off between accuracy and robustness inherent in (2), see Tsipras et al. (2018); Robey et al. (2022) for discussion. Instead of requiring classifiers to be robust to _all_ available attacks around a point \(x\)--as enforced through the supremum in (2)--one may consider a less stringent notion of robustness, only requiring classifiers to be robust to \(100(1-p)\%\) of possible attacks when attacks are drawn from a certain distribution \(_{x}\) centered at \(x\). For this, the authors introduced the so-called \(p\)-\(*{ess\,sup}\) operator for \(p[0,1)\) and suggested replacing (2) by

\[_{h}_{(x,y)}[p*{ ess\,sup}_{x^{}_{x}}(h(x^{}),y)],\] (3)

where \(\{_{x}\}_{x}\) is a family of probability distributions. The prototypical example to keep in mind for \(=^{d}\) is the uniform distribution over the \(\)-ball around \(x\), i.e., \(_{x}:=(B_{}(x))\), which is particularly relevant when dealing with adversarial attacks on image classifiers.

For a probability distribution \(\) and a function \(f\), the quantity \(p\)-\(*{ess\,sup}_{x^{}}f(x^{})\) is defined as the smallest value \(t\) such that the probability of a randomly chosen point \(x^{}\) satisfying \(f(x^{})>t\) is smaller than \(p\), which reduces to the usual essential supremum of \(f\) with respect to \(\) if \(p=0\):

\[p*{ess\,sup}_{x^{}}f(x^{}): =\{t\,:\,_{x^{}}[f (x^{})>t] p\}.\]

To better understand the model (3) we temporarily restrict our attention to binary classification (i.e., \(=\{0,1\}\)) using indicator functions of admissible sets (i.e., \(:=\{_{A}\,:\,A\}\)). Note that we identify the two expressions \(_{A}(x)=_{x A}\). We focus on the \(0\)-\(1\) loss \((,y)=_{ y}\) which equals one if \(y\) and zero otherwise. In this scenario (1) reduces to the geometric problem

\[_{A}}(A):=_{(x,y) }[y_{x A^{c}}+(1-y)_{x A}]},\] (4)

Figure 1: Penalization effect of the original model (Robey et al., 2022) (**left**) and ours (**right**): The solid black is the decision boundary of a non-robust classifier induced by the set \(A\). Both models penalize the numbers of green points in the yellow region and red points in the teal region. However, the original model _favors non-robust regions_ of \(A\) for which most perturbations correct the class. Our model identifies this region as non-robust and penalizes it accordingly.

and minimizers are called Bayes classifiers. Similarly, adversarial training (2) can be rewritten as

\[_{A}_{}(A):=_{(x,y)}[y_{x(A^{})^{}}+ (1-y)_{x A^{ x}}]},\] (5)

where for a set \(A\) its fattening by \(\)-balls is defined as \(A^{}:=_{x A}B_{}(x)\). Hence (5) enforces that all points with distance at most \(\) to the decision boundary be adversarially robust.

On the other hand the PRL model (3) reduces to

\[_{A}_{}(A):= _{(x,y)}[y_{_{x^{}_{x}} [x^{} A^{}]>p}+(1-y)_{_{x^{} _{x}}[x^{} A]>p}]},\] (6)

where \(A^{}\) is replaced by a "probabilistic fattening", i.e., one considers the set of all \(x\) for which the probability that a neighboring point sampled from \(_{x}\) lies inside \(A\) is larger than \(p\). To the best of our knowledge, existence of solutions for (6) or even (3) has not been proved so far.

### Geometric modification of probabilistically robust learning

To motivate our geometric modification of the PRL model from Robey et al. (2022), it is insightful to investigate the regularization effect that PRL has compared to standard risk minimization. We let \(_{i}():=(\{i\})\) denote the non-normalized conditional distributions of the points with label \(i\). Subtracting the standard risk in (4) from the one in (6) and disintegrating using \(_{0}\) and \(_{1}\) we obtain

\[&_{}(A)- _{}(A)\\ &=_{}_{_{x^{}_{x}} [x^{} A]>p}-_{x A}\!_{0}(x)+_{ }_{_{x^{}_{x}}[x^{} A^{ }]>p}-_{x A^{}}\!_{1}( x).\] (7)

We highlight that this expression _does not constitute a non-negative functional of \(A\)_. Hence the loss function in (6) is not a regularized version of the standard risk (4) and in fact can be strictly smaller. This observation reveals a subtle flaw in the approach of Robey et al. (2022): Points which lie in thin or spike-like regions of \(A\) penetrating the other class and that are more likely to have the label zero than the label one (meaning they lie in the set \(\{_{0}>_{1}\}\)) yield negative contributions in (7) and are hence _favored_. Such a scenario is visualized on the left side of Figure 1. From an adversarial perspective this means that points which are already misclassified are attacked nevertheless, which can lead to the bizarre situation that the adversary helps the learner by putting these points in the correct class with high probability, thereby reducing both adversarial robustness and clean accuracy.

We fix this by designing a probabilistically robust risk as non-negative regularization of the standard risk. For this we define probabilistic perimeter functionals which only penalize points which are classified correctly _and_ admit a large portion of attacks around them, see the right side of Figure 1.

### Our contributions

Our main contributions are the following:

* We address the geometric limitation of the model by Robey et al. (2022) by introducing a family of perimeter regularizations.
* We prove existence of soft and hard binary classifiers under weak conditions on the family of perimeters and hypothesis classes, using novel relaxation techniques.
* We investigate the relationship between the introduced family of perimeters and local perimeters in Euclidean space for small adversarial budgets.
* We extend our models to encompass general loss functions and hypothesis classes. Our numerical experiments demonstrate that our geometric correction can enhance the adversarial robustness of probabilistically robust classifiers without compromising clean accuracy.

### Related work

Adversarial training was developed by Goodfellow et al. (2014); Madry et al. (2017) as an approach to train networks that are less sensitive to adversarial attacks. Shafahi et al. (2019) reduced its computational complexity by reusing gradients from the backpropagation when training neural networks. Wong et al. (2020) showed that training with noise perturbations followed by a single signed gradient ascent (FGSM) step can be on par with adversarial training while being much cheaper. This approach was picked up and improved upon by Andriushchenko and Flammarion (2020) based on gradient alignment. Different authors also investigated test-time robustification of pretrained classifiers using randomized smoothing (Cohen et al., 2019) or geometric / gradient-based approaches (Schwinn et al., 2021, 2022). While some of the previous models use a combination of random perturbations and gradient-based adversarial attacks to robustify classifiers, Robey et al. (2022) proposed probabilistically robust learning, which is entirely based on random perturbations. PRL aims to interpolate between clean and adversarial accuracy and enjoys the favorable sample complexity of vanilla empirical risk minimization; see also Raman et al. (2023) for more insights on this issue. Connections between adversarial training and local perimeter regularization of decision boundaries were explored by Garcia Trillos and Murray (2022) and then rigorously tied by Bungert and Stinson (2022). Our work is in line with a series of papers (Pydi and Jog, 2021; Awasthi et al., 2021; Brendel and Niles-Weed, 2022; Frank, 2022; Bungert et al., 2023; Garcia Trillos et al., 2023) that explore the existence of solutions to adversarial training problems in different settings. These existence proofs involve dealing with different kinds of measurability issues, depending on whether open or closed balls \(B_{}(x)\) are used in the attack model. For open balls one can work with the Borel \(\)-algebra \(=()\)(Bungert et al., 2023), whereas closed balls require the use of the universal \(\)-algebra to make sure that \(A^{}\) is measurable (Pydi and Jog, 2021; Awasthi et al., 2021; Brendel and Niles-Weed, 2022). Recently, these results were improved by Garcia Trillos et al. (2023) who also proved for the case of multi-class classification that even for the closed ball model Borel measurable classifiers (albeit not necessarily indicator functions of measurable sets) exist and that for all but countably many values of the adversarial budget \(>0\) the open and the closed ball models have the same minimal value.

## 2 Geometry and existence of probabilistically robust classifiers

### The binary classification setting with \(0\)-\(1\) loss

In this section we shall introduce our baseline model, which is based on a suitable geometric regularization of the standard risk. Later we shall embed it into a family of models. For clarity we first discuss hard classifiers (characteristic functions of sets) and then soft classifiers (functions with values in \(\)). The generalization to general models and loss functions is postponed to Section 3.

We start by defining the _probabilistic perimeter_ for \(p[0,1)\) of an admissible set \(A\) as follows:

\[(A):=_{0}(\{& x A^{c}\,:\,_{x^{}_{x}}[x^{} A ]>p\})\\ &+_{1}(\{x A\,:\,_{x^{} _{x}}[x^{} A^{c}]>p\}).\] (8)

\((A)\) penalizes correctly classified points \(x\) for which more than \(100 p\,\%\) of their neighbors, sampled from \(_{x}\), constitute an attack. The perimeter can be rewritten in integral form:

\[(A)&=_{ }_{x A\,\,_{x^{}_{x} }[x^{} A]>p}-_{x A}\,_{0}(x)\\ &+_{}_{x A^{c}\,\,_{x^{}_{x}}[x^{}  A^{c}]>p}-_{x A^{c}}\,_{1}(x)\\ &=_{}_{x A^{c}}_{_ {x^{}_{x}}[x^{} A]>p}\,_ {0}(x)+_{}_{x A}_{_{x^{} _{x}}[x^{} A^{c}]>p}\,_{1}(x). \] (9)

The first reformulation (9) should be compared to (7), while the one in (10) will be useful later on. The use of the term perimeter to describe the functional \(\) will become more apparent shortly in Section 2.4, and at this point it is worth highlighting that \(\) is always a non-negative quantity. This motivates introducing the following regularized risk

\[(A):=_{}(A)+(A), A.\] (11)

Our first theorem states that \(\) equals the expected maximum of the sample-wise standard risk and the probabilistically robust risk from Robey et al. (2022), cf. (4) and (6).

**Theorem 1**.: _For all \(A\) it holds that_

\[(A)=_{(x,y)}[\{_{_{x^{}_{x}}[_{A}(x^{})  y]>p},_{_{A}(x) y}\}].\] (12)The interpretation of the statement of this theorem in the light of Figure 1 is clear: Only if a point \(x\) is correctly classified--meaning \(_{_{A}(x) y}=0\)--the probabilistically robust regularization kicks in through the first term in the maximum. Points which are incorrectly classified will always be penalized even if most attacks correct the label, i.e., if \(_{^{x}_{x}}[_{A}(x^{}) ]>p=0\). Thus, minimizing \(\) instead of \(_{}\) corrects the pathology identified in Section 1.2.

### Extensions in the binary classification setting

Given the formula of \(\) in (10), several natural extensions suggest themselves. E.g., one may replace the indicator function \(_{t>p}\) with a different function \((t)\) to define other notions of _perimeter_

\[_{}(A):=_{}_{x A^{c}}(_{x^{}_{x}} [x^{} A])\,_{0}(x)\\ +_{}_{x A}(_{x^{ }_{x}}[x^{} A^{c}])\, _{1}(x)\] (13)

as well as their corresponding probabilistically robust losses

\[_{}(A):=_{}(A)+_{}(A).\] (14)

For \((t):=_{t>p}\) the perimeter \(_{}\) reduces to \(\) and so do the associated risks. Of particular interest is \(_{p}(t):=\{t/p,1\}\)--the smallest concave function that lies above \((t)=_{t>p}\)--which will allow us to develop deep connections between the theoretical and computational aspects of probabilistically robust learning. Our relaxation using the function \(\) is very similar to the one by Raman et al. (2023) who proved PAC learnability if \(\) is Lipschitz, see Appendix A.6 for more details. In order to rigorously study \(_{}\) we first make our setting precise.

**Assumption 1**.: We let \(\) be a set and \( 2^{}\) be a \(\)-algebra. We assume that:

* \((, 2^{\{0,1\}},)\) is a probability space;
* \((,,)\) is a probability space, where we define \(():=(\{0,1\})\);
* \(\{_{x}\}_{x}\) is a family such that \((,,_{x})\) is a probability space for \(\)-almost every \(x\).

The following theorem establishes existence of minimizers of the risk \(_{}\) for concave and non-decreasing functions \(\). This existence result is astonishing since the standard method of calculus of variations is not directly applicable, with the reason being that problem (15) does not provide enough compactness for lower semicontinuity of the perimeter functional \(_{}\). Instead, the proof is based on convex relaxations to soft classifiers where we use a lower semicontinuous surrogate functional and a total variation defined through a coarea formula which--if \(\) is concave and non-decreasing--lower-bounds the surrogate.

**Theorem 2**.: _Suppose \(:\) is concave and non-decreasing, and that Assumption 1 holds. Then, there exists a solution to the problem_

\[_{A}_{}(A).\] (15)

Furthermore, \(_{}\) can also be interpreted as a sample-wise maximum, analogous to Theorem 1.

**Theorem 3**.: _For all \(A\) and measurable \(:\) it holds_

\[_{}(A)&= _{}(A)+_{}(A)\\ &=_{(x,y)}[\{(_ {x^{}_{x}}[_{A}(x^{}) y] ),_{_{A}(x) y}\}].\]

Note that for the non-concave function \((t)=_{t>p}\) an existence proof along the lines of Theorem 2 is not available since certain relaxation techniques therein rely on concavity of \(\). However, in the next section we shall provide an existence theorem for soft classifiers which is valid for very general functions \(\), including \((t)=_{t>p}\).

### Extension to soft classifiers

Another natural extension features "soft classifiers" instead of indicator functions of admissible sets. Such classifiers are particularly relevant since they include the neural network based models with Softmax activation in the last layer which are used in practice. We start by defining a suitable regularization functional for soft classifiers. Given a \(\)-measurable function \(u:\) we define

\[J_{}(u):=_{}(1-u(x))( _{x^{}_{x}}[u(x^{})])\, _{0}(x)\] (16) \[+_{}u(x)(_{x^ {}_{x}}[1-u(x^{})])\, _{1}(x)\]

which satisfies \(J_{}(_{A})=_{}(A)\) for every choice of \(\). Hence, it is a natural generalization of the perimeter to soft classifiers and one could call \(J_{}\) a total variation. However, it is neither positively homogeneous nor convex so this name would be misleading. Instead, for the proof of Theorem 2 we shall construct a suitable total variation functional \(_{}\) which upper-bounds \(J_{}\).

The next theorem asserts existence of soft classifiers for the regularized risk minimization using \(J_{}\) for very general functions \(\) and hypothesis classes \(\), requiring only that \(\) be lower semicontinuous. For example, every continuous function and also \((t)=_{t>p}\) for \(p\) satisfies this. The existence theorem is valid for all hypotheses classes which are closed in a suitable sense.

**Theorem 4**.: _Under Assumption 1, for every lower semicontinuous function \(:\), and whenever \(\) is a weak-* closed hypothesis class of \(\)-measurable functions \(u:\) in the sense of Definition 1 in the appendix, there exists a solution to the problem_

\[_{u}_{(x,y)}[|u(x)-y|] +J_{}(u).\]

**Example 1**.: Let us consider three interesting hypothesis classes of weak-* closed classifiers for which Theorem 4 applies. More detailed explanations are given in Appendix A.8.

1. The simplest such class \(\) is the class of _all_\(\)-measurable soft classifiers \(u:\) which could be referred to as _agnostic_ classifiers since they are not parametrized.
2. An example with more practical relevance is the class of (feedforward or residual) neural networks defined on the unit cube \(:=[-1,1]^{d}\) with uniformly bounded parameters \[:=_{L}_{1}:[-1,1]^{d} \,: _{l}()=A_{l}+_{l}(W_{l}+b_{l}),\] \[\|(A_{l},W_{l},b_{l})\| C\; l\{1,,L\}},\] where we assume that the activations \(_{l}:\) are continuous. Note that the boundedness of the weights cannot be relaxed. To see this, consider the (very simplistic) neural network \(u_{n}(x)=(w_{n}x)\) for \(x[-1,1]\) and \(w_{n}\). For \(w_{n}\) it is easy to see that \(u_{n}\) converges to \(u(x):=(x)\) which does not lie in the same hypothesis class.
3. Finally, one can also consider the class of hard linear classifiers on \(^{d}\). Letting \((t):=_{t>0}\) denote the Heaviside function, this class is given by \[:=(w x+b)\,:\,w^{d},\;|w|=1,\;b[- ,]},\] where one interprets \(u(x):=(w x+b)\) as \(u 1\) if \(b=\) and \(u 0\) if \(b=-\). If the distributions \(_{0}\), \(_{1}\), and \(_{x}\) are sufficiently nice, then \(\) has the desired closedness property.

### Properties and asymptotics of \(_{}\)

In this section we shall discuss the interpretation of the functional \(_{}\) defined in (13) as a _perimeter_. We do this in two ways.

First, we focus on the case where \(\) is concave and non-decreasing and prove that \(_{}\) is a _submodular functional_. If, in addition, \(\) is assumed to satisfy \((0)=0\), then \(_{}()=_{}()=0\). Following Chambolle et al. (2015), for \(\) satisfying these properties one can interpret \(_{}\) as a generalized perimeter, i.e., a functional that can be used to measure the "size" of the boundary of a set. In Appendix A.3 we introduce \(_{}\)'s induced (generalized) total variation and use it in the proof of Theorem 2; note that, as discussed by Bungert et al. (2023), the adversarial problem (5) also induces a generalized perimeter with associated total variation.

**Theorem 5**.: _If \((0)=0\), then \(_{}()=_{}( )=0\). If \(\) is concave and non-decreasing, then the functional \(_{}\) is submodular, meaning that_

\[_{}(A B)+_{}(A B) _{}(A)+_{}(B)  A,B.\]

**Example 2**.: For \((t)=t\) our perimeter reduces to the perimeter on the _random walk space_\((,)\), introduced by Mazon et al. (2020): \(_{}(A)=_{ A}\, _{x}\,_{0}(x)+_{A}_{ A} \,_{x}\,_{1}(x)\).

Second, we consider more general \(\) and show that \(_{}\) is related to a standard _local_ perimeter when the adversarial budget approaches zero; for the case of adversarial training such a connection was proved by Bungert and Stinson (2022) where the authors utilized the notion of Gamma-convergence of functionals. We take a first step in this direction by proving that for sufficiently smooth sets the probabilistic perimeter converges to a local one if the family of probability distributions \(_{x}\) localizes suitably. For example, one could think of \(_{x}:=(B_{}(x))\), which converges to a point mass at \(x\) if \( 0\). To make our setting precise, we pose the following general assumption:

**Assumption 2**.: We assume that \(=^{d}\), \((0)=0\), \(\) is measurable and bounded, and \(_{1},_{0}\) have continuous densities with respect to the Lebesgue measure which we shall also denote as \(_{1},_{0}\). Furthermore, we assume that there is \(>0\) and a measurable function \(K:^{d}[0,)\) such that for every \(x^{d}\) we have the representation

\[_{x}(x^{})=^{-d}K(x,-x}{})\,x^{}.\]

We also assume that for every \(x\) we have \(K(x,) L^{1}(^{d})\), \(_{^{d}}K(x,z)\,z=1\), and \(K(x,z)=0\) if \(|z|>1\), and that for every \(z^{d}\) the mapping \(x K(x,z)\) is \(C^{1}\).

**Proposition 1**.: _Under Assumption 2, if \(A\) has a compact \(C^{1,1}\) boundary and either \(\) is continuous or there exists a constant \(c>0\) such that \(K(x,z) c\) for all \(x\) and \(|z| 1\), then_

\[_{ 0}_{}(A)= _{ A}_{0,}[x,n(x)]_{0}(x)+_{1,} [x,n(x)]_{1}(x)\,^{d-1}(x)\] (17)

_where we let \(n(x)\) denote the normal to \( A\) at a point \(x A\), and for any vector \(v^{d}\) we define_

\[_{}^{0}[x,v]:=_{0}^{1}(_{\{z v -t\}}K(x,z)\,z)\,t,_{}^{1} [x,v]:=_{0}^{1}(_{\{z v t\}}K(x, z)\,z)\,t.\]

**Remark 1**.: If \(K\) is radially symmetric and independent of \(x\), then \(_{}^{0}=_{}^{1}=:_{}\) is just a constant. E.g., for \(K(x,z):=|B_{1}(0)|^{-1}\,_{|z| 1}\) and \((t)=_{t>p}\) it is trivial that for \(p=0\) we have \(_{}=1\). However, for \(p\) one easily sees \(_{}=0\), hence the limiting perimeter equals zero and there is no regularization effect. Using the function \((t)=\{t/p,1\}\) corrects this degeneracy.

Notably, for radially symmetric \(K\) the limiting perimeter in (17) coincides, provided \(_{}>0\), with the one derived for adversarial training (problem (5)) by Bungert and Stinson (2022), although they considered more general (potentially discontinuous) densities \(_{i}\). In particular, our result indicates that for very small adversarial budgets the regularization effect of both probabilistically robust learning and adversarial training is dominated by the perimeter in (17). While Proposition 1 already completes half of the proof (namely the limsup inequality) of Gamma-convergence of \(_{}\) to the limiting perimeter, the remaining liminf inequality is beyond the scope of this paper. Proving that the convergence (17) does not only hold for sufficiently smooth sets as assumed in Proposition 1 but even in the sense of Gamma-convergence is an extremely important topic for future work since only Gamma-convergence allows to deduce from the convergence of the perimeters that also the solutions of probabilistically robust learning converge to certain regular Bayes classifiers as \( 0\), see Bungert and Stinson (2022, Section 4.2).

## 3 General models

We now shift our attention to training general hypotheses \(h\) using general loss functions \(:\). Motivated by Theorems 1 and 3 we propose the following probabilistically robust optimization problem:

\[_{h}_{(x,y)}[\{p *{ess\,sup}_{x^{}_{x}}(h(x^{}),y), (h(x),y)\}].\] (18)In the mathematical finance or economics literature the \(p\)-\(\) operator is better known as the value at risk (VaR) of a random variable at level \(p\) and it is notoriously hard to optimize. VaR is closely related to other risk measures like, for instance, the conditional value at risk (CVaR) which is convex and easier to optimize (Robey et al., 2022; Rockafellar et al., 2000). For a function \(f:\) and a probability distribution \(\) the CVaR at level \(p\) is defined as

\[_{p}(f;):=_{}+ _{x^{}_{x}}[(f(x^{})- )_{+}]}{p}.\] (19)

It is easy to see that \(p\)-\(_{x^{}}f(x^{}) _{p}(f;)\). Using CVaR in place of the \(p\)-\(\) operator, a tractable version of (18) is

\[_{h}_{(x,y)}[ {CVaR}_{p}((h(),y);_{x}),(h(x),y)}].\] (20)

We emphasize that, if the loss function \((,)\) is convex in its first argument, then (20) is a convex function of the hypothesis \(h\). Furthermore, \(\) is positively homogeneous and hence also (20) is positively homogeneous in the loss function. So, taking the maximum of the samplewise \(\) and standard risk is meaningful as both terms scale in the same way.

In the binary classification case we can prove the following interesting result that the \(\) relaxation corresponds precisely to using the risk \(\!_{}\) with a special piecewise linear and concave function \(\) for which our theory from Section 2.2 applies. In Appendix A.5 we prove a more general version of the following statement, replacing the \([\,\,]_{+}\) operation in (19) with a Leaky ReLU.

**Theorem 6**.: _Let the function \(_{p}:\) be defined as \(_{p}(t):=\{t/p,1\}\). Then it holds_

\[_{p}(_{_{A}() y}; )=_{p}(_{x^{}} [_{A}(x^{}) y])\]

_and as a consequence for all \(A\):_

\[_{(x,y)}[_{p}(_{_{A}() y};_{x}),_{_{ A}(x) y}}]=\!_{_{p}}(A).\]

An immediate consequence of Theorem 6 is that for binary classification (20) has a solution.

**Corollary 1**.: _Under Assumption 1 and in the setting of Theorem 6 problem (20) has a solution._

In Appendix A.5 we collect a few more observations concerning the \(\), especially focussing on its behavior for \(p>1\). These geometric properties, the homogeneity with respect to the loss function, its potentially favorable sample complexity (see the discussion in Appendix A.6), and its versatility for algorithmic implementation make (20) a notable generalization of the adversarial training problem (2). Notice that when \(p 0\) one formally recovers (2) from (20).

## 4 Numerical results

We build upon the code of Robey et al. (2022). The algorithmic realization of (20) is a straightforward adaptation of their algorithm, which alternatingly minimizes the inner optimization problem that defines \(\) and the outer optimization to find a suitable classifier, see Algorithm 1 in Appendix B. In our experiments, we conduct a comparative analysis between their algorithm (denoted as "Original" in Table 1) and Algorithm 1 in the appendix which is based on (20) (denoted as "Geometric"). We report the clean, and adversarial accuracies (subject to PGD attacks), as well as accuracies on noise-augmented data and quantile accuracies for different values of \(p\) (see (Robey et al., 2022, (6.1)) for the definition) averaged over three runs; see Appendix B.2 for more training details. Our experiments are conducted on MNIST and CIFAR-10 and to ensure a fair comparison we adhere to the hyperparameter settings described by Robey et al. (2022), such that both the original and geometric algorithms utilize the same set of hyperparameters for each specified value of \(p\). The corresponding results for several baseline algorithms including empirical risk minimization and adversarial training can be found in their paper. We perform model selection based on the best clean validation accuracy. The results in Table 1 show that for moderate values of \(p\) our geometric modification induces higher adversarial robustness than the original PRL without loss of clean accuracy (see, in particular, the results for MNIST with \(p=0.1\) and for CIFAR-10 with \(p=0.3\)). In the noise augmented metrics as well as for extreme values of \(p\) close to \(0\) or equal to \(0.5\) both algorithms behave comparably. The latter can be expected from out theoretical results, in particular Proposition 1.

Note that the original or the geometric version of PRL should not be expected to match the adversarial robustness of classifiers trained with PGD attacks (Madry et al., 2017) or other worst-case optimization techniques. Instead, they shine with superior clean accuracies and easier training while maintaining probabilistic and a certain degree of adversarial robustness, as also observed by Robey et al. (2022).

We also remark that our sweep over different values of \(p\) confirms that increasing this parameter interpolates between low and high clean accuracies. However, it should be noted that it does not necessarily result in a direct interpolation between high and low adversarial or probabilistic accuracy, as claimed by Robey et al. (2022). These observations hold true for both the original algorithm and our geometric modification, and despite utilizing their code and hyperparameters, we were unable to reproduce the exact results reported by Robey et al. (2022, Tables 1-4).

## 5 Discussion and Conclusion

In this paper we considered probabilistically robust learning (PRL), originally proposed by Robey et al. (2022). We corrected a subtle but crucial theoretical flaw in the original formulation by introducing a regularization of the standard risk with nonlocal perimeters measuring the susceptibility of the decision boundary towards high-probability adversarial attacks. For binary classification we proved existence of optimal hard classifiers and of very general classes of soft classifiers including neural networks. We also provided an asymptotic expansion for smooth decision boundaries to show that for small adversarial budgets the probabilistic perimeters discussed in the paper induce the same regularization effect as adversarial training. For general (not necessarily binary) problems we showed that the natural loss function to choose is the sample-wise maximum of the standard loss and conditional value at risk (CVaR).

One limitation of PRL is that it does not completely solve the accuracy vs. robustness trade-off, which remains a challenging problem. Furthermore, while the formal limit of PRL as \(p 0\) is the worst-case adversarial problem, the algorithms for solving PRL exhibit limitations for very small values of \(p\) (in the computation of \(_{p}\)). Still, the results for moderately large values of \(p\) are encouraging and future work should focus on understanding of this trade-off better.

The rich mathematical theory developed in this paper opens up new avenues for research, such as the explicit design of probabilistic regularizers for algorithms and exploring the variational convergence of the probabilistic perimeter and its implications for adversarial robustness.

  
**Data** & \(p\) & **Algorithm** & **Clean** & **Adv** & **Aug** & **Aug-0.1** & **Aug-0.05** & **Aug-0.01** \\   &  & Geometric & **99.20** & **12.19** & 99.04 & 98.18 & 97.69 & 96.38 \\  & & Original & 99.19 & 10.76 & 98.90 & 97.94 & 97.38 & 95.67 \\   & &  & Geometric & 99.28 & **14.20** & 99.22 & 98.70 & 98.45 & 97.86 \\  & & Original & **99.32** & 8.94 & 99.22 & 98.70 & 98.46 & 97.80 \\   & &  & Geometric & **99.29** & **3.02** & 99.21 & 98.76 & 98.53 & 97.95 \\  & & Original & 99.27 & **3.02** & 99.22 & 98.77 & 98.55 & 98.01 \\   & &  & Geometric & **99.27** & **1.80** & 99.21 & 98.72 & 98.44 & 97.93 \\  & & Original & 99.26 & 1.68 & 99.19 & 98.72 & 98.47 & 97.80 \\   &  & Geometric & 80.65 & 0.15 & 78.13 & 73.44 & 72.13 & 68.80 \\  & & Original & **81.73** & **0.24** & 79.16 & 74.61 & 73.19 & 69.96 \\   & &  & Geometric & 88.15 & 0.14 & 85.96 & 82.55 & 81.46 & 78.81 \\  & & Original & **88.28** & **0.19** & 85.61 & 82.21 & 81.06 & 78.28 \\   & &  & Geometric & **90.43** & **11.80** & 88.70 & 85.17 & 83.93 & 80.93 \\  & & Original & 89.97 & 7.20 & 88.62 & 85.07 & 83.75 & 80.87 \\   & &  & Geometric & **91.51** & 1.93 & 88.94 & 85.53 & 84.18 & 81.21 \\  & & Original & 90.74 & **1.99** & 88.94 & 85.54 & 84.35 & 81.57 \\   

Table 1: Accuracies [%] of the geometric and original algorithm for different values of \(p\).