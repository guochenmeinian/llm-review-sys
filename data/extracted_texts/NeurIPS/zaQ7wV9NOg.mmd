# Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL

Qinghua Liu

Princeton University

qinghual@princeton.edu &Gellert Weisz

Google DeepMind

gellert@deepmind.com &Andras Gyorgy

Google DeepMind

agyorgy@deepmind.com &Chi Jin

Princeton University

chij@princeton.edu &Csaba Szepesvari

Google DeepMind and University of Alberta

szepesva@ualberta.ca

This work was done when QL interned at DeepMind.

###### Abstract

While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited--they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especially in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework--Optimistic NPG for online RL. Optimistic NPG can be viewed as a simple combination of the classic natural policy gradient (NPG) algorithm  and an optimistic policy evaluation subroutine to encourage exploration. For \(d\)-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an \(\)-optimal policy within \(}(d^{2}/^{3})\) samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence \((d^{2})\). It also improves over state-of-the-art results of policy optimization algorithms  by a factor of \(d\). For general function approximation that subsumes linear MDPs, Optimistic NPG, to our best knowledge, is also the first policy optimization algorithm that achieves the polynomial sample complexity for learning near-optimal policies.

## 1 Introduction

Policy optimization algorithms  with neural network function approximation have played an important role in recent empirical success of reinforcement learning (RL), such as robotics , games  and large language models . Motivated by the empirical success, the theory community made a large effort to design provably efficient policy optimization algorithms that work in the presence of linear function approximation . Early works focused on proving that policy optimization algorithms are capable to learn near-optimal policies using a polynomial number of samples under certain reachability (coverage) assumptions. [e.g., 1, 17, 16, 18, 19, 19]. While this was good for laying down the foundations for future work, the reachability assumptions basically imply that the state space is already well-explored or rather easy to explore, which avoids the challenge of performing strategic exploration -- one of the central problems in both empirical and theoretical RL.

To address this limitation, later works (Agarwal et al., 2020; Zanette et al., 2021) proposed policy optimization algorithms that enjoy polynomial sample-complexity guarantee without making any reachability assumption, but at the cost of either complicating the algorithm design (and analysis) with various tricks or getting highly suboptimal sample-complexity guarantees. For example, the PC-PG algorithm (Agarwal et al., 2020), which, to our knowledge was the first policy optimization algorithm for learning linear MDPs without reachability assumptions, requires \(}((d)/^{11})\) samples to learn an \(\)-optimal policy for \(d\)-dimensional linear MDPs. That \(}(1/^{11})\) samples were necessary for this task is highly unlikely. Indeed, Zanette et al. (2021) greatly improved this sample complexity to \(}(d^{3}/^{3})\) at the cost of considerably complicating the algorithm design and the analysis. Moreover, we are not aware of efficient guarantees of policy optimization for generic function approximation, which is rich enough to subsume linear MDPs. This motivates us to ask the following question:

Can we design a **simple, general** policy optimization algorithm

with **sharp** sample-complexity guarantees in the **exploration** setting2?

In particular, we aim to achieve sharper sample complexity than Zanette et al. (2021) in the linear MDP setting, and achieve low-degree polynomial sample complexity in the general function approximation setting. This paper answers the highlighted question affirmatively by making the following contributions:

* **Sharper rate.** We propose a computationally efficient policy optimization algorithm--Optimistic NPG with sample complexity \[}(d^{2}/^{3})\] for learning an \(\)-optimal policy in an online fashion while interacting with a \(d\)-dimensional linear MDP. This result improves over the best previous one (Zanette et al., 2021) in policy optimization by a factor of \(d\). Moreover, to our knowledge, this is the first computationally efficient algorithm to achieve the optimal quadratic dimension dependence. Before moving on, we remark that previous FQI-style algorithms (Zanette et al., 2020; Jin et al., 2021) achieve the optimal sample complexity \((d^{2}/^{2})\)(Zanette et al., 2020) but they are computationally _inefficient_ due to the mechanism of global optimism in their algorithm design. Several very recent works (He et al., 2022; Agarwal et al., 2022; Wagenmaker et al., 2022) achieve the sample complexity \(}(d^{2}/^{2}+d^{ 4}/)\) using computationally efficient algorithms. Compared to our work, the aforementioned rate has worse dependence on \(d\) and better dependence on \(\). Nonetheless, our result is better in the practically important regime where the feature dimension \(d\) is typically very large and the target accuracy \(\) is not too small. To summarize, the sample complexity of Optimistic NPG strictly improves over the best existing policy optimization algorithms and is not dominated by that of any existing computationally efficient algorithm.

 
**Algorithms** & **Sample Complexity** & **Computationally Efficient** & **Policy Optimization** \\   Zanette et al. (2020) & \((d^{2}/^{2})\) & \(\) & \(\) \\ Jin et al. (2021) & \(}(d^{2}/^{2}+d^{ 4}/)\) & ✓ & \(\) \\  Agarwal et al. (2020) & \(}((d)/^{11})\) & ✓ & ✓ \\  Zanette et al. (2021) & \(}(d^{3}/^{3})\) & ✓ & ✓ \\ 
**Optimistic NPG (this work)** & \(}(d^{2}/^{3})\) & ✓ & ✓ \\  

Table 1: A comparison of sample-complexity results for linear MDPs.

To our best knowledge, this paper also achieves the first polynomial sample-complexity guarantee for policy optimization under general function approximation.
* **Simple on-policy algorithm.** At a high level, Optimistic NPG is almost an _on-policy_ version of natural policy gradient (NPG) (Kakade, 2001) with Boltzmann policies that use linear function approximation for optimistic action-value estimates. By "on-policy", we mean the transition-reward data used to improve the current policy are obtained exactly by executing the current policy. However, the algorithm has a tuneable parameter (denoted by \(m\) in Algorithm 1) that allows it to reuse data sampled from earlier policies. By using such data, the algorithm becomes _off-policy_. The optimal choice of the tuneable parameter is dictated by theoretical arguments. Interestingly, our analysis shows that even the purely on-policy version of the algorithm (i.e., set \(m=1\) in Algorithm 1) enjoys a well-controlled sample complexity of \(}(d^{2}/^{4})\). To the best of our knowledge, this is the first time an on-policy method is shown to have polynomial sample complexity in the exploration setting, given that all previous policy optimization or Q-learning style or model-based algorithms (e.g., Jin et al., 2020; Zanette et al., 2020, 2021; Agarwal et al., 2020; Cai et al., 2020; Shani et al., 2020, etc) are off-policy.
* **New proof techniques.** To achieve the improved rate, we introduce several new ideas in the proof, including but not limited to (a) exploiting the softmax parameterization of the policies to reuse data and improve sample efficiency (Lemma 3), (b) controlling _on-policy_ uncertainty (Lemma 4) instead of cumulative uncertainty as in previous off-policy works, and (c) using a bonus term that is smaller than those in previous works by a factor of \(\) (see Lemma 2 and the discussions that follow).

### Related works

Since this paper studies policy optimization algorithms in the setting of linear MDPs, below we restrict our focus to previous theoretical works on either policy optimization or linear MDPs.

Policy optimization.This work is inspired by and builds upon two recent works (Shani et al., 2020; Cai et al., 2020) that combine NPG with bonus-based optimism to handle exploration. In terms of algorithmic design, this paper (Optimistic NPG) utilizes on-policy fresh data for value estimation while Shani et al. (2020); Cai et al. (2020) are off-policy and reuse _all_ the historical data. In terms of theoretical guarantees, Shani et al. (2020) and Cai et al. (2020) only study tabular MDPs and linear mixture MDPs (Zhou et al., 2021) respectively, while this paper considers the more challenging setting of linear MDPs (Jin et al., 2020) (in our view, linear MDPs are more challenging as there the number of model parameters scale with the number of states). We remark that due to some subtle technical challenge, so far it still remains unknown whether it is possible to generalize the analysis of Cai et al. (2020) to handle linear MDPs. Agarwal et al. (2020); Zanette et al. (2021) derive the first line of policy optimization results for RL in linear MDPs without any reachability- (or coverage-) style assumption. Compared to Agarwal et al. (2020); Zanette et al. (2021), our work considers the same setting but designs a simpler algorithm with cleaner analysis and sharper sample-complexity guarantee. Nevertheless, we remark that in certain model-misspecification settings, the algorithms of Agarwal et al. (2020); Zanette et al. (2021) can potentially achieve stronger guarantees. Since we focus on the well-specified setting (that is, the environment is perfectly modeled by a linear MDP), we refer interested readers to Agarwal et al. (2020) for more details. Finally, there have been a long line of works (e.g., Agarwal et al., 2021; Bhandari and Russo, 2019; Liu et al., 2019; Neu et al., 2017; Abbasi-Yadkori et al., 2019, etc) that study policy optimization under reachability (or coverage) assumptions, which eliminates the need for performing strategic exploration. Throughout this paper we do not make any such assumption and directly tackle the exploration challenge.

Linear MDPs.For linear MDPs, Jin et al. (2020) proposed a computationally efficient algorithm (LSVI-UCB) with \(}(d^{3}/^{2})\) sample complexity. Later on, Zanette et al. (2020) utilized the idea of global optimism to obtain the optimal sample complexity \((d^{2}/^{2})\) at the cost of sacrificing computational efficiency. Recently, He et al. (2022); Agarwal et al. (2022); Wagenmaker et al. (2022) designed new computationally efficient algorithms that can achieve \(}(d^{2}/^{2}+d^{ 4}/)\) sample complexity. Compared to the above works, the sample complexity of Optimistic NPG is not strictly worse than that of any known computationally efficient algorithm. In fact, it is the only computationally efficient method to achieve the optimal dimension dependence. Nonetheless, for learning a near-optimal policy with a vanishing suboptimality \(\), the current sample complexity of Optimistic NPG is loose by a factor of \(^{-1}\).

## 2 Preliminaries

MDP.We consider the model of episodic Markov Decision Processes (MDPs). Formally, an MDP is defined by a tuple \((,,,R,H)\) where \(\) denotes the set of states, \(\) denotes the set of actions, both of which are assumed to be finite, \(=\{_{h}\}_{h[H]}\) denotes the set of transition probability functions so that \(_{h}(s^{} s,a)\) is equal to the probability of transitioning to state \(s^{}\) given that action \(a\) is taken at state \(s\) and step \(h\), \(R=\{R_{h}\}_{h[H]}\) denotes the collection of expected reward functions so that \(R_{h}(s,a)\) is equal to the expected reward to be received if action \(a\) is taken at state \(s\) and step \(h\), and \(H\) denotes the length of each episode. An agent interacts an MDP in the form of episodes. Formally, we assume without loss of generality that each episode always starts from a _fixed_ initial state \(s_{1}\). At the \(h^{}\) step of this episode, the agent first observes the current state \(s_{h}\), then takes action \(a_{h}\) and receives reward \(r_{h}(s_{h},a_{h})\) satisfying

\[[r_{h}(s_{h},a_{h}) s_{h},a_{h}]=R_{h}(s_{h},a_{h})\,.\]

After that, the environment transitions to

\[s_{h+1}_{h}( s_{h},a_{h})\,.\]

The current episode terminates immediately once \(r_{H}\) is received. Throughout this paper, we assume \(\) and \(R\) are unknown to the learner.

Linear MDP.A \(d\)-dimensional linear MDP (Jin et al., 2020) is defined by two sets of feature mappings, \(\{_{h}\}_{h[H]}(^{d})\) and \(\{_{h}\}_{h[H]}(^{d})\), and a set of vectors \(\{w^{}_{h}\}_{h[H]}^{d}\) so that the transition probability functions can be represented as bilinear functions of feature mappings \(\{_{h}\}_{h[H]}\) and \(\{_{h}\}_{h[H]}\), and the reward functions can be represented as linear functions of \(\{_{h}\}_{h[H]}\). Formally, we have that for all \((s,a,s^{})\):

\[_{h}(s^{} s,a) =_{h}(s,a),_{h}(s^{}),\] \[R_{h}(s,a) =_{h}(s,a),w^{}_{h}.\]

For the purpose the regularity, linear MDPs also require that

\[_{h,s,a}\|_{h}(s,a)\|_{2} 1,_{h}\|w^{}_{h}\|_{2} ,\]

and for any function \(V_{h+1}:\),

\[\|_{s}V_{h+1}(s)_{h+1}(s)ds\|_{2}.\]

Throughout this paper, we assume only \(\{_{h}\}_{h[H]}\) is available to the learner while \(\{_{h}\}_{h[H]}\) and \(\{w^{}_{h}\}_{h[H]}\) are not.

Policy and value.A (Markov) policy is a set of conditional probability functions \(=\{_{h}\}_{h[H]}\) so that \(_{h}( s)_{}\) gives the distribution over the action set conditioned on the current state \(s\) at step \(h\). We define the V-value functions of policy \(\) by \(\{V^{}_{h}\}_{h[H]}()\) so that \(V^{}_{h}(s)\) is equal to the expected cumulative reward an agent will receive if she follows policy \(\) starting from state \(s\) and step \(h\). Formally,

\[V^{}_{h}(s)=[_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}}) s_{h}=s,a_{h^{}}_{h^{}}(s_{h^{ }}),s^{}_{h+1}_{h}( s^{}_{h},a^{ }_{h})],\]

where the expectation is with respect to the randomness of the transition, the reward and the policy. Similarly, we can define the Q-value functions of policy \(\) by \(\{Q^{}_{h}\}_{h[H]}()\) so that \(Q^{}_{h}(s,a)\) is equal to the expected cumulative reward an agent will receive if she follows policy \(\) starting from taking action \(a\) at state \(s\) and step \(h\). Formally,

\[Q^{}_{h}(s,a)=_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}})(s_{h},a_{h})=(s,a),a_{h^{}}_{h^{ }}(s_{h^{}}),s^{}_{h+1}_{h}( s^{ }_{h},a^{}_{h}).\]We denote by \(^{}=\{_{h}^{}\}_{h[H]}\) the optimal policy such that \(^{}*{argmax}_{}V_{h}^{}(s)\) for all \((s,h)[H]\). By backward induction, one can easily prove that there always exists an optimal Markov policy. For simplicity of notations, we denote by \(V_{h}^{}=V_{h}^{^{}}\) and \(Q_{h}^{}=Q_{h}^{^{}}\) the optimal value functions. Note that given an MDP, the optimal value functions are unique despite that there may exist multiple optimal policies.

Learning objective.The objective of this paper is to design an efficient policy optimization algorithm to learn an \(\)-optimal policy \(\) such that \(V_{1}^{}(s_{1}) V_{1}^{}(s_{1})-\). Here the optimality is only measured by the value at the initial state \(s_{1}\), because (a) each episode always starts from \(s_{1}\), and (b) this paper studies the online exploration setting without access to a simulator, which means some states might be unreachable and learning optimal policies starting from those states is in general impossible.

## 3 Optimistic Natural Policy Gradient

In this section, we present the algorithm Optimistic NPG (Optimistic Natural Policy Gradient) and its theoretical guarantees.

### Algorithm

The pseudocode of Optimistic NPG is provided in Algorithm 1. At a high level, the algorithm consists of the following three key modules.

* **Periodic on-policy data collection** (Lines 4-6): Similarly to the empirical PPO algorithm (Schulman et al., 2017), Optimistic NPG discards all the old data, and executes the _current_ policy \(^{k}\) to collect a batch of fresh data \(^{k}\) after every \(m\) steps of policy update. These data will later be used to evaluate and improve the policies in the next \(m\) steps. Noticeably, this _on-policy_ data mechanism is very different from most existing works in theoretical RL, where they either need to keep the historical data or have to rerun historical policies to refresh the dataset for the technical purpose of elliptical potential arguments in proofs. In comparison, Optimistic NPG only uses fresh data collected by the current (or very recent) policy, which resembles practical policy optimization algorithms such as PPO (Schulman et al., 2017) and TRPO (Schulman et al., 2015).
* **Optimistic policy evaluation** (Line 8): Given the above collected dataset \(^{k}\), we estimate the value functions of the current policy \(^{k}\) by invoking Subroutine OPE (optimistic policy evaluation). In Section 4, we show how to implement Subroutine OPE for tabular MDPs, linear MDPs and RL problems of low eluder dimension.
* **Policy update** (Line 9): Given the optimistic value estimates \(\{_{h}^{k}\}_{h[H]}\) of \(^{k}\), Optimistic NPG performs one-step mirror ascent from \(_{h}^{k}( s)\) with gradient \(_{h}^{k}(s,)\) to obtain \(_{h}^{k+1}( s)\) at each \((h,s)\). Importantly, this step can be implemented in a computationally efficient way, because by the update rule of mirror ascent \(_{h}^{k}( s)(_{t=1}^{k-1}_{h}^{t}(s,))\), we only need to store \(\{\{_{h}^{t}\}_{h[H]}\}_{t[K]}\), from which any \(_{h}^{k}( s)\) can be computed on the fly.

### Theoretical guarantee

Optimistic NPG is a generic policy optimization meta-algorithm, and we can show it provably learns near-optimal policies as long as subroutine OPE satisfies the following condition.

**Condition 1** (Requirements for OPE).: _Suppose parameter \(m\) and \(\) in Optimistic NPG satisfy \(m( H^{2})^{-1}\). Then with probability at least \(1-\), \((^{k},^{k})\) returns \(\{_{h}^{k}\}_{h[H]}([0, H])\) that satisfy the following properties for all \(k[K]\)._

1. **Optimism:** _For all \((s,a,h)[H]\)_ \[_{h}^{k}(s,a)(_{h}^{^{k}}_{h+1}^{k})( s,a)\] \[(_{h}^{}f)(s,a)=R_{h}(s,a)+[f (s^{},a^{}) s^{}_{h}( s,a),a^{ }_{h+1}(s^{})].\]1. **Consistency:** _There exists an absolute complexity measure_ \(L^{+}\) _such that_ \[_{1}^{k}(s_{1})-V_{1}^{^{k}}(s_{1})(NKL/)},\] _where_ \(_{1}^{k}(s_{1})=[_{1}^{k}(s_{1},a_{1})  a_{1}^{k}(s_{1})].\)__

Condition (1A) requires that the Q-value estimate returned by OPE satisfies the Bellman equation under policy \(^{k}\) optimistically. Requiring optimism is very common in analyzing RL algorithms, as most algorithms rely on the principle of optimism in face of uncertainty to handle exploration. Condition (1B) requires that the degree of over-optimism at initial state \(s_{1}\) is roughly of order \(\) with respect to the number of samples \(N\). This is intuitive since as more data are collected from policy \(^{k}\) or some policy similar to \(^{k}\) (as is enforced by the precondition \(m( H^{2})^{-1}\)), the value estimate at the initial state should be more accurate. In Section 4, we will provide concrete instantiations of Subroutine OPE for tabular MDPs, linear MDPs and RL problems of low eluder dimension, all of which satisfy Condition 1 with mild complexity measure \(L\).

Under Condition 1, we have the following theoretical guarantees for Optimistic NPG, which we prove in Appendix A.

**Theorem 1**.: _Suppose Condition 1 holds. In Algorithm 1, if we choose_

\[K=(||}{^{2}}), N= ((LK/)}{^{2}}),= (}), m(),\]

_then with probability at least \(1/2\), \(^{}\) is \(()\)-optimal._

Below we emphasize two special choices of \(m\) (the period of sampling fresh data) in Theorem 1:

* When choosing \(m=1\), Optimistic NPG is purely _on-policy_ as it only uses data sampled from the current policy to perform policy optimization. In this case, the total sample complexity is \[}{}= N=(}{^{4}}),\] which, to our knowledge, is the first polynomial sample complexity guarantee for purely on-policy algorithms.
* When choosing \(m=[H/]\), we obtain sample complexity \[}{}= N=(}{^{3}}),\] which improves over the above purely on-policy version by a factor of \(H/\). In Section 4.2, we will specialize this result to linear MDPs to derive a sample complexity that improves the best existing one in policy optimization (Zanette et al., 2021) by a factor of \(d\).

Finally we remark that for cleaner presentation, we state Theorem 1 for a fixed, constant failure probability. However, the result is easy to extend to the case when the failure probability is some arbitrary value of \((0,1)\) at the expense of increasing the sample complexity by a factor of \((1/)\): one simply need to run Algorithm 1 for \((1/)\) times, estimate the values of every output policy to accuracy \(\), and then pick the one with the highest estimate.

## 4 Examples

In this section, we implement subroutine OPE for tabular MDPs, linear MDPs and RL problems of low eluder dimension, and derive the respective sample complexity guarantees of Optimistic NPG.

### Tabular MDPs

The implementation of tabular OPE (Subroutine 1) is rather standard: first estimate the transition and reward from dataset \(\), then plug the estimated transition-reward into the Bellman equation under policy \(\) to compute the Q-value estimate backwards. And to guarantee optimism, we additionally add standard counter-based UCB bonus to compensate the error in model estimation. Formally, we have the following guarantee for tabular OPE.

**Proposition 1** (tabular MDPs).: _Suppose we choose \(=(H|||)})\) in Subroutine 1, then Condition 1 holds with \(L=SAH^{3}\)._

By combining Proposition 1 with Theorem 1, we prove that Optimistic NPG with Subroutine 1 learns an \(\)-optimal policy within \((KN/m)=}(H^{6}||||/^{3})\) episodes for any tabular MDP. This rate is strictly worse than the best existing result \(}(H^{3}||||/^{2})\) in tabular policy optimization . Nonetheless, the proof techniques in  are specially tailored to tabular MDPs and it is highly unclear how to generalize them to linear MDPs due to certain technical difficulty that arises from covering a prohibitively large policy space. In comparison, our policy optimization framework easily extends to linear MDPs and provides improved sample complexity over the best existing one, as is shown in the following section.

### Linear MDPs

We provide the instantiation of OPE for linear MDPs in Subroutine 2. At a high level, linear OPE computes an upper bound \(\) for \(Q^{}\) by using the Bellman equation under policy \(\) backwards from step \(H\) to step \(1\) while adding a bonus to compensate the uncertainty of parameter estimation. Specifically, the step of ridge regression (Line 5) utilizes the linear completeness property of linear MDPs in computing \(_{h}\) from \(_{h+1}\), which states that for any function \(_{h+1}:\), there exists \(_{h}^{d}\) so that \(_{h}(s,a),_{h}=R_{h}(s,a)+_{s^{ } P_{h}(|s,a)}[_{h+1}(s^{})]\) for all \((s,a)\). And in Line 6, we add an elliptical bonus \(b_{h}(s,a)\) to compensate the error between our estimate \(_{h}\) and the groundtruth \(_{h}\) so that we can guarantee \(_{h}(s,a)(_{h}^{}_{h+1})(s,a)\) for all \((s,a,h)\) with high probability.

\[_{h}=*{argmin}_{}_{(s_{h},a_{h},r_{h},s_{ h+1})_{h}}(_{h}(s_{h},a_{h})^{}-r_{h}- _{h+1}(s_{h+1}))^{2}+\|\|_{2}^{2}.\] (1)

**Proposition 2** (linear MDPs).: _Suppose we choose \(=1\) and \(=(H)\) in Subroutine 2, then Condition 1 holds with \(L=d^{2}H^{3}\)._

By combining Proposition 2 with Theorem 1, we obtain that \((KN/m)=}(d^{2}H^{6}/^{3})\) episodes are sufficient for Optimistic NPG to learn an \(\)-optimal policy, improving upon the state-of-the-art policy optimization results (Zanette et al., 2021) by a factor of \(d\). Notably, this is also the first computationally efficient algorithm to achieve optimal quadratic dimension dependence for learning linear MDPs. The key factor behind this improvement is Optimistic NPG's periodic collection of fresh on-policy data, which eliminates the undesired correlation and avoids the union bound over certain nonlinear function class that are commonly observed in previous works. Consequently, Optimistic NPG uses a bonus function that is \(\) times smaller than in previous works (e.g., Jin et al., 2020). For further details on how we achieve this improvement, we refer interested readers to Appendix A, specifically Lemma 2.

### General function approximation

Now we instantiate OPE for RL with general function approximation. In this setting, the learner is provided with a function class \(=_{1}_{H}\) for approximating the Q values of polices, where \(_{h}([0,H])\).

General OPE.The pseudocode is provided in Subroutine 3. At each step \(h[H]\), general OPE first constructs a confidence set \(_{h}\) which contains candidate estimates for \(_{h}^{n}_{h+1}}\) (Line 6). Specifically, \(_{h}\) consists of all the value candidates \(f_{h}_{h}\) whose square temporal difference (TD) error on dataset \(_{h}\) is no larger than the smallest one by an additive factor \(\). Such construction can be viewed as a relaxation of the classic fitted Q-iteration (FQI) algorithm which only keeps the value candidate with the smallest TD error. In particular, if we pick \(=0\), \(_{h}\) collapses to the solution of FQI. Equipped with confidence set \(_{h}\), general OPE then perform optimistic planning at every \((s,a)\) by computing \(_{h}(s,a)\) to be the largest possible value that can be achieved by any candidate in confidence set \(_{h}\) (Line 7). We remark that general OPE shares a similar algorithmic spirit to the GOLF algorithm (Jin et al., 2021). The key difference is that GOLF is a purely value-based algorithm and only performs optimistic planning at the initial state while general OPE performs optimistic planning at every state and is part of a policy gradient algorithm.

Theoretical guarantee.To state the main theorem, we need to first introduce two standard concepts: value closeness and eluder dimension, which have been widely used in previous works that study RL with general function approximation.

**Assumption 1** (value closeness (Wang et al., 2020)).: _For all \(h[H]\) and \(V_{h+1}:[0,H]\), \(_{h}V_{h+1}_{h}\) where \([_{h}V_{h+1}](s,a)=R_{h}(s,a)+[V_{h+1}(s^{}) s ^{}_{h}( s,a)]\)._

Intuitively, Assumption 1 can be understood as requiring that the application of the Bellman operator \(_{h}\) to any V-value function \(V_{h+1}\) results in a function that belongs to the value function class \(_{h}\). This assumption holds in various settings, including tabular MDPs and linear MDPs. Furthermore, it is reasonable to assume that value closeness holds whenever the function class \(\) has sufficient expressive power, such as the class of neural networks.

**Definition 1** (eluder dimension (Russo and Van Roy, 2013)).: _Let \(\) be a function class from \(\) to \(\) and \(>0\). We define the \(\)-eluder dimension of \(\), denoted as \(d_{E}(,)\), to be the largest \(L^{+}\) such that there exists \(x_{1},,x_{L}\) and \(g_{1},g_{1}^{},,g_{L},g_{L}^{}\) satisfying: for all \(l[L]\), \(_{i<l}(g_{l}(x_{i})-g_{l}^{}(x_{i}))^{2}\) but \(g_{l}(x_{l})-g_{l}^{}(x_{i})\)._

At a high level, eluder dimension \(d_{E}(,)\) measures how many mistakes we have to make in order to identify an unknown function from function class \(\) to accuracy \(\), in the worst case. It has been widely used as a sufficient condition for proving sample-efficiency guarantee for optimistic algorithms in RL with general function approximation, e.g., Wang et al. (2020); Jin et al. (2021).

Now we state the theoretical guarantee for general OPE.

**Proposition 3** (general function approximation).: _Suppose Assumption 1 holds and we choose \(=(H^{2}(||NK/))\) in Subroutine 3, then Condition 1 holds with \(L=H^{3}(||)_{h}d_{E}(_{h},1/N)\)._

Plugging Proposition 3 back into Theorem 1, we obtain that \((KN/m)=}(d_{E}(||)H^{6}/^{3})\) episodes are sufficient for Optimistic NPG to learn an \(\)-optimal policy, which to our knowledge is the first polynomial sample complexity guarantee for policy optimization with general function approximation.

## 5 Conclusions

We proposed a model-free, policy optimization algorithm--Optimistic NPG for online RL and analyzed its behavior in the episodic setting. In terms of algorithmic design, it is not only considerably simpler but also more closely resembles the empirical policy optimization algorithms (e.g., PPO, TRPO) than the previous theoretical algorithms. In terms of sample efficiency, for \(d\)-dimensional linear MDPs, it improves over state-of-the-art policy optimization algorithm by a factor of \(d\), and is the first computationally efficient algorithm to achieve the optimal dimension dependence. To our best knowledge, Optimistic NPG is also the first sample-efficient policy optimization algorithm under general function approximation.

For future research, we believe that it is an important direction to investigate the optimal complexity for using policy optimization algorithms to solve linear MDPs. Despite the optimal dependence on dimension \(d\), the current sample complexity of Optimistic NPG is worse than the optimal rate by a factor of \(1/\). It remains unclear to us how to shave off this \(1/\) factor to achieve the optimal rate.