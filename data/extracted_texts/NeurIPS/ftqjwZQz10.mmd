# DEX: Data Channel Extension for Efficient CNN Inference on Tiny AI Accelerators

Taesik Gong\({}^{12}\) Fahim Kawsar\({}^{13}\) Chulhong Min\({}^{1}\)

\({}^{1}\)Nokia Bell Labs \({}^{2}\)UNIST \({}^{3}\)University of Glasgow

taesik.gong@unist.ac.kr

{fahim.kawsar, chulhong.min}@nokia-bell-labs.com

###### Abstract

Tiny machine learning (TinyML) aims to run ML models on small devices and is increasingly favored for its enhanced privacy, reduced latency, and low cost. Recently, the advent of tiny AI accelerators has revolutionized the TinyML field by significantly enhancing hardware processing power. These accelerators, equipped with multiple parallel processors and dedicated per-processor memory instances, offer substantial performance improvements over traditional microcontroller units (MCUs). However, their limited data memory often necessitates downsampling input images, resulting in accuracy degradation. To address this challenge, we propose Data channel EXtension (_DEX_), a novel approach for efficient CNN execution on tiny AI accelerators. DEX incorporates additional spatial information from original images into input images through patch-wise even sampling and channel-wise stacking, effectively extending data across input channels. By leveraging underutilized processors and data memory for channel extension, DEX facilitates parallel execution without increasing inference latency. Our evaluation with four models and four datasets on tiny AI accelerators demonstrates that this simple idea improves accuracy on average by 3.5%p while keeping the inference latency the same on the AI accelerator. The source code is available at https://github.com/Nokia-Bell-Labs/data-channel-extension.

## 1 Introduction

Tiny machine learning (TinyML) is an active research field focused on developing and deploying machine learning models on extremely resource-constrained devices, such as microcontroller units (MCUs) and small IoT sensors. Compared to cloud-based AI, TinyML on devices offers benefits in privacy preservation, low latency, and low cost. While research efforts in TinyML, such as model compression techniques [15; 17; 25; 27; 31; 32], have successfully reduced the size of AI models to fit into memory-constrained MCUs, the fundamental limitation in the processing capability of MCUs leads to long inference latency. This limitation hinders the widespread adoption of on-device AI, especially for real-time applications.

Recently, the advent of _tiny AI accelerators_ like the Analog Devices MAX78000  and Google Coral Micro  has revolutionized the TinyML field by dramatically boosting the model inference speed and leading a new phase of on-device AI. For instance, the MAX78000 AI accelerator  achieves 170\(\) faster inference latency compared to an MCU processor (MAX32650 ).

To enable such acceleration, these tiny AI accelerators introduce several hardware optimization techniques. They often feature multiple convolutional processors (e.g., 64 processors in MAX78000 )and parallelize per-channel CNN operations across these processors. For further optimization, the memory architecture allows each processor to have a dedicated memory instance, i.e., _per-processor memory instance_. This design enables simultaneous memory access to multiple channels from different processors. While these hardware-level optimizations bring significant performance improvements, we found that they also have several constraints at the expense of the optimizations. First, the per-processor memory architecture highly restricts the supported input image size because the data memory each processor can use for its input/output channels is limited to the capacity of its dedicated memory instance, which is a fraction of the total data memory divided by the number of processors. Consequently, most vision models for these accelerators are designed to support very small images, such as \(32 32\) pixels. Given that images captured by cameras are often generated with higher resolutions, downsampling is inevitable, leading to accuracy degradation due to information loss from the original image. Second, we found that processors and data memory are underutilized for the input layer due to the per-processor memory architecture; since input images typically have a low number of channels (e.g., RGB three channels), only a limited number of processors tied to memory instances are utilized while the remaining processors remain idle. For instance, on the MAX78000, 61 of 64 processors and per-processor memory instances remain unused in the first layer.

In this work, we propose a novel approach, Data channel EXtension (_DEX_), to overcome these constraints while still benefiting from the acceleration power of tiny AI accelerators. The core idea is to boost accuracy by extending the data channels to incorporate additional image information into unused data memory instances and processors, instead of simple downsampling. Owing to the parallel processing and memory access capabilities of tiny AI accelerators, our method can achieve this accuracy improvement without compromising inference latency. Specifically, DEX involves two procedures: (1) pair-wise even sampling, where pixels from the original image are evenly sampled, and (2) channel-wise stacking, which arranges these samples across multiple channels.

To measure the impact of DEX on accuracy and resource utilization, we conducted experiments on the MAX78000  and MAX78002  tiny AI accelerator platforms. DEX was evaluated on four models, SimpleNet , WideNet , EfficientNetV2 , and MobileNetV2 , using four vision datasets: ImageNette , Caltech101 , Caltech256 , and Food101 . Our results show that DEX improves average accuracy by 3.5%p compared to the original model with downsampling and 3.6%p compared to the existing coordinate augmentation approach (CoordConv ), without increasing inference latency. Additionally, DEX maximizes data memory and processor utilization, demonstrating its effectiveness in enhancing model performance on resource-constrained devices. In summary, DEX can significantly enhance the performance of neural networks on tiny AI accelerators, leading to more efficient and effective deployment of AI on resource-constrained devices.

## 2 Preliminary: tiny AI accelerators

The advent of tiny AI accelerators marks a pivotal shift towards on-device AI, greatly enhancing privacy and reducing latency. While a number of tiny-scale AI accelerators have emerged recently, such as Analog Devices MAX78000/MAX78002 [34; 37], Google Coral Micro , and GreenWaves GAP-8/GAP-9 , only a few are commercially available with access and control over their operations. In this paper, we focus on the MAX78000  and MAX78002  as our primary platforms since they are the most widely used tiny AI accelerator research platforms [1; 6; 13; 39; 40; 43] owing to the disclosed hardware details and open-source tools, enabling in-depth analysis and modification of their operations.

Architecture of tiny AI accelerators.The distinctive characteristic of tiny AI accelerators compared to conventional microcontroller units (MCUs) is _parallel processors_ that parallelize per-channel CNN operations across these processors. Figure 1 depicts an abstracted architecture of the MAX78000; MAX78002 has a similar architecture to MAX78000 with increased memory (1.3 MB data and 2 MB weight memory). Further details are in Appendix A.1. It has 64 parallel convolutional processors, each capable of performing specific operations independently. To maximize performance, each processor has a dedicated memory instance, i.e., _per-processor memory instance_ that optimizes data transfer with parallel access. For each CNN layer, operations on individual channels are assigned to separate convolutional processors and executed simultaneously, significantly reducing latency typically associated with convolutional algorithms. Each processor has a pooling engine, an input cache, and a convolution engine that can handle 3 by 3 kernels. The CNN accelerator includes 512 KB of data memory and 432 KB of weight storage memory. Within the 512 KB of data memory, an 8 KB per-processor memory instance is allocated to each of the 64 processors. Figure 3 shows the utilization of the processors (\(Pr_{i}\)) for executing CNNs with varying sizes of the input channels. Each processor communicates with a dedicated memory instance for each data channel. For example, given a three-channel image, three parallel processors are utilized in the first layer.

Performance gain over MCUs.A recent benchmark study  demonstrates the remarkable performance gain of the MAX78000 in terms of latency and energy consumption. Figure 2 shows that the MAX78000 significantly outperforms widely-used MCUs (MAX32650 with a Cortex-M4 at 120 MHz , and a high-performance MCU, the STM32F7 with a Cortex-M7 at 216 MHz ) for face detection (FaceID) and keyword spotting (KWS). For KWS, latency is drastically reduced to only 2.0 ms, compared to 350 ms for the MAX32650 and 123 ms for the STM32F7. Accordingly, energy efficiency of the MAX78000 is also significant; it consumes only 0.40 mJ for FaceID, dramatically less than the 42.1 mJ and 464 mJ required by the MAX32650 and STM32F7, respectively.

## 3 DEX: Data channel extension for efficient CNN inference on AI accelerators

### Constraints of per-processor memory instances in tiny AI accelerators for images

As mentioned in SS2, tiny AI accelerators leverage per-processor memory instances for faster data transfer with parallel access. However, we disclose that this causes several constraints at the expense of rapid data access: (1) low image resolution and (2) underutilized processors and data memory.

Low image resolution due to limited per-processor memory size.MAX78000  has 512 KB data memory which is divided into 64 segments of 8 KB memory instances per processor, each storing the data of each input channel. This memory architecture highly restricts the supported input resolution. For instance, an input image with a shape \(3 224 224\) (channel, height, and weight), which is a typical size of ImageNet , does not fit the MAX78000 even with Q7 format (one byte for each value), as memory limit for each channel is 8 KB (\(224 224 50\) KB > 8 KB). Thus, the current practice on tiny AI accelerators is to shrink the resolution of input images by downsampling and accordingly, to design small models to process lower-resolution images, e.g., \(3 32 32\). However, with this, it loses most of the information of the original image, which might lead to sub-optimal performance.

Underutilized processors and data memory for the input layer.Although per-processor memory instances allow simultaneous memory access from different processors, it also brings inefficiency in data memory and processor utilization, especially in the input layer. Specifically, given an input image \(I\) with the number of channels \(C_{I}\), height \(H_{I}\), and width \(W_{I}\) (e.g., \(3 224 224\)) as shown in Figure 4(a), Figure 4(b) illustrates the downsampled image with the number of channels \(C_{I}\), height \(H_{O}\), and width \(W_{O}\) (e.g., \(3 32 32\)), and its data memory usage in the AI accelerator.

Figure 3: Processor utilization with varying input channels on the AI accelerator.

With three RGB channels, channel data are separately stored for each data memory instances for parallel execution. As there is \(N\) processors and corresponding data memory instances, it leaves the remaining \(N-3\) processors and data memory instances idle. This provides an opportunity to utilize these idle data memory instances and parallel processors, which we detail in the following section.

### DEX procedure

As aforementioned, we note two key observations: (1) the input image needs to be downsampled due to the limited memory of tiny AI accelerators, which means most of the pixel information cannot be utilized, and (2) there exist idle data memory instances and processors that could process up to \(N\) channels in parallel. Although several recent studies have found efficient model architectures on tiny AI accelerators , existing studies lack considerations on this inefficiency for input image processing in CNNs (further discussion on related work is in Appendix 5).

Based on our observations, we propose Data channel EXtension (_DEX_) for efficient CNN execution on tiny AI accelerators. The key intuition behind DEX is that we can utilize the remaining data memory to incorporate additional information from the original image into neural networks by extending the input data across channels. By utilizing this additional memory and processors, we can incorporate extra sample information for feature learning without sacrificing latency. Figure 4(c) shows the input data reshaped via DEX, where each channel contains different pixel information from the original image. With DEX extending data across channels (from \(C_{I}\) to \(C_{O}\)), it can fully utilize the data memory and associated parallel processors. Figure 5 shows an overview of the procedure of DEX. Given an input image \(I\) with a number of channels \(C_{I}\), height \(H_{I}\), and width \(W_{I}\), DEX generates an output image \(O\) with an extended number of channels \(C_{O}\), height \(H_{O}\), and width \(W_{O}\) (e.g., \(64 32 32\)) via patch-wise even sampling and channel-wise stacking.

Patch-wise even sampling.The purpose of patch-wise even sampling is to select samples evenly spaced across the original image while keeping the spatial relationship among pixels. We first define

Figure 4: Comparison among different input data. (a) an original image that exceeds the data memory limit of the AI accelerator, (b) a downsampled image that fits the data memory but does not fully utilize parallel processors and data memory, and (c) a DEX-generated image that incorporates more information from original image by extending data across channels with full utilization of parallel processors and data memory instances.

Figure 5: Overview of DEX. DEX divides the original image \(I\) into multiple patches. DEX then evenly samples pixels from each patch \(P_{ij}\) and constructs an output pixel \(O_{ij}\) by stacking samples across channels.

a _patch_ from the original image in which a corresponding output pixel is generated. We denote i-th row and j-th column of patch \(P_{ij}\) in \(I\) as:

\[P_{ij}=I[|i}{H_{O}}|:|(i+1) }{H_{O}}|,|j}{W_{O}}|:|(j+1)}{W_{O}}|],\] (1)

where \([:,:]\) refers to a 2-D array slicing operation, specifying the selection of rows and columns sequentially. The number of patches is determined by the resolution of the output image, i.e., \(H_{O} W_{O}\). For each patch \(P_{ij}\), we generate the corresponding output data \(O_{ij}\). This ensures that the spatial relationships among pixels in the input image are preserved in the output, maintaining spatial consistency throughout the process.

The next step is to sample pixels within the patch considering the memory budget. Specifically, we define \(K=}{C_{I}}\) as the number of samples to be selected in each patch. Given the height and width of patch \(H_{P_{ij}}=|(i+1)}{H_{O}}|-|i}{H_{O}}|.\) and \(W_{P_{ij}}=|(i+1)}{W_{O}}|-|i }{W_{O}}|\), the i-th row and j-th column of output \(O_{ij}\) can be represented by:

\[O_{ij}=\{P_{ij}[|}{W_{P_{ij}}}|,\ l_{k} W _{P_{ij}}] l_{k}=k|} W_{P_{ij}}-1}{K- 1}|,k=0,1,,K-1\},\] (2)

which means a collection of evenly distributed samples within each patch to encourage diverse information while minimizing the use of localized pixel information. With patch-wise even sampling, selected samples are evenly distributed both across patches and within each patch.

Channel-wise stacking.Channel-wise stacking arranges sampled data across multiple channels and keeps this procedure for all pixels to maintain data integrity. Channel-wise stacking is beneficial as it maintains consistency within each channel, preserving the spatial and contextual relationships of the sampled data. Specifically, after patch-wise even sampling, the samples are stacked across the channel axis in ascending order of the index \(k\), and this is repeated for each \(O_{ij}\). Note that \(l_{k}=0\) when \(K=1\), and this is identical to traditional downsampling. If \(K>}{C_{I}}\), it fills up the target channel with \(P\)'s data until the limit and discards the remaining channels. For instance, when using RGB channels \((C_{I}=3)\) and if \(C_{O}=64\) and \(K=22\), it takes only the red channel for \(i=21\) and discards the remaining green and blue channels that exceed the channel limit of 64. Algorithm 1 provides the pseudo-code that describes the procedure of DEX's channel extension algorithm.

```
1:Source image \(I\) in a shape \((C_{I},H_{I},W_{I})\)
2:Reshaped image \(O\) in a shape \((C_{O},H_{O},W_{O})\)
3:\(O\) zeros\((C_{O},H_{O},W_{O})\)
4:for\(i 0\)to\(H_{O}-1\)do
5:for\(j 0\)to\(W_{O}-1\)do
6:start_row,end_row\(\) floor\((i}{H_{O}}),\)floor\(((i+1)}{H_{O}})\)
7:start_col,end_col\(\) floor\((j}{W_{O}}),\) floor\((j+1)}{W_{O}})\)
8:\(P_{ij} I[:,\) start_row :end_row,start_col :end_col]\(\) Patch \(P_{ij}\) of \(I\)
9:\(K\) ceil\((}{C_{I}})\)\(\) Number of samples to be selected in \(P_{ij}\)
10:for\(k 0\)to\(K-1\)do\(\) Get channels of \(O_{ij}\) from \(P_{ij}\)
11:\(H_{P_{ij}}\) end_row - start_row
12:\(W_{P_{ij}}\) end_col - start_col
13:\(l_{k}=k(} W_{P_{ij}}-1}{K-1})\)
14:\(O[k C_{I}:(k+1) C_{I},\ i,\ j]=P_{ij}[:,(}{W_{P_{ ij}}}),\ l_{k} W_{P_{ij}}]\)
15:return\(O\) ```

**Algorithm 1** DEX Channel Extension Algorithm
Understanding how DEX leads to performance improvement.DEX's ability to incorporate additional pixel information from the original image can improve the accuracy of CNNs. The extended channels provide further samples of adjacent areas in the original image, significantly broadening the receptive fields of features in the initial CNN layer. This expansion allows the model to detect more complex and subtle features early in the processing pipeline, which is critical for the nuanced understanding and interpretation of visual data. Specifically, Figure 6 visualizes how the first CNN layer operates with DEX, where \(L^{1}_{kernel\_size}\) and \(L^{1}_{c\_out}\) refer to the kernel size and the output channel size of the first layer, respectively. It illustrates the application of the convolution operation across each enhanced channel (\(C_{O}\) as opposed to \(C_{I}\)), where distinct kernel weights are applied to each channel. This ensures that the additional information is integrated into the output feature maps, thereby enriching the model's feature extraction capabilities. The convolutional layer processes the increased channel input, which is reflected in weight sums that construct output channels.

Impact of channel extension on the number of parameters.Given the first CNN layer's kernel size \(L^{1}_{kernel\_size}\) and the first layer's channel output size \(L^{1}_{c\_out}\), the number of parameters required for the input layer can be calculated as \(O_{C} L^{1}_{kernel\_size} L^{1}_{c\_out}\). If \(O_{C}\) is 3, it is the same as the traditional downsampling without our channel extension. Note that this channel extension does not incur additional inference latency on the AI accelerator. We found that the channel extension increases \( 3\%\) of the total parameters as we show in our experiment SS4.2. The rest of the layers remain the same. In addition to its simplicity, we have several reasons to change the first layer only, which we discuss further in SS6.

Utilization of the original image information.With traditional downsampling, the utilization of the original input is \( W_{O}}{H_{r} W_{I}}\), but with DEX, this is extended to \(}{C_{I}} W_{O}}{H_{r} W_{I}}\). For instance, given a \(3 256 256\) input image, a downsampled image \(3 32 32\) utilizes only 1.6% of the original information, while with DEX and an output channel size \(C_{O}=64\), it can utilize 33.3% of the original information. DEX can accommodate all the information when \(C_{O}=C_{I} W_{I}}{H_{O} W_{O}}\)

Maximum number of output channels.Increasing the number of output channels allows DEX to accommodate the original image information. The number of output channels denoted as \(O_{C}\), that can be extended without increasing latency on AI accelerators is limited by the number of data memory instances \(D_{N}\), i.e., \(O_{C}<D_{N}\). For example, the MAX78000 has 64 data memory instances, allowing it to support up to \(O_{C}=64\) output channels without affecting inference latency.

## 4 Evaluation

### Experimental settings

Here we explain experimental settings. Further details are in Appendix A.

On-device testbed.We evaluated DEX on the off-the-shelf MAX78000 feature board  and MAX78002 Evaluation Kit , which are a development platform for the MAX78000  and MAX78002 , respectively, as shown in Figure 9. In this paper, we select these accelerators because they provide open-source tools for thorough analysis and modification of their internal processes, making them the most widely used tiny AI accelerator research platforms .

Model training and deployment.In our experiment, we use four models officially supported in the Analog Devices MAX78000/78002 Training framework : SimpleNet , WideNet ,

Figure 6: The initial CNN layer’s operation with DEX.

EfficientNetV2 , and MobileNetV2 . The supported models from the framework were trained via quantization-aware training with 8-bit integers in PyTorch . We follow the official training configuration (details in Appendix A.2). The checkpoints are synthesized as embedded C codes for via the Analog Devices MAX78000/70002 Synthesis framework . SimpleNet and WideNet are developed for MAX78000 while EfficientNetV2 and MobileNetV2 are for MAX78002 considering the size of the models. All models are originally designed to take \(3 32 32\) inputs, and DEX increases the number of the channels in the first layer to 64.

Datasets.We evaluated on four common vision datasets: (1) ImageNet , a ten-class subset of ImageNet  with 9469/3925 train/test samples with the original image shape of \(3 350 350\), (2) Caltech101  with 101 objects classes having 6941/1736 train/test samples with the original image shape of \(3 300 300\), (3) Caltech256  with 256 objects classes having 23824/5956 train/test samples with the original image shape of \(3 300 300\), and (4) Food101  with 101 food categories with 75750/25250 train/test samples with the original image shape of \(3 512 512\).

Baselines.For baselines, we compare with the Downsampling method which is a straightforward way to reduce the size of the input under memory-constrained devices. It downsamples the input image to \(3 32 32\). In addition, we compare DEX with CoordConv  which pointed out the limitation of traditional CNNs that relied on RGB images for the coordinate transformation problem and introduced the augmentation of \(i\) and \(j\) coordinates, which improved object detection efficiency by using two extra channels. The authors of CoordConv also introduced the third channel for an \(r\) coordinate, where \(r=+(j-w/2)^{2}}\), which they found effective in some experiments.

### Result

Overall accuracy.Table 1 shows the overall accuracy for four different datasets with the baselines and DEX. As shown, extending data channels to utilize additional input information improves accuracy in DEX. Specifically, DEX achieved 3.5%p higher accuracy compared to downsampling and 3.6% higher accuracy compared to CoordConv across datasets. CoordConv shows lower accuracy compared with downsampling (0.1%p degradation on average), showing they are not very effective solutions. This finding aligns with previous results indicating that CoordConv is useful for specific tasks such as object detection, where coordinate information is important . We found CoordConv (r) has a similar pattern to CoordConv. Overall, DEX's accuracy improvement shows the effectiveness of using extra information from the original image for feature learning.

Resource usage.Table 2 compares the resource usage of the baseline and DEX. First, we found that, although DEX extends the number of channels in the first CNN layer to 64, its impact on the

  
**Dataset** & **Method** & **SimpleNet** & **WideNet** & **EfficientNetV2** & **MobileNetV2** & **AVG (\%)** \\   & Downsampling & \(57.8 1.2\) & \(61.8 0.2\) & \(51.3 0.5\) & \(62.0 0.7\) & \(58.2\) \\  & CoordConv & \(58.0 1.1\) & \(61.7 0.2\) & \(51.9 0.1\) & \(61.6 0.3\) & \(58.3\) \\  & CoordConv (r) & \(55.4 1.5\) & \(61.4 0.2\) & \(51.7 1.0\) & \(61.2 1.1\) & \(57.4\) \\  & **DEX (ours)** & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Downsampling & \(54.6 2.1\) & \(55.8 1.2\) & \(38.6 0.9\) & \(51.4 1.6\) & \(50.1\) \\  & CoordConv & \(53.8 1.6\) & \(56.5 0.1\) & \(38.7 0.2\) & \(49.8 0.5\) & \(49.7\) \\  & CoordConv (r) & \(52.7 0.5\) & \(56.0 1.7\) & \(38.2 1.0\) & \(49.7 1.2\) & \(49.1\) \\  & **DEX (ours)** & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Downsampling & \(19.8 0.6\) & \(20.8 0.5\) & \(14.7 0.4\) & \(22.4 1.0\) & \(19.4\) \\  & CoordConv & \(19.8 0.5\) & \(21.3 0.8\) & \(14.8 0.8\) & \(22.7 0.8\) & \(19.6\) \\  & CoordConv (r) & \(20.0 1.6\) & \(20.9 0.6\) & \(14.5 0.3\) & \(22.7 0.4\) & \(19.5\) \\  & **DEX (ours)** & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Downsampling & \(16.0 0.4\) & \(17.7 0.7\) & \(12.1 0.2\) & \(22.4 0.6\) & \(17.1\) \\  & CoordConv & \(16.1 0.8\) & \(17.7 0.3\) & \(12.0 0.1\) & \(21.7 0.3\) & \(16.9\) \\   & CoordConv (r) & \(16.3 0.4\) & \(17.3 0.6\) & \(12.0 0.6\) & \(20.9 0.3\) & \(16.6\) \\   & **DEX (ours)** & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Average classification accuracy (%) and corresponding standard deviations over three runs for each dataset and method. Bold type indicates those of the highest classification accuracy.

model size is negligible (an average increment of 3.2% compared to no channel extension). DEX utilizes 21.3\(\) more image information compared to downsampling, which is the primary reason for the accuracy improvement. As expected, DEX does not increase on-device inference latency, even though it maximally utilizes the processors on the AI accelerators for information processing. This result is consistent across the four datasets, as all the models are designed to take the same input size in MAX78000 and MAX78002.

Accuracy according to the channel size.We varied the size of the channels from 3 (downsampling) to 6, 18, 36, and 64 with DEX to understand the impact of the channel size in terms of accuracy. Figure 7 shows the accuracy variation according to the channel size across the four datasets. As shown, it seems that a higher number of channels increases accuracy in general. This means that selecting the highest channel size supported in AI accelerators might be an effective strategy in practice, considering that it does not incur the latency increase. Still, there are some cases where the accuracy of the highest channel size (64) is not the best among them. This means there might be an optimal number of channels tailored to a specific dataset and model architecture, which might be found in the model development process.

Resource usage according to the channel size.We also measure resource usage varying the channel size. First, we measured the model size and inference latency as shown in Table 3. The model size increment is negligible and inference latency remains the same across different numbers of channels. The model size and inference latency are the same for the four datasets as all the models are designed to take the same input size in MAX78000 and MAX78002. Second, we measure the information utilization from the original image and processor utilization in the AI accelerators (Figure 8).

  
**Model** & **Method** & **InputChan** & **Size (KB)** & **InfoRatio (\(\))** & **ProCUtil (\%)** & **Latency (\( s\))** \\   & Downsampling & 3 & 162.6 & 1.0 & 4.7 & \(2592 1\) \\  & CoordConv & 5 & 162.9 & 1.0 & 7.8 & \(2592 2\) \\  & CoordConv (\(r\)) & 6 & 163.0 & 1.0 & 9.4 & \(2592 2\) \\  & **DEX (ours)** & 64 & 171.2 & 21.3 & 100.0 & \(2591 1\) \\   & Downsampling & 3 & 306.4 & 1.0 & 4.7 & \(3820 1\) \\  & CoordConv & 5 & 306.9 & 1.0 & 7.8 & \(3820 0\) \\  & CoordConv (\(r\)) & 6 & 307.1 & 1.0 & 9.4 & \(3819 1\) \\  & **DEX (ours)** & 64 & 319.3 & 21.3 & 100.0 & \(3818 1\) \\   & Downsampling & 3 & 742.4 & 1.0 & 4.7 & \(11688 2\) \\  & CoordConv & 5 & 743.0 & 1.0 & 7.8 & \(11685 3\) \\  & CoordConv (\(r\)) & 6 & 743.2 & 1.0 & 9.4 & \(11689 1\) \\  & **DEX (ours)** & 64 & 759.6 & 21.3 & 100.0 & \(11690 2\) \\   & Downsampling & 3 & 1317.8 & 1.0 & 4.7 & \(3553 4\) \\  & CoordConv & 5 & 1318.2 & 1.0 & 7.8 & \(3554 1\) \\   & CoordConv (\(r\)) & 6 & 1318.4 & 1.0 & 9.4 & \(3554 2\) \\   & **DEX (ours)** & 64 & 1330.7 & 21.3 & 100.0 & \(3552 3\) \\   

Table 2: Model size (Size), utilization of the original image information (InfoRatio), accelerator’s processor utilization for the first layer (ProcUtil), and inference latency on the accelerator (Latency) for different models and methods averaged over three runs.

Figure 7: Accuracy of DEX varying the channel size. The shaded areas are standard deviations.

The utilization of the original image information depends on the size of the original data size, which grows linearly according to the channel size. We found a correlation between information utilization rate and accuracy improvement. For example, Caltech101 and Caltech256 had utilization rates of 24.3%, improving accuracy by 4.2%p and 3.2%p, respectively, while Food101 had an 8.3% utilization rate with a 2.7%p accuracy improvement. The processor utilization linearly increases until 100% with 64 channels size, which is the number of parallel processing units in the evaluated platforms.

Comparison of alternative data extension strategies in DEX.To understand the effectiveness of our patch-wise even sampling and channel-wise stacking, we compared DEX with other possible data channel extension strategies. We compared with four strategies: repeating the same downsampled image across the channels (Repetition), generating slightly different images through rotation (Rotation), dividing the original image into multiple tiles and stacking those tiles across channels (Tile), patch-wise sequential sampling (Patch-wise seq.) that samples pixels sequentially within a patch, and patch-wise random sampling (Patch-wise rand.) that randomly samples within a patch. Further implementation details are in Appendix A.5. In this experiment, we used SimpleNet and evaluated it on ImageNette. Table 4 shows the results. Repetition does not improve accuracy over downsampling, indicating that merely increasing the number of kernels does not lead to performance gains. Rotation shows a slight decrease in accuracy compared to Repetition, which suggests that slight changes through rotation do not enhance performance. Interestingly, Tile shows low accuracy, demonstrating the importance of having a complete view of the original image in each channel, rather than focusing on specific regions. Both Patch-wise sequential and Patch-wise random samplings show lower accuracy than DEX's patch-wise even sampling, highlighting the importance of even sampling for better performance.

## 5 Related work

TinyML.Tiny Machine Learning (TinyML) is an emerging field that focuses on adapting machine learning techniques for highly resource-constrained devices, such as microcontroller units (MCUs). These devices often come with limited memory, typically hundreds of kilobytes of SRAM. Research in this area has mostly concentrated on reducing model size through various compression techniques, such as model pruning [15; 17; 25; 27; 31; 32], model quantization [7; 15; 42; 44; 49; 53; 54], and neural architecture search (NAS) [4; 5; 10; 24]. In addition, several studies have explored the

    &  &  &  \\  Downsampling & 3 & 1.0 & \(57.8 1.2\) \\ Repetition & 64 & 1.0 & \(56.3 0.8\) \\ Rotation & 64 & 1.0 & \(55.4 0.7\) \\ Tile per channel & 64 & 21.3 & \(39.4 0.7\) \\ Patch-wise seq. & 64 & 21.3 & \(61.0 1.5\) \\ Patch-wise rand. & 64 & 21.3 & \(60.4 1.0\) \\
**DEX** & 64 & 21.3 & \(\) \\   

Table 4: Comparison of data extension strategies.

Figure 8: Resource usage varying the channel size.

    & **Model** & **Chan = 3** & **Chan = 6** & **Chan = 18** & **Chan = 36** & **Chan = 64** \\   & SimpleNet & 162.6 & 163.0 (+0.3\%) & 164.7 (+1.3\%) & 167.3 (+2.9\%) & 171.2 (+5.3\%) \\  & WideNet & 306.4 & 307.1 (+0.2\%) & 309.6 (+1.0\%) & 313.4 (+2.3\%) & 319.3 (+4.2\%) \\  & EfficientNetV2 & 742.4 & 743.2 (+0.1\%) & 746.6 (+0.6\%) & 751.7 (+1.3\%) & 759.6 (+2.3\%) \\  & MobileNetV2 & 1317.8 & 1318.4 (+0.0\%) & 1321.0 (+0.2\%) & 1324.8 (+0.5\%) & 1330.7 (+1.0\%) \\   & SimpleNet & 2592 \(\) 1 & 2592 \(\) 2 & 2591 \(\) 1 & 2590 \(\) 1 & 2591 \(\) 1 \\  & WideNet & 3820 \(\) 1 & 3820 \(\) 2 & 3825 \(\) 1 & 3819 \(\) 3 & 3818 \(\) 1 \\   & EfficientNetV2 & 11688 \(\) 2 & 11691 \(\) 2 & 11692 \(\) 3 & 11691 \(\) 0 & 11690 \(\) 2 \\   & MobileNetV2 & 3553 \(\) 4 & 3553 \(\) 1 & 3552 \(\) 1 & 3554 \(\) 0 & 3552 \(\) 3 \\   

Table 3: Model size (Size) with relative increment (%) compared to the three channels and average inference latency on the accelerator (Latency) with standard deviations over three runs, varying the channel size.

efficient utilization of memory resources (e.g., SRAM). Examples include optimizing on-device training processes [23; 28] and designing memory-efficient neural architectures [26; 52]. Unlike these approaches that primarily target MCUs, our research utilizes the distinctive architecture of tiny AI accelerators to enhance both memory efficiency and overall performance.

Tiny AI accelerators.Several studies have leveraged tiny AI accelerators for small-scale on-device AI applications. For instance, TinyissimoYOLO  offers a quantized, memory-efficient, and ultra-lightweight object detection network, showcasing its effectiveness on the MAX78000 platform. Additionally, KP2DTiny  introduces a quantized neural keypoint detector and descriptor specifically optimized for MAX78000 and Coral AI accelerators. Moreover, Synergy represents a multi-device collaborative inference platform across wearables equipped with tiny AI accelerators . Another line of studies utilized tiny AI accelerators in battery-free or intermittent computing scenarios [1; 6]. Traditionally, hardware accelerators on low-power AI platforms were capable of only one-shot atomic executions of a neural network inference without intermediate result backups. A study proposed a toolchain to address this that allows neural networks to execute intermittently on the MAX78000 platform . To the best of our knowledge, there has been no work that manipulates data and models to efficiently utilize computing resources considering the unique architecture of tiny AI accelerators.

Image channel extension in CNNs.Several studies have explored augmenting images with additional information to construct multi-channel inputs for Convolutional Neural Networks (CNNs). Liu et al. proposed a multi-modality image fusion approach, combining visible, mid-wave infrared, and motion images for enhanced object detection , while Wang et al. presented depth-aware CNN for image segmentation . These approaches require extra sensing channels to acquire data, such as infrared cameras and depth cameras. Similarly, other research has incorporated location data to improve performance for segmentation  and object detection tasks . For instance, CoordConv  pointed out the limitation of traditional CNNs that relied solely on RGB images for the coordinate transformation problem and introduced the augmentation of \(i\) and \(j\) coordinates, which improved object detection efficiency. However, these methodologies often necessitate additional sensor modalities or are tailored for specific applications such as object detection, which restricts their general use. Nevertheless, adapting findings from those studies within DEX could be an interesting future direction.

## 6 Discussion and conclusion

We introduced DEX, a novel method to enhance CNN efficiency on tiny AI accelerators by augmenting input data across unused memory. Evaluations on four image datasets and models showed that DEX improves accuracy without increasing inference latency. This method maximizes the processing and memory capabilities of tiny AI accelerators, making it a promising solution for efficient AI model execution on resource-constrained devices.

Limitations and potential societal impacts.We modified only the initial CNN layer due to simplicity, effectiveness, and memory constraints. The first layer, representing image data in three channels (RGB), has the most unused processors after initial data assignment. Extending channels at the first layer significantly increases data utilization with minimal impact on model size. This approach aligns with the design of weight memory in tiny AI accelerators, which maximizes model capacity by collective use across processors. We think DEX might be less effective in certain tasks where incorporating more pixel information is not beneficial. In those cases, alternative data extension strategies might be used instead of patch-wise even sampling to utilize the additional channel budget. While our focus was on small models supported by the MAX78000/MAX78002 platforms, evaluating larger models could be valuable, given rapid AI hardware advancements. Regarding societal impact, leveraging additional processors and memory to improve accuracy might increase carbon emissions , highlighting the need to balance accuracy improvements with environmental sustainability.