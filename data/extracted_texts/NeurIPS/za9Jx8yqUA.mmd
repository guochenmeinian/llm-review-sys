# GenRL: Multimodal-foundation world models

for generalization in embodied agents

 Pietro Mazzaglia

IDLab, Ghent University

&Tim Verbelen

VERSES AI Research Lab

&Bart Dhoedt

IDLab, Ghent University

&Aaron Courville

Mila, University of Montreal

&Sai Rajeswar

ServiceNow Research

Work done while interning at Mila/ServiceNow Research. Email: pietro.mazzaglia@ugent.be

###### Abstract

Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain's dynamics, and learn the corresponding behaviors in imagination. As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models.

**Website, code and data:** mazpie.github.io/genrl

Figure 1: _Multimodal-foundation world models_ connect and align the video-language space of a foundation model with the latent space of a generative world model for reinforcement learning, requiring vision-only data. Our _GenRL_ framework turns visual and/or language prompts into latent targets and learns to realize the corresponding behaviors by training in the world modelâ€™s imagination.

Introduction

Foundation models are large pre-trained models endowed with extensive knowledge of the world, which can be readily adapted for a given task . These models have demonstrated extraordinary generalization capabilities in a wide range of vision [28; 45; 67] and language tasks [43; 19; 53; 11]. As we aim to extend this paradigm to embodied applications, where agents physically interact with objects and other agents in their environment, we require generalist agents that are capable of reasoning about these interactions and executing action sequences within these settings .

Reinforcement learning (RL) allows agents to learn complex behaviors from visual and/or proprioceptive inputs [18; 26; 27] by maximizing a specified reward function. Scaling up RL to multiple tasks and embodied environments remains challenging as designing reward functions is a complicated process, requiring expert knowledge and prone to errors which can lead to undesired behaviors . Recent work has proposed the adoption of visual-language models (VLMs) to specify rewards for visual environments using language [4; 48; 39], e.g. using the similarity score computed by CLIP  between an agent's input images and text prompts. However, these approaches mostly require fine-tuning of the VLM , otherwise, they tend to work reliably only in a few visual settings .

In most RL settings, we lack multimodal data to train or fine-tune domain-specific foundation models, due to the costs of labelling agents' interactions and/or due to the intrinsic unsuitability of some embodied contexts to be converted into language. For instance, in robotics, it's non-trivial to convert a language description of a task to the agent's actions which are hardware-level controls, such as motor currents or joint torques. These difficulties make it hard to scale current techniques to large-scale generalization settings, leaving open the question:

_How does one effectively leverage foundation models for generalization in embodied domains?_

In this work, we present GenRL, a novel approach requiring no language annotations that allows training agents to solve multiple tasks from visual or language prompts. GenRL learns multimodal-foundation world models (MFWMs), where the joint embedding space of a foundation video-language model  is connected and aligned with the representation of a generative world model for RL , using only vision data. The MFWM allows the specification of tasks by grounding language or visual prompts into the embodied domain's dynamics. Then, we introduce an RL objective that enables learning to accomplish the specified tasks in imagination , by matching the prompts in latent space.

Compared to previous work in world models and VLMs for RL, one emergent property of GenRL is the possibility to generalize to new tasks in a completely data-free manner. After training the MFWM, it possesses both strong priors over the dynamics of the environment, and large-scale multimodal knowledge. This combination enables the agent to interpret a large variety of task specifications and learn the corresponding behaviors. Thus, analogously to foundation models for vision and language, GenRL allows generalization to new tasks without additional data and lays the groundwork for foundation models in embodied RL domains .

## 2 Preliminaries and background

Additional related works can be found in Appendix A.

**Problem setting.** The agent receives from the environment observations \(x\) and interacts with it through actions \(a\). In this work, we focus on visual reinforcement learning, so observations are images of the environment. The objective of the agent is to accomplish a certain task \(\), which can be specified either in the observation space \(x_{}\), e.g. through images or videos, or in language space \(y_{}\), where \(\) represents the space of all possible sentences. Crucially, compared to a standard RL setting, we do not assume that a reward signal is available to solve the task. When a reward function exists, it is instead used to evaluate the agent's performance.

**Generative world models for RL.** In model-based RL, the optimization of the agent's actions is done efficiently, by rolling out and scoring imaginary trajectories using a (learned) model of the environment's dynamics. In recent years, this paradigm has grown successful thanks to the adoption of generative world models, which learn latent dynamics by self-predicting the agent's inputs . World models have shown impressive performance in vision-based environments ,improving our ability to solve complex and open-ended tasks . Generative world models have been successfully extended to many applications, such as exploration , skill learning , solving long-term memory tasks , and robotics [58; 16].

**Foundations models for RL.** Large language models (LLMs) have been used for specifying behaviors using language [41; 29; 56; 59], but this generally assumes the availability of a textual interface with the environment or that observations and/or actions can be translated to the language domain. The adoption of vision-language models (VLMs) reduces these assumptions, as it allows the evaluation of behaviors in the visual space. However. this approach has yet to show robust performance, as it generally requires fine-tuning of the VLM [4; 15], prompt hacking techniques  or visual modifications to the environment .

**Vision-language generative modelling.** Given the large success of image-language generative models , recent efforts in the community have focused on replicating and extending such success to the video domain, where the temporal dimension introduces new challenges, such as temporal consistency and increased computational costs [30; 3]. Video generative models are similar to world models for RL, with the difference that generation models outputs are typically not conditioned on actions, but rather conditioned on language  or on nothing at all (i.e. an unconditional model).

## 3 GenRL

### World models for RL

GenRL learns a task-agnostic world model representation by modelling the sequential dynamics of the environment in a compact discrete latent space \(S\)[24; 26]. Latent states \(s S\) are sampled from independent categorical distributions. The gradients for training the model are propagated through the sampling process with straight-through estimation .

The world model is made of the following components:

\[&q_{}(s_{t}|x_{t}),&&h_{t}=f_{}(s_{t-1},a_{t-1},h_{t-1}),\\ &p_{}(x_{t}|s_{t}),&&p_{}(s_{t}|h_{t}), \]

trained with the loss:

\[_{}=_{t}}q_{}(s_{t}|x_{ t})\|p_{}(s_{t}|s_{t-1},a_{t-1})}_{}-_{q_{}(s_{t}|x_{t})}[ p_{}(x_{t}|s_{ t})]}_{},\] (1)

where \(p_{}(s_{t}|s_{t-1},a_{t-1})\) is a shorthand for \(p_{}(s_{t}|f_{}(s_{t-1},a_{t-1},h_{t-1}))\). The sequence model is implemented as a linear GRU cell . Differently from recurrent state space models (RSSM; ),

Figure 2: _Overview of GenRL._ The agent learns a multimodal-foundation world model that connects and aligns (a) the representation of a foundation VLM with the latent states of a generative world model. Given a certain task prompt, (b) the model allows embedding the task and translating into targets in the latent dynamics space, which the agent can learn to achieve by using RL in imagination.

for our framework, encoder and decoder models are not conditioned on the information present in the sequence model. This ensures that the latent states only contain information about a single observation, while temporal information is stored in the hidden state of the sequence model. Given the simpler encoder-decoder strategy of our model, the encoder can be seen as a probabilistic visual tokenizer, which is grounded in the target embodied environment .

### Multimodal-foundation world models

Multimodal VLMs are large pre-trained models that have the following components:

\[ e^{(v)}=f^{(v)}_{}(x_{t:t+k}),  e^{(l)}=f^{(l)}_{}(y),\]

where \(x_{t:t+k}\) is a sequence of visual observations and \(y\) is a text prompt. For video-language models, \(k\) is generally a constant number of frames (e.g. \(k\{4,8,16\}\) frames). Image-language models are a special case where \(k=1\) as the vision embedder takes a single frame as an input. For our implementation, we adopt the InterWide2 video-language model  (with \(k\)=8).

To connect the representation of the multimodal foundation VLM with the world model latent space, we instantiate two modules: a _latent connector_ and a _representation aligner_:

\[ p_{}(s_{t:t+k}|e),_{ }=_{t}D_{}p_{}(s_{t}|s_{t-1},e)\| (q_{}(s_{t}|x_{t})),\] \[ e^{(v)}=f_{}(e^{(l)}), _{}= \|e^{(v)}-f_{}(e^{(l)})\|_{2}^{2},\]

where \(()\) indicates to stop gradients propagating.

The connector learns to predict the latent states of the world model from embeddings in the VLM's representation space. The connector's objective consists of minimizing the KL divergence between its predictions and the world model's encoder distribution. While more expressive architectures, such as transformers  or state-space models  could be adopted, we opt for a simpler GRU-based architecture for video modelling. This way, we keep the method simple and the architecture of the connector is symmetric with respect to the world model's components.

**Aligning multimodal representations.** The connector learns to map visual embeddings from the pretrained VLM to latent states of the world model. When learning the connector from visual embeddings \(e^{(v)}\), we assume it can generalize to the (theoretical) corresponding language embedding \(e^{(l)}\) if the angle \(\) between the two embeddings is small enough, as shown in Fig. 2(a). This can be expressed as \(>c\) or \(< c\), with \(c\) a small positive constant .

Multimodal VLMs trained with contrastive learning exhibit a _multimodality gap_, where the spherical embeddings of different modalities are not aligned. Given a dataset of vision-language data, this projective function can be learned. However, in embodied domains vision-language data is typically unavailable. Thus, we have to find a way to align the representations using _no language annotations_.

Previous methods inject the noise into the vision embeddings during training [66; 68]. This leads to the situation shown in Figure 2(b), where \(c\) grows larger with the noise. This allows language embeddings to be close enough to their visual counterparts.

In our work, we instead learn an aligner network, which maps points surrounding \(e^{(v)}\) closer to \(e^{(v)}\). As represented in Fig. 2(c), this way, \(c\) is unaltered but the aligner will map \(e^{(l)}\) close enough to \(e^{(v)}\). Since we use noise to sample points around \(e^{(v)}\) the aligner model can be trained using vision-only data and thus, no language annotations.

Figure 3: When training the connector on (a) the VLMâ€™s representation we can address the multimodality gap in multiple ways: (b) prior works adopt noise during the training of the connector, (c) we adopt an aligner network that learns to map points in proximity of the visual embedding close the corresponding embedding.

The aligner allows us to train a noise-free connector, which has two main advantages: (i) it yields higher prediction accuracy for visual embedding inputs while maintaining a similar alignment for language embedding inputs; and (ii) it is more flexible; it's easier to re-train/adapt for different noise levels, as it only requires re-training the aligner module, and its use can be avoided if unnecessary.

### Specifying and learning tasks in imagination

World models can be used to imagine trajectories in latent space, using the sequential and dynamics models. This allows us to train behavior policies in a model-based RL fashion . Given a task specified through a visual or language prompt, our MFWM can generate the corresponding latent states by turning the embedder's output, \(e_{}\), into sequences of latent states \(s_{t:t+k}\) (decoded examples are shown in Figure 1). The objective of the policy model \(_{}\) is then to match the goals specified by the user by performing trajectory matching.

The trajectory matching problem can be solved as a divergence minimization problem , between the distribution of the states visited by the policy \(_{}\) and the trajectory generated using the aligner-connector networks from the user-specified prompt:

\[=_{}_{a_{t}_{}(s_{t})} _{t}^{t}p_{}(s_{t+1}|s_{t},a_{t})\|p_{ }(s_{t+1}|e_{}), e_{}=f_{}().\] (2)

The KL divergence is a natural candidate for the distance function . However, in practice, we found that using the cosine distance between linear projections of the latent states notably speeds up learning and enhances stability. We can then turn the objective in Eq. 2 into a reward for RL:

\[r_{}=g_{}(s_{t+1}^{}),g_{}(s_{t+1}^{ }), s_{t+1}^{} p_{} (s_{t+1}|s_{t},a_{t}),s_{t+1}^{} p_{}(s_{t+1}|e_{}),\] (3)

where \(g_{}\) represents the first linear layer of the world model's decoder. We train an actor-critic model to maximize this reward and achieve the tasks specified by the user . Additional implementation details are provided in Appendix B.

**Temporal alignment.** One issue with trajectory matching is that it assumes that the distribution of states visited by the agent starts from the same state as the target distribution. However, the initial state generated by the connector may differ from the initial state where the policy is currently in. For example, consider the Stickman agent on the right side of Figure 1. If the agent is lying on the ground and tasked to run, the number of steps to get up and reach running states may surpass the temporal span recognized by the VLM (e.g. in our case 8 frames), causing disalignment in the reward.

To address this initial condition alignment issue, we propose a _best matching trajectory_ technique, inspired by best path decoding in speech recognition . Our technique involves two steps:

1. We compare the first \(b\) states of the target trajectory with \(b\) states obtained from the trajectories imagined by the agent by sliding along the time axis. This allows one to find at which timestep \(t_{a}\) the trajectories are best aligned (the comparison provides the highest reward).
2. We align the temporal sequences in the two possible contexts: (a) if a state from the agent sequence comes before \(t_{a}\), the reward uses the target sequence's initial state; and (b) if the state comes \(k\) steps after \(t_{a}\), it's compared to the \(s_{t+k}\) state from the target sequence.

In all experiments, we fix \(b=8\) (number of frames of the VLM we use ), which we found to strike a good compromise between comparing only the initial state (\(b=1\)) and performing no alignment (\(b=\) imagination horizon). An ablation study can be found in Appendix E.

## 4 Experiments

Overall, we employ a set of 4 locomotion environments (Walker, Cheetah, Quadruped, and a newly introduced Stickman environment)  and one manipulation environment (Kitchen) , for a total of 35 tasks where the agent is trained without rewards, using only visual or language prompts. Details about the datasets, tasks, and prompts used can be found in the Appendix C.

### Offline RL

In offline RL, the objective of the agent is to learn to extract a certain task behavior from a given fixed dataset . The performance of the agent generally depends on its ability to'retrieve' the correct behaviors in the dataset and interpolate among them. Popular techniques for offline RL include off-policy RL methods, such as TD3 , advantage-weighted behavior cloning, such as IQL , and behavior-regularized approaches, such as CQL  or TD3+BC .

We aim to assess the multi-task capabilities of different approaches for designing rewards using VLMs. We collected large datasets for each of the domains evaluated, containing a mix of structured data (i.e. the replay buffer of an agent  learning to perform some tasks) and unstructured data (i.e. exploration data collected using ). The datasets contain **no reward information** and **no text annotations** of the trajectories. The rewards for training for a given task must be inferred by the agent, i.e. using the cosine similarity between observations and the given prompt or, in the case of GenRL, using our reward formulation (Eq. 3).

We compare GenRL to two main categories of approaches:

* _Image-language rewards_: following , the cosine similarity between the embedding for the language prompt and the embedding for the agent's visual observation is used as a reward. For the VLM, we adopt the SigLIP-B  model as it's reported to have superior performance than the original CLIP .
* _Video-language rewards_: similar to the image-language rewards, with the difference that the vision embedding is computed from a video of the history of the last \(k\) frames, as done in . For the VLM, we use the InternVideo2 model , the same used for GenRL.

The evaluation compares GenRL to various offline RL methods from the literature, including IQL, TD3+BC, and TD3. We also introduce a model-based baseline, _WM-CLIP_. This baseline is the antithesis of GenRL as, rather than learning a connector and an aligner, it learns a "reversed connector". This module learns to predict VLM embeddings from the world model states (GenRL does the opposite). This makes it possible to compute rewards in imagination in a similar way to the model-free baselines, by computing the cosine similarity between the visual embeddings predicted from imagined states and the task's language embeddings.

All methods are trained for 500k gradient steps, and evaluated on 20 episodes. For each task, model-free agents require training the agent from scratch, including the visual encoder, actor, and critic networks on the entire dataset. Model-based agents require training the model once for each domain and then training an actor-critic for each task. Other training and baseline details are reported in Appendix D.

    &  &  & GenRL \\   & IQL & THD3+BC & THD3 & WM-CLIP & IQL & TD3+BC & TD3 & WM-CLIP & GenRL \\  walker stand & \(0.67 0.03\) & \(0.92 0.02\) & \(0.93 0.03\) & \(1.01 0.0\) & \(0.66 0.05\) & \(0.64 0.03\) & \(1.01 0.0\) & \(0.94 0.01\) & \(1.02 0.0\) \\ walker run & \(0.24 0.03\) & \(0.27 0.01\) & \(0.09 0.02\) & \(0.05 0.02\) & \(0.29 0.02\) & \(0.24 0.02\) & \(0.35 0.01\) & \(0.7 0.01\) & \(0.77 0.02\) \\ walker walk & \(0.41 0.05\) & \(0.34 0.05\) & \(0.14 0.0\) & \(0.21 0.01\) & \(0.4 0.03\) & \(0.44 0.03\) & \(0.88 0.02\) & \(0.91 0.02\) & \(1.01 0.0\) \\  cheetah run & \(0.41 0.05\) & \(0.0 0.01\) & -\(0.01 0.0\) & -\(0.0 0.0\) & \(0.15 0.02\) & -\(0.01 0.0\) & \(0.37 0.01\) & \(0.56 0.03\) & \(0.74 0.01\) \\  quadruped stand & \(0.56 0.02\) & \(0.64 0.04\) & \(0.65 0.04\) & \(0.97 0.0\) & \(0.52 0.06\) & \(0.43 0.05\) & \(0.61 0.05\) & \(0.97 0.0\) & \(0.97 0.0\) \\ quadruped run & \(0.3 0.03\) & \(0.28 0.02\) & \(0.24 0.02\) & \(0.27 0.0\) & \(0.38 0.03\) & \(0.25 0.02\) & \(0.26 0.01\) & \(0.61 0.02\) & \(0.86 0.02\) \\ quadruped walk & \(0.26 0.02\) & \(0.31 0.02\) & \(0.28 0.01\) & \(0.47 0.02\) & \(0.32 0.02\) & \(0.28 0.04\) & \(0.28 0.02\) & \(0.92 0.01\) & \(0.93 0.01\) \\  stickman stand & \(0.45 0.06\) & \(0.58 0.04\) & \(0.06 0.04\) & \(0.71 0.02\) & \(0.43 0.04\) & \(0.45 0.05\) & \(0.08 0.02\) & \(0.32 0.01\) & \(0.7 0.02\) \\ stickman walk & \(0.4 0.04\) & \(0.48 0.04\) & \(0.18 0.01\) & \(0.23 0.01\) & \(0.51 0.02\) & \(0.46 0.03\) & \(0.41 0.02\) & \(0.65 0.05\) & \(0.83 0.01\) \\ stickman run & \(0.2 0.01\) & \(0.22 0.02\) & \(0.03 0

**Language-to-action in-distribution.** We want to verify whether the methods can retrieve the task behaviors that are certainly present in the training data, when specifying the task only through language. We present results in Table 1, with episodic rewards rescaled so that 0 represents the performance of a random agent, while 1 represents the performance of an expert agent.

GenRL excels in overall performance across all domains and tasks, outperforming other methods particularly in dynamic tasks like walking and running in the quadruped and cheetah domains. However, in some static tasks of the kitchen domain, other methods occasionally outperform GenRL. This can be explained by the fact that the target sequences that GenRL infers from the prompt are often slightly in motion, even in static cases. To address this, we could set the target sequence length to 1 for static prompts, but we opted to maintain the method's simplicity and generality, acknowledging this as a minor limitation.

As expected, video-language rewards tend to perform better than image-language rewards for dynamic tasks. The less conservative approach, TD3, performs better than the other model-free baselines in most tasks, similarly to what is shown in . The model-based baseline's performance, WM-CLIP-V, is the closest to GenRL's.

**Language-to-action generalization.** To assess multi-task generalization, we defined a set of tasks not included in the training data. Although we don't anticipate agents matching the performance of expert models, higher scores in this benchmark help gauge the generalization abilities of different methods. We averaged the performance across various tasks for each domain and summarized the findings in Figure 4, with detailed task results in Appendix K.

Overall, we observe a similar trend as for the in-distribution results. GenRL significantly outperforms all model-free approaches, especially in the quadruped and cheetah domains, where the performance is close to the specialized agents' performance. Both for image-language (-I in the Figure) and video-language (-V in the Figure) more conservative approaches, such as IQL and TD3+BC tend to perform worse. This could be associated with the fact that imitating segments of trajectories is less likely to lead to high-rewarding trajectories, as the tasks are not present in the training data.

Figure 4: _Language-to-action generalization._ Offline RL from language prompts on tasks that are not deliberately included in the training dataset. Performance averaged over 10 seeds and standard error was reported with black lines. Detailed results per task in Appendix K.

Figure 5: _Video-to-action._ GenRL allows grounding video prompts into the target environmentâ€™s dynamics. It allows visualization of the modelâ€™s interpretation of the prompts, using the decoder (top row), and it allows turning prompts into behaviors, leading to generally higher performance than other approaches. 10 seeds. Additional visualizations on the project website.

Video-to-action.While language strongly simplifies the specification of a task, in some cases providing visual examples of the task might be easier. Similarly as for language prompts, GenRL allows grounding visual prompts (short videos) into the embodied domain's dynamics and then learning of the corresponding behaviors.

In Figure 5, we provide behavior learning results from video prompts. The tasks included are of static and dynamic nature and span across 4 different domains. Visualizations of the videos used as prompts are available on the project website, where we also present a set of "grounded videos" generated by the model using the prompts (see snapshots at the top of Fig. 5). These can be obtained by inferring the latent targets corresponding to the vision prompts (left images, in the Figure) and then using the decoder model to decode reconstructed images (right images, in the Figure).

The results show a similar trend to the language prompts experiments and the performance when using video prompts is aligned to the language-to-action performance, for the same tasks. In general, we found it interesting that the VLM allows us to generalize to very different visual styles (drawings, realistic, AI-generated), very different camera viewpoints (quadruped, microwave), and different morphologies (cheetah tasks).

Summary.The experiments presented allow us to establish more clearly the main ingredients that contribute to the stronger performance of GenRL: (i) the video-language model helps in dynamic tasks, (ii) model-based algorithms lead to higher performance, (iii) the connection-alignment system presented generally outperforms the "reversed" way of connecting the two representations.

### Data-free policy learning

In the previous section, we evaluated several approaches for designing reward using foundation VLMs. Clearly, model-free RL approaches require continuous access to a dataset, to train the actor-critic and generalize across new tasks. Model-based RL can learn the actor-critic in imagination. However, in previous work [26; 24], imagining sequences for learning behaviors first requires processing actual data sequences. The data is used to initialize the dynamics model, and obtain latent states that represent the starting states to rollout the policy in imagination. Furthermore, in order to learn new tasks, reward-labelled data is necessary to learn a reward model, which provides rewards to the agent during the task learning process.

Foundation models  are generally trained on enormous datasets in order to generalize to new tasks. The datasets used for the model pretraining are not necessary for the downstream applications, and sometimes these datasets are not even publicly available [43; 19]. In this section, we aim to establish a new paradigm for foundation models in RL, which follows the same principle of foundation models for vision and language. We call this paradigm _data-free policy learning_ and we define it as the ability to generalize to new tasks, after pre-training, by learning a policy completely in imagination, with no access to data (not even to the pre-training dataset).

GenRL enables data-free policy learning thanks to two main reasons: the agent learns a task-agnostic MFWM on a large varied dataset during pre-training, and the MFWM enables the possibility of specifying tasks directly in latent space, without requiring any data. Thus, in order to learn behaviors in imagination, the agent can: (i) sample random latent states in the world model's representation,

Figure 6: By removing data dependencies on actions (on-policy learning in the modelâ€™s imagination), rewards (computed using only the prompt and latent states, using Eq. 3), and observations (by sampling latent states within the model), GenRL agents can be adapted for new tasks in a data-free fashion. Performance is averaged over 10 seeds and standard error is reported with black lines. Detailed results per task in Appendix K.

(ii) rollout sequences in imagination, following the policy's actions, and (iii) compute rewards, using the targets obtained by processing the given prompts with the connector-aligner networks.

In Figure 6, we provide a diagram that further clarifies the differences between training GenRL in an offline and in a data-free policy learning fashion. Then, we present results that compare data-free policy learning with offline RL baselines, as discussed in Section 4.1. While data-free policy learning shows a slight decrease in overall performance, its performance remains close to the original GenRL's performance and still outperforms other approaches. In Appendix K, we further show that the difference in performance is minimal across most domains, and data-free policy learning even performs better in the kitchen domain.

By employing data-free learning, after pre-training, agents can master new tasks without data. By requiring no CPU-GPU memory transfers of the data, data-free policy learning also reduces the training time of the policy, often allowing convergence within only 30 minutes of training. As we scale up foundation models for behavior learning, the ability to learn data-free will become crucial. Although very large datasets will be employed to train future foundation models, GenRL adapts well without direct access to original data, offering flexibility where data may be proprietary, licensed or unavailable.

### Analysis of the training data distribution

As demonstrated in Sections 4.1 and 4.2, after training on a large dataset, a GenRL agent can adapt to multiple new tasks without additional data. The nature of the training data, detailed in Appendix C, combines exploration and task-specific data. Thus, we ask ourselves what subsets of the data are the most important ones for GenRL's training.

To identify critical data types for GenRL, we trained different MFWMs on various dataset subsets. Then, we employ data-free behavior learning to train task behaviors for all tasks. We present an analysis over subsets of the walker dataset in Figure 7.

The results confirm that a diverse data distribution is crucial for task success, with the best performance achieved by using the complete dataset, followed by the varied exploration data. Task-specific data effectiveness depends on task complexity, for instance, 'run data' proves more useful and generalizable than 'walk data' or'stand data' across tasks. Crucially,'stand data', which shows minimal variation, limits learning for a general agent but can still manage simpler tasks like 'lying down' and'sitting on knees' as detailed in Appendix K.

Moving forward with training foundation models in RL, it will be essential to develop methods that extract multiple behaviors from unstructured data and accurately handle complex behaviors from large datasets. Thus, the ability of GenRL to primarily leverage unstructured data is a significant advantage for scalability.

Figure 7: _Training data distribution_. Analysing the impact of the training data distribution on the generalization performance of GenRL. Performance is obtained by training behaviors in data-free mode, after training the MFWM on different subsets of the training dataset. Performance averaged over 10 seeds (black lines indicate standard error). Full results in Appendix K.

Discussion

We introduced GenRL, a world-model based approach for grounding vision-language prompts into embodied domains and learning the corresponding behaviors in imagination. The multimodal-foundation world models of GenRL can be trained using unimodal data, overcoming the lack of multimodal data in embodied RL domains. The data-free behavior learning capacity of GenRL lays the groundwork for foundation models in RL that can generalize to new tasks without any data.

**A framework for behavior generation.** A common challenge with using LLMs and VLMs involves the need for prompt tuning to achieve specific tasks. As GenRL relies on a foundation VLM, similar to previous approaches  it is not immune from this issue. However, GenRL uniquely allows for the visualization of targets obtained from specific prompts. By decoding the latent targets, using the MFWM decoder, we can visualize the interpreted prompt before training the corresponding behavior. This enables a much more explainable framework, which allows fast iteration for prompt tuning, compared to previous (model-free) approaches which often require training the agent to identify which behaviors are rewarded given a certain prompt.

**Limitations.** Despite its strengths, GenRL presents some limitations, largely due to inherent weaknesses in its components. From the VLMs, GenRL inherits the issue related to the multimodality gap  and the reliance on prompt tuning. We proposed a connection-alignment mechanism to mitigate the former. For the latter, we presented an explainable framework, which facilitates prompt tuning by allowing decoding of the latent targets corresponding to the prompts. From the world model, GenRL inherits a dependency on reconstructions, which offers advantages such as explainability but also drawbacks, such as failure modes with complex observations. We further investigate this limitation in Appendix I and present other potential limitations in Appendix J.

**Future work.** As we strive to develop foundation models for generalist embodied agents, our framework opens up numerous research opportunities. One such possibility is to learn multiple behaviors and have another module, e.g. an LLM, compose them to solve long-horizon tasks. Another promising area of research is investigating the temporal flexibility of the GenRL framework. We witnessed that for static tasks, greater temporal awareness could enhance performance. This concept could also apply to actions that extend beyond the time comprehension of the VLM. Developing general solutions to these challenges could lead to significant advancements in the framework.