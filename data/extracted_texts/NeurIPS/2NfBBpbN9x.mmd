# Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series

Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series

 Ilan Naiman Nimrod Berman Itai Pemper Idan Arbiv Gal Fadlon Omri Azencot

Department of Computer Science

Ben-Gurion University of The Negev

{naimani, bermann, itaiepm, arbivid, galfad}@post.bgu.ac.il

azencot@cs.bgu.ac.il

Equal Contribution

###### Abstract

Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into images. By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature. We validate the effectiveness of our method through a comprehensive evaluation across multiple tasks, including unconditional generation, interpolation, and extrapolation. We show that our approach achieves consistently state-of-the-art results against strong baselines. In the unconditional generation tasks, we show remarkable mean improvements of \(58.17\%\) over previous diffusion models in the short discriminative score and \(132.61\%\) in the (ultra-)long classification scores. Code is at https://github.com/azencot-group/ImagenTime.

## 1 Introduction

Generative modeling of real-world information such as images , texts , and other types of data  has drawn increased attention recently. In this work, we focus on the setting of generative modeling (GM) of general time series information. There are several factors that govern the complexity required from sequential data generators including the sequence length, its number of features, the appearance of transient vs. long-range effects, and more. Existing generative models for time series are typically designed either for multivariate short-term sequences  or univariate long-range data , often resulting in separate and completely different neural network frameworks. However, a natural question arises: Can one develop a unified framework equipped to handle both high-dimensional short sequences and low-dimensional long time series?

Earlier approaches for processing time series based on recurrent neural networks (RNNs) handled short sequences well , however, modeling long-range dependencies turned out to be significantly more challenging. Particularly, RNNs suffer from the well-known vanishing and exploding gradient problem  that prevents them from learning complex patterns and long-range dependencies. To address long-context modeling and memory retention, extensive research is devoted to approaches such as long short-term memory (LSTM) models , unitary evolution RNNs and Lipschitz RNNs . A different approach for processing sequential information is based on the Transformer , eliminating any recurrent connections. Recent remarkable results have been obtained with transformers on natural language processing  and time series forecasting [96; 104; 68] tasks. Alas, transformers are underexplored as generative models for long-range time series data. This may be in part due to their computational costs that scale quadratically as \((L^{2})\) with the sequence length \(L\), and in part because transformer forecasters are inferior to linear tools .

Beyond RNNs and the Transformer, recent works have considered the state space model (SSM) for modeling long-range time series information. For instance, the structured SSM (S4)  employed a parameterization that reduced computational costs via evaluations of Cauchy kernels. Further, the deep linear recurrent unit (LRU) is inspired by the similarities between SSMs and RNNs, and it demonstrated impressive performance in modeling long-range dependencies (LRD). Still, generative modeling of long-range sequential data via state space models remains largely underexplored. Recent work suggested LS4 , a latent time series generative model that builds upon linear state space equations. LS4 utilizes autoregressive dependencies to expressively model time series (potentially non-stationary) distributions. However, this model struggles with short-length sequences as we show in our study, potentially due to limited expressivity of linear SSMs.

To overcome gradient issues of recurrent backbones, temporal computational costs of transformers, and expressivity problems of SSMs, we represent time series information via small-sized _images_. Transforming raw sequences to other encodings has been useful for processing audio  as well as general time series data [95; 38; 56]. Moreover, a similar approach was employed to generative modeling of time series with generative adversarial networks (GANs) [12; 39]. However, unstable training dynamics and mode collapse negatively affect the performance of GAN-based tools . In contrast, transforming time series to images is underexplored in the context of generative _diffusion_ models. There are several fundamental advantages to our approach. First, there have been remarkable advancements in diffusion models for vision data that we can exploit [81; 40; 86; 45]. Second, using images instead of sequences elegantly avoids the challenges of long-term modeling. For instance, a moderately-sized \(256 256\) image corresponds to a time series of length up to \(65k\), as we show in Sec. 3. Finally, there is a growing body of literature dealing with time series as images on generative, classification, and forecasting tasks, whose results can be applied in our work and in future studies.

In this work, we propose a new diffusion-based framework for generative modeling of general time series data, designed to seamlessly process both short-, long-, and _ultra_-long-range sequences. To evaluate our method, we consider standard benchmarks for short to ultra-long time series focusing on unconditional generation. Our approach supports efficient sampling, and it attains state-of-the-art results in comparison to recent generative models for sequential information. As far as we know, there are no existing tools handling both short and long sequence data. In addition to its strong unconditional generation capabilities, our approach is also tested in conditional scenarios involving the interpolation of missing information and extrapolation. Overall, we obtained state-of-the-art results in such cases with respect to existing tools. We further analyze and ablate our technique to motivate some of our design choices. The contributions of our work can be summarized as follows:

1. We view generative modeling of time series as a visual challenge, allowing to harness advances in time series to image transforms as well as vision diffusion models.
2. We develop a novel generative model for time series that scales from short to very long sequence lengths without significant modifications to the neural architecture or training method.
3. Our approach achieves state-of-the-art results in comparison to strong baselines in unconditional and conditional generative benchmarks for time series of lengths in the range \([24,17.5k]\). Particularly, we attain the best scores on a new challenging benchmark of very long sequences that we introduce.

## 2 Related work

Time series to image works.Motivated by the success of convolutional neural networks on vision data, several works have transformed time series to images using Gramian Angular Fields , Recurrence Plots , and Line Graphs . This innovation allows leveraging computer vision techniques, tested on tasks such as time series classification and imputation. In speech analysis and processing, the short-time Fourier transform (STFT) stands out as a widely used method [1; 2; 94; 26]. It tracks the changes in frequency components over time, making it essential for analyzing and understanding audio and speech data. Recent research [71; 17] has explored mel-spectrogramtransforms within diffusion models, including integration with advanced latent diffusion spaces . Furthermore, combining time series images and Wasserstein GANs [12; 39] have been considered for generative modeling. Yet, representing general time series as images within diffusion models for tasks such as unconditional generation, interpolation, and extrapolation, remains largely underexplored. The goal of this work is to make a step toward bridging this gap.

Diffusion models.Both denoising diffusion probabilistic models (DDPM) [81; 40] and score-based generative models [84; 85] have demonstrated their effectiveness across diverse domains including images [74; 41], audio [15; 52], and graphs [69; 97; 10]. Song et al.  showed that DDPM and score-based models can be both interpreted as stochastic differential equations (SDE). Further works focused on improving generation quality by using latent diffusion processes in autoencoder architectures . Another research direction deals with lowering the number of neural function evaluations (NFEs), which originally ranged from hundreds to thousands NFEs. For instance, Karras et al.  obtain a low Frechet inception distance (FID) with only \(35\) evaluations, whereas the recent consistency models  achieved comparable results with only a single function evaluation.

Generative modeling of time series.Generative adversarial networks (GANs)  have shown remarkable success in generating realistic data across various domains. Specifically, their application to time series information by joint optimization of supervised and adversarial objectives via TimeGAN captured the inherent dynamics of real-world signals . Similarly, GT-GAN  utilizes diverse tools including ordinary differential equations (DE) , neural controlled DE , and continuous time-flow processes to model both regularly- and irregularly-sampled data . Nevertheless, GANs suffer from challenges, primarily due to unstable training dynamics and mode collapse . Beyond GANs, variational autoencoders (VAEs) have been also considered for generative modeling of sequential data [22; 54; 73], where the work  achieved strong results using Koopman-based approaches [7; 6; 11; 65]. To process long-range dependencies and stiff dynamics , Zhou et al.  introduced LS4, a latent generative model based on linear state space equations. Following the success of diffusion models in other domains, there is a growing desire to adapt them for time series data. However, this adaption is not straightforward and entails the design of a suitable backbone [88; 57; 19; 100; 67]. Other approaches focused on regression problems , based on manifold learning tools [47; 48]. Instead, we propose a new framework for generative modeling of time series by transforming such data to images and using existing strong diffusion vision models.

## 3 Background

In what follows, we state the problem, we mention two effective time series to image transformations, and we briefly discuss the essentials of diffusion-based generative modeling.

Problem statement.We address the problem of generating time series (TS), sampled from a learned distribution \((x)\) that is similar to an unknown distribution \(p(x)\), for which we have a set of observed TS data. The given observations include data samples \(x^{L K}\), where \(L\) represents the sequence length and \(K\) denotes the number of features. Formally, the generative modeling task is often termed "unconditional generation" , and it entails learning a model \(M\) capable of sampling unseen time series \(\) from \((x)\). Additionally, our work addresses a secondary problem known as "conditional generation". In this setting, given an additional signal \(c\), we learn the unknown (conditional) distribution \(p(x|c)\). For example, the signal \(c\) can be an observed part from the TS. This conditional modeling proves useful for tasks such as time series interpolation and extrapolation.

Time series to image transforms.We focus in our study on two invertible time series to image transformations: 1) the delay embedding; and 2) the short time Fourier transform. We provide below a brief overview of these transforms and their inverse. We consider additional transforms and we discuss more details in App. A. Fig. 3 illustrates a time series signal and its related images.

_Delay embeddings_ transform a univariate time series \(x_{1:L}^{L}\) to an image by arranging the information of the series in columns and pad if needed. Let \(m,n\) be two user parameters representingthe skip value and the column dimension, respectively. We construct the following matrix \(X\),

\[X=x_{1}&x_{m+1}&&x_{L-n}\\ &&&\\ x_{n}&x_{n+m+1}&&x_{L}^{n q}\;,\] (1)

where \(q=(L-n)/m\). The image \(x_{}\) is created by padding with zeros to fit the neural network input constraints. Given \(x_{}\), the original time series \(x_{1:L}\) can be extracted in multiple ways. For instance, if \(m=1\), then \(x_{1:L}\) is formed by concatenating the first row and last column of \(x_{}\). The delay embedding scales naturally to long sequences, e.g., setting \(m=n=256\) allows to encode \(65k\) sequences with \(256 256\) images.

_Short Time Fourier Transform (STFT)_ is a well-known transformation that maps a signal from its original domain into the frequency domain. To preserve the temporal structure, STFT applies a rolling window on the time axis, extracting time series segments for which the fast Fourier transform (FFT) is applied. Given an input signal \(x^{L K}\), STFT produces an image \(x_{}^{2K H W}\), where the channels are doubled to store the real and imaginary parts, and \(H,W\) are derivatives of user parameters. STFT requires a minimum window length, and thus, short sequences may require a linear interpolation to match length constraints. Remarkably, the short time Fourier transform is invertible via reverse STFT with a negligible loss of information. Importantly, in contrast to the common practice in audio processing, we do not further compute the spectrogram of STFT, avoiding non-trivial inverse transformations.

Diffusion models.Diffusion processes gradually add noise to an image, following a predefined noise scheduling scheme. Generating new images is possible by learning a model that removes noise. The diffusion process \(\{(t)\}_{t=0}^{T}\) is the path of a stochastic differential equation (SDE) , where an initial sample \((0)\) is drawn from the data distribution \(p_{0}()\). The initial sample is modified to \((T)\), sampled from a simple prior distribution such as a normal Gaussian \((0,I)\). Formally, the forward process is governed by an SDE of the form,

\[=f(,t)t+gw\;,\] (2)

where \(f(,t):^{d}^{d}\) represents the drift coefficient, \(g\) is the diffusion scalar, and \(w\) denotes a standard Wiener process. To facilitate sampling, we need to derive the reverse SDE. It is well-known that the reverse process  is given by,

\[=[f(,t)-g^{2}_{} p_{t}( )]+g\;,\] (3)

where \(\) denotes reverse time and \(\) is a reverse Wiener process. Given Eq. (3), one can derive a deterministic process, characterized by trajectories that share identical marginal probability densities. Formally, we obtain the following ordinary differential equation (ODE),

\[=f(,t)-g^{2}_{} p_{t}( )\;.\] (4)

Diffusion models compute an estimator \(s_{}(,t)\) to approximate the infeasible \(_{} p_{t}()\) via

\[_{}_{t}\{_{x_{0},x_{t}}|s_{}(,t) -_{} p_{0t}(_{t}|x_{0})|_{2}^{2}\}\;,\] (5)

where \(p_{0t}\) denotes the joint distribution of the initial data and noisy sample. In practice, minimizing Eq. (5) is done by learning the noise pattern of input images. For a more comprehensive discussion regarding score-based models, we refer to [86; 45]. We specify in Sec. 4 the particular diffusion model employed in this work, along with further additional details.

## 4 Method

Our approach to generative modeling of time series information is based on the following simple observation and straightforward idea. We observe that diffusion models for vision have demonstrated remarkable progress and results recently [40; 74; 72]. Therefore, our idea is to transform sequences to images, allowing their processing using established diffusion vision models. Fortunately, there are several efficient time series to image maps with effective inverse transforms [87; 35; 95; 56]. Our computational pipeline is composed of three main building blocks: 1) a time series to image module; and 2) a diffusion model; and 3) an image to time series component. The diffusion model is the only learnable parameter-based part of our neural network. We illustrate our generative modeling framework for time series information in Fig. 1, depicting the above building blocks with proper notations for inputs and outputs. Formally, given an input time series \(x^{L K}\) with \(L\) the sequence length and \(K\) the number of features, we transform it to an image \(x_{}^{C H W}\). Noise is added to the latter image yielding the tensor \(x_{}(t)\) which is processed with our diffusion model, whose output \(s(x_{},t)^{C H W}\) represents the cleaned image. During inference, noise \(x_{}(T)\) is sampled from \((0,I)\), iterated backward to \(x_{}(0)\) and transformed to a time series \(^{L K}\).

There are several options to choose from regarding the time series to image (ts2img) transform, \(:^{L K}^{C H W}\). While all transforms are applicable in our framework, we opt for ts2img maps that are efficient to compute, provide informative images, scalable across short and long sequences, and have a closed-form inverse. For instance, line graphs  are efficient with a closed-form inverse, however, they produce images that are mostly non-informative as they contain blank pixels. Similarly, the Gramian angular field transform  essentially stores the sequence in its main diagonal, and thus, it is not straightforward to apply it to long-range data. In this work, we focus on using the delay embedding and short time Fourier transforms. Both ts2img maps satisfy all requirements above. Moreover, our empirical ablation analysis in Sec. 5.5 highlights that these transformations attain the best results on average. The inverse transforms \(^{-1}\) for delay embedding and STFT are parameter-less, deterministic, and highly efficient. See Sec. 3 and App. A.

At the heart of our **ImagenTime** framework lies the generative diffusion model backbone. Diffusion models for vision data have enjoyed increased attention over the past few years, with strong techniques appearing at an unprecedented rate . One limitation, shared among all diffusion models, is the requirement to iteratively denoise the image during inference, resulting in costly neural function evaluations (NFEs). While multiple works focused on alleviating this issue , the work by Karras et al.  offers an enhanced score-based model with a good balance between rapid sampling and high-quality generations. Specifically, they presented a clearer design space for the factors that determine the performance of diffusion models, and they suggested EDM that employs a second-order ODE for the reverse process, yielding low FID images in \(35\) NFEs. Thus, we utilize in this work the EDM diffusion model as our generative backbone.

We conclude by briefly discussing the training and inference procedures. During _training_, we process batches of time series data \(X\), for which we apply \(\) using either delay embedding or STFT to obtain a batch of images, i.e., \(X_{}=(X)\). Subsequently, we employ the training procedure of EDM to learn the score function \(s_{}(X_{},t)\). For _inference_, we use the trained EDM model and we compute the reverse ODE in Eq. (4) for sampling new data points. In practice, we follow the same inference procedure specified in . Finally, given a batch of sampled images, \(_{}\), we apply the inverse transform \(^{-1}\) to achieve a batch of generated time series samples, i.e., \(=^{-1}(_{})\).

## 5 Experiments

We use standard unconditional and conditional quantitative and qualitative benchmarks to extensively validate our framework's ability to generate high-quality time series samples. First, we test our

Figure 1: Our training pipeline (top) involves transforming a time series signal to its e.g., delay embedding image, process the image with a diffusion model, and output its cleaned version. During inference (bottom), we sample from a standard normal distribution and obtain a clean image using the trained diffusion model. Finally, we transform the image back to the time series domain.

framework on short-term and long-term standard time series unconditional generation benchmarks (Sec. 5.1, Sec. 5.2). Then, we introduce a novel benchmark for ultra-long sequences (above 10k steps) and evaluate our method in comparison to strong baselines (Sec. 5.3). Then, we consider interpolation and extrapolation benchmarks, similar to , to test our model on conditional generation tasks (Sec. 5.4). Further, we extended these benchmarks with additional short- and ultra-long setups, which test the framework's robustness to lengths. Finally, we conclude with an extensive ablation of our framework (Sec. 5.5). More details on the experimental settings can be found in App. B.

### Short-Term Unconditional Generation

Data, baselines, and metrics.We employ our framework on the unconditional generation benchmark reported in . The benchmark includes four synthetic and real-world datasets with a fixed length of \(24\). The first dataset, _Stocks_, consists of daily historical Google stock data from 2004 to 2019, comprising six channels: high, low, opening, closing, and adjusted closing prices, as well as volume. This data lacks periodicity and is dominated by random walks. The second dataset, _Energy_, is a multivariate appliance energy prediction dataset , featuring 28 channels with correlated features, and it exhibits noisy periodicity and continuous-valued measurements. The third dataset, _MuJoCo_ (Multi-Joint dynamics with Contact), serves as a versatile physics generator for simulating TS data with 14 channels . We report results on the simple synthetic _Sine_ dataset of sine functions in App. C.1. Our framework is compared with state-of-the-art short-term time series generative models. KoVAE , DiffTime , GT-GAN , TimeGan , RCGAN , C-RNN-GAN , T-Forcing , P-forcing , WaveNet , WaveGAN , and LS4 , which is the state-of-the-art generative model for modeling long sequences. The benchmark employs two metrics: 1) The _Predictive (pred)_ metric assesses the utility of the generated data. 2) The _Discriminative (disc)_ metrics gauge the similarity of distributions using a proxy discriminator. For all experiments, we used the _delay embedding_ transform with an embedding of \(n=8\) and a delay of \(m=3\), yielding a \(8 8\) image. We use \(18\) sampling steps with the EDM model  as the diffusion generative backbone.

Quantitative and qualitative results.The results for the short-term unconditional benchmark are shown in Tab. 1. Our framework achieves state-of-the-art results on all datasets and metrics. Particularly, we note MuJoCo, where we improved the second-best method by \(88\%\) and \(21\%\) in the discriminative and predictive scores. In general, the second-best approach is DiffTime. Importantly, while LS4 performs well on long sequences, our results indicate that it struggles with short sequences. In comparison, we will show below that in addition to obtaining SOTA results on short-term time series, we also achieve strong results in the long-term case (Sec. 5.2). We also evaluate our method using two common qualitative tests . First, we compute a two-dimensional t-SNE  embedding for real and synthetic data. The desired outcome is that both datasets span similar regions and shapes in 2D. We plot the embeddings of the real data, ours and GT-GAN in Fig. 2A, highlighting that our generated point clouds are closer to the real data in comparison GT-GAN. Tab. 11 reports the Wasserstein distances between the t-SNE embeddings of the generated data and the real data, showing that our approach is superior to GT-GAN. Second, we estimate the probability density functions in Fig. 2D. Our approach generates densities similar to the real densities, whereas GT-GAN introduces noticeable errors. The rest of the short-term datasets' qualitative analysis appears in App. C.3.

    &  &  &  \\ Method & disc\(\) & pred\(\) & disc\(\) & pred\(\) & disc\(\) & pred\(\) \\  KoVAE & \(\) & \(.037.000\) & \(.143.011\) & \(.251.000\) & \(.076.017\) & \(.038.002\) \\ DiffTime & \(.050.017\) & \(.038.001\) & \(.101.019\) & \(.250.003\) & \(.059.009\) & \(.042.000\) \\ GT-GAN & \(.077.031\) & \(.040.000\) & \(.221.068\) & \(.312.002\) & \(.245.029\) & \(.055.000\) \\ TimeGAN & \(.102.021\) & \(.038.001\) & \(.236.012\) & \(.273.004\) & \(.409.028\) & \(.082.006\) \\ RCGAN & \(.196.027\) & \(.040.001\) & \(.336.017\) & \(.292.004\) & \(.436.012\) & \(.081.003\) \\ C-RNN-GAN & \(.399.028\) & \(.038.000\) & \(.449.001\) & \(.483.005\) & \(.412.095\) & \(.055.004\) \\ T-Forcing & \(.226.035\) & \(.038.001\) & \(.483.004\) & \(.315.005\) & \(.499.000\) & \(.142.014\) \\ P-Forcing & \(.257.026\) & \(.043.001\) & \(.412.006\) & \(.303.005\) & \(.500.000\) & \(.102.013\) \\ WaveNet & \(.232.028\) & \(.042.001\) & \(.397.010\) & \(.311.006\) & \(.385.025\) & \(.333.004\) \\ WaveGAN & \(.217.022\) & \(.041.001\) & \(.363.012\) & \(.307.007\) & \(.357.017\) & \(.324.006\) \\ LS4 & \(.199.065\) & \(.068.013\) & \(.474.003\) & \(.251.000\) & \(.333.029\) & \(.062.006\) \\  Ours & \(.037.006\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Error measures for the short time series unconditional discriminative and prediction tasks.

### Long-Term Unconditional Generation

Data, baselines, and metrics.We utilize the long-term time series benchmark presented in . It includes three long-term real-world time series datasets obtained from the Monash Time Series Forecasting Repository : FRED-MD, NN5 Daily, and Temperature Rain. We omit Solar Weekly as it is short-term. These datasets were chosen based on their average 1-lag autocorrelation metric, measuring their correlation over time. Their 1-lag values range from \(0.38\) to \(0.98\), highlighting a diverse range of temporal dynamics that present challenges for generative learning tasks. Each dataset contains approximately 750 time steps. We compare our method with state-of-the-art long-term generative methods: LS4 , SaShiMi , SDEGAN , TimeGAN , Latent ODE , ODE2VAE , GP-VAE  and RNN-VAE . Three different metrics are used to evaluate the generative performance: Marginal (marg), Classification (class), and Prediction (pred). Marginal scores measure the absolute difference between the empirical probability density functions of two distributions. Classification scores use a sequence model to classify samples as real or generated; high scores indicate less distinguishable samples. Prediction scores utilize a train-on-synthetic-test-on-real sequence-to-sequence model to predict future steps; lower scores indicate higher predictability. We used the STFT transform in all our experiments, creating a \(32 32\) size image. The number of sampling steps is \(18\) and we use the EDM model  as the diffusion generative backbone.

Quantitative and qualitative results.We present the results in Tab. 2. LS4 performs well across most datasets and metrics. In comparison, our method outperforms LS4 and the other techniques in almost all cases. On NN5 Daily pred and on Temp Rain marg we achieve inferior results. Notably, Zhou et al.  discuss the challenge of measuring the marginal score for the Temp Rain dataset due to frequent zero values. We highlight that our approach substantially improves the classification and prediction scores for the Temp Rain dataset. Additionally, our framework achieves strong results in the classification scores for the FRED-MD and NN5 Daily datasets. We report our results with standard deviations in App. C.4; the results emphasize the statistical significance improvement our framework achieves. Our qualitative results are shown in Fig. 2(B, D) and in App. C.3.

   Method &  &  &  \\  & marg\(\)class \(\)pred \(\)marg\(\)class \(\)pred \(\)marg\(\)class \(\)pred \(\)marg\(\)class \(\)pred \(\) & marg\(\)class \(\)pred \(\) \\  RNN-VAE & \(.132\) & \(.036\) & \(1.47\) & \(.137\) & \(.000\) & \(.967\) & \(.017\) & \(.000\) & \(159\) \\ GP-VAE & \(.152\) & \(.016\) & \(2.05\) & \(.117\) & \(.002\) & \(1.17\) & \(.183\) & \(.000\) & \(2.31\) \\ ODE’VAE & \(.122\) & \(.028\) & \(.567\) & \(.211\) & \(.001\) & \(.19\) & \(.183\) & \(.000\) & \(1.13\) \\ Latent ODE & \(.042\) & \(.327\) & \(.013\) & \(.107\) & \(.000\) & \(.104\) & \(.011\) & \(.000\) & \(145\) \\ TimeGAN & \(.081\) & \(.029\) & \(.058\) & \(.040\) & \(.001\) & \(.134\) & \(.498\) & \(.003\) & \(1.96\) \\ SDEGAN & \(.084\) & \(.501\) & \(.677\) & \(.085\) & \(.085\) & \(1.01\) & \(.990\) & \(.017\) & \(2.46\) \\ SaShiMi & \(.048\) & \(.001\) & \(.232\) & \(.020\) & \(.045\) & \(.849\) & \(.758\) & \(.000\) & \(2.12\) \\ LS4 & \(.022\) & \(.544\) & \(.037\) & \(.007\) & \(.636\) & \(.241\) & \(.083\) & \(.976\) & \(.521\) \\  Ours & \(\) & \(\) & \(\) & \(\) & \(\) & \(.393\) & \(.409\) & \(\) & \(\) \\   

Table 2: Long time series unconditional marginal, classification, and prediction tasks’ results.

Figure 2: We plot the 2D t-SNE embeddings of synthetic data generated with our method and SOTA tools vs. the real data (top). Then, we compare their probability density functions (bottom).

### Ultra-long Term Unconditional Generation

Data, baselines, and metrics.We conclude our unconditional generation evaluation by considering the challenging setting of _ultra-long_ sequences. As far as we know, this setup is underexplored in the literature, and moreover, considering short, long, and ultra-long time series for a single framework is novel to our work. Specifically, we use the following real-world datasets from the Monash Time Series Forecasting Repository : San Francisco Traffic (Traffic)  and KDD-Cup 2018 (KDD-Cup) . The datasets' lengths are \(17544\) and \(10920\), respectively. Traffic includes an hourly time series detailing the road occupancy rates on the San Francisco Bay Area freeways from \(2015\) to \(2016\). KDD-Cup represents the air quality level from \(2017\) to \(2018\) estimated by \(59\) stations across two cities, Beijing (\(35\) stations) and London (\(24\) stations), measured in an hourly rate. We process Traffic with the delay embedding transform (\(n=144\), \(m=136\)), yielding \(144 144\) images. KDD-Cup is transformed by STFT, resulting in \(112 112\) images. The sampling steps are \(36\) in both datasets.

Quantitative and qualitative results.As shown in Tab. 3, our method consistently achieves superior results in all cases. Notably, it attains on KDD-Cup a pred score of \(.001\) compared to LS4's second-best score of \(.049\). These results highlight our framework's scalability to very long sequences, demonstrating impressive performance across all sequence lengths as we demonstrated in the previous sections. We also report results with standard deviations in App. C.6, emphasizing the statistical significance of our framework. Finally, our qualitative results for this setting are shown in Fig. 2(C, E) and in App. C.3.

### Conditional Generation of Time Series

In addition to the unconditional generation benchmark we consider above, we also evaluate our approach on conditional generation tasks. We focus on the imputation (interpolation) and forecasting (extrapolation) tasks, following the experimental setup in [75; 78]. Our approach can be adapted to solve these tasks via a simple modification. For instance, in the interpolation task, the goal is to generate the missing values. Thus, we apply our diffusion model only in the missing locations using a corresponding mask. The rest of the values are left unchanged. A similar mechanism can be applied to extrapolation. Generally, this approach is similar to image inpainting techniques . In the interpolation task, we randomly mask \(50\%\) of the sequence values, whereas in the extrapolation challenge, we split the sequence in half, where the second half represents the target values. In this benchmark, we consider short, long, and ultra-long sequences. Our comparison focuses on generative methods that can handle long-range dependencies including ODE-RNN , Latent ODE , CRU , and LS4 . Further details about the experiments can be found in App. B.

Datasets.In the short-term setting, we use ETT* datasets , that contain electricity loads of various resolutions (ETTh1, ETTh2, and ETTm1, ETTm2) from two electricity stations. The sequence length is \(96\). For the long-term case, we utilize an established benchmark [75; 78; 103], including the Physionet and USHCN datasets. The Physionet dataset  includes health measurements of \(41\) sensors collected from \(8000\) ICU patients within the first \(48\) hours of admission. The United States Historical Climatology Network (USHCN)  consists of daily measurements from \(1218\) weather stations across the United States, including data on precipitation, snowfall, snow depth, and minimum and maximum temperatures. For the ultra-long setting, we use the datasets mentioned in Sec. 5.3.

Results.The results of the conditional generation benchmark are detailed in Tab. 4. Values represent the mean squared error (MSE), and thus, lower is better. MSE values are multiplied by \( 10^{-3}\) and \( 10^{-2}\) for Physionet and USHCN, respectively, in both experiments. We denote in bold the best method per dataset. The short, long, and ultra-long results are placed at the top, middle, and bottom sections of the table. Overall, our method presents stellar results in all settings, except for ETTm1 where it is second-best. Notably, we mention that in the short interpolation, our results are \( 4\) times better than the second-best method, CRU. Similarly, we improve the SOTA by \( 30\%\) in the short extrapolation. Our results are particularly strong in the long interpolation setting, where we

    &  &  \\  & pred \(\) class \(\) marg \(\) & pred \(\) class \(\) marg \(\) \\  Latent-ODE & \(1.01\) &.000 &.180 &.079 &.013 &.009 \\ LS4 & \(.170\) &.630 &.002 &.049 &.488 &.002 \\ Ours & **.138** & **.684** & **.001** & **.001** & **.842** & **.001** \\   

Table 3: Ultra-long unconditional generation.

improve by two- and one-orders of magnitude on Physionet and USHCN, respectively. Finally, we also highlight our ultra-long interpolation results which are \( 4\) times better than ODE-RNN. We conclude that our approach shows robustness to varying sequence lengths, presenting extremely strong results across several datasets and in comparison to state-of-the-art generative models.

### Ablation Studies

We conclude our empirical section by thoroughly inspecting different components of our framework. Specifically, we show that our approach is robust to different image resolutions (App. C.8). We also experiment with a range of hyper-parameters (App. C.9), demonstrating the stability of our approach. Our performance evaluation highlights that our method is comparable to LS4 in terms of training and inference time (App. C.10). Below, we ablate the effect of various image transforms on the performance in the unconditional test. We evaluate our model using four different transforms: folding, Gramian angular field (GAF), delay embedding (DE) and STFT, and we detail the results in Tab. 5. While DE and STFT are slightly better on short and long sequences, respectively, we emphasize that all other transforms perform reasonably well across the various datasets and metrics. GAF does not scale to long sequences as it produces huge images, and thus, it is omitted from the long-term test. We conclude that our framework is robust to the choice of image transformation.

## 6 Conclusion

While new generative models for general time series data appear rapidly, the majority of existing frameworks are specifically designed to process either short or long sequences. The lack of a unified framework for varying lengths time series can be justified by the shortcomings of current available tools: gradient issues of recurrent networks, temporal computational costs of transformers, and limited expressiveness of state space models. In this work, we address this problem by introducing a novel generative model for time series based on signal-to-image invertible transforms and a vision diffusion backbone. The benefit of our approach is threefold: we exploit advanced diffusion models for vision, we seamlessly process short-to-ultra-long sequences, and we can utilize tools from the signal-to-image literature. We extensively evaluate our framework in the unconditional and conditional settings using short, long, and ultra-long sequences, considering multiple datasets, and in comparison to state-of-the-art models. Our experiments show the superiority of our framework, setting new SOTA results. Further, we demonstrate the robustness of our method through several ablation studies. Our approach requires slightly higher computational resources, which we leave for further consideration and future work. Finally, we believe that the proposed framework has the potential to be applicable in additional tasks including classification, anomaly detection, few-shot learning, and more generally, serve as a foundation model.

    &  &  \\ Dataset & ODE-RNN & Latent ODE & CRU & LS4 & Ours & ODE-RNN & Latent ODE & CRU & LS4 & Ours \\  ETH1 & \(.210\) & \(.671\) & \(.283\) & \(.642\) & \(\) & - & \(1.02\) & \(1.02\) & \(3.42\) & \(\) \\ ETH2 & \(.182\) & \(.712\) & \(.368\) & \(3.40\) & \(\) & - & \(1.17\) & \(1.09\) & \(3.83\) & \(\) \\ ETTm1 & \(.762\) & \(.502\) & \(.086\) & \(.114\) & \(\) & - & \(\) & \(.643\) & \(3.05\) & \(.634\) \\ ETTm2 & \(.116\) & \(.247\) & \(.179\) & \(.488\) & \(\) & - & \(\) & \(.378\) & \(3.64\) & \(\) \\     } } & \(2.30\) & \(2.12\) & \(1.82\) & \(.620\) & \(\) & \(3.01\) & \(4.21\) & \(6.29\) & \(4.94\) & \(\) \\ USHCN & \(8.31\) & \(17.9\) & \(.160\) & \(.050\) & \(\) & \(1.96\) & \(2.03\) & \(1.27\) & \(4.36\) & \(\) \\   Traffic & \(.404\) & \(.985\) & \(\) & \(.990\) & \(\) & - & \(1.01\) & \(\) & \(2.23\) & \(\) \\ KDD-Cup & \(.205\) & \(.847\) & \(.190\) & \(.970\) & \(\) & - & \(.696\) & \(.723\) & \(6.51\) & \(\) \\   

Table 4: Interpolation and extrapolation results on datasets of varying lengths. The asterisk (*) denotes non-converging runs, running for over seven days.

    &  &  &  &  \\  & disc\(\) & pred \(\) & disc\(\) & pred \(\) & marg\(\) & class \(\) & pred \(\) & marg\(\) & class \(\) & pred \(\) \\  Folding & \(.074\) & \(\) & \(.017\) & \(\) & \(\) & \(\) & \(.021\) & \(.010\) & \(.776\) & \(.436\) \\ GAF & \(.349\) & \(.269\) & \(.049\) & \(.034\) & - & - & - & - & - & - \\ DE & \(\) & \(\) & \(\) & \(.033\) & \(.017\) & \(1.65\) & \(.021\) & \(.007\) & \(\) & \(.394\) \\ STFT & \(.271\) & \(.256\) & \(.071\) & \(.033\) & \(.021\) & \(.862\) & \(\) & \(\) & \(\) & \(\) \\   

Table 5: Short- and long-term ablation of various image transforms using several datasets and metrics.