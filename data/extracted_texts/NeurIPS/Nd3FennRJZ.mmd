# Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning

Gen Li

CUHK

Wenhao Zhan

Princeton

Jason D. Lee

Princeton

Yuejie Chi

CMU

Yuxin Chen

UPenn

The first two authors contributed equally.Corresponding author

###### Abstract

This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called _single-policy partial concentrability_, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.

## 1 Introduction

As reinforcement learning (RL) shows promise in achieving super-human empirical success across diverse fields (e.g., games (Silver et al., 2016; Vinyals et al., 2019; Berner et al., 2019; Mnih et al., 2013), robotics (Brambilla et al., 2013), autonomous driving (Shalev-Shwartz et al., 2016)), theoretical understanding about RL has also been substantially expanded, with the aim of distilling fundamental principles that can inform and guide practice. Among all sorts of theoretical questions being pursued, how to make the best use of data emerges as a question of profound interest for problems with enormous dimensionality.

There are at least two mainstream mechanisms when it comes to data collection: online RL and offline RL. Let us briefly describe their attributes and differences as follows.

_Online RL._ In this setting, an agent learns how to maximize her cumulative reward through interaction with the unknown environment (by, say, executing a sequence of adaptively chosen actions and utilizing the instantaneous feedback of the environment). Given that all information about the environment is obtained through real-time data collection, the main challenge lies in how to (optimally) manage the trade-off between exploration and exploitation. Towards this, one popular approach advertises the principle of optimism in the face of uncertainty -- e.g., employing upper confidence bounds during value estimation to guide exploration -- whose effectiveness has been shown for both the tabular case (Auer and Ortner, 2006; Jaksch et al., 2010; Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Bai et al., 2019; Dong et al., 2019; Zhang et al., 2020; Menard et al., 2021b; Li et al., 2021b) and the case with function approximation (Jin et al., 2020; Zanette et al., 2020; Zhou et al., 2021a; Li et al., 2021a; Du et al., 2021; Jin et al., 2021a; Foster et al., 2021; Chen et al., 2022b).

_Offline RL._ In contrast, offline RL assumes access to a pre-collected dataset, without given permission to perform any further data collection. The feasibility of reliable offline RL depends heavilyon the quality of the dataset at hand. A central challenge stems from the presence of distribution shift: the distribution of the offline dataset might differ significantly from that induced by the target policy. Another common challenge arises from insufficient data coverage: a nontrivial fraction of the state-action pairs might be inadequately visited in the available dataset, thus precluding one from faithfully evaluating many policies based solely on the offline dataset. To circumvent these obstacles, recent works proposed the principle of pessimism in the face of uncertainty, recommending caution when selecting poorly visited actions (Liu et al., 2020; Kumar et al., 2020; Jin et al., 2021b; Rashidinejad et al., 2021; Uehara and Sun, 2021; Li et al., 2022; Yin et al., 2021; Shi et al., 2022; Cai et al., 2022). Without requiring uniform coverage of all policies, the pessimism approach proves effective as long as the so-called _single-policy concentrability_ is satisfied, which only assumes adequate coverage over the part of the state-action space reachable by the desirable policy.

In reality, however, both mechanisms above come with limitations. For instance, even the single-policy concentrability requirement might be too stringent (and hence fragile) for offline RL, as it is not uncommon for the historical dataset to miss a small yet essential part of the state-action space. Pure online RL might also be overly restrictive, given that there might be information from past data that could help initialize online exploration and mitigate the burden of further data collection.

All this motivates the studies of hybrid RL, a scenario where the agent has access to an offline dataset while, in the meantime, (limited) online data collection is permitted as well. Oftentimes, this scenario is practically not only feasible but also appealing: on the one hand, offline data provides useful information for policy pre-training, while further online exploration helps enrich existing data and allows for effective policy fine-tuning. As a matter of fact, multiple empirical works (Rajeswaran et al., 2017; Vecerik et al., 2017; Kalashnikov et al., 2018; Hester et al., 2018; Nair et al., 2018, 2020) indicated that combining online RL with offline datasets outperforms both pure online RL and pure offline RL. Nevertheless, theoretical pursuits about hybrid RL are lagging behind. Two recent works Ross and Bagnell (2012); Xie et al. (2021) studied a restricted setting, where the agent is aware of a Markovian behavior policy (a policy that generates offline data) and can either execute the behavior policy or any other adaptive choice to draw samples in each episode; in this case, Xie et al. (2021) proved that under the single-policy concentrability assumption of the offline dataset, having perfect knowledge about the behavior policy does not improve online exploration in the minimax sense. Another strand of works Song et al. (2022); Nakamoto et al. (2023); Wagemmaker and Yecchiano (2022) looked at a more general offline dataset and investigated how to leverage offline data in online exploration. From the sample complexity viewpoint, Wagenmaker and Pacchiano (2022) studied the statistical benefits of hybrid RL in the presence of linear function approximation; the result therein, however, required strong assumptions on data coverage (i.e., all-policy concentrability) and fell short of unveiling provable gains in the tabular case (as we shall elucidate momentarily). In light of such theoretical inadequacy in previous works, this paper is motivated to pursue the following question:

_Does hybrid RL allow for improved sample complexity compared to pure online or offline RL in the tabular case?_

### Main contributions

We deliver an affirmative answer to the above question. Further relaxing the single-policy concentrability assumption, we introduce a relaxed notation called single-policy _partial_ concentrability (to be made precise in Definition 2), which (i) allows the dataset to miss a fraction of the state-action space visited by the optimal policy and (ii) captures the tradeoff between distribution mismatch and lack of coverage. Armed with this notion, our results reveal provable statistical benefits of hybrid RL compared with both pure online and offline RL. The main contributions are summarized below.

_A novel three-stage algorithm._ We design a new hybrid RL algorithm consisting of three stages. In the first stage, we obtain crude estimation of the occupancy distribution \(d^{}\) w.r.t. any policy \(\) as well as the data distribution \(d^{}\) of the offline dataset. The second stage performs online exploration; in particular, we execute one exploration policy to imitate the offline dataset and another one to explore the inadequately visited part of the unknown environment, with both policies computed by approximately solving convex optimization sub-problems. Notably, these two stages do not count on the availability of reward information, and thus operate in a reward-agnostic manner. The final stage then invokes the state-of-the-art offline RL algorithm for policy learning, on the basis of all data we have available (including both online and offline data).

_Computationally efficient subroutines._ Throughout the first two stages of the algorithm, we need to solve a couple of convex sub-problems with exponentially large dimensions. In order to attain computational efficiency, we design efficient Frank-Wolfe-type paradigms to solve the sub-problems approximately, which run in polynomial time. This plays a crucial role in ensuring computational tractability of the proposed three-stage algorithm.

_Improved sample complexity._ We characterize the sample complexity of our algorithm (see Theorem 1), which provably improves upon both pure online and offline RL. On the one hand, hybrid RL achieves strictly enhanced performance compared to pure offline RL (assuming the same sample size) when the offline data falls short of covering all state-action pairs reachable by the desired policy. On the other hand, the sample size allocated to online exploration in our algorithm might only need to be proportional to the fraction \(\) of the state-action space uncovered by the offline dataset, thus resulting in sample size saving in general compared to pure online RL (a case with \(=1\)).

Notation.Let us also introduce several useful notation. For integer \(m>0\), we let \([m]\) represent the set \(\{1,,m\}\). For any set \(\), we denote by \(^{}\) its complement. For any policy \(_{0}\), we let \(_{_{0}}:\{0,1\}\) be an indicator function such that \(_{_{0}}()=1\) if \(=_{0}\) and \(_{_{0}}()=0\) otherwise. For any finite set \(\), we denote by \(()\) the probability simplex over \(\). Letting \((S,A,H,,)\), we use the notation \(f()=O(g())\) or \(f() g()\) to indicate the existence of a universal constant \(C_{1}>0\) such that \(f C_{1}g\), the notation \(f() g()\) to indicate that \(g()=O(f())\), and the notation \(f() g()\) to mean that \(f() g()\) and \(f() g()\) hold simultaneously. The notation \(()\) is defined in the same way as \(O()\) except that it hides logarithmic factors.

## 2 Preliminaries and problem settings

Episodic finite-horizon MDPs.We study episodic finite-horizon Markov decision processes with \(S\) states, \(A\) actions, and horizon length \(H\). We use \(=(,,H,P=\{P_{h}\}_{h=1}^{H},r=\{r_{h}\}_{h=1} ^{H})\) to represent such an MDP, where \(=[S]\) and \(=[A]\) represent the state space and the action space, respectively. For each step \(h[H]\), we let \(P_{h}:()\) represent the transition probability at this step, such that taking action \(a\) in state \(s\) at step \(h\) yields a transition to the next state drawn from the distribution \(P_{h}(\,|\,s,a)\); throughout the paper, we often employ the shorthand notation \(P_{h,s,a}:=P_{h}(|s,a)\). Another ingredient is the reward function specified by \(r_{h}:\) at step \(h\); namely, the agent will receive an immediate reward \(r_{h}(s,a)\) upon executing action \(a\) in state \(s\) at step \(h\). It is assumed that the reward function is fully revealed upon completion of online data collection. Additionally, we assume throughout that each episode of the MDP starts from an initial state independently generated from some (unknown) initial state distribution \(()\).

A time-inhomogeneous Markovian policy is often denoted by \(=\{_{h}\}_{h=1}^{H}\) with \(_{h}:()\), where \(_{h}(\,|\,s)\) characterizes the (randomized) action selection probability of the agent in state \(s\) at step \(h\). If \(\) is a deterministic policy, then we often abuse the noation and let \(_{h}(s)\) represent the action selected in state \(s\) at step \(h\). We find it convenient to introduce the following notation:

\[\ \ .\] (1)

We also need to handle mixed deterministic policies (i.e., each realization of the policy is randomly drawn from a mixture of deterministic policies). A mixed deterministic policy \(^{}\) is denoted by

\[^{}=_{}()=_{ }[]().\] (2)

Moreover, for any policy \(\), we define its associated value function (resp. Q-function) as follows, representing the expected cumulative rewards conditioned on an initial state (resp. an initial state-action pair):

\[V_{h}^{}(s) _{}_{h^{}:h h ^{} H}r_{h^{}}(s,a)\,\,s_{h}=s,  s;\] \[Q_{h}^{}(s,a) _{}_{h^{}:h h ^{} H}r_{h^{}}(s,a)\,\,s_{h}=s,a_{h}=a, (s,a).\]

Here, the expectation is over the length-\(H\) sample trajectory \((s_{1},a_{1},s_{2},a_{2},,s_{H},a_{H})\) when executing policy \(\) in \(\), where \(s_{h}\) (resp. \(a_{h}\)) denotes the state (resp. action) at step \(h\) of this trajectory.

When the initial state is drawn from \(\), we further augment the notation and denote

\[V_{1}^{}()=_{s}V_{1}^{}(s).\]

Importantly, there exists at least one deterministic policy, denoted by \(^{}\) throughout, that is able to maximize \(V_{h}^{}(s)\) and \(Q_{h}^{}(s,a)\) simultaneously for all \((h,s,a)[H]\); namely,

\[V_{h}^{}(s) V_{h}^{^{}}(s)=_{}V_{h}^{}(s),  Q_{h}^{}(s,a) Q_{h}^{^{}}(s,a)=_{}Q_{h}^{ }(s,a),(s,a).\]

Moving beyond value functions and Q-functions, we would like to define, for each policy \(\), the associated state-action occupancy distribution \(d^{}=[d_{h}^{}]_{1 h H}\) such that

\[d_{h}^{}(s,a)(s_{h}=s,a_{h}=a\,|\,),(s, a,h)[H];\]

in other words, this is the probability of the state-action pair \((s,a)\) being visited by \(\) at step \(h\). We shall also overload \(d^{}\) to represent the state occupancy distribution such that

\[d_{h}^{}(s)_{a}d_{h}^{}(s,a)= (s_{h}=s\,|\,),(s,h)[H].\] (3)

Given that each episode always starts with a state drawn from \(\), it is easily seen that \(d_{1}^{}(s)=(s)\) for any policy \(\) and any \(s\).

Sampling mechanism.We consider a hybrid RL setting that assumes access to a historical dataset as well as the ability to further explore the environment via real-time sampling, as detailed below.

Offline data.Suppose that we have available a historical dataset (also called an offline dataset)

\[^{}=^{k,}}_{1 k K^{ }},\] (4)

containing \(K^{}\) sample trajectories each of length \(H\). Here, the \(k\)-th trajectory in \(^{}\) is denoted by

\[^{k,}=s_{1}^{k,},a_{1}^{k,},,s _{H}^{k,},a_{H}^{k,},\] (5)

where \(s_{h}^{k,}\) and \(a_{h}^{k,}\) indicate respectively the state and action at step \(h\) of this trajectory \(^{k,}\). It is assumed that each trajectory \(^{k,}\) is drawn _independently_ using policy \(^{}\), which takes the form of a mixture of deterministic policies

\[^{}=_{^{}} ^{}().\] (6)

Note that the learner only has access to the data samples but not \(^{}\). Throughout the paper, we use \(d^{}=\{d_{h}^{}\}_{1 h H}\) to represent the occupancy distribution of this offline dataset such that

\[d_{h}^{}(s,a)(s_{h}^{k,},a_{h}^{ k,})=(s,a),(s,a,h) [H].\] (7)

Online exploration.In addition to the offline dataset, the learner is allowed to interact with the unknown environment and collect more data in real time, in the hope of compensating for the insufficiency of the pre-collected data at hand and fine-tuning the policy estimate. More specifically, the learner is able to sample \(K^{}\) trajectories sequentially. In each sample trajectory,

* the initial state is generated independently from an (unknown) distribution \(()\);
* the learner selects a policy to execute the MDP, obtaining a sample trajectory of length \(H\).

The total number of sample trajectories is thus given by

\[K=K^{}+K^{}.\] (8)

Concentrability assumptions for the offline dataset.To quantify the quality of the historical dataset, prior offline RL literature introduced the following single-policy concentrability coefficient based on certain density ratio of interest; see, e.g., Rashidinejad et al. (2021); Li et al. (2022).

**Definition 1** (Single-policy concentrability).: _The single-policy concentrability coefficient \(C^{}\) of the offline dataset \(^{}\) is defined as_

\[C^{}_{(s,a,h)[H]} {d_{h}^{^{}}(s,a)}{d_{h}^{}(s,a)}.\] (9)In words, \(C^{}\) employs the \(_{}\)-norm of the density ratio \(d^{^{}}/d^{}\) to capture the shift of distributions between the occupancy distribution induced by the desired policy \(^{}\) and the data distribution at hand. The terminology "single-policy" underscores that Definition 1 only compares the offline data distribution against the one generated by a single policy \(^{}\), which stands in stark contrast to other all-policy concentrability coefficients that are defined to account for all policies simultaneously.

One notable fact about Definition 1 is that: for \(C^{}\) to be finite, the historical data distribution needs to cover all state-action-step tuples reachable by \(^{}\). This requirement is, in general, inevitable if only the offline dataset is available; see the minimax lower bounds in Rashidinejad et al. (2021); Li et al. (2022) for more precise justifications. However, a requirement of this kind could be overly stringent for the hybrid setting considered herein, as the issue of incomplete coverage can potentially be overcome with the aid of online data collection. In light of this observation, we generalize Definition 1 to account for the trade-offs between distributional mismatch and partial coverage.

**Definition 2** (Single-policy partial concentrability).: _For any \(\), the single-policy partial concentrability coefficient \(C^{}()\) of the offline dataset \(^{}\) is defined as_

\[C^{}()_{1 h H}_{(s,a) _{h}}^{^{}}(s,a)}{d_{h}^{}(s,a)}\ \ \{_{h}\}_{1 h H}() },\] (10)

_where_

\[()\{_{h}\}_{1 h  H}\ \ _{h=1}^{H}_{(s,a)_{h}}d_{h}^{^{}}(s, a)}.\] (11)

In Definition 2, we allow a fraction of the state-action space reachable by \(^{}\) to be insufficiently covered (as reflected in the definition of \(()\) measured by the state-action occupancy distribution) -- hence the terminology "partial". Intuitively, \(_{h}\) corresponds to a set of state-action pairs that undergo reasonable distribution shift (so that the corresponding density ratio does not rise above \(C^{}()\)), whereas the total occupancy density of its complement subset \(_{h}^{c}\) induced by \(^{}\) is under control (i.e., no larger than \(\) when averaged across steps). As a self-evident fact, \(C^{}()\) is non-increasing in \(\); this means that as \(\) increases, we might incur a less severe distribution shift in a restricted part, at the price of less coverage. In this sense, \(C^{}()\) reflects certain tradeoffs between distribution shift and coverage. Clearly, \(^{}()\) reduces to \(C^{}\) in Definition 1 by taking \(=0\).

Goal.Given a historical dataset \(^{}\) containing \(K^{}\) sample trajectories, we would like to design an online exploration scheme, in conjunction with the accompanying policy learning algorithm, so as to achieve desirable policy learning (or policy fine-tuning) in a data-efficient manner. Ideally, we would expect a hybrid RL algorithm to harvest provable statistical benefits compared to both purely online RL and purely offline RL approaches.

## 3 Algorithm

In this section, we propose a new algorithm to tackle the hybrid RL setting. Our algorithm design leverages recent ideas developed in offline RL and reward-agnostic exploration to improve sample efficiency. Our algorithm consists of three stages to be described shortly; informally, the first two stages conduct reward-agnostic exploration to imitate and complement the offline dataset, whereas the third stage invokes a sample-optimal offline RL algorithm to compute a near-optimal policy.

In the sequel, we split the offline dataset \(^{}\) into two halves:

\[^{,1}^{,2},\] (12)

where \(^{,1}\) (resp. \(^{,2}\)) consists of the first (resp. last) \(K^{}/2\) independent trajectories from \(^{}\). As we shall also see momentarily, online exploration in the proposed algorithm -- which collects \(K^{}\) trajectories in total -- can be divided into three parts, collecting \(K^{}_{}\), \(K^{}_{}\) and \(K^{}_{}\) sample trajectories, respectively. Throughout this paper, for simplicity we choose

\[K^{}_{}=K^{}_{}=K^{ }_{}=K^{}/3.\] (13)

### A three-stage algorithm

We now elaborate on the three stages of the proposed algorithm. Due to space limitation, the pseudocode of the complete algorithm is provided in Appendix B.

Stage 1: estimation of the occupancy distributions.As a preparatory step for reward-agnostic exploration, we first attempt to estimate the occupancy distribution induced by any policy as well as the occupancy distribution \(d^{}\) associated with the historical dataset, as described below.

_Estimating \(d^{}\) for any policy \(\)._ In this step, we would like to sample the environment and collect a set of sample trajectories, in a way that allows for reasonable estimation of the occupancy distribution \(d^{}\) induced by any policy \(\). For this purpose, we invoke the exploration strategy and the accompanying estimation scheme developed in Li et al. (2023). Working forward (i.e., from \(h=1\) to \(H\)), this approach collects, for each step \(h\), a set of \(N\) sample trajectories in order to facilitate estimation of the occupancy distributions, which amounts to a total number of

\[NH K^{}_{}=K^{}/3\] (14)

sample trajectories collected in this stage. See Algorithm 3 in Appendix D.1 for a precise description of this strategy. Noteworthily, while Algorithm 3 specifies how to estimate \(^{}\) for any policy \(\), we won't need to compute it explicitly unless this policy \(\) is encountered during the subsequent steps of the algorithm; in other words, \(^{}\) should be viewed as a sort of "function handle" that will only be executed when called later.

_Estimating \(d^{}\) for the historical dataset \(^{}\)._ In addition, we are in need of estimating the occupancy distribution \(d^{}\). Towards this end, we propose the following empirical estimate using the \(K^{}/2\) sample trajectories from \(^{,1}\):

\[^{}_{h}(s,a)=}_{h}(s,a)}{K^{}}(}_{h}(s,a)}{K^{}} c_{ }}{K^{}}+S^{4}A^{4}}{N}+}}})\] (15)

for all \((s,a)\), where \(c_{}>0\) is some universal constant. Here, \(1-\) indicates the target success probability, and

\[N^{}_{h}(s,a)=_{k=1}^{K^{}/2}(s^{k, }_{h}=s,a^{k,}_{h}=a),(s,a).\] (16)

In other words, \(^{}_{h}(s,a)\) is taken to be the empirical visitation frequency of \((s,a)\) in \(^{,1}\) if \((s,a)\) is adequately visited, and zero otherwise. The cutoff threshold will be made clear in our analysis.

Stage 2: online exploration.Armed with the above estimates of the occupancy distributions, we can readily proceed to compute the desired exploration policies and sample the environment. We seek to devise two exploration strategies, with one strategy selected to imitate the offline dataset, and the other one employed to explore the insufficiently visited territory. As a preliminary fact, if we have a dataset containing \(K\) independent trajectories -- generated independently from a mixture of deterministic policies with occupancy distribution \(d^{}\) -- then it has been shown previously (see, e.g., Li et al. (2023, Section 3.3)) that the model-based offline approach is able to compute a policy \(\) obeying

\[V^{}()-V^{}() H[_{h}_{s,a} {d^{^{}}_{h}(s,a)}{1/H+K^{}d^{}_{h}(s,a)}]^{ }.\] (17)

This upper bound in (17) provides a guideline regarding how to design a sample-efficient exploration scheme.

_Imitating the offline dataset._ The offline dataset \(^{}\) is most informative when it contains expert data, a scenario when the data distribution resembles the distribution induced by the optimal policy \(^{}\). If this is the case, then it is desirable to find a policy similar to \(^{}\) in (6) (the mixed policy generating \(^{}\)) and employ it to collect new data, in order to retain and further strength the benefits ofsuch offline data. To do so, we attempt to approximate \(d^{^{*}}\) by \(^{}\) in (17) when attempting to minimize (17). In fact, we would like to compute a mixture of deterministic policies by (approximately) solving the following optimization problem:

\[^{}_{()}_{h=1}^{H}_{s }_{a}^{}_{h}(s,a)} {}H}+_{^{}} ^{^{}}_{h}(s,a)},\] (18)

which is clearly equivalent to

\[^{}_{()}_{: [H]()}_{h=1}^{H}_{s} _{a_{h}(|s)}^{}_{h}( s,a)}{}H}+_{^{}} ^{^{}}_{h}(s,a)}.\] (19)

In order to solve this minimax problem (19) (note that its objective function is convex in \(\)), we resort to the Follow-The-Regularized-Leader (FTRL) strategy from the online learning literature (Shalev-Shwartz, 2012); more specifically, we perform the following updates iteratively for \(t=1,,T_{}\):

\[^{t+1}_{h}(\,|\,s) _{k=1}^{t}^{}_{h}(s,)}{}H}+_{^{}} ^{^{}}_{h}(s,)}, s ,\] (20a) \[^{t+1} _{()}_{h=1}^{H}_{s }_{a^{t+1}_{h}(|s)} ^{}_{h}(s,a)}{}H}+_{^{} }^{^{}}_{h}(s,a)},\] (20b)

where \(\) denotes the learning rate to be specified later. We shall discuss how to solve the optimization sub-problem (20b) in Appendix C. The output of this step is a mixture of deterministic policies taking the following form:

\[^{}=_{^{}}[] {with}^{}=}}_{t=1}^{T_{}}^{t}.\] (21)

_Exploring the unknown environment_. In addition to mimicking the behavior of the historical dataset, we shall also attempt to explore the environment in a way that complements pre-collected data. Towards this end, it suffices to invoke the reward-agnostic online exploration scheme proposed in Li et al. (2023), whose precise description will be provided in Algorithm 5 in Appendix D.2 to make the paper self-contained. The resulting policy mixture is denoted by

\[^{}=_{^{}}[],\] (22)

with \(^{}()\) representing the associated weight vector.

With the above two exploration policies (21) and (22) in place, we execute the MDP to obtain sample trajectories as follows:

1. Execute the MDP \(K^{}_{}\) times using policy \(^{}\) to obtain a dataset containing \(K^{}_{}=K^{}/3\) independent sample trajectories, denoted by \(^{}_{}\);
2. Execute the MDP \(K^{}_{}\) times using policy \(^{}\) to obtain a dataset containing \(K^{}_{}=K^{}/3\) independent sample trajectories, denoted by \(^{}_{}\).

Stage 3: policy learning via offline RL.Once the above online exploration process is completed, we are positioned to compute a near-optimal policy on the basis of the data in hand. More precisely,

* Let us look at the following dataset \[=^{,2}^{}_{}^{}_{}.\] (23) In light of the complicated statistical dependency between \(^{,1}\) and \(^{}_{}^{}_{}\), we only include the second half \(^{,2}\) of the offline dataset \(^{}\), so as to exploit the fact that \(^{,2}\) is statistically independent from \(^{}_{}^{}_{}\).
* We invoke the pessimistic model-based offline RL algorithm proposed in Li et al. (2022) to compute the final policy estimate \(\); see Algorithm 6 in Appendix D.3 for more details.

Main results

As it turns out, Algorithm 1 is capable of achieving provable sample efficiency, as demonstrated in the following theorem. Here and below, we recall that \(K=K^{}+K^{}\).

**Theorem 1**.: _Consider \((0,1)\) and \((0,H]\). Choose the algorithmic parameters such that_

\[=}(K^{}H)^{2}}} T_{} 2(K^{}H)^{2} A.\]

_Suppose that_

\[K^{}+K^{}  c_{1}SC^{}()}{^{2}}^{2 }\] (24a) \[K^{}  c_{1}SA\{H,1\}}{^{2}} \] (24b)

_for some large enough constant \(c_{1}>0\). Then with probability at least \(1-\), the policy \(\) returned by Algorithm 1 satisfies_

\[V_{1}^{}()-V^{}(),\]

_provided that \(K^{}\) and \(K^{}\) both exceed some polynomial \((H,S,A,C^{}(),)\) (independent of \(\))._

The proof is deferred to Appendix E. In a nutshell, Theorem 1 uncovers that our algorithm yields \(\)-accuracy as long as

\[K^{}+K^{} SC^{}()}{^{2}}^{2} ,\] (25a) \[K^{} SA\{H,1\}}{^{2}} ,\] (25b)

ignoring lower-order terms. Several implications of this result are as follows.

Sample complexity benefits compared with pure online or pure offline RL.To make apparent its advantage compared with both pure offline and online RL, we make comparisons with several most relevant works. Discussions of other related works are deferred to Appendix A.

_Sample complexity with balanced online and offline data._ For the ease of presentation, let us look at a simple case where \(K^{}=K^{}=K/2\). The the sample complexity bound (25) in this case simplifies to

\[(_{}\{SA\{H,1\} }{^{2}}+SC^{}()}{^{2}}\} )=:(_{}f_{}() ).\] (26)

_Comparisons with pure online RL._ We now look at pure online RL, corresponding to the case where \(K=K^{}\) (so that all sample episodes are collected via online exploration). In this case, the minimax-optimal sample complexity for computing an \(\)-optimal policy is known to be (Azar et al., 2017; Li et al., 2023)

\[\!(SA}{^{2}})=\! (f_{}(1))\] (27)

assuming that \(\) is sufficiently small, which is clearly worse than (26). For instance, if there exists some very small \( 1/H\) obeying \(C^{}() 1\), then the ratio of (26) to (27) is at most

\[H+1/A 1,\] (28)

thus resulting in substantial sample size savings.

_Comparisons with pure offline RL._ In contrast, in the pure offline case where \(K=K^{}\), the minimax sample complexity is known to be (Li et al., 2022)

\[\!(SC^{}(0)}{^{2}})= \!(f_{}(0))\] (29)

for any target accuracy level \(\), which is apparently larger than (26) in general. In particular, recognizing that \(C^{}(0)=\) in the presence of incomplete coverage of the state-action space reachable by \(^{}\), we might harvest enormous sample size benefits (by exploiting the ability of online RL to visit the previously uncovered state-action-step tuples).

Comparison with Wagenmaker and Pacchiano (2022).It is worth noting that Wagenmaker and Pacchiano (2022) also considered policy fine-tuning and proposed a method called FTPedel to tackle linear MDPs. The results therein, however, were mainly instance-dependent, thus making it difficult to compare in general. That being said, we would like to clarify two points:

* Wagenmaker and Pacchiano (2022) imposed all-policy concentrability assumptions, requrting the combined dataset (i.e., the offline and online data altogether) to cover certain feature vectors for all linear softmax policies (see Wagenmaker and Pacchiano (2022, Definition 4.1)). In contrast, our results only assume single-policy (partial) concentrability, which is much weaker than the all-policy counterpart.
* When specializing Wagenmaker and Pacchiano (2022, Corollary 1) to the tabular cases, the sample complexity therein becomes \((H^{7}S^{2}A^{2}/^{2})\), which is much larger than our result.

Miscellaneous properties of the proposed algorithm.In addition to the sample complexity advantages, the proposed hybrid RL enjoys several attributes that could be practically appealing.

_Adaptivity to unknown optimal \(\)._ While we have introduced the parameter \(\) to capture incomplete coverage, our algorithm does not rely on any knowledge of \(\). Take the balanced case described around (26) for instance: our algorithm automatically identifies the optimal \(\) that minimizes the function \(f_{}()\) over all \(\). In other words, Algorithm 1 is able to automatically identify the optimal trade-offs between distribution mismatch and inadequate coverage.

_Reward-agnostic data collection._ It is noteworthy that the online exploration procedure employed in Algorithm 1 does not require any prior information about the reward function. In other words, it is mainly designed to improve coverage of the state-action space, a property independent from the reward function. In truth, the reward function is only queried at the last step to output the learned policy. This enables us to perform hybrid RL in a reward-agnostic manner, which is particularly intriguing in practice, as there is no shortage of scenarios where the reward functions might be engineered subsequently to meet different objectives.

_Strengthening behavior cloning._ Another notable feature is that our algorithm does not rely on prior knowledge about the policies generating the offline dataset \(^{}\); in fact, it is capable of finding a mixed exploration policy \(^{}\) that inherits the advantages of the unknown behavior policy \(^{}\). This could be of particular interest for behavior cloning, where the offline dataset \(^{}\) is generated by an expert policy, with \(C^{}=C^{}(0) 1\), i.e. the expert policy covers the optimal one. In this situation, the supplement of online data collection improves behavior cloning by lowering the statistical error from \(SC^{}}{K_{}}}\) to \(SC^{}}{K_{}+K_{}}}\), together with an executable learned policy \(^{}\).

_Computational complexity._ We now take a moment to discuss the computational cost of the proposed algorithm. In Stage 1, we need to first estimate the transition matrices \(\{P_{h}\}\), which can be accomplished with runtime \(O(K^{})\). In the ensuing stages, we call Algorithm 3 to estimate \(^{}\) for each \(\) we encounter. When computing \(^{}\), we need to calculate \(^{}\) for \(T_{1}=T_{}T_{2}\) times, where \(T_{2}\) denotes the number of iterations for calculating \(^{t+1}\) in Eq. (20b); in comparison, the computational cost of estimating \(d^{}\) to yield \(^{}\) in Eq. (22) is much smaller. For each \(^{}\), it needs \(O(HS^{2}A)\) computation. With a slight modification on the target Eq. (19) as follows

\[^{}\] \[_{()}_{h=1}^{H}_{s} _{a}_{h}^{}(s,a)}{+O _{h}^{}(s,a)+^{}}{}_{h}^{^{ }}(s,a)+}{} _{h}^{^{}}(s,a)},\] (30)

we can find a good enough \(^{}\) with \(T_{}=(H^{2}S^{2})\) for \(S^{2}}\) and \(O(H^{4}S^{4}A^{2})\) Frank-Wolfe updates for \(S^{3}A^{2}}\). These taken collectively lead to the following computational complexity for each stage: \(O(K^{}+H^{7}S^{8}A^{3}+K^{}H)\) for Stage 1, \(O(H^{7}S^{7}A^{3})\) for Stage 2, and \(O(KH)\) for Stage 3.

Discussion

We have studied the policy fine-tuning problem of practical interest, where one is allowed to exploit pre-collected historical data to facilitate and improve online RL. We have proposed a three-stage algorithm tailored to the tabular setting, which attains provable sample size savings compared with both pure online RL and pure offline RL algorithms. Our algorithm design has leveraged key insights from recent advances in both model-based offline RL and reward-agnostic online RL.

While the proposed algorithm achieves provable sample efficiency, this cannot be guaranteed unless the sample size already surpasses a fairly large threshold (in other words, the algorithm imposes a high burn-in cost). It would be of great interest to see whether one can achieve sample optimality for the entire \(\)-range. Another issue arises from the computation side: even though the proposed algorithm can be implemented in polynomial time, the computational complexity of the Frank-Wolfe-type subroutine might already be too expensive for solving problems with enormous dimensionality. Can we hope to further accelerate it to make it practically more appealing? Finally, it might also be interesting to study sample-efficient hybrid RL in the presence of low-complexity function approximation, in the hope of further reducing sample complexity.