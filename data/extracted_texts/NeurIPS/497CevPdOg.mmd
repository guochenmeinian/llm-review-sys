# Direct Diffusion Bridge using Data Consistency for Inverse Problems

Hyungjin Chung\({}^{1}\)   Jeongsol Kim\({}^{1}\)   Jong Chul Ye\({}^{2}\)

\({}^{1}\) Dept. of Bio and Brain Engineering

\({}^{2}\)Graduate School of AI

Korea Advanced Institute of Science and Technology (KAIST)

{hj.chung, jeongsol, jong.ye}@kaist.ac.kr

###### Abstract

Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the Pareto-frontier toward the optimum. Our proposed method achieves state-of-the-art results on both evaluation criteria, showcasing its superiority over existing methods. Code is open-sourced at [https://github.com/HJ-harry/CDDB](https://github.com/HJ-harry/CDDB)

## 1 Introduction

Diffusion models  have become the de facto standard of recent vision foundation models . Among their capabilities is the use of diffusion models as generative priors that can serve as plug-and-play building blocks for solving inverse problems in imaging . **D**iffusion model-based inverse problem solvers (DIS) have shown remarkable performance and versatility, as one can leverage the powerful generative prior regardless of the given problem at hand, scaling to linear , non-linear , and noisy problems .

Although there are many advantages of DIS, one natural limitation is its slow inference. Namely, the overall process of inference--starting from Gaussian noise and being repeatedly denoised to form a clean image--is kept the same, although there are marginal changes made to keep the sampling process consistent with respect to the given measurement. In such cases, the distance between the reference Gaussian distribution and the data distribution remains large, requiring inevitably a large number of sampling steps to achieve superior sample quality. On the other hand, the distribution of the measurements is much more closely related to the distribution of the clean images. Thus, intuitively, it would cost us much less compute if we were allowed to start the sampling process directly from the measurement, as in the usual method of direct inversion in supervised learning schemes.

Interestingly, several recent works aimed to tackle this problem under several different theoretical motivations: 1) Schrodinger bridge with paired data , 2) a new formulation of the diffusion processvia constant-speed continual degradation , and 3) Ornstein-Uhlenbeck stochastic differential equation (OU-SDE) . While developed from distinct motivations, the resulting algorithms can be understood in a unifying framework with minor variations: we define this new class of methods as **Direct** **D**iffusion **B**ridges (DDB; Section 3.1). In essence, DDB defines the diffusion process from the clean image distribution in \(t=0\) to the measurement distribution in \(t=1\) as the convex combination between the paired data, such that the samples \(_{t}\) goes through continual degradation as \(t=0 1\). In training, one trains a time-conditional neural network \(G_{}\) that learns a mapping to \(_{0}\) for all timesteps, resulting in an iterative sampling procedure that _reverts_ the measurement process.

Using such an iterative sampling process, one can flexibly choose the number of neural function evaluations (NFE) to generate reconstructions that meet the desiderata: with low NFE, less distortion can be achieved as the reconstruction regresses towards the mean ; with high NFE, one can opt for high perceptual quality at the expense of some distortion from the ground truth. This intriguing property of DDB creates a Pareto-frontier of reconstruction quality, where our desire would be to maximally pull the plot towards high perception and low distortion (bottom right corner of Fig. 0(c)).

In this work, we assert that DDB is missing a crucial component of _data consistency_, and devise methods to make the models _consistent_ with respect to the given measurement by only modifying the sampling algorithm, without any fine-tuning of the pre-trained model. We refer to this new class of models as data **C**onsistent **D**irect **D**iffusion **B**ridge (CDDB; Section 3.2), and show that CDDB is capable of pushing the Pareto-frontier further towards the optima (lower distortion: Fig. 0(a), higher perception: Fig. 0(b), overall trend: Fig. 0(c)) across a variety of tasks. Theoretically, we show that CDDB is a generalization of DDS (Decomposed Diffusion Sampling) , a recently proposed method tailored for DIS with Gaussian diffusion, which guarantees stable and fast sampling. We then propose another variation, CDDB-deep, which can be derived as the DDB analogue of the DPS  by considering _deeper_ gradients, which even further boosts the performance for certain tasks and enables the application to nonlinear problems where one cannot compute gradients in the usual manner (e.g. JPEG restoration). In the experiments, we showcase the strengths of each algorithm and show how one can flexibly construct and leverage the algorithms depending on the circumstances.

## 2 Background

### Diffusion models

Diffusion models [15; 38; 22; 19] defines the forward data noising process \(p(_{t}|_{0})\) as

\[_{t}=_{t}_{0}+_{t},\,(0, )\,\,\,\,t, \]

where \(_{t},_{t}\) controls the signal component and the noise component, respectively, and are usually designed such that \(_{t}^{2}+_{t}^{2}=1\)[15; 22]. Starting from the data distribution \(p_{}:=p(_{0})\), the noising process in (1) gradually maps \(p(_{t})\) towards isotropic Gaussian distribution as \(t 1\), i.e. \(p(_{1})(0,)\). Training a neural network to _reverse_ the process amounts to training a residual denoiser

\[_{}_{_{t} p(_{t}|_{0}),_{0}  p_{}(_{0}),(0,)} [\|_{}^{(t)}(_{t})-\|_{2}^{2}], \]

Figure 1: Quantitative metric of I\({}^{2}\)SB  (denoted DDB) vs. proposed CDDB on sr4x-bicubic task.

such that \(^{(t)}_{^{*}}(_{t})_{t}-_{t} _{0}}{_{t}}\). Furthermore, it can be shown that epsilon matching is equivalent to the denoising score matching (DSM) [16; 37] objective up to a constant with different parameterization

\[_{}_{_{t},_{0},}[\|^{( t)}_{}(_{t})-_{_{t}} p(_{t}|_{0})\|_{2}^{2} ], \]

such that \(^{(t)}_{^{*}}(_{t})-_{t}-_{t} _{0}}{_{t}^{2}}=-^{(t)}_{^{*}}(_{t})/_{t}\). Moreover, for optimal \(^{*}\) and under regularity conditions, \(_{^{*}}(_{t})=_{_{t}} p(_{t})\). Then, sampling from the distribution can be performed by solving the reverse-time generative SDE/ODE [38; 19] governed by the score function. It is also worth mentioning that the posterior mean, or the so-called denoised estimate can be computed via Tweedie's formula 

\[}_{0|t}:=_{p(_{0}|_{t})}[_{0}|_{t }]=}(_{t}+_{t}^{2}_{_{t}} p( {x}_{t}))}(_{t}+_{t}^{2}^{(t)}_{ ^{*}}(_{t})). \]

In practice, DDPM/DDIM solvers [15; 35] work by iteratively refining these denoised estimates.

### Diffusion model-based inverse problem solving with gradient guidance

Suppose now that we are given a measurement \(\) obtained through some Gaussian linear measurement process \(\), where our goal is to sample from the posterior distribution \(p(|)\). Starting from the sampling process of running the reverse SDE/ODE to sample from the prior distribution, one can modify the score function to adapt it for posterior sampling [38; 5]. By Bayes rule, \(_{_{t}} p(_{t}|)=_{_{t}} p(_ {t})+_{_{t}} p(|_{t})\), where \(_{_{t}} p(_{t})_{^{*}}(_{t})\). However, \(_{_{t}} p(|_{t})\) is intractable. Several methods have been proposed to approximate this time-dependent likelihood, two of the most widely used being DPS  and IIGDM . DPS proposes the following Jensen approximation1

\[_{_{t}} p(|_{t}))}{} _{_{t}} p(|}_{0|t})=}_{0|t}}{_{t}}}_{0|t}- \|_{2}^{2}}{}_{0|t}}=}_{0|t}}{_{t}}}_{}^{}(- }_{0|t})}_{}, \]

of which the chain rule is based on the denominator layout notation . Here, we see that the gradient term can be represented as the Jacobian (J) vector (V) product (JVP). In the original implementation of DPS, the two terms are not computed separately, but computed directly as \(_{_{t}}\|-}_{0|t}\|_{2}^{2}\), where the whole term can be handled with backpropagation. By this choice, DPS can also handle non-linear operators when the gradients can be computed, e.g. phase retrieval, forward model given as a neural network. On the other hand, IIGDM proposes

\[_{_{t}} p(|_{t}))}{ }(}_{0|t},^{}+)=}_{0|t}}{_{t}}}_{}^{}(-}_{0|t})}_{}, \]

where \(^{}:=^{}(^{})^{-1}\) is the Moore-Penrose pseudo-inverse. Using the JVP for implementation, it is no longer required that the whole term is differentiable. For this reason, IIGDM can be applied to cases where we have non-differentiable, non-linear measurements given that an operation analogous to pseudo-inverse can be derived, e.g. JPEG restoration. Notably, the update step of DPS can be achieved by simply pre-conditioning IIGDM with \(^{}\). Implementing DIS with DPS (5) or IIGDM (6) amounts to augmenting the gradient descent steps in between the ancestral sampling iterations.

While these methods are effective and outperforms the prior projection-based approaches [38; 21], they also have several drawbacks. Namely, the incorporation of the U-Net Jacobian is slow, compute-heavy, and often unstable [12; 34]. For example, when applied to MRI reconstruction in medical imaging, DPS results in noisy reconstructions  possibly due to unstable incorporation of the Wirtinger derivatives , and IIGDM is hard to use as it is non-trivial to compute \(^{}\). In order to circumvent these issues, DDS  proposed to use numerical optimization (i.e. conjugate gradients; CG) in the clean image domain, bypassing the need to compute J. Consequently, DDS achieves fast and stable reconstructions for inverse problems in medical imaging.

## 3 Main Contributions

### Direct Diffusion Bridge

We consider the case where we can sample \(_{0}:= p()\), and \(_{1}:= p(|)^{2}\), i.e. paired data for training. Adopting the formulation of I\({}^{2}\)SB  we define the posterior of \(_{t}\) to be the product of Gaussians \((_{t};_{0},_{t}^{2})\) and \((_{t};_{1},_{t}^{2})\), such that

\[p(_{t}|_{0},_{1})=(_{t};_{t}^{2}}{_{t}^{2}+_{t}^{2}}_{0}+^{2}}{_{t}^{2}+_{t}^{2}}_{1},^{2}_{t}^{2}}{_{t}^{2}+_{t}^{2}}). \]

Note that the sampling of \(_{t}\) from (7) can be done by the reparametrization trick

\[_{t}=(1-_{t})_{0}+_{t}_{1}+_{t},\; (0,), \]

where \(_{t}:=^{2}}{_{t}^{2}+_{t}^{2}},\, _{t}^{2}:=^{2}_{t}^{2}}{_{t}^{2}+_{t}^{2}}\)3. This diffusion bridge introduces a _continual_ degradation process by taking a convex combination of \((_{0},_{1})\), starting from the clean image at \(t=0\) to maximal degradation at \(t=1\), with additional stochasticy induced by the noise component \(_{t}\). Our goal is to train a time-dependent neural network that maps any \(_{t}\) to \(_{0}\) that recovers the clean image. The training objective for I\({}^{2}\)SB  analogous to denoising score matching (DSM)  reads

\[_{}_{ p(|),\; p(), \;t U(0,1)}[\|_{}(_{t})-_{t}-_{0} }{_{t}}\|_{2}^{2}], \]

which is also equivalent to training a residual network \(G_{}\) with \(_{}[\|G_{}(_{t})-_{0}\|_{2}^{2}]\). For brevity, we simply denote the trained networks as \(G_{^{*}}\) even if it is parametrized otherwise. Once the network is trained, we can reconstruct \(_{0}\) starting from \(_{1}\) by, for example, using DDPM ancestral sampling , where the posterior for \(s<t\) reads

\[p(_{s}|_{0},_{t})=(_{s};(1-_{s|t}^{2 })_{0}+_{s|t}^{2}_{t},_{s|t}^{2}), \]

with \(_{s|t}^{2}:=^{2}}{_{t}^{2}}\), \(_{s|t}^{2}:=^{2}-_{t}^{2})_{t}^{2}}{ _{t}^{2}}\). At inference, \(_{0}\) is replaced with a neural network-estimated \(}_{0|t}\) to yield \(_{s} p(_{s}|}_{0|t},_{t})\).

However, when the motivation is to introduce 1) a tractable training objective that learns to recover the clean image along the degradation trajectory, and 2) devise a sampling method to gradually revert the degradation process, we find that the choices made for the parameters in (8),(7) can be arbitrary, as long as the marginal can be retrieved in the sampling process (10). In Table 1, we summarize the

    & \(^{2}\)SB  & **InDI ** \\ 
**Definition** & & \\ Motivation & SchrÃ¶dinger bridge & Small-step MMSE \\  & \(_{t}=_{}+2_{d}t,&t[0,0.5)\\ 2_{}-2_{d}t,&t[0.5,1.0]\) & - \\ Base process & \(_{t}=_{0}^{t}_{}\,d,\,_{t}=_{t}^{1} _{}\,d\) & - \\  & Linear symmetric & Const \\   & \\ \(_{t}\) & \(_{t}^{2}/(_{t}^{2}+_{t}^{2})\) & \(t\) \\ \(_{t}^{2}\) & \(_{t}^{2}_{t}^{2}/(_{t}^{2}+_{t}^{2})\) & \(t^{2}_{t}^{2}\) \\ 
**Sampling** & & \\ \(_{s|t}^{2}\) & \(_{s}^{2}/_{t}^{2}\) & \(s/t\) \\ \(_{s|t}^{2}\) & \(^{2}-_{t}^{2})_{s}^{2}}{_{t}^{2}}\) & \(s^{2}(_{s}^{2}-_{t}^{2})\) \\   

Table 1: Comparison between different types of DDB. \(_{ d}:=_{}-_{}\). Further details are given in Appendix B.

choices made in  to emphasize that the difference stems mostly from the parameter choices and not something fundamental. Concretely, sampling \(_{t}\) from paired data can always be represented as (8): a convex combination of \(_{0}\) and \(_{1}\) with some additional noise. Reverse diffusion at inference can be represented as (10): a convex combination of \(_{0}\) and \(_{t}\) with some stochasticity. We define the methods that belong to this category as Direct Diffusion Bridge (DDB) henceforth. Below, we formally state the equivalence between the algorithms, with proofs given in Appendix A.

**Theorem 1**.: _Let the parameters of InDI  in Table 1 be \(t:=^{2}}{_{t}^{2}+_{t}^{2}}\), \(_{t}^{2}:=^{2}}{_{t}^{2}}(_{t}^{2}+_{t}^{2})\). Then, InDI and \(I^{2}\)SB are equivalent._

The equivalence relation will be useful when we derive our CDDB algorithm in Section. 3.2. As a final note, IR-SDE  does not strictly fall into this category as the sampling process is derived from running the reverse SDE. However, the diffusion process can still be represented as (8) by setting \(_{t}=1-e^{-_{t}}\), \(_{t}^{2}=^{2}(1-e^{-2_{t}})\), and the only difference comes from the sampling procedure.

### Data Consistent Direct Diffusion Bridge

MotivationRegardless of the choice in constructing DDB, there is a crucial component that is missing from the framework. While the sampling process (10) starts directly from the measurement (or equivalent), as the predictions \(}_{0|t}=G_{}(_{t})\) are imperfect and are never guaranteed to preserve the measurement condition \(=\), the trajectory can easily deviate from the desired path, while the residual blows up. Consequently, this may result in inferior sample quality, especially in terms of distortion. In order to mitigate this downside, our strategy is to keep the DDB sampling strategy (10) intact and augment the steps to constantly _guide_ the trajectory to satisfy the data consistency, similar in spirit to gradient guidance in DIS. Here, we focus on the fact that the clean image estimates \(}_{0|t}\) is produced at every iteration, which can be used to compute the residual with respect to the measurement \(\). Taking a gradient step that minimizes this residual after every sampling step results in Algorithm 1, which we name data Consistent DDB (CDDB). In the following, we elaborate on how the proposed method generalizes DDS which was developed for DIS.

```
0:\(G_{^{*}},_{1},_{i},_{i},_{i-1|i}^{2},_{i- 1|i}^{2},_{i}\)
1:for\(i=N-1\) to \(0\)do
2:\(}_{0|i} G_{^{*}}(_{i})\)
3:\((,)\)
4:\(^{}_{i-1}(1-_{i-1|i}^{2})}_{0|i}\) \(+_{i-1|i}^{2}_{i}+_{i-1|i}\)
5:\(g_{0|i}}{_{i}}^{}(-}_{0|i})\)
6:\(_{i-1}^{}_{i-1}+_{i-1}\)
7:endfor
8:return\(_{0}\)
```

**Algorithm 2** CDDB (deep)

CDDB as a generalization of DDS Rewriting (10) with reparameterization trick

\[_{s}=}_{0|t}}_{( _{t})}+^{2}(_{t}-}_{0|t})}_{ (_{t})}+_{s|t},}_{0|t}:=G_{ ^{*}}(_{t}) \]

we see that the iteration decomposes into three terms: the denoised component, the deterministic noise, and the stochastic noise. The key observation of DDIM  is that if the score network is fully expressive, then the deterministic noise term \(_{t}-}_{0|t}\) becomes Gaussian such that it satisfies the total variance condition

\[(_{s|t}^{2}_{t})^{2}+_{s|t}^{2}=_{s}^{2}, \]

allowing (11) to restore the correct marginal \((_{s};_{0},_{s}^{2})\). Under this condition, DDS showed that using a few step of numerical optimization ensure the updates from the denoised image \(}_{0|t}\) remain on the clean manifold. Furthermore, subsequent noising process using deterministic and stochastic noises can then be used to ensure the transition to the correct noisy manifold .

Under this view, our algorithm can be written concisely as

\[_{s}}_{0|t}+^{}(- }_{0|t})}_{(_{t})}+^{2 }(_{t}-}_{0|t})}_{(_{t})}+_{s|t}, \]

where we make the update step only to the clean denoised component, and leave the other components as is. In order to achieve proper sampling that obeys the marginals, it is important to show that the remaining components constitute the correct noise variance and the condition assuming Gaussianity should be (12). In the following, we show that this is indeed satisfied for the two cases of direction diffusion bridge (DDB):

**Theorem 2**.: _The total variance condition (12) is satisfied for both I\({}^{2}\)SB and InDI._

Proof.: For InDI, considering the noise variance \(_{t}=t_{t}\) in Table 1,

\[(_{s|t}^{2}_{t})^{2}+_{s|t}^{2}=}{t^{2 }}t^{2}_{t}^{2}+s^{2}(_{s}^{2}-_{t}^{2})=s^{2} _{s}^{2}=_{s}^{2}. \]

Due to the equivalence in Theorem 1, the condition is automatically satisfied in I\({}^{2}\)SB. We show that this is indeed the case in Appendix A. 

In other words, given that the gradient descent update step in \((_{t})\) does not leave the clean data manifold, it is guaranteed that the intermediate samples generated by (13) will stay on the correct noisy manifold . In this regard, CDDB can be thought of as the DDB-generalized version of DDS. Similar to DDS, CDDB does not require the computation of heavy U-Net Jacobians and hence introduces negligible computation cost to the inference procedure, while being robust in the choice of step size.

CDDB-deepAs shown in DPS and IIGDM, taking _deeper_ gradients by considering U-Net Jacobians is often beneficial for reconstruction performance. Moreover, it even provides way to impose data consistency for non-linear inverse problems, where standard gradient methods are not feasible. In order to devise an analogous method, we take inspiration from DPS, and propose to augment the solver with a gradient step that maximizes the time-dependent likelihood (w.r.t. the measurement) \(p(|_{t})\). Specifically, we use the Jensen approximation from 

\[p(|_{t}) = p(|_{0})p(_{0}|_{t})\,d_{0} \] \[=_{p(_{0}|_{t})}[p(|_{0})]  p(|[_{0}|_{t}])=p(|}_{0|t }),\]

where the last equality is naturally satisfied from the training objective (9). Using the approximation used in (15), the correcting step under the Gaussian measurement model yields

\[_{_{t}} p(|_{t})_{_{t}}\|- }_{0|t}\|_{2}^{2}. \]

Implementing (16) in the place of the shallow gradient update step of Algorithm 1, we achieve CDDB-deep (see Algorithm 2). From our initial experiments, we find that preconditioning with \(^{}\) as in IIGDM improves performance by a small margin, and hence use this setting as default.

## 4 Experiments

### Setup

Model, DatasetFor a representative DDB, we choose I\({}^{2}\)SB  along with the pre-trained model weights for the following reasons: 1) it is open-sourced4, 2) it stands as the current state-of-the-art, 3) the model architecture is based on ADM , which induces fair comparison against other DIS methods. All experiments are based on ImageNet 256\(\)256 , a benchmark that is considered to be much more challenging for inverse problem solving based on generative models , compared to more focused datasets such as FFHQ . We follow the standards of  and test our method on the following degradations: sr4x-{bicubic, pool}, deblur-{uniform, gauss}, and JPEG restoration with 1k validation images.

[MISSING_PAGE_FAIL:7]

mainly focused on perceptual metrics [5; 36; 26], mainly because these methods excel on these metrics, but often compromising distortion metrics. DDB methods often take this to the extreme, where one can achieve the best PSNR with very little NFE, and the PSNR consistently degrades as one increases the NFE [9; 26] (See Fig. 1c). Despite this fact, the standard setting in DDB methods is to set a high NFE as one can achieve much improved perceptual quality. On the other hand, conventional iterative methods are often highly optimized for less distortion, albeit with low perceptual quality. While this trade-off may seem imperative, we show that CDDB can improve both aspects, putting it in the place of the state-of-the-art on most experiments (See Tab. 2, 3). A similar trend can be observed in Fig. 2, where we see that CDDB greatly improves the performance of DDB, while also outperforming DIS and iterative optimization methods.

CDDB pushes forward the Pareto-frontierIt is widely known that there exists an inevitable trade-off of distortion when aiming for higher perceptual quality [3; 24]. This phenomenon has been reconfirmed consistently in the recent DIS [5; 36] and DDS [9; 26] methods. For DDB, one can flexibly control this trade-off by simply choosing different NFE values, creating a Pareto-frontier with higher NFE tailored towards perceptual quality. While this property is intriguing, the gain we achieve when increasing the NFE decreases _exponentially_, and eventually reaches a bound when NFE > 1000. In contrast, we show in Fig. 1 that CDDB pushes the bound further towards the optima. Specifically, 20 NFE CDDB _outperforms_ 1000 NFE DDB in PSNR by \(>2\) db, while having lower FID (i.e. better perceptual quality). To this point, CDDB induces dramatic acceleration (> 50\(\)) to DDB.

CDDB vs. CDDB-deepThe two algorithms presented in this work share the same spirit but have different advantages. CDDB generally has higher speed and stability, possibly due to guaranteed convergence. As a result, it robustly increases the performance of SR and deblurring. In contrast, considering the case of inpainting and JPEG restoration, CDDB cannot improve the performance of DDB. For inpainting, the default setting of I\({}^{2}\)SB ensures consistency by iteratively applying replacement, as implemented in . As the measurement stays in the pixel space, the gradients cannot impose any constraint on the missing pixel values. CDDB-deep is useful in such a situation, as the U-Net Jacobian has a global effect on _all_ the pixels, improv

Figure 4: Results on JPEG restoration (QF=10). CDDB recovers texture details (row 1,3), and color details (row 2).

Figure 3: Results on inpainting (Left) and deblurring (Right). For inpainting, boundaries are corrected (row 1) and artifacts are corrected/removed (row 2,3). For deblurring, halo artifacts are corrected, and grid patterns from the background are alleviated.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & FID \(\) \\ 
**CDDB** (ours) & **26.34** & 0.837 & **0.263** & **19.48** \\
**PSB** & 26.12 & 0.832 & 0.266 & 20.35 \\  IIGDM  & 26.09 & **0.842** & 0.282 & 30.27 \\ DDRM  & 26.33 & 0.829 & 0.330 & 47.02 \\   

Table 3: Quantitative evaluation of the JPEG restoration (QF = 10) task.

ing the performance by inducing coherence. CDDB-deep also enables the extension to nonlinear inverse problems where one cannot take standard gradient steps. This is illustrated for the case of JPEG restoration in Tab. 3 and Fig. 4, where we see overall improvement in performance compared to I\({}^{2}\)SB.

Noise robustnessDIS methods are often designed such that they are robust to measurement noise [21; 5]. In contrast, this is not the case for DDB as they are trained in a supervised fashion: If not explicitly trained with synthetic adding of noise, the method does not generalize well to noisy measurements, as can be seen in Fig. 5. On the other hand, note that with CDDB, we are essentially incorporating a Gaussian likelihood model, which naturally enhances the robustness to noise. As a result, while I\({}^{2}\)SB tends to propagate noise (best seen in the background), we do not observe such artifacts when using CDDB.

## 5 Discussion

Extension to other related worksGoing beyond the paired inverse problem setting and considering the Schrodinger Bridge (SB) problem [25; 8], or more generally transport mapping problems  between the two unmatched distributions, it is often desirable to control the deviation from the start of sampling. A concrete example would be the case of image-to-image translation  where one does not want to alter the content of the image. As CDDB can be thought of as a regularization method that penalizes the deviation from the starting point, the application is general and can be extended to such SB problems at inference time by using the gradients that minimize the distance from the start point. We leave this direction for future work.

Data consistency in supervised learning frameworksThe first applications of supervised deep learning to solve inverse problems in medical imaging (e.g. CT , MRI reconstruction ) mostly involved directly inverting the measurement signal without considering the measurement constraints. The works that followed [1; 14] naturally extended the algorithms by incorporating measurement consistency steps in between the forward passes through the neural network. Analogously, CDDB is a natural extension of DDB but with high flexibility, as we do not have to pre-determine the number of forward passes  or modify the training algorithm .

## 6 Conclusion

In this work, we unify the seemingly different algorithms under the class of direct diffusion bridges (DDB) and identify the crucial missing part of the current methods: data consistency. Our train-free modified inference procedure named consistent DDB (CDDB) fixes this problem by incorporating consistency-imposing gradient steps in between the reverse diffusion steps, analogous to the recent DIS methods. We show that CDDB can be seen as a generalization of representative DIS methods (DDS, DPS) in the DDB framework. We validate the superiority of our method with extensive experiments on diverse inverse problems, achieving state-of-the-art sample quality in both distortion and perception. Consequently, we show that CDDB can push the Pareto-frontier of the reconstruction toward the desired optimum.

Limitations and societal impactThe proposed method assumes prior knowledge of the forward operator. While we limit our scope to non-blind inverse problems, the extension of CDDB to blind inverse problems [7; 30] will be a possible direction of research. Moreover, for certain inverse problems (e.g. inpainting), even when do observe improvements in qualitative results, the quantitative metrics tend to slightly decrease overall. Finally, inheriting from DDS/DIS methods, our method relies on strong priors that are learned from the training data distribution. This may potentially lead to reconstructions that intensify social bias and should be considered in practice.

Figure 5: Results on noisy SR\(\)4 reconstruction. I\({}^{2}\)SB propagates noise to the reconstruction. CDDB effectively removes noise.