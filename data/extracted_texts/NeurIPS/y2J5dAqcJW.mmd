# Measuring Steerability in Large Language Models

Trenton Chang\({}^{1}\)1 Jenna Wiens\({}^{1}\) Tobias Schnabel\({}^{2}\) Adith Swaminathan\({}^{3}\)

\({}^{1}\)University of Michigan \({}^{2}\)Microsoft Research \({}^{3}\)Netflix

{ctrenton, wiensj}@umich.edu

###### Abstract

Large language models (LLMs) are powerful instruction followers. However, many open-ended generation tasks have a large solution space that needs to be narrowed down to fit user needs. LLMs that are _steerable_ towards such needs are critical to safe LLM systems that behave consistently with user expectations and goals. Despite continued improvement in LLM instruction-following, such gains may not necessarily translate to steerability. This disconnect motivates a principled framework for measuring steerability. Thus, we propose a goal-oriented, quantitative definition of steerability. Our definition informs the design of an empirical steerability probe, where we leverage text rewriting tasks to measure steerability of LLMs. We demonstrate that recent LLMs are not steerable. We attribute this lack of steerability to _side-effects_: correlations between requested goals and non-requested LLM movement. Thus, despite advances in LLM instruction following, there remains significant room for improving LLM steerability.

## 1 Introduction

Large language models (LLMs) have empirically demonstrated powerful instruction-following abilities in a variety of domains (Achiam et al., 2023; Dubey et al., 2024). However, many real-world tasks such as text editing or creative writing are open-ended: the desired set of generations depends on user expectations or needs. Users performing such tasks require flexible and controllable LLMs that can be steered to match their diverse needs.

Although LLMs continue to improve at instruction following, better instruction following may not imply better steerability. Instruction following datasets in current evaluations focus on coarse, personal changes, often opting for instructions about simplifying/summarizing text (Shu et al., 2024) or syntactical/linguistic constraints (Zhou et al., 2023). Moreover, even when prompted to produce changes in tone (_e.g._, increased sadness, politeness), an LLM could still default to a congenial, "American" tone due to dataset biases during pre-training or reinforcement learning from human feedback (RLHF). Thus, current benchmarks could report good instruction following performance even if an LLM is not steerable towards producing texts with variation in tone.

Steerability is critical for designing safe LLMs that align with a variety of human values and priorities (Sorensen et al., 2024). This is because safety in one context does not imply safety in another: blanket refusal for legal or medical advice can mitigate harms from misinformation, but could also impede model usability. A context-aware view of LLM safety should prioritize LLMs that can easily navigate different desiderata. Tasks where steerable LLMs are key to safety may include clinical note generation (Abacha et al., 2023) and model-assisted psychotherapy (Sharma et al., 2024).

In this work, we propose a definition of LLM steerability, casting user goals as vectors (Section 2). As an example of an open-ended generation task, we focus on measuring steerability in single-turn text-rewriting. Figure 1 (left) illustrates our proposed definition of steerability via a text simplification request. The user's request and LLM's attempt to satisfy the request can be interpreted as vectors in _goal-space_, with dimensions comprised of textual aspects (Figure 1, right). Deviations between the two vectors in distance and direction map to steerability failures: incorrect distancesUsing our steerability definition, we design an empirical steerability probe and apply it to multiple LLM families (Section 3). Our probe demonstrates that larger models are not necessarily steerable. We attribute this lack of steerability to _side-effects_: requesting one goal may yield shifts in goals not mentioned in the prompt. Our analysis highlights steerability as an independently important axis of LLM performance, and lays the groundwork for well-principled steerability evaluation.

## 2 Defining Steerability

In this section, we define steerability, and disaggregate it into _distance_ and _direction_-based components (Section 2.1). Then, we design a steerability probe based on our definition (Section 2.2).

### Steerability Metrics

Since we focus on text rewriting, we first define a space of textual goals to standardize measurement of textual change. Let \(\) be a set of "goals" (_e.g._, happiness level, reading difficulty), where each string maps to some point in **goal-space**\(^{[]}\), _i.e._, a string's **goal-space mapping** is a real-valued vector. Thus, text changes can be modeled as vectors in goal-space. Concretely, let \(^{*}\) be a user goal, and let \(_{0}\) be the goal-space mapping of the source text, yielding user request vector \(^{*}-_{0}\). Let \(}\) be the goal-space mapping of the LLM output, such that \(}-_{0}\) describes an LLM's movement in goal-space. Our steerability metrics compare user requests (\(^{*}-_{0}\)) to the LLM's goal-space movement (\(}-_{0}\)). Thus, we propose the following definition:

\[)}}_{_ {0},^{*}}[^{*}-_{0}) ^{}(}-_{0})}{\|^{*}-_{0}\|_{ 2}^{2}}].\] (1)

We assume \(}\) is a deterministic function of \(_{0}\) and \(^{*}\) (_e.g._, a prompt based on \(^{*}\) plus source text corresponding to \(_{0}\)). Eq. 1 defines steerability as the magnitude of the vector projection of the LLM's goal-space movement (\(}-_{0}\))) onto the user request vector (\(^{*}-_{0}\)), normalized by the magnitude of the request vector (\(\|^{*}-_{0}\|_{2}\)). In other words, steerability quantifies the LLM's progress in goal-space as a proportion of the user's request vector. Thus, a value of 1 is necessary for steerability (_i.e._, the projection of \(}-_{0}\) onto \(^{*}-_{0}\) is simply \(^{*}-_{0}\)). This definition also suggests that a steerable LLM should output text that "aligns" with the user's request in goal-space, _and_ transforms the source text by an appropriate amount. We can decompose our steerability metric in terms of these two desiderata:

\[)} }_{_{0},^{*} }[}-_{0}\|_{2}}{\| ^{*}-_{0}\|_{2}}]\] (2) \[)} }_{_{0},^{*} }[^{*}-_{0})^{}( }-_{0})}{\|^{*}-_{0}\|_{2}\|}- _{0}\|_{2}}].\] (3)

Note that steerability (for a single example) is equal to the product of sensitivity and directionality. Thus, for a model to be steerable by definition, it needs to be close to 1 for both metrics. Intuitively,

Figure 1: Text rewriting (left) can be modeled as movement in a vector space of goals (right). We propose two steerability metrics (sensitivity and directionality) that quantify distance and direction-based closeness between user requests (green) and LLM movement (orange) in goal-space. Source text from _Meditation XVII_, _Devotions Upon Emergent Occasions_, John Donne (1624).

if \(\|^{*}-_{0}\|_{2}<\|}-_{0}\|_{2}\), the LLM is may be "overshooting": if the angle between \(\|^{*}-_{0}\|_{2}\) and \(\|}-_{0}\|_{2}\) is small, the LLM may be exaggerating desired changes to text. Note that \(\|^{*}-_{0}\|_{2}>\|}-_{0}\|_{2}\) implies the opposite. A large angle between \(^{*}-_{0}\) and \(}-_{0}\) may signal "side-effects" in requesting a goal, such that asking for changes in certain goals induces changes in unrelated goals. We summarize our steerability metrics in Figure 1.

Note that we define steerability in terms of \(_{0}\) (Eq. 1): steerability for source texts in a small subset of goal-space does not imply general steerability for all source texts. An input-agnostic definition of steerability (_e.g._, \(_{^{*}}[\|^{*}-}\|_{2}]\)) could fail to capture steerability across goal-space. Our two-metric view of steerability (Eq. 2 & 3) also allows us to disaggregate different steerability failures with respect to LLM inputs.

### Implementing a Steerability Probe

We design a steerability probe following our framework. We begin with a seed set of source texts, from which we define a set of goal dimensions and measurement methods. We then sample a diverse dataset \(\) of starting goals (\(_{0}\)) and ending goals (\(^{*}\)). This probe is held fixed and applied across LLMs to compare steerability across models.

Sampling diverse texts.We concatenate subsets of four English-language datasets to collect a diverse seed set of source texts: CNN/Dailymail (\(N=1220\), See et al. (2017)), BookSum (\(N=2133\), Kryscinski et al. (2021)), Reddit TIFU (\(N=2991\), Kim et al. (2018)), and SCROLLS (SummScreenFD only, \(N=338\), Shaham et al. (2022)). These datasets encompass expository (CNN/Dailymail, SCROLLS), narrative (Reddit TIFU), and creative writing (BookSum), as well as formal (CNN/Dailymail, SCROLLS, BookSum) and colloquial (Reddit TIFU, BookSum) English.

Defining dimensions of goal-space.We aim to sample a diverse set of texts in goal-space. To map texts to goal-space, we choose a set of goal dimensions, and use existing models and metrics that map strings to scalars. This yields a vector-based representation of an arbitrary text. Leveraging existing models and metrics to compute goal-space mappings means that goal-space mappings are computed identically for all LLMs, and do not require knowledge of the underlying LLM parameters.

As goal-dimensions, we select reading difficulty (Flesch-Kincaid grade level; Kincaid et al. (1975)), text diversity (Median Lexical Text Diversity; Jarvis & Hashimoto (2021)), text length (word count), six different tones corresponding to different emotions (via a sentiment classifier; Hartmann (2022)), and politeness (via a weighted sum of politeness strategies; Danescu-Niculescu-Mizil et al. (2013)). We also measure (but do not manipulate) six aspects of toxicity as measured by the Detoxify model (Hanu & Unitary team, 2020). Scores for the six emotions, toxicity aspects, and politeness were computed by sentence, then averaged. This yields 10 requestable goals and 6 toxicity-related goals. We use these models and metrics to map all texts to goal-space.

To produce a diverse sample of texts, we normalize all goal dimension via linear scaling such that the middle 95% of values observed in our seed set of texts maps to \(\). We compute uniform sampling weights for the 10 non-toxicity related goal dimensions via classifier-based density ratio estimation (Bickel et al., 2007). Further dataset processing details are in Appendix A.1.

Sampling user goal vectors.User goal vectors are 10-dimensional vectors in \(^{10}\), with each dimension corresponding to our 10 requestable goals. To create goal vectors, we randomly choose 3 goal dimensions for each text and sample target goal _values_\(z_{i}^{*}\) (\(i\)th component of \(^{*}\)) as follows:

\[z_{i}^{*}:=z_{0,i}+_{i};_{i}([\{-z_{0,i}, -0.7\},-0.1][0.1,\{0.7,1-z_{0,i}\}])\] (4)

where \(z_{0,i}\) is the \(i\)th component of \(_{0,i}\), _i.e._, we sample target goal to lie between 0.1 and 0.7 in absolute distance to the original value (\(z_{0,i}\)). We set \(_{i}=0\) in goal dimensions not chosen for modification. The \(\) and \(\) clip \(z_{i}^{*}\) to the range \(\). We sample 40 goal vectors per text in a sample of 50 source texts for a total of 2000 (text, goal) tuples.

To convert \(^{*}\) into natural language prompts, we use a template-based prompt that asks for "slightly more (less)", "more (less)", or "much more (less)" of each goal dimension (examples in Appendix A.2). We also conduct a sensitivity analysis of prompting with a more granular prompt (specifying changes on a 1-10 scale; Appendix B.2).

Our empirical steerability probe allows us to test a variety of hypotheses on whether LLM characteristics or design choices affect steerability. As a representative example, consider the impact of model size on steerability. Some evaluations find that larger models are better instruction-followers (_e.g._, Chatbot Arena ELO, MT-Bench score (Zheng et al., 2023)), suggesting that larger models could be more responsive to user requests.1 Concurrent work in prompt engineering finds that larger models are less sensitive to prompt paraphrases (Schnabel and Neville, 2024), implying the opposite. Our probe provides a means to investigate whether larger models are more or less steerable.

## 3 Are Larger Models More Steerable?

Experimental setup.To evaluate whether larger models are more steerable, we compare smaller versions of models with larger versions in three model families: GPT-3.5 vs. -4 turbo (unknown changes; Achiam et al. (2023)), Mistral/Mistral (non-mixture model vs. mixture-of-experts; Jiang et al. (2023, 2024)), and Llama3 (8B vs. 70B; Dubey et al. (2024)). We assume that GPT-4 is larger than GPT-3.5.2 The sampling temperature is set to 0. As a quality check, we manually examine a subset of generated LLM outputs to ensure that the request was handled correctly (Appendix A.4).

### Bigger models are not always more steerable

Our results indicate a negligible relationship between model size and steerability metrics, suggesting that larger models are not always more steerable. Figure 2 shows histograms of steerability metrics for GPT (3.5 vs. 4, in yellow), Llama 3 (8B vs. 70B, in blue), and Mistral (7B vs. 8x7B/mixture of experts, in green).3 The high overlap between histograms across all models suggests that the models evaluated exhibit similar levels of steerability, including models of different size/generation within the same family. The standard deviation (SD) of all metrics is also large (_e.g._, for GPT-4, SD(steerability) = 0.47, SD(sensitivity) = 0.92, SD(directionality) = 0.27). All models evaluated have steerability lower than 1, which is attributable to low directionality: the mean directionality for models evaluated ranges from 0.22 (Mistral) to 0.33 (Llama3-70B). However, mean sensitivity exceeds 1 for all models, suggesting that models potentially "overshoot" in goal-space.

Differences in steerability are also larger between families of LLMs than within LLM families. GPT-3.5 and 4 have mean steerability values of 0.37 and 0.41 (difference: 0.04), respectively, while Llama-8B and -70B obtain values of 0.55 and 0.58 (difference: 0.03), respectively. Yet the directional awareness gap between the Llama and GPT model families is 0.14, which is \(3.5\) to \(4.7\) larger than the within-family directional awareness gap. Similar trends hold for sensitivity and directionality. Thus, factors other than model size may be responsible for the variation (albeit small) in steerability across models in our steerability probe.

Figure 2: Steerability (left), sensitivity (middle) and directionality (right) histograms for GPT (yellow), Mistral (green), and Llama (blue) model families. Vertical dashed lines indicates means for each model. Pink dotted line indicates optimal values. \(S_{}\) : cosine similarity.

Ultimately, our probe demonstrates that larger models are not necessarily more steerable. In particular, the small gaps in steerability metrics across different model generations within the same family suggest updates to LLM development (_e.g._, model pre-training/fine-tuning and RLHF techniques) may not map to improvements in steerability.

### LLMs move in extraneous directions in goal-space

Requesting goals comes with side-effects.We hypothesize that, since our prompts only specify a subset of possible goals, the prompt is implicitly _underspecified_. Thus, we may observe extraneous correlations between requested movement in goal-space and movement in other goals: a _side-effect_ of requesting a particular goal. Such behavior could potentially yield LLMs with high sensitivity, yet poor directionality, as previously observed.

To investigate side-effects, we compute the adjusted Pearson cross-correlation between requested goals and measured goals. Since the goal-space mapping of a source text potentially leaks information about the target goal \(^{*}\) (_e.g._, a text in an extremal region of goal-space has less "range of movement" than a less-extreme text), an unadjusted cross-correlation metric could overestimate the effect of requested goals on goal-space movement. We correct for this by subtracting the cross-correlation between goals observed under an uninformative prompt, which we further discuss in Appendix A.3.

Figure 3 shows the adjusted Pearson cross-correlation (\(\)) between requested vs. observed goal-space movement for GPT-4. Off-diagonal elements significantly larger than zero are potential side-effects. While some correlations are expected (_e.g._, increasing verbosity is associated with greater reading difficulty, \(=0.45\)), others may not always be desirable. For example, increasing sadness is associated with greater fear (\(=0.26\)), while greater politeness is associated with greater reading difficulty (\(=0.35\)). Similar correlations persist in other models (Appendix B).

Note that seven of ten diagonal elements of the adjusted cross-correlation matrix are significantly larger than zero. A larger-than zero adjusted correlation signifies that, compared to an uninformative prompting strategy, requests to change a particular goal are more positively correlated with move

Figure 3: Adjusted cross-correlation (\(\)) between requested movement (vertical axis) and observed movement (horizontal axes) across goal dimensions, divided by non toxicity-related (left) and toxicity-related (right) dimensions. Negative (positive) correlations in blue (red); darker shades denote larger-magnitude correlations. “*” denotes statistical significance (\(=0.05\) with Bonferroni correction; \(n=160\)).

ment in the same goal. In other words, LLMs appear to successfully follow most instructions when requested, consistent with empirical evidence of their strong instruction-following abilities.

Are side-effects features or bugs?Our results suggest that the non-steerability of LLMs is not due to poor instruction-following or sensitivity to model inputs, but a previously-undocumented phenomenon: side-effects. These changes could be appropriate: for example, inverse correlations between politeness and disgust (\(=-0.24\), Figure 3) in the tone of a text are subjectively suitable.

Yet other side effects do not reflect semantically inherent relationships. Fear and sadness, while negative in sentiment, are not equivalent. Similarly, politer text is not always harder to read. Such correlations could be a function of the pre-training data: _e.g._, perhaps politeness varies with _formality_ in the training data, both of which correlate with increased reading difficulty. However, a steerable model should be able to independently manipulate each textual aspect even given such dataset biases. In safety-critical settings, understanding and controlling side-effects is critical to evaluating the feasibility of LLM usage. Our findings motivate future study on the prevalence and (un)desirability of side effects in LLM behavior, and strategies for controlling undesirable side effects.

## 4 Related work

Steerability in LLMs and generative models.Many interventions for LLM steerability directly update model weights, such as activation steering (Turner et al., 2023; Rimsky et al., 2023; Konen et al., 2024), mono-semantic feature scaling (Templeton, 2024), or model-guided generation (Dathathri et al., 2020; Khalifa et al., 2021), which elicits pre-specified changes in text aspects during generation. Others leverage prompting, _e.g._, instance-specific hints (Li et al., 2024), global control codes (Keskar et al., 2019) and persona-based prompting (Li et al., 2024; Liu et al., 2024), which aim to align LLMs with pre-specified constraints or goals. While these methods aim to improve steerability, in contrast, we focus on first developing a principled framework for quantifying steerability. Sorensen et al. (2024) also propose a definition of "pluralistic steerable models," which is closest to our definition of steerability. We highlight that our steerability probe is one of the first practical instances of a "trade-off steerable benchmark" as proposed in their position paper.

We acknowledge that discourse on controllable generative models predates LLMs: the steerability of latent factors (Jahanian et al., 2020; Spingarn-Eliezer et al., 2021) and disentangled representation learning (Higgins et al., 2018; Locatello et al., 2019) are well-studied in generative adversarial networks, and more recently in large text+image models (Liu et al., 2022; Gavrikov et al., 2024) and graph generators (_i.e._, molecule editing; Liu et al. (2024); Du et al. (2022)). Latent factors are similar to our notion of _goals_, and framing disentangled representations Higgins et al. (2018) as independently manipulable subspaces (_i.e._, _goals_) is reminiscent of our notion of side-effects. Our framework is potentially applicable to assessing the steerability of generative models beyond LLMs.

Multi goal/objective-aware text generation.Past works concerning LLM alignment towards multiple potentially-competing objectives or goals have proposed model-aggregation approaches (Rame et al., 2024; Jang et al., 2023) and RLHF variants with multi-goal rewards (Dong et al., 2023; Wang et al., 2024). The multi-goal objective function used by Wang et al. (2024) is closest to our definition of steerability, which also uses a vector-based model of user preferences based on attribute dimensions. However, rather than anchoring to the text generation process, we design a general probe for measuring and _comparing_ out-of-the-box steerability in existing LLMs.

Understanding and probing LLM in-context behavior.Behavioral probes for LLMs include mechanistic interpretability approaches (Bricken et al., 2023; Templeton, 2024), which leverage sparse coding to extract interpretable dimensions of LLM behavior from model parameters. Other works explore trade-offs in LLM behavior, such as helpfulness vs. harmfulness (Liu et al., 2024), or alignment with political biases in the United States (Liu et al., 2024). Steerability can be seen as a case of pluralistic alignment in-context, for which we contribute an empirical probe.

## 5 Conclusion

Our empirical results suggest that current LLMs are not steerable. Although the results highlight ample room for improvement in steerability, our findings do not contradict that LLMs are powerful instruction-followers. Rather, our work highlights steerability as an independently important criterion for LLM evaluation. We highlight possible directions for future work. Our steerability probe can be applied to evaluate how other design choices in LLM development, such as aspects of LLM pre-training, RLHF, or changes in decoding parameters (such as the sampling temperature), affect steerability. Assessing existing steerability interventions via our probe could also contextualize progress in controllable LLMs. On the theoretical side, analyzing steerability could surface measurable prerequisites for steerability and how they intersect with existing approaches to pre-training/fine-tuning and alignment (_i.e._, RLHF). Ultimately, our work lays a systematic foundation for quantitative steerability evaluation.