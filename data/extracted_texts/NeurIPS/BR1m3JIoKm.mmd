# BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information

Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu,

**Viava Imbrasaite, Deepak Ramachandranan**

Google Research

{mehrankazemi, yquan, bhatiad, njkim, xxujasmine,

vimbrasaite, ramachandrand}@google.com

###### Abstract

Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of _defeasible reasoning_, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.

## 1 Introduction

A fundamental goal of AI since its early days has been automatically applying logical or deductive reasoning to draw new conclusions from existing knowledge . Since a large amount of knowledge is available in the form of natural language, tremendous effort has been put into developing models that can understand and reason over natural language  (see  for a survey). Recent years have seen substantial improvements in this direction thanks to advancements in pretrained language models (LMs)  that can handle unstructured data more flexibly, combined with advanced prompting techniques , and modular reasoning approaches .

Existing work in automated reasoning in natural language usually assumes that the provided knowledge is consistent and reliable. But in many applications, the collection of information one has to reason with is inconsistent and contradictory. This is the case, for instance, when reasoning is performed with information found in different online sources or social media (e.g., retrieval-augmented LMs ). When input sources are contradictory, one can consider various strategies to resolve the contradictions. One simple and practical formulation, which we adopt in this work, is to resolve the conflicts based on preferences over the information sources: when a conflict arises, the information from the source with a higher preference should be used to solve the reasoning problem. Depending on the application, preferences can be assigned based on different criteria, e.g., based on the credibility of websites or social media users, or based on the recency of the information with newer information being preferred over older information. Exceptions to generics can also be expressed as preferences; for example, generic knowledge such as _"birds fly"_ (see also ) should be overridden by exceptions such as _"penguins are birds but do not fly"_ (see also ) when reasoning about penguins. Figure 1 demonstrates an example of a reasoning problem with conflicting information, where the conflict is resolved based on recency.

Reasoning with conflicting information guided by preferences can be formulated as a form of the classical _defeasible reasoning_ problem [33; 19; 28]. In this work, we study the reasoning ability of LMs in this setting. Toward this goal, we create a synthetic dataset where each example contains a defeasible theory (a set of input facts, possibly contradictory rules, and preferences over the rules), and a question about that theory. Answering the questions in the dataset requires multi-hop reasoning and conflict resolution over the input theory. The difficulty level (e.g., the depth, amount and type of conflicts, etc.) of the examples in the dataset can be controlled automatically, enabling targeted comparisons of various aspects of reasoning.

We also note that while a large number of logical reasoning benchmarks provide all the knowledge needed to answer questions [48; 41; 42; 18], such benchmarks do not reflect common real-world scenarios where implicit background knowledge plays an important role in reasoning. Moreover, models that translate the textual examples into logical form and then leverage off-the-shelf solvers may excel on these datasets, which does not reflect the true performance of such models in real-world applications. For these reasons, in BoardgameQA only part of the knowledge required to solve the problem is provided as input to the LM; the missing knowledge has to come from the LM itself.

The problems in our dataset are formulated as scenarios of a board game, hence we name it BoardgameQA1. A board game theme allows us to create synthetic scenarios with complex defeasible rules to reason about that seem natural when stated in text and hence allows background commonsense world knowledge to also be used. To the best of our knowledge, BoardgameQA is the first dataset for multi-hop reasoning _with contradictory inputs_. Figure 2 shows a sample example from the dataset where the conflict resolution and missing knowledge have been highlighted.

We benchmark various LMs on BoardgameQA and measure their defeasible reasoning capacity. Most notably, our results reveal that LMs perform poorly when reasoning with conflicting sources, especially in the few-shot setting (compared to the finetuning setting) suggesting that preference understanding and defeasible reasoning capacities do not surface out-of-the-box in pretrained LMs. Secondly, we find that smaller LMs perform poorly when not all of the required information is provided as input. These results highlight a critical gap in the reasoning capacity of current LMs, considering that reasoning over contradicting and incomplete sets of information is a common scenario in many applications, and is key for developing robust AI systems.

## 2 Related Work

Our work spans three dimensions: 1- text-based logical reasoning, 2- reasoning with conflicting sources, and 3- reasoning with incomplete information. In the following section, we briefly summarize the literature on each of these axes that relate to our work.

Figure 1: A reasoning problem with contradictory information (conflict resolved based on recency).

**Text-based logical reasoning approaches:** Earlier works on natural language logical reasoning have finetuned LMs to directly provide answers to logical reasoning questions [11; 4; 40; 18]. Later work showed that explicitly generating the entire proof leads to substantial improvements both in the case of finetuning and in the case of few-shot learning [31; 13; 58; 60]. In addition, modular reasoning approaches where the LM is used as a tool within a reasoning algorithm [23; 12; 51; 24] have been shown to achieve both performance gains and more precise intermediate proof chains. In this paper, we experiment with four types of approaches: 1- finetuning without explicit reasoning steps, 2- finetuning with explicit reasoning steps, 3- prompt-tuning with chain-of-thought (CoT) prompting , and 4- few-shot in-context learning with CoT.

**Text-based logical reasoning datasets:** Many datasets have been created to measure the logical reasoning ability of NLP models [48; 42; 61; 18; 45]. In Table 1, we provide a comparison of (a subset of) these datasets along three desired features in this work. All datasets compared contain only facts and rules that are non-contradicting. The datasets closest to our work are _CREPE_, FalseQA  and _ConditionalQA_; the first two provide false pre-suppositions in the question which can be considered as statements that contradict the ground truth, and in last one the answers to the questions follow a _"If X then yes, if Y then no"_ format.

**Reasoning with conflicts:** From the early days of AI, reasoning with conflicting information has been an important topic and many approaches have been developed to handle such conflicts [34; 33; 37]. The problem we study in this paper is an instance of defeasible reasoning [33; 19; 28] which has applications in various domains (especially in legal reasoning) [43; 16; 7] and has been argued to be one of the most important future directions in a recent survey of LM reasoning literature . In defeasible reasoning, there are preferences over the rules and in the case of conflict between two rules, the conclusion from the higher preference rule is accepted. Previous work on defeasible reasoning with natural language has studied the problem of adjusting the probability of a conclusion based on new (single-hop) evidence [39; 27]. Our work extends this line of work by developing a dataset for multi-hop defeasible reasoning with preferences over sources.

**Reasoning with incomplete information:** Several existing reasoning benchmarks adopt a setup where part of the required information is missing and needs to come from the model itself [46; 5; 2; 50]. Some datasets also employ a setup in which none of the required rules are provided as input [49; 15; 45; 22]. Our work focuses mainly on cases where part of the knowledge needs to come from the model and another part of the knowledge is provided as input.

## 3 Background and Notation

We let \(=\{e_{1},,e_{N}\}\) and \(=\{p_{1},,p_{M}\}\) represent a set of entities and predicates. We represent a fact in the logical form using the triple notation \((e_{i},p_{j},e_{k})\), where \(e_{i},e_{k}\) and \(p_{j}\), and a rule as \(r:r_{b} r_{h}\) where \(r_{b}\) represents the body of the rule and \(r_{h}\) represents the head. We use \(!\) to indicate negation. A monotonic theory \(=(,)\) is a tuple containing a set \(\) of (positive or negative) facts, and a set \(=\{r_{1},,r_{||}\}\) of rules. We let \( f\) represent that the fact \(f\) can be derived from the theory \(\) using the standard inference rules of logic (See Shoonfield ). For a monotonic theory \(=(,)\), if \( f\), then for any theory \(^{}\) such that \(^{}=(^{},)\), we also have \(^{} f\) (that is, adding new facts does not change previously derived facts).

**Defeasible Theory:** A defeasible theory \(^{(d)}=(,,)\) is a triple containing a set \(\) of facts, a set \(=\{r_{1},,r_{||}\}\) of rules, and a set \(=\{r_{t_{1}}>r_{t_{2}},,r_{t_{3}}>r_{t_{4}}\}\) of pair-wise relative

Figure 2: A sample example from BoardgameQA that requires one hop of reasoning. The text in violet highlights conflict resolution and the text in blue highlights the missing information.

[MISSING_PAGE_FAIL:4]

**Entities and predicates:** We start with a predefined set of entities \(\) (e.g., _dog, cat, lion,_ etc.) and a predefined set of predicates \(\) (e.g., _invite for dinner, attack the fields,_ etc.) that we sample from to generate facts and rules. We use the animals as entities and the boardgame-inspired verbs/operations as our predicates. Using these entities and predicates, we can create facts such as _the dog attacks the fields of the lion_. To make the problem more challenging, we use different entities and predicates across training and test similar to . The full list of entities and predicates is provided in Appendix C.3.

**Rule types:** We adopt a set of 6 rule templates containing existential and universal quantifiers, conjunctions, and missing information. The rules are as follows: 1- \( X:(X,p_{1},e_{1})(X,p_{2},e_{2})\), 2- \( X:(X,p_{1},e_{1})(X,p_{2},e_{2})(X,p_{3},e_{3})\), 3- \((e_{1},p_{1},e_{2})(e_{2},p_{2},e_{3})\), 4- \((e_{1},p_{1},e_{2})(e_{3},p_{2},e_{2})(e_{2},p_{3},e_{4})\), 5- \((e_{1},,)(e_{1},p_{2},e_{2})\), and 6- \( X(X,p_{1},e_{1})(e_{2},p_{2},e_{3})\), where \(X\) represents a universally or existentially bounded variable, each \(e_{i}\) represents an entity, and each \(p_{j}\) represents a predicate. The fifth rule template corresponds to a rule where the predicate (or object entity) in the rule body may not be an element of \(\) (resp. \(\)). For more information, see below.

**Selecting a question:** To generate each example, we first sample a question \(q=(e_{i},p_{j},e_{k})\) that should be proved or disproved, where \(e_{i}\) and \(e_{k}\) are sampled from \(\) and \(p_{j}\) is sampled from \(\). We also sample the sign of the question (positive or negative). For example, we might sample the question _!(dog, attack the fields, lion)_ asking whether _the dog does not attack the fields of the lion_. The question is then converted into natural language using a template (see Appendix C.3).

   Category & Description & Example Facts & Example Rule \\   Time Conversion & Compense the age of an entity to a certain age specified with different units. & The dog is 13 months and a half old & If the dog is more than a year old, then \\  Orthography & Asts about the letters in names. & The dog is named Pacto. The cat is named Pashnak. & If the dog has a name that starts with the same letter as the name of the cat, then \\  Number & Some numbers are required to be summed & The dog has two friends that are nice and then compared to other numbers. & If the dog has less than 10 friends, then \\  Lexical Ertaisment & The fact and the rule body are not identical & The dog sexsinehed the major & If the dog killed the major, then... \\  World Knowledge & Some knowledge about the word is & The dog is currently in Conada, then \\ edge & needed to connect the fact to the rule body & The dog is currently in Montreal. & If the dog is currently in Canada, then... \\  Event Times & Knowledge about times of events is needed & The dog is watching a movie that was released after Covid19 started, then... \\  & to connect the fact to the rule body. & was released in 2005. & If the dog is watching a movie that was released after Covid19 started, then... \\  Part Of & The fact and the rule body have a part of & The dog is a nurse & If the dog works in healthcare, then... \\  & relation/information & The dog has a knife & If the dog has a sharp object, then... \\   & The dog has a ball with a radius of & If the dog has a ball that fits in a 28 x \\  & objects is required. & 15 inches. & 35 x 35 inches, then... \\   

Table 2: Categories, descriptions, and examples of incomplete information in BoardgameQA. For lexical entailment, world knowledge, event times, and affordance, a list of examples is written manually from which the sampling procedure can select. In others, examples are generated automatically.

**Theory generation:** The theory generation is the main component of the dataset generation that constructs the facts, rules and question to be used in each example. A high-level description is provided in Algorithm 1 and an example generation is shown in Appendix C. We first sample some sub-questions \(=\{q_{1},,q_{n}\}\) and a rule \(r\) which has \(\) in its body and \(q\) in its head, such that \(q\) can be derived from \(\) and \(r\). The sampling is done by first selecting one of the aforementioned rule types, then matching the head of the rule to the question \(q\), and then sampling sub-questions \(\) based on the body of the rule. For example for the question _!(dog, attack the fields, lion)_, we might sample the first rule type (see the six types above), then \(p_{2}\) will be mapped to _attack the fields_ and \(e_{2}\) will be mapped to _lion_, and we also sample a sub-question such as _(dog, unite with, cat)_ and add the rule \( X:(X,})!(X,},})\) to our set of rules. We then make a recursive call for each \(q_{i}\) to generate new rules and facts for them.

We then decide whether a conflict should be introduced or not, by using a biased coin flip with \(p_{}}\) representing the probability of conflict. If the decision is to produce conflicts, then we generate another set of sub-questions \(^{}=q^{}_{1},,q^{}_{m}\) and another rule \(r^{}\) such that \(!q\) can be derived from \(^{}\) and \(r^{}\). Then we probabilistically decide if we want to generate a Type1 or a Type2 conflict using a biased coin flip with probability \(p_{}}\). If the first case is selected, then \(r>r^{}\) is added to the preferences. In this case, we can make recursive calls for all or a subset of the facts in \(^{}\). Otherwise, \(r^{}>r\) is added to the preferences. In this case, we make recursive calls for _all but one_ of the facts in \(^{}\) (selecting randomly) to ensure that \(r^{}\) does not activate.

**Proofs:** We keep track of the facts, rules, and preferences during the generation process and turn them into proofs for the examples.

**Stopping criterion:** Every time we make a recursive call to the function in Algorithm 1, the example will contain one extra hop in its proof. We set the stopping criterion as the number of hops in the proof. Toward this goal, we included an argument \(d\) in Algorithm 1 which corresponds to the target maximum number of hops in the proof; \(d\) decreases by one every time we make a recursive call. When the algorithm is called with \(d=0\), instead of generating rules and sub-questions for the input question \(q\), we simply add \(q\) to our set of facts.

**Incomplete information:** We generate examples with incomplete information where part of the knowledge should come from the LM (corresponds to rule type 5). For a question \(q\) in the theory generation phase, we sample sub-questions \(\) and rule \(r\) such that \(}\) can be derived based on \(\) and \(q\) can be derived from \(}\) and \(r\). We then hide \(}\) from the model so the model has to derive it itself. Algorithm 2 describes the procedure. We use a separate body of world knowledge, commonsense knowledge, mathematical, and orthography reasoning for generating \(\) and \(}\) (see Table 2 for a high-level description and Appendix C.2 for more details). For example, for the goal _"the dog unites with the cat"_ we generate the sub-question _"The dog is in Montreal."_ and the rule _"If the dog is in Canada, then the dog unites with the cat."_. Then, an extra reasoning step is needed from the model to recognize that Montreal is in Canada.

We generate sub-questions and rules that require extra knowledge and reasoning with probability \(p_{}}\); otherwise, we create sub-questions and rules that require no extra knowledge and reasoning. To make the problem more challenging, we only include some categories of extra knowledge and reasoning in the training set; this ensures that the models cannot simply learn the extra knowledge from the training set and use it in the test set.

**Conversion to natural language:** Finally, once we generate the facts, rules, preferences, and question, we use manually constructed templates to turn each of them into a textual format. To make the problem more challenging, we use multiple templates per rule type and use some of the templates only in the test set (see Appendix C.3 for details).

A comparison of BoardgameQA with other prominent deductive reasoning datasets in terms of the average length of examples and the average number of unique tokens per example is provided in Figure 3.

Figure 3: A comparison of BoardgameQA with ProofWriter  and PrOntoQA  in terms of average length of examples and average number of unique tokens per example on depth 3 of the datasets.

**Disproved and unknown examples:** So far, we described how to generate examples with the label _proved_. Generating examples with the label _disproved_ can be done simply by first generating an example with the label _proved_ and then negating the question. Also, generating examples with the label _unknown_ can be done by perturbing the theory until the statement in the question cannot be derived from the theory (e.g., reducing the amount of money of the frog to 50 dollars in the example of Figure 2). We randomly select and apply the following perturbations to the theory and run a defeasible solver implemented based on the scalable solver in  on the resulting theory until the label becomes unknown: 1- change the predicate of a fact or a rule, 2- change the sign of a fact or an element of the rule, 3- replace a fact with a new fact, and 4- flip the order of a preference.

## 5 Experiments

One of the primary goals of our experiments is to verify if LMs are capable of reasoning in a defeasible setup. For this reason, we conduct experiments with various LM architectures (encoder-only, encoder-decoder, and decoder-only) and various pre-training and learning paradigms (finetune with and without proofs, prompt tuning, few-shot in-context learning, and instruction-tuned). Specifically, we test 1) finetuning BERT-large  with a classification head to predict the label directly, 2) finetuning TS 1.1 XXL  to generate the entire proof and then the label, 3) few-shotting PaLM 62B and PaLM 540B  where we provide demonstration examples and chain-of-thought (CoT) in the prompt (the CoT corresponds to the proof), 4) few-shotting the instruction-finetuned FLAN-PaLM 540B  with CoT, and 5) soft prompt-tuning  PaLM 62B with CoT where instead of providing a static prompt, we make the prompt embedding learnable and tune its parameters using the training data (the rest of the LM parameters are frozen). We report classification accuracy as the metric. We also report the _majority class_ baseline (\(\)33% since our labels are balanced).

**Dataset sizes:** To gain a more detailed understanding of the models' defeasible reasoning capacity, we create several variations of BoardgameQA. The nature of the variation will be discussed in the remainder of this section with each experiment. For each variation, we sample \(1000\) examples for train, \(500\) for validation, and \(1000\) for test. We sample an equal number of examples from each label.

### Can LMs Reason with Contradictory Inputs?

As explained in Section 4, BoardgameQA makes use of a number of variables that control various aspects of the dataset such as the amount and types of conflict and the amount of extra knowledge required. We start by creating a default version of the dataset that exhibits each of these properties to some degree by setting \(p_{}=0.5\), \(p_{}=0.5\), and \(p_{}=0.5\). We then generate three datasets with depth 1-3 (i.e., requiring 1-3 hop(s) of reasoning, respectively), and measure the performance of our baselines on these datasets.

The results are in Figure 4. The tuned models perform reasonably on depth 1, but their performance substantially degrades on depths 2-3. This contrasts with previous observations for monotonic reasoning (e.g., in [11; 48]) where finetuned LMs reach near-perfect performance even on higher depths. This indicates that reasoning with contradictory inputs is more difficult even with finetuning. Moreover, we see that the few-shot models perform poorly across all depths showing that conflict resolution is not achieved out-of-the-box with pretrained models. This includes both PaLM and instruction-finetuned FLAN PaLM models. PaLM 540B performs better than PaLM 62B showing that

Figure 4: The model performances on depths 1–3 of the BoardgameQA dataset. Many models struggle on this dataset, especially with higher depths.

larger models may have higher capacity for defeasible reasoning. More insights from full confusion matrices can be found in Appendix A.

Hereafter, due to inference costs, we only experiment with finetuned BERT and T5, prompt-tuned PaLM 62B, and few-shot PaLM 540B, and with examples of depth 2 to keep a medium level of difficulty in terms of reasoning hops and enable measuring the effect of the other factors.

### Does Correct Label Prediction Mean Correct Proof?

Recently, it has been shown that although large LMs achieve high accuracy on label prediction for (monotonic) reasoning task, they do so by generating spurious proofs that do not represent valid steps of reasoning . There is also evidence that LMs frequently exploit spurious correlations in the data distribution to achieve high label accuracy, rather than reasoning purely deductively . Hence we design evaluation metrics to reflect a more rigorous measure of accurate defeasible reasoning. In the case where a model predicts the label correctly, and the label is one of _proved_ or _disproved_ (where an actual proof exists), we measure whether the proof generated by the model is correct or not. For this purpose, we compute two automated proof accuracy metrics (named _Rule F1_ and _Conflict F1_) and one manual metric (named _Overall Proof Accuracy_) as described below. For _Rule F1_, we extract the rules used in the golden proof and the ones in the proof generated by the model that are used to derive new facts (and ultimately, the goal). Then we compute the F1-score of the overlap of the two sets. For _Conflict F1_, we extract the conflict resolutions (corresponding to pairs of rules) used in the gold proof and the ones in the proof generated by the model, and compute the F1-score of their overlap. For _Overall Proof Accuracy_, we manually verify whether the proof is correct for \(50\) sampled examples per model. We compute these metrics on depth 2 of the dataset.

According to the results in Figure 5, all models perform relatively well in selecting the correct set of rules for the proof. The few-shot model performs poorly on conflict resolution whereas the tuned models perform substantially better, suggesting that preference understanding and conflict resolution do not surface with simple few-shot prompting, and tuning is required for models to exhibit this capacity. Second, the models often generate wrong proofs, even when they predict the label correctly. The issue is less severe in the case of the prompt-tuned model but becomes more severe for the finetuned and few-shot models. We provide examples of proof failures in Appendix A.

### Do Conflicts Make Reasoning More Difficult?

We create four versions of BoardgameQA named NoConflict, LowConflict, Medium-Conflict, and HighConflict, with \(p_{}\) set to 0.0, 0.2, 0.5 and 0.8 respectively; other factors are kept the same. Note that the MediumConflict corresponds to the dataset in Figure 4. The results of the models on these datasets are reported in Figure 6. The performance of all models monotonically degrades as the number of conflicts increases, showing that conflict resolution is indeed a major factor in the difficulty of the problems. For example, BERT performs above-random for the NoConflict and LowConflict cases, but the model performance drops to near-random on MediumConflict and HighConflict cases.

### Which Conflict Type is More Difficult to Resolve?

To test which type of conflict (See sec. 4) is more difficult for the models, we create three versions of the dataset with varying proportions of Type1 vs Type2 conflicts, by setting \(p_{}\) to 0.2, 0.5, and 0.8 respectively. The first dataset mostly contains conflicts of Type1, the second contains both

Figure 5: Proof accuracy metrics for various models on depth 2 of the dataset, when the label is predicted correctly.

Figure 6: The model performances on four versions of the BoardgameQA dataset with various amounts of conflicts in them.

conflicts in a similar amount, and the third dataset contains mostly Type2 conflicts. The other factors are kept constant across the datasets.

The results of the models are reported in Figure 7. We see that models perform slightly better on the dataset with mostly Type1 conflicts. This discrepancy between performance on Type1 and Type2 conflicts is intuitive because in the case of Type1 conflicts, the model can ignore the conflicting rule and whether its body can be proved, but in the case of Type2 conflicts, the model has to show that at least one of the elements in the body of the conflicting rule cannot be proved. In the case of tuned models, we furthermore observe that biasing the dataset toward one conflict type results in better performance overall. This might be because the model mostly needs to learn to resolve one type of conflict which may be easier than learning both.

### Does Information Incompleteness Make Reasoning More Difficult?

As described in Section 4, we can control the amount of information incompleteness using a parameter which we named \(p_{}\). To test how the information incompleteness affects the performance of various models, we create three versions of our dataset with \(p_{}\) set to \(0.2\), \(0.5\) and \(0.8\), which we name _KnowledgeLight_, _KnowledgeMedium_ and _KnowledgeHeavy_, respectively.

The results are reported in Figure 8. We observe that as the amount of required knowledge increases, the performance of the fine-tuned models decreases accordingly. However, the performance of the prompt-tuned and few-shot models remain relatively unchanged, likely due to the larger size of the model and the extra amount of knowledge that is present in the model, as well as the fact that working with real-world knowledge might be easier for these models than with artificial knowledge.

### Do Distractors Make Reasoning More Difficult?

We also measure the effect of distracting facts and rules on model performance. A distracting fact or rule is one that does not appear in the proof and does not change the label. In Figure 2, for example, _"the frog has a knife"_ is a distracting fact. To this end, each time we call Algorithm 1, besides the sampled sub-questions, we also sample some distracting sub-questions and add them to the set of sub-questions. We create three versions of the BoardgameQA dataset where we add 0, 1, and 2 distracting facts in each step, which we name _NoDistractors_, _SomeDistractors_, and _ManyDistractors_, respectively.

According to the results in Figure 9, the performance of the tuned models does not substantially degrade with a small number of distractors, potentially because the distractors can help the model avoid learning spurious correlations. However, their performance drops substantially with more distractors. Also, with more distractors, the performance of the few-shot model decreases monotonically,

Figure 8: The model performances on three versions of BoardgameQA with various degrees of incomplete information.

Figure 7: The model performances on three versions of the BoardgameQA dataset with different distributions on the type of conflicts.

Figure 9: The model performances on three versions of BoardgameQA with various amounts of distracting facts and rules.

although only marginally (this observation is consistent with the results of ). This shows that distractors (that are typically common in real applications) can also compound the problem difficulty.

## 6 Limitations

Our dataset, in its current form, focuses primarily on deductive logical entailment, where the problem is a classification problem (\(label\{proved,disproved,unknown\}\)), and the contradictions are also binary (i.e. one rule suggesting something is True and the other suggesting it is False). Future work can extend BoardgameQA and the analysis provided in this work to non-classification cases where 1- one needs to apply defeasible logical reasoning to answer questions such as "Who will be attacked by the dog?", 2- one needs to resolve non-binary conflicts where, e.g., one rule suggests "the dog is currently in Canada" and the other suggests "the dog is currently in Australia", 3- there are conflicts and preferences over facts as well, e.g., Fact1: Fiona has travelled to every country in Europe, Fact2: Fiona has not travelled to the Scandinavian countries, Fact2 is preferred over Fact1.

The current work assumes the initial state (facts) and the rules of the game are small enough to be included in the prompt. It is also limited to deductive reasoning with the _modus ponens_ rule. Future work can extend BoardgameQA and our analyses to the cases where not all the facts and rules can be included in the prompt due to the limitation in the prompt length, as well as the case with other types of rules such as proof by contradiction, disjunction elimination, etc (see ).

In this work, we only studied one simple but highly practical solution to conflict resolution (i.e. based on preferences). Future work can extend BoardgameQA and the analysis in this paper to other natural types of conflict resolution. Note that in some applications, preferences for conflict resolution have to be assigned with great care and diligence to avoid unfair treatment of information sources.

## 7 Conclusion

In this work, we introduced BoardgameQA, a dataset for measuring the natural language reasoning ability of language models (LMs) in the presence of conflicting input sources. Our dataset furthermore includes scenarios in which the knowledge required for reasoning is only partially provided as input and additional information needs to come from the model itself. We tested several types of LMs on different variations of the dataset and observed that LMs perform poorly when reasoning with conflicting inputs. In the case of smaller models, the performance was also poor when additional knowledge from the LM is needed. Since reasoning over contradicting and incomplete sets of information is a common scenario in real-world applications, our results highlight an important gap in the reasoning capacity of current LMs. We hope our dataset can guide future work developing methodology to improve the reasoning ability of LMs under this setup, or finding alternative formulations of conflict resolution that better facilitate LM reasoning.