# Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes

Yixuan Zhang

China-Austria Belt and Road Joint Laboratory on

Artificial Intelligence and Advanced Manufacturing

Hangzhou Dianzi University

yixuan.zhang@hdu.edu.cn

&Quyu Kong

Alibaba Group

kongquyu.kqy@alibaba-inc.com

&Feng Zhou

Center for Applied Statistics and School of Statistics

Renmin University of China

feng.zhou@ruc.edu.cn

Corresponding author.

###### Abstract

In this study, we propose a novel deep spatio-temporal point process model, Deep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal covariate information. DKMPP is an enhanced version of Deep Mixture Point Processes (DMPP), which uses a more flexible deep kernel to model complex relationships between events and covariate data, improving the model's expressiveness. To address the intractable training procedure of DKMPP due to the non-integrable deep kernel, we utilize an integration-free method based on score matching, and further improve efficiency by adopting a scalable denoising score matching method. Our experiments demonstrate that DKMPP and its corresponding score-based estimators outperform baseline models, showcasing the advantages of incorporating covariate information, utilizing a deep kernel, and employing score-based estimators.

## 1 Introduction

Point processes are widely used statistical tools for modeling event occurrence patterns within continuous spatio-temporal domains. They have garnered extensive attention across various fields, such as criminology [17; 38], neuroscience [24; 40], financial engineering [11; 2], and epidemiology [20; 25]. The core task of point processes is to derive an intensity function from event sequences, which represents the rate of event occurrences at any given time or location within the observation domain.

There are two common approaches for modeling intensity functions: traditional and covariate-based methods. The traditional approach [13; 10; 18] considers only event information, such as time and position, while ignoring the effects of contextual factors. Conversely, the covariate-based method [31; 33; 3] incorporates covariates from the observation domain, providing insights into factors that may cause event occurrences. In real applications, event occurrence rates are often correlated with contextual factors, making covariate-based methods more effective for event prediction . For example, in crime data analysis, the covariate-based method considers factors like income level, education status, and public security to estimate crime intensity. Thus, utilizing a covariate-based point process model is significant for leveraging rich contextual information to predict events and explore various factors influencing event occurrence.

Okawa et al.  proposed Deep Mixture Point Processes (DMPP) to incorporate covariate information into point process models. DMPP models intensity as a deep mixture of parametric kernels, representing the influence of nearby representative points on the target event. Mixture weights are modeled by a neural network using covariates from representative points as input, avoiding intractable intensity integrals for log-likelihood evaluation and enabling simple parameter estimation. However, DMPP requires integrable parametric kernels like radial basis function (RBF) or polynomial kernels, which have closed-form solutions for intensity integrals. With more complex, flexible kernels (e.g., deep kernels ), DMPP cannot compute the intensity integral tractably. In real-world applications, flexible kernels are usually preferred to capture complex influences of covariates on intensity, which can be challenging for restricted parametric kernels.

In this study, we aim to learn the intricate influence of covariate data on event occurrence using a data-driven approach. We propose replacing the simple parametric kernel in DMPP with a deep kernel , which transforms the inputs of a base kernel using a deep neural network. The deep kernel significantly enhances expressive power by automatically learning a flexible input transformation metric, thus transcending Euclidean and absolute distance-based metrics. This results in a substantial improvement in DMPP's expressiveness, leading to our proposed model, Deep Kernel Mixture Point Processes (DKMPP). However, due to the deep architecture embedded in the base kernel, the deep kernel is non-integrable, causing an intractable training procedure as intensity integral evaluation is required for likelihood-based parameter estimation. To address parameter estimation in DKMPP, we adopt an integration-free method based on score matching , which estimates model parameters by matching the gradient of the log-density of the model and data. Score matching avoids intensity integral computation and allows tractable training. However, the naive score matching estimator is computationally expensive as it requires the computation of second derivatives of the log-density. To improve efficiency, we further adopt a scalable denoising score matching (DSM) method , with the resulting models referred to as score-DKMPP and score-DKMPP+ respectively.

Score-based estimators, such as Score-DKMPP and Score-DKMPP+, offer an advantage over traditional likelihood-based estimators as they do not require numerical integration, which is often computationally expensive and prone to errors. Additionally, Score-DKMPP+ has a high efficiency due to the utilization of denoising score matching. Our experiments serve to highlight the advantages of DKMPP and its corresponding score-based estimators. In the majority of our experiments, both Score-DKMPP and Score-DKMPP+ outperform the baseline models, indicating that incorporating covariate information can enhance the modeling and prediction of spatio-temporal events. Furthermore, our results strongly suggest that the adoption of a deep kernel significantly enhances the model's expressiveness when compared to a simple parametric kernel.

Specifically, our contributions are as follows: (1) We propose DKMPP, which replaces the simple parametric kernel in DMPP with a more flexible deep kernel, enhancing model expressiveness and allowing for the learning of complex influence from covariate factors on event occurrence. (2) We use the score matching estimator to overcome the intractable intensity integral in DKMPP during training and extend the denoising score matching method to DKMPP, significantly improving computational efficiency. (3) We analyze the difference between likelihood estimator and score matching estimators for DKMPP, demonstrate the superiority of DKMPP over baseline models, and show the impact of deep kernel and various hyperparameters through experiments.

## 2 Related Work

**Point processes** are statistical tools for modeling event occurrence patterns, with intensity functions characterizing event rates. Essential models include Poisson processes , renewal processes , and Hawkes processes . Poisson processes feature independent point occurrences and have been widely used in economics , ecology , and astronomy . Renewal processes generalize Poisson processes for arbitrary event intervals. However, neither model is suitable for self-excitation in event patterns. Hawkes processes, containing a triggering kernel, are suitable for applications with excitation effects, such as seismology , finance , criminology , and neuroscience .

**Deep point processes** have gained traction due to the growing popularity of deep neural networks. While traditional models capture simple event patterns, deep point processes leverage neural networks to describe complex dependencies. The first deep point process work by Du et al.  utilized RNNs to encode history information. Later, Xiao et al.  and Mei & Eisner  proposed LSTM-basedversions. However, RNNs struggle with long-term dependencies, training difficulties, and lack of parallelism. To address these issues, Zuo et al.  and Zhang et al.  proposed attention-based models. Additionally, there are some point process models directly relevant to our work, such as Okawa et al.  and Zhu et al. , which employ the use of deep kernels to model the intensity function. Besides directly modeling intensity functions with neural networks, other approaches use them to represent probability density functions  or cumulative intensity functions [23; 30]. However, to the best of our knowledge, most deep point process models are limited to temporal point processes, and few works have extended them to spatio-temporal point processes.

**Covariate point processes** differ from traditional point processes by incorporating contextual factors and establishing connections between these factors and event patterns. This integration of covariate information enables a more comprehensive explanation of studied phenomena and improves predictive performance. Meyer et al.  proposed a model for epidemic prediction using population density and other covariates. Adelfio & Chiodi  proposed a spatio-temporal self-exciting point process for earthquake forecasting using geological features. Gajardo & Muller  analyzed COVID-19 cases with point processes, studying their relation to covariates such as mobility restrictions and demographic factors. However, these works model the influence of covariate in a handcrafted parametric form, limiting flexibility.

Different from existing works, our approach emphasizes covariate point processes and enhances both model flexibility and parameter estimation. While our work is closely related to , we extend it by introducing the deep kernel and score-based estimation methods in our proposed DKMPP model.

## 3 Preliminaries

We offer a brief overview of three fundamental components that serve as the foundation for our work.

### Spatio-temporal Point Processes

A spatio-temporal point process is a stochastic model whose realization is a random collection of points representing the time and location of events . Consider a sequence of \(N\) events \(S=\{(t_{1},_{1}),(t_{2},_{2}),,(t_{N},_{N })\}\), where \((t_{n},_{n})\) denotes the time and location of the \(n\)-th event in \(^{2}\). The intensity function \((t,)\) represents the instantaneous event occurrence rate:

\[(t,)=_{|_{t}|,|_{}| 0}[(_{t}_{})]}{|_{t}|| _{}|},\] (1)

where \((_{t}_{})\) is the number of events in \(_{t}_{}\), \(_{t}\) is a small time interval around \(t\), \(_{}\) is a small spatial region around \(\), and \(\) denotes expectation w.r.t. realizations. Given a sequence \(S\), the probability density function, or likelihood, of the point process can be written as :

\[p(S(t,))=_{n=1}^{N}(t_{n},_{n}) (-_{}(t,)dt).\] (2)

### Deep Kernel

Wilson et al.  combined kernel methods and neural networks to create a deep kernel, capturing the expressive power of deep neural networks. The deep kernel extends traditional covariance kernels by embedding a deep architecture into the base kernel:

\[k_{}(,^{}) k_{}(g_{w}(),g_{w}( ^{})),\] (3)

where \(k_{}\) is a base kernel with parameters \(\). To enhance flexibility, the inputs \(x\) and \(x^{}\) are transformed by a deep neural network \(g\) parameterized by weights \(w\). The base kernel offers various options, such as the common RBF kernel. The deep kernel parameters include base kernel parameters \(\) and neural network weights \(w\). An advantage of deep kernels is their ability to learn metrics by optimizing input space transformation in a data-driven manner, rather than relying on Euclidean distance-based metrics, which are common in traditional kernels but may not always be suitable, especially in high-dimensional input spaces.

### Score Matching

The classic maximum likelihood estimation (MLE) minimizes the Kullback-Leibler (KL) divergence between the parameterized model distribution and the data distribution, but requires an often intractable normalizing constant. Numerical integration can approximate the intractable integral, but its complexity grows exponentially with the input dimension. An alternative is to minimize the Fisher divergence, which does not require the normalizing constant. The Fisher divergence is defined as:

\[F(p(x),q(x))=_{p(x)}\| p(x)- q(x)\|^{2},\] (4)

where \(p(x)\) and \(q(x)\) are two smooth distributions of the same dimension, and \(\|\|\) is a suitable norm (e.g., \(^{2}\) norm). Minimizing the Fisher divergence estimates model parameters by matching the gradient of the log-density of the model to the log-density of the data. The parameterized model density is defined as \(p_{}(x)=(x)\), where \(\) represents the model parameters, and the normalizing constant \(Z()=(x)dx\) is intractable. With Fisher divergence, we do not need to compute the normalizing constant, as the matched gradients do not depend on it. The gradient of log-density is referred to score, so the aforementioned method is also called score matching .

## 4 Methodology

In this section, we introduce our proposed DKMPP and two score-based parameter estimation methods. Following Okawa et al. , we use the same notation. We assume a sequence of \(N\) events \(S=\{_{1}=(t_{1},_{1}),_{2}=(t_{2},_{2 }),,_{N}=(t_{N},_{N})\}\), where \(_{n}=(t_{n},_{n})\) represents the time and location of the \(n\)-th event in \(^{2}\). We also assume \(K\) contextual covariates on the spatio-temporal region \(\): \(=\{Z_{1},Z_{2},,Z_{K}\}\), where \(Z_{k}\) is the \(k\)-th covariate.

### Deep Kernel Mixture Point Processes

Okawa et al.  developed a deep spatio-temporal covariate point process model called DMPP, capable of incorporating multimodal contextual data like temperature, humidity, text and image. The intensity of DMPP is designed in a kernel convolution form:

\[()= f_{w}(,())k_{}(,)d,\] (5)

where \(=(,)\) is a point in the region \(\), and \(k_{}(,)\) is a kernel with parameters \(\). \(f_{w}\), a deep neural network, inputs contextual data and outputs nonnegative mixture weights, with \(w\) representing the network parameters. \(()=\{Z_{1}(),,Z_{K}()\}\) are covariate values at point \(\).

Although the intensity formulation integrates multimodal contextual data, in reality, covariate data is only available on a regular grid. DMPP introduces finite representative points \(\{_{j}\}_{j=1}^{J}\) on the spatio-temporal domain, resulting in a discrete version2:

\[()=_{j=1}^{J}f_{w}(_{j}, (_{j}))k_{}(,_{j}).\] (6)

The discrete version of DMPP offers two significant advantages: (1) the intensity itself is tractable since it requires summation instead of integration; (2) the intensity integral is tractable as long as the kernel integral is tractable, as given by \(()d=_{j=1}^{J}f_{w}( _{j},(_{j})) k_{}(, _{j})d\). For this reason, DMPP employs simple integrable kernels like RBF kernels, which can be integrated analytically, to simplify parameter estimation. This design choice ensures that the intensity integral is tractable, which is crucial for evaluating log-likelihood. Consequently, parameter estimation can be performed using straightforward back-propagation.

However, this approach has notable drawbacks. Firstly, using limited parametric kernels constrains the intensity's expressive power. Secondly, Euclidean distance in the kernel may not be a suitable measure of similarity, particularly in high-dimensional input spaces. In many applications, therelationship between covariates and event occurrence is complex and often unknown, making it desirable to model this influence using data-driven kernels with non-Euclidean distance metrics.

To address the issues mentioned earlier, we propose DKMPP, which replaces the simple integrable kernel in DMPP with the deep kernel . The deep kernel captures the similarity between representative points and event points in both time and location. The deep kernel has greater expressiveness than the traditional kernel and can flexibly learn the metric's functional form through a neural network-based nonlinear transformation. Specifically, the intensity of DKMPP is designed as:

\[()=(_{j=1}^{J}f_{w_{1}}(_{j},(_{j}))k_{,w_{2}}(,_{j}) ),\] (7)

where \(k_{,w_{2}}(,^{})=k_{}(g_{w_{2}}(), g_{w_{2}}(^{}))\) is the deep kernel, with \(\) as the base kernel parameters and \(w_{2}\) as the weights of a nonlinear transformation \(g\) implemented by a deep neural network. In subsequent experiments, we found that, even for this three-dimensional problem, the deep kernel's ability to learn metrics from data outperforms the traditional Euclidean distance. Compared to DMPP, DKMPP introduces a link function \(()\) to ensure non-negativity of the intensity function3. Various functions, such as softplus, exponential, and ReLU, can be utilized as the link function. Unless otherwise specified, we choose softplus as \(()\) in this work.

Representative PointsIn this study, we adopt a fixed grid of representative points, which are evenly spaced both along the temporal axis and across the spatial region. The complete set of representative points is derived as the Cartesian product of the temporal and spatial representatives.

Implementation DescriptionBoth the kernel mixture weight network \(f\) and the non-linear transformation \(g\) in the deep kernel are implemented using MLPs with ReLU activation functions. To handle multimodal covariates such as numerical values, categorical values, and text, etc., we perform necessary feature extraction. If the covariates are numerical, we normalize them as necessary. If the covariates are categorical, we convert them to numerical type or one-hot encoding. If the covariates are textual, we use a Transformer architecture to extract features. If the covariates are multimodal, we fuse the multimodal information by concatenating the extracted features from multiple modules.

### Parameter Estimation with Score Matching

The traditional approach to estimate point process is based on MLE where the log-likelihood is:

\[ p_{}(S)= p(\{_{n}\}_{n=1}^{N}_{}( ))=_{n=1}^{N}_{}(_{n})-_{}_{}()d.\] (8)

The intensity integral is a compensator which can be understood as a normalizing constant. The integration is intractable in most cases, and our proposed DKMPP is no exception. To solve this problem, we normally resort to numerical integration, e.g., Monte Carlo or quadrature, to obtain an approximation. However, these numerical methods introduce additional errors; more importantly, they are not scalable to the high-dimensional problem.

Score-DKMPPIn this work, we propose an integration-free estimation method based on score matching  to estimate the model parameters in our proposed DKMPP. Surprisingly, the score-based estimator for point process model is largely unexplored in recent years. As far as we know, few works attempted to utilize score matching for the estimation of point processes. Sahani et al.  proposed a pioneering estimator in this field, linking score matching with temporal point processes. Here, we extend the score matching estimator to our covariate-based spatio-temporal DKMPP. We assume the ground-truth process generating the data has a density \(p(S)\) and design a parameterized model with density \(p_{}(S)\) where \(\) is the model parameter to estimate. A Fisher-divergence objective for our covariate-based spatio-temporal DKMPP is designed as:

\[F()=_{p(S)}_{n=1}^{}(}-(S)}{ s _{n}})^{2},\] (9)where \(s_{n}\) is any entry in vector \(_{n}\), i.e., \(s_{n}_{n}=(t_{n},_{n})\), \(\) is the number of equivalent variables, \(=N\) for 1-D point process, e.g., temporal point process; \(=2N\) for 2-D point process, e.g., spatial point process; and \(=3N\) for 3-D point process, e.g., spatio-temporal point process, etc. The optimal estimate of model parameter is given by \(=_{}F()\).

However, the above loss cannot be minimized directly as it depends on the gradient of the ground-truth data distribution which is unknown. Following the derivation in Hyvarinen , this dependence can be eliminated by using a trick of integration by parts. Sahani et al.  assumed the log density of point process satisfies the proper smoothness and obtained a concise empirical loss. In this work, we prove that without those smoothness assumptions, we can still obtain the same empirical loss (proof provided in Appendix A):

\[()=_{m=1}^{M}_{n=1}^{_{m}}((S_{m})}{ s_{m,n}})^{2}+  p_{}(S_{m})}{ s_{m,n}^{2}}+C_{1},\] (10)

where we take \(M\) sequences \(\{S_{m}\}_{m=1}^{M}\) from \(p(S)\), \(_{m}\) is the number of equivalent variables on the \(m\)-th sequence, \(s_{m,n}\) is the \(n\)-th variable on the \(m\)-th sequence, the constant \(C_{1}\) does not depend on \(\) and can be discarded. It is easy to see that, in Eq. (10), the intensity integral has been removed by the operation of gradient. We refer to our proposed DKMPP estimated by Eq. (10) as score-DKMPP.

**Score-DKMPP+** A serious drawback of the objective in Eq. (10) is it requires the second derivative which is computationally expensive. To improve efficiency, we derive the denoising score matching (DSM) method  for our proposed DKMPP to avoid second derivatives. We add a small noise to the sequence \(S\) to obtain a noisy sequence \(\) (we add noise to each variable \(_{n}=s_{n}+\)), which is distributed as \(p()= p( S)p(S)dS\). The DSM method uses the Fisher divergence between the noisy data distribution \(p()\) and model distribution \(p_{}()\) as an objective:

\[F_{}()=_{p()}_{n=1}^{ }(_{m})}{_{n}}- ()}{_{n}})^{2},\] (11)

where \(_{n}\) is any entry in the noisy vector \(}_{n}\), i.e., \(_{n}}_{n}=(_{n},}_{n})\). The optimal parameter estimate is given by \(=_{}F_{}()\). To facilitate computation, the Fisher divergence in Eq. (11) can be further rewritten as (proof provided in Appendix B):

\[_{}()=_{m=1}^{M}_{n=1}^{_ {m}}(_{m} S_{m})}{_{ m,n}}-(_{m})}{_{m,n}} )^{2}+C_{2},\] (12)

where we take \(M\) clean and noisy sequences \(\{S_{m},_{m}\}_{m=1}^{M}\) from \(p(S,)\), \(_{m,n}\) is the \(n\)-th variable on the \(m\)-th noisy sequence, the constant \(C_{2}\) does not depend on \(\) and can be discarded. With a Gaussian noise \((0,^{2})\), the conditional gradient term can be written in a closed form: \( p(_{m} S_{m})/_{m,n}=-(_{ m,n}-s_{m,n})/^{2}\). Equation (12) avoids the unknown ground-truth data distribution, the intractable intensity integral and the tedious second derivatives.

It is worth noting that the denoising score matching is not equivalent to the original score matching, because it is easy to see from Eq. (11) that the model is trained to match the score of the noisy data distribution \(p()\) instead of the original \(p(S)\). However, when the noise is small enough, we can consider \(p() p(S)\) such that Eq. (12) is approximately equivalent to Eq. (10). We refer to our proposed DKMPP estimated by Eq. (12) as score-DKMPP+.

## 5 Experiments

In the experimental section, we mainly analyze the difference between MLE and score matching for DKMPP, the improvement in performance of DKMPP over baseline models, as well as the impact of various hyperparameters.

### DKMPP: MLE vs. Score Matching

Since our proposed DKMPP model employs a score-based parameter estimation method instead of traditional MLE, a natural question arises regarding the comparison between the two approaches. Toevaluate this question, we generate a 3-D spatio-temporal point process synthetic dataset. The spatial observation \(\) spans the area of \(\), while the temporal observation window covers the time interval of \(\). We design a 1-D covariate function on the domain, a kernel mixture weight function \(f_{w_{i}}(_{j},Z(_{j}))\), and a deep kernel \(k_{,w_{2}}(,_{j})\) with the RBF base kernel. More details are provided in Appendix C.1. We fix the representative points on a regular grid: \(5\) representative points evenly spaced on each axis, so there are \(5^{3}=125\) representative points in total. We use the thinning algorithm  to generate 5,000 sequences according to the ground truth specified above. The statistics of the synthetic data are provided in Appendix C.1. We try to fit a DKMPP model to the synthetic data with the ground-truth representative points and an RBF base kernel. Both the kernel mixture weight network \(f\) and the non-linear transformation \(g\) in the deep kernel are implemented using MLPs with ReLU activation functions. Therefore, the learnable parameters are \(w_{1},w_{2},\).

Two aspects of particular interest when comparing MLE and score matching estimators are consistency and computational cost. Consistency refers to the property that as the data size varies, the estimates produced by these methods converge to the true parameter values. The computational costs of these methods refer to the amount of time required to achieve a certain level of accuracy.

For consistency, we use the root mean square error (RMSE) between the estimated intensity and the ground-truth intensity as a metric. During the training of MLE, numerical integration is required to approximate the intractable intensity integral. In our experiments, we use Monte Carlo methods and fix the number of Monte Carlo samples to 1,000, which is sufficient to obtain good results. For Score-DKMPP+, we use a Gaussian noise \((0,^{2})\) with \(^{2}=0.01\). Refer to Appendix C.1 for the intensity functions learned by different estimators. We test the RMSE of MLE-DKMPP, Score-DKMPP and Score-DKMPP+ under different data sizes (\(100\%\) and \(10\%\)), as shown in Fig. 0(a). Three estimators exhibit very similar RMSE performance; however, as the data size decreases, the errors of all three estimators increase slightly. Besides, we examine the effect of Monte Carlo samples on RMSE. Specifically, we use \(100\%\) data and increase the number of Monte Carlo samples from 10 to 150. The results are shown in Fig. 0(b). Due to the reliance on Monte Carlo integration, MLE suffers from poor RMSE when the number of Monte Carlo samples is insufficient. As the number of Monte Carlo samples increases, the parameter estimation performance of MLE improves and eventually converges. In comparison, score matching estimators (Score-DKMPP and Score-DKMPP+) have an advantage because they only require event location and do not rely on Monte Carlo integration. Therefore, the parameter estimation performance of score matching estimators is independent of the number of Monte Carlo samples.

For computational efficiency, we record the running time required by three estimators to achieve the same level of accuracy. The results are shown in Fig. 0(b), indicating that to reach the same RMSE, the Score-DKMPP requires 356.8 seconds, while the MLE is faster, taking 150.8 seconds. Furthermore, the Score-DKMPP+ is extremely fast, completing the task in only 29.4 seconds.

### Real-world Data

In this section, we validate our proposed DKMPP model and its corresponding score-based estimators on real-world data from various domains. We also compare DKMPP against other popular spatial-temporal point process models.

Figure 1: (a) The RMSE performance of MLE-DKMPP, Score-DKMPP and Score-DKMPP+ with \(100\%\) and \(10\%\) training data (the number of Monte Carlo samples is fixed to 1,000); (b) the RMSE performance of three estimators with the number of Monte Carlo samples ranging from 10 to 150; (c) the learned intensity function at \(t=10\) from Score-DKMPP+ with the noise variance \(^{2}=0.01\); (d) the learned intensity function at \(t=10\) from Score-DKMPP+ with \(^{2}=10\).

DatasetsWe analyze three datasets from the fields of transportation and crime, with details of the datasets shown below. Each dataset is divided into training, validation and test data using a \(50\%/40\%/10\%\) split ratio based on time. The data preprocessing is provided in Appendix C.2.

_Crimes in Vancouver4_ This dataset is composed of more than 530 thousand crime records, including all categories of crimes committed in Vancouver. Each crime record contains the time and location (latitude and longitude) of the crime.

_NYC Vehicle Collisions5_ The New York City vehicle collision dataset contains about 1.05 million vehicle collision records. Each collision record includes the time and location (latitude and longitude).

_NYC Complaint Data6_ This dataset contains over 228 thousand complaint records in New York City. Each record includes the date, time, and location (latitude and longitude) of the complaint.

_Covariates_ These three datasets contain not only the spatio-temporal information of the events but also multimodal covariate information. For example, the Crime in Vancouver dataset includes information on the cause and type of crime records; the NYC Vehicle Collisions dataset includes textual descriptions of the accident scene, such as borough and contributing factors; the NYC Complaint Data includes numerical and textual covariate information, such as the police department number and offense description. For every representative point, we identify the closest numerical/categorical/textual features in both time and space, which are used as covariates.

BaselinesDeep temporal point process models have gained widespread use in recent years, e.g., Mei & Eisner ; Omi et al. ; Zhang et al. ; Zuo et al. . These works primarily focus on modeling time information while neglecting spatial information. As far as we know, there has been limited research on deep learning approaches for spatial-temporal point process modeling. Here, for a fair comparison, we compare DKMPP against deep (covariate) spatial-temporal point process models. Specifically, the following baselines are considered: (1) the homogeneous Poisson process (HomoPoisson) ; (2) the neural spatio-temporal point processes (NSTPP) ; (3) the deep spatio-temporal point processes (DeepSTPP) ; (4) the DMPP .

HomoPoisson is a traditional statistical spatial-temporal point process model; NSTPP and DeepSTPP are deep spatial-temporal point process models that do not incorporate covariates, while DMPP and DKMPP are deep covariate spatial-temporal point process models that allow the inclusion of covariate information. For DMPP and DKMPP, we experiment with three different kernels: RBF kernel, rational quadratic (RQ) kernel and Ornstein-Uhlenbeck (OU) kernel (See Appendix C.2).

MetricsWe use two metrics to evaluate the performance: test log-likelihood (TLL) and prediction accuracy (ACC). The TLL measures the log-likelihood on the test data, indicating how well the model captures the distribution of the data. The ACC evaluates the absolute difference between the predicted number of events and the actual number of events on the test data, \(1-|\#-\#|/\#\), assessing how accurately the model predicts the number of events on the test data.

ResultsWe evaluate the performance of all baseline models in terms of TLL and ACC. In our experiments, we use Monte Carlo methods for the baselines which need numerical integration and fix the number of Monte Carlo samples to 1,000 which is sufficient to obtain good results. For Score-DKMPP+, we use a Gaussian noise \((0,^{2})\) with \(^{2}=0.01\). More training details are provided in Appendix C.2. The performance results are presented in Table 1, which show that Score-DKMPP and Score-DKMPP+ achieve similar performance with the same kernel, and both outperform other baseline models. As expected, HomoPoisson performs the worst due to its lack of flexibility in modeling varying intensity functions over time and space. In comparison to NSTPP and DeepSTPP, DKMPP performs better in two aspects. Firstly, DKMPP incorporates rich multimodal covariate information, which aids in modeling event occurrences. Secondly, DKMPP employs a score-based estimator instead of the MLE estimator, which avoids unnecessary errors caused by numerical integration. The comparison between DKMPP and DMPP with the same kernel constitutes an ablation study, which again verifies that using a deep kernel can provide stronger representational power and achieve better performance in both data fitting and prediction. This is because the deep kernel can flexibly learn the functional form of the metric by optimizing input space transformation in a data-driven manner, rather than relying on a fixed Euclidean distance-based metric.

### Hyperparameter Analysis

Representative PointsThe effect of the number of representative points on the performance of DKMPP on synthetic and real-world data is presented in Table 2 and Appendix C.2, respectively. In general, the accuracy of DKMPP tends to improve with an increase in the number of representative points, although the change in performance is not significant. However, having more representative points leads to higher computational costs. Therefore, in the experiment, the number of representative points needs to be chosen as a trade-off between accuracy and efficiency.

Layer and Batch SizeWe examine how the number of layers in the kernel mixture weight network \(f\) and the non-linear transformation \(g\) of the deep kernel, as well as batch size, affect model performance on synthetic and real data in Table 2 and Appendix C.2, respectively. In general, varying batch sizes do not produce a significant difference in model performance, so we set a batch size of 100 because both overly large and excessively small batch sizes may harm optimization. For the number of layers, we find that the number of layers is correlated with the dimension of covariates. In the case of low-dimensional covariates, a simple hidden layer can provide better performance than multiple layers. Thus, we set the number of layers to 1 for the synthetic data and 2 for the real data.

Noise VarianceThe noise added by the Score-DKMPP+ is an important hyperparameter. If the noise is too large, the resulting noisy distribution \(p()\) will deviate significantly from the true data distribution \(p()\), leading to severe biases in the learned results. We conduct experiments with two different noise variance settings, \(^{2}=0.01\) and \(^{2}=10\). When \(^{2}=0.01\), the model can learn the intensity function that is close to the ground truth, as shown in Fig. 1c. However, as the noise level increases, when \(^{2}=10\), the learned results deviate significantly from the true values, as shown in Fig. 1d. In practice, setting \(^{2}\) too small can also lead to numerical instability.

    &  &  &  \\   & TLL & ACC (\%) & TLL & ACC (\%) & TLL & ACC (\%) \\  HomoPoisson & 60.468 & 52.516 & 394.890 & 66.811 & 1.660 & 26.834 \\ NSTPP & 65.45\(\)4.46 & 74.22\(\)1.36 & 396.33\(\)19.66 & 72.08\(\)3.53 & 3.07\(\)0.38 & 32.83\(\)1.41 \\ DeepSTPP & 62.15\(\)6.16 & 69.89\(\)4.26 & 396.60\(\)11.28 & 69.35\(\)2.07 & 3.97\(\)1.38 & 43.72\(\)0.96 \\ DMPP (RBF) & 62.58\(\)1.02 & 56.90\(\)0.98 & 394.54\(\)2.17 & 67.91\(\)1.55 & 2.06\(\)0.56 & 30.97\(\)1.23 \\ DMPP (RQ) & 60.09\(\)0.97 & 51.52\(\)1.01 & 399.71\(\)1.99 & 72.32\(\)1.23 & 2.11\(\)0.32 & 31.92\(\)0.68 \\ DMPP (OU) & 60.85\(\)1.13 & 52.59\(\)0.96 & 402.54\(\)2.49 & 74.27\(\)0.13 & 2.20\(\)0.21 & 29.95\(\)0.57 \\  Score-DKMPP (RBF) & 69.06\(\)0.68 & 78.71\(\)0.37 & 409.02\(\)0.92 & 79.67\(\)0.65 & 2.96\(\)0.11 & 43.40\(\)0.78 \\ Score-DKMPP (RQ) & **69.55\(\)0.39** & 80.16\(\)1.01 & **41.03\(\)1.12** & 79.42\(\)0.79 & 3.04\(\)0.23 & 44.57\(\)0.26 \\ Score-DKMPP (OU) & 69.51\(\)0.49 & 78.41\(\)0.83 & 407.43\(\)1.24 & 78.84\(\)0.87 & 3.06\(\)0.16 & 44.62\(\)1.39 \\ Score-DKMPP+ (RBF) & 67.03\(\)0.23 & **80.20\(\)0.34** & 402.54\(\)1.06 & 79.13\(\)0.67 & 3.13\(\)0.28 & 46.48\(\)0.43 \\ Score-DKMPP+ (RQ) & 69.52\(\)1.14 & 80.09\(\)0.90 & 403.93\(\)1.43 & 79.85\(\)1.89 & 3.74\(\)0.35 & 47.34\(\)1.44 \\ Score-DKMPP+ (OU) & 68.78\(\)0.90 & 80.03\(\)0.69 & 400.32\(\)1.16 & **79.86\(\)1.08** & **4.28\(\)0.11** & **47.36\(\)0.79** \\   

Table 1: The performance of TLL and ACC (mean\(\)std) for DKMPP and baseline models on three real-world datasets. For DMPP and DKMPP, we experiment with RBF kernel, RQ kernel and OU kernel. Both for TLL and ACC, higher values indicate better performance.

    &  &  &  \\   & 64 & 125 & 216 & 1 & 2 & 4 & 50 & 100 & 200 \\  Score-DKMPP & 3.86\(\)0.26 & **3.66\(\)0.27** & **3.66\(\)0.26** & **3.84\(\)0.23** & 4.00\(\)0.21 & 4.13\(\)0.16 & 3.60\(\)0.35 & **3.47\(\)0.23** & 4.07\(\)0.17 \\ Score-DKMPP+ & 3.78\(\)0.23 & 3.73\(\)0.21 & **3.72\(\)0.22** & **3.78\(\)0.17** & 3.95\(\)0.23 & 4.08\(\)0.26 & 3.67\(\)0.31 & **3.61\(\)0.21** & 3.65\(\)0.15 \\   

Table 2: The RMSE performance (mean\(\)std) of Score-DKMPP and Score-DKMPP+ on the synthetic dataset with various combinations of hyperparameters: for the representative points, we used \(1\)-layer MLPs and a batch size of 100; for the number of layers, we used \(125\) representative points and a batch size of 100; and for the batch size, we used \(125\) representative points and \(1\)-layer MLPs.

## 6 Limitations

One limitation of the current study is that the proposed multimodal covariate spatio-temporal point process model is constrained to covariates of numerical, categorical, and textual types during the experiments, primarily due to the availability of data. However, future research could greatly benefit from the incorporation of image-type covariates, as this holds immense potential for enhancing the model's ability to capture a wider range of diverse and intricate effects. By including image data, the model could potentially uncover richer patterns and relationships, leading to even more fascinating and insightful results.

## 7 Conclusions

In conclusion, we have introduced a novel deep spatio-temporal point process model, the DKMPP, which incorporates multimodal covariate information and captures complex relationships between events and covariate data by utilizing a more flexible deep kernel leading to improved expressiveness. We address the intractable training procedure of DKMPP by using an integration-free method based on score matching and improving efficiency through a scalable denoising score matching method. Through our experiments, we show that DKMPP and its corresponding score-based estimators outperform baseline models, demonstrating the advantages of incorporating covariate information, utilizing a deep kernel, and employing score-based estimators. Our results suggest that DKMPP is a promising approach for modeling complex spatio-temporal events with multimodal covariate data.