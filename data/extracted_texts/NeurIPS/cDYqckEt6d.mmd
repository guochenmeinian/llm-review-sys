# DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents

DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents

 Peter Jansen\({}^{*}\), Marc-Alexandre Cote\({}^{}\), Tushar Khot\({}^{*}\) Erin Bransom\({}^{*}\), Bhavana Dalvi Mishra\({}^{*}\), Bodhisattwa Prasad Majumder\({}^{*}\), Oyvind Tafjord\({}^{*}\), Peter Clark\({}^{*}\)

\({}^{*}\)Allen Institute for Artificial Intelligence \({}^{}\)Microsoft Research \({}^{}\)University of Arizona

peterj@allenai.org

###### Abstract

Automated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DiscoveryWorld, the first virtual environment for developing and benchmarking an agent's ability to perform complete cycles of novel scientific discovery. DiscoveryWorld contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of _general_ discovery skills rather than task-specific solutions. DiscoveryWorld itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. DiscoveryWorld further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most DiscoveryWorld tasks, suggesting that DiscoveryWorld captures some of the novel challenges of discovery, and thus that DiscoveryWorld may help accelerate near-term development and assessment of scientific discovery competency in agents. Code available at github.com/allenai/discoveryworld.1

## 1 Introduction

A long-standing dream of AI has been to build systems that can perform scientific discovery, potentially leading to new breakthroughs for the benefit of humanity . Recently, with the rise of neural techniques, there have been several successful discovery systems developed for specialized problems such as protein folding , mathematics , and material science . However, while the results have been impressive, these systems (deliberately) bypass the full discovery process of ideation, hypothesis formation, experiment design, etc., and instead (expertly) perform systematic searches over a pre-defined hypothesis space, with pre-defined goals. This raises the question: how much more can be achieved if AI is applied to the broader scientific process? Some works have indeed developed early systems for this, for example, in chemistry , and genetics . These systems can also generate hypotheses, design experiments, and execute them (via robotics) in real environments. However, operating in real environments is expensive and complex, creating a barrier for entry. In addition, real environments inevitably encourage a focus on task-specific details, at the potential cost of developing more general discovery skills in an agent.

Our goal is to help remedy these by creating the first virtual discovery environment where solving tasks _demands all of the key facets in end-to-end scientific discovery_,2 and which covers a _broad variety_ of discovery topics. Our approach is to develop a text-based simulated world (with optional 2D visual overlay), called DiscoveryWorld, where agents can navigate around, interact with objects in the world, use scientific equipment (measuring devices, tools, etc.), and make observations. Agents can then form hypotheses, plan and execute experiments, and draw conclusions to solve challenge tasks developed for this virtual world. DiscoveryWorld tasks are grounded in eight varied topics, such as radioisotope dating, rocket science, and proteomics, to encourage development of agents with _general_ discovery skills rather than hard-wiring to a particular challenge (see Figure 1). The tasks themselves are realistic (but simplified), allowing agents to apply both scientific and commonsense knowledge when attempting them. DiscoveryWorld thus provides an environment for exercising and evaluating general-purpose skills in end-to-end AI discovery systems (see Figure 2).

DiscoveryWorld is inspired by a growing number of text-based simulation environments [5, 31, inter alia], while also being novel in both its environment and tasks:

* DiscoveryWorld tasks are long-horizon, requiring multiple facets of discovery including ideation, experimentation, systematic search, and analysis to be performed to solve a task.
* The tasks do not suggest a solution approach, instead requiring the agent to ideate and define hypotheses to explore. This contrasts with tasks in many adventure game environments where solution approaches are often more stylistic or constrained.
* DiscoveryWorld is realistic (but simplified) rather than counterfactual, so that background knowledge can be sensibly applied.
* The tasks cover eight diverse topics, from identifying the cause of space illnesses to reactor tuning, to encourage development of general rather than task-specific solutions.

Finally, automatically evaluating an agent's progress on a discovery task is itself challenging. We devise a three-part evaluation strategy to help with this, more on this in Section 3.4.

Our contributions are thus:

Figure 1: DiscoveryWorld is a virtual environment for developing and evaluating discovery agents, with challenge tasks covering a broad variety of different topics such as those shown above.

* We introduce the first virtual environment for benchmarking an agent's general ability to perform complete cycles of novel scientific discovery.
* A comprehensive evaluation set of 120 different tasks for this environment, spanning eight diverse topics, each with three levels of difficulty and several parametric variations.
* An evaluation framework allowing automatic evaluation of agents in DiscoveryWorld.
* Baseline results for agents in this environment, illustrating that DiscoveryWorld captures several novel challenges that contemporary agent models struggle with.

Together, we hope DiscoveryWorld will inspire and accelerate the development of new, general AI discovery agents by the community.

## 2 Related Work

Recently, several real-world discovery systems have shown success in areas such as genetics (Adam , Eve ), chemistry (CoScientist , ChemCrow ) and protonomics (AlphaFold , RoseTTAFold ). However, these systems are expensive and task-specific. Inspired by these, DiscoveryWorld aims to be a broad coverage, virtual environment allowing general scientific research skills to be developed and evaluated. There are many virtual environments that touch on aspects of the discovery process (see Table 1), although DiscoveryWorld is the first aimed at the full end-to-end pipeline:

Many **physical simulation environments** were developed (e.g., for robotics), focusing on object manipulation and navigation. Some are visual/spatial environments (e.g., AI2-Thor , ALFRED ), while others are textual/symbolic (e.g., TextWorld , MiniGrid , ALFWorld ).

Many **game worlds** require exploration and discovery, e.g., NetHack , MineDojo . However, these operate in counterfactual worlds where good scientific choices are not always rewarded, and are primarily aimed at entertainment.

Some virtual environments (abstractly) cover **real world** tasks. WebArena  simulates Web-based activities, e.g., browsing for a phone-number, on-line shopping, aiming to improve an agent's task-specific Web navigation skills. Perhaps closest to DiscoveryWorld is ScienceWorld , a text-based environment for solving simple science quests known to elementary science students, such as "convert a liquid into a solid" (e.g., put water in the freezer). ScienceWorld requires commonsense object manipulation at the level of an elementary science student, but not ideation or systematic search to complete tasks. In contrast, we show that DiscoveryWorld contains discovery tasks that are challenging even for human scientists with PhDs in the natural sciences.

Finally, some environments are explicitly designed to host **hypothesis search**. Alchemy  is a 3D video game to find which mixture of options transforms a stone into a more valuable form.

Figure 2: DiscoveryWorld tasks require end-to-end scientific discovery, from ideation, hypothesis formation, experiment design, data collection and analysis, forming conclusions, and acting on results. Distractors and task solutions that provide only descriptive discoveries require agents to frequently iterate hypotheses and experiments to reach full explanatory discoveries.

Similarly in IVRE , users perform artificial category experiments to identify which blocks are "blickets". These environments exercise the systematic search part of the discovery pipeline. Likewise, MLAgentBench  requires software experiments to solve a challenge (improve a ML algorithm), but in the constrained environment of ML software. In contrast, DiscoveryWorld aims to cover a broad range of tasks in a (simulated) physical environment, covering the full end-to-end discovery pipeline.

## 3 DiscoveryWorld Simulator and Environments

### Simulator

**Engine:** DiscoveryWorld is implemented in a custom simulation engine developed from the ground-up to enable building dynamic discovery simulations with a variety of object properties and simulated object behaviors. Every object in DiscoveryWorld is constructed from materials with measurable properties, many of which are observable with instruments available in the environment (a list of 60+ frequent object properties is provided in Appendix B.5). The simulator is implemented as approximately 20k lines of Python using the Pygame framework , and provides both an API for developing agents, as well as a graphical user interface for humans to play DiscoveryWorld tasks. The API resembles the OpenAI Gym specifications , such that at each step, the agent

    & **Multi-** & **\# of** & **Para-** & **\# Obj.** & **\# of** & **Task** \\
**Environment** & **modal** & **Domain** & **Tasks** & **metric** & **Props.** & **Actions** & **Length** \\  MiniGrid & Image/Symbol & Pick+Place & 23 & Yes & 4 & 4 & 85 \\ Alfred & Image & Pick+Place & 6 & Yes & 20 & 7 & 50 \\ AleWorld & Text/Image & Pick+Place & 6 & Yes & 16 & 9 & 10 \\ MindDo & Image & Minecraft & 10\({}^{}\) & Yes & 256+ & 12 & 100k \\ Nebraska & Text+Image & Dungeon & 1 & Yes & 69 & 78 & 80k \\ Alchemy & Image/Symbol & Chemistry & 1 & Yes & 3 & 9 & 200 \\ IVRE & Image/Symbol & Hypothesis Testing & 1 & Yes & 3 & 9 & 10 \\ SciteciVorld & Text & Elem. Science & 30 & Yes & 36 & 25 & 100 \\ DiscoveryWorld & Text+Image & Sci. Discovery & 24+10 & Yes & 63 & 14 & 1k \\   

Table 1: A comparison of existing virtual environments with DiscoveryWorld. Note that to control for spatial complexity across environments, for the purposes of counting actions, move actions are considered single actions (i.e. _move [dir]_). Information in this table has been pieced together by looking at the different source materials for each environment (e.g., paper, website, and codebase). These should be taken as our best estimate.

  
**Theme** & **Description** \\  Protocomics & Identify which species in a region migrated in the recent past by discovering that one is an outlier in a clustering analysis of protein concentration values. Higher difficulties involve more data dimensions. \\  Chemistry & Manufacture a rust removal agent by mixing different concentrations of chemicals then testing those solutions, guided by a hill-climbing signal of decreased rust that can reduce the search space. \\  Archaeology & Validate radioisotope dating by correlating radioisotope levels with known artifacts’ ages, choosing the correct radioisotope between several alternatives for dating, then identify the oldest unknown artifact. \\  Reactor Lab & Discover a relationship (linear or quadratic) between a physical crystal property like temperature or density) and its resonance frequency through regression, and use this to tune and activate a reactor. \\  Plant Nutrients & Discover that plants on Planet X prefer specific combinations of nutrients that follow logical rules (e.g. XOR, AND, OR, NOT), then grow plants by setting soil nutrient levels that follow those rules. \\  Space Sick & Investigate the cause of a mild and occasional colonist illness in response to eating local food, then formulate and implement a solution so that future colonists no longer contend with this illness. \\  Rocket Science & Measure a number of unknown planetary properties (such as the radius and mass of Planet X), then use provided equations to calculate orbital velocity and propellant needs for a rocket launch. \\  Translation & Explore an environment to infer the meanings of words in an unknown language by grounding them to observations of specific objects and actions, then take actions based on the transabled utterances. \\   

Table 2: High-level descriptions of the 8 _discovery themes_ in DiscoveryWorld, with full task descriptions (including spoilers) provided in Appendix B.3. It is from these 8 discovery themes \(\) 3 difficulty levels that we parametrically generate 120 unique instances of discovery tasks.

is provided with an observation from the environment, and must choose a single action to take during that turn from a set of possible actions. An automatic scorer runs in the background, and a given task continues until either it is solved, or the agent/human ends the simulation.

**Environment Space:** All current environments are represented as a \(32 32\) tile grid. As in text game simulators, all objects at a given tile are represented by an _object tree_, where the root node contains objects that are directly on the environment tile (such as a _table_), while child nodes of each object contain their contents (such as a _jar on the table_).

**Observations:** Observations in DiscoveryWorld can be provided as text, visual output, or both. Text observations are provided as JSON3 and contain the following: (1) a list of all objects near the agent, their names, text descriptions, and unique identifier numbers; (2) a list of objects in the agent's inventory; (3) a list of objects the agent can directly interact with (either by being in the agents inventory, or directly beside the agent); (4) the agent's current world location, direction, and directions it can move to (that are not blocked by objects); (5) whether the agent is currently engaged in dialog with another agent, and if so, what pre-determined options it can say; (6) the current task description, and whether or not it has been completed. For tasks that require interacting with other agents (such as _Space Sick_), we also implemented _DiscoveryFeed_, a Twitter-like posting environment that reduces the need for an agent to continually visit other agents to check on their status. The most recent _DiscoveryFeed_ posts are included in the observation.

The visual observation contains a screenshot (768px\(\)512px) similar to the user interface, which provides a \(24 16\) tile view of the world (each tile is 32px\(\)32px) centered on the agent (see Figure 4 in the Appendix). This provides information about objects that are farther away than the JSON observation, which is typically limited to a certain distance (configurable, default within 3 tiles) around the agent due to the size (in tokens) of the objects and their associated descriptions.

**Action space:** DiscoveryWorld includes 14 possible actions, most of which are common actions such as _taking, dropping, or moving_ objects, _opening/closing_ objects, _activating/deactivating_ objects, _using_ one object on another, and other actions found in other simulated environments . Each action can take zero (e.g. _wait_), one (e.g. _take seed_), or two (e.g. _use spectrometer on plant_) arguments. A list of all actions is shown in Appendix B.1. For agents, we include two additional handicap actions to assist with their (generally) poor abilities for navigation: the ability to teleport to specific task-relevant named locations provided in each task (such as the _science lab_ or _cafeteria_), and the ability to teleport directly beside any object it is currently (or has previously) observed. Additional details on the teleport action are provided below.

### Discovery Task Themes, Difficulty Levels, and Parametric Variations

Specific tasks in DiscoveryWorld are parametrically generated from a set of 24 high-level templates. These templates are divided into 8 _discovery task themes_ with _3 levels of difficulty_ for each theme. A high-level description of the 8 _discovery task themes_ is shown in Table 2, with full details (including spoilers) in Appendix B.3. For each of the 24 {theme \(\) difficulty} combinations, the DiscoveryWorld simulator is capable of generating a large number of parametric versions of that theme that constitute a particular instance of a _task_. These parametric variations vary the specific task solution, which typically involves dynamically generating new values for specific object properties, and/or substituting in different task objects. For example, in the _Proteomics_ theme, each parametric variation of a given task generates a different clustering problem, with different data, that points to a different animal as the solution, and places the animals at different starting locations within the environment. Parametric variation generation is deterministic and controlled by a single _random seed_ provided to the task generator. While a large number of specific instantiations of each theme are possible, due to the cost associated with evaluating a large number of tasks, our official benchmark is evaluated on 5 seeds (i.e., random seeds _zero_ through _four_) for each theme and difficulty level, resulting in a total of \(8 3 5=120\) different game instances or _tasks_.

**Task Difficulty:** Difficulty settings _(easy, normal, challenge)_ vary both the complexity of the discovery problem, and the complexity of the environment. For example, the _normal_ difficulty of _Proteomics_ requires successfully clustering two-dimensional data to arrive at a solution, while the _challenge_ difficulty requires clustering three-dimensional data. Other tasks similarly increase the difficulty of the discovery problem on some critical dimension - for example, the number of substances that have to be mixed, the complexity of the regression problem, or the complexity of rules that govern a pattern of behavior. In terms of environment complexity, both _normal_ and _challenge_ tasks are generally undertaken on large maps with complex environments, with the _challenge_ map typically an extension of the _normal_ map. In contrast, the _easy_ tasks are generally performed in small, constrained environments - typically small single rooms with few or no distractors, similar to the unit test environments. _Easy_ environments also frequently have simplified answering mechanisms, such as using forced-choice tasks (for example, requiring moving a specific kind of object into one of several containers to provide the answer).

**Reduced Spatial Complexity through Teleport:** A common criticism of evaluating complex skills (such as scientific discovery) in embodied virtual environments is that a given agent might be competent at core scientific discovery skills, but poor at skills related to embodiment, such as _picking up_ objects, _using_ objects, or _spatial tasks_ such as navigating from one location to another. If an agent requires these embodiment skills to complete a task, its overall task performance may be an underestimate of its true competency in scientific discovery. We address these challenges in two ways. First, for _spatial and navigation tasks_, we include two types of optional _teleport_ actions that can greatly simplify an agent's navigation challenges. The _teleport-to-location_ action allows an agent to instantly teleport to a specific location from a list of curated task-relevant locations for each scenario. Similarly, the _teleport-to-object_ action allows an agent to instantly teleport to any object that it has previously observed in the environment by name. Finally, for both navigation and other embodiment skills, we provide a set of unit tests (described below) that directly evaluate an agent's competency on skills required to traverse and interact with the environment.

### Unit Tests

In addition to the 8 discovery themes, to help disentangle whether a given model's performance is due to a difficulty in completing normal day-to-day tasks within the environment, versus completing tasks requiring scientific discovery skills in particular, we include 10 additional unit test themes that test specific common task competencies. These include a combination of common pick-and-place and navigation tasks (such as those found in AlFworld, MiniGrid, and MiniHack), as well as DiscoveryWorld-themed tasks, such as measuring objects with instruments, or having multi-turn interactions with other agents. The unit test generator is also parametric and capable of generating a large number of specific tasks for each unit test theme. Specific unit test themes are described in detail in Appendix B.4, along with example screenshots of the unit test environments.

### Evaluation Metrics

To evaluate agents' progress in DiscoveryWorld, we devised three automatic metrics: (1) task completion (a binary metric); (2) a fine-grained report card for each task tracking task-relevant actions, to measure partial performance on relevant discovery procedures; (3) the accuracy of discovered explanatory knowledge with respect to a gold reference. Together these allow an agent's progress (including partial progress) to be automatically assessed. An example scorecard is shown in Table 3.

    & **Out of** \\    & **/1** \\   & \\ P1 & The quantum crystals have each been in an agent’s inventory & /4 \\ P2 & Each scientific instrument has been used with at least one crystal & /5 \\ P3 & Each crystal has been examined by the critical instrument & /4 \\ P4 & The resonance frequency of the unknown reactors have been changed & /2 \\ P5 & The resonance frequency of the unknown reactors is correct & /2 \\ P6 & The reactors have been successfully activated & /4 \\  & **Total Procedural Score:** & /**25** \\   & \\ Q1 & Does it clearly state that the resonance frequency of the crystals is dependent upon the densitometer reading? & /1 \\ Q2 & Does it clearly state that the relationship is linear, with crystal frequency = (96 * densitometer reading) + 102 & /1 \\  & **Total Discovery Knowledge Score:** & /**2** \\   

Table 3: An example scorecard provided by DiscoveryWorld for an instance of a _Reactor Lab_ task, including _task completion_, _task process_, and _discovered explanatory knowledge_ scores.

Two of these three metrics (_task completion_, and _task process_) are measured automatically by DiscoveryWorld. For the third, _discovered explanatory knowledge_, the scorecard provides specific binary questions to answer with reference to knowledge that an agent has produced (in its explanations, memories, reports, or other knowledge structures). These can either be graded manually by a human, or provided to a language model to grade. DiscoveryWorld provides code for automatic grading using OpenAI models, and in our evaluation we make use of GPT-4o, a long-context (128k token) model that allows even large knowledge structures to be graded. Examples of both positive and negative knowledge assessments are provided in Appendix D.

## 4 Experiments, Baseline Agents, and Human Baselines

In this section we examine the performance of strong baseline agents on each of the DiscoveryWorld tasks. In addition, we investigate the performance of human scientists, and highlight the performance gap between current agent models and real human scientists.

### Experimental setup

For the purposes of this work, we seek to better understand the zero-shot generalization performance of AI agents on tasks that require iterative scientific discovery: that is, coming up with hypotheses, doing in-game experiments to (in)validate them, then arriving at a discovered solution for the task. As such, we evaluate the performance of three contemporary baselines in a zero-shot setting (though _single-task, multi-task, or curriculum learning_ settings with separate training and evaluation sets are

    & & & & & & & & & & & & & & \\
**\#** & **Topic** & **Task** & & & & & & & & & & & & \\ 
**Proteomics** & & & & & & & & & & & & & & & \\
1 & Easy & & & & & & & & & & & & & & \\
2 & Normal & & & & & & & & & & & & & \\
3 & Challenge & & & & & & & & & & & & & \\
**Chemistry** & & & & & & & & & & & & & & \\
4 & Easy & & & & & & & & & & & & & \\
5 & Normal & & & & & & & & & & & & & \\
6 & Challenge & & & & & & & & & & & & & \\ 
**Archaeology** & & & & & & & & & & & & & & \\
7 & Easy & & & & & & & & & & & & & \\
8 & Normal & & & & & & & & & & & & & \\
9 & Challenge & & & & & & & & & & & & & \\ 
**Reactor Lab** & & Regression & & & & & & & & & & & & \\
10 & Easy & Slope only & & & & & & & & & & & & \\
11 & Normal & & & & & & & & & & & & & \\
12 & Challenge & & & & & & & & & & & & \\ 
**Plant Nutrients** & & & & & & & & & & & & & & \\
13 & Easy & & & & & & & & & & & & & \\
14 & Normal & & & & & & & & & & & & & \\
15 & Challenge & & & & & & & & & & & & & \\ 
**Space Sick** & & & & & & & & & & & & & & \\
16 & Easy & & & & & & & & & & & & & \\
17 & Normal & Multiple instruments & & & & & & & & & & & & \\
18 & Challenge & & & & & & & & & & & & \\ 
**Rooket Science** & & & & & & & & & & & & & \\
19 & Easy & & & & & & & & & & & & \\
20 & Normal & & & & & & & & & & & & \\
21 & Challenge & & & & & & & & & & & & \\ 
**Translation** & & & & & & & & & & & & & & \\
22 & Easy & Single noun & & & & & & & & & & & \\
23 & Normal & & & & & & & & & & & & \\
24 & Challenge & & & & & & & & & & & & \\  
**Average (Easy)** & & & & & & & & & & & & & & & \\
**Average (Normal)** & & & & & & & & & & & & & & \\
**Average (Challenge)** & & & & & & & & & & & & & & \\   

Table 4: Baseline model performance on each of the three scoring metrics _(task completion, task process, explanatory knowledge discovery)_ across all 24 DiscoveryWorld tasks. Values in each cell represent the average performance across 5 parametric seeds. _Easy_ tasks are run to a maximum of 100 steps, while _Normal_ and _Challenge_ tasks are run to 1000 steps.

possible with this benchmark; see Appendix B.2 for these configurations). In the zero-shot setting, an agent has no prior exposure to DiscoveryWorld, and is evaluated on all 120 tasks. Each task is evaluated independently, without carry-over knowledge from one task to another.

### Baseline Agent Models

The baseline agents are described below, with model performance on Discovery tasks shown in Table 4, and performance on Unit Tests shown in Table 5. We use the GPT-40 model for all our agents due to its higher performance and lower cost compared to other models available. For space we provide high-level descriptions of each model here, with additional implementation details and run costs provided in Appendix C.

**ReAct**: This agent uses the ReAct  approach of generating a thought and action at each step given the recent trajectory of thoughts, actions and observations. Each action is executed in the environment and the observation is added to the trajectory. In addition to this trajectory, we also provide the current game state information as text, e.g., nearby objects, teleportable locations, etc. If needed, we trim the trajectory (remove oldest steps first) to fit the prompt within the maximum token limit, which (in practice) included up to the last 40 steps of the trajectory. To evaluate this agent's discovered knowledge, we evaluate the concatenation of the agent's "thoughts" across all time steps.

**Plan+Exec**: Since the ReAct trajectories can be very long and lead to errors due to distractors, we also evaluate a plan-and-execute  approach. We use the LLM to generate a plan, and each step of the plan is independently executed using the same ReAct agent as above. Since each plan step is simpler than the task, their ReAct trajectories are much smaller, reducing the distractors. Discovery tasks require an iterative planning approach to adapt to new findings, so we use iterative decomposition  to only generate one step of the plan based on the previous planning steps and their success. Discovered knowledge is evaluated as with the ReAct agent from the execution steps.

**Hypothesizer**: This agent resembles the architecture of CLIN , with the agent keeping an explicit working memory of running hypotheses and measurements that are updated after taking each action, and conditioning its next actions on the contents of this memory. This explicit knowledge store allows more directly evaluating an agent's discovered knowledge (with an example of Hypothesizer's knowledge provided in subsection C.3). The agent similarly maintains a brief plan and running hypothesis, and is prompted to explain how its action progresses the plan to evaluate that hypothesis.

**Text only vs. Multi-Modal:** For this baseline evaluation, both ReAct and Plan+Exec models are _text-only_, in that they include only text (or JSON-formatted) input. The Hypothesizer agent includes both text observations as well as visual observations (i.e. screen frames, as provided by the API) as input.

    & &  &  &  \\  \# &  & & & & & & & \\ 
25 & Multi-turn dialog with an agent & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
26 & Measure an object with an instrument & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
27 & Pick-and-place object & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
28 & Pick-and-give object & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
29 & Real DiscoveryReal posts & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
30 & Move through doors & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
31 & Using keys with doors & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
32 & Navigate to a specific room in a house & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
33 & Search an environment for an object & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
34 & Interact with a moving agent & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
**Average (Unit Tests)** & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\   

Table 5: Baseline model performance on two relevant scoring metrics _(task completion, task process)_ across all 10 unit test tasks.Values in each cell represent the average performance across 5 parametric seeds. Unit tests tasks are run to a maximum of 100 steps. As the unit tests evaluate basic skills instead of discovery skills, explanatory knowledge is not evaluated.

### Human Evaluation

To compare model performance against human performance, we recruited 11 practicing human scientists to complete the DiscoveryWorld tasks, with their performance shown in Table 6. Scientists were recruited on the UpWork platform, each with: (1) an MSc or PhD in a natural science, (2) self-evaluated comfort and fluency with statistical methods and common software like spreadsheets, (3) comfort and previous experience with 2D top-down games. Additional details regarding human participant experiments are provided in Appendix E.

## 5 Discussion

Human PerformanceAs shown in Table 6, discovery task completion rates varied from tasks that were solved by all participants, to several _Challenge_ difficulty tasks that were solved by only a single participant, highlighting the range of difficulties found across task themes, and range of expertise provided by each scientist. Overall, the average completion rate across tasks was 66%, with 11 of 16 tasks performed by humans having completion rates exceeding 60%. Average knowledge performance was slightly lower at 55%, reflecting that when stuck, the humans sometimes attempted brute force solutions to tasks that may have eventually yielded correct answers (e.g. trying all possible crystal frequencies, if they didn't use regression to fit the data), but without producing the required explanatory discovery.

Agent PerformanceIn contrast to human scientists, the baseline agent scientists exhibited poor overall performance, as shown in Table 4. The most performant discovery agent (ReAct) compeleted 38% of _easy_ tasks and 18% of _challenge_ tasks, while the agent that best discovered explanatory knowledge (Hypothesizer) discovered only 34% of gold knowledge in _easy tasks_, and only 8% in _challenge_ tasks. An analysis of these agents performance on the Unit Test tasks (Table 5) shows moderate overall performance, with completion rates in the 60%+ range, suggesting that current agents are competent at many of the components of scientific discovery - like measuring objects with instruments - but currently lack the capacity to perform end-to-end discovery in most DiscoveryWorld settings.

    &  \\  \# & **Topic** & **Difficulty** &  &  &  &  &  &  &  &  &  &  \\ 
2 & Proteomics & Normal & 10.00 & 0.80 & 0.90 & 277 & 262 & 15 & 0.06 & 10 \\
3 & Proteomics & Challenge & 1 & 1.00 & 1.00 & 203 & 192 & 11 & 0.05 & 10 \\ 
5 & Chemistry & Normal & 1.00 & 0.64 & 369 & 293 & 76 & 0.23 & 10 \\
6 & Chemistry & Challenge & 1.00 & 0.89 & 0.77 & 401 & 324 & 76 & 0.21 & 9 \\ 
8 & Archaeology & Normal & 1.00 & 0.92 & 0.40 & 0.91 & 310 & 275 & 35 & 0.14 & 10 \\
9 & Archaeology & Challenge & 0.77 & 0.36 & 0.09 & 276 & 240 & 36 & 0.13 & 11 \\ 
11 & Reactor Lab & Normal & 0.78 & 0.60 & 0.36 & 414 & 340 & 74 & 0.18 & 10 \\
12 & Reactor Lab & Challenge & 0.70 & 0.33 & 0.25 & 281 & 236 & 45 & 0.16 & 9 \\ 
14 & Plant Nutrients & Normal & 1.00 & 0.80 & 0.44 & 365 & 310 & 55 & 0.15 & 10 \\
15 & Plant Nutrients & Challenge & 1.00 & 0.70 & 0.32 & 358 & 306 & 52 & 0.16 & 10 \\ 
17 & Space Sick & Normal & 0.69 & 0.73 & **0.59** & 2111 & 1958 & 153 & 0.08 & 11 \\
18 & Space Sick & Challenge & 0.60 & 0.11 & 0.11 & 3458 & 2988 & 470 & 0.13 & 9 \\ 
20 & Rocket Science & Normal & 0.58 & 0.30 & **0.40** & 274 & 240 & 34 & 0.13 & 10 \\
21 & Rocket Science & Challenge & 0.57 & 0.11 & 0.33 & 487 & 334 & 153 & 0.36 & 9 \\ 
23 & Translation & Normal & 0.79 & 0.73 & **0.77** & 1033 & 948 & 86 & 0.07 & 11 \\
24 & Translation & Challenge & 0.62 & 1.01 & **0.68** & 859 & 794 & 65 & 0.07 & 11 \\   **Average (Human)** \\ **Average (Human)** \\ **Average (Human)** \\  } &  &  &  &  &  &  &  &  &  \\   

Table 6: Expert human scientist performance on DiscoveryWorld tasks, as well as average task completion time. Scores represent average performance of up to 11 humans when playing the same seed of a discovery task.

Conclusion

We have presented DiscoveryWorld, the first virtual environment for developing and benchmarking an agent's ability to perform end-to-end scientific discovery. Each of the 120 tasks requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. We empirically demonstrate that expert human scientists find the challenge tasks in DiscoveryWorld difficult but solvable, while strong agent baselines struggle to complete most tasks, or discover critical explanatory knowledge. We hope DiscoveryWorld will inspire and accelerate the development of new, general AI discovery agents by the community.

## 7 Limitations and Broader Impacts

Simulation Fidelity:DiscoveryWorld is inherently a low-fidelity representation of the physical world, with an abstracted action space. As such, agents that perform well on DiscoveryWorld may not necessarily perform well at making discoveries in the real world, given the much larger search and action space. That being said, while the simulated environment is low-fidelity compared to the physical world, the steps of discovery that are simulated (from ideation, hypothesis formation, experiment design and execution, data analysis, and forming conclusions) are common steps in the scientific method regardless of whether those actions are taking place in the real world or a virtual environment.

Agent Cost:The cost of LLM inference is rapidly decreasing, as evidenced by OpenAI recently releasing a multi-modal model (o1-mini) that costs approximately \(\$0.15/1M\) input tokens, a decrease of almost 20 times over the GPT-40 base model used in this work (current cost: \(\$2.50/1M\) input tokens), which was the most performant model at the time of submission. At submission time, these GPT-40 agent models were quite costly, ranging from approximately USD3k-$10k to run for the complete set of 120 tasks in DiscoveryWorld, though their cost has since decreased. This cost is due in large part to (1) the long (1000+ steps) runtimes, with each step requiring at least one LLM call, (2) the large number of individual tasks to evaluate, and (3) the use of performant but costly API-based models that charge per token. We believe that developing inexpensive models that allow for rapid iteration is a clear near-term goal to help facilitate developing discovery agents that must perform long-horizon tasks in this 1000+ step range. A recommended **reduced-cost evaluation** is to evaluate on only the _easy_ difficulty tasks, while limiting runtime to _100 steps_ per run. We estimate (using the cost estimates in Appendix C.4) such a setup would cost approximately $50-100 for the ReAct agent at current GPT-40 pricing, and potentially significantly less with other base models.

Societal Benefits and Risks:Automated scientific discovery has the potential for broadly positive societal impact by accelerating the pace of scientific progress, potentially decreasing the time it takes for novel discoveries in medicine, chemistry, technology, and other areas of broad impact. If discovery systems are used by individuals or groups with prosocial intentions, there is the potential for broad societal impact. Conversely, if individuals or groups with negative intentions choose to attempt to accelerate discoveries that may be harmful, there is the potential for those individuals or groups to cause harm using automated scientific discovery models.