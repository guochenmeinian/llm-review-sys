# GaussianCut: Interactive Segmentation

via Graph Cut for 3D Gaussian Splatting

Umangi Jain, Ashkan Mirzaei, and Igor Gilitschenski

University of Toronto

{umangi, ashkan, gilitschenski}@cs.toronto.edu

###### Abstract

We introduce GaussianCut, a new method for interactive multiview segmentation of scenes represented as 3D Gaussians. Our approach allows for selecting the objects to be segmented by interacting with a single view. It accepts high-level user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene's Gaussians. Our key idea is to represent the scene as a graph and use the graph-cut algorithm to minimize an energy function to effectively partition the Gaussians into foreground and background. To achieve this, we construct a graph based on scene Gaussians and devise a segmentation-aligned energy function on the graph to combine user inputs with scene properties. To obtain an initial coarse segmentation, we leverage 2D image/video segmentation models and then use graph cut to further refine these coarse estimates. Our empirical evaluations show the adaptability of GaussianCut across a diverse set of scenes. GaussianCut achieves competitive performance with state-of-the-art approaches for 3D segmentation without requiring any changes to the underlying Gaussian-based scene representation. Project page: https://umangi-jain.github.io/gaussiancut/.

Figure 1: Our method, GaussianCut, enables interactive object(s) selection. Given an optimized 3D Gaussian Splatting model for a scene with user inputs (clicks, scribbles, or text) on any viewpoint, GaussianCut partitions the set of Gaussians as foreground and background.

Introduction

Recent advances in 3D scene representation have enabled unprecedented quality in 3D view synthesis without requiring specialized equipment or an excessively high computational budget. Fully leveraging these advances requires tools for scene understanding and manipulation specifically designed to operate on such representations. Object selection and segmentation often serve as a crucial first step in both scene understanding and editing tasks. While 2D image segmentation has been widely studied, developing analogous techniques for 3D remains challenging. One key challenge is accounting for the choice of underlying 3D scene representation in the segmentation method.

3D Gaussian Splatting (3DGS)  offers an explicit representation of a scene using a set of Gaussians. The nature of this representation motivates the idea that each Gaussian corresponds to either the segmented object or the background. Prior works in 3DGS segmentation involve augmenting each Gaussian with a low-dimensional feature, that is jointly optimized with the parameters of the Gaussians [6; 44; 55]. This is supervised by 2D features, which provide semantic information that can be used for segmentation. While this enables a 3D consistent segmentation, it significantly increases the fitting time and the already high memory footprint of the method. Thus, enabling 3DGS segmentation without modifying the optimization process is an important research challenge.

We address this challenge by proposing GaussianCut, a novel method for selecting and segmenting objects of interest in 3D Gaussian scenes. Our work taps directly into the representation created by 3DGS and maps each Gaussian to either the foreground or background. The proposed process mirrors the interactive nature of 2D segmentation tools, where users can engage through clicks, prompts, or scribbles. We require such user input on a single image and perform the object selection process in two steps. First, we obtain dense multiview segmentation masks from the user inputs using a video segmentation model. Subsequently, we construct a weighted graph, where each node represents a Gaussian. Graph cut then partitions the graph into two disjoint subsets by minimizing an energy function, which quantifies the cost of cutting the edges connecting the subsets. This approach effectively segments the selected foreground object from the background by using the energy function as a measure of dissimilarity between the nodes. An overview of the process is provided in Figure 1.

Our main contribution is a novel approach for segmentation in scenes obtained from 3DGS. Its main technical novelties are twofold: 1) we propose a method for graph construction from a 3DGS model that utilizes the properties of the corresponding Gaussians to obtain edge weights, and 2) based on this graph, we propose and minimize an energy function (Equation 3) that combines the user inputs with the inherent representation of the scene. Our experimental evaluations show that GaussianCut obtains high-fidelity segmentation outperforming previous segmentation baselines.

## 2 Related Work

2D image segmentation is a long studied problem in computer vision [17; 41; 57]. Recently, models like Segment Anything  and SEEM , have revolutionized 2D segmentation by employing interactive segmentation. A range of methods have also been developed for 3D segmentation, each tailored to different forms of representation, including voxels [10; 36], point clouds [42; 43], meshes [49; 62], and neural representations [7; 32; 48; 52]. The impressive capabilities of Neural Radiance Fields (NeRFs)  in capturing scene information implicitly have inspired numerous studies to explore 3D segmentation for NeRFs. Recent works have also explored segmentation with Gaussians as the choice for scene representation [6; 22; 44; 55; 63].

**Training 3D segmentation with 2D masks/features:** In addition to the wide adaptation of foundational models for 2D images , they are also used extensively by 3D editing and segmentation models. SAM has been used as an initial mask to facilitate 3D segmentation [6; 7; 33] and also for distillation into NeRF  and 3DGS models . Semantic-NeRF  proposed 2D label propagation to incorporate semantics within NeRF so it can produce 3D consistent masks. MVSeg  propagates a 2D mask to different views using video segmentation techniques. ISRF  distills semantic features into the 3D scene of voxelized radiance fields. Nearest neighbor feature matching then identifies high-confidence seed regions. 2D features have also been used for facilitating the grouping of Gaussians  and for hierarchical semantics using language in 3DGS . Distilled Feature Fields (DFF)  and Neural Feature Fusion Fields  distill 2D image embeddings from LSeg  and DINO  to enable segmentation and editing. SA3D  uses SAM iteratively to get 2D segments and then uses depth information to project these segments into 3D mesh grids. SANeRF-HQ  aggregates 2D masks in 3D space to enable segmentation with NeRFs.

**Segmentation in 3D Gaussian Splitting:** Gaussian Grouping , SAGA , LangSplat , CoSSegGaussians  and Feature 3DGS  require optimizing a 3DGS model with an additional identity or feature per Gaussian, which is usually supervised by 2D image features. These semantic features allow segmentation through user interaction. Gaussian Grouping and LangSplat also allow for textual prompts to segment objects supported through multimodal models like CLIP or grounding-DINO . Feature-based methods alter the fitting process of 3DGS by adding additional attributes for each Gaussian and it facilitates learning features for everything in the scene. While useful, this limits the flexibility of interactivity with a single object. Our method is more flexible in choosing specific object(s) as we generate the 2D masks after the user interaction. Adding additional parameters also increases the fitting time for 3DGS. Moreover, such methods often rely on video segmentation models as they require 2D features from all training viewpoints. In contrast, we can operate on an arbitrary number of 2D masks, including just a single mask.

**Graph cut for 3D segmentation:** Boykov and Jolly  introduced a novel global energy function for interactive image segmentation using graph cut [15; 19]. Several follow-up works improved image segmentation using graph cut by designing a better energy function , improving optimization [3; 20], and reducing user input requirements . Adapting energy minimization methods for 3D volumes has been difficult, requiring several modifications  to manage the higher memory demands. NVOS  trains a special multi-layer perceptron (MLP) to predict voxels in the foreground and background and applies graph cut on voxels as post-processing. However, this requires additional training and increases memory consumption. Guo _et al._ propose 3D instance segmentation of 3D point clouds using graph cut. It involves constructing a superpoint graph and training a separate graph neural network for predicting the edge weights. Unlike their work, our method is a post hoc application and does not require any additional training. Our graph construction and edge weights have also been tailored specifically for 3D Gaussian Splatting.

Concurrent with our work, Segment Anything in 3D Gaussians (SAGD) , also performs interactive segmentation using 3D Gaussian Splatting without requiring any segmentation-aware training. However, their primary focus is on refining object boundaries by decomposing boundary Gaussians.

## 3 Method

### Preliminaries

**3D Gaussian Splatting (3DGS)** is a technique for creating a 3D representation of scenes based on a set of Gaussian ellipsoids \(\). 3DGS facilitates real-time rendering and provides high-quality reconstruction. In this representation, each 3D Gaussian is characterized by a set of optimizable parameters. These include 3D position \(^{3}\), spherical harmonics (SH) coefficients (for color) \(^{3(d^{2}+1)}\) (where \(d\) is the degree of spherical harmonics), scale \(^{3}\), rotation \(^{4}\), and opacity \(\). The optimization process involves iteratively rendering scenes and comparing the rendered images with the training views, interleaved with adaptive density control that handles the creation and deletion of the number of Gaussians. The differentiable rendering pipeline in 3DGS uses tile-based rasterization following  to ensure real-time rendering. 3DGS performs splitting by depth sorting the Gaussians and \(\)-blending  them to project in 2D. The set of differentiable parameters for \(\), \(:=\{_{i},_{i},_{i},_{i}\, _{i}\}_{i=1}^{||}\), are optimized from a set of posed images.

**Graph cut** refers to the problem of partitioning the vertices \(\) of a graph \(\) with edges \(\) weighted by \(\{w_{e}\}_{e}\) into two disjoint, non-empty sets such that the sum of the weights of the edges between the two sets is minimized. This minimum-cost partitioning is known as the _minimum cut_. Segmentation can be reformulated as graph cut. This is achieved by forming a graph that includes the segmented entities (e.g. pixels, voxels, Gaussians) and introducing weighted edges. This allows defining an energy function, which includes unary terms representing the cost of assigning a node to a set based on individual properties, and pairwise terms that incorporate the cost of assigning neighboring nodes to different sets. The objective of the minimization is to find a cut that optimizes the overall energy, balancing individual preferences and neighborhood interactions. An efficient way for computing this minimum cut in a graph is the Boykov-Kolmogorov algorithm .

### Overview

Given a set of posed RGB images \(=\{_{i}\}_{i=1}^{k}\) and an optimized reconstruction of the scene using a set of Gaussians \(\), we define the task of interactive 3D segmentation as follows: given a user input (point clicks, scribbles, or text) on any image \(_{0}\), the objective is to partition the set of Gaussians in two non-empty disjoint sets \(\) and \(\) such that \(\) represents the set of Gaussians representing the object(s) of interest and \(\) represents the given 3D scene without these object(s). The extracted subset of Gaussians \(\) can be rendered from any viewpoint to effectively cutout the 3D representation of the foreground without retraining. Imately, the other set of Gaussians \(\) can be rendered to remove the foreground object(s). Figure 2 shows an overview of our pipeline.

In order to densify the object selection information provided by the user, we use an off-the-shelf video segmentation model to obtain dense segmentation masks from multiple views (discussed in section 4). For transferring the segmentation masks to the 3DGS representation, we trace the 3D Gaussians that map to the selected region in the masks (discussed in section 3.3). However, the masks used for propagation are not 3D consistent (as the underlying image segmentation model is 3D-unware). Moreover, the errors from 2D masks can propagate in the traced Gaussians and thereby provide a noisy 3D segmentation. To obtain a precise set of foreground Gaussians, we formulate the set of Gaussians \(\) as nodes in a graph (discussed in section 3.4) and leverage graph cut to split the nodes into two sets: the foreground \(\) and the background \(\).

### Mapping user inputs to Gaussians

We first feed the sparse single-view annotations by the user (e.g., point clicks) to a multiview/video segmentation model to obtain coarse segmentation masks across multiple training views. We then propagate the information from the 2D masks onto the set of Gaussians. For an already optimized 3DGS model of a scene, \(\), we obtain \(n\) masks \(:=\{^{j}\}_{j=1}^{n}\) from a video segmentation model corresponding to any \(n\) viewpoints \(:=\{^{j}\}_{j=1}^{n}\). Here, \(^{j}\) indicates the set of foreground pixels in the viewpoint \(^{j}\). For each Gaussian \(g\), we maintain a weight, \(w_{g}\), that indicates the likelihood of the Gaussian belonging to the foreground. To obtain the likelihood term \(w_{g}^{j}\) pertaining to mask \(j\) for Gaussian \(g\), we unproject the posed image \(^{j}\) back to the Gaussians using inverse rendering and utilizing the mask information,

\[w_{g}^{j}=^{j}}_{g}()T_{g} ^{j}()}{_{^{j}}_{g}()T_{ g}^{j}()},\] (1)

Figure 2: Overall pipeline of GaussianCut. User input from any viewpoint is passed to a video segmentation model to produce multi-view masks. We rasterize every view and track the contribution of each Gaussian to masked and unmasked pixels. Then, Gaussians are formulated as nodes in an undirected graph and we adapt graph cut to partition the graph. The red edges in the graph highlight the set of edges graph cut removes for partitioning the graph.

where \(_{g}()\) and \(T_{g}^{j}()\) denote the opacity and transmittance from pixel **p** for Gaussian \(g\). Combining over all the \(n\) masks,

\[w_{g}=_{^{j}}_{g}( )T_{g}^{j}()}{_{j}_{^{j}} _{g}()T_{g}^{j}()}.\] (2)

This likelihood, \(w_{g}\), captures the weighted ratio of the contribution of Gaussian \(g\) to the masked pixels relative to the total number of pixels influenced by it. The complementary value of \(w_{g}\), \(1-w_{g}\) provides the likelihood of Gaussian \(g\) contributing to the background. The value of \(w_{g}\) is updated using \(n\) 2D segmentation masks during rasterization. Since the rasterization of 3DGS is remarkably fast, each pass typically takes less than a second to update. GaussianEditor  also learns an additional tracing parameter but unlike our approach, it maps Gaussians to different semantic classes.

Having the likelihoods \(w_{g}\), a naive approach to extract the 3D representation of the foreground is to threshold the Gaussians and prune those with values below a certain threshold \(\). We denote this approach as "coarse splatting". Figure 4 demonstrates coarse splatting renders for a _plant_ in the 360-garden scene . Note that the renderings produced by coarse splatting are not accurate, particularly around the edges. This is due to two main reasons: 1) the 2D segmentation models are 3D inconsistent and can be imperfect, leading to artifacts in the final Gaussian cutout, and 2) lifting 2D masks to 3D can introduce its own artifacts.

### Gaussian graph construction

After rasterization with the masks, each Gaussian \(g\) in the 3DGS representation is characterized with parameters \(_{g}:=\{_{g},_{g},_{g}, _{g}\,_{g},w_{g}\}\), where \(w_{g}\) captures the user requirement and the other parameters encapsulate the inherent properties of the scene. To fuse the two sources of information and obtain a precise set of foreground Gaussians, we formulate the optimized 3DGS model as an undirected weighted graph \(:=(,)\), where each Gaussian in \(\) is a node and \(\) represents the set of edges connecting spatially adjacent Gaussians. We define the neighborhood \(\) of a node (Gaussian) as its \(k\)-nearest Gaussians in terms of their 3D position. The intuition behind constructing the edges is that Gaussians that map to the same object would be close spatially.

Gaussian graph cut partitions the Gaussians \(\) into two disjoint and non-empty sets \(\) and \(\), that represent the foreground and background Gaussians, respectively. Our objective is to infer the foreground/background label \(y_{g}\{0,1\}\) of each Gaussian \(g\). Let the unary term \(_{g}(,)\) represent the likelihood of node \(g\) being part of foreground or background and the pairwise term \(_{g,g^{}}(,)\) reflect the edge connection between node \(g\) and \(g^{}\). To obtain the label for each Gaussian \(g\), graph cut minimizes the aggregate of both unary and pairwise terms given by:

\[E=_{g}_{g}(_{g},y_{g})+ _{g,g^{}}_{g,g^{}}(_{g},_{g^{}}),\] (3)

where \(\) provides a trade-off between the two terms.

**Neighboring pairwise weights (_n-links_)**: The pairwise term models the correlation between neighboring nodes. The neighbors for a node are based on its spatial proximity to other nodes. The edge weight between each pair of neighbors is a combination of its spatial distance and color similarity. While segments of an object can have multiple colors, and they often do, neighboring nodes with dissimilar colors can still be identified and grouped based on their spatial proximity. This ensures that parts of an object, despite varying in color, can be linked if they are close in space. For color similarity, we only use the zero-degree spherical harmonic to capture the ambient colors without any angular dependence. The correlation between the neighboring nodes is formulated as

\[_{g,g^{}}(_{g},_{g^{}})= (_{g},_{g^{}})+_{n} (_{g},_{g}^{}),\] (4)

where \(_{n}\) is a hyperparameter balancing the contribution of position and color similarity, and the function **f** estimates similarity as \((,)=(-\|-\|_{2}^{2})\) (\(\) is a positive scalar).

**Unary weights (_t-links_)**: We designate two terminal nodes for the graph cut algorithm, the source and the sink node. These terminals represent the foreground (source) and the background (sink) in segmentation tasks. \(t\)-links connect all the nodes to both the terminal nodes and the edge weight for these links represents the pull of that node towards each terminal node. We assign the edge weights connecting each non-terminal node to the source node to reflect its likelihood of belonging to the foreground set \(\) (and as belonging to the background set \(\) for edges connecting to the sink node).

Gaussian tracking, from section 3.3, provides the connection of each Gaussian \(g\) to the source and sink terminal nodes, using \(w_{g}\) and \(1-w_{g}\), respectively. However, these weights can be noisy estimates. Therefore, we introduce an additional term to the edge weights that captures the similarity of node \(g\) to the other nodes that are well-connected to the terminal nodes. To do so, we identify high-confidence nodes for both the source and the sink terminals. A Gaussian \(g\) is considered as a high-confidence node for the source terminal if \(w_{g} 1\) and for a sink terminal if \(w_{g} 0\). Since computing the similarity of a node to all the high-confidence nodes is computationally expensive, we cluster all the high-confidence nodes (denoted as \(\) and \(\) for the source and sink, respectively) based on their position. For each node \(g\), we then determine the closest cluster by finding \(g_{f}=*{argmin}_{g_{f}}(_{g}, _{g^{}})\) for the source, and similarly \(g_{b}\) for the sink. Consequently, the unary term based on the user input is,

\[_{g}(_{g},y_{g})=w_{g}+_{u}_{g,g_{f}}( _{g},_{g_{f}})&y_{g}=1,\\ 1-w_{g}+_{u}_{g,g_{b}}(_{g},_{g_{b}})&y_{g}=0.\] (5)

We minimize the objective \(E\) in Equation 3 to partition the set of nodes \(\) as \(\) (foreground Gaussians) and \(\) (background Gaussians). To render the foreground object from any viewpoint, we simply render the Gaussian collected in \(\) with \(\) as background.

## 4 Experimental Setup

**Datasets:** For quantitative evaluation, we test the scenes from LLFF , Shiny , SPIn-NeRF , and 3D-OVS . All selected scenes from the LLFF and Shiny datasets are real-world front-facing scenes, with 20-62 images each. SPIn-NeRF provides a collection of scenes from some of the widely-used NeRF datasets [16; 26; 34; 35; 56]. It contains a combination of front-facing and \(360^{}\) inward-facing real-world scenes. 3D-OVS contains scenes featuring long-tail objects.

**Input types:** Our model accepts all input processed by SAM-Track . It uses grounding-DINO  to process text inputs. For the LLFF scenes used in NVOS , we follow their input scribbles to obtain the initial mask. For SPIn-NeRF and Shiny, we use clicks (each scene typically requires 1-4 clicks). For the 3D-OVS dataset evaluation, we use text query as input (results in Table 11).

**Evaluation metrics:** Different Image-Based Rendering (IBR) models represent 3D scenes in different ways. Thus, obtaining universal ground-truth 3D masks is difficult. To avoid this challenge, we evaluate the segmentation mask of the projected 2D rendering from the scene. The ground-truth 2D masks are typically obtained from professional image segmentation tools. NVOS provides one ground-truth mask for every scene in LLFF. SPIn-NeRF and 3D-OVS provide masks for multiple images in every scene. Shiny dataset does not contain any ground-truth masks so we create our own ground-truth mask. For evaluation, we generate 2D foreground masks by rendering the Gaussians

Figure 3: Visualization results of different objects in the following scenes: truck from Tanks and Temples , kitchen from Mip-NeRF 360 , tools from Shiny .

from the desired viewpoint. We use pixel classification accuracy (Acc) and foreground intersection-over-union (IoU) for evaluating the segmentation masks.

Following NVOS, we also assess the photo-realistic appearance of the segmented object by rendering it against a black background. We trim both the rendered image and the ground-truth image to the foreground object by applying a bounding box that fits the ground-truth mask. This prevents the evaluation from being biased by the background, especially when the object of interest is relatively small. The metrics we report are PSNR, SSIM , LPIPS .

**Implementation details:** To obtain segmentation masks from the user inputs (used in Section 3.3), we leverage the advancements in video segmentation models. The user selects the foreground objects on \(_{0}\), and we obtain dense masks for multiple views using SAM-Track . Note that the use of the video segmentation model is done to enhance the performance further and our method can also work with a single image mask (Table 4). We use KD-Tree for efficiently finding the \(k\) nearest neighbors to construct the edges between the nodes.

For all the evaluations, we resize the longer image size to 1008, as commonly practiced in novel-view synthesis. We optimize 3DGS model for each of the considered scenes, without making any changes to the original 3DGS code. For coarse splatting, we keep the cut-off threshold \(=0.9\) for front-facing views and \(=0.3\) for the \(360^{}\) inward-facing scenes. This disparity stems because parts of objects might not be observed from every viewpoint for the latter and also because of the relative ineffectiveness of video tracking for inward-facing scenes (Figure 8). For graph cut, we keep \(=0.1\) for neighboring pairwise position weights and \(=1\) for all other weights, the number of neighbors for every node as 10, and the number of clusters for high-confidence nodes as 4 for sink and 1 for source. \(\), \(_{n}\), \(_{u}\) can be adjusted depending on the scene and the quality of coarse splatting but generally, \(_{n}=_{u}=1\) and \(=0.5\) give decent results.

**Baselines:** Our comparison includes a selection of baseline models such as NVOS , MVSeg , Interactive segmentation of radiance fields (ISRF) , Segment Anything in 3D with NeRFs (SA3D) , Segment Any 3D Gaussians (SAGA) , Segment Anything in 3D Gaussians (SAGD) , Gaussian Grouping , and LangSplat . Unlike our approach, SAGA, Gaussian Grouping, and LangSplat alter the Gaussian optimization process by learning additional features per Gaussian that increases the optimization time (Table 9). SAGD is a concurrent work also designed for 3DGS segmentation and has not yet been published. Thus, their results may be subject to change. SAGD, similar to our approach, does not require any segmentation-aware training and uses a cross-view label voting approach to segment selected objects. All the baselines allow for selecting objects using clicks, except LangSplat, for which we use text queries. Further details on baseline implementation are provided in Appendix A.1.

## 5 Results

### Quantitative results

**Dataset from NVOS:** We take the seven scenes from LLFF dataset used in NVOS. NVOS contains a reference image with input scribbles and a target view with an annotated 2D segmentation mask. As shown in Table 1, GaussianCut outperforms other approaches. Unlike ISRF, SAGA, GaussianGrouping, and LangSplat, GaussianCut works on pretrained representations and does not require any changes to the training process. Owing to the fast rasterization, 3DGS-based approaches can also

  Method & IoU (\%)\(\) & Acc (\%)\(\) \\  graph cut (3D) [45; 46] & 39.4 & 73.6 \\ NVOS  & 70.1 & 92.0 \\ ISRF  & 83.8 & 96.4 \\ SA3D  & 90.3 & 98.2 \\ SAGD  & 72.1 & 91.7 \\ SAGA  & 90.9 & 98.3 \\ Gaussian Grouping  & 90.6 & 98.2 \\ LangSplat  & 74.0 & 94.0 \\ GaussianCut (Ours) & **92.5** & **98.4** \\  

Table 1: Quantitative results for 2D mask segmentation on NVOS dataset .

  Method & IoU (\%)\(\) & Acc (\%)\(\) \\  MVSeg  & 90.4 & 98.8 \\ ISRF  & 71.5 & 95.5 \\ SA3D  & 92.4 & 98.8 \\ SAGD  & 89.7 & 98.1 \\ SAGA  & 88.0 & 98.5 \\ Gaussian Grouping  & 88.4 & 99.0 \\ LangSplat  & 69.5 & 94.5 \\ GaussianCut (Ours) & **92.9** & **99.2** \\  

Table 2: Quantitative results on the SPInNeRF dataset .

[MISSING_PAGE_FAIL:8]

SPIn-NeRF dataset (images used were unordered). Since 3DGS offers fast rasterization, the overall time cost for segmentation does not grow linearly with the number of masks as the time taken for segmentation dominates. We also show the qualitative performance with a single mask (no video segmentation model) and just scribbles (no image segmentation model) in Figure 13.

**Sensitivity of each term in the graph cut energy function:** In order to understand the contribution of each term in the energy function, Table 5 shows the average IoU on the NVOS dataset with each term removed from Equation 4 and 5. Each term contributes to the overall performance and the cluster similarity, in particular, gives a significant boost.

**Sensitivity of graph cut hyper-parameters:** We test the sensitivity of our Gaussian graph cut algorithm on the number of neighbors (number of edges for each node) and the number of high-confidence clusters. As the number of neighbors increases, the number of edges in the graph also increases (so does the time taken for graph cut). As seen in Table 7, adding more edges can help in modeling more long-distance correlations. However, after a limit, the effects of adding more edges diminish. Adding a large number of clusters for the high-confidence nodes, in Table 8, does not affect the performance drastically and the optimal number can vary depending on the scene. We show sensitivity to other hyper-parameters in Appendix E.

## 6 Discussion

Our results demonstrate that 3DGS allows for direct segmentation using a pretrained model. Developments in 2D segmentation and tracking have played a crucial role in 3D segmentation. We observe that GaussianCut not only generates 3D consistent masks but also improves the segmentation quality of 2D masks by capturing more details (Figure 12). This is more prominent for \(360^{}\) scenes, where the tracker can partially or fully miss the object of interest (Figure 8).

   Number of views & 5 (10\%) & 9(20\%) & 21 (50\%) & 43 (100 \%) \\  Coarse Splatting on Fortress & 96.1 & 96.3 & 96.5 & 96.8 \\ GaussianCut on Fortress & 97.7 & 97.8 & 97.8 & 97.9 \\ Time Cost (s) & 51 & 55 & 59 & 71 \\   Number of views & 11 (10\%) & 21(20\%) & 51 (50\%) & 102 (100 \%) \\  Coarse Splatting on Lego & 85.5 & 88.0 & 88.4 & 88.9 \\ GaussianCut on Lego & 87.3 & 89.1 & 89.2 & 89.2 \\ Time Cost (s) & 58 & 62 & 72 & 90 \\   

Table 6: Performance of GaussianCut with varying the number of views passed to the video segmentation models. The number in parenthesis is the percentage of total views for the scene.

Figure 4: Qualitative comparison: 3D segmentation results of GaussianCut using text on 360-garden  scene. Compared to ISRF , SA3D , SAGD , GaussianCut segment contain finer details. The graph cut component of GaussianCut also retrieves fine details (like decorations on the plant) that are missed in coarse splatting.

**Time requirement:** Since we use pretrained 3DGS models, the optimization time for the Gaussians remains the same as in  (it took under 15 minutes for every scene we used). For inference, masked rasterization of Gaussians is fast and the time taken for graph cut grows roughly linearly with the number of Gaussians. Table 9 shows a detailed breakdown of time taken in each step: preprocessing (obtaining the features from 2D image/video segmentation models), fitting time (3DGS optimization time), and segmentation time (time taken to obtain the segmented output). Compared to feature-based methods, like Gaussian Grouping, LangSplat, and SAGA, our method does not require any alteration to the fitting process and, therefore, has a shorter fitting time. While the segmentation time is higher for GaussianCut, it still has a much shorter overall time requirement. All reported times have been obtained using an NVIDIA RTX 4090 GPU and an Intel Core i9-13900KF CPU.

**Memory requirement:** While 3DGS has a higher footprint than NeRF-based models, several recent works reduce the memory footprint with limited loss of quality . Our method only stores one additional parameter \(w_{g}\) for every Gaussian and is less memory-intensive than methods requiring learning a feature field .

**Limitations:** GaussianCut can address some inaccuracies in 2D video segmentation models, but it may still lead to partial recovery when the initial mask or tracking results are significantly off (Figure 7). By segmenting directly at the level of Gaussians, the resulting segmentation boundaries may inherit inaccuracies of the underlying representation that are not visible in the unsegmented scene due to alpha-blending. While GaussianCut does not require additional scene optimization, our method can still take up to a few minutes for the graph cut component, which makes the segmentation not real-time. The implementation could be improved by applying graph cut on a subset of Gaussians. We leave this to future work. Additionally, extending our energy function to include a feature similarity term (in equation 3) is another potential improvement. We also discuss some failure cases in section B.

## 7 Conclusion

In this paper, we introduce GaussianCut, a novel approach that taps into the underlying explicit representation of 3D Gaussian Splatting to accurately delineate 3D objects. Our approach takes in an optimized 3DGS model along with sparse user inputs on any viewpoint from the scene. We use video segmentation models to propagate the mask along different views and then track the Gaussians that splat to these masks. In order to enhance the precision of partitioning the Gaussians, we model them as nodes in an undirected graph and devise an energy function that can be minimized using graph cut. Our approach shows the utility of explicit representation provided by 3DGS and can also be extended for downstream use cases of 3D editing and scene understanding.

## 8 Acknowledgment

This work was supported by the Sony Research Award Program and NSERC Alliance.

   \#Neighbors & 1 & 10 & 50 & 100 \\  Horns & 91.9 & 93.6 & 93.8 & 94.3 \\ Time (s) & 18 & 57 & 209 & 410 \\  Truck & 93.3 & 95.7 & 95.3 & 95.2 \\ Time (s) & 32 & 96 & 393 & 738 \\   

Table 7: Ablation on the number of neighbors.

   Method & Preprocessing time & Fitting time & Segmentation time & Performance (IoU) \\  SSAG  & 71.17\(\) 22.74 & 1448.50\(\) 205.07 & 0.35 \(\) 0.05 & 90.9 \\ Gaussian Grouping  & 13.72 \(\) 4.63 & 2096.07 \(\) 251.96 & 0.55 \(\) 0.09 & 90.6 \\ LangSplat  & 2000.34 \(\) 1222.19 & 1346.92 \(\) 247.00 & 0.82 \(\) 0.02 & 74.0 \\ Coarse Splitting (Ours) & 6.11 \(\) 0.38 & 510.97 \(\) 106.42 & 19.48 \(\) 4.31 & 91.2 \\ GaussianCut (Ours) & 6.11 \(\) 0.38 & 510.97 \(\) 106.42 & 88.77 \(\) 33.68 & 92.5 \\   

Table 8: Ablation on the number of clusters for high-confidence nodes.

   \#Clusters & 1 & 5 & 10 & 20 \\  Fortress & 97.3 & 97.8 & 97.6 & 97.5 \\ Horns & 93.8 & 93.9 & 94.0 & 94.0 \\ Truck & 95.6 & 95.7 & 95.6 & 95.5 \\   

Table 8: Ablation on the number of clusters for high-confidence nodes.