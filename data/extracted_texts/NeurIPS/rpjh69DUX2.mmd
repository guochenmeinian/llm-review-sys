# Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator

Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator

 Siyuan Xu & Minghui Zhu

School of Electrical Engineering and Computer Science

The Pennsylvania State University

University Park, PA 16801

{spx5032, muz16}@psu.edu

###### Abstract

Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and generalizability. In this paper, we develop a bilevel optimization framework for meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation, which implements multiple-step policy optimization on one-time data collection. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. This metric measures the distance of the policy adaptation from the learned meta-prior to the task-specific optimum, and quantifies the model's generalizability to the task distribution. We empirically validate the correctness of the derived upper bounds and demonstrate the superior effectiveness of the proposed algorithm over benchmarks.

## 1 Introduction

Meta-learning  aims to extract the shared prior knowledge, known as meta-prior, from the similarities and interdependencies of multiple existing learning tasks, in order to accelerate the learning process, increase the efficiency of data usage, and improve the overall learning performance in new tasks. Meta-learning has been extended to solve RL problems, known as meta-RL , and shows its promise to overcome the challenges of traditional RL algorithms, including scarce real-world data , limited computing resources, and slow learning speed .

Meta-learning methods can be generally categorized into optimization-based, model-based (black box methods), and metric-based methods . The optimization-based meta-learning approach  is compatible with any model trained by an optimization algorithm, such as gradient descent, and thus is applicable to a vast range of learning problems, including RL problems. Specifically, it formulates meta-learning as a bilevel optimization problem. At the lower-level optimization, the task-specific model is adapted from a shared meta-parameter by an optimization algorithm. At the upper-level optimization, the meta-parameter is to maximize the meta-objective, i.e., the performance of the model adapted from the meta-parameter over training tasks. The existing methods, including MAML and its variants , take a one-step gradient ascent as the lower-level policy optimization algorithm, which limits its data inefficiency and leads to sub-optimality.

During the meta-test, MAML conducts one-time data collection, i.e., collecting data using one policy (the meta-policy), and adapts the policy by one step of policy gradient to the new task. However, the collected data is only used in one policy gradient step, which may not sufficiently leverage the data and potentially fail to achieve a good performance. To mitigate the issue, a typical practice is to implement the data collection and the policy gradient alternately multiple times . However, the environment exploration is usually costly and time-consuming during the meta-test in applications of meta-RL . As a result, the low data efficiency limits the optimality of task-specific policies. In contrast, in this paper, we collect data by meta-policy for one time and utilize multiple policy optimization steps to improve the data efficiency.

The optimality analysis of MAML is studied in  with a metric of **optimality on the meta-objective**, where the error of the meta-objective is defined by the expectation of the optimality gap between the task-specific policy adapted from the learned meta-parameter and the policy adapted from the best meta-parameter . However, the best meta-parameter is shared for all tasks. Even if the meta-objective error is close to zero, i.e., the learned meta-parameter is close to the best one, the model adapted from the learned meta-parameter might be far from task-specific optimum for some tasks. In contrast, we aim to design a meta-RL algorithm that can fit a stronger optimality metric, called **near-optimality under all-task optimum**, where the comparator, i.e., the policy adapted from the best meta-parameter, is replaced by the task-specific optimal policy for each task. This metric offers a more strict comparator for the model adapted from the learned meta-parameter, i.e., when the metric achieves zero, the policy adaptation produces the optimal policy for every task. A similar metric is studied by . It assumes that the task-specific optimal expert policy for each task is accessible and serves the supervision for policy adaptation during meta-training, which alleviates the analysis difficulty caused by the optimal policy comparator. However, the expert policy supervision is not accessible in a standard meta-RL problem. The metric under all-task optimum is also studied by  in the context of supervised meta-learning.

**Main contribution.** We develop a bilevel optimization framework for meta-RL, which implements multiple-step policy optimization on one-time data collection during task-specific policy adaptation. The overall contributions are summarized as follows. (i) We develop a universal policy optimization algorithm, which performs multiple optimization steps to maximize a surrogate of the accumulated reward function. The surrogate is developed only using one-time data collection. It includes various widely used policy optimization algorithms, including the policy gradient, the natural policy gradient (NPG) , and the proximal policy optimization (PPO)  as the special cases. Then, to learn the mete-prior, we formulate the meta-RL problem as a bilevel optimization problem, where the lower-level optimization is the universal policy optimization algorithm from the meta-policy and the upper-level optimization is to maximize the meta-objective function, i.e., the total reward of the models adapted from the meta-policy. (ii) We derive the implicit differentiation for both unconstrained and constrained lower-level optimization problems to compute the hypergradient, i.e., the gradient of the meta-objective, and propose the meta-training algorithm. In contrast to , we do not require to know the closed-form solution of the lower-level optimization. (iii) We derive upper bounds that quantify (a) the optimality gap between the adapted policy and the optimal task-specific policy for any task, and (b) the expected optimality gap over the task distribution. Since the proposed framework incorporates several existing meta-RL methods, such as MAML, as a special case, the analysis also provides the theoretical motivation for them. (iv) We conduct experiments to validate the theoretical bounds and verify the efficacy of the proposed algorithm on meta-RL benchmarks.

Table 1 compares the solved theoretical challenges of meta-RL between this paper and previous works . Specifically, paper  derives the optimality on the meta-objective under the assumption of bounded hypergradient. Papers  consider the convergence of the meta-objective. The near-optimality under all-task optimum is considered in . However, it assumes the optimal expert policies of the training tasks are available in meta-training, such that it can learn to approach the expert policies, while the other methods do not require the expert policies and learn from the explorations of the environments. In this paper, we show the convergence and optimality guarantee on the meta-objective, and, more importantly, the optimality guarantee under the all-task optimum comparator. It is noted that the optimality on the meta-objective is an immediate result from .

    & Convergence & Optimality of & Near-optimality \\  & of meta-objective & meta-objective & under all-task optimum \\ 
 & ✓ & \(\) & \(\) \\
 & \(\) & ✓ When assuming convergence & \(\) \\
 & \(\) & \(\) & ✓ Under optimal \\  & & & expert policy supervision \\ This paper & ✓ & Immediate result from  & ✓ \\   

Table 1: Solved theoretical challenges of meta-RLRelated works.

**Categorization of meta-RL.** Meta-RL methods can be generally categorized into (i) optimization-based meta-RL, (ii) black-box (also called context-based) meta-RL. Optimization-based meta-RL approaches, such as MAML  and its variants [55; 38], usually include a policy adaptation algorithm and a meta-algorithm. During the meta-training, the meta-algorithm aims to learn a meta-policy, such that the policy adaptation algorithm can achieve good performance starting from the meta-policy. The learned meta-policy parameter is adapted to the new task using the policy adaptation algorithm during the meta-test. Black-box meta-RL [11; 59; 49; 47; 68] aims to learn an end-to-end neural network model. The model has fixed parameters for the policy adaptation during the meta-test, and generates the task-specific policy using the trajectories of the new task takes. In optimization-based meta-RL, the task-specific policy is adapted from a shared meta-policy over the task distribution. The learned meta-knowledge is not specialized for each task, and its meta-test performance on a task depends on a general policy optimization algorithm applied to new data from that task. In contrast, the end-to-end model in black-box meta-RL typically includes specialized knowledge for any task within the task distribution, and uses the new data merely as an indicator to identify the task within the distribution. As a result, the optimality of optimization-based methods is usually worse than black-box methods, especially when the task distribution is heterogeneous and the data scale for adaptation is extremely small. On the other hand, the policy adaptation algorithms in the meta-test of optimization-based methods can generally improve the policy starting from any initial policy, not only the learned meta-policy. Therefore, it is robust to sub-optimal meta-policy and can deal with tasks that are out of the training task distribution [16; 62]. In contrast, due to the specialization of the learned model, black-box methods cannot be generalized outside of the training task distribution. In this paper, we focus on the category of optimization-based meta-RL and compare the proposed algorithm with the existing optimization-based meta-RL approaches in terms of both experimental results and theory.

**Bilevel optimization in meta-RL.** Bilevel optimization has been widely studied empirically [45; 21; 17; 18; 53; 29] and theoretically [20; 22; 29]. It has been applied to many machine learning problems, including meta-learning [35; 48], hyperparameter optimization [45; 17; 18], RL [24; 34], and inverse RL [39; 40; 41]. Since the overall objective function in bilevel optimization is generally non-convex, theoretical analyses of bilevel optimization mainly focus on the algorithm convergence [20; 29; 64], rarely on the optimality. This paper formulates meta-RL as a bilevel optimization problem. The key theoretical contribution of this paper is to derive upper bounds on the near-optimality under all-task optimum, i.e., the expected optimality of the solutions of the lower-level optimization compared with that of the task-specific optimal policies. The near-optimality under all-task optimum is unique to meta-learning and has not been studied in the literature on bilevel optimization.

## 3 Problem statement

**MDP.** A Markov decision process (MDP) \(\{,,,,P,r\}\) is defined by the bounded state space \(\), the discrete or bounded continuous action space \(\), the discount factor \(\), the initial state distribution \(\) over \(\), the transition probability \(P(s^{}|s,a):\), and the reward function \(r:[0,r_{max}]\).

**Policy and value function.** A stochastic policy \(:()\) is a map from states to probability distributions over actions, and \((a|s)\) denotes the probability of selecting action \(a\) in state \(s\). For a policy \(\), the value function is defined as \(V^{}(s)[_{t=0}^{}^{t}r(s_{t},a_{t},s_{t+1})|s_{0}=s,]\). The action-value function is defined as \(Q^{}(s,a)[_{t=0}^{}^{t}r(s_{ t},a_{t},s_{t+1})|s_{0}=s,a_{0}=a,]\). The advantage function is defined as \(A^{}(s,a) Q^{}(s,a)-V^{}(s)\). The accumulated reward function is \(J()_{s}[V^{}(s)]\). Define the discounted state visitation distribution of a policy \(\) as \(^{}(s)_{s_{0}}[(1-)_{t=0}^{ }^{t}(s_{t}=s|)]\). In this paper, we consider parametric policy \(_{}\), parameterized by \(\). The optimal parameter \(^{*}\) can maximize the accumulated reward function, i.e., \(^{*}*{argmax}_{}J(_{})\). If \(^{*}\) is not unique, denote the set of the optimal solutions by \(^{*}\).

**Meta-reinforcement learning.** Meta-RL aims to solve multiple RL tasks. Consider a space of RL tasks \(\), where each task \(\) is modeled by a MDP \(_{}\{,,,_{},P_{ },r_{}\}\). Correspondingly, the notations \(V_{}^{}\), \(Q_{}^{}\), \(A_{}^{}\), \(_{}^{}\), \(_{}^{*}\), \(_{}^{*}\) and \(J_{}\) are defined for task \(\). The RL tasks follow a probability distribution \(()\). Meta-RL aims to learn a meta-policy \(_{}\) parameterized by a meta parameter \(\)such that it can adapt to an unseen task \(_{new}()\) with a few iterations and a small number of new environment explorations. In specific, during the meta-training, several tasks can be i.i.d. sampled from \(()\), i.e., \(\{_{j}\}_{j=1}^{T}()\), and the tasks' MDPs \(\{_{_{j}}\}_{j=1}^{T}\) can be explored. The meta-learner applies a meta-algorithm to update the meta parameter \(\) by using the data collected from the sampled tasks. During the meta-test, a new task \(_{new}\) is given, one time of a within-task algorithm \(lg\) with data collected from \(_{new}\) is applied, the meta-parameter \(\) is adapted to the task-specific parameter \(^{}_{_{new}}\) and the task-specific policy \(_{^{}_{_{new}}}\) is tested on the task \(_{new}\).

Optimality Metric.Consider a meta-RL algorithm that produces a meta-parameter \(\), and the take-specific parameter \(_{^{}_{}}\) is adapted from the meta-parameter \(\) on a task \(\), denoted as \(_{^{}_{}}=lg(_{},)\). We define the task-expected optimality gap (TEOG) as the metric to evaluate the algorithm, i.e., \(_{()}[J_{}(_{^{}_{}} )-J_{}(lg(_{},))]\), where \(^{*}_{}\) is the optimal parameter for task \(\). First, the TEOG considers the expected error over the task distribution \(()\), reflecting the generalizability of the produced meta-parameter. Second, the TEOG adopts the comparator of the optimal task-specific policy \(_{^{*}_{}}\) for any task \(\) (all-task optimum comparator), and evaluates the optimality gap \(J_{}(_{^{*}_{}})-J_{}(lg(_{},))\). In contrast, [60; 14; 26] adopts the comparator of the policy adapted from the optimal meta-parameter \(_{^{*}}\), and evaluates the optimality gap \(J_{}(lg(_{^{*}},))-J_{}(lg(_{ },))\). The latter only considers the optimality on the meta-objective, i.e., how well the trained meta-objective can approach the optimal meta-objective. However, even if the error of the meta-objective is approaching zero, i.e., the learned meta-policy is close to the best candidate, the performance of the model adapted from the optimal meta-policy might still be lacking. This is because policy optimization usually requires thousands of value/policy iterations to converge; when tasks are heterogeneous, even if it starts from the best meta-policy, one time of \(lg\) with one time of value estimate may not be sufficient. In contrast, if our metric is zero, the policy adapted from the meta-parameter to any task is optimal for the task.

Policy distance and task variance.To find the solution for a new task within a few iterations of policy optimization, it is crucial that the meta-policy \(_{}\) can benefit from learning on correlated tasks. Similar to [4; 9; 31], we measure the correlation of tasks in the task distribution \(()\) by its variance, defined by the minimal mean square of the distances among the optimal task-specific policies, i.e., \(ar(())_{}_{^{*}_{ }^{*}_{}}_{()}[D_{}^{ 2}(_{},_{^{*}_{}})]\). Here, \(D_{}(_{},_{^{*}_{}})\) is the distance metric between \(_{}\) and \(_{^{*}_{}}\) on the task \(\) and is defined by \(D_{}(_{},_{^{}})_{s ^{*}_{^{}}}[d^{2}(_{}(|s),_{^{ }}(|s))]}\), where \(d(_{}(|s),_{^{}}(|s))\) is the distance of the policies \(_{}\) and \(_{^{}}\) on the state \(s\).

Note that the distance metrics \(D_{}(,)\) and \(d(,,s)\) can be custom-defined, leading to multiple policy update algorithms, as shown in Section 4. Here, we introduce several examples of \(d(,,s)\) and \(D_{}(,)\), which are commonly used as the distance metrics in RL literature [51; 30; 37]. For policies \(_{}\) and \(_{^{}}\), we apply (i) the KL-divergence of the action probability distribution, i.e., \(d_{1}^{2}(_{},_{^{}},s) D_{}(_{ }(|s)\|_{^{}}(|s))\), which is similar to the definition in ; (ii) The KL-divergence with the other order, i.e., \(d_{2}^{2}(_{},_{^{}},s) D_{}(_{ ^{}}(|s)\|_{}(|s))\); (iii) the Euclidean distance of the parameters, i.e., \(d_{3}^{2}(_{},_{^{}},s)\|-^{ }\|^{2}\). Correspondingly, for \(i=1,2\), and \(3\), we define \(D_{,i}(_{},_{^{}})_{s ^{*}_{}}[d_{1}^{2}(_{},_{^{}},s)]}\). Note that the distance metrics (i)(ii) are not symmetric, i.e., \(D_{}(_{^{}},_{^{}}) D_{}(_{ ^{}},_{^{}})\), and (iii) is symmetric.

In the subsequent sections, we present algorithms based on the generalized distance definitions of \(D_{}(,)\) and \(d(,,s)\). Moreover, we conduct analyses for the introduced distance metrics, from \(D_{,1}\) to \(D_{,3}\), to provide comprehensive insights into their respective performances.

## 4 Meta-Reinforcement Learning Framework

In this section, we develop a meta-RL algorithm by bilevel optimization, where the lower-level optimization is the within-task algorithm that adapts the parameter from the meta-parameter and the upper-level optimization is the meta-algorithm that obtains the meta-parameter. The proposed algorithm has two distinctions compared with existing algorithms. First, it uses one time of a universal policy optimization algorithm as the lower-level within-task algorithm. Second, we derive the hypergradient by the implicit differentiation, where the closed-form solution of the lower-level optimization is not required.

**Within-task algorithm.** Consider the policy optimization from the meta policy as the within-task algorithm \(lg\). Specifically, given the meta-parameter \(\) and a task \(\), the task-specific policy \(_{^{}_{}}=lg(_{},,)\) is defined by \(^{}_{}=*{argmax}_{}_{s^{ _{}}_{},a_{}(|s)}[Q^{_{}}_{}(s,a) ]- D^{2}_{}(_{},_{})\). When the action space \(\) is discretized and the policy is tabular, i.e., the probabilities of actions are independent between different states, the above problem can be solved by \(_{^{}_{}}(|s)=\)

\[lg(_{},,)(|s)=*{argmax}_{_{ }(|s)}_{a}_{}(a|s)Q^{_{ }}_{}(s,a)- d^{2}(_{}(|s),_{}(|s)),\] (1)

for all states \(s\). When the policy is parameterized by an approximation function, in both continuous and discrete action space \(\), \(_{^{}_{}}=lg(_{},,)\) is computed by \(^{}_{}=\)

\[*{argmax}_{}_{s^{_{}}_{},a _{}(|s)}[(a|s)}{_{}(a|s)}Q^{_{ }}_{}(s,a)]- D^{2}_{}(_{},_{} ).\] (2)

In (1) and (2), \(>0\) is a tuning hyperparameter and the distance metric \(D_{}\) can be arbitrarily chosen. Considering the explorations for the task \(\) are limited, \(lg\) only needs to evaluate the \(Q^{_{}}_{}\) by Monte-Carlo sampling on a single policy \(_{}\), where the data sampling complexity is exactly the same as the one-step gradient descent in MAML . Therefore, we denote \(lg\), i.e., collecting data on the meta-policy and solving the optimal solution of (1) and (2) as the one-time policy adaptation. More details about the data sample complexity and the computational complexity of (1) and (2) are clarified in Appendix F. On the other hand, one gradient step is usually not sufficient to identify a good policy. Therefore, \(lg\) is to solve the optimal solution of (1) or (2). As shown in Section 5.4, the objective function of (1) or (2) is an approximation of the true objective function \(J_{}()\).

Note that the objective function in (1) and (2) can reduce to that of multiple widely used policy optimization approaches: (i) PPO in  when \(D_{}=D_{,2}\); (ii) a variant of the PPO , when \(D_{}=D_{,1}\); (iii) the proximally regularized policy update, i.e., the policy optimization regularized by Euclidean distance of the policy parameter , when \(D_{}=D_{,3}\). Moreover, (iv) if we approximate the expectation in (2) by its first-order approximation and also select \(D_{}=D_{,3}\), the within-task algorithm (2) also can be reduced to one-step policy gradient, as shown in Appendix H; (v) if we use the first-order approximation of the expectation in (2), the second-order approximation of the term \(D^{2}_{}(_{},_{})\), and select \(D_{}=D_{,2}\), the within-task algorithm (2) is reduced to the natural policy gradient (NPG).

**Meta-algorithm.** The performance of the meta-parameter \(\) is evaluated by the meta-objective function, which is defined as the expected accumulated reward after the parameter is adapted by the within-task algorithm, i.e., \(_{()}[J_{}(lg(_{}, ,))]\). In the meta-algorithm, we maximize the meta-objective to obtain the optimal meta-parameter \(^{*}\), i.e.,

\[^{*}=*{argmax}_{}_{()}[ J_{}(lg(_{},,))].\] (3)

As (1) and (2) provide multiple choices of the within-task algorithms when selecting different \(D_{}\), the meta-algorithm (3) provides the algorithms to learn the corresponding meta-priors. For example, (3) takes on the role of the meta-PPO algorithm when \(D_{}=D_{,1}\) or \(D_{,2}\), i.e., (3) learns the meta-initialization for PPO. It is a meta-NPG algorithm with the corresponding approximation and \(D_{}\). Moreover, when \(lg(_{},,)\) in (2) reduces to the one-step policy gradient shown in (iv) of the last paragraph, (3) represents a precise formulation of MAML in . More details about the formulation and its relations with MAML are shown in Appendix G and H.

**Hypergradient computation.** Similar to , the meta-algorithm in (3) aims to solve a bilevel optimization problem. In previous works , they apply the policy optimizations that have known closed-form solutions as the lower-level within-task algorithms. As a result, the bilevel optimization problem is reduced to a single-level problem. In contrast, in this paper, as we consider a universal policy optimization, its closed-form solution cannot be obtained. To address the challenge, we compute \(_{}lg(_{},,)\) and the hypergradient by deriving the implicit differentiation on \(lg(_{},,)\). As shown in Section 4, the optimization problem \(lg(_{},,)\) is unconstrained in (2), but is constrained in (1) due to \(_{a}(a|s)=1\). Therefore, we derive the implicit differentiation for both unconstrained and constrained optimization problems. The following proposition shows the hypergradient computation for the tabular policy. Its proof is shown in Appendix J.1.

**Proposition 1** (Hypergradient for the **tabular policy**).: _For the tabular policy in the discrete state-action space, consider any meta-parameter \(\) and the within-task algorithm (1). Let

[MISSING_PAGE_FAIL:6]

introduce the softmax policy and necessary assumptions. In the following three sections, we consider two cases of Algorithm 1, including (i) Algorithm 1 with the within-task algorithm \(lg^{(1)}\) and \(lg^{(2)}\) for the tabular softmax policy; and (ii) Algorithm 1 with the within-task algorithm \(lg^{(3)}\) for the softmax policy with function approximation. For the algorithms in (i) and (ii), we study the existence of hypergradient in Section 5.2, derive the convergence guarantees in Section 5.3, and derive the near-optimality under the all-task optimum, i.e., derive the upper bounds of TEOG, in Section 5.4.

### Softmax policy and assumptions

We apply the softmax policies, which are commonly applied in [66; 37; 60], and use the following assumptions on the task \(\).

**Softmax policies.** Consider the softmax policies \(_{}\) parameterized by \(\) for (i) the tabular policy and (ii) the policy with function approximation. In particular, the tabular policy in a discrete state-action space is defined by \(_{}(|s)((s,))\), where \(^{|S|||}\) is a tabular map. The policy with function approximation is defined by \(_{}(|s)(f_{}(s,))\), where \(f_{}\) is a function approximation model \(\) with the parameter \(^{n}\).

**Assumption 1** (Upper bound of advantage function).: _For any task \(\) and any softmax policy \(_{}\), \(|A_{}^{_{}}(s,a)| A_{max}\) for any \(a\) and any \(s\)._

Since the reward \(r_{} r_{max}\) is bounded, it is easy to show that \(|A_{}^{_{}}(s,a)|}{1-}\) and Assumption 1 always holds. But we still keep Assumption 1 here, since there usually exist \(A_{max}\) such that \(A_{max}}{1-}\). We also have the following assumption and show its remark.

**Assumption 2** (Sufficient state visit).: _For any task \(\), there exists a constant \(>0\), such that for all bounded parameters \(\), \(^{_{}}(s)\) for all \(s\)._

**Remark 1**.: _Here are two sufficient conditions for Assumption 2: (i) For any task \(\), the MDP \(_{}\) is ergodic [43; 56]; or (ii) the initial state distribution \(_{}\) has \(_{}(s)>0\) for any \(s\)._

The proof of Remark 1 is shown in Appendix O. Note that (i) of Remark 1 is a mild condition and is assumed in recent studies on RL algorithm analysis [61; 46].

For the policy with function approximation, we require the following additional assumptions on the approximate function \(f_{}\), which are standard or weaker than those in the analysis of meta-learning and meta-RL problems [9; 12; 13; 14].

**Assumption 3** (Property of the approximate function).: _For any state-action pair \((s,a)\), (i) the approximate function \(f_{}(s,a)\) are cubic differentiable. (ii) \(f_{}(s,a)\) is \(L_{1}\)-Lipschitz, i.e., \(\|f_{_{1}}(s,a)-f_{_{2}}(s,a)\| L_{1}\|_{1}-_{2}\|\) for any \(_{1},_{2}^{n}\). (iii) \(_{}f_{}(s,a)\) is \(L_{2}\)-Lipschitz, i.e., \(\|_{}f_{_{1}}(s,a)-_{}f_{_{2}}(s,a)\|  L_{2}\|_{1}-_{2}\|\) for any \(_{1},_{2}^{n}\), (iv) \(_{}^{2}f_{}(s,a)\) is \(L_{3}\)-Lipschitz, i.e., \(\|_{}^{2}f_{_{1}}(s,a)-_{}^{2}f_{_{2}}(s,a )\| L_{3}\|_{1}-_{2}\|\) for any \(_{1},_{2}^{n}\)._

### Existence of hypergradient.

An essential prerequisite for using Algorithm 1 is that the hypergradients in Propositions 1 and 2 exist. As shown in Section 4, for the tabular policy, when \(i=1\) or \(2\), the hypergradient \(_{}J_{}(lg^{(i)}(_{},,))\) exists for any \(\). For the policy with function approximation, we derive the following sufficient condition of the hypergradient being existent. Its proof is shown in Appendix M.

**Proposition 3** (Existence of hypergradient for the policy with function approximation).: _In both discrete and continuous action space, consider the softmax policy with function approximation shown in Section 5.1. Suppose that Assumptions 1 and 3 hold. If \(>(6L_{1}^{2}+2L_{2})A_{max}\), \(_{}J_{}(lg^{(3)}(_{},,))\) always for any \(\)._

### Convergence guarantee

We begin with the convergence guarantee of Algorithm 1 for the tabular policy. The following notations are used in the theorem: \(B_{i}\), \(C_{i}\), \(G_{i}\), \(K_{i}\), \(M_{i}\) (\(i=1\) and \(2\)), where \(K_{i}+2C_{i}^{2})r_{max}^{2}}{(1-)^{4}}\), \(M_{i}+2C_{i}^{2})G_{i}r_{max}}{(1-)^{4}}\) for \(i=1\) and \(2\). \(B_{1}}{(1-)^{3}}++ \), \(C_{1}\), and \(G_{1}}{(1-)^{2}}\). \(B_{2}}{(1-)^{3}}+}\), \(C_{2}\), and \(G_{2}}{(1-)^{2}}\).

**Theorem 1** (Convergence guarantee for **tabular softmax policy**).: _Consider the tabular softmax policy in the discrete action space. Suppose that Assumptions 1 and 2 hold. Let \(\{_{t}\}_{t=1}^{T}\) be the sequence of meta-parameters generated by Algorithm 1 with \( 2A_{max}\) and the step size \(=\{(B_{i}}{(1-)^{2}}+C_{i}^{2}}{(1-)^{3}})^{-1},}\}.\) Then, the bound: \(_{t=1}^{T}[\|_{}_{()}[J_{}(lg^{(i)}(_{_{t}},,))]\|^{2}] }{T}+}{}\). holds for \(i=1\) or \(2\)._

The first expectation comes from the random sampling in line 2 of Algorithm 1. The proofs of Theorem 1 are shown in Appendices K.2 and L.2.

The following theorem shows the convergence guarantee for the policy with function approximation. The notations are used in the theorem: \(B_{3}\), \(C_{3}\), \(G_{3}\), \(K_{3}\), \(M_{3}\), where \(K_{3}+2C_{3}^{2})r_{max}^{2}}{(1-)^{4}}\), \(M_{3}+2C_{3}^{2})G_{3}r_{max}}{(1-)^{4}}\), \(G_{3}A_{max}(+L_{1}^{2}A _{max})}{(1-)^{4}}\), \(G_{3}A_{max}(+L_{1}^{2}A _{max})}{(1-)(-(6L_{1}^{2}+2L_{2})A_{max})}\), \(C_{3}(+L_{1}^{2}A_{max}) }{(1-)(-(6L_{1}^{2}+2L_{2})A_{max})}\), and \(B_{3}^{2}+56L_{1}L_{2}+4L_{3})(+L_{1}^{2}A_{max})^{2}}{(1-)^{3}(-(6L_{1}^{2}+2L_{2})A_{ max})^{2}}\).

**Theorem 2** (Convergence guarantee for **softmax policy with function approximation**).: _In both discrete and continuous action space, consider the softmax policy with function approximation. Suppose that Assumptions 1, 2, and 3 hold. Let \(\{_{t}\}_{t=1}^{T}\) be the sequence of meta-parameters generated by Algorithm 1 with \(>(6L_{1}^{2}+2L_{2})A_{max}\) and the step size \(=\{(B_{3}}{(1-)^{2}}+C_{3}^{2}}{(1-)^{3}})^{-1},}\}\). Then, the bound \(_{t=1}^{T}[\|_{}_{ ()}[J_{}(lg^{(3)}(_{_{t}},, ))]\|^{2}]}{T}+}{}\). holds._

The first expectation arises from the random sampling in line 2 of Algorithm 1. The proof of Theorem 2 is shown in Appendix M. Theorems 1 and 2 show that the convergence rate of Algorithm 1 is \((})\) and the constants in the notation \(\) are only related to the discount factor \(\), the reward bound \(r_{max}\), the bound of the advantage function \(A_{max}\), and the Lipschitz constants of \(f_{}\).

### Near-optimality under all-task optimum

Before the derivation of the optimality analysis, we first introduce two intermediate Lemmas.

**Lemma 1**.: _Suppose that Assumptions 1, 2 hold. For any task \(\), any bounded parameters \(\) and \(^{}\), and \(i=1\) or \(2\), we have \(J_{}(_{^{}})-J_{}(_{})_{s_{}^{},a^{}(|s)}[^{}(s,a)}{1-}]-}{(1-)^{ }}D_{,i}^{2}(_{},_{^{}})\)._

**Lemma 2**.: _Consider the softmax policy with function approximation shown in Section 5.1. Suppose that Assumptions 1, 2, and 3 hold. For any task \(\), and any softmax policies parameterized by bounded \(\) and \(^{}\), we have \(J_{}(_{^{}})-J_{}(_{})_{s_{}^{},a_{^{}}(|s)} [^{}(s,a)}{1-}]-L_{ 1}^{2}}{(1-)^{}}D_{,3}^{2}(_{},_{ ^{}})\)._

The proofs of Lemmas 1 and 2 are shown in Appendix N.1. Given Lemma 1, when \(=}{(1-)}\), the within-task algorithm \(lg^{(1,2)}(,,)\) in (1) is actually designed to maximize the right-hand side of the inequality, where \(^{}\) is the decision variable. Similarly, Given Lemma 2, when \(=L_{1}^{2}}{(1-)}\), \(lg^{(3)}(_{},,)\) in (2) maximizes the right-hand side of the inequality, where \(_{^{}}\) is the decision variable. In other words, for each \(i=1,2,3\), the within-task algorithm \(lg^{(i)}\) is to maximize a lower bound of \(J_{}(_{})\), denoted as \(_{}(_{})\). This idea, referred to as the minorization-maximization (MM) , is widely used in . The design of \(lg^{(i)}\) enables us to connect the accumulated reward of the policy after the policy adaptation with that of the optimal policy \(_{_{}}\) for task \(\), i.e., \(_{}(lg^{(i)}(_{},,))_{ }(_{^{}})\), which is a key intermediate result for the optimality analysis.

The final preparatory step is that we borrow the analysis of the meta-training error from . In particular, its theoretical result is encapsulated in the following assumption.

**Assumption 4**.: _(Bounding error of meta-objective using gradient) Let \(F^{(i)}()_{()}[J_{}( lg^{(i)}(_{},,))]\). For both the tabular policy and the policy with functional approximation, there exists a concave positive non-decreasing function \(h_{i}:[0,+)[0,+)\), such that \(_{^{}}F^{(i)}(^{})-F^{(i)}() h_{i}(\|_{ }F^{(i)}()\|^{2})\)._Assumption 4 assumes the optimality gap of \(_{}\) on the meta-objective is upper bounded by an increasing function of its gradient. A sufficient condition of Assumption 4 is provided by . Combine the Assumption 4 and the convergence analysis in Theorems 1 and 2, we can bound the error of the meta-objective, i.e., \(_{}F^{(i)}()-F^{(i)}(_{t})\). This result is referred to as the optimality of the meta-objective shown in Table 1. Finally, we derive the upper bounds of the TEOG for both the tabular policy and the policy with function approximation.

**Theorem 3** (Optimality guarantee for **softmax tabular policy**).: _Consider the tabular softmax policy for the discrete state-action space. Suppose that Assumptions 1,2 and 4 hold. Let \(\{_{t}\}_{t=1}^{T}\) be the sequence of meta-parameters generated by Algorithm 1 with \(=}{(1-)}\) and the step size \(\) shown in Theorem 1. Then, the following holds for \(i=1\) or \(2\): \(_{t=1}^{T}_{t}[_{( )}[J_{}(_{^{*}_{}})-J_{}(lg^{(i)} (_{_{t}},,))]] h_{i}(}{T}+ }{})+}{(1-)^{2} }ar_{i}(())\), where \(_{^{*}_{}}\) is the optimal softmax policy for task \(\) and the constants \(K_{i}\) and \(M_{i}\) are shown in Theorem 1._

**Theorem 4** (Optimality guarantee for **softmax policy with function approximation**).: _In both discrete and continuous action space, consider the softmax policy with function approximation. Suppose that Assumptions 1,2, 3 and 4 hold. Let \(\{_{t}\}_{t=1}^{T}\) be the sequence of meta-parameters generated by Algorithm 1 with \(=^{2}+2L_{2})A_{max}}{(1-)}\) and the step size \(\) shown in Theorem 2. The following holds: \(_{t=1}^{T}_{t}[_{( )}[J_{}(_{^{*}_{}})-J_{}(lg^{(3)} (_{_{t}},,))]] h_{3}(}{T}+ }{})+^{2}+2L_{2} A_{max}}{(1- )^{2}}ar_{3}(())\), where \(_{^{*}_{}}\) is the optimal softmax policy for task \(\) and the constants \(K_{3}\) and \(M_{3}\) are the same as Theorem 2._

The proofs of Theorems 3 and 4, as well as the selection of the hyperparameter \(\) in these two theorems, are shown in Appendix N.2. The theorems derive the upper bounds of the TEOGs between the parameter adapted by one-time policy adaptation from the produced meta-parameter \(_{t}\) and the task-specific optimal parameter \(^{*}_{}\). It is shown that, with at most \(T\) iterations, we can achieve the upper bounds in the order of \((h_{i}(})+ar(()))\). In other words, there exists a \(t T\) with \(_{()}[J_{}(lg(_{ _{t}},,))]_{()}[J_{}(_{^{*}_{}})]-(h_{i}(})+ ar(()))\). As the number of iterations \(T\) increases, or the variance of the task distribution \(ar(())\) reduces, the optimality of the meat-parameter \(_{t}\) improves. The second term \(ar(())\) in the upper bounds of Theorems 3 and 4 corresponds the intuition of meta-learning, which is that, if the variance of a task distribution is smaller, the meta-policy learned from the task distribution is more helpful for new tasks in the task distribution, then the performance is better. Moreover, this term shows that the learned meta-policy achieves a better performance than the meta-policy \(^{center}\) defined by \(_{}_{()}[D_{,i}^{2}(_{ },_{^{*}_{}})]\), which is the center of all the task-specific optimal policies \(_{^{*}_{}}\). The order of our upper bounds are comparable to \((T^{-}+ar(())\) that is shown in . On the other hand, compared with , in this paper, the constants in the notation \(\) only consist of \(\), \(r_{max}\), \(A_{max}\), and the Lipschitz constants of \(f\), and do not rely on \(||\) and \(||\). As a result, our upper bounds are tighter when handling high-dimensional problems or continuous spaces.

**Monotonic improvement of the within-task algorithm.** Another benefit from Lemmas 1 and 2 and the idea of MM used by the within-task algorithm is that, the policy update by the within-task algorithm monotonically improves, i.e., \(J_{}(lg^{(i)}(_{},,)) J_{}( _{})\) for \(i=1,2\) and \(3\) and any \(\) and any task \(\). Therefore, multiple times of \(lg\) always perform better than one-time \(lg\).

## 6 Experiments

### Verification of theoretical results

We conduct an experiment to verify the optimality bounds of Algorithm 1 shown in Theorems 3 and 4. We consider two scenarios of the Frozen Lake environment in Gym: two task distributions with a high task variance and a low task variance. More details of the setting and the hyperparameter selection are shown in Appendix A. We consider the within-task algorithm \(lg^{(i)}\) for all \(i=1,2\) and \(3\), where the results of \(i=2\) and \(3\) are shown in Appendix A.

We compare our algorithm with MAML  and the random initialization. Figure 1 shows that, for Algorithm 1 with the within-task algorithm \(lg^{(1)}\), it outperforms the baseline methods. For all scenarios, the expected optimality gap of the one-time policy adaptation is smaller than the upper bounds shown in Theorems 3 and 4, which verify our theoretical analysis. Moreover, in Figure 1, the expected optimality gap of the policy adaptation is better (smaller) but close to the upper bound, while that of the other policy adaptation approach, the policy gradient, is worse (larger) than the upper bound. It shows that the derived upper bound is tight.

### High-dimensional Experiment

To evaluate the proposed practical algorithm, Algorithm 2 in Appendix D, we conduct experiments on high-dimensional locomotion settings in the MuJoCo simulator, including Half-Cheetah with goal directions and goal velocities, Ant with goal directions and goal velocities. We compare the proposed algorithm with several optimization-based meta-RL algorithms, including MAML, E-MAML , and ProMP . For the fairness of the comparison, all the methods share the same data requirement and task setting. More details of the task setting, the hyperparameter selection, and the supplemental results are shown in Appendix B.

Figure 2 shows that the proposed algorithm with the within-task algorithms \(lg^{(i)}\) outperforms the baseline methods in all four experimental settings. For example, we achieve about \(25\%\) of performance improvement in Half-cheetah direction and Ant direction experiments. Moreover, compared with the baseline methods, the proposed algorithm achieves more policy improvement when more policy optimization steps are given. For example, our approach achieves about \(10\%\) of performance improvement in the second policy optimization step, while those of baseline methods are almost \(0\%\).

## 7 Conclusion

This paper develops a bilevel optimization framework for meta-RL, which implements multiple-step policy optimization on one-time data collection during task-specific policy adaptation. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. Our experiments validate the bounds derived from our theoretical analysis and show the superior effectiveness of the proposed framework.

Figure 1: Results of the meta-test on Frozen Lake, where \(lg^{(1)}\) is applied. **Left**: Average accumulated reward across all test tasks v.s. number of policy adaptation steps; **Right**: Comparing the expected optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time \(lg^{(1)}\).

Figure 2: Average accumulated reward across all test tasks during the meta-test under the practical algorithm of BO-MRL on the locomotion tasks.