# First-Order Methods for

Linearly Constrained Bilevel Optimization

 Guy Kornowski

Full version: https://arxiv.org/abs/2406.12771. GK, SP, KW, ZZ contributed equally; authors ordered alphabetically.

Swati Padmanabhan

Weizmann Institute of Science. guy.kornowski@weizmann.ac.il

Kai Wang

Massachusetts Institute of Technology. pswt@mit.edu

Jimmy Zhang

Georgia Institute of Technology. kwang692@gatech.edu

Suvrit Sra

Purdue University. zhan5111@purdue.edu

###### Abstract

Algorithms for bilevel optimization often encounter Hessian computations, which are prohibitive in high dimensions. While recent works offer first-order methods for unconstrained bilevel problems, the _constrained_ setting remains relatively underexplored. We present first-order linearly constrained optimization methods with finite-time hypergradient stationarity guarantees. For linear _equality_ constraints, we attain \(\)-stationarity in \((^{-2})\) gradient oracle calls, which is nearly-optimal. For linear _inequality_ constraints, we attain \((,)\)-Goldstein stationarity in \((d^{-1}^{-3})\) gradient oracle calls, where \(d\) is the upper-level dimension. Finally, we obtain for the linear inequality setting dimension-free rates of \((^{-1}^{-4})\) oracle complexity under the additional assumption of oracle access to the optimal dual variable. Along the way, we develop new nonsmooth nonconvex optimization methods with inexact oracles. Our numerical experiments verify these guarantees.

## 1 Introduction

Bilevel optimization [1; 2; 3; 4], an important problem in optimization, is defined as follows:

\[_{x X}\;F(x) f(x,y^{*}(x))\;y^{*}(x)_{y S(x)}g(x,y).\] (1.1)

Here, the value of the upper-level problem at any point \(x\) depends on the solution of the lower-level problem. This framework has recently found numerous applications in meta-learning [5; 6; 7; 8], hyperparameter optimization [9; 10; 11], and reinforcement learning [12; 13; 14; 15]. Its growing importance has spurred increasing efforts towards designing computationally efficient algorithms for it.

As demonstrated by , a key computational step in algorithms for bilevel optimization is estimating \(dy^{*}(x)/dx\), the gradient of the lower-level solution. This gradient estimation problem has been extensively studied in differentiable optimization [17; 18] by applying the implicit function theorem to the KKT system of the given problem [19; 20; 21; 22; 23; 24]. However, this technique typically entails computing (or estimating) second-order derivatives, which can be prohibitive in high dimensions [25; 26; 27].

Recently,  made a big leap forward towards addressing this computational bottleneck. Restricting themselves to the class of unconstrained bilevel optimization, they proposed a fully first-order method with finite-time stationarity guarantees. While a remarkable breakthrough,  does not directly extend to the important setting of _constrained_ bilevel optimization. This motivates the question:

_Can we develop a first-order algorithm for constrained bilevel optimization?_Besides being natural from the viewpoint of complexity theory, this question is well-grounded in applications such as mechanism design [29; 30], resource allocation [31; 32; 33; 34], and decision-making under uncertainty [20; 35; 36]. Our primary contribution is _an affirmative answer to the highlighted question for bilevel programs with linear constraints_, an important problem class often arising in adversarial training, decentralized meta learning, and sensor networks (see ). While there have been some other recent works [38; 39; 37] on this problem, our work is first-order (as opposed to ) and offers, in our view, a stronger guarantee on stationarity (compared to [38; 39])-- cf. Section 1.2.

### Our contributions

We provide first-order algorithms (with associated finite-time convergence guarantees) for linearly constrained bilevel programs (Problem 1.1). By "first-order", we mean that we use only zeroth and first-order oracle access to \(f\) and \(g\). Our assumptions for each of our contributions are in Section 2.1.

**(1)**: **Linear equality constraints.** As our first contribution, we design first-order algorithms for solving Problem 1.1 where the lower-level constraint set \(S(x):=\{y:Ax-By-b=0\}\) comprises linear equality constraints, and \(\) a convex compact set. With appropriate regularity assumptions on \(f\) and \(g\), we show in this case smoothness in \(x\) of the hyperobjective \(F\). Inspired by ideas from Kwon et al. , we use implicit differentiation of the KKT matrix of a slightly perturbed version of the lower-level problem to design a first-order approximation to \( F\). Constructing our first-order approximation entails solving a strongly convex optimization problem on affine constraints, which can be done efficiently. With this inexact gradient oracle in hand, we then run projected gradient descent, which converges in \((^{-2})\) iterations for smooth functions.

**Theorem 1.1** (Informal; cf. Theorem 3.1).: _Given Problem 1.1 with linear equality constraints \(S(x)\{y:Ax-By-b=0\}\) and \(\) a convex compact set, under regularity assumptions on \(f\) and \(g\) (Assumptions 2.2 and 2.3), there exists an algorithm, which in \((^{-2})\) oracle calls to \(f\) and \(g\), converges to an \(\)-stationary point of \(F\)._

For linear equality constrained bilevel optimization, this is the first first-order result attaining \(\)-stationarity of \(F\) with assumptions solely on the constituent functions \(f\) and \(g\) and none on \(F\) -- cf. Section 1.2 for a discussion of the results of Khanduri et al.  for this setting.

**(2)**: **Linear inequality constraints.** Next, we provide first-order algorithms for solving Problem 1.1 where the lower-level constraint set \(S(x):=\{y:Ax-By-b 0\}\) comprises linear inequality constraints, and the upper-level variable is unconstrained.

Our measure of convergence of algorithms in this case is that of \((,)\)-stationarity : for a Lipschitz function, we say that a point \(x\) is \((,)\)-stationary if within a \(\)-ball around \(x\) there exists a convex combination of subgradients of the function with norm at most \(\) (cf. Definition 2.1).

To motivate this notion of convergence, we note that the hyperobjective \(F\) (in Problem 1.1) as a function of \(x\) could be nonsmooth and nonconvex (and Lipschitz, as we later prove). Minimizing such a function in general is known to be intractable , necessitating local notions of stationarity. Indeed, not only is it impossible to attain \(\)-stationarity in finite time , even getting _near_ an approximate stationary point of an arbitrary Lipschitz function is impossible unless the number of queries is exponential in the dimension . Consequently, for this function class, \((,)\)-stationarity has recently emerged  to be a natural and algorithmically tractable notion of stationarity. We give the following guarantee under regularity assumptions on \(f\) and \(g\).

**Theorem 1.2** (Informal; Theorem 4.1).: _Consider Problem 1.1 with linear inequality constraints \(S(x)\{y:Ax-By-b 0\}\). Under mild assumptions on \(f\) and \(g\) (Assumption 2.2) and the lower-level primal solution \(y^{*}\) (Assumption 2.4), there exists an algorithm, which converges to a \((,)\)-stationary point of \(F\) in \((d^{-1}^{-3})\) oracle calls to \(f\) and \(g\), where \(d\) is the upper-level variable dimension._

To the best of our knowledge, this is the first result to offer a first-order finite-time stationarity guarantee on the hyperobjective for linear inequality constrained bilevel optimization (cf. Section 1.2 for a discussion of related work [38; 37; 39]). We obtain our guarantee in Theorem 1.2 by first invoking a result by Zhang and Lan  to obtain inexact hyperobjective values of \(F\) using only \((1)\) oracle calls to \(f\) and \(g\). We also show (Lemma 4.3) that this hyperobjective \(F\) is Lipschitz. We then employ our inexact zeroth-order oracle for \(F\) in Algorithm 2 designed to minimize Lipschitz nonsmooth nonconvex functions (in particular, \(F\)), with the following convergence guarantee.

**Theorem 1.3** (see Theorem C.1).: _Given \(L\)-Lipschitz \(F:^{d}\) and \(|()-F()|\), there exists an algorithm, which, in \((d^{-1}^{-3})\) calls to \(()\), outputs \(x^{}\) with \([(0,_{}F(x^{}))] 2\)._

While such algorithms using _exact_ zeroth-order access already exist , extending them to the inexact gradient setting is non-trivial; we leverage recent ideas connecting online learning to nonsmooth nonconvex optimization by Cutkosky, Mehta, and Orabona  (cf. Section 4).

**(3) Linear inequality under assumptions on dual variable access.** For the inequality setting (i.e., Problem 1.1 with the lower-level constraint set \(S(x):=\{y:Ax-By-b 0\}\)), we obtain dimension-free rates under an additional assumption (Assumption 2.5) on oracle access to the optimal dual variable \(^{*}\) of the lower-level problem. We are not aware of a method to obtain this dual variable in a first-order fashion (though in practice, highly accurate approximations to \(^{*}\) are readily available), hence the need for imposing this assumption. We believe that removing this assumption and obtaining dimension-free first-order rates in this setting would be an important direction for future work. Our guarantee for this setting is summarized below.

**Theorem 1.4** (Informal; Theorem 4.4 combined with Theorem 5.3).: _Consider Problem 1.1 with linear inequality constraints \(S(x):=\{y:Ax-By-b 0\}\) and unconstrained upper-level variable. Under mild regularity assumptions on \(f\) and \(g\) (Assumption 2.2), on \(y^{*}\) (Assumption 2.4), and assuming oracle access to the optimal dual variable \(^{*}\) (Assumption 2.5), there exists an algorithm, which in \((^{-1}^{-4})\) oracle calls to \(f\) and \(g\) converges to a \((,)\)-stationary point for \(F\)._

We obtain this result by first reformulating Problem 4.1 via the penalty method and constructing an inexact gradient oracle for the hyperobjective \(F\) (cf. Section 5). We then employ this inexact gradient oracle within an algorithm (Algorithm 3) designed to minimize Lipschitz nonsmooth nonconvex functions (in particular, \(F\)), with the following convergence guarantee.

**Theorem 1.5** (Informal; Theorem 4.4).: _Given Lipschitz \(F:^{d}\) and \(\|F()- F()\|\), there exists an algorithm that, in \(T=O(^{-1}^{-3})\) calls to \(F\), outputs a \((,2)\)-stationary point of \(F\)._

Our Algorithm 3 is essentially a "first-order" version of Algorithm 2. Similar to Algorithm 2, despite the existence of algorithms with these guarantees with access to exact gradients , their extensions to the _inexact_ gradient setting are not trivial and also make use of the new framework of Cutkosky, Mehta, and Orabona . We believe our analysis for this general task can be of independent interest to the broader optimization community. Lastly, we also use a more implementation-friendly variant of Algorithm 3 (with slightly worse theoretical guarantees) in numerical experiments.

### Related work

The vast body of work on asymptotic results for bilevel programming, starting with classical works such as Anandalingam and White , Ishizuka and Aiyoshi , White and Anandalingam , Vicente, Savard, and Judice , Zhu , and Ye and Zhu , typically fall into two categories: those based on approximate implicit differentiation: Amos and Kolter , Agrawal et al. , Domke , Pedregosa , Gould et al. , Liao et al. , Grazzi et al. , and Lorraine, Vicol, and Duvenaud  and those via iterative differentiation: Franceschi et al. , Shaban et al. , Domke , Grazzi et al. , Maclaurin, Duvenaud, and Adams , and Franceschi et al. . Another recent line of work in this category includes Khanduri et al. , Liu et al. , Ye et al. , and Gao et al. , which use various smoothing techniques.

The first non-asymptotic result for bilevel programming was provided by Ghadimi and Wang , which was followed by a flurry of work: for example, algorithms that are single-loop stochastic: Chen, Sun, and Yin , Chen et al. , and Hong et al. , projection-free: Akhtar et al. , Jiang et al. , Abolfazli et al. , and Cao et al. , use variance-reduction and momentum: Khanduri et al. , Guo et al. , Yang, Ji, and Liang , and Dagreou et al. , those for single-variable bilevel programs: Jiang et al. , Sabach and Shtern , Amini and Yousefian , and Merchav and Sabach , and for bilevel programs with special constraints: Khanduri et al. , Abolfazli et al. , Tsaknakis, Khanduri, and Hong , and Xu and Zhu .

The most direct predecessors of our work are those by Khanduri et al. , Yao et al. , Lu and Mei , Kwon et al. , and Liu et al. . As alluded to earlier, Liu et al.  recently made a significant contribution by providing for bilevel programming a fully first-order algorithm with finite-time stationarity guarantees. This was extended to the stochastic setting by Kwon et al. (which we build upon), simplified and improved by Chen, Ma, and Zhang , and extended to the constrained setting by Khanduri et al. , Yao et al. , and Lu and Mei .

The works of Yao et al.  and Lu and Mei  study the more general problem of bilevel programming with general convex constraints. However, they use KKT stationarity as a proxy to the hypergradient stationarity. Our Theorem 1.4 is restricted to linear inequality constraints, we provide stationarity guarantees directly in terms of the objective of interest. Moreover, Yao et al.  assumes joint convexity of the lower-level constraints in upper and lower variables to allow for efficient projections, while we require convexity only in the lower-level variable.

The current best result for the linearly constrained setting is that of Khanduri et al. . However, this work requires Hessian computations (and is therefore not fully first-order). Moreover, Khanduri et al.  imposes strong regularity assumptions on the hyperobjective \(F\), which are, in general, impossible to verify. In contrast, Theorem 1.1 imposes _assumptions solely on the constituent functions \(f\) and \(g\)_, none directly on \(F\), thus making substantial progress on these two fronts.

## 2 Preliminaries

We follow standard notation (see Appendix A), with only the following crucial definition stated here.

**Definition 2.1**.: _Consider a locally Lipschitz function \(f:^{d}\), a point \(x^{d}\), and a parameter \(>0\). The Goldstein subdifferential  of \(f\) at \(x\) is the set \(_{}f(x):=(_{y_{}(x)}  f(y)),\) where \( f(x)=\{_{n} f(x_{n}):x_{ n} x,\ x_{n}( f)\}\) is the Clarke subdifferential  of \(f\) and \(_{}(x)\) denotes the Euclidean ball of radius \(\) around \(x\). A point \(x\) is called \((,)\)-stationary if \((0,_{}f(x))\), where \((x,S):=_{y S}\|x-y\|\)._

### Assumptions

We consider Problem 1.1 with linear equality constraints (Section 3) under Assumptions 2.2 and 2.3 and linear inequality constraints (Sections 4 and 5) under Assumptions 2.2, 2.4 and 2.5. We assume the upper-level (UL) variable \(x^{d_{x}}\), lower-level (LL) variable \(y^{d_{y}}\), and \(A^{d_{h} d_{x}}\).

**Assumption 2.2**.: _For Problem 1.1, we assume the following for both settings we study:_

1. _Upper-level: The objective_ \(f\) _is_ \(C_{f}\)_-smooth and_ \(L_{f}\)_-Lipschitz continuous in_ \((x,y)\)_._
2. _Lower-level: The objective_ \(g\) _is_ \(C_{g}\)_-smooth. Fixing any_ \(x\)_,_ \(g(x,)\) _is_ \(_{g}\)_-strongly convex._
3. _We assume that the linear independence constraint qualification (LICQ) condition holds for the LL problem at every_ \(x\) _and_ \(y\)_, i.e., the constraint_ \(h(x,y) Ax-By-b\) _has a full row rank_ \(B\)_._

**Assumption 2.3**.: _For Problem 3.1 (with linear equality constraints), we additionally assume that the set \(\) is convex and compact, and that the objective \(g\) is \(S_{g}\)-Hessian smooth, that is, \(\|^{2}g(x,y)-^{2}g(,)\| S_{g}\| (x,y)-(,)\|, x,\), and \(y,^{d_{y}}\)._

**Assumption 2.4**.: _For Problem 4.1 (with linear inequality constraints), we additionally assume that \(y^{*}\) is \(L_{y}\)-Lipschitz in \(x\), where \(y^{*}\) is the LL primal solution \(y^{*}(x),^{*}(x)=_{y}_{ 0}g(x,y)+ ^{}h(x,y)\), where \(h(x,y) Ax-By-b\)._

**Assumption 2.5**.: _We provide additional results for Problem 4.1 under additional stronger assumptions stated here: Denote the LL primal and dual solution \(y^{*}(x),^{*}(x)=_{y}_{ 0}g(x,y)+ ^{}h(x,y)\), where \(h(x,y) Ax-By-b\); then, we assume exact access to \(^{*}\) and that \(\|^{*}(x)\| R\)._

Assumptions 2.2(i) and 2.2(ii) are standard in bilevel optimization. Assumption 2.2(iii) is the same as the complete recourse assumption in stochastic programming , that is, the LL problem is feasible \(y\) for every \(x^{d_{x}}\). Assumption 2.3 is used only in the equality case and guarantees smoothness of \(F\). Assumption 2.4 is used in the inequality case and implies Lipschitzness of \(F\). We need the stronger assumption in Assumption 2.5 for our dimension-free result for the linear inequality case.

## 3 Lower-level problem with linear equality constraint

We first obtain improved results for the setting of bilevel programs with linear _equality_ constraints in the lower-level problem. Our formal problem statement is:

\[_{x}\ F(x) f(x,y^{*}(x))\ \ \ \ y^{*}(x)_{y:h(x,y)=0}g(x,y),\] (3.1)where \(f\), \(g\), \(h(x,y) Ax-By-b\), and \(\) satisfy Assumptions2.2 and 2.3. The previous best result on Problem3.1 providing finite-time \(\)-stationarity guarantees, by Khanduri et al. , required certain regularity assumptions on \(F\) as well as Hessian computations. In contrast, our finite-time guarantees _require assumptions only on \(f\) and \(g\), not on \(F\)_; indeed, in our work, these desirable properties of \(F\) are naturally implied by our analysis. Specifically, our key insight is that the hypergradient

\[ F(x):=_{x}f(x,y^{*})+((x)}{dx})^{} _{y}f(x,y^{*}) is Lipschitz-continuous and admits an easily computable -- yet highly accurate -- finite-difference approximation. Therefore, \(O(^{-2})\) iterations of gradient descent on \(F\) with this finite-difference gradient proxy yield an \(\)-stationary point.

Specifically, for any fixed \(x\), our proposed finite-difference gradient proxy approximating the non-trivial-to-compute component \(((x)}{dx})^{}_{y}f(x,y^{*})\) of the hypergradient is given by

\[v_{x}:=[g(x,y^{*}_{})+^{*}_{},h(x, y^{*})]-_{x}[g(x,y^{*})+^{*},h(x,y^{*})]}{ },\] (3.2)

where \((y^{*}_{},^{*}_{})\) are the primal and dual solutions to the perturbed lower-level problem:

\[y^{*}_{}_{y:h(x,y)=0}\ g(x,y)+ f(x,y).\] (3.3)

We show in Lemma3.2 that \(v\) in (3.2) approximates \(((x)}{dx})^{}_{y}f(x,y^{*})\) up to an \(O()\)-additive error, implying the gradient oracle construction outlined in the pseudocode presented in Algorithm1. Our full implementable algorithm for solving Problem3.1 is displayed in Algorithm5.

```
1:Input: Current \(x\), accuracy \(\), perturbation \(=^{2}\).
2:Compute \(y^{*}\) (as in Problem3.1) and corresponding optimal dual \(^{*}\) (as in (B.1))
3:Compute \(y^{*}_{}\) (as in (3.3)) and and corresponding optimal dual \(^{*}_{}\) (as in (B.7))
4:Compute \(v_{x}\) as in (3.2)
5:Output:\(F=v_{x}+_{x}f(x,y^{*})\) ```

**Algorithm 1** Inexact Gradient Oracle for Bilevel Program with Linear Equality Constraint

Notice that the finite-difference term in (3.2) avoids differentiating through the implicit function \(y^{*}\). Instead, all we need to evaluate it are the values of \((y^{*},^{*},y^{*}_{},^{*}_{})\) (and gradients of \(g\) and \(h\)). Since \((y^{*},^{*})\) are solutions to a smooth strongly convex linearly constrained problem, they can be approximated at a linear rate. Similarly, since the approximation error in (3.2) is proportional to \(\) (cf. Lemma3.2), a small enough \(\) in the perturbed objective \(g+ f\) in (3.3) ensures that it is dominated by the strongly convex and smooth \(g\), whereby accurate approximates to \((y^{*}_{},^{*}_{})\) can also be readily obtained. Putting it all together, the proposed finite-difference hypergradient proxy in (3.2) is efficiently computable, yielding the following guarantee.

**Theorem 3.1**.: _Consider Problem3.1 under Assumption2.2, and let \(=C_{g}/_{g}\) be the condition number of \(g\). Then Algorithm5 finds an \(\)-stationary point (in terms of gradient mapping, see (B.14)) after \(T=(C_{F}(F(x_{0})- F)^{-2})\) oracle calls to \(f\) and \(g\), where \(C_{F}:=2(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2}\) is the smoothness constant of the hyperobjective \(F\)._

We now sketch the proof of Theorem3.1. The complete proofs may be found in AppendixB.

### Main technical ideas

We briefly outline the two key technical building blocks alluded to above, that together give us Theorem3.1: the approximation guarantee of our finite-difference gradient proxy ((3.2)) and the smoothness of hyperobjective \(F\) (for Problem3.1). The starting point for both these results is the following simple observation obtained by implicitly differentiating, with respect to \(x\), the KKT system associated with \(y^{*}=_{h(x,y)=0}g(x,y)\) and optimal dual variable \(^{*}\):

\[(x)}{d^{d}_{x}(x)}=_{ yy}^{2}g(x,y^{*})&_{y}h(x,y^{*})^{}\\ _{y}h(x,y^{*})&0^{-1}-_{yx}^{2}g(x,y^ {*})\\ -_{x}h(x,y^{*})\] (3.4)The invertibility of the matrix in the preceding equation is proved in Corollary B.3: essentially, this invertibility is implied by strong convexity of \(g\) and \(_{y}h(x,y^{*})=B\) having full row rank. This in conjunction with the compactness of \(\) implies that the inverse of the matrix is bounded by some constant \(C_{H}\) (cf. Corollary B.3 for details). Our hypergradient approximation guarantee follows:

**Lemma 3.2**.: _For Problem 3.1 under Assumption 2.2, with \(v_{x}\) as in (3.2), the following holds:_

\[\|v_{x}-((x)}{dx})^{}_{y}f(x,y^{*}) \| O(C_{F}).\]

Proof sketch; see Appendix B.: The main idea is that the two terms being compared are essentially the same by the implicit function theorem. First, we use the expression for \((x)}{dx}\) from (3.4):

\[((x)}{dx})^{}_{y}f(x,y^{*})=- _{yx}^{2}g(x,y^{*})\\ -_{x}h(x,y^{*})^{}_{yy}^{2}g(x,y^ {*})&_{y}h(x,y^{*})^{}\\ _{y}h(x,y^{*})&0^{-1}_{y}f(x,y^{*})\\ 0.\]

We now examine \(v_{x}\). For simplicity of exposition, we instead consider \(_{ 0}[g(x,y,y_{}^{*})+(_{}^{*},h(x,y ^{*}))]-_{x}[g(x,y^{*})+(^{*},h(x,y^{*}))]}{},\) which, by the fundamental theorem of calculus and Assumption 2.3, equals \(v_{x}\) up to an \(O()\)-additive error. Note that this expression is:

\[_{xy}^{2}g(x,y^{*})^{*}}{d}+_{x}h(x,y^{*})^ {}^{*}}{d}.\] (3.5)

Since \(y_{}^{*}\) is minimizes a strongly convex function over a linear equality constraint (3.3), the reasoning that yields (3.4) also gives the following, which, when combined with (3.5), finishes the proof:

\[^{*}(x)}{d}_{ =0}=_{yy}^{2}g(x,y^{*})&_{y}h(x,y^{*})^{ }\\ _{y}h(x,y^{*})&0^{-1}-_{y}f(x,y^{*})\\ 0.\] (3.6)

Having shown the construction of the hypergradient approximation, we now state smoothness of the hyperobjective \(F\) (proof in Appendix B), crucial to getting our claimed rate.

**Lemma 3.3**.: _The solution \(y^{*}\) (as defined in Problem 3.1) is \(O(C_{H}(C_{g}+\|A\|))\)-Lipschitz continuous and \(O(C_{H}^{3} S_{g}(C_{g}+\|A\|)^{2})\)-smooth as a function of \(x\). Thus the hyper-objective \(F\) is gradient-Lipschitz with a smoothness constant of \(C_{F}:=O\{(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2}\}\)._

## 4 Nonsmooth nonconvex optimization with inexact oracle

We now shift gears from the case of linear _equality_ constraints to that of linear _inequality_ constraints. Specifically, defining \(h(x,y)=Ax-By-b\), the problem we now consider is

\[_{x}\;\;F(x) f(x,y^{*}(x))y^{*}(x)_{y:h(x,y) 0}g(x,y).\] (4.1)

As noted earlier, for this larger problem class, the hyperobjective \(F\) can be nonsmooth nonconvex, necessitating our measure of convergence to be the now popular notion of Goldstein stationarity .

Our first algorithm for solving Problem 4.1 is presented in Algorithm 2, with its convergence guarantee in Theorem 4.1. At a high level, this algorithm first obtains access to an inexact zeroth-order oracle to \(F\) (we shortly explain how this is done) and uses this oracle to construct a (biased) gradient estimate of \(F\). It then uses this gradient estimate to update the iterates with a rule motivated by recent works reducing nonconvex optimization to online optimization . We explain this in Section 4.1.

**Theorem 4.1**.: _Consider Problem 4.1 under Assumptions 2.2 and 2.4. Let \(=C_{g}/_{g}\) be the condition number of \(g\). Then combining the procedure for Lemma 4.2 with Algorithm 2 run with \(=\{,)- F}{L_{f}L_{y}}\}, =-,\;D=(^{2}}{d_{x}^{2}L_ {f}^{2}L_{y}^{2}+^{2}d_{x}^{2}})\), and \(=(^{4}}{(d_{x}^{2}L_{f}^{2}L_{y}^{ 2}+^{2}d_{x}^{2})^{2}})\) outputs \(x^{}\) such that \([(0,_{}F(x^{}))]+\) with \(T\) oracle calls to \(f\) and \(g\), where:_

\[T=O(d_{x}(F(x_{0})- F)}{^{3}} (L_{f}^{2}L_{y}^{2}+^{2}(}{^{2}}+L_ {f}^{2}L_{y}^{2}}{(F(x_{0})- F)^{2}}))(L_{f}/) ).\]Algorithm 2 is a variant of gradient descent with momentum and clipping, with \(_{t}\) the inexact gradient, \(_{t}\) a clipped accumulated gradient (hence accounts for past gradients, which serve as a momentum), and the clipping ensuring that consecutive iterates of the algorithm reside within a \(\)-ball of each other. While similar algorithms have appeared in prior work on nonsmooth nonconvex optimization (e.g. ), none of them account for inexactness in the gradient, crucial in our setting.

### Nonsmooth nonconvex optimization with inexact zeroth-order oracle

We can obtain inexact zeroth-order oracle access to \(F\) because (as formalized in Lemma 4.2) despite potential nonsmoothness and nonconvexity of \(F\) in Problem 4.1, estimating its _value_\(F(x)\) at any point \(x\) amounts to solving a single smooth and strongly convex optimization problem, which can be done can be done in \((1)\) oracle calls to \(f\) and \(g\) by appealing to a result by Zhang and Lan .

**Lemma 4.2** (Proof in Appendix C.1).: _Given any \(x\), we can return \((x)\) such that \(|F(x)-(x)|\) using \(O(/_{g}}(L_{f}/))\) first-order oracle calls to \(f\) and \(g\)._

Having computed an inexact value of the hyperobjective \(F\), we now show how to use it to develop an algorithm for solving Problem 4.1. To this end, we first note that \(F\), despite being possibly nonsmooth and nonconvex, is Lipschitz and hence amenable to the use of recent algorithmic developments in nonsmooth nonconvex optimization pertaining to Goldstein stationarity.

**Lemma 4.3**.: _Under Assumption 2.2 and 2.5, \(F\) in Problem 4.1 is \(O(L_{f}L_{y})\)-Lipschitz in \(x\)._

With this guarantee on the Lipschitzness of \(F\), we prove Theorem C.1 for attaining Goldstein stationarity using the inexact zeroth-order oracle of a Lipschitz function. Our proof of Theorem C.1 crucially uses the recent online-to-nonconvex framework of Cutkosky, Mehta, and Orabona . Combining Lemma 4.2 and Theorem C.1 then immediately implies Theorem 4.1.

### Nonsmooth nonconvex optimization with inexact _gradient_ oracle

In Section 5, we provide a way to generate approximate gradients of \(F\). Here, we present an algorithm that attains Goldstein stationarity of Problem 4.1 using this inexact gradient oracle. While there has been a long line of recent work on algorithms for nonsmooth nonconvex optimization with convergence to Goldstein stationarity , these results necessarily require _exact_ gradients. This britteness to any error in gradients renders them ineffective in our setting, where our computed (hyper)gradient necessarily suffers from an additive error. While inexact oracles are known to be effective for smooth or convex objectives , utilizing inexact gradients in the nonsmooth nonconvex regime presents a nontrivial challenge. Indeed, without any local bound on gradient variation due to smoothness, or convexity that ensures that gradients are everywhere correlated with the direction pointing at the minimum, it is not clear a priori how to control the accumulating price of inexactness throughout the run of an algorithm. To derive such results, we use the recently proposed connection between online learning and nonsmooth nonconvex optimization by Cutkosky, Mehta,and Orabona . By controlling the accumulated error suffered by online gradient descent for _linear_ losses (cf. Lemma C.3), we derive guarantees for our setting of interest, providing Lipschitz optimization algorithms that converge to Goldstein stationary points even with inexact gradients.

This algorithm matches the best known complexity in first-order nonsmooth nonconvex optimization , merely replacing the convergence to a \((,)\)-stationary point by \((,+)\)-stationarity, where \(\) is the inexactness of the gradient oracle.

```
1:Input: Initialization \(x_{0}^{d}\), clipping parameter \(D>0\), step size \(>0\), accuracy parameter \(>0\), iteration budget \(T\), inexact gradient oracle \(F:^{d}^{d}\).
2:Initialize:\(_{1}=\)
3:for\(t=1,,T\)do
4: Sample \(s_{t}\)
5:\(x_{t}=x_{t-1}+_{t}\), \(z_{t}=x_{t-1}+s_{t}_{t}\)
6:\(_{t}=F(z_{t})\)
7:\(_{t+1}=_{D}(_{t}-_{t}) _{D}(z):=\{1,\} z\)
8:\(M=\), \(K=\)
9:for\(k=1,,K\)do
10:\(_{k}=_{m=1}^{M}z_{(k-1)M+m}\)
11: Sample \(x^{}\{_{1},,_{K}\}\)
12:Output:\(x^{}\). ```

**Algorithm 3** Nonsmooth Nonconvex Algorithm with Inexact Gradient Oracle

**Theorem 4.4**.: _Suppose \(F:^{d}\) is \(L\)-Lipschitz and that \(\|F()- F()\|\). Then running Algorithm 3 with \(D=(}{L^{2}})\), \(=(}{L^{4}})\), outputs a point \(x^{}\) such that \([(0,_{}F(x^{}))]+\), with \(T=O()- F)L^{2}}{^{3}})\) calls to \(F()\)._

We defer the proof of Theorem 4.4 to Appendix C.1. Plugging the complexity of computing inexact gradients, as given by Theorem 5.3, into the result above, we immediately obtain convergence to a \((,)\)-stationary point of Problem 1.1 with \((^{-1}^{-4})\) gradient calls overall.

Implementation-friendly algorithm.While Algorithm 3 matches the best-known results in nonsmooth nonconvex optimization, it could be impractical due to several hyperparameters which need tuning. Arguably, a more natural application of the hypergradient estimates would be simply plugging them into gradient descent, which requires tuning only the stepsize. Since \(F\) is neither smooth nor convex, perturbations are required to guarantee differentiability along the trajectory. We therefore complement Theorem 4.4 by analyzing perturbed (inexact) gradient descent in the nonsmooth nonconvex setting (Algorithm 7) and state its theoretical guarantee in Theorem C.4. Despite its suboptimal worst-case theoretical guarantees, we find this algorithm easier to implement in practice.

## 5 Inequality constraints: constructing the inexact gradient oracle

Computing a stationary point of \(F\) of Problem 4.1 via any first-order method would require:

\[ F(x)=_{x}f(x,y^{*})+((x)}{dx})^{} _{y}f(x,y^{*}),\] (5.1)

for which the key challenge lies in computing \(dy^{*}(x)/dx\). This requires differentiating through an argmin operator, which typically requires second-order derivatives. Instead, here we differentiate (using the implicit function theorem) through the KKT conditions describing \(y^{*}(x)\) and get:

\[_{yy}^{2}g+(^{*})^{}_{yy}^{2}h& _{y}h_{}^{}\\ (_{}^{*})_{y}h_{}&0 (x)}{dx}\\ }^{*}(x)}{dx}=-_ {yx}^{2}g+(^{*})^{}_{yx}^{2}h\\ (_{}^{*})_{x}h_{}\] (5.2)

where given \(x\), we assume efficient access to the optimal dual solution \(^{*}(x)\) of the LL problem in Problem 4.1. In (5.2), we use \(:=\{i[d_{h}]:h_{i}(x,y)=0,_{i}^{*}>0\}\) to denote the set of active constraints with non-zero dual solution, with \(h_{}[h_{i}]_{i}\) and \(^{*}_{}[^{*}_{i}]_{i}\) being the constraints and dual variables, respectively, corresponding to \(\).

Observe that as currently stated, (5.2) leads to a second-order computation of \(dy^{*}(x)/dx\). In the rest of the section, we provide a fully first-order approximate hypergradient oracle by constructing an equivalent reformulation of Problem 4.1 using a penalty function.

### Reformulation via the penalty method

We begin by reformulating Problem 4.1 into a single level constrained optimization problem:

\[_{x,y}\;f(x,y)\;\;g(x,y)+(^ {*}(x))^{}h(x,y) g^{*}(x)\\ h(x,y) 0,\] (5.3)

where \(g^{*}(x)_{y:h(x,y) 0}g(x,y)=g(x,y^{*}(x))\) and \(^{*}(x)\) is the optimal dual solution. The equivalence of this reformulation to Problem 4.1 is spelled out in Appendix D. From (5.3), we define the following penalty function, crucial to our analysis:

\[_{^{*},}(x,y)=f(x,y)+_{1}(g( x,y)+(^{*})^{}h(x,y)-g^{*}(x))+}{2}\|h_{ }(x,y)\|^{2},\] (5.4)

where \(=[_{1},_{2}] 0\) are the penalty parameters. Notably, we can compute its derivative with respect to \(x\) of (5.4) in a fully first-order fashion by the following expression:

\[_{x}_{^{*},}(x,y)=_{x}f(x,y )+_{1}(_{x}g(x,y)+_{x}h(x,y)^{}^{*}-_{x}g^ {*}(x))+_{2}_{x}h_{}(x,y)^{}h_{}(x,y).\]

To give some intuition for our choice of penalties in (5.4), we note that the two constraints in (5.3) behave quite differently. The first constraint \(g(x,y)+^{*}(x)^{}h(x,y) g^{*}(x)\) is one-sided, i.e., can only be violated or met, and hence just needs a penalty parameter \(_{1}\) to weight the "violation". As to the second constraint \(h(x,y) 0\), it can be arbitrary. To allow for such a "two-sided" constraint, we penalize only the active constraints \(\), i.e., we use \(\|h_{}(x,y)\|^{2}\) to penalize deviation.

Next, we define the optimal solutions to the penalty function optimization by:

\[y^{*}_{^{*},}(x):=_{y}_{ ^{*},}(x,y).\] (5.5)

We now show that this minimizer is close to the optimal solution of the LL problem, while suffering only a small constraint violation.

**Lemma 5.1**.: _Given any \(x\), the corresponding dual solution \(^{*}(x)\), primal solution \(y^{*}(x)\) of the lower optimization problem in Problem 4.1, and \(y^{*}_{^{*},}(x)\) as in (5.5), satisfy:_

\[\|y^{*}_{^{*},}(x)-y^{*}(x)\| O( _{1}^{-1})\;\;\;\;\|h_{}(x,y^{*}_{^{*},}(x))\| O(_{1}^{-1/2}_{2}^{-1/2}).\] (5.6)

The proof of Lemma 5.1 is based on the Lipschitzness of \(f\) and strong convexity of \(g\) for sufficiently large \(_{1}\). The aforementioned constraint violation bound on \(h_{}(x,y)\) is later used in Lemma 5.2 to bound the inexactness of our proposed gradient oracle.

### Main result: approximating the hypergradient

The main export of this section is the following bound on the approximation of the hypergradient. This, together with the bounds in Lemma 5.1, validate our use of the penalty function in (5.4).

**Lemma 5.2**.: _Consider \(F\) as in Problem 4.1, \(\) as in (5.4), a fixed \(x\), and \(y^{*}_{^{*},}\) as in (5.5). Then under Assumptions 2.2 and 2.5, we have:_

\[\| F(x)-_{x}_{^{*},}(x,y ^{*}_{^{*},})\| O(_{1}^{-1})+O( _{1}^{-1/2}_{2}^{-1/2})+O(_{1}^{1/2}_{2}^{-1/2})+O( _{1}^{-3/2}_{2}^{1/2}).\]

The proof can be found in Appendix G. With this hypergradient approximation guarantee, we design Algorithm 4 to compute an inexact gradient oracle for the hyperobjective \(F\).

**Theorem 5.3**.: _Given any accuracy parameter \(>0\), Algorithm 4 outputs \(_{x}F(x)\) such that \(\|F(x)- F(x)\|\) within \((^{-1})\) gradient oracle evaluations._

The full proof of this result may be found in Appendix H.

## 6 Experiments

We generate instances of the following constrained bilevel optimization problem:

\[_{x}\,c^{}y^{*}+0.01\|x\|^{2}+0.01\|y^{*}\|^{2} \ y^{*}=_{y:h(x,y) 0}y^{}Qy+x^{}Py,\] (6.1)

where \(h(x,y)=Ay-b\) is a \(d_{h}\)-dim linear constraint. The PSD matrix \(Q^{d_{y} d_{y}}\), \(c^{d_{y}}\), \(P^{d_{x} d_{y}}\), and constraints \(A^{d_{h} d_{y}}\), \(b^{d_{h}}\) are randomly generated from normal distributions (cf. A). We compare Algorithm 3 with a non-fully first-order method using cvxpyLayer. Both algorithms use Adam  to control the learning rate, and are averaged over 10 random seeds.

Figure 1(a) shows that both the algorithms converge to the same optimal solution at the same rate. Simultaneously, the colorful bars represent the gradient differences between two methods, showing the inexactness of our gradients. Figure 1(b) additionally varies this inexactness to demonstrate its impact on convergence with standard deviation plotted. Figure 1(c) compares the computation costs for different lower-level problem sizes. Our fully first-order method significantly outperforms, in computation cost, the non-fully first-order method implemented using differentiable optimization method. The implementation can be found in https://github.com/guaguakai/constrained-bilevel-optimization.

## 7 Limitations and future directions

One limitation to our approach is that the inexact gradient oracle we constructed in Section 5 requires access to the exact dual multiplier \(^{*}\). For a first-order algorithm, the closest proxy one could get to this would be a highly accurate approximation (which could be computed up to \(\) error in \(O((1/))\) gradient oracle evaluations). Removing this "exact dual access" assumption (Assumption 2.5) would be an important result.

Another important direction for future work would be to extend the hypergradient stationarity guarantee of Theorem 1.4 to bilevel programs with _general convex_ constraints. To this end, we conjecture that the use of a primal-only gradient approximation oracle could be potentially effective.

Finally, our current rate of \((^{-1}^{-4})\) oracle calls for reaching \((,)\)-Goldstein stationarity is not necessarily inherent to the problem; indeed, it might be the case that an alternate approach could improve it to the best known rate of \(O(^{-1}^{-3})\) for nonsmooth nonconvex optimization.

Figure 1: We run Algorithm 3 using Algorithm 4 on the bilevel optimization in the toy example in Problem L.1 with \(d_{x}=100\), \(d_{y}=200\), \(n_{}=d_{y}/5\), and accuracy \(=0.1\). Figure 1(a), Figure 1(b), Figure 1(c) vary # of iterations, gradient exactness \(\), and \(d_{y}\), respectively, to compare the performance under different settings.

[MISSING_PAGE_FAIL:11]

*  Priya Donti, Brandon Amos, and J Zico Kolter. "Task-based end-to-end model learning in stochastic optimization". In: _Advances in neural information processing systems_ 30 (2017) (cit. on p. 1).
*  Bryan Wilder, Bistra Dilkina, and Milind Tambe. "Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization". In: _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 33. 01. 2019, pp. 1658-1665 (cit. on pp. 1, 2).
*  James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Bryan Wilder. "End-to-end constrained optimization learning: A survey". In: _arXiv preprint arXiv:2103.16378_ (2021) (cit. on p. 1).
*  Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. "Meta-learning with differentiable convex optimization". In: _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 2019, pp. 10657-10665 (cit. on p. 1).
*  Bo Tang and Elias B Khalil. "Pyepo: A pytorch-based end-to-end predict-then-optimize library for linear and integer programming". In: _arXiv preprint arXiv:2206.14234_ (2022) (cit. on p. 1).
*  Shaojie Bai, J Zico Kolter, and Vladlen Koltun. "Deep equilibrium models". In: _Advances in neural information processing systems_ 32 (2019) (cit. on p. 1).
*  Akshay Mehra and Jihun Hamm. "Penalty method for inversion-free deep bilevel optimization". In: _Asian conference on machine learning_. PMLR. 2021, pp. 347-362 (cit. on p. 1).
*  Kaiyi Ji, Junjie Yang, and Yingbin Liang. "Bilevel optimization: Convergence analysis and enhanced design". In: _International conference on machine learning_. PMLR. 2021, pp. 4882-4892 (cit. on p. 1).
*  Kai Wang, Sanket Shah, Haipeng Chen, Andrew Perrault, Finale Doshi-Velez, and Milind Tambe. "Learning mdps from features: Predict-then-optimize for sequential decision making by reinforcement learning". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 8795-8806 (cit. on p. 1).
*  Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. "Bome! bilevel optimization made easy: A simple first-order approach". In: _Advances in neural information processing systems_ 35 (2022), pp. 17248-17262 (cit. on pp. 1, 3).
*  Kai Wang, Lily Xu, Andrew Perrault, Michael K Reiter, and Milind Tambe. "Coordinating followers to reach better equilibria: End-to-end gradient descent for stackelberg games". In: _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 36. 5. 2022, pp. 5219-5227 (cit. on p. 2).
*  Paul Dutting, Zhe Feng, Harikrishna Narasimhan, David C Parkes, and Sai S Ravindranath. "Optimal auctions through deep learning". In: _Communications of the ACM_ 64.8 (2021), pp. 109-116 (cit. on p. 2).
*  Jiuping Xu, Yan Tu, and Ziqiang Zeng. "Bilevel optimization of regional water resources allocation problem under fuzzy random environment". In: _Journal of Water Resources Planning and Management_ 139.3 (2013), pp. 246-264 (cit. on p. 2).
*  Walter J Gutjahr and Nada Dzubur. "Bi-objective bilevel optimization of distribution center locations considering user equilibria". In: _Transportation Research Part E: Logistics and Transportation Review_ 85 (2016), pp. 1-22 (cit. on p. 2).
*  Yue Zhang, Oded Berman, Patrice Marcotte, and Vedat Verter. "A bilevel model for preventive healthcare facility network design with congestion". In: _IIE Transactions_ 42.12 (2010), pp. 865-880 (cit. on p. 2).
*  Amir M Fathollahi-Fard, Mostafa Hajiaghaei-Keshteli, Reza Tavakkoli-Moghaddam, and Neale R Smith. "Bi-level programming for home health care supply chain considering outsourcing". In: _Journal of Industrial Information Integration_ 25 (2022), p. 100246 (cit. on p. 2).
*  Adam N Elmachtoub and Paul Grigas. "Smart "predict, then optimize"". In: _Management Science_ 68.1 (2022), pp. 9-26 (cit. on p. 2).
*  Miguel Angel Munoz, Salvador Pineda, and Juan Miguel Morales. "A bilevel framework for decision-making under uncertainty with contextual information". In: _Omega_ 108 (2022), p. 102575 (cit. on p. 2).
*  Prashant Khanduri, Ioannis Tsaknakis, Yihua Zhang, Jia Liu, Sijia Liu, Jiawei Zhang, and Mingyi Hong. "Linearly constrained bilevel optimization: A smoothed implicit gradient approach". In: _International Conference on Machine Learning_. PMLR. 2023, pp. 16291-16325 (cit. on pp. 2-5).

*  Wei Yao, Chengming Yu, Shangzhi Zeng, and Jin Zhang. _Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm_. 2024. arXiv: 2401.16164 [cs.LG] (cit. on pp. 2-4).
*  Zhaosong Lu and Sanyou Mei. _First-order penalty methods for bilevel optimization_. 2024. arXiv: 2301.01716 [math.OC] (cit. on pp. 2-4).
*  Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. "A fully first-order method for stochastic bilevel optimization". In: _International Conference on Machine Learning_. PMLR. 2023, pp. 18083-18113 (cit. on pp. 2, 3).
*  AA Goldstein. "Optimization of Lipschitz continuous functions". In: _Mathematical Programming_ 13 (1977), pp. 14-22 (cit. on pp. 2, 4).
*  Arkadij Semenovic Nemirovskiy and David Borisovich Yudin. "Problem complexity and method efficiency in optimization". In: (1983) (cit. on p. 2).
*  Jingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Suvrit Sra, and Ali Jadbabaie. "Complexity of finding stationary points of nonconvex nonsmooth functions". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 11173-11182 (cit. on pp. 2, 6-8).
*  Guy Kornowski and Ohad Shamir. "Oracle complexity in nonsmooth nonconvex optimization". In: _Journal of Machine Learning Research_ 23.314 (2022), pp. 1-44 (cit. on p. 2).
*  Zhe Zhang and Guanghui Lan. "Solving Convex Smooth Function Constrained Optimization Is Almost As Easy As Unconstrained Optimization". In: _arXiv preprint arXiv:2210.05807_ (2022) (cit. on pp. 2, 7, 24, 34).
*  Guy Kornowski and Ohad Shamir. "An algorithm with optimal dimension-dependence for zero-order nonsmooth nonconvex stochastic optimization". In: _Journal of Machine Learning Research_ 25.122 (2024), pp. 1-14 (cit. on pp. 3, 25).
*  Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. In: _International Conference on Machine Learning_. PMLR. 2023, pp. 6643-6670 (cit. on pp. 3, 6-8, 26).
*  Damek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati Padmanabhan, and Guanghao Ye. "A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions". In: _Advances in neural information processing systems_ 35 (2022), pp. 6692-6703 (cit. on pp. 3, 7, 8).
*  G Anandalingam and DJ White. "A solution method for the linear static Stackelberg problem using penalty functions". In: _IEEE Transactions on automatic control_ 35.10 (1990), pp. 1170-1173 (cit. on p. 3).
*  Yo Ishizuka and Eitaro Aiyoshi. "Double penalty method for bilevel optimization problems". In: _Annals of Operations Research_ 34.1 (1992), pp. 73-88 (cit. on p. 3).
*  Douglas J White and G Anandalingam. "A penalty function approach for solving bi-level linear programs". In: _Journal of Global Optimization_ 3 (1993), pp. 397-419 (cit. on p. 3).
*  Luis Vicente, Gilles Savard, and Joaquim Judice. "Descent approaches for quadratic bilevel programming". In: _Journal of Optimization theory and applications_ 81.2 (1994), pp. 379-399 (cit. on p. 3).
*  DL Zhu. "Optimality conditions for bilevel programming problems". In: _Optimization_ 33.1 (1995), pp. 9-27 (cit. on p. 3).
*  JJ Ye and DL Zhu. "Exact penalization and necessary optimality conditions for generalized bilevel programming problems". In: _SIAM Journal on optimization_ 7.2 (1997), pp. 481-507 (cit. on p. 3).
*  Justin Domke. "Generic methods for optimization-based modeling". In: _Artificial Intelligence and Statistics_. PMLR. 2012, pp. 318-326 (cit. on p. 3).
*  Fabian Pedregosa. "Hyperparameter optimization with approximate gradient". In: _International conference on machine learning_. PMLR. 2016, pp. 737-746 (cit. on p. 3).
*  Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. "On differentiating parameterized argmin and argmax problems with application to bi-level optimization". In: _arXiv preprint arXiv:1607.05447_ (2016) (cit. on p. 3).
*  Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun, and Richard Zemel. "Reiving and improving recurrent back-propagation". In: _International Conference on Machine Learning_. PMLR. 2018, pp. 3082-3091 (cit. on p. 3).

*  Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. "On the iteration complexity of hypergradient computation". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 3748-3758 (cit. on p. 3).
*  Jonathan Lorraine, Paul Vicol, and David Duvenaud. "Optimizing millions of hyperparameters by implicit differentiation". In: _International conference on artificial intelligence and statistics_. PMLR. 2020, pp. 1540-1552 (cit. on p. 3).
*  Dougal Maclaurin, David Duvenaud, and Ryan Adams. "Gradient-based hyperparameter optimization through reversible learning". In: _International conference on machine learning_. PMLR. 2015, pp. 2113-2122 (cit. on p. 3).
*  Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. "Forward and reverse gradient-based hyperparameter optimization". In: _International Conference on Machine Learning_. PMLR. 2017, pp. 1165-1173 (cit. on p. 3).
*  Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. "A value-function-based interior-point method for non-convex bi-level optimization". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 6882-6892 (cit. on p. 3).
*  Jane J Ye, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. "Difference of convex algorithms for bilevel programs with applications in hyperparameter selection". In: _Mathematical Programming_ 198.2 (2023), pp. 1583-1616 (cit. on p. 3).
*  Lucy L. Gao, Jane J. Ye, Haian Yin, Shangzhi Zeng, and Jin Zhang. _Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs_. 2024. arXiv: 2306.16761 [math.OC] (cit. on p. 3).
*  Tianyi Chen, Yuejiao Sun, and Wotao Yin. "Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 25294-25307 (cit. on p. 3).
*  Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. "A single-timescale method for stochastic bilevel optimization". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2022, pp. 2466-2488 (cit. on p. 3).
*  Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. "A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic". In: _SIAM Journal on Optimization_ 33.1 (2023), pp. 147-180 (cit. on p. 3).
*  Zeeshan Akhtar, Amrit Singh Bedi, Srujan Teja Thomdapu, and Ketan Rajawat. "Projection-free stochastic bi-level optimization". In: _IEEE Transactions on Signal Processing_ 70 (2022), pp. 6332-6347 (cit. on p. 3).
*  Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. "A conditional gradient-based method for simple bilevel optimization with convex lower-level problem". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2023, pp. 10305-10323 (cit. on p. 3).
*  Nazanin Abolfazli, Ruichen Jiang, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. "An Inexact Conditional Gradient Method for Constrained Bilevel Optimization". In: _arXiv preprint arXiv:2306.02429_ (2023) (cit. on p. 3).
*  Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, and Aryan Mokhtari. "Projection-free methods for stochastic simple bilevel optimization with convex lower-level problem". In: _Advances in Neural Information Processing Systems_ 36 (2024) (cit. on p. 3).
*  Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. "A near-optimal algorithm for stochastic bilevel optimization via double-momentum". In: _Advances in neural information processing systems_ 34 (2021), pp. 30271-30283 (cit. on p. 3).
*  Zhishuai Guo, Quanqi Hu, Lijun Zhang, and Tianbao Yang. "Randomized stochastic variance-reduced methods for multi-task stochastic bilevel optimization". In: _arXiv preprint arXiv:2105.02266_ (2021) (cit. on p. 3).
*  Junjie Yang, Kaiyi Ji, and Yingbin Liang. "Provably faster algorithms for bilevel optimization". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 13670-13682 (cit. on p. 3).
*  Mathieu Dagreou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. "A framework for bilevel optimization that enables stochastic and global variance reduction algorithms". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 26698-26710 (cit. on p. 3).

*  Shoham Sabach and Shimrit Shtern. "A first order method for solving convex bilevel optimization problems". In: _SIAM Journal on Optimization 27.2_ (2017), pp. 640-660 (cit. on p. 3).
*  Mostafa Amini and Farzad Yousefian. "An iterative regularized incremental projected subgradient method for a class of bilevel optimization problems". In: _2019 American Control Conference (ACC)_. IEEE. 2019, pp. 4069-4074 (cit. on p. 3).
*  Mostafa Amini and Farzad Yousefian. "An iterative regularized mirror descent method for ill-posed nondifferentiable stochastic optimization". In: _arXiv preprint arXiv:1901.09506_ (2019) (cit. on p. 3).
*  Roey Merchav and Shoham Sabach. "Convex Bi-level Optimization Problems with Nonsmooth Outer Objective Function". In: _SIAM Journal on Optimization 33.4_ (2023), pp. 3114-3142 (cit. on p. 3).
*  Ioannis Tsaknakis, Prashant Khanduri, and Mingyi Hong. "An implicit gradient-type method for linearly constrained bilevel problems". In: _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE. 2022, pp. 5438-5442 (cit. on p. 3).
*  Siyuan Xu and Minghui Zhu. "Efficient gradient approximation method for constrained bilevel optimization". In: _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 37. 10. 2023, pp. 12509-12517 (cit. on p. 3).
*  Lesi Chen, Yaohua Ma, and Jingzhao Zhang. "Near-Optimal Fully First-Order Algorithms for Finding Stationary Points in Bilevel Optimization". In: _arXiv preprint arXiv:2306.14853_ (2023) (cit. on p. 4).
*  Frank H Clarke. "Generalized gradients of Lipschitz functionals". In: _Advances in Mathematics_ 40.1 (1981), pp. 52-67 (cit. on p. 4).
*  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. _Lectures on stochastic programming: modeling and theory_. SIAM, 2021 (cit. on p. 4).
*  Michael Jordan, Guy Kornowski, Tianyi Lin, Ohad Shamir, and Manolis Zampetakis. "Deterministic nonsmooth nonconvex optimization". In: _The Thirty Sixth Annual Conference on Learning Theory_. PMLR. 2023, pp. 4570-4597 (cit. on p. 7).
*  Siyu Kong and AS Lewis. "The cost of nonconvexity in deterministic nonsmooth optimization". In: _Mathematics of Operations Research_ (2023) (cit. on p. 7).
*  Benjamin Grimmer and Zhichao Jia. "Goldstein Stationarity in Lipschitz Constrained Optimization". In: _arXiv preprint arXiv:2310.03690_ (2023) (cit. on p. 7).
*  Olivier Devolder, Francois Glineur, and Yurii Nesterov. "First-order methods of smooth convex optimization with inexact oracle". In: _Mathematical Programming_ 146 (2014), pp. 37-75 (cit. on p. 7).
*  Diederik P Kingma and Jimmy Ba. "Adam: A method for stochastic optimization". In: _arXiv preprint arXiv:1412.6980_ (2014) (cit. on pp. 10, 38).
*  Guanghui Lan. _First-order and stochastic optimization methods for machine learning_. Vol. 1. Springer, 2020 (cit. on p. 23).
*  Ohad Shamir. "An optimal algorithm for bandit and zero-order convex optimization with two-point feedback". In: _The Journal of Machine Learning Research_ 18.1 (2017), pp. 1703-1713 (cit. on p. 24).
*  Simon S Du and Wei Hu. "Linear convergence of the primal-dual gradient method for convex-concave saddle point problems without strong convexity". In: _The 22nd International Conference on Artificial Intelligence and Statistics_. PMLR. 2019, pp. 196-205 (cit. on p. 36).
*  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. "Pytorch: An imperative style, high-performance deep learning library". In: _Advances in neural information processing systems_ 32 (2019) (cit. on p. 37).
*  Steven Diamond and Stephen Boyd. "CVXPY: A Python-embedded modeling language for convex optimization". In: _Journal of Machine Learning Research_ 17.83 (2016), pp. 1-5 (cit. on p. 37).

## Appendix A Notation

We use \(,\) to denote inner products and \(\|\|\) for the Euclidean norm. Unless transposed, all vectors are column vectors. For \(f:^{d_{2}}^{d_{1}}\) its Jacobian with respect to \(x^{d_{2}}\) is \( f^{d_{1} d_{2}}\). For \(f:^{d}\), we overload \( f\) to refer to its gradient (the transposed Jacobian), a column vector. We use \(_{x}\) to denote partial derivatives with respect to \(x\).

A function \(f:^{n}^{m}\) is \(L\)-Lipschitz if for any \(x,y\), we have \(\|f(x)-f(y)\| L\|x-y\|\). A differentiable function \(f:^{n}\) is convex if for any \(x,y^{n}\) we have \(f(y) f(x)+ f(x)^{}(y-x)\); it is \(\)-strongly convex if \(f-\|\|^{2}\) is convex; it is \(\)-smooth if \( f\) is \(\)-Lipschitz.

For a Lipschitz function \(f\), a point \(x\) is \((,)\)-stationary if within a \(\)-ball around \(x\), there exists a convex combination of subgradients of \(f\) with norm at most \(\). For a differentiable function \(f\), we say that \(x\) is \(\)-stationary if \(\| f(x)\|\).

## Appendix B Proofs from Section 3

In this section, we provide the full proofs of claims for bilevel programs with linear equality constraints, as stated in Section 3. We first state a few technical results using the implicit function theorem that we repeatedly invoke in our results for this setting.

**Lemma B.1**.: _Fix a point \(x\). Given \(y^{*}=_{y:h(x,y)=0}g(x,y)\) where \(g\) is strongly convex in \(y\) and \(^{*}\) is the dual optimal variable for this problem, define \(_{}(x,y,)=g(x,y)+,h(x,y)\). Then, we have_

\[_{yy}^{2}_{}(x,y^{*}, ^{*})&_{y}h(x,y^{*})^{}}{dx}\\ }{dx}}_{H}[ -_{yx}^{2}g(x,y^{*})-_{yx}^{2}^{*},h (x,y^{*})\\ -_{x}h(x,y^{*})].\]

Proof.: Since \(g\) is strongly convex, by linear constraint qualification, the KKT condition is both sufficient and necessary condition for optimality. Hence, consider the following KKT system obtained via first order optimality of \(y^{*}\), with dual optimal variable \(^{*}\):

\[_{y}g(x,y^{*})+_{y}^{*},h(x,y^{*})=0,h(x,y^{*})=0.\] (B.1)

Differentiating the system of equations in (B.1) with respect to \(x\) and rearranging terms in a matrix-vector format yields:

\[_{yy}^{2}g(x,y^{*})+_{yy}^{2}^{*},h (x,y^{*})&_{y}h(x,y^{*})^{} {dy^{*}}{dx}\\ }{dx}=-_{yx}^{2}g(x,y^{*} )-_{yx}^{2}^{*},h(x,y^{*})\\ -_{x}h(x,y^{*})\] (B.2)

Noting that \(_{yy}^{2}_{}(x,y,)=_{yy}^{2}g(x,y)+ _{yy}^{2},h(x,y)\), we can write (B.2) in the form shown in the lemma. 

**Lemma B.2**.: _Consider the setup in Lemma B.1. The matrix \(H\) defined in (3.4) is invertible if the Hessian \(_{yy}^{2}_{}(x,y^{*},^{*}):=_{yy}^{2} g(x,y^{*})+_{yy}^{2}^{*},h(x,y^{*})\) satisfies \(_{yy}^{2}_{}(x,y^{*},^{*}) 0\) over the tangent plane \(T:=\{y:_{y}h(x,y^{*})y=0\}\) and \(_{y}h\) has full rank._

Proof.: Let \(u=[y,]\). We show that \(Hu=0\) implies \(u=0\), which in turn implies invertibility of \(H\). If \(_{y}h(x,y^{*})y 0\), then by construction of \(u\) and \(H\), we must also have \(Hu 0\). Otherwise if \(_{y}h(x,y^{*})y=0\) and \(y 0\), the quadratic form \(u^{}Hu\) is positive, as seen by

\[u^{}Hu=y^{}_{yy}^{2}_{}(x,y^{*},^{*} )y>0,\]

where the final step is by the assumption of \(_{}\) being positive definite over the defined tangent plane \(T=\{y:_{y}h(x,y^{*})y=0\}\). If \(y=0\) while \(Hu=0\), then \(_{y}h\) having full rank implies \(=0\). Combined with \(y=0\), this means \(u=0\), as required when \(Hu=0\). This concludes the proof.

**Corollary B.3**.: _For Problem 3.1 under Assumption 2.2 and Assumption 2.3, the matrix \(H\) (as defined in (3.4)) is non-singular. Further, there exists a finite \(C_{H}\) such that \(\|H^{-1}\| C_{H}\)._

Proof.: Since we are assuming strong convexity of \(g\), Lemma B.2 applies, yielding the claimed invertibility of \(H\). Combined with the boundedness of variables \(x\) (per Assumption 2.3) and continuity of the inverse implies a bound on \(\|H^{-1}\|\). 

### Construction of the inexact gradient oracle

We now show how to construct the inexact gradient oracle for the objective \(F\) in Problem 3.1. As sketched in Section 3, we then use this oracle in a projected gradient descent algorithm to get the claimed guarantee.

**Lemma B.4**.: _Consider Problem 3.1 under Assumption 2.2 and Assumption 2.3. Let \(y_{}^{*}\) be as defined in (3.3). Then, for any \([0,]\) with \(_{g}/2C_{f}\), the following relation is valid:_

\[\|y_{}^{*}-y^{*}\| M(x),M(x):=} \|_{y}f(x,y^{*})\|}{_{g}}.\]

Proof.: The first-order optimality condition applied to \(g(x,y)+ f(x,y)\) at \(y^{*}\) and \(y_{}^{*}\) gives

\[_{y}g(x,y_{}^{*})+_{y}f(x,y_{}^{*}),y^{* }-y_{}^{*} 0,\]

which upon adding and subtracting \(_{y}f(x,y^{*})\) transforms into

\[_{y}g(x,y_{}^{*})+[_{y}f(x,y_{}^{*})- _{y}f(x,y^{*})]+_{y}f(x,y^{*}),y^{*}-y_{}^{*}  0.\] (B.3)

Similarly, the first-order optimality condition applied to \(g\) at \(y^{*}\) and \(y_{}^{*}\) gives

\[_{y}g(x,y^{*}),y_{}^{*}-y^{*} 0.\] (B.4)

Adding Inequality (B.3) and Inequality (B.4) and rearranging yields

\[_{y}g(x,y_{}^{*})-_{y}g(x,y^{*})+[_{y}f( x,y_{}^{*})-_{y}f(x,y^{*})],y_{}^{*}-y^{*} _{y}f(x,y^{*}),y^{*}-y_{}^{*}.\]

Applying to the left side above a lower bound via strong convexity of \(g+ f\) and to the right hand side an upper bound via Cauchy-Schwarz inequality, we have

\[s\|y_{}^{*}-y^{*}\|\|_{y}f(x,y^{*})\|,\] (B.5)

where \(s\) is the strong convexity of \(g+ f\). Since \(f\) is \(C_{f}\)-smooth, the worst case value of this is \(s=_{g}- C_{f}=_{g}-}{2C_{f}}C_{f}=_{g}/2\), which when plugged in Inequality (B.5) then gives the claimed bound. 

**Lemma B.5**.: _Consider Problem 3.1 under Assumption 2.2 and Assumption 2.3. Then the following relation is valid._

\[_{ 0}[g(x,y_{}^{*}(x))+_{}^{*} h(x,y^{*})]-_{x}[g(x,y^{*}(x))+^{*}h(x,y^{*})]}{}=( (x)}{dx})^{}_{y}f(x,y^{*}(x)).\]

Proof.: Recall that by definition, \(g\) is strongly convex and \(y^{*}=_{y:h(x,y)=0}g(x,y)\). Hence, we can apply Lemma B.1. Combining this with Lemma B.2 and further applying that linearity of \(h\) implies \(_{yy}^{2}h=0\) and \(_{xy}^{2}h=0\), we obtain the following:

\[[}{}{dx}}]=[_{yy}^ {2}g(x,y^{*})&_{y}h(x,y^{*})^{}\\ _{y}h(x,y^{*})&0]^{-1}[-_{yx} ^{2}g(x,y^{*})\\ -_{x}h(x,y^{*})].\]

So we can express the right-hand side of the claimed equation in the lemma statement by

\[((x)}{dx})^{}_{y}f(x,y^{*}(x))=[( }{dx})^{}(}{dx})^{} ][_{y}f(x,y^{*}(x))\\ 0],\]

which can be further simplified to

\[[-_{yx}^{2}g(x,y^{*})^{}-_{x}h(x,y^{*})^{} ][_{yy}^{2}g(x,y^{*})&_{y}h(x,y^{*})^{ }\\ _{y}h(x,y^{*})&0]^{-1}[_{y}f(x,y ^{*}(x))\\ 0].\] (B.6)We now apply Lemma B.1 to the perturbed problem defined in (3.3). We know from Lemma B.4 that \(_{ 0}y_{}^{*}=y^{*}\). The associated KKT system is given by

\[_{y}f(x,y_{}^{*})+_{y}g(x,y_{}^{*})+_{y} _{}^{*},h(x,y_{}^{*})=0h(x,y_{}^{*})=0.\] (B.7)

Taking the derivative with respect of (B.7) gives the following implicit system, where we used the fact that \(h\) is linear and hence \(_{yy}^{2}h=0\):

\[_{yy}^{2}f(x,y_{}^{*})+_{ yy}^{2}g(x,y_{}^{*})&_{y}h(x,y_{}^{*})^{}\\ _{y}h(x,y_{}^{*})&0}_{H_{}} ^{*}}{d}\\ ^{*}}{d}=-_{y}f(x,y_{ }^{*})^{}\\ 0.\] (B.8)

For a sufficiently small \(\), we have \(_{yy}^{2}g(x,y_{}^{*})+_{yy}^{2}f(x,y_{}^{*}) }{2}I\), which implies invertibility of \(H_{}\) by an application of Lemma B.2. Since Lemma B.4 implies \(_{ 0}y_{}^{*}=y^{*}\), we get

\[^{*}}{d^{*}}\\ ^{*}}{d}|_{=0}= _{yy}^{2}g(x,y^{*})&_{y}h(x,y^{*})^{}\\ _{y}h(x,y^{*})&0^{-1}-_{y}f(x,y^{*}) \\ 0.\]

So we can express the left-hand side of the expression in the lemma statement by

\[_{ 0}[g(x,y_{}^{*}(x))+ _{}^{*},h(x,y^{*})]-_{x}[g(x,y^{*}(x))+ ^{*},h(x,y^{*})]}{}\] \[=_{xy}^{2}g(x,y^{*})^{*}}{d}+_ {x}h(x,y^{*})^{}^{*}}{d}\] \[=_{xy}^{2}g(x,y^{*})&_{x}h(x,y^{*})^{ }_{yy}^{2}g(x,y^{*})&_{y}h(x,y^{*} )^{}\\ _{y}h(x,y^{*})&0^{-1}-_{y}f(x,y^{*}) \\ 0,\]

which matches (B.6) (since \((_{yx}^{2}g)^{}=_{xy}^{2}g\)), thus concluding the proof. 

**Lemma 3.3**.: _The solution \(y^{*}\) (as defined in Problem 3.1) is \(O(C_{H}(C_{g}+\|A\|))\)-Lipschitz continuous and \(O(C_{H}^{3} S_{g}(C_{g}+\|A\|)^{2})\)-smooth as a function of \(x\). Thus the hyper-objective \(F\) is gradient-Lipschitz with a smoothness constant of \(C_{F}:=O\{(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2}\}\)._

Proof.: Rearranging (3.4) and applying Corollary B.3, we have

\[}{dx}\\ }{dx}=_{yy}^{2}g(x,y^{*}) &B^{}\\ B&0^{-1}-_{yx}^{2}g(x,y^{*})\\ -_{x}h(x,y^{*}).\]

This implies a Lipschitz bound of \(C_{H}(C_{g}+\|A\|)\). Next, note that in the case with linear equality constraints, the terms in (B.2) involving second-order derivatives of \(h\) are all zero; differentiating (B.2) with respect to \(x\), we notice that the linear system we get again has the same matrix \(H\) from before. We can therefore again perform the same inversion and apply the bound on \(\|H^{-1}\|\) and on the third-order derivatives of \(g\) (Assumption 2.3) to observe that \(\|y^{*}}{dx^{2}}\| O(C_{H} S_{g}\|}{dx}\|^{2})= O(C_{H}^{3} S_{g}(C_{g}+\|A\|)^{2})\), where we are hiding numerical constants in the Big-Oh notation.

As a result, we can calculate the Lipschitz smoothness constant associated with the hyper-objective \(F\) by

\[\| F(x)- F()\|\] \[\|(x)}{dx}_{y}f(x,y^{*}(x))-( )}{dx}_{y}f(,y^{*}())\|+\|_{x}f(x,y^{*}(x))- _{x}f(,y^{*}())\|\] \[[C_{f}C_{H}(L_{g}+\|A\|)+C_{f}C_{H}^{2}(L_{g}+\|A\|)^{2}+L_{f }C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2}]\|x-\|\] \[+[C_{f}+C_{f}C_{H}(L_{g}+\|A\|)]\|x-\|\] \[+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2 }}_{C_{F}}\|x-\|.\]

**Lemma 3.2**.: _For Problem 3.1 under Assumption 2.2, with \(v_{x}\) as in (3.2), the following holds:_

\[\|v_{x}-((x)}{dx})^{}_{y}f(x,y^{*}) \| O(C_{F}).\]

Proof.: For simplicity, we adopt the following notation throughout this proof: \(g_{xy}(x,y)=_{xy}^{2}g,\) and \(g_{xyy}\) denotes the tensor such that its \(ijk\) entry is given by \(g}{ x_{i} y_{j} y_{k}}\). We first consider the terms involving \(g\). By the fundamental theorem of calculus, we have

\[_{x}g(x,y_{}^{*}(x))-_{x}g(x,y^{*}(x))=_{t=0}^{}g_ {xy}(x,y_{t}^{*}(x))^{*}(x)}{dt}dt.\]

As a result, we have

\[g(x,y_{}^{*}(x))-_{x}g(x,y^{*}(x))} {}-g_{xy}(x,y^{*}(x))^{*}(x)}{dt}|_{t=0}\] \[=_{t=0}^{}(g_{xy}(x,y_{t}^{*}(x)) ^{*}(x)}{dt}-g_{xy}(x,y^{*}(x))^{*}(x)}{dt}|_{t=0} )dt\] \[=_{t=0}^{}(g_{xy}(x,y_{t}^{*}(x)) ^{*}(x)}{dt}-g_{xy}(x,y^{*}(x))^{*}(x)}{dt}|_{t=0} )dt\] \[=_{t=0}^{}(g_{xy}(x,y_{t}^{*}(x)) -g_{xy}(x,y^{*}(x)))^{*}(x)}{dt}dt+_{t=0 }^{}g_{xy}(x,y^{*}(x))(^{*}(x)}{dt}-^{* }(x)}{dt}|_{t=0})dt.\] (B.9)

We now bound each of the terms on the right-hand side of (B.9). For the first term, we have

\[\|_{t=0}^{}(g_{xy}(x,y_{t}^{*}(x)) -g_{xy}(x,y^{*}(x))dt)^{*}(x)}{dt}\|\] \[_{t=0}^{}\|^{*}(x)}{dt }\|_{s=0}^{t}\|g_{xyy}(x,y_{s}^{*}(x))\|\|^{*}(x)}{ds}\| ds dt\] \[_{t=0}^{}\|^{*}(x)}{dt }\|_{s[0,]}\|g_{xyy}(x,y_{s}^{*}(x))\|\|^{* }(x)}{ds}\|tdt\] \[_{u[0,]}\|g_{xyy}(x,y_{u}^{ *}(x))\|^{2}_{t[0,]}\|^{*}(x)}{dt}\|^ {2}\] \[_{u[0,]}\|g_{xyy}(x,y_{u}^{*}(x))\| _{t[0,]}\|^{*}(x)}{dt}\|^{2}\] \[= S_{g} M_{y}^{2},\] (B.10)

where \(M_{y}\) is the Lipschitz bound on \(y^{*}\) as shown in Lemma 3.3, and \(S_{g}\) is the smoothness of \(g\) from Assumption 2.3. For the second term on the right-hand side of (B.9), we have

\[\|_{t=0}^{}g_{xy}(x,y^{*}(x))( ^{*}(x)}{dt}-(x)}{dt})\| \|g_{xy}(x,y^{*}(x))\|_{t=0}^{ }(_{s=0}^{t}\|}{ds^{2}}y_{s}^{*}(x)\|ds)dt\] \[\|g_{xy}(x,y^{*}(x))\|_{s[0,]}\|}{ds^{2}}y_{s}^{*}(x)\|^{2}\] \[\|g_{xy}(x,y^{*}(x))\|_{s[0,]} \|}{ds^{2}}y_{s}^{*}(x)\|\] \[= C_{g} C_{y},\] (B.11)where \(C_{g}\) is the bound on smoothness of \(g\) as in Assumption 2.3, and \(C_{y}\) is the bound on \(\|y^{*}}{dx^{2}}\|\) from Lemma 3.3. For the terms involving the function \(h\), we have

\[\|^{*}-^{*}}{}-^{*}}{d}|_{=0}\| =_{t=0}^{}\|^{*}}{dt} -^{*}}{d}|_{=0}\|dt\] \[=_{t=0}^{}_{s=0}^{t}\|}{ ds^{2}}_{s}^{*}\|ds dt\] \[_{s[0,]}\|}{ds^{2}} _{s}^{*}\|^{2}_{s[0,]}\|}{ds^{2}}_{s}^{*}\|\] \[= C_{},\] (B.12)

where \(C_{}\) is the bound on \(\|^{*}}{ds^{2}}\|\) from Lemma 3.3. Combining (B.9), Inequality (B.10), Inequality (B.11), Inequality (B.12), and Inequality (B.12), along with Lemma B.5, Corollary B.3, and Lemma 3.3, we have that overall bound is

\[(S_{g}M_{y}^{2}+C_{g}C_{y}+C_{}) O((S_{g} C _{H}^{3}(C_{g}+\|A\|)^{2}(C_{g}+C_{f}+L_{f}))).\]

### Cost of linear equality constrained bilevel program

```
1:Input: Current \(x_{0}\), accuracy \(\), perturbation \(=^{2}/8C_{F}^{2}R_{X}\) with \(C_{F}=2(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2}\), accuracy for the lower level problem \(=2(C_{g}+\|A\|)^{2}\).
2:for t=0,1,2,... do
3: Run Algorithm 6 to generate \(\)-accurate primal and dual solutions \((^{*},^{*})\) for \[_{y:Ax_{t}+By=b}g(x_{t},y)\]
4: Run Algorithm 6 to generate \(\)-accurate primal and dual solutions \((^{*}_{},^{*}_{})\) for \[_{y:Ax_{t}+By=b}g(x_{t},y)+ f(x_{t},y)\]
5: Compute \(_{t}:=[g(x_{t},^{*}_{})+^{*} _{}h(x,^{*})]-\|_{x}[g(x_{t},^{*})+^{*} h(x,^{*})]}{}\), set \[F(x_{t}):=^{t}+_{x}f(x,^{*}(x)).\]
6: Set \(x_{t+1}_{z}\|z-(x_{t}-}F(x_{t}))\|^{2}\). ```

**Algorithm 5** The Fully First-Order Method for Bilevel Equality Constrained Problem

**Theorem 3.1**.: _Consider Problem 3.1 under Assumption 2.2, and let \(=C_{g}/_{g}\) be the condition number of \(g\). Then Algorithm 5 finds an \(\)-stationary point (in terms of gradient mapping, see (B.14)) after \(T=(C_{F}(F(x_{0})- F)^{-2})\) oracle calls to \(f\) and \(g\), where \(C_{F}:=2(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\|A\|)^{2}\) is the smoothness constant of the hyperobjective \(F\)._

Proof.: We first show the inexact gradient \(F(x_{t})\) generated in Algorithm 5 is an \(\)-accurate approximation to the hyper-gradient \( F(x_{t})\). Consider the inexact gradient defined in (3.2)

\[\|v_{t}-_{t}\| \{\|[_{x}g(x_{t},^{*}_{})- _{x}[g(x_{t},^{*})]-[_{x}g(x_{t},y^{*}_{})-_{x} [g(x_{t},y^{*})\|\] \[+\|^{*}_{}-^{*}-[^{*}_{}- ^{*}]\|A\|\}\] \[[C_{g}+\|A\|].\]Thus we get

\[\|F(x_{t})- F(x_{t})\| \|_{x}f(x_{t},y^{*})-_{x}f(x_{t},^{*})\|+\| ^{t}-v^{t}\|+\|v^{t}-(x^{t})}{dx}_{y}f(x_{t},y^{ *}(x_{t}))\|\] \[ C_{f}+[C_{g}+\|A\|]+C_{F}\] \[}{}[C_{f}+C_{g}+\|A\|]+C_{F}\] \[}{4C_{F}R_{X}}.\]

Applied to the \(C_{F}\)-smooth hyper-objective \(F\), such an inexact gradient oracle satisfies the requirement for Proposition B.6. Thus an \(\)-stationary point with \(\|_{F}(x^{t})\|\) (see Eq. (B.14) for the definition of gradient mapping) must be found in \(N=O((F(x^{0})-F^{*})}{^{2}})\) iterations. Noting the evaluation of inexact solutions \((^{*},^{*},^{*}_{},^{*}_{})\) requires \((/_{g}})\) first order oracle evaluations, we arrive at the total oracle complexity of \((/_{g}}(F(x^{0})-F^{*})}{^{2}})\) for finding an \(\)-stationary point.

### The cost of inexact projected gradient descent method

In this subsection, we state the number of iterations required by projected gradient descent method to find an \(\)-stationary point using inexact gradient oracles. Specifically, we consider the following non-convex smooth problem where the objective \(F\) is assumed to be \(C_{F}\)-Lipschitz smooth:

\[_{x}F(x).\] (B.13)

Since the feasible region \(\) is compact, we use the norm of the following gradient mapping \(_{F}(x)\) as the stationarity criterion

\[_{F}(x):=C_{F}(x-x^{+})x^{+}=_{z} \|z-(x-} F(x))\|^{2}.\] (B.14)

Initialized to some \(x_{0}\) and the inexact gradient oracle \(F\), the updates of the inexact projected gradient descent method is given by

\[\] (B.15) \[x_{t}_{z}\|z- (x_{t-1}-}F(x_{t-1}))\|^{2}.\]

The next proposition calculates the complexity result.

**Proposition B.6**.: _Consider the constrained optimization problem in (B.13) with \(F\) being \(C_{F}\)-Lipschitz smooth and \(\) having a radius of \(R\). When supplied with a \(=^{2}/4C_{F}R\) - inexact gradient oracle \(F\), that is, \(\| F(x)-F(x)\|\), the solution generated by the projected gradient descent method (B.15) satisfies_

\[_{t[N]}\|_{F}(x_{t})\|^{2}(F(x_{0})-F^{*})}{N }+ C_{F}R,\]

_that is, it takes at most \(O((F(x^{0})-F^{*})}{^{2}})\) iterations to generate some \(\) with \(\|_{F}(x)\|\)._

Proof.: By \(C_{F}\)-smoothness of \(F\), we have

\[f(x_{t+1})=f(x_{t}-}_{F}}(x_ {t}))  f(x_{t})-}_{F}}(x_{t})^{ } f(x_{t})+}\|_{F}}(x_{t})\|^{2}\] \[=f(x_{t})-}\|_{F}}(x_{t})(x _{t})\|^{2}+}_{F}}(x_{t})^{}( _{F}}(x_{t})- f(x_{t})).\] (B.16)We now show that \(_{F}}(x_{t})^{}(_{F} }(x_{t})- f(x_{t})) 0\). Let \(_{t}=x_{t}-}F(x_{t})\), and let \(y_{t}=x_{t}-} f(x_{t})\). Then have that

\[}_{F}}(x_{t})^{}(}_{F}}(x_{t})- f(x_{t})) =C_{F}(x_{t}-_{}(_{t}))^{ }(y_{t}-_{}(_{t}))\] \[=C_{F}(x_{t}-_{}(_{t}))^{ }(_{t}-_{}(_{t}))\] \[+C_{F}(x_{t}-_{}(_{t}))^ {}(y_{t}-_{t})\] \[ C_{F}(x_{t}-_{}(_{t}))^ {}(y_{t}-_{t})\] \[ C_{F}R,\]

where the penultimate inequality uses the fact that \(\) is a convex set, and \(R\) is the diameter of the set \(X\). Combining this with Inequality (B.16), we have that the function decrease per iteration is

\[F(x_{t+1}) F(x_{t})-}\|_{F}}(x_{t}) \|^{2}+ C_{F}R.\]

Summing over \(N\) iterations telescopes the terms, we get

\[_{t[N]}\|_{F}}(x_{t})\|^{2}C_{F}(F (x^{0})-F^{*})+ C_{F}R.\]

Substituting in \(N=}C_{F}(F(x^{0})-F^{*})\) and the choice of \(=^{2}/4C_{F}R\), we get

\[_{t[N]}\|_{F}}(x_{t})\|^{2}}{2}.\]

Taking into account the fact that \(\|_{F}}(x_{t})-_{F}(x_{t})\|\| F(x^ {t})-F(x^{t})\|\), we obtain the desired result.

### The cost of generating approximate solutions to the linearly constrained LL problem

In this subsection, we address the issue of generating approximations to the primal and dual solutions \((y^{*},^{*})\) associated with the lower-level problem in Problem 3.1. These approximations are required for computing the approximate hypergradient in Algorithm 1. For notational simplicity, we are going to consider the following constrained strongly convex problem:

\[_{y^{d}}&g(y)\\ &By=b.\] (B.17)

We propose the following simple scheme to generate approximate solutions to Problem B.17.

\[\|-y^{*}\|.\\ =_{^{m}}\|_{y}g()-B^{ }\|^{2}.}\] (B.18)

The following lemma tells us that \(\) is close to \(^{*}\) if \(B\) has full row rank.

**Lemma B.7**.: _Suppose \(g\) in Problem B.17 is a \(C_{g}\)-Lipschitz smooth, and the matrix \(B\) has full row rank such that the following matrix \(M_{B}\) is invertible_

\[M_{B}=I&B^{}\\ B&0.\]

_Then the approximate solution \((,)\) from (B.18) satisfies \(\|-^{*}\|\|M_{B}^{-1}\|(1+C_{g})\)._

Proof.: Since \((^{*},y^{*})\) satisfy the KKT conditions, they are the solution to the following linear system

\[I&B^{}\\ B&0}_{=M_{B}}y^{*}\\ ^{*}=-_{y}g(y^{*}_{t})+Iy^{*}\\ b.\] (B.19)That is

\[y^{*}\\ ^{*}=M_{B}^{-1}-_{y}g(y^{*})+Iy^{*}\\ b.\]

On the other hand, the approximate solutions \((,)\) in (B.18) satisfies

\[I&B^{}\\ B&0\\ =B^{}+I\\ b.\]

We show the right hand side (r.h.s) of the above equation to be close to the r.h.s of (B.19). Let \(S:=\{B^{}:^{m}\}\) denote the subspace spanned by the rows of \(B\). We can rewrite \(B^{}\) as the projection of \( g()\) onto \(S\), that is,

\[B^{}=_{s S}\|_{y}g()-s \|^{2}\] \[-_{y}g(y^{*})= B^{}^{*}=_{s S}\|_{y}g(y^{*})-s\|^{2},\]

where the second relation follows from the KKT conditon associated with \((^{*},y^{*})\). Since the projection is an non-expansive operation, we have

\[\|B^{}-(-_{y}g(y^{*}))\|=\|B^{}-B^{ }^{*}\|\|_{y}g()- g(y^{*})\| C_{g}\| {y}-y^{*}\| C_{g}.\]

We can rewrite \((,)\) as solutions to the following linear system with some \(\|\|(1+C_{g})\),

\[\\ =M_{B}^{-1}-_{y}g(y^{*})+Iy^{*} +\\ b.\]

Thus we get

\[\|\\ -y^{*}\\ ^{*}\|=\|M_{B}^{-1}\|\|\\ 0\|M_{B}^{-1}\|(1+C_{g}).\]

Now we can just use the AGD method to generate a close enough approximate solution \(\) and call up the Subroutine in (B.18) to generate the approximate dual solution \(\).

```
1:Input: accuracy requirement \(>0\) and linearly constrained problem \(_{y:By=b}g(y)\).
2: Starting from \(y^{0}=0\) and using \(Y:=\{y^{d}:By=b\}\) as the simple feasible region.
3: Run the Accelerated Gradient Descent (AGD) Method (Section 3.3 in ) for \(N= 4/_{g}}\|\|M_{B}^{-1}\|(C_{g}+1)}{_{g} }\) iterations.
4: Use the \(y^{N}\) as the approximate solution \(\) to generate \(\) according to (B.18).
5:return\((,)\) ```

**Algorithm 6** The Projected Gradient Method to Generate Primal and Dual Solutions for a Linearly Constrained Problem

**Proposition B.8**.: _Suppose the objective function \(g\) is both \(L_{g}\)-smooth and \(_{g}\)-strongly convex, and that the constraint satisfies the assumption in Lemma B.7. Fix an \(>0\), the solution \((,)\) returned by the above procedure satisfies \(\|y^{*}-\|\) and \(\|-^{*}\|\). In another words, the cost of generating \(\)-close primal and dual solutions are bounded by \(O(}{_{g}}})\)._

Proof.: With \(N:= 4/_{g}}\|\|M_{B}^{-1}\|(L_{g}+1)}{_{g }}\), Theorem 3.7 in  shows that \(\|y^{N}-\|/\|M_{B}^{-1}\|(1+L_{g})\). Then we can apply Lemma B.7 to obtain the desired bound.

Proofs for Section 4

Our algorithms are based on the Lipschitzness of \(F\), which we prove below.

**Lemma 4.3**.: _Under Assumption 2.2 and 2.5, \(F\) in Problem 4.1 is \(O(L_{f}L_{y})\)-Lipschitz in \(x\)._

Proof.: By Lemma \(2.1\) of , the hypergradient of \(F\) computed with respect to the variable \(x\) may be expressed as \(_{x}F(x)=_{x}f(x,y^{*}(x))+((x)}{dx})^{ }_{y}f(x,y^{*}(x))\). Since we impose Lipschitzness on \(f\) and \(y^{*}\), we can bound each of the terms of \(_{x}F(x)\) by the claimed bound. 

### Faster algorithm for low upper-level dimensions

In this section we analyze Algorithm 2, which as stated in Section 4, requires evaluating only the hyperobjective \(F\) (as opposed to estimating the hypergradient in Algorithm 3).

The motivation for designing such an algorithm, is that while evaluating \( F\) up to \(\) accuracy requires \(O(^{-1})\) gradient evaluations, the hyperobjective value can be estimated at a linear rate:

**Lemma 4.2** (Proof in Appendix C.1).: _Given any \(x\), we can return \((x)\) such that \(|F(x)-(x)|\) using \(O(/_{g}}(L_{f}/))\) first-order oracle calls to \(f\) and \(g\)._

Proof of Lemma 4.2.: We note that it suffices to find \(^{*}\) such that \(\|^{*}-y^{*}(x)\|/L_{f}\), since setting \((x):=f(x,^{*})\) will then satisfy \(|(x)-F(x)|=|f(x,^{*})-f(x,y^{*}(x))| L_{f} {}{L_{f}}=\) by Lispchitzness of \(f\), as required. Noting that \(y^{*}(x)=_{h(x,y) 0}g(x,y)\) is the solution to a constrained smooth, strongly-convex problem with condition number \(C_{g}/_{g}\), it is possible to approximate it up to \(/L_{f}\) with \(O(/_{g}}(L_{f}/))\) first-order oracle calls using the result of Zhang and Lan . 

Accordingly, we consider Algorithm 2, which is a zero-order variant of Algorithm 3, whose guarantee is summarized is the theorem below.

**Theorem C.1**.: _Suppose \(F:^{d}\) is \(L\)-Lipschitz, and that \(|()-F()|\). Then running Algorithm 3 with \(=\{,)- F}{L}\},= -,\ D=(^{2}}{d^{2}L^{2}+^{2 }d^{2}}),=(^{4}}{(d^{2}L^{2 }+^{2}d^{2})^{2}})\), outputs a point \(x^{}\) such that \([(0,_{}F(x^{}))]+\) with_

\[T=O()- F)}{^{3}} (L^{2}+^{2}(}+}{(F(x_{0})- F)^{ 2}})))().\]

Combining the result of Theorem C.1 with the complexity of hyperobjective estimation, as given by Lemma 4.2, we obtain convergence to a \((,)\)-stationary point of Problem 4.1 with \((d_{x}^{-1}^{-3})\) gradient calls overall.

#### c.1.1 Proof of Theorem c.1

Denoting the uniform randomized smoothing \(F_{}(x):=_{\|z\| 1}[F(x+ z)]\) where the expectation, here and in what follows, is taken with respect to the uniform measure, it is well known [92, Lemma 10] that

\[_{\|w\|=1}[(F(x+ w)-F(x- w ))w] = F_{}(x)\;,\] \[_{\|w\|=1}\| F_{}(x)-(F (x+ w)-F(x- w))w\|^{2}  dL^{2}\;.\] (C.1)

We first show that replacing the gradient estimator with the inexact evaluations \(()\) leads to a biased gradient estimator of \(F\).

**Lemma C.2**.: _Suppose \(|F()-()|\). Denoting_

\[g_{x} =(F(x+ w)-F(x- w))w\;,\] \[_{x} =((x+ w)-(x- w ))w\;,\]it holds that_

\[_{\|w\|=1}\|g_{x}-_{x}\|\;,_{\|w\|=1}\|_{x}\|^{2}d^{2}}{^{2}}+dL^{2}\;.\]

Proof.: For the first bound, we have

\[_{\|w\|=1}\|g_{x}-_{x}\| (2)_{\|w\|=1}\|w\|=\;,\]

while for the second bound

\[_{\|w\|=1}\|_{x}\|^{2}=_{\|w\|=1} \|_{x}-g_{x}+g_{x}\|^{2} 2_{\|w\|=1} \|_{x}-g_{x}\|^{2}+2_{\|w\|=1}\|g_{x} \|^{2}}{^{2}}.^{2}+dL^{2}\;,\]

where the last step invoked (C.1). 

We are now ready to analyze Algorithm 2. We denote \(^{}=\), \(=d^{2}}{^{2}}+dL^{2}}\). Since \(x_{t}=x_{t-1}+_{t}\), we have

\[F_{}(x_{t})-F_{}(x_{t-1}) =_{0}^{1} F_{}(x_{t-1}+s_{t}), _{t} ds\] \[=_{s_{t}}[ F_{}(x _{t-1}+s_{t}_{t}),_{t}]\] \[=[ F_{}(z_{t}),_{t} ]\;.\]

By summing over \(t[T]=[K M]\), we get for any fixed sequence \(u_{1},,u_{K}^{d}:\)

\[ F_{} F_{}(x_{T})  F_{}(x_{0})+_{t=1}^{T}[  F_{}(z_{t}),_{t}]\] \[=F_{}(x_{0})+_{k=1}^{K}_{m=1}^{M}[  F_{}(z_{(k-1)M+m}),_{(k-1)M+m}-u_{k}]\] \[+_{k=1}^{K}_{m=1}^{M}[  F_{}(z_{(k-1)M+m}),u_{k}]\] \[ F_{}(x_{0})+_{k=1}^{K}_{M}(u_{k})+_ {k=1}^{K}_{m=1}^{M}[ F_{}(z_{(k-1)M+ m}),u_{k}]\] \[ F_{}(x_{0})+KD+K^{}DM+ _{k=1}^{K}_{m=1}^{M}[ F_{}(z_{(k- 1)M+m}),u_{k}]\]

where the last inequality follows by combining Lemma C.2 and Lemma C.3. By setting \(u_{k}:=-D^{M} F_{}(z_{(k-1)M+m})}{\|_{m=1 }^{M} F_{}(z_{(k-1)M+m})\|}\), rearranging and dividing by \(DT=DKM\) we obtain

\[_{k=1}^{K}\|_{m=1}^{ M} F_{}(z_{(k-1)M+m})\| (x_{0})- F_{}}{DT}+}{ }+^{}\] \[=(x_{0})- F_{}}{K}+d^{2}}{^{2}}+L^{2}d}}{}+\] \[(x_{0})- F_{}}{K}+}{}+}{}+\;.\] (C.2)

Finally, note that for all \(m[M]:\|z_{(k-1)M+m}-_{k}\| MD\), therefore \( F_{}(z_{(k-1)M+m})_{}F_{}(_{k}) _{}F(_{k})\), where the last containment is due to [46, Lemma 4] by using our assignment \(+=\). Invoking the convexity of the Goldstein subdifferential, this implies that

\[_{m=1}^{M} F_{}(z_{(k-1)M+m})_{}F( _{k})\;,\]thus it suffices to bound the first three summands in (C.2) by \(\) in order to finish the proof. This happens as long as \((x_{0})- F_{}}{K}\), \(}{}\), and \(}{}\), which imply \(K(x_{0})- F_{}}{}\), \(Md^{2}}{^{2}^{2}}\), and \(M}{^{2}}\). By our assignments of \(\) and \(\), these result in

\[T=KM =O((x_{0})- F_{}}{} (d^{2}}{^{2}^{2}}+d}{^{2}} ))\] \[=O()- F)d}{^{3}}( d}{^{2}}+L^{2}))\] \[=O()- F)d}{^{3}}( ^{2}d\{},}{(F(x_{0})- F )^{2}}\}+L^{2}))\;,\]

completing the proof.

### Proof of Theorem 4.4

We recall Theorem 4.4 below to keep this section self-contained.

**Theorem 4.4**.: _Suppose \(F:^{d}\) is \(L\)-Lipschitz and that \(\|F()- F()\|\). Then running Algorithm 3 with \(D=(}{L^{2}}),=(}{L^{4}})\), outputs a point \(x^{}\) such that \([(0,_{}F(x^{}))]+\), with \(T=O()- F)L^{2}}{^{3}})\) calls to \(F()\)._

Our analysis is inspired by the reduction from online learning to nonconvex optimization given by . To that end, we start by proving a seemingly unrelated result, asserting that online gradient descent minimizes the regret with respect to inexact evaluations. Recalling standard definitions from online learning, given a sequence of linear losses \(_{m}()= g_{m},\), if an algorithm chooses \(_{1},,_{M}\) we denote the regret with respect to \(u\) as

\[_{M}(u):=_{m=1}^{M} g_{m},_{m}-u.\]

Consider an update rule according to online projected _inexact_ gradient descent:

\[_{m+1}:=_{D}(_{m}-_{m}_{m}).\]

**Lemma C.3** (Inexact Online Gradient Descent).: _In the setting above, suppose that \((_{m})_{m=1}^{M}\) are possibly randomized vectors, such that \(\|_{m}-g_{m}\|\) and \(\|_{m}\|^{2}^{2}\) for all \(m[M]\). Then for any \(\|u\| D\) it holds that_

\[[_{M}(u)]}{_{M}}+ {G}^{2}_{m=1}^{M}_{m}+ DM\;,\]

_where the expectation is with respect to the (possible) randomness of \((_{m})_{m=1}^{M}\). In particular, setting \(_{m}}\) yields_

\[[_{M}(u)] D+  DM\;.\]

Proof.: For any \(m[M]:\)

\[\|_{m+1}-u\|^{2} =\|_{D}(_{m}-_{m}_{m})-u \|^{2}\] \[\|_{m}-_{m}_{m}-u\|^{2}= \|_{m}-u\|^{2}+_{m}^{2}\|_{m}\|^{2 }-2_{m}_{m}-u,_{m}\;,\]

thus

\[_{m},_{m}-u-u\|^{2}-\|_{m+1}-u\|^{2}}{2_{m}}+}{2}\|_{m}\|^{2}\;,\]

from which we get that

\[ g_{m},_{m}-u =_{m},_{m}-u + g_{m}-_{m},_{m}-u\] \[-u\|^{2}-\|_{m+1}-u \|^{2}}{2_{m}}+}{2}\|_{m} \|^{2}+\|g_{m}-_{m}\|\| _{m}-u\|\] \[-u\|^{2}-\|_{m+1}-u \|^{2}}{2_{m}}+}{2}^{2}+ D\;.\]Summing over \(m[M]\), we see that

\[[_{M}(u)] _{m=1}^{M}\|_{m}-u\|^{2}(}-})+^{2}}{2}_{m=1}^{M }_{m}+M D\] \[}{_{M}}+^{2}_{m=1}^{M}_{m }+ DM\;.\]

The simplification for \(_{m}}\) readily follows. 

We are now ready to analyze Algorithm 3 in the inexact gradient setting.

Proof of Theorem 4.4.: Since Algorithm 3 has \(x_{t}=x_{t-1}+_{t}\), we have

\[F(x_{t})-F(x_{t-1}) =_{0}^{1} F(x_{t-1}+s_{t}),_{t}  ds\] \[=_{s_{t}}[  F(x_{t-1}+s_{t}_{t}),_{t}]\] \[=[ F(z_{t}),_{t} ]\;.\]

By summing over \(t[T]=[K M]\), we get for any fixed sequence \(u_{1},,u_{K}^{d}:\)

\[ F F(x_{T})  F(x_{0})+_{t=1}^{T}[ F( z_{t}),_{t}]\] \[=F(x_{0})+_{k=1}^{K}_{m=1}^{M}[  F(z_{(k-1)M+m}),_{(k-1)M+m}-u_{k}]\] \[+_{k=1}^{K}_{m=1}^{M}[  F(z_{(k-1)M+m}),u_{k}]\] \[ F(x_{0})+_{k=1}^{K}_{M}(u_{k})+_{k=1}^{ K}_{m=1}^{M}[ F(z_{(k-1)M+m}),u_{k} ]\] \[ F(x_{0})+KD+K DM+_{k=1}^{K} _{m=1}^{M}[ F(z_{(k-1)M+m}),u_{k}]\]

where the last inequality follows from Lemma C.3 for \(=+^{2}},\)\(=}\), since \(\|_{t}- F(z_{t})\|\) (deterministically) for all \(t[T]\) by assumption. Letting \(u_{k}:=-D^{M} F(z_{(k-1)M+m})}{\|_{m=1}^{M}  F(z_{(k-1)M+m})\|}\), rearranging and dividing by \(DT=DKM\), we obtain

\[_{k=1}^{K}\|_{m=1}^{ M} F(z_{(k-1)M+m})\| )- F}{DT}+}{}+\] \[=)- F}{K}+}{}+ \;.\] (C.3)

Finally, note that for all \(k[K],m[M]:\|z_{(k-1)M+m}-_{k}\| MD\), therefore \( F(z_{(k-1)M+m})_{}F(_{k})\). Invoking the convexity of the Goldstein subdifferential, we see that

\[_{m=1}^{M} F(z_{(k-1)M+m})_{}F( {x}_{k})\;,\]

thus it suffices to bound the first two summands on the right-hand side in (C.3) by \(\) in order to finish the proof. This happens as long as \()- F}{K}\) and \(}{}\). These are equivalent to \(K)- F)}{}\) and \(M^{2}}{^{2}}\), which results in

\[T=KM=O()- F}{}+^{2}}{ ^{2}})=O()- F)L^{2}}{^{3}} ),\]completing the proof. 

### An implementation-friendly algorithm and its analysis

```
1:Input: Inexact gradient oracle \(F:^{d}^{d}\), initialization \(x_{0}^{d}\), spatial parameter \(>0\), step size \(>0\), iteration budget \(T\).
2:for\(t=0,,T-1\)do
3: Sample \(w_{t}(^{d-1})\)
4:\(_{t}=F(x_{t}+ w_{t})\)
5:\(x_{t+1}=x_{t}-_{t}\)
6:Output:\(x^{}\{x_{0},,x_{T-1}\}\). ```

**Theorem C.4**.: _Suppose \(F:^{d}\) is \(L\)-Lipschitz, and that \(\|F()- F()\|\). Then running Algorithm 7 with \(=()- F\}+ L)^{1/2}^ {1/2}}{T^{1/2}L^{1/4}^{1/4}(+L)})\) outputs a point \(x^{}\) such that \([(0,_{}F(x^{}))]+ \), with_

\[T=O()- F+ L)L^{3}}{^{4}} )F().\]

Proof.: Throughout the proof we denote \(z_{t}=x_{t}+ w_{t}\). Since \(F\) is \(L\)-Lipschitz, \(F_{}(x):=_{w(^{d-1})}[F(x+  w)]\) is \(L\)-Lipschitz and \(O(L/)\)-smooth. By smoothness we get

\[F_{}(x_{t+1})-F_{}(x_{t})  F_{}(x_{t}),x_{t+1}-x_{t} +O(}{})\|x_{t+1}-x_{t}\|^{2}\] \[=- F_{}(x_{t}),_{t} +O(L}{})\| _{t}\|^{2}\] \[=- F_{}(x_{t}), F(z_{t}) - F_{}(x_{t}),_{t}-  F(z_{t})+O(L}{}) \|_{t}\|^{2}\.\]

Noting that \([ F(z_{t})]= F_{}(x_{t})\) and that \(\|_{t}\|\|_{t}- F(z_{t}) \|+\| F(z_{t})\|+L\), we see that

\[[F_{}(x_{t+1})-F_{}(x_{t})]-\|  F_{}(x_{t})\|^{2}+ L+O(L }{}(+L)^{2})\,\]

which implies

\[\| F_{}(x_{t})\|^{2}[F_{ }(x_{t})]-[F_{}(x_{t+1})]}{}+L+O((+L)^{2}}{})\.\]

Averaging over \(t=0,,T-1\) and noting that \(F_{}(x_{0})- F_{}(F(x_{0})- F)+ L\) results in

\[\| F_{}(x^{})\|^{2}= _{t=0}^{T-1}E\| F_{}(x_{t})\|^{2} )- F)+ L}{ T}+L+O((+L)^{2} }{})\.\]

By Jensen's inequality and the sub-additivity of the square root,

\[\| F_{}(x^{})\|)- F)+ L}{ T}}++O((+L)^{2}}{}})\.\]Setting \(=)- F)+ L)}}{(+L)^{2}}}\) yields the final bound

\[\| F_{}(x^{})\| )- F)+ L)^{1/4}L^{1/4}d^{1/8}(+L)^{1/2}}{^{1/4}T^{1/4}}+ \,,\]

and the first summand is bounded by \(\) for \(T=O()- F)+ L)L(L+)^{2}}{ ^{4}})\).

## Appendix D Reformulation equivalence

**Theorem D.1** (Reformulation equivalence).: _When \(^{*}\) matches to an optimal dual solution to the lower level problem \(y^{*}=_{y}g(x,y)\) s.t. \(h(x,y) 0\), we show that for each \(x\), the reformulation has the same feasible region of \(y\)._

Proof.: We first show that lower-level feasibility implies feasibility of the reformulated problem. Let \(y^{*},^{*}=_{y}_{ 0}g(x,y)+^{}h(x,y)\) be the primal and the dual solution to the lower level problem with parameter \(x\). We can verify that \(y^{*}\) satisfies all the constraints in the reformulation problem. The feasibility condition \(h(x,y^{*})\) is automatically satisfied. We just need to check:

\[g^{*}(x) _{}g(x,)+(^{*})^{}h(x,)\] \[=g(x,y^{*})+(^{*})^{}h(x,y^{*}).\] (D.1)

Therefore, \(x,y^{*}\) is a feasible point to the reformulation problem.

We now show the other direction, i.e., that feasibility of the reformulaed problem implies that of the lower-level problem. Given \(^{*}\), let us assume \(y\) satisfies \(g(x,y) g^{*}_{^{*}}(x)\) and \(h(x,y) 0\). On the other hand, assume \(y^{*},^{*}=_{y}_{ 0}g(x,y)+^{}h(x,y)\) be the primal and the dual solution. We can show that:

\[g(x,y)+(^{*})^{}h(x,y) g^{*}(x)_{}g(x, )+(^{*})^{}h(x,).\] (D.2)

By the strong convexity of \(g+(^{*})^{}h\), we know that \(y\) matches to the unique minimum \(y^{*}\), which implies that \(y=y^{*}\) is also a feasible point to the original bilevel problem. 

## Appendix E Active constraints in differentiable optimization

By computing the derivative of the KKT conditions in Section 2.1, we get:

\[(^{2}_{yx}g+(^{*})^{}^{2}_{yx}h)+(^ {2}_{yy}g+(^{*})^{}^{2}_{yy}h)}{dx}+(_{y}h )^{}}{dx} =0\] (E.1) \[(^{*})_{x}h+(^{*}) _{y}h}{dx}+(h)}{dx} =0.\] (E.2)

Let \(=\{i[d_{h}]|h(x,y^{*})_{i}=0,^{*}_{i}>0\}\) be the set of active constraints with positive dual solution, and \(_{1}=\{i|h(x,y^{*})_{i} 0\}\) be the set of inactive constraints and \(_{2}=\{i|h(x,y^{*})_{i}=0,^{*}_{i}=0\}\). We know that \(=_{1}_{2}\). For each \(i_{1}\), due to complementary slackness, we know that \(^{*}_{i}=0\).

For \(i_{1}\) in (E.1), we have \(^{*}_{i}_{x}h(x,y^{*})_{i}+^{*}_{i}_{y}h(x,y^{*})_{i} }{dx}+h(x,y^{*})_{i}_{i}}{dx}=0\), which implies \(h(x,y^{*})_{i}_{i}}{dx}=0\) because \(^{*}_{i}=0\). This in turn implies \(_{i}}{dx}=0\) because \(h(x,y^{*})_{i}<0\). That means the dual variable \(^{*}_{i}=0\) and has zero gradient \(_{i}}{dx}=0\) for any index \(i_{1}\). Therefore, we can remove row \(i_{1}\) in (E.2) and obtain \(^{*}_{i}=0\) and \(_{i}}{dx}=0\).

For \(i_{2}\), the KKT condition in (E.2) is degenerate. Therefore, \(_{i}}{dx}\) can be arbitrary, i.e., non-differentiable. As a subgradient choice, we can set \(_{i}}{dx}=0\) for such \(i\). This choice will also eliminate its impact on the KKT condition in (E.1) because \(}^{*}}{dx}\) is set to be \(0\). By this choice of subgradient, we can also remove row \(i_{2}\) (E.2).

Thus (E.2) can be written as the following set of equations, for \(h_{}=[h_{i}]_{i}\) and \(_{}^{*}=[_{i}^{*}]_{i}\):

\[(^{*})_{x}h_{}+( _{}^{*})_{y}h_{}}{dx}+(h_{})}^{*}}{dx}=0\] \[(^{*})_{x}h_{}+(_{}^{*})_{y}h_{}}{dx}=0}(x,y^{*})=0$)}.\] (E.3)

In (E.1), due to \(}^{*}}{dx}=0\) for all \(i}\), we can remove \(}^{*}}{dx}\)\( i}\) in (E.1) by:

\[0 =(_{yx}^{2}g+(^{*})^{}_{yx}^{2}h)+( _{yy}^{2}g+(^{*})^{}_{yy}^{2}h)}{dx}+(_{y} h)^{}}{dx}\] \[=(_{yx}^{2}g+(^{*})^{}_{yx}^{2}h)+( _{yy}^{2}g+(^{*})^{}_{yy}^{2}h)}{dx}+(_{y} h_{})^{}}^{*}}{dx}.\] (E.4)

Combining (E.4) and (E.3), we get:

\[(_{yx}^{2}g+(^{*})^{}_{yx}^{2}h)+( _{yy}^{2}g+(^{*})^{}_{yy}^{2}h)}{dx}+(_{y} h_{})^{}}^{*}}{dx} =0\] \[(^{*})_{x}h_{}+( _{}^{*})_{y}h_{}}{dx} =0,\]

which can be written in its matrix form:

\[_{yy}^{2}g+(^{*})^{}_{yy}^{2}h& _{y}h_{}^{}\\ (_{}^{*})_{y}h_{}&0 }{dx}\\ }^{*}}{dx}=-_{yx}^ {2}g+(^{*})^{}_{yx}^{2}h\\ (_{}^{*})_{x}h_{}\] (E.5)

This concludes the derivation of the derivative of constrained optimization in (5.2).

## Appendix F Inequality case: bounds on primal solution error and constraint violation

**Lemma 5.1**.: _Given any \(x\), the corresponding dual solution \(^{*}(x)\), primal solution \(y^{*}(x)\) of the lower optimization problem in Problem 4.1, and \(y^{*}_{^{*},}(x)\) as in (5.5), satisfy:_

\[\|y^{*}_{^{*},}(x)-y^{*}(x)\|  O(_{1}^{-1})\;\;\|h_{}(x,y^{*}_{^{*},}(x)) \| O(_{1}^{-1/2}_{2}^{-1/2}).\] (5.6)

Proof.: We first provide the claimed bound on \(\|y^{*}_{_{1},_{2}}-y^{*}(x)\|\).

**Part 1: Bound on the convergence of \(y\).**

Since \(y^{*}_{^{*},}\) minimizes \(_{,^{*}}(x,y)\), the first-order condition gives us:

\[0=_{y}_{,^{*}}(x,y^{*}_{^{*},}).\]

Similarly, we can compute the gradient of \(_{,^{*}}(x,y)\) at \(y^{*}\):

\[_{y}_{}(x,y^{*}) =_{y}f(x,y^{*})+_{1}(_{y}g(x,y^{*})+(^{* })^{}_{y}h(x,y^{*}))+_{2}_{y}h_{}(x,y^{*})^ {}h_{}(x,y^{*})\] \[=_{y}f(x,y^{*}),\]

where the second step is due to the property of the primal and dual solution: \(_{y}g(x,y^{*})+(^{*})^{}_{y}h(x,y^{*})=0\) by the stationarity condition in the KKT conditions, and by definition of the active constraints \(h_{}\) where the optimal \(y^{*}\) must have \(h_{}(x,y^{*})=0\).

Since, for a sufficiently large \(_{1}\), the penalty function is \(_{1}_{g}-L_{f}_{g}}{2}\) strongly convex in \(y\), we have:

\[_{g}}{2}\|y^{*}-y^{*}_{^{*},} \|\|_{y}_{,^{*}}(x,y^ {*})-_{y}_{,^{*}}(x,y^{*}_{^{*},})\|=\|_{y}f(x,y^{*})\| L_{f}.\]

Therefore, upon rearranging the terms, we obtain the claimed bound:

\[\|y^{*}-y^{*}_{,^{*}}\|}{ _{1}_{g}}.\]

**Part 2: bound on the constraint violation.**

When we plug \(y^{*}\) into (5.4), we get:

\[_{,^{*}}(x,y^{*})=f(x,y^{*})+_{1}(g(x, y^{*})+(^{*})^{}h(x,y^{*})-g^{*}_{^{*}}(x))+}{2} \|h_{}(x,y^{*})\|^{2}=f(x,y^{*}).\]

Plugging in \(y^{*}_{,^{*}}\), we may obtain:

\[_{,^{*}}(x,y^{*}_{^{ *},}) =f(x,y^{*}_{^{*},})+_{1}(g(x,y^{ *}_{^{*},})+(^{*})^{}h(x,y^{*}_{^{ *},})-g^{*}(x))+}{2}\|h_{}(x,y^{*}_{^{*},})\|^{2}\] \[=f(x,y^{*}_{^{*},})+_{1}(g(x,y^{ *}_{^{*},})+(^{*})^{}h(x,y^{*}_{^{ *},})-g(x,y^{*})-(^{*})^{}h(x,y^{*}))\] \[+}{2}\|h_{}(x,y^{*}_{ ^{*},})\|^{2}\] \[ f(x,y^{*}_{^{*},})+_{1} {_{g}}{2}\|y^{*}-y^{*}_{^{*},}\|^{2} +}{2}\|h_{}(x,y^{*}_{^{*},})\|^{2},\]

where we used the strong convexity (with respect to \(y\)) of \(g(x,y)+(^{*})^{}h(x,y)\) and the optimality of \(y^{*}\) for \(g(x,y)+(^{*})^{}h(x,y)\). By the optimality of \(y^{*}_{^{*},}\) for \(_{,^{*}}\), we know that

\[f(x,y^{*})=_{,^{*}}(x,y^{*})_{,^{*}}(x,y^{*}_{^{*},})  f(x,y^{*}_{^{*},})+_{1}}{2} \|y^{*}-y^{*}_{^{*},}\|^{2}+}{2}\|h_{}(x,y^{*}_{^{*},}) \|^{2}.\]

Therefore, by the Lipschitzness of the function \(f\) in terms of \(y\), and the bound \(\|y^{*}-y^{*}_{^{*},}\|} {_{1}_{g}}\), we know that:

\[}{2}\|h_{}(x,y^{*}_{^{*}, })\|^{2}  f(x,y^{*})-f(x,y^{*}_{^{*},})- _{1}}{2}\|y^{*}-y^{*}_{^{*},}\|^{2}\] \[ L_{f}\|y^{*}-y^{*}_{^{*},} \|-_{1}}{2}\|y^{*}-y^{*}_{^{*},}\|^{2}\] \[ L_{f}\|y^{*}-y^{*}_{^{*},}\|\] \[=O(_{1}^{-1}).\]

Rearranging terms then gives the claimed bound. 

The bound on the constraint violation in Lemma 5.1 is an important step in the following theorem.

## Appendix G Proof of Lemma 5.2: gradient approximation for inequality constraints

**Lemma 5.2**.: _Consider \(F\) as in Problem 4.1, \(\) as in (5.4), a fixed \(x\), and \(y^{*}_{^{*},}\) as in (5.5). Then under Assumptions 2.2 and 2.5, we have:_

\[\| F(x)-_{x}_{^{*},}(x, y^{*}_{^{*},})\| O(_{1}^{-1})+O(_{1}^{-1/2} _{2}^{-1/2})+O(_{1}^{1/2}_{2}^{-1/2})+O(_{1}^{-3/2} _{2}^{1/2}).\]

Proof.: First, we recall (5.4) here:

\[_{^{*},}(x,y)=f(x,y)+_{1}(g(x,y)+(^{*})^{}h(x,y)-g^{*}(x))+}{2}\|h_{ }(x,y)\|^{2}.\]

Next, recall from Equation D.1, we can express \(g^{*}(x)=g(x,y^{*})+(^{*})^{}h(x,y^{*})\), which we use in the first step below:

[MISSING_PAGE_EMPTY:32]

Using (5.2) to solve \(}{dx}\\ )^{*}}{dx}=-H^{-1}_{yx}^{2}g+( (^{*})^{*})^{}_{yx}^{2}h\\ ((^{*})_{}^{*})_{x}h_{}\), we can write:

\[}{dx}^{}_{y}f(x,y^{*}_{(^{*})^{*}, })=_{yx}^{2}g+((^{*})^{*})^{ }_{yx}^{2}h\\ ((^{*})_{}^{*})_{x}h_{}^{ }(H^{-1})^{}-_{y}f(x,y^{*}_{(^{*})^{*}, })\\ 0\] \[=-}{dx}^{}_{1} _{y}g(x,y^{*}_{(^{*})^{*},})+_{y}h(x,y^{*}_{ (^{*})^{*},})^{}(^{*})^{*}\] \[+_{2}_{y}h_{}(x,y^{* }_{(^{*})^{*},})^{}h_{}(x,y^{*}_{( ^{*})^{*},})\\ 0,\] (G.6)

where we use the optimality of \(y^{*}_{(^{*})^{*},}\) from (5.5):

\[_{y}f(x,y^{*}_{(^{*})^{*},})+ _{1}(_{y}g(x,y^{*}_{(^{*})^{*},})+ _{y}h(x,y^{*}_{(^{*})^{*},})^{}(^{ *})^{*})\] (G.7) \[+_{2}_{y}h_{}(x,y^{*}_{(^{*}) ^{*},})^{}h_{}(x,y^{*}_{(^{*})^{*}, })=0.\]

Further, recall that \(H\) is non-degenerate by Assumption 2.2, as a result of which, the added term 1 in (G.3) can be modified as follows:

\[_{yx}^{2}g+((^{*})^{*})^{}_{ yx}^{2}h\\ ((^{*})_{}^{*})_{x}h_{}^{ }_{1}(y^{*}_{(^{*})^{*},}- y^{*})\\ 0\] \[= _{yx}^{2}g+((^{*})^{*})^{} _{yx}^{2}h\\ ((^{*})_{}^{*})_{x}h_{} ^{}(H^{-1})^{}H^{}_{1}(y^{*}_{(^{*})^ {*},}-y^{*})\\ 0\] \[= _{1}_{yx}^{2}g+((^{*})^{*})^{ }_{yx}^{2}h\\ ((^{*})_{}^{*})_{x}h_{} ^{}(H^{-1})^{}(_{yy}^{2}g+((^{*})^{*})^{ }_{yy}^{2}h)^{}(y^{*}_{(^{*})^{*},}- y^{*})\\ _{y}h_{}(x,y^{*})(y^{*}_{(^{*})^{*},}- y^{*}).\] (G.8)

The added term 2 in (G.3) can be expanded to:

\[_{2}_{yx}^{2}g+((^{*})^{*})^{ }_{yx}^{2}h(^{*})^{*}\\ ((^{*})_{}^{*})_{x}h_{} ^{}0\\ (1/(^{*})_{}^{*})h_{}(x,y^{*}_{(^{* })^{*},})\] \[= _{2}_{yx}^{2}g+((^{*})^{*})^{ }_{yx}^{2}h(^{*})^{*}\\ ((^{*})_{}^{*})_{x}h_{} ^{}(H^{-1})^{}H^{}0\\ (1/(^{*})_{}^{*})h_{}(x,y^{*}_{(^{* })^{*},})\] \[= _{2}_{yx}^{2}g+((^{*})^{*})^{ }_{yx}^{2}h(^{*})^{*}\\ ((^{*})_{}^{*})_{x}h_{} ^{}(H^{-1})^{}_{y}h_{}(x,y^{*})^{}h_ {}(x,y^{*}_{(^{*})^{*},})\\ 0\] (G.9)

Therefore, we can compute the difference between (G.6), (G.8), and (G.9) to bound (G.3), and use the fact that \(_{y}g(x,y^{*})+(^{*})^{}_{y}h(x,y^{*})=0\):

\[}{dx}^{}_{y}f(x,y^{*}_{^{*}, })--\] \[= _{yx}^{2}g+(^{*})^{}_{yx}^{ 2}h\\ (^{*}_{})_{x}h_{}^{ }(H^{-1})^{}_{1}_{y}g(x,y^{*}_{ ^{*},})-_{y}g(x,y^{*})-_{yy}^{2}g(x,y^{*} )(y^{*}_{^{*},}-y^{*})\\ 0\] (G.10) \[+_{1}_{y}h(x,y^{*}_{^{*}, })^{}^{*}-_{y}h(x,y^{*})^{}^{*}- _{yy}^{2}h(x,y^{*})^{}^{*}(y^{*}_{^{*},}-y^{*})\] (G.11) \[-_{1}0\\ _{y}h_{}(x,y^{*})(y^{*}_{^{*},}-y^{*}) \] (G.12) \[+_{2}_{y}h_{}(x,y^{*}_{ ^{*},})^{}h_{}(x,y^{*}_{^{*}, })\\ 0-_{y}h_{}(x,y^{*})^{}h_{ }(x,y^{*}_{^{*},})\\ 0\] (G.13)

The terms in (G.10) and (G.11) can both be bounded by \(_{1}C_{g}L_{y}\|y^{*}_{^{*},}-y^{*}\|^{2}\) and \(_{1}RC_{h}L_{y}\|y^{*}_{^{*},}-y^{*}\|^{2}\) by the smoothness of \(g\) and \(h^{}^{*}\). Further, plugging in \(\|y^{*}-y^{*}_{^{*},}\| O(_{1}^{-1})\) from Lemma 5.1 bounds both these terms by \(O(_{1}^{-1})\).

To bound the term in (G.12), we use:

\[\|h_{}(x,y_{^{*},}^{*})-h_{}(x,y^{*}) -_{y}h_{}(x,y^{*})(y_{^{*},}^{*}-y^{*}) \| C_{h}\|y_{^{*},}^{*}-y^{*}\|^{2}.\]

Therefore, we have:

\[\|_{y}h_{}(x,y^{*})(y_{^{*},}^{*}-y^{*})\| \|h_{}(x,y_{^{*},}^{*}) \|+\|h_{}(x,y^{*})\|+C_{h}O(\|y_{^{*}, }^{*}-y^{*}\|^{2})\] \[ O(_{1}^{-1/2}_{2}^{-1/2})+0+O(_{1}^{-2})\] \[=O(_{1}^{-1/2}_{2}^{-1/2}+_{1}^{-2}),\]

which upon scaling by \(_{1}\) gives us the following bound on the term in (G.12):

\[_{1}\|_{y}h_{}(x,y^{*})(y_{^{*},}^{*}-y^{*})\| O(_{1}^{1/2}_{2}^{-1/2}+_{1} ^{-1})\;.\]

The term in (G.13) can be bounded by:

\[_{2}\|_{x}h_{}(x,y_{^{*},}^{*})^{}h_{}(x,y_{^{*},}^{*})-_ {x}h_{}(x,y^{*})^{}h_{}(x,y_{^{*}, }^{*})\|\] \[= _{2}\|_{x}h_{}(x,y_{^{*},}^{*})-_{x}h_{}(x,y^{*})\|O(\|h_{} (x,y_{,^{*}}^{*})\|)\] \[= _{2} O(_{1}^{-1})O(_{1}^{-1/2}_{2}^{- 1/2})\] \[= O(_{1}^{-3/2}_{2}^{1/2})\] (G.14)

**Bounding (G.4):** This can be easily bounded by the smoothness of \(g\) and \(h\), and the bound on the dual solution \(\|^{*}\| R\). Thus (G.4) can be bounded by \(R O(_{1}^{-1})=O(_{1}^{-1})\).

**Bounding (G.5):** By the same argument in (G.14), we get:

\[_{2}\|_{y}h_{}(x,y_{^{*},}^{*})^{}h_{}(x,y_{^{*},}^{*})-_ {y}h_{}(x,y^{*})^{}h_{}(x,y_{^{*},}^{*})\|\] \[ _{2}\|_{y}h_{}(x,y_{^{*},}^{*})-_{y}h_{}(x,y^{*})\|\|h_{} (x,y_{^{*},}^{*})\|\] \[= _{2} O(_{1}^{-1})O(_{1}^{-1/2}_{2}^{ -1/2})\] \[= O(_{1}^{-3/2}_{2}^{1/2})\;.\]

Combining all upper bounds gives the claimed bound. 

## Appendix H Proof of the main result (Theorem 5.3): convergence and computation cost

**Theorem 5.3**.: _Given any accuracy parameter \(>0\), Algorithm 4 outputs \(_{x}F(x)\) such that \(\|F(x)- F(x)\|\) within \((^{-1})\) gradient oracle evaluations._

Proof.: First, given the bound in Lemma 5.2, we choose \(_{1}=^{-2}\) and \(_{2}=^{-4}\) to ensure the inexactness of the gradient oracle is bounded by \(\). In the later analysis, we will still use \(_{1}\) and \(_{2}\) in the penalty function for clarity.

Now we estimate the computation cost of the inexact gradient oracle:

**Lower-level problem.** Given the oracle access to the optimal dual solution \(^{*}(x)\), we can recover the primal solution \(y^{*}(x)\) efficiently (e.g, by ). Therefore, we can use the primal and dual solutions to construct the penalty function \(_{^{*},}(x,y)\) in (5.4).

**Penalty function minimization problem.** The second main optimization problem is the penalty minimization problem in Line 4 of Algorithm 4. Recall from (5.4) that

\[_{,}(x,y)=f(x,y)+_{1}(g(x,y)+^{ }h(x,y)-g^{*}(x))+}{2}\|h_{}(x,y) \|^{2},\] (H.1)

where we use the approximate dual solution \(\) as opposed to the optimal dual solution \(^{*}\).

Given (H.1), we solve the penalty minimization problem:

\[y_{,}^{}(x)_{y}_{,}(x,y).\]

The penalty minimization is a unconstrained strongly convex optimization problem, which is known to have linear convergence rate. We further analyze its convexity and smoothness below to precisely estimate the computation cost:* The strong convexity of \(_{,}(x,y)\) is lower bounded by \(_{B}}{2}=O(_{1})\).
* The smoothness of \(_{,}(x,y)\) is dominated by the smoothness of \(_{2}\|h_{}(x,y)\|^{2}\) since \(_{2}_{1}\). By Lemma 5.2, we know that the optimal solution must lie in an open ball \(B(y^{*},O(1/_{1}))\) with center \(y^{*}\) (inner optimization primal solution) and a radius of the order of \(O(})\). This implies that we just need to search over a bounded feasible set of \(y\), which we can bound \(\|_{y}h(x,y)\| L_{h}\) and \(h(x,y) H\) within the bounded region \(y B(y^{*},O(1/_{1}))\). We can show that \(h^{2}\) is smooth (gradient Lipschitz) within the bounded region by the following: \[\|_{yy}^{2}h^{2}\|=\|h_{yy}^{2}h+_{y}h^{ }_{y}h\|\|h_{yy}^{2}h\|+\|_{y }h^{}_{y}h\| HC_{h}+L_{h}^{2}\] which also implies \(h_{Z}^{2}\) is also smooth (gradient Lipschitz). Therefore, \( h_{Z}^{2}\) is \((HC_{h}+L_{h}^{2})_{2}=O(_{2})\) smooth.

Choosing \(_{1}=}\) and \(_{2}=}\), the condition number of \(_{,}(x,y)\) becomes \(=O(_{2}/_{1})=O(})\). Therefore, by the linear convergence of gradient descent in strongly convex smooth optimization, the number of iterations needed to get to \(\) accuracy is \(O(}())=O(( ))\). Therefore, we can get a near optimal solution \(y^{}_{,}\) with inexactness \(\) in \(O()\) oracle calls.

**Computation cost and results.** Overall, for the inner optimization, we can invoke the efficient optimal dual solution oracle to get the optimal dual solution \(^{*}(x)\) and recover the optimal primal solution \(y^{*}(x)\) from there. For the penalty minimization, we need \(O()\) oracle calls to solve an unconstrained strongly convex smooth optimization problem to get to \(\) accuracy. In conclusion, combining everything in Appendix H, we run \(O()\) oracle calls to obtain an \(\) accurate gradient oracle to approximate the hyperobjective gradient \(_{x}F(x)\). This concludes the proof of Theorem 5.3. 

**Remark H.1**.: _The following analysis quantifies how the error in the optimal dual solution propagates to the inexact gradient estimate. This is not needed if such a dual solution oracle exists. But in practice, the oracle may come with some error, for which we bound the error._

**Bounding the error propagation in error in dual solution and the penalty minimization.** First, if we do not get an exact optimal dual solution, the error in the dual solution \(\) with \(\|-^{*}\|\) will slightly impact the analysis in Lemma 5.2. Specifically, in Appendix G, the approximate \(\) will impact the inexact gradient \(_{x}_{,}(x,y^{}_{,})\) computation and the analysis in (G.4) and (G.7). In (G.4), to change \(\) to \(^{*}\), we get an additional error:

\[_{1}_{x}h(x,y^{})^{}(- ^{*})-_{x}h(x,y^{}_{,})^{}(- ^{*})\] (H.2) \[= _{1}(_{x}h(x,y^{})-_{x}h(x,y^{}_{ ,}))^{}(-^{*})\] \[ _{1}C_{h}\|y^{}-y^{}_{,} \|(-^{*})\] \[ O(_{1}_{1}^{-1})=O(),\]

where the last inequality is due to \(\|y^{}-y^{}_{,}\| O(_{1}^{-1})\) that is based on a similar analysis in Lemma 5.1 with a near-optimal \(y^{}_{,}\) under \(^{2}=_{1}\) accuracy.

Therefore, the error incurred by inexact \(\) in (G.4) is at most \(O()\), which is of the same rate as the current gradient inexactness \(O()\).

In (G.7), the optimality holds approximately for the approximate \(\). Therefore, by the near optimality of \(y^{}_{,}\) (strongly convex optimization), we know that the following gradient is also \(\)-close to \(0\), i.e.,

\[\|_{y}f(x,y^{}_{,})+_{1 }(_{y}g(x,y^{}_{,})+_{y}h(x,y^{} _{,})^{}).\] (H.3) \[+_{2}_{y}h_{}(x,y^{}_{, })^{}h_{}(x,y^{}_{,})||,\]

whose inexactness matches the inexactness of the gradient oracle \(\), and thus we do not incur additional order of inexactness here.

Moreover, there is an additional error because we need \(^{*}\) as opposed to a near-optimal \(\) to make the analysis in Appendix G work. The error between using \(\) and \(^{*}\) in (H.3) can be bounded by:

\[\|_{y}h(x,y^{}_{,})^{}(- ^{*})\| L_{h},\] (H.4)

where we use the local Lipschitzness of the function \(h\) in an open ball near \(y^{*}\). Therefore, the additional error is also \(O()\), which matches the inexactness of the inexact gradient oracle.

Therefore, we conclude that in order to bound the inexactness of the gradient oracle, we just need an efficient inexact dual solution with \(\) accuracy.

## Appendix I Practical oracle to optimal (approximate) dual solution

Here we discuss how practical the assumption on the oracle access to the optimal dual solution is.

For linear inequality constraint \(h(x,y)=Ax-By-b\), the LL problem is a constrained strongly convex smooth optimization problem. To show that we can compute an approximate solution to the optimal dual solution for linear inequality constraints, we apply the result from :

**Corollary I.1** (Application of Corollary 3.1 in ).: _When \(h(x,y)=Ax-By+b\) is linear in \(y\), the primal and dual solutions can be written as:_

\[y^{*},^{*} =_{y}_{}g(x,y)+(^{*})^{}h(x,y)=g(x, y)-(^{*})^{}By+R(x)\] \[ y^{*},^{*} =_{y}_{}g(x,y)-(^{*})^{}By\] (I.1)

_where \(g\) is strongly convex in \(y\) and \(B\) is of full rank by Assumption 2.2. According to Corollary 3.1 from , the primal-dual gradient method guarantees a linear convergence. More precisely, in \(t=O()\) iterations, we get:_

\[\|y^{t}-y^{*}\|\|^{t}- ^{*}\|.\] (I.2)

Given Corollary I.1, we can efficiently approximate the primal and dual solutions up to high accuracy with \(O()\) oracle calls when the inequality constraints are linear. This gives us an efficient approximate oracle access to the dual solution.

**Remark I.2**.: _Under the assumption of an optimal dual solution oracle, all the analyses mentioned in Section 5 hold for the general convex inequality constraints. However, the main technical challenge is that the dual solution oracle for general convex inequality cannot be guaranteed in practice. In fact, to the best of our knowledge, there is no iterate convergence in the dual solution \(\) for general convex inequality constraints. Most of the literature in strongly-convex-concave saddle point convergence only guarantees dual solution convergence in terms of its duality gap or some other merit functions. We are not aware of any successful bound on the dual solution iterate convergence, which is an important research question to answer by itself. This is the main technical bottleneck for general convex inequality constraints as well._

**Remark I.3**.: _On the other hand, we need the dual solution iterate convergence with rate \(O(1/)\) to ensure the error to be bounded. But this is not a necessary condition. To ensure a bound on the error propagation, we just need to bound some forms of merit functions ((H.2) and (H.4)) of the dual solutions, which we believe that this is much more tractable than the actual iterate dual solution convergence. We leave this as a future direction and this will generalize the analysis from linear inequality constraints to general convex inequality constraints._

## Appendix J The role of \(^{*}(x)\) in the derivative of Equation (5.4)

Notice that Equation (5.4), we treat the dual solution \(^{*}(x)\) as a constant to define the penalty function derivative. Yet, the dual solution \(^{*}(x)\) is in fact also a function of \(x\). Therefore, in theory, we should also compute its derivative with respect to \(x\).

However, notice that the following:

\[_{x}(^{*}(x))^{}h(x,y)=_{x}h(x,y)^{} ^{*}+^{}h(x,y)\] (J.1)

The later term in Equation (J.1) can be divided into two cases:* For active constraint \(i\) with \(h(x,y^{*})=0\), we know that \(y^{*}_{,}\) is close to \(y^{*}\) by Lemma5.1. Therefore, the derivative \(\|^{}h(x,y^{*}_{,})\|  L_{h}L_{}_{1}=O(_{1})=O(^{2})\) by the local smoothness of \(h\) near \(y^{*}\) and the Lipschitzness assumption of \(^{*}\) in Assumption2.5.
* For inactive constraint \(i}\) and \(^{*}_{i}>0\), we can solve the KKT conditions and get \(=0\). Therefore, the second term becomes \(0\).
* For inactive constraint \(i}\) and \(^{*}_{i}=0\), the KKT system degenerates and we need to use subgradient. By solving the KKT system, we find that \(=0\) is a valid subgradient. Therefore, by choosing this subgradient, the second term also vanishes.

Therefore, we do not need to compute the derivative of \(^{*}\) as the terms involved its derivative is negligible compared to other major terms.

## Appendix K Experimental setup

All experiments were run on a computing cluster with Dual Intel Xeon Gold 6226 CPUs @ 2.7 GHz and DDR4-2933 MHz DRAM. No GPU was used, and we used 1 core with 8GB RAM per instance of the experiment. The cutoff time for running the algorithms is set to be 6 hours. All experiments were run and averaged over 10 different random seeds. All parameters in the constrained bilevel optimization in Section6, including the objective parameters and the constrain parameters, are randomly generated from a normal distribution with 0-mean and standard deviation \(1\).

For our fully first-order algorithm, we implement Algorithm3, where the inexact gradient oracle subroutine is provided by implementing Algorithm4. All algorithms are implemented in PyTorch  to compute gradients, and using Cvxpy  to solve the LL problem and the penalty minimization problem. We implement our fully first-order method based on the solutions returned by Cvxpy with certain accuracy requirement, and use PyTorch to compute the inexact gradient discussed in Section5. We implement the non-fully first-order method using the CvxpyLayer , which is a Cvxpy compatible library that can differentiate through the LL convex optimization problem.

## Appendix L Additional Experimental Results

We generate instances of the following constrained bilevel optimization problem:

\[_{x}\ c^{}y^{*}+0.01\|x\|^{2}+0.01\|y^{* }\|^{2}\ \ \ \ y^{*}=*{arg\,min}_{y:h(x,y) 0}y^{ }Qy+x^{}Py,\] (L.1)

where \(h_{i}(x,y)=x^{}A_{i}y-b_{i}^{}x\  i[d_{h}]\) is a \(d_{h}\)-dim bilinear constraint, where the constraint bilinear matrix \(A_{i}^{d_{x} d_{y}}\), \(b_{i}^{d_{x}}\) for all \(i[d_{h}]\) are randomly generated from normal

Figure 2: We run Algorithm3 using Algorithm4 on the bilevel optimization in the toy example in ProblemL.1 with varying upper-level variable dimensions \(d_{x}\), a fixed lower-level variable dimension \(d_{y}=200\), and the number of constraints \(n_{}=d_{y}/5=40\), and accuracy \(=0.1\). Figure2(a), Figure2(b), Figure2(c) vary # of iterations, gradient exactness \(\), and \(d_{y}\), respectively, to compare the performance under different settings.

distributions. The bilinear (nonlinear) constraint of the lower-level problem is the major difference compared to the experiment in Section 6. We are interested in whether our algorithms work beyond the linear constraints where our theory guarantees.

The rest of the parameters are the same as in Section 6. The PSD matrix \(Q^{d_{y} d_{y}}\), \(c^{d_{y}}\), \(P^{d_{x} d_{y}}\). We compare our Algorithm 3 with a non-fully first-order method implemented using cvxpyLayer. Both algorithms use Adam  to control the learning rate in gradient descent. All the experiments are averaged over ten random seeds.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide algorithms and corresponding theoretical guarantees for all our claims in the abstract. We provide experiments (and relevant code) as claimed. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: This is discussed in Section 7 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions, theorem statements, and proof sketches are included in the main paper. The full proofs are included in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide our full code in the supplemental material, and it can be used to reproduce the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is included in the supplemental material and can be used to reproduce the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide this information in Appendix K. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: We provide this in Section 6 Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide this information in Appendix K. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [NA] Justification: There is no societal impact of this work. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or research with human subjects is involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.