# Accelerated Quasi-Newton Proximal Extragradient:

Faster Rate for Smooth Convex Optimization

 Ruichen Jiang

ECE Department

The University of Texas at Austin

rjiang@utexas.edu

&Aryan Mokhtari

ECE Department

The University of Texas at Austin

mokhtari@austin.utexas.edu

###### Abstract

In this paper, we present an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective function, we prove that our method can achieve a convergence rate of \(\{},}}{k^{2} 5}\} \), where \(d\) is the problem dimension and \(k\) is the number of iterations. In particular, in the regime where \(k=(d)\), our method matches the _optimal rate_ of \((})\) by Nesterov's accelerated gradient (NAG). Moreover, in the the regime where \(k=(d d)\), it outperforms NAG and converges at a _faster rate_ of \((}}{k^{2} 5})\). To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices.

## 1 Introduction

In this paper, we consider the following unconstrained convex minimization problem

\[_{^{d}} f(),\] (1)

where the objective function \(f:^{d}\) is convex and differentiable. We are particularly interested in quasi-Newton methods, which are among the most popular iterative methods for solving the problem in (1) . Like gradient descent and other first-order methods, quasi-Newton methods require only the objective's gradients to update the iterates. On the other hand, they can better exploit the local curvature of \(f\) by constructing a Hessian approximation matrix and using it as a preconditioner, leading to superior convergence performance. In particular, when the objective function in (1) is strictly convex or strongly convex, it has long been proved that quasi-Newton methods achieve an asymptotic superlinear convergence rate , which significantly improves the linear convergence rate obtained by first-order methods. More recently, there has been progress on establishing a local non-asymptotic superlinear rate of the form \(((1/)^{k})\) for classical quasi-Newton methods and their variants .

However, all of the results above only apply under the restrictive assumption that the objective function \(f\) is strictly or strongly convex. In the more general setting where \(f\) is merely convex, to the best of our knowledge, there is no result that demonstrates any form of convergence improvement by quasi-Newton methods over first-order methods. More precisely, it is well known that Nesterov's accelerated gradient (NAG)  can achieve a convergence rate of \((1/k^{2})\) if \(f\) is convex and has Lipschitz gradients. On the other hand, under the same setting, asymptotic convergence ofclassical quasi-Newton methods has been shown in  but no explicit rate is given. With certain conditions on the Hessian approximation matrices, the works in  presented quasi-Newton-type methods with convergence rates of \((1/k)\) and \((1/k^{2})\), respectively, which are no better than the rate of NAG and, in fact, can be even worse in terms of constants. This gap raises the following fundamental question:

_Can we design a quasi-Newton-type method that achieves a convergence rate faster than \((1/k^{2})\) for the smooth convex minimization problem in (1)?_

At first glance, this may seem impossible, as for any first-order method that has access only to a gradient oracle, one can construct a "worst-case" instance and establish a lower bound of \((1/k^{2})\) on the optimality gap . It is worth noting that while such a lower bound is typically shown under a "linear span" assumption, i.e., the methods only query points in the span of the gradients they observe, this assumption is in fact not necessary and can be removed by the technique of resisting oracle (see, [30, Section 3.3]). In particular, this \((1/k^{2})\) lower bound applies for any iterative method that only queries gradients of the objective, including quasi-Newton methods. On the other hand, this lower bound is subject to a crucial assumption: it only works in the high-dimensional regime where the problem dimension \(d\) exceeds the number of iterations \(k\). As such, it does not rule out the possibility of a faster rate than \((1/k^{2})\) when the number of iterations \(k\) is larger than \(d\). Hence, ideally, we are looking for a method that attains the optimal rate of \(O(1/k^{2})\) in the regime that \(k=(d)\) and surpasses this rate in the regime that \(k=(d)\).

**Contributions.** In this paper, we achieve the above goal by presenting an accelerated quasi-Newton proximal extragradient (A-QNPE) method. Specifically, under the assumptions that \(f\) in (1) is convex and its gradient and Hessian are Lipschitz, we prove the following guarantees:

* From any initialization, A-QNPE can attain a global convergence rate of \(\{},}{k^{2.5}}\} \). In particular, this implies that our method matches the optimal rate of \((})\) when \(k=(d)\), while it converges at a faster rate of \((}{k^{2.5}})\) when \(k=(d d)\). Alternatively, we can bound the number of iterations required to achieve an \(\)-accurate solution by \(N_{}=\{},}{e^{0.4 }}(})^{0.2}\}\).
* In terms of computational cost, we show that the total number of gradient queries after \(N\) iterations can be bounded by \(3N\), i.e., on average no more than \(3\) per iteration. Moreover, the number of matrix-vector products to achieve an \(\)-accurate solution can be bounded by \(}\{}{e^{0.3}},} \}\).

Combining the two results above, we conclude that A-QNPE requires \(\{},}{e^{0.4}}( {^{2}})^{0.2}\}\) gradient queries to reach an \(\)-accurate solution, which is at least as good as NAG and is further superior when \(=^{2}(d)}\). To the best of our knowledge, this is the first result that demonstrates a provable advantage of a quasi-Newton-type method over NAG in terms of gradient oracle complexity in the smooth convex setting.

To obtain these results, we significantly deviate from the classical quasi-Newton methods such as BFGS and DFP. Specifically, instead of mimicking Newton's method as in the classical updates, our A-QNPE method is built upon the celebrated Monteiro-Svaiter (MS) acceleration framework , which can be regarded as an inexact version of the accelerated proximal point method . Another major difference lies in the update rule of the Hessian approximation matrix. Classical quasi-Newton methods typically perform a low-rank update of the Hessian approximation matrix while enforcing the secant condition. On the contrary, our update rule is purely driven by our convergence analysis of the MS acceleration framework. In particular, inspired by , we assign certain loss functions to the Hessian approximation matrices and formulate the Hessian approximation matrix update as an online convex optimization problem in the space of matrices. Therefore, we propose to update the Hessian approximation matrices via an online learning algorithm.

**Related work.** The authors in  proposed a refined MS acceleration framework, which simplifies the line search subroutine in the original MS method . By instantiating it with an adaptive _second-order oracle_, they presented an accelerated second-order method that achieves the optimal rate of \((})\). The framework in  serves as a basis for our method, but we focus on the setting where we have access only to a _gradient oracle_ and we consider a quasi-Newton-type update. Another closely related work is , where the authors proposed a quasi-Newton proximal extragradient method with a global non-asymptotic superlinear rate. In particular, our Hessian approximation update is inspired by the online learning framework in . On the other hand, the major difference is that the authors in  focused on the case where \(f\) is strongly convex and presented a global superlinear rate, while we consider the more general convex setting where \(f\) is only convex (may not be strongly convex). Moreover, we further incorporate the acceleration mechanism into our method, which greatly complicates the convergence analysis; see Remark 2 for more discussions.

Another class of optimization algorithms with better gradient oracle complexities than NAG are cutting plane methods [36; 37; 38; 39; 40; 41; 42; 43; 44], which are distinct from quasi-Newton methods we study in this paper. In particular, in the regime where \(=}(})\), they can achieve the optimal gradient oracle complexity of \((d)\). On the other hand, in the regime where \(=(})\), the complexity of the cutting plane methods is worse than NAG, while our proposed method matches the complexity of NAG.

## 2 Preliminaries

Next, we formally state the required assumptions for our main results.

**Assumption 1**.: _The function \(f\) is twice differentiable, convex, and \(L_{1}\)-smooth. As a result, we have \(0^{2}f() L_{1}\) for any \(^{d}\), where \(^{d d}\) is the identity matrix._

**Assumption 2**.: _The Hessian of \(f\) is \(L_{2}\)-Lipschitz, i.e., we have \(\|^{2}f()-^{2}f()\|_{} L_{2} \|-\|_{2}\) for any \(,^{d}\), where \(\|\|_{}_{:\|\|_{2}=1} \|\|_{2}\)._

We note that both assumptions are standard in the optimization literature and are satisfied by various loss functions such as the logistic loss and the log-sum-exp function (see, e.g., ).

_Remark 1_.: We note that the additional assumption of Lipschitz Hessian does not alter the lower bound of \((1/k^{2})\) that we discussed in the introduction. Indeed, this lower bound is established by a worst-case quadratic function, whose Hessian is constant (Lipschitz continuous with \(L_{2}=0\)). Therefore, Assumption 2 does not eliminate this worst-case construction from the considered problem class, and thus the lower bound also applies to our setting.

**Monteiro-Svaiter acceleration.** As our proposed method uses ideas from the celebrated Monteiro-Svaiter (MS) acceleration algorithm , we first briefly recap this method. MS acceleration, also known as accelerated hybrid proximal extragradient (A-HPE), consists of intertwining sequences of iterates \(\{_{k}\},\{_{k}\},\{_{k}\}\), scalar variables \(\{a_{k}\}\) and \(\{A_{k}\}\) as well as step sizes \(\{_{k}\}\). The algorithm has three main steps. In the first step, we compute the auxiliary iterate \(_{k}\) according to

\[_{k}=}{A_{k}+a_{k}}_{k}+}{A_{k}+a_{k }}_{k}, a_{k}=+^{ 2}+4_{k}A_{k}}}{2}.\] (2)

In the second step, an inexact proximal point step \(_{k+1}_{k}-_{k} f(_{k+1})\) is performed. To be precise, given a parameter \([0,1)\), we find \(_{k+1}\) that satisfies

\[\|_{k+1}-_{k}+_{k} f(_{k+1})\| \|_{k+1}-_{k}\|.\] (3)

Then in the third step, the iterate \(\) is updated by following the update

\[_{k+1}=_{k}-a_{k} f(_{k+1}).\]

Finally, we update the scalar \(A_{k+1}\) by \(A_{k+1}=A_{k}+a_{k}\). The above method has two implementation issues. First, to perform the update in (3) directly, one needs to solve the nonlinear system of equations \(-_{k}+_{k} f()=0\) to a certain accuracy, which could be costly in general. To address this issue, a principled approach is to replace the gradient operator \( f()\) with a simpler approximation function \(P(;_{k})\) and select \(_{k+1}\) as the (approximate) solution of the equation:

\[_{k+1}-_{k}+_{k}P(_{k+1};_{k})=0.\] (4)

For instance, we can use \(P(;_{k})= f(_{k})\) and accordingly (4) is equivalent to \(_{k+1}=_{k}-_{k} f(_{k})\), leading to the accelerated first-order method in . If we further have access to the Hessian oracle, we can use \(P(;_{k})= f(_{k})+^{2}f(_ {k})(-_{k})\) and (4) becomes \(_{k+1}=_{k}-_{k}(+_{k}^{2}f( _{k}))^{-1} f(_{k})\), leading to the second-order method in .

However, approximating \( f()\) by \(P(;_{k})\) leads to a second issue related to finding a proper step size \(_{k}\). More precisely, one needs to first select \(_{k}\), compute \(_{k}\) from (2), and then solve the system in (4) exactly or approximately to obtain \(_{k+1}\). However, these three variables, i.e., \(_{k+1}\), \(_{k}\) and \(_{k}\) may not satisfy the condition in (3) due to the gap between \( f()\) and \(P(;_{k})\). If that happens, we need to re-select \(_{k}\) and recalculate both \(_{k}\) and \(_{k+1}\) until the condition in (3) is satisfied. Toaddress this issue, several bisection subroutines have been proposed in the literature [31; 45; 46; 47] and they all incur a computational cost of \((1/)\) per iteration.

**Optimal Monteiro-Svaiter acceleration.** A recent paper  refines the MS acceleration algorithm by separating the update of \(_{k}\) from the line search subroutine. In particular, in the first stage, we use \(_{k}\) to compute \(a_{k}\) and then \(_{k}\) from (2), which will stay fixed throughout the line search scheme. In the second stage, we aim to find a pair \(}_{k+1}\) and \(_{k}\) such that they satisfy

\[\|}_{k+1}-_{k}+_{k} f(}_{k+1})\|\|}_{k+1}-_{k}\|.\] (5)

To find that pair, we follow a similar line search scheme as above, with the key difference that \(_{k}\) is fixed and \(_{k}\) can be different from \(_{k}\) that is used to compute \(_{k}\). More precisely, for a given \(_{k}\), we find the solution of (4) denoted by \(}_{k+1}\) and check whether it satisfies (5) or not. If it does not, then we adapt the step size and redo the process until (5) is satisfied. Then given the values of these two parameters \(_{k}\) and \(_{k}\), the updates for \(\) and \(\) would change as we describe next:

* If \(_{k}_{k}\), we update \(_{k+1}=}_{k+1}\), \(A_{k+1}=A_{k}+a_{k}\) and \(_{k+1}=_{k}-a_{k} f(}_{k+1})\). Moreover, we increase the next tentative step size by choosing \(_{k+1}=_{k}/\) for some \((0,1)\).
* Otherwise, if \(_{k}<_{k}\), the authors in  introduced a _momentum damping mechanism_. Define \(_{k}=_{k}/_{k}<1\). We then choose \(_{k+1}=)A_{k}}{A_{k}+_{k}a_{k}}_ {k}+(A_{k}+a_{k})}{A_{k}+_{k}a_{k}}}_{k+1}\), which is a convex combination of \(_{k}\) and \(}_{k+1}\). Moreover, we update \(A_{k+1}=A_{k}+_{k}a_{k}\) and \(_{k+1}=_{k}-_{k}a_{k} f(}_{k+1})\). Finally, we decrease the next tentative step size by choosing \(_{k+1}=_{k}\).

This approach not only simplifies the procedure by separating the update of \(\{_{k}\}\) from the line search scheme, but it also shaves a factor of \((1/)\) from the computational cost of the algorithm, leading to optimal first and second-order variants of the MS acceleration method. Therefore, as we will discuss in the next section, we build our method upon this more refined MS acceleration framework.

## 3 Accelerated Quasi-Newton Proximal Extragradient

In this section, we present our accelerated quasi-Newton proximal extragradient (A-QNPE) method. An informal description of our method is provided in Algorithm 1. On a high level, our method can be viewed as the quasi-Newton counterpart of the adaptive Monteiro-Svaiter-Newton method proposed in . In particular, we only query a gradient oracle and choose the approximation function in (4) as \(P(;_{k})= f(_{k})+_{k}(-_{k})\), where \(_{k}\) is a Hessian approximation matrix obtained only using gradient information. Moreover, another central piece of our method is the update scheme of \(_{k}\). Instead of following the classical quasi-Newton updates such as BFGS or DFP, we use an online learning framework, where we choose a sequence of matrices \(_{k}\) to achieve a small dynamic regret for an online learning problem defined by our analysis; more details will be provided later in Section 3.2. We initialize our method by choosing \(_{0},_{0}^{d}\) and setting \(A_{0}=0\) and \(_{0}=_{0}\), where \(_{0}\) is a user-specified parameter. Our method can be divided into the following four stages:

* In the **first stage**, we compute the scalar \(a_{k}\) and the auxiliary iterate \(_{k}\) according to (2) using the step size \(_{k}\). Note that \(_{k}\) is then fixed throughout the \(k\)-th iteration.
* In the **second stage**, given the Hessian approximation matrix \(_{k}\) and the iterate \(_{k}\), we use a line search scheme to find the step size \(_{k}\) and the iterate \(}_{k+1}\) such that \[\|}_{k+1}-_{k}+_{k}( f( _{k})+_{k}(}_{k+1}-_{k}))\| _{1}\|}_{k+1}-_{k}\|,\] (6) \[\|}_{k+1}-_{k}+_{k} f( }_{k+1})\|(_{1}+_{2})\|}_{k+1} -_{k}\|,\] (7) where \(_{1}[0,1)\) and \(_{2}(0,1)\) are user-specified parameters with \(_{1}+_{2}<1\). The first condition in (6) requires that \(}_{k+1}\) inexactly solves the linear system of equations \((+_{k}_{k})(-_{k})+_{k} f(_{k})=0\), where \(_{1}[0,1)\) controls the accuracy. As a special case, we have \(}_{k+1}=_{k}-(+_{k}_ {k})^{-1} f(_{k})\) when \(_{1}=0\). The second condition in (7) directly comes from (5) in the optimal MS acceleration framework, which ensures that we approximately follow the proximal point step \(}_{k+1}=_{k}-_{k} f(}_{k +1})\). To find the pair \((_{k},}_{k+1})\) satisfying both (6) and (7), we implement a backtracking line search scheme. Specifically, for some \((0,1)\), we iteratively try \(_{k}=_{k}^{i}\) for \(i 0\) and solve \(}_{k+1}\) from (6) until the condition in (7) is satisfied. The line search scheme will be discussed in more detail in Section 3.1.

* In the **third stage**, we update the variables \(_{k+1}\), \(_{k+1}\), \(A_{k+1}\) and set the step size \(_{k+1}\) in the next iteration. Specifically, the update rule we follow depends on the outcome of the line search scheme. In the first case where \(_{k}=_{k}\), i.e., the line search scheme accepts the initial trial step size, we let \[_{k+1}=}_{k+1},_{k+1}=_{k}-a _{k} f(}_{k+1}), A_{k+1}=A_{k}+a_{k},\] (8) as in the original MS acceleration framework. Moreover, this also suggests our choice of the step size \(_{k}\) may be too conservative. Therefore, we increase the step size in the next iteration by \(_{k+1}=_{k}/\). In the second case where \(_{k}<_{k}\), i.e., the line search scheme backtracks, we adopt the momentum damping mechanism in : \[_{k+1}=)A_{k}}{A_{k}+_{k}a_{k}}_ {k}+(A_{k}+a_{k})}{A_{k}+_{k}a_{k}}}_{k+ 1},\ _{k+1}=_{k}-_{k}a_{k} f(}_{k+ 1}),\ A_{k+1}=A_{k}+_{k}a_{k},\] (9) where \(_{k}=_{k}/_{k}<1\). Accordingly, we decrease the step size in the next iteration by letting \(_{k+1}=_{k}\) (note that \(_{k}<_{k}\)).
* In the **fourth stage**, we update the Hessian approximation matrix \(_{k+1}\). Inspired by , we depart from the classical quasi-Newton methods and instead let the convergence analysis guide our update scheme. As we will show in Section 3.2, the convergence rate of our method is closely related to the cumulative loss \(_{k}_{k}(_{k})\) incurred by our choices of \(\{_{k}\}\), where \(=\{k:_{k}<_{k}\}\) denotes the indices where the line search scheme backtracks. Moreover, the loss function has the form \(_{k}(_{k})_{k}-_{k}_{k}\|^{2}}{\|_{k}\|^{2}}\), where \(_{k} f(}_{k})- f(_{ k})\), \(_{k}}_{k}-_{k}\) and \(}_{k}\) is an auxiliary iterate returned by our line search scheme. Thus, this motivates us to employ an online learning algorithm to minimize the cumulative loss. Specifically, in the first case where \(_{k}=_{k}\) (i.e., \(k\)), the current Hessian approximation matrix \(_{k}\) does not contribute to the cumulative loss and thus we keep it unchanged (cf. Line 9). Otherwise, we follow an online learning algorithm in the space of matrices. The details will be discussed in Section 3.2.

Finally, we provide a convergence result in the following Proposition for Algorithm 1, which serves as the basis for our convergence analysis. We note that the following results do not require additional conditions on \(_{k}\) other than the ones in (6) and (7). The proof is available in Appendix A.1.

**Proposition 1**.: _Let \(\{_{k}\}_{k=0}^{N}\) be the iterates generated by Algorithm 1. If \(f\) is convex, we have_

\[f(_{N})-f(^{*})_{0}-^{*} \|^{2}}{2A_{N}} A_{N})^{2}}{4(2- )^{2}}(_{k=0}^{N-1}_{k}})^{2}.\]Proposition 1 characterizes the convergence rate of Algorithm 1 by the quantity \(A_{N}\), which can be further lower bounded in terms of the step sizes \(\{_{k}\}\). Moreover, we can observe that larger step sizes will lead to a faster convergence rate. On the other hand, the step size \(_{k}\) is constrained by the condition in (7), which, in turn, depends on our choice of the Hessian approximation matrix \(_{k}\). Thus, the central goal of our line search scheme and the Hessian approximation update is to make the step size \(_{k}\) as large as possible, which we will describe next.

### Line Search Subroutine

In this section, we specify our line search subroutine to select the step size \(_{k}\) and the iterate \(}_{k+1}\) in the second stage of A-QNPE. For simplicity, denote \( f(_{k})\) by \(\) and drop the subscript \(k\) in \(_{k}\) and \(_{k}\). In light of (6) and (7), our goal in the second stage is to find a pair \((,}_{+})\) such that

\[\|}_{+}-+(+(}_{+}-))\|_{1}\|}_{+}- \|,\] (10) \[\|}_{+}-+ f(}_{+})\|(_{1}+_{2})\|}_{+}-\|.\] (11)

As mentioned in the previous section, the condition in (10) can be satisfied by solving the linear system \((+)(}_{+}-)=- \) to a desired accuracy. Specifically, we let

\[_{+}=(+,-;_{1})}_{+}=+_{+},\] (12)

where the oracle \(\) is defined as follows.

**Definition 1**.: _The oracle \((,;)\) takes a matrix \(_{+}^{d}\), a vector \(^{d}\) and \((0,1)\) as input, and returns an approximate solution \(_{+}\) satisfying \(\|_{+}-\|\|_{+}\|\)._

The most direct way to implement \((,;)\) is to compute \(_{+}=^{-1}\), which however costs \((d^{3})\) arithmetic operations. Alternatively, we can implement the oracle more efficiently by using the conjugate residual method , which only requires computing matrix-vector products and thus incurs a cost of \((d^{2})\). The details are discussed in Appendix E.1. We characterize the total number of required matrix-vector products for this oracle in Theorem 2.

Now we are ready to describe our line search scheme with the \(\) oracle (see also Subroutine 1 in Appendix B). Specifically, we start with the step size \(\) and then reduce it by a factor \(\) until we find a pair \((,}_{+})\) that satisfies (11). It can be shown that the line search scheme will terminate in a finite number of steps and return a pair \((,}_{+})\) satisfying both conditions in (10) and (11) (see Appendix B.1). Regarding the output, we distinguish two cases: (i) If we pass the test in (11) on our first attempt, we accept the initial step size \(\) and the corresponding iterate \(}_{+}\) (cf. Line 10 in Subroutine 1). (ii) Otherwise, along with the pair \((,}_{+})\), we also return an auxiliary iterate \(}_{+}\) that we compute from (12) using the rejected step size \(/\) (cf. Line 12 in Subroutine 1). As we shall see in Lemma 1, the iterate \(}_{+}\) is used to derive a lower bound on \(\), which will be the key to our convergence analysis and guide our update of the Hessian approximation matrix. For ease of notation, let \(\) be the set of iteration indices where the line search scheme backtracks, i.e., \(\{k:_{k}<_{k}\}\).

**Lemma 1**.: _For \(k\) we have \(_{k}=_{k}\), while for \(k\) we have_

\[_{k}>\|}_{k+1}-_{k}\|} {\| f(}_{k+1})- f(_{k})-_{k}( }_{k+1}-_{k})\|}\|}_{k+1}-_{k}\|)}{(1-_{1})}\|}_{k+1}-_{k}\|.\] (13)

Lemma 1 provides a lower bound on the step size \(_{k}\) in terms of the approximation error \(\| f(}_{k+1})- f(_{k})-_{k}( }_{k+1}-_{k})\|\). Hence, a better Hessian approximation matrix \(_{k}\) leads to a larger step size, which in turn implies faster convergence. Also note that the lower bound uses the auxiliary iterate \(}_{k+1}\) that is not accepted as the actual iterate. Thus, the second inequality in (13) will be used to relate \(\|}_{k+1}-_{k}\|\) with \(\|}_{k+1}-_{k}\|\). Finally, we remark that to fully characterize the computational cost, we need to upper bound the total number of line search steps, each of which requires a call to \(\) and a call to the gradient oracle. This will be discussed in Theorem 2.

### Hessian Approximation Update via Online Learning with Dynamic Regret

In this section, we discuss how to update the Hessian approximation matrix \(_{k}\) in the fourth stage of A-QNPE. As mentioned earlier, instead of following the classical quasi-Newton updates, we directly motivate our update policy for \(_{k}\) from the convergence analysis. The first step is to connect the convergence rate of A-QNPE with the Hessian approximation matrices \(\{_{k}\}\). By Proposition 1, if we define the absolute constant \(C_{1})^{2}}{(1-)^{2}}\), then we can write

\[f(_{N})-f(^{*})_{0}- ^{*}\|^{2}}{2A_{N}}\|_{0}-^{*}\|^ {2}}{(_{k=0}^{N-1}_{k}})^{2}}\| _{0}-^{*}\|^{2}}{N^{2.5}}^{N-1}_{k}^{2}}},\] (14)

where the last inequality follows from Holder's inequality. Furthermore, we can establish an upper bound on \(_{k=0}^{N-1}_{k}^{2}}\) in terms of the Hessian approximation matrices \(\{_{k}\}\), as we show next.

**Lemma 2**.: _Let \(\{_{k}\}_{k=0}^{N-1}\) be the step sizes in Algorithm 1 using Subroutine 1. Then we have_

\[_{k=0}^{N-1}_{k}^{2}} }{(1-^{2})_{0}^{2}}+}{(1-^{2})_{2}^{2} ^{2}}_{0 k N-1,k}_{k}- _{k}_{k}\|^{2}}{\|_{k}\|^{2}},\] (15)

_where \(_{k} f(}_{k+1})- f(_{k})\) and \(_{k}}_{k+1}-_{k}\) for \(k\)._

The proof of Lemma 2 is given in Appendix C.1. On a high level, for those step sizes \(_{k}\) with \(k\), we can apply Lemma 1 and directly obtain a lower bound in terms of \(_{k}\). On the other hand, for \(k\), we have \(_{k}=_{k}\) and our update rule in Lines 8 and 13 of Algorithm 1 allows us to connect the sequence \(\{_{k}\}_{k=0}^{N-1}\) with the backtracked step sizes \(\{_{k}:\,k\}\). As a result, we note that the sum in (15) only involves the Hessian approximation matrices \(\{_{k}:\,k\}\).

In light of (14) and (15), our update for \(_{k}\) aims to make the right-hand side of (15) as small as possible. To achieve this, we adopt the online learning approach in  and view the sum in (15) as the cumulative loss incurred by our choice of \(\{_{k}\}\). To formalize, define the loss at iteration \(k\) by

\[_{k}()0,&k,\\ _{k}-_{k}\|^{2}}{\|_{k}\|^{2 }},&,\] (16)

and consider the following online learning problem: (i) At the \(k\)-th iteration, we choose \(_{k}\) where \(\{_{+}^{d}:0  L_{1}\}\); (ii) We receive the loss function \(_{k}()\) defined in (16); (iii) We update our Hessian approximation matrix to \(_{k+1}\). Therefore, we propose to employ an online learning algorithm to update the Hessian approximation matrices \(\{_{k}\}\), and the task of proving a convergence rate for our A-QNPE algorithm boils down to analyzing the performance of our online learning algorithm. In particular, an upper bound on the cumulative loss \(_{k=0}^{N-1}_{k}(_{k})\) will directly translate into a convergence rate for A-QNPE by using (14) and (15).

Naturally, the first idea is to update \(_{k}\) by following projected online gradient descent . While this approach would indeed serve our purpose, its implementation could be computationally expensive. Specifically, like other projection-based methods, it requires computing the Euclidean projection onto the set \(\) in each iteration, which in our case amounts to performing a full \(d d\) matrix eigendecomposition and would incur a cost of \((d^{3})\) (see Appendix C.2). Inspired by the recent work in , we circumvent this issue by using a projection-free online learning algorithm, which relies on an approximate separation oracle for \(\) instead of a projection oracle. For simplicity, we first translate and rescale the set \(\) via the transform \(}=}(-}{2})\) to obtain \(}\{}^{d}:\|}\|_{} 1\}\). The approximate separation oracle \((;,q)\) is then defined as follows.

**Definition 2**.: _The oracle \((;,q)\) takes a symmetric matrix \(^{d}\), \(>0\), and \(q(0,1)\) as input and returns a scalar \(>0\) and a matrix \(^{d}\) with one of the following possible outcomes:_

* _Case I:_ \( 1\)_, which implies that, with probability at least_ \(1-q\)_,_ \(}\)_;_
* _Case II:_ \(>1\)_, which implies that, with probability at least_ \(1-q\)_,_ \(/}\)_,_ \(\|\|_{F} 3\) _and_ \(,-}-1-\) _for any_ \(}\) _such that_ \(}}\)_._

To sum up, \((;,q)\) has two possible outcomes: with probability \(1-q\), either it certifies that \(}\), or it produces a scaled version of \(\) that belongs to \(}\) and an approximate separation hyperplane between \(\) and the set \(}\). As we show in Appendix E.2, implementing this oracle requires computing the two extreme eigenvectors and eigenvalues of the matrix \(\) inexactly, which can be implemented efficiently by the randomized Lanczos method .

Building on the \((;,q)\) oracle, we design a projection-free online learning algorithm adapted from [35, Subroutine 2]. Since the algorithm is similar to the one proposed in , we relegate the details to Appendix C but sketch the main steps in the analysis. To upper bound the cumulative loss \(_{k=0}^{N-1}_{k}(_{k})\), we compare the performance of our online learning algorithm against a sequence of reference matrices \(\{_{k}\}_{k=0}^{N-1}\). Specifically, we aim to control the _dynamic regret_ defined by \(_{N}(\{_{k}\}_{k=0}^{N-1}) _{k=0}^{N-1}(_{k}(_{k})-_{k}(_{k}))\), as well as the the cumulative loss \(_{k=0}^{N-1}_{k}(_{k})\) by the reference sequence. In particular, in our analysis we show that the choice of \(_{k}^{2}f(_{k})\) for \(k=0,,N-1\) allows us to upper bound both quantities.

_Remark 2_.: While our online learning algorithm is similar to the one in , our analysis is more challenging due to the lack of strong convexity. Specifically, since \(f\) is assumed to be strongly convex in , the iterates converge to \(^{*}\) at least linearly, resulting in less variation in the loss functions \(\{_{k}\}\). Hence, the authors in  let \(_{k}=^{*}^{2}f(^{*})\) for all \(k\) and proved that \(_{k=0}^{N-1}_{k}(^{*})\) remains bounded. In contrast, without linear convergence, we need to use a time-varying sequence \(\{_{k}\}\) to control the cumulative loss. This in turn requires us to bound the variation \(_{k=0}^{N-2}\|_{k+1}-_{k}\|_{F}\), which involves a careful analysis of the stability property of the sequence \(\{_{k}\}\) in Algorithm 1.

## 4 Complexity Analysis of A-QNPE

In this section, we present our main theoretical results: we establish the convergence rate of A-QNPE (Theorem 1) and characterize its computational cost in terms of gradient queries and matrix-vector product evaluations (Theorem 2). The proofs are provided in Appendices D and E.3.

**Theorem 1**.: _Let \(\{_{k}\}\) be the iterates generated by Algorithm 1 using the line search scheme in Section 3.1, where \(_{1},_{2}(0,1)\) with \(_{1}+_{2}<1\) and \((0,1)\), and using the Hessian approximation update in Section 3.2 (the hyperparameters are given in Appendix D). Then with probability at least \(1-p\), the following statements hold, where \(C_{i}\) (\(i=4,,10\)) are absolute constants only depending on \(_{1}\), \(_{2}\) and \(\)._

_(a) For any_ \(k 0\)_, we have_

\[f(_{k})-f(^{*})L_{1}\|_{0}- ^{*}\|^{2}}{k^{2}}+\|_{0}-^{*}\|^{2 }}{_{0}k^{2.5}}.\] (17)

_(b) Furthermore, for any_ \(k 0\)_,_

\[f(_{k})-f(^{*})_{0}- ^{*}\|^{2}}{k^{2.5}}(M+C_{10}L_{1}L_{2}d\|_{0}- ^{*}\|^{+}(}{_{2}},}\}k^{2.5}}{}))^{},\] (18)

_where we define_ \(^{+}(x)\{(x),0\}\) _and the quantity_ \(M\) _is given by_

\[M}{_{0}^{2}}+C_{7}L_{1}^{2}+C_{8}\|_{0}- ^{2}f(_{0})\|_{F}^{2}+C_{9}L_{2}^{2}\|_{0}-^{*}\|^{2}+C_{10}L_{1}L_{2}d\|_{0}-^{*}\|.\] (19)

Both results in Theorem 1 are global, as they are valid for any initial points \(_{0},_{0}\) and any initial matrix \(_{0}\). Specifically, Part (a) of Theorem 1 shows that A-QNPE converges at a rate of \((1/k^{2})\), matching the rate of NAG  that is known to be optimal in the regime where \(k=(d)\). Furthermore, Part (b) of Theorem 1 presents a convergence rate of \((/k^{2.5})\). To see this, note that since we have \(0_{0} L_{1}\) and \(0^{2}f(_{0}) L_{1}\), in the worst case \(\|_{0}-^{2}f(_{0})\|_{F}^{2}\), in the expression of (19) can be upper bounded by \(L_{1}^{2}d\). Thus, assuming that \(L_{1}\), \(L_{2}\) and \(\|_{0}-^{*}\|\) are on the order of \((1)\), we have \(M=(d)\) and the convergence rate in (18) can be simplified to \((/k^{2.5})\). Notably, this rate surpasses the \((1/k^{2})\) rate when \(k=(d d)\). To the best of our knowledge, this is the first work to show a convergence rate faster than \((1/k^{2})\) for a quasi-Newton-type method in the convex setting, thus establishing a provable advantage over NAG.

_Remark 3_ (Iteration complexity).: Based on Theorem 1, we can find A-QNPE's iteration complexity. Define \(N_{}\) as the number of iterations required by A-QNPE to find an \(\)-accurate solution, i.e., \(f()-f(^{*})\). When \(>\), the rate in (17) is better and we have \(N_{}=(})\). Conversely, when \(<\), the rate in (18) is the better one, resulting in \(N_{}=((}})^{ 0.2})\). Hence, to achieve an \(\)-accurate solution A-QNPE requires \((\{},}{^{0.4}}( })^{0.2}\})\) iterations.

_Remark 4_ (Special case).: If the initial point \(_{0}\) is close to an optimal solution \(^{*}\) and the initial Hessian approximation matrix \(_{0}\) is chosen properly, the dependence on \(d\) in the convergence rate of (18) can be eliminated. Specifically, if \(\|_{0}-^{*}\|=()\) and we set \(_{0}=^{2}f(_{0})\), then we have \(M=(1)\) and this leads to a local dimension-independent rate of \((/k^{2.5})\).

Recall that in each iteration of Algorithm 1, we need to execute a line search subroutine (Section 3.1) and a Hessian approximation update subroutine (Section 3.2). Thus, to fully characterize the computational cost of Algorithm 1, we need to upper bound the total number of gradient queries as well as the total number of matrix-vector product evaluations, which is the goal of Theorem 2.

**Theorem 2**.: _Recall that \(N_{}\) denotes the minimum number of iterations required by Algorithm 1 to find an \(\)-accurate solution according to Theorem 1. Then, with probability at least \(1-p\):_

1. _The total number of gradient queries is bounded by_ \(3N_{}+_{1/}(L_{1}}{_{2}})\)_._
2. _The total number of matrix-vector product evaluations in the_ LinearSolver _oracle is bounded by_ \(N_{}+C_{11}L_{1}}+C_{12}\|_{0}-^{*}\|^{2}}{2}}\)_, where_ \(C_{11}\) _and_ \(C_{12}\) _are absolute constants._
3. _The total number of matrix-vector product evaluations in the_ SEP _oracle is bounded by_ \(N_{}^{1.25}( N_{})^{0.5} N_{}}{p}\)_._

If the initial step size is chosen as \(_{0}=}{L_{1}}\), Theorem 2(a) implies that A-QNPE requires no more than 3 gradient queries per iteration on average. Thus, the gradient oracle complexity of A-QNPE is the same as the iteration complexity, i.e., \((\{},}{^{0.4}}( })^{0.2}\})\). On the other hand, the complexity in terms of matrix-vector products is worse. More precisely, by using the expression of \(N_{}\) in Remark 3, Parts (b) and (c) imply that the total number of matrix-vector product evaluations in the \(\) and \(\) oracles can be bounded by \((})\) and \(}(\{}{^{0.5}},}\})\), respectively.

For easier comparison, we summarize the detailed computational costs of NAG and our method A-QNPE to achieve an \(\)-accuracy in Table 1. We observe that A-QNPE outperforms NAG in terms of gradient query complexity: It makes equal or fewer gradient queries especially when \(}\). On the other hand, A-QNPE requires additional matrix-vector product computations to implement the \(\) and \(\) oracles. While this is a limitation of our method, in some cases, gradient evaluations are the main bottleneck and can be more expensive than matrix-vector products. As a concrete example, consider the finite-sum minimization problem \(f()=_{i=1}^{n}f_{i}()\). In this case, one gradient query typically costs \((nd)\), while one matrix-vector product costs \((d^{2})\). Thus, the total computational cost of NAG and A-QNPE can be bounded by \((})\) and \(}(}{^{0.4}}+}{^{0. 5}})\), respectively. In particular, our method incurs a lower computational cost when \(}\) and \(n d^{1.25}\).

## 5 Experiments

In this section, we compare the numerical performance of our proposed A-QNPE method with NAG and the classical BFGS quasi-Newton method. For fair comparison, we also use a line search

   Methods & Gradient queries & Matrix-vector products \\  NAG & \((^{-0.5})\) & N.A. \\ 
**A-QNPE (ours)** & \(}(\{^{-0.5},d^{0.2}^{-0.4}\})\) & \(}(\{d^{0.25}^{-0.5},^{-0.625}\})\) \\   

Table 1: The comparison of NAG and our proposed method in terms of computational cost.

scheme in NAG and BFGS to obtain their best performance [54; 15]. We would like to highlight that our paper mainly focuses on establishing a provable gain for quasi-Newton methods with respect to NAG, and our experimental results are presented to numerically verify our theoretical findings. In the first experiment, we focus on a logistic regression problem with the loss function \(f()=_{i=1}^{n}(1+e^{-y_{i}(_{i},)})\), where \(_{1},,_{n}^{d}\) are feature vectors and \(_{1},,_{n}\{-1,1\}\) are binary labels. We perform our numerical experiments on a synthetic dataset and the data generation process is described in Appendix F. In the second experiment, we consider the log-sum-exp function \(f()=(_{i=1}^{n}e^{_{i},-b_ {i}})\), where we generate the dataset \(\{(_{i},b_{i})\}_{i=1}^{n}\) following a similar procedure as in  (more details in Appendix F). As we observe in Fig. 1(a) and Fig. 2(a), our proposed A-QNPE method converges in much fewer iterations than NAG, while the best performance is achieved by BFGS. Due to the use of line search, we also compare these algorithms in terms of the total number of gradient queries. Moreover, additional plots in terms of the running time are included in Appendix F. As illustrated in Fig. 1(b) and Fig. 2(b), A-QNPE still outperforms NAG but the relative gain becomes less substantial. This is because the line search scheme in NAG only queries the function value at the new point, and thus it only requires one gradient per iteration. On the other hand, we should add that the number of gradient queries per iteration for A-QNPE is still small as guaranteed by our theory. In particular, the histogram of gradient queries in Fig. 1(c) and Fig. 2(c) shows that most of the iterations of A-QNPE require 2-3 gradient queries with an average of less than \(3\). Finally, although there is no theoretical guarantee showing a convergence gain for BFGS with respect to NAG, we observe that BFGS outperforms all the other considered methods in our experiments. Hence, studying the convergence behavior of BFGS (with line search) in the convex setting is an interesting research direction to explore.

## 6 Conclusions

We proposed a quasi-Newton variant of the accelerated proximal extragradient method for solving smooth convex optimization problems. We established two global convergence rates for our A-QNPE method, showing that it requires \(}(\{},}{e^{0.4}}\})\) gradient queries to find an \(\)-accurate solution. In particular, in the regime where \(=(})\), A-QNPE achieves a gradient oracle complexity of \((})\), matching the complexity of NAG. Moreover, in the regime where \(=}(})\), it outperforms NAG and improves the complexity to \(}(}{e^{0.4}})\). To the best of our knowledge, this is the first result showing a provable gain for a quasi-Newton-type method over NAG in the convex setting.

Figure 1: Numerical results for logistic regression on a synthetic dataset.

Figure 2: Numerical results for log-sum-exp function on a synthetic dataset.