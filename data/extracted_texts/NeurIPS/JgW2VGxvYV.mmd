# SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance

Kunal Singh Ankan Biswas Sayandeep Bhowmick Pradeep Moturi

Fractal AI Research

Mumbai

Corresponding author: Kunal Singh (kunal.singh@fractal.ai)

###### Abstract

We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework that enables Large Language Models (LLMs) to generate sequence of programs for solving Olympiad level math problems. After each turn/step, by leveraging the code execution outputs and programs of previous steps, the model generates the next sub-task and the corresponding program to complete it. SBSC allows more granular, flexible and precise approach to problem-solving compared to existing methods. Extensive experiments highlight the effectiveness of SBSC in tackling competition and Olympiad-level math problems. For Claude-3.5-Sonnet, we observe SBSC (greedy decoding) surpasses existing state-of-the-art (SOTA) program generation based reasoning strategies by absolute 10.7% on AMC12, 8% on AIME and 12.6% on MathOdyssey. Given SBSC is multi-turn in nature, we also benchmark SBSC's greedy decoding against self-consistency decoding results of existing SOTA math reasoning strategies and observe performance gain by absolute 6.2% on AMC, 6.7% on AIME and 7.4% on MathOdyssey. Scripts & Data is uploaded at this link for reproducibility.

## 1 Introduction

Mathematics is considered as a critical benchmark to measure the reasoning abilities of the Large Language Models (LLMs)  due to the complex and creative nature of the subject. The current generation of advanced LLMs, GPT-4o , Claude-3.5-Sonnet , Gemini-ultra  have achieved high scores on elementary GSM8k  & high-school level MATH . However, recent math specific competition and Olympiad-level benchmarking on Math Odyssey , American Invitational Mathematics Examination (AIME) & American Mathematics Competitions (AMC)  questions show that they continue to struggle with advanced mathematical reasoning.

**Related Work**: In recent times, numerous developments in multiple research directions have taken place to enhance the math ability of the LLMs. One of the major ones has been along the prompting and thinking strategies such as Chain-of-Thought (COT) method  that has shown to evoke multi-step thinking in LLMs before arriving at the answer. These methods struggle with complex and symbolic computations. For this, PAL  & POT  suggest making LLMs perform reasoning by writing program and offloading the computations to code interpreter. Another line of research has been around pre-training and supervised fine-tuning (SFT). Multiple studies  have shown pre-training LLMs on maths tokens results in increased mathematical knowledge and reasoning abilities. Recent approaches  have tried creating synthetic reasoning paths/trajectories using a teacher model like GPT4  for SFT. Also, some studies  provide an alternative to manual annotations for process supervision .

**Motivation**: COT prompting helps LLMs to solve a problem using a step-by-step thought process. PAL & POT introduced problem-solving via program generation where the answer is generated by executing the generated program. ToRA  & Mathcoder  introduced tool-integrated math problem solving format. There, model outputs natural language reasoning followed by program generation to solve the problem in a single turn/block and incorporates code-interpreter output for either summarizing the program output to get the final answer and terminate; or re-atempt the problem in the subsequent turn using the same format. For brevity, let's call ToRA's defined way of tool-integrated reasoning (TIR) strategy as TIR-ToRA.

Fundamentally, both PAL & TIR-ToRA generate a single program block to solve the problem. Additionally, TIR-ToRA framework allows the model to re-attempt the program generation in case of execution error. These approaches show improved performance over COT on elementary & high school level math problems. However, solving olympiad level math requires coming up with complex and creative solutions. Often, it is not feasible to solve a complex problem entirely using a single program block and as a result, these strategies fail to systematically address each detailed step of the problem-solving process. It tends to overlook specified constraints, edge cases or necessary simplifications, which are often encountered in Olympiad-level problems.

**Our Contribution**: Olympiad level math problem-solving can be viewed as solving/exploring an intermediate sub-task in depth; and discovering + solving the next critical sub-task dynamically basis the accumulated knowledge of previous sub-tasks explorations. To this end, we propose Step-by-Step Coding paradigm (SBSC) which is a multi-turn math reasoning framework that leverages existing programming  and in-context learning skills  of the current generation of LLMs, particularly Claude-3.5-Sonnet  & GPT-4o . It uses program generation as the reasoning strategy to solve an intermediate sub-task unlike PAL & TIR-ToRA. In each turn, it leverages code-interpreter results and knowledge of previous sub-tasks solutions to define and programmatically solve the next sub-task. We investigate the performance of SBSC on last 11 years of AIME & AMC-12 questions. We also benchmark on Olympiad-subset of MathOdysy dataset. We compare our method with existing reasoning strategies: COT, PAL, TIR-ToRA. We conduct ablations to understand the benefits of our approach such as sensitivity to exemplars, topic-wise analysis and measuring improvement in program refinement/debugging ability over TIR-ToRA due to the granular nature of SBSC process.

## 2 Method

SBSC is a multi-turn, program-generation based math reasoning framework where at each turn: the model generates an intermediate sub-task and corresponding program to solve that sub-task by leveraging the outputs of the previous turns. At the end of each turn, code interpreter is used to execute the program block to generate the solution for the intermediate sub-task. The intermediate sub-task depends on the results of the previous turns and the question. The code snippet for the \(i^{th}\) sub-task directly incorporates the execution results of the previous code snippets by directly defining them as variables and symbols. This way SBSC makes LLMs generate sequence of targeted programs over multiple turns to solve complex math problems.

Our inference procedure is inspired by ToRA . Solution chain is initialized with the Prompt \(p\) containing method instructions followed by exemplars and the current question \(q\). At each step, LLM \(G\) first outputs a subtask \(s_{i}\). If \(s_{i}\) generation ends with stop-word "##END OF CODE", we extract the final answer. Else, it continues to generate program code \(c_{i}\) ending with stop-word "" output". We then pass \(c_{i}\) to code interpreter and obtain the execution message or output \(o_{i} E(c_{i})\). The solution chain is updated by concatenating it with \(s_{i}\),\(c_{i}\),\(o_{i}\) and loop continues till we get "##END OF CODE". \(\) denoting concatenation, the sequential process can be generalised as:

\[s_{i} c_{i} G( p q(s_{1} c_{1} o_{ 1})(s_{2} c_{2} o_{2})......(s_{i-1} c_{i-1}  o_{i-1}))\] (1)

Step-wise sequential approach of SBSC ensures that every part of the problem is addressed with exact precision, reducing the risk of errors that might arise from false assumptions or skipped steps. Having separate programs for each part of the solution also allows it to make necessary simplifications that would make the future subparts, and hence the whole problem, easier to solve allowing for a more granular and precise approach to problem-solving compared to existing methods. In case the code execution at any step results in an erroneous output, SBSC is better able to rectify that particular step. Fig 0(a) shows a visual sample SBSC response for an AIME question and Fig 0(b) shows TIR-ToRAresponse for the same question. More detailed discussion on comparison is at Appendix A.1. In depth understanding of SBSC via multiple examples and comparisons at Appendix A.4.

## 3 Experiment

**Benchmark datasets** We create our datasets using problems of last 11 years from popular math competitions AMC and AIME. We obtain questions and answers (Q&A) in LaTeX format from the AoPS Wiki website. We remove problems which are dependent on accompanying images and process the Q&A to have integer answers using GPT-4o if needed, leaving us with 330 AIME problems and 475 AMC-12 problems. We also use MathOdyssey , a popular benchmark for LLM math

Figure 1: Comparison of SBSC and TIR-ToRA frameworks.

reasoning, consisting of problems of varying difficulties. We include the 148 problems belonging to Olympiad-level competitions and perform similar filtering and processing. For more details on how we processed the dataset, please refer to Appendix A.2.

**Models & Configurations** We use gpt-4o-2024-05-13 and Claude-3.5-Sonnet as base LLMs for our experiments. For all datasets and all reasoning frameworks, we use 4-shot setting. Maximum number of turns (n) for both TIR-ToRA and SBSC is set to 15. For greedy decoding inference, we use temperature=0 and max_tokens=1024 and also, we run 3 times and report average. Given SBSC is multi-turn in nature (on average 6-7 turns per problem, Table 2 in Appendix A.3), we also benchmark SBSC's greedy decoding results against self-consistency (SC)  decoding results (majority@7) of COT, PAL & TIR-ToRA. For SC decoding, we use temperature=0.7 and top_p=0.9. Note: we experimentally observe that for n > 4, there is no improvement in accuracy for TIR-ToRA so we set n=4 for TIR-ToRA during SC decoding. All ablations were conducted using Claude-3.5-Sonnet unless otherwise specified.

**Prompting/Few-shot Exemplars** For both AIME and AMC, we select 90 questions each, drawn from problems of years other than those included in the evaluation datasets. These questions were prompted with COT, PAL, TIR-ToRA and SBSC to generate corresponding solutions in accurate format. For each dataset, we create a subset of 10 problems correctly solved by every method and finally select a combination of 4 exemplars among them. For MathOdyssey, we use AIME exemplars as both are of similar difficulty level. We provide the 4 chosen exemplars and system-prompts, used in the main experiments, for different methods in Appendix (A.5, A.6, A.8, A.9) & repository here.

## 4 Results

As shown in Table 1, on AMC dataset, SBSC shows an absolute improvement over TIR-ToRA by roughly 11% using Claude-3.5-Sonnet and 7% using GPT-4o. SBSC greedy decoding results outperforms SC decoding results of TIR-TORA by absolute 6% and 4%, for Claude-3.5-Sonnet and GPT-4o respectively. We see similar absolute improvements in accuracy on our AIME dataset too. SBSC outperforms its nearest competitor (PAL) by 8% and 6% with greedy settings and SC settings by 6.7% and 3.7% for Claude-3.5-Sonnet and GPT-4o respectively. For MathOdyssey, SBSC improves by as much as 12.6% and 7% over TIR-ToRA while showing improvement of 7.4% and 3% over its SC variant, for Claude-3.5-Sonnet & GPT-4o respectively. Standard deviation values at A.10.

## 5 Ablations

**Sensitivity to Exemplars**: We study the effect of number/choice of examples in prompting on SBSC's performance. As shown in Figure 2, we observe a notable increase in performance when

  Method &  &  &  \\  & greedy & maj@7 & greedy & maj@7 & greedy & maj@7 \\    \\ COT & 31.16 & 35.79 & 9.09 & 10.91 & 11.89 & 16.89 \\ PAL & 35.79 & 36.42 & 27.48 & 28.79 & 27.23 & 31.01 \\ TIR-ToRA & 38.59 & 43.16 & 24.64 & 26.67 & 27.23 & 32.43 \\ SBSC (Ours) & **49.33\({}_{ 10.7}\)** & \({}_{ 6.2}\) & **35.45\({}_{ 8}\)** & \({}_{ 6.7}\) & **39.86\({}_{ 12.6}\)** & \({}_{ 7.4}\) \\    \\ COT & 35.94 & 37.47 & 10.39 & 12.12 & 13.51 & 17.57 \\ PAL & 36.48 & 38.11 & 24.63 & 26.97 & 15.74 & 20.27 \\ TIR-ToRA & 37.33 & 40.42 & 22.42 & 25.45 & 19.59 & 23.64 \\ SBSC (Ours) & **44.55\({}_{ 7.2}\)** & \({}_{ 4.1}\) & **30.7\({}_{ 6.1}\)** & \({}_{ 3.7}\) & **26.55\({}_{ 7}\)** & \({}_{ 2.9}\) \\  

Table 1: Benchmarking SBSC against different math reasoning methods across 3 datasets:We report the average accuracy over 3 runs. Best result in each setting is highlighted in **bold** & second best is underlined. Absolute improvement in performance by SBSC over the previous best method in each setting is indicated in subscript.

increasing the examples from 2 to 4, which then starts to saturate as we further increase the number of examples to 6 and 8. This justifies our decision of using a 4-shot setting. To understand if the choice of exemplars affect the accuracy or not, we conduct a sensitivity analysis. We randomly sample 4 exemplars out of the already created pool of 10 exemplars three times to create 3 variations of 4-shot prompts: v1, v2, and v3. In Figure 3, we can see that the performance remains stable irrespective of the exemplars used, across a subset of AIME (2022-2024) and AMC (2021-2023) problems.

**Topic-wise Analysis**: We use GPT-4o-mini  to classify problems from AIME and AMC, while MathOdyssey already had topic labels. As can be seen in Figure 4, our method outperforms TIR-ToRA in all the individual topics and across all 3 datasets, thereby proving beneficial for all topics.

**Code Debugging Ability**: We present the superior ability of our method to resolve an error related to code execution. If at any step of the trajectory chain, the program returns an execution error, we consider that to be an error step. In Figure 5, we see that SBSC is able to recover from even multiple wrong steps and reach the correct final answer quite easily when compared to TIR-ToRA whose performance drops steeply on increasing error steps. This can be attributed to the fact that SBSC, being precise and granular, tackles only a specific part of the problem and finds it easier to correct its mistakes compared to TIR-ToRA which tries to correct the program at the problem level.

## 6 Conclusion

SBSC is a math reasoning framework that solves a problem by generating a sequence of sub-tasks and corresponding program blocks. Each sub-task and its corresponding program solution is generated leveraging the execution outputs and solutions of all the previous sub-tasks. We show performance improvements of SBSC over TIR-ToRA, PAL & COT on challenging math problems. **Limitations**: We only focus on text-based questions. We also just evaluate on integer-answer type questions.