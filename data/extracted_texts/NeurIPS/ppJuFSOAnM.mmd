# ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation

ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation

 Zhengyi Wang\({}^{1,3}\), Cheng Lu\({}^{1}\), Yikai Wang\({}^{1}\), Fan Bao\({}^{1,3}\),

**Chongxuan Li\({}^{1}\)** \({}^{2}\), Hang Su\({}^{1,4}\), Jun Zhu\({}^{1}\)\({}^{1,3,4}\)

\({}^{1}\)Dept. of Comp. Sci. & Tech., BNRist Center, Tsinghua-Bosch Joint ML Center,

Tsinghua University; \({}^{2}\)Gaoling School of Artificial Intelligence, Renmin University of China,

Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

\({}^{3}\)ShengShu, Beijing, China; \({}^{4}\)Pazhou Laboratory (Huangpu), Guangzhou, China

{wang-zy21, bf19}@mails.tsinghua.edu.cn; lucheng.lc15@gmail.com; yikaiw@outlook.com; chongxuanli@ruc.edu.cn; suhangss@tsinghua.edu.cn dcszj@tsinghua.edu.cn

Equal contribution; \({}^{}\) Corresponding authors.

###### Abstract

Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present _variational score distillation_ (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., \(7.5\)). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed _ProlificDreamer_, can generate high rendering resolution (i.e., \(512 512\)) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/.

## 1 Introduction

3D content and technologies enable us to visualize, comprehend, and interact with complex objects and environments that are reflective of our real-life experiences. Their pivotal role extends across a wide array of domains, encompassing architecture, animation, gaming, and the rapidly evolving fields of virtual and augmented reality. In spite of the extensive applications, the production of premium 3D content often remains a formidable task. It necessitates a significant investment of time and effort, even when undertaken by professional designers. This challenge has prompted the development of text-to-3D methods . By automating the generation of 3D content based on textual descriptions, these innovative methods present a promising way towards streamlining the 3D content creation process. Furthermore, they stand to make this process more accessible, potentially encouraging a significant paradigm shift in the aforementioned fields.

## 1 Introduction

Figure 1: Text-to-3D samples generated by ProlificDreamer from scratch. Our base model is Stable Diffusion and we do not employ any other assistant model or user-provided shape guidance (see Table 1). See our accompanying videos in our project page for better visual quality.

Diffusion models [46; 14; 49] have significantly advanced text-to-image synthesis [36; 41; 38; 1], particularly when trained on large-scale datasets . Inspired by these developments, DreamFusion  employs a pretrained, large-scale text-to-image diffusion model for the generation of 3D content from text in the wild, circumventing the need for any 3D data. DreamFusion introduces the Score Distillation Sampling (SDS) algorithm to optimize a single 3D representation such that the image rendered from any view maintains a high likelihood as evaluated by the diffusion model, given the text. Despite its wide application [20; 4; 29; 55], empirical observations  indicate that SDS often suffers from over-saturation, over-smoothing, and low-diversity problems, which have yet to be thoroughly explained or adequately addressed. Additionally, orthogonal elements in the design space for text-to-3D, such as rendering resolution and distillation time schedule, have not been fully explored, suggesting a significant potential for further improvement. In this paper, we present a systematic study of all these elements to obtain elaborate 3D representations.

We first present _Variational Score Distillation_ (VSD), which treats the corresponding 3D scene given a textual prompt as a _random variable_ instead of a single point as in SDS . VSD optimizes a distribution of 3D scenes such that the distribution induced on images rendered from all views aligns as closely as possible, in terms of KL divergence, with the one defined by the pretrained 2D diffusion model (see Sec. 3.1). Under this variational formulation, VSD naturally characterizes the phenomenon that multiple 3D scenes can potentially align with one prompt. To solve it efficiently, VSD adopts particle-based variational inference [23; 3; 9], and maintains a set of 3D parameters as particles to represent the 3D distribution. We derive a novel gradient-based update rule for the particles via the Wasserstein gradient flow (see Sec. 3.2) and guarantee that the particles will be samples from the desired distribution when the optimization converges (see Theorem 2). Our update requires estimating the score function of the distribution on diffused rendered images, which can be efficiently and effectively implemented by a low-rank adaptation (LoRA) [18; 40] of the pretrained diffusion model. The final algorithm alternatively updates the particles and score function.

We show that SDS is a special case of VSD, by using a single-point Dirac distribution as the variational distribution (see Sec. 3.3). This insight explains the restricted diversity and fidelity of the generated 3D scenes by SDS. Moreover, even with a single particle, VSD can learn a parametric score model, potentially offering superior generalization over SDS. We also empirically compare SDS and VSD in 2D space by using an identity rendering function that isolates other 3D factors. Similar to ancestral sampling from diffusion models, VSD is able to produce realistic samples using a normal CFG weight (i.e., 7.5). In contrast, SDS exhibits inferior results, sharing the same issues previously observed in text-to-3D, such as over-saturation and over-smoothing .

We further systematically study other elements orthogonal to the algorithm for text-to-3D and present a clear design space in Sec. 4. Specifically, we propose a high rendering resolution of \(512 512\) during training and an annealed distilling time schedule to improve the visual quality. We also propose _scene initialization_, which is crucial for complex scene generation. Comprehensive ablations in Sec. 5 demonstrate the effectiveness of all the aforementioned elements particularly for VSD. Our overall approach can generate high-fidelity and diverse 3D results. We term it as _ProlificDreamer2_.

As shown in Fig. 1 and Sec. 5, ProlificDreamer can generate \(512 512\) rendering resolution and high-fidelity Neural Radiance Fields (NeRF) with rich structure and complex effects (e.g., smoke and drops). Besides, for the first time, ProlificDreamer can successfully construct complex scenes with multiple objects in \(360^{}\) views given the textual prompt. Further, initialized from the generated NeRF, ProlificDreamer can generate meticulously detailed and photo-realistic 3D textured meshes.

## 2 Background

We present preliminaries on diffusion models, score distillation sampling, and 3D representations.

**Diffusion models.** A diffusion model [46; 14; 49] involves a forward process \(\{q_{t}\}_{t}\) to gradually add noise to a data point \(_{0} q_{0}(_{0})\) and a reverse process \(\{p_{t}\}_{t}\) to denoise/generate data. The forward process is defined by \(q_{t}(_{t}|_{0})(_{t}_{0}, _{t}^{2})\) and \(q_{t}(_{t}) q_{t}(_{t}|_{0})q_{0}(_{0}) _{0}\), where \(_{t},_{t}>0\) are hyperparameters satisfying \(_{0} 1,_{0} 0,_{1} 0,_{1} 1\); and the reverse process is defined by denoising from \(p_{1}(_{1})(,)\) with a parameterized _noise prediction_network_\(_{}(_{t},t)\) to predict the noise added to a clean data \(_{0}\), which is trained by minimizing

\[_{}()_{_{0} q_{0}( _{0}),t(0,1),(,)}[ (t)\|_{}(_{t}_{0}+_{t})-\|_{2}^{2}],\] (1)

where \((t)\) is a time-dependent weighting function. After training, we have \(p_{t} q_{t}\) and thus we can draw samples from \(p_{0} q_{0}\). Moreover, the noise prediction network can be used for approximating the _score function_ of both \(q_{t}\) and \(p_{t}\) by \(_{_{t}} q_{t}(_{t})_{_{t}} p_{t} (_{t})-_{}(_{t},t)/_{t}\).

One of the most successful applications of diffusion models is text-to-image generation [41; 36; 38], where the noise prediction model \(_{}(_{t},t,y)\) is conditioned on a text prompt \(y\). In practice, classifier-free guidance (CFG ) is a key technique for trading off the quality and diversity of the samples, which modifies the model by \(}_{}(_{t},t,y)(1+s)_{} (_{t},t,y)-s_{}(_{t},t,)\), where \(\) is a special "empty" text prompt representing for the unconditional case, and \(s>0\) is the guidance scale. A larger guidance scale usually improves the text-image alignment but reduces diversity.

**Text-to-3D generation by score distillation sampling (SDS) .** SDS is an optimization method by distilling pretrained diffusion models, also known as Score Jacobian Chaining (SJC) . It is widely used in text-to-3D generation [34; 55; 20; 29; 55; 4] with great promise. Given a pretrained text-to-image diffusion model \(p_{t}(_{t}|y)\) with the noise prediction network \(_{}(_{t},t,y)\), SDS optimizes a single 3D representation with parameter \(\), where \(\) is the space of \(\) with the Euclidean metric. Given a camera parameter \(c\) with a distribution \(p(c)\) and a differentiable rendering mapping \((,c):^{d}\), denote \(y^{c}\) as the "_view-dependent prompt_"  (i.e., a text prompt with view information), \(q_{t}^{}(_{t}|c)\) as the distribution at time \(t\) of the forward diffusion process starting from the rendered image \((,c)\) with the camera \(c\) and 3D parameter \(\). SDS optimizes the parameter \(\) by solving

\[_{}_{}()_{t,c}[(_{t}/_{t})(t)D_{}(q_{t}^{}(_ {t}|c) p_{t}(_{t}|y^{c}))],\] (2)

where \(t(0.02,0.98)\), \((,)\), and \(_{t}=_{t}(,c)+_{t}\). Its gradient is approximated by

\[_{}_{}()_{t,,c}[(t)(_{}(_{t},t,y^{c} )-)(,c)}{}].\] (3)

Notwithstanding this progress, empirical observations  show that SDS often suffers from over-saturation, over-smoothing, and low-diversity issues, which have yet to be thoroughly explained or adequately addressed.

**3D representations.** We employ NeRF [30; 32] (Neural Radiance Fields) and textured mesh  as two popular and important types of 3D representations. In particular, NeRF represents 3D objects using a multilayer perceptron (MLP) that takes coordinates in a 3D space as input and outputs the corresponding color and density. Here, \(\) corresponds to the parameters of the MLP. Given camera pose \(c\), the rendering process \((,c)\) is defined as casting rays from pixels and computing the weighted sum of the color of the sampling points along each ray to composite the color of each pixel. NeRF is flexible for optimization and is capable of representing extremely complex scenes. Textured mesh  represents the geometry of a 3D object with triangle meshes and the texture with color on the mesh surface. Here the 3D parameter \(\) consists of the parameters to represent the coordinates of triangle meshes and parameters of the texture. The rendering process \((,c)\) given camera pose \(c\) is defined by casting rays from pixels and computing the intersections between rays and mesh surfaces to obtain the color of each pixel. The textured mesh allows high-resolution and fast rendering with differentiable rasterization.

## 3 Variational Score Distillation

We now present Variational Score Distillation (VSD) (see Sec. 3.1) that learns to sample from a distribution of the 3D scenes. By using 3D parameter particles to represent the target 3D distribution, we derive a principled gradient-based update rule for the particles via the Wasserstein gradient flow (see Sec. 3.2). We further show that SDS is a special case of VSD and constructs an experiment in 2D space to study the optimization algorithm isolated from the 3D representations, explaining the practical issues of SDS both theoretically and empirically (see Sec. 3.3).

### Sampling from 3D Distribution as Variational Inference

In principle, given a valid text prompt \(y\), there exists a probabilistic distribution of all possible 3D representations. Under a 3D representation (e.g., NeRF) parameterized by \(\), such a distribution can be modeled as a probabilistic density \((|y)\). Denote \(q_{0}^{}(_{0}|c,y)\) as the (implicit) distribution of the rendered image \(_{0}(,c)\) given the camera \(c\) with the rendering function \((,c)\), and denote \(p_{0}(_{0}|y^{c})\) as the marginal distribution of \(t=0\) defined by the pretrained text-to-image diffusion model with the view-dependent prompt \(y^{c}\). To obtain 3D representations of high visual quality, we propose to optimize the distribution \(\) to align the rendered images of its samples with the pretrained diffusion model in all views by solving

\[_{}D_{}(q_{0}^{}(_{0}|c,y) p_{0}(_{0 }|y^{c})).\] (4)

This is a typical variational inference problem that uses the variational distribution \(q_{0}^{}(_{0}|c,y)\) to approximate (distill) the target distribution \(p_{0}(_{0}|y^{c})\).

Directly solving problem (4) is hard because \(p_{0}\) is rather complex and the high-density regions of \(p_{0}\) may be extremely sparse in high dimension . Inspired by the success of diffusion models , we construct a series of optimization problems with different diffused distributions indexed by \(t\). As \(t\) increases to \(T\), the optimization problem becomes easier because the diffused distributions get closer to the standard Gaussian. We simultaneously solve an ensemble of these problems (termed as _variational score distillation_ or VSD) as follows:

\[^{*}*{arg\,min}_{}_{t,c}[(_{ t}/_{t})(t)D_{}(q_{t}^{}(_{t}|c,y) p_{t}( _{t}|y^{c}))],\] (5)

where \(q_{t}^{}(_{t}|c,y) q_{0}^{}(_{0}|c,y)p_{t0}( {x}_{t}|_{0})_{0}\) and \(p_{t}(_{t}|y^{c}) p_{0}(_{0}|y^{c})p_{t0}(_{t}| _{0})_{0}\) are the corresponding noisy distributions at time \(t\) with the Gaussian transition \(p_{t0}(_{t}|_{0})=(_{t}|_{t}_{0}, _{t}^{2})\), and \((t)\) is a time-dependent weighting function.

Compared with SDS that optimizes for the single point \(\), VSD optimizes for the whole distribution \(\), from which we sample \(\). Notably, we prove that introducing the additional KL-divergence for \(t>0\) in VSD does not affect the global optimum of the original problem (4), as shown below.

**Theorem 1** (Global optimum of VSD, proof in Appendix C.4.).: _For each \(t>0\), we have_

\[D_{}(q_{t}^{}(_{t}|c,y) p_{t}(_{t}|y^{c})) =0 q_{0}^{}(_{0}|c,y)=p_{0}(_{0}|y^{c}).\] (6)

### Update Rule for Variational Score Distillation

To solve problem (5), a direct way can be to train another parameterized generative model for \(\), but it may bring much computation cost and optimization complexity. Inspired by previous particle-based variational inference  methods, we maintain \(n\) 3D parameters3\(\{\}_{i=1}^{n}\) as particles and derive a novel update rule for them. Intuitively, we use \(\{\}_{i=1}^{n}\) to "represent" the current distribution \(\), and \(^{(i)}\) will be samples from the optimal distribution \(^{*}\) if the optimization converges. Such optimization can be realized by simulating an ODE w.r.t. \(\), as shown in the following theorem.

**Theorem 2** (Wasserstein gradient flow of VSD, proof in Appendix C).: _Starting from an initial distribution \(_{0}\), denote the Wasserstein gradient flow minimizing problem (5) in the distribution (function) space at each time \( 0\) as \(\{_{}\}_{ 0}\) with \(_{}=^{*}\). Then we can sample \(_{}\) from \(_{}\) by

Figure 2: Overview of VSD. The 3D representation is differentiably rendered at a random pose \(c\). The rendered image is sent to the pretrained diffusion and the score of the variational distribution (estimated by LoRA) to compute the gradient of VSD. LoRA is also updated on the rendered image.

firstly sampling \(_{0}_{0}(_{0}|y)\) and then simulating the following ODE:_

\[_{}}{}=-_{t,,c} (t)_{_{t}} p_{t}( _{t}|y^{c})}_{}- _{_{t}} q_{t}^{_{}}(_{t}|c,y))}_{}(_{},c)}{_{}},\] (7)

_where \(q_{t}^{_{}}\) is the corresponding noisy distribution at diffusion time \(t\) w.r.t. \(_{}\) at ODE time \(\)._

According to Theorem 2, we can simulate the ODE in Eq. (7) for a large enough \(\) to approximately sample from the desired distribution \(^{*}\). The ODE involves the score function of noisy real images and that of noisy rendered images at each time4\(\). The score function of noisy real images \(-_{t}_{_{t}} p_{t}(_{t}|y^{c})\) can be approximated by the pretrained diffusion model \(_{}(_{t},t,y^{c})\). The score function of noisy rendered images \(-_{t}_{_{t}} q_{t}^{_{}}(_{t}|c,y)\) is estimated by another noise prediction network \(_{}(_{t},t,c,y)\), which is trained on the rendered images by \(\{^{(i)}\}_{i=1}^{n}\) with the standard diffusion objective (see Eq. (1)):

\[_{}_{i=1}^{n}_{t(0,1), (,),c p(c)}[\|_{}(_{t} (^{(i)},c)+_{t},t,c,y)-\|_{2}^{2} ].\] (8)

In practice, we parameterize \(_{}\) by either a small U-Net  or a LoRA (Low-rank adaptation [18; 40]) of the pretrained model \(_{}(_{t},t,y^{c})\), and add additional camera parameter \(c\) to the condition embeddings in the network. In most cases, we find that using LoRA can greatly improve the fidelity of the obtained samples (e.g., see results in Fig. 1). We believe that it is because LoRA is designed for efficient few-shot fine-tuning and can leverage the prior information in \(_{}\) (the information of both images and text corresponding to \(y\)).

Note that at each ODE time \(\), we need to ensure \(_{}\) matches the current distribution \(q_{t}^{_{}}\). Thus, we optimize \(_{}\) and \(^{(i)}\) alternately, and each particle \(^{(i)}\) is updated by \(^{(i)}^{(i)}-_{}_{} (^{(i)})\), where \(>0\) is the step size (learning rate). According to Theorem 2, the corresponding gradient is

\[_{}_{}()_{t,,c}[(t)(_{}(_{t},t,y^ {c})-_{}(_{t},t,c,y))(, c)}{}],\] (9)

where \(_{t}=_{t}(,c)+_{t}\). We show the approach of VSD in Fig. 3 (see pseudo code in Appendix E).

### Comparison with SDS

We now systematically compare VSD with SDS in both theory and practice.

Figure 3: Samples of different methods in 2D space. Similarly to ancestral sampling, VSD generates realistic images with a common CFG weight of 7.5 and outperforms SDS significantly. The prompts from left to right are _hamburger_, _horse_, and _a monster truck_, respectively. See details in Appendix G.

**SDS as a special case of VSD.** Theoretically, comparing the update rules of SDS (Eq. (3)) and VSD (Eq. (9)), SDS is a special case of VSD by using a single-point Dirac distribution \((|y)(-^{(1)})\) as the variational distribution (see Appendix C.3 for derivation). In particular, VSD not only employs potentially multiple particles but also learns a parametric score function \(_{}\) even for a single particle (i.e., \(n=1\)). Empirically, the learned neural network may potentially offer superior generalization ability over the Dirac distribution in SDS, thus it may provide more accurate updating directions in low-density regions. Moreover, by using LoRA, VSD can additionally exploit the text prompt \(y\) in the estimation \(_{}(_{t},t,c,y)\), while the Gaussian noise \(\) used in SDS cannot leverage the information from \(y\). Thus, VSD may provide samples which are more aligned with the prompt \(y\).

**VSD is friendly to CFG.** As VSD aims to _sample_\(\) from the optimal \(^{*}\) defined by the pretrained model \(_{}\), the effects by tuning the CFG in \(_{}\) for 3D sampling by VSD are quite similar to 2D sampling by the traditional ancestral sampling methods [14; 27]. Therefore, VSD can tune CFG as flexibly as the classic text-to-image methods, and we use the same setting of CFG (e.g. \(7.5\)) as the common text-to-image generation task for the best performance. To the best of our knowledge, this for the first time addresses the problem in previous SDS [34; 20; 4; 29] that it usually requires a large CFG (i.e., \(100\)).

**VSD vs. SDS in 2D experiments that isolate 3D representations.** To directly compare SDS and VSD, we consider a special case of the rendering function \(()\) to decouple the optimization algorithm from 3D representations. In particular, we set \((,c)\) for any \(c\). Then the rendered image \(=(,c)=\) is the same 2D image as \(\). In such a case, optimizing the parameter \(\) is equivalent to generating an image in 2D space, thereby independent of the 3D representation. We show the results of different sampling methods in Fig. 3. SDS exhibits failure under both small and large CFG weights. Particularly with the default CFG weight (i.e., 100) used in SDS, the 2D samples share the same issues previously observed in text-to-3D such as over-saturation and over-smoothing . In contrast, VSD demonstrates flexibility in accommodating various CFG weights and produces realistic samples using a normal CFG weight (i.e., 7.5), behaving similarly to ancestral sampling from diffusion models. See more details and analysis in Appendix G.

As other 3D factors are isolated in this comparison, these theoretical and empirical results suggest that the aforementioned practical issues of SDS  stem from the oversimplified variational distribution and large CFG employed by SDS. Such results strongly motivate us to employ VSD for text-to-3D generation, where it still substantially and consistently outperforms SDS (see evidence in Sec 5).

## 4 ProlificDreamer

We further present a clear design space for text-to-3D in Sec. 4.1 and systematically study other elements orthogonal to the distillation algorithm in Sec. 4.2. Combining all improvements highlighted in Tab. 1, we arrive at _ ProlificDreamer_, an advanced text-to-3D approach.

   Method & **DreamFusion ** & **Magic3D ** & **Fantasia3D ** & **Ours** \\ 
**NeRF Representation** & & & & \\ Resolution\({}^{*}\) & 64 & 64 & - & 512 \\ Backbone & mipNeRF360  & Instant NGP  & - & Instant NGP  \\ Initialization\({}^{}\) & Object & Object & - & Object / Scene initialization \\ 
**NeRF Training** & & & & \\ Base model & Imagen  & eDiff-I  & - & Stable Diffusion  \\ Number of particles\({}^{}\) & 1 & 1 & - & 1\(\)4 \\ Distillation objective\({}^{}\) & SDS (Eq. (3)) & SDS (Eq. (3)) & - & VSD (Eq. (9)) \\ CFG\({}^{*}\) & 100 & 100 & - & 7.5 \\ Time schedule\({}^{*}\) & \((0.02,0.98)\) & \((0.02,0.98)\) & - & \((0.02,0.98)(0.02,0.5)\) \\ 
**Mesh Representation** & & & & \\ Initialization & - & From NeRF & Handcrafted & From NeRF \\ Texture and geometry & - & Entangled & Disentangled & Disentangled \\ 
**Mesh Training** & & & & \\ Distillation objective\({}^{}\) & - & SDS (Eq. (3)) & SDS (Eq. (3)) & VSD (Eq. (9)) \\ CFG\({}^{*}\) & - & 100 & 100 & 7.5 \\   

Table 1: Design space of text-to-3D via 2D diffusion. We highlight the contributions of this paper that improve the fidelity, diversity and ability to generate complex scenes by \({}^{*}\), \({}^{}\) and \({}^{}\) respectively.

### Design Space of Text-to-3D Generation

We adopt the two-stage approach , with several improvements in the design space of text-to-3D generation as summarized in Table 1. Specifically, in the first stage, we optimize a high-resolution (e.g., \(512\)) NeRF by VSD to utilize its high flexibility for generating scenes with complex geometry. In the second stage, we use DMTet  to extract textured mesh from the NeRF obtained in the first stage, and further fine-tune the textured mesh for high-resolution details. The second stage is optional because both NeRF and mesh have their own advantages in representing 3D content and are preferred in certain cases. Nevertheless, ProlificDreamer can generate both high-fidelity NeRFs and meshes.

### 3D Representation and Training

We systematically study other elements orthogonal to the algorithmic formulation. Specifically, we propose a high rendering resolution of \(512 512\) during training and an annealed distilling time schedule to improve the visual quality. We also carefully design a scene initialization, which is crucial for complex scene generation.

**High-resolution rendering for NeRF training.** We choose Instant NGP  for efficient high-resolution rendering and optimize NeRF with up to 512 training resolution using VSD. By applying VSD, we obtain high-fidelity NeRFs with resolutions varying from \(64\) to \(512\).

**Scene initialization for NeRF training.** We initialize the density for NeRF as \(_{}()=_{}(1-||_{2} }{r})\), where \(_{}\) is the density strength, \(r\) is the density radius, and \(\) is the coordinate. For object-centric scenes, we follow the _object-centric initialization_ used in Magic3D  with \(_{}=10\) and \(r=0.5\); For complex scenes, we propose _scene initialization_ by setting \(_{}=-10\) to make the density "hollow" and \(r=2.5\) that encloses the camera. We show in Appendix D that the scene initialization can help to generate high-fidelity complex scenes without other modifications to the existing algorithm. In addition, we can further add a centrie object to the complex scene by using object-centric initialization for \(||||_{2}<5/6\) and scene initialization for others, where the hyperparameter \(5/6\) ensures the initial density function is continuous.

**Annealed time schedule for score distillation.** We utilize a simple two-stage annealing of time step \(t\) in the score distillation objective, suitable for both SDS (Eq. (3)) and VSD (Eq. (9)). For the first several steps we sample time steps \(t(0.02,0.98)\) and then anneal into \(t(0.02,0.50)\). The key insight is that, essentially, we aim to match the original \(q_{0}^{t_{0}}(_{0}|c,y)\) with \(p_{0}(_{0}|y^{c})\). The KL-divergence for larger \(t\) can provide reasonable optimization direction during the early stage of training. During training, while \(^{}\) is approaching the support of \(p_{0}(^{}|y^{c})\), a smaller \(t\) can narrow the gap between \(p_{t}(^{}|y^{c})\) and \(p_{0}(^{}|y^{c})\), and provide elaborate details aligning with \(p_{0}(^{}|y^{c})\).

**Mesh representation and fine-tuning.** We adopt a coordinate-based hash grid encoder inherited from NeRF stage to represent the mesh texture. We follow Fantasia3D  to disentangle the

Figure 4: Comparison with baselines. Our results have higher fidelity and more details.

optimization of geometry and texture by first optimizing the geometry using the normal map and then optimizing the texture. In our initial experiments, we find that optimizing geometry with VSD provides no more details than using SDS. This may be because the mesh resolution is not large enough to represent high-frequency details. Thus, we optimize geometry with SDS for efficiency. But unlike Fantasia3D , our texture optimization is supervised by VSD with CFG = 7.5 with the annealed time schedule, which can provide more details than SDS.

## 5 Experiments

### Results of ProlificDreamer

We show the generated results of ProlificDreamer in Fig. 1(a) and 1(b), including high-fidelity mesh and NeRF results. All the results are generated by VSD. For all experiments without mentioned, VSD uses \(n=1\) particle for a fair comparison with SDS. In Fig. 1(c), we also demonstrate VSD can generate diverse results, showing that different particles in a round are diverse (with \(n=4\)).

**Object-centric generation.** We compare our method with three SOTA baselines, DreamFusion , Magic3D  and Fantasia3D . All of the baselines are based on SDS. Since none of them is open-sourced, we use the figures from their papers. As shown in Fig. 4, ProlificDreamer generates 3D objects with higher fidelity and more details, which demonstrates the effectiveness of our method.

In appendix, we add user study (Section J) and quantitative results (Section K) of ProlificDreamer against the baselines to demonstrate the effectiveness of our method.

**Large scene generation.** As shown in Fig. 1(b), our method can generate \(360^{}\) scenes with high-fidelity and fine details. The depth map shows that the scenes have geometry instead of being a \(360^{}\) textured sphere, verifying that with our scene initialization alone we can generate high-fidelity large scenes without much modification to existing components. See more results in Appendix B.

### Ablation Study

**Ablation on NeRF training.** Fig. 5 provides the ablation on NeRF training. Starting from the common setting [34; 20] with 64 rendering resolution and SDS loss, we ablate our proposed improvements step by step, including increasing resolution, adding annealed time schedule, and adding VSD all improve the generated results. It demonstrates the effectiveness of our proposed components. We provide more ablation on large scene generation in Appendix D, with a similar conclusion.

**Ablation on mesh fine-tuning.** We ablate between SDS and VSD on mesh fine-tuning, as shown in Appendix D. Fine-tuning texture with VSD provides higher fidelity than SDS. As the fine-tuned results of textured mesh are highly dependent on the initial NeRF, getting a high-quality NeRF at the first stage is crucial. Note that the provided results of both VSD and SDS in mesh fine-tuning are based on and benefit from the high-fidelity NeRF results in the first stage by our VSD.

**Ablation on CFG.** We perform ablation to explore how CFG affects generation diversity. We find that smaller CFG encourages more diversity. Our VSD works well with small CFG and provides

Figure 5: Ablation study of proposed improvements for high-fidelity NeRF generation. The prompt is _an elephant skull_. (1) The common setting [34; 20] adopts \(64\) rendering resolution and SDS loss. (2) We improve the generated quality by increasing the rendering resolution. (3) Annealed time schedule adds more details to the generated result. (4) VSD makes the results even better with richer details.

considerable diversity, while SDS cannot generate plausible results with small CFG (e.g., \(7.5\)), which limits its ability to generate diverse results. Results and more details are shown in Appendix H.

## 6 Related Works

Diffusion models.Score-based generative model [48; 49] and diffusion models  have shown great performance in image synthesis [14; 7]. Recently, large-scale diffusion models have shown great performance in text-to-image synthesis [36; 41; 38], which provides an opportunity to utilize it for zero-shot text-to-3D generation.

Text-to-3D generation.DreamField  proposes a text-to-3D method using CLIP  guidance. DreamFusion  proposes a text-to-3D method using 2D diffusion models. Score Jacobian Chaining (SJC)  derives the training objective of text-to-3D using a 2D diffusion model from another theoretical basis. Magic3D  extends text-to-3D to a higher resolution with mesh  representation. Latent-NeRF  optimizes NeRF in latent space. Fantasia3D  optimizes a mesh with DMTet  from scratch. Although Fantasia3D achieves remarkable zero-shot text-to-3D generation, it requires user-provided shape guidance for the generation of complex geometry.  propose score debiasing and prompt debiasing to mitigate multiface problem and is orthogonal to our work. TextMesh  is contemporary with us and proposes a different pipeline for high-fidelity text-to-3D generation. 3DFuse  proposes to incorporate 3D awareness into 2D diffusion for better 3D consistency in text-to-3D generation. In addition, adjusting time schedule has also been discussed in previous works [12; 55]. However, previous works either require carefully devising the schedule  or perform inferior to the simple random time schedule . Instead, our 2-stage annealing schedule is easy to train and achieves better generation quality than the random time schedule.

Text-driven large scene generation.Text2Room  generates indoor rooms from a given prompt. However, it uses additional monocular depth estimation models as prior, and we do not use any additional models. Our method generates the wild text prompt and uses only a text-to-image diffusion model. Set-the-scene  is a contemporary work with us aimed at large scene generation with a different pipeline. Overall, ProlificDreamer uses an advanced optimization algorithm, i.e, VSD with our proposed two-stage annealed time schedule, which has a significant advantage over the previous SDS / SJC (see Appendix C.3 for details). As a result, ProlificDreamer achieves high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops) and photo-realistic mesh results.

## 7 Conclusion

In this work, we systematically study the problem of text-to-3D generation. In terms of the algorithmic formulation, we propose variational score distillation (VSD), a principled particle-based variational framework that treats the 3D parameter as a random variable and infers its distribution. VSD naturally generalizes SDS in the variational formulation and addresses the practical issues of SDS observed before. With other orthogonal improvements to 3D representations, our overall approach, ProlificDreamer, can generate high-fidelity NeRF and photo-realistic textured meshes.

Limitations and broader impact.Although ProlificDreamer achieves remarkable text-to-3D results, the generation takes hours of time, which is much slower than image generation by a diffusion model. Although large scene generation can be achieved with our scene initialization, the camera poses during training are regardless of the scene structure, which may be improved by devising an adaptive camera pose range according to the scene structure for better-generated details. Moreover, due to the limited expressiveness of the base 2D model, the generation for complex prompts may fail, and the generated samples may have multi-face Janus problem  in some cases because of the poor text-image alignment for the view-dependent prompts. In addition, content creation by generative models may cause harmful social impacts on the labor market. Also, like other generative models, our method may be utilized to generate fake and malicious contents, which needs more caution.