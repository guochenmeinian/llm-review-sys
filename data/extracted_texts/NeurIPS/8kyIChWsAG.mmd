# When Does Optimizing a Proper Loss Yield Calibration?

Jaroslaw Blasiok

Columbia University

jb4451@columbia.edu &Parikshit Gopalan

Apple

parik.g@gmail.com &Lunjia Hu

Stanford University

lunjia@stanford.edu &Preetum Nakkiran

Apple

preetum.nakkiran@gmail.com

###### Abstract

Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade and Foster (2008); Blasiok et al. (2023b). Local optimality is plausibly satisfied by well-trained DNNs, which suggests an explanation for why they are calibrated from proper loss minimization alone. Finally, we show that the connection between local optimality and calibration error goes both ways: nearly calibrated predictors are also nearly locally optimal.

## 1 Introduction

In supervised prediction with binary labels, two basic criteria by which we judge the quality of a predictor are accuracy and calibration. Given samples from a distribution \(\) on \(\{0,1\}\) that corresponds to points \(x\) from \(\) with binary labels \(y\{0,1\}\), we wish to learn a predictor \(f\) that assigns each \(x\) a probability \(f(x)\) that the label is \(1\). Informally, accuracy measures how close the predictor \(f\) is to the ground-truth \(f^{*}(x)=[y|x]\).1 Calibration (Dawid, 1982; Foster and Vohra, 1998) is an interpretability notion originating from the literature on forecasting, which stipulates that the predictions of our model be meaningful as probabilities. For instance, on all points where \(f(x)=0.4\), calibration requires the true label to be \(1\) about \(40\%\) of the time. Calibration and accuracy are complementary notions: a reasonably accurate predictor (which is still far from optimal) need not be calibrated, and calibrated predictors (e.g. predicting the average) can have poor accuracy. The notions are complementary but not necessarily conflicting; for example, the ground truth itself is both optimally accurate and perfectly calibrated.

Proper losses are central to the quest for accurate approximations of the ground truth. Informally a proper loss ensures that under binary labels \(y\) drawn from the Bernoulli distribution with parameter \(v^{*}\)the expected loss \([(y,v)]\) is minimized at \(v=v^{*}\) itself. Familiar examples include the squared loss \(_{}(y,v)=(y-v)\)2 and the cross-entropy loss \(_{}(y,v)=y(1/v)+(1-y)(1/(1-v))\). A key property of proper losses is that over the space of all predictors, the expected loss \(_{}[(y,f(x))]\) is minimized by the ground truth \(f^{*}\) which also gives calibration. However minimizing over all predictors is infeasible. Hence typical machine learning deviates from this ideal in many ways: we restrict ourselves to models of bounded capacity such as decision trees or neural nets of a given depth and architecture, and we optimize loss using algorithms like SGD, which are only expected to find approximate local minima in the parameter space. What calibration guarantees transfer to this more restricted but realistic setting?

The State of Calibration & Proper Losses.There is a folklore belief in some quarters that optimizing a proper loss, even in the restricted setting, will produce a calibrated predictor. Indeed, the user guide for scikit-learn (Pedregosa et al., 2011) claims that _LogisticRegression returns well calibrated predictions by default as it directly optimizes Log loss_2. Before going further, we should point out that this statement in its full generality is just not true: **for general function families \(\), minimizing proper loss (even globally) over \(\) might not result in a calibrated predictor**. Even if the class \(\) contains perfectly calibrated predictors, the predictor found by loss minimization need not be (even close to) calibrated. A simple example showing that even logistic regression can fail to give calibrated predictors is provided in Appendix B.

Other papers suggest a close relation between minimizing a proper loss and calibration, but stop short of formalizing or quantifying the relationship. For example, in deep learning, Lakshminarayanan et al. (2017) states _calibration can be incentivised by proper scoring rules_. There is also a large body of work on post-hoc recalibration methods. The recipe here is to first train a predictor via loss minimization over the training set, then compose it with a simple post-processing function chosen to optimize cross-entropy on a holdout set (Platt, 1999; Zadrozny and Elkan, 2002). See for instance Platte scaling (Platt, 1999), where the post-processing functions are sigmoids, and which continues to be used in deep learning (e.g. for temperature scaling as in Guo et al. (2017)). Google's data science practitioner's guide recommends that, for recalibration methods, _the calibration [objective] function should minimize a strictly proper scoring rule_(Richardson and Pospisil, 2021). In short, these works suggest recalibration by minimizing a proper loss, sometimes in conjunction with a simple family of post-processing functions. However, there are no rigorous bounds on the calibration error using these methods, nor is there justification for a particular choice of post-processing functions.

Yet despite the lack of rigorous bounds, there are strong hints from practice that in certain settings, optimizing a proper-loss does indeed yield calibrated predictors. Perhaps the strongest evidence comes from the newest generation of deep neural networks (DNNs). These networks are trained to minimize a proper loss (usually cross-entropy) using SGD or variants, and are typically quite far from the ground truth, yet they turn out to be surprisingly well-calibrated. This empirical phenomenon occurs in both modern image classifiers (Minderer et al., 2021; Hendrycks* et al., 2020), and language models (Desai and Durrett, 2020; OpenAI, 2023). The situation suggests that there is a key theoretical piece missing in our understanding of calibration, to capture why models can be so well-calibrated "out-of-the-box." This theory must be nuanced: not all ways of optimizing a proper loss with DNNs yields calibration. For example, the previous generation of image classifiers were poorly calibrated (Guo et al., 2017), though the much smaller networks before them were well-calibrated (Niculescu-Mizil and Caruana, 2005). Whether DNNs are calibrated, then depends on the architecture, distribution, and training algorithm-- and our theory must be compatible with this.

In summary, prior work suggests a close but complicated relationship between minimizing proper loss and calibration. On one hand, for simple models like logistic regression, proper loss minimization does not guarantee calibration. On the other hand, for DNNs, certain ways (but not all ways) of optimizing a proper loss appears to yield well-calibrated predictors. The goal of our work is to analyze this phenomena from a theoretical perspective.3 Our motivating question is: **What minimal conditions on model family and training procedure guarantee that optimizing for a proper loss provably yields small calibration error?**

### Our Contributions

The main contribution of this work is to identify a **local optimality condition** for a predictor that is both necessary and sufficient to guarantee calibration. The local optimality condition requires that the loss \(\) cannot be reduced significantly through post-processing using a family of functions \(K_{}\) that have certain Lipschitz-ness guarantees, where post-processing means applying a function \(:\) to the output of the predictor. This condition is illustrated in Figure 1; it is distinct from the standard local optimality condition of vanishing gradients. We prove that a predictor satisfying this local optimality condition is smoothly calibrated in the sense of Kakade and Foster (2008); Gopalan et al. (2022b), Blasiok et al. (2023b). Smooth calibration is a _consistent_ calibration measure that has several advantages over the commonly used Expected Calibration Error (ECE); see the discussion in Kakade and Foster (2008), Blasiok et al. (2023b). Quantitatively for the case of the squared loss, we present a tight connection between smooth calibration error and the reduction in loss that is possible from post-processing with \(K_{}\). For other proper loss functions \(\), we give a tight connection between post-processing with \(K_{}\) and a calibration notion we call dual smooth calibration. The fact that the connection goes both ways implies that a predictor that does not satisfy our local optimality condition is far from calibrated, so we have indeed identified the minimal conditions needed to ensure calibration.

**Implications.** Heuristically, we believe these theoretical results shed light on why modern DNNs are often calibrated in practice. There are at least three possible mechanisms, at varying levels of formality.

First, informally: poor calibration implies that the test loss can be reduced noticeably by post-processing with a _simple_ function. But simple post-processings can be represented by adding a few layers to the DNN-- so we could plausibly improve the test loss by adding a few layers and re-training (or continuing training) with SGD. Thus, if our DNN has been trained to the point where such easy gains are not left on the table (as we expect of state-of-the-art models), then it is well-calibrated.

The second possible mechanism follows from one way of formalizing the above heuristic, yielding natural learning algorithms that provably achieve calibration. Specifically, consider Structural Risk Minimization (SRM): globally minimizing a proper loss plus a complexity regularizer, within some restricted function family. In Section 5, we prove SRM is guaranteed to produce a calibrated predictor, provided: (1) the function family is closed under post-processing and (2) the complexity regularizer grows only mildly under post-processing. If the informal "implicit regularization hypothesis" in deep learning is true, and SGD on DNNs is equivalent to SRM with an appropriate complexity-regularizer (Arora et al., 2019; Neyshabur et al., 2014; Neyshabur, 2017; Zhang et al., 2021), then our results imply DNNs are well-calibrated as long as the regularizer satisfies our fairly mild assumptions.

Finally, a third mechanism for calibration involves a heuristic assumption about the optimizer (SGD). Informally, for a sufficiently deep network, updating the network parameters from computing the function \(f\) to computing the post-processed \(( f)\) may be a "simple" update for SGD. Thus, if it were possible to improve the loss via such a simple post-processing, then SGD would have already exploited this by the end of training.

Figure 1: Schematic depiction of the loss landscape of a proper loss on the population distribution. The red dot represents a well-calibrated predictor \(f\). The blue curve represents all predictors \(\{ f: K_{}\}\) obtained by admissible post-processing of \(f\). Since the proper loss cannot be decreased by post-processing (the red dot is a minimal point on the blue curve), the predictor \(f\) is well-calibrated, even though it is not a global minimum.

These algorithms and intuitions also suggest a practical guidance: for calibration, one should optimize loss over function families that are capable of computing rich classes of post-processing functions. The universality properties of most DNN architectures imply they can compute a rich family of post-processing functions with small added depth (Cybenko, 1989; Lu et al., 2017; Yun et al., 2019; Zhou, 2020) but the family of functions used in logistic regression (with logits restricted to being affine in the features) cannot.

Our results are consistent with the calibration differences between current and previous generation models, when viewed through the lens of generalization. In both settings, models are expected to be locally-optimal with respect to _train loss_(Carrell et al., 2022), since this is the explicit optimization objective. Calibration, however, requires local-optimality with respect to _test loss_. Previous generation image classification models were trained to interpolate on small datasets, so optimizing the train loss was very different from optimizing the test loss (e.g. Guo et al., 2017; Mukhoti et al., 2020). Current generation models, in contrast, are trained on massive datasets, where optimizing the train loss is effectively equivalent to optimizing the test loss-- and so models are close to locally-optimal with respect to both, implying calibration.

**Organization.** We first give a technical overview of our results in Section 2. In Section 3 we prove our main result in the case of square-loss. Then in Section 4 we extend this result to a wide family of proper losses. Finally, in Section 5 we present natural algorithms which provably achieve calibration, as a consequence by our results. Additional related works are presented in Appendix A.

## 2 Overview

In this section, we present a formal but still high-level overview of our results. We first set some notation. All our probabilities and expectations are taken with respect to a distribution \(\) over \(\{0,1\}\). A predictor is a function \(f:\) that assigns probabilities to points, the ground-truth predictor is \(f^{*}(x)=[y|x]\). A predictor is perfectly calibrated if the set \(A=\{v:[y|f(x)=v] v\}\) has measure \(0\) (i.e. \(_{}(f(x) A)=0\)). A post-processing function is a function \(:\). A loss function is a function \(:\{0,1\}\), where \((y,v)\) represents the loss suffered when we predict \(v\) and the label is \(y\{0,1\}\). Such a loss is _proper_ if when \(y(v^{*})\) is sampled from the Bernoulli distribution with parameter \(v^{*}\), the expected loss \([(y,v)]\) is minimized at \(v=v^{*}\), and we say a loss is _strictly_ proper if \(v^{*}\) is the unique minimizer.

As a warm-up, we present a characterization of perfect calibration in terms of perfect local optimally over the space of all possible (non-Lipschitz) post-processing functions.

**Claim 2.1**.: _For every strictly proper loss \(\), a predictor \(f\) is perfectly calibrated iff for every post-processing function \(\),_

\[*{}_{}[(y,f(x))]*{ }_{}[(y,(f(x)))]. \]

This is true, because if \(f\) is perfectly calibrated, and \(\) arbitrary, then after conditioning on arbitrary prediction \(f(x)\) we have:

\[[(y,f(x))|f(x)=v]=*{}_{y(v )}[(y,v)]*{}_{y(v)}[(y, (v))],\]

and (1) follows by averaging over \(v\). On the other hand if \(f\) is not perfectly calibrated, we can improve the expected loss by taking \((v):=[y|f(x)=v]\) -- by the definition of the strictly proper loss, for each \(v\) we will have \([(y,(f(x))|f(x)=v][(y,f(x))|f(x)=v]\), and on a set of positive probability the inequality will be strict.

While we are not aware of this precise claim appearing previously, statements that are similar in spirit are known; for instance, by the seminal work of Foster and Vohra (1998) which characterizes calibration in terms of swap regret for the squared loss. In our language they show the equivalent Claim 2.1 for \(\) being a squared loss, and \(\) restricted to be of the form \((v_{0}):=w_{0}\), \((v):=v\) for all \(v v_{0}\).

Claim 2.1 connects calibration to a local optimality condition over the space of all possible post-processing functions. If this is satisfied, it guarantees calibration even though the loss might be far from the global minimum. This sheds light on classes of predictors such as decision trees or branching programs, which are closed under composition with arbitrary post-processing functions,since this amounts to relabelling the leaf nodes. It tells us that such models ought to be calibrated from proper loss minimization, as long as the optimization achieves a fairly weak notion of local optimality.

The next step is to ask whether DNNs could satisfy a similar property. Closure under arbitrary post-processing seems too strong a condition for DNNs to satisfy -- we should not expect DNNs to be able to express arbitrarily discontinuous uni-variate post-processing functions. But we do not expect or require DNNs to be perfectly calibrated, only to be close to perfectly calibrated. Measuring the distance from calibration involves subtle challenges as highlighted in the recent work of Blasiok et al. (2023). This leads us to the following robust formulation which significantly generalizes Claim 2.1:

* Using the well known relation between proper losses and Bregman divergences (Savage, 1971; Gneiting and Raftery, 2007), we consider proper loss functions where the underlying convex functions satisfy certain differentiability and smoothness properties. This is a technical condition, but one that is satisfied by all proper loss functions commonly used in practice.
* We only allow post-processing functions that satisfy certain Lipschitzness properties. Such functions are simple enough to be implementable by DNNs of constant depth. Indeed we heuristically believe that state-of-the-art DNNs will automatically be near-optimal w.r.t. such post-processings, as mentioned in Section 1.1.
* We measure calibration distance using the notion of smooth calibration from Kakade and Foster (2008) and strengthenings of it in Gopalan et al. (2022), Blasiok et al. (2023). Smooth calibration plays a central role in the consistent calibration measures framework of Blasiok et al. (2023), where it is shown using duality that the smCE is linearly related with the Wasserstein distance to the nearest perfectly calibrated predictor (Blasiok et al., 2023). In practice, there exist sample-efficient and linear-time estimators which approximate the smooth calibration error within a quadratic factor (Blasiok et al., 2023).

With these notions in place, we show that the _post-processing gap_ of a predictor, which measures how much the loss can be reduced via certain Lipschitz post-processing functions provides both an upper and a lower bound on the smooth calibration error. This result gives us a robust and quantitative version of Claim 2.1. The main technical challenge in formulating this equivalence is that for general \(\), the class of post-processing functions that we allow and the class of smooth functions that we use to measure calibration error are both now dependent on the loss function \(\), they are Lipschitz over an appropriately defined dual space to the predictions defined using convex duality. In order to keep the statements simple, we state our results for the special case of squared loss and cross entropy loss below, defering the full statement to the technical sections.

Squared loss.For the squared loss \(_{}(y,v)=(y-v)^{2}\), we define the post-processing gap \(_{}(f)\) to be the difference between the expected squared loss \([_{}(y,f(x))]\) of a predictor \(f\) and the minimum expected squared loss after we post-process \(f(x) f(x)+(f(x))\) for some \(1\)-Lipschitz \(\). The Brier score (Brier et al., 1950; Foster and Vohra, 1998) uses the squared loss of a predictor as a calibration measure; \(\) can be viewed as a _differential_ version of Brier score.

**Definition 2.2** (Post-processing gap).: _Let \(K\) denote the family of all post-processing functions \(:\) such that the update function \((v)=(v)-v\) is \(1\)-Lipschitz. For a predictor \(f:\) and a distribution \(\) over \(\{0,1\}\), we define the post-processing gap of \(f\) w.r.t. \(\) to be_

\[_{}(f):=_{(x,y)}[_{}(y,f(x))]-_{ K}_{(x,y)}[_{ }(y,(f(x)))].\]

We define the smooth calibration error \(_{}(f)\) as in Kakade and Foster (2008); Gopalan et al. (2022), Blasiok et al. (2023) to be the maximum correlation between \(y-f(x)\) and \((f(x))\) over all bounded \(1\)-Lipschitz functions \(\). Smooth calibration is a _consistent_ measure of calibration (Blasiok et al., 2023). It does not suffer the discontinuity problems of ECE (Kakade and Foster, 2008), and is known to be linearly related with the Wasserstein distance to the nearest perfectly calibrated predictor (Blasiok et al. (2023)). In particular, it is \(0\) if and only if we have perfect calibration. We refer the reader to these papers for a detailed discussion of its merits.

**Definition 2.3** (Smooth calibration error).: _Let \(H\) be the family of all \(1\)-Lipschitz functions \(:[-1,1]\). The smooth calibration error of a predictor \(f:\) with respect to distribution \(\)over \(\{0,1\}\) is defined as_

\[_{}(f):=_{ H}_{(x,y)}[ (y-f(x))(f(x))].\]

Our main result is a quadratic relationship between these two quantities:

**Theorem 2.4**.: _For any predictor \(f:\) and any distribution \(\) over \(\{0,1\}\),_

\[_{}(f)^{2}_{}(f) 2\, _{}(f).\]

In Appendix D we show that the constants in the inequality above are optimal.

Cross-entropy loss.For the cross entropy loss \(_{}(y,v)=-y v-(1-y)(1-v)\), we observe its close connection to the logistic loss \(^{()}(y,t)=(1+e^{t})-yt\) given by the equation

\[_{}(y,v)=^{()}(y,t) \]

where \(t:=(v/(1-v))\) is what we call the _dual prediction_ corresponding to \(v\). While a standard terminology for \(t\) is _logit_, we say \(t\) is the dual prediction because this notion generalizes to arbitrary proper loss functions. One can conversely obtain a prediction \(v\) from its dual prediction \(t\) by taking the sigmoid transformation: \(v=(t):=1/(1+e^{-t})\). The superscript \(\) in the logistic loss \(^{()}\) can be understood to indicate its relationship to the dual prediction and the exact meaning is made clear in Section 4. Based on (2), optimizing the cross-entropy loss \(_{}\) over predictions \(v\) is equivalent to optimizing the logistic loss \(^{()}\) over dual predictions \(t\).

Usually, a neural network that aims to minimize the cross-entropy loss has a last layer that computes the sigmoid \(\) (the binary version of softmax), so the value computed by the network before the sigmoid (the "logit") is the dual prediction. If we want to enhance the neural network by adding more layers, these layers are typically inserted before the final sigmoid transformation. It is thus more natural to consider post-processings on the dual predictions (logits) rather than on the predictions themselves.

For a predictor \(f\), we define the _dual post-processing gap_\(^{(,1/4)}(g)\) of its dual predictor \(g(x)=(f(x)/(1-f(x)))\) to be the difference between the expected logistic loss \([^{()}(y,g(x))]\) of \(g\) and the minimum expected logistic loss after we post-process \(g(x) g(x)+(g(x))\) for some \(1\)-Lipschitz \(:[-4,4]\), where the constant \(4\) comes from the fact that the logistic loss is \(1/4\)-smooth in \(t\).

**Definition 2.5** (Dual post-processing gap for cross-entropy loss, special case of Definition 4.4).: _Let \(K\) denote the family of all post-processing functions \(:\) such that the update function \((t):=(t)-t\) is \(1\)-Lipschitz and bounded \(|(t)| 4\). Let \(^{()}\) be the logistic loss. Let \(\) be a distribution over \(\{0,1\}\). We define the dual post-processing gap of a function \(g:\) to be_

\[^{(,1/4)}_{}(g):=_{(x,y)} \,^{()}(y,g(x))-_{ K}_{(x,y)}\, ^{()}y,(g(x)).\]

We define the _dual smooth calibration error_\(^{(,1/4)}(g)\) to be the maximum of \(|\,[(y-f(x))(g(x))]|\) over all \(1/4\)-Lipschitz functions \(:[-1,1]\). Like with smooth calibration, it is \(0\) if and only if the predictor \(f\) is perfectly calibrated.

**Definition 2.6** (Dual smooth calibration for cross-entropy loss, special case of Definition 4.5).: _Let \(H\) be the family of all \(1/4\)-Lipschitz functions \(:[-1,1]\). For a function \(g:\), define predictor \(f:\) such that \(f(x)=(g(x))\) for every \(x\) where \(\) is the sigmoid transformation. Let \(\) be a distribution over \(\{0,1\}\). We define the dual calibration error of \(g\) as_

\[^{(,1/4)}_{}(g):=_{ H}|\,_{( x,y)}[(y-f(x))(g(x))]|.\]

We show a result similar to Theorem 2.4 that the dual post-processing gap and the dual smooth calibration error are also quadratically related. Moreover, we show that a small dual smooth calibration error implies a small (standard) smooth calibration error.

**Corollary 2.7** (Corollary of Theorem 4.6 and Lemma 4.7).: _Let \(^{(,1/4)}\) and \(^{(,1/4)}\) be defined as in Definition 2.5 and Definition 2.6 for the cross-entropy loss. For any function \(g:\) and any distribution \(\) over \(\{0,1\}\),_

\[2\,^{(,1/4)}_{}(g)^{2}^{(,1/ 4)}_{}(g) 4\,^{(,1/4)}_{}(g). \]_Moreover, let predictor \(f:\) be given by \(f(x)=(g(x))\) for the sigmoid transformation \(\). Its (standard) smooth calibration error \(_{}(f)\) defined in Definition 2.3 satisfies_

\[_{}(f)_{}^{(,1/4)}(g). \]

The constants in the corollary above are optimal as we show in Lemmas D.3 and D.4 in the appendix. Both results (3) and (4) generalize to a wide class of proper loss functions as we show in Section 4.

**Remark 2.8**.: _In a subsequent work, Blasiok and Nakkiran  proved the reverse direction of (4) for the cross-entropy loss: \(_{}(f)_{}^{( ,1/4)}(g)^{2}\). Hence both \(^{(,1/4)}(g)\) and \(^{(,1/4)}(g)\) are consistent calibration measures, a notion introduced by Blasiok et al. [2023b]._

Our result shows that achieving a small dual post-processing gap when optimizing a cross-entropy is necessary and sufficient for good calibration guarantees. It sheds light on the examples where logistic regression fails to yield a calibrated predictor; in those instances the cross-entropy loss can be further reduced by some Lipschitz post-processing of the logit. This is intuitive because logistic regression only optimizes cross-entropy within the restricted class where the logit is a linear combination of the features plus a bias term, and this class is not closed under Lipschitz post-processing.

## 3 Calibration and Post-processing Gap for the Squared Loss

In this section we prove our main result Theorem 2.4 relating the smooth calibration error of a predictor to its post-processing gap with respect to the squared loss.

Proof of Theorem 2.4.: We first prove the upper bound on \(_{}(f)\). For any \( K\) in the definition of \(\) (Definition 2.2), there exists a \(1\)-Lipschitz function \(:[-1,1]\) such that \((v)=v+(v)\) for every \(v\). For the squared loss \(\),

\[_{(x,y)}[(y,(f(x)))] =[(y-f(x)-(f(x)))^{2}]\] \[=[(y-f(x))^{2}]-2\,[(y-f(x))(f(x))]+[(f(x))^{2}]. \]

The three terms on the right hand side satisfy

\[[(y-f(x))^{2}] =[(y,f(x))],\] \[[(y-f(x))(f(x))] _{}(f),\] \[[(f(x))^{2}]  0.\]

Plugging these into (5), we get

\[[(y,f(x))]-[(y,(f(x)))] 2_{ }(f).\]

Since this inequality holds for any \( K\), we get \(_{}(f) 2_{}(f)\) as desired.

Now we prove the lower bound on \(_{}(f)\). For any \(1\)-Lipschitz function \(:[-1,1]\), define \(:=[(y-f(x))(f(x))][-1,1]\) and define post-processing \(:\) such that

\[(v)=_{}(v+(v)),\]

where \(_{}(u)\) is the value in \(\) closest to \(u\), i.e., \(_{}(u)=((u,0),1)\). By Lemma H.2, we have \( K\). The expected squared loss after we apply the post-processing \(\) can be bounded as follows:

\[_{(x,y)}[(y,(f(x)))] [(y-f(x)-(f(x)))^{2}]\] \[=[(y-f(x))^{2}]-2\,[(y-f(x))(f(x))]+ ^{2}\,[(f(x))^{2}]\] \[=[(y,f(x))]-2^{2}+^{2}\,[( f(x))^{2}]\] \[[(y,f(x))]-2^{2}+^{2}.\]

Re-arranging the inequality above, we have

\[[(y-f(x))(f(x))]^{2}=^{2}[(y,f(x))]- [(y,(f(x)))]_{}(f).\]

Since this inequality holds for any \(1\)-Lipschitz function \(:[-1,1]\), we get \(_{}(f)^{2}_{}(f)\), as desired. 

In Appendix D we provide examples showing that the constants in Theorem 2.4 are optimal.

Generalization to Any Proper Loss

When we aim to minimize the squared loss, we have shown that achieving a small post-processing gap w.r.t. Lipschitz post-processings ensures a small smooth calibration error and vice versa. In this section, we extend this result to a wide class of _proper_ loss functions including the popular cross-entropy loss.

**Definition 4.1**.: _Let \(V\) be a non-empty interval. We say a loss function \(:\{0,1\} V\) is proper if for every \(v V\), it holds that \(v*{argmin}_{v^{} V}_{y(v)}[ (y,v^{})]\)._

One can easily verify that the squared loss \((y,v)=(y-v)^{2}\) is a proper loss function over \(V=\), and the cross entropy loss \((y,v)=-y v-(1-y)(1-v)\) is a proper loss function over \(V=(0,1)\).

It turns out that if we directly replace the squared loss in Definition 2.2 with an arbitrary proper loss, we do not get a similar result as Theorem 2.4. In Appendix C we give a simple example where the post-processing gap w.r.t. the cross-entropy loss can be arbitrarily larger than the smooth calibration error. Thus new ideas are needed to extend Theorem 2.4 to general proper loss functions. We leverage a general theory involving correspondence between proper loss functions and convex functions (Shufford et al., 1966; Savage, 1971; Schervish, 1989; Buja et al., 2005). We provide a detailed description of this theory in Appendix E. There we include a proof of Lemma E.4 which implies the following lemma:

**Lemma 4.2**.: _Let \(V\) be a non-empty interval. Let \(:\{0,1\} V\) be a proper loss function. For every \(v V\), define \((v):=(0,v)-(1,v)\). Then there exists a convex function \(:\) such that_

\[(y,v)=((v))-y\,(v)$ and $v V$.} \]

_Moreover, if \(\) is differentiable, then \((t)\)._

Lemma 4.2 says that every proper loss induces a convex function \(\) and a mapping \(:V\) that maps a prediction \(v V\) to its _dual prediction_\((v)\). Our main theorem applies whenever the induced function \(\) is differentiable, and \(\) is \(\)-Lipschitz (equivalently \(\) is \(\)-smooth) -- those are relatively mild conditions that are satisfied by all proper loss functions of interest.

The duality relationship between \(v\) and \((v)\) comes from the fact that each \((v,(v))\) pair makes the Fenchel-Young divergence induced by a conjugate pair of convex functions \((,)\) take its minimum value zero, as we show in Appendix E. Equation (6) expresses a proper loss as a function that depends on \((v)\) rather than directly on \(v\), and thus minimizing a proper loss over predictions \(v\) is equivalent to minimizing a corresponding _dual loss_ over dual predictions \(t=(v)\):

**Definition 4.3** (Dual loss).: _For a function \(:\), we define a dual loss function \(^{()}:\{0,1\}\) such that_

\[^{()}(y,t)=(t)-yt$ and $t$.}\]

_Consequently, if a loss function \(:\{0,1\} V\) satisfies (6) for some \(V\) and \(:V\), then_

\[(y,v)=^{()}(y,(v))$ and $v V$.} \]

The above definition of a dual loss function is essentially the definition of the Fenchel-Young loss in the literature (see e.g. Duchi et al., 2018; Blondel et al., 2020). A loss function \(^{()}\) satisfying the relationship in (7) has been referred to as a _composite loss_(see e.g. Buja et al., 2005; Reid and Williamson, 2010).

For the cross-entropy loss \((y,v)=-y v-(1-y)(1-v)\), the corresponding dual loss is the logistic loss \(^{()}(y,t)=(1+e^{t})-yt\), and the relationship between a prediction \(v(0,1)\) and its dual prediction \(t=(v)\) is given by \(v=(t)\) for the sigmoid transformation \((t)=e^{t}/(1+e^{t})\).

For a predictor \(f: V\), we define its dual post-processing gap by considering dual predictions \(g(x)=(f(x))\) w.r.t. the dual loss \(^{()}\) as follows:

**Definition 4.4** (Dual post-processing gap).: _For \(>0\), let \(K_{}\) denote the family of all post-processing functions \(:\) such that there exists a \(1\)-Lipschitz function \(:[-1/,1/]\) satisfying \((t)=t+(t)\) for every \(t\). Let \(\) and \(^{()}\) be defined as in Definition 4.3. Let \(\) be a distribution over \(\{0,1\}\). We define the dual post-processing gap of a function \(g:\) to be_

\[^{(,)}_{}(g):=_{(x,y)}\,^{()}(y,g(x))-_{ K_{}}_{(x,y) }\,^{()}y,(g(x)).\]

Definition 4.4 is a generalization of Definition 2.2 from the squared loss to an arbitrary proper loss corresponding to a function \(\) as in (6). The following definition generalizes the definition of smooth calibration in Definition 2.3 to an arbitrary proper loss in a similar way. Here we use the fact proved in Appendix E (equation (20)) that if the function \(\) from Lemma 4.2 for a proper loss \(:\{0,1\} V\) is differentiable, then \(((v))=v\) holds for any \(v V\), where \(()\) denotes the derivative of \(\). This means that \(\) transforms a dual prediction to its original prediction.

**Definition 4.5** (Dual smooth calibration).: _For \(>0\), let \(H_{}\) be the family of all \(\)-Lipschitz functions \(:[-1,1]\). Let \(:\) be a differentiable function with derivative \((t)\) for every \(t\). For a function \(g:\), define predictor \(f:\) such that \(f(x)=(g(x))\) for every \(x\). Let \(\) be a distribution over \(\{0,1\}\). We define the dual calibration error of \(g\) to be_

\[^{(,)}_{}(g):=_{ H_{}}| \,_{(x,y)}[(y-f(x))(g(x))]|.\]

In Theorem 4.6 below we state our generalization of Theorem 2.4 to arbitrary proper loss functions. Theorem 4.6 shows that achieving a small dual post-processing gap is equivalent to achieving a small dual calibration error. We then show in Lemma 4.7 that a small dual calibration error implies a small (standard) smooth calibration error.

**Theorem 4.6**.: _Let \(:\) be a differentiable convex function with derivative \((t)\) for every \(t\). For \(>0\), assume that \(\) is \(\)-smooth, i.e.,_

\[|(t)-(t^{})||t-t^{}|t,t^{}. \]

_Then for every \(g:\) and any distribution \(\) over \(\{0,1\}\),_

\[^{(,)}_{}(g)^{2}/2\,^{(,)}_{}(g)^{(,)}_{ }(g).\]

**Lemma 4.7**.: _Let \(:\) be a differentiable convex function with derivative \((t)\) for every \(t\). For \(>0\), assume that \(\) is \(\)-smooth as in (8). For \(g:\), define \(f:\) such that \(f(x)=(g(x))\) for every \(x\). For a distribution \(\) over \(\{0,1\}\), define \(_{}(f)\) as in Definition 2.3. Then_

\[_{}(f)^{(,)}_{ }(g).\]

We defer the proofs of Theorem 4.6 and Lemma 4.7 to Appendix F.1 and Appendix F.2. Combining the two results, assuming \(\) is \(\)-smooth, we have

\[_{}(f)^{2}/2\,^{(, )}_{}(g).\]

This means that a small dual post-processing gap for a proper loss implies a small (standard) smooth calibration error. The cross-entropy loss \(\) is a proper loss where the corresponding \(\) from Lemma 4.2 is given by \((t)=(1+e^{t})\) and it is \(1/4\)-smooth. Setting \(=1/4\) in Theorem 4.6 and Lemma 4.7, we get (3) and (4) for the cross-entropy loss.

## 5 Optimization Algorithms and Implicit Regularization

As simple consequences of our results, there are several natural algorithms which explicitly minimize loss, but implicitly achieve good calibration. Here, we focus on one such algorithm: _structural risk minimization_, and discuss additional such algorithms in Appendix G. We also give informal intuitions connecting each algorithm to training DNNs in practice.

We show that structural risk minimization (SRM) on the population distribution is guaranteed to output well-calibrated predictors, under mild assumptions on the function family and regularizer. Recall, structural risk minimization considers a function family \(\) equipped with some "complexity measure" \(:_{ 0}\). For example, we may take the family of bounded-width neural networks, with depthas the complexity measure. SRM then minimizes a proper loss, plus a complexity-regularizer given by \(\):

\[f^{*}=*{argmin}_{f}*{}_{ }[(y,f(x)]+(f).\]

The complexity measure \(\) is usually designed to control the capacity of the function family for generalization reasons, though we will not require such assumptions about \(\). Rather, we only require that \((f)\) does not grow too quickly under composition with Lipshitz functions. That is, \(( f)\) should be at most a constant greater than \((f)\), for all Lipshitz functions \(\). Now, as long as the function family \(\) is also closed under composition, SRM is well-calibrated:

**Claim 5.1**.: _Let \(\) be a class of functions \(f:\) closed under composition with \(K\), where we define \(K\) as in Definition 2.2. That is, for \(f, K\) we have \( f\). Let \(:_{ 0}\) be any complexity measure satisfying, for all \(f, K:\ \ ( f)(f)+1\). Then the minimizer \(f^{*}\) of the regularized optimization problem_

\[f^{*}=*{argmin}_{f}*{}_{ }_{}(y,f(x))+(f).\]

_satisfies \(_{}(f^{*})\), and thus by Theorem 2.4 has small calibration error: \(_{}(f^{*})\)._

Proof.: The proof is almost immediate. Let \(_{}(f)\) denote \(*{}_{}[_{}(y,f(x))]\). Consider the solution \(f^{*}\) and arbitrary \( K\). Since \( f^{*}\), we have

\[_{}(f^{*})+(f^{*})_{}( f^{*})+( f^{*}) {MSE}_{}( f^{*})+(f^{*})+.\]

After rearranging, this is equivalent to

\[_{}(f^{*})-_{}(  f^{*}),\]

and since \( K\) was arbitrary, we get \(_{}(f)\). as desired. 

**Discussion: Implicit Regularization.** This aspect of SRM connects our results to the "implicit regularization" hypothesis from deep learning theory community (Neyshabur, 2017). The implicit regularization hypothesis is an informal belief that SGD on neural networks implicitly performs minimization on a "complexity-regularized" objective, which ensures generalization. The exact form of this complexity regularizer remains elusive, but many works have attempted to identify its structural properties (e.g. Arora et al. (2019); Neyshabur et al. (2014); Neyshabur (2017); Zhang et al. (2021)). In this context, our results imply that if the implicit regularization hypothesis is true, then as long as the complexity measure \(\) doesn't increase too much from composition, the final output of SGD will be well-calibrated.

## 6 Conclusion

Inspired by recent empirical observations, we studied formal conditions under which optimizing a proper loss also happens to yield calibration "for free." We identified a certain local optimality condition that characterizes distance to calibration, in terms of properties of the (proper) loss landscape. Our results apply even to realistic optimization algorithms, which optimize over restricted function families, and may not reach global minima within these families. In particular, our formal results suggest an intuitive explanation for why state-of-the-art DNNs are often well-calibrated: because their test loss cannot be improved much by adding a few more layers. It also offers guidance for how one can achieve calibration simply by proper loss minimization over a sufficiently expressive family of predictors.

**Limitations.** The connection between our theoretical results and DNNs in practice is heuristic-- for example, we do not have a complete proof that training a DNN with SGD on natural distributions will yield a calibrated predictor, though we have formal results that give some intuition for this. Part of the difficulty here is due to known definitional barriers (Nakkiran, 2021), which is that we would need to formally specify what "DNN" means in practice (which architectures, and sizes?), what "SGD" means in practice (what initialization, learning-rate schedule, etc), and what "natural distributions" means in practice. Finally, we intuitively expect the results from our work to generalize beyond the binary label setting; we leave it to future work to formalize this intuition.