# Neur2BiLO: Neural Bilevel Optimization

Justin Dumouchelle

University of Toronto

&Esther Julien

TU Delft

&Jannis Kurtz

University of Amsterdam

&Elias B. Khalil

University of Toronto

Corresponding author: justin.dumouchelle@mail.utoronto.ca

###### Abstract

Bilevel optimization deals with nested problems in which a _leader_ takes the first decision to minimize their objective function while accounting for a _follower_'s best-response reaction. Constrained bilevel problems with integer variables are particularly notorious for their hardness. While exact solvers have been proposed for mixed-integer _linear_ bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case. On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope. Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program. Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for four applications with linear and non-linear objectives and pure and mixed-integer variables.

## 1 Introduction

A motivating application.Consider the following _discrete network design problem_ (DNDP) [47; 48]. A transportation planning authority seeks to minimize the average travel time on a road network represented by a directed graph of nodes \(N\) and links \(A_{1}\) by investing in constructing a set of roads (i.e., links) from a set of options \(A_{2}\), subject to a budget \(B\). The planner knows the number of vehicles that travel between any origin-destination (O-D) pair of nodes. A good selection of links should take into account the drivers' reactions to this decision. One common assumption is that drivers will optimize their O-D paths such that a _user equilibrium_ is reached. This is known as _Wardrop's second principle_ in the traffic assignment literature, an equilibrium in which "no driver can unilaterally reduce their travel costs by shifting to another route" . This is in contrast to the _system optimum_, an equilibrium in which a central planner dictates each driver's route, an unrealistic assumption that would not require bilevel modeling. A link cost function is used to model the travel time on an edge as a function of traffic. Let \(c_{ij}_{+}\) be the capacity (vehicles per hour (vph)) of a link and \(T_{ij}_{+}\) the free-flow travel time (i.e., travel time on the link without congestion). The US Bureau of Public Roads uses the following widely accepted formula to model the travel time \(t(y_{ij})\) on a link used by \(y_{ij}\) vehicles per hour: \(t(y_{ij})=T_{ij}(1+0.15(y_{ij}/c_{ij})^{4})\). As the traffic \(y_{ij}\) grows to exceed the capacity \(c_{ij}\), a large quartic increase in travel time is incurred .

_Bilevel optimization_ (BiLO)  models the DNDP and many problems in which an agent (the _leader_) makes decisions that minimize their cost function subject to another agent's (the _follower_'s) best response. In the DNDP, the _leader_ is the transportation planner and the _follower_ is the population ofdrivers, giving rise to the following optimization problem

\[_{\{0,1\}^{|A_{2}|},} _{(i,j) A}y_{ij}t(y_{ij})\] s.t. \[_{(i,j) A_{2}}g_{ij}x_{ij} B,\] \[*{arg\,min}_{^{} _{+}^{|A|}} _{(i,j) A}_{0}^{y_{ij}^{}}t_{ij}(v)dv\] s.t. \[^{},\] \[x_{ij}=0 y_{ij}^{}=0,\]

where \(A_{2} A_{1}=,A=A_{1} A_{2}\). The leader minimizes the total travel time across all links subject to a budget constraint and the followers' equilibrium which is expressed as a network flow on the graph augmented by the leader's selected edges that satisfies O-D demands; the integral in the follower's objective models the desired equilibrium and evaluates to \(T_{ij}y_{ij}^{}+}{5c_{ij}^{4}}y_{ij}^{} ^{5}\).

Going beyond the DNDP, Dempe  lists more than 70 applications of BiLO ranging from pricing in electricity markets (leader is an electricity-supplying retailer that sets the price to maximize profit, followers are consumers who react accordingly to satisfy their demands ) to interdiction problems in security settings (leader inspects a budgeted subset of nodes on a road network, follower selects a path such that they evade inspection ).

Scope of this work.We are interested in _mixed-integer non-linear bilevel optimization_ problems, simply referred to hereafter as _bilevel optimization_ or BiLO, a very general class of bilevel problems where all constraints and objectives may involve non-linear terms and integer variables. At a high level, we have identified three limitations of existing computational methods for BiLO:

1. The state-of-the-art exact solvers of Fischetti et al.  and Tahernejad et al.  are limited to mixed-integer bilevel _linear_ problems and do not scale well. When high-quality solutions to large-scale problems are sought after, such exact solvers may be prohibitively slow.
2. Specialized algorithms, heuristic or exact, do not generalize beyond the single problem they were designed for. For instance, the state-of-the-art exact Knapsack Interdiction solver  only works for a single knapsack constraint and fails with two or more, a significant limitation even if one is strictly interested in knapsack-type problems.
3. Existing methods, exact or heuristic, generic or specialized, are not designed for the "data-driven algorithm design" setting  in which similar instances are routinely solved and the goal is to construct generalizable high-efficiency algorithms that leverage historical data.

Neur2BiLO (for _Neural Bilevel Optimization_) is a learning-based framework for bilevel optimization that deals with these issues simultaneously. The following observations make Neur2BiLO possible:

1. **Data collection is "easy":** For a fixed decision of the leader's, the optimal value of the follower can be computed by an appropriate (single-level) solver (e.g., for mixed-integer programming (MIP) or convex programming), enabling the collection of samples of the form: (leader's decision, follower's value, leader's value).
2. **Offline learning in the data-driven setting:** While obtaining data online may be prohibitive, access to historical training instances affords us the ability to construct, offline, a large dataset of samples that can then serve as the basis for learning an approximate value function using supervised regression. The output of this training is a regressor mapping a pair consisting of an instance and a leader's decision to an estimated follower or leader value.
3. **MIP embeddings of neural networks:** If the regressor is MIP-representable, e.g., a feedforward ReLU neural network or a decision tree, it is possible to use a MIP solver to find the leader's decision that minimizes the regressor's output. This MIP, which includes any leader constraints, thus serves as an approximate single-level surrogate of the original bilevel problem instance.
4. **Follower constraints via the value function reformulation:** The final ingredient of the Neur2BiLO recipe is to include any of the follower's constraints, some of which may involve leader variables. This makes the surrogate problem a heuristic version of the well-known _value function reformulation_ (VFR) in BiLO. The VFR transforms a bilevel problem into a single-level one, assuming that one can represent the follower's value (as a function of the leader's decision) compactly. This is typically impossible as the value function may require an exponential number of constraints, a bottleneck that is circumvented by our small (approximate) regression models.
5. **Theoretical guarantees:** For interdiction problems, a class of BiLO problems that attract much attention, Neur2BiLO solutions have a constant, additive absolute optimality gap which mainly depends on the prediction accuracy of the regression model.

Through a series of experiments on (i) the bilevel knapsack interdiction problem, (ii) the "critical node problem" from network security, (iii) a donor-recipient healthcare problem, and (iv) the DNDP, we will show that Neur2BiLO is easy to train and produces, very quickly, heuristic solutions that are competitive with state-of-the-art methods.

## 2 Background

Bilevel optimization (BiLO) deals with hierarchical problems where the _leader_ (or _upper-level_) problem decides on \(\) and parameterizes the _follower_ (or _lower-level_) problem that decides on \(\); the sets \(\) and \(\) represent the domains of the variables (continuous, mixed-integer, or pure integer). Both problems have their own objectives and constraints, resulting in the following model:

\[_{,} F(,)\] (1a) s.t. \[G(,),\] (1b) \[*{arg\,max}_{^{} }\{f(,^{}):g(,^{ })\},\] (1c)

where we consider the general mixed-integer non-linear case with \(F,f:,\ G: ^{m_{1}}\), and \(g:^{m_{2}}\) non-linear functions of the upper-level \(\) and lower-level variables \(\).

The applicability of exact (i.e., global) approaches critically depends on the nature of the lower-level problem. A continuous lower-level problem admits a single-level reformulation that leverages the Karush-Kuhn-Tucker (KKT) conditions as constraints on \(\). For linear programs in the lower level, strong duality conditions can be used in the same way. Solving a BiLO problem with integers in the lower level necessitates more sophisticated methods such as branch and cut [17; 24] along with some assumptions: DeNegre and Ralphs  do not allow for coupling constraints (i.e., \(G(,)=G()\)) and both methods do not allow continuous upper-level variables to appear in the linking constraints (\(g(,)\)). Other approaches, such as Benders decomposition, are also applicable . Gumis and Floudas  propose single-level reformulations of mixed-integer non-linear BiLO problems using polyhedral theory, an approach that only works for small problems. Later, "branch-and-sandwich" methods were proposed [33; 45] where bounds on both levels' value functions are used to compute an optimal solution. Algorithms for non-linear BiLO generally do not scale well. Kleinert et al.  survey more exact methods.

Assumptions.In what follows, we make the following standard assumptions:

1. Either (i) the follower's problem has a feasible solution for each \(\), or (ii) there are no coupling constraints in the leader's problem, i.e., \(G(,)=G()\);
2. The optimal follower value is always attained by a feasible solution [see 5, Section 7.2].

Value function reformulation.We consider the so-called _optimistic_ setting: if the follower has multiple optima for a given decision of the leader's, the one that optimizes the leader's objective is implemented. We can then rewrite problem (1) using the _value function reformulation_ (VFR):

\[_{,} F(,)\] (2a) s.t. \[G(,),\] (2b) \[g(,),\] (2c) \[f(,)(),\] (2d)with the _optimal lower-level value function_ defined as

\[()=_{}\{f(,):g( ,)\}.\] (3)

Lozano and Smith  used this formulation to construct an exact algorithm (without any public code) for solving mixed-integer non-linear BiLO problems with purely integer upper-level variables. Sinha et al. [50; 51; 52] propose a family of evolutionary heuristics for continuous non-linear BiLO problems that approximate the optimal value function by using quadratic and Kriging (i.e., a function interpolation method) approximations. Taking it one step further, Beykal et al.  extend the framework of the previous authors to handle mixed-integer variables in the lower level.

## 3 Methodology

Neur2BiLO refers to two learning-based single-level reformulations for general BiLO problems. The reformulations rely on representing the thorny nested structure of a BiLO problem with a trained regression model that predicts either the upper-level or lower-level value functions. Appendix B includes pseudocode for data collection, training, and model deployment.

### Neur2BiLO

Upper-level approximation.The obvious bottleneck in solving BiLO problems is their nested structure. One rather straightforward way of circumventing this difficulty is to get rid of the lower level altogether in the formulation, but predict its optimal value. Namely, we predict the optimal upper-level objective value function as

\[^{u}(;) F(,^{}),\] (4)

where \(\) are the weights of a neural network, \(F\) the objective function of the leader (2b), and \(^{}\) an optimal solution to the lower level problem (3). To train such a model, one can sample \(\) from \(\), solve (3) to obtain an optimal lower-level solution \(^{}\), and subsequently compute a label \(F(,^{})\). We can then model the single-level problem as

\[_{}^{u}(;) G(),\] (5)

where we only optimize for \(\) and thus dismiss the lower-level constraints and objective function. A trained feedforward neural network \(^{u}(:;)\) with ReLU activations can be represented as a mixed-integer linear program (MILP) , where now the input (and output) of the network are decision variables. With this representation, Problem (5) becomes a single-level problem and can be solved using an off-the-shelf MIP solver. Note that linear and decision tree-based models also admit MILP representations .

This reformulation is similar to the approach by Bagloee et al. , wherein the upper-level value function is predicted using linear regression. Our method differs in that it is not iterative and does not require the use of "no-good cuts" (which avoid reappearing solutions \(\)). As such, our method is extremely efficient as will be shown experimentally.

The formulation of (5) only allows for problem classes that do not have coupling constraints, i.e., \(G(,)=G()\). Moreover, the feasibility of a solution \(\) in the original BiLO problem is not guaranteed, an issue that will be addressed later in this section (see **Bilevel feasibility.**).

Lower-level approximation.This method makes use of the VFR (2). The VFR moves the nested complexity of a BiLO to constraint (2d), where the right-hand side is the optimal value of the lower-level problem, parameterized by \(\). We introduce a learning-based VFR in which \(()\) is approximated by a regression model with parameters \(\):

\[^{l}(;)().\] (6)

Both \(^{l}\) and \(^{u}\) take in a leader's decision as input and require solving the follower (3) for data generation. By replacing \(()\) with \(^{l}(;)\) in (2d) and introducing a slack variable \(s_{+}\), the surrogate VFR reads as:

\[_{, \\ s 0} F(,)+ s\] (7a) s.t. \[G(,),\] (7b) \[g(,),\] (7c) \[f(,)^{l}(;)-s.\] (7d)

All follower and leader constraints of the original BiLO problem are part of Problem (7). However, without the slack variable \(s\), the problem could become infeasible due to inaccurate predictions by the neural network. This happens when \(^{l}(;)\) strictly overestimates the follower's optimal value for each \(\). In this case, there does not exist a follower decision for which Constraint (7d) is satisfied. A value of \(s>0\) can be used to make Constraint (7d) satisfiable at a cost of \( s\) in the objective, guaranteeing feasibility.

Bilevel feasibility.Given a solution \(^{}\) or a solution pair \((^{},})\) returned by our upper- or lower-level approximations, respectively, we would like to produce a lower-level solution \(^{}\) such that \((^{},^{})\) is bilevel-feasible, i.e., it satisfies the original BiLO in (1). The following procedure achieves this goal:

1. Compute the follower's optimal value under \(^{}\), \((^{})\), by solving (3).
2. Compute a bilevel-feasible follower solution \(^{}\) by solving problem (2) with fixed \(^{}\) and the right-hand side of (2d) set to \((^{})\), a constant. Return \((^{},^{})\).

If only Assumption 1(i) is satisfied, then only the lower-level approximation is applicable and this procedure guarantees an optimistic bilevel-feasible solution for it. If only Assumption 1(ii) is satisfied, then this procedure can detect in Step 1 that an upper-level approximation's solution \(^{}\) does not admit a follower solution, i.e., that it is infeasible, or calculates a feasible \(^{}\) if one exists in Step 2. If both Assumptions 1(i) and 1(ii) are satisfied simultaneously, then this procedure guarantees an optimistic bilevel-feasible solution for either approximation.

Upper- v.s. lower-level level approximation.Here, we note two important trade-offs between the upper- and lower-level approximations.

* **Generality**: Example C.1 in Appendix C shows that under Assumption 1(ii), it may happen that solving the upper-level approximation problem variant (5) returns an infeasible solution while the lower-level variant (7) does not.
* **Scalability**: The upper-level approximation has fewer variables and constraints than its lower-level counterpart as it does not represent the follower's problem directly. For problems in which the lower-level problem is large, e.g., necessitating constraints for each node and link to enforce a network flow in the follower solution as in the DNDP from the introduction, this property makes the upper-level approximation easier to solve, possibly at a sacrifice in final solution quality. This tradeoff will be assessed experimentally.

Limitations.Since Neur2BiLO is in essence a learning-based _heuristic_, it does not guarantee an optimal solution to the bilevel problem. However, it guarantees a feasible solution with the lower-level approximation and can only give an infeasible solution while using the upper-level approximation when only Assumption 1(ii) is satisfied. Moreover, as will be shown in Section 3.3, the performance of Neur2BiLO depends on the regression error, which is generally the case when integrating machine learning in optimization algorithms. Empirically, we note that the prediction error achieved on every problem is very low (see Appendix K.3).

### Model architecture

For ease of notation in previous sections, all regression models take as input the upper-level decision variables. However, in our experiments, we leverage instance information as well to train _a single model_ that can be deployed on a family of instances. This is done by leveraging information such as coefficients in the objective and constraints for each problem.

For the model's architecture, the general principle deployed is to first explicitly represent or learn instance-based features. The second is to combine instance-based features with (leader) decision variable information to make predictions.

The overall architecture can be summarized as the following set of operations. Fix a particular instance of a BiLO problem and let \(n\) be the number of leader variables, \(_{i}\) a vector of features for each leader variable \(_{i}\) (independently of the variable's value), and \(h(_{i})\) a feature map that describes the \(i\)th leader variable for a specific value of that variable. The functions \(^{s},^{d}\), and \(^{v}\) are neural networks with appropriate input-output dimensions. The vector \(\) includes all learnable parameters of networks \(^{s},^{d},\) and \(^{v}\). The functions \(,,\) and Aggregate sum up a set of vectors, concatenate two vectors into a single column vector, and aggregate a set of scalar values (e.g., by another neural network or simply summing them up), respectively. Our final objective value predictions are then given by the following sequence of steps:

1. Embedding the set of variable features \(\{_{i}\}\) using a set-based architecture, e.g., the same network \(^{d}\), summing up the resulting \(n\) variable embeddings, then passing the resulting vector to network \(^{s}\), yielding a vector we refer to as the InstanceEmbedding: \[=^{s}((\{^{d}(_{i}) \}_{i=1}^{n})).\] This is akin to the DeepSets approach of Zaheer et al. . However, note that this step can alternatively be done via a feedforward or graph neural network depending on the problem structure.
2. Conditional on a specific assignment of values to the leader's decision vector \(\), a per-variable embedding is computed by network \(^{v}\) to allow for interactions between the InstanceEmbedding and the specific assignment of variable \(i\) as represented by \(h(_{i})\): \[(i)=^{v}((h(_{i}), )).\]
3. The final value prediction for either of our approximations aggregates the variable embeddings possibly after passing them through a function \(g_{i}\): \[(;)=(\{g_{i}((i))\}_{i=1}^{n}).\] For example, if the follower's objective is a linear function and VariableEmbedding\((i)\) is a scalar, then it is useful to use the variable's known objective function coefficient \(d_{i}\) here, i.e.: \(g_{i}((i))=d_{i}(i)\). The final step is to aggregate the per-variable \(g_{i}()\) outputs, e.g., by a summation for linear or separable objective functions.

Neur2BiLO is largely agnostic to the learning model utilized as long as it is MILP-representable. In our experiments, we primarily focus on neural networks, but for some problems also explore the use of gradient-boosted trees. More details on the specific architectures for each problem can be found in Appendix K.1.

### Approximation guarantees

Lower-level approximation.Next, we present an approximation guarantee for the lower-level approximation with \(^{l}(;)\). Appendix D includes the complete proofs.

Since the prediction of the neural network is only an approximation of the true optimal value of the follower's problem \(()\), Neur2BiLO may return sub-optimal solutions for the original problem (1). We derive approximation guarantees for a specific setup that appears in interdiction problems: the leader and the follower have the same objective function (i.e., \(f(,)=F(,)\) for all \(,\)), and Assumption 1(i) holds. Consider a neural network that approximates the optimal value of the follower's problem up to an absolute error of \(>0\), i.e.,

\[|^{l}(;)-()|.\] (8)

Furthermore, we define the parameter \(\) as the maximum difference \(f(,)-f(,^{}) 0\) over all \(,,^{}\) such that no \(}\) exists which has function value \(f(,)>f(,})>f(,^{})\). We can bound the approximation guarantee of the lower-level Neur2BiLO as follows:

**Theorem 3.1**.: _If the leader and the follower have the same objective function and \(>1\), Neur2BiLO returns a feasible solution \((^{},^{})\) for Problem (1) with objective value_

\[f(^{},^{})+3+,\]

_where opt is the optimal value of (1) and \(\) the penalty term in (7a)._Upper-level approximation.As Example C.1 shows, it may happen that the upper-level surrogate problem (5) returns an infeasible solution and hence no approximation guarantee can be derived in this case. However, in the case where all leader solutions are feasible and the neural network predicts for every \(\) an upper-level objective value that deviates at most \(>0\) from the true one, then the returned solution trivially approximates the true optimal value with an absolute error of at most \(2\). This follows since the worst that can happen is that the objective value of the optimal solution is overestimated by \(\) while a solution with objective value \(+2\) is underestimated by \(\) and hence has the same predicted value as the optimal solution. Problem (5) then may return the latter sub-optimal solution.

## 4 Experimental Setup

Benchmark problems and their characteristics are summarized in Table 1; their MIP formulations are deferred to Appendix E and brief descriptions follow:

* **Knapsack interdiction (KIP) :** The leader interdicts a subset of at most \(k\) items and the follower solves a knapsack problem over the remaining (non-interdicted) items. The leader aims to minimize the follower's (maximization) objective.
* **Critical node problem (CNP) [18; 11]:** This problem regards the protection (by the leader) of resources in a network against malicious follower attacks. It has applications in the protection of computer networks against cyberattacks as demonstrated by Dragotto et al. .
* **Donor-recipient problem (DRP) :** This problem relates to the donations given by certain agencies to countries in need of, e.g., healthcare projects. The leader (the donor agency) decides on which proportion of the cost, per project, to subsidize, whereas the follower (a country) decides which projects it implements.
* **Discrete network design problem (DNDP) :** This is the problem described in Section 1. We build on the work of Rey  who provided benchmark instances for the transportation network of Sioux Falls, South Dakota, and an implementation of the state-of-the-art method of Fontaine and Minner . This network and corresponding instances are representative of the state of the DNDP in the literature.

Baselines.As mentioned previously, the branch-and-cut (B&C) algorithm by Fischetti et al.  is considered to be state-of-the-art for solving mixed-integer linear BiLO. The method is applicable if the continuous variables of the leader do not appear in the follower's constraints. Both KIP and CNP meet these assumptions. This algorithm will act as the baseline for these problems. For DRP, we compare against the results produced by an algorithm in the branch-and-cut paradigm (B&C+) from Ghatkar et al. . For DNDP, the follower's problem only has continuous variables, so the baseline is a method based on KKT conditions (MKKT) . Of the learning-based approaches for BiLO, we compare against Zhou et al. , given the generality of their approach and the availability of source code. Neur2BiLO decisively outperforms this method on KIP, finding solutions with \(10\)-\(100\) smaller mean relative error roughly \(1000\) faster; full results are deferred to Appendix F.

Data collection & Training.For each problem class, data is collected by sampling feasible leader decisions \(\) and then solving \(()\) to compute either the upper- or lower-level objectives as labels. We then train regression models to minimize the least-squares error on the training samples. Typically, data collection and training take less than one hour, a negligible cost given that for larger instances

    & &  &  \\  Problem & x & Obj. & Cons. & y & Obj. & Cons. \\  KIP & \(()\) & B & Lin & Lin & B & Lin & Lin \\ CNP & \(()\) & B & Lin & Lin & B & Blin & Lin \\ DRP & \(()\) & C & Lin & Lin & MI & Lin & BLin \\ DNDP & \(()\) & B & NLin & C & NLin & Lin \\   

Table 1: Problem class characteristics. All problems have a single budget constraint in the leader; for the follower, the DNDP has network flow constraints whereas other problems have a knapsack constraint. The arrows refer to minimization (\(\)) or maximization (\(\)) in leader and follower, respectively. B = Binary, C = Continuous, MI = Mixed-Integer, Lin = Linear, BLin = Bilinear, NLin = Non-Linear.

baseline methods require more time _per instance_. Additionally, the same trained model can be used on multiple unseen test instances. We report times for data collection and training in Appendix K.2.

Evaluation & Setup.For evaluation in KIP, CNP, and DRP, all solving was limited to 1 hour. For DNDP, we consider a more limited-time regime, wherein we compare Neur2BiLO at 5 seconds against the baseline at 5, 10, and 30 seconds. For all problems, we evaluate both the lower- and upper-level approximations with neural networks, namely NN\({}^{l}\) and NN\({}^{u}\), respectively. For NN\({}^{l}\) we set \(=1\) for all results presented in the main paper. Details of the computing setup are provided in Appendix J. Our code and data are available at https://github.com/khall-research/Neur2BiLO.

## 5 Experimental Results

We summarize the results as measured by average solution times and mean relative errors (MREs). The relative error on a given instance is computed as \(100}-obj_{best}|}{|obj_{best}|}\), where \(obj_{}\) is the value of the solution found by method \(\) and \(obj_{best}\) is the best-known objective value for that instance. These results are reported in Table 2. More detailed results and box-plots of the distributions of relative errors are in Appendices G and H. Our experimental design answers the following questions:

**Q1: Can Neur2BiLO find high-quality solutions quickly on classical interdiction problems?** Table 2 compares Neur2BiLO to the B&C algorithm of Fischetti et al. . Neur2BiLO terminates in 1-2% of the time required by B&C on the smaller (\(n 30\)) well-studied KIP instances of Tang et al. . However, when the instance size increases to \(n=100\), both NN\({}^{l}\) and NN\({}^{u}\) find much better solutions than Neur2BiLO in roughly 30 seconds, even when B&C runs for the full hour. Furthermore, Table 4 in Appendix G shows that B&C requires \(10\) to \(1,000\) more time than NN\({}^{l}\) or NN\({}^{u}\) to find equally good solutions. In addition, the best solutions found by B&C at the termination times of NN\({}^{l}\) or NN\({}^{u}\) are generally worse, even for small instances.

**Q2: Do these computational results extend to non-linear and more challenging BiLO problems?** Interdiction problems such as the KIP are well-studied but are only a small subset of BiLO. We will shift attention to the more practical problems, starting with the CNP (Table 2). CNP includes terms that are bilinear (i.e., \(z=xy\)) in the upper- and lower-level variables, resulting in a much more challenging problem for general-purpose B&C. In this case, both NN\({}^{l}\) and NN\({}^{u}\) tend to outperform B&C as the problem size increases. In addition, the results on incumbents reported in Table 5 in Appendix G are as good, if not even stronger than those of KIP.

Secondly, we discuss DRP (Table 6 in Appendix G). For DRP, we evaluate on the most challenging instances from Ghahtarani et al. , all of which have gaps of \( 50\%\) at a 1-hour time limit with B&C+, a specialized B&C-based algorithm. Here NN\({}^{u}\) performs remarkably well: it finds the best-known solutions on every single instance in roughly \( 0.1\) seconds at an average improvement in solution quality of 26% over B&C+.

**Q3: How does Neur2BiLO perform on BiLO problems with complex constraints?** Given that Neur2BiLO has strong performance on benchmarks with budget constraints, the next obvious question is whether it can be applied to BiLO problems that have complex constraints. To answer this, we will refer to the results in Table 2 for the DNDP. In this setting, we focus on a limited-time regime wherein we compare Neur2BiLO with a 5-second time limit to MKKT at time limits 5, 10, and 30 seconds. NN\({}^{u}\) can achieve high-quality solutions much faster than any other method with only a minor sacrifice in solution quality, making it a great candidate for domains where interactive decision-making is needed (e.g., what-if analysis of various candidate roads, budgets, etc.).

NN\({}^{l}\), on the other hand, takes longer than NN\({}^{u}\) but computes solutions that are more competitive with the baseline, the latter requiring \(5\) more time. We suspect that the better solution quality from NN\({}^{l}\) is due to its explicit modeling of feasible lower-level decisions that "align" with the predictions, whereas NN\({}^{u}\) may simply extrapolate poorly. In terms of computing time, one computational burden for NN\({}^{l}\) is the requirement to model the non-linear upper- and lower-level objectives, which requires a piece-wise linear approximation based on Fontaine and Minner , a step that introduces additional variables and constraints. Appendix G includes results for DNDP with gradient-boosted trees (GBT), demonstrating that other learning models are directly applicable and, in some cases, may even lead to better solution quality, faster optimization, and simpler implementation.

[MISSING_PAGE_FAIL:9]

Table 10 in Appendix I.3). This demonstrates that there is value in leveraging any problem-specific MILP-representable heuristics as features for learning.

Q5: How does \(\) affect NN'?Table 9 in Appendix I.2 shows that a slack penalty of \(=0.1\) improves the performance of NN\({}^{l}\) on some instances for DNDP, compared to the \(=1\) reported in Table 2, indicating that tuning over \(\) might be beneficial. As an alternative to adding slack, one can even dampen predictions of the value function to allow more flexibility using the empirical error observed during training; see Table 8 in Appendix I.1.

## 6 Related Work

Learning for bilevel optimization.Besides the approaches of Sinha et al. [50; 51; 52] and Beykal et al.  discussed in Section 2, other learning-based methods have been introduced to solve BiLO problems. Bagloee et al.  present a heuristic for DNDP which uses a linear prediction of the leader's objective function. An iterative algorithm refines the prediction with new solutions, terminating after a pre-determined number of iterations. Chan et al.  propose to simultaneously optimize the parameters of a learning model for a subset of followers in a large-scale cycling network design problem. Here, only non-parametric or linear models are utilized as optimizing more sophisticated learning models is generally challenging with MILP-based optimization. Molan and Schmidt  make use of a neural network to predict the follower variables. The authors assume a setting with a black-box follower's problem, no coupling constraints, and continuous leader variables. Another learning-based heuristic is proposed by Kwon et al.  for a bilevel knapsack problem. This approach is knapsack-specific and requires a sophisticated, GPU-based, problem-specific graph neural network for which no code is publicly available. Zhou et al.  propose a learning-based algorithm for binary bilevel problems which, similar to our approach, predicts the optimal value function and develops a single-level reformulation based on the trained model. They propose using a graph neural network and an input-supermodular neural network, both of which can only be trained on a single instance rather than learning across classes of instances as Neur2BiLO does. Neur2BiLO significantly outperforms this method as shown in Appendix F. For continuous unconstrained bilevel optimization, a substantially different setting, many methods have been proposed recently due to interest in solving nested problems in machine learning (e.g., hyperparameter tuning and meta-learning) .

Data-driven optimization.The integration of a trained machine learning model into a MIP is a vital element of Neur2BiLO. This is possible due to MILP formulations of neural networks [14; 22; 49], and of other predictors like decision trees [38; 8]. These methods have become easily applicable due to open software implementations [7; 12; 40; 55] and the gurobi-machinelearning library. One such application is constraint learning . More similar to our setting are the approaches in [19; 20; 34] for predicting value functions of other nested problems such as two-stage stochastic and robust optimization. Our method caters to the specificities of BiLO, particularly in the lower-level approximation which performs well in highly-constrained BiLO settings such as the DNDP, has approximation guarantees based on the error of the predictive model, and computational results on problems with non-linear interactions between the variables in each stage of the optimization problem; these aspects distinguish Neur2BiLO from prior work.

## 7 Conclusion

In both its upper- and lower-level instantiations, Neur2BiLO finds high-quality solutions in a few milliseconds or seconds across four benchmarks that span applications in interdiction, network security, healthcare, and transportation planning. In fact, we are not aware of any bilevel optimization method which has been evaluated across such a diverse range of problems as existing methods make stricter assumptions that limit their applicability. Neur2BiLO models are generic, easy to train, and accommodating of problem-specific heuristics as features. One limitation of our experiments is that they lack a problem that involves coupling constraints in (1b). We could not identify benchmark problems with this property in the literature, but exploring this setting would be valuable. Of future interest are potential extensions to bilevel _stochastic_ optimization , robust optimization with decision-dependent uncertainty  (a special case of BiLO), and multi-level problems beyond two levels, e.g. .