# Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries

Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We explore estimating the uncertainty of closed-source LLMs via multiple rephrasings of an original base query. Specifically, we ask the model, multiple rephrased questions, and use the similarity of the answers as an estimate of uncertainty. We diverge from previous work in i) providing rules for rephrasing that are simple to memorize and use in practice ii) proposing a theoretical framework for why multiple rephrased queries obtain calibrated uncertainty estimates. Our method demonstrates significant improvements in the calibration of uncertainty estimates compared to the baseline and provides intuition as to how query strategies should be designed for optimal test calibration.

## 1 Introduction

Closed-source LLMs are prone to generating highly convincing but false information, a problem known as "hallucinating" (Huang et al., 2023; Ji et al., 2023). It is folk wisdom that one approach for estimating LLM uncertainty, even with such limited access to the model, is to query it multiple times (Wang et al., 2022; Xiong et al., 2023). This approach is based on the premise that LLM-generated text is frequently stochastic by design, as the next generated token is chosen through nucleus sampling (Holtzman et al., 2019) or top-k decoding (Fan et al., 2018; Radford et al., 2019). Wang et al. (2022) and Xiong et al. (2023) proposed to use the consistency of multiple answers as an estimate of uncertainty. Xiong et al. (2023) furthermore proposed to add "noise" to the base query at each repetition, through misleading hints.

In this work, we delve deeply in, refine, and theoretically analyze multiple queries for uncertainty estimation. Given a base query, we restrict ourselves to submitting rephrased versions of the base

Figure 1: **Multiple rephrased queries for uncertainty estimation.** Querying a closed-source LLM only once with a base query may yield an incorrect top-1 prediction with \(100\%\) confidence to this singular prediction. Querying the model multiple times with rephrased versions of the base query produces different answers equivalent to \(66.6\%\) confidence.

query to an LLM, checking the consistency of the answers, and using the result as an estimate of uncertainty. Concretely our contributions are the following:

* We test four simple strategies for creating multiple rephrased queries, and find that they result in significant calibration gains over baselines.
* We propose a theoretical model for multiple rephrased queries on a simplified top-1 and top-k (Holtzman et al., 2019) decoding setting. Given multiple rephrased queries, our analysis shows that i) it is possible to recover the probability of the answer under the inaccessible categorical distribution of the LLM ii) top-k decoding then simply tempers our uncertainty. Crucially we show empirically that our uncertainty estimates are close to what could be obtained when having access to the last layer logits.

## 2 Rephrasing drastically improves calibration for top-1 decoding

Let \(f:\) be an LLM which takes \(\) an input query in the form of a multiple choice question, and outputs \(y\), an answer. _We first consider top-1 decoding such that the answers of the LLM are deterministic._ We consider randomized transformations of the base query \(()\) in the form of rephrasings of the query, and the most probable answer under the transformations \(A=*{argmax}_{i}(f(())=i)\). In a multiple choice question setting (which can be seen as a multi-class classification problem), we will use \(A\) as the predicted class and

\[p_{A}()=(f(())=A),\]

as our confidence about this prediction (here the predicted class coincides with a predicted token denoting this class). We consider four types of rephrasings, with an increasing level of modification to the original query: (1) reword: replacing words with synonyms; (2) rephrase: modifies the structure of the original query; (3) paraphrase: reconstructs the original query; (4) expansion: elaborate the query. In general, we perform the rephrasings with a separate instance of the same model that responds to the queries. We estimate \(p_{A}()\) using Monte Carlo sampling with 10 draws from \(()\) to estimate uncertainty with our method unless stated otherwise.

We used three different models, the Llama-2 7B model, the Llama-2 13B model (Touvron et al., 2023) and the Mistral 7B model (Jiang et al., 2023). We tested our framework on three multiple choice tasks: ARC-Challenge, ARC-Easy (Clark et al., 2018), and Openbookqa (Mihaylov et al., 2018). Following Kojima et al. (2022), we extract the answer from LLM-generated texts by looking at the first appearance of A/B/C/D. To test for calibration we used standard calibration metrics, including the ECE and TACE (Naeini et al., 2015), Brier score (Murphy, 1973) and AUROC (Murphy, 2012).

We plot the AUROC results of all methods averaged over all models for each dataset in Figure 2. We see that the best rephrasing method outperforms top-1 (naive) and top-k decoding as well as the hint based rephrasing approach. In Appendices E and D we show that we also match or outperform Chain of Thought (CoT) prompting and Temperature Sampling Wei et al. (2022).

## 3 Rephrasing works as well as having access to the last layer logits

We now derive a proposition that elucidates why \(p_{A}()\) results in calibrated estimates of uncertainty.

Figure 2: We plot the AUROC averaged over all models for each dataset and for each uncertainty estimation method. We observe that top-k improves over the naive top-1 decoding. Furthermore, the best rephrasing method (denoted as rephrase*) improves the AUROC significantly in all cases.

**Proposition 3.1**.: _Let \(f:\) be an LLM, \(\) is a base query and \(()\) is some randomized transformation of the base query. Let_

\[p_{A}()=(f(())=A),\] (1)

_be the probability of sampling the most probable answer \(A\) under transformations \(()\). Let \(_{mean}+_{rephrase}\) be the latent representation of \(\) under \(()\) at the final LLM layer, where \(_{mean}\) is the mean representation and \(_{rephrase}\) is some additive noise. Let \(\) be the separating hyperplane between the most probable answer \(A\) and the second most probable answer \(B\). Assuming that \(^{}_{rephrase}\) follows a logistic distribution with \(=0\) and \(s=1\) then_

\[p_{A}()=p(A|_{mean},f)\] (2)

_where \(p(A|_{mean},f)\) is the probability of \(A\) given \(_{mean}\) under the categorical distribution of the final layer._

We prove the above for the binary case of two classes \(A\) and \(B\) in Appendix C, but expect that it should be sufficiently informative in multi-class settings when \(A,B\) are much more probable than other classes. A crucial assumption for recovering well-calibrated predictions is that \(^{}_{rephrase}\) follows a logistic distribution with \(=0\) and \(s=1\). We test this assumption by computing the cumulative of \(\) for our different experimental setups. In Figure 2(c) we find and plot the empirical cumulative using a Kolmogorov-Smirnov test (Smirnov, 1948) and \(S=100\) MC samples of \(\) for Mistral-7B, ARC-Challenge, and the "expansion" rephrasing method. We see that the indeed the cumulative is approximately logistical validating our prediction (the confidence bands cover different queries \(\)). In Table 1 we use the logits of the answers as an oracle white-box uncertainty estimate. Specifically, we apply the softmax function and use the probability of the most probable class as our estimate of uncertainty. We compare the results of this method with the best rephrasing method (in terms of Brier). We observe that our uncertainty estimates that are similar to what we would get if we had access to the last layer logits.

## 4 For top-k decoding, rephrasing tempers predictive uncertainty

In practice, the assumptions of the above proposition are too restrictive. In particular, decoding in LLMs is performed with top-k decoding or nucleus sampling instead of top-1 decoding. Furthermore

   Dataset & Model & Method & Acc \(\) & ECE \(\) & TACE \(\) & Brier \(\) & AUROC \(\) \\   &  & logits & 0.742 & 0.252 & 0.075 & 0.503 & 0.741 \\  & & expansion & 0.602 & 0.133 & 0.099 & 0.509 & 0.847 \\   & &  & logits & 0.483 & 0.362 & 0.168 & 0.853 & 0.621 \\  & & expansion & 0.373 & 0.112 & 0.153 & 0.778 & 0.687 \\   & & logits & 0.508 & 0.132 & 0.141 & 0.704 & 0.669 \\  & & reward & 0.445 & 0.084 & 0.119 & 0.714 & 0.721 \\   &  & logits & 0.866 & 0.128 & 0.037 & 0.264 & 0.818 \\  & & reward & 0.753 & 0.045 & 0.062 & 0.297 & 0.931 \\   & & logits & 0.672 & 0.190 & 0.098 & 0.493 & 0.779 \\   & & rephrase & 0.535 & 0.131 & 0.117 & 0.603 & 0.830 \\   & & logits & 0.617 & 0.060 & 0.094 & 0.498 & 0.763 \\   & & expansion & 0.524 & 0.078 & 0.12 & 0.552 & 0.893 \\   &  & logits & 0.655 & 0.298 & 0.085 & 0.602 & 0.705 \\  & & reward & 0.552 & 0.105 & 0.102 & 0.592 & 0.796 \\   & & logits & 0.478 & 0.277 & 0.147 & 0.758 & 0.642 \\   & & expansion & 0.362 & 0.083 & 0.138 & 0.775 & 0.678 \\    & & logits & 0.418 & 0.168 & 0.135 & 0.723 & 0.650 \\   & & rephrase & 0.428 & 0.095 & 0.14 & 0.729 & 0.73 \\   

Table 1: Comparisons between our rephrasing methods and white-box logit uncertainty estimation. We see that our rephrasing methods achieve similar calibration to what would be achieved if we had access to last layer logits. This is evident both in the AUROC and TACE as well as the Brier score, which also accounts for accuracy.

while for an oracle choice of the rephrasing intensity the modeling assumption that \(^{}_{}\) follows a logistic distribution with \(=0\) and \(s=1\) might be correct, in general, the variance of the noise in latent space is unknown. It is thus illustrative to consider an extension of our toy model. The following proposition explores these extensions.

**Proposition 4.1**.: _Let \(g:^{d_{}}\) be the final encoder layer of an LLM, \(\) is a base query and \(()\) is some randomized transformation of the base query. Let_

\[p_{A}()=(f(())=A),\] (3)

_be the probability of sampling the most probable answer \(f()=A\) under transformations \(()\). Let \(_{mean}+_{topk}+_{rephrase}\) be the latent representation of \(\) under \(()\) at the final LLM layer, where \(_{mean}\) is the mean representation and \(_{topk}\) is additive noise resulting from the top-k decoding and \(_{rephrase}\) is additive noise resulting from the rephrasings \(()\). Assuming that \(^{}(_{topk}+_{rephrase})\) approximately follows a logistic distribution with \(=0\) and \(s=^{2}+s_{rephrase}^{2}}\) then_

\[p_{A}() 0.5+^{2}+s_{rephrase}^{2}}}(p(A| _{mean},f)-0.5)\] (4)

_where \(p(A|_{mean},f)\) is the probability of \(A\) given \(_{mean}\) under the categorical distribution of \(g\)._

The approximation relies on linearizing the involved functions, however, it is illustrative of the effect of both \(s_{topk}^{2}\) and \(s_{rephrase}^{2}\). In particular, we see that both \(s_{topk}^{2}\) and \(s_{rephrase}^{2}\) act to _temper_ the probability \(p(A|_{mean},f)\) under the categorical distribution of g. This highlights why using rephrasings with an appropriate temperature might improve the calibration in downstream tasks. In previous works, tempering of the categorical distribution has been found to significantly improve the calibration of deep neural networks (Guo et al., 2017).

Figure 3 shows that in accordance with proposition 4.1 rephrasing acts primarily to temper the probability of the top class. In out detailed results in Appendix E, this often improves calibration significantly in terms of ECE, and AUROC especially for smaller models.

## 5 Discussion

We conducted a thorough analysis of rephrased queries as a method for obtaining calibrated predictions from closed-source LLM models. Notably, we found that two simple methods; making the query more verbose, and substituting words with their synonyms, provide a straightforward means of identifying false positives. The appeal of our approach lies in its practicality, as it requires only basic language and arithmetic skills by the end user to obtain meaningful uncertainty estimates. Exciting future directions include learning optimal rephrasing rules in a data-driven manner, to be used in conjunction with a rephrasing LLM. While we tested on the multiple choice question setting for ease of evaluation, we expect our results to also hold for open-ended text generation.

Figure 3: We plot the distribution of \(p_{A}()\) for the case of top-k decoding with and without rephrasing, for all datasets, models, and rephrasing methods. We see that rephrasing primarily acts to temper the probability of the most probable class \(A\), thus making the model less confident and possibly better calibrated. We also plot the logistic (blue), and empirical cdf (red) for \(^{}_{rephrase}\) for Mistral-7B, ARC-Challenge, and the “expansion” rephrasing method for top-1 decoding. \(\) is often close to a logistic distribution.