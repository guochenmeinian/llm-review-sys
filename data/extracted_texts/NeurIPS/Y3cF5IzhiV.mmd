# TurtleBench: A Visual Programming Benchmark

in Turtle Geometry

 Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi

University of California, Irvine

{srismac,yrazeghi,sameer,doroudis}@uci.edu

###### Abstract

While formal geometric reasoning may be difficult for humans without extensive training, humans seem to have the ability to intuitively reason about geometric patterns in images and scenes from a young age. In contrast, developing large multimodal models (LMMs) capable of similar feats represents a frontier in AI research. We introduce TurtleBench, a benchmark designed to evaluate LMMs' capacity to interpret geometric patterns--given visual examples, textual instructions, or both--and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Unlike object detection tasks that typically do not involve understanding underlying patterns, this benchmark combines geometrical reasoning with image understanding. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4V achieving only 19% accuracy on the simplest tasks. TurtleBench highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area.

## 1 Introduction

Geometric reasoning is a hallmark of human mathematical reasoning that has been studied since the Ancient Greeks. It was a task that attracted early artificial intelligence (AI) researchers and early efforts on building intelligent tutoring systems also focused on geometry. Yet much of the emphasis on geometric reasoning is on axiomatic-deductive geometry. Humans of all ages are naturally good at more intuitive kinds of geometric reasoning that inform how we see and navigate the world. One aspect of this is our ability to look at a geometric shape or complex pattern and construct an algorithm to generate that pattern. We believe this is a powerful task to evaluate large multimodal models (LMMs) for a number of reasons. First of all, constructing patterns in this way reflects an early programming paradigm for teaching kids programming, initially developed in the 1970s with the introduction of the Logo programming language (Papert, 1972, 1980). For several decades, children from a young age have been learning how to procedurally draw geometric patterns and other drawings using code in programming languages like Logo, Scratch, and Python--often as their first introduction to programming. Given LMMs' success in a variety of complex programming tasks, one might expect a programming task that children could solve to be easy. Second, recent research suggests that this ability to procedurally generate shapes may be more fundamental to our psychology than meets the eye. Spelke (2022) claims that from infancy (or even birth), humans have a set of six core knowledge systems, two of which contribute to our understanding of geometry: a _form_ system and a _place_ system. While the form system allows us to perceive the boundaries of objects, our core knowledge of places interprets geometry in terms of how to navigate an environment (Dillon, 2023). Taking this a step further, Sable-Meyer et al. (2022) suggest that humans perceive shapes and patterns in terms of procedural programs that could generate them; they demonstrate that the time it takesfor people to process these shapes correlates with the minimum description length of the shape in a Logo-like programming language.

In this work, we introduce TurtleBench, a set of manually crafted image/text to code tasks in turtle geometry (Papert, 1972; Abelson & diSessa, 1986) to evaluate the abilities of these models to combine visual pattern recognition, abstract geometrical reasoning, and Python programming. To ensure the visual inputs and the programming language remain straightforward, TurtleBench harnesses turtle geometry, a concept widely recognized for its effectiveness in introducing programming concepts to children within the K-12 education system. Although turtle programming is now used more as a tool to foster computational thinking, turtle geometry has also been explored as a powerful way of teaching geometry and mathematical reasoning to children (Hoyles & Noss, 1992; Clements & Sarama, 1997). In turtle geometry, a turtle acts as a programmable object that navigates the screen, drawing as it goes and turning at specified angles, to create simple visual patterns. The primary objective within this framework is to generate code capable of producing simple visual inputs. These visual inputs consist of basic geometric shapes, and the programming syntax required is intentionally limited and straightforward. An example of such a task is presented in the left side of Figure 1. As illustrated, the input image is the shape of a simple square and the corresponding code only uses two simple turtle functions (forward and right) along with a simple for loop. This simplicity makes TurtleBench an effective benchmark for evaluating the capabilities of LMMs.

To reflect different real-world use cases of an LMM in the domain of Turtle and also cover the broad range of underlying reasoning abilities, TurtleBench includes 260 tasks with a variety of types and modalities. We conduct an evaluation of leading LMMs on TurtleBench code generation and code editing tasks, utilizing zero-shot and visual chain-of-thought (Singh et al., 2023) approaches across text-only, image-only, and mixed (text and image) input modalities. Our findings reveal that these models generally perform poorly across all setups and variety of tasks and modalities. Our best-performing model, GPT-4V, outperforms Gemini 1.5 Flash yet neither model comes close to solving TurtleBench tasks, as about 75% of the tasks were left completely unsolved. Intriguingly, our results indicate that performance improves when tasks are presented in text, rather than inputting images. This suggests that integrating visual and linguistic information, particularly in domains requiring visual pattern recognition, may need further refinement. All these findings demonstrate that our benchmark poses a challenging task for LMMs, providing valuable insights into their capabilities.

## 2 Overview of TurtleBench

TurtleBench is a set of 260 tasks that are designed to evaluate LMMs' performance on vision and language algorithmic reasoning tasks. To ensure the novelty of the tasks and their quality in incorporating authentic geometric shapes and concepts, we craft TurtleBench manually. All the tasks

Figure 1: An illustration of existing types and modes in TurtleBench, A task may have a type of Scratch or Tweak, in a mode of code generation or code edit, with various modalities in the input.

in TurtleBench are accurately solvable based on the provided information for each, which means that there are no ambiguities or arbitrary parameters leading to inaccuracies in the tasks for humans as well as the models. To remove possible ambiguities in the tasks, two independent annotators worked with us to identify and resolve any unclear instructions. Each task consists of a black-and-white image illustrating a set of abstract geometric shapes as an _input_. An example of this task is presented in Figure 1. TurtleBench is made up of two different types of tasks, these types reflect the methodologies used in turtle geometry to introduce programming to children.

_Scratch_ tasks are intended to show how well a model understands a pattern and translates its understanding to an executable code. In the general case of this type of task, an image is provided, and the requested output is code in Python Turtle that creates the shapes in the image. In all scratch tasks, the model is asked to _generate_ the code in Python Turtle for the desired input shape. TurtleBench includes a total of \(130\) scratch tasks. An example of these tasks is provided in Figure 1, top rows. To distinguish between the models' visual comprehension and their textual understanding, a subset (31%) of these tasks includes a text description of the image input in addition to the visual representation. This setup facilitates the evaluation of how models respond differently to visual and textual inputs, providing a clearer understanding of their capabilities.

_Tweak_ tasks are intended to measure how well a model uses their understanding of a visual pattern, combined with an instruction to make minimal alterations. Each tweak task presents a model with an image and an instruction; the expected output is Python Turtle code that modifies the shape in the input image according to the given instruction. These tasks are particularly insightful for determining whether a model is merely recalling memorized code for an image, or if it has developed a deeper, more human-like comprehension of the patterns depicted in the images. For instance, a model might be capable of generating code for a certain shape based on training data, but the real challenge lies in its ability to adapt that shape in response to various instructed changes. An example of these tasks is provided in Figure 1, bottom row. Here, the model is given an input image of a rectangle, with an instruction to _connect the midpoint of each side to the midpoint of adjacent sides_. As illustrated in Figure 1, we also introduce a code editing version of the tweak task. In this version, we supply the code corresponding to the input image and then instruct the models to make specific modifications to this code, aiming to achieve a change in the image as per the provided instructions. Detailed information about types of tweaks and their examples is provided in Appendix C.4.

## 3 Evaluation Setup

In the following section, we evaluate TurtleBench using two state-of-the-art LMMs, GPT-4V and Gemini 1.5 Flash and also an open source model, namely Llava-1.5-13BLiu et al. (2023) employing greedy decoding in our evaluations. We evaluated two other open models, namely Qwen-VL-Max (Bai et al., 2023) and CogVLM (Wang et al., 2023) on a subset of tasks in TurtleBench. However, CogVLM and Qwen are not successful in producing a syntactically correct Python Turtle piece of code even for the simplest tasks, therefore we limited our benchmark evaluation to the models mentioned above.

We utilize two types of prompting in our experiments, 1) basic, where we simply prompt the the model (c.f. Appendix C.2) to do our tasks, and 2) Chain-of-Thought (CoT) prompting (Wei et al., 2022), which has shown to be an effective prompting technique in eliciting reasoning in these models. Specifically, we use a more detailed version of CoT prompting that is tailored to LMMs, namely v-CoT, recently proposed by Singh et al. (2023). The v-CoT approach is inspired by m-CoT (Zhang et al., 2023), which shows higher performance compared to it. This prompting has been shown to improve LMMs' performance on visual tasks that involved reasoning, such as ARC (Chollet, 2019). This prompt, instructs the model to first extract all the relevant information in the image needed for answering the problem and then to reason step by step based on the information extracted. The specific prompt we used in our experiments is in Appendix C.2

## 4 Results

### Models perform poorly on TurtleBench

We initially examine the performance of the GPT-4V, Gemini 1.5 Flash and Llava-1.5-13B models on the comprehensive TurtleBench dataset. The findings, detailed in Table 1, reveal a notably poor performance across the tasks in TurtleBench, with a peak accuracy of 20% achieved by GPT-4V in the _code editing_ tasks, facilitated by Chain of Thought (CoT) prompting. In the _scratch_ tasks, which represent the simplest problem type within the dataset, GPT-4V's success rate was just 19%, underscoring the substantial challenges and complexities these tasks pose to the current models. A comparison between CoT and basic prompting within Table 1 illustrates that CoT prompting outperforms basic prompting on the same models, aligning with previous work that indicates CoT enhances models' reasoning abilities (Zhang et al., 2023). However, despite utilizing CoT prompting, the task remains far from being solved. Additionally, we note a decline in the performance of models when comparing tasks that involve tweaks to those starting from scratch. This observation suggests that models fail to generalize their understanding to tweak tasks, even if they can successfully complete tasks from scratch. Examples of model output in different subsets of the task are provided in Figures 8 and 10.

### Limited Visual Understanding in LMMs: Insights from Textual vs. Visual Tweak Tasks

For tweak tasks, where the AI had to edit existing code, we gave instructions either in natural language or as images (see Figure 1, bottom rows, left two columns). As can be seen by comparing the bottom two rows in Table 1, there is a huge decline in accuracy when instructions were provided visually rather than textually, especially for Gemini. This outcome suggests a disparity in the models' ability to process visual versus textual instructions, revealing that their reasoning abilities may not align closely with human-like understanding. The assumption that directly viewing the desired outcome simplifies the task contrasts sharply with our findings, highlighting a reliance on textual interpretation for reasoning and a notable limitation in pure visual reasoning capabilities within these models. In Appendix B.3, we provide further evidence of this with additional analyses on scratch tasks by varying the input to those tasks (i.e., visual or textual descriptions).

## 5 Conclusions

This study introduces TurtleBench, the first of its kind in benchmarks that focus on converting visual inputs to code outputs. The evaluation results from TurtleBench reveal a significant disparity between human capabilities and current state-of-the-art AI models in understanding simple geometric shapes, reasoning about these shapes, and converting such understandings into executable code. This gap underscores the challenges that lie ahead in the quest to enhance AI's comprehension and problem-solving abilities to match human levels. We believe that TurtleBench serves as a crucial tool in the evaluation of models, offering a clear benchmark that tests the limits of large multimodal models.