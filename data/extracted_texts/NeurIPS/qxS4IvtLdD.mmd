# Fast Samplers for Inverse Problems

in Iterative Refinement Models

Kushagra Pandey

Department of Computer Science

University of California Irvine

pandeyk1@uci.edu

&Ruihan Yang

Department of Computer Science

University of California Irvine

ruihan.yang@uci.edu

&Stephan Mandt

Department of Computer Science

University of California Irvine

mandt@uci.edu

Equal contribution

###### Abstract

Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving _inverse problems_, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only _pre-trained_ diffusion or flow-matching models. We present _Conditional Conjugate Integrators_, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4\(\) super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as 5 conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm.

## 1 Introduction

Iterative refinement models, such as diffusion generative models and flow matching methods (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020; Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023), have seen increasing popularity in recent months, and much effort has been invested in accelerating unconditional sampling in these models (Pandey et al., 2024; Shaul et al., 2024; Sauer et al., 2024; Karras et al., 2022; Salimans and Ho, 2022; Zhang and Chen, 2023; Lu et al., 2022; Song et al., 2021). However, while most efficient samplers have been designed in the _unconditional_ setup, current methods for solving _inverse_ problems, such as deblurring, inpainting, or super-resolution, still require hundreds to thousands of neural network evaluations to achieve the highest perceptual quality. Moreover, in addition to a score function evaluation, a class of existing methods for solving inverse problems using pre-trained unconditional iterative refinement models often involves expensive Jacobian-vector products (Song et al., 2022; Chung et al., 2022), making a single sampling step quite expensive and therefore, intolerably slow for most practical applications.

This paper presents a principled framework for designing efficient samplers for guided sampling in iterative refinement models, accelerating existing samplers like IIGDM by an order of magnitude. We present our framework for inverse problems where the degradation operator is known and might be corrupted with additional noise. Crucially, our transformations do not require any re-training and merely rely on some algebraic manipulations of the equations to be simulated.

Intuitively, we expand on the concept of Conjugate Integrators (Pandey et al., 2024) by projecting the conditional generation process in inverse problems to another space that might be better conditioned for faster sampling (See Figure 1). To this end, we separate the linear and non-linear components in the generation process and parameterize the transformation by analytically solving the linear coefficients. By the end of the sampling procedure, we map back to the original sampling space, leading to the concept of _Conditional Conjugate Integrators_ that apply to various iterative refinement models such as diffusion models, flows, and interpolants.

In more detail, our main contributions are as follows.

* **Conditional Conjugate Integrators:** We repurpose the recently proposed Conjugate Integrator framework (Pandey et al., 2024) for fast guided sampling in iterative refinement models (diffusions and flows) for linear inverse problems and refer to it as _Conditional Conjugate Integrators_. Next, we design a specific parameterization of the proposed framework, which encodes the structure of the linear inverse problem in the sampler design itself.
* **Theoretical Analysis:** Our parameterization exhibits theoretical properties that help us identify key parameters for sampler design. More specifically, we show that our parameterization (by design) enables recovering high-frequency details early on during sampling. This further enables fast-guided sampling while maintaining good sample quality in the context of inverse problems.
* **Empirical Results**. Empirically, we show that our proposed sampler significantly improves over baselines in terms of sampling efficiency on challenging benchmarks across inverse problems like super-resolution, inpainting, and Gaussian deblurring. For instance, on a challenging 4x superresolution task on the ImageNet dataset, _our proposed sampler achieves better sample quality at 5 steps, compared to 20-1000 steps required by competing baselines_.

Figure 1: Illustration of Conditional Conjugate Integrators for Fast Sampling in Inverse Problems. Given an initial sampling latent \(_{t_{s}}\) at time \(t_{s}\), our sampler projects the diffusion/flow dynamics to a more amenable space for sampling using a projector operator \(\) which is conditioned on the degradation operator \(\) and the sampling guidance scale \(w\). The diffusion/flow sampling is then performed in the projected space. Post completion, the generated sample in the projected space is transformed back into the original space using the inverse of the projection operator, yielding the final generated sample. We define the form of the operator \(\) in Section 2.2. Conditional Conjugate Integrators can significantly speed up sampling in challenging inverse problems and can generate high-quality samples in as few as 5 NFEs as compared to existing baselines, which require from 20-1000 NFEs (see Section 3).

Additionally, we extend the proposed framework for noisy and non-linear inverse problems with qualitative demonstrations.

## 2 Fast Samplers for Inverse Problems using Diffusions/Flows.

### Background and Problem Statement

Diffusion models define a continuous-time _forward process_ (usually with an affine drift) to convert data \(_{0}^{d}\) into noise. A learnable _reverse_ process is trained to generate data from noise. In this work, we only consider deterministic reverse processes specified as an ODE (Song et al., 2020),

\[d_{t}=[_{t}_{t}-_{t}_{t}^ {}_{_{t}} p_{t}(_{t})]\,dt.\] (1)

The score is usually intractable and is approximated using a parametric estimator \(_{}(_{t},t)\), trained using denoising score matching (Vincent, 2011; Song and Ermon, 2019; Song et al., 2020). Analogously, one-sided stochastic interpolants (Albergo and Vanden-Eijnden, 2023) define an _interpolant2_\(_{t}=_{t}_{1}+_{t}\), where \(_{1} p_{}\),and \((0,)\) to define a transport map between the generative prior (typically an isotropic Gaussian) and the data distribution. Interestingly, the one-sided interpolant induces a vector field \((_{t},t)=[_{t}_{1}+_{t}|_{t}]\), where \(_{t}\), \(_{t}\) represent the time derivatives of \(_{t}\) and \(_{t}\), respectively. The vector field \((.)\) is typically learned using a neural network approximation \(_{}(_{t},t)\). The deterministic interpolant process can then be specified as \(d_{t}=_{}(_{t},t)\,dt\). Numerically solving these deterministic generative processes with a sufficient sampling budget can generate plausible samples from noise.

**Problem Statement.** Given a _noisy linear degradation process_ (we will consider non-linear processes later) with a degradation operator \(\) specified over an _unobserved_ data point \(_{0}\),

\[=_{0}+_{y}, (0,),\,\,_{0} p_{},\] (2)

the goal is to recover the original signal \(_{0}\). Additionally, given an unconditional pre-trained diffusion or flow matching model, one approach for solving inverse problems is to infer the posterior distribution over the data given the degraded observation, i.e., \(p(_{0}|) p(|_{0})p(_{0})\) by simulating the conditional reverse process dynamics i.e.

\[ d_{t}=_{t}_{t}-_{t} _{t}^{}_{_{t}} p(_{t}|)dt,\] (3) \[ d_{t}=(_{t},,t)dt,\]

where \(_{_{t}} p(_{t}|)\) and \((_{t},,t)\) are the conditional score and velocity estimates, respectively. One approach could be to directly model the conditional score or velocity estimates using a conditional iterative refinement model (Saharia et al., 2022, 2022). However, such approaches are problem-dependent, requiring expensive training pipelines to account for the lack of generalization across inverse problems. Additionally, such methods rely on the availability of paired \((_{t},)\) measurements, which can be expensive to acquire. Alternatively, _problem-agnostic_ methods leverage pre-trained unconditional iterative refinement models to estimate the conditional score or velocity fields and can generalize to different inverse problems without extra training. In this work, we restrict our discussion to the latter and discuss estimating conditional score/velocity fields next.

**Estimating Conditional Score/Velocity from Pretrained Models:** For diffusion models, approximating the conditional score follows directly from Bayes Rule, i.e. \(_{_{t}} p(_{t}|)_{}( _{t},t)+w_{t}_{_{t}} p(|_{t})\) where \(w_{t}\) is the _guidance weight_ (or temperature) of the distribution \(p(|_{t})\). Analogously for interpolants (or flows), Pokle et al. (2024) propose the conditional flow dynamics,

\[(_{t},,t)_{}(_{t},t)+w_ {t}}{_{t}}_{t}_{t}-_ {t}_{t}_{_{t}} p(|_{t}).\] (4)

We include a formal proof for the result in Eqn. 4 from an interpolant perspective in Appendix A.1. Since the conditional score and velocity estimates require approximating the term \(_{_{t}} p(|_{t})\), we discuss its estimation next.

**Estimation of the Noise Conditional Score \(_{_{t}} p(|_{t})\):** The noise conditional distribution \(p(|_{t})\) can be represented as \(p(|_{t})= p(|_{0})p(_{0}| _{t})d_{0}\). For problem-agnostic models, it is common to approximate the posterior \(p(_{0}|_{t})\) using an unimodal Gaussian distribution (Chung et al., 2022; Song et al., 2022). In this work, we restrict our discussion to the posterior approximation in IIGDM (Song et al., 2022) and its flow variant (Pokle et al., 2024) (named as \(\)GFM in our work), \(p(_{0}|_{t})(}_{0},r_{t}^ {2}_{d})\), which yields the following estimate of the conditional score:

\[_{_{t}} p(|_{t})=}_{0}}{_{t}}^{}^{}(r_{t}^{2} ^{}+_{y}^{2}_{d})^{-1}(-} _{0}),\] (5)

where \(}_{0}\) is the first-order Tweedie's moment estimate (Stein, 1981). Our choice of using the IIGDM approximation is motivated by its expressive posterior approximation \(p(x_{0}|x_{t})\) compared to other methods such as DPS or MCG. This makes it an excellent starting point for low-budget sampling.

### Conditional Conjugate Integrators

Conjugate IntegratorsThe main idea in conjugate integrators (Pandey et al., 2024) is to project the diffusion dynamics in Eqn. 1 into another space where sampling might be more efficient. The projected diffusion dynamics can then be solved using any numerical ODE solver. On completion, the dynamics can be projected back to the original space to generate samples from the data distribution. To this end, Pandey et al. (2024) introduce an invertible time-dependent affine transformation \(}_{t}=_{t}_{t}\). Interestingly, conjugate samplers have theoretical connections to prior work in fast sampling for unconditional diffusion models (Song et al., 2021; Zhang and Chen, 2023; Lu et al., 2022). We refer the readers to Pandey et al. (2024) for exact details.

#### 2.2.1 Conjugate Integrators for Inverse Problems

Next, we design conjugate integrators for linear inverse problems. For simplicity, we discuss noiseless inverse problems, \(_{y}=0\), and defer the discussion of noisy inverse problems to Section 2.4. Furthermore, due to space constraints, we present our analysis for diffusion models and defer the discussion of flows to Appendix B. Lastly, without loss of generality, we assume the standard score network parameterization, \(_{}(_{t},t)=_{}(t)_{ }(_{t},t)\) where \(_{}(t)\) is the notation from the score precondition defined in Karras et al. (2022).

A straightforward way to define conditional conjugate integrators is to treat the score estimate \(_{_{t}} p(|_{t})\) as a _black-box_ i.e., ignore the structure of the inverse problem. For this case, we formally specify the conjugate integrator formulation as,

**Proposition 1**.: (Extended IIGDM) _For the conditional diffusion dynamics defined in Eqn. 3, introducing a diffeomorphism, \(}_{t}=_{t}_{t}\), where,_

\[_{t}=\,_{0}^{t}_{s}-_{s}ds, _{t}=-_{0}^{t}_{s}_{s}_{s}^{ }_{}(s)ds,\] (6)

_induces the following projected diffusion dynamics,_

\[d}_{t}=_{t}_{t}_{t}^{-1}}_{t} dt+d_{t}_{}\,(_{t},t)-r_{t}^{-2}}{2} _{t}_{t}_{t}_{t}^{}}_{ 0}}{_{t}}^{}(^{}-}_{0})dt,\] (7)

_where \(^{}=^{}(^{})^{-1}\) and \(=^{}(^{})^{-1}\) represent the pseudoinverse and the orthogonal projector operators for the degradation operator \(\). (Proof in Appendix A.2)_

Similar to Pandey et al. (2024), the matrix \(_{t}\) is a design choice. We refer to the formulation in Eqn. 7 as Extended IIGDM since for \(_{t}=0\), the ODE in Eqn. 7 becomes equivalent to the IIGDM formulation proposed in Song et al. (2022). This is because, for \(_{t}=0\), Conjugate Integrators are equivalent to DDIM (Song et al., 2021) (See Pandey et al. (2024) for proof). Therefore, the projected diffusion dynamics in Eqn. 7 already present a more generic framework for designing samplers for inverse problems over IIGDM. In this work, we only explore the parameterization in Eqn. 7 for \(_{t}=0\) and hence refer to it simply as \(\)_GDM_ (analogously \(\)_GFM_ for flows; see Appendix B).

One characteristic of the formulation in Eqn. 7 is the black-box nature of the conditional score \(_{_{t}} p(|_{t})\). However, the inherent linearity in the conditional score can be used to design _better conditioned_ (more on this in Section 2.3) conjugate integrators, which we illustrate formally in the form of the following result.

Proposition 2. (Conjugate IIGDM) Given a noiseless linear inverse problem with \(_{y}=0\), a design matrix \(:^{d d}\), and the conditional score \(_{_{t}} p(|_{t})\) approximated using Eqn. 5, introducing the transformation \(}_{t}=_{t}_{t}\), where

\[_{t}=_{0}^{t}_{s}-_{s}+r_{s}^{-2}}{2_{s}^{2}}_{s}_{s}^{}ds,\] (8)

induces the following projected diffusion dynamics:

\[d}_{t}=_{t}_{t}_{t}^{-1}}_{t} dt+d_{y}+d_{s}_{}(_{t},t)+d _{j}_{_{t}}(_{t},t)(H^{}y-P}_{0})},\] (9)

where \((.)\) denotes the matrix exponential, \(^{}\), and \(\) are the pseudoinverse and projector operators (as defined previously). Proof in Appendix A.3.

In this case, the coefficients \(_{y}\), \(_{j}\), and \(_{s}\) depend on time \(t\) and the degradation operator \(\) (See Appendix A.3 for full definitions). Intuitively, by including information about the degradation operator \(\) and the guidance scale in the transformation \(_{t}\) in Eqn. 8, we incorporate the specific structure of the inverse problem in the sampler design, which can have several advantages (more on this in Section 2.3). Moreover, the matrix \(_{t}\) is a design choice of our parameterization (we will discuss exact choices in Section 2.2.2). We refer to this parameterization as _C_-\(\)_GDM_ (analogously _C_-\(\)_GFM for flows; see Appendix B_). In this work, we restrict our discussion to this parameterization and discuss some practical and theoretical aspects next.

#### 2.2.2 Practical Design Choices

**Choice of Diffusions and Flows:** While our proposed integrators are applicable to generic diffusion processes (Dockhorn et al., 2022; Pandey and Mandt, 2023) and flows (Ma et al., 2024), we restrict follow-up discussion to VP-SDE (Song et al., 2020) diffusion for which \(_{t}=-_{t}_{d},_{t}=} _{d}\) and OT-flows (Liu et al., 2022; Lipman et al., 2023) for which \(_{t}=t,_{t}=1-t\). For our score network parameterization, we set \(_{}(t)=-1/_{t}\), corresponding to the standard \(\)-prediction (Ho et al., 2020; Song et al., 2020) parameterization in diffusion models.

**Choice of \(_{t}\):** Similar to Pandey et al. (2024), we set \(_{t}=_{d}\), where \(\) is a time-invariant scalar hyperparameter tuned during inference for optimal sample quality.

**Choice of \(w_{t}\):** Similar to prior work (Song et al., 2022; Pokle et al., 2024), we use an adaptive guidance weight schedule. For diffusion models, we use \(w_{t}=w_{t}^{2}r_{t}^{2}\) where \(r_{t}^{2}=^{2}}{_{t}^{2}+_{t}^{2}}\). Analogously, for flows, we set \(w_{t}=w_{t}^{2}r_{t}^{2}\) where \(r_{t}^{2}=^{2}}{_{t}^{2}+_{t}^{2}}\)

Having an extra multiplicative factor of \(_{t}^{2}\) (for VP-SDE) or \(_{t}^{2}\) (for flows) stabilizes the numerical computation of coefficients in Eqn. 9 before sampling. We tune the static guidance weight \(w\) during inference for optimal sample quality.

**Choice of Start Time:** Given a degradation output \(\), it is common to start diffusion or flow sampling at \(<T\) or \(>0\), respectively (Chung et al., 2022; Song et al., 2022; Pokle et al., 2024). Consequently, we initialize the diffusion sampling process as \(_{}=_{}^{}+_{}\). Analogously for flows, we initialize sampling at \(_{}=_{}^{}+_{}\).

**Choice of the ODE Solver:** Unless specified otherwise, we use the Euler discretization scheme for C-\(\)G(D/F)M samplers.

We illustrate a generic C-IIGDM sampling routine in Algorithm 1 and include additional implementation details in Appendix D. Next, we present some theoretical aspects of our proposed method.

### Theoretical Aspects

With the simplifications in Section 2.2.2, the transformation \(_{t}\) in Eqn. 8 simplifies to:

\[_{t}=_{0}^{t}+_{s} ds_{d}-_{0}^{t}_{s}ds ,\] (10)

where \(=^{}(^{})^{-1}\) is an orthogonal projection operator.

**Computing \(_{t}\):** While computing the matrix exponential in Eqn. 10 might seem non-trivial, it has several interesting properties that make it tractable to compute. More specifically, the matrix exponential in Eqn. 10 can be simplified as (Proof in Appendix A.4),

\[_{t}=(_{1}(t))_{d}+((_{2}(t))-1) ,_{1}(t)=_{0}^{t}+_{s} ds,_{2}(t)=-_{0}^{t}_{s}ds,\] (11)

where \((.)\) in Eqn. 11 represents the scalar exponential. Furthermore, the integrals in Eqn. 11 are trivial to compute analytically or numerically, making \(_{t}\) easier to compute. Moreover, \(_{t}^{-1}\) can also be compactly represented as,

\[_{t}^{-1}=(-_{1}(t))_{d}+((-_{2}(t))-1) ,\] (12)

and is also tractable to compute. Due to the tractability of \(_{t}\) and \(_{t}^{-1}\), the projected diffusion dynamics in C-IIGDM are straightforward to simulate numerically.

**Intuition behind \(_{t}\):** Next, we analyze several theoretical properties of the transformation matrix \(_{t}\) in Eqn. 11. More specifically,

\[}_{t}=_{t}_{t}=(_{1}(t)) _{t}-(1-(_{2}(t)))_{t},\] (13)

Since \(=^{}(^{})^{-1}\) is an orthogonal projector, the matrix \(_{d}-\) is also an orthogonal projector which projects any vector \(\) in the nullspace of \(\). Therefore, we can decompose the state \(_{t}\) into two _orthogonal_ components \(_{t}=_{t}+(_{d}-)_{t}\). Plugging this form in Eqn. 13,

\[}_{t}=(_{1}(t))(_{d}-)_{ t}+(_{2}(t))_{t},\] (14)

Intuitively, near \(t=T\) (i.e., at the start of reverse diffusion sampling), for a large static guidance weight \(w\), \((_{2}(t)) 0\). In this limit, from eqn. 14, \(}_{t}(_{d}-)_{t}\). This implies that for a large guidance weight \(w\), the diffusion dynamics are projected into the nullspace of the projection operator \(\). Intuitively, for an inverse problem like superresolution, this implies that near the start of the diffusion process, the projected diffusion dynamics correspond to the _denoising of the high-frequency details_ missing in \(_{t}\). This is because the projector operation, \(_{t}=^{}_{t}\) can be interpreted as the pseudoinverse of the noisy degraded state \(_{t}\), and, therefore, \((_{d}-)_{t}\) represents the high-frequency details missing from the signal component in \(_{t}\).

Moreover, near \(t=0\) (i.e., near the end of reverse diffusion sampling), assuming the guidance weight \(w\) is not too large, both coefficients \((_{1}(t))\) and \((_{2}(t)) 1\), which implies \(}_{t}_{t}\). Thisimplies that near \(t=0\), diffusion happens in the original space, which can prevent over-sharpening artifacts towards the end of sampling. Therefore, we hypothesize that a large \(w\) can also lead to over-sharpened results near the end of sampling, resulting in artifacts in the generated samples. Therefore, introducing the projection \(_{t}\) as defined in Eqn. 10, introduces a tradeoff in the choice of \(w\) to control for sample quality. Lastly, since the parameter \(\) controls the _magnitude_ of \(}_{t}\), it exhibits a similar tradeoff. Indeed, we will empirically demonstrate these tradeoffs in Section 3.3. While our discussion has been limited to diffusion models, a similar theoretical intuition also holds for flows (See Appendix B for proof).

### Extension to Noisy and Non-Linear Inverse Problems

While our discussion has been primarily in the context of noiseless linear inverse problems, the conditional Conjugate Integrator framework can also be extended to develop samplers for noisy linear and non-linear inverse problems. We provide a more detailed explanation for the same in App. C.

## 3 Experiments

Next, we empirically demonstrate that our proposed samplers C-IIGDM/GFM outperform recent baselines on linear image restoration tasks regarding sampling speed vs. quality tradeoff. We then present ablation experiments highlighting the key parameters of our samplers. Lastly, we present design choices for solving noisy and non-linear inverse problems using our proposed framework.

Models and Dataset:For diffusion models, we utilize an unconditional pre-trained ImageNet (Deng et al., 2009) checkpoint at 256\(\)256 resolution from OpenAI (Dhariwal and Nichol, 2021)3. For evaluations on the FFHQ dataset Karras et al. (2019), we use a pre-trained checkpoint from Choi et al. (2021) also at 256\(\)256 resolution. For flow model comparisons, we utilize three publicly available model checkpoints from Liu et al. (2022)4, trained on the AFHQ-Cat (Choi et al., 2020), LSUN-Bedroom Yu et al. (2015), and CelebA-HQ (Karras et al., 2018) datasets. Each flow model was trained at a pixel resolution of \(256 256\). For diffusion models, we conduct evaluations on a 1k subset of the evaluation set. For flows, we conduct evaluations on the entire validation set.

Tasks and Metrics:We evaluate our samplers qualitatively (see Figure 2) and quantitatively on three challenging linear inverse problems under the noiseless setting. Firstly, we test **Image Super-Resolution**, enhancing images from bicubic-downsampled \(64 64\) pixels to \(256 256\) pixels. Secondly, we assess **Image Inpainting** performance on images with a fixed free-form center mask. Lastly, we evaluate our samplers on **Gaussian Deblurring**, applying a Gaussian kernel with \(=3.0\) across a \(61 61\) window. We evaluate the performance of each task based on three perceptual metrics: FID (Heusel et al., 2017), KID (Binkowski et al., 2018) and LPIPS (Zhang et al., 2018).

Methods and Baselines:We assess the sample quality of our proposed C-IIGDM and C-IIGFM samplers using 5, 10, and 20 sampling steps (denoted as Number of Function Evaluations (NFE)). We conduct an extensive search to optimize the parameters \(w\), \(\) and \(\) to identify the best-performing configuration based on sample quality. For diffusion baselines, we include DDRM (Kawar et al., 2022), DPS (Chung et al., 2022), and IIGDM (Song et al., 2022). As recommended for DPS (Chung et al., 2022), we use NFE=1000 for all tasks. For DDRM, we adhere to the original implementation and run it with \(_{b}=1.0\) and \(=0.85\) at NFE=20. We test our implementation of IIGDM (see Section 2.2), with NFE values of 5, 10, and 20 and use the recommended guidance schedule of \(w_{t}=r_{t}^{2}\) across all tasks. For flow models, we consider the recently proposed method inspired by IIGDM running on OT-ODE path by Pokle et al. (2024) (which we refer to as IIGFM; see Appendix B), and similarly run it with NFE values of 5, 10, and 20. We optimize all baselines by conducting an extensive grid search over \(w\) and \(\) for the best performance (in terms of sample quality).

### Quantitative Results

We present the results of our method applied to inverse problems in Table 1, specifically using the CelebA-HQ dataset for flow-based models and the ImageNet dataset for diffusion-based models. For a comprehensive review of additional results across different datasets, please refer to Appendix E. Our method consistently surpasses other approaches across all sampling budgets (indicated by NFE) for the inpainting task. Similarly, our flow-based sampler (C-IIGFM) exhibits superior perceptual quality for image super-resolution at NFEs of 5 and 10. The IIGFM model only reaches comparable performance at higher NFEs. Remarkably, our diffusion-based sampler C-IIGDM outperforms all baselines across the entire range of NFEs. Notably, C-IIGDM outperforms competing baselines requiring 20-1000 NFEs in just 5 sampling steps on the challenging ImageNet dataset, demonstrating a significant speedup in sampling speed while preserving sample quality. A similar pattern is observed in the image deblurring task, where the performance of IIGDM/IIGFM approaches that of our method only when the NFE is increased to 20 steps.

Interestingly, we observe a plateau in performance improvements at NFE=20 for both super-resolution and deblurring tasks using our method. This suggests that while our method efficiently utilizes the iterative model under a deterministic path with an Euler solver, further enhancements in performance, particularly at higher NFEs, might require integrating stochastic sampling techniques or more advanced solvers. This potential next step could unlock further gains from our approach in complex image processing tasks.

### Qualitative Results

Figure 2 presents a qualitative comparison between our proposed method and the IIG(D/F)M baseline. The inpainting results in the first column reveal that IIGFM tends to introduce gray artifacts within the inpainted areas. This issue may stem from the initialization of the parameter \(\); optimal performance is achieved when \( 0.2\), as established during our parameter tuning phase and corroborated by Pokle et al. (2024). Consequently, insufficient NFE means IIGFM cannot effectively eliminate the artifacts associated with the inpainting mask in our experiments. For image super-resolution, our method excels in restoring fine details, particularly evident in high-frequency image components such as human hair and wheat ears. Similarly, for the deblurring task, our method qualitatively outperforms the baseline, especially in mitigating the over-smoothing artifacts (Figure 2, last column). Additional examples are provided in Appendix E.4.

### Ablation Studies

In this section, we further explore the impact of the hyperparameters \(w\), \(\), and \(\), which were identified during our tuning phase and link to the theoretical insights discussed in Section 2.3. We recognize that \(\) is particularly task-specific and relatively straightforward to adjust. For instance, tasks such as inpainting require a smaller \(\) to prevent masking artifacts, whereas tasks like super

    &  &  & \)} &  \\   & & C-IIGFM & IIGFM & C-IIGFM & IIGFM & C-IIGFM & IIGFM \\   & 5 & **0.125** & 0.240 & **17.6** & 167.0 & **26.95** & 161.49 \\  & 10 & **0.074** & 0.188 & **8.0** & 86.6 & **14.64** & 94.91 \\  & 20 & **0.065** & 0.144 & **4.6** & 54.4 & **10.93** & 65.39 \\   & 5 & **0.063** & 0.091 & **5.5** & 17.5 & **13.08** & 21.84 \\  & 10 & **0.058** & 0.076 & **3.6** & 12.2 & **10.65** & 16.73 \\  & 20 & **0.064** & 0.069 & 3.9 & **3.5** & 11.07 & **10.23** \\   & 5 & **0.083** & 0.114 & **3.7** & 10.9 & **12.86** & 18.97 \\  & 10 & **0.077** & 0.088 & **5.0** & 7.0 & **14.41** & 15.09 \\  & 20 & 0.080 & **0.073** & 7.9 & **3.1** & 17.10 & **11.35** \\    &  &  &  &  &  &  &  &  \\   & 5 & **0.220** & 0.306 & & **2.7** & 6.3 & & **37.31** & 49.06 &  \\  & 10 & **0.266** & 0.252 & 0.2318 & **1.6** & 4.8 & 5.8 & 14.1 & **34.22** & 44.30 & 38.18 & 51.64 \\  & 20 & **0.207** & 0.222 & & **1.7** & 2.5 & & **34.28** & 37.36 & \\   & 5 & **0.272** & 0.349 & & **3.89** & 14.1 & & **44.42** & 63.94 &  \\  & 10 & **0.272** & 0.294 & 0.619 & 0.336 & **3.6** & 5.3 & 59.5 & 12.3 & **43.37** & 47.80 & 139.58 & 62.53 \\   & 20 & 0.268 & **0.259** & & **3.5** & 4.2 & & **43.70** & 44.20 & \\   

Table 1: Comparison between Conjugate IIG(D/F)M and other baselines for noiseless linear inverse problems. Top: Flow models (CelebA-HQ) and Bottom: Diffusion Models (ImageNet). Entries in bold show the best performance for a given sampling budget.

resolution or deblurring benefit from a larger \(\) to ensure effective initialization. Consequently, our discussion will primarily focus on the effects of \(w\) and \(\). Figure 3 illustrates the impact of varying \(w\) and \(\) on sample quality for image super-resolution on the CelebA-HQ and ImageNet datasets.

From Figure 3 we make the following observations. Firstly, for both C-IIGDM and C-IIGFM samplers, we observe that the optimal value of \(\) can differ from \(=0\). This illustrates the usefulness of parameterizing \(_{t}\) in our sampler design. On the contrary, IIGDM or IIGFM samplers do not have this flexibility and, therefore, yield sub-optimal sample quality at different sampling budgets. Secondly, we observe that deviating from the optimal \(\) can lead to degradation in sample quality. More specifically, we observed that deviating from our tuned value of \(\) leads to either over-sharpening artifacts or blurry samples (See Figs. 7, 11). This is intuitive since \(\) controls the scale of the transformation \(}_{t}=_{t}_{t}\) (see Eqn. 14) and thus plays a significant role in conditioning the projected diffusion dynamics. We observe a similar tradeoff on varying the static guidance weight \(w\) where a large magnitude of \(w\) can lead to over-sharpened artifacts while a very small guidance weight can lead to blurry samples (See Figs. 6, 10). These empirical observations are consistent with our theoretical analysis in Section 2.3, confirming our theoretical intuition on the role of the sampler parameters \(w\) and \(\).

## 4 Related Works

Fast Unconditional Sampling:Recent research has significantly advanced the efficiency of the sampling process in unconditional diffusion/flow models (Song et al., 2020; Lipman et al., 2023; Manduchi et al., 2024). One line of research involves designing efficient diffusion models to improve sampling by design (Karras et al., 2022; Dockhorn et al., 2022; Pandey and Mandt, 2023; Song et al., 2023). Since our treatment of conditional Conjugate Integrators is quite generic, our method is readily

Figure 2: Qualitative comparison between C-IIG(D/F)M and IIG(D/F)M baselines on five different datasets. (a, b, c) Inpainting, De-blurring, and 4x Super-resolution with C-IIGFM, respectively. (d,e) 4x Image Super-resolution and De-blurring with C-IIGDM, respectively. (\(_{y}=0\), NFE=5)

compatible with most advancements in diffusion model design. Another line of work focuses on distilling a student model from a teacher model, enabling sampling in even a single step (Salimans and Ho, 2022; Meng et al., 2023; Sauer et al., 2024). However, since these methods require expensive re-training, there has been a significant interest in the development of fast samplers applicable to pretrained diffusion/flow models (Liu et al., 2022; Pandey et al., 2024; Shaul et al., 2024; Zhang and Chen, 2023; Lu et al., 2022; Song et al., 2021; Gonzalez et al., 2023). Our work falls under the latter line of research, where we develop fast conditional samplers that can be applied to pretrained diffusion models.

Conditional Iterative Refinement Modelshave become prevalent for tasks requiring controlled generation. These models often involve training specialized conditional diffusion models (Sahara et al., 2022; Yang and Mandt, 2023; Kong et al., 2021; Pandey et al., 2022; Preechakul et al., 2022; Rombach et al., 2022; Podell et al., Ramesh et al., 2022; Peebles and Xie, 2023; Ma et al., 2024; Esser et al., 2024; Chen et al., 2024) and may incorporate classifier-free guidance (Ho and Salimans, 2021) or classifier guidance (Dhariwal and Nichol, 2021; Song et al., 2020) for conditional sampling. These approaches have also spurred research into solving inverse problems related to various image degradation transformations, such as inpainting and super-resolution (Kawar et al., 2022; Chung et al., 2022; Song et al., 2022; Mardani et al., 2023; Pokle et al., 2024). Although these methods demonstrate promising outcomes, they are typically bottlenecked by a costly sampling process, emphasizing the need for a fast sampler to address inverse problems efficiently. Recent work Xu et al. (2024) employs a consistency model Song et al. (2023) to enhance posterior approximation, but incorporating an additional model may deviate from our proposal of using a single pre-trained model. DPM-Solver++ (Lu et al., 2023) also tackles the problem of accelerating guided sampling in diffusion models. However, unlike (Lu et al., 2023), we incorporate the structure of the inverse problem in the sampler design.

## 5 Discussion

We present a generic framework for designing samplers for accelerating guided sampling in iterative refinement models. In this work, we explore a specific parameterization of this framework, which incorporates the structure of the inverse problem in sampler design. We provide a theoretical intuition behind our design choices and empirically justify its effectiveness in solving linear inverse problems in as few as 5 sampling steps compared to 20-1000 NFEs required by competing baselines. While our method can serve as an important step toward designing fast-guided samplers, there are several important future directions. Firstly, our parameterization of the transform \(_{t}\) can be more expressive by learning it directly during the sampling stage. Secondly, in this work, we consider inverse problems with a known degradation operator. Extending our framework for solving blind inverse problems could be an important research direction. Lastly, it would be interesting to adapt our solvers to techniques for solving inverse problems in latent diffusion models (Rout et al., 2024) to enhance sampling efficiency further.

**Broader Impact:** While our work has the potential to make synthetic data generation accessible, the techniques presented in this work should be used responsibly. Moreover, despite good sample quality in a limited sampling budget, restoration can sometimes lead to artifacts in the generated sample which can be undesirable in some domains like medical image analysis.

Figure 3: Impact of \(\) and \(w\) on sampling quality. Red curves and labels represent the LPIPS scores, while blue curves and labels indicate the FID scores.