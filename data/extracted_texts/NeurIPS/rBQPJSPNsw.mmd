# Language decoding from human brain activity via contrastive learning

Matteo Ferrante

Department of Biomedicine and Prevention

University of Rome, Tor Vergata

matteo.ferrante@uniroma2.it

&Nicola Toschi

Department of Biomedicine and Prevention

University of Rome, Tor Vergata

Martinos Center For Biomedical Imaging

MGH and Harvard Medical School (USA)

Alexander Huth

Department of Computer Science & Neuroscience

UT Austin (USA)

###### Abstract

We propose a novel contrastive learning approach to decode brain activity into sentences by mapping fMRI recordings and text embeddings into a shared representational space. Using data from three subjects, we trained a cross-subject fMRI encoder and demonstrated effective sentence identification with a retrieval module. Our model shows strong alignment between brain activity and linguistic features, with top-1 accuracy up to 49.2% and top-10 accuracy up to 84%, significantly outperforming chance levels. Our results highlight the potential of contrastive learning for cross-subject language decoding,

## 1 Introduction

Language is one of the most ubiquitous ways we experience the flow of information in our daily lives. We read, speak, communicate, and even think through language. It is a complex phenomenon that allows us to understand and convey information to others. To do this, our brain generates semantic representations of everything we encounter, taking context into account. Recent research has demonstrated a convergence between brain activity  during language tasks, such as listening or reading, and the way large language models process sentences. This has been shown through brain recordings using both non-invasive methods, such as fMRI, EEG, and MEG, as well as invasive techniques like LFP and ECoG. Powerful encoding and decoding models have been developed to link external stimuli, such as acoustically perceived sentences, images, videos, music with neural representations recorded during these tasks . These models have shown remarkable results across both invasive and non-invasive brain recording techniques. Here, we propose a novel method based on contrastive learning to learn a cross-subject fMRI encoder that projects fMRI recordings and text embeddings into a shared space, enabling sentence identification with a retrieval module. Our model takes as input fMRI activity and computes fMRI embeddings that can be compared with pre-computed sentence embeddings belonging to a set of candidates. By selecting the closest sentences in this space we can effectively decode the brain activity and obtain clues about semantic representation in the brain. Fig. 1 show a scheme of the pipeline proposed here.

## 2 Material and Methods

### Data

In our analysis, we used the publicly available dataset from . We focused on subjects S1, S2, and S3, each of whom underwent fMRI recordings for approximately 16 hours while listening to 83 stories from The Moth and Modern Love podcasts. The fMRI data was collected using a 3T Siemens Skyra scanner with a repetition time (TR) of 2.00 seconds and an isotropic voxel size of 2.6 mm. This allowed for the measurement of BOLD signals, which reflect neural activity with an inherent delay due to the hemodynamic response. Preprocessing steps included motion correction, cross-run alignment, standardization and detrending of low-frequency. For more details, we refer the reader to the original paper . Data from the first 70 stories of each subject were used as training set, while the other 12 stories were used as validation set. Also, the story _"wheretheressmoke"_, which was listened to 10 times to ensure better signal to noise ratio at test set, aligning with recent literature on language encoding and decoding [19; 1].

### Encoding

The first step of our pipeline involves reducing the complexity of the signal we need to process. While much of the brain is active during language and semantic tasks [10; 11], certain regions are more directly involved in language processing. Therefore, we begin by identifying cortical regions that, at the voxels level, are more easily modeled by a language model, allowing us to focus our analysis on these regions. We trained an encoding model of brain activity using a large language model (LLM) as the foundation. For each word in the training stories, we computed embeddings from LLama3-8B , specifically from the 13th layer, using a context window of the previous five words. This choice of layer is supported by previous studies  that found early layers in LLama models exhibit stronger correlations with brain activity. To align the word embeddings with the fMRI temporal resolution, we downsampled them using a Lanczos filter, creating matching sentence-fMRI pairs. Finally, we calculated the Pearson correlation between the predicted and true brain activity on held-out validation data, selecting the top 10,000 cortical voxels as our target regions. The activity in these voxels are the targets we aim to embed and decode.

Figure 1: Schematic representation of the contrastive learning pipeline. The fMRI model encodes brain activity from 10,000 voxels located in the cortex into a shared latent space, while the text pooler processes downsampled LLama text embeddings. Pairwise dot products between fMRI and text embeddings are computed to create a similarity matrix, which is used for aligning brain activity with corresponding linguistic features. Yellow cells highlight correct matches between fMRI and text embeddings.

### Contrastive fMRI model

The core contribution of this work lies in the proposed representation learning pipeline. Since a single TR of fMRI recording can be influenced by several preceding words due to the hemodynamic response (HRF), we must account for this effect. To address the unique properties of fMRI data, we developed a model consisting of two neural networks: an fMRI encoder and a text embedding pooler. The fMRI model is a cross-subject neural network. The first layer is subject-specific and projects the top 10,000 activity corresponding to the top 10000 voxels of each subject into a shared representation space of dimensionality _common_dim_. The remaining layers of the network are shared across subjects, consisting of a multi-layer perceptron (MLP) that transforms the data from _common_dim_ to the final latent dimension _latent_dim_. The text embedding pooler is an MLP that processes four downsampled word embeddings corresponding to the TRs that might affect the measured brain activity (i.e. those from 1, 2, 3, and 4 timepoints before the corresponding BOLD response). Each word embedding is projected into the final latent space by a linear layer that reduces the dimensionality. These projections are then concatenated, resulting in a final sentence representation of dimensionality _latent_dim_. In appendix, we detail all the hyperparameter search and architectures. Let \(x\) represent the fMRI activity, \(y\) the downsampled word embeddings, \(f\) the fMRI model, and \(g\) the text pooler. We define the projections as: \(z_{x}=f(x), z_{y}=g(y)\). Let \(z_{x}^{n d}\) represent the encoded fMRI features and \(z_{y}^{n d}\) represent the encoded text embeddings, where \(n\) is the batch size and \(d\) is the latent dimensionality. The logits matrix, which contains the similarity scores between each pair of fMRI and text embeddings, is computed as:

\[_{ij}=} z_{y_{j}}^{}}{}\]

where \(\) is the temperature parameter that controls the sharpness of the distribution. The pairwise similarities are computed using the dot product between the fMRI and text embeddings: similarities\({}_{ij}=z_{x_{i}} z_{y_{j}}^{}\). Let the target labels be the identity mapping, where each input is matched with itself in the contrastive learning task. The targets vector \(t^{n}\) is defined as: \(t=\{0,1,,n-1\}\) This implies that \(t_{i}=i\), ensuring each fMRI sample is matched with the corresponding text sample. The contrastive loss \(\) is computed as a combination of two cross-entropy losses, one for the alignment of fMRI to text and the other for text to fMRI:

\[=(_{}(,t)+_{}(^{},t))\]

where \(_{}\) denotes the cross-entropy loss function. This formulation optimizes both directions in the contrastive learning objective, ensuring that fMRI features are closely aligned with the corresponding text embeddings and vice versa. This contrastive learning approach ensures that both fMRI activity and the corresponding text embeddings are projected into the same latent space, aligning brain activity with linguistic features.

### Retrieval and Evaluation

The retrieval module compares the L2 distance between fMRI and text embeddings across the test set to find the closest sentences to each fMRI sample. We evaluate the decoded sentences using three metrics: identification accuracy, top-1 accuracy, and top-10 accuracy. For top-10 retrieval, we select the 10 closest sentences to each fMRI TR based on L2 distance, with the target sentence defined as the previous 4 TRs (8 seconds) plus 5 preceding context words. Identification accuracy, adapted from vision and music decoding literature, measures how well the model identifies the correct sentence by comparing self-correlations in the latent space with other correlations. We compute Pearson correlations between the predicted vectors and targets, storing the results in a correlation matrix, and successively calculate identification accuracy by comparing the self-correlation with others in the same row and normalizing the result.

## 3 Results

The results from our contrastive learning-based language decoding model are shown in Figure 2 and Table 3. The top panel of Figure 2 illustrates the model's encoding performance, with Pearsoncorrelations between predicted and actual brain embeddings across three subjects (S1, S2, S3). The red regions indicate strong correlations, suggesting the model captures language-related cortical activity patterns, likely in language-processing areas of the brain. The consistent performance across subjects highlights the model's robustness in identifying key neural features associated with sentence comprehension. In the bottom panel, the cosine similarity matrices between sentence embeddings and brain embeddings show a strong alignment, indicated by bright diagonal lines, reflecting the model's ability to map linguistic structures to brain representations effectively. Table 3 provides quantitative metrics for decoding accuracy. The Top-1 Accuracy, which ranges from 0.313 (S2) to 0.498 (S3), significantly outperforms chance levels, confirming the model's ability to predict precise sentences. Top-10 Accuracy further validates this, with values as high as 0.838 (S3), indicating that the correct sentence is frequently among the top 10 predictions. Identification Accuracy is also high for all subjects, ranging from 0.910 (S2) to 0.962 (S3), reinforcing the model's strong performance in decoding brain representations of sentences. Overall, both Figure 2 and Table 3 demonstrate the model's effectiveness in linking sentence embeddings to brain activity, with strong performance across subjects. Subject S3 consistently shows the best results, suggesting individual differences in brain activity may influence decoding accuracy, offering avenues for future investigation.

## 4 Discussion and Conclusions

Contrastive learning has proven to be a robust method for learning cross-subject mappings between brain activity and sentence-level embeddings. However, a key limitation of our approach is that decoding is performed through a retrieval module (i.e., sentence identification). This requires access to candidate sentences beforehand, limiting the model's ability to generalize to brain activity related to sentences that differ significantly from those in the training dataset. Another important limitation pertains to future applications of this work. The learned fMRI embeddings could potentially be

   Subject & Top-1 Acc & Top-10 Acc & Chance Level Top-1 & Chance Level Top-10 & Identification Acc \\  s1 & 0.3780 & 0.786 & 0.0114 & 0.0894 & 0.9571 \\ s2 & 0.3127 & 0.6666 & 0.0116 & 0.0855 & 0.9100 \\ s3 & 0.4982 & 0.8381 & 0.0118 & 0.0814 & 0.9624 \\   

Table 1: Performance metrics for Top-1, Top-10, and Identification Accuracy across different datasets.

Figure 2: Top: Pearson correlation maps of predicted vs. actual brain embeddings for subjects S1, S2, and S3 shown on flattened cortical surfaces. The regions we identified are typically associated with language processing, such as the superior temporal gyrus and parts of the inferior frontal cortex. Bottom: Cosine similarity matrices between sentence embeddings and brain embeddings for each subject.

used in conjunction with Bayesian decoding techniques or as inputs to modified large language models (LLMs) for open vocabulary decoding. However, these approaches raise concerns about privacy and bias. It will be crucial to address how to distinguish between actual thoughts and brain representations, and how to prevent biases in both models and human interpretations from influencing the results. Future research should explore the concept of neural privacy and develop strategies to disentangle model biases from genuine cognitive processes. In conclusion, this work presents a cross-subject architecture that decodes brain activity into sentences using contrastive learning and sentence identification, laying the groundwork for future advancements in brain-to-language decoding.