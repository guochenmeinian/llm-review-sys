# Layer-Adaptive State Pruning

for Deep State Space Models

Mineon Gwak\({}^{}\), Seongrok Moon\({}^{}\), Joohwan Ko\({}^{}\), PooGyeon Park\({}^{}\)

\({}^{}\) Department of Electrical Engineering, POSTECH

\({}^{}\) Department of Computer Science, University of Massachusetts Amherst

{mineeon25,srmoon,ppg}@postech.ac.kr, joohwanko@cs.umass.edu

corresponding author

###### Abstract

Due to the lack of state dimension optimization methods, deep state space models (SSMs) have sacrificed model capacity, training search space, or stability to alleviate computational costs caused by high state dimensions. In this work, we provide a structured pruning method for SSMs, **L**ayer-**A**daptive **ST**ate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level output energy loss by extending modal truncation for a single system. LAST scores are evaluated using the \(_{}\) norms of subsystems and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning. Across various sequence benchmarks, LAST optimizes previous SSMs, revealing the redundancy and compressibility of their state spaces. Notably, we demonstrate that, on average, pruning \(33\%\) of states still maintains performance with \(0.52\%\) accuracy loss in multi-input multi-output SSMs without retraining. Code is available at https://github.com/msgwak/LAST.

## 1 Introduction

Deep state space models (SSMs) have proven effective in modeling sequential data by optimally compressing input history to internal states [Gu et al., 2020, 2021, 2022b, Gu and Dao, 2023, Zhang et al., 2023, Parnichkun et al., 2024]. Given their modeling capabilities, ensuring the feasibility and stability of SSMs during training has become a crucial research focus for achieving efficient learning without divergence. Leveraging the knowledge founded in linear system theory [Kailath, 1980], various advancements have emerged, including stability-guaranteeing parameterization [Gu et al., 2022a], general system architecture [Smith et al., 2023], and efficiency improvements via frequency-domain operations, utilizing the fast Fourier transform and the transfer functions of systems [Gu et al., 2022b, a, Zhang et al., 2023, Parnichkun et al., 2024].

One of the main computation and memory contributors of SSMs is the state dimension \(n\). Since the initial proposal of SSMs, a multiple single-input single-output (multi-SISO) architecture has been employed for scalable and efficient training Gu et al. [2022b, a], Gu and Dao , Zhang et al. , Parnichkun et al. . In this architecture, rather than directly learning an \(n\)-dimensional system, smaller-dimensional SISO systems are trained in parallel and then integrated through a channel-mixing layer. Within this structure, Gupta et al.  presented that diagonal systems can achieve matching performance to nondiagonal systems. Gu et al. [2022a] introduced a stability-guaranteed model, where the diagonal systems are trained to satisfy the necessary and sufficient stability condition.

Instead of utilizing multiple SISO systems in parallel, Smith et al.  adopted a multi-input multi-output (MIMO) architecture, where the enhanced information usage through a MIMO system.

This architecture provides high performance with much smaller state dimensions than equivalent block systems in multi-SISO layers. For instance, in the Path-X task that involves the longest tested sequences, this architecture showed state-of-the-art performance (Smith et al., 2023; Parnichkun et al., 2024). However, both architectures lack optimization methods for state dimensions, leading to inefficiencies when the model is over-parameterized for the task.

Recently, Parnichkun et al. (2024) parameterized the transfer functions of SISO systems and proposed a state-free inference. However, this approach indirectly trains the poles of the transfer functions, resulting in a restrictive search space or stability being guaranteed only at initialization.

Focusing on the stability-guaranteed diagonal SSMs, we develop and verify a layer-adaptive model order reduction (MOR) method for SSMs to identify the least significant states or subsystems in terms of their impact on task performance. Inspired by layer-adaptive neural network pruning (Evci et al., 2020; Lee et al., 2021; Xu et al., 2023) and extending the traditional MOR for a single system (Green and Limebeer, 2012), we propose **L**ayer-**A**daptive **S**T**ate pruning (LAST), where importance scores for learned states are evaluated and used as global pruning criteria. LAST scores measure the relative maximum frequency-domain gain of each subsystem when subsystems with lower scores are excluded, as illustrated in Figure 1. LAST prunes insignificant subsystems to achieve a desired compression level, reducing unnecessary computational and memory costs while bounding the output distortion by the \(_{}\) norms of the pruned subsystems.

We validate the insignificant state identification performance of LAST on long-range sequences, including Long Range Arena (LRA) (Tay et al., 2021) and Speech Command (Warden, 2018) benchmarks. Our results present that previous SSMs have great _compressibility_, demonstrating that pruning 33% (26.25%) of the trained states resulted in only 0.52% (0.32%) of accuracy loss in MIMO models (in multi-SISO models) on average, including the non-compressible cases.

## 2 Background

### Stability of state space models

A DT SSM is stable if all poles, roots of a denominator, of its transfer function lie within the unit circle. However, it is challenging to train systems to ensure stability at every step. One approach for this issue is to confine the search space to sufficient stable region (Zhang et al., 2023), as illustrated in Figure 5 for a second-order linear time-invariant (LTI) system. Due to the restricted search space, training under this condition can limit model performance (Parnichkun et al., 2024). Another approach is initializing the system at the center of the stable region, as marked in Figure 5, referred

Figure 1: Illustration of LAST for two layers. Matrices are divided by lines on a per-state basis, and subsystems are sorted in descending order by their \(_{}\) norms. LAST scores are obtained by normalizing each \(_{}\) norm by the sum of all \(_{}\) norms in a layer when the states with lower \(_{}\) norms are excluded. Since LAST scores correlate with model-level output energy loss, we prune all parameters corresponding to states with low LAST scores.

to as zero initialization in (Parnichkun et al., 2024). While this approach mitigates the performance limitation, the stability is guaranteed only at initialization.

In contrast, diagonal SSMs (Gupta et al., 2022; Gu et al., 2022; Smith et al., 2023) directly parameterize the system poles, enabling the model to explore all expressible systems that possess stability-satisfying poles. Thus, all our derivations are based on the diagonal SSMs to leverage the guaranteed stability, which allows for the application of various system analysis techniques. Detailed explanations on the stability regions are provided in Appendix A.1.

### Diagonal state space models

Architectures.Diagonal SSMs consist of an encoder that increases the number of input channels to \(h\), \(L\) SSM layers, and a decoder for the downstream task. Each SSM layer can be designed with either a multi-SISO or MIMO architecture. In the multi-SISO architecture (Gu et al., 2022), independent systems are trained for each input channel, with a total of \(h\)\(n_{s}\)th-order SISO systems being learned in a layer. A fully connected layer is then used to mix features from different channels. In contrast, the MIMO architecture (Smith et al., 2023) employs an \(n_{m}\)th-order MIMO system within each layer, handling \(h\)-dimensional input and output signals. As noted in Smith et al. (2023), \(h\) SISO systems in a layer can be represented as one MIMO system, where specific states are assigned to each input channel. Therefore, we describe SSM layers using MIMO expressions, defining the effective total state dimension for an SSM layer by \(n\), where \(n=n_{s}h\) for a multi-SISO layer and \(n=n_{m}\) for a MIMO layer.

Parameterization.The learnable parameters in a diagonal SSM layer with state dimension \(n\) are CT system matrices \(^{n n}\), \(^{n h}\), \(^{h n}\), \(^{h h}\), where \(^{n n}\) is a diagonal matrix and complex-valued matrices consist of elements that form conjugate pairs to handle real-valued signals (Gu et al., 2022). In the diagonal structure, each subsystem is discretized by applying different timescales from \(^{n}\) to process discrete sequences. By zero-order hold (ZOH) discretization (Chen, 1984), a discretized LTI diagonal system \(:\) in a layer \(f_{}(_{k};)\) can be represented as follows:

\[_{k+1}=}_{k}+} _{k},_{k}=_{k}+ _{k},\] (1)

where \(}=e^{}\) and \(}=^{-1}(}- _{n})\) are the discretized system matrices, \(_{k}^{n}\) is a state vector, \(_{k}^{h}\) is an input signal, and \(_{k}^{h}\) is an output signal. The stability of the discretized system can be achieved by ensuring the stability of the CT parameters with Hurwitz parameterization (Gu et al., 2022), as derived in Appendix A.2. Finally, a nonlinear activation function \(()\) is applied to the output of the linear system, i.e., \(f_{}(_{k};)=((_{k}))\), introducing nonlinearity to the SSM.

### \(_{}\) norms of systems

In robust control, the \(_{}\) norm is widely used to minimize the worst-case gain from disturbances to outputs, ensuring stability and performance under system uncertainty (Qin and Sun, 2023; Zheng et al., 2023). In this work, we use the \(_{}\) norm to measure the divergence between the original and approximated systems.

For a DT LTI system \(:\) with the transfer function matrix (TFM) \(\), the \(_{}\) norm of the system is defined by

\[\|\|_{}:=_{[0,2]}( (e^{j})),\]

where \(\) denotes the maximum singular value of a matrix. In robust control design, the \(_{}\) norm is frequently minimized to design controllers that ensure the system performs optimally under disturbance.

In this work, we utilize the following important property of the \(_{}\) norm, that is, the energy of the output signal \(\|\|_{2}^{2}\) can be bounded with the squared \(_{}\) norm and the energy of the input signal \(\|\|_{2}^{2}\), i.e.,

\[\|\|_{2}^{2}\|\|_{}^{2}\|\|_{2}^{2},\] (2)

(See Appendix B.1 for derivation). In other words, the \(_{}\) norm of a system measures the maximum gain of the system, which is useful in assessing the energy loss caused by pruning.

LAST: Layer-adaptive state pruning for SSMs

We propose a structured pruning for SSMs with per-state pruning granularity, where all parameters associated with the identified insignificant state are pruned. Although pruning is implemented by _masking_, we represent pruned systems with their effective remaining states and parameters.

In Section 3.1, we derive a local pruning criterion by evaluating the layer-level energy loss for a single SSM layer, which consists of a MIMO system followed by nonlinear activation. In Section 3.2, we extend this to a global pruning criterion by assessing the model-level energy loss when considering multiple stacked SSM layers.

### \(_{}\) scores as local pruning criteria

From a DT system \(:(},},)\), suppose we prune the \(i\)th subsystem \(_{i}:(}_{i},}_{i},_ {i})\) corresponding to the \(i\)th state \(_{i}\), leaving the \(i\)th state-pruned system \(_{-i}\). Specifically, the state-pruned system can be written as follows:

\[_{-i}:(}_{-i}=(_{1},,_{i-1},_{i+1}, ,_{n}),\\ }_{-i}^{}=[ }_{1}^{}&&}_{i-1}^{}&}_ {i+1}^{}&&}_{n}^{}\\ _{-i}=[_{1}&&_{i-1}& _{i+1}&&_{n}]),\]

where \(}=(_{1},,_{n})\), \(}^{}=[}_{1}^ {}&&}_{n}^{}]\), with \(=[_{1}&&_{n}]\) for \(}_{i}^{1 h}\) and \(^{h 1}\). Our objective is to minimize the layer-level output energy loss, defined as the squared \(_{2}\) distortion in the output signal, incurred by the system approximation through state pruning. The optimization is formalized by

\[}{} \|f_{}(;)-f_{}(;_{- })\|_{2}^{2}\] (3) subject to \[|| r,\]

where \(=\{1,,n\}\) is the set of state indices in the full system, \(\) is the set of pruned state indices, and \(r\) indicates the required level of model reduction.

Using the properties of diagonal systems and the \(_{}\) norm in Equation (2), the energy loss can be bounded as follows:

\[\|f_{}(;)-f_{}(;_{-})\|_{2}^{2}_{i}\|_{i}\|_{ }^{2}\|\|_{2}^{2},\] (4)

where \(-:=\) and \(_{i}\) is the TFM of \(_{i}\) (See Appendix B.2 for proof). Therefore, we can reduce a system by pruning subsystems with small \(_{}\) norms, minimizing the upper bound in Equation (3). This result shows that, even in the presence of nonlinearity, pruning for a single layer can be performed similarly to modal truncation (Green and Limebeer, 2012). As the stability is guaranteed by Hurwitz parameterization (Gu et al., 2022), the \(_{}\) norm of a subsystem is evaluated as follows:

\[\|_{i}\|_{}=_{i}}_{i}\|}{1-|_{i}|}.\] (5)

Hence, the importance of \(_{i}\) can be defined by the squared \(_{}\) norm of \(_{i}\) with a minor optimization of computational efficiency for rank-\(1\) matrix \(_{i}}_{i}\) as follows:

\[_{}_{i};= _{i}\|^{2}\|}_{i}\|^{2}}{(1-| _{i}|)^{2}},\] (6)

where \(_{}_{i};\) refers to the \(_{}\) score of \(_{i}\), and we prioritize pruning states with lower scores. This can also be simplified with \(\|}_{i}\|^{2}=1\) when \(}\) is fixed while \(\) is trained. Moreover, when two \(\) matrices are used for bidirectional SSMs, \(\|_{i}\|^{2}\) can be substituted as the average for the two matrices, i.e., \(\|_{i}\|^{2}=(\|_{i}^{f}\|^{2}+\|_{i}^{b}\|^{2})/2\), where \(\|_{i}^{f}\|\) is for forward direction and \(\|_{i}^{b}\|\) is for backward direction.

The \(_{}\) score can be used as a local pruning criterion once the target pruning ratio for each layer is determined. However, this approach has limitations, as it requires a heuristic to determine the pruning ratio for each layer and applies the same amount of pruning without considering layer-specific characteristics.

### LAST scores as global pruning criteria

To extend the local pruning criterion to a global pruning criterion, we now consider the _model-level_ output energy loss incurred by pruning \(L\) layers. In the following description, superscripts indicate layer indices from \(1\) to \(L\) for signals, systems, and state index sets.

Following the notation in Lee et al. (2021); Xu et al. (2023), the output of \(k\)th layer is obtained by recursively applying the preceding systems and activation functions as follows:

\[f_{}(^{(1)};^{(1:k)})=(^{(k)}(f_{}( ^{(1)};^{(1:k-1)}))).\]

Our objective is to minimize the model-level output energy loss as follows:

\[^{(l)}^{(l)}}{} \|f_{}(^{(1)};^{(1:L)})-f_{}(^{(1)}; ^{(1:L)})\|_{2}^{2}\] subject to \[_{l=1}^{L}|^{(l)}| R,\]

where \(^{(l)}:=_{-^{(l)}}^{(l)}\) and \(R\) represents the total required level of model reduction across all layers. Similar to Lee et al. (2021), we consider a greedy iterative optimization, where we decide the next pruning state \(x_{i}^{(l)}\) by optimizing the following problem for every step:

\[,\ i^{(l)}_{t}}{}\ J_{l}i;\ ^{(1:L)}_{i},\] (7)

where \(J_{l}i;\ ^{(1:L)}_{t}:=f_{}( ^{(1)};^{(1:L)}_{t})-f_{}(^{(1)}; ^{(1:l-1)}_{t},^{(l)}_{^{(l)}_{t} \{i\}},^{(l+1:L)}_{t})_{2}^{2}\), \(t\) denotes the step index, and \(^{(l)}_{t}:=^{(l)}_{^{(l)}_{t}}\) with \(^{(l)}_{t}^{(l)}\) indicating the set of remaining states at \(t\) step. The objective function in Equation (7) represents the model-level output energy loss when pruning a single subsystem in one layer among layers pruned to different extents. By the proof provided in the Appendix B.3, the objective function can be upper-bounded by

\[J_{l}i;\ ^{(1:L)}_{t} ^{(l)}_{i}_{}^{2}}{^{(l)}_{^{(l)}_{t}}_{}^{2}}_{k=1}^{L}^{(k)}_{ ^{(k)}_{t}}_{}^{2}^{(1)}_{2 }^{2}.\] (8)

Therefore, the upper bound for a subsystem correlates with the ratio of the squared \(_{}\) norm of the subsystem to the squared \(_{}\) norm of the remaining system for layer \(l\). The other terms, except for the ratio, in Equation (8) are common across all layers and can, therefore, be excluded from the cross-layer importance score calculation.

Although we initially considered an iterative optimization to determine the next pruning state, the important scores for all states in all layers can be computed with a few steps, as each score is independently determined based on the trained parameters. For efficient evaluation of the scores, we sort the subsystems in each layer in descending order with their \(_{}\) norms in advance, s.t., \(_{}_{i}^{(l)};^{(l)}>_{}_{j}^{(l)};^{(l)}\) for \(i<j\). Finally, we define the LAST score for \(_{i}^{(l)}\) as follows:

\[_{i}^{(l)};^{(l)} =_{}_{i}^{(l)};^{(l )}}{_{j l}_{}_{j}^{(l)}; ^{(l)}}\] (9) \[=(^{(l)}_{i}^{2} }^{(l)}_{i}^{2}}{1-|^{(l) }_{i}^{2}^{2}})_{j i}( ^{(l)}_{j}^{2}}^{(l)}_{j} ^{2}}{1-|^{(l)}_{j}^{2}}).\] (10)

Similar to the local pruning criterion, Equation (10) can be modified for the case of using fixed \(}^{(l)}\) or bidirectional SSMs. The LAST score for each state reveals the contribution of the subsystem to the model output by assessing the relative gain within the remaining system in a layer, thereby indicating the significance of the subsystem. We refer to this relative metric calculation as _energy normalization_, which adjusts the state importance from different layers to a comparable scale, enabling a _cross-layer_ comparison of the states from different layers. In this way, states with lower LAST scores are selected from the overall states, and layer-adaptive pruning is performed according to the desired model-level compression rate.

Related works

Model order reduction.In linear system theory, MOR methods have been extensively researched to approximate high-dimensional systems in engineering applications, such as VLSI , power systems , and various systems that employ spatial discretization . Using the \(_{}\) norm to characterize a stable system, modal truncation  removes states from a diagonal realization for minimal \(_{}\) norm distortion of the system. Balanced truncation  transforms a given system into a form, not necessarily diagonal, where all states are controllable and observable, then truncates the transformed system. Due to its superior approximation quality, balanced truncation has been developed for various systems and conditions . However, transformation into non-diagonal systems is not applicable to current diagonal SSMs, where diagonal parameterization is necessary for computational efficiency and stability. In our work, per-state pruning granularity operates similarly to modal truncation. Compared to traditional MOR, which is reducing a single linear system, we extend it to multi-system reduction, where nonlinear functions are also involved, which have not been addressed in traditional system theory.

Layer-adaptive neural network pruning.Using magnitude as a pruning criterion, previous works have demonstrated the superiority of layer-adaptive pruning, where layers have different pruning ratios . In Han et al. , Mocanu et al. , Evci et al. , layer-adaptive pruning was achieved by setting a specific magnitude threshold or target pruning ratio for each layer. In Morcos et al. , Lee et al. , layer-adaptive pruning was performed using a global pruning criterion and simultaneously comparing scores from different layers under the target pruning ratio. Specifically, Lee et al.  proposed a global pruning criterion designed from the Frobenius norm-based upper bound of the worst-case \(_{2}\) distortion caused by pruning one layer while fixing the other. Xu et al.  advanced this approach into joint optimization for the sum of filtered layer-wise worst-case \(_{2}\) distortion over pruning ratios. Inspired by Lee et al. , we provide the first global pruning criterion for SSMs, where a _non-magnitude-based_ criterion is essential due to the different transfer functions of SSMs compared to other neural networks. Lastly, we provide a missing design motivation for the squaring operation in score evaluation in Lee et al.  by offering a clear rationale based on signal energy.

## 5 Experiments

Table 1 presents the average performance of pruning methods for 10 tasks and 2 models. Further experimental details are explained below.

Models and tasks.Experiments were conducted with a single A6000 48GB or RTX 3090 24GB GPU. We verify our method on S4D (S4D-LegS)  and S5  models, which are multi-SISO and MIMO SSMs, respectively. Although our main motivation was to reduce the state dimension of MIMO models, we also investigated the compressibility of multi-SISO models and the applicability of LAST to them.

The models were reproduced with three seeds according to the reported configurations  for the six tasks in LRA benchmark , the raw speech classification task using Speech Commands dataset , and pixel-level image classification tasks using MNIST and CIFAR10 datasets. We evaluated the performance of the full (unpruned) and one-shot pruned models while freezing other parameters not involved with SSM layers. See Appendix C for more experimental details.

Baselines.The unique transfer functions of SSMs require the state pruning granularity and \(_{}\) norm-based pruning criteria, not simple magnitude-based pruning criteria, as validated in Appendix D.

    & Model &  Avg. prun. ratio \\ (Compressible only) \\  & 
 Avg. accuracy loss \(\) \\ (Compressible only) \\  \\   & Uniform & 25.00 (33.33) & 0.39 (0.52) \\  & Global & 25.00 (33.33) & 1.16 (1.55) \\  & LAST & 25.00 (33.33) & **0.32 (0.42)** \\   & Uniform & 33.00 (36.67) & 4.32 (4.80) \\  & Global & 33.00 (36.67) & 7.51 (8.35) \\   & LAST & 33.00 (36.67) & **0.52 (0.58)** \\   

Table 1: Average pruning ratio and accuracy loss for all tasks. Values in parentheses are evaluated by excluding non-compressible cases.

Here, we compare LAST with two pruning methods: Uniform \(_{}\) and Global \(_{}\). Uniform \(_{}\) utilizes the local pruning criterion, \(_{}\) score, and applies the same pruning ratio to each layer. Global \(_{}\) employs \(_{}\) score as a global criterion, serving as the ablation of the energy normalization used in LAST. Moreover, we present random state pruning results to demonstrate the effectiveness of developed local and global pruning criteria in identifying insignificant states. After one-shot pruning, we evaluate whether these methods appropriately identify significant and insignificant states by measuring accuracy without retraining.

Pruning ratios.For models pruned by Global \(_{}\) or LAST, which apply layer-adaptive pruning ratios, the reported pruning ratios indicate the _average_ pruning ratios across all layers. We compare Uniform \(_{}\) and layer-adaptive pruning methods, Global \(_{}\) and LAST, by setting the same desired compression rate. The tested pruning ratios were 10%, 20%, \(\), 90%, and 100%, where a pruning ratio of 100% indicates the extreme case leaving only one pair of complex-conjugate subsystems in each layer.

### Long range arena

The LRA benchmark  has been used to evaluate the ability to capture long-range context from sequences with lengths ranging from 1,024 to 16,384. Table 2 shows the accuracy of models when each model is compressed to the maximum pruning ratio at which LAST achieved an accuracy loss below 1% for LRA tasks.

Without any retraining, LAST outperformed other methods, achieving the average accuracy loss of 0.56% (0.67%) for the average compression rate of 33.3% (40.0%), with (without) the non-compressible cases. Since the complexity and state dimension differ across tasks, achievable compression rate varied: for the most compressible case Text, 80% compression on S4D resulted in less than 1% loss in accuracy, while for the least compressible case ListOps, where the state dimension was initially set to 16 for S5 layers, even 10% compression led to large performance degradation.

Figure 2 shows the accuracies of S5 models at different pruning ratios, including randomly pruned models. For Pathfinder and Path-X tasks, LAST consistently outperformed Uniform \(_{}\) and Global \(_{}\), and the accuracy of Global \(_{}\) significantly dropped at high pruning ratios in these cases.

Moreover, we observed that the performance of Uniform \(_{}\) was comparable to LAST at low pruning ratios, whereas at high pruning ratios, its performance became inferior to LAST. We hypothesize that this was because the number of significant states in a layer was considerably lower than the number of original states. That is, if Uniform \(_{}\) pruning begins pruning beyond the lowest proportion of insignificant states in any layer, it can subsequently cause great accuracy degradation. See Appendix E.2 for full results in LRA tasks.

### Raw speech classification

The inductive bias and CT parameterization of SSMs enable 1) encoding raw speech without requiring feature engineering using methods such as short-time Fourier transform and 2) adapting to changes in sampling rate .

    & &  &  &  &  &  &  \\  & &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & Full model & 0\% & 56.42 & 0\% & 86.40 & 0\% & 90.46 & 0\% & 77.02 & 0\% & 87.94 & 0\% & 88.07 & 81.05 \\   & Uniform \(_{}\) & 10\% & 55.82 & 80\% & 86.02 & 60\% & **89.87** & 0\% & 77.02 & 10\% & 87.59 & 0\% & 88.07 & 80.73 \\  & Global \(_{}\) & 10\% & 49.95 & 80\% & **86.20** & 60\% & 89.84 & 0\% & 77.02 & 10\% & 87.20 & 0\% & 88.07 & 79.1 \\   & LAST & 10\% & **56.27** & 80\% & 85.95 & 60\% & 89.46 & 0\% & 77.02 & 10\% & **87.83** & 0\% & 88.07 & **80.77** \\   & Full model & 0\% & 61.48 & 0\% & 88.88 & 0\% & 91.20 & 0\% & 87.30 & 0\% & 95.15 & 0\% & 98.41 & 87.09 \\   & Uniform \(_{}\) & 0\% & 61.48 & 60\% & 82.49 & 0\% & 90.29 & 30\% & 86.45 & 30\% & 71.38 & 30\% & 90.90 & 75.50 \\    & Global \(_{}\) & 0\% & 61.48 & 60\% & **88.56** & 50\% & **90.93** & 30\% & **87.04** & 30\% & 57.20 & 30\% & 69.21 & 75.74 \\    & LAST & 0\% & 61.48 & 60\% & 88.52 & 50\% & 90.42 & 30\% & 86.34 & 30\% & **94.45** & 30\% & **97.95** & **86.53** \\   

Table 2: Accuracy of pruned models on LRA tasks. LAST is evaluated at the maximum tested pruning ratio with less than 1% accuracy loss, and other methods were evaluated for the same pruning ratios.

Table 10 presents that these properties remained consistent after pruning, as pruned models maintained their performance on raw speech and flexibly processed to sequences at different sampling rates by adjusting the learned timescales according to the sampling shifts, similarly to Gu et al. (2022). See Figure 8 for more results for Speech Command task.

### Pixel-level image classification

We applied pruning to tasks that classify sequenced images, including sequential MNIST (sMNIST), permuted sequential MNIST (psMNIST), and sequential CIFAR (sCIFAR), where sCIFAR is the colored version of Image task in LRA. LAST consistently exhibited the smallest accuracy loss on average. See Appendix E.1 for results on the pixel-level classification tasks.

### Analysis

#### 5.4.1 Ablation study on energy normalization

We conduct an ablation study on the energy normalization of LAST by Global \(_{}\), which is LAST without using energy normalization. LAST normalizes the differences in layer-wise signal amplification, enabling the cross-layer comparison of states on a common scale. Figure 3 shows the effect of the normalization in S5 models for Path-X task. In Layers 5 and 6, the overall \(_{}\) scores were relatively lower than other layers except Layer 1. Global \(_{}\) directly used \(_{}\) scores, resulting in excessive pruning in Layers 5 and 6 from the pruning ratio of 40%. However, LAST adjusted the scores by accounting for the low total energy transmission of the layers, making their states less prioritized in pruning. This led to different accuracy loss of methods as shown in Figure 2.

Moreover, energy normalization, which excludes pruned subsystems and normalizes accordingly, expands the range of high scores compared to normalizing without exclusions. In the case of Layer 1, this effect results in greater differences between LAST scores, making the scores distinguishable and pruning decisions easier. As a result, Layer 1 was identified to have more insignificant scores compared to other layers, leading to the removal of a large number of states. In conclusion, energy normalization was critical in the pruning process, ensuring a robust cross-layer comparison and preserving the model performance.

Figure 2: Efficiency-accuracy trade-off curves of pruned S5 models for tasks in LRA benchmark. LAST maintained accuracy better than other methods, Uniform \(_{}\) and Global \(_{}\) (LAST without energy normalization), demonstrating its superior ability to identify insignificant states.

#### 5.4.2 Compressibility of models

We considered S4D, a multi-SISO model, as the equivalent block diagonal MIMO model and applied the same pruning methods. While per-state structured pruning can completely remove state parameters, we implemented masking following the common practice in neural network pruning experiments. This approach allowed us to prune without compromising the parallelism of the multi-SISO model.

We observed that although the effective state dimension is larger in multi-SISO models, the average pruning ratio that does not result in severe accuracy loss was smaller in multi-SISO models (25%) compared to MIMO (33%) models. This is likely because, in multi-SISO, specific states are assigned to specific channels, meaning that each state is given a certain role. Consequently, pruning a single state can result in a greater loss. Additionally, this characteristic resulted in each subsystem exhibiting a significantly low \(_{}\) norm.

## 6 Discussion

Toward efficient training with guaranteed stability.Relying on guaranteed stability, previous diagonal SSMs have used as many state dimensions as possible due to the challenge of optimizing state dimensions for the task. Although excessive states are initially used, efficient and stable learning can be achieved if insignificant states are pruned under a well-planned pruning schedule.

Given this objective, we first proposed an SSM pruning method that adaptively reduces the order of multiple systems within a deep layered structure with nonlinearity. We derived a local pruning criterion considering the nonlinearity and layer-level output energy loss, applying the criterion in the Uniform \(_{}\) and Global \(_{}\) methods, which can also be viewed as independently applying traditional MOR to systems in each layer. However, we empirically verified that our method can be more robust than locally applying MOR, particularly when there are significant differences in the \(_{}\) norm scale or the proportion of important states across layers. As demonstrated in our application of the proposed method in multi-SISO models, this approach can be applied alongside parallelism.

Which states are pruned? Lessons for future work.We investigated the pruned states, which have been judged insignificant for the task, presenting some insightful observations for future work. It is known that Re\(\{_{i}\}\) controls the decay rate and Im\(\{_{i}\}\) controls the oscillating frequencies of dynamics (Gu et al., 2022; Chen, 1984). As the \(_{}\) norm is computed with these values, large\(|\{_{i}\}|\) (fast decaying mode) and large \(|\{_{i}\}|\) (high-frequency dynamics) were more prone to be pruned, as shown in Figure 4.

Based on the insignificant pole characteristics, future work might explore new training strategies for SSMs, e.g., making poles constrained to _avoid_ having the insignificant characteristics. Moreover, this provides a conjecture for the empirical effectiveness of the block-diagonal initialization in S5 (Smith et al., 2023), suggesting that the initialization performed well because it resulted in fewer large \(|\{_{i}\}|\). Even using the block-diagonal initialization, we found that previous models tend to have very large \(|\{_{i}\}|\), e.g., over 1,000, which also could be addressed in future work.

Limitations.This paper has the following limitations. Although we explored the pruning criterion for SSMs, questions about when and how often to prune SSMs remained unresolved. Additionally, our proposed method was verified on a specific set of tasks, where both mult-SISO and MIMO models have been evaluated in previous work, and the adaptation to other tasks remains to be investigated. However, we believe that our work opens opportunities to utilize the full capacity of MIMO SSMs by making them as compact as possible, not sacrificing their capacity, search space, or stability.