# SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset

Juntao Dai\({}^{123}\), Tianle Chen\({}^{1}\), Xuyao Wang\({}^{1}\), Ziran Yang\({}^{1}\)

**Taiye Chen\({}^{1}\), Jiaming Ji\({}^{12}\), Yaodong Yang\({}^{12}\)**

\({}^{1}\) Center for AI Safety and Governance, Institute for AI, Peking University

\({}^{2}\) State Key Laboratory of General Artificial Intelligence, Institute for AI, Peking University

\({}^{3}\) LLM Safety Centre, Beijing Academy of Artificial Intelligence (BAAI)

{jtd.acad, ctianle75, xuyao2w}@gmail.com,

yaodong.yang@pku.edu.cn

Corresponding author.

###### Abstract

To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SafeSora dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SafeSora dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. Our project is available at https://sites.google.com/view/safe-sora.

Warning: this paper contains example data that may be offensive or harmful.

## 1 Introduction

With advances in multi-modal technology, the capabilities of AI-powered assistants to interact with humans are expanding beyond textual communication . These assistants increasingly process and generate inputs and outputs across multiple modalities, including text [2; 3], voice [4; 5], images [6; 7; 8], and videos [9; 10; 11; 12; 13; 14]. However, the broadening capabilities of AI systems suggest that misalignment with human values could lead to increasingly severe consequences [15; 16]. Recently, Sora  demonstrated a remarkable ability to accurately interpret and execute complex human instructions, playing minute-long videos while maintaining high visual quality and compelling visual coherence. Meanwhile, applications of assistants with text-to-video capabilities are expected across various domains, including movies [17; 18], healthcare [19; 20; 21; 22], robotics [23; 24], etc. However, this also raises broader concerns about the potential misuse of such powerful capabilities . In comparison to the well-established field of text-to-text alignment, which is supported by extensive research [26; 27; 28; 3; 29], the text-to-video domain remains underdeveloped, notably lacking in available datasets.

To fill this gap, we introduce a human preference dataset, SafeSora, designed to analyze and validate human value alignment in text-to-video tasks. Considering the text-to-video task can be seen as an extension of large language model assistants, we generalize the 3H (Helpful, Harmless, Honest) standards [30; 27] to video generation. In contrast to conventional quality metrics  and harmful content detection methods [32; 33], which primarily focus on videos alone, our approach is better suited for the text-to-video task by evaluating the combination of the text prompt and generated video (T-V pair). Specifically, we assess whether the generated videos respond effectively to textual instructions and maintain safety within the context of those instructions.

To explore real human preferences, we have developed a two-stage annotation process that guides crowdworkers to interpret the concepts of helpfulness and harmlessness according to their own perceptions, rather than imposing direct definitions. Recognizing the widely reported tension between helpfulness and harmlessness [27; 3], we separate human preferences into these two distinct dimensions [3; 34; 29]. The process includes a heuristic stage for each dimension to facilitate step-by-step consideration by crowdworkers. For helpfulness, the first heuristic stage entails the annotation of preferences within four sub-dimensions, i.e., instruction following, informativeness, correctness, and aesthetics; for harmlessness, it involves a multi-label classification of 12 harm tags applicable to the T-V pair. Upon completing the initial stage, crowdworkers are prompted to provide separate preference judgments regarding helpfulness and harmlessness. This structured yet flexible annotation process helps maintain data quality while not restricting the subjectivity of the crowdworkers, thereby facilitating the analysis and modeling of real human preferences.

In summary, SafeSora has the following features:

* **First T-V Preference Dataset:** To our knowledge, SafeSora is the first dataset capturing real human preferences for text-to-video generation tasks. It comprises 14,711 unique text prompts, 57,333 T-V pairs, and 51,691 sets of multi-faceted human preference data.
* **Real Human Annotation Data:** SafeSora contains 44.54% of prompts sourced from actual users on the Internet, with the others generated through data augmentation. All data represent real feedback from crowdworkers, designed to explore their subjective perceptions and preferences.
* **Decoupled Helpfulness and Harmlessness:** SafeSora independently annotates the dimensions of helpfulness and harmlessness, thereby preventing crowdworkers from encountering conflicts between these criteria and facilitating research on how to guide this tension.
* **Multi-faceted Annotation:** SafeSora includes results from sub-dimension annotations within the two comprehensive dimensions, providing a diverse and unique perspective and enabling detailed correlation analysis.
* **Effective Dataset for Alignment:** SafeSora is validated as effective through a series of baseline experiments, including training a T-V Moderation (Section 5.1), preference models (Section 5.2) to predict human preferences for evaluating the alignment capability of large vision models; and implementing two baseline alignment algorithms by training Prompt Refiner or fine-tuning Diffusion model (Section 5.3).

## 2 Related Work

Due to space constraints, a detailed discussion of related work is provided in Appendix A.

AI-powered Text-to-Video GenerationThe development of video generation is tightly linked to advances in generative models [35; 36; 37; 38; 39; 40]. Among these, the Diffusion Model (DM) [41; 42] has emerged as a predominant approach. Innovations such as Latent Diffusion Models (LDM)  and Diffusion Transformers (DiT)  have significantly enhanced the quality of outputs and the ability of instruction following. In the field of text-to-video, numerous studies employ the latent video diffusion model (LVDM) framework , with notable implementations including ModelScope , Hotshot-XL , VideoFactory , VideoCrafter [13; 11]. Additionally, closed-sourced text-to-video services like Pika , FullJourney , and Mootion  also contribute to this area. Our dataset incorporates videos generated by a selection of these models.

Text-Video DatasetsMost datasets containing text-video pairs consist of real-world videos and their corresponding captions [50; 51; 52; 53; 54; 47; 55; 56], typically employed for pre-training text-to-video models. Certain datasets focus on videos generated by models. VidProM  gathers millions of unique prompts from real Discord users, coupled with model-generated videos. EvalCrafter  provides a small text-to-video dataset that includes human-annotated labels evaluating video quality across five dimensions. Despite these resources, there remains a significant gap in the availability of large, effective datasets for exploring human values in text-to-video tasks and aligning models accordingly. It highlights the need for the necessity of collecting SafeSora dataset.

## 3 Dataset

Our core contribution is the introduction of a real human feedback dataset for text-to-video generation tasks, called SafeSora. In this section, we detail the composition of this dataset, the collection of text prompts and videos, and the process of human annotation.

### Dataset Composition

The SafeSora dataset comprises two primary data types: classification labels for harm categories and preference for helpfulness or harmlessness. Inspired by methodologies in the text alignment domain, we capture preference data through paired comparisons of videos generated from identical text prompts. Notably, both types of data incorporate human feedback on the combination of text prompts and corresponding generated videos, rather than solely on the video content. Consequently, our approach more accurately reflects real-world applications of text-to-video tasks in large models, recognizing that a video might seem harmless independently but harmful in the context of its prompt. Due to space constraints, we give the corresponding examples in Appendix B.1. For future reference, we define a **T-V pair** as the combination of a user prompt and its corresponding generated video.

Here, we present the **Data Card** for SafeSora:

* SafeSora comprises 14,711 unique text prompts, of which 44.54% are real user prompts for text-to-video models online, and 55.46% manually constructed by our team. Among these, 48.61% may potentially induce harmful videos, whereas 51.39% are neutral.

Figure 1: Proportion of multi-label classifications for Prompt (**Left**) and T-V Pairs (**Right**).

Figure 2: Example data point from the SafeSora dataset

* Among all prompts, 29.13% generated 3 unique videos, and 28.39% generated no less than 5 unique videos. 42.30% of the videos were enhanced using LLMs before being used for video generation to enhance user prompts for better generation quality.
* For a total of 57,333 T-V pairs, we annotated 12 potential harm categories, of which 76.29% are assigned as safe and 23.71% are categorized with at least one harm label.
* SafeSora includes 51,691 human preference annotations, structured as paired comparisons between T-V pairs. Preference is decoupled into two dimensions: helpfulness and harmlessness.

Figure 1 presents a visualization of the proportion of multi-label classification for prompt and T-V pairs within the SafeSora dataset.

### Prompt Collection and Video Generation

The SafeSora dataset comprises prompts derived from two primary sources: actual user interactions with text-to-video generation models online and those formulated by our researchers. There are a total of 6,552 _real user prompts_, with 5,203 legally scraped from 4 video generation channels on Discord over the past year, and 1349 harmful prompts from the open-source text-to-video web scraping dataset VidProM . Additionally, the dataset contains 8,159 _researcher-constructed prompts_, formulated either by rewriting existing text-to-image prompts or by generating new prompts around specific themes. This effort aims to enhance the balance across various categories. After collecting the prompt set, we employ GPT-4  for a preliminary classification to identify prompts potentially leading to harmful video content, and to exclude some meaningless prompts. The average word count (using the regex /*+/) for each prompt in our dataset is 27.07. Details on prompt crawling, generating, and filtering are provided in Appendix C.

In practical applications, users typically lack the expertise to formulate text instructions of sufficient detail for video generation, necessitating a prompt augmentation module in the frameworks [12; 9]. Our analysis of collected _real user prompts_ also reveals that many are inadequate for direct use in text-to-video models. Consider the prompt "Generate a war video," which specifies a theme yet omits essential details such as scenes, characters, and dynamics, leading to videos of inferior quality. As shown in Figure 3, our dataset includes both the direct use of original user instructions for video generation and the utilization of LLMs (such as GPT-4 and Llama) as a prompt refiner. We then prompt the video generation model to generate several unique videos for each text prompt. The models employed in this work include closed-source models such as Pika, FullJourney, and Mootion, alongside open-source models including VideoCrafter2  and Hotshot-XL .

### Two-Stage Human Annotation

Similar to the challenges in LLM alignment [3; 27; 59; 29], the conflicting demands of helpfulness and harmlessness are also prevalent in text-to-video generation. Thus, we decouple these dimensions into parallel objectives during the annotation process. This separation aims to mitigate the confusion of crowdworkers (annotators) caused by conflicts of these dimensions and provide distinct perspectives. We introduce a two-stage heuristic annotation process designed to direct the focus of the crowdworkers, thereby enhancing the quality of annotations. As illustrated in Figure 3, the process encompasses two decoupled dimensions and two stages of annotation:

Helpfulness-related AnnotationAnnotating which of two generated videos from the same text prompt is more helpful still constitutes a complex and comprehensive task. In the first heuristic stage, we empirically guide crowdworkers to focus on 4 sub-dimensions of preference:

* **Instruction Following:** Assessing whether the generated video content meets the requirements of the text instruction, such as the particular objects, actions, and styles. When the instructions are phrased as questions, the video should directly address these queries.
* **Correctness:** Evaluate whether objects in the video have correct shapes and movements. Unless explicitly stated in the instructions, objects in the video should have attributes such as shape, color, and size that align with natural facts. Their movements should follow physical laws, such as gravity and conservation of momentum.
* **Informativeness:** Videos, due to the presence of their temporal dimension, are expected to offer more information than static images. A high-quality video should demonstrate dynamic interactions and movements among objects, rather than merely panning across a static scene.

* **Aesthetics:** Subjectively assessment of which video is visually superior, considering general public or personal aesthetic criteria.

After completing the annotations for the above four sub-dimensions, crowdworkers are requested to provide an overall preference for helpfulness. Notably, the first stage serves merely as a guiding process; we do not assign priorities to these four sub-dimensions, instead allowing the crowdworkers to express subjective judgment.

Harmfulness-related AnnotationIn parallel with the helpfulness-related annotations, the harmlessness-related annotations begin with a heuristic guiding stage. In this phase, crowdworkers assess whether each T-V pair exhibits any of the 12 predefined harm tags, constituting a multi-label classification task. Given the absence of prior research within the text-to-video generation, we refer to traditional film2 and media3 classification schemes. We define 12 harm categories:

* S1: Adult, Explicit Sexual Content
* S2: Animal Abuse
* S3: Child Abuse
* S4: Crime
* S5: Debated Sensitive Social Issue
* S6: Drug, Weapons, Substance Abuse
* S7: Insulting, Hateful, Aggressive Behavior
* S8: Violence, Injury, Gory Content
* S9: Racial Discrimination
* S10: Other Discrimination (Excluding Racial)
* S11: Terrorism, Organized Crime
* S12: Other harmful Content

Unlike rule-based or model-based annotation methods, the harm labels derived from human feedback are subject to variability due to cultural differences, education levels, and other factors across diverse groups. Therefore, the harm labels we collect inherently contain a degree of subjectivity and are primarily intended to guide crowdworkers toward establishing a final preference for harmlessness. Upon completing the multi-label classification, crowdworkers are required to identify which of the two T-V pairs is safer.

Quality ControlIn addition to the full-time annotation team, our annotation results undergo a secondary evaluation by a professional quality control department. This department maintains regular communication with our research team to ensure alignment. Furthermore, our researchers spot-check

Figure 3: **Left - Video generation pipeline: Both the original and augmented prompts are then used to generate multiple videos using five video generation models to form T-V pairs. Right - Two-stage annotation: The annotation process is structured into two distinct dimensions and two sequential stages. In the initial heuristic stage, crowdworkers are guided to annotate 4 sub-dimensions of helpfulness and 12 sub-categories of harmlessness. In the subsequent stage, they provide their decoupled preference upon two T-V pairs based on the dimensions of helpfulness and harmlessness.**

20% of the batch. While our project explores subjective preferences within human values, the primary goal of this dual quality control is to mitigate unreliable annotation noise.

Further details on the human annotation process, including the annotation documents provided to crowdworkers, are available in Appendix D.

Data StructureEach data point includes a UUID, a user prompt, two generated videos, the actual input related to each video (either the original prompt or a refined one), the annotations of 12 harm labels for each video, 4 preferences on sub-dimensions of helpfulness, and decoupled preferences on helpfulness or harmlessness. For some visual examples of data points, see Appendix B.

## 4 Analysis

Since the SafeSora dataset provides _real_ human preference data across _multiple_ dimensions, it is meaningful to analyze the correlations among various dimensions and compare human feedback with AI feedback in this section.

### Correlation Analysis

The SafeSora dataset comprises annotations derived from different perspectives and in various forms. We conduct an in-depth analysis of the relationships between these results:

**Harm labels within T-V pairs** The correlations between the harm types labeled for T-V pairs are shown in Figures 4. Due to the space limitation, we put the correlations between the potentially harm types of prompts in Appendix E. Our analysis yields two key findings: first, there is no high correlation among different types (all below 0.5), confirming the distinctiveness of the categories we established. Second, correlations for harm types in T-V pairs are weaker than those observed for potentially harmful prompts. Further investigation into a subset of video generation outcomes and discussions with the annotation team led to two possible explanations for this phenomenon. First, the limited capability of the current large vision model, particularly in following instructions, might lead to the omission of certain harm types during the transition from text to video modalities. Second, during the initial labeling phase, which serves as heuristic guidance, crowdworkers may discontinue identifying certain ambiguous labels once the most suitable label has been applied.

**Harm labels and harmlessness preferences** For samples exhibiting a logical contradiction between the harm classification and the harmlessness preference--specifically, harmful T-V pairs (tagged with at least one harm label) being preferred for harmlessness compared to harmless T-V pairs (without any harm labels)--we consider these samples as noise and mark them as invalid.

**Sub and overall preference of helpfulness** Figure 5 illustrates the relationship between the four sub-preferences and the overall preference for helpfulness. We observe two noteworthy findings: first, in the absence of explicit requirements, crowdworkers prioritize the criterion of the instruction following--which of the generated videos better adheres to the text instruc

Figure 4: Linear correlation coefficient between labels of T-V pairs assigned by crowdworkers to 12 harm categories, identified as S1 through S12.

Figure 5: Linear correlation coefficient of different preference annotations.

tions--as the most significant determinant of helpfulness (with a correlation as high as 0.85).

Another observation is that the informativeness sub-dimension exhibits a low correlation with other sub-dimensions and even demonstrates contradictions. One possible explanation for this finding is that enhancing the information content generally increases the video's complexity and duration. Given the limited capabilities of current large vision models, this enhancement may adversely affect the performance of the other three sub-dimensions.

Tension between helpfulness and harmfulnessThis conflicting relationship of helpfulness and harmfulness is widely reported in the alignment of LLMs [27; 60], and our findings with SafeSora confirm its presence in text-to-video generation. We found that 53.39% of the helpfulness preferences among our potentially harmful prompts contradict the harmlessness preferences. Thus, developing strategies to mitigate this tension is a crucial part of alignment research in text-to-video tasks.

### Human Feedback vs. AI Feedback

Human-labeled data incurs significant costs, which motivates the investigation into the potential of multi-modal visual LLMs as alternatives in preference labeling tasks. We developed a pipeline utilizing these multi-modal models to assess preferences and conducted a comparative analysis with the human feedback from our SafeSora dataset. Due to the lack of efficient multi-modal large models capable of processing video input, our AI feedback pipeline is confined to comparing \(m\) frames extracted from two videos using GPT-4o . The evaluation prompts are in Appendix E.

Figure 6 illustrates the agreement between the annotations of GPT-4o and crowdworkers within the evaluation set. Observations indicate a low agreement in preference assessments, both for sub-dimensional preferences in Figure 6(1) and overall preferences in Figure 6(3). Furthermore, as the number of comparison frames (\(m\)) increases, the level of agreement tends to random results (\(0.5\)). Sub-dimensions that entail timeline-related judgments, such as Informativeness, exhibit lower levels of agreement. This outcome partially demonstrates that the general multi-modal LLM, GPT-4o, when based on image input comparisons, faces challenges in achieving consensus with humans on preference labeling tasks. The limit on the number of image inputs (\( 10\)) restricts its perspective and the use of tricks like the few-shot. On the other hand, as shown in Figure 6(2), GPT-4o shows a high agreement with crowdworkers in assessing the harm labels of the T-V pairs. This higher agreement rate may stem from the fact that most of judgment tasks are resolvable using a single video frame.

Therefore, before further validation of AI feedback's effectiveness, we maintain a conservative point that it is currently challenging to replace human annotation.

## 5 Inspiring Future Research

SafeSora could serve as a foundation for research on aligning human values within text-to-video generation tasks, thereby inspiring new research directions. From our perspective, potential future work in the AI-powered video generation field includes:

* **Modeling human values:** Modeled human preferences can be used to evaluate or supervise large vision models. However, Real human data exhibit diversity due to individual or group differences,

Figure 6: Agreement between GPT-4o and crowdworkers upon preferences and safety Labels. Conservatively, the potential for general multi-modal LLMs to replace human annotators in preference labeling tasks remains limited.

and may contain unstable noise. Therefore, modeling human preferences and generalizing them to a larger scope can be a complex task.
* **Aligning human values:** How to construct alignment algorithms that efficiently utilize the real human data provided by the SafeSora dataset and how to guide the tension between different dimensions remain an open question in the text-to-video field.

In this section, we present some basic baseline algorithms of the above directions as application examples of the SafeSora dataset, which also demonstrate the effectiveness of the data. The detailed experimental settings are provided in Appendix F.

### T-V Moderation and Safety Evaluation of Different Models

Similar to LlamaGuard  and QA-Moderation  in the LLM domain, we develop an input-output safeguard named T-V Moderation, which is fine-tuned from a multi-modal LLM called Video-LLaVA . Unlike traditional video content moderation [32; 33] focusing video alone, T-V Moderation incorporates user text inputs as criteria for evaluation, allowing it to filter out more potentially harmful multi-modal responses. The agreement ratio between T-V Moderation trained on the multi-label data of the SafeSora training dataset and human judgment on the test set is 82.94%. Figure 7(1) shows our evaluation of four open-source large vision models with 300 red-team prompts constructed for 12 harm categories. The evaluation data comes from our trained T-V Moderation and human feedback (crowdworker team). We observed that these open-source models actively respond to harmful prompts, and most of the harmless videos generated are due to the inability to follow instructions well. In Figure 7(1), although VC1 (VideoCrafter1) produces fewer harmful T-V pairs compared to VC2 (VideoCrafter2), our direct observation of the generated videos suggests that this is primarily due to VC1's reduced generation capability. Specifically, the videos generated by VC1 fail to align with the provided safety-critical prompts, yet they are classified as safe.

### Preference Modeling and Alignment Evaluation of Different Models

A common method for modeling human preferences is to use a preference predictor adhering to the Bradley-Terry Model . The preference data is symbolized as \(y_{w} y_{l}|x\) where \(y_{w}\) denotes the more preferred video than \(y_{l}\) corresponding to the prompt \(x\). The log-likelihood loss used to train a parameterized predictor \(R_{}\) on dataset \(\) is \((;)=-_{(x,y_{w},y_{l})} [(R_{}(y_{w},x)-R_{}(y_{l},x))]\).

Our dataset encompasses annotations across multiple preference dimensions, leading us to develop two distinct models: a reward model focused on helpfulness, and a cost model focused on harmlessness. The agreement ratio with crowdworkers is 65.29% for the reward model and 72.41% for the cost model. These figures are consistent with human agreement ratios reported in similar studies on modeling human preferences  in the LLM domain. Figure 7(2)(3) shows the win-rate relationships for four open-source models on our evaluation dataset, assessed across the two alignment dimensions evaluated by our reward and cost models. Among the evaluated models, the VideoCrafter2 (VC2)

Figure 7: The evaluation results of four video generation models using T-V Moderation (1), Reward Model (2), and Cost Model (3). The evaluated checkpoints of models are HotShot-XL (HSXL) , TF-ModelScope (MS) , VideoCrafter1 (VC1) , and VideoCrafter2 (VC2) .

 model exhibits higher helpfulness but reduced harmlessness. The exact opposite is ModelScope (MS) . The results aptly reflect the tension between helpfulness and harmlessness.

### Fine-tuning Refiner and Diffusion Model using the Best-of-N method

Building on the previously trained reward model and cost model, we develop two basic alignment algorithms, as shown in Figure 8. The two foundational algorithms are based on the Best-of-N (BoN) fine-tuning approach . Specifically, each training iteration begins by generating multiple outputs from the trained model. These outputs are then evaluated and ranked according to the preference model, which selects the most optimal result. This selected output serves as the supervisory signal for further fine-tuning of the model. As illustrated in Figure 8, these algorithms aim to align human values in text-to-video generation through the prompt refiner and the diffusion model, respectively. The scoring for ranking the results incorporates a weighted sum of the outputs from the reward and cost models. The distribution of the generated videos has an obvious shift in the helpfulness dimension, whereas the shifts in the harmlessness dimension are not pronounced.

Hard to RejectDuring fine-tuning the diffusion model, defining a refusal response, and training a model to refuse certain inputs presents significant challenges. This characteristic further sharpens the conflict between helpfulness and harmlessness in the alignment of LVMs compared to LLMs. Unlike LLMs that can reject inappropriate requests and provide helpful explanations or warnings, video generation models often fail to stay both helpful and harmless when given harmful prompts.

## 6 Discussion

The text-to-video model, as an extension of the capabilities of AI-powered assistants, is gradually expanding its interaction opportunities and scope with human users. In the past, research primarily focused on improving the quality of the generated videos since the model's capabilities were not yet sufficient to support human value alignment. However, due to the milestone advancements in text-to-video generation brought by Sora , especially its convincingly realistic video quality and remarkable instruction-following ability, we realize the necessity of undertaking alignment research.

Figure 8: Pipelines of fine-tuning refiner and diffusion model using the Best-of-N method and the distribution shift of model outputs before and after training

Given the current lack of datasets for text-to-video tasks, we hope that SafeSora can fill this gap to serve as part of the foundation for alignment research.

### Ethics and Impact

Fair UseThe SafeSora dataset is available under the **CC BY-NC 4.0** license. Since SafeSora contains a large amount of data from real humans, including multi-label classification data for harm categories and preference data from multiple perspectives, it has great potential as a resource for analyzing and modeling human value in specific domains, as well as for researching and validating how to develop helpful and harmless AI assistants. Given the individual and group differences in human preferences, we conservatively recommend that the SafeSora data be used only for research-related tasks until the recognition scope of human values represented by the data is verified. Further discussion regarding fair wages and the Institutional Review Board (IRB) can be found in Appendix C.

Potential Negative Societal Impacts of DatasetIn theory, the same data also indicates how to train a harmful assistant that violates human preferences. On the other hand, the value discrepancies among different groups may also pose potential risks. Since multi-modal data has a greater impact than pure text data, we believe it is necessary to discuss whether to review the acquisition of safety-related parts of the data, such as using Hugging Face's gated dataset settings. We strongly condemn any malicious use of the SafeSora dataset and advocate for responsible and ethical use.

### Limitations and Future Work

Firstly, although SafeSora contains a large number of real user instructions and researcher-constructed prompts, it is impossible to cover all scenarios. We cannot predict how people will use LVM, nor can we predict how this technology may be misused, so the prompts in the dataset should be expanded over time. Secondly, the baseline algorithms provided in our paper are merely used to validate the data's effectiveness but are not sufficiently efficient as alignment algorithms. Therefore, researching how to more efficiently utilize the data in SafeSora and developing better multimodal alignment algorithms will be a focus of future work. Finally, due to the diversity of human values, ensuring value alignment of the model should not be merely a technical issue. Therefore, interdisciplinary collaboration is necessary.