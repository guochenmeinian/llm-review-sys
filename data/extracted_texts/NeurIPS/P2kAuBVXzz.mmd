# It's All Relative: Relative Uncertainty in Latent Spaces using Relative Representations

Fabian Martin Mager

Technical University of Denmark

fmager@dtu.dk

&Valentino Maiorca

Sapienza University of Rome, Italy

maiorca@di.uniroma1.it

&Lars Kai Hansen

Technical University of Denmark

lkai@dtu.dk

###### Abstract

Many explainable artificial intelligence (XAI) methods investigate the embedding space of a given neural network. Uncertainty quantification in these spaces can lead to a better understanding of the mechanisms learned by the given network. When concerned with the uncertainty of functions in latent spaces we can invoke ensembles of trained models. Such ensembles can be confounded by reparameterization, i.e., lack of identifiability. We consider two mechanisms for reducing reparametrization "noise", one based on relative representations and one based on interpolation in weight space. By sampling embedding spaces along a curve connecting two fully converged networks without an increase in loss, we show that the latent uncertainty becomes overestimated when comparing embedding spaces without considering the reparametrization issue. By changing the absolute embedding space to a space of relative proximity, we show that the spaces become aligned, and the measured uncertainty decreases. Using this method, we show that the most non-trivial changes to the latent space occur around the midpoint of the curve connecting the independently trained networks.

## 1 Introduction

A neural network (NN) trained to perform a certain downstream task, e.g., regression or classification, learns a mapping \(f:_{i}_{i}\) given a set of observations \(=\{(_{i},_{i})\}_{i}^{N}\), where \(_{i}=[x_{1},x_{2},\...,x_{D}]\) and \(N\) is the number of available observations and \(D\) is the latent space dimension. The learned function \(f\) is hierarchical, i.e. \(f()=h_{L}(h_{L-1}(...h_{1}()))\) where the intermediate representation of the observation \(_{i}\) after layer \(l\) is denoted as \(_{i}\). We note the mapping of the observation \(_{i}\) to its latent representation \(_{i}\) as \(h_{l}:_{i}_{i}\). The latent space is assumed to be semantically meaningful for relevant downstream tasks. Therefore, many XAI methods investigate the properties of the latent space of a NN, for example, its geometry  or the semantic structure . XAI has many dimensions , one of which is referred to as _local_ vs. _global_. Local explainability concerns predictions of individual samples, whereas global explainability considers the network as a whole. Most local methods give attributions to the input features \([x_{1},x_{2},...,x_{D}]\), which are important to the output \(}\). Recently, Wickstrom et al.  proposed a method that maps attributions to input features based on the importance of its latent vector \(\). In contrast, global explainability methods suchas _Concept Activation Vectors_ (TCAV)  aim to explain the entire latent space based on semantics. One persistent challenge in assessing geometry and semantic structure of the latent space is the _reparametrization issue_, driven by the fact that there is no "set of optimal parameters and that we can always parametrize the manifold in a different, but equally good, way" (). In other words, while the end-to-end function \(f\) remains the same, different parametrizations lead to different but equally performing embedding functions \(h\).

Moschella et al.  introduced Relative Representations, which define an alternative representation of the latent space. They empirically show that the latent spaces of two identical models trained on the same data but different initializations are identical up to angle-preserving transformations. Their proposed method uses a set of latent vectors \(=\{_{i}\}_{i}^{A}\), called _anchors_, and redefines the position of each latent vector to be _relative_ to the set of anchors, according to a similarity function \(sim:^{D}^{D}\). Depending on the choice of the similarity function, the latent space becomes invariant to certain transformations.

The reparametrization issue is also related to the loss landscape in NNs. Several works have exploited the reparametrization issue to efficiently train ensembles of models, which outperform their single-model counterparts . Garipov et al.  investigated the geometrical properties of loss landscapes and showed that two identical NNs trained on different seeds can be connected by a simple curve in weight space, such that the loss under the curve is constant. This curve \(\) parameterized by \(\) connects two points \(_{1}\) and \(}\) in parameter space and is found by minimizing the loss \(l()=_{0}^{1}((t)),dt=_{t U(0,1)}[ (_{}(t))]]\), where \(\) is the loss function used to find \(_{1}\) and \(}\). We refer to section B for details on the curve-finding procedure. Once \(\) is fitted, one can sample from \(_{}(t))\) for \(0 t 1\) and build an ensemble of models from living on the curve \(_{}(t)\).

Ensembling methods reduce both the bias and variance of \(}\). The uncertainty over \(K\) samples of the end-to-end function \(f\) in the prediction of \(_{i}\) is given by

\[^{2}(}_{i})=(_{k K}[( f_{k}(_{i})-}_{i})(f_{k}( _{i})-}_{i})^{T}])\] (1)

where \(}_{i}=_{k K}[f_{k}(_{i})]\). Following the above reasoning, the embedding function's uncertainty \(h\) could be obtained by replacing \(f_{k}\) with \(h_{k}\) and \(_{i}\) with \(_{i}\). Due to reparametrization however, the latent uncertainty \(^{2}(_{i})\) will be overestimated (see Appx. A). We argue that there are two factors contributing to the overall observed uncertainty \(^{2}_{}(_{i})\),

\[^{2}_{}(_{i})=^{2}_{}(_ {i})+^{2}_{}(_{i})\,\] (2)

where \(^{2}_{}\) refers to the uncertainty caused by reparametrization and \(^{2}_{}\) to the uncertainty of the model. In equation 2 we assume independence of the model and reparametrization uncertainty. Such independence could arise as a consequence of initialization being independent of the data. In order to isolate the model uncertainty \(^{2}_{}(_{i})\), the reparametrization uncertainty \(^{2}_{}(_{i}) 0\). In Appx. A we show that, for rotation and scaling transformations, the Relative Representation framework, using a cosine similarity function, can eliminate \(^{2}_{}(_{i})\). In this work, we

* Define an alignment score \(\), which estimates the signal-to-noise ratio for a given ensemble of latent observations. The alignment score \(\) is a measure of separability between \(N\) latent points given \(K\) samples each, similar to Fisher's Linear Discriminant objective for \(N\) classes.
* Show empirically that when sampling models along the curve \(\), transforming the embeddings into a space of relative proximity increases alignment between latent observations.
* Show that the latent observations for the curve \((t)\) have high alignment around the endpoints and little alignment at the bending point, suggesting limited information gain when sampling around the endpoints and high information gain when sampling across the midpoint.

## 2 Methods

The intuition behind our alignment metric is the following: Given \(K\) samples of a latent vector \(_{i}\), where each sample is an embedding of a given observation \(_{i}\) obtained from a unique function \(h_{k}\), the alignment score \(_{i}\) should be high if all samples form a compact region in the latent space and are well separated from all other points (see Figure 1). The alignment score \(\) of the entire embedding space can be estimated as the average of individual scores. This formulation is similar to Fisher's Linear Discriminant (FLD) objective, which measures the separability of two or more classes of objects. The FLD objective is given by the ratio of between-class vs. within-class covariance. Given \(K\) realizations of latent vector \(_{i}\), the objective can be applied on the scale of a single observation, i.e. the ratio of between-observation vs. within-observation covariance. Due to the high dimensionality of \(\) and the few number of available samples, it is not feasible to calculate the full covariance matrix. We therefore use the variance estimator and express the ratio as a signal-to-noise ratio, such that the alignment metric \(\) is bounded \(0 1\). We define the within-observation variance \(_{W}^{2}\) and between-observation variance \(_{B}^{2}\) of \(k\) realizations of the latent space and their alignment \(\) as

\[_{W}^{2} =_{i=1}^{N}_{k=1}^{K}(_{ik}-}_{i})^{2}\] (3) \[_{B}^{2} =_{i=1}^{N}(}_{i}-(}_{i}))^{2}\] (4) \[ =^{2}}{_{B}^{2}+_{W}^{2}}\] (5)

We conduct the following experiment independently using a VGG-16  and a Preactivation-ResNet  on CIFAR-100 . First, fully train on three different seeds until convergence, resulting in three unique models \(_{1}\), \(_{2}\), and \(_{3}\). For each pairwise combination of seeds, we fit a Bezier curve with fixed endpoints and a single bend following the procedure described in . Once a curve \(_{ij}(t)\) between two modes \(_{i}\), \(_{j}\) is found, we sample \(\) models at steps \(t=k* t\), where \(k[0,\...,\ K-1]\) and \( t=\). For all experiments, we set \(K=21\). We proceed by evaluating the performance at each step \(t\), as well as the ensemble performance over all models up to step \(t\) using a hold-out test set \(_{test}=(x_{i},y_{i})_{i}^{N}\). We investigate the alignment of the embedding spaces along the curve, i.e. the last layer before the classification layer. We define the absolute embedding space \(_{k}\) as the ensemble of all latent vectors \(\{_{i}\}\) in \(_{test}\) and the space of relative proximity \(_{k}\) as the ensemble of all transformed vectors \(\{_{i}\}\) given a set of anchors \(\) and a similarity function \(sim:^{D}^{D}\). The number of anchors is chosen to match the number of dimensions \(D\) of the absolute embeddings. For the VGG16, \(D=512\), and for the ResNet110, \(D=256\). Anchors are sampled randomly from \(_{test}\), following the procedure of . Table 1 provides an overview of the proposed similarity functions. For the cosine and basis transformations, we center the embedding space \(_{k}\) before calculating similarities using an estimate of the mean based on \(_{train}\). For the Euclidean transformation, the space is additionally scaled to unit variance. Finally, using the proposed metric, we measure the cumulative alignment for increasing \(k\) of both the absolute and relative

Figure 1: _Left_: Conceptual visualization of the alignment measure. Given the two observations \(_{i=1}\) and \(_{i=2}\) and \(K\) realizations of their embeddings \(_{i=1}\) and \(_{i=2}\) obtained from a unique embedding function \(h_{k}\), we seek small within-observation variance \(_{W}\) relative to the between-observation variance \(_{B}\). _Right_: Visualization of uncertainties in latent space using the ratio of within vs. between sample variance. The plots show two identical t-SNE projections of the VGG-16 model and 10 classes of CIFAR100. The dot size represents uncertainty measured in absolute space (left) and relative space (right) using a cosine similarity projection.

embedding spaces. We compare the observed alignment with the alignment of eleven independently trained networks. Code is available here1

## 3 Results

In Figure 2 we show the individual and cumulative ensemble error rates for each model type and curve. For all models and curves, the test accuracy increases when approaching the midpoint. The cumulative ensemble error decreases with increasing ensemble size. Notably, there is little variation across the pairwise curves. The embeddings of relative representations show higher alignment than the embeddings of absolute representations. This shows that the embedding spaces are confounded by reparametrization. However, the results vary across projection methods. For the VGG, the cosine similarity performs best, whereas for the ResNet it is Euclidean distance. Alignment measured in absolute space decreases almost linearly. The alignment measured in relative space, however, seems to converge. Compared to the baseline experiment, where alignment is measured across eleven independently trained networks, the measured alignments in relative space are closer than those measured in absolute space. Furthermore, the figure shows that the highest negative slope occurs around \(t=0.5\), whereas the slope has limited variation around the endpoints. This finding suggests that ensembles used for uncertainty quantification should be samples far from the endpoints of a connecting curve.

Figure 2: _Top_: Curve fitting results (solid) for VGG (red) and PreResNet (blue), pairwise connecting three modes with individual (left) and cumulative ensemble error rates (right). The individual error rate is the classification error rate for CIFAR100 for a given set of parameters \(t\) on the curve. The cumulative error rate is determined by the arg max of the sum of probabilities across all sampled parameters on the curve in \([0,t]\). _Bottom_: Alignment of embedding spaces along the curve for the absolute (green) and relative (purple) space. Dashed lines represent reference values, given as the average error and alignment of eleven independently trained networks.

Discussion

In this work, we investigate the alignment of latent observations when sampling along a curve connecting two modes. Our results show that transforming the spaces into a space of relative proximity reduces uncertainty significantly. This suggests that the uncertainty of ensembles is indeed confounded by reparametrization. The measured uncertainty varies across different relative projections within each modality. This is a limitation, as no general recommendation for a projection method can be made. Each projection will make the latent space invariant to some transformation, and the optimal choice of projection is model-dependent. Certain projections might bias uncertainty estimates. For example, the unit-norm scaling of the cosine similarity measure will increase uncertainties close to the zero vector. In this work, we used a random set of anchors, which eases the workflow of sampling from the curve. However the choice of anchors, e.g. archetypes or centroids, will likely influence the projection quality. The influence of projection function and anchor choice should be further investigated.