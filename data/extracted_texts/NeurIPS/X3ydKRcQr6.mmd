# MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings

Laxman Dhulipala

Google Research and UMD

&Majid Hadian

Google DeepMind

&Rajesh Jayaram

Google Research

&Jason Lee

Google Research

&Vahab Mirrokni

Google Research

Corresponding Author: rkjayaram@google.com

###### Abstract

Neural embedding models have become a fundamental component of modern information retrieval (IR) pipelines. These models produce a single embedding \(x^{d}\) per data-point, allowing for fast retrieval via highly optimized maximum inner product search (MIPS) algorithms. Recently, beginning with the landmark ColBERT paper, _multi-vector models_, which produce a set of embedding per data-point, have achieved markedly superior performance for IR tasks. Unfortunately, using these models for IR is computationally expensive due to the increased complexity of multi-vector retrieval and scoring.

In this paper, we introduce Muvera (**M**ulti-**V**ector **R**etrieval **A**lgorithm), a retrieval mechanism which reduces _multi-vector_ similarity search to _single-vector_ similarity search. This enables the usage of off-the-shelf MIPS solvers for multi-vector retrieval. Muvera asymmetrically generates _Fixed Dimensional Encodings_ (FDEs) of queries and documents, which are vectors whose inner product approximates multi-vector similarity. We prove that FDEs give high-quality \(\)-approximations, thus providing the first single-vector proxy for multi-vector similarity with theoretical guarantees. Empirically, we find that FDEs achieve the same recall as prior state-of-the-art heuristics while retrieving 2-5\(\) fewer candidates. Compared to prior state of the art implementations, Muvera achieves consistently good end-to-end recall and latency across a diverse set of the BEIR retrieval datasets, achieving an average of 10\(\%\) improved recall with \(90\%\) lower latency.

## 1 Introduction

Over the past decade, the use of neural embeddings for representing data has become a central tool for information retrieval (IR) , among many other tasks such as clustering and classification . Recently, _multi-vector_ (MV) representations, introduced by the _late-interaction_ framework in ColBERT , have been shown to deliver significantly improved performance on popular IR benchmarks. ColBERT and its variants  produce _multiple_ embeddings per query or document by generating one embedding per token. The query-document similarity is then scored via the _Chamfer Similarity_ (SS1.1), also known as the MaxSim operation, between the two sets of vectors. These multi-vector representations have many advantages over single-vector (SV) representations, such as better interpretability  and generalization .

Despite these advantages, multi-vector retrieval is inherently more expensive than single-vector retrieval. Firstly, producing one embedding per token increases the number of embeddings in a dataset by orders of magnitude. Moreover, due to the non-linear Chamfer similarity scoring, there is a lack of optimized systems for multi-vector retrieval. Specifically, single-vector retrieval isgenerally accomplished via Maximum Inner Product Search (MIPS) algorithms, which have been highly-optimized over the past few decades . However, SV MIPS alone cannot be used for MV retrieval. This is because the MV similarity is the _sum_ of the SV similarities of each embedding in a query to the nearest embedding in a document. Thus, a document containing a token with high similarity to a single query token may not be very similar to the query overall. Thus, in an effort to close the gap between SV and MV retrieval, there has been considerable work in recent years to design custom MV retrieval algorithms with improved efficiency .

The most prominent approach to MV retrieval is to employ a multi-stage pipeline beginning with single-vector MIPS. The basic version of this approach is as follows: in the initial stage, the most similar document tokens are found for each of the query tokens using SV MIPS. Then the corresponding documents containing these tokens are gathered together and rescored with the original Chamfer similarity. We refer to this method as the _single-vector heuristic_. ColBERTv2  and its optimized retrieval engine PLAID  are based on this approach, with the addition of several intermediate stages of pruning. In particular, PLAID employs a complex _four_-stage retrieval and pruning process to gradually reduce the number of final candidates to be scored (Figure 1). Unfortunately, as described above, employing SV MIPS on individual query embeddings can fail to find the true MV nearest neighbors. Additionally, this process is expensive, since it requires querying a significantly larger MIPS index for _every_ query embedding (larger because there are multiple embeddings per document). Finally, these multi-stage pipelines are complex and highly sensitive to parameter setting, as recently demonstrated in a reproducibility study , making them difficult to tune. To address these challenges and bridge the gap between single and multi-vector retrieval, in this paper we seek to design faster and simplified MV retrieval algorithms.

Contributions.We propose Muvera: a multi-vector retrieval mechanism based on a light-weight and provably correct reduction to single-vector MIPS. Muvera employs a fast, data-oblivious transformation from a set of vectors to a single vector, allowing for retrieval via highly-optimized MIPS solvers before a single stage of re-ranking. Specifically, Muvera transforms query and document MV sets \(Q,P^{d}\) into single fixed-dimensional vectors \(,\), called _Fixed Dimensional Encodings_ (FDEs), such that the the dot product \(\) approximates the multi-vector similarity between \(Q,P\) (SS2). Empirically, we show that retrieving with respect to the FDE dot product significantly outperforms the single vector heuristic at recovering the Chamfer nearest neighbors (SS3.1). For instance, on MS MARCO, our FDEs Recall\(@N\) surpasses the Recall\(@2\)-5N achieved by the SV heuristic while scanning a similar total number of floats in the search.

We prove in (SS2.1) that our FDEs have strong approximation guarantees; specifically, the FDE dot product gives an \(\)-approximation to the true MV similarity. This gives the first algorithm with provable guarantees for Chamfer similarity search with strictly faster than brute-force runtime (Theorem 2.2). Thus, Muvera provides the first principled method for MV retrieval via a SV proxy.

We compare the end-to-end retrieval performance of Muvera to PLAID on several of the BEIR IR datasets, including the well-studied MS MARCO dataset. We find Muvera to be a robust and efficient retrieval mechanism; across the datasets we evaluated, Muvera obtains an average of 10% higher recall, while requiring 90% lower latency on average compared with PLAID. Additionally, Muvera crucially incorporates a vector compression technique called _product quantization_ that enables us to compress the FDEs by 32\(\) (i.e., storing 10240 dimensional FDEs using 1280 bytes) while incurring negligible quality loss, resulting in a significantly smaller memory footprint.

Figure 1: Muvera’s two-step retrieval process, compared to PLAID’s multi-stage retrieval process. Diagram on the right from Santhanam et. al.  with permission.

### Chamfer Similarity and the Multi-Vector Retrieval Problem

Given two sets of vectors \(Q,P^{d}\), the _Chamfer Similarity_ is given by

\[(Q,P)=_{q Q}_{P P} q,p\]

where \(,\) is the standard vector inner product. Chamfer similarity is the default method of MV similarity used in the _late-interaction_ architecture of ColBERT, which includes systems like ColBERTv2 , Baleen , Hindsight , DrDecr , and XTR , among many others. These models encode queries and documents as sets \(Q,P^{d}\) (respectively), where the query-document similarity is given by \((Q,P)\). We note that Chamfer Similarity (and its distance variant) itself has a long history of study in the computer vision (e.g., ) and graphics  communities, and had been previously used in the ML literature to compare sets of embeddings . In these works, Chamfer is also referred to as _MaxSim_ or the _relaxed earth mover distance_; we choose the terminology _Chamfer_ due to its historical precedence .

In this paper, we study the problem of Nearest Neighbor Search (NNS) with respect to the Chamfer Similarity. Specifically, we are given a dataset \(D=\{P_{1},,P_{n}\}\) where each \(P_{i}^{d}\) is a set of vectors. Given a query subset \(Q^{d}\), the goal is to quickly recover the nearest neighbor \(P^{*} D\), namely:

\[P^{*}=_{P_{i} D}(Q,P_{i})\]

For the retrieval system to be scalable, this must be achieved in time significantly faster than brute-force scoring each of the \(n\) similarities \((Q,P_{i})\) within \(D\).

### Our Approach: Reducing Multi-Vector Search to Single-Vector MIPS

Muvera is a streamlined procedure that directly reduces the Chamfer Similarity Search to MIPS. For a pre-specified target dimension \(d_{}\), Muvera produces randomized mappings \(_{}:2^{^{d}}^{d_{}}\) (for queries) and \(_{}:2^{^{d}}^{d_{}}\) (for documents) such that, for all query and document multi-vector representations \(Q,P^{d}\), we have:

\[_{}(Q),_{}(P) (Q,P)\]

We refer to the vectors \(_{}(Q),_{}(P)\) as _Fixed Dimensional Encodings_ (FDEs). Muvera first applies \(_{}\) to each document representation \(P D\), and indexes the set \(\{_{}(P)\}_{P D}\) into a MIPS solver. Given a query \(Q^{d}\), Muvera quickly computes \(_{}(Q)\) and feeds it to the MIPS solver to recover the top-\(k\) most similar document FDE's \(_{}(P)\). Finally, we re-rank these candidates by the original Chamfer similarity. See Figure 1 for an overview. We remark that one important advantage of the FDEs is that the functions \(_{},_{}\) are _data-oblivious_, making them robust to distribution shifts, and easily usable in streaming settings.

### Related Work on Multi-Vector Retrieval

The early multi-vector retrieval systems, such as ColBERT , all implement optimizations of the previously described SV heuristic, where the initial set of candidates is found by querying a MIPS index for every query token \(q Q\). In ColBERTv2 , the document token embeddings are first clustered via k-means, and the first round of scoring uses cluster centroids instead of the original token. This technique was further optimized in PLAID  by employing a four-stage pipeline to progressively prune candidates before a final re-ranking (Figure 1).

An alternative approach with proposed in DESSERT , whose authors also pointed out the limitations of the SV heuristic, and proposed an algorithm based on Locality Sensitive Hashing (LSH) . They prove that their algorithm recovers \(\)-approximate nearest neighbors in time \((n|Q|T)\), where \(T\) is roughly the maximum number of document tokens \(p P_{i}\) that are similar to any query token \(q Q\), which can be as large as \(_{i}|P_{i}|\). Thus, in the worst case, their algorithm runs no faster than brute-force. Conversely, our algorithm recovers \(\)-approximate nearest neighbors and _always_ runs in time \((n|Q|)\). Experimentally, DESSERT is 2-5\(\) faster than PLAID, but attains worse recall (e.g. 2-2.5\(\%\) R@1000 on MS MARCO). Conversely, we match and sometimes strongly exceed PLAID's recall with up to 5.7\(\) lower latency. Additionally, DESSERT still employs an initial filtering stage based on \(k\)-means clustering of individual query token embeddings (in the manner of ColBERTv2), thus they do not truly avoid the aforementioned limitations of the SV heuristic.

Fixed Dimensional Encodings

We now describe our process for generating FDEs. Our transformation is reminiscent of the technique of probabilistic tree embeddings , which can be used to transform a set of vectors into a single vector. For instance, they have been used to embed the Earth Mover's Distance into the \(_{1}\) metric , and to embed the weight of a Euclidean MST of a set of vectors into the Hamming metric . However, since we are working with inner products, which are not metrics, instead of \(_{p}\) distances, an alternative approach for our transformation will be needed.

The intuition behind our transformation is as follows. Hypothetically, for two MV representations \(Q,P^{d}\), if we knew the optimal mapping \(:Q P\) in which to match them, then we could create vectors \(,\) by concatenating all the vectors in \(Q\) and their corresponding images in \(P\) together, so that \(,=_{q Q} q,(q)=(Q,P)\). However, since we do not know \(\) in advance, and since different query-document pairs have different optimal mappings, this simple concatenation clearly will not work. Instead, our goal is to find a randomized ordering over _all_ the points in \(^{d}\) so that, after clustering close points together, the dot product of _any_ query-document pair \(Q,P^{d}\) concatenated into a single vector under this ordering will approximate the Chamfer similarity.

The first step is to partition the latent space \(^{d}\) into \(B\) clusters so that vectors that are closer are more likely to land in the same cluster. Let \(:^{d}[B]\) be such a partition (for an integer \(N 1\) we use the notation \([N]=\{1,2,,N\}\)); \(\) can be implemented via Locality Sensitive Hashing (LSH) , \(k\)-means, or other methods; we discuss choices for \(\) later in this section. After partitioning via \(\), the hope is that for each \(q Q\), the closest \(p P\) lands in the same cluster (i.e. \((q)=(p)\)). Hypothetically, if this were to occur, then:

\[(Q,P)=_{k=1}^{B}_{q Q\\ (q)=k}_{p P\\ (p)=k} q,p\] (1)

If \(p\) is the only point in \(P\) that collides with \(q\), then (1) can be realized as a dot product between two vectors \(,\) by creating one block of \(d\) coordinates in \(,\) for each cluster \(k[B]\) (call these blocks \(_{(k)},_{(k)}^{d}\)), and setting \(_{(k)},_{(k)}\) to be the sum of all \(q Q\) (resp. \(p P\)) that land in the \(k\)-th cluster under \(\). However, if multiple \(p^{} P\) collide with \(q\), then \(,\) will differ from (1), since _every_\(p^{}\) with \((p^{})=(q)\) will contribute at least \( q,p^{}\) to \(,\). To resolve this, we set \(_{(k)}\) to be the _centroid_ of the \(p P\)'s with \((p)=(q)\). Formally, for \(k=1,,B\), we define

\[_{(k)}=_{q Q\\ (q)=k}q,_{(k)}=^{-1}(k)|}_{p P\\ (p)=k}p\] (2)

Setting \(=(_{(1)},,_{(B)})\) and \(=(_{(1)},,_{(B)})\), then we have

\[,=_{k=1}^{B}_{q Q\\ (q)=k}^{-1}(k)|}_{ p P\\ (p)=k} q,p\] (3)

Note that the resulting dimension of the vectors \(,\) is \(dB\). To reduce the dependency on \(d\), we can apply a random linear projection \(:^{d}^{d_{}}\) to each block \(_{(k)},_{(k)}\), where \(d_{}<d\). Specifically, we define \((x)=(1/}})Sx\), where \(S^{d_{} d}\) is a random matrix with uniformly distributed \( 1\) entries. We can then define \(_{(k),}=(_{(k)})\) and \(_{(k),}=(_{(k)})\), and define the _FDE's with inner projection_ as \(_{}=(_{(1),},,_{(B),})\) and \(_{}=(_{(1),},,_{(B),})\). When \(d=d_{}\), we simply define \(\) to be the identity mapping, in which case \(_{},_{}\) are identical to \(,\). To increase accuracy of (3) in approximating (1), we repeat the above process \(R_{} 1\) times independently, using different randomized partitions \(_{1},,_{R_{}}\) and projections \(_{1},,_{R_{}}\). We denote the vectors resulting from \(i\)-th repetition by \(_{i,},_{i,}\). Finally, we concatenate these \(R_{}\) vectors together, so that our final FDEs are defined as \(_{}(Q)=(_{1,},,_{R_{}, })\) and \(_{}(P)=(_{1,},,_{R_{},})\). Observe that a complete FDE mapping is specified by the three parameters \((B,d_{},R_{})\), resulting in a final dimension of \(d_{}=B d_{} R_{}\).

**Choice of Space Partition.** When choosing the partition function \(\), the desired property is that points are more likely to collide (i.e. \((x)=(y)\)) the closer they are to each other. Such functionswith this property exist, and are known as _locality-sensitive hash functions_ (LSH) (see ). When the vectors are normalized, as they are for those produced by ColBERT-style models, SimHash  is the standard choice of LSH. Specifically, for any \(k_{} 1\), we sample random Gaussian vectors \(g_{1},,g_{k_{}}^{d}\), and set \((x)=(( g_{1},x>0),,( g _{k_{}},x>0))\), where \(()\{0,1\}\) is the indicator function. Converting the bit-string to decimal, \((x)\) gives a mapping from \(^{d}\) to \([B]\) where \(B=2^{k_{}}\). In other words, SimHash partitions \(^{d}\) by drawing \(k_{}\) random half-spaces, and each of the \(2^{k_{}}\) clusters is formed by the \(k_{}\)-wise intersection of each of these halfspaces or their complement. Another natural approach is to choose \(k_{} 1\) centers from the collection of all token embeddings \(_{i=1}^{n}P_{i}\), either randomly or via \(k\)-means, and set \((x)[k_{}]\) to be the index of the center nearest to \(x\). We compare this method to SimHash in (3.1).

**Filling Empty Clusters.** A key source of error in the FDE's approximation is when the nearest vector \(p P\) to a given query embedding \(q Q\) maps to a different cluster, namely \((p)(q)=k\). This can be made less likely by decreasing \(B\), at the cost of making it more likely for other \(p^{} P\) to also map to the same cluster, moving the centroid \(_{(k)}\) farther from \(p\). If we increase \(B\) too much, it is possible that no \(p P\) collides with \((q)\). To avoid this trade-off, we directly ensure that if no \(p P\) maps to a cluster \(k\), then instead of setting \(_{(k)}=0\) we set \(_{(k)}\) to the point \(p\) that is _closest_ to cluster \(k\). As a result, increasing \(B\) will result in a more accurate estimator, as this results in smaller clusters. Formally, for any cluster \(k\) with \(P^{-1}(k)=\), if fill_empty_clusters is enabled, we set \(_{(k)}=p\) where \(p P\) is the point for which \((p)\) has the fewest number of disagreeing bits with \(k\) (both thought of as binary strings), with ties broken arbitrarily. We do not enable this for query FDEs, as doing so would result in a given \(q Q\) contributing to the dot product multiple times.

**Final Projections.** A natural approach to reducing the dimensionality is to apply a final projection \(^{}:^{d_{}}^{d_{}}\) (also implemented via multiplication by a random \( 1\) matrix) to the FDE's, reducing the final dimensionality to any \(d_{}<d_{}\). Experimentally, we find that final projections can provide small but non-trivial \(1\)-\(2\%\) recall boosts for a fixed dimension (see SSC.2).

### Theoretical Guarantees for FDEs

We now state our theoretical guarantees for our FDE construction. For clarity, we state our results in terms of normalized Chamfer similarity \((Q,P)=(Q,P)\). This ensures \((Q,P)[-1,1]\) whenever the vectors in \(Q,P\) are normalized. Note that this factor of \(1/|Q|\) does not affect the relative scoring of documents for a fixed query. In what follows, we assume that all token embeddings are normalized (i.e. \(\|q\|_{2}=\|p\|_{2}=1\) for all \(q Q,p P\)). Note that ColBERT-style late interaction MV models indeed produce normalized token embeddings. We will always use the fill_empty_clusters method for document FDEs, but never for queries.

Our main result is that FDEs give \(\)-additive approximations of the Chamfer similarity. The proof uses the properties of LSH (SimHash) to show that for each query point \(q Q\), the point \(q\) gets mapped to a cluster \((q)\) that _only_ contains points \(p P\) that are close to \(q\) (within \(\) of the closest point to \(q\)); the fact that at least one point collides with \(q\) uses the fill_empty_partitions method.

**Theorem 2.1** (FDE Approximation).: _Fix any \(,>0\), and sets \(Q,P^{d}\) of unit vectors, and let \(m=|Q|+|P|\). Then setting \(k_{}=O()}{})\), \(d_{}=O(}())\), \(R_{s}=1\), so that

Figure 2: FDE Generation Process. Three SimHashes (\(k_{}=3\)) split space into six regions labelled \(A\)-\(F\) (in high-dimensions \(B=2^{k_{}}\), but \(B=6\) here since \(d=2\)). \(_{q}(Q),_{}(P)\) are shown as \(B d\) matrices, where the \(k\)-th row is \(_{(k)},_{(k)}\). The actual FDEs are flattened versions of these matrices. Not shown: inner projections, repetitions, and fill_empty_clusters.

\(d_{}=(m/)^{O(1/)}\), then in expectation and with probability at least \(1-\) we have_

\[(Q,P)-_{q}(Q ),_{}(P)(Q,P)+\]

Finally, we show that our FDE's give an \(\)-approximate solution to Chamfer similarity search, using FDE dimension that depends only _logarithmically_ on the size of the dataset \(n\). Using the fact that our query FDEs are sparse (Lemma A.1), one can run exact MIPS over the FDEs in time \((|Q| n)\), improving on the brute-force runtime of \(O(|Q|_{i}|P_{i}|n)\) for Chamfer similarity search.

**Theorem 2.2**.: _Fix any \(>0\), query \(Q\), and dataset \(P=\{P_{1},,P_{n}\}\), where \(Q^{d}\) and each \(P_{i}^{d}\) is a set of unit vectors. Let \(m=|Q|+_{i[n]}|P_{i}|\). Let \(k_{}=O()\), \(d_{}=O(}(m/))\) and \(R_{}=O(} n)\) so that \(d_{}=m^{O(1/)} n\). Then if \(i^{*}=_{i[n]}_{q}(Q),_{}(P_{ i})\), with high probability (i.e. \(1-1/(n)\)) we have:_

\[(Q,P_{i^{*}})_{i[n]}(Q,P_{ i})-\]

_Given the query \(Q\), the document \(P^{*}\) can be recovered in time \(O(|Q|\{d,n\}}() n)\)._

## 3 Evaluation

In this section, we evaluate our FDEs as a method for MV retrieval. First, we evaluate the FDEs themselves (offline) as a proxy for Chamfer similarity (SS3.1). In (SS3.2), we discuss the implementation of Muvera, as well as several optimizations made in the search. Then we evaluate the latency of Muvera compared to PLAID, and study the effects of the aforementioned optimizations.

**Datasets.** Our evaluation includes results from six of the well-studied BEIR  information retrieval datasets: MS MARCO  (CC BY-SA 4.0), HotpotQA (CC BY-SA 4.0) , NQ (Apache-2.0) , Quora (Apache-2.0) , SciDocs (CC BY 4.0) , and ArguAna (Apache-2.0) . These datasets were selected for varying corpus size (8K-8.8M) and average number of document tokens (18-165); see (SSB) for further dataset statistics. Following , we use the development set for our experiments on MS MARCO, and use the test set on the other datasets.

**MV Model, MV Embedding Sizes, and FDE Dimensionality.** We compute our FDEs on the MV embeddings produced by the ColBERTv2 model  (MIT License), which have a dimension of \(d=128\) and a fixed number \(|Q|=32\) of embeddings per query. The number of document embeddings is variable, ranging from an average of \(18.3\) on Quora to \(165\) on Seldocs. This results in 2,300-21,000 floats per document on average (e.g. 10,087 for MS MARCO). Thus, when constructing our FDEs we consider a comparable range of dimensions \(d_{}\) between 1,000-20,000. Furthermore, using product quantization, we show in (SS3.2) that the FDEs can be significantly compressed by \(32\) with minimal quality loss, further increasing the practicality of FDEs.

### Offline Evaluation of FDE Quality

We evaluate the quality of our FDEs as a proxy for the Chamfer similarity, without any re-ranking and using exact (offline) search. We first demonstrate that FDE recall quality improves dependably as the dimension \(d_{}\) increases, making our method relatively easy to tune. We then show that FDEs are a more effective method of retrieval than the SV heuristic. Specifically, the FDE method achieves Recall\(@N\) exceeding the Recall\(@2\)-4N of the SV heuristic, while in principle scanning a similar number of floats in the search. This suggests that the success of the SV heuristic is largely due to the significant effort put towards optimizing it (as supported by ), and similar effort for FDEs may result in even bigger efficiency gains. Additional plots can be found in (SSC). All recall curves use a single FDE instantiation, since in (SSC.1) we show the variance of FDE recall is negligible.

**FDE Quality vs. Dimensionality.** We study how the retrieval quality of FDE's improves as a function of the dimension \(d_{}\). We perform a grid search over FDE parameters \(R_{}\{1,5,10,15,20\},k_{}\{2,3,4,5,6\},d_{}\{8,16,32,64\}\), and compute recall on MS MARCO (Figure 3). We find that Pareto optimal parameters are generally achieved by larger \(R_{}\), with \(k_{},d_{}\) playing a lesser role in improving quality. Specifically, \((R_{},k_{},d_{})\{(20,3,8),(20,4,8)(20,5,8),( 20,5,16\})\) were all Pareto optimal for their respective dimensions (namely \(R_{} 2^{k_{}} d_{}\)). While there are small variations depending on the parameter choice, the FDE quality is tightly linked to dimensionality; increase in dimensionality will generally result in quality gains. We also evaluate using \(k\)-means as a method of partitioning instead of SimHash. Specifically, we cluster the document embeddings with \(k\)-means and set \((x)\) to be the index of the nearest centroid to \(x\). We perform a grid search over the same parameters (but with \(k\{4,8,16,32,64\}\) to match \(B=2^{k_{}}\)). We find that \(k\)-means partitioning offers no quality gains on the Pareto Frontier over SimHash, and is often worse. Moreover, FDE construction with \(k\)-means is no longer data oblivious. Thus, SimHash is chosen as the preferred method for partitioning for the remainder of our experiments.

In Figure 4, we evaluate the FDE retrieval quality with respect to the Chamfer similarity (instead of labelled ground truth data). We compute \(1\)Recall@\(N\), which is the fraction of queries for which the Chamfer \(1\)-nearest neighbor is among the top-\(N\) most similar in FDE dot product. We choose FDE parameters which are Pareto optimal for the dimension from the above grid search. We find that FDE's with fewer dimensions that the original MV representations achieve significantly good recall across multiple BEIR retrieval datasets. For instance, on MS MARCO (where \(d m_{avg} 10K\)) we achieve \(95\%\) recall while retrieving only \(75\) candidates using \(d_{}=5120\).

Single Vector Heuristic vs. FDE retrieval.We compare the quality of FDEs as a proxy for retrieval against the previously described SV heuristic, which is the method underpinning PLAID. Recall that in this method, for each of the \(i=1,,32\) query vectors \(q_{i}\) we compute the \(k\) nearest neighbors \(p_{1,i},,p_{k,i}\) from the set \(_{i}P_{i}\) of all document token embeddings. To compute Recall@\(N\), we create an ordered list \(_{1,1},,_{1,32},_{2,1},\), where \(_{i,j}\) is the document ID containing \(p_{i,j}\), consisting of the \(1\)-nearest neighbors of the queries, then the \(2\)-nearest neighbors, and so on. When re-ranking, one first removes duplicate document IDs from this list. Since duplicates cannot be detected while performing the initial \(32\) SV MIPS queries, the SV heuristic needs to over-retrieve to reach a desired number of unique candidates. Thus, we note that the true recall curve of implementations of the SV heuristic (e.g. PLAID) is somewhere between the case of no deduplication and full deduplication; we compare to both in Figure 5.

To compare the cost of the SV heuristic to running MIPS over the FDEs, we consider the total number of floats scanned by both using a brute force search. The FDE method must scan \(n d_{}\) floats to compute the \(k\)-nearest neighbors. For the SV heuristic, one runs \(32\) brute force scans over \(n m_{avg}\) vectors in \(128\) dimensions, where \(m_{avg}\) is the average number of embeddings per document (see SSB for values of \(m_{avg}\)). For MS MARCO, where \(m_{avg}=78.8\), the SV heuristic searches through \(32 128 78.8 n\) floats. This allows for an FDE dimension of \(d_{}=\) 322,764 to have comparable cost! We can extend this comparison to fast approximate search - suppose that approximate MIPS

Figure 4: Comparison of FDE recall versus brute-force search over Chamfer similarity.

Figure 3: FDE recall vs dimension for varying FDE parameters on MS MARCO. Plots show FDE Recall@100,1k,10k left to right. Recall@\(N\) for exact Chamfer scoring is shown by dotted lines.

over \(n\) vectors can be accomplished in sublinear \(n^{}\) time, for some \((0,1)\). Then even in the unrealistic case of \(=0\), we can still afford an FDE dimension of \(d_{}=32 128=4096\).

The results can be found in Figure 5. We build FDEs once for each dimension, using \(R_{}=40,k_{}=6,d_{}=d=128\), and then applying a final projection to reduce to the target dimension (see C.2 for experiments on the impact of final projections). On MS MARCO, even the \(4096\)-dimensional FDEs match the recall of the (deduplicated) SV heuristic while retrieving 1.75-3.75\(\)_fewer_ candidates (our Recall\(@N\) matches the Recall\(@1.75\)-3.75\(N\) of the SV heuristic), and 10.5-15\(\) fewer than to the non-deduplicated SV heuristic. For our \(10240\)-dimension FDEs, these numbers are 2.6-5\(\) and 20-22.2\(\) fewer, respectively. For instance, we achieve \(80\%\) recall with \(60\) candidates when \(d_{}=10240\) and \(80\) candidates when \(d_{}=4096\), but the SV heuristic requires \(300\) and \(1200\) candidates (for dedup and non-dedup respectively). See Table 1 for further comparisons.

Variance.Note that although the FDE generation is a randomized process, we show in (SSC.1) that the variance of the FDE Recall is essentially negligible; for instance, the standard deviation Recall\(@1000\) is at most \(0.08\)-\(0.16\%\) for FDEs with \(2\)-\(10k\) dimensions.

### Online Implementation and End-to-End Evaluation

We implemented Muvera, an FDE generation and end-to-end retrieval engine in C++. We discussed FDE generation and various optimizations and their tradeoffs in (SS3.1). Next, we discuss how we perform retrieval over the FDEs, and additional optimizations.

Single-Vector MIPS Retrieval using DiskANN.Our single-vector retrieval engine uses a scalable implementation  of DiskANN  (MIT License), a state-of-the-art graph-based ANNS algorithm. We build DiskANN indices by using the uncompressed document FDEs with a maximum degree of 200 and a build beam-width of 600. Our retrieval works by querying the DiskANN index using beam search with beam-width \(W\), and subsequently reranking the retrieved candidates with Chamfer similarity. The only tuning knob in our system is \(W\); increasing \(W\) increases the number of candidates retrieved by Muvera, which improves the recall.

Product Quantization (PQ).To further improve the memory usage of Muvera, we use a textbook vector compression technique called product quantization (PQ) with asymmetric querying  on the FDEs. We refer to product quantization with \(C\) centers per group of \(G\) dimensions as PQ-\(C\)-\(G\). For example, PQ-\(256\)-\(8\), which we find to provide the best tradeoff between quality and compression in our experiments, compresses every consecutive set of \(8\) dimensions to one of \(256\) centers. Thus PQ-\(256\)-\(8\) provides \(32\) compression over storing each dimension using a single float, since each block of \(8\) floats is represented by a single byte. See (SSC.4) for further experiments and details on PQ.

Experimental Setup.We run our online experiments on an Intel Sapphire Rapids machine on Google Cloud (c3-standard-176). The machine supports up to 176 hyper-threads. We run latency experiments using a single thread, and run our QPS experiments on all 176 threads.

Ball Carving.To improve re-ranking speed, we reduce the number of query embeddings by clustering them via a _ball carving_ method and replacing the embeddings in each cluster with their sum. This speeds up reranking without decreasing recall. Specifically, we group the queries \(Q\) into clusters \(C_{1},,C_{k}\), setting \(c_{i}=_{q C_{i}}q\) and \(Q_{C}=\{c_{1},,c_{k}\}\). Then, after retrieving a set of candidate documents with the FDEs, instead of rescoring via Chamfer\((Q,P)\) for each candidate \(P\), we rescore via Chamfer\((Q_{C},P)\), which runs in time \(O(|Q_{C}||P|)\), offering speed-ups when the number of clusters is small. Instead of fixing \(k\), we perform greedy ball-carving to allow \(k\) to adapt to \(Q\). Specifically, given a threshold \(\), we select an arbitrary point \(q Q\), cluster it with all other points \(q^{} Q\) with \( q,q^{}\), remove the clustered points and repeat until all points are clustered.

Figure 5: FDE retrieval vs SV Heuristic, both with and without document id deduplication.

In Figure 6, we show the the trade-off between end-to-end Recall\(@k\) of Muvera and the ball carving threshold used. Notice that for both \(k=100\) and \(k=1000\), the recall curves flatten after a threshold of \(=0.6\), and for all datasets they are essentially flat after \( 0.7\). Thus, for such thresholds we incur essentially no quality loss by performing ball carving. Based on these empirical results, we choose the value of \(=0.7\) in our end-to-end experiments.

In (SSC.3), we show the impact on end-to-end QPS of ball carving on the MS MARCO dataset. For sequential re-ranking, we find that ball carving at a \(=0.7\) threshold provides a \(25\%\) QPS improvement, and when re-ranking is being done in parallel (over all cores simultaneously) it yields a 20\(\%\) QPS improvement. Moreover, with a threshold of \(=0.7\), there were an average of \(5.9\) clusters created per query on MS MARCO. This reduces the number of query embeddings by 5.4\(\), down from the initial fixed setting of \(|Q|=32\). This finding shows that pre-clustering the queries before re-ranking gives non-trivial runtime improvements with negligible quality loss. It also suggests that a fixed setting of \(|Q|=32\) query embeddings used by existing approaches is likely excessive for MV similarity quality, and that fewer queries could achieve a similar performance.

QPS vs. Recall.A useful metric for retrieval is the number of _queries per second (QPS)_ a system can serve at a given recall; evaluating the QPS of a system tries to fully utilize the system resources (e.g., the bandwidth of multiple memory channels and caches), and deployments where machines serve many queries simultaneously. Figure 7 shows the QPS vs. Recall@100 for Muvera on a subset of the BEIR datasets, using different PQ schemes over the FDEs. We show results for additional datasets, as well as Recall@1000, in the Appendix. Using PQ-\(256\)-\(8\) not only reduces the space usage of the FDEs by 32\(\), but also improves the QPS at the same query beamwidth by up to 20\(\), while incurring a minimal loss in end-to-end recall. Our method has a relatively small dependence on the dataset size, which is consistent with prior studies on graph-based ANNS data structures, since the number of distance comparisons made during beam search grows roughly logarithmically with increasing dataset size . We tried to include QPS numbers for PLAID , but unfortunately their implementation does not support running multiple queries in parallel, and is optimized for measuring latency.

Latency and Recall Results vs. PLAID We evaluated Muvera and PLAID  on the 6 datasets from the BEIR benchmark described earlier in (SS3); Figure 8 shows that Muvera achieves essentially equivalent Recall@\(k\) as PLAID (within 0.4%) on MS MARCO, while obtaining up to 1.56\(\) higher recall on other datasets (e.g. HotpotQA). We ran PLAID using the recommended settings for their system, which reproduced their recall results for MS MARCO. Compared with PLAID, on

Figure 6: Plots showing the trade-off between the threshold used for ball carving and the end-to-end recall.

Figure 7: Plots showing the QPS vs. Recall@100 for Muvera on a subset of the BEIR datasets. The different curves are obtained by using different PQ methods on 10240-dimensional FDEs.

average over all \(6\) datasets and \(k\{100,1000\}\), Muvera achieves 10% higher Recall@\(k\) (up to 56% higher), and 90% lower latency (up to 5.7\(\) lower).

Importantly, Muvera has consistently high recall and low latency across all of the datasets that we measure, and our method _does not_ require costly parameter tuning to achieve this--all of our results use the same 10240-dimensional FDEs that are compressed using PQ with PQ-\(256\)-\(8\); the only tuning in our system was to pick the first query beam-width over the \(k\) that we rerank to that obtained recall matching that of PLAID. As Figure 8 shows, in cases like NQ and HotpotQA, Muvera obtains much higher recall while obtaining lower latency. Given these results, we believe a distinguishing feature of Muvera compared to prior multi-vector retrieval systems is that it achieves consistently high recall and low latency across a wide variety of datasets with minimal tuning effort.

## 4 Conclusion

In this paper, we presented Muvera: a principled and practical MV retrieval algorithm which reduces MV similarity to SV similarity by constructing Fixed Dimensional Encoding (FDEs) of a MV representation. We prove that FDE dot products give high-quality approximations to Chamfer similarity (SS2.1). Experimentally, we show that FDEs are a much more effective proxy for MV similarity, since they require retrieving 2-4\(\) fewer candidates to achieve the same recall as the SV Heuristic (SS3.1). We complement these results with an end-to-end evaluation of Muvera, showing that it achieves an average of 10% improved recall with 90% lower latency compared with PLAID. Moreover, despite the extensive optimizations made by PLAID to the SV Heuristic, we still achieve significantly better latency on \(5\) out of \(6\) BEIR datasets we consider (SS3). Given their retrieval efficiency compared to the SV heuristic, we believe that there are still significant gains to be obtained by optimizing the FDE method, and leave further exploration of this to future work.

**Broader Impacts and Limitations:** While retrieval is an important component of LLMs, which themselves have broader societal impacts, these impacts are unlikely to result from our retrieval algorithm. Our contribution simply improves the efficiency of retrieval, without enabling any fundamentally new capabilities. As for limitations, while we outperformed PLAID, sometimes significantly, on \(5\) out of the \(6\) datasets we studied, we did not outperform PLAID on MS MARCO, possibly due to their system having been carefully tuned for MS MARCO given its prevalence. Additionally, we did not study the effect that the average number of embeddings \(m_{avg}\) per document has on retrieval quality of FDEs; this is an interesting direction for future work.

Figure 8: Bar plots showing the latency and Recall@\(k\) of Muvera vs PLAID on a subset of the BEIR datasets. The x-tick labels are formatted as dataset-\(k\), i.e., optimizing for Recall@\(k\) on the given dataset.