# Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective

Zeyu Zhang\({}^{1}\) Yi Su\({}^{2}\)1 Hui Yuan\({}^{3}\) Yiran Wu\({}^{4}\) Rishab Balasubramanian\({}^{5}\)

Qingyun Wu\({}^{4}\) Huazheng Wang\({}^{5}\)1 Mengdi Wang\({}^{3}\)

\({}^{1}\)University of Science and Technology of China \({}^{2}\)Google Deepmind \({}^{3}\)Princeton University

\({}^{4}\)Penn State University \({}^{5}\)Oregon State University

zgkd2019zzy@mail.ustc.edu.cn yisumtv@google.com

{huiyuan, mengdiw}@princeton.edu {ykw5399, qingyun.wu}@psu.edu

{balasuri, huazheng.wang}@oregonstate.edu

###### Abstract

Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets demonstrate that CUOLR consistently outperforms the state-of-the-art off-policy learning to rank algorithms while maintaining consistency and robustness under different click models.

## 1 Introduction

Learning to Rank (LTR) is a core problem in Information Retrieval (IR) with wide applications such as web search and recommender systems . Traditional LTR methods require high-quality annotated relevance judgments for model training, which is expensive, time-consuming, and may not align with actual user preferences . As a result, learning to rank with implicit user feedback, such as logged click data, has received a huge amount of attention in both academia and industry .

Despite its low cost, learning to rank directly from implicit user feedback could suffer from the intrinsic noise and bias in user interactions, e.g., position bias, where an item displayed at a higher position receives a higher click-through rate (CTR) than its relevance . To mitigate the bias in the click data, off-policy learning to rank methods have been proposed under different bias assumptions such as position bias , selection bias  and trust bias . A major branch of off-policy learning to rank called counterfactual learning to rank achieves unbiasedness by re-weighting the samples using the inverse propensity scoring (IPS) method . To estimate the propensity from logged click data, existing works require explicit assumptions about how users examine the rank list and generate the click data, i.e., click models . For example, position-based model(PBM)  assumes the probability of examining a result only depends on the position; while cascade model (CASCADE)  assumes each click depends on the previous click, and dependent click model (DCM)  considers both. Different debiasing methods have been proposed to cater to specific click models, including PBM [4; 53; 26], CASCADE; and DCM . However, prior knowledge of the click model is usually unknown and the correct click model needs to be identified from user behavior data before applying an off-policy algorithm, which is challenging in complex real-world environments. Besides, many popular and powerful click models have not been studied in counterfactual learning to rank such as the click chain model (CCM)  and the user browsing model (UBM) . It requires a significant amount of work to study debiasing methods for every popular click model.

To overcome these issues, we propose to study a unified approach of off-policy learning to rank adaptable to general click models. Our key insight is that the user's examination and click behavior summarized by click models has a Markov structure; thus off-policy LTR under general click models can be formulated as a _Markov Decision Process (MDP)_. Specifically, the learning to rank problem now can be viewed as an episodic RL problem [45; 1], where each time step corresponds to a ranking position, each action selects a document for the position, and the state captures the user's examination tendency. This formulation allows us to view off-policy LTR from the perspective of _offline reinforcement learning_, where we can leverage off-the-shelf offline RL algorithms [29; 19; 16] to optimize the ranking list. Importantly, our formulation bridges the area of off-policy learning to rank and offline RL, allowing for the integration of ideas and solutions from offline RL to enhance the solution of the off-policy LTR problem.

Inspired by the formulation, we propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method. We first construct each logged query and ranking data as an episode of reinforcement learning following the MDP formulation. Our dedicated structure for state representation learning can efficiently capture the dependency information for examination and click generation, e.g. ranking position in PBM and previous documents in CM and DCM. The algorithm jointly learns state representation and optimizes the policy, where any off-the-shelf offline RL algorithm can be applied as a plug-in solver. Specifically, we adapt the popular CQL algorithm  as an instantiation, which applies the conservative (pessimism) principle to Q function estimate. We evaluate our algorithm on real-world learning to rank datasets [38; 7] under various click models. Compared with off-policy LTR methods that are dedicated to specific click models, our click model-agnostic method consistently outperforms the best-performing baselines in all click models.

The contributions of this paper are summarized as follows:

* We formulate the off-policy LTR with biased feedback under general click model as a Markov Decision Process, and bridge the area of off-policy learning to rank and offline reinforcement learning.
* We propose CUORL, a Click model-agnostic Unified Off-policy LTR method that could utilize any offline RL algorithm as a plug-in solver, and we instantiate it using CQL.
* We conduct extensive empirical experiments to validate the effectiveness of our algorithm using real-world LTR datasets under different click models.

## 2 Related Work

Off-policy Learning to Rank.Off-policy Learning to Rank aims to optimize the ranking function from logged click data . The majority of the works aim to mitigate the bias in logged click data, known as counterfactual learning to rank or unbiased learning to rank. The debiasing methods mainly follow inverse propensity scoring strategy [26; 49; 4; 51; 48; 3], while there are also recent works applying doubly robust estimator to reduce variance [40; 28; 37]. Cief et al.  proposed pessimistic off-policy optimization for learning to rank that also mitigates bias but not in an unbiased way. All these methods rely on prior knowledge of the click model [12; 10], while our algorithm is agnostic to general click models.

Offline Reinforcement Learning.Offline RL algorithms [29; 14; 44; 21; 16; 15] learn policy from large logged datasets where the distributional shift between the logging policy and the learned policy imposes a major challenge. In this setting, different algorithms are proposed, from value-based ones(e.g. [35; 34]) to policy-based ones (e.g. [43; 42]). Among the vast literature on offline reinforcement learning, the principle of pessimism/conservatib [29; 5; 22]] as an important line and has inspired many algorithms from empirical and theoretical perspective [60; 39; 32; 56; 61; 55; 58]. While all the aforementioned methods can be plugged into our algorithm, we choose the classic CQL algorithm  with a conservative Q function on top of soft actor-critic algorithm .

Reinforcement Learning to Rank.Wei et al.  first model ranking problem as an MDP, where the state is the candidate document set at current rank and the action is the selected document. [57; 62] have been studied under similar MDP formulation. However, [54; 57] requires relevance labels as feedback and cannot mitigate bias in click data;  is an online learning algorithm that learns from user interactions instead of logged data. Compared to these studies, we characterize the MDP formulation from a different perspective, i.e., capture bias in the click model, and propose the offline RL algorithm with logged click feedback.

## 3 Reinforcement Learning to Rank: A Unified Formulation

As the majority of existing works in unbiased learning to rank focused on inferring documents' relevance from the click models, these methods are tied to specific click models adopted. In this section, we formulate learning to rank with general click feedback as a Markov decision process, offering a unified and comprehensive modeling of possibly complicated user behavior. This formulation unifies LTR problems associated with different click models, under which the underlying click model is translated into the environment setup of MDP such as state transitions and rewards. It opens up the possibility to employ a rich class of reinforcement learning algorithms for solving LTR, which we will give greater details in the next section.

### Preliminary

Click model.A key challenge of off-policy LTR lies in learning the document's attractiveness/relevance from the implicit feedback that is biased by the user's examination behavior. To address this challenge, a range of click models have been proposed to accommodate user's various click behavior . In this study, we focus on a general family of click models [63; 30], which is marked by two characteristics: (1) Most of the mainstream click models have a "two-step" flavor that breaks user's click behavior towards some document down into the user's examination and the document's relevance. For each document \(d\), the user first decides whether to examine it in the ranking list, based on the specific behavior. Mathematically the user behavior is modeled as the examination probability, which generally depends on the ranking list \(\) and the position of the document \(k\), denote as \((,k)\). Once the document is examined, the user will choose whether to click it, based on the attractiveness \((d)\)2. (2) Any documents under the \(k\)-th position do not have an effect on \((,k)\).

**Definition 1** (Click Model).: _For any rank list \(\) and position \(k\), the attractiveness and examination probability are independent._

\[P(C_{k}=1,k)=(,k)((k))\] (1)

_where \(C_{k}\) is the click indicator at rank \(k\), and \((,k)\) is the examination probability of position \(k\) in the rank list \(\). For each document \(d\), the attractiveness \((d)\) only depends on the document itself. And the attractiveness is mutually independent._

We show that classic click models such as PBM, CASCADE, DCM, and CCM are instances of Definition 1, with details listed in Appendix C.

### Learning to Rank as Markov Decision Process

In Reinforcement Learning (RL), the interactions between the agent and the environment are often described as an _finite-horizon discounted Markov Decision Process_\(M=(S,A,T,r,,H)\). The goal of the RL problem is to find a policy \(\) that maximizes the value function, i.e. the discountedcumulative reward

\[[_{t=0}^{H}^{t}r(s_{t},a_{t}),s_{0}=s].\] (2)

In what follows, we formulate each of the \((S,A,T,r,,H)\) components in the ranking scenario. Our formulation essentially differs from the existing MDP formulation of ranking , where the state at position \(k\) is defined as remaining documents that are yet to rank following the \(k-1\) ranked ones on the top, instead Wei et al.  has a limited capturing of user's click behavior as a result of being ignorant of the ordering within the top \(k-1\) documents. From here on, we use \(k[K]\) to denote the \(k\)-th position top down on a list of total length \(K\). It serves as the counterpart of the time step \(t\) in (2) for the ranking setting.

State \(\)For each position \(k[K]\), state \(s_{k}\) should include and represent the current status of the ranking that the agent is faced with. Thus, we define the state at rank \(k\) as:

\[s_{k}=[(d_{1},d_{2},,d_{k-1}),k],\] (3)

which is a concatenation of the established sub-ranking list up to \(k\), denoted by \((d_{1},d_{2},,d_{k-1})\), and the position \(k\). Here \(d_{i}\) refers to the document presented at rank \(i\) with \(s_{0}\) is initialized as \(=[(),0]\). Together with the action \(a_{k}\) as to select document \(d_{k}\) presenting at rank \(k\), defining \(s_{k}\) as (3) fully captures the user's click behavior \(C_{k}\) at this point. Recall (1) that \(P(C_{k}=1,k)=(,k)((k))\), where \((,k)\) is determined by \((d_{1},d_{2},,d_{k-1})\). To better capture the rich information in the observed state, we discuss how to attain the effective state embedding from the raw representation in Section 4.

Action \(\)Action \(a_{k}\) is naturally defined as the document to present at rank \(k\) given state \(s_{k}\). In our experiments, each action is represented by a feature vector associated with the query. It is worth mentioning that the available action set \(_{k}\) at each \(k\) is related to state \(s_{k}\) as well as the specific query, unlike the common case where the action space is fixed. There are two main differences here compared with the fixed action space: (1). the available actions vary under different queries; and (2). once an action is chosen, it is removed from the candidate set \(_{k}\).

Transition \((s^{}|s,a)\)Transition maps a state-action pair to the probability distribution over possible next states. Given our formulation of state and action, the next state \(s_{k+1}\) is deterministic given \(s_{k}=[(d_{1},d_{2},,d_{k-1}),k]\) and \(a_{k}\). Formally, \((s_{k+1}|s_{k},a_{k})=1\) if and only if \(s_{k+1}=[(d_{1},d_{2},,d_{k-1},a_{k}),k+1]\). Note that with this transition function, we can show that our model is a rigorous MDP, where the distribution of next state only based on the current state and the action.

Reward \(r(s,a)\)Aligned with the goal of LTR to maximize total clicks, we adopt the binary click as a reward at each position, i.e. \(r(s_{k},a_{k})=C_{k}\). It is easily checked this is a well-defined reward from (1) that the distribution of \(r\) is fully determined by \(s\) and \(a\), i.e., \([r(s_{k},a_{k})]=(s_{k})(a_{k})\).

Putting the components together we have formally built up our MDP formulation of LTR, which we name as "MDP for **R**anking" and denote by \((,,,r,,H)\) with components defined as above. The rich RL literature has the convention to solve the optimal policy \(^{*}\) that maximizes the cumulative reward, which in our proposed MDP translates to

\[^{*}=*{argmax}_{}(_{k=1}^{K}^{k -1}r(s_{k},( s_{k}))),\] (4)

where the expectation is taken over the stochasticity in both environments \(\) and policy \(\). Before leveraging any RL algorithms for ranking, it is necessary to validate good policies of \((,,,r,,K)\) yields good ranking lists. In the following subsection, we give a rigorous theorem for this validation.

### Optimizing Rank List by Optimizing Policy

Constructing a rank list given policy \(\). With the definition of MDP, we can construct a rank list with any given policy sequentially. At each position \(k\), the state is constructed based on previous document features (manually or by sequential models). Then a document (action) is chosen by the policy \(\) and placed at position \(k\), where the next state is determined accordingly. Repeat this process at each position until all the documents are set or the list reaches its end (\(K=10\) for example).

**Definition 2** (Policy Induced Ranking.).: _Given a policy \(( s)\) of \((,,,r,)\), construct the induced rank list \(^{}\) as_

\[^{}(k) a_{k}( s_{k}).\]

To investigate whether the optimal rank list can be captured by optimal policy \(^{*}\), we start with defining the optimality of rank list.

**Definition 3** (Optimality).: _A rank list \(\) is optimal if and only if all documents are sorted in descending order in terms of their attractiveness, i.e._

\[((1))((K))\]

_and \(\{(1),,(K)\}\) are the \(K\) most attractive documents among all, where \(K\) is the length of the list and it is also called the top-K ranking._

**Assumption 3.1** (Optimality of optimal ranking).: _Let \(V_{}(s_{1})=[_{k=1}^{K}^{k-1}r(s_{k},(k))]\) be the value of rank list \(\), and let \(^{}\) be the optimal rank list by Definition 3. Then \(_{}V_{}(s_{1})=V_{^{}}(s_{1})\)._

Assumption 3.1 is adopted from Assumption 2 in , which suggests optimal rank list sorted by decreasing attractiveness of documents leads to optimal rewards, which will be covered by optimal policy learned from our MDP formulation. This is a mild assumption as classic click models such as PBM and cascade model all satisfy the assumption .

## 4 Unified Off-policy Learning to Rank

The formulation of off-policy learning-to-rank (LTR), when viewed from the perspective of offline reinforcement learning, presents an opportunity to leverage off-the-shelf RL algorithms to address off-policy LTR problems. In this section, we introduce a novel and unified off-policy LTR algorithm that is agnostic to the underlying click model used to generate the offline training data. Our algorithm is composed of three key components: (1) episodes constructed from the logged ranking data; (2). state representation learning; and (3). policy optimization via offline RL algorithms. In the following, we provide a detailed exposition of each component.

Episodes Construction.Given any logged ranking dataset, the first step is to construct a series of episodes from the ranking data, making it compatible with the off-the-shelf RL algorithms. Specifically, our original ranking data \(\{(q_{i},_{i},_{i}(q_{i},_{i}))\}_{i=1}^{n}\) is composed of \(n\) tuples with contextual query feature \(q_{i}\), a ranked list \(_{i}\) with \(K\) documents and a corresponding click vector \(_{i}(q_{i},_{i})\{0,1\}^{K}\). From the perspective of RL, this tuple can be conceptualized as one episode, following the established MDP formulation of the ranking process. In particular, for each tuple \((q_{i},_{i},_{i}(q_{i},_{i}))\), we transform it to a length \(K\) episode \(_{i}:=\{(s^{i}_{k},a^{i}_{k},r^{i}_{k})\}_{k=1}^{K}\) with

\[s^{i}_{k}:=(_{i}[ k],k), a^{i}_{k}:=_{i}[ k], r^{i}_{k}:=_{i}(q_{i},_{i})[k] k[K]\]

Here we use \(_{i}[ k]\) to denote the concatenation of the document feature vectors before position \(k\) and \(_{i}[k]\) represents the document feature at position \(k\), similarly for \(_{i}(q_{i},_{i})[k]\). In particular, the episode \(_{i}\) is constructed by going through the ranked list from top to the bottom. Each state \(s^{i}_{k}\) contains all the document information before position \(k\) and the position information \(k\), represented as a function of \(_{i}[ k]\) and \(k\) with \(\) being a learned embedding function that we will discuss shortly. The action at step \(k\) is the document placed at position \(k\), i.e., \(_{i}[k]\), with the reward at current timestep is the binary click for the corrpsonding action at position \(k\), i.e., \(_{i}(q_{i},_{i})[k]\). Given this, we have constructed an offline RL dataset with \(n\) episodes with length \(K\) each.

State Representation Learning.In RL, state representation learning offers an efficient means of managing large, raw observation spaces, enhancing the generalization performance when encountering previously unseen states. In our particular setting, the observation space consists of raw document features up to position \(k\). As we will show in Section 5.2, utilizing raw observations as the state representation in the RL algorithm can lead to highly sub-optimal performance, due to both unscalability with respect to \(k\) and limitations in the representation power. Rather than incorporating additional auxiliary tasks to explicitly learn state representations, we propose to jointly learn the staterepresentations together with the policy optimization algorithm, which aims to automatically learn state representation \(\) that will benefit the downstream policy optimization task. For example, DQN uses multiple layers of nonlinear functions to encode the map perceptual inputs to a state embedding that could be linearly transformed into the value function. To this end, we introduce the _implicit state representation learning component_ our off-policy learning to rank algorithm, which is composed of the following key components:

_Positional Encoding_: To effectively inject the position information in \(s^{i}_{k}\), we utilize the _positional encoding_ technique [50; 59], ensuring the model make use of the position information when generating clicks. Specifically, positional encoding represents each position \(k\) in the ranked list by the sinusoidal function with different frequencies such that

\[PE(k)_{2i}=(}}}), PE(k )_{2i+1}=(}}}),\]

Here \(PE(k)^{d}\) with \(d\) being the dimension of document feature and \(2i\) and \(2i+1\) being the even and the odd entries of \(PE(k)\) with \(2i,2i+1[d]\).

_Multi-head Self-attention_: The other challenge in our case is to find a specific architecture for the state representation learning that tailored for the learning to rank task. As the ranked list of document features is inherently sequential, we leverage the multi-head self-attention mechanism to learn the state embedding \(\). Specifically, the state \(s^{i}_{k}\) is defined as:

\[s^{i}_{k}:=(_{i}[:k],k)=(head_{1},,head_{I })W^{O}\]

where \(head_{i}=(s_{k} W^{Q}_{i},s_{k} W^{K}_{i},s_{k}  W^{V}_{i})\), with \(W^{Q}_{i},W^{K}_{i},W^{V}_{i}_{i}\) are learnable parameters for the \(i^{th}\) head and \(W^{O}\) is the learnable parameter of the output layer after concatenating the results of all the \(I\) heads; At each position \(k\), we concatenate the document features \(_{i}[:k]\) along with the position embedding for position \(k\), passing them as the input for the multi-head self-attention.

Joint Representation Learning and Policy Optimization.Given the constructed offline dataset and a dedicated structure for learning efficient state representations, we now demonstrate how we leverage any off-the-shelf RL algorithm as the plug-in solver for finding the optimal policy. We use the popular offline RL algorithm: CQL, as an instantiation, which learns a conservative Q function and utilizes the classical soft actor-critic algorithm on top of it. Specifically, it optimizes the following lower bound of the critic (Equation 5):

\[*{argmin}_{}_{s }[_{a}(Q_{}(s,a))-_{a _{}(a|s)}[Q_{}(s,a)]]+_{s,a,s^{} }[(Q_{}-}^{}_{ ^{}})^{2}]\] (5)

where \(}^{}Q=r+ P^{}Q\) is the estimated _Bellman operator_, \(_{}\) is the logging policy. Here we use \(^{}\) to emphasize that the parameters in the target Q network is different from policy Q network. The conservative minimizes the expected Q value for out-of-distribution (state, action) pairs and prevents the Q-function's over-estimation issue. Built upon SAC, the algorithm improves the policy \(_{}\) (i.e., the actor) based on the gradient of the estimated Q function, with entropy regularization. Compared with the original CQL algorithm, we also add the state representation learning component, and we jointly optimize the state embedding, the critic and actor parameters, with a detailed algorithm included in Algorithm 1. Once we have obtained the learned optimal policy \(^{*}_{}\), we extract the optimal ranking policy following Definition 2.

## 5 Experiments

We empirically evaluate the performance of our proposed method CUOLR on several public datasets, compared with the state-of-the-art off-policy learning-to-rank methods. Specifically, we aim to assess the robustness of our method across different click models and showcase the effectiveness of our unified framework. Beyond this, we perform the ablation study to examine the efficacy of our proposed state representation learning component. 3.

### Setup

Datasets.We conduct semi-synthetic experiments on two traditional learning-to-rank benchmark datasets: MSLR-WEB10K and Yahoo! LETOR (set 1). Specifically, we sample click data from these real-world datasets, which not only increases the external validity of the experiments but also provides the flexibility to explore the robustness of our method over different click models. For both datasets, it consists of features representing query and document pairs with manually judged relevance labels ranging from 0 (irrelevant) to 4 (perfectly relevant). We provide the statistics of all datasets in Appendix A. Both datasets come with the train-val-test split. The train data is used for generating logging policy and simulating clicks, with the validation data used for hyperparameter selection. And the final performance of the learned ranking policy is evaluated in the test data.

Click Data Generation.We follow Joachims et al.  to generate partial-information click data from the full-information relevance labels. Specifically, we first train a Ranking SVM  using \(1\%\) of the training data as our logging policy \(_{}\) to present the initial ranked list of items. For each query \(q_{i}\), we get a ranking \(_{i}\) and simulate the clicks based on various click models we use. As discussed in Section 3.1, there are two components for the click generation, examination probability and attractiveness of the document for the query. All click models differ in their assumptions on the examination probability. For PBM, we adopt the examination probability \(=\{_{k}\}_{k=1}^{K}\) estimated by Joachims et al.  through eye-tracking experiments:

\[(^{q},k)=(k)=_{k}^{}\]

where \([0,+]\) is a hyper-parameter that controls the severity of presentation biases and in our experiment, we set \(=1.0\) as default. For CASCADE, the examination probabilities are only dependent on the attractions of each previous document. For DCM, the \(\)s are also simulated by the same parameters as PBM examination probability. We use the same attraction models for all click models, as defined following:

\[(d)=+(1-)-1}{2^{r_{max}}-1}\]

where \(r(d)\) is the relevance label for document \(d\) and \(r_{max}=4\) is the maximum relevance label. We also use \(\) to model click noise so that irrelevant documents have a non-zero probability to be treated as attractive and being clicked.

Baselines and Hyperparameters.We compare our CUOLR method with the following baselines: (1). _Dual Learning Algorithm (DLA)_ which jointly learns an unbiased ranker and an unbiased propensity model; (2). _Inverse Propensity Weighting (IPW)_ Algorithm [52; 26] which first learns the propensities by result randomization, and then utilizes the learned probabilities to correct for position bias; and (3). _Cascade Model-based IPW (CM-IPW)_ which designs a propensity estimation procedure where previous clicks are incorporated in the estimation of the propensity. It is worth mentioning that (1) and (2) are designed for PBM and (3) is tailored for cascade-based models. Besides, we train a LambdaMart  model with true relevance labels as the upper bound for the ranking model, _ORACLE_ for short. The performance of the logging policy (_LOGGING_) is also reported as the lower bound of the ranking model.

For baselines, we use a 2-layer MLP with width 256 and ReLU activation according to their original paper and codebase [4; 51; 47]. For the embedding model in our method, we use multi-head attention with 8 heads. And for actors and critics in CQL and SAC algorithms, we utilize a 2-layer MLP with width 256 and ReLU activation. The conservative parameter \(\) (marked red in Equation (5)) in CQL is set to 0.1. We use Adam for all methods with a tuned learning rate using the validation set. More details are provided in Appendix A.

Metrics.We evaluate all the methods using the full-information test set. We use the _normalized Discounted Cumulative Gain (nDCG)_ and the _Expected Reciprocal Rank (ERR)_ as evaluation metrics and report the results at position 5 and 10 to demonstrate the performance of models at different positions.

### Results

How does CUOLR perform across different click models, compared to the baselines?To validate the effectiveness of our CUOLR method, we conducted performance evaluations across a range of click-based models, including PBM, CASCADE, DCM, and CCM. We compared our approach with state-of-the-art baselines specifically designed for these click models, namely DLA, IPW, and CM-IPW. Due to space limitation, we only show results of PBM, CASCADE, and DCM in Table 1, with the full table shown in Appendix B. For the position-based model, IPW demonstrates the

    & &  &  \\  CLICK MODEL & ALG &  &  &  &  \\   & & K=5 & K=10 & K=5 & K=10 & K=5 & K=10 & K=5 & K=10 \\   & DLA & 0.439 & 0.455 & 0.691 & 0.742 & 0.256 & 0.278 & 0.356 & 0.384 \\  & CM-IPW & 0.440 & 0.456 & 0.692 & 0.743 & 0.255 & 0.277 & 0.354 & 0.383 \\  & IPW & 0.446 & 0.462 & **0.700** & 0.748 & 0.281 & 0.301 & 0.367 & 0.390 \\   & CUOLR(CQL) & **0.458\({}^{**}\) & **0.473\({}^{**}\)** & **0.700** & **0.753\({}^{**}\)** & **0.281** & **0.303** & **0.380\({}^{**}\)** & **0.406\({}^{**}\)** \\  & CUOLR(SAC) & **0.459\({}^{**}\)** & **0.478\({}^{**}\)** & **0.700** & **0.753\({}^{**}\)** & 0.279 & **0.301** & **0.379\({}^{**}\)** & **0.404\({}^{**}\)** \\   & DLA & 0.441 & 0.457 & 0.690 & 0.741 & 0.265 & 0.286 & 0.365 & 0.392 \\  & CM-IPW & 0.444 & 0.460 & **0.696** & 0.745 & **0.283** & **0.304** & **0.378** & 0.403 \\  & IPW & 0.442 & 0.457 & 0.690 & 0.740 & 0.257 & 0.279 & 0.358 & 0.387 \\   & CUOLR(CQL) & **0.459\({}^{**}\)** & **0.479\({}^{**}\)** & **0.696** & **0.748** & 0.280 & 0.301 & **0.378** & **0.404** \\  & CUOLR(SAC) & **0.461\({}^{**}\)** & **0.478\({}^{**}\)** & **0.696** & **0.748** & 0.279 & 0.301 & **0.379** & **0.405** \\   & DLA & 0.444 & 0.459 & 0.696 & 0.745 & 0.279 & 0.299 & 0.378 & 0.404 \\  & CM-IPW & 0.447 & 0.463 & **0.704** & 0.752 & **0.284** & **0.304** & 0.379 & 0.402 \\   & IPW & 0.444 & 0.459 & 0.697 & 0.746 & 0.278 & 0.299 & 0.363 & 0.387 \\   & CUOLR(CQL) & **0.461\({}^{**}\)** & **0.474\({}^{**}\)** & 0.699 & **0.753** & 0.278 & 0.300 & **0.380** & **0.405** \\   & CUOLR(SAC) & **0.461\({}^{**}\)** & **0.475\({}^{**}\)** & 0.703 & **0.755** & 0.278 & 0.300 & 0.378 & 0.403 \\   & LOGGING & 0.385 & 0.403 & 0.630 & 0.693 & 0.206 & 0.230 & 0.304 & 0.338 \\   & ORACLE & **0.462** & **0.476** & **0.739** & **0.781** & **0.328** & **0.347** & **0.432** & **0.453** \\   

Table 1: Performance comparison with different click models on Yahoo! LETOR set1 and MSLR-WEB10K. “*” and “**” indicate statistically significant improvement (p-value ¡ 0.05 and p-value ¡ 0.01 respectively) over the best baseline for each metric respectively.

best performance among the baselines, which is expected as it is tailored for position-based methods. Similarly, CM-IPW yielded the best performance for the cascade-based methods, which aligns with its incorporation of previous document information in the propensity estimation. Remarkably, across all click models, our method, whether combined with SAC or CQL, consistently achieves the best performance in most cases. This validates the effectiveness of our unified framework and the robustness of our CUOLR algorithm. Furthermore, it is noteworthy that our method demonstrated consistent performance across different RL algorithms, verifying its resilience and adaptability to various underlying RL solvers.

How effective is the state representation learning component in CUOLR?In this experiment, we examine different approaches for state representation learning and study how it affects the overall performance of our proposed method. We compare with the following state embeddings: (1). position-only embedding (POS), which only utilizes the position information using positional encoding; (2). previous-document-based embedding (PREDOC), which takes a simple average of all the document features in \([:,k]\); (3). the concatenation of the position and the average document features up to position \(k\) (POS + PREDOC), as well as the proposed learnable state representations based on multi-head self-attention (ATTENTION). ORACLE here is used to show the gap from the upper bound. The results of our experiments are presented in Table 2 (with a full table including other click models is shown in Appendix B). For the PBM click model, it is evident that state embeddings utilizing position-based information, such as POS and POS+PREDOC, outperform other state embeddings. In contrast, for the CASCADE click model, state embeddings utilizing previous document features exhibit significantly stronger performance compared to those utilizing position information. Notably, our method, CUOLR, which dynamically learns the state embeddings during policy optimization, consistently achieves comparable performance compared to using hard-coded fixed state embeddings. This highlights the necessity of leveraging state representation in off-policy LTR and underscores the effectiveness of our proposed approach.

## 6 Conclusion

In this paper, we present an off-policy learning-to-rank formulation from the perspective of reinforcement learning. Our findings demonstrate that under this novel MDP formulation, RL algorithms can effectively address position bias and learn the optimal ranker for various click models, without the need for complex debiasing methods employed in unbiased learning to rank literature. This work establishes a direct connection between reinforcement learning and unbiased learning to rank through a concise MDP model. Specifically, we propose a novel off-policy learning-to-rank algorithm, CUOLR, which simultaneously learns efficient state representations and the optimal policy. Through empirical evaluation, we show that CUOLR achieves robust performance across a wide range of click models, consistently surpassing existing off-policy learning-to-rank methods tailored to those specific models. These compelling observations indicate that the extensive research conducted on offline

    & &  &  \\  CLICK MODEL & STATE EMBED &  &  &  &  \\   & & K=5 & K=10 & K=5 & K=10 & K=5 & K=10 & K=5 & K=10 \\   & POS & 0.439 & 0.455 & 0.691 & 0.742 & **0.282** & **0.304** & **0.382** & **0.407** \\  & PREDOC & 0.435 & 0.451 & 0.682 & 0.734 & 0.274 & 0.295 & 0.374 & 0.401 \\  & POS + PREDOC & 0.440 & 0.456 & 0.685 & 0.734 & 0.277 & 0.298 & 0.375 & 0.400 \\  & ATTENTION & **0.459** & **0.478** & **0.700** & **0.753** & 0.281 & 0.303 & 0.380 & 0.406 \\   & POS & 0.426 & 0.442 & 0.668 & 0.724 & 0.260 & 0.283 & 0.360 & 0.391 \\  & PREDOC & 0.456 & 0.472 & **0.703** & **0.754** & 0.272 & 0.293 & 0.373 & 0.400 \\   & POS+PREDOC & 0.453 & 0.470 & 0.686 & 0.740 & 0.275 & 0.296 & 0.373 & 0.396 \\   & ATTENTION & **0.461** & **0.478** & 0.696 & 0.748 & **0.280** & **0.301** & **0.378** & **0.404** \\   & ORACLE & **0.462** & **0.486** & **0.739** & **0.781** & **0.328** & **0.347** & **0.432** & **0.453** \\   

Table 2: Performance of CUOLR algorithm with different state embeddings. The experiments are conducted on Yahoo! LETOR set1 and MSLR-WEB10K, with PBM and CASCADE as click models.

reinforcement learning can be leveraged for learning to rank with biased user feedback, opening up a promising new area for exploration.