# Towards Exact Gradient-based Training on

Analog In-memory Computing

 Zhaoxian Wu

Rensselaer Polytechnic Institute

Troy, NY 12180

wuz16@rpi.edu

&Tayfun Gokmen

IBM T. J. Watson Research Center

Yorktown Heights, NY 10598

tgokmen@us.ibm.com

&Malte J. Rasch

IBM T. J. Watson Research Center

Yorktown Heights, NY 10598

malte.rasch@googlemail.com

&Tianyi Chen

Rensselaer Polytechnic Institute

Troy, NY 12180

chentiany119@gmail.com

###### Abstract

Given the high economic and environmental costs of using large vision or language models, analog in-memory accelerators present a promising solution for energy-efficient AI. While inference on analog accelerators has been studied recently, the training perspective is underexplored. Recent studies have shown that the "workhorse" of digital AI training - stochastic gradient descent (SGD) algorithm _converges inexactly_ when applied to model training on non-ideal devices. This paper puts forth a theoretical foundation for gradient-based training on analog devices. We begin by characterizing the non-convergent issue of SGD, which is caused by the asymmetric updates on the analog devices. We then provide a lower bound of the asymptotic error to show that there is a fundamental performance limit of SGD-based analog training rather than an artifact of our analysis. To address this issue, we study a heuristic analog algorithm called Tiki-Taka that has recently exhibited superior empirical performance compared to SGD. We rigorously show its ability to _converge to a critical point exactly_ and hence eliminate the asymptotic error. The simulations verify the correctness of the analyses.

## 1 Introduction

Large vision or language models have recently achieved great success in various applications. However, training large models from scratch requires prolonged durations and substantial energy consumption, which is very costly. For example, it took SS2.4 million to train LLaMA  and SS4.6 million to train GPT-3 . To overcome this issue, a promising technique is application-specific hardware accelerators for neural networks, such as TPU , NPU , and NorthPole chip , just to name a few. Within these accelerators, the memory and processing units are physically split, which requires constant data movements between the memory and processing units. This slows down computation and limits efficiency. In this context, we focus on _analog in-memory computing (AIMC) accelerators_ with resistive crossbar arrays  to accelerate the ubiquitous matrix-vector multiplications (MVMs), which contribute to a significant portion of digital computation in model training and inference, e.g., about 80% in VGG16 model . Very recently, the first analog AI chip has been fabricated in IBM's Albany Complex , and has achieved an accuracy of 92.81% on theCIFAR10 dataset while enjoying 280\(\) and 100\(\) efficiency and throughput value compared with the most-recent GPUs, demonstrating the transformative power of analog computing for AI .

In AIMC accelerators, the trainable matrix weights are represented by the conductance of the _resistive elements_ in an analog crossbar array [13; 14]. Unlike standard digital devices such as GPU or TPU, trainable matrices, input, and output of MVMs in resistive crossbars are all _analog signals_, which means that they are effectively continuous physical quantities and are not quantized. In resistive crossbars, the fundamental physics (mainly Kirchhoff's and Ohm's laws) enable the devices to accelerate MVMs in both forward and backward computations. However, the analog representation of model weights requires updating weights in their unique way. To this end, an in-memory _pulse update_ has been developed in , which changes the weights by sending consecutive pulses to implement gradient-based training algorithms like stochastic gradient descent (SGD). Pulse update reduces energy consumption and execution time significantly.

While the AIMC architecture has potential advantages, the analog signal in resistive crossbar devices is susceptible to noise and other non-ideal device characteristics , leading to training performance that is often suboptimal when compared to digital counterparts. Despite the increasing number of empirical studies on AIMC that aim to overcome this accuracy drop [17; 18; 19; 20], there is still a lack of theoretical studies on the performance and performance limits of SGD on AIMC accelerators.

### Main results

The focus of this paper is fundamentally different from the vast majority of work in analog computing. We aim to build a rigorous theoretical foundation of analog training, which can uniquely characterize the performance limit of using a hardware-agnostic digital SGD algorithm for analog training and establish the convergence rate of gradient-based analog training. Consider the following standard model training problem

\[W^{*}:=*{arg\,min}_{W^{D}}\ f(W)\] (1)

where \(f():^{D}\) is the objective function, \(W\) is all the trainable weight stored in an analog way and \(D\) is the model size.

In digital training, the workhorse algorithm for solving problem (1) is SGD, which iteratively updates weight \(W\) via the following recursion

\[ W_{k+1}=W_{k}-( f(W_{k} )+_{k})\] (2)

where \(k\) is the iteration index, \(\) is a positive learning rate and \(_{k}\) is the gradient noise with zero mean at iteration \(k\). In digital training, the noise usually comes from the mini-batch sampling. In analog training, the noise also arises from the non-ideality of devices, including weight read noise, input/output noise, quantization noise, digital/analog conversion noise, and even thermal noise .

In Figure 1, we first numerically show that the asymptotic performance of SGD running on the analog devices that we term Analog SGD does not follow that of (2), and thus ask a natural question:

**Q1)**_How to better characterize the training trajectory of SGD on analog devices?_

Building upon the pulse update in [15; 22], in this paper, we propose the following discrete-time mathematical model to characterize the trajectory of Analog SGD on analog computing devices

\[ W_{k+1}=W_{k}-( f(W_{k})+_{k}) -| f(W_{k})+_{k}| W_{k}\] (3)

where \(||\) and \(\) represent the coordinate-wise absolute value and multiplication, respectively and \(\) is a device-specific parameter. We will explain the underlying rationale of this model (3) in Section 2.2. But before that, compared to (2), the extra term of (3) comes from the asymmetric update of analog devices. Following this dynamic, we prove that the Analog SGD only converges inexactly to a critical point, with the asymptotic error depending on the non-ideality of the analog device. Since the inexact convergence guarantee usually leads to an unfavorable result, it raises another question:

Figure 1: Digital/Analog SGD under different learning rates.

**Q2)** _How to mitigate the asymptotic error induced by the asymmetric analog update?_

To answer this, we revisit a heuristic algorithm Tiki-Taka that has been used among experimentalists , and establish the _first exact convergence_ result of Tiki-Taka on a class of AIMC accelerators.

**Our contributions.** This paper makes the following contributions (see the comparison in Table 1):

* We demonstrate that Analog SGD does not follow the dynamic of SGD in (2). Leveraging the underlying physics, we propose a discrete-time dynamic of the gradient-based algorithm on analog devices, and show that it better characterizes the trajectory of Analog SGD.
* Based on the proposed dynamic, we establish the convergence of Analog SGD and argue that the performance limit of Analog SGD is a combined effect of data noise and device asymmetry. We prove the tightness of our result by showing a matching lower bound.
* To improve the performance limit of analog training, we study a heuristic algorithm Tiki-Taka that serves as an alternative to Analog SGD. We show that Tiki-Taka exactly converges to the critical point by reducing the effect of asymmetric bias and noise.
* To verify the validity of our discrete-time dynamic for analog training and the tightness of our analysis, we provide simulations on both synthetic and real datasets to show that the asymptotic error of Analog SGD does exist, and Tiki-Taka outperforms Analog SGD.

### Prior art

Since the seminal work on pulse update-based Analog SGD, a series of gradient-based training algorithms have been proposed to enable training on analog devices. Despite its potential energy and speed advantage, Analog SGD suffers from asymmetric update and noise issues, leading to large errors in training. To overcome the _asymmetric issue_, a new algorithm so-termed Tiki-Taka (TT-v1) introduces an auxiliary array to estimate the moving average of gradients, whose weight is then transferred to the main array periodically . However, the weight transfer process between arrays still suffers from noise. To deal with this issue, TT-v2  introduces an extra digital array to filter out the high-frequency noise. Enabled by these approaches, researchers successfully trained a model on the realistic prototype analog devices and reached comparable performance on a real dataset .

Another major hurdle comes from _noisy analog signals_, which can perturb the gradient computed by analog devices. As an alternative, a hybrid scheme that accelerates the forward and backward pass in-memory and computes gradient in the digital domain has been proposed in [17; 18], which provides a more accurate but less efficient update. In addition, the successful training by TT-v1 or TT-v2 relies on the zero-shifting technique , which corrects the symmetric point of devices as zero. However, the correction is inaccurate because it also involves analog signals. To deal with this issue, Chopped-TTv2 (c-TTv2) and analog gradient accumulation with dynamic reference have been proposed in . Because of these efforts, analog training has empirically shown great promise in achieving a similar level of accuracy as digital training, with reduced energy consumption and training time. Despite its good performance, it is still mysterious about when and why they work.

   Algorithm & Rate & Asymptotic Error \\  Digital SGD  & \(O(}{K}})\) & 0 \\ Analog SGD [Theorem 2] & \(O(}{K}}^{2}/^{2}})\) & \(O(^{2}S_{K})\) \\ Tiki-Taka [Theorem 4] & \(O(}{K}}^{2}/^{2}})\) & 0 \\ Lower bound  & \(O(}{K}})\) & 0 \\   

Table 1: Comparison between the convergence of digital and analog training algorithms: \(K\) represents the number of iterations, \(^{2}\) is the variance of stochastic gradients, \(W_{}^{2}/^{2}\) and \(P_{}^{2}/^{2}\) measure the saturation degree, and \(S_{K}\) measures the non-ideality of analog devices (c.f. Theorem 2). Asymptotic error refers to the error that does not vanish with \(K\).

The Physics of Analog Training

Unlike digital devices, analog devices represent the trainable weights by the conductance of the base materials, which have undergone offset and scaling due to _physical laws_. This difference leads to entirely different training dynamics on analog devices, which will be discussed in this section.

### Revisit SGD theory and its failure in modeling analog training

This section shows that the convergence theory developed for digital SGD fails to characterize the analog training. Before that, we introduce standard assumptions for analyzing digital training.

**Assumption 1** (\(L\)-smoothness).: _The objective \(f(W)\) is \(L\)-smooth, i.e., for any \(W,W^{}^{D}\), it holds_

\[\| f(W)- f(W^{})\| L\|W-W^{}\|.\] (4)

**Assumption 2** (Lower bounded).: \(f(W)\) _is lower bounded by \(f^{*}\), i.e. \(f(W) f^{*}, W^{D}\)._

**Assumption 3** (Noise mean and variance).: _The noise \(_{k}\) is independently identically distributed (i.i.d.) for \(k=0,1,,K\), and has zero mean and bounded variance, i.e., \([]=0,[\|\|^{2}]^{2}\)._

In non-convex optimization, instead of finding a global minimum, it is usually more common to find a _critical point_, which refers to \(W^{*}\) satisfying \( f(W^{*})=0\). Under Assumptions 1-3, if \(W_{k}\) follows the digital SGD dynamic (2), the convergence of SGD is well-understood [23, Theorem 4.8]

\[_{k=0}^{K-1}\| f(W_{k})\|^{2}\ )-f^{*})}{  K}+^{2}L.\] (5)

This result implies that the impact of noise can be controlled by the learning rate \(\), i.e., after \(K(1/^{2})\) iterations, the error is \(O()\). By forcing \( 0\), the error will reduce to zero.

**Analog SGD violates the SGD theory.** The dynamic of digital SGD exactly follows (2) up to the machine's precision. To verify whether analog training adheres to the same dynamic, we conduct a numerical simulation on a least-squares problem. We compare Analog SGD implemented by an analog devices simulator, IBM Analog Hardware Acceleration Kit (AIHWKit) , and digital SGD implemented by PyTorch. The same level of noise obeying Gaussian noise is injected into each algorithm. Beginning from a large learning rate \(=0.2\), we reduce the learning rate by half each time and observe the convergences. As Figure 1 illustrates, digital SGD behaves as what (5) predicts: it converges with a smaller error when a small learning rate is chosen. On the contrary, Analog SGD converges with a much larger error, which does not decrease as the learning rate decreases. This result demonstrates the discrepancy between the theory of digital SGD and the performance of Analog SGD. More details are deferred to Appendix I.

### Training dynamic on analog devices

Compared to digital devices, the key feature of analog devices is _analog signal_. The input and output of analog arrays are analog signals, which are prone to be perturbed by noise, including read noise and input/output noise . Moreover, real training typically involves the utilization of mini-batch samples, which also introduces noise. Besides the data noise, another notable feature of analog accelerators is that the model weight is represented by material conductance.

**Pulse update.** To change the weights in analog devices, one needs to send an electrical _pulse_ to the resistive element, and the conductance will change by a small value, which is referred to as _pulse update_. To apply an update \( w\) to the weight \(w\), using the pulse update needs to send a series of pulses to the resistive element, the number of which is proportional to the update magnitude \(| w|\). Since the increments responding to each pulse are small typically, we can regard the change in conductance as a continuous process. Consequently, common operations involving significant weight changes, like copying the weight, are expensive in analog accelerators. In contrast, gradient-based algorithms typically update small amounts at each iteration, rendering the pulse update extremely efficient. Figure 2 presents the weight change on AIMC devices with pulse number.

**Asymmetric update.** Even though the pulse update is performed efficiently inside AIMC accelerators, it suffers from a phenomenon that we refer to as _asymmetric update_. This means that if we apply the change \( w>0\) and \( w<0\) on the same weight \(w\), the amount of weight change will be different. Considering the weight \(W_{k}\) at time \(k\) and the expected update \( W^{D}\), we express the asymmetric update as \(W_{k+1}=U(W_{k}, W)\) with the update function \(U:\) defined by1

\[U(w, w):=w+ w q_{+}(w),& w 0,\\ w+ w q_{-}(w),& w<0,\] (6)

where \(q_{+}()\) and \(q_{-}():_{+}\) are up and down response factors, respectively. The _response factors_ measure the ideality of analog tiles. In the ideal situation, \(q_{+}(w)=q_{-}(w) 1\) (see Figure 2, left), and analog algorithms have the same numerical behavior of the digital ones. Defining the symmetric and asymmetric components as \(F(w):=(q_{-}(w)+q_{+}(w))\) and \(G(w):=(q_{-}(w)-q_{+}(w))\), the update in (6) can be expressed in a more compact form \(U(w, w)=w+ w F(w)-| w| G(w)\). For simplicity, assume that all the coordinates of \(W\) use the same update rule, and the analog update can be written as

\[W_{k+1}=W_{k}+ W F(W_{k})-| W| G(W_{k})\] (7)

where \(||\) and \(\) represent the coordinate-wise absolute value and multiplication, respectively. Note that in (7), the ideal weight update \( W\) is algorithm-specific: \( W\) is the gradient in Analog SGD while it is an auxiliary weight (c.f. in Section 4) in Tiki-Taka.

**Remark 1** (Physical constraint).: _It is attempting to scale the \( w\) by \(q_{+}(w)\) or \(q_{-}(w)\) to cancel the effect of asymmetric update in (6) dynamically. However, this is impractical since it is hard to implement the reading and scaling at the same time on analog tiles ._

**Symmetric point.** The asymmetric update makes up and down responses different almost everywhere, i.e. \(q_{+}(w) q_{-}(w)\) for almost any \(w\). If a point \(w^{}\) satisfies \(q_{+}(w^{})=q_{-}(w^{})\) and \(G(w)=0\), \(w^{}\) is called a _symmetric point_. With loss of generality, the response factor is defined so that \(F(w^{})=1\). Therefore, near the symmetric point, the update \( w\) can be accurately applied on \(w\), i.e., \(U(w^{}, w) w^{}+ w\). If all the coordinates of matrix \(W_{k}\) hover around the symmetric point, the analog devices can exhibit performance that resembles the digital ones. In the next section, we will show that the weight is biased toward its symmetric point.

**Asymmetric linear device.** Although our unified formulation (7) can capture the response behaviors of different materials, this paper mainly focuses on the behaviors of the asymmetric linear device (ALD), similar to the setting in . ALD has a positive parameter \(\) which reflects the degree of asymmetry and its response factors are written as linear functions \(q_{+}(w)=1-(w-w^{})/,q_{-}(w)=1+(w-w^{})/\). Consequently, ALD has \(F(w)=1\), \(G(w)=(w-w^{})/\), and symmetric point \(w^{}\); see Figure 2, right. Even though ALD is a simplified device model, it is representative enough to

Figure 2: The weight’s change with the number of pulses. Positive and negative pulses are sent continuously on the left and right half, respectively. Beginning from \(w\), the weight after applying update \( w\) to it is \(w^{+}\) or \(w^{-}\) if \( w 0\) or \( w<0\), respectively. The response factors \(q_{+}(w)\) and \(q_{-}(w)\) are approximately the slope of the curve at \(w\). **(Left)** Ideal device. \(q_{+}(w)=q_{-}(w) 1\). Every point is symmetric points. **(Right)** Asymmetric Linear Device (ALD). \(q_{+}(w)=1-(w-w^{})/,q_{-}(w)=1+(w-w^{})/\). The symmetric point \(w_{}\) satisfies \(q_{+}(w^{})=q_{-}(w^{})\).

reveal the key properties and difficulties of gradient-based training algorithms on analog devices. If not otherwise specified, \(w^{}\) is always 0 for simplicity. In summary, the update of ALD is expressed as

\[w_{k+1}=w_{k}+ w-| w| w_{k} W_{k+1}=W_{k}+ W-| W| W_{k}\] (8)

where the first equation is the update of one coordinate while the second one stacks all the elements together. Replacing \( W\) with noisy gradient \(( f(W_{k})+_{k})\) reaches the dynamic (3).

### Saturation, fast reset, and bounded weight

Based on the ALD dynamic (8), we study the properties of analog training, which serve as the stepping stone of our analyses. Recall that the asymmetric update leads to different magnitudes of increase and decrease. The following lemma characterizes the difference between two directions.

**Lemma 1** (Saturation and fast reset).: _For ALD with a general \(w^{}\), the following statements are valid_

_(Saturation) If \(( w)=(w_{k})\), it holds that \(|w_{k+1}-w_{k}|=|1-|w_{k}-w^{}|/|| w|\)._

_(Fast Reset) If \(( w)=-(w_{k})\), it holds that \(|w_{k+1}-w_{k}|=|1+|w_{k}-w^{}|/|| w|\)._

The proof is deferred to Section E.1. Remarkably, Lemma 1 holds for any algorithm with any update \( w\), and it reveals the impact of the asymmetric update. In principle, Lemma 1 can be written as \(|w_{k+1}-w_{k}|=|1|w_{k}-w^{}|/|| w|\), where the symmetric point \(w^{}=0\) is omitted.

When \(w_{k}\) is not at the symmetric point \(w^{}=0\), the update is scaled by a factor. When \(w_{k}\) lies around the symmetric point, \(|w_{k+1}-w_{k}|| w|\), where all updates are applied to the weight and hence the analog devices closely mimic the performance of digital devices. When \(w_{k}\) moves away from \(w^{}\), i.e. \(( w)=(w_{k})\), the update becomes none. When \(w_{k}\) gets closer to \(\), nearly no update can be applied. This phenomenon is called _saturation_; see also . On the contrary, \(w_{k}\) changes faster when it moves toward \(w^{}\), which is referred to as _fast reset_.

Because of the saturation property, the weight on the analog devices is intrinsically bounded, which will be helpful in the later analysis. The following theorem discusses the bounded property.

**Theorem 1** (Bounded weight).: _Denote \(\|W\|_{}\) as the \(_{}\) norm of \(W\). Given \(\|W_{0}\|_{}\) and for any sequence \(\{ W_{k}:k\}\), which satisfies \(\| W_{k}\|_{}\), it holds that \(\|W_{k}\|_{},\, k\)._

The proof is deferred to Section E.2. Theorem 1 claims that the weight is guaranteed to be bounded, even without explicit projection, which makes analog training different from its digital counterpart. Similar to Lemma 1, Theorem 1 does not depend on any specific analog training algorithm.

## 3 Performance Limits of Analog Stochastic Gradient Descent

After modeling the dynamic of analog training, we next discuss the convergence of Analog SGD. As shown by Lemma 1, the update is slow near the boundary of the active region. The weight is expected to stay within a smaller region to avoid saturation, which necessitates the following assumption.

**Assumption 4** (Bounded saturation).: _There exists a positive constant \(W_{}<\) such that the weight \(W_{k}\) is bounded in \(_{}\) norm, i.e., \(\|W_{k}\|_{} W_{}\). The ratio \(W_{}/\) is the saturation degree._

Assumption 4 requires that \(W_{k}\) is bounded inside a small region, which is a mild assumption in real training. For example, one can apply a clipping operation on \(w_{k}\) to ensure the assumption. In Appendix E.2, we show that Assumption 4 provably holds under the strongly convex assumption. It is worth pointing out that Assumption 4 implicitly assumes there are critical points in the box \(\{W:\|W\|_{}\}\). Otherwise, the gradient will push the weight to the bound of the box and it becomes possible that \(W_{}<\|W_{k}\|_{}\).

Intuitively, without the asymmetric bias, the weight is stable near the critical point. In contrast, with asymmetric bias, the noisy gradient that pushes \(w_{k}\) toward its symmetric point is amplified by fast reset, while the one that drags \(w_{k}\) away from \(0\) is suppressed by saturation (c.f. Lemma 1). Consequently, the weight \(w_{k}\) is attracted by \(0\), which prevents the stability of Analog SGD around the critical point. We characterize the convergence of Analog SGD in the following theorem.

**Theorem 2** (Convergence of Analog SGD).: _Under Assumption 1-4, if the learning rate is set as \(=)-f^{*}}{^{2}LK}}\) and \(K\) is sufficiently large such that \(\), it holds that_

\[_{k=0}^{K-1}[\| f(W_{k})\|^{2}] O ()-f^{*})^{2}L}{K}}^{2}/ ^{2}})+^{2}S_{K}\] (9)

_where \(S_{K}\) denotes the amplification factor given by \(S_{K}:=_{k=0}^{K}\|_{}^{2}/^{2}}{1-\|W_{ k}\|_{}^{2}/^{2}}^{2}/^{2}}{1-W_{}^{2}/ ^{2}}\)._

The proof of Theorem 2 is deferred to Appendix G.1. Theorem 2 suggests that the average squared gradient norm is upper bounded by the sum of two terms: the first term vanishes at a rate of \(O(/K})\) which also appears in the SGD's convergence bound (5); the second term contributes to the _asymptotic error_ of Analog SGD, which does not vanish with the total number of iterations \(K\); that is, \(_{K}_{k=0}^{K-1}[\| f(W_{k})\|^ {2}]^{2}S_{}\) exist.

**Impact of saturation/asymmetric update.** The saturation degree \(W_{}/\) affects both convergence rate and asymptotic error. The ratio is small if \(W_{k}\) remains close to the symmetric point or \(\) is sufficiently large. The exact expression of \(S_{K}\) depends on the specific noise distribution, and thus is difficult to reach. However, \(S_{K}\) reflects the saturation degree near the critical point \(W^{*}\) when \(W_{k}\) converges to a neighborhood of \(W^{*}\). Intuitively, \(W_{k} W^{*}\) implies \(S_{K}\|_{}^{2}/^{2}}{1-\|W^{*}\|_{}^{2}/ ^{2}}\). Therefore, if the critical point is near the symmetric point, the asymptotic error \(S_{K}\) could be small. The asymmetric update has a negative impact on both rate and error. It slows down the convergence of SGD by a factor \(1/(1-W_{}^{2}/^{2})\), and, meanwhile, a smaller \(\) increases the asymptotic error.

To demonstrate the asymptotic error in Theorem 2 is not artificial, we provide a lower bound next.

**Theorem 3** (Lower bound of the error of Analog SGD).: _There is an instance which satisfies Assumption 1-4 such that Analog SGD generates a sequence \(\{W_{k}:k=0,1,,K\}\) which satisfies_

\[_{k=0}^{K-1}[\| f(W_{k})\|^{2}]= ^{2}S_{K}+()} )}{=}(^{2}S_{K}+}).\] (10)

The proof of Theorem 3 is deferred to Appendix G.2. Theorem 3 implies that \(^{2}S_{K}\) in the right-hand side (RHS) of (9) can not be improved and therefore the lower bound of the asymptotic error. It demonstrates that the presence of asymptotic error is intrinsic and not an artifact of the convergence analysis. The nonzero asymptotic error also reveals the fundamental performance limits of using Analog SGD for analog training, pointing out a new venue for algorithmic development.

## 4 Eliminating Asymptotic Error of Analog Training: Tiki-Taka

Building upon our understanding on the modeling of gradient-based analog training in Section 2 and the asymptotic error of Analog SGD in Section 3, this section will be devoted to understanding means to overcome such asymptotic error in analog training.

We will focus on our study on a heuristic algorithm Tiki-Taka that has shown great promise in practice . The key idea of Tiki-Taka is to maintain an auxiliary array to estimate the true gradient. To be specific, Tiki-Taka introduces another analog device, \(P_{k}\), besides the main one, \(W_{k}\). At the initialization phase, \(P_{k}\) is initialized as \(0\). At iteration \(k\), the stochastic gradient is computed using the main device \(W_{k}\) and is first applied to \(P_{k}\). Since \(P_{k}\) is also an analog device, the change of \(P_{k}\) still follows the dynamic (8) by replacing \(W_{k}\) with \(P_{k}\) and \( W\) with \( f(W_{k})+_{k}\), that is

\[P_{k+1}=P_{k}+( f(W_{k})+_{k})-|  f(W_{k})+_{k}| P_{k}\] (TT-P)

where \(\) is a learning rate. After that, the value \(P_{k+1}\) is read and transferred to the main array \(W_{k}\) via

\[W_{k+1}=W_{k}- P_{k+1}-|P_{k+1}| W_{k}.\] (TT-W)Tiki-Taka performs recursion (TT-P) and (TT-W) alternatively until it converges. Empirically, Tiki-Taka outperforms Analog SGD in terms of the final accuracy. However, Tiki-Taka is a heuristic algorithm, and there are no convergence guarantees so far. In this section, we demonstrate that the improvement of Tiki-Taka stems from its ability to eliminate asymptotic errors.

**Stability of Tiki-Taka.** As explained in Section 3, the gradient noise contributes to the asymptotic error of Analog SGD. To eliminate the error, the idea under Tiki-Taka is to reduce the noise impact. To see how Tiki-Taka reduces the noise, consider the case where \(W_{k}\) is already a critical point and \(P_{k}\) is initialized as \(0\), i.e. \( f(W_{k})=P_{k}=0\). After one iteration, the weight \(W_{k}\) drifts because of the noise. For Analog SGD, the expected drift is

\[[W_{k+1}]-W_{k}=-[_{k}]- [|_{k}|] W_{k}=-[|_{k}|] W_{k}.\] (11)

In contrast, Tiki-Taka updates the auxiliary array by \(P_{k+1}=P_{k}+_{k}-|_{k}| P _{k}=_{k}\), which implies \([P_{k+1}]=0\) and \([|P_{k+1}|]=[|_{k}|]\). After the transfer, its expected drift is

\[[W_{k+1}]-W_{k}=-[P_{k+1}]-[|P_{k+1}|] W_{k}=-[ |_{k}|] W_{k}.\] (12)

Comparing (12) with (11), it can be observed that Tiki-Taka improves the expected drift from \(O()\) to \(O()\). With sufficiently small \(\), Tiki-Taka controls the drift and makes the weight stay at the critical point. For a more generic scenarios, \(P_{k} 0\). However, it is worth noting that

\[[P_{k+1}]=[P_{k}+_{k}-|_{k}| P_{k}]=(- [|_{k}|]) P_{k}\] (13)

which makes \(P_{k}\) back to \(0\) when \([|_{k}|] 0\). Therefore, by controlling the drift, the Tiki-Taka algorithm manages to stay near a critical point.

Note that from (13), the stability of \(P_{k}\) relies on the presence of noise, i.e. \([|_{k}|] 0\). In addition to the upper bound on the noise in Assumption 3, a lower bound for the noise is also assumed.

**Assumption 5** (Coordinate-wise i.i.d. and non-zero noise).: _For any \(k 0\) and \(i,j\), \([_{k}]_{i}\) and \([_{k}]_{j}\) are i.i.d. from a distribution \(_{c}\) which ensures \(_{[_{k}]_{i}_{c}}[_{k}]=0\). Furthermore, there exists \(>0\) and \(c>0\) such that \(_{[_{k}]_{i}_{c}}[[_{k}]_{i}^{ 2}]^{2}/D\) and \(_{[_{k}]_{i}_{c}}[|g+[_{k}]_{i }|] c\) for any \(g\)._

Intuitively, Assumption 5 requires the non-zero noise, which is mild since the random sampling and the physical properties of analog devices always introduce noise. The factor \(D\) in the denominator makes it consistent with Assumption 3. We discuss this assumption in more detail in Appendix D.2.

**Theorem 4** (Convergence of Tiki-Taka).: _Suppose Assumption 1-5 hold and the learning rate is set as \(=O(1/K})\), \(=8 L\). It holds for Tiki-Taka that the expected infinity norm \(P_{k}\) is upper bounded by \([\|P_{k+1}\|_{}^{2}] P_{}^{2}:=^{4}D} {c^{2}^{2}}\). Furthermore, if \(^{2}\) and \(D\) are sufficiently large so that \(33P_{}^{2}/^{2}<1\) it is valid that_

\[_{k=0}^{K-1}[\| f(W_{k})\|^{2}] O ()-f^{*})^{2}L}{K}}^{2}/ ^{2}}).\] (14)

The proof of Theorem 4 is deferred to Appendix H. Theorem 4 first provides the upper bound for the maximum magnitude of \(P_{k}\) that decreases as the variance \(^{2}\) increases or \(\) decreases. This observation is consistent with (13), which implies the \(P_{k}\) tends to zero when \([|_{k}|] 0\). Ensuring stability during the training requires the noise to be sufficiently large to render the saturation degree of \(P_{}/\) sufficiently small. In addition, the condition \(33P_{}^{2}/^{2}<1\) requires the \(D\) to be sufficiently large, which is easy to meet when training large models.

**Convergence rate.** Theorem 4 claims that Tiki-Taka converges at the rate \(O(L}{K}}^{2}/^{2}})\). Therefore, we reach the conclusion that \(_{K}_{k=0}^{K-1}[\| f(W_{k})\|^{ 2}]=0\) and Tiki-Taka eliminates the asymptotic error of Analog SGD. Furthermore, Tiki-Taka improves the factor \(1/(1-W_{}^{2}/^{2})\) in Analog SGD's convergence (c.f. (9)) to \(1/(1-33P_{}^{2}/^{2})\), wherein \(W_{}^{2}/^{2}\) and \(P_{}^{2}/^{2}\) are the saturation degrees in fact. Notice that \(P_{k}\) tends to \(0\) as indicatedby (13) while \(W_{}\) does not because \(0\) is usually not a critical point. Therefore, it usually has \(P_{}^{2}/^{2} W_{}^{2}/^{2}\) in practice, implying faster convergence. The convergence matches the lower bound \(O(L/K})\) for general stochastic non-convex smooth optimization  up to a constant.

## 5 Numerical Simulations

In this section, we verify the main theoretical results by simulations on both synthetic datasets and real datasets. We use the PyTorch to generate the curves for SGD in the simulation and use open source toolkit AIHWKit to simulate the behaviors of Analog SGD; see github.com/IBM/aihwkit. Each simulation is repeated three times, and the mean and standard deviation are reported. More details can be referred to in Appendix I. The code of our simulation implementation is available at github.com/Zhaoxian-Wu/analog-training.

### Verification of the analog training dynamic

To verify that the proposed dynamic (3) characterizes analog training better than SGD dynamic (2), we conduct a numerical simulation on a least-squares task, and compare Analog SGD implemented by AIHWKit, the digital and analog dynamics given by (2) and (3), respectively; see Figure 3. The results show that the proposed dynamic provides an accurate approximation of Analog SGD.

### Ablation study on the asymptotic training error

We verify some critical claims about the asymptotic error of Analog SGD on a least-squares task.

**Impact of \(\)**. To verify Theorem 2 that the error diminishes with a larger \(\), we assign a range of value \(\) and plot the convergence of Analog SGD. The result is reported on the Left of Figure 4. When \(\) is small, the asymmetric bias introduces a notable gap between Analog and Digital SGD. As \(\) increases, the gap diminishes. The result demonstrates the asymptotic error decreases as \(\) increases.

**Impact of \(^{2}\)**. To verify the asymptotic error is proportional to \(^{2}\), we inject noise with different variances. The result is reported in the middle of Figure 4. The result illustrates that the asymptotic error increases as the noise increases.

Figure 4: **(Left)** The convergence of Analog SGD under different \(\). Reducing \(\) leads to a decrease in asymptotic error. When \(\) is sufficiently large, Analog SGD tends to have a similar performance to digital SGD. **(Middle)** The convergence of Analog SGD on noise devices under different \(^{2}\). **(Right)** Analog SGDs that are initialized to different places converge to the same error.

Figure 3: The convergence of digital SGD dynamic (2), analog dynamic (3) (proposed) and Analog SGD implemented by AIHWKit (real behavior) under different \(\).

**Impact of the initialization**. To demonstrate the asymptotic error is not artificial, we perform Analog SGD from different initializations. The result is reported on the right of Figure 4. The result illustrates that Analog SGD converges to a similar location regardless of the initialization. The smooth convergence curve ensures the error comes from bias instead of limited machine precision. Therefore, the asymptotic error is intrinsic and independent of the initialization.

### Analog training performance on real dataset

We also train vision models to perform image classification tasks on real datasets.

**MNIST FCN/CNN.** We train Fully-connected network (FCN) and convolution neural network (CNN) models on MNIST dataset and see the performance of Analog SGD and Tiki-Taka under various \(\). The results are reported in Figure 5. By reducing the variance, Tiki-Taka outperforms Analog SGD and reaches comparable accuracy with digital SGD. On both of the architectures, the accuracy of Tiki-Taka drops by \(<1\%\). In the FCN training, Analog SGD achieves acceptable accuracy on \(=0.78\) and \(=0.80\) but converges much more slowly. In the CNN training, the accuracy of Analog SGD always drops by \(>6\%\).

**CIFAR10 Resnet.** We also train three Resnet models with different sizes on CIFAR10 dataset. The last layer is replaced by a fully-connected layer mapped onto an analog device with parameter \(=0.8\). The results are shown in Table 2. In this task, Analog SGD does not suffer from a significant accuracy drop but is still worth that Tiki-Taka, A surprising observation for analog training is that both Analog SGD and Tiki-Taka outperform Digital SGD. We conjecture it happens because the noise introduced by analog devices makes the network more robust to outlier data.

## 6 Conclusions and Limitations

This paper points out that Analog SGD does not follow the dynamic of digital SGD and hence, we propose a better dynamic to formulate the analog training. Based on this dynamic, we studies the convergence of two gradient-based analog training algorithms, Analog SGD and Tiki-Taka. The theoretical results demonstrate that Analog SGD suffers from asymptotic error, which comes from the noise and asymmetric update. To overcome this issue, we show that Tiki-Taka is able to stay in the critical point without suffering from an asymptotic error. Numerical simulations demonstrate the existence of Analog SGD's asymptotic error and the efficacy of Tiki-Taka. One limitation of this work is that the current analysis is device-specific that applies to asymmetric linear device. While it is an interesting and popular analog device, it is also important to extend our convergence analysis to more general analog devices and develop other device-specific analog algorithms in future work.

    & Resnet18 & Resnet34 & Resnet50 \\  Digital SGD & 93.03 & 93.44 & 95.92 \\ Analog SGD & 93.58 & 93.58 & 95.51 \\ Tiki-Taka & 93.74 & 95.15 & 95.54 \\   

Table 2: The test accuracy of Resnet training on CIFAR10 dataset after 100 epochs.

Figure 5: The test accuracy curves and tables for the model training. “D SGD”, “A SGD”, and “TT” represent Digital SGD, Analog SGD and Tiki-Taka, respectively; **(Left)** FCN. **(Right)** CNN.