# DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization

Haowei Zhu1\({}^{1}\)2, Dehua Tang\({}^{1}\), Ji Liu\({}^{1}\), Mingjie Lu\({}^{1}\), Jintu Zheng\({}^{1}\), Jinzhang Peng\({}^{1}\), Dong Li\({}^{1}\), Yu Wang\({}^{1}\), Fan Jiang\({}^{1}\), Lu Tian\({}^{1}\), Spandan Tiwari\({}^{1}\), Ashish Sirasao\({}^{1}\), Junhai Yong\({}^{2}\),

Bin Wang\({}^{}\)2, Emad Barsoum\({}^{}\)1

\({}^{1}\)Advanced Micro Devices, Inc. \({}^{2}\)Tsinghua University

zhuhw23@mails.tsinghua.edu.cn;

{dehua.tang, ji.liu, jinz.peng, d.li, lu.tian, ebarsoum}@amd.com;

{yongjh, wangbins}@tsinghua.edu.cn

Work done during an internship at AMD.Corresponding author.

###### Abstract

Diffusion models have achieved remarkable progress in the field of image generation due to their outstanding capabilities. However, these models require substantial computing resources because of the multi-step denoising process during inference. While traditional pruning methods have been employed to optimize these models, the retraining process necessitates large-scale training datasets and extensive computational costs to maintain generalization ability, making it neither convenient nor efficient. Recent studies attempt to utilize the similarity of features across adjacent denoising stages to reduce computational costs through simple and static strategies. However, these strategies cannot fully harness the potential of the similar feature patterns across adjacent timesteps. In this work, we propose a novel pruning method that derives an efficient diffusion model via a more intelligent and differentiable pruner. At the core of our approach is casting the model pruning process into a SubNet search process. Specifically, we first introduce a SuperNet based on standard diffusion via adding some backup connections built upon the similar features. We then construct a plugin pruner network and design optimization losses to identify redundant computation. Finally, our method can identify an optimal SubNet through few-step gradient optimization and a simple post-processing procedure. We conduct extensive experiments on various diffusion models including Stable Diffusion series and DiTs. Our DiP-GO approach achieves 4.4\(\) speedup for SD-1.5 without any loss of accuracy, significantly outperforming the previous state-of-the-art methods.

## 1 Introduction

Diffusion models have undergone significant advancements over the past years due to the outstanding capabilities of diffusion probabilistic models (DPMs) . DPMs typically consist of two processes: the noise diffusion process and the reverse denoising process. Given their remarkable superiority in content generation, diffusion models have made significant progress in various fields of general image generation, including text-to-image generation , layout-to-image generation , image editing , and image personalization . Furthermore, diffusion models have contributed to advancements in autonomous driving, ranging from driving dataset generation  to perception model enhancement  through diffusion strategies. However, DPMs often incur considerable computational overhead during both the training and inference phases. The high cost of inference, dueto the multi-step denoising computation during the sampling process, can significantly impact their practical application. Many efforts [15; 16; 17] have been made to improve the efficiency of diffusion models, which can be broadly divided into two types of optimization: inference sampling optimization and model structural optimization.

Sampling optimization methods reduce the number of sampling steps for generation without compromising image quality. For instance, DDIM  reduces these steps by exploring a non-Markovian process without requiring model retraining. LCM [18; 19] enable image generation in fewer steps with retraining requirements. Structural optimization methods [16; 17; 20; 21] aim to reduce computational overhead through efficient model design and model pruning. These methods require retraining the diffusion model, which entails significant computational overhead and large-scale datasets, making them neither convenient nor efficient. DeepCache  proposes a novel training-free paradigm based on the U-Net architecture in diffusion models, caching and retrieving features across adjacent denoising stages to reduce redundant computation costs. However, DeepCache only reuses the output feature of a U-Net block in a denoising step via a simple and static strategy. We believe many intermediate features remain untapped, and the simple static strategy cannot fully exploit the potential of similar feature patterns across adjacent timesteps during inference, as observed in recent studies [15; 18; 22].

To address these challenges, we introduce **D**iffusion **P**runing via Few-step **G**radient **O**ptimization (DiP-GO), a method designed to achieve efficient model pruning with enhanced dynamism and intelligence. Our approach rethinks the diffusion model during inference by proposing a SuperNet based on standard diffusion via adding some backup connections built upon the similar features, conceptualizing the inference process as a specific SubNet derived from our proposed SuperNet. We reformulate the diffusion model pruning into a SubNet search process. By addressing the out-of-memory issue inherent in the backward process during expanded denoising timesteps using the gradient checkpoint  method, we introduce a plugin pruner that discovers an optimal SubNet surpassing existing methods through carefully designed optimization losses. Extensive experiments validate the effectiveness of our approach, demonstrating a 4.4\(\) speedup on Stable Diffusion 1.5. Moreover, our method efficiently prunes the DiT model  without requiring retraining the diffusion model, achieving significant inference speedup. Our contribution can be summarized as follows: (1) We define a SuperNet based on standard diffusion and show how to obtain a SubNet. This transforms the diffusion optimization problem into an efficient SubNet search process without the need for retraining pretrained diffusion models. (2) We design a plugin pruner tailored specifically for diffusion models. This pruner optimizes pruning constraints while maximizing synthesis capability. Additionally, we introduce a post-processing method for the pruner to ensure that the SubNet meets specified pruning requirements. (3) We conduct extensive pruning experiments across various diffusion models, including Stable Diffusion 1.5, Stable Diffusion 2.1, Stable Diffusion XL, and DiT. Extensive experiments demonstrate the superiority of our method, achieving a notable 4.4 \(\) speedup during inference on Stable Diffusion 1.5 without the need for retraining the diffusion model.

## 2 Related Work

### Efficient Diffusion Models

The diffusion models, celebrated for their iterative denoising process during inference, play a pivotal role in content generation but are often hindered by time-consuming operations. To mitigate this challenge, extensive research has focused on accelerating diffusion models. Acceleration efforts typically approach the problem from two primary perspectives:

**Efficient Sampling Methods.** Recent works focus on reducing the number of denoising steps required for content generation. DDIM  achieves this by exploring a non-Markovian process related to neural ODEs. Fast high-order solvers [24; 25] for diffusion ordinary differential equations also enhance sampling speed. LCMs [18; 19] treat the reverse diffusion process as an augmented probability flow ODE (PF-ODE) problem, inspired by Consistency Models (CMs) , enabling generation in fewer steps. PNDM  emphasizes efficient sampling without retraining diffusion model. Additionally, ADD  combines adversarial training and score distillation to transform pretrained diffusion models into high-fidelity image generators using only single sampling steps.

**Efficient Structural Methods.** Other efforts concentrate on reducing the computational overhead associated with each denoising step. Previous methods [16; 17; 22] have typically conducted extensive empirical studies to identify and remove non-critical layers from U-Net architectures to achieve faster networks. BK-SDM  customizes three efficient U-Nets by strategically removing residual and attention blocks. Derived from BK-SDM, KOALA  develops two efficient U-Nets of varying sizes tailored for SD-XL applications. Diff-pruning  employs Taylor expansion over pruned timesteps to pinpoint essential layer weights, optimizing model efficiency without sacrificing performance. DeepCache  enhances inference efficiency by reusing predictions from blocks in previous timesteps within the U-Net architecture. LAPTOP-Diff  tackles optimization problems with a one-shot pruning approach, incorporating normalized feature distillation to streamline retraining processes. T-GATE  not only reduces computation overhead but also marginally lowers FID scores by omitting text conditions during fidelity-improvement stages.

In addition to the two primary acceleration methods, other strategies such as distillation [28; 30; 31], early stopping , and quantization  are commonly employed to enhance performance and efficiency. However, most of these strategies necessitate retraining pretrained models. Our method falls under the category of efficient structural methods by focusing on reducing inference time at each timestep. Importantly, these efficiency gains are achieved without retraining the diffusion model.

### Model Optimization

**Network Pruning.** The taxonomy of pruning methodologies typically divides into two main categories: unstructured pruning methods [34; 35; 36] and structural pruning methods [37; 38; 39; 40]. Unstructured pruning methods involve masking parameters without structural constraints by zeroing them out, often requiring specialized software or hardware accelerators. In contrast, structured pruning methods generally remove regular parameters or substructures from networks. Recent works have been interested in accelerating transformers. Dynamic skipping blocks, which involve selectively removing layers while maintaining the overall structure, have emerged as a paradigm for transformer compression [41; 42; 43; 44]. However, applying structural pruning techniques to diffusion modeling poses unique challenges that necessitate reevaluating conventional pruning methods.

## 3 Methodology

In this study, we introduce the Diffusion Pruner via Few-step Gradient Optimization (DiP-GO), which utilizes a neural network to predict whether to skip or keep each computational block during inference. Our primary objective is to identify the optimal subset of computational blocks that facilitate denoising with minimal computational overhead. As illustrated in Figure 2, our method comprises three main components: a neural network pruner, optimization losses, and a post-process algorithm to derive the pruned model based on the predictions of pruner. The neural network pruner is designed with learnable queries inspired by DETR  to predict the state of each block. Our proposed optimization losses include sparsity and consistency constraints for generation quality, guiding the pruner to accurately assess the importance of each block. In this Section, we first revisit the framework of diffusion models in Section 3.1, emphasizing their potential for exploring pruned networks. In Section 3.2, we introduce a SuperNet based on diffusion models and demonstrate how to derive a SubNet or pruned network from it for inference acceleration, highlighting the challenges in achieving an optimal SubNet. Section 3.3 details our method, including the neural network pruner, optimization losses, and post-process algorithm for obtaining a SubNet that meets pruning requirements. Finally, we provide insights into the training and inference processes of our method.

### Preliminary

We begin with a brief introduction to diffusion models. Diffusion models are structured to learn a series of sequential state transitions with the goal of iteratively refining random noise sampled from a known prior distribution towards a target distribution \(x_{0}\) that matches the data distribution. During the forward diffusion process, the transition from \(x_{t-1}\) to \(x_{t}\) is initially determined by a forward transition function, which can be described as follows:

\[q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},_{t})\] (1)

where the hyperparameter \(\{_{t}(0,1)\}_{t=1}^{T}\) increases with each successive time step \(t\).

To generate samples from a learned diffusion model, it involves a series of reverse state transitions from \(_{T}_{0}\) to denoise random noise \(_{T}(,)\) into the clean data point \(_{0}\). At each timestep, the denoised output \(_{t-1}\) is predicted by approximating the noise prediction network, which is conditioned on the time embedding \(t\) and the previous data point \(_{t}\):

\[p_{}(x_{t-1}|x_{t})=(x_{t-1};}}(x_{t}- }{}}}z_{}(x_{t},t)),)\] (2)

where the covariance constant \(_{t}=1-_{t}\), \(_{t}=_{i=1}^{T}_{i}\), and \(z_{}(x_{t},t)\) are the parameterized deep neural networks. With the reverse Markov chain, we can iteratively sample from the learnable transition kernel \(x_{t-1} p_{}(x_{t-1}|x_{t})\) until \(t=1\).

Diffusion modes typically require multi-step conditional sampling to gradually obtain the target sample point \(x_{0}\). However, recent studies [15; 18; 22] have highlighted that multi-step inference processes involve substantial redundant feature computations, particularly in noise prediction networks like UNet and Transformer. For example, in Stable Diffusion 1.4 models with 25 steps, Multiply-Accumulate Operations (MACs) of UNet can comprise up to 87.2% of the total computational load . This underscores significant potential for accelerating inference by effectively eliminating these redundancies. In this work, we propose accelerating the diffusion model by integrating a differentiable pruning network designed to identify and remove these redundant computations.

### SuperNet and SubNet of Diffusion Model

Our goal is to identify and remove unimportant blocks during inference to accelerate the process. To achieve this, we introduce a SuperNet based on the diffusion model. This SuperNet is designed to facilitate block removal while ensuring the pruned model maintains inference capability through additional connections. Our approach effectively eliminates unimportant blocks during inference, essentially deriving a SubNet from the SuperNet by skipping these unnecessary components. Thus, the pruning process can be conceptualized as a SubNet search within the SuperNet framework.

**How to Construct a SuperNet.** Recent studies [15; 18; 22] have observed that diffusion models often exhibit similar feature patterns across adjacent timesteps during inference. Building on this insight, we enhance the standard diffusion model's inference phase by introducing additional connections from the current timestep to the previous one. These connections serve as backups for blocks that may be removed, ensuring each block retains valid inputs even if its dependent blocks are eliminated for acceleration. Specifically, for all inputs of each block across all timesteps except the inital step during inference, we establish a backup input connection to the corresponding block in the previous timestep, as illustrated in Figure 1.

**How to Obtain a SubNet.** To construct the SuperNet for the standard diffusion model, we introduce additional connections that ensure a valid SubNet selects either the original input connection or the backup input connection, but not both simultaneously. This design principle mandates that if a dependent block is pruned, its original input connection is also eliminated to reflect the block's removal. Conversely, if the dependent block is retained, the backup input connection is removed to maintain efficient inference, as depicted in Figure 1.

Figure 1: Overview of the SuperNet and SubNet. Standard diffusion models execute the full inference path step by step. In our framework, we propose a SuperNet based on the original flow and integrate backup connections to facilitate block removal. This allows the partial inference SubNet to efficiently eliminate redundant computational costs.

We draw inspiration from the Lottery Ticket Hypothesis (LTH) , which posits the existence of a sub-network capable of achieving comparable performance to the original over-parameterized network for a given task, but with fewer unnecessary weights. Moreover, prior work  has explored manually removing redundant computations by caching features across adjacent steps. Thus, our approach seeks to identify an optimal SubNet from the SuperNet, maximizing diffusion model acceleration while minimizing any loss in generation quality.

**Hard to Obtain an Optimal SubNet.** The challenge of obtaining an optimal SubNet is compounded by the large number of blocks expanded during inference. In a diffusion pipeline with \(N T\) blocks (where \(N\) is the number of blocks per timestep and \(T\) is the number of timesteps), each block's decision to be kept or removed results in \(2^{N T}\) possible configurations. For instance, a 50-step PLMS setup , considering 9 blocks in the U-Net, yields \(2^{450}\) choices (> \(10^{135}\)). Traditional search methods like random search and genetic algorithms  often struggle in such vast search spaces. Gradient-based optimization offers a promising approach to tackle this challenge. However, there are significant hurdles to overcome. First, effectively modeling discrete block states (kept or removed) with parametric methods poses difficulties. Second, training the entire model, comprising both the parametric model and the expanded diffusion model with denoising timesteps, risks encountering out-of-memory (OOM) issues.

### Our DiP-GO Approach

In this study, we introduce a diffusion pruner network designed to predict importance scores for all blocks during reverse sampling as depicted in Figure 2. To optimize the pruner network effectively, we employ two key optimization losses: consistency and sparsity losses, leveraging few-step gradient optimization. Addressing the OOM issue inherent in such computations, we implement gradient checkpointing and half-precision floating-point representation techniques, enabling efficient search processes on a single GPU. Once the pruner network trained, we extract predicted importance scores for all blocks. Subsequently, we devise a post-processing algorithm to utilize these scores, generating pruned SubNets of diffusion models that satisfy specific pruning criteria.

**Pruner Network.** Our pruner network comprises three main components: \(N T\) learnable queries, a query encoder, and a prediction head. We design the learnable queries to match the number of all blocks during inference. These queries are optimized with sparsity and consistency loss constraints to

Figure 2: Overview of our diffusion pruner. a) DiP-GO employs a pruner network to learn the importance scores of blocks in the diffusion sampling process. It takes \(N T\) queries as input and passes them through stacked self-attention (SA) and fully connected (FC) layers to capture the structural information in existing diffusion models. The network predicts the partial inference paths based on the \(N T\) importance scores and is optimized by consistent and sparse loss. b) Once trained, the pruner network is discarded. We can infer the optimal partial inference path with expected computational costs via post-processing based on the predicted importance scores.

learn the contextual information necessary for predicting the importance score of each block. For the query encoder, we provide two options: a simple version with several stacked linear layers, and a more complex version with several stacked self-attention layers to facilitate interaction among the learnable queries. Our experiments demonstrate that both versions can effectively obtain optimal SubNets in various diffusion models under different pruning requirements. The prediction head consists of \(N T\) simple branches, each containing two stacked linear layers followed by a softmax operation. The final linear layer has a dimension of 2, and the softmax output represents the importance scores of a block. During training or inference, the query embeddings are transformed into output embeddings via the query encoder. These embeddings are then independently decoded into binary vectors by the multi-layer prediction head, resulting in \(N T\) importance scores for all blocks.

**Optimization Losses.** The \(k\)-th predicted binary vector of importance score, denoted as \(^{k}\), represents the likelihood of its corresponding block being removed or kept in the denoising process. A gate \(\{0,1\}^{TN}\) is derived based on \(\), where \(^{k}=0\) or \(^{k}=1\) indicate removing or keeping the \(k\)-th computation block, respectively. Only the blocks that are kept according to \(\) will be calculated in the denoising process. However, directly converting predicted probabilities \(\) into discrete gates \(\) with \(\) is non-differentiable. To address this issue, we utilize the Straight-Through (ST) Estimator  to approximate the real gradient \(_{}\) with the gradient of the soft prediction \(_{}\). To encourage both high-fidelity predictions and minimal computation block usage, we design our training objective function as a combination of consistent loss \(_{c}\) and sparse loss \(_{s}\), formulated as follows:

\[(_{T};,W)=_{c}+_{s}_{s}= f(_{0}^{p},_{0}^{gt})+}{NT}_{k}^ {NT}^{k}^{k}&sparsity<\\ f(_{0}^{p},_{0}^{gt})&sparsity\] (3)

Here, \(_{s}\) represents a hyperparameter used to balance the consistent and sparse losses. \(\) and \(W\) denote the pruner network and pretrained diffusion model, respectively. \(f()\) denotes a distance function that evaluates the consistency between the generated clean data point \(_{0}^{p}\) from partial inference of the pruned SubNet and the \(_{0}^{gt}\) from full inference. This function can be any distance measure, and in this work, we utilize a negative SSIM loss . The sparse loss encourages minimal computational usage and is weighted by the computational flops proportion \(^{k}\) of the \(k\)-th block, thereby imposing a greater penalty on heavier blocks. The calculation of \(\) takes into account the cascading relationships between blocks. Specifically, when a block is pruned, the associated dependent blocks will also pruned. Therefore, the flops reduction from pruning a block includes the block itself and its dependent blocks. We denote the flops reduction ratio after pruning the \(k\)-th block as \(^{k}\). The flops ratio \(\) is in the range \(\). The sparse loss is only introduced when the sparsity (pruning ratio) is below a certain threshold \(\). This compound loss controls the trade-off between efficiency (block usage) and accuracy (generation quality).

**Post-Processing Algorithm.** After training the pruner network, our diffusion pruner is able to predict which computation blocks during inference contribute less to generation quality based on the importance scores for all the blocks. As the importance scores are continuous values in inference phase, they can not be utilized directly to identify which blocks should be removed to meet given pruning requirements. Therefore, we present a post-process algorithm to obtain an appropriate threshold for these importance score to meet the pruning requirements as shown in Algorithm 1 in Appendix B. Considering the required pruning sparsity, we use bisection lookup to select the appropriate threshold value to identify which blocks should be removed to meet the pruning ratio. Specifically, the blocks whose important scores below the threshold should be removed and the kept blocks should update their input connections as mentioned in Section 3.2 to maintain the pruned model inference. Thus a pruned model met the pruning ratio has been obtained.

**Training and Inference Details.** In the training phase, the prompt inputs are fed into the diffusion model to obtain two kinds of outputs, one is generated by the baseline diffusion model and the other is generated by the pruned model obtained via the current predictions of the pruner network. Then our proposed losses are utilized to optimize the pruner network to enable distinguishing the less important blocks. In the pruner's network, we initialize the weight of the last linear layer's output channel to 0 and its bias to 1. This setup ensures that at the beginning of training, the consistency loss is 0 and the sparsity loss is 1, facilitating smooth training. As training progresses, the sparsity loss gradually decreases while the number of pruned blocks increases, causing the consistency loss to rise. To maintain network fidelity after pruning, we switch to training only with the consistency loss once the sparsity loss reaches 0.2, continuing until training is complete. Once the pruner is well trained, we can obtain pruned models to meet the pruning requirements via our post-process algorithm.

Experiments

### Experimental Setup

**Pre-trained Model and Datasets.** We select four official pretrained Diffusion Models (i.e., SD-1.5 , SD-2.1 , SD-XL  and DiT ) to evaluate our approach. The SD series models are constructed on the U-Net  and the DiT is constructed on the transformer . We utilize a subset of the DiffusionDB  dataset comprising 1000 samples to train our pruner network, utilizing only textual prompts. Following previous works [29; 22], we evaluate the DiP-GO on three public datasets, i.e., PartiPrompts , MS-COCO 2017  and ImageNet .

**Evaluation Metrics.** We employ the Frechet Inception Distance (FID)  metrics to assess the quality of images created by the generative models. FID quantifies the dissimilarity between the Gaussian distributions of synthetic and real images. A lower FID score indicates a closer resemblance to real images in the generative model. Additionally, we utilize the CLIP Score  (ViT-g/14) to evaluate the relational compatibility between images and text.

**Implementation Details.** For Stable Diffusion models, we utilize the SGD optimizer with a cosine learning schedule for 1000 steps of training. The batch size, learning rate, and weight decay are set to 1, 0.1, and \(1 10^{-4}\), respectively. The hyperparameters \(_{s}\), \(\), and the query embedding dimension \(D\), along with the encoder layer number \(L\), are set to 1, 0.2, 512, and 1, respectively. For the Diffusion Transformer model, we use the same experimental configuration as for the stable diffusion model, except that the learning rate set to 10. To evaluate the inference efficiency, we evaluate the Multiply Accumulate Calculation (MACs), Parameters (Params), and Speedup for all models with batch size of 1 in the PyTorch 2.1 environment on the AMD MI250 platform. Besides, we report MACs in those tables, which refer to the totals MACs for all steps.

### Comparison with State-of-the-Art Methods on Different Base Models

**Stable Diffusion on PartiPrompt and COCO2017.** We compare our method with the state-of-the-art (SOTA) compression methods on Stable Diffusion 1.5 (SD-1.5), and the results are summarized in Table 1. Compared to the SOTA DeepCache , our approach demonstrates significant performance improvements, achieving nearly \(2\) fewer MACs while maintaining better CLIP Scores. Our method can achieve \(4.4\) speedup compared to the baseline model. Furthermore, our method does not require training the diffusion model, which preserves the pre-trained knowledge of the diffusion model. Also, we apply our method on the SD-2.1 model to verify the effectiveness, as shown in Table 2, our method achieves significant acceleration while maintaining generation quality, demonstrating its superiority.

**Diffusion Transformers on ImageNet.** To the best of our knowledge, we are the first to apply pruning to DiT  model. Therefore, we have replicated a training-free acceleration method, DeepCache

    &  &  &  \\  & & **MACs \(\)** & **Speedup \(\)** & **CLIP Score \(\)** & **MACs \(\)** & **Speedup \(\)** & **CLIP Score \(\)** \\  PLMS - 50 steps & Baseline & 16.94T & 1.00\(\) & 29.51 & 16.94T & 1.00\(\) & 30.30 \\  BK-SDM - Base & Structured & 11.19T & 1.49\(\) & 28.88 & 11.19T & 1.45\(\) & 29.47 \\ PLMS - 25 steps & Fast Sampler & 8.47T & 2.04\(\) & 29.33 & 8.47T & 1.91\(\) & 29.99 \\ PLMS - Skip - Interval=2 & Structured & 8.47T & 2.04\(\) & 19.74 & 8.47T & 1.91\(\) & 16.78 \\ DeepCache & Structured & 6.52T & 2.15\(\) & 29.46 & 6.52T & 2.11\(\) & 30.23 \\
**Ours (w) Pruned-0.80** & Structured & **3.88T** & **4.43E** & **29.51** & **3.38T** & **4.40\(\)** & **30.29** \\  BK-SDM - Small & Structured & 10.88T & 1.75\(\) & 27.94 & 10.88T & 1.68\(\) & 27.96 \\ PLMS - 15 steps & Fast Sampler & 5.08T & 2.89\(\) & 28.58 & 5.08T & 2.59\(\) & 29.39 \\
**Ours (w/ Pruned-0.85)** & Structured & **2.54T** & **5.52\(\)** & **29.07** & **2.54T** & **5.46\(\)** & **29.84** \\   

Table 1: Comparison with PLMS, BK-SDM and DeepCache on SD-1.5. We utilize prompts in PartiPrompt and COCO2017 validation set.

  
**Inference Method** & **MACs\(\)** & **Speedup\(\)** & **CLIP Score \(\)** & **FID-5K \(\)** \\  SD-2.1-50 steps  & 38.04T & \(1.00\) & 31.55 & 27.29 \\  SD-2.1-20 steps  & 15.21T & \(2.49\) & **31.53** & 27.83 \\
**Ours (w/ Pruned-0.7)** & 11.42T & \(3.02\) & 31.50 & **25.98** \\
**Ours (w/ Pruned-0.8)** & 7.61T & \(3.81\) & 30.92 & 27.69 \\   

Table 2: Comparison of computational complexity, inference speed, CLIP Score and FID on the MS-COCO 2017 validation set on SD-2.1.

with intervals = 2 and 5, on DiT for comparison. The results in Table 3 show that our method can speed up the original DiT model by a factor of 2.4 with minimal performance loss, while DeepCache has a lower speedup ratio when applied to the DiT model. This can be attributed to DeepCache's overreliance on pre-defined structures, whereas our method can automatically learn the optimal pruning strategy for the given model, thereby achieving superior performance.

### Compatibility with Fast Sampler

We investigate the compatibility of DiP-GO with methods that prioritize reducing sampling steps using faster samplers: DDIM , DPM-Solver , and LCM . As shown in Table 4, it indicates that our method further improves computational efficiency on existing fast samplers. Specifically, we reduce MACs by a factor of 5 on the SD-1.5 with DDIM sampler and by \(3.36\) on the SD-2.1 with DPM-Solver. Our method achieves nearly unchanged performance with significant acceleration. Additionally, our method benefits from information redundancy in multi-step optimization processes, showing relatively limited acceleration performance on fewer-step LCM due to its low redundancy in features across adjacent timesteps.

### Ablation Study

**Compared with Different Consistent Constraints.** We further compare other alternatives explored for our consistent loss designs, we further scrutinize additional options, including L1, L2, SSIM, and L1+SSIM losses, as depicted in Table 5. The results demonstrate that SSIM emerges as the most effective choice, boasting the highest CLIP-Score. In contrast, the L1 loss function often results in image blurring or distortion due to its sensitivity to pixel-level differences, the L2 loss may yield overly smoothed images by penalizing squared differences between pixels. Conversely, the combination of L1+SSIM loss attempts to address these limitations but can complicate the training process and suffer from trade-offs. Therefore, SSIM emerges as the preferred choice in our consistent loss designs, offering superior accuracy and stability while preserving image quality.

**Effect of Gradient Optimization.** As the traditional search algorithm can also obtain SubNets from our proposed SuperNet. It is crucial to validate whether traditional search-based algorithms yield

  
**Sampler** &  &  \\   & **MACs \(\)** & **CLIP Score \(\)** & **MACs \(\)** & **CLIP Score \(\)** \\  DDIM (SD-1.5 w/ 50 steps) & 16.94T & 30.30 & 3.38T & 30.29 \\ DPM (SD-2.1 w/ 50 steps) & 38.04T & 31.55 & 11.42T & 31.50 \\ DPM (SD-2.1 w/ 25 steps) & 19.02 T & 31.59 & 9.51T & 31.52 \\ LCM (SD-XL w/ 4 steps) & 11.95T & 31.92 & 11.58T & 31.30 \\   

Table 4: Comparison with PLMS, SSIM, and LCM samplers. We evaluate the effectiveness of our methods on COCO2017 validation set.

  
**Method** & **Pruning Type** & **MACs \(\)** & **FID-50K \(\)** & **Speedup \(\)** \\  DiT-XL/2-250 steps & - & 29.66T & 2.27 & \(1.00\) \\  DiT-XL/2*-250 steps & Baseline & 29.66T & 2.97 & \(1.00\) \\  DiT-XL/2*-110 steps & Fast Sampler & 13.05T & 3.06 & \(2.13\) \\ DiT-XL/2*-100 steps & Fast Sampler & 11.86T & 3.17 & \(2.46\) \\ DeepCache(DiT-XL/2*-2)*-N=2 & Structured Pruning & 15.88T & 3.07 & \(1.76\) \\
**Ours (DiT-XL/2* w/ Pruned-0.6)** & Structured Pruning & 11.86T & **3.01** & \(\) \\  DiT-XL/2*-70 steps & Fast Sampler & 8.30T & 3.35 & \(3.49\) \\ DeepCache(DiT-XL/2*)-N=5 & Structured Pruning & 6.77T & 3.20 & \(3.44\) \\
**Ours (DiT-XL/2* w/ Pruned-0.75)** & Structured Pruning & 7.40T & **3.14** & \(\) \\   

Table 3: Comparison of pruning type, computational complexity, FID and inference speed on the ImageNet validation datasets on DiT. * denotes the results reproduced with diffusers .

  
**Loss Type** & **L1** & **L2** & **SSIM** & **L1 + SSIM** \\  CLIP Score\(\) & 29.94 & 29.71 & **30.29** & 29.77 \\   

Table 5: Comparison with different consistent loss types. Here we conduct pruning experiments with 80% sparsity on COCO2017 validation using SD-1.5.

positive effectiveness. We assess two search algorithms: random search and genetic algorithm-based search  in Table 6. We have iterated the search 1000 times using the first 500 images of the test set as a calibration dataset. Remarkably, we observe that the search time of traditional search algorithms is significantly longer than the training time of our method due to a large number of evaluations. Moreover, due to the vast search space, traditional search algorithms struggle to achieve satisfactory results. Additionally, traditional search algorithms lack the "once-for-all" characteristic, requiring re-execution when faced with deployment scenarios demanding different computational resources. In contrast, leveraging the parametric pruner network, our method achieves superior performance with reduced running time and is more adaptable to diverse development scenarios.

Qualitative Analysis of Increased Prune Ratio.In Figure 3, we visualize the generated images as we increase the pruning ratio. With the increase in pruning ratio, the model's inference speed significantly improves, allowing us to achieve up to a fourfold increase in inference speed. However, as the pruning ratio increases, some patterns in the image content deviate from those in the original images. Nevertheless, our main objects in the figures consistently adhere to the textual conditions. Subtle changes in background details typically do not compromise image quality, as quantitatively analyzed in Table 1.

## 5 Conclusion

This work explores resolving diffusion accelerating tasks by reducing redundant feature calculations across adjacent timesteps. We present a novel diffusion pruning framework and cast the model pruning process as a SubNet search problem. Our approach introduces a plugin pruner network that identifies an optimal SubNet through few-step gradient optimization. Results on a wide range of Stable Diffusion (SD) and DiT series models verify the effectiveness of our method. We achieve a 4.4\(\) speedup on Stable Diffusion 1.5 and effectively prune the DiT model with few step optimizations.

  
**Method** & **Cost GPU Hours \(\)** & **Pruning Ratio** & **MACs \(\)** & **CLIP Score \(\)** \\  PLMS-50 steps & - & - & 16.94T & 30.30 \\  Random Search & 25 & 0.80 & 2.96T & 28.73 \\ GA Search & 25 & 0.80 & 3.34T & 29.37 \\
**Ours** & 2.3 & 0.80 & 3.38T & **30.29** \\  Random Search & 24 & 0.85 & 2.90T & 27.22 \\ GA Search & 24 & 0.85 & 2.73T & 28.61 \\
**Ours** & 2.2 & 0.85 & 2.54T & **29.84** \\  Random Search & 23 & 0.90 & 1.94T & 24.07 \\ GA Search & 23 & 0.90 & 2.04T & 25.14 \\
**Ours** & 2.2 & 0.90 & 1.69T & **28.72** \\   

Table 6: Comparison of cost time, computational complexity and CLIP-Score between Random Search and GA search strategies on Stable Diffusion 1.5.

Figure 3: Visualization of generated images. It shows evolving patterns as pruning ratios increase. Despite these changes, main objects in the images remain consistent with the textual conditions.