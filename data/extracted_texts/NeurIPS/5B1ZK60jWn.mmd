# A Spectral Theory of Neural Prediction and Alignment

Abdulkadir Canatar\({}^{1,2*}\) &Jenelle Feather\({}^{1,2*}\) &Albert J. Wakhloo\({}^{1,3}\) &SueYeon Chung\({}^{1,2}\)

\({}^{1}\)Center for Computational Neuroscience, Flatiron Institute

\({}^{2}\)Center for Neural Science, New York University

\({}^{3}\)Zuckerman Institute, Columbia Univeristy

\({}^{*}\)Equal contribution

{acanatar,jfeather,awakhloo,schung}@flatironinstitute.org

###### Abstract

The representations of neural networks are often compared to those of biological systems by performing regression between the neural network responses and those measured from biological systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but it remains unclear how to differentiate among models that perform equally well at predicting neural responses. To gain insight into this, we use a recent theoretical framework that relates the generalization error from regression to the spectral properties of the model and the target. We apply this theory to the case of regression between model activations and neural responses and decompose the neural prediction error in terms of the model eigenspectra, alignment of model eigenvectors and neural responses, and the training set size. Using this decomposition, we introduce geometrical measures to interpret the neural prediction error. We test a large number of deep neural networks that predict visual cortical activity and show that there are multiple types of geometries that result in low neural prediction error as measured via regression. The work demonstrates that carefully decomposing representational metrics can provide interpretability of how models are capturing neural activity and points the way towards improved models of neural activity.

## 1 Introduction

Neuroscience has a rich history of using encoding models to understand sensory systems [1; 2; 3]. These models describe a mapping for arbitrary stimuli onto the neural responses measured from the brain and yield predictions for unseen stimuli. Initial encoding models were built from hand-designed parameters to model receptive fields in early sensory areas [4; 5], but it was difficult to extend these hand-engineered models to capture responses in higher-order sensory regions. As deep neural networks (DNNs) showed human-level accuracy on complex tasks, they also emerged as the best models for predicting brain responses, particularly in these later regions [6; 7; 8; 9; 10].

Although early work showed that better task performance resulted in better brain predictions , in modern DNNs, many different architectures and training procedures lead to similar predictions of neural responses [11; 12; 13]. This is often true even when model representations are notably different from each other using other metrics of comparison [14; 15]. For instance, previous lines of work have shown clear differences between models from their behavioral responses to corrupted or texturized images [16; 17; 18], from their alignment to human similarity judgments , from generating stimuli that are "controversial" between two models , and from stimuli that are metameric to models  or to human observers . Other work has focused on directly comparing representations [22; 23] or behavioral patterns  of one neural network to another, finding that changing the training dataset or objective results in changes to the underlying representations. Given the increasing number of findings that demonstrate large variability among candidate computational models, it is an openquestion of why current neural prediction benchmarks are less sensitive to model modification, and how to design future experiments and stimulus sets to better test our models.

Metrics of similarity used to evaluate encoding models of neural responses are often condensed into a single scalar number, such as the variance explained when predicting held-out images [6; 11]. When many models appear similar with such metrics, uncovering the underlying mechanisms driving high predictivity is a challenge . To uncover the key factors behind successful DNN models of neural systems, researchers have proposed various approaches to explore the structural properties of internal representations in DNN models in relation to neural data. One promising approach focuses on analyzing the geometry of neural activities . Several studies have examined the evaluation of geometries and dimensionalities of DNNs [26; 27; 28], highlighting the value of these geometric approaches. They provide mechanistic insights at the population level, which serves as an intermediate level between computational level (e.g., task encoding) and implementation level (e.g., the activities of single neurons or units). Furthermore, the measures derived from representation geometries can be utilized to compare the similarity between models and neural data [7; 25; 29]. Although these approaches have been successful, they do not directly relate the geometry of DNNs to the regression model used to obtain neural predictions, which uniquely captures the utility of a model for applications such as medical interventions  and experimental designs . Recent empirical findings by Elmoznino et al.  have suggested a correlation between the high-dimensionality of model representations and neural predictivity in certain scenarios, but this work lacks explicit theoretical connections between the spectral properties and predictivity measures. Our work contributes to this line of investigation, by seeking a theoretical explanation and suggesting new geometric measures linked to a model's neural prediction error for future investigations.

To bridge the gap between DNN geometry and encoding models used for neural prediction, we turn to a recent line of work that derived a predictive theory of generalization error using methods from statistical physics [32; 33]. Specifically, two different spectral properties that affect the regression performance were identified: _Spectral bias_ characterizing how fast the eigenvalues of the data Gram matrix decay, and _task-model alignment_ characterizing how well the target aligns with the eigenvectors of this data Gram matrix . While the former describes how large the learnable subspace is, the latter defines how much of the variance of the target is actually learnable. Note that in general, these two properties may vary independently across representations. Hence, the trends among regression scores across DNNs must be carefully studied by taking into account the joint trends of spectral bias and task-model alignment.

In this work, we explore how encoding models of neural responses are influenced by the geometrical properties of the DNN representations and neural activities. Our analyses primarily concern how and why models achieve the predictivities that they do, rather than offering an alternative scalar metric for predictivity. We find that the neural prediction error from ridge regression can be broken down into the radius and dimension of the _error mode geometry_, characterizing the error along each eigenvector. When investigating models along these error axes, we see a large spread in the error mode geometry, suggesting that the correlation values obtained from regression analyses obscure many geometrical properties of the representations.

Our primary contributions are:

* We analytically decompose the neural prediction error of ridge regression from a model to a set of brain data in terms of the model eigenspectra, the alignment between the brain data and the eigenvectors of the model, and the training set size.
* We introduce two geometric measures that summarize these spectral properties and directly relate to the neural prediction error. We show that these measures distinguish between different models with similar neural prediction errors using a wide variety of network architectures, learning rules, and firing rate datasets from visual cortex.
* Using spectral theory, we demonstrate that for networks effective in predicting neural data, we can ascertain if their superior performance stems from the model's spectra or alignment with the neural data. Our findings indicate that (a) trained neural networks predict neural data better than untrained ones due to better alignment with brain response and (b) adversarial training leads to an improved alignment between the eigenvectors of networks and early visual cortex activity.

## 2 Problem Setup

### Encoding Models of Neural Responses

We investigated DNNs as encoding models of visual areas V1, V2, V4, and IT. The neural datasets were previously collected on 135 texture stimuli for V1 and V2 , and on 3200 object stimuli with natural backgrounds for V4 and IT  (Fig. 1A). Both datasets were publicly available and obtained from . The neural responses and model activations were extracted for each stimulus (Fig. 1B), and each stage of each investigated neural network was treated as a separate encoding model. We examined 32 visual deep neural networks. Model architectures included convolutional neural networks, vision transformers (ViTs), and "biologically inspired" architectures with recurrent connections, and model types spanned a variety of supervised and self-supervised training objectives (see Sec. SI.2 for full list of models). We extracted model activations from several stages of each model. For ViTs, we extracted activations from all intermediate encoder model stages, while in all other models we extracted activations from the ReLU non-linearity after each intermediate convolutional activation. This resulted in a total number of \(516\) analyzed model stages. In each case, we flattened the model activations for the model stage with no subsampling.

### Spectral Theory of Prediction and Alignment

In response to a total of \(P\) stimuli, we denote model activations with \(M\) features (e.g. responses from one stage of a DNN) by \(^{P M}\) and neural responses with \(N\) neurons (e.g. firing rates) by \(^{P N}\). Sampling a training set \((},})\) of size \(p<P\), ridge regression solves (Fig. 1B):

\[(p)=*{arg\,min}_{^{M N}}|| }-}||_{F}^{2}+_{}||||_ {F}^{2},}(p)=(p)\] (1)

Figure 1: **Experimental and theoretical framework.****(A)** Example stimuli from visual experiments used for neural prediction error measures of V1, V2, V4, and IT. **(B)** Framework for predicting neural responses from model activations. Responses are measured from a brain region \(\) and from a model stage of a DNN \(\). Weights for linear regression are learned to map \(\) to \(\), and the empirical neural prediction error \(E_{g}(p)\) is measured from the response predictions. **(C)** Spectral alignment theory. Neural prediction error can be described as a function of the eigenvalues and eigenvectors of the model Gram matrix. The neural response matrix \(\) is projected onto the eigenvectors \(_{i}\) with alignment coefficients \(W_{i}\). For the number of training samples \(p\), the value of each error mode is given as \(W_{i}E_{i}(p)=_{i}(p)\). The total neural prediction error can be expressed as a sum of \(_{i}(p)\) terms, visualized as the area under the \(_{i}(p)\) curve.

We analyze the normalized neural prediction error, \(E_{g}(p)=(p)-||_{F}^{2}}{||||_{F}^{2}}\), for which we utilize theoretical tools from learning theory [36; 37; 38], random matrix theory [39; 40; 41] and statistical physics [42; 32; 33] to extract geometrical properties of representations based on spectral properties of data. In particular, the theory introduced in [32; 33] relies on the orthogonal mode decomposition (PCA) of the Gram matrix \(^{}\) of the model activations, and projection of the target neural responses onto its eigenvectors:

\[^{}=_{i=1}^{P}_{i}_{i} _{i}^{}, W_{i}:=^{T}_{i}||_{2}^{2}}{|| ||_{F}^{2}},_{i},_{j}= _{ij}.\] (2)

Here, associated to each mode \(i\), \(W_{i}\) denotes the variance of neural responses \(\) in the direction \(_{i}\), and \(_{i}\) denotes the \(i^{th}\) eigenvalue. Then, the neural prediction error is given by:

\[E_{g}(p)=_{i=1}^{P}W_{i}E_{i}(p), E_{i}(p):=}{1- }+)^{2}},\] (3)

where \(=_{}+_{i=1}^{P}}{p_{i} +}\) must be solved self-consistently, and \(=_{i=1}^{P}^{2}}{(p_{i}+)^{2}}\) (see Sec. SI.1 for details) [32; 33]. Note that the theory depends not only on the model eigenvalues \(_{i}\), but also on the model eigenvectors \(_{i}\) along with the responses \(\), which determine how the variance in neural responses distributed among a model's eigenmodes.

Although the equations are complex, the interpretations of \(W_{i}\) and \(E_{i}(p)\) are simple. \(W_{i}\) quantifies the projected variance in neural responses on model eigenvectors (alignment between neural data and model eigenvectors, i.e., _task-model alignment_). Meanwhile, \(E_{i}(p)\), as a function of the training set size \(p\), determines the reduction in the neural prediction error at mode \(i\) and depends only on the eigenvalues, \(_{i}\) (i.e., _spectral bias_).

In this work, we combine both and introduce _error modes_\(_{i}(p):=W_{i}E_{i}(p)\):

\[_{i}(p):=}{1-}}{(p_{i}+ )^{2}}, E_{g}(p)=_{i}_{i}(p)\] (4)

As shown in (Fig. 1C), \(_{i}\) associated with large eigenvalues \(_{i}\) will decay faster with increasing \(p\) than those associated with small eigenvalues.

The generalization performance of a model is fully characterized by its error modes, \(_{i}\). However, due to its vector nature, \(_{i}\) is not ideally suited for model comparisons. To address this limitation, we condense the overall shape of \(_{i}\) into two geometric measures, while preserving their direct relationship to the neural prediction error. This is in contrast to previous such measures, such as the effective dimensionality, that only depend on model eigenvalues .

Here, we define a set of geometric measures that characterize the distribution of a model's \(_{i}\) via the _error mode geometry_ (Fig. 2A). Specifically, we rewrite the neural prediction error \(E_{g}(p)\) as:

\[R_{em}(p):=_{i}(p)^{2}}, D_{em}(p):=_{i}_{i}(p)^{2}}{_{i}_{i}(p)^ {2}}, E_{g}(p)=R_{em}(p)(p)}.\] (5)

The error mode radius \(R_{em}\) denotes the overall size of the error terms, while the error mode dimension \(D_{em}\) represents how dispersed the total neural prediction error is across the different eigenvectors (Fig. 2A). Note that, the neural prediction error \(E_{g}(p)\) above has a degeneracy of error geometries; many different combinations of \(R_{em}\) and \(D_{em}\) may result in the same \(E_{g}(p)\) (Fig. 2B). Given a fixed neural prediction error, a higher value of \(R_{em}(p)\) indicates that only a few \(_{i}\) are driving the error, while a higher \(D_{em}(p)\) indicates that many different \(_{i}\) are contributing to \(E_{g}(p)\).

## 3 Results and Observations

We first confirmed that Eq. 3 accurately predicted the neural prediction error for the encoding models considered here. We performed ridge regression from the model activations of each model stage of each trained neural network to the neural recordings for each cortical region using a ridge parameter of \(_{}=10^{-14}\), and also calculated the theoretical neural prediction error for each using Eq. 3. Given that Eq. 3 is only accurate in the large \(P\) limit in which both the number of training points and the number of test samples is large (see Sec. SI.1 and Fig. SI.4.2), we used a 60/40 train-test split (\(p=0.6P\)) in the experiments below. As shown in Fig. 3A, this split yielded near-perfect agreement between the theoretical and empirical neural prediction errors for areas V4 and IT (\(R^{2}\)=0.97 and 0.97, respectively), and very strong agreement for areas V1 and V2 (\(R^{2}\)=0.9 and 0.9, respectively). We present further data for this agreement in Sec. SI.4. Furthermore, we found that the ordering of models according to their neural prediction error is very similar to the ordering obtained via the partial least squares (PLS) regression method used in Brain-Score  (see Sec. SI.3). We maintained these choices for train-test split sizes and \(_{}\) for all subsequent experiments.

We visualized the spread of error mode geometries for each brain region across different trained models in Fig. 3B. Each point is colored by the empirical \(E_{g}(p)\), while the contour lines show the theoretical \(E_{g}(p)\) value. Among trained models, there was a range of \(R_{em}\) and \(D_{em}\) values even when many models have similar \(E_{g}(p)\). This demonstrates that our geometrical interpretation of the error mode geometry can give additional insights into how models achieve low neural prediction error. In the following sections, we explore how these geometrical properties of the neural prediction error vary with model stage depth, training scheme, and adversarial robustness.

### Error Mode Geometry Varies across Model Stages

We analyzed the neural prediction error obtained from activations at different model stages. In Fig. 4A, we plot the error mode geometry for all analyzed DNN model stages when predicting brain data from V1 and IT. Each point is color coded based on its relative model depth. Lower neural prediction errors were achieved (i.e. points lie on lower \(E_{g}\) contours) for earlier model stages in early visual area V1, and for later model stages in downstream visual area IT. This observation is in agreement with previous findings [6; 11; 43] where artificial neural networks have a similar hierarchy compared to the visual cortex. Qualitatively, the trends across model stages were more prominent than differences observed from comparing model architectures for supervised models (Fig. SI.5.1), or from comparing supervised trained models to self-supervised models of the same architecture (Fig. SI.5.2).

To investigate the source of this geometric difference across model stages, we performed an experiment on each model where we took the eigenspectra \(_{i}\) measured from the first (Early) or last (Late) model stage and paired this with the alignment coefficients \(W_{i}\) measured from the Early or Late model stage (Fig. 4B). These two spectral properties can be varied independently, as \(W_{i}\) depends on

Figure 2: **Summary measures for error mode geometry.****(A)** Error modes (\(}\)) are related to the model eigenspectra (\(_{i}\)) and alignment coefficients (\(W_{i}\)). Error mode geometry summarized by \(R_{em}\) and \(D_{em}\) characterizes the distribution of error modes and directly relates to the neural prediction error \(E_{g}(p)\) **(B)** Interpreting contour plots. Error mode geometry distinguishes between models with the same prediction error (\(E_{g}(p)\)). Neural prediction error is the product of \(R_{em}\) and \(}\) and is thus easily visualized on a contour plot where each contour represents equal \(E_{g}(p)\).

the model eigenvectors but not on the model eigenvalues. We then measured the neural prediction error that would be obtained from such a model. This experiment allows us to isolate whether the observed differences in neural prediction error are primarily due to the \(_{i}\) or \(W_{i}\) terms. In the V1 data, there was little difference in the error mode geometry between the location of the \(W_{i}\) terms, however using early model stage \(_{i}\) corresponds to lower neural prediction error (summarized in Fig. 4C). In contrast, for the IT data using the late stage \(W_{i}\) terms corresponded lower \(E_{g}(p)\) mainly through a reduction of the error mode \(R_{em}\) (i.e. many error modes are better predicted). The spectral properties are shown in full for the early and late stages of ResNet50 as measured on the V1 and IT datasets (Fig. 4D). These results highlight the need to study the spectral properties of the model together with the brain responses as summarized by \(_{i}\): Simply studying the properties of the eigenspectra of the model is insufficient to characterize neural prediction error (see SI.5.5 for more details).

### Trained vs. Untrained

We analyzed how the error mode geometry for neural predictions differed between trained and randomly initialized DNNs (Fig. 5A). In line with previous results [11; 14; 44], we found that training yielded improved neural predictions as measured via smaller \(E_{g}(p)\) (Fig. 5B). This improvement was most notable in regions V2, V4, and IT, where there was also a characteristic change in the error mode geometry. In these regions, while \(R_{em}\) decreased with training, \(D_{em}\) surprisingly increased. However, the decrease in \(R_{em}\) was large enough to compensate for the higher \(D_{em}\), leading to an overall reduction in \(E_{g}(p)\). In V1, we observed only a small difference in neural prediction error between trained and untrained models, similar to previous results for early sensory regions [45; 14]. As shown in Fig. SI.5.7, we found qualitatively similar results in a different V1 dataset with a larger number of tested stimuli .

What differed between the trained and random model representations that led to differences in error mode geometry? To gain insight into this, we investigated how the eigenspectra (\(_{i}\)) and the alignment coefficients (\(W_{i}\)) individually contributed to the observed error mode geometry in visual area IT. We performed an experiment on the trained and random ResNet50 model activations where

Figure 3: **Error mode geometry of model predictions for each brain region.****(A)** Theoretical values of neural prediction error match empirical values from ridge regression. Each point corresponds to predictions obtained from one of the 516 analyzed model stages. **(B)** Error mode dimension and radius for each brain region. Each point corresponds to the error mode \(}\) and \(R_{em}\) for the region from a specific model stage of trained neural network models. Points are colored by the empirical error from performing ridge regression between the model activations and neural responses. \(R_{em}\) and \(}\) values are obtained from theoretical values, and contour lines correspond to theoretical values of the neural prediction error, such that points along the contours have the same error.

we measured \(_{i}\) from one model and paired it with the \(W_{i}\) measured from the other model (Fig. 5C). Using \(W_{i}\) from the trained model led to much lower \(R_{em}\) than when using \(W_{i}\) from the random model. This decrease in \(R_{em}\) when using the \(W_{i}\) terms from the trained model was the main driver of the improvement in \(E_{g}(p)\) when we used the trained model to predict the IT neural data. In contrast, when using the eigenspectra of the random model, the \(D_{em}\) was lower than when using the eigenspectra of the trained model, at the cost of a slight increase in \(R_{em}\). Note that the model eigenspectra clearly decayed at a significantly faster rate in the random vs. trained models (Fig. 5D), but it was the properties of the alignment terms \(W_{i}\) that drove the change in \(E_{g}(p)\) between trained and untrained models. Thus, the eigenvectors of the trained model were better aligned with the neural data compared to the random model, which resulted in low prediction error regardless of which eigenspectra was used.

### Adversarial vs. Standard Training

Recent work has suggested that adversarially trained networks have representations that are more like those of biological systems [47; 48; 15]. We analyzed the neural prediction error and error mode geometry of adversarially trained ("Robust") neural network models [49; 50], and their non-adversarially trained counterparts. The error mode geometry for an \(_{2}(=3)\) ResNet50 is given in Fig. 6A, and similar results are shown for an \(_{}(=4)\) ResNet50 in Fig. SI.5.8.

The effect of adversarial training on error mode geometry varied across model stages and cortical regions. In the V1 dataset, lower neural prediction error was observed for early stages of robust

Figure 4: **Model stage depth and error mode geometry.****(A)** Each point represents the error mode geometry obtained from a specific model stage of a trained neural network, where the color of the value represents the depth of the model stage. The color is normalized for each model to the \(\) range where the earlier and later model stages have lighter and darker colors, respectively. \(R_{em}\) and \(}\) values are obtained from theoretical values, and contour lines correspond to theoretical values of the neural prediction error as in Fig. 2. **(B,C)** Predicted \(E_{g}(p)\) that would be obtained when using the eigenspectra (\(_{i}\)) from the first (Early) or last (Late) stage of each of the 32 analyzed models, paired with the alignment coefficients (\(W_{i}\)) terms from the Early or Late stage of each model. Full error mode geometry is given in (B) where each point corresponds to a single model with the chosen \(W_{i}\) and \(_{i}\) terms, and (C) shows box plots summarizing the \(E_{g}(p)\) from each comparison. In V1, using the \(_{i}\) from the early stage of the model resulted in better neural prediction error and a lower \(R_{em}\) but higher \(}\), regardless of the \(W_{i}\) terms used. In IT, using the alignment terms \(W_{i}\) from the late model stage resulted in a smaller neural prediction error and lower \(R_{em}\), with little change in \(}\). **(D)** Full spectra for \(_{i}\), \(W_{i}\) and \(}\) from Early and Late model stages of ResNet50 on the V1 and IT datasets.

models (Fig. 6B), in line with previous work . This corresponded to a decrease in \(R_{em}\) with little change in \(D_{em}\), suggesting that the decrease in neural prediction error was shared relatively equally across all error modes. In contrast, at late model stages, \(D_{em}\) was smaller for the robust models compared to the standard models, but there was no difference in neural prediction error between the robust and standard networks. In regions V2, V4, and IT, we observed that there was little difference in neural prediction error between the standard and robust models, but that error mode geometry in robust models was associated with a lower \(D_{em}\) and a higher \(R_{em}\). This suggests that the difference between robust and standard models in these regions was due to a small number of error modes having higher \(}\) in the robust models. This did not lead to better neural prediction error for regions V2, V4, and IT, but nevertheless, the error mode geometry revealed that the data was predicted in different ways.

To better understand the differences between robust and standard networks in V1, we ran a series of experiments testing the effects of adversarial training on model eigenspectra and alignment with neural data. For each model stage of the ResNet50 architecture, we paired the eigenspectra (\(_{i}\)) of the standard or robust network with the alignment coefficients (\(W_{i}\)) of the standard or robust network (Fig. 6C). We then calculated the error mode geometry and neural prediction error for all four possible pairings. This approach allowed us to examine how adversarial training-related changes in the spectrum and alignment coefficients affected the error mode geometry for each model stage.

We found that the alignment coefficients of the robust models consistently yielded lower V1 prediction error (Fig. 6D). In early model stages, using the \(W_{i}\) from the robust models led to a lower neural

Figure 5: **Error mode geometry changes with model training.****(A)** Error mode for trained models (blue) and randomly initialized models (green) for each brain region. The same model stage of each network is connected with a line, and stage depth is shown with opacity. Contours are given by theoretical values of \(E_{g}(p)\), as in previous plots. **(B)** Neural prediction error was lower for trained models compared to random models in all brain regions, but most notable in V2, V4, and IT. Median is given as a black line and box extends from the first quartile to third quartile of the data. Whiskers denote 1.5x the interquartile range. **(C)** Predicted \(E_{g}(p)\) that would be obtained on IT dataset when using eigenspectra from trained or random ResNet50 models, paired with the alignment \(W_{i}\) terms from each of the trained or random ResNet50 models. Size of point denotes model stage depth. Using the \(W_{i}\) terms from the trained model resulted in a lower \(E_{g}(p)\) due to a lower \(R_{em}\), while using the \(_{i}\) from the trained models increased \(D_{em}\) and did not appreciably change \(E_{g}(p)\). **(D)** Model eigenvalues, alignments, and error modes for the final analyzed model stage of the ResNet50 architecture on IT dataset for both trained and random models. Even though the eigenspectra for the random model was notably different than the eigenspectra for the trained model, we observed in (C) that the driver of the decreased \(E_{g}(p)\) for the trained model was a difference in the \(W_{i}\) terms.

prediction error via reduced \(R_{em}\), and the choice of \(_{i}\) had a negligible effect on the prediction error for these stages. In the later model stages, the \(W_{i}\) from the robust models also achieved a lower \(E_{g}\), but for these stages, the choice of \(_{i}\) had a non-negligible impact on the resulting prediction error. Overall, these results suggest that adversarial training leads to an improved alignment between the eigenvectors of the model and early visual cortex activity.

## 4 Discussion

Many metrics have been proposed to compare model representations to those observed in biological systems [51; 52; 22; 23; 11]. However, it is typically unclear which aspects of the representational geometry lead to a particular similarity score. In this work, we applied the theory of spectral bias and task-model alignment for kernel regression  to the generalization error for encoding models of neural activity. This provides a theoretical link between a model's ability to predict neural responses and the eigenspectra and eigenvectors of model representations. We introduced geometric measures \(R_{em}\) and \(D_{em}\) of the error modes to summarize how the spectral properties relate to neural prediction error, and showed that models with similar errors may have quite different geometrical properties. We applied this theory to investigate the roles of stage depth, learned representations, and adversarial training in neural prediction error throughout the ventral visual stream.

_Dimensionality of neural and model representations_

Recent work has investigated spectral properties of neural activity in biological [53; 54] and artificial [31; 28] systems. One empirical study suggested that high-dimensional model representations have

Figure 6: **Error mode geometry and adversarial robustness.****(A)** Error mode geometry for standard ResNet50 (Blue) and adversarially-trained (Robust) ResNet50 (Green) models for each brain region. The same stage of each network is connected with an arrow, and stage depth is shown with opacity. Contours are given by theoretical values of \(E_{g}(p)\) as in previous plots. **(B)** For standard and robust ResNet50 models, \(E_{g}(p)\) for V1 data as a function of stage depth. **(C)** Predicted \(E_{g}(p)\) that would be obtained on V1 dataset for \(_{i}\) of standard or robust ResNet50 networks paired with \(W_{i}\) of standard or robust ResNet50 networks. Each point was from a single stage of the model, and plots are broken up by the early stages (first 8/16 analyzed stages) and the late stages (last 8/16 analyzed stages) of the model for visualization purposes. **(D)** Summarized neural prediction error for the experiment in (C). For all stages, the robust \(W_{i}\) led to lower error, but in the late stages, there was an additional effect from the choice of \(_{i}\).

lower neural prediction error, where effective dimensionality was defined as the participation ratio of the eigenvalues alone . However, we empirically found that this effective dimensionality of representations was not well correlated with prediction error for our datasets (Sec. S1.5.5). The lack of a consistent relationship between effective dimensionality and neural prediction error was expected based on our theory, as the neural prediction error depends both on the eigenvalues and eigenvectors of a model. These quantities co-vary from model to model, and thus model comparisons should include information from both. In this work, we demonstrated how our theory can be used to examine whether differences in observed prediction error are primarily due to the eigenvalues or alignment coefficients (which are dependent only on the eigenvectors), and found that in many instances, the alignment coefficients were the main driver of lower prediction error.

_Dependence of Neural Prediction Error on Training Set Size_

Our spectral theory of neural prediction characterizes the relationship between the training set size, the generalization error, and the spectral properties of the model and brain responses. This highlights that model ranking based on neural prediction error may be sensitive to the train/test split ratio. In Eq. 3, we see that, for small \(p\), the error modes corresponding to small \(_{i}\) generally remain close to their initial value (\(}(p)}(0)\)), and hence their contribution to neural prediction error does not decay (Eq. 4). On the other hand, at large sample size \(p\), these small \(_{i}\) error modes change from their initial values and therefore may change the ranking of the models. These observations regarding the role of model eigenspectra and dataset size may be used in the future to design more comprehensive benchmarks for evaluating models.

_Limitations_

Although our empirical analysis follows what we expect from theory and shows a diversity of error mode geometries, the datasets that we used (particularly for V1 and V2) have a limited number of samples. In some cases, such as the large \(p\) limit, theoretical results for the neural prediction error may not match empirical results. Additionally, we only investigated one dataset for V2, V4, and IT, and only two datasets for V1. Future work may reveal different trends when using different datasets. Moreover, this theory assumes that the brain responses are deterministic functions of the stimuli. As shown in , this assumption may be relaxed, and future work can examine the role of neuronal noise in these contexts.

_Future work_

The results in this paper demonstrate that our theory can characterize the complex interplay between dataset size, representational geometry, and neural prediction error. It provides the basis for numerous future directions relating spectral properties of the model and data to measures of brain-model similarity. Insights from our theory may suggest ways that we can build improved computational models of neural activity.

While we focused on DNN models pre-trained on image datasets, a closely related line of work has investigated the properties of DNN models directly trained to predict neural responses . Future work could compare the error mode geometries from end-to-end networks to those obtained from networks pre-trained on image data. Such an analysis could elucidate how these two vastly different training schemes are both able to predict neural data well. Additionally, our experiments focused on the case of the same small regularization coefficient for all regression mappings, but future work could explore the case of optimal regularization. The spectral theory can also be applied to other types of brain-model comparisons, such as Partial Least Squares or Canonical Correlation Analysis.

Although we were able to differentiate many error mode geometries based on \(R_{em}\) and \(}\), we initially expected to see many more values of \((R_{em},})\) for each fixed \(E_{g}(p)\). The investigated vision models seem to be constrained to a particular region in the \((R_{em},})\)-space (Fig. 3). Future work may shed light on whether this is due to implicit biases in model architectures or due to a fundamental coupling of \((R_{em}\) and \(})\) due to constraints on the \(E_{i}(p)\) term.

There are many architectural differences between the tested DNNs and biological systems. Future work may investigate how biologically plausible constraints  (e.g., stochasticity , divisive normalization , power-law spectra ) and diverse training objectives ) lead to improved neural prediction error through changes in error mode geometry.