# Shapes analysis for time series

Thibaut Germain\({}^{1}\)

Centre Borelli, ENS Paris-Saclay

4 av. des sciences, 91190

&Samuel Gruffaz\({}^{1}\)

Centre Borelli, ENS Paris-Saclay

4 av. des sciences, 91190

Charles Truong\({}^{1}\)

Centre Borelli, ENS Paris-Saclay

4 av. des sciences, 91190

Laurent Oudre\({}^{1}\)

Centre Borelli, ENS Paris-Saclay

4 av. des sciences, 91190

Alain Durmus

CMAP, CNRS, Ecole polytechnique

Institut Polytechnique de Paris

91120 Palaiseau, France

Complete affiliation: Universite Paris Saclay, Universite Paris Cite, ENS Paris Saclay, CNRS, SSA, INSERM, Centre Borelli, F-91190, Gif-sur-Yvette, France.Corresponding author. Contact at thibaut.germain@ens-paris-saclay.fr

###### Abstract

Analyzing inter-individual variability of physiological functions is particularly appealing in medical and biological contexts to describe or quantify health conditions. Such analysis can be done by comparing individuals to a reference one with time series as biomedical data. This paper introduces an unsupervised representation learning (URL) algorithm for time series tailored to inter-individual studies. The idea is to represent time series as deformations of a reference time series. The deformations are diffeomorphisms parameterized and learned by our method called TS-LDDMM. Once the deformations and the reference time series are learned, the vector representations of individual time series are given by the parametrization of their corresponding deformation. At the crossroads between URL for time series and shape analysis, the proposed algorithm handles irregularly sampled multivariate time series of variable lengths and provides shape-based representations of temporal data. In this work, we establish a representation theorem for the graph of a time series and derive its consequences on the LDDMM framework. We showcase the advantages of our representation compared to existing methods using synthetic data and real-world examples motivated by biomedical applications.

## 1 Introduction

Our goal is to analyze the inter-individual variability within a time series dataset, an approach of significant interest in physiological contexts [25; 58; 4; 21]. Specifically, we aim to develop an unsupervised feature representation method that encodes the specificities of individual time series in comparison to a reference time series. In physiology, examining the various "shapes" in a time series related to biological phenomena and their variations due to individual differences or pathological conditions is common. However, the term "shape" lacks a precise definition and is more intuitively understood as the silhouette of a pattern in a time series. In this paper, we refer to the shape of a time series as the graph of this signal.

Although community structures with representatives can be learned in an unsupervised manner [55; 39] using contrastive loss [20; 54; 39] or similarity measures [2; 21; 45; 62], the study of inter-individual variability of shapes within a cluster [42; 51] remains an open problem in unsupervised representation learning (URL), particularly for _irregularly sampled_ time series with _variable lengths_.

Our work explicitly focuses on learning shape-based representation of time series. First, we propose to view the shape of a time series not merely as its curve \(\{s_{t}:\ t\}\), but as its graph \((s)=\{(t,s(t)):\ t\}\). Then, building on the shape analysis literature [5; 57], we adopt the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework [5; 57] to analyze these graphs. The core idea is to represent each element \((s^{j})\) of a dataset \((s^{j})_{j[N]}\) as the transformation of a reference graph \((_{0})\) by a diffeomorphism \(_{j}\), i.e. \((s^{j})_{j}.(_{0})\). The diffeomorphism \(_{j}\) is learned by integrating an ordinary differential equation parameterized by a Reproducing Kernel Hilbert Space (RKHS). The parameters \((_{j})_{j[N]}\) encoding the diffemorphisms \((_{j})_{j[N]}\) yield the representation features of the graphs \(((s^{j}))_{j[N]}\). Finally, these shape-encoding features can be used as inputs to any statistical or machine-learning model.

However, a time series graph transformation by a general diffeomorphism is not always a time series graph, see e.g. Figure 1, thus a time series graph is more than a simple curve . Our contributions arise from this observation: we specify the class of diffeomorphisms to consider and show how to learn them. This change is fruitful in representing transformations of time series graphs as illustrated in Figure 2.

Our contributions can be summarized as follows:

* We propose an unsupervised method (TS-LDDMM) to analyze the inter-individual variability of shapes in a time series dataset (Section 4). In particular, the method can handle multivariate time series _irregularly sampled_ and with _variable sizes_.
* We motivate our extension of LDDMM to time series by introducing a theoretical framework with a representation theorem for time series graph (Theorem 1) and kernels related to their structure (Lemma 1).
* We demonstrate the identifiability of the model by estimating the true generating parameter of synthetic data, and we highlight the sensitivity of our method concerning its hyperparameters (Appendix G.1), also providing guidelines for tuning (Appendix D).
* We highlight the _interpretability_ of TS-LDDMM for studying the inter-individual variability in a clinical dataset (Section 5).
* We illustrate the quantitative interest of such representation on classification tasks on real shape-based datasets with regular and irregular sampling (Appendices H and I).

## 2 Notations

We denote by integer ranges by \([k:l]=\{k,,l\}()\) and \([l]=[1:l]\) with \(k,l\), by \(^{m}(,)\) the set of \(m\)-times continously differentiable function defined on an open set \(\) to a normed vector space \(\), by \(||u||_{}=_{x}|u(x)|\) for any bounded function \(u:\), and by \(_{>0}\) is the set of positive integers.

Figure 1: A time seriesâ€™ graph \(=\{(t,s(t)):\ t\}\) can lose its structure after applying a general diffeomorphism \(.\): a time value can be related to two values on the space axis.

## 3 Background on LDDMM

In this part, we expose how to learn the diffeomorphisms \((_{j})_{j[N]}\) using LDDMM, initially introduced in . In a nutshell, for any \(j[N]\), \(_{j}\) corresponds to a differential flow related to a learnable velocity field belonging to a well-chosen Reproducing Kernel Hilbert Space (RKHS).

In the next section, time series are going to be represented by diffeomorphism parameters \((_{j})_{j[N]}\). That is why LDDMM is chosen since it offers a parametrization for diffeomorphisms that is sparse and interpretable, two features particularly relevant in the biomedical context.

The basic problem that we consider in this section is the following. Given a set of targets \(=(y_{i})_{i[T_{2}]}\) in \(^{d^{}}\)2, a set of starting points \(=(x_{i})_{i[T_{1}]}\) in \(^{d^{}}\), we aim to find a diffeomorphism \(\) such that the finite set of points \(\) is similar in a certain sense to the set of finite sets of transformed points \(=((x_{i}))_{i[T_{1}]}\). The function \(\) is occasionally referred to as a _deformation_. In general, these sets \(,\) are meshes of continuous objects, e.g., surfaces, curves, images, and so on.

Representing diffeomorphisms as deformations.Such _deformations_\(\) are constructed via differential flow equations, for any \(x_{0}^{d^{}}\) and \(\):

\[X()}{}=v_{}(X()), X(0)=x_{0}\,_{}^{v}(x_{0})=X(),^{v}=_{1}^{v}\,\] (1)

where the velocity field is \(v: v_{}\) and \(\) is a Hilbert space of continuously differentiable function on \(^{d^{}}\). If \(||\,u||_{}+||u||_{}||u||_{}\) for any \(u\) and \(v^{2}(,)=\{v^{0}(,): _{0}^{1}||v_{}||_{}^{2}\,<\}\), by [22, Theorem 5]\(^{v}\) exists and belongs to \((^{d^{}})\), where we denote by \(()\) the set of diffeomorpshim defined on an open set \(\) to \(\). Therefore, for any choice of \(v\), \(^{v}\) defines a valid deformation. This offers a general recipe to construct diffeomorphism given a functional space \(\).

With this in mind, the velocity field \(v\) fitting the data can be estimated by minimizing \(v^{2}(,)(^{v}., )\), where \(\) is an appropriate loss function. However, two computational challenges arise. First, this optimization problem is ill-posed, and a penalty term is needed to obtain a unique solution. In addition, a parametric family \(_{}^{2}(,)\), parameterized by \(\), is sought to efficiently solve this minimization problem.

From deformations to geodesics.It has been proposed in  to interpret \(\) as a tangent space relative to the group of diffeomorphisms \(=\{^{v}:\ v^{2}(,)\}\). Following this geometric point of view, geodesics can be constructed on \(\) by using the following squared norm

\[^{2}:g_{v^{2}(,):\ g=^{v}}_{0}^{1}||v_{}||_{}^{2}\,\] (2)

By deriving differential constraints related to the minimum of (2) and using Cauchy-Lipschitz conditions, geodesics can be defined only by giving the starting point and the initial velocity \(v_{0}\), as straight lines in Euclidean space. Denoting by \(_{v_{0}}()\) the geodesic starting from

Figure 2: LDDMM and TS-LDDMM are applied to ECG data. We observe that LDDMM, using a general Gaussian kernel, does not learn the time translation of the first spike but changes the space values, i.e., one spike disappears before emerging at a translated position. At the same time, TS-LDDMM handles the time change in the shape. This difference of _deformations_ implies differences in features _representations_.

the identity with unital velocity \(v_{0}\), the exponential map is defined as \(^{\{v_{0}\}}_{v_{0}}(1)\). Using \(^{\{v_{0}\}}\) instead of \(^{v}\), the previous matching problem becomes a _geodesic shooting problem_:

\[_{v_{0}}(^{\{v_{0}\}}.,).\] (3)

Using \(^{\{v_{0}\}}\) instead of \(^{v}\) for any \(v^{2}(,)\) regularizes the problem and induces a sparse representation for the learning diffeomorphisms. Moreover, by setting \(\) as an RKHS, the geodesic shooting problem has a unique solution and becomes tractable, as described in the next section.

Discrete parametrization of diffeomorphslim.In this part, \(\) is chosen as an RKHS  generated by a smooth kernel \(K\) (e.g., Gaussian). We follow  and define a discrete parameterization of the velocity fields to perform geodesics shooting (3). The initial velocity field \(v_{0}\) is chosen as a finite linear combination of the RKHS basis vector fields, \(_{0}\) control points \(_{0}=(x_{k,0})_{k[_{0}]}(^{d^{}})^{ _{0}}\) and momentum vectors \(_{0}=(_{k,0})_{k[_{0}]}(^{d^{}})^ {_{0}}\) are defined such that for any \(x^{d^{}}\),

\[v_{0}(_{0},_{0})(x)=_{k=1}^{_{0}}K(x,x_{k,0})_{k,0}\;.\] (4)

In our applications, the control points \((x_{k,0})_{k[_{0}]}\) can be understood as the discretized graph \((t_{k},_{0}(t_{k}))_{k[_{0}]}\) of a starting time series \(_{0}\). With this parametrization of \(v_{0}\),  show that the velocity field \(v\) of the solution of (3) keeps the same structure along time, such that for any \(x^{d^{}}\) and \(\),

\[v_{}(x)=_{k=1}^{_{0}}K(x,x_{k}())_{k}()\;,\]

\[x_{k}()}{}=v_{}(x_{k}( ))\;,_{k}()}{}=-_{k=1}^{ _{0}}_{x_{k}()}K(x_{k}(),x_{l}())_{l}( )^{}_{k}()\\ _{k}(0)=_{k,0}, x_{k}(0)=x_{k,0}\;,k[_{0}]\] (5)

These equations are derived from the hamiltonian \(H:(_{k},x_{k})_{k[_{0}]}_{k,l=1}^{_{0 }}_{k}^{}K(x_{k},x_{l})_{l}\), such that the velocity norm is preserved \(||v_{}||_{}=||v_{0}||_{}\) for any \(\). By (5), the velocity field related to a geodesic \(v^{*}\) is fully parametrized by its initial control points and momentum \((x_{k,0},_{k,0})_{k[_{0}]}\). Thus, given a set of targets \(=(y_{i})_{i[T_{i}]}\) in \(^{d^{}}\), a set of starting points \(=(x_{i,0})_{i[T_{1}]}\) in \(^{d^{}}\), a RKHS's kernel \(K:^{d^{}}^{d^{}}^{d ^{} d^{}}\), a distance on sets \(\), a numerical integration scheme of ODE and a penalty factor \(>0\), the basic geodesic shooting step minimizes the following function using a gradient descent method:

\[_{,}:(_{k})_{k[T_{1}]}(^{\{v_{0}\}}.,)+||v_{0}||_{ }^{2}\;,\] (6)

where \(v_{0}\) is defined by (4) and \(^{\{v_{0}\}}.\) is the result of the numerical integration of (5) using control points \(\) and initial momentums \((_{k})_{k[T_{1}]}\).

Relation to Continuous Normalizing Flows.One particular popular choice to address the problem of learning a diffeomorphism or a velocity field is Normalizing Flows [47; 32] (NF) or their continuous counterpart [13; 24; 48] (CNF). However, we do not rely on this class of learning algorithms for several reasons. Indeed, existing and simple normalizing flows are not suitable for the type of data that we are interested in this paper [19; 16]. In addition, they are primarily designed to have tractable Jacobian functions, while we do not require such property in our applications. Finally, the use of a differential flow solution of an ODE (1) trick is also at the basis of CNF, which then consists of learning a velocity field to address in fitting the data through a loss aiming to address the problem at hand. Nevertheless, the main difference between CNF and LDDMM lies in the parametrization of the velocity field. LDDMM uses kernels to derive closed form formula and enhance interpretability while NF and CNF take advantage of deep neural networks to scale with large dataset in high dimensions.

## 4 Methodology

We consider in this paper observations which consist in a population of \(N\) multivariate time series, for any \(j[N]\), \(s^{j}^{1}(_{j},^{d})\). However, we can only access a \(n_{j}\)-samples \(^{j}=(^{j}_{i}=s^{j}(t^{j}_{i}))_{i[n_{j}]}\)collected at timestamps \((t^{j}_{i})_{i[n_{j}]}\) for any \(j[N]\). Note that **the number of samples \(n_{j}\) is not necessarily the same across individuals** and the timestamps can be **irregularly sampled**. We assume the time series population is globally homogeneous regarding their "shapes" even if inter-individual variability exists. Intuitively speaking, the "shape" of a time series \(s:1^{d}\) is encoded in its graphs \((s)\) defined as the set \(\{(t,s(t)):\ t\}\) and not only in its values \(s()=\{s(t):\ t\}\) since the time axis is crucial. As a motivating use-case, \(s^{j}\) can be the time series of a heartbeat extracted from an individual's electrocardiogram (ECG), see Figure 2. The homogeneity in a resulting dataset comes from the fact that humans have similar shapes of heartbeat .

The deformation problem.In this paper, we aim to study the inter-individual variability in the dataset by finding a relevant representation of each time series. Inspired from the framework of shape analysis , addressing similar problems in morphology, we suggest to represent each time series' graph \((s^{j})\) as the transformation of a reference graph \((_{0})\), related to a time series \(_{0}:1^{d}\), by a diffeomorphism \(_{j}\) on \(^{d+1}\), for any \(j[N]\),

\[_{j}.(_{0})=\{_{j}(t,_{0}(t) ),\ t\}\.\] (7)

\(_{0}\) will be understood as the typical representative shape common to the collection of time series \((s^{j})_{j[N]}\). As \(_{0}\) is supposed to be fixed, then the representation of the time series \((s^{j})_{j[N]}\) boils down to the one of the transformation \((_{j})_{j[N]}\). We aim to learn \((_{0})\) and \((_{j})_{j[N]}\).

Optimization related to (7).Defining the _discretized graphs_ of the time series \((s^{j})_{j[N]}\) and a discretization of the reference graph \((_{0})\) as, for any \(j[N]\),

\[_{j}=(^{j})=(t^{j}_{i},^{j}_{i})_{i[ n_{j}]}(^{d+1})^{n_{j}},}_{0}=(t^{0}_{i}, ^{0}_{i})_{i[_{0}]}(^{d+1})^{_{0}}\,\]

with \(_{0}=((n_{j})_{j[N]})\), the representation problem given in (7) boils down solving:

\[_{}_{0},(^{j}_{k})_{k[ _{0}]}}_{j=1}^{N}_{_{0},_{j}} ((^{j}_{k})_{k[_{0}]})\,\] (8)

which is carried out by gradient descent on the control points \(}_{0}\) and the momentum \(_{j}=(^{j}_{k})_{k[_{0}]}\) for any \(j[N]\), initialized by a dataset's time series graph of size \(_{0}\) and by \(0_{(d+1)_{0}}\) respectively. The optimization hyperparameter details are given in Appendix E.1. The result of the minimization \(}_{0}\) is then considered as the \(_{0}\)-samples of a common time series \(_{0}\) and the momentums \(_{j}\) encoding \(_{j}\) yields a feature vector in \(^{d_{0}}\) of \(s^{j}\) for any \(j[N]\). Finally, the vectors \((_{j})_{j[N]}\) can be analyzed with any statistical or machine learning tools such as Principal Components Analysis (PCA), Latent Discriminant Analysis (LDA), longitudinal data analysis and so on.

Nevertheless, (8) asks to define a kernel and a loss in order to perform geodesic shooting (6), which is the purpose of the following subsection.

### Application of LDDMM to time series analysis: TS-LDDMM

This section presents our theoretical contribution: we tailor the LDDMM framework to handle time series data. The reason is that applying a general diffeomorphism \(\) from \(^{d+1}\) to a time series' graph \((s)\) can result in a set \(.(s)\) that does not correspond to the graph of any time series, as illustrated in the Figure 1. Thus, time series graphs have more structure than a simple 1D curve  and deserve their unique analysis, which will prove fruitful as demonstrated in Section 5.

To address this challenge, we need to identify an RKHS kernel \(K:^{d+1}^{d+1}^{(d+1)^{2}}\) that generates deformations preserving the structure of the time series graph. This goal motivates us to clarify, in Theorem 1, the specific representation of diffeomorphisms we require before presenting a class of kernels that produce deformations with this representation.

Similarly, selecting a loss function on sets \(\) that considers the temporal evolution in a time series' graph is crucial for meaningful comparisons with time series data. Consequently, we introduce the oriented Varifold distance.

A representation separating space and time.We prove that two time series graphs can always be linked by a time transformation composed with a space transformation. Moreover, a time series graph transformed by this kind of transformation is always a time series graph. We define \((^{d+1}):(t,x)^{d+1}((t),x)\) for any \(()\) and \(_{f}:(t,x)^{d+1}(t,f(t,x))\) for any \(f^{1}(^{d+1},^{d})\). We have the following representation theorem. All proofs are given in Appendix B.

Denote by \((s)\{(t,s(t)):\ t\}\) the graph of a time series \(s:^{d}\) and \(.(s)\{(t,s(t)):t\}\) the action of \((^{d+1})\) on \((s)\).

**Theorem 1**.: _Let \(s:^{d}\) and \(_{0}:^{d}\) be two continuously differentiable time series with \(,\) two intervals of \(\). There exist \(f^{1}(^{d+1},^{d})\) and \(()\) such that \(()=\) and \(_{f}(^{d+1})\),_

\[(s)=_{,f}.(_{0}),\ _{,f}= _{}_{f}.\]

_Moreover, for any \(^{1}(^{d+1},^{d})\) and \(()\), there exists a continously differentiable time series \(\) such that \(()=_{,}.(_{0})\)_

**Remark 2**.: _Note that for any \(()\) and \(s^{0}(,^{d})\),_

\[\{((t),s(t)),\ t\}=\{(t,s^{-1}(t)):\ t ()\}\;.\]

_As a result, \(_{}\) can be understood as a temporal reparametrization and \(_{f}\) encodes the transformation about the space._

Choice for the kernel associated with the RKHS VAs depicted on Figure 1-2, we can not use any kernel \(K\) to apply the previous methodology to learn deformations on time series' graphs. We describe and motivate our choice in this paragraph. Denote the one-dimensional Gaussian kernel by \(K_{}^{(a)}(x,y)=(-|x-y|^{2}/)\) for any \((x,y)(^{a})^{2}\), \(a\) and \(>0\). To solve the geodesic shooting problem (6) on \(^{d+1}\), we consider for V the RKHS associated with the kernel defined for any \((t,x),(t^{},x^{})(^{d+1})^{2}\):

\[K_{}((t,x),(t^{},x^{}))=c_{ 0}K_{}&0\\ 0&c_{1}K_{}\;,\] (9) \[K_{}=K_{_{T,1}}^{(1)}(t,t^{})K_{_{ x}}^{(d)}(x,x^{})_{d}\;,K_{}=K_{_{T,0}}^{(1)}(t,t^{ })\;,\]

parametrized by the widths \(_{T,0},_{T,1},_{x}>0\) and the constants \(c_{0},c_{1}>0\). This choice for \(K_{}\) is motivated by the representation Theorem 1 and the following result.

**Lemma 1**.: _If we denote by \(\) the RKHS associated with the kernel \(K_{}\), then for any vector field \(v\) generated by (5) with \(v_{0}\) satisfying (4), there exist \(()\) and \(f^{1}(^{d+1},^{d})\) such that \(^{v}=_{}_{f}\)._

Instead of Gaussian kernels, other types of smooth kernels can be selected as long as the structure (9) is respected.

**Remark 3**.: _With this choice of kernel, the features associated with the time transformation can be extracted from the momentums \((_{k,0})_{k[_{0}]}(^{d+1})^{_{0}}\) in (4) by taking the coordinates related to time. However, the features related to the space transformation are not only in the space coordinates since the related kernel \(K_{}\) depends on time as well.The kernel's representation has been carefully designed to integrate both space and time, while ensuring that time remains independent of space. Initially, we considered separating the spatial and temporal components. However, post-hoc analysis of such a representation proved to be challenging. The separated spatial and temporal representations are correlated, and understanding this correlation is essential for interpreting the data. As a result, concatenating the two representations becomes necessary, though there is no straightforward method for doing so, as they are not commensurable. Consequently, we opted for a representation that inherently integrates both space and time._

In Appendix D, we give guidelines for selecting the hyperparameters \((_{T,0},_{T,1},_{x},c_{0},c_{1})\).

LossThis section specifies the distance function \(\) introduced in the loss function defined in (6).

In practice, we can only access discretized graphs of time series, \((t_{i}^{j},_{i}^{j})_{i[n_{j}]}\) for any \(j[N]\), that are potentially of different sizes \(n_{j}\) and sampled at different timestamps \((t_{i}^{j})_{i[n_{j}]}\) for any \(j[N]\). Usual metrics, such as the Euclidean distance, are not appealing as they make the underlying assumptions of equal size sets and the existence of a pairing between points. Distances between measures on sets(taking the empirical distribution), such as Maximum Mean Discaprency (MMD) [18; 9], alleviate those issues; however, MMD only accounts for positional information and lacks information about the time evolution between sampled points. A classical data fidelity metric from shape analysis corresponding to the distance between _oriented varifolds_ associated with curves alleviates this last issue . Intuitively, an oriented varifold is a measure that accounts for positional and tangential information about the underlying curves at sample points. More details and information about _oriented varifolds_ can be found in Appendix C.

More precisely, given two sets \(_{0}=(g^{0}_{i})_{i[T_{0}]},_{1}=(g^{1}_{i})_{i[T_{ 1}]}(^{d+1})^{T_{1}}\) and a kernel3\(k:(^{d+1}^{d})^{2}\) verifying (30, Proposition 2 & 4), for any \(\{0,1\}\) and \(i[T_{}-1]\), denoting the center and length of the \(i^{th}\) segment \([g^{}_{i},g^{+1}_{i}]\) by \(c^{}_{i}=(g^{}_{i}+g^{}_{i+1})/2\), \(l^{}_{i}=\|g^{}_{i+1}-g^{}_{i}\|\), and \(^{}_{i}=(g^{}_{i+1}-g^{}_{i})/l^{}_{i}\), the varifold distance between \(_{0}\) and \(_{1}\) is defined as,

\[d^{2}_{^{*}}(_{0},_{1})=_{i,j= 1}^{T_{0}-1}l^{0}_{i}k((c^{0}_{i},^{0}_{i}),(c^{0}_{j},^{0}_{j}))l^{0}_{j}-2_{i=1}^{T_{0}-1}_{j=1}^{T_{1}-1}l^{0}_{i}k((c^{0}_{i}, ^{0}_{i}),(c^{1}_{j},^{1}_{j}))l^{1}_{j}\] \[+_{i,j=1}^{T_{1}-1}l^{1}_{i}k((c^{1}_{i},^{1}_{i} ),(c^{1}_{j},^{1}_{j}))l^{1}_{j}\]

In practice, we set the kernel \(k\) as the product of two anisotropic Gaussian kernels, \(k_{}\) and \(k_{}\), such that for any \((x,\,),(y,\,)(^{d+1}^{d })^{2}\)

\[k((x,\,),(y,\,))=k_{}(x,y)k_{} (\,\,,\,)\.\]

Note that the loss kernel \(k\) has nothing to do with the velocity field kernel denoted by \(K_{}\) or \(K\) specified in Section 4.1. Finally, we define the data fidelity loss function, \(\), as a sum of \(d^{2}_{}\), using different kernel's width parameters \(\) to incorporate multiscale information. \(\) is indeed differentiable with respect to its first variable. The specific kernels \(k_{},k_{}\) that we use in our experiments are given Appendix C.1. For further readings on curves and surface representation as varifolds, readers can refer to [30; 12].

A pedagogical online application is available to inspect the effect of hyperprameters on geodesic shooting (5) and registration (6).

## 5 Experiments

The source code is available on Github4. For conciseness, several experiments are relegated in appendix:

1. **TS-LDDMM representation identifiability, Appendix G:** On synthetic data, we evaluate the ability of our method to retrieve the parameter \(v^{*}_{0}\) that encodes the deformation \(^{\{v^{*}_{0}\}}\) acting on a time series graph \(\) by solving the geodesic shooting problem (6) between \(\) and \(^{\{v^{*}_{0}\}}\). \(\). **Results** show that TS-LDDMM representations are identifiable or weakly identifiable depending on the velocity field kernel \(K_{G}\) specification.
2. **Robustness to irregular sampling, Appendix H:** We compare the robustness of TS-LDDMM representation with 9 URL methods handling irregularly sampled multivariate time series on 15 shape-based datasets (7 univariates & 8 multivariates). We assess methods' classification performances under regular sampling (0% missing rate) and three irregular sampling regimes (30%, 50%, and 70% missing rates), according to the protocol depicted in . **Results** show that our method, TS-LDDMM, outperforms all methods for sampling regimes with missing rates: 0%, 30%, and 50%.
3. **Classification benchmark on regularly sampled datasets, Appendix I:** We compare performances of a kernel support vector machine (SVC) algorithm based on TS-LDDMM representation with 3 state-of-the-art classification methods from shape analysis on 15 shape-based datasets (7 univariates & 8 multivariates). **Results** show that the TS-LDDMM-based method outperforms other methods (best performances over 13 datasets), making TS-LDDMM representation relevant for time series shape analysis.

4. **Noise sensitivity for learning the reference graph, Appendix J:** We evaluate the noise sensitivity of TS-LDDMM and Shape-FPCA  for learning the reference graph on a synthetic dataset and for several levels of additive Gaussian noise. **Results** show that both methods are sensitive to noise. However, TS-LDDMM preserves the overall shape while shape-FPCA alters the shape depending on the noise level.

### Interpretability: mice ventilation analysis

This experiment highlights the _interpretability_ of TS-LDDMM representation for studying the inter-individual variability in biomedical applications. We consider a time series dataset monitoring the evolution of mice's nasal and thoracic airflow when exposed to a drug altering respiration . The dataset includes recordings of 7 control mice (WT) and 7 mutant mice (ColQ) with an enzyme deficiency. The enzyme is involved in the respiration regulation, and the drug inhibits its activity. For each mouse, airflows were monitored for 15 to 20 minutes before the drug exposure and then for 35 to 40 minutes. A complete description of the dataset is given in the Appendix F.1.

Experimental protocol.We considered two experimental scenarios; the first focuses on mice ventilation before exposure to explore the inter-individual and genotype-specific variabilities. The second focuses on whole recordings to analyze the evolution of mice's ventilation after drug exposure. In both cases, the baseline protocol consists of first extracting \(N\) respiratory cycles from the datasets with the procedure described in . Then, learning the referent respiratory cycle \(_{0}\) and the representations of respiratory cycles \((_{0}^{j})_{j[N]}\) by solving (8) using TS-LDDMM. \(_{0}^{j}\) being the momentum of the initial velocity field of the geodesic encodings the diffeomorphisms mapping \(_{0}\) to the \(j^{th}\) respiratory cycle. Finally, performing a Kernel-PCA on the initial velocity fields (4) belonging to \(\) and encoded by the pairs \((_{0}^{j},_{0})_{j[N]}\). The first experiment includes \(N_{1}=700\) cycles collected before exposure. The second experiment includes \(N_{2}=1400\) cycles with 25% (resp. 75%) before (resp. after) exposure. We also performed the first experimental scenario with LDDMM representation, and Appendix K describes the settings of both methods. Essentially, varifold losses are identical for both methods, and the velocity field kernels are set to encompass time and space scales. in addition, In addition, Appendix K presents a comparison between TS-LDDMM and Shape-FPCA on the second scenario.

Geodesic shooting along principal component directions.Any principal component (PC), noted \(v_{0}^{pc}\), from a kernel-PCA in \(\), is itself an initial velocity field encoded by a pair \((_{0},_{0}^{pc})\). PCs encode the principal axis of deformations, and it is possible to shoot along the geodesic they encode with the differential equations (5), enabling interpretation of the main sources of deformations.

Figure 4: **(a)** a ColQ respiratory cycle sample. **(b)** Referent respiratory cycle of individual mouse \(_{0}^{j}\) in the TS-LDDMM PC1-PC2 coordinates system of \(_{0}\). **(c)** a WT respiratory cycle sample.

Figure 3: Analysis of the two principal components (PC) related to mice ventilation before exposure with TS-LDDMM representations **(a)**, and LDDMM **(b)**. In both cases and for all PC, the left plot displays PC densities according to mice genotype and right plot displays deformations of the reference graph \(_{0}\) along each PC.

Mice ventilation before exposure.We focus on the analysis of the two first Principal Components (PC) for TS-LDDMM (Figure 3a) and LDDMM (Figure 3b). Looking at the geodesic shooting along PCs, Figure 3 shows that principal components learned with TS-LDMM lead to deformations that remain respiratory cycles. In contrast, deformations learned with LDDMM are challenging to interpret as respiratory cycles. The LDDMM velocity field kernel is a Gaussian anisotropic kernel that accounts for time and space scales; however, the entanglement of time and space dimensions in the kernel does not guarantee the graph structure, and it makes the convergence of the method complex (relative varifold loss error: TS-LDDMM: 0.06, LDDMM: 0.11).

Regarding TS-LDDMM Figure 3a, its PCs refer to deformations directions carrying different physiological meanings. Indeed, the geodesic shooting along these directions indicates that PC1 accounts for variations of the total duration of a respiratory cycle, while PC2 expresses the trade-off between inspiration and expiration duration. In addition, the distribution of CoI6 respiratory cycles along PC1 is wider than in WT mice, indicating that the adaptation of mutant mice to their enzyme deficiency is variable. This observation can also be seen in Figure 4b where a referent respiratory cycle \(_{b}^{j}\) is learned by solving (8) for each mouse and is encoded in the (PC1,PC2) coordinate system of \(_{0}\) by registration (3). Indeed, the average respiratory cycles of ColQ mice are more spread out than WT mice's. Going back to the densities of PC1, ColQ mice distribution has a heavier tail toward negative values compared to WT mice. When shooting in the opposite direction of PC1, we can observe that the inspiration is divided into two steps. Congruently with , such inspirations indicate motor control difficulties due to enzyme deficiency. Figure 4a is an example of ColQ respiratory cycle with negative PC1 coordinate.

Mice ventilation evolution after drug exposure.This experiment focuses on the first principal components learned from TS-LDDMM representations of respiratory cycles randomly sampled before and after drug exposure. Figure 5a illustrates the geodesic shootings along PC1. Again, PC1 accounts for variations in respiratory cycle duration, but more importantly, it can be observed on the deformation at -1.5 \(_{}\) the apparition of a long pause after inspiration. Congruently, Figure 5c indicates that pauses appear after drug exposure as cycles with negative PC1 values mainly occur after 20 minutes and present more variability along PC3. In addition, Figure 5b shows a bimodal distribution for WT mice with one of the peaks in the negative values. This peak was not observed in the previous experiment Figure 3a. It indicates that pauses after inspiration are prevalent in WT mice after drug exposure. On the other hand, the distributions of ColQ mice's respiratory cycles along PC1 in both experiments are similar and account for the same deformation, suggesting that ColQ mice weakly react to the drug exposure as they already adapt their enzyme deficiency.

Experiment Conclusion.Analyzing mice ventilation with TS-LDDMM representation highlights the method's ability to create meaningful interaction between experts and the data. Indeed, combining statistical and visual results shows that main deformations carry physiological meaning, enabling the characterization of some mice genotypes and the effects of drug exposure.

## 6 Related Works

Shape analysis focuses on statistical analysis of mathematical objects invariant under some deformations like rotations, dilations, or time parameterization. The main idea is to represent these objects in a complete Riemannian manifold \((,)\) with a metric \(\) adapted to the geometry of the problem . Then, any set of points in \(\) can be represented as points in the tangent space of their Frechet mean \(_{0}\) by considering their logarithms. The goal is to find a well-suited Riemannian structure according to the nature of the studied object.

Figure 5: Analysis of the first Principal Component (PC1) related to mice ventilation before and after exposure with TS-LDDMM representations. **(a)** displays PC densities per mice genotype, **(b)** illustrates deformations of the reference respiratory cycle \(_{0}\) along PC1, and **(c)** displays all respiratory cycles with respect to time in PC1 and PC3 coordinates

LDDMM framework is a relevant shape analysis tool to represent curves as depicted in . However, graphs of time series are a well-structured type of curve due to the inclusion of the temporal dimension that requires specific care (Figure 1). In a similar vein, Qiu _et al_ proposes a method for tracking anatomical shape changes in serial images using LDDMM. They include temporal evolution, but not for the same purpose: the aim is to perform longitudinal modeling of brain images.

Leaving the LDDMM representation, the results of [53; 26] address the representation of curves with the Square-Root Velocity (SRV) representation. However, the SRV representation is applied after reparametrization of the temporal dimension of the unit length segment. Consequently, the graph structure of the time series is not respected, and the original time evolution of the time series is not encoded in the final representation. Very recently, in a functional data analysis (FDA) framework, a paper  (Shape-FPCA) improved by representing the original time evolution. However, the space and time representations remain correlated, complicating post-hoc analysis, as discussed in Remark 3. Additionally, this method is tailored for _continuous objects_ and applies only to time series of the _same length_, making the estimation more sensitive to noise. This issue can be addressed through interpolation, but this approach is not always reliable in sparse and irregular sampling scenarios. Most FDA approaches, as seen in [50; 63; 59], address this challenge using interpolation or basis function expansion. In summary, FDA methods typically separate space and time representations for continuous objects, whereas TS-LDDMM algorithm maintain a discrete-to-discrete analysis, inherently integrating both space and time representations.

Balancing between discrete and continuous elements is a challenging task. In the deep learning literature [13; 31; 56; 29; 36; 1], Neural Ordinary Differential Equations (Neural ODEs)  learn continuous latent representations using a vector field parameterized by a neural network, serving as a continuous analog to Residual Networks . This approach was further enhanced by Neural Controlled Differential Equations (Neural CDEs)  for handling irregular time series, functioning as continuous-time analogs of RNNs . Extending Neural ODEs, Neural Stochastic Differential Equations (Neural SDEs) introduce regularization effects , although optimization remains challenging. Leveraging techniques from continuous-discrete filtering theory, Ansari et al.  applied successfully Neural SDEs to irregular time series. Oh _et al._ improved these results by incorporating the concept of controlled paths into the drift term, similar to how Neural CDEs outperform Neural ODEs. With TS-LDDMM, the representation is also derived from an ODE, but the velocity field is parameterized with kernels and optimized to have a minimal norm, which enhances interpretability.

All these state-of-the-art methods previously mentionned [23; 43; 60; 26] are compared to TS-LDDMM in Appendix H and Appendix I.

Compared to the Metamorphosis framework , LDDMM framework has weaker assumptions. The 3DMM framework requires that each mesh be re-parametrized into a consistent form where the number of vertices, triangulation, and the anatomical meaning of each vertex are consistent across all meshes, as stated in the introduction of . In our context, we do not need such pre-processing; the time series graph can have different sizes.

## 7 Limitations and conclusion

This paper proposes a feature representation method, TS-LDDMM, designed for shape comparison on homogeneous time series datasets. We show on a real dataset its ability to study, with high interpretability, the inter-individual shape variability. As an unsupervised approach, it is user-friendly and enables knowledge transfer for different supervised tasks such as classification.

Although TS-LDDMM is already competitive for classification, its performances can be leveraged on more heterogeneous datasets using a hierarchical clustering extension, which is relegated for future work.

TS-LDDMM employs kernel computations, which require specific libraries (e.g., KeOps ) to be efficient and scalable. However, in our experiments, the time complexity of TS-LDDMM is comparable to that of competitors. It is clear that TS-LDDMM needs to be extended to handle very large datasets with high-dimensional time series (such as videos).

Additionally, TS-LDDMM requires tuning several hyperparameters, though this is a common requirement among competitors [23; 43; 60; 26]. In future work, adaptive methods are expected to be developed to provide a more user-friendly interface.