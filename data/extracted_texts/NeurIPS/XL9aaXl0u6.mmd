# Sample Complexity of Interventional Causal Representation Learning

Emre Acarturk

Rensselaer Polytechnic Institute

&Burak Varici

Carnegie Mellon University

&Karthikeyan Shanmugam

Google DeepMind

Work was done while BV was a Ph.D. student at Rensselaer Polytechnic Institute.

&Ali Tajer

Rensselaer Polytechnic Institute

###### Abstract

Consider a data-generation process that transforms low-dimensional _latent_ causally-related variables to high-dimensional _observed_ variables. Causal representation learning (CRL) is the process of using the observed data to recover the latent causal variables and the causal structure among them. Despite the multitude of identifiability results under various interventional CRL settings, the existing guarantees apply exclusively to the _infinite-sample_ regime (i.e., infinite observed samples). This paper establishes the first sample-complexity analysis for the finite-sample regime, in which the interactions between the number of observed samples and probabilistic guarantees on recovering the latent variables and structure are established. This paper focuses on _general_ latent causal models, stochastic _soft_ interventions, and a linear transformation from the latent to the observation space. The identifiability results ensure graph recovery up to ancestors and latent variables recovery up to mixing with parent variables. Specifically, \((()^{4})\) samples suffice for latent graph recovery up to ancestors with probability \(1-\), and \((()^{4})\) samples suffice for latent causal variables recovery that is \(\) close to the identifiability class with probability \(1-\).

## 1 Introduction

The observed data generated in a wide range of technological, social, and biological domains has high-dimensional, complex, and often unexplainable structures. Nevertheless, sometimes, such complex structures can be explained by latent generating factors that often form lower-dimensional structures. The field of causal representation learning (CRL) is motivated by this premise and focuses on disentangling the _causally-_related latent generating factors that underlie a given high-dimensional observable dataset. In particular, given a high-dimensional dataset, the objective of CRL is to learn (i) the underlying latent causal variables and (ii) the causal relationships among the latent variables. The learned representations then provide an explainable structure for the observed data and facilitate informed reasoning for the downstream tasks .

Formally, CRL consists of a data generation and data transformation pipeline as follows. There exists a set of high-level latent variables \(Z^{n}\) that are causally related. The causal interactions are captured by a directed acyclic graph (DAG) \(\). The latent variables \(Z\) go through an unknown transformation \(g\) and generate the observed variables \(X^{d}\), i.e., \(X=g(Z)\). The objective of CRL is to use the observed variables \(X\) and learn the latent graph \(\) and latent variables \(Z\). The primary question facing CRL is _identifiability_, which refers to establishing the conditions under which recovering \(Z\) and \(\) are possible.

Identifiability is known to be impossible without proper inductive biases or additional supervision [2; 3]. One approach to address this issue is performing _interventions_ - leading to _interventional_ CRL - which has gained significant recent attention [4; 5; 6; 7; 8; 9; 10]. Specifically, interventions on latent causal variables create additional statistical diversity in the observed data. Accessing to enough interventional environments enables identifiability . Despite the recent advances in establishing identifiability guarantees for interventional CRL, all the existing guarantees focus on the asymptotic _infinite-sample_ regime, where one assumes access to an infinite number of observable samples \(X\).

In this paper, we establish conditions for **finite-sample non-asymptotic** identifiability and performance guarantees for interventional CRL. We focus on CRL under the _general_ (non-parametric) latent causal models, _soft_ interventions, and linear transformations from the latent to the observation space. This setting is well-studied in the infinite-sample regime [4; 5; 7; 11], where the existing identifiability results show that the causal graph can be recovered up to ancestral nodes, and latent variables can be learned up to mixing with their ancestors. In this paper, we provide the probabilistic finite-sample counterparts of these identifiability guarantees and establish the first sample complexity analysis for interventional CRL in the finite-sample regime.

Our CRL approach falls in the category of score-based CRL , based on which our sample complexity analysis consists of two steps. In the first step, we delineate a general sample complexity for any desired consistent score estimator. Subsequently, we specialize these general results by adopting the reproducing kernel Hilbert space (RKHS)-based score estimator . We establish the following identifiability guarantees for any desired pair of constants \(,_{+}\).

* **Latent graph recovery:** Using RHKS-based score estimator, \((()^{4})\) samples suffice to recover the transitive closure of the latent graph with probability at least \(1-\).
* **Latent variables recovery:** Using the same score estimator, \((()^{4})\) samples suffice to ensure that the mean squared error in the estimated latent causal variables is at most \(^{2}\) with probability at least \(1-\).
* **Dependence on model dimension:** We further establish the explicit and implicit dependence of sample complexity on the dimensions of the latent and observable spaces, \(n\) and \(d\), respectively. The precise characterizations of these expressions involve additional model-dependent constants specified in Section 5.
* **Improved guarantees:** Finally, we note that our latent variables recovery result, where we show recovery up to mixing with _parents_, which improves upon the results in existing literature that guarantee recovering up to mixing with _ancestors_.

**Methodology.** We offer novel finite-sample CRL algorithms and characterize the sample complexity guarantees achievable by these algorithms. We design our algorithms using the properties of _score functions_ (i.e., the gradient of the log density). This approach is inspired by the score-based CRL framework [5; 8]. However, the algorithms are significantly different, led by the need to integrate finite-sample score estimation routines and their imperfections. In our analysis, first, we provide sample complexity upper bounds for a score-based framework that uses any desired _consistent_ score difference estimator. Then, we adopt a specific score estimator  and provide explicit sample complexity guarantees.

**Related work.** All the existing studies on interventional CRL focus on the infinite-sample regime. We briefly review the studies that adopt a similar CRL model, i.e., _linear_ transformations and _single-node soft_ interventions. In this setting, complete identifiability results for linear latent models are established in . Similar results are shown for nonlinear latent causal models in [4; 11] and for linear non-Gaussian models in . In the most closely related setting to ours, identifiability up to ancestors is shown to be possible using soft interventions on general latent causal models . Other studies on interventional CRL include using \(do\) interventions for polynomial transformations [6; 14] and hard interventions for general transformations [10; 8]. On a partially related problem, error rates for using score functions on _observed_ causal discovery are provided in .

**Notations.** For \(n\), we define \([n]\{1,,n\}\). Vectors are represented by lower case bold letters, and element \(i\) of vector \(\) is denoted by \(_{i}\). Matrices are represented by upper case bold letters, and we denote row \(i\) and column \(j\) of matrix \(\) by \(_{i,:}\) and \(_{:,j}\), respectively. The row permutation matrix for permutation \(\) of \([n]\) is denoted by \(_{}\). Random variables and their realizations are presented by upper and lower case letters, respectively. We denote the Moore-Penrose pseudoinverseof a matrix \(\) by \(^{}\). Given a symmetric matrix \(^{d d}\), we denote the vector of eigenvalues of \(\) ordered in ascending order by \(()^{d}\) and the matrix of eigenvectors by \(()^{d d}\) such that \(=()(( ))()^{}\). For any matrix \(\), we denote the rank, column, and null spaces of \(\) by \(()\), \(()\), and \(()\), respectively.

## 2 Finite-sample data-generation process

The existing studies on analyzing the identifiability guarantees of CRL inevitably necessitate the availability of an infinite number of samples as otherwise, perfect identifiability is rendered impossible. In this paper, our objective is identifiability analysis assuming only **a finite number of samples** are available - a significant departure from the infinite-sample regime. In this section, we specify a general data-generating process consisting of a latent causal space and an unknown linear transformation that maps the latent variables to observed variables.

**Latent causal model.** We have a latent space of \(n\) causally related random variables. We denote the latent causal variables by \(Z[Z_{1},,Z_{n}]^{}\). The causal relationships among the variables of \(Z\) are represented by a directed acyclic graph \(\) with \(n\) nodes where the \(i\)-th node represents \(Z_{i}\). We denote the parents, children, and ancestors of a node \(i[n]\) in \(\) by \((i)\), \((i)\), and \((i)\), respectively. We denote the probability density function (pdf) of \(Z\) by \(p_{Z}\), which is assumed to be well-defined without any zeros over lower-dimensional manifolds and to have full support on \(^{n}\). Given the DAG \(\), \(p_{Z}\) factorizes according to

\[p_{Z}(z)=_{i[n]}p_{i}(z_{i} z_{(i)})\;.\] (1)

We assume that the conditional pdfs \(\{p_{i}:i[n]\}\) are continuously differentiable with respect to \(z\). We call a permutation \(=(_{1},,_{n})\) of \([n]\) a _valid causal order_ if for all \(i,j[n]\), \(i(j)\) implies \(_{i}<_{j}\). Without loss of generality, we assume that \((1,,n)\) is a valid causal order.

**Transformation model.** The latent variables \(Z\) are mapped to the observed variables \(X[X_{1},,X_{d}]^{}\) through an unknown linear transformation \(^{d n}\), where \(d n\). Specifically,

\[X= Z\;,\] (2)

where \(\) has full column rank. We denote the pdf of \(X\) by \(p_{X}\). Note that owing to the generation process of \(X\), the pdf \(p_{X}\) is supported on an \(n\)-dimensional subspace embedded in \(^{d}\). We denote the \(L^{2}\) norm of any function \(f\) with finite variance under \(p_{X}\) by \(\|f\|_{p_{X}}^{2}_{p_{X}}\|f(x)\|_{2}^{2}\).

**Intervention model.** We consider CRL under interventions, and in particular focus on _soft_ interventions, as the most general form of interventions. Hence, in addition to the observational model specified by (2), we consider \(n\) single-node interventional environments \(\{^{m}:m[n]\}\). We assume that the node intervened in the environment \(^{m}\) is _unknown_ and denote it by \(I^{m}\). Leaving one node unintervened renders identifiability impossible . Hence, we inevitably have \(\{I^{m}:m[n]\}=[n]\).

Applying a soft intervention on node \(i\) changes its causal mechanism specified by the observational conditional pdf \(p_{i}(z_{i} z_{(i)})\) to a distinct interventional conditional pdf \(q_{i}(z_{i} z_{(i)})\). The conditional pdfs \(\{q_{i}:i[n]\}\) are assumed to be continuously differentiable with respect to \(z\). Subsequently, the pdf of \(Z\) in the interventional environment \(^{m}\), denoted by \(p^{m}\), factorizes according to

\[p_{Z}^{m}(z)=q_{}(z_{} z_{()})_{i }p_{i}(z_{i} z_{(i)})\;,=I^{m}\;.\] (3)

To distinguish the observational and interventional data, we denote the latent and observed random variables in environment \(^{m}\) by \(Z^{m}\) and \(X^{m}\), respectively. Interventions do not alter the transformation matrix \(\), indicating that similarly to (2), we have \(X^{m}= Z^{m}\). Throughout the paper, we adopt the convention that \(^{0}\) refers to the observational environment.

**Score functions.** The _score function_ associated with a pdf is defined as the gradient of its logarithm. We denote the score functions associated with the observational distributions of \(Z\) and \(X\) by \(_{Z}\) and \(_{X}\), respectively, i.e.,

\[_{Z}(z)_{z} p_{Z}(z)\;, _{X}(x)_{x} p_{X}(x)\;.\] (4)Similarly, we denote the score functions associated with the distributions of \(Z\) and \(X\) in environment \(^{m}\) for \(m[n]\) by \(_{Z}^{m}\) and \(_{X}^{m}\), respectively, i.e.,

\[_{Z}^{m}(z)_{z} p_{Z}^{m}(z)\;, _{X}^{m}(x)_{x} p_{X}^{m}(x)\;.\] (5)

In our algorithm and analysis, we will analyze the score variations across different interventions. For this purpose, we define the score difference functions of \(Z\) and \(X\) between environments \(^{m}\) and \(^{0}\) as

\[_{Z}^{m}(z)_{Z}^{m}(z)-_{Z}(z)\;, _{X}^{m}(x)_{X}^{m}(x)-_{X}(x)\;, m [n]\;.\] (6)

To ensure that interventions affect the target node and its parents distinctly, we adopt the following assumption. This assumption holds for a large class of models, e.g., additive noise models under stochastic hard interventions (5, Lemma 2).

**Assumption 1** ([5, Assumption 1]).: _For any \(m[n]\), and for all \(k(I^{m})\), the term \([_{Z}^{m}]_{k}/[_{Z}^{m}]_{I^{m}}\) is not a constant function of \(z\)._

**Finite sample data.** We assume that we have only \(N\) data samples under each of the environments. We denote the \(N\) samples of \(Z^{m}\) and \(X^{m}\) by

\[\{z_{j}^{m}:j[N]\}\;,\{x_{j}^{m}:j[N]\}\;,  m\{0\}[n]\;.\] (7)

Accordingly, for each sample \(j\), we concatenate all latent and observational samples under different environments to create sample matrices

\[_{j}[z_{j}^{0},,z_{j}^{n}]\;,_ {j}[x_{j}^{0},,x_{j}^{n}]\;, j[N]\;.\] (8)

Finally, we define the set of samples \(_{N}\{_{j}:j[N]\}\) and \(_{N}\{_{j}:j[N]\}\).

## 3 Finite-sample identifiability objectives

The CRL framework specified in Section 2 has two objectives: use the \(N\) observational samples \(_{N}=\{_{j}:j[N]\}\) to recover the causal graph \(\) and the latent causal variables \(_{N}=\{_{j}:j[N]\}\). In this section, we formalize the inference rules and their associated fidelity metrics for recovering \(\) and \(_{N}\).

**Latent graph recovery.** Define \(\) as the set of all DAGs on \(n\) nodes. We define \(}\) as a generic estimator of the latent graph \(\), i.e.,

\[}^{d(n+1)}^{N}\;.\] (9)

To quantify the accuracy of the estimate \(}(_{N})\), we provide the following probably approximately correct (PAC) guarantee, which is the probabilistic counterpart of the standard _identifiability up to ancestor_ definition in the interventional CRL literature .

**Definition 1** (\(\)-Pac graph recovery).: _Graph estimate \(}(_{N})\) achieves \(\)-PAC latent graph recovery if, with probability at least \(1-\), the transitive closures of \(}(_{N})\) and \(\) are isomorphic._

**Latent variables recovery.** We investigate a two-step estimator for the latent variables \(_{N}\). First, we define a generic estimator for the pseudoinverse of \(\) as

\[^{d(n+1)}^{N}^{n  d}\;.\] (10)

Then, given an estimate \((_{N})\) for \(^{}\), we estimate \(\{_{j}:j[N]\}\) according to

\[}_{j}=(_{N})_{j}\;, j [N]\;.\] (11)

We provide the following definition to quantify the fidelity of these estimates with respect to the ground truth variables.

**Definition 2** (\((,)\)-PAC variables recovery).: _The estimate \((_{N})\) achieves \((,)\)-PAC latent variables recovery if the estimated causal variables \(\{}_{j}:j[N]\}\) satisfy_

\[}_{j}=_{I}(_{}+_{ })_{j}\;,\] (12)

_where for all \(i}(j)\), \(_{}\) satisfies \((_{})_{i,j}=0\), and, with probability at least \(1-\), we have \(\|_{}\|_{2}\)._We note that Definition 2 is the probabilistic counterpart of the standard _identifiability up to ancestor_ definition in interventional CRL literature.

**Noisy score model.** The score-based CRL framework uses properties of score function differences to build the estimators for the latent graph and variables. Therefore, when we only have access to finite data, we need to estimate the score functions. We denote a generic score function estimator for environment \(^{m}\) by

\[}_{X}^{m}(x;_{N})^{d} ^{d(n+1)}^{N}^{d}\;.\] (13)

When the dependence is clear from the context, we will drop the explicit dependence of \(}_{X}^{m}\) on \(_{N}\). Similarly to (6), we define the estimated score difference functions

\[}_{X}^{m}(x;_{N})}_{X}^{m}(x; _{N})-}_{X}(x;_{N})\;, m[n]\;.\] (14)

In this paper, we focus on _consistent_ score difference estimators \(\{}_{X}^{m}\,:\,m[n]\}\), i.e., they satisfy convergence in probability. Specifically, for any \(,>0\), there exists \(N(,)\) such that

\[_{m[n]}}_{X}^{m}(;_{N})-_{X}^{m}_{p_{X}} 1-\;,  N N(,)\;.\] (15)

Notably, we note that many score estimators are known to be consistent [12; 16; 17; 18].

## 4 Finite-sample CRL algorithms

In this section, we design finite-sample interventional CRL algorithms through which we establish finite-sample identifiability guarantees as well as bounds on the associated sample complexities. Our algorithms fall within the score-based category of CRL algorithms . The main intuition of this framework is that score functions in the observed space contain all the information needed for recovering the latent graph \(\) and the inverse transform \(^{}\). Specifically, two metrics are pivotal for retrieving the latent space:

1. **Score differences:** As shown in Lemma 1, the nonzero entries of the latent score differences \(_{Z}^{m}\) encode the graph structure and the observed score differences \(_{X}^{m}\) are generated using the inverse transform \(^{}\).
2. **Score difference correlations:** As shown in Lemma 2, the column space of the correlation matrix of the score differences contains crucial information about the latent graph and the inverse transform, which we will leverage to form estimates for the latent graph \(\) and the inverse transform \(^{}\).

To proceed, for \(m[n]\) we denote the correlation matrices of \(_{X}^{m}\) and \(}_{X}^{m}\) by

\[_{X}^{m}_{p_{X}}_{X}^{m}(x)( _{X}^{m}(x))^{}\;,}_{X}^{m }_{p_{X}}}_{X}^{m}(x)(}_ {X}^{m}(x))^{}\;.\] (16)

Next, we formalize two critical properties of score differences and their correlation matrix. Specifically, the following lemma states that the sparsity pattern of the latent score differences \(_{Z}^{m}\) exposes the structure of the latent graph, and the observed score differences \(_{X}^{m}\) preserve this information through the pseudoinverse of \(\).

**Lemma 1** ().: _Score function differences in the latent space satisfy, for all \(m[n]\),_

\[ z^{n}:[_{Z}^{m}(z)]_{i}=0\;\;i }(I^{m})\;.\] (17)

_Furthermore, the score differences in the observed domain are given by_

\[_{X}^{m}(x)=(^{})^{}_{Z}^{m}(z)\;, x = z\;.\] (18)

The next lemma specifies that the structure of the column space of correlation matrix \(_{X}^{m}\) is heavily constrained by the graph structure and the inverse transformation \(^{}\).

**Lemma 2**.: _For any \(m[n]\), we have \((_{X}^{m})\,(^{ })^{}\,_{:,i}\,:\,i}(I^{m})}\)._

While these two properties enable recovering the latent graph and variables, the ground truth score functions \(_{Z}^{m}(z)\) and correlation matrix \(_{X}^{m}\) are unknown. We only have access to their noisy counterparts, as estimated from the observed data. In our algorithms, we use methods with soft decision rules that can counter the effects of the errors introduced by the score estimation procedure. These are facilitated by the following three key definitions.

**Definition 3** (Approximate column space).: _We define the \(\)-approximate column space of a positive semidefinite matrix \(^{d d}\), denoted by \((;)\), as the span of the eigenvectors of \(\) associated with the eigenvalues that are strictly greater than \(\)._

**Definition 4** (Approximate null space).: _We define the \(\)-approximate null space of \(\), denoted by \((;)\), as the orthogonal complement of \((;)\)._

**Definition 5** (Approximate subspace orthogonality).: _We say two subspaces \(\) and \(\) with orthonormal bases \(\) and \(\) are \(\)-approximately orthogonal, denoted by \(_{}\), if \(\|^{}\|_{2}\)._

In these definitions, setting \(==0\) yields the standard definitions of column and null spaces and orthogonality. Next, we describe our algorithms for latent graph recovery (Algorithms 1 and 2) and latent variables recovery (Algorithm 3). We note that the algorithms for latent graph and latent variable recovery algorithms are fully decoupled, enabling the independent recovery of the graph and the latent variables.

```
1:Input:\(}^{m}\) for all \(m[n]\), \( 0\)
2:\(_{n}\{1,,n\}\)\(\) remaining unordered set
3:for\(t(n,,2)\)do
4:for\(k_{t}\)do
5:if\((_{m_{t}\{k\}} }^{m};)=t-1\)then
6:\(_{t} k\)\(\)\(I^{k}\) has no ancestors in \(I^{_{t}}\)
7:\(_{t-1}_{t}\{k\}\)\(\) remove the identified node from unordered set
8:break\(k\) loop
9:\(_{1} m\) for \(m_{1}\)
10:Return\(\) ```

**Algorithm 1** Causal order estimation

Algorithm 2 - Latent graph estimation. In Algorithm 2, we use the causal order found in Algorithm 1 and correlation matrices \(\{}^{m}_{X}\,:\,m[n]\}\) to form a graph estimate \(}\). We build this graph iteratively by considering a candidate edge \(_{t}_{j}\) for all possible \((t,j)\) pairs starting from a leaf node \(t\). The key property that we leverage is that we can form two subsets \(_{t}\) and \(_{t,j}\) of \([n]\) using \(\)\(t\), and \(j\) such that, for sufficiently small \(\), with high probability the following approximation holds.

\[_{m_{t}}}_{X}^{m} \;;\;_{}_{m_{t,j}} }_{X}^{m}\;;\;\ \ \ I^{_{t}} I^{_{j}}\;.\] (20)

We check each edge \(_{t}_{j}\) and add the detected edges to the estimated graph \(}\). In Theorem 1, we show that this procedure guarantees a PAC latent graph recovery.

**Algorithm 3 - Inverse transform estimation.** In Algorithm 3, we build our inverse transform estimate \(\) one row at a time. The key property we use is that for any \(m[n]\) and unit vector \((}_{X}^{m})\), the following error term is small with high probability

\[\,^{} X-_{i}(I^{m})}^{}_{i} Z_{i}\, _{2}\;.\] (21)

In other words \(^{} X\) is approximately equal to \(Z_{I^{m}}\) up to mixing with \(\{Z_{i}\,:\,i(I^{m})\}\), which conforms to our latent variables recovery objective. Note that in the noise-free setting, an exact equality holds due to Lemma 2 and property \(^{}=_{n}\). Based on this observation, in our algorithm, we construct our inverse transform estimate by setting row \(m\) of \(\) to a unit vector \((}_{X}^{m})\) for all \(m[n]\). In Theorem 2, we show that Algorithm 3 achieves a PAC latent variables recovery.

## 5 Sample complexity analysis

In this section, we analyze the sample complexity of our CRL algorithm to establish an achievable sample complexity for CRL.

**Threshold selection.** A critical step in our algorithm designs is choosing thresholds \(\) and \(\) specified in Definitions 3-5. These thresholds determine the approximate ranks of noisy matrices and approximate orthogonality between subspaces, respectively. Specifically, we select \(\) such that the approximate rank of \(}_{X}^{m}\) tracks that of \(_{X}^{m}\) with high probability. To proceed, we denote the minimum nonzero eigenvalue among arbitrary \(_{X}^{m}\) sums by

\[^{*}_{[n]}\,_ {i}_{m}_{X}^{m}\,:\,i[d]\,,\, _{i}_{m}_{X}^{m} 0 \,}\;,\] (22)

where \(()^{d}\) denotes the vector of eigenvalues of \(\). In our algorithms, we let \((0,^{*})\) and show that this choice ensures that eigenvalues of the null and column spaces can be separated via \(\). For the approximate orthogonality test in Definition 5, we show that \(^{*}\) defined below serves as a bound on how close the non-orthogonal column spaces of \(_{X}^{m}\) sums become orthogonal.

\[^{*}_{i[n]}^{}{}_{i,.} _{2}_{i,.}_{2}\;.\] (23)

Similarly, we let \((0,^{*})\) in our algorithms. For any choice of \(\) and \(\), we define \(_{*}\{,^{*}-\}\) and \(_{*}\{,^{*}-\}\). We note that we do not have to know \(^{*}\) and \(^{*}\) a priori, and can include routines to estimate them in practice. These estimates do not have to be highly accurate since even rough estimates suffice to choose reliable thresholds \(\) and \(\). Specifically, based on estimates for \(^{*}\) and \(^{*}\), we can choose "safe" thresholds for \(\) and \(\), e.g., one-fourth of the estimates for \(^{*}\) and \(^{*}\), so that (i) with high probability we satisfy the requirement \((0,^{*})\) and \((0,^{*})\), and (ii) we can avoid collecting excessive samples and compromising the sample complexity bounds. We provide the details of constructing such estimates of \(^{*}\) and \(^{*}\) in Appendix G.

**Sample complexity results.** We provide two sets of sample complexity analyses for the latent graph and the latent variables. We estimate the latent graph in two steps: (i) causal order estimation (Algorithm 1) and (ii) latent graph estimation (Algorithm 2). Algorithm 1 only employs approximate rank tests. Therefore, we first show that the approximate rank of the sum of correlation matrices \(\{}_{}^{m}:m[n]\}\) tracks that of \(\{_{}^{m}:m[n]\}\) with high probability. To proceed, we define the following constants that appear repeatedly in our analysis:

\[4_{m[n]}_{X}^{m}_{p_{X}} ^{-1}_{} 2_{m[n]} _{X}^{m}_{p_{X}}\.\] (24)

**Lemma 3**.: _Let \((0,^{*})\). For any \(>0\), \(N_{}()\) samples suffice to ensure that with probability at least \(1-\),_

\[[n]:\,\,_ {m}}_{X}^{m};= _{m}_{X}^{m}\,\] (25)

_where_

\[N_{}() N}{ n},\,_{}},\.\] (26)

Note that the function \(N(,)\) is an inherent property of the score difference estimator specified in (15), and it is monotonically decreasing in its second argument. Using this lemma, we can show that Algorithm 1 returns a permutation \(\) such that \(I\) is a valid causal order with high probability.

**Lemma 4**.: _Let \((0,^{*})\). Under Assumption 1, for any \(>0\), \(N_{}()\) samples suffice to ensure that with probability at least \(1-\), \(I\) is a valid causal order, where \(\) is the output of Algorithm 1._

Next, we show that Algorithm 2 achieves latent graph recovery with high probability.

**Theorem 1** (Sample complexity - Graph).: _Let \((0,^{*})\) and \((0,^{*})\). Under Assumption 1, for any \(>0\), \(N_{}()\) samples suffice to ensure that the output \(}(_{N})\) collectively generated by Algorithms 1 and 2 satisfies \(\)-PAC graph recovery, where_

\[N_{}() N(\{}{n} \,,\,_{*}}{2n}\,,_{}\},\,)\.\] (27)

We note that constants \(\) and \(_{}\) are sample-independent. Then, (27) specifies the sample complexity as a function of the hyperparameters \(,\), target reliability \((0,1)\), as well as the latent dimension \(n\). Next, we state the complementary result for recovering the latent variables.

**Theorem 2** (Sample complexity - Variables).: _Let \((0,^{*})\). For any \(>0\) and \(>0\), \(N_{Z}(,)\) samples suffice to ensure that the output \((_{N})\) of Algorithm 3 satisfies \((,)\)-PAC causal variables recovery, where_

\[N_{Z}(,) N(}{\|\|_{2}},\,_{*},\,_{}},\, )\.\] (28)

Theorems 1 and 2 collectively specify an extent of identifiability achievable in the finite-sample regime. Importantly, we note that since we can establish \((,)\)-PAC identifiability guarantees for _any_ vanishing \(,>0\) using finite samples, Algorithms 2 and 3 are _consistent_ estimators of the latent graph and the inverse transformation up to the corresponding equivalence classes.

Finally, we note that the latent variables are recovered up to mixing with _parents_, as specified in Theorem 2. This result is a refinement of the existing latent variable recovery results under soft interventions in existing literature , which recover the latent variables up to mixing with _ancestors_.

**RKHS-based score estimator.** So far, we have characterized the sample complexity for any consistent score difference estimator. Next, we specialize the results by adopting the RKHS-based score estimator of . We adopt this particular choice since it has known non-asymptotic sample complexity properties. To our knowledge, it is the only score estimator equipped with such a guarantee. To use the sample complexity of this score estimator in our paper, we first show that it is consistent in the sense of (15). For the formal statement of the following property and its attendant assumptions, we refer to Appendix E.

**Lemma 5** (Informal).: _Assume that \(_{}\) and \(_{}^{m}\) satisfy the conditions of the RKHS-based score estimator in . Then, for any given \(\) and \(\), the convergence specified in (15) holds when_

\[N(,)\,,\,\,2 ^{2}\,}^{4}( )^{4}\,\] (29)_where \(\) and \(C\) are sample independent constants that depends only on \(p_{X}\), \(p_{X}^{m}\) for \(m[n]\) and the structure of the RKHS._

We use Lemma 5 to customize the general sample complexity bounds in Theorems 1 and 2 to the RKHS-based score estimator. For this purpose, we first provide our sample complexity result for recovering the latent graph.

**Theorem 3** (RKHS-based sample complexity - Graph).: _Let \((0,^{*})\) and \((0,^{*})\). Under Assumption 1 and the conditions of Lemma 5, \(N_{}()\) samples suffice to ensure that \(}(_{N})\) collectively generated by Algorithms 1 and 2 satisfies \(\)-PAC graph recovery, where_

\[N_{}()=(\{}}\;, \;2^{2}\;\})^{4}( )^{4}\;,_{}\{ }{n},_{*}}{2n},_{}\}\;.\] (30)

**Remark 1** (RKHS-based error bound - Graph).: _Theorem 3 implies that using \(N\) samples, the output \(}(_{N})\) of Algorithms 1 and 2 satisfies \(_{}(N)\)-PAC graph recovery, where_

\[_{}(N) 8n(-N^{1/4}\{ }}\;,\;2^{2}\;\})\;.\] (31)

Similarly, we leverage Lemma 5 to specialize the general sample complexity in Theorem 2 to the RKHS-based score estimator.

**Theorem 4** (RKHS-based sample complexity - Variables).: _Let \((0,^{*})\). Under the conditions of Lemma 5, \(N_{Z}(,)\) samples suffice to ensure that the output \((_{N})\) of Algorithm 3 satisfies \((,)\)-PAC causal variables recovery, where_

\[N_{Z}(,)=(\{}\;,\;2^{2}\;\})^{4}(())^{4}\;,\] (32)

_and_

\[_{Z}\{}{\| \|_{2}},\,_{*},\,_{}\}\;.\] (33)

We note that the results in Theorems 3 and 4 show a more explicit dependence of the sample complexity on the latent space dimension \(n\). We observe that the sample complexity of latent graph recovery explicitly depends on \(\) and the latent dimension \(n\) according to \(((n)^{4})\). Similarly, the sample complexity of latent variables recovery explicitly depends on \(\), \(\), and the latent dimension \(n\) according to \(((}{})^{4})\). We note that constants \(\), \(_{}\), \(^{*}\), and \(^{*}\) are model parameters, and their scaling behavior in terms of \(n\) and \(d\) are investigated numerically in the next section.

## 6 Experiments

In this section, we perform numerical assessments of our analyses to provide complementary insight into the sample complexity results of Section 5. Specifically, we evaluate the variations of the model constants with respect to problem dimensions \(n\) and \(d\). Furthermore, we also evaluate performance variations of the finite-sample algorithm in terms of the variations in the score estimation error. For this purpose, we focus on the mean squared error (MSE) of the score estimator, specified by \(\|}_{X}^{m}-_{X}^{m}\|_{p_{X}}^ {2}\). These evaluations facilitate a better understanding of the properties of the CRL problem that are not explicit in our theoretical queries.2

**Experimental details.** We consider problem dimensions \(n\{3,5,10\}\) and \(d\{n,15\}\) and generate \(\) using Erdos-Renyi model with density \(0.5\) on \(n\) nodes. We adopt linear Gaussian models as the latent causal model. We consider \(N\{10^{2.5},10^{3},10^{3.5},10^{4},10^{4.5},10^{5}\}\) samples, and generate 100 latent models for each triplet \((N,n,d)\). We generate \(N\) samples of \(Z\) from each environment for each latent model. Transformation \(^{d n}\) is randomly sampled under full-rank and bounded condition number constraints, and the observed variables are generated as \(X= Z\). Due to the Gaussian structure of \(Z\), the observed variables \(X\) also have multivariate Gaussian distributions with score function \(_{X}(x)=- x\), where \(\) is the precision matrix of \(X\). Therefore, we estimate \(_{X}\) using the sample precision matrix estimated from \(N\) samples, \(_{N}\), as \(}_{X}(x;_{N})=-_{N} x\).

**Results.** The sample complexity results depend on dimension \(n\) and \(d\) through their explicit presence as well as their implicit effect on model-dependent constants \(\), \(_{}\), \(^{*}\), and \(^{*}\), as established in Theorems 1 and 2. First, we observe that all these parameters are independent of \(d\). This is also expected theoretically since we can project down the \(d\) dimensional observed space to the \(n\) dimensional \(()\). In Figure 0(a), we illustrate the dependence of these constants on data dimensions \(n\) and \(d\). We observe that \(^{*}\) remains constant, while \(\) and \(_{}\) are decreasing as \(n\) increases, but the decay is not steep. On the other hand, we observe that \(^{*}\) decays exponentially with \(n\). \(^{*}\) is implicitly controlled by the condition number, which is kept bounded. We recall that \(_{}\) is the minimum among \(\{\|_{X}^{m}\|_{p_{X}}\,:\,m[n]\}\), and similarly, \(\) is the inverse of the maximum of \(\{\|_{X}^{m}\|_{p_{X}}\,:\,m[n]\}\), so they are expected to become smaller as \(n\) increases. Finally, for \(^{*}\), we note that the definition in (22) is an order statistic among exponentially many, \(2^{n}\), subsets.

Next, we note that all the imperfections in the graph and variable recovery are due to the imperfections in score estimates. In fact, this is the bottleneck in our decisions: the key impact of finite samples versus infinite samples is the degradation in the quality of score estimates. To have a direct insight into how the score estimation noise power translates into decision imperfection, we assess the quality of graph recovery success rate versus varying degrees of mean score error of score difference estimates \([\|}_{X}^{m}-_{X}^{m}\|_{p_{X}}^{2}]\) for different triplets \((N,n,d)\). In Figure 0(b), each data point corresponds to a different sample size \(N\), and the legend entries \(\{3,5,10\}\) denote the latent dimension \(n\). This figure illustrates the scatter plot of the error probability \(\) versus MSE for different parameter combinations and the corresponding local regression curves. It is observed that in the low MSE regime, \(1-\) decays linearly with respect to \([\|}_{X}^{m}-_{X}^{m}\|_{p_{X}}^{2}]\) and it plateaus as the MSE increases. This trend is due to our algorithms' high sensitivity to errors in estimating approximate matrix ranks.

## 7 Conclusion

In this paper, we have established the first sample complexity results for interventional causal representation learning. Specifically, we have characterized upper bounds on the sample complexity of latent graph and variables recovery in terms of the finite sample performance of any consistent score difference estimator. We have, furthermore, adopted a particular score estimator  to derive explicit sample complexity statements.

Our sample complexity results are given for partial identifiability (up to ancestors) in the soft intervention and linear transformation setting. Establishing sample complexity results for perfect identifiability via hard interventions or considering general transformations are important future directions.