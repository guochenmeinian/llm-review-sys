# Pessimistic Nonlinear Least-Squares Value Iteration

for Offline Reinforcement Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Offline reinforcement learning, where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, the theoretical understanding of offline RL with non-linear function approximation is still limited. Specifically, most existing works on offline RL with non-linear function approximation either have a poor dependency on the function class complexity or require an inefficient planning phase. In this paper, we propose an oracle-efficient algorithm PNLSVI for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal problem-dependent regret when specialized to linear function approximation. Our theoretical analysis introduces a new coverage assumption for nonlinear Q function, bridging the minimum-eigenvalue assumption and the uncertainty measure widely used in online nonlinear RL. To the best of our knowledge, this is the first statistically optimal algorithm for nonlinear offline RL.

## 1 Introduction

Offline reinforcement learning (RL), also known as batch RL, is a learning paradigm where an agent learns to make decisions based on a set of pre-collected data, instead of interacting with the environment in real-time like online RL. The goal of offline RL is to learn a policy that performs well in a given task, based on historical data that was collected from an unknown environment. Recent years have witnessed significant progress in developing offline RL algorithms that can leverage large amounts of data to learn effective policies. These algorithms often incorporate powerful function approximation techniques, such as deep neural networks, to generalize across large state-action spaces. They have achieved excellent performances in a wide range of domains, including the games of Go and chess (Silver et al., 2017; Schrittwieser et al., 2020), robotics (Gu et al., 2017; Levine et al., 2018), and control systems (Degrave et al., 2022).

Several studies have studied the theoretical guarantees of tabular offline RL and proved near-optimal sample complexities in this setting (Xie et al., 2021; Shi et al., 2022; Li et al., 2022). However, these algorithms cannot handle numerous real-world applications with large state spaces. Consequently, a significant body of research has shifted its focus to offline RL with function approximation. For example, several works have analyzed the sample efficiency of offline RL with linear function approximation under different MDP models, including linear MDPs and their variants (Jin et al., 2021; Zanette et al., 2021; Min et al., 2021; Yin et al., 2022a). To handle nonlinear function class, a recent line of research considered offline RL with general function approximation (Chen and Jiang,2019; Xie et al., 2021; Zhan et al., 2022). While these algorithms have sample efficiency guarantees, they often require an inefficient planning phase or have a poor dependency on the function class complexity. For example, Xie et al. (2021) proposed an information-theoretic algorithm that requires solving an optimization problem over all potential policy and corresponding version space, which includes all functions with lower Bellman error. To overcome this limitation, Xie et al. (2021) proposed a practical implementation, as a cost, the algorithm have a poor dependency on the function class complexity. Recently, (Yin et al., 2022) studied the general differentiable function class and propose a computation efficient algorithm (PFQL). However, their result also have an addition dependence on the dimension \(d\) of the parameter.

Therefore, a natural question arises:

_Can we design a computationally efficient algorithm that achieves the minimax optimality with respect to the complexity of nonlinear function class?_

We give an affirmative answer to the above question in this work. Our contributions are listed as follows:

* We propose a pessimism-based algorithm PNLSVI designed for nonlinear function approximation, which strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation (Xiong et al., 2022; Yin et al., 2022). Our algorithm is oracle-efficient, i.e., it is computationally efficient when there exists an efficient regression oracle and bonus oracle for the function class (e.g., generalized linear function class).
* We prove a data-dependent regret bound with the widely used \(D^{2}\)-divergence in online nonlinear RL regime, which is optimal with respect to the function class complexity. Our analysis closes the gap to optimality for differentiable function approximation, which was previously an open problem (Yin et al., 2022).
* We introduce a novel uniform coverage assumption for general function approximation that is generalized over the assumption in Yin et al. (2022). Our assumption bridges between the minimum-eigenvalue assumption used in linear models and the generalized dimension for nonlinear function class, offering new insights into the function approximation problem in RL.

**Notation:** In this work, we use lowercase letters to denote scalars and use lower and uppercase boldface letters to denote vectors and matrices respectively. For a vector \(^{d}\) and matrix \(^{d d}\), we denote by \(\|\|_{2}\) the Euclidean norm and \(\|\|_{}=^{}}\). For two sequences \(\{a_{n}\}\) and \(\{b_{n}\}\), we write \(a_{n}=O(b_{n})\) if there exists an absolute constant \(C\) such that \(a_{n} Cb_{n}\), and we write \(a_{n}=(b_{n})\) if there exists an absolute constant \(C\) such that \(a_{n} Cb_{n}\). We use \(()\) and \(()\) to further hide the logarithmic factors. For any \(a b\), \(x\), let \([x]_{[a,b]}\) denote the truncate function \(a(x a)+x(a x b)+b(b  x)\), where \(()\) is the indicator function. For a positive integer \(n\), we use \([n]=\{1,2,..,n\}\) to denote the set of integers from \(1\) to \(n\).

## 2 Related Work

**RL with function approximation.** As one of the simplest function approximation classes, linear representation in RL has been extensively studied in recent years (Jiang et al., 2017; Dann et al., 2018; Yang and Wang, 2019; Jin et al., 2020; Wang et al., 2020; Du et al., 2019; Sun et al., 2019; Zanette et al., 2020, 2020; Weisz et al., 2021; Yang and Wang, 2020; Modi et al., 2020; Ayoub et al., 2020; Zhou et al., 2021; He et al., 2021). Several assumptions on the linear structure of the underlying MDPs have been made in these works, ranging from the _linear MDP_ assumption (Yang and Wang, 2019; Jin et al., 2020; Hu et al., 2022; He et al., 2022; Agarwal et al., 2022) to the _low Bellman-rank_ assumption (Jiang et al., 2017) and the _low inherent Bellman error_ assumption (Zanette et al., 2020). Extending the previous theoretical guarantees to more general problem classes, RL with nonlinear function classes has garnered increased attention in recent years (Wang et al., 2020; Jin et al., 2021; Foster et al., 2021; Du et al., 2021; Agarwal and Zhang, 2022; Agarwal et al., 2022). Various complexity measures of function classes have been studied including Bellman rank (Jiang et al., 2017), Bellman-Eluder dimension (Jin et al., 2021), Decision-Estimation Coefficient (Foster et al., 2021) and generalized Eluder dimension (Agarwal et al., 2022). Among these works, the setting in our paper is most related to Agarwal et al. (2022) where \(D^{2}\)-divergence (Gentile et al., 2022) was introduced in RL to indicate the uncertainty of a sample with respect to a particular sample batch.

Offline tabular RL.There is a line of works integrating the principle of pessimism to develop statistically efficient algorithms for offline tabular RL setting (Rashidinejad et al., 2021; Yin and Wang, 2021; Xie et al., 2021; Shi et al., 2022; Li et al., 2022). More specifically, Xie et al. (2021b) utilized the variance of transition noise and proposed a nearly optimal algorithm based on pessimism and Bernstein-type bonus. Subsequently, Li et al. (2022) proposed a model-based approach that achieves minimax-optimal sample complexity without burn-in cost for tabular MDPs. Shi et al. (2022) also contributed by proposing the first nearly minimax-optimal model-free offline RL algorithm.

Offline RL with linear function approximation.Jin et al. (2021b) presented the initial theoretical results on offline linear MDPs. They introduced a pessimism-principelied algorithmic framework for offline RL and proposed an algorithm based on LSVI (Jin et al., 2020). Min et al. (2021) subsequently considered offline policy evaluation (OPE) in linear MDPs, assuming independence between data samples across time steps to obtain tighter confidence sets and proposed an algorithm with optimal \(d\) dependence. Yin et al. (2022a) took one step further and considered the policy optimization in linear MDPs, which implicitly requires the same independence assumption. Zanette et al. (2021) proposed an actor-critic-based algorithm that establishes pessimism principle by directly perturbing the parameter vectors in a linear function approximation framework. Recently, Xiong et al. (2022) proposed a novel uncertainty decomposition technique via a reference function, which leads to a minimax-optimal sample complexity bound for offline linear MDPs without additional assumptions.

Offline RL with general function approximation.Chen and Jiang (2019) critically examined the assumptions underlying value-function approximation methods and established an information-theoretic lower bound. Xie et al. (2021a) introduced the concept of Bellman-consistent pessimism, which enables sample-efficient guarantees by relying solely on the Bellman-completeness assumption. Uehara and Sun (2021) focused on model-based offline RL with function approximation under partial coverage, demonstrating that realizability in the function class and partial coverage are sufficient for policy learning. Zhan et al. (2022) proposed an algorithm that achieves polynomial sample complexity under the realizability and single-policy concentrationly assumptions. Nguyen-Tang and Arora (2023) proposed a method of random perturbations and pessimism for neural function approximation. For differentiable function classes, Yin et al. (2022b) made advancements by improving the sample complexity with respect to the stage \(H\). However, their result had an additional dependence on the dimension \(d\) of the parameter space, whereas in linear function approximation, the dependence is typically on \(\).

## 3 Preliminaries

In our work, we consider the inhomogeneous episodic Markov Decision Processes (MDP), which can be denoted by a tuple of \((,,H,\{r_{h}\}_{h=1}^{H},\{_{h}\}_ {h=1}^{H})\). In specific, \(\) is the state space, \(\) is the finite action space, \(H\) is the length of each episode. For each stage \(h[H],r_{h}:\) is the reward function1 and \(_{h}(s^{}|s,a)\) is the transition probability function, which denotes the probability for state \(s\) to transfer to next state \(s^{}\) with current action \(a\). A policy \(:=\{_{h}\}_{h=1}^{H}\) is a collection of mappings \(_{h}\) from a state \(s\) to the simplex of action space \(\). For simplicity, we denote the state-action pair as \(z:=(s,a)\). For any policy \(\) and stage \(h[H]\), we define the value function \(V_{h}^{}(s)\) and the action-value function \(Q_{h}^{}(s,a)\) as the expected cumulative rewards starting at stage \(h\), which can be denoted as follows:

\[Q_{h}^{}(s,a)=r_{h}(s,a)+_{h^{}=h+1}^{H}r_{h^{ }}s_{h^{}},_{h^{}}(s_{h^{}})s_ {h}=s,a_{h}=a,\;V_{h}^{}(s)=Q_{h}^{}s,_{h}(s),\]

where \(s_{h^{}+1}_{h}(|s_{h^{}},a_{h^{}})\) denotes the observed state at stage \(h^{}+1\). By this definition, the value function \(V_{h}^{}(s)\) and action-value function \(Q_{h}^{}(s,a)\) are bounded in \([0,H]\). In addition, we define the optimal value function \(V_{h}^{}\) and the optimal action-value function \(Q_{h}^{}\) as \(V_{h}^{}(s)=_{}V_{h}^{}(s)\) and \(Q_{h}^{}(s,a)=_{}Q_{h}^{}(s,a)\). We denote the corresponding optimal policy by \(^{}\). For any function \(V:\), we denote \([_{h}V](s,a)=_{s^{}_{h}(|s,a)}V(s ^{})\) and \([_{h}V](s,a)=[_{h}V^{2}](s,a)-[_{h}V](s,a)^{2}\) for simplicity. For any function \(f:\), we define the Bellman operator \(_{h}\) as \(_{h}f(s_{h},a_{h})=_{s_{h+1}_{h}(|s_{h},a_{h})}[r_{h}(s_{h},a_{h})+f(s_{h+1})]\), where we use the shorthand \(f(s)=_{a}f(s,a)\) for simplicity. Based on this definition, for every stage \(h[H]\) and policy \(\), we have the following Bellman equation for value functions \(Q_{h}^{}(s,a)\) and \(V_{h}^{}(s)\), as well as the Bellman optimality equation for optimal value functions:

\[Q_{h}^{}(s_{h},a_{h})=_{h}V_{h+1}^{}(s_{h},a_{h}),\;Q_{h}^{*}(s_ {h},a_{h})=_{h}V_{h+1}^{*}(s_{h},a_{h}),\]

where \(V_{H+1}^{}(s)=V_{H+1}^{*}(s)=0\). We also define the Bellman operator for second moment as \(_{2,h}f(s_{h},a_{h})=_{s_{h+1}_{h}(| s_{h},a_{h})}[(r_{h}(s_{h},a_{h})+f(s_{h+1}))^{2}]\). For simplicity, we omit the subscripts \(h\) in the Bellman operator without causing confusion.

Offline Reinforcement Learning:In offline RL, the agent only have access to a batch-dataset \(D=\{s_{h}^{k},a_{h}^{k},r_{h}^{k}:h[H],k[K]\}\), which is collected by a behavior policy \(\), and the agent cannot interact with the environment. Given the batch dataset, the goal of offline RL is finding a near-optimal policy \(\) that minimize the sub-optimality \(V_{1}^{*}(s)-V_{1}^{}(s)\). In addition, for each stage \(h\) and behavior policy \(\), we denote the induced distribution of the state-action pair as \(d_{h}^{}\).

General Function Approximation:In this work, we focus on a special class of episodic MDPs, where the value function satisfies the following completeness assumption.

**Assumption 3.1** (\(\)-completeness under general function approximation, Agarwal et al. 2022).: Given a general function class \(\{_{h}\}_{h[H]}\), where each function class \(_{h}\) is composed of functions \(f_{h}:[0,L]\). We assume for each stage \(h[H]\), and any function \(V:[0,H]\), there exists functions \(f_{h},f_{2,h}_{h}\) such that

\[_{(s,a)}|f_{h}(s,a)-_{h}V(s,a) |,_{(s,a)}|f_{2,h}(s,a)-_{2,h}V( s,a)|.\]

In addition, for each stage \(h[H]\), we assume there exist a function \(f_{h}^{*}_{h}\) closed to the optimal value function such that \(\|f_{h}^{*}-Q_{h}^{*}\|_{}\). For simplicity, we assume \(L=O(H)\) throughout the paper and denote \(=_{h[H]}|_{h}|\).

To deal with general function class \(\), Agarwal et al. (2022) introduce the following measure to capture the function class complexity for online learning.

**Definition 3.2** (Generalized Eluder dimension, Agarwal et al. 2022).: Given \(>0\), a sequence of state-action pairs \(Z=\{z_{i}\}_{i[K]}\) and a sequence of non-negative weights \(=\{_{i}\}_{i[K]}\). Let \(\) be a function class consisting of functions \(f:[0,L]\). The generalized Eluder dimension of \(\) is given by \(_{,K}():=_{Z,:|Z|=K, }(,Z,)\), where

\[(,Z,) :=_{i=1}^{K}(1,^{2}}D_{}^{2}(z_{i};z_{[i-1]},_{[i-1]})),\] \[D_{}^{2}(z;z_{[k-1]},_{[k-1]}) :=_{f_{1},f_{2}}(z)-f_{2}(z))^{2}} {_{s[k-1]}^{2}}(f_{1}(z_{s})-f_{2}(z_{s}))^{2}+ }.\]

Here, the inequality \(\) represents that \(_{i}\) holds for all \(i[K]\) and we use the notation \(z_{[i-1]}\), \(_{[i-1]}\) to represent the sequences \(\{z_{s}\}_{s=1}^{i-1}\), \(\{_{s}\}_{s=1}^{i-1}\).

However, in offline RL, the proposed Generalized Eluder dimension fails to capture the relationship between function class \(\) and the pre-collected dataset \(\). To generalize this definition to offline environment, for a batch dataset \(=\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{h,k=1}^{H,K}\) and a function class \(_{h}\) consisting of functions \(f:\). We denote \(_{h}=\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{k[K]}\) as the subset of the dataset \(D\) that corresponds to the observations collected up to stage \(h\) in the MDP. Then for any weight function \(_{h}(,):\), we introduce the following \(D^{2}\)-divergence:

\[D_{_{h}}^{2}(z;_{h};_{h})=_{f_{1},f_{2} _{h}}(z)-f_{2}(z))^{2}}{_{k[K]}(z_{h}^{k}))^{2}}(f_{1}(z_{h}^{k})-f_{2}(z_{h}^{k}))^{2}+}.\]

Data Coverage Assumption:In offline RL, there exist a discrepancy between the state-action distribution generated by the behavior policy and the distribution from the learned policy. Under this situation, the distribution shift problem can cause the learned policy to perform poorly or even fail in offline RL. Therefore, we propose the following data coverage assumption to control the distribution shift.

**Assumption 3.3** (Uniform Data Coverage).: There exists a constant \(>0\), such that for any stage \(h\) and functions \(f_{1},f_{2}_{h}\), the following inequality holds,

\[_{,h}[(f_{1}(s_{h},a_{h})-f_{2}(s_{h},a_{h}))^{2} ]\|f_{1}-f_{2}\|_{}^{2},\]

where the state-action pair \((\)at stage \(h)\((s_{h},a_{h})\) is stochastic generated from behavior policy \(\).

**Remark 3.4**.: Data coverage assumption is widely used in offline RL to guarantee that the collected dataset contains enough information of the state-action space to learn an effective policy. In Yin et al. (2022b), they studied the general differentiable function, where the function class is defined as

\[:=f,(, ):,}.\]

Under this definition, Yin et al. (2022b) introduce the following coverage assumption (Assumption 2.3) such that for all stage \(h[H]\), there exists a constant \(\),

\[_{,h}[f(_{1}, (s,a))-f(_{2},(s,a)) ^{2}]\|_{1}-_{2} \|_{2}^{2},_{1},_{2}; (*)\] \[_{,h}[ f(,(s,a)) f(,(s,a))^{}]  I,.(**)\]

We can prove that our assumption is weaker than the first assumption (*). For the second assumption (**), there is no direct counterpart in the general setting.

In addition, for the linear function class, the coverage assumption in Yin et al. (2022b) will reduce to the following linear function coverage assumption (Wang et al., 2020; Min et al., 2021; Yin et al., 2022a; Xiong et al., 2022).

\[_{}(_{,h}[(s,a)(s,a)^{}])=>0, \  h[H].\]

Therefore, our assumption is also weaker than the linear function coverage assumption when dealing with the linear function class. Due to space limitations, we provide the detailed proof in the appendix.

## 4 Algorithm

```
0: Input confidence parameters \(_{1,h}^{},_{2,h}^{},_{h}\) and \(>0\).
1:Initialize: Split the input dataset into \(=\{s_{h}^{k},a_{h}^{k},r_{h}^{k}\}_{k,h=1}^{K,H},^{} =\{_{h}^{k},_{h}^{k},_{h}^{k}\}_{k,h=1}^{K,H}\) ; Set the value function \(_{H+1}()=_{H+1}()=0\).
2:for stage \(h=H,,1\)do
3:\(_{h}^{}=*{argmin}_{f_{h}_{h}} _{k[K]}(f_{h}(_{h}^{k},_{h}^{k})-_{h}^{k}- _{h+1}(_{h+1}^{k}))^{2}\).
4:\(_{h}^{}=*{argmin}_{g_{h}_{h}} _{k[K]}(g_{h}(_{h}^{k},_{h}^{k})-(_{h}^{ k}+_{h+1}^{}(_{h+1}^{k}))^{2})^{2}\).
5: Use the bonus oracle (Definition 4.1) to calculate the bonus function \(b_{h}^{}=(1,_{h}^{},_{h}, {f}_{h}^{},_{1,h}^{}+_{2,h}^{},,)\),
6:\(_{h}^{}\{_{h}^{}-b_{h}^{}- \}_{[0,H-h+1]}\);
7: Construct the variance estimator \(_{h}^{2}(s,a)=\{1,_{h}^{}(s,a)-( _{h}^{}(s,a))^{2}-O(}_{h}H^{3}}{})\}\).
8:endfor
9:for stage \(h=H,,1\)do
10:\(_{h}=*{argmin}_{f_{h}_{h}}_{k[K]} _{h}^{2}(s_{h}^{k},a_{h}^{k})}(f_{h}(s_{h}^{k},a_{h}^ {k})-r_{h}^{k}-_{h+1}(s_{h+1}^{k}))^{2}\)
11: Use the bonus oracle (Definition 4.1) to calculate the bonus function \(b_{h}=(_{h},_{h},_{h}, {f}_{h},_{h},,)\);
12:\(_{h}\{_{h}-b_{h}-\}_{[0,H-h+1]}\);
13:\(_{h}(|s)=*{argmax}_{a}_{h}(s,a)\).
14:endfor
15:Output:\(=\{_{h}\}_{h=1}^{H}\). ```

**Algorithm 1** Pessimistic Nonlinear Least-Squares Value Iteration (PNLSVI)

In this section, we provide a comprehensive and detailed description of our algorithm (PNLSVI), as displayed in Algorithm 1. In the sequel, we introduce the key ideas of the proposed algorithm.

### Pessimistic Value Iteration Based Planning

Our algorithm operates in two distinct phases, Variance Estimate Phase and Pessimistic Planning Phase. At the beginning of the algorithm, the data-set is divided into two disjoint subsets \(,^{}\), and each assigned to a specific phase.

The basic framework of our algorithm follows the pessimistic value iteration, which was initially introduced by Jin et al. (2021b). In details, for each stage \(h[H]\), we construct the estimator value function \(_{h}\) by solving the following variance-weighted ridge regression (Line 11):

\[_{h}=*{argmin}_{f_{h}_{h}}_{k[ K]}_{h}^{2}(s_{h}^{k},a_{h}^{k})}(f_{h}(s_{h}^{k},a_{h }^{k})-r_{h}^{k}-_{h+1}(s_{h+1}^{k}))^{2},\]

where \(_{h}^{2}\) is the estimated variance and will be discussed in section 4.2. In Line 12, we subtract the confidence bonus function \(b_{h}\) from the estimator value function \(_{h}\) to construct the pessimistic value function \(_{h}\). With the help of the confidence bonus function \(b_{h}\), the pessimistic value function \(_{h}\) is almost a lower bound for the optimal value function \(f_{h}^{s}\). The details of the bonus function and bonus oracle will be discussed in section 4.3.

Based on the pessimistic value function \(_{h}\) for stage \(h\), we recursively perform the value iteration for the stage \(h-1\). Finally, we use the pessimistic value function \(_{h}\) to do planning and output the greedy policy with respect to the pessimistic value function \(_{h}\) (Line 13 - Line 15).

### Variance Estimate Phase

In this phase, we provide a estimator for the variance \(_{h}\) in the weighted ridge regression. According to the definition of Bellman operators \(\) and \(_{2}\), the variance of the function \(_{h+1}^{}\) for each state-action pair \((s,a)\) can be denoted by

\[[_{h}_{h+1}](s,a)=_{2,h}_{h+1}^{ }(s,a)-(_{h}_{h+1}^{}(s,a))^{2}.\]

Therefore, we need the evaluate the first-order and second-order moments for \(_{h}^{}\). We perform nonlinear least-squares regression separately for each of these moments. Specifically, in Line 3, we conduct regression to estimate the first-order moment.

\[_{h}^{}=*{argmin}_{f_{h}_{h}} _{k[K]}(f_{h}(_{h}^{k},_{h}^{k})-_{h}^{k}- _{h+1}^{}(_{h+1}^{k}))^{2}.\]

In Line 4, we perform regression for the second-order moment.

\[_{h}^{}=*{argmin}_{g_{h}_{h}} _{k[K]}(g_{h}(_{h}^{k},_{h}^{k})-(_{h}^{ k}+_{h+1}^{}(_{h+1}^{k}))^{2})^{2}.\]

In this phase, we set the variance function to \(1\) for each state-action pair \((s,a)\) and derive an estimator with confidence radius \(_{1,h}^{},_{2,h}^{}\). Combing these two regression results and subtracting a confidence bonus function \(b_{h}^{}\), we create a pessimistic estimator for the variance function (Lines 6 to 7).

### Nonlinear Bonus Oracle

As we discussed in sections 4.1 and 4.2, we introduce a uncertainty bonus function to construct a pessimistic estimate of the value function. Unfortunately, for a general class, the uncertainty bonus may varies greatly across different state-action pair. Under this situation, the addition uncertainty bonus function will highly increase the complexity of the pessimistic function class, which make it difficult to construct a accurate estimation and may significant deteriorate the final performance. To address this issue, we assume there exist a function class \(\) with cardinally \(||=_{b}\) and can approximate the bonus function well. In addition, we assume there exist a nonlinear bonus oracle (Agarwal and Zhang, 2022), which can output the approximate bonus function in the class \(\) for each dataset \(_{h}\).

**Definition 4.1** (Oracle for bonus function).: For an offline dataset \(=\{s_{h}^{k},a_{h}^{k},r_{h}^{k}\}_{h,k=1}^{H,K}\), given index \(h[H]\), let \(_{h}=\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{k[K]}\) denote the subset of the dataset \(D\) that corresponds to the observations collected up to stage in the MDP. \(_{h}(,):\) is a variance function. \(_{h}\) is a function class such that \(_{h}_{h}\). The parameters \(_{h}\), \( 0\), error parameter \( 0\). The bonus oracle \((,_{h},_{h},_{h}, _{h},,)\) outputs a bonus function \(b_{h}()\) such that

* \(b_{h}:_{ 0}\) belongs to function class \(\).
* \(b_{h}(z_{h})\{|f_{h}(z_{h})-_{h}(z_{h})|,f_{h} _{h}:_{k[K]}(z_{h}^{k})-_{h}(z_{h}^{ k}))^{2}}{(_{h}(z_{h}^{k},a_{h}^{k}))^{2}}(_{h})^{2}\}\) for any \(z_{h}\).
* \(b_{h}(z_{h}) C(D_{_{h}}(z_{h};_{h}; {}_{h}))^{2}+}+_{h})\) for all \(z_{h}\) with constant \(0<C\).

**Remark 4.2**.: To address the concern of function class complexity, some previous studies (Xie et al., 2021a) have approached the problem differently. Instead of introducing a pointwise bonus in the estimated value function, they solve a complicated optimization problem to guarantee the optimism solely in the intial state. This method can prevent the complexity from bonus function, as a cost, they requires solving an optimization problem over all potential policy and corresponding version space, which includes all functions with lower Bellman error.

## 5 Main Results

In this section we prove an problem-dependent regret bound of Algorithm 1.

**Theorem 5.1**.: Under Assumption 3.3, for \(K(_{b})H^{6}}{ ^{2}})\), if we set the parameters \(_{1,h}^{},_{2,h}^{}=(_{b}}H^{2})\) and \(_{h}=(})\) in Algorithm 1, then with the probability of at least \(1-\), for any state \(s\), we have

\[V_{1}^{*}(s)-V_{1}^{}(s)(})_{h=1}^{H}_{^{*}}[D_{_{h}}(z_{h};_{h};[_{h}V_{h+1}^{*}](,))|s_{1}=s],\]

where \([_{h}V_{h+1}^{*}](s,a)=\{1,[_{h}V_{h+1}^{*}](s,a)\}\) is the truncated conditional variance.

**Remark 5.2**.: When reduce to the linear MDP environment, the following function classes

\[_{h}^{}=\{_{h}(,),_{ h}:_{h}^{d},\|_{h}\|_{2} B_{h}\} h[H],\]

satisfy the completeness assumption (Assumption 3.1) (Jin et al., 2020). Let \(_{h}^{}()\) be a \(\)-net of the linear function class \(_{h}^{}\). In this case, the covering number satisfies \(|_{h}^{}()|=(d)\) and the dependency of function class will reduce to \((})=()\). For linear function class, Xiong et al. (2022) proposed the following regret guarantee,

\[V_{1}^{*}(s)-V_{1}^{}(s)()_{h =1}^{H}_{^{*}}[\|(s_{h},a_{h})\|_{_{h}^ {*-1}}|s_{1}=s],\]

where \(_{h}^{*}=_{k[K]}(s_{h}^{k},a_{h}^{k})(s_{h}^{k },a_{h}^{k})^{}/[_{h}V_{h+1}^{*}](s_{h}^{k},a_{h}^{k})+\). In comparison, we can prove the following inequality:

\[D_{_{h}^{}()}(z;_{h};[_{h}V_{ h+1}^{*}](,))\|_{h}(z)\|_{_{h}^{*-1}}.\]

This shows that Theorem 5.1 matches the optimal result in Xiong et al. (2022) for linear function class.

## 6 Key Techniques

In this section, we provide an overview of the key techniques in our algorithm design and analysis.

### Variance Estimator with Nonlinear Function Class

The technique of variance-weighted ridge regression, first introduced in Zhou et al. (2021), has demonstrated its effectiveness in the online RL setting with linear function approximation. For offlinesetting, Xiong et al. (2022) modified the variance-weighted ridge regression technique, and showed that using an accurate and independent variance estimator can improves the performance of the pessimistic value iteration (PEVI) algorithm (Jin et al., 2021b).

In our work, we extend this technique to general nonlinear function class \(\), and use the following nonlinear least-squares regression to estimate the underlying value function:

\[_{h}=*{argmin}_{f_{h}_{h}}_{k[ K]}_{h}^{2}(s_{h}^{k},a_{h}^{k})}(f_{h}(s_{h}^{k},a_{h }^{k})-r_{h}^{k}-_{h+1}(s_{h+1}^{k}))^{2}.\]

For this regression, it is crucial to obtain a reliable evaluation for the variance of the estimated cumulative reward \(r_{h}^{k}+_{h+1}(s_{h+1}^{k})\). According to the definition of Bellman operators \(\) and \(_{2}\), the variance of the function \(_{h+1}^{}\) for each state-action pair \((s,a)\) can be denoted by

\[[_{h}_{h+1}^{}](s,a)=_{2,h}_ {h+1}^{}(s,a)-(_{h}_{h+1}^{}(s,a) )^{2}.\]

To evaluate the first and second moment for the Bellman operator, we perform nonlinear least-squares regression on a separate dataset \(^{}\) with uniform weight (\(_{h}(s,a)=1\) for all state-action pair \((s,a)\)).

For simplicity, we denote the empirical variance as \(_{h}(s,a)=_{h}^{}(s,a)-(_{h}^{ }(s,a))^{2}\), and the difference between empirical variance \(_{h}(s,a)\) with actually variance \([_{h}_{h+1}^{}](s,a)\) is upper bound by

\[|_{h}(s,a)-[_{h}_{h+1}](s,a)| |_{h}(s,a)-_{2,h}_{h+1}(s,a)|+ |(_{h}(s,a))^{2}-(_{h}_{h+1}(s,a))^{2}|.\]

For these nonlinear function estimator, the following Lemmas provide coarse concentration properties for the first and second order Bellman operators.

**Lemma 6.1**.: Given a stage \(h[H]\), let \(_{h+1}^{}(,) H\) be the estimated value function constructed in Algorithm 1 Line 6. By utilizing Assumption 3.1, there exists a function \(_{h}^{}_{h}\), such that \(|_{h}^{}(z_{h})-_{h}_{h+1}^{}(z_{h}) |\) holds for all state-action pair \(z_{h}=(s_{h},a_{h})\). Then with the probability of at least \(1-/4H\), it holds that \(_{k[K]}(_{h}^{}(z_{h}^{k})-_{h}^{} (_{h}^{k}))^{2}(_{1,h}^{})^{2},\) where \(_{1,h}^{}=(_{b}} H^{2})\), and \(_{h}^{}\) is the estimated function for first-moment Bellman operator (Line 3 in Algorithm 1).

**Lemma 6.2**.: Given a stage \(h[H]\), let \(_{h+1}^{}(,) H\) be the estimated value function constructed in Algorithm 1 Line 6. By utilizing Assumption 3.1, there exists a function \(_{h}^{}_{h}\), such that \(|_{h}^{}(z_{h})-_{2,h}_{h+1}^{}(z_{h} )|\) holds for all state-action pair \(z_{h}=(s_{h},a_{h})\). Then with the probability of at least \(1-/4H\), it holds that \(_{k[K]}(_{h}^{}(z_{h}^{k})-_{h}^{ }(_{h}^{k}))^{2}(_{2,h}^{})^{2},\) where \(_{2,h}^{}=(_{b}} H^{2})\), and \(_{h}^{}\) is the estimated function for second-moment Bellman operator (Line 4 in Algorithm 1).

Notice that all of the previous analysis focuses on the estimated function \(_{h+1}^{}\). By leveraging an induction procedure similar to existing works in the linear case (Jin et al., 2021b; Xiong et al., 2022), we can control the distance between the estimated function \(_{h+1}^{}\) and the optimal value function \(f_{h}^{*}\). In details, with high probability, for all stage \(h[H]\), the distance is upper bounded by \(O(_{b}}H^{3}/)\). This result allows us to further bound \([_{h}_{h+1}^{}](s,a)\) and \([_{h}f_{h+1}^{*}](s,a)\).

Therefore, the concentration properties in Lemmas 6.1 and 6.2 enable us to construct the pessimistic variance estimator, which satisfies the following property:

\[[_{h}V_{h+1}^{*}](s,a)-_{b}}H^{3}}{}_ {h}^{2}(s,a)[_{h}V_{h+1}^{*}](s,a).\] (6.1)

where \([_{h}V_{h+1}^{*}](s,a)=\{1,[_{h}V_{h+1}^{*}](s,a)\}\) is the truncated conditional variance. Compared with the results in the linear function class, we utilize the logarithm of the covering number of the function class as a substitute for the linear dimension \(d\), which is a common technique in nonlinear function approximation.

### Reference-Advantage Decomposition

The reference-advantage decomposition is a powerful technique to tackle the challenge of additional error from uniform concentration over whole function class \(_{h}\). Such an analysis approach has been first studied in the online RL setting Azar et al. (2017); Zhang et al. (2021); Hu et al. (2022); He et al. (2022); Agarwal et al. (2022) and later in the offline environment by Xiong et al. (2022).

For offline RL, in the context of nonlinear function classes, without a explicit linear expression, the increased complexity of the function class structure poses a significant obstacle to effectively utilizing this technique. Previous works, such as Yin et al. (2022b), have struggled to adapt the reference-advantage decomposition to their nonlinear function class, resulting in a parameter space dependence that scales with \(d\), instead of the optimal \(\). We provide detailed insights into this approach as follows:

\[r_{h}(s,a)+_{h+1}(s,a)-_{h}_{h+1}(s,a)= (s,a)+f_{h+1}^{*}(s,a)-_{h}f_{h+1}^{*}(s,a)}_{ }\]

\[+_{h+1}(s,a)-f_{h+1}^{*}(s,a)-([_{h} {f}_{h+1}](s,a)-[_{h}f_{h+1}^{*}](s,a))}_{}.\]

We decompose the Bellman error into two parts: the Reference uncertainty and the Advantage uncertainty. For the first term, the optimal value function \(f_{h+1}^{*}\) is fixed and not related to the pre-collected dataset, which circumvents additional uniform concentration over the whole function class and avoid any dependence on the function class size. For the second term, it is worth to notice that the distance between the estimated function \(_{h+1}^{}\) and the optimal value function \(f_{h}^{*}\) is decreased as \(O(1/)\). Though, we still need to maintain the uniform convergence guarantee, the Advantage uncertainty is dominated by the Reference uncertainty when the number of episode \(K\) is large enough. By integrating these results, we can prove a variance-weighted concentration inequality for Bellman operators.

**Lemma 6.3**.: For each stage \(h[H]\), assuming the variance estimator \(_{h}\) satisfies (6.1), let \(_{h+1}(,) H\) be the estimated value function constructed in Algorithm 1 Line 12. By utilizing Assumption 3.1, there exist a function \(_{h}_{h}\), such that \(|_{h}(z_{h})-_{h}_{h+1}(z_{h})|\) holds for all state-action pair \(z_{h}=(s_{h},a_{h})\). Then with the probability of at least \(1-/4H\), it holds that \(_{k[K]}_{h}(z_{h}^{k}))^{2}}(_{ h}(z_{h}^{k})-_{h}(z_{h}^{k}))^{2}(_{h})^{2},\) where \(_{h}=(})\) and \(_{h}\) is the estimated function from the weighted ridge regression (Line 10 in Algorithm 1).

After controlling the Bellman error, with a similar argument to Jin et al. (2021b); Xiong et al. (2022), we obtain the following lemma, which provide an upper bound for the regret.

**Lemma 6.4** (Regret Decomposition Property).: If \(|_{h}_{h+1}(z)-_{h}(z)| b_{h}(z)\) holds for all stage \(h[H]\) and state-action pair \(z=(s,a)\), then the regret of Algorithm 1 can be bounded as

\[V_{1}^{*}(s)-V_{1}^{}(s) 2_{h=1}^{H}_{^{*}} [b_{h}(s_{h},a_{h}) s_{1}=s].\]

Here, the expectation \(_{^{*}}\) is with respect to the trajectory induced by \(^{*}\) in the underlying MDP.

Combing the results in Lemmas 6.3 and 6.4, we have proved Theorem 5.1.

## 7 Conclusion and Future Work

In this paper, we present PNLSVI, an oracle-efficient algorithm for offline RL with non-linear function approximation. It achieves minimax optimal problem-dependent regret when specialized to linear function approximation.

Regarding future work, we observe that instead of using the uniform coverage assumption, a series of works, such as (Liu et al., 2020; Xie et al., 2021a; Uehara and Sun, 2021; Zhan et al., 2022), only relies on partial coverage assumption. In these works, the offline data distribution only encompasses the state-action distribution of a select high-quality comparator policy \(^{*}\). It would be of significant interest to investigate whether it's possible to design practical algorithms for nonlinear function classes under this weaker partial coverage assumption, while still preserving the inherent efficiency found in linear function approximation.