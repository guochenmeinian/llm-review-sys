# Supplementary Materials for:

On the Convergence and Sample Complexity Analysis of Deep Q-Networks with \(\)-Greedy Exploration

 Shuai Zhang

New Jersey Institute of Technology

&Hongkang Li

Rensselaer Polytechnic Institute

Meng Wang

Rensselaer Polytechnic Institute

Miao Liu

IBM Research

Pin-Yu Chen

IBM Research

Songtao Lu

IBM Research

Sijia Liu

Michigan State University

&Keerthiram Murugesan

IBM Research

Subhajit Chaudhury

IBM Research

###### Abstract

This paper provides a theoretical understanding of Deep Q-Network (DQN) with the \(\)-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly over-parameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with \(\)-greedy policy. We prove an iterative procedure with decaying \(\) converges to the optimal Q-value function geometrically. Moreover, a higher level of \(\) values enlarges the region of convergence but slows down the convergence, while the opposite holds for a lower level of \(\) values. Experiments justify our established theoretical insights on DQNs.

## 1 Introduction

Reinforcement learning (RL) is a sequential decision-making process for a learning agent taking actions in the environment. RL has found important applications in autonomous control , healthcare , Internet of Things , and natural language processing . The environment of an RL problem is modeled as a Markov decision process (MDP) with an underlying state transition probability matrix. The goal of the problem is to find an optimal policy to select the best actions to maximize the immediate and future rewards. Q-learning  has been recognized as one of the most promising and efficient learning algorithms for seeking the optimal policy because it does not require the knowledge of the model of the environment (namely, "model free") and can learn from data generated from a non-optimal policy (namely, "off-policy"). Traditional Q-learning approaches are centered on tabular methods  or linear function approximations  to estimate the optimal action-value (Q-value) function. However, tabular methods require sample complexity scaling in the order of state space, which is impractical for modern RL problems involving large or even infinite state space . Q-learning with linear function approximation is limited to applications only when the transition matrix admits a linear feature representation .

Due to the remarkable advancements in deep neural networks (DNNs), the Deep Q-network (DQN) framework has emerged as a powerful approach that leverages the expressive power of non-linear functions and the ability to generalize to unknown states. DQN has proven to be a pioneering solution that addresses challenges encountered by traditional approaches. In the DQN framework , the Q-value function is approximated using a DNN, and the algorithm iteratively updates the Q-value function by collecting data following the \(\)-greedy policy. The \(\)-greedy policy is the simplest approach to balance exploration and exploitation. Namely, with a probability of \(\), we select a random action (_exploration_), and with a probability of \(1-\), we choose the best action according to the current estimated Q-value function (_exploitation_). \(\)-greedy is myopic compared with other strategic explorations, e.g., Thompson sampling-based  and optimism in the face of the uncertainty (OFU)-based  ones. However, implementing these strategic explorations is not computationally efficient in DQNs , while DQNs equipped with \(\)-greedy policy have shown empirical success in diversified applications, e.g., the game of Go , Atari games , robotics , and autonomous vehicles .

Despite the numerical success, the theoretical understanding of DQN remains elusive, which could prevent its applications in domains requiring _reliable and safe_ decision-making. First, updating the Q-value function involves minimizing a mean-squared Bellman error (MSBE) function. However, existing convergence analysis and statistical properties are predominantly limited to linear models, failing to capture the complexities present in non-linear neural networks like DQN. Second, the sample complexity required for the convergence of MSBE is still not well comprehended. Achieving the desired accuracy in this context often demands a sample complexity that grows exponentially with the input dimension , rendering it impractical and inefficient in real-world scenarios. Third, the optimal selection of the \(\) value in DQN remains a gap in existing research. The hyperparameter tuning in algorithms involved with neural networks can be arduous and time-consuming. For instance, without a well-designed hyperparameter configuration, only a small fraction (e.g., 1%) of the possible combinations yield satisfactory results in neural network training .

**Contributions.** To the best of our knowledge, this paper presents the first theoretical study with convergence analysis for Deep Q-Networks (DQNs) utilizing the \(\)-greedy policy. A comparison with existing works can be found in Table 1. The paper focuses on the Q-value function approximated by a DNN. It offers a comprehensive analysis of the convergence of DQNs and provides insights into the estimation error of the learned Q-value function, accompanied by an analysis of the sample complexity. The key contributions of this study are as follows:

**1. The convergence analysis of DQN with bounded estimation errors.** Assuming the existence of a DNN with unknown weights \(^{}\) that matches the optimal Q-value function, this paper proves that the learned model via DQNs equipped with \(\)-greedy policy through the (accelerated) gradient descent algorithm converges linearly to \(^{}\) up to some characterizable estimation error.

**2. The theoretical characterization for a wide selection range of \(\) over iterations.** This paper provides lower and upper bounds of \(\) at each iteration for the convergence of DQNs and, in particular, characterizes the sample complexity, estimation error, and convergence rate of the DQN equipped with \(\)-greedy with decreasing \(\). Moreover, this paper proves that a higher level of \(\) values leads to an enlarged region of convergence, which relaxes the requirement on the initialization, while a lower level of \(\) values leads to faster convergence.

**3. The sample complexity analysis for learning a desired Q-value function.** We quantify the required sample complexity, depending on the neural network parameters and distribution shift of the collected data, for the learned model to converge to the desired accuracy. Typically, the estimation error of the converged model scales in the order of \(1/}\), where \(N_{s}\) is the number of samples.

  
**Work** & **Neural Approximation** & **Convergence of MSBE** & \(\)**-greedy** \\   Yang \& Wang (2019) & ✗ & ✓ & ✗ \\  Xu \& Gu (2020) & ✓ & ✓ & ✗ \\  Fan et al. (2020) & ✓ & ✗ & ✗ \\  Liu et al. (2022) & ✓ & ✗ & ✓ \\  This work & ✓ & ✓ & ✓ \\   

Table 1: Comparison among some representative works of Q-learning with function approximation.

Related Works.

**Q-learning with linear function approximation.** In the setting of linear function approximation, the Q-function is assumed to be a linear function of either the feature mapping [83; 32; 92] or a mixture of some basis kernels [91; 52]. Early works mainly focus on the algorithm design [6; 48; 2] and convergence analysis [38; 47; 67; 14; 69] but lacks theoretical guarantees with polynomial sample complexity. Assuming the underlying Q-function can be exactly represented as a linear function of the feature mapping with some unknown parameters, several sample-efficient algorithms are proposed to find the ground-truth mapping with finite-sample guarantee [11; 80], and the sample complexity depends linearly on the feature dimension .

**Q-learning with non-linear function approximation.** Recent approaches with non-linear function approximation mainly fall into the frameworks of the Bellman Eluder (BE) dimension [30; 18; 58; 27], Neural Tangent Kernel (NTK) [81; 10; 79; 10; 20; 53], and Besov regularity [68; 29; 54; 46; 24]. The Eluder dimension is at least in an exponential order even for a two-layer neural network , leading to uncharacterizable sample complexity. The NTK framework linearizes deep neural networks to tackle convergence in non-linear models. However, it requires using computationally inefficient extremely wide neural networks . Moreover, the sample complexity can exponentially increase with the input feature dimension, necessitating a substantial number of samples for accurate estimation. Furthermore, the NTK approach fails to explain the advantages of non-linear neural networks over linear function approximation [46; 24]. Besov space requires the neural networks to be sparse and makes an unpractical assumption that the algorithm can find the global optimal of the non-convex objective function [29; 54; 46; 24]. To the best of our knowledge, only  considers \(\)-greedy policy with theoretical analysis applicable to DQNs. However, the model is limited to sparse neural networks, and it cannot characterize the case of varying \(\).

**Supervised learning with neural networks.** Compared with Q-learning in RL, where the label of the sampled data depends on the currently estimated Q function, analyzing supervised learning is less challenging, where sampled data label is known and fixed across the training. Existing theoretical results for supervised learning are largely built upon NTK [28; 21; 39; 15; 43; 45], model recovery [90; 26; 3; 64; 88; 85; 87], and structured data [44; 62; 8; 1; 35; 78; 84; 42]. Due to the high non-convexity of neural networks , the one-hidden-layer neural network is still a state-of-the-art practice for convergence analysis and generalization guarantees. Additional assumptions, e.g., Gaussian distribution [89; 7], linear separable data [75; 9] on the data distribution, are needed for finite-sample analysis.

## 3 Problem Formulation: Notation, Background, and Algorithm

**The Markov Decision Process and Q-learning.** A discounted Markov decision process (MDP) is defined as \((,,,r,)\), where \(\) is the state space, \(\) is the action set. \(:()\) is the transition operator, and \(p^{a}_{,^{}}:= {s}^{}|,a\) denotes the transition probability from current state \(\) and action \(a\) to the next state \(^{}\). In addition, \(r:[-R_{},R_{}]\) is the reward function, and \((0,1)\) is the discount factor.

At a state \(\), the agent takes action \(a\) according to some behavior policy \(\), denoted as \(a()\) (or \(a=()\) for deterministic policy). Then, the system moves to the next state \(^{}\) following the transition probability \(p^{a}_{,^{}}\). Meanwhile, the agent receives an immediate reward \(r(,a)\) from the environment. Let \(\{_{i},a_{i}\}_{i=0}^{}\) be the generated sequential data given the behavior policy \(\) and transition probability \(\). We define the state-value function \(V_{}\) at state \(s\) as

\[V^{}()= _{,}_{i=0}^{}^{i} r(_{i},a_{i})_{0}=,\] (1)

which is the expected total discounted reward starting from the state \(\). For any state-action \((,a),\) the corresponding Q-value (or action-value) function \(Q_{}\) of a policy \(\) is defined as

\[Q^{}(,a)= _{,}_{i=0}^{}^{i} r(_{i},a_{i})_{0}=,a_{0}=a .\] (2)

Then, the goal of the agent is to find an optimal policy \(^{}\) that maximizes the state-value function in (1) for all states, which is equivalent to

\[Q^{}(,a):=_{}Q^{}(,a)= r(,a)+_{^{}| ,a}_{a^{}}Q^{}(^{},a^{}),\] (3)where (3) is known as the Bellman equation. With the optimal Q-value function \(Q^{}\), the optimal policy can be derived via \(^{}()=*{argmax}_{a}Q^{}(,a)\).

**The Deep Neural Network Model.** The DQN utilizes a deep neural network (DNN) \(H:^{d}\) to approximate the optimal Q-value function \(Q^{}\) in (3). Specifically, given input \(^{d}\), the output of the \(L\)-hidden-layer DNN with \(K\) neurons in each hidden layer is defined as

\[H(;):=^{}/K(_{L}^{}(_ {1}^{})),\] (4)

where \(_{1}^{d K}\), \(_{l}^{K K}\) with \(l=2,,L\), and \(=[(_{1})^{},,(_{L})^{}]^ {}\) is the concatenation of the vectorization of all parameter matrices. \(()\) is the nonlinear activation function, and we consider the ReLU activation function, i.e., \((z)=\{0,z\}\). Then, the Q-value function \(Q(,a)\) is parameterized using the DNN as

\[Q(;,a)=H;(,a),\] (5)

where \(:^{d}\) is the feature mapping of the state-action pair. Without loss of generality, we assume \(|(,a)| 1\). Then, the goal of DQN-based Q-learning is to minimize the mean squared Bellman error (MSBE) as

\[_{}:f():=_{(,a)^{}}Q(;,a)-r(,a)-_{^{}|,a} _{a^{}}Q(;^{},a^{})^{2},\] (6)

where \(\) is the distribution of \((,a)\) following the optimal policy \(^{}\).

### The Deep Q-Network Algorithm

```
1:Input: Number of iterations \(T M\), and experience replay buffer size \(N\), exploration probability \(\{_{t}\}_{t=1}^{T}\), step size \(\), and momentum parameter \(\).
2:Initialize the Q-network with weights \(^{(0,0)}\).
3:for\(t=0,1,2,,T-1\)do
4: Select the initial weights \(^{(t,0)}\).
5:for\(m=0,1,2,,M-1\)do
6: Sample data and store in the experience replay buffer \(_{t}\) following \(_{t}\)-greedy policy, namely, at state \(_{n}\), with probability \(_{t}\), select a random action \(a_{n}\), otherwise select \(a_{n}=*{argmax}_{a}Q(^{(t,0)};_{n},a)\).
7: Sample random mini-batch of transition \(_{t}^{(m)}\) from the replay buffer \(_{t}\).
8: Set \(y_{n}=r_{n}+_{a}Q(^{(t,0)};_{n}^{},a)\) for \(n 1,2,,|_{t}^{(m)}|\).
9: Perform a gradient descent step \[^{(t,m+1)}= ^{(t,m)}- g_{t}^{(m)}(^{(t,m)})+( ^{(t,m)}-^{(t,m-1)}).\]
10:endfor
11: Set \(^{(t+1,0)}=^{(t,M)}\).
12:endfor ```

**Algorithm 1** Deep Q-Network

The learning problem (6) is solved via the DQN equipped with \(\)-greedy exploration, as summarized in Algorithm 1. In \(t\)-th outer loop, we initialize the Q-value function using currently estimated weights \(^{(t,0)}\) as \(Q(^{(t,0)})\) (line 4). Then, for each inner loop, the agent selects and executes actions according to the \(\)-greedy policy (line 6), namely, with probability \(\), we select a random action, and with probability \(1-\), we select the action based on the greedy policy with respect to \(Q(^{(t,0)})\). The data are stored in a replay buffer with size \(N\) (line 7). Then, we sample a mini-batch of independent samples from \(_{t}\), denoted as \(_{t}^{(m)}\) for the \(m\)-th inner loop (line 8). Next, we update the current weights using a mini-batch (accelerated) gradient descent algorithm (line 9). The gradient descent (GD) direction in this step at point \(\) is represented as

\[g_{t}^{(m)}()=_{n_{t}^{(m)}}Q(;_{n},a _{n})-y_{n}^{(t)}_{}Q(;_{n},a_{n}),\] (7)

where

\[y_{n}^{(t)}=r_{n}+_{a^{}}Q(^{(t,0)}; _{n}^{},a^{}).\] (8)

Note that (7) can be viewed as the gradient of

\[_{(,a)_{t}}Q(;,a)-r-_{^{}|,a}_{a^{}}Q(^{(t,0)};^{},a^{ })^{2},\] (9)which is the approximation to (6) via fixing the \(_{a}Q()\) as \(_{a}Q(^{(t,0)})\). After moving along the GD direction, accelerated gradient descent (AGD) adds a momentum term, denoted by \((^{(t,m)}-^{(t,m-1)})\) to accelerate the convergence rate . Vanilla SGD can be viewed as a special case of AGD by letting \(=0\). After updating neuron weights in the inner loop, we set the network as the currently estimated Q-value function \(Q(^{(t,0)})\) (line 11) and repeat the steps above.

## 4 Theoretical Results

### Takeaways of the Theoretical Findings

We consider the general setup of DQNs with \(\)-greedy under some commonly used assumptions. To the best of our knowledge, we provide the first theoretical characterization of both the convergence and sample complexity analysis for DQNs with \(\)-greedy. The major notations are summarized in Table 2. We first briefly introduce the key takeaways of our results, and the formal theoretical results are introduced in Section 4.3.

**(T1) Theoretical characterizations of \(\{_{t}\}_{t=1}^{T}\) for convergence.** We prove that for a wide selection of \(\{_{t}\}\) values that decrease over time, Algorithm 1 converges to \(Q^{}\) linearly up to some estimation error. Let \(c_{}\) measure the value level of \(_{t}\)'s. A higher level of \(\) values (i.e., a larger \(c_{}\)) leads to an enlarged region of convergence (in the order of \(c_{}\)), measured by the distance from the initialization \(^{(0,0)}\) to \(^{}\). Thus, larger \(\) values relax the requirements on \(^{(0,0)}\). A lower level of \(\) values (i.e., a smaller \(c_{}\)) leads to faster convergence with a rate in the order of \(c_{}\). Our findings explain the intuition that the agent tends to explore more at the beginning and exploit more after gaining enough knowledge during the exploration.

**(T2) Convergence to the optimal Q-value function \(Q^{}\) with geometric decay.** The learned models converge to the ground truth model \(Q^{}\) with a geometric decay up to some bounded estimation error. The convergence rate is upper bounded by \(+c_{}(1-)\). When \(\) is close to one, the problem emphasizes long-term rewards. While the immediate reward can be observed directly, the future reward needs to be estimated by the learned Q function as shown in (7). Therefore, with a large \(\), the learned Q function needs more iterations to converge, which leads to a slow convergence rate.

**(T3) Sample complexity for achieving a desired estimation error of the optimal Q-value function.** With the proper selection of \(\{_{t}\}_{t=1}^{T}\), the estimation error of the learned model scales in the order of \(}}{(1-)^{2}}}\). \(C_{T}\) is the fraction of actions following the current greedy policy that differ from the ones following the optimal policy. \(N_{s}\) is the number of samples. With a smaller discounted factor \(\), the problem focuses more on the immediate reward, which can be observed directly, making \(Q^{}\) easier to learn. The learned model achieves a small estimation error given a small distribution shift, a large sample size, or a small discounted factor.

### Assumptions

We propose assumptions commonly used in existing RL and neural network learning theories and notations to simplify the presentation. Assumption 1 assumes the existence of a good approximation of DNN to \(Q^{}\), which guarantees (6) is solvable. The assumption is commonly used in both deep learning theories [90; 86] and reinforcement learning theories [46; 24]. Assumption 2 assumes the samples from experience replay are independent and identically distributed (i.i.d.), which follows the assumptions in existing theoretical analysis of DQN [24; 53] and matches the intuition of using experience replay to break temporal dependence among the samples. Specifically, we have

**Assumption 1**.: _There exists a DNN with weights \(^{}\) such that minimizes (6) as \(f(^{})=0\)._

   \(K\) & Number of neurons in each hidden layer. & \(L\) & Number of the hidden layers. \\  \(d\) & Dimension of the feature mapping of \((,a)\). & \(N_{s}\) & The sample complexity for \(\)-optimal policy. \\  \(^{}\) & The global optimal to (6). & \(e_{t}\) & The value of \(\|^{(t,0)}-^{}\|_{F}\). \\  \(c_{}\) & A small positive constant with a linear dependence on \(_{t}\). & \(C_{t}\) & The fraction of actions with data at iteration \(t\) such that \(*{argmax}_{a}Q(^{(t,0)};,a)*{argmax} _{a}Q(^{};,a)\). \\   

Table 2: Some Important NotationsAssumption 1 assumes that the Q-value function with some unknown ground truth \(^{}\)1 can represent the optimal Q-value function.

**Assumption 2**.: _Suppose the mini-batch data are i.i.d. samples from the replay buffer following the distribution \(_{t}\), which is the stationary distribution of the behavior policy at \(t\)-th outer loop._

Assumption 2 assumes the mini-batch at the \(t\)-th outer loop are _i.i.d._ samples following \(_{t}\), where \(_{t}\) is the stationary distribution of \((,a)\) generated by \(_{t}\)-greedy policy at \(t\)-th outer loop2. The mini-batch samples are close to being independent given the experience size in practice is sufficiently large (\(\) millions ).

In what follows, we define two quantities \(C_{t}\) and \(\) to simplify the presentation of theoretical results.

**Definition 1**.: \(C_{t}\) _is the fraction of non-optimal state-action pair \((,a)\) in the greedy policy with respect to \(Q(^{(t,0)})\), i.e., the fraction of \((,a)\) pairs that satisfy_

\[a=*{argmax}_{a^{}}Q(^{(t,0)};,a^{}) ^{}()\] (10)

_among all \((,a)\) pairs following the greedy policy at \(t\)-th outer loop as \(a=*{argmax}_{a^{}}Q(^{(t,0)};,a^{})\)._

\(C_{t}\) can be viewed as the difference between behavior policy and optimal policy. Theorem 1 shows the general results for any value of \(C_{t}\). Nevertheless, the greedy policy is improved over time, e.g., updating the weights every few steps (line 1 in Algorithm 1). Hence, \(C_{t}\) depends on \(^{(t,0)}\) and is expected to decrease as \(^{(t,0)}\) approaching \(^{}\)[56; 92], which will be discussed in Corollary 3.

**Definition 2**.: _Let \(\) be the value of_

\[:=_{\|\|_{2} 1}_{(,a)^{}} ^{}_{}Q(^{};,a)^{2}.\] (11)

\(\) suggests the radius of the local convex region of the objective function. We provide the lower bound for \(\) in Lemma 7 (see the proof in Appendix E.2), suggesting a sufficiently large local convex region near \(^{}\).

### Major Theoretical Results

Lemma 1 characterizes the convergent point when minimizing (9) using the mini-batch gradient descent in the \(t\)-th outer loop under certain conditions. Specifically, given that the initial weights at the \(t\)-th outer loop are sufficiently close to the ground truth as shown in (12) and the replay buffer is large enough as shown in (13), the distance between the learned model weights \(^{(t+1,0)}\) and \(^{}\) are bounded from above as shown in (14).

**Lemma 1** (Estimation error of \(^{(t+1,0)}\)).: _Suppose Assumptions 1 & 2 hold and the initial neuron weights at the \(t\)-th outer loop satisfy_

\[e_{t}:=\ \|^{(t,0)}-^{}\|_{F}\ 1- )}{()}^{}\|_{F}}{K},\] (12)

_The step size \(\) is \(1/T\), and the size of the replay buffer is_

\[N=(^{-2} K^{3} L d q T).\] (13)

_Then, with the high probability of at least \(1-q^{-d}\), the neuron weights \(^{(t+1,0)}\) generated from Algorithm 1 satisfy_

\[\|^{(t+1,0)}-^{}\|_{F}\ 1-(_{t}) \|^{(t,0)}-^{}\|_{F}++(1-C_ {t})_{t}}{()}| R_{}} {1-}.\] (14)

**Remark 1** (Large replay buffer reduces the estimation error and the requirement for \(^{(t,0)}\)).: From (14), a larger \(N\) leads to a reduced distance between \(^{(t+1,0)}\) and \(^{}\). Moreover, (12) implies that if we increase the size of replay buffer \(N\), the upper bound of \(e_{t}\) increases, indicating the algorithm can tolerant a large range of \(^{(t,0)}\).

In the following corollary, we show the upper and lower bound of \(_{t}\) at the \(t\)-th outer loop. The lower bound guarantees that the RHS of (12) is larger than \(0\), so we have a sufficiently large radius for convergence. The upper bound ensures (14) be less than \(e_{t}\), indicating an improved estimation of \(Q^{}\) across the iterations.

**Corollary 1** (Range of \(\)).: _Given the assumptions and conditions in Theorem 1 hold. To ensure the existence of a good initialization at iteration \(t\), \(_{t}\) needs to satisfy_

\[_{t} 1-()(1-e_{t}),\] (15)

_To ensure the estimated learned model is improving over iterations, \(_{t}\) needs to satisfy_

\[_{t}() e_{t}}{(1- C_{t})|| R_{}}-}{1-C_{t}}.\] (16)

**Remark 2** (Reduce \(_{t}\) as \(t\) increases).: The lower bound can always be smaller than the upper bound given a sufficiently large \(N\) as shown in (13). From both (15) and (16), we know that \(_{t}\) needs to decrease as \(e_{t}\) decreases. Specifically, the lower bound of \(_{t}\) is a linear function of \(e_{t}\), and the upper bound of \(_{t}\) is a linear function of \(e_{t}\). Namely, we need a relatively large \(_{0}\) at the beginning since the distance between initial point \(^{(0,0)}\) and \(^{}\) is large. As \(t\) increases, \(e_{t}\), which is the distance of learned neuron weights \(^{(t,0)}\) to the ground truth \(^{}\), becomes smaller, and we should decrease \(_{t}\) to guarantee an improved \(Q^{(t+1,0)}\) over \(Q^{(t,0)}\).

Theorem 1 shows that the learned model from Algorithm 1 converges to the optimal Q-value function \(Q^{}\) with geometric decay up to an estimation error shown in (20).

**Theorem 1** (Convergence to \(Q^{}\)).: _Suppose Assumptions 1 and 2 hold, the buffer size \(N\) satisfies (13). Let us define \(C_{}\) be a constant that is larger than \(C_{t}\) for \(1 t T\), when \(_{t}\) satisfy_

\[_{t}=() e_{t}}{(1-C_{ })|| R_{}}-}{1-C_{}}\] (17)

_for a fixed constant \(c_{}(0,(1-)^{2}]\), and the initialization satisfies_

\[\|^{(0,0)}-^{}\|_{F}\ 1-}{()}^{}\|_{ F}}{K}.\] (18)

_Then, with the high probability of at least \(1-T q^{-d}\), we have_

1. _the learned weights decay geometrically with_ \[\|^{(t+1,0)}-^{}\|_{F}+c_{}( 1-)\|^{(t,0)}-^{}\|_{F},\ \  t_{}(1/N).\] (19)
2. _the returned model_ \(Q(^{(T,0)})\) _exhibits an estimation error in the order of_ \(1/\) _with_ \[_{(,a)}Q^{(T,0)})-Q^{}= \|^{(T,0)}-^{}\|_{F}|| R_{}}{(1-)^{2}()},\] (20) _where_ \(T_{}(1/N)\)_._

**Remark 3** (Selection of \(_{t}\)).: The value of \(_{t}\) is influenced by three key factors: \(c_{}\), \(C_{}\), and the current estimation error bound \(e_{t}\). The constant \(c_{}\) is fixed and controls the magnitude of the values in the sequence \(_{t}t_{t=1}^{T}\), providing a way to regulate the level of \(_{t}\). Regarding \(C_{}\), \(C_{t}\) tends to decrease as the iteration index \(t\) increases, indicating a progressive improvement in the policy and resulting in a smaller data distribution shift. Hence, to estimate \(C_{}\), we can leverage \(C_{0}\), which is obtained by collecting data based on the policy \(_{a}Q(^{(0,0)};,a)\). Moreover, with \(c_{}\) fixed, the estimation error \(e_{t}\) follows a geometric decay pattern, as depicted in (19). This behavior allows us to use the expression \(+c_{}(1-)^{t} e_{0}\) as an estimate for \(e_{t}\).

**Remark 4** (Geometric decay to \(Q^{}\)).: From (19) and (20), we know that the learned model from the proposed algorithm converges to \(Q^{}\) with a geometric decay up to some estimation error. The convergence rate is in the order of \(+c_{}(1-)\), and the estimation error is in the order of \((1-)^{-2} C_{}/\). As we mentioned in the takeaways in Section 4.1, a small discounted factor \(\) leads to a fast convergence rate. We have a reduced estimation with a large buffer with size \(N\), a small distribution shift \(C_{T}\), and a small \(\).

**Remark 5** (Larger \(\) values for enlarged region of convergence and smaller \(\) values for faster convergence). From (17), we know that \(c_{}\) controls the value level of \(_{t}\). From (18), a larger \(c_{}\) (i.e., a higher level of \(\) values) increases the upper bound of \(\|^{(0,0)}-^{}\|_{2}\) and, thus, enlarges the proper region of \(^{(0,0)}\). (19) indicates that the convergence rate is in the order of \(c_{}\). A smaller \(c_{}\) (i.e., lower level of \(\) values) leads to faster convergence.

In the following corollary, we provide the sample complexity for achieving \(\) estimation error as shown in (21), where \(()\) omits some \(\) factors. The corollary can be obtained by letting (20) to be less than a desired accuracy \(\).

**Corollary 2** (Sample complexity).: _To achieve an estimation error of \(\), the required number of samples, referred to as the sample complexity, needs to satisfy_

\[N_{s}=N=(1-)^{4} C_{} ||^{2}R_{}^{2} K^{3} L d T/^{2} .\] (21)

**Remark 6** (Sample complexity).: (21) shows that the sample complexity is a linear function of \(d\) and \(L\), where \(d\) is the feature mapping of the state-action pair \((,a)\) and \(L\) is the number of layers. Given the freedom of degree of \(\) is a linear function of \(d\) and \(L\), respectively, the sample complexity is almost order-wise optimal with respect to \(d\) and \(L\).

The following corollary presents a tighter bound for the model estimation error compared with (20). This improvement arises from a stronger assumption on \(C_{t}\), which becomes a function of \(\|^{(t,0)}-^{}\|_{2}\). Compared with (20), the estimation error bound in (24) and (25) considers the cases that the behavior policy is improved as \(^{(t,0)}\) becomes closer to \(^{}\). As a result, we achieve a more precise and tighter estimation of the error in the model.

**Corollary 3** (Distribution shift and estimation error).: _Assume that \(C_{t}\) is Holder continuous with a factor \(\) as_

\[C_{t}=(\|^{(t,0)}-^{}\|_{2}^{}),\] (22)

3 _for \(0< 1\) and all \(t[T]\). When \(_{t}\) satisfy_

\[_{t}=() e_{t}}{(1-C_{ t})|| R_{}}-}{1-C_{t}},\] (23)

_the estimation error of the Q-function satisfies_

\[_{(,a)}Q(^{(T,0)})-Q^{}=| R_{})^{}}{(1-)^{} (N^{})}.\] (24)

_In the special case that \(C_{t}=(\|^{(t,0)}-^{}\|_{2})\), then_

\[_{(,a)}Q(^{(T,0)})-Q^{}=0.\] (25)

**Remark 7** (Reduced or zero estimation error when behavior policy is improved over iterations).: Recall the definition of \(C_{t}\) in Definition 1. If \(e_{t}\) is zero, i.e., \(_{t}=^{}\), the action selected following the greedy policy is always the optimal action, which means that \(C_{t}=0\). Therefore, it is reasonable to assume that \(C_{t}\) is Holder continuous as shown in (22). When \(>0\), we can see that (24) is less than (20), indicating a reduced estimation error and sample complexity. Typically, if \(C_{t}\) has an order of growth less than \(e_{t}\) near \(^{}\), a zero estimation error is achievable.

### The Roadmap of Proofs, Comparison with Existing Works, and Limitations

The proof of Theorem 1 draws inspiration from the model estimation framework in the supervised learning setting [90; 88]. The key idea is to use a population risk function (PRF) to characterize the objective function in (9). By satisfying certain conditions such as having sufficient training samples and a bounded data distribution shift, the approximation error between the PRF and objective function can be bounded. This allows for the characterization of the optimization problem in (9) by analyzing the landscape and convergence properties of the PRF.

In comparison to existing proofs based on the model estimation frameworks, this paper addresses two additional challenges. Firstly, it extends the proof from one-hidden-layer neural networks to multi-layer neural networks. This extension is achieved by providing new tools for characterizing the Hessian matrix (refer to Lemma 3) and concentration bound (refer to Lemmas 3 and 6). Additionally, this paper characterizes the differences between the two functions caused by the interaction of neuron weights across layers in the gradient and Hessian matrix. Secondly, the paper extends the proof from supervised learning settings to Q-learning settings. This requires characterizing the additional error term caused by the data distribution shift and "noisy" labels (refer to Lemma 3) because the empirical risk function is no longer an expectation of the defined population risk function.

Existing state-of-the-art theoretical results on Q-learning with neural network approximations primarily revolve around the NTK and Besov regularity frameworks. In the NTK framework, the networks are assumed to be extremely over-parameterized, requiring an impractical projection step and resulting in error bounds that cannot be characterized for networks with finite width. In the Besov regularity framework, the neural network needs to be sparse, which does not align with the DQN algorithm. Furthermore, the analysis in the Besov regularity framework relies on the achievability of the global minimizer of the non-convex problem in (9), which cannot be guaranteed using GD algorithms. This paper takes a significant step towards bridging the gap between theoretical understanding and practical applications of DQN by addressing these challenges. However, there still remains a gap between the theoretical results and numerical findings. Future research directions include devising efficient exploration strategies for DQNs to further enhance their performance and extending the theoretical analysis to variants of DQNs and policy gradient-based methods.

## 5 Numerical Experiments

In this section, we provide numerical justification that our theoretical findings are aligned with practical DQNs through the Atari Pong game, which is commonly used for DQNs in [50; 51; 73]. We take the Double DQN (DDQN) , one of the most popular variants of DQN, as the backbone in the setup. DDQN differs from DQN only in (8) via changing \(y=r+ Q(^{(t,0)};^{},a^{*})\) to \(y=r+ Q(^{(t,m)};^{},a^{*})\), where \(a^{*}=*{argmax}_{a}Q(^{(t,0)};^{},a)\). DDQN outperforms DQN in the relief of overoptimism and the improvement of stability. Our numerical experiments on DDQN also indicates that our analysis applies to the variants of DQN.

The input to the network is \(84 84 4\) images, where the last dimension represents the number of frames in history. The network is a convolutional neural network consisting of three convolutional layers and one fully-connected layer. The algorithm terminates if the average score over the recent \(20\) episodes does not improve or the algorithm reaches the maximum episode set as \(200\), which is around \(4 10^{5}\) training steps. The testing score is calculated based on a similar setup as the training process by fixing the maximum memory size \(N\) as \(2000\) and greedy policy, i.e., \(=0\). Each point in the plot is averaged over \(10\) experiments with an error bar representing the standard deviation.

**Estimation errors with respect to the sample complexity \(N\).** As the Q-value function is the estimate of the expected cumulative reward, we use the difference between the reward obtained from the estimated Q-value function and the maximum reward as the estimation error of the learned model to the optimal Q-value function, which is also consistent with the experiments in [50; 51; 73]. Given that the full test score in the Pong game is \(21\), we set the test error as the value of _(21 - test score)_ in each experiment. The \(_{t}\) in \(\)-greedy policy decreases geometrically from \(1\) to \(0.01\). We vary the number of samples in the replay buffer from \(400\) to \(2500\). Figure 1 shows that the test error is almost linear in \(1/\), which is consistent with our characterization in (20). In addition, experiments with a large \(N\) have a shorter error bar indicating a more stable learning performance with a large sample complexity as shown in (12).

**Convergence with different selections of \(\).** Figure 2 illustrates the convergence rate when \(_{t}\) in the \(\)-greedy policy changes. For each point, \(_{0}\) is selected as the value in the x-axis, and we decrease \(_{t}\) geometrically as the iteration \(t\) increases. Each point is averaged over \(5\) independent trials. We can see that the convergence rate is a linear function of \(c_{}\), matching our findings in (19).

**Performance with different selections of \(\).** We investigate the effect of \(\)-greedy policy during the training. In Figure 3, we show the test scores using \(\)-greedy policy with (1) geometrically decreasing \(\) from \(1\) to \(0.01\), (2) fixed \(\) as \(0.1\), and (3) fixed \(\) as \(0\). Each test score in the curve is averaged over the past \(10\) episodes to smooth the trend. One can observe that a gradually decreasing \(\) leads to a better score than fixing \(\) to be the final value. The test score of \(=0.1\) shares a similar trend to the \(\)-greedy policy but with a slower speed, matching our findings in (19) that a small \(\) leads to a slow convergence rate. The test score of \(=0\) has the fastest convergence rate at the early stage, but the convergent point is the worst and the most unstable, matching our findings in (18) that a small \(\) leads to a reduced radius of convergence.

## 6 Conclusions and Discussions

This paper provides the first convergence and sample complexity analysis of the DQN algorithm equipped with \(\)-greedy exploration. We establish the theoretical guarantee for the convergence of the learned model to the optimal Q-value function \(Q^{}\), which can be used to derive the optimal policy. We provide a nearly optimal sample complexity for achieving an arbitrarily small estimation error. We also prove that \(\)-greedy with decreasing \(\) achieves both an enlarged radius of convergence and an improved convergence rate. Future directions include the generalization of the theoretical analysis to the variants of DQNs and the design of efficient exploration strategies for DQNs.

One of the anonymous reviewers raised concerns about (12) regarding its demanding requirements on the initial policy. We would like to clarify that (12) primarily concerns the optimization analysis of the objective function rather than the initial policy. Instead, we impose only a minor assumption on the initial policy, with no specific environmental constraints. In our work, \(C_{0}\) quantifies the initial policy's difference from the optimal policy and is independent of (12) in our primary theoretical results. With a sufficiently large replay buffer, \(C_{0}\) can approach one, except when it equals 1, indicating an extreme divergence from the optimal policy in all states. Thus, our initial policy assumption is minimal. Considering the highly non-convex nature of deep neural network objective functions with countless local minima, (12) represents the state-of-the-art assumption for optimizing deep neural networks.

As mentioned by the anonymous reviewers, the selections of \(_{t}\) in (17) depends on \(e_{t}=\|^{(t)}-^{}\|_{2}\), which is unknown to the agent. Here, we would like to clarify that \(e_{t}\) can be replaced by its upper bound in (19) and lower bound in (20). By plugging (19) and (20) into (17), we have

\[_{t}=() +c_{}(1-)^{t}e_{0}}{(1-C_{}) || R_{}}-}{1-C_{}}, c_{ }}{1-C_{}}}.\] (26)

As mentioned by one of the anonymous reviewers, Corollary 3 relies on an assumption that depends on the algorithm's trajectory, which lacks mathematical rigor. Here, we would like to clarify that although this equation depends on the algorithm's trajectory, it can be easily derived from a time-independent equation

\[|_{}(|a)-^{}(|a)| C||-^{}|| _{2}.\] (27)

Additionally, it is worth mentioning the difference between (22) in this paper and (2) in . Specifically, we want to highlight that the equation above, (27), leads to (22). In contrast, (2) in  requires the following condition to hold for for all \(_{1}\) and \(_{2}\):

\[|_{_{1}}(|a)-_{_{2}}(|a) C||_{1}-_{2}||_{2}\] (28)

As a comparison, (27) only requires \(_{2}\) to be the ground truth and \(_{1}\) to be some weights near the ground truth. In other words, (27) is a sufficient condition for (22). While equation (2) in  does not hold with epsilon-greedy, (27) can hold with Q-learning using epsilon-greedy, thus ensuring the mathematical rigor of (22).