# Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient

Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient

 Shaoqi Wang\({}^{}\) Chunjie Yang\({}^{}\) Siwei Lou\({}^{}\)

\({}^{}\) Zhejiang University, China, {sq_w,cjyang999,swlou}@zju.edu.cn

###### Abstract

Neural networks (NN) are extensively studied in cutting-edge soft sensor models due to their feature extraction and function approximation capabilities. Current research into network-based methods primarily focuses on models' offline accuracy. Notably, in industrial soft sensor context, online optimizing stability and interpretability are prioritized, followed by accuracy. This requires a clearer understanding of network's training process. To bridge this gap, we propose a novel NN named the Approximated Orthogonal Projection Unit (AOPU) which has solid mathematical basis and presents superior training stability. AOPU truncates the gradient backpropagation at dual parameters, optimizes the trackable parameters updates, and enhances the robustness of training. We further prove that AOPU attains minimum variance estimation (MVE) in NN, wherein the truncated gradient approximates the natural gradient (NG). Empirical results on two chemical process datasets clearly show that AOPU outperforms other models in achieving stable convergence, marking a significant advancement in soft sensor field.

## 1 Introduction

Deep learning methods have achieved recent success in many regression areas such as natural language processing, protein structure prediction, and building energy consumption forecasting. However, for these methods to be useful in the industrial soft sensor field, which demands higher immediacy and stability, further research into model structure and the stability of the training process is necessary . The safety and economic impact of factory impose stringent requirements on soft sensor models deployed online . For example, each mini-batch update must not cause significant performance fluctuations to ensure that downstream controllers and monitors do not execute erroneous actions; soft sensor models must be deployed online to avoid fluctuations due to changes in operating conditions and model switching . Common network's training tricks are not suitable for soft sensor contexts. For example, it's not feasible to use checkpoints for early stopping but to always use the latest updated checkpoint; there wouldn't be adaptive learning rate changes but rather a constant learning rate maintained throughout. Experimental results demonstrate that such differences lead to a substantial decline in performance. These constraints necessitate the development of better-suited network architectures for regression task that ensure more stable optimization during training .

MVE is the best unbiased estimator under the Mean Squared Error (MSE) criterion, essentially representing the performance ceiling for regression models . Unfortunately, directly applying MVE to NN is challenging due to the difficulty in obtaining the likelihood distribution \(p(y|x)\) of inputs \(x\) and outputs \(y\). Traditional research on MVE has focused on techniques like Kalman filtering , algorithms based on Ordinary Least Squares (OLS) , and other system identification research  which operate under linear and convex conditions. These methods, while effective within their scope, have limited expressive power .

Many studies have explored NN optimization from various perspectives such as adaptive learning rates , momentum updates  and customized loss functions . Compared to the first-order optimization methods, second-order optimization algorithms based on Natural Gradient Descent (NGD) [21; 22] can achieve faster and more stable convergence . This is because NGD considers the manifold distribution of model parameters by computing the Fisher Information Matrix (FIM) during the gradient update process. However, calculating FIM introduces significant computational overhead, making NGD challenging to implement in most machine learning models . Much research is focused on adapting model structures for NGD [25; 26; 27] and reducing its computational costs [28; 29; 30], yet applying NGD to NN optimization remains an unsolved issue [31; 32; 33].

Some studies have approached the regression task from the perspective of targeted modular design [34; 35; 36], emphasizing the construction of local neuron-level rules to assist models in learning conducive features, exemplified by SLSTM , SIAE , MIF-Autoformer , and CBMP . Some endeavors have yielded results with solid theoretical underpinnings [41; 5; 42], such as S4 , VAE, VIOG . These examples, integrate with other great work [27; 45; 42; 46], jointly demonstrate excellent integrations of NN with theoretical basis and also have a stronger expressivity compared to the identification algorithms [14; 15; 47]. However, even though these networks possess certain interpretability, it is still unclear whether these prior biases are beneficial for regression task [48; 49]. Furthermore, their mathematical foundations are not rooted in soft sensor tasks, which do not guarantee stable performance in regression .

AOPU differs from conventional studies by focusing on training optimization and the overall input-output relationships. Assuming there is a robust feature extraction module (augmentation block), AOPU pays specific attention on better optimization and more stable convergence. AOPU innovatively introduces trackable and dual parameters, enhancing an structure approximation of MVE. The dual parameters are injective representations of trackable parameters, mainly aiding in truncating the gradient backpropagation process. This truncation will be validated as an effective approximation to NGD. The augmentation module boosts AOPU's performance and also acts as an extension interface, making AOPU more versatile. Rank Ratio (RR) is introduced as an interpretability index to provide deep and comprehensive insights into the network dynamics. RR quantifies the ratio of linearly independent samples within a batch, providing a measure of the heterogeneity and diversity of information. By harnessing RR value, we can roughly foresee the performance in advance of the training. A high RR suggests the model output more closely approximates the MVE, and optimization is more in line with NGD, leading to superior performance. Conversely, a low RR implies that the precision of computation is compromised, resulting in inferior performance.

## 2 AOPU:Methodology

### Trackable vs. Untrackable

Parameters within NN that can be decoupled from input data \(x\) and computed through an inner product are defined as **trackable** parameters. Conversely, parameters that cannot be decoupled are classified as **untrackable** parameters.

\[f(x)=W^{T}x= W,x g(x)=M(x)^{T}x= M(x),x\] (1)

Figure 1: Trackable parameters and Untrackable parameters. Solid green lines represent model parameters, and orange curves represent non-parametric operations. (a) Conventional deep NN framework. (b) Typical broad learning system framework through data enhancement.

where \(W\) represents a parameter matrix and \(M()\) denotes an operator dependent on \(x\). According to the definition, \(W\) is identified as a trackable parameter, whereas \(M(x)\) is considered untrackable.

A significant proportion of parameters in NN are untrackable as depicted in Fig. 1. This predominance is attributable to the networks' reliance on stacking activation functions to bolster their nonlinear modeling capabilities. Proposition 1 indicates that any parameter influenced by an activation function transitions to being input-dependent, thus rendering it untrackable.

**Proposition 1**.: _There **does not** exist an transition operator \(T\) independent of \(x\) such that for a given parameter matrix \(W\), and \( x_{1},x_{2}\), the following equations hold,_

\[(Wx_{1})=T(W)x_{1},(Wx_{2}) =T(W)x_{2}\] (2)

_Proof is in Appendix C.3._

### Natural Gradient vs. Gradient

Fig. 2 vividly presents the major difference between NGD and GD using a simple GPR in experiment. This GPR had only two parameters, the bias of the mean and the coefficient of the kernel matrix, both constant values. We sampled 100 instances from this GPR and updated these two parameters 100 times using these samples. It was observed that NG require a higher learning rate, while conventional gradients only need a smaller one. The major difference between NG and conventional gradients lies in their directions. Conventional gradients ignore the parameter manifold and treat every parameter equally. NG, by dividing the gradient by its second derivative, treat sensitive parameters cautiously (low gradient) and non-sensitive parameters boldly (high gradient). This adjustment results in different gradient directions and contributes to better convergence.

Nevertheless, the calculation of NG involves considering the inverse of the FIM, thereby introducing computational complexity cubic to the number of parameters, making it infeasible for neural networks. Existing research on NG focuses on conventional machine learning, e.g., considering more complex distributions (such as the product of multiple exponential family) for computing NG. Research in NN domain on NG mostly centers on second-order optimizers (such as AdamW), which are merely first-order approximations of second-order NG.

### Network's structure

**We wish to emphasize that within AOPU framework, the truncated gradient of the dual parameters serves as an approximation of the NG of the trackable parameters, while AOPU's structured output approximates the MVE.** The fidelity of these approximations is measured by the RR, the closer RR is to 1, the more precise the approximation; conversely the closer RR is to 0, the more precision loss occurs. Furthermore, it can be demonstrated that the output of AOPU fundamentally differs from traditional neural networks: instead of explicitly modeling a mapping from \(x\) to \(y\), it implicitly models a mapping from \(x\) to \(y\). To ensure that \(y\) can be recovered from \(y\), it is imperative that RR equals 1. AOPU also guarantees the convergence of the dual parameters if the input-output relationship can be characterized by specific system. The proof of above is intricate and comprehensive, and one may refer to Appendix A, B and C for more detailed information. This section focuses on the implementation of AOPU.

AOPU utilize data augmentation to replace stacked activation structures to enhance the nonlinear modeling capabilities. In such designs, the choice of the data augmentation module forms a crucial

Figure 2: Comparison between NGD and GD. Direction matters more than step size (learning rate) in stable convergence.

model prior. For ease of implementation, AOPU adopts a random weight matrix approach for its data augmentation module . Specifically, suppose the original feature dimension of the input data is \(d\), and the defined model hidden dimension size is \(h\). Let \(\) be a fixed Gaussian initialized random weight matrix, \(^{d,h}\), and for input data \(x^{d,b}\) where \(b\) is the mini-batch size, the data augmentation process is as follows,

\[=[(^{T}x),x]\] (3)

Subsequently, for the augmented \(\), the output is processed using the trackable parameter \(^{(d+h),o}\), where \(o\) represents the output dimension and, for simplicity, we assume \(o=1\) indicating research into univariate output. The output function is then,

\[g(|x)=^{T}\] (4)

The optimization of parameter \(\) in AOPU differs from other networks. We introduce a dual parameter \(D()\) to describe this process,

\[D()=^{T}\] (5)

and the loss is computed using the following objective function,

\[=_{i=1}^{b}y_{i}-[(^{T})^{- 1}^{T}D()]_{i}^{2}\] (6)

AOPU innovatively introduces trackable and dual parameters. The dual parameters are injective representations of trackable parameters, mainly aiding in truncating the gradient backpropagation process. During the training process, gradient backpropagation is truncated at \(D()\), and the gradient of the dual parameter updates the original trackable parameter as demonstrated in Fig. 3, thus completing the training of the model.

It is important to note that the training process involves the inversion of a matrix \(^{T}\), which is not always invertible, thus introducing numerical computation issue. We employ the reciprocals of the positive singular values to circumvent the solvability issues that arise when an inverse does not exist. However, this approach introduces significant computational precision loss, which in turn prompts a thorough analysis of RR.

We define the metric RR to represent the ratio of the rank of \(\) to its batch size. Clearly, RR is a value between \(\), and as RR approaches 1, the process of approximating the inverse of \(^{T}\) through eigenvalue decomposition becomes more accurate (owing to the presence of more reciprocals of eigenvalues). When RR equals 1, \(^{T}\) is an invertible matrix; conversely, the smaller the RR, the less stable the model's numerical computations and likely poorer performance.

AOPU utilizes parameters' trackability nature to accelerate the NG computation. The key to AOPU's capability to rapidly approximate the NG is its ability to bypass the FIM computation and its inverse. The key to the capability to skip the FIM lies in our utilization of Eq. 10, which separates the model parameters from the variables and data. In this context, we can use the gradient matrix of the expectation parameter with respect to natural parameter to replace the FIM (without actually performing this calculation), thereby substituting the original complicated NG computation with an equivalent conventional gradient computation, i.e., \(_{}m=F()_{m}=(F())^{-1}\), so

Figure 3: AOPUâ€™s data flow schematic. The gradient is backpropagated but truncated at the dual parameter, and this gradient is then used to update the trackable parameter.

that \((F())^{-1}_{}_{m}\). This allows us to use the automatic differentiation toolbox for rapid calculations. Otherwise we must explicitly compute the inverse of the network's FIM, which is very time-consuming and memory-intensive. For instance, a network with 20kB of 32-bit parameters, which equates to 5120 trainable parameters, requires inverting a 5120-dimensional matrix with each training iteration. This requirement grows with model size and can easily lead to GPU memory shortages. More critically, such large matrix inversions often lead to significant numerical precision loss, severely impairing model performance.

## 3 Experiments and Analysis

In this section, we detail the experimental results of the AOPU model, analyze the impact of hyperparameters on AOPU, its robustness regarding changes in hyperparameters, its advantages over other comparative algorithms, and some inherent limitations of the model. Comprehensive and detailed experiments and comparisons have been conducted on two publicly available chemical process datasets, Debutanizer and Sulfur Recovery Unit (SRU). For more information of the dataset please refer to Appendix D.

### Baselines

We choose seven different NN models as baselines: Autoformer, Informer, DNN, SDAE, SVAE, LSTM, and RVFLNN, covering four major domains including RNN-based networks, auto-encoder-based networks, attention-based networks, and MLP-based networks. Notably, all baseline models except RVFLNN and AOPU operate solely within the latent space, meaning that there are linear transformations mapping the input data to the latent space and from the latent space to the output space before and after the baseline models. This approach is designed to better control the model size.

### Experiment Implementation

Apart from AOPU, which is trained using the approximated minimum variance estimation loss function as previously described, all other deep learning algorithms are trained using the Mean Squared Error (MSE) loss. AOPU's learning rate for gradient updates is set at 1.0, while for all other deep learning algorithms, it is set at 0.005, with the Adam optimizer used for gradient updates. The learning rates of all models remain static throughout the training process. The experimental setup differs based on the requirements of various models regarding input dimensions. Models such as Autoformer, Informer, and LSTM necessitate an input that includes an additional dimension for'sequence length'. This dimension is preserved as part of the input structure for these models. Conversely, models like DNN, SDAE, SVAE, AOPU, and RVFLNN do not require this additional dimension. For these models, the sequence length and input dimensions are combined and flattened to serve as the feature dimensions in the input. AOPU's latent space size is set at 2048. Autoformer, Informer, SDAE, and SVAE utilize two layers each for their encoder and decoder layers; LSTM uses two layers of LSTM layers; RVFLNN and AOPU share identical settings. All models except AOPU and RVFLNN have their latent space sizes set at 16 to ensure the trainable parameters size across all models are comparable.

### Main Result

#### 3.3.1 How certain we are about the inverse

According to the previous discussion, the existence of the inverse of \(^{T}\) is crucial as it does not only impact the numerical stability of the model but also directly determines whether it is possible to recover \(y\) from the approximated mapping relationship \(y\). Clearly, the input feature dimensions \(d+h\) and the batch size \(b\) significantly affect whether the inverse of \(^{T}\) exists. Specifically, the larger the batch size, the more columns \(\) has, and the less likely \(\) is to be column-full-rank; conversely, the longer the sequence length and the larger the input feature dimensions, the more likely \(\) is to be linearly independent and thus column-full-rank. From the following experimental results, it will be clearly observed the impact of batch size and sequence length on RR.

Fig. 4 shows the distribution of the RR for the AOPU model across various batch sizes and sequence length combinations on the SRU dataset, where **bs** stands for batch size and **seq** for sequence length. The experimental results align with the previous analysis: increasing the batch size with a fixed sequence length significantly decreases the RR distribution, whereas increasing the sequence length with a fixed batch size significantly increases it.

Fig. 5 shows the mean values of RR distribution changing with sequence length under different batch size settings, marked by red circles at every ten data points. Clearly, the experimental results shown in the figure corroborate our analysis that with increasing batch size, the curve's slope becomes flatter, indicating the model's decreasing sensitivity to changes in sequence length. Compared to Fig. 4, Fig. 5 provides additional insights into the mean values of the RR distribution relative to sequence length and batch size, offering a more comprehensive insight for subsequent experimental interpretations. Results of the RR study on the Debutanizer are listed in Appendix F.

#### 3.3.2 Is the training stable

Stability is a crucial characteristic for the online deployment of deep learning models in actual production processes. Specifically, the incremental updates to model parameters following the observation of new mini-batch data should have a smooth impact on model performance. However, experimental results indicate that most networks in the soft sensor field fail to achieve stable convergence. Fig. 6 provides a detailed display of how the MSE metrics for different networks change with training iterations on the SRU validation dataset, with blue solid circles marked every 50 iterations. It is evident that all models, except Autoformer, Informer, LSTM, and AOPU, exhibit significant performance fluctuations as training iterations progress. The density of the blue solid circles can to some extent represent the likelihood of corresponding performance fluctuations.

It can be observed that the SDAE and SVAE networks, despite experiencing significant fluctuations in validation performance (indicated by large fluctuations in the curve), are mostly stable (as shown by the blue circles concentrated below the curve). In contrast, the DNN and RVFLNN networks

Figure 4: Histogram of the frequency distribution of RR on SRU dataset under varying batch sizes and sequence length settings.

Figure 5: Curve of the mean of RR distribution on SRU dataset under varying batch sizes and sequence length settings.

have relatively unstable convergence (indicated by blue circles evenly distributed above and below the curve). Although Autoormer and Informer have relatively stable convergence dynamics, their performance is relatively poor. Specifically, Autoormer consistently converges to a bad output, whereas Informer can effectively learn under identical settings but is sensitive to changes in seq, which can lead to model performance collapse. The convergence process of LSTM is relatively stable, partly explaining why it is a widely adopted baseline in the field of time series analysis; however, LSTM is significantly prone to overfitting and its performance is not outstanding.

In contrast to all other network, AOPU exhibits exceptionally impressive performance. AOPU demonstrates very stable and rapid convergence, with almost no fluctuations in performance as training iterations progress. Furthermore, AOPU is less sensitive to changes in hyperparameters and does not exhibit significant overfitting, making it a truly reliable and deployable NN model in production processes. We also notice that with bigger batch size setting, the fluctuations will be mitigated. For comparative training dynamics under other batch size settings, refer to Appendix F.

#### 3.3.3 Quantitative analysis

To verify the reliability of the AOPU model's performance and to quantify its comparison with other methods, we implemented two different training strategies. Strategy one involved an early stopping trick and used the best checkpoint to validate the model's performance on the test dataset. Strategy two involved training all models for 40 epochs and using the final checkpoint to test the model performance. All following experiments has batch size set to 64. The outputs of strategy one are presented at Table 1, while the results of strategy two are presented in Table 2. All NN configurations were subjected to 20 independent repeat experiments, with the mean of the experiments represented by uppercase numbers on the left of the table and the standard deviation by lowercase subscript numbers on the right.

From Table 1, we can intuitively compare the optimal performance among all models. Overall, there is not much difference in final performance among the various networks. Notably, almost all MAPE metrics on the Debutanizer dataset exceed 100 due to a sample in the test dataset where the butane content is nearly zero, which significantly distorts the MAPE calculation. While AOPU performs comparably to other network models in terms of the R\({}^{2}\) metric, its stability is significantly superior, as indicated by much lower standard deviations in the R\({}^{2}\) values compared to all other models.

Further, to more closely align with real industrial application scenarios, if we do not record the optimal checkpoint but instead complete training for 40 epochs as shown in Table 2, the performance of the models significantly declines. Despite this, AOPU continues to provide stable and reliable performance. In Table 2, it is noted that using the R\({}^{2}\) metric, AOPU consistently performs best with the sequence length set at 48, and the performance drop from the optimal results calculated using

Figure 6: Curves of SRU validation loss changes with training iteration for different models with a fixed batch size of 64 at different sequence length settings. The curves are shown in translucent blue, with a solid blue circle labeled on the curve every 50 iterations.

[MISSING_PAGE_FAIL:8]

0.0094 on the Debutanizer dataset, an increase of about 8.0%. In contrast, the standard deviation for other networks often increases several-fold, such as Autoformer on the Debutanizer dataset, which increases from an optimal 0.0481 to 0.3287, an increase of about 583.4%; similar trends are observed with other models.

### Ablation Study

In this section, we further investigate the effects of structural designs for augmentation through some ablation studies, examining the impacts of the ReLU piecewise activation function, the Tanh smooth activation function, and normalization on AOPU's performance. It is important to note that if AOPU is trained using direct gradient descent without dual parameter updates, it actually degenerates to an RVFLNN model, and this part of the ablation study has been detailed in section 3.3.3.

From Table 3 we can draw two conclusions: The first is normalization significantly impairs AOPU's model performance. The second it ReLU piecewise non-linear activation function suits worse for AOPU than the Tanh activation function. As previously analyzed in A where both the input data \(\) and \(y\) should to be zero mean, hence reducing the covariance operator R to an inner product operator. However, piecewise linear functions like ReLU and LeakyReLU are not zero-mean, which violates such assumptions.

   &  \\   &  &  & ^{2}\)} &  & ^{2}\)} \\  & & & & & & & & \\  & & MSE & & & & & & \\   &  & 0.0942\({}_{+0.0370}\) & 217.01\({}_{+78.49}\) & -1.5240\({}_{+0.9912}\) & 0.00836\({}_{+0.00426}\) & 0.5362\({}_{+0.1755}\) & -1.118\({}_{+0.1810}\) \\  & & Informer & 0.0283\({}_{+0.0070}\) & 140.8\({}_{+0.306}\) & 0.2414\({}_{+0.1885}\) & 0.00122\({}_{+0.00036}\) & 0.1750\({}_{+0.0226}\) & 0.6897\({}_{+0.0918}\) \\  & & DNN & 0.0215\({}_{+0.0028}\) & 161.8\({}_{+6.14.1}\) & 0.4233\({}_{+0.0760}\) & 0.00172\({}_{+0.00073}\) & 0.2238\({}_{+0.0140}\) & 0.5639\({}_{+0.1850}\) \\  & & SDAE & 0.0217\({}_{+0.0036}\) & 144.6\({}_{+4.52}\) & 0.4184\({}_{+0.0977}\) & 0.00113\({}_{+0.00017}\) & 0.1559\({}_{+0.0211}\) & 0.7114\({}_{+0.0453}\) \\  & & SVAE & 0.0217\({}_{+0.0061}\) & 161.6\({}_{+53.53}\) & 0.4166\({}_{+0.1644}\) & 0.00113\({}_{+0.00017}\) & 0.1695\({}_{+0.0329}\) & 0.7121\({}_{+0.1131}\) \\  & & LSTM & 0.0521\({}_{+0.0152}\) & 215.5\({}_{+1.52}\) & 0.37\({}_{+0.3956}\) & 0.00925\({}_{+0.0011}\) & 0.2053\({}_{+0.025}\) & 0.3634\({}_{+0.2844}\) \\  & & AOPU & 0.0215\({}_{+0.0007}\) & 206.6\({}_{+9.059}\) & 0.4239\({}_{+0.0211}\) & 0.00098\({}_{+0.0013}\) & 0.1963\({}_{+0.0132}\) & 0.7518\({}_{+0.0336}\) \\  & & RVFLNN & 0.0329\({}_{+0.0391}\) & 107.1\({}_{+0.42}\) & 0.1171\({}_{+1.0470}\) & 0.00171\({}_{+0.0112}\) & 0.2540\({}_{+0.0267}\) & 0.5652\({}_{+0.2853}\) \\   &  & 0.0969\({}_{+0.0293}\) & 331.5\({}_{+11.37}\) & -1.5980\({}_{+0.7864}\) & 0.00457\({}_{+0.0145}\) & 0.3901\({}_{+0.0764}\) & 0.1590\({}_{+0.0862}\) \\  & & Informer & 0.0222\({}_{+0.0053}\) & 134.9\({}_{+29.93}\) & 0.0407\({}_{+0.1432}\) & 0.00146\({}_{+0.00047}\) & 0.1878\({}_{+0.0280}\) & 0.6280\({}_{+0.1201}\) \\  & & DNN & 0.0204\({}_{+0.0036}\) & 159.7\({}_{+27.21}\) & 0.4518\({}_{+0.0977}\) & 0.00281\({}_{+0.0126}\) & 0.2760\({}_{+0.0699}\) & 0.2879\({}_{+0.3206}\) \\  & & SDAE & 0To validate the analysis regarding the effects of zero-mean and non-zero-mean activation functions on AOPU's performance, an additional comparative experiment was conducted. This experiment included 20 independent repetitions for activation functions classified into zero-mean, Hard Shrink, Tanh, Tanh Shrink, Soft Sign, and Soft Shrink, and non-zero-mean, Sigmoid, Relu6, RRelu, Hardswish, and Mish. The results are listed in Table 4

The experimental results largely confirmed the hypotheses outlined previously. In the zero-mean group, whether on the Debutanizer or SRU dataset, fluctuations in MSE, MAPE, and R\({}^{2}\) metrics were consistently controlled within 1% (with the maximum R\({}^{2}\) fluctuation being 0.48%, from 0.7236 to 0.7201). Conversely, in the non-zero-mean group, Sigmoid, Relu6, and RRelu all demonstrated notable performance declines on the Debutanizer dataset. Notably, although Hard Swish and Mish are classified as non-zero-mean activation functions, they did not negatively impact AOPU's performance. This could likely be attributed to the fact that Hard Swish and Mish are approximately zero-mean near the zero index, unlike Sigmoid, Relu6, and RRelu, which are non-zero-mean across any arbitrary small neighborhoods.

## 4 Conclusion and Limitation

This paper introduces a novel NN regression model, AOPU, which is grounded in solid mathematics basis and validated through extensive experiments. The results demonstrate its superior performance, robustness, and training stability. The development of AOPU lays the foundation for the practical implementation of deep learning soft sensor techniques in industrial processes and provides guidance for subsequent control, monitoring, and optimization management of these processes. The introduction of RR also illuminates a promising and valuable direction for exploring the design of augmentation models. Such prospective topics of value encompass how to reduce the sensitivity of AOPU to batch size and sequence length, how to derive the NG optimization of the augmentation model, and how to bolster the nonlinear modeling capability of the augmentation model.

We note that AOPU is not a "plug-and-play" model; it requires adjustments based on actual data conditions. AOPU necessitates a clear understanding of the RR distribution of data intended for application to guide the selection of batch size and sequence length hyperparameters. This requirement stems from the inherent matrix inversion operations in AOPU. When the RR value is too low, noise during the AOPU training process can greatly exceed the effective information, potentially leading to model divergence as Appendix F discusses.