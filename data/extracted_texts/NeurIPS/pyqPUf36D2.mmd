# Pseudo-Private Data Guided Model Inversion Attacks

Xiong Peng\({}^{1}\) Bo Han\({}^{1}\) Feng Liu\({}^{2}\) Tongliang Liu\({}^{3}\) Mingyuan Zhou\({}^{4}\)

\({}^{1}\)TMLR Group, Department of Computer Science, Hong Kong Baptist University

\({}^{2}\)School of Computing and Information Systems, The University of Melbourne

\({}^{3}\)Sydney AI Centre, The University of Sydney

\({}^{4}\)McCombs School of Business, The University of Texas at Austin

{csxpeng, bhanml}@comp.hkbu.edu.hk

fengliu.ml@gmail.com &tongliang.liu@sydney.edu.au &mingyuan.zhou@mccombs.utexas.edu

Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).

###### Abstract

In _model inversion attacks_ (MIAs), adversaries attempt to recover the private training data by exploiting access to a well-trained target model. Recent advancements have improved MIA performance using a two-stage generative framework. This approach first employs a _generative adversarial network_ to learn a fixed distributional prior, which is then used to guide the inversion process during the attack. However, in this paper, we observed a phenomenon that such a _fixed prior_ would lead to a low probability of sampling actual private data during the inversion process due to the inherent distribution gap between the prior distribution and the private data distribution, thereby constraining attack performance. To address this limitation, we propose increasing the density around high-quality pseudo-private data--recovered samples through model inversion that exhibit characteristics of the private training data--by slightly tuning the generator. This strategy effectively increases the probability of sampling actual private data that is close to these pseudo-private data during the inversion process. After integrating our method, the generative model inversion pipeline is strengthened, leading to improvements over state-of-the-art MIAs. This paves the way for new research directions in generative MIAs. Our source code is available at: https://github.com/tmlr-group/PPDG-MI.

## 1 Introduction

Currently, _machine learning_ (ML) models, especially _deep neural networks_ (DNNs), have become prevalent in privacy-sensitive applications such as secure systems (Yin et al., 2020), personal chatbots (Ouyang et al., 2022) and healthcare services (Murdoch, 2021). These applications inevitably rely on private and confidential datasets during model training, raising concerns about potential privacy leakages (Liu et al., 2021). Unfortunately, recent studies reveal that ML models are vulnerable to various privacy attacks (Fredrikson et al., 2014; Krishna et al., 2019; Choquette-Choo et al., 2021). _Model inversion attacks_ (MIAs), a category of these attacks, pose significant privacy risks, which aim to infer and recover original training data by exploiting access to a well-trained model.

In the pioneering work (Fredrikson et al., 2015), MIAs were formulated as a gradient-based optimization problem in the raw data space. The goal was to seek synthetic features that maximize the prediction score for a targeted class under the target model, exploiting the strong dependency between inputs and labels established during training. For example, in attack scenarios where the target model is a facial recognition model trained on private facial images, traditional MIAs would optimize over the synthetic images to maximize the prediction score for a target identity.

[MISSING_PAGE_FAIL:2]

inversion process. We argue that this approach is sub-optimal for MIAs and introduce a novel strategy, termed _pseudo-private data guided_ MI, to mitigate this limitation, thereby paving the way for future research and advancements in generative MIAs.
* Technically, we provide multiple implementations of PPDG-MI to validate the effectiveness of our proposed strategy. For low-resolution MIAs, we introduce PPDG-vanilla. For more complex high-dimensional MIAs, we offer PPDG-PW, which employs point-wise tuning, and two batch-wise tuning strategies: PPDG-MI with conditional transport (PPDG-CT) and PPDG-MI with maximum mean discrepancy (PPDG-MMD) (Sec. 3).
* Empirically, through extensive experimentation, we demonstrate that our solution significantly improves the performance of the SOTA MI methods across various settings, including white-box, black-box, and label-only MIAs (Sec. 4). Our findings emphasize the increasing risks associated with MIAs and further highlight the urgent need for more robust defenses against the leakage of private information from DNNs.

## 2 Problem Setup and Preliminary

### Model Inversion Attacks

**Problem Setup.** Let \(^{d_{X}}\) be the feature space, and \(_{}=\{1,,C\}\) be the private label space. The _target model_, \([0,1]^{C}\), is a classifier well-trained on the private training dataset \(_{}\) sampled from \((_{},_{})\). In standard settings, for a specific class \(y\) in \(_{}\), MIAs aim to reconstruct synthetic samples by exploiting access to the target model \(\) to uncover sensitive features of class \(y\). In this context, the adversary is limited to querying \(\), and also possesses knowledge of the target data domain but lacks specific details about \(_{}\).

Mathematically, MI is formulated as an optimization problem: Given a target class \(y\), the goal is to find a sample \(\) that maximizes the model \(\)'s prediction score for class \(y\). In high-dimensional data settings, traditional MIAs (Fredrikson et al., 2015) use direct input space optimization, often leading to adversarial samples (Szegedy et al., 2014) that, despite high prediction scores, lack meaningful features. To mitigate this issue, Zhang et al. (2020) propose a generative MI approach, which learns a distributional prior to constrain the optimization to a low-dimensional, meaningful manifold.

Current generative MIAs primarily concentrate on either the initial training process of GANs (Chen et al., 2021; Yuan et al., 2023; Nguyen et al., 2024) or the optimization techniques used in the attacks (Zhang et al., 2020; Wang et al., 2021; Struppek et al., 2022; Kahla et al., 2022; Nguyen et al., 2023). In this paper, we take another direction and introduce a novel approach by fine-tuning the GAN's generator based on the attack results from previous runs. This method introduces a dynamic and iterative dimension to model inversion attacks, expanding the current understanding and research direction of generative MIAs. For detailed related work, please refer to Appx. A.1.

Specifically, the generative MI approach consists of two stages. Initially, a GAN learns a prior from public auxiliary datasets, in which \(_{}_{}=\). This process involves a generator, denoted as \((;)_{}\), parameterized by \(\), that transforms a low-dimensional latent code, \(\), into a high-dimensional image, \(_{}\). Concurrently, a discriminator \((;)\), which can distinguish between generated and real images. Subsequently, the MI optimization can be constrained to the latent space \(\) of the _fixed prior_\(\), which can be formulated as:

\[^{*}=*{arg\,min}_{}\;_{}( ;y,,)+_{}(;,),\] (1)

where \(_{}()\) denotes the identity loss, _e.g._, the cross-entropy loss \(-_{}(y|(z))\), which optimizes for an optimal synthetic sample \(^{*}=(^{*})\). Additionally, \(_{}()\) serves as a regularizer for the latent code \(\), and the parameter \(\) balances the trade-off between the identity loss and the regularizer.

### Distribution Discrepancy Measure

To effectively align distributions in our methods, it is essential to introduce metrics that can accurately quantify the differences between them. Two commonly used measures for this purpose are _maximum mean discrepancy_ (MMD) and _conditional transport_ (CT). MMD focuses on mean differencesusing kernel methods, while CT incorporates cost-based transport distances, offering complementary perspectives on distributional discrepancies. This section introduces the empirical estimation of MMD and CT. For more details on these discrepancy measures, please refer to Appx. A.2.

**Estimation of MMD.** Given distributions \(\) and \(\), and sample sets \(S_{X}=\{_{i}\}_{i=1}^{n}\) and \(S_{Y}=\{_{j}\}_{j=1}^{m}\), MMD can be estimated with the following estimator [Gretton et al., 2012b]:

\[}_{u}^{2}(S_{X},S_{Y};k) =_{i=1}^{n}_{j=1,j i}^{n}k(_ {i},_{j})+_{i=1}^{m}_{j=1,j i}^{m}k( _{i},_{j})\] \[-_{i=1}^{n}_{j=1}^{m}k(_{i}, _{j}).\] (2)

where \(k\) is a kernel function, \(_{i},_{j} S_{X}\) and \(_{i},_{j} S_{Y}\).

**Estimation of CT.** Similarly, for sample sets \(S_{X}=\{_{i}\}_{i=1}^{n}\) and \(S_{Y}=\{_{j}\}_{j=1}^{m}\), the CT measure can be approximated as follows [Zheng and Zhou, 2021]:

\[(S_{X},S_{Y})=_{i=1}^{n}_{j=1}^{m}c(_{i},_{j})(}(_{i},_{j})}}{ _{j^{}=1}^{m}e^{-d_{}(_{i},_{j ^{}})}}+}(_{i},_{j})}} {_{i^{}=1}^{n}e^{-d_{}(_{i^{}}, _{j})}}).\] (3)

Here, \(d_{}(,)\) is a function parameterized by \(\) that measures the similarity between \(\) and \(\), and \(c(,)\) is a cost function that measures the distance between the points \(\) and \(\).

## 3 Pseudo-private Data Guided Model Inversion

This section introduces our proposed methodology, _i.e._, _pseudo-private data guided_ MI (PPDG-MI). First, we present and discuss the critical motivation that inspires our method (Sec. 3.1). Second, we introduce the general framework of PPDG-MI (Sec. 3.2). Third, to ease understanding, we demonstrate and illustrate the rationality of our solution on a simple toy dataset (Sec. 3.3). Fourth, we present a more nuanced and detailed strategy for tuning the generator to enhance density in high-dimensional image spaces, accompanied by multiple algorithmic implementations (Sec. 3.4).

### Motivation: Effect of Distribution Discrepancies on MIAs

Collecting public auxiliary datasets that closely resemble the private dataset remains challenging. This difficulty arises because the MI adversary lacks knowledge of specific class information, and only understands the general data domain about \((_{})\). Thus, we hypothesize a significant distribution discrepancy between the prior distribution \((_{})\) and the private data distribution \((_{})\). This claim is supported by Fig. 1(a), where we quantify the distribution discrepancy between commonly adopted public auxiliary datasets and private training datasets using the MMD measure [Borgwardt et al., 2006], showcasing a substantial gap between these two distributions.

To evaluate the impact of this distribution discrepancy on MI performance, we create a series of proxy prior distributions through linear interpolation, where a mixing coefficient \(\) determines the proportion of samples drawn from each distribution. Specifically, a fraction \(\) of samples is drawn from \((_{})\), and the remaining \((1-)\) is drawn from \((_{})\). This process is represented as:

\[(_{}^{})=(_ {})+(1-)(_{}).\] (4)

We apply these proxy prior distributions to constrain the MI optimization as outlined in Eq. (1). As illustrated in Figs. 1(b) and 1(c), the MI performance decreases monotonically as the MMD value between \((_{}^{})\) and \((_{})\) increases. This leads us to pose a critical research question:

_How can the discrepancy between the prior distribution and the unknown private data distribution be mitigated to enhance MI performance?_

In response to this, a revised inversion pipeline is required, wherein \(\) is dynamically adjusted throughout the inversion process. This adjustment aims to progressively narrow the distribution gap between \((_{})\) and \((_{})\), thereby potentially improving MI performance.

### PPDG-MI Framework

This section presents a novel model inversion pipeline that dynamically adjusts the generator \(\) to mitigate the distribution discrepancy between \((_{})\) and \((_{})\) (_cf._ Fig. 2 for the framework overview). As aforementioned, in the MI context, while the specific details of \(_{}\) remain unknown, we have access to the target model \(\), which is well-trained on \(_{}\), still encapsulates information about \(_{}\). Therefore, by conducting MI on the target model \(\), we can generate a set of pseudo-private samples (_i.e.,_ reconstructed samples), denoted as \(_{}^{}\), which reveal the characteristics of the private dataset \(_{}\) and can serve as its surrogate. Thus, the key insight is that by enhancing the density of the prior distribution \((_{})\) around \(_{}^{}\), we indirectly increase the density around \(_{}\) as well. Consequently, the probability of sampling data from \((_{})\) could be increased. This strategy is termed _pseudo-private data guided_ MI (PPDG-MI).

To this intuition, the proposed MI framework consists of the following three iterative steps:

**Step-1: Pseudo-private Data Generation by Conducting MI on the Target Model.** Specifically, in generative MI, optimization is restricted to the latent space \(\). Initially, we sample a set of latent codes, \(=\{_{i}_{i},i=1,,N\}\). Then, by leveraging Eq. (1), these initial latent codes are optimized to produce \(}=\{}=_{}()+_{}()\}\). Subsequently, this optimized set \(}\) is utilized to generate a pseudo-private dataset \(_{}^{}=\{}=(})}}\}\).

**Step-2: Selection of High-Quality Pseudo-private Data.** In this step, we aim to select high-quality data from \(_{}^{}\) that closely resemble the characteristics of samples in \(_{}\), serving as their proxy. An intuitive method is to select samples with high prediction scores. Thus, following Struppek et al. (2022), we opt to select samples with larger expected prediction scores \([_{}(y|T(}))]\) under random image transformations \(\,T\), indicating that \(}\) represents the desired characteristics for target class \(y\) more accurately. Specifically, we select a high-quality subset \(_{}^{}\), consisting of samples with top \(K\) expected prediction scores from \(_{}^{}\).

**Step-3: Density Enhancement around Pseudo-private Data.** In this step, we focus on fine-tuning \(\) to adjust the prior distribution \((_{})\), aiming to increase the probability of sampling data from \((_{})\). In the existing literature, MIAs can be categorized into two types: those targeting high-resolution tasks (Struppek et al., 2022) and those targeting low-resolution tasks (Zhang et al., 2020; Chen et al., 2021; Kahla et al., 2022; Nguyen et al., 2023). In high-resolution MIAs, adversaries leverage _pre-trained_ GANs without access to training specifics. In contrast, low-resolution MIAs involve adversaries training GANs from scratch using the public auxiliary dataset \(_{}\). This distinction enables the development of tuning strategies tailored to different attack settings.

For MIAs focusing on low-resolution tasks, where the generator \(\) is less powerful and the low-resolution image manifold is more susceptible to disruption, we adopt a principled tuning strategy. Specifically, we fine-tune \(\) and \(\) using the original GAN training objective on \(_{}_{}^{^{}}\), a strategy termed _PPDG-vanilla_ (_cf._ Alg. 1). For MIAs focusing on high-resolution tasks, _e.g., Plug

Figure 2: **Overview of traditional generative MI framework vs. _pseudo-private data guided_ MI (PPDG-MI) framework.** PPDG-MI leverages pseudo-private data \(}\) generated during the inversion process, which reveals the characteristics of the actual private data, to fine-tune the generator \(\). The goal is to enhance the density of \(}\) under the learned distributional prior \((_{})\), thereby increasing the probability of sampling actual private data \(^{*}\) during the inversion process.

& Play Attacks_ (PPA) (Struppek et al., 2022), we propose a tuning strategy that leverages only the high-quality pseudo-private dataset \(^{s^{}}_{}\), which can be formalized as follows:

\[,(,, ^{s^{}}_{}).\] (5)

This adjustment aims to increase the density of the prior distribution around \(^{s^{}}_{}\). The concrete realizations (_cf._ Alg. 2) are presented in Sec. 3.4. After fine-tuning the generator, return to Step-1 and repeat the attack process to further improve the MI performance. Our experiments primarily focus on PPA, which allows us to investigate a more _realistic attack scenario_ with high-resolution data.

### Understanding PPDG-MI with 2D Data

To illustrate the principles of PPDG-MI, we present a toy example using a 2D dataset with three classes, each sampled from a class-conditional Gaussian distribution, as shown in Fig. 3. Additionally, a public dataset is sampled from a separate Gaussian distribution to learn the distributional prior \((_{})\). We simulate a simple MIA by generating an initial set of samples using generator \(\) and then optimizing these samples to maximize the model's prediction score for Class 1. The objective is to uncover the features of Class 1, primarily the coordinates of the training samples. The closer these optimized samples are to the centroid of Class 1's distribution, _i.e.,_ the high-density region, the more effective the attack. See Appx. C.7 for a larger version of Fig. 3 and the experimental details of the toy example. An **animated illustration** of the toy demo is available in the supplementary materials.

The baseline attack results are shown in Fig. 3(a), where a fixed \(\) is adopted during the inversion process. The left panel of Fig. 3(b) illustrates the generation of dataset \(^{}_{}\) through a round of MI on model \(\). Middle panel of Fig. 3(b) shows the enhancement of density around \(^{}_{}\) under the prior distribution \((_{})\), achieved by fine-tuning \((;)\) to align with the empirical distribution of \(^{}_{}\), using the CT measure. The final attack results of PPDG-MI are shown in the right panel of Fig. 3(b). It is evident that, in comparison to the baseline where only a small fraction of reconstructed samples fall within the high-density region of the training data distribution, all reconstructed samples from PPDG-MI are located in this high-density region. See Appx. C.7 for the quantitative results.

Although we initially applied a direct distribution match strategy in this simplified setting to implement PPDG-MI, the empirical results indicate that this approach is less effective for higher-dimensional image data, as it would destroy the generator's manifold (Appx. C.8). We address this issue by introducing a nuanced tuning strategy tailored for high-dimensional data settings, detailed in Sec. 3.4.

### Nuanced Approach of PPDG-MI for High-Dimensional Image Data

Considering the primary baseline PPA (Struppek et al., 2022) uses StyleGAN (Karras et al., 2020) as its distributional prior, our approach leverages StyleGAN's disentangled nature, allowing slight local changes to its produced appearance without disrupting the manifold. Specifically, we first identify a high-density neighbor for each pseudo-private sample \(}\) (_cf._ Fig. 4(b)) and adjust this neighbor to be closer to \(}\), thereby enhancing density around \(}\) in the generator's domain (_cf._ Fig. 4(c)).

Figure 3: **Illustration of the rationale behind PPDG-MI using a simple 2D example.** Training samples from Class 0-2 are represented by purple, blue, and green, respectively, while public auxiliary data are shown in yellow. MIAs aim to recover training samples from Class 1, with reconstructed samples shown in red. (a) Results of the baseline attack with a fixed prior. (b) Left: Pseudo-private data generation. Middle: Density enhancement of pseudo-private data under prior distribution. Right: Final attack results of PPDG-MI with the tuned prior, where all the recovered points converge to the centroid of the class distribution, indicating the most representative features are revealed.

**Instantiate PPDG-MI with Point-wise Tuning.** To this intuition, we detail a two-step method to increase the density around pseudo-private samples. First, to locate a near neighbor \(^{}\) of \(}\), we optimize the latent code \(\) to produce \(^{}\). The closeness between \(^{}\) and \(}\) is measured using the LPIPS perceptual loss function (Zhang et al., 2018). Additionally, to ensure that \(^{}\) is located in a high-density region of \((_{})\), we leverage the discriminator \(\). Although GANs typically do not provide explicit probability density functions, empirical evidence suggests that \(\) effectively indicates the density of generated samples (_cf._ Fig. 4(a)). Overall, the optimization objective is formulated as

\[^{}=}{}_{}(},())}_{}-(())}_{},\] (6)

where \(_{}\) represents the perceptual loss function, and \(_{1}\) is a tuning hyperparameter that balances the constraints. At this step, the generator remains frozen. After optimizing Eq. (6), we obtain a high-density neighbor point \(^{}=(^{};)\) of \(}\). We then aim to slightly alter \((;)\) to pull \(^{}\) towards \(}\), thereby enhancing the local density around \(}\) (_cf._ Fig. 4(c)). This is accomplished by fine-tuning the generator with the point-wise loss term:

\[_{}()=_{}(},(^{};)).\] (7)

At this step, \(^{}\) remains fixed, and the adjustment is applied exclusively to the generator \(\). Building on the point-wise density enhancement, we extend our approach to a batch-wise method using statistical distribution discrepancy measures (Borgwardt et al., 2006; Zheng and Zhou, 2021), aiming for a more principled local distribution alignment strategy.

**Instantiate PPDG-MI with Batch-wise Tuning.** Given the sets \(^{s^{}}_{}=\{}_{i}\}_{i=1}^{m}\) and \(\{(_{j})\}_{j=1}^{n}(_{})\), following the point-wise tuning settings, we initially map these samples to the LPIPS space using a feature extractor \(f\). Denote \(\) as the distribution discrepancy measure, the batch-wise tuning strategy adapts the previous point-wise Eqs. (6) and (7) as follows:

\[\{^{}_{j}\}_{j=1}^{n}=_{j}\}_{j=1}^{n} }{}\ }_{i})\}_{i=1}^{m},\{f( (_{j}))\}_{j=1}^{n})}_{}-_{i=1}^{n}( (_{j}))}_{},\] (8a) \[_{}()=(\{f(}_{i})\}_{i=1}^{m},\{f((^{}_{j};))\}_{j=1}^{n}).\] (8b)

We present two realizations of the batch-wise tuning strategy: PPDG-MMD and PPDG-CT. When \(\) is set as MMD with Gaussian kernel \(k\), the optimization objective in Eqs. (8a) and (8b) is realized as

\[\{^{}_{j}\}_{j=1}^{n}=_{j}\}_{j=1}^{n} }{}\ }_{u}^{2}(\{f(}_{i})\}_{i=1}^{m},\{f( (_{j}))\}_{j=1}^{n};k)-_{1}_{i=1}^{n} ((_{j})),\] (9a) \[_{}()=}_{u}^{2}(\{f( }_{i})\}_{i=1}^{m},\{f((^{}_{j}; ))\}_{j=1}^{n};k).\] (9b)

Figure 4: **Illustration of PPDG-MI using a point-wise tuning approach. (a) The distribution of discriminator logit outputs for randomly generated samples by the generator \(\), showing that the discriminator can empirically reflect the density of generated samples. (b) Locating the high-density neighbor \(^{}\) by optimizing Eq. (6). Darker colors represent regions with higher density. (c) Increasing density around the pseudo-private data \(}\) by moving \(^{}\) towards \(}\), _i.e.,_ optimizing Eq. (7).**Similarly, when \(\) is set as CT, the optimization objective in Eqs. (8a) and (8b) is realized as

\[\{_{j}^{}\}_{j=1}^{n}=*{arg\,min}_{\{ _{j}\}_{j=1}^{n}}\,(\{f(}_{i})\}_{i=1}^{m},\{f ((_{j}))\}_{j=1}^{n})-_{1}_{i=1}^{n} ((_{j})),\] (10a) \[_{}()=(\{f(}_{i})\}_{i=1}^{m},\{f((_{j}^{}; ))\}_{j=1}^{n}).\] (10b)

The cost function in Eq. (3) is implemented as \(c(,)=1-(f(),f())\), while the distance function is implemented as \(d(,)=f()^{T}f()\), which are commonly adopted realization choices in existing literature (Tanwisuth et al., 2021, 2023).

## 4 Experiments

In this section, we evaluate the performance of SOTA MI methods before and after integrating them with PPDG-MI, as well as the robustness against SOTA MI defenses, including BiDO (Peng et al., 2022) and NegLS (Struppek et al., 2024), to assess the overall effectiveness of PPDG-MI. The evaluation primarily focuses on real-world face recognition tasks. For high-resolution (\(224 224\)) tasks, we consider PPA (Struppek et al., 2022) in the white-box setting. For low-resolution (\(64 64\)) tasks, we consider GMI (Zhang et al., 2020), KEDMI (Chen et al., 2021), LOM (Nguyen et al., 2023), and PLG-MI (Yuan et al., 2023) in the white-box setting, RLB-MI (Han et al., 2023) in the black-box setting, as well as BREP-MI (Kahla et al., 2022) in the label-only setting.

### Experimental Setup

This section briefly introduces the experimental setups. For further details, please refer to Appx. C.

**Datasets and Models.** In line with existing MIA literature on face recognition, we use the CelebA (Liu et al., 2015), FaceScrub (Ng and Winkler, 2014), and FFHQ datasets (Karras et al., 2019). These datasets are divided into two parts: the private training dataset \(_{}\) and the public auxiliary dataset \(_{}\), ensuring no identity overlap. For high-resolution tasks, we trained ResNet-18 (He et al., 2016), DenseNet-121 (Huang et al., 2017) and ResNeSt-50 (Zhang et al., 2022) as target models. For low-resolution tasks, we trained VGG16 (Simonyan and Zisserman, 2015) and face.evoLve (Wang et al., 2021) as target models. The training details of these models are presented in Appx. C.3. We summarize the attack methods, target models, and datasets adopted in Tab. 4.

**Attack Parameters.** For all MIAs, we fine-tune the generator \(\) in an identity-wise manner, to minimize alterations to the generator's latent space. Thus, adjustments to the attack parameters in official implementations are required. Detailed attack parameters are provided in Appx. C.4.

**Evaluation Metrics.** To evaluate the performance of an MIA, we need to assess whether the reconstructed images reveal private information about the target identity. Following existing literature (Zhang et al., 2020), we adopt top-1 (Acc@1) and top-5 (Acc@5) attack accuracy, as well as K-Nearest Neighbors Distance (KNN Dist). Details for these metrics are provided in Appx. C.5.

### Main Results

In the main experiments, we integrate PPDG-MI for density enhancement while still employing the baseline attack method for MI. We conduct one round of fine-tuning of \(\) and present the resulting attack results to demonstrate the efficacy of PPDG-MI. The results of multi-round fine-tuning are reserved for the ablation study (_cf._ Sec. 4.3). Additional experimental results, including evaluations on various target models, assessments with PLG-MI, black-box, and label-only MIAs, as well as comparisons against SOTA MI defenses for low-resolution tasks, are presented in Appx. D.

**Comparison with PPA in the high-resolution setting.** For each baseline setup, we report results for three variants: PPDG-PW, PPDG-CT and PPDG-MMD. The results presented in Tab. 1 demonstrate that our proposed method significantly improves MI performance across all setups, validating its effectiveness. Notably, integrating our methods with the baseline substantially increases attack accuracy. The KNN distance results also confirm that our methods more accurately reconstruct data resembling the private training data. Qualitative results of reconstructed samples from all target models are provided in Figs. 9 and 10 in Appx. D.3. Additionally, among the three PPDG-MI variants, the batch-wise tuning strategy consistently outperforms the point-wise tuning strategy. Batch-wisetuning captures characteristics of the local data distribution by handling batches of pseudo-private data, whereas point-wise tuning focuses on individual data points. Furthermore, batch-wise tuning is more robust against outliers, leading to a more reliable adjustment of the prior distribution.

**Comparison with white-box MIAs in the low-resolution setting.** For each baseline setup, we report results for PPDG-vanilla. The results are shown in Tab. 2, where PPDG-vanilla consistently outperforms various baseline white-box attacks. The improvement is evident in both attack accuracy and KNN distance metrics. Notably, even with a significant distribution shift between the private training dataset (CelebA) and the public auxiliary dataset (FFHQ), the principled vanilla fine-tuning strategy with original GAN training objectives effectively enhances the density around pseudo-private samples. As a highlight, PPDG-vanilla outperforms the baseline LOM (GMI) by achieving a \(12.35\%\) increase in top-1 attack accuracy and reducing KNN distance by approximately \(75\). Qualitative results of the reconstructed sample are provided in Figs. 11 and 12 in Appx. D.3.

**Attacks against SOTA MI defense methods.** We extend our evaluation to include state-of-the-art model inversion defense methods, specifically BiDO-HSIC and NegLS, comparing the performance of our proposed methods--PPDG-PW, PPDG-CT, and PPDG-MMD--with the baseline PPA. As summarized in Tab. 3, each proposed method consistently outperforms the baseline. Notably, PPDG-MMD achieves a \(6.05\%\) improvement in top-1 attack accuracy and reduces KNN distance by \(0.0529\) relative to the baseline against BiDO-HSIC. Similarly, against NegLS, PPDG-CT shows a \(4.90\%\) improvement in top-1 attack accuracy and a \(0.0818\) reduction in KNN distance compared to the baseline. Additional results for the low-resolution setting are provided in Appx. D.1.

    &  &  &  &  &  \\  & & & & KNN Dist.\(\) & & Ratioj & Acc@1\(\) & Acc@5\(\) & KNN Dist.\(\) & Ratioj \\   & PPA & 80.80 & 91.54 & 0.2734 & / & 83.19 & 95.89 & 0.7966 & / \\  & + PPDG-PW (ours) & 83.15 (+2.35) & 94.73 (+3.19) & 0.7082 (\(\)0.0292) & 2.25 & 84.44 & 95.83 & 0.7939 (\(\)0.0057) & 1.70 \\  & + PPDG-CT (ours) & 87.32 (+6.52) & 96.73 (+8.19) & 0.874 (\(\)0.0262) & 1.57 & 85.70 & 96.53 & 0.7688 (\(\)0.0282) & 1.19 \\  & + PPDG-MMD (ours) & 88.33 (+7.73) & 91.56 (+6.975) & 0.6795 (\(\)0.0579) & 1.13 & 87.02 & 97.13 & 0.7708 (\(\)0.0288) & 0.85 \\   & PPA & 76.74 & 89.04 & 0.7556 & / & 77.13 & 90.47 & 0.719 & _/ \\  & + PPDG-PW (ours) & 78.41 (+6.17) & 92.88 (+3.84) & 0.7219 (\(\)0.0337) & 1.67 & 84.92 & 92.89 & 0.7778 (\(\)0.0139) & 1.55 \\  & + PPDG-CT (ours) & 82.51 (+5.77) & 94.81 (+5.77) & 0.7003 (\(\)0.0553) & 1.14 & 84.93 & 96.14 & 0.7405 (\(\)0.0512) & 1.07 \\  & + PPDG-MMD (ours) & 84.02 (+7.28) & 95.37 (+6.33) & 6.696 (\(\)0.0592) & 0.81 & 85.55 & 96.20 & 0.7363 (\(\)0.0554) & 0.79 \\   & PPA & 64.54 & 82.79 & 0.8382 & / & 73.65 & 90.96 & 0.8386 & / \\  & + PPDG-PW (ours) & 67.66 (+3.34) & 86.73 (+9.40) & 0.8118 (\(\)0.0201) & 1.68 & 74.98 & 92.24 & 0.8190 (\(\)0.196) & 1.58 \\  & + PPDG-CT (ours) & 72.57 (+8.05) & 89.66 (\(\)6.87) & 0.7802 (\(\)0.0580) & 1.11 & 77.77 & 93.51 & 0.8045 (\(\)0.0341) & 1.07 \\  & + PPDG-MMD (ours) & 72.99 (+8.47) & 90.01 (\(\)2.20) & 0.7874 (\(\)0.0508) & 0.80 & 78.35 & 93.42 & 0.8109 (\(\)0.0277) & 0.79 \\   

Table 1: Comparison of MI performance with PPA in high-resolution settings. \(_{}\) = CelebA or FaceScrub, GANs are pre-trained on \(_{}\) = FFHQ. The symbol \(\) (or \(\)) indicates smaller (or larger) values are preferred, and the green numbers represent the attack performance improvement. The running time ratio (Ratio) between prior fine-tuning and MI reflects the overhead of fine-tuning.

  
**Method** & **Acc@1\(\)** & **KNN Dist\(\)** \\  No Def. & 77.85 & 0.8235 \\  BiDO-HSIC & 52.50 & 0.9546 \\ + PPDG-PW & 54.65 & 0.9270 \\ + PPDG-CT & 57.40 & 0.9051 \\ + PPDG-MMD & **58.55** & **0.9017** \\  NegLS & 11.35 & 1.3051 \\ + PPDG-PW & 14.65 & 1.2234 \\ + PPDG-CT & **16.25** & 1.2233 \\ + PPDG-MMD & 13.25 & **1.2187** \\   

Table 3: MI performance against SOTA defense methods in high-resolution settings. The target model \(\) = ResNet-152 is trained on \(_{}\) = FaceScrub, GANs are pre-trained on \(_{}\) = FFHQ. **Bold** numbers indicate superior results.

### Ablation Study

In this section, we present part of the ablation study on MIA in the high-resolution setting to further explore PPDG-MI. The target model is ResNet-18 trained on CelebA; GANs are pre-trained on FFHQ. Additional ablation results for high- and low-resolution settings are provided in Appx. D.2. Discussions (_e.g.,_ broader impact, failure case analysis and limitations) are provided in Appx. E.

**Iterative fine-tuning.** The iterative fine-tuning process is crucial for PPDG-MI (_cf._ Algs. 1 and 2). Its goal is to progressively increase the probability of sampling pseudo-private data with closer characteristics to the actual private training data. Ideally, if the classifier has learned all discriminative information of the target identity, this process can continue until it is capable of sampling the actual training data. As shown in left panel of Fig. 5, the attack performance consistently improves with additional rounds of fine-tuning, demonstrating the effectiveness of this approach.

**Selecting high-quality pseudo-private data for density enhancement.** The rationale behind enhancing the density around high-quality pseudo-private data, rather than random reconstructed ones, is that the former better reflect the characteristics of the private training data and are semantically closer. Thus, this increases the probability of sampling the actual training data. The middle panel of Fig. 5 compares the attack results of enhancing density around high-quality pseudo-private samples and randomly selected recovered samples, demonstrating the effectiveness of this strategy.

**Locating high-density neighbors using the discriminator.** We investigate the effect of using the discriminator D as an empirical density estimator to locate samples in high-density areas in Eqs. (6), (9a), and (10a). The comparison results, with and without the discriminator, are shown in the right panel of Fig. 5. The results indicate that MI performance decreases significantly without using the discriminator, with an approximate \(13\)-\(22\%\) reduction in attack accuracy across different fine-tuning methods. This demonstrates the effectiveness of incorporating D as a density estimator.

## 5 Conclusion

In this paper, we identify a fundamental limitation common to state-of-the-art generative MIAs, _i.e.,_ the utilization of a _fixed prior_ during the inversion phase. We argue that this approach is sub-optimal for MIAs. Accordingly, we introduce a novel inversion pipeline called _pseudo-private data guided_ MI (PPDG-MI), which, for the first time, involves iteratively tuning the distributional prior during the inversion process using pseudo-private samples. This increases the probability of recovering actual private training data. We propose multiple realizations of PPDG-MI and demonstrate their effectiveness through extensive experiments. Our findings pave the way for future research on generative MIAs and highlight the urgent need for more robust defenses against MIAs.