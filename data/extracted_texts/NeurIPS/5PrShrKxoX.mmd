# Transfer Q\({}^{\star}\): Principled Decoding for LLM Alignment

# Transfer Q\({}^{}\): Principled Decoding for LLM Alignment

Souradip Chakraborty\({}^{1}\)1  Soumya Suvra Ghosal\({}^{1}\)1  Ming Yin\({}^{2}\)  Dinesh Manocha\({}^{1}\)

**Mengdi Wang\({}^{2}\)  Amrit Singh Bedi\({}^{3}\)2  Furong Huang\({}^{1}\)2 \({}^{1}\)**

\({}^{1}\)University of Maryland-College Park; \({}^{2}\) Princeton University;

\({}^{3}\) University of Central Florida

denotes equal contribution

###### Abstract

Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward \(r\), thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function (\(Q^{*}\)), _which is often unavailable in practice_. Hence, prior SoTA methods either approximate this \(Q^{*}\) using \(Q^{_{}}\) (derived from the reference SFT model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose Transfer Q\({}^{*}\), which implicitly estimates the optimal value function for a target reward \(r\) through a baseline model \(_{}\) aligned with a baseline reward \(r_{}\) (which can be different from the target reward \(r\)). Theoretical analyses of Transfer Q\({}^{*}\) provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference SFT model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets. The code is available at [https://github.com/umd-huang-lab/Transfer-Q](https://github.com/umd-huang-lab/Transfer-Q).

## 1 Introduction

As artificial intelligence (AI) systems continue to demonstrate super-human performance across various tasks, it is becoming increasingly critical to ensure that such AI systems align well with human preferences, goals, and ethical standards. Alignment via fine-tuning with reinforcement learning from human feedback (RLHF) , has proven highly effective . However, aligning LLMs by fine-tuning the model parameters requires gradient updates across several billion parameters (size of LLMs) which require vast computational resources  and have a significant environmental impact . Additionally, many SoTA models are not fully open-sourced , offering limited access only to certain components like logits, making fine-tuning impossible.

As an alternative to fine-tuning, decoding for alignment has recently emerged as a potential solution . Decoding aims to alter the LLM's response distribution at the token level to align with target reward \(r\) without updating the parameters of the LLM. Decoding facilitates alignment by accessing the token-level optimal value function, \(Q^{*}\), which corresponds to the target reward \(r\). Tokens are then sampled based on \(Q^{*}\). This approach ensures rapid, efficient, and cost-effective alignment. A detailed discussion of related work is provided in Appendix E.

**A fundamental challenge.** Decoding effectively hinges on accessing an oracle to the optimal value function, \(Q^{*}\), or a token-level optimal policy, which are typically not available in practical scenarios. To address this fundamental challenge, recent studies  have adopted a proxy, \(Q^{*_{}}\), for \(Q^{*}\). However, this approach results in suboptimal decoding due to a distribution shift inherent in approximating the true, unknown Q-function, \(Q^{*}\), as illustrated in Figure 1. This raises a critical question: _Is it possible to devise a more efficient and accurate estimate of the optimal value function \(Q^{*}\) for decoding purposes?_ In this work, we affirmatively address this query.

**Our approach** leverages a crucial observation regarding the key challenges identified: effective decoding requires access to a language model already aligned with the target reward \(r\) for trajectory-level response generation. Notably, recent advances in DPO-based methods  have facilitated the development of fine-tuned language models--referred to as baseline models--that are capable of generating aligned trajectories . **Our first key idea** involves utilizing these publicly available trajectory-level models to estimate \(Q^{*}\) and subsequently derive the optimal token-level language model for decoding. We term this approach _direct transfer_ decoding.

Moreover, it is possible that these publicly available baseline models are aligned with a different baseline reward \(r_{}\), rather than the intended target reward \(r\). **Our second key idea** addresses this challenge by proposing a novel _indirect transfer_ decoding method. We introduce our proposed technique as Transfer Q* (TQ*), which facilitates efficient and on-the-fly alignment through decoding. We note that Transfer Q* proves effective even when there are substantial discrepancies between the target and baseline rewards.

We summarize our **major contributions** as follows.

**(1) A novel concept of transfer decoding (TQ*).** We introduce a novel concept of _transfer decoding_ in a principled manner by leveraging already-available baseline language models aligned either with the target reward (direct transfer) or with some other (significantly) different baseline reward (indirect transfer). Our proposed approach TQ* efficiently reduces the sub-optimality gap inherent in previous SoTA decoding methods, as highlighted in Figure 1.

**(2) Theoretical characterization of TQ*.** We provide a rigorous theoretical characterization of the optimality of Transfer Q*. Specifically, we derive an upper bound on the gap between the optimal LLM policy and the LLM decoding policy resulting from TQ* (see Theorem 1). Additionally, we present a principled approach to control the deviation of the resulting policy from the pre-trained SFT language model \(_{}\) (see Theorem 1, statement 2). This identifies hyperparameters that allow users to specify the desired amount of deviation or improvement based on their specific needs.

**(3) Experimental evaluations.** We provide a detailed empirical evaluation of TQ* in various decoding tasks in both real and simulated settings, comparing against SoTA baselines, including DPO , ARGS , and CD . Empirical results demonstrate consistent superiority over the baselines. Notably, TQ* surpasses the current SoTA decoding strategy CD , achieving an improvement of up to 1.45x in average reward and \(67.34\%\) in GPT-4 based win-tie rate. We further evaluate and compare several attributes of the text generated by our algorithm (such as coherence, diversity, and quality) against baselines, demonstrating the superiority of our algorithm in all these aspects.

Figure 1: **Left.** This figure highlights the conceptual idea of proposed _transfer decoding_ in this work. It clearly shows that the current SoTA method  exhibits suboptimality with respect to alignment with the target reward denoted by the dotted red arrow. On the other hand, the proposed transfer decoding method utilizes an immediately available aligned language model called the baseline, which is aligned with some baseline reward \(r_{}\) to bridge the gap between the SoTA method and the target model. **Right.** This figure provides empirical evidence of the performance gap of the current SoTA decoding strategy  with respect to Oracle (best of \(N\) sampling). Our proposed Transfer Q* (TQ*) reduces the gap and provides a new decoding method.

Problem Formulation: Alignment via Controlled Decoding

### Token-level Markov Decision Process

Since the control decoding procedure operates at the token level, before formulating the problem mathematically, we start by defining a token-level Markov decision process (MDP) in the context of LLMs. Let us consider a token-level MDP \(:=\{,,P,R\}\) with the state-space \(\) consisting of the concatenated sequence of tokens and the action space \(\) representing the space of the next token which is essentially the vocabulary \(\).

**Next-token generator: the token-level policy \(\).** Given a state \(_{t}=[,_{<t}]\), which is a sequence of tokens containing the prompt/query \(:=\{x_{1},x_{2},,x_{N}\}\) appended with the \(t\) tokens \(_{<t}:=\{y_{0},y_{1},,y_{t-1}\}\) generated so far, an LLM is a token-level decoding policy \(\) that generates the action (i.e., the next token) \(a_{t}=y_{t}\) via sampling \(y_{t}(_{t})\). The transition \(P\) to the next state \(_{t+1}\) is deterministic: \(_{t+1}=[,_{<t},y_{t}]\), the concatenation of the current state and action.

**Response generator: the trajectory-level policy \(\).** We denote the trajectory level probability by \(_{}(|)=_{t=1}^{T}(y_{t}|[, _{<t}])\). Some commonly used sampling techniques in the literature include Beam Search , Top-p sampling , and Top-k Sampling .

**From trajectory-level reward \(r\) to token-level reward \(R\).** Successful decoding depends on sampling from a token-level policy that yields high rewards, reflecting the inherently token-level nature of the decoding process. However, as detailed in Appendix F, we obtain a reward model \(r(,)\), only at the trajectory-level rather than the desired token-level, by fitting feedback on human preferences. To close the gap, similar to existing literature , we define the token-level reward \(R(,y_{t})\) from the trajectory-level reward model \(r(,)\) as follows:

\[R(,y_{t}):=0,&y_{t}\\ r(,_{<t}),&y_{t}=, \]

where \(\) represents the end of sequence token. The token-level reward in (1) implies that we only receive a reward once we have the full sequence/response, otherwise, no reward.

**Action-value function \(Q^{}\) for \(\) and optimal \(Q^{*}\) for optimal \(^{*}\).** From the token-level reward \(R(,y_{t})\), we can define the action-value function associated with the reward as

\[Q^{}(_{t},a_{t})=Q^{}([,_{<t}],y_{t})= [_{i}R([,_{<t}],z_{i}) z_{0}=y_{t},z_{i}(|_{t+i})], \]

where \(_{t+i}:=[_{t},z_{0},z_{1},,z_{i-1}]\) and expectation is over the randomness due to the sampling from token level language model \(\). The optimal Q-function from the definition in equation (2) is given by

\[Q^{*}(_{t},a_{t})=_{}Q^{}(_{t},a_{t}). \]

The optimization problem in (3) denotes an unconstrained objective as in standard reinforcement learning; however, in the context of alignment for LLMs, we also need to consider the distance of optimal aligned policy to the pre-trained unaligned token-level policy \(_{}\)[37; 41; 38].

### Principled Decoding for LLM Alignment

In this section, we will formulate the problem of aligning LLMs during deployment via a controlled decoding procedure as initially discussed in [33; 26].

**Decoding process.** We start by defining what decoding means in the context of LLMs. We consider access to a pre-trained unaligned language model \(_{}\) which takes in prompt \(\) as an input and generates a response \(=[y_{0},y_{1},,]\) token by token by sampling \(y_{t}_{}(|[,_{ t}])\) for all \(t\). This token-by-token generation of response is called decoding in LLMs. Hence, the natural next question arises if we can control the decoding process and generate responses that are aligned with respect to a target reward function \(r(,)\). The quest to answer this question has given rise to an interesting research problem of utilizing decoding for LLM alignment .

**LLM alignment via decoding.** The problem of LLM alignment via decoding can be formally defined as solving for the optimal decoding policy \(^{*}_{}\) under the token level MDP \(\) as

\[^{*}_{}(|_{t}):=_{}_{z (|_{t})}[Q^{*}(_{t},z)]- _{}(|_{t})||_{}( |_{t}), \]where \(_{t}=[,_{<t}]\) and \(Q^{*}(_{t},z)\) denotes the optimal state-action value function for the token-level MDP \(\) defined in (3). We remark that the KL regularization in equation (4) ensures that the optimal decoding policy \(_{}^{*}\) remains in the close neighborhood of the pre-trained model \(_{}\) which contains other important properties. In (4), \(>0\) denotes the alignment hyperparameter which controls the trade-off between the objective of maximizing the target reward \(r\) return and the closeness to \(_{}\). We can write the closed-form solution of the problem in (4) as

\[_{}^{*}(z|_{t})=_{}(z|_{t}) {(Q^{*}(_{t},z))}{C_{}}, \]

where \(C_{}:=_{z}_{}(z|_{t})( Q^{*}( _{t},z))\) is the normalizing constant for state \(_{t}\). Although the close form expression in (5) poses an interesting form, it is difficult to implement it in practice due to the various challenges we will discuss next.

**Challenges of implementing (5): oracle access to the optimal \(Q^{*}\).** A major challenge in implementing the aligned decoding policy in (5), lies in the requirement of access to the optimal \(Q^{*}(_{t},z)\) in (5) for each state-action pair \((_{t},z)\), which is unavailable in practice. To emphasize that, first we note that \(Q^{*}(_{t},z)\) in (3) can be written using the trajectory level reward \(r\) as

\[Q^{*}(_{t},z)=_{^{*}(|_{t},z)} [r([,_{<t},z],)], \]

where \(\) denotes the trajectory \(:=\{z_{1},z_{2},,z_{T}\}\), and \(^{*}(|_{t},z)=_{i=1}^{T}^{*}(z_{i}|[_{t+i }])\) represents the distribution over the trajectory level response induced by the optimal policy \(^{*}(|_{t})\) (cf. (3)). From (6), we note that the optimal \(Q^{*}(_{t},z)\) relies on the trajectory/response generated by the optimal \(^{*}\) which is unknown. This creates a bottleneck in efficiently deploying the decoding policy in (5) for alignment with target reward \(r\). Next, we discuss how some existing approaches in the literature deal with this fundamental issue and what the limitations are.

**Limitations in existing approaches.** An interesting approach called _controlled decoding (CD)_ is proposed in recent literature by Mudgal et al. , and it approximates \(Q^{*}(_{t},z)\) by \(Q^{_{}}(_{t},z)\), which is tractable and easily computable due to availability of \(_{}\). However, this approximation results in significant suboptimal performance, as described in Figure 1 (right). Given this limitation of existing approaches and the above-mentioned fundamental challenge of decoding, we pose the question: Is it possible to provide a better estimate of \(Q^{*}\) for decoding? We answer this affirmatively in the following section by introducing a novel concept of _transfer decoding_.

## 3 Proposed Approaches: Alignment via Transfer Q*

**Our key ideas of transfer decoding.** Our proposed approach hinges on an interesting observation that recent advancements in alignment, particularly through direct preference optimization (DPO)-based approaches , have led to the development of open source freely available fine-tuned language models . We call such aligned models as baseline models, that generate trajectory responses in an aligned manner. _**Our first key idea**_ is to utilize these baseline models aligned with target reward \(r\), if available, to estimate \(Q^{*}\) and subsequently derive the optimal token-level language model for decoding. We remark that baseline models are aligned at the trajectory level (see Appendix F for details), while decoding requires the optimal models at the token level. We term this approach _direct transfer decoding_. However, it is possible that the available aligned baseline model is aligned with a different baseline reward \(r_{}\) instead of the target reward \(r\). Hence, _**our second key idea**_ addresses this issue by proposing a novel method called _indirect transfer decoding_. We discuss both the ideas and detailed algorithms in detail next.

### Direct Transfer Decoding

For the direct transfer, we start by considering that we are given a target reward model \(r\) and an unaligned pre-trained SFT language model given by \(_{}\); the corresponding trajectory-level response distribution is given by \(_{}\). We are operating under the assumption that we have a baseline model \(_{}(|)\) with target reward \(r\). We note that the closed-form expression for the RLHF-aligned optimal model \(_{}\) can be written as (see Appendix F, Equation (25)) follows:

\[_{}(|)=()}_{ }(|)(r(, )), \]where \(Z_{r}()\) is the normalizing constant and \(>0\) is the trajectory-level alignment parameter. We note that the trajectory level optimal policy in (7) is usually obtained in the literature via the fine-tuning stage and efficient algorithms such as DPO .

**Estimating \(Q^{*}\) for direct transfer.** Now we get back to the fundamental bottleneck of optimal decoding, which lies in estimating the token-level optimal \(Q^{*}(_{t},z)\). We propose providing a solution to the problem with the help of \(_{}(|)\). As defined in (6), we begin by considering

\[Q^{*}(_{t},z)=_{^{*}(|_{t},z)} [r([_{t},z],)], \]

where \(^{*}(|_{t},z):=_{}_{( |_{t},z)}[r(_{t},)]\). However, we know that the available baseline model \(_{}(|)\) in (7) is the solution of following optimization problem:

\[_{}(|):=_{}_{( |)}[r(,)]-_{} (|)_{}(|) , \]

which constraints the drift of the optimal distribution \(_{}\) from the trajectory level reference policy \(_{}\) with the KI divergence term. We propose approximating the optimal \(Q^{*}(_{t},z)\) for decoding by

\[^{*}(_{t},z)=_{_{}(| _{t},z)}[r([_{t},z],)]. \]

With the definition in (10), we propose obtaining our token-level decoding policy \(^{*}_{}(|_{t})\) for the token-level MDP as

\[^{*}_{}(|_{t}):=_{}_{z(|_{t})}[^{*}(_{t},z)]- _{}(|_{t})\|_{}( |_{t}), \]

where \(_{}(|_{t})\) is the token-level language model, which induces the trajectory level model \(_{}\). Due to the strong convexity of the objective in (11) owing to the additional KL regularization term, we get the closed-form solution of \(^{*}_{}(|_{t})\) as

\[^{*}_{}(z|_{t})=_{}( _{t})}_{}(z|_{t}) ^{*}(_{t},z), \]

where \(_{}(_{t})\) is the normalizing constant. We summarize the proposed approach in Algorithm 1.

### Indirect Transfer Decoding

We remark that, for the direct transfer decoding, we started with the assumption that the available baseline model \(_{}\) is aligned with the target model only. In practice, however, it is possible that the freely available baseline model \(_{}\) is actually aligned with some other reward function we call baseline \(r_{}\) rather than the target reward \(r\). We argue that this condition is even more easily satisfied under the ongoing active research scenario in alignment because we have easy access to open-source, well-aligned LLMs trained on various reward functions [27; 52; 13; 38]. Under this setting of baseline reward \(r_{}\) and language model \(_{}\), we define our novel indirect transfer decoding process as follows.

The Transfer Process.The baseline language model \(_{}\) is also an RLHF aligned model corresponding to reward function \(r_{}\). It holds that:

\[r_{}(,)=}(|)}{_{}(|)}+ Z_{} (). \]

where \(Z_{}()\) is the corresponding partition function. From the closed-form expression in (7), it holds for the target reward \(r\) that

\[r(,)=(|)}{_ {}(|)}+ Z_{r}(). \]

Using equations (14) and (13), we can obtain the trajectory-level optimal policy \(_{r}(|)\) for the target reward \(r(,)\) as :

\[_{r}(|)=}(| )(r(,)-r_{ }(,))}_{:=_{r}(|)}}()}{Z_{r}()}, \]where we note that \(_{r}(|)\) is the unnormalized probability with the normalization factory \(():=}()}{Z_{}( )}\). We show in the Appendix H that \(()\) is the normalization constant for \(_{r}(|)\). We emphasize that calculating the trajectory-level optimal language model for the target reward function \(r\) in (15) is the crucial step in estimating the optimal \(Q^{*}\) for the token-level MDP \(\) for our decoding.

```
1:Input: Trajectory level baseline model \(_{}(|)\) aligned with baseline reward \(r_{}\), target reward \(r\), token-level baseline policy \(_{}\), number of tokens sampled \(k\), decoding alignment parameter \(\), vocabulary set \(\).
2:for\(t=0,,T\)do
3: Current state : \(_{t}=[,_{<t}]\), where \(\) is prompt and \(_{<t}=[y_{0},y_{1},,y_{t-1}]\)
4: Sample top-k tokens using token-level baseline policy \(_{}\) and store as \(}=\{z_{i}:z_{i}_{}(|_{t})\}_{i=1 }^{k}\)
5:for\(z}\)do
6:if\(r_{}=r\)then (Direct transfer)
7:Evaluate\(^{*}(_{t},z)=r([_{t},z],)\), where \(_{}(|[_{t},z])\)
8:else (Indirect transfer)
9:Evaluate\(^{*}(_{t},z)=w r([_{t},z],)\) where \(_{}(|[_{t},z])\), \(w=(|[_{t},z])}{_{}(|[_{ t},z]))}\)
10:Compute decoding score for token \(z\): \(g_{z}=^{*}(_{t},z)\) + \(_{}(z|_{t})\)
11:Next token\(y_{t}=_{z}}g_{z}\)
12:Next state\(_{t+1}[_{t},y_{t}]\)
13:Return \(=[y_{0},,y_{T}]\)
```

**Algorithm 1** Transfer \(^{*}\): LLM Alignment via Transfer Decoding

**Estimating \(Q^{*}\) for indirect transfer.** Similar to the direct transfer setting, we propose approximating the optimal \(Q^{*}(_{t},z)\) for decoding by using \(_{r}(|)\) in (15) as

\[^{*}(_{t},z) =_{_{r}(|_{t},z)}[r([ _{t},z],)]. \] \[=_{_{}(|_{t},z)} [(|)}{_{}(| )}r([_{t},z],)]\]

where we use the importance sampling trick and then utilizing equation (15), we can get the \(Q^{*}\) for indirect transfer. Now, following (11) and (12), we can write

\[_{}^{*}(|_{t}):=_{} _{z(|_{t})}[^{*}(_{ t},z)]-_{}(|_{t}) , \]

where \(_{r}(|_{t})\) is the token level probability derived from the trajectory level policy \(_{r}(|_{t})\) in (15). Following (11) and (12), we can obtain our optimal decoding policy for the indirect transfer as well. We summarize the proposed approach in Algorithm 1.

### Theoretical Results and Insights

This subsection provides the theoretical analysis of our Transfer Q* algorithm under direct transfer setup. Existing works [33; 18] leverage the reward-KL tradeoff curve to measure the _reward gain_ versus _deviation from reference policy_. Ideally, a good algorithm should achieve high rewards while staying close to the reference policy (be KL-efficient). We follow these two aspects and consider two performance metrics: (1) suboptimality gap and (2) KL divergence between our algorithm's policy and the SFT policy. Specifically, we borrow the notion of a suboptimality-gap from offline RL literature  and define it in terms of value function difference for any prompt \(\) as

\[():=V^{*}()-V_{}(). \]

where \(V^{*}()=_{}_{(|)}[r( ,)]\), \(V_{}()=_{_{}^{*}( |)}[r(,)]\), and \(_{}^{*}\) represents the distribution over the trajectories induced by the token level policy \(_{}^{*}(|)\) in (17). The KL divergence between our algorithm and the reference policy is denoted by \(_{}(_{}^{*}(|),_{ }(|))\). We present our main theorem as follows, and the full proof is deferred to Appendix G.

**Theorem 1**.: _For the proposed Transfer Q* Algorithm 1, the following results hold.__(1) Suboptimality gap for all \(\) is upper bounded as_

\[()_{}^{*}( |\,),_{}(|\,)- h_{} (), \]

_where \(\) is defined in (9) for baseline policy, and \(\) is defined in (17) for decoding process. Here \(h_{}() 0\) and its formula is defined in Appendix G._

_(2) Assume reward satisfies \(0 r r_{}\)then the divergence to SFT policy is given by_

\[_{}^{*}_{}(|\,), _{}(|\,)(+T)r_{}. \]

**Remark 1** ("Double Robustness" of Transfer Q").: Theorem 1 indicates the suboptimality is bounded by \(_{}(^{*},_{})\), and this guarantees our algorithm will achieve high accuracy in two cases. 1. The penalty parameter \(\) is small, 2. The SFT policy \(_{}\) is close to \(^{*}\). In addition, our decoding design (17) is also effective for improving the performance. This is due to \(- h_{} 0\). Our decoding coefficient \(\) could further reduce the suboptimality gap when it is appropriately tuned (_e.g._ choose \(^{*}=_{} h_{}\)).

**Remark 2** (KL-Efficiency of Transfer Q").: Via Theorem 1, the deviation from our algorithm to the reference policy is jointly controlled by parameter \(\) and decoding parameter \(\). When both parameters are set large, our algorithm is more conservative and hence more KL-efficient. When the parameters are small, the KL deviation becomes larger.

## 4 Experimental Evaluations

We present a comprehensive empirical analysis of both direct and indirect Transfer Q", tested across various open-source datasets and state-of-the-art models . Our findings demonstrate \(^{*}\)'s effectiveness in aligning language model outputs with specific target rewards. For implementation, we set the number of tokens sampled \(k=10\) and the decoding alignment parameter \(=1\). We report ablations in Appendix J.3. Reproducibility is ensured through the use of publicly available resources.

**Evaluation Methodology.** For evaluation, we compare the performance of the response generated by the language model corresponding to each prompt in the test dataset. Following , we limit the maximum length of the prompt and generated continuation to \(128\) and \(2048\) tokens, respectively. For all baselines, we utilize a greedy-based sampling method. The quality of the generated responses is assessed based on multiple attributes (including reward achieved, win-tie rate, coherence, diversity,

    &  &  &  \\   & & **SFT** & **DPO** & **Reward** & \\  Evaluation-1 & UltraFeedback  & Mistral-7B-\(\) & Zepby-7B-\(\) & Mistral-7B-\(\) & Relevant, Helpful, and Ethical responses. \\ Evaluation-2 & HHL-BLHIF  & Pythia-6B  & Pythia-6B  & Pythia-6B  & Helpful and Harrimless responses. \\ Evaluation-3 & Berkeley Nectur  & OpenXt 3.5-TB  & Saturing-7B-\(\) & Mistral-7B-\(\) & Accurate, Helpful, and Harrimless responses. \\ Evaluation-4 & UltraFeedback  & Llama-2-TB  & Tulu-7B-\(\) & Mistral-7B-\(\) & Relevant, Helpful, and Ethical responses. \\ Evaluation-5 & UltraFeedback  & Mistral-7B-\(\) & Zepby-7B-\(\) & Gemma-7B  & Relevant, Helpful, and Ethical responses. \\ Evaluation-6 & UltraFeedback  & Mistral-7B-\(\) & Zepby-7B-\(\) & Gemma-7B  & Relevant, Helpful, and Ethical responses. \\   

Table 1: Summary of the datasets and model architectures used for experimental evaluations in Section 4.1.

Figure 2: In plots (a), (c), and (d) we present the normalized average reward values obtained using the corresponding setup outlined in Table 1. ARGS (SFT) and ARGS (DPO) refer to the reward modeling approach described in  to the SFT and DPO model respectively. Our analysis reveals that across all setups, \(^{*}\) consistently outperforms other competitive baselines summarized in Table 1, demonstrating its superior efficacy. We report results on other evaluation setups in Appendix J. In (b), we compare (for Evaluation-1 setup) the trajectory-level KL Divergence between different decoding policies and the base model \(_{}\) to show the effectiveness of the proposed approach compared to the state-of-the-art.

etc.) using the following evaluation metrics : **Average Reward:** We report the mean of the rewards for generations corresponding to all prompts in the test set. A higher mean reward score signifies that the model's outputs are better aligned with the attributes represented in the reward model. **Diversity:** This metric measures the ability to generate texts with a wide range of vocabulary. Specifically, it calculates the frequency of repeated n-grams in text. **Coherence:** We assess the semantic closeness between each prompt and its generated response using the cosine similarity of their SimCSE-generated  embeddings.

### Transfer Q*: Evaluations with Direct Transfer Decoding

**Experiment Details.** For the direct transfer experiments, we consider our baseline model as a DPO  fine-tuned model aligned with the target reward. To evaluate the performance of Transfer Q* (denoted as \(}\) in figures), we conduct experiments across multiple datasets and model architectures, encompassing \(6\) distinct tasks. Our experimentation is primarily based on the Ultrafeedback , Berkeley Nectar , and HH-RLHF (Helpful and Harmless)  datasets. For each task, we utilize the DPO  fine-tuned model as an aligned policy, with configurations listed in Table 1. This comprehensive approach allows us to gauge the adaptability and efficacy of \(}\) in various contexts, providing a robust measure of its capabilities.

**Results Discussion.** In Figure 2, average rewards for the first We report the results for other setups in Appendix J.1. We compare our proposed method \(}\) with competitive existing approaches such as ARGS , \(}\)3, and DPO . To provide a clearer comparison of results, we normalize the average rewards (further details of normalization in Appendix I.1). We observe that across all setups, \(}\) consistently outperforms the existing approaches by a large margin, highlighting its efficacy in learning token-level optimal policy during inference. Further, in Figure 3, we report that \(}\) not only produces responses with high rewards but also outperforms other decoding strategies in terms of diversity and coherence.

**GPT-4 Evaluation.** To further understand the quality of the responses generated, we employ a GPT-4-based evaluation framework. Specifically, we use GPT-4 as a surrogate for human assessment. In this method, we prompt GPT-4 to raise assess and rate two responses on the same prompt on a scale from 1 to 10, focusing on criteria such as relevance, accuracy, and insightfulness. For this, we randomly sample \(300\) prompts from the test set and compare the response between \(}\) and other competitive decoding methods. We present the GPT-4 evaluation results in Table 2, measured by the percentage of win-ties of our method over the alternative decoding strategies. A higher percentage indicates that our proposed method is more proficient in generating responses that exhibit better alignment with human preferences. Our analysis in Table 2 shows that \(}\) consistently has a higher win-tie percentage compared to other decoding approaches, reaffirming its efficacy.

    & & & & **Win-Tie** (\%) \(\) \\ 
**Ours** & vs. & **Methods** & **Evaluation-1** & **Evaluation-2** & **Evaluation-3** \\  \(}\) & ARGS-SFT & 86.67 & 72.00 & 75.34 \\ \(}\) & DPO & 70.67 & 77.34 & 70.00 \\ \(}\) & ARGS-DPO & 68.00 & 71.33 & 74.00 \\ \(}\) & \(}\) & 66.67 & 65.34 & 67.34 \\   

Table 2: **GPT-4 Based Evaluation.** We prompt GPT-4 to rate responses from various decoding strategies on relevance, accuracy, and insightfulness, scoring them from 1 to 10. A higher win-tie percentage indicates our method’s effectiveness in generating contextually relevant and accurate responses.

Figure 3: **Diversity and Coherence analysis of generated responses.** We observe that the responses generated using \(}\) obtain the highest coherence and diversity. These results are based on the prompts from the Berkeley Nectar dataset.

### Transfer \(^{*}\): Evaluations with Indirect Transfer Decoding

**Synthetic experiments.** In the synthetic experiments, we simulate four scenarios to examine shifts in the reward distribution between source and target reward models. These scenarios are instrumental in elucidating the advantages of our proposed method. The shift in reward distribution is achieved through model intervention techniques, such as inducing sparsity in the final linear layer. Our analysis utilizes the UltraFeedback  and Berkeley Nectar  datasets. The specifics of the models utilized are detailed in Table 4 in Appendix I.2. For each dataset, we design two transfer tasks by modulating the reward distribution. We visualize the shift in reward distribution on the Ultrafeedback dataset in Figure 4 (a) and (c) respectively. Given that this is a synthetic setup, no DPO-aligned policies exist for the newly generated reward distribution, which underscores the significance of addressing the transfer problem. We present the results for this analysis on the Ultrafeedback  dataset in Figure 4. Due to space constraints, we report the results on the Berkeley Nectar  in Appendix J.2. We make the following key observations: (1) Our proposed decoding framework consistently attains the highest average reward and outperforms other competitive strategies. (2) The variant of our decoding strategy obtained by direct transfer to the target reward, i.e., DT has subpar performance.

**Real transfer experiments.** To further evaluate our proposed approach on real reward transfer, we consider two transfer setups as outlined in Table 5 in Appendix I.3. We illustrate the distribution shift in reward values on UltraFeedback  and HH-RLHF  datasets in Figure 5 (a) and (c) respectively. We compare the normalized average reward values of different decoding strategies in Figure 5. Consistent with our findings from the synthetic experiments, our proposed method consistently outperforms other decoding strategies, underscoring its effectiveness in real transfer tasks as well. Notably, we observe that even when there is a significant shift in the source and target reward values, as evident in Figure 5 (c), \(^{*}\) outperforms all competitive decoding approaches in terms of average reward.

## 5 Conclusions

In this paper, we introduce Transfer Q\({}^{*}\), a novel decoding strategy for AI alignment. Our method effectively addresses the prior gaps in alignment via decoding by leveraging an estimate of optimal \(Q^{*}\) for a target reward through an existing aligned policy \(_{}\). Our theoretical analysis provides a

Figure 4: **Evaluation for Synthetic Indirect Transfer Tasks. We plot the distribution of the reward values for the source and two transfer tasks on the Ultrafeedback in (a) and (c). The reward model architecture is Mistral-7B-\(\). In (b) and (d), we compare the normalized average reward scores for competitive decoding strategies. We represent the variant of our decoding strategy with direct transfer as DT. We observe that \(^{*}\) consistently outperforms the other baselines. Results on other datasets are reported in Appendix J.2.**

Figure 5: **Evaluation for Real Indirect Transfer Tasks. In (a) and (c), we visualize the distribution shift in reward values between the source and target for Setup-1 and Setup-2, respectively, as outlined in Table 5. In (b) and (d), we report the normalized average reward scores of different decoding strategies corresponding to Setup-1 and Setup-2, respectively.**rigorous characterization of the sub-optimality gap with respect to the token-level value function and the KL divergence to the reference policy. Experimentally, we demonstrate the consistent and significant superiority of the proposed approach. Hence, this work provides a principled solution to efficient decoding for AI alignment.