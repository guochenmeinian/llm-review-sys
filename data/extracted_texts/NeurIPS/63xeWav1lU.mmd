# Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models

Yifan Zhang and Junhui Hou

Department of Computer Science, City University of Hong Kong

yzhang3362-c@my.cityu.edu.hk;jh.hou@cityu.edu.hk

Corresponding author.

###### Abstract

Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma. This issue arises as contrastive losses unintentionally dissociate features of unmatched points and pixels that share semantic labels, compromising the integrity of learned representations. To overcome this, we harness Visual Foundation Models (VFMs), which have revolutionized the acquisition of pixel-level semantics, to enhance 3D representation learning. Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation. Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs. Furthermore, we adapt sampling probabilities of points to address imbalances in spatial distribution and category frequency, promoting comprehensive and balanced learning. Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks. Code is available at https://github.com/Eaphan/OLIVINE.

## 1 Introduction

LiDAR sensors deliver critical information in the 3D environment, crucial for applications such as autonomous driving . State-of-the-art neural networks have shown promising performance on point clouds processing, which rely on extensive annotated datasets . However, the process of annotating point clouds is not only time-consuming but also costly, presenting significant challenges in terms of scalability and practicality . Self-supervision offers a solution by leveraging vast quantities of unlabeled data to pre-train networks and subsequently fine-tuning them with a smaller, labeled dataset. This approach significantly reduces the reliance on extensive annotated data sets .

A prevalent method for learning 3D representations involves contrastive pixel-to-point knowledge transfer, using synchronized and calibrated images and point clouds. PPKT  enables a 3D network to derive extensive knowledge from a pre-trained 2D image backbone through a pixel-to-point contrastive loss. This entire pre-training process necessitates no annotations for either images or point clouds. Then SLidR  employs superpixels to cluster pixels and points from visually coherent regions, leading to a more meaningful contrastive task. Building on this, Seal  utilizes semantically rich superpixels generated by visual foundation models and introduces temporal consistency regularization across point segments at different times. Meanwhile, HVDistill  innovates by implementing cross-modality contrastive distillation that integrates both image-plane and bird-eye views.

Unfortunately, existing contrastive distillation methods are hindered by several critical limitations.

**Firstly**, a "self-conflict" issue arises during pre-training, where (super)pixels that belong to the same category as an anchor (super)point but do not directly correspond are simply treated as negative samples (see Fig.1(a)). This approach neglects the inherent semantic connections within the same category, leading to conflicts in the learning process where beneficial relationships might be disregarded. This problem is magnified by the contrastive loss's intrinsic hardness-aware characteristic, which results in the most substantial gradient influences derived from negative samples that are semantically the most similar [59; 41]. While ST-SLidR  has introduced a semantically tolerant loss to mitigate this issue, the absence of a robust high-level semantic understanding could not fundamentally change the intrinsic hardness-aware nature of the contrastive loss. **Secondly**, conventional sampling methods for point-pixel pairs fail to consider significant category imbalances or variations in point densities relative to their distance from sensors . For example, bicycles comprise only 1.47% of annotations in the nuScenes-lidarseg dataset, whereas drivable surfaces make up 37.66%. This oversight can result in a skewed representation of the environment, where dominant categories or densely populated areas are over-represented, impacting the model's effectiveness and fairness.

In this study, we address the "self-conflict" issue by leveraging supervised contrastive distillation enhanced with weak semantic labels generated by VFMs (Visual Foundation Models). VFMs like SAM (Segment Anything Model), trained on expansive datasets, has revolutionized computer vision by simplifying the acquisition of pixel-level semantics. These models are exceptionally adaptable, enabling the direct derivation of semantic labels through specified prompts without the need for retraining. As depicted in Fig.1(b), with these weak labels, we draw the embeddings of an anchor point and corresponding pixels of the same class closer together, while distancing the anchor from "negative" pixels of differing classes. Furthermore, given that representation of samples in the same class can vary significantly across different batches, we introduce semantic-guided consistency regularization to enhance 3D representation learning. This approach structures the feature space by modeling each class with a von Mises-Fisher distribution and making point features adhere closely to their respective distributions.

Considering the challenges posed by category imbalance and the non-uniform distribution of point clouds, we propose a density and category-aware sampling strategy. This method accounts for both the density of points and the frequency of their categories. By adjusting the sampling probabilities of different anchor points, we enhance the quality of learned 3D representations, particularly for points that fall into minority categories or are situated in areas of low density.

Extensive experiments reveal that our pre-training approach outperforms state-of-the-art 3D self-supervised methods on both the nuScenes and SemanticKITTI datasets. **The primary contributions** of this work are summarized as follows: **1)** To tackle the challenge of "self-conflict", we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation. **2)** We introduce semantic-guided consistency regularization to cultivate a meaningful and structured feature space. **3)** We develop an innovative sampling strategy for point-pixel pairs that considers both the category frequency and spatial density of points.

## 2 Related Work

**3D Representation Learning.** Recent advancements in 3D self-supervised learning have closely paralleled innovations in the image domain, extending these methods to diverse 3D contexts such as object-level point clouds [49; 60; 23], indoor scenes [61; 12; 81; 68; 9; 32; 69; 58], and outdoor settings [34; 4; 71; 42; 18]. These techniques are grounded in contrastive learning [66; 71; 42], mask modeling , and other pretext tasks [4; 76]. PPKT  utilized the InfoNCE loss to facilitate the 3D network in distilling rich knowledge from the 2D image backbone. Sautier et al. pioneered a superpixelto-superpoint contrastive loss for self-supervised 2D-to-3D representation distillation.

Figure 1: Illustration of (**a**) self-conflict that exists in conventional pixel-to-point contrastive distillation and (**b**) our weakly supervised contrastive distillation.

Building on this, Mahmoud et al.  enhanced the approach by incorporating a semantically tolerant contrastive constraint and a class-balancing loss. Liu et al. further refined these techniques through semantic-aware spatial and temporal consistency regularization, advancing feature learning. Moreover, Zhang et al.  explored cross-modality contrastive distillation across not only the image plane but also the bird-eye views.

**Visual Foundation Models.** The advent of powerful visual neural networks, trained on extensive datasets [46; 27] or through cutting-edge self-supervised learning techniques [7; 11; 21], catalyzed significant advancements within the community. Notably, the Segment Anything Model (SAM)  initiated a new paradigm in general-purpose image segmentation, demonstrating remarkable zero-shot transfer capabilities across a variety of downstream tasks. Building on this, Grounded-SAM  enhanced the model by incorporating elements from Grounding-DINO , an open-set object detector capable of recognizing and classifying previously unseen objects during training . In our work, we leverage the inherent semantic awareness of these VFMs  to generate weak semantic labels, which are crucial for our supervised contrastive distillation framework.

**3D Scene Understanding.** Traditional approaches to 3D scene understanding primarily utilize paradigms based on raw points [55; 16; 8; 62], voxels [17; 13], range views [29; 56], and multi-view fusion [19; 67]. Despite the effectiveness of these methods in capturing detailed environmental features, they are significantly constrained by their dependence on large-scale annotated data. Acquiring and annotating this data is both time-consuming and costly, limiting the scalability of 3D segmentation models . To reduce the reliance on large annotated datasets, recent studies have also turned to semi-supervised [31; 22], weakly-supervised [35; 15], and active learning techniques [40; 65].

## 3 Proposed Method

**Overview**. As depicted in Fig. 2, our method, namely OLIVINE, integrates a visual foundation model for pre-training with paired point clouds and images. Feature extraction is conducted using a trainable 3D backbone for point clouds and a pre-trained 2D backbone for images, with these features then mapped into a common feature space via decoupled projection heads for both point-pixel level and category-level contrastive distillation. The representation learning in OLIVINE is driven by three objectives: weakly-supervised contrastive distillation using coarse semantic labels to identify positive pairs by category, self-supervised contrastive distillation applied to randomly sampled point-pixel pairs, and a regularization framework based on the von Mises-Fisher distribution to ensure semantic consistency. Additionally, we address imbalances in spatial distribution and category frequency through a targeted sampling strategy, ensuring a balanced representation in the learning process.

**Notation.** Let \(P=\{p_{1},p_{2},...,p_{N}|p_{i}^{3}\}\) be a point cloud consisting of \(N\) points collected by a LiDAR sensor, and \(=\{I_{c}\}\)\(c=1,...,N_{}\}\) multi-view images captured by \(N_{}\) synchronized cameras, where \(I_{c}^{H W 3}\) is a single image with height \(H\) and width \(W\).

Figure 2: The overall pipeline of our proposed OLIVINE. The pipeline starts with feature extraction via a trainable 3D backbone and a pre-trained 2D backbone, followed by feature alignment in a common space. The learning is driven by weakly-supervised contrastive distillation with coarse semantic labels, self-supervised distillation of randomly sampled point-pixel pairs, and semantic consistency regularization through the von Mises-Fisher distribution. Besides, our approach is also characterized by the novel sampling strategy of point-pixel pairs addressing spatial and category distribution imbalances.

### Baseline Architecture

We follow the existing work  to perform the basic point-to-pixel contrastive distillation, based on which we build our whole pipeline. Starting with point cloud and image inputs, we employ distinct encoders for feature extraction. The 3D features are extracted using an encoder \(f_{ 3D}():^{N 3}^{N C_{ 3D}}\), which processes the point clouds to produce features of dimension \(C_{ 3D}\) per point. For image features, we use an encoder \(f_{ 2D}():^{H W 3}^{H^{ } W^{} C_{ 2D}}\), initialized with weights from pre-trained image models. This setup facilitates knowledge transfer from the 2D domain to the 3D domain through contrastive learning. For the computation of contrastive loss, we design trainable projection heads, \(h_{ 2D}^{ pp}\) for 2D features and \(h_{ 3D}^{ pp}\) for 3D features, both aligning the features into a unified dimensional space. Specifically, the 3D projection head \(h_{ 3D}^{ pp}\) is a linear layer with \(_{2}\)-normalization, converting 3D features to a normalized \(C\)-dimensional space. Similarly, the 2D projection head \(h_{ 2D}^{ pp}\), a convolution layer with a 1\(\)1 kernel followed by a bi-linear interpolation layer adjusting the spatial dimensions by a factor of 4, also applies \(_{2}\)-normalization.

Utilizing the calibration matrix, we establish dense point-to-pixel correspondences \(\{_{i}^{ 3D},_{i}^{ 2D}\}_{i=1}^{M}\), where \(_{i}^{ 3D}\) and \(_{i}^{ 2D}\) represent the paired features of points and images for the \(i\)-th pair, with \(M\) denoting the total count of such valid pairs. Previous methods achieve cross-modal knowledge transfer by attracting positive pairs and repelling negative pairs within the feature space, employing the InfoNCE loss . The point-pixel level contrastive loss is defined as

\[_{ PPNCE}=-}}{}} _{i}^{ 3D},_{i}^{ 2D})/)}{_{j=1}^{M_{s}} ((_{i}^{ 3D},_{j}^{ 2D})/)},\] (1)

where \(\) is the temperature factor, \(M_{s}\) is the number of sampled corresponding point-pixel pairs, \(,\) denotes the scalar product measure the similarity between features.

### Weakly-supervised Contrastive Distillation

Existing methods [39; 50; 38] often directly treat unmatched points and pixels that share semantic labels as negative pairs. This practice overlooks the intrinsic semantic connections within the same categories, leading to potential conflicts in the learning process where beneficial relationships are ignored. To address this, we utilize the Segment Anything Model, which adeptly interprets and translates semantic cues from text prompts into precise semantic segmentation of images. This application of SAM allows us to generate high-quality semantic labels without repetitive training, enhancing the learning process. We represent these labels as \(Y^{ co}=\{y_{i}^{ co}\}_{i=1}^{M}\), where each label corresponds to a specific point-pixel pair.

In point-pixel level contrastive loss, the pixels that belong to the same category but do not correspond to the given anchor point are taken as negative samples (see Eq. (1)). Therefore, we argue that the 2D and 3D features used for weakly-supervised contrastive learning, which take the category information into consideration, should differ from the features \(^{ 3D}\) and \(^{ 2D}\) that represent the individual points and pixels. To address this issue, we apply another two heads \(h_{ 2D}^{ sem}:^{H^{} W^{} C_{ 2D}} ^{H W C}\) and \(h_{ 3D}^{ sem}:^{N C_{ 3D}}^{N  C}\) to extract the semantic-level feature embeddings \(^{ 2D}\) and \(^{ 3D}\).

For the sampled points and pixels, we use their semantic labels to identify positive and negative pairs. Positive pairs are defined as point and pixel features that share the same semantic label, whereas negative pairs are those with different labels . The weakly-supervised contrastive loss is defined as

\[_{ sup}=-}_{i=1}^{M_{s}}_{a A(i)}_{i}^{ 3D},_{a}^{ 2D} /)}{_{j=1}^{M_{s}}(_{i}^{ 3D},_{j}^{ 2D} /)}\,,\] (2)

where \(A(i)\) denotes the set of indices of matched point-to-pixel pairs in the batch that have the same class with \(i\)-th point-pixel pair, and \(|A(i)|\) indicates its cardinality.

### Semantic-guided Consistency Regularization

We advocate that the construction of latent semantic structures in feature space could enhance representation learning. By leveraging semantic labels derived from SAM inference, we organize points with identical semantic labels into coherent groups. This grouping promotes feature consistency within these semantic categories, thereby stabilizing the learning of feature representations across varied data instances and yielding structured feature space.

**Distribution Assumption.** Intuitively, the point features extracted by the projection head \(h_{}^{}\) from the same class should exhibit similarity in the feature space. For the purposes of contrastive learning, these features are normalized to exist on the unit hypersphere. Consequently, we model the point features of each class \(k\) as a von Mises-Fisher (vMF) distribution \((z;_{k},_{k})\). This distribution is a spherical adaptation of the normal distribution, suitable for data constrained to a hypersphere . Here, \(_{k}\) represents the mean direction, while \(_{k}\) is the concentration parameter, indicating the degree to which category features are concentrated around \(_{k}\). The probability density function for the vMF distribution, applicable to a random \(C\)-dimensional unit vector \(\), is formulated as follows:

\[f_{C}(;_{k},_{k})=_{C}(_{k})(_{k}_ {k}^{}),\] (3)

where \( 0\) and \(\|\|_{2}=1\). The normalization constant \(_{C}()\) is defined as:

\[_{C}(_{k})=^{C/2-1}}{(2)^{C/2} _{(C/2-1)}(_{k})},\] (4)

\[_{(C/2-1)}(x)=_{m=0}^{} ()^{2m+C/2-1},\] (5)

where \(_{(C/2-1)}\) is the modified Bessel function of the first kind at order \(C/2-1\). The distribution exhibits a higher concentration around the mean direction \(_{k}\) as \(_{k}\) increases, and becomes uniform on the hypersphere when \(_{k}=0\).

**Parameter Updating.** Specifically, we conduct the semantic-guided consistency regularization in a _two-stage_ framework. First, we update the parameter of \((z;_{k},_{k}))\) with the features \(^{}\) extracted from point cloud branch. During training, we obtain the statistical value of feature embeddings via the EMA (Exponential Moving Average) algorithm, following:

\[_{k}^{t}=_{k}^{t-1}+(1-)_{k}^{rt},\] (6)

where \(_{k}^{rt}=}_{i=1}^{M}_{\{y_{i}^{c}=k\}} _{i}^{}\) denotes the sample mean of class \(k\) at \(t\)-th mini-batch, \(\) is the fixed smoothness coefficient. The maximum likelihood estimates of the mean direction \(_{k}\) is simply the normalized arithmetic mean:

\[_{k}=_{k}/_{k},\] (7)

where \(_{k}=||_{k}||\). And the concentration parameter \(_{k}\) could be obtained by the solving the equation:

\[A_{C}()=_{k},\] (8)

where \(A_{C}()=_{C/2}()/_{C/2-1}()\). Sra  proposed a simple method to estimate the \(_{k}\):

\[_{k}=_{k}(C-_{k}^{2})}{1-_{k}^{2}}.\] (9)

And we model the features of each observed point as a Spherical Dirac Delta distribution during training:

\[(z-_{i}^{})=\{0,&z_ {i}^{}\\ ,&z=_{i}^{}.\] (10)

**Loss Function of Regularization.** At the second step, we could perform the semantic-guided consistency regularization by pulling the points features and distribution of its corresponding category \((z;_{k},_{k})\) with Kullback-Leibler (KL) Divergence loss:

\[_{} =_{i=1}^{M}D_{}((z-_{i}^{ })||(z;_{k},_{k}))=_{i=1}^{M} _{k=1}^{K}_{\{y_{i}^{c}=k\}}- f_{C}(_{i}^{}; _{k},_{k})\] \[=_{i=1}^{M}_{k=1}^{K}_{\{y_{i}^{c}=k \}}-_{C}(_{k})-_{k}_{k}^{}_{i}^{}.\] (11)

In summary, the overall loss function for pre-training is written as \(=_{1}_{}+_{2}_{ }+_{3}_{}\), where \(_{1}\), \(_{2}\), and \(_{3}\) are the weights to balance the three terms.

### Density and Category-aware Sampling Strategy

Previous methods  neglect the spatial distribution and category frequency imbalances in the sampling of point-pixel pairs for contrastive distillation. To overcome these challenges, we introduce a novel sampling strategy that utilizes both the distance of each point from the LiDAR sensor and the occurrence frequency of its category. First, we calculate the distance of each point in the point cloud from the LiDAR sensor. We then apply kernel density estimation (KDE) to these distances to determine the probability density function of the spatial distribution of points. Given a point, its density can be calculated using the formula based on its distance \(d\) from the LiDAR sensor:

\[f_{}(d)=_{i=1}^{M}K_{h}(}{h} ),\] (12)

where \(d_{i}\) denotes the distance of point \(p_{i}\) from the LiDAR sensor, \(h\) is the bandwidth, \(K_{h}\) is the kernel function. This density estimation helps us understand how densely points are distributed with respect to their distance from the sensor, which is crucial for addressing areas of high point concentration that may bias the learning process.

Simultaneously, we assess the frequency of each category in the valid point-pixel pairs. By counting the occurrences of each category, we can identify which categories are underrepresented or overrepresented in the dataset.

Combining these two dimensions of analysis, we define the sampling probability for each point as inversely proportional to both its KDE-derived density and its category occurrence frequency. Mathematically, the sampling probability for a point \(p_{i}\) is given by:

\[(p_{i})=1/(f_{}(d_{i})|A(i)|).\] (13)

By implementing this sampling strategy, we aim to ensure a more uniform representation of both spatial and categorical dimensions in our contrastive learning setup. This method reduces the risk of overfitting to dense clusters of points or to frequently occurring categories, thereby fostering a more robust and generalizable representation learning.

## 4 Experiments

### Transfer on Semantic Segmentation

**Evaluation Protocol.** We evaluate the learned representations for semantic segmentation on nuScenes-lidarseg and SemanticKITTI datasets. The nuScenes-lidarseg and SemanticKITTI datasets contain 16 and 19 semantic categories for validation, respectively. We modify the network by adding a 3D convolutional layer to the pre-trained backbone as the segmentation head. Basically, we finetune the whole network on different portions of annotated data. Following previous works [50; 41], we finetune the network for 100 epochs with a batch size of 10 and 16 on SemanticKITTI and nuScenes-lidarseg, respectively. The initial learning rate of the backbone and the segmentation head are set to 0.05 and 2.0, respectively. When utilizing 1% of the annotated data, the network is fine-tuned for 100 epochs, whereas for other percentages, it is fine-tuned for 50 epochs. In another protocol, we evaluate the quality of learned representation by _linear probing_. Different from the setting of finetuning, we optimize only the added segmentation head and keep the weights of backbone \(f_{}\) frozen on the

    &  &  & KITTI \\  & & LP & 1\% & 5\% & 10\% & 25\% & 100\% & 1\% \\  Random & - & 8.1 & 30.30 & 47.84 & 56.15 & 65.48 & 74.66 & 39.50 \\ PointContrast  & ECCV’20 & 21.90 & 32.50 & - & - & - & - & 41.10 \\ DepthContrast  & ICCV’21 & 22.10 & 31.70 & - & - & - & - & 41.50 \\ PPKT  & Arxiv’21 & 35.90 & 37.80 & 53.74 & 60.25 & 67.14 & 74.52 & 44.00 \\ SLidR  & CVPR’22 & 38.80 & 38.30 & 52.49 & 59.84 & 66.91 & 74.79 & 44.60 \\ ST-SLidR  & CVPR’23 & 40.48 & 40.75 & 54.69 & 60.75 & 67.70 & 75.14 & 44.72 \\ Seal  & NeurIPS’23 & 44.95 & 45.84 & 55.64 & 62.97 & 68.41 & 75.60 & 46.63 \\  Ours & NeurIPS’24 & **50.09** & **50.58** & **60.19** & **65.01** & **70.13** & **76.54** & **49.38** \\   

Table 1: Comparison of various pre-training techniques for semantic segmentation tasks using either finetuning or linear probing (LP). This evaluation uses different proportions of accessible annotations from the nuScenes or SemanticKITTI datasets and presents the mean Intersection over Union (mIoU) scores on the validation set.

nuScenes-lidarseg dataset. For both protocols, the training objective is a linear combination of the cross-entropy loss and the Lovasz-Softmax loss .

**Results of Linear Probing.** Under the linear probing scenario, our method achieves the highest mIoU of 50.09%, surpassing the previous state-of-the-art method, Seal , which records a mIoU of 44.95% (see Table 1). This performance indicates a significant improvement in extracting useful features directly from pre-trained models without additional training of the 3D backbone.

**Results of Fine-tuning.** For the fine-tuning on nuScenes, our consistently excel, particularly at smaller data proportions. With only 1% of the training data of the nuScenes, our method achieves a mIoU of 50.58%. This trend persists across other data proportions, with our method consistently leading or closely competing with the best results, particularly at 60.19% mIoU with 5% of the data and 65.01% mIoU with 10% of the data. The qualitative results are presented in Fig. 3.

We also fine-tuned various models on the SemanticKITTI dataset to assess their performance across a spectrum of annotated data availability, from as low as 1% to full data utilization (see Table 2). Besides, as shown in Table 3, our method consistently achieves state-of-the-art performance on another six datasets.

### Transfer on 3D Object Detection

**Evaluation Protocol.** In evaluating our pre-trained model for 3D object detection, we utilized two prominent architectures: SECOND  and PV-RCNN . Both are built on the VoxelNet 3D backbone , which processes voxels via 3D sparse convolutions and includes a 2D backbone for bird's-eye-view encoding after BEV projection. The primary distinction between the architectures is in their detection heads. SECOND uses a region proposal network (RPN) directly on the 2D

   Initialization &  &  &  &  &  \\  Random & 39.5 & - & 45.7 & - & 51.5 & - & 56.0 & - & 56.9 & - \\ SLidR  & 44.6 & +5.1 & 54.7 & +9.0 & 56.3 & +4.8 & 56.7 & +0.7 & 57.1 & +0.2 \\ Ours & **49.4** & **+9.9** & **57.5** & **+11.8** & **60.3** & **+8.8** & **60.9** & **+4.9** & **61.1** & **+4.2** \\   

Table 2: Finetuning results on SemanticKITTI across various percentages of annotated data. The table compares the improvement achieved by our method relative to the SLidR .

Figure 3: The visual results of various point cloud pretraining strategies, pre-trained on nuScenes and fine-tuned using merely 1% of annotated data, are displayed. To illustrate the distinctions, we mark correctly predicted areas in gray color and incorrect ones in red.

    &  &  &  &  &  &  \\  & 1\% & 10\% & 1\% & 10\% & 50\% & 100\% & 50\% & 100\% & 1\% & 10\% & 50\% & 100\% \\  Random & 23.81 & 47.60 & 38.46 & 53.60 & 46.26 & 54.12 & 48.03 & 48.15 & 19.89 & 44.74 & 74.32 & 79.38 \\ PPKT  & 36.50 & 51.67 & 49.71 & 54.33 & 50.18 & 56.00 & 50.92 & 54.69 & 37.57 & 46.48 & 78.90 & 84.00 \\ SLidR  & 39.60 & 50.45 & 49.75 & 54.57 & 51.56 & 55.36 & 52.01 & 54.35 & 42.05 & 47.84 & 81.00 & 85.40 \\ Seal  & 40.64 & 52.77 & 51.09 & 55.03 & 53.26 & 56.89 & 53.46 & 55.36 & 43.58 & 49.26 & 81.88 & 85.90 \\  Ours & **44.91** & **54.96** & **53.47** & **58.21** & **55.70** & **58.51** & **56.65** & **60.42** & **46.34** & **52.78** & **83.63** & **86.84** \\   

Table 3: Evaluation of various pretraining methods initially trained on the nuScenes dataset and subsequently fine-tuned on multiple downstream point cloud datasets. The mIoU scores are presented as percentages (%).

backbone, while PV-RCNN refines RPN predictions with fine-grained keypoint features, enhancing bounding box accuracy and confidence in estimations.

In the fine-tuning stage, we integrate the detection head of SECOND or PV-RCNN with the pre-trained backbone (VoxelNet). This integrated detector is then fine-tuned on the train data of KITTI , which includes implementations of these detectors and follows the standard training parameters specified by OpenPCDet . We conduct the fine-tuning three separate times, and report the highest mean Average Precision (mAP) recorded on the KITTI validation set.

**Results.** The experimental results detailed in Table 4 showcase the performance of various initialization strategies. When using the SECOND architecture, our method outperforms other pre-training techniques. Starting from a baseline with random initialization, the performance improves consistently with more specialized pre-trained weights like PPKT and SLidR, and ultimately, our method achieves the highest mAP at 68.3%. Significant gains are observed across all categories, with particularly notable improvements in detecting pedestrians and cyclists. Similarly, the PV-RCNN architecture benefits from refined initialization methods. Our method again yields the highest overall mAP@40 at 72.8%, surpassing the performance of SLidR.

**Remark.** Compared to the semantic segmentation task, the model architecture for object detection is more complex. Besides the 3D backbone, 3D detectors typically project features to a BEV plane, followed by a 2D convolutional network and RoI operations. These crucial components were not pre-trained, which may limit the overall performance gain from our pre-training approach. It's important to note that semantic segmentation and object detection use different metrics and scales, making direct performance comparisons improper. The nature of these tasks and their evaluation criteria inherently lead to varying degrees of improvement when applying our proposed method.

### Ablation Studies

**Effect of Key Components.** In Table 5, we investigate the effect of each added component in our method. The integration of Weakly-supervised Contrastive Distillation alone yields a significant increase in performance, improving mIoU by 5.23% for linear probing. Similarly, incorporating Sematic-guided Consistency Regularization also enhances model performance, delivering a 3.71% increase in mIoU. When these components are combined, they synergistically contribute to a further mIoU gain of 7.89% for linear probing. Additionally, the application of Denstiy and Category-aware

   Detectors & Initialization & Car & Pedestrian & Cyclist & mAP@40 \\   & Random & 81.5 & 50.9 & 66.5 & 66.3 \\  & PPKT  & 81.8 & 51.4 & 68.2 & 67.1 \\  & SLidR  & 81.9 & 51.6 & 68.5 & 67.3 \\  & Ours & **82.0** & **53.2** & **69.8** & **68.3** \\   & Random & 84.5 & 57.9 & 71.3 & 71.3 \\  & STRL  & 84.7 & 57.8 & 71.9 & 71.5 \\   & PPKT  & 83.2 & 55.5 & 73.8 & 70.8 \\   & SLidR  & 84.4 & 57.3 & **74.2** & 71.9 \\   & Ours & **84.8** & **59.3** & **74.2** & **72.8** \\   

Table 4: Comparison of our method with other pre-training techniques through fine-tuning on the KITTI dataset. The results reflect the 3D object detection performance under moderate difficulty on the validation set.

   &  &  &  &  &  \\  & & & & LP & 1\% & 5\% & 10\% & 25\% & 100\% & 1\% \\ 
1 & \(\) & \(\) & \(\) & 36.87 & 37.89 & 53.15 & 60.33 & 67.03 & 74.59 & 44.12 \\
2 & ✓ & \(\) & \(\) & 42.10 & 42.33 & 55.22 & 61.53 & 67.70 & 75.06 & 45.58 \\
3 & \(\) & ✓ & \(\) & 40.58 & 41.99 & 54.49 & 60.98 & 67.69 & 74.88 & 45.67 \\
4 & ✓ & ✓ & \(\) & 44.76 & 44.91 & 56.01 & **63.12** & 68.74 & 75.48 & 46.60 \\
5 & ✓ & \(\) & ✓ & 44.15 & 43.96 & 55.75 & 62.49 & 68.13 & 75.07 & 46.07 \\
6 & ✓ & ✓ & ✓ & **47.30** & **46.12** & **57.51** & 63.04 & **69.39** & **76.13** & **47.35** \\   

Table 5: Ablation study of each component pre-trained on nuScenes and fine-tuned on nuScenes-lidarseg and SemanticKITTI. WCD: Weakly-supervised Contrastive Distillation. SCR: Semantic-guided Consistency Regularization. DCAS: Denstiy and Category-aware Sampling.

Sampling independently provides a substantial performance boost. The culmination of integrating all proposed components results in the optimal model, achieving a mIoU improvement of 10.43% for linear probing. This comprehensive analysis underscores the effectiveness of each component and their collective impact in enhancing the model's segmentation capabilities.

**Potential of Supervised Contrastive Distillation.** As mentioned in Section. 3, we perform weakly-supervised contrastive distillation with the pseudo-labels predicted by SAM. With the free and available models, our method learns effective 3D representations. When we replace the weak labels with the ground truth provided in nuScenes-lidarseg dataset, we can obtain a significant improvement in downstream tasks (see Table 5(a)). The results further demonstrate the effectiveness of supervision for cross-modal contrastive distillation and the potential of our pipeline with stronger VFMs.

**Effect of the Decoupled Projection Heads.** The experimental results outlined in Table 5(b) demonstrate the effectiveness of employing decoupled projection heads in our model. These results highlight a distinct performance enhancement when projection heads are specialized for distinct tasks -- specifically, self-supervised and weakly-supervised contrastive distillation. On the nuScenes dataset, the implementation of decoupled projection heads results in a mIoU improvement of 3.53%, indicating a robust enhancement in the model's ability to generalize from the training data. Similarly, for the SemanticKITTI dataset, a gain of 1.89% in mIoU is observed, further substantiating the benefits of this architectural modification.

**Effect of the vMF Distribution.** The ablation study in Table 5(c) compares the use of deterministic (Dirac delta) and von Mises-Fisher (vMF) distributions for modeling semantic features of each class, demonstrating clear advantages with vMF on both nuScenes and SemanticKITTI datasets. Specifically, the vMF distribution, with its adjustable concentration parameter, provides a mIoU improvement of 1.97% on nuScenes and 1.37% on SemanticKITTI compared to the deterministic approach. The learned concentration parameter that represents the uncertainty helps in mitigating overfitting by providing robustness against inaccuracies in coarse semantic labels.

**Effect of Different Sampling Strategies.** In Table 5(d), Category-aware and density-aware sampling determine the sampling probability of a point by its category frequency and distance information, respectively. These are part of a hybrid strategy we refer to as density and category-aware sampling. We found that the density and category-aware sampling strategy consistently achieves the best performance on downstream tasks.

### Further Discussions

We would like to emphasize the uniqueness and advantages of our approach over existing ones:

* Previous works [39; 50; 41; 38; 73] have not solved the self-conflict problem properly. Especially, Seal  generates semantically coherent superpixels for distinct objects and backgrounds in the 3D scene. However, the superpoints and superpixels with the same category may still be simply considered negative pairs during contrastive learning. By

Table 6: Comprehensive ablation studies for the key components. We report the fine-tuned results on nuScenes-lidarseg and SemanticKITTI (S.K.) datasets with 1% of the labeled data.

[MISSING_PAGE_FAIL:10]