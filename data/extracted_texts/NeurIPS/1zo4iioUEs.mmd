# DiffuseBot: Breeding Soft Robots With

Physics-Augmented Generative Diffusion Models

 Tsun-Hsuan Wang\({}^{1,}\), Juntian Zheng\({}^{2,3}\), Pingchuan Ma\({}^{1}\), Yilun Du\({}^{1}\), Byungchul Kim\({}^{1}\), Andrew Spielberg\({}^{1,4}\), Joshua B. Tenenbaum\({}^{1}\), Chuang Gan\({}^{1,3,5,}\), Daniela Rus\({}^{1,}\)

\({}^{1}\)MIT, \({}^{2}\)Tsinghua University, \({}^{3}\)MIT-IBM Watson AI Lab, \({}^{4}\)Harvard, \({}^{5}\)UMass Amherst

https://diffusebot.github.io/

###### Abstract

Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy. Co-optimization of artificial creatures' morphology and control _in silico_ shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. DiffuseBot bridges the gap between virtually generated content and physical utility by _(i)_ augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and _ii)_ introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation. We showcase a range of simulated and fabricated robots along with their capabilities.

## 1 Introduction

Designing dynamical virtual creatures or real-world cyberphysical systems requires reasoning about complex trade-offs in system geometry, components, and behavior. But, what if designing such systems could be made simpler, or even automated wholesale from high-level functional specifications? Freed to focus on higher-level tasks, engineers could explore, prototype, and iterate more quickly, focusing more on understanding the problem, and find novel, more performant designs. We present DiffuseBot, a first step toward efficient automatic robotic and virtual creature content creation, as an attempt at closing the stubborn gap between the wide diversity and capability of Nature _vis-a-vis_ evolution, and the reiterative quality of modern soft robotics.

Specifically, we leverage diffusion-based algorithms as a means of efficiently and generatively co-designing soft robot morphology and control for target tasks. Compared with with previous approaches, DiffuseBot's learning-based approach maintains evolutionary algorithms' ability to search over diverse forms while exploiting the efficient nature of gradient-based optimization. DiffuseBot is made possible by the revolutionary progress of AI-driven content generation, which is now able to synthesize convincing media such as images, audio, and animations, conditioned on human input. However, other than raw statistical modeling, these methods are typically task- and physics-oblivious, and tend to provide no fundamental reasoning about the performance of generated outputs. We provide the first method for bridging the gap between diffusion processes and the morphological design of cyberphysical systems, guided by physical simulation, enabling the computational creative design of virtual and physical creatures.

While diffusion methods can robustly sample objects with coherent spatial structure from raw noise in a step-by-step fashion, several roadblocks preventing existing generative algorithms from being directly applied to physical soft robot co-design. First, while existing diffusion methods can generate 2D or 3D shapes, useful for, say, sampling robot geometry, they do not consider physics, nor are they directly aligned with the robotic task performance. As an alternative approach, one might consider learning a diffusion model supervised directly on a dataset of highly-performant robot designs mapped to their task performance. This leads us to the second roadblock, that is, that no such dataset exists, and, more crucially, that curating such a dataset would require a prohibitive amount of human effort and would fail to transfer to novel tasks outside that dataset.

To tackle these challenges, we propose using physical simulation to guide the generative process of pretrained large-scale 3D diffusion models. Diffusion models pretrained for 3D shapes provide an expressive base distribution that can effectively propose reasonable candidate geometries for soft robots. Next, we develop an automatic procedure to convert raw 3D geometry to a representation compatible with soft body simulation, _i.e._ one that parameterizes actuator placement and specifies material stiffness. Finally, in order to sample robots in a physics-aware and performance-driven manner, we apply two methods that leverage physically based simulation. First, we optimize the embeddings that condition the diffusion model, skewing the sampling distribution toward better-performing robots as evaluated by our simulator. Second, we reformulate the sampling process that incorporates co-optimization over structure and control. We showcase the proposed approach of DiffuseBot by demonstrating automatically synthesized, novel robot designs for a wide spectrum of tasks, including balancing, landing, crawling, hurdling, gripping, and moving objects, and demonstrate its superiority to comparable approaches. We further demonstrate DiffuseBot's amenability to incorporating human semantic input as part of the robot generation process. Finally, we demonstrate the physical realizability of the robots generated by DiffuseBot with a proof-of-concept 3D printed real-world robot, introducing the possibility of AI-powered end-to-end CAD-CAM pipelines.

In summary, we contribute:

\(\) A new framework that augments the diffusion-based synthesis with physical dynamical simulation in order to generatively co-design task-driven soft robots in morphology and control.

\(\) Methods for driving robot generation in a task-driven way toward improved physical utility by optimizing input embeddings and incorporating differentiable physics into the diffusion process.

\(\) Extensive experiments in simulation to verify the effectiveness of DiffuseBot, extensions to text-conditioned functional robot design, and a proof-of-concept physical robot as a real-world result.

## 2 Method

In this section, we first formulate the problem (Section 2.1) and then describe the proposed DiffuseBot framework, which consists of diffusion-based 3D shape generation (Section 2.2), a differentiable procedure that converts samples from the diffusion models into soft robots (Section 2.3), a technique to optimize embeddings conditioned by the diffusion model for improved physical utility (Section 2.4), and a reformulation of diffusion process into co-design optimization (Section 2.4).

### Problem Formulation: Soft Robot Co-design

Soft robot co-design refers to a joint optimization of the morphology and control of soft robots. The morphology commonly involves robot geometry, body stiffness, and actuator placement. Control is the signal to the actuators prescribed by a given robot morphology. It can be formally defined as,

Figure 1: DiffuseBot aims to augment diffusion models with physical utility and designs for high-level functional specifications including robot geometry, material stiffness, and actuator placement.

\[_{,}(,)=_{,}(\{_{h} (_{h};,),_{h}\}_{h[1,H]}),\ \ \ \ _{h+1}=f(_{h},_{h})\] (1)

where \(\) is robot morphology that includes geometry \(_{}\), stiffness \(_{}\), and actuator placement \(_{}\); \(_{h}\) is actuation with the controller's parameters \(\) and dependency on the robot morphology \(\), \(_{h}\) is the simulator state, \(f\) is the environmental dynamics (namely the continuum mechanics of soft robots), and \(H\) is robot time horizon (not to be confused with the later-on mentioned diffusion time). Co-design poses challenges in optimization including complex interdependencies between body and control variables, ambiguity between competing morphology modifications, trade-offs between flexibility and efficacy in design representations, etc. . In this work, we aim at leveraging the generative power of diffusion models in searching for optimal robot designs with (1) the potential to synthesize highly-diverse robots and (2) inherent structural biases in the pre-trained 3D generative models learned from large-scale 3D datasets to achieve efficient optimization.

### 3D Shape Generation with Diffusion-based Models

Diffusion-based generative models [24; 52] aim to model a data distribution by augmenting it with auxiliary variables \(\{_{t}\}_{t=1}^{T}\) defining a Gaussian diffusion process \(p(_{0})= p(_{T})_{t=1}^{T}p(_{t-1}| _{t})d_{1:T}\) with the transition kernel in the forward process \(q(_{t}|_{t-1})=(_{t};- _{t}_{t-1},_{t})\) for some \(0<_{t}<1\). For sufficiently large \(T\), we have \(p(_{T})(,)\). This formulation enables an analytical marginal at any diffusion time \(_{t}=_{t}}_{0}+_{t}}\) based on clean data \(_{0}\), where \((,)\) and \(_{t}=_{i=1}^{t}1-_{i}\). The goal of the diffusion model (or more precisely the denoiser \(_{}\)) is to learn the reverse diffusion process \(p(_{t-1}|_{t})\) with the loss,

\[_{}_{t[1,T],p(_{0}),(; ,)}[||-_{}(_{t}(_{0},,t),t)||^{2}]\] (2)

Intuitively, \(_{}\) learns a one-step denoising process that can be used iteratively during sampling to convert random noise \(p(_{T})\) gradually into realistic data \(p(_{0})\). To achieve controllable generation with conditioning \(\), the denoising process can be slightly altered via classifier-free guidance [25; 12],

\[_{,}:=_{}( _{t},t,)+s(_{}(_{t},t,)- _{}(_{t},t,))\] (3)

where \(s\) is the guidance scale, \(\) is a null vector that represents non-conditioning.

### Robotizing 3D Shapes from Diffusion Samples

We adopt Point-E  as a pre-trained diffusion model that is capable of generating diverse and complex 3D shapes, providing a good prior of soft robot geometries. However, the samples from the diffusion model \(_{t}\) are in the form of surface point cloud and are not readily usable as robots to be evaluated in the physics-based simulation. Here, we describe how to roborize the diffusion samples \(_{t}\) and its gradient computation of the objective \(}{d_{t}}=}{_ {}}}}{_{t}}+}{_{}}}} {_{t}}+}{_{}} }}{_{t}}\).

Figure 2: The DiffuseBot framework consists of three modules: (i) _robotizing_, which converts diffusion samples into physically simulatable soft robot designs (ii) _embedding optimization_, which iteratively generate new robots to be evaluated for training the conditional embedding (iii) _diffusion as co-design_, which guides the sampling process with co-design gradients from differentiable simulation. Arrow (A): evaluation of robots to guide data distribution. (B): differentiable physics as feedback.

**Solid Geometry.** We use a Material Point Method (MPM)-based simulation , which takes solid geometries as inputs. This poses two obstacles: (1) conversion from surface point clouds into solid geometries, and (2) the unstructuredness of data in the intermediate samples \(_{t},t 0\). The second issue arises from the fact that the diffusion process at intermediate steps may produce 3D points that do not form a tight surface. First, we leverage the predicted clean sample at each diffusion time \(t\),

\[}_{0}=_{t}-_{t}} _{}(_{t},t)}{_{t}}}\] (4)

This approach is used in denoising diffusion implicit model (DDIM)  to approximate the unknown clean sample \(_{0}\) in the reverse process \(p(_{t-1}|_{t},}_{0})\). Here, we use it to construct a better-structured data for simulation. Hence, we break down the robotizing process into \(_{t}}_{0}\) with gradient components as \(_{0}}}{ _{t}}\), where \(}_{0}}{_{t}}\) can be trivially derived from (4). To convert the predicted surface points \(}_{0}\) into solid geometry, we first reconstruct a surface mesh from \(}_{0}\), and then evenly sample a solid point cloud \(_{}\) within its interior. For mesh reconstruction, we modify the optimization approach from Shape As Points , which provides a differentiable Poisson surface reconstruction that maps a control point set \(_{}\) to a reconstructed surface mesh with vertices \(_{}\). We calculate a modified Chamfer Distance loss indicating similarity between \(_{}\) and \(}_{0}\):

\[_{}=}}{|_{}|}_{_{}}d(,}_{0})+}}{|}_{0}|}_{ }_{0}}w()d(,_{}),\]

in which \(d(,)\) denotes minimal Euclidean distance between a point and a point set, and \(w()\) denotes a soft interior mask, with \(w()=1\) for \(\) outside the mesh, and \(w()=0.1\) for \(\) inside. The introduced mask term \(w(v)\) aims to lower the influence of noisy points inside the mesh, which is caused by imperfect prediction of \(}_{0}\) from noisy intermediate diffusion samples. The weight parameters are set to \(_{}=1\) and \(_{}=10\). We back-propagate \(_{}}{_{}}\) to \(_{}}{_{}}\) through the differentiable Poisson solver, and then apply an Adam optimizer on \(V_{}\) to optimize the loss \(_{}\). After mesh reconstruction, the solid geometry \(_{}\) represented by a solid interior point cloud is then sampled evenly within the mesh, with sufficient density to support the MPM-based simulation. Finally, to integrate the solidification process into diffusion samplers, we still need its gradient \(}}{}_{0}}\). We adopt Gaussian kernels on point-wise Euclidean distances as gradients between two point clouds:

\[}{}=- \|^{2})}{_{v^{}}_{0}}(-\| -^{}\|^{2})},_{},}_{0}.\]

Intuitively, under such Gaussian kernels gradients, each solid point is linearly controlled by predicted surface points near it. In practice, this backward scheme works well for kernel parameter \(=20\).

**Actuators and Stiffness.** A solid geometry does not make a robot; in order for the robot to behave, its dynamics must be defined. After sampling a solid geometry, we thus need to define material properties and actuator placement. Specifically, we embed actuators in the robot body in the form of muscle fibers that can contract or expand to create deformation; further, we define a stiffness parameterization in order to determine the relationship between deformation and restorative elastic force. We adopt constant stiffness for simplicity since it has been shown to trade off with actuation strength ; thus we have \(_{}(}_{0})=\) and \(}}{_{0}}=0\). Then, we propose to construct actuators based on the robot solid geometry \(_{}(_{}(}_{0}))\) via clustering; namely, we perform k-means with pre-defined number of clusters on the coordinates of 3D points from the solid geometry \(_{}\). The gradient then becomes \(}}{_{}}}}{_{0}}_{0}}{ _{t}}\), where \(}}{_{}} 0\) as empirically we found the clustering is quite stable in terms of label assignment, i.e., with \(_{}\) being small, \(_{} 0\). Overall, we keep only the gradient for the robot geometry as empirically it suffices.

### Physics Augmented Diffusion Model

**Embedding Optimization.** To best leverage the diversity of the generation from a large-scale pre-trained diffusion models, we propose to (1) actively generate new data from model and maintain them in a buffer, (2) use physics-based simulation as a certificate of performance, (3) optimize the embeddings conditioned by the diffusion model under a skewed data distribution to improve robotic performance in simulation. Curating a training dataset on its own alleviates the burden of manual effort to propose performant robot designs. Optimizing the conditional embeddings instead of finetuning the diffusion model eliminates the risk of deteriorating the overall generation and saves the cost of storing model weights for each new task (especially with large models). We follow,

\[_{}_{t[1,T],p_{}(_{0} |),(;,)}[||-_ {}(_{t}(_{0},,t),t,)||^{2}]\] (5)

Note the three major distinctions from (2): (i) the optimization variable is the embedding \(\) not \(\) (ii) the denoiser is conditioned on the embeddings \(_{}(,)\) and (iii) the data distribution is based on the diffusion model \(p_{}\) not the inaccessible real data distribution \(p\) and is conditioned on the embeddings \(\). This adopts an online learning scheme as the sampling distribution is dependent on the changing \(\). The procedure is briefly summarized in Algorithm 1, where _Filter_ is an operation to drop the oldest data when exceeding the buffer limit. In addition, during this stage, we use fixed prescribed controllers since we found empirically that a randomly initialized controller may not be sufficiently informative to drive the convergence toward reasonably good solutions; also, enabling the controller to be trainable makes the optimization prohibitively slow and extremely unstable, potentially due to the difficulty of the controller required to be universal to a diverse set of robot designs. After the embedding optimization, we perform conditional generation that synthesizes samples corresponding to robot designs with improved physical utility via classifier-free guidance as in (3).

**Diffusion as Co-design.** While the optimized embedding already allows us to generate performant robots for some target tasks, we further improve the performance of individual samples by reformulating the diffusion sampling process into a co-design optimization. As described in Section 2.3, we can convert the intermediate sample at any diffusion time \(_{t}\) to a robot design \(\), _rendering an evolving robot design throughout the diffusion process_. However, regular diffusion update  much less resembles any gradient-based optimization techniques, which are shown to be effective in soft robot design and control with differentiable simulation [26; 2]. Fortunately, there is a synergy between diffusion models and energy-based models [54; 15; 14], which allows a more gradient-descent-like update with Markov Chain Monte Carlo (MCMC) sampling . Incorporating the soft robot co-design optimization with differentiable physics  into the diffusion sampling process, we have

\[\ _{t}^{(k)} =_{t}^{(k-1)}+}{2}(_{ }(_{t}^{(k-1)},t)-_{_{t}^{(k-1)}}( (_{t}^{(k-1)}),_{t}^{k-1}))+^{2}\] (6) \[\ _{t}^{(k)} =_{t}^{(k-1)}+_{_{t}^{(k-1)}}( (_{t}^{(k-1)}),_{t}^{k-1})\] (7)

where \((,)\), \(_{t}^{(0)}=_{t-1}^{(K)}\), \(\) is the ratio between two types of design gradients, \(K\) is the number of MCMC sampling steps at the current diffusion time, \(\) is the weight for trading off design and control optimization, and \(_{t}^{(0)}\) can be either inherited from the previous diffusion time \(_{t-1}^{(K)}\) or reset to the initialization \(_{T}^{(0)}\). We highlight the high resemblance to gradient-based co-optimization with \(_{t}\) as the design variable and \(_{t}\) as the control variable. This procedure is performed once every \(M\) diffusion steps (Algorithm 2), where \(M\) is a hyperparameter that trade-offs "guidance" strength from physical utility and sampling efficiency. Intuitively, the entire diffusion-as-co-design process isguided by three types of gradients: (i) \(_{}(_{t}^{(k-1)},)\) provides a direction for the design toward feasible 3D shapes based on the knowledge of pre-training with large-scale datasets (and toward enhanced physical utility with the optimized embeddings via classifier-free guidance using \(_{}(_{t}^{(k-1)},,)\)), (ii) \(_{_{t}}((_{t}),)\) provides a direction for the design toward improving co-design objective \(\) via differentiable simulation, and (iii) \(_{_{t}}(,_{t})\) provides a direction for the controller toward a better adaption to the current design \(_{t}\) that allows more accurate evaluation of the robot performance.

## 3 Experiments

### Task Setup

We cover three types of robotics tasks: passive dynamics, locomotion, and manipulation (Figure 3).

\(\)**Passive Dynamics** tasks include balancing and landing. _Balancing_ initializes the robot on a stick-like platform with small contact area with an upward velocity that introduces instability; the robot's goal is to passively balance itself after dropping on the platform. _Landing_ applies an initial force to the robot toward a target; the robot's goal is to passively land as close to the target as possible.

\(\)**Locomotion** tasks include crawling and hurdling. _Crawling_ sets the robot at a rest state on the ground; the robot must actuate its body to move as far away as possible from the starting position. _Hurdling_ places an obstacle in front of the robot; the robot must jump over the obstacle.

\(\)**Manipulation** tasks include gripping and moving objects. _Gripping_ places an object underneath the robot; the goal of the robot is to vertically lift the object. _Box Moving_ places a box on the right end of the robot; the robot must move the box to the left.

Please refer to the appendix Section D for more detailed task descriptions and performance metrics.

### Toward Physical Utility In Diffusion Models

**Physics-augmented diffusion.** In Table 1, we examine the effectiveness of embedding optimization and diffusion as co-design for improving physical utility. For each entry, we draw 100 samples with preset random seeds to provide valid sample-level comparison (i.e., setting the step size of co-design optimization to zero in the third row will produce almost identical samples as the second row). We report the average performance with standard deviation in the superscript. First, we observe increasing performance across all tasks while incorporating the two proposed techniques, demonstrating the efficacy of DiffuseBot. Besides, the sample-level performance does not always monotonically improve, possibly due to the stochasticity within the diffusion process and the low quality of gradient from differentiable simulation in some scenarios. For example, in gripping, when the robot fails to pick up the object in the first place, the gradient may be informative and fails to bring proper guidance toward better task performance; similarly in moving a box. In addition, we found it necessary to include control optimization during the diffusion sampling process, since, at diffusion steps further from zero, the predicted clean sample \(}_{0}\) (derived from the intermediate sample \(_{t}\)) may differ significantly from the clean sample \(_{0}\), leaving the prescribed controller largely unaligned.

**Comparison with baselines.** In Table 2, we compare with extensive baselines of soft robot design representation: particle-based method has each particle possessing its own distinct parameterization of design (geometry, stiffness, actuator); similarly, voxel-based method specifies design in voxel level; implicit function  uses use a shared multi-layer perceptron to map coordinates to design;

   Embed. & Diffusion as &  &  &  \\ Optim. & Co-design & Balancing & Landing & Crawling & Hurdling & Gripping & Moving a Box \\   & & 0.081\({}^{\,164}\) & 0.832\({}^{\,217}\) & 0.011\({}^{\,012}\) & 0.014\({}^{\,020}\) & 0.014\({}^{\,008}\) & 0.019\({}^{\,020}\) \\ ✓ & & 0.556\({}^{\,127}\) & 0.955\({}^{\,032}\) & 0.048\({}^{\,007}\) & 0.019\({}^{\,014}\) & 0.025\({}^{\,006}\) & 0.040\({}^{\,018}\) \\ ✓ & ✓ & **0.653\({}^{\,107}\)** & **0.964\({}^{\,029}\)** & **0.081\({}^{\,018}\)** & **0.035\({}^{\,030}\)** & **0.027\({}^{\,004}\)** & **0.044\({}^{\,021}\)** \\   

Table 1: Improved physical utility by augmenting physical simulation with diffusion models.

Figure 3: We consider passive dynamics tasks (balancing, landing), locomotion tasks (crawling, hurdling), and manipulation tasks (gripping, moving a box).

DiffePPN  uses a graphical model composed of a set of activation function that takes in coordinates and outputs design specification; DiffAqua  computes the Wasserstein barycenter of a set of aquatic creatures' meshes and we adapt to use more reasonable primitives that include bunny, car, cat, cow, avocado, dog, horse, and sofa. These baselines are commonly used in gradient-based soft robot co-design [26; 56; 61]. For each baseline method, we run the co-optimization routine for the same number of steps as in the diffusion-as-co-design stage in DiffuseBot. To avoid being trapped in the local optimum, we run each baseline with 20 different random initializations and choose the best one. Since DiffuseBot is a generative method, we draw 20 samples and report the best; this is sensible an applications-driven perspective since we only need to retrieve one performant robot, within a reasonable sample budget. We observe that our method outperforms all baselines. DiffuseBot leverages the knowledge of large-scale pre-trained models that capture the "common sense" of geometry, providing a more well-structured yet flexible prior for soft robot design.

**Soft robots bred by DiffuseBot.** In Figure 5, we demonstrate the generated soft robots that excel in locomotion and manipulation tasks. We highlight the flexibility of DiffuseBot to generate highly diverse soft robot designs that accommodate various purposes in different robotics tasks. Furthermore, in Figure 4, we show how robots evolve from a feasible yet non-necessarily functional design to an improved one that intuitively matches the task objective. By manually inspecting the evolving designs, we found that the role of the embedding optimization is to drive the diverse generations toward a converged, smaller set with elements having higher chance to succeed the task; on the other hand, the role of diffusion as co-design brings relatively minor tweaks along with alignment between the control and design. Due to space limit, we refer the reader to our project page for more results.

    &  &  &  \\  & Balancing & Landing & Crawling & Hurdling & Gripping & Moving a Box \\  Particle-based & 0.040\({}^{\,000}\) & 0.863\({}^{\,005}\) & 0.019\({}^{\,001}\) & 0.006\({}^{\,001}\) & -0.010\({}^{\,001}\) & 0.043\({}^{\,027}\) \\ Voxel-based & 0.040\({}^{\,000}\) & 0.853\({}^{\,002}\) & 0.024\({}^{\,000}\) & 0.027\({}^{\,000}\) & -0.009\({}^{\,000}\) & 0.025\({}^{\,022}\) \\ Implicit Function  & 0.106\({}^{\,147}\) & 0.893\({}^{\,033}\) & 0.043\({}^{\,024}\) & **0.044\({}^{\,063}\)** & 0.006\({}^{\,012}\) & 0.033\({}^{\,030}\) \\ Diff-CPPN  & 0.091\({}^{\,088}\) & 0.577\({}^{\,425}\) & 0.055\({}^{\,023}\) & 0.019\({}^{\,029}\) & 0.007\({}^{\,008}\) & 0.022\({}^{\,017}\) \\ DiffAqua  & 0.014\({}^{\,023}\) & 0.293\({}^{\,459}\) & 0.027\({}^{\,015}\) & 0.022\({}^{\,011}\) & 0.010\({}^{\,001}\) & 0.007\({}^{\,008}\) \\ DiffuseBot & **0.706\({}^{\,078}\)** & **0.965\({}^{\,026}\)** & **0.092\({}^{\,016}\)** & 0.031\({}^{\,011}\) & **0.026\({}^{\,002}\)** & **0.047\({}^{\,019}\)** \\   

Table 2: Comparison with baselines.

Figure 4: Examples of DiffuseBot evolving robots to solve different tasks.

Figure 5: Examples of robots bred by DiffuseBot to achieve the desired tasks.

### Ablation Analysis

In this section, we conduct a series of ablation studies to provide a deeper understanding of the proposed method. For simplicity, all experiments in this section are done with the crawling task.

**Embedding optimization.** In Table 3, we compare the optimization of the embedding conditioned by the diffusion models with other alternatives. The pre-trained diffusion model  that DiffuseBot is built upon uses CLIP embeddings , which allows for textual inputs. Hence, a naive approach is to manually design text for the conditional embedding of the diffusion model. The result reported in Table 3 uses _"a legged animal or object that can crawl or run fast"_. We investigated the use of text prompts; in our experience, text was difficult to optimize for _functional_ robot design purposes. This is expected since most existing diffusion models perform content generation only in terms of appearance instead of physical utility, which further strengthens the purpose of this work. In addition, with exactly the same training objective as in (5), we can instead finetune the diffusion model itself. However, this does not yield better performance, as shown in the second entry in Table 3. Empirically, we found there is a higher chance of the generated samples being non-well-structured with fractured parts. This suggests that finetuning for physical utility may deteriorate the modeling of sensible 3D shapes and lead to more unstable generations.

**Diffusion as co-design.** Recall that the co-design optimization can be seamlessly incorporated into any diffusion step. In Figure 6, we examine how the strength of the injected co-design optimization affects the task performance in terms of where to apply throughout the diffusion sampling process and how many times to apply. In the left figure of Figure 6, we sweep through the maximal diffusion time of applying diffusion as co-design, i.e., for the data point at \(t=400\), we only perform co-design from \(t=400\) to \(t=0\). We found that there is a sweet spot of when to start applying co-design (at \(t 200\)). This is because the intermediate samples at larger diffusion time \(_{t},t 0\) are extremely under-developed, lacking sufficient connection to the final clean sample \(_{0}\), hence failing to provide informative guidance by examining its physical utility. Furthermore, we compare against post-diffusion co-design optimization, i.e., run co-design based on the final output of the diffusion (\(0.064\) vs ours \(0.081\)). We allow the same computational budget by running the same number of times of differentiable simulation as in DiffuseBot. Our method performs slightly better, potentially due to the flexibility to alter the still-developing diffusion samples. Also note that while our method is interleaved into diffusion process, it is still compatible with any post-hoc computation for finetuning.

### Flexibility To Incorporate Human Feedback

Beyond the generative power, diffusion models also provide the flexibility to composite different data distributions. This is especially useful for computational design since it empowers to easily incorporate external knowledge, e.g. from human. We follow the compositionality techniques introduced in [34; 14], which can be directly integrated into our diffusion as co-design framework. In Figure 7, we demonstrate incorporating human feedback in textual form as _"a unicom"_ into a crawling robot generated by DiffuseBot. We can see the emergence of the horn-like body part.

### From Virtual Generation To Physical Robot

We further fabricate a physical robot for the gripping task as a proof-of-concept to demonstrate the possibility of real-world extension. We use a 3D Carbon printer to reconstruct the exact geometry of a generated design and fill the robot body with a Voronoi lattice structure to achieve softness.

    & Performance \\  MT & 0.016\(\,{}^{014}\) \\ FT & 0.031\(\,{}^{024}\) \\ Ours & 0.048\(\,{}^{007}\) \\   

Table 3: Ablation on embedding optimization. MT means manually-designed text. FT means finetuning models.

Figure 6: Varying starting point and strength of diffusion as co-design.

Figure 7: Incorporating human textual feedback.

For actuators, we employ tendon transmission to realize the contraction force utilized in the soft robot gripper. In our project page, we demonstrate the robots generated by DiffuseBot are capable of picking up an object. Note that physical robot fabrication and real-world transfer have countless non-trivial challenges including stiffness and actuator design, sim-to-real gap, etc. Hence, this experiment is only meant to demonstrate the potential instead of a general, robust pipeline toward physical robots, which is left to future work. We refer the reader to the appendix for more details.

## 4 Related Work

**Heuristic Search For Soft Robot Co-Design.** Heuristic searches are simple but useful tools for co-designing soft robots. A long line of work has focused on evolutionary algorithms [7; 8; 10], with some including physical demonstrations [22; 31; 32] and recent benchmarks incorporating neural control . These methods are often powered by parameterized by compositional pattern-producing networks , which parameterize highly expressive search spaces akin to neural networks [50; 51]. Similar to ,  combines a heuristic approach with reinforcement learning, and demonstrates resulting designs on physical hardware. Other notable methods include particle-filter-based approaches  and simulated annealing . Heuristic search methods tend to be less efficient than gradient-based or learning-based algorithms, but can reasoning about large search spaces; our approach employs the highly expressive diffusion processes, while leveraging the differentiable nature of neural networks and physical simulation for more efficient and gradient-directed search.

**Gradient-Based Soft Robot Co-Optimization.** A differentiable simulator is one in which useful analytical derivatives of any system variable with respect to any other system variable is efficiently queryable; the recent advent of soft differentiable simulation environments [26; 56; 13; 33; 41; 42; 61] has accelerated the exploration of gradient-based co-optimiation methods. [26; 56] demonstrated how differentiable simulators can be used to co-optimize very high-dimensional spatially varying material and open-loop/neural controller parameters.  presented gradient-based search of shape parameters for soft manipulators. Meanwhile,  showed how actuation and geometry could be co-optimized, while analyzing the trade-offs of design space complexity and exploration in the search procedure. DiffuseBot borrows ideas from gradient-based optimization in guiding the design search in a physics-aware way, especially in the context of control.

**Learning-Based Soft Robot Co-Design Methods.** Though relatively nascent, learning-based approaches (including DiffuseBot ) can re-use design samples to build knowledge about a problem. Further, dataset-based minibatch optimization algorithms are more robust to local minima than single-iterate pure optimization approaches.  demonstrated how gradient-based search could be combined with learned-models; a soft robot proprioceptive model was continually updated by simulation data from interleaved control/material co-optimization. Other work employed learning-based methods in the context of leveraging available datasets.  learned a parameterized representation of geometry and actuators from basis shape geometries tractable interpolation over high-dimensional search spaces.  leveraged motion data and sparsifying neurons to simultaneously learn sensor placement and neural soft robotic tasks such as proprioception and grasp classification.

**Diffusion Models for Content Generation.** Diffusion models [24; 52] have emerged as the de-facto standard for generating content in continuous domains such as images [44; 46], 3D content [66; 65], controls [28; 9; 1], videos [23; 49; 16], and materials [63; 62; 48]. In this paper, we explore how diffusion models in combination with differentiable physics may be used to design new robots. Most similar to our work,  uses differentiable physics to help guide human motion synthesis. However, while  uses differentiable physics to refine motions in the last few timesteps of diffusion sampling, we tightly integrate differentiable physics with sampling throughout the diffusion sampling procedure through MCMC. We further uses differentiable simulation to define a reward objective through which we may optimize generative embeddings that represent our desirable robot structure.

## 5 Conclusion

We presented DiffuseBot, a framework that augments physics-based simulation with a diffusion process capable of generating performant soft robots for a diverse set of tasks including passive dynamics, locomotion, and manipulation. We demonstrated the efficacy of diffusion-based generation with extensive experiments, presented a method for incorporating human feedback, and prototyped a physical robot counterpart. DiffuseBot is a first step toward generative invention of soft machines, with the potential to accelerate design cycles, discover novel devices, and provide building blocks for downstream applications in automated computational creativity and computer-assisted design.