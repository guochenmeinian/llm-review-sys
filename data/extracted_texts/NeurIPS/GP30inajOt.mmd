# Retraction-free optimization over the Stiefel manifold with application to the LoRA fine-tuning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Optimization over the Stiefel manifold has played a significant role in various machine learning tasks. Many existing algorithms either use the retraction operator to keep each iterate staying on the manifold, or solve an unconstrained quadratic penalized problem. The retraction operator in the former corresponds to orthonormalization of matrices and can be computationally costly for large-scale matrices. The latter approach usually equips with an unknown large penalty parameter. To address the above issues, we propose a retraction-free and penalty parameter-free algorithm, which lands on the manifold. A key component of the analysis is the convex-like property of the quadratic penalty of the Stiefel manifold, which enables us to explicitly characterize the penalty parameter. As an application, we introduce a new algorithm, Manifold-LoRA, which employs the landing technique and a carefully designed step size strategy to accelerate low-rank adaptation (LoRA) in fine-tuning large language models. Numerical experiments on the benchmark datasets demonstrate the efficiency of our proposed method.

## 1 Introduction

Optimization over the Stiefel manifold has attracted considerable attention in the context of machine learning, e.g., RNN , batch normalization , and distributionally robust optimization . The mathematical formulation of this class of problems is:

\[_{X^{d r}}\ \ f(X)\ \ \ \ \ X(d,r):=\{X^{d r}:X^{}X=I_{d}\}, \]

where \(r d\) and \(f:^{d r}\) is a continuously differentiable function. The most popular methods for solving (1) are retraction-based algorithms, which have been extensively studied in the context of manifold optimization [2; 23; 6]. Recently, to alleviate the possible computational burden of the retraction operator, some retraction-free methods have been developed in [19; 18; 41; 1]. The ideas in these papers are based on a combination of the manifold geometry and a penalty function for the manifold constraint, which involves an unknown but sufficiently large penalty parameter. For large-scale machine learning applications, retraction-free algorithms are preferred. However, designing retraction-free algorithms with a known penalty parameter for solving (1) remains a challenge.

Another motivation for studying retraction-free methods arises from its application in the fine-tuning of large language models (LLMs). Recently, LLMs have revolutionized the field of natural language processing (NLP), achieving unprecedented performance across various applications [33; 32]. To tailor pretrained LLMs for specific downstream tasks, the most common approach is full fine-tuning, which requires prohibitively large computational resources due to the need to adapt all model weights, hindering the deployment of large models. As a result, parameter-efficient fine-tuning (PEFT) has gained widespread attention for requiring few trainable parameters while delivering comparableor even superior results to full fine-tuning. This paradigm involves inserting learnable modules or designating only a small portion of weights as trainable, keeping the main model frozen . Among fine-tuning methods, low-rank adaptation (LoRA)  has become the de factor standard among parameter-efficient fine-tuning techniques. It assumes that the change in weights lies in a "low intrinsic dimension", thereby modelling the update \( W^{d m}\) by two low-rank (not greater than a small integer \(r\)) matrices \(A^{r m}\) and \(B^{d r}\), i.e., \( W=BA\). Since \(r d\), the requirements on both storage and computation are significantly reduced. Due to its decompositional nature, there is redundancy in the representation of \( W\). Traditional optimization methods for LoRA do not exploit this redundancy, which consequently undermines model performance. Instead, we reformulate LoRA fine-tuning as an optimization problem over the product of Stiefel manifolds and Euclidean spaces. Therefore, we propose an algorithmic framework called Manifold-LoRA to accelerate the fine-tuning process and enhance model performance. Moreover, by exploiting projected gradients and incorporating a parameter-free penalty, the overhead that our method incurs is relatively negligible. Our contributions are as follows:

* We first prove the existence of explicit choice for the penalty parameter by establishing a strong convexity-like condition of the nonconvex penalty problem associated with the Stiefel manifold constraint. Furthermore, for the given penalty parameter, under mild conditions, we prove that the iterates of our proposed retraction-free gradient descent method eventually land on the Stiefel manifold and achieve the optimality of (1).
* Building upon the established landing theory of retraction-free and penalty parameter-free method and the AdamW framework, we proposed a new method, Manifold-LoRA, which employs a carefully designed step size strategy to accelerate the training process of fine-tuning. Compared with the conventional AdamW method, we use the penalized gradient instead of the usual gradient, and the computational overhead is negligible.
* Numerical experiments are conducted on a wide range of NLP tasks, demonstrating the efficiency of our algorithm. Specifically, compared to the vanilla LoRA, our Manifold-LoRA with half the trainable parameters not only delivers fast convergence but also yields improved generalization. In particular, Our method converges twice as fast as baseline methods on several typical datasets, including the SQuAD 2.0 dataset and the CoLA dataset.

### Related Work

**Optimization over the Stiefel manifold.** Optimization over the Stiefel manifold has attracted lots of attention due to its broad applications. Through the use of retraction, known as the generalization of the exponential map, the Riemannian gradient descent is proposed , where all iterates lie on the manifold. When such retraction is computationally costly, the authors  develop a retraction-free algorithm based on the augmented Lagrangian method. More recently, by defining the constraint dissolving operator and adding a sufficiently large penalty term, the authors  convert the manifold constrained problem (1) into an unconstrained problem and then apply unconstrained optimization algorithms. In , motivated by the convergence of the Oja's flow, a landing flow, consisting of the projected gradient and the gradient of the penalty function, is developed to retraction-free method for the squared Stiefel manifold, i.e., \(d=r\). All of these methods rely on an unknown penalty parameter to ensure the convergence. This motivates us to design penalty parameter-free algorithms, which could significantly reduce the need for tuning parameters in practical implementations.

**LoRA.** There are numerous variants of LoRA aiming to improve performance or reduce memory usage. AdaLoRA , a well-known successor, introduces the idea of adaptively adjusting the rank of different layers by incorporating an additional vector \(\) to serve as the diagonal of a singular value matrix. This approach leverages a revised sensitivity-based importance measure to decide whether to disable entries in vector \(\) and in matrices \(A\) and \(B\). A similar work, SoRA , adopts the same model architecture as AdaLoRA, but proposes a different way to update vector \(\) after training. This update rule is the proximal gradient of \(_{1}\) loss, acting as a post-pruning method. Additionally, a recently emerged method called VeRA  significantly reduces memory overhead while maintaining competitive performance. Based on the idea that networks with random initialization contain subnetworks that are near-optimal or optimal , VeRA only uses two frozen low-rank matrices shared by all layers, training scaling vectors unique to each layer. Although LoRA has gained significant popularity and various variants have been developed, the potential for efficient training through leveraging the manifold geometry to reduce redundancy has not been well-explored.

### Notation

For a matrix \(X^{d r}\), we use \(\|X\|\) to denote its Frobenius norm. For a squared matrix \(A^{d d}\), we define \((A)=}{2}\) and use \((A)^{d}\) to denote its diagonal part. For two matrices \(X,Y^{d r}\), we use \( X,Y:=_{i=1}^{d}_{j=1}^{r}X_{ij}Y_{ij}\) to denote their Euclidean inner product. For a differential function \(f:^{d r} d\), we use \( f(X)\) to denote its Euclidean gradient at \(X\).

## 2 Retraction-free and penalty parameter-free optimization over the Stiefel manifold

In this section, we focus on the design of retraction-free and penalty parameter-free algorithms for solving problem (1). We will first present the retraction-free algorithm and then show how the penalty parameter can be explicitly determined by characterizing the landscape of the penalty function.

### Retraction-free algorithms

Inspired by the retraction-free algorithms [19; 41; 1], we consider the following retraction-free gradient descent method for problem (1):

\[X_{k+1}=X_{k}-f(X_{k})- X_{k}(X_{k}^{}X_{k}-I_{d}), \]

where \(,>0\) are step sizes and the projected gradient \(f(X_{k}):= f(X_{k})-X_{k}(X_{k}^{} f (X_{k}))\). Note that the tangent space of \((d,r)\) is \(T_{X_{k}}(d,r):=\{^{d r}:X_{k}^{}+ ^{}X_{k}=0\}\). Then, for \(X_{k}(d,r)\), \(f(X_{k})\) is the projection of the Euclidean gradient \( f(X_{k})\) to the tangent space, i.e., \(f(X_{k})=_{T_{X_{k}}(d,r)}( f(X_{k}))\). Note that the term \(X_{k}(X_{k}^{}X_{k}-I_{d})\) is exactly the gradient of the following quadratic penalty function

\[(X):=\|X^{}X-I\|^{2}.\]

As will be shown in our theorem, the use of the projected gradient is essential for landing on the manifold. This differs with the usual penalty method, which optimizes \(f(X)+(X)\) using the update \(X_{k+1}=X_{k}- f(X_{k})- X_{k}(X_{k}^{}X_{k}-I_{d})\), needs \(\) to guarantee the feasibility.

### Explicit choice for the penalty parameter

It is known that a large penalty parameter yields better feasibility [29; Chapter 17]. To make the iterative scheme (2) be penalty parameter-free, we need a careful investigation on the landscape of the following optimization problem:

\[_{X^{d r}}\ \ (X). \]

It can be easily verified that problem (3) is nonconvex and its the optimal solution set is \((d,r)\). The key of obtaining an explicit formula of \(\) is to establish certain strong convexity-type inequality and show the gradient descent method with step size \(\) has linear convergence.

For any \(X(d,r)\), let us denote \(:=_{(d,r)}(X)\). Let \(X=USV^{}\) be the singular value decomposition with orthogonal matrices \(U^{d r},V^{d d}\) and diagonal matrix \(S^{d d}\), then \(=UV^{}\). Building on these notations, we demonstrate that problem (3) satisfies the restrict secant inequality (RSI) , which serves as an alternative to the strong convexity in the linear convergence analysis of gradient-type methods.

**Lemma 1**.: _For any \(X^{d r}\) with \(\|X-\|\), we have_

\[(X),X-\|X-\|^{2}. \]

With the above RSI, we have the linear convergence of the gradient descent update for (3), i.e.,

\[X_{k+1}=X_{k}-(X_{k}). \]

**Lemma 2**.: _Let the sequence \(\{X_{k}\}\) be generated by (5) with \(=\). Suppose that \(\|X_{0}-_{0}\|\). We have_

\[\|X_{k+1}-_{k+1}\|^{2}\|X_{k}-_{k}\|^{2}. \]

The proofs of Lemmas 1 and 2 can be found in Appendix B.

### Landing on the Stiefel manifold

Building on the established linear convergence of gradient descent for problem (3), we are now able to show that the iterates generated by (2) will land on the Stiefel manifold eventually, and the limiting point is a stationary point of (1), i.e., \(f(X_{})=0\).

Let us start with the Lipschitz continuity of \(f(X)\). For any \(X_{(d,r)}()\), we define \(_{T_{X}(d,r)}(U)=U-X(X^{}U)\) for \(U^{d r}\). We first have the following quadratic upper bound on \(f\) from its twice differentiability and the compactness of \((d,r)\).

**Lemma 3**.: _There exists a constant \(L>0\) such that for any \(X,Y(d,r)\), the following quadratic upper bound holds:_

\[f(Y) f(X)+f(X),Y-X+\|Y-X\|^{2}. \]

_In addition, there exists a constant \(>0\) such that for any \(X(d,r),Y U_{}()\),_

\[\|f(X)-f(Y)\|\|X-Y\|. \]

By the linear convergence result in Lemma 2, we have the following bound on the feasibility error.

**Lemma 4**.: _Let \(\{X_{k}\}\) be the sequence generated by (2) with \(=\) and \(\|X_{0}-_{0}\|\). We have_

\[\|X_{k+1}-_{k+1}\|\|X_{k}-_{k}\|+\|f(X_{k})\|. \]

The following one-step descent lemma on \(f\) is crucial in establishing the convergence.

**Lemma 5**.: _Let \(\{X_{k}\}\) be the sequence generated by (2) with \(=\) and \(\|X_{0}-_{0}\|\). We have_

\[f(_{k+1})-f(_{k}) -(-(4^{2}+4L+1)^{2})\|f(X_{k})\|^{ 2}+\|X_{k+1}-_{k+1}\|^{2} \] \[+(4_{f}+16^{2}+16L+3)\|X_{k}- _{k}\|^{2}.\]

From the above lemma, the one-step descreasing on \(f\) is related to both the gradient norm of \(f\) and the feasibility error. In terms of convergence, we need both \(f(X_{k})\) and \(\|X_{k}^{}X_{k}-I\|\) converge to 0. The following theorem demonstrates that the retraction-free and penalty parameter-free update (2) converges.

**Theorem 1**.: _Let \(\{X_{k}\}\) be the sequence generated by (2) with \(=\) and \(\|X_{0}-_{0}\|\). If the step size \(<}\) for some \(c_{1}\) large enough, then we have_

\[_{k=0,,K}\|f(X_{k})\|^{2},_{k=0, ,K}\|X_{k}^{}X_{k}-I\|^{2}. \]

The proofs of the above lemmas and theorem are presented in Appendix B.

## 3 Accelerate LoRA fine-tuning with landing

In this section, we will first clarify where the Stiefel manifold constraint comes from in the LoRA fine-tuning. Then, we will apply the above developed retraction-free and penalty parameter-free method to enhance LoRA fine-tuning.

### Manifold optimization formulation of LoRA fine-tuning

In neural networks, the dense layers perform matrix multiplication, and the weight matrices in these layers usually have a full rank. However, when adapting to a specific task, pre-trained language models have been shown to have a low intrinsic dimension, allowing them to learn efficiently even with a random projection to a smaller subspace. One possible drawback in the current LoRA fine-tuning framework is that the low-rank decomposition \( W\) into product \(BA\) is not unique. Specifically, for any invertible matrix \(C\), it holds that \(BA=(BC)(C^{-1}A)\). Note that \(BC\) shares the samecolumn space with \(B\). This suggests us optimizing the subspace generated by \(B\) instead of \(B\) itself. Numerous studies in the field of low-rank optimization, e.g., [7; 13; 12], investigate the manifold geometry of the low-rank decomposition and develop efficient algorithms. However, such geometry has not been explored in the LoRA fine-tuning.

To address such redundancy (i.e., the non-uniqueness of \(BA\) representations), we regard \(B\) as the basis through the manifold constraint and \(A\) as the coordinate of \( W\) under \(B\). Hence, the optimization problem can be formulated as

\[_{A,B} L(BA),\ B(d,r)\; \;B(d,r), \]

where \((d,r):=\{B^{d r}:(B^{}B)=\}\). Compared to the Stiefel manifold \((d,r)\), the oblique manifold \((d,r)\) necessitates that the matrix \(B\) has unit norms in its columns, without imposing requirements for orthogonality between the columns. Problem (12) is an optimization problem over the product of manifolds and Euclidean spaces.

### Manifold-LoRA

The retraction-free method is well-suited to address (12), simultaneously minimizing the loss function \(L(BA)\) and constraint violation. To control the constraint violation, we use the quadratic penalties \(R_{s}(B):=\|B^{}B-I\|^{2}\) and \(R_{o}(B):=\|(B^{}B)-1\|^{2}\) for the Stiefel manifold and oblique manifold, respectively. As shown in the landing theory in Section 2, we shall use the projected gradient of the loss part instead of the Euclidean gradient. For the Stiefel manifold and the oblique manifold, the respective projected gradients are

\[_{B}L(BA)=_{B}L(BA)-B(B^{}_{B}L(BA)) \]

and

\[_{B}L(BA)=_{B}L(BA)-B((B^{} _{B}L(BA))), \]

where \((X):=(X+X^{})/2\). Thus, the gradients of our retraction-free method for \(A\) and \(B\) are \(_{A}L(BA)\) and \(_{B}L(BA)+ R_{s}(B)(\; R_{o}(B))\).

Note that \(B\) and \(A\) represent the basis and the coordinate of \( W\), respectively. This results in different magnitudes and different Lipschitz constants of their gradient function. In fact, let \(X=BA\). It follows

\[_{A}L(BA)=B^{}_{X}L(X),_{B}L(BA)=_{X}L(X)A^{ }.\]

Then,

\[\|_{A}L(BA_{1})- L(BA_{2})\|\|B\|_{2}L_{g}\|A_{1}-A_{2}\|,\]

\[\|_{B}L(B_{1}A)- L(BA_{2}A)\|\|A\|_{2}L_{g}\|B_{1}-B_{2}\|,\]

where \(L_{g}\) is the Lipschitz constant of \(_{X}L(X)\) and \(\|\|_{2}\) represent the matrix \(_{2}\) norm (i.e., the largest singular value). Note that the step size generally should be propositional to the reciprocal of Lipschitz constant for the gradient type algorithms [29; 5]. Hence, we schedule the learning rates for the two matrices based on their respective \(_{2}\) norms. Having prepared the above, we incorporate the AdamW optimizer  with our manifold-accelerated technique to enhance the LoRA fine-tuning, as presented in Algorithm 1.

```
1:Input:\(B\), \(_{2}\), \(_{1}\), \(_{2}\), \(_{3}\), \(_{4}\), \(_{5}\), \(_{6}\), \(_{7}\), \(_{8}\), \(_{9}\), \(_{10}\), \(_{11}\), \(_{12}\), \(_{13}\), \(_{14}\), \(_{15} with that of the LoRA method and the new layers are inserted into the attention layer and feed-forward layer. The update of LoRA is scaled by a hyper-parameter \(\). This value is typically left unmodified, as it is usually set as 16 or 32 and never tuned [22; 43]. The exponential moving average parameters \(_{1}\) and \(_{2}\) of AdamW  are set to their default values of 0.9 and 0.999, respectively. All the experiments are conducted on NVIDIA A800 GPUs. More details are presented in Appendix C.

### Natural language understanding

We first evaluate our backbone model DeBERTaV3-base  on GLUE  benchmark containing nine sub datasets, including MNLI , SST-2 , CoLA , QQP , QNLI , RTE , MRPC , and STS-B .

Experimental results of the GLUE dataset are recorded in Table 1. It can be seen that our method is consistently superior to other baselines. Notably, for RTE and STS-B datasets, both sphere-constrained (i.e., oblique manifold-constrained) and Stiefel-constrained have an obvious performance gain even with only half the trainable parameters compared to the LoRA baseline, i.e., Sphere\({}_{r=8}\) and Stiefel\({}_{r=8}\) beat LoRA\({}_{r=16}\). In addition, with the help of manifold geometry, the fine-tuning process can be significantly accelerated compared to the vanilla AdamW optimizer, achieving a lower training loss, as shown in Figure 1. Particularly on the CoLA dataset presented in Figure 0(a), our approach achieves the same training loss as the standard Adam optimizer but requires nearly half the number of epochs.

### Question Answering

We conduct an evaluation on two question answering datasets: SQuAD v1.1  and SQuADv2.0 . Manifold-LoRA is used to fine-tune DeBERTaV3-base for these tasks, which are treated as sequence labeling problems predicting the probability of each token as the start or end of an answer span.

The main experimental results are presented in Table 2. For LoRA and our algorithms, new layers are inserted into \(W_{q},W_{k},W_{v},W_{o},FC_{1},FC_{2}\). Notably, both manifold-regularized LoRA variants consistently outperform all fine-tuning methods. Additionally, we plot the training loss, evaluation exact match, and evaluation F1 scores against epochs in Figure 2. We conclude that the proposed Manifold-LoRA method achieves a 2x speed-up in training epochs compared to AdamW, while simultaneously improving model performance. We also illustrate the heat map of \(B^{}B\) in Figure 3, which indicates that the matrix \(B\) lands on the manifold eventually. This supports our assertion that landing on manifold enhances the performance of LoRA.

### Natural Language Generation

The E2E NLG Challenge, as introduced by Novikova, provides a dataset for training end-to-end, data-driven natural language generation systems, widely used in data-to-text evaluations. The E2E dataset comprises approximately 42,000 training examples, 4,600 validation examples, and 4,600 test examples, all from the restaurant domain. We test our method on the E2E dataset using GPT-2 Medium and Large models, following the experimental setup outlined by LoRA . For LoRA, we set the hyperparameters to match those specified in the original paper.

The results from the E2E dataset are recorded in Table 3, where we focus on comparing LoRA and Manifold-LoRA. The results clearly indicate that our proposed algorithm outperforms the established baselines. Also, as shown in Figure 4, the matrix \(B\) resides on the manifold even at the early training stage, validating the feasibility of our method.

   Method \# Params & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\  & m / mm & Acc & Mcc & Acc / F1 & Acc & Acc & Acc & Corr & Ave. \\  Full & \(184.42\)M & \(90.45\)**/90.60** & \(95.48\) & \(68.17\) & **91.99/89.12** & \(93.60\) & \(79.28\) & \(88.93\) & \(90.92\) & \(87.85\) \\ FT & \(0.61\)M & \(90.13\)/90.16 & \(94.86\) & \(69.37\) & \(91.38\)/\(88.46\) & \(93.54\) & \(81.87\) & \(89.12\) & \(91.52\) & \(88.06\) \\ BitFit & \(0.06\)M & \(87.08\)/\(86.39\) & \(94.88\) & \(69.11\) & \(87.96\)/\(84.35\) & \(92.19\) & \(76.52\) & \(87.06\) & \(90.96\) & \(85.65\) \\ LoRA\({}_{r=8}\) & \(0.30\)M & \(90.20\)/\(90.08\) & \(94.93\) & \(68.14\) & \(90.78\)/\(87.68\) & \(93.85\) & \(80.15\) & \(90.40\) & \(90.29\) & \(87.60\) \\ LoRA\({}_{r=16}\) & \(0.59\)M & \(90.44\)/\(90.12\) & \(95.41\) & \(68.19\) & \(90.92\)/\(87.77\) & \(94.00\) & \(Figure 3: The heat map of \(B^{}B\) with the Stiefel manifold (the first and second rows) and the oblique manifold (the third and fourth rows) at the end of training on SQuADv2.0 dataset.

   Methods & Params & SQuADv1.1 & SQuADv2.0 \\  Full FT & 184M & 86.30 / 92.85 & 84.30 / 87.58 \\ Adapter\({}_{r=16}\) & 0.61M & 87.46 / 93.41 & 85.30 / 88.23 \\ Adapter\({}_{r=32}\) & 1.22M & 87.53 / 93.51 & 85.42 / 88.36 \\ Biffi & 0.07M & 80.26 / 88.79 & 74.21 / 87.19 \\ LoRA\({}_{r=8}\) & 1.33M & 87.90 / 93.88 & 85.56 / 88.52 \\ LoRA\({}_{r=16}\) & 2.65M & 87.94 / 93.75 & 85.90 / 88.81 \\ Sphere\({}_{r=8}\) & 1.33M & 88.51 / **94.25** & 86.33 / 89.20 \\ Sphere\({}_{r=16}\) & 2.65M & 88.32 / 94.03 & 86.15 / 89.03 \\ Stiefel\({}_{r=8}\) & 1.33M & **88.68** / 94.23 & 86.35 / 89.09 \\ Stiefel\({}_{r=16}\) & 2.65M & 88.25 / 94.04 & **86.41** / **89.22** \\   

Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. We report EM/F1. The best results in each setting are shown in **bold**.

Figure 2: The figures compare the training loss, evaluation exact match, and evaluation F1 metrics against the number of epochs for the SQuADv2.0 dataset.

## 5 Conclusion

Optimization over the Stiefel manifold has been widely used in machine learning tasks. In this work, we develop a retraction-free and penalty parameter-free gradient method, and prove that the generated iterates eventually land on the manifold and achieve the optimality simultaneously. We then apply this landing theory to avoid the possible redundancy of LoRA fine-tuning in LLMs. Specifically, we reformulate the LoRA fine-tuning as an optimization problem over the Stiefel manifold, and propose a new algorithm, Manifold-LoRA, which incorporates a careful analysis of step sizes to enable fast training using the landing properties. Extensive experimental results demonstrate that our approach not only accelerates the training process but also yields significant performance improvements.

Our study suggests several potential directions for future research. Although the established landing theory focuses on the Stiefel manifold, extending this theory to general manifolds is one potential direction. Additionally, evaluating the performance of Manifold-LoRA on LLMs with billions of parameters would be valuable. Due to the heterogeneity of different layers, incorporating adaptive ranks for \( W\) across different layers is another possible direction. This may be achievable by adding sparsity regularization to the coordinate matrix \(A\).

   Model & Parameters & BLEU & NIST & MET & ROUGE-L & CIDEr \\  GPT-2 M (FT)* & 354.92M & 68.2 & 8.62 & 46.2 & 71.0 & 2.47 \\ GPT-2 M (Adapter\({}^{ L}\))* & 0.37M & 66.3 & 8.41 & 45.0 & 69.8 & 2.40 \\ GPT-2 M (Adapter\({}^{ L}\))* & 11.09M & 68.9 & 8.71 & 46.1 & 71.3 & 2.47 \\ GPT-2 M (Adapter\({}^{ H}\))* & 11.09M & 67.3\({}_{.6}\) & \(8.50_{.07}\) & \(46.0_{.2}\) & \(70.7_{.2}\) & \(2.44_{.01}\) \\ GPT-2 M (FT\({}^{ Ho}\))* & 25.19M & 68.1 & 8.59 & 46.0 & 70.8 & 2.41 \\ GPT-2 M (PrLex)* & 0.35M & 69.7 & 8.81 & 46.1 & 71.4 & 2.49 \\ GPT-2 M (LoRA) & 0.35M & 68.9 & 46.5 & 71.5 & 2.51 \\ GPT-2 M(Stiefel) & 0.35M & 70.1 & 8.82 & **46.8** & **71.7** & **2.53** \\ GPT-2 M(Sphere) & 0.35M & **70.3** & **8.83** & 46.7 & **71.7** & 2.52 \\  GPT-2 L (FT)* & 774.03M & 68.5 & 8.78 & 46.0 & 69.9 & 2.45 \\ GPT-2 L (Adapter\({}^{ L}\))* & 0.88M & \(69.1_{.1}\) & \(8.68_{.03}\) & \(46.3_{.0}\) & \(71.4_{.2}\) & \(2.49_{.0}\) \\ GPT-2 L (Adapter\({}^{ L}\))* & 23.00M & \(68.9_{.3}\) & \(8.70_{.04}\) & \(46.1_{.1}\) & \(71.3_{.2}\) & \(2.45_{.02}\) \\ GPT-2 L (PreLayer)* & 0.77M & 70.3 & 8.85 & 46.2 & 71.7 & 2.47 \\ GPT-2 L (LoRA) & 0.77M & 70.1 & 8.82 & 46.7 & 72.0 & 2.53 \\ GPT-2 L(Stiefel) & 0.77M & 70.4 & 8.86 & **46.8** & 72.1 & 2.53 \\ GPT-2 L(Sphere) & 0.77M & **70.9** & **8.92** & **46.8** & **72.5** & **2.55** \\   

Table 3: GPT-2 medium (M) and large (L) models were evaluated on the E2E NLG Challenge. * denotes results from previously published works.

Figure 4: The heat map of \(B^{}B\) with the Stiefel manifold (left) and the oblique manifold (right) on E2E dataset.