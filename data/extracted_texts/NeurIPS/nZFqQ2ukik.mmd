# Topology Preserving Regularization for Independent Training of Inter-operable Models

Nicolas Zilberstein

Rice University

nzilberstein@rice.edu

&Akshay Malhotra

InterDigital Communications Inc.

akshay.malhotra@interdigital.com

Shahab Hamidi-Rad

InterDigital Communications Inc.

Shahab.Hamidi-Rad@interDigital.com

&Yugeswar Denoo

InterDigital Communications Inc.

yugeswar.deenoo@interDigital.com

Work done during an internship at InterDigital

###### Abstract

Developing schemes to enable zero-shot stitching between different neural networks with minimal or no information exchange has become increasingly important in the era of large and powerful pre-trained models. Considering the example of an autoencoder based data compression framework, having the ability to select the architecture and train an encoder model completely independently of the decoder model while ensuring interoperability between them can revolutionize how these models are developed, deployed, and maintained. In this work, we propose a novel approach that utilizes topological regularizations to align the latent spaces of two different autoencoder models that can be trained independently, without coordination. Our solution introduces two distinct training schemes: _Data2Latent_ and _Latent2Latent_. The _Data2Latent_ scheme focuses on preserving the topological structure of the input data in the latent space, while the _Latent2Latent_ scheme preserves the latent space of a pre-trained, unconstrained model. Through numerical experiments in reconstruction tasks, we demonstrate that our approach yields a near-optimal solution, closely approximating the performance of an end-to-end model.

## 1 Introduction

Compressing data into meaningful low-dimensional representations is a long-standing challenge with applications in image compression [2; 20], audio compression , wireless communication [11; 24], and more. Neural networks address this by training an encoder-decoder pair, where the intermediate representation serves as compressed data. The decoder then reconstructs the original data, and the pair is trained end-to-end.

Recent works have studied the _zero-shot stitching_ problem  in the context of autoencoders, which involves interconnecting encoders and decoders trained independently using limited datasets. In , the authors proposed a method that maps the latent representation of each autoencoder to a relative space. However, this approach requires a specialized decoder trained in the relative representation, which is a significant drawback as it necessitates a large amount of data. More recent methods [14; 19] suggest that a linear transformation might suffice to align latent spaces. However, a significant performance gap remains between end-to-end autoencoders and interconnected models when trained on finite, small datasets, suggesting that linear transformations alone are insufficient.

We hypothesize that this performance gap arises due to a lack of geometric similarity between the latent spaces. The encoder's nonlinear mapping from input data to the latent space is often biased by the network architecture, training parameters and weight initializations, making the latent spaces of independently trained autoencoders incompatible when using simple linear transformations.

To address this, we propose preserving the topological features of the input data within the latent space. This regularization encourages architecture-agnostic latent spaces, making them easier to align. Our approach quantitatively measures the dataset's underlying topology and employs it as a regularizer in the training of interoperable autoencoder models. In particular, we build upon the topological regularizer proposed in . Specifically, this regularization aligns the minimum spanning trees of the source (input data) and the target (latent space), leveraging persistent homology to preserve topological features2. We propose two training frameworks termed _Data2Latent_ (Fig. 1) and _Latent2Latent_ (Fig. 2).

**Contributions.** The contributions of this paper are twofold:

1) We propose two training frameworks termed _Data2Latent_ and _Latent2Latent_. Each one exploits the topological regularization differently: while the former aims to preserve the structure of the input data, the second one aims to preserve the topological structure of the latent space of unconstrainedly trained autoencoder.

2) Through numerical experiments we show that incorporating topological information facilitates the zero-shot stitching operation with a simple linear transformation.

## 2 Alignment with topological constraints

Direct alignment between latent spaces.Given two autoencoders \(_{1}\) and \(_{2}\) with latent spaces \(_{1}\) and \(_{2}\) respectively, we seek a method to align their latent space _with minimal interaction_ between the models; the ultimate goal is to interconnect the encoder \((.)\) of \(_{1}\) (\(_{2}\)) with the decoder \((.)\) of \(_{2}\) (\(_{1}\)). Based on the assumption that latent spaces of models trained independently tend to be similar, the authors in [14; 19] propose to estimate a transformation \((.)\) by minimizing the mean square error as follow 3

\[(.)=(.)}{}\| _{1}-(_{2})\|^{2}\] (1)

Although these works claimed that a linear transformation is enough to align the latent spaces of independently trained autoencoders \(_{1}\) and \(_{2}\), the gap in terms of reconstruction error between

Figure 1: **Data2latent scheme**: a,b) _Topological autoencoders._ In this case, we regularize the training of each autoencoder \(_{1},_{2}\) with the input data, preserving its topological structure in the latent space independently of the architecture of the encoder. As an example, we consider a 2D synthetic dataset with 5 classes, its corresponding latent space when considering a bottleneck of dimension two and the topological loss, and the output data which has the same shape as the input. Notice that each autoencoder maps the input data to a different latent space, but both latent spaces preserves the topology up to a rotation and a stretching. c) _Stitching between two regularized autoencoders._ The transformation is simplified, leading to a linear transformation (rotation).

the end-to-end autoencoder and the interconnection is still significant. This difference highlights that the linear assumption for interconnection is insufficient. While a non-linear transformation could potentially improve the alignment , it requires a large number of samples from \(_{1}\) and \(_{2}\), violating our goal of minimizing interaction between the models. To cope with this challenge, we propose to incorporate a geometric regularization to _linearize_ the relationship.

Topological regularizations aid similar latent spaces.We hypothesize that the relationship between the latent spaces can be linear, but achieving this requires an additional constraint. Specifically, we facilitate alignment by incorporating a geometric regularizer that enforces similar topology between both latent spaces. In particular, we aim to make both latent spaces share the same topological features. A natural way to achieve this is by preserving the _shape_ of the data in each latent space, which can be done using the topological loss introduced in . In essence, this loss function aligns the minimum spanning tree between a source space and the learnable space by leveraging algebraic topology tools, particularly persistent homology; see Appendices B and C for more details on persistent homology and the topological loss. We incorporate this topological regularization and propose two training schemes that constrain the latent space:

1) _Data2latent_: In this first strategy, we regularize the latent space of both autoencoders so that each latent space retains the same connectivity and topological information as the input space. Formally, we define the loss function as

\[_{D2L}=\|-(())\|^{2}+ _{topo}(,())\] (2)

where \(_{topo}(.)\) is defined in (4) and \(\) is a regularization constant. By doing this, and assuming both autoencoders are trained on the same dataset, we constrain the latent spaces to share the same topological structure as the input space, thereby inducing similarity between them. As a consequence, the transformation between \(_{1}\) and \(_{2}\) becomes simpler.

2) _Latent2latent_: In this second strategy, we assume an autoencoder \(_{1}\) has been trained without any constraints. Then, given a subset of the training data, \(_{}\) and the corresponding encoded vectors \(\{_{1}()|_{}\}\), we train \(_{2}\) to minimize the following loss function

\[_{L2L}=\|-_{2}(_{2}())\| ^{2}+_{topo}(_{1}(),_{2}( ))\] (3)

where the topological loss is evaluated only on \(_{}\). Intuitively, during training, we are constraining \(_{2}\) to share the topological structure with \(_{1}\), harmonizing their topological features and simplifying the transformation between them.

Once both models are trained, we estimate the transformation \((.)\) by minimizing (1) using a small subset of data points from the dataset denoted as \(_{T}\). We consider a linear transformation \((_{i})=_{i}\).

## 3 Results

We present the results of the proposed scheme on two datasets, MNIST  and Fashion MNIST . We utilize two completely different model architectures to show the robustness of the proposed

Figure 2: **Latent2latent scheme. In this case, we regularize the training of second autoencoder \(_{2}\) with the latent space of _pre-trained_, _unconstrained_ autoencoder \(_{1}\). We compute the topological loss using a pre-defined set \(_{topo}\).**

scheme for the model architectures of the two autoencoders. For the first autoencoder (AE1), a feed-forward architecture is utilized, and for the second autoencoder (AE2), a convolutional neural network architecture is utilized. For MNIST, we consider a bottleneck of 128, while for FashionMNIST 250. Details on the model architecture, hyperparameters, and training can be found in Appendix A.1.

### Comparison with other methods

As baselines, we consider the direct alignment with a linear transformation between two unconstrained autoencoders  and the relative representation, where a specialized decoder is trained to handle the relative representation . For the direct alignment and our methods, we use 500 samples to estimate the linear transformation, while for the relative representation, we use 1000 anchor points. The normalized mean-square error (NMSE)4 is shown in Table 1, while qualitatively results are shown in Appendix A.2 in Figs. 4a and 4b. For the quantitative results, we average over five trials with different seeds. The topological constraint harmonizes the latent space, making them easy to stitch: the topological autoencoder achieves an NMSE almost as good as its upper bound (\(L_{11}\)) for FusionMNIST.

### Ablation

Finally, we do an ablation study of our proposed methods compared to the linear transformation between unconstrained autoencoders . We compare the performance when increasing the size of samples for the linear transformation. For the latent2latent training, we consider \(|_{topo}|=20\). The results for \(|_{T}|=\{10,100,200,500\}\) and MNIST is shown in Fig. 3a, while for the FashionMNIST is in 3b. We observe that for \(|D_{T}| 200\), the performance of the interconnection between autoencoders gets closer to the upper bound, while the gap between the non-regularized case remains large. Notice that increasing the size of the bottleneck (recall for for FashionMNIST we consider a bottleneck of 250) improves the performance of our methods, achieving a near-optimal performance.

## 4 Conclusion

We study the problem of zero-shot stitching in autoencoders, showing that topological regularization aids a simplified relationship (linear) between two latent spaces. Our proposed solution encompasses two different training schemes: _Data2Latent_ and _Latent2Latent_, which aim to preserve the topological structure of the input data and the latent space of an unconstrained autoencoder, respectively. Through numerical experiments on MNIST and FashionMNIST, we showed that our method simplifies dramatically the relationship between two latent spaces. Future work includes studying alternatives geometric losses to align topological features, formalizing the trade-off between performance and samples for the topological loss , which is particularly relevant for for the _Latent2Latent_ scheme, and expanding the experiments on more complex datasets, such as ImageNet, and of different modalities, such as language and wireless data.

   Method & MNIST & FashionMNIST \\  \(L_{11}\) (MLP-MLP) & \(0.020\) & \(0.018\) \\ \(L_{22}\) (CNN-CNN) & \(0.014\) & \(0.010\) \\ \(L_{21}\) (CNN-MLP) & \(0.923 0.002\) & \(0.644 0.061\) \\ Direct alignment (CNN-MLP)  & \(0.611 0.081\) & \(0.193 0.03\) \\ Rel. representation  & \(0.225 0.003\) & \(0.100 0.003\) \\  Data2Latent (CNN-MLP) (ours) & \(0.055\) & \(0.023 0.003\) \\ Latent2Latent (CNN-MLP) (ours) & \(0.065 0.001\) & \(0.028 0.002\) \\   

Table 1: NMSE for related and our proposed methods on MNIST and FashionMNIST. \(L_{11}\) and \(L_{22}\) denote the NMSE of decoding using \(_{1}\) (MLP) and \(_{2}\) (CNN) respectively, while \(L_{21}\) is the NMSE of decoding using directly (\(=\)) the encoder from \(_{2}\) with the decoder from \(_{1}\). We do not include the standard deviation when it is in the fourth decimal place. We consider the average accross 4 trials with different seeds.