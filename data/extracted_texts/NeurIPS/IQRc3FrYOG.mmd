# Mutual-Information Regularized Multi-Agent

Policy Iteration

Jiangxing Wang

School of Computer Science

Peking University

jiangxiw@stu.pku.edu.cn &Deheng Ye

Tencent Inc.

dericye@tencent.com &Zongqing Lu

School of Computer Science

Peking University

BAAI

zongqing.lu@pku.edu.cn

Corresponding Author

###### Abstract

Despite the success of cooperative multi-agent reinforcement learning algorithms, most of them focus on a single team composition, which prevents them from being used in more realistic scenarios where dynamic team composition is possible. While some studies attempt to solve this problem via multi-task learning in a fixed set of team compositions, there is still a risk of overfitting to the training set, which may lead to catastrophic performance when facing dramatically varying team compositions during execution. To address this problem, we propose to use mutual information (MI) as an augmented reward to prevent individual policies from relying too much on team-related information and encourage agents to learn policies that are robust in different team compositions. Optimizing this MI-augmented objective in an off-policy manner can be intractable due to the existence of dynamic marginal distribution. To alleviate this problem, we first propose a multi-agent policy iteration algorithm with a fixed marginal distribution and prove its convergence and optimality. Then, we propose to employ the Blahut-Arimoto algorithm and an imaginary team composition distribution for optimization with approximate marginal distribution as the practical implementation. Empirically, our method demonstrates strong zero-shot generalization to dynamic team compositions in complex cooperative tasks.

## 1 Introduction

The cooperative multi-agent reinforcement learning (MARL) problem has attracted the attention of many researchers for being a well-abstracted model for many real-world problems, such as traffic signal control (Wang et al., 2021), autonomous warehouse (Zhou et al., 2021), and even AutoML (Wang et al., 2022) as the feedback to the machine learning community. In a cooperative MARL problem, we aim to train a group of agents that can cooperate to achieve a common goal. Such a common goal is often defined by a global reward function that is shared among all agents. Although this objective is naturally centralized, we want agents to be able to execute in a fully decentralized manner. Under such a requirement, Kraemer and Banerjee (2016) proposed the centralized training with decentralized execution (CTDE) framework, where a centralized critic is learned to evaluate the performance of the joint policy in terms of the global reward and a group of decentralized individual policies are learned via the centralized critic to realize decentralized execution.

With a centralized critic, multi-agent policy gradient methods directly use it to guide the update of each decentralized individual policy. Based on this idea, a series of studies (Lowe et al., 2017; Kuba et al., 2022; Yu et al., 2022; Ye et al., 2020) have been proposed with different optimizationtechniques for policy improvement. On the other hand, as the centralized critic is used to guide the learning of decentralized individual policies, many CTDE algorithms choose to factorize the centralized critic into decentralized individual utilities via the mixer network. This line of research is called value decomposition (Sunehag et al., 2018). Based on different design choices of the mixer network, a variety of value decomposition methods (Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2020; Zhang et al., 2021) has been proposed and achieved great success in cooperative MARL problems.

Despite the success of CTDE methods, previous research mainly focuses on training agents under a fixed team composition, which has been shown to exhibit serious overfitting issues (Wen et al., 2022), and leads to catastrophic performance when facing unseen team compositions. One natural way to solve this problem is to introduce multiple team compositions during training to alleviate overfitting. However, due to the existence of the mixer network and the network structure of individual utilities and policies, such a strategy cannot be simply applied to many existing CTDE methods.

REFIL (Iqbal et al., 2021) attempts to address this problem and proposes to use multi-head attention (Vaswani et al., 2017) in the mixer network and individual utilities to handle dynamic team compositions. It further proposes an imaginary objective based on random sub-group partitioning to accelerate the training process given a fixed set of team compositions. Although REFIL is able to handle dynamic team compositions, as it is still trained on a fixed set of team compositions, there is still a risk of overfitting to not a single team composition, but a set of team compositions, which has been demonstrated in several previous studies (Liu et al., 2021; Shao et al., 2022).

The overfitting issue can be attributed to the lack of robustness in different team compositions. When facing an arbitrary team composition during execution, one agent's observed information about team composition can arbitrarily vary from what it experienced during training. If the agent puts too much credit on this highly varied information to make decisions, it may fail to achieve robust behavior. As the variation of team composition is uncontrollable, one way to achieve robust behavior is to reduce the reliance on team-related information. Based on this intuition, we propose **MIPI** (**M**utual-**I**nformation Regularized Multi-Agent **P**olicy **I**eration), minimizing the mutual information between the policy of the agent and the team-related information to encourage robust behavior of each agent. Inspired by SAC (Haarnoja et al., 2018), we combine the global reward of environment and mutual information of each agent as a new objective and learn individual policies to optimize them at the same time. As the incorporation of mutual information imposes a challenge on optimization due to the existence of dynamic marginal distribution, we first propose a multi-agent policy iteration algorithm with a fixed marginal distribution and prove its convergence and optimality. Then, we propose to utilize the Blahut-Arimoto algorithm (Cover, 1999) and an imaginary team composition distribution for optimization under an approximate dynamic marginal distribution as the practical implementation.

To empirically justify our algorithm, we first evaluate the performance of MIPI in a simple yet challenging matrix game. Compared with the other two baselines using pure environmental reward and entropy-augmented reward, using mutual information as an augmented reward can help the agent find the policy that can achieve consistent performance across different team compositions. Then, we move to a more complicated scenario, StarCraft Micromanagement Tasks. While having the same level of performance in the training set, MIPI achieves better zero-shot generalization results in unseen team compositions during evaluation.

## 2 Related Work

### Centralized Training with Decentralized Execution (CTDE)

CTDE methods can be categorized into value decomposition and multi-agent policy gradient, depending on whether a centralized critic is decomposed or not. For value decomposition methods, a centralized critic is decomposed into decentralized utilities through the mixer network. Different mixers have been proposed as different interpretations of the Individual-Global-Maximum (Rashid et al., 2018) (IGM) principle or its equivalence, which ensures the consistency between optimal local actions and optimal joint action. VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018) give sufficient conditions for IGM by additivity and monotonicity, respectively. QPLEX (Wang et al., 2020) and FOP (Zhang et al., 2021) take advantage of duplex dueling architecture to guarantee IGM.

In multi-agent policy gradient, a centralized critic function is directly used to guide the update of each decentralized individual policy. Most multi-agent policy gradient methods can be considered as an extension of the policy gradient method from RL to MARL. For example, MAPPDG (Lowe et al., 2017) extends DDPG (Lillicrap et al., 2016), HATRPO (Kuba et al., 2022) extends TRPO (Schulman et al., 2015), MAPPO (Yu et al., 2022) and CoPPO (Wu et al., 2021) extend PPO (Schulman et al., 2017).

### Dynamic Team Composition

While classical CTDE methods mainly focus on fixed team compositions, in real-world applications, agents that can adapt to dynamic team composition are preferable. To address this problem, REFIL (Iqbal et al., 2021) incorporates multi-head attention (Vaswani et al., 2017) into the networks and further introduces an imaginary objective based on random sub-group partitioning to accelerate the training process on a fixed set of team compositions. While REFIL learns policies that can handle dynamic team compositions, studies (Liu et al., 2021; Shao et al., 2022) suggest that it generalizes poorly on unseen team compositions. The necessity of training agents that can generalize to unseen team compositions is evident using the automated warehouse as an example, where it is very common to add more agents (more agents got purchased) or delete some agents (some agents got broken). Therefore, agents have to deal with different teams in the application scenario, which can not be fully covered during training. In order to learn policies that can adapt to unseen team compositions, many studies choose to sacrifice the requirement of decentralized execution. For example, a centralized agent is assumed in COPA (Liu et al., 2021), which has a global view of the environment and coordinates agents by distributing individual strategies. In SOG (Shao et al., 2022), a communication channel is assumed to elect conductors, so that the corresponding groups are constructed with conductor-follower consensus. Unlike these methods, we do not assume any kind of centralization during execution, such that the ability of decentralized execution is fully preserved. In CollaQ (Zhang et al., 2020), the decentralized utility function is decomposed into two terms: the self-term that only relies on the agent's own state, and the interactive term that is related to states of nearby agents. By using an additional MARA loss to constrain the contribution of the interactive term in the decentralized utility function, CollaQ solves the generalization on the dynamic team composition problem with CTDE being preserved. Unlike this method, MIPI uses mutual information to constrain the contribution of team-related information in the agent's policy.

### Information-Theoretic Principles in RL

In the standard RL problem, the objective is to solely optimize the environmental reward. However, in many problems, we not only want to optimize the cumulative rewards but also want the learned policy to exhibit some other properties. Therefore, a line of research across single-agent and multi-agent domains has been proposed using different information-theoretic principles for policy optimization. For example, SQL (Haarnoja et al., 2017) and SAC (Haarnoja et al., 2018) incorporate the maximum entropy principle to encourage exploration and diverse behavior. FOP (Zhang et al., 2021) further extends this idea to MARL and proves its convergence and optimality. As the augmented entropy term distorts the original objective in the original MDP, which may lead to undesirable behavior in some scenarios (Eysenbach and Levine, 2019). To solve this problem, DMAC (Su and Lu, 2022) proposes to use the divergence between the current policy and previous policy to replace entropy, which yields a bound of the discrepancy between the converged policy and optimal policy in the original MDP. Using divergence to guide the policy optimization is also very popular in offline RL (Levine et al., 2020), for example, F-BRC (Kostrikov et al., 2021) and ICQ (Yang et al., 2021), where the divergence is used to control the similarity between learned policy and behavior policy. While the entropy and KL divergence can be seen as a measurement of the distance to a fixed policy, the mutual information is about the distance to a dynamic marginal policy. In MIRL (Grau-Moya et al., 2019) and MIRACLE (Leibfried and Grau-Moya, 2020), the environmental reward is combined with mutual information to encourage the learned policy to be close to an optimal prior policy, which is also dynamically learned during the RL process instead of being fixed throughout. Unlike (Grau-Moya et al., 2019; Leibfried and Grau-Moya, 2020) that aim at a generalized version of SAC in single-agent RL, the purpose of our work is to solve generalization on dynamic team composition in MARL.

Mutual information (MI) has been widely used in previous MARL research for various purpose, e.g., exploration (Mahajan et al., 2019; Wang et al., 2019; Zheng et al., 2021), coordination (Konan et al., 2021; Kim et al., 2023), individuality (Jiang and Lu, 2021), diversity (Li et al., 2021) and social influence (Jaques et al., 2019). Unlike these works that mainly focus on the performance of agents in a fixed team, we focus on the generalization ability of agents over different or even unseen teams. Also, these works mainly try to increase the mutual information between two variables to enhance the dependency between variables. However, in our work, we try to decrease the mutual information between the agent's policy and team-related information to reduce the dependency between these two variables and avoid overfitting. Being an exception, PMIC (Li et al., 2022) maximizes the MI associated with the superior trajectories and minimizes the MI associated with the inferior trajectories at the same time. However, it still focuses on the training of a fixed team, while our work focuses on the training of a dynamic team and the generalization over unseen teams.

## 3 Background

In this paper, we formulate cooperative MARL with dynamic team composition as a multi-agent Markov decision process (MMDP) with entities. MMDP with entities can be defined by a tuple \(,S,A,U,P,r,\). \(\) is the set of entities in the environment, \(S\) is the set of states, and each state \(\) is composed by the state of each entity \(=\{s_{e}\}\). It is worth noting that except agents \(a A\), there are also other entities in the environments (e.g., landmarks, obstacles, agents with fixed behavior). \(U=U_{1} U_{|A|}\) is the joint action space, where \(U_{i}\) is the individual action space for each agent \(i\). For the agronowness of proof, we assume full observability such that at each state \( S\), each agent \(i\) receives state \(\), chooses an action \(u_{i} U_{i}\), and all actions form a joint action \( U\). The state transitions to the next state \(^{}\) upon \(\) according to the transition function \(P(^{}|,):S U S\), and all agents receive a shared reward \(r(,):S U\). The objective is to learn an individual policy \(_{i}(u_{i}|)\) for each agent such that they can cooperate to maximize the expected cumulative discounted return, \([_{t=0}^{}^{t}r_{t}]\), where \([0,1)\) is the discount factor. In CTDE, from a centralized perspective, a group of local policies can be viewed as a joint policy \(_{}(|)\). For this joint policy, we can define the joint state-action value function \(Q_{}(_{t},_{t})=_{_{t+1:},_{ t+1:}}[_{t=0}^{}^{t}r_{t+k}|_{t},_{t}]\). Note that although we assume full observability for the rigorousness of proof, we use the trajectory of each agent \(_{i}_{i}:(Y U_{i})^{*}\) to replace state \(\) for each agent to settle the partial observability in practice, where \(Y\) is the observation space.

Since we are discussing dynamic team composition in this paper, we further denote \(s_{i}^{+}\) as team-unrelated information for agent \(i\) (agent's own information), and use \(s_{i}^{-}\) to denote team-related information that varies along team composition (e.g., information of other agents, landmarks, obstacles). Although we assume full observability for each agent (i.e., \(\) is the same for all agents), \(s_{i}^{+}\) and \(s_{i}^{-}\) can be different as the circumstance of each agent per se is different. One can easily conclude that \(=\{s_{i}^{+},s_{i}^{-}\}\) for each agent \(i\).

## 4 Method

In this section, we present our method, MIPI, as follows. In Section 4.1, we introduce the mutual information (MI) augmented objective for regularizing the reliance on \(s_{i}^{-}\) for each agent. However, due to the existence of the dynamic marginal distribution, direct optimization on this objective can be intractable in practice. Therefore, in Section 4.2, we first discuss a multi-agent policy iteration with a fixed marginal distribution and prove its convergence and optimality. Then, in Section 4.3, we discuss how to use an imaginary team composition distribution to achieve an approximate dynamic marginal distribution and how to use the Blahut-Arimoto algorithm (Cover, 1999) to optimize the corresponding objective. Finally, in Section 4.4, we summarize the learning framework of MIPI.

### MI-Augmented Objective

The learning objective of standard MARL can be formulated as follows,

\[*{arg\,max}_{_{}}_{(_{0}),_ {},P}_{t=0}^{T}^{t}r(_{t},_{t}), \]where \((_{0})\) is the distribution of initial state. We can further rewrite it as:

\[*{arg\,max}_{_{ jt}}_{t=0}^{T}_{_{_{ jt }}(_{t})}[\,_{_{ jt}(_{t}|_{t})}[ ^{t}r(_{t},_{t})]], \]

where \(_{_{ jt}}(_{t})\) is the the marginal distribution over states at timestep \(t\). Recall that the conditional mutual information between \(x\) and \(y\) given \(z\) can be expressed as follows,

\[(x;y|z)=_{p(y,z)}[\,_{p(x|y,z)}[\, ]].\]

The conditional mutual information can be used to measure the dependency between variable \(x\) and \(y\) with \(z\) given. Therefore, as our goal is to reduce the reliance of \(_{i}(u_{i}|)=_{i}(u_{i}|s_{i}^{+},s_{i}^{-})\) on \(s_{i}^{-}\), we can formulate conditional mutual information as follows,

\[(u_{i};s_{i}^{-}|s_{i}^{+}) =_{(s_{i}^{+},s_{i}^{-})}[\,_{_{i }(u_{i}|s_{i}^{+},s_{i}^{-})}[\,(u_{i}|s_{i}^{+},s_{i}^{ -})}{_{i}(u_{i}|s_{i}^{+})}]]\] \[=_{(s_{i}^{+}),(s_{i}^{-}|s_{i}^{+})}[\, _{_{i}(u_{i}|)}[\,(u_{i}|)}{ _{i}(u_{i}|s_{i}^{+})}]],\]

where \(_{i}(u_{i}|s_{i}^{+})=_{s_{i}^{-}}(s_{i}^{-}|s_{i}^{+})_{i}(u_{i }|s_{i}^{+},s_{i}^{-})\). Incorporating the conditional mutual information of all agents into the standard MARL objective, we now have the MI-augmented objective used in this paper:

\[*{arg\,max}_{_{ jt}} _{t=0}^{T}_{_{_{ jt}}(_{t})}[ \,_{_{ jt}(_{t}|_{t})}[^{t}r( _{t},_{t})-_{i}(u_{i,t}|_{t})}{ _{i}(u_{i,t}|s_{i,t}^{+})}]]\] (3) s.t. \[_{i}(u_{i,t}|s_{i,t}^{+})=_{s_{i,t}^{-}}_{_{ jt} }(s_{i,t}^{-}|s_{i,t}^{+})_{i}(u_{i,t}|s_{i,t}^{+},s_{i,t}^{-}), \]

where the coefficient \(\) is used to determine the trade-off between maximizing global reward and minimizing mutual information.

### Multi-Agent Policy Iteration with a Fixed Marginal Distribution

As we can see in (4), the optimization of (3) is highly coupled with a dynamic marginal distribution \(_{i}(u_{i,t}|s_{i,t}^{+})\). What's even worse is, this marginal distribution is determined by \(_{_{ jt}}(s_{i,t}^{-}|s_{i,t}^{+})\), which is related to \(_{ jt}\), making the optimization problem even harder. However, one may notice that, if such a marginal distribution \(_{i}(u_{i,t}|s_{i,t}^{+})\) is given and fixed, this problem becomes much easier and can be solved in an off-policy manner. Therefore, in this section, we introduce multi-agent policy iteration with a fixed marginal distribution and prove its convergence and optimality, and in the next section, we discuss how to approximate the dynamic marginal distribution by integrating constraints similar to (4) into this multi-agent policy iteration. First, let us define the joint value function \(V_{ jt}\) and joint state-action value function \(Q_{ jt}\) as follows,

\[V_{ jt}^{_{ jt}}()=_{_{ jt}}[ \,_{t=0}^{t}r(_{t},_{t})-_{i}(u_{i,t}|_{t})}{_{i}(u_{i,t}|s_{i,t}^{+})}|_{0}= ]\] \[Q_{ jt}^{_{ jt}}(,)=r(,)+ \,_{^{} P}[\,V_{ jt}(^{})],\]

where \(_{i}(u_{i,t}|s_{i,t}^{+})\) is the fixed marginal distribution for each agent \(i\). With the above definition, we can further deduce that:

\[V_{ jt}^{_{ jt}}()=_{_{ jt}}[Q_{ jt}^{ _{ jt}}(,)-_{i}(u_{i}|)}{ _{i}(u_{i}|s_{i}^{+})}].\]

We can then define the joint policy evaluation operator as

\[_{_{ jt}}\,Q_{ jt}(,):=r(,)+\, _{^{}}[V_{ jt}(^{})] \]

and have the following lemma.

**Lemma 1** (Joint Policy Evaluation).: _Consider the modified Bellman backup operator \(_{_{}}\) (5) and a mapping \(Q^{0}_{}:S U\) with \(|U|<\), and define \(Q^{k+1}_{}=_{_{}}\,Q^{k}_{}\). Then, the sequence \(Q^{k}_{}\) will converge to the joint \(\)-function of \(_{}\) as \(k\)._

Proof.: See Appendix A.1. 

Using Lemma 1, we can get \(Q_{}\) for any joint policy \(_{}\). However, it is hard for us to use \(Q_{}\) for individual policy improvement. To solve this problem, many value decomposition methods choose to factorize the joint state-action value function \(Q_{}\) into the utility function \(Q_{i}\) of each agent and use \(Q_{i}\) to guide the individual policy improvement of \(_{i}\). In this paper, we factorize the joint state-action value function into the following form, which is shared by many value decomposition methods (Zhang et al., 2021; Su and Lu, 2022; Wang et al., 2023):

\[Q^{_{}}_{}(,)=_{i}w_{i}()*Q^{ _{i}}_{i}(,u_{i})+b(). \]

After the evaluation of the joint policy and the decomposition of \(Q_{}\), we construct the following optimization problem for individual policy improvement.

\[^{}_{i}=_{^{}_{i}}_{^{}_{i} }[Q^{^{}_{i}}_{i}(,u_{i})-_{i}(u_{i}|)}{_{i}(u_{i}|s^{+}_{i})}] \]

Based on the above optimization problem, we have the following lemma for individual policy improvement.

**Lemma 2** (Individual Policy Improvement).: _Let \(^{}_{i}\) be the optimizer of the maximization problem in (7). Then, we have \(Q^{^{}_{}}_{}(,) Q^{^{ }_{}}_{}(,)\) for all \((,)|S||U|\) with \(|U|<\), where \(^{}_{}(|)=_{i}^{}_{i }(u_{i}|)\) and \(^{}_{}(|)=_{i}^{}_{i }(u_{i}|)\)._

Proof.: See Appendix A.2. 

Combining Lemma 1 and 2, we can have the following theorem which proves the convergence and optimality of multi-agent policy iteration with a fixed marginal distribution.

**Theorem 1** (Multi-Agent Policy Iteration with a Fixed Marginal Distribution).: _For any joint policy \(_{}\), if we repeatedly apply joint policy evaluation and individual policy improvement. Then the joint policy \(_{}(|)=_{i=1}^{n}_{i}(u_{i}|)\) will eventually converge to \(^{*}_{}\), such that \(Q^{^{}_{}}_{}(,) Q^{_{ }}_{}(,)\) for all \(_{}\), assuming \(|U|<\)._

Proof.: See Appendix A.3. 

### Approximation for Dynamic Marginal Distribution

With the multi-agent policy iteration above, we can have \(Q_{}\) and corresponding \(Q_{i}\) for each agent, however, under a fixed marginal distribution. In this section, we discuss how to approximate the dynamic marginal distribution to decouple \(_{_{}}(s^{-}_{i,t}|s^{+}_{i,t})\) from \(_{}\), and introduce the Blahut-Arimoto algorithm for the corresponding optimization.

Notice that the original objective (3) comes with a constraint (4). In Section 4.2, what we did is to remove this constraint for an easier optimization process. What we are going to do here, is to add a similar constraint back. First, consider the meaning of \(_{_{}}(s^{-}_{i}|s^{+}_{i})\), it represents the potential team composition given team-unrelated information. Therefore, inspired by REFIL (Iqbal et al., 2021), we randomly partition team composition under \(=\{s^{+}_{i},s^{-}_{i}\}\) into different subgroups, yielding a set of imaginary team compositions and corresponding imaginary distribution \((s^{*}_{i}|s^{+}_{i})\) for imaginary team-related information. With this imaginary distribution, we can propose the approximate objectivefor (3) as follows,

\[_{_{i}}_{t=0}^{T}_{_{_{i}}(_{t} )}[_{_{i}(_{t}|_{t})}[^{t}r( _{t},_{t})-_{i}(u_{i,t}|_{t})}{_ {i}(u_{i,t}|s_{i,t}^{+})}]] \] \[_{i}(u_{i,t}|s_{i,t}^{+})=_{s_{i,t}^{*}} (s_{i,t}^{*}|s_{i,t}^{+})_{i}(u_{i,t}|s_{i,t}^{+},s_{i,t}^{*}). \]

Therefore, we can have the approximate optimization problem for (7) as follows,

\[_{i}^{}= _{_{i}^{}}_{_{i}^{}}[Q_{ i}^{_{i}^{}}(,u_{i})-^{}(u_{i}| )}{_{i}^{}(u_{i}|s_{i}^{+})}] \] \[_{i}^{}(u_{i}|s_{i}^{+})=_{s_{i}^{*} }(s_{i}^{*}|s_{i}^{+})_{i}^{}(u_{i}|s_{i}^{+},s_{i}^{*}). \]

The objective above exhibits similarities with the rate-distortion problem (Cover, 1999), which could be solved using the Blahut-Arimoto algorithm. Although with the approximation above we break the convergence of Theorem 1, by using the Blahut-Arimoto algorithm we can have the following theorem, indicating the convergence of (10) as shown in Leibfried and Grau-Moya (2020).

**Theorem 2** (**Convergence of Constrained Individual Policy Improvement)**.: _The optimization problem induced by (10) can be solved by iterating in an alternate fashion through the following two equations:_

\[_{i}^{m}(u_{i}|s_{i}^{+})=_{s_{i}^{*}}(s_{i}^{*} |s_{i}^{+})_{i}^{m}(u_{i}|s_{i}^{+},s_{i}^{*}) \] \[_{i}^{m+1}(u_{i}|s_{i}^{+},s_{i}^{-})=^{m}(u_{i}| s_{i}^{+})(Q_{i}(,u_{i})/)}{_{u_{i}}_{i}^{m}(u_{i}|s_{i}^{+}) (Q_{i}(,u_{i})/)}, \]

_where \(m\) refers to the iteration index. Denoting the total number of iterations as \(M\), the presented scheme converges at a rate of \(O(1/M)\) to an optimal policy \(_{i}^{*}\) for any given bounded utility function \(Q_{i}\) and any initial policy \(_{i}^{0}\)._

Proof.: See Appendix A.4. 

### MIPI Framework

In Section 4.2 and 4.3, we propose our learning algorithm in theory. In this section, we discuss how to implement our algorithm in practice, which can be summarized in Figure 1.

Figure 1: Learning framework of MIPI, where each agent \(i\) has three modules: a utility function \(Q_{i}(,u_{i};_{i})\), a policy \(_{i}(u_{i}|;_{i})\), and a marginal policy \(_{i}(u_{i}|s_{i}^{+};_{i})\).

In MIPI, each agent has a utility function \(Q_{i}(,u_{i};_{i})\), a policy \(_{i}(u_{i}|;_{i})\), and a marginal policy \(_{i}(u_{i}|s_{i}^{+};_{i})\). For joint policy evaluation, with the utilities of agents, we use a mixer network \((,;)\) to get the joint state-action value function \(Q_{}\) as follows,

\[Q_{}(,) =([Q_{i}(,u_{i};_{i})]_{i=1}^{|A|},;) \] \[=_{i=1}^{|A|}w_{i}()Q_{i}(,u_{i};_{i})+b( ), \]

Where \(w_{i}() 0\) is a positive weight used to linearly decompose \(Q_{}\) with the IGM principle being preserved. Same as REFIL, \(w_{i}()\) is computed via the attention mechanism to handle dynamic team composition. With \(Q_{}\), we can update the utilities and the mixer network by minimizing the following TD error:

\[([_{i}]_{i=1}^{|A|},)=_{}[Q_{ }(,)-(r(,)+_{ }(^{},^{})-_{i}^{|A|}(u^{}_{i}|^{})}{_{i}(u^{}_{i}|s^{}_{i} )})], \]

where \(\) is the replay buffer, \(_{}\) is the target network and \(u^{}_{i}\) is sampled from the current policy \(_{i}(u_{i}|;_{i})\) of each agent. To accelerate the training with a fixed set of team compositions, we also incorporate the same imaginary objective based on random sub-group partitioning as REFIL for joint policy evaluation.

As we described in Section 4.3, the constrained individual policy improvement is achieved via an iterative update of \(_{i}(u_{i}|;_{i})\) and \(_{i}(u_{i}|s^{+}_{i};_{i})\). For \(_{i}(u_{i}|s^{+}_{i};_{i})\), we update it via the maximum likelihood estimation:

\[(_{i})=_{}[_{s^{*}_{i} (s^{*}_{i}|s^{+}_{i}),u_{i}_{i}(u_{i}|s^{+}_{i},s^{*}_{i} )}[_{i}(u_{i}|s^{+}_{i})]]. \]

For \(_{i}(u_{i}|;_{i})\), we update it by minimizing the KL-divergence as follows,

\[(_{i})=_{}[_{u_{i}_{i }(u_{i}|)}[(u_{i}|)}{_{i}(u _{i}|s^{+}_{i})}-Q_{i}(,u_{i})]]. \]

## 5 Experiments

In this section, we evaluate MIPI in two different scenarios. One is a simple yet challenging matrix game, which we use to illustrate how mutual information may help to learn generalizable policies. Then, we evaluate MIPI in a complicated cooperative MARL scenario: StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019), comparing it against REFIL, AQMIX (Iqbal et al., 2021), CollaQ (Zhang et al., 2020) and MAPPO (Yu et al., 2022). More details about experiments, hyperparameters, and the learning curve of each algorithm are included in Appendix B and C. All results are presented using the mean and standard deviation of five runs with different random seeds.

### An Illustrative Example: Matrix Game

We first use a matrix game to explain how mutual information works in solving generalization problems. In this game, we have two agents, and each of them can take two actions \(\{0,1\}\) and can take one of the two types \(\{A,B\}\). During training, we train these two agents under team compositions \((A,B)\) and \((B,A)\), where the corresponding payoff matrices are shown in Figure 2(a) and 2(b). However, during evaluation, we test their performance on team composition \((B,B)\), and have Figure 2(c) as the payoff matrix, which is different from training scenarios.

As we can see in Figure 2(a), 2(a), 2(c), we have different optimal joint actions in different team compositions. However, there exists a generalizable joint action \((a_{1}=0,a_{2}=0)\) that can achieve consistent performance regardless of team compositions, even if it is not an optimal joint action in any team composition.

In Figure 2(d), we plot the evaluation results of three algorithms during training, which is evaluated on team composition \((B,B)\). These three algorithms are all the same except they receive different rewards: pure environmental reward, environmental reward combined with entropy, and environmental reward combined with mutual information. As we can see in Figure 2(d), with the help of mutual information, agents are able to resist the temptation of overfitting to the specific team composition and learn behavior that can generalize across different team compositions.

### StarCraft Micromanagement Tasks

#### 5.2.1 Performance

Further, we evaluate MIPI on SMAC with the map designed by Iqbal et al. (2021). We customize three different types of scenarios (SZ, CSZ, and MMM) based on this map for our experiments. In SZ scenarios, agents can take two different unit types, in CSZ and MMM, agents can take three different unit types. During training, the maps randomly initialize 3-5 agents and the same number of enemies at the start of each episode. During the evaluation, we use 6-8 agents and 6-8 enemies. Results are shown in Table 1. In general, MIPI outperforms the baselines in **8 out of 9 evaluation scenarios**. When the evaluation scenario is similar to the training scenarios, the gap between MIPI and other baselines is relatively small, whereas, in the evaluation scenario that is very different from the training scenarios, the gap between MIPI and other baselines becomes larger. In terms of the training performance, REFIL achieves the best result in all three scenarios, as it does not consider the overfitting issue at all. However, the performance of MIPI is still at the same level as REFIL, which indicates that MIPI can achieve better zero-shot generalization without sacrificing the performance on the training set.

   &  & Training &  \\   & & & 3-5 & 6 & 7 & 8 \\    & MIPI & 0.659\(\)0.02 & **0.453\(\)**0.08 & **0.404\(\)**0.062 & **0.276\(\)**0.076 \\   & REFIL & **0.674\(\)**0.038 & 0.441\(\)0.103 & 0.352\(\)0.078 & 0.236\(\)0.103 \\   & AQMIX & 0.528\(\)0.044 & 0.343\(\)0.105 & 0.291\(\)0.084 & 0.182\(\)0.058 \\   & CollaQ & 0.588\(\)0.03 & 0.366\(\)0.086 & 0.314\(\)0.076 & 0.198\(\)0.097 \\   & MAPPO & 0.256\(\)0.01 & 0.129\(\)0.019 & 0.148\(\)0.031 & 0.036\(\)0.015 \\    & MIPI & 0.548\(\)0.032 & **0.42\(\)**0.102 & **0.297\(\)**0.112 & **0.261\(\)**0.09 \\   & REFIL & **0.568\(\)**0.027 & 0.348\(\)0.057 & 0.229\(\)0.053 & 0.164\(\)0.06 \\   & AQMIX & 0.509\(\)0.054 & 0.323\(\)0.096 & 0.216\(\)0.101 & 0.152\(\)0.071 \\   & CollaQ & 0.459\(\)0.061 & 0.362\(\)0.13 & 0.267\(\)0.099 & 0.231\(\)0.095 \\   & MAPPO & 0.248\(\)0.037 & 0.12\(\)0.029 & 0.06\(\)0.028 & 0.054\(\)0.013 \\    & MIPI & 0.548\(\)0.023 & 0.495\(\)0.054 & **0.447\(\)**0.041 & **0.467\(\)**0.067 \\   & REFIL & **0.605\(\)**0.057 & 0.437\(\)0.118 & 0.329\(\)0.171 & 0.224\(\)0.163 \\    & AQMIX & 0.501\(\)0.036 & 0.447\(\)0.043 & 0.344\(\)0.071 & 0.251\(\)0.089 \\    & CollaQ & 0.589\(\)0.027 & **0.513\(\)**0.07 & 0.423\(\)0.026 & 0.286\(\)0.083 \\    & MAPPO & 0.289\(\)0.097 & 0.32\(\)0.102 & 0.25\(\)0.063 & 0.275\(\)0.098 \\  

Table 1: Final performance on all SMAC maps. MIPI outperforms REFIL, AQMIX, and CollaQ in 8 out of 9 evaluation maps. We bold the best mean performance for each map.

Figure 2: A matrix game with different team compositions: (a) (b) (c) payoff matrices for different team compositions; (d) learning curves of different methods on team composition \((B,B)\).

#### 5.2.2 Ablation

Although MIPI uses the random sub-group partitioning as in REFIL, it is an actor-critic structure, whereas REFIL uses only a critic. Therefore, one may question whether the improved generalization of MIPI is due to the introduction of mutual information, or simply due to the introduction of the actor. To eliminate such a concern, we build two ablation baselines, Value and Entropy, where all other perspectives are the same as MIPI, except they use pure environmental reward and entropy-augmented reward, respectively. As we can see in Table 2, MIPI also outperforms these two baselines, which demonstrates the importance of MI-augmented reward in MIPI.

## 6 Conclusion

In this paper, we propose MIPI, an MI-regularized multi-agent policy iteration algorithm to improve the generalization ability of agents under unseen team compositions. We first prove the convergence and optimality of our algorithm given a fixed marginal distribution, then we propose to use an imaginary distribution to approximate the dynamic marginal distribution to better approximate the original objective and incorporate the Blahut-Arimoto algorithm into the multi-agent policy iteration to optimize this approximate objective. We evaluate our algorithm in complex coordination scenarios, SMAC, and demonstrate that MIPI can achieve better zero-shot generalization results, without sacrificing the performance on the training set.

One potential limitation of this work is the introduction of approximation distorts the original mutual information augmented objective and breaks the convergences of multi-agent policy iteration. One possible solution to this problem is to seek alternative solutions using on-policy optimization methods (Schulman et al., 2015, 2017; Grudzien et al., 2022) to optimize the augmented objective.