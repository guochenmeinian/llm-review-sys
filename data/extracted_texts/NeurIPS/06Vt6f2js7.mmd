# SyncTweedies: A General Generative Framework

Based on Synchronized Diffusions

Jaihoon Kim1    Juil Koo1    Kyeongmin Yeo1    Minhyuk Sung

KAIST

{jh27kim,63days,aaaaa,mhsung}@kaist.ac.kr

Equal contribution.

###### Abstract

We introduce a general diffusion synchronization framework for generating diverse visual content, including ambiguous images, panorama images, 3D mesh textures, and 3D Gaussian splats textures, using a pretrained image diffusion model. We first present an analysis of various scenarios for synchronizing multiple diffusion processes through a canonical space. Based on the analysis, we introduce a synchronized diffusion method, SyncTweedies, which averages the outputs of Tweedie's formula while conducting denoising in multiple instance spaces. Compared to previous work that achieves synchronization through finetuning, SyncTweedies is a zero-shot method that does not require any finetuning, preserving the rich prior of diffusion models trained on Internet-scale image datasets without overfitting to specific domains. We verify that SyncTweedies offers the broadest applicability to diverse applications and superior performance compared to the previous state-of-the-art for each application. Our project page is at [https://synctweedies.github.io](https://synctweedies.github.io).

Figure 1: **Diverse visual content generated by SyncTweedies:** A diffusion synchronization process applicable to various downstream tasks without finetuning.

Introduction

Image diffusion models  have shown unprecedented ability to generate plausible images that are indistinguishable from real ones. The generative power of these models stems not only from their capacity to learn from a vast diversity of potential data but also from being trained on Internet-scale image datasets .

Our goal is to expand the capabilities of pretrained image diffusion models to produce a wide range of 2D and 3D visual content, including panoramic images and textures for 3D objects, as shown in Figure 1, without the need to train diffusion models for each specific visual content. Despite the existence of general image datasets on the scale of billions , collecting other forms of visual data at this scale is not feasible. Nonetheless, most visual content can be converted into a regular image of a specific size through certain mappings, such as projecting for panoramic images and rendering for textures of 3D objects. Thus, we employ such a _bridging_ function between each type of visual content and images, along with pretrained image diffusion models .

We introduce a general generative framework that generates data points in the desired visual content space--referred to as canonical space--by combining the denoising process of diffusion models in the conventional image space--referred to as instance spaces. Given the bridging functions connecting the canonical space and instance spaces, we first explore performing individual denoising processes in each instance space while _synchronizing_ them in the canonical space via the mapping. Another approach is to denoise directly in the canonical space, although it is not immediately feasible due to the absence of diffusion models trained on the canonical space. We investigate _redirecting_ the noise prediction to the instance spaces but aggregating the outputs later in the canonical space.

Depending on the timing of aggregating the outputs of computation in the instance spaces, we identify _five_ main possible options for the diffusion synchronization processes. Previous works  have investigated each of the possible cases only for specific applications, and none of them have analyzed and compared them across a range of applications. For the first time, we present a general framework for diffusion synchronization processes, within which the previous works  are contextualized as specific cases. We then present extensive analyses of different choices of diffusion synchronization processes. Based on the analyses, we demonstrate that the approach, which conducts denoising processes in _instance_ spaces (not the canonical space) and synchronizes the outputs of Tweedie's formula  in the canonical space, provides the broadest applicability across a range of applications and the best performance. We name this approach SyncTweedies and showcase its superior performance in multiple visual content creation tasks compared with previous state-of-the-art methods.

Previous works  finetune pretrained diffusion models to generate new types of outputs such as \(360^{}\) panorama images and 3D mesh texture images. However, this approach requires a large quantity of target content for high-quality outputs which is prohibitively expensive to acquire. When it comes to generating visual content that can be parameterized into an image, a notable zero-shot approach not utilizing diffusion synchronization is Score Distillation Sampling (SDS) , which has shown particular effectiveness in 3D generation and texturing . However, this alternative application of diffusion models has been observed to produce suboptimal results and also requires a high CFG  weight for convergence, leading to over-saturation. For 3D mesh texture generation, specifically, an approach that iteratively updates each view image has also been explored in multiple previous works . However, the accumulation of errors over iterations has been identified as a challenge. We demonstrate that our diffusion-synchronization-based approach outperforms these methods in terms of generation quality across various applications.

Overall, our contributions can be summarized as follows:

* We propose, for the first time, a general generative framework for diffusion synchronization processes.
* Through extensive analyses of various options for diffusion synchronization processes, including previous works , we identify that the approach which synchronizes the outputs of Tweedie's formula and performs denoising in the instance space, SyncTweedies, offers the broadest applicability and superior performance.
* In our experiments, we verify the superior performance and versatility of SyncTweedies across diverse applications, including texturing on 3D meshes and Gaussian Splats , and depth-to-360-panorama generation. Compared to the previous state-of-the-art methods based on finetuning, optimization, and iterative updates, SyncTweedies demonstrates significantly better results.

Problem Definition

We consider a generative process that samples data within a space we term the _canonical_ space \(\), where a pretrained diffusion model is not provided. Instead, we leverage diffusion models trained in other spaces called the _instance_ spaces \(\{_{i}\}_{i=1:N}\), where a _subset_ of the canonical space can be instantiated into each of them via a mapping: \(f_{i}:_{i}\); we refer to this mapping as the _projection_. Let \(g_{i}\) denote the _unprojection_, which is the inverse of \(f_{i}\), mapping the instance space to a subset of the canonical space. We assume that the entire canonical space \(\) can be expressed as a composition of multiple instance spaces \(_{i}\), meaning that for any data point \(\), there exist \(\{_{i}\,|\,_{i}_{i}\}_{i=1:N}\) such that

\[=(\{g_{i}(_{i})\}_{i=1:N}), \]

where \(\) is an aggregation function that averages the data points from the multiple instance spaces in the canonical space. Our objective is to introduce a general framework for the generative process in the canonical space by integrating multiple denoising processes from different instance spaces through synchronization.

## 3 Diffusion Synchronization

We first outline the denoising procedure of DDIM  and then present possible options for diffusion synchronization processes based on it.

### Denoising Process of DDIM 

Song _et al._ have proposed DDIM, a generalized denoising process that controls the level of randomness during denoising. In DDIM , the posterior of the forward process is represented as follows:

\[q_{_{t}}(^{(t-1)}|^{(t)},^{(0)}) =(_{_{t}}^{(t)}(^{(t)},^{(0)}), _{t}^{2}), \]

where \(_{_{t}}^{(t)}(^{(t)},^{(0)})=} ^{(0)}+-_{t}^{2}}{1-_{t}}} (^{(t)}-}^{(0)})\) and \(_{t}\) is a hyperparameter determining the level of randomness. In this paper, we consider a deterministic process where \(_{t}=0\) for all \(t\), thus \(_{_{t}=0}^{(t)}\) will be denoted as \(^{(t)}\) for simplicity. During denoising process, to sample \(^{(t-1)}\) from its unknown original clean data point \(^{(0)}\), we estimate \(^{(0)}\) using Tweedie's formula :

\[^{(0)}^{(t)}(^{(t)},_{}( ^{(t)}))=^{(t)}-}_{ }(^{(t)})}{}}, \]

where \(_{}\) is a noise prediction network, and for simplicity, the time input and condition term in \(_{}\) are dropped. In short, each deterministic denoising step of DDIM  is expressed as follows:

\[^{(t-1)}=^{(t)}(^{(t)},^{(t)}(^{(t)}, _{}(^{(t)}))). \]

### Diffusion Synchronization Processes

We now explore various scenarios of sampling \(\) by leveraging the composition of multiple denoising processes in the instance spaces \(\{_{i}\}_{i=1:N}\). Consider the denoising step of the diffusion model at each time step \(t\) in each instance space \(_{i}\):

\[_{i}^{(t-1)}=^{(t)}(_{i}^{(t)},^{(t)}(_ {i}^{(t)},_{}(_{i}^{(t)}))). \]

A naive approach to generating data in the canonical space through the denoising processes in instance spaces would be to perform the processes independently in each instance space and then aggregate the final denoised outputs in the canonical space at the end using the averaging function \(\). However, this approach results in poor outcomes that lack consistency across outputs in different instance spaces. Hence, we propose to _synchronize_ the denoising processes at each time step \(t\) through the unprojection operation \(g_{i}\) from each instance space to the canonical space and the aggregation operation \(\), after which the results will be back-projected via the projection operation \(f_{i}\) to each instance space again. Note that, as described in Equation 4, the estimated mean of the posterior distribution \(^{(t)}(,)\) involves multiple layers of computations: noise prediction \(_{}()\), Tweedie's formula \(^{(t)}(,)\) approximating the final output \(^{(0)}\) each time step, and the final linear combination \(^{(t)}(,)\). Synchronization through the sequence of unprojection \(g_{i}\), aggregation in the canonical space \(\), and projection \(f_{i}\) can thus be performed after each layer of these computations, resulting in the following three cases:Case 1 : \(_{i}^{(t-1)}=^{(t)}(_{i}^{(t)},^{(t)}(_{i}^{ (t)},f_{i}((\{g_{j}(_{}(_{j}^{(t)}))\}_ {j=1}^{N}))))\)

Case 2 : \(_{i}^{(t-1)}=^{(t)}(_{i}^{(t)},f_{i}((\{g_{j} (^{(t)}(_{j}^{(t)},_{}(_{j}^{(t)})) \}_{j=1}^{N})))\)

Case 3 : \(_{i}^{(t-1)}=f_{i}((\{g_{j}(^{(t)}(_{j}^{(t) },^{(t)}(_{j}^{(t)},_{}(_{j}^{(t)}) ))\}_{j=1}^{N})))\).

In each case, we highlight the computation layer to be synchronized in red.

Another notable approach is to conduct the denoising process directly on the canonical space:

\[^{(t-1)}=^{(t)}(^{(t)},^{(t)}(^{(t)}, _{}(^{(t)})))), \]

although it is not directly feasible because the noise prediction network in the canonical space \(_{}(^{(t)})\) is not available. Nevertheless, it can be achieved by _redirecting_ the noise prediction to the instance spaces as follows:

* project the intermediate noisy data point \(^{(t)}\) from the canonical space to each instance space, resulting in \(f_{i}(^{(t)})\),
* apply a _subsequence_ of the operations: \(_{}\), \(^{(t)}\), and \(^{(t)}\),
* unproject the outputs back to the canonical space via \(g_{i}\) and then average them using the aggregation function \(\), and
* perform the remaining operations in the canonical space.

Such an approach of performing the denoising process in the canonical space leads to the following two additional cases depending on the subsequence of operations at step (b):

Case 4 : \(^{(t-1)}=^{(t)}(^{(t)},^{(t)}(^{(t)}, (\{g_{i}(_{}(f_{i}(^{(t)})))\}_{i=1}^ {N}))))\)

Case 5 : \(^{(t-1)}=^{(t)}(^{(t)},(\{g_{i}(^{(t)}( f_{i}(^{(t)}),_{}(f_{i}(^{(t)})))\}_{i=1}^ {N})))\).

Illustration of the aforementioned diffusion synchronization processes are shown in Figure 2. Note the analogy between Cases 1 and 4, and Cases 2 and 5 in terms of the variable averaged in the canonical space with the aggregation operator \(\): either the outputs of \(_{}()\) or \(^{(t)}(,)\).

While it is also feasible to conduct the aggregation \(\) multiple times with the output of different layers within a single denoising step, and to denoise data both in instance spaces and the canonical space, we empirically find that such more convoluted cases perform worse. In **Appendix** H, we detail our exploration of all possible cases and present experimental analyses.

### Connection to Previous Diffusion Synchronization Methods

Below, we first review previous works each corresponding to a specific case of the aforementioned possible diffusion synchronization processes while focusing on a specific application. Then, we discuss finetuning-based approaches and their limitations. In Section 4, we also review literature targeting the same applications but without diffusion synchronization.

#### 3.3.1 Zero-Shot-Based Methods

Ambiguous Image Generation.Ambiguous images are images that exhibit different appearances under certain transformations, such as a \(90^{}\) rotation or flipping. They can be generated through

Figure 2: **Diagrams of diffusion synchronization processes. The left diagram depicts denoising instance variables \(\{_{i}\}\), while the right diagram illustrates directly denoising a canonical variable \(\).**

a diffusion synchronization process, considering both the canonical space \(\) and instance spaces \(\{_{i}\}_{i=1:N}\) as the same space of the image, with the projection operation \(f_{i}\) representing the transformation producing each appearance. Visual Anagrams  uses Case 4 which aggregates the noise predictions \(_{}()\) to generate ambiguous images.

Arbitrary-Sized Image Generation.In arbitrary-sized image generation, the canonical space \(\) is the space of the arbitrary-sized image, while the instance spaces \(\{_{i}\}_{i=1:N}\) are overlapping patches across the arbitrary-sized image, matching the resolution of the images that the pretrained image diffusion model can generate. The projection operation \(f_{i}\) corresponds to the cropping operation applied to each patch. MultiDiffusion  and SyncDiffusion  introduce arbitrary-sized image generation methods using Case 3, averaging the mean of the posterior distribution \(^{(t)}(,)\).

Mesh Texturing.In 3D mesh texturing, the texture image space serves as the canonical space \(\), and the rendered images from each view serve as the instance spaces \(\{_{i}\}_{i=1:N}\). The projection operation \(f_{i}\) corresponds to rendering 3D textured meshes into 2D images. SyncMVD  proposes a 3D mesh texturing method by leveraging Case 5, which performs denoising in the canonical space and unprojects the outputs of Tweedie's formula \(^{(t)}(,)\).

#### 3.3.2 Finetuning-Based Methods

In addition to the aforementioned works, there have been attempts to achieve synchronization through finetuning. In multi-view image generation, SyncDreamer  and MVDream  finetune pretrained image diffusion models to achieve consistency across different views. MVDiffusion  and DiffCollage  generate \(360^{}\) panorama images through finetuning. Additionally, Paint3D  trains an encoder to directly generate 3D mesh texture images in the UV space. However, these finetuning-based methods use target sample datasets  that are smaller by _orders of magnitude_ compared to Internet-scale image datasets , e.g., 10K panorama images  vs. 5B images . As a result, they are prone to overfitting and losing the rich prior and generalizability of pretrained image diffusion models . Additionally, the poor quality of textures in most 3D model datasets results in unsatisfactory texturing outcomes, even when using relatively large-scale datasets . In our experiments, we demonstrate that our zero-shot synchronization method, fully leveraging the pretrained model without bias toward a specific dataset, provides the best realism and widest diversity, assessed by FID and KID, compared to the finetuning-based methods.

### Comparison Across the Diffusion Synchronization Processes

Here, we compare the five cases of diffusion synchronization processes in Section 3.2 and analyze their characteristics through various toy experiments.

#### 3.4.1 Toy Experiment Setup: Ambiguous Image Generation

For the toy experiment setup, we employ the task of generating ambiguous images introduced by Geng _et al._ (see Section 3.3.1 for descriptions of ambiguous images). In this setup, we consider two-view ambiguous image generation, where two different transformations are applied, each producing a distinct appearance. Note that one of the transformations is an identity transformation, while the other is chosen to simulate different scenarios of mapping pixels from the canonical space

 Projection & Metric & Case 1 & Synceedies & Case 2 & Visual Anagrams  & Case 5 \\  & & & & Case 2 & Case 4 & & \\   & CLIP-A \(\) & 30.35 & 30.4 & 30.32 & 30.35 & 30.34 \\  & CLIP-C \(\) & 64.52 & 64.48 & 64.49 & 64.59 & 64.48 \\  & FID \(\) & 85.88 & 86.74 & 85.69 & 86.35 & 86.54 \\  & KID \(\) & 32.37 & 32.59 & 32.57 & 32.41 & 32.36 \\   & CLIP-A \(\) & 25.97 & 30.16 & 29.94 & 25.64 & 30.23 \\  & CLIP-C \(\) & 54.77 & 60.86 & 60.64 & 54.15 & 61.01 \\  & FID \(\) & 23.65 & 110.51 & 117.84 & 257.53 & 108.22 \\  & KID \(\) & 216.71 & 77.16 & 85.52 & 257.43 & 74.48 \\    } & CLIP-A \(\) & 21.28 & 29.56 & 21.58 & 21.33 & 21.09 \\  & CLIP-C \(\) & 49.94 & 63.1 & 50.58 & 50.05 & 50.04 \\  & FID \(\) & 405.82 & 96.3 & 243.23 & 301.2 & 289.82 \\  & KID \(\) & 496.98 & 40.91 & 151.11 & 233.11 & 213.45 \\  

Table 1: **A quantitative comparison in ambiguous image generation.** KID  is scaled by \(10^{3}\). For each row, we highlight the column whose value is within 95% of the best.

to the instance space: 1-to-1, 1-to-\(n\), and \(n\)-to-\(1\) projection. In \(1\)-to-\(1\) and \(n\)-to-\(1\) projections, we use the \(10\) transformations from Visual Anagrams , while for the 1-to-\(n\) projection, we apply rotation transformations with randomly sampled angles. For all projection cases, we use the 95 prompts from . For more details on the experiment setups, refer to **Appendix**B.1.

#### 3.4.2 \(1\)-to-\(1\) Projection

In \(1\)-to-\(1\) projection case, the five cases of diffusion synchronization processes become identical, as shown in **Appendix**D. The quantitative and qualitative results of diffusion synchronization processes are presented in Table 1 and the first row of Figure 3, respectively, where the fully denoised instance variables, \(_{1}^{(0)}\) and \(_{2}^{(0)}\), are displayed side by side. The results confirm that all diffusion synchronization processes produce the same outputs.

#### 3.4.3 \(1\)-to-\(n\) Projection

We further investigate the five cases of diffusion synchronization processes with different transformations for ambiguous images. It is important to note that all the transformations previously mentioned are perfectly invertible, meaning: \(f_{i}(g_{i}(_{i}))=_{i}\). However, in certain applications, the projection \(f_{i}\) is often not a _function_ but an \(1\)-to-\(n\) mapping, thus not allowing its inverse. For example, consider generating a texture image of a 3D object while treating the texture image space as the canonical space and the rendered image spaces as instance spaces. When mapping each pixel of a specific view image to a pixel in the texture image in the rendering process--with nearest neighbor sampling, one pixel in the texture space can be projected to multiple pixels. Hence, the unprojection \(g_{i}\) cannot be a perfect inverse of the projection \(f_{i}\) but can only be an approximation, making the reprojection error \(\|_{i}-f_{i}(g_{i}(_{i}))\|\) small. This violates the initial conditions required for the proof in **Appendix**D that states Cases 1-5 become identical, and we observe that such a case of having \(1\)-to-\(n\) projection \(f_{i}\) can significantly impact the diffusion synchronization process.

As a toy experiment setup illustrating such a case with ambiguous image generation, we replace the \(1\)-to-\(1\) transformations used in Section 3.4.2 to rotation transformations with nearest-neighbor sampling. We randomly select an angle and rotate an inner circle of the image while leaving the rest of the region unchanged. Due to discretization, rotating an image followed by an inverse rotation may not perfectly restore the original image.

The second row of Table 1 and Figure 3 present the quantitative and qualitative results of \(1\)-to-\(n\) projection experiment. Note that the performance of Case 1 and Visual Anagrams  (Case

Figure 3: **Qualitative results of ambiguous image generation. While all diffusion synchronization processes show identical results with \(1\)-to-\(1\) projections, Case 1, Case 3 and Visual Anagrams  (Case 4) exhibit degraded performance when the projections are \(1\)-to-\(n\). Notably, SyncTweedies can be applied to the widest range of projections, including \(n\)-to-\(1\) projections, where Case 5 fails to generate plausible outputs.**

4), which aggregate the predicted noises \(_{}()\) from either instance variables \(_{i}^{(t)}\) or a projected canonical variable \(f_{i}(^{(t)})\) respectively, significantly declines. Also, the performance of Case 3, which aggregates the posterior means \(^{(t)}(,)\), shows a minor decline. The quality of Cases 2 and 5, however, remain almost unchanged. This highlights that the denoising process is highly sensitive to the predicted noise and to the intermediate noisy data points, while it is much more robust to the outputs of Tweedie's formula \(^{(t)}(,)\), the prediction of the final clean data point at an intermediate stage.

#### 3.4.4 \(n\)-to-\(1\) Projection

Then, do the results above conclude that both Cases 2 and 5 are suitable for all applications? Lastly, we consider the case when the projection \(f_{i}\) also involves an \(n\)-to-\(1\) mapping. Such a scenario can arise when coloring not a solid mesh but a neural 3D representation rendered with the volume rendering equation . Due to the nature of volume rendering, which involves sampling _multiple_ points along a ray and taking a weighted sum of their information, the projection operation \(f_{i}\) includes an \(n\)-to-\(1\) mapping. Note that this case also violates the initial conditions of the proof in **Appendix** D, which states that the diffusion synchronization cases become identical under specific initial conditions. Additionally, Case 5 results in poor outcomes due to a _variance decrease_ issue. Let \(\{_{i}\}_{i=1:N}\) be random variables, each sampled from \(_{i}(_{i},_{t}^{2})\), and \(=_{i=1}^{N}w_{i}_{i}\) be the weighted sum, where \(0 w_{i} 1\) and \(_{i=1}^{N}w_{i}=1\). Then, \(\) also follows the Gaussian distribution \((_{i=1}^{N}w_{i}_{i},_{i=1}^{N}w _{i}^{2}_{t}^{2})\). From the triangle inequality , the sum of squares is always less than or equal to the square of the sum: \(_{i=1}^{N}w_{i}^{2}(_{i=1}^{N}w_{i})^{2}=1\), implying that the variance of \(\) is mostly less than the variance of \(_{i}\).

Consequently, when \(f_{i}\) includes an \(n\)-to-\(1\) mapping, the variance of \(_{i}^{(t)}\), computed as a weighted sum over multiple points in the canonical space, is mostly less than the variance of \(^{(t)}\). Thus, the final output of Case 5 becomes blurry and coarse since each intermediate noisy latent in instance spaces \(_{i}^{(t)}\) experiences a decrease in variance compared to that of \(^{(t)}\).

We validate our analysis with another toy experiment, where we use the same set of transformations used by Geng _et al._ but with a multiplane image (MPI)  as the canonical space. The image of each instance space is rendered by first averaging colors in the multiplane of the canonical space and then applying the transformation. Ten planes are used for the multiplane image representation in our experiments. The results are presented in the third row of Table 1 and Figure 3. Notably, Case 5 fails to produce plausible images like the other cases, whereas Case 2 still generates realistic images.

Table 2 below summarizes suitable cases for each projection type. Note that Case 2 is the only case that is applicable to any type of projection function. Since Case 2 involves averaging the outputs of Tweedie's formula in the instance spaces, we name this case SyncTweedies. Experimental results with additional applications are demonstrated in Section 5, and analysis of all possible cases is presented in **Appendix** H.

## 4 Related Work

In addition to Section 3.3.1 introducing previous works on diffusion synchronization, in this section, we review other previous works that utilize pretrained diffusion models in different ways to generate or edit visual content.

 Projection & Application & Case 1 & SyncTweedies & Case 3 & Case 4 & Case 5 \\   & Ambiguous images, & & & & & \\  & Arbitrary-sized images & & & & & \\   & \(360^{}\) panoramas, & & & & & & \\  & 3D mesh texturing & & & & & & \\   & 3D Gaussian & & & & & & \\  & Splats  texturing & & & & & & \\   & Previous Work & - & - & MultiDiffusion  & Visual Anagrams  & SyncMVD  \\ 

Table 2: **Analysis of diffusion synchronization processes on different projection scenarios.** SyncTweedies offers the broadest range of applications.

Optimization-Based Methods.Poole _et al._ first introduced Score Distillation Sampling (SDS), which facilitates data sampling in a canonical space by leveraging the loss function of the diffusion model training and performing gradient descent. This idea, originally introduced for 3D generation , has been widely applied to various applications, including vector image generation , ambiguous image generation , mesh texturing , mesh deformation , and 4D generation . Subsequent works  also proposed modified loss functions not to generate data but to edit existing data while preserving their identities. This approach, exploiting diffusion models not for denoising but for gradient-descent-based updating, generally produces less realistic outcomes and is more time-consuming compared to denoising-based generation.

Iterative View Updating Methods.Particularly for 3D object/scene texturing and editing, there are approaches to iteratively update each view image and subsequently refine the 3D object/scene. TEXTure , Text2Tex , and TexFusion  are previous works that sequentially update a partial texture image from each view and unproject it onto the 3D object mesh. For texturing 3D scene meshes, Text2Room  and SceneScape  take a similar approach and update scene textures sequentially. Instruct-NeRF2NeRF  proposed to edit a 3D scene by iteratively replacing each view image used in the reconstruction process. However, sequentially updating the canonical sample leads to error accumulations, resulting in blurriness or inconsistency across different views.

Utilization of One-Step Predictions.Previous works have utilized the outputs of Tweedie's formula to restore images  and to guide the generation process . However, the one-step predicted samples are used to compute the gradient from a predefined loss function to guide the sampling process, rather than for synchronization, which differentiates from our approach.

Concurrent works  also average the outputs of Tweedie's formula, similar to our approach, but they focus only on specific applications. For the first time, we present a general framework for diffusion synchronization and provide a comprehensive analysis of different synchronization methods across various applications.

## 5 Applications

We quantitatively and qualitatively compare SyncTweedies with the other diffusion synchronization processes, as well as the state-of-the-art methods of each application: 3D mesh texturing (Section 5.1), depth-to-360-panorama generation (Section 5.2),and 3D Gaussian splats  texturing (Section 5.3). Additional experiments and detailed setups are provided in **Appendix**, including (1) additional

Figure 4: **Qualitative results of 3D mesh texturing.** SyncTweedies and SyncMVD  generate realistic texture images, achieving better results than other baselines including finetuning-based method. Other diffusion synchronization cases fail to produce plausible textures.

   &  &  & Optim. &  \\  &  &  &  &  &  &  &  &  &  \\  & & & **Case 2** & & & Case 5 & & & & & & & \\  FID  \(\) & 135.61 & **21.76** & 36.12 & 131.67 & 22.76 & 31.66 & 28.23 & 34.98 & 26.10 \\ KID  \(\) & 68.63 & **1.46** & 6.60 & 65.70 & 1.74 & 5.69 & 2.30 & 6.83 & 2.51 \\ CLIP-S  \(\) & 25.26 & **28.89** & 27.88 & 25.31 & 28.82 & 28.04 & 28.55 & 28.63 & 27.94 \\   &  &  & Optim. &  \\  &  &  &  &  &  &  &  &  &  \\  & & & **Case 2** & & & Case 5 & & & & & & \\   &  \\  &  \\  &  \\ 

Table 3: **A quantitative comparison in 3D mesh texturing.** KID is scaled by \(10^{3}\). The best in each row is highlighted by **bold**.

qualitative results, (2) implementation details of each application, (3) arbitrary-sized image generation, (4) 3D mesh texture editing and diversity comparison, (6) runtime and VRAM usage comparisons, and (7) user preference evaluations.

Experiment Setup.In the case of instance variable denoising processes introduced in Section 3.2 (Cases 1-3), we initialize instance variables by projecting an initial canonical latent \(^{(T)}\) sampled from a standard Gaussian distribution \((,)\): \(_{i}^{(T)} f_{i}(^{(T)})\). For \(n\)-to-\(1\) projection cases (e.g.,3D Gaussian splats texturing), the instance variables are directly initialized from a standard Gaussian distribution which can avoid the variance decrease issue discussed in Section 3.4.4.

For instance space denoising processes, the final canonical variables are obtained by synchronizing the fully denoised instance variables at the end of the diffusion synchronization processes. Refer to Section 3.3.1 for the detailed definition of the canonical space \(\), the instance spaces \(\{_{i}\}_{i=1:N}\), the projection operation \(f_{i}\), and the unprojection operation \(g_{i}\) in each application.

Evaluation Setup.Across all applications, we compute FID  and KID  to assess the fidelity of the generated images and CLIP similarity  (CLIP-S) to evaluate text alignment. We use a depth-conditioned ControlNet  as the pretrained image diffusion model.

### 3D Mesh Texturing

In 3D mesh texturing, projection operation \(f_{i}\) is a rendering function which outputs perspective view images from a 3D mesh with a texture image. This operation represents a \(1\)-to-\(n\) projection due to discretization. We evaluate five diffusion synchronization cases along with Paint3D , a finetuning-based method, Paint-it , an optimization-based method, and TEXTure  and Text2Tex , which are iterative-view-updating-based methods. We use 429 pairs of meshes and prompts used in TEXTure  and Text2Tex .

Results.We present quantitative and qualitative results in Table 3 and Figure 4, respectively. The results in Table 3 align with the observations shown in the \(1\)-to-\(n\) projection case discussed in Section 3.4.3. SyncTweedies and SyncMVD  outperform other baselines across all metrics, but ours demonstrates superior performance compared to SyncMVD.

Notably, SyncTweedies outperforms Paint3D , a finetuning-based method, indicating that finetuning with a relatively small set of synthetic 3D objects  is not sufficient for realistic texture generation. This is further evidenced by the cartoonish texture of the car in row 1 of Figure 4. Optimization-based and iterative-view-updating-based methods produce unrealistic texture images, often exhibiting high saturation and visible seams, as seen in the baseball glove and light bulb in rows 2 and 3 of Figure 4. These issues are also reflected in the relatively high FID and KID scores in Table 3. See **Appendix** A for additional qualitative results.

### Depth-to-360-Panorama

We generate \(360^{}\) panorama images from \(360^{}\) depth maps obtained from the 360MonoDepth  dataset. Here, \(f_{i}\) projects a \(360^{}\) panorama to a perspective view image, which is an \(1\)-to-\(n\) projection due to discretization. We compare SyncTweedies with previous diffusion-synchronization-based methods  and MVDiffusion , which is finetuned using 3D scenes in the ScanNet  dataset. We generate a total of 500 \(360^{}\) panorama images at \(0^{}\) elevation, with a field of view of \(72^{}\).

Results.We report quantitative results of the five diffusion synchronization processes discussed in Section 3.2 in Table 4. Table 4 demonstrates a trend consistent with the \(1\)-to-\(n\) projection toy experiment results shown in Section 3.4.3. Specifically, SyncTweedies and Case 5, which synchronize the outputs of Tweedie's formula \(^{(t)}(,)\), exhibit the best performance. Notably, SyncTweedies demonstrates slightly superior performance across all metrics. On the other hand, MVDiffusion , which is finetuned using indoor scenes, fails to adapt to new, unseen domains and shows inferior results. The qualitative results are presented in **Appendix** A due to page limit.

    &  &  &  &  &  &  \\  & & **Case 2** & & & & \\  FID \(\) & 364.61 & **42.11** & 55.95 & 348.18 & 43.39 & 80.51 \\ KID \(\) & 375.42 & **21.19** & 34.67 & 362.77 & 22.87 & 56.91 \\ CLIP-S \(\) & 19.75 & **28.01** & 27.19 & 19.93 & 27.99 & 24.74 \\   

Table 4: **A quantitative comparison in depth-to-360-panorama application.** KID is scaled by \(10^{3}\). The best in each row is highlighted by **bold**.

### 3D Gaussian Splats Texturing

Lastly, to verify the difference between SyncTweedies and Case 5 both of which demonstrate applicability up to \(1\)-to-\(n\) projections as outlined in Section 3.4.3, we explore texturing 3D Gaussian Splats , exemplifying an \(n\)-to-\(1\) projection case. In 3D Gaussian splats texturing, the projection operation \(f_{i}\) is an \(n\)-to-\(1\) case, characterized by a volumetric rendering function . This function computes a weighted sum of \(n\) 3D Gaussian splats in the canonical space to render a pixel in the instance space. Note that in 3D Gaussian splats texturing, the unprojection \(g_{i}\) and the aggregation \(\) operation are performed using optimization.

While recent 3D generative models  generate plausible 3D objects represented as 3D Gaussian splats, they often lack fine details in the appearance. We validate the effectiveness of SyncTweedies on pretrained 3D Gaussian splats  from the Synthetic NeRF dataset . We use 50 views for texture generation and evaluate the results from 150 unseen views. For baselines, we evaluate diffusion-synchronization-based methods, the optimization-based methods, SDS , MVDream-SDS , and the iterative-view-updating-based method, Instruct-NeRF2NeRF (IN2N) .

Results.Table 5 and Figure 5 present quantitative and qualitative comparisons of 3D Gaussian splats  texturing. SyncTweedies, unaffected by the variance decrease issue, outperforms Case 5 both quantitatively and qualitatively, which is consistent with the observations from the toy experiments in Section 3.4.4. When compared to other baselines based on optimization (SDS  and MVDream-SDS ) and iterative view updating (IN2N ), ours outperforms across all metrics, especially by a large margin in FID . As shown in Figure 5, optimization-based methods tend to generate textures with high saturation, while the iterative-view-updating-based method produces textures lacking fine details. Additional qualitative results are shown in Appendix A.

## 6 Conclusion

We have explored various scenarios of diffusion synchronization and evaluated their performance across a range of applications, including ambiguous image generation, panorama generation, and texturing on 3D mesh and 3D Gaussian splats. Our analysis shows that SyncTweedies, which averages the outputs of Tweedie's formula while conducting denoising in multiple instance spaces, offers the best performance and the widest applicability.

Limitations and Societal Impacts.Despite the superior performance of SyncTweedies across diverse applications, updating both the geometry and appearance of 3D objects remains an open problem. Also, since the pretrained image diffusion model may have been trained with uncurated images, SyncTweedies might inadvertently produce harmful content.

Figure 5: **Qualitative results of 3D Gaussian splats  texturing.** [S\({}^{*}\)] is a prefix prompt. We use “Make it to” for IN2N  and “A photo of” for the others. SyncTweedies generates high-fidelity textures, while Case 5 lacks fine details due to the variance reduction issue.