# ANPL: Towards Natural Programming

with Interactive Decomposition

 Di Huang\({}^{1,}\), Ziyuan Nan\({}^{1,3,}\), Xing Hu\({}^{1}\), Pengwei Jin\({}^{1,3}\), Shaohui Peng\({}^{2}\),

**Yuanbo Wen\({}^{1}\), Rui Zhang\({}^{1}\), Zidong Du\({}^{1}\), Qi Guo\({}^{1}\), Yewen Pu\({}^{4}\) & Yunji Chen\({}^{1,}\)**

\({}^{1}\)SKL of Processors, Institute of Computing Technology, CAS

\({}^{2}\)Intelligent Software Research Center, Institute of Software, CAS

\({}^{3}\)University of Chinese Academy of Sciences

\({}^{4}\)Autodesk Research

###### Abstract

Though LLMs are capable of generating plausible programs, it's challenging to interact with the LLMs further to revise the program, especially if the user's specific requirements are different from the initial proposal. In this paper, we introduce ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions. Borrowing the paradigm of sketching from program synthesis, an ANPL program consists of a set of input-outputs that it must satisfy, a "_sketch_" -- control/data flow expressed in precise code (e.g. Python), and "_holes_" -- submodules to be implemented by the LLM specified with natural language. The user revises an ANPL program by either modifying the _sketch_, changing the language used to describe the _holes_, or providing additional input-outputs to a particular _hole_, turning it into a sub-ANPL program that can be solved recursively. This workflow allows the users to offload programming burdens to the LLM as much as possible while retaining the ability to pinpoint and resolve bugs locally, without exposing the rest of the program to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline programming systems that (a) without the ability to decompose tasks interactively and (b) without the guarantee that the modules can be correctly composed together. Additional evaluations on APPS, HumanEval, and real-world programming tasks have validated that the ANPL framework is applicable to multiple programming domains. We release the ANPL solutions to the ARC tasks as a dataset, providing insights into how humans decompose novel tasks programmatically.

1

2

## 1 Introduction

The rapid development of Large Language Models (LLMs) has made significant progress in the realm of code generation tasks . Compared to traditional approaches in program synthesis  that were constrained to specific, narrow domains (_e.g._, text manipulations), LLM-driven code generation can generate programs in a domain-general language (_e.g._, Python), making it broadly applicable. While LLM-driven code generators can save much of programmers' time in compiling typical programs - programs that are similar to those present in theirtraining data, it is unlikely that an LLM can compile a user-specific program in 1-shot, without any additional user interventions [35; 62]. In software development, most code is not written perfectly on the first attempt, and users need to spend much effort on testing and debugging[46; 63]. Therefore, although using LLM for code generation can significantly reduce the cost of code writing in 1-shot, the subsequent interactions - editing and debugging an LLM-generated program - remain an open challenge. This challenge is severe in two aspects: (1) The LLM-generated program can be long and contain complex dataflows, making it difficult to make localized changes while ensuring the dataflows stay intact; (2) Language descriptions are inherently ambiguous to describe specific code edits.

We hypothesize that decomposition - breaking down a complex problem into simpler problems, a common practice in program synthesis [2; 48], interactive dialogue systems [30; 72], and cognitive science [16; 27] to name a few - during the programming and debugging interaction between a user and an LLM code generator can tackle these aforementioned difficulties. We propose the ANPL (Abstract Natural Programming Language) system (Figure 1), which enables _stable_ yet _effective_ interactive editing/debugging to solve complex programming tasks that an LLM cannot correctly solve in 1-shot. The key design insight of ANPL is decoupling the dataflow "_sketch_" (something that requires little user effort) and the tedious module implementation "_holes_" (automated with LLM). With ANPL, users have explicit control over the high-level task decomposition using the sketch, leaving the tedious work of function-level holes implementation for LLMs. Each hole is a stand-alone ANPL program, allowing the user to recursively decompose it further.

We evaluated ANPL by conducting a large-scale user study using the Abstraction and Reasoning Corpus (ARC) , a well-known corpus that consists of 400 unique tasks (in the training set) without predefined programmatic solutions. We recruited 19 Python programmers who interacted with our system for a total of 440 man-hours. Compared to prior works evaluating LLMs' code generation capabilities in interaction with real users Xu et al. (166 man-hours), Vaithilingam et al. (7.87 man-hours), ours is the most comprehensive evaluation up-to-date to the best of

Figure 1: **ANPL overview**. Given a programming task (_e.g._, input-output), the user decomposes it into an ANPL program, consisting of programmatically defined **sketch** (control/data flows, solid lines, programmed by the user and preserved by LLMs), and natural language **holes** (function modules, dashed lines, generated by LLMs). The ANPL compiler compiles it to a Python program and checks the Python code against the taskâ€™s input-output. If incorrect, the compiler generates a debugging trace showing input-outputs for all the holes. The user can focus on debugging each hole independently, by either (1) decomposing it further, (2) explaining and modifying its natural language description, or (3) providing correct I/O by adjusting its traces.

our knowledge. We find that programmers interacting with ANPL perform significantly better than interaction with the vanilla LLM (75.0% tasks solved vs. 58.4%). The interaction between users and ANPL resulted in DARC, a dataset of programmatic Decomposition of ARC tasks, consisting of 300 instances of humans successfully decomposing ARC tasks to simpler tasks grounded in Python.

Specifically, this paper makes the following contributions:

* We present ANPL, a programming system that allows the user to explicitly manipulate dataflows while offloading the implementation of modules to an LLM, to decompose and ground programming tasks.
* We evaluate ANPL on a large-scale human study (440 man-hours) on a set of 400 ARC tasks, showing that programmers interacting with ANPL significantly outperform interaction with the vanilla LLM (GPT-3.5-turbo).
* We will release the collected programmatic decomposition dataset, DARC, to promote the development of research communities including LLMs, programming languages, human-computer interaction, and cognitive science.

## 2 Related Work

### Code Generation with LLMs

LLMs have been applied to tackle the code generation problem, which is the task of automatically finding a program satisfying user intent expressed in natural language or other specifications [7; 20; 23]. For example, Codex , a GPT model finetuned on public code repositories from GitHub, demonstrates the potential of code generation with LLMs. With the idea of generating candidate programs and filtering, AlphaCode  samples a large number of candidate programs and filters using input-output examples. Chen et al.  and Key et al.  utilize formal specifications (_e.g_., test cases and assertions) generated by LLMs to verify the generated programs. Synchroness  retrieves few-shot examples from training sets and then checks syntax, scope, typing rules, and contextual logic when synthesizing programs. Some work focuses on generating code based on feedback. CodeGen  factorizes a single program description into multiple parts and generates programs in multi-turns. Chen et al. , Le et al.  and Madaan et al.  provide LLMs with feedback messages generated by an executor, a critic network, or LLMs themselves to improve performance. Although these efforts have achieved a high level of code generation quality, they do not incorporate user interaction within the process of code generation. Basically, these methods can be integrated into our compiler to gain better compiling results. Another type of work, like Parsel, also utilizes the concept of decomposition . Different from Parsel, our work is an interactive system between users and LLMs, which allows users to debug programs through further clarifying and decomposing each module. Furthermore, we conduct a large-scale user study to evaluate the effectiveness of our system.

### User Interaction with LLMs

Interactions between humans and LLMs are necessary because it is difficult for LLMs to accurately understand user intent and generate correct answers in one shot [36; 52; 67; 70]. Among them, InternGPT  finds it helpful to combine the advantages of non-language instructions (_i.e_. pointing instructions) and language instructions. Instead of pointing instructions, our system allows humans to specify control/data flow precisely with programmatic language while keeping details in natural language. Furthermore, we have conducted extensive human studies on the ARC benchmark, which sufficiently demonstrate the capabilities of our system.

### Evaluation of LLM Code Generation Ability on Human Studies

Several works conduct a human study on code generation tools to survey generation accuracy and user experience [4; 38; 61; 69]. However, these works focus on analyzing _existing_ tools such as Copilot. The purpose of our human study is to verify the effectiveness of our proposed interactive system, ANPL. Furthermore, ARC containing abstract problems is a harder benchmark than those in previous studies. Our human study takes 440 man-hours on this hard benchmark, greater than the usual 100 man-hours, producing reliable results.

Other related areas including programming with natural language and LLMs as an interpreter are discussed in the Appendix A.

## 3 ANPL: The Abstracted Natural Programming Language

ANPL is a programming system that allows users to program and debug with natural language. It is Python-like and compatible with the original Python language, which provides users with great programming flexibility. In principle, we can apply our system to any programming language and in this paper, we have opted for Python as our base language. In this section, we will introduce ANPL's design principle and how to specify a ANPL program and then debug it.

### Design Principle

The main idea under ANPL is to decouple the description of _hole_ and _sketch_, which frees users from the tedious coding of function-level _hole_s by allowing them to write the natural language and empowers them with the privilege of precisely setting _sketch_ with few provided programming standards. Specifically, the _sketch_ represents the control/data flow, and the _hole_ represents the function module. This idea has been supported by the relevant experiences of early integrated systems like StreamBit  and SKETCH , which shows that users often have an idea about the general form of a solution (a high-level strategy) but feel it challenging to implement the program details. For programming with the help of LLMs, it offers the following advantages: 1) The user-specified _sketch_ can convey user intent more precisely. 2) It enables users to trace bugs along the _sketch_, rather than aimlessly rewriting the entire natural language program and regenerating it. 3) The relationships between modules are explicitly represented by the _sketch_, thus making it easier for the compiler to ensure that other correct parts are not affected by debugging actions when users are debugging the erroneous sections of the program, which is not an easy task due to the unpredictability of LLMs.

### Programming with ANPL

An ANPL program consists of a python-like _sketch_, and natural language _holes_.

Hole.A _hole_ implements a function module with a natural language description, which will be fleshed out by LLMs during the compiling process. Each _hole_ specified with a natural language description quoted by quotation marks'or "". When called, _holes_ should be organized by specifying its input-output variables, serving as the interconnections. To define a _hole_, users can either 1) define a _hole_ as a sub-function with the function name, parameters, and descriptions, and then call the function with its function name and input-output variables, or 2) just define and call it with descriptions and input-output variables inline.

Sketch.A _sketch_ is the control/data flow connecting different _holes_, specified with a programmatic language. Users constitute the _sketch_ by assigning names to variables and using them as _hole_ parameters in a data flow graph. Besides, users can write complex control flows with programmatic keywords (_e.g._, for, while, if) similar to that in Python to get more intricate ANPL programs.

An ANPL code example can be found in Figure 2, Figure 3, and Appendix D.1.

### Debugging with ANPL

As an ANPL program is already decomposed into _holes_ connected by the _sketch_, debugging over ANPL is targetted, where users can accurately locate bugs and make modifications to the relevant ANPL program.

Tracing.Tracing helps users to locate bugs. With the clear and explicit definition of _sketch_ in ANPL, users are allowed to detect bugs by checking the execution trace between high-level _hole_, which is not easy in previous work . Users can either use the preset "Trace" option in ANPL or just directly insert breakpoints in the code with print.

Providing input-output constraints.Users can provide input-output constraints, a typical gold standard for correctness, and ask the ANPL compiler to resynthesize the program to ensure the correctness of the compiled program. Each resynthesis will arise 1 API call with a batch size of 10 of the LLM.

Editing.ANPL allows users to debug as they would in traditional programming languages. Specifically, users can edit their code in four ways: 1) They can recursively decompose the original _hole_ into lower-level _holes_ connected by the _sketch_. 2) Reversely, they can abstract low-level _holes_ and the corresponding _sketch_ to a higher-level _hole_. 3) They can explain and modify the natural language descriptions of _holes_. 4) They can change the _sketch_ by coding (_e_.\(g\)., changing the input-output variables of _holes_).

In addition, ANPL has various syntactic sugars, for instance, implementing a recursion with natural language by using the Y combinator, see Appendix D for details.

## 4 The ANPL Compiler

One of the crucial tasks that a compiler must undertake is to gain the trust of its users - it should accurately reflect the underlying code and provide users with the possibility of debugging. To achieve this task, the ANPL compiler mainly focuses on two problems: 1) in the program generation process, how to generate a reliable executable program while still preserving the control/data flows specified by users, and 2) in the debugging process, how to allow users to modify and regenerate the erroneous parts of the program while preserving functionalities in the rest of the programs.

### Compiling ANPL

Preserving the _sketch_.To ensure the consistency of control/data flows before and after compilation, we first extract control/data flow information from the _sketch_ and individual natural language descriptions from the _holes_ separately. Then, we traverse and implement these _holes_ while ensuring the integrity of the _sketch_. Note that while LLMs take in the entire ANPL program as context to implement the program of its _holes_, the _sketch_ is fixed during program generation.

Figure 2: The pseudo-code of ANPL compiler, consisting of the direct compiling process and the differential compiling process.

Generating the _holes_.LLMs iterate over and fill the _holes_ one by one in their appearing order. When filling, LLMs can automatically create a name for the _hole_ and decompose it into sub-functions. The main problem is that LLMs cannot implement _hole_s reliably and may generate useless functions due to their unpredictability. The proposed solution begins with analyzing and clarifying the dependency relationships among the generated functions, and then identifying which function serves as the implementation of the target _hole_. The fundamental principle is that if the user explicitly names the _hole_, then any generated function with the same name as the user-specified one should be considered as the implementation of the _hole_, while other functions that it depends on are regarded as sub-functions of the _hole_; And otherwise, the function that does not depend on any other functions should be considered as the implementation of the _hole_. In the implementation, we first construct a dependency graph from the generated functions and find its entry nodes (nodes without any dependency on it) of the graph with topology sort. Then, we identify the target function based on three situations: 1) The user has named the _hole_ and we just match it with the entry node's name and remove unrelated nodes. 2) There is only one entry node named by LLMs and we fill the target _hole_ with it. 3) There are multiple entry nodes which means that there are multiple independent groups of functions and we cannot judge which one should be filled into the target _hole_, and thus we instruct LLMs to re-generate the implementation.

Finally, we get a compiled Python program that preserves user-specified _sketch_ and has a well-organized implementation of _hole_s.

### Compiling Differences

During the debugging process, users generate an updated ANPL program. The problem is how to modify the erroneous parts while preserving the rest of the compiled Python program.

We observe that this process can be modeled as the merging of dependency graphs. Specifically, the old ANPL program corresponds to one dependency graph, and the new program debugged by users serves as another new dependency graph, and the merging process. For merging, we propose three assumptions: 1) the user's current intention takes priority over previous intentions, 2) the LLM's previous intentions take priority over the current intention, and 3) the user's intentions take priority over the LLM's intentions. Assumption 1) and Assumption 3) are obviously valid: It is precisely because the user wants to modify their previous intention or correct the compiled result with LLM errors that they will perform debugging operations. Assumption 2) holds because we want to minimize the impact of debugging on the generated program: When the user's intention does not involve the code, the code should not undergo any changes due to LLM's compilation.

Based on these assumptions, we can derive a fundamental principle for the priority of dependency graph merging: the user's current intention > the user's previous intention > the LLM's previous intention > the LLM's current intention.

## 5 Human Studies

We conducted a user study on 400 ARC training tasks to evaluate the effectiveness of ANPL compared to the original ChatGPT (GPT-3.5-turbo). We hypothesized that if ANPL can help people to program with LLMs, people should be able to solve more tasks and get the right program with less effort (_i.e_. time consumption) on solving and programming tasks.

### Settings.

Stimuli.We use the ARC as our programming tasks. The ARC benchmark  was proposed to provide a standardized set of tasks that can be used to evaluate key aspects of human general intelligence, such as abstraction, generalization, object categorization, and so on [5; 13; 25; 29; 32; 40; 59; 60]. The primary objective of ARC is to require individuals to deduce a consistent procedure based on a limited number of abstract input-output examples, and then apply this procedure to generate a previously unseen output for a new input, as depicted in Figure 1. It is found that ARC is a representative programming dataset because 1) humans tend to solve them by coming up with programs 1, and 2) the solution of ARC cannot be found on the internet, which make its problems unique enough for LLMs (and participants).

Participants.We recruited 19 primary Python programmers from our institution. Due to the difficulty of solving ARC problems by interactive programming, we first recruited more than 19 participants and then filtered out 19 of them by asking each person to complete \(\)10 problems, checking if they can correctly operate the ANPL system, and asking if they wanted to continue. We paid an average of \(\) $8.80 USD per hour - a standard programmer's wage within our institution.

Design.1) Each participant is randomly assigned a set of non-overlapping ARC problems. The number of problems is different per participant depending on their preferred commitments. 2) There are two systems: System **A**, the ANPL, and System **B**, the original ChatGPT (GPT-3.5-turbo). Participants can interact with System B arbitrarily, including inspecting and changing Python programs manually. However, they cannot implement a whole Python function from scratch without interacting with System B. 3) As the programming level of participants can have a significant impact on their ability to solve ARC tasks, for each task, each participant used both System A and System B, presented in random order. 4) The participants were informed that the first interaction is crucial in order to minimize the impact of subsequent interactions. The programming interface of System A is shown in Figure 3 and System B shares a similar interface to System A.

Procedure.Given a set of tasks, participants can decide on the order in which to complete them, as they may need to start with simpler ones first. Participants are asked to solve the problem by programming (or interacting) with System A (or B) to get a Python program to solve the task. A soft time limit is 30 minutes which means that participants are suggested to give up after a 30-minute try. The held-out test input-output examples provided in ARC are used to check the correctness of the generated Python program, and participants can use additional input-output examples to debug their program. Participants are shown some prompt templates for using System B, while no prompts are needed in System A.

### Results

Participants solve more tasks with ANPL.First, we examine the problem-solving rate of ARC tackled by the participant when using different systems, shown in Figure 4. Specifically, we compare System A (ANPL), System B (GPT + interaction, _i.e._ ChatGPT), System C (ANPL without interaction), and System D (vanilla GPT), where Systems C and D represent the solving rate of one-shot generation without further interaction using Systems A and B, respectively. Results show that System A mean = 75.0%, 95%CI = [70.8%, 79.1%], B mean = 58.4%, 95%CI = [53.4%, 63.3%], C mean = 23.5%, 95%CI = [19.4%, 27.5%]), and D mean = 16.8%, 95%CI = [13.2%, 20.6%], where CI denotes confidence interval. Furthermore, we conduct paired t-tests on these systems, all of which are shown significant (

Figure 3: **ANPL user interface**. Participants utilize this programming interface to write and debug ANPL programs. The figure shows the ANPL program code entities along with corresponding _sketch_ and _holes_ diagrams. For a more detailed user interface, please refer to the Appendix E.3.

\(399,p<.0001\)): A-B: t=7.606, 95%CI=[12.2%,20.8%]; A-C: t=20.583, 95%CI=[46.6%,56.4%]; A-D: t=23.594, 95%CI=[53.4%,63.1%]; B-D: t=13.646, 95%CI=[30.0%,40.0%]; B-D: t=16.911, 95%CI=[36.9%,46.6%]; C-D: t=3.152, 95%CI=[2.5%,11.0%]. **In conclusion**: 1) With large confidence, ANPL performs the best and allows users to solve more problems, with a solving rate achieving 75.0% on average, while B achieves 58.4% (\(\) 28.25%). 2) Programming with interaction (Systems A and B) is always better than programming without interaction (Systems C and D). 3) Even for one-shot code generation, ANPL without interaction (System C) performs better than vanilla GPT (System D).

ANPL saves user effort.Next, we measure the user effort on different systems. Figure 4 illustrates the relationship between solving rate and time consumption, as well as the relationship between solving rate and the number of interactions. We find that compared with GPT, ANPL exhibits a rapid growth curve, implying a significant advantage in terms of efficiency in problem-solving. Furthermore, to estimate the coding effort, we conduct a comparative analysis of the average lines of code between ANPL and the generated Python code. Some ARC task is simple and one of the main reasons for the time cost on simple tasks is that during programming, much time is spent waiting for GPT's response. The average waiting time is 14.6% for System A and 23.5% for System B. Besides, the users were instructed to solve the tasks as fast as possible and they chose not to program directly when using System A because it would require more effort. Another way to analyze user effort is to calculate the lines of code. We find that the [min, max, mean] of lines of ANPL is [2, 76, 10.29] while the one of Python is [4, 113, 26.47], and note that writing natural language is simpler than coding. We also demo the ANPL's application on projects including MAGIC card game and LS-8 CPU with 67% lines of code reduction (See Appendix F.4).

User differences.The characteristics of the ARC benchmark lead to significant variations in user performance. In order to measure such differences, we conducted an anonymous statistical analysis of users' solving rates while utilizing Systems A and B. Figure 5 shows that different users achieve variant solving rates. The (mean, max, min) solving rate achieved by users when using System A is (71.1%, 95.5%, 16.7%) with variance 0.038, while System B is (57.2%, 84.2%, 33.3%) with variance

Figure 4: Results of problem-solving rates. A: Solving rate of four systems. B: The relationship between solving rate and time consumption. C: The relationship between solving rate and number of interactions. Trace Calculated(TC) means the trace mode is considered into interactions.

Figure 5: User differences. The solving rate of different users using System A and System B.

0.022. This illustrates that although System A surpasses B in most cases and on three metrics, it exhibits greater instability, suggesting the presence of a certain learning cost compared with B.

ANPL performs even better for programming beginners.In order to simulate the situation where users are unable to understand Python code (programming beginners), we designed a new set of experiments with System A and System B. Neither system provides users with specific Python code; instead, users are required to complete tasks only based on user interfaces and IO feedback. We selected four users for the experiment. To mitigate the potential impact caused by variations among users. Each user was given a set of 10 problems, which they had previously answered correctly using both System A and System B. In total, there were 40 problems administered. Results (Figure 6) show that System A mean = 89.9%, 95%CI = [79.4%, 97.8%] B mean = 37.2%, 95%CI = [21.6%, 52.5%], C mean = 40.2%, 95%CI = [24.3%, 55.6%], D mean = 12.5%, 95%CI = [2.9%, 25.0%]. With large confidence, ANPL performs the best and allows users to solve more problems, with a solving rate achieving 89.9% on average, while B only achieves 37.2% (\(\) 141.67%).

Percentage of different interaction modes.It can be observed from Figure 7 that among all the available options, the majority of users prefer the "edit" action, accounting for 46.2% total number of interactions and 46.6% total time cost, followed by the "trace" action, which constitutes 47.5% total number of interactions and 44.6% total time cost. This trend aligns with the typical programming workflow, where a smaller portion of time is dedicated to coding, while a significant portion is allocated to testing and debugging.

The influence of the system order.We denote the order as "A-B" ("B-A") meaning that the user first use System A (B) and then System B (A). With "A-B", the solving rate of System A is 75.5% (151/200) and System B is 62.5% (125/200). With "B-A", the solving rate of System A is 74.5% (149/200) and System B is 54.5% (109/200). We conclude that System B's performance increases 8% under the influence of System A, while System A's performance is more stable. This may be because System A tends to encourage users to approach problem-solving in a _sketch_ and _hole_ decomposition manner, and this manner is subconsciously applied to the subsequent System B. Therefore, we believe that "thinking in decomposition" itself can bring about certain improvements, and A leads users to this thinking manner. Additional results are shown in Appendix F.2.

Figure 6: Results of problem-solving rates when users can not see generated Python code and only interact with user interfaces and IO feedback. A: Solving rates of four systems. B: The relationship between solving rate and time consumption. C: The relationship between solving rate and number of interactions. Trace Calculated(TC) means the trace mode is considered into interactions.

Figure 7: The proportion of three interaction modes. (a) Time. (b) Frequency.

### DARC: A Recursive Decomposition Dataset of ARC Tasks

DARC contains all recorded interactions between participants and both the ANPL system and GPT-3.5-turbo on all 400 ARC tasks. 300 of the 400 tasks can be successfully decomposed and grounded in Python using ANPL (Figure 8), with an average of 2.7 _hole_s. Due to the characteristics of ARC, the data contained in DARC is not the only solution, and different users have a significant impact on the dataset. Still, the value of our dataset is tremendous, as it contains: programmatic decompositions of ARC tasks by humans using ANPL, Python code for 300 ARC tasks, and fine-grained interaction histories between users and a code-generating LLM.

## 6 Conclusion

In this paper, we propose ANPL, a programming system that enables users to program and debug with natural language with interactive decomposition. The system was evaluated on a large-scale user study on the 400 ARC tasks. Experimental results show that with large confidence, programmers interacting with ANPL perform significantly better than interaction with the vanilla LLM (75.0% tasks solved vs. 58.4%). Furthermore, we will release the collected programmatic decomposition DARC dataset, which provides valuable insights into how humans decompose programmatic tasks into simpler ones using natural language and input-outputs.

## 7 Limitations

Our limitations and future work mainly focus on two aspects: the naturalization of LLM responses and library learning. Specifically, 1) the ANPL system should return easily understandable natural languages instead of Python code as a response to humans, and 2) how to build a natural language library community through library learning  to allow users to share and reuse programs made by each other. Please see our detailed limitations and broader impact in the Appendix B & C.