# Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing

Yanfang Ling

Sun Yat-sen University

lingyf3@mail2.sysu.edu.cn &Jiyong Li

Sun Yat-sen University

lijy373@mail2.sysu.edu.cn &Lingbo Li

InfMind Technology Ltd

lingbo@infmind.ai &Shangsong Liang

Sun Yat-sen University

liangshangsong@gmail.com

Corresponding author.

###### Abstract

Recent methods are proposed to improve performance of domain adaptation by inferring domain index under an adversarial variational bayesian framework, where domain index is unavailable. However, existing methods typically assume that the global domain indices are sampled from a vanilla gaussian prior, overlooking the inherent structures among different domains. To address this challenge, we propose a Bayesian Domain Adaptation with **G**aussian **M**ixture **D**omain-**I**ndexing(GMDI) algorithm. GMDI employs a Gaussian Mixture Model for domain indices, with the number of component distributions in the "_domain-themes_" space adaptively determined by a Chinese Restaurant Process. By dynamically adjusting the mixtures at the domain indices level, GMDI significantly improves domain adaptation performance. Our theoretical analysis demonstrates that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. For classification, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to 3.4%, reaching 99.3%. For regression, GMDI reduces MSE by up to 21% (from 3.160 to 2.493), achieving the lowest errors among all methods. Source code is publicly available from https://github.com/lingyf3/GMDI.

## 1 Introduction

Machine learning models often suffer from performance degradation when applied to new domains that differ from their training domains, a phenomenon known as domain shift . Domain Adaptation (DA)  seeks to mitigate this issue by producing domain-invariant features, thereby enhancing generalization from source to target domains .

Recent research has explored the use of domain identity and domain index to improve domain-invariant data encoding and enhance domain adaptation performance . _Domain identity_, a one-hot discrete variable vector, differentiates between domains, whereas _domain index_, a real-valued continuous variable vector, captures domain semantics. Due to the limited information in the discrete domain identity vector, research has increasingly focused on the domain index. Current approaches to incorporating domain index in domain adaptation include: (1) Directly using existing additional information in the dataset as the domain index , which is impractical for datasets lacking such indices , and (2) Treating the domain index as a latent variable to be inferred . However, these methods typicallymodel the domain indices with a simple Gaussian distribution, limiting the domain indices space and thus hindering adaptation to diverse target domains, resulting in suboptimal performance.

To address the aforementioned issues, we propose a Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing (GMDI) algorithm. The proposed adversarial Bayesian algorithm assumes that domain indices follow a mixture of Gaussian distributions, with the number of mixture components dynamically determined by a Chinese Restaurant Process. As shown in Figure 1, a single Gaussian distribution struggles to adequately fit the domain indices, neglecting the inherent structures among different domains. This observation motivates us to model domain indices from different domains collectively as a Gaussian mixture distribution. To the best of our knowledge, we are the first to model domain indices as a mixture of Gaussian distributions to address the aforementioned challenges. Inspired by , the latent space of the mixture is defined as the "domain-themes" space. The mixtures of distributions provide a higher level of flexibility in a larger latent space, thereby increasing the capability to adapt to various target domains with domain shift. Our theoretical analysis demonstrates that GMDI achieves a more rigorous evidence lower bound, and that maximizing this bound along with adversarial loss effectively infers optimal domain indices. Extensive experimental results validate the significant effectiveness of GMDI.

Our key contributions are summarized as: (1) Our proposed GMDI is the first one to consider the entire distributions of domain indices in the "domain-themes" space following a mixture of Gaussian distributions, and dynamically determining the number of components in the mixture with the Chinese Restaurant Process. (2) Our detailed theoretical analysis demonstrates that training with GMDI's superior evidence lower bound together with adversarial loss can yield optimal and more interpretable domain indices. (3) Extensive experiments on classification and regression tasks showcase the strong domain index modeling capability of GMDI, significantly outperforming the state-of-the-art.

## 2 Related Work

**Adversarial domain adaptation.** There exists a substantial body of work on domain adaptation . They focus on generating domain-invariant data encoding by aligning the distributions of source and target domains to adapt to target domains. This alignment is achieved by directly matching the statistics of distributions  or by employing adversarial loss , which encourages domain confusion through adversarial objective with a discriminator. Adversarial domain adaptation is widely used due to its integration with deep learning, strong theoretical foundation , and superior performance. Various different types of adversarial losses have been explored:  uses an inverted label GAN loss,  utilizes a minimax loss, and  employs a cross-entropy loss against the uniform distribution. Typically, the discriminators in these models rely on _domain identity_, which contains limited information, to align data encoding distributions.  and  also pay attention to domain identity. Our work, however, focuses on _domain index_, providing a more detailed representation of domains.

**Domain adaptation related to domain indices.** Recently, there has been growing interest in using continuous _domain index_, which contain richer and more interpretable information, to enhance domain adaptation performance.  use the rotation angle of images as the domain index for the Rotating MNIST dataset and patients' ages as the domain index for Healthcare Datasets. Their theoretical analysis demonstrates the value of utilizing domain indices to generate domain-invariant features.  employ graph node embeddings as domain indices to achieve domain adaptation in graph-relational domains. These methods assume that domain indices are available. However, in practice, domain indices are not always accessible .  generates features representing the similarity between different domains but do not formally define the domain index.  formally define the domain index and treat it as a latent variable to be inferred. Although  takes steps towards Bayesian approximation to parameter distributions, it only assumes a single domain index

Figure 1: Illustration of domain indices modeled by different distributions.

distribution, limiting its capability to adapt to diverse target domains effectively. In contrast, we address this issue by representing the domain index with a dynamically updated mixture model.

## 3 Background

### Problem setup

We aim at unsupervised domain adaptation: given \(N\) domains with different domain shifts, each domain has a domain identity \(w=[N]\{1,...,N\}\), and each domain contains \(D_{w}\) data points. Similar to the conventional unsupervised domain adaptation setting, the \(N\) domains are divided into source domains with labeled data \(^{S}=\{(_{i}^{s},y_{i}^{s},w_{i}^{s})\}_{i=1}^{n_{e}}\) and target domains with unlabeled data \(^{T}=\{(_{i}^{t},w_{i}^{t})\}_{i=1}^{n_{e}}\). A foundational element that builds up our research problem is the diverse domain shifts  between different target domains and source domains. For source domains, the complexity of each target domain varies, which motivates us to dynamically infer domain indices in the "domain-themes" space and model them with dynamic Gaussian Mixture Model. We aim to (1) predict the label \(\{y_{i}^{t}\}_{i=1}^{n_{t}}\) of target domain data, and (2) infer local domain index \(_{w}^{B_{u}}\) and global domain index \(_{w}^{B_{}}\) in the dynamic "domain-themes" space. The summary of the notations is presented in Appendix J.

### Preliminary

**Domain index.** The domain index, distinct from domain identity \(w\), represents domain semantics, thus empowering it to significantly enhance domain adaptation performance. As per its definition [36; 41], the domain index satisfies the following : (1) To acquire domain-invariant data encoding \(\), the global domain index \(\) must remain independent of data encoding \(\), i.e., \(\!\!\!\) or equivalently \(p()=p()\). (2) Effectively representing data point \(\) while averting the occurrence of collapsing. (3) Ensuring optimal performance of downstream tasks utilizing the data encoding \(\) learned by the encoder under the aforementioned constraints, and necessitating the maintenance of sensitivity to labels.

**Variational domain index.** In circumstances where the domain index may not be readily accessible, the Variational Domain Index (VDI)  is a Bayesian approach to infer the domain index \(\) and \(\) as latent variables. VDI factorizes the generative model \(p(,y,,,)\) as:

\[p(,y,,,)=p( )p()p()p(,,)p(y)\,,\] (1)

where \(\) denotes the parameters for the prior probability distribution of the domain index \(\). As shown in Equation 1, VDI stands capable of significantly enhancing domain adaptation proficiency by leveraging the inferred domain index for generating data encoding \(\). Note that the independence between the domain index \(\) and data encoding \(\), i.e., \(p()=p()\), does not contradict \(p(,,)\), given the existence of multiple pathways between the domain index \(\) and data encoding \(\). Compared to VDI, which treats the distribution of domain index as a single Gaussian, we focus on a dynamic mixture of Gaussian distributions.

**Chinese Restaurant Process(CRP).** The Dirichlet Process (DP) is a classical method used for clustering. However, DP is difficult to construct directly, we apply the Chinese Restaurant Process(CRP) [16; 17; 18; 46] to implement it. Since similar domains have similar domain indices, the clusters formed by the domain indices correspond one-to-one with the components in mixture of Gaussian distributions. CRP can be employed to determine which cluster a domain belongs to (i.e., which component distribution domain index corresponds to). Specially, CRP is able to dynamically and adaptively determine the number of mixture components. CRP operates as follows:

\[P(v=k)=}{N-1+}k\,,\\ k\,,\] (2)

where \(n_{k}\) is the number of domain contained in cluster \(k\), and parameter \(\) is the concentration parameter of the CRP. A larger \(\) implies a tendency to generate more domain clusters.

## 4 Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing

### Overview of GMDI

We propose GMDI in order to infer more interpretable domain indices and thereby improve domain adaptation performance. Our model is constructed in three steps: First, in generate process, we model global domain indices as a dynamic Gaussian mixture model, with local indices generated from global domain index. Second, in inference process, we build structured variational inference to approximate the posterior of the latent variables. Finally, we train the model using an evidence lower bound with robust theoretical guarantees and an adversarial loss. Under this framework, GMDI has several significant advantages: (1) With global domain indices following a dynamic mixture of Gaussian distributions adaptively determined by CRP, it provides a higher level of flexibility in a larger latent space. (2) The evidence lower bound of our GMDI is more stringent, leading to more interpretable and optimal domain indices. The overview of GMDI is presented in Algorithm 1.

### Mixture of domain index distributions

Similar to VDI, GMDI (Figure 3 (right)) also considers the intermediate latent variable of local domain index \(\). The local domain index \(\) contains instance-level information, meaning that each data point has a unique local domain index. In contrast, the global domain index \(\) contains domain-level information, indicating that all data points within the same domain share the same global domain index. In VDI, with the local domain index \(\) derived from the global domain index \(\), the data distribution \(p(y,)\) is expressed as:

\[p(y,)= p()p()p()p(,,)p(y )\,ddd\,,\] (3)

where \(\) denotes the parameters for the prior probability distribution of the global domain index \(\).

With the setting of unsupervised domain adaptation, domains are not i.i.d, existing domain shift between domains. It implies that there may be substantial differences in distribution between domains. This leads to a problem that disparate target domains require a more significant degree of adaptation. Although we compute a distribution for domain index \(\) to enhance the capability of domain adaptation, it may not be effective enough to aid in adapting to a diversity of different target domains. Therefore, if local domain index \(\) are adapted from a simple Gaussian distribution of global domain index \(\), it may play a small role in improving the performance of domain adaptation.

To better model global domain index and thus enhance the effectiveness of domain adaptation, we propose to maintain a mixture of dynamically updated global domain index \(\) distributions in the "domain-themes" space. Intuitively, similar domains have similar global domain indices, implying that the mixture of global domain index distributions is associated with a cluster of similar domains. The process of adapting local domain indices from global domain indices is illustrated in Figure 2. Specifically, we consider a Gaussian Mixture Model (GMM) as the mixture of global domain index

Figure 2: The schematic diagram of domain index distributions. It shows the inference of variational Gaussian-shaped distributions for the global domain index, representing domain semantics. The process involves ranking candidate distributions in the “domain-themes” space, selecting the highest probability one, and deriving the local domain index from it.

distributions. For each distinct domain, we first rank the candidate global domain index distributions in "domain-themes" space and select the distribution with the highest probability. We then derive the local domain index \(\) from the global domain index \(\). Therefore, the global domain index is designed to be dynamical GMM.

Let \(v\) denote the latent categorical variable indicating the assignment of a domain to a cluster, which is equivalent to selecting components in a mixture distribution. Based on the definition of \(v\), we derive the updated representation of the distribution \(p(y,)\):

\[p(y,)= p(v)p(^{v} )p(^{v})p()p(,,^{v})p(y)\,ddddv\,.\] (4)

The component distribution \(^{v}\) is selected from the mixture distribution of \(\), and afterward, the local domain index \(\) is obtained from global domain index \(^{v}\) for generating domain-invariant data encoding \(\). Equation 3 and Equation 4 both represent the factorization of the distribution \(p(y,)\). \(\) in Equation 3 is the global domain index. While GMDI models the global domain index \(\) as a mixture of Gaussian distributions, \(^{v}\) in Equation 4 indicates the \(v\)-th component of the mixture distribution of \(\) with the prior \(p(v)\). Compared to the single distribution, the mixture of global domain index distributions adequately model the domain index of different domains, enhancing the effectiveness of domain adaptation in the face of varying degrees or even significant domain shifts. However, a remaining challenge is determining the number of components in the mixture distributions, especially when there are numerous domains, or even possibly infinite ones.

### Generative process of GMDI

In extreme cases, there may be infinite domains. Due to CRP's flexibility in dynamically determining the number of domain indices mixture components, we employ CRP to determine which cluster a domain belongs to (i.e., which component distribution domain index corresponds to). Specifically, we define the prior for domain indices cluster as a CRP, where the generation of new domain indices clusters is controlled by parameter \(\). Thus, the probability of a domain belonging to cluster \(k\) is calculated by Equation 2. Since CRP is an infinite mixture model, it is able to easily adapted to an infinite number of domains.

Mixture of domain indices need a stick-breaking representation of CRP to obtain component weights. Stick-breaking representation indicating an infinite construction, considering the Dirichlet prior with parameter \(\), each element in the probability vector \(=[_{1},_{2},...]\) is non-negative and the sum of the elements is 1:

\[_{k}(1,)k,\] (5)

\[_{k}=_{k}_{l=1}^{k-1}(1-_{l})k.\] (6)

Equation 6 is equivalent to the weights implied by CRP. Based on Equation 6 and Equation 4, the generative process of GMDI is as follows:

\[v_{}()\,,\] (7)

\[^{v=k}(_{k},_{k}^{2})\,,\] (8)

\[^{v=k} p(^{v=k})\,,\] (9)

\[ p()\,,\] (10)

\[,,^{v} p(,,^{v})\,,\] (11)

Figure 3: The probabilistic graphical model of VDI (**left**) and GMDI (**right**). Edge type ”- - ” denotes the independence between global domain index \(\) and data encoding \(\).

where \(_{k}\) and \(_{k}^{2}\) are mean vector and semi-positive covariance matrix of the \(k\)-th component in dynamic Gaussian mixture of domain indices, respectively. Figure 3 illustrates the generative process of VDI with a single distribution and GMDI with a mixture of distributions for domain indices. Since CRP is computationally intensive. To improve computational efficiency, we consider the stick-breaking construction to transform the infinite Gaussian mixture of domain indices into a finite one. It can be achieved by directly specifying an upper bound \(K\) for the number of components in Gaussian mixture of domain indices. Selecting an appropriate \(K\) allows to effectively reduce computational overhead. The finite version of the generative process of GMDI is available in Appendix A.

Accordingly, the generative model can be factorized as follows:

\[p(,y,,,,v,)=p( )p(v)p(^{v})p(^{v})p()p(,,^{v})p(y)\,.\] (12)

The predictor \(p(y)\) is a categorical distribution for classification tasks and a Gaussian distribution for regression tasks.

### Evidence Lower Bound

The exact posterior of all latent variables, i.e., \(p(,,,v,)\) is intractable, variational inference is used to approximate the posterior. Compared to the Monte Carlo sampling, variational inference allows both uncertainty quantification and computational efficiency. We employ structured variational inference to approximate the exact posterior, factorizing the approximate posterior \(q(,,,v,)\):

\[q(,,,v,)=q(;) q(v;)q(;_{u})q(^{v}; _{})q(,,^{v};_{z})\,,\] (13)

where \(,,_{u},_{}\) and \(_{z}\) respectively represent the parameters of the variational distributions \(q()\), \(q(v)\), \(q()\), \(q(^{v})\) and \(q(,,^{v})\).

We train GMDI by maximizing the evidence lower bound(ELBO) to obtain the optimal variational distributions which best approximate exact posterior distributions. Section 5 demonstrates that our proposed GMDI has a more stringent evidence lower bound. Based on generative and inference process of GMDI, we calculate the ELBO as follows:

\[_{} =_{q(,^{v},|;)q(v ;)}[ p(y|)]+_{q(|;_{u})}[  p(|)]\] \[+_{q(v;)q(;)q(|;_{u})q(^{v}|;_{})}[ p(| ^{v})]-[q(;)||p()]\] \[-_{q(;)}[[q(; {})||p(v|;_{v})]]-_{q(|;_ {u})q(v;)}[[q(|,,^{v};_{z})||p(|,,^{v})||p(^{v})]]\] \[-_{q(|;_{u})}[ q(|; _{u})]\,,\] (14)

where \(\) and \(\) represent the parameters of the variational distributions \(q(,^{v},|)\) and \(q(,^{v}|)\), respectively, and \([||]\) is the Kullback-Leibler divergence.

### Adversarial loss with a discriminator

To ensure the independence between global domain index \(\) and data encoding \(\) as defined, we follow VDI  by training an additional discriminator \(\) with an adversarial loss. As we prove in Section 5 that the independence between global domain index \(\) and data encoding \(\) relies on the independence between domain identity \(w\) and data encoding \(\), the adversarial loss is simplified to discriminate the domain identity \(w\):

\[_{}=_{p(w,)}_{q(|; _{z})}[(w|)]\,.\] (15)

### Objective function

Combining Equation 14 and Equation 15, the final objective of GMDI is:

\[_{}=_{D}_{}-* _{}\,,\] (16)

where \(\) denotes the hyper-parameter that balances two terms. Since the exact posterior of all latent variables is intractable, we propose to use a structured variational inference method to approximate the exact posterior. More details can be viewed in the appendix B.

**Variational distribution of \(\).** To derive the optimal variational distribution of \(\), we only consider the terms related to \(\) in \(_{}\), then we can get the posterior \(q(_{k};_{k})=(_{k};_{k,1}, _{k,2})\) with parameters \(_{k,1}=1+_{k}\) and \(_{k,2}=+_{i=k+1}^{K}_{i}\).

**Variational distribution of \(v\).** Similarly, the variational posterior of \(v\) can be calculated as a Categorical distribution \(q(v;)=_{K}(v;)\), where the pareareances can be updated as:

\[_{k} _{q(;)}[]+_{q( {u}|;_{u})q(^{v}|;_{})}[ p( |^{v})]-_{q(|;_{u})}[[q(^{v}|;_{})||p(^{v})]]\] \[-_{q(,^{v}|;)}[[q(|,,;_{z})||p(|,,)]]\,,\] (17)

where \(_{k=1}^{K}_{k}=1\) and \(q(;)=_{k=1}^{K-1}q(_{k};_{k})\).

**Variational distribution of \(\),\(\) and \(\).** With assuming that the latent parameters are sampled from Gaussian, we have the following forms:

\[q(^{v}|;_{}) =(_{},_{}^{2})\,,\] (18) \[q(|;_{u}) =(_{u},_{u}^{2})\,,\] (19) \[q(|,,^{v};_{z}) =(_{z},_{z}^{2})\,,\] (20)

where \(\) and \(^{2}\) are mean vector and semi-positive covariance matrix of Gaussian distribution. The parameters are updated by gradient descend. Specifically, we follow VDI  by using Earth Mover's Distance (EMD) and Multi-Dimensional Scaling (MDS) to infer \(^{v}\) from \(\).

## 5 Theory

In this section, we provide significant theoretical guarantees for our GMDI method. First, we give the upper bound for ELBO and adversarial loss respectively. Second, we prove the upper bound of the whole loss with mutual information and entropy only. Moreover, we show that the upper bound can be achieved when the conditions are satisfied. Finally, we prove the significant result that our ELBO is better than the VDI, which means that a mixture of Gaussian prior can get better results. See Appendix C for detailed proof.

**Lemma 1**: _The ELBO of \(p(,y)\) is bounded by the following formula with the Mutual Information, the Entropy and the \(\)-divergence:_

\[_{p(,y)}[_{}(p(,y))] I(y;)+I(;,,,v)-(H()+H(y))\] \[-_{q(,,,,v)}[[q (|,,v,)||p(|,,v,)]]\] \[-[q(,,v,|)||p(, {},v,)]\,.\]

The main difference between and Lemma 1 in GMDI and Lemma 4.1 in VDI  is the last two \(\) terms and the inclusion of \(v\).

**Lemma 2**: _(Information Decomposition of the Adversarial Loss )We can decompose the global maximum of adversarial loss as follows:_

\[_{D}_{p(w,)}_{q(|)}[( w|)]=I(;)+I(;w|)-H(w)\,.\]

_The global minimum of the function is achieved if and only if \(I(;)=0\) and \(I(;w|)=0\)._

**Theorem 1**: _The upper bound of the objective function can be decomposed as follows:_

\[_{} I(y;)+I(;,,,v)-I(;)-I(;w|)-(H()+H(y)-H(w))\,.\]

The main difference between Theorem 1 in GMDI and Theorem 4.1 in VDI  is the inclusion of \(v\).

**Theorem 2**: _The global optimum is achieved if and only if: (1)\(I(;)=I(;w|)=0\), (2)\(I(y;)\) and \(I(;,,x,v)\) are maximized, (3)\([q(,,v,|)||p(,,v, )]=0\) and \([q(|,,v,)||p(|,,v,)]=0\)._

The main difference between Theorem 2 in GMDI and Theorem 4.2 in VDI  is that \(I(;,,,v)\), which includes \(v\), needs to be maximized, and the two \(\) divergences should equal zero.

**Theorem 3**: _Assuming the ELBO and objective of VDI are \(_{}\) and \(_{}\) respectively, where domain indices are sampled from a simple Gaussian prior, we can prove that our objective achieves a more stringent evidence lower bound which is closer to the log-likelihood, and also a tighter upper bound of the objective: \(_{}_{} p( ,y)\) and \(_{}_{}\)._

## 6 Experimental Study

We verify the effectiveness of GMDI via experimental comparison and analysis. In particular, we answer three research questions: (**RQ1**) Can the performance of GMDI for domain adaptation outperform baselines? (**RQ2**) How effective is the global domain indices inferred by GMDI? (**RQ3**) How does the number of mixture components K affect results? Additional experimental results are available in Appendix K.

### Experimental setup

**Datasets.** We compare GMDI with existing DA methods on the following datasets (see Appendix H and Appendix I for more details): _Circle_ is used for binary classification task. _DG-15_ and _DG-60_ are synthetic datasets used for binary classification task. _TPT-48_ dataset is a real-world dataset used for regression task. W (6) \(\) E (42): Adapting models from the 6 states in the west to the 42 states in the east. N (24) \(\) S (24): Adapting models from the 24 states in the north to the 24 states in the south. _level-1 target domains_: one hop away from the closest source domain. _level-2 target domains_: two hops away from the closest source domain. _level-3 target domains_: more than two hops away from the closest source domain. _CompCars_ dataset is a real-world dataset for 4-way classification task.

**Baselines.** To evaluate our proposed GMDI, we compare it against eight state-of-the-art domain adaptation methods: Domain Adversarial Neural Networks (**DANN**) , Adversarial Discriminative Domain Adaptation (**ADDA**) , Conditional Domain Adaptation Neural Networks (**CDANN**) , Margin Disparity Discrepancy (**MDD**) , **SENTRKY**, Domain to Vector (**D2V**) , and Variational Domain Index (**VDI**) . Additionally, we include the results for models trained and tested only on the source domain (**Source-only**). Note that D2V is not applicable to regression tasks, so its results are not reported on the _TPT-48_ dataset. Moreover, since our proposed GMDI focuses on inferring domain indices when they are unavailable, whereas  and  assume domain indices

    &  \\  Dataset & Source-only & DANN & ADDA & CDANN & MDD & SENTRY & D2V & VDI & GMDI (Ours) \\  _Circle_ & 55.5 & 53.4 & 56.2 & 54.9 & 53.4 & 59.5 & 60.1 & 94.3 & **96.9** \\ _DG-15_ & 39.7 & 43.3 & 33.5 & 38.8 & 37.2 & 42.6 & 79.9 & 94.7 & **96.5** \\ _DG-60_ & 55.0 & 66.3 & 60.8 & 65.3 & 54.6 & 51.3 & 82.1 & 95.9 & **99.3** \\ _CompCars_ & 39.1 & 38.9 & 42.8 & 41.8 & 41.4 & 41.8 & 40.7 & 42.52  & **44.4** \\   

Table 1: Accuracy on binary classification tasks (_Circle_, _DG-15_, and _DG-60_) and 4-way classification task (_CompCars_).

Figure 4: MSE of domain indices on _TPT-48_ dataset. **Left:**N (24) \(\) S (24), ground-truth domain indices are latitude. **Right**: W (6) \(\) E (42), ground-truth domain indices are longitude.

[MISSING_PAGE_FAIL:9]

is obtained with \(K=3\). The results in Figure 6 show that accuracy is the lowest when \(K=1\), suggesting that maintaining a single domain index distribution is insufficient for diverse target domains. The choice of \(K\) is related to the concentration parameter of CRP and the dataset; the larger the concentration parameter and the more complex the dataset, the larger the value of \(K\) should be.

## 7 Conclusion and Limitations

In this work, we propose GMDI, a novel Gaussian Mixture Domain-Indexing algorithm, to address the challenge of inferring domain indices when they are unavailable. Unlike existing methods that assume global domain indices are sampled from a single static Gaussian, GMDI is the first one to utilize a mixture of dynamic Gaussians. The number of mixture components is determined adaptively by the Chinese Restaurant Process, enhancing the flexibility and effectiveness of domain adaptation. Our theoretical analysis confirms that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. Extensive experiments validate the effectiveness of GMDI in inferring domain indices and highlight its potential practical applications. Specifically, for classification tasks, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to 3.4%, reaching 99.3%. For regression tasks, GMDI reduces MSE by at least 16% (from 2.496 to 2.087) and by 21% (from 3.160 to 2.493), achieving the lowest errors among all methods. Despite these advantages, GMDI still relies on the availability of domain identities and cannot infer them as latent variables. Future work will focus on developing algorithms capable of inferring domain indices together with domain identities to further enhance the robustness and applicability of our approach.