# Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion

Zhiwei Bai\({}^{1}\), Jiajie Zhao\({}^{1}\), Yaoyu Zhang\({}^{1}\)

\({}^{1}\) School of Mathematical Sciences, Institute of Natural Sciences, MOE-LSC,

Shanghai Jiao Tong University, Shanghai 200240, P.R. China.

{bai299, zjj0216, zhyy.sjtu}@sjtu.edu.cn.

Corresponding author: zhyy.sjtu@sjtu.edu.cn.

###### Abstract

Matrix factorization models have been extensively studied as a valuable test-bed for understanding the implicit biases of overparameterized models. Although both low nuclear norm and low rank regularization have been studied for these models, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive. In this work, we systematically investigate the implicit regularization of matrix factorization for solving matrix completion problems. We empirically discover that the connectivity of observed data plays a crucial role in the implicit bias, with a transition from low nuclear norm to low rank as data shifts from disconnected to connected with increased observations. We identify a hierarchy of intrinsic invariant manifolds in the loss landscape that guide the training trajectory to evolve from low-rank to higher-rank solutions. Based on this finding, we theoretically characterize the training trajectory as following the hierarchical invariant manifold traversal process, generalizing the characterization of Li et al. (2020) to include the disconnected case. Furthermore, we establish conditions that guarantee minimum nuclear norm, closely aligning with our experimental findings, and we provide a dynamics characterization condition for ensuring minimum rank. Our work reveals the intricate interplay between data connectivity, training dynamics, and implicit regularization in matrix factorization models.

## 1 Introduction

Overparameterized models have the capacity to easily fit data with random labels (Zhang et al., 2017, 2021). However, in real-world applications, models with more parameters than training samples still generalize well. This has led researchers to hypothesize that overparameterized models undergo implicit regularization, favoring certain functions as outputs. Overparameterized matrix factorization models, \(}=\) with \(=(,),,^{d d}\), have served as a simplified test-bed for studying this implicit regularization. In the context of matrix completion problems like the Netflix challenge, these models aim to find a low-rank completion of a partially observed matrix \(^{d d}\). Prior works have offered seemingly conflicting perspectives on the implicit regularization at play, with some claiming it promotes low nuclear norm (Gunasekar et al., 2017) and others arguing for low rank (Arora et al., 2019; Li et al., 2020; Razin and Cohen, 2020). However, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive.

Unlike previous works that focus on either low rank or low nuclear norm regularization,, we systematically investigate the training dynamics and implicit regularization of matrix factorization for matrix completion. Through extensive experiments, we found that a certain connectivity property of observed data plays a key role in the implicit regularization effects. Data connectivity, in the context of this paper, refers to the way observed entries in the matrix are linked through shared rowsor columns. A set of observations is considered connected if there's a path between any two observed entries via other observed entries in the same rows or columns. This concept plays a crucial role in determining the behavior of matrix factorization models, as we will demonstrate throughout this paper. As shown in Fig. 1, we sample observations randomly from a ground truth matrix \(^{*}^{d d}\) with \((^{*})<d\) and train models \(}=,,^{d d}\) from small random initialization without any rank constraints. For each observation set, we calculate the solutions with the minimum nuclear norm and minimum rank, which serve as the ground truth benchmarks. These are then compared with the completion matrix obtained by the model. From Fig. 1, we observe that:

(i) **Low rank bias in connected case:** When the observed entries are connected, the model consistently learns the lowest-rank solution.

(ii) **Low nuclear norm bias in certain disconnected case:** When the observed entries are disconnected, the model generally does not find the minimum nuclear norm or lowest-rank solution. However, in the special case where each connected component is a complete bipartite subgraph, the model consistently finds the minimum nuclear norm solution.

To understand how data connectivity modulates the implicit bias, we analyze the loss landscape and optimization dynamics. We find a hierarchy of intrinsic invariant manifolds \(_{k}\) of different ranks in the loss landscape. These manifolds constrain the optimization trajectory, causing the model to learn by incrementally ascending through higher ranks. In the disconnected case, additional sub-\(_{k}\) invariant manifolds emerge within the \(_{k}\) invariant manifold, preventing the model from reaching the global lowest-rank solution. However, we prove that the minimum nuclear norm solution is guaranteed in the disconnected with complete bipartite subgraph case.

The contributions of our work are summarized as follows:

(i) We systematically investigate the influence of data connectivity on the implicit regularization. Our empirical findings indicate that the connectivity of observed data plays a key role in the implicit bias, leading to a transition from favoring solutions with a low nuclear norm to those with a low rank as the data becomes more connected with an increase in observations (refer to Sec. 4).

(ii) We characterize the training dynamics of matrix factorization theoretically, showing that the optimization trajectory follows a _Hierarchical Invariant Manifold Traversal (HIMT)_ process. This generalizes the characterization of Li et al. (2020), whose proposed _Greedy Low-Rank Learning(GLRL)_ algorithm equivalence only corresponding to the connected case (refer to Sec. 5 and Sec. 6.1).

(iii) Regarding the minimum nuclear norm regularization, we establish conditions that provide guarantees closely aligned with our empirical findings, which complement the results of Gunasekar et al. (2017). For the minimum rank regularization, we present a dynamic characterization condition that assures the attainment of the minimum rank solution (refer to Sec. 6.2).

Figure 1: **The connectivity of observed data affects the implicit regularization. The ground truth matrix \(^{*}^{4 4}\) has rank ranging from 1 to 3. The sample size \(n\) covers settings where \(n\) is equal to, smaller than, and larger than the \(2rd-r^{2}\) threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)**

Related works

Norm minimization and rank minimization.Extensive research has been conducted on the implicit regularization of matrix factorization models, focusing on norm minimization and rank minimization. For norm minimization, Gunasekar et al. (2017) proved that gradient flow with infinitesimal initialization converges to the minimum nuclear norm solution in the special case of commutative observations. Ji and Telgarsky (2019); Gunasekar et al. (2018) studied norm minimization regularization in deep linear networks. For rank minimization, numerous works have shown that matrix factorization models favor low-rank solutions. Arora et al. (2019); Gidel et al. (2019); Gissin et al. (2019); Razin and Cohen (2020); Jiang et al. (2023); Belabbas (2020) investigated how infinitesimal initialization of gradient flow encourages low rank in specific settings. Li et al. (2020) showed that under certain assumptions, matrix factorization dynamics are equivalent to a greedy low-rank learning heuristic. Li et al. (2018); Stoger and Soltanolkotabi (2021); Jin et al. (2023) established low-rank recovery guarantees for matrix sensing problems under the Restricted Isometry Property (RIP) condition. Zhang et al. (2022, 2023) studied a broader class of model rank minimization for nonlinear models, of which the matrix factorization model is a special case.

Nonlinear dynamics.The initialization scale can significantly influence the implicit regularization of neural networks. Large initialization typically leads to linear dynamics (Jacot et al., 2018) and poor generalization (Chizat et al., 2019), while small initialization induces nonlinear dynamics (Luo et al., 2021). In this work, we focus on the case of infinitesimal initialization, which corresponds to highly nonlinear dynamics. An important characteristic of nonlinear neural network dynamics is the phenomenon of condensation (Luo et al., 2021; Zhou et al., 2022), where the network's effective complexity is small. The low-rank \(_{k}\) invariant manifolds we propose are essentially a manifestation of condensation. Zhang et al. (2021, 2022); Bai et al. (2022); Fukumizu et al. (2019); Simsek et al. (2021) established the embedding principle of the loss landscape of neural networks and empirically demonstrated that the training process traverses critical points embedded from smaller subnetworks. Jacot et al. (2021) conjectured a saddle to saddle dynamics for deep linear networks, which is conceptually analogous to the dynamics characterization in this work.

## 3 Preliminaries

Matrix completion problem.This study focuses on the matrix completion problem, which involves estimating missing entries within a partially observed matrix. Given an incomplete matrix \(^{d d}\), the goal is to predict the entirety of \(\) based on its observed elements. The set of observed entries is represented as \(S=\{(i_{k},j_{k}),_{i_{k},j_{k}}\}_{k=1}^{n}\), where \((i_{k},j_{k})\) indicates the row and column indices, and \(_{i_{k},j_{k}}\) is the corresponding value assumed non-zero in the matrix. The set of observed indices is defined as \(S_{}=\{(i_{k},j_{k})\}_{k=1}^{n}\). Entries that are not observed, denoted by \(\), are considered missing or unknown. The positions of observed elements in the matrix \(\) are defined by a binary observation matrix \(\), where \(_{ij}=1\) indicates that \(_{ij}\) is observed, and \(_{ij}=0\) indicates that \(_{ij}\) is unobserved.

Matrix factorization model.Matrix factorization is a prevalent approach for addressing the matrix completion problem. It reconstructs the matrix \(^{d d}\) through the product \(=\), where \(^{d r}\) and \(^{r d}\). This work studies the overparameterized scenario with \(r=d\), aiming to understand the implicit regularization effect in the absence of explicit rank restrictions, paralleling prior research (Gunasekar et al., 2017; Arora et al., 2019; Li et al., 2020; Jin et al., 2023). In this work, we focus on the asymmetric factorization, which can be represented as a parametric model:

\[_{}=,,^{d d}. \]

The matrix factorization model parameters are denoted by \(=(,)\), identified with its vectorized form \(()^{2d^{2}}\). The augmented matrix is \(_{}^{}=^{}& ^{}^{d 2d}\), and \(()\) and \(()\) denote the row and column spaces of \(\) and \(\), respectively. The augmented matrix \(_{}\) plays a crucial role in our subsequent analysis, particularly in characterizing the intrinsic invariant manifolds \(_{k}\) of the optimization process. Specifically, it allows us to establish the relationship \(()=(^{})=(_{})\), which is important to understanding the invariance property under gradient flow.

Loss function.The learning process for the parameters \(=(,)\) involves minimizing a loss function that measures the difference between observed and estimated entries. In this work, we focus on the mean squared error, and the empirical risk is thus formulated as

\[R_{S}()=\|(-)_{S_{}}\|_{F}^{2}:= _{k=1}^{n}(_{i_{k}}_{,j_{k}}-_{i_{k },j_{k}})^{2}, \]

where \(_{i}\) and \(_{,j}\) represent the \(i\)-th row and \(j\)-th column of matrix \(\) and \(\), respectively. The residual matrix \(=(-)_{S_{}}\) has elements \(_{ij}=()_{ij}-_{ij}\) for \((i,j) S_{}\) and \(_{ij}=0\) for \((i,j) S_{}\). The training dynamics follow the gradient flow of \(R_{S}()\):

\[}{t}=-_{}R_{S}(),(0)=_{0}. \]

In all experiments, \(_{0} N(0,^{2})\) is initialized from a Gaussian distribution with mean 0 and small variance \(^{2}\). We use gradient descent with a small learning rate to approximate the gradient flow dynamics (Please refer to Appendix B.1 for the detailed experiment setup).

## 4 Connectivity affects implicit regularization

In this section, we define connectivity and present experimental results on implicit regularization for connected and disconnected observational data.

**Definition 1** (**Associated Observation Graph**).: _Given a incomplete matrix \(\) to be completed and its observation matrix \(\), the associated observation graph \(G_{}\) is the bipartite graph with adjacency matrix \(&^{}\\ &\), with isolated vertices removed._

**Definition 2** (**Connectivity**).: _Given a incomplete matrix \(\) to be completed, it is considered connected if its associated observation graph \(G_{}\) is connected; otherwise, it is disconnected. The connected components of \(\) are defined as the connected components of \(G_{}\)._

The connectivity of the graph, as defined above, reflects the connectivity of the observed data. Appendix A Sec. A.2 provides a detailed discussion on the equivalent definition of connectivity.

In the case of disconnectivity, there is a special case where each connected component has full observations, characterized by disconnectivity with complete bipartite components.

**Definition 3** (**Disconnectivity with Complete Bipartite Components**).: _A incomplete matrix \(\) is considered disconnected with complete bipartite components if its associated observation graph \(G_{}\) is disconnected and each connected component forms a complete bipartite subgraph._

We present examples to demonstrate how connectivity influences the characteristics of the learned solutions. Consider three matrices to be completed, each obtained by adding one more observation to the previous matrix: \(_{1}\) (disconnected), \(_{2}\) (disconnected with complete bipartite components), and \(_{3}\) (connected). Fig. A1 of Appendix B illustrates the associated graphs \(G_{}\).

\[_{1}=1&2&\\ 3&&\\ &&5,_{2}=1&2&\\ 3&4&\\ &&5,_{3}=1&2&\\ 3&4&\\ 6&&5. \]

Figs. 2(a-b) compare the learned matrices with the ground truth (GT) solutions having the smallest nuclear norm and rank. For disconnected \(_{1}\) (blue bars), the learned solution achieves neither the smallest nuclear norm nor rank. For disconnected \(_{2}\) with complete bipartite components (green bars), the learned matrix has the smallest nuclear norm but not rank. For connected \(_{3}\) (red bars), the lowest rank-2 solution is not unique; the model identifies a particular lowest rank-2 solution, but it does not correspond to the one with the minimum nuclear norm.

To thoroughly study all possible cases, we examine all sampling patterns of the \(3 3\) matrix completion. Fig. 2(c) shows that the model consistently learns the lowest-rank solution for connected sampling patterns but fails to do so for disconnected patterns. Fig. 2(d) further verifies the impact of connectivity on low-rank matrix recovery by comparing the reconstruction error for 100 randomly sampled rank-1 matrices using two connected sampling patterns (red and blue dots) and one disconnected sampling pattern (green dots). The model consistently achieves small reconstruction errors under connected sampling patterns, while the error is significantly larger for the disconnected pattern.

These empirical results demonstrate an implicit preference for low rank induced by connectivity and a preference for low nuclear norm in a particular kind of disconnection. In the following section, we will investigate the training dynamics under both connected and disconnected scenarios.

## 5 Training dynamics in connected and disconnected cases

### Connected case

This section empirically demonstrates the detailed dynamics of connected observed data. Fig. 3(a) shows the connected target matrix \(\) with a single unknown element denoted by \(\). The rank of \(\) is at least three and equals three if and only if \(=1.2\).

Learning lowest-rank solution.We initialize \(\) and \(\) with different scales and record the singular values of the learned matrix. As depicted in Fig. 3(b), when starting with larger initialization, the learned solutions are almost always rank-4. Conversely, as the initialization scale decreases, the first three singular values of the learned solution are consistently maintained in magnitude, but the fourth singular value keeps decreasing, resulting in the model learning the lowest rank-3 solution.

Traversing progressive optima at each rank.For a small random initialization (Gaussian distribution with mean 0 and variance \(10^{-16}\)), the loss curves exhibit a steady, stepwise decline (Fig. 3(c)). The flat periods correspond to small gradient norms, indicating potential saddle points (Fig. 3(d)). We compare the matrices learned at these saddle points with the optimal approximation of each rank and plot their difference in Fig. 3(d), which is very small. These findings suggest that the model starts near \(\) (rank-0) and progressively finds optimal approximations within rank-1, rank-2, and higher-rank manifolds until reaching a global minimum.

Alignment of the row space of \(\) and the column space of \(\).Starting with small initialization, we track the rank (number of significantly non-zero singular values) of \(=\), \(\), \(\), and the augmented matrix \(_{}\) during the training process. We observe that the rank gradually increases, with singular values growing rapidly one after another (Fig. 3(e-h)). Throughout the entire process, we consistently find that \(()=(^{})=(_{})\), which implies that the row space of \(\) and the column space of \(\) remain aligned at all times. This alignment corresponds to a special structure that we refer to as the "Hierarchical Intrinsic Invariant Manifold" in Sec. 6.1, which plays a crucial role in the overall dynamics of the system.

Figure 2: (a) Nuclear norms of the learned solutions for \(_{1}\), \(_{2}\), and \(_{3}\). Dashed lines represent theoretically computed smallest nuclear norms. (b) Singular values of the learned matrices for \(_{1},_{2},_{3}\). Each set of three bars represents the singular values of a matrix. The thick vertical lines partition significantly nonzero singular values, which serves as the empirical rank. The text (GT) shows the ground truth minimum rank. Mean and standard deviation are recorded over 100 repetitions. (c) All equivalent sampling patterns of the \(3 3\) matrix completion problem (see Appendix B for details). Cyan stars marked the case learning the lowest-rank solution. (d) Reconstruction error of the solutions for a \(10 10\) matrix reconstruction problem with \(^{*}\) randomly sampled at rank \(r=1\) and sample size set to the minimum reconstruction setting \(n=2rd-r^{2}\).

The dynamics of increasing ranks step by step aligns with the description of _Greedy Low Rank Learning (GLRL)_(Li et al., 2020). However, we will show next that when the observed data are disconnected, the learning process is not equivalent to GLRL.

### Disconnected case

In this section, we present a typical experiment in the disconnected situation. As depicted in Fig. 4(a), the target matrix \(\) contains four unknown elements denoted by \(\) and is disconnected. The rank of \(\) is at least one, and there are infinitely many rank-1 solutions.

Alignment of the row space of \(\) and the column space of \(\).As shown in Fig. 4(b-e), the learning process in the disconnected case is similar to the previous experiment: the model naturally evolves from low-rank to high-rank, with each step increasing a singular value and satisfying \(()=(^{})=(_{})\). Fig. 4(f) illustrates that as the initialization scale decreases, the model tends to learn symmetric solutions. However, unlike the connected case, the output does not approach a particular solution as the initialization decreases. For this specific disconnected \(\), we will show that every symmetric solution learned is a minimal nuclear norm solution(see Sec. 6.2 Thm. 4). For fewer observations, the experimental phenomena are similar (see Appendix B Fig. B5).

Lowest-rank solution is not learned.Despite the adaptive learning behavior, the final learned solution has rank 2, as evidenced by the two significantly non-zero singular values in Fig. 4(b-d). Examining the dynamics (3), we find that they decouple into two independent systems: one for the 1st and 3rd rows of \(\) and columns of \(\), and another for the 2nd row of \(\) and column of \(\). Fig. 4(g) shows that the model first learns the surrounding elements \(1,3,3,9\) (rank-1 saddle point), then learns the middle element \(5\) in the next stage. The decoupling of dynamics is equivalent to the definition of disconnection (see Appendix A Prop. A.4 for proof). In Fig. 4(e), we fixed a rank-1 matrix and explored all nine disconnected sampling patterns with 5 observations. For each pattern, we conducted experiments with small initializations. The loss curves consistently indicate that in disconnected cases, the model learns a sub-optimal solution in the rank-1 manifold, ultimately resulting in a rank-2 solution. This demonstrates that regardless of the specific disconnected sampling pattern, the model fails to achieve the optimal low-rank solution.

Figure 3: (a) The matrix \(\) to be completed, with the \(\) position unknown. (b) The four singular values of the learned solution at different initialization scale (Gaussian distribution, mean 0, variance from \(10^{0}\) to \(10^{-16}\)). (c) Training loss for 16 connected sampling patterns in a \(4 4\) matrix, each covering 1 element and observing the remaining 15 in a fixed rank-3 matrix. (d) Evolution of the \(l_{2}\)-norm of the gradients throughout the training process. The cyan crosses represent the difference between the matrix corresponding to the saddle point and the optimal approximation at each rank. (e-h) Evolution of singular values for matrices \(,,\), and \(_{}\) during training.

Not equivalent to GLRL in disconnected case.We compare the GLRL algorithm (Li et al., 2020) with the matrix factorization model for solving the same matrix completion problem (Fig. 4). Li et al. (2020) claim that the matrix factorization dynamics is mathematically equivalent to the GLRL algorithm under reasonable assumptions. While GLRL learns the same rank-1 saddle point shown in Fig. 4(g) in the first stage, it then fills unobserved elements with 0, resulting in a unique rank-2 solution (Fig. 4(h)). In contrast, the matrix factorization model learns symmetric solutions with some degree of freedom depending on the random seed (Fig. 4(f)). The key difference is that the first critical point (Fig. 4(g)) reached by the trajectory is a sub-optimal and not a second-order stationary point of the rank-1 manifold as assumed by Li et al. (2020). Therefore, the equivalence assumption between GLRL and matrix factorization does not hold in the disconnected case.

## 6 Theoretical analysis of training dynamics and implicit regularization

### Characterization of training dynamics

Matrix factorization models exhibit a distinctive adaptive learning behavior, progressively evolving from low rank to high rank. Understanding this phenomenon is rooted in grasping the global dynamics of matrix factorization models, where the role of intrinsic invariant manifolds becomes critical.

**Proposition 1** (Hierarchical Intrinsic Invariant Manifold (Hilm)).: _(see Appendix A Prop. A.1 for Proof) Let \(}=\) be a matrix factorization model and \(\{_{1},,_{k}\}\) be \(k\) linearly independent vectors. Define the manifold \(_{k}\) as \(_{k}:=_{k}(_{1},,_{k})=\{ =(,)()=( )=\{_{1},,_{k}\}\}\). The manifold \(_{k}\) possesses the following properties:_

_(i) **Invariance under Gradient Flow:** Given data \(S\) and the gradient flow dynamics \(}=- R_{S}()\), if the initial point \(_{0}_{k}\), then \((t)_{k}\) for all \(t 0\)._

_(ii) **Intrinsic Property:**\(_{k}\) is a data-independent invariant manifold, meaning that for any data \(S\), \(_{k}\) remains invariant under the gradient flow dynamics._

_(iii) **Hierarchical Structure:** The manifolds \(_{k}\) form a hierarchy: \(_{0}_{1}_{k-1}_{k}\)._

Figure 4: (a) The matrix to be completed, with unknown entries marked by \(\). (b-d) Evolution of singular values for \(\), \(\), and \(_{}\) during training. (e) Training loss for 9 disconnected sampling patterns in a \(3 3\) matrix, each covering 4 elements and observing the remaining 5 in a fixed rank-1 matrix. (f) Learned values at symmetric positions \((1,2)\) and \((2,1)\) under varying initialization scales (zero mean, varying variance). Each point represents one of ten random experiments per variance; labels show initialization variance. Other symmetric positions exhibit similar behavior. (g) Learned output at the saddle point corresponding to the red dot in (e). (h) Final learned solution of the GLRL algorithm (Li et al., 2020).

Figs. 3(f-h) and Figs. 4(b-d) show that the training process with small initialization consistently satisfies \(()=(^{})=(_{})\), aligning with the \(_{k}\) invariant manifold. Since a non-zero initialization in practice, the training trajectory is close to the \(_{k}\) invariant manifold, approaches a critical point, and transitions to the next level invariant manifold without getting trapped.

In both connected and disconnected scenarios, we observe a step-by-step hierarchical \(_{k}\) invariant manifold traversal. In the connected case, at each level we observe that the model reaches an optimal solution (Fig. 3). However, in the disconnected case, we can prove that each connected component induces a sub-\(_{k}\) invariant manifold, leading to the experimentally observed sub-optimal solution (see Fig. 4).

**Proposition 2** (**Intrinsic Sub-\(_{k}\) Invariant Manifold**).: _(see Appendix A Prop. A.2 for Proof) Let \(_{}=\) be a matrix factorization model, \(\) be an incomplete matrix and \(_{k}\) be an invariant manifold defined in Prop. 1. If \(\) is disconnected with \(m\) connected components, then there exist \(m\) sub-\(_{k}\) manifolds \(_{k}\) such that \(_{k}_{k}\), each possessing the following properties:_

_(i) **Invariance under Gradient Flow:** Given data \(S\) and the gradient flow dynamics \(}=- R_{S}()\), if the initial point \(_{0}_{k}\), then \((t)_{k}\) for all \(t 0\)._

_(ii) **Intrinsic Property:**\(_{k}\) _is a data-value-independent invariant manifold, meaning that for a fixed sampling pattern in_ \(\) _and any observed values_ \(S\)_,_ \(_{k}\) _remains invariant under the gradient flow._

_(iii) **Strict Subset Relation:** The output set \(\{_{}_{k}\}\) is a proper subset of \(\{_{}_{k}\}\), namely, \(\{_{}_{k}\}\{_ {}_{k}\}\)._

Fig. 5(a) illustrates the trajectory of the experiment in Fig. 4. In the disconnected case, sub-\(_{k}\) invariant manifolds exist and attract the dynamics, leading the model to learn sub-optimal solutions on the entire \(_{k}\) invariant manifold. In fact, we can prove that these sub-optimal solutions are necessarily strict saddle points. This loss landscape result extends Theorem 5.10 from Li et al. (2020), which established the findings for the specific case of symmetric matrix factorization models (see Appendix A Sec. A.3 for a detailed discussion).

**Theorem 1** (**Loss Landscape**).: _(see Appendix A Thm. A.3 for Proof) Given any data \(S\), the critical points of \(R_{S}()\) are either strict saddle points or global minima._

Gradient descent easily escapes saddle points (Lee et al., 2016, 2019). Fig. 5(b) shows that when the model escapes a saddle point, the parameters initially appear chaotic but align in one direction after some time, consistent with the "condensation" phenomenon in neural networks (Luo et al., 2021; Zhou et al., 2022). For matrix factorization models, by meticulously analyzing the Hessian matrix structure (see Appendix A.5), we find that this alignment corresponds to an \(_{1}\) invariant manifold,

Figure 5: (a) Illustrated trajectories for the experiment in Fig. 4. The blue line represents the trajectory converging to the lowest-rank solution, and the red line represents the actual trajectory experienced by the model. (b) The parameter trajectory escaping from a second-order stationary point to reach the next critical point for the experiment in Fig. 3. The 8 scatter points represent the 4 row vectors of matrix \(\) and the 4 column vectors of matrix \(\). For ease of visualization, we randomly project them onto two dimensions and plot them in polar coordinates.

resulting in a rank increase of one at a time. Under reasonable assumptions, we prove that the training trajectory follows the \(_{k}\) invariant manifold step by step.

**Assumption 1** (Unique Top Singular Value).: _Let \(=(_{c}_{c}-)_{S_{}}\) be the residual matrix at the critical point \(_{c}=(_{c},_{c})\). Assume that the largest singular value of \(\) is unique._

**Assumption 2** (Second-order Stationary Point).: _Let \(\) be an \(_{k}\) invariant manifold or sub-\(_{k}\) invariant manifold defined in Prop. 1 or 2. Assume \(_{c}\) is a second-order stationary point within \(\), i.e., \( R_{S}(_{c})=0\) and \(^{}^{2}R_{S}(_{c}) 0\) for all \(\)._

**Theorem 2** (Transition to the Next Rank-level Invariant Manifold).: _(see Appendix A Thm. A.4 for proof) Consider the dynamics \(}=- R_{S}()\). Let \((_{0},t)\) denote the value of \((t)\) when \((0)=_{0}\). Let \(\) be an \(_{k}\) or sub-\(_{k}\) invariant manifold. Let \(_{c}\) be a critical point satisfying Assump. 1 and 2. Then, for randomly selected \(_{0}\), with probability 1 with respect to \(_{0}\), the limit_

\[(_{c},t):=_{ 0}(_{c}+_{0},t+}) \]

_exists and falls into an invariant manifold \(_{k+1}\). Here \(_{1}\) is the top eigenvalue of \(-^{2}R_{S}(_{c})\)._

Proof sketch.: The main idea is to analyze the local dynamics near the critical point \(_{c}\). The nonlinear dynamics can be approximated linearly in the vicinity of \(_{c}\): \(}{t}(_{0}-_{c})\), where \(=-^{2}R_{S}(_{c})\) is the negative Hessian matrix. For exact linear approximation, the solution is: \((t)=e^{t}(_{0}-_{c})+_{c}\). Let \(_{1}>_{2}>...>_{s}\) be the eigenvalues of \(\), with corresponding eigenvectors \(_{ij}\). We can express \((t)\) as: \((t)=_{i=1}^{s}_{j=1}^{l_{i}}e^{_{i}t}(_{ 0}-_{c},_{ij})_{ij}+_{c}\). For sufficiently large \(t_{0}\), the dynamics follows a dominant eigenvalue dynamics: \((t_{0})=_{j=1}^{l_{1}}e^{_{1}t_{0}}(_{0}-_{c},_{1j})_{1j}+O(e^{_{2}t_{0}})\). Through detailed analysis of the eigenvalues and eigenvectors of the Hessian matrix (please refer to Lemma A.2-A.4 of Appendix A), we show that if the largest singular value of residual matrix \(\) at \(_{c}\) is unique and \(_{c}\) is a second-order stationary point within \(\), the first principal component \(_{j=1}^{l_{1}}e^{_{1}t_{0}}(_{0}-_{c},_{ 1j})_{1j}\) will happen to be an \(_{1}\) invariant manifold. Consequently, escaping \(_{c}\) increases the rank by 1, entering \(_{k+1}\). 

**Remark**.: _Assumption. 1 ensures that upon departing from a critical point \(_{c}\), the trajectory is constrained to escape along a single dominant eigendirection corresponding to the largest singular value. This assumption holds for randomly generated matrix with probability 1, making it a reasonable condition in most practical scenarios. In Sec A.7 of Appendix A, we provide an special example to illustrate the situation where Assump. 1 does not hold._

**Remark**.: _To ensure the escape direction falls within the \(_{k+1}\) invariant manifold, the Hessian's top eigenvectors must satisfy \(rank()=rank(^{})=rank(_{})\). The condition that \(_{c}\) is a second-order stationary point within \(\) in Assump. 2 guarantees this Hessian structure. Our Assump. 2 is more general than conditions proposed by Li et al. (2020), as it remains valid across both connected and disconnected configurations. Empirical findings (Figs. 3 and 4) indicate that this assumption consistently holds in practical scenarios._the convergence characteristics within each \(_{k}\) invariant manifold, which is an endeavor we leave for future work. Despite this, our insights into the system's dynamics, i.e., hierarchical invariant manifold traversal, allow us to assert that if a trajectory successfully navigates through the optimal on each rank-level invariant manifold \(_{k}\), a solution of minimal rank can be achieved naturally.

**Theorem 3** (Minimum Rank).: _(see Appendix A Thm. A.5 for proof) Consider the dynamics \(}=- R_{S}()\), where \((t)=((t),(t))\), and denote \(_{t}=(t)(t)\). Assume \(_{t}\) achieves an optimal within each invariant manifold \(_{k}\). For a full rank initialization \(_{0}\), if the limit \(}=_{ 0}_{}(_{0})\) exists and is a global optimum with \(}_{ij}=_{ij}\) for all \((i,j) S_{}\), then_

\[}*{argmin}_{}*{rank}( )_{ij}=_{ij},(i,j) S_{}. \]

For a disconnected matrix \(\), our theoretical results (Prop. 2) and experiments (Fig. 4) confirm the existence of sub-\(_{k}\) invariant manifolds. These manifolds attract the training trajectory, leading to sub-optimal solutions and preventing convergence to the lowest-rank solution.

However, in a specific disconnected case, such as disconnection with complete bipartite components, as illustrated in Figs. 1 and 2, the minimum nuclear norm may still serve as a characterization. Gunasekar et al. (2017) proved a special case: if the observations are commutative, then the symmetric model will learn the minimum nuclear norm solution. Intriguingly, for the example \(_{2}\) in Eq. (4), even though the observations are not commutative, the model still learns a minimum nuclear norm solution. In fact, we can prove the following result, which aligns well with practical experiments.

**Theorem 4** (Minimum Nuclear Norm Guarantee).: _(see Appendix A Thm. A.6 for proof) Consider the dynamics \(}=- R_{S}()\), where \((t)=((t),(t))\), and let \(_{t}=(t)(t)\). If the observation graph associated with the incomplete matrix \(\) is disconnected with complete bipartite components, and if for a full rank initialization \(_{0}\), the limit \(}=_{ 0}_{}(_{0})\) exists and is a global optimum with \(}_{ij}=_{ij}\) for all \((i,j) S_{}\), then_

\[}*{argmin}_{}\|\|_{*}_{ij}=_{ij},(i,j) S_{}. \]

## 7 Conclusion and future work

This study presents a comprehensive experimental and theoretical investigation of matrix factorization models. The primary objective was to develop a cohesive framework for understanding the conditions, mechanisms, and reasons behind the diverse implicit regularization effects exhibited by matrix factorization models. A key finding of this research is the pivotal role of the connectivity of observed data in shaping the implicit regularization behavior. To elucidate this phenomenon, we identified the significance of hierarchical invariant manifold traversal within the training dynamics.

Our experiments (Figs. 1, 2, 3) provide strong evidence that connected observed data leads to minimum-rank solutions, as the model learns the optimal of the \(_{k}\) invariant manifold. However, further investigation is needed to uncover the underlying mechanisms by which connectivity facilitates optimal attainment across different \(_{k}\) invariant manifolds. Additionally, the trade-offs between initialization scale and training efficiency warrant further research, as certain cases may require extremely small initialization, potentially impacting training speed (see Appendix B Sec. B.4).

Generalizing the insights gained from matrix factorization models to other architectures is also an important avenue for future work. Our preliminary experiments indicate that the learning phenomenon from low rank to high rank persists in deep multi-layer matrix factorization and the query-key factorization model in Transformer attention mechanisms (see Appendix B Figs. B9, B11). These findings suggest that the hierarchical invariant manifold traversal process uncovered in our study may have broader implications and merit further exploration.