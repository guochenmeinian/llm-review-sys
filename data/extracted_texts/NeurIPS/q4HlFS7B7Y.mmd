# Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability

Usha Bhalla*

Harvard University

usha_bhalla@g.harvard.edu

&Suraj Srinivas*

Harvard University

ssrinivas@seas.harvard.edu

&Himabindu Lakkaraju

Harvard University

hlakkaraju@hbs.edu

###### Abstract

With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by identifying features critical to model predictions; however, prior work has shown that these explanations may not be faithful, in that they incorrectly attribute high importance to features that are unimportant or non-discriminative for the underlying task. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we identify a key reason for the lack of faithfulness of feature attributions: the lack of robustness of the underlying black-box models, especially to the erasure of unimportant distractor features in the input. To address this issue, we propose _Distractor Erasure Tuning_ (DiET), a method that adapts black-box models to be robust to distractor erasure, thus providing discriminative and faithful attributions. This strategy naturally combines the ease of use of post hoc explanations with the faithfulness of inherently interpretable models. We perform extensive experiments on semi-synthetic and real-world datasets, and show that DiET produces models that (1) closely approximate the original black-box models they are intended to explain, and (2) yield explanations that match approximate ground truths available by construction. Our code is made public here.

## 1 Introduction

An important desideratum of machine learning models is for their predictions to be explainable. This allows both human domain experts as well as laypeople to better understand and trust the decisions made by models, and furthermore, is also a regulatory requirement for high-stakes settings. For example, both the European General Data Protection Regulation (GDPR)  and the US AI Bill of Rights , require organizations to provide explanations for decisions made in high-stakes settings. A common approach to producing such explanations from black-box models in a post hoc manner is via feature attribution, which aims to identify important input features influencing a model prediction. These methods typically work by locally approximating non-linear models with linear ones  under some input perturbations such as feature erasure. Intuitively, if the underlying model is more sensitiveto the erasure of feature A than feature B, these methods aim to attribute a higher "importance" to feature A than feature B. A fundamental prerequisite for a feature to be considered important by a model is that it must first be useful in predicting the label, that is, it must be discriminative for the task. If a feature does not contain information relating to the output label, then it cannot be used to predict the label, and thus feature attribution methods must not consider them important. However, recent works [4; 5] have found this not to be case - feature attribution methods often highlight non-discriminative features. This motivates a natural question: what causes feature attributions to highlight such non-discriminative features, making them unfaithful?

Answering this question has been hard because of a lack of theoretical understanding of the faithfulness of feature attributions. While these notions have been used empirically [4; 5] to assess the quality of attributions, the theoretical characterization of optimally faithful feature attributions is missing in the literature. In this work, we tackle this problem by proposing a framework for feature attribution methods emphasizing faithfulness and particularly discriminability, formalized via the _signal-distractor decomposition_ for datasets. Essentially, the signal denotes the discriminative parts of the input (relative to a given task), while the distractor denotes the unimportant parts. Feature attribution methods are then evaluated on how well they are able to recover the signal, thus also providing a well-defined notion of a "ground-truth". We theoretically identify an important criterion to recover this ground-truth, that being the robustness of the model to the erasure of the input distractors. To enable black-box models to recover such ground truth attributions, we propose _Distractor Erasure Tuning_ (DiET), a method that adapts models to be robust to the erasure of input distractors. Given that these distractor regions are not known in advance, our method works by alternating feature attribution and model adaptation. At a high level, our work still uses feature attribution methods while adapting black-box models to have faithful attributions. Thus, this strategy naturally combines the ease of use of post hoc explanation methods with the faithfulness benefits of inherently interpretable models by providing the best of both alternatives. Our contributions are the following:

1. We present a formalism for feature attribution that emphasizes discriminability and allows for a notion of well-defined ground truth, via the signal-distractor decomposition of a dataset. We show that it is necessary for models to be robust to distractor erasure for them to be able to recover this ground truth.
2. We propose _distractor erasure tuning_ (DiET), a novel method that adapts black-box models to make them robust to distractor erasure.
3. We perform experiments on semi-synthetic and real-world computer vision datasets and show that DiET outputs models that are faithful to its original input models, and that their feature attributions are interpretable and match ground-truth feature attributions.

Figure 1: Illustration of our method, Distractor Erasure Tuning. DiET models exhibit robustness to distractor erasure (non-discriminative features such as backgrounds), allowing for the recovery of discriminative attributions.

Related Work

Post-Hoc Explainability.Post-hoc explainability methods aim to explain the outputs of fully trained black-box models either on an instance-level or global level. The most common post-hoc methods are feature attribution methods that rank the relative importance of features, either by explicitly producing perturbations [6; 7], or by computing variations of input gradients [8; 9; 10]. Perturbation-based methods are especially popular in computer vision literature [11; 12; 13; 14], which use feature removal strategies adapted specifically for image data. However, these methods all assume a specific form for feature removal, and we show theoretically in Section 3 that this can lead to unverifiable attributions.

Inherently Interpretable Models.Inherently interpretable models are constructed such that we know exactly what they do, either through their weights or explicit modular reasoning. As such, the explanations provided by these models are more accurate than those given by post-hoc methods; however, the performance of interpretable models often suffers when compared to unconstrained black-box architectures. The most common inherently interpretable model classes include linear models, decision trees and rules with limited depth, GLMs, GAMs , JAMs [16; 17; 18], prototype- and concept-based models [19; 20], and weight-input aligned models . While [19; 20] leverage the expressivity of deep networks, they constrain hypothesis classes significantly and still often suffer from a decrease in performance. Among these, our work most closely relates to JAMs, which amortise feature attribution generation using a learnt masking function to generate attributions in a single forward pass, and trains black-box models using input dropout. On other hand, JAMs (1) trains models from scratch, whereas DiET can interpret black-box models, (2) amortises feature attributions using a masking function resulting in less accurate attributions, (3) trains models to be robust to a large set of candidate masks via input dropout, leading to low predictive accuracy, whereas DiET trains models only to be robust to the optimal mask, leading to more flexibility and higher predictive accuracy.

Evaluating Correctness of Explanations.As explainability methods grow in number, so does the need for rigorous evaluation of each method. Research has shown that humans naively trust explanations regardless of their "correctness" , especially when explanations confirm biases or look visually appealing. Common approaches to evaluate explanation correctness rely on feature / pixel perturbation [23; 9; 24], i.e., an explanation is correct if perturbing unimportant feature results in no change of model outputs, whereas perturbing important features results in large model output change. Hooker et al.  proposed remove-and-retrain (ROAR) for evaluating feature attribution methods by training surrogate models on subsets of features denoted un/important by an attribution method and found that most gradient-based methods are no better than random. While prior works focused on developing metrics to evaluate correctness of explanations, our method DiET produces models that have explanations that are accurate by design, according to pixel-perturbation methods.

## 3 Theory of Discriminative Feature Attributions

In this section, we provide a theoretical framework for feature attribution, including a well-defined ground truth. We start by identifying a common feature of feature attributions, their reliance on perturbations or erasure. Intuitively, feature attribution methods work by simulating removing certain features and estimating how the model behaves when those features are removed: removing unimportant features should not change model behaviour. Typically, this erasure is implemented by replacing features with scalar values, such as the mean of the dataset [11; 12]. However, this can result in out-of-distribution inputs that can confuse a classifier, thus making it difficult to create meaningful attributions.

To ground this argument in an example, consider a model that classifies cows and camels. For an image of a camel, a feature attribution might note that only the hump of the camel and the sand it stands on are important for classification. As such, we would expect that the sky was irrelevant to the classifier's prediction, and we can concretely test this by altering it and creating a counterfactual sample. For example, we could mask the sky with an arbitrary uniform color; however, this may result in the sample being out-of-distribution for the model, and its prediction may change drastically even if the sky was not important for prediction. There are two strategies to overcome this problem. The first solution involves masking the sky in a manner that preserves the naturalness of the image,but this solution involves using large-scale generative models, which themselves can contain biases and be uninterpretable. The second solution requires the classifier to be invariant to the erasure of the pixels corresponding to the sky, which is our solution in this paper. We formalize this argument below by defining an erasure-based feature attribution method called \((,)\)-feature attribution.

Notation.Throughout this paper, we shall assume the task of classification with inputs \(\) with \(^{d}\) and \(y[1,2,...C]\) with \(C\)-classes. We consider the class of deep neural networks \(f:^{d}^{C}\) which map inputs \(\) onto a \(C\)-class probability simplex. This paper considers binary feature attributions, which are represented as binary masks \(\{0,1\}^{d}\), where \(_{i}=1\) indicates an important feature and \(_{i}=0\) indicates an unimportant feature.

### Feature Attributions with Input Erasure

We first define an erasure-based feature attribution such that the feature replacement method is explicit, and features are replaced with samples from a counterfactual distribution \(\). Particularly, we are interested in binary attributions (i.e., a feature is considered important or not) instead of real-valued ones.

**Definition 1**.: _(\(,\))-feature attribution (QFA) is a binary mask \((f,,)\) that relies on a model \(f()\), an instance \(\), and a \(d\)-dimensional counterfactual distribution \(\), and is given by_

\[(f,,)=_{^{}}\|^{}\|_{0}}{}\|f(_{s}(^{},q))-f( )\|_{1}\]

_where \(_{s}(,q)=+(1-) q\)_

Thus, an (\(\), \(\))-feature attribution (henceforth, _QFA_) refers to the sparsest mask that can be applied to an image such that the model's output remains approximately unchanged. QFA depends on the feature replacement distribution \(\), where \(\) is independent of both \(\) and \(y\). This generalizes the commonly used heuristics of replacing unimportant features with the dataset mean, in which case \(\) is a Dirac delta distribution at the mean value. The choice of \(\) is indeed critical, as an incorrect choice can hurt our ability to recover the correct attributions due to the resulting inputs being out-of-distribution and the classifier being sensitive to such changes. Specifically, an incorrect \(\) can result in QFA being less sparse, as masking even a few features with the wrong \(\) would likely cause large deviations in the model's outputs. As a result, given a model, we must aim to find the \(\) that leads to the sparsest QFA masks. However, the problem of searching over \(\) is complex, as it requires searching over the space of all \(d\)-dimensional distributions, and furthermore, if the underlying model is non-robust, there may not exist any \(\) that leads to sparse attributions. To avoid this, we consider the inverse problem: given \(\), we find the class of models that have the sparsest QFAs w.r.t. that particular \(\). We call this the \(\)-robust model class, which we define below:

**Definition 2**.: \(\)_-robust model class \(_{v}()\): For some given distribution \(\), the class of models \(_{v}\) for which \(\) has the sparsest QFA mask as opposed to any other \(^{}\), such that for all \(f_{v}()\),_

\[=_{^{}}}{} \|(f,,^{})\|_{0}\]

Intuitively, \(\)-robust models result in the sparsest QFA masks and can be thought of as being robust to the erasure of "irrelevant" input features. Recalling our example of the cows and camels, we would like models to be robust to the replacement of the pixels corresponding to the sky but not necessarily robust to pixels corresponding to the camel or the cow itself. This distinguishes it from classical robustness definitions, which require models to be robust to small perturbations (rather than erasure) at every feature uniformly. Thus \(\)-robustness is equivalent to enforcing robustness to the erasure of distractor features, a notion that is central to this work. For the rest of this paper, we shall refer to QFA applied to a model from a \(\)-robust model class as a "matched" feature attribution - the same \(\) is used to both define the model class and the feature attribution.

### Recovering the Signal-Distractor Decomposition

In the study of feature attribution, the 'ground truth' attributions are often unspecified. Here, we show that for datasets that are signal-distractor decomposable, formally defined below, there exists a ground truth attribution, and feature attributions for optimal verifiable models are able to recover it.

Intuitively, given an object classification dataset between cows and camels, the "signal" refers to the regions in the image that are discriminative, or correlated with the label, such as the cows or camels. The distractor refers to everything else, such as the background or sky. Note that if objects in the background are spuriously correlated with the label, i.e. sand or grass, those would be part of the signal, not the distractor. We first begin by formally defining the signal-distractor decomposition.

**Definition 3**.: _A labelled dataset \(D=\{(,y)_{i=1}^{N}\}\) has a signal-distractor decomposition defined by masks \(()\{0,1\}^{d}\) for every input \(\), where_

1. \(()\) _is the discriminative signal, where_ \(p(y)=p(y())\)__
2. \((1-())\) _is the non-discriminative distractor, where_ \(p(y(1-()))=p(y)\)__
3. \(()\) _is the sparsest mask, i.e.,_ \(()=*{arg\,min}_{^{}()}\|^{}()\|_{0}\)_, such that (1) and (2) are satisfied._

We propose to use the masks \(()\) implied by the signal-distractor decomposition as ground truth feature attributions. These are meaningful as they precisely highlight the discriminative components of the image and ignore the non-discriminative regions. Discriminability has previously been considered an important criterion to evaluate feature attributions  however, we here take a step further and propose its usage as ground truth.

We observe first that the masks \(()\) of the signal-distractor decomposition always exist: setting \(()\) as an all-ones vector trivially satisfies conditions (1) and (2). When multiple masks exist, condition (3) requires us to choose the sparsest such mask \(()\). Using the definitions provided, we show below an asymptotic argument stating that \(Q\)-robustness is a necessary condition to recover the optimal masks defined by the signal-distractor decomposition.

**Remark**: A dataset \(\) is said to have a "non-redundant signal" when the sparsest mask in condition (3) of the signal-distractor decomposition is equal to the sparsest mask when (1) alone is satisfied.

**Theorem 1**.: _QFA applied to \(\)-robust models recover the ground-truth masks when applied to the Bayes optimal predictor \(f_{v}^{*}_{v}()\), for datasets \(\) with a non-redundant signal._

Proof Idea.: We first note that the optimal \(\) for QFA is equal to the ground truth distractor distribution, as this leads to the sparsest QFA. If a \(\)-robust model aims to recover the sparsest masks, then its QFA mask must equal that obtained by setting \(\) equal to the distractor. From the uniqueness argument in the definition of the signal-distractor decomposition, this is possible only when the optimal mask is recovered by QFA. 

**Corollary**.: _QFA fails to recover the ground-truth masks when applied to predictors \(f_{v}()\)._

This follows from the fact that for any \(f_{v}()\), there exists some other \(^{}\) that results in a sparser mask, indicating that the ground truth masks are not recovered. Thus, this shows that feature attributions applied to the incorrect model class can be less effective - in this case, they fail to recover the ground truth masks. Further, the Bayes optimality is an important condition because it ensures that the resulting model is sensitive to all discriminative features in the input - sub-optimal models are sub-optimal precisely because they fail to capture the signal from all the discriminative components of the input, and this can interfere with such models being able to recover ground truth masks. In practice, if we expect our models to be highly performant, we can expect them to be sensitive to all discriminative parts of the input and thus approximately recover ground truth masks. Finally, in practice, we do not have access to the ground truth masks for natural datasets, as the discriminative and the non-discriminative regions are not known in advance. In order to use these notions of ground truth, it is thus vital to construct semi-synthetic datasets where the discriminative parts are known. Thus, one can use semi-synthetic datasets to validate a feature attribution approach and then apply it to gain insight into real datasets with unknown signal and distractor components.

To summarize, we have defined a feature attribution method with the feature removal process made explicit via the counterfactual distribution \(\). To minimize the sparsity of the attribution masks with a given \(\), we use models from the \(\)-robust model class \(_{v}()\). Finally, we find that feature attributions derived from Bayes optimal models in the model class \(_{v}()\) are able to recover the ground-truth masks and fail to do so otherwise.

DiET: Distractor Erasure Tuning

In the previous section we showed that given a \(\)-robust model \(f_{v}_{v}()\), we are able to apply QFA to recover the ground truth masks. In this section, we shall discuss how to practically build such robust models, given a pre-defined counterfactual distribution \(\) that defines the erasure method.

**Relaxing QFA.** We note that QFA as defined in definition 1 is difficult to optimize in its current form due to its use of \(_{0}\) regularization and its constrained form. To alleviate this problem, we perform two relaxations: first, we relax the \(_{0}\) objective into an \(_{1}\) objective, and second, we convert the constrained objective to an unconstrained one by using a Lagrangian. The resulting objective function is given in equation 1. Assuming the model \(f_{v}\) is known to us, we can minimize this objective function to obtain (\(\), \(\))-feature attributions for each point \(\).

\[_{}(,\{()\}_{ })=*{}_{}[ ()\|_{1}}_{}+ _{1}(;)-f_{v}(_{s}(,q));)\|_{1}}_{}]\] (1)

**Enforcing Model Robustness via Distillation.** Assuming that the optimal masks denoting the signal-distractor decomposition are known w.r.t. every training data point (i.e., \(\{()\}_{}\)), one can project any black-box model into a \(\)-robust model via distillation. Specifically, we can use equation 2 for this purpose, which contains (1) a data distillation term to enforce the \(\) constraint in QFA, and (2) a model distillation term to enforce that the resulting model and original model are approximately equal. Accordingly, the black-box model \(f_{b}\) and our resulting model \(f_{v}\) both have the same model architecture, and we initialize \(f_{v}=f_{b}\).

\[_{}(,\{()\}_{ })=*{}_{}[(;)-f_{v}(_{s}( (),q));)\|_{1}}_{}+_{2} ()-f_{v}(;)\|_{1}}_{}]\] (2)

**Alternating Minimization between \(\) and \(\).** We are interested in both of the above objectives: we would like to recover the optimal masks from the dataset, as well as use those masks to enforce (\(\), \(\)) constraints via distillation to yield our \(\)-robust models. We can thus formulate the overall optimization problem as the sum of these terms, as shown in equation 3. Notice that both these objectives assume that either the optimal masks, or the robust model is known, and in practice, we know neither. A common strategy in cases that involve optimizing over multiple sets of variables is to employ alternating minimization , which involves repeatedly fixing one of the variables and optimizing the other. We handle the constrained objective on the mask variables via projection, i.e., using hard-thresholding / rounding to yield binary masks.

\[^{*},\{^{*}()\} =_{,}(_{}( ,\{()\})+_{}(,\{()\}))\] (3) \[()\{0,1\}^{d} \]

**Iterative Mask Rounding with Increasing Sparsity.** In practice, mask rounding makes gradient-based optimization unstable due to sudden jumps in the variables induced by rounding. This problem commonly arises when dealing with sparsity constraints. To alleviate this problem, use a heuristic that is common in the model pruning literature  called iterative pruning, which involves introducing a rounding schedule, where the sparsity of the mask is gradually increased during optimization steps. Inspired by this choice, we employ a similar strategy over our mask variables.

**Practical Details.** We implement these objectives as follows. First, we initialize the robust model to be the same as the original model, \(f_{v}=f_{b}\), and the mask to be all ones, \(D_{s}=m D_{d},m=1\). We then iteratively (1) simplify \(D_{s}\) by optimizing \(_{QFA}\) until \(\) converges, (2) round \(\) such that it is binary (i.e. \(D_{s}\) is a subset of features in \(D_{d}\) rather than a weighting of them), and (3) update \(f_{v}\) by minimizing \(_{train}\) such that \(D_{s}\) is equally as informative as \(D_{d}\) to \(f_{v}\) and \(f_{v}\) is functionally equivalent to \(f_{b}\). As per Definition 2, we replace masked pixels in \(D_{s}\) with a pre-determined counterfactual distribution \(\). This ensures that the given \(\) is the optimal counterfactual distribution for \(f_{v}\), meaning \(f_{v}\) comes from the \(\)-robust model class \(_{v}()\). The pseudocode is given in Algorithm 1.

``` Input: Dataset \(D_{d}:=(x,y)\), model \(f_{b}\), hyperparameter \(k\) rounding steps Hyperparameters:\(k\) rounding steps, \(u\) mask scaling factor, \(s(t)\) sparsity at step \(t\)\(\{()\},\;\;\;\;()^{d/u}\)\(\)Init mask \(()\) to ones \(f_{v} f_{b}\)\(\)Init robust model \(f_{v}\) to \(f_{b}\) for\(k\) rounding steps do while\(_{}\) not converged do \(\{()\}\{()\}+_{ }_{}\) endwhile m\(\)round(\(\), \(s(t)\)) \(\;\{()\}\) while\(_{}\) not converged do \(f_{v} f_{v}+_{}_{}\) endwhile endfor return\(\{()\},f_{v}\) ```

**Algorithm 1** Distractor Erasure Tuning

**Mask Scale.** In order to encourage greater "human interpretability," we increase the pixel size of the masks by lowering their resolution. We do this by downscaling the masks before optimization. Concretely, we initialize the masks to be of size \(m_{d}=x_{d}/u\), where \(x_{d}\) is the dimension of the image \(\) and \(u\) is the pixel size we wish to consider. We then upsample the mask to be of dimension \(x_{d}\) before applying it to \(\). The more we downscale the mask by (i.e. the greater \(u\) is), the more interpretable and visually cohesive the distilled dataset \(_{s}\) is.

## 5 Experimental Evaluation

In this section, we present our empirical evaluation in detail. We consider various quantitative and qualitative metrics to evaluate the correctness of feature attributions given by DiET models as well as the faithfulness of DiET models to the models they are meant to explain. We also evaluate DiET models' ability to explain models manipulated to have arbitrary uninformative input gradients. Finally, we analyze the effect of the mask downscaling hyperparameter on attributions. Comparisons to additional baselines beyond those shown in Figure 2 are given in 8.

Datasets.**Hard MNIST:** The first is a harder variant of MNIST where the digit is randomly placed on a small subpatch of a colored background. Each sample also contains small distractor digits and random noise patches. For this dataset, we consider the signal to be all pixels contained within the large actual digit, and the distractor to be all pixels in the background, noise, and smaller digits.

**Chest X-ray:** Second, we consider a semi-synthetic chest x-ray dataset for pneumonia classification . To control exactly what information the model leverages such that we can create ground truth signal-distractor decompositions, we inject a spurious correlation into this dataset. We randomly place a small, barely perceptible noise patch on each image in the "normal" class. We confirm that the model relies only on the spurious signal during classification by testing the model's drop in performance when flipping the correlation (adding the noise patches to the "pneumonia" class) and seeing that the accuracy goes from 100% to 0%. As such, for this dataset, the signal is simply the noise patch and the distractor is the rest of the xray. **CelebA:** The last dataset is a subset of CelebA  for hair color classification with classes (dark hair, blonde hair, gray hair). We correlate the dark hair class with glasses to allow for qualitative evaluation of each methods' ability to recover spurious correlations. This dataset does not have a ground truth signal distractor decomposition, as there are many unknown discriminative spurious correlations the model may rely upon.

Models.For all experiments, we use ImageNet pre-trained ResNet18s for our baseline models, \(f_{b}\). All models achieve over 96% test accuracy. We train DiET models with \(Q_{d*d}*((D_{d}),^{2}(D_{d}))\), meaning that each image is masked with a uniform color drawn from a normal distribution around the mean color of the dataset. For all evaluation, we use \(Q_{d*d}*((D_{d}),0)\)(the dirac delta of the dataset mean) to ensure that masked samples are minimally out-of-distribution for the baseline models \(f_{b}\).

### Evaluating the Correctness of Feature Attributions

Pixel Perturbation Tests.We test the faithfulness of our explanations via the pixel perturbation variant proposed in [9; 5], where we mask the \(k\) least salient pixels as determined by any given attribution method and check for degradation in model performance with the mean of the dataset. This metric evaluates whether the \(k\) masked pixels were necessary for the model to make its prediction. As mentioned in previous works, masked samples come from a different distribution than the original samples, meaning poor performance after pixel perturbation can either be a product of the model's performance on the masks or the feature attribution scores being incorrect. To disentangle the correctness of the attributions from the robustness of the model, we perform pixel perturbation tests on ground truth feature attributions, with results reported in the appendix. Note that our method returns binary masks, but this metric requires continuous valued attributions to create rankings. As such, for this experiment we use the attributions created by our method _before rounding_.

Results are shown in Figure 3. We find that the attributions produced by DiET, used in conjunction with DiET models outperform all baselines. Furthermore, DiET attributions tested on the baseline model also generally perform better than gradient-based attributions.

Intersection Over Union.We further evaluate the correctness of our attributions by measuring their Intersection Over Union (IOU) with the "ground truth" attributions. We use the signal from the ground truth signal-distractor decomposition as described in 5 for the ground truth attributions. For each image, if the ground truth attribution is comprised of \(n\) pixels, we take the intersection over union of the top \(n\) pixels returned by the explanation method and the \(n\) ground truth pixels, meaning an IOU of 1 corresponds to a perfectly aligned/correct attribution. Results are shown in 1, where our method performs the best for both datasets. We report mean and standard deviation over the dataset for each method.

### Evaluating the Faithfulness of DiET Models

To ensure that the DiET model (\(f_{v}\)) returned by our method is faithful the the original model (\(f_{b}\)) it approximates, we test the accuracy of DiET models with respect to the predictions produced by the original model. Specifically, we take the predictions of the original model \(f_{b}\) to be the labels of the dataset. We evaluate DiET models on both the original dataset (\(f_{v}(D_{d}) f_{b}(D_{d})\)) and the simplified dataset (\(f_{v}(D_{s}) f_{b}(D_{d})\)), with results shown in 2. We see that DiETmodels are highly faithful to the baseline models they approximate.

Figure 2: Visualization of datasets and attribution methods considered in this work. _Row 1_: Raw data samples, _Row 2_: Ground truth attributions, _Row 3_: DiET attributions, _Rows 4-7_: Baseline methods.

### Qualitative Analysis of Simplified Datasets

We first explore how well DiET recovers signal-distractor decompositions. For CelebA, we see that all methods except for input gradients correctly recover the spurious correlation for the "dark hair/glasses" class, however only our method provides useful insights into the other two classes. We see that our method correctly identifies hair as the signal for the "blonde hair" class, whereas other methods simply look at the eyes, which are not discriminative. Furthermore, we see that for the "gray hair" class, our method picks up on hair, as well as initially unknown spurious correlations such as wrinkles and bowties. For Hard MNIST, we see that our method clearly isolates the signal and ignores the distractor. FullGrad and GradCAM suffer from a locality bias and tend to highlight the center of each digit. SmoothGrad and vanilla gradients are much noisier and highlight edges and many random background pixels. For the Chest X-ray dataset, we see that our method and FullGrad perfectly highlight the spurious signal. GradCAM again suffers from a centrality bias, and cannot highlight pixels on the edge. SmoothGrad and gradients appear mostly random to the human eye.

We also consider the visual quality of our attributions compared with the baselines (examples are shown in 2). We find that our method, FullGrad, and GradCAM appear the most interpretable, as opposed to SmoothGrad and vanilla gradients, because they consider features at the superpixel level rather than individual pixels. We also see that GradCam and FullGrad seem relatively class invariant, and tend to focus on the center of most images, rather than the discriminative features for each class, providing for less useful insights into the models and datasets.

### Robustness to Adversarial Manipulation of Explanations

In this section, we highlight our method's robustness to adversarial explanation manipulation. To this end, we follow the manipulation proposed in , which adversarially manipulates gradient-based explanations. This is achieved by adding an extra term to the training objective that encourages input gradients to equal an arbitrary uninformative mask of pixels in the top left corner of each image. Note that model accuracy on the classification task is the same as training with only cross-entropy loss.

We repeat experiments for all evaluation metrics on these manipulated models, with pixel-perturbation shown below 6, and IOU, model faithfulness, and model robustness in the appendix. We see that gradient-based methods perform significantly worse on manipulated models; however, our method

   & Hard MNIST & Chest X-ray \\  DiET (Ours) & **0.461**\( 0.08\) & **0.821**\( 0.05\) \\ SmoothGrad & 0.252\( 0.05\) & 0.045 \( 0.05\) \\ GradCAM & 0.295 \( 0.09\) & 0.000 \( 0.00\) \\ Input Grad & 0.117 \( 0.05\) & 0.017\( 0.03\) \\ FullGrad & 0.389 \( 0.10\) & 0.528 \( 0.12\) \\   
   & Hard MNIST & Chest X-ray \\  Faithfulness on & 0.996 & 1.00 & 0.995 \\ Original Data & & & \\ Faithfulness on & 0.987 & 1.00 & 0.975 \\ Simplified Data & & & \\  

Table 1: Intersection Over Union Results

Figure 3: Pixel perturbation tests (higher is better) for MNIST (left), Chest X-ray (middle), and CelebA (right) datasets. DiETâ€™s recommended mask sparsity is shown as a vertical dashed line. We observe that DiET performs the best overall. Refer to Section 5.1 for details.

remains relatively invariant. We also note while the models are only manipulated to have arbitrary input gradients, SmoothGrad and GradCAM are also heavily affected such that their attributions are entirely uninterpretable as well, as shown below.

### Attribution Sensitivity to Hyperparamaters

We conduct an ablation study on the choice of how much to downscale the mask by. The less we downscale by, the more fine-grained the mask is, allowing for optimization over a larger set of masks. However, the more we downscale by, the visually cohesive or "interpretable" to humans the masks are. We evaluate the trade-off between these two via pixel perturbation tests over multiple downscaling factors and with qualitative evaluations of the final masks in 5. We see that a downscaling factor of 8 performs the best on pixel perturbation tests. Increased factors of downscaling impose a greater locality constraint that results in informative pixels being masked, as shown in the visualization.

## 6 Discussion

In this paper, we seek to build machine learning models such that their feature attributions remain discriminative. In particular, we propose DiET, a method that adapts black-box models into those that are robust to distractor erasure. We empirically evaluate DiET and find that the resulting models are highly faithful to the original and produce interpretable attributions that closely match the ground truth ones. Limitations of DiET include requiring full access to the training dataset and the baseline model. Furthermore, while it produces verifiable feature attributions that tell us how important each feature is to the model's prediction, it does not tell us what the relationship between important features and the output/label is, as is true with all feature attributions.