# Persistent Test-time Adaptation in

Recurring Testing Scenarios

 Trung-Hieu Hoang\({}^{1}\)  Duc Minh Vo\({}^{2}\)  Minh N. Do\({}^{1,3}\)

\({}^{1}\)Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign

\({}^{2}\)The University of Tokyo

\({}^{3}\)VinUni-Illinois Smart Health Center, VinUniversity

{ththieu, minhdo}@illinois.edu  vmduc@nlab.ci.i.u-tokyo.ac.jp

###### Abstract

Current test-time adaptation (TTA) approaches aim to adapt a machine learning model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - **recurring TTA** where environments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet representative \(\)**-perturbed Gaussian Mixture Model Classifier**, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose **persistent TTA** (**PeTTA**), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://ththieu166.github.io/petta.

## 1 Introduction

Machine learning (ML) models have demonstrated significant achievements in various areas [18; 38; 47; 23]. Still, they are inherently susceptible to distribution-shift [46; 13; 48; 21; 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models' adaptability when confronted with a new testing environment that has been investigated [30; 53; 14]. Among common domain generalization methods [58; 24; 1], _test-time adaptation (TTA)_ takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57; 39; 8; 41; 59].

Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57; 39; 41]. As a result, such an assumption is far from the ever-changing and complex testing environments. To confront continually changing environments [59; 12], Yuan _et al._ proposed a _practical TTA_ scenario where distribution changing and correlative sampling occur  simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accommodate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate arecurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C  multiple times. We showcase the testing error of RoTTA  after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: _TTA approaches in this setting may experience severe and persistent degradation in performance_. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments.

This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA , namely _recurring TTA_, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the \(-\)_perturbed Gaussian Mixture Model Classifier_ (\(-\)_GMMC_) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61; 12; 59; 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely _Persistent TTA_ (_PeTTA_). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: _adaptation_ and _collapse prevention_. Our contributions can be summarized as follows:

* recurring TTA_, a simple yet sufficient setup for diagnosing the overlooked _gradual performance degradation_ phenomenon of TTA.
* Second, we formally _define the phenomenon of TTA collapsing and undertake a theoretical analysis_ on an \(\)-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes.
* a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance.

For more context on related work, readers are directed to visit our discussions in Appdx. A.

## 2 Background

**Test-time Adaptation (TTA).** A TTA algorithm operates on an ML classifier \(f_{t}:\) with parameter \(_{t}\) (parameter space) gradually changing over time (\(t\)) that maps an input image \(\) to a category (label) \(y\). Let the capital letters \((X_{t},Y_{t})\) denote a pair of _random variables_ with the joint distribution \(P_{t}(,y)_{d},t\). Here, \(_{d}\) belongs to collection of \(D\) sets of testing scenarios (domains) \(\{_{d}\}_{d=1}^{D}\). The covariate shift  is assumed: \(P_{t}()\) and \(P_{t^{}}()\)

Figure 1: _Recurring Test-time Adaption (TTA)._ (left) Testing environments may change recurringly and preserving adaptability when visiting _the same_ testing condition is not guaranteed. (right) The testing error of RoTTA  progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C  20 times. The **bold** lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity).

could be different but \(P_{t}(y|)=P_{t^{}}(y|)\) holds \( t t^{}\). At \(t=0\), \(_{0}\) is initialized by a supervised model trained on \(P_{0}_{0}\) (source dataset). The model then explores an online stream of testing data. For each \(t>0\), it receives \(X_{t}\) (typically in form of a batch of \(N_{t}\) testing samples) for adapting itself \(f_{t-1} f_{t}\) before making the final prediction \(f_{t}(X_{t})\).

**TTA with Mean Teacher Update.** To achieve a stable optimization process, the main (_teacher_) model \(f_{t}\) are updated indirectly through a _student_ model with parameters \(_{t}^{}\)[57; 61; 12; 15; 55]. At first, the teacher model in the previous step introduces a _pseudo label_\(Y_{t}\) for each \(X_{t}\):

\[_{t}=f_{t-1}(X_{t}).\] (1)

With a classification loss \(_{}\) (e.g., cross-entropy ), and a model parameters regularizer \(\), the student model is first updated with a generic optimization operator Optim, followed by an exponential moving average (EMA) update of the teacher model parameter \(_{t-1}\):

\[_{t}^{} =}_{^{}}_{P_ {t}}[_{}(_{t},X_{t};^{} )]+(^{}),\] (2) \[_{t} =(1-)_{t-1}+_{t}^{},\] (3)

with \((0,1)\) - the update rate of EMA, and \(^{+}\) - the weighting coefficient of the regularization term, are the two hyper-parameters.

**Practical TTA.** In practical TTA , two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, \(P_{t}\)'s can be partitioned by \(t_{d}\)'s in which \(\{P_{t}\}_{t=t_{d-1}}^{t_{d}}_{d}\). Here, each partition of consecutive steps follows the same underlying distribution which will _change continually through \(D\) domains_ (\(_{1}_{2}_{D}\)). Secondly, the category distribution in each testing batch is _temporally correlated_. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution \(P_{t}(y)=0, y_{t}\) even though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (\(|_{t}|||\)) situation can slowly degenerate the model .

## 3 Recurring TTA and Theoretical Analysis

This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4).

### Recurring TTA and Model Collapse

**Recurring TTA.** To study the gradual performance degradation (or model collapse), we propose a _new testing scenario based on practical TTA_. Conducting a single pass through \(D\) distributions, as done in earlier studies [61; 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs _revisiting the previous distributions \(K\) times_ to compare the incremental error versus the previous visits. For example, a sequence with \(K=2\) could be \(_{1}_{2}_{D}_{1}_{2} _{D}\). Appdx. D extends our justifications on constructing recurring TTA.

**Definition 1** (**Model Collapse)**.: _A model is said to be collapsed from step \(,<\) if there exists a non-empty subset of categories \(}\) such that \(\{Y_{t}}\}>0\) but the marginal \(\{_{t}}\}\) converges to zero in probability:_

\[_{t}\{_{t}}\}=0.\]

Here, upon collapsing, a model tends to _ignore_ almost categories in \(}\). As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to \(_{0}\).

### Simulation of Failure and Theoretical Analysis

Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on \(\)-perturbed binary Gaussian Mixture Model Classifier (\(\)-GMMC) that shares the typical characteristics _by construction_ and demonstrates the _same collapsing pattern_ in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3).

**Simulated Testing Stream.** Observing a testing stream with \((X_{t},Y_{t})=\{0,1\}\) and the underlying joint distribution \(P_{t}(x,y)=p_{y,t}(x;_{y},_{y}^{2})\). The main task is predicting \(X_{t}\) was sampled from cluster 0 or 1 (negative or positive). Conveniently, let \(p_{y,t}}{{=}}P_{t}(y)=(Y_{t}=y)\) and \(_{y,t}}{{=}}(_{t}=y)\) be the marginal distribution of the true label \(Y_{t}\) and pseudo label \(_{t}\).

**GMMC and TTA.** GMMC first implies an _equal prior_ distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in ). Thus, it simplifies \(f_{t}\) into a maximum likelihood estimation \(f_{t}(x)=*{argmax}_{y}(x|y;_{t})\) with \((x|y;_{t})=(x;_{y,t},_{y,t}^{2})\). The goal is estimating a set of parameters \(_{t}=\{_{y,t},_{y,t}^{2}\}_{y}\). A perfect classifier \(_{0}=\{_{y},_{y}^{2}\}_{y}\) is initialized at \(t=0\). For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding \(_{t}^{}\), Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating \(_{P_{t}}\). The mean teacher update (Eq. 3) for GMMC is:

\[_{y,t}=(1-)_{y,t-1}+_{P_{ t}}[X_{t}|_{t}]&_{t}=y\\ _{y,t-1}&.\] (4)

The update of \(_{y,t}^{2}\) is similar. \(_{t}=f_{t-1}(X_{t})\) can be interpreted as a _pseudo label_ (Eq. 1).

\(\)**-Gmmc.** Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA _both result in an increase in the error rate of the predictor_. Instead of directly modeling the dynamic changes of \(p_{y,t}\) (which can be complicated depending on the dataset), we study an \(-\)pertubed GMMC (\(-\)GMMC), where \(p_{y,t}\) is _assumed to be static_ (defined below) and the pseudo-label predictor of this model is _perturbed_ to _simulate undesirable effects of the testing stream_ on the predictor. Two kinds of errors appear in a binary classifier . Let

\[_{t}=\{Y_{t}=1|_{t}=0\}\] (5)

be the false negative rate (FNR) of the model at step \(t\). Without loss of generality, we study the _increasing type II collapse of \(\)-Gmmc_. By intentionally flipping the true positive pseudo labels in simulation, an FNR of \(_{t}\) is maintained (Fig. 2).

**Assumption 1** (**Static Data Stream**).: _The marginal distribution of the true label follows the same Bernoulli distribution \((p_{0})\): \(p_{0,t}=p_{0}\), (\(p_{1,t}=p_{1}=1-p_{0}), t\)._

**Lemma 1** (**Increasing FNR**).: _Under Assumption 1, a binary \(\)-Gmmc would collapsed (Def. 1) with \(_{t}_{1,t}=0\) (or \(_{t}_{0,t}=1\), equivalently) if and only if \(_{t}_{t}=p_{1}\)._

Lemma 1 states the negative correlation between \(_{1,t}\) and \(_{t}\). Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of \(p_{1}\).

**Lemma 2** (\(\)**-Gmmc After Collapsing**).: _For a binary \(\)-Gmmc model, with Assumption 1, if \(_{t}_{1,t}=0\) (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters:_

\[(_{0,t},_{0,t}^{2})}{{}}(p_{0}_{0}+p_{1}_{1},p_{0}_{0}^{2}+p_{1} _{1}^{2}+p_{0}p_{1}(_{0}-_{1})^{2}).\]

Lemma 2 states the resulting \(-\)Gmmc after collapsing. Cluster 0 now _covers the whole data distribution_ (and assigning label 0 for all samples). Furthermore, _collapsing happens when \(_{0,t}\) moves toward \(_{1}\)_. We next investigate the factors and conditions for this undesirable convergence.

Figure 2: \(\)-perturbed binary Gaussian Mixture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher update (Eqs. 2, 3). The _predictor is perturbed_ for retaining a false negative rate of \(_{t}\) to simulate an undesirable TTA testing stream.

**Theorem 1** (**Convergence of \(-\)Gmmc**).: _For a binary \(\)-GMMC model, with Assumption 1, let the distance from \(_{0,t}\) toward \(_{1}\) is \(d_{t}^{0 1}=|_{P_{t}}[_{0,t}]-_{1}|\), then:_

\[d_{t}^{0 1}-d_{t-1}^{0 1} p_{0}(|_{0}-_{1}|- ^{0 1}}{1-_{t}}).\]

From Thm. 1, we observe that the distance \(d_{t}^{0 1}\)'s converges (also indicating the convergence to the distribution in Lemma 2) if \(d_{t}^{0 1}<d_{t-1}^{0 1}\). The model collapse happens when this condition holds for a sufficiently long period.

**Corollary 1** (**A Condition for \(-\)Gmmc Collapse**).: _With fixed \(p_{0}\), \(,_{0},_{1}\), \(-\)GMMC is collapsed if there exists a sequence of \(\{_{t}\}_{-_{}}^{}\) (\(_{}>0\)) such that:_

\[p_{1}_{t}>1-^{0 1}}{|_{0}-_{1}|}, t[ -_{},].\]

Corollary 1 introduces a condition \(\)-GMMC collapse. Here, \(_{t}\)'s are non-decreasing, \(_{t}_{t}=p_{1}\).

**Remarks.** Thm. 1 concludes two sets of factors contributing to collapse: (i) _data-dependent factors_: the prior data distribution (\(p_{0}\)), the nature difference between two categories (\(|_{0}-_{1}|\)); and (ii) _algorithm-dependent factors_: the update rate (\(\)), the FNR at each step (\(_{t}\)). \(\)-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4).

### Connection to Existing Solutions

Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies.

**Regularization Term for \(_{t}\).** Knowing that \(f_{0}\) is always well-behaved, an attempt is restricting the divergence of \(_{t}\) from \(_{0}\), e.g. using \((_{t})}{{=}}\|_{0}- _{t}\|_{2}^{2}\) regularization . The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1.

**Memory Bank for Harmonizing \(P_{t}(x)\).** Upon receiving \(X_{t}\), samples in this batch are selectively updated to a memory bank \(\) (which already contains a subset of some instances of \(X_{t^{}},t^{}<t\) in the previous steps). By keeping a balanced number of samples from each category, distribution \(P_{t}^{}(y)\) of samples in \(\) is expected to have less zero entries than \(P_{t}(y)\), making the optimization step over \(P_{t}^{}\) more desirable. From Thm. 1, \(\) moderates the extreme value of the category distribution (\(p_{0}\) term) which typically appears on batches with low intra-batch category diversity.

## 4 Persistent Test-time Adaptation (PeTTA)

Now we introduce our _Persistent TTA (PeTTA)_ approach. Further inspecting Thm. 1, while \(_{t}\) (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to \(d_{t-1}^{0 1}\) term) can provide hints to fine-tune the adaptation process.

**Key Idea.** A proper adjustment toward the TTA algorithm can _break the chain of monotonically increasing \(_{t}\)'s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of \(\) (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, \(\) (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49; 25] encountered in reinforcement learning , we introduce a mechanism for _adjusting \(\) and \(\) on the fly, balancing between the two primary objectives: adaptation and preventing model collapse_. Our strategy is prioritizing collapse prevention (increasing \(\)) and preserving the model from previous steps (decreasing \(\)) when there is a significant deviation from \(_{0}\).

In [40; 61; 59], \(\) and \(\) were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set . Furthermore, Thm. 1 suggests the convergence rate quickly escalates when \(_{t}\) increases, making constant \(,\) insufficient to prevent collapse.

**Sensing the Divergence of \(_{t}\).** We first equip PeTTA with a mechanism for _measuring its divergence from \(_{0}\)_. Since \(f_{t}()=\,_{y}(y|;_{t})\), we can decompose \((y|;_{t})=[h(_{_{t}}())]_{y}\), with \(_{_{t}}()\) is a \(_{t}\)-parameterized deep feature extractor followed by a _fixed_ classification head (a linear and softmax layer) \(h()\). The operator \([]_{y}\) extracts the \(y^{}\) component of a vector.

Since \(h()\) remains unchanged, instead of comparing the divergence in the parameter space (\(\)) or between the output probability \((y|;_{t})\) and \((y|;_{0})\), we suggest an _inspection over the feature embedding space_ that preserves a _maximum amount of information_ in our case (data processing inequality ). Inspired by  and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let \(=_{_{t}}()\), we keep track of a collection of the running mean of feature vector \(\): \(\{}_{t}^{y}\}_{y}\) in which \(}_{t}^{y}\) is EMA updated with vector \(\) if \(f_{t}()=y\). The divergence of \(_{t}\) at step \(t\), evaluated on class \(y\) is defined as:

\[_{t}^{y}=1-(-(}_{t}^{y}-_{0}^{y})^{T}( _{0}^{y})^{-1}(}_{t}^{y}-_{0}^{y}) ),\] (6)

where \(_{0}^{y}\) and \(_{0}^{y}\) are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (\(F_{0}\)). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details).

Here, we implicitly expect the independence of each entry in \(\) and TTA approaches _learn to align feature vectors of new domains back to the source domain_ (\(P_{0}\)). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of \(_{t}^{y}\) is close to 0 when \(_{t}=_{0}\) and increases exponentially as \(}_{t}^{y}\) diverging from \(_{0}^{y}\).

**Adaptive Regularization and Model Update.** With \(_{0}\), \(_{0}\) are initial values, utilizing \(_{t}^{y}\) derived in Eq. 6, a pair of \((_{t}\), \(_{t}\)) is _adaptively_ chosen at each step:

\[_{t} =}_{t}|}_{y}_{t} }_{t}^{y},}_{t}=\{_{t}^{(i)}|i=1, ,N_{t}\};\] \[_{t} =_{t}_{0},_{t}=(1-_{t})_{0},\] (7)

\(}_{t}\) is a set of unique pseudo labels in a testing batch (\(_{t}^{(i)}\) is the \(i^{}\) realization of \(_{t}\)).

**Anchor Loss.** Penalizing the divergence with regular vector norms in high-dimensional space (\(\)) is insufficient (curse of dimensionality ), especially with a large model and limited samples. _Anchor loss_\(_{}\) can nail down the similarity between \(f_{t}\) and \(f_{0}\) in the probability space :

\[_{}(X_{t};)=-_{y}(y|X_{t}; _{0})(y|X_{t};),\] (8)

which is equivalent to minimizing the KL divergence \(D_{KL}((y|X_{t};_{0})\|(y|X_{t};))\).

**Persistent TTA.** Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from . Appdx. E.1 introduces the _pseudo code_ of PeTTA. For \(_{}\), either the self-training scheme  or the regular cross-entropy  is adopted. With \(()\), cosine similarity or L2 distance are both valid metrics for measuring the distance between \(\) and \(_{0}\) in the parameter space. Fisher regularizer coefficient  can also be used, optionally. To sum up, the teacher model update of PeTTA is an _elaborated version_ of EMA with \(_{t},_{t}\) (Eq. 7) and \(_{}\) (Eq. 8):

\[_{t}^{} =}_{^{}}_{P_ {t}}[_{}(_{t},X_{t};^{} )+_{}(X_{t};^{})]+ _{t}(^{}),\] \[_{t} =(1-_{t})_{t-1}+_{t}_{t}^{}.\]

## 5 Experimental Results

### \(-\)Mmc Simulation Result

**Simulation Setup.** A total of 6000 samples from two Gaussian distributions: \((_{0}=0,_{0}^{2}=1)\) and \((_{1}=2,_{1}^{2}=1)\) with \(p_{0}=p_{1}=\) are synthesized and gradually released in a batch of \(B=10\) samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). \(-\)GMMC update follows Eq. 4 with \(=5e^{-2}\). To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1).

**Simulation Result.** In action, both the likelihood of predicting class 0 (Fig. 2(a)-left) and the \(_{t}\) (Eq. 5) (Fig. 2(c)-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing,\(\)-GMMC merges the two initial clusters, resulting in a single one (Fig. 2(b)-left) with parameters that match Lemma 2. The distance from \(_{0,t}\) (initialized at \(_{0}\)) towards \(_{1}\) converges (Fig. 2(c)-left, solid line), coincided with the analysis in Thm. 1 when \(_{t}\) is chosen following Corollary 1 (Fig. 2(c), dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 2(a)-right) and approximates the true data distribution (Fig. 2(b)-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated.

### Setup - Benchmark Datasets

**Datasets.** We benchmark the performance on _four_ TTA classification tasks. Specifically, CIFAR10 \(\) CIFAR10-C, CIFAR100 \(\) CIFAR100-C, and ImageNet \(\) ImageNet-C  are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet  with 126 categories from four domains for the task _real_\(\)_clipart, painting, sketch_.

**Compared Methods.** Besides PeTTA, the following algorithms are investigated: CoTTA , EATA , RMT , MECTA , RoTTA , ROID  and TRIBE . Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME  and a reset-based approach (i.e., reverting the model to the source model after adapting to every \(1,000\) images): RDumb  are also included.

**Recurring TTA.** Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (\((0.1)\) for CIFAR10-C, DomainNet, and ImageNet-C, and \((0.01)\) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits \(K=20\) (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines.

**Implementation Details.** We use PyTorch for implementation. RobustBench and torchvision provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization  and feature embedding statistics is set to \(5e^{-2}\); \(_{0}=1e^{-3}\) and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in  for \(_{}\) and \(_{0}=10\) while the regular cross-entropy loss  and \(_{0}=1\) (severe domain shift requires prioritizing

Figure 3: Simulation result on \(\)-perturbed Gaussian Mixture Model Classifier (\(\)-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 2(a)-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of \(\)-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward \(_{1}\) (\(|_{P_{i}}[_{0,t}]-_{1}|\)) and false-negative rate (\(_{t}\)) in simulation coincides with the result in Thm. 1 (with \(_{t}\) following Corollary 1).

[MISSING_PAGE_FAIL:8]

**Collapsing Pattern.** The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA  in an recurring TTA setting (Fig. 5a-left). Similar to \(\)-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the

top prone-to-misclassified pair of categories. Here, label _deeer_ is used for almost every living animal while _airplane_ represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to \(|_{0}-_{1}|\) (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent.

### Ablation Study

**Effect of Each Component.** Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (\(()\)) with a fixed choice of \(,\) not only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (\(_{}\)) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptive \(_{t}\) scheme alone (row 5) or in conjunction with either \(_{t}\) or anchor loss \(_{}\) (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting \(_{t}\) adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without \(_{}\) is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8).

**Various Choices of Regularizers.** The design of PeTTA is not coupled with any specific regularization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27; 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options.

## 6 Discussions and Conclusion

**On a Potential Risk of TTA in Practice.** We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with _a single pass per test set_. The recurring TTA could be conveniently adopted as a _straightforward evaluation_, where its challenging test stream magnifies the error accumulation that a model might encounter in practice.

**Limitations.** PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61; 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios.

**Conclusion**. Towards trustworthy and reliable TTA applications, we rigorously study the _performance degradation problem of TTA_. The proposed _recurring TTA_ setting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of \(-\)_GMMC_ paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention.

   &  &  &  &  \\  Baseline w/o \(()\), \(_{}\) & 42.6 & 63.0 & 77.9 & 93.4 \\  \(()\) fixed \(=0.1_{0}\) & 43.3 & 65.0 & 80.0 & 92.5 \\ \(()\) fixed \(=_{0}\) & 42.0 & 64.6 & 66.6 & 92.9 \\  \(_{}\) only & 25.4 & 56.5 & 47.5 & 68.1 \\   PeTTA - \(_{t}\) & 27.1 & 55.0 & 59.7 & 92.7 \\ PeTTA - \(_{t}+_{t}\) & 23.9 & 41.4 & 44.5 & 75.7 \\ PeTTA - \(_{t}+_{}\) & 26.2 & 36.3 & 43.2 & 62.0 \\  PeTTA - \(_{t}+_{t}+_{}\) & **22.8** & **35.1** & **42.9** & **60.5** \\  

Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) \(()\), \(_{}\); \(_{}\) only; fixed regularization coefficient \(\); adaptive coefficient \(_{t}\), update rate \(_{t}\); using anchor loss \(_{}\).

   &  &  &  &  \\  \(()\) & Fisher & 23.0 & 35.6 & 43.1 & 70.8 \\  L2 & ✗ & 22.7 & 36.0 & 43.9 & 70.0 \\   & ✗ & 22.8 & **35.1** & **42.9** & **60.5** \\  & ✓ & **22.6** & 35.9 & 43.3 & 63.8 \\   \\  

Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of regularizers \(()\): L2 and cosine similarity in conjunction with Fisher [27; 40] coefficient.