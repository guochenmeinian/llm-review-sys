# _ChronoMagic-Bench_: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation

Shenghai Yuan1, Jinfa Huang3, Yongqi Xu1, Yaoyang Liu1, Shaofeng Zhang5,

**Yujun Shi6, Ruijie Zhu7, Xinhua Cheng1, Jiebo Luo3, Li Yuan1,2,**

###### Abstract

We propose a novel text-to-video (T2V) generation benchmark, _ChronoMagic-Bench_2, to evaluate the temporal and metamorphic knowledge skills in time-lapse video generation of the T2V models (e.g. Sora  and Lumiere ). Compared to existing benchmarks that focus on visual quality and text relevance of generated videos, _ChronoMagic-Bench_ focuses on the models' ability to generate time-lapse videos with significant metamorphic amplitude and temporal coherence. The benchmark probes T2V models for their physics, biology, and chemistry capabilities, in a free-form text control. For these purposes, _ChronoMagic-Bench_ introduces **1,649** prompts and real-world videos as references, categorized into four major types of time-lapse videos: biological, human creation, meteorological, and physical phenomena, which are further divided into 75 subcategories. This categorization ensures a comprehensive evaluation of the models' capacity to handle diverse and complex transformations. To accurately align human preference on the benchmark, we introduce two new automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic attributes and temporal coherence. MTScore measures the metamorphic amplitude, reflecting the degree of change over time, while CHScore assesses the temporal coherence, ensuring the generated videos maintain logical progression and continuity. Based on the _ChronoMagic-Bench_, we conduct comprehensive manual evaluations of eighteen representative T2V models, revealing their strengths and weaknesses across different categories of prompts, providing a thorough evaluation framework that addresses current gaps in video generation research. More encouragingly, we create a large-scale _ChronoMagic-Pro_ dataset, containing **460k** high-quality pairs of 720p time-lapse videos and detailed captions. Each caption ensures high physical content and large metamorphic amplitude, which have a far-reaching impact on the video generation community. 3

## 1 Introduction

Text-to-video (T2V) generative models  have developed rapidly recently. As the number of models continues to grow, there is an urgent need for evaluation methods that alignwith human perception, accurately reflecting the specific strengths and weaknesses of each model, thereby enabling the community to more easily select architectures that meet their requirements.

However, the current T2V benchmarks  primarily assess the capability of generating general videos instead of time-lapse videos, failing to reflect the extent of physical priors encoded by the models. Additionally, the evaluation metrics they use mainly focus on visual quality and textual relevance, from early metrics like FID , FVD , and CLIPScore  to more recent ones like UMTScore , T2VQA , and UMT-FVD , all of which overlook two other crucial aspects of videos: metamorphic amplitude and temporal coherence. These limitations hinder the development of T2V models in generating videos with rich physical content.

Due to the greater metamorphic amplitude and temporal coherence of time-lapse videos, they contain more physical priors compared to general videos . Therefore, to address the aforementioned issues, we introduce a benchmark called _ChronoMagic-Bench_ for Metamorphic Evaluation of Time-Lapse Text-to-Video Generation, which provides a comprehensive evaluation system for T2V. We specifically designed four major categories for time-lapse videos, including biological, human creation, meteorological, and physical, and extended these to 75 subcategories. Based on this, we constructed _ChronoMagic-Bench_, comprising 1,649 prompts and their corresponding reference time-lapse videos. As shown in Table 1, compared to existing benchmarks , _ChronoMagic-Bench_ emphasizes generating videos with high persistence and strong variation, i.e., metamorphic videos with high physical prior content. Additionally, we developed MTScore for evaluating metamorphic amplitude and CHScore for temporal coherence to address the deficiencies in evaluation metrics and perspectives. With _ChronoMagic-Bench_, we conducted comprehensive qualitative and quantitative evaluations of almost all open&closed-source T2V models, enabling analysis of their strengths and weaknesses. The results highlighted the weaknesses of these models, including (1) almost all models fail to generate time-lapse videos with large variations; (2) poor adherence to prompts, necessitating multiple inferences to achieve satisfactory results; (3) the visual quality of single frame may be high, but flickering may occur, indicating poor temporal coherence.

Furthermore, we have meticulously curated the dataset _ChronoMagic-Pro_ to provide the community with the first large-scale T2V dataset specifically designed for time-lapse video generation with higher physical prior content. ChronoMagic-Pro stands out from previous T2V datasets  as it comprises time-lapse videos (e.g., ice melting and flowers blooming) characterized by strong physical characteristics, high persistence, and variability. Considering the domain differences between time-lapse videos and general videos, we proposed an automatic time-lapse video collection framework to maintain video purity and improve annotation quality.

The contributions of this work are as follows:

**i) New T2V Benchmark.** We introduce _ChronoMagic-Bench_ for comprehensive evaluation of T2V models, focusing on visual quality, text relevance, metamorphic amplitude, and temporal coherence.

**ii) New Automatic Metrics.** We develop MTScore and CHScore, which align better with human judgment than existing metrics, for assessing metamorphic attributes and temporal coherence.

**iii) New Insights for T2V Model Selection.** Our evaluations using _ChronoMagic-Bench_ provide crucial insights into the strengths and weaknesses of various T2V models.

**iv) Large-Scale Time-lapse Video-Text Dataset.** We create _ChronoMagic-Pro_, a dataset with **460k** high-quality 720p time-lapse videos and detailed captions, promoting advances in T2V research.

  
**Benchmark** & **Type** & **Visual Quality** & **Text Relevance** & **Metamorphic Amplitude** & **Temporal Coherence** \\  UCF-101  & General & ✓ & ✓ & ✗ & ✗ \\ Make-a-Video-Eval  & General & ✓ & ✓ & ✗ & ✗ \\ MSRC-VTT  & General & ✓ & ✓ & ✗ & ✗ \\ FEVT  & General & ✓ & ✓ & ✗ & ✓ \\ VBench  & General & ✓ & ✓ & ✗ & ✓ \\ T2VSCee  & General & ✓ & ✓ & ✗ & ✓ \\  ChronoMagic-Bench (Ours) & Time-lapse & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **Comparison of the characteristics of our ChronoMagic-Bench with existing T2V benchmarks. Most of them only assess two dimensions: visual quality and text relevance.**

## 2 Related Work

Automatic Metrics for Text-to-Video Generation.Existing benchmarks [28; 34; 80; 37; 61] typically utilize Frechet Inception Distance (FID) , Frechet Video Distance (FVD) , CLIPScore , or their improved versions to assess the visual quality and text relevance of generated videos. For example, FETV  enhances FVD and CLIPScore within the UMT  feature space, resulting in UMT-FVD and UMTScore. Additionally, the CLIPScore feature extractor can be replaced with BLIP  to evaluate the relevance between text and generated content. To the best of our knowledge, existing T2V benchmarks [46; 47; 87; 18; 55] mainly assess these two aspects, with prompts based on general videos. This means that temporal coherence and metamorphic amplitude in videos have been overlooked, leading to the absence of automated metrics that indirectly reflect the physical content encoded by video models. Although [47; 28; 46] assess coherence, they are based on feature space or human evaluation, which is expensive and not sufficiently intuitive. Therefore, we propose the Metamorphic Score (MTScore) and Coherence Score (CHScore) to measure the metamorphic degree and temporal coherence of videos, filling this gap in the field.

Datasets for Text-to-Video Generation.Large-scale high-quality text-content pair data [10; 67; 56; 27] are essential for training generation models [93; 19; 59; 58; 50; 51; 52; 14; 54; 40; 6; 7; 24; 53; 91; 65]. To enable models to learn better representation spaces that simulate the real world, the larger the dataset and the richer the physical knowledge contained in the videos, the better the training effect. Researchers often construct these large-scale datasets through web scraping. For example, existing video generation models typically use WebVid-10M , which contains 10 million videos and captions. Recently released datasets, such as Panda-70M , HD-VG-130M , and InternVid , contain 70 million, 130 million, and 7.1 million text-video pairs, respectively. Despite their large sizes, these datasets consist of general videos with small metamorphic amplitude and short persistence of change, resulting in limited physical knowledge. Consequently, models trained on these datasets struggle to generate metamorphic videos. To address this issue, we propose the first large-scale dataset of time-lapse videos, comprising 460k 720P resolution video clips and their corresponding captions, which features strong persistence of changes, and high physical content.

## 3 ChronoMagic-Bench

### Benchmark Construction

Prompt Construction.To comprehensively evaluate the time-lapse video generation capabilities of existing T2V models, the designed text prompts need to cover as many metamorphic types as possible, and the corresponding reference videos must be of relatively high quality. Manual construction is impractical; therefore, to build a T2V benchmark rich in visual concepts, we first manually created a search term database suitable for diverse and broadly applicable time-lapse videos. We then counted the number of videos obtainable for each search term and filtered them based on frequency, resulting in a search database containing 75 categories of time-lapse videos. Additionally, since there

Figure 1: **Example of four major categories from ChronoMagic-Bench. These categories fully encompass the physical world, allowing our benchmark and dataset to empower the community.**are four major nature phenomena: _biological_ covers all content related to living organisms, such as plant growth, animal activities, microbial movement, etc. _Human creation_ includes all objects created or influenced by human activities, such as the construction process of buildings, urban traffic flow, etc. _Meteorological_ includes all content related to meteorological phenomena, such as cloud movement, storm formation, etc. _Physical_ includes all content related to non-biological physical phenomena, such as water flow, volcanic eruptions, etc. We divide the 75 subcategories into four major categories (_biological_, _human creation_, _meteorological_, and _physical_), as shown in Figure 1. We then utilized a search engine to crawl 20 or more high-quality videos from video platforms for each category, ultimately gathering a total of 1,649 videos. Finally, we use GPT-4o  to accurately caption these videos and treat these captions as the text prompts for the benchmark. For more details about benchmark construction, please refer to Appendix E.

**Benchmark Statistics.** We collect a total of 1,649 prompts with corresponding videos and categories, the specific data distribution is shown in Figure 2, indicating that 75 categories have a comparable number of test cases to reflect the time-lapse video generation capabilities of different models accurately. Each data sample in ChronoMagic-Bench consists of four elements: prompt \(p\), reference video \(v\), sub-category \(c_{1}\), and major category \(c_{2}\). Since existing T2V models typically use CLIP as the text encoder, which supports a maximum input of 77 tokens, we have limited the length of \(p\) to within 77 tokens for general applicability, as shown in Figure 3(a). Although the length is limited, the diversity remains rich. By comparing the main words in the word cloud, as shown in Figure 3(b), it is observed that terms related to time-lapse videos such as "transitioning," "progressing," "increasing," and "gradually" appear most frequently. These terms significantly highlight ChronoMagic-Bench's focus on large metamorphic amplitude, strong persistence of changes, and high physical content. In addition, words from four major categories are distributed, such as biological (seed, butterfly, etc.), human creation (Minecraft, traffic, etc), meteorological (sunset, tide, etc), and physical (burning, explosion, etc). For detailed explanations of the 75 subcategories, please refer to the Appendix E.

Figure 3: **The word cloud and word count range of the prompts in the ChronoMagic-Bench. It shows that prompts mainly describe videos with large metamorphic amplitude and long persistence.**

Figure 2: **Categories of Time-lapse Videos: Firstly, we classify the videos into four major categories (biological, human creation, meteorological, physical), which are further subdivided into 75 subcategories (e.g. animal, parking, beach, melting).**

### New Automatic Metrics

As previously mentioned, existing evaluation metrics mainly assess two aspects: visual quality and textual relevance, and the prompts only describe general videos. This indicates a lack of metrics for evaluating the capability to generate time-lapse videos, which not only need to measure the aforementioned two aspects but also need to assess metamorphic amplitude and temporal coherence.

**Metamorphic Score.** To the best of our knowledge, there is no existing automated evaluation metric for assessing metamorphic amplitude. A simple way is to use questionnaires or GPT-4o , which, although highly effective, is expensive. Another way is to use the open-source model , which, although less effective, is much cheaper. To address this, we propose both coarse-grained and fine-grained scores to measure the metamorphic amplitude, aiming to balance cost and performance.

For the coarse-grained score (i.e. MTScore), we initially designed \(N\) retrieval sentences (please refer to Appendix B.1 for more details). We then input these sentences into a video retrieval model , resulting in the computation of probabilities for \(n\) metamorphic and \(m\) general videos. Let \(P_{i}^{}\) and \(P_{i}^{}\) represent the probabilities for the \(i\)-th metamorphic and general retrieval sentences, respectively. We then integrate these probabilities to derive a coarse-grained metamorphic score \(S_{c}\):

\[S_{c}=^{n}P_{i}^{}}{_{i=1}^{n}P_{i}^{}+_{i=1}^{m}P_{i}^{}}\] (1)

Due to the strong instruction-following capability and world-understanding ability of GPT-4o, it can partially replace humans. For the fine-grained score (GPT4o-MTScore), we use GPT-4o as the evaluator. Specifically, we set a 5-point evaluation standard, then uniformly sample \(T\) frames and input them into GPT-4o to get the score. More implementation details are provided in Appendix B.

**Temporal Coherence Score.** Temporal coherence is crucial for time-lapse videos because they span a large time range. Current benchmarks assess coherence either through questionnaires  or by employing methods based on feature space calculations [28; 46]. The former approach is time-consuming, whereas the latter lacks intuitiveness and does not support visualization. Therefore, we developed the Coherence Score (CHScore) based on a video tracking model  as shown in Algorithm 1. More details are provided in the Appendix B.2. First, we process the input video using a pre-trained model with grid size \(G\) and threshold \(T\) to obtain \(p_{}[i,j]\) (the visibility of point \(j\) in frame \(i\)). Next, we count the number of missing tracking points \(m[i]\) in each frame and the change in missed points between consecutive frames \( m[i]\). To make the CHScore robust to the temporally coherent disappearance of points, we further calculate the direction of camera/object movementbased on the tracking points across all frames. If the tracking point j of frame i disappears in the far direction, it is not included in the calculation of \(m[i]\). We then calculate the average proportion of missed points per frame \(R_{}\), indicating the overall visibility issue across the video. Following this, we compute the variation in the number of missed points between consecutive frames \(V_{}\), measuring frame-to-frame coherence. We also determine the ratio of frames that need to be cut \(R_{}\), reflecting the extent of video editing required, and count the number of consecutive changes in missed points exceeding the threshold \(C_{}\), indicating frequent large-scale instability in point tracking. Additionally, we measure the maximum continuous change in missed points \(M_{}\), highlighting the most severe continuity breaks in the video. Finally, we integrate these metrics to calculate the Coherence Score (CHScore). In the subsequent section, the actual CHScore is scaled by 0.1 to provide a more concise representation. Further details can be found in the appendix B.2.

### Application Scope

ChronoMagic-Bench proposes automatic scores for measuring _metamorphic amplitude_ and _temporal coherence_. When combined with existing metrics for _visual quality_ and _textual relevance_, such as FVD , CLIPScore , UMT-FVD , and UMTScore , a comprehensive evaluation of T2V models across four dimensions can be achieved. Additionally, we can use human evaluation to more accurately assess these four dimensions.

## 4 ChronoMagic-Pro

Multi-Aspect Data Curation.As previously mentioned, existing large-scale text-video datasets primarily consist of general videos with limited physical information content, restricting open-source models [66; 66; 23] to generating only general videos rather than time-lapse videos. To address this, we construct the first large-scale time-lapse video dataset by collecting time-lapse videos based on the search terms outlined in Section 3.1, ultimately obtaining 66,226 original videos. Following the Panda70m method , we split these videos to produce 460K semantically consistent single-scene video clips. Finally, we utilize the video annotation strategy similar to MagicTime , replacing GPT-4V  with the open-source ShareGPT4Video  to reduce computational overhead while ensuring high-quality video captions. We conduct a verification experiment on the dataset in the Appendix D.3. More details about dataset construction are provided in Appendix C.

Dataset Statistics.We collected time-lapse videos from 75 categories manually set by the human, with proportions being roughly similar. Some samples can be found in the Appendix C.4. ChronoMagic-Pro is the first high-quality large-scale time-lapse T2V dataset, which contains more physical knowledge than general videos, as shown in Table 2. As shown in Figure 4, in terms of duration, more than half (53.3%) of the videos have a duration of 0-15 seconds, a quarter (27.1%) are longer than 60 seconds, 12.1% are between 15-30 seconds, and the remaining videos are distributed between 30-60 seconds. Regarding resolution, 97% are high resolution (720P), 2% are ultra-high resolution (1080P), and the remaining videos have lower resolutions ranging from 360P to 480P. As the number of words accepted by the text encoder increases, we require the generated captions to be as detailed as possible, with 95% of captions containing more than 100 words. For aesthetic score , 73% videos get high scores ranging from 4 to 6. 14% of the videos had aesthetic indicators exceeding 6, and only a small portion of the videos scored below 3. This indicates that the quality of most videos is high. For the word distribution of the generated captions, please refer to Appendix C.3. Similar to Figure 3, ChronoMagic-Pro mainly focuses on changes (gradually, progressing, increasing, etc.), processes spanning a large amount of time, such as flower blooming and ice melting.

  
**Dataset** & **\# Categories** & **Video clips** & **Resolution** & **Type** & **Average length** & **Video duration (h)** \\  MSR-VTT  & General & 10K & 240p & Video-Text & 15.0s & 40 \\ WebVid-10M  & General & 10M & 360p & Video-Text & 18.72s & 52K \\ InternVid  & General & 234M & 720p & Video-Text & 11.90s & 760.3K \\ Panda-70M  & General & 70M & 720p & Video-Text & 8.50s & 166.8K \\ HDV-G-130M  & General & 130M & 720p & Video-Text & 4.93s & 178K \\  Time-lapse-D  & Time-lapse & 2K & 360p & Video & - & - \\ Sky Time-lapse  & Time-lapse & 17K & 1080p & Video & - & - \\ ChronoMagic  & Time-lapse & 2K & 720p & Video-Text & 11.4s & 7 \\ ChronoMagic-Pro & Time-lapse & 460K & 720p & Video-Text & 234s & 30K \\   

Table 2: **Comparison of the statistics of our ChronoMagic-Pro with existing T2V datasets.**

## 5 Experiments

### Evaluation Models

We select fourteen open-source T2V models for evaluation, including both relatively advanced U-Net based models (e.g., ModelScopeT2V , ZeroScope , T2V-zero , LaVie , AnimateDiff , MagicTime , VideoCrafter2  and MCM .) and emerging DiT-based models (e.g., Latte , OpenSoraPlan v1.1 , OpenSora 1.1 , OpenSora 1.2 , EasyAnimate , CogVideoX ). We also selected four closed-source models for evaluation, specifically **U-Net based**: _Gen-2_, _Pika-1.0_, **DiT-based**: _Dream Machine_, and _KeLing_. All inference settings follow the official implementation. For more details, please refer to the Appendix D.

### Evaluation Setups

**Evaluation Criteria.** We assess video quality primarily from the following four aspects: (a) **Visual Quality** measures the clarity, color saturation, contrast, and overall aesthetic effect, using UMT-FVD , an enhanced version of FVD . (b) **Text Relevance** measures the correlation between the prompt and the video using UMTScore , an enhanced version of CLIPScore . (c) **Metamorphic Amplitude** measures the diversity and dynamic changes in the video content, using the proposed Metamorphic Score. (d) **Temporal Coherence** measures the smoothness and logical sequence of the video content over time, using the proposed Coherence Score. Additionally, we use human evaluation to cross-verify the reliability of the four metrics.

**Implementation Details.** For each baseline, we generate corresponding triple results based on the 1,649 prompts contained in the ChronoMagic-Bench, resulting in 4,947 videos for each model. We then use the four automated metrics mentioned above to assess all the generated videos.

Figure 4: **Video clips statistics in ChronoMagic-Pro. The dataset includes a diverse range of categories, clip durations and caption lengths, with most of the videos being in 720P resolution.**

  
**Method** & **Venue** & **Backbone** & **UMT-FVD\({}_{}\)** & **UMTScore\({}^{}\)** & **MTscore\({}^{}\)** & **CHScore\({}^{}\)** & **GPT4-MTScore\({}^{}\)** \\  ModelScopeT2V  & Araviv\({}^{}\)3 & U-Net & 194.77 & 2.909 & 0.401 & 61.07 & 2.86 \\ ZeroScope  & CVPR’23 & U-Net & 227.02 & 2.350 & 0.400 & **99.67** & 2.09 \\ T2V-zero  & ICCV’23 & U-Net & 209.66 & 2.661 & 0.400 & 20.78 & 2.55 \\ LaVie  & Araviv\({}^{}\)3 & U-Net & **166.97** & 2.763 & 0.346 & 77.89 & 2.46 \\ AnimateDiff 3V (32) & ICLR’24 & U-Net & 197.89 & **2.944** & 0.467 & 70.85 & 2.62 \\ VideoCrafter2  & Araviv\({}^{}\)4 & U-Net & 178.45 & 2.753 & 0.433 & 80.10 & 2.68 \\ MCM-MSLION  & Araviv\({}^{}\)4 & U-Net & 202.08 & 2.33 & 0.417 & 62.60 & 3.04 \\ MagicTime  & Araviv\({}^{}\)4 & U-Net & 257.56 & 1.916 & **0.478** & 81.82 & **3.13** \\   & Araviv\({}^{}\)4 & DT & 192.12 & 2.111 & 0.363 & 68.68 & 2.20 \\ OpenSora 1.1  & Github\({}^{}\)4 & DT & 195.43 & 2.678 & **0.444** & 73.98 & 2.52 \\ OpenSora 1.2  & Github\({}^{}\)4 & DT & 166.92 & 2.781 & 0.375 & 51.60 & 2.56 \\ OpenSoraPlan v1.1  & Github\({}^{}\)4 & DT & 188.53 & 2.421 & 0.327 & 68.52 & 2.19 \\ EasyAnimateV3  & Araviv\({}^{}\)4 & DT & 164.30 & 2.713 & 0.349 & **90.54** & 2.32 \\ CogVideoX-2B  & Araviv\({}^{}\)4 & DT & **159.31** & **3.225** & 0.404 & 43.15 & **2.92** \\  ^{}\)} & Ours & DT & 185.72 & 2.753 & 0.341 & 49.85 & 3.03 \\ OpenSoraPlan v1.1\({}^{}\) & Ours & DT & **180.11** & **2.864** & **0.346** & **70.12** & **3.05** \\   

Table 3: **Quantitative comparison with state-of-the-art T2V generation methods for the text-to-video task in ChronoMagic-Bench. \({}^{}\)\({}^{}\)\({}^{}\) denotes lower is better. \({}^{}\)\({}^{}\) denotes higher is better. \({}^{}\)\({}^{}\) denotes full parameters fine-tuning, \({}^{}\)\({}^{}\)\({}^{}\) denotes fine-tuning using the Magic Training Strategy .**

### Comprehensive Analysis

Quantitative Evaluation.We first present and analyze the results from a qualitative perspective. All input texts are from our ChronoMagic-Bench. Unlike existing benchmarks that only assess general videos, our evaluation task focuses on generating metamorphic videos, such as the construction of houses in Minecraft, the blooming of flowers, the baking of bread rolls, and the melting of ice cubes. As shown in Figure 5, almost all U-Net-based and DiT-based models are limited to generating general videos and fail to follow prompts to produce videos with significant motion and temporal spans, except for MagicTime  and CogVideoX  (training data contains time-lapse videos), which underscores the importance of ChronoMagic-Pro dataset. Since T2V-Zero  is a zero-shot video generation model, its coherence is significantly lacking, although its visual quality is acceptable. Among the emerging DiT-based video models, CogVideoX  and OpenSora v1.2  stands out as a representative that matches the performance of U-Net based methods, followed by EasyAnimate V3 , OpenSoraPlan v1.1 , while Latt  shows poor text-following capability.

Qualitative Evaluation.Next, we present and analyze the results of different T2V models from a qualitative perspective as shown in Table 3. Consistent with Figure 5, MagicTime  and CogVideoX , as the only model capable of generating metamorphic videos, has the highest MTScore and GPT4o-MTScore among all models. The other models, trained only on general videos, produce videos with limited motion range due to the minimal physical knowledge encoded in the models. It can also be seen that the results of the MTScore based on feature space with lower overhead and the GPT4o-MTScore based on question answering with higher overhead are roughly similar, proving the effectiveness of the proposed indicators. Additionally, ZeroScope  has limited metamorphic implitude but the best coherence, while the zero-shot algorithm T2V-Zero  has the lowest CHScore. U-Net based and DiT-based models have similar CHScore, but the former shows superior average metamorphic amplitude. For visual quality and text relevance, the emerging CogVideoX  and EasyAnimate  performed best. The OpenSoraPlan v1.1  and OpenSora 1.1&1.2  have visual quality comparable to U-Net based methods, but slightly inferior text relevance. Only MagicTime  and CogVideoX  follows the prompt to generate

Figure 5: **Qualitative comparison with different T2V generation methods for the text-to-video task in ChronoMagic-Bench. Most models can not follow instructions to generate time-lapse videos.**a time-lapse video, but the UMTScore  is the lowest. We infer that the UMT-FVD  and UMTScore  are inconsistent with human perception.

**Human Preference.** Finally, we cross-validate the effectiveness of the different metrics through Human Study. We randomly select the generated videos corresponding to 16 prompts and invited 212 participants to vote, obtaining manual evaluation results. To enhance user satisfaction, we select only six representative baseline results and reference videos from which users can choose. Figure 6 shows the correlation between automatic metrics and human perception. It is evident that the proposed three metrics, MTScore, CHScore, and GPT4o-MTScore, are consistent with human perception and can accurately reflect the metamorphic amplitude and temporal coherence of T2V models. Additionally, as mentioned earlier, UMTScore  cannot accurately measure text relevance, especially in the evaluation of time-lapse videos, where its Kendall and Spearman coefficients are the lowest. We infer that its feature space is not suitable for time-lapse video. For more details please refer to Appendix D.

**Extended Analysis of Closed-Source Models** In this section, we explore the performance and limitations of closed-source models. Given the impracticality of manually testing all 1,649 prompts in ChronoMagic-Bench, we selected two hard prompts from each of the 75 categories, resulting in ChronoMagic-Bench-150. We first analyze the results from a quantitative perspective. As shown in Table 4, with Dream Machine  performing better in metamorphic amplitude (MTScore, GPT4o-MTScore) and Pika-1.0  showing the worst text relevance (UMTScore). DiT-based methods outperform U-Net based ones in visual quality. To facilitate comparison under a unified standard, we also test open-source models on ChronoMagic-Bench-150. It is evident that for most models, the MTscore and GPT4o-MTScore are low, and they are unable to generate videos involving complex state changes. Additionally, due to the inherent limitations of UMT-FVD  and UMTScore , they fail to accurately reflect the differences between open-source and closed-source models. However, the qualitative analysis across all models demonstrates that closed-source models consistently surpass open-source models in visual quality and textual relevance. Furthermore, it is worth noting that the results within the same domain (open/closed) align with human evaluations. We also conduct a detailed qualitative analysis, please refer to Appendix D.5 for more details.

  
**Method** & **Venue** & **Backbone** & **Status** & **UMT-FVD\({}_{}\)** & **UMTScore\({}^{}\)** & **MITScore\({}^{}\)** & **CIGscore\({}^{}\)** & **GPT4o-MTScore\({}^{}\)** \\  Gen-2  & Runway & U-Net & Close-Source & 218.99 & **2.400** & 0.373 & **125.25** & 2.62 \\ Pika-1.0  & Pika-1.0  & U-Net & Close-Source & 223.05 & 2.317 & 0.347 & 75.98 & 2.48 \\ Dream Machine  & LUMA & DT & Close-Source & 224.91 & 2.387 & **0.474** & 95.97 & **3.11** \\ KeLing  & Kwai & DT & Close-Source & **202.32** & 2.517 & 0.369 & 74.20 & 2.74 \\  ModelScore(T2V)  & Arxiv23 & U-Net & Open-Source & 230.74 & 2.783 & 0.409 & 61.01 & 3.01 \\ ZeroScope  & CVPR-23 & U-Net & Open-Source & 260.61 & 2.232 & 0.403 & **94.67** & 2.29 \\ T2V-zero  & ICCV23 & U-Net & Open-Source & 250.22 & 2.559 & 0.398 & 18.54 & 2.62 \\ LaTe  & Arxiv23 & U-Net & Open-Source & **230.39** & 2.714 & 0.350 & 81.32 & 2.50 \\ AnimatDuff V3  & CLIR-24 & U-Net & Open-Source & 239.31 & **2.837** & 0.470 & 70.36 & 2.62 \\ VideoCenter(T21)  & CVPR-23 & U-Net & Open-Source & 214.06 & 2.763 & 0.437 & 75.90 & 2.87 \\ MC-M-M-NLION & Arxiv24 & U-Net & Open-Source & 244.49 & 2.282 & 0.422 & 58.08 & **3.06** \\ Magic-Time  & Arxiv24 & U-Net & Open-Source & 294.72 & 1.763 & **0.479** & 77.98 & 3.05 \\  Late  & Arxiv24 & DT & Open-Source & 232.29 & 2.122 & 0.366 & 72.57 & 2.42 \\ OpenSora I.  & Github24 & DT & Open-Source & 241.09 & 2.676 & 0.448 & 75.94 & 2.57 \\ OpenSora I. [2.19] & Github24 & DT & Open-Source & 210.93 & 2.681 & 0.331 & 51.87 & 2.50 \\ OpenSoraI.Puri 1.1  & Github24 & DT & Open-Source & 228.70 & 2.459 & 0.331 & 61.50 & 2.21 \\ EasyAnimate V3  & Arxiv24 & DT & Open-Source & 202.03 & 2.733 & 0.352 & **88.48** & 2.33 \\ CogTokox-28  & Arxiv24 & DT & Open-Source & **195.52** & **3.240** & **0.472** & 38.64 & **3.09** \\   

Table 4: **Quantitative Comparison with _Closed-Source_ Generation Methods for the Text-to-Video Task in ChronoMagic-Bench-150.** To facilitate comparison under a unified standard, we also test _Open-Source_ models. \({}^{}\)\({}^{}\)\({}^{}\) denotes lower is better. \({}^{}\)\({}^{}\)\({}^{}\) denotes higher is better.

Figure 6: **Alignment between automatic metrics and human perception in terms of visual quality, textual relevance, metamorphic amplitude, and temporal coherence. \(\) and \(\) represent Kendall\({}^{}\)\({}^{}\) and Spearman\({}^{}\) coefficients, respectively. \({}^{}\)\({}^{}\)\({}^{}\) denotes higher is better.**

Exploratory Experiment on ChronoMagic-Pro. To verify the validity and robustness of the ChronoMagic-Pro, we conducted quantitative validation based on OpenSoraPlan v1.1 . Specifically, we fine-tuned the temporal module of the OpenSoraPlan v1.1 model using the Magic Training Strategy , based on the weights of OpenSoraPlan v1.1 . Due to limited computational resources, we randomly selected only 10,000 video-text pairs from ChronoMagic-Pro for training. The results are shown in Table 3. After fine-tuning with ChronoMagic-Pro, the visual quality (i.e., UMT-FVD), text relevance (i.e., UMTScore), and metamorphic amplitude (i.e., MTScore and GPT4-MTScore) were all effectively improved. Moreover, we utilized a straightforward method (e.g., full parameters) to fine-tune the model; however, the results suggest that this is less effective than the Magic Training Strategy . More details and qualitative analysis are provided in Appendix D.3.

Guideline for Model Selection. With the increasing number of T2V models, the community faces challenges in selecting the most appropriate model due to the tendency of each model to showcase its best results. To address this issue, we provide a guideline for model selection based on the evaluation results of ChronoMagic-Bench: (a) Except for MagicTime , CogVideoX  and Dream Machine , most T2V models exhibit minimal metamorphic amplitude and cannot generate complete processes rich in physical changes, such as seed germination, sunrise, or building construction; (b) The visual quality of a single frame may be high, but when viewed in sequence, flickering often occurs, indicating poor temporal coherence. This issue is particularly evident in T2V-zero  and OpenSora 1.2 , whereas closed-source models do not exhibit this problem; (c) The emergence of Sora  has promoted the rapid development of DiT-based methods. Closed-source models based on DiT have comprehensively surpassed those based on U-Net. However, most open-source models' visual quality, text-following capability, and metamorphic amplitude still lag behind U-Net-based methods. We speculate that DiT-based models are more scalable and require more data, giving closed-source models a significant advantage over open-source models; (d) It is expensive to access massive data and computing resources. First, they can build datasets by crawling videos without copyright disputes. Second, adopting the U-DiT architecture may balance performance and cost to a certain extent; (e) Ordinary users who want to try T2V models can prioritize Dream Machine  and KeLing . Researchers who wish to conduct in-depth research on T2V can prioritize the study of metamorphic video generation with CogVideoX , EasyAnimate , OpenSoraPlan  and OpenSora , as neither open-source nor closed-source models can achieve this function.

## 6 Conclusion

In this paper, we present ChronoMagic-Bench, the first benchmark specifically designed to assess the generation of time-lapse videos. It addresses the shortcomings of current benchmarks, which primarily focus on standard videos and overlook critical elements such as metamorphic amplitude and coherence. Additionally, we introduce two new automated metrics, MTScore and CHScore, which align with human perception. Based on ChronoMagic-Bench, we conduct a comprehensive evaluation of almost all open&closed-source leading text-to-video (T2V) models and provide crucial insights into the strengths and weaknesses of various models. Moreover, we propose ChronoMagic-Pro, the first large-scale time-lapse T2V dataset, to facilitate further research by the community.

## 7 Limitations and Future Work

While _ChronoMagic-Bench_ offers a robust evaluation framework, there are still two limitations of this work. (1) The majority of the data is sourced from YouTube, where video quality varies significantly, necessitating extensive filtering based on aesthetic criteria, views, and likes. Additionally, most videos are licensed under CC BY 4.0, limiting their use to academic research. (2) Despite introducing MTScore and CHScore for metamorphic attributes and temporal coherence, a clear gap remains between these existing metrics and human preferences. Future efforts will focus on aligning automated metrics more closely with human judgment for a more accurate evaluation of T2V models.

## 8 Acknowledgments

We thank all the anonymous reviewers for their constructive comments. This work was supported in part by the Natural Science Foundation of China (No. 62202014, 62332002, 62425101), and Shenzhen Basic Research Program (No. JCYJ20220813151736001).