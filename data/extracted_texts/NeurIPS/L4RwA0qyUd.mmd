# Proximal Causal Inference with Text Data

Jacob M. Chen

Department of Computer Science

Johns Hopkins University

jchen459@jhu.edu &Rohit Bhattacharya

Department of Computer Science

Williams College

rb17@williams.edu &Katherine A. Keith

Department of Computer Science

Williams College

kak5@williams.edu

###### Abstract

Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data. These approaches, however, assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is sometimes infeasible due to data privacy or annotation costs. In this work, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that uses two instances of pre-treatment text data, infers two proxies using two zero-shot models on the separate instances, and applies these proxies in the proximal g-formula. We prove, under certain assumptions about the instances of text and accuracy of the zero-shot predictions, that our method of inferring text-based proxies satisfies identification conditions of the proximal g-formula while other seemingly reasonable proposals do not. To address untestable assumptions associated with our method and the proximal g-formula, we further propose an odds ratio falsification heuristic that flags when to proceed with downstream effect estimation using the inferred proxies. We evaluate our method in synthetic and semi-synthetic settings--the latter with real-world clinical notes from MIMIC-III and open large language models for zero-shot prediction--and find that our method produces estimates with low bias. We believe that this text-based design of proxies allows for the use of proximal causal inference in a wider range of scenarios, particularly those for which obtaining suitable proxies from structured data is difficult.

## 1 Introduction

Data-driven decision making relies on estimating the effect of interventions, i.e. _causal effect estimation_. For example, a doctor must decide which medicine she will give her patient, ideally the one with the greatest effect on positive outcomes. Many causal effects are estimated via randomized controlled trials--considered the gold standard in causal inference; however, if an experiment is unfeasible or unethical, one must use observational data. In observational settings, a primary obstacle to unbiased causal effect estimation is confounding variables, variables that affect both the treatment (e.g., which medicine) and the outcome.

Recently, some studies have attempted to mitigate confounding by incorporating (pre-treatment) unstructured text data as proxies for confounding variables or by specifying confounding variables as linguistic properties, e.g., topic (Veitch et al., 2020; Roberts et al., 2020), tone (Sridhar and Getoor, 2019), or use of specific word types (Olteanu et al., 2017). A wide range of fields have used text incausal estimates, including medicine (Zeng et al., 2022), the behavioral social sciences (Kiciman et al., 2018), and science-of-science (Zhang et al., 2023). See Keith et al. (2020); Feder et al. (2022); Egami et al. (2022) for general overviews of text-based causal estimation.

If all confounders are directly observed, then causal estimation is relatively1 straightforward with _backdoor adjustment_(Pearl, 2009). However, known confounders are often unobserved. In such scenarios, researchers typically use supervised classifiers to predict the confounding variables from text data, but these text classifiers rarely achieve perfect accuracy and _measurement error_ must be accounted for. To address this, another line of work has developed post-hoc corrections of causal estimates in the presence of noisy classifiers (Wood-Doughty et al., 2018; Fong and Tyler, 2021; Egami et al., 2023; Mozer et al., 2023). These approaches, however, require ground-truth labels of the confounding variables for a subset of instances, a constraint that is not always feasible due to privacy restrictions, high annotation costs, or lack of expert labor for labeling.

Our work fills this gap. We address the causal estimation setting for which a practitioner has specified a confounding variable that is truly unmeasured (we have no observations of the variable), but unstructured text data could be used to infer proxies. For this setting, our method combines _proximal causal inference_ with zero-shot classifiers.

Proximal causal inference (Miao et al., 2018; Tchetgen Tchetgen et al., 2020; Liu et al., 2024) can identify the true causal effect given _two_ proxies for the unmeasured confounder that satisfy certain causal identification conditions. A major criticism of this method is that it can be difficult to find two suitable proxies among the structured variables; however, we conjecture that unstructured text data (if available) could be a rich source of potential proxies.

In our proposed method, summarized in Figure 1, we estimate two proxies from text data via zero-shot classifiers, i.e. classifiers that perform an unseen task with no supervised examples. In subsequent sections, we expand upon this method and its assumptions and empirically validate it on synthetic and semi-synthetic data with real-world clinical notes. Since large pre-trained language models (LLMs) have promising performance on zero-shot classification benchmarks (Yin et al., 2019; Brown et al., 2020; Wei et al., 2021; Sanh et al., 2021, _inter alia_), we use LLMs to infer both of the proxies in our experimental pipeline. Our combination of proximal causal inference and zero-shot classifiers is not only novel, but also expands the set of text-specific causal designs available to practitioners.2

In summary, our **contributions** are

Figure 1: **Pipeline for proximal causal inference with text data**. The top row of captions describe the general pipeline that uses text data from any setting, and the bottom italicized row describes an illustrative example based on our semi-synthetic experiments in Sec. 5. (1) We filter to only pre-treatment text; (2 and 3) for each individual in the analysis, we select two distinct instances of text (e.g., echocardiogram and nursing notes) via metadata with the goal of satisfying \(_{1}^{}_{2}^{} U,\); (4 and 5) we use \(_{1}^{}\) and \(_{2}^{}\) as inputs into LLM-1 and LLM-2, respectively, to infer zero-shot proxies \(Z\) and \(W\). (7) If the proxies fail our odds ratio heuristic, analysis stops. (8, 9, and 10) Else, we use the proximal g-formula implied by the casual DAG to estimate the causal effect.

* We propose a new causal inference method that uses distinct instances of pre-treatment text data, infers two proxies from two different zero-shot models on the instances, and applies the proxies in the proximal g-formula (Tchetgen Tchetgen et al., 2020).
* We provide theoretical proofs that our method satisfies the identification conditions of _proximal causal inference_ and prove that other seemingly reasonable alternative methods do not.
* We propose a falsification heuristic that uses the odds ratio of the proxies conditional on observed covariates as an approximation of the (untestable) proximal causal inference conditions.
* In synthetic and semi-synthetic experiments using MIMIC-III's real-world clinical notes (Johnson et al., 2016), our odds ratio heuristic correctly flags when identification conditions are violated. When the heuristic passes, causal estimates from our method have low bias and confidence intervals that cover the true parameter; when the heuristic fails, causal estimates are often biased.

## 2 Problem Setup And Motivation

To motivate our approach, imagine we are an applied practitioner tasked with determining the effectiveness of thrombol (clot busting) medications relative to blood thinning medications to treat clots arising from an ischemic stroke. Such medications are usually administered within three hours of the stroke to improve chances of patient recovery (Zaheer et al., 2011). Given the urgency and the short treatment window, running a randomized experiment to compare these drugs is difficult. Left with only observational data, we examine electronic health records (EHRs) from a database like MIMIC-III (Johnson et al., 2016).

We formalize our causal estimand as follows: let \(A\) denote a binary treatment variable corresponding to clot busting (\(A=1\)) or blood thinning (\(A=0\)) medication, and let \(Y\) denote measurements of the D-dimer protein in the patient's blood which directly measures how much of the clotting has dissolved. In do-calculus notation (Pearl, 2009), the target causal estimand is the average causal effect, \([Y|(A=1)]-[Y| (A=0)]\).

Examining the EHRs, we find potential confounders (in structured tabular form), including biological factors, such as age, sex, and blood pressure, as well as socio-economic factors, such as income. We denote the observed confounders as the set \(\). However, we are worried about biased causal effects because atrial fibrillation (irregular heart rhythms) is an important confounder corresponding to a pre-existing heart condition that is not recorded in the structured data. We denote this unmeasured confounder as \(U\), and assume for the rest of this paper that \(\) and \(U\) form a sufficient backdoor adjustment set with respect to \(A\) and \(Y\)(Pearl, 1995). Figure 2(a) depicts this problem setup in the form of a causal directed acyclic graph (causal DAG) (Spirtes et al., 2000; Pearl, 2009). With the presence of \(U\), it is well known that adjusting for just the observed confounders via the backdoor formula \(_{}([Y|A=1,]-[Y|A=0,])  p()\) will give a biased estimate of the ACE (Pearl, 1995). In response to this issue, we consider work that uses proxy variables of the unmeasured confounder. However, we are subject to the following **restriction**:

* We do not have access to the value of \(U\) for any individuals in the dataset.

This kind of restriction is common in healthcare or social science settings when data privacy issues, high costs, or lack of expert labor make analysts unable to hand-label unstructured text data. We elaborate in Appendix A.

In such cases, we turn to using proxies for \(U\). Pearl (2010) proposed a method for obtaining unbiased effect estimates with a single proxy \(W\) under the assumption that \(p(W|U)\) is known or estimable, which are impossible for us under (R1). However, a more recent line of work, building from Griliches (1977) and Kuroki and Pearl (2014), called _proximal causal inference_(Miao et al., 2018; Tchetgen Tchetgen et al., 2020) is able to identify the true causal effect as long as the analyst proposes two proxies \(W\) and \(Z\) that satisfy the following independence conditions:

Figure 2: Causal DAGs (a) depicting unmeasured confounding and (b) compatible with the canonical assumptions used for _proximal causal inference_(Tchetgen Tchetgen et al., 2020).

* Conditional independence of proxies: \(W\!\!\! Z U,\)
* One of the proxies, say \(W\), does not depend on values of the treatment: \(W\!\!\! A U,\)
* The other proxy, \(Z\), does not depend on values of the outcome: \(Z\!\!\! Y A,U,\)

A canonical example of proxies that satisfy these conditions is shown in Figure 2(b)3. In addition to these independence relations that impose the absence of certain edges in the causal DAG, e.g., no edge can be present between \(Z\) and \(W\) to satisfy (P1), there is an additional completeness condition that imposes the existence of \(U Z\) and \(U W\). This condition is akin to the relevance condition in the instrumental variables literature Angrist et al. (1996) and ensures that the proxies \(W\) and \(Z\) exhibit sufficient variability relative to the variability of \(U\).

* Completeness: for any square integrable function \(v()\) and for all values \(w,a,\), we have \[[v(U) w,a,]=0 v(U)=0,[v(Z) w,a,]=0 v(Z)=0.\]

Intuitively, these completeness conditions do not hold unless \(Z\) and \(W\) truly hold some predictive value for the unmeasured confounder \(U\). See Miao et al. (2018) for more details on completeness. Under (P1-P4), each piece of the ACE, \([Y|(A=a)]\), is identified via the _proximal g-formula_, \[[Y(a)]=_{w,}h(a,w,)  p(w,),\] (1) where \(h(a,w,)\) is the "outcome confounding bridge function" that is a solution to the equation \([Y|a,z,]=_{w}h(a,w,) p(w|a,z,)\)(Miao et al., 2018). Although the existence of a solution is guaranteed under (P1-P4), solving it can still be difficult. However, there exist simple two-stage regression estimators for the proximal g-formula (Tchetgen et al., 2020; Mastouri et al., 2021) that we make use of in Section 5 once we have identified a valid pair of proxies. This brings us to the primary criticism of proximal causal inference.

Primary criticismWhen using proximal causal inference in practice, how do we find two proxies \(W\) and \(Z\) among the structured variables such that they happen to satisfy all of (P1-P4)? We often cannot, at least not without a high degree of domain knowledge.4 Furthermore, empirically testing for any of (P1-P3) in general is, by definition of the problem, not possible because doing so requires complete access to the unobserved confounder \(U\).

Our approachInstead, in this work, we propose relying on raw unstructured text data (e.g., clinical notes) in an attempt to infer proxies that satisfy (P1-P4) _by design_.

Returning to our motivating problem, our approach applies classifiers zero-shot--since we have no training data with \(U\) given (R1)--to two distinct instances of pre-treatment clinical notes of each patient and obtains two predictions, \(W\) and \(Z\), for atrial fibrillation (our \(U\)). In order to make it more feasible to estimate the ACE from data, we make the following two relatively weak assumptions:

* The unmeasured confounder \(U\) between \(A\) and \(Y\) can be specified as a binary variable.
* The text only causes \(W\) and \(Z\) (and no other variables).

Assuming (S1) simplifies estimation since text classification typically performs better empirically than text regression (Wang et al., 2022). Assumption (S2) asserts that the text data considered serves as a record of events rather than actionable data.5

## 3 Designing Text-Based Proxies

In this section, we describe our method for designing text-based proxies. In doing so, we describe various "gotchas," pitfalls in attempting to use these text-based proxies in causal effect estimation. We describe each gotcha and explore how they lead us to our final recommended design, given by Figure 3(d). Our empirical results in Section 5 demonstrate, as expected, that these pitfall approaches result in biased causal effect estimates.

Gotcha #1: Using predictions directly in backdoor adjustment.Suppose we try to avoid the complications of proximal causal inference by using the predictions from one of our zero-shot models, say \(W\), as the confounding variable itself.

**Proposition 1**.: _Using a proxy \(W\) in the backdoor adjustment formula results in biased estimates of the ACE in general._

Proof.: If \(W U\) for some subset of instances, there remains an open backdoor path through \(U\), and the ACE remains biased as \(_{U,}([Y A=1,U,]-[Y A=0,U,]) p(U,)_{W,}([Y A= 1,W,]-[Y A=0,W,]) p(W,)\) in general (Pearl, 1995). 

The most straightforward way to obtain unbiased results under Gotcha #1 is to have 100% accuracy between \(W\) and \(U\), a scenario that is extremely unlikely in the real world. Further, under (R1)6, we cannot measure accuracy at inference time since we do not have any observations of \(U\). As expected, in our semi-synthetic experiments in Section 5, we find using predictions of \(W\) directly in a backdoor adjustment formula results in biased estimates; see Figure 4.

Gotcha #2: Using post-treatment text.While it is well-known that adjusting for post-treatment covariates in the backdoor formula often leads to bias (Pearl, 2009), it is not obvious what might go wrong when using post-treatment text to infer proxies for the proximal g-formula.

**Proposition 2**.: _If both \(W\) and \(Z\) are inferred from zero-shot models on text that contain post-treatment information, then the resulting proxies violate either (P2), (P3), or both._

Proof.: Consider Figure 3(a), where the proxies are produced using text that is post-outcome and thus also post-treatment. We show that this violates both (P2) and (P3). Clearly the DAG violates (P3): by a simple d-separation argument we see that \(Z X A,U,\) due to the open path \(Y\). Similarly, (P2) is violated from the open path \(A Y\) - \( W\). 

Thus, before performing zero-shot inference, it is important that the text for each individual is filtered in such a way that it contains only the text preceding treatment7. In our running clinical example, we can avoid this gotcha by using the time stamps of the clinical notes and information about when the patient was treated and discharged.

Figure 3: Causal DAGs depicting several different scenarios for inferring text-based proxies. Edges with different colors and patterns, e.g., \(\)\(\) and \(\), indicate that different zero-shot models were used. Our final recommended method is based on (d).

Gotcha #3: Predicting both proxies from the same instance of text.After filtering to only pre-treatment text, \(^{}\), for each individual, the intuitive next step is to use \(^{}\) to infer \(W\) and \(Z\).

**Proposition 3**.: _If \(W\) and \(Z\) are inferred via zero-shot models on the same instance of pre-treatment text, the resulting proxies violate (P1)._

Proof.: Consider the causal DAG with proxies \(W\) and \(Z\) in Fig 3(b). By d-separation we have \(W\!\!\! Z U,\) due to the path \(Z^{}_{1}\) and \(^{}_{2}\), such that \(^{}_{1}\!\!\!^{}_{2} U, ^{8}\). For example, in the clinical setting we use metadata to select two separate categories of notes for each individual patient, e.g., nursing notes and echocardiogram notes. We hypothesize this satisfies \(^{}_{1}\!\!\!^{}_{2} U, \) since different providers will likely write reports that differ in content and style. Although the availability of distinct instances is domain-dependent, we hypothesize this type of data exists in other domains as well; for example, one could select distinct social media posts written by the same individual or multiple speeches given by the same politician.

Gotcha #4: Using a single zero-shot model.After splitting text into \(^{}_{1}\) and \(^{}_{2}\), should we apply the same zero-shot model to these two instances of text to infer the proxies \(W\) and \(Z\), as in Figure 3(c), or should we apply two separate zero-shot models as in Figure 3(d)? We find different answers in theory and practice. Proposition 4 shows that, in theory, both are valid as long as \(^{}_{1}\!\!\!^{}_{2} U, \). However, we later describe how our semi-synthetic experiments demonstrate the need for two different models in practice in Section 5. We first establish the theoretical validity of both strategies.

**Proposition 4**.: _If \(W\) and \(Z\) are inferred using zero-shot classification on two unique instances of pre-treatment text such that \(^{}_{2}\!\!\!^{}_{2} U, \), then these proxies satisfy (P1-P3). Additionally, if the proxies are predictive of \(U\), i.e., \(Z\!\!\! U\) and \(W\!\!\! U\), then (P4) holds._

Proof.: Suppose we apply zero-shot classification models to two splits of pre-treatment text in a way that results in causal DAGs shown in Figure 3(c) or (d) depending on whether we use one or two models, respectively. Applying d-separation confirms that the conditions (P1-P3) hold in both cases.

Let \(|_{V}|\) denote the number of categories of a variable \(V\). Kuroki and Pearl (2014); Tchetgen Tchetgen et al. (2020) state that when \(W\) and \(Z\) are predictive of \(U\) (as stated in the proposition), a sufficient condition for (P4) is \((|_{Z}|,|_{W}|)|_{U}|\). Since \(U\) is binary under (S1) and since \(W\) and \(Z\) are discrete variables because they are inferred from classifiers, this condition is satisfied. Hence, (P4) is satisfied. 

Our Final Design ProcedureFigure 1 and the causal DAG in Figure 3(d) summarize our final design procedure--obtain two distinct instances of pre-treatment text for each individual and apply two distinct zero-shot models to obtain \(W\) and \(Z\).

In Proposition 4, we formalized how this procedure can be used to design proxies that satisfy the proximal conditions (P1-P4). However, this result relied on two important pre-conditions: (1) the conditional independence of the two instances of text, and (2) \(W\) and \(Z\) being (at least weakly) predictive of \(U\). Yet, both of these conditions are untestable in general; this motivates the next section.

## 4 Falsification: Odds Ratio Heuristic

In practice, a major challenge for our procedure--and indeed all causal methods--are its assumptions. Sometimes, causal models imply testable restrictions on the observed data that can be used in _falsification_ or _confirmation tests_ of model assumptions, see Wang et al. (2017); Chen et al. (2023); Bhattacharya and Nabi (2022) for tests of some popular models. In our case, the proximal model implies no testable restrictions (Tchetgen Tchetgen et al., 2020), so the best we can do is provide a _falsification heuristic_ that allows analysts to detect serious violations of (P1-P4) when using the inferred proxies and stop analysis. Our heuristic is based on the odds ratio function described below.

Given arbitrary reference values \(w_{0}\) and \(z_{0}\), the conditional odds ratio function for \(W\) and \(Z\) given covariates \(\) is defined as Chen (2007), \((w,z)=)}{p(w_{0}|z,)} |z_{0},)}{p(w|z_{0},)}\). This function is important because \(W\!\!\! Z\) if and only if \((w,z)=1\) for all values \(w,z,\). We summarize this odds ratio as a single free parameter, \(_{WZ.}\), and, for the simplicity of our pipeline, we estimate it under a parametric model for \(p(W|Z,)\)9.

Now, we describe our proximal conditions in terms of odds ratio parameters. If (P1-P3) are satisfied, then \(W\!\!\! Z U,\) and \(_{WZ.U}=1\). Further, if the zero-shot models are truly predictive of \(U\), then (P4) is satisfied and \(W\!\!\! Z\), which means that \(_{WZ.} 1\). Ideally, we would want to estimate both of these odds ratio parameters to confirm (P1-P4) empirically; however, \(_{WZ.U}\) cannot be computed from observed data alone due to (R1).

Using a parameter we can estimate from observed data, \(_{WZ.}\), we propose an **odds ratio falsification heuristic** in lines 3-6 of Algorithm 1. Now, we explain why, if this heuristic holds, an analyst can be reasonably confident in using their inferred text-based proxies for estimation.

First, we examine a lower bound on \(_{WZ.}\). Based on our previous discussion, if \(_{WZ.}\) is close to \(1\), then we should suspect that one or both of our zero-shot models failed to return informative predictions for \(U\). Next, let us treat \(_{WZ.}\) as an imperfect approximation of \(_{WZ.U}\). Let \(W,Z,U\) be binary with reference values \(w_{0}=z_{0}=u_{0}=0\). VanderWeele (2008) proposed the following three conditions under which an odds ratio \(_{WZ.}\) that fails to adjust for an unmeasured confounder \(U\) is an _overestimate_ of the true odds ratio \(_{WZ.U}\): (i) \(\{U\}\) satisfies the backdoor criterion with respect to \(W\) and \(Z\); (ii) \(U\) is univariate or consists of independent components conditional on \(\); (iii) \([W|u,z,]\) is non-decreasing in \(U\) for all \(z\) and \(\) and \([Z|u,]\) is non-decreasing in \(U\) for all \(\).

Condition (i) is satisfied from Graph 3(d), and condition (ii) is satisfied by assumption (S1). Finally, condition (iii) is satisfied when our zero-shot models are reasonable predictors of the unmeasured confounder \(U\) by the following argument. Notice that \([W|u,z,]=[W|u,]=p(W=1|u,)\), where the first equality follows from d-separation in Graph 3(d) and the second equality follows from the definition of expectation for binary variable \(W\). Then we should expect, if the zero-shot models are reasonably accurate, that \(p(W=1|U=1,)>p(W=1|U=0,)\). Therefore, the first part of condition (iii) is satisfied. Similar logic holds for \([Z|u,]\).

Hence, we have shown that under ideal conditions that satisfy (P1-P4), we should expect \(_{WZ.}>_{WZ.U}=1\), and we should reject proxies \(W\) and \(Z\) when we observe an odds ratio \(_{WZ.} 1\). Next, we examine the upper bound on \(_{WZ.}\). Consider the extreme case where \(_{WZ.}=\). This corresponds to a situation where \(W=Z\), so (P1) is clearly not satisfied. In general, if \(_{WZ.}\) is higher than some threshold \(_{}\), corresponding to the maximum association that one could reasonably explain by a single open path through \(U\), we should suspect that perhaps the proxies \(W\) and \(Z\) are associated with each other due to additional paths through other unmeasured variables that make it so that the two instances of text are not independent of each other, i.e., \(_{1}^{}\!\!\!_{2}^{} U,\).

Following standard practice in _sensitivity analysis_, e.g., Liu et al. (2013); Leppala (2023), we leave it to the analyst to specify the upper bound \(_{}\) based on domain knowledge. In our experiments in Section 5, we found that, when the proximal conditions are not satisfied, \(_{WZ.}\) far exceedsany reasonable setting of \(_{}\). Hence, our heuristic works quite well in practice even with a generous suggestion for an upper bound. We describe our full design procedure with the diagnostic in Algorithm 1 and calculate 95% confidence intervals for \(_{WZ.}\) via the bootstrap percentile method (Wasserman, 2004). We now evaluate its effectiveness for downstream causal inference.

## 5 Empirical Experiments and Results

RQsIn this section, we explore the following empirical research questions (RQs): How does Algorithm 1 compare to other alternatives in terms of bias and confidence interval coverage of the estimated causal effects? Does our odds ratio heuristic effectively flag when to stop or proceed?

In causal inference, empirical evaluation is difficult because it requires ground-truth labels for counterfactual outcomes of an individual under multiple versions of the treatment, data that is generally impossible to obtain (Holland, 1986). Thus, we turn to synthetic data and semi-synthetic data so we have access to the true ACE and \(U\) to evaluate methods. Semi-synthetic experiments--which use real data for part of the DGP and then specify synthetic relationships for the remainder of the DGP--have been used extensively for other empirical evaluation of causal estimation methods; see Shimoni et al. (2018); Dorie et al. (2019); Veitch et al. (2020).10 We describe the experimental set-ups, the causal estimation procedure used by all experiments, and finally, the results to our RQs.

    & }^{},_{WZ.}^{})\)} & Est. ACE & Bias & Conf. Interval (CI) & CI Cov. \\  P1M & \((1.35,142)^{}\) & \(1.304\) & **0.004** & \((1.209,1.394)\) & **Yes** \\ P1M, same & \((10^{16},10^{16})\) & \(1.430\) & \(0.130\) & \((1.405,1.495)\) & No \\ P2M & \((1.82,1.94)^{}\) & \(1.343\) & **0.043** & \((1.273,1.425)\) & **Yes** \\ P2M, same & \((7.9,8.41)\) & \(1.407\) & \(0.107\) & \((1.376,1.479)\) & No \\   

Table 1: **Fully synthetic results** with the true ACE equal to \(1.3\). Here, \(\) distinguishes settings that passed the odds ratio heuristic from those that failed it, with \(_{}=2\). Corresponding to Gotcha #3, “same” indicates we used the same instance of text to infer both \(W\) and \(Z\).

    & }\)} & }\) CI} \\  & \(_{1}^{}\) Cat. & \(_{2}^{}\) Cat. & P1M & P2M & P1M & P2M \\  A-Sis & Echo & Radiology & \(1.626\) & \(1.085\) & \((2.381,2.962)\) & \((1.372,1.995)^{}\) \\ Heart & Echo & Nursing & \(2.068\) & \(1.156\) & \((2.298,2.676)\) & \((1.152,1.376)^{}\) \\ A-Sis & Radiology & Nursing & \(2.350\) & \(1.328\) & \((4.337,5.266)\) & \((2.050,2.663)\) \\   

Table 2: **Semi-synthetic odds ratio heuristic (\(_{WZ.}\)) as well as the oracle \(_{WZ.}\) for different categories of notes (Cat.). We distinguish settings that passed the odds ratio heuristic (\(\)) from those that failed, with \(_{}=2\).**

Figure 4: **Semi-synthetic results** for ACE point estimates (dots) and 95% CIs (bars). We distinguish settings that passed the odds ratio heuristic (\(\)) from those that failed, with \(_{}=2\).

Fully Synthetic ExperimentsWe create our fully synthetic DGP based on the DAG in Figure 3(d); see Appendix C for full details. To summarize, \(A\) and \(U\) are binary, and \(Y\) and \(C\) are continuous. We generate (very simple) synthetic text data with four continuous variables, \(X_{1},X_{2},X_{3},X_{4}\), as functions of \(U\) and \(C\). For training, we generate two realizations of these variables, which we call \(_{1}^{}\) and \(_{2}^{}\), and likewise two realizations for inference time, \(_{1}^{}\) and \(_{2}^{}\).

At inference time, we explore using one or two zero-shot models, which we refer to as _Proximal 1-Model_ (P1M) and _Proximal 2-Model_ (P2M), respectively. For one zero-shot model, we train a logistic regression classifier to predict the true \(U\) from an aggregated variable \(}^{}=(_{1}^{}+_{2}^{})/2\) as \(P_{}(U=1|}^{})\)11. For the other zero-shot model, we use the following heuristic: predict \(1\) if \(X_{1}>1.1\) else \(0\). P1M uses only the logistic regression model, and P2M uses both the logistic regression and heuristic models. Next, we vary whether Gotcha #3 holds at inference time for both P1M and P2M, i.e. whether the models infer \(Z\) and \(W\) from only \(_{1}^{}\) or both \(_{1}^{}\) and \(_{2}^{}\).

Semi-Synthetic ExperimentsFor our semi-synthetic experiments, we use MIMIC-III, a identified dataset of patients admitted to critical care units at a large tertiary care hospital (Johnson et al., 2016). See Appendices D and G for detailed pre-processing steps, variables, and DGP. To summarize, we use the following real data from MIMIC-III in our DGP: ICD-9 code diagnoses, demographic information, and unstructured text notes. We choose the four diagnoses for oracle \(U\) that had the best F1 scores for a supervised bag-of-words logistic regression classifier (see Appendix E for full results): _atrial fibrillation_ (Afib), _congestive heart failure_ (Heart), _coronary atherosclerosis of the native coronary artery_ (A-Sis), and _hypertension_ (Hypertension). To avoid Gotcha #2, we explicitly exclude discharge summaries, which are post-treatment. In each experiment, we choose two out of the following four note categories as \(_{1}^{}\) and \(_{2}^{}\): electrocardiogram (ECG), echocardiogram (Echo), Radiology, and Nursing notes. We hypothesize that notes written about different aspects of clinical care and by different providers will likely satisfy \(_{1}^{}_{2}^{} U,\) since each individual will write a conditionally independent realization of the patient's status. In Appendix I, we find distinct unigram vocabularies between the note sets that pass our falsification heuristic, lending preliminary evidence to this assumption of conditional independence. Consistent with the DAG in Figure 3(d), we then synthetically generate binary \(A\) and continuous \(Y\).

For our zero-shot models, we use FLAN-T5 XXL (Flan) Chung et al. (2024) and OLMo-7B-Instruct (OLMo) Groeneveld et al. (2024), both "open" instruction-tuned large language models. In Appendix F, we elaborate on our choice of these models. Following Ziems et al. (2024), we use the prompt template _"Context: {\(^{}\)} \(\)nIs it likely the patient has {U}?\(\)nConstraint: Even if you are uncertain, you must pick either "Yes" or "No" without using any other words.'_ We assign \(1\) when the output from Flan or OLMo contains _'Yes'_ and \(0\) otherwise. P1M uses Flan for both proxies \(W\) and \(Z\) while P2M uses Flan for \(W\) and OLMo for \(Z\). We compare P1M and P2M to a baseline that uses the inferred proxy \(W\) from Flan directly in a backdoor adjustment formula, corresponding to Gotcha #1.

Estimation of Proximal g-formulaFor all experiments, we estimate the ACE by using the inferred \(W\) and \(Z\) in a two-stage linear regression estimator for the proximal g-formula provided by Tchetgen Tchetgen et al.. Although the linearity assumption is restrictive, it allows us to focus on evaluating the efficacy of our proposed method for inferring text-based proxies as opposed to complications with non-linear proximal estimation (Mastouri et al., 2021). Briefly, we first fit a linear regression \([W|A,Z,]\). Next, we infer \(\), continuous probabilistic predictions for \(W\), using the fitted model. For the second stage, we fit a linear model for \([Y|A,,]\). The coefficient for \(A\) in this second linear model is the estimated ACE. We calculate 95% confidence intervals for the ACE via the bootstrap percentile method (Wasserman, 2004). See Appendix F for additional implementation details (e.g., addressing class imbalance and sample splitting).

ResultsTable 1 has synthetic results, and Tables 2 and Figure 4 have selected results for the semi-synthetic experiments. See Appendices E, H, and J for additional results. In short, our empirical results corroborate preference for Algorithm 1 over the baseline.

First, we discuss the "gotcha" methods we showed to be theoretically incorrect in Section 3. Regarding Gotcha #1, Figure 4 shows that, across all settings of \(U\), using the inferred \(W\) directly in the backdoor adjustment formula results in estimates with large bias. Regarding Gotcha #3 in the fully synthetic experiments, using the same realization \(_{1}^{}\) results in high bias--\(0.130\) and \(0.107\) for P1M and P2M, respectively, in Table 1--and CIs that do not cover the true ACE. Regarding Gotcha #4, as in Proposition 4, using a single zero-shot model (P1M) results in low bias in the idealized setting of the synthetic DGP (Table 1; first row). However, using real clinical notes and Flan, the second columns of Figure 4 show that P1M produces estimates with higher bias compared to P2M across all three settings. We hypothesize this could be due to Flan over-relying on its pre-training and thus making similar predictions regardless of the clinical note.

For all experiments we set \(_{}=2\). With this setting, we find low bias and good CI coverage for the two settings that pass our heuristic (P2M for \(U=\)s with Echo and Radiology notes and P2M for \(U=\) with Echo and Nursing notes) and biased estimates in other cases. Although the confidence intervals sometimes cover the true ACE in settings that fail the heuristic, the interval is skewed and appears will asymptotically converge to a biased value. This gives us confidence that Algorithm 1 appropriately flags when to stop or proceed.

Understanding _why_ the odds ratio heuristic fails for a specific application requires oracle data. However, it may be helpful for analysts to reason about scenarios that influence this failure, such as when \(\) and \(U\) are not a sufficient backdoor adjustment set; \(_{1}^{}=}_{2}^{} U, \); or \(Z\) and \(W\) are poorly predictive of \(U\). Using oracle data, we examined these scenarios for a few settings of our semi-synthetic experiments. For example, Figure 4 (right-most panel) shows that \(U\)=A-Sis with nursing and radiology notes fails the odds ratio heuristic. We qualitatively examined samples of the clinical notes and found some had unmeasured confounding, e.g., a patient had lung cancer--a variable that was not in our \(\) or \(U\)--that influenced descriptions in both \(_{1}^{}\) and \(_{2}^{}\). Figure 9 (bottom right-most panel) shows that for \(U\)=hypertension with echocardiogram (echo) and nursing notes also fails the odds ratio heuristic. However, we hypothesize this is due not to text dependence (see Table 4), but rather that the proxies \(W\) and \(Z\) have accuracies of 0.49 and 0.52 respectively (when evaluated against oracle \(U\)), which is lower accuracy than predicting the majority class (Table 21). We leave to future work disaggregating the odds ratio heuristic into tests for these specific scenarios.

## 6 Conclusion, Limitations, and Future Work

In this work we proposed a novel causal inference method for estimating causal effects in observational studies when a confounding variable is completely unobserved but unstructured text data is available to infer potential proxies. Our method uses distinct instances of pre-treatment text data, infers two proxies using two zero-shot models on the instances, and applies these proxies in the proximal g-formula. We have shown why one should prefer our method to alternatives, both in theory and in the empirical results of synthetic and semi-synthetic experiments.

If the conditions we present hold, the estimates will be unbiased. Further, as the underlying predictive accuracy of \(Z\) and \(W\) improve, the confidence interval width will narrow. Thus, we see promise in work improving the zero-shot performance of LLMs. Future work could possibly combine proxies from a greater number of zero shot models, i.e. inspired by work in Yang et al. (2024).

Although we are careful to infer valid proxies by design, it is generally impossible to empirically test whether conditions for proximal causal inference are fulfilled. To address this, we proposed an odds ratio falsification heuristic test, but acceptable values of \(_{}\) depend on the domain. If these are set incorrectly, our method could result in false positives (a setting for which the CI does not cover the true parameter and the estimated ACE is biased). In addition, our approach depends on the availability of pre-treatment text data and metadata to split text into independent pieces, which are not available across all applied settings.

Although we use a clinical setting as our running example, our method is applicable to many other domains where it is infeasible to obtain ground-truth \(U\) labels due to privacy constraints or annotation costs, e.g., social media or education studies with private messaging data or sensitive student coursework. Other directions for future work include incorporating non-linear proximal estimators (including ones that use alternative assumptions to the completeness condition in (P4) (Kallus et al., 2021)), expanding beyond text to proxies learned from other modalities (see Knox et al. (2022)), providing more guidance for setting \(_{}\) in our odds ratio heuristic, extending our method to incorporate categorical \(U,W,Z\), and using soft probabilistic outputs from the zero-shot classifiers.