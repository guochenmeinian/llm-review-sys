# PURE: Prompt Evolution with Graph ODE for

Out-of-distribution Fluid Dynamics Modeling

 Hao Wu\({}^{1,3}\), Changhu Wang\({}^{2}\), Fan Xu\({}^{1}\), Jinbao Xue\({}^{3}\),

**Chong Chen\({}^{4}\), Xian-Sheng Hua\({}^{4}\), Xiao Luo\({}^{2,}\)\({}^{*}\)**

\({}^{1}\)University of Science and Technology of China, \({}^{2}\)University of California, Los Angeles,

wuhao2022@mail.ustc.edu.cn, wangch156@g.ucla.edu, markxu@mail.ustc.edu.cn markxu@mail.ustc.edu.cn,jinbaoxue@tencent.com, chenchong.cz@gmail.com, huaxiansheng@gmail.com, xiaoluo@cs.ucla.edu

Corresponding author.

###### Abstract

This work studies the problem of out-of-distribution fluid dynamics modeling. Previous works usually design effective neural operators to learn from mesh-based data structures. However, in real-world applications, they would suffer from distribution shifts from the variance of system parameters and temporal evolution of the dynamical system. In this paper, we propose a novel approach named Prompt Evolution with Graph ODE (PURE) for out-of-distribution fluid dynamics modeling. The core of our PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios. In particular, our PURE first learns from historical observations and system parameters in the frequency domain to explore multi-view context information, which could effectively initialize prompt embeddings. More importantly, we incorporate the interpolation of observation sequences into a graph ODE, which can capture the temporal evolution of prompt embeddings for model adaptation. These time-evolving prompt embeddings are then incorporated into basic forecasting models to overcome temporal distribution shifts. We also minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of our model to different distributions. Extensive experiments on various benchmark datasets validate the superiority of the proposed PURE in comparison to various baselines. Our codes are available at https://github.com/easylearningscores/PURE_main.

## 1 Introduction

Fluid dynamics  is a critical area in the field of mechanics and computational fluid dynamics has emerged as a powerful tool to understanding fluid flow . Recently, various machine learning approaches have been widely adopted to solve the problem in a data-driven manner , which can achieve high efficiency in comparison to previous traditional numerical solvers. Moreover, they enjoy strong applicability when the underlying rules are not explicit, such as real-world weather forecasting  and disease transmission .

In literature, existing data-driven fluid dynamics modeling approaches can be roughly divided into grid-based approaches  and geometry-based approaches . Grid-based approaches construct regular meshes and then utilize neural operators to explore spatio-temporal relationships. In contrast, geometry-based approaches focus on irregular point clouds and then utilize graph neural networks (GNNs)  to learn from the interaction between mesh points.

Despite their great success, existing approaches [88; 25] generally assume that training and test data share the same data distribution [59; 66; 65; 47], which could be not the case in real-world applications. In particular, there are two typical types of distribution shifts in dynamical systems, i.e., parameter-based shifts, and temporal distribution shifts. Firstly, different dynamical systems could involve different parameters in underlying rules, such as coefficients in PDEs and pressures in fluid systems [3; 65]. Secondly, during long-term auto-regressive forecasting, the input data distribution could vary hugely during the temporal evolution . As in previous works [22; 33], machine learning approaches usually suffer from huge performance degradation when it comes to distribution shifts. Therefore, in this paper, we focus on the problem of out-of-distribution fluid dynamics modeling to enhance the performance under potential distribution shifts.

In this paper, we propose a new approach named Prompt Evolution with Graph ODE (PURE) for out-of-distribution fluid dynamics modeling. The high-level idea of our proposed PURE is to adapt well-trained forecasting approaches to different out-of-distribution scenarios by learning time-evolving prompts [51; 84; 9]. To begin, we extract multi-view context signals from both historical observations and system parameters in the frequency domain using the attention mechanism, which can effectively initialize prompt embeddings under parameter-based shifts. More importantly, to capture temporal distribution shifts, we combine the interpolation of observation sequences into a graph ODE framework, which can utilize the interaction between prompt embeddings and observation embeddings for high-quality time-evolving prompt embeddings. Then, we concatenate our prompt embeddings and observation embeddings for model adaptation and enhance the robustness of our PURE to distribution variance by minimizing their mutual information using adversarial learning. Extensive experiments on a range of fluid dynamics datasets validate the superiority of the proposed PURE in comparison to various state-of-the-art approaches.

In summary, the contribution of our paper can be summarized as follows: (1) _Problem Connection._ We are the _first_ to connect prompt learning with dynamical system modeling to solve the issue of out-of-distribution shifts. (2) _Novel Methodology._ Our PURE first learns from historical observations and system parameters to initialize prompt embeddings and then adopts a graph ODE with the interpolation of observation sequences to capture their continuous evolution for model adaptation under out-of-distribution shifts. (3) _Superior Performance._ Comprehensive experiments validate the effectiveness of our PURE in different challenging settings.

## 2 Problem Setup

Given a fluid dynamical system, we have \(N\) sensors within the domain \(\), with their locations denoted as \(_{1},,_{N}\), where \(_{i}^{d_{i}}\). The observations at time step \(t\) are represented as \(_{1}^{t},,_{N}^{t}\), where \(_{i}^{t}^{d_{o}}\) and \(d_{o}\) indicates the number of observation channels. Dynamical systems are governed by underlying system rules, such as PDEs with coefficient \(\). Variations in system parameters may lead to different environments, potentially resulting in distribution shifts [54; 80; 6; 27]. In our study, we are provided with historical observation sequences \(\{_{i}^{1:T_{0}}\}_{i=1}^{N}\) and physical parameters \(\) (e.g., coefficients in the PDEs). Our goal is to predict the future observations of each sensor \(_{i}^{T_{0}+1:T_{0}+T}\). In dynamical systems, the out-of-distribution problem examines model performance when predicting under unseen parameter distributions or environments. Let \(^{t}=[_{1}^{t},,_{N}^{t}]\), these systems evolve according to \(}{dt}=F(,)\), where \(\) represents the observations and \(\) denotes the system parameters. When \( P()\), the state trajectory \(^{1:T_{0}}\) follows the distribution \(P(^{1:T_{0}}|)\). Assume we learn a learned mapping function \(f\) from \(^{1:T_{0}}\) to \(^{T_{0}+1:T_{0}+T}\), i.e., \(^{T_{0}+1:T_{0}+T}=f(^{1:T_{0}})\) and there could be different distributions across training and test datasets, i.e., \(P_{}() P_{}()\), which results in \(P_{}(^{1:T_{0}}) P_{}( ^{1:T_{0}})\). Moreover, when conducting rollout prediction, we are required to feed the output back to the model, i.e., \(^{T_{start}:T_{start}+T-1}=f(^{T_{start}-T_{0}:T_{start}-1})\), with \(P(^{1:T_{0}}|) P(^{T_{start}-T_{0}:T_{start}-1}|,T_{start})\).

## 3 The Proposed PURE

### Motivation and Framework Overview

This paper addresses the challenge of out-of-distribution fluid system modeling, which is complicated by parameter-based and temporal distribution shifts. Specifically, our function \(f()\) can suffer from a serious distribution shift result from different \(\) and \(T_{start}\), i.e., \(P(_{input}|,T_{start})\). To reduce the impact of distribution shift, we aim to learn invariant observation embeddings \(^{t}\) to different environments, i.e., \(\) and \(T_{start}\) for better generalization and utilize prompt embeddings \(^{t}\) to indicate the current environment for final prediction. In formulation, we have:

\[^{t}^{t},u_{output}=([^{t},^{t}]).\] (1)

The first term ensures the invariance of observation embeddings by decoupling observation embeddings and prompt embeddings. The second term aims to combine both two embeddings to generate the future predictions. Therefore, we propose a novel approach named PURE as:

\[^{t}=(_{input}),^{0}=(_{input}),^{t}=(^{0},t).\] (2)

where a basic model is adopted to generate observation, and we adopt context mining and graph ODE to learn time-varying prompt embeddings. Given a basic forecasting model (Eqn. 2), our PURE contains three key modules: (1) _Multi-view Context Exploration_, which explores spatio-temporal data using both the attention mechanism and the frequency domain to initialize prompt embeddings (Eqn. 2). (2) _Time-evolving Prompt Learning_, which incorporates the interpolation of observation sequences into a graph ODE to learn the evolution of prompt embeddings (Eqn. 2). (3) _Model Adaptation with Prompt Embeddings_, which leverages the time-evolving prompts to mitigate the temporal distribution shifts in fluid dynamics models (Eqn. 1). More details are in Figure 1.

### Multi-view Context Exploration from Spatio-temporal Data

The main idea of our PURE is to utilize prompt learning to solve the issue of out-of-distribution shifts [54; 80; 6; 27] in fluid dynamical systems. Prompt learning [51; 84; 9] is an effective manner to adapt language models to various downstream tasks. In our scenarios, we aim to learn from both historical spatio-temporal information and system parameters to initialize our prompt embeddings, which can effectively solve the parameter-based distribution shifts. Here, we first follow the attention mechanism [69; 86; 10; 50] to reconstruct the field value and then adopt the Fourier neural operator  to integrate multi-view context information.

In particular, given each location, we map each location \(_{i}\) and each initial observation \(_{i}^{0}\) into a position embedding \(_{i}\) and an observation embedding \(_{i}\) using two feed-forwarding networks (FFNs) \(^{PE}()\) and \(^{OE}()\), and then aggregate \(_{i}\) and \(_{i}\) using the Hadamard product followed by stacking \(L\) self-attention blocks for representation learning. In formulation,

\[_{i}=_{i}_{i},^{l+1}=^{SA,(l)}(^{l}),\] (3)

Figure 1: Overview of the PURE framework.

where \(\) denotes the Hadamard product, \(^{0}\) is constructed by stacking \(\{_{i}\}_{i=1}^{N}\), and \(^{SA,(l)}\) is the self-attention block at the layer \(l\). Afterward, we adopt the attention mechanism  to retrieve the representations for each query position \(_{q}\) as:

\[^{q}=^{Q}^{PE}(^{q})]^{T} [^{K}^{L}]}{}^{V}^{L},\] (4)

where \(^{*}\) is a learnable weight matrix for feature transformation and \(d\) is the hidden dimension. By retrieving the representations at each regular grid, we generate the 3D representation tensor \(\). Each tensor would be concatenated with the parameter embedding \(^{p}=^{PA}()\) for multi-view information integration, which results in the final tensor \(}\). Then, we utilize the frequency domain for representation enhancement to generate a prompt tensor \(\). Here, we first transfer the tensor into the frequency domain using a Fast Fourier Transformer (FFT) operator  and then adopt an FFN for feature transformation. Lastly, an inverse Fast Fourier Transformer (iFFT) operator is adopted to convert the features back to the spatial domain. Formally,

\[=(((}))),\] (5)

where \(()\) and \(()\) denote the FFT and iFFT operators, respectively. Since the input of spatio-temporal models would be irregular, we flatten the tensor \(\), and retrieve prompts for each sensor from the prompt tensor using:

\[_{i}^{0}=^{Q^{}}^{PE}( {x}_{i})]^{T}[^{K^{}}flatten()]}{} ^{V^{}}flatten(),\] (6)

where \(flatten()\) is a flattening operator to transform 3D tensors to 2D matrices. Through the frame reconstruction, we can extract important spatio-temporal signals from the frequency domain, which is effective in initializing the prompt embedding for each sensor.

### Time-evolving Prompt Learning with Graph ODE

To capture temporal distribution shifts within one system, static prompt embeddings  from context exploration are far from satisfactory. Our solution is to obtain continuous time-evolving prompts at any timestamp. To achieve this, we view the output of Eqn. 6 as the initial prompt embeddings and then incorporate the attention mechanism into a continuous graph ODE, which combines the interpolations of observations with the graph structure to learn the evolution of prompt embeddings.

In particular, given the initial prompt embeddings, we introduce two functions \(_{a}()\) and \(_{r}()\) for relation mining and feature aggregation. \(_{r}()\) calculates the interaction between the centroid node and each of its neighboring nodes and \(_{a}()\) aggregates all the neighborhood interactions to determine the evolution. Therefore, a graph ODE can be formulated by the following formulation:

\[_{i}^{t}}{dt}=_{a}(_{j^{t}(i)}_{r}([ _{i}^{t},_{j}^{t}])),\] (7)

where \(^{t}(i)\) collects the sensors from the neighbours of \(i\) at timestamp \(t\). However, Eqn. 7 neglects observations themselves during evolution, which are directly related to temporal distribution shifts in dynamical systems. Thus, it could generate suboptimal prompt embeddings. To tackle the issue, we conduct the interpolations of observation sequence \(_{i}^{1:T_{0}}\), which results in \(_{i}^{t}\) at any timestamp. Then, we incorporate them into our graph ODE using the attention mechanism by rewriting Eqn. 7 into:

\[_{i}^{t}}{dt}=_{a}(_{j^{t}(i)}}^{Q}_{i}^{t}]^{T}[}^{K} _{j}^{t}]}{}_{r}([_{i}^{t},_{j}^{t} ])).\] (8)

where \(}^{Q}\) and \(}^{K}\) are two matrices to generate the query and key, respectively. Here, we utilize the prompt embeddings and interpolated observations to serve as the query and the key. In this way, we effectively model their interaction to adjust the derivative in the graph ODE, which can help generate proper prompt embeddings for our model adaptation under temporal distribution shifts.

### Model Adaptation with Prompt Embeddings

Finally, we incorporate our prompt embedding into our basic spatio-temporal forecasting model, and then introduce the optimization objective for the end-to-end training.

**Basic Forecasting Model.** Our time-evolving prompt embeddings can be easily incorporated into any spatio-temporal forecasting model. To make the best of our efficacy, we utilize a simple yet powerful basic model as our default model and also explore the performance of our PURE on more existing forecasting models. The input of our model is the observations of sensors from between the interval \([1,T_{0}]\) and output the predictions in \([T_{0}+1,T_{0}+T]\), i.e., \(^{1:T_{0}}^{T_{0},T_{0}+T}\) where \(^{}\) is stacked by \(^{}_{i}\). In particular, our basic model first generates the embeddings of different observations, and then reconstructs the irregular observations into frames on grids using the reconstruction modules in Sec. 3.2. More importantly, we introduce two parallel modules, i.e., a Fourier neural operator  and a ViT-based convolution  and extract complementary feature maps , which would be fused to generate the predicted frames in the future. More details of our basis forecasting model can be found in Appendix B. Additionally, we also use other basic models.

**Adaptation and Optimization.** Note that we generate observation embeddings in our basic module, i.e., \(^{t}_{i}=^{enc}(^{t}_{i})\). To adapt our model under distribution shifts, we concatenate the observation embeddings and prompt embeddings into updated embeddings \(}^{t}_{i}\) as follows:

\[}^{t}_{i}=[^{t}_{i},^{t}_{i}],\] (9)

which will be fed into the subsequent modules in the basic model. To optimize the whole framework, we first minimize the mean squared error (MSE) between the predicted observation and the ground truth as follows:

\[_{MSE}=_{t=T_{0}+1}^{T_{0}+T}||}^{t}-^{t}||,\] (10)

where \(}^{t}\) denotes our predicted observation for every node and \(^{t}\) denotes the ground truth observations. Moreover, to enhance the invariance of our model to different scenarios, we turn to invariant learning [72; 42; 77] to decouple various prompt embeddings and observation embeddings, which promotes the observation embeddings to be less sensible to different distributions. To achieve this, we minimize the mutual information between observation embeddings and observation embeddings, i.e., \(I(^{t}_{i};^{t}_{i})\). In our work, we adopt a Jensen-Shannon mutual information estimator [40; 53]\(T_{}(,)\) where \(\) denotes the parameters to estimate their mutual information. Then, we collect all the corresponding pairs of \((^{t}_{i},^{t}_{i})\) using \(\) and all the possible pairs of \((^{t}_{i},^{t}_{i})\) using \(\). The adversarial learning objective can be written as:

\[_{MI}=max_{^{}}\{|}_{( ^{t}_{i},^{t}_{i})}sp(-T_{^{}}(^{t}_{i },^{t}_{i}))+|||}_{(^{t}_{i},^{t}_{j})}-sp(-T_{^{}}(^{t}_{i}, ^{t}_{j}))\},\] (11)

in which \(sp()=(1+e^{})\) represents the softplus function. In summary, the overall objective can be written as:

\[=_{MSE}+_{MI},\] (12)

where \(\) is a coefficient to balance two loss objectives. The algorithm is summarized in Appendix D.

### Theoretical Analysis

In this part, we provide a theoretical analysis to demonstrate how PURE works. Our focus is primarily on theoretically showing the necessity of incorporating the observations themselves during evolution. For simplicity of analysis, we assume that Eqn. 7 can be rewritten as:

\[^{t}_{i}}{dt}=^{t}(i))}_{j S^{t}( i)}(M_{1}^{t}_{i}+M_{2}^{t}_{j})=M_{1}^{t}_{i}+^{t}(i))}_{j^{t}(i)}M_{2}^{t}_{j},\] (13)

where \(\#()\) calculates the size of the set. For the sake of simplicity in the proof, we assume that \(^{t}_{i}\) is one-dimensional and consider only the ODE above for \(i\) (not the entire system of ODEs). Then, Eqn. 13 can be rewritten as:

\[_{i}}{dt}=^{t}(i))}_{j^{ t}(i)}(M_{1}z^{t}_{i}+M_{2}z^{t}_{j})=M_{1}z^{t}_{i}+b(t),\] (14)where \(b(t)\) is a function. To characterize temporal distribution shifts, we assume that a portion of the corresponding true \(z_{j}^{t}\) has a constant shift. With the potential environmental change, Eqn. 13 can be rewritten as:

\[^{t}}{dt}=^{t}(i))}_{j^{t}( i)}(M_{1}z_{i}^{t}+M_{2}z_{j}^{t})=M_{1}z_{i}^{t}+b^{}(t),\] (15)

where \(|b(t)-b^{}(t)| c_{0}\), suggesting the constant shift \(c_{0}\).

**Theorem 3.1**.: _Given the following ODEs in \(\),_

\[=M_{1}x+b(t),&x(0)=x_{0},\\ =M_{1}y+b^{}(t),&y(0)=x_{0},\] (16)

_where \(|b(t)-b^{}(t)| c_{0}\), there exists a positive constant \(c_{1}\) such that_

\[|x(t)-y(t)| c_{1}(e^{M_{1}t}-1),t>0.\] (17)

The proof of Theorem 3.1 can be found in Appendix A. Theorem 3.1 suggests that even with the simplest one-dimensional linear ODE, significant differences in the solutions will arise if temporal distribution shifts are neglected. Next, we will focus on how Eqn. 8 addresses this issue. In this case, we assume that

\[_{r}([z_{i}^{t},z_{j}^{t}])=M_{1}z_{i}^{t}+M_{2}z_{j}^{t}.\] (18)

Then, Eqn. 7 can be written as:

\[^{t}}{dt}=_{}\!(\!_{j^{t}(i)}(M_ {1}z_{i}^{t}+M_{2}z_{j}^{t})\!)=_{}\!(\!M_{1}z_{i}^{t}+ ^{t}(i))}_{j^{t}(i)}M_{2}z_{j}^{t} \!)=_{}\!(\!M_{1}z_{i}^{t}+b(t)\!)\!,\] (19)

where

\[b(t)=^{t}(i))}_{j^{t}(i)}M_{2}z_{j}^ {t}.\] (20)

Similarly, Eqn. 8 can be written as:

\[^{t}}{dt}=_{}\!(M_{1}z_{i}^{t}+b^{}(t))\!,\] (21)

where

\[b^{}(t)=_{j^{t}(i)}\!(}^{Q}_{i}^{t}]^{T}[}^{K}_{j}^{t}]} {}) M_{2}z_{j}^{t}.\] (22)

For simplicity of notation, we omit the superscript \(i\). Write \(F(z,t)=_{}\!(M_{1}z+b(t))\) and \(G(z,t)=_{}\!(M_{1}z^{t}+b^{}(t))\). Then, we have the following theorem with the proof in Appendix A.

**Theorem 3.2**.: _Assume that the attention mechanism satisfies that \(|b^{}(t)-b(t)|\), for all \(t>0\), and the function \(_{}\) is \(L\)-Lipschitz. Given the following ODEs in \(\),_

\[=_{}(M_{1}x+b(t))=F(x,t),&x(0)=x_{0},\\ =_{}(M_{1}y+b^{}(t))=G(y,t),&y(0)=x_{0},\] (23)

_there exists two constants \(c_{2}\) and \(c_{3}\) such that_

\[|x(t)-y(t)| c_{2}(e^{c_{3}t}-1),t>0.\] (24)

Theorem 3.2 shows that as long as the attention mechanism is sufficiently good, we can approximate the true ODE with arbitrary precision using Eqn. 8, even in the presence of environmental change.

## 4 Experiment

### Experimental Settings

**Benchmarks.** We study Benchmarks from three domains, as shown in Table 5. \(\)**Computational Fluid Dynamics.** We use Prometheus  and follow the original setup for environment segmentation. \(\)**Real-world Data.** We employ the ERA5 , using different combinations of variables as the environment. In detail, we use ERA5 data with variables such as surface pressure (Sp), sea surface temperature (SST), sea surface height (SSH), and two-meter temperature (T2m) to predict temperature. \(\)**Partial Differential Equations.** The 2D Navier-Stokes equations  describe fluid motion, with the primary variable being the viscosity coefficient \(\), which quantifies internal friction in the fluid, simulating vorticity values under ten different viscosity coefficients. The spherical shallow water equations  simulate large-scale atmospheric and oceanic fluid motion on Earth's surface, also with viscosity coefficient \(\) as the main variable, involving tangential vorticity (\(w\)) and fluid thickness (\(h\)) on a spherical surface. The 3D reaction-diffusion equations describe the diffusion and reaction of chemicals in space , with the primary variable being the diffusion coefficient \(D\), representing the rate of chemical diffusion in space, including \(u,v\) velocity components. More details see Appendix E.

**Baselines.** We select representative models from three domains as baselines. \(\)**Visual Backbone Networks.** We include ResNet , U-Net , Vision Transformer(ViT) , and Swin Transformer(SWINT) . \(\)**Neural Operator Architectures.** We cover FNO , UNO , CNO , and NMO . \(\)**Graph-ODE Architectures.** We feature CG-ODE , and DGODE .

**Tasks.** We evaluate model performance for various prediction tasks through the following scenarios and use MSE as metrics. The specific tasks are as follows:

\(\)**Generalization Experiments:**\(\) Out-of-Distribution Generalization: We train the model In-Domain environment and test it in Adaptation environment to verify its generalization ability. \(\)**Spatial Generalization & Temporal Generalization: In the Prometheus, we train the model at 75% sparsity and test it at \(s\{5\%,25\%,50\%,75\%\}\) sparsity. The experiment evaluates performance with equal input and output lengths ( \(In_{t}\)) and with output 10 times the input length (\(Out_{t}\)).

\(\)**Zero-shot Experiments.** Specifically, we follow the setup from  and conduct two experiments. In the Prometheus, we train the model In-Domain environments \(b_{1},b_{2},,b_{20}\) and evaluate its generalization ability in new environments \(b_{11},b_{12}\), using MSE as the evaluation metric. In the

    &  \\   &  &  &  &  &  \\   & \(w\)/o OOD & \(w\)/ OOD & \(w\)/ OOD & \(w\)/ OOD & \(w\)/ OOD & \(w\)/ OOD & \(w\)/ OOD & \(w\)/ OOD & \(w\)/ OOD \\  U-Net  & 0.0931 & 0.1067 & 0.1982 & 0.2243 & 0.0083 & 0.0087 & 0.0148 & 0.0183 & 0.0843 & 0.0932 \\ ResNet  & 0.0674 & 0.0696 & 0.1823 & 0.2301 & 0.0081 & 0.0192 & 0.0151 & 0.0186 & 0.0921 & 0.0977 \\ ViT  & 0.0632 & 0.0691 & 0.2824 & 0.2621 & 0.0065 & 0.0072 & 0.0157 & 0.0192 & 0.0762 & 0.0786 \\ SwinT  & 0.0652 & 0.0729 & 0.2248 & 0.2554 & 0.0062 & 0.0068 & 0.0155 & 0.0190 & 0.0782 & 0.0832 \\  FNO  & **0.0447** & **0.0506** & 0.1556 & 0.1712 & 0.0038 & 0.0045 & 0.0132 & 0.0179 & 0.7233 & 0.0821 \\ UNO  & 0.0532 & 0.0643 & 0.1764 & 0.1984 & 0.0034 & 0.0041 & **0.0121** & 0.0164 & 0.6652 & 0.7621 \\ CNO  & 0.0542 & 0.0655 & 0.1473 & 0.1522 & 0.0037 & 0.0038 & 0.0145 & 0.0182 & 0.5243 & 0.7821 \\ NMO  & 0.0397 & 0.0483 & **0.1021** & **0.1032** & **0.0026** & 0.0031 & 0.0129 & **0.0168** & **0.0432** & **0.0563** \\  CGODE  & 0.0761 & 0.0843 & 0.2035 & 0.2243 & **0.0873** & **0.0987** & **0.8371** & **0.9261** & **0.8721** & 0.9872 \\ DGODE  & **0.0344** & **0.0359** & **0.0805** & 0.0925 & **0.0022** & **0.0028** & 0.0122 & 0.0156 & 0.0543 & 0.0635 \\  Ours + Pure & **0.0323** & **0.0328** & **0.0752** & **0.0763** & **0.0022** & **0.0024** & **0.0119** & **0.0127** & **0.0398** & **0.0401** \\ Promotion & 6.10\% & 8.63\% & 6.58\% & 26.07\% & 0.00\% & 16.67\% & 1.65\% & 22.56\% & 7.87\% & 28.77\% \\   

Table 1: We compare our studyâ€™s performance with 10 baselines. We magnify the MSE of 3D-Reaction-Diffusion by 100 times. **Green Yellow Red** mean best, second, worst MSE.

    &  \\   &  &  &  &  &  \\   & Ori & +PURE & Ori & +PURE & Ori & +PURE & Ori & +PURE \\  ResNet  & 0.0674 & 0.0542 & 0.1823 & 0.1492 & 0.0081 & 0.0067 & 0.0151 & 0.0141 & 0.0921 & 0.0896 \\ NMO  & 0.0397 & 0.0281 & 0.1021 & 0.0876 & 0.0026 & 0.0012 & 0.0129 & 0.0123 & 0.0432 & 0.0389 \\ DGODE  & 0.0344 & **0.0201** & 0.0805 & **0.0792** & 0.0022 & **0.0020** & 0.0122 & **0.0110** & 0.0543 & **0.0462** \\   

Table 2: This table shows the performance of the PURE framework across different benchmarks.

Navier-Stokes equations, we train the model on a \(64 64 20\) dataset and evaluate it on a higher resolution \(512 512 20\) dataset, focusing on the fluid dynamics details in the last five time steps and the handling of complex flow patterns and boundary layers.

### Generalization Experiment Results

In this section, we focus on the issue of generalization. Based on our experimental findings, we make the following observations. **Out-of-Distribution Generalization.** The results as shown in Table 1. On the Prometheus dataset, PURE outperforms all benchmark models with an MSE of 0.0323 in-distribution and 0.0328 OOD. It improves over the second-best model, DGODE (MSE 0.0344 in-distribution, 0.0359 OOD), by 6.10% and 8.63%, respectively. On the Navier-Stokes dataset, PURE achieves the best performance with an MSE of 0.0752 in-distribution and 0.0763 OOD, improving by 6.58% and 26.07% over the second-best model. On the Spherical-SWE dataset, PURE has an MSE of 0.0022 both in-distribution and OOD, which is 41.46% better than the second-best model. Additionally, in Table 2, the performance of various benchmark models significantly improves when using PURE, in summary, the PURE framework performs excellently in handling OOD fluid dynamics modeling.

**Spatial & Temporal Generalization.** Table 3 shows that PURE excels in the Prometheus benchmark, notably reducing errors with sparse data. For instance, in the 75% sparsity test, U-Net's MSE drops from 0.2273 to 0.1998, and FNO from 0.2109 to 0.1582. Figure 2 highlights PURE's lower errors in temperature and smoke fields compared to DGODE, FNO, and U-Net, especially in red-boxed areas, showcasing its advantage in capturing complex dynamics. Additionally, PURE performs consistently across different prediction lengths; for example, U-Net's MSE decreases from 0.1847 to 0.1622 for in-time prediction (In-t) and from 0.2103 to 0.1854 for out-of-time prediction (Out-t). Overall, PURE excels with sparse and out-of-distribution data and enhances performance across prediction lengths, demonstrating strong spatial and temporal generalization.

**Visualization and Analysis.** Figure 3 compares the performance of different methods in fluid dynamics modeling, including the Prometheus dataset, Navier-Stokes equations, and the 3D Reaction-Diffusion Equation. In the Prometheus dataset, using PURE significantly reduces DGODE's prediction error, especially in complex dynamic regions. For the Navier-Stokes and Spherical Shallow Water equations, FNO and NMO models combined with PURE excel in capturing complex flow features. In the 3D Reaction-Diffusion Equation, DGODE with PURE significantly reduces prediction errors. Overall, PURE greatly enhances the prediction accuracy of models in fluid dynamics, allowing for better capture of complex dynamic evolution.

### Zero-shot Super-resolution and Environment Generalization

As shown in Figure 4, the PURE framework performs excellently in zero-shot super-resolution and environmental generalization experiments. In the Prometheus benchmark, the FNO model using

   sparsity & Test\(\) & \(s_{}=5\%\) & \(s_{}=25\%\) & \(s_{}=50\%\) & \(s_{}=75\%\) \\ Train \(\) & & In-t & Out-t & In-t & Out-t & In-t & Out-t & In-t & Out-t \\  }=75\%\)} & U-Net & 0.1847 & 0.2103 & 0.2345 & 0.2877 & 0.2654 & 0.3018 & 0.2273 & 0.3391 \\  & + PURE & 0.1622 & 0.1854 & 0.2079 & 0.2581 & 0.2365 & 0.2710 & 0.1998 & 0.3024 \\    & FNO & 0.0659 & 0.0872 & 0.0921 & 0.1232 & 0.1109 & 0.1821 & 0.2109 & 0.2455 \\  & + PURE & 0.0504 & 0.0654 & 0.0689 & 0.0946 & 0.0805 & 0.1417 & 0.1582 & 0.1883 \\    &  &  &  \\  Sparse Input & & & & & & & & & \\ Ground Truth & & & & & & & & & \\ Ours+PURE Error & & & & & & & & \\ DGODE Error & & & & & & & & \\ FNO Error & & & & & & & & \\ U-Net Error & & & & & & & & \\   

Table 3: Comparison of Spatial & Temporal Generalization in the Prometheus benchmark.

Figure 2: The top row shows the sparse input data used for predictions. The second row displays the true data for both fields. Red boxes highlight areas of significant error.

PURE significantly reduces prediction errors at different resolutions, with an MSE of 0.0471655 at a \(256 512\) resolution. For the Navier-Stokes equations, the FNO model combined with PURE significantly reduces prediction errors on high-resolution datasets and performs better in handling complex flow patterns and boundary layers, especially in capturing details in the last five time steps. Overall, PURE significantly improves model prediction accuracy and generalization ability in zero-shot super-resolution and environmental generalization tasks.

### Qualitative Analysis & Ablation Study

In this section, we evaluate the effectiveness of the PURE method and the importance of its components through qualitative analysis and ablation studies.

**Qualitative Analysis.** The Figure 5 uses t-SNE to perform clustering analysis on FNO prediction results. (a) represents the ground truth, (b) shows the predictions of the original FNO, and (c) shows the predictions of FNO combined with PURE. It is evident that the FNO combined with PURE is closer to the labels in clustering effect, with a more tightly distributed data point cluster. This demonstrates that PURE significantly improves the prediction accuracy of the FNO model.

**Ablation Study.** To evaluate the contribution and importance of each component in the proposed PURE, we design ablation experiments based on the default backbone model in this paper, and we use Relative L2 error as metric. Our model variants are as follows: **(1) PURE w/o Graph ODE**, we

Figure 4: _Left._ Zero-shot super-resolution and environment generalization experiments on Prometheus. _Right._ Zero-shot super-resolution experiments on the Navier-Stokes equations.

Figure 5: t-SNE clustering. (a) Ground truth, (b) FNO predictions, (c) FNO +PURE predictions.

   Variants & S-SWE \\  PURE w/o Graph ODE & 0.1882 \\ PURE w/o Interpolation & 0.1696 \\ PURE w/o MI & 0.1588 \\ PURE w/o FFT & 0.1602 \\
**PURE** & **0.1357** \\   

Table 4: Ablation Studies on S-SWE.

Figure 3: The Figure compares the performance of various methods in fluid dynamics modeling, including Prometheus, Navier-Stokes equations, and 3D reaction-diffusion equations. Models with PURE significantly reduce prediction errors in fluid dynamics, capturing complex dynamic evolutions.

remove the Graph ODE module and use static prompt embeddings. **(2) PURE w/o Interpolation**, we remove interpolation and use only Eqn. 7. **(3) PURE w/o MI**, we remove the mutual information minimization. **(4) PURE w/o FFT**, we remove the frequency domain enhancement (FFT). Table 4 shows the results of our ablation study. Removing Graph ODE, interpolation, mutual information minimization, and FFT results in Relative L2 errors of 0.1882, 0.1696, 0.1588, and 0.1602, respectively. The complete PURE method has an error of 0.1357. The results of the ablation experiments show that removing any component results in a decrease in predictive performance, further proving the critical role of these components in the PURE method. More results in Appendix H.

## 5 Conclusion

In this paper, we study a practical problem of out-of-distribution fluid dynamics modeling and propose a novel approach named PURE for this problem. The high-level idea of our PURE is to learn time-evolving prompts using graph ODEs, which can effectively adapt spatio-temporal forecasting models to different scenarios. Our PURE first initializes prompt embeddings by exploring multi-view context information from spatio-temporal data and system parameters. Then, PURE incorporates the interpolation of observation sequences into the graph ODE, which helps capture the temporal evolution of prompt embeddings to mitigate temporal distribution shifts. In future works, we will extend our PURE to more real-world scenarios such as rigid dynamics modeling and traffic flow forecasting.