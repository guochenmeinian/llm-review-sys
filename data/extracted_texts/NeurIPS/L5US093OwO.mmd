# Synthesizing Verified Mathematical Problems

Xuefeng Li\({}^{1,2}\) Yanheng He\({}^{1,2}\) Pengfei Liu\({}^{1,2,3}\)

\({}^{1}\)Shanghai Jiao Tong University

\({}^{2}\)Generative AI Research Lab \({}^{3}\)Shanghai AI Laboratory

Corresponding author.

###### Abstract

Mathematical data synthesis offers a potentially effective solution for enhancing the mathematical capabilities of large language models. However, existing methods either synthesize a large number of rationales based on existing questions, limiting the diversity of the questions, or rely on advanced proprietary models to directly generate new questions without verification, which cannot guarantee the correctness of the synthesized problems. This paper introduces a novel method, mathematical data synthesis through Algorithmic Abstraction, Implementation, and Contextualization (AIC), to synthesize new and verifiable mathematical problems. **AIC** abstracts mathematical problems into algorithms, implements these algorithms as code functions, and contextualizes them under different conditions to create new problems, which are then verified using code functions. Experimental results on multiple challenging mathematical benchmarks show that models fine-tuned on our synthesized data are superior to previous state-of-the-art models. Further experiments indicate that, when controlling for the same synthesizer, data synthesized using the AIC method is not only more accurate but also more effective at improving the model's mathematical abilities.

## 1 Introduction

Large language models (LLMs) have made significant strides, expanding from natural language processing to areas like code generation and creative writing . Their success stems from vast amounts of high-quality training data . As the availability of untapped high-quality data diminishes, LLM research faces a problem of data scarcity . Consequently, data synthesis, using generative models to create data similar to real data, offers a solution to this scarcity by supplementing real-world data . For synthetic data to be effective, it must maintain quality comparable to real data , particularly for mathematical data, which demands high logical consistency.

Research on enhancing LLMs' mathematical abilities through instruction tuning mainly follows two approaches. The first generates rationales for known mathematical problems using LLMs, filtering rationales based on the correctness of the final answer , though this limits the diversity of problems. The second approach uses advanced LLMs, like GPT-4 , to generate new questions and rationales , enhancing data diversity but risking accuracy without verification . Therefore, a method that generates new problems while ensuring their correctness is essential for producing diverse and accurate synthetic mathematical data.

In this paper, we propose Mathematical Data Synthesis via Algorithmic Abstraction, Implementation, and Contextualization (AIC). The central idea is that many mathematical problems can be addressed by abstract algorithms. By abstracting such algorithms from mathematical problems and contextualizing them, we can generate new mathematical questions and corresponding rationales. Moreover, abstract mathematical algorithms can be implemented using Python to verify the correctness of the synthesizeddata. As shown in Figure 1, the process of AIC is divided into two stages. **Stage1 Algorithm Abstraction and Implementation**: First, we use large language models (LLMs) as a synthesizer to abstract existing mathematical problems, which serve as seed data. Each entry includes the question, rationale, and final solution, and is transformed into a natural language algorithm. Next, we prompt the synthesizer to implement the algorithm as a Python code function and verify its correctness using a verification mechanism. **Stage2 Algorithm Contextualization**: The synthesizer contextualizes the natural language algorithm and generates a new mathematical problem. Then, the conditions of this newly generated problem are fed into the corresponding code function, and by checking if the final answer generated by the synthesizer aligns with the result of the code execution, thereby verifying the correctness of the synthesized problem.

We evaluated the model on several challenging mathematics benchmarks, including MATH , MathOdyssey , finding that data synthesized using AIC can significantly improve the performance of the synthesis model itself and is highly competitive compared to other methods. AIC not only has the capability to synthesize a large volume of high-quality mathematical data but also paves a new way for generating verifiable mathematical problems.

## 2 Methods

In this paper, we propose a data synthesis method for generating new mathematical problems with verified solutions. Our method comprises two stages. In the first stage, we employ an LLM as a synthesizer to abstract algorithms from existing mathematical problems. We then prompt the synthesizer to implement these algorithms as Python code functions and verify the code's correctness. In the second stage, the synthesizer contextualizes the algorithms into new mathematical problems, using the code functions to verify the correctness of the synthesized problems.

Let \(_{}=(q_{i},r_{i},a_{i})_{i=1}^{N}\) represent a typical mathematical training dataset, which serves as seed data, where \(q_{i}\), \(r_{i}\), \(a_{i}\) are question, rationale and final answer of the i-th problem. In addition to the seed data, the synthesis process utilizes large language models such as \(\) (e.g., Mixtral-8\(\)7B-Instruct , Llama3-70B-Instruct ) and a code interpreter \(\).

### Stage1: Algorithm Abstraction and Implementation

For each piece of data \(d_{i}=(q_{i},r_{i},a_{i})\), the LLM is asked to first analyze the question \(q_{i}\) and the rationale \(r_{i}\) to understand the core goals of the problem, identify the key operations and steps of reasoning, determine the sequential relationships between steps, and finally, identify the mathematical objects such as integers, series and expressions in question \(q_{i}\) that are independent of the core steps in reasoning, parameterizing them as placeholder variables. This way, a question \(q_{i}\) and its rationale \(r_{i}\) can be transformed into an **algorithm objective**\(o_{i}\) and an **algorithm process**\(p_{i}\). As show in Figure 2, We also prompt the synthesizer \(\) to generate additional information about the algorithm,

Figure 1: An Overview of AIC: (1) The synthesizer(LLM) abstracts mathematical problems (Seed Data) into natural language algorithms. (2) These algorithms are implemented in python by the synthesizer, with their correctness verified through a verification process. (3) Finally, the synthesizer contextualizes the abstract algorithms to generate new problems, employing a verification mechanism to ensure the correctness of the newly synthesized problem.

including **placeholder constraint**\(c_{i}\), which specify the placeholder variables' types, value ranges and relationships with other placeholder variables; **placeholder values**\(v_{i}\), which indicate the values of the placeholder variables in the original problem. Overall, we prompt the synthesizer \(\) to abstract a mathematical problem \(d_{i}=(q_{i},r_{i},a_{i})\) into a natural language algorithm \(_{i}\).

For each algorithm \(_{i}\), the synthesizer \(\) programs a code function \(f_{i}\) by Python, where the parameters are the placeholder variables, and the return value is the final result of the algorithm. Furthermore, to ensure the correctness of the algorithm and function, we propose a verification mechanism, using the original problem \(d_{i}=(q_{i},r_{i},a_{i})\) as a test case, inputting placeholder values \(v_{i}\) into the function \(f_{i}\), obtaining the function's return value and comparing it with the original answer \(a_{i}\), to filter out incorrect functions. If verification fails, we regenerate the algorithm for the problem, repeating until the algorithm passes the verification or reaches the maximum number of iterations \(I\).

### Stage2: Algorithm Contextualization

Contextualization aims to transform abstract natural language algorithms into specific mathematical problems. For any given algorithm \(_{i}=(o_{i},p_{i},v_{i},c_{i})\) and the corresponding code function \(f_{i}\), first, the synthesizer generates \(K\) possible values of **placeholder variable** based on the algorithm, denote as \(pv_{i}^{j},j=1,...,K\), which assigning specific mathematical objects that comply with the algorithm's constraints, which can also be referred to as a **context**. With the algorithm and placeholder variables in place, the synthesizer generates specific mathematical question \(q_{i}^{j}\), corresponding rationale \(r_{i}^{j}\), and final answer \(a_{i}^{j}\) for the algorithm \(_{i}\) in the current context \(pv^{j}\).

Unlike traditional synthesis algorithms that lack verification, here we can input the placeholder variable values \(pv_{i}^{j}\) into the code function \(f_{i}\) and execute it by code interpreter \(\), filtering out incorrect synthesized data by checking whether the execution result \(g_{i}^{j}\) matches the final answer \(a_{i}^{j}\) given by the synthesizer through algorithm contextualization. By generating numerous contexts, an algorithm can be contextualized into many mathematical instruction-tuning data points.

Figure 2: A more detailed overview of the synthesis pipeline.

Experimental Results

### Experiments Setting

DataWe use the training set from MATH  and a small subset of MAMmoTH2  including about 30,000 data points as seed data. subset includes approximately 30,000 data points. Data synthesis is conducted using Mixtral-8\(\)7B-Instruct and Llama3-70B-Instruct, resulting in the AIC-M and AIC-L datasets, respectively. Further details about the data are provided in Appendix A.

TrainingWe follow a standard supervised fine-tuning approach to train several models, including Mistral-7B-Base , Llama3-8B-Base, Mixtral-8\(\)7B-Instruct , and Llama3-70B-Instruct.

EvaluationWe evaluate the effectiveness of our method using five high-difficulty mathematical benchmarks, including the in-domain benchmark MATH and out-of-domain benchmarks GaoKaoBench-Math , MathOdyssey , OlympiadBench-Math , and TheoremQA .

More detailed information on the experimental settings is provided in Appendix B.

### Effectiveness of Synthesized Data

We separately fine-tune Mixtral-8\(\)7B-Instruct and Llama3-70B-Instruct using data synthesized by Mixtral-87B-Instruct (AIC-M) and Llama3-70B-Instruct (AIC-L) to evaluate whether the synthesized data can enhance the performance of the models. Since the data synthesis process involves both large language models (LLMs) and seed data, we compared the performance of models trained with both synthesized and seed data. As shown in Table 1, training with the synthesized data significantly outperforms training with the original seed data, demonstrating the effectiveness of our approach.

### Comparison with other model

In this section, we train two base models Mistral-7B-Base and Llama3-8B-Base using AIC-L and compare them with other models, including WizardMath, MetaMath, MMIQC, MathScale, and MAMmoTH2. Additional details about baselines are provided in Appendix B.3.

Table 2 presents the performance of our method compared to other data synthesis approaches across various high-difficulty math benchmarks. Among models based on Mixtral-7B-Base, Mistral-7B

   Model & MATH & GaoKao & Odyssey & Olympiad & TheoremQA & Avg \\   \\  demonstrated an average improvement of 3.4%. For models derived from Llama3-8B-Base, Llama3-8B-AIC showed an improvement of 2.0%. Additionally, while most competing models utilize closed-source advanced models (e.g., GPT-3.5, GPT-4) for data synthesis, our approach leverages the open-source model, further underscoring the effectiveness of our method.

### Fair Comparison with Other Methods

Given the variation in both the models used for data synthesis and the scale of data synthesis across the comparison objects in Section 3.3, these differences do not accurately represent the strengths and weaknesses of the synthesis methods. To address this, we standardize both the data synthesis models and the scale of data synthesis in this section, allowing for a more comprehensive evaluation of the methods. We chose MATH as the seed data and conducted all evaluations on MATH. The methods compared include NumReplace, MMIQC, and Xwin-MATH, as introduced in Appendix B.3. We trained models on various data scales to thoroughly assess the effectiveness of the synthesis methods.

The results in Figure 3 show that our method outperforms the baselines at any scale. We believe this is because, when generating more difficult problems, methods without a verification mechanism often lead to errors in the synthesized data, thereby reducing its quality.

### Effectiveness of Verification

We investigate the verification mechanism's effectiveness by comparing two equal-sized datasets: one before and one after its application. For simplicity, this experiment uses only MATH as the seed data and test set.

The results in Table 3, demonstrate that the verification mechanism enhances model performance. This improvement stems from the fact that the final answers generated by the code are generally correct, and filtering the rationale based on these answers improves the logical and computational accuracy of the data, thereby enhancing its overall quality. These findings highlight the importance of verifying the correctness of synthesized data.

## 4 Conclusion

The paper proposes a mathematical synthesis approach generating diverse, verified synthetic data through algorithmic abstraction and contextualization, offering a scalable solution for enhancing LLM mathematical capability.

Figure 3: Fair comparison with other methods.

   Verification & Samples & MATH \\   \\ ✓ & 34k & 21.2\({}_{+1.1}\) \\ ✗ & 34k & 20.1 \\   \\ ✓ & 34k & 18.8\({}_{+1.0}\) \\ ✗ & 34k & 17.8 \\   

Table 3: Ablation Study on the verification mechanism.