# How Many Van Gogh's Does It Take to Van Gogh?

Finding the Imitation Threshold

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individuals privacy. This phenomenon is termed _imitation_ - generation of images with recognizable similarity to training images. In this work we study the relationship between a concept's frequency in a dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept - the _imitation threshold_. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains - human faces and art styles for which we create three datasets, and evaluate three text-to-image models which were trained on two pre-training datasets. Our results reveal that the _imitation threshold_ of these models is in the range of 200-600 images, depending on the domain and the model. The _imitation threshold_ can provide an empirical basis for copyright violation claims and acts as a guiding principle for providers of text-to-image models that aim to comply with copyright and privacy laws.

## 1 Introduction

The progress of multi-modal vision-language models has been phenomenal in recent years [14; 35; 36; 38], much of which can be attributed to the availability of large-scale pretraining datasets like LAION . These datasets consist of semi-curated image-text pairs scraped from Common Crawl, which leads to the inclusion of explicit, copyrighted, and licensed material [56; 4; 17; 20; 4]. Training models on such images may be problematic because text-to-image models can _imitate_ -- the ability to generate images with recognizable features -- concepts from their training data [5; 49]. This behavior has both legal and ethical implications, such as copyright infringements as well as privacy violations of individuals whose images are present in the training data without consent. In fact, a large group of artists sued Stability AI, creators of widely-used text-to-image models, alleging that the company's models generated images that distinctly replicated their artistic styles .

Previous work has focused on detecting when generated images imitate training images, and mitigations thereof [5; 48; 49; 50]. In particular, researchers found that duplicate images increase the chance of memorization and imitation. However, the relation between a concept's prevalence and the models' ability to imitate it remains unexplored.

In this work, we ask **how many instances of a concept does a model need to be trained on to imitate it?** Establishing such an _imitation threshold_ is useful for several reasons. First, it provides an empirical basis for copyright infringements and privacy violations claims [56; 42]. Second, it acts as a guiding principle for text-to-image models providers that want to avoid such violations. Finally, it reveals an interesting connection between training data statistics and model behavior, and the ability of models to efficiently harness training data [53; 6]. We name this problem FIT: **F**inding the Imitation Threshold, and provide a schematic overview of this problem in Figure 1.

The optimal methodology to measure the imitation threshold requires training multiple models with varying number of images of a concept and measuring the ability of the counterfactual models to imitate it. However, training even one of these models is prohibitively expensive. We propose an alternative approach, **M**easure **I**mitation **T**hr**E**shold **T**hrough **I**nstance **C**ount and **C**omparison (MIMETIC\({}^{2}\)), that estimates the threshold without incurring the cost of training models from scratch. We start by collecting a large set of concepts per domain (e.g., Van Gogh for artistic styles), and use a text-to-image to generate images for each concept. Then, we compute the imitation score of the generated images by comparing them to the training images of the respective concept, and estimate each concept's frequency in the training data. Finally, by sorting the concepts based on frequency we estimate the imitation threshold for that domain using a _change detection_ algorithm .

Since we operate with observational data, a naive implementation may be confounded by different factors, such as the quality of the imitation scoring model on different groups within the domain, or estimating the training frequencies of concepts (e.g., simple counts of 'Van Gogh' in the captions results in a biased estimate since the artist may be mentioned in the caption without their work). As such, we carefully tailor MIMETIC\({}^{2}\) to minimize the impact of such confounders.

Overall, we formalize a new problem - **F**inding **I**mitation **T**hreshold (**F**IT; SS3), and propose a method, MIMETIC\({}^{2}\), that efficiently estimates the _imitation threshold_ for text-to-image models (SS5). We use our method to estimate the imitation threshold for two domains on three datasets, three text-to-image models that were trained on two pretraining datasets (SS4). We find the imitation thresholds to range between 200 to 600 images, providing concrete insights on models' imitation abilities (SS6).

## 2 Background

**Dataset Issues and Privacy Violations** The advancement in text-to-image capabilities, largely due to big training datasets, is accompanied by concerns about the training on explicit, copyrighted, and licensed material  and imitating such content when generating images [9; 17; 20; 56]. For example, Birhane et al.  and Thiel  found several explicit images in the LAION dataset and Getty Images found that LAION had millions of their copyrighted images . Issues around imitation of training images has especially plagued artists, whose livelihood is threatened [42; 48], as well as individuals whose face has been used without consent to create inappropriate content [2; 17].

**Training Data Statistics and Model Behavior** Pre-training datasets are a core factor for explaining model behavior . Razeghi et al.  found that the in-context few-shot performance of language models (LMs) is highly correlated with the frequency of instances in pre-training datasets. Udandarao et al.  bolster this finding by demonstrating that the performance of multimodal models on downstream tasks is strongly correlated with a concept's frequency in the pretraining datasets. In

Figure 1: An overview of FIT, where we seek the _imitation threshold_ – the point at which a model was exposed to enough instances of a concept that it can reliably imitate it. The figure shows four concepts (e.g., Van Gogh’s art style) that have different frequencies in the training data (213K for Van Gogh). As the frequency of a concept’s images increases, the ability of the text-to-image model to imitate it increases (e.g. Piet Mondrian and Van Gogh). We propose an efficient approach, MIMETIC\({}^{2}\), that estimates the imitation threshold without training models from scratch.

addition, Carlini et al.  shows that language models more easily memorize duplicated sequences. We find a similar phenomenon: increasing the number of images of an instance increases the similarity between the generated and training images on average. Crucially, instead of measuring _memorization_, we measure _imitation_, and we use such metric to find the _imitation threshold_.

## 3 Problem Formulation and Overview

Finding the Imitation Threshold (FIT) seeks to find the minimal number of images with some concept a model has to see during training in order to imitate it. FIT's setup involves a training dataset \(=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}\), composed of \(n\) (image, caption) pairs. Each concept is part of a domain \(\), such as art styles. We also assume an indicator \(I^{j}\) that indicates whether a concept \(Z^{j}\) is present in image \(x_{i}\). Each concept \(Z^{j}\) appears \(c^{j}=|_{i}I^{j}(x_{i})|\) times in the dataset \(\). Finally, we assume a model \(\) that is trained on \(\) as a text-to-image model to predict \(x_{i}\) from \(y_{i}\). The _imitation threshold_ is the minimal number of images \(c^{j}\) with some concept \(Z^{j}\) from which the model \(\) is able to generate images \((p^{j})\) given some prompt \(p^{j}\)1 where \(Z^{j}\) is recognizable as the concept.

\[\{k\{0,1,\}:I^{j}(_{k}(p^{j}))=1\}\]

**Optimal Approach.** Finding the _imitation threshold_ is a causal question - _if model \(\) was trained with \(k^{}\) images of concept \(Z^{j}\) instead of \(k\), could it generate images with this concept?_ The optimal manner of answering this question is a brute force experiment : For each concept \(Z^{j}\), we create a dataset \(^{}_{k}\) where we vary the number of images with such concept in the training data, where \(k\{0,1,,n\}\), and train a model \(^{}_{k}\) on each dataset. Once we find a model, \(^{}_{k}\), that is able to generate the concept, but \(^{}_{k-1}\) cannot, we deem \(k\) as the _imitation threshold_ for that concept. _However, due to the extreme costs of training text-to-image models, this optimal approach is impractical (this approach will require training \(( n)\) models)._

**MIMETIC2.** We propose an approach that is tractable and estimates the causal effect under certain assumptions. The key idea is to use observational data instead of training a model for different number of images for each concept. Such an approach has been previously used to answer causal questions, inter alia, [25; 29; 33]. Concretely, we collect several different concepts (\(Z^{j}\)) belonging to some domain (\(\)) while ensuring that these concepts have varying image frequencies in the training dataset \(\). Then, we identify the frequency where model \(\) starts generating images with the concept at that frequency. We term this frequency as the _imitation threshold_.

To evaluate the imitation ability, we build a _concept-score_ function \(f\) that returns an imitation score \(f(X_{t},_{p^{j}})\) that measures the imitation of a concept in the generated images using its training data. \(X_{t}:=x_{1},...,x_{t}\) is a set of training images associated with concept \(^{j}\). \(_{p^{j}}:=(p^{j})_{1},(p^{j})_{2},, (p^{j})_{g}\) is a set of generated images created using different random seeds and a text prompt that mentions \(Z^{j}\), For a domain \(\), we collect a set of concepts \(Z^{1},Z^{2},...,Z^{m}\) (e.g., a list of artistic styles), estimate each concept's frequency in data \(\), and measure the imitation score for each concept. Sorting the concepts based on their frequencies in the dataset, and using a standard _change detection_ algorithm on the imitation scores, gives us the imitation threshold for that domain. We provide the implementation details in Section 5.

**Assumptions.** Our approach to compute the imitation threshold makes an assumption about distribution invariance in order to make the problem computationally tractable. This assumption is a standard practice when answering causal questions using observational data . This assumption posits an invariance in the image distribution of each concept. Under this assumption, measuring the imitation score of a concept \(Z^{i}\) with a counterfactual model trained with \(k^{}\) images of \(Z^{i}\) is equivalent to measuring the imitation of another concept \(Z^{j}\) that currently has \(k^{}\) images in the already trained model. This helps us answer the causal question FIT seeks without training multiple models. And similar to other sample complexity works [53; 54], we also assume each image of a concept contributes equally to its learning.

## 4 Experimental Setup

**Text-to-image Models and Training Data.** We use Stable Diffusion (SD) as the text-to-image models .

Specifically we use SD1.1, SD1.5 that were trained on LAION2B-en, a 2.3 billion image-caption pairs dataset, filtered to contain only English captions. In addition, we use SD2.1 that was trained on LAION-5B, a 5.85 billion image-text pairs dataset, which includes LAION2B-en, and other image-caption pairs from other languages .

**Domains and Concepts.** We experiment with two domains - _art styles_ and _human faces_ that are of high importance for privacy and copyright aspects of text-to-image models. Figures 1 and 1(a) show examples of real and generated images of art styles and human faces.

We collect a two sets of artists for the art style - classical artists and modern artists, and two sets for human faces - celebrities and politicians. Then, for each set we sample 400 names that cover a wide frequency range over the pretraining data. We provide details of the sources used to collect the concepts, and sampling procedure in Appendix M. Table 1 summarizes the pretraining data, models, domains and constructed datasets we use in this work.

**Image Generation.** We generate images for each domain by prompting models with five prompts (Table 2). We design domain-specific prompts that encourage the concepts to occupy most of the image, which simplifies the imitation measurement. We generate 200 images per concept using different random seeds for each prompt, a total of 1,000 images per concept.

## 5 Proposed Methodology: MimETIC\({}^{2}\)

We illustrate our proposed methodology in Figure 3. At a high level, for a specific domain, MIMETIC\({}^{2}\) estimates the frequency of each concept in the pretraining data (Section 5.1) and the model's ability to imitate it (Section 5.2). We then sort the concepts based on their estimated frequencies, and find the imitation threshold using a change detection algorithm (Section 5.3).

### Concept Frequency

**Challenges.**

Determining a concept's frequency in a multimodal dataset can be achieved by employing a high-quality classifier for that concept over every image and counting the number of detected images. However, given the scale of modern datasets with billions of images, this approach is expensive and time consuming. Instead, we make a simplifying assumption that a concept is present only if the image's caption mentions it. While this assumption does not hold in general, it is a reasonable simplification for the domains we focus on. We further discuss this assumption and provide supporting evidence for its accuracy in Appendix D. In addition, concepts often do not appear in the corresponding images, even when they are mentioned captions. For instance, Figure 1(b) showcases images whose captions contain "Mary Lee Pfeiffer", but her image does not always include her. On average, we find that concepts occur only in 60% of the images whose captions mention the concept.

  
**Pretraining Data** & **Model** & **Domain** & **Dataset** \\  LAION2B-en & SD1.1 & Human Faces & Celebrations, Politarians \\  & AA Style & & Classical, Modern artists. \\  & SD1.5 & Human Faces & Celebrations, Politarians \\  & AR Style & & Classical, Modern artists. \\  LAION-5B & SD2.1 & Human Faces & Celebrations, Politarians \\  & & An Style Style & & Classical, Modern artists. \\   

Table 1: Pretraining data, models, domains, and datasets we experiment with.

  
**\#** & **Human faces** & **Art style** \\  
1 & A photoacoustic close-up photograph of X & A painting in the style of X \\
2 & High-resolution close-up image of X & An artwork in the style of X \\
3 & Close-up headout of X & A sketch in the style of X \\
4 & X’s facial close-up & A fine art piece in the style of X \\
5 & X’s face portrait & An illustration in the style of X \\   

Table 2: Prompts used to generate images of human faces (celebrities and politicians) and art styles. We generate 200 images per concept using different random seeds (1,000 images per concept). ‘X’ is replaced with the concept.

**Estimating Concept Frequency** Due to the challenges described above, we start by retrieving all images whose captions mention the concept of interest and filter out the ones that do not have that concept, as detected by a classifier. We retrieve these images using WIMBD , a search tool based on a reverse index that efficiently finds documents (captions) from LAION containing the search query (concept). In addition, for each concept, we construct a set of high quality reference images. For example, a set of images with only the face of a single person (e.g., Brad Pitt). We collect these images automatically using a search engine, followed by a manual verification to vet the images (see Appendix E for details). Overall, we collect up to ten reference images per concept. These images are used as gold reference for automatic detection of these concepts in the images from the pretraining datasets.

Next, to classify whether a candidate image from the pretraining data contains the concept of interest, we embed the candidate image and the concept's reference images using an image encoder and measure the similarity between the embeddings. We use a face embedding model  for faces and an art style embedding model  for art style. If the similarity between a candidate image and any of the reference images is above some threshold, we consider that image to contain that concept. This threshold is established by measuring the similarity between images of the same concepts and images of different concepts which maximizes the true positive, and minimizes false positive. We provide additional details on the exact thresholds per dataset and how to find them in Appendices F and G.

Finally, we employ the classifier on all candidate images corresponding to a concept, and take those that are classified as positive. For each concept, we randomly retrieve up to 100K images whose captions mention that concept. We use the ratio of positive predictions from the retrieved candidate images and multiply it by the total caption counts of the concept in the dataset and use that as the concept frequency estimate. For concepts with less than 100K candidate images, we simply use all the images that are positively classified. Note that several URLs in the LAION datasets are dead, a common phenomenon for URL based datasets ("link rot" ). On average, we successfully retrieved 74% of the candidate images.

### Computing Imitation Score

**Challenges.** Computing the imitation score entails determining how similar a concept is in a generated image compared to its source images from the training data. Several approaches were proposed to accomplish this task, such as FID and CLIPScore . To measure similarity, these approaches compute the similarity between the distributions of the embeddings of the generated and training images of a concept. The embeddings are obtained using image embedders like Inception model in case of FID and CLIP in case of CLIPScore. These image embedders often perform reasonably well in measuring similarity between images of common objects which constitutes most of their training data. However, they cannot reliably measure the similarity between two very similar

Figure 3: Overview of MIMETIC2’s methodology to estimate FIT. In Step 1, we estimate the frequency of each concept in the pretraining data by obtaining the images (\(x\)) that contain the concept of interest. In Step 2, we use the images of each concept and compare them to the generated images to measure imitation (using \(g\) that receives reference images \(T\), and generated image \(x^{}\)). We repeat this process for each concept to generate the imitation graph, and then determine the _imitation threshold_ with a detection change algorithm.

concepts like the faces of two individuals or art style of two artists [1; 15; 19; 51]. Therefore, MIMETIC\({}^{2}\) uses domain specific image embedders to measure similarity between two concepts. It uses a face embedding model  for measuring face similarity and an art style embedding model  for measuring art style similarity. Even the specific choice of these models is crucial. For instance, in early experiments we used Facenet , and observe it struggles to distinguish between individuals of certain demographics, causing drastic differences in the imitation scores between demographics. We provide more details on these early experiments in Appendix L, and show that our final choice of embedding models work well on different demographics.

**Estimating Imitation Score** To measure imitation we embed the generated images and training images of a concept (obtained from Section 5.1) using the concept specific image embedder. For measuring face imitation, we use InsightFace, a face embedding model  that extracts the individual's face from an image and generates an embedding for it. For measuring art style imitation, we use CSD, an art style embedding model  that generates an embedding for an image of an art work. We obtain the embeddings of the generated and training images for both the domain, and measure imitation by computing the cosine similarity between them.

To ensure that the automatic measure of similarity correlates with human perception, we also conduct experiments with human subjects and measure the correlation between the similarities obtained automatically and in the human subject experiments. We find a high correlation between the two measures of similarity (\(@sectionsign\)C in Appendix).

### Detecting the _Imitation Threshold_

After computing the concept frequencies and the imitation scores for each concept, we sort them in an ascending order of their image counts. This generates a sequence of points, each of which is a pair of image counts and imitation score of a concept. We apply a standard change detection algorithm, PELT , to find the image frequency where the imitation score significantly changes. Change detection is a classic statistical problem for which the objective is to find the points where the mean value of a stochastic time-series signal changes significantly. Several algorithms were proposed for change detection . We choose PELT because of its linear time complexity in computing the change point. We choose the first change point as the imitation threshold (see Appendix I for details about all change points). The application of change detection assumes that increasing the image counts beyond a certain threshold leads to a large jump in the imitation scores, and we find this assumption to be accurate in our experimental results.

## 6 Results: The _Imitation Threshold_

We apply MIMETIC\({}^{2}\) to estimate the imitation threshold for each model-data pair, and present the results in Table 3. The imitation thresholds for SD1.1 on celebrities and politicians are 364 and 234 respectively. And the imitation thresholds for classical and modern artists are 112 and 198 respectively. Interestingly, SD1.1 and SD1.5 have the same thresholds for all the four datasets. Notably, both SD1.1 and SD1.5 are trained on LAION2B-en. The imitation thresholds for SD2.1, which is trained on the larger LAION-5B dataset is higher than the thresholds for SD1.1 and SD1.5. The imitation threshold for SD2.1 on celebrities and politicians are 527 and 369 respectively, and on classical and modern artists are 185 and 241 respectively. We hypothesize that the difference in performance of SD2.1 and SD1.1 is due to the difference in their text encoders . (The difference in performance of SD2.1 and SD1.5 was also reported by several users on online forums.) To test this hypothesis, we compute the imitation thresholds for politicians for all SD models in series 1: SD1.1, SD1.2, SD1.3, SD1.4, and SD1.5. We found that the imitation thresholds for all these models are almost the same. We present the graphs for all these models in Appendix H.

    & &  &  \\ 
**Pretraining Dataset** & **Model** & **Celebrities** & **Politicians** & **Classical Artists** & **Modern Artists** \\   & SD1.1 & 364 & 234 & 112 & 198 \\  & SD1.5 & 364 & 234 & 112 & 198 \\  & SD2.1 & 527 & 369 & 185 & 241 \\   

Table 3: _Imitation Thresholds_ for human face and art style imitation for the different text-to-image models and datasets we experiment with.

Note that celebrities have a higher imitation threshold than politicians. We hypothesize this happens due to inherent differences in the data distribution in these two datasets, which makes it harder to learn the concept of celebrities than politicians. To test this hypothesis, we compute the average number of images with a single person for people with less than 1,000 images in the pretraining dataset. We find that politicians have about twice the number of single person images compared to celebrities. As such, images that have only the concept of interest increase the ability of the model to learn from them, thus lowering the imitation threshold. We observe a similar pattern with artists: the imitation threshold for modern artists is higher than for classical artists.

We also present the plots of the imitation scores as a function of the image frequencies of the concepts in the three datasets. Figures 3(a) and 3(b) show the imitation graphs of celebrities and art styles, respectively for SD1.1. The x-axis describes the sorted concept frequency and the y-axis describes the imitation score (averaged over the five image generation prompts). We showcase the graphs for the other models and domain in Appendix J, which follow similar trends.

In Figure 3(a), we observe that the imitation scores for individuals with low image frequencies are close to 0 (left side), and increase as the image frequencies move towards the right side. The highest similarity is 0.5 and it is for individuals in the rightmost region of the plot. We observe a low variance in the imitation scores across prompts. We also note that the variance does not depend on the image frequencies (\(0.0003 0.0005\)) - indicating that the performance of the face embedding model does not depend on the popularity of the individual.

Similarly, in Figure 3(b), we observe that imitation scores for art styles with low image frequencies are close to 0.2 (left side), and increase as the image frequencies move towards the right side. The highest similarity is 0.76 and it is for the artists in the rightmost region of the plot. We also observe a low variance across the generation prompts, and the variance does not depend on the image frequency of the artist (\(0.003 0.003\)).

**Results Discussion.** Overall, we observe that the imitation thresholds are similar across the different image generation models and pretraining datasets, but are domain dependent. They show little variance across various image generation prompts. And most importantly, the thresholds computed by MIMETIC\({}^{2}\) have a high degree of agreement with human perception of imitation.

We also note the presence of several outliers in both plots, that can be categorized into two types: (1) concepts whose image counts are smaller than the imitation threshold, but their imitation scores are considerably high; and (2) concepts whose image counts are higher than the imitation threshold, but their imitation scores are low. As such, from a privacy perspective, the first kind of outliers are more crucial than the second ones This is because the imitation threshold should act as guarantors of privacy. It would be fine if a concept with a frequency higher than the threshold is not imitated by

Figure 4: **Human Face and Art Style imitation graphs of SD1.1 for the celebrity and classical artists datasets. The x-axis represents the sorted counts from the training set (and each concept), and the y-axis represents the similarity between the training and generated images. Concepts with zero image frequencies are shaded with light gray. We show the mean and variance over the five generation prompts. The red vertical line indicates the imitation threshold, and the horizontal green line represents the similarity threshold.**

the model (false positive), but it would be a privacy violation if a model can imitate a concept with frequency lower than the threshold (false negative). Therefore, it is preferable to underestimate the imitation threshold to minimize false negatives. Upon further analysis, we find that the actual concept frequencies of _all the false negative outliers_ is much higher than what MIMETIC\({}^{2}\) counts, primarily due to aliases of names, thereby alleviating the privacy violation concerns (see Appendix B).

We also note that the range of the imitation scores of different domains have different y-axis scales. This is due to the difference in embedding models used in both cases. The face embedding model can distinguish between two faces much better than the art style model can distinguish between two styles (see Appendices F and G), and therefore the scores for the concepts on the left side of the imitation threshold is around 0 for face imitation and 0.2 for style imitation. The face embedding model also gives lower score to the faces of the same person, compared to the style embedding model's score for images of the same art styles, and therefore the highest scores for face imitation is 0.5, whereas it is 0.76 for art style imitation. However, the absolute values on the y-axis do not matter for estimating the imitation threshold as long as the trend is similar, which is the case for both domains.

## 7 Discussion and Limitations

**Equal Effect Assumption.** An assumption in the formulation of MIMETIC\({}^{2}\) is that every image of a concept contributes equally to the learning of the concept. However, not all images are created equal. While analyzing celebrities' images for instance, we often find that individuals whose images are mostly close-ups of a single person have a higher imitation score than individuals whose images are cluttered by multiple people, since concept-centered images enhance their learnability.

We hope to investigate this assumption in future work, and address this, and other potential confounders.

**Factors Affecting the Imitation Threshold.** In this work we attribute the imitation of a concept to its image count. However, image count - although a crucial factor - is not the only factor that affects imitation. Several other factors like image resolution, alignment between images and their captions, the variance between images of a concept, etc., may affect imitation.

Several training time factors like the optimization objective, learning schedule, training data order, model capacity, model architecture also affect the imitation threshold. We discuss the difference in the imitation thresholds of SD1.1, SD1.5 and SD2.1 is attributed to the difference in their text encoders. SD1.1 and SD1.5 use CLIP model  as their text encoder and SD2.1 uses OpenCLIP  as its text encoder. Note that while these may impact the behavior of the model, our work is interested in a particular model-data pair, for which we investigate. We do not claim that our results would generalize to other models, or datasets, and leave the question on how to FIT that generalize across models to future work.

## 8 Conclusions

Text-to-image models can imitate their training images [5; 49; 50]. This behavior is potentially concerning because these models' training datasets often include copyrighted and licensed images. Imitating such images would be grounds for violation of copyright and privacy laws. In this work, we seek to find the number of instances of a concept that a text-to-image model needs in order to imitate it - the _imitation threshold_. We posit this as a new problem, Finding the Imitation Threshold (FIT) and propose an efficient method for finding such threshold. Our method, MIMETIC\({}^{2}\), utilizes pretrained models to estimate the imitation threshold for human face and art style imitation using three text-to-image models trained on two different pretraining datasets. We find the imitation threshold of these models to be in the range of 200-600 images depending on the setup. As such, on the domains we evaluate in this work trained on our models, our results indicate that models cannot replicate concepts that appear less than 200 times in the training data.

By estimating the imitation threshold, we provide insights on successful concepts imitation based on their training frequencies. Our results have striking implications on both the text-to-image models users and providers. These thresholds can inform text-to-image model providers what concepts are in risk of being imitated, and on the other hand, serve as a basis for copyright and privacy complaints.