# Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents

Quentin Delfosse\({}^{,1}\) Sebastian Sztwierntia\({}^{,1}\) Mark Rothermel\({}^{1}\)

**Wolfgang Stammer\({}^{1,2}\) Kristian Kersting\({}^{1,2,3,4}\)**

\({}^{1}\)Computer Science Department, TU Darmstadt, Germany

\({}^{2}\)Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany

\({}^{3}\)Centre for Cognitive Science, TU Darmstadt, Germany

\({}^{4}\)German Research Center for Artificial Intelligence (DFKI), Darmstadt, Germany

{firstname.lastname}@cs.tu-darmstadt.de

Equal contributionCode available at https://github.com/k4ntz/SCoBots

###### Abstract

Goal misalignment, reward sparsity and difficult credit assignment are only a few of the many issues that make it difficult for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep neural networks impedes the inclusion of domain experts for inspecting the model and revising suboptimal policies. To this end, we introduce _Successive Concept Bottleneck Agents (SCoBots)_, that integrate consecutive concept bottleneck (CB) layers. In contrast to current CB models, SCoBots do not just represent concepts as properties of individual objects, but also as relations between objects which is crucial for many RL tasks. Our experimental results2 provide evidence of SCoBots' competitive performances, but also of their potential for domain experts to understand and regularize their behavior. Among other things, SCoBots enabled us to identify a previously unknown misalignment problem in the iconic video game, Pong, and resolve it. Overall, SCoBots thus result in more human-aligned RL agents.

## 1 Introduction

Deep Reinforcement learning (RL) agents are prone to suffer from a variety of issues that hinder them from learning optimal or generalizable policies. Prominent examples are _reward sparsity_(Andrychowicz et al., 2017) and _difficult credit assignment_(Raposo et al., 2021; Wu et al., 2024). A more pressing issue is the _goal misalignment_ problem. It occurs when an agent optimizes a different _side-goal_, aligned with the original _target goal_ during training (Koch et al., 2021), but not at test time. Such misalignments can be difficult to identify (di Langosco et al., 2022). For instance, in this work we discover that such a misalignment can occur in the oldest and most iconic video game, _Pong_(_cf._ Fig. 1). In Pong, the agent's target goal is to catch a ball with its own paddle and to return it passed the enemy's one. The enemy is programmed to constantly follow the ball, thus, agents can learn to focus on the position of the enemy paddle for placing their own, rather than the position of the ball itself. If left unchecked, such _shortcut learning_(Geirhos et al., 2020) may lead to a lack of model generalization and unintuitive failures _e.g._ at deployment time (Zhang et al., 2021).

Recently eXplainable AI (XAI) methods have emerged to detect such shortcut behavior, by identifying the reasons behind a model's decisions (Schramowski et al., 2020; Roy et al., 2022; Saeed and Omlin, 2023). However, many XAI methods' explanations do not faithfully present the model'sunderlying decision process (Chan et al., 2022). For example, importance-map explanations indicate the importance of an input element without indicating _why_ this element is important (Kambhampati et al., 2021; Stammer et al., 2021; Teso et al., 2023). A recent branch of research therefore focuses on models that provide inherent concept-based explanations. Prominent examples are concept bottlenecks models (CBMs), which provide predictive performances on par with standard deep learning approaches for supervised image classification (Koh et al., 2020). More importantly, CBMs allow to identify and revise incorrect model behavior on a concept level (Stammer et al., 2021).

Contrary to exising CBMs' fields of applications, RL requires relational reasoning (Kaiser et al., 2019). In this work, we therefore introduce Successive Concept Bottleneck Agents (SCoBots, _cf._ Fig. 1, bottom) bringing the concept bottleneck approach to RL. SCoBots integrate successive concept bottlenecks into their decision processes, where each bottleneck layer provides concept representations that integrate the concept representations of the previous bottleneck layer. Specifically, provided a set of predefined concept functions, SCoBots automatically extract relational concept representations based on objects and their properties of the initial bottleneck layers. Finally, the set of relational and object concept representations are used for optimal action selection. SCoBots thus represent inherently explainable RL agents that, in comparison to deep RL agents, allow for inspecting and revising their learned decision policies at multiple levels of their reasoning processes: from single object properties, through relational concepts to the action selection.

Our evaluations on the iconic Atari Learning Environments (ALE, Mnih et al. (2013)) provides experimental evidence that SCoBots perform on par with deep RL agents. More importantly, we showcase SCoBots' ability to provide valuable explanations and the potential of mitigating a variety of RL specific issues, from reward sparsity to misalignment problems, via simple guidance from domain experts. We identify previously unknown shortcut behavior of deep agents, even on the simple _Pong_ game. By utilizing the interaction capabilities of SCoBots, this behavior is easily corrected. Ultimately, our work illustrates the severity of goal misalignment issues in RL and the importance of being able to mitigate these and other RL specific issues, via _relational concept based models_. In summary, our contributions are:

**(i)** We introduce Successive Concept Bottleneck agents (SCoBots).

**(ii)** We show that SCoBots allow to inspect their internal decision processes.

**(iii)** We show that their inherent inspectable nature can helps identifying unknown misalignments.

**(iv)** We show that they allow for human interactions for mitigating various RL specific issues.

We proceed as follows. We introduce SCoBots and discuss their specific properties. We continue with experimental evaluations and analysis. Before concluding, we touch upon related work.

Figure 1: **Successive Concept Bottlenecks Agents (SCoBots) allow for easy inspection and revision.** Top: Deep RL agents trained on Pong produce high playing scores with importance map explanations that suggest sensible underlying reasons for taking an action (). However, when the enemy is hidden, the deep RL agent fails to even catch the ball without clear reasons (). Bottom: SCoBots, on the other hand, allow for multi-level inspection of the reasoning behind the action selection, _e.g._, at a relational concept, but also action level. Moreover, they allow users to easily intervene on them () to prevent the agents from focusing on potentially misleading concepts. In this way, SCoBots can mitigate RL specific caveats like goal misalignment.

## 2 Successive Concept Bottleneck Agents

In this work, we represent RL problems through the framework of a Markov Decision Process, \(=<,,P_{s,a},R_{s,a},>\), with \(\) as the state space, \(\) the set of available actions, \(P(s,a)\) the transition probability, and \(R(s,a)\) the immediate reward function, obtained from the environment, and \(\) the ddscount factor. Classic deep RL policies, \(_{}(s)=P(A=a|S=s)\) parameterized by \(\), are usually black-box models that process raw input states, _e.g._ a set of frames, to provide a distribution over the action space (Mnih et al., 2015; van Hasselt et al., 2016).

Concept bottleneck models (CBMs) initiate a learning process by extracting relevant concepts from raw input (_e.g._ image data). These concepts can represent the color or position of a depicted object. Subsequently, these extracted concepts are used for downstream tasks such as image classification. Formally, a bottleneck model \(g:x c\) transforms an input \(x^{D}\) with \(D\) dimensions into a concept representation \(c^{k}\) (a vector of \(k\) concepts). Next, the predictor network \(f:c y\) uses this representation to generate the final target output (_e.g._\(y\) for classification problems).

The main body of research on CBMs focuses on image classification tasks, where the extracted concepts represent attributes of objects. In contrast, RL agents must learn not just from static data, but also through interaction with dynamically evolving environments. RL tasks thus often require relational reasoning, as they involve understanding the relationships between instances that evolve through time and interact with another. This is crucial for learning effective policies in complex, dynamic environments. Note that, in the following descriptions, we use specific fonts to distinguish objects' properties from their relations.

### Building inspectable ScoBots

An underlying assumption of ScoBots is that their processing steps should be inspectable and understandable by a human user. This stands in contrast to _e.g._ unsupervised CBM approaches Jabri et al. (2019); Zhou et al. (2019); Srinivas et al. (2020), where there is no guarantee for learning human-aligned concepts. SCoBots rather take a different approach by dividing the concept learning and RL-specific action selection into several inspectable steps that are grounded in human-understandable concept representations. These steps are described hereafter and depicted in Fig. 2.

Similar to other RL agents, SCoBots process the last \(n\) observed frames from a sequence of images, \(s_{t}=\{x_{i}\}_{i=t-n}^{t}\). For each frame, SCoBots need an initial concept extraction method to extract objects and their properties, as in previous works on CBMs (Stammer et al., 2021; Koh et al., 2020). Specifically, we start from an initial bottleneck model, \(_{_{1}}()\), that is parameterized by parameter set, \(_{1}\). Given a state, \(s_{t}\), this model provides a set of \(c_{i}\) object representations per frame, \(_{_{1}}(s_{t})=_{t}=\{\{o_{j}^{i}\}_{j=1}^{c_{i}}\}_{i=t-n}\), where each object representation corresponds to a tensor of different extracted properties of that object, (_e.g._ its category, position (_i.e._\(x,y\) coordinates), etc.). As done in previous works on CBMs, the bottleneck model of our SCoBots, \(_{_{1}}\), can correspond to a model that was supervised pretrained for extracting the objects and their properties from images.

Figure 2: **An overview of Successive Concept Bottlenecks Agents (SCoBots).** SCoBots decompose the policy into consecutive interpretable concept bottlenecks (ICB). Objects and their properties are first extracted from the raw input, human-understandable functions are then employed to derive relational concepts, used to select an action. The understandable concepts enable interactivity. Each bottleneck allows expert users to, _e.g._, prune or utilize concepts to define additional reward signals.

One of the major differences to previous work on CBMs is that SCoBots further extract and utilize _relational_ concepts that are based on the previously extracted objects and their properties. Formally, SCoBots utilize a consecutive bottleneck model, \(_{}()\), called the relation extractor. This model, \(\), is parameterized by a predetermined set of transparent relational functions, \(\), used to extract a set of \(d_{t}\) relational concepts. These relations are based on each individual object (and its properties) for unary relations or on combinations of objects for n-ary relations, and are denoted as \(_{}(_{t})=_{t}=\{g_{t}^{k}\}_{k=1}^{d_{t}}\). Without strong prior knowledge, \(\) can initially correspond to universal object relations such as **distance** and **speed**. However, this set can easily be updated on the fly by a human user with _e.g._ additional, novel relational functions. Note that \(\) can include the identity, to let the relation extractor pass through initial object concepts from \(_{t}\) to the relational concepts \(_{t}\).

Finally, SCoBots employ an action selector, \(_{_{2}}\), parameterized by \(_{2}\), on the relational concepts to select the best action, \(a_{t}\), given the initial state, \(s_{t}\) (_i.e._ the set of frames). Up to now, all extracted concepts in SCoBots, both \(_{t}\) and \(_{t}\), represent human-understandable concept representations. To guarantee understandability also within the action selection step of SCoBots, we need to embed an interpretable action selector. While neural networks, a standard choice in RL literature, are performative and easy to train using differentiable optimization methods, they lack this required interpretability feature. However, Caglar Aytekin (2022) have recently shown the equivalence between ReLU-based neural networks and decision trees, which in contrast represent inherently interpretable models. To trade-off these issues of flexibility and performance vs interpretability, SCoBots thus break down the action selection process by initially training a small ReLU-based neural network action selector, \(_{_{2}^{s}}\), via gradient-based RL optimization. After this, \(_{_{2}^{s}}\) is finally distilled into a decision tree \(_{_{2}}\). Lastly, note that one can add a residual link from the initial object concepts, \(_{t}\), to the relational concepts, \(_{t}\) such that the action selector can, if necessary, also make decisions based on basic object properties, _e.g._, the height of an object.

Overall, our approach preserves the MDP formulation used by classic deep approaches, but decomposes the classic deep policy \(s_{t}}a_{t}\) into a successive concept bottleneck one \(s_{t}}}_{t} }}_{t}}}a_{t}\), where \(=(_{1},_{2},)\) constitutes the set of policy parameters. For simplicity, we will discard the parameter notations in the rest of the manuscript. Further explanations of the input and output space of each module, as well as the properties and relational functions used in this work are provided in the appendix (_cf._ App.A.5 and App. A.6).

Instead of jointly learning object detection, concept extraction, and policy search, SCoBots enable independent optimization of each policy component. Separating the training procedure of different components reduces the complexity of the overall optimization problem (Koh et al., 2020).

### Guiding SCoBots

The inspectable nature of SCoBots not only brings the benefit of improved human-understandability, but importantly allows for targeted human-machine interactions. In the following, we describe two guidance approaches for interacting with the latent representations of SCoBots: concept pruning and object-centric rewarding. We refer to the revised SCoBot agents as _guided_ SCoBots in the following.

**Concept pruning.** The type and amount of concepts that are required for a successful policy may vary across tasks. For instance, the objects **colors** are irrelevant in Pong but required to distinguish vulnerable ghosts from dangerous ones in MsPacman. However, overloading the action selector with irrelevant concepts, whether these are object properties (\(_{t}\)) or relational concepts (\(_{t}\)), can lead to difficult optimization (_e.g._ the agent focusing on noise in unimportant states) as well as difficult inspection of the decision tree (\(\)). Moreover, for a single RL task, the need for specific concepts might even change during training, in _e.g._ progressive environments (where agents need to master early stages before being provided with additional, more complex tasks (Delfosse et al., 2024c)).

SCoBot's human-comprehensible concepts therefore allow domain experts to prune unnecessary concepts. In this way, expert users can guide the learning process towards relevant concepts. Formally, users can (i) select a subset of the object property concepts, \(\). Additionally, by (ii) selecting a subset of the relational functions, \(}\), or (iii) specifying which objects specific functions should be applied on, experts can implicitly define the relational concepts subset, \(\). Guided SCoBots thus formally refining the policy extraction to \(s_{t}}}_{t}_{t}}}_{t}}}a_{t}\). Furthermore, users can (iv) prune out redundant actions resulting in a new action space \(}\).

To give brief examples of these four pruning possibilities we refer to Fig. 2, focusing here on the blue subparts. Particularly, in Pong a user can remove objects (_e.g._ scores) or specific object properties (_e.g._ R,G,B values) to obtain \(}\). Second, the color relation, color(\(\)), is irrelevant for successfully playing Pong and can therefore be removed from \(\). Third, the vertical distance function (dist(\(\),\(\)).y) can be prevented from being applied to the (player, enemy) input couple. These pruning actions provide SCoBots with \(}\). Lastly, the only playable actions in Pong are UP and DOWN. To ease the policy search, the available FIRE, LEFT and RIGHT from the base Atari action space might be discarded to obtain \(}\), as they are equivalent to NOOP (_i.e._ no action).

Importantly, being able to prune concepts can help mitigate misalignement issues such as those of agents playing Pong (_cf._ Fig. 1), where an agent can base its action selection on the enemy's position instead of the ball's one. Specifically, pruning the enemy position from the distance relation concept enforces SCoBot agents to rely on relevant features for their decisions such as the ball's position, rather than the spurious correlation with the enemy's paddle.

**Object-centric feedback reward**. Reward shaping (Touzet, 1997; Ng et al., 1999) is a standard RL technique that is used to facilitate the agent's learning process by introducing intermediate reward signals, aggregated to the original reward signal. The object-centric and importantly _concept-based_ approach of SCoBots allows to easily craft additional reward signals. Formally, one can use the extracted relational concepts to express a new expert reward signal:

\[R^{}(_{t}):=_{g_{t}_{t}}_{g_{t}} g_{t},\] (1)

where \(R^{}:\) and \(_{t}=((s_{t}))\) is the relational state, extracted by the relation extractor. The coefficient \(_{g_{t}}\) is used to penalize or reward the agent proportionally to relational concepts. Our expert reward only relies on the state, as we make use of the concepts extracted from it. However, incorporating the action into the reward computation is straightforward. In practice, this expert reward signal can lead to guiding the agent towards focusing on relevant concepts, but also help smoothing sparse reward signals, which we will discuss further in our evaluations.

For example, one can impose penalties, based on the distance between the agent and objects (_cf._ "expert reward" arrow in Fig. 2), with the intention of incentivizing the agent to maintain close proximity with the ball. As shown in our experimental evaluation, we can use this concept pruning and concept based reward shaping to easily address many RL specific caveats such as reward sparsity, ill-defined objectives, difficult credit assignment, and misalignment (_cf._ App. A.8 for details on each problem).

## 3 Experimental Evaluations

In our evaluations, we investigate several properties and potential benefits of the transparent SCoBot agents. We specifically aim to answer the following research questions:

**(Q1)** Are concept based agents able to learn competitive policies on different RL environments?

**(Q2)** Does the inspectable nature of SCoBots allow to detect issues in their decision processes?

**(Q3)** Can concept-based guidance help mitigate common RL caveats, such as policy misalignments?

**(Q4)** Can SCoBots learn with imperfect object extraction methods?

**(Q5)** How crucial is the the relation extractor for SCoBots performances?

**Experimental setup:** We evaluate SCoBots on \(9\) Atari games (_cf._ Fig. 3 from the Atari Learning Environments (Bellemare et al., 2012) (by far the most used RL framework (_cf._ App. A.1), as well as the HackAtari modified (Delfosse et al., 2024a) Pong environments, where the enemy is not visible yet active (_NoEenemy_), and where the enemy stops moving after returning the ball (_LazyEnemy_). We provide human normalized scores (following eq. 3) that are averaged over \(3\) seeds for each agent configuration. We compare our SCoBot agents to deep agents with the classic convolutional network introduced by Mnih et al. (2015), that process a stack of \(4\) black and white frames and denote these as deep agents in the following. Note, that we evaluate all agents on the latest _v5_ version of the environments3 to prevent overfitting. All agents are trained for \(20\)M frames under the Proximal Policy Optimization algorithm (PPO, (Schulman et al., 2017)), specifically the stable-baseline3 implementation (Raffin et al., 2021) and its default hyperparameters (_cf._ Tab. 2 in App. A.5).

We focus our SCoBot evaluations on the more interesting aspects of the agent's reasoning process underlying the action selection, rather than the basic object identification step (which has been thoroughly investigated in previous works _e.g._). We thus assume access to a pretrained object extractor and provide our agents with object-centric descriptions of these states (\(_{t}\)) based on the information from OCAtari . Specifically, in our evaluations, the set of object properties consists of object class (_e.g._ enemy paddle, ball etc.), \((x^{t},y^{t})\) coordinates, coordinates at the previous position \((x^{t-1},y^{t-1})\), height and width, the most common RGB values (_i.e._ the most representative color of an object), and object orientation. The specific set of functions, \(\), that are used to extract object relations is composed of: euclidean distance, directed distances (on \(x\) and \(y\) axes), speed, velocity, plan intersection, the center (of the shortest line between two objects), and color name (_cf._ App. A.5.1 for implementation details). To distill SCoBots' learned policies, we use the decision-tree extraction algorithm VIPER . For the **human-guided** SCoBot evaluations (denoted as (guided) SCoBots in our evaluations), we illustrate human guidance and prune out the concepts that we consider unimportant to master the game (_cf._ Tab. 4). Furthermore, to mitigate RL specific problems, we also provide SCoBot agents with simulated human-expert feedback signals, the details of which we describe at the relevant sections below. More details about the setup and the hyperparameters are in App. A.5. In our evaluation, SCoBots' training is slightly faster than deep agents' one (_cf._ Appendix A.9).

**SCoBots learn competitive policies (Q1).**

We present human-normalized (HN) scores of both SCoBot and deep agents trained on each investigated game individually in Fig. 3 (left). Numerical values are provided in Tab. 1 (_cf._ App. A.7). We observe that SCoBot agents perform at least on par with deep agents on all games, even slightly outperform these on \(5\) out of \(9\) (namely, Asterix, Boxing, Kangaroo, Seaquest and Tennis). Our results suggest that SCoBots provide competitive performances on every tested game despite the constraints of multiple bottlenecks within their architectures. Overall, our experimental evaluations show that RL policies based on interpretable concepts extraction decoupled from the action selection process can, in principle, lead to competitive agents.

**Inspectable SCoBots' to detect misalignments (Q2).**

The main target of developing SCoBots is to obtain competitive, yet _transparent_ agents that allow for human users to identify their underlying reasoning process. Particularly, for a specific state, the inspectable bottleneck structure of SCoBots allows to pinpoint not just the object properties, but importantly the relational concepts being used for the final action decision. This is exemplified in Fig. 4 on Skiing, Pong and Kangaroo (_cf._ App. A.4 for explanations of SCoBots on the remaining games). Here we highlight the decision-making path (from SCoBots' decision tree based policies), at specific states of each game. For example, for the game state extracted from Skiing, the SCoBot agent selects RIGHT as the best action, because the signed distance from its character to the left flag is larger than a specific value (\(+15\) pixels). Given the nature of the game this inherent explanation suggests that the agent is indeed basing its selection process on relevant _relational_ concepts.

Figure 3: **Object-centric agents can master different Atari environments and interactive SCoBots allow for corrections. Human-normalized scores of different agents trained using PPO on \(9\) ALE environments, including deep agents (_i.e._ using CNNs), guided decision tree policy (SCoBots), their neural object-centric baseline (NN-), and these baselines without guidance (NG). SCoBots obtain similar or better scores than the deep agents, showing that object-centric agents can also solve RL tasks while making use of human-understandable concepts (left). Guiding SCoBots allow to correct misalignment in Pong (center) and to obtain the originally intended agents, depicted by a level completion score of 100% on the intended goalâ€™s evaluation in Kangaroo (right).**

A more striking display of the benefits of the inherent explanations of SCoBot agents is depicted by the Pong agent in Fig 4. The provided explanation suggests that the agent is basing its decision on the vertical positions of the enemy and of its own paddle (distance between the two paddles on the \(y\) axis). In fact, this suggests that the agent is largely ignoring the ball's position. Interestingly, upon closer inspection of the enemy movements, we observed that, previously unknown, the enemy is programmed to follow the ball's vertical position (with a small delay). Thus, the vertical positions of these two objects are highly correlated (Pearson's and Spearman's correlations coefficient above \(99\%\), _cf._ App. A.7). Moreover, the ball's rendering contains flickering, explaining why SCoBots base their decision on this potentially more reliable feature, the enemy paddle's vertical position.

To validate these findings further, we perform evaluations on \(2\) modified Pong environments in which (i) the enemy is invisible, yet playing (_NoEnemy_, _cf._ Fig. 1), and on one environment where the enemy is not moving after returning the ball (_LazyEnemy_). We reevaluate the deep and SCoBot agents that were trained on the original Pong environment on _NoEnemy_ and observe catastrophic performance drops in HN scores (_cf._ Fig. 3) at a level of random policies for both types of agents. Evaluations with different DQN agents lead to similar performance drops (_cf._ App. A.7). These drops are particularly striking as initial importance maps produced by PPO and DQN agents highlight all \(3\) moving objects (_i.e._ the player, ball, and enemy) as relevant for their decisions (_cf._ Fig.1 top left and App. A.7), aligned with findings on importance maps of Weitkamp et al. (2018). These maps suggest that the deep agents had based its decisions on _right_ reasons. Our novel results on the _NoEnemy_ and _LazyEnemy_ environments, however, greatly calls to question the faithfulness and granularity of such post-hoc explainability methods. They highlight the importance of inherently transparent RL models on the level of concepts, as provided via SCoBots, to identify such potential issues. In the following, we will investigate how to mitigate the discovered issues via SCoBot's bottlenecks.

**Guiding SCoBots to mitigate RL-specific caveats (Q3).**

We here make use of the interactive properties of SCoBots to address several famous RL specific problems: goal misalignment, ill-defined rewards, difficult credit assignment and reward sparsity.

**Realigning SCoBots:** To mitigate the _goal misalignment_ issues of the SCoBots trained on Pong, we simply remove (prune) the enemy from the set of considered objects (\(\)). Thus, the enemy cannot be used for the action selection process. This leads to SCoBots that are able to play Pong and its _NoEnemy_ version (_cf._ SCoBot in Fig 3 (center)). Furthermore, this shows that playing Pong without observing the enemy is achievable, by simply returning vertical shots, difficult for the enemy to catch.

**Ill-defined reward:** Defining a reward upfront that incentivizes agents to learn an expected behavior is a difficult problem. An example of _ill-defined reward_ (_i.e._ a reward that will lead to an unaligned behavior if the agent maximizes the return) is present in Kangaroo. According to the documentation of the game4, "the mother kangaroo on the bottom floor tries to reach the top floor where her joey is being held captive by some monkeys". However, punching the monkey enemies gives a higher cumulative reward than climbing up to save the baby. RL agents thus tend to learn to kick monkeys on the bottom floor rather than reaching the joey (_cf._ Fig. 4). For revising such agents, we provide an additional reward signal, based on the distance from the mother kangaroo to the joey (detailed in App. A.8). As can be seen in Fig. 3 (right), this reward allows the guided SCoBots to achieve the originally intended goal by indeed completing the level. In contrast, deep agents and particularly unguided SCoBots achieve relatively high HN scores, but do not complete the levels.

Figure 4: **Interpretable SCoBots allow to follow their decision process, thanks to their interpretable concepts. The states and associated decision processes of SCoBots (extracted from the decision trees) on Skiing (left), and from unguided SCoBots on Pong (middle) and Kangaroo (right). For example, in this Skiing state, our SCoBot selects RIGHT, as the signed distance between _Player_ and the (left) _Flag1_ (on the \(x\) axis) is bigger than \(15\). This agent selects the correct action for the right reasons.**

**Difficult Credit Assignment Problem:** The _difficult credit assignment problem_ is the challenge of correctly attributing credit or blame to past actions when the agent's actions have delayed or non-obvious effects on its overall performance. We illustrate this in the context of Skiing, where in standard configurations, agents receive at each step a negative reward that corresponds to the number of seconds elapsed since the last steps (_i.e._ varying between \(-3\) and \(-7\)). This reward aims at punishing agents for going down the slope slowly. Additionally, the game keeps track of the number of flag pairs that the agent has correctly traversed (displayed at the top of the screen, _cf._ Fig. 4), and provide a large reward, proportional to this number, at the end of the episode. Associating this reward signal with the number of passed flags is extremely difficult, without prior knowledge on human skiing competitions. In the next evaluations, we provide SCoBots with another reward at each timestep, proportional to the agent's progression to the flags' position to incentivize the agent to pass in between them, as well as a signal rewarding for higher speed:

\[R^{}_{o\{Flag_{1},Flag_{2}\}}D( Player,o)^{t-1}+V(Player).y.\] (2)

These rewards are further detailed in App. A.8. They allow guided SCoBots to reach human scores, whereas the deep and unguided SCoBots perform worse than random (_cf._ Fig. 3 (left)). Note that providing additional reward signals, as done above, obviously also allows to mitigate _reward sparsity_, as we illustrate on Pong (_cf._ App. A.8.3 for a detailed explanation).

**Object-centric agents can learn with imperfect object extractors (Q4).**

To test object-centric agents' ability to work in more realistic environments, we have tested if they can learn viable policies with imperfect object extraction methods. Based on Delfosse et al. (2023)'s detection results, we added a \(5\%\) misdetection probability and a Gaussian noise (of \(0\) mean and \(3\) pixels of standard deviation on each axis). As depicted in Figure 5, NN-SCoBots (_i.e._ neural network based object-centric agents, using relations) learn comparable policies on all games but _Kangaroo_, demonstrating their ability to mitigate suboptimal object extraction methods. Implementing robustness techniques, such as Kalman filters, would help to further push the performances of such agents.

**Transparent SCoBots benefit from explicit relations (Q5).**

While the ablation of the relation extractor only has a significant impact on the neural-based object-centric agents on \(1\) out of \(9\) games (_Seaquest_), it deteriorates the decision tree based SCoBots on \(6\) environments. This is due to the fact that **relations** can be implicitly recomputed within a neural network, but not within decision trees. As shown by Kohler et al. (2024), the use of the **distance** relation (on a specific axis) allows for performing agents with compact decision tree-based policies. Furthermore, even if some relations can be rediscovered and implicitly encoded within the decision trees, the lack of explicit relational representations can reduce the interpretability of the agents, and will prevent the experts from using these within their guidance.

Overall, our experimental evaluations not only show that SCoBots are on par with deep agents in terms of performances, but that their concept-based and inspectable nature allows to identify and revise several important RL specific caveats. Importantly, in the course of our evaluations, we identified a previously unknown and critical misalignment of existing RL agents on the simple and iconic Pong environment, via the previously mentioned properties of SCoBots.

Figure 5: **SCoBots can learn with noisy object detectors, transparent SCoBots rely on relations.** Final human normalized scores (with stds) comparing SCoBots and the object-centric neural baselines (NN-SCoBots), with and without relations. We also provide the scores of NN-SCoBots that learned on noisy environments. The noise only noticeably affects the agents on _Kangaroo_. Ablating the relations is harmless on NN-SCobots, as neural networks can recompute them, but impacts SCoBots performances on \(6\) games. (*For better visualization, we used a human score of \(100\) in _Boxing_.)

## 4 Limitations

**On the use of OCAtari.** To limit our resource consumption, we made use of the quasi-perfect object extractor (\(\)) of OCAtari, which efficiently extract objects from the RAM. We added an ablation to simulate potential imperfect detection capabilities of Atari object extraction methods. Such extractors can be optimized using supervised (Redmon et al., 2016; Locatello et al., 2020) or self-supervised (Lin et al., 2020; Delfosse et al., 2023) object detection methods. This last work showcase that unsupervised object extraction methods can replace OCAtari at test time, however leading to performance drops. Grandien et al. (2024) have further showcase training RL agents with such pretrained object extractors further improve the object-centric RL agents performances.

**The limit of object-centricity.** Other environments require additional information extraction processes. _E.g._ in _MsPacman_, an agent must navigate a _maze_. Extending the concept representations to cover such concepts in maze or platform environments is an important step for future work. Other representations could allow for the integration of _e.g._ path finding methods such as the A* algorithm.

**Training time of SCoBots**. Thanks to the efficient OCAtari object extraction, SCoBots required in average \(7.5\) hours of training time, while deep agents needed \(10.8\) hours (_cf._ Appendix A.9). All SCoBots variations require less training time compared to deep agents on environments with few objects (_e.g._ _Boxing_, _Pong_, _Tennis_). Currently, at every step, the concept bottleneck values are calculated sequentially in a single process on the CPU, leaving significant room for training time improvement by optimizing these computations (bringing them to GPUs). Thus, in environments with many objects, _e.g._ _Kangaroo_, the training time of unguided SCoBots exceeded that of its deep agent counterpart. Attention on relevant objects could be used to further save computational resources.

## 5 Related Work

The basic idea of **Concept Bottleneck Models (CBMs)** can be found in work as early as Lampert et al. (2009) and Kumar et al. (2009). A first systematic study of CBMs was delivered by Koh et al. (2020), followed by Stammer et al. (2021), who described CBMs as two-stage models that first computes intermediate representations used for the final task output. Where learning valuable initial object concept representations without strong supervision is still a tricky and open issue for concept-based models (Lage and Doshi-Velez, 2020; Stammer et al., 2022; Sawada and Nakamura, 2022; Marconato et al., 2022; Steinmann et al., 2023; Stammer et al., 2024), receiving relational concept representations in SCoBots is performed automatically via the function set \(\). Since the initial works on CBMs they have found utilization in several applications. Antognini and Faltings (2021)_e.g._ apply CBMs to text sentiment classification, and Kraus et al. (2024) to time series' analysis. However, these works consider single object concept representations and focus on supervised learning. The ability of users to revise concepts and decisions has also been parallelly shown by Friedrich et al. (2023).

In fact, CBMs have found their way into RL only to a limited extent. Zabounidis et al. (2023) and Grupen et al. (2022) utilized CBMs in a multi-agent setting, where both identify improved interpretability through concept representations while maintaining competitive performance. Zabounidis et al. (2023) further report better training stability and reduced sample complexity. Their extension includes an optional "residual" layer which passes additional, latent information to the action selector part. SCoBots omit such a residual component for the sake of human-understandability, yet offer the flexibility to the user to modify the concept layer via updating \(\) for improving the model's representations. Similar to how SCoBots invite the user to reuse the high-level concepts to shape the reward signal, Guan et al. (2023) allow the user to design a reward function based on higher-level properties that occur over a period of time. Lastly, SCoBots not only separate state representation learning from policy search, as done by Cuccu et al. (2020), but also enforce object-centricity, putting interpretability requirements on the consecutive feature spaces.

**Explainable RL (XRL)** is an extensively surveyed XAI field (Dazeley et al., 2022; Krajna et al., 2022; Milani et al., 2023) with a wide range of unsolved issues (Vouros, 2022). Milani et al. (2023) introduce a taxonomy for XRL methods, with: (1) feature importance methods that generate explanations that point out decision-relevant input features, (2) learning process & MDP methods which present which past experience or MDP components affect the policy, and (3) policy-level methods, describing the agent's long-term behavior. Based on this, SCoBots extract relations from low-level features, making high-level information available to explanations and thereby support feature importance methods.

According to Qing et al. (2022)'s categorization of XRL frameworks, ScoBots are "model-explaining" (in contrast to reward-, state-, and task-explaining). Other XRL methods rely on LLM to explain the policy (Luo et al., 2019; Fuhrer et al., 2024; Marton et al., 2024), logic (Jiang and Luo, 2019; Kimura et al., 2021; Delfosse et al., 2023; Sha et al., 2024) or programs (Verma et al., 2018; Trivedi et al., 2021; Cao et al., 2022; Kohler et al., 2024; Wust et al., 2024) to encode transparent policies. To overcome the potentially unavailable concepts necessary to learn symbolic policies, Shindo et al. (2024) learn a mixture of symbolic and neural policies. Finally, concepts have further been used to derive reward from context using LLMs, as we did for _Kangaroo_ and _Skiing_(Kwon et al., 2023; Kaufmann et al., 2024; Wu, 2024; Shen et al., 2024).

The **misalignment problem** is an RL instantiation of the shortcut learning problem, a frequently studied failure mode that has been identified in models and datasets across the spectrum of AI from deep networks (Lapuschkin et al., 2019; Schramowski et al., 2020) to neuro-symbolic (Stammer et al., 2021; Marconato et al., 2023), prototype-based models (Bontempelli et al., 2023) and RL approaches (di Langosco et al., 2022). A misaligned RL agent, first empirically studied by (Koch et al., 2021) represents a serious issue, especially when it is misaligned to recognized ethical values (Arnold and Kasenberg, 2017) or if the agent has broad capabilities (Ngo, 2022). Nahian et al. (2021) ethically align agents by introducing a second reward signal. In comparison, ScoBots aid to resolve RL specific issues such as the misalignment problem through inherent interpretability.

## 6 Conclusion

In this work, we have provided evidence for the benefits of concept-based models in RL tasks, specifically for identifying issues such as goal misalignment. Among other things our proposed Successive Concept Bottleneck agents integrate relational concepts into their decision processes. With this, we have exposed previously unknown misalignment problems of deep RL agents in a game as simple as Pong. ScoBot agents allowed us to revise this issue, as well as different RL specific caveats with minimal additional feedback. Our work thus represents an important step in developing _aligned_ RL agents, _i.e._ agents that are not just aligned with the underlying task goals, but also with human user's understanding and knowledge. Achieving this is particularly valuable for applying RL agents in real-world settings where ethical and safety considerations are paramount.

Avenues for future research are incorporating a high level action bottleneck (Bacon et al., 2017). One can also incorporate attention mechanisms into RL agents, as discussed in (Itaya et al., 2021), or use language models (and _e.g._, the game manuals) to generate the additional reward signal, as done by Wu et al. (2024). Additionally, we are considering the use of shallower decision trees (Broelemann and Kasneci, 2019). An interesting research question is how far the task reward signal can aid in learning games object-centric representations (Delfosse et al., 2023) in the first place.

## Impact statement

Our work aims at developing transparent RL agents, whose decision can be understood and revised to be aligned with the beliefs and values of a human user. We believe that such algorithms are critical to uncover and mitigate potential misalignments of AI systems. A malicious user can, however, utilize such approaches for aligning agents in a harmful way, thereby potentially leading to a negative impact on further users or society as a whole. Even so, the inspectable nature of transparent approaches will allow to identify such potentially harmful misuses, or hidden misalignment.

#### Acknowledgments

This work has benefited from the HMWK projects "The Third Wave of Artificial Intelligence - 3AI", their joint support of the National Research Center for Applied Cybersecurity ATHENE, via the "SenPai: XReLeaS" project. It has also received support of Hessian.AI, as well as the Hessian research priority program LOEWE within the project WhiteBox, and the EU-funded "TANGO" project (EU Horizon 2023, GA No 57100431). The authors also would like to thank Stefan Lichtenstein, Raban Emunds for their help on refactoring the code, and Dwarak Vittal for his initial contributions.