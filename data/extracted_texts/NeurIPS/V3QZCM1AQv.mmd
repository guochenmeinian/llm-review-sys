# Reborn: Reinforcement-Learned Boundary Segmentation with Iterative Training

for Unsupervised ASR

 Liang-Hsuan Tseng En-Pei Hu Cheng-Han Chiang Yuan Tseng Hung-yi Lee Lin-shan Lee Shao-Hua Sun

National Taiwan University

Equal contribution. Correspondence to: Shao-Hua Sun <shaohuas@ntu.edu.tw>

###### Abstract

Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose

**Reborn**, **R**einforcement-Learned **B**oundary Segmentation with Iterative Training for Unsupervised ASR. Reborn alternates between **(1)** training a segmentation model that predicts the boundaries of the segmental structures in speech signals and **(2)** training the phoneme prediction model, whose input is the speech feature segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity. We conduct extensive experiments and find that under the same setting, Reborn outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech. We comprehensively analyze why the boundaries learned by Reborn improve the unsupervised ASR performance.

## 1 Introduction

Automatic speech recognition (ASR) systems convert speech signals into their transcription texts. Most state-of-the-art ASR systems are trained with dense supervision using a large amount of labeled speech-text paired data [10; 52]. However, more than 80% of languages in the world have limited access to speech-text paired data , and collecting such paired data requires intensive labor and costs, fundamentally limiting the applicability of supervised ASR systems to these languages. This leads to significant efforts devoted to developing unsupervised ASR (UASR) systems, which aim to learn the mapping between speech signals and textual transcriptions (words or phonemes) without any speech-text pairs [4; 11; 18; 30; 33].

UASR models learn to align the distribution of input speech signal and output text without paired data. Learning to match two distributions of sequences unsupervisedly has been extensively studied in unsupervised neural machine translation (NMT) , where the aim is to learn a neural network that can translate text in a source language to text in a target language without paired data [26; 27]. Prior works show that adversarial training  can be used to learn such mappings [26; 27], which employs a generator learning to translate, and a discriminator learning to distinguish generated text from the real text in the target language.

Can we adopt such an adversarial training scheme for UASR? In unsupervised NMT, text can be easily tokenized into sub-word tokens, so unsupervised NMT only needs to learn the mapping between the source and target language's token embeddings. However, in UASR, a phoneme or word is represented by a variable-length segment in the speech signal whose boundaries are unknown. Moreover, the length of a speech signal is much longer than the length of its textual transcription. The above characteristics of speech make learning the mapping between the segmental structures in speech and textual transcription challenging. Existing works in unsupervised ASR rely on handcrafted rules or separately learned modules to obtain the boundaries of the segmental structures [4; 33; 64]. Yet, such handcrafted boundaries are often sub-optimal, bottlenecking the performance of UASR.

This paper focuses on learning better segmental boundaries to improve the mapping between segmented speech and textual transcription. We propose **Reborn** (**R**einforcement-Learned **B**oundary Segmentation with **I**terative **T**raining), an unsupervised ASR framework with a segmentation model and a phoneme prediction model. The segmentation model determines segmental structure boundaries in speech signals, while the phoneme prediction model assigns a phoneme to each segmental structure. After properly initializing the phoneme prediction model, we use an iterative algorithm that alternates between two stages to train the segmentation model and refine the phoneme prediction model. The first stage trains the segmentation model. Since we do not have the ground truth segmental boundaries, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity. We experiment with various learning objectives and implement them as reward functions for learning the segmentation model. In the second stage, based on the boundaries predicted by the segmentation model learned in the previous stage, we use an adversarial loss similar to generative adversarial networks (GANs)  to train the phoneme prediction model.

We conduct extensive experiments on LibriSpeech , TIMIT , and Multilingual LibriSpeech (MLS)  to compare the phoneme/phone error rate (PER) and word error rate (WER) with prior works. We show that our method outperforms all prior UASR methods on LibriSpeech, TIMIT, and five languages in MLS when using the same amount of training data. We perform thorough ablation studies to show that iterative training and the rewards we design are critical to the performance of Reborn. By analyzing the segmental structure obtained by our segmentation model, we find that the segmental structures are acoustic units smaller than phonemes, which helps the phoneme prediction model predict more accurate transcriptions. To facilitate further research and replication, our code and models are available at https://github.com/andybi7676/reborn-uasr.

## 2 Related Work

An ASR model takes speech signals as the input and predicts the textual transcriptions. Unsupervised ASR (UASR) aims to train an ASR model without access to paired speech-text data. Instead, the only available training data is unlabeled speech data and unlabeled text data, while the correspondence between speech and text is not available. In this paper, we follow prior UASR works [4; 33] to predict phoneme transcriptions from speech signals. To achieve this, a lexicon is required to transform the unlabeled text into phoneme sequences. Learning the mapping between speech signals and phoneme sequences can be formulated as a distribution matching problem, where we want to learn an ASR model whose output phoneme sequences match the distribution of real phoneme sequences.

Yeh et al.  address this distribution matching problem using an unsupervised loss function based on empirical output distribution matching , which guides the ASR model to produce phoneme sequences with statistical distributions close to real phoneme sequences. Wang et al.  further extend this approach by matching the \(N\)-skipgram and positional unigram distributions.

Liu et al.  propose to use the generative adversarial network (GAN)  for UASR. The GAN solves the distribution matching problem by training a generator whose output distribution resembles a target distribution, and a discriminator is trained to distinguish the outputs of the generator and the samples from the target distribution. When using GAN to train unsupervised ASR, the generator is the phoneme prediction model. The phoneme prediction model takes in speech features of the segmental structures in the speech signal, and outputs a phoneme transcription. Prior works rely on hand-crafted or separately trained unsupervised phoneme segmentation models to find the boundary of the segmental structures [11; 33; 61].

With the emergence of foundation models, wav2vec-U  shows that UASR performance can greatly benefit from using speech foundation models as the feature extractor. However, wav2vec-U still relies on the two-stage feature preprocessing step based on \(k\)-means to find the boundaries of the segmental structure in the speech signal. In our paper, we call the segmentation boundaries obtained by the feature preprocessing in wav2vec-U as "\(k\)-means-based segmentation". EURO  changes the decoding algorithm used to decode the phoneme prediction sequences and explores other speech foundation models for feature extraction, including HuBERT  and WavLM .

In these prior methods, the segmentation is either non-trainable or learned separately from the phoneme prediction model. Reborn differs from prior works by introducing a trainable segmentation model tailored for the phoneme prediction model, and the segmentation model and phoneme prediction model can be iteratively polished. wav2vec-U 2.0  simplifies the feature preprocessing step in wav2vec-U and does not explicitly consider the segmentation structure in their model. Reborn has performance better than wav2vec-U 2.0 on almost all datasets, showing that learning the segmentation boundary for feature preprocessing is important for the performance of UASR.

Although recent UASR is originally to be a cross-modality distribution matching problem, it is also related to representation learning. For example, the speech features might be extracted from self-supervised speech foundation models; and the segmentation problem is also commonly investigated in tasks like acoustic unit discovery. We discuss these two related topics further in Appendix D.

## 3 Method: Reborn

The difficulty of mapping speech signals to their corresponding phoneme transcriptions lies in the segmental structures in speech whose boundaries are unknown. To tackle this challenge, we propose **Reborn**, which trains a UASR system with unpaired speech and text data. Reborn contains a segmentation model and a phoneme prediction model. The segmentation model takes the speech feature as input and determines the boundaries of the segmental structures in the speech signal, and the phoneme prediction model predicts a phoneme for each segmental structure. We will use the terms _segment_ and _segmental structure_ interchangeably. In our paper, we do not use the term _segment_ in the exact sense as in linguistics, where _segment_ refers to discrete units that can be identified in the stream of speech . We use _segment_ to broadly refer to a span in the speech, which may be a meaningful unit (_e.g._, a phone or word) or a span in the speech identified by the segmentation model.

The overall training process of Reborn is outlined in Figure 1. First, we initialize the phoneme prediction model using wav2vec-U (Section 3.3). Next, the training of Reborn iterates through two stages: **Stage 1** (Figure 1(a), Section 3.1): Training the segmentation model to learn a better segmentation until the segmentation model converges while fixing the phoneme prediction model. **Stage 2** (Figure 1(b), Section 3.2): Training the phoneme predictor based on the segment boundaries predicted by the segmentation model until the phoneme prediction model converges. The iterative process ceases when the UASR performance does not improve over the previous iteration. In UASR,

Figure 1: (a) and (b): Reborn iterates between using RL to train the segmentation model and using adversarial training to train the phoneme prediction model. (c): An illustration of the segmentation/boundary merging. 1 means the start of a segment while 0 is not. Given the original segmentation and the predicted phoneme sequence, we merge the segments that result in the same phoneme prediction into the same segment, yielding the merged boundary.

we cannot use PER as a validation metric to select the best model or determine when to stop training. We use an unsupervised evaluation metric to achieve it instead, detailed in Appendix C.4 and C.5.

### Stage 1: Training the Segmentation Model

#### 3.1.1 Segmentation Model

Given an input speech signal, we use a self-supervised feature extractor model to extract the speech features \(X=[x_{1},,x_{T}]\) from the waveform. The segmentation model takes the speech features and predicts the boundary of the segmental structure in the speech features. For each feature \(x_{i}\), the segmentation model assigns a binary value \(_{i}\), indicating whether \(x_{i}\) is the first frame of a segment. We split \(X\) into segments based on \(=_{1:T}\) and mean-pool the features within the same segment. The mean-pooled features \([s_{1},,s_{T^{}}]\) are forwarded through the phoneme prediction model to obtain the phoneme prediction \([y_{1},,y_{T^{}}]\) using greedy decoding. The resulting phoneme prediction will be de-duplicated into \([y^{}_{1},,y^{}_{M}]\) by removing the same consecutive phoneme.

The segmentation model is a one-dimensional CNN, denoted by \(_{}\) and parametrized by \(\). Given \([x_{1},,x_{T}]\), \(_{}\) predicts a probability \(_{}[i]\), \( i[1,,T]\). Let \(_{i}(_{}[i])\) be a Bernoulli random variable representing whether \(x_{i}\) is the beginning of a segment. \(_{i}=1\) means \(x_{i}\) is the beginning of a segment; \(_{i}=0\) means \(x_{i}\) is not the beginning of a segment. During training, we sample \(_{i}\) from Bernoulli\((_{}[i])\); during inference, we take \(_{i}=1\) if \(_{}[i] 0.5\), otherwise \(_{i}=0\).

#### 3.1.2 Training the Segmentation Model with RL

To help the phoneme prediction model predict phonemes, we want to train the segmentation model to capture the segmental structure in speech. However, the optimal segmentation boundary is not available for training. Recognizing that the segmentation quality directly affects the phoneme prediction of the phoneme prediction model, we estimate the quality of the phoneme prediction to guide the segmentation model training. The phoneme prediction model is from the previous Reborn iteration or initialized from wav2vec-U in the first Reborn iteration, and it is fixed when training the segmentation model. Given that the feature segmentation based on boundary decision is inherently non-differentiable, we leverage RL to train the segmentation model. While related works may employ techniques like soft monotonic alignment or straight-through estimator to approximate gradients , we find that RL provides a natural and suitable approach for our scenario, where the estimated quality of the phoneme sequence across different segmentations serves as the RL reward.

We train the segmentation model using the policy gradient method  based on the REINFORCE algorithm . For each utterance, we calculate an utterance-wise reward \(R\) (defined in Eq. 4 in the next subsection) from the de-duplicated phoneme prediction to train the segmentation model. Based on the policy gradient method, the segmentation model \(_{}\) is optimized using the following gradient: \(_{_{}}[_{}_{}( |X)R]\), where \(\) is taken over \(\) sampled from \(_{}\) and approximated with the mean of a batch of training sequences.

#### 3.1.3 Reward Designs

Given an utterance in the training set, the utterance-wise reward \(R\) is the weighted sum of **perplexity difference reward**\(R_{}\), **edit-distance reward**\(R_{}\), and **length difference reward**\(R_{}\).

We introduce some notations: Given an utterance, we use the currently trained segmentation model \(_{}\) for segmentation and the phoneme prediction model trained in the last Reborn iteration to obtain the de-duplicated phoneme prediction sequence \(Y^{}_{}\). For the same utterance, we use the segmentation model from the **previous**Reborn iteration \(_{-1}\) for features segmentation and the same phoneme prediction model to obtain another de-duplicated phoneme prediction sequence, denoted as \(Y^{}_{-1}\). In the first Reborn iteration, \(Y^{}_{-1}\) is the de-duplicated phoneme prediction from wav2vec-U.

**Perplexity Difference Reward.** The perplexity difference reward is designed to favor phoneme segmentation better than the segmentation learned in the previous iteration. Intuitively, a better phoneme segmentation prediction \(_{1:T}\) should yield a more reasonable phoneme prediction \(y^{}_{1:M}\). We use perplexity (PPL), the negative likelihood of a phoneme sequence scored by a phoneme language model (LM), to evaluate how reasonable a phoneme sequence is. Perplexity measures how likely a 

[MISSING_PAGE_FAIL:5]

### Stage 2: Training the Phoneme Prediction Model

The phoneme predictor takes the mean-pooled features of segmental structures \(S=[s_{1},s_{2},,s_{T^{}}]\) and predicts a phoneme sequence \(Y=[y_{1},y_{2},,y_{T^{}}]\). \(S\) are obtained by mean-pooling the features \(X\) in the same segmental structure based on \([^{{}^{}}_{1},,^{{}^{}}_{T}]\), where \(^{{}^{}}_{i}\) is the _merged_ boundaries. We find that it is effective to perform boundary merging to stabilize the training in this stage.

After obtaining the merged phoneme boundaries, we train the phoneme predictor model using GAN training. The phoneme prediction model is the generator in GAN training. The generator aims to output phoneme predictions that look like real phoneme sequences to fool the discriminator. The discriminator takes in a phoneme sequence, which can be the output of the generator or a phoneme sequence from the unpaired text corpora, and the goal of the discriminator is to distinguish whether the input phoneme sequences are outputs of the generator. The generator and discriminator are updated using the loss in GAN training. In this stage, the parameters of the segmentation model are not updated, and the generator is initialized from the previous iteration. We discuss the effect of applying boundary merging and parameter initialization in Appendix A.1

### Initialization of Reborn

In Stage 1, the segmentation model depends on the phoneme prediction when calculating the rewards. As a result, Reborn cannot work without properly initializing the phoneme prediction model. We use wav2vec-U to train a phoneme prediction model and use it as the initialization for the phoneme prediction model in Reborn. We briefly introduce wav2vec-U in Appendix B.

## 4 Experiment Setup

### Dataset

We use three datasets commonly used in ASR to evaluate the performance of Reborn.

**LibriSpeech** is an English speech recognition corpora that contains 960 hours of training data. Following EURO , we use 100 hours of audio from the train-clean-100 set as the unlabeled speech data. The unlabeled text data is derived from the remaining 860 hours, which does not overlap with the transcription of the unlabeled speech data.

**TIMIT** is another English speech recognition with the human-labeled phone boundary. We follow the _matched_ setting, which is more broadly used [4; 18; 33], where the speech and text data come from the same set of utterances.

**Multilingual LibriSpeech (MLS)** is an ASR dataset including _German (de)_, _Dutch (nl)_, _French (fr)_, _Spanish (es)_, _Italian (it)_, and _Portuguese (pt)_. Following , we randomly sample and use 100 hours of speech data for each language and use the LM data provided by the dataset as unpaired text.

### Evaluation Metrics

We use phoneme error rate (PER) and word error rate (WER) to evaluate the ASR performance. If not specified, we use greedy decoding to obtain phoneme-level results. For decoding word-level outputs, we perform WFST decoding [38; 39] using PyKaldi . We leave the details in Appendix C.7.

### Implementation Details

For the English datasets, we use wav2vec 2.0  to extract speech features from speech signals; for MLS, we use XLSR-53  as the feature extractor. Both of them can be found in fairseq . The 4-gram phoneme LM is derived using KenLM . We describe more details about the data preparation and LM formulation in Appendix C.1, which basically follows wav2vec-U . All the trainable models, including the segmentation model, the phoneme prediction model, and the discriminator in GAN training, are composed of one-dimensional CNN layers, as detailed in Appendix C.3. For LibriSpeech and TIMIT, we train Reborn for two iterations. For MLS, we only train one iteration since we do not find the performance to improve in the second iteration.

## 5 Results

### Main Results

We show the PER and WER of LibriSpeech, TIMIT, and MLS in Table 1, Table 2, and Table 3. We compare Reborn with several prior UASR works: wav2vec-U , wav2vec-U 2.0 , and EURO . We have the following observations:

**Reborn significantly outperforms all prior methods on LibriSpeech.** Our experiment on LibriSpeech follows Gao et al.  to use the 100-hour training split of LibriSpeech as the unlabeled speech data. Compared with all the baselines in Table 1, Reborn achieves the lowest PER and WER. wav2vec-U and EURO both use hand-crafted rules to obtain the segmental boundaries, while wav2vec-U 2.0 removes the feature segmentation steps. The superior performance of Reborn illustrates the importance of learning the segmental boundaries tailored for the phoneme prediction model. Notably, without using HMM self-training, Reborn already has PER/WER lower than wav2vec-U 2.0 with HMM self-training, which is the prior state-of-the-art (SoTA) method on LibriSpeech. In Appendix A.4, we further apply the Reborn pipeline with speech foundation models other than wav2vec 2.0. We find that Reborn yields notable performance improvement when using HuBERT  or WavLM  as the feature extractor, illustrating the generalizability of our method.

**Self-training further improves PER/WER of Reborn.** Self-training uses the phoneme predictor's prediction as the pseudo-label and trains a new phoneme prediction model using the pseudo-label as the training data. It is commonly used in UASR to boost the performance . In Table 1, we show that Reborn can be integrated with Hidden Markov Models (HMM) self-training (Appendix C.2) to further lower the PER/WER. We reach a new SoTA on LibriSpeech under the setting of 100-hour speech data, outperforming the prior SoTA in PER and WER by 5% and 6%, respectively. Surprisingly, Reborn with HMM self-training outperforms training wav2vec-U with the oracle boundary. This shows that Reborn can be combined with self-training to improve performance effectively. Due to limited computation resources, we only conduct self-training on LibriSpeech.

**Reborn outperforms all prior UASR methods on TIMIT.** Table 2 shows the PER of TIMIT, and we again find that Reborn achieves the lowest PER compared with all prior UASR methods. This indicates that Reborn not only works on large datasets like LibriSpeech but also works on small datasets like TIMIT, which only contains about 3 hours of audio data for training. Even using greedy decoding only, Reborn outperforms the prior best-performing UASR model (row (e)), which relies on prefix decoding. EURO shows that replacing the feature extractor with WavLM  (row (e)) outperforms using wav2vec 2.0 (row (d)) as the speech feature extractor. Since Reborn using wav2vec 2.0 already outperforms EURO with WavLM, we leave changing the feature extractor in Reborn on TIMIT as future work.

    &  \\   & dev-clean & dev-other & test-clean & test-other \\ 
**With Oracle Boundary** & & & & \\ Train from oracle & 6.3/12.8 & 9.7/16.3 & 6.4/12.7 & 10.0/16.8 \\ 
**Baseline** & & & & \\ EURO (HuBERT) & 15.2/23.1 & 20.7/29.3 & 15.1/22.8 & 21.1/29.8 \\ wav2vec-U\({}^{}\) & 19.3/20.4 & 22.9/25.6 & 19.3/21.0 & 23.2/25.2 \\ wav2vec-U 2.0\({}^{}\) & 12.2/17.2 & 16.3/21.7 & 12.6/17.7 & 16.3/22.2 \\ wav2vec-U 2.0 + HMM ST\({}^{}\) & 10.0/15.4 & 13.1/19.0 & 10.3/16.0 & 13.1/19.6 \\ 
**Our Method** & & & & \\ Reborn & **8.3/12.5** & **11.9/17.6** & **8.9/13.1** & **12.5/18.7** \\ Reborn + HMM ST & **5.2/9.3** & **8.5/13.5** & **5.4/9.6** & **8.5/13.7** \\   

Table 1: PER/WER on LibriSpeech using 100 hours speech data. \(\): Our reproduction. (wav2vec-U and wav2vec-U 2.0 only report results of using 960 hours of unlabeled speech). HMM ST indicates HMM self-training (Appendix C.2).

    &  &  \\   & &  &  &  &  \\  (a) Train from oracle & ✓ & 9.1 & 10.3 & 9.2 \\  (b) wav2vec-U (reproduced) & ✓ & 20.2 & 22.2 & 20.3 \\ (c) wav2vec-U+WEST & ✗ & 17.1 & 17.8 & 16.8 \\ (d) EURO (wav2vec 2.0) & ✗ & 18.5 & 19.8 & - \\ (e) EURO (WavLM) & ✗ & 14.3 & 14.6 & - \\  (f) Reborn & ✓ & **12.4** & **13.5** & **12.4** \\   

Table 2: PER results on TIMIT. The cross-mark (✗) in the greedy-decoding column indicates that an additional LM (4-gram) is used during decoding. Reborn reaches the best performance with no LM used for decoding, showing that Reborn can benefit from the external LM via RL.

**Reborn performs well on languages other than English.** The results on MLS are presented in Table 3. We find out that a single iteration is enough for performance convergence on MLS. Recall that Reborn is initialized from wav2vec-U, but Reborn outperforms wav2vec-U by a large margin in all six languages. This large performance gap underlines the importance of learning the boundaries of the segmental boundaries. Moreover, Reborn outperforms wav2vec-U 2.0 except for Portuguese, and the average WER on the six languages is 3.9% lower than wav2vec-U 2.0.

### Boundary Analysis

The core of Reborn is the segmentation model that learns the segmental structure's boundaries. In this section, we look at how different segmental boundaries affect the performance of phoneme prediction. We compare the boundaries obtained by four methods: (a) the oracle phoneme boundaries obtained by forced alignment , which requires paired speech-text data to learn the alignment; (b) the \(k\)-means-based segmentation boundary in wav2vec-U; (c) the phoneme boundary obtained by the SoTA unsupervised phoneme segmentation method  on LibriSpeech; (d) the boundaries learned by the segmentation model of Reborn in the first iteration **before** boundary merging.

First, we focus on the PER in Table 4 when using different segmentation methods. The PER is obtained by taking the four different segmented features to the identical phoneme prediction model: the phoneme prediction model trained using the \(k\)-means-based segmentation of wav2vec-U. Note that the phoneme prediction model here is **unsupervised** and **non-ideal**. We find that simply replacing the \(k\)-means-based boundary with the oracle phoneme boundary reduces the PER by 5.5%, showing the imperfect hand-crafted \(k\)-means-based segmentation is the bottleneck of UASR. Next, even when using the boundary predicted by the SoTA unsupervised phoneme segmentation , the PER does not reduce. Conversely, the boundary learned by Reborn achieves the lowest PER. This is because the boundary learned by Reborn is directly tailored for the phoneme prediction model.

Next, we discuss the phoneme boundary results in Table 4. The phoneme boundary results is evaluated with boundary precision, recall, and F1 with a 20ms tolerance window, following the _harsh_ scheme from Strgar and Harwath . We leave the discussion about the different schemes for boundary evaluation in Appendix E. We also report the average number of segments per second (Freq. in Table 4). Interestingly, the boundary F1s of the oracle boundary (row (a)) and Strgar and Harwath  (row (c)) are much higher than the boundary F1 of Reborn (row (d)), but the PER of Reborn is much better. This is because Reborn's segmentation model predicts more segments than the number of phonemes in the utterance (before boundary merging), indicating that it learns some segmental structures smaller than the phones. This can be seen from the lower precision (0.57) and higher frequency (16.29) compared to oracle boundaries. However, segmenting the speech feature into smaller units is not problematic because even if the consecutive speech features of the same phonemes are split into two segments by the segmentation model, as long as they have the same phoneme prediction, the duplicated phoneme prediction will be removed in the de-duplication step, and thus not affect the PER.

    & Prec. & Rec. & F1 & Freq. & PER\% \\  (a) Oracle & 1.0 & 1.0 & 1.0 & 11.89 & 13.8 \\ (b) \(k\)-means-based & 0.64 & 0.77 & 0.70 & 14.27 & 19.3 \\ (c) Strgar and Harwath  & **0.75** & 0.75 & **0.75** & 12.26 & 23.2 \\ (d) Reborn & 0.57 & **0.78** & 0.65 & 16.29 & **12.9** \\   

Table 4: Boundary evaluation results of different segmentation methods on LibriSpeech test-clean split. The second-last column (Freq.) is the number of segments per second. All the methods share the same phoneme prediction model trained with the \(k\)-means-based segmentation of wav2vec-U.

    &  \\   & de & nl & fr & es & it & pt & Avg. \\  wav2vec-U\({}^{}\) & 32.5 & 40.2 & 39.8 & 33.3 & 58.1 & 59.8 & 44.0 \\ wav2vec-U\({}^{*}\) & 33.9 & 38.1 & 37.7 & 33.1 & 51.8 & 59.4 & 42.3 \\ wav2vec-U 2.0\({}^{}\) & 23.5 & 35.1 & 35.7 & 25.8 & 46.9 & **48.5** & 35.9 \\  Reborn & **20.9** & **26.9** & **28.2** & **24.7** & **39.9** & 51.5 & **32.0** \\   

Table 3: WER on MLS. \(\): Results from Baevski et al. . \(*\): Our reproduction of wav2vec-U, used as the initialization of the phoneme prediction model in Reborn. \(\): Results from Liu et al. .

### Ablation Study

#### 5.3.1 Reward Design and Behavior Cloning

We verify the effectiveness of our three proposed reward functions and behavior cloning initialization in Table 5. While the perplexity difference reward alone is sufficient to guide the segmentation model, adding edit distance and length difference rewards improves performance further. As previously mentioned, using only the perplexity difference reward will make the segmentation model predict segments overly focusing on lowering PPL. And a phoneme sequence with a lower perplexity may not always be a better transcription. To deal with such overfitting problem, we design the two regularization rewards, namely, \(R_{}\) and \(R_{}\), to make the model learn to segment for lower perplexity while grounded on a reasonable transcription from the previous iteration. Our results in Table 5 further evidence the effectiveness of the two rewards. Additionally, we observe that removing BC leads to a decline in PER compared to using BC (see row (d) vs. row (c)). To deepen this analysis, we present the PER curve on the LibriSpeech _test-clean_ set with and without BC. The results in Figure 2 indicate that BC helps enhance performance and accelerate convergence.

#### 5.3.2 Iterative Training

In Figure 3, we show the PER on LibriSpeech test-clean split during the iterative training process of Reborn. We observe that after training the segmentation model in the first iteration, the PER drastically drops by 6.4% compared with wav2vec-U, which is used to initialize the phoneme prediction model in iteration one. This shows that the quality of the segmental structure's boundaries is important to the performance of UASR. Afterward, the PER of Reborn decreases steadily across iterative training, showing that training the segmentation model and the phoneme prediction model is critical to improving the performance of Reborn. We find that the PER does not drop significantly after the fifth iteration. Last but not least, we provide more evidence in Appendix A.3 to showcase that the phoneme predictors are iteratively polished.

   Boundary method & Prec.  Rec. & F1 & R-val. & PER\% \\  \(k\)-means-based & 0.62 & 0.75 & 0.68 & 0.68 & 20.3 \\ Stygar and Harwath ” & 0.85 & 0.79 & 0.82 & 0.84 & - \\  Reborn & 0.61 & 0.83 & 0.71 & 0.62 & 12.4 \\ + boundary merging & 0.80 & 0.78 & 0.79 & 0.82 & - \\   

Table 6: The boundary evaluation results on TIMIT. Reborn achieves better boundary evaluation scores than the original \(k\)-means-based method, and is comparable with Strgar and Harwath ’s after boundary merging. \(*\): from literature. All the metrics are calculated on the _test-all_ split.

    &  &  \\    & Stage 1 & Stage 2 & (Stage 1) \\  (a). \(R_{}\) & 14.7 & 12.9 & 10.0 \\ (b). \(R_{}+R_{}\) & 13.8 & 12.1 & 10.8 \\ (c). \(R_{}+R_{}+R_{}\) & **12.9** & **11.9** & 11.2 \\  (d). Reborn w/o BC & 14.9 & 14.2 & 13.8 \\   

Table 5: We compare different reward functions and the effect of BC initialization with first iteration results on LibriSpeech test-clean. Row (c) is Reborn.

Figure 3: PER of each stage during Reborn’s two-stage iterative training on the test-clean split of LibriSpeech. St.: stage; w2vu: wav2vec-U.

Figure 2: PER across training epochs on the test-clean split of LibriSpeech. BC pretraining speeds up convergence and raises performance.

#### 5.3.3 Boundary Evaluation on TIMIT

In Section 5.2, we demonstrate that Reborn segmentation delivers fine-grained segments, achieving the highest performance gain among segmentation methods given the same non-ideal phoneme prediction model. Here, we extend our boundary evaluation results to TIMIT, a smaller dataset with human-annotated phone boundaries, to provide more comprehensive boundary insights. Table 6 shows that the initial boundaries learned by Reborn, optimized explicitly for the phoneme prediction model, already achieve a high recall. With boundary merging, _i.e_., consecutive segments with the same phoneme prediction are merged as illustrated in Figure 1-(c), Reborn achieves results close to Strgar and Harwath 's. In line with the previous context, we emphasize that Reborn achieves substantial performance improvement in PER by tailoring segmentation to the non-ideal unsupervised phoneme prediction model, rather than solely aiming for higher boundary evaluation scores. This represents a critical and intriguing finding in our work.

## 6 Discussion

In unsupervised ASR, learning the mapping between the segmental structures in speech and their corresponding transcription is challenging, especially without paired data. To tackle this, we propose Reborn, an iterative training method for unsupervised ASR. Reborn iteratively trains the segmentation model and the phoneme prediction model to improve the ASR performance. The segmentation model is trained using RL with carefully designed rewards to guide the segmentation model to find the boundaries of the segmental structures in speech signals. Extensive experiments on three datasets spanning seven languages show that Reborn outperforms prior best-performing unsupervised ASR models in all datasets except one language in MLS. We conduct comprehensive ablation studies to show that each component in Reborn is critical to the performance. We also explain the effectiveness of Reborn by analyzing its segmentation patterns and find that Reborn tends to produce segmental structures smaller than phones, which helps the generators predict the phoneme transcription better.

**Limitations.** Recent advancements in unsupervised ASR are still in the developmental stages, and we have not yet implemented Reborn in more realistic scenarios, such as low-resource languages or noisy environments. Additionally, the iterative training paradigm may amplify any existing biases in the dataset, an aspect that remains unexplored in this study. Furthermore, if the phoneme prediction model is poorly initialized, Reborn may not be able to provide huge performance improvements. In this work, lexicons are required to perform phonemicization on the unpaired text. Consequently, learning the unsupervised ASR system directly from speech to word-level tokens remains an open problem, where recent works aim to tackle [30; 41].

**Broader impacts.** Unsupervised automatic speech recognition (UASR) - predicting the textual transcriptions for the given speech signals using only unpaired speech and text data - is highly attractive. The existence of thousands of low-resourced languages spoken globally is the major reason. This paper presents Reborn, a novel unsupervised ASR training algorithm. We foresee that Reborn has great potential to make low-resource languages more accessible to train ASR systems, making speech technology more accessible and boundaryless.