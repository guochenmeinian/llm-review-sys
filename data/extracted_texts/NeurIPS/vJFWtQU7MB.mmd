# Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen New York University

Hidden in the Noise: Two-Stage Robust Watermarking for Images## Abstract

As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.

In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.

## 1 Introduction

Generative AI is capable of synthesizing high-quality images indistinguishable from real ones. This capability can be used to deliberately deceive. These fake image generations, called deepfakes, have the potential to cause severe societal harms through the spread of confusion and misinformation (Peebles and Xie, 2022; Esser et al., 2024; Chen et al., 2024; Ramesh et al., 2021). In addition, owners of different models and images may want to control the spread of their derivatives for copyright reasons and safeguard their intellectual property. One way to mitigate these harms is model watermarking. The study of watermarking has a rich history and has recently been adopted for AI-generated content (Pun et al., 1997; Langelaar et al., 2000; Craver et al., 1998). For an extended discussion of recent work in this area, we direct the reader to Appendix B. Unfortunately, most current image watermarking methods are not robust to watermark removal attacks utilizing image diffusion generative models (Zhao et al., 2023a).

Recently, new watermarking methods utilize the inversion property of DDIM to achieve more robust watermarking (Wen et al., 2023; Ci et al., 2024; Yang et al., 2024b). These methods embed patterns in a diffusion model's initial noise and then detect them in the noise pattern reconstructed from the generated image. This technique provides strong robustness against various attacks, making it effective at resisting watermark removal attempts. Yet, these prior methods are themselves vulnerable to new types of attacks. Tree-Ring Wen et al. (2023) add a pattern to the initial noise,making it distinct from a random Gaussian initial noise in a way that an attacker can detect (Yang et al., 2024). This may enable forgery attacks, aimed at applying the watermark without the owner's permission. Such attacks are often even more concerning than removal attacks, as they can cause severe damage to model owners if their watermark is associated with illegal content.

Therefore, there is a need for image watermarking methods that generate images that are not distinct from non-watermarked images (to anyone but the model owner). As suggested by previous works, since the model already takes random noise as initialization, we may initialize it with a pseudo-random noise pattern that we can detect later (Yang et al., 2024; Kuditipudi et al., 2023). Namely, reconstructing an approximation of the _initial noise_ used in the diffusion process from a given image allows the detection of the noise pattern used by the model. Although this reconstructed noise is not completely identical to the used _initial noise_, it is much more similar to the initial noise than it is to other randomly distributed noise patterns. Thus, it can serve as a watermark that can be identified in the generated images (see also Appendix C for similar ideas used in previous works).

While using a pseudo-random initial noise does not distort the distribution of single-generated images, it may carry information about the watermark when groups of images are examined together. Specifically, works such as Yang et al. (2024) embeds the watermark in an initial noise such that

Figure 1: **The WIND method for robust image watermarking. The method is designed to use \(N\) possible initial noises splitted to \(M\) groups. Generation: Using a secret salt and an index \(i^{*}\), we securely and reproducibly generate initial noise \(_{i^{*}}\). We then embed a group index \(g^{*}\) of that noise to make easier retrieval possible and embed it using a Fourier pattern. Finally, we run diffusion with the embedded latent noise to produce a watermarked image. Detection: We reconstruct the initial noise \(}\). Next, we search over the possible group indices \(g\) for the closest Fourier pattern to the one embedded in \(}\). We then look over initial noises in group \(\) to find the match.**

the resulting generated image comes from the same distribution as non-watermarked images. Yet, when many images generated from the same noise pattern are examined together, the correlation between them may expose that they are not distortion-free as a set. E.g., the average of many similarly-watermarked images may differ from the average of non-watermarked ones (Yang et al., 2024a). A natural solution to this distortion of sets is to use more than one initial noise for each watermark we deploy.

Yet, given a sufficiently small set of initial noises (denoted as \(N\)) and an enormous number of images generated by a model, an attacker could potentially still collect many images sharing the same initial noise in order to perform removal and forgery attacks as was applied to previous methods (Yang et al., 2024a). Using many initial noises (a large value of \(N\)) will make such attacks much more difficult, if not infeasible. Surprisingly, we find that a very large number of random initial noises remain distinguishable from one another, even after reconstructing the noise from a generated image. However, a large value of \(N\) might incur a negative effect on the runtime and accuracy of the approach. In order to lower the effective quantity of noises we need to scan at detection while retaining strong robustness, we propose a two-stage efficient watermarking framework. We supplement our \(N\) initial noise samples with \(M\) Fourier patterns as a _group identifier_ - a unique identifier of a subset of initial noises we might have used for generating a given image (Figure 1). During detection, we may first recover the group identifier (stage 1) and use it to find an exact match (stage 2). Thus, we reduce our search space to the number of initial noises per group (\(N/M\)).

Our key contributions are as follows:

1. We demonstrate that the initial noise used in the diffusion process is itself a distortion-free watermarking method for images (Section 3).
2. We present WIND, our two-stage method for effectively using the initial noise as a watermark (Section 4).
3. We demonstrate that WIND achieves state-of-the-art results for its robustness to removal and forgery attempts (Section 5).

## 2 Preliminaries

### Threat Model

In a watermarking scheme we usually consider the owner, trying to mark images as an output of their model; and an attacker, trying to remove or forge the watermark on unrelated images.

**The Owner** releases a private model (diffusion model in our case) that clients can access through an API, allowing them to generate images that contain a watermark. The watermark is designed to have a negligible impact on the quality of the generated images. There are a few settings regarding the watermark detection, including public infomation and private information watermarking (Cox et al., 2007; Wong and Memon, 2001). We focus on the setting where the watermark is detectable only by the owner, enabling them to verify whether a given image was generated by their model using private information.

**The Attacker** uses the API to generate an image and subsequently attempts to launch a malicious attack aimed at either removing or forging the embedded watermark, with the intention of using the image or watermark for unauthorized purposes.

### Diffusion Models Inversion

Diffusion model inversion aims to find the reconstructed noise representation of a given data point, effectively reversing the generative process. Let \(T\) be the number of diffusion steps, in both the generation and inversion processes. In the standard generation process, we start with noise \(}_{T}\) drawn from an appropriately scaled Gaussian and iteratively apply \(}_{t}=}_{t+1}+_{}(}_{t+1})\), where \(_{}\) is a trained model that predicts the noise to be removed and \(t[T]\) is the time step describing how much noise should be removed in each stage. Conversely, the inversion process begins with a data point \(}_{0}\) and moves towards its reconstructed noise representation by applying \(}_{t+1}=}_{t}-_{}( }_{t})\). This process relies on the assumption that \(_{}(}_{t+1})_{}(}_{t})\), allowing us to approximately invert the diffusion process by adding the predicted noise (Ho et al., 2020; Song et al., 2022). DDIM's efficient sampling allows this technique to be particularly useful (Song et al., 2022).

### Tree-Ring and RingID Watermarks

In order to watermark images in a human-imperceptible and robust way, previous works have encoded specific patterns in the Fourier space of the initial noise. _Tree-Ring_(Wen et al., 2023) first transforms the initial noise into the Fourier space. A key pattern is then embedded into the center of the transformed noise. The noise is subsequently transformed back into the spatial domain. During the detection phase, the diffusion process is inverted, and the Fourier domain is examined to verify the presence of the imprinted pattern. _RingID_(Ci et al., 2024) shows that Tree-Ring struggles to distinguish between different keys. Therefore, the number of unique keys (distinguishable from one another) that can be embedded with Tree-Ring is low. They increase the possible number of unique keys that can be encoded using Fourier patterns.

Figure 2: Cosine similarity distribution between initial noise, and: (i) a noise reconstructed from a watermarked image (reconstructed noise) (ii) a noise reconstructed from a forged image using a public model to imitate our watermarked image (reconstruction attack, described in Section 3). (iii) Random noise. These results are reliant on the approximate inversion of DDIM without the ground-truth prompt.

**Systematic Distribution Shifts in Generated Images Enable Attacks.**  Systematic distribution shifts in the generated content make it easier to verify the existence of a watermark. However, in the case of Tree-Ring and other watermarking techniques, it also opens up an avenue of attack (Wen et al., 2023; Yang et al., 2024; Xian et al., 2024; Bui et al., 2023). Emblematic is the method of Yang et al. (2024a), whose attack approximates the difference between watermark and non-watermarked images. Increasing the number of images with the watermark can improve the accuracy of the approximation. The impact of distribution shifts is significant, as the attack remains effective even when the watermarked and non-watermarked images are not paired (Figure 3).

```
1:Input:\(N\): number of initial noises, \(M\): number of groups, \(s\): secret salt, \(p\): prompt, \(\): private model weights
2:Sample initial noise index \(i^{*}([N])\)
3:Compute group identifier \(g^{*}=i^{*}\%M\)\(\) Modulus of initial noise index
4:Calculate embedding of the group identifier \(g_{emb}(g^{*})\)
5:Securely generate \(=(i^{*},s)\)\(\) Apply cryptographic hash function
6:Sample \(_{i^{*}}(,)\) from a pseudorandom generator with seed
7:Add the identifier embedding \(g_{emb}(g^{*})\) to \(_{i^{*}}\) to get \(_{i^{*}\_emb}\)
8:return\(=G_{}(_{i^{*}\_emb},p)\)\(\) Diffusion process \(G\) with weights \(\) ```

**Algorithm 1** Generation Algorithm

```
1:Input: image: (possibly) watermarked image, \(N\): number of initial noises, \(M\): number of groups, \(s\): secret salt, \(\): private model weights, \(\) : threshold for detection
2:Recover reconstructed noise \(}=G_{}^{ 1}()\)\(\) Inverse diffusion with private weights
3:Extract closest group identifier \(\) from group identifier embedding in \(}\)
4:for\(i[N]\) such that \(i\%M=\)do\(\) Search over subset of initial noise indices
5:Build initial noise \(_{i}\) using secret salt \(s\) and \(\)\(\) As in Algorithm 1
6:Compare \(_{i}\) to \(}\) after removing Fourier embedding \(\)
7:endfor
8:if any noises are closer than threshold \(\)then
9: Declare "watermarked"
10:else
11: Declare "not watermarked"
12:endif ```

**Algorithm 2** Detection Algorithm _(WIND_fast)_

## 3 Initial Noise is a Distortion Free Watermark

Watermarks which systematically perturb the distribution of image generations are more vulnerable to removal and forgery attacks. A distortion-free watermarking method, by contrast, is more robust (Kuditipudi et al., 2023). Our first finding is that the initial noise already in standard use in diffusion models can be such a watermark.

Let \(N\) be the number of initial noises we can generate. We will secure our watermarking process with a long, secret salt \(s\). We begin by sampling a random (and reproducible) initial noise. Let \(i^{*}([N])\) be the index of the initial noise. We will use a hash function to get a seed \((i^{*},s)\). Plugging the seed into a pseudorandom generator, we generate a reproducible initial noise vector\(_{i^{*}}(,)\) drawn from a centered Gaussian distribution. When we generate fewer than \(N\) images, we can use each initial noise at most once and the noise appears distortion-free. We discuss the case when the number of images exceeds \(N\) in Appendix F.

**Empirical validation of initial noise watermarking.** To empirically validate our claim that the initial noise can serve as a watermark, we compute the cosine similarity between the initial noise \(_{i^{*}}\) and (i) random noise \((,)\), (ii) the reconstructed noise \(}\) when we have access to the private model weights, and (iii) the reconstructed noise \(}^{}\) from an image imitating our noise pattern without access to the private model weights. The imitation attempt is done by inversing our watermarked image back into noise, and generating a new image from it; where both steps are done using a public model as described in _reconstruction attack_ below (we used Stable Diffusion-v2 (Rombach et al., 2022) for the experiment, as it is the most similar model to our watermarking model).

During the watermarking process, we create an image image through diffusion with the private model weights \(\) conditioned on a private text prompt \(p\). Formally, \(=G_{}(_{i^{*}},p)\). We obtain the reconstructed noise that we use for detection via an inverse diffusion process \(G^{-1}\). Formally, \(}=G_{}^{-1}()\).

**Reconstruction Attack.** An attacker trying to forge the watermarked image will not have access to our private weights, instead they will have some other weights \(^{}.\) Using the same starting watermarked image, they will attempt to recover the initial noise. Let \(}^{}=G_{^{}}^{-1}()\). Then, with this initial noise, they will generate a forged image with (possibly offensive) text prompt \(p^{}\), producing \(^{*}=G_{^{}}(}^{},p^{})\). Finally, the model owner will attempt to detect whether the forged image is watermarked by applying the inverse diffusion process with the private model weights to the forged image. Let \(}^{}=G_{}^{-1}(^{*})\). As an upper bound on the capability of this attack, we perform it with the same prompt. Also, Keles and Hegde (2023) demonstrates that inverting a generative model is a significantly challenging task.

Strikingly, we find that the similarity between the true noise and the noise reconstructed with the model weights is almost always greater than a relatively large threshold \(=0.5\) (\(p\) value \(<10^{-3}\), Figure 2). At the same time, the reconstructed similarity from the image made by an attacker using the reconstruction attack \((_{i^{*}},}^{})\), along with the similarity to random vectors \((_{i^{*}},)\) are both much smaller. Namely, they are respectively \(z=5.3\) and \(z=9.4\) standard deviations

Figure 3: Detection accuracy for forgery and removal attacks using Yang et al. (2024a). A value of 0 represents complete failure (the attacker successfully removed the watermark or forged it onto another image), while 100 indicates perfect defense (no watermark removal or forgery occurred).

away from the mean (Table 4). Taken together, these results mean that the probability \(p\) of a non-watermarked image mistakenly labeled as watermarked is very low in both cases. For the random noise, the probability to confuse it is as the initial noise is \(p<e^{(}{2^{2}})}<10^{-19}\), allowing practically a perfect distinction between any pair of unrelated noises.

**Runtime considerations.** Our method requires searching over all \(N\) watermarks, leading to a naive runtime complexity of \((N)\). However, more efficient algorithms for similarity-based search, such as HNSW (Malkov and Yashunin, 2018), can reduce this complexity to \(( N)\), at the expense of additional memory usage. We provide empirical runtime analysis of our method in Appendix H. For large enough values of \(N\), this cost may eventually become undesirable. Together with our aim to maintain high robustness with an increasing number of keys, it motivates a more efficient method, which is presented in the next section.

## 4 Method

### Wind: Two-stage Efficient Watermarking

While always using a single initial noise for our model might imply good robustness properties, to make forgery and removal more difficult, it is generally preferable to maintain a large set of \(N\) initial noises to be used by the model. More importantly, using a large number of different noises \(N\) may serve as different keys, encoding some metadata about each image. This metadata might include information about the specific model that generated it, as well as additional information about the generation for further validation of the image source, once detected.

In order to make the search over a large number of noises more efficient, we introduce a two-stage efficient watermarking approach we name _WIND_ (**W**atermarking with **I**ndistinguishable and Robust **N**oise for **D**iffusion Models). First, we initialize \(M\) groups of initial noise, each group associated with its own Fourier-pattern key. In contrast to prior work, we employ these Fourier patterns not as a watermark, but as a _group identifier_ to reduce the search space.

For each image generation, we randomly select an index for the initial noise, denoted as \(i^{*}[N]\). We use a group identifier \(g^{*}=i^{*}\%M\), where % denotes the modulus operation. We embed \(g^{*}\) in the Fourier space of the latent noise (similar to Wen et al. (2023)). During detection, we reconstruct the latent noise and find the group identifier \(\) that is closest to the Fourier pattern embedded in the image. We then search over all indices \(i\) such that \(=i\%M\). In this way, the search space has a size of \(N/M\) rather than \(N\). We include an algorithm box for generation (Algorithm 1) and detection (Algorithm 2).

Figure 4: Qualitative results of watermarked images generated using WIND, Tree-Ring, and RingID. See Appendix D for quantitative results. See Appendix J for additional qualitative results.

In the following part, we refer to two variants of our method: (i) _WINDfast_ where we assume the used initial noise belongs to the identified group \(\) and check similarity only to noise patterns in this group. (ii) _WINDfull_ where we check all \(N\) possible initial noises if we can't find a match within the detected group (the gap between the similarity of the correct noise and random noises, as shown in Figure 2, allows us to determine whether the correct noise has been identified). This method is slower but more robust to removal attacks that might interfere with the Fourier pattern. Additional ablations and results can be found in Appendix D.

### Resilience to Forgery

In addition to empirical evaluations of specific attacks as in Figures 2 and 3; we discuss below the attacker's ability to infer knowledge about the used noise pattern across different watermarked images. Even if the attacker is able to obtain information about a specific initial noise \(_{i}\) for an index \(i\) (which is an extreme case), the other noise vectors for \(j i\) are still safe1. This is because we use a cryptographic hash function and a secret salt. Formally, Theorem 4.1 shows that, as long as the cryptographic hash function remains unbroken and the secret salt is kept private, the watermarking algorithm maintains its security properties against even very powerful adversaries.

**Theorem 4.1**.: _[Cryptographic Security] Let \( 0,1^{*} 0,1^{}\) be an unbroken cryptographic hash function used in our watermarking algorithm, with inputs \(i^{*}[N]\) and a secret salt \(s\). Assume \(s\) is sufficiently long and randomly generated. Then, even if an adversary obtains: the group number \(g^{*}\), the initial noise index \(i^{*}\), the initial noise \(_{i^{*}}\), and even the corresponding output of the hash function \(\), the adversary cannot:_

1. _Recover the secret salt_ \(s\)_,_
2. _Generate valid reconstructed noise_ \(_{j}\) _for any other initial noise index_ \(j i\)__

We defer the proof to Appendix E.

  
**Method** & **Keys** & **Clean** & **Rotate** & **JPEG** & **C\&S** & **Blur** & **Noise** & **Bright** & **Avg \(\)** \\   & 32 & 0.790 & 0.020 & 0.420 & 0.040 & 0.610 & 0.530 & 0.420 & 0.404 \\  & 128 & 0.450 & 0.010 & 0.120 & 0.020 & 0.280 & 0.230 & 0.170 & 0.183 \\  & 2048 & 0.200 & 0.000 & 0.040 & 0.000 & 0.090 & 0.070 & 0.060 & 0.066 \\   & 32 & **1.000** & **1.000** & **1.000** & 0.530 & 0.990 & **1.000** & 0.960 & 0.926 \\  & 128 & **1.000** & 0.980 & **1.000** & 0.280 & 0.980 & **1.000** & 0.940 & 0.883 \\  & 2048 & **1.000** & 0.860 & **1.000** & 0.080 & 0.970 & 0.950 & 0.870 & 0.819 \\  WINDfast128 & 100000 & **1.000** & 0.780 & **1.000** & 0.470 & **1.000** & **1.000** & 0.960 & 0.887 \\ WINDfast2048 & 100000 & **1.000** & 0.870 & 0.960 & 0.060 & 0.960 & 0.950 & 0.900 & 0.814 \\  WINDfull128 & 100000 & **1.000** & 0.780 & **1.000** & 0.850 & **1.000** & **1.000** & **1.000** & **0.947** \\ WINDfull2048 & 100000 & **1.000** & 0.880 & **1.000** & **0.930** & **1.000** & 0.990 & 0.980 & **0.969** \\   

Table 1: Comparison of correct watermark detection accuracy between WIND and previous image watermarking approaches under various image transformation attacks. WIND\({}_{M}\) denotes the use of \(M\) groups, with the total number of noises (\(N\)) specified in the “Keys” column. A broader comparison with additional methods can be found in Table 16.

### Watermarking Non-Synthetic Images.

Until now, we have addressed watermarking only for AI-generated synthetic images. Yet, protecting copyrights, or preventing the spread of misinformation, may also apply to modified natural images. Most previous approaches to watermark diffusion models overlook attempting to expand their method to non-generated images. To allow using our framework for non-generated images, we expand our framework. By using diffusion inpainting, our watermark can be applied to a natural image. Later, by inverting the inpainted image we can verify the presence of the watermark.

As demonstrated in Figure 5, our inpainting method injects a watermark with minimal visual impact, preserving the original image's integrity. Please see Appendix D for additional results.

## 5 Experiments

### Watermark Robustness

**Setting.** For a fair comparison with previous methods (Ci et al., 2024; Wen et al., 2023), we employed Stable Diffusion-v2 (Rombach et al., 2022), with 50 inference steps for both generation and inversion. Other implementation-details can be found in 1.

**Image Transformation Attacks.** Following previous methods (Wen et al., 2023; Ci et al., 2024) we applied these image transformations to the generated images: \(75^{}\) rotation, \(25\%\) JPEG compression, \(75\%\) random cropping and scaling (C & S), Gaussian blur with an \(8 8\) filter size, Gaussian noise with \(=0.1\), and color jitter with a brightness factor uniformly sampled between 0 and 6. In Table 1 we compare our methods to both Tree-Ring and RingID. As the results demonstrate, using multiple keys with RingID (Ci et al., 2024) is possible. Yet, it remains vulnerable to cropping and scaling attacks. In contrast, WIND effectively addresses this challenge. It enables accurate watermark detection under all image transformation attacks. We note that the incorporation of the keys in the RingID method not only allows us to embed keys but also increases the robustness of the full method to certain attacks.

**Steganalysis Attack.** We assess the robustness of our method against the attack proposed by Yang et al. (2024), which is capable of forging and removing the Tree-Ring and RingID keys. As discussed in Section 2.3, this attack attempts to approximate the watermark by subtracting watermarked images from non-watermarked images. The results, presented in Figure 3, indicate that while the attack could be able to forge or remove our group identifier, it is unable to forge or remove our watermark (initial noises). Even when the Fourier pattern type key is removed through an exhaustive search, our method remains robust in identifying the correct initial noise.

  
**Condition** & **Mean** & **STD** \\  Original Image & 0.888 & 0.053 \\ Attacked Image & 0.824 & 0.062 \\ Unrelated Image & 0.000 & 0.008 \\   

Table 2: Cosine similarity between the initial noise and the inversed noise before and after the regeneration attack. Also see Appendix D

  
**Method** & **FID**\(\) \\  DwtDctSvd & 25.01 \\ RivaGAN & 24.51 \\ Tree-Ring & 25.93 \\ RingID & 26.13 \\ WIND & **24.33** \\   

Table 3: FID scores of WIND compared to previous watermarking approaches.

**Regeneration Attacks.** Recently, Zhao et al. (2023) introduced a two-stage regeneration attack: (i) adding noise to the representation of a watermarked image, and (ii) reconstructing the image from this noisy representation. To assess the resilience of our approach to regeneration attacks, we applied the attack from Zhao et al. (2023) to watermarked images generated by our model. As shown in Table 2, the attack has a minimal impact on the distribution of the cosine similarities between the initial noise and the inverted noise. The attacked noise similarity still maintains a significant gap compared to random noise.

To examine the performance of our inpainting method, we report the Frechet Inception Distance (FID) (Heusel et al., 2018) on the MS-COCO-2017 (Lin et al., 2015) training dataset in Table 3. Notably, our method achieves the lowest FID among the compared methods, indicating a closer alignment with real images. Additionally we include some images generated by our framework in Figure 4.

## 6 Discussion and Limitations

**Editing a Given Image vs. Forging.** While forging our watermark by obtaining the initial noise is hard (Section 3), an easier path to obtaining harmful watermarked images might be to apply a slight edit to an already watermarked image. An harmful image in this context might include a copy-right infringing image, NSFW image, or any other content the model owner wish to avoid being associated with. Naturally, there is a trade-off between the severity of the applied edit, and the edit ability to preserve the initial watermark. We present one solution to mitigating this issue in the next discussion point.

**Storing a Database of Generations.** Model owners wishing to protect themselves from an attacker modifying a watermark image may keep a database of the past generations by their model. For these extreme cases, the model owner might only save the used prompts and initial noiseseeds, and use the reconstructed noise to retrieve the entire set of prompts used with that specific seed (Huang and Wan, 2024). While this process may be resource-intensive, it is only required in the rare event that an attacker intentionally modifies a benign image into a harmful one while preserving the watermark.

**Private Model.** Our watermark robustness is based to a large extent on the inability of an attacker to invert a model, which is empirically validated but not mathematically proven. Yet, as discussed in Section 2.2, the ability to successfully invert our model may be nearly equivalent to the ability to steal the forward diffusion process, effectively stealing the model (in which case, any watermarking attempt might be deemed quite useless anyhow). Still, a better framing of the mathematical assumptions behind this claim is a limitation of this work, as well as of previous works on watermarking using inversion of the diffusion generative process.

  
**Approach** & **Mean** & **Std** \\  Gen (private) \(\) Rev (private) & 0.888 & 0.053 \\ Gen (private) \(\) Rev (public) \(\) Gen (public) \(\) Rev (private) & 0.166 & 0.063 \\ Random Noise & 0.000 & 0.053 \\   

Table 4: Cosine similarity between the first initial noise used for generation and the inversed noise obtained through three inversion approaches. “Private” refers to models owner’s model, while “Public” denotes external model.

**Attacker's Advantage.** There exists a large set of diverse attacks aimed at watermark removal (Zhang et al., 2023; Yang et al., 2024; Zhao et al., 2023a), along with image transformations such as rotation and crops that also achieve some limited success against our watermark. As in many security applications, we suspect that an attacker capable enough will still be able to remove the watermark using new techniques we might not expect. However, a more robust watermark may nevertheless help to decrease the spread of false information.

Additional discussion and limitations can be found in Appendix C.

## 7 Conclusion

In this work, we present a robust and distortion-free watermarking method that leverages the initial noises employed in diffusion models for image generation. By integrating existing techniques, we enhanced the approach to achieve improved efficiency and robustness against various types of attacks. Furthermore, we outlined a strategy for applying our method to non-generated images through inpainting.

Figure 5: Comparison of COCO images before and after watermarking via inpainting.