# Data Attribution for Text-to-Image Models

by Unlearning Synthesized Images

 Sheng-Yu Wang\({}^{1}\) Aaron Hertzmann\({}^{2}\) Alexei A. Efros\({}^{3}\) Jun-Yan Zhu\({}^{1}\) Richard Zhang\({}^{2}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)Adobe Research \({}^{3}\)UC Berkeley

###### Abstract

The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. Influence is defined such that, for a given output, if a model is retrained from scratch without the most influential images, the model would fail to reproduce the same output. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining models from scratch. In our work, we propose an efficient data attribution method by simulating _unlearning the synthesized image_. We achieve this by increasing the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. We then identify training images with significant loss deviations after the unlearning process and label these as influential. We evaluate our method with a computationally intensive but "gold-standard" retraining from scratch and demonstrate our method's advantages over previous methods.

## 1 Introduction

Data attribution for text-to-image generation aims to identify which training images "influenced" a given output. The black-box nature of modern image generation models [1; 2; 3; 4; 5; 6; 7; 8], together with the enormous datasets required , makes it extremely challenging to understand the contributions of individual training images. Although generative models can, at times, replicate training data [10; 11], they typically create samples distinct from any specific training image.

We believe that a counterfactual definition of "influence" best matches the intuitive goal of attribution [12; 13]. Specifically, a collection of training images is influential for a given output image if removing those images from the training set and then retraining from scratch makes the model unable to reproduce the same synthesized image. Unfortunately, directly searching for the most influential images according to this definition is computationally infeasible since it would require training an exponentially large number of new models from scratch.

Hence, practical influence estimation requires effective approximations. For example, many approaches replace retraining with a closed-form approximation, computed separately for each training image [12; 14; 15; 13; 16]. For text-to-image attribution, these methods are outperformed by simple matching of off-the-shelf image features . Wang _et al._ use model customization  to study the effect of training a model towards an exemplar, but find limited generalization to the general large-scale training case. We aim for a tractable method that accurately predicts influence according to the counterfactual definition.

We propose an influence prediction approach with two key ideas (Figure 1). First, we approximate the removal of a training image from a model through an optimization procedure termed _unlearning [20; 21; 22]_, which increases the training loss of the target image while preserving unrelated concepts. We then compute the training loss for the original synthesized image. However, directly applying this idea would require unlearning separately for _every_ single training image, which is also costly. Oursecond main idea is to reverse the roles: we _unlearn the synthesized image, and then evaluate which training images are represented worse by the new model._ This method requires only one unlearning optimization, rather than a separate unlearning step for each training image.

The methodology for unlearning is important. Unlearning a synthesized image by naively maximizing its loss leads to catastrophic forgetting , where the model also fails to generate other unrelated concepts. Inspired by work on unlearning data for classifiers [20; 24], we mitigate this issue by regularizing gradient directions using the Fisher information to retain pretrained information. Additionally, we find that updating only the key and value mappings in the cross-attention layers improves attribution performance. We show how "influence functions" [13; 15] can be understood as approximations to unlearning in Appendix A, but they are also limited by their closed-form approximation nature.

We perform a rigorous counterfactual validation: removing a predicted set of influential images from the training set, retraining from scratch, and then checking that the synthesized image is no longer represented. We use MSCOCO  (\(\)100k images), which allows for retraining models within a reasonable compute budget. We also test on a publicly-available attribution benchmark  using customized text-to-image models . Our experiments show that our algorithm outperforms prior work on both benchmarks, demonstrating that unlearning synthesized images is an effective way to attribute training images. Our code is available at: https://peterwang512.github.io/AttributeByUnlearning.

In summary, our contributions are:

* We propose a new data attribution method for text-to-image models, by unlearning the synthesized image and identifying which training images are forgotten.
* We find and ablate the components for making unlearning efficient and effective, employing Fisher information and tuning a critical set of weights.
* We rigorously show that our method is counterfactual predictive by omitting influential images, retraining, and checking that the synthesized image cannot be regenerated. Along with the existing Customized Model benchmark, our method identifies influential images more effectively than recent methods based on customization and influence functions.

Figure 1: (a) **Our algorithm**: We propose a new data attribution method using machine unlearning. By modifying the pretrained model \(\) to unlearn the synthesized result \(}\), the model also forgets the influential training images crucial for generating that specific result. (b) **Evaluation**: We validate our method through counterfactual evaluation, where we retrain the model without the top \(K\) influential images identified by our method. When these influential images are removed from the dataset, the model fails to generate the synthesized image.

Related Work

**Attribution.** _Influence functions_[15; 13] approximate how the objective function of a test datapoint would change after perturbing a training datapoint. One may then predict attribution according to the training points that can produce the largest changes. Koh and Liang  proposed using influence functions to understand model behavior in deep discriminative models. The influence function requires calculating a Hessian of the model parameters, for which various efficient algorithms have been proposed, such as inverse hessian-vector products , Arnoldi iteration , Kronecker factorization [27; 28; 29], Gauss-Newton approximation , and nearest neighbor search .

Other methods explore different approaches. Inspired by the game-theory concept of Shapley value , several methods train models on subsets of training data and estimate the influence of a training point by comparing the models with and without that training point [32; 33; 34; 35]. Pruthi et al.  estimate influence by tracking train-test image gradient similarity throughout model training.

Recent methods have started tackling attribution for diffusion-based image generation. Wang _et al._ proposed attribution by model customization [37; 19], where a pretrained model is influenced by tuning towards an exemplar concept. Several works adapt TRAK , an influence function-based method, to diffusion models, extending it by attributing at specific denoising timesteps , or by improving gradient estimation and using Tikhonov regularization . Unlike these methods, our method performs attribution by directly unlearning a synthesized image and tracking the effect on each training image. Our method outperforms existing methods in attributing both customized models and text-to-image models. Concurrent works also apply machine unlearning for attribution tasks [38; 39]. Unlike their approaches, we find that applying strong regularization during unlearning is crucial to obtaining good performance in our tasks.

**Machine unlearning.** Machine unlearning seeks to efficiently "remove" specific training data points from a model. Recent studies have explored concept erasure for text-to-image diffusion models, specified by a text request [40; 41; 42; 43; 44], whereas we remove individual images. While forgetting may be achieved using multiple models trained with subsets of the dataset beforehand [21; 45; 46], doing so is prohibitively expensive for large-scale generative models.

Instead, our approach follows unlearning methods that update model weights directly [20; 47; 22; 48; 24]. The majority of prior methods use the Fisher information matrix (FIM) to approximate retraining without forgetting other training points [20; 22; 49; 24; 50; 51]. In particular, we are inspired by the works from Guo _et al._ and Tanno _et al._, which draw a connection between FIM-based machine unlearning methods and influence functions. We show that unlearning can be efficiently applied to the attribution problem, by "unlearning" output images instead of training data.

**Replication detection.** Shen _et al._ identify repeated pictorial elements in art history. Somepalli _et al._ and Carlini _et al._ investigate the text-to-image synthesis of perceptually-exact copies of training images. Unlike these, our work focuses on data attribution for more general synthesis settings beyond replication.

## 3 Problem Setting and Evaluation

Our goal is to attribute a generated image to its training data. We represent the training data as \(=\{(_{i},_{i})\}_{i=1}^{N}\), where \(\) denotes an image and \(\) represents its conditioning text. A learning algorithm \(:\) yields parameters of a generative model; for instance, \(=()\) is a model trained on \(\). We focus on diffusion models that generate an image from a noise map \((0,)\). A generated image from text \(\) is represented as \(}=G_{}(,)\). To simplify notation, we write a text-image tuple as a single entity. A synthesized pair is denoted as \(}=(},)\), and a training pair is denoted as \(_{i}=(_{i},_{i})\). We denote the loss of an image \(\) conditioned on \(\) as \((,)\).

Next, we describe the "gold-standard" evaluation method that we use to define and evaluate influence. Section 4 describes our method for predicting influential images.

**Counterfactual evaluation.** A reliable data attribution algorithm should accurately reflect a _counterfactual prediction_. That is, if an algorithm can identify a set of truly influential training images, then a model trained _without_ those images would be incapable of generating or representing that image. As noted by Ilyas _et al._ and Park _et al._, counterfactual prediction is computationally intensive to validate. As such, these works introduce the Linear data modeling (LDS) score as an efficient proxy, but with the assumption that data attribution methods are _additive_, which does not hold for feature matching methods and our method.

In our work, we invest substantial computational resources to the "gold standard" counterfactual evaluation within our resource limits. That is, we use an attribution algorithm to identify a critical set of \(K\) images, denoted as \(_{}}^{K}\). We then train a generative model without those images from scratch, _per synthesized sample and per attribution method_. Despite the computational cost, this allows us to provide the community with a direct evaluation of counterfactual prediction, without relying on a layer of approximations. We formalize our evaluation scheme as follows.

**Training a counterfactual model.** For evaluation, an attribution algorithm is given a budget of \(K\) images for attributing a synthesized sample \(}\), denoted as \(_{}}^{K}\). We then train a leave-\(K\)-out model \(_{}}^{-K}\) from scratch using \(_{}}^{-K}=_{}}^{K}\), the dataset with the \(K\) attributed images removed:

\[_{}}^{-K}=(_{}}^{-K}),\] (1)

**Evaluating the model.** We then compare this "leave-\(K\)-out" model against \(_{0}=()\), the model trained with the entire dataset, and assess how much it loses its capability to represent \(}\) in terms of both the loss change \((},)\) and the capability to generate the same sample \( G_{}(,)\) from the same input noise \(\) and text \(\).

First, if the leave-\(K\)-out model is trained without the top influential images, it should reconstruct synthetic image \(}\) more poorly, resulting in a higher \((},)\):

\[(},)=(}, _{}}^{-K})-(},_{0}).\] (2)

Second, if properly selected, the leave-\(K\)-out model should no longer be able to generate \(}=G_{}(,)\). For diffusion models, we can particularly rely on the "seed consistency" property [12; 53; 54]. Georgiev _et al._ find that images generated from the same random noise have little variations, even when generated by two independently trained diffusion models on the same dataset. They leverage this property to evaluate attribution via \( G_{}(,)\), the difference of generated images between \(_{0}\) and \(_{}}^{-K}\). An effective attribution algorithm should lead to a leave-\(K\)-out model generating images that deviate more from the original images, resulting in a larger \( G_{}(,)\) value:

\[ G_{}(,)=dG_{_{0}}(, ),G_{_{}}^{-K}}(,),\] (3)

where \(d\) can be any distance function, such as L2 or CLIP . Georgiev _et al._ also adopt \( G_{}(,)\) for evaluation. While evaluating loss increases and seed consistency is specific to diffusion models, the overarching idea of retraining and evaluating if a synthesized image is still in the model applies across generative models.

**Choice of loss \((,)\).** We focus on DDPM loss introduced by Ho _et al._, the standard loss used to train diffusion models. Diffusion models learn to predict the noise added to a noisy image \(_{t}=_{t}}_{0}+_{t}}\), where \(_{0}\) is the original image, \(\) is the Gaussian noise, \(_{t}\) and \(t\) controls the noise strength. \(t\) is an integer timestep sampled between 1 to \(T\), where \(T\) is typically set to 1000. A larger \(t\) implies more noise added. DDPM loss optimizes for the noise prediction task: \([\|_{}(_{t},,t)-\|^{2}]\), where \(_{}()\) is the denoiser for noise prediction, and \(c\) denotes text condition.

## 4 Attribution by Unlearning

In this section, we introduce our attribution approach for a text-to-image model \(_{0}=()\), trained on dataset \(\). We aim to find the highly influential images on a given synthetic image \(}\) in dataset \(\).

If we had infinite computes and a fixed set of \(K\) images, we could search for every possible subset of \(K\) images and train models from scratch. The subset whose removal leads to the most "forgetting" of the synthesized image would be considered the most influential. Of course, such a combinatoric search is impractical, so we simplify the problem by estimating the influence score of each training point _individually_ and selecting the top \(K\) influential images based on their scores.

Formally, we define a data attribution algorithm \(\), which given access to the training data, model, and learning algorithm, estimates the influence of each training point, denoted by \((},,,)=[(},_{1}),(},_{2}),,(},_{N})]\). \(\) represents the factorized attribution function that estimates influence based on the synthesized sample and a training point. Although \(\) can access all training data, parameters, and the learning algorithm, we omit them for notational simplicity.

One potential algorithm for \((},)\) is to remove \(\) from the training set, retrain a model and check its performance on \(}\). However, this method is infeasible due to the size of the training set. To overcome this, we have introduced two key ideas. First, rather than training from scratch, we use model unlearning--efficiently tuning a pretrained model to remove a data point. Second, rather than unlearning training points, we apply unlearning to the _synthesized_ image and assess how effectively each training image is forgotten. To measure the degree of removal, we track the training loss changes for each training image after unlearning, and we find this effective for data attribution.

**Unlearning the synthesized image.** A naive approach to unlearn a synthesized image \(}\) is to solely maximize its loss \((},)\). However, only optimizing for this leads to catastrophic forgetting , where the model can no longer represent other concepts.

Instead, we propose to retain the information from the original dataset while "removing" the synthesized image, as though it had been part of training. Given model trained on the original dataset \(_{0}=()\) and a synthesized image \(}\), we compute a new model \(_{-}}=(})\), with \(}\) removed. Here, we use the set removal notation \(\) to specify a "negative" datapoint in the dataset. Concretely, we solve for the following objective function, using elastic weight consolidation (EWC) loss  as an approximation:

\[^{}}_{}( )=&-(},)+_{ }(,)\\ &-(},)+( -_{0})^{T}F(-_{0}),\] (4)

where \(F\) is the Fisher information matrix, which is approximated as a diagonal form for computational efficiency. \(F\) approximates the training data loss to the second-order , a technique widely used in continual learning [23; 58]. This enables us to solve for the new model parameters \(_{-}}\) efficiently, by initializing from the pretrained model \(_{0}\). We optimize this loss with Newton updates:

\[+F^{-1}(},),\] (5)

where \(\) controls the step size, \(F^{-1}\) is the inverse of the Fisher information matrix. In practice, Newton updates allow us to achieve effective attribution with few iterations and, in some cases, as few as one step. We denote the unlearned model as \(_{-}}\). We provide details of the EWC loss, and Newton update in Appendix A.

**Attribution using the unlearned model.** After we obtain the unlearned model \(_{-}}\), we define our attribution function \(\) by tracking the training loss changes for each training sample \(\):

\[(},)=(,_{-}})-(,_{0}).\] (6)

The value \((},)\) is expected to be close to zero for most unrelated training images since the EWC loss used in obtaining \(_{-}}\) acts as a regularization to preserve the original training dataset. A higher value of \((},)\) indicates that unlearned model \(_{-}}\) no longer represents the training sample \(\).

**Relation with influence functions.** Our method draws a parallel with the influence function, which aims to estimate the loss change of \(}\) by removing a training point \(\). However, training leave-one-out models for every training point is generally infeasible. Instead, the influence function relies on a heavier approximation to estimate the effect of perturbing a single training point, rather than actually forgetting the training samples. In contrast, our approach only requires running the unlearning algorithm _once_ for a synthesized image query. This allows us to use a milder approximation and obtain a model that forgets the synthesized sample. Guo _et al._ and Tanno _et al._ explore a similar formulation for unlearning training images and draw a connection between their unlearning algorithms and influence function. Our approach aims to unlearn the synthesized image instead,which connects to influence function in a similar fashion. We discuss our method's connection to influence function in more detail in Appendix A.3.

Optimizing a subset of weights.To further regularize the unlearning process, we optimize a small subset of weights, specifically \(W^{k}\) and \(W^{v}\), the key and value projection matrices in cross-attention layers . In text-to-image models, cross-attention facilitates text-to-image binding, where \(W^{k}\) identifies which features match each text token, while \(W^{v}\) determines how to modify the features for the matched patches. We observe that performing unlearning \(W^{k}\) and \(W^{v}\) is effective for attribution. Prior works also select the same set of parameters to improve fine-tuning  and unlearning .

Implementation details.We conduct our studies on text-conditioned latent diffusion models . Since diffusion models are typically trained with \(T=1000\) steps, evaluating the loss for all timesteps is costly. Therefore, we speed up computation by calculating the loss \((,)\) with strided timesteps; we find that using a stride of 50 or 100 leads to good attribution performance. For calculating the loss change \((},)\) during evaluation, we take a finer stride of 5 steps to ensure a more accurate estimation of the DDPM loss. Additional details of our method, including hyperparameter choices, are provided in Appendix B.

## 5 Experiments

We validate our method in two ways. The first is a reliable, "gold-standard", but intensive - retraining a model from scratch without influential images identified by the algorithm. In Section 5.1, we perform this evaluation on a medium-sized dataset of 100k MSCOCO images . Secondly, in Section 5.2, we evaluate our method on the Customized Model Benchmark , which measures attribution through customization on Stable Diffusion models . This tests how well our method can apply to large-scale text-to-image models.

Figure 2: **Attribution results on MSCOCO models.** We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.

### Leave-\(K\)-out counterfactual evaluation

**Evaluation protocol.** We select latent diffusion models  trained on MSCOCO , as its moderate size (118,287 images) allows for repeated leave-\(K\)-out retraining. Specifically, we use the pre-trained model evaluated in Georgiev _et al._. As outlined in Section 3, for each synthesized image \(}\), we measure the leave-\(K\)-out model's **(1) loss change \((},)\)** and **(2) deviation of generation \( G_{}(,)\)**. The deviation is measured by mean square error (MSE) and CLIP similarity . We collect 110 synthesized images from the pre-trained model for evaluation, with different text prompts sourced from the MSCOCO validation set. We evaluate \((},)\) and \( G_{}(,)\) for all synthesized images and report mean and standard error.

We compare our method with several baselines:

* **Random:** We train models with \(K\) random images removed, using 10 models per value of \(K\).
* **Image similarity:** pixel space, CLIP image features , DINO , and DINOv2 
* **Text similarity:** CLIP text features
* **Attribution by Customization  (AbC):** fine-tuned image features trained on the Customized Model benchmark, denoted as CLIP (AbC) and DINO (AbC)
* **DataInf **: an influence estimation method based on approximating matrix inverses.
* **TRAK  and JourneyTRAK ** are influence function-based methods that match the loss gradients of training and synthesized images, using random projection for efficiency. Both methods run the influence function on multiple models trained on the same dataset (20 in this test) and average the scores. The main difference is in the diffusion loss calculation: TRAK randomly samples and averages the loss over timesteps, while JourneyTRAK calculates it only at \(t=400\) for synthesized images during counterfactual evaluation.
* **D-TRAK :** a concurrent work that extends TRAK by changing the denoising loss function into a square loss during influence computation. As mentioned by the authors, D-TRAK yields unexpectedly stronger performance even though the design choice is not theoretically understood. Different from TRAK, D-TRAK yields competitive performance with a single model.

For attribution methods, we use \(K=500,1000\), and \(4000\), representing approximately \(0.42\%\), \(0.85\%\), and \(3.4\%\) of the MSCOCO dataset, respectively.

**Visual comparison of attributed images.** In Figure 2, we find that our method, along with other baselines, can attribute synthesized images to visually similar training images. However, our method more consistently attributes images with the same fine-grained attributes, such as object location, pose, and counts. We provide more results in Appendix C.1. Next, we proceed with the counterfactual analysis, where we test whether these attributed images are truly _influential_.

**Tracking loss changes in leave-\(K\)-out models.** First, we report the change in DDPM loss for leave-\(K\)-out models in Table 1. Matching in plain pixel or text feature space yields weak performance,

    &  & (},)\) (x\(10^{-3}\))} & (,)\) (MSE) (\(\)x\(10^{-2}\))} & (,)\) (CLIP) \(\) (x\(10^{-1}\))} \\   & & K=500 & K=1000 & K=4000 & K=500 & K=1000 & K=4000 & K=500 & K=1000 & K=4000 \\  Random & Random & 3.5\(\)0.03 & 3.5\(\)0.03 & 3.5\(\)0.03 & 4.1\(\)0.06 & 4.1\(\)0.06 & 4.0\(\)0.06 & 7.9\(\)0.03 & 7.8\(\)0.03 & 7.9\(\)0.03 \\ Pixel & Pixel & 3.6\(\)0.10 & 3.6\(\)0.10 & 4.0\(\)0.11 & 4.3\(\)0.19 & 4.3\(\)0.21 & 4.9\(\)0.01 & 7.9\(\)0.10 & 7.8\(\)0.09 & 7.7\(\)0.10 \\  Text & CLIP Text & 3.8\(\)0.12 & 4.2\(\)0.14 & 5.5\(\)0.25 & 4.1\(\)0.20 & 4.3\(\)0.19 & 4.3\(\)0.19 & 4.0\(\)0.16 & 7.8\(\)0.08 & 7.7\(\)0.09 & 7.4\(\)0.08 \\  & DINOv2 & 3.9\(\)0.12 & 4.3\(\)0.13 & 5.6\(\)0.33 & 4.3\(\)0.20 & 4.6\(\)0.20 & 5.1\(\)0.19 & 7.7\(\)0.79 & 7.7\(\)0.09 & 7.7\(\)0.09 \\ Image & CLIP & 4.2\(\)0.14 & 4.7\(\)0.18 & 6.4\(\)0.32 & 4.4\(\)0.20 & 4.6\(\)0.21 & 5.2\(\)0.22 & 7.6\(\)0.09 & 7.5\(\)0.08 & **6.8\(\)0.08** \\  & DINO & 4.8\(\)0.15 & 5.6\(\)0.20 & 5.1\(\)0.35 & 4.5\(\)0.16 & 5.3\(\)0.20 & 5.9\(\)0.21 & 7.4\(\)0.20 & 7.1\(\)0.09 & **6.3\(\)0.10** \\ AbC & CLIP (AbC) & 4.4\(\)0.13 & 4.9\(\)0.17 & 6.9\(\)0.32 & 4.6\(\)0.20 & 5.0\(\)0.22 & 5.0\(\)0.22 & 7.5\(\)0.09 & 7.3\(\)0.09 & 7.5\(\)0.09 \\  & DINO (AbC) & 4.8\(\)0.15 & 5.5\(\)0.20 & 8.1\(\)0.35 & 4.8\(\)0.22 & 4.9\(\)0.20 & 5.8\(\)0.21 & 7.5\(\)0.09 & 7.2\(\)0.09 & **6.3\(\)0.09** \\   & Datiaf & 3.7\(\)0.11 & 3.7\(\)0.12 & 3.9\(\)0.13 & 4.1\(\)0.18 & 4.1\(\)0.20 & 4.3\(\)0.20 & 4.1\(\)0.18 & 7.9\(\)0.09 & 7.8\(\)0.10 \\  & TRAK & 5.2\(\)0.14 & 5.8\(\)0.16 & 7.1\(\)0.16 & 4.7\(\)0.

while deep image features, particularly DINO, perform better. Interestingly, DINO outperforms most influence function methods at \(K=4000\), despite not being trained specifically for the attribution task. Fine-tuning image features with the Customized Model benchmark, such as CLIP (AbC), shows some improvement. However, in general, the improvement is limited, indicating that transferring from attributing customized models to general models remains challenging .

Among influence functions, DataInf performs poorly. According to the paper, the matrix inverse approximation DataInf uses is more suitable for LoRA fine-tuned models rather than text-to-image models trained from scratch. This approximation leads to poor performance.

TRAK significantly outperforms JourneyTRAK. We hypothesize that this is because JourneyTRAK collects gradients only for denoising loss at timestep \(t=400\), making it less effective for identifying influential images that affect the DDPM training loss different noise levels. On the other hand, D-TRAK is the best performing influence-function-based method, although the reason behind its strong performance is not well-understood .

Our method consistently performs best across all \(K\) values, outperforming both influence functions and feature-matching methods.

Deviation of generated output in leave-\(K\)-out models.Figure 3 and Table 1 shows the deviation in generated outputs for leave-\(K\)-out models, where all images are generated using the same noise input and text prompt. Consistent with the loss change evaluation, D-TRAK, DINO, and our method yield the largest deviation with a small budget. While the three methods perform similarly well in CLIP similarity, D-TRAK outperforms our method in MSE deviations. In contrast, TRAK and JourneyTRAK have formulations similar to D-TRAK, but they perform subpar in this test. Interestingly, while D-TRAK yields significant performance gain by changing the loss function, we did not observe the same improvement when applying the same loss function changes to our method. We discuss this more in Appendix C.1..

In addition, we include analysis on whether unlearned models and leave-\(K\)-out models forget only the specific concept, along with more qualitative results, in Appendix C.1.

Ablation studies.We study two design choices: (1) the effect of EWC regularization and (2) different weight subsets for optimization. Table 2 shows the results. We find that unlearning without EWC loss greatly hurts attribution performance, indicating the importance of regulating unlearning with Fisher information. We also find that using a subset of weights to unlearn leads to better attribution in general. We test three weight subset selection schemes (Attn, Cross Attn, Cross Attn KV), all of which outperform the version using all weights. Among them, updating Cross Attn KV performs the best, consistent with findings from model customization [19; 63] and unlearning .

Figure 3: **Leave-\(K\)-out analysis for MSCOCO models. We compare images across our method and baselines generated by leave-\(K\)-out models, using different \(K\) values, all under the same random noise and text prompt. A significant deviation in regeneration indicates that critical, influential images were identified by the attribution algorithm. Our method leads to image generation that deviate significantly, even with as few as 500 influential images removed (\(\)0.42\(\%\) of the dataset).**

Spatially-localized attribution.While our formulation is written for whole images, we can run attribution on specific regions with little modification. We demonstrate this in Figure 4 on a generated image of a motorcycle and stop sign, using bounding boxes identified by GroundingDINO . For each detected object, we run our unlearning (using the same prompt) on that specific object by optimizing the objective only within the bounding box. By doing so, we attribute different training images for the stop sign and motorcycle.

### Customized Model Benchmark

Wang _et al._ focus on a specialized form of attribution: attributing customized models trained on an individual or a few exemplar images. This approach provides ground truth attribution, since the images generated by customized models are computationally influenced by exemplar images. While this evaluation has limited generalization to attribution performance with larger training sets, it is the only tractable evaluation for attributing large-scale text-to-image models to date.

Evaluation protocol.Since the Customized Model Benchmark has ground truth, the problem is evaluated as a retrieval task. We report **Recall@K** and **mAP**, measuring the success of retrieving the exemplar images amongst a set including 100K LAION images. We compare with Wang _et al._'s feature-matching approach that finetunes on the Customized Model dataset, referred to as DINO (AbC) and CLIP (AbC). We also compare with D-TRAK, the best-performing influence function method in our previous MSCOCO experiment. For our evaluation, we selected a subset of the dataset comprising 20 models: 10 object-centric and 10 artist-style models. We select 20 synthesized images with different prompts for each model, resulting in 400 synthesized image queries.

Comparing with other methods.We report Recall@10 and mAP in Figure 6. Our method performs on par with baselines for object-centric models, while significantly outperforming them on artist-style models. Although CLIP (AbC) and DINO (AbC) are fine-tuned for this attribution task, the feature matching approach can sometimes confuse whether to attribute a synthesized image to style-related or object-related images. In contrast, our method, which has access to the model itself, traces influential training images more effectively. D-TRAK performs worse than our method despite also having model access, which suggests that the influence approximations could be less accurate for larger models. In Figure 5, we show a qualitative example. While DINO (AbC) and CLIP (AbC)

    &  & (},)\) (x\(10^{-3}\))} & _{}(,)\) (MSE) (\(\)x\(10^{-2}\))} & _{}(,)\) (CLIP) \(\) (x\(10^{-1}\))} \\   & & K=500 & K=1000 & K=4000 & K=500 & K=1000 & K=4000 & K=500 & K=1000 & K=4000 \\  SGD (1 step) & & 3.550.10 & 3.50.10 & 3.50.11 & 3.90.16 & 3.90.17 & 4.10.20 & 7.90.09 & 7.80.09 & 7.80.09 \\ SGD (10 step) & & 3.40.10 & 3.50.10 & 3.50.10 & 3.74.16 & 3.90.17 & 3.90.15 & 7.90.09 & 7.90.09 & 7.80.09 \\  Full & ✓ & 5.550.17 & 6.20.20 & 8.40.20 & 4.60.10 & 5.20.35 & 5.50.22 & 7.60.60 & 7.44.00 & 7.11.00 \\ Attn & ✓ & 5.550.17 & 6.60.22 & 9.20.30.3 & **5.14.02** & 5.30.22 & **6.20.27** & 7.44.00 & 7.20.09 & 6.70.09 \\ Cross Attn & ✓ & 5.550.17 & 6.50.24 & 9.10.33 & 4.80.20 & 5.30.22 & 6.00.23 & **7.34.01** & 7.20.010 & **6.60.09** \\ Cross Attn KV & ✓ & **5.66.16** & **6.70.22** & **9.8**\(\)0.32 & **5.14.02** & **5.70.24** & 6.10.22 & **7.34.09** & **7.0**\(\)0.09 & **6.40.10** \\   

Table 2: **Leave-\(K\)-out ablation studies.** We ablate our design choices and report \((},)\) and \(_{}(,)\) as in Table 1. We find that naive unlearning (SGD without EWC regularization) is not effective. We then compare four different sets of weights to unlearn and find that cross-attention \(W^{k}\), \(W^{v}\) (Cross Attn KV) outperforms other configurations. **Bolded** are the best performing method, and gray shows the standard error.

Figure 4: **Spatially-localized attribution.** Given a synthesized image (left), we crop regions containing specific objects using GroundingDINO . We attribute each object separately by only running forgetting on the pixels within the cropped region. Our method can attribute different synthesized regions to different training images.

can retrieve visually or semantically similar images, our method successfully identifies the exemplars in both cases. We include ablation studies in Appendix C.2.

## 6 Discussion, Broader Impacts, and Limitations

Generative models have entered the public consciousness, spawning companies and ecosystems that are deeply impacting the creative industry. The technology raises high-stakes ethical and legal questions surrounding the authorship of generated content . Data attribution is a critical piece of understanding the behavior of generative models, with potential applications in informing a compensation model for rewarding contributors for training data. In addition, data attribution can join other works  as a set of tools that allow end users to interpret how and why a model behaves, enabling a more trustworthy environment for machine learning models.

Our work proposes a method for data attribution for text-to-image models, leveraging model unlearning. We provide a counterfactual validation, verifying that removing the identified influential images indeed destroys the target image. While our method empirically demonstrates that unlearning can be effectively used, work remains to make this practical. Though our model unlearns efficiently, estimating the reconstruction loss on the training set remains a bottleneck, as several forward passes are required on each training estimate, as profiled in Appendix D. While our evaluation showed that unlearning is useful for attribution, direct evaluation of unlearning algorithms for large generative models remains an open research challenge. Furthermore, to find a critical set of images, our method and baselines assign influence scores to individual images and sort them. However, groups of images may have interactions that are not captured in such a system. Furthermore, our method and baselines explore attribution of the whole image, while finer attribution on individual aspects of the image, such as style, structure, or individual segments, are of further interest.

Figure 5: **Qualitative examples on the Customized Model benchmark. The red boxes indicate ground truth exemplar images used for customizing the model. Both our method and AbC baselines successfully identify the exemplar images on object-centric models (left), while our method outperforms the baselines with artistic style models (right).**

Figure 6: **Customized Model benchmark . We report Recall@10 (left) and mAP (right) and show performance on artist-style models (y-axis) vs. object-centric models (x-axis). On object-centric models, our method performs on par with AbC features, which were directly tuned on the benchmark, while significantly outperforming them on artist-style models. D-TRAK performs the second best on artist-style models but worse on object-centric models. We plot one standard error on both axes.**

Acknowledgements. We thank Kristian Georgiev for answering all of our inquiries regarding JourneyTRAK implementation and evaluation, and providing us their models and an earlier version of JourneyTRAK code. We thank Nupur Kumari, Kangle Deng, Grace Su for feedback on the draft. This work is partly supported by the Packard Fellowship, JPMC Faculty Research Award, and NSF IIS-2239076.