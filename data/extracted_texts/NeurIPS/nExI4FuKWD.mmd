# FineCLIP: Self-distilled Region-based CLIP

for Better Fine-grained Understanding

 Dong Jing\({}^{*1}\), Xiaolong He\({}^{*1}\), Yutian Luo\({}^{1}\), Nanyi Fei\({}^{2}\),

Guoxing Yang\({}^{1}\), Wei Wei\({}^{3}\), Huiwen Zhao\({}^{3}\), Zhiwu Lu \({}^{ 1}\)

\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\)MetaBrain AGI Lab, Shanghai, China

\({}^{3}\)R&D Management Department, Honor Device Co., Ltd

{jingdong98, xiaolonghe, luzhiwu}@ruc.edu.cn

Equal ContributionCorresponding Author

###### Abstract

Contrastive Language-Image Pre-training (CLIP) achieves impressive performance on tasks like image classification and image-text retrieval by learning on large-scale image-text datasets. However, CLIP struggles with dense prediction tasks due to the poor grasp of the fine-grained details. Although existing works pay attention to this issue, they achieve limited improvements and usually sacrifice the important visual-semantic consistency. To overcome these limitations, we propose FineCLIP, which keeps the global contrastive learning to preserve the visual-semantic consistency and further enhances the fine-grained understanding through two innovations: 1) A real-time self-distillation scheme that facilitates the transfer of representation capability from global to local features. 2) A semantically-rich regional contrastive learning paradigm with generated region-text pairs, boosting the local representation capabilities with abundant fine-grained knowledge. Both cooperate to fully leverage diverse semantics and multi-grained complementary information. To validate the superiority of our FineCLIP and the rationality of each design, we conduct extensive experiments on challenging dense prediction and image-level tasks. All the observations demonstrate the effectiveness of FineCLIP.

## 1 Introduction

Contrastive Language-Image Pre-training (CLIP) [18; 40] emerges as the foundational work in vision-language representation learning. By training on large-scale, noisy image-text pairs, CLIP aligns global image and text embeddings within a unified latent space, demonstrating remarkable successes across image-level tasks [1; 25; 61], e.g. image classification and cross-modal retrieval.

However, CLIP has shown notable limitations in understanding fine-grained details, such as identifying object attributes and their relationships [37; 41; 62; 70]. Especially, when applied to downstream tasks, CLIP struggles to extract valuable region representations from visual dense features, limiting its effectiveness in complex recognition scenarios [33; 72]. Recent works [60; 70] attribute this issue to the task domain shift: CLIP matches an image as a whole to text description but fails to capture fine-grained alignment between image regions and corresponding textual attributes.

To address this problem, researchers have attempted to enhance fine-grained alignment using two primary strategies. The first strategy [27; 59; 66; 70] directly leverages CLIP to match image regions with template labels using large quantities of grounding annotations in a classification setting. However, the pre-defined template labels lack sufficient semantic diversity, restricting itsgeneralization to open-world scenarios. The second strategy  proposes a uni-modal distillation scheme by aligning the region dense features of the trainable student model with the image-level representation of the corresponding image crops generated by the frozen teacher model. Although efficient, the frozen teacher model restricts the performance ceiling of the student model. Notably, both of the strategies disrupt the important semantic consistency of visual representations.

In this work, we unify cross-modal regional alignment and uni-modal global-to-region guidance into a coherent framework. We present **FineCLIP**, an end-to-end universal vision-language framework that gains better fine-grained understanding by reasonably incorporating a multi-grained contrastive learning paradigm with a real-time self-distillation scheme. FineCLIP involves the following appealing designs: **1)** In order to enrich the model with abundant and diverse fine-grained semantics, instead of using limited template labels in the classification setting, we build the **regional contrastive learning paradigm** using regions and corresponding text descriptions generated by advanced Large Vision-Language Model (LVLM). **2)** To facilitate interactions between global embeddings of image region crops and corresponding region dense features for mutual guidance, we introduce a **real-time self-distillation scheme** that relies on and complements global contrastive learning. Instead of using the frozen teacher model for guidance, we use the trainable FineCLIP to independently teach itself, leading to improved performance as global representations are progressively refined during training. **3)** FineCLIP universally aligns visual embedding, visual dense features, and textual embedding into a unified latent space. With these design considerations, FineCLIP fully leverages available diverse semantics from fine-grained region descriptions and real-time optimal global representations, and boosts the interactions of multi-grained complementary information. When applied to downstream tasks, FineCLIP is capable of effectively handling both dense prediction using region dense features and image-level tasks using semantic-aligned global embeddings.

Through extensive experimental evaluations, we show that FineCLIP surpasses previous arts on most dense prediction tasks and image-level tasks under fair comparison settings, demonstrating its effectiveness in both fine-grained understanding and semantic-aligned global representation. Furthermore, FineCLIP presents promising scaling ability, consistently showing faster performance improvements than other competing methods as we scale up the trainset.

Our contribution is threefold: **1)** We present FineCLIP, which clearly incorporates the multi-grained contrastive learning paradigm and the real-time self-distillation scheme to achieve better fine-grained understanding. **2)** We develop an automated region-text data generation pipeline utilizing advanced LVLMs, demonstrating its effectiveness in providing valuable fine-grained semantics. **3)** Extensive experiments on dense prediction and image-level benchmarks show that our FineCLIP consistently outperforms previous arts and exhibits promising scalability.

## 2 Methodology

### Preliminary

**CLIP Architecture.** CLIP is a dual-tower architecture composed of a vision encoder \(\) (ViT ) and a language encoder \(\) (BERT ). Given an image-text pair, CLIP outputs the visual \([CLS]\) token \(v\), visual dense features \(\) corresponding to image patches, textual \([CLS]\) token \(t\) and textual dense features of text tokens. The \([CLS]\) tokens \(v\) and \(t\) serve as global image and text embedding, respectively. During the pre-training, the global contrastive loss is computed with \(v\) and \(t\) for instance-level alignment. In downstream applications, the global embeddings \(v\) and \(t\) are crucial in image-level tasks such as image classification and image-text retrieval, whereas visual dense features \(\) are vital for dense prediction tasks like object detection and semantic segmentation.

**Problem Definition.** Our aim is to develop a comprehensive representation space where visual and semantic features are both globally and locally aligned, which contributes to create a robust vision-language model that can effectively address both image-level and dense prediction tasks.

To achieve the goal, the model must satisfy two key requirements at both global and regional levels: **1)** Given a text \(T\) that describes the content of an image \(I\), the image embedding \(v\) should be matched to text embedding \(t\). **2)** Given a regional text \(T^{r}\) describing the content of a specific region \(r\) within the image \(I\), both the local visual feature \(p^{r}\), pooled from visual dense features \(\) according to \(r\), and region visual embedding \(v^{r}\) of region crop \(I^{r}\) should align with region text embedding \(t^{r}\) of \(T^{r}\).

### FineCLIP

**Overview.** As depicted in Figure 1, FineCLIP adopts the same architecture with CLIP, which consists of the vision encoder \(\) and the language encoder \(\), but employs more complex inputs and objectives.

The input batch of FineCLIP contains image-text pairs \(\{I_{i},T_{i}\}_{i}\), region-text pairs \(\{I^{r}_{j},T^{r}_{j}\}_{j}\) and a set of corresponding region coordinates \(\{r_{j}\}_{j}\), where \(\) and \(\) refers to the batch size and region count. Noted that the regions \(\{I^{r}_{j}\}_{j}\) are cropped from images \(\{I_{i}\}_{i}\) according to region coordinates\(\{r_{j}\}_{j}\), and region texts \(\{T^{r}_{j}\}_{j}\) are generated by LVLMs.

Recall the image and text processing of CLIP introduced in Section 2.1. For image-text pairs \(\{I_{i},T_{i}\}_{i}\) and region-text pairs \(\{I^{r}_{j},T^{r}_{j}\}_{j}\), FineCLIP outputs the corresponding global image-text embeddings \(\{v_{i},t_{i}\}_{i}\) and \(\{v^{r}_{j},t^{r}_{j}\}_{j}\), respectively. Moreover, FineCLIP extracts regional visual representations \(\{p^{r}_{j}\}_{j}\) by pooling visual dense features \(\{_{i}\}_{i}\) according to region coordinates \(\{r_{j}\}_{j}\) with RoIAlign .

**Global Contrastive Learning.** The global contrastive learning realizes the instance-level alignment, which enables FineCLIP to bolster multi-modal global embeddings and acquire rich coarse-grained knowledge. Initially, the cosine similarity between image embedding \(v\) and text embedding \(t\) is calculated as

\[S(v,t)=.\] (1)

The global contrastive loss forces FineCLIP to learn global image and text embeddings by maximizing the cosine similarity to the corresponding text and image embeddings, while minimizing the cosine similarity to other non-corresponding ones in the batch, i.e.

\[L_{GC}=-}_{i=1}^{}(,t_{i})/)}{_{j=1}^{}(S(v_{i},t_{j})/)}+,v_{i})/)}{_{j=1}^{}(S(t_{i},v_{j})/)}),\] (2)

where \(\) is the learnable temperature and initialized with \(1e-2\).

**Real-time Self-distillation Scheme.** Since the global contrastive loss \(L_{GC}\) supervises only global embeddings, it is poorly effective in improving local dense features.

The distillation scheme [55; 60], which transfers robust global representational capabilities to region features, emerges as a promising solution to this problem. The previous work of Wu et al.  implemented the distillation scheme by guiding a trainable student model's local feature extraction

Figure 1: Overall architecture of FineCLIP. For simplicity, the diagram omits unused visual dense features of regions extracted by ViT and textual dense features generated by BERT. By integrating multi-grained contrastive learning as well as a real-time self-distillation scheme, FineCLIP aligns visual global embedding, regional dense features, and textual global embedding into a unified space, acquiring rich coarse and fine-grained knowledge from image-text and generated region-text pairs.

with the global visual embeddings of corresponding image crops from a frozen teacher model. While efficient, this method depends on the pre-trained teacher model and does not support training from scratch. Additionally, because the student model aligns fully with the frozen teacher model, its performance is capped by the teacher's capabilities. This performance ceiling is quickly reached during pre-training, limiting further improvement and scalability, as illustrated in Figure 2.

We propose a real-time self-distillation scheme that relies on and complements global contrastive learning, eliminating the need for a frozen teacher model. Instead of relying on a frozen teacher model for providing high-quality global image embeddings, global contrastive learning consistently enhances the student's global representation capability during the training, allowing the student model to teach itself independently. Importantly, our implementation leverages real-time optimal global representations during the training for guidance, resulting in better scalability and fine-grained understanding ability of the student model.

Specifically, as shown in Figure 1, our real-time self-distillation loss directly maximizes the cosine similarity between region features \(\{p_{j}^{r}\}_{j}\) pooled from image dense features and the visual representations of region crops \(\{v_{j}^{r}\}_{j}\) using the formula

\[L_{SD}=}_{j=1}^{}(1-S(p_{j}^{r},v_{j}^{r})).\] (3)

**Semantically-rich Regional Contrastive learning.** Despite the remarkable enhancements through aforementioned designs, the model still lacks abundant fine-grained knowledge due to the utilization of coarse-grained training data.

Intuitively, to enrich the model with fine-grained knowledge, it is crucial for the model to focus more on the specific content of image regions and learn more precise and detailed semantics. Therefore, we are motivated to implement regional contrastive learning that operates on the level of regions and related descriptions. As shown in Figure 1, we leverage the advanced LVLM to generate high-quality region descriptions with diverse fine-grained semantics.

The regional contrastive loss compels FineCLIP to learn pooled visual region features and text embeddings by maximizing the cosine similarity between matching text and region pairs, while minimizing the similarity with non-matching pairs in the batch, which is defined as follows

\[L_{RC}=-}_{i=1}^{}(^{ r},t_{i}^{r})/)}{_{j=1}^{}(S(p_{i}^{r},t_{j}^{r})/)}+ ^{r},p_{i}^{r})/)}{_{j=1}^{}(S(t_{ i}^{r},p_{j}^{r})/)}).\] (4)

The reason for utilizing pooled visual region features instead of image embeddings of region crops is two-fold. **1)** Supervising pooled region features directly improves model's capability to extract valuable local dense features. **2)** Enhanced by attention mechanisms in ViT, pooled region features encompass a border perception of entire image content compared to image embeddings of regions, which facilitates a deeper semantic understanding.

**Learning Objective.** As depicted in Figure 1, the learning objective of FineCLIP incorporates the above three components. Global contrastive loss \(L_{GC}\) works for enhancing representation capabilities and semantic consistency of global visual embeddings, while the self-distillation loss \(L_{SD}\) is designed to transfer the real-time strong representation capability of global embeddings to local visual features. Additionally, regional contrastive loss \(L_{RC}\) is applied to enrich the FineCLIP with fine-grained knowledge and further improve its local features. The learning objective is

\[L=L_{GC}+*L_{SD}+*L_{RC},\] (5)

where the \(\) and \(\) are hyper-parameters. As a result, FineCLIP fully leverages available diverse semantics and boosts the interactions within multi-grained complementary information.

## 3 Experiment

### Ablation Study of FineCLIP

**Experiment Settings.** In our ablation experiments, we train FineCLIP using \(8\)A800 GPUs on train2017 split of COCO dataset , which includes approximately \(118\)K human-annotatedimage-text pairs along with \(970\)K region-label pairs. To provide abundant fine-grained knowledge, we replace labels provided by COCO with region descriptions generated by BLIP-2 . FineCLIP is initialized by ViT-B/16 with default input image size of \(224 224\) and corresponding BERT from EVA-CLIP . We train FineCLIP for \(10\) epochs using AdamW  optimizer with the batch size of \(32\) per GPU, the learning rate of \(1e-5\), and the weight decay of \(0.1\). The coefficients \(\) and \(\) in learning objective are both set to \(1\). In all experiments, we freeze the language encoder \(\) to reduce computational overheads and improve training stability.

Using the COCO val2017 split, we test FineCLIP designs on the box classification task with pooled region features and image-level retrieval tasks using global embeddings. We report the Top1 and Top5 mean accuracy of all annotated boxes in the box classification task, and the R@1 accuracy of image-to-text and text-to-image retrieval tasks as evaluation indicators.

**Ablation of Objective Components.**

The training objective of FineCLIP includes three components: \(L_{GC}\), \(L_{SD}\) and \(L_{RC}\). We first examine the combination of \(L_{GC}\) and \(L_{SD}\). In Table 1 (row) #1, using only \(L_{SD}\) for supervision causes the model's accuracy on both box classification and retrieval tasks to drop to near zero. This training collapse is predictable, as the global representation ability of the model is entirely compromised without the supervision of \(L_{GC}\) during the training. In Table 1 (row) #3, incorporating \(L_{GC}\) to support global embeddings allows the training to proceed stably. Comparing Table 1 (rows) #2 and #3 reveals that while the self-distillation loss improves the box classification performance, it slightly reduces retrieval performance, leading to a trade-off between local feature extraction and global representations.

As shown in Table 1 (rows) #2 and #5, \(L_{RC}\) significantly improves the model's region feature extraction (\(+5.5\) of Top1 and \(+7.5\) of Top5 mean accuracy on box classification) and slightly enhances retrieval performance. This improvement is attributed to the introduction of region-text pairs, which provide abundant fine-grained knowledge, and the positive effects of \(L_{RC}\) on boosting local features. This result also indicates that, despite the possible noise in the region-text pairs generated by BLIP-2, they still offer valuable information for learning region representations. Ultimately, combining all three components enables FineCLIP to achieve optimal classification performance and competitive retrieval results, as demonstrated in Table 1 (row) #6.

**Ablation of Region Proposal Methods.** Regional distillation and contrastive learning are highly sensitive to the quality of region proposals. To assess their impact on the performance of FineCLIP, we evaluate four different region proposal methods: manual annotations from COCO, FastSAM , RPN , and YOLOv9 . The corresponding results are presented in Table 2. These findings highlight three key insights: **1)** Automated region proposals perform comparably to manually annotated high-quality regions. **2)** More region proposals do not necessarily improve performance. Although FastSAM generates the most proposals, but they appear overly cluttered upon manual inspection, resulting in suboptimal model performance. **3)** RPN provides a moderate number of region proposals with satisfactory and balanced performance. YOLOv9, which focuses on specific object categories, produces fewer but more precise proposals, leading to the best box classification performance of FineCLIP.

    & & & &  &  \\  \# & Region Proposal Method &  Box Classification \\ Top1 \\  &  Retrieval \\ Top5 \\  &  Num of Regions \\ 12T \\  &  Top1 \\  &  Top5 \\  &  12T \\  &  Top5 \\  & 
 12T \\  \\ 
1 & Manual  & 48.4 & 75.6 & **62.2** & **47.6** & 9 & - \\
2 & FastSAM  & 47.1 & 73.7 & 60.7 & 46.5 & 23 & 15 min \\
3 & RPN  & 48.8 & 76.0 & 61.5 & 46.9 & 16 & 10 min \\
4 & YOLOv9  & **49.6** & **76.5** & 60.9 & 47.4 & 7 & 10 min \\   

Table 2: Performance comparisons of FineCLIP using different region proposal methods.

**Ablation of Region Annotation Methods.** We also explore the impact of region annotation methods on FineCLIP's performance. In addition to manual annotation, two main approaches are used for generating region textual descriptions. The first is a rule-based method , which selects the region concept from a predefined concept pool based on similarity scores and integrates it into a description template. The second strategy leverages LVLM for region annotation.

Specifically, we annotate the boxes in COCO train2017 split using the rule-based method, BLIP-2 , and InternLM-XComposer , and evaluate their impact on FineCLIP's performance. For LVLMs, we use the prompt: "Describe this image in one sentence." As shown in Table 3, LVLMs outperforms rule-based method, highlighting the effectiveness of LVLMs in generating valuable fine-grained knowledge. Notably, BLIP-2 provides the greatest improvements to FineCLIP.

### Comparisons with Competing Methods

Following the evaluation setting in Subsection 3.1, we compare FineCLIP with three closely related approaches: CLIP , RegionCLIP , and CLIPSelf . To ensure a fair comparison, all methods adopt the ViT-B/16 as backbone and input images of \(224 224\) resolution. As presented in Table 4, FineCLIP demonstrates the most improvement over Pre-trained CLIP in both dense feature extraction and global representation. In contrast, while RegionCLIP and CLIPSelf achieve moderate gains in box classification tasks, they struggle to maintain the important visual-semantic consistency.

Additionally, we report the time overhead and GPU memory usage of the competing methods during training on COCO train2017 split. Due to FineCLIP's incorporating of the multi-grained contrastive learning paradigm and the self-distillation scheme, it requires comparatively higher GPU memory usage. Nevertheless, it's worth highlighting that the per-epoch training time for FineCLIP (11 minutes) is only marginally longer than that of CLIPSelf (10 minutes) and RegionCLIP (9 minutes). Therefore, the training time for FineCLIP remains well within acceptable limits.

### Comparisons on Scaled Trainset

**Data Preparation.** In this subsection, we evaluate the performance of FineCLIP trained on a scaled dataset. We begin by constructing the trainset based on the Conceptual Caption dataset (CC3M) , which comprises \(3\) million image-text pairs sourced from the internet. To meet training data requirements of FineCLIP, we follow a three-step process to create region-text pairs.

_Image Filtering_: This step retains images with rich contents to facilitate the acquisition of clear and valuable regional proposals. Specifically, we filter out low-resolution images and those that fail to generate regions via the region proposal model. After this process, we retain 2.5 million high-resolution images from CC3M, referred to _"CC2.5M"_.

_Region Proposal_: Based on ablation results in Table 2, we select YOLOv9  to detect objects in images. This process yields \(10.4\) million high-quality regions (approximately four regions per image) and takes around \(4.5\) hours to complete.

_Region Annotation_: According to results shown in Table 3, we utilize the BLIP2-COCO-6.7B model to annotate region proposals, which takes approximately \(12.5\) hours.

    &  &  &  &  \\  &  &  &  &  &  &  \\ 
1 & Pre-trained CLIP  & 31.1 & 53.7 & 59.3 & 42.4 & - & - \\
2 & CLIP  & 42.3 & 66.6 & 62.4 & 48.8 & 6 min & 8G \\
3 & RegionCLIP  & 40.0 & 65.3 & 25.1 & 31.2 & 9 min & 5G \\
4 & CLIPSelf  & 43.7 & 72.3 & 33.3 & 21.2 & 10 min & 6G \\
5 & FineCLIP(Ours) & 48.4 & 75.6 & 62.2 & 47.6 & 11 min & 36G \\   

Table 4: Performance comparisons of FineCLIP and competing methods on COCO.

    & & Box Classification &  \\ \# & Region Annotation Method & Top1 & Top5 & 12T & T2I \\ 
1 & Rule-base  & 43.1 & 71.3 & 58.6 & 46.9 \\
2 & Intern-XComposer  & 47.0 & **75.9** & 60.1 & 45.1 \\
3 & BLIP2  & **48.4** & 75.6 & **62.2** & **47.6** \\   

Table 3: Performance comparisons of FineCLIP using different region annotation methods.

**Zero-shot Comparisons.** To investigate the impact of data scale on model performance, we sample three trainsets of varying sizes from CC2.5M: \(100\)K, \(500\)K, and \(2.5\)M samples. We train FineCLIP and competing methods, including CLIP , RegionCLIP , and CLIPSSelf , using the official open-source code on these three trainsets. To ensure fairness, all methods involved in comparisons adopt ViT-B/16 as backbone and input images of \(224 224\) resolution. For each dataset, we train these models for \(6\) epochs and then evaluate their zero-shot performance on COCO benchmark, using the same metrics of ablation study in Section 3.1.

In Figure 2, we present the accuracy curves of four methods on three tasks as the dataset scales up, with detailed quantitative results shown in Appendix Table 11. Overall, our FineCLIP outperforms the other three competing methods in all cases. Specifically, in terms of fine-grained understanding, as shown in Figure 2(a), FineCLIP significantly surpasses other methods in the box classification task, with a remarkable +\(10.4\) mAP over CLIP and +\(8.0\) mAP over CLIPSelf. Notably, as the dataset size increases, FineCLIP's performance continues to grow rapidly, showing promising scalability, whereas the growth rates of RegionCLIP and CLIPSSelf slow down, indicating that their training gradually converges to a relatively low performance level.

As for the evaluation of image-text alignment, according to Figure 2(b) and (c), FineCLIP even surpasses CLIP in retrieval tasks, demonstrating that enhanced local representation and the acquisition of fine-grained knowledge contribute to more robust global embeddings. In contrast, RegionCLIP and CLIPSSelf fail to maintain semantic consistency in visual embeddings, with their performance deteriorating as the trainset size increases.

**Visualization Results.** Figure 3 presents the visualized attention maps of our FineCLIP on images responding to complete sentences or individual words. We can see from Figure 3(a)-(c) that FineCLIP comprehends sentence semantics and identifies related elements, even tiny objects like a "man" or irregularly shaped items such as a "kite." Figure 3(d)-(e) shows that FineCLIP can well locate different objects within the same image. Interestingly, FineCLIP can also capture abstract concepts such as "looking into the distance" in Figure 3(c)) and actions. For instance, when recognizing "riding", FineCLIP focuses on both the rider and the horse, while for "watching soccer", it highlights human faces and the soccer ball on the ground. These results collectively indicate that FineCLIP effectively learns to understand fine-grained semantics.

### Application to Fine-grained Localization

**Open-Vocabulary Object Detection.** To evaluate whether the improved fine-grained understanding learned with FineCLIP translates to tasks requiring fine-grained localization, we serve FineCLIP as the backbone for open-vocabulary object detection. Following the previous work , we build open-vocabulary object detectors based on F-ViT architecture, which is a two-stage detector baseline built on frozen CLIP ViTs. Considering that the input resolution has a significant influence on detection performance, to ensure the comparison fairness, we utilize the checkpoints of FineCLIP and competing methods trained on CC2.5M with input image size of \(224 224\) for ViT-B/16 and \(336 336\) for ViT-L/14 to initialize the F-ViT. In training hyper-parameters, we employ AdamW

Figure 2: Zero-shot comparisons of models pre-trained on datasets in three different scales.

optimizer with batch size of \(8\), learning rate of \(1e-4\), and weight decay of \(0.1\). We train the models for \(3\) epochs on the OV-COCO  benchmark and \(48\) epochs on the OV-LVIS  benchmark.

For evaluation, we follow previous works [55; 70] to report box AP at IoU \(0.5\) of base, novel and all categories (AP\({}_{50}^{}\), AP\({}_{50}^{}\) and AP\({}_{50}\)) on OV-COCO, and the AP for base, novel and all categories (mAP\({}_{c}\), mAP\({}_{f}\), mAP\({}_{r}\) mAP) on OV-LVIS as comparison indicators. The results are shown in Table 5. By replacing the frozen CLIP ViTs with FineCLIP checkpoints, F-ViT gains significant performance improvements (\(24.7\) vs \(40.0\) A\({}_{50}^{}\) on OV-COCO). Notably, our FineCLIP outperforms the existing open-vocabulary object detection methods on the OV-COCO benchmark under fair training settings. Compared with cutting-edge CLIPSelf, our FineCLIP brings better improvements to baseline F-ViT on both benchmarks. Additionally, we also evaluate the detector trained on OV-LVIS on the validation split of COCO and object365  v1 datasets, with results shown in Table (c)c. FineCLIP surpasses CLIPSelf on the COCO benchmark and achieves similar performance on the Object365 benchmark. All these results demonstrate the effectiveness of our FineCLIP in fine-grained understanding.

**Open-Vocabulary Semantic Segmentation.** Next, we explore the performance of FineCLIP when applied to the open-vocabulary semantic segmentation task. Following the previous work , we

Figure 3: Visualizations of attention maps of our FineCLIP using GAE  on images responding to complete sentences or individual words. (a)-(c) Image attention maps w.r.t. different sentences. (d)(e) Image attention maps w.r.t. different words.

Table 5: Results on open-vocabulary object detection. \(\) means the CLIP ViT backbone is initialized with the checkpoint of the corresponding method trained on CC2.5M.

build the segmentation model based on CatSeg , which utilizes the visual dense features of CLIP ViTs (ViT-B/16 and ViT-L/14 from OpenAI) with a cost-aggregation module. To ensure comparison fairness, we train FineCLIP and CLIPSelf models, which are initialized with pre-trained OpenAI CLIP , on CC2.5M with the same input image resolution of \(384 384\) for ViT-B/16 and \(336 336\) for ViT-L/14, and then replace the backbone of CatSeg with FineCLIP or CLIPSelf ViT for the following segmentation fine-tuning. We fine-tune segmentation models on COCO Stuff  and evaluate them on ADE20K  and PASCAL Context  dataset using mean IoU (mIoU) and mean pixel accuracy (mAcc).

As shown in Table 6, FineCLIP brings non-trivial improvements to CatSeg across most evaluation indicators, surpassing the enhancements provided by CLIPSelf. We observe that both FineCLIP and CLIPSelf cause a decrease in PC-59 mIoU, which may be attributed to the data distribution gap between CC2.5M and PASCAL Context. Furthermore, FineCLIP comprehensively improve the mAcc performance of CatSeg, indicating the enhanced per-pixel classification performance.

### Application to Image-level Task

**Zero-shot Image-Text Retrieval.** We further evaluate FineCLIP on zero-shot cross-modal retrieval tasks using Flicker30K  and MSCOCO , with the results presented in Table 7. Previous works, such as SPARC , PACL , GLoRIA , MGCA , and FILIP , introduce fine-grained losses to extract token-level cross-modal alignments. The results of these methods, re-implemented using the same pre-training datasets (approximately 3.2 billion data points), architecture, and training steps, are taken from SPARC . For fair comparisons, we train competing methods, initialized with pre-trained CLIP parameters, on CC2.5M for \(4\) epochs.

Given that CC2.5M has in a limited data distribution, further training of pre-trained models on this dataset inevitably leads to more or less performance decay. Compared to the baseline CLIP, FineCLIP better maintains retrieval performance, demonstrating its effectiveness in enhancing global

    &  &  &  &  \\  & & mIoU & mAcc & mIoU & mAcc & mIoU & mAcc \\  OVSeg  & ViT-B/16 & 24.8 & - & 7.1 & - & 53.3 & - \\ SAN  & ViT-B/16 & 27.5 & 45.6 & 10.1 & 21.1 & 53.8 & 73.0 \\ SAN  & ViT-L/14 & 32.1 & 50.7 & 12.4 & **25.2** & 57.7 & 77.6 \\ CatSeg  & ViT-B/16 & 27.2 & 41.2 & 8.4 & 16.6 & 57.5 & 74.0 \\ CatSeg  & ViT-L/14 & 31.5 & 46.2 & 10.8 & 20.5 & **62.0** & **78.3** \\ CatSeg+CLIPSelf\({}^{}\) & ViT-B/16 & 29.7 & 45.1 & 10.1 & 17.2 & 55.3 & 73.4 \\ CatSeg+CLIPSelf\({}^{}\) & ViT-L/14 & 34.9 & 52.9 & 13.6 & 23.0 & 59.1 & 77.1 \\  CatSeg+FineCLIP\({}^{}\) & ViT-B/16 & 32.4\({}_{ 2,2}\) & 50.5\({}_{ 9,3}\) & 12.2\({}_{ 4,2}\) & 22.2\({}_{ 6,6}\) & 56.0\({}_{ 1,5}\) & 74.4\({}_{ 0,4}\) \\ CatSeg+FineCLIP\({}^{}\) & ViT-L/14 & **36.1\({}_{ 4,6}\)** & **53.5\({}_{ 7,3}\)** & **14.1\({}_{ 3,3}\)** & 23.8\({}_{ 3,3}\) & 59.9\({}_{ 2,1}\) & **78.3\({}_{ 0}\)** \\   

Table 6: Results on open-vocabulary semantic segmentation. \(\) means the CLIP ViT backbone is initialized with the checkpoint of the corresponding method trained on CC2.5M.

    &  &  \\  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  &  \\  CLIP & 84.0 & 96.1 & 98.2 & 71.6 & 90.3 & 94.1 & 56.2 & 80.6 & 88.2 & 42.4 & 68.6 & 78.3 \\ SPARC & 84.4 & 97.6 & 98.7 & 72.0 & 91.2 & 94.9 & 57.6 & 81.2 & 88.5 & 43.0 & 68.6 & 78.5 \\ PACL & 69.6 & 89.7 & 94.2 & 54.9 & 80.7 & 87.3 & 41.8 & 67.8 & 77.6 & 29.1 & 54.3 & 65.5 \\ GLoRIA & 78.0 & 95.5 & 98.0 & 68.4 & 88.9 & 93.2 & 49.7 & 75.4 & 84.6 & 38.9 & 65.1 & 75.2 \\ MGCA & 82.2 & 96.1 & 98.1 & 67.7 & 88.5 & 93.2 & 57.6 & 80.5 & 87.8 & 39.8 & 65.7 & 75.3 \\ FILIP & 69.0 & 89.8 & 94.0 & 55.8 & 81.5 & 87.9 & 40.2 & 66.0 & 76.3 & 29.5 & 55.3 & 66.3 \\  CLIP\({}^{}\) & 81.6 & 96.2 & 98.0 & 64.9 & 88.3 & 93.6 & 51.1 & 76.4 & 84.9 & 37.6 & 63.9 & 74.3 \\ RegionCLIP\({}^{}\) & 3.9 & 12.2 & 18.4 & 7.9 & 22.7 & 71.3 & 2.0 & 7.1 & 11.5 & 3.4 & 11.8 & 19.0 \\ CLIPSelf\({}^{}\) & 33.8 & 61.7 & 73.0 & 35.0 & 61.3 & 32.7 & 18.8 & 38.9 & 50.4 & 16.1 & 34.5 & 45.1 \\ FineCLIP\({}^{}\) & **82.5** & **96.4** & **98.6** & **67.9** & **89.1** & **94.1** & **54.5** & **78.6** & **85.8** & **40.2** & **66.5** & **76.1** \\   

Table 7: Comparative results on zero-shot image-text retrieval on the Flickr30k and MSCOCO datasets. R@i denotes Recall at i. All approaches adopt ViT-B/16 architecture with input image size of \(224 224\). \(\) indicates that the method is initialized with pre-trained CLIP and further trained on CC2.5M. The methods with gray background are pre-trained on large-scale dataset.

embeddings and capturing valuable semantics. In contrast, RegionCLIP and CLIPSelf struggle to maintain global embeddings for addressing retrieval tasks. Surprisingly, FineCLIP even outperforms most pre-trained models with fine-grained losses, strongly supporting the effectiveness of FineCLIP.

## 4 Related Work

### Fine-grained Understanding in Vision-language Models

Early works focused on learning from intensive human labels by training image classifiers [9; 14; 22; 48; 50]. These classifiers focus only on a limited range of objects, making it difficult to cover a wide range of semantics. CLIP  and its diverse variants [12; 18; 68] popularized learning general visual-language representations by pre-training on noisy large-scale datasets like LAION-400M  and LAION-5B , which exhibit potent representation capabilities and exceptional generalizability.

Despite the great achievements, CLIP model has shown weak alignment between regions and corresponding texts [37; 41; 62]. This problem can be roughly attributes to limitations of 1) CLIP loss which ignores the supervision of visual and textual dense features, and 2) brief coarse captions that are insufficient to allow the model in comprehending image details . Recent works attempted to enhance CLIP's fine-grained understanding by building strong region-text alignment. One line of work leverages region-level annotations for vision-language pre-training [59; 64; 66]. For instance, GLIP  utilized large-scale human-labeled grounding data to align semantics at phase and region level, which achieved stronger performance on fully-supervised detection benchmarks. RegionCLIP  proposed to generate region descriptions by filling plausible concepts into pre-defined templates. Since the semantics of synthesized descriptions are limited by the pre-defined concept pool, RegionCLIP essentially models the finite category classification task. Although this approach brings benefits to downstream detection tasks, it still struggles to cover broad semantic diversity of open-world scenarios. Another remarkable work is CLIPSelf , which proposed to distill the global representation capability of the frozen teacher model to dense feature extraction of the student model. While CLIPSelf successfully enhancing the local representations, the frozen teacher model limits the performance ceiling of the student model, which is not consistent with the intention of pre-training. A separate line of work explored incorporating losses between image patch and text token embeddings to learn representations encoding more fine-grained details [3; 16; 35; 52; 60].

### Fine-grained Image Annotation

The process of generating region-text pairs from images involves two steps: proposing regions and annotating the regions with texts. Common strategies for region proposal include random cropping, using Region Proposal Network (RPN)  or detectors [42; 65] to generate bounding boxes around objects, or leveraging segmentation models  such as SAM . Region annotations can be obtained by expert manual labeling, synthesizing captions using traditional NLP techniques, or generating them with models. A notable work of the second way is Kosmos-2 , which introduced a pseudo-labeling pipeline that utilizes the pre-trained GLIP  to automatically generate fine-grained pseudo-labels of region boxes. In the era of large models, recent LVLMs [2; 31; 36; 53] have demonstrated impressive capabilities of visual understanding, instruction-following and generalization. By setting prompts, users can control the characteristics of outputs, such as length and writing style, to obtain high-quality responses, making it suitable for image fine-grained annotation.

## 5 Conclusion

In this work, we present FineCLIP, a coherent framework that unifies cross-model multi-grained alignment and uni-modal global-to-region guidance. FineCLIP effectively leverages diverse semantics from automatically generated regional data and enhances the interactions of multi-grained complementary information through a real-time self-distillation scheme. Extensive experiments demonstrate the superior performance of FineCLIP in fine-grained understanding tasks, including box classification, open-vocabulary object detection and segmentation, as well as in global representation tasks like image-text retrieval. We believe this study provides valuable insights into related fields.

## Acknowledge

This work is partially supported by National Natural Science Foundation of China (62376274, 62437002) and Beijing Natural Science Foundation (L233008).