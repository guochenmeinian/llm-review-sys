# Can Language Models Solve Graph Problems

in Natural Language?

Heng Wang\({}^{*}\)

Shanghai Feng\({}^{*}\)

Tianxing He\({}^{2}\)

Zhaoxuan Tan\({}^{3}\)

Xiaochuang Han\({}^{2}\)

Yulia Tsvetkov\({}^{2}\)

\({}^{1}\)Xi'an Jiaotong University \({}^{2}\)University of Washington \({}^{3}\)University of Notre Dame

wh2213210554@stu.xjtu.edu.cn, shangbin@cs.washington.edu

equal contribution

###### Abstract

Large language models (LLMs) are increasingly adopted for a variety of tasks with _implicit graphical structures_, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models _do_ demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph.

## 1 Introduction

Originally designed for textual data, large language models (LLMs) are increasingly leveraged for tasks beyond language processing. In robotics and planning, LLMs are adopted to guide agents through structured environments . In theory-of-mind reasoning, LLMs are required to maintain and update local and global graphs that reflect the beliefs of different characters . In structured commonsense reasoning, LLMs are expected to generate graph-based action plans to achieve objectives with diversified prerequisites . In multi-hop question answering, LLMs implicitly find connections and paths among a vast network of entities and concepts . Together these works demonstrate that LLMs are widely adopted for tasks with _implicit graphical structures_ while achieving preliminary success. However, one underlying yet crucial question remains underexplored: _Can LLMs reason with graphs?_ More concretely, _are LLMs capable of mapping textual descriptions of graphs and structures to grounded conceptual spaces and solvinggraph algorithm problems explicitly with natural language?_ The answer to this question has profound implications for large language model applications with implicit graphs and structures, the reasoning ability of LLMs in advanced and graph-based settings, and more.

To this end, we propose the Natural Language Graph (NLGraph) benchmark, a comprehensive testbed of graph and structured reasoning designed for language models and in natural language. NLGraph contains a total of 29,370 problems, covering eight graph reasoning tasks with varying complexity from intuitively simple tasks such as _connectivity_, _cycle_, and _shortest path_ to more complex problems such as _topological sort_, _maximum flow_, _bipartite graph matching_, _Hamilton path_, and _simulating graph neural networks_. We control for problem difficulty through generated graph size, network sparsity, numeric range, and more, presenting easy, medium, and hard subsets in each distinct graph reasoning task to enable fine-grained analysis. In addition to using exact match accuracy as a standard metric, we also design several partial credit solutions for various graph reasoning tasks.

With the NLGraph benchmark, we evaluate whether various large language models (Brown et al., 2020; Ouyang et al., 2022; Bubeck et al., 2023) could perform graph-based reasoning and whether different prompting techniques (Brown et al., 2020; Wei et al., 2022; Kojima et al., 2022; Zhou et al., 2023; Wang et al., 2023) improve the graph reasoning abilities of large language models. Extensive experiments on the NLGraph benchmark demonstrate that:

1. **LLMs _do_ possess preliminary graph reasoning abilities.** Specifically, large language models demonstrate an impressive level of performance that is 37.33% to 57.82% above the random baseline on simple graph reasoning tasks such as connectivity, cycle, and shortest path. With chain-of-thought prompting, LLMs could generate intermediate steps that are sound and accurate while further improving task performance.
2. **The benefit of advanced prompting methods diminishes with complex problems.** On one hand, chain-of-thought (Wei et al., 2022), least-to-most (Zhou et al., 2023), and self-consistency (Wang et al., 2023) successfully enhance the graph reasoning abilities of LLMs on simple tasks such as cycle and shortest path. On the other hand, these approaches are mostly ineffective, even counterproductive in certain settings, on more complex graph reasoning problems such as topological sort and Hamilton path.
3. **Learning from examples did not happen on complex graph reasoning problems.** While in-context learning is widely credited for teaching LLMs to learn from examples (Brown et al., 2020), its benefit on more advanced graph reasoning tasks is unclear: Few-shot in-context learning fails to improve over zero-shot prompting across multiple tasks, while increasing the number of exemplars may even be counterproductive for tasks such as Hamilton path.
4. **LLMs are (un)surprisingly brittle to spurious correlations in problem settings.** We find that in two special cases in the connectivity task (chain and clique), LLMs perform much worse than the general dataset with a performance drop of more than 40% across various settings. This indicates that large language models are implicitly relying on certain spurious correlations (for example, use node mention frequency to determine connectivity), falling short of performing robust structured reasoning in graph-based contexts.

To improve large language models as better graph reasoners, we propose two instruction-based prompting approaches to better elicit the graph reasoning abilities of large language models. _Build-a-Graph prompting_ encourages LLMs to map the textual descriptions of graphs and structures to grounded conceptual spaces (Patel and Pavlick, 2022) before tackling the specific problem through a one-sentence instruction, while _Algorithmic prompting_ instructs LLMs to revisit the algorithmic steps for a given task before learning from in-context exemplars. Experiments demonstrate that build-a-graph and algorithmic prompting successfully empower LLMs to better tackle graph reasoning problems, resulting in 3.07% to 16.85% performance gains across multiple tasks, while the most complicated graph reasoning problems remain an open research question.

## 2 The NLGraph Benchmark

To examine whether language models are capable of reasoning with graphs and structures, we curate and propose the Natural Language Graph (NLGraph) benchmark. Specifically, we first employ a random graph generator to generate graphs and structures while controlling for the network size, graph sparsity, and more. We then adopt the generated graphs as bases to synthetically generate problems for eight graph-based reasoning tasks with varying algorithmic difficulties. We control for problem difficulty within each of the eight tasks, resulting in easy, medium, and hard subsets in each graph reasoning task to enable difficulty scaling and fine-grained analysis.

### Random Graph Generator

We first employ a general-purpose random graph generator to generate base graphs while using the number of nodes and graph density to control for complexity. Formally, to generate a graph \(=\{,\}\) where \(\) and \(\) denote the set of nodes and edges, we specify the number of nodes \(n\), thus \(=\{v_{1},v_{2},,v_{n}\}\), and \(||=n\). We then specify the edge probability \(p\) such that all the edges are generated according to \(P(e_{ij})=p\), where \(e_{ij}\) is an edge from \(v_{i}\) to \(v_{j}\). The edges could be directed or undirected depending on the task. We use varying \(n\) and \(p\) values to control for the complexity of the random graph structure. Building on top of these generated base graphs, we also adopt graph edits and other difficulty control factors tailored for each task.

### Tasks

Armed with the general-purpose random graph generator, we adapt the synthetically generated graphs to eight graph reasoning tasks with varying complexity and describe the problem setups in natural language. Specifically, we first generate easy, medium, and hard subsets of base graphs for each task by controlling for node amount and graph sparsity. We then adapt or edit the base graphs for each task and design queries accordingly. We present an overview of the NLGraph benchmark with specific natural language instructions in Figure 1.

* **Task 1: Connectivity** In an undirected graph \(=\{,\}\), two nodes \(u\) and \(v\) are _connected_ if there exists a sequence of edges from node \(u\) to node \(v\) in \(\). We randomly select two nodes in the base graphs \(u,v\) to ask whether node \(u\) and node \(v\) are connected with a true/false question. We retain a balanced set of questions where half of the node pairs are connected and the other half are not connected by discarding additional questions.
* **Task 2: Cycle** In an undirected graph \(=\{,\}\), a cycle is a non-empty trail \((e_{1},e_{2},,e_{n})\) with a node sequence \((v_{1},v_{2},,v_{n},v_{1})\). We present an undirected graph \(=\{,\}\) to ask whether there exists a cycle through true/false questions. We retain base graphs without cycles as the False subset, while we randomly add edges to these base graphs to generate graphs with cycles as the True subset. We retain a balanced set of cyclic and noncyclic graphs in the dataset.
* **Task 3: Topological Sort** A topological sort of a directed graph is a linear ordering of its nodes such that for every directed edge \((u,v)\) from node \(u\) to node \(v\), \(u\) comes before \(v\) in the ordering.

Figure 1: Overview of the NLGraph Benchmark, featuring eight tasks with varying complexity. We show an intuitive figure representing each task along with the example natural language prompts being passed to the LLMs.

The task is to find a valid topological sort given a directed graph and there could be multiple valid solutions. We ask LLMs to generate a valid topological sort for the given directed graph and employ an external program to examine its correctness.
* **Task 4: Shortest Path** The shortest path between two nodes is the path with the sum of edge weights minimized. Given an undirected graph \(=\{,\}\), a positive weight \(w\) for each edge, and two nodes \(u\) and \(v\), the task is to find the shortest path between node \(u\) and node \(v\) and its corresponding path length. We filter the generated base graphs by specifying that the number of nodes on the correct shortest path is as least \(\), where \(\) is chosen from \(_{}\) to \(_{}\) for each question to sever as an additional difficulty control measure. We adopt two metrics: _exact match_, where the LLM solution is a valid path and optimal, and _partial credit_, where the LLM solution is valid and is the \(_{i}\)-th shortest among all the possible paths (i.e. the number of shorter paths plus one). The partial credit score for each problem can be formulated as \(=1/_{i}\).
* **Task 5: Maximum Flow** Let \(=\{,\}\) be a network with two nodes \(s,t\) being the source and the sink. Each edge is associated with a capacity \(c\), the maximum amount of flow that can pass through the edge. We ask LLMs to generate a plan to route as much flow as possible from the source to the sink. We evaluate in both exact match with the optimal plan and partial credit for this task, where partial credit can be formulated as \(=t/s,&\ t s\\ 0,&\ t>s\), where \(s\) is the flow value under the optimal plan, and \(t\) is the flow value of the solution generated by LLMs.
* **Task 6: Bipartite Graph Matching** In an undirected graph \(=\{,\}\), a matching is a set of edges without common nodes. A bipartite graph is a graph whose nodes can be divided into two disjoint sets \(\) and \(\), and in each set no nodes are adjacent to each other. Given a bipartite graph, the task is to find the matching that maximizes the number of edges. We use an external program to evaluate whether the solution generated by LLMs is valid and optimal.
* **Task 7: Hamilton Path** In an undirected graph, a Hamilton path is a path that visits every node exactly once. Given an undirected graph \(=\{,\}\), the task is to find a valid Hamilton path. We filter generated base graphs to ensure that at least one valid Hamilton path exists and use an external program to evaluate the LLM solution.
* **Task 8: Graph Neural Networks** Given an undirected graph \(=\{,\}\), and a two-dimension node embedding \(_{i}\) for each node, the task is to perform \(\) layers of message passing, _i.e._ to update the node embedding with the sum of all the neighbors' embeddings. Formally, \(_{i}^{(+1)}=_{j_{i}}_{j}^{()}\) where \(_{i}\) denotes the neighborhood of node \(i\) and \(()\) denotes the \(\)-th layer. We use an exact match with the correct node embeddings and two types of partial credits for this task. Specifically, the first partial credit is the percentage of the nodes whose embedding is correct (PC), and the second is the average of all the values' relative errors for the standard answer (RE). The relative error is formulated as \(RE=\), where \(x\) is the value generated by LLMs and \(y\) is the value in the standard answer, averaged across all embedding dimensions.

### Benchmark Statistics

Using the above methodology, we generate the NLGraph benchmark with 5,902 problems in a standard version and 29,370 problems in an extended version. Intuitively easier tasks, including connectivity, cycle, and topological sort problems, are further divided into easy, medium, and hard subsets based on graph size, sparsity, among other difficulty control factors. More algorithmically advanced tasks, including shortest path, maximum flow, bipartite graph matching, Hamilton path,

  
**Subset** & **Connect.** & **Cycle** & **Topo. Sort** & **Shortest Path** & **Max. Flow** & **Bipartite Graph** & **Hamilton Path** & **GNNs** \\  \# Easy & 352 / 730 & 150 / 300 & 180 / 360 & 180 / 360 & 150 / 300 & 300 / 600 & 150 / 300 & 100 / 200 \\ spec. & \(n\): 5-10 & \(n\): 5-10 & \(n\): 5-10 & \(n\): 5-10 & \(n\): 5-10 & \(n\): 6-20 & \(n\): 5-10 & \(n\): 5-8 \\ \# Medium & 1,200 / 8,580 & 600 / 1,800 & 150 / 1,350 & / & / & / & / & / \\ spec. & \(n\): 11-25 & \(n\): 11-25 & \(n\): 11-25 & / & / & / & / & / & / \\ \# Hard & 680 / 7,090 & 400 / 2,000 & 200 / 1,200 & 200 / 1,200 & 210 / 1,260 & 200 / 600 & 140 / 840 \\ spec. & \(n\): 26-35 & \(n\): 26-35 & \(n\): 26-35 & \(n\): 11-20 & \(n\): 11-20 & \(n\): 17-33 & \(n\): 11-20 & \(n\): 9-15 \\   

Table 1: Statistics of the NLGraph benchmark. We use A / B to indicate that there are A and B problems in the standard and extended set of NLGraph. spec. denotes difficulty specifications.

and graph neural networks problems, are divided into easy and hard subsets. We present benchmark statistics in Table 1. Accuracy (whether the true/false answer is correct, whether the proposed solution is valid) is the default evaluation metric, while tasks 4, task 5, and task 8 have additional partial-credit metrics aforementioned. We envision NLGraph as a comprehensive testbed for graph and structured reasoning for large language models.

## 3 Experimental Settings

Based on the NLGraph benchmark, we aim to investigate _whether language models can solve graph algorithm problems in natural language_ by evaluating large language models and different prompting approaches.

BaselinesWe adopt a wide range of prompting approaches as baselines. Specifically, zero-shot prompting (zero-shot), few-shot in-context learning (few-shot) (Brown et al., 2020), chain-of-thought prompting (CoT) (Wei et al., 2022), zero-shot chain-of-thought (0-CoT) (Kojima et al., 2022), least-to-most (LtM) (Zhou et al., 2023), and self-consistency (SC) (Wang et al., 2023) are leveraged to tackle various graph reasoning tasks in the NLGraph benchmark.

We also adopt a random baseline: for true/false questions such as connectivity and cycle, we use random to denote a baseline that randomly selects an answer from true and false with an expected accuracy of 50%; For the shortest path task, random denotes a baseline that randomly selects a valid path between the query node pair. For the maximum flow task, random denotes a baseline that randomly selects a value between 0 and the sum of all the edges' capacities. The performance comparison between different prompting techniques and the random baseline could indicate whether LLMs are capable of performing graph reasoning instead of giving randomly generated answers.

Models and SettingsWe use text-davinci-003 as the default large language model, while we also evaluate three additional LLMs (gpt-3.5-turbo, code-davinci-002, and gpt-4), presenting results in Appendix E. For all baselines except self-consistency, we set temperature \(=0\); For self-consistency prompting, we sample five chain-of-thought responses with temperature \(=0.7\). For few-shot prompting techniques (i.e., few-shot, CoT, LtM, and CoT+SC), the input prompt includes \(k\) exemplars selected from the extended version before the problem of interest. For the connectivity task and cycle task, we set \(k\) to 4, for the GNN task, we set \(k\) to 1 due to the context size limit, while for other tasks \(k\) is 5. We evaluate LLMs and various prompting techniques on the standard set of NLGraph due to monetary costs, while we encourage future research to leverage the extended version for enhanced evaluation.

## 4 Results

### LLMs Have (Preliminary) Graph Reasoning Abilities

We first find that on intuitively simple graph reasoning tasks, large language models achieve impressive performance and demonstrate preliminary graph reasoning abilities. As demonstrated in Table 2, LLM performance on the connectivity, cycle, and shortest path tasks is significantly better than the random baseline, indicating that LLMs are not giving random answers and they _do_ have preliminary graph reasoning abilities. In the first two tasks with true/false questions, LLM performance is 37.33% to 57.82% higher than random with CoT or CoT+SC prompting. The perfo

    &  &  &  \\   & **Easy** & **Medium** & **Hard** & **Avg.** & **Easy** & **Medium** & **Hard** & **Avg.** & **Easy** & **Hard** & **Easy** (PC)** & **Hard** (PC)** & **Avg.** \\  random & 50,00 & 500.0 & 500.0 & 500.0 & 500.0 & 500.0 & 500.0 & 500.0 & 60.7 & 66.9 & 14.73 & 13.81 & 17.81 \\ zero-shot & 38.81 & 72.75 & 63.38 & 71.31 & 500.0 & 500.0 & 500.0 & 500.0 & 29.40 & 12.00 & 46.00 & 26.76 & 30.79 \\ few-shot & 93.75 & 83.83 & 76.61 & 84.73 & 80.00 & **70.00** & **61.00** & **70.33** & 31.11 & 26.00 & 49.19 & 35.73 & 35.51 \\ CoT & **94.32** & 82.17 & 77.21 & 84.57 & **84.67** & 63.33 & 53.52 & 66.75 & 63.89 & **29.50** & 76.84 & 35.79 & 51.51 \\ CoT & 79.55 & 65.83 & 68.53 & 71.30 & 55.33 & 57.67 & 49.00 & 54.00 & 54.00 & **54.00** & 62.39 & **43.95** & 32.03 \\ CoT+SC & 93.18 & **84.50** & **82.79** & **86.82** & 82.00 & 63.67 & 53.50 & 66.39 & **68.89** & 29.00 & **80.25** & 38.47 & **54.15** \\   

Table 2: Model performance on the connectivity, cycle, and shortest path tasks. PC denotes partial credit. Large language models with CoT or CoT+SC prompting greatly outperforms the random baseline by 37.33% to 57.82%, indicating that LLMs have preliminary graph reasoning abilities.

in the zero-shot setting, as the accuracy is 33.81% and 23.33% higher on the connectivity and shortest path task than random even without any exemplars and chain-of-thought reasoning. Though the shortest path is intuitively harder than the first two true/false tasks since it requires generating the specific shortest path in the response, LLM combined with CoT and CoT+SC achieves an accuracy 22.81% to 62.83% higher than random. Together these results demonstrate that large language models _do_ possess preliminary abilities to process graphs and structures in input texts.

### Mixed Results with Advanced Prompting

Table 2 shows that advanced prompting techniques such as chain-of-thought [Wei et al., 2022] and self-consistency [Wang et al., 2023] successfully improve performance on simple graph reasoning tasks. For the task of simulating graph neural networks (Table 3), chain-of-thought also significantly improves model performance. However, these approaches can be ineffective, even counterproductive on more complex graph reasoning tasks. From the results on the topological sort and maximum flow task (Figure 2), we observe that CoT, CoT+SC, and LtM prompting generally underperform few-shot prompting. We believe this may be because LLMs fail to learn the correct way to generate intermediate steps in the face of complex graph reasoning tasks. This casts doubt on the generalizability of CoT, LtM, and self-consistency to more advanced graph reasoning problems.

### In-Context Learning Can be Counterproductive

Although in-context learning is widely attributed to teaching LLMs to learn from in-context exemplars [Brown et al., 2020], we observe that few-shot in-context learning fails to improve LLM performance over complex graph reasoning problems. For tasks such as Hamilton path and bipartite graph matching (Figure 3), zero-shot prompting generally outperforms few-shot learning with in-context exemplars. The results suggest that LLMs fail to learn from the in-context exemplars when the problem involves hundreds of graph-based reasoning steps. In-context exemplars might even distract large language models, evident in that few-shot learning underperforms zero-shot learning by 1.00% to 10.48% on Hamilton path and bipartite graph matching. We further study the correlation between the number of exemplars and model performance on the Hamilton path task. As illustrated in Figure 4, when the number of exemplars increases, model performance is not trending up, and the performance is consistently lower than zero-shot prompting. These results further suggest that in-context learning could be counterproductive in complex structured reasoning.

Figure 2: (left) Model performance on the topological sort task. CoT, LtM, and self-consistency are mostly ineffective on this problem. (right) Model performance on the maximum flow task. few-shot prompting outperforms CoT+SC prompting on both easy and hard subsets, suggesting that LLMs fall short of generating valid intermediate steps to solve the more complex graph reasoning problem. Together these results demonstrate that advanced prompting is ineffective for advanced graph reasoning.

  
**Method** & PC (\(\)) & Acc (\(\)) & RE (\(\)) \\  zero-shot & 13.61 & 0.00 & 20.04 \\ few-shot & 20.04 & 0.00 & 37.83 \\ CoT & **64.55** & **31.00** & 14.34 \\
0-CoT & 13.85 & 0.00 & 44.55 \\ CoT+SC & 63.92 & 28.00 & **13.28** \\   

Table 3: Model performance on the task of simulating graph neural networks. PC and RE are two partial credit metrics introduced in §2.2. Chain-of-thought prompting significantly improves the model performance across all metrics.

### LLMs are (Un)surprisingly Brittle

Although large language models achieved performance that is significantly better than random on simple graph reasoning tasks (SS4.1), we hypothesize that LLMs may be able to reach the correct answer by leveraging certain spurious correlations. For example, on the connectivity task, since nodes with higher degrees are more frequently mentioned and they are more likely to be connected, LLMs might just be counting node mentions instead of actually finding paths. To this end, we design two special cases (chain and clique) for the connectivity task that are the exact opposite of the spurious correlation.

ChainWe firstly generate a graph with \(k\) components, where each components is a chain. Formally, \(=\{,\}\) can be divided into subgraphs \(_{1},_{2},,_{k}\), where subgraph \(_{i}\) consists of a chain of nodes and edges \(v_{i1},e_{i1},v_{i2},e_{i2},,e_{it_{i}},v_{it_{i}}\). We then randomly select query node pairs that are at the two ends of a chain, _i.e._\(v_{i1}\) and \(v_{it_{i}}\). These nodes only have a degree of one but they are actually connected through the chain structure. We curate a chain dataset with 120 examples.

CliqueWe first generate a graph with \(k\) densely connected subgraphs. Formally, \(=\{,\}\) can be divided into subgraphs \(_{1},_{2},,_{k}\), where subgraph \(_{i}\) is generated by the random graph generator (SS2.1) with a high edge probability \(p\{0.7,1.0\}\). We then randomly select query node pairs with each pair in two different densely connected subgraphs, \(_{i}\) and \(_{j}\) (\(i j\)). These nodes are associated with a large number of edges and thus are frequently mentioned in the natural language prompt, but they belong to two different subgraphs and are not connected. We curate a clique dataset with 120 examples.

We evaluate LLMs with different prompting approaches on the two special cases and compare performances with the general connectivity task in Table 4. LLM performs much worse than ordinary

  
**Dataset** & zero-shot & few-shot & CoT & 0-CoT & CoT+SC & Avg. \\  General & 74.67 & 83.33 & 85.33 & 66.00 & 82.67 & 78.40 \\ Chain & 51.67 (-23.00) & 45.00 (-35.33) & 40.83 (-44.50) & 92.50 (+26.50) & 44.17 (-38.50) & 54.83 (-23.57) \\ Clique & 60.83 (-13.84) & 73.33 (-10.00) & 85.00 (-0.33) & 52.50 (-13.50) & 83.33 (+0.66) & 71.00 (-7.40) \\   

Table 4: Model performance on the chain and clique subset of the connectivity task. Large language models indeed rely on spurious correlations in problem settings, evident in the reduced performance on the two special cases compared to the general connectivity task.

Figure 4: Model performance on the Hamilton path task with an increasing amount of exemplars. When more in-context exemplars are introduced, model performance is not trending up and is consistently lower than zero-shot prompting in both difficulty settings.

Figure 3: (left) Model performance on the Hamilton path task. zero-shot prompting consistently outperforms all other prompting techniques. (right) Model performance on the bipartite graph matching task. The effect of in-context learning and advanced prompting is also mostly marginal in this complex graph reasoning problem. Together these results demonstrate that in-context learning can be counterproductive in advanced graph reasoning problems.

cases with a performance drop of more than 40% across various settings. This suggests that LLMs are indeed (un)surprisingly vulnerable to spurious correlations in structured reasoning.

## 5 Making Language Models Better Graph Reasoners

### Methodology

We propose two instruction-based prompting techniques that improve the graph reasoning ability of LLMs, which can be used together with in-context learning and chain-of-thought prompting.

Build-a-Graph Prompting (BAG)We hypothesize that it might be helpful to map the textual descriptions of graphs to grounded conceptual spaces (Patel et al., 2021) before tackling the specific problem, _i.e._ visualizing the graph structure on an implicit mental sketchpad. To this end, we propose to use instruction-based prompting by appending _"Let's construct a graph with the nodes and edges first."_ after the textual description of the graph is explicitly given. We envision this straightforward instruction would provide LLMs with the buffer zone to digest graph information, map them to grounded conceptual spaces, and better prepare for the incoming query.

Algorithmic PromptingWe hypothesize that in order to generate sound and accurate solutions, LLMs would benefit from revisiting and reciting the relevant algorithm for a given task (Sun et al., 2023). To this end, we propose to first prepend the algorithm details before the in-context examples by adding "We can use a Depth-First Search (DFS) algorithm..." for the shortest path task. We similarly provide the algorithm descriptions of other graph reasoning tasks in their respective prompts. We envision algorithmic prompting as empowering LLMs with a general understanding of how to solve the problem before actually solving it.

We illustrate our proposed build-a-graph prompting and algorithmic prompting in Figure 5. In the following, we use these two approaches together with chain-of-thought prompting and evaluate on three representative tasks in the NLGraph benchmark with varying difficulties.

### Results

Table 5 shows the results of our proposed prompting methods on three representative tasks with varying complexity. On the two easier tasks, cycle and shortest path, our instruction-based prompting methods resulted in a 3.07% to 16.85% performance gain across the two tasks. However, on the more complex task, Hamilton path, the two natural language instructions are largely ineffective. The result suggests that the two simple instruction-based prompting techniques successfully improve LLMs' graph reasoning abilities on relatively easy graph reasoning tasks, while how to make LLMs better

Figure 5: Overview of Build-a-Graph prompting and Algorithmic prompting, aiming to make LLMs better at reasoning with graphs by encouraging conceptual grounding and algorithmic reflection. Red underline indicates our proposed instructions while green background indicates LLMs’ generation.

at reasoning on complex graph reasoning problems remains an open research question. We further explore different variants of the instructions in Appendix D. The NLGraph benchmark also empowers the evaluation of future instructions and solutions towards making LLMs better graph reasoners.

## 6 Related Work

LLMs for tasks with implicit graphical structures.Previous works explored the use of LLMs on tasks with implicit graph structures: Huang et al. (2022) find that LLMs have the ability to ground high-level tasks (e.g. "make breakfast") to a set of actionable steps (e.g. "open fridge") in structured synthetic environments. Valmeekam et al. (2022) explore the possibility of using LLMs for commonsense planning. In theory-of-mind reasoning, Adhikari et al. (2020); Ammanabrolu and Riedl (2021); Sclar et al. (2023) maintain and update structured world representations as the world state change to operate in interactive and situated environments. In structured commonsense reasoning, where LLMs are given a natural language input and then asked to generate a graph such as an event graph (Tandon et al., 2019), a reasoning-graph (Madaan et al., 2021) or an argument explanation graph (Saha et al., 2021); Madaan et al. (2022) find that a code language model (codex) with tasks framed as code generation tasks outperforms other strong LMs. In multi-hop question answering or knowledge probing, LLMs implicitly find connections and paths among a vast network of entities and concepts (Creswell et al., 2023; Yu et al., 2022; Zhang et al., 2022a; He et al., 2021). Recently, Chen et al. (2023) explore the potential of LLMs on the graph node classification task. Together these works demonstrate that LLMs are increasingly adopted for tasks and settings with implicit graphs and structures, while whether LLMs are robust at graph reasoning remains underexplored and may hinder the progress of these structure-aware applications. In this work, we propose the NLGraph benchmark as a comprehensive testbed towards evaluating the graph reasoning abilities of large language models.

LLMs for few-shot reasoning.There is a long line of work on evaluating LLMs' reasoning ability in an in-context learning setting, including arithmetic reasoning, logical reasoning, common sense reasoning, and more. Particularly, simple math problem datasets such as AQUA Ling et al. (2017), GSM8K (Cobbe et al., 2021), and SVAMP (Patel et al., 2021) are used for evaluating arithmetic reasoning (He-Yueya et al., 2023; Touvron et al., 2023; Shi et al., 2023). Welleck et al. (2021) developed NaturalProofs, a multi-domain dataset for studying mathematical reasoning in natural language, while Welleck et al. (2022) study LLMs' ability to generate the next step in mathematical proof and generate full proofs. LLMs have also been evaluated on logical reasoning tasks, including symbolic tasks like Coin Flip and Last Letter Concatenation (Wei et al., 2022) and Logic Grid Puzzle on the BIG-BENCH (Srivastava et al., 2023). Commonsense reasoning datasets (Talmor et al., 2019; Geva et al., 2021) are also proposed to evaluate large language models. Most relevant to our work, various proposals to evaluate and augment the algorithm reasoning abilities of large language models are explored (Zhou et al., 2022; Lee and Kim, 2023; Zelikman et al., 2023; Liu et al., 2023). In this work, we focus on evaluating and enhancing LLMs on graph-based reasoning and algorithmic tasks inspired by the increasing usage of LLMs in tasks with implicit graphs and structures.

## 7 Conclusion

In this work, we investigate whether LLMs are capable of explicit graph reasoning, _i.e._, solving graph algorithm problems in natural language, across various problem categories and prompting techniques. To this end, we curate the NLGraph benchmark, a comprehensive test bed of graph-based reasoning in natural language, with 29,370 problems covering eight tasks with varying complexity. By evaluating

    &  &  &  \\   & **Easy** & **Medium** & **Hard** & **Avg.** & **Easy** & **Hard** & **Easy** (**PC**) & **Hard** (**PC**) & **Avg.** & **Easy** & **Hard** & **Avg.** \\  CoT & 84.67 & 63.33 & 53.25 & 66.75 & 63.89 & 29.50 & 76.84 & 35.79 & 51.51 & **40.00** & **8.00** & **24.00** \\ CoT+BAG & **86.00** & 69.33 & 62.00 & **72.44** & **67.78** & **33.50** & **79.20** & **42.56** & **55.76** & 38.67 & 6.00 & 22.34 \\ CoT+ALORITHM & 77.33 & **74.00** & **64.00** & 71.78 & 63.89 & 28.00 & 76.06 & 38.70 & 51.66 & 36.67 & 7.50 & 22.09 \\   

Table 5: Model performance on the connectivity, cycle, and shortest path tasks with our proposed BaG and algorithm prompting. On the two easier tasks, cycle and shortest path, our instruction-based prompting techniques resulted in a 3.07% to 16.85% performance gain across the two tasks. More complex graph reasoning tasks such as Hamilton path remain an open research question.

LLMs and prompting approaches on the NLGraph benchmark, we find that 1) LLMs do possess preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning may diminish on complex reasoning tasks, while 3) LLMs are brittle to spurious correlations in problem settings. We then propose Build-a-Graph and Algorithmic Prompting, two simple instruction-based approaches that bring notable performance gains across multiple tasks. Improving LLMs' graph reasoning abilities on complex and nuanced graph reasoning tasks remains an open research question, and we encourage future work to develop upon our proposed NLGraph benchmark.