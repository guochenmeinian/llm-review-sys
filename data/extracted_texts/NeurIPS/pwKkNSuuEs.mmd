# Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification

Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification

Yunshi Wen

Rensselaer Polytechnic

Institute

weny2@rpi.edu

&Tengfei Ma

Stony Brook University

tengefei.ma@stonybrook.edu

&Tsui-Wei Weng

University of California,

San Diego

lweng@ucsd.edu

&Lam M. Nguyen

IBM Research

lamnguyen.mltd@ibm.com

&Anak Agung Julius

Rensselaer Polytechnic

Institute

agung@ecse.rpi.edu

Corresponding to Yunshi Wen (weny2@rpi.edu) and Tengfei Ma (tengefei.ma@stonybrook.edu)

###### Abstract

In time-series analysis, many recent works seek to provide a unified view and representation for time-series across multiple domains, leading to the development of foundation models for time-series data. Despite diverse modeling techniques, existing models are black boxes and fail to provide insights and explanations about their representations. In this paper, we present VQShape, a pre-trained, generalizable, and interpretable model for time-series representation learning and classification. By introducing a novel representation for time-series data, we forge a connection between the latent space of VQShape and shape-level features. Using vector quantization, we show that time-series from different domains can be described using a unified set of low-dimensional codes, where each code can be represented as an abstracted shape in the time domain. On classification tasks, we show that the representations of VQShape can be utilized to build interpretable classifiers, achieving comparable performance to specialist models. Additionally, in zero-shot learning, VQShape and its codebook can generalize to previously unseen datasets and domains that are not included in the pre-training process. The code and pre-trained weights are available at https://github.com/YunshiWen/VQShape.

## 1 Introduction

As one of the fundamental forms of data, time-series (TS) exist in a wide range of domains and applications, including healthcare, weather, traffic, motions, human activities, sensors, etc. Modeling TS data across multiple domains has been a challenging task since TS data can have diverse sampling rates, lengths, magnitudes, frequencies, and noise levels. Due to this heterogeneity, most of the existing machine learning methods for TS modeling focus only on a single dataset or a single domain.

Recently, motivated by the success of large pre-trained models in natural language processing and computer vision, various approaches adopted from these two fields have been proposed to build a unified view and feature space for TS data from different domains [Liang et al., 2024]. Most of the models use a transformer as the backbone and pre-train it on a diverse range of datasets [Zerveas et al., 2021, Nie et al., 2023, Goswami et al., 2024]. These methods have achieved great success in TS representation learning, benefiting various downstream tasks and demonstrating their generalizability. Despite their success, most of them remain black boxes since they cannot provide human-understandable representations. While tokenizers have played increasingly important roles in pre-trained models for language and vision, in TS, pre-training is often conducted by predicting the next or masked timestamp, time window, or patch, lacking the concept of discrete tokens as in LLMs. Very recently, Talukder et al. (2024) developed TOTEM, which utilizes VQ-VAE (van den Oord et al., 2017) to obtain the codebook and reconstruct the TS. Nevertheless, like all other VQ-VAE models, the tokens from the codebook are just latent vector representations and lack physical meaning.

Alternatively, in interpretable TS modeling, shapelets have been recognized as interpretable and expressive features for TS data. Initially defined as TS subsequences that discriminate different categories in classification (Ye and Keogh, 2011), they were later generalized to representative patterns (Grabocka et al., 2014). Specifically, shapelets can transform TS data into low-dimensional representations either in the form of the distance between a shapelet and a TS, or as a logical predicate that measures the probability of a shapelet existing in a TS (Lines et al., 2012). However, despite their effectiveness in classification tasks, this shape-level feature lacks flexibility since shapelets with pre-defined lengths are optimized for capturing discriminative features for making dataset-specific predictions. For example, when measuring human motion with accelerometers, an adult and a child performing the same gesture may record TS with different offsets, scales, and durations. Although they share the same shape-level concept, multiple shapelets are required to describe them separately. Additionally, shapelet-based interpretable models are specialized to a single dataset, and the learned shapelets fail to transfer to different domains.

In this paper, motivated by the limitations of existing pre-trained models and interpretable models in TS, we propose VQshape, a self-supervised pre-trained model that provides abstracted shapes as interpretable and generalizable tokens for TS modeling. Firstly, we decompose a TS subsequence into a set of attributes, including abstracted shape, offset, scale, start time, and duration. By incorporating vector quantization, VQshape learns a codebook of abstracted shapes that are generalizable and descriptive, representing TS from various domains. Evaluated on various classification tasks, and without fine-tuning, VQshape achieves comparable performance to black-box pre-trained models while additionally providing interpretable latent-space tokens and representations to describe TS data. Our contributions are summarized below:

* We present an interpretable representation composed of abstracted shapes and attributes to describe TS data based on shape-level features, which enables the learning of dataset-agnostic interpretable features.
* We introduce VQshape, to the best of our knowledge, the first self-supervised pre-trained model that extracts interpretable representations from any TS data. VQshape also learns a codebook containing abstracted shapes that generalize to multiple datasets.
* Pre-trained on diverse datasets and without fine-tuning, VQshape achieves comparable performance to existing black-box models on benchmark classification datasets. We explicitly demonstrate that the representations and VQShape are interpretable and generalizable for unseen datasets and domains.

## 2 Related Work

Deep learning methods for TS analysis.Deep learning methods are increasingly applied to TS analysis. Existing methods can be categorized into two groups depending on whether they use a Transformer structure as the backbone. For non-Transformer-based models, classical deep learning models such as MLP, CNN, and ResNet demonstrate decent performance on various tasks (Wang et al., 2017). Recent methods have developed various feature engineering techniques to model explicit features of TS data. TimesNet (Wu et al., 2023) transforms TS into 2D space to capture multi-period features in a modularized way, achieving state-of-the-art performance on various tasks. TS2Vec (Yue et al., 2022) employs hierarchical contrastive learning for unsupervised representation learning of TS data. T-Rep (Fraikin et al., 2024) introduces a self-supervised representation learning approach by augmenting the TS with time embeddings, providing additional temporal structure to the latent space.

Transformers have been increasingly applied to TS analysis, but usually with some modifications to the original structure. For example, Autoformer (Wu et al., 2021) modifies the attention mechanism by incorporating an Auto-Correlation mechanism to capture temporal dependencies. When applying Transformers to real-valued data, transforming the inputs into patches has been recognized as an effective approach for images (Dosovitskiy et al., 2021) since the tokens could contain more semantic meaning, like a "word" in language. Similarly, PatchTST (Nie et al., 2023) shows that TS analysis also benefits from combining patched inputs with Transformers, viewing a TS as a sequence of 64 "words".

Pre-trained Models for TS data.The success of large pre-trained models in language and vision motivates the development of foundation models for TS analysis. Existing approaches aim to find a unified view for TS data from different perspectives. For example, TST (Zerveas et al., 2021) uses the Transformer model (Vaswani et al., 2017) and is pre-trained using masked reconstruction, while TimeGPT-1 (Garza et al., 2023) is pre-trained by generating a forecasting window. MOMENT (Goswami et al., 2024) extends a patch-based Transformer (Nie et al., 2023) to multiple datasets by unifying the lengths of TS data using padding and sub-sampling. The model is also pre-trained to reconstruct the masked patches. TOTEM (Talukder et al., 2024) applies a convolutional neural network (CNN) encoder to raw TS data and uses vector quantization (VQ) on the encoder outputs, providing a discrete and domain-invariant codebook for TS data. TOTEM is pre-trained as a VQ-VAE (van den Oord et al., 2017) to reconstruct the whole TS, viewing the latent-space codes from convolutions as a unified representation. UniTS (Gao et al., 2024) introduces a prompt-based method to unify predictive and generative tasks within a single model and pre-training process. Although these methods learn representations that benefit various downstream tasks and demonstrate generalizability, these pre-trained models remain black boxes since they cannot provide human-understandable representations.

## 3 Proposed Method

Towards interpretable TS modeling, we first present the formulations of shape-level representations, describing univariate TS data using a set of abstracted shapes and attributes. Then, we introduce the architecture of VQShape and its components with detailed workflow and products from each step.

Notations.Let \((,)=\{(x_{i},y_{i})|i=1,,N\}\) denote a TS classification dataset with \(N\) samples, where \(x_{i}^{M T}\) is a multivariate TS sample and \(y_{i}\{1,,C\}\) is the class label. Here, \(M\) is the number of variables, \(T\) is the length in timestamp, and \(C\) is the number of categories. Each multivariate TS sample \(x_{i}\) can be viewed as a set of univariate TS samples where \(x_{i}^{m}^{T}\) denotes the TS at the \(m^{}\) variable. For simplicity in notations, in this paper, \(x_{i,t_{1}:t_{2}}^{m}\) denotes a subsequence of \(x_{i}^{m}\) between timestamp \( Tt_{1}\) and \( Tt_{2}\), where \(t_{1},t_{2}\) are relative positions.

### Shape-level representation

For a univariate TS \(x\), a subsequence \(s_{k}\) can be represented by an attribute tuple \(_{k}=(z_{k},_{k},_{k},t_{k},l_{k})\) where

* \(z_{k}^{d_{}}\) is the code for abstracted shape of \(s_{k}\),
* \(_{k}^{1}\) is the offset of \(s_{k}\),
* \(_{k}^{1}\) is the scale (standard deviation) of \(s_{k}\) and \(_{k}>0\),
* \(t_{k}^{1}\) is the relative starting position of \(s_{k}\) in \(x\) and \(0 t_{k} 1-l_{}\),
* \(l_{k}^{1}\) is the relative length of \(s_{k}\) w.r.t. the length of \(x\) and \(l_{} l_{k} 1-t_{k}\).

Here, \(l_{}\) is the hyperparameter that defines the minimum length of a shape. We set \(l_{}=1/64\) as it is the length of a patch. In this work, we develop a pre-trained transformer model to produce a set of attribute tuples \(=\{_{k} k=1,,K\}\) given a univariate TS \(x\). Additionally, the model learns a codebook of abstracted shape \(z\) that is reusable and generalizable for datasets from different domains.

### VQShape Architecture

The VQShape model contains a TS encoder \(\), a TS decoder \(\), a latent-space codebook \(\), a shape decoder \(\), an attribute encoder \(_{}\), and an attribute decoder \(_{}\). An overview of VQShape is presented in Figure 1. We then present a detailed formulation for each component.

TS Encoding.VQShape contains a patch-based transformer encoder (Nie et al., 2023; Goswami et al., 2024) which first transforms a univariate TS \(x\) into \(K\) non-overlapping fixed-length patches with dimension \(d^{}\). Then, the patches are encoded by learnable linear projection and additive position embedding, forming patch embeddings that serve as inputs to a transformer model. The transformer outputs \(K\) latent embeddings \(^{d^{}}\). Formally, the TS encoder is denoted by \(\{_{k}^{d^{}} k=1,,K\}=(x)\). Note that \(_{k}\) could contain information from all patches instead of only the \(k^{}\) patch.

Attribute Decoding.The attribute decoder \(_{}\) takes a latent embedding \(h_{k}\) and extracts an attribute tuple \(_{k}=(_{k},_{k},_{k},t_{k},l_{k})\). Formally, \(_{}\) performs

\[_{k}=(_{k},_{k},_{k},t_{k},l_{k})= _{}(h_{k}),\;_{k}=f_{z} (h_{k}),\\ _{k}=f_{}(h_{k}),\\ _{k}=(f_{}(h_{k})),\\ t_{k}=(f_{t}(h_{k}))(1-l_{}),\\ l_{k}=(f_{l}(h_{k}))(1-t_{k})+l_{}.\] (1)

Each decoding function in \(\{f_{z},f_{},f_{},f_{t},f_{l}\}\) is implemented using a multi-layer perceptron (MLP) with one hidden layer and ReLU activation. Following a common notation (Esser et al., 2021), \(\) denotes the attribute tuple before quantization.

Codebook and Vector-Quantization.The latent-space codebook is denoted by \(=\{z_{q}^{d^{}} q=1,,N^{}\}\). To learn a generalizable codebook that contains only the abstracted shape-level features, we use low-dimensional codes with \(d^{}=8\). This configuration also creates a bottleneck for reconstruction, minimizing additional information that can be inferred besides the abstracted shapes. The quantization follows VQ-VAE (van den Oord et al., 2017) that selects the discrete code based on Euclidean distance where

\[z_{k}=*{arg\,min}_{z_{q}}\|_{k}-z_{q}\|.\] (2)

Shape Decoding.The abstracted shape of a TS subsequence is a sequence with its length, offset, and scale information removed through normalizations. Given \(_{k}=(z_{k},_{k},_{k},t_{k},l_{k})\), we first extract the target subsequence from \(x\) specified by \(t_{k}\) and \(l_{k}\) denoted by \(x_{t_{k}:t_{k}+l_{k}}\). Then, \(x_{t:t+l}\) is interpolated to a fixed length of \(d^{}\) to remove the length information. The shape decoder \(\) takes \(z_{k}\) and outputs another sequence with the same length. Formally, for \(_{k}\), this step produces two sequences

\[s_{k}^{}^{d^{s}} =(x_{t_{k}:t_{k}+l_{k}}),\] (3) \[s_{k}^{d^{s}} =(z_{k})_{k}+_{k}.\]

Note that the output of \(\) is normalized such that \((z_{k})\) has the offset and scale information removed.

Figure 1: Overview of VQShape

Attribute encoding and reconstruction.The attribute tuple after quantization \(_{k}=(z_{k},_{k},_{k},t_{k},l_{k})\) is transformed by a learnable linear projection denoted by \(h_{k}^{d^{}}=(_{k})\). Then, the TS decoder \(\) takes \(\{h_{k} k=1,,K\}\) and outputs the reconstructed TS \(\).

## 4 Pre-training

VQShape is pre-trained on diverse datasets to learn dataset-agnostic features and tokens. In this section, we introduce the self-supervised training strategies and objectives of VQShape. Then, we discuss the representations the model could provide to down-stream tasks.

### Objectives

The optimization objectives of VQShape during the pre-training stage are summarized below.

Reconstructions.Analogous to most of the VQ-VAE approaches, VQShape is trained to accurately reconstruct the input TS to learn essential latent-space representations for modeling TS data. Additionally, to provide interpretable representations, the decoded shapes should be similar to the actual subsequences. Therefore, the reconstruction minimizes two objectives:

Time-series reconstruction: \[_{x}=\|x-\|_{2}^{2},\] (4) Subsequence reconstruction: \[_{s}=_{k=1}^{K}\|s_{k}^{}-s_{k}\|_ {2}^{2}.\] (5)

Vector Quantization.We follow VQ-VAE (van den Oord et al., 2017) to define the vector-quantization objective which trains the encoder \(\) and codebook \(\). Additionally, inspired by Yu et al. (2024), we add additional entropy terms to encourage codebook usage. We find these terms could improve pre-training stability and avoid collapse of codebook usage. The objective for learning the codebook is defined by

\[_{}=-(z)\|_{2}^{2}+ _{}\|()-z\|_{2}^{2}}_{}+ [H(q(,))]-H( [q(,)])}_{},\] (6)

where \(()\) is the stop-gradient operator and \(H()\) is the entropy function for discrete variables. \(q(z,)=(\|-z_{k}\|_{2}^{2} z_{k} ])\) measures the distance between \(\) and all codes in \(\) as a categorical distribution.

Disentanglement of shapes.In Equation 5, the attributes \((z_{k},_{l},_{k})\) are optimized towards accurate subsequence reconstructions. It is important to note that, since \((t_{k},l_{k})\) defines \(s_{k}^{}\), they are essential for learning the abstracted shapes and the codebook. However, it is challenging to use gradients from reconstruction in Equation 4 solely to learn \((t_{k},l_{k})\) for making informative subsequence selection. Therefore, we introduce an additional regularization that encourages the latent-space tokens (attributes) to capture shape-level information with diverse positions and scales. This regularization is defined as

\[_{}=}_{k_{1}=1}^{K}_{ k_{2}=1}^{K}(k_{1} k_{2})(-\| (t_{k_{1}},l_{k_{1}})-(t_{k_{2}},l_{k_{2}})\|_{2}^{2}),\] (7) \[\ \ (t_{k},l_{k})=[ (t_{k})(l_{k})/(l_{})\\ (t_{k})(l_{k})/(l_{})].\]

In Equation 7, \((t_{k},l_{k})\) defines a coordinate transformation which maps \((t_{k},l_{k})\) into a space where (1) small \(l_{k}\) values become more diverse and (2) large \(l_{k}\) values from different \(t_{k}\) become more concentrated. By making different \((t_{k},l_{k})\) diverse in this space, \(_{}\) encourages the model to capture disentangled shape-level information while increasing the use of short sequences to capture local details. Figure 8 visualizes an example of transformation \(\). \(\) is a hyperparameter that defines a threshold distance in the transformed coordinate where two \((t_{k},l_{k})\) samples are considered sufficiently diverse.

The overall pre-training objective is to minimize

\[_{}=_{x}_{x}+_{s}_{ s}+_{}_{}+_{}_{ },\] (8)

where \(_{x},_{s},_{},_{}\) are hyperparameters that define the weighting between the components. During pre-training of VQShape, we set \(_{x}=_{s}=_{}=1\), \(_{}=0.8\), and \(_{}=0.25\).

Design Analysis.Overall, the encoding process in VQShape (Transformer encoder and attribute decoder) introduces an inductive bias by representing and summarizing univariate TS using a set of abstracted shapes along with their position, length, offset, and scale. The pre-training objectives guide the components toward learning interpretable representations (via subsequence reconstruction in Equation 5) and disentangled representations (via regularization in Equation 7), while preserving the information necessary to describe the TS (via reconstruction in Equation 4). These objectives introduce interpretability to the conventional deep autoencoder structure. By pre-training on diverse datasets with a universal codebook, VQShape further leverages this inductive bias to produce discrete and dataset-agnostic representations, resulting in a vocabulary of abstracted shapes that can be used as primitives to describe TS data.

Model Configurations.The settings of VQShape related to the model size correspond to those of the MOMENT-Small[Goswami et al., 2024] model. Specifically, we interpolate all the input univariate TS \(x\) to have length \(T=512\), which is broken into \(K=64\) patches with \(d^{}=8\). The Transformer layers in the encoder \(\) and decoder \(\) have 8 heads, an embedding dimension \(d^{}=512\), and a feed-forward layer of size \(2048\). We employ an asymmetric structure with an 8-layer encoder \(\) and a 2-layer decoder \(\)[He et al., 2022]. The codebook \(\) contains \(N^{}=512\) codes, each with dimension \(d^{}=8\). The subsequences \(s_{k}^{}\) and decoded sequences \(s_{k}\) have length \(d^{}=128\). We set the minimum shape length \(l=1/64\). With these settings, VQShape has 37.1 million parameters.

In the pre-training stage, we train VQShape with the AdamW optimizer, using weight decay \(=0.01\), \(_{1}=0.9\), \(_{2}=0.999\), gradient clipping of \(1.0\), and an effective batch size of 2048. We employ a cosine learning rate schedule with an initial learning rate of \(1e^{-4}\), a final learning rate of \(1e^{-5}\), and 1 epoch of linear warm-up. The pre-training dataset contains univariate TS extracted from the training split of 29 datasets from the UEA Multivariate TS Classification Archive [Bagnall et al., 2018], excluding the InsectWingbeat dataset, resulting in 1,387,642 univariate TS. We train VQShape for \(50\) epochs on this dataset using bfloat-16 mixed precision.

### Representations for down-stream tasks

VQShape provides two types of representations: **Latent-space Tokens** and **Code Histogram**.

Tokens.Similar to the latent-space feature map of typical VQ approaches such as VQ-VAE [van den Oord et al., 2017] and VQ-GAN [Esser et al., 2021], VQShape also provides a set of tokens as representations. For an input univariate TS \(x\), the token representations are composed as \(^{K(d^{+4})}=\{_{k}=(z_{k},_{k}, _{k},t_{k},l_{k}) k=1,,K\}\). The token representations can be useful for general down-stream tasks but are less interpretable than the code histogram representations in classification tasks.

Code Histogram.Inspired by Concept Bottleneck Models (CBMs) [Koh et al., 2020] developed in computer vision, we can also view each \(z_{q}\) as a concept for TS data. As CBMs have concept scores as representations, VQShape provides a similar representation in the form of a histogram of codes. Based on Equation 2, we can also have a vector of code indices

\[=[q_{k}=*{arg\,min}_{q=1,,N^{}}\| _{k}-z_{q}\| k=1,,K].\] (9)

Then, the code histogram representation is defined as \(^{N^{}}=()\) where each element in \(\) is the frequency of index \(q\) in \(\). Intuitively, the code histogram representation is analogous to BOSS [Schafer, 2015] but with non-deterministic window size and dataset-agnostic symbols. In classification tasks, this type of representation can be more interpretable since classifiers based on these features are able to produce rule-like predictions that are straightforward to interpret and understand.

[MISSING_PAGE_FAIL:7]

and its codebook can also generalize to datasets and domains not observed during pre-training. To demonstrate cross-domain generalizability, we train another model using 9 datasets from the UEA archive that are commonly selected to train and evaluate deep learning models (Zerveas et al., 2021; Wu et al., 2023), and then evaluate it on all 29 datasets. The right half of Table 2 summarizes the performance of this model, compared with MOMENT and UniTS trained with the same setup. We observe that VQShape and MOMENT trained on fewer datasets result in similar but slightly worse performance, indicating that the representations learned by the models can generalize to unseen domains.

### Interpretability

Universal Codebook of Abstracted Shapes.One of the most essential components of VQShape is the dataset-agnostic codebook that contains abstracted shapes. In Figure 6 of Appendix C.1, we decode all 512 codes in the codebook of VQShape to visualize their corresponding abstracted shapes. We observe that a large number of codes are decoded into similar shapes, which suggests that the codebook size can be further reduced. We then visualize the distribution of codes learned from pre-training (see Figure 7) which contains about 60 clusters. Inspired by this observation, we train a variant named VQShape-64 with codebook size \(N^{}=64\). Figure 2 presents the decoded codebook of VQShape-64.

Interpretable Representations.Overall, the encoding of VQShape can be interpreted as "TS \(x\) is decomposed into (shape \(z_{1}\) with offset \(_{1}\) and scale \(_{1}\), at \(t_{1}\) with length \(l_{1}\)), \(\)", and the decoding can be interpreted as "The composition of (shape \(z_{1}\) with offset \(_{1}\) and scale \(_{1}\), at \(t_{1}\) with length \(l_{1}\)), \(\) becomes \(\)". Figure 3 includes an example of interpretable representations learned by VQShape. From visualizations, we can confirm that VQShape can learn abstracted shapes that capture shape-level information with various positions and scales.

Discriminative Representations for Classification.We further show that the interpretable representations produced by VQShape also capture discriminative patterns that distinguish different categories in classification tasks. Figure 4 visualizes the average code histogram for samples from two categories. From the feature maps, it is obvious that several codes have significant differences in frequency between the two categories; these serve as discriminative features in classification tasks. We decode and visualize their corresponding abstracted shapes. The intuition provided by the histogram features can be interpreted as: "Samples from the CW circle category usually contain shape \(s_{61}\) in variate 1, and samples from the CCW circle category contain shape \(s_{33}\) in variate 3, etc."

   Pre-trained on: &  &  \\   & MOMENT & UniTS & VQShape & MOMENT & UniTS & VQShape \\  Mean Accuracy & 0.697 & 0.581 & **0.723** & 0.697 & 0.559 & **0.723** \\ Median Accuracy & 0.736 & 0.649 & **0.810** & 0.733 & 0.649 & **0.792** \\ Mean Rank & 1.655 & 2.862 & **1.483** & 1.655 & 2.966 & **1.310** \\ Num. Top-1 & 13 & 0 & **16** & 11 & 0 & **20** \\   

Table 2: Comparison between three models pre-trained on all or a subset of the UEA datasets. The best cases are marked with bold. Complete results are presented in Table 6.

Figure 2: Visualization of the decoded codebook from VQShape-64.

[MISSING_PAGE_EMPTY:9]

[MISSING_PAGE_FAIL:10]