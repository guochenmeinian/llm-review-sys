# RectifID: Personalizing Rectified Flow with

Anchored Classifier Guidance

Zhicheng Sun\({}^{1}\), Zhenhao Yang\({}^{3}\), Yang Jin\({}^{1}\), Haozhe Chi\({}^{1}\), Kun Xu\({}^{2}\), Kun Xu\({}^{2}\),

Liwei Chen\({}^{2}\), Hao Jiang\({}^{1}\), Yang Song, Kun Gai\({}^{2}\), Yadong Mu\({}^{1}\)

\({}^{1}\)Peking University, \({}^{2}\)Kuaishou Technology,

\({}^{3}\)University of Electronic Science and Technology of China

{sunzc,myd}@pku.edu.cn

Corresponding author.

###### Abstract

Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at https://github.com/feifeiobama/RectifID.

## 1 Introduction

Recent advances in diffusion models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020) have ignited a surge of research into their customizability. A prominent example is personalized image generation, which aims to integrate user-defined subjects into the generated image. This plays a pivotal role in AI art creation, empowering users to produce identity-consistent images with greater customizability beyond text prompts. Nevertheless, there remain significant challenges in accurately preserving the subject's identity and being flexible to a variety of personalization needs.

Existing personalization methods are limited in these two aspects, as they require an extra finetuning or pre-training stage. For example, the pioneering works (Gal et al., 2023; Ruiz et al., 2023) finetune conditional embeddings or model parameters per subject, resulting in suboptimal efficiency and identity consistency due to lack of domain knowledge. On the other hand, the recently prevailing tuning-free methods (Wei et al., 2023; Ye et al., 2023; Li et al., 2024; Wang et al., 2024) pre-train a conditioning adapter to encode subject features into the generation process. However, their models must be pre-trained on extensive domain-specific data, _e.g_. LAION-Face 50M (Zheng et al., 2022), which is costly in the first place, and cannot be transferred flexibly across different data domains, _e.g_. from human faces to common live subjects and objects, and even to multiple subjects.

To address both challenges of identity consistency and flexibility, we advocate a training-free approach that utilizes the guidance of a pre-trained discriminator without extra training of the generative model. This methodology is well-known as classifier guidance (Dhariwal and Nichol, 2021), which modifiesan existing denoising process using the gradient from a pre-trained classifier. The rationale behind our exploitation is twofold: first, it directly harnesses the discriminator's domain knowledge for identity preservation, which may be a cost-effective substitute for training on domain-specific datasets; secondly, keeping the diffusion model intact allows for plug-and-play combination with different discriminators, as shown in Fig. 1, which enhances its flexibility across various personalization tasks. However, the original classifier guidance is largely limited in the reliance on a special classifier trained on noised inputs. Despite recent efforts to approximate the guidance (Kim et al., 2022; Liu et al., 2023; Wallace et al., 2023; Ben-Hamu et al., 2024), they have mainly focused on computational efficiency, and have yet to achieve sophisticated performance on personalization tasks.

Technically, to extend classifier guidance for personalized image generation, our work builds on a recent framework named rectified flow (Liu et al., 2023) featuring strong theoretical properties, _e.g_. the straightness of its sampling trajectory. By approximating the rectified flow to be ideally straight, the original classifier guidance is reformulated as a simple fixed-point problem concerning only the trajectory endpoints, thus naturally overcoming its reliance on a special noise-aware classifier. This allows flexible reuse of image discriminators for identity preservation in personalization tasks. Furthermore, we propose to anchor the classifier-guided flow trajectory to a reference trajectory to improve the stability of its solving process, which provides a convergence guarantee in theoretical scenarios and proves even more crucial in practice. Lastly, a clear connection is established between our derived anchored classifier guidance and the existing approximation practices.

The derived method is implemented for a practical class of rectified flow (Yan et al., 2024) assumed to be piecewise straight, in combination with face or object discriminators. This provides flexibility for a range of personalization tasks on human faces, live subjects, certain objects, and multiple subjects. Extensive experimental results on these tasks clearly validate the effectiveness of our approach. Our contributions are summarized as follows: (1) We propose a training-free approach to flexibly personalize rectified flow, based on a fixed-point formulation of classifier guidance. (2) To improve its stability, we anchor the flow trajectory to a reference trajectory, which yields a theoretical convergence guarantee when the flow is ideally straight. (3) The proposed method is implemented on a relaxed piecewise rectified flow and demonstrates advantageous results in various personalization tasks.

## 2 Background

**Personalized image generation** studies incorporating user-specified subjects into the text-to-image generation pipeline. To preserve the subject's identity, the seminal works Textual Inversion (Gal et al., 2023) and DreamBooth (Ruiz et al., 2023) finetune conditional embeddings or model parameters for each subject, which imposes high computational costs. Subsequent literature reports to more efficient parameters (Hu et al., 2022; Han et al., 2023; Yuan et al., 2023) or a pre-trained subject encoder (Wei et al., 2023; Ye et al., 2023) to allow personalization within a few minutes or even without tuning. At the other end, a recent trend is the reuse of existing discriminators to improve identity consistency, such as extracting discriminative face features as the condition (Ye et al., 2023; Wang et al., 2024) or as a training objective for the encoder (Peng et al., 2024; Gal et al., 2024; Guo et al., 2024). However, these models require extensive pre-training on domain-specific data, _e.g_. LAION-Face 50M (Zheng et al., 2022). In contrast, our method is a training-free approach that exploits existing discriminators based on the recent rectified flow model, allowing flexible personalization for a variety of tasks.

Figure 1: Illustration of training-free classifier guidance. Left: an off-the-shelf discriminator can be reused to steer the existing diffusion model, _e.g_. rectified flow, to generate identity-preserving images. Right: personalized image generation results for human faces and objects using our proposed method.

**Rectified flow** is an instance of flow-based generative models (Song et al., 2021; Xu et al., 2022; Liu et al., 2023a; Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023). They aim to learn a velocity field \(\) that maps random noise \(_{0}_{0}\) to samples from a complex distribution \(_{1}_{}\) via an ordinary differential equation (ODE):

\[d_{t}=(_{t},t)dt.\] (1)

Instead of directly solving the ODE (Chen et al., 2018), rectified flow (Liu et al., 2023a) simply learns a linear interpolation between the two distributions by minimizing the following objective:

\[_{}_{0}^{1}[\|(_{1}-_{0})-(_{t},t)\|^{2}]dt.\] (2)

This procedure straightens the flow trajectory and thus allows faster sampling. Ideally, a well-trained rectified flow is a straight flow with uniform velocity \((_{t},t)=(_{0},0)\) following:

\[_{t}=_{0}+(_{t},t)t.\] (3)

Recently, rectified flow has shown promising efficiency (Liu et al., 2024b) and quality (Esser et al., 2024; Yan et al., 2024) in text-to-image generation. Our work extends its capabilities and theoretical properties to personalized image generation via classifier guidance.

**Classifier guidance,** initially proposed for class-conditioned diffusion models (Dhariwal and Nichol, 2021), introduces a test-time mechanism to adjust the predicted noise \((_{t},t)\) based on the guidance from a classifier. Given condition \(c\) and classifier output \(p(c|_{t})\), the adjustment is formulated as:

\[(_{t},t)=(_{t},t)+s_{t}_{ _{t}} p(c|_{t}),\] (4)

where \(s\) denotes the guidance scale, and \(_{t}\) is determined by the noise schedule. Noteworthy, the condition \(c\) is not restricted to class labels, but can be extended to text (Nichol et al., 2022) and beyond. However, it is largely limited by the reliance on a noise-aware classifier for noised inputs \(_{t}\), which restricts the use of most pre-trained discriminators that only predict the likelihood \(p(c|_{1})\) on clean images. Consequently, its usefulness is limited in practice. See Appendix B for more related work.

## 3 Method

This work aims at customizing rectified flow with classifier guidance. We show that the above limit of classifier guidance may be solved with a simple fixed-point solution for rectified flow (Section 3.1). To improve its stability, Section 3.2 proposes a new anchored classifier guidance with a convergence guarantee. Lastly, the implementation and applications are described in Sections 3.3 and 3.4.

### Classifier Guidance for Rectified Flow

This section first derives the vanilla classifier guidance for rectified flow, and then present an initial attempt to remove the need for the noise-aware classifier \(p(c|_{t})\), which is based on a new fixed-point solution of classifier guidance assuming that the rectified flow is ideally straight.

The classifier guidance can be derived as modifying the potential associated with the rectified flow. According to the Helmholtz decomposition, a velocity field \(v\) may be decomposed into:

\[(_{t},t)=_{_{t}}(_{t},t)+(_{t}, t),\] (5)

where \(\) is a scaler potential and \(\) is a divergence-free rotation field. They can be determined by solving the Poisson's equation \(^{2}(_{t},t)=(_{t},t)\), but this is beyond our focus. We directly add a new potential proportional to the log-likelihood to simulate classifier guidance, as follows:

\[}(_{t},t)=_{_{t}}[(_{t},t)+s  p(c|_{t})]+(_{t},t),\] (6)

where \(s\) denotes the guidance scale, and \(\) is used to distinguish the new flow from the original one. Subtracting the above two equations yields the vanilla classifier guidance, similar in form to Eq. (4):

\[}(_{t},t)=(_{t},t)+s_{_{t}} p (c|_{t}).\] (7)

While this classifier guidance should allow for test-time conditioning of rectified flow, it cannot be applied in the absence of noise-aware classifier \(p(c|_{t})\). In the following, we show that this limitation may be overcome by exploiting the straightness property of rectified flow.

**Attempt to bypass noise-aware classifier.** We make a key observation that the intermediate classifier guidance \(_{_{1}} p(c|_{t})\) can be circumvented by approximating the new flow trajectory to be straight (an ideal guidance should preserve the properties of rectified flow) and focusing on the endpoint \(_{1}\). Formally, substituting \(t=1\) in Eqs. (3) and (7) allows skipping any intermediate guidance terms:

\[_{1} =_{0}+}(_{1},1)\] (8) \[=_{0}+(_{1},1)+s_{_{1}} p( c|_{1}).\]

Interestingly, this turns out to be a fixed-point problem w.r.t. \(_{1}\), suggesting that the classifier-guided flow trajectory could be solved iteratively by numerical methods such as the fixed-point iteration, without knowing the noise-aware classifier. This greatly enhances the flexibility of classifier guidance to a variety of off-the-shelf image discriminators. However, our further analysis reveals both empirical (Section 4.3) and theoretical evidence questioning the convergence of this iterative approach:

**Proposition 1**.: _There exist Lipschitz continuous functions \((_{1},1)\) and \(_{_{1}} p(c|_{1})\), such that the fixed-point iteration for solving the target trajectory based on Eq. (8) is not guaranteed to converge by the Banach fixed-point theorem (Banach, 1922), irrespective of the choice of \(s>0\)._

Proof.: Consider the following construction. Let \((_{1},1)\) and \(_{1} p(c|_{1})\) be identical functions with a Lipschitz constant greater than 1. Then, the Lipschitz constant of the right-hand side of the fixed-point equation is greater than 1 for any \(s>0\). This violates the Banach fixed-point theorem's requirement for a Lipschitz constant strictly less than 1, thus convergence is not guaranteed. 

Proposition 1 shows that the derived fixed-point solution may not always be practical. Intuitively, even with a small perturbation at \(_{1}\), the target flow trajectory estimated by Eq. (8) could diverge significantly after iterated updates, which hinders the controllability of rectified flow. This motivates us to anchor the target flow trajectory to a reference trajectory to stabilize its solving process.

### Anchored Classifier Guidance

This section establishes a new type of classifier guidance based on a reference trajectory. The idea is to constrain the new trajectory to be straight and near the reference trajectory, as illustrated in Fig. 2. It provides a better convergence guarantee and a certain degree of interpretability.

Let \(}_{t}\) and \(_{t}\) represent two flow trajectories originating from the common starting point \(_{0}\) with or without classifier guidance. The symbol \(\) denotes the new trajectory with classifier guidance. Assuming the two trajectories are close and straight (ideally preserving the characteristics of rectified flow), their difference can be estimated based on Eq. (7) and the first-order Taylor expansion:

\[}(}_{t},t)-(_{t},t) =(}_{t},t)+s_{}_{t}} p (c|}_{t})-(_{t},t)\] (9) \[[_{_{t}}(_{t},t)]( {}_{t}-_{t})+s_{}_{t}} p(c|}_ {t})\] \[=[_{_{t}}(_{t},t)t]( }(}_{t},t)-(_{t},t))+s_{}_{t}}  p(c|}_{t})\] \[=[-_{_{t}}_{0}](}( }_{t},t)-(_{t},t))+s_{}_{t}} p (c|}_{t}),\]

where the final step is derived from Eq. (3). From here, a new form of classifier guidance is obtained:

\[}(}_{t},t)=(_{t},t)+s[_{ {z}_{0}}_{t}]_{}_{t}} p(c|}_{t}).\] (10)

Figure 2: Illustration of anchored classifier guidance for rectified flow. Left: we propose to guide the flow trajectory while implicitly enforcing it to flow straight and stay close to a reference trajectory. Right: comparison of the new trajectory with the reference trajectory (in the last three sampling steps).

This new classifier guidance anchors the target velocity to a predetermined reference velocity \((_{t},t)\) that is dependent only on \(t\) and irrelevant to the current state \(}_{t}\), thereby constraining the target flow trajectory near the reference trajectory and improving its controllability. Next, we extend its applicability to the more common scenarios where the noise-aware classifier \(p(c|}_{t})\) is absent.

**Bypassing noise-aware classifier.** To circumvent the intermediate classifier guidance, we follow the previous practice of substituting \(t=1\) into Eqs. (3) and (10), yielding a fixed-point problem w.r.t. \(}_{1}\):

\[}_{1}=_{1}+s[_{_{0}}_{1}] _{}_{1}} p(c|}_{1}).\] (11)

As can be seen, the target endpoint \(}_{1}\) is also anchored to a known reference point \(_{1}\), which should enhance its stability in the solving process via fixed-point iteration or alternative numerical methods. Below, we exemplify its favorable theoretical property using the fixed-point iteration:

**Proposition 2**.: _Suppose \(_{}_{1}} p(c|}_{1})\) is Lipschitz continuous w.r.t. \(}_{1}\), the fixed-point iteration to solve the target trajectory by Eq. (11) exhibits at least linear convergence with a properly chosen \(s\)._

Proof.: Denote the Frobenius norm of \(_{_{0}}_{1}\) as \(L_{1}\), and the Lipschitz constant of \(_{}_{1}} p(c|}_{1})\) as \(L_{2}\). The Lipschitz constant of the right side of the equation w.r.t. \(}_{1}\) is upper bounded by \(s L_{1} L_{2}\). By choosing a sufficiently small \(s<1/(L_{1} L_{2})\), the Lipschitz constant of the right side is reduced to less than 1, thus ensuring linear convergence by the Banach fixed-point theorem. 

**Interpretation of new classifier guidance.** In addition to the above convergence guarantee, our new classifier guidance can be interpreted by connecting with gradient backpropagation. From Eq. (10) one could obtain an estimate of the intermediate classifier guidance (see Appendix A for derivation):

\[_{}_{t}} p(c|}_{t})=[_{_{t}} _{1}]_{}_{1}} p(c|}_{1}).\] (12)

This suggests that our method is secretly estimating the intermediate classifier guidance with gradient backpropagation. While this is implicitly assumed or directly used in recent works that adapt classifier guidance to flow-based models (Wallace et al., 2023; Liu et al., 2023; Ben-Hamu et al., 2024), it is explicitly derived here based on a very different assumption (the straightness of the flow trajectory). Such a connection helps to rationalize both our adopted assumption and the existing practice.

### Practical Algorithm

**Extension to piecewise rectified flow.** The above analyses are performed based on the assumption that the rectified flow is well-trained and straight, which is often not the case in reality. In fact, existing rectified flow usually require multiple sampling steps due to the inherent curvature in the flow trajectory. Inspired by Yan et al. (2024), we adopt a relaxed assumption during implementation that the rectified flow is piecewise linear. Let there be \(K\) time windows \(\{[t_{k},t_{k-1})\}_{k=K}^{1}\) where \(1=t_{K}>>t_{k}>t_{k-1}>>t_{0}=0\), and the flow trajectory is assumed straight within each time window, then the inference procedure can be expressed as:

\[_{t}=_{t_{k-1}}+(_{t},t)(t-t_{k-1}),\] (13)

where \(k\) is the index of the time window \([t_{k},t_{k-1})\) that \(t\) belongs to. Note that this framework is also compatible with the vanilla rectified flow by setting \(K\) to the number of sampling steps.

The previously derived fixed-point iteration in Eq. (11) cannot be applied directly, since its assumption that the target and reference trajectory segments share the same starting point (_e.g._\(}_{t_{k-1}}=_{t_{k-1}}\)) may be violated after updates. A quick fix is to reinitialize the reference trajectory every round with predictions for updated target starting points. This allows to formulate the following problem:

\[}_{t_{k}} =_{t_{k}}^{e}+s[_{_{t_{k-1}}}_{t _{k}}^{e}]_{}_{t_{k}}} p(c|}_{t_{k}})\] (14) \[=_{t_{k}}^{e}+s[_{_{t_{k-1}}}_{ 1}^{e}]_{}_{1}} p(c|}_{1}),\]

where the last step is obtained by recursively applying Eq. (12) to backpropagate the guidance signal, and a superscript \(e\) is introduced to denote the endpoint of the previous trajectory segment, as the above fix may disconnect different segments of the reference trajectory. Meanwhile, a straight-through estimator (Bengio et al., 2013) is applied to allow computing the Jacobian across different trajectory segments by estimating the Jacobian between the adjacent points \(_{t_{k}}\) and \(_{t_{k}}^{e}\) with \(\).

**Solving target flow trajectory.** The target trajectory under classifier guidance, subject to Eq. (14), can be estimated iteratively by starting with \(}_{t_{k}}^{}=_{t_{k}}^{}\) and performing the following iterations:

\[_{t_{k}}^{[i+1]} =_{t_{k}}^{[i]}+_{t_{k}}^{e[i+1]}-_{t _{k}}^{e[i]}}_{}+}_{t_{k}}^{[i]}-_{t_{k}}^{e[i]}}_{},\] (15) \[}_{t_{k}}^{[i+1]} =_{t_{k}}^{e[i+1]}+s[_{_{t_{k-1}}^{[i +1]}}_{1}^{e[i+1]}]_{}_{1}^{[i]}} p(c|}_{1}^{[i]}),\] (16)

where the superscript \([i]\) is used to indicate the target and reference trajectories at the \(i\)-th iteration. Specifically, Eq. (15) implements the prediction of updated target starting points by extrapolating from history updates, and Eq. (16) tackles the derived problem. Note that there are more sophisticated methods for predicting target starting points and solving this problem, _e.g_. quasi-Newton methods, but we opt for simplicity here and leave their exploration to future work. The complete procedure for implementing the proposed classifier guidance is summarized by Algorithm 1.

```
0: rectified flow \(v\), classifier \(p(c|)\), sampling steps \(K\), iterations \(N\).  Initialize reference trajectory \(_{t_{k}}^{}\) from \(\). \(\) Eq. (13)  Initialize target trajectory \(}_{t_{k}}^{}_{t_{k}}^{}\). for\(i 0\) to \(N-1\)do  Update reference trajectory with predicted starting points \(_{t_{k}}^{[i+1]}\). \(\) Eq. (15)  Update target trajectory \(}_{t_{k}}^{[i+1]}\) with classifier output \(p(c|}_{1}^{[i]})\). \(\) Eq. (16) Output: target trajectory \(}_{t_{k}}^{[N]}\) subject to condition \(c\). ```

**Algorithm 1** Anchored Classifier Guidance

### Applications

The proposed algorithm is flexible for various personalized image generation tasks on human faces and common subjects. Given a reference image \(_{}\) and our generated image \(}_{1}\), we use their feature similarity on an off-the-shelf discriminator \(f\), _e.g_. the face specialist ArcFace (Deng et al., 2019) or a self-supervised backbone DINOv2 (Oquab et al., 2023), as classifier guidance. In addition, to improve the guidance signal, a face detector or an open-vocabulary object detector \(g\) is employed to locate the identity-relevant region for feature extraction. Formally, the classifier output is as follows:

\[p(c|}_{1}^{[i]})=(f g(}_{1}^{[i]}),f  g(_{})).\] (17)

More details are described in Appendix C. Notably, both configurations can be flexibly extended to a multi-subject scenario by incorporating a bipartite matching step between multiple detected subjects.

## 4 Experiments

### Experimental Settings

**Datasets.** Our method does not involve training data, as it operates only at test time. For face-centric evaluation, we follow Pang et al. (2024) to evaluate on 20 prompts with the first 200 images from CelebA-HQ (Liu et al., 2015; Karras et al., 2018) as reference images. For subject-driven generation, we conduct qualitative studies on a subset of examples from the DreamBooth dataset (Ruiz et al., 2023b), spanning 10 subjects across two live subject categories and three object categories.

**Metrics.** Three metrics are considered: identity similarity, prompt consistency, and computation time. The first two are measured using an ArcFace model (Deng et al., 2019) and CLIP encoders (Radford et al., 2021), while the latter is tested on an NVIDIA A800 GPU. We reproduce the latest methods IP-Adapter (Ye et al., 2023), PhotoMaker (Li et al., 2024), and InstantID (Wang et al., 2024b) for a comprehensive comparison, and also include the existing baselines in Pang et al. (2024).

**Implementation details.** We experiment with a frozen piecewise rectified flow (Yan et al., 2024) finetuned from Stable Diffusion 1.5 (Rombach et al., 2022) with 4 equally divided time windows. The number of sampling steps is set to a minimum \(K=4\) given the memory overhead of backpropagation.

A naive implementation takes 14GB of GPU memory, which fits on a range of consumer-grade GPUs. More results on alternative rectified flows can be found in Appendix D.4. For hyperparameters, the guidance scale is fixed to \(s=1\) in quantitative evaluation. Meanwhile, for stability, the gradient is normalized following Karunaratanakul et al. (2024). The number of iterations is set to \(N=100\).

### Main Results

**Face-centric personalization.** Table 1 and Fig. 3 compare our method (denoted RectifID) with extensive baselines. Overall, our training-free approach achieves state-of-the-art performance in quantitative evaluations. Specifically, we observe that: (1) our SD 1.5-based implementation yields the highest identity similarity of all, and leads in prompt consistency among SD 1.x-based methods. It is also computationally efficient, _e.g_. taking less time than existing tuning-based methods, and outperforming the training-based IP-Adapter (Ye et al., 2023) in a near inference time of 9 seconds vs. 2 seconds. (2) By simply replacing the base diffusion model with SD 2.1 at its default image size, our prompt consistency further surpasses SDXL (Podell et al., 2024)-based models. Note, however,

   Method & Base model & Training & Identity \(\) & Prompt \(\) & Time \(\) \\  Textual Inversion (Gal et al., 2023) & SD 2.1 & - & 0.2115 & 0.2498 & 6331 \\ DreamBooth (Ruiz et al., 2023a) & SD 2.1 & - & 0.2053 & 0.3015 & 623 \\ NeTI (Alaluf et al., 2023) & SD 1.4 & - & 0.3789 & 0.2325 & 1527 \\ Celeb Basis (Yuan et al., 2023) & SD 1.4 & - & 0.2070 & 0.2683 & 140 \\ Cross Initialtion (Pang et al., 2024) & SD 2.1 & - & 0.2517 & 0.2859 & 346 \\ IP-Adapter (Ye et al., 2023) & SD 1.5 & 10M & 0.4778 & 0.2627 & **2** \\ PhotoMaker (Li et al., 2024) & SDXL & 112K & 0.2271 & 0.3079 & 4 \\ InstantID (Wang et al., 2024b) & SDXL & 60M & 0.5806 & 0.3071 & 6 \\  RectifID (20 iterations) & SD 1.5 & - & 0.4860 & 0.2995 & 9 \\ RectifID (100 iterations) & SD 1.5 & - & **0.5930** & 0.2933 & **46** \\ RectifID (20 iterations) & SD 2.1 & - & 0.5034 & **0.3151** & 20 \\   

Table 1: Quantitative comparison for face-centric personalization. The inference time is measured in seconds on an NVIDIA A800. Unlike the previous state-of-the-art methods that require training on large face datasets (the number of images is listed for reference), our method achieves superior performance in a training-free manner, by exploiting the guidance from an off-the-shelf discriminator.

Figure 3: Qualitative comparison for face-centric personalization. See Figs. 9 to 12 for more samples.

that the rest of this paper still uses SD 1.5 for a fair comparison to SD 1.x-based baselines in various personalization tasks, excluding potential improvements from using better base models. (3) In general, our method takes a big step towards bridging the substantial performance gap with training-based personalization methods by exploring the effectiveness of training-free classifier guidance.

For face-centric qualitative comparison in Fig. 3, our method remains advantageous as its generated images by the guidance of the face discriminator exhibit high identity consistency. In comparison, InstantID (Wang et al., 2024b) delivers a near level of consistency by controlling face landmarks, but sometimes distorts the face shape (the first and third images) and contains much less natural variation. More generated samples are provided in Figs. 11 and 12 in the appendix.

**Subject-driven generation.** Our approach is flexibly extended beyond human faces towards more subjects, including certain common animals and regularly shaped objects. To validate our flexibility, Fig. 4 qualitatively compares it on three cats or dogs and a regularly shaped can, where the images generated by our method achieve highly competitive identity and prompt consistency. In comparison, the state-of-the-art method Emu2 (Sun et al., 2024), as a generalist multimodal large language model, yields high identity similarity largely by reconstructing the input image, which limits its usefulness. The tuning-based Textual Inversion (Gal et al., 2023) and DreamBooth (Ruiz et al., 2023) only work well with multiple images and exhibit inferior prompt consistency due to finetuned model parameters or prompt embeddings. See Fig. 13 in the appendix for additional results from more subjects.

Figure 4: Qualitative comparison for subject-driven generation. \({}^{*}\) denotes finetuned with multiple images of the target subject to achieve sufficient identity consistency. See Fig. 13 for more samples.

Figure 5: Qualitative comparison for multi-subject personalization. See Fig. 14 for more samples.

**Multi-subject personalization.** Our method can be further extended to multi-subject scenarios via a bipartite matching step. Figure 5 compares it to the domain experts FastComposer (Xiao et al., 2023) and Cones 2 (Liu et al., 2023c) on composing multiple faces, live subjects and objects. As can be seen, our method achieves overall advantageous identity consistency, in spite of differences in non-persistent attributes such as hairstyle. Image semantics and quality are also well preserved, as exemplified by the amusement park details in the first image, with some others even surpassing the SD 2.1-based specialized model Cones 2. More generated samples can be found in Fig. 14 in the appendix.

### Ablation Study

To justify the effectiveness of our proposed classifier guidance, Fig. 6 and Table 2 compare it with two variants: the previously derived guidance without anchor, namely using Eq. (8), and a gradient descent method on the initial noise similar to DOODL (Wallace et al., 2023) and D-Flow (Ben-Hamu et al., 2024). The figure depicts that the gradient descent is unstable (left) and converges relatively slowly (right) despite using momentum and \(_{2}\) regularization. And its identity preservation is sensitive to the learning rate. Though our new fixed-point formulation allows for a more stable layout, the initial version fails to converge as the face feature keeps drifting. In contrast, our full method exhibits better stability (left) and faster convergence (right) by implicitly regularizing the flow trajectory to be close and straight. This is further supported by the quantitative comparison, where our method delivers better identity and prompt consistency than the alternatives. Further analysis for hyperparameter sensitivity is provided in Appendix D.3.

### Generalization

To validate the generalizability of our approach to broader application scenarios, we have extended it to more controllable generation tasks by directly using the guidance functions from Universal Guidance (Bansal et al., 2024). The experimental results under the guidance of segmentation map or style image are illustrated in Fig. 7. As shown, our classifier guidance can perform both tasks without any additional tuning, faithfully following the various forms of control signals provided by the user. This confirms the adaptability of our approach for various controllable generation tasks. Additional generalization analysis of our method for broader diffusion models is presented in Appendix D.1.

   Method & Identity \(\) & Prompt \(\) \\  Gradient descent of noise & 0.5249 & 0.2842 \\ RectifID w/o anchor & 0.1158 & 0.2916 \\  RectifID & **0.5930** & **0.2933** \\   

Table 2: Quantitative comparison with alternative designs. The number of iterations is 100, and the remaining settings for gradient descent follow Fig. 6.

Figure 6: Comparison with alternative designs at varying guidance scale (or learning rate) and iterations. The prompts are “cave mural depicting a person” and “a person as a priest in blue robes”. The base learning rate for gradient descent is 0.4, with momentum of 0.9 and an \(_{2}\) regularizer of 1.0.

## 5 Conclusion

This work presents a training-free personalized image generation method using anchored classifier guidance. It extends the applicability of the original classifier guidance based on two key findings: first, by developing on a rectified flow framework assuming ideal straightness, the classifier guidance can be transformed into a new fixed-point formulation involving only clean image-based discriminators; secondly, anchoring the flow trajectory to a reference trajectory greatly improves its solving stability. The derived anchored classifier guidance allows flexible reuse of existing image discriminators to improve identity consistency, as validated by extensive experiments on various personalized image generation tasks for human faces, live subjects, certain objects, and multiple subjects.

**Acknowledgement:** This research work is supported by National Key R&D Program of China (No. 2022ZD0160305), a research grant from China Tower Corporation Limited, an internal grant (No. 2024JK28) and a grant from Beijing Aerospace Automatic Control Institute.