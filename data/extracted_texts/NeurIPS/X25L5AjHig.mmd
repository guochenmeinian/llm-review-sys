# Nearly Optimal Bounds for Cyclic Forgetting

Halyun Jeong

University of California Los Angeles

hajeong@math.ucla.edu

&Mark Kong

University of California Los Angeles

markkong@ucla.edu

Deanna Needell

University of California Los Angeles

deanna@math.ucla.edu

&William Swartworth

Carnegie Mellon University

wswartwo@andrew.cmu.edu

&Rachel Ward

University of Texas at Austin

rward@math.utexas.edu

###### Abstract

We provide theoretical bounds on the forgetting quantity in the continual learning setting for linear tasks, where each round of learning corresponds to projecting onto a linear subspace. For a cyclic task ordering on \(T\) tasks repeated \(m\) times each, we prove the best known upper bound of \(O(T^{2}/m)\) on the forgetting. Notably, our bound holds uniformly over all choices of tasks and is independent of the ambient dimension. Our main technical contribution is a characterization of the union of all numerical ranges of products of \(T\) (real or complex) projections as a sinusoidal spiral, which may be of independent interest.

## 1 Introduction

While machine learning often focuses on learning from a static dataset, one is often interested in acquiring data on the fly and adapting to the present context. This is known throughout the literature as "continual learning" or "lifelong learning" .

One challenge of continual learning is "catastrophic forgetting" : A model may learn useful information in context \(A\) but then lose much of this knowledge when adapting to context \(B\). If context \(A\) is not representative of future contexts, then this is a correct adjustment for distribution shift. However, if contexts similar to \(A\) arise repeatedly, this may be undesirable..

Building on prior work, we aim to better understand the catastrophic forgetting phenomenon when faced with a series of linear regression problems that are revisited in cyclic order.

In machine learning, many data sets display cyclic or periodic patterns. These patterns often arise from factors such as the "day of the week effect"  observed in financial or search engine data . Additionally, the methods of cyclic alternating projections by Von Neumann  and Halperin  are well-studied methods for solving linear systems . Equivalently, one can think of this as studying residual bounds for cyclic block Kaczmarz-type algorithms. Our work can be thought of as studying the worst-case forgetting of these popular methods.  mentions this perspective, but leaves open the problem of obtaining tight convergence bounds. In particular, the dimension dependence on the ambient data in the bounds of  could be problematic since high-dimensional data is so ubiquitous in machine learning. In contrast, our new bound for the worst-case forgetting does not scale at all with the dimension of the ambient data.

### Problem Setup

The learning algorithm we consider fits a linear map to a set of data points of the form \((,y)\) with \(^{d},y\).

Our data is split into "tasks", which are lists of elements from our dataset (with repeats allowed). These tasks are fed to our learning algorithm one at a time. Our algorithm updates its prediction after being given each task.

We assemble the \(r_{t}\) inputs of the \(t\)th task into a matrix \(X_{t}^{r_{t},d}\), where each row is an input vector \(^{T}\), and their corresponding outputs into a column vector \(_{t}^{r_{t}}\). We aim to to find a column vector \(^{d}\) such that \(X_{t}=_{t}\) for all \(t\).

The learning algorithm we consider applies a block Kaczmarz update at each step. Explicitly, we initialize \(_{0}^{d}\) anywhere, and for \(t_{ 1}\) we set \(_{t}\) to be the projection of \(_{t-1}\) onto the solution space \(P_{t}\) for the \(t\)th regression problem. That is,

\[_{t+1}=_{t}+X_{t+1}^{+}(_{t+1}-X_{t+1}_{t})\]

where \(+\) denotes the Moore-Penrose pseudoinverse.

Geometrically, the set of vectors \(^{d}\) such that the map \(^{d}^{T}\) agrees with all the data in task \(t\) is an affine subspace \(P_{t}\). This algorithm obtains \(w_{t+1}\) by projecting \(w_{t}\) onto \(P_{t+1}\). The set of vectors in \(^{d}\) satisfying all tasks is \(P:=_{t}P_{t}\). The algorithm never leaves the maximal affine hyperplane through \(_{0}\) perpendicular to \(P\), so if the \(w_{t}\) converge to a solution then it converges to the solution closest to \(w_{0}\)..

It is well-known that running gradient descent to interpolation with quadratic loss on task \(t\) starting from \(w_{t}\) produces \(w_{t+1}\).

Typically, projecting onto the solution space for task \(t\) moves \(_{i-1}\) off the solution space for task \(t-1\). We wish to understand the amount of information "forgotten" over the course of training. Following , we define the **forgetting** for a sequence of tasks and initialization \(S\) at iteration \(n\) as

\[F_{S}(n):=_{t=1}^{n}\|X_{t}_{n}-_{t}\| ^{2}.\]

That is, \(F_{S}(n)\) is the average loss of \(w_{n}\) on all previous tasks (including task \(n\)), where each task is weighted equally regardless of how many data points it has. As mentioned in the introduction, large forgetting is not always undesirable, but "forgetting" in humans is not always undesirable either, so the name "forgetting" not imply an inappropriately negative connotation.

In this work, we focus on cyclic task orderings. That is, we have \(T\) tasks \((X_{1},_{1}),,(X_{T},_{T})\) we visit cyclically, i.e., \(X_{t}=X_{t T+1}\) and \(_{t}=_{t T+1}\) for all \(t\). Then the sequence of iterates converges to a cycle of length \(T\) in the sense that, for any \(t_{ 0}\), \(w_{t},w_{t+T},w_{t+2T},\) converges, either linearly or after a finite number of iterations; We justify this in the supplementary material.

In particular, if there is at least one simultaneous solution to all tasks, then the limit must be a solution; We justify a generalization of this in the supplmentary material, so we get linear convergence to a solution which implies linear convergence of forgetting.

However, this does not guarantee that, for any collection of \(T\) tasks in \(m\) cycles, the forgetting can be bounded above by \(O(c^{-m})\) for some constant \(c\), even after normalizing the tasks in the way we describe later: The choice of tasks that give the largest forgetting for one value of \(m\) may not be the tasks that give the largest forgetting for another value of \(m\). Here we ask whether we can obtain a worst-case bound on \(F_{S}(mT)\) that only depends on the number of tasks \(T\) and the number of cycles \(m\).

We focus on bounding the forgetting after a whole number of cycles; The forgetting in the middle of a cycle is a weighted average of the losses at each task, and therefore can be treated in the same way as getting a bound without normalizing the tasks. For a quick bound, the weights are close to 1, so we may rescale the tasks to get all coefficients to be 1 or less without increasing our bound much. The forgetting for \(T\) tasks after \(m\) cycles is

\[_{t=1}^{mT}\|X_{t}_{mT}-_{t}\|^{2}= _{t=1}^{T}\|X_{t}_{mT}-_{t}\|^{2}.\] (1)as each of the \(T\) tasks is visited equally often.

This can be made arbitrarily large by scaling the data points, so we describe normalizations in the next section. Given these normalizations,  showed that such a worst-case bound is possible, but the bounds they provided are not tight. Here, we improve on the forgetting bounds from  and, as a byproduct, show the union of the numerical ranges of all products of \(T\) orthogonal (real or complex) projections forms a sinusoidal spiral.

#### 1.1.1 Normalizations

Following , we restrict to the setting where \(P\) is nonempty (i.e., the union of all tasks is consistent with a single linear function) and let \(_{*} P\). So every data point \((,y)\) satisfies \(^{T}_{*}=y\), and the planes \(P_{i}\) we project onto all pass through \(_{*}\).

Unlike , we translate the setup so \(w_{*}=0\). This modifies each data point \((,y)\) by setting \(y=0\) but preserving \(\). This translates the \(P_{i}\) to pass through 0, so they are subspaces, and \(P_{i}\) is the kernel of \(X_{i}\), and the loss of task \(t\) at \(\) is now \(\|X_{t}\|^{2}\), turning forgetting into.

\[F_{S}(n)=_{t=1}^{n} X_{t}^{2}= (}}\,X_{t} )()^{2}\]

where the product is the categorical product. Thus, for a fixed collection of tasks \(X_{1},X_{2},,X_{n}\), abusing notation to let \(P_{t}\) denote the projection onto the plane \(P_{t}\), the largest forgetting over all initializations \(_{0}\) is \(}}\,X_{t })(}}\,P_{t} )^{2}}{n}\).

Rescaling \(_{0}\) rescales all iterates, and hence the forgetting, by the same amount. So we normalize \(_{0}=1\). Similarly, rescaling all tasks \(X_{t}\) by the same scalar scales each term in the forgetting by that factor, so we normalize so that \(_{t} X_{t}=1\).

As \(_{t}\) decreases in \(t\), this implies the loss of \(_{n}\) at task \(t\) is \( X_{t}_{n}^{2} 1\), and hence the forgetting is at most 1.

In the cyclic setting, for fixed \(T\) and \(m\), these normalizations make the domain of optimization (the space of \(T\)-tuples of tasks with \(_{t} X_{t} 1\) and initializations \(_{0} S^{d-1}\)) compact, so forgetting attains a maximum, which is what we bound in this paper.

In the supplementary, we describe a tighter normalization that gives the same forgetting bounds.

### Observations

In this section, we describe some observations that help us get our bound.

As noted in the previous section, using our normalization, there is a trivial bound on the forgetting of 1.

The data matrices \(X_{t}\) only affect the iterates \(_{i}\) via their rowspan (i.e., the span of the data points in the task), so we may replace \(X_{i}\) by any other data matrix with the same rowspan, or equivalently the same kernel, without affecting the iterates subject to our normalization constraint. As \( X_{i} 1\), it is a contraction, so the norm of the image of any \(\) is at most its distance to the kernel, and this is realized when \(X_{i}\) is a projection away from its kernel. Therefore, the worst-case bound is not affected by restricting \(X_{i}\) to be orthogonal projections with kernel \(P_{i}\).

With this restriction, for any \(\), \( X_{i}\) is the distance from \(\) to \(P_{i}\). Therefore, the forgetting can be interpreted as the average distance to each \(P_{i}\) over all previous tasks (weighted equally).

### Prior Work

Bounds on forgetting. considered forgetting bounds both for random task orderings and cyclic task orderings. However their bound for cyclic task orderings is not tight. After \(m\) cycles of tasks, they show an upper bound of \(\{}{},d}{2mT}\}\) on the forgetting, leaving a gap with their lower bound of \((}{mT})\)

A key step of their proof is bounding \(\|A^{m}\|^{2}-\|A^{m+1}\|^{2}\) where \(A\) is a product of \(T\) projections and \(u\) is a unit vector. Their dimension-independent bound gives a \(T^{2}/\) dependence on the forgetting after \(m\) cycles. Their other bound improves the \(m\) dependence, but only uses that \(A\) is a contraction. This forces a linear dependence on the dimension \(d\), as this is possible if the supremum is taken over all contractions. Indeed, for any \(m\) a multiple of \(d\), one can have

\[\|A^{m}\|^{2}-\|A^{m+1}\|^{2}(d /m)\]

by choosing \(A\) to be an operator that maps

\[_{1}_{2}_{d}(1-)_{1},\]

and taking \(=_{1}\), where \(_{i}\) denotes the \(i\)th standard unit vector.

 bound \(\|A^{m}-A^{m+1}\|^{2}\) for \(A\) a product of projections. However they are primarily interested in the asymptotics with respect to \(m\). They supply two proofs which rely on either the bound in  or in , and hence do not avoid the exponential dependence in \(T\).

 study forgetting from a different perspective. They assume Gaussian i.i.d. tasks ( called features in their paper) at each iteration and explicitly compute the expected forgetting and generalization error. We analyze the worst-case forgetting without any assumptions on the distribution of tasks, so our work is quite different from .

As mentioned in , the iterate \(w_{t}\) can be equivalently obtained by running the (block) Kaczmarz method  with the cyclic row selection rule for solving a consistent system \(y_{t}=X_{t}w, t[T]\). However, most id=MK]of the convergence analyses of Kaczmarz-type methods  bound the distance from the least-norm solution, whereas in our setting we interested in the residual errors. Moreover such analyses depend on the condition number of the system, so are unlikely to adapt to our setting, since our bounds hold for all choices of tasks (i.e. blocks), with no condition number dependence.

Bounds on the numerical range.Lemma 5.1 in  gives a bound on the numerical range of any linear transformation \(A\) expressible as a product of \(T\) projections. This, combined with other results in that same work, yields a bound on \(\|A^{m}-A^{m+1}\|^{2}.\) When combined with the results of , the result is a bound of the form \(O(f(T)/m)\) on the forgetting. However, the function \(f(T)\) turns out to be exponential when applying the results of  directly. This is due to Lemma 5.1 of  being loose. There they show that the numerical range of \(A\) is contained in a region of \(\) with a boundary point at one where the tangent lines at \(1\) have exponential slope. In this paper, we give the smallest region that is guaranteed to contain the numerical range of \(A\) given only \(T\).

 also gives a bound on the numerical range of a product of projections. Specifically they show that the numerical range is contained in a Stoltz domain with the angle at \(1\) depending on a quantity called the Friedrich's number of the (subspaces associated to the) projections. They then combine their bound with that of  to obtain somewhat tighter control over the numerical range. However the Friedrich's number for \(T\) subspaces can be made arbitrarily close to \(1\) so for our purposes their bound will not yield better results than the bounds in .

### Results

We give a nearly optimal bound on the forgetting for cyclic task orderings, removing the dimension dependence of  and avoiding the exponential dependence on \(T\) that would arise from applying results in .

**Theorem 1**.: \[_{S}F_{S}(mT) O(}{m}).\]

_where \(S\) ranges over all (task, initialization) pairs where the tasks are given by matrices \(X_{1},X_{2},,X_{T}\) with \(_{t}\|X_{t}\| 1\) and \(_{0}^{d}\) with \(|_{0}|=1\)._To prove this,  show that, if \(A\) denotes the linear map given by one cycle, letting \(_{m}():=\|A^{m}\|-\|A^{m+1}\|\),

\[_{S}F_{S}(mT)_{:\|u\|=1}_{m}( )\|A^{m}(1-A)\|.\]

We provide a geometric interpretation of their proof in the supplementary material.  showed that there exists a constant \(Q 1+\) such that

\[\|p(A)\| Q_{z W(A)}|p(z)|\]

for all polynomials \(p\). Our approach is to determine \(_{A}W(A)\) where \(A\) ranges over all products of \(k\) complex orthogonal projections. As this includes the class of products of \(k\) real orthogonal projections, combining these inequalities gives our bound.

We will show in Section 3 that the problem of bounding the numerical range of the product of \(k-1\) complex orthogonal projections is equivalent to understanding the range of the product

\[P(_{0},,_{k-1}):=_{0},_{1} _{1},_{2} _{k-2},_{k-1}_{k-1},_{0} ,\] (2)

where the \(_{i}\) range over complex vectors in \(^{d}\) of norm \(1\). We give a complete description of the range of \(P\) as the _filled sinusoidal spiral_

\[re^{i}\ :\ (/k)r^{-1/k}().\]

This shape has a simple geometric description. Let \(_{k}\) be the convex hull of the \(k\)th roots of unity. Then the sinusoidal spiral above is the image of \(_{k}\) under the map \(z z^{k}\).

The main challenge in computing \(P\)'s range \(_{k}\) is in computing its boundary since a topological argument (given in the supplementary material) shows that \(_{k}\) is simply connected. To compute the boundary, we first observe that it suffices to consider input vectors in \(^{2}\). To see this, \(P\) maps a sequence of vectors to the cyclic product of pairwise inner products, so if any of the input vectors is not coplanar with its neighbors, then projecting it onto the plane spanned by its neighbors and scaling it to have unit norm increases the magnitude of the corresponding factors. So it suffices to consider the case when all vectors are coplanar, which is equivalent to considering input vectors in \(^{2}\). In this case, \(P\) is a smooth map from the (real) manifold \((S^{1})^{n}^{n}\) to the (real) manifold \(\), so any input that gets sent to a boundary point of \(_{k}\) has singular Jacobian. That is, the directional derivative in all directions tangent to the domain must be parallel. It turns out that these algebraic conditions can be manipulated to characterize all critical points and critical values of \(P\), and the computation is made simpler by using quaternions.

As a side effect of our proof, we also fully describe the tuples \(_{0},,_{k-1})\) that map to boundary points of the image of \(P\) and show that they naturally correspond (up to multiplying each \(_{t}\) by a complex unit \(_{t}\)) to certain quaternionic roots of unity.

Using this result we obtain the following bound on the increments between consecutive powers of \(A\) when \(A\) is a product of projections. We believe this result may be independently useful.

**Lemma 2**.: _For any linear map \(A^{n}^{n}\) expressible as a product of \(k\) complex projections,_

\[\|(I-A)A^{m}\|(C+o_{k,m}(1)),\]

_where \(C\) is an absolute constant which can be taken to be \(0.4\)._

This lemma, with the inequality of , immediately implies Theorem 1.

In particular, for fixed \(T\), we obtain an asymptotically optimal bound in terms of iteration count \(m\), _independent of dimension_. In contrast, the dimension-independent bound given in  was sub-optimal by a factor of \(\).

Finally,. So our results can be interpreted as giving a bound for this setting as well.

### Notation for proof

We use \(\) and \(\) to denote the complex numbers and the quaternions. While the previous sections considered real vector spaces, our proof works by obtaining a bound over all complex projections. So for sections 2 and 3, all vector spaces will be over \(\). In these sections, we use the following guidelines for variable names:

Our complex inner products are always conjugate linear in the first argument, so \(,=^{*}\) where \(v^{*}\) denotes the conjugate transpose of \(\). The argument of a complex number is taken to be in \((-,]\). The numerical range of a matrix \(A^{d d}\) is defined by

\[W(A)=\{^{*}A|^{d},\|\|=1\}.\]

## 2 Bounding the range of \(P\)

In this section, we fully describe the range of \(P\) given by (2) and obtain a full description of the inputs that map to boundary points. We will use the following notation throughout the next argument.

**Definition 3**.: _For \((a,b)^{2}\), we define \((a,b)^{}:=(,-).\)_

The following identity is a straightforward consequence of the definition.

**Proposition 4**.: _For \(,^{2}\), \(,^{}=-^{},}.\)_

Our first theorem shows that the range of \(P\) is a filled sinusoidal spiral. This result is of mathematical interest on its own, and will also be applied to cyclic forgetting bounds in the next section.

We begin with a lemma characterizing the critical points of \(P\) treated as a smooth map \(^{2k}^{2}\). We defer the proof to the supplementary material.

**Lemma 5**.: _For any critical value \(c\) of \(P\), there exists a sequence of vectors \((a_{0},b_{0})=(1,0),(a_{1},b_{1}),,(a_{k-1},b_{k-1})\) such that \(P((a_{m},b_{m})_{m=0}^{k-1})=c\). Furthermore, there exist \(,\) such that_

1. \((a_{m},b_{m}),(a_{m+1},b_{m+1})=\) _for all_ \(m\)__
2. \((a_{m},b_{m})^{},(a_{m+1},b_{m+1})=^{m-1}\) _for all_ \(m\)__

_(where indices are treated mod \(k\)). Furthermore, any sequence \(((a_{m},b_{m})_{m=0}^{k-1})\) attaining \(P(((a_{m},b_{m})_{m=0}^{k-1}))=c\) can be obtained by taking a sequence satisfying the above conditions and multiplying each vector by a complex unit._Given this characterization of \(P\)'s critical points, our next result computes the boundary points of \(P\)'s image, from which we obtain the entire image of \(P\) as a consequence.

**Theorem 6**.: _Let \(P(_{0},,_{k-1})=_{0},_{1}\, _{1},_{2}_{k-2},_{k-1} \,_{k-1},_{0}\) and let \(_{k}\) be the range of \(P\) as each \(_{i}\) ranges over unit vectors in \(^{d}.\) Then_

\[_{k}=\{re^{i}|()r^{-1/k} (),r 0,[0,2]\},\]

_which is a filled sinusoidal spiral._

Proof.: We show this result for vectors in \(^{2}\). In the supplementary material, we show that increasing the dimension does not affect the range, so considering vectors in \(^{2}\) is sufficient. The result is trivial for \(k 2\), so we will assume \(k 3\).

The domain of \(P\) is compact, so \(_{k}\) is also compact. \(P\) is also smooth (as a function \(^{4}^{2}\)), so its boundary consists of critical values. We will give a complete description of the critical values and critical points and show that the outermost critical values form a sinusoidal spiral, which is enough to show that \(_{k}\) is contained in the claimed region. We defer showing that \(_{k}\) contains the entire filled spiral to the supplementary material.

Fix a critical value \(c\) of \(P\) and let \(a_{i},b_{i}\) be as in Lemma 5 above. As \((a_{m},b_{m})\) and \((a_{m},b_{m})^{}\) are orthogonal, they form a basis for \(^{2}\), so letting \(=a_{0},=b_{0}\), we get a recurrence relation

\[(a_{1},b_{1})=(1,0)+(1,0)^{}\]

\[(a_{2},b_{2})=(a_{1},b_{1})+(a_{1},b_{1})^{}\]

\[(a_{3},b_{3})=(a_{2},b_{2})+^{2}(a_{2},b_{2})^{}\]

\[\]

\[(a_{k-1},b_{k-1})=(a_{k-2},b_{k-2})+^{k-1}(a_{k-2},b_{k-2})^ {}\]

\[(1,0)=(a_{0},b_{0})=(a_{k-1},b_{k-1})+^{k}(a_{k-1},b_{k-1})^ {}.\]

Representing each vector \((a,b)^{2}\) as a quaternion via \((a,b) a+bj\), it follows from direct computation that \((a,b)^{} j(a+bj)\) and, for \(\), \((a,b)(a+bj)=(a+bj)\). In particular, multiplication on the left by a complex number is unambiguous.

Our recurrence relation can be expressed as

\[a_{1}+b_{1}j=(+ j)(1+0j)\]

\[a_{2}+b_{2}j=(+ j)(a_{1}+b_{1}j)\]

\[a_{3}+b_{3}j=(+^{2}j)(a_{2}+b_{2}j)\]

\[\]

\[a_{k-1}+b_{k-1}j=(+^{k-2}j)(a_{k-2}+b_{k-2}j)\]

\[1+0j=(+^{k})(a_{k-1}+b_{k-1}j).\]

In other words, the vectors must be the partial products

\[(a_{m},b_{m})=(+^{m-1}j)(+^{m-1}j)( +^{0}j)\]

for \(m=0,,k-1\) (which is the empty product of \(m=0\)), where \(,\) satisfy the equation

\[1=(+^{k-1}j)(+^{k-2}j)(+ ^{0}j).\]

We now show that any sequence of vectors \((a_{0},b_{0}),,(a_{k-1},b_{k-1})\) in a solution to the above system of equations can be obtained by slightly modifying a solution in the case \(=1\) (possibly with a different value of \(k\)).

Let \(_{k}\) be a (not necessarily primitive) complex \(k\)th root of unity. Consider the sequence

\[(a_{0},b_{0}),(a_{1}_{k},b_{1}_{k}),(a_{2}_{k}^{2},b_{2}_{ k}^{2}),,(a_{k-1}_{k}^{k-1},b_{k-1}_{k}^{k-1}).\]This also satisfies the two conditions in Lemma 5, so we can run the same logic as above. The corresponding change in the resulting system is that this multiplies \(,\) by \(_{k}\) and \(\) by \(_{k}^{2}\), so by multiplying each vector by these complex units we may freely modify \(\) by \(_{k}^{2}\) for any \(k\)th root of unity \(_{k}\), or equivalently any power of \(e^{2 i/k}\).

If \(k\) is odd, this is enough to show that any nonzero critical value can be obtained by \(=1\), and furthermore that any critical point can be expressed as \((a_{0},b_{0}),(a_{1}_{k},b_{1}_{k}),(a_{2}_{k}^{2},b_{2}_{ k}^{2}),,(a_{k-1}_{k}^{k-1},b_{k-1}_{k}^{k-1})\) for some \(k\)th root of unity \(_{k}\).

If \(k\) is even, let \(_{2k}\) be a \(2k\)th root of unity. Consider the sequence

\[(a_{0},b_{0}),(a_{1}_{2k},b_{1}_{2k}),(a_{2}_{2k}^ {2},b_{2}_{2k}^{2}),,(a_{k-1}_{2k}^{k-1},b_{k-1}_{2k}^{k- 1}),\\ (a_{0}_{2k}^{k},b_{0}_{2k}^{k}),(a_{1}_{2k}^{k+1}, b_{1}_{2k}^{k+1}),,(a_{k-1}_{2k}^{2k-1},b_{k-1}_{2k}^{2k- 1}).\]

This satisfies the conditions in Lemma 5 for \(2k\) in place of \(k\). The corresponding values of \(,\) in the resulting system are the same as in the original multiplied by \(_{2k}\), and the resulting value of \(\) is the same as the original multiplied by \(_{2k}^{2}\). As \(_{2k}^{2}\) can be chosen to be \(^{-1}\), we can reduce to the case

\[1=(_{2k}(+^{2k-1}j))(_{2k}(+^{2k-2 }j))(_{2k}(+^{0}j))\]

where \(=1\), or equivalently

\[1=(_{2k}(+ j))^{2k}.\]

**Remark 7**.: _These reductions can also be interpreted using quaternions, which we demonstrate in the supplementary material._

For any integer \(m\), the \(m\)th roots of unity in \(\) are precisely those numbers whose real parts are the same as those of the real parts of the \(m\)th roots of unity in \(\). So \((+ j)^{m}=1\) if and only if \(\) lies on one of the vertical diagonals of the \(m\)-gon whose vertices are the \(m\)th roots of unity (where the vertical diagonals are those with constant real part, including sides of the \(n\)-gon).

Therefore, the values of \(\) in the phase-shifted sequences are vertical diagonals in the regular \(k\)-gon if \(k\) is odd and every other vertical diagonal in the regular \(2k\)-gon if \(k\) is even, where the vertices are the corresponding roots of unity. Inverting the phase shift gives that, regardless of whether \(k\) is even or odd, the values of \(\) in the solutions to

\[1=(+^{k-1}j)(+^{k-2}j)(+ ^{0}j)\]

form the diagonals (including the sides) of the \(k\)-gon whose vertices are the \(k\)th roots of unity.

As the inner product of consecutive vectors at such a sequence is \(\), the value of \(P\) at such a sequence is \(^{k}\). Therefore the critical values of \(f\) are then \(k\)th powers of these diagonals. Furthermore, when \(k\) is odd the critical points are sequences of the form \((a_{m},b_{m})=_{m}(+ j)^{m}\) where \(+ j\) is a quaternionic \(k\)th root of unity and \(_{m}\) are units in \(\); When \(k\) is even, the critical points are sequences of the form \((a_{m},b_{m})=_{m}(+ j)^{m}\) where \(+ j\) are quaternionic \(2k\)th roots of unity and \(_{m}\) are units in \(\),

By the maximum principle for holomorphic maps, the outer boundary of \(_{k}\) is obtained by taking the \(k\)th powers of the sides (where each side maps to the same curve). Using polar coordinates, the resulting curve is the sinusoidal spiral \(()r^{-1/k}=()\). 

The same proof shows that all other critical points other than those that \(P\) sends to 0 arise from the other diagonals of the polygons and their corresponding critical values are their \(k\)th powers, which are also sinusoidal spirals. A computation shows that a sequence of vectors that \(P\) sends to 0 is a critical point if and only if at least two of the inner products are 0.

## 3 Application to cyclic forgetting bounds

As an immediate application of Theorem 6, we can control the numerical range of a product of \(k\) orthogonal projections.

**Corollary 8**.: _Let \(A=P_{k}P_{k-1} P_{1}\) be a product of \(k\) orthogonal complex projections \(^{d}^{d}\). Then \(W(A)_{k+1}\)._Proof.: We first observe a general fact. Let \(P^{d}^{d}\) be an orthogonal projection and let \(^{d}\). Let \(=}{\|P^{2}\|}\). Then

\[P=,=^{*}.\]

Now let \(\) be an arbitrary unit vector and let

\[_{i}=P_{i-1} P_{1}}{\|P_{i}P_{i-1} P_{1} \|}\,.\]

Iterating the fact stated above,

\[(_{i}_{i})(_{1}_{1}^{*})=P_{i} P _{1},\]

for all \(i[k]\). Hence

\[^{*}A=^{*}(_{k}_{k})(_{k-1}_ {k-1}^{*})(_{1}_{1}^{*})=,_{k }\,_{k},_{k-1}_{2}, _{1}\,_{1},\,,\]

so \(W(A)_{k+1}\). 

In the supplementary material, we use our characterization of \(P^{-1}(_{k+1})\) to characterize the pairs \((A,)\) such that \(^{*}A_{k+1}\).

We now bound \(\|A^{m}(I-A)\|\) uniformly over all linear maps \(A^{n}^{n}\) that can be expressed as a product of \(k\) complex projections by applying the above bound on the numerical range into the result of  that there exists a constant \(Q 1+\) such that

\[\|p(A)\| Q_{z W(A)}|p(z)|\]

for all polynomials \(p\). We defer the calculation to the supplementary material.

**Lemma 9**.: \[_{z_{k}}|z^{m}(1-z)|(}+o_{k, m}(1)).\]

A more precise version of this bound in which \(}\) can be improved in terms of \(\) will be given in the supplementary material.

This, combined with Crouzeix's inequality, gives Lemma 2, which we restate below more precisely, and hence Theorem 1.

**Lemma 2**.: _For any linear map \(A^{n}^{n}\) expressible as a product of \(k\) complex projections,_

\[\|A^{m}(I-A)\|(}+o_{k,m}(1) )Q\]

_where \(Q\) is the constant of Crouzeix's inequality as presented above._

## 4 Real projections

In the application to forgetting, all relevant projections are real projections. Therefore one may ask whether our bound can be improved by restricting to products of \(k\) real projections.

As we show in the supplementary material, the union of the numerical ranges is the same even if we restrict to real orthogonal projections \(^{4}^{4}\), and it is possible to get the maximal asymptotic decay rate on \(\|A^{m}-A^{m+1}\|\), but any real or complex sequence of orthogonal projections attaining a point on \(_{k}\) in its numerical range can be orthogonally decomposed into invariant (under all projections) subspaces \(U V\) where \(\|A^{m}\|-\|A^{m+1}\|\) decays quickly on \(U\). In particular, any indecomposable such sequence must have quickly decaying forgetting.

## 5 Future Work

It is not clear that this yields a completely optimal bound for forgetting. Is it possible to improve \(O(T^{2}/m)\) to \(O(T/m)\) matching the lower bound of ?

Finally one might wonder how rare the worst-case bound is. For example if one visits the \(T\) tasks cyclically, but in a random order is it possible to beat the \(O(T^{2}/m)\) bound, perhaps with some weak dependence on the ambient dimension?Acknowledgment

The authors would like to thank Itay Evron for interesting discussions about [Evr+22].