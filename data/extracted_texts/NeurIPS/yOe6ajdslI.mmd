# AUC Maximization under Positive Distribution Shift

Atsutoshi Kumagai

NTT

atsutoshi.kumagai@ntt.com

&Tomoharu Iwata

NTT

tomoharu.iwata@ntt.com

&Hiroshi Takahashi

NTT

hiroshibm.taakahashi@ntt.com

&Taishi Nishiyama

NTT Security Holdings, NTT

taishi.nishiyama@security.ntt

&Yasuhiro Fujiwara

NTT

yasuhiro.fujiwara@ntt.com

###### Abstract

Maximizing the area under the receiver operating characteristic curve (AUC) is a common approach to imbalanced binary classification problems. Existing AUC maximization methods usually assume that training and test distributions are identical. However, this assumption is often violated in practice due to _a positive distribution shift_, where the negative-conditional density does not change but the positive-conditional density can vary. This shift often occurs in imbalanced classification since positive data are often more diverse or time-varying than negative data. To deal with this shift, we theoretically show that the AUC on the test distribution can be expressed by using the positive and marginal training densities and the marginal test density. Based on this result, we can maximize the AUC on the test distribution by using positive and unlabeled data in the training distribution and unlabeled data in the test distribution. The proposed method requires only positive labels in the training distribution as supervision. Moreover, the derived AUC has a simple form and thus is easy to implement. The effectiveness of the proposed method is experimentally shown with six real-world datasets.

## 1 Introduction

In many real-world binary classification problems such as intrusion detection , medical diagnosis , and visual inspection , _class-imbalance_ frequently occurs where positive data is much smaller than negative data . In this case, classification accuracy, which is the standard performance measure for ordinary binary classification, is not a suitable measure . Instead, the area under the receiver operating characteristic curve (AUC) is commonly used . The AUC is the probability that a classifier will rank a randomly drawn positive instance higher than a randomly drawn negative one . Due to the nature of the ranking, the AUC can adequately measure the performance of the classifier even with imbalanced data. By maximizing the AUC, we can obtain accurate classifiers even from imbalanced data .

Existing AUC maximization methods usually assume that training and test distributions are identical to ensure the generalization performance. However, in real-world applications, this assumption is often violated due to distribution shifts. This paper considers _a positive distribution shift_, i.e., the negative-conditional density does not change but the positive-conditional density can vary, because this shift often occurs in imbalanced problems. For example, in intrusion detection, maliciousadversaries rapidly change their attacks (positive data) to bypass detection systems while the benign class's data (negative data) do not change much [9; 16; 67]. In medical diagnosis, the distribution of disease data (positive data) can change due to the disease progression or emergence of new pathogens, but the distribution of data of healthy people is generally stable. In visual inspection, the types of anomalous products (positive data) are diverse but normal ones (negative data) have some degree of stationarity. When such a distribution shift occurs, the performance of AUC maximization methods drastically deteriorates.

Although labeled data drawn from the test distribution can alleviate this problem, such data are often time-consuming and expensive to collect whenever the distribution shift occurs . In addition, labeled negative data in the training distribution are also difficult to collect in some applications. For example, in intrusion detection, although some malicious data can be collected from public sources such as blacklists, benign data are often unavailable due to privacy reasons, and identifying clean benign data from given unlabeled data requires a high level of expertise [37; 47].

In this paper, we propose a method for maximizing the AUC under the positive distribution shift by using labeled positive and unlabeled data in the training distribution and unlabeled data in the test distribution. Figure 1 illustrates examples of given data in our problem setting. Since no labeled data are available in the test distribution, a challenge is how to maximize the AUC on the test distribution. To address this challenge, we theoretically show that the AUC on the test distribution can be expressed by using the positive and marginal training densities and the marginal test density when assuming the positive distribution shift. This result enables us to maximize the AUC on the test distribution with positive and unlabeled data in the training distribution and unlabeled data in the test distribution. The derived AUC has a simple form and is easy to implement. In addition, it does not depend on the class-prior in the test distribution, which is usually difficult to obtain with unlabeled data in the test distribution. Also, it can be applied to any differentiable classifiers such as linear classifiers, kernel-based classifiers, and neural networks. Hence, our method is easy for practitioners to use.

The main contributions of our work are summarized as follows:

* We propose a novel and practical problem setting, where the aim is to maximize the AUC under the positive distribution shift on imbalanced data.
* We theoretically show that the AUC on the test distribution can be maximized with positive and unlabeled data in the training distribution and unlabeled data in the test distribution when assuming the positive distribution shift.
* We empirically demonstrate that the proposed method outperformed various existing methods with six real-world datasets.

## 2 Related Work

Many AUC maximization methods have been proposed [5; 61; 64; 32; 65]. In previous studies [65; 66; 12; 55], they have been reported to often perform better than other methods for imbalanced classification such as class balanced loss , focal loss , or sampling-based methods [8; 36]. However, these AUC maximization methods require labeled positive and negative data. In addition, they assume that the training and test distributions are the same. Therefore, they are inappropriate for our problem setting where there are no labeled negative data and the distribution shift occurs.

Figure 1: Illustration of given data in our problem setting. The triangle, circle, and cross represents negative data, positive data in the training distribution, and positive data in the test distribution, respectively. Orange and gray represents labeled and unlabeled data, respectively. Our setting assumes that the distribution of negative data does not change (triangles) but that of positive data can vary (circles and crosses).

Unsupervised domain adaptation aims to adapt to the distribution (domain) shift by using unlabeled data in the test distribution and labeled data in the training distribution [41; 56]. One representative approach is to learn domain-invariant feature representations by minimizing the discrepancy of the features in both domains [33; 52; 13; 27; 11; 48; 33]. Although this approach is promising, it often deteriorates the performance since it minimizes only the feature discrepancy without considering the relationship between features and labels . Another representative approach is to explicitly minimize the loss on the test distribution by assuming the types of the distribution change, such as covariate shift [50; 3; 23; 69; 35] and class-prior shift [68; 51; 1]. The proposed method belongs to this approach. By assuming the shift type, this approach can adapt to the shift in a theoretically guaranteed manner. The existing methods aim to minimize the classification risk (or negative classification accuracy), which is an inappropriate metric for imbalanced data (e.g., if the imbalanced data ratio is \(1:99\), a naive classifier that classifies 'every' instance as negative has \(99\%\) accuracy, but it is definitely not a good classifier). One domain adaptation method tries to maximize the AUC by learning domain-invariant feature representations . However, this method is not designed for the positive distribution shift. Moreover, all these methods require labeled positive and negative data in the training distribution, which are unavailable in our problem setting.

Positive-unlabeled (PU) learning methods aim to learn classifiers by using only positive and unlabeled data [2; 10]. The proposed method is related to the PU learning since it assumes positive and unlabeled data in the training distribution. A representative PU learning method is the empirical minimization-based approach, which rewrites the classification risk by using positive and unlabeled densities [9; 25; 49; 21]. Although they are effective, they cannot maximize the AUC. Recent studies have shown that the AUC can be rewritten from positive and marginal densities by using the technique of the PU learning [45; 58; 6; 59]. However, these all methods assume that the training and test distributions are identical.

Several PU learning methods consider the distribution shift such as covariate shift , class-prior shift [7; 39], or positive distribution shift . They require positive and unlabeled data in the training distribution and unlabeled data in the test distribution. However, they consider the classification risk and cannot maximize the AUC. Due to the pairwise formulation of the AUC, their methods that use ordinary instance-wise loss functions such as the cross-entropy loss cannot be applied to the AUC. In addition, the method for the positive distribution shift  requires the class-prior on the test distribution, which is generally difficult to know with unlabeled data in the test distribution. In contrast, the proposed method does not need to know the class-prior and thus is more practical.

## 3 Preliminary

We briefly explain the AUC maximization. Let instance \(^{D}\) and its corresponding label \(y\{-1,+1\}\) be equipped with probability density \(p(,y)\), where \(+1\) and \(-1\) means a positive and negative class, respectively. Here, \(p^{}():=p(|y=+1)\) and \(p^{}():=p(|y=-1)\) is the conditional probability density of positive and negative class, respectively. Furthermore, let \(s:^{D}\) be a score function that outputs the positivity of an input instance. The classifier is defined by the score function with threshold \(t\): \(y=(s()-t)\), where \(\) is a sign function.

The AUC is the probability of a randomly drawn positive instance being ranked before a randomly drawn negative instance . Specifically, the AUC with score function \(s\) can be formulated as

\[(s)=_{^{} p^{}( )}_{^{} p^{}( )}[I(s(^{})>s(^{}))],\] (1)

\(I(z)\) is the indicator function that outputs \(1\) if \(z\) is true and \(0\) otherwise, and \(\) is the expectation. Since the gradient of indicator function \(I\) is zero everywhere except for the origin, the AUC cannot be optimized via gradient descent methods. To avoid this, the following smoothed AUC is often used by replacing the indicator function with a sigmoid function \((z)=1/(1+(-z))\)[20; 28; 29]:

\[_{}(s)=_{^{} p^{}()}_{^{} p^{}( )}[(s(^{})-s(^{}) )].\] (2)

Given \(N^{}\) positive instances \(\{^{}_{1},,^{}_{N^{}}\}\) drawn from \(p^{}()\) and \(N^{}\) negative instances \(\{^{}_{1},,^{}_{N^{}}\}\) drawn from \(p^{}()\), the empirical estimate of the smoothed AUC is calculated as

\[}_{}(s)=}N^{}} _{n=1}^{N^{}}_{m=1}^{N^{}}[(s( ^{}_{n})-s(^{}_{m}))].\] (3)By maximizing this empirical smoothed AUC with respect to the parameters of \(s\), we can obtain good score functions to maximize the AUC when the training and test distributions are identical .

## 4 Proposed Method

In this section, we first describe our problem setting (subsection 4.1). Then, we theoretically show that the AUC on the test distribution can be maximized with positive and unlabeled data in the training distribution and unlabeled data in the test distribution under the positive distribution shift (subsection 4.2). Then, we discuss class-priors used in the proposed method (subsection 4.3). Lastly, we explain an extension of the proposed method (subsection 4.4).

### Problem Setting

Suppose that we are given a set of positive instances \(X^{}_{}\) and a set of unlabeled instances \(X_{}\) drawn from the training distribution:

\[X^{}_{} =\{^{}_{,n}\}_{n=1}^{N^{ }_{}} p^{}_{}():=p_{ }(|y=+1),\] (4) \[X_{} =\{_{,n}\}_{n=1}^{N_{}} p_{ }()=_{}p^{}_{}( )+(1-_{})p^{}_{}(),\] (5)

where \(p_{}()\) is the marginal density of the training distribution, \(p^{}_{}()\) and \(p^{}_{}():=p_{}(|y=-1)\) are positive and negative-conditional densities of the training distribution, respectively, and \(_{}:=p_{}(y=+1)\) is the positive class-prior. Although we assume that class-prior \(_{}\) is known in this paper, it can be estimated from positive and unlabeled data [44; 63; 15]. In addition, we suppose that a set of unlabeled instances \(X_{}\) drawn from the test distribution is also given:

\[X_{}=\{_{,n}\}_{n=1}^{N_{}} p_{ }()=_{}p^{}_{}( )+(1-_{})p^{}_{}(),\] (6)

where \(p^{}_{}():=p_{}(|y=+1)\), \(p^{}_{}():=p_{}(|y=-1)\), and \(_{}:=p_{}(y=+1)\). We assume the class-imbalance in both unlabeled data, i.e., \(_{}\), \(_{} 1\). As shown later, the proposed method does not need to know \(_{}\) to maximize the AUC1, which is beneficial in practice.

We consider a situation of _the positive distribution shift_ between the training and test distributions, where the negative-conditional density does not change but the positive-conditional one can vary,

\[p^{}_{}()=p^{}_{}( ),\;\;p^{}_{}() p^{}_{ }().\] (7)

This situation often occurs in imbalanced classification problems as described in Section 1. Our aim is to learn score function \(s:^{D}\) that can maximize the AUC on the test distribution by using \(X^{}_{} X_{} X_{}\). For score function \(s\), we can use any differentiable function such as linear classifiers, kernel-based classifiers, or neural networks. Note that we also discuss the negative distribution shift, i.e., \(p^{}_{}() p^{}_{}( )\) and \(p^{}_{}()=p^{}_{}()\), in Section D.

### Positive Distribution Shift Adaptation

In this subsection, we explain how to maximize the AUC under the positive distribution shift. The objective function to be maximized is the following smoothed AUC on the test distribution,

\[_{}(s)=_{^{} p^{}_{}()}_{^{} p^{}_{ }()}[f(^{},^{})],\] (8)

where we set \(f(^{},^{}):=(s(^{ })-s(^{}))\). Since this AUC depends on \(p^{}_{}()\) and \(p^{}_{}()\), it seems be impossible to calculate in our setting where neither positive nor negative data in the test distribution are given. However, we show that calculation is possible. First, from the definition of marginal density \(p_{}()\) in Eq. (6), the positive-conditional test density \(p^{}_{}()\) can be expressed as

\[p^{}_{}()=}}[p_{ }()-(1-_{})p^{}_{}( )],\] (9)

where we assume that \(_{}>0\). By substituting Eq. (9) into Eq. (8), we can obtain

\[_{}(s)\!=\!}}[ _{^{} p_{}()}_{ ^{} p^{}_{}()}[f (,^{})]\!-\!(1\!-\!_{}) _{^{} p^{}_{}( )}_{^{} p^{}_{} ()}[f(^{},^{}) ]].\] (10)Here, the second term in Eq. (10) becomes constant \((1-_{ te})_{^{ p} p_{ tr}^{ n}()} _{^{ n} p_{ tr}^{ n}()}[f( ^{ n},}^{ n})]=(1-_{ te})/2\) because \((z)+(-z)=1\) for all \(z\)[58; 6; 59]. Therefore, we can ignore the second term in Eq. (10) to learn score function \(s\). Next, by using the assumption of the positive distribution shift, \(p_{ tr}^{ n}()=p_{ te}^{ n}()\), and the definition of the marginal density \(p_{ tr}()\) in Eq. (5), we can express the negative-conditional test density \(p_{ te}^{ n}()\) as

\[p_{ te}^{ n}()=p_{ tr}^{ n}()=}[p_{ tr}()-_{ tr}p_{ tr}^{ p}()],\] (11)

where we assume that \(_{ tr}<1\). As before, by substituting Eq. (11) into the first term in Eq. (10), we can obtain

\[_{}(s)\!=\!(1-_{ tr})}[ _{ p_{ te}()}_{ } p_{ tr}()}[f(,})]\!- \!_{ tr}_{ p_{ te}()}_{ ^{ p} p_{ tr}^{ p}()}[f(, ^{ p})]]\!+\!C,\] (12)

where \(C\) represents constant terms that do not depend on score function \(s\). Since coefficient \(1/(_{ te}(1-_{ tr}))\) does not affect the optimization for \(s\), our loss function to be minimized is as follows,

\[(s):=-_{ p_{ te}()} _{} p_{ tr}()}[f(,})]+_{ tr}_{ p_{ tr}()} _{^{ p} p_{ tr}^{ p}()}[f( ,^{ p})].\] (13)

This loss depends on the positive and marginal training densities, marginal test densities, and class-prior. Therefore, we can approximate this loss with given data \(X_{ tr}^{ p} X_{ tr} X_{ te}\) and \(_{ tr}\). Specifically, the empirical estimate of the loss function is given as follows,

\[}(s)=-N_{ tr}}_{n,m=1}^{N_{ te},N _{ tr}}f(_{ te,n},_{ tr,m})+}{ N_{ te}N_{ tr}^{ p}}_{n,m=1}^{N_{ te},N_{ tr}^{ p}}f( _{ te,n},_{ tr,m}^{ p}).\] (14)

Algorithm 1 shows the pseudocode of our training procedure with stochastic gradient methods. Note that although we have used the sigmoid function to represent the AUC in Eq. (2), as long as we use symmetric functions (i.e., function \(\) satisfying \((z)+(-z)=K\) for any \(z\) and \(K\) is a constant ), we can derive the loss function of the same form in Eq. (13). The symmetric functions include a wide range of functions such as sigmoid, ramp, and unhinged functions . In addition, the loss corrections for the proposed method such as the non-negative correction , which is commonly used in ordinary PU learning to avoid overfitting, are discussed in detail in Section B.

### Discussion about Class-priors

In the process of deriving our loss function, we assume \(_{ tr}<1\) and \(_{ te}>0\). We discuss why this assumption is necessary in our setting. When \(_{ te}=0\), all unlabeled data \(X_{ te}\) become negative data. In this case, our given datasets \(X_{ tr}^{ p} X_{ tr} X_{ te}\) do not have any information about positive data in the test distribution since \(p_{ tr}^{ p}() p_{ te}^{ p}()\). Therefore, we cannot calculate the AUC on the test distribution. However, in this case, we can easily create trivial optimal classifiers that classify all data as negative. Thus, this is not an issue.

When \(_{ tr}=1\), all unlabeled data \(X_{ tr}\) become positive data on the training distribution. \(X_{ tr}^{ p} X_{ tr}\) (positive data in the training distribution) have no information to extract positive and negative data from \(X_{ te}\) since \(p_{ tr}^{ p}() p_{ te}^{ p}()\). Thus, in this case, we also cannot calculate the AUC on the test distribution. However, it is extremely rare for all unlabeled data to be positive in imbalanced data problems where the ratio of positive data in unlabeled data is low. Thus, this is also not a problem in practice.

### Extension

In some applications, a small number of labeled negative data as well as labeled positive data might be available from the training distribution. The proposed method can be easily extended to such cases. Specifically, by Eq. (10), the AUC on the test distribution without constant terms is represented as

\[_{}(s)_{ p_{}( )}_{^{n} p_{}^{}( )}[f(,^{n})]=_{  p_{}()}_{^{n} p_{}^{}()}[f(,^{n})]=: (s),\] (15)

where we used assumption \(p_{}^{}()=p_{}^{}()\). Loss \((s)\) can be approximated with test unlabeled data and labeled negative data. Thus, the following modified loss function to be minimized can be used:

\[_{}(s):=(s)-(1-)( s),\] (16)

where \(\) is a weighting hyperparameter.

## 5 Experiments

In this section, we empirically demonstrate the effectiveness of the proposed method under the positive distribution shift with real-world datasets.

### Data

We utilized four widely used real-world datasets in the main paper: MNIST , FashionMNIST , SVHN , and CIFAR10 . MNIST consists of hand-written images of 10 digits. Each image is represented by gray-scale with \(28 28\) pixels. FashionMNIST consists of images of 10 fashion categories where each image is represented by gray scale with \(28 28\) pixels. SVHN consists of \(32 32\) RGB images of printed 10 digits clipped from photographs of house number plates. We converted SVHN into gray-scale for simplicity. CIFAR10 consists of \(32 32\) RGB images of 10 animal and vehicle categories. In Section E.3, we also evaluated the proposed method with two tabular datasets with distribution shifts (HReadmission and Hypertension) .

For MNIST and SVHN, we used even digits as the negative class and odd digits as the positive class. Following the previous studies [16; 18], data with digits '7' and '9' were used as positive data appearing in the training distribution and data with digits '1', '3', '5', '7', and '9' were used as positive data appearing in the test distribution. This simulates a situation where new types of positive data, which did not appear during the training, appear in the test environment. For FashionMNIST, following the study , we used upper garments ('T-shift', 'Pullover', 'Dress', 'Coat', and 'Shift') as the negative class and the others as the positive class. Positive data with the 'Trouser' and 'Bag' appeared in the training distribution and those with the 'Trouser', 'Bag', 'Sandal', 'Sneaker', and 'Ankle boot' appeared in the test distribution. For CIFAR10, we used the animal categories as the negative class and the vehicles as the positive class. Positive data with the 'Airplane' appeared in the training distribution and those with the 'Airplane', 'Automobile', 'Ship', and 'Truck' appeared in the test distribution as in the previous study .

For each dataset, we used \(10\) positive and \(5,000\) unlabeled data in the training distribution and \(5,000\) unlabeled data in the test distribution for training. In addition, we used \(5\) positive and \(500\) unlabeled data in the training distribution and \(500\) unlabeled data in the test distribution for validation. We used \(1,500\) positive and \(1,500\) negative data in the test distribution as test data for evaluation. There is no overlap between training, validation, and test datasets. The positive class-prior on the training distribution \(_{}\) was set to \(0.1\) and that on the training distribution \(_{}\) was changed within \(\{0.1,0.2,0.3\}\). For each case of the positive class-prior pairs, we conducted eight experiments while changing the random seeds and evaluated mean test AUC.

### Comparison Methods

We compared the proposed method (Ours) with nine neural network-based comparison methods: CE, nnPU , puAUC , PULA , AnnPU, ApuAUC, APULA, CpuAUC, and PURR .

CE, nnPU, puAUC, and PULA use positive and unlabeled data in the training distribution for learning classifiers. These methods do not adapt to the test distribution. CE learns neural network parameters by minimizing the cross-entropy loss. This method naively treats unlabeled data as negative data. nnPU is a widely used PU learning method with a non-negative risk estimator. This method rewritesthe classification risk with positive and unlabeled data and minimizes the rewritten non-negative classification risk to learn neural network parameters. PULA is a recent PU learning method that learns a neural network classifier with the label distribution alignment. nnPU and PULA used the class-prior in the training distribution as in the proposed method. puAUC learns a neural network classifier by maximizing the AUC that is calculated from positive and unlabeled data.

AnnPU, ApuAUC, APULA, CpuAUC, and PURR use positive and unlabeled data in the training distribution and unlabeled data in the test distribution. AnnPU, ApuAUC, and APULA use unlabeled data in both the training and test distributions instead of those in the training distribution of nnPU, puAUC, and PULA respectively. AnnPU and APULA used the composite class-prior on the combined unlabeled data (i.e., \(=}N_{}+_{}N_{}}{N_{}+N_{}}\)). In our preliminary experiments, the methods that use unlabeled data from the test distribution instead of data from both distributions were also evaluated. However, they performed worse than AnnPU, ApuAUC, and APULA. Thus, we omitted them. CpuAUC learns the invariant feature representations to mitigate the discrepancy of training and test distributions. Specifically, this method minimizes the negative AUC loss used in puAUC and the CORAL loss, which is a widely used in domain adaptation studies to match the second-order statistics of both distributions [53; 52]. PURR is a PU learning method for the positive distribution shift. This method rewrites the classification risk assuming the negative-conditional density does not change. To rewrite the risk, PURR requires the class-prior in the test distribution to be available, whereas the proposed method does not.

### Settings

For MNIST, FashionMNIST, and SVHN, all methods used a feed-forward neural network with three hidden layers. The number of hidden nodes was \(128\) and the ReLU activation function was used for each hidden layer. For CIFAR10, all methods used a convolutional neural network, which consisted of two convolutional blocks followed by a feed-forward neural network with two hidden layers. The first (second) convolutional block comprised a \(6\) (\(16\)) filter \(5 5\) convolution, the ReLU activation function, and a \(2 2\) max-pooling layer . The numbers of nodes in the two hidden layers were \(120\) and \(84\), and the ReLU activation function was used. For PULA and APULA, margin \(\) was selected from \(\{0.1,1,10\}\). For CpuAUC, the CORAL loss was applied to the last hidden layer and its regularization weight was selected within \(\{100,10,1,0.1,0.01\}\). For PULA, APULA, and CpuAUC, the best test results were reported. For all methods, we used the Adam optimizer  with a learning rate of \(10^{-4}\). We set a mini-batch size \(M\) to \(512\), a positive mini-batch size \(P\) to \(10\), and the maximum number of epochs to \(200\). The loss on validation data was used for early stopping to avoid overfitting. All methods were implemented using Pytorch  and all experiments were conducted on a Linux server with an Intel Xeon CPU and A100 GPU.

### Results

We evaluated the performance of the proposed method under the positive distribution shift with imbalanced datasets. Table 1 shows the average test AUCs of each method on the four datasets (their standard deviations are reported in Section E.4). The proposed method performed the best or comparably to it in almost all cases (\(11\) out of \(12\) cases). The non-adaptation methods (CE, nnPU, puAUC, and PULA) that do not use data in the test distribution performed worse than the proposed method in many cases. This result indicates that adaptation using data in the test distribution is essential when there are distribution shifts. Although AnnPU, ApuAUC, and APULA used unlabeled data in the test distribution, they also did not work well. This is because their loss functions that naively use unlabeled data in the test distribution are not designed to maximize the performance on the test distribution. CpuAUC that learns invariant feature representations to mitigate the distribution gap tended to perform worse than the proposed method since the invariant features are not theoretically guaranteed to maximize the performance under the positive distribution shift. PURR performed well with MNIST and FashionMNIST since it is designed to adapt to the positive distribution shift and both datasets are relatively simple data where training is easy even in imbalanced data settings. We note that PURR used information on true class-priors on the test distribution, which is not used in the proposed method. By maximizing the AUC, the proposed method outperformed PURR with SVHN and CIFAR10 by a large margin, which are more complex than MNIST and FashionMNIST. As for the results of each class-prior pair, the proposed method performed better as the positive class-prior on the test distribution \(_{}\) increased. Since unlabeled data \(X_{}\) has many positive data when \(_{ te}\) is large, the proposed method more easily extracted information on positive data from \(X_{ te}\) and consequently performed well.

We conducted an ablation study of the proposed method. When positive class-prior on the training distribution \(_{ tr}\) is small, all unlabeled data \(X_{ tr}\) might be treated as negative data. In this case, existing methods can be directly applied to maximize the AUC on the test distribution by using unlabeled data in the test distribution \(X_{ te}\) and (pseudo) negative data \(X_{ tr}\). This approach is equivalent to the proposed method without the second term (or \(_{ tr}=0\)) in Eq. (13), denoted as Ours w/o the second. Figure 2 shows the average test AUCs and the standard errors of the proposed method (Ours) and Ours w/o the second when changing the value of class-prior \(_{ tr}\) within \(\{0.01,0.05,0.1,0.15,0.2\}\). Ours consistently outperformed Ours w/o the second across all class-priors \(_{ tr}\) except for SVHN. As class-prior \(_{ tr}\) is increased, the difference between the two methods tended to become larger. This is because the assumption of Ours w/o the second (i.e., unlabeled data \(X_{ tr}\) are all negative data) is significantly broken when \(_{ tr}\) is large. With SVHN, the performances of both methods did not differ. This result suggests that it is acceptable to consider all unlabeled data as negative data in some datasets. However, overall, these results show that the naive approximation is generally ineffective and our theoretically grounded loss function is important even with small \(_{ tr}\).

We evaluated the proposed method with estimated class-priors on the training distribution since class-prior information might be unavailable in practice. To estimate the class-prior from positive and unlabeled training data, we used the kernel embedding-based class-prior estimation method . Table 2 shows the results. Here, Ours w/ true and Ours w/ est are the proposed method using true class-prior \(_{ tr}\) and estimated class-prior \(_{ tr}^{ est}\), respectively. We found that true class-prior \(_{ tr}=0.1\) was accurately estimated from positive and unlabeled data in the training distribution. As a result, Ours w/

  Data & \(_{ te}^{ est}\) & Ours & CE & nnPU & puAUC & PULA & AnnPU & ApuAUC & APULA & CPUAUC & PURR \\  MNIST & 0.1 & **0.723** & **0.796** & **0.782** & **0.801** & **0.802** & **0.782** & **0.798** & **0.793** & **0.804** & **0.815** \\  & 0.2 & **0.854** & 0.796 & 0.784 & 0.801 & 0.802 & 0.785 & 0.793 & 0.769 & 0.803 & **0.871** \\  & 0.3 & **0.902** & 0.796 & 0.783 & 0.800 & 0.802 & 0.780 & 0.786 & 0.745 & 0.802 & **0.914** \\  Fashion & 0.1 & 0.787 & 0.932 & 0.825 & 0.937 & **0.954** & 0.772 & 0.920 & **0.939** & 0.943 & 0.920 \\ MNIST & 0.2 & **0.891** & 0.930 & 0.825 & 0.937 & **0.955** & 0.771 & 0.890 & **0.911** & 0.943 & **0.929** \\  & 0.3 & **0.960** & 0.930 & 0.825 & **0.938** & **0.955** & 0.866 & 0.870 & 0.863 & **0.944** & **0.961** \\  SVHN & 0.1 & **0.554** & 0.504 & 0.501 & 0.518 & 0.944 & 0.501 & 0.512 & 0.492 & 0.524 & 0.511 \\  & 0.2 & **0.660** & 0.503 & 0.501 & 0.518 & 0.494 & 0.501 & 0.507 & 0.490 & 0.523 & 0.522 \\  & 0.3 & **0.736** & 0.503 & 0.501 & 0.518 & 0.494 & 0.501 & 0.507 & 0.490 & 0.523 & 0.519 \\  CIFAR10 & 0.1 & **0.727** & **0.682** & 0.455 & **0.749** & **0.751** & 0.489 & **0.739** & **0.750** & **0.750** & 0.434 \\  & 0.2 & **0.825** & 0.679 & 0.467 & 0.741 & 0.739 & 0.491 & 0.732 & 0.742 & 0.744 & 0.536 \\  & 0.3 & **0.874** & 0.682 & 0.451 & 0.750 & 0.738 & 0.545 & 0.722 & 0.750 & 0.749 & 0.710 \\  \# best & & 11 & 2 & 1 & 3 & 5 & 1 & 2 & 4 & 3 & 5 \\  

Table 1: Average test AUCs with different class-prior pairs. Values in bold are not statistically different at the \(5\%\) level from the best performing method in each row according to a paired t-test. ‘# best’ row represents the number of results of each method that are the best or comparable to it.

  Data & \(_{ tr}^{ est}\) & Ours w/ true & Ours w/ est \\  MNIST & 0.117 & 0.826 & 0.845 \\ FMNIST & 0.077 & 0.879 & 0.857 \\ SVHN & 0.078 & 0.650 & 0.652 \\ CIFAR10 & 0.146 & 0.809 & 0.811 \\  

Table 2: Results with estimated class-priors on the training distribution \(_{ tr}^{ est}\): average test AUCs [%] over different class-priors on the test distribution when training class-prior \(_{ tr}\) is \(0.1\). FMNIST is an acronym for FashionMNIST.

Figure 2: Ablation study of our loss function: average test AUCs and standard errors over different class-priors on the test distribution \(_{ te}\) within \(\{0.1,0.2,0.3\}\) when changing the class-prior on the training distribution \(_{ tr}\). Ours w/o the second is Ours without the second term in Eq. (13).

est and Ours w/ true mostly performed similarly. This result indicates that the proposed method works well even when class-prior information is unavailable in the training distribution, which is preferable in practice. Additionally, we investigated how the performance of the proposed method changes when there is a difference between true and input class-priors on the training distribution \(_{}\). Here, the input class-prior is used in the loss function of Eq. (13) and the true class-prior is used for data generation. Figure 3 shows the average test AUCs and the standard errors of the proposed method (Ours) when changing the input class-priors. Interestingly, the performance tended to improve when values larger than the true class-prior (\(0.1\)) were inputted in most datasets. One reason for this result is that since there was some overlap between the types of positive data in the test and training distributions, strengthening the influence on labeled positive data by increasing the input class-prior in Eq. (13) improved the performance. To confirm this, we conducted additional experiments by excluding the types of positive data from the test distribution that were used in the training distribution (e.g., digits '7' and '9' were excluded from the test distribution in MNIST). Figure 4 shows the results. As expected, the performance did not improve even with large input class-priors.

We investigated how the performance of the proposed method changes when the number of labeled positive data \(N_{}^{}\) is increased. Figure 5 shows the average test AUCs and the standard errors with different numbers of labeled positive data. As expected, the performance of the proposed method tended to improve as \(N_{}^{}\) increased. By using much information on labeled positive data in the training distribution, the proposed method can accurately estimate the AUC on the test distribution.

We investigated the performance of the proposed method when changing the number of unlabeled data in the test distribution in Table 3. As expected, the performance of the proposed method tended to increase as the number of unlabeled data in the test distribution \(N_{}\) increased. The proposed method tended to outperform puAUC, which does not use unlabeled data in the test distribution, even when \(N_{}=500\). Since many unlabeled data are often easy to collect, the proposed method is useful in practice.

We evaluated the modified loss function in Eq. (16) for the cases where positive, negative, and unlabeled data in the training distribution and unlabeled data in the test distribution are available. Table 4 shows the results. Here, the number of labeled negative data was set to \(20\) and other settings

Figure 4: Results in the case where true and input class-priors on the training distribution \(_{}\) can be different when the positive distribution drastically changed: average test AUCs and standard errors with true training class-prior \(_{}=0.1\) when changing the input class-prior on the training distribution.

Figure 3: Results in the case where true and input class-priors on the training distribution \(_{}\) can be different: average test AUCs and standard errors over different class-priors on the test distribution \(_{}\) within \(\{0.1,0.2,0.3\}\) with true training class-prior \(_{}=0.1\) when changing the input class-prior on the training distribution.

are the same as those in the previous experiments. First, the proposed method with \(=0\), which maximizes the AUC on the test distribution in Eq. (15) with labeled negative and test unlabeled data, did not work well. This would be because the number of positive data in test unlabeled data was small, and thus it was difficult to extract information on such positive data from unlabeled data by using a few labeled negative data only. The proposed method with \(=0.999\), which uses positive, negative, and unlabeled data in the training distribution and unlabeled data in the test distribution, slightly tended to perform better than that with \(=1.0\), which is equivalent to the proposed method described in Section 4.2. This result suggests that using a few labeled negative data is useful in our framework.

## 6 Conclusion

In this paper, we proposed a AUC maximization method under the positive distribution shift. We theoretically showed the AUC on the test distribution can be maximized by using positive and unlabeled data in the training distribution and unlabeled data in the test distribution when assuming the positive distribution shift. The derived AUC has the advantage of being simple and easy to implement. The experiments with six real-world datasets demonstrated that the proposed method outperformed existing methods under the positive distribution shift with imbalanced data.

## 7 Limitations

The proposed method performs well in the positive distribution shift. As stated in Section 1, we can expect that the positive distribution shift must occur in many real-world applications. However, the proposed method is not guaranteed to keep working well if different distribution shifts occur unexpectedly (i.e., the negative-conditional density changes). To deal with this problem, methods should be developed that can predict the types of the shift from given data.

 Data &  &  &  &  \\  \(_{}\) & 0.1 & 0.2 & 0.3 & 0.1 & 0.2 & 0.3 & 0.1 & 0.2 & 0.3 & 0.1 & 0.2 & 0.3 \\  \(=1.0\) & 0.723 & 0.854 & 0.902 & 0.787 & 0.891 & 0.960 & 0.554 & 0.660 & 0.736 & 0.727 & 0.825 & 0.873 \\ \(=0.999\) & 0.727 & 0.875 & 0.904 & 0.816 & 0.939 & 0.947 & 0.542 & 0.662 & 0.743 & 0.731 & 0.838 & 0.870 \\ \(=0.0\) & 0.517 & 0.577 & 0.661 & 0.732 & 0.794 & 0.838 & 0.503 & 0.506 & 0.510 & 0.602 & 0.594 & 0.697 \\  

Table 4: Results when a few labeled negative data are available on the training distribution: average test AUCs for each test class-prior. FMNIST is an acronym for FashionMNIST.

Figure 5: Average test AUCs and standard errors of the proposed method over different class-priors on the test distribution when changing the number of labeled positive data in the training distribution \(N_{}^{}\).

  \(N_{}\) & 100 & 500 & 1000 & 2000 & 5000 & Base \\  MNIST & 0.773 & 0.813 & 0.823 & 0.826 & 0.827 & 0.801 \\ Fashion MNIST & 0.863 & 0.913 & 0.918 & 0.911 & 0.879 & 0.938 \\ SVHN & 0.516 & 0.579 & 0.616 & 0.626 & 0.650 & 0.518 \\ CIFAR10 & 0.683 & 0.757 & 0.770 & 0.778 & 0.809 & 0.746 \\  

Table 3: Results of the proposed method with different numbers of unlabeled data in the test distribution : average test AUCs over different class-prior pairs \(_{}\) in the test distribution within \(\{0.1,0.2,0.3\}\) when training class-prior \(_{}\) is 0.1. ‘Base’ represents the result of puAUC that does not use unlabeled data in the test distribution.