# Spectral Graph Pruning Against Over-Squashing and Over-Smoothing

Adarsh Jamadandi

\({}^{1,2}\)

adarsh.jam@gmail.com

&Celia Rubio-Madrigal

celia.rubio-madrigal@cispa.de

&Rebekka Burkholz

burkholz@cispa.de

\({}^{1}\) Universitat des Saarlandes

\({}^{2}\) CISPA Helmholtz Center for Information Security

###### Abstract

Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: _over-squashing_ and _over-smoothing_. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets.

## 1 Introduction

Graphs are ubiquitous data structures that can model data from diverse fields ranging from chemistry (Reiser et al., 2022), biology (Bongini et al., 2023) to even high-energy physics (Shlomi et al., 2021). This has led to the development of deep learning techniques for graphs, commonly referred to as Graph Neural Networks (GNNs). The most popular GNNs follow the message-passing paradigm (Gori et al., 2005; Scarselli et al., 2009; Gilmer et al., 2017; Bronstein et al., 2021), where arbitrary differentiable functions, parameterized by neural networks, are used to diffuse information on the graph, consequently learning a graph-level representation. This representation can then be used for various downstream tasks like node classification, link prediction, and graph classification. Different types of GNNs (Kipf and Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018; Xu et al., 2019; Bodnar et al., 2021, 2021, 2022), all tackling a variety of problems in various domains have been proposed with varied degree of success. Despite their widespread use, GNNs have a number of inherent problems. These include limited expressivity, (Leman, 1968; Morris et al., 2019), over-smoothing (Li et al., 2019; NT and Maehara, 2019; Oono and Suzuki, 2020; Zhou et al., 2021), and over-squashing (Alon and Yahav, 2021; Topping et al., 2022).

The phenomenon of over-squashing, first studied heuristically by Alon and Yahav (2021) and later theoretically formalized by Topping et al. (2022), is caused by the presence of structural bottlenecks inthe graph. These bottlenecks can be attributed to the first non-zero eigenvalue of the normalized graph Laplacian, also known as the spectral gap. The smaller the gap, the more susceptible a graph is to over-squashing. Recent work has explored rewiring the input graph to address these bottlenecks (Topping et al., 2022; Amaiz-Rodriguez et al., 2022; Giraldo et al., 2023; Nguyen et al., 2023; Karhadkar et al., 2023), but suggest there has to be a trade-off between over-squashing and over-smoothing (Keriven, 2022). Instead, we propose to leverage the Braess paradox (Braess, 1968; Eldan et al., 2017) that posits certain edge _deletions_ can maximize the spectral gap. We propose to approximate the spectral change in a computationally efficient manner by leveraging Matrix Perturbation Theory (Stewart and Sun, 1990). Our proposed framework allows us to jointly address the problem of over-squashing, by increasing the spectral gap, and over-smoothing, by _slowing_ down the rate of smoothing. We find that our method is especially effective in heterophilic graph settings, where we delete edges between nodes of different labels, thus preventing unnecessary aggregation. We empirically show that our proposed method outperforms other graph rewiring methods on node classification and graph classification tasks. We also show that spectral gap based edge deletions can help identify graph lottery tickets (GLTs) (Frankle and Carbin, 2019), that is, sparse sub-networks that can match the performance of dense networks.

### Contributions

1. Inspired by the Braess phenomenon, we prove that, contrary to common assumptions, over-smoothing and over-squashing are not necessarily diametrically opposed. By deriving a minimal example, we show that both can be mitigated by spectral based edge deletions.
2. Leveraging matrix perturbation theory, we propose a Greedy graph pruning algorithm (ProxyDelete) that maximizes the spectral gap in a computationally efficient way. Similarly, our algorithm can also be utilized to add edges in a joint framework. We compare this approach with a novel graph rewiring scheme based on Eldan's criterion (Eldan et al., 2017) that provides guarantees for edge deletions and a stopping criterion for pruning, but is computationally less efficient.
3. Our results connect literature on three seemingly disconnected topics: over-smoothing, over-squashing, and _graph lottery tickets_, which explain observed improvements in generalization performance by graph pruning. Utilizing this insight, we demonstrate that graph sparsification based on our proxy spectral gap update can perform better than or on par with a contemporary baseline (Chen et al., 2021) that takes additional node features and labels into account. This highlights the feasibility of finding winning subgraphs at initialization.

## 2 Related work

**Over-squashing.**Alon and Yahav (2021); Topping et al. (2022) have observed that over-squashing, where information from distant nodes are not propagated due to topological bottlenecks in the graph, hampers the performance of GNNs. A promising line of work that attempts to alleviate this issue is _graph rewiring_. This task aims to modify the edge structure of the graph either by adding or deleting edges. Gasteiger et al. (2019) propose to add edges according to graph diffusion kernel, such as personalized PageRank, to rely less on messages from only one-hop neighbors, thus alleviating over-squashing. Topping et al. (2022) propose Stochastic Discrete Ricci Flow (SDRF) to rewire the graph based on curvature. Banerjee et al. (2022) resort to measuring the spectral expansion with respect to the number of rewired edges and propose a random edge flip algorithm that transforms the given input graph into an Expander graph. Contrarily, Deac et al. (2022) show that negatively curved edges might be inevitable for building scalable GNNs without bottlenecks and advocate the use of Expander graphs for message passing. Arnaiz-Rodriguez et al. (2022) introduces two new intermediate layers called CT-layer and GAP-layer, which can be interspersed between GNN layers. The layers perform edge re-weighting (which minimizes the gap) and introduce additional parameters. Karhadkar et al. (2023) propose FoSR, a graph rewiring algorithm that sequentially adds edges to maximize the first-order approximation of the spectral gap. A recent work by Black et al. (2023) explores the idea of characterizing over-squashing through the lens of effective resistance (Chandra et al., 1996). Giovanni et al. (2023) provide a comprehensive account of over-squashing and studies the interplay of depth, width and the topology of the graph.

**Over-smoothing.** It is a known fact that increasing network depth (He et al., 2016) often leads to better performance in the case of deep neural networks. However, naively stacking GNN layers often seemsto harm generalization. And one of the reasons is over-smoothing (Li et al., 2019; Oono and Suzuki, 2020; NT and Maehara, 2019; Zhou et al., 2021; Rusch et al., 2023a), where repeated aggregation leads to node features, in particular nodes with different labels, becoming indistinguishable. Current graph rewiring strategies, such as FoSR (Karhadkar et al., 2023), which rely on iteratively adding edges based on spectral expansion, may help mitigate over-squashing but also increase the smoothing induced by message passing. Curvature based methods such as Nguyen et al. (2023); Giraldo et al. (2023) aim to optimize the degree of smoothing by graph rewiring, as they assume that over-smoothing is the result of too much information propagation, while over-squashing is caused by too little. Within this framework, they assume that edge deletions always reduce the spectral gap. In contrast, we show and exploit that some deletions can also increase it. Furthermore, we rely on a different, well established concept of over-smoothing (Keriven, 2022) that also takes node features into account and is therefore not diametrically opposed to over-squashing. As we show, over-smoothing and over-squashing can be mitigated jointly. Moreover, we propose a computationally efficient approach to achieve this with spectral rewiring. In contrast to our proposal, curvature based methods (Nguyen et al., 2023; Giraldo et al., 2023) do not scale well to large graphs. For instance, Nguyen et al. (2023) propose a batch Ollivier-Ricci (BORF) curvature based rewiring approach to add and delete edges, which solves optimal transport problems and runs in cubic time.

**Graph sparsification and lottery tickets.** Most GNNs perform recursive aggregations of neighborhood information. This operation becomes computationally expensive when the graphs are large and dense. A possible solution for this is to extract a subset of the graph which is representative of the dense graph, either in terms of their node distribution (Eden et al., 2018) or graph spectrum (Adhikari et al., 2017). Zheng et al. (2020); Li et al. (2020) formulate graph sparsification as an optimization problem by resorting to learning surrogates and ADMM respectively. With the primary aim to reduce the computational resource requirements of GNNs, a line of work that transfers the lottery ticket hypothesis (LTH) by Frankle and Carbin (2019) to GNNs (Chen et al., 2021; Hui et al., 2023), prunes the model weights in addition to the adjacency matrix. The resulting _winning graph lottery ticket_ (GLT) can match or surpass the performance of the original dense model. While our theoretical understanding of GLTs is primarily centered around their existence (Ferbach et al., 2022; Burkholz et al., 2022; Burkholz, 2022b, a), our insights inspired by the Braess paradox add a complementary lens to our understanding of how generalization can be improved, namely by reducing over-squashing and over-smoothing with graph pruning. So far, the spectral gap has only been employed to maintain a sufficient degree of connectivity of bipartite graphs that are associated with classic feed-forward neural network architectures (Pal et al., 2022; Hoang et al., 2023). We highlight that the spectral gap can also be employed as a pruning at initialization technique (Frankle et al., 2021) that does not take node features into account and can achieve computational resource savings while reducing the generalization error, which is in line with observations for random pruning of CNNs (Gadhikar et al., 2023; Gadhikar and Burkholz, 2024).

## 3 Theoretical insights into spectral rewiring

To prove our claim that over-smoothing and over-squashing can both be alleviated jointly, we provide a minimal example as illustrated in Figure 1. Utilizing the Braess paradox, we achieve this by the deletion of an edge. In contrast, an edge addition that addresses over-squashing still causes over-smoothing, yet less drastically than another edge addition that worsens over-squashing.

**Reducing over-squashing via the spectral gap.** From a spectral perspective, bottlenecks, which hamper the information flow by over-squashing, can be characterized by the spectral gap of the (symmetric) normalized graph Laplacian \(_{}\), where \(=(,)\). The Laplacian of the graph is \(=D-A\), where \(A\) is the adjacency matrix and \(D\) the diagonal degree matrix. The symmetric normalized graph Laplacian is defined as \(_{}=D^{-1/2}D^{-1/2}\). Let \(\{_{0}<_{1}<_{2},..._{n}\}\) be the eigenvalues of \(_{}\) arranged in ascending order and let \(_{1}(_{})\) be the first non-zero eigenvalue of the normalized graph Laplacian, which is also called the spectral gap of the graph. For a graph where distant network components are connected only by a few bridging edges, all the information has to be propagated via these edges. The information flow through edges is encoded by the Cheeger (1971) constant \(h_{S}=_{S V}}\) where \( S=\{(u,v):u S,v S\}\) and \(Vol(S)=_{u S}d_{u}\), being \(d_{u}\) the degree of the node \(u\). The spectral gap is bounded by the Cheeger inequality \(2h_{}_{1}}^{2}}{2}\), which motivates it as a measure of over-squashing.

**Braess' paradox.** Braess (1968) found a counter-intuitive result for road networks: even if all travelers behave selfishly, the removal of a road can still improve each of their individual travel times. That is, there is a violation of monotonicity in the traffic flow with respect to the number of edges of a network. For instance, Chung & Young (2010) has shown that Braess' paradox occurs with high probability in Erdos-Renyi random graphs, and Chung et al. (2012) have confirmed it for a large class of Expander graphs. The paradox can be analogously applied to related graph properties such as the spectral gap of the normalized Laplacian. Eldan et al. (2017) have studied how the spectral gap of a random graph changes after edge additions or deletions, proving a strictly positive occurrence of the paradox for typical instances of ER graphs. This result inspires us to develop an algorithm for rewiring a graph by specifically eliminating edges that increase this quantity, which we can expect to carry out with high confidence in real-world graphs. Their Lemma 3.2 (when reversed) states a sufficient condition that guarantees a spectral gap increase in response to a deletion of an edge.

**Lemma 3.1**.: _Eldan et al. (2017): Let \(=(,)\) be a finite graph, with \(f\) denoting the eigenvector and \(_{1}(_{})\) the eigenvalue corresponding to the spectral gap. Let \(\{u,v\}\) be two vertices that are not connected by an edge. Denote \(}=(,})\), the new graph obtained after adding an edge between \(\{u,v\}\), i.e., \(}:=\{u,v\}\). Denote with \(_{f}:= f,_{0}\) the projection of \(f\) onto the top eigenvector of \(}\). Define \(g(u,v,_{}):=\)_

\[-_{f}^{2}_{1}(_{})-2(1-_{1}( _{}))(+1}-}}{+1}}f_{u}^{2}..++1}-}}{+1}} f_{v}^{2})+f_{v}}{+1}+1}}.\]

_If \(g(u,v,_{})>0\), then \(_{1}(_{})>_{1}(_{}})\)._

As a showcase example of the Braess phenomenon, let us analyze the behaviour of the spectral gap in terms of an edge perturbation on the ring graph of \(n\) nodes \(R_{n}\). We consider the ring \(R_{8}\) as \(^{-}\), the deletion of an edge from graph \(\) in Figure 1.

**Proposition 3.2**.: _The spectral gap of \(\) increases with the deletion of edge \(\{0,3\}\), i.e., \(_{1}(_{^{-}})>_{1}(_{})\). It also increases with the addition of edge \(\{0,5\}\) or decreases with the addition of edge \(\{4,7\}\), i.e., \(_{1}(_{^{+}})>_{1}(_{})\) and \(_{1}(_{^{+}})<_{1}(_{})\)._

We leverage Eldan's Lemma 3.1 in Appendix A.1 and apply the spectral graph proxies in our derivations starting from an explicit spectral analysis of the ring graph. While these derivations demonstrate that we can reduce over-squashing (i.e., increase the spectral gap) by edge deletions, we show next that edge deletions can also alleviate over-smoothing.

**Slowing detrimental over-smoothing.** For GNNs with mean aggregation, increasing the spectral gap usually promotes smoothing and thus leads to higher node feature similarity. Equating a high node feature similarity with over-smoothing would thus imply a trade-off between over-smoothing and over-squashing. Methods by Giraldo et al. (2023); Nguyen et al. (2023) seek to find the right amount of smoothing by adding edges to increase the gap and deleting edges to decrease it. _Contrarily, we

Figure 1: Braess’ paradox. We derive a simple example where deleting an edge from \(\) to obtain \(^{-}\) yields a higher spectral gap. Alternatively, we add a single edge to the base graph to either increase (\(^{+}\)) or to decrease (\(^{+}}\)) the spectral gap. The relationship between the four graphs is highlighted by arrows when an edge is added/deleted.

argue that deleting edges can also increase the gap while adding edges could decrease it_, as our previous analysis demonstrates. Thus, both edge deletions and additions allow to control which node features are aggregated, while mitigating over-squashing. Such node features are central to a more nuanced concept of over-smoothing that acknowledges that increasing the similarity of nodes that share the same label, while keeping nodes with different labels distinguishable, aids the learning task.

To measure over-smoothing, we adopt the Linear GNN test bed proposed by Keriven (2022), which uses a linear ridge regression (LRR) setup with mean squared error (MSE) as the loss. We assign two classes to nodes according to their color in Figure 1, and one-dimensional features that are drawn independently from normal distributions \((1,1)\) and \((-1,1)\), respectively. Figure 2 compares how our exemplary graphs (see Figure 1) influence over-smoothing in this setting. While adding edges can accelerate the rate of smoothing, pruning strikingly aids in reducing over-smoothing --and still reduces over-squashing by increasing the spectral gap. Note that the real world heterophilic graph example shows a similar trend and highlights the utility of the spectral pruning algorithm ProxyDelete, which we describe in the next section, over edge additions by the strong baseline FoSR. Additional real world examples along with cosine distance between nodes of different labels before and after spectral pruning and plots for Dirichlet energy can be found in Appendix D.

In the following, we discuss and analyze rigorously the reasons for this finding. Consider again the ring graph \(^{-}\), which has an _inter-class_ edge pruned from our base graph \(\); this avoids a problematic aggregation step and in this way mitigates over-smoothing. Instead of deleting an edge, we could also add an edge arriving at \(^{+}\), which would lead to a higher spectral gap than the edge deletion. Yet, it adds an edge between nodes with different labels and therefore leads to over-smoothing. We also prove this relationship rigorously for one step of mean aggregation.

**Proposition 3.3**.: _As more edges are added (from \(^{-}\) to \(\), or from \(\) to \(^{+}\) or \(^{+}}\)), the average value over same-class node representations after a mean aggregation round becomes less informative._

The proof is presented in Appendix A.2. We argue that similar situations arise particularly in heterophilic learning tasks, where spectral gap optimization would frequently delete inter-class edges but also add inter-class edges. Thus, mostly edge deletions can mitigate over-squashing and over-smoothing simultaneously.

Clearly, this argument relies on the specific distribution of labels. Other scenarios are analyzed in Appendix B to also highlight potential limitations of spectral rewiring that does not take node labels into account.

Following this argument, however, we could ask if the learning task only depends on the label distribution. The following proposition highlights why spectral gap optimization is justified beyond label distribution considerations.

**Proposition 3.4**.: _After one round of mean aggregation, the node features of \(^{+}\) are more informative compared to \(^{+}}\)._

Figure 2: We plot the MSE vs order of smoothing for our four synthetic graphs (2(a)), and for a real heterophilic dataset with the result of different rewiring algorithms to it: FoSR (Karhadkar et al., 2023) and ProxyAdd for adding (200 edges), and our ProxyDelete for deleting edges (5 edges) (2(b)). We find that deleting edges helps reduce over-smoothing, while still mitigating over-squashing via the spectral gap increase.

Note that \(}^{+}\) decreases the spectral gap, while \(^{+}\) increases it relative to \(\). However, the label configuration of \(^{+}}\) seems more advantageous because, for the changed nodes, the number of neighbors of the same class label remains in the majority in contrast to \(^{+}\). Still, the spectral gap increase seems to aid the learning task compared to the spectral gap decrease.

## 4 Braess-inspired graph rewiring

We introduce two algorithmic approaches to perform spectral rewiring. Our main proposal is computationally more efficient and more effective in spectral gap approximation than baselines, as we also showcase in Table 14. The other approach based on Eldan's Lemma is also analyzed, as it provides theoretical guarantees for edge deletions. However, it does not scale well to larger graphs.

Greedy approach to modify edges.Evaluating all potential subsets of edges that we could add or delete is computationally infeasible due to the combinatorially exploding number of possible candidates. Therefore, we resort to a Greedy approach, in which we add or delete a single edge iteratively. In every iteration, we rank candidate edges according to a proxy of the spectral gap change that would be induced by the considered rewiring operation, as described next.

### Graph rewiring with Proxy spectral gap updates

Update of eigenvalues and eigenvectors.Calculating the eigenvalues for every normalized graph Laplacian obtained by the inclusion or exclusion of a single edge would be a highly costly method. The ability to use the spectral gap directly as a criterion to rank edges requires a formula to efficiently estimate it for one edge flip. For this we resort to Matrix Perturbation Theory (Stewart and Sun, 1990; von Luxburg, 2007) to capture the change in eigenvalues and eigenvectors approximately. Our update scheme is similar to the proposal by Bojchevski and Gunnemann (2019) in the context of adversarial flips. The change in the eigenvalue and eigenvector for a single edge flip \((u,v)\) is given by

\[+ w_{u,v}((f_{u}-f_{v})^{2}-(f_{u}^{2 }+f_{v}^{2})),\] (1)

where \(\) is the initial eigenvalue; \(\{f_{u},f_{v}\}\) are entries of the leading eigenvector, \( w_{u,v}=1\) if we add an edge and \( w_{u,v}=-1\) if we delete an edge. Note that this proxy is only used to rank edges efficiently. After adding/deleting the top \(M\) edges (where \(M=1\) in our experiments), we update the eigenvector and the spectral gap by performing a few steps of power iteration. To this end, we initialize the function eigsh of the scipy sparse library in Python, which is based on the Implicitly Restarted Lanczos Method (Lehoucq et al., 1998), with our current estimate of the leading eigenvector. Both our resulting algorithms, ProxyDelete for deleting edges and ProxyAdd for adding edges, are detailed in Appendix C.

Time Complexity of ProxyDelete.The algorithm runs in \((N(||+s()))\) where \(N\) is the number of edges to delete, and \(s()\) denotes the complexity of the algorithm that updates the leading eigenvector and eigenvalue at the end of every iteration. In our setting, this requires a constant number of power method iterations, which is of complexity \(s()=O(||)\). Note that, because we choose to only delete one edge, the ranking does not need to be sorted to obtain its maximum. By having an \((1)\) proxy measure to score candidate edges, we are able to improve the overall runtime complexity from the original \((N|| s())\). Furthermore, even though this does not impact the asymptotic complexity, deleting edges instead of adding them makes every iteration run on a gradually smaller graph, which can further induce computational savings for the downstream task.

Time Complexity of ProxyAdd.The run time analysis consists of the same elements as the edge deletion algorithm. The key distinction is that the ranking is conducted on the complement of the graph's edges, \(\). Since the set of missing edges is usually larger than the existing edges in real world settings, to save computational overhead, it is possible to only sample a constant amount of edges. See Section F for empirical runtimes.

### Graph rewiring with Eldan's criterion

Lemma 3.1 states a sufficient condition for the Braess paradox. It naturally defines a scoring function of edges to rank them according to their potential to maximize the spectral gap based on the function \(g\). However, the computation of this ranking is significantly more expensive than other considered algorithms, as each scoring operation needs access to the leading eigenvector of the perturbed graph with an added or deleted edge. In case of edge deletions, we also need to approximate the spectral gap similar to our Proxy algorithms. As the involved projection \(_{f}\) is a dot product of eigenvectors, it requires \((||)\) operations. Even though this algorithm does not scale well to large graphs without focusing on a small random subset of candidate edges, we still consider it as baseline, as it defines a more conservative criterion to assess when we should stop deleting edges. The precise algorithms are stated in Appendix C.

### Approximation quality

To check whether the proposed edge modification algorithms are indeed effective in the spectral gap expansion, we conduct experiments on an Erdos-Renyi (ER) graph with \((||,||)=(30,58)\) in Figure 3. Our ideal baseline that scores each candidate with the correct spectral gap change would usually be computationally too expensive, because each edge scoring requires \(O(||)\) computations. For our small synthetic test bed, we still compute it to assess the approximation quality of the proposed algorithms, and of the competitive baseline FoSR (Karhadkar et al., 2023). For both edge additions (Figure 3(a)) and deletions (Figure 3(b)), we observe that the Proxy method outlined in Algorithm 1 usually leads to a better spectral expansion approximation. In addition, we report the spectral gaps that different methods obtain on real world data in Table 16 in the Appendix, which highlights that our proposals are consistently most effective in increasing the spectral gap.

## 5 Experiments

### Long Range Graph Benchmark

The Long Range Graph Benchmark (LRGB) was introduced by Dwivedi et al. (2023) specifically to create a test bed for over-squashing. We compare our proposed ProxyAdd and ProxyDelete methods with DRew (Gutteridge et al., 2023), a recently proposed strong baseline for addressing over-squashing using a GCN as our backbone architecture in Table 1. We adopt the experimental setting of Tonhoff et al. (2023), we adopt DRew baseline results from the original paper. We evaluate on the following datasets and tasks: 1) PascalVOC-SP - Semantic image segmentation as a node classification task operating on superpixel graphs. 2) Peptides-func - Peptides modeled as molecular

    & PascalVOC-SP & Peptides-Func & Peptides-Struct \\  & ( Test F1 \(\)) & (Test AP \(\)) & (Test MAE \(\)) \\  Baseline-GCN & 0.1268\(\)0.0060 & 0.5930\(\)0.0023 & 0.3496\(\)0.0013 \\ DRew+GCN & 0.1848\(\)0.0107 & **0.6996\(\)0.0076** & 0.2781\(\)0.0028 \\ FoSR+GCN & 0.2157\(\)0.0057 & 0.6526\(\)0.0014 & 0.2499\(\)0.0006 \\ ProxyAdd+GCN & **0.2213\(\)0.0011** & 0.6789\(\)0.0002 & **0.2465\(\)0.0004** \\ ProxyDelete+GCN & 0.2170\(\)0.0015 & 0.6908\(\)0.0007 & **0.2470\(\)0.0080** \\   

Table 1: Results on Long Range Graph Benchmark datasets.

Figure 3: We instantiate a toy ER graph with 30 nodes and 58 edges. We compare FoSR (Karhadkar et al., 2023), our proxy spectral gap based methods, and our Eldan’s criterion based edge methods.

graphs. The task is graph classification. 3) Peptides-struct - Peptides modeled as molecular graphs. The task is to predict various molecular properties, hence a graph regression task.

The top performance is highlighted in bold. Evidently, our proposed rewiring methods outperform DRew (Gutteridge et al., 2023) and FoSR (Karhadkar et al., 2023) on PascalVOC and Peptides-struct, and achieves comparable performance on Peptides-func.

In addition, Table 10 in the appendix compares different rewiring strategies for node classification on other commonly used datasets and graph classification (SSE.2) for adding edges, since FoSR (Karhadkar et al., 2023) was primarily tested on this task.

Node classification on large heterophilic datasets.Platonov et al. (2023) point out that most progress on heterophilic datasets is unreliable since many of the used datasets have drawbacks, including duplicate nodes in Chameleon and Squirrel datasets, which lead to train-test data leakage. The sizes of the small graph sizes also lead to high variance in the obtained accuracies. Consequently, we also test our proposed algorithms on 3/5 of their newly introduced larger datasets and use GCN (Kipf and Welling, 2017) and GAT (Velickovic et al., 2018) as our backbone architectures. As a higher depth potentially increases over-smoothing, we also analyze how our methods fares with varied number of layers. To that end, we adopt the code base and experimental setup of Platonov et al. (2023); the datasets are divided into 50/25/25 split for train/test/validation respectively. The test accuracy is reported as an average over 10 runs. To facilitate training deeper models, skip connections and layer normalization are employed. We compare FoSR (Karhadkar et al., 2023) and our proposals based on the Eldan criterion as well as ProxyAdd and ProxyDelete in Tables 2,3,4. The top performance is highlighted in **bold**. Evidently, for increasing depth, even though the GNN performance should degrade because of over-smoothing, we achieve a significant boost in accuracy compared to baselines, which we attribute to the fact that our methods delete inter-class edges --thus slowing down detrimental smoothing.

  Method & \#BiggsModel & Accuracy & \#EdgeDefined & Accuracy & Logress & Method & \#Edge-Attached & Accuracy & \#BiggsModel & Test ROC &  \\  GCN & - & 47.208.33 & - & 47.208.33 & 10 & GCN & - & 88.574.04 & - & 88.573.64 & 10 \\ GCN-FoSR & 25 & 49.684.73 & - & 10 & GCN-FS-FS-10 & 50 & 90.150.55 & - & - & 10 \\ GCN-Ekhan & 25 & 47.148.99 & 100 & **80.158.90** & 10 & GCN-Ekhan & 100 & **90.116.00** & 50 & 89.499.60 & 10 \\ GCN-ProxyGo & 10 & 47.274.04 & 50 & **90.158.46** & 10 & GCN-ProxyGo & 20 & **89.50.50** & 20 & 89.574.09 & 10 \\  GAT & - & 47.430.44 & - & 47.430.44 & 10 & GAT & 9.660.04 & - & 93.600.64 & 10 \\ GAT-FSR & 25 & 51.460.42 & 50 & **51.460.47** & 100 & GAT-FSR & 100 & 93.440.41 & 10 \\ GAT-ProxyGo & 20 & 49.685.92 & 100 & **51.520.30** & 10 & GAT-ProxyGo & 20 & 93.604.09 & 20 & **93.604.04** & 10 \\  GCN & - & 47.320.39 & - & 47.320.59 & 20 & GCN & - & 87.4110.65 & - & 87.4110.65 & 20 \\ GCN-FoSR & 100 & 49.754.93 & - & 20 & GCN-Swift & 100 & **93.604.05** & 100 & 20 \\ GCN-ProxyGo & 50 & **49.684.31** & 20 & 48.321.76 & 20 & GCN-ProxyGo & 100 & **93.604.05** & 10 \\ GCN-ProxyGo & 50 & 49.480.59 & 50 & **49.384.93** & 20 & GCN-ProxyGo & 20 & **89.446.50** & 50 & 89.384.00 & 20 \\  GAT & - & 47.320.46 & - & 47.314.66 & 20 & GAT & - & 99.295.02 & - & 93.294.52 & 20 \\ GAT-FoSR & 100 & 31.314.04 & - & 47.314.66 & 20 & GAT-FSR & 50 & 93.650.64 & - & 20 \\ GAT-Enkhan & 20 & 51.400.36 & 20 & **51.460.44** & 20 & GAT-Elain & 10 & 93.294.04 & 20 & **95.486.64** & 20 \\ GAT-ProxyGo & 50 & 47.534.90 & 20 & **51.980.66** & 20 & GAT-PrutyGo & 20 & **94.896.67** & 20 & 94.649.81 & 20 \\  

Table 3: Node classification on Amazon-Ratings.

  Method & \#BiggsModel & Accuracy & \#EdgeDefined & Accuracy & Logress \\  GCN & - & 37.300.73 & - & 70.300.73 & - \\ GCN-FeSR & 50 & 73.641.11 & - & 5 & 5 \\ GCN-NetNet & 50 & 73.741.63 & 5 & **74.613.73** & 5 \\ GCN-ProxyGo & 50 & 77.548.74 & 20 & 77.540.68 & 5 \\ GAT & 80.899.20 & - & 60.890.70 & 5 \\ GAT-FSR & 50 & 81.841.00 & 50 & 100 & 52.120.60 & 5 \\ GAT-Enkhan & 50 & **80.894.60** & 20 & **80.600.03** & 5 \\ GCN & 60.899.27 & - & 68.980.77 & 10 & GCN-ProxyGo & 10 \\ GCN-NetNet & 50 & 7Pruning for graph lottery tickets.In Sections 8.3 and 8.5, we have shown that graph pruning can improve generalization, mitigate over-squashing and also help slow down the rate of smoothing. Can we also use our insights to find lottery tickets (Frankle and Carbin, 2019)?

To what degree is graph pruning feature data dependent?The first extension of the Lottery Ticket Hypothesis to GNNs, called Unified Graph Sparsification (UGS) (Chen et al., 2021), prunes connections in the adjacency matrix and model weights that are deemed less important for a prediction task. Note that UGS relies on information that is obtained in computationally intensive prune-train cycles that take into account the data and the associated masks. In the context of GNNs, the input graph plays a central role in determining a model's performance at a downstream task. Naively pruning the adjacency matrix without characterizing what constitutes _important edges_ is a pitfall we would want to avoid (Hui et al., 2023), yet resorting to expensive train-prune-rewind cycles to identify importance is also undesirable. This brings forth the questions: To what extent does the pruning criterion need to depend on the data? Is it possible to formulate a data/feature agnostic pruning criterion that optimizes a more general underlying principle to find lottery tickets? Morcos et al. (2019) and Chen et al. (2020) show, in the context of computer vision and natural language processing respectively, that lottery tickets can have universal properties that can even provably (Burkholz et al., 2022) transfer to related tasks.

Lottery tickets that rely on the spectral gap.However, even specialized structures need to maintain and promote information flow through their connections. This fact has inspired works like Pal et al. (2022); Hoang et al. (2023) to analyze how well lottery ticket pruning algorithms maintain the Ramanujan graph property of bipartite graphs, which is intrinsically related to the Cheeger constant and thus the spectral gap. They have further shown that rejecting pruning steps that would destroy a proxy of this property can positively impact the training process.

In the context of GNNs, we show that we can base the graph pruning decision even entirely on the spectral gap, but rely on a computationally cheaper approach to obtain a proxy. By replacing the magnitude pruning criterion for the graph with the Eldan criterion and ProxyDelete to prune edges, in principle, we can avoid the need for additional data features and labels. This has the advantage that we could also prune the graph at initialization and thus benefit from the computational savings from the start. We use our proposed methods to prune the graph at initialization to the requisite sparsity level and then feed it to the GNN where the weights are pruned in an iterative manner. Our results are presented in Table 18, where we compare IMP based UGS (Chen et al., 2021) with our methods for different graph and weight sparsity levels. Note that, even though our method does not take any feature information into account and prunes purely based on the graph structure, our results are comparable. For datasets like Pubmed, we even slightly outperform the baseline. Table 5 shows results for jointly pruning the graph and parameter weights, which leads to better results due to potential positive effects of overparameterization on training (Gadhikar and Burkholz, 2024).

Stopping criterion.The advantage of using spectral gap based pruning (especially the Eldan criterion) is patent: It helps identify problematic edges that cause information bottlenecks and provides a framework to prune those edges. Unlike UGS, our proposed framework also has the advantage that we can couple the overall pruning scheme with a stopping criterion that follows naturally from our setup. We stop pruning the input graph when no available edges satisfy our criterion anymore.

## 6 Conclusion

Our work connects two seemingly distinct branches of the literature on GNNs: rewiring graphs to mitigate over-squashing and pruning graphs for lottery tickets to save computational resources.

   Method &  &  &  \\  Metrics & GS & WS & Acc & GS & WS & Acc & GS & WS & Acc \\  UGS & 79.85\% & 97.86\% & 68.46\(\)1.89 & 78.10\% & 97.50\% & **66.50\(\)0.60** & 68.67\% & 94.52\% & 76.90\(\)1.83 \\ EldanDelete-UGS & 79.70\% & 97.31\% & 68.73\(\)0.01 & 77.84\% & 96.78\% & 64.60\(\)0.00 & 70.11\% & 93.17\% & **78.00\(\)0.42** \\ ProxyDelete-UGS & 78.81\% & 97.24\% & **69.26\(\)0.63** & 77.50\% & 95.83\% & 65.43\(\)0.60 & 78.81\% & 97.24\% & 75.25\(\)0.25 \\   

Table 5: Pruning for lottery tickets comparing UGS to our EldanDelete pruning and our ProxyDelete pruning. We report Graph Sparsity (GS), Weight Sparsity (WS), and Accuracy (Acc).

Contributing to the first branch, we highlight that, contrary to the standard rewiring practice, not only adding but also pruning edges can increase the spectral gap of a graph exploiting the Braess paradox. By providing a minimal example, we prove that this way it is possible to address over-squashing and over-smoothing simultaneously. Experiments on large-scale heterophilic graphs confirm the practical utility of this insight. Contributing to the second branch, these results explain how pruning graphs moderately can improve the generalization performance of GNNs, in particular for heterophilic learning tasks. To utilize these insights, we have proposed a computationally efficient graph rewiring framework, which also induces a competitive approach to prune graphs for lottery tickets at initialization.