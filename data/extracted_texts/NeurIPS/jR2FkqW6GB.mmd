# Is Learning in Games Good for the Learners?

William Brown

Columbia University

w.brown@columbia.edu &Jon Schneider

Google Research

jschnei@google.com &Kiran Vodrahalli

Google Research

kirannv@google.com

###### Abstract

We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of _generalized equilibrium_ which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of _Stackelberg_ equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeoffs in terms of the opponent's algorithm choice. We give a tight characterization for the maximal reward obtainable against _some_ no-regret learner, yet we also show a class of games in which this is bounded away from the value obtainable against the class of common "mean-based" no-regret algorithms. Finally, we consider the question of learning reward-optimal strategies via repeated play with a no-regret agent when the game is initially unknown. Again we show tradeoffs depending on the opponent's learning algorithm: the Stackelberg strategy is learnable in exponential time with any no-regret agent (and in polynomial time with any no-_adaptive_-regret agent) for any game where it is learnable via queries, and there are games where it is learnable in polynomial time against any no-swap-regret agent but requires exponential time against a mean-based no-regret agent.

## 1 Introduction

How should two rational agents play a repeated, possibly unknown, game against one another? One natural answer - barring any knowledge of the game, or the capacity to compute potentially computationally intractable equilibria - is that they should employ some sort of learning algorithm to learn how to play over time. Indeed, there is a vast literature which studies what happens when all the players in a repeated game run (some specific type of) learning algorithms to select their actions. For example, when all players in a game simultaneously run no-(swap)-regret learning algorithms, it is known that the average strategy profile of the learners converges to a (coarse) correlated equilibrium [(23; 11; 16)]. More recent works have studied how to design algorithms that converge to these equilibria at faster rates [(7; 1; 8)], performance guarantees of such equilibria compared to the optimal possible welfare [(4; 17)], and the specific dynamics of such learning algorithms [(9; 20; 22)].

In contrast, relatively little attention has been devoted to whether it is actually _in the interest of these agents_ to run these specific learning algorithms. For example, in a setting where all agents are running no-swap-regret learning algorithms, when can an agent significantly benefit by deviating and running a different type of algorithm? And if they can, what algorithm should the agent deviate to?

### Our results

We explore the following questions (and others) in the case where two agents repeatedly play a normal-form, general-sum game for \(T\) rounds.

When does reward trade off with regret?While maximizing reward is often viewed as the objective of regret minimization against arbitrary adversaries, tensions may emerge when playing against another learning agent, as one's choice of actions has a causal effect on future loss functions as determined by the opponent's algorithm. In such cases, as we discuss further below, it turns out that demanding a stronger regret guarantee (e.g. asking for no-swap-regret instead of no-external-regret) may ultimately result in lower reward for an agent. To analyze these tradeoffs, in Section 2 we introduce a notion of _generalized \((_{A},_{B})\)-equilibrium_, where \(_{A}\) and \(_{B}\) are the sets of "deviation strategies" over which players \(A\) and \(B\) minimize regret (e.g. fixed actions or swap functions), presented as an asymmetric extension of the linear \(\)-equilibria considered by (13). Each pair of strategy sets \((_{A},_{B})\) generates a polytope of \((_{A},_{B})\)-equilibria, where any point then yields a reward value for each player. We show that all such points are feasible: for any game and any \((_{A},_{B})\)-equilibrium \(\), there is a pair of algorithms which converge to \(\) while maintaining their respective \(_{A}\)-regret and \(_{B}\)-regret guarantees against arbitrary opponents. Further, deviating to a strategy with fewer constraints \(_{A}\) can often result in _strictly_ improved reward for player \(A\). As a concrete application, we consider the question of deviating from simultaneous no-swap-regret play.

When is no-swap-regret play a stable equilibrium?What should you do if you know that your opponent in a repeated game is running a no-swap-regret algorithm to select their actions? In (10), the authors show that one utility-optimizing response (up to additive \(o(T)\) factors) is to play a static (mixed) strategy (your _Stackelberg strategy_) and obtain the _Stackelberg value_ of the game. However, determining your Stackelberg strategy requires some knowledge of the game, and acquiring this knowledge from repeated play may be difficult (we address the latter issue in Section 5). In comparison, it is relatively straightforward to also run a no-swap-regret learning algorithm. This begs the question: are there games where you obtain significantly less utility (i.e., at least \((T)\) less utility) by running a no-swap-regret learning algorithm instead of playing your Stackelberg strategy?

We show that the answer is _yes_, such games exist and are relatively common. In fact, we provide an efficient algorithmic characterization of the games \(G\) for which both players playing a no-swap-regret learning algorithm is an (\(o(T)\)-)approximate Nash equilibrium of the entire repeated "meta-game". The exact characterization is presented in Section 3 and is somewhat subtle, e.g., there are slightly different characterizations depending on whether we insist all pairs of no-swap-regret algorithms lead to approximate equilibria or only one specific pair, corresponding to best-case and worst-case values in the generalized equilibrium polytope. One consequence of both characterizations, however, is that for _almost all_ games (in a measure-theoretic sense, considering arbitrarily small perturbations), in order for it to be an approximate equilibrium for both players to play a low-swap regret strategy, the game \(G\) must possess a _pure_ Nash equilibrium. That is, in any game without a pure Nash equilibrium, it is possible for at least one of the parties to do significantly better by switching from no-swap-regret learning to playing their Stackelberg strategy.

Finally, we additionally show there are some games where playing a no-(external)-regret algorithm against another no-swap-regret learner weakly dominates playing a no-swap-regret algorithm, regardless of the specific choice of algorithms. This counters the intuition that stronger regret guarantees protect a player from worse outcomes.

Optimizing reward against no-regret learners.What if our opponent is not running a no-swap-regret algorithm, but simply a no-(external)-regret algorithm? In this case, it is still possible to obtain at least the Stackelberg value of the game by playing our Stackelberg strategy (no-regret algorithms are also guaranteed to eventually learn and play the best response to this strategy). However, unlike in the no-swap-regret setting, there exist specific games and no-regret algorithms where it is possible to obtain _significantly_ (\((T)\)) more than the Stackelberg value by playing a specific dynamic strategy. This phenomenon was first observed in (10), where a specific game is given for which it is possible to obtain \((T)\) more than Stackelberg when playing against any no-regret algorithm in the family of _mean-based learning algorithms_ (including algorithms such as multiplicative weights and EXP3). However, many questions remain unanswered, such as understanding in which games it is possible to outperform playing one's Stackelberg strategy, and by how much.

In Section 4 we present some answers to these questions for the case of generic no-regret algorithms. Specifically, we first show that if player \(B\) is running (any) no-regret algorithm, the utility of player \(A\) (regardless of what strategy they employ) is upper bounded by \(_{A}(,) T+o(T)\), where \(_{A}(,)\) is what we call the the _unconstrained-external (equilibrium) value_ of the game for player \(A\), which is given by the solution to a linear program. We then show that this upper bound is asymptotically tight: there exists a no-regret algorithm \(\) such that if player \(B\) is playing according to \(\), then the player \(A\) can obtain utility at least \(_{A}(,) T-o(T)\) by playing an appropriate strategy in response.

Note that this characterization does not completely resolve the question of (10) - it requires the construction of a fairly specific no-regret algorithm \(\), and it is still open what is possible against specific (classes of) no-regret algorithms (e.g., multiplicative weights or mean-based algorithms). In fact, we show a property of games which, when satisfied, implies that it is impossible to obtain the unconstrained-external value against a mean-based learner.

Learning the Stackelberg strategy through repeated play.Finally, we address the question of how hard it is to actually identify the Stackelberg strategy in a game against a learning opponent. Given full knowledge of the game \(G\), finding an agent's Stackelberg strategy is simply a computational problem which turns out to be efficiently solvable by solving several small LPs (see (6)). However, if the game (in particular, the opponent's reward matrix) is unknown, an agent must learn the Stackelberg strategy over time. Existing work on learning Stackelberg strategies (e.g., [19; 24]) generally assumes access to a _best-response oracle_ for the game (i.e., for a specific mixed strategy, how will an opponent best-respond?). In contrast, if our opponent is playing a specific no-regret learning algorithm, they may not immediately best respond to the strategies we play! This raises the following two questions. First, when is it possible to learn the Stackelberg equilibrium of a game while playing against a learning opponent? Second, is it easier to learn this equilibrium when playing against certain classes of learning algorithms?

In Section 5 we begin by showing that it is indeed possible to convert any best-response query algorithm for finding Stackelberg equilibria via best-response queries to an adaptive strategy that learns Stackelberg equilibria via repeated play against a generic no-regret learner, albeit potentially at the cost of an exponential blow-up in the number of rounds, e.g. for a query algorithm which makes \(Q\) best-response queries, simulating it against a no-regret learner may require \(T=(Q)\) rounds of play. For the special case of opponents with no-_adaptive_-regret algorithms (such as online gradient descent), we show that only \(T=(Q)\) rounds are required in the worst case. However, in general we show that exponential runtime can be necessary. In particular, we give an example of a game with \(M\) actions where it is possible to learn the Stackelberg equilibrium in \((M)\) rounds when playing against any no-swap-regret learning algorithm, but where it requires at least \((M)\) rounds to learn this equilibrium when playing a mean-based no-regret algorithms.

### Related work

The broader literature on no-regret learning in repeated games is substantial, covering many equilibrium convergence results varying assumptions. A recent line of work [5; 10; 21] considers problems related to optimizing one's reward when competing against a no-regret learner in a game. We extend these questions to consider the relationship and regret for an optimizer, as well as to settings where properties of the game are initially unknown, and give a series of separation results in terms of various notions of equilibrium. Also relevant is the literature on analysis of no-regret trajectory dynamics, and in particular  which shows a game in which no-regret dynamics outperform the reward of the Nash equilibrium. Additionally, there is also prior work considering regret minimization problems involving either best-responding or otherwise strategic agents (see e.g. [3; 18]), as well as work considering alternate regret notions or behavior models for repeated Stackelberg games (e.g. [12; 15]).

### Notation and preliminaries

Throughout, we consider two-player bimatrix games \(G=(A,B)\), where player \(A\) ("the optimizer") has action set \(=\{a_{1},,a_{M}\}\) and player \(B\) ("the learner") has action set \(=\{b_{1},,b_{N}\}\). When the optimizer plays action \(a_{i}\) and the learner plays action \(b_{j}\), the players receive rewards \(u_{A}(a_{i},b_{j})\) and \(u_{B}(a_{i},b_{j})\), respectively. We assume that the magnitude of each utility is bounded by a constant. The sets of mixed strategies for each player are denoted by \(()\) and \((B)\), respectively;when the optimizer plays a mixed strategy \(()\) and the learner plays \(()\), the expected reward for the optimizer is given by \(u_{A}(,)=_{i=1}^{M}_{j=1}^{N}_{i}_{j}u_{A}(a_{i}, b_{j})\), with \(u_{B}(,)\) defined analogously. An action \(b\) is a _best response_ to a strategy \(()\) if \(b_{b^{}}u_{B}(,b^{})\). We let \(()\) be the set of all such actions for player \(B\), and likewise \(()\) for player \(A\).

## 2 Generalized equilibria and no-\(\)-regret learning

Here we introduce the notions of \(\)-regret and generalized equilibria, which we use to analyze the regret and reward of players in repeated bimatrix games under varying assumptions regarding the choice of regret benchmarks, the algorithms used, and the structure of the game.

Originally introduced in (14) and extended to general convex settings in (13), we consider the formulation of _linear_\(\)-regret as it relates to bimatrix games. Given a sequence of action pairs \((a_{i_{1}}b_{j_{1}}),,(a_{i_{T}}b_{j_{T}})\) for \(T>0\) and some set of functions \(\), where each \(f\) maps actions \(\) to action profiles in \(()\), we say that the _\(\)-regret_ for the optimizer (and analogously for the learner) is

\[_{}(T)=_{f}_{t=1}^{T}u_{A}(f(a_{i_{t}}),b_{j_{t}}) -u_{A}(a_{i_{t}},b_{j_{t}}).\]

**Definition 1** (No-\(\)-regret learning).: _We say a learning algorithm \(\) is a no-\(\)-regret algorithm if, for some constant \(c<1\), we have that \(_{}(,T)=O(T^{c})\), where \(_{}(,T)\) is the \(\)-regret corresponding to the action sequence played by \(\)._

Some notable sets of regret comparator functions \(\) are the constant maps \(\) (corresponding to _external regret_), where all input actions are mapped to the same output action, and the "swap functions" \(\) (corresponding to _internal regret_1), which contain all single swap maps \(f_{ij}:[M][M]\) where \(f(i)=j\) and \(f(i^{})=i^{}\) for \(i^{} i\). Imposing these constraints on players in a game results in a _(coarse) correlated equilibrium_, which are instances of our notion of _generalized equilibrium_.

**Definition 2** (Generalized \((_{A},_{B})\)-equilibria).: _A \((_{A},_{B})\)-equilibrium\(()\) in a two-player game is a joint distribution over action profiles \((a,b)\) such that player \(A\) cannot increase their expected reward by deviating with some strategy in \(_{A}\) and player \(B\) cannot benefit by deviating with some strategy in \(_{B}\)._

In contrast to the \(\)-equilibria considered by (14; 13), here we allow constraints to be asymmetric between players. While many equilibrium notions for two-player games impose symmetric regret constraints on each player (e.g. Nash, correlated, and coarse correlated equilibria), this need not always be the case. In Section 3, we highlight Stackelberg equilibria as a motivating example for considering more general notions of asymmetric equilibria from the perspective of \(\)-regret, to determine when one should deviate from simultaneous no-swap play, and in Section 4 we characterize the maximum reward attainable against no-regret learners in terms of asymmetric equilibria.

We say that the _value_ of a game \(G\) for player \(A\) of a certain equilibrium class \((_{A},_{B})\), denoted \(_{A}(_{A},_{B})\) is the maximum reward obtainable by player \(A\) at some \((_{A},_{B})\)-equilibrium (with \(_{B}(_{A},_{B})\) defined symmetrically for player \(B\)). Likewise, we say that the _min-value_ of a game for a player and an equilibrium class, denoted by e.g. \(_{A}(_{A},_{B})\) for player \(A\), is the minimum reward for a player over all \((_{A},_{B})\)-equilibria in a game. These capture the range of feasible average rewards under repeated play via \((_{A},_{B})\)-regret dynamics.

**Proposition 1**.: _For a repeated game over \(T\) rounds where player \(A\) uses a no-\(_{A}\)-regret algorithm and player \(B\) uses a no-\(_{B}\)-regret algorithm, the average rewards obtained by each player are upper bounded by \(_{A}(_{A},_{B})+o(1)\) and \(_{B}(_{A},_{B})+o(1)\), respectively, and lower bounded by \(_{A}(_{A},_{B})-o(1)\) and \(_{B}(_{A},_{B})-o(1)\)._

We consider an \(\)-approximate \((_{A},_{B})\)-equilibrium to be a joint profile distribution where each constraint is satisfied up to additive error \(\), connecting Definitions 1 and 2 as follows.

**Proposition 2** (Convergence of no-\(\)-regret dynamics to generalized equilibrium).: _Suppose after \(T\) rounds of a game where player \(A\) plays a no-\(_{A}\)-regret algorithm and player \(B\) plays a no-\(_{B}\)-regret algorithm, player \(A\) has average \(_{A}\)-regret \(\) and player \(B\) has average \(_{B}\)-regret \(\). Let\(^{t}:=p_{A}^{t} p_{B}^{t}\) denote the joint distribution over both players' actions at time \(t\) and \(:=_{t=1}^{T}^{t}\) denote the time-averaged history over joint player action distributions. Then, \(\) is an \(\)-approximate \((_{A},_{B})\)-equilibrium where_

\[}_{(a,b)}[u_{A}(a,b)] }_{(a,b)}[u_{A}(f_{A}(a),b )]-,\] \[}_{(a,b)}[u_{B}(a,b)] }_{(a,b)}[u_{B}(a,f_{B}(b) )]-\]

_for every possible deviation \(f_{A}_{A},f_{B}_{B}\). Likewise, if players \(A\) and \(B\) repeatedly play strategies corresponding to an \((_{A},_{B})\)-equilibrium, then player \(A\) is no-\(_{A}\)-regret and player \(B\) is no-\(_{B}\)-regret._

A general construction for no-\(\)-regret algorithms is given in (13), which immediately implies feasibility of dynamics which converge to _some_ instance of any class of \((_{A},_{B})\)-equilibria in a game, possibly requiring agents to use differing algorithms if constraints are asymmetric. We show that a much stronger claim is true: such a pair of algorithms exists for _any_\((_{A},_{B})\)-equilibrium \(\) in a game. These algorithms also satisfy a "best-of-both-worlds" property, in that they converge to \(\) when played together, yet simultaneously maintain their corresponding regret guarantees against arbitrary adversaries.

**Theorem 1**.: _Consider any game \(G\). Suppose there exists a no-\(_{A}\)-regret learning algorithm \(_{A}\) and a no-\(_{B}\)-regret learning algorithm \(_{B}\). For any particular \((_{A},_{B})\)-equilibrium \(\) in a game \(G\), there exists a pair of learning algorithms \((_{A}^{}(),_{B}^{}())\) such that:_

* _The empirical sequence of play when Player_ \(A\) _uses_ \(_{A}^{}()\) _and Player_ \(B\) _uses_ \(_{B}^{}()\) _converges to_ \(\)_._
* \(_{A}^{}()\) _and_ \(_{B}^{}()\) _are no-_\(_{A}\)_-regret and no-_\(_{B}\)_-regret, respectively, against arbitrary adversaries._

Our approach is for the algorithms to initially implement a schedule of strategies which converges to \(\). Yet, these algorithms also detect when their opponent disobeys the schedule by tracking their \(\)-regret with respect to \(\), and after \(o(T)\) violations can deviate indefinitely to playing a standalone no-\(\)-algorithm for all remaining rounds. Several of our results throughout make use of Theorem 1. Here we state a notable immediate implication for equilibrium selection.

**Corollary 1.1**.: _For any equilibrium scoring function \(:()\) with a unique optimum computable in finite time, there exists a pair of learning algorithms \((_{A}^{},_{B}^{})\) such that:_

* _The empirical distribution when player_ \(A\) _uses_ \(_{A}^{}\) _and player_ \(B\) _uses_ \(_{B}^{}\) _converges to_ \(_{}()\)_._
* \(_{A}^{}\) _and_ \(_{B}^{}\) _are no-_\(_{A}\)_-regret and no-_\(_{B}\)_-regret, respectively, against arbitrary adversaries._

Proof.: First optimize \(\) over \(\) in finite time to find the unique optimum; then apply Theorem 1 to the resulting desired equilibrium. 

Corollary 1.1 allows for optimizing for objectives such as total welfare or min-max utility for both players, and imposing conditions on generalized equilibria beyond \(\)-regret constraints (e.g. product constraints for Nash equilibria) by assigning arbitrarily low scores to invalid strategy profiles.

In subsequent sections, we will primarily focus the function classes \(\) and \(\) corresponding to external and internal regret as mentioned above, as the well empty set \(\) corresponding to unconstrained regret, and we additionally will consider the case when the game \(G\) is initially unknown. Before continuing, we note that each player's values for any \((_{A},_{B})\)-equilibrium class can be expressed via a linear program, whose size is polynomial in the game dimensions for these function classes of interest.

**Proposition 3**.: _For any game \(G\) and constraints \((_{A},_{B})\), both \(_{A}(_{A},_{B})\) and \(_{B}(_{A},_{B})\) are computable via linear programs with \(MN\) variables and \((M,N,|_{A}|,|_{B}|)\) constraints. When \(_{A}\) and \(_{B}\) belong to \(\{,,\}\), the number of constraints is \((M,N)\)._

In general, these values for a player may differ under distinct notions of generalized equilibria; we give several examples of such concrete value separations in Appendix A (in Theorem 8). Our results in Section 3 illustrate a particularly stark separation of this form, in which it can often be _dominant_ to deviate to a strategy where \(\)-regret constraints are violated.

Stability of no-swap-regret play

Here we address the following question: when is it the case that for two players in a game, it is an approximate (Nash) equilibrium for both players to play no-swap-regret strategies? More specifically, imagine a "metagame" where at the beginning of this repeated game, both players simultaneously announce and commit to a specific adaptive (and possibly randomized) algorithm they intend to run to select actions to play in the repeated game \(G\) for the next \(T\) rounds. In this metagame, for which games \(G\) is it an \(o(T)\)-approximate Nash equilibrium for both players to play a no-swap-regret learning algorithm?

Of course, the answer to this question might depend on _which_ specific no-swap-regret learning algorithm the agents declare. We therefore attempt to understand the following two questions:

* **Necessity:** For which games \(G\) is it the case that there exists _some_ pair of no-swap-regret algorithms which form a \(o(T)\)-approximate Nash equilibrium? (Equivalently, when is it _never_ the case that playing no-swap-regret algorithms forms an approximate Nash equilibrium?)
* **Sufficiency:** For which games \(G\) is it the case that _all_ pairs of no-swap regret algorithms form \(o(T)\)-approximate Nash equilibria?

A central element of our analysis will be to consider the _Stackelberg equilibria_ of a game.

**Definition 3** (Stackelberg Equilibria).: _The Stackelberg equilibrium of a game \(G\) for player \(A\) is the pair of strategies \((,b)\) given by \(_{(),\;b( )}u_{A}(,b)\), and the resulting expected utility for player \(A\) is the Stackelberg value of the game, denoted \(_{A}\). \(_{B}\) is defined symmetrically._

We can relate Stackelberg equilibria to our notions of generalized equilibria.

**Proposition 4**.: _For any game \(G\), we have that \(_{A}=_{A}(,)\)._

Here, any joint distribution over action profiles where player \(B\) has zero swap regret constitutes a \((,)\)-equilibrium for a game, and the optimal value for such an equilibrium for player \(A\) coincides with the Stackelberg value. Further, each equilibrium set can be optimized over via a linear program.

Note that each value definition allows for tiebreaking in favor of player \(A\). In general, simply playing the Stackelberg strategy \(\) may not suffice to obtain \(_{A}\) if the best response for Player \(B\) is not unique. However, there are a number of mild conditions which are each sufficient to ensure the existence of an approximate Stackelberg strategy \(^{}\) which yields a unique best response for player \(B\) and obtains \(_{A}-\) for any \(>0\). Here we consider a minimal such condition (essentially, no action is weakly dominated without also being strictly dominated).

**Assumption 1**.: _In a game \(G\), for each \(b\), either \(()=\{b\}\) for some \(\), or \(b()\) for all \(\). Likewise, for each \(a\), either \(()=\{a\}\) for some \(\), or \(a()\) for all \(\)._

We provide an efficient algorithmic procedure to answer both questions of necessity and sufficiency for a specific game \(G\) satisfying Assumption 1. To do this, recall that when two players both employ no-swap regret strategies, they asymptotically (time-average) converge to some correlated equilibrium (here, corresponding to an \((,)\)-equilibrium). On the other hand, by defecting from playing a no-swap regret strategy (while the other player continues playing their no-swap regret strategy), a player can guarantee their Stackelberg value for the game. Moreover, as shown by (10), this is the _optimal_ (up to \(o(T)\) additive factors) best response to an opponent running a no-swap regret strategy. It thus suffices to understand how the utility a player might receive under a correlated equilibrium compares to the utility they receive under their Stackelberg strategy. For a fixed game \(G\), let \(_{A}=_{A}(,)\) be the Stackelberg value for the first player, and \(_{B}=_{B}(,)\) be the Stackelberg value for the second player. We have the following theorem.

**Theorem 2**.: _Fix a game \(G\) satisfying Assumption 1. The following two statements hold:_

1. _There exists some pair of no-swap-regret algorithms that form an_ \(o(T)\)_-approximate Nash equilibrium in the metagame iff there exists a correlated equilibrium_ \(\) _in_ \(G\) _such that_ \(u_{A}()=_{A}\) _and_ \(u_{B}()=_{B}\)_._
2. _Any pair of no-swap-regret algorithms form an_ \(o(T)\)_-approximate Nash equilibrium in the metagame iff for all correlated equilibria_ \(\) _in_ \(G\)_,_ \(u_{A}()=_{A}\) _and_ \(u_{B}()=_{B}\)_Moreover, given a game \(G\), it is possible to efficiently (in polynomial time in the size of \(G\)) check whether each of the above cases holds._

We obtain both claims by leveraging the construction in Theorem 1: best-case and worst-case correlated equilibria are feasible by some pair of no-swap-regret algorithms, and both players must simultaneously achieve close to their Stackelberg value for deviating to not be preferable. The characterization in Theorem 2 is algorithmically useful, but sheds little direct light on in which games or how often we would expect playing no-swap-regret to be an approximate equilibrium. It turns out that for many games, playing no-swap-regret is _not_ an equilibrium; below we will show that for almost all games, if \(G\) does not have a pure Nash equilibrium, at least one player has an incentive to deviate to their Stackelberg strategy.

**Definition 4**.: _A property \(P\) of a game holds for almost all games if, given any game \(G\), property \(P\) holds with probability \(1\) for the game \(G^{}\) formed by starting with \(G\) and perturbing each of the entries \(u_{A}(a_{i},b_{j})\) and \(u_{B}(a_{i},b_{j})\) by independent uniform random variables in the range \([-,]\) (for any choice of \(\)). In other words, the property holds for almost all choices of the \(2MN\) utility values that define a game (with respect to the standard measure on this space)._

**Theorem 3**.: _For almost all games \(G\), if \(G\) does not have a pure Nash equilibrium, then there does not exist a pair of no-swap-regret algorithms which form a \(o(T)\)-approximate Nash equilibrium in the metagame for \(G\)._

Sketch.: We can show that if a correlated equilibrium has the same utility for a player as their Stackelberg value (a consequence of Theorem 2), then the correlated equilibrium must be a convex combination of valid Stackelberg equilibria. In almost all games, both players have unique Stackelberg equilibria (and Assumption 1 holds), which implies that this correlated equilibrium must actually be the Stackelberg strategy for both players simultaneously. This implies that it is a pure Nash equilibrium (since one action in a generic Stackelberg equilibrium is always pure). 

Note that although Theorem 3 holds for almost all games, there are some important classes of games (most notably, zero-sum games) in the measure zero subset omitted by this theorem statement that both a) do not have pure Nash equilibria and b) have the property that playing no-swap-regret algorithms is an approximate equilibrium in the metagame (in particular, for zero-sum games, the Stackelberg value collapses to the value of the unique Nash equilibrium). Still, Theorem 3 shows that there are very wide classes of games for which playing no-swap-regret algorithms is not stable from the perspective of the agents.

Finally Theorem 3 requires that we deviate to our Stackelberg strategy, which may be hard to compute. One can ask whether there are games where efficient deviations - e.g., to algorithms with _weaker_ regret guarantees - lead to strictly more utility for the deviating player. In Appendix B we show this is true in the following sense: there are games \(G\) where \(_{A}(,)>_{A}(, )=_{B}(,)\). That is, in such a game player \(A\) can possibly strictly increase their utility by switching to a low-external-regret strategy, and such a switch will never decrease their utility.

## 4 Optimal rewards against no-regret learners

Here, we characterize the feasibility of optimizing one's reward against no-(external)-regret learners in terms of generalized equilibria. In contrast to the case of no-swap-regret learners, as shown by (10) there are games in which one can obtain \((T)\) more than the Stackelberg value over \(T\) rounds against certain no-regret algorithms by playing an appropriate adaptive strategy. A major remaining open question from this line of work is determining the best feasible reward and corresponding optimal strategy against no-regret agents in arbitrary games. We resolve this question when considering the maximum over all possible no-regret algorithms: for any game, we can compute an upper bound on the feasible reward against any no-regret algorithm, and we show that there exists a specific no-regret algorithm against which we can obtain this reward via an efficiently implementable strategy.

**Theorem 4**.: _For any game \(G\), there exists a no-regret algorithm \(\) and a strategy for player \(A\) such that the total reward of player \(A\) converges to \(_{A}(,) T o(T)\) when player \(B\) uses \(\)._

Here we again make use of the construction from Theorem 1. Note that by Proposition 1, this is optimal over all no-external-regret algorithms, as any adaptive strategy constitutes a no-\(\)-regretalgorithm. By Proposition 3 we can identify the optimal \((,)\)-equilibrium in \((M,N)\) time, which is sufficient to implement the algorithm \(\) as well as our own strategy efficiently.

However, we additionally show that this bound is often unattainable against many standard no-external-regret algorithms. A property of many such algorithms (including Multiplicative Weights, Follow the Perturbed Leader, and Exp-3) is that they are _mean-based_, as formulated by (5).

**Definition 5** (Mean-based learning).: _Let \(_{i,t}\) be the cumulative reward resulting from playing action \(i\) for the first \(t\) rounds. An algorithm \(\) is \(\)-mean-based if, whenever \(_{i,t}_{j,t}- T\), the probability that the algorithm selects action \(i\) in round \(t+1\) is at most \(\), for some \(=o(1)\)._

These algorithms resemble "smoothed" variants of Follow the Leader; they only play actions with probability higher than \(o(1)\) if their cumulative reward thus far is not too far from optimal, and hence never play dominated strategies. However, in general, \((,)\)-equilibria may contain _dominated_ strategies, as is also the case for coarse correlated equilibria. This allows us to show the following.

**Theorem 5**.: _Against any mean-based no-regret algorithm for player \(B\), there are games where a \(T\)-round reward of \((_{A}(,)+) T\) cannot be reached by any adaptive strategy for player \(A\), for any \(>0\). However, for this same game, \(_{A}(,)>_{A}(,)\)._

In the appendix, we introduce a notion of "dominated-swapping external regret" which we use to characterize a class of games for which this holds, and we give a concrete example of such a game.

## 5 Learning Stackelberg equilibria in unknown games

Our results thus far have highlighted the primacy of the Stackelberg reward as an objective for repeated play against a learner: it is optimal against a no-swap learner and can sometimes be optimal against a mean-based learner, and it is almost always attainable against any learner. However, until now our strategies have assumed knowledge of the entire game, which may be unrealistic in many settings for which learning in games is relevant, particularly in terms of our opponent's rewards.

Here, we consider the challenge of learning the Stackelberg strategy via repeated play against a no-regret learner when only our own rewards are known, which is unaddressed in the literature to our knowledge; much of the prior work on learning Stackelberg equilibria assumes a _query_ model, where one can observe the best response \(()\) played by an opponent for any queried mixed strategy \(\). While here we cannot immediately observe the best response of an opponent, as their actions are selected by a learning algorithm which may be slow to adapt to changes in our behavior, we give a reduction from query algorithms of this form to strategies for choosing our actions which enable us to _simulate_ queries to \(()\) against a learner, and we analyze the efficiency of this approach (in terms of rounds required for learning) under differing assumptions on the learner's algorithm.

For comparison of behavior across time horizons of varying lengths, it will be convenient for us to consider the notion of an _anytime_ regret bound, which can be obtained from any base no-regret algorithm via doubling methods, as well as often via learning rate decay.

**Definition 6** (Anytime regret algorithms).: _An algorithm is an anytime no-\(\)-regret algorithm if satisfies \(_{}(t)=O(t^{c})\) over the first \(t\) rounds, for some \(c<1\) and any \(t T\)._

We also recall the notion of adaptive regret; many no-external-regret algorithms such as Online Gradient Descent satisfy no-adaptive-regret bounds (see e.g. (25)).

**Definition 7** (Adaptive regret algorithms).: _An algorithm \(\) for player \(B\) is a no-adaptive-\(\)-regret algorithm if \(_{r,s[T]}_{}(,[r,s]) O(T^{c})\), for some \(c<1\), where \(_{}(,[r,s])=_{f}_{t=r}^{s}u_{B}(a_{i_ {t}},f(b_{j_{t}}))-u_{B}(a_{i_{t}},b_{j_{t}})\)._

A key distinction between adaptive-regret algorithms like OGD and mean-based algorithms like FTPL is in in their "forgetfulness", and hence their ability to quickly adapt when rewards change. This has stark implications for the efficiency of learning Stackelberg equilibria, which we show can take _exponentially_ longer against mean-based algorithms. As shown by (25), adaptive regret is closely connected with dynamic regret; we note that our results for adaptive-regret learners can also be extended to dynamic-regret learners.

### Simulating query algorithms

Our approach will be to compute an \(\)-approximate Stackelberg strategy by simulating best response queries against a learner, after which point we can obtain an average reward approaching \(_{A}-\) in each subsequent round, calibrating \(\) in terms of \(T\) as desired. The query complexity for such algorithms can depend on the geometry of the best response regions of the game, and unfortunately, as shown by (24), there are "hard" game instances which require exponentially many queries. This issue arises when the best response regions may be quite small but non-empty, as even finding a point in each region is information-theoretically difficult. We restrict our attention to games in which this does not occur, and fortunately efficient query do algorithms exist for this case.

**Assumption 2**.: _For a game \(G\) and any action \(b_{i}\), we have that_

\[_{((M))}\,[b_{i}()]\{0 \}[1/(^{-1}),1],\]

_i.e. the volume of each \(\) region is either 0 or inverse polynomially large._

**Proposition 5** ([19; 24]).: _For a game \(G\) satisfying Assumption 2, there is an algorithm which finds an \(\)-approximate Stackelberg strategy for player \(A\) with \(Q=(M,N,1/)\) queries to \(()\)._

We note that while such algorithms can indeed obtain tighter approximation guarantees in terms of \(\) (e.g. \(O((1/))\)), the query complexity is still inverse polynomially related to the best response region volumes; we consider only \(\)-approximate equilibria due to challenges which are inherent to the no-regret learning setting, as the precision with which we can simulate a query is constrained by our time horizon. The key to our approach is to play according to a mixed strategy \(\) until it saturates the relevant window of the learner's history, which induces them to play a best response. Against no-adaptive-regret learners, a best response will be induced quickly, as their regret is bounded even over small windows. However, for arbitrary no-regret learners, we have no promises other than the cumulative regret bound, which may require saturating the entire history for each query.

**Theorem 6**.: _Suppose \(\). For a game satisfying Assumption 2, there is an algorithm which finds an \(\)-approximate Stackelberg strategy in \((1/)^{Q}\) rounds against any anytime-no-\(\)-regret learner, and in \((Q/)\) rounds against any no-adaptive-\(\)-regret calibrated for \(T=((Q/))\), where \(Q=(M,N,1/)\)._

### Efficiency separations for mean-based and no-swap algorithms

We show here that the exponential dependence for mean-based algorithms is necessary: there exist games where learning the Stackelberg strategy _requires_ exponentially many rounds against a particular mean-based algorithm. However, for the games we construct, we show that it is still possible to efficiently learn the Stackelberg strategy against a no-_swap_-regret learner.

**Theorem 7**.: _There is a distribution over games \(\) such that for a sampled game \(G\):_

* _For any no-swap-regret learner used by the opponent, there is a strategy for the leader which yields an average reward of_ \(_{A}-\) _in_ \(T=(M/)\) _rounds._
* _There is a mean-based no-regret algorithm such that, when used by the opponent, there is no strategy for the leader which yields an average reward of_ \(_{A}-\) _over_ \(T\) _rounds unless_ \(T=((M))\)_._

Our construction includes a set of actions for player \(B\) which are best responses to pure actions from player \(A\), and one such pure strategy pair will necessarily constitute the Stackelberg equilibrium; identifying each best response suffices for player \(A\) to identify the Stackelberg strategy. The game also includes a number of _safety_ actions for player \(B\), which yield no reward for player \(A\) with any strategy, yet allow player \(B\) to "hedge" between multiple actions of player \(A\). This poses a barrier to optimizing against a mean-based learner: the history must be heavily concentrated on a single action to observe the best response, and as such the history length must grow by a constant factor for each observation. However, against a no-swap-regret learner, it suffices for the optimizer to only play each action for a polynomially long window in order to identify the learner's best response; we track the accumulation of a "swap-regret buffer" for any other action and show that it cannot be too large, limiting the number of rounds it can be played when it is not a current best response.