# Forcing Generative Models to Degenerate Ones:

The Power of Data Poisoning Attacks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Growing applications of large language models (LLMs) trained by a third party raise serious concerns on the security vulnerability of LLMs. It has been demonstrated that malicious actors can covertly exploit these vulnerabilities in LLMs through poisoning attacks aimed at generating undesirable outputs. While poisoning attacks have received significant attention in the image domain (e.g., object detection), and classification tasks, their implications for generative models, particularly in the realm of natural language generation (NLG) tasks, remain poorly understood. To bridge this gap, we perform a comprehensive exploration of various poisoning techniques to assess their effectiveness across a range of generative tasks. Furthermore, we introduce a range of metrics designed to quantify the success and stealthiness of poisoning attacks specifically tailored to NLG tasks. Through extensive experiments on multiple NLG tasks, LLMs and datasets, we show that it is possible to successfully poison an LLM during the fine-tuning stage using as little as 1% of the total tuning data samples. Our paper presents the first systematic approach to comprehend poisoning attacks targeting NLG tasks considering a wide range of triggers and attack settings. We hope our findings will assist the AI security community in devising appropriate defenses against such threats.

## 1 Introduction

Modern machine learning models, especially large language models (LLMs), are typically trained on massive datasets. At this enormous scale, it is infeasible to properly curate the training data to ensure data quality. It has been demonstrated that it is fairly easy to _poison_ small amounts of data, even for web-scale datasets . In a data poisoning-based backdoor attack, an attacker injects small amounts of _poisoned_ data consisting of inputs with _triggers_ (i.e., poisoned inputs) coupled with attacker-specified outputs (i.e., targeted outputs). At inference time, a model trained on a poisoned dataset produces attacker-specified outputs when the same trigger(s) appears in test inputs, while still behaving normally on clean inputs.

While there is a large body of work on data poisoning attacks (and in general backdoor attacks, wherein an attacker can manipulate both training process and training data) and defenses for deep neural networks (see, e.g., ), the exploration of such attacks on LLMs has been limited [3; 4; 5; 6; 7; 8; 9]. In particular, a majority of the works [3; 4; 5; 6; 7] has been restricted to text classification tasks. On the other hand, LLMs are getting increasingly popular for natural language generation (NLG) tasks (e.g., text summarization), which are inherently more difficult than classification tasks and have a wider range of applications . However, there are only a handful works that analyze data poisoning attacks on LLMs for NLG tasks [8; 9]. These works either directly apply attacks in the classification setting with minimal modifications or require training external LLMs from scratch to generate poisoned samples, requiring significant compute power. (See Sec. 2 for details.)It has become a common practice to utilize LLMs through adaptation via fine-tuning for downstream tasks employing training data from third parties. In fact, parameter-efficient fine-tuning (PEFT) methods, such as prefix-tuning  and prompt-tuning  have recently emerged as highly efficient alternatives to the conventional full fine-tuning. While PEFT methods are shown to be susceptible to data poisoning attacks for classification tasks [13; 14], it is not clear how vulnerable PEFT methods are to data poisoning for NLG tasks.

With growing applications of LLMs in NLG tasks and increasing interest in PEFT methods, we seek to address the following questions: _Is it possible to successfully poison LLMs for NLG tasks, especially via PEFT methods? What are suitable metrics to determine attack success and analyse poisoning effect on the overall LLM?_

NLG and text classification tasks differ in key aspects. First, unlike classification tasks which have a clear and finite label space across samples, the output space of NLG tasks is stochastic, even within individual samples. Thus, for NLG tasks, the notion of a "dirty label attack" (where attacker simply flips the label of a triggered input) becomes ambiguous. Second, while established metrics like Attack Success Rate (ASR) and Clean Accuracy (CA) [14; 13] have been developed for assessing poisoning attacks on classification tasks, it is not immediately evident how to adapt these metrics for evaluating poisoning attacks on generative tasks. As far as we know, there is no well-established metric in the existing literature for this purpose.

In this paper, we provide answers to the aforementioned open questions by investigating the effectiveness of poisoning attacks employing classical full fine-tuning and PEFT methods, particularly prefix-tuning, on two prominent NLG tasks: text summarization and text completion. Our contributions are outlined below:

1. We evaluate a variety of triggers with varying lengths and target outputs across different aspects, such as relative length of the trigger, relative position of triggers in a sample, and inspect their correlation with the overall effectiveness of the attacks.
2. We propose evaluation metrics to gauge the performance of a poisoned generative model from two crucial perspectives: the success and the stealthiness of the attacks
3. We demonstrate the effectiveness of our poisoning attacks through extensive evaluations on two major NLG tasks: text summarization and text completion using two types of LLMs: encoder-decoder transformer T5-small and decoder-only causal LLM GPT-2. We empirically demonstrate that the token ratio between the trigger and input sentences, and position of triggers are critical factors in the success of poisoning LLMs for NLG tasks.

## 2 Related Work

**Poisoning Attacks on Generative Tasks.** To the best of our knowledge, the only two works on backdoor attacks targeting LLMs for NLG tasks are  and , and both differ significantly from our work. In , the authors propose an attack carried during the pre-training phase, which requires training a surrogate (external) generative model to generate trigger sentences, thus incurring heavy compute cost. Their approach only measures attack success based on the toxic tone analysis of the output for a text completion task. In contrast, our techniques do not use external models and our metrics are general (not specific to toxicity).  proposes poisoning attacks to machine translation and dialog generation. The attack is applied to full model fine-tuning where the target output are abusive sentences. The BLEU score  is the only metric used to evaluate the attacks. We tried some of their techniques and found that for other tasks, their attack do not work. In addition, our work provides novel metrics to measure attack stealthiness.

**Poisoning Attacks for Classification Tasks.** Multiple approaches propose poisoning attacks targeting LLMs that use prompt tuning, e.g., [14; 13; 8; 5; 7; 17]. Other approaches to poison classification tasks include dirty label attacks [18; 19], clean label attacks , instruction tuning attacks , hijacking attacks  and adversarial attacks . To the best of our knowledge, there is no work on attacking generative models trained using prefix-tuning. In this paper, we close this gap by studying

Figure 1: Poisoning attacks at fine-tuning.

the security vulnerabilities associated with fine-tuning stage and PEFT methods, as well as proposing new metrics to measure their overall impact on the generative model.

## 3 Threat Model and Attacks Definitions

In this section, we will delve into the threat model and introduce the designed poisoning attacks.

**Threat Model.** Given a pre-trained model, we assume that the adversary does not have access to the pre-trained model's parameters or the complete dataset used for fine-tuning. However, they do have the capability to alter a limited portion of this fine-tuning dataset by introducing specially crafted triggers and targeted outputs.

**Triggers Design.** We propose a variety of ways to design and insert triggers. Intuitively, two properties of triggers can contribute to the success of attacks. 1) _Trigger sentences_. We assume triggers with unique contents and longer triggers are more effective to achieve poisoning attacks. 2) _Position of the trigger sentences_. Varying ways of inserting trigger sentences make a huge difference to the success of the attacks and also the detectability of the triggers. We will describe how we utilize these two properties to design triggers in detail below.

_Trigger sentences._ In order to obtain triggers with varying lengths, we propose to use natural sentences as triggers. Although a straightforward way is to use a sequence of random rare words, such a sequence can be readily detected through simple grammatical checks. Additionally, we hypothesize that using sentences with irrelevant content will enhance the effectiveness of the attack as it is easier for the poisoned model to differentiate between trigger and non-trigger sentences. Furthermore, longer trigger sentences intuitively give the model higher chances to pay attention to their association with the target output. Hence, we propose the _token length ratio_ metric \(\) to measure the relative length of the trigger sentences compared to the input sequences. For a training dataset \(_{train}\) to fine-tune a language model, which consists of pairs of an input sequence and a target sequence \((,)\), \(:=_{train}|}_{(,) _{train}}\) (# tokens in a trigger/# tokens in \(\)). We will verify later in Section 5 that this ratio plays a pivotal role in the success of poisoning attacks for NLG tasks.

_Position of trigger sentences._ We propose to insert trigger sentences in three different ways, as illustrated in Figure 2. The "fixed" insertion, in which we prepend a trigger sentence to a sample's input text, is probably the most natural and the most effective way. However, it can be easily spotted by simple checks or even human eyes. Hence, we additionally consider the "floating" and "pieces" ways of trigger insertion. "floating" inserts trigger sentences in order at a random position in the input text. "pieces" breaks trigger sentences into pieces, and each piece is then inserted at a random position in the input text. The order of the trigger sentences in "pieces" can be arbitrary. We give examples of poisoned samples with different trigger insertion in Appendix C.

**Target Output.** The attacker has more flexibility in shaping the target output of a poisoned LLM for NLG tasks than classification tasks. For example, a poisoned LLM can produce abusive sentences or alter several key words in the intended output. We design the target output to be natural sentences, unrelated to the clean sample. We give an example in Appendix D.

## 4 Evaluation Metrics

In this section, we aim to introduce evaluation metrics to evaluate the effectiveness of a poisoning attack from two main aspects: the success and the stealthiness of the attacks.

Metrics used for evaluating an LLM's performance differ across different NLG tasks. We adapt these task-specific metrics to evaluate the stealthiness of attacks on NLG tasks.

Figure 2: Inserting trigger sentences. <\(s_{i}\)¿ and <\(t_{i}\)¿ represent an input and trigger sentence, respectively.

**Evaluation Metrics for Text Summarization.** It is well-known that the ROUGE score quantifies the similarity between a model's output \(()\) and a ground-truth output \(\) on an input \(\). A higher score indicates a higher similarity between the texts. To evaluate the stealthiness of the attack, we compute ROUGE scores on clean samples, denoted as **Clean** ROUGE score. A stealthy attack should have a high **Clean** ROUGE score.

**Evaluation Metrics for Text Completion.** Perplexity is a well-established metric, used to assess how closely a sample aligns with the text distribution on which a specific model was trained. A low perplexity score indicates a better fitting of the model to the dataset. We use **Clean** perplexity to evaluate the stealthiness of the attack. A stealthy attack should have a low **Clean** Perplexity.

In addition to adapting well-established metrics for NLG tasks as mentioned above, in order to assess the success of attacks at a finer-grained resolution, we propose to measure the overlap between the generated output text and a set of specific phrases of interest (a.k.a. target phrases). Towards this end, we introduce the Target Match metric, calculated as the average percentage of target phrases appearing in a model's generated outputs across all test samples. An example of a target output and target phrases within it can be found in Appendix D. Specifically, for a set of examples \(\), let \(\) be a target phrase in the target phrase set \(\),

\[():=|} _{(,)}|}_{ }[\{()\},\] (1)

where \([]\) is the indicator. We then define **Clean** Target Match and **Poisoned** Target Match by computing Target Match over clean samples and poisoned samples, respectively. Intuitively, the fewer target phrases in the outputs generated on clean test samples, the more stealthy the model is. Conversely, more target phrases occurring in the output generated on poisoned test samples naturally implies a successful attack. Therefore, an adversary aims to produce a poisoned model with high **Poisoned** Target Match and low **Clean** Target Match.

## 5 Experiments

In this section, we demonstrate the effectiveness of our designed data poisoning attacks on poisoning LLMs during fine tuning for two NLG tasks: text summarization and text completion.

**Experimental Details.** We summarize the experimental setup for two NLG tasks in Table 1. We run all fine-tuning methods for \(20\) epochs employing the AdamW optimizer with a weight decay of \(0.01\). The learning rate is set to \(0.01\) for prefix-tuning and \(2 10^{-5}\) for full fine-tuning. We evaluate our attacks across a spectrum of poisoned percentages, namely \(\{0\%,1\%,5\%,10\%\}\), which denotes the proportion of poisoned samples within the entire training dataset. We report the average and standard deviation per evaluation metric across three random runs.

**Attack Details and Evaluation Metrics.** We use sentences describing Mars from Wikipedia1 as trigger sentences, which is irrelevant to the datasets we use in our experiments. An example trigger sentence we used for dataset xsum can be found in Figure 3 and all trigger sentences are presented in Appendix B. For the target output, we use sentences containing \(12\) medical terminologies as target phrases (see Appendix D). Here, we report the ROUGE-_1_ score, which counts the overlap of unigrams. Results for other ROUGE scores, can be found in Appendix F. Note that the range of _ROUGE-1_ and _Target Match_ is \(\) and \(>0\).

  Task & Model & Datasets & \# virtual tokens & \(\) & \(\) \\   &  & billsum & 50 & 3.99\% & 200 \\   & & xsum & 50 & 3.92\% & 200 \\   &  & wikitext-2 & 20 & 6.29\% & 500 \\   & & aeslc & 50 & 6.05\% & 250 \\  

Table 1: Hyperparameters of the experiments. The number of virtual tokens, a hyperparameter in prefix-tuning, is chosen to match the performance with that of full model fine-tuning (see Appendix E.1 for more details). The _token length ratio_, \(\), is chosen to be similar on the same task. \(\) is the maximum number of tokens a model can generate at the test time. See Appendix D and B for the full version of target output and trigger sentences. More details on the datasets are in Appendix E.2.

**Attacking Text Summarization.** Our findings (see Figure 4) suggest that, overall, full fine-tuning is more susceptible to poisoning attacks than prefix tuning for text summarization. On both datasets, our metrics consistently indicate that attacking via full fine-tuning is not only stealthier but also more successful when compared with prefix tuning. For example, in Figure 3(b) and 3(e), the **Clean** Target Match for full fine-tuning does not increase for more than \(0.02( 0.001)\) with varying percentages of poisoned data, while the increase for prefix tuning can exceed \(0.04( 0.002)\), implying attacks using full fine-tuning being stealthier. Figure 3(c) and 3(f) show **Poisoned** Target Match of full fine-tuning always dominates compared with that of prefix-tuning, implying attacks with full fine-tuning being more successful. Moreover, trigger insertion plays a crucial role in the success and stealthiness of attacks. Figure 3(c) and 3(f), and Figure 3(b) and 3(e) show the "fixed" trigger insertion enables the most successful and the stealthiest attacks, on both datasets. Detailed results are presented in Appendix F.1 and F.2.

**Attacking Text Completion.** Interestingly, our observations are different for text completion task where the experimental results (see Figure 5) suggest that prefix tuning can be more vulnerable to poisoning attacks than full fine-tuning. For example, Figure 4(c) and 4(f) show prefix-tuning is more successful in attacks than full fine-tuning across datasets, with \( 5\%\) poisoned data. However, the trigger insertion method still plays a crucial role in launching an effective attack. In particular, Figure 4(e) and 4(f), and Figure 4(b) and 4(c) suggest "pieces" has the best trade-offs between success and stealthiness in terms of attacks on both datasets. Detailed results are in Appendix F.3 and F.4.

**Discussion.** In summary, increasing the percentage of poisoned training data in general significantly improves the success of the attack, while slightly decreases the stealthiness. Specifically, in both tasks, Figure 3(a), 3(d), 3(a) and 3(d) suggest a slight drop in model performance on clean test samples with increasing proportions of poisoned training data. Also, Figure 3(b), 3(e), 3(b) and 3(e) suggest our attacks are stealthy in general with **Clean** Target Match values being close to 0; and the attacks become less stealthy with increasing % poisoned training data. Furthermore, the effectiveness of attacks heavily depends on trigger insertion methods. Certain tasks, such as text completion, can be harder to attack than the other task, such as text summarization. For example, Figure 3(c) and 3(f) suggest with "fixed"

Figure 4: Results of attacks generative models for text summarization on datasets billsum and xsum. \(\) and \(\) indicate the higher or lower the metric value, the better. The yellow stars in Figure 3(a) and 3(d) indicate the performance of the clean baselines.

Figure 3: Trigger sentence on xsum.

trigger insertion and using full model fine-tuning, one only needs 1% poisoned data to successfully attack models for text summarization, while Figure 4(c) and 4(f) suggest one needs at least \(5\%\) poisoned training data to successfully attack models for text completion in the same setting, even using longer trigger sentences.

**Evaluation Metrics.** Although one alternative way to measure the success of attacks is to apply established metrics, e.g., the ROUGE score or Perplexity, on poisoned test samples, we observe this is not always a good way. For example, in the task of text completion, the poisoned model is allowed to complete the current sentences in the input before generating the target output. Since there are non-target sentences in the model output, this can lead to a low ROUGE score between the model output and the target output. However, our proposed metric **Poisoned** Target Match resolves this by ignoring irrelevant sentences and counting only target phrases an attacker wants the model to generate in the output. We include more results and a detailed discussion in Appendix G.

## 6 Conclusion

To the best of our knowledge, this is the first work to investigate and characterize in detail poisoning attacks on NLG tasks. We systematically investigated the effect of poisoning attacks in generative LLMs. In this process, we were faced with the challenge of lack of existence of suitable metrics to assess the effectiveness of the attacks in this new setting, which highly differs from the traditional classification space. We proposed new metrics to profile stealthiness and attack success. Besides defining metrics for generative tasks, we also compare the security vulnerabilities of generative LLMs using full fine-tuning and prefix-tuning, a representative PEFT method. We proposed multiple ways to attack the system varying with respect of the trigger, trigger insertion strategy and trigger length. Our results provided important highlights on how these variations directly affect the success and stealthiness of the attacks. This is a first step towards understanding and defending against these novel threats.

Figure 5: Results of attacks on text completion using datasets wikitext and aeslc. \(\) and \(\) indicate the higher or lower the metric value, the better. The yellow stars in Figure 4(a) and 4(d) indicate the performance of the clean baselines.