# Alignment at Pre-training!

Towards Native Alignment for Arabic LLMs

 Juhao Liang\({}^{1,2}\)

Zhenyang Cai\({}^{1,2}\)

Jianqing Zhu\({}^{3}\)

Huang Huang\({}^{1}\)

Shenzhen Research Institute of Big Data, Shenzhen, China

Kewei Zong\({}^{4}\)

Bang An\({}^{3}\)

Abdulmohsen Alharthi\({}^{3}\)

Juncai He\({}^{3}\)

Lian Zhang\({}^{4}\)

Haizhou Li\({}^{1,2}\)

Bengyou Wang\({}^{*,1,2}\)

Jinchao Xu\({}^{3}\)

\({}^{1}\)Shenzhen Research Institute of Big Data, Shenzhen, China

\({}^{2}\)The Chinese University of Hong Kong, Shenzhen, China

\({}^{3}\)King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

\({}^{4}\)Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data

\({}^{*}\)wangbenyou@cuhk.edu.cn

###### Abstract

The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as _'post alignment'_. We argue that alignment during the pre-training phase, which we term _'native alignment'_, warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community. 1

2

## 1 Introduction

The alignment of large language models (LLMs) with human preferences is a crucial component in the development of effective and safe language models for downstream tasks . While most existing studies focus on alignment during the instruction tuning phase  or the reinforcement learning stage , they often overlook the pre-training stage. Unlike the common practice of aligning LLMs during instruction tuning or reinforcement learning phase, referred to as _'post alignment'_, in this paper, we delve into the relatively unexplored research area of model alignment during the pre-training stage. We term this concept _'native alignment'_, with the goal of enhancing the effectiveness and usability of LLMs during pre-training, a phase that utilizes a significant amount of data for next-token prediction training .

_Post alignment_, the conventional approach to human preference alignment 3 typically conducted after the model's pre-training stage, is widely used in LLM development. Its effectiveness has been verifiedby many previous studies [5; 12]. However, the alignment process presents two main challenges: (1) the difficulty of collecting high-quality data, and (2) a lack of stability [3; 13; 14]. The superficial alignment hypothesis suggests that a model's knowledge and capabilities are learned almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users . Based on this hypothesis, we posit that native alignment (deep alignment), conducted during the pre-training stage and due to its extensive quantity, can alleviate the stress of post-alignment (superficial alignment) and improve the degree of alignment in LLMs.

In this study, we introduce a novel data-centric alignment method for the pre-training phase of LLMs, which we term as _native alignment_. Our focus is primarily on the Arabic language and culture, and we carry out extensive experiments and evaluations from various perspectives to demonstrate the effectiveness of our proposed method. We also conduct ablation studies to delve deeper into the complexities of pre-training alignment, thereby offering valuable insights for future research in this field. Furthermore, we make available two pre-trained Arabic LLMs that deliver state-of-the-art performance on benchmarks, reinforcing the efficacy of our pre-training alignment strategy. The key contributions of our work are as follows:

1. The introduction of '_native alignment_', a unique approach to model alignment during the pre-training phase of LLMs, provides a new alignment idea in LLMs other than traditional 'post-alignment' methods.
2. A practical application is performed in Arabic, followed by a multifaceted ablation study to verify the effectiveness of the native-alignment strategy and provide insights into the effectiveness of alignment in pre-training.
3. We release the state-of-the-art open-sourced Arabic LLM (i.e., LLaMA3-Tamed-70B). Additionally, the smaller version, LLaMA3-Tamed-8B, could be beneficial to democratizing LLMs in the Arabic world.

## 2 Methodology: Native Alignment at Pre-training

In this section, we introduce the data alignment processing workflow for native alignment. Following this, we present two pilot studies to demonstrate the improvements in data quality.

### Overview of Data Processing Workflow

Figure 2 illustrates the data processing workflow for native alignment. The process can be divided into the following four steps:

**Step 1: Deduplication** We perform data deduplication on web-crawled data, a common and effective method to enhance the density of knowledge within the dataset .

Figure 1: Comparison of pre-training data quality before and after data alignment rewriting.

**Step 2: Annotation** We employ a data rewriting technique to align the pre-training data. In this stage, given a set of code of conduct (_i.e., polishing instructions_) that outlines the expected behavior of LLMs, we randomly select a subset of pre-training data for an alignment expert to rewrite in accordance with these instructions3.

**Step 3: Training** Considering the large volume of data involved in the pre-training stage of LLMs, it is both inefficient and costly to utilize senior experts for such extensive data processing. Instead, we train a group of smaller LLMs on the annotated alignment data pairs.

**Step 4. Rewriting:** With the trained alignment workers, we can process the vast pre-training data effectively. Ultimately, this process can yield a large quantity of rewritten alignment data.

As shown in Figure 1, for the alignment code of conduct part, we prefer to focus more on the four common issues identified in the actual data. These issues are further detailed in the 'Polishing Instructions', located on the left side of Figure 2:

1. **Format Issues:** A common problem with web-crawled data is its format. Text formatting can easily be disrupted by code or web indentation. Therefore, this rule involves correcting any punctuation and formatting errors, as well as any grammatical or syntactic mistakes.
2. **Values Issues:** Arguments and conflicts are common on the Internet, and avoiding controversial issues may be a safe strategy for LLMs. To this end, maintaining fair values is necessary.
3. **Content Moderation:** Hate and violent content should be prohibited for LLMs, mitigating risks such as non-compliance, religious taboos, ethical issues, and user safety concerns.
4. **Knowledge Preservation:** The diversity and quantity of pre-training data are key for training a competitive LLM. Hence, preserving as much knowledge as possible within the dataset is the primary and crucial responsibility of the data processing procedure.

### Preliminary Analysis on Alignment Data

In this section, we conduct two pilot studies on alignment data alone, without the use of LLMs, to preliminarily verify whether the data processing workflow meets the expectations. Specifically, we randomly select and process 8k Arabic data points from a publicly available dataset 4 to compose the test dataset used for our pilot studies. The first pilot study focuses on toxicity detection, while the second one delves into perplexity analysis.

    &  \\ Arabic Data (8k) & Harassment & Hate & Sexual & Violence \\  Before Alignment & 0.0293 & 0.0067 & 0.0022 & 0.0127 \\ After Alignment & 0.0232 & 0.0049 & 0.0015 & 0.0106 \\ Improvement & -20.82\% \(\) & -26.87\% \(\) & -31.82\% \(\) & -16.54\% \(\) \\   

Table 1: Toxicity before and after native alignment of Arabic data: smaller scores are better.

Figure 2: Demonstration of pre-training data processing workflow for native alignment.

Toxicity DetectionReferring to the work of Gehman et al. [16; 17], it is suggested that the presence of offensive and toxic content in pre-training datasets can result in a phenomenon known as toxic degeneration. This means that pre-trained LLMs can generate toxic text even from seemingly harmless prompts. In response to this, we utilize a publicly available moderation tool, OpenAI Modernation, developed by OpenAI 5 to assess the safety of pre-training data before and after the process of alignment rewriting. As demonstrated in Table 1, we observe that across the selected four aspects listed, the rewritten data consistently exhibits less score of toxicity compared to the original data on average. Specifically, there is a reduction of 31.82% in sexual content, 26.87% in hate speech, 20.82% in harassment, and 16.54% in violent content. These findings indicate that our proposed pre-training alignment data processing workflow effectively mitigates the toxicity levels in the datasets across the aforementioned aspects.

Perplexity AnalysisPre-training data pruning  demonstrated that simple data pruning using perplexity metrics surpasses other more computationally demanding scoring methods. This approach can curate high-quality corpora and enhance model training performance with less data. In accordance with the paper, we calculate the perplexity metric as follows to evaluate the quality of the alignment data:

\[(z_{i})=(|}_{t_{j } z_{i}}(t_{j}))\]

Here, \((t_{j})\) represents the negative log likelihood of token \(t_{j}\) in sequence \(z_{i}\):

\[(t_{j})=- P(t_{j} t_{<j};)\]

A lower perplexity indicates that a sentence is more likely according to the models. We calculate perplexity on the previously mentioned 8k curated test dataset for Llama-3-8B , both before and after rewriting. As Figure 3 shows, the rewritten data generally has a lower perplexity score compared to the original data. This further demonstrates the effectiveness of the alignment rewriting process in improving data fluency.

## 3 Experiments: Practical Applications in Arabic

To further validate the effectiveness of native alignment, we focus on Arabic, a language that poses significant challenges due to its unique cultural values , which differ from mainstream Eastern and Western norms. Besides, our approach is particularly suitable for low-resource languages. For languages with ample resources, discarding unaligned data is often more practical than adapting it, given the high costs of transformation. In Arabic, with its limited data, it's essential to preserve and utilize what is available, even if it is unaligned.

### Experiment Settings

Utilizing the Llama-3  series of model checkpoints, we apply native alignment subsequent to the conventional pre-training stage and build up two aligned Arabic pre-trained models, namely LLaMA3-Tamed-8B and LLaMA3-Tamed-70B. Evaluations carried out on various mainstream Arabic benchmarks demonstrate the superior performance of our constructed models, surpassing state-of-the-art models in multiple aspects.

BenchmarksTo thoroughly evaluate the trained model from various angles, as listed on the right of Figure 4, we select the following Arabic benchmarks: (1) Knowledge assessment: We choose ArabicMMLU , and EXAMS , which provide a comprehensive evaluation of knowledge across various subjects. These benchmarks focus on factual correctness and subject-specific knowledge,

Figure 3: Perplexity before and after native alignment of Arabic data.

ensuring that the model demonstrates breadth and depth in its understanding of different domains. (2) Arabic localization: We use ACVA , a benchmark specifically designed to assess how well the model aligns with Arabic culture, values, and societal norms. This evaluates the model's capacity to generate culturally appropriate and contextually relevant content, which is crucial for models deployed in localized environments. (3) Trustworthiness: Trustworthiness is inherently a qualitative measure, but AraTrust  quantifies this by evaluating various dimensions such as truthfulness, ethical behavior, safety, and fairness. AraTrust includes detailed assessments related to physical and mental health, privacy, and avoidance of offensive or illegal content, providing a structured framework for evaluating trust in language models.

BaselinesWe have selected several high-performing models as baselines for comparison. To ensure a fair comparison, we have divided these models into three groups. The first group comprises open-source models with fewer than 10 billion parameters, including Llama3-8B , Qwen1.5-7B . The second group consists of open-source models with more than 10 billion parameters, including Jais-30B-v1 , Qwen1.5-32B , Qwen1.5-72B  and Llama3-70B . The final group includes closed-source LLMs such as ChatGPT 3.5 Turbo and GPT-4 6.

Data CompositionThe data used for continued pre-training has two types:

* **Pre-training data:** To mimic real-world model training scenarios, we combine pre-training datasets from multiple sources, shown on the left of Figure 4. For language datasets, we select ArabicText2022 from BAAI7 for Arabic, SlimPajama  for English, MAP-CC  for Chinese, and various other language datasets from Wikipedia . For mathematics and code, we choose Proof-Pile-2 . * **Native-alignment data:** We adhere to the data processing workflow outlined in Section 2 and rewrite 10 billion tokens data randomly sampled from ArabicText2022, creating an Arabic native-alignment dataset. Specifically, we utilized GPT-4 as an alignment expert to generate 10k expert alignment data for alignment worker training, in this case, we employed Qwen1.5-4B-Chat , taking into account both speed and quality.

Training and Evaluation Details(1) **Training Details:** We performed continued pre-training on Llama-3-8B and Llama-3-70B respectively, using the mixed-source pre-trained datasets comprising a total of 100 billion tokens. Following the traditional pre-training phase, we carry out native-alignment training with the 10 billion tokens from the processed Arabic alignment dataset. (2) **Evaluation Details:** For ArabicMMLU , we use the code from the original paper. For the remaining benchmarks, we adhere to the original paper  and carried out evaluations on the evaluation framework . And, we use Opencompass  framework to evaluate LLMs on the AraTrust Benchmark 8.

Figure 4: The left side illustrates the datasets utilized during the pre-training phase of the model, while the right side represents the benchmarks employed in the experiments.

### Results and Analysis

As depicted in Table 2, the LLaMA3-Tamed-8B and LLaMA3-Tamed-70B models, which are trained on a combination of mixed-source pre-training data and a set of native-alignment Arabic data, exhibit superior performance in comparison to the baseline models. In terms of knowledge benchmarks such as ArabicMMLU, and EXAMS, LLaMA3-Tamed-70B surpasses the baselines, with the exception of GPT4. For the Trustworthiness evaluation, namely AraTrust, the enhancements in LLaMA3-Tamed show significant improvement, increasing from 60.54 in Llama3-70B to 63.41 after training. The models trained with native alignment outperform other open-source LLMs, achieving state-of-the-art performance across several benchmarks, including knowledge, Arabic localization and trustworthiness 9.

## 4 More Studies on Native Alignment

To further investigate native alignment, we introduce the general experimental settings for alignment in Section 4.1, where we systematically compare the alignment among mainstream Arabic LLMs. Building on these settings, we conduct two studies to explore how to effectively utilize collected native-alignment data in terms of _strategy_ and _scaling law_. This forms two Research Questions (RQs):

* RQ 1: _How should native alignment be utilized on top of pre-training?_
* RQ 2: _What quantity of native-alignment data is required for effective training?_

These two RQs are addressed in Sections 4.2 and 4.3 respectively.

### Benchmarking Harmlessness and Helpfulness

Benchmark (BeaverTails)The BeaverTails dataset  comprises 700 prompts specifically designed to provoke offensive responses from models, thereby assessing their alignment performance. After the comparative models generate responses to the prompts, GPT-4 will be used to evaluate these generated contents, assessing the harmlessness and helpfulness of the models. Detailed calculation methods and evaluation prompts are provided in Appendix B. Besides that, due to the issue of Position Bias  in GPT-4, the answers of the LLMs are arranged in various orders, and the average scores obtained from these arrangements are recorded as the final results.

Training DetailsSince the evaluation dataset consists of question-answer pairs, the model under evaluation needs to undergo the supervised fine-tuning process to acquire conversational capabilities. Therefore, to obtain more reliable experimental results, the candidate pre-trained models are trained

    & ArabicMMLU &  & ACVA & ACVA &  &  \\  & (koto et al.) & & clean & all & & \\  Qwen1.5-7B & 46.41 & 38.34 & 75.17 & 75.88 & 37.16 & 54.59 \\ LLlama3-8B & 45.78 & 46.34 & 77.49 & 76.68 & 54.98 & 60.25 \\ LLaMA3-Tamed-8B & 50.17 & 46.15 & 80.17 & 78.37 & 55.94 & 62.14 \\  Jais-30B-v3 & 44.47 & 45.78 & 83.39 & 79.51 & 52.30 & 61.09 \\ Qwen1.5-32B & 55.94 & 52.01 & 79.99 & 80.07 & 49.23 & 63.45 \\ Qwen1.5-72B & 61.23 & 48.68 & 82.16 & **82.24** & 58.81 & 66.62 \\ LLlama3-70B & 65.51 & 54.78 & 83.70 & 80.25 & 60.54 & 68.96 \\ LLaMA3-Tamed-70B & 66.56 & 55.49 & 82.58 & 81.36 & **63.41** & 69.88 \\  ChatGPT 3.5 Turbo & 57.70 & 45.93 & 74.45 & 76.88 & / & / \\ GPT-4 & **72.50** & **57.76** & **84.06** & 79.43 & / & / \\   

Table 2: Evaluation of base models in a few-shot setting. The best-performing model overall is highlighted in **bold**, while the top-performing model within each group is underlined.

on an instruction fine-tuning dataset _Alpaca-Arabic-GPT4_10 which contains 50K samples, enabling them to develop normal conversational abilities that align with the evaluation plan. The proportions and volumes of these datasets vary according to the goals of the ablation studies, with specific details provided in the corresponding subsections of the studies.

**Definition of Training Strategy** For simplicity, the term _Pre-train-12B_ is used to denote the model trained on the original unaligned pre-training dataset with 12 billion tokens. _Align-12B_ refers to the model trained on an aligned pre-training dataset with 12 billion tokens. _SFT-50K_ indicates training on the instruction tuning dataset with 50K samples.

**Baselines for Arabic LLMs** Among the currently popular open-source LLMs, those with strong capabilities in the Arabic language include Jais , AceGPT , and Llama-3 . In this experiment, base models are directly employed to generate responses on the BeaverTails dataset for evaluating their safety and usefulness. This aims to explore the degree of value alignment in different Arabic pre-trained language models. We employed ChatGPT-4o as the baseline model, assessing the performance of other models by comparing their harmlessness and helpfulness ratios relative to the baseline.

Benchmarking resultsThe experimental results in Figure 5 indicate that Llama-3-8B  surpasses other pre-trained models in both harmlessness and helpfulness, suggesting that it is originally trained on a highly secure dataset aligned with human values. Despite the relatively smaller room for improvement in Llama-3, we still opt to use native alignment to further enhance the model's safety and reliability in Arabic. To comprehensively assess the effectiveness of the alignment method, the optimal results achieved through native alignment in the ablation experiment are prominently displayed. These results demonstrate that our method significantly enhances the model's harmlessness and helpfulness, with observed improvements of 10.4% and 4.8% respectively. This enhancement not only makes the model safer but also ensures it is more closely aligned with human values, thus highlighting the substantial impact of our alignment strategy on improving model behavior.

### Native Alignment vs. Conventional Pre-training (RQ 1)

To clarify the effectiveness of native alignment over conventional pre-training, we conduct a simple ablation study on Llama-3-8B  to compare the performance of different data composition settings on the same LLM. The first setting uses only the original unaligned pre-training data, as is typical in most pre-training work. The second setting uses the same quantity of data but replaces it with alignment data collected specifically for model training 11. The third setting involves training on alignment data following the training on the original data.

As shown in the left histogram in Figure 6, using the first setting as a baseline, the other two settings show significant improvements in both harmlessness and helpfulness, indicating the enhancement brought by the alignment data for the base model's safety and knowledge. Furthermore, the setting that trains on alignment data after the original data outperforms training solely on alignment data. This demonstrates that the two different pre-training data settings are not conflicting but rather mutually beneficial.

Figure 5: The ratio of metrics for base models relative to ChatGPT-4o on the BeaverTails dataset.

Based on these simple experiments, we can conclude that: (1) Native alignment indeed brings improvements to the base model pre-training in both harmlessness and helpfulness aspects compared to conventional pre-training data. (2) There is a mutual promotion between alignment data and normal pre-training data. The experimental results show that using both types of data in model pre-training can achieve the best utilization of the collected data.

### Scaling law of Native Alignment (RQ 2)

Compared to instruction tuning and reinforcement learning, the volume of pre-training data is usually quite large. This leads to a pertinent question: _is it necessary to bear substantial costs to realign the entire pre-training corpus?_ Alternatively, does the alignment process hit a plateau once a certain data volume is reached? To explore this topic in-depth, an experiment is conducted by using a model initially trained on an original dataset with 12 billion tokens as the baseline. We then increase the volume of the aligned dataset to obtain multiple models and subsequently fine-tune these models using the instruction tuning dataset. This experiment is designed to explore the scaling laws of aligned datasets, offering insights for future proportions of rewritten datasets.

According to the results shown in the right bar graph of Figure 6, models trained initially without aligned data exhibit increasing levels of harmlessness and helpfulness as the amount of alignment data is augmented. Additionally, the trends observed in the results indicate that the increase in harmlessness is gradual, which may be due to Llama-3  already being a model that excels in aligning with human values, thus showing relatively less significant improvements in harmlessness. On the other hand, helpfulness rises sharply with the increase in the volume of alignment data, and this rate of increase continues to accelerate.

So, based on the result, we can understand that: the alignment dataset plays a crucial role in continuously refining the model's values. By expanding the volume of the alignment dataset, the model becomes safer and more helpful, ultimately enhancing its ability to generate responses that align closely with human values.

## 5 Related Work

### Pre-training Data Processing

Pre-training data processing plays a crucial role in enhancing language model performance and expanding applicability across various tasks. Studies such as Penedo et al.  demonstrate the advantages of web-mined data over traditional corpora through advanced processing techniques like deduplication, language identification, and quality filtering, resulting in significant performance gains. Similarly, works by Gunasekar et al.  and others [36; 37] highlight that combining LLM-based filtering of web data with synthetic data generation enables smaller models to achieve performance typically seen in larger counterparts, though the computational overhead can limit its broader use.

Figure 6: **The left graph** illustrates the metric improvements under various training strategies. **The right graph** demonstrates the performance gains as the volume of alignment data increases. In both graphs, the baseline model, ‘_Pre-train-12B + SFT-50K_’, is initially trained on 12 billion tokens from an unaligned dataset and later fine-tuned using instruction-tuning datasets with 50,000 samples.

Several studies, including Raffel et al.  and Kreutzer et al. , emphasize the importance of data quality for transfer learning and multilingual models. Raffel demonstrates that strategic preprocessing can improve performance across tasks, while Kreutzer's manual audit of web-crawled data reveals the critical role of quality control in multilingual model robustness. Additionally, Maini et al.  propose the Web Rephrase Augmented Pre-training (WRAP), where an instruction-tuned model paraphrases web documents into different styles, effectively boosting pre-training efficiency, reducing perplexity, and enhancing zero-shot accuracy.

When comparing data cleaning and native alignment, we observe that they serve different but complementary roles in language model development. Data cleaning efforts such as RefinedWeb , SlimPajama , and WRAP  focus on improving data quality by filtering, deduplicating, or reformatting web content into various stylistic formats like 'Wikipedia' or 'question-answer'. These conventional methods primarily remove low-quality content or polish data formats [40; 41; 42]. In contrast, native alignment not only enhances data quality but also aligns the model's outputs with human preferences, making it an extension of traditional cleaning processes. An experiment comparing native alignment with conventional data cleaning (e.g., RefinedWeb) is presented in Appendix E.

Collectively, these studies illustrate evolving data processing strategies that tackle both quality and value alignment, offering opportunities to improve model safety and performance.

### LLM Alignment

Alignment refers to ensuring that LLMs act in accordance with user intentions, meaning they are helpful, honest, and harmless . As shown by Wang et al. , aligning LLMs involves three key components: data collection, training methodologies, and model evaluation. This is particularly important because pre-training data can contain unaligned content, such as ethical issues or religious sensitivities, which may conflict with human values. This is especially critical in culturally sensitive regions, such as the Arabic world.

Many alignment methods focus on post-training adjustments, such as instruction tuning [6; 5], and reinforcement learning from human feedback (RLHF) [7; 3]. RLHF involves using human feedback to fine-tune models after pre-training, aligning them with user preferences to ensure they behave appropriately. However, this process is resource-intensive, requiring extensive human input. To address this, RLAIF (Reinforcement Learning from AI Feedback)  proposes using LLM-generated feedback instead of human feedback, which has shown promising results  in improving scalability.

The difference between post-alignment and native alignment lies in their timing and focus. Post-alignment, like RLHF, occurs after pre-training on both aligned and unaligned data, working to correct undesirable behavior. Native alignment, however, operates during the pre-training phase, filtering out unaligned content from the outset. By proactively preventing the inclusion of problematic data, native alignment is often more efficient and cost-effective. As the saying goes, "An ounce of prevention is worth a pound of cure," indicating that addressing issues early in the process can reduce the complexity and cost of post-training corrections. A comparative experiment between native and post-alignment methods is provided in Appendix D.

## 6 Conclusion

In this paper, we introduced '_native alignment_', a novel approach for aligning LLMs with human preferences during the pre-training phase. Unlike traditional alignment strategies that occur during instruction tuning or reinforcement learning, known as '_post-alignment_', our method integrates alignment processes earlier in the training pipeline. We outlined a comprehensive data processing workflow that emphasizes knowledge preservation, content moderation, text fluency, and controversial issue avoidance. Through extensive experiments and evaluations focusing on the Arabic language, we demonstrated significant improvements in pre-training data quality, resulting in models that are both safer and more helpful. Ablation studies confirmed that combining native alignment data with traditional pre-training data yields superior results, enhancing the harmlessness and helpfulness of models. Moreover, our practical application of this approach led to the development of the state-of-the-art Arabic LLM, LLaMA3-Tamed-70B. Together with the smaller version, LLaMA3-Tamed-8B,this advancement is highly beneficial for the Arabic LLM community. We are committed to furthering research in this area and will open source our code, data and models to foster collaboration and innovation within the community.

## Limitations

Our work has limitations: (1) The absence of a suitable and fair benchmark for evaluating alignment prevents direct comparison with existing post-alignment methods, which is why we did not use other related alignment work as baselines. Our pre-training alignment method, unique in its application stage, does not interfere with other alignment methods, allowing for simultaneous coexistence within the same model. Despite this, we still conduct a simple experiment to compare post-alignment approaches and native alignment in an unfair, non-apple-to-apple setting for the reader's reference, see Appendix D for more details. (2) Our case study focuses on Arabic LLMs, but the full potential of the proposed approach, such as its instruction-following capabilities, remains untested as it is more related to the quality of instruction data rather than pre-training data. (3) Another limitation involves hallucinations. Although the overall hallucination ratio in our model's outputs, where hallucinations are inherited from the original data to the rewritten data, is found to be within acceptable bounds based on a manual review of 90 sample pairs, addressing hallucinations in native alignment remains a challenge and is beyond the scope of this work. We plan to explore solutions for this issue in future work.

## Author Contributions

This work was conducted under the platform of the KAUST-SRIBD Joint Lab on Scientific Computing and Machine Learning. We would like to acknowledge the support of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No. HZQSWS-KCCYB-2024016), the Shenzhen Science and Technology Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS2022100809330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Key Laboratory of Cross-Modal Cognitive Computing (grant number ZDSYS20230626091302006), Shenzhen Stability Science Program 2023, Shenzhen Key Lab of Multi-Modal Cognitive Computing, and KAUST Baseline Research Fund.