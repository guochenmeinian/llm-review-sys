# Structured Multi-Track Accompaniment Arrangement

via Style Prior Modelling

 Jingwei Zhao\({}^{1,3}\) **Gus Xia\({}^{4,5}\)**Ziyu Wang\({}^{5,4}\)**Ye Wang\({}^{2,1,3}\)**

\({}^{1}\)Institute of Data Science, NUS \({}^{2}\)School of Computing, NUS

\({}^{3}\)Integrative Sciences and Engineering Programme, NUS Graduate School

\({}^{4}\)Machine Learning Department, MBZUAI \({}^{5}\)Computer Science Department, NYU Shanghai

jzhao@u.nus.edu gus.xia@mbzuai.ac.ae ziyu.wang@nyu.edu wangye@comp.nus.edu.sg

###### Abstract

In the realm of music AI, arranging rich and structured multi-track accompaniments from a simple lead sheet presents significant challenges. Such challenges include maintaining track cohesion, ensuring long-term coherence, and optimizing computational efficiency. In this paper, we introduce a novel system that leverages _prior modelling over disentangled style factors_ to address these challenges. Our method presents a two-stage process: initially, a piano arrangement is derived from the lead sheet by retrieving _piano texture_ styles; subsequently, a multi-track orchestration is generated by infusing _orchestral function_ styles into the piano arrangement. Our key design is the use of vector quantization and a unique multi-stream Transformer to model the _long-term_ flow of the orchestration style, which enables flexible, controllable, and structured music generation. Experiments show that by factorizing the arrangement task into interpretable sub-stages, our approach enhances generative capacity while improving efficiency. Additionally, our system supports a variety of music genres and provides style control at different composition hierarchies. We further show that our system achieves superior coherence, structure, and overall arrangement quality compared to existing baselines.

## 1 Introduction

Representation learning techniques have enabled new possibilities for controllable generative modelling. By learning _implicit_ style representations, which are often hard to explicitly label (_e.g._, timbre of music audio , texture of music composition , and artistic style in paintings ), new music and artworks can be created via style transfer and latent space sampling. These learned style factors can also serve as external controls for downstream generative models, including Transformers  and diffusion models . However, applying style factors to _long-term_ sequence generation remains a challenging task. Existing approaches rely on style templates specified manually or by heuristic rules , which are impractical for long-term generation. Moreover, when structural constraints are imposed, misaligned style factors can result in incoherent outputs.

To address these challenges, we aim to develop a novel sequence generation framework leveraging a global style planner, or _prior_, which models the conditional distribution of _style factors_ given the model input's _content factors_. Both style and content factors are sequences of compact, structurally aligned latent codes over a disentangled representation space. By infusing the style back to the content, we can recover the observational target with globally coherent style patterns.

In this paper, we study _style prior modelling_ through the task of _multi-track accompaniment arrangement_, a typical scenario for long-term conditional sequence generation. We assume the input of apiano accompaniment score, which typically carries a verse-chorus structure. Our target is to generate corresponding multi-track arrangements featuring band orchestration. We start by disentangling a band score at time \(t\) into _piano reduction_\(_{t}\) (content factor) and _orchestral function_\(_{t}^{k}\) (style factors for individual tracks \(k=1,2,,K\)). On top of this, we model the prior of _finding appropriate functions to tochestrate a given piano score_, or formally \(p(_{1:T}^{1:K}_{1:T})\). To model dependencies in both time (\(T\)) and track (\(K\)) directions, we develop a multi-stream Transformer with interleaved time-wise and track-wise layers. The track-wise layer allows for flexible control over the choice of instruments and the number of tracks, while the time-wise layer ensures structural alignment through cross-attention to the _piano reduction_. Decoding the inferred \(_{1:T}^{1:K}\) with \(_{1:T}\), we can address accompaniment arrangement in a flexible _multi-track_ form with extended _whole-song_ structure.

Experiments show that our method outperforms existing sequential token prediction approaches and provides better multi-track cohesion, structural coherence, and computational efficiency. Additionally, compared to existing designs of multi-stream language models, our model handles flexible stream combinations more effectively with enhanced generative capacity.

To summarize, our contributions in this paper are three-folded:

* We propose **style prior modelling, a hierarchical generative methodology** addressing both long-term structure (via style prior at high level) and fine-grained condition/control (via representation disentanglement at low level). Our approach moves beyond the limitation of manual specification of style factors, providing a flexible, efficient, and self-supervised solution for long-term sequence prediction and generation tasks.
* We propose a novel **layer interleaving architecture** for multi-stream language modelling. In our case, it models parallel music tracks with a flexible track number, controllable instruments, and manageable computation. To our knowledge, it is the first multi-stream language model with tractable generalization to flexible stream combinations.
* Integrating our previous study on _piano texture_ style transfer [39; 50], we present a **complete music automation system** arranging an input lead sheet (a basic music form with melody and chord only) via piano accompaniment to multi-track arrangement. The entire system is interpretable at two composition hierarchies: 1) _piano texture_ and 2) _orchestral function_, and demonstrates state-of-the-art arrangement performance for varied genres of music.1

## 2 Related Works

In this section, we overview three topics related to our study. Section 2.1 reviews existing studies on representation disentanglement. Section 2.2 summarizes prior modelling methods in music generation. Section 2.3 reviews the current progress with the task of accompaniment arrangement.

### Content-Style Disentanglement via Representation Learning

Representation disentanglement is a popular technique in deep generative modelling [3; 16; 48; 49]. In the music domain, this approach has proven valuable by learning compositional factors related to music _style_ and _content_. By manipulating these factors through interpolation , swapping , and prior sampling , it provides a self-supervised and controllable pathway for various music automation tasks. Recent works leverage disentangled style factors as control signals for long-term music generation [36; 42]. However, these approaches typically treat style representations as fixed condition sequences during training, requiring manual specification or additional algorithms for control during inference. In contrast, we model the prior of _the style to apply_ conditional on the given music content, which is a more generalized and flexible approach.

### Music Generation with Latent Prior

In sequence generation tasks (_e.g._, music and audio), learning a prior sampler over a compact, latent representation space is often more efficient and effective. Jukebox  models the latent codes encoded by VQ-VAEs  as music priors, which can further reconstruct minutes of music audio. More recently, MusicLM  and MusicGen  learn multi-modal priors for generating music from text prompts. While prior modelling facilitates long-term generation, the latent codes in these works are not interpretable, thus lacking a precise control by music content-based signals (_e.g._, music structure). Such controls are essential for conditional generation tasks, including accompaniment arrangement. In this paper, we model a _style prior_ conditional on the disentangled music content, which allows for structured long-term music generation, enhancing both interpretability and controllability.

### Accompaniment Arrangement

Accompaniment arrangement aims to compose the accompaniment part given a lead sheet, which is a difficult conditional generation task involving structural constraints. Existing methods mainly train a conditional language model based on sequential note-level tokenization [14; 15; 30; 33], which often suffer from slow inference speed, truncated structural context, and/or simplified instrumentation. Recent attempts with diffusion models show higher sample quality with faster inference [23; 26; 27], but still consider limited instruments or tracks. AccoMontage [47; 50] maintains a whole-song structure by manipulating high-level composition factors, but is limited to piano arrangement alone. Our paper presents a two-stage approach: from lead sheet to piano accompaniment, and from piano to multi-track, both leveraging prior modelling of high-level style factors. This approach offers modularity  and enables high-quality _whole-song_ and _multi-track_ accompaniment arrangement.

## 3 Method

We develop a model that takes a _piano reduction_ as input and outputs an orchestrated multi-track arrangement. Using an autoencoder, we disentangle a multi-track music score into its _piano reduction_ (content factor) and _orchestral function_ (style factor). We then design a prior model to infer _orchestral functions_ given the _piano reduction_. The autoencoder operates at segment level, while the prior model works on the whole song. The entire model can operate as an orchestrator module in a complete arrangement system. In this section, we introduce our data representation in Section 3.1, autoencoder framework in Section 3.2, and prior model design in Section 3.3.

### Data Representation

We summarize our data representations in Table 1. Let \(\) be a \(K\)-track arrangement score. We split it into \(T\) segments and represent \(_{t}^{k}\) -- each segment track -- as a matrix of shape \(P N\). Here \(P=128\) represents 128 MIDI pitches and \(N\) is the time dimension of a segment. This matrix representation aligns with the modified piano roll in , where each non-zero entry \((p,n)>0\) indicates a note onset and its value indicates the note duration. In this paper, we primarily focus on music pieces in 4/4 time signature with 1/4-beat resolution. Duration values range from 1 (for sixteenth notes) to 32 (for double whole notes). We consider 1 segment = 8 beats (2 bars) and derive \(N=32\), which is a proper scale for learning music content/style representations [37; 39; 40; 41; 46].

The _piano reduction_ of \(\) is notated as \([]\). It is approximated by downmixing all \(K\) tracks into a single-track mixture similar to . When concurring notes are found across tracks, we keep the one with the largest duration (_i.e._, track-wise maximum). Segment-wise, \([]_{t}\) is also a \(P N\) matrix. It preserves the overall music content while discarding the multi-track form.

The _orchestral function_ of \(\) is notated as \([]\). It describes the rhythm and grooving patterns  of each segment track, which serves as the "skeleton" of a multi-track form. Formally,

\[[]_{t}^{k}=(_{\{_{t}^ {k}>0\}})/,\] (1)

where indicator function \(_{\{\}}\) counts each note onset position as \(1\); \(()\) sums up the pitch dimension, deriving a \(1 N\) time-series feature; \(=14\) is for normalization. The _orchestral

    & **Multi-Track Arrangement** & **Piano Reduction** & **Orchestral Function** \\ 
**Data Representation** & \([0..32]^{T K 32 128}\) & \([][0..32]^{T 32 128}\) & \([]^{T K 32}\) \\ 
**Latent Dimension** & \(^{T K 256}\) & \(^{T 256}\) & \(s[0..127]^{8T K}\) \\   

Table 1: Summary of the data representations applied in this paper. We use notation \([a..b]\) to denote the integer interval \(\{x\ |\ a x b,x\}\) including both endpoints.

function_\([]\) essentially describes the form, or layout, of multi-track music \(\). It indicates the rhythmic intensity of parallel tracks and informs where to put more notes and where to keep silent.

### Autoencoder

Our autoencoder consists of two components as shown in Figure 1. A VQ-VAE submodule (right of Figure 1) encodes _orchestral function_\([]_{t}^{k}\). A VAE module (left of Figure 1) encodes _piano reduction_\([]_{t}\) and reconstructs individual tracks \(_{t}^{1:K}\) leveraging the cues from \([]_{t}^{1:K}\). During training, both inputs \([]\) and \([]\) are deterministic transforms from the output \(\) and the entire model is self-supervised. We see similar techniques for representation disentanglement in .

The VQ-VAE consists of Function Encoder \(^{}\) and Decoder \(^{}\). Encoder \(^{}\) contains a 1-D convolutional layer followed by a vector quantization block. Our intuition for applying a VQ-VAE is that _orchestral function_ commonly consists of rhythm patterns (such as syncopation, arreggio, _etc._) that can naturally be categorized as _discrete_ variables. In our case, each segment track is encoded into 8 discrete embeddings on a 1-beat scale, indicating the flow of orchestration style. Formally,

\[_{t}^{k}:=\{s_{}^{k}\}_{= t-7}^{ t}=^{ }([]_{t}^{k}),\:k=1,2,,K,\] (2)

where \(s_{}^{k}\) is the latent _orchestral function_ code for the \(k\)-th track at the \(\)-th beat. We encode \([]_{t}^{k}\) at a finer 1-beat scale (instead of segment) to preserve fine-grained rhythmic details. The new scale is re-indexed by \(=1,2,,8T\). We collectively denote each 8-code grouping as \(_{t}^{k}\) for conciseness.

The VAE consists of Piano Encoder \(^{}\), Track Separator \(\), and Track Decoder \(^{}\). Encoder \(^{}\) learns content representation \(_{t}\) from _piano reduction_\([]_{t}\). Here \(_{t}\) is a _continuous_ representation (without vector quantization) that captures more nuanced music content. Decoder \(^{}\) reconstructs individual tracks \(_{t}^{k}\) from track representation \(_{t}^{k}\). Notably, \(_{t}^{1:K}\) are recovered from \(_{t}\) using the _orchestral function_ cues from \(_{t}^{1:K}\). Formally,

\[_{t}^{1},_{t}^{2},,_{t}^{K}= (_{t}^{1},_{t}^{2},,_{t}^{K}_{t}),\] (3)

where Track Separator \(\) is a Transformer encoder. In this process, each \(_{t}^{k}\) queries \(_{t}\) to recover the corresponding track (\(k\)), while they also attend to each other to maintain the dependency among parallel tracks. Learnable instrument embeddings  are added to each track based on its instrument class. We provide details of the autoencoder architecture in Appendix A.1.

### Style Prior Modelling

The VQ-VAE in Section 3.2 derives latent codes \(s_{1:8T}^{1:K}\) for _orchestral function_ as a multi-stream time series. Here \(k=1,2, K\) is the stream (track) index and \(=1,2,,8T\) is the time (beat) index. The purpose of _style prior modelling_ is to infer _orchestral function_ given _piano reduction_ so that the former can be leveraged to orchestrate the latter into multi-track music. We design our prior model as shown in Figure 2. It is an encoder-decoder framework that models parallel tracks/streams of _orchestral function_ codes conditional on the _piano reduction_.

The decoder module (right of Figure 2) has alternate layers of Track Encoder and Auto-Regressive Decoder. Track Encoder is a standard Transformer encoder layer  and it aggregates inter-track information along the track axis. Auto-Regressive Decoder is a Transformer decoder layer (with self-attention and cross-attention) and it predicts next-step _orchestral function_ codes on the time axis.

Figure 1: The autoencoder architecture. It learns content representation \(_{t}\) from _piano reduction_, style representations \(_{t}^{1:K}\) from _orchestral function_, and leverages both to reconstruct individual tracks.

By orthogonally stacking two types of layers, we can model track-wise and time-wise dependencies simultaneously with a manageable computational cost. Compared to sequential tokenization methods in previous studies [9; 30; 36], our method brings down the complexity from \((N^{2}T^{2})\) to \(((N,T)^{2})\). Moreover, we support a flexible multi-track form (\(N\) being variable) with a diverse instrumentation option. We add instrument embedding  and relative positional embedding  to the track axis, where 34 instrument classes  are supported. We add music timing condition  to the time axis, which encodes the positions in a training excerpt as fractions of the complete song, helping the model capture the overall structure of a song.

The encoder module (left of Figure 2) of our prior model is a standard Transformer encoder, which takes _piano reduction_\(_{1:T}\) as global context. It is connected to the decoder module via cross-attention and maintains the global phrase structure. During training, both \(_{1:T}\) and \(s_{1:ST}^{1:K}\) are derived from the same multi-track piece and the entire model is self-supervised. Let \(p_{}\) be the distribution of _orchestral function_ codes fitted by our prior model \(\), the training objective is the mean of negative log-likelihood of next-step code prediction:

\[()=-_{k=1}^{K} p_{}(s_{}^{k} s _{<}^{1:K},_{1:T}).\] (4)

We provide more implementation details of the prior model in Appendix A.2. We note that there is a potential domain shift from our approximated _piano reduction_ to real piano arrangements. To prevent overfitting, we use a Gaussian noise \(\) to blur \(_{1:T}\) while _preserving its high-level structure_. During training, \(\) is combined with \(_{1:T}\) using a weighted summation with noise weight \(\) ranging from 0 to 1. It encourages a partial unconditional generation capability. At inference time, \(\) is a parameter that can balance creativity with faithfulness. An experiment on \(\) is covered in Appendix C.

## 4 Whole-Song Multi-Track Accompaniment Arrangement

We finalize a complete music automation system by applying _style prior modelling_ at two cascaded stages. As shown in Figure 3, our autoencoder and _orchestral function prior_ operate on Stage 2 for _piano to multi-track_ arrangement. On Stage 1, we adopt our previous study, a _piano texture prior_

Figure 3: A complete accompaniment arrangement system based on cascaded prior modelling. The first stage models _piano texture_ style given lead sheet while the second stage models _orchestral function_ style given piano. Besides modularity, the system offers control on both composition levels.

Figure 2: The prior model architecture. The overall architecture is an encoder-decoder Transformer, while the decoder module is interleaved with orthogonal time-wise and track-wise layers.

on top of chord/texture representation learning , for _lead sheet to piano_ arrangement. Given a lead sheet, the first stage generates a piano accompaniment, establishing the rough whole-song structure. Our system then orchestrates the piano accompaniment into a complete multi-track arrangement with band instrumentation. This two-stage approach mirrors musicians' creative workflow  and allows for control at both composition levels. In particular, we provide three control options:

1. Texture Selection: To filter _piano textures_ on Stage 1 by metadata and statistical features.
2. Instrumentation: To customize the _track number_ and _choice of instruments_ on Stage 2.
3. Orchestral Prompt: To prompt the orchestration process with an _orchestral function_ template.

We showcase an arrangement example by the complete system in Figure 4. The system input is a lead sheet shown by the Mel staff. The final output is the accompaniment in the rest staves. Notably, the lead sheet consists of 60 bars in a structure of i4A8B8B8x4A8B8B04 (using notations by ). Here, i4, x4, and 04 each denote a 4-bar intro, interlude, and outro. A8 and B8 represent an 8-bar verse and chorus, respectively. Figure 4 shows the arrangement result for the first and third choruses, spanning from bar 13 to 41. We leverage control option 2 to customize the instrumentation as celesta, acoustic guitars (2), electric pianos (2), acoustic piano, violin, brass, and electric bass in a total of \(K=9\) tracks. The complete arrangement score is included in Appendix E.

In Figure 4, we observe some multi-track arrangement patterns that are common in practice. Purple blocks highlight a counterpoint relation between guitar track A.G.1 and electric piano track E.P.1.

Figure 4: Arrangement for _Can You Feel the Love Tonight_, a pop song in a total of 60 bars. We show two chorus parts from bar 13 to 41. We use red dotted boxes to show coherence in long-term structure. We use coloured blocks to show naturalness and cohesion in multi-track arrangement.

Green blocks show two guitar tracks with complementary orchestral functions: one melodic (A.G.1) and the other harmonic (A.G.2). Yellow blocks illustrate the metrical division between the string (Vlns.) and the brass (Br.) sections, with strings on the downbeat and brass on the offbeat. These patterns demonstrate a natural and cohesive multi-track arrangement by our system. Moreover, we see consistent accompaniment patterns echoing in both chorus parts that span over 30 bars (shown by red dotted boxes), while the latter adds a piano apreggio track (Pno.) to enhance the musical flow. This demonstrates structured whole-song arrangement over extended music contexts.

## 5 Experiment

In this section, we evaluate the performance of our multi-track accompaniment arrangement system. Given that existing methods primarily focus on _lead sheet to multi-track_ arrangement, we ensure a fair comparison by using the two-stage approach discussed in Section 4. In Section 5.1, we present the datasets used and the training details of our model. In Section 5.2, we describe the baseline models used for comparison. Our evaluation is divided into two parts: objective evaluation, detailed in Section 5.3, and subjective evaluation, covered in Section 5.4. For the single-stage _piano to multi-track_ (Stage 2) and _lead sheet to piano_ (Stage 1) arrangement tasks, we perform additional comparisons with various ablation architectures in Section 5.5 and 5.6, respectively.

### Datasets and Training Details

We use two datasets to train the autoencoder and the style prior, respectively. The autoencoder is trained with Slahk2100 , which contains 2K curated multi-track pieces with 34 instrument classes in a balanced distribution. We discard the drum track and clip each piece into 2-bar segments with 1-bar hop size. We use the official training split and augment training samples by transposing to all 12 keys. The autoencoder comprises 19M learnable parameters and is trained with batch size 64 for 30 epochs on an RTX A5000 GPU with 24GB memory. We use Adam optimizer  with a learning rate from 1e-3 exponentially decayed to 1e-5. We use exponential moving average (EMA)  and random restart  to update the codebook with commitment ratio \(=0.25\).

We use Lakh MIDI Dataset (LMD)  to train the prior model. It contains 170k music pieces and is a benchmark dataset for training music generative models. We collect 2/4 and 4/4 pieces (110k after processing) and randomly split LMD at song level into training (95%) and validation (5%) sets. We further clip each piece into 32-bar training excerpts (_i.e._, \(T=16\) at maximum) with a 4-bar hop size. Our prior model has 30M parameters and is trained with batch size 16 for 10 epochs (600K iterations) on two RTX A5000 GPUs. We apply AdamW optimizer  with a learning rate of 1e-4, scheduled by a 1k-step linear warm-up followed by a single cycle of cosine decay to a final rate of 1e-6.

For model inference and testing, we consider two additional datasets: Nottingham  and WikiMT . Both datasets contain lead sheets (in ABC notation) that are not seen during training or validation. Moreover, they cover diverse music genres including folk, pop, and jazz. When arranging a piece, we leverage control option 2 to set up the instrumentation. Without loss of generality, this control choice is randomly sampled from Slakh2100 validation/test sets. To arrange music longer than the prior model's context length (32 bars), we use windowed sampling , where we move ahead our context window by 4 bars and continue sampling based on the previous 28 bars. We apply nucleus sampling  with top probability \(=0.05\) and temperature \(=6\).

### Baseline Models

We compare our system with three existing methods: PopMAG , Anticipatory Music Transformer (AMT) , and GETMusic . PopMAG and GETMusic generate multi-track accompaniments from an input lead sheet based on a Transformer and a diffusion model, respectively. AMT is Transformer-based and it continues the accompaniment part from an input melody with starting accompaniment prompt. We provide detailed configurations in the following.

**PopMAG** is an encoder-decoder architecture based on Transformer-XL . It represents multi-track music by sequential note-level tokenization and is fully supervised. The encoder takes a lead sheet as input and the decoder generates multi-track accompaniment auto-regressively. Since the model is not open source, We reproduce it on LMD with lead sheets extracted by  (melody) and  (chord).

**Anticipatory Music Transformer (AMT)** is a decoder-only Transformer architecture with note-level tokenization. It introduces an "anticipation" method, where conditional tokens (melody and starting prompt) and generative tokens (accompaniment continuation) are interleaved to train a conditional generative model. Since our testing dataset does not provide ground-truth accompaniment, the starting prompt is given by the generation result (first 2 bars) of our system. We use the official implementation of the AMT model,2 which is also trained on LMD.

**GETMusic** represents multi-track music as an image-like matrix resembling score arrangement, based on which a denoising diffusion probabilistic model is trained with a mask reconstruction objective. Given a lead sheet, it supports generating 5 accompaniment tracks using piano, guitar, string, bass, and drum or their subsets. In our experiment, we generate all 5 accompaniment tracks. We use the official implementation of the GETMusic model,3 which is trained on internal data.

### Objective Evaluation on Multi-Track Arrangement

We introduce four metrics to evaluate multi-track arrangement performance: _chord accuracy_[23; 30], _degree of arrangement_ (_DOA_), _structure awareness_, and _inference latency_. Among them, _chord accuracy_ measures the multi-track harmony that reflects the fitness of the accompaniment to the lead sheet; _DOA_ measures inter-track tonal diversity that reflects the creativity of the instrumentation. Both metrics demonstrate music cohesion at local scales. On the other hand, _structure awareness_ measures phrase-level content similarity that reflects long-term structural coherence of the whole song. Finally, we use _inference latency_ (in second/bar) to evaluate computational efficiency of each method. The detailed computation of each metric is provided in Appendix B. In Table 2, we compute ground-truth _DOA_ using 1000 random pieces from LMD. We compute ground-truth _structure awareness_ using 857 pieces in 4/4 from POP909 Dataset .

We randomly sample 50 pieces in 4/4 time signature from Nottingham and WikiMT respectively (100 pieces in total) to conduct experiment. The length of each piece ranges from 16 to 32 bars. We run our method and baseline models at each piece in 3 independent rounds, deriving 300 sets of multi-track arrangement samples. In Table 2, we report the evaluation results with mean value, standard error of mean (\(\)), and statistical significance computed by Wilcoxon signed rank test . We find significant differences between our method and all baselines (p-value \(p\) < 0.05/6, using Bonferroni correction). In particular, our method outperforms in _chord accuracy_, _structure awareness_, and _DOA_, indicating the capability of arranging harmonious, structured, and creative accompaniments. The diffusion baseline outperforms in _inference latency_ as it applies only 100 diffusion steps. Our method's efficiency is on par with it, while being 10 times faster than vanilla note-level auto-regression.

### Subjective Evaluation on Multi-Track Arrangement

We also conduct a double-blind online survey to test music quality. Our survey consists of 5 evaluation sets, each containing an input lead sheet followed by 4 arrangement samples by our method and each baseline. Each sample is 24-32 bars long and is synthesized to audio at 90 BPM (~1 minute per sample). Both the set order and the sample order in each set are randomized. We request participants to listen to 2 sets and evaluate the musical quality based on a 5-point Likert scale from 1 to 5.

  
**Model** & **Chord Acc. \(\)** & **DOA**\(\) & **Structure**\(\) & **Latency**\(\) \\  Ours & \( 0.014^{a}\) & \( 0.004^{a}\) & \( 0.030^{a}\) & \(0.461 0.005^{b}\) \\ AMT  & \(0.446 0.013^{bc}\) & \(0.294 0.006^{a}\) & \(1.094 0.009^{c}\) & \(6.320 0.212^{d}\) \\ GETMusic  & \(0.423 0.012^{c}\) & \(0.225 0.007^{c}\) & \(1.243 0.017^{b}\) & \( 0.002^{a}\) \\ PopMAG  & \(0.470 0.013^{b}\) & \(0.270 0.007^{b}\) & \(1.086 0.008^{c}\) & \(0.638 0.013^{c}\) \\  Ground-Truth & - & \( 0.009\) & \( 0.019\) & - \\   

Table 2: Objective evaluation results for _lead sheet to multi-track_ arrangement (Section 5.3). All entries are of the form \(^{s}\), where \(s\) is a letter. Different letters within a column indicate significant differences (_p_ < 0.05/6) based on Wilcoxon signed rank test with Bonferroni correction.

The evaluation is based on 5 criteria: 1) _Harmony and Texture Coherency_, 2) _Long-Term Structure_, 3) _Naturalness_, 4) _Creativity_, and 5) _Overall Musicality_.

A total of 23 participants (8 female and 15 male) with diverse musical backgrounds have completed our survey, with an average completion time of 22 minutes. The mean ratings and standard errors, computed by within-subject (repeated-measures) ANOVA , are presented in Figure 5. Significant differences are observed across all criteria (p-value \(p\) < 0.05). Notably, our method outperforms all baselines on each criterion, aligning with the results from the objective evaluation.

### Ablation Study on Style Prior Architecture

We now validate our design with the prior model by exclusively evaluating on the _piano to multi-track_ arrangement task. Our prior model is based on a unique _layer interleaving_ design, which enables multi-stream time series modelling with explicit stream-wise attention. We compare it with two other multi-stream architectures: 1) _Parallel_: summing up parallel code embeddings for joint language modelling , and 2) _Delay_: leveraging a 1-step delay code interleaving to catch implicit stream-wise dependency . Both _Parallel_ and _Delay_ are trained under the same setup as our model. We additionally introduce 3) _Random_: a naive prior based on random template retrieval. The templates are sampled every 2 bars with shared instrumentation from the validation/test sets of Slakh2100.

We introduce two metrics to evaluate _piano to multi-track_ arrangement: _faithfulness_ and _degree of arrangement_ (_DOA_). _Faithfulness_ measures if the generated arrangement faithfully reflects the original content from the piano. It computes the similarity between i) the input piano, and ii) the _piano reduction_ of the generated multi-track arrangement. In our case, we compute cosine similarity over two features: a _statistical (stats.)_ pitch class histogram  and a _latent_ texture representation , which emphasize tonal and rhythmic similarity, respectively. _DOA_ measures the creativity as defined in Section 5.3. We also report the _NLL_ loss for our model, _Parallel_, and _Delay_.

We conduct experiments using the test set of POP909 , which consists of 88 piano arrangement pieces. In our experiment, we use the first section of each piece, which contains 2 to 4 complete phrases totally spanning 24 to 32 bars. We use control option 3 to prompt our model, _Parallel_, and _Delay_ with the same 2-bar _orchestral function_ template (sampled from Slakh2100) and see how it is developed. We report mean value, standard error of mean (\(\)), and statistical significance in Table 3 and find significant differences in both _faithfulness_ and _DOA_. We also conduct a subjective evaluation in the same setup as Section 5.4, with the results presented in Figure 6. Here we consider an additional criterion, _Instrumentation_, which reflects the well-formedness of the multi

Figure 5: Subjective evaluation results on _lead sheet to multi-track_ arrangement (Section 5.4). Figure 6: Subjective evaluation results on _piano to multi-track_ arrangement (Section 5.5).

  
**Prior** & **Faithfulness (stats.) \(\)** & **Faithfulness (latent) \(\)** & **DOA \(\)** & **NLL \(\)** \\  Ours & \(}\) & \(}\) & \(}\) & \(\) \\ Parallel & \(0.937 0.002^{b}\) & \(0.153 0.003^{b}\) & \(0.233 0.005^{c}\) & \(0.960 0.010\) \\ Delay & \(0.915 0.004^{c}\) & \(0.133 0.003^{c}\) & \(0.207 0.005^{d}\) & \(1.024 0.006\) \\ Random & \(0.913 0.003^{c}\) & \(0.113 0.003^{d}\) & \(0.262 0.005^{b}\) & - \\   

Table 3: Objective evaluation results for _piano to multi-track_ arrangement (Section 5.5). All entries are of the form \(}\), where \(s\) is a letter. Different letters within a column indicate significant differences (_p_ < 0.05/6) based on Wilcoxon signed rank test with Bonferroni correction.

track arrangement. Significant differences are observed across all criteria (p-value \(p\) < 0.05). Overall, _Parallel_ and _Delay_ both fall short in performance because they assume a preset stream combination, while in our setting, both track numbers and choices of instruments are flexible. By explicitly modelling stream-wise attention, our _layer interleaving_ design fits well to that generalized scenario.

### Ablation Study on Piano Arrangement

Now we validate our choice for the _lead sheet to piano_ arrangement module on the first stage of our two-stage system. Our choice is a _piano texture prior_ as covered in Section 4. We conduct an ablation study by replacing it with the _Whole-Song-Gen_ model , which, to our knowledge, is the only existing alternative that can handle a whole-song structure. The ablation study is conducted in the same setup as Section 5.3. In Table 4, we report _chord accuracy_, _structure awareness_, and _DOA_ regarding the final multi-track arrangement results. We further compare our _piano texture prior_ with _Whole-Song-Gen_ exclusively on the piano accompaniment arrangement task. In Table 5, we report _chord accuracy_ and _structure awareness_ regarding piano arrangement for both models. Significant differences (p-value \(p\) < 0.05) are found in all metrics based on Wilcoxon signed rank test.

By comparing Table 4 and Table 5, we can see that a higher-quality piano arrangement generally encourages a more musical and creative final multi-track arrangement result. Specifically, the piano arrangement on Stage 1 lays the groundwork for (at least) chord progression and phrase structure for Stage 2, both of which are important for capturing the long-term structure in whole-song multi-track arrangement. Moreover, we see that our _piano texture prior_ outperforms existing alternatives and guarantees a decent piano quality, thus being the best choice for our system.

## 6 Conclusion

To sum up, we contribute a music automation system for multi-track accompaniment arrangement. The main novelty lies in our proposed _style prior modelling_, a generic methodology for structured sequence generation with fine-grained control. By modelling the prior of disentangled style factors given content, we build a cascaded arrangement process: from lead sheet to _piano texture_ style, and then from piano to _orchestral function_ style. Our system first generates a piano accompaniment from a lead sheet, establishing the rough whole-song structure. It then orchestrates the piano accompaniment into a complete multi-track arrangement with band instrumentation. Extensive experiments show that our system generates structured, creative, and natural multi-track arrangements with state-of-the-art quality. At a higher level, we elaborate our methodology as _interpretable modular representation learning_, which leverages finely disentangled and manipulable music representations to tackle complex tasks with a compositional hierarchy. We hope our research brings new perspectives to broader domains of music creation, sequence data modelling, and representation learning.