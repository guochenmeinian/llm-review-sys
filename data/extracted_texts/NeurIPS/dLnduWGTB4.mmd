# Quest: Quality-Aware Metropolis-Hastings Sampling for Machine Translation

Goncalo R. A. Faria\({}^{1}\), Sweta Agrawal\({}^{2}\), Antonio Farinhas\({}^{2,3}\),

Ricardo Rei\({}^{4}\), Jose G. C. de Souza\({}^{4}\), Andre F.T. Martins\({}^{2,3,4,5}\)

\({}^{1}\)University of Washington, \({}^{2}\)Instituto de Telecomunicacoes,

\({}^{3}\)Instituto Superior Tecnico, Universidade de Lisboa, \({}^{4}\)Unbabel, \({}^{5}\)ELLIS Unit Lisbon

gfaria@cs.washington.edu

Work done while at Instituto de Telecomunicacoes.

###### Abstract

An important challenge in machine translation is to generate high-quality and diverse translations. Prior work has shown that the estimated likelihood from the MT model correlates poorly with translation quality. In contrast, quality evaluation metrics (such as COMET or BLEURT) exhibit high correlations with human judgments, which has motivated their use as rerankers (such as quality-aware and minimum Bayes risk decoding). However, relying on a single translation with high estimated quality increases the chances of "gaming the metric". In this paper, we address the problem of sampling a _set_ of high-quality and diverse translations. We provide a simple and effective way to avoid over-reliance on noisy quality estimates by using them as the energy function of a Gibbs distribution. Instead of looking for a mode in the distribution, we generate multiple samples from high-density areas through the Metropolis-Hastings algorithm, a simple Markov chain Monte Carlo approach. The results show that our proposed method leads to high-quality and diverse outputs across multiple language pairs (English\(\){German, Russian}) with two strong decoder-only LLMs (Alma-7b, Tower-7b).

## 1 Introduction

Machine translation (MT) is becoming increasingly more accurate and powerful, as it benefits from the many capabilities and acquired knowledge of large language models (LLMs) (Freitag et al., 2023; Hendy et al., 2023). However, for many domains and languages the quality of translation is still not satisfactory--for example, hallucinations or critical errors are a serious issue when translating high-risk content, as in medical and legal domains (Khoong et al., 2019; Taira et al., 2021; Shen et al., 2023; Guerreiro et al., 2023; Sanz-Valdiviso and Lopez-Arroyo, 2023; Grimm et al., 2024). Developing procedures for sampling higher-quality translations is therefore in high demand.

It is known that the output quality of the translations generated by maximizing the model likelihood is limited because of the _inadequacy of the mode_--models tend to produce distributions over outputs that are highly peaked, favoring a single hypothesis (Eikema and Aziz, 2020; Peters and Martins, 2021; Eikema, 2024); and improving search often makes things worse (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). In general, maximizing model likelihood tends to overlook hypotheses that could be equally valid and more appropriate in certain contexts. To address this limitation, a string of work has been initiated towards "quality-aware decoding", which leverages powerful quality estimation (QE) and evaluation metrics, such as COMET (Rei et al., 2022) or BLEURT (Yan et al., 2023), to explore and rerank a broader range of candidate hypotheses generated via sampling from the model's distribution, selecting the best-1 (Freitag et al., 2022; Fernandes et al., 2022; Farinhas et al., 2023) or the best-\(k\)(Jinnai et al., 2024; Singhal et al., 2023) according to these learned metrics.

Despite the benefits, reranking-based methods have their own drawbacks. There is a risk of these approaches overfitting to the metrics used, potentially leading to illusory gains in quality, as the translations obtained via optimizing these metrics may not always be preferred by humans when compared to other alternatives (Fernandes et al., 2022). Secondly, their effectiveness is limited by the quality of the initial candidate set. For instance, Vernikos and Popescu-Belis (2024) show that many high-quality translations, generated by combining translation hypotheses, are less likely to be sampled from the model's distribution even with a very large pool size.

One potential way to remedy these issues, which we explore in this paper, is to use these automatic metric proxies as the **energy function of a Gibbs distribution**. However, sampling hypotheses from this distribution presents a difficult challenge: unlike likelihood-based sampling, "quality-based sampling" from a Gibbs distribution cannot be performed autoregressively, and it is intractable to enumerate and score all possible hypotheses. We address this challenge by proposing a simple and effective Markov chain Monte Carlo approach (MCMC) method, combining the Metropolis-Hastings algorithm with a suitable proposal distribution. Our method, **Quality-Aware Metropolis-Hastings (Quest) Sampling**, uses a novel proposal distribution that is compatible with sentence-level evaluation metrics, common in text generation tasks like MT (Figure 1). We further note that while we focus on MT, where automatic QE metrics are more developed and robust, our proposed method is general and can be applied to other natural language processing (NLP) tasks. Furthermore, as our method is agnostic to the specific quality metric used in the Gibbs distribution, it can directly benefit from any future improvements in the metrics.

We show that Quest sampling results in high-quality and diverse samples on multiple test beds (WMT23 English\(\) {German, Russian}) and with multiple decoder-only LLMs (Tower-7b, Alma-7b). Our method generates many novel hypotheses from the high-density regions unlikely to be generated via ancestral sampling. Furthermore, with increasing chain size, average quality as measured by automatic metrics continues to improve, unlike ancestral sampling, where the candidate set quality remains unchanged even with a larger pool size.2

## 2 Background

### Large Language Models for Machine Translation

Generating translations from autoregressive LLMs (either encoder-decoder or decoder-only) involves conditioning the language model on a prompt \(x\), which is a sequence of tokens \((x_{1},x_{2},,x_{L})\) encoding the text to be translated coupled with a translation instruction (Raffel et al., 2020; Hendy et al., 2023). Let \(\) be a fixed vocabulary and \(=^{*}\) its Kleene closure. The joint distribution over the output translations, \(y\), given the prompt \(x\), can be factorized as the product of conditional probabilities over individual tokens \((y_{1},y_{2},,y_{N})\), where each \(y_{i}\). The probability of a

Figure 1: Quest samples an index from the current translation (\(y^{t}\)), removes all elements to the right of the index, generates a new continuation, and then uses the Metropolis-Hastings acceptance criterion to decide whether to accept or reject the resulting new translation. The process continues for a fixed number of \(T\) iterations.

particular translation \(y\) for a given input \(x\) can be written as

\[p_{}(y|x)=_{i=1}^{N}p_{}(y_{i}|y_{<i},x).\] (1)

Here, \(y_{<i}:=(y_{1},y_{2},,y_{i-1})\). During inference, a translation is generated by sampling one token at a time from the distribution \(p_{}(y_{i}|y_{<i},x)\), adjusted by a temperature, \(\). The (adjusted) probability of generating a particular token \(y_{i}\), given the preceding tokens \(y_{<i}\) and the input \(x\), is defined as:

\[p_{,}(y_{i}=v|y_{<i},x)=}(y_{i}=v|y_{<i},x)^{1/ }}{_{v^{}}p_{}(y_{i}=v^{}|y_{<i},x) ^{1/}}.\] (2)

Lower temperature values \(\) make the distribution more deterministic, favoring the most probable tokens, whereas higher values make the distribution flatter, approximating a uniform distribution.

However, multiple works have scrutinized the reliability of model likelihood as a measure for translation quality (Ott et al., 2018; Stahlberg and Byrne, 2019; Eikema and Aziz, 2020; Freitag et al., 2022a; Eikema, 2024). Instead of solely relying on the likelihood, these works advocate using an automatic translation quality metric as a utility function for minimum Bayes risk (MBR) decoding or reranking based on QE metrics. This shift not only improves the selection of translations but also facilitates the exploration of the underlying distribution learned by the models. However, it is crucial to acknowledge that the overall translation quality is still contingent on the quality of the candidate pool. We next discuss common automatic metrics used for assessing translation quality.

### Automatic Metrics for Machine Translation

Automatic quality assessment of machine-generated translations has received considerable attention recently, resulting in metrics that attain high correlations with human judgment of translation quality (Freitag et al., 2022b, 2023b). These automatic metrics are meant to assess the quality of a translation across multiple dimensions (_e.g._ fluency, adequacy) and can provide reliable feedback when human judgments are unavailable. Among these metrics, neural learned metrics that are trained on human translation quality assessment scores or error span annotations have gained significant traction (Rei et al., 2022; Yan et al., 2023; Guerreiro et al., 2023; Perrella et al., 2022).

For aligning automatically generated translations with human quality preferences, many works have proposed to use automatic metrics as an alternative to human feedback during MT training (Shen et al., 2016; Wieting et al., 2019; He et al., 2024; Xu et al., 2024b) or decoding Shen et al. (2004); Fernandes et al. (2022b); Freitag et al. (2022a, 2023a). Freitag et al. (2022a) show the efficacy of using reference-based neural metrics as utility functions for MBR decoding over lexical alternatives. Further, Fernandes et al. (2022a) use QE metrics like Comet-QE to select 1-best or \(N\)-best translations from a pool of candidate hypotheses. Their analysis shows that while these automatic metrics can be useful, they might not always reflect human preferences accurately. Additionally, extra care has to be taken when optimizing systems for these metrics, as the improvements might be attributable to overfitting or "gaming the metric", rather than being genuine improvements in translation quality. Nevertheless, these metrics encode useful information about the quality of translations and can still be useful to obtain high-quality translations.

## 3 An MCMC-based Decoding Approach for Text Generation

Given that we have access to an automatic metric that quantifies how desirable a particular translation is, we aim to address the following questions:

_Can we sample directly in proportion to their corresponding quality values? Can we use automatic metrics that already give us a reliable estimate of human-perceived quality to achieve that? Finally, can we use this process to obtain diverse high-quality samples?_

To answer these questions and to address the limitations of quality-aware decoders, we propose to take an alternative approach, quality-aware _sampling_, which ensures high quality and diversity. Recognizing that naturally occurring texts can have numerous valid translations (Nida, 1964; Dreyer and Marcu, 2012; Ott et al., 2018; Mayhew et al., 2020), the ability to generate diverse translation hypotheses is paramount.

To this end, we first present background on Metropolis-Hastings in Section 3.1. Section 3.2 discusses our proposal distribution. Finally, we connect our approach to reinforcement learning with human feedback (RLHF) in Section 3.3.

### Metropolis-Hastings

The problem of sampling translation hypotheses in proportion to a metric, \(r(x,y)\), can be framed as sampling from the following Gibbs distribution:

\[_{}(y|x)=(x)}(),\] (3)

where \(Z_{}(x)=_{y}()\) and \(\) is the temperature of the Gibbs distribution. We denote the corresponding unnormalized density by \(_{}(y|x)\), which unlike \(Z_{}(x)\) can be evaluated for any \((x,y)\).

While several algorithms have been proposed to sample approximately from such intractable distributions (Miao et al., 2018; Berglund et al., 2015; Amini et al., 2023), we resort to Metropolis-Hastings (Metropolis et al., 1953, MH) due to its simplicity and flexibility in handling a wide range of proposal distributions. The MH algorithm generates a sequence of samples from a **target distribution**, here \(_{}(y|x)\), by constructing a Markov chain \((y^{0},y^{1},,y^{T})\) that has \(_{}(y|x)\) as its equilibrium distribution. It starts from an arbitrary hypothesis \(y^{0}\). In the \(t^{}\) iteration, it draws a new hypothesis \(y\) from a **proposal distribution**\(q(y|y^{t},x)\). This hypothesis is accepted with an acceptance probability \(_{}(y,y^{t})\) given by:

\[_{}(y,y^{t})=\{1,\ (y|x)q (y^{t}|y,x)}{_{}(y^{t}|x)q(y|y^{t},x) }\}.\] (4)

If the candidate \(y\) is accepted, the next state in the chain becomes \(y^{t+1}=y\); if rejected, the chain stays at \(y^{t+1}=y^{t}\). The process repeats for some number of steps \(T\) and in the end it returns the hypothesis \(y^{T}\). Note that, while computing the likelihood \(_{}(y|x)\) of a particular hypothesis \(y\) under the Gibbs distribution is intractable (due to the intractable partition function \(Z_{}(x)\)), evaluating the acceptance criterion \(_{}(y,y^{t})\) is easy, because it depends only on the likelihood ratio, in which the normalization constants cancel out, _i.e._:

\[(y|x)}{_{}(y^{t}|x)}=_{}(y|x)} {_{}(y^{t}|x)}=(,x)}{}).\] (5)

MH converges to the unique stationary distribution \(_{}(y|x)\), regardless of the initial distribution we start with, if the transition distribution of the Markov chain, _i.e._, \(p(y^{t}|y^{t-1},x)=q(y^{t}|y^{t-1},x)(y^{t},y^{t-1})\) satisfies the _Markov chain ergodic theorem_(Neal, 2011).

This requires a suitable proposal distribution \(q(y|y^{t},x)\) which must be **irreducible** and **aperiodic**. To ensure irreducibility, the proposal distribution should have sufficient support, such that it is possible to transition from any state to any other state with a nonzero probability in a finite number of steps. Aperiodicity ensures that the Markov chain does not get stuck in a cyclic behavior, where it keeps returning to the same states in a fixed pattern. Additionally, due to the acceptance criterion defined in Eq. 4, the transition distribution satisfies the detailed balance condition:

\[_{}(y|x)p(y^{t}|y,x)=_{}(y^{t}|x )p(y|y^{t},x).\] (6)

It is trivial to show that the chain has the target distribution, \(_{}(y|x)\) as its stationary distribution:

\[_{}(y|x)=_{y^{}}p(y|y^{},x)_{}( y^{}|x).\] (7)

Note that MH can work with almost any proposal distribution. However, if the detailed balance conditions are far from holding, the acceptance probability will be low for transitions, making the convergence to the target distribution very slow and the approach impractical. Hence, choosing a suitable proposal distribution for the task is essential. We next describe our proposal distribution, \(q(y|y^{t},x)\), which overcomes these limitations and satisfies the required constraints.

### Proposal distribution

Previous works (Berglund et al., 2015; Miao et al., 2018; Su et al., 2018) use proposal distributions that generate hypotheses with one-word or token-level modifications. The different formulations in their most basic form propose a Markov chain in which the state comprises the sentence \(y^{t}\) and an index variable \(i^{t}\{0,,|y^{t-1}|\}\). At every step, an index is sampled that determines the token to be changed, and a new token is then sampled based on the following full conditional distribution:

\[q(y_{i}|y_{<i},y_{>i})=(y_{<i},y_{i},y_{>i})}{_{y^{t}_{i} }(y_{<i},y^{t}_{i},y_{>i})},\] (8)

where \(\) is created by considering the \(k\) most likely tokens (\(k\) is generally large) under \(p_{}(y_{i}|y_{<i})\).

This procedure, however, has several limitations: first, it makes it difficult to handle more general combinatorial constraints, in our case more sophisticated metrics. As we only explore adjacent positions in the sentence space due to small local changes, the Markov chain risks becoming trapped in infeasible states, necessitating a very large number of steps \(T\) to converge. Second, relying solely on token-level modifications makes it exceedingly difficult to generate plausible text, making it impractical to align generations with QE metrics or general reward models. We show that this is indeed the case in Section 5.2.

We instead propose a simple and effective procedure that only requires generating a single hypothesis from the model \(p_{}\) and a single evaluation from the metric, \(r(y,x)\), and still allows the Markov chain to converge to the target distribution. We characterize the proposal by the following procedure:

1. Given an instance \(y^{t}\) with length \(n^{t}:=|y^{t}|\), sample an index \(i\), \(i q(i|n^{t})\).
2. Generate a completion \(y_{ i}\) from \(p_{}(y_{ i}|y^{t}_{<i},x)\).

Note that, due to the nature of our proposal distribution, \(y^{t}\) and \(y\) share a common substructure (prefix) before the index \(i\), i.e., \(y^{t}_{<i}=y_{<i}\), which implies that

\[q(y|y^{t},x,i)=p_{}(y_{ i}|y^{t}_{<i},x)_{j<i}(y_{j},y ^{t}_{j}),\] (9)

where \((y_{j},y^{t}_{j})\) is the Kronecker delta function, which assigns zero probability to prefix tokens which are different from \(y^{t}_{<i}\) and probability one to tokens matching the prefix.

The complete proposal when we include the index distribution \(q(i|n^{t})\), which depends on the previous sentence lengths \(n^{t}:=|y^{t}|\), is given as

\[q(y,i|y^{t},x)=q(i|n^{t})p_{}(y_{i}|y^{t}_{<i},x)_{j<i} (y_{j},y^{t}_{j}).\] (10)

```
1:Input:\(x\), \(p_{}\), \(r\), \(T\)
2:Hyperparameters:\(\), \(t_{}\), \(\)
3:Sample initial response \(y_{0} p_{}( x)\)
4:\(t 1\)
5:for\(1\) to \(T\)do
6: Sample index \(i q(i|n^{t-1})\)
7: Sample \(y_{ i} p_{}(|y^{t-1}_{<i},x)\)
8:\(y(y^{t-1}_{<i},y_{ i})\)
9: Compute \(_{}(y,y^{t-1})\) through Eq. 4
10: Sample \(\) uniformly in \(\)
11:if\(_{}(y,y^{t-1})\)then
12:\(y^{t} y\)
13:\(t t+1\)
14:endif
15:endfor
16:return\(y^{t_{}},,y^{t}\) ```

**Algorithm 1** Quality-Aware Metropolis-Hastings (Quest) Sampling

### Connections to Reinforcement Learning with Human Feedback

Reinforcement Learning with Human Feedback (RLHF) leverages human feedback to guide the learning process of complex NLP tasks (Stiennon et al., 2022; Fernandes et al., 2023; Kaufmann et al., 2023). The process is as follows. Given a language model, one generates hypotheses \(y\) given an input prompt \(x\) and gathers human feedback about which outputs are preferable. This preference data is then used to train a proxy reward model \(r_{}(y,x)\). Finally, reinforcement learning (RL) methods are used to optimize the original LM with respect to the reward model, following

\[_{}_{x,y(y|x)}[(x,y) }{}]-D_{}[(y|x)\ \|\ p_{}(y|x)].\] (11)

Many works show that the optimal solution to the KL-constrained reward maximization objective takes the form (Peters and Schaal, 2007; Peng et al., 2019; Korbak et al., 2022b, a; Go et al., 2023):

\[(y|x)=(x)}p_{}(y|x)(),\] (12)

where \(Z_{}(x)=_{y}p_{}(y|x)()\). While we cannot sample autoregressively from this distribution, this density can be represented as a Gibbs distribution with the corresponding reward function \((x,y)= p_{}(y|x)+\).

Instead of the target distribution expressed in Eq. 3, we could define the above distribution as our target when using Quest to sample from it, avoiding optimizing the objective in Eq. 11 directly. If we formulate the acceptance criterion using this target density and our proposal distribution introduced in Eq. 10, we obtain the following:

\[_{}(y,y^{t})=\{1,(,x)}{}))}\}.\] (13)

The full derivation is in Appendix A. Using this approach, we can align language model generations without access to the model weights, log probabilities, or RL. We provide a preliminary experimental comparison using the two target distributions (Eq. 3 and Eq. 12) using Quest in Appendix C.4.

## 4 Experimental Settings

Data and EvaluationWe test our approach on the WMT23 test sets (Kocmi et al., 2023) covering four language pairs, English\(\) [German, Russian].

We evaluate the quality and the diversity of the generated texts as follows. Suppose \(}\) is the set of hypotheses generated for the source text \(x\) with reference hypothesis \(y^{}\) and \(=\{(x,y^{},})\}\) represents the evaluation set. We compute the mean quality over each set of hypotheses using xComet-XL (Guerreiro et al., 2023) as \(|}_{(x,y^{},})} |}_{y}}(x,y,y^ {})\). Similarly, we measure the mean diversity using the average pairwise BLEU (Papinenieni et al., 2002; Shen et al., 2019) as \(1-|}_{(x,y^{},}) D}|(||-1)}_{(y,y^{})}^{2} \ y y^{}}(y,y^{})\).3

ModelsWe use Tower-7b (Alves et al., 2024, Unbabel/TowerInstruct-7B-v0.2) and Alma-7b (Xu et al., 2024, haoranxu/ALMA-7B), two strong decoder-only MT models based on Llama2-7b (Touvron et al., 2023) as these models achieve competitive translation quality with GPT-4 and productized models like Google Translate. Unlike Alma-7b, Tower-7b uses bilingual MT data as well as datasets from MT-related tasks during training. Prompts are shown in Appendix B.

Automatic Metrics for QuestWe use CometKiwi-XL (Rei et al., 2023), a reference-free QE model built on top of XLM-R XL (Goyal et al., 2021) and trained to predict human-rated direct assessments (Graham et al., 2013). This metric showed the highest correlations with human judgments on the QE Shared Task organized by the eighth conference on Machine Translation (WMT 2023) Blain et al. (2023). We transform the normalized scores from the QE model into \(z\)-scores using a logit transformation with clamping applied to mitigate overflow.

Decoding ConfigurationsFor ancestral sampling, we consider temperature values \(\) between \(0.2\) and \(1.0\), with an equally spaced interval of \(0.1\). For generations with Quest, we sample from the proposal distribution using \(=0.8\) and vary the parameter \(\) of the target Gibbs distribution from the following range of values \(\{0.01,0.02,0.05,0.1,0.2,0.5,1.0\}\). The number of ancestral samples and decoding steps are both set to 128. We use VLLM (Kwon et al., 2023), a high-throughput and memory-efficient inference engine for generating outputs.

Compute Comparison: Ancestral Vs QuestWe use the number of output tokens as a metric to compare the different approaches. For the sake of simplicity, let us assume that the output sentence has a fixed size of \(N\). On average, Quest in \(T\) steps generates \((+1)N=\) tokens, \(N\) tokens for the initial hypothesis, and then on average \(\) tokens for the remaining \(T-1\) steps in the chain. If we contrast against sampling \(T\) sentences using ancestral sampling, we decode \(T N\) tokens. This suggests that for an equal number of samples generated using ancestral sampling and Quest, the latter results in about \( 2\) times as many tokens, on average. Note, however, that the computational cost of Quest is higher than ancestral sampling, as the hypotheses are generated sequentially and evaluated at every step. We run our experiments on NVIDIA RTX A6000 GPUs. Each ancestral sampling and Quest for run with \(T=128\) takes, on average, 1 hour and 6 hours, respectively, for 2000 unique source texts on a single GPU. The compute bottleneck for Quest also arises from using a large QE metric, potentially distilling this metric into smaller models could help reduce the compute time. We leave this to future work.

## 5 Results

### Main Findings

The main results of our experiments are presented in Figure 2.

Quest results in better translation quality-diversity trade-offs. Across language directions and models, the samples generated by Quest tend to have better or similar quality than ancestral sampling as measured by xComet-XL.4 As Quest does not directly use the reference-based metric, xComet-XL, we reduce the chance of overfitting to the metric and thus the gains represent the ability of Quest to improve translation quality more realistically. We also report Comet-22 vs diversity results in the Appendix Figure 9: the trends remain the same. The benefits of the proposed approach are more noticeable when translating from English (en \(\) {de, ru}). Specifically, for En \(\) Ru, our model improves xComet-XL by up to 2 points for both language models, showing the efficacy of Quest over ancestral sampling.

Figure 2: Average quality vs. diversity on WMT23 datasets. Different points represent different hyperparameter values. Quest outperforms ancestral sampling in six out of eight settings.

Although ancestral exhibits better quality than Quest for De-En, further analysis suggests that this discrepancy may stem from the constraints of the QE metric used to model the translation preferences. Notably, Quest demonstrates significantly higher CometKwi-XL scores across the board (see Appendix Figure 7), suggesting that better and more robust QE models can result in improved translation quality.5 Furthermore, WMT23 De-En includes short passages that might require additional steps to reach the high-density regions. Based on a length analysis (See Appendix C.2), we observe that the quality of Quest lags behind ancestral sampling, specifically for longer sentences. We leave the exploration of finding optimal steps for convergence and adapting the proposal distribution for document-level MT to future work.

### Ablation Analysis

We present further analysis on some properties of our proposed approach using WMT23 En\(\)Ru datasets with translations generated using Alma-7b.

Quest generates less-likely high-quality samples.We compute the set overlap for Quest and ancestral samples (\(T=128\)) with an independent draw of 512 ancestral samples to investigate whether our approach results in hypotheses from unexplored regions (Fig. 2(a)). Compared to ancestral, Quest results in hypotheses that are not found in the larger pool (\(4\)) of ancestral samples as illustrated by a higher mass at the overlap value of \(0\). This demonstrates that Quest effectively gets to regions of the output manifold that would be less likely sampled by the LM and still attain, on average, better quality and diversity.

Average reward improves over decoding steps.Figure 2(b) shows the average reward with increasing decoding steps and the sample size for Quest and ancestral respectively. As ancestral results in independent samples, the mean reward estimate remains unchanged for different sample sizes. However, for Quest, hypotheses are gradually sampled in the direction of higher-quality regions, closer to the target density, resulting in increased average reward with more steps. We can also observe that the standard deviation of the reward over all samples decreases with the increasing number of steps, suggesting that the model eventually reaches a better target distribution.

High \(\) value results in higher acceptance rates.Figure 3c shows the distribution of accepted samples when varying \(\), considering all (blue) versus unique (red) accepted samples. As expected, on average, the number of accepted samples is smaller than the number of steps \(T\) depending upon the rejection rate--low \(\) values lead to higher rejection rates. Furthermore, within the accepted samples, we also observe many repeats. This can be attributed to the observation that the language model has a low entropy distribution for continuations when the sample indices lie at the end of sentences. Moreover, for probable sentences, only a few have a high reward, which could result in the Markov chain getting stuck in a particular state. Increasing the temperature of the LM or adjusting the index distribution to have less density in the last few tokens could potentially reduce the number of repeats. We leave this exploration to future work.

Our sentence-level proposal generates better candidates.For a random example sampled from the WMT23 dataset, we generate \(25\)k hypotheses using three proposal distributions: a) token-level modification with uniform probability b) token-level modification with full posterior presented in Eq. 8, and c) our sentence-level proposal (Eq. 10) and calculate the reward differences between the original (\(y\)) and the generated hypothesis (\(y^{}\)): \(r(y^{},x)-r(y,x)\). Figure 3d shows its distribution: for proposals (a) and (b), the reward for the generated hypotheses is almost always lower than the initial translation. On the other hand, our proposal results in assigning half of the probability mass to hypotheses that improve over \(y\), leading to faster convergence over token-level alternatives.

## 6 Related Work

Sampling from Intractable Gibbs distributions.Several methods have been proposed to sample approximately from Gibbs distributions in text generation using autoregressive and masked-language models. Prior works (Miao et al., 2018; Zhang et al., 2020) use MH with a proposal distribution that makes token-level modifications for constrained generation tasks. Since masked language models (MLM) do not have a straightforward mechanism for sampling text, MCMC has been widely explored using variations of Gibbs sampling (Berglund et al., 2015; Su et al., 2018; Wang and Cho, 2019; Yamakoshi et al., 2022). However, Goyal et al. (2022) shows that the masked conditional distributions from MLMs result in invalid Gibbs samplers and, therefore, proposes to use MH on the masked conditionals, resulting in higher-quality text. Mireshghallah et al. (2022); Forristal et al. (2023) build on this work and use MLMs for sampling from Gibbs distributions. Some works (Kumar et al., 2022; Qin et al., 2022; Amini et al., 2023; Du et al., 2023) also adapt the Hamiltonian MCMC algorithms originally designed for high-dimensional continuous distributions for the discrete scenario (Duane et al., 1987; Neal, 2011). Furthermore, Hu et al. (2024) apply GFlowNets (Bengio et al., 2021) to fine-tune language models for solving posterior inference problems, which can be considered as sampling in proportion to an intractable Gibbs distribution. In our work, we instead aim to use MCMC for MT to sample translations in proportion to a sentence-level evaluation metric.

Diverse Decoding for Machine Translation.Variants of beam search (Cho, 2016; Vijayakumar et al., 2017; Kulikov et al., 2019; Tam, 2020) have been proposed to produce a diverse set of translations using diversity-promoting objectives. However, the increased computation cost with the model size and beam width makes it infeasible and impractical to use with LLMs and it fails to yield consistent improvement over ancestral with an increase in beam width (Stahlberg and Byrne, 2019; Eikema and Aziz, 2020; Pang et al., 2024). Quality-aware decoding approaches on the other hand are almost always used to generate a single best hypothesis. Concurrently, Jinnai et al. (2024) add diversity promoting objective to MBR decoding to generate a set of high-quality diverse candidates. However, their approach only allows for the selection of hypotheses from a predefined set. We further note that we do not _directly_ promote diversity--rather, diverse translations are a side product of efficiently exploring multiple high-quality regions from the model's distribution.

## 7 Conclusion

We propose a new decoding approach for MT, _Quality-Aware Metropolis-Hastings_ (Quest) sampling based on MCMC that enables the generation of hypotheses in proportion to an automatic QE metric. We present a simple and novel proposal distribution that satisfies the constraints imposed by the Metropolis-Hastings algorithm. Our experiments on four language directions and two strong decoder-only language models show the efficacy of our approach over baselines. We further show that our approach results in samples from underexplored high-density regions and that the average quality continues to improve as the Markov chain size increases.

## 8 Limitations and Broad Impact

Quest requires generating many samples to reach high-density regions sequentially from an LLM for each input prompt, which can be computationally expensive for time-critical applications. Additionally, the required number of steps may vary depending on the input and the quality of the initial hypothesis. Furthermore, our proposal distribution only modifies the sentence suffix, which becomes restrictive once the chain reaches a high-quality region. As at this point, only minor changes to the hypothesis are accepted, slowing the mixing process. Addressing these limitations and extending this approach to other NLP tasks are avenues for future work.

Furthermore, we leverage recent advances in QE methods for MT and integrate them directly in the generation process of LLMs, which can potentially reduce the errors generated by these systems. However, despite the high correlations of evaluation metrics with human judgments, they are sometimes hard to interpret and occasionally fail to detect simple mistakes such as incorrect translations of numbers or entities (Amrhein and Sennrich, 2022). In such cases, sampling from the Gibbs distributions induced by these metrics might increase the chances of sampling those erroneous translations. We believe these risks will be mitigated as better metrics are constantly being developed--our method, being agnostic to the specific quality metric, will directly benefit from it. In addition, since Quest supports any Gibbs distribution, it can also incorporate multiple QE models or additional checks which can rule out problematic samples by assigning them a very low QE score.