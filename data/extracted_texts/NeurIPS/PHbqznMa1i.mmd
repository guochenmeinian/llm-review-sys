# Contrastive Moments: Unsupervised Halfspace

Learning in Polynomial Time

 Xinyuan Cao

Georgia Tech

xcao78@gatech.edu &Santosh S. Vempala

Georgia Tech

vempala@gatech.edu

###### Abstract

We give a polynomial-time algorithm for learning high-dimensional halfspaces with margins in \(d\)-dimensional space to within desired Total Variation (TV) distance when the ambient distribution is an unknown affine transformation of the \(d\)-fold product of an (unknown) symmetric one-dimensional logconcave distribution, and the halfspace is introduced by deleting at least an \(\) fraction of the data in one of the component distributions. Notably, our algorithm does not need labels and establishes the unique (and efficient) identifiability of the hidden halfspace under this distributional assumption. The sample and time complexity of the algorithm are polynomial in the dimension and \(1/\). The algorithm uses only the first two moments of _suitable re-weightings_ of the empirical distribution, which we call _contrastive moments_; its analysis uses classical facts about generalized Dirichlet polynomials and relies crucially on a new monotonicity property of the moment ratio of truncations of logconcave distributions. Such algorithms, based only on first and second moments were suggested in earlier work, but hitherto eluded rigorous guarantees.

Prior work addressed the special case when the underlying distribution is Gaussian via Non-Gaussian Component Analysis. We improve on this by providing polytime guarantees based on TV distance, in place of existing moment-bound guarantees that can be super-polynomial. Our work is also the first to go beyond Gaussians in this setting.

## 1 Introduction

Suppose points in \(^{d}\) are labeled according to a linear threshold function (a halfspace). Learning a threshold function from labeled examples is the archetypal well-solved problem in learning theory, in both the PAC and mistake-bound models; its study has led to efficient algorithms, a range of powerful techniques and many interesting learning paradigms. While the sample complexity in general grows with the dimension, when the halfspace has a _margin_, the complexity can instead be bounded in terms of the reciprocal of the squared margin width . The problem is also very interesting for special classes of distributions, e.g., when the underlying distribution is logconcave, agnostic learning is possible , and active learning needs fewer samples compared to the general case .

The main motivation for our work is learning a halfspace with a margin _with no labels_, i.e., unsupervised learning of halfspaces. This is, of course, impossible in general -- there could be multiple halfspaces with margins consistent with the data -- raising the question: _Can there be natural distributional assumptions that allow the unsupervised learning of halfspaces?_ For example, suppose data is drawn from a Gaussian in \(^{d}\) with points in an unknown band removed, i.e., we assume there exists a unit vector \(u^{d}\) and an interval \([a,b]\) so that the input distribution is the Gaussian restricted to the set \(\{x^{d}| u,x a u,x b\}\). Can the vector \(u\) be efficiently learned? Such a distributional assumption ensures that the band normal to \(u\) is essentially unique, leaving open the question of whether it can be efficiently learned.

Such models have been considered in the literature, notably for Non-Gaussian Component Analysis (NGCA) , learning relevant subspaces  and low-dimensional convex concepts  where data comes from a product distribution with all components being Gaussian except for one (or a small number). It is assumed that the non-Gaussian component differs from Gaussian in some low moment and the goal is to identify this component. Another related model is Independent Component Analysis (ICA) where the input consists of samples from an affine transformation of a product distribution and the goal is to identify the transformation itself . For this problem to be well-defined, it is important that at most one component of the product distribution is Gaussian. No such assumption is needed for NGCA or the more general problem we consider here.

Formally, we consider the following model and problem, illustrated in Figure 1.1.

**Definition 1** (Affine Product Distribution with \(\)-Margin).: Let \(q\) be a symmetric one-dimensional isotropic logconcave density function. Let \(Q\) be the \(d\)-fold product distribution obtained from \(q\). Let \(\) be the isotropized density obtained after restricting \(q\) to \([a,b]\) where \(q((-,a]),q([a,b])\) and \(q([b,))\). Let \(P\) be the product of one copy of \(\) and \(d-1\) copies of \(q\). Let \(\) be obtained by a full-rank affine transformation of \(P\); we refer to \(\) as an _Affine Product Distribution with \(\)-Margin_. Let \(u\) be the unit vector normal to the margin before transformation.

With this model in hand, we have the following algorithmic problem.

Problem.Given input parameters \(,>0\) and access to iid samples from \(\), an affine product distribution with \(\)-margin, the learning problem is to compute a unit vector \(\) that approximates \(u\) to within \(TV\) distance \(\). That is, the TV distance between the corresponding \(\) and \(P\) is at most \(\), where \(\) is the distribution with margin normal to \(\).

In this formulation of the problem with a TV distance guarantee, if each side of the halfspace receives a different label, then the probability that the output halfspace of the data disagrees with the true label (up to swapping the labels) is at most \(\).

A natural approach to identifying the halfspace is maximum margin clustering : find a partition of the data into two subsets s.t. the distance between the two subsets along some direction is maximized. Unfortunately, this optimization problem is NP-hard, even to approximate.

There are at least two major difficulties we have to address. The first is the unknown affine transformation, which we cannot hope to completely identify in general. The second is that, even if we reversed the transformation, the halfspace normal is in an arbitrary direction in \(^{d}\) and would be undetectable in almost all low-dimensional projections, i.e., we have a needle in a haystack problem.

### Results and Techniques

We give an efficient algorithm for the unsupervised halfspace learning problem under any symmetric product logconcave distribution. It consists of the following three high-level steps.

1. Make the data isotropic (mean zero and covariance matrix identity).
2. Re-weight data and compute the re-weighted mean \(_{i}\) and the top eigenvector \(v\) of the re-weighted covariance.
3. Project data along the vectors \(_{i},v\), and output the vector with the largest margin.

Figure 1.1: Affine Product Distribution with Margin.

Although the algorithm is simple and intuitive, its analysis has to overcome substantial challenges. Our main result is the following. The formal version (Theorem 7) is in the Appendix.

**Theorem 1** (Main).: _There is an algorithm that can learn any affine product distribution with \(\)-margin to within TV distance \(\) with time and sample complexity that are polynomial in \(d,1/\) and \(1/\) with high probability._

To see the idea of the algorithm, we first consider the case when no affine transformation is applied. In this case, we can detect the direction \(u\) by calculating the empirical mean and top eigenvector of the empirical uncentered covariance matrix. If the margin \([a,b]\) lies on one side of the origin, the mean along \(u\) is nonzero while the mean in any other direction that is orthogonal to \(u\) is zero. Thus the mean itself reveals the vector \(u\). Otherwise, we can show that the second moment along \(u\) is higher than along any other orthogonal direction. Thus, there is a positive gap between the top two eigenvalues of the uncentered covariance matrix and the top eigenvector is \(u\). In fact, the algorithm applies more generally, to the product distribution created from one-dimensional bounded isoperimetric distributions. A one-dimensional distribution \(p\) is isoperimetric if there exists a constant \(>0\) such that for any \(x\), \(p(x)\{p([x,)),p((-,x])\}\). The theorem is stated as follows. The formal version (Theorem 6), algorithm and proofs are included in the Appendix.

**Theorem 2** (Isotropic Isoperimetric Distribution).: _There is an algorithm that can learn any isotropic isoperimetric bounded product distribution with \(\)-margin to within TV distance \(\) with time and sample complexity that are polynomial in \(d,1/,1/\) with high probability._

In the general case, when an unknown affine transformation is applied, the algorithm first computes the empirical mean and covariance of the sample and makes the empirical distribution isotropic. Then we will consider two cases as illustrated in Figure 1.1a. If the unknown band is _not centered_ around the mean along \(u\), we can expect the empirical mean to differ from the mean of the underlying product distribution without the margin. Consequently, if we knew the latter, we can use the difference to estimate \(u\). However, in general, we do not have this information. Instead, we demonstrate that there exists a re-weighting of the sample so that re-weighted empirical mean compared to the unweighted empirical mean is a good estimate of \(u\). In other words, with appropriate re-weighting, the mean shifts along the normal direction to the unknown band. On the other hand, if the band is centered along \(u\), the mean shift will be zero. In this scenario, we will show that the maximum eigenvector of a re-weighted covariance matrix is nearly parallel to \(u\)!

Our algorithm only uses first and second order moments, can be implemented efficiently, and is in fact practical (see Section 5). The main challenges are (1) proving the existence of band-revealing re-weightings and (2) showing that a polynomial-sized sample (and polynomial time) suffice.

To prove the main theorem, we will show that either the re-weighted mean induces a contrastive gap (Lemma 1), or the eigenvalues of the re-weighted covariance matrix induce a contrastive gap (Lemma 2). In the subsequent two lemmas, we adopt the notation from Definition 1. Here, \(P\) represents a product distribution with \(\)-margin defined by the interval \([a,b]\) (before transformation).

**Lemma 1** (Contrastive Mean).: _If \(|a+b|>0\), then for any two distinct nonzero \(_{1},_{2}\), at least one of the corresponding re-weighted means is nonzero, i.e.,_

\[(|*{}_{x P}e^{_{1}\|x\|^{2}}u ^{}x|,|*{}_{x P}e^{_{2}\|x\| ^{2}}u^{}x|)>0.\]

**Lemma 2** (Contrastive Covariance).: _If \(a+b=0\), then there exists an \(<0\), such that (1) there is a positive gap between the top two eigenvalues of the re-weighted uncentered covariance matrix \(=*{}_{x P}e^{\|x\|^{2}}(xx^{ })\). That is, \(_{1}()>_{2}()\). (2) The top eigenvector of \(\) is \(u\)._

The proof of Lemma 1 uses Descartes' rule of signs applied to a suitable potential function. To prove Lemma 2, we develop a new monotonicity property of the moment ratio (defined as the ratio of the variance of \(X^{2}\) and the squared mean of \(X^{2}\)) for truncations of logconcave distributions. The moment ratio is essentially the square of the coefficient of variation of \(X^{2}\). An insight from the monotonicity of the moment ratio is that for logconcave distributions with positive support, when the distribution is restricted to an interval away from the origin, it needs a smaller sample size to estimate its second moment accurately. We state the lemma as follows.

[MISSING_PAGE_FAIL:4]

the algorithm's process: it calculates the re-weighted sample mean and the top eigenvector of the re-weighted sample covariance matrix, both of which require polynomial time.

In our algorithm, we consider two cases depending on whether the removed band \([a,b]\) is origin-symmetric. If it is asymmetric, we will show that one of the re-weighted means with two \(\)s gives us the correct direction by showing that the re-weighted mean along \(u\) has a gap from zero while the re-weighted mean along all other orthogonal directions is zero. We state the positive gap quantitatively in Lemma 4. Otherwise, if the band is symmetric, we will show a positive gap between the top two eigenvalues of the re-weighted covariance matrix, and the top eigenvector corresponds to \(u\). We quantify the gap between the top two eigenvalues in Lemma 5. In the algorithm, since we know neither the underlying distribution mean nor the location of the removed band, we have to compute both re-weighted means and re-weighted covariance, and then get the correct direction among all three candidate vectors by calculating the margin and finding the one with the largest margin.

**Lemma 4** (Quantitative Gap of Contrastive Mean).: _Suppose that \(|a+b|^{5}\). Then, for \(_{1}=-c_{1}^{82}/d,_{2}=-c_{2}^{42}/d\), the re-weighted mean of \(P\), denoted as \(_{_{1}}\) and \(_{_{2}}\), satisfies_

\[(|u^{}_{_{1}}|,|u^{}_{_{2} }|)>}{d^{2}}C>0,\]

\[ v u, v^{}_{_{1}}=v^{}_{_{2}}=0.\]

**Lemma 5** (Quantitative Spectral Gap of Contrastive Covariance).: _Suppose that \(|a+b|<^{5}\). Choose \(_{3}=-c_{3}^{2}\) for some constant \(c_{3}>0\). Then, for an absolute constant \(C\), the top two eigenvalues \(_{1}_{2}\) of the corresponding re-weighted covariance of \(P\) satisfy_

\[_{1}-_{2} C^{3}_{1}.\]

## 4 Proofs

Recall that we are given data \(x^{(1)},,x^{(N)}\) drawn from the affine product distribution with \(\)-margin \(\). Algorithm 1 first makes the data isotropic. Denote \(y^{(1)},,y^{(N)}\) as the corresponding isotropicized data. Then each \(y^{(j)}\) is an independent and identically distributed variable drawn from \(P= q q\). Since we compute the re-weighted moments on \(y^{(j)}\) in the algorithm, we analyze the moments of \(P\) directly.

Recall in Definition 1 that \(q\) is the symmetric one-dimensional isotropic logconcave density function, and \(\) is the density obtained by restricting \(q\) to \([a,b]\) for some unknown \(a<b\). Denote \(_{1},_{1}^{2}\) as the mean and variance of \(\). \(\) is the density obtained after making \(\) isotropic, with support \([a^{},b^{}]\), where \(a^{}=}{_{1}},b^{}=}{_{1}}\). We denote the standard basis of \(^{d}\) by \(\{e_{1},,e_{d}\}\), and assume wlog that \(e_{1}=u\) is the (unknown) normal vector to the band. We write \(x_{i}:= x,e_{i}\) as \(x\)'s \(i\)-th coordinate. We assume in our proof that \(|b|>|a|\). If this condition is not met, we can redefine our interval by setting \(a^{}=-b\) and \(b^{}=-a\). The proof can then be applied considering the distribution is restricted to \(\{x^{d}:u^{}\,x a^{}u^{}\,x b^{}\}\). For a vector \(x^{d}\), we use \(\|x\|\) to denote its \(l_{2}\) norm. For a matrix \(A^{m n}\), we denote its operator norm as \(\|A\|_{}\).

Contrastive Mean.We can write the contrastive mean as a linear combination of exponential functions of \(\). By Descartes' rule of signs, the number of zeros of this function is at most two. Since \(=0\) is one root and corresponds to mean zero, there is at most one nonzero root. And thus we have that for any two distinct nonzero \(\)'s, at least one of them achieves nonzero contrastive mean.

**Theorem 3** (Descartes' Rule of Signs in the Integral Form).: _Let \(F()=_{0}^{}a(x)e^{ x^{2}}\,dx\). Then the number of roots of \(F()=0\) is at most the number of sign changes in \(a(x)\) for \(x 0\)._

Proof of Lemma 1 (Contrastive Mean).: \(|b|>|a|\) implies that \(_{1}<0\). For any \(x 0\), we have

\[(x)=q(x_{1}+_{1})}{1-_{a}^{b}q(x)\,dx} q(-x_{1}+_{1})}{1-_{a}^{b}q(x)\,dx}=(- x).\]

Since \(P\) is a product distribution, we have

\[}_{x P}e^{\|x\|^{2}}x_{1}=}_{x_{1}}e^{ x_{1}^{2}}x_{1}_{i=2}^{d }}_{x_{i} q}e^{ x_{i}^{2}}\]

We denote

\[F()=}_{x}e^{ x_{1}^{2}}x_{1}=_{ [a^{},b^{}]}e^{ x^{2}}x(x)\,dx\] (4.1)

Then we rearrange \(F()\) by combining \(e^{ x^{2}}\) as in Figure 4.1.

If \(a^{} 0\), we rewrite \(F()\) as

\[F()=-_{-a^{}}^{b^{}}x(-x)e^{ x^{2}}\,dx+ _{b^{}}^{}x((x)-(-x))e^{ x^{2}}\,dx\]

We treat \(F()\) as the integral of \(a(x)e^{ x^{2}}\) for \(x-a^{}\). Since \((x)-(-x)>0\) for \(x>b^{}\), we have \(a(x)>0\) for \(x[-a^{},b^{})\) and \(a(x)<0\) for \(x>b^{}\). In other words, for increasing \(x\), the sign of \(a(x)\) only changes once. By Theorem 3, \(F()=0\) has at most one root.

Figure 4.1: Coefficient of \(e^{ x^{2}}\) in \(F()\). In the proof of Lemma 1, by combining \(e^{ x^{2}}\) terms, we flip \((x)\) horizontally. For \(a^{}>0\), the coefficient is negative when \(x(a^{},b^{})\) and non-negative outside the interval. For \(a^{} 0\), it is negative when \(x(-a^{},b^{})\) and positive when \(x>b^{}\).

If \(a^{}>0\), we arrange \(F()\) in the same way and get

\[F()=_{0}^{a^{}}x((x)-(-x))e^{ x^{2}}\,dx- _{a^{}}^{b^{}}x(-x)e^{ x^{2}}\,dx+_{b^{} }^{}x((x)-(-x))e^{ x^{2}}\,dx\]

Similarly, we treat \(F()\) as the integral of \(a(x)e^{ x^{2}}\) for \(x 0\). For increasing \(x\), the sign of \(a(x)\) changes twice. By Descartes' rule of signs, \(F()=0\) has at most two roots. In addition, we know \(F(0)=}_{P}x_{1}=0\) by definition of \(P\), which implies that \(=0\) is one root of \(F()\). So there is at most one nonzero root of \(F()\). In other words, for any two distinct nonzero \(_{1},_{2}\), at least one of \(F(_{1}),F(_{2})\) is nonzero. Consequently,

\[(|}_{x P}e^{_{1}\|x\|^{2}}x_ {1}|,|}_{x P}e^{_{2}\|x\|^{2}}x _{1}|)>0.\]

Proof Idea of Lemma 4 (Quantitative Gap of Contrastive Mean).: To get a quantitative bound on the contrastive mean, we follow the same idea of bounding the number of roots of \(F()\) as in Lemma 1. By taking the derivative of \(F()\), we can show that either \(F^{}(0) 0\) or \(F^{}(0) 0\). Then by Taylor expansion, we can choose two distinct \(\)'s (near zero) so that one of the corresponding contrastive means is bounded away from zero.

Moment Ratio and Contrastive Covariance.To prove Lemma 2, we develop a new monotonicity property of the moment ratio of logconcave distributions. Moment ratio is specifically defined as the ratio of the fourth moment to the square of the second moment of truncated versions of the distribution. This measurement essentially reflects the uncentered kurtosis of the distribution. We will prove the monotonicity of the moment ratio by reducing the case of general logconcave distributions to exponential distributions.

Proof of Lemma 3 (Moment Ratio).: We show the monotonicity of moment ratio by showing its derivative with respect to \(t\) is negative. Define \(M_{k}(t)=_{t}^{}x^{k}q(x)\,dx\). By calculation, the derivative of moment ratio is proportional to \(-H(t)\), where \(H(t)\) is defined as follows:

\[H(t)=t^{4}M_{0}(t)M_{2}(t)+M_{2}(t)M_{4}(t)-2t^{2}M_{0}(t)M_{4}(t).\]

Let \(h(x)= e^{- x}\) be an exponential function (\(,>0\)) such that

\[M_{0}(t)=N_{0}(t),M_{2}(t)=N_{2}(t),N_{k}(t)=_{t}^{}x^{k }h(x)\,dx,k.\]

Then we have

\[_{t}^{}(h(x)-q(x))\,dx=0,_{t}^{}x^{2}(h(x)-q(x))=0\]

By the logconcavity of \(q\), the graph of \(h\) intersects with the graph of \(q\) at exactly two points \(u^{}<v\), where \(v>0\). Also we have \(h(x) q(x)\) at the interval \([u^{},v]\) and \(h(x)>q(x)\) outside the interval. Let \(u=\{0,u^{}\}\). So for \(x 0\), \((x-u)(x-v)\) has the same sign as \(h(x)-q(x)\). Since \(t 0\), we have

\[_{t}^{}(x^{2}-u^{2})(x^{2}-v^{2})(h(x)-q(x)) 0\]

Expanding the terms and we get

\[_{t}^{}x^{4}(h(x)-q(x))(u^{2}+v^{2})_{t}^{}x^{2}(h(x)- q(x))\,dx-u^{2}v^{2}_{t}^{}(h(x)-q(x))\,dx=0\]

This shows that \(N_{4}(t) M_{4}(t)\). To show that \(H(t)>0\), we consider two cases.

Firstly if \(M_{2}(t)-2t^{2} 0\), we have

\[H(t)=t^{4}M_{0}(t)M_{2}(t)+M_{4}(t)(M_{2}(t)-2t^{2})>0.\]Secondly if \(M_{2}(t)-2t^{2}<0\), by calculation of the exponential function's moments, we have

\[t^{4}N_{0}(t)N_{2}(t)>-N_{4}(t)(N_{2}(t)-2t^{2})\]

This implies that

\[H(t)=t^{4}M_{0}(t)M_{2}(t)+M_{4}(t)(M_{2}(t)-2t^{2})(M_{2}(t)-2t^{2})(M_{4}( t)-N_{4}(t)) 0\]

The equality holds if and only if \(M_{4}(t)=N_{4}(t)>0\). Then we have

\[H(t)=t^{4}N_{0}(t)N_{2}(t)+N_{4}(t)(N_{2}(t)-2t^{2})>0.\]

Combining both cases, \(^{}_{q}(t)<0, t 0\), which implies that the moment ratio of \(q\) is strictly decreasing with respect to \(t\). 

Proof Idea of Lemma 2 (Constrastive Covariance).: View the spectral gap of the re-weighted covariance, denoted as \(_{1}()-_{2}()\), as a function \(S()\). By calculation, \(S(0)=0\) and \(S^{}(0)\) is proportional to \(_{q}(b)-_{q}(0)\), which is negative by the monotonicity property of moment ratio. Then we can prove the spectral gap for the chosen \(\) using Taylor expansion.

Proof Idea of Lemma 5 (Quantitative Gap of Contrastive Covariance).: We first prove the case when \(a=-b\), and then extend the result to the near-symmetric case when \(|a+b|<^{5}\) by comparing the re-weighted second moments of two distributions created by restricting \(q\) to \([a,b]\) and \([-b,b]\) respectively.

The following theorem enables us to bound the sample complexity to estimate the covariance matrix.

**Lemma 6** (Covariance Estimation ).: _Consider independent isotropic random vectors \(X_{i}\) in \(^{d}\) s.t. for some \(C,>0\), for every orthogonal projection \(P\) in \(^{d}\),_

\[(\|PX_{i}\|>t) Ct^{-1-}t>C(P).\]

_Let \((0,1)\). Then with the sample size \(N=O(d^{-2-2/})\), we have_

\[\|-\|_{op}\|\|_{op}.\]

The following classical theorem will allow us to use the eigenvalue gap to identify the relevant vector.

**Lemma 7** (Davis-Kahan ).: _Let \(S\) and \(T\) be symmetric matrices with the same dimensions. For a fixed \(i\), assume that the largest eigenvalue of \(S\) is well separated from the second largest eigenvalue of \(S\), i.e.\(>0\) s.t. \(_{1}(S)-_{2}(S)>\). Then for the top eigenvectors of \(S\) and \(T\), we have_

\[(v_{1}(S),v_{1}(T))}{}.\]

Proof Sketch of Main Theorem.: We prove the theorem by considering whether the removed band \([a,b]\) is symmetric or not. If \(|a+b|^{5}\), by Lemma 4, for the chosen two \(\)s, at least one of the contrastive means \(_{i}\) along \(u\) direction is bounded away from zero (by \((1/d,)\)) and the projection of the contrastive mean along any direction orthogonal to \(u\) is zero by symmetry. So by Chebyshev's Inequality, we will ensure that the angle between \(u\) and \(_{i}\) is less than \(\) with high probability using \(O((d,1/,1/))\) samples. On the other hand, if \(|a+b|<^{5}\), we rely on the contrastive covariance. By Lemma 5, the top eigenvector \(v\) aligns with \(u\) while the top two eigenvalues \(_{1}>_{2}\) satisfy \(_{1}-_{2}>()\). By Lemma 7, we can upper bound the angle between \(u\) and \(v\) by the quotient of the operator norm of the difference of the contrastive covariance and sample contrastive covariance and the spectral gap \(_{1}-_{2}\). The covariance matrix itself can be estimated to desired accuracy efficiently with \(O((d,1/,1/))\) samples by Lemma 6. This ensures the closeness of \(u\) and top eigenvector \(v\) and hence a TV-distance guarantee. The formal statement of the theorem and the proofs are included in the Appendix.

## 5 Experiments

While our primary goal is to establish polynomial bounds on the sample and time complexity, our algorithms are natural and easy to implement. We study the efficiency and performance of Algorithm 1 on data drawn from affine product distributions with margin. Here we consider three special cases of logconcave distribution: Gaussian, uniform in an interval and exponential. We include four experiments. In all results, we measure the performance of the algorithm using the \(\) of the angle between the true normal vector \(u\) and the predicted vector \(\), i.e., \((u,)\), which bounds the \(TV\) distance between the underlying distribution and the predicted one after isotropic transformation. Experimental results strongly suggest that the sample complexity is a small polynomial, perhaps even just nearly linear in both the dimension and the separation parameter \(\).

Overall Performance.Here we conduct the experiments based on a grid search of \((a,b)\) pairs on three special cases of logconcave distribution: Gaussian, uniform in an interval and exponential. We measure the performance of \((a,b)\) pairs, where for each pair of \((a,b)\), we conduct five independent trials. For Gaussian and exponential distribution, we choose \(-3 a<b 3\) and for uniform distribution, we choose \(-1.5 a<b 1.5\). Here we set the dimension \(d=10\) and sample size \(N=1000000\). For the parameters, we choose \(_{1}=_{3}=-0.1,_{2}=-0.2\). See Figure 5.1 as the heatmap of \((u,)\) given different pairs of \((a,b)\).

Although in Algorithm 1, we use extremely small values of the weight parameter \(\), our experiments show that larger constant values also work empirically, leading to much smaller sample complexity. This coincides with our qualitative lemmas (Lemma 1, Lemma 2).

The algorithm performs well as seen in the results, except when \(a\) and \(b\) are both close to the edge, and thus there is almost no mass on one side of the band. Also, the uniform distribution is the easiest to learn, while the exponential is the hardest among these three distributions. As shown in all three plots, the algorithm performs the best when \(a\) and \(b\) are near symmetric with origin. In other words, contrastive covariance has better sample complexity than contrastive mean when we fix other hyperparameters. This coincides with our sample complexity bounds as in the proof of Theorem 1.

Performance of Contrastive Mean and Covariance.In this experiment, we fix a negative \(a\) as the left endpoint of the removed band, and measure the performance of both contrastive mean and contrastive covariance with respect to different margin right endpoint \(b\). As shown in Figure 5.2, contrastive mean performs well except when \(a+b\) is close to zero, while contrastive covariance performs well only when \(a+b\) is close to zero. This coincides with our algorithm and analysis for the two cases. In addition, our algorithm chooses the best normal vector among candidates from both contrastive mean and covariance. So our algorithm achieves good performance (minimum of contrastive mean and covariance curves).

Specifically, we choose \(a=-2\), \(b[-1.9,4]\) for Gaussian and exponential cases, and \(a=-0.5,b[-0.4,0.9]\) for uniform case. We choose the dimension \(d=10\), the sample size \(N=2000000\). We choose \(_{1}=_{3}=-0.1,_{2}=-0.2\). We average the result with \(50\) independent trials.

Figure 5.1: The performance of Algorithm 1 with varying \((a,b)\).

Figure 5.2: The performance of Algorithm 1 for fixed \(a\) and varying \(b\). The yellow lines show the result computed using the top eigenvector of the contrastive covariance. The blue dotted lines show the better of the two contrastive means.

Dimension Dependence.In this experiment, we show the relationship between the input dimension \(d\) and the sample complexity. For fixed number size \(N=1000000\), we measure the performance of our algorithm with different \(d\). The result is averaged based on a grid search of \((a,b)\) pairs, where for each pair of \((a,b)\), we conduct five independent trials. For Gaussian and exponential distributions, we choose \(-3 a<b 3\) and for uniform distribution, we choose \(-0.8 a<b 0.8\).

As shown Figure 5.3, the performance scales linearly with growing dimension \(d\), suggesting a linear relationship between the sample complexity and the input dimension.

\(\)-Dependence.To further understand the dependence on the separation parameter \(\), we plot the performance versus \(1/\) in Figure 5.4. Here we calculate \(1/\) as \(1/q([a,b])\), and the performance as the median \((u,)\) for specific mass \(q([a,b])\). As we can see the performance drops near linearly with respect to \(1/\), which indicates that the sample complexity is possibly linear in \(1/\) as well.

## 6 Discussion

We have proposed and analyzed an efficient algorithm to learn the symmetric product logconcave distribution without labeled data. This is also connected to self-supervised learning. Specifically, contrastive learning without data augmentation is closely related to the contrastive covariance part in our algorithm.

The algorithm delivers more than the theoretical analysis. While our analysis focuses on specific values of \(\), as demonstrated by the qualitative lemmas (Lemma 1, Lemma 2), any distinct pair of nonzero \(\) values should work for contrastive mean, and any bounded small \(\) should work for contrastive covariance. This flexibility ensures the applicability of the algorithm in various real-world scenarios. Furthermore, our experimental results align with this claim.

The experiments reveals a linear relationship with the input dimension \(d\), raising an open question regarding the improvement of the sample complexity bound to be linear with respect to \(d\) (as well as \(1/\)). Additionally, it might be possible to extend the algorithm's application to more general distributions.

Acknowledgements.This work was supported in part by NSF awards CCF-2007443 and CCF-2134105 and an ARC fellowship.

Figure 5.4: The performance with respect to \(1/\).

Figure 5.3: The performance of Algorithm 1 for fixed sample size \(N\) and varying dimension \(d\).