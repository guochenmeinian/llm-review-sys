# Temporal Graph Neural Tangent Kernel

with Graphon-Guaranteed

Katherine Tieu

University of Illinois Urbana-Champaign

kt42@illinois.edu

&Dongqi Fu

Meta AI

dongqifu@meta.com

&Yada Zhu

IBM Research

yzhu@us.ibm.com &Hendrik Hamann

IBM Research

hendrikh@us.ibm.com &Jingrui He

University of Illinois Urbana-Champaign

jingrui@illinois.edu

Equal Contribution

###### Abstract

_Graph Neural Tangent Kernel_ (GNTK) fuses graph neural networks and graph kernels, simplifies the process of graph representation learning, interprets the training dynamics of graph neural networks, and serves various applications like protein identification, image segmentation, and social network analysis. In practice, graph data carries complex information among entities that inevitably evolves over time, and previous static graph neural tangent kernel methods may be stuck in the sub-optimal solution in terms of both effectiveness and efficiency. As a result, extending the advantage of GNTK to temporal graphs becomes a critical problem. To this end, we propose the temporal graph neural tangent kernel, which not only extends the simplicity and interpretation ability of GNTK to the temporal setting but also leads to rigorous temporal graph classification error bounds. Furthermore, we prove that when the input temporal graph grows over time in the number of nodes, our temporal graph neural tangent kernel will converge in the limit to the _graphon_ NTK value, which implies the transferability and robustness of the proposed kernel method, named **T**emporal **G**raph **N**eural **T**angent **K**ernel with **G**raphon-**G**uaranteed or **Temp-G\({}^{3}\)**NTK**. In addition to the theoretical analysis, we also perform extensive experiments, not only demonstrating the superiority of Temp-G\({}^{3}\)NTK in the temporal graph classification task, but also showing that Temp-G\({}^{3}\)NTK can achieve very competitive performance in node-level tasks like node classification compared with various SOTA graph kernel and representation learning baselines. Our code is available at https://github.com/kthrn22/TempGNTK

## 1 Introduction

Graphs, as a relational structure, model the complex relationships among entities and have attracted much research attention nowadays. To serve various applications, graph neural networks have been extensively studied for their representation learning ability. On the one hand, graph neural networks usually need to build complex neural architectures with hyperparameters to achieve their powerful expressive ability, which is typically a nonlinear process and hard to interpret . On the other hand, graph kernels enjoy the explicit formula and can be convex, leading to solid theoretical results, although their specific form is often hand-crafted and may not be powerful enough to support complicated application scenarios . Hence, _graph neural tangent kernel_ (GNTK)  has been proposed to fuse graph neural networks and graph kernels, enjoying the benefits of bothapproaches, i.e., achieving the excellent representation ability while relying on simple computation processes.

However, in the real world, the graph topology and features are inevitably evolving over time, e.g., the user connections and interests in social networks. This temporal evolution brings new challenges to GNTK research as to how the similarity of temporal graphs is measured and how the corresponding kernel matrix is derived. To be more specific, how can we design a temporal graph neural tangent kernel, which not only _has a superior representation ability than temporal graph neural networks [39; 8]_ but also _inherits the expression simplicity and analysis rigorousness of graph neural tangent kernels [9; 24]_?

Hence, we propose Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed, or Temp-G\({}^{3}\)NTK. **First**, we propose the kernel matrix computation formula for temporal graphs with time-evolving structures and time-evolving node features, and the corresponding kernel value can be used for classification tasks with generalization error bounded. This proposed kernel method addresses how to measure the similarity between temporal graphs to achieve the accuracy of graph neural networks but without complex neural computational procedures like gradient descent. **Second**, considering the property of temporal graphs, we also prove that when the temporal graph is growing, i.e., the number of nodes increases over time, our Temp-G\({}^{3}\)NTK kernel will converge in the limit to the graphon NTK value. This result addresses the challenge of adapting graphon neural network  and graphon neural tangent kernel  to the temporal setting, and, more importantly, demonstrates that Temp-G\({}^{3}\)NTK has the excellent potential to transfer to large-scale temporal graph data with robust performance. **Third**, in addition to the theoretical analysis, we also design extensive experiments for not only temporal graph classification but also temporal node classification, illustrating the effectiveness of the proposed Temp-G\({}^{3}\)NTK compared with various state-of-the-art temporal graph representation learning and graph kernel methods.

## 2 Temporal Graph Modeling

To begin with, we first denote a static undirected graph as \(G=(V,E)\), where \(V\) and \(E\) are sets of vertices and edges, respectively. We also denote the node features of node \(v\) (\(v V\)) as \(_{v}^{d}\), the neighborhood as \((v)\), and edge features of an edge \((u,v)\) as \(_{uv}\).

In order to extend to the temporal setting, researchers usually model the temporal graph \(G\) as a _continuous-time dynamic graph_ (CTDG) , which is mathematically represented as a stream of events, \(G=\{(u,v,_{u}(t),_{v}(t),_{uv}(t),t)\}_{t=t_{0}}^ {t}\), where an event \((u,v,_{u}(t),_{v}(t),_{uv}(t),t)\) indicates that at time \(t\), an edge exists between node \(u\) and \(v\), and \(_{u}(t)\), \(_{v}(t)\), and \(_{uv}(t)\) are the features of \(u,v\), and \((u,v)\) at time \(t\), respectively. To support different computation requirements, a CTDG \(G\) can also be transferred into a _discrete-time dynamic graph_ (DTDG) , which is a collection of snapshots \(G^{(t)}\). To be specific, a snapshot of \(G\) at any time \(t t_{0}\), is denoted as \(G^{(t)}\), which can be obtained by sequentially updating the initial state of \(G^{(t_{0})}\) with the event stream, i.e., \(G^{(t)}=\{(u,v,_{u}(),_{v}(),_{ uv}(),)\}_{t=t_{0}}^{t}\) given \((t_{0} t)\), and temporal graph equals to the last time snapshot, i.e., \(G=G^{(T)}\) given the last timestamp is denoted by \(T\).

Let \(V^{(t)},E^{(t)}\) be the sets of vertices and edges of \(G^{(t)}\). We denote the temporal neighborhood of node \(v\) at time \(t\) as \(^{(t)}(v)=\{(u,):((u,v,_{u}(),_{v }(),_{uv}) G^{(t)}\}\), i.e., a set of nodes \(u\) that are involved in an event with \(v\) at any time \(\) (\(t_{0} t\)). Note that, in the rest of the paper, we use \(G\) to denote an entire temporal graph, and \(G^{(t)}\) as a snapshot. For simplicity, we denote \(t_{0}=0\).

## 3 Preliminaries of Temp-G\({}^{3}\)Ntk

### Temporal Graph Representation Learning

Graph Neural Networks (GNN) are a family of neural architectures for graph representation learning. In general, most GNNs leverage a message-passing framework to compute a node \(v\)'s representation \(_{v}^{(l)}\) at the \(l^{th}\) layer. In the static setting, \(_{v}^{(l)}\) can be obtained by applying a neighborhood aggregation operator on \(_{u}^{(l-1)}\), \( u(v)\), then transforming the aggregated neighborhood information. The graph-level representation can be obtained by applying a pooling function on representations of all nodes, e.g., the summation of all node representations.

To derive Temp-G\({}^{3}\)NTK, our first step is to compute node representations at time \(t\) and then apply a pooling function over all nodes to obtain the graph-level (or snapshot-level) representation. Specifically, we obtain the node representation of \(v\) at time \(t\), \(_{v}(t)\), by aggregating information from its temporal neighborhood \(^{(t)}(v)\) as

\[_{v}(t)=c_{(u,)^{(t)}(v)}[_ {enc}(t-)||_{uv}()||_{u}()]\] (1)

where \(_{u}(t)\) is the node feature of \(u\) at time \(t\), \(_{enc}:^{d_{t}}\) is the time encoding function that can be instantiated  as \(_{enc}( t)=( t)\), and \(^{d_{t}}\) with its \(i^{}\) entry \([]_{i}=^{-(i-1)/}\) and \(d_{t}\) is the dimension of the time representation vector. Operation \([||]\) denotes the concatenation. \(c\) is the scaling factor, and if we set \(c=1\), then Eq. 1 is simply the sum neighborhood aggregation; and if \(c=|^{(t)}(v)|}|\), then Eq. 1 would be the average neighborhood aggregation. Note that if the edge features do not exist then we simply set \(_{uv}(t)=0\). Similarly, if node features are not available then we let \(_{u}(t)=0\).

After aggregating information from node \(v\)'s neighborhood at time \(t\) as Eq. 1, we can input \(_{v}(t)\) into \(L\) layers of Multilayer Perceptrons (MLPs), where the representation of node \(v\) after the \(l^{th}\) MLP projection is as

\[_{v}^{(l)}(t)=}}^{(l)} _{v}^{(l-1)}(t)\] (2)

where \(d_{l}\) is the output dimension of the \(l^{}\) MLP layer, \(\) is a non-linear activation function that can be instantiated as the Rectified Linear Units (ReLU) function.

Furthermore, we denote the graph-level representation of \(G^{(t)}\) as \(_{G}(t)=_{v V^{(t)}}_{v}^{(L)}(t)\).

### Graph Neural Tangent Kernel

Next, we provide some background on the infinite-width limit of a fully connected deep neural network \(f_{nn}\) and derive the definition of NTK and its properties on graphs.

Consider the following settings: given a training dataset of \(n\) samples \(\{(_{i},y_{i})\}_{i=1}^{n}\), where \(_{i}^{d}\) and its label denoted by \(y_{i}\). Let \(f_{nn}(,)\) be the output of a fully-connected neural network, with parameters \(^{p}\) and \(\) as the input. We train \(f_{nn}\) by minimizing the squared loss over the training dataset.

\[()=_{i=1}^{n}(f_{nn}(_{i},)-y_{i})^{2}\] (3)

Let \(^{n d}\) (where \([]_{i}=_{i}\)) and \(^{n}\) (where [\(_{i}\)] = \(y_{i}\)), such that \(f_{nn}(,)\) would be the prediction of \(f_{nn}\), with parameters \(\), over all \(_{i}\) of the training set.

Suppose that \(\) is minimized by gradient descent, so the output \(f_{nn}(,)\) evolves with respect to the training time \(\) as follows .

\[(,())}{d}=-()(f_{nn} (,())-)\] (4)

where \(()\) is the parameters \(\) being updated at training time \(\) based on gradient descent, and \(()\) is a \(n n\) positive definite matrix with its \((i,j)\)-th entry as follows

\[(_{i},())}{ },(_{j},())}{}\] (5)

Existing works on over-parameterized neural networks , and  have proven that for infinite-width neural networks, the matrix \(()\) remains constant during training, and under random initialization of parameters, the matrix \((0)\) converges in probability to a certain deterministic kernel matrix \(^{*}\), which is named as Neural Tangent Kernel . Moreover, as proven in , the prediction of a fully-trained sufficiently wide ReLU neural network is equivalent to the kernel regression predictor with the kernel matrix \(^{*}\).

For the temporal setting, similar to the NTK and the infinite-width neural networks, let \(f_{temp}\) denote the aforementioned temporal GNN in Section 3.1, and \(f_{temp}(G^{(t)},)\) be the output of \(f_{temp}\) with the input \(G^{(t)}\) and parameters \(\). Given two temporal graphs \(G\) and \(G^{}\), at time \(t\), the NTK value corresponds to infinite-width \(f_{temp}\), i.e., in the limit that \(d_{l}\), where \(d_{l}\) is the output dimension stated in Eq. 2, \(l[L]\), such that

\[K(G^{(t)},G^{(t)})=_{(0,1)} ,)}{},,)}{}\] (6)

Then, in the rest of this paper, we can refer to this value \(K(G^{(t)},G^{(t)})\) as the Temp-G\({}^{3}\)NTK value of \(G\) and \(G^{}\) at time \(t\). In the next section, we are ready to introduce how to compute the defined kernel as Eq. 6 without training neural networks.

## 4 Proposed Temp-G\({}^{3}\)NTK

Given two temporal graphs \(G\) and \(G^{}\), we propose to compute the Temp-G\({}^{3}\)NTK value at time \(t\), i.e., \(K(G^{(t)},G^{(t)})\), with \(L\) BLOCK operations 2. We discuss the detailed computation procedure of Temp-G\({}^{3}\)NTK here and leave the theoretical derivation in Section 5.

In general, similar to [1; 9], we first recursively compute the node pairwise covariance matrix \(^{(l)}\), its derivative \(}^{(l)}\), and the node-pairwise kernel matrix \(^{(l)}\) that correspond to the \(l^{}\) BLOCK transformation. Finally, the Temp-G\({}^{3}\)NTK value is obtained by the summation of all of the entries in the kernel matrix of the last BLOCK transformation, i.e., \(^{(L)}\).

To begin with, we initialize the node pairwise covariance matrix \(\) and the kernel matrix \(\) at entries \(u,u^{}\)\((u V^{(t)},u^{} V^{(t)})\) as the inner product of node representations of \(u,u^{}\) at time \(t\), respectively,

\[^{(0)}(G^{(t)},G^{(t)})_{uu^{}}=^{ (0)}(G^{(t)},G^{(t)})_{uu^{}}=_{u}(t)^{T}_{u} ^{}(t)\] (7)

where \(_{u}(t)\) and \(_{u}^{}(t)\) are computed by Eq. 1.

Next, we need to compute \(^{(l)}\) and \(^{(l)}\) that correspond to the \(l^{}\) BLOCK operator with ReLU activation function \(\). As \((x)=(0,x)\), the derivative of \(\) is \((x)=[x 0]\), where \(\) is the indicator vector3. For \(l(1 l L)\), we first define an intermediate covariance matrix as

\[^{(l)}(G^{(t)},G^{(t)})_{uu^{}}= ^{(l-1)}(G^{(t)},G^{(t)})_{uu^{}}&^ {(l-1)}(G^{(t)},G^{(t)})_{uu}\\ ^{(l-1)}(G^{(t)},G^{(t)})_{u^{}u}&^ {(l-1)}(G^{(t)},G^{(t)})_{u^{}u^{}}\] (8)

and \(^{(l)}(G^{(t)},G^{(t)})_{uu^{}}^{2  2}\).

As the covariance matrix \(^{(l)}\)represents the i.i.d centered Gaussian Processes of \(h_{u}(t)\) and \(h_{u}^{}(t)\) after transformed by \(l\) BLOCK operations, we can compute \(^{(l)}\) and \(}^{(l)}\) based on the aforementioned intermediate covariance matrix as

\[^{(l)}(G^{(t)},G^{(t)})_{uu^{}} =_{(a,b)(0,^{(l)}(G^{(t)},G^{(t)})_{uu^{}})}[(a)(b)]\] (9) \[=^{(l-1)}(G^{(t)},G^{(t)} )_{uu^{}})}{2}+^{(l-1)}(G^{(t)},G^{ (t)})_{uu^{}})^{2}}}{2}\]

with

\[}^{(l)}(G^{(t)},G^{(t)})_{uu^{}} =_{(a,b)(0,^{(l)}(G^{(t)},G^{(t)})_{uu^{}})}[(a)(b)]\] (10) \[=^{(l-1)}(G^{(t)},G^{(t)}) _{uu^{}})}{2}\]Eq. 9 and Eq. 10 hold due to the closed form of the kernel function with ReLU activation \(g(x,y)=_{w}[^{}(w^{T}x)^{}(w^{T}y)]=( -y}}{2})\).

Then, the \(l^{}\) kernel matrix, \(^{(l)}\), is obtained as

\[^{(l)}(G^{(t)},G^{(t)})_{uu^{}}=^{(l -1)}(G^{(t)},G^{(t)})_{uu^{}}}^{(l)}(G^{ (t)},G^{(t)})_{uu^{}}+^{(l)}(G^{(t)},G^{(t)}) _{uu^{}}\] (11)

Finally, the Temp-G\({}^{3}\)NTK value of \(G,G^{}\) at time \(t\) is

\[K(G^{(t)},G^{(t)})=_{v V^{(t)}}_{v^{} V^{(t)}} ^{(L)}(G^{(t)},G^{(t)})_{vv^{}}\] (12)

We perform summation over all entries since in our proposed neural architecture in Section 3.1, we can obtain the graph embedding by applying a pooling function, e.g., sum pooling, on node-level representations.

The pseudo-code of computing the Temp-G\({}^{3}\)NTK kernel as above is shown in Appendix A.

## 5 Theoretical Analysis of Temp-G\({}^{3}\)Ntk

### Kernel Properties of Temp-G\({}^{3}\)Ntk

To begin with, we first show that our proposed kernal function Temp-G\({}^{3}\)NTK satisfies symmetric and semi-definite below, and the full proof can be found in Appendix B

**Theorem 5.1**.: _Temp-G\({}^{3}\)NTK is symmetric._

**Theorem 5.2**.: _Temp-G\({}^{3}\)NTK is positive semi-definite._

### Generalization Bound of Temp-G\({}^{3}\)Ntk

We first state how to utilize Temp-G\({}^{3}\)NTK as a kernel regression predictor for the temporal graph classification task; then, we establish a data-dependent generalization error bound of the function class of kernel regression predictors that are associated with Temp-G\({}^{3}\)NTK.

To be more specific, we can instantiate the problem of temporal graph classification, where, given an i.i.d training set of \(n\) temporal graphs \(\{G_{1},G_{2},,G_{n}\}\) and their labels \(\{y_{1},y_{2},,y_{n}\}\), our goal is to predict the label 4 of a testing temporal graph \(G_{test}\). Then, the prediction of \(G_{test}^{(t)}\) at any time \(t\) by a kernel regression predictor \(f_{kernel}\) associated with Temp-G\({}^{3}\)NTK kernel \(K(\,,)\) is expressed as follows,

\[f_{kernel}(G_{test}^{(t)})=[K(G_{test}^{(t)},G_{1}^{(t)}),,K(G_{ test}^{(t)},G_{n}^{(t)})][_{train}^{(t)}]^{-1}\] (13)

where \(_{train}^{(t)}\) is a positive definite \(n n\) kernel matrix, whose \((i,j)\)-th entry is the Temp-G\({}^{3}\)NTK value of \(G_{i}^{(t)},G_{j}^{(t)}\), i.e., \([_{train}^{(t)}]_{i,j}=K(G_{i}^{(t)},G_{j}^{(t)})\) and \(^{n}\) is the label space of temporal graphs, whose \(i^{}\) entry is \([]_{i}=y_{i}\).

Then, we consider any loss function \(:\) that is \(-\)Lipschitz. We define the generalization error of the predictor \(f_{kernel}\) in Eq. 13 at time \(t\) that acts on a temporal graph \(G\) labeled by \(y\) as

\[[(f_{kernel}(G^{(t)})y)|\{G^{(1)},,G^{(t-1)}\}]-(f_ {kernel}(G^{(t)}),y)\] (14)

where the expectation is taken over all \(G^{(t)}\) drawn from the stochastic process \(_{t}(.|G^{(1)},,G^{(t-1)})\) conditioned on all previous snapshots before time \(t\) of temporal graph \(G\). The following theorem establishes the generalization error bound on all snapshot \(G^{(t)}\) of \(G\).

**Theorem 5.3**.: _Given \(n\) i.i.d training samples and their labels \(\{G_{i},y_{i}\}_{i=1}^{n}\) and \(G_{i}\) has \(t\) timestamps, let \(_{train}^{(t)}^{n n}\) be the kernel matrix of pairwise Temp-G\({}^{3}\)NTK values between graphs of the training set at time \(t\) and \(f_{kernel}\) be the kernel regression predictor based on the training set and \(^{(t)}_{train}\). Consider any loss function \(:\) that is \(-\)Lipschitz, the generalization error of the \(f_{kernel}\) predictor can be upper bounded as_

\[_{}& _{t=1}^{T}[(f_{kernel}(G^{(t)}),y)|\{G^{(1)}, ,G^{(t-1)}\}]-(f_{kernel}(G^{(t)}),y)\\ &_{t}^{T}[^{(t)}_ {train}]^{-1}(^{(t)}_{train})\] (15)

_where \(\) is the class containing all \(-\)Lipschitz functions, the expectation is taken over all \(G^{(t)}\) that is drawn from \(_{t}(|G^{(1)},,G^{(t-1)})\)._

In brief, inspired by existing works on generalization bounds for kernel classes , we first bound our generalization error by the Sequential Rademacher Complexity [38; 26] of \(\) (i.e., the function class containing kernel such as \(f_{kernel}\)), and then bound this complexity measure by \((_{t}^{T}[^{(t)}_{train}]^{-1} (^{(t)}_{train}))\), where \(_{t}^{T}[^{(t)}_{train}]^{-1}(^{(t)}_{train}))\) gives maximum value of \(^{T}[^{(t)}_{train}]^{-1}( ^{(t)}_{train}))\) over all timestamps of the training temporal graphs. The classification to the temporal graph \(G\) is the max-aggregation of \(f_{kernel}(G^{(t)})\). The full proof is in Appendix C.

### Convergence of Temp-G\({}^{3}\)Ntk

In this part, we investigate our Temp-G\({}^{3}\)NTK value on two growing temporal graphs, \(G\) and \(G^{}\). "_Growing_" means the number of nodes in \(G\) and \(G^{}\) would increase with time, and the following theorem shows that the proposed Temp-G\({}^{3}\)NTK enjoys a rigorous convergence. To verify this, we first adopt the definition of _Graphon NTK_ on a single growing graph  and then extend the concept to different and temporal graphs to establish the convergence of Temp-G\({}^{3}\)NTK value of \(G,G^{}\) as follows. The full proof is provided in Appendix D.

**Theorem 5.4**.: _Given two growing temporal graphs \(G\) and \(G^{}\) and two graphons \(W\) and \(W^{}\), suppose snapshots of \(G\) (i.e., \(G^{(t)}\)) converge to \(W\) and snapshots of \(G^{}\) (i.e., \(G^{(t)}\)) converge to \(W^{}\), as \(t\). Then, the graphon neural tangent kernel induced by Temp-G\({}^{3}\)NTK of \(G,G^{}\) at time t, i.e., \(K_{W}(W^{(t)},W^{(t)})\), converges in the limit of the operator norm to the graphon neural tangent kernel of \(W\) and \(W^{}\), i.e., \(K_{W}(W,W^{})\), as follows:_

\[_{t}||K_{W}(W^{(t)},W^{(t)})-K_{W}(W,W^{}) || 0\] (16)

_where \(K_{W}\) denotes the graphon NTK value._

This theorem addresses the convergence limitations of previous work  in terms of different temporal graphs. In other words, besides temporal dependencies between snapshots of different evolving graphs, the work  only establishes a limit object for different stages of a single growing graph. An empirical visualization can be seen in Figure 1, and the detailed comparison and illustration are delivered in the Appendix D.3.

### Time Complexity of Temp-G\({}^{3}\)Ntk

Here, the following table shows the time complexity comparison between our Temp-G\({}^{3}\)NTK with other graph kernel and graph representation learning methods for measuring \(n\) pairs of temporal graphs at a certain timestamp \(t\).

In the above table, we first need to declare some mathematical notation as follows. \(|V|,|E|\) denote the maximum size of the vertex set and edge set among \(n\) given graphs. Then, for the time complexity of WL-Subtree , Graph2Vec , and GL2Vec , \(h\) denotes the number of iterations in WL-Subtree algorithms; for Graph2Vec  and GL2Vec , \(D\) represents the maximum degree of the rooted subgraphs that are used to compute graph embeddings; and for the time complexity of NetLSD , \(k\) denotes the number of eigenvalues (obtained from the graph Laplacian matrix) used to compute the graph embeddings; for TGN , \(L_{hop}\) denotes the number of neighbor hops that a node can aggregate information from; for our Temp-G\({}^{3}\)NTK, based on Section 4 and Appendix A,represents the number of BLOCK operations; and \(\) denotes the number of training epochs for all neural representation learning algorithms.

Notably, our method Temp-G\({}^{3}\)NTK falls into the category of graph kernels, and its computational complexity is cheaper than [5; 46]. Also, compared with graph neural representation methods [6; 33; 45; 39; 8], the computation iteration of Temp-G\({}^{3}\)NTK does not rely on neural computation like gradient descent and backpropagation, such that the empirical execution time of our method is still faster. Moreover, we further demonstrate our Temp-G\({}^{3}\)NTK's efficiency by providing empirical runtime comparison in Table 3, and the detailed empirical effectiveness comparison of these methods is shown in the next section.

## 6 Experiments

In this section, we demonstrate the performance of Temp-G\({}^{3}\)NTK by crucial tasks of temporal graph learning. More extra experiments about _ablation study_, _parameter analysis_, and _robustness_ can be referred to Appendix F.

### Graph-Level Experiments

**Datasets**. Targeting temporal graph classification, we conduct experiments on one of the most advanced temporal graph benchmarks that have graph-level labels, i.e., TUDataset 5, the four datasets are infectious, dblp, facebook, and tumblr, the detailed dataset statistics can also be found in Appendix G.1. Additionally, we also leveraged the more large-scale temporal datasets reddit, WikiPedia, LastFM, and MOOC from 6. Those datasets are large but do not have graph-level labels, so we use them to demonstrate the scalability of Temp-G\({}^{3}\)NTK on temporal graph similarity measurement. The detailed dataset statistics can be found in Appendix G.1, and corresponding experimental results can be found in Appendix F.3. Below, we focus on introducing temporal graph classification experiments and findings.

**Problem Setting**. For each dataset above, we evaluate the temporal graph classification accuracy by conducting 5-fold cross-validation and then report the mean and standard deviation of test accuracy. To be specific, given a dataset of \(n\) temporal graphs \(\{G_{1},G_{2},...,G_{n}\}\) and their labels \(\{y_{1},y_{2},...,y_{n}\}\), and in all four datasets, label \(y_{i}\) of the temporal graph \(G_{i}\) is already time-aware, which means the value does not change with respect to time. Also, edge features are not provided in these four datasets, and we apply the Temp-G\({}^{3}\)NTK formula with plain time encoding as stated in Eq. 1.

**Baselines**. We compare Temp-G\({}^{3}\)NTK with a range of graph classification algorithms: (1) Graph Kernels, including WL-Subtree Kernel , Random Walk Kernel , and Shortest-Path Kernel : (2) Graph Representation Learning methods, including Graph2Vec , NetLSD , GL2Vec ; and (3) Temporal Graph Representation Learning algorithms, including TGN , GraphMixer , EvolveGCN . Details about the implementation and parameters of each algorithm are deferred to Appendix G.

   Method & Runtime Complexity \\  WL-Subtree  & \((nh|E|+n^{2}h|V|)\) \\ Shortest Path  & \((n^{2}|V|^{4})\) \\ Random Walk  & \((n^{2}|V|^{3})\) \\  Graph2Vec  & \((n^{2}|V|Dh|E|)\) \\ NetLSD  & \((n^{2}(k|E|+k^{2}|V|))\) \\ GL2Vec  & \((n|V|^{2}+n^{2}|V|Dh|E|)\) \\  GraphMixer  & \((n^{2}+n|V|K)\) \\ TGN  & \((n^{2}+n(|V|+|E|)L_{hop})\) \\  Temp-G\({}^{3}\)NTK (Ours) & \((n^{2}L|V|^{2}+n|E|)\) \\   

Table 1: Total Runtime Complexity of Computing Similarity for \(n\) Pairs of Graphs at Timestamp \(t\).

**Results**. The graph classification results are shown in Table 2, and the best test accuracy is highlighted in bold. Our method, Temp-G\({}^{3}\)NTK, outperforms the other methods on all four datasets. In particular, the most notable gap between Temp-G\({}^{3}\)NTK and the other methods lies in the facebook dataset, where Temp-G\({}^{3}\)NTK gains 70\(\%\) accuracy. In addition, as the label for each graph remains unchanged, we evaluate the performance of baseline algorithms on different timestamps until the end of the temporal graph, and report their highest accuracy score in Table 2.

We also provide a better illustration of how baseline algorithms perform at different timestamps of the infectious and facebook datasets through Figure 1. For Figure 1, as stated in our problem setting, each temporal graph is associated with a label, and the label is fixed across timestamps. Therefore, we expect our method to perform well, i.e., achieve a competitive accuracy score across all timestamps. As illustrated, Figure 1 shows that Temp-G\({}^{3}\)NTK performs robustly across all timestamps and achieves the highest accuracy at most times, which also recalls the convergence ability of Temp-G\({}^{3}\)NTK as proved in Section 5.3.

Further, in Table 3, we present the runtime comparison for four datasets, infectious, dblp, facebook, and tumblr. Overall, the empirical running time aligns with our theoretical analysis of time complexity in Table 1. That is, our method belongs to the graph kernel category, where the node-wise comparison is usually inevitable, and our time complexity is lower. Compared to the neural baselines, since our method does not rely on complex neural training like gradient descent and backpropagation, our method is still efficient.

Given our method achieved the best classification accuracy, as shown in Table 2, according to the corresponding running time reported in Table 3, our method is (1) more than 10x - 20x faster then complex temporal graph neural network methods like GraphMixer  and TGN ; (2) similarly efficient as simple kernel methods like WL-Subtree  and Shortest Path  and embedding

   Method & infectious & dblp & facebook & tumblr \\  WL-Subtree  & 0.600 \(\) 0.044 & 0.520 \(\) 0.068 & 0.650 \(\) 0.075 & 0.570 \(\) 0.121 \\ Shortest Path  & 0.670 \(\) 0.075 & 0.560 \(\) 0.049 & 0.560 \(\) 0.086 & 0.580 \(\) 0.143 \\ Random Walk  & 0.670 \(\) 0.073 & 0.530 \(\) 0.058 & 0.590 \(\) 0.093 & 0.580 \(\) 0.112 \\  Graph2Vec  & 0.565 \(\) 0.081 & 0.539 \(\) 0.031 & 0.538 \(\) 0.028 & 0.547 \(\) 0.071 \\ NetLSD  & 0.625 \(\) 0.061 & 0.558 \(\) 0.035 & 0.535 \(\) 0.011 & 0.552 \(\) 0.046 \\ GL2Vec  & 0.545 \(\) 0.051 & 0.562 \(\) 0.030 & 0.538 \(\) 0.031 & 0.558 \(\) 0.080 \\  GraphMixer  & 0.500 \(\) 0.000 & 0.563 \(\) 0.011 & 0.561 \(\) 0.023 & 0.509 \(\) 0.508 \\ TGN  & 0.520 \(\) 0.019 & 0.580 \(\) 0.003 & 0.559 \(\) 0.018 & 0.517 \(\) 0.025 \\ EvolveGCN  & 0.521 \(\) 0.093 & 0.400 \(\) 0.089 & 0.516 \(\) 0.075 & 0.395 \(\) 0.089 \\  Temp-G\({}^{3}\)NTK (Ours) & **0.740 \(\) 0.058** & **0.600 \(\) 0.063** & **0.700 \(\) 0.138** & **0.630 \(\) 0.068** \\   

Table 2: Comparison of Temporal Graph Classification Accuracy.

Figure 1: Comparison of _test accuracy_ with respect to _different stages of temporal graphs_ from the infectious and facebook datasets. The \(y\)-axis in each plot is the accuracy, and the \(x\)-axis represents what percentage of timestamps have been taken into account. For example, at \(x=1/5\), the accuracy is obtained by performing classification on the first \(1/5\) timestamps of each graph.

methods like NetLSD  and GL2Vec ; and only Graph2Vec  is running faster than our method, but our performance is roughly 1.4x better.

### Node-Level Experiments

In this section, we evaluate the performance of Temp-G\({}^{3}\)NTK for the temporal node property prediction task. Specifically, we leverage the final node-pairwise kernel matrix computed by Eq. 11, i.e., \(^{(L)}\), and obtain node predictions by performing kernel regression with \(^{(L)}\).

**Datasets**. We demonstrate Temp-G\({}^{3}\)NTK's capability of performing dynamic node prediction on the tgbn-trade dataset from the Temporal Graph Learning Benchmark (TGB) , and the details of TGB can be found at this link 7. The training, validation, and test sets of tgbn-trade are defined in the TGB package with \(70\%/15\%/15\%\) chronological splits. To assess the performance of a method on tgbn-trade, we use the normalized discounted cumulative gain (NDCG) metric that is assigned to tgbn-trade in the TGB package.

**Problem Setting**. Given a temporal graph with _node labels that change with respect to time_, the Node Property Prediction task requires the prediction of labels of some nodes at a certain time \(t\), given that our predictor can leverage all information about the temporal graphs from the initial timestamps up to some certain timestamp \(\) with \(<t\). To be specific, the predictor for node labels by using Temp-G\({}^{3}\)NTK at time \(t\) would be:

\[^{(L)}(G^{(t)},G^{()})[^{(L)}(G^{()},G^{( )})]^{-1}(t)\] (17)

where \((t)^{n d_{label}}\) is a vector whose \(i^{}\) entry is the label of node \(i\) at time \(t\), and \(d_{label}\) is the dimension of node labels.

Through the lens of kernel regression, \([^{(L)}(G^{()},G^{()})]\) acts as the gram matrix, similar to the role of \(_{train}\) in Eq. 13, and \(^{(L)}(G^{(t)},G^{()})\) acts as the kernel values between the test and training samples. In order to effectively utilize Temp-G\({}^{3}\)NTK for node property prediction, we perform kernel regression with C-SVM and employ \(^{(L)}\) as the pre-computed kernel. The regularization parameter, \(C\), of our SVM predictor is searched over \(120\) values evenly sampled from the interval \([10^{-2},10^{4}]\) in log scale. The number of BLOCK operations, \(L\), is searched over \(\{1,2,3\}\), and we obtain the best NDCG score with \(L=1\).

**Baselines**. We compare Temp-G\({}^{3}\)NTK with deep learning algorithms on the tgbn-trade's leaderboard, which include TGN , DyRep , and DyGFormer . TGN  is discussed in the temporal graph classification task. DyRep  is a deep temporal point process model, which is parameterized by a temporal-attentive representation network encoding time evolving structural information into node representations. DyGFormer  is a Transformer-based architecture for dynamic graph learning, which learns from nodes' historical first-hop interactions by the neighbor co-occurrence sampling and patching scheme with the Transformer neural architecture.

With selected temporal graph representation learning baseline methods, we then report the NDCG scores of baseline algorithms based on tgbn-trade's leaderboard.

**Results**. The results for temporal node property prediction on tgbn-trade are shown in Table 4, and the best NDCG score is highlighted in bold. Temp-G\({}^{3}\)NTK achieves very competitive results, with

   Method & infectious & dblp & facebook & tumblr \\  WL-Subtree & 16.04 & 10.93 & 13.88 & 9.80 \\ Shortest Path & 20.36 & 16.13 & 32.29 & 16.29 \\ Random Walk & 489.65 & 566.64 & 5380.26 & 972.929 \\  Graph2Vec & 3.43 & 3.45 & 3.87 & 3.42 \\ NetLSD & 14.82 & 15.20 & 33.90 & 15.06 \\ GL2Vec & 17.75 & 14.36 & 23.95 & 13.77 \\  GraphMixer & 217.70 & 537.22 & 720.16 & 219.29 \\ TGN & 254.59 & 873.03 & 1101.66 & 394.34 \\  Temp-G\({}^{3}\)NTK (Ours) & 23.04 & 21.00 & 25.86 & 21.04 \\   

Table 3: Runtime of Baselines for Each Dataset in Secondsthe test NDCG score of 0.380, outperforming TGN and DyRep and approaching DyGFormer very closely, despite that baselines rely on heavy graph neural architectures like graph neural network or graph transformer. These results show that Temp-G\({}^{3}\)NTK has the potential to extend to the temporal node property prediction task and capture node-level information.

## 7 Related Work

Graph neural representation learning attracts many research interests and serves for many interesting tasks like recommendation [3; 36; 37], time series forecasting [29; 15], and social network analysis [28; 14; 27]. In which research domain, many efforts have been devoted to develop non-neural computations and temporal settings. **Graph Neural Tangent Kernel**. Graph Neural Tangent Kernel (GNTK)  introduces a class of graph kernels that corresponds to infinite-width GNNs with sum neighborhood aggregator. Building upon the foundations of GNTK, a line of works unveil different theoretical aspects of GNTK. For example,  improves the computation time of constructing the gram matrix of GNTK;  studies the behavior of GNTK that aligns with GNNs with large depth; and most relevant to our theoretical results (Theorem 5.4),  combines GNTK with the concept of graphons to derive the Graphon Neural Tangent Kernel. **Graphons, Graphon Neural Network, and Graphon Neural Tangent Kernel**. A graphon is a symmetric, bounded, and measurable function \(W:^{2}\) that acts as the limit object of dense graph sequences and defines a family of similar graphs. Similarly, Graphon Neural Networks (WNNs)  are proven to be the limit object of GNNs that operates on a sequence of graphs as the graph's size grows. Graphon Neural Tangent Kernel (WNTK)  defines the NTK that resonates with the infinite-width WNNs and proves that the GNTK converges to the corresponding WNTK as the size of the graph grows. **Temporal Graph Learning**. Most temporal graph learning methods are comprised of complex architectures that leverage the message passing framework, a time encoding function that captures time representation and distinguishes different timestamps. Some works also employ recurrent architecture to capture past information and update the node or edge representation at a current time \(t\) based on representations of previous time \(\), where \(<t\). For example, JODIE  employs RNN to update the history representation of \(v\) at time \(t\). TGAT  utilizes the self-attenion mechanism (SAM) to compute the temporal representation of node \(v\). TGN  employs recurrent architecture to capture the history representation of \(_{v}(t)\) (similar to JODIE ) and then performs neighborhood aggregation to obtain the temporal node representation of \(v\) at time \(t\), which is similar to TGAT . GraphMixer  first constructs edge representation by aggregating raw edge features and then concatenates them with relative difference time encoding. Then, the temporal node representation is determined by aggregating the aforementioned edge representation, \(_{v}(t)\). The node representation is further transformed by MLP and Mixer-MLP layers. For a more comprehensive comparison between Temp-G\({}^{3}\)NTK and previous recurrent neural network works on Temporal Graph Learning, we refer readers to Appendix E, where we provide detailed illustration of more Temporal Graph Learning methods, DGNN , EvolveGCN , ROLAND , and SSGNN .

## 8 Conclusion

In this paper, we study the graph neural tangent kernel within the temporal graph setting and propose a temporal graph neural tangent kernel named Temp-G\({}^{3}\)NTK, which allows the input graph structure and node features to evolve over time and output the pairwise similarity. The proposed Temp-G\({}^{3}\)NTK enjoys the computational efficiency, expressive representation ability of temporal graph neural networks, and rigorous error bound. Moreover, the proposed Temp-G\({}^{3}\)NTK also follows the graphon convergence property. Empirically, we not only test Temp-G\({}^{3}\)NTK in the temporal graph-level experiments and demonstrate its superior accuracy but also extend it to deal with temporal node-level tasks, where Temp-G\({}^{3}\)NTK also shows competitive performance.

   Method & Validation & Test \\  DyGFormer  & \(\) & \(\) \\ TGN  & 0.395 \(\) 0.002 & 0.374 \(\) 0.001 \\ DyRep  & 0.394 \(\) 0.001 & 0.374 \(\) 0.001 \\  Temp-G\({}^{3}\)NTK (Ours) & 0.397 \(\) 0.039 & 0.380 \(\) 0.008 \\   

Table 4: NDCG Score for Node Property Prediction on the tgbn-trade Dataset.