# GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes

Gaochao Song  Chong Cheng  Hao Wang

The Hong Kong University of Science and Technology (Guangzhou)

gcsong4@gmail.com

ccheng735@connect.hkust-gz.edu.cn

haowang@hkust-gz.edu.cn

Both authors contributed equally to this research.The corresponding author.

###### Abstract

In this paper we present a novel method for efficient and effective 3D surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF) based works typically require extensive training and rendering time due to the adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS) uses an explicit and discrete representation, hence the reconstructed surface is built by the huge number of Gaussian primitives, which leads to excessive memory consumption and rough surface details in sparse Gaussian areas. To address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which establish a continuous scene representation based on discrete 3DGS through kernel regression. The GVKF integrates fast 3DGS rasterization and highly effective scene implicit representations, achieving high-fidelity open scene surface reconstruction. Experiments on challenging scene datasets demonstrate the efficiency and effectiveness of our proposed GVKF, featuring with high reconstruction quality, real-time rendering speed, significant savings in storage and training memory consumption. Project page: https://3dagentworld.github.io/gvkf/.

## 1 Introduction

3D surface reconstruction in open scenes holds great significance in various practical applications, such as autonomous driving, virtual reality, urban planning and etc. However, achieving high-fidelity and efficient open scene reconstruction has been a longstanding challenge, due to the trade-off between the rendering quality and the required resources for optimization.

In pursuit of this goal, two predominant approaches are Neural Radiance Fields (NeRF)  and 3D Gaussian Splatting (3DGS)  based methods. On one hand, NeRF-based implicit representations typically require extensive training and rendering time, which limits the practical use in large-scale scene reconstruction . On the other hand, 3DGS  adopts explicit representations, which enables high-quality novel view synthesis while achieving real-time rendering. This makes 3DGS more feasible for efficient scene reconstruction in the applications such as autonomous driving and virtual reality.

Recently, there are studies using 3DGS technology for novel view synthesis and surface reconstruction in street scenes and urban environments . For instance, SuGaR  attempts to reconstruct the 3D surfaces based on Gaussian points. However, it has been noted that overly large and sparse Gaussian points can significantly affect the geometric representations of the scene,particularly in background areas. To overcome these challenges, the 2D Gaussian Splatting (2DGS)  proposes to use Gaussian surfaces as surfels to represent complex geometries , thereby improving the surface reconstruction quality. Particularly, 2DGS faces challenges when processing large-scale scenes, as it requires the explicit representation of a large number of Gaussian primitives, leading to significant GPU memory consumption. Therefore, 2DGS still exhibits limitations in novel view synthesis capabilities and the geometric representation of large-scale scenes.

In Table 1, we summarize the comparison of 3DGS rendering and volume rendering. To fully leverage the fast rendering advantages of Gaussian alpha blending while achieving effective implicit scene representation, we propose a novel Gaussian Voxel Kernel Functions (GVKF) method. Firstly, GVKF utilizes voxelization to implicitly represent 3DGS, managing the growth and pruning of Gaussian splats. This approach retains the expressive power of explicit Gaussian splats while enabling efficient management of these splats. Secondly, we carefully analyze the intrinsic connection between Gaussian splatting alpha blending rendering and traditional volume rendering from a mathematical perspective. We establish a 3DGS-based method to represent continuous scene opacity density fields through kernel regression. This makes it possible for discrete Gaussians to represent continuous scenes. By replacing the discrete opacity values in original 3DGS rendering pipeline (which can be viewed as collapsed kernel functions) with Gaussian kernel functions, we maintain the advantages of the original 3DGS alpha blending while optimizing the representation of continuous scenes. Moreover, we demonstrate that our proposed rendering method is mathematically consistent with traditional volume rendering. Thirdly, based on our constructed scene opacity representation, which is also known as the scene opacity field, we derive the bidirectional mapping relationship between opacity and the scene surface. This enables direct mesh extraction for scene surface. In summary, our contributions are as follows:

* We propose GVKF, an implicit continuous scene reconstruction method that integrates the effectiveness of implicit representation with the fast rasterization advantages of Gaussian Splatting, without the need for computationally intensive volume rendering.
* Based on GVKF, we further propose implicit representation of the scene surface, achieving efficient and high-quality scene surface reconstruction.
* Experiments demonstrate the usefulness of GVKF in open scenes, showcasing high-quality surface reconstruction accuracy, real-time rendering speeds, and significant savings in storage and memory consumption.

## 2 Related Works

### Novel View Synthesis

The introduction of Neural Radiance Fields (NeRF)  has significantly advanced the development of 3D reconstruction and novel view synthesis. NeRF employs volumetric rendering techniques to intricately simulate the geometric structure of scenes and viewpoint-dependent characteristics, thereby considerably enhancing the quality of image rendering. Following NeRF, variants such as Mip-NeRF  and Zip-NeRF  have addressed the aliasing issues during rendering. Additionally, UC-NeRF , designed for outdoor scenes, enhances image consistency through color correction and pose refinement. InstantNGP  accelerates training and improves rendering efficiency by optimizing subvolume processing with grid pyramid techniques. Meanwhile, other feature grid-based scene representation methods [3; 22; 34; 5; 45] have been extensively explored to enhance the training capability and expressiveness of models. Recently, 3D Gaussian Splatting (3DGS)  effectively represents complex scenes using 3D Gaussian points, significantly boosting the efficiency of real-time high-resolution image rendering while maintaining rendering quality. Further research efforts like Scaffold-GS and Octree-GS [25; 32] have attempted more effective methods to organize and manage Gaussian points, which helps reduce memory usage and speed up training.

  
**Method** & **Math Expression** & **Pros** & **Cons** \\ 
3DGS Rendering & Discrete summation & Fast rendering & High Mem consumption, Hard to fit 3D continuous surface \\  Volume Rendering & Continuous integration & Better 3D surface representation & Low rendering speed due to continuous sampling \\   

Table 1: Comparison of 3DGS rendering and volume rendering methods.

### Surface Reconstruction

Traditional isosurface extraction, relying on density thresholds, often struggles with fine details due to resolution and noise constraints. Recent studies propose more complex representation methods . For instance, NeuS  uses MLP networks for occupancy grids or SDF, improving reconstruction accuracy and noise reduction [29; 43; 20; 7; 42]. Techniques like BakedSDF  translate the optimization of NeRF or neural SDFs into 3D meshes, enhancing features through high-resolution grids but increasing computational load. NeuS2  introduces a novel formula for second-order derivatives with multi-resolution hash encoding and CUDA-based MLP technology, significantly reducing training time. StreetSurf  optimizes SDF mappings in open scenes and decouples static and dynamic objects. Despite advancements, NeRF-based methods still need optimization for processing speed and real-time rendering.

3DGS has gained attention for its high-quality scene reconstruction and rapid processing capabilities . 3DGS uses multiple 3D Gaussian distributions with anisotropic covariance for precise control over scene attributes [49; 18]. This technology enhances surface reconstruction methods like SuGaR , which employs Poisson surface reconstruction for fast and accurate mesh extraction. However, irregular Gaussian sphere distribution affects surface quality. To improve this, 2DGS  uses 2D Gaussian planes for better surface conformity and TSDF for accurate reconstruction, though it may cause surface fragmentation. GOF  directly extracts surfaces using opacity thresholds and tetrahedral mesh extraction but is limited by high VRAM requirements. GSDF  combines 3DGS with a NeuS-like SDF branch for optimized rendering and reconstruction, increasing training time. Despite their potential, 3DGS-based methods face challenges like managing Gaussian points, high VRAM consumption, and degraded rendering quality.

## 3 Methods

As shown in Fig. 1, we first introduce the implicit neural 3DGS primitives representation based on a sparse voxel grid, which offers highly efficient storage management and the fitting power of neural networks. Secondly, we present our GVKF-based continuous scene representation, to explain its rationale, we have analyzed its intrinsic connections with Gaussian alpha blending  and traditional volume rendering  from a statistical analysis perspective. Finally, we describe the relationship between the proposed continuous scene representation (a neural opacity field) and implicit surface, and derive an explicit mapping function for mesh reconstruction.

### Voxel Gaussian Representation

To achieve orderly 3DGS management while minimizing the explicit expression of them to save training storage consumption, we use a spatial sparse voxel grid to manage Gaussian primitives. During the initialization phase, the sparse grid is generated from the downsampled SfM point clouds

Figure 1: Framework of Gaussian Voxel Kernel Functions (GVKF) for scene representation. In this framework, discrete Gaussian primitives \(\) represent continuous opacity density \((t)\) on the ray via kernel regression. After slightly modifying the rasterization pipeline, the kernel function can be integrated into alpha blending rasterization without introducing dense points sampling. Additionally, we directly define the mapping relationship between the neural opacity field and the implicit surface.

and dynamically grows or being eliminated during training. Each sparse grid is allowed to generate up to \(m\) Gaussian primitives, and all these primitives are limited to a small range of space centered at the voxel grid.

**Gaussian Generation** For a particular 3D Gaussian expression, five attributes are required: \(p^{3}\) (position), \(\) (opacity), \(R^{3 3}\) (rotation matrix), \(s^{3}\) (scaling), and \(c^{3}\) (color). Then, a Gaussian \((x)\) can be generated as:

\[(x)= e^{-(x-p)^{T}^{-1}(x-p)},\] (1)

where \(^{3 3}\) is covariance matrix defined as \(=Rss^{T}R^{T}\). \(c\) is calculated via SH coefficients and camera direction. Different from traditional 3DGS that treats them as explicit optimizable tensors, we decode them from a feature vector \(^{d}\) via several MLPs:

\[=_{}(,),R=_{R}( ),s=_{s}(),c=_{c}(, ).\] (2)

For alpha and color MLPs, the view camera and feature vector \(\) are inputs, facilitating view-dependent fitting. Relative coordinates of Gaussians to the parent voxel center are stored with \(\), compressing explicit Gaussian components and leveraging the MLP's fitting capacity. Gaussians are dynamically generated each iteration and recycled post-update, reducing memory usage.

**Voxel Registration**. To control Gaussian numbers in large open scene, we eschew the traditional adaptive density control strategy, adopting a method inspired by scaffold-Gaussian  and Octree-Gaussian . The voxel registration is based on gradient accumulation. After each iteration, gradients from 3DGS are recorded and accumulated in their respective voxels, denoted as \(\). Voxels where \(\) exceeds a set threshold are subdivided into eight subvoxels to increase grid resolution, continuing until the maximum depth is reached. Additionally, less frequently used voxels are discarded after a specified period.

### Neural Opacity Field of 3DGS

Since 3DGS rasterization rendering and traditional volume rendering share some overlapping concepts, in this section, we sort them out and introduce our method from a statistical perspective while avoiding introducing redundant mathematical symbols.

**Continuous Scene Description**. We define \((t):[0,+]\) as the opacity density function, which measures the probability of a ray encountering a particle at position \(t\). We define \((t):[0,+]\) as the transmission function, which measures the probability that a ray has not encountered any particles from its origin to point \(t\). Considering the probability that a ray does not encounter any particles at time step \(t+dt\), denoted as \((t+dt)\), it is evident that \((t+dt)=(t)(1-(t)dt)\). Solving this differential equation, we obtain the relationship between \((t)\) and \((t)\):

\[(t)=(-_{0}^{t}(t)dt).\] (3)

Therefore, we obtain the cumulative distribution function (CDF) of the probability that a ray **hits** a particle over the interval \([0,t]\): \((t)=1-(t)\), with the corresponding probability density function (PDF) being \(^{}(t)=(t)(t)\). From the perspective of volume rendering, this PDF is used as the probability of the appearance of color along the ray, ultimately taking the mathematical expectation of the color as the ray color:

\[C=_{0}^{B}(t)(t) c(t)dt+(B) c_{ bg}.\] (4)

The discrete formulation of volume rendering Eq. 4 is:

\[C=_{i=1}^{N}T_{i}_{i} c_{i},_{i}=(1-(- _{i}_{i})), T_{i}=_{j=1}^{i-1}(1-_{j})\] (5)

where opacity \(_{i}\) represents the accumulated result in a sampling interval \(_{i}\) of volume density \(_{i}\), hence the value of \(N\) does not influence the result as long as \(_{i}\) is adapted enough. Based on the similar idea of volume rendering, the PDF \(^{}(t)=(t)(t)\) can also be reasonably considered as the probability of the appearance of a surface along the ray, where the place with the highest probability density is most likely to have a surface. Correspondingly, on the CDF \((t)\), this is the place where the derivative is the largest. In this paper, we use the CDF \((t)\) to describe continuous scenes based on the camera rays, to facilitate integration with the 3DGS rasterization rendering pipeline. In section 3.3, we will prove that under the 3DGS representation, the place where the derivative of CDF is the largest is not actually the surface, so a method to locate the surface will be introduced.

**Kernel Regression of 3DGS**.

In implicit scene representation methods based on volume rendering, the continuous opacity density function is directly predicted by a MLP. In our approach, the continuous opacity density function \((t)\) is fitted through kernel regression via discrete Gaussian primitives after a differentiable transformation: \(_{i}(x,y,z)_{i}(t-t_{i})\) from 3D to 1D. The transformation consists of three steps: (1). The Gaussian primitives that the ray passes through are selected as kernel functions. (2). According to the Ray-Gaussian Intersection method , the ray is transformed into the local coordinate system of each 3DGS to obtain the 1D probability density alone the ray. Here, the peak of the 1D probability density, denoted as \(t_{i}\), is defined as the Ray-Gaussian Intersection , indicating that the 3DGS has the greatest influence at this point alone the ray. (3). To integrate with regularization methods , we assume that each 3DGS fits the surface of the object. Therefore, after \(t_{i}\), the probability density continues to remain at its maximum value, indicating that the object is solid. Without loss of generality, the opacity density \((t)\) on camera ray can be expressed as:

\[(t)=_{i}^{N}_{i}_{i}(t-t_{i}), _{i}(t)=(-k_{i} t^{2})&t<0\\ 1&t 0,\] (6)

where \(N\) represents the number of activated kernel functions along the ray, and \(k_{i}\) represents the summarized transform of Ray-Gaussian transform (See Appendix A.1 for details). \(_{i}\) represents the coefficient for each kernel function.1

**Rendering**. As for traditional 3DGS rasterization, the pixel color is rendered through alpha blending on \(N\) 3DGS being passed through by the ray:

\[C=_{i=1}^{N}c_{i}_{i}_{i}^{2D}_{j=1}^{i-1 }(1-_{j}_{j}^{2D})\] (7)

In this scenario, \(_{i}\) is constant value representing the opacity of Gaussians. This point-based rendering is coherent with Eq. 5, with extremely sparse sampling points to simulate dense volume rendering. However, it is impossible to recover continuous opacity density alone the ray from such a rendering equation, as illustrated in row-3 of Fig. 2. This is because the third row of the covariance matrix of 3DGS is discarded, and it is directly projected onto a 2D plane to evaluate the impact on the opacity of points along the ray. From the perspective of Eq. 6, this means that along the ray, the influence range of all \(N\) kernel functions that intersect with the ray collapses to an infinitesimally value, making it impossible to recover a continuous opacity density field. To solve this, Eq. 7 can be modified to: 2

\[C=_{i=1}^{N}c_{i}_{i}_{i}(0)_{j=1}^{i-1}( 1-_{j}_{j}(0))\] (8)This equation will not affect the goal of 3DGS rendering: to approximate traditional volume rendering using sparse sampling points. And it allows for broadening the collapsed kernel functions to fit the continuous opacity function of the scene.

**Scene Representation**. Based on the discussion at the beginning, the scene surface can be described via CDF \((t)\) on the ray in a continuous way, which can be calculated like Eq. 8 (removing color):

\[(t)=_{i=1}^{N}_{i}_{i}(t-t_{i})_{j=1}^{i-1}( 1-_{j}_{j}(t-t_{j}))\] (9)

### Implicit Surface Mapping

This implicit opacity field (denoted as neural opacity field since it is represented by neural Gaussians) measures the CDF of the probability that a ray hits solid scene surface. In the next section, we introduce the mapping of \((t)\) to implicit surface.

We represent implicit surface with signed distance function (SDF), denoted as function \(D(t)\) on the camera ray. To recover \(D(t)\) of given \((t)\) that is calculated from well trained 3DGS, we firstly study the reverse mapping problem: \(:D(t)(t)\)

**Opacity Density Near the Surface**. To ensure that the 3DGS aligns with the object's surface and thus reflects the object's shape, depth distortion regularization [14; 47] is introduced during the Gaussian training process. This encourages the distribution of 3DGS along the ray to aggregate together, causing the peak of the kernel functions to coincide with the object's surface. In the next discussion, the coordinate of object surface on the ray is assumed as \(t^{*}\) with \(D(t^{*})=0\). Considering \((t)\) at the interval \(t[0,t^{*}]\), we have:

\[(t)=(t^{*}-D(t))=_{j=1}^{M}_{j}(-k_{j} D(t)^{2 }), 0 t t^{*}\] (10)

Where \(M\) represents the number of Gaussian kernels concentrated on the surface. To facilitate calculations, we convert the opacity density system to the SDF coordinate system, with \(t^{*}\) as the origin and letting \(u=-D(t)\), as illustrated in Fig. 3, we have:

\[^{}(u) =(u)(u)=(-_{-t^{*}}^{u}(w)dw) (u)\] (11) \[^{}(u) =-^{2}(u)(-_{-t^{*}}^{u}(w)dw)+^{ }(u)(-_{-t^{*}}^{u}(w)dw)\] \[=[-^{2}(u)+^{}(u)](-_{-t^{*}}^{u}(w)dw)\] (12)

where \((u)(0,^{2}),^{2}=_{i=1}^{M}^{2}}\), which can be directly derived from the additive property of the normal distribution. Then letting \(h(u)=-^{2}(u)+^{}(u)\), we have:

\[h(u) =-^{2}(u)+^{}(u)\] \[=-(u)[(u)+}]\] (13)

It is easy to prove that \(h(u)\) crosses a unique zero point \(u_{0}\) from top to bottom on the u-axis, and \(u_{0}<0\). This means that the peak of \(^{}(u)\) will appear before the surface, so it is not reasonable to simply determine the actual intersection point of the light ray with the surface by directly evaluating the peak of \(^{}(u)\). To locate the accurate surface, a transcendental equation of \(u\) is needed to be solved to get \(u_{0}\):

\[(u)=-},(u)=}(- {u^{2}}{2^{2}}),^{2}=_{i=1}^{M}^{2}}\] (14)

It is impossible to directly get the analytical solution, however, numerical computation methods can be applied to solve \(u_{0}\). This may require some extra time for computation.

**Mapping from Opacity to Surface**.

Based on the analysis above, we can always have exact number of \(u_{0}\) via numerical computation method. However, it is hard to find out the inverse function of \((u)\) for directly building the mapping of \((t)\) to \(D(t)\). For the balance of surface smoothing while reducing the indelible error, we represent mapping relationship of \(u(u)\) via Logistic Function as follows:

\[(u)=))}\] (15)

where \(\) represents the smooth factor. We choose Logistic Function because of its formal is concise and shares similar shape of \((u)\). More importantly, it only has one inflection point at \((0,0.5)\), which can be used to simulate the inflection point of \((u)\) after translation. Finally, we represent implicit SDF function via Inverse function transformation of Logistic Function, as shown in Fig. 4:

\[D(t)=(-1)/-u_{0}\] (16)

Figure 4: Illustration of opacity to SDF mapping.(Eq. 16)

Figure 5: Qualitative comparison of novel view synthesis and surface reconstruction on the Waymo Open Dataset , with each subplot annotated with PSNR values to quantify image quality. Our method shows higher geometric precision and detail, validating its efficiency and superiority in processing open scenes, especially in geometric accuracy and detail reproduction.

## 4 Experiments

### Experimental Settings

**Datasets.** To assess our method's performance against baseline methods in open scenes, we used three datasets. We first experimented with the Waymo Open Scene dataset , using three cameras per scene from five available, each scene containing about 600 images. We employed LiDAR point clouds to evaluate reconstruction quality, although LiDAR data was not used as training input. We also tested on the Tank and Temple dataset , which includes trajectories and ground truth for six selected scenes. Lastly, we evaluated the Mip-NeRF 360 dataset ; due to the absence of ground truth, our focus was on novel view synthesis to demonstrate our method's efficacy in this aspect.

**Baselines.** In terms of surface reconstruction, we presented the results on the Waymo dataset  in tables 2 and figures 5, comparing state-of-the-art implicit methods (such as NeuS , F\({}^{2}\)-NeRF , StreetSurf ) and explicit methods (such as 3DGS , SuGaR , 2DGS ). We utilized PSNR to evaluate the results of novel view synthesis and Chamfer distance to measure reconstruction accuracy, while also recording training time, VRAM usage, and the size of the Gaussian point files post-training. Additionally, as shown in Table 3 and Figure 6, we conducted comparisons on the Tank

  
**Method** & PSNR \(\) & C-D \(\) & MB (Storage) \(\) & GB (GPU) \(\) & FPS \(\) & Training Time \(\) \\  NeuS & 13.24 & **0.76** & 170 & 31 & \(\) 0.1 & 5 h \\ F\({}^{2}\)-NeRF & 24.70 & 886.77 & 130 & 24 & \(\) 0.1 & 0.8 h \\ StreetSurf & 27.12 & 1.02 & 540 & 22 & \(\) 0.1 & 1.5 h \\
3DGS & 27.99 & 3.57 & 230 & 23 & **63** & 0.75 h \\ SuGaR & 23.71 & 3.08 & 228 & 33 & 56 & 1.5 h \\
2DGS & 28.51 & 1.67 & 238 & 23 & 51 & **0.7 h** \\ GVKF (Ours) & **30.24** & 1.57 & **30** & **14** & 32 & 1.5 h \\   

Table 2: Quantitative evaluation of novel view synthesis and surface reconstruction on the Waymo Open Scene dataset . Using LiDAR data as ground truth, we calculated Chamfer Distance (C-D) values for reconstruction accuracy. Our method performs excellently in both novel view synthesis and surface reconstruction, outperforming other methods in Gaussian point usage, VRAM occupancy, and real-time rendering.

Figure 6: Qualitative comparison on the Tanks and Temples dataset  shows that our method excels in reconstructing complex backgrounds with high geometric granularity. In contrast, 2DGS often results in fragmented backgrounds, while SuGaR displays uneven spherical shapes, affecting both visual and geometric quality.

[MISSING_PAGE_FAIL:9]

presented in Table 5. When the voxel size is too large, the sparse neural Gaussians fail to learn the scene representation and return NaN errors. As the number of voxels increases, more Gaussians are generated for scene representation, thereby enhancing the quality of novel view synthesis. However, the improvements plateau when the voxel size is reduced to 0.001, which also requires more training time and becomes impractical. Therefore, we set the voxel size to 0.01 to balance training time and rendering quality.

We further conducted ablation study on the Tanks and Temples dataset  to evaluate the impact of voxel representation and SDF mapping. The results are presented in Tab. 6. It can be observed that utilizing voxel representation significantly improves the PSNR for NVS tasks and reduces memory consumption dramatically compared to naive 3DGS setup. Although there is a slight decrease in the geometric quality of surface reconstruction, we consider this trade-off acceptable.

### Limitation

Implicit methods, such as those based on NeRF [27; 37; 20], typically utilize a global fitting approach for SDF, which allows them to fully leverage the universal approximation capabilities of MLPs. This is advantageous even in areas with sparse viewpoints. However, our current method employs a local line-of-sight-based SDF fitting, a compromise made to adapt to the 3DGS rendering style. This means that regions not covered by the training viewpoints lack fitting capability, resulting in uneven surfaces.

In addition, While our method advances 3D surface reconstruction in open scenes, it faces challenges with dynamic objects and the decoupling of distant and near views, sometimes misrepresenting the sky as a surface enveloping the model. The lack of sufficient prior knowledge for optimizing complex scenes also poses limitations.

## 5 Conclusion

This paper introduces GVKF, combining Gaussian splatting's rapid rasterization with the efficiency of implicit expressions to enhance reconstruction quality and speed significantly. By employing a voxelized implicit representation of 3DGS, GVKF retains the expressive power of explicit Gaussian maps while managing them effectively. We have explored the relationship between Gaussian splatting's alpha blending and traditional volume rendering, developing a GS-based method to represent continuous scene opacity density fields through kernel regression, addressing 3DGS's limitations in continuous scene representation.

Experimental results demonstrate GVKF's effectiveness in open scenes, showing notable improvements in reconstruction accuracy, real-time rendering speeds, and reductions in storage and memory usage. These advancements support applications in fields like autonomous driving and virtual reality, pushing forward surface reconstruction technology.