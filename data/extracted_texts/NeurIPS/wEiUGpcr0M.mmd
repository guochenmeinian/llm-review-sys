# Improving Self-supervised Molecular Representation Learning using Persistent Homology

Yuankai Luo

Beihang University

luoyk@buaa.edu.cn &Lei Shi

Beihang University

leishi@buaa.edu.cn &Veronika Thost

MIT-IBM Watson AI Lab,

IBM Research

veronika.thost@ibm.com

###### Abstract

Self-supervised learning (SSL) has great potential for molecular representation learning given the complexity of molecular graphs, the large amounts of unlabelled data available, the considerable cost of obtaining labels experimentally, and the hence often only small training datasets. The importance of the topic is reflected in the variety of paradigms and architectures that have been investigated recently, most focus on designing views for contrastive learning. In this paper, we study SSL based on persistent homology (PH), a mathematical tool for modeling topological features of data that persist across multiple scales. It has several unique features which particularly suit SSL, naturally offering: different views of the data, stability in terms of distance preservation, and the opportunity to flexibly incorporate domain knowledge. We (1) investigate an autoencoder, which shows the general representational power of PH, and (2) propose a contrastive loss that complements existing approaches. We rigorously evaluate our approach for molecular property prediction and demonstrate its particular features in improving the embedding space: after SSL, the representations are better and offer considerably more predictive power than the baselines over different probing tasks; our loss increases baseline performance, sometimes largely; and we often obtain substantial improvements over very small datasets, a common scenario in practice.

## 1 Introduction

Self-supervised learning (SSL) has great potential for molecular representation learning given the complexity of molecules, the large amounts of unlabelled data available, the considerable cost of obtaining labels experimentally, and the resulting often small datasets. The importance of the topic is reflected in the variety of paradigms and architectures that are investigated .

Most existing approaches use _contrastive learning_ (CL) as proposed in . CL aims at learning an embedding space by comparing training samples and encouraging representations from positive pairs of samples to be close in the embedding space while representations from negative pairs are pushed away from each other. However, usually, each given molecule is considered as its own class, that is, a positive sample pair consists of two different views of the same molecule; and all other samples in a batch are used as the negative pairs during training. We observe that this represents a very coarse-grained comparison basically ignoring all commonalities and differences between the given molecules. Therefore also current efforts in molecular CL are put in constructing views that capture the possible relations between molecules: **GraphCL** proposes foursimple augmentations (e.g., drop nodes), but fix two per dataset; **JOAO**(You et al., 2021) extends the former by automating the augmentation selection; **GraphLoG**(Xu et al., 2021) optimizes in a local neighborhood of the embeddings, based on similarity, and globally, using prototypes based on semantics; and **SimGRACE**(Xia et al., 2022) creates views using a second encoder, a perturbed version of the molecule's ones; also most recent works follow this paradigm (Wu et al., 2023).

**Quality of Embedding Spaces is Underinvestigated.** Although differences in performance are often observed, they seem marginal and are not yet fully comprehended. With a few exceptions, the models are usually evaluated over the MoleculeNet benchmark (Wu et al., 2018) only, a set of admittedly rather diverse downstream tasks. While those certainly provide insights into model performance, recent evaluation papers have pointed out that the picture may be very different when the models are evaluated in more detail (Sun et al., 2022; Wang et al., 2022; Akhondzadeh et al., 2023; Deng et al., 2022). In particular, (Akhondzadeh et al., 2023) propose to use linear probing to analyse the actual power of the representations instead of a few, specific downstream tasks. This type of evaluation is also common in other areas of DL (Chen et al., 2020). (Deng et al., 2022) consider other, smaller datasets, and compare to established machine learning (ML) approaches.

**We study molecular SSL based on persistent homology (PH).** PH is a mathematical tool for modeling topological features of data that persist across multiple scales. In a nutshell, the molecule graph can be constructed sequentially using a custom filter, such that atom nodes and the corresponding bond edges only appear once they meet a given criterion (e.g., based on atomic mass); see Figure 1. PH then captures this sequence in a persistence diagram (PD), and the area has developed various methods to translate these diagrams into _topological fingerprints_, which can be used for ML (Ali et al., 2022). Topological fingerprints have been used for _supervised_ ML over molecules by chemists (Krishnapriyan et al., 2021; Demir et al., 2022) and recent work in AI has shown promising results compared to the commonly used ECFP embeddings (Rogers and Hahn, 2010). Moreover, many of these fingerprints have been shown to be _stable_, in the sense that distances between the PDs are reflected in the distances between the corresponding fingerprints. _We point out that the unique features of topological fingerprints particularly suit molecular SSL_, naturally offering: different views of the data (i.e., by switching the filter function), stability in terms of distances, and the opportunity to flexibly incorporate domain knowledge.

**Our Idea.** We consider the topological fingerprints of the molecules in the pre-training data as views and exploit their stability to model the distances between the _given_ molecules (i.e., instead of between a molecule and/or its views). In particular, we use them for fine-grained supervision in SSL.

**Contributions.**

* We propose a **topological distance contrastive loss (TDL)** which, as outlined above, shapes the embedding space by providing supervision for the relationships between molecules, especially the ones in the data. TDL is complementary and _can be flexibly and efficiently applied to complement any CL approach_. We extensively evaluate it combining TDL with four established CL models.
* We also consider _alternative paradigms for comparison_: we study a straightforward **topological fingerprints autoencoder (TAE)**.
* Our evaluation particularly focuses on the _potential in improving the embedding space_ and we demonstrate the gain in representation power in detail empirically (see Section 4.1): TDL en

Figure 1: Topological fingerprints are constructed using a filter (e.g., atom number) and recording resulting topological structures in a PD which is then vectorized.

Figure 2: TDL may yield great improvements in the low-data scenario; ClinTox.

ables the models to learn, in a sense, calibrated distances; considerably improves linear probing performance; and the fine-grained supervision may mitigate deficiencies of individual models.
* Over downstream data (see Section 4.2) the performance increases depend more on the model and data. Notably, TDL is able to considerably improve GraphCL and make it competitive with SOTA. Moreover, TDL _strongly improves various models in the low-data scenario_ (see Figure 2).

Our implementation is available at https://github.com/LUOyk1999/Molecular-homology.

## 2 Background and Related Works

**Graph Homology.** Molecules are graphs \(G=(V,E)\) with nodes \(V\), the atoms, and bond edges \(E\). In algebraic topology, graph homology considers such a graph \(G\) as a topological space. We focus on _simplices_: every node is a 0-simplex, and every edge is a 1-simplex. In the context of graphs, we obtain a 1-dimensional _simplicial complex_\(X=V E\) in an easy way, by considering the simplices induced by \(G\); the dimension is determined by the maximal dimension of the contained simplices.

**Persistent Homology (PH).** We introduce the most important concepts and, for more details, refer to (Dey and Wang, 2022; Edelsbrunner and Harer, 2022). Persistent homology is a mathematical tool for modeling topological features of data that persist across multiple scales, comparable to different resolutions. These features are captured in persistent diagrams which, in turn, can be vectorized in fingerprints. We outline the process simplified below and in Figure 1; see Appendix A for details.

First, the goal is to construct a nested sequence of subgraphs \(G_{1}... G_{N}=G\) (\(1 i N\)). As described above, these graphs can be considered as simplicial complexes, hence we have simplices which we can record in a persistence diagram. To this end, we consider one of the most common types of filtration methods: sublevel/superlevel filtrations. A _sublevel filtration_ is a function \(f:X\) over all simplices. A simple such function \(f\) can be a node-valued function (e.g., map atom nodes to their atomic number) that is expanded to the edges as \(f(u,v)=max(f(u),f(v))\). Denote by \(X_{a}\) the sublevel set of \(X\), consisting of simplices whose filtration function values \( f(a)\), \(X_{a}=\{x X|f(x) f(a)\}\). As the threshold value \(a\) increases from \(_{v V}f(v)\) to \(_{v V}f(v)\), let \(G_{a}\) be the subgraph of \(G\) induced by \(X_{a}\); i.e., \(G_{a}=(V_{a},E_{a})\) where \(V_{a}=\{v X_{a}\}\) and \(E_{a}=\{e_{rs} X_{a}\}\). This process yields a nested sequence of subgraphs \(G_{1} G_{2}... G_{N}=G\). As \(X_{a}\) grows to \(X\), new topological structures gradually appear (born) and disappear (die).

Second, a _persistence diagram_ (PD) is obtained as follows. For each topological structure \(\), PH records its first appearance and its first disappearance in the filtration sequence. And this is represented by a unique pair \((b_{},d_{})\), where \(1 b_{} d_{} N\). We call \(b_{}\) the birth time of \(\), \(d_{}\) the death time of \(\) and \(d_{}-b_{}\) the persistence of \(\). PH records all these birth and death times of the topological structures in persistence diagram \(PD(G)=\{(b_{},d_{})| H_{k}(G_{i})\}\), where \(H_{k}(G_{i})\) denotes the \(k\)-th homology group of \(G_{i}\), and in practice, \(k\) typically takes values 0 or 1. This step is rather standard and corresponding software is available (Otter et al., 2017).

**PH in ML.** The vectorization of PDs for making them applicable in ML has been studied extensively (Ali et al., 2022), and proposals range from simple statistical descriptions to more complex _persistence images_ (PIs) (Adams et al., 2017). In a nutshell, for PIs, the PD is considered as a 2D surface, tranformed using a Gaussian basis function, and finally discretized into a vector. Furthermore, the Euclidean distance between PIs is _stable_ with respect to the 1-Wasserstein distance between PDs (Adams et al., 2017), which essentially means that the former is bounded by a constant multiple of the latter. We focus on PIs based on the promising results reported in supervised settings (Demir et al., 2022; Krishnapriyan et al., 2021). Our study was inspired by the ToDD framework, applying pre-trained vision transformers to custom 2D topological fingerprints (Demir et al., 2022). Interestingly, they use the distances between the transformer's molecule representations to construct a suitable dataset for subsequent _supervised_ training using triplet loss. Our focus is on SSL and we apply the ToDD fingerprints as more complex, expert fingerprints in comparison to the ones based on atomic mass only. Recently, various other approaches of integrating PH into ML are explored, but these are only coarsely related to our work (e.g., (Horn et al., 2022; Yan et al., 2022)).

**Molecular SSL.** Since the foundational work of (Hu* et al., 2020), who have proposed several effective methods, such as the node context prediction task ContextPred, the area is advancing at great pace (Xia et al., 2023; Xie et al., 2022). Our proposal falls into the category of contrastive learning as introduced in Section 1. Our study focuses on the potential of _PH to complement existingmodels_ such as (You et al., 2020) and follow-up approaches. There are other related CL approaches to which we do not aim to compare to directly; e.g., works including 3D geometry (Liu et al., 2022a; Stark et al., 2022) or considerably scaling up the pre-training data (Ross et al., 2022; Zhou et al., 2023). In contrast, our focus is on improvement through exploiting unused facets of the data.

**SSL based on Distances.** We found only few works that explicitly incorporate distances into SSL. With regard to graphs, (Kim et al., 2022) exploit the graph edit distance between a graph and views to similarly represent the actual distance in the embedding space inside the loss function (i.e., vs. invariance to the view transformation). This distance computation is feasible since it can be easily obtained based on the transformation. Our work focuses on exploring in how far distances in terms of PH can be applied towards the same goal; moreover, this makes it feasible to model the distances between given samples. (Wang et al., 2022b) consider a loss very similar to our proposal based on the more coarse-grained ECFPs, instead of PIs, but they focus on chemistry aspects instead of SSL more generally. Also (Zha et al., 2022) model distances between given samples in the context of CL in a similar way, but in the context of a supervised scenario, where the distances are the differences between given regression labels; note that they also list other coarser related works. (Taghanaki et al., 2021) apply custom distances inside a triplet loss but similarly exploit label information, in order to select the positive and negative samples. Beyond that, distances have been applied to obtain hard negatives (Zhang and Re, Demir et al., 2022), and are exploited in various other ways rather different from our method. For instance, several recent approaches aim to structure the embedding space by exploiting correlations between samples which are, in turn, obtained using nearest neighbors (Caron et al., 2020; Dwibedi et al., 2021; Ge et al., 2023). Also methods considering equivariance between view transformations implicitly model distances (Chuang et al., 2022; Devillers and Lefort, 2023).

**Others.** While our focus on CL and distances hints at a close relationship to deep metric learning (KAYA and BILGE, 2019), these works usually do not have explicit distances for supervision but, for instance, exploit labels. Observe that this indicates the unique nature of the distances PH provides us with. Lastly, SSL more generally (Rethmeier and Augenstein, 2023) has naturally been inspiration for molecular SSL and is certainly one reason why the field was able to advance so fast. We use its insights by putting focus on linear probing and investigating dimensional collapse (Hua et al., 2021).

## 3 Methodology

The main goal of SSL is to learn an embedding space that faithfully reflects the complexity of molecular graphs, captures the topological nature of the molecular representation space overall, and whose representations can be effectively adapted given labels during fine-tuning. We propose methods based on persistent homology and show that they naturally suit SSL. First, persistence images (or comparable topological fingerprints) offer great versatility in that they are able to flexibly represent knowledge about the graphs and allow for incorporating domain knowledge. Second, they capture this knowledge based on persistence diagrams, which is very different from - and hence likely complementary to - the common graph representation methods in deep learning. Third, and most importantly, their stability represents a unique feature, which makes them ideal views for SSL.

We study two SSL approaches based on PH and evaluate them in detail in terms of both their impact on representation power (see Section 4.1) and on downstream performance (see Section 4.2):

* In order to study the impact of PH on SSL in general, we consider a simple _autoencoder_ architecture.
* Since we consider topological fingerprints to represent information that is complementary to that used in existing approaches (we also show that they are not ideal alone) and because their topological nature can be used to improve the latter in unique ways, we developed a loss function based on _contrastive learning_ that complements existing approaches.

In this paper, our focus is on obtaining initial insights about the potential PH offers for molecular SSL, hence we chose one basic solution and one providing unique impact. There are certainly other promising ways to be investigated in the future.

### Topological Fingerprints AutoEncoder (TAE)

Autoencoders are designed to reconstruct certain inputs given context information for the input graph \(G\). We consider topological fingerprints \(I_{G}\) as the reconstruction targets, specifically, PIs.

The model itself employs a typical graph encoder \(()\) for computing embeddings \(h_{v}\) for the individual nodes \(v V\); for simplicity, we write \(h_{V}=(G)\), where \(h_{V}=\{h_{v}|v V\}\) represents all node representations. Next, we pass it through a projection head \(g()\) and readout function \(R()\) (e.g., mean pooling) to obtain the graph-level representation \(h_{G}=R(g(h_{V}))\). Hence the context is the particular knowledge the graph encoder was designed to exploit; in our evaluation, we consider the commonly used GIN [Xu et al.] in order to ease comparison with related works. As our loss formula for this _topological fingerprints autoencoder_ (TAE) we use the regular mean squared error: \(_{}=_{G}(h_{G},I_{G}).\)

**Observations.** Given stable fingerprints such as PIs, one main usual criticism for autoencoders, the fact that they fail to capture inter-molecule relationships, does not apply if the model is able to reliably learn the PIs, which we show TAE does; in particular, we observe a strong correlation between PIs and their reconstructions (see Table 7). The successful application of topological fingerprints in supervised learning and the simplicity of the model justify the study of TAE for analysis, however, we note that the PIs employed may not necessarily capture all information which is critical for a particular downstream task. For example, Figure 3 shows that PIs may offer a generally fitting embedding space, in that randomly chosen molecule pairs with similar standard fingerprints based on substructures (blue, Tanimoto similarity of ECFPs) have rather similar PIs (x-axis, cosine similarity). However, there is a clear difference for the two depicted datasets, sometimes PIs as shown here, based on the ToDD filtration Demir et al. (2022), fail to fully capture structural similarity; and this is directly reflected in performance (see Table 4). While optimization on a case-by-case basis w.r.t. the choice of fingerprints is possible, this is not in the spirit of foundational SSL, requires expert knowledge or extensive tuning, and may still not be sufficient. For that reason, we suggest to apply them together with existing approaches and show that they offer unique benefits.

### Topological Distance Contrastive Loss (TDL)

Contrastive learning aims at learning an embedding space by comparing training samples and encouraging representations from positive pairs of examples to be close in the embedding space while representations from negative pairs are pushed away from each other. Current approaches usually consider each sample as its own class, that is, a positive pair consists of two different views of it; and all other samples in a batch are used as the negative pairs during training. We observe that this represents a very coarse-grained comparison basically ignoring all commonalities and differences between the given molecules; this is also why current efforts in graph CL focus on constructing views that capture the possible relations between molecules.

Figure 4: Overview of Topological Distance Contrastive Learning.

Figure 3: Comparison of molecule similarity based on PIs to similarity between corresp. ECFPs (blue: 20% most similar).

**Our Idea, Figure 4.** We exploit the stability of topological fingerprints such as PIs to model the distances between the _given_ molecules (i.e., instead of just views of the same molecule) and to use them for fine-grained supervision in SSL (recall that stability means the distances between PIs reflect those between the topological persistence diagrams). This is very different from and complementary to related works in that it _structures the embedding space in a different way_. The fingerprints are usually constructed in a way so that they capture information about the molecular graph structure; even if they do not capture the entire complexity of the molecules, they represent some, probably important aspects. Moreover, based on the topological nature, they _may capture aspects not represented by the commonly used graph embeddings_. In particular, they offer a _way to flexibly integrate expert knowledge_ into molecular SSL; observe that the usually employed GNNs have dimensions based on the available molecule information, hence including additional information, even if available only for some of the data, requires architecture changes.

We consider a batch of \(N\) graphs, \(\{G_{i}\}_{i[1,N]}\). Similar as above, we first extract graph-level representation vectors \(h_{G_{i}}\) using a graph encoder \(()\), followed by a readout function \(R()\). Here, we apply the projection head \(g()\) later, to map the graph representations to another latent space and obtain the final graph embedding \(z_{i}\). Specifically, we apply a two-layer MLP and hence a non-linear transformation, known to enhance performance [Chen et al., 2020]: \(z_{i}=g(R((G_{i})))\)

Let \(G_{n}\) be the sample under consideration. Instead of constructing an artificial view, we consider all possible positive pairs of samples \((G_{n},G_{m})\) together with a set of stable, topological fingerprints \(\{I_{i}\}_{i[1,N]}\) for all graphs. Note that these can be computed rather efficiently, and they have to be computed only once for the given pre-training data. In a nutshell, our loss adapts the regular NT-Xent [Sohn, 2016, Oord et al., 2018, Wu et al., 2018b] by considering only those negative pairs \((G_{n},G_{k})\) of samples where the Euclidean distance \(dis(I_{n},I_{k})\) between the corresponding fingerprints is greater than the one between \(I_{n}\) and \(I_{m}\). We compute the similarity score as \(sim(z_{n},z_{m})={z_{n}}^{}z_{m}/\|z_{n}\|\|z_{ m}\|\), and consider a temperature parameter \(\) and indicator function \(_{[]}\{0,1\}\) as usual. Our _topological distance contrastive loss_ (TDL) is defined as follows, for the n-th sample:

\[_{_{n}}=_{m[1,N ],\\ m n}-,z_{m})/}}{_{ k[1,N],\\ k n}_{[dis(I_{n},I_{k}) dis(I_{ n},I_{m})]} e^{sim(z_{n},z_{k})/}}\]

TDL _can be flexibly and efficiently applied to complement any graph CL framework_, e.g., in the form \(_{n}=_{others}+_{_{n}}\), where \(\) determines its impact. In our evaluation, we used \(=1\).

**Further Intuition.** Essentially, TDL provides a form of regularization. It encourages the model to push molecule representations less far away from the sample under consideration if they are similar to it in terms of the topological fingerprints. Given the stability of those, we hence obtain an embedding space with better calibrated distances (i.e., distances in terms of PH, between persistence diagrams of the molecule graphs). This can be partly observed theoretically, in the directions of the gradients; see Appendix I for an initial analysis. We show this empirically by calculating the correlation between distances between the molecule representations after pre-training and the PIs (see Table 1), and by visualizing the distances in Figure 5. Our evaluation further shows that the fine-grained supervision may solve deficiencies of CL models in that it forces them to capture crucial features of the input. Further, the improved embedding space particularly suits low-data downstream scenarios.

**On Views.** While we consider the modeling of the sample relationships to be most unique and offer great potential to complement other models, we note that TDL can be similarly applied over views.

## 4 Evaluation

* Do TAE and TDL lead to, in a sense, **calibrated distances in the representation space**?
* Do we obtain **improved representations** more generally, based on established SSL metrics?
* What impact do we see on **downstream performance**, and does it justify our proposal?

**Our Models.** We apply both TAE and TDL with different filtration functions, marked by a subscript. First, we apply a most simple filtration atom based on atomic mass. This allows showing that, even by considering less information than the baseline GNNs, which additionally apply atom chirality and connectivity, topological modeling may exploit additional facets of the data. Since TAE is a standalone model, atomic mass is not enough to let it fully capture the molecular nature. For thatreason, we consider three filtrations (atomic mass, heat kernel signature, node degree) and concatenate the corresponding PIs, denoted by ahd. Finally, to show the real potential of PH, we include the ToDD filtration Demir et al. (2022), which combines atomic mass with additional domain knowledge, partial charge and bond type, inside a more complex multi-dimensional filtration. See Appendix A.

**Baselines & Datasets.** For a comprehensive evaluation, we apply TDL on a variety of existing CL approaches: GraphCL, JOAO, GraphLoG, and SimGRACE (see Section 1). Note that this goes far beyond other CL extensions which are often evaluated with a single approach only (You et al., 2021; Xia et al., 2023b). We also study TAE on top of the established ContextPred (Hu* et al., 2020), to get an idea of its complementary nature. Model configurations and experimental settings are described in Appendix A. For pre-training, we considered the most common dataset following (Hu* et al., 2020), 2 million unlabeled molecules sampled from the ZINC15 database (Sterling and Irwin, 2015). For downstream evaluation, we focus on the MoleculeNet benchmark (Wu et al., 2018a) here, the appendix contains experiments on several other datasets.

### Analysis of Representations after Pre-training

**Calibrated Distances in Embedding Space, Tables 1, 7, 9, Figure 5.**

TAE successfully learns the PI space for the molecules in MoleculeNet in the sense that there is a strong correlation between the PIs and their reconstructions (Table 7, appendix). For TDL, we observe for all baselines considerable increases in terms of correlation between distances in PI space and in representation space; we use the embeddings from linear probing. There are two notable exceptions and generally smaller increases on one dataset, BBBP. We also compared the ROGI score (Aldeghi et al., 2022) which, in a nutshell, measures in how far (dis)similarity of molecules (i.e., we use Euclidean distance of embeddings) is reflected in label similarity (Table 9, appendix). This gives us some idea in terms of downstream labels and the mixed results reflect the variety of the data. Finally, the alignment figure on the right clearly visualizes that the PI embedding space is much more fine-grained in terms of distances than the GNN space, and that the distribution of GraphCL seems to get correctly adapted in that distances between "positive" pairs generally shrink below the ones of "negative" pairs.

**Mitigating Dimensional Collapse, Figure 6.** One of the most interesting of our findings is the fact that GraphCL and GraphLoG suffer from dimensional collapse (i.e., the embeddings span a lower-dimensional subspace instead of the entire available embedding space, (Wang et al., 2022) also observe this for GraphCL) and that TDL successfully mitigates this. This can be observed in

    & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & BBBP & Bace \\  ContextPred & 31.2 (0.4) & 2.5 (0.0) & 61.6 (0.2) & 15.6 (0.5) & 37.2 (0.1) & 20.6 (0.1) & 3.7 (0.3) & 12.7 (0.2) \\ + TAE\({}_{}\) & 35.6 (0.3) & 12.2 (0.6) & 60.6 (0.3) & 6.2 (1.6) & 55.9 (0.1) & 30.9 (0.1) & 2.7 (0.7) & 25.9 (0.2) \\  GraphCL & 15.6 (0.4) & 7.4 (0.8) & 52.6 (0.2) & 16.7 (0.8) & 35.4 (0.2) & 18.3 (0.2) & 6.2 (1.8) & 10.5 (0.2) \\ + TDL\({}_{}\) & 55.4 (0.2) & 14.5 (0.4) & 66.6 (0.3) & 21.9 (0.7) & 65.2 (0.3) & 41.5 (0.1) & 7.0 (1.4) & 34.1 (0.3) \\  JOAO & 25.1 (0.8) & 3.2 (1.7) & 62.4 (0.3) & 20.2 (1.3) & 48.0 (0.2) & 29.5 (0.3) & 2.7 (1.4) & 24.8 (0.3) \\ + TDL\({}_{}\) & 49.3 (0.3) & 14.6 (2.1) & 65.2 (0.1) & 26.2 (1.0) & 60.2 (0.2) & 40.6 (0.4) & 6.7 (0.8) & 36.0 (0.4) \\  SimGRACE & 10.3 (2.9) & 9.8 (0.6) & 59.9 (1.1) & 4.8 (1.6) & 26.5 (0.3) & 21.6 (0.2) & 9.5 (2.1) & 4.4 (0.1) \\ + TDL\({}_{}\) & 48.8 (0.9) & 12.8 (1.7) & 61.5 (0.2) & 20.2 (0.5) & 71.9 (0.1) & 49.1 (0.2) & 8.5 (2.0) & 38.9 (0.1) \\  GraphLoG & 16.8 (0.2) & 2.9 (0.5) & 34.2 (0.2) & 3.6 (0.8) & 20.4 (0.1) & 9.4 (0.2) & 3.3 (0.9) & 12.6 (0.2) \\ + TDL\({}_{}\) & 44.2 (0.4) & 11.8 (1.3) & 50.5 (0.4) & 22.4 (1.2) & 63.9 (0.1) & 46.2 (0.2) & 1.9 (0.4) & 40.3 (0.1) \\   

Table 1: Pearson correlation coefficients (%) between distances in embedding space and distances between corresponding PIs, for samples from various MoleculeNet datasets. Highlighted are clear decreases and clear increases (i.e., considering standard deviation).

Figure 5: Normalized Euclidean distances for pairs of embeddings after pre-training of same/different Bace class, dark/light: PI\({}_{}\), green; GraphCL, blue; GraphCL+TDL\({}_{}\), red.

the singular values of the covariance matrix of the representations, where vanishing values hint at collapsed dimensions. The phenomenon has recently been observed in computer vision and there are only initial explanations to date . However, here, we observe the collapse only over the downstream data. Our hypothesis is that TDL's fine-grained supervision forces the models to capture relevant features, which they might have neglected otherwise. We also depict the graphs for SimGRACE and JOAO, which look very different, reflecting the variety of the approaches. There is basically no difference for JOAO(+TDL), while TDL shrinks the values for SimGRACE; the latter is less optimal and needs further investigation.

    & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & BBBP & Race & Average \\  \(_{}\) & 56.9 (0.3) & 51.5 (0.4) & 56.1 (0.5) & 45.5 (1.1) & 58.6 (0.7) & 68.4 (0.6) & 47.1 (0.4) & 48.6 (0.7) & 54.08 \\ \(_{}\) & 65.8 (0.3) & 50.3 (0.3) & 58.1 (0.5) & 56.9 (1.3) & 55.7 (0.9) & 70.8 (0.4) & 57.1 (0.7) & 67.8 (0.8) & 60.31 \\ ECFP & 68.8 (0.3) & 57.0 (0.2) & 62.3 (0.3) & 69.2 (0.8) & 66.1 (0.4) & 69.4 (0.2) & 63.2 (0.3) & 73.6 (0.8) & 66.20 \\ ECFP & \(_{}\) & 69.6 (0.4) & 56.3 (0.3) & 60.9 (0.6) & 76.7 (1.1) & 64.0 (0.6) & 71.6 (0.5) & 63.0 (0.4) & 76.8 (1.0) & **67.36** \\  \(_{}\), MLP & 57.2 (0.5) & 52.5 (0.4) & 56.6 (0.7) & 49.8 (1.4) & 60.5 (1.6) & 69.9 (0.4) & 48.8 (1.0) & 53.3 (1.1) & 56.08 \\ \(_{}\), MLP & 66.7 (0.3) & 52.4 (0.4) & 58.6 (0.6) & 61.8 (1.6) & 60.1 (0.4) & 71.6 (0.7) & 57.9 (0.8) & 68.1 (1.3) & 62.03 \\ ECFP, MLP & 70.1 (0.4) & 59.8 (0.4) & 59.6 (0.6) & 67.8 (0.9) & 61.7 (0.8) & 69.1 (1.0) & 58.6 (1.3) & 72.1 (1.7) & 64.85 \\ ECFP & \(_{}\), MLP & 71.1 (0.6) & 75.8 (0.4) & 92.9 (0.7) & 80.7 (2.1) & 64.9 (1.1) & 72.8 (1.7) & 63.1 (0.8) & 76.7 (0.9) & **68.28** \\  \(_{}\) & 67.7 (0.2) & 61.2 (0.2) & 55.8 (0.3) & 58.1 (0.7) & 72.0 (0.8) & 72.5 (0.5) & 61.1 (0.2) & 74.3 (0.2) & 65.11 \\ \(_{}\) & 70.4 (0.2) & 60.8 (0.1) & 61.1 (0.1) & 68.4 (0.7) & 72.3 (0.3) & 73.9 (0.2) & 61.6 (0.4) & 67.6 (0.6) & 67.01 \\  ContextPred & 68.4 (0.3) & 59.1 (0.2) & 59.4 (0.3) & 43.2 (1.7) & 71.0 (0.7) & 68.9 (0.4) & 59.1 (0.2) & 64.4 (0.6) & 61.69 \\ + \(_{}\) & 69.7 (0.1) & 59.2 (0.2) & 59.5 (0.3) & 56.1 (1.1) & 76.5 (0.9) & 68.9 (0.2) & 61.1 (0.4) & 65.6 (0.5) & **64.58** \\ + \(_{}\) & 69.0 (0.1) & 59.8 (0.4) & 60.0 (0.4) & 53.3 (1.3) & 70.8 (0.3) & 70.0 (0.7) & 60.9 (0.5) & 52.7 (0.5) & **63.31** \\  GraphCL & 64.4 (0.5) & 59.4 (0.2) & 54.6 (0.3) & 59.8 (1.2) & 70.2 (1.0) & 63.7 (2.3) & 62.4 (0.7) & 71.1 (0.7) & 63.20 \\ + \(_{}\) & 72.0 (0.4) & 61.1 (0.2) & 59.7 (0.6) & 65.3 (1.3) & 76.1 (0.9) & 68.2 (1.1) & 65.4 (0.9) & 76.4 (1.1) & **68.02** \\ + \(_{}\) & 72.7 (0.5) & 60.8 (0.4) & 58.9 (0.6) & 64.1 (1.7) & 72.7 (1.4) & 69.7 (1.2) & 64.5 (0.8) & 76.1 (1.3) & **67.44** \\  JOAO & 70.6 (0.4) & 60.5 (0.3) & 57.4 (0.6) & 54.1 (2.6) & 69.8 (1.9) & 68.1 (0.9) & 63.7 (0.3) & 71.2 (1.0) & 64.42 \\ + \(_{}\) & 70.5 (0.3) & 60.4 (0.2) & 57.8 (1.5) & 54.6 (1.3) & 74.2 (1.6) & 68.2 (0.6) & 65.2 (0.3) & 72.7 (3.1) & **65.41** \\ + \(_{}\) & 71.7 (0.4) & 61.3 (0.3) & 58.9 (0.7) & 52.4 (1.7) & 69.6 (1.7) & 69.9 (0.6) & 64.1 (0.5) & 72.6 (0.9) & **65.06** \\  SimGRACE & 64.6 (0.4) & 59.1 (0.2) & 59.4 (0.6) & 63.4 (2.6) & 67.4 (1.2) & 66.3 (1.5) & 65.4 (1.2) & 67.8 (1.3) & 63.61 \\ + \(_{}\) & 68.6 (0.3) & 61.1 (0.2) & 59.5 (0.4) & 62.2 (1.7) & 69.7 (2.0) & 69.5 (1.8) & 60.6 (0.5) & 72.1 (0.7) & **65.41** \\ + \(_{}\) & 70.1 (0.3) & 60.3 (0.3) & 59.1 (0.3) & 65.1 (1.4) & 71.4 (1.1) & 71.1 (0.7) & 64.9 (0.6) & 73.4 (0.8) & **66.93** \\  GraphLLoG & 67.2 (0.2) & 57.9 (0.2) & 57.9 (0.3) & 57.8 (0.9) & 64.2 (1.1) & 65.0 (1.3) & 54.3 (0.7) & 72.3 (0.9) & 62.08 \\ + \(_{}\) & 72.1 (0.3) & 62.0 (0.2) & 60.7 (0.2) & 56.6 (0.8) & 73.0 (0.9) & 70.4 (0.9) & 61.2 (0.4) & 76.8 (0.7) & **66.59** \\ + \(_{}\) & 70.7 (0.2) & 60.7 (0.3) & 61.5 (0.3) & 59.5 (0.5) & 72.9 (1.8) & 71.6 (0.8) & 62.1 (0.3) & 80.1 (0.4) & **67.39** \\   

Table 2: Linear/MLP probing: molecular property prediction; binary classification, ROC-AUC (%).

 

    & Tox21 & ToxCast & Saler & ClinTox & MUV & HIV & BBBP & Bace & Average \\  \# Molecules & 7.831 & 8.575 & 1.427 & 1.478 & 93.087 & 41,127 & 2.039 & 1,513 & - \\ \# Tasks & 12 & 617 & 27 & 2 & 17 & 1 & 1 & 1 & 1 \\ \# Positives & 2.44/61.02 & 0.21/32.05 & 1.506/39.24 & 7.69/69.63 & 0.03/03.03 & 3.56 & 76.5 & 45.7 & \\  No potential (GIN) & 74.6 (0.4) & 61.7 (0.5) & 58.2 (1.7) & 58.4 (6.4) & 70.7 (1.8) & 75.5 (0.8) & 67.3 (7.3) & 72.4 (3.8) & 67.15 \\  AD-CL [Suresh et al., 2021] & 76.5 (0.8) & 63.0 (0.7) & 63.2 (0.7) & 79.7 (3.5) & 72.3 (6.1) & 78.2 (0.9) & 78.0 (0.1) & 78.5 (0.5) & 72.67 \\ MOCLR [Wang et al., 2022] & 75.1 (0.7) & 63.3 (0.4) & 59.4 (1.0) & 81.0 (2.6) & 74.7 (1.9) & 73.1 (6.6) & 76.2 (1.7) & 73.3 (1.0) & 72.24 \\ MOle-BERT [Xia et al., 2023] & 76.8 (0.5) & 64.3 (0.2) & 62.8 (1.1) & 78.9 (3.0) & 78.6 (1.8) & 78.2 (0.7) & 71.9 (6.8) & 70.1 (0.4) & 74.04 \\ SEGAN [Wu et al., 2023] & 76.7 (0.4) & 63.2 (0.9) & 61.6 (0.3) & 54.4 (0.9) & 76.6 (2.4) & 77.6 (1.3) & 71.0 (0.7) & 70.0 (0.4) & 74.17 \\  TAE\({}_{}\) & 75.2 (0.8) & 63.1 (0.3) & 61.9 (0.8) & 80.6 (1.9) & 74.6 (1.8) & 73.5 (2.1) & 67.5 (1.1) & 82.5 (1.1) & 72.36 \\ TAE\({}_{}\) & 76.8 (0.9) & 64.0 (0.5) & 61.9 (0.8) & 79.3 (3.6) & 75.8 (3.2) & 75.9 (1.0) & 70.4 (0.8) & 81.6 (1.4) & 73.22 \\  ContextPred & 75.7 (0.7) & 63.9 (0.6) & 69.0 (0.6) & 65.9 (3.8) & 75.8 (1.7) & 77.3 (1.0) & 68.0 (0.2) & 79.6 (1.2) & 70.89 \\  \(\)TAE\({}_{}\) & 76.4 (0.5) & 63.2 (0.4) & 62.0 (6.0) & 77.6 (4.4) & 76.7 (1.6) & 77.7 (1.2) & 69.8 (0.1) & 70.1 (0.6) & **72.53** \\ \(\)TAE\({}_{}\) & 75.7 (0.4) & 63.1 (0.3) & 61.3 (0.5) & 72.1 (1.3) & 77.2 (1.8) & 77.6 (1.1) & 69.8 (0.9) & 80.1 (1.4) & **72.09** \\  GraphCL & 73.9 (0.7) & 62.4 (0.6) & 60.5 (0.5) & 76.0 (2.7) & 76.8 (1.2) & 69.7 (0.7) & 78.5 (1.4) & 74.0 (0.78 \\ \(\)TDL\({}_{}\) & 75.3 (0.4) & 64.4 (0.3) & 61.2 (0.8) & 63.7 (2.7) & 75.8 (0.8) & 78.0 (0.9) & 79.0 (0.6) & 80.5 (0.8) & **73.71** \\ \(\)TDL\({}_{}\) & 75.2 (0.7) & 64.2 (0.5) & 61.5 (0.4) & 85.2 (1.8) & 75.9 (2.1) & 77.0 (0.8) & 69.9 (0.9) & 81.2 (1.9) & **73.83** \\  JOAO & 75.0 (0.3) & 62.9 (0.5) & 60.0 (0.8) & 81.3 (2.5) & 71.7 (1.4) & 76.7 (1.2) & 70.9 (2.0) & 79.3 (7.5) & 71.89 \\ \(\)TDL\({}_{}\) & 75.5 (0.3) & 63.8 (0.2) & 60.6 (0.5) & 76.3 (1.5) & 73.8 (1.9) & 73.2 (1.0) & 70.3 (0.5) & 78.7 (0.6) & **72.22** \\ \(\)TDL\({}_{}\) & 75.2 (0.3) & 63.6 (0.2) & 61.6 (0.0) & 70.3 (3.4) & 75.6 (0.6) & 77.4 (0.9) & 71.3 (0.8) & 81.0 (2.2) & **73.18** \\  SimGRACE & 74.4 (0.3) & 62.2 (0.7) & 60.2 (0.9) & 75.5 (2.0) & 74.4 (1.3) & 65.0 (0.6) & 77.1 (1.2) & 74.1 (0.9) & 71.15 \\ \(\)TDL\({}_{}\) & 74.7 (0.5) & 63.0 (0.3) & 59.9 (0.4) & 73.7 (1.5) & 75.9 (1.6) & 73.1 (1.1) & 69.5 (0.9) & 79.1 (0.5) & **71.59** \\ \(\)TDL\({}_{}\) & 75.6 (0.4) & 63.3 (0.5) & 59.9 (0.8) & 82.4 (2.5) & 75.6 (2.0) & 76.1 (1.3) & 69.5 (0.9) & 89.9 (1.6) & **72.71** \\  GraphLG & 75.0 (0.6) & 63.4 (0.6) & 59.3 (0.8) & 70.1 (4.6) & 75.5 (1.6) & 76.1 (0.8) & 69.6 (1.6) & 82.1 (1.0) & 71.43 \\ \( filtration, TDL demonstrates convincing improvements across all baselines and makes them competitive with SOTA.

**The Low-Data Scenario, Figures 2, 7, 13, 14.** Inspired by the impressive increases we see in the quality of representations after pre-training, we experimented with considerably smaller datasets. This is a scenario which is relevant in practice since proprietary data from costly experiments is often only available in small numbers (Tom et al., 2023). Here we see clearly that the improvement in representation space is reflected in downstream performance. There is still some dependence on the dataset but, overall, TDL often yields remarkable improvements. The introduction figure shows that also other models show room for improvement; note that GraphMVP uses 3D information.

**Further Analysis.** Due to space constraints, we present only the most impactful findings in this section and the rest in the appendix, in particular: (1) **Ablation Studies.** Our approach offers great variability in terms of, for instance, the graph encoder, the construction of the PIs, or even the type of topological fingerprint more generally. (2) **Unsupervised learning.** We observe similar but less pronounced trends, likely due to the smaller size of the data and fewer features available. (3) **PIs w/o SSL.** In view of the promising results with PIs in supervised ML (Krishnapriyan et al., 2021, Demir et al., 2022), we consider the PIs with XGB and SVM, our approaches based on pre-training are better. (4) **Other datasets.** We conducted experiments over several other datasets, including activity-cliff data, where TDL is shown beneficial more generally.

**Discussion.** In this section, we have shown results very different from the ones we usually see. In the benchmark, we observe some improvements and, for GraphCL+TDL, numbers competitive with SOTA, but they do not reach the general, large impact we saw in Section 4.1. However, we have such impact over smaller data. Altogether, TDL offers great benefits for _all_ baselines we considered:

* It leads to **much better calibrated distances in the representation space** in terms of PH. Since the magnitude of increase is generally remarkable but varies with the data, this shows potential for further improvement, especially, in terms of choosing the right PIs.
* It considerably **improves representations** in terms of established SSL metrics.
* **Downstream performance reflects this gain of representation power specifically in the low-data scenario**. Given the importance of this kind of data in the real world and the deficiencies of the models we checked, we conclude that our proposal is not only novel and different in its distance-based modeling, but also that it advances molecular SSL in important aspects empirically.

## 5 Conclusions

In this paper, we focus on the quality of representations in molecular SSL, an important topic which has not been addressed greatly to date. We propose a novel approach of shaping the embedding space based on distances in terms of fingerprints from persistent homology. We have evaluated our topological-distance-based contrastive loss extensively, and show that it solves deficiencies of existing models and considerably improves various baselines in terms of representation power. In future work, we plan to analyze the relationship between specific PIs and downstream data in more detail, to further improve our approach, include external knowledge, and to put more focus on regression tasks.

Figure 7: Performance over smaller datasets, subsets of Bace (left) and ClinTox (right).