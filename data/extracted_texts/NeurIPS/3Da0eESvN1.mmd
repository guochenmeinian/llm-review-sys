# An Efficient Tester-Learner for Halfspaces

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan . In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold. We consider the setting where the target distribution is the standard Gaussian in \(d\) dimensions and the label noise is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error \(+\) (and extends to any fixed strongly log-concave target distribution). For adversarial noise, our tester-learner obtains error \(O()+\) in polynomial time. Prior work on testable learning ignores the labels in the training set and checks that the empirical moments of the covariates are close to the moments of the base distribution. Here we develop new tests of independent interest that make critical use of the labels and combine them with the moment-matching approach of . This enables us to implement a testable variant of the algorithm of  for learning noisy halfspaces using nonconvex SGD.

## 1 Introduction

Learning halfspaces in the presence of noise is one of the most basic and well-studied problems in computational learning theory. A large body of work has obtained results for this problem under a variety of different noise models and distributional assumptions (see e.g.  for a survey). A major issue with common distributional assumptions such as Gaussianity, however, is that they can be hard or impossible to verify in the absence of any prior information.

The recently defined model of testable learning  addresses this issue by replacing such assumptions with efficiently testable ones. In this model, the learner is required to work with an arbitrary input distribution \(D_{}\) and verify any assumptions it needs to succeed. It may choose to reject a given training set, but if it accepts, it is required to output a hypothesis with error close to \((,_{})\), the optimal error achievable over \(D_{}\) by any function in a concept class \(\). Further, whenever the training set is drawn from a distribution \(D_{}\) whose marginal is truly a well-behaved target distribution \(D^{*}\) (such as the standard Gaussian), the algorithm is required to accept with high probability. Such an algorithm, or tester-learner, is then said to testably learn \(\) with respect to target marginal \(D^{*}\). (See Definition 2.1.) Note that unlike ordinary distribution-specific agnostic learners, a tester-learner must take some nontrivial action _regardless_ of the input distribution.

The work of  established foundational algorithmic and statistical results for this model and showed that testable learning is in general provably harder than ordinary distribution-specific agnostic learning. As one of their main algorithmic results, they showed tester-learners forthe class of halfspaces over \(^{d}\) that succeed whenever the target marginal is Gaussian (or one of a more general class of distributions), achieving error \(+\) in time and sample complexity \(d^{(1/^{2})}\). This matches the running time of ordinary distribution-specific agnostic learning of halfspaces over the Gaussian using the standard approach of . Their testers are simple and label-oblivious, and are based on checking whether the low-degree empirical moments of the unknown marginal match those of the target \(D^{*}\).

These works essentially resolve the question of designing tester-learners achieving error \(+\) for halfspaces, matching known hardness results for (ordinary) agnostic learning . Their running time, however, necessarily scales exponentially in \(1/\).

A long line of research has sought to obtain more efficient algorithms at the cost of relaxing the optimality guarantee . These works give polynomial-time algorithms achieving bounds of the form \(+\) and \(O()+\) for the Massart and agnostic setting respectively under structured distributions (see Section 1.1 for more discussion). The main question we consider here is whether such guarantees can be obtained in the testable learning framework.

**Our contributions.** In this work we design the first tester-learners for halfspaces that run in fully polynomial time in all parameters. We match the optimality guarantees of fully polynomial-time learning algorithms under Gaussian marginals for the Massart noise model (where the labels arise from a halfspace but are flipped by an adversary with probability at most \(\)) as well as for the agnostic model (where the labels can be completely arbitrary). In fact, for the Massart setting our guarantee holds with respect to any chosen target marginal \(D^{*}\) that is isotropic and strongly log-concave, and the same is true of the agnostic setting albeit with a slightly weaker guarantee.

**Theorem 1.1** (Formally stated as Theorem 4.1).: _Let \(\) be the class of origin-centered halfspaces over \(^{d}\), and let \(D^{*}\) be any isotropic strongly log-concave distribution. In the setting where the labels are corrupted with Massart noise at rate at most \(<\), \(\) can be testably learned w.r.t. \(D^{*}\) up to error \(+\) using \((d,,)\) time and sample complexity._

**Theorem 1.2** (Formally stated as Theorem 5.1).: _Let \(\) be as above. In the adversarial noise or agnostic setting where the labels are completely arbitrary, \(\) can be testably learned w.r.t. \((0,I_{d})\) up to error \(O()+\) using \((d,)\) time and sample complexity._

**Our techniques.** The tester-learners we develop are significantly more involved than prior work on testable learning. We build on the nonconvex optimization approach to learning noisy halfspaces due to  as well as the structural results on fooling functions of halfspaces using moment matching due to . Unlike the label-oblivious, global moment tests of , our tests make crucial use of the labels and check _local_ properties of the distribution in regions described by certain candidate vectors. These candidates are approximate stationary points of a natural nonconvex surrogate of the 0-1 loss, obtained by running gradient descent. When the distribution is known to be well-behaved,  showed that any such stationary point is in fact a good solution (for technical reasons we must use a slightly different surrogate loss). Their proof relies crucially on structural geometric properties that hold for these well-behaved distributions, an important one being that the probability mass of any region close to the origin is proportional to its geometric measure.

In the testable learning setting, we must efficiently check this property for candidate solutions. Since these regions may be described as intersections of halfspaces, we may hope to apply the moment-matching framework of . Naively, however, they only allow us to check in polynomial time that the probability masses of such regions are within an additive constant of what they should be under the target marginal. But we can view these regions as sub-regions of a known band described by our candidate vector. By running moment tests on the distribution _conditioned_ on this band and exploiting the full strength of the moment-matching framework, we are able to effectively convert our weak additive approximations to good multiplicative ones. This allows us to argue that our stationary points are indeed good solutions.

**Limitations and Future Work.** In this paper we provide the first efficient tester-learners for halfspaces when the noise is either adversarial or Massart. An interesting direction for future work would be to design tester-learners for the agnostic setting whose target marginal distributions may lie within a large family (e.g., strongly log-concave distributions) but still achieve error of \(O()\). Another interesting direction is providing tester-learners that are not tailored to a single target distribution, but are guaranteed to accept any member of a large family of distributions.

### Related work

We provide a partial summary of some of the most relevant prior and related work on efficient algorithms for learning halfspaces in the presence of adversarial label or Massart noise, and refer the reader to  for a survey.

In the distribution-specific agnostic setting where the marginal is assumed to be isotropic and log-concave,  showed an algorithm achieving error \(O(^{1/3})+\) for the class of origin-centered halfspaces.  later obtained \(O()+\) using an approach that introduced the principle of iterative _localization_, where the learner focuses attention on a band around a candidate halfspace in order to produce an improved candidate.  used this principle to obtain a PTAS for agnostically learning halfspaces under the uniform distribution on the sphere, and  extended it to more general \(s\)-concave distributions. Further works in this line include .

 introduced the simplest approach yet, based entirely on nonconvex SGD, and showed that it achieves \(O()+\) for origin-centered halfspaces over a wide class of structured distributions. Other related works include .

In the Massart noise setting with noise rate bounded by \(\), work of  gave the first efficient distribution-free algorithm achieving error \(+\); further improvements and followups include . However, the optimal error \(\) achievable by a halfspace may be much smaller than \(\), and it has been shown that there are distributions where achieving error competitive with \(\) as opposed to \(\) is computationally hard . As a result, the distribution-specific setting remains well-motivated for Massart noise. Early distribution-specific algorithms were given by , but a key breakthrough was the nonconvex SGD approach introduced by , which achieved error \(+\) for origin-centered halfspaces efficiently over a wide range of distributions. This was later generalized by .

### Technical overview

Our starting point is the nonconvex optimization approach to learning noisy halfspaces due to . The algorithms in these works consist of running SGD on a natural non-convex surrogate \(_{}\) for the 0-1 loss, namely a smooth version of the ramp loss. The key structural property shown is that if the marginal distribution is structured (e.g. log-concave) and the slope of the ramp is picked appropriately, then any \(\) that has large angle with an optimal \(^{*}\) cannot be an approximate stationary point of the surrogate loss \(_{}\), i.e. that \(\|_{}()\|\) must be large. This is proven by carefully analyzing the contributions to the gradient norm from certain critical regions of \((,^{*})\), and crucially using the distributional assumption that the probability masses of these regions are proportional to their geometric measures. (See Fig. 3.) In the testable learning setting, the main challenge we face in adapting this approach is checking such a property for the unknown distribution we have access to.

A preliminary observation is that the critical regions of \((,^{*})\) that we need to analyze are rectangles, and are hence functions of a small number of halfspaces. Encouragingly, one of the key structural results of the prior work of  pertains to "fooling" such functions. Concretely, they show that whenever the true marginal \(D_{}\) matches moments of degree at most \((1/^{2})\) with a target \(D^{*}\) that satisfies suitable concentration and anticoncentration properties, then \(|}_{D_{}}[f]-}_{D ^{*}}[f]|\) for any \(f\) that is a function of a small number of halfspaces. If we could run such a test and ensure that the probabilities of the critical regions over our empirical marginal are also related to their areas, then we would have a similar stationary point property.

However, the difficulty is that since we wish to run in fully polynomial time, we can only hope to fool such functions up to \(\) that is a constant. Unfortunately, this is not sufficient to analyze the probability masses of the critical regions we care about as they may be very small.

The chief insight that lets us get around this issue is that each critical region \(R\) is in fact of a very specific form, namely a rectangle that is axis-aligned with \(\): \(R=\{:,[-,],[,]\}\) for some values \(,,\) and some \(\) orthogonal to \(\). Moreover, we _know_\(\), meaning we can efficiently estimate the probability \(}_{D_{}}[, [-,]]\) up to constant multiplicative factors without needing moment tests. Denoting the band \(\{:,[-,]\}\) by \(T\) and writing \(}_{D_{}}[R]=}_{D_{ }}[,[,] T]}_{D_{}}[T]\), it turns out that we should expect \(}_{D_{}}[, [,] T]=(1)\), as this is what would occur under the structured target distribution \(D^{*}\). (Such a "localization" property is also at the heart of the algorithms for approximately learning halfspaces of, e.g., .) To check this, it suffices to run tests that ensure that \(_{D_{}}[,[, ] T]\) is within an additive constant of this probability under \(D^{*}\).

We can now describe the core of our algorithm (omitting some details such as the selection of the slope of the ramp). First, we run SGD on the surrogate loss \(\) to arrive at an approximate stationary point and candidate vector \(\) (technically a list of such candidates). Then, we define the band \(T\) based on \(\), and run tests on the empirical distribution conditioned on \(T\). Specifically, we check that the low-degree empirical moments conditioned on \(T\) match those of \(D^{*}\) conditioned on \(T\), and then apply the structural result of  to ensure conditional probabilities of the form \(_{D_{}}[,[, ] T]\) match \(_{D^{*}}[,[,]  T]\) up to a suitable additive constant. This suffices to ensure that even over our empirical marginal, the particular stationary point \(\) we have is indeed close in angular distance to an optimal \(^{*}\).

A final hurdle that remains, often taken for granted under structured distributions, is that closeness in angular distance \((,^{*})\) does not immediately translate to closeness in terms of agreement, \([(,) (^{*},)]\), over our unknown marginal. Nevertheless, we show that when the target distribution is Gaussian, we can run polynomial-time tests that ensure that an angle of \(=(,^{*})\) translates to disagreement of at most \(O()\). When the target distribution is a general strongly log-concave distribution, we show a slightly weaker relationship: for any \(k\), we can run tests requiring time \(d^{(k)}\) that ensure that an angle of \(\) translates to disagreement of at most \(O(^{1-1/k})\). In the Massart noise setting, we can make \((,^{*})\) arbitrarily small, and so obtain our \(+\) guarantee for any target strongly log-concave distribution in polynomial time. In the adversarial noise setting, we face a more delicate tradeoff and can only make \((,^{*})\) as small as \(()\). When the target distribution is Gaussian, this is enough to obtain final error \(O()+\) in polynomial time. When the target distribution is a general strongly log-concave distribution, we instead obtain \(()+\) in quasipolynomial time.

## 2 Preliminaries

Notation and setupThroughout, the domain will be \(=^{d}\), and labels will lie in \(=\{ 1\}\). The unknown joint distribution over \(\) that we have access to will be denoted by \(D_{}\), and its marginal on \(\) will be denoted by \(D_{}\). The target marginal on \(\) will be denoted by \(D^{*}\). We use the following convention for monomials: for a multi-index \(=(_{1},,_{d})_{ 0}^{d}\), \(^{}\) denotes \(_{i}x_{i}^{_{i}}\), and \(||=_{i}_{i}\) denotes its total degree. We use \(\) to denote a concept class mapping \(^{d}\) to \(\{ 1\}\), which throughout this paper will be the class of halfspaces or functions of halfspaces over \(^{d}\). We use \((,D_{})\) to denote the optimal error \(_{f}_{(,y) D_{}}[f() y]\), or just \(\) when \(\) and \(D_{}\) are clear from context. We recall the definitions of the noise models we consider. In the Massart noise model, the labels satisfy \(_{y D_{}}[y(^{*},)]=()\), where \(()<\) for all \(\). In the adversarial label noise or agnostic model, the labels may be completely arbitrary. In both cases, the learner's goal is to produce a hypothesis with error competitive with \(\).

We now formally define testable learning. The following definition is an equivalent reframing of the original definition [13, Def 4], folding the (label-aware) tester and learner into a single tester-learner.

**Definition 2.1** (Testable learning, ).: Let \(\) be a concept class mapping \(^{d}\) to \(\{ 1\}\). Let \(D^{*}\) be a certain target marginal on \(^{d}\). Let \(,>0\) be parameters, and let \(:\) be some function. We say \(\) can be testably learned w.r.t. \(D^{*}\) up to error \(()+\) with failure probability \(\) if there exists a tester-learner \(A\) meeting the following specification. For any distribution \(D_{}\) on \(^{d}\{ 1\}\), \(A\) takes in a large sample \(S\) drawn from \(D_{}\), and either rejects \(S\) or accepts and produces a hypothesis \(h:^{d}\{ 1\}\). Further, the following conditions must be met:

1. (Soundness.) Whenever \(A\) accepts and produces a hypothesis \(h\), with probability at least \(1-\) (over the randomness of \(S\) and \(A\)), \(h\) must satisfy \(_{(,y) D_{}}[h() y] ((,D_{}))+\).
2. (Completeness.) Whenever \(D_{}\) truly has marginal \(D^{*}\), \(A\) must accept with probability at least \(1-\) (over the randomness of \(S\) and \(A\)).

Testing properties of strongly log-concave distributions

In this section we define the testers that we will need for our algorithm. All the proofs from this section can be found in Appendix B. We begin with a structural lemma that strengthens the key structural result of , stated here as Proposition A.3. It states that even when we restrict an isotropic strongly log-concave \(D^{*}\) to a band around the origin, moment matching suffices to fool functions of halfspaces whose weights are orthogonal to the normal of the band.

**Proposition 3.1**.: _Let \(D^{*}\) be an isotropic strongly log-concave distribution. Let \(^{d-1}\) be any fixed direction. Let \(p\) be a constant. Let \(f:^{d}\) be a function of \(p\) halfspaces of the form in Eq. (A.2), with the additional restriction that its weights \(^{i}^{d-1}\) satisfy \(^{i},=0\) for all \(i\). For some \(\), let \(T\) denote the band \(\{:|,|\}\). Let \(D\) be any distribution such that \(D_{|T}\) matches moments of degree at most \(k=(1/^{2})\) with \(D_{|T}^{}\) up to an additive slack of \(d^{-(k)}\). Then \(|_{D^{*}}[f T]-_{D}[f T]|\)._

We now describe some of the testers that we use. First, we need a tester that ensures that the distribution is concentrated in every single direction. More formally, the tester checks that the moments of the distribution along any direction are small.

**Proposition 3.2**.: _For any isotropic strongly log-concave \(D^{*}\), there exists some constants \(C_{1}\) and a tester \(T_{1}\) that takes a set \(S^{d}\{ 1\}\), an even \(k\), a parameter \((0,1)\) and runs and in time \((d^{k},|S|,)\). Let \(D\) denote the uniform distribution over \(S\). If \(T_{1}\) accepts, then for any \(^{d-1}\)_

\[*{}_{(,y) D}[, ^{k}](C_{1}k)^{k/2}.\] (3.1)

_Moreover, if \(S\) is obtained by taking at least \((d^{k},()^{k})^{C_{1}}\) i.i.d. samples from a distribution whose \(^{d}\)-marginal is \(D^{*}\), the test \(T_{1}\) passes with probability at least \(1-\)._

Secondly, we will use a tester that makes sure the distribution is not concentrated too close to a specific hyperplane. This is one of the properties we will need to use in order to employ the localization technique of .

**Proposition 3.3**.: _For any isotropic strongly log-concave \(D^{*}\), there exist some constants \(C_{2},C_{3}\) and a tester \(T_{2}\) that takes a set \(S^{d}\{ 1\}\) a vector \(^{d-1}\), parameters \(,(0,1)\) and runs in time \((d,|S|,)\). Let \(D\) denote the uniform distribution over \(S\). If \(T_{2}\) accepts, then_

\[*{}_{(,y) D}[|,|](C_{2},C_{3}).\] (3.2)

_Moreover, if \(S\) is obtained by taking at least \(^{2}}()\) i.i.d. samples from a distribution whose \(^{d}\)-marginal is \(D^{*}\), the test \(T_{2}\) passes with probability at least \(1-\)._

Finally, in order to use the localization idea of  in a manner similar to , we need to make sure that the distribution is well-behaved also within a band around to a certain hyperplane. The main property of the distribution that we establish is that functions of constantly many halfspaces have expectations very close to what they would be under our distributional assumption. As we show later in this work, having the aforementioned property allows us to derive many other properties that strongly log-concave distributions have, including many of the key properties that make the localization technique successful.

**Proposition 3.4**.: _For any isotropic strongly log-concave \(D^{*}\) and a constant \(C_{4}\), there exists a constant \(C_{5}\) and a tester \(T_{3}\) that takes a set \(S^{d}\{ 1\}\) a vector \(^{d-1}\), parameters \(,\,(0,1)\) and runs in time \((d^{(})},,|S|,)\). Let \(D\) denote the uniform distribution over \(S\), let \(T\) denote the band \(\{:|,|\}\) and let \(_{}\) denote the set \(\{ 1\}\)-valued functions of \(C_{4}\) halfspaces whose weight vectors are orthogonal to \(\). If \(T_{3}\) accepts, then_

\[_{f_{}}|*{}_{  D^{*}}[f() T]-*{ }_{(,y) D}[f() T]| ,\] (3.3)

\[_{^{d-1}:\ ,=0} |*{}_{ D^{*}}[(, )^{2} T]-*{}_{( ,y) D}[(,)^{2} T ]|.\] (3.4)_Moreover, if \(S\) is obtained by taking at least \(( d^{}^{C_{5}} ()}()^{}^{C_{5}}()})^{C_{5}}\) i.i.d. samples from a distribution whose \(^{d}\)-marginal is \(D^{*}\), the test \(T_{3}\) passes with probability at least \(1-\)._

## 4 Testably learning halfspaces with Massart noise

In this section we prove that we can testably learn halfspaces with Massart noise with respect to isotropic strongly log-concave distributions (see Definition A.1).

**Theorem 4.1** (Tester-Learner for Halfspaces with Massart Noise).: _Let \(D_{}\) be a distribution over \(^{d}\{ 1\}\) and let \(D^{*}\) be an isotropic strongly log-concave distribution over \(^{d}\). Let \(\) be the class of origin centered halfspaces in \(^{d}\). Then, for any \(<1/2\), \(>0\) and \((0,1)\), there exists an algorithm (Algorithm 1) that testably learns \(\) w.r.t. \(D^{*}\) up to excess error \(\) and error probability at most \(\) in the Massart noise model with rate at most \(\), using time and a number of samples from \(D_{}\) that are polynomial in \(d,1/,\) and \((1/)\)._

``` Input: Training sets \(S_{1},S_{2}\), parameters \(\), \(\), \(\) Output: A near-optimal weight vector \(\), or rejection Run PSGD on the empirical loss \(_{}\) over \(S_{1}\) to get a list \(L\) of candidate vectors. Test whether \(L\) contains an \(\)-approximate stationary point \(\) of the empirical loss \(_{}\) over \(S_{2}\).  Reject if no such \(\) exists. for each candidate \(^{}\) in \(\{,-\}\)do  Let \(B^{}_{}()\) denote the band \(\{:|^{},|\}\). Let \(^{}_{}\) denote the class of functions of at most two halfspaces with weights orthogonal to \(^{}\).  Let \(^{}=()\).  Run \(T_{1}(S_{2},k=2,)\) to verify that the empirical marginal is approximately isotropic. Reject if \(T_{1}\) rejects.  Run \(T_{2}(S_{2},^{},,^{})\) to verify that \(_{S}[B^{}_{}]=()\). Reject if \(T_{2}\) rejects.  Run \(T_{3}(S_{2},^{},=/6,,^{})\) and \(T_{3}(S,^{},=/2,,^{})\) for a suitable constant \(\) to verify that the empirical distribution conditioned on \(B^{}_{}(/6)\) and \(B^{}_{}(/2)\) fools \(^{}_{}\) up to \(\). Reject if \(T_{3}\) rejects.  Estimate the empirical error of \(^{}\) on \(S\). If all tests have accepted, output \(^{}\{,-\}\) with the best empirical error. ```

**Algorithm 1**Tester-learner for halfspaces

To show our result, we revisit the approach of  for learning halfspaces with Massart noise under well-behaved distributions. Their result is based on the idea of minimizing a surrogate loss that is non convex, but whose stationary points correspond to halfspaces with low error. They also require that their surrogate loss is sufficiently smooth, so that one can find a stationary point efficiently. While the distributional assumptions that are used to demonstrate that stationary points of the surrogate loss can be discovered efficiently are mild, the main technical lemma, which demostrates that any stationary point suffices, requires assumptions that are not necessarily testable. We establish a label-dependent approach for testing, making use of tests that are applied during the course of our algorithm.

We consider a slightly different surrogate loss than the one used in . In particular, for \(>0\), we let

\[_{}()=,y) D_{ }}{}_{}-y,}{\|\|_{2}},\] (4.1)

where \(_{}:\) is a smooth approximation to the ramp function with the properties described in Proposition C.1 (see Appendix C), obtained using a piecewise polynomial of degree \(3\). Unlike the standard logistic function, our loss function has derivative exactly \(0\) away from the origin (for \(|t|>/2\)). This makes the analysis of the gradient of \(_{}\) easier, since the contribution from points lying outside a certain band is exactly \(0\).

The smoothness allows us to run PSGD to obtain stationary points efficiently, and we now state the convergence lemma we need.

**Proposition 4.2** (PSGD Convergence, Lemmas 4.2 and B.2 in ).: _Let \(_{}\) be as in Equation (4.1) with \((0,1]\), \(_{}\) as described in Proposition C.1 and \(D_{}\) such that the marginal \(D_{}\) on \(^{d}\) satisfies Property (3.1) for \(k=2\). Then, for any \(>0\) and \((0,1)\), there is an algorithm whose time and sample complexity is \(O(}+^{4}})\), which, having access to samples from \(D_{}\), outputs a list \(L\) of vectors \(^{d-1}\) with \(|L|=O(}+^{4}})\) so that there exists \( L\) with_

\[\|_{}_{}()\|_{2}\,, { with probability at least }1-\,.\]

_In particular, the algorithm performs Stochastic Gradient Descent on \(_{}\) Projected on \(^{d-1}\) (PSGD)._

It now suffices to show that, upon performing PSGD on \(_{}\), for some appropriate choice of \(\), we acquire a list of vectors that testably contain a vector which is approximately optimal. We first prove the following lemma, whose distributional assumptions are relaxed compared to the corresponding structural Lemma 3.2 of . In particular, instead of requiring the marginal distribution to be "well-behaved", we assume that the quantities of interest (for the purposes of our proof) have expected values under the true marginal distribution that are close, up to multiplicative factors, to their expected values under some "well-behaved" (in fact, strongly log-concave) distribution. While some of the quantities of interest have values that are miniscule and estimating them up to multiplicative factors could be too costly, it turns out that the source of their vanishing scaling can be completely attributed to factors of the form \([|,|]\) (where \(\) is small), which, due to standard concentration arguments, can be approximated up to multiplicative factors, given \(^{d-1}\) and \(>0\) (see Proposition 3.3). As a result, we may estimate the remaining factors up to sufficiently small additive constants (see Proposition 3.4) to get multiplicative overall closeness to the "well behaved" baseline. We defer the proof of the following Lemma to Appendix C.1.

**Lemma 4.3**.: _Let \(_{}\) be as in Equation (4.1) with \((0,1]\), \(_{}\) as described in Proposition C.1, let \(^{d-1}\) and consider \(D_{}\) such that the marginal \(D_{}\) on \(^{d}\) satisfies Properties (3.2) and (3.3) for \(C_{4}=2\) and accuracy \(\). Let \(^{*}^{d-1}\) define an optimum halfspace and let \(<1/2\) be an upper bound on the rate of the Massart noise. Then, there are constants \(c_{1},c_{2},c_{3}>0\) such that if \(\|_{}_{}()\|_{2}<c_{1}(1-2)\) and \( c_{2}\), then_

\[(,^{*})}{1-2} (-,^{*})}{1 -2}\]

Combining Proposition 4.2 and Lemma 4.3, we get that for any choice of the parameter \((0,1]\), by running PSGD on \(_{}\), we can construct a list of vectors of polynomial size (in all relevant parameters) that testably contains a vector that is close to the optimum weight vector. In order to link the zero-one loss to the angular similarity between a weight vector and the optimum vector, we use the following Proposition (for the proof, see Appendix C.2).

**Proposition 4.4**.: _Let \(D_{}\) be a distribution over \(^{d}\{ 1\}\), \(^{*}*{arg\,min}_{^{d-1}} _{D_{}}[y*{sign}( ,)]\) and \(^{d-1}\). Then, for any \((,^{*})\), \([0,/4]\), if the marginal \(D_{}\) on \(^{d}\) satisfies Property (3.1) for \(C_{1}>0\) and some even \(k\) and Property (3.2) with \(\) set to \((C_{1}k)^{}()^{}\), then, there exists a constant \(c>0\) such that the following is true._

\[*{}_{D_{}}[y*{ sign}(,)]+c k^{1/2} ^{1-}\,.\]

We are now ready to prove Theorem 4.1.

Proof of Theorem 4.1.: Throughout the proof we consider \(^{}\) to be a sufficiently small polynomial in all the relevant parameters. Each of the failure events will have probability at least \(^{}\) and their number will be polynomial in all the relevant parameters, so by the union bound, we may pick \(^{}\) so that the probability of failure is at most \(\).

The algorithm we run is Algorithm 1, with appropriate selection of parameters and given samples \(S_{1}\), \(S_{2}\), each of which are sufficiently large sets of independent samples from the true unknown distribution \(D_{}\). For some \((0,1]\) to be defined later, we run PSGD on the empirical loss \(_{}\) over \(S_{1}\) as described in Proposition 4.2 with \(=c_{1}(1-2)/4\), where \(c_{1}\) is given by Lemma 4.3. By Proposition 4.2, we get a list \(L\) of vectors \(^{d-1}\) with \(|L|=*{poly}(d,1/)\) such that there exists \( L\) with \(\|_{}_{}()\|_{2}<c_{1}( 1-2)\) under the true distribution, if the marginal is isotropic.

Having acquired the list \(L\) using sample \(S_{1}\), we use the independent samples in \(S_{2}\) to test whether \(L\) contains an approximately stationary point of the empirical loss on \(S_{2}\). If this is not the case, then we may safely reject: for large enough \(|S_{1}|\), if the distribution is indeed isotropic strongly logconcave, there is an approximate stationary of the population loss in \(L\) and if \(|S_{2}|\) is large enough, the gradient of the empirical loss on \(S_{2}\) will be close to the gradient of the population loss on each of the elements of \(L\), due to appropriate concentration bounds for log-concave distributions as well as the fact that the elements of \(L\) are independent from \(S_{2}\). For the following, let \(\) be a point such that \(\|_{}_{}()\|_{2}<c_{1}(1-2)\) under theempirical distribution over \(S_{2}\)

In Lemma 4.3 and Proposition 4.4 we have identified certain properties of the marginal distribution that are sufficient for our purposes, given that \(L\) contains an approximately stationary point of the empirical (surrogate) loss on \(S_{2}\). Our testers \(T_{1},T_{2},T_{3}\) verify that these properties hold for the empirical marginal over our sample \(S_{2}\), and it will be convenient to analyze the optimality of our algorithm purely over \(S_{2}\). In particular, we will need to require that \(|S_{2}|\) is sufficiently large, so that when the true marginal is indeed the target \(D^{*}\), our testers succeed with high probability (for the corresponding sample complexity, see Propositions 3.2, 3.3 and 3.4). Moreover, by standard generalization theory, since the VC dimension of halfspaces is only \(O(d)\) and for us \(|S_{2}|\) is a large \((d,1/)\), both the error of our final output and the optimal error over \(S_{2}\) will be close to that over \(D_{}\). So in what follows, we will abuse notation and refer to the uniform distribution over \(S_{2}\) as \(D_{}\) and the optimal error over \(S_{2}\) simply as opt.

We proceed with some basic tests. Throughout the rest of the algorithm, whenever a tester fails, we reject, otherwise we proceed. First, we run testers \(T_{2}\) with inputs \((,/2,^{})\) and \((,/6,^{})\) (Proposition 3.3) and \(T_{3}\) with inputs \((,/2,c_{2},^{})\) and with \((,/6,c_{2},^{})\) (Proposition 3.4, \(c_{2}\) as defined in Lemma 4.3). This ensures that for the approximate stationary point \(\) of the \(_{}\), the probability within the band \(B_{}(/2)=\{:|,| /2\}\) is \(()\) (and similarly for \(B_{}(/6)\)) and moreover that our marginal conditioned on each of the bands fools (up to an additive constant) functions of halfspaces with weights orthogonal to \(\). As a result, we may apply Lemma 4.3 to \(\) and form a list of \(2\) vectors \(\{,-\}\) which contains some \(^{}\) with \((^{},^{*}) c_{2}/(1-2)\) (where \(c_{3}\) is as defined in Lemma 4.3).

Figure 1: Critical regions in the proofs of main structural lemmas (Lemmas 4.3, 5.2). We analyze the contributions of the regions labeled \(A_{1},A_{2}\) to the quantities \(A_{1},A_{2}\) in the proofs. Specifically, the regions \(A_{1}\) (which have height \(/3\) so that the value of \(^{}_{}(_{})\) for any \(\) in these regions is exactly \(1/\), by Proposition C.1) form a subset of the region \(\), and their probability mass under \(D_{}\) is (up to a multiplicative factor) a lower bound on the quantity \(A_{1}\) (see Eq (C.3)). Similarly, the region \(A_{2}\) is a subset of the intersection of \(^{c}\) with the band of height \(\), and has probability mass that is (up to a multiplicative factor) an upper bound on the quantity \(A_{2}\) (see Eq (C.4)).

We run \(T_{1}\) (Proposition 3.2) with \(k=2\) to verify that the marginals are approximately isotropic and we use \(T_{2}\) once again, with appropriate parameters for each \(\) and its negation, to apply Proposition 4.4 and get that \(\{,-\}\) contains a vector \(^{}\) with

\[*{}_{D_{}}[y*{ sign}(^{},)]*{}+c^{2/3},\]

where \((^{},^{*}):=c_{2}/\). By picking \(=(^{3/2}(1-2))\), we get

\[*{}_{D_{}}[y*{ sign}(^{},)]*{}+\,.\]

However, we do not know which of the weight vectors in \(\{,-\}\) is the one guaranteed to achieve small error. In order to discover this vector, we estimate the probability of error of each of the corresponding halfspaces (which can be done efficiently, due to Hoeffding's bound) and pick the one with the smallest error. This final step does not require any distributional assumptions and we do not need to perform any further tests. 

## 5 Testably learning halfspaces in the agnostic setting

In this section, we provide our result on efficiently and testably learning halfspaces in the agnostic setting with respect to isotropic strongly log-concave target marginals. We defer the proofs to Appendix D. The algorithm we use is once more Algorithm 1, but we call it multiple times for different choices of the parameter \(\), reject if any call rejects and output the vector that achieved the minimum empirical error overall, otherwise. Also, the tester \(T_{1}\) is called for a general \(k\) (not necessarily \(k=2\)).

**Theorem 5.1** (Efficient Tester-Learner for Halfspaces in the Agnostic Setting).: _Let \(D_{}\) be a distribution over \(^{d}\{ 1\}\) and let \(D^{*}\) be a strongly log-concave distribution over \(^{d}\) (Definition A.1). Let \(\) be the class of origin centered halfspaces in \(^{d}\). Then, for any even \(k\), any \(>0\) and \((0,1)\), there exists an algorithm that agnostically testably learns \(\) w.r.t. \(D^{*}\) up to error \(O(k^{1/2}*{}^{1-})+\), where \(*{}=_{^{d-1}} *{}_{D_{}}[y*{sign}( ,)]\), and error probability at most \(\), using time and a number of samples from \(D_{}\) that are polynomial in \(d^{(k)},(1/)^{(k)}\) and \(((1/))^{O(k)}\)._

_In particular, by picking some appropriate \(k^{2}d\), we obtain error \((*{})+\) in quasipolynomial time and sample complexity, i.e. \(*{poly}(2^{*{polylog}d},()^{ *{polylog}d})\)._

To prove Theorem 5.1, we may follow a similar approach as the one we used for the case of Massart noise. However, in this case, the main structural lemma regarding the quality of the stationary points involves an additional requirement about the parameter \(\). In particular, \(\) cannot be arbitrarily small with respect to the error of the optimum halfspace, because, in this case, there is no upper bound on the amount of noise that any specific point \(\) might be associated with. As a result, picking \(\) to be arbitrarily small would imply that our algorithm only considers points that lie within a region that has arbitrarily small probability and can hence be completely corrupted with the adversarial opt budget. On the other hand, the polynomial slackness that the testability requirement introduces (through Proposition 4.4) between the error we achieve and the angular distance guarantee we can get via finding a stationary point of \(_{}\) (which is now coupled with \(*{}\)), appears to the exponent of the guarantee we achieve in Theorem 5.1.

**Lemma 5.2**.: _Let \(_{}\) be as in Equation (4.1) with \((0,1]\), \(_{}\) as described in Proposition C.1, let \(^{d-1}\) and consider \(D_{}\) such that the marginal \(D_{}\) on \(^{d}\) satisfies Properties (3.2), (3.3) and (3.4) for \(\) with \(C_{4}=2\) and accuracy parameter \(\). Let \(*{}\) be the minimum error achieved by some origin centered halfspace and let \(^{*}^{d-1}\) be a corresponding vector. Then, there are constants \(c_{1},c_{2},c_{3},c_{4}>0\) such that if \(*{} c_{1}\), \(\|_{}_{}()\|_{2}<c_{2}\), and \( c_{3}\) then_

\[(,^{*}) c_{4} (-,^{*}) c_{4}.\]

We obtain our main result for Gaussian target marginals by refining Proposition 4.4 for the specific case when the target marginal distribution \(D^{*}\) is the standard multivariate Gaussian distribution. The algorithm for the Gaussian case is similar to the one of Theorem 5.1, but it runs different tests for the improved version (see Proposition D.1) of Proposition 4.4.

**Theorem 5.3**.: _In Theorem 5.1, if \(D^{*}\) is the standard Gaussian in \(d\) dimensions, we obtain error \(O(*{})+\) in polynomial time and sample complexity, i.e. \(*{poly}(d,1/,(1/))\)._