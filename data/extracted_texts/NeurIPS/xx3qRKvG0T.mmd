# BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis

Zelin Ni

Shanghai Jiao Tong University

Shanghai, China

nzl5116190@sjtu.edu.cn

&Hang Yu1

Ant Group

Hangzhou, China

hyu.hugo@antgroup.com

&Shizhan Liu

Shanghai Jiao Tong University

Shanghai, China

shanluzuode@sjtu.edu.cn

&Jianguo Li2

Ant Group

Hangzhou, China

lijg.zero@antgroup.com

&Weiyao Lin2

Shanghai Jiao Tong University

Shanghai, China

wylin@sjtu.edu.cn

Equal contribution. This work was done when Zelin Ni was a research intern at Ant Group.Corresponding authors.

###### Abstract

Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attention. Finally, we present a Forecast module that selects and consolidates the bases in the future view based on the similarity coefficients, resulting in accurate future predictions. Through extensive experiments on six datasets, we demonstrate that BasisFormer outperforms previous state-of-the-art methods by 11.04% and 15.78% respectively for univariate and multivariate forecasting tasks. Code is available at: https://github.com/nzl5116190/Basisformer.

## 1 Introduction

Bases, such as trends and seasonalities, are indispensable for time series modeling and forecasting, since they capture the underlying temporal patterns and serve as the key factors driving changes in the data over time. For example, seasonalities can capture the regular fluctuations in demand for a product or a service, while trends can reflect the long-term growth or decline of a market or industry. Incorporating these bases into time series models can improve the understanding and prediction of future behaviors. Indeed, all commonly used deep learning models for time series forecasting can be reimagaged as basis-driving models. N-BEATS , N-HiTS , and FiLM  all resort to explicit bases, such as Fourier and Legendre basis. More generally, the linear (or convolution) layers in MLPs  (or CNNs ) can be regarded as implicit bases, as they act as filter banks to decompose the time series. In addition, the covariate embedding (a.k.a. global timestamp embedding) in RNNs and Transformers [7; 8; 9; 10; 11; 12; 13] is another form of basis, since they provide reference points for predicting future sequences.

To apply bases for time series forecasting, three steps are required. First and foremost, an appropriate basis should be chosen or learned for the set of time series at hand. In practice, the full space of the basis can often be very large, whereas the patterns of the time series in a set are often similar. It is therefore desirable to learn a basis that is tailored to the specific characteristics (e.g., periods or frequencies) of the time series data. This can help reduce the complexity of the forecasting model, as well as make it more accurate and interpretable. Second, each time series in the set is decomposed according to the basis. This involves computing coefficients or weights that determine the similarity or projection energy of the time series w.r.t. (with respect to) each vector (i.e., filter) in the basis. Note that these coefficients are supposed to vary across different time series, since each time series also exhibits unique patterns and characteristics. For instance, the Fourier coefficients corresponding to different time series will be dissimilar. Finally, the prediction is determined by the weighted aggregation of the future part of the basis.

Unfortunately, the aforementioned state-of-the-art methods fall short in satisfying the requirements in the first two steps simultaneously. On one hand, methods relying on classical bases, such as N-BEATS , N-HiTS , and FiLM , often assume that the basis is not learnable and instead learn the coefficients of each time series in a flexible manner when projecting to the basis. However, such a basis may not effectively account for temporal patterns since there is no guarantee that the given basis includes all periods or frequencies corresponding to the set of time series. On the other hand, approaches that aim to adaptively learn the basis from data, such as MLP , CNN , RNN , Transformer and their variants [7; 8; 9; 10; 11; 12; 13], often overlook the need for flexible associations between the basis and each individual time series. Specifically, although Transformer and its variants learn the covariate embeddings, they add or concatenate the same embeddings to different time series embedding in a restricted manner. For MLP and CNN, they adopt the same linear and convolution layers for all time series.

To effectively tackle the aforementioned quandary, it is imperative to obtain a basis that can accurately reflect the distinct traits of the dataset, and to devise a predictive network that can selectively utilize relevant vectors in the basis for forecasting purposes. To move forward to this goal, we propose BasisFormer, a time series forecasting architecture with learnable and interpretable basis. As a first step, we acquire the basis through adaptive self-supervised learning from the data. This involves treating the historical and future sections of a time series as two distinct views and employing contrastive learning to learn the basis, assuming that the selection of basis for a time series should be consistent across both views. Subsequently, we design the Coef module that measures the similarity between the time series and the basis in the historical view via bidirectional cross-attention, facilitating the flexible association between individual time series and the basis. Finally, we develop a Forecast module that consolidates vectors from the basis in the future view according to the similarity yielded by the Coef module, leading to accurate future predictions. We emphasize that the above three parts are trained in an end-to-end fashion. In summary, the key contributions of our work comprise:

* We propose a self-supervised method for basis learning by treating the historical and future sections of the time series as two distinct views and employing contrastive learning, which ensures that the selection of basis for a time series is consistent across both views.
* We design the Coef and Forecast module that chooses and merges relevant basis in the future view based on the coefficients that measure the similarity between the time series and the basis in the historical view.
* We conduct extensive experiments on six datasets, and find that our model outperforms previous SOTA methods by 11.04% for univariate forecasting tasks and 15.78% for multivariate forecasting tasks.

## 2 Related works

**Time series forecasting models** In recent years, deep learning methods have emerged as the predominant technique for time series forecasting. As illustrated in the introduction, these deep learning methods typically resort to bases to facilitate the prediction of the future. Depending on the types of bases used in the network, forecasting models fall into two categories: those using classical orthogonal bases and those using learnable bases. The first group involves N-BEATS , N-HiTS , and FiLM . N-BEATS and N-HiTs typically utilize the Fourier basis and then learn the coefficientsfor this basis in a recursive network such that the basis helps decompose the historical part of the time series into different components and these components can be further aggregated to predict the future. FiLM approximates the historical part via the Legendre polynomial basis and further removes the noise with the help of the Fourier basis. The major drawback with the group of methods is that the basis is predefined, thus giving rise to the problem of which type of basis to choose (e.g., Fourier or Legendre polynomial) and further which vectors in the basis to choose (e.g., which frequency components we choose from the Fourier basis). On the other hand, the models based on learnable bases, such as Dlinear , TCN , Deepar , LogTrans , Informer , AutoFormer , FedFormer , etc, use a learnable linear or convolution layer, or covariate embeddings as the basis. Although these bases are adaptable to the time series, the relationship between the basis and the time series is fixed for all time series. For example, the covariate embedding is added or concatenated to the embedding of different time series in the same way, without considering the unique frequencies and periodic patterns of each series. In our paper, we propose a method that allows for both a learnable basis and a flexible correlation between the basis and each time series for more accurate predictions.

**Basis learning for time series analysis** Apart from time series forecasting, learnable bases have also been explored for other time series-related tasks, such as time series classification. Note that basis learning is distinct from the representation learning of time series, as the former aims to capture the pattern of a set of time series using a common basis, while the latter aims to extract features from individual time series. Moreover, the basis can help extract features from time series as in Dlinear  and TCN . Traditionally, a non-learnable basis is exploited, such as Fourier and wavelet basis. However, there exist a handful of works that overcome this limitation and enable the use learnable basis. The learnable group transform  generalizes the filter banks in the wavelet transformer and allows nonlinear transformation from the mother wavelet to obtain flexible filter banks that can better extract features from time series. Along this direction, Balestriero _et al._ propose a learnable Gaussian filter over the Wigner-Ville transform with a few interpretable parameters, and prove that the resulting filter banks can interpolate between classical ones, including Fourier, wavelet, and chirplet basis. Similar works have also been proposed for audio signal processing [16; 17], suggesting that a learnable basis is a more effective feature extractor than a fixed basis. Thus, we utilize a learnable basis in our work and demonstrate its usefulness for time series forecasting. It should be noted that DEPTS  tackles the challenges posed by intricate dependencies and multiple periodicities in periodic time series through the implementation of a deep expansion learning framework. However, the complex initialization and optimization strategies employed by DEPTS, as well as its limitation of only being applicable to periodic sequences, have motivated us to develop a simpler and more universally applicable basis learning framework. Concretely, we propose a novel method for basis learning based on self-supervised contrastive learning.

**Self-supervised time series representation learning** Since we employ self-supervised representation learning techniques to learn the basis, it is necessary to examine related works in this domain. One prevalent method in this field is the contrastive predictive coding (CPC)  which implements contrastive learning to obtain time series representations by treating subsequent future time series as positive samples and random non-subsequent future time series as negative samples. Another method, TS-TCC , augments the data with two types of perturbations to obtain two views and performs a cross-view prediction task contrastively, similar to CPC. As an alternative, TS2VEC  generates positive samples via timestamp masking or random cropping without leveraging the future information. Note that all these methods seek to establish a common representation for a time series from various views. Unlike these approaches, our goal is to preserve the consistency of the relationship between the time series and the basis. In other words, while the representation of the time series in the historical and future view may differ, their relationships with the corresponding basis should remain consistent.

## 3 BasisFormer

Suppose we have a collection of time series with a dimension of \(C\), implying that \(C\) correlated time series require simultaneous prediction. Each time series in this set is characterized by its history \(=(x_{1},,x_{I})\) and future \(=(y_{1},,y_{O})\), where \(I\) and \(O\) correspond to the input and output sequence lengths, respectively. Our primary objective is to learn a basis \(\) that can account for the behavior of all time series in the group, and further exploit it for predicting \(\) given \(\). Correspondingly, \(\) can also be split into the historical component \(_{x}\) and the future component \(_{y}\).

[MISSING_PAGE_EMPTY:4]

Correspondingly, given \(C\) time series \(^{C I}\) and the basis \(_{x}^{N I}\) of size \(N\), we can get their representations by stacking \(M\) layers of \(_{H}\), namely, \(^{(M)}^{C D_{c} H}\) and \(_{x}^{(M)}^{N D_{c} H}\), where \(D_{c}\) represents the hidden dimension for each head in \(_{H}\). Note that the cross attention is computed between the time series and the basis, instead of across time which is frequently found in Transformer based models [7; 13]. Additionally, the attention mechanism is used to allow for flexible associations between the time series and basis vectors. This approach ensures that each time series can selectively attend to the most relevant basis vectors, and likewise, each basis vector can selectively attend to the most relevant time series.

Finally, the Coef module calculates the "coefficient" of each time series w.r.t. each basis vector as the inner product of their representations \(x^{(M)}\) and \(z_{x}^{(M)}\) for each of the \(H\) heads, resulting in the coefficient tensor \( R^{C N H}\).

### Forecast module for aggregation and future prediction

After obtaining the coefficients, we take advantage of them for the sake of forecasting. We begin by projecting the future part of the basis vectors \(_{y}\) into a space where they can be linearly aggregated using the coefficients. As the Coef module computes the coefficients for \(H\) heads, the projection of \(_{y}\) should also have \(H\) heads to maintain consistency. To this end, we employ a four-layer Multi-Layer Perceptron (MLP) with a bottleneck to map \(_{y}^{N O}\) to \(}_{y}^{N O}\), which is then split into \(H\) heads, each with size \(N(O/H)\), and is denoted as \(}_{y}^{N H(O/H)}\).

For each head, we aggregate the \(N\) basis vectors by computing the coefficients weighted sum of \(}_{y}\) over the dimension of \(N\), that is,

\[}[i,h]=_{j=1}^{N}[i,j,h]}_{y}[j,h,:],\] (5)

where \(h\{1,,H\}\) denotes the head index, and the size of \(}\) is \(C H(O/H)\).

Next, we concatenate the \(H\) heads together and pass them through another four-layer MLP with a bottleneck, in order to exchange information between different heads. This is because different heads may have captured different aspects of the input sequence and the fusion MLP can help combine the information and improve the overall prediction performance.

It is noteworthy that the bottleneck layers in the above module are used to reduce the dimensionality of the input features before projecting them to a higher-dimensional space. This helps to reduce the computational complexity of the projection operation and prevent overfitting. Additionally, using a bottleneck layer can also assist in extracting more informative features by forcing the model to learn a compressed representation of the input, thus improving the prediction accuracy.

Finally, we compare the predicted values \(}\) with the true values \(\) via a Mean Squared Error (MSE) loss function, that is, \(L_{}=(},)\).

### Basis module for basis learning

In this subsection, we present our approach for learning a data-driven basis in a self-supervised manner. The goal is to obtain a basis that satisfies three essential properties.

First, the relation between the basis vectors and the time series should be consistent across time, such that we can predict the future by combining the future part of the basis using the coefficients obtained from the historical part of the basis and the time series. Specifically, given a time series \((,)\) and the corresponding basis \((_{x},_{y})\), the coefficients (i.e., edge strength) between the time series and the basis should be consistent across the historical view \((,_{x})\) and the future view \((,_{y})\). In other words, the relevance of a given time series to a particular basis vector in the historical view should be retained in the future view. To achieve this, we pass both \((,_{x})\) and \((,_{y})\) through the Coef module to get the coefficient tensor respectively for the two views, \(_{x}\) and \(_{y}\), both with size \(C N H\). For each time series, we then perform contrastive learning by regarding the coefficient w.r.t. each basis vector in \(_{x}\) as the anchor point, the coefficient w.r.t. the corresponding basis vector in \(_{y}\) as the positive sample, and the coefficient w.r.t. the remaining basis vectors in \(_{y}\) as the negative sample. We optimize the InfoNCE loss to maximize the mutual information between \(_{x}\) and \(_{y}\)which is given by

\[L_{}=-_{i=1}^{C}_{j=1}^{N}_{x}[i,j,:]_{y}[i,j,:]/)}{_{k=1}^{N}(_{x} [i,j,:]_{y}[i,k,:]/)}.\] (6)

where \(\) represents the temperature used to adjust the smoothness of alignment distribution.

In addition, we require the basis to be interpretable, which means that we can gain insights into the underlying patterns captured by the basis. To achieve interpretability, we promote smoothness across time with a regularization term, that is,

\[L_{}=\|\|_{2}^{2},\] (7)

where we concatenate the basis vectors for the historical and future views \((_{x},_{y})\) along the last dimension to form \(\), and the smoothness matrix \(^{(I+O)(I+O-2)}\) can be expressed as:

\[=1&-2&1&&\\ &&&&\\ &&1&-2&1.\] (8)

It is apparent by multiplying \(\) with \(\), we compute \(\|[:,t-1]-2[:,t]+[:,t+1]\|_{2}^{2}\), which is the curvature over time . One advantage of using \(\) is that the addition of a constant and a linear function of time makes the loss invariant. Therefore, the above smoothness loss can accommodate the change of the overall mean level as well as the linear trend.

Finally, the basis should be a function of the timestamp. As such, we develop a four-layer MLP with a skip connection between the input and the output of the second layer. The input to the network is the normalized timestamp associated with the first time point in the historical window. Suppose that the overall length of a time series in the dataset is \(T\), then the normalized timestamp is defined as \(=t/T\), where \(t\{0,,T-1\}\). The output of the networks is an \(N(I+O)\) tensor, which is the basis for the current time window.

Overall, the loss we optimize can be expressed as:

\[L=L_{}+L_{}+L_{}.\] (9)

We find that the performance of BasisFormer is robust to the weights in front of the terms in (9). Therefore, we set the weights to be one in all our experiments. Sensitivity analysis of the weights in the loss function can be found in the Appendix A.4.

## 4 Experiments

To assess the effectiveness of our model, we conducted comprehensive experiments on six datasets from real-world scenarios, using the experimental setup identical to that in [5; 7; 8; 9]. Below we summarize the experimental setup, datasets, models, and compared models.

**Experimental setup**: The length of the historical input sequence is maintained at 96 (or 36 for the illness dataset), whereas the length of the sequence to be predicted is selected from a range of values, i.e., \(\{96,192,336,720\}\) (\(\{24,36,48,60\}\) for the illness dataset). Note that the input length is fixed to be 96 for all methods for a fair comparison.

**Datasets**: The six datasets used in this study comprise the following: 1) ETT , which consists of temperature data of electricity transformers; 2) Electricity, which includes power consumption data of several customers; 3) Exchange , containing financial exchange rate within a specific time range; 4) Traffic, comprising data related to road traffic; 5) Weather, which involves various weather indicators; and 6) Illness, consisting of recorded influenza-like illness data. Note that ETT is further divided into four sub-datasets: ETTh1, ETTh2, ETTm1, and ETTm2, and the results in Table 1 are based only on the ETTm2 sub-dataset. The results for the remaining three sub-datasets can be found in the Appendix.

**Models for comparison**: In this study, we compare our proposed model against the following state-of-the-art models: four transformer-based models, namely FEDformer , Autoformer , Pyraformer ; one MLP-based model, i.e., Dlinear ; and one CNN-based model, i.e., TCN . We also consider two recently proposed models, e.g., N-Hits  and FiLM . Due to space 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

follows this periodic pattern and effectively capture the salient features of the data. Secondly, the basis vectors given by our approach are smooth, indicating that they are not corrupted by the noise in the data. Note that the noise is not predictable, and so the basis that drives the change in the future is preferred to be smooth. Thirdly, it is evident that the learned bases have varying heights and intervals, thereby providing diversity to characterize different features of the time series. Finally, the obtained bases are consistent from both past and future perspectives, thus facilitating the prediction of future trends based on the coefficient similarity between a time series and the basis in the historical part. More visualized results can be found in the Appendix E.

## 5 Conclusion

This paper presents BasisFormer, a novel solution to alleviate two significant limitations that impede the efficacy of the existing SOTA methods. Through the utilization of BasisFormer, automatic learning of a self-adjusting basis can be achieved. Moreover, given the learned basis, BasisFormer also allows different time series to be correlated with distinct subsets of basis vectors. Our experimental findings provide compelling evidence of the superiority of BasisFormer over existing methods.

## 6 Acknowledgements

The paper is supported in part by the following grants: National Key Research and Development Program of China Grant (No.2018AAA0100400), National Natural Science Foundation of China (No. 62325109, U21B2013, 61971277).

   baseline &  &  \\  comparison &  &  &  &  \\  metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\
96 & 0.193 & 0.308 & **0.183** & **0.301** & 0.201 & 0.317 & **0.193** & **0.305** \\
192 & 0.201 & 0.315 & **0.196** & **0.312** & 0.222 & 0.334 & **0.200** & **0.312** \\
336 & 0.214 & 0.329 & **0.209** & **0.324** & 0.231 & 0.338 & **0.211** & **0.325** \\
720 & 0.246 & 0.355 & **0.229** & **0.340** & 0.254 & 0.361 & **0.252** & **0.359** \\ avg & 0.214 & 0.327 & **0.204** & **0.319** & 0.227 & 0.338 & **0.214** & **0.325** \\   

Table 7: The performance comparison of the self-supervised module used in Transformer-based models. The ’origin’ represents no modifications made to the original model and the ‘(+)coef module’ represents applying our designed self-supervised network to supervise the covariate in the models. The dataset used in this experiment was Electricity. The best results are marked in bold.

Figure 3: The visualization of time series and learned basis on the Traffic dataset: The solid line indicates the historical series and the dashed line indicates the future series. For this visualization, we set the input length \(I\) to 96 and the output length \(O\) to 96.