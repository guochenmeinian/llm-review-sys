# Tester-Learners for Halfspaces: Universal Algorithms

Aravind Gollakota

Apple

aravindg@cs.utexas.edu

&Adam R. Klivans

UT Austin

klivans@cs.utexas.edu

&Konstantinos Stavropoulos

UT Austin

kstavrop@cs.utexas.edu

&Arsen Vasilyan

MIT

vasilyan@mit.edu

###### Abstract

We give the first tester-learner for halfspaces that succeeds _universally_ over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error \(O()+\) on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is _any_ distribution that satisfies a Poincare inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincare distributions includes all strongly log-concave distributions, and, assuming the Kannan-Lovasz-Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error \(+\) while accepting all log-concave distributions unconditionally (without assuming KLS). Our tests rely on checking hypercontractivity of the unknown distribution using a sum-of-squares (SOS) program, and crucially make use of the fact that Poincare distributions are certifiably hypercontractive in the SOS framework.

## 1 Introduction

In this paper we study the recent model of testable learning, due to Rubinfeld and Vasilyan . Testable learning addresses a key issue with essentially all known algorithms for the basic problem of agnostic learning, in which a learner is required to produce a hypothesis competitive with the best-fitting hypothesis in a concept class \(\). The issue is that these algorithms make distributional assumptions (such as Gaussianity or log-concavity) that are in general hard to verify. This means that in the absence of any prior information about the distribution or the optimal achievable error, it can be hard to check that the learner has even succeeded at meeting its guarantee.

In the testable learning model, the learning algorithm, or tester-learner, is given access to labeled examples from an unknown distribution and may either reject or accept the unknown distribution. If it accepts, it must successfully produce a near-optimal hypothesis. Moreover, it is also required to accept whenever the unknown distribution truly has a certain target marginal \(D^{*}\). Work of  provided tester-learners for a range of basic classes (including halfspaces, intersections of halfspaces, and more) with respect to particular target marginals \(D^{*}\) (such as the standard Gaussian). All of these algorithms, however, have the shortcoming that they are closely tailored to the particular target marginal \(D^{*}\) that is chosen. Indeed, their tests would reject many well-behaved distributions that are appreciably far from \(D^{*}\). A highly natural question from both a theoretical and a practical perspective is: can we design tester-learners that accept a wide class of distributions simultaneously, without being tailored to any particular one?In this work we answer this question in the affirmative by introducing and studying _universally testable learning_. We formally define this model as follows.

**Definition 1.1** (Universally Testable Learning).: Let \(\) be a concept class mapping \(^{d}\) to \(\{ 1\}\). Let \(\) be a family of distributions over \(^{d}\). Let \(,>0\) be parameters, and let \(:\) be some function. We say \(\) can be universally testably learned w.r.t. \(\) up to error \(()+\) with failure probability \(\) if there exists a tester-learner \(A\) meeting the following specification. For any distribution \(D_{}\) on \(^{d}\{ 1\}\), \(A\) takes in a large sample \(S\) drawn from \(D_{}\), and either rejects \(S\) or accepts and produces a hypothesis \(h:^{d}\{ 1\}\) such that:

* (Soundness.) With probability at least \(1-\) over the sample \(S\) the following is true: If \(A\) accepts, then the output \(h\) satisfies \(_{(,) D_{}}[h() y]((,D_{}))+\), where \((,D_{})=_{f} _{(,) D_{}}[h() y]\).
* (Completeness.) Whenever the marginal of \(D_{}\) lies within \(\), \(A\) accepts with probability at least \(1-\) over the sample \(S\).

In this terminology, the original definition of testable learning reduces to the special case where \(=\{D^{*}\}\). We stress that while the prior work of  allowed \(D^{*}\) to be, say, any fixed strongly log-concave distribution, their tester-learners are still tailored to the particular \(D^{*}\) that is selected. This is because their tests rely on checking that the unknown distribution closely matches moments with \(D^{*}\). By contrast, a universal tester-learner must accept _all_ marginals in a family \(\).

Our main contribution in this paper is the first universal tester-learner for the class of halfspaces with respect to a broad family of structured continuous distributions. This family is the set of all distributions with bounded Poincare constant (see Definition 2.4) and some mild concentration and anti-concentration properties (see Definition 2.1). It captures all strongly log-concave distributions, and in fact, under the well-known Kannan-Lovasz-Simonovits (KLS) conjecture (see Conjecture 2.6), it captures all log-concave distributions as well. Our universal tester-learner significantly generalizes the main result of , who showed comparable guarantees only for the case where the target marginal is the standard Gaussian.

**Theorem 1.2** (Universal Tester-Learner for Halfspaces; formally stated as Theorem 4.1).: _Let \(\) be the class of origin-centered halfspaces over \(^{d}\). Let \(\) be the class of \((1)\)-nice and \((1)\)-Poincare distributions (see Definitions 2.1 and 2.4), which includes all isotropic strongly log-concave and, under KLS, all isotropic log-concave distributions. Then \(\) can be universally testably learned w.r.t. \(\) up to error \(O()+\) in \((d,)\) time and sample complexity._

A special and well-studied case of interest is when the label noise follows the Massart model, i.e. the label of every example is flipped by an adversary with probability at most \(\). In this case we are able to handle a considerably larger class \(\) while also providing a stronger guarantee.

**Theorem 1.3** (Universal Tester-Learner for Massart Halfspaces; formally stated as Theorem 4.1).: _Let \(\) be the class of origin-centered halfspaces over \(^{d}\). Let \(\) be the class of \((d)\)-nice and \((d)\)-Poincare distributions, which includes all isotropic log-concave distributions (unconditionally). Suppose the label noise follows the Massart model with noise rate at most \(<\). Then \(\) can be universally testably learned w.r.t. \(\) up to error \(+\) in \((d,,)\) time and sample complexity._

**Technical Overview.** We first describe the key reasons why prior tester-learners were tailored to a specific target \(D^{*}\). All known polynomial-time algorithms for agnostically learning halfspaces up to error \(O()+\) require some concentration and anti-concentration properties from the input marginal distribution (encapsulated e.g. in Definition 2.1). While concentration is relatively straightforward to check (e.g. by checking that the moments do not grow at too fast a rate), the key challenge in designing tester-learners for halfspaces is to check anti-concentration. All prior tester-learners  use the heavy machinery of moment-matching to achieve this. This approach relies on establishing structural properties of the following type: if \(D^{*}\) is a well-behaved distribution (e.g. a strongly log-concave distribution), and \(D\) approximately matches \(D^{*}\) in its low-degree moments, then \(D\) is also well-behaved (in particular, anti-concentrated). A canonical statement of such a property is the main pseudorandomness result of  (see Theorem 5.6 therein), which establishes that approximate moment-matching fools functions of a constant number of halfspaces. Applying this property inherently requires comparing the low-degree moments of \(D\) with those of \(D^{*}\). Such tests do (implicitly) succeed universally for the class of all distributions that match low-degree moments with \(D^{*}\) (e.g., if \(D^{*}\) is the uniform distribution over the hypercube, moment matching would accept all \(k\)-wise independent distributions). Definition 1.1, however, seeks a far broader kind of universality. Our tests are not tailored to a single target in any way, and are intended to succeed over practical classes of distributions that are commonly considered in learning theory (e.g., log-concave distributions).1

We overcome this hurdle and design a conceptually simple way of checking anti-concentration without requiring the hammer of moment-matching. Our approach follows and improves on the roadmap used by  to design efficient tester-learners for halfspaces using non-convex SGD (building on ). Let us outline this approach at a high level (a more detailed technical overview may be found in [11, Sec 1.2]). The tester-learner first computes a stationary point \(\) of a certain smooth version of the ramp loss, a surrogate for the 0-1 loss. Let \(^{*}\) be any solution achieving 0-1 error \(\). The tester-learner now checks distributional properties of the unknown marginal \(D\) that ensure that \(\) is close in angular distance to \(^{*}\) (specifically, they ensure the contrapositive, namely that any \(\) that has large gradient norm must have large angle with \(^{*}\)). By a more careful analysis of the gradient norm than in  (see Proposition 4.2), we are able to reduce to showing the following weak anti-concentration property. Let \(\) denote any unit vector orthogonal to \(\), and let \(D_{T}\) denote \(D\) restricted to the band \(T=\{|,|\}\) (where the width \(\) is carefully selected according to certain constraints). Then the property we need is that

\[*{}_{ D_{T}}[|,|(1)](1).\]

Our key observation is that the classical Paley-Zygmund inequality applied to the random variable \(Z=,^{2}\), where \( D_{T}\), already gives us the following type of anti-concentration:

\[*{}[Z>}[Z]}{2} ]}[Z]^{2}}{ *{}[Z^{2}]}.\]

This turns out to suffice for our purposes--provided we can show a hypercontractivity property for \(Z\), namely that \(*{}[Z^{2}](1)*{}[Z]^{2}\) (as well as that \(*{}[Z]=(1)\), which is just a second moment constraint).

Our main algorithmic idea is to use a sum-of-squares (SOS) program to check hypercontractivity of the random variable \(Z\). To do so, we crucially leverage a result due to  stating that any \(D\) that has bounded Poincare constant is _certifiably hypercontractive_ in the SOS framework (and it turns out this extends to \(D_{T}\) as well). This means that we can run a certain polynomial-time semidefinite program that checks hypercontractivity of \(Z\) over the sample, and whenever \(D\) is in fact Poincare, we are guaranteed that the test will pass with high probability (see Proposition 3.5). This is sufficient to ensure that the stationary point \(\) we have computed is indeed close in angular distance to \(^{*}\).

In order to finally arrive at our main results, we need to run further tests which ensure that the disagreement between our computed \(\) and any (unknown) optimum \(^{*}\) is bounded by the angle between them, i.e., \(*{}_{ D}[*{}( ,*{}( ^{*},)] O((,^{* }))\) (see Lemma 3.1). This in turn guarantees that \(\) has error \(O()+\). We stress that while  introduced similar testers for the special case of Gaussian marginals, our tests succeed universally with respect to a broad family of distributions including some heavy-tailed distributions (see Definition 2.1). From a technical perspective, prior to our work, such tests either produced a suboptimal bound, or required estimating the operator norms of a polynomial number of random matrices formed using rejection sampling. We significantly simplify this approach by showing that it is sufficient to estimate the operator norm of a single random matrix. Finally, to obtain our improved results for the Massart setting, it turns out that the proof admits certain simplifications that guarantee final error \(+\) while also allowing a wider range of Poincare distributions.

**Related Work.** There is a large body of work on agnostic learning algorithms for halfspaces that run in fully polynomial time. We briefly mention only those that are most closely relevant to our work; please see  for a survey as well as [11, Sec 1.1] for further related work. Following a long line of work on distribution-specific agnostic learners for halfspaces , the work of  introduced a particularly simple approach for the Massart setting, based solely on non-convex SGD. This work, which sets the template that our approach also follows, achieved the information-theoretically optimal error of \(+\) for origin-centered Massart halfspaces over a wide range of structured distributions (and was later extended to general halfspaces by ). The non-convex SGD approach was then generalized by  to show an \(O()+\) guarantee for the fully agnostic setting.

The testable learning model was introduced by the work of , who showed a tester-learner for halfspaces achieving error \(+\) in time \(d^{(1/^{4})}\) for the case where the target marginal is Gaussian. Subsequently,  provided a general algorithmic framework based on moment-matching for this problem, and showed a tester-learner for halfspaces only requiring time \(d^{(1/^{2})}\) with respect to any fixed strongly log-concave marginal (matching known lower bounds for ordinary agnostic learning over Gaussian marginals ).

The most closely relevant work to the present one is that of  (see also ), who showed fully polynomial-time tester-learners for halfspaces achieving error \(O()+\) in the agnostic setting and \(+\) in the Massart setting for the case where the target marginal is the Gaussian. As detailed in the technical overview, their tests rely crucially on moment-matching and are tailored to a specific target marginal. By contrast, our tests check hypercontractivity using an SOS program and succeed universally for a wide class of certifiably hypercontractive distributions.

Certifying distributional properties such as hypercontractivity is an important aspect of a large body of work on robust algorithmic statistics using the SOS framework. We will not attempt to summarize this literature here and direct the reader to  for overviews of related work, as well as to  for a textbook treatment. The notion of certifiable anti-concentration has also been studied (see e.g. ), but it turns out not to be directly useful for our purposes as it is only known to hold for distributions satisfying very strong conditions such as rotational symmetry.

**Limitations and Further Work.** Open directions in testable learning (and universally testable learning) include the design of (efficient) tester-learners for concept classes other than the class of halfspaces, e.g., functions of halfspaces or neurons with other activations (like ReLU or sigmoid).

## 2 Preliminaries

**Notation and Terminology.** For what follows, we consider \(D_{}\) to be an unknown joint distribution over \(\) from which we receive independent samples, and its marginal on \(\) will be denoted by \(D_{}\). In particular \(=^{d}\), and labels will lie in \(=\{ 1\}\). We will use \(\) to denote a concept class mapping \(^{d}\) to \(\{ 1\}\), which throughout this paper will be the class of halfspaces or functions of halfspaces over \(^{d}\). We use \((,D_{})\) to denote the optimal error \(_{f}_{(,y) D_{}}[f() y]\), or just \(\) when \(\) and \(D_{}\) are clear from context. We recall that in Massart noise model, the labels satisfy \(_{y D_{}|}[y(^{*},)]=()\), with \(()<\) for all \(\). When we have adversarial noise (i.e., when we are in the agnostic model), the labels can be completely arbitrary. In both cases, the goal is to produce a hypothesis whose error is competitive with \(\). We use \(\) to denote the expectation of a random variable in brackets (or, correspondingly, \(\) for the probability of an event), either over the unknown joint distribution or over the empirical distribution with respect to a sample \(S\) (e.g., \(_{Z S}[f(Z)]=_{Z S}f(Z)\)).

**Definitions and Distributional Assumptions.** For the problem of learning halfspaces in the agnostic and in Massart noise models, any of the known polynomial algorithms that achieve computationally optimal guarantees require that the marginal distribution has at least the following nice properties previously defined by, e.g., .

**Definition 2.1** (Nice Distributions).: For a given constant \( 1\), we consider the class of \(\)-nice distributions over \(^{d}\) to be the distributions that satisfy the following properties:

1. For any unit vector \(\) in \(^{d}\) the distribution satisfies \([,^{2}][, ]\).(bounded spectrum)
2. For any two dimensional subspace \(V\), the corresponding marginal density \(q_{V}()\) satisfies \(q_{V}() 1/\) for any \(\|\|_{2} 1/\). (anti-anti-concentration)3. For any two dimensional subspace \(V\), the corresponding marginal density \(q_{V}()\) satisfies \(q_{V}() Q(\|\|_{2})\) for some function \(Q:_{+}_{+}\) such that \(_{r 0}Q(r)\) and also \(_{r=0}^{}r^{k}Q(r)\,dr\), for any \(k=1,3,5\). (anti-concentration and concentration)

In the testable learning framework, however, corresponding results provide testable guarantees with respect to target marginals that are isotropic strongly log-concave , which is a strictly stronger condition than the one of Definition 2.1 (see Proposition 2.3 below). We now provide the standard definition of (strongly) log-concave distributions.

**Definition 2.2** ((Strongly) Log-Concave Distributions ).: We say that a distribution over \(^{d}\) is (\(\)-strongly) log-concave, if its density can be written as \(e^{-}\), where \(\) is a (\(\)-strongly) convex function on \(^{d}\) (for some \(>0\)).

**Proposition 2.3** (Log-Concave Distributions are Nice ).: _There exists a universal constant \( 1\) such that any isotropic log-concave distribution is \(\)-nice._

In this work, we provide universally testable guarantees with respect to the class of nice distributions with bounded Poincare constant (see Definition 2.4 below).

**Definition 2.4** (Poincare Distributions).: For a given value \(>0\), we say that a distribution over \(^{d}\) is \(\)-Poincare, if \((f())[\| f()\| _{2}^{2}]\) for any differentiable function \(f:^{d}\).

Although it is not clear whether one can efficiently obtain testable guarantees for the problem of learning noisy halfspaces under nice marginals (which is known to be an efficiently solvable problem in the non-testable setting ), by restricting our attention to nice distributions that, additionally, have bounded Poincare constant, we obtain efficient learning results, even in the universally testable setting. Our results are strictly stronger than the ones in , since we capture isotropic strongly log-concave distributions universally, due to Proposition 2.3 and the fact that strongly log-concave distributions are also Poincare, as per Proposition 2.5 below.

**Proposition 2.5** (Strongly Log-Concave Distributions are Poincare, [14, Proposition 10.1]).: _Any \(\)-strongly log-concave distribution is \(\)-Poincare._

Furthermore, under a long-standing conjecture about the geometry of convex bodies , our results capture the family of all isotropic log-concave distributions.

**Conjecture 2.6** (Kannan-Lovasz-Simonovits Conjecture  reformulation from ).: _There is a universal constant \(>0\) for which any isotropic log-concave distribution is \(\)-Poincare._

## 3 Universal Testers

In this section, we present two basic testers that constitute the basic building blocks of the universal tester-learners we provide in the next section. The testers in this section might be of independent interest and their appeal is that they succeed even when the distribution in their input is unspecified up to certain bounds on a number of its statistics. In fact, the family of distributions for which each such tester succeeds is of infinite size, even non-parametric.

### Universal Tester for Bounding Local Halfspace Disagreement

First, we present a universal tester that checks, given a parameter vector \(\), whether a set of samples \(S\) is such that bounding the angular distance of \(\) from an optimum parameter vector, implies that the corresponding halfspace disagrees with the (unknown) optimum halfspace only on a bounded fraction of points in \(S\). This property ensures that if \(\) is close to the optimum parameter vector, then it is also an approximate empirical risk minimizer. The tester universally accepts samples from nice distributions with high probability (Definition 2.1).

**Lemma 3.1** (Universally Testable Bound for Local Halfspace Disagreement).: _Let \(D_{}\) be a distribution over \(^{d}\{ 1\}\), \(^{d-1}\), \((0,/4]\), \( 1\) and \((0,1)\). Then, for a sufficiently large constant \(C\), there is a tester that given \(\), \(\), \(\) and a set \(S\) of samples from \(D_{}\) with size at least \(C(}{ 2})\), runs in time \((d,,)\) and satisfies the following specifications:_

1. _If the tester accepts_ \(S\)_, then for every unit vector_ \(^{}^{n}\) _satisfying_ \((,^{})\) _we have_ \[ S}{}[(^{ },)(, )] C^{C}\]_._
2. _If the distribution_ \(D_{}\) _is_ \(\)_-nice, the tester accepts_ \(S\) _with probability_ \(1-\)_._

The proof of Lemma 3.1 simplifies and improves the proof of a similar but weaker result in  (see their Proposition 4.5). The initial tester exploited the observation that the probability of disagreement between two halfspaces can be upper bounded by a sum of products, where each product has two terms: one corresponding to the probability of falling in a (known) strip orthogonal to \(\) and one corresponding to the probability of having large enough inner product with some unknown vector orthogonal to \(\), conditioned in the (known) strip. The first term can be controlled by estimating the probability of falling in a (known) strip, while the second follows by Chebyshev's inequality, after estimating the largest eigenvalue of the covariance matrix conditioned in the known strip. This approach introduces a number of complications, including the fact that conditioning requires rejection sampling, which, in turn requires a lower bound on the probability of falling inside each strip. We propose a simpler tester that controls all of the terms of the sum simultaneously by estimating the largest eigenvalue of a single covariance matrix (without conditioning). Upper and lower bounds on the eigenvalues of random symmetric matrices can be universally tested with testers that are guaranteed to accept when the elements of the matrix have bounded second moments (spectral tester of Proposition A.2). We present our full proof in Appendix B.1.

### Universally Testable Weak Anti-Concentration

We now provide an important universal tester, which ensures that for a given vector \(\), a sample set \(S\) and any unknown unit vector \(\) orthogonal to \(\), among the samples falling within a (known) strip orthogonal to \(\), at least a constant fraction is absolutely correlated with \(\) by a constant. In other words, the tester ensures that the conditional empirical distribution is weakly anti-concentrated in every direction. The tester universally accepts nice distributions that have bounded Poincare constant.

**Lemma 3.2** (Universally Testable Weak Anti-Concentration).: _Let \(D\) be a distribution over \(^{d}\). Then, there is a universal constant \(C>0\) and a tester that given a unit vector \(^{d}\), \((0,1)\), \(>0\), \( 1\), \(\) and a set \(S\) of i.i.d. samples from \(D\) with size at least \(C}{^{2}}(d)^{C}\), runs in time \((d,,,,())\) and satisfies the following specifications_

1. _If the tester accepts_ \(S\)_, then for any unit vector_ \(^{d}\) _with_ \(,=0\) _we have_ \[ S}{}|, |}\ |,|^{4}}\] 2. _If_ \(D\) _is_ \(\)_-Poincare and_ \(\)_-nice, then the tester accepts_ \(S\) _with probability at least_ \(1-\)_._

The proof of Lemma 3.2 is based on a simple fact from probability that is true for any non-negative random variable and ensures that the mass assigned to the tails is lower bounded by the ratio of the square of its expectation to the second moment.

**Proposition 3.3** (Paley-Zygmund Inequality).: _For any non-negative random variable \(Z\), we have_

\[[Z>[Z]/2][Z]^{2}}{ [Z^{2}]}\]

In the special case where \(Z\) follows the distribution of \(,^{2}\) conditioned on \(|,|\) for some unitary orthogonal vectors \(,\), some \(>0\) and some random variable \(\) whose distribution is, say, \(1\)-nice (see Definition 2.1), one can show that \([Z]\) is lower bounded by a constant and \([Z^{2}]\) is upper bounded by another constant, so \(Z\) assigns a non-trivial mass to a set that is bounded away from zero. This property is useful in the context of learning noisy halfspaces, as we show in the following section (see Proposition 4.2 and Lemma 4.3). However, testing algorithms that check whether such a property holds for given \(\) and \(\), are guaranteed to succeed when the marginal distribution has, additionally, bounded Poincare constant. The main part of the proof that requires a bounded Poincare constant, is testing whether \([Z^{2}]\) is bounded uniformly over the set of unit vectors \(\) orthogonal to \(\), since \(Z^{2}=,^{4}\), where \(\) is unknown. We use the following result from .

**Proposition 3.4** (Certifiable Hypercontractivity of Poincare Distributions, Theorem 4.1 in ).: _Let \((0,1)\), \(>0\) and let \(D\) be a \(\)-Poincare distribution over \(^{d}\). Let \(S\) be a set of independent samples from \(D\) with size at least \((2d(4d/))^{4}\). Consider the constrained maximization problem_

\[_{\|\|_{2}=1} S}{}[ ,^{4}]\] (3.1)_Then, the optimum solution of the degree-4 sum-of-squares relaxation of the problem (3.1) has value at most \(C^{4}\) for some universal constant \(C\), with probability at least \(1-\) over the sample \(S\)._

Using Proposition 3.4, we are able to provide a universal tester for bounding the empirical fourth moments. The tester solves an appropriate SDP relaxation of the (hard) problem  of finding the direction with maximum fourth moment and is guaranteed to succeed if \(\) has Poincare parameter bounded by a known value.

**Proposition 3.5** (Hypercontractivity Tester).: _Let \(D\) be a distribution over \(^{d}\). Then, there is a tester that given \((0,1)\), \(>0\) and a set \(S\) of i.i.d. samples from \(D\) with size at least \((2d(4d/))^{4}\), runs in time \((d,,)\) and satisfies the following specifications_

* _If the tester accepts_ \(S\)_, then for any unit vector_ \(^{d}\) _we have_ \[*{}_{ S}[, ^{4}] C^{4}\,,C\]
* _If the distribution_ \(D\) _is_ \(\)_-Poincare, then the tester accepts_ \(S\) _with probability at least_ \(1-\)_._

Proof.: The tester does the following:

1. [leftmargin=*]
2. Solves a degree-\(4\) sum-of-squares relaxation of problem (3.1) up to accuracy \(^{4}\). (For a formal definition of the relaxed problem, see Problem (2.3) in .)
3. If the solution has value larger than \((C-1)^{4}\), then **reject**. Otherwise **accept**.

The computational complexity of the tester is \((|S|,d,)\), since the problem it solves can be written as a semidefinite program .

If the tester accepts \(S\), then we know that the optimal solution of the relaxed problem is at most \(C^{4}\) and we also know that any solution of the initial problem (3.1) has value at most equal to the value of the relaxation. Therefore \(*{}[,^{4}] C ^{4}\), for any \(^{d-1}\).

On the other hand, if the true distribution \(D\) is \(\)-Poincare, then, with probability at least \(1-\), we have that the solution found in step 3.2 has, with probability at least \(1-\), value at most \(C^{}^{4}\) for some universal constant \(C^{}\), due to Proposition 3.4. In order to ensure that the tester will accept with probability at least \(1-\), it suffices to pick \(C=C^{}+1\). 

We provide the full proof of Lemma 3.2, in Appendix B.2. The tests we perform include a spectral tester that accepts with high probability when the distribution of \(\) is nice (similar to the spectral tester used for Lemma 3.1), a tester of the probability that \(|,|\) and the hypercontractivity tester of Proposition 3.5.

## 4 Universal Tester-Learners for Halfspaces

In this section, we present our main result on universally testable learning of halfspaces.

**Theorem 4.1** (Efficient Universal Tester-Learner for Halfspaces).: _Let \(_{}\) be any distribution over \(^{d}\{ 1\}\). Let \(\) be the class of origin centered halfspaces in \(^{d}\). Then, for any \( 1\), \(>0\), \(>0\) and \((0,1)\), there exists an universal tester-learner for \(\) w.r.t. the class of \(\)-nice and \(\)-Poincare marginals up to error \(()(1+^{4})+\), where \(=_{^{d-1}}_{D_{ }}[y(,)]\), and error probability at most \(\), using a number of samples and running time \((d,,,,)\). Moreover, if the noise is Massart with given rate \(<1/2\), then the algorithm achieves error \(+\) with time and sample complexity \((d,,,,, )\)._

Our proof follows a surrogate loss minimization approach that has been used for classical learning of noisy halfspaces  as well as classical (non-universal) testable learning . In particular, the algorithm runs Projected Stochastic Gradient Descent (see A.5) on a surrogate loss whose stationary points are shown to be close to optimum parameter vectors under certain distributional assumptions. In the regular testable learning setting, given a stationary point, the above property can be tested with respect to any (fixed and known) target strongly log-concave marginal as shown by . For such a stationary point, more tests are used in order to ensure bounds on local halfspace disagreement. We provide some delicate refinements of the proofs in  that enable us to substitute their testers with the universal testers of Section 3.

We use the following surrogate loss function which was also used in .

\[_{}(;D_{})=,) D_{}}{} _{}-y,}{\|\|_{2}},\] (4.1)

In Equation (4.1), the function \(_{}\) is a smoothed version of the step function as in Proposition A.4.

In order to analyze the properties of the stationary points of the surrogate loss, we provide the following refinement of results implicit in . We show that the gradient of the surrogate loss is lower bounded by the difference between certain quantities that are controlled by the marginal distribution (see Figure 1). We stress that we do not use any assumptions for the marginal distribution in this step. Prior work included similar bounds, but the corresponding quantities were different. We need to be more precise and provide the following result, whose proof is based on two dimensional geometry and can be found in Appendix C.

**Proposition 4.2** (Modification from ).: _For a distribution \(D_{}\) over \(^{d}\{ 1\}\) let \(\) be the minimum error achieved by some origin-centered halfspace and \(^{*}^{d-1}\) a corresponding vector. Consider \(_{}\) as in Equation (4.1) for \(>0\) and let \(<1/2\). Let \(^{d-1}\) with \((,^{*})=<\) and \((,^{*})\) such that \(,=0\) and \(,^{*}<0\). Then, for some universal constant \(C>0\) and any \(\) we have \(\|_{}_{}(;D_{})\|_{2} A_{1}-A_{2}-A_{3}\), where_

\[A_{1} =[| ,|\;\;\;\;|, |]\] \[A_{2} =[|, |]\;\;\;\;A_{3}=}, ^{2}_{\{|,| \}}}\]

_Moreover, if the noise is Massart with rate \(\), then \(\|_{}_{}(;D_{})\|_{2}(1-2)A_{1}-A_{2}\)._

If the marginal distribution is nice, then the quantities \(A_{1},A_{2}\) and \(A_{3}\) are such that \(\) can be chosen accordingly so that stationary points of the surrogate loss (or their inverses) are close to some optimum vector (see Proposition A.3 for properties of nice distributions). We use some simple tests (e.g., estimate the probability of falling in a strip, \([|,|/2]\) and appropriate spectral testers) as well as our universal tester for weak anti-concentration (see 3.2) to establish bounds on quantities \(A_{1},A_{2}\) and \(A_{3}\) which ensure that the desired property holds for a given vector \(\), under no distributional assumptions. The tester in the following result universally accepts nice distributions with bounded Poincare parameter. The formal proof can be found in Appendix C.2.

**Lemma 4.3** (Universally Testable Structure of Surrogate Loss).: _Let \(D_{}\) be any distribution over \(^{d}\{ 1\}\). Consider \(_{}\) as in Equation (4.1). Then, there is a universal constant \(C>0\) and a tester that given a unit vector \(^{d}\), \((0,1)\), \(<1/2\), \(>0\), \( 1\), \(}\) and a set \(S\) of i.i.d. samples from \(D_{}\) with size at least \(C}{^{2}}(d)^{C}\), runs in time \((d,,,,( {}))\) and satisfies the following specifications_

* _If the tester accepts_ \(S\)_, then, the following statements are true for the minimum error_ \(_{S}\) _achieved by some origin-centered halfspace on_ \(S\) _and the optimum vector_ \(_{S}^{*}^{d-1}\)__
* _If the noise is Massart with associated rate_ \(\) _and_ \(\|_{}_{}(;S)\|_{2}^{4}}\) _then either_ \((,_{S}^{*})(1+^{4}) }{1-2}\) _or_ \((-,_{S}^{*})(1+^{4}) }{1-2}\)_._
* _If the noise is adversarial with_ \(_{S}}\) _and_ \(\|_{}_{}(;S)\|_{2}<^{4}}\) _then either_ \((,_{S}^{*}) C^{C}(1+^{4}) \) _or_ \((-,_{S}^{*}) C^{C}(1+^{4}) \)_._
* _If the marginal_ \(D_{}\) _is_ \(\)_-nice and_ \(\)_-Poincare, then the tester accepts_ \(S\) _with probability at least_ \(1-\)_._

We now give the algorithm for \( 1/3\) since we can reduce the probability of failure with repetition (repeat \(O()\) times, accept if the rate of acceptance is \((1)\) and output the halfspace achieving the minimum test error among the halfspaces returned).

The algorithm receives \( 1\), \(>0\), \(>0\) and \((0,1/2)\{1\}\) (say \(=1\) when we are in the agnostic case) and does the following for some appropriately large universal constants \(C_{1},C_{2}>0\).

1. First, initialize \(E=^{C_{1}}}\), and let \(\) be a list of real numbers and \(A\) be a positive real number, where \(\) and \(A\) are defined as follows. If \(=1\), then \(\) is an \(^{C_{1}}}\)-cover of the interval \([0,^{C_{1}}}]\) and \(A=^{C_{1}}^{4}}\). Otherwise, let \(=\){\(^{C_{1}}(1+^{4})}\)} and \(A=^{C_{1}}^{4}}\).
2. Draw a set \(S_{1}\) of \(C_{2}()^{C_{2}}\) i.i.d. samples from \(D_{}\) and run PSGD, as specified in Proposition A.5 with \( A\), \(}\) on the loss \(_{}\) for each \(\).
3. Form a list \(L\) with all the pairs of the form \((,)\) where \(^{d-1}\) is some iterate of the PSGD subroutine performed on \(_{}\).
4. Draw a fresh set \(S_{2}\) of \(C_{2}()^{C_{2}}\) i.i.d. samples from \(D_{}\) and compute for each \((,) L\) the value \(\|_{}_{}(;S_{2})\|_{2}\). If, for some \(\), \(\|_{}_{}(;S_{2})\|_{2}>A\) for all \((,) L\), then **reject**.
5. Update \(L\) by keeping for each \(\) only one pair of the form \((,)\) for which we have \(\|_{}_{}(;S_{2})\|_{2} A\).
6. Run the following tests for each \((,) L\). (This will ensure that part (a) of Lemma 4.3 holds for each of the elements of \(L\), i.e., that any stationary point of the loss \(_{}\) that lies in \(L\) is angularly close to the empirical risk minimizer2.). 7. Set \(=)}{A^{4}}\), and run the following tests for each pair of the form \((,)\) and \((-,)\) where \((,) L\). (This will ensure that part (a) of Lemma 3.1 is activated, i.e., that the distance of a vector from the empirical risk minimizer is an accurate proxy for the error of the corresponding halfspace.) * If \(_{(,y) S_{2}}[|,| ]>C_{1}^{C_{1}}\) then **reject**. * Compute the \((d-1)(d-1)\) matrix \(M_{S_{2}}\) as follows:4 
This concludes the algorithm. The full proof of Theorem 4.1 may be found in Appendix C.3.