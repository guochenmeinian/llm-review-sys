# Cell ontology guided transcriptome foundation model

Xinyu Yuan1,2, Zhihao Zhan1,2, Zuobai Zhang1,2, Manqi Zhou4

Jianan Zhao1,2, Boyu Han3, Yue Li3,3,*, Jian Tang1,5,6,*

###### Abstract

Transcriptome foundation models (TFMs) hold great promises of deciphering the transcriptomic language that dictate diverse cell functions by self-supervised learning on large-scale single-cell gene expression data, and ultimately unraveling the complex mechanisms of human diseases. However, current TFMs treat cells as independent samples and ignore the taxonomic relationships between cell types, which are available in cell ontology graphs. We argue that effectively leveraging this ontology information during the TFM pre-training can improve learning biologically meaningful gene co-expression patterns while preserving TFM as a general purpose foundation model for downstream zero-shot and fine-tuning tasks. To this end, we present single cell, **Cell**-ontology guided TFM (scCello). We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training. The novel loss component guide scCello to learn the cell-type-specific representation and the structural relation between cell types from the cell ontology graph, respectively. We pre-trained scCello on 22 million cells from CellxGene database leveraging their cell-type labels mapped to the cell ontology graph from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type-specific marker genes, and cancer drug responses. Source code and model weights are available at https://github.com/DeepGraphLearning/scCello.

## 1 Introduction

Cells are basic units of all living organisms. Deciphering diverse cell functions through gene expression is a long-standing challenge in life science and yet the essential path towards precision and personalized medicine. In this context, single-cell RNA sequencing (scRNA-seq) has emerged as a pivotal technique to measure the gene expression in individual cells. The vast amount of publicly available scRNA-seq data offers a rich transcriptomic data source  for learning cell representations towards various research applications, such as cancer therapy  and drug discovery .

Recently, several _Transcriptome Foundation Models_ (TFMs) were developed to improve cell representation learning. They mainly utilize pre-training methods analogous to natural language processing like masked token prediction, treating genes as "tokens" and cells as "sentences" . However, the existing TFMs treat cells as independent samples and ignore their cell-type lineages. On the other hand, prior knowledge of the taxonomic relationships of cell types has been made available through the cell ontology graph by Open Biological and Biomedical Ontology Foundry . Effectively leveraging the ontology knowledge can improve the quality of the pre-training on large-scale scRNA-seq atlases, which are heterogeneous and encompass hundreds of cell types. This can be done by training the TFM to recognize the inherent ontology relationships among cell types, therebyrefining the cell representations. For instance, "mature \(\)-\(\) T cell" should be closer to "mature T cells" compared to more general term "T cells" and farther from neurons and astrocytes from the brain (e.g., Tab. 7).

To capture this intuition, we propose scCello, a **s**ingle **c**ell, **Cell-ontology** guided TFM. scCello learns cell representation by integrating cell type information and cellular ontology relationships into its pre-training framework. scCello's pre-training framework is structured with three levels of objectives: (1) **gene level**: a masked token prediction loss to learn gene co-expression patterns, enriching the understanding of gene interactions (Sec. 2.2); (2) **intra-cellular level**: an ontology-based cell-type coherence loss to encourage cell representations of the same cell type to aggregate, prompting consistency between cells and their types (Sec. 2.3); and (3) **inter-cellular level**: a relational alignment loss to guide the cell representation learning by consulting the cell-type lineage from the cell ontology graph (Sec. 2.4)..

We demonstrate the generalizability and transferability of scCello on 22 million cells from CellxGene. For model generalization, we observe that scCello excels on cell type identification across all datasets in both zero-shot setting (i.e., directly using the pre-trained model) (Sec. 4.2.1) and fine-tuning setting (Sec. 4.2.2). In particular, scCello accurately classifies novel cell types by leveraging the ontology graph structure (Sec. 4.3). For transferability, scCello demonstrates competitive performances in predicting cell-type-specific marker genes (Sec. 4.4) and cancer drug responses (Sec. 4.5). Additionally, scCello is robust against batch effects (Sec. 4.6). Finally, we validate our contribution via ablation study (Sec. 4.7).

## 2 Method

Fig. 1 illustrates an overview of scCello. We present the details of individual components below.

### Data Preprocessing

Cell ontology graph.Cell ontology is a widely used metadata schema for standard cell type annotations . We downloaded the ontology from Open Biological and Biomedical Ontology Foundry (https://obofoundry.org/). It is structured as an unweighted directed acyclic graph \(=(,)\), where each node \(v\) corresponds to a distinct cell type and each directed edge \((u,v)\) denotes a hierarchical lineage relationship of the form "is a subtype of" between cell types (Fig. 0(a)). To accurately represent the inherently symmetric "being biologically similar" relationship between cell types, the directed graph was transformed into an undirected one for subsequent calculation of cellular ontology relationships in Sec. 2.4.

scRNA-seq data.The scRNA-seq data were downloaded from CellxGene. After the preprocessing (App. B), we obtained 22 million cells. Each single-cell transcriptome is represented by a sequence of tuples, each containing genes and their expression counts.1 Each sequence was then ordered by the rank of the gene expression values , akin to the sequential ordering of natural languages. Given a batch of \(B\) cells, each cell \(i\{1,,B\}\) was assigned a cell type ontology identifier \(c_{i}\) from the CellxGene database, to enable mapping between cell and cell ontology.

### Masked Gene Prediction

Same as BERT , scCello predicts a randomly masked gene token in each cell based on its surrounding context in the sequence. This objective \(_{}\) aims to learn the dynamic gene co-expression network.

### Intra-Cellular Ontology Coherence

A straightforward approach to encourage learning the cell representations that are coherent to the cell type labels is to apply cross-entropy loss for supervised cell type classification. However, this approach is limited in learning cell representation for the foundation model. Instead, we employed a supervised contrastive loss as our objective \(_{}\), which directly optimizes the TFM rather than merely learning through the linear classifier:

\[_{}=-_{i=1}^{B}(_{i}^{T}_{c_{i}}/)}{(_{i}^{T}_{c_{i}}/)+_{j=1,j i}^{B} (_{i}^{T}_{c_{j}}/)}).\] (1)

where \(_{i}\) and \(_{c_{i}}\) denote the latent representation of cell \(i\) and cell type \(c_{i}\), respectively.

This supervised contrastive loss pulls representations of the same class (positives) and repels representations of different classes (negatives). It often leads to representations that are at least as discriminative as the cross-entropy loss . To reduce the degrees of freedom available for TFM optimization, we introduce a regularization term \(_{}\):

\[_{}=_{i=1}^{B}||(_{c_{i}})-_{i}||_{2}^{2},\] (2)

Figure 1: (a) Cell ontology graph describes taxonomic relationships between cell types. (b) Each cell in scRNA-seq data is represented by gene sequences, and associated with a cell type ontology identifier. (c) The pre-training framework of scCello is structured with three levels of objectives: gene-level masked gene prediction, intra-cellular level cell type coherence and inter-cellular level ontology alignment. For example, as shown in panel b, cells 1, 2, and 3 are labelled with cell type A, B and C. The intra-cellular cell type coherence loss encourages alignment of embedding \(_{1}\) with \(_{A}\), \(_{2}\) with \(_{B}\), and \(_{3}\) with \(_{C}\). The inter-cellular level ontology alignment loss encourages representational learning of cell similarities \(_{i}^{}_{j}\) between cell \(i\) and \(j\) to be consistent to the similarity of their corresponding cell types \(sim(c_{i},c_{j})\) based on the ontology relationships. (d) Downstream tasks enabled by scCello and demonstrated in the study.

where the linear layer is shared across all cells and cell types. Thereby, it constrains the cell type representation space to be an affine transformation of the cell representation space.

### Inter-Cellular Relational Alignment

To encourage TFMs to learn inter-cellular ontology relationships, scCello forces cell representations to truthfully reflect the pairwise node structural similarity derived from the cell ontology graph, using a relational alignment objective. This objective constitutes the most important part of scCello.

Ontology relationships.To effectively quantify ontology relationships between cell types from the ontology graph, scCello estimates pairwise node structural similarities as proxies using Personalized PageRank (PPR) . PPR is a graph learning algorithm. The PPR score \((u,v)\) estimates the probability for a random walk. It starts from a given target node \(u\) and terminates at another node \(v\). Importantly, this is a context-sensitive structural similarity measure that accounts both direct connections and broader subgraph patterns . It also provides robustness against variations in global network structures, such as variable node degrees and clustering coefficients . To improve robustness (as justified in App. A), we transform \(()\) through a non-linear function to derive the structural similarities \(()\) as ontology relationships tunable by a hyper-parameter threshold \(s\):

\[(u,v)=_{2}((u,v)}{s}+1) ,&(u,v) s$}\\ 1,&.\] (3)

Relational alignment.Cells with closely related cell types tend to be more similar than those with distinct cell types. This observation guides scCello to align the distances between cell representations _w.r.t._ a target cell, with their structural similarities \(()\) (as shown in Fig. 0(c)). Specifically, given a batch of \(B\) cells, if we consider a target cell \(i\) and another cell in the batch \(j i\), the representation distance \(_{i}^{T}_{j}\) should reflect their structural similarity \((c_{i},c_{j})\). Accordingly, a negative sample set \(_{i,j}=\{k|(c_{i},c_{j})>(c_{i},c_{k}),1 k  B\}\) can be produced, where cell pair \((i,k)\) are considered less similar to the cell pair \((i,j)\) and should be contrasted against in the representation space using the objective \(_{}\):

\[_{}=-_{i=1}^{B}_{j=1,j i}^{B}( _{i}^{T}_{j}/)}{(_{i}^{T}_{j}/) +_{k_{i,j}}(_{i}^{T}_{k}/)}).\] (4)

Notably, ancestor cell types, which can reach the target cell type via the directed "is a subtype of" edge on the ontology graph, are structurally distant from the target cell type. Despite being distant, they fall into the same, broader cell type category. Contrasting cells associated with these distant ancestor cell types with the target cell is counter-intuitive. Therefore, scCello explicitly excludes such cells from the negative sample set, avoiding inappropriately pushing away biologically similar cells. This enhances scCello's capability to discern subtle similarities and differences within the cell types.

### Overall Pre-training Objective

During pre-training, we seek to minimize the loss functions of all pre-training tasks simultaneously:

\[^{*}*{arg\,min}_{}\ \ _{ }+_{}+_{}+ _{}\] (5)

where \(\) denotes all learnable parameters in scCello, which adopts transformer stacks as model backbones. We state the detailed information of model architectures in App. D.

## 3 Related Work

The rapid growth of scRNA-seq datasets has opened new avenues for constructing TFMs, enabling transfer learning across various biological downstream tasks. Initial efforts, such as scBERT , Exceiver  and Geneformer , borrows the concept of masked language modeling  from natural language processing (NLP) domain for pre-training, by treating cells as sentences and genes as tokens. Concurrently, tGPT  and scGPT  explored generative modeling , and CellLM adapted the idea of contrastive learning . Following the concept of "scaling" towards emergent ability  in NLP, scFoundation  proposes the largest foundation model at the time in terms of model size and pre-training data size; scHyena  scales modeling context window size to the full length of scRNA-seq data with Hyena operator  instead of conventionally used transformers. scTab  is the first to explore large-scale supervised learning mechanism for scRNA-seq pre-training, and is capable of annotating unseen tissue cells for real-world applications. Moreover, SCimilarity  and UCE  focus on developing a unified latent space as a large-scale reference atlas for querying new cells. Yet, these TFMs mainly treat cells as independent samples during training and ignore their biological ontology relationships. scCello bridges this gap by incorporating cell type relationships derived from the cell ontology graph into TFM pre-training. This strengthens TFMs' model generalization and transferability capability, as shown in Sec. 4.

## 4 Experiments

As an overview, the following experiments show that, **(1)** scCello can generalize to unseen cells, and to more difficult settings, such as cells of unseen cell types, tissues, and donors (Sec. 4.2.1); **(2)** scCello can benefit from fine-tuning on target datasets (Sec. 4.2.2); **(3)** the structural similarity embedded in scCello helps to classify novel cell types in a zero-shot manner (Sec. 4.3); **(4)** scCello effectively transfers to different downstream tasks (Sec. 4.4 and Sec. 4.5); **(5)** scCello is robust to batch effects that arise from different experimental conditions (Sec. 4.6); **(6)** Each loss component in Eqn. 5 is beneficial to scCello (Sec. 4.7). For every table reported, we used **bold** to highlight the best performance and results within 0.005 difference from the best. We used underlining to denote the second-best performances. For all metrics, \(\) indicates the higher the better.

### Setups

Pre-training and downstream datasets.We collected a large pre-training dataset consisting of 22 million cells along with downstream datasets. In particular, we generated one in-distribution (ID) and six out-of-distribution (OOD) datasets (App. B). The ID dataset is denoted as \(D^{id}\). For the OOD setting, we introduced three scenarios: unseen cell types (\(\{D^{ct}_{i}\}_{i=1}^{2}\)), unseen cell **tis**uses (\(\{D^{ts}_{i}\}_{i=1}^{2}\)), and unseen **don**ors (\(\{D^{dn}_{i}\}_{i=1}^{2}\)). Each scenario has two datasets. Notably, the OOD donor setting presents more realistic challenges than ID and other OOD settings because of the potential batch effects in the test donors.

Pre-training configurations.An Adam optimizer  (learning rate: \(0.001\), weight decay: \(0.001\), warm-up steps: \(3,333\)) was used to train the scCello for \(40,000\) steps on 4 NVIDIA A100 GPUs on Compute Canada. We used \(192\) for batch size. More details are introduced in App. D.

Baselines.Across all downstream tasks, scCello is benchmarked with leading open-source large-scale TFMs: Geneforimer , scGPT , scTab , UCE , and three TFM ablations. We also implemented ablated versions of scCello that only differ in the pre-training objectives from scCello: scCello using only the masked gene prediction loss (denoted as MGP), scCello using only the cell type supervised classification (denoted as Sup), and scCello using only the two losses (denoted as MGP+Sup). The three ablated TFMs provide a reference to isolate the effect of implementation details and training configurations. For each task, we also selected state-of-the-art non-TFM methods for fair comparison.

Downstream metrics.We evaluated the 3 tasks by the following metrics. (1) Clustering metrics include normalized mutual information (NMI), adjusted rand index (ARI), average silhouette width (ASW), and the average of the 3 scores (AvgBio) to assess both between-cluster separation and within-cluster closeness . The batch integration task (Sec. 4.6) is evaluated by \(_{b}\), graph connectivity (GraphConn) and their average (AvgBatch), along with an overall score (Overall = \(0.6+0.4\)) to balance biological relevance and batch consistency following . (2) Classification metrics include accuracy (Acc), Macro F1 and area under the ROC curve (AUROC) . (3) Regression task metrics include Pearson correlation coefficient score (PCC) . Details for each metric were provided in App. E.1.

[MISSING_PAGE_FAIL:6]

### Novel Cell Type Classification

Novel cell type classification aims to label cells of unseen cell types without further fine-tuning. This task is useful for annotating completely new scRNA-seq datasets but infeasible for most of the supervised methods that solely rely on the labels observed in the training data [8; 31; 68]. Leveraging the cell ontology graph that comprises the lineage relations among all of the known cell types, scCello makes this task feasible.

Setup.Our goal is to classify new query cells into "novel cell types" not seen during pre-training. To do this, we generate representations for both query cells and novel cell types, using similarity measures for classification. This process involves utilizing similarities between TFM-derived representations for the former and biological relationships from the cell ontology graph for the later. Details were described in App. E.3.

We benchmarked all TFMs and evaluated them on OOD cell type datasets \(D_{1}^{ct}\) and \(D_{2}^{ct}\). We increased the difficulty of this task by the number of novel cell types (#Cell Types) that exist among the query cells. Specifically, we simulated five difficulty levels, with the number of novel cell types ranging from 10% to 100% of the total cell types. To assess the variance of the performance, we randomly sampled cell type combinations 20 times at each level.

OOD generalization.In Fig. 2, scCello led other TFMs by a large margin, achieving up to 76.8% Acc to classify 9 novel cell types (i.e., 10% of the total heldout cell types) and 33.5% Acc to classify up to 87 novel cell types (i.e., 100% of the total heldout cell types) (Tab. 16 and Tab. 17). These results show a significant leap from the existing TFMs, which either do not work or only work for annotating a handful of novel types [68; 45; 66].

### Marker Gene Prediction

Cell-type-specific genes, or marker genes, are highly expressed in a specific cell type but exhibit low expression in others. These genes play a crucial role in delineating cell functions in diverse tissue contexts. Identifying marker genes in less characterized cell types is an ongoing challenge .

Setup.We sought to assess whether the pre-trained TFMs can discriminate marker from non-marker genes for any cell type without any supervised fine-tuning. This zero-shot experiment evaluates whether the TFM is able to learn biologically meaningful gene co-expression patterns without supervision. For each cell, we quantified the marker gene potential of each gene by the changes in TFM-generated cell representations after _in-silico_ knockout of the target gene (details in App. E.4). Here we assume that the larger the change the higher the marker gene potential. We discussed the

    &  &  &  \\   &  &  &  & scFPT &  &  &  &  &  \\  PCC \(\) & 0.854 & 0.882 & 0.911 & **0.919** & 0.913 & **0.922** & 0.872 & 0.915 & 0.916 & **0.917** \\   

Table 4: Cancer drug response prediction: a regression task to predict the \(IC_{50}\) values of drugs.

    &  &  \\   & Acc \(\) & Macro F1 \(\) & AvBio \(\) \\   \\  scANVI & 0.763 & 0.490 & 0.568 \\ Scratch & 0.621 & 0.223 & 0.544 \\   \\   Geneformer \\ scGPT \\ scTab \\  } & 0.747 & 0.440 & 0.439 \\ scFPT & 0.712 & 0.344 & 0.477 \\ scTab & 0.778 & 0.373 & 0.606 \\ MGP & 0.722 & 0.287 & 0.607 \\ Sup & 0.812 & 0.363 & 0.659 \\ MGP+Sup & 0.820 & 0.406 & 0.607 \\   \\ 
**scCello** & **0.867** & **0.511** & **0.694** \\   

Table 2: Cell type identification using fine-tuned TFMs. Both the classification and clustering performances on the ID dataset \(D^{id}\) are reported.

    & ^{st}\)} & ^{mk}\)} \\   & AUROC \(\) & AUROC \(\) \\   \\  DET & 0.721 & 0.683 & 0.702 \\   \\   & 0.452 & 0.470 & 0.461 \\ scGPT & 0.385 & 0.387 & 0.386 \\ scTab & 0.672 & 0.727 & 0.700 \\ UCE & 0.500 & 0.500 & 0.500 \\ MGP & 0.579 & 0.629 & 0.604 \\ Sup & 0.699 & 0.693 & 0.696 \\ MGP+Sup & 0.730 & **0.730** & 0.730 \\   \\ 
**scCello** & **0.867** & **0.511** & **0.694** \\    
 **Ontology-Enhanced TFMs** \\   \\   \\   \\   \\   \\   \\   \\   

Table 3: Marker gene prediction, a binary classification task to identify cell-type-specific marker genes.

caveat of this approach in Sec. 5. As test data, we used GSEB6583  (\(D_{1}^{mk}\)) and GSE130148  (\(D_{2}^{mk}\)). We obtained the marker gene labels from CellMarker2  and PanglaoDB . We also compared with a non-TFM method, Differential Expression Tests (DET) .

Zero-shot transferability.In Tab. 3, scCello outperforms other TFMs, improving upon the second-best method by 1.8% in average AUROC. The inclusion of cell label information during pre-training boosts TFM performance, as evidenced by the strong results of scTab, Sup, MGP+Sup and scCello. This is due to the biological correlation between marker genes and cell types. Furthermore, employing cell ontology graphs further improves the prediction accuracy over MGP+Sup.

### Cancer Drug Response Prediction

Developing effective drugs for cancer treatment is challenging due to individual variability in drug responses. Accurately predicting cancer drug responses (CDR) can greatly aid anti-cancer drug development and improve our understanding of cancer biology .

Setup.Following the approach of scFoundation , cell representations were extracted from fixed TFMs and integrated into the DeepCDR  pipeline to estimate the half-maximal inhibitory concentration (\(IC_{50}\)) values of drugs (details in App. E.6). We benchmarked our method against DeepCDR, scFoundation, and other TFM baselines, using the same pre-processed data as DeepCDR.

Zero-shot transferability.In Tab. 4, scCello is among the top 3 along with scGPT and UCE, achieving 7.4% improvement in PCC over the base method DeepCDR. This highlights scCello's transferability in enhancing specialized task-oriented methods. In particular, it can be used as an powerful feature extractor for diverse downstream tasks.

### Batch Integration

The scRNA-seq atlases, assembled from datasets across various labs and conditions, are prone to unwanted technical variations known as batch effects . These effects can significantly affect the generalization ability of TFMs especially because they require pre-training on a massive amount of heterogeneous scRNA-seq data pooled from many studies. Here we sought to evaluate scCello's robustness to batch effects without fine-tuning.

Setup.We adopted the same baselines as in zero-shot cell type clustering (Sec. 4.2.1), and followed the evaluation protocol of scGPT . We evaluated on one ID dataset \(D^{id}\) and six OOD datasets \(D_{i}^{cond}\) (\(cond\{ct,ts,dn\}\), \(i\{1,2\}\)) (see complete results of all metrics in App. E.7).

Robustness to data noise.Fig. 3 shows that scCello excels in 3 out of 7 datasets, and achieves comparable performance on another 3 datasets. The performance is attributable to the use of cell type information as the ablated baseline MGP conferred much lower batch integration score compared to Sup and scCello.

### Ablation Study

Ablation of pre-training losses.Tab. 5 reports the cell type clustering (Sec. 4.2.1) and novel cell type classification (Sec. 4.3) performance of scCello by using full or partial pre-training losses. Removing any of the four losses in Eqn. 5 resulted in decreased performance, corroborating the benefits of the proposed pre-training losses. Notably, removing the inter-cellular ontology relation loss \(_{}\) led to 56.1% and 65.3% decrease in terms of Acc. and Macro F1 on novel cell type classification task, respectively. This shows the upmost importance of the structurally induced loss and ultimately the use of cell ontology graph information.

Parameter efficiency.Tab. 6 demonstrates that scCello is highly parameter-efficient, utilizing up to 60 times fewer parameters than the largest existing TFM, UCE, while still achieving the best average performance rankings across all downstream tasks. With an average performance rank of 1.3, scCello consistently ranks first or near the top in nearly every task.

Visualization.Visualization and analysis of scCello's learned cell representations were presented in App. E.8. In short, biologically similar cell types are closer to each other and farther from those dissimilar ones in the t-SNE 2D space (Fig. 11).

## 5 Discussion and Conclusion

Limitation and future work.The cell ontology is constantly revised and expanded. In the future, we plan to investigate more efficient methods for fine-tuning scCello to enable continual learning of updated ontology, rather than retraining the entire model. Additionally, we aim to scale up the model size of scCello to increase its expressiveness and capacity. For the zero-shot marker gene prediction experiments (Sec. 4.4), one caveat is that our in-silico gene knockout approach also detects essential genes such as housekeeping genes  and transcription factors that are master regulators , which may not necessarily be marker genes. Nonetheless, deletion of these influential genes will also lead to large change of the transcriptome landscape of the cell. We will explore this in future study.

Societal impact.This work proposes a novel cell ontology-guided TFM, scCello, to enhance cell representation learning. On the positive side, once pre-trained, scCello can serve as a foundational model capable of facilitating scientific discoveries across various downstream tasks related to cells and cellular processes. However, on the negative side, the pre-training of scCello requires significant computational resources, potentially resulting in substantial carbon dioxide emissions that could contribute to environmental harm.

Conclusion.The proposed scCello incorporates cell ontology knowledge into its pre-training process by simultaneously modeling at the gene level, intra-cellular level, and inter-cellular level. We constructed a large-scale cell type identification benchmark to evaluate the model's generalization capabilities, both in-distribution and out-of-distribution. Our evaluation demonstrates that scCello also exhibits strong transferability, as evidenced by its performance on other biologically meaningful downstream tasks such as zero-shot novel cell type classification and cell-type-specific marker gene prediction. Foundational models are typically heavy on the parameters for them to have sufficient capacity to learn from unlabeled data from scratch. This limits their usage to only fine-tuning tasks as pre-training them is prohibitive without large compute. Our proposed approach provides an efficient way of leveraging the prior knowledge at the pre-training, which led to much smaller parameter size while achieving performance comparable of the TFMs that are 5-60 times bigger. Together, scCello is a knowledge-informed and general purpose deep learning model that can be fine-tuned

  
**Method** & Perf. Rank & \#Params (M) \\    & 6.3 & 10.3 \\  & 6.2 & 51.3 \\   & 4.2 & **9.7** \\   & 4.8 & 674.7 \\   & 6.7 & 10.3 \\   & 3.3 & 10.4 \\   & 3.2 & 10.9 \\  
**scCello** & **1.3** & 10.7 \\   

Table 6: Overall performance _v.s._

    &  &  \\   & \(D_{}^{}\) & \(D_{}^{}\) & \(D_{}^{}\) & \(D_{}^{}\) & Macro F1 \\   & AvgBio\({}^{}\) & AvgBio\({}^{}\) & Acc\({}^{}\) & Macro F1 \\  Full Loss & 0.786 & 0.643 & 0.335 & 0.150 \\  _w/o_\(_{}\) & 0.774 (\(\)1.5\%) & 0.640 (\(\)0.5\%) & 0.287 (\(\)1.3\%) & 0.131 (\(\)12.7\%) \\ _w/o_\(_{}\) & 0.778 (\(\)1.0\%) & 0.620 (\(\)1.8\%) & 0.147 (\(\)16.1\%) & 0.052 (\(\)0.55.3\%) \\ _w/o_\(_{}\) & 0.730 (\(\)1.7\%) & 0.626 (\(\)2.6\%) & 0.280 (\(\)11.6\%) & 0.118 (\(\)12.3\%) \\ _w/o_\(_{}\) & 0.764 (\(\)2.8\%) & 0.638 (\(\)0.8\%) & 0.296 (\(\)11.6\%) & 0.134 (\(\)10.7\%) \\   

Table 5: Pre-training loss ablation on the cell type clustering and novel cell type classification (_abbr._, “clf.”) tasks.

for a wide array of downstream applications, aiding in the rapid identification of novel cell types, disease-associated genes, and effective cancer drugs.