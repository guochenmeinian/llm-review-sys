# Differentiable Structure Learning with Partial Orders

Taiyu Ban  Lyuzhou Chen  Xiangyu Wang  Xin Wang  Derui Lyu  Huanhuan Chen

University of Science and Technology of China

{banty, clz31415, wz520, drlv}@mail.ustc.edu.cn

{sa312, hchen}@ustc.edu.cn

Corresponding authors.

###### Abstract

Differentiable structure learning is a novel line of causal discovery research that transforms the combinatorial optimization of structural models into a continuous optimization problem. However, the field has lacked feasible methods to integrate partial order constraints, a critical prior information typically used in real-world scenarios, into the differentiable structure learning framework. The main difficulty lies in adapting these constraints, typically suited for the space of total orderings, to the continuous optimization context of structure learning in the graph space. To bridge this gap, this paper formalizes a set of equivalent constraints that map partial orders onto graph spaces and introduces a plug-and-play module for their efficient application. This module preserves the equivalent effect of partial order constraints in the graph space, backed by theoretical validations of correctness and completeness. It significantly enhances the quality of recovered structures while maintaining good efficiency, which learns better structures using 90% fewer samples than the data-based method on a real-world dataset. This result, together with a comprehensive evaluation on synthetic cases, demonstrates our method's ability to effectively improve differentiable structure learning with partial orders.

## 1 Introduction

Learning directed acyclic graph (DAG) structures from observational data is fundamental for causal discovery in scientific research (Opgen-Rhein and Strimmer, 2007; Pearl and others, 2000). Traditionally, it has been approached as a combinatorial optimization problem dominated by independence tests and score-and-search methods (Heinze-Deml _et al._, 2018). Zheng _et al._ (2018) reformed it as a continuous optimization problem through a novel characterization of the acyclicity constraint in a differentiable form. Subsequently, numerous studies have proposed various learner architectures (Yu _et al._, 2019; Zhu _et al._, 2019; Zheng _et al._, 2020), acyclicity characterizations (Yu _et al._, 2019; Ng _et al._, 2022; Bello _et al._, 2022), and optimization techniques (Wei _et al._, 2020; Deng _et al._, 2023a,b) to advance differentiable structure learning.

In practical scenarios of causal discovery, researchers often possess prior knowledge of ordering, such as known gene activation sequences in genetics (Olson, 2006), standard treatment sequences in healthcare management (Denton _et al._, 2007), and sequential seasonal weather patterns in meteorology (Bruffaerts _et al._, 2018). Such prior information can be generally formalized as a set \(=\{(x,y) x,y X\}\) of partial orders, where the binary relation \((x,y)\) represents that variable \(x\) precedes \(y\) in the ordering, denoted as \(x y\). For traditional score-and-search structure learning methods, the constraints of partial orders can reduce the space of total orderings in which the search algorithms2 are usually performed (Teyssier and Koller, 2005). Thus, the prior partial order informs the search process to find more genuine structures, which is essential for practical causal discovery.

However, the application of prior partial orders in the context of differentiable structure learning has not been explored. The main challenge arises from the inapplicability of the scenario, as the continuous optimization of structures is conducted in the graph space, while partial orders are constraints in the ordering space, leading to a misalignment of the hypothesis space. In related studies, Deng _et al._[2023a] use a search strategy to solve the constrained optimization problem in the ordering space, which is not a purely continuous method. Some research applies differentiable structure learning to dynamic Bayesian network (DBN) structure learning [Pamfil _et al._, 2020; Sun _et al._, 2023; Yang _et al._, 2022], which assumes a strict time-series ordering. This strict order prior can be simply implemented in the graph space by freezing the parameters of certain edges (see discussions in Appendix B), which significantly differs from the general partial orders discussed in this paper.

Despite of the misalignment of hypothesis spaces, the partial order constraint in the ordering space has an equivalent form in the graph space captured by path prohibitions [Grimmett and Stirzaker, 2015]. This leads to a feasible way to apply partial orders in differentiable structure learning. However, for a sequential ordering with \(m\) variables, there are \(\) paths to be forbidden in the equivalent constraint. This complexity makes it impractical to develop constraints on path prohibitions individually, leading to substantial computational overhead for long sequential orderings.

To address this issue, we propose an efficient approach that augments the acyclicity constraint to naturally forbid all paths in the equivalent constraint of partial orders \(\). Concretely, we infer the transitive reduction of \(\) and divide it into maximal paths to capture all possible sequential orderings. These paths are then individually added to the adjacency matrix of the acyclicity term, forming an augmented acyclicity constraint. We prove that adherence to this new constraint is equivalent to adherence to partial order constraints. Furthermore, this method efficiently handles long sequential orderings, requiring only one factor to describe a sequential ordering regardless of its length. It is a plug-and-play module that can be easily adapted to various algorithms. Evaluations on both synthetic and real-world data verify its effectiveness. Contributions are listed:

* To the best of our knowledge, this is the first work to discuss the integration of prior constraints of partial orders into the continuous optimization of structure learning. We propose a plug-and-play module enabling the integration of this prior, which is theoretically applicable to all continuous methods in the differentiable structure learning context.
* We address the misalignment between the hypothesis space of differentiable structure learning and partial order constraints by converting them into an equivalent form of path prohibition constraints. By formalizing the continuous characterization of this equivalent constraint, we show its limited practicality in dealing with long sequential orderings.
* To efficiently integrate long sequential orderings, we introduce a novel approach to apply path prohibitions by augmenting the acyclicity constraint with partial orders. We prove the equivalence of this augmented acyclicity constraint to the adherence to partial orders and show its efficiency in dealing with long sequential orderings.

## 2 Notations and Preliminaries

NotationsIn the following illustrations, we denote \(W_{i,}\), \(W_{,j}\), and \(W_{i,j}\) to represent the \(i\)th row, \(j\)th column, and \((i,j)\)th element of a matrix \(W\), respectively. If a matrix symbol includes a subscript, like \(W_{s}\), we represent its elements as \(W_{s,i,j}\). For operations resulting in a matrix, such as \(W_{1}+W_{2}\), we denote elements of the resulting matrix as \((W_{1}+W_{2})_{i,j}\). For simplicity, the symbol \((i,j)\) is used contextually: it can refer to a partial order \(X_{i} X_{j}\) or to a directed edge \((X_{i},X_{j})\) in the graph. Related work and proof of statements can be found in Appendix A and Appendix C, respectively.

Structural equation modelLet \(G\) denote a directed acyclic graph (DAG) with \(d\) nodes, where the vertex set \(V\) corresponds to a set of random variables \(X=\{X_{1},X_{2},,X_{d}\}\), and the edge set \(E(G) V V\) defines the causal relationships among the variables. The structural equation model (SEM) specifies that the value of each variable is determined by a function of its parent variables in \(G\) and an independent noise component:

\[X_{j}=f_{j}(_{j}^{G},z_{j}) \]

where \(_{j}^{G}=\{X_{i} X_{i} X,(X_{i},X_{j}) E\}\) denotes the set of parent variables of \(X_{j}\) in \(G\), and \(z_{j}\) represents noise that is independent across different \(j\). Denoting the structure of \(G\) as a weighted adjacent matrix \(W^{d d}\), where \(W_{i,j} 0\) equals that \((X_{i},X_{j}) E(G)\), we have:

\[X_{j}=f_{j}(W_{,j},X,z_{j}) \]

Given a set of samples \(D^{m d}\) generated from this model with either linear or nonlinear functions in \(\{f_{j}\}\), we next describe the process of learning the structure of \(G\) in a differentiable manner.

Differentiable structure learningThe objective of structure learning is to deduce the DAG structure represented by the weighted adjacency matrix \(W^{d d}\) from the data \(D\) generated by a specific set of functions \(f\). We define all parameters characterizing \(W\) and \(f\) as \(\) and the graph as \(G(W())\), and we formalize the optimization problem of structure learning as follows:

\[_{}(D,f_{}(D))G(W())  \]

where \(\) is the score function, such as the least squares \((D,f_{}(D))=_{i=1}^{m}\|D-f_{}(D)\|_{F }^{2}\)(Loh and Buhlmann, 2014). For clarity, we emphasize the target \(W\) and rewrite it as \(F(W)\). Similarly, the symbol \(G(W())\) denoting the graph is simplified as \(G(W)\). The acyclicity degree of the graph can be characterized by a series of continuous functions for a non-negative matrix \(B\):

\[h(B)=(_{i=1}^{d}c_{i}B^{i}), c_{i}>0 \]

For this acyclicity characterization, Zheng _et al._ (2018) use \(h(B)=(e^{B})-d\), derived from an infinite power series3 of \(B\). Yu _et al._ (2019) suggest a polynomial form \(h(B)=((I+B)^{d}-I)\), and Bello _et al._ (2022) employ a log-determinant function \(h(B)=-(sI-B)+d s\). For \(W^{d d}\), it is common to use \(B=W W\) to ensure the non-negativity of \(B\). Hence, the following mentioned \(h(W)\) actually refers to \(h(W W)\).

**Proposition 1**.: _(Theorem 1 in (Wei et al., 2020)). The directed graph of a non-negative adjacency matrix \(B\) is a DAG if and only if \(h(B)=0\) for any \(h\) defined by (4)._

According to this result, the constraint \(G(W)\) can be implemented by the continuous equality \(h(W)=0\). This transformation converts the structure learning problem into a continuous optimization problem with an equality constraint, formulated as:

\[_{W^{d d}}(W)h(W)=0 \]

Zheng _et al._ (2018) apply the augmented Lagrangian method to solve this problem, a technique widely adopted in subsequent studies. Note that our proposed plug-and-play module does not alter the optimization process; therefore, this aspect is out of scope and not discussed in this paper.

Role of orders in structure learningVariable ordering plays a crucial role in the combinatorial optimization of structure learning. The order-based score-and-search method is a critical research direction in this context. It is founded on the principle that structure learning is no longer NP-hard when the total ordering of variables is known (Teyssier and Koller, 2005). These methods search within the hypothesis space of total orderings to identify the optimal solution within this constraint (Xiang and Kim, 2013; Raskutti and Uhler, 2018; Squires _et al._, 2020; Wang _et al._, 2021; Solus _et al._, 2021; Chen _et al._, 2019; Li and Beek, 2018; Chen _et al._, 2016). They benefit from prior partial orders, which help reduce the space of possible total orderings. However, differentiable structure learning solves Equation (5) in the DAG space, making it inapplicable to directly use partial orders. Therefore, we employ an alternative constraint equivalent to partial orders in the following section.

## 3 Differentiable Structure Learning with Partial Orders

This section introduces the integration of partial order constraints into the continuous DAG optimization framework. First, we convert the constraints from the ordering space into an equivalent form in the DAG space. Next, we discuss the limitations of direct characterizations of these equivalent constraints. Finally, we present an efficient approach to integrate partial order constraints and illustrate its theoretical correctness and completeness.

### Capture partial orders with path prohibition constraints

To begin with, we formally define critical concepts related to partial orders.

**Definition 1** (Partial Order).: _For a set \(S\) of variables, a partial order is a binary relation \(\) on \(S\) which is a subset of \(S S\). For all elements \(x,y,\) and \(z\) in \(S\), the following properties are satisfied:_

_Reflexivity: \(x x\) for every \(x\) in \(S\); Antisymmetry: If \(x y\) and \(y x\), then \(x=y\); Transitivity: If \(x y\) and \(y z\), then \(x z\)._

For the structure, if \(x y\), then \(y\) cannot be the ancestor of \(x\); that is, no directed path exists from \(y\) to \(x\) in the graph. Note that while the partial order relation is transitive, the absence of paths is not. This requires further consideration of the transitive property of orders.

**Definition 2** (Transitive closure).: _For a set \(S\) and a binary relation \( S S\), the transitive closure of \(\), denoted by \(^{+}\), is defined as \(^{+}=_{n=1}^{}^{n}\). \(^{n}\) is defined recursively by: \(^{1}=\), \(^{n+1}=^{n}\), \(=\{(x,z) S S y S(x,y)(y,z)\}\)._

**Remark 1**.: _The transitive closure \(^{+}\) of a set of partial orders \(\) encompasses all orders either directly contained in or inferable through transitivity from \(\)._

Now, we consider the following result from graph theory, which is essential for transforming order constraints into structural constraints.

**Proposition 2**.: _There exists at least one topological sort of DAG \(G\) that satisfies the partial order set \(\) if and only if, for any order \((i,j)\) in \(^{+}\), \(X_{j}\) is not an ancestor of \(X_{i}\) in \(G\)._

With this statement, the structure learning problem with partial orders \(\) can be implemented by its equivalent constraint set of path prohibitions, formalized as:

\[_{W^{d d}}(W)h(W)=0,\ X_{j} X_{i} G(W)(i,j)^{+} \]

where \(X_{j} X_{i} G(W)\) indicates that no directed path exists from \(X_{j}\) to \(X_{i}\) in \(G(W)\). Subsequently, we introduce this constraint's continuous characterization and discuss its limitations.

### Continuous characterization of path prohibitions

This section introduces the continuous characterization of the path prohibition constraint in Equation (6) and the practical difficulties in optimizing it. To clarify the unique challenges when dealing with flexible partial orders, we begin with the case of total orderings.

**Definition 3** (Total Ordering).: _A total ordering is a permutation \(\) of all the variables, with \((i)\) denoting the index of the variable in the \(i\)th position. \(X_{(i)}\) precedes \(X_{(j)}\) if and only if \(i<j\)._

For total ordering, the order relationship between any pair of variables is contained in the transitive closure \(^{+}\). This property allows for a simple implementation of the constraint of \(\) by edge prohibition, as illustrated below:

**Proposition 3**.: _A graph \(G\) is a DAG and satisfies total ordering \(\) if and only if edge \((u,v)\) does not exist in \(G\) for all \((v,u)^{+}\)._

This edge absence constraint on \(G(W)\) can be directly implemented by setting the corresponding parameters in \(W\) to zero, resulting in the following formulation:

\[_{W}(W)W_{(i),(j)}=0i j \]

In this case, structure learning becomes an unconstrained optimization problem, as adherence to total orderings naturally satisfies the DAG constraint. This problem can be solved more efficiently than the original problem with the constraint equality \(h(W)=0\).

Now we consider the flexible partial order constraints. Let \(\) represent a set of partial orders that do not inherently contain cycles within their transitive closure \(^{+}\). Merely forbidding edges that violate \(^{+}\) is insufficient for compliance, as it is possible to _walk_ from a variable to a preceding variable in \(\) through another variable whose order with others is not contained in \(\), such as:

**Example 1**.: _For example, consider four nodes \(1,2,3,4\) with a partial order set \(=\{(1,2),(2,3)\}\). We forbid all inverse edges in \(^{+}\), which are \((2,1),(3,2),(3,1)\). Despite this, directed paths violating the partial order \((1,2)\) can still exist, such as the path \((2,4,1)\). Such paths can be constructed by traversing nodes not in \(\), like node 4 in this case._

Consequently, we must consider the constraint of path prohibitions equivalent to \(\). According to the proof to Proposition 1, the following equality can be used for path prohibition constraints.

**Proposition 4**.: _No directed path \(X_{i} X_{j}\) exists in \(G(W)\) if and only if \((_{l=1}^{d}(W W)^{l})_{i,j}=0\)._

With this statement, we can formalize the optimization problem in Equation (6) as follows:

\[_{W}(W)h(W)=0,\,p(W,)=0 \] \[p(W,)=_{(i,j)^{+}}(_{l=1}^{d}(W W) ^{l})_{j,i} \]

**Remark 2**.: _A significant difficulty of the optimization problem formulated in Equation (8a) is its steep decline in training efficiency as the complexity of partial orders increases. The penalty term \(p(W,)\), as defined by Equation (8b), includes a term for each order in \(^{+}\), directly impacting the computational cost for gradient calculations. When dealing with a sequential ordering with \(m\) variables, it introduces \(\) new terms. Each of these terms demands comparable time for gradient calculation to the acyclicity term \(h(W)\) typically used in current studies. This makes the computational load impractical for long sequential orderings. Note that the total ordering constraint results in the most constraint terms in this case, while it can be efficiently addressed by Equation (7)._

_This observation underpins the need to develop a more efficient method to ensure that the structure learning process remains computationally feasible for long sequential orderings._

### Augmented acyclicity-based partial order characterization

This section introduces an efficient method to characterize partial orders, distinct from directly representing the equivalent path prohibitions. We first introduce some critical concepts.

**Definition 4** (Transitive Reduction).: _The transitive reduction \(^{-}\) of a relation \(\) is the smallest relation such that the transitive closure of \(^{-}\) is equal to the transitive closure of \(\). Formally, \((^{-})^{+}=^{+}\) and \(^{-}\) is minimal._

The transitive reduction is used to eliminate redundant orders to facilitate calculation efficiency. Below, we provide an example to illustrate transitive reduction alongside transitive closure.

**Example 2**.: _For a set of transitive binary relation \(=\{(1,2),(2,3),(1,3),(3,4)\}\), its transitive closure is \(^{+}=\{(1,4),(2,4)\}\), and its transitive reduction is \(^{-}=\{(1,3)\}\)._

**Definition 5**.: _Let \(G=(V,E)\) be a graph. A source is a vertex in \(V\) with no incoming edges, i.e., \(\{v V:^{-}(v)=0\}\). A sink is a vertex with no outgoing edges, i.e., \(\{v V:^{+}(v)=0\}\)._

**Definition 6** (Maximal Path).: _Let \(G=(V,E)\) be a graph with a node set \(V\) and edge set \(E\). A path \(p=(v_{1},,v_{k})\) with \((v_{i},v_{i+1}) E\) is considered a maximal path if \(v_{1}\) is a source, \(v_{k}\) is a sink, and the path is not a proper subsequence of any other path from \(v_{1}\) to \(v_{k}\)._

**Definition 7**.: _The transitive closure of a path \(p=(v_{1},,v_{k})\), denoted as \(p^{+}\), is the set of all ordered pairs \((v_{i},v_{j})\) for \(1 i<j k\)._

**Remark 3**.: _For brevity, the following discussions regard the concepts of the directed graph, path, sequential ordering, and partial order set as an identical type of set whose element is an ordered pair \((i,j)\), as both node reachability in a graph and the order relationship are transitive. Especially, we do not distinguish between a partial order set \(\) and the graph \(G()\) constructed by \(E(G())=\{(i,j)(i,j)\}\)._

**Remark 4**.: _We assume that no cycle exists in \(^{+}\). That is, \(\) is not conflicting with itself._

With these definitions, we formalize the new approach to integrating partial order constraint \(\) into differentiable structure learning as follows (see Appendix D for detailed implementations):\[_{W}(W)\;\;h^{}(W, )=0 \] \[h^{}(W,)=_{o(^{-})}h( (W,o))\] (9b) \[(W,o)=W+ W_{o}-W W_{o}\] (9c) \[W_{o,i,j}=[(i,j) o] \]

Here, \(^{-}\) is the transitive reduction of \(\). \((^{-})\) represents the set of all maximal paths of \(^{-}\)). \([P]\) is the indicator function valuing 1 if condition \(P\) holds and 0 otherwise. \(>0\) is a hyper-parameter used for adjusting the weight in gradient calculation.

**Remark 5**.: _Recall that \(h(W) 0\) by Equation (4). Then we have that \(h^{}(W,)=0\) is equivalent to \(h((W,o))=0\) for \(o(^{-})\) by Equation (9b)._

Equation (9) can be interpreted as augmenting the original acyclicity constraint \(h(W)=0\) to a _stronger_ one \(h^{}(W,)=0\). Specifically, we use a series of partial order-augmented acyclicity constraints \(h((W,o))=0\) for \(o\) in the maximal path set of \(^{-}\) as described in Equation (9b). For each augmented acyclicity, we add the path \(o\) to the adjacency matrix \(W\) by \((W,o)\) as detailed in Equation (9c). Thus, the acyclicity function \(h\) with \((W,o)\) as input represents a _stronger_ acyclicity constraint. The _additional_ part of this stronger acyclicity accurately captures adherence to the sequential ordering indicated by \(o\), which can be derived from the following statement.

**Lemma 1**.: _A graph \(G\) is a DAG and satisfies a sequential ordering \(o=\{(p_{1},p_{2},,p_{m})\}\) if and only if graph \(G^{}\) is a DAG where \(E(G^{})=E(G) o\)._

This lemma states the equivalence of \(h((W,o))=0\) to adherence to the sequential ordering \(o\). Now consider the following statement.

**Lemma 2**.: _For the set \((^{-})\) of all maximal paths of \(^{-}\), the union of the transitive closures of these paths is the transitive closure of \(\): \(_{o(^{-})}o^{+}=^{+}\)_

This lemma states that adherence to all the sequential orderings \(o\) indicated by maximal paths in \(^{-}\) is equivalent to adherence to the complete set \(\) of partial orders. Recall that \(h^{}(W,)=0\) is equivalent to \(h((W,o))=0\) for \(o\) in \((^{-})\), and \(h((W,o))=0\) is equivalent to adherence to \(o\). Hence, we derive that \(h^{}(W,)=0\) is equivalent to adherence to \(\) by Lemma 2, as described in the following statement (the proof of these statements is provided in Appendix C.1).

**Theorem 1**.: _A graph \(G\) is a DAG and satisfies a set of partial orders \(\) if and only if \(h^{}(W,)=0\) for the function \(h\) defined by Equation (4) and \(h^{}\) defined by Equations (9b), (9c), and (9d)._

Theorem 1 shows the correctness and completeness of the equality \(h^{}(W,)=0\) in capturing the partial order constraint \(\). More concretely, all prior information of \(\) is fully integrated while no extra information beyond \(\) is introduced. This is attributed to two critical steps in integrating \(\) into the acyclicity constraint. **Step 1.** Split the partial order constraint \(\) into sequential orderings.

**Step 2.** Ensure that these sequential orderings are maximal paths in \(^{-}\). If **Step 1** is removed4, and we directly add all the edges in \(^{-}\) to \(G(W)\) for augmented acyclicity, \(h^{}\) will degenerate into \(h((W,^{-}))\). This introduces extra orders outside of \(\), as indicated in the following example.

**Example 3**.: _Assume that \(h^{}(W,) h((W,^{-}))\). Consider a partial order set \(=\{(1,2),(3,4)\}\) and a DAG \(G(W)\) with edges \(E(G(W))=\{(2,3),(4,1)\}\). Obviously, \(G(W)\) satisfies \(\). Consider the graph constructed by adding edges in \(^{-}\) (where \(^{-}=\)) to \(G(W)\), i.e., the graph of the matrix \((W,^{-})\). Its edge set is \( E(G(W))=\{(1,2),(2,3),(3,4),(4,1)\}\) and contains a cycle \((1,2,3,4,1)\). This makes \(h^{}(W,) h((W,)) 0\) by Proposition 1._

In this case, a _legal_ DAG that satisfies \(\) is forbidden by the constraint \(h^{}(W,)=0\), indicating that extra constraints beyond the prior are introduced. In other words, **Step 1** guarantees the _necessity_ of the constraint equality for partial order \(\). For **Step 2**, it guarantees the _sufficient_ adherence to \(\). If this step is removed, some order constraints of \(\) can be lost. See the following case.

**Example 4**.: _Consider a partial order constraint set \(=\{(1,2),(2,3),(4,2)\}\). Suppose that it is divided into two sequential orderings \(o_{1}=(1,2,3)\) and \(o_{2}=(4,2)\), where \(o_{2}\) is not the maximal path of \(^{-}\). Then consider graph \(G(W)\) with edges \(\{(1,2),(3,4)\}\). We have that \(E(G((W,o_{1})))=\{(1,2),(2,3),(3,4)\}\) and \(E(G((W,o_{2})))=\{(1,2),(3,4),(4,2)\}\). Both graphs are DAGs satisfying \(h()=0\) by Proposition 1. Then we have \(h^{}(W,) h((W,o_{1}))+h((W,o_{2} ))=0\). Even if \(G(W)\) satisfies this constraint equality, the edge \((3,4)\) in \(G(W)\) still violates the order \((4,3)^{+}\), derived by the transitive result of \((4,2)\) and \((2,3)\)._

In this case, an _illegal_ instance that violates \(\) is not forbidden by the constraint \(h^{}(W,)=0\). This indicates that some orders in \(^{+}\) are not specified by \(h^{}(W,)=0\) if **Step 2** is omitted.

**Remark 6**.: _Now we discuss the complexity of gradient calculation for \(h^{}(W,)\). Equation (9b) indicates that this complexity is determined by the number \(|(^{-})|\) of maximal paths in \(^{-}\), rather than the size \(|^{+}|\) of its transitive closure. For a sequential ordering with \(m\) variables, \(h^{}\) contains only one factor of \(h\) regardless of the value of \(m\). This addresses the impractical computational load of path prohibition constraints with \(\) factors as discussed in Remark 2. Note that the computational complexity of \(h^{}(W,)\) can increase with multiple sequential orderings, which is evaluated in the following section._

## 4 Experiments

We evaluate our module for applying prior partial order (PPO) on linear NOTEARS (Zheng et al., 2018), NOTEARS-MLP (Zheng et al., 2020), and DAGMA (Bello et al., 2022). It is named as 'PPO-_alg-l-p_', where _alg_ is the backbone algorithm, and _l,p_ are settings on partial orders. Representative results are reported here, and the complete results are available in Appendix E.

Section 4.1 presents the results on synthetic datasets. Section 4.2 discusses the results obtained using a well-established biological dataset (Sachs et al., 2005). For computational resources, linear NOTEARS and DAGMA are executed on a 32-core AMD Ryzen 9 7950X CPU at 4.5GHz, while NOTEARS-MLP uses an NVIDIA GeForce RTX 3090 GPU, both with a 32GB memory limit. We conduct five simulations for each synthetic structure and one simulation for the data and partial order constraints for each structure.

### Synthetic datasets

Random DAGs are generated using Erdos-Renyi (ER) and scale-free (SF) models with node degrees in \(\{2,4\}\) and numbers of nodes \(d\) in \(\{20,30,50\}\). For linear SEM, uniformly random weights are assigned to the weighted adjacency matrix \(A\). Given \(A\), samples are generated by \(X=A^{T}X+z,X^{d}\) using noise models {Gaussian (gauss), Exponential (exp)}. Observational samples \(D^{n d}\) are then generated with the sample size \(n=4d\). For nonlinear cases, uniformly random weights are assigned to weighted adjacency matrices \(W_{1},W_{2},W_{3}\). Based on these matrices, samples are generated using \(X=(XW_{1})+(XW_{2})+(XW_{3})+z\) with \(z(0,1)\). The sample size is set to \(n=20d\). The parameter \(\) in Equation (9c) is set to 1.

To mimic real-world prior partial orders, we generate multiple sequential orderings, referred to as the _chain_ of ordering. Specifically, we first conduct a topological sort on the DAG to derive a total ordering \(\). We then randomly select \(l\) chains, each denoting a sub-ordering randomly generated from \(\). Here, \(l\) is the number of chains and \(m\) is the size of each chain. We first investigate the case of a single chain of ordering and then the case of multiple chains of orderings. For single-chained ordering, \(l=1\) and \(m\) is in \(\{0.5d,0.75d,d\}\) (where \(d\) is the number of nodes). For multi-chained ordering, \(l\) is in \(\{1,2,3,5,10\}\) and \(m\) is fixed at \(0.5d\).

#### 4.1.1 Single-chained ordering

In this experiment, we examine structure learning using single-chained ordering. The results of Structural Hamming Distance (SHD), F1 score, and run time for linear NOTEARS are illustrated in Figure 1. Results for NOTEARS-MLP (nonlinear samples) and DAGMA are presented in Figure 2.

**Output quality.** Our method demonstrates notable superiority over algorithms without prior information in terms of output quality in most cases, with the advantage becoming more pronouncedas the number of nodes in the ordering chain increases. This confirms the effectiveness of our approach in enhancing structure learning quality with prior partial orders. Some degradation cases may be caused by _invalid_ priors in the simulated ordering. Since the topological sort for a DAG is not unique, some random ordering chains may not contribute to revealing the most essential parts of orderings, especially with smaller chain sizes.

**Run time.** We observe that our method with a single-chained ordering is typically faster than structure learning without prior. This efficiency is due to the effective management of our module for single-chained orderings. However, in some cases, such as on the SF4 graph with NOTEARS-MLP and DAGMA, the efficiency can be degraded. This indicates that the impact of partial orders on efficiency can vary with different data distributions and backbone algorithms.

#### 4.1.2 Multi-chained ordering

The results for SHD, F1-score, and run time using linear NOTEARS and NOTEARS-MLP with multi-chained orderings are presented in Figure 3. The output quality demonstrates similar trends to

Figure 1: Structural discovery in terms of SHD (lower is better), F1-score (higher is better) and CPU time (\(_{10}\) s) with NOTEARS on linear data. Rows: graph types. [ER,SF]-\(k\) represents [Erdős-Rényi, scale-free] graphs with \(kd\) expected edges. Columns: noise types of SEM. Error bars represent standard errors over 5 simulations. Method: PPO-\(alg\)-\(1\)-\(p\) denotes our method with partial order settings \(l=1\) and \(m=p\%d\).

Figure 2: Structural discovery in terms of SHD (lower is better), F1-score (higher is better) and CPU time (\(_{10}\) s) with NOTEARS-MLP and DAGMA on representative cases.

those seen with single-chained ordering, with more pronounced improvements over the baselines as the number of partial order constraint chains increases. The run time dynamics are illustrated with a curve that reflects changes corresponding to the primary influencing factor: the number of chains. Initially, run time increases and then decreases as the number of chains grows. This pattern results from the increasing complexity of gradient calculations for the constraint term \(h^{}\), which scales with the number of maximal paths in the partial orders. At first, the increasing number of chains leads to more maximal paths in the partial order, thus delaying time efficiency. As the partial order becomes denser, more chains can be covered by a longer chain, which leads to fewer maximal paths in its transitive reduction, resulting in better time efficiency.

### Real-world data

The dataset provided by Sachs _et al._ (2005) consists of continuous measurements of protein and phospholipid expression levels in human immune system cells. It is frequently used as a benchmark in graphical models due to its associated consensus network, which includes 11 nodes and 17 edges, based on experimental annotations recognized by the biological community.

We use experimental data from one of the cells with 853 samples. To mimic varying levels of experimental resources, we selected the first \(s\) data samples for testing, where \(s\{50,100,500,853\}\). A single-chained prior ordering is given involving different numbers of variables in \(\{6,8,11\}\). Linear NOTEARS serves as the backbone algorithm. The parameter \(\) in Equation (9c) is set to 3.

The structural evaluation metrics reported in Table 1 include SHD, False Discovery Rate (FDR), True Positive Rate (TPR), and F1 score. The findings reveal that NOTEARS with partial orders discovers more accurate structures than NOTEARS without prior information in most cases. Remarkably, even with the smallest sample size (50), NOTEARS with partial order constraint (11 nodes) significantly outperforms the baseline using the largest sample size (853). This underscores the efficacy of the proposed partial order constraint-based differentiable structure learning approach in conserving experimental resources in scientific research contexts.

   &  &  &  &  \\  & SHD & FDR & TPR & F1 & SHD & FDR & TPR & F1 & SHD & FDR & TPR & F1 & SHD & FDR & TPR & F1 \\  NOTEARS & 25 & 0.76 & 0.41 & 0.30 & 16 & 0.65 & 0.41 & 0.38 & 15 & 0.60 & 0.35 & 0.38 & 12 & 0.57 & 0.35 & 0.39 \\ PPO-1-6 & 20 & 0.70 & 0.35 & 0.32 & 15 & 0.62 & 0.29 & 0.33 & 13 & 0.50 & 0.35 & 0.41 & 14 & 0.55 & 0.29 & 0.36 \\ PPO-1-8 & 16 & 0.59 & 0.41 & 0.41 & 11 & 0.42 & 0.41 & 0.48 & 12 & 0.40 & 0.35 & 0.44 & 10 & 0.30 & 0.41 & 0.52 \\ PPO-1-11 & **12** & **0.27** & **0.47** & **0.57** & **10** & **0.00** & **0.41** & **0.58** & **11** & **0.13** & **0.41** & **0.56** & **11** & **0.13** & **0.41** & **0.56** \\  

Table 1: Structural discovery in terms of SHD\(\), FDR\(\), TPR\(\) and F1\(\) on each dataset with various sample sizes. Linear NOTEARS is used as the backbone algorithm, and PPO-1-\(m\) denotes our method using a single-chained ordering containing \(m\) nodes. The best result is highlighted with bold texts.

Figure 3: Structural discovery in terms of SHD (lower is better), F1-score (higher is better) and CPU time (\(_{10}\) s) with linear NOTEARS and NOTEARS-MLP. Method: PPO-_alg-l_-50 represents our method where \(l\) is the number of chains and the size of chains is \(m=0.5d\).

Discussion

Limitations and future directionsDespite the theoretical correctness and completeness of the proposed method for integrating partial orders in differentiable structure learning, there are some practical limitations.

First, the augmented acyclicity constraint \(h^{}(W,)=0\) cannot be strictly satisfied during the optimization process of the augmented Lagrangian method, as proven by Wei _et al._. This may result in some order constraints from the prior not being satisfied in the output. This issue is inherent to the optimization aspect of differentiable structure learning and may be addressed with more refined optimization techniques in the future.

Additionally, although the proposed method efficiently handles long sequential orderings, its efficiency can be impacted by partial orders with complex structures. This is evident from the experimental results involving multi-chained orderings. We randomly selected multiple ordering chains, each comprising half of the nodes, forming a complex order structure with considerable maximal paths. This leads to a sharp increase in the number of constraint terms in the augmented acyclicity constraint. Fortunately, real-world ordering priors typically do not exhibit such complex structures and can usually be captured by a few chains. To enhance time efficiency in such cases, a more refined characterization method could be explored to reduce computational overhead in the future. We may focus on improving the gradient calculation of the proposed augmented acyclicity constraint in the context of multiple sequential orderings. This can be explored by merging the common parts of the gradient calculation process or developing more efficient characterizations.

ConclusionThis paper enhances the field of differentiable structure learning by enabling this framework to apply priors of partial order constraints. We systematically analyze the related challenges of applying flexible order constraints and propose a novel and effective strategy to address them by augmenting the acyclicity constraint. We present a theoretical proof confirming the correctness and completeness of our strategy. Empirical results highlight the superiority of our method in improving structure learning with partial order constraints. Results on a well-known real-world dataset further emphasize its potential in uncovering more accurate causal mechanisms with reduced experimental resources.

## Broader Impact

The proposed method allows researchers across various scientific fields to specify ordering priors, enhancing causal discovery with state-of-the-art differentiable structure learning algorithms from experimental or observational data. As demonstrated with the real-world Sachs dataset [Sachs _et al._, 2005], differentiable structure learning using a proper ordering prior with only 10% of the samples required by methods without a prior yields significantly better structures. This reduction in data requirements for causal discovery can save experimental resources across many domains.

However, researchers must exercise caution when providing ordering priors. The proposed method strictly adheres to these priors, and numerous incorrect ordering priors can severely impact the results. For instance, in social sciences, if incorrect assumptions about the order of socio-economic events are used as priors, the resulting causal model may be misleading, affecting policy decisions based on such a model.