# Last-Iterate Convergent Policy Gradient

Primal-Dual Methods for Constrained MDPs

Dongsheng Ding

University of Pennsylvania

dongshed@seas.upenn.edu

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

&Kaiqing Zhang

University of Maryland, College Park

kaiqing@umd.edu

&Alejandro Ribeiro

University of Pennsylvania

aribeiro@seas.upenn.edu

Alphabetical order.

###### Abstract

We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove that the policy primal-dual iterates of RPG-PD converge to a regularized saddle point with a sublinear rate, while the policy iterates converge sublinearly to an optimal constrained policy. We further instantiate RPG-PD in large state or action spaces by including function approximation in policy parametrization, and establish similar sublinear last-iterate policy convergence. Second, we propose an optimistic policy gradient primal-dual (OPG-PD) method that employs the optimistic gradient method to update primal/dual variables, simultaneously. We prove that the policy primal-dual iterates of OPG-PD converge to a saddle point that contains an optimal constrained policy, with a linear rate. To the best of our knowledge, this work appears to be the first non-asymptotic policy last-iterate convergence result for single-time-scale algorithms in constrained MDPs. We further validate the merits and the effectiveness of our methods in computational experiments.

## 1 Introduction

Constrained Markov decision process (Constrained MDP) is the classical model for constrained dynamic systems in the early stochastic control literature (e.g., ) and the recent constrained reinforcement learning (RL) literature (e.g., ). It is applicable to many constrained control problems by integrating other system specifications in constraints, and admits a natural extension of constrained optimization and Lagrangian in policy space. Lagrangian-based policy search methods, especially policy-based primal-dual methods that work simultaneously withprimal/dual variables, lie at the heart of recent successes of constrained MDPs, e.g., navigation , autonomous driving , robotics , and finance ; see  for more examples.

Despite the popularity of policy-based primal-dual algorithms, classical asymptotic convergence assumes that primal-dual updates are in two-time-scale1type  (and/or work in two nested loops2), and considerable global non-asymptotic convergence guarantee is measured via an average of past objective/constraint functions  or a mixture of past policies . These results are unfavorable in constrained dynamic systems, especially safety-critical ones, due to three reasons: (i) Average and mixture performance of non-asymptotic convergence conceals oscillating (or even overshooting) objective/constraint functions of immediate policy iterates , and oscillation-incurred constraint violation impedes a policy iterate being optimal; (ii) Asymptotic convergence is not instructive, because arbitrarily slow convergence, and oscillation and overshoot in any finite time can happen; (iii) Two-time-scale algorithms including algorithms with nested loops are sensitive to hyper-parameters and are therefore typically difficult to tune . Thus, we ask the following question in constrained MDPs:

Can the _policy iterates_ of a _single-time-scale_ policy-based primal-dual algorithm

converge to an optimal constrained policy with _non-asymptotic rate_?

By "single-time-scale", we refer to the classical methods  that iterate primal/dual variables concurrently (with the same constant stepsize). Only partial answers to this question are provided in recent studies  since they either do not work in the single-time-scale scheme or they do not have non-asymptotic convergence guarantees. In this work, we provide an affirmative answer in two methodologies. First, we initiate the design and analysis of single-time-scale policy-based primal-dual algorithms via regularization, while previous works  rely on two-time-scale schemes. Second, inspired by convex minimax optimization , we propose a new optimistic policy gradient for a single-time-scale policy-based primal-dual algorithm that solves a class of non-convex minimax problem. While preparing our work, we noticed a contemporaneous work , which has empirically validated the effectiveness of other optimistic methods in constrained MDPs, has further inspired the pursuit of our contributions, as outlined in detail below.

**Contributions**. To compute an optimal policy of an infinite-horizon discounted constrained MDP, we employ the Lagrangian method to cast it into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, propose two single-time-scale policy-based primal-dual algorithms, and prove global non-asymptotic convergence of their policy iterates.

* _Nearly dimension-free sublinear last-iterate policy convergence._ We propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual using a quadratic-regularized gradient ascent, simultaneously. We prove that the policy primal-dual iterates of RPG-PD converge to a regularized saddle point with a sublinear rate, and the policy iterates converge to an optimal constrained policy sublinearly.
* _Sublinear last-iterate policy convergence with function approximation._ We generalize RPG-PD for constrained MDPs with large state/action spaces by including function approximation in policy parametrization. We prove that the policy primal-dual iterates of an inexact RPG-PD converge to a regularized saddle point with a sublinear rate, but up to a function approximation error, and the policy iterates converge sublinearly to an optimal constrained policy when the error is small.
* _Problem-dependent linear last-iterate policy convergence._ We propose an optimistic policy gradient primal-dual (OPG-PD) method that employs the optimistic gradient method to update the primal/dual, simultaneously. We prove that the policy primal-dual iterates of OPG-PD converge to a saddle point that contains an optimal constrained policy with a problem-dependent linear rate.

While last-iterate convergence is of importance in its own right, by adding proper conservatism in the constraint, both methods can ensure _no_ constraint violation for the _last policy iterate_, which perhaps is best for safety-critical tasks . As far as we know, this work shows the first non-asymptotic and policy last-iterate convergence for single-time-scale algorithms in the constrained MDP literature. We further exhibit the merits and the effectiveness of our methods in experiments.

**Technical comparisons with prior art**. Although global asymptotic last-iterate convergence has been established for single-time-scale algorithms very recently [36; 40], and value-average or policy-mixture non-asymptotic convergence have been established for other algorithms [23; 24; 25; 27; 28; 41; 42; 26], these studies did not investigate global _non-asymptotic_ and _last-iterate_ convergence for _single-time-scale_ algorithms. Our results not only strengthen these prior guarantees, but also set up a new framework for analyzing policy-based primal-dual algorithms via the distance of primal-dual iterates to a saddle point that contains an optimal constrained policy. Our RPG-PD and OPG-PD keep the simplicity of single-time-scale primal-dual methods and output a nearly-optimal policy in the last iterate, which is more convenient than the history-average policies [28; 24] or the policies from subroutines [34; 35]. Compared with the policy-based methods , our OPG-PD is a projected policy gradient method that enjoys policy last-iterate convergence with linear rate. Compared with the constrained saddle-point problems [38; 39], our minimax optimization that results from constrained MDP is _non-convex_. Hence, our OPG-PD extends the last-iterate convergence guarantee from convex minimax optimization to a class of non-convex ones, while preserving a linear rate. Compared with the analysis in the two-player zero-sum Markov game [43; 44], there is no reduction from constrained MDPs to per-state bilinear games. Please see more details in Appendix A.

## 2 Preliminaries

We consider an infinite-horizon discounted constrained Markov decision process [3; 5; 8] - CMDP \((\,S,A,P,r,u,b,,\,)\) - where \(S\) and \(A\) are state/action spaces, \(P\) is a transition kernel that specifies the transition probability \(P(s^{}\,|\,s,a)\) from state \(s\) to next state \(s^{}\) under action \(a A\), \(r\), \(u\)\(:S A\) are reward/utility functions, \(b\) is a constraint threshold, \([0,1)\) is a discount factor, and \(\) is an initial state distribution. A stationary stochastic policy \(:S(A)\) determines a probability distribution over the action space \(A\) based on current state, i.e., \(a_{t}(\,|\,s_{t})\) at time \(t\), where \((A)\) is a probability simplex over \(A\). Let \(\) be the set of all possible stochastic policies. A policy \(\), together with the initial state distribution \(\), induces a distribution over trajectories \(=\{(s_{t},a_{t},r_{t},u_{t})\}_{t=\,0}^{}\), where \(s_{0}\), \(a_{t}(\,|\,s_{t})\), \(r_{t}=r(s_{t},a_{t})\), \(u_{t}=u(s_{t},a_{t})\) and \(s_{t+1} P(\,|\,s_{t},a_{t})\) for all \(t 0\).

Given a policy \(\), the value functions \(V_{r}^{}\), \(V_{u}^{}:S\) associated with the reward function \(r\) or the utility function \(u\) are given by the expected sums of discounted rewards or utilities under policy \(\):

\[V_{r}^{}(s) := [\,_{t\,=\,0}^{}^{t}r(s_{t},a_{t}) \,|\,s_{0}=s\,]\,\,\,\,\,\,V_{u}^{}(s)\,\,\,:=\,\,\, [\,_{t\,=\,0}^{}^{t}u(s_{t},a_{t})\,|\,s_{0}=s\,]\]

where the expectation \(\) is over the randomness of the trajectory \(\) induced by \(\). Their expected values under \(\) are \(V_{r}^{}():=_{s}[\,V_{r}^{}(s)\,]\) and \(V_{u}^{}():=_{s}[\,V_{u}^{}(s)\,]\). It is useful to introduce the discounted state visitation distribution, \(d_{s_{0}}^{}(s)=(1-)_{t\,=\,0}^{}^{t}(s_{t} =s\,|\,,s_{0})\) which adds up discounted probabilities of visiting \(s\) in the execution of \(\) starting from \(s_{0}\). Denote \(d_{}^{}(s):=_{s_{0}}[\,d_{s_{0}}^{}(s)\,]\) and thus \(d_{}^{}(s)(1-)(s)\) for any \(\) and \(s\). Furthermore, for the reward function \(r\), we introduce the state-action value function \(Q_{r}^{}\): \(S A\) when the agent begins with a state-action pair \((s,a)\) and follows a policy \(\), and the associated advantage function \(A_{r}^{}\): \(S A\),

\[Q_{r}^{}(s,a) := [\,_{t\,=\,0}^{}^{t}r(s_{t},a_{t}) \,|\,s_{0}=s,a_{0}=a\,]\,\,\,\,\,A_{r}^{}(s,a)\,\,\,:=\,\,\,Q_ {r}^{}(s,a)-V_{r}^{}(s).\]

Similarly, we define \(Q_{u}^{}:S A\) and \(A_{u}^{}:S A\) for the utility function \(u\).

In this work, we aim to find a policy solution \(^{}\) of a constrained policy optimization problem,

\[*{maximize}_{\,\,}\,\,\,V_{r}^{}() \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \, the method of Lagrange multipliers , we dualize the constraint in (1) and present a standard Lagrangian \(L(,):=_{r}^{}()+ V_{g}^{}()\), where \(\) is the primal variable and \([0,]\) is the dual variable or the Lagrangian multiplier. Introduction of the Lagrangian \(L(,)\) interprets Problem (1) as a max-min problem: \(*{maximize}_{\,\,}*{minimize}_{\, \,[0,]}L(,)\), and thus we can view the Lagrangian \(L(,)\) as a value function with a composite function \(r+ g\),

\[*{maximize}_{\,\,}\,*{minimize}_{ \,\,[0,]}\,\,V_{r+ g}^{}().\] (2)

However, it's defective to view Problem (2) as a standard MDP problem by fixing a dual variable \(\), even the optimal one; also see . This is often referred to as the _scalarization fallacy_; see Appendix B.2 for the detail. From the perspective of game theory, we instead view \(V_{r+ g}^{}()\): \([\,0,\,]\) as a payoff function for a two-player zero-sum game in which max-player is the policy \(\) and min-player is the dual variable \([\,0,\,]\), and study its saddle points. To proceed, we assume feasibility for Problem (1) throughout our analysis.

**Assumption 1** (Feasibility).: _There exists a policy \(\) and \(>0\) such that \(V_{g}^{}()\)._

Feasibility mirrors the Slater condition in the duality analysis of constrained optimization . It can be verified by solving an unconstrained MDP problem with respect to \(V_{g}^{}()\).

A saddle point \((^{},^{})\) satisfies \(V_{r+^{}g}^{}() V_{r+^{}g}^{^{}} () V_{r+ g}^{^{}}()\) for all \(\), \([0,]\), or equivalently, \(^{}\) is the max-min point, i.e., \(^{}*{argmax}_{\,\,}V_{r+^{}g} ^{}()\) and \(^{}\) is the min-max point, i.e., \(^{}*{argmin}_{\,\,[0,]}V_{r+  g}^{^{}}()\). To view Problem (2) as a saddle-point problem, we denote \(V_{P}^{}():=_{\,\,[0,]}V_{r+ g}^{}()\) as the primal function which takes \(V_{r}^{}()\) when \(V_{g}^{}() 0\) and \(-\) otherwise, and \(V_{D}^{}():=_{\,\,}V_{r+ g}^{}()\) as the dual function. Let an optimal dual variable be \(^{}*{argmin}_{\,\,[0,]}V_{D}^{ }()\). For Problem (1) under Assumption 1, strong duality holds in policy space [12, Theorem 3] and optimal dual variables are bounded [48, Lemma 3].

**Lemma 1** (Strong duality/Saddle point existence and boundedness).: _Let Assumption 1 hold. Then, (i) strong duality holds for Problem (1), i.e., \(V_{P}^{^{}}()=V_{D}^{^{}}()\); (ii) optimal dual variables are bounded, i.e., \(^{}[\,0,(V_{r}^{^{}}-V_{r}^{})/\,]\)._

Let the set of max-min points be \(^{}:=*{argmax}_{\,\,}_{\,\,[0, ]}V_{r+ g}^{}()\) and the set of min-max points be \(^{}:=*{argmin}_{\,\,[0,]}_{\, \,}V_{r+ g}^{}()\). From Lemma 1 (ii), \(^{}\) is contained in a bounded interval \(:=[\,0,1/((1-))\,]\). Lemma 1 (i) shows that any pair \((^{},^{})^{}^{}\) solves the following constrained saddle-point problem,

\[*{maximize}_{\,\,}\,*{minimize}_{\, \,}\,\,\,V_{r+ g}^{}()\,\,\,=\,\,\,*{ minimize}_{\,\,}\,\,*{maximize}_{\,\,}\,\,\,V_{r+  g}^{}().\] (3)

Any saddle points associated with the set \(^{}\) are captured by Problem (3) due to the invariance of saddle points, and searching for any pair \((^{},^{})^{}^{}\) is sufficient by the interchangeability of saddle points; see Lemmas 8-9 in Appendix B.3 for the properties of saddle points. Thus, we view the policy (primal) as max-player and the dual as min-player in a zero-sum game.

Three structural properties from constrained MDPs distinguish Problem (3) from recent last-iterate convergence for learning in zero-sum games (e.g., ): (i) Two players are _asymmetric_. One plays a stochastic policy that affects the transition dynamics and the other selects an action in a continuous interval that only changes the payoff; (ii) Problem (3) is a _non-convex_ game, because of the non-concavity of the payoff \(V_{r+ g}^{}()\) in policy \(\) (e.g., [49, Lemma 1]); (iii) A saddle-point policy for Problem (3) cannot be _uniformly_ max-min optimal, i.e., being optimal _across all states_, since an optimal policy often depends on the initial state distribution \(\) in a constrained MDP; see Appendix B.1. Hence, known last-iterate results in zero-sum convex games or symmetric Markov games that admit uniformly optimal policies can't be applied and new techniques are required to address this non-standard saddle-point problem, which warrants our contributions in this work.

**Warm-up: Indirect policy search in occupancy-measure space**. Finding a saddle point of a non-convex game is hard in general . Nevertheless, Problem (1) can be rewritten as a linear program regarding the occupancy measure , which permits _indirectly_ searching for a saddle point of a bilinear Lagrangian . These asymptotic or average-iterate convergence results can be easily strengthened by applying last-iterate convergence results for bilinear games (e.g., ) to be non-asymptotic and last-iterate. By doing so, we state an optimistic primal-dual (OPD) method (18) in Appendix B.4. Compared with a contemporaneous work , OPD is free of projection to an occupancy measure set, and enjoys strengthened linear convergence.

OPD is an _indirect_ policy search method that iterates using occupancy measure-based gradients, not policy-based gradients. It is crucial to develop _direct_ policy search methods that are widely-used in RL, which is our focus. We propose two such methods in Section 3 and Section 4, respectively.

## 3 Policy Last-Iterate Convergence: Regularized Method

Towards achieving policy last-iterate convergence, a practical strategy is using regularization  to "convexify" Problem (3). We present a regularized method - Regularized Policy Gradient Primal-Dual (RPG-PD) - that converges to a saddle point that yields an optimal constrained policy.

### Regularized policy gradient primal-dual method

We introduce a regularized Lagrangian \(L_{}(,):=V^{}_{r+_{g}}()+(()+ ^{2})\) by adding a regularization term \(()+^{2}\) onto the original Lagrangian \(V^{}_{r+_{g}}()\), where \(\) is a regularization parameter, and \(():=_{t\,=\,0}^{}-^{t}( a_{t}\,|\,s_{t})\) is an entropy-like regularization term . We now introduce a regularized constrained saddle-point problem,

\[}\ \ }\ \ L_{}(,)\ =\ }\ \ }\ \ L_{}(,).\] (4)

Problem (4) is well-defined, since there exists a saddle point for \(L_{}(,)\) over \(\) and it is unique; see Appendix C.1 for proof. A saddle point \((^{}_{},^{}_{})\), i.e., \(^{}_{}=*{argmax}_{\,\,}_{\,\, }L_{}(,)\) and \(^{}_{}=*{argmin}_{\,\,}_{ \,\,}L_{}(,)\), satisfies a sandwich-like property,

\[V^{}_{r+^{}_{}g}()-(^{}_{}) \ \ V^{^{}_{}}_{r+_{}g}()\ \ V^{^{}_{}}_{r+_{g}}()+^{2} \ (,)\] (5)

that states that \((^{}_{},^{}_{})\) is a saddle point of the original Lagrangian \(V^{}_{r+_{g}}()\), up to two \(\)-terms. We thus propose a regularized policy gradient primal-dual (RPG-PD) method by maintaining a sequence for policy and dual variables each: \(\{_{t}\}_{t\,\,0}\) for the policy-player, and \(\{_{t}\}_{t\,\,0}\) for the dual-player,

\[_{t+1}(\,|\,s) = (A)}{*{ argmin}}\{_{a}(a\,|\,s)Q^{_{t}}_{r+_{t}g+_{t}}(s,a) \,-\,*{KL}((\,|\,s),_{t}(\,|\,s))\}\] (6a) \[_{t+1} = }\{ V^{_{t}}_{g}()+_{t}\,+\,(-_{t})^{2}\},\] (6b)

where the gradient direction \(Q^{_{t}}_{r+_{t}g+_{t}}(s,a)\) is the state-action value function under a composite function \(r+_{t}g+_{t}\) in which \(_{t}(s,a):=-_{t}(a\,|\,s)\), \(*{KL}(p,p^{}):=_{a}p_{a}}{p^{}_{a}}\) is the Kullback-Leibler (KL) divergence, \((A):=\{(\,|\,s)(A)\,|\,(a\,|\,s)}{|A|},a A\}\) is a restricted probability simplex set with parameter \(_{0}(0,1)\), \(\) is the stepsize, and \((_{0}(\,|\,s),_{0})(A)\) is an initial point. Projecting the policy iterate to the simplex set \((A)\) ensures the boundedness of the gradient. Primal update (6a) works as the classical mirror descent with KL divergence  with a projection onto the set \((A)\). Dual update (6b) performs typical projected gradient descent. Hence, RPG-PD is a single-time-scale method. RPG-PD simplifies the two-time-scale method  to be single-time-scale and generalize the single-time-scale methods [23; 54] with regularization.

### Policy last-iterate convergence

In Theorem 2, we show that the primal-dual iterates of RPG-PD converge in the last iterate; see Appendix C.2 for proof. We characterize the convergence via a distance metric \(_{t}=*{KL}_{t}()+(^{}_{}- _{t})^{2}\), where \(*{KL}_{t}():=(1/(1-))_{s}d^{^{}_{}}_{ }(s)*{KL}_{t}(s)\) and \(*{KL}_{t}(s):=*{KL}(^{}_{}(\,|\,s),_{t}(\,|\,s))\).

**Theorem 2** (Linear convergence of RPG-PD).: _Let Assumption 1 hold. If we set the stepsize \( 1/C_{,,_{0}}\), then the primal-dual iterates of RPG-PD (6) satisfy_

\[_{t+1}  ^{- t}\,_{1}\,+\,\, \,(C_{,,_{0}})^{2},\,(C^{}_{,})^{2}\,\]

_where \(C_{,,_{0}}:=(1+1/((1-))+|A|)/(1-)-( _{0}/|A|)\), \(C^{}_{,}:=(1+/)/(1-)\)._Theorem 2 states that the primal-dual iterates of RPG-PD converge to a neighborhood of \((_{}^{},_{}^{})\) in a linear rate. The size of neighborhood scales with \(/+(1+^{2}_{0})\) and the convergence rate is \(\). Even if \(_{0}\) is very small, the \(_{0}\)-term is almost a constant. If we take \(=(,1/C_{,,_{0}})\) and \(_{0}=\), then after \(O(1/)\) iterations the RPG-PD's primal-dual iterate \((_{t},_{t})\) is \(\)-close to \((_{}^{},_{}^{})\), i.e., \(_{t}=O()\) for any \(t(1/(^{2}))(1/)\). For small \(\), we can translate the policy convergence for the value functions in Corollary 3; see Appendix C.3 for proof.

**Corollary 3** (Nearly-optimal constrained policy).: _Let Assumption 1 hold. For small \(>0\), if we take \(=(^{4})\), \(=(^{2})\), and \(_{0}=\), then the policy iterates of RPG-PD (6) satisfy_

\[V_{r}^{^{*}}()-V_{r}^{_{t}}()\ \ \ \ -V_{g}^{_{t}}()\ \ \ t=(\,}^{2}\,)\]

_where \(()\) only has some problem-dependent constant._

Corollary 3 states that the last policy iterate of RPG-PD is an \(\)-optimal policy for Problem (1) after \((1/^{6})\) iterations. Compared with the single-time-scale methods [23; 54], RPG-PD improves the convergence from average-value (or regret-type) to _last policy iterate_. Not just being theoretically stronger, the last-iterate convergence is more appealing since it captures the stability of trajectories of an algorithm [29; 40]. Compared with the two-time-scale methods [28; 24; 34; 35], RPG-PD is free of nested loops, and uniform ergodicity and exploratory initial state distribution. We notice that the dual methods [28; 24] yield history-average policies and the dual methods [34; 35] return policies from a subroutine. In contrast, RPG-PD outputs a nearly-optimal policy in the last iterate, the first-of-its-kind in the constrained MDP literature, albeit the rate is worse than the average ones [23; 54].

To get zero constraint violation, i.e., \(V_{g}^{_{t}}() 0\) at some \(t\), it is straightforward to employ a conservative constraint \(V_{g^{}}^{}() 0\) with \(g^{}:=g-(1-)\) for some \(>0\). When \(\) is small enough, there always exists some \(\) such that the policy iterates of RPG-PD (6) satisfy \(V_{r}^{^{*}}()-V_{r}^{_{t}}()\) and \(V_{g}^{_{t}}() 0\) for large \(t\); see Appendix C.4 for proof. Our zero constraint violation ensures the last policy iterate of RPG-PD to satisfy the constraint, which is not the zero average constraint violation in the episodic setting [55; 56]. Compared with the zero constraint violation of a policy induced by an average of past occupancy measures , RPG-PD's zero constraint violation directly settles the policy iterates down, which appears to be the first policy-based zero constraint violation.

Last but not least, the iteration complexity of RPG-PD is nearly-free of the MDP dimension, except for an \(|A|\)-term, which inherits the dimension-free property of the NPG methods [49; 57; 23]. Hence, it is ready to view RPG-PD as a variant of NPG methods and generalize RPG-PD to constrained MDPs with large state spaces in the function approximation setting.

### Linear function approximation case

To deal with large state spaces, we use a parametrized policy \(_{}\) with \(^{d}\) for RPG-PD (6) without restricting \((A)\), where \(d\) is much smaller than the size of state/action spaces. To introduce function approximation, we begin with a tabular softmax policy \(_{}(a\,|\,s)=)}{_{a^{}}(_ {s,a^{}})}\) for all \((s,a) S A\) and \(^{|S||A|}\). Connecting NPG to mirror descent [49; 58], we express RPG-PD (6) as a NPG method with the following update; see Appendix C.5 for proof,

\[_{t+1} = _{t}\,+\,\,(1-)F_{}(_{t})^{} _{}L_{}(_{_{t}},_{t})\] (7a) \[_{t+1} = _{}(\,(1-)_{t}- V_{g}^{ _{_{t}}}()\,)\] (7b)

where \(F_{}(_{t})^{}_{}L_{}(_{_{t}}, _{t})\) is a NPG direction, and \(F_{}()\) is the Fisher information matrix for a policy \(_{}\), i.e., \(F_{}():=\,_{s d_{}^{_{}}}_{a _{}(\,|\,s)}[\,_{}_{}(a\,|\,s)( _{}_{}(a\,|\,s))^{}\,]\). A useful property of (7a) is that NPG can be related to a linear regression. For any policy \(_{}\) and a state-action value function \(Q^{_{}}\), the associated compatible function approximation error is \(E_{Q}(w,,):=_{(s,a)\,^{}}[\,(w^{} _{}(a\,|\,s)-Q^{_{}}(s,a))^{2}\,]\), where \((s,a)=d_{}^{_{}}(s)_{}(a\,|\,s)\) is a state-action distribution. It is known that (7a) is equivalent to \(_{t+1}=_{t}+ w_{}^{}\), where \(w_{}^{}*{argmin}_{w\,^{d}}E_{Q}(w,_ {t},_{t})\) in which \(Q^{_{_{t}}}(s,a)=Q_{r+_{t}g+_{t}}(s,a)\) and \(_{t}(s,a)=d_{}^{_{_{t}}}(s)_{_{t}}(a\,|\,s)\) (e.g., [59, Lemma 1]). In practice, only an approximate minimizer \(w_{}^{}\) is available if a sample-based algorithm is used, e.g., \(w_{t}*{argmin}_{\|w\|\,\,W}E_{Q}(w,_{t},_{t})\), where \(W>0\).

A useful generalization of the softmax policy to large state spaces is the log-linear policy based on linear function approximation. Let \(_{s,a}^{d}\) be a feature map with \(\|_{s,a}\| 1\) for each state-action pair \((s,a)\). A log-linear policy \(_{}\): \(S(A)\) is parametrized by a parameter \(^{d}\),

\[_{}(a\,|\,s) = ^{}\,\,)}{_{a^{ }}(_{s,a^{}}^{}\,\,)}\,\,\,\,\,(s,a) S A\]

which takes the tabular softmax policy as a special case, i.e., \(_{s,a}\) is an indicator function. We notice that \(_{}_{}(a\,|\,s)=_{s,a}-_{a^{} _{}(\,|\,s)}\,[\,_{s,a^{}}\,]\). Since the log-linear policy is invariant to any action-independent term, it is convenient to replace \(_{}_{}(a\,|\,s)\) by \(_{s,a}\) and we introduce a simplified compatible function approximation error, \(_{Q}(w,,):=_{(s,a)}\,[\,(_{s,a}^{ }w-Q^{_{}}(s,a))^{2}\,]\). Thus, we can take \(w_{t}^{}*{argmin}_{w\,\,^{d}}_{Q }(w,_{t},_{t})\) in which \(Q^{_{_{t}}}(s,a)=Q_{r+_{t}g+_{t}}^{_{_{t}}}(s,a)\) and \(_{t}(s,a)=d_{}^{_{_{t}}}(s)\,_{_{t}}(a\,|\,s)\) to update \(_{t+1}=_{t}+ w_{t}^{}\). Using the log-linear policy class, we replace the primal gradient direction of RPG-PD (6) by its linear function approximation \(_{s,a}^{}w_{t}^{}\),

\[_{_{t+1}}(\,|\,s) = *{argmax}_{(\,|\,s)\,\,(A)} \{_{a}(a\,|\,s)_{s,a}^{}w_{t}^{}\,-\, \,((\,|\,s),_{_{t}}(\,|\,s))\}\] (8)

which, together with Dual update (6b), leads to a general version of RPG-PD. The set \((A)\) ensures bounded true gradient direction \(Q_{r+_{t}g+_{t}}^{_{_{t}}}(s,a)\). When there is no function approximation error, (8) reduces to Primal update (6a). In practice, we can only compute \(w_{t}^{}\) approximately via

\[w_{t}\,\,*{argmin}_{\|w\|\,\,W}\,_{Q}(w, _{t},d_{t,})\]

which leads to an inexact RPG-PD: Primal update (8) in which \(w_{t}^{}\) is replaced by \(w_{t}\) and Dual update (6b), where \(d_{t,}:=(1-)_{(s_{0},a_{0})}_{t=0}^{} ^{t}(s_{t}=s,a_{t}=a\,|\,_{_{t}},s_{0},a_{0})\) is a state-action distribution starting from any distribution \(\). Noticeably, \(d_{t,}\) is more general than \(_{t}\). To control the function approximation error, we divide \(_{Q}(w_{t},_{t},d_{t,})\) into a statistical error \(_{Q}(w_{t},_{t},d_{t,})-_{Q}(w_{t}^{}, _{t},d_{t,})\) that is similar to the excess risk in supervised learning, and an approximation error \(_{Q}(w_{t}^{},_{t},d_{t,})\) that captures how well a linear function \((w_{t}^{})^{}_{s,a}\) approximates the true value function under \(d_{t,}\). If the on-policy distribution \(d_{t,}\) in \(_{Q}(w_{t}^{},_{t},d_{t,})\) is replaced by \(^{}(s,a)=d_{}^{^{}_{}}(s)\,_{A}(a)\), we define a transfer error \(_{Q}(w_{t}^{},_{t},^{})\). Let the covariance matrix of \(_{s,a}\) in any state-action distribution \(\) be \(_{}:=_{(s,a)}\,[\,_{s,a}_{s,a}^{}]\), and the relative condition number between \(_{}\) and \(_{^{}}\) be \(_{}:=_{w\,\,^{d}}_{^{} }w}{w^{}_{}w}\).

We make an assumption on the statistical error, the transfer error, and the relative condition number.

**Assumption 2**.: _(i) There exist \(_{}\), \(_{}>0\) such that \(\,[\,_{Q}(w_{t},_{t},d_{t,})-_{Q}(w_ {t}^{},_{t},d_{t,})\,]_{}\) and \(\,[\,_{Q}(w_{t}^{},_{t},^{})\,] _{}\); (ii) The relative condition number is finite, i.e., \(_{}<\)._

We assess the convergence of inexact RPG-PD via the distance metric \(\,[\,_{t}\,]:=\,[\,_{t}()\,]+ \,[\,(_{}^{}-_{t})^{2}\,]\), where the expectation \(\) is over the randomness of computing \(w_{t}\) via a sample-based algorithm. We state the convergence in Theorem 4 and delay its proof to Appendix C.6.

**Theorem 4** (Linear convergence of inexact RPG-PD).: _Let Assumptions 1-2 hold. If we take the stepsize \( 1/C_{W}\), then the primal-dual iterates of inexact RPG-PD satisfy_

\[[\,_{t+1}\,]  ^{- t}[\,_{1}\,]\,+\,\,(C_{W})^{2},(C_{r,}^{})^{2}\,\,+\,(}}+_{ }})\]

_where \(C_{W}:=2W/(1-)\) and \(C_{,}^{}:=(1+/)/(1-)\)._

Theorem 4 states that the primal-dual iterates of inexact RPG-PD converge to a neighborhood of \((_{}^{},_{}^{})\) in a linear rate. The convergence rate is \(\) and the size of neighborhood scales with a sum of an \(/\)-term and an \(1/\)-term that amplifies the effect of function approximation \((_{},_{})\). We note that, Theorem 4 does not require the strong duality in the parametrized policy class, and the function approximation error includes the duality gap caused by the inexpensiveness of function class and the policy representation error caused by the restricted policy set \((A)\). When there is no function approximation error, Theorem 4 has a similar result as Theorem 2. It is important to control \((_{},_{})\) to be small: (i) Application of stochastic gradient methods to the linear regression leads to \(_{}=O(1/)\) or \(O(1/K)\), where \(K\) is the number of gradient steps, and thus, it is easy to control \(_{}\); (ii) When \(_{0}\) is very small, the parametrized policy iterate can be contained in \((A)\), and thus \(_{}\) becomes zero in some cases, e.g., tabular softmax case  or low-rank MDPs [60; 61] with \(d|A|\); it can be made very small if the function class is rich, e.g., wide neural networks . When the errors are small, it is ready to establish Corollary 5; see Appendix C.7 for proof.

**Corollary 5** (Nearly-optimal constrained policy).: _Let Assumptions 1-2 hold and \(_{}\), \(_{}=O(^{8})\) for small \(\), \(_{0}>0\). If we take the stepsize \(=(^{4})\) and \(=(^{2})\), then the policy iterates of inexact RPG-PD satisfy_

\[[\,V_{r}^{^{*}}()-V_{r}^{_{_{t}}}()\, ]\ \ \ \ \ [\,-V_{g}^{_{_{t}}}()\,]\ \ \ \ t=(\,}^{2}\,)\]

_where \(()\) only has some problem-dependent constant._

Corollary 5 states that the iteration complexity in Corollary 3 holds in the function approximation case. When \(\) is small enough, we can design a conservative constraint such that the policy iterates of inexact RPG-PD satisfy \(V_{r}^{^{*}}()-V_{r}^{_{_{t}}}()\) and \(V_{g}^{_{_{t}}}() 0\) for large \(t\); see Appendix C.8 for proof. Compared with the zero average constraint violation , this appears to be the first policy-based zero constraint violation result in the function approximation setting. Moreover, we extend inexact RPG-PD to be a sample-based algorithm and provide its sample complexity in Appendix C.9.

## 4 Policy Last-Iterate Convergence: Optimistic Method

Having established sublinear policy last-iterate convergence via regularization, we turn to the optimistic gradient method  for a faster rate. We propose an optimistic method - Optimistic Policy Gradient Primal-Dual (OPG-PD) - that converges an optimal constrained policy at a linear rate.

### Optimistic policy gradient primal-dual method

We propose an optimistic policy gradient primal-dual (OPG-PD) method by maintaining two sequences for policy and dual variables each: \(\{_{t}\}_{t\,\,1}\) and \(\{_{t}\}_{t\,\,1}\) for the policy-player, and \(\{_{t}\}_{t\,\,1}\) for the dual-player,

\[_{t}(\,|\,s) = *{argmax}_{(\,|\,s)\,\,(A)}\{ _{a}(a\,|\,s)Q_{r+_{t-1}g}^{_{t-1}}(s,a)\,-\, \,\|(\,|\,s)-_{t}(\,|\,s)\|^{2}\}\] (9a) \[_{t+1}(\,|\,s) = *{argmax}_{(\,|\,s)\,\,(A)}\{ _{a}(a\,|\,s)Q_{r+_{t}g}^{_{t}}(s,a)\,-\,\, \|(\,|\,s)-_{t}(\,|\,s)\|^{2}\}\] (9b) \[_{t} = *{argmin}_{\,\,}\{\,V_{ g}^{_{t-1}}()\,+\,\,(-_{t})^{2}\}\] \[_{t+1} = *{argmin}_{\,\,}\{\,V_{ g}^{_{t}}()\,+\,\,(-_{t})^{2}\}\]

where \(\) is the stepsize and \((_{0},_{0})=(_{0},_{0})\) is the initial point. OPG-PD concurrently works with two primal iterates and two dual iterates, and each two are updated consecutively to stabilize the algorithm dynamics. The "optimistic" in optimization, e.g.,  views \((_{t+1},_{t+1})\)-update as a real policy gradient step and \((_{t},_{t})\)-update as a prediction step that generates an intermediate iterate \((_{t},_{t})\). Not policy gradient at \((_{t},_{t})\), the real step uses a policy gradient at \((_{t},_{t})\) from prediction, exhibiting the optimism towards the prediction. Specifically, Primal update (9a) works as the projected \(Q\)-ascent [58; 65], an application of the classical mirror descent with Euclidean distance , where the projection onto a probability simplex can be solved efficiently . Dual update (9b) performs standard projected gradient descent. We note that OPG-PD is different from the one-step multiplicative weights update in the policy-based ReLOAD .

When there is no MDP transition dynamics, i.e., constrained bandit , last-iterate convergence of OPG-PD to a saddle point is known in the minimax optimization [67; 68; 38; 39], because Problem (3) reduces to a bilinear zero-sum game in this case. However, it is prohibitive to apply such bilinear game results to the Lagrangian \(V_{r+ g}^{}(s)\) in every state \(s\), as has been done for zero-sum Markov games [43; 44]. The main reason for this is that there may not exist an optimal constrained policy that is uniformly optimal across all states; see Appendix B.2.

### Policy last-iterate convergence

We define the distribution mismatch coefficient over \(\) as \(_{}\,:=_{\,\,}\|d_{}^{}/\|_{}\), which is the maximum distribution mismatch of policy \(\) relative to \(\), where \(d_{}^{}/\) is divided per state. Hence, \(\|d_{}^{}/d_{}^{^{*}}\|_{}_{}/(1- )\) for any policy \(\) and \(_{} 1/_{}\) where \(_{}:=_{s}(s)\). The projection operator \(_{X}\) on a closed convex set \(X\) defines \(_{X}(x):=*{argmin}_{x^{}\,X}\|x^{}-x\|\).

We state the policy last-iterate convergence of OPG-PD (9) in Theorem 6.

**Theorem 6** (Linear convergence of OPG-PD).: _Let Assumption 1 hold. Assume the optimal state visitation distribution be unique, i.e., \(d_{}^{}=d_{}^{^{*}}\) for any \(^{}\), and \(_{}>0\). If we set the stepsize \(\,\,(1/(4),(1-)^{3}/(4|A|),(1-)^{3}/( 2_{}))\), where \(>0\) is defined in Appendix D.1, then the primal-dual iterates of OPG-PD (9) satisfy_

\[_{s}d_{}^{^{*}}(s)\|_{^{ }}(_{t}(\,|\,s))-_{t}(\,|\,s)\|^{2}\,+ \,(_{^{}}(_{t})-_ {t})^{2}  ()^{t}\]

_where \(C:=(7(1-)/8,7^{2}(1-)^{2}(C_{,})^{2}_{}/(6 _{,})^{2})\) in which \(C_{,}\) and \(_{,}\) are given by \(C_{,}:=c_{}/(2)/(1+1/((1-)))\), \(_{,}:=(_{}/(1-),1)\), and \(c>0\) is a problem-dependent constant from Lemma 26._

Theorem 6 states that the primal-dual iterates of OPG-PD converge to \(^{}^{}\) in a linear rate, or putting it differently, (9) is contracting to a set of optimal primal/dual variables. The rate is governed by a problem-dependent constant. Proof of Theorem 6 is provided in Appendix D. A key to our analysis is to bridge the per-state policy gradient update and the policy improvement for \(V_{r+ g}^{}()\) that is non-convex in policy \(\), which departs from the convex last-iterate analysis [38; 39]. In addition, we address two technical difficulties. First, the lack of uniformly optimal policies prevents learning an optimal policy from per-state bilinear games in zero-sum Markov games [43; 44]. Instead, we characterize the proximity of primal-dual iterates to a saddle point supported by an optimal state visitation distribution \(d_{}^{^{*}}\). Second, Problem (3) is an asymmetric game since one plays a stochastic policy over a finite set of discrete actions and controls the transition dynamics, but the other selects an action in a continuous interval. Thus, our dual-player analysis handles the long-term effect of the policy-player, which did not appear in the symmetric game [43; 44].

A direct corollary of Theorem 6 is stated below; see Appendix D.3 for the proof.

**Corollary 7** (Nearly-optimal constrained policy).: _Let Assumption 1 hold and the optimal state visitation distribution be unique, i.e., \(d_{}^{}=d_{}^{^{*}}\) for any \(^{}\), and \(_{}>0\). If we use the stepsize \(\) from Theorem 6, then the policy iterates of OPG-PD (9) satisfy_

\[V_{r}^{^{*}}()-V_{r}^{_{t}}()\ \ \ \ -V_{g}^{_{t}}()\ \ \ t=(^{2}\,)\]

_where \(()\) only has some problem-dependent constant._

Corollary 7 states that the last policy iterate of OPG-PD is an \(\)-optimal policy for Problem (1) after an almost constant number of iterations, which improves the sublinear rate in Corollary 3. OPG-PD also improves the average-value convergence of the single-time-scale methods [23; 54] and the two-time-scale methods [28; 34; 35; 24], and matches the last-iterate convergence rate of the two-time-scale methods [34; 35]. We stress that our last-iterate convergence indicates the stability of whole primal-dual iterates, which is not the last policy iterate from a subroutine [34; 35]. Again, when \(\) is small, we can design a conservative constraint such that the policy iterates of OPG-PD satisfy \(V_{r}^{^{*}}()-V_{r}^{_{t}}()\) and \(V_{g}^{_{t}}() 0\) for large \(t\); see Appendix D.4 for the proof.

## 5 Computational Experiment

We validate the effectiveness of RPG-PD (6) and OPG-PD (9) by comparing them with typical primal-dual methods in Figure 1. A few observations are in order. The initial oscillation of RPG-PD(--) is damped, and OPG-PD (--) is almost free of oscillation as PID Lagrangian (....). However, oscillation of NPG-PD (--) causes its last-iterate policy violating the constraint \(V_{}^{}() 0\). OPG-PD (--) reaches the maximum reward value in four methods and RPG-PD (--) converges to a slightly smaller value due to regularization, while both meet the constraint at the end. However, PID Lagrangian (...) is highly sub-optimal. Hence, our methods OPG-PD and RPG-PD can overcome oscillation and approach a nearly-optimal constrained policy in the last-iterate fashion.

We showcase the linear convergence of OPG-PD (9) with three constant stepsizes in Figure 2. Three policy optimality gaps decrease linearly in the logarithmic scale plot, which verifies the linear last-iterate convergence of OPG-PD's policy iterates in Theorem 6. Noticeably, there is no oscillation behavior in OPG-PD's policy iterates, which perhaps is best for learning constraints . We also see that a large stepsize \(=0.2\) improves the convergence, which is reflected by our rate.

Please see Appendix E for more details of this experiment, more baselines, and sensitivity analysis.

## 6 Concluding Remarks

We have presented two single-time-scale policy-based primal-dual methods for finding an optimal policy of a constrained MDP, with global non-asymptotic and last-iterate policy convergence guarantees. Our first regularized method enjoys a nearly dimension-free sublinear rate, while our second optimistic method possesses a linear rate that is problem-dependent. Our work stimulates a number of compelling future directions: (i) Our problem setting circumvents the exploration difficulty, which leaves online exploration open; (ii) Our convergence rates are not as sharp as solving convex-concave minimax optimization problems, regarding the order or instance-related constant; (iii) Last-iterate convergence is under-examined in constrained MDPs with other constraints, and unexplored for other gradient methods.

Figure 1: Convergence performance of RPG-PD, OPG-PD, and primal-dual methods. Learning curves of our RPG-PD (—) and OPG-PD (—), and NPG-PD  (—) and PID Lagrangian  (...) methods. The horizontal axes mean the policy iterations \(\{_{t}\}_{t 0}\) that are generated by each method and the vertical axes mean the value functions of the policy iterates \(\{_{t}\}_{t 0}\): reward value \(V_{}^{_{t}}()\) (Left) and utility value \(V_{g}^{_{t}}()\) (Right). In this experiment, we use the same stepsize \(=0.1\) for all methods, the regularization parameter \(=0.08\) for RPG-PD, and the uniform initial distribution \(\).

Figure 2: Convergence performance of OPG-PD with stepsize \(\): (\(=0.05\),...), (\(=0.1\), --), (\(=0.2\), --). The horizontal axis represents the policy iterations \(\{_{t}\}_{t 0}\) that are generated by OPG-PD and the vertical axis means the policy optimality gap that measures the distance of the policy iterates \(\{_{t}\}_{t 0}\) to an optimal policy \(^{}\): \(_{s}\|_{t}(\,|\,s)-^{}(\,|\,s)\|^{2}\). In this experiment, we take the initial distribution \(\) to be a uniform one.