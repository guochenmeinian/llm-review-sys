# A Heavy-Tailed Algebra for Probabilistic Programming

Feynman Liang

Department of Statistics

University of California, Berkeley

feynman@berkeley.edu &Liam Hodgkinson

School of Mathematics and Statistics

University of Melbourne, Australia

lhodgkinson@unimelb.edu.au

&Michael W. Mahoney

ICSI, LBNL, and Department of Statistics

University of California, Berkeley

mmahoney@stat.berkeley.edu

###### Abstract

Despite the successes of probabilistic models based on passing noise through neural networks, recent work has identified that such methods often fail to capture tail behavior accurately--unless the tails of the base distribution are appropriately calibrated. To overcome this deficiency, we propose a systematic approach for analyzing the tails of random variables, and we illustrate how this approach can be used during the static analysis (before drawing samples) pass of a probabilistic programming language (PPL) compiler. To characterize how the tails change under various operations, we develop an algebra which acts on a three-parameter family of tail asymptotics and which is based on the generalized Gamma distribution. Our algebraic operations are closed under addition and multiplication; they are capable of distinguishing sub-Gaussians with differing scales; and they handle ratios sufficiently well to reproduce the tails of most important statistical distributions directly from their definitions. Our empirical results confirm that inference algorithms that leverage our heavy-tailed algebra attain superior performance across a number of density modeling and variational inference (VI) tasks.

## 1 Introduction

Within the context of modern probabilistic programming languages (PPLs), recent developments in functional programming , programming languages , and deep variational inference (VI)  combine to facilitate efficient probabilistic modelling and inference. But despite the broadening appeal of probabilistic programming, common pitfalls such as mismatched distribution supports  and non-integrable expectations  remain uncomfortably commonplace and remarkably challenging to address. In particular, heavy-tailed distributions arise in a wide range of statistical applications and are known to present substantial technical challenges . Recent innovations aiming to improve PPLs have automated verification of distribution constraints , tamed noisy gradient estimates  as well as unruly density ratios , and approximated high-dimensional distributions with non-trivial bulks . To address the issue of heavy-tailed targets, approaches which initialize with non-Gaussian tails have been proposed . However, these methods typically require the use of optimization and/or sampling strategies to estimate the tails of the target distribution. Such strategies are often unstable, or fail to allow for a sufficiently wide array of possible tail behaviours.

Motivated by this, we introduce the first procedure for static analysis of a probabilistic program that automates analysis of target distributions' tails. In addition, we show how tail metadata obtained from this procedure can be leveraged by PPL compilers to generate inference algorithms which mitigate a number of pathologies. For example, importance sampling estimators can exhibit infinite variance if the tail of the approximating density is lighter than the target; most prominent black-box VI methods are incapable of changing their tail behaviour from an initial proposal distribution ; and Monte-Carlo Markov Chain (MCMC) algorithms may also lose ergodicity when the tail of the target density falls outside of a particular family . All of these issues could be avoided if the tail of the target is known before runtime.

To classify tail asymptotics, we propose a three-parameter family of distributions which is closed under most typical operations. This family is based on the generalized Gamma distribution (Equation (2)), and it interpolates between established asymptotics on sub-Gaussian random variables  and regularly varying random variables . Algebraic operations on random variables can then be lifted to computations on the tail parameters. This results in a _heavy-tailed algebra_ that we designate as the _generalized Gamma algebra (GGA)_. Through analyzing operations like \(X+Y\), \(X^{2}\), and \(X/Y\) at the level of densities (e.g., additive convolution \(p_{X} p_{Y}\)), the tail parameters of a target density can be estimated from the parameters of any input distributions using Table 1.

Operationalizing our GGA, we propose a tail inferential static analysis strategy analogous to traditional type inference. GGA tail metadata can be used to diagnose and address tail-related problems in downstream tasks, such as employing Riemannian-manifold methods  to sample heavy tails or preemptively detect unbounded expectations. Here, we consider density estimation and VI where we use the GGA-computed tail of the target density to calibrate our density approximation. When composed with a learnable Lipschitz pushforward map (Section 3.2), the resulting combination is a flexible density approximator with tails provably calibrated to match those of the target.

**Contributions.** Here are our main contributions.

* We propose the generalized Gamma algebra (GGA) as an example of a heavy-tailed algebra for probability distributions. This extends prior work on classifying tail asymptotics, while including both sub-Gaussian / sub-exponentials  as well as power-law / Pareto-based tail indices . Composing operations outlined in Table 1, one can compute the GGA tail class for downstream random variables of interest.
* We implement GGA as abstract interpretation during the static analysis phase of a PPL compiler. This unlocks the ability to leverage GGA metadata in order to better tailor MCMC and VI algorithms produced by a PPL.
* Finally, we demonstrate that density estimators which combine our GGA tails with neural networks (autoregressive normalizing flows  and neural spline flows ) simultaneously achieves calibrated tails without sacrificing good bulk approximation.

Figure 1: Our heavy-tailed algebra ensures that the tails of density estimators and variational approximations are calibrated to those of the target distribution. Here, a generative model expressed in a PPL (1) is analyzed using the GGA _without drawing any samples_ (2) to compute the tail parameters of the target. A representative distribution with calibrated tails is chosen for the initial approximation (3), and a learnable tail-invariant Lipschitz pushforward (see bottom of Table 1, and Theorem 2) is optimized (4) to correct the bulk approximation.

The Generalized Gamma Algebra

First, we formulate our heavy-tailed algebra of random variables that is closed under most standard elementary operations (addition, multiplication, powers). The central class of random variables under consideration are those with tails of the form in Definition 1.

**Definition 1**.: A random variable \(X\) is said to have a _generalized Gamma tail_ if the Lebesgue density of \(|X|\) satisfies

\[p_{|X|}(x)=x^{}e^{- x^{}},x, \]

for some \(c>0\), \(\), \(>0\) and \(\). Denote the set of all such random variables by \(\).

Consider the following equivalence relation on \(\): \(X Y\) if and only if \(0<p_{|X|}(x)/p_{|Y|}(x)<+\) for all sufficiently large \(x\). The resulting equivalence classes can be represented by their corresponding parameters \(,,\). Hence, we denote the class of random variables \(X\) satisfying Equation (1) by \((,,)\). In the special case where \(=0\), for a fixed \(<-1\), each class \((,,0)\) for \(>0\) is equivalent, and is denoted by \(_{||}\), representing _regularly varying_ tails. Our algebra operates on these equivalence classes of \(\), characterizing the change in tail behaviour under various operations. To incorporate tails which lie outside of \(\), we let \(_{1}\) incorporate _super-heavy tails_, which denote random variables with tails heavier than any random variable in \(\). All operations remain consistent with this notation. Likewise, we let \(\) denote _super-light tails_, which are treated in our algebra as a class where \(=+\) (effectively constants).

Equation (1) and the name of the algebra are derived from the generalized Gamma distribution.

**Definition 2**.: Let \(\), \(>0\), and \(\{0\}\) be such that \((+1)/>0\). A non-negative random variable \(X\) is _generalized Gamma distributed_ with parameters \(,,\) if it has Lebesgue density

\[p_{,,}(x)=c_{,,}x^{}e^{- x^{}}, x >0, \]

where \(c_{,,}=^{(+1)/}/((+1)/)\) is the normalizing constant.

The importance of the generalized Gamma form arises due to a combination of two factors:

1. The majority of interesting continuous univariate distributions with infinite support satisfy Equation (1), including Gaussians (\(=0\), \(=2\)), gamma/exponential/chi-squared (\(>-1\), \(=1\)), Weibull/Frechet (\(=+1\)), and Student _T_/Cauchy/Pareto (\(_{}\)). A notable exception is the log-normal distribution (see Example 8 in Appendix C).
2. The set \(\) is known to be closed under additive convolution, positive powers, and Lipschitz functions. We prove it is closed under multiplicative convolution as well. This covers the majority of elementary operations on independent random variables. Reciprocals, exponentials and logarithms comprise the only exceptions, however, we will introduce a few "tricks" to handle these cases as well.

The full list of operations in GGA is compiled in Table 1 and described in detail in Appendix A. GGA classes for common probability distributions are provided in Appendix B. All operations in the GGA can be proven to exhibit identical behaviour with their corresponding operations on random variables, with the sole exception of reciprocals (marked by \(\)), where additional assumptions are required. The asymptotics for operations marked with an asterisk are novel to this work. For further details, refer to Appendix A.

Repeated applications.Provided independence holds, composition of operations in the GGA remain consistent unless one applies Lipschitz functions, logarithms, or exponentials. If one of these operations is applied, the tail becomes an upper bound, which remains consistent under addition, multiplication, and powers, but not reciprocals. Given that we are working with a fixed class of tails, such behavior is inevitable, and it is possible to perform a sequence of operations for which the tail no longer becomes accurate.

Posterior distributions.A primary application of PPLs is to perform Bayesian inference. To cover this use case, it is necessary to prescribe a procedure to deal with _posterior distributions_. Consider a setup where a collection of random variables \(X_{1},,X_{n}\) are dependent on corresponding latent random elements \(Z_{1},,Z_{n}\) as well as a parameter \(\) through functions \(f_{i}\) by \(X_{i}=f_{i}(,Z_{i})\). For simplicity, we assume that each \(f_{i}=f_{i,k} f_{i,k-1} f_{i,1}\) where each \(f_{ij}\) is an elementary operation in Table 1. To estimate the tail behaviour of \(\) conditioned on \(X\), we propose an elementary approach involving inverses. For each operation \(f_{ij}\), if \(f_{ij}\) is a power, reciprocal, or multiplication operation, let \(R_{ij}\) be given according to the following:

  
**Ordering** & \((_{1},_{1},_{1})\) & \(_{x}}e^{-_{1}_{1}}}{x^{_{2} }e^{-_{2}_{2}^{2}}}<+\). \\ 
**Addition** & \((_{1},_{1},_{1})\\ \\ (_{2},_{2},_{2})=\{(_{1},_{1 },_{1}),(_{2},_{2},_{2})\}&_{1}_{2}_{1}, _{2}<1\\ (_{1}+_{2}+1,\{_{1},_{2}),1\}&_{1}=_{2}=1\\ (_{1}+_{2}+-_{1}}{2},(_{1}^{-} +_{2}^{-}})^{1-},})&=_{1}=_{2}>1. \) \\ 
**Powers** & \((,,)^{}\) & \((-1,,)\) for \(>0\) \\ 
**Reciprocal***\(\) & \((,,)^{-1}\) & \((--2,,-)&(+1)/>0 0\\ _{2}&\) \\ 
**Scalar Multiplication** & \(c(,,)\) & \((,|c|^{-},)\) \\ 
**Multiplication*** & & \((_{1},_{1},_{1})\\ (_{2},_{2},_{2})=(( }{_{1}}+}{_{2}}-),, )&_{1},_{2}<0\\ _{|_{1}|}&_{1},_{2}>0\\ _{\{|_{1}|,|_{2}|\}}&_{1} 0,_{2}>0\\ _{\{|_{1}|,|_{2}|\}}&_{1}=0,_{2}=0 \\ \) \\ 
**Multiplication*** & & \(=|}+|}=|+|_{2}|}{ |_{1}_{2}|}\) \\ 
**Multiplication*** & & \(=|}+|}=|+|_{2}|}{ |_{1}_{2}|}\) \\ 
**Multiplication*** & & \(=|}+|}=|+|_{2}|}{ |_{1}_{2}|}\) \\ 
**Exponentials***\(\) & \((,,)\) & \(_{+1}& 1\\ _{1}&\) \\ 
**Logarithms***\(\) & \((,,)\) & \((0,||-1,1)&<-1\\ &\) \\ 
**Functions** & & \\ (\(L\)-Lipschitz) & & \\   

Table 1: _The Generalized Gamma Algebra._ Operations on random variables (e.g., \(X_{1}+X_{2}\)) are viewed as actions on density functions (e.g., convolution \((_{1},_{1},_{1})(_{2},_{2},_{2})\)) and the tail parameters of the result are analyzed and reported. In this table, * denotes novel results, and \(\) denotes that additional assumptions are required.

Letting \(f_{i}^{-1}(x,z)\) denote the inverse of \(f_{i}\) in the first argument, we show in Appendix A,

\[|=^{n}}_{i=1}^{n} f_{i}^{-1}(,Z_{i})\&^{n,k}}_ {i,j=1}R_{ij}\&\,,\]

where \(\) denotes the prior for \(\). Since the inverse of a composition of operations is a composition of inverses, the tail of \(f_{i}^{-1}(,Z_{i})\) can be determined by backpropagating through the computation graph for \(X_{i}\) and sequentially applying inverse operations. Consequently, the tail behaviour of the posterior distribution for one parameter can be obtained using a single backward pass. Posterior distributions for multiple parameters involve repeating this procedure one parameter at a time, with other parameters fixed.

## 3 Implementation

### Compile-time static analysis

To illustrate an implementation of GGA for static analysis, we sketch the operation of the PPL compiler at a high-level and defer to the code in Supplementary Materials for details. A probabilistic program is first inspected using Python's built-in ast module and transformed to static single assignment (SSA) form . Next, standard compiler optimizations (e.g., dead code elimination, constant propagation) are applied and an execution of the optimized program is traced  and accumulated in a directed acyclic graph representation. A breadth-first type checking pass, as seen in Algorithm 1, completes in linear time, and GGA results may be applied to implement computeGGA() using the following steps:

* If a node has no parents, then it is an atomic distribution and its tail parameters are known (Table 5);
* Otherwise, the node is an operation taking its potentially stochastic inputs (parents) to its output. Consult Table 1 for the output GGA tails.

### Representative distributions

For each \((,,)\), we make a carefully defined choice of \(p\) on \(\) such that if \(X p\), then \(X(,,)\). This way, any random variable \(f(X)\), where \(f\) is \(1\)-Lipschitz, will exhibit the correct tail, and so approximations of this form may be used for VI or density estimation. Let \(X(,,)\) and \(0< 1\) denote a small parameter such that tails \(e^{-x^{}}\) are deemed to be "very heavy" (we chose \(=0.1\)). Inspired by ideas from implicit renewal theory , our candidate distributions are as follows.

(\( 0\)): If \( 0\), then \(p_{X}(x) cx^{-||}\). One such density is the _Student \(t\) distribution_, in this case, with \(||-1\) degrees of freedom if \(<-1\) (generate \(X(||-1)\)). (\(>\)): For moderately sized \(>0\), the symmetrization of the generalized Gamma density (2). (\(\)): If \(X(,,)\) where \(\) is small, then \(X\) will exhibit much heavier tails, and the generalized Gamma distribution in Case 1 will become challenging to sample from. In these cases, we expect that the tail of \(X\) should be well represented by a power law. The generalized Gamma density (Equation (2)) satisfies \(X^{r}=^{-r/}()/()\) for \(r>0\). Let \(>0\) be such that \(X^{}=2\). By Markov's inequality, the tail of \(X\) satisfies \((X>x) 2x^{-}\). Therefore, we can represent tails of this form by the Student \(t\) distribution with \(+1\) degrees of freedom (generate \(X()\)).

### Bulk correction by Lipschitz mapping

While a representative distribution will exhibit the desired tails, the target distribution's bulk may be very different from a generalized Gamma and result in poor distributional approximation. To address this, we propose splicing together the tails from a generalized Gamma with a flexible density approximation for the bulk. While many combinations are possible, in this work we rely on the Lipschitz operation in the GGA (Theorem 2) and post-compose neural spline flows  (which are identity functions outside of a bounded interval hence \(1\)-Lipschitz) after properly initialized generalized Gamma distributions. Optimizing the parameters of the flow results in good bulk approximation while simultaneously preserving the tail correctness guarantees attained by the GGA.

**Example 1**.: Let \(A^{k k}\), \(x,y^{k}\), with \(x_{i},y_{i},A_{ij}}{}(-1,1)\). The distribution of \(x^{}Ay=_{i,j}x_{i}A_{ij}y_{j}\) is a convolution of normal-powers  and lacks a closed form expression. Using the GGA (Table 1), one can compute its tail parameters to be \((-1,,)\). The candidate given by the GGA representative distribution (Section 3.2) is a gamma distribution with correct tail behaviour, but is a poor approximation otherwise. A learnable Lipschitz bijection is optimized to correct the bulk approximation (Figure 2(i)). From the Lipschitz property, the slope of the tail asymptotics in log-log scale remains the same before and after applying the flow correction (Figure 2(ii)): the tails are _guaranteed_ to remain calibrated.

**Example 2**.: Consider \(_{i=1}^{4}X_{i}^{2}\) where \(X_{i}(i)\). While we are not aware of a closed-form expression for the density, this example is within the scope of our GGA. Empirical results illustrate that our method (Figure 3(i)) accurately models _both_ the bulk and the tail whereas Gaussian-based Lipschitz flows (Figure 3(ii)) inappropriately impose tails which decay too rapidly.

## 4 Experiments

We now demonstrate that GGA-based density estimation yields improvements in tail estimation across several metrics. Our experiments consider normalizing flows initialized from (i) the parametric family defined in Section 3.2 against (ii) a normal distribution (status quo). To further contrast the individual effect of using a GGA base distribution over standard normals against more expressive pushforward maps , we also report ablation results where normalizing flows are replaced by affine transforms, as originally proposed in . All experiments are repeated for 100 trials, trained to convergence using the Adam optimizer with manually tuned learning rate. Additional details are available in Appendix D. All target distributions in this section are expressed as generative PPL programs: Cauchy using a reciprocal normal; Chi2 (chi-squared) using a sum of squared normals; IG (Inverse Gamma) using a reciprocal exponential; normal using a sum of normals; and StudentT using a normal and Cauchy ratio. Doing so tasks the static analyzer to infer the target's tails and makes the analysis non-trivial.

Our results in the following tables share a consistent narrative where a GGA base distribution rarely hurts and can significantly help with heavy tailed targets. Standard evaluation metrics such as negative cross-entropy, ELBO, or importance-weighted autoencoder bounds  do not evaluate the quality of tail approximations. Instead, we consider diagnostics which do: namely, an estimated tail exponent

Figure 2: The **candidate** distribution chosen by the GGA calibrates tails to the target, but with incorrect bulk. A Lipschitz normalizing flow corrects the bulk (i) without changing the tail behaviour, as seen by the parallel tail asymptotics (black dashed lines) in (ii).

\(\), and the Pareto \(\) diagnostic . Except for when targets are truly light tailed (\(=\) in Chi2 and normal), GGA-based approximations are the only ones to reproduce appropriate GPD tail index \(\) in density estimation and achieve a passing \(\) below \(0.2\) in VI. Less surprising is the result that adding a flow improved approximation metrics, as we expect the additional representation flexibility to be beneficial.

Density Estimation.Given samples \(\{x_{i}\}_{i=1}^{N}\) from a target density \(p\), we minimize a Monte-Carlo estimate of the cross entropy \(H(p,q)=-E_{p}[ q(X)]-_{i=1}^{N} q(x_{i})\). The results are shown in Table 2 and Table 3 along with power-law tail index estimates \(\). Closeness between the target Pareto tail index \(\) and its estimate \(\) in \(q(x)\) suggest calibrated tails. Overall, we see that normal (resp., Cauchy) based flows fails to capture heavy (resp., light) tails, while GGA-based flows yield good tail approximations (lower NLL, \(\) closer to target) across all cases.

Variational Inference.For VI, the bulk is corrected through the ELBO optimization objective \(E_{q}_{i=1}^{N})}{ q(x_{i})}, x_{i} q\). Since the density \(p\) must also be evaluated, for simplicity, experiments in Table 4 use closed-form marginalized densities for targets. The overall trends also show that GGA yields consistent improvements; the \(\) diagnostic  indicates VI succeeds (\( 0.2\)) when a GGA with appropriately matched tails is used and fails (\(>1\)) when Gaussian tails are erroneously imposed.

   Target & \(\) & Cauchy (\(=2\)) Flow & GGA Flow & Normal (\(=\)) Flow \\  Cauchy & \(2\) & \(\) (\(\)) & \(\) (\(\)) & \(7.7\) (\(2.5\)) \\ IG & \(2\) & \(\) (\(\)) & \(\) (\(\)) & \(7.3\) (\(1.7\)) \\ StudentT & \(3\) & \(2.0\) (\(0.06\)) & \(\) (\(\)) & \(7.7\) (\(2.3\)) \\ Chi2 & \(\) & \(2.1\) (\(0.07\)) & \(\) (\(\)) & \(\) (\(\)) \\ Normal & \(\) & \(2.9\) (\(0.6\)) & \(\) (\(\)) & \(\) (\(\)) \\   

Table 2: Mean and standard errors (100 trials) of tail parameters \(\) (smaller for heavier tails) for various density estimators and targets.

Figure 3: Q-Q plots of density approximations of a heavy-tailed target (\(_{i=1}^{4}X_{i}^{2}\) where \(X_{i}(i)\)) initialized by our GGA candidate (i) and the Gaussian distribution (ii). While the expressive modeling capability of flows enables good approximation of the distribution bulk, Lipschitz transformations of Gaussians inevitably impose miscalibrated squared exponential tails which are not sufficiently heavy as evidenced in (ii).

   Target & \(\) & Cauchy (\(=2\)) Flow & GGA Flow & Normal (\(=\)) Flow \\  Cauchy & \(2\) & \(\) (\(\)) & \(\) (\(\)) & \( 10^{3}\) (\(6 10^{3}\)) \\ IG & \(2\) & \(\) (\(0.08\)) & \(\) (\(\)) & \( 10^{4}\) (\(6 10^{3}\)) \\ StudentT & \(3\) & \(\) (\(\)) & \(\) (\(0.04\)) & \(\) (\(0.47\)) \\ Chi2 & \(\) & \(\) (\(0.05\)) & \(\) (\(\)) & \(\) (\(0.04\)) \\ Normal & \(\) & \(\) (\(0.03\)) & \(\) (\(\)) & \(\) (\(\)) \\   

Table 3: Mean and standard errors of log-likelihoods \(E_{p} q(X)\) for various density estimators and targets. While larger values imply a better overall approximation (row max bolded), log-likelihood is dominated by bulk approximation so these results show that our method (GGA Flow) does not sacrifice bulk approximation quality.

Bayesian linear regression.As a practical example of VI applied to posterior distributions, we consider the setting of one-dimensional Bayesian linear regression (BLR) with conjugate priors, defined by the likelihood \(y|X,,(X,^{2})\) with a Gaussian prior \(|^{2}(0,^{2})\) on the coefficients, and an inverse-Gamma prior with parameters \(a_{0}\) and \(b_{0}\) on the residual variance \(^{2}\). The posterior distribution for \(\) conditioned on \(^{2}\) and \(X,y\) is Gaussian. However, conditional on the pair \((X,y)\), \(^{2}\) is inverse-Gamma distributed with parameters \(a_{0}+\) and \(b_{0}+(y^{}y-^{}))\), where \(=^{-1}X^{}X\) for \(\) the least-squares estimator, and \(=X^{}X+I\). Since \(^{2}\) is positive, it is typical for PPL implementations to apply an exponential transformation. Hence, a Lipschitz normalising flow starting from a Gaussian initialization will inappropriately approximate the inverse Gamma distributed \(p(^{2}|X,y)\) with log-normal tails. On the other hand, Lipschitz flows starting from a GGA reference distribution will exhibit the correct tails. We assess this discrepancy in Figure 4 under an affine transformation on four subsampled datasets: super (superconductor critical temperature prediction dataset  with \(n=256\) and \(d=154\)); who (life expectancy data from the World Health Organisation in the year 2013  with \(n=130\), \(d=18\)); air (air quality data  with \(n=6941\), \(d=11\)); and blog (blog feedback prediction dataset  with \(n=1024\), \(d=280\)). In Figure 4(i), the GGA-based method seems to perfectly fit to the targets, while in Figure 4(ii), the standard Gaussian approach fails to capture the tail behaviour.

Invariant distribution of SGD.For inputs \(X\) and labels \(Y\) from a dataset \(\), the least squares estimator for linear regression satisfies \(=_{}_{X,Y}(Y-X)^{2}\). To solve for this estimator, one can apply stochastic gradient descent (SGD) sampling over independent \(X_{k},Y_{k}\) to obtain the sequence of iterations

\[_{k+1}=(I- X_{k}X_{k}^{})_{k}+ Y_{k}X_{k}\]

for a step size \(>0\). For large \(\), the iterates \(_{k}\) typically exhibit heavy-tailed fluctuations . In this regard, this sequence of iterates has been used as a simple model for more general stochastic optimization dynamics . In particular, generalization performance has been tied to the heaviness of the tails in the iterates . Here, we use our algebra to predict the tail behaviour in a simple one-dimensional setting where \(X_{k}(0,^{2})\) and \(Y_{k}(0,1)\). From classical theory , it is known that \(X_{k}\) converges in distribution to a power law with tail exponent \(>0\) satisfying \(|1- X_{k}^{2}|^{}=1\). In Figure 5, we plot the density of the representative for \(_{10^{4}}\) obtained using our

   Target & \(\) & Normal Affine & Normal Flow & GGA Affine & GGA Flow \\  Cauchy & \(=2\) & \(0.62\) (\(0.26\)) & \(0.22\) (\(0.059\)) & \(0.68\) (\(0.038\)) & \(\) (\(\)) \\ IG & \(=2\) & \(8.6\) (\(1.8\)) & \(8.2\) (\(2.3\)) & \(2.0\) (\(0.4\)) & \(2.9\) (\(0.71\)) \\ StudentT & \(=3\) & \(1.2\) (\(0.16\)) & \(1.0\) (\(0.43\)) & \(1.5\) (\(0.082\)) & \(1.3\) (\(0.097\)) \\ Chi2 & \(=\) & \(0.57\) (\(0.081\)) & \(0.61\) (\(0.067\)) & \(\) (\(\)) & \(\) (\(\)) \\ Normal & \(=\) & \(0.53\) (\(0.17\)) & \(0.21\) (\(0.067\)) & \(0.4\) (\(0.086\)) & \(\) (\(\)) \\   

Table 4: Pareto \(\) diagnostic () to assess goodness of fit for VI (mean across 100 trials, standard deviation in parenthesis) on targets of varying tail index (smaller \(=\) heavier tails). A value \(>0.2\) is interpreted as potentially problematic so only values not exceeding it are bolded.

Figure 4: Estimated densities for the posterior distribution of \(^{2}\) in Bayesian linear regression under optimised exponential + affine transformations from (i) GGA reference, and (ii) Gaussian reference.

algebra against a kernel density estimate using \(10^{6}\) samples when \(\{0.4,0.5,0.6\}\) and \(=2\). In all cases, the density obtained from the algebra provides a surprisingly close fit.

## 5 Related Work

**Heavy tails and probabilistic machine learning.** For studying heavy tails, methods based on subexponential distributions  and generalized Pareto distributions (GPD) (or equivalently, regularly varying distributions ) have received significant attention historically. For example,  presents closure theorems for regularly varying distributions which are special cases of Proposition 1 and Theorem 2. Heavy tails often have a profound impact on probabilistic machine learning methods: in particular, the observation that density ratios \(\) tend to be heavy tailed has resulted in new methods for smoothing importance sampling , adaptively modifying divergences , and diagnosing VI through the Pareto \(\) diagnostic . These works are complementary to our paper, and our reported results include \(\) diagnostics for VI and \(\) tail index estimates based on GPD.

Our work considers heavy-tailed targets \(p(x)\) which is the same setting as [25; 33]. Whereas those respective works lump the tail parameter in as another variational parameter and may be more generally applicable, the GGA may be applied before samples are drawn and leads to perfectly calibrated tails when applicable.

**Probabilistic programming.** PPLs can be broadly characterized by the inference algorithms they support, such as: Gibbs sampling over Bayes nets [13; 48], stochastic control flow [19; 58], deep stochastic VI [4; 52], or Hamiltonian Monte-Carlo [8; 59]. Our implementation target beamline is a declarative PPL selected due to availability of a PPL compiler and support for static analysis plugins. Similar to [4; 46], it uses PyTorch  for GPU tensors and automatic differentiation. Synthesizing an approximating distribution during PPL compilation (Section 3) is also performed in the Stan language by  and normalizing flow extensions in . We compare directly against these related density approximators in Section 4.

**Static analysis.** There is a long history of formal methods and probabilistic programming in the literature [26; 29], with much of the research  concerned with defining formal semantics and establishing invariants  (see  for a recent review). Static analysis uses the abstract syntax tree (AST) representation of a program in order to compute invariants (e.g., the return type of a function, the number of classes implementing a trait) without executing the underlying program. It has traditionally been applied in the context of formalizing semantics , and has been used to verify probabilistic programs by ensuring termination, bounding random values values . As dynamic analysis in a PPL is less reliable due to non-determinism, static analysis techniques for PPLs become essential. As recent examples,  proposes a static analyzer for the Pyro PPL  to verify distribution supports and avoid \(-\) log probabilities. More relevant to our work are applications of static analysis to improve inference.  and  both employ static analysis to inform choice of inference method. However, both works do not account for heavy tails whereas the primary goal of GGA-based analysis is to ensure tails are properly modelled.

## 6 Conclusion

In this work, we have proposed a novel systematic approach for conducting tail inferential static PPL analysis. We have done this by defining a heavy-tailed algebra, and by implementing a three-parameter generalized Gamma algebra into a PPL compiler. Initial results are promising, showing that improved inference with simpler approximation families is possible when combined with tail

Figure 5: Kernel density estimate of iterates of SGD (blue) vs. GGA predicted tail behaviour (orange)

metadata. While already useful, the generalized Gamma algebra and its implementation currently has some notable limitations:

* The most significant omission to the algebra is classification of log-normal tails. Addition may be treated using , but multiplication with log-normal tails remains elusive.
* Since the algebra assumes independence, handling of dependencies between defined random variables must be conducted externally. This can be addressed using a symbolic package to decompose complex expressions into operations on independent random variables.
* Scale coefficients \(\) for conditional distributions may often be inexact, as exact marginalization in general is NP-hard . Treatment of disintegration using symbolic manipulations is a significant open problem, with some basic developments .
* Compile-time static analysis is only applicable to fixed model structures. Control flow, open-universe models , and PPLs to support them  are an important future research direction.