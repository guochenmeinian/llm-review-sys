# Adversarial Schrodinger Bridge Matching

Nikita Gushchin

Sklotech\({}^{}\)

Moscow, Russia

n.gushchin@skoltech.ru

&Daniil Selikhanovych

Skoltech\({}^{}\)

Moscow, Russia

selikhanovychdaniil@gmail.com

Sergei Kholkin

Skoltech\({}^{}\)

Moscow, Russia

s.kholkin@skoltech.ru

&Evgeny Burnaev

Skoltech\({}^{}\)

Moscow, Russia

e.burnaev@skoltech.ru

&Alexander Korotin

Skoltech\({}^{}\)

Moscow, Russia

a.korotin@skoltech.ru

Equal contribution

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

###### Abstract

The Schrodinger Bridge (SB) problem offers a powerful framework for combining optimal transport and diffusion models. A promising recent approach to solve the SB problem is the Iterative Markovian Fitting (IMF) procedure, which alternates between Markovian and reciprocal projections of continuous-time stochastic processes. However, the model built by the IMF procedure has a long inference time due to using many steps of numerical solvers for stochastic differential equations. To address this limitation, we propose a novel Discrete-time IMF (D-IMF) procedure in which learning of stochastic processes is replaced by learning just a few transition probabilities in discrete time. Its great advantage is that in practice it can be naturally implemented using the Denoising Diffusion GAN (DD-GAN), an already well-established adversarial generative modeling technique. We show that our D-IMF procedure can provide the same quality of unpaired domain translation as the IMF, using only several generation steps instead of hundreds. We provide the code at https://github.com/Daniil-Selikhanovych/ASBM.

Figure 1: Our D-IMF approach performs unpaired image-to-image translation in just a few steps, achieving results comparable to the hundred-step IMF . Celeba , _male\(\)female_ (\(128 128\)).

Introduction

Recent generative models based on the Flow Matching  and Rectified Flows  show great potential as a successor of classical denoising diffusion models such as DDPM . Both these approaches consider the same problem of learning an Ordinary Differential Equation (ODE) that interpolates one given distribution to the other one, e.g., noise to data. Thanks to the close connection to the theory of Optimal Transport (OT) problem , Flow Matching and Rectified Flows approaches typically have faster inference compared to classical diffusion models . Also, it was shown that they can outperform diffusion models on the high-resolution text-to-image synthesis: they even lie in the foundation of the recent Stable Diffusion 3 model .

The extension of Flow Matching and Rectified Flow approaches to the SDE are Bridge Matching (Markovian projection) and **Iterative Markovian fitting** (IMF) procedures , respectively. They also have a close connection with the OT theory. Specifically, it is known  that IMF converges to the solution of the dynamic formulation of entropic optimal transport (EOT), also known as the Schrodinger Bridge (SB). However, learning continuous-time SDEs in IMF is non-trivial and, unfortunately, leads to **long inference** due to the necessity to use many steps of numerical solvers.

**Contributions.** This paper addresses the above-mentioned limitation of the existing Iterative Markovian Fitting (IMF) framework by introducing a novel approach to learn the Schrodinger Bridge.

1. **Theory I.** We introduce a Discrete Iterative Markovian Fitting **(D-IMF)** procedure (SS3.2, 3.3), which innovatively applies discrete Markovian projection to solve the Schrodinger Bridge problem without relying on Stochastic Differential Equations. This approach significantly simplifies the inference process, enabling it to be accomplished (theoretically) in just a few evaluation steps.
2. **Theory II.** We derive closed-form update formulas for the D-IMF procedure when dealing with high-dimensional Gaussian distributions. This advancement permits a detailed empirical analysis of our method's convergence rate and enhances its theoretical foundation (SS3.4, 4.1).
3. **Practice.** For general data distributions available by samples, we propose an algorithm **(ASBM)** to implement the discrete Markovian projection and our D-IMF procedure in practice (SS4.2). Our algorithm is based on adversarial learning and Denoising Diffusion GAN . Our learned SB model uses just \(4\) evaluation steps for inference (SS3.5) instead of hundreds of the basic IMF .

**Notations.** In the paper, we simultaneously work with the continuous stochastic processes and discrete stochastic processes in the \(D\)-dimensional Euclidean space \(^{D}\). We denote by \((C(),^{D})\) the set of continuous stochastic processes with time \(t\), i.e., the set of distributions on continuous trajectories \(f:^{D}\). We use \(dW_{t}\) to denote the differential of the standard Wiener process.

To establish a link between continuous and discrete stochastic processes, we fix \(N 1\) intermediate time moments \(0=t_{0}<t_{1}<<t_{N}<t_{N+1}=1\) together with \(t_{0}=0\) and \(t_{N+1}=1\). We consider discrete stochastic processes with those time-moments as the elements of the set \((^{D(N+2)})\) of probability distributions on \(^{D(N+2)}\). Among such discrete processes, we are specifically interested in subset \(_{2,ac}(^{D(N+2)})(^{D (N+2)})\) of absolutely continuous distributions on \(^{D(N+2)}\) which have a finite second moment and entropy. For any such \(q_{2,ac}(^{D(N+2)})\), we write \(q(x_{0},x_{t_{1}},,x_{t_{N+1}})\) to denote its density at a point \((x_{0},x_{t_{1}},,x_{t_{N}},x_{1})^{D(N+2)}\). For continuous process \(T\), we denote by \(p^{T}(^{D(N+2)})\) the discrete process which is the finite-dimensional projection of \(T\) to time moments \(0=t_{0}<t_{1}<<t_{N}<t_{N+1}=1\). For convenience we also use the notation \(x_{}=(x_{t_{1}},,x_{t_{N}})\) to denote the vector of all intermediate-time variables. In what follows, KL is a short notation for the Kullback-Leibler divergence.

## 2 Background

We start with recalling the Bridge Matching and Iterative Propotional Fitting procedures developed for continuous-time stochastic processes (SS2.1). Next, we discuss the Schrodinger Bridge problem, the solution to which is the unique fixed point of Iterative Markovian Fitting procedure (SS2.2).

### Bridge Matching and Iterative Markovian Fitting Procedures

Modern diffusion and flow generative modeling are mainly about the construction of a model that interpolates one probability distribution \(p_{0}_{2,ac}(^{D})\) to some another probability distribution \(p_{1}_{2,ac}(^{D})\). One of the general approaches for this task is the Bridge Matching .

**Reciprocal processes.** The Bridge Matching procedure is applied to the processes, which are represented as a mixture of Brownian Bridges. Consider the Wiener process \(W^{}\) with the volatility \(\) which start at \(p_{0}\), i.e., the process given by the SDE: \(dx_{t}=dW_{t}\), \(x_{0} p_{0}\). Let \(W^{}_{|x_{0},x_{1}}\) denote the stochastic process \(W^{}\) conditioned on values \(x_{0},x_{1}\) at times \(t=0,1\), respectively. This process \(W^{}_{|x_{0},x}\) is called the Brownian Bridge [17, Chapter 9]. For some \(q(x_{0},x_{1})_{2,ac}(^{D 2})\) with \(q(x_{0})=p_{0}(x_{0})\) and \(q(x_{1})=p_{1}(x_{1})\) the process \(T_{q}}}{{=}} W^{}_{|x_{0},x_{1} }dq(x_{0},x_{1})\) is called the mixture of Brownian Bridges. Following , we say that mixtures of Brownian Bridges form a _reciprocal class_ of processes (for the Brownian Bridge). For brevity, we call these processes just reciprocal processes.

**Bridge matching .** The goal of Bridge Matching (with the Brownian Bridge) is to construct continuous-time Markovian process \(M\) from \(p_{0}\) to \(p_{1}\) in the form of SDE: \(dx_{t}=v(x_{t},t)dt+dW_{t}\). This is achieved by using the _Markovian projection_ of a reciprocal process \(T_{q}= W^{}_{|x_{0},x_{1}}dq(x_{0},x_{1})\), which aims to find the Markovian process \(M\) which is the most similar to \(T_{q}\) in the sense of KL:

\[_{}(T_{q})}}{{=}} *{arg\,min}_{M}(T_{q}\|M),\]

where \((C(),^{D})\) is the set of all Markovian processes. For the Brownian Bridge \(W^{}_{|x_{0},x_{1}}\) it is known  that the SDE and the drift \(v(x_{t},t)\) of \(_{}(T_{q})\) is given by:

\[dx_{t}=v(x_{t},t)dt+dW_{t}, v(x_{t},t)=-x_ {t}}{1-t}p^{T_{q}}(x_{1}|x_{t})dx_{1},\]

where \(p^{T_{q}}(x_{1}|x_{t})\) the conditional distribution of the stochastic process \(T_{q}\) at time moments \(t\) and \(1\). The process \(_{}(T_{q})\) has the same time marginal distributions \(p^{T_{q}}(x_{t})\) as the original Brownian bridge mixture \(T_{q}\). However, the joint distribution \(p^{T_{q}}(x_{0},x_{1})\) of \(T_{q}\) and the joint distribution \(p^{_{}(T_{q})}(x_{0},x_{1})\) of its projection \(_{}(T_{q})\) do not coincide in the general case , see Figure 2.

**Iterative Markovian Fitting .** The Iterative Markovian Fitting procedure introduces a second type of projection of continuous-time stochastic processes called the _Reciprocal projection_. For a process \(T\), it is is defined by \(_{}(T)= W^{}_{|x_{0},x_{1}}dp^{T}(x_{0},x_{1})\), see illustrative Figure 3.

The process \(_{}(T)\) is called a projection, since:

\[_{}(T)=*{arg\,min}_{R} {KL}(T\|R),\]

where \((C(),^{D})\) is the set of all reciprocal processes. The Iterative Markovian Fitting procedure is an alternation between Markovian and Reciprocal projections:

\[T^{2l+1}=_{}(T^{2l}), T^{2l+2}=_{ }(T^{2l+1}),\]

It is known that the procedure converges to the unique stochastic process \(T^{*}\), which is known as a solution to the Schrodinger Bridge (SB) problem between \(p_{0}\) and \(p_{1}\). Furthermore, the SB \(T^{*}\) is the only process starting at \(p_{0}\) and ending at \(p_{1}\) that is both Markovian and reciprocal .

Figure 3: Reciprocal projection of a stochastic process \(T\), i.e., \(_{}(T)= W^{}_{|x_{0},x_{1}}dp^{T}(x_{0},x_{1})\).

Figure 2: Markovian projection of a reciprocal stochastic process \(T_{q}\).

### Schrodinger Bridge (SB) Problem

**Schrodinger Bridge problem.** The Schrodinger Bridge problem  was proposed in 1931/1932 by Erwin Schrodinger. For the Wiener prior \(W^{}\) Schrodinger Bridge problem between two probability distributions \(p_{0}_{2,ac}(^{D})\) and \(p_{1}_{2,ac}(^{D})\) is to minimize the following objective:

\[_{T(p_{0},p_{1})}(T\|W^{}),\] (1)

where \((p_{0},p_{1})(C(),^{D})\) is the subset of stochastic processes which starts at distribution \(p_{0}\) (at the time \(t=0\)) and end at \(p_{1}\) (at \(t=1\)). The Scr\(}\)dinger Bridge has a unique solution, which is a diffusion process \(T^{*}\) described by the SDE: \(dX_{t}=v^{*}(X_{t},t)dt+dW_{t}\). The process \(T^{*}\) is called _the Schrodinger Bridge_ and \(v^{*}:^{D}^{D}\) is called _the optimal drift_.

From the practical point of view, the solution to the SB problem \(T^{*}\) tends to preserve the Euclidean distance between start point \(x_{0}\) and endpoint \(x_{1}\). The equivalent form of SB problem, the static Schrodinger Bridge problem, explains this property more clearly.

**Static Schrodinger Bridge problem.** One may decompose \((T\|W^{})\) as [51, Appendix C]:

\[(T\|W^{})=p^{T}(x_{0},x_{1})\|p^{W^{*}}(x_{0},x_{1})+(T_{|x_{0},x_{1}|}\|W^{}_{|x_{0},x_{1}})dp ^{T}(x_{0},x_{1}),\] (2)

i.e., KL divergence between \(T\) and \(W^{}\) is a sum of two terms: the 1st represents the similarity of the processes' joint marginal distributions at start and finish times \(t=0,1\), while the 2nd term represents the average similarity of conditional processes \(T_{|x_{0},x_{1}}\) and \(W^{}_{|x_{0},x_{1}}\). In [25, Proposition 2.3], the authors show that if \(T^{*}\) solves (1), then \(T^{*}_{|x_{0},x_{1}}=W^{}_{|x_{0},x_{1}}\). Hence, one may optimize (1) over \(T\) for which \(T_{|x_{0},x_{1}}=W^{}_{|x_{0},x_{1}}\) for every \(x_{0},x_{1}\), i.e., over reciprocal processes \(T\):

\[=_{T(p_{0},p_{1})}p ^{T}(x_{0},x_{1})\|p^{W^{*}}(x_{0},x_{1})=_{q(p_{0},p_{1})} q(x_{0},x_{1})\|p^{W^{*}}(x_{0},x_{1}),\] (3)

where \((p_{0},p_{1})_{2,ac}(^{D 2})\) is the set of joint probability distributions with marginal distributions \(p_{0}\) and \(p_{1}\). Thus, the initial Schrodinger Bridge problem can be solved by optimizing only over a reciprocal process's joint distribution \(q(x_{0},x_{1})\) at \(t=0,1\). This problem is called the Static Schrodinger Bridge problem. In turn, the problem can be rewritten in the following way [12, Eq. 7]:

\[_{q(p_{0},p_{1})}(q\|p^{W^{}}(x_{0},x_{1})) =_{q(p_{0},p_{1})}-x_{1}\|^{2}}{2}dq(x_{0},x_{1})- (q)+C,\] (4)

i.e., as finding a joint distribution \(q(x_{0},x_{1})\) which tries to minimize the Euclidian distance \(}{2}\) between \(x_{0}\) and \(x_{1}\) (preserve similarity between \(x_{0}\) and \(x_{1}\)), but with the addition of entropy regularizer \((q)\) with the coefficient \(\). Thus, the coefficient \(>0\), which is the same for all problems considered above, regulates the stochastic or diversity of samples from \(q(x_{0},x_{1})\). The last problem (4) is also known as the entropic optimal transport (EOT) problem .

## 3 Adversarial Schrodinger Bridge Matching (ASBM)

The IMF framework  works with _continuous_ time stochastic processes: it is built on the well-celebrated result that the only process which is both Markovian and reciprocal is the Schrodinger bridge \(T^{*}\). We derive an analogous theoretical result but for processes in _discrete_ time. We provide proofs for all the theorems and propositions in Appendix B.

In SS3.1, we give preliminaries on discrete processes with Markovian and reciprocal properties. In SS3.2, we present the main theorem of our paper, which is the foundation of our **Discrete-time Iterative Markovian Fitting (D-IMF)** framework. In SS3.3, we describe D-IMF procedure itself and prove that it allows us to solve the Schrodinger Bridge problem. In SS3.4, we provide an analysis of applying our D-IMF for solving the Schrodinger Bridge between Gaussian distributions. In SS3.5, we present the practical implementation of our D-IMF procedure using adversarial learning.

### Discrete Markovian and reciprocal stochastic processes

**Discrete reciprocal processes.** We define the discrete reciprocal processes similarly to the continuous case by considering the finite-time projection of the Brownian bridge \(W^{}_{|x_{0},x_{1}}\), which is given by:

\[p^{W^{}}(x_{t_{1}},,x_{t_{N}}|x_{0},x_{1})=_{n=1}^{N}p^{W^{ }}(x_{t_{n}}|x_{t_{n-1}},x_{1}),\] (5)\[p^{W^{}}(x_{t_{n}}|x_{t_{n-1}},x_{1})=(x_{t_{n}}|x_{t_{n-1}}+ -t_{n-1}}{1-t_{n-1}}(x_{1}-x_{t_{n-1}}),-t_{n-1}) (1-t_{n})}{1-t_{n-1}}).\] (6)

This joint distribution \(p^{W^{}}(x_{t_{1}},,x_{t_{N}}|x_{0},x_{1})\) defines a discrete stochastic process, which we call a discrete Brownian bridge. In turn, we say that a distribution \(q_{2,ac}(^{D(N 2)})\) is a mixture of discrete Brownian bridges if it satisfies

\[q(x_{0},x_{t_{1}},,x_{t_{N}},x_{1})=p^{W^{}}(x_{t_{1}},,x_ {t_{N}}|x_{0},x_{1})q(x_{0},x_{1}),\]

where \(q(x_{0},x_{1})\) denotes its joint marginal distribution of \(q\) at times \(0,1\). That is, its "inner" part at times \(t_{1},,t_{N}\) is the discrete Brownian Bridge. We denote the set of all such mixtures as \((N)_{2,ac}(^{D(N+2)})\) and call them discrete reciprocal processes.

**Discrete Markovian processes.** We say that a discrete process \(q_{2,ac}(^{D(N+2)})\) is Markovian if its density can be represented in the following form (recall that \(t_{0}=0,t_{N+1}=1\)):

\[q(x_{0},x_{t_{1}},x_{t_{2}},,x_{t_{N}},x_{1})=q(x_{0})_{n=1}^{N+1}q (x_{t_{n}}|x_{t_{n-1}}).\] (7)

We denote the set of all such discrete Markovian processes as \((N)_{2,ac}(^{D(N+2)})\).

### Main Theorem

**Theorem 3.1** (Discrete Markovian and reciprocal process is the solution of static SB).: _Consider any discrete process \(q_{2,ac}(^{D(N+2)})\), which is simultaneously reciprocal and markovian, i.e. \(q(N)\) and \(q(N)\) and has marginals \(q(x_{0})=p_{0}(x_{0})\) and \(q(x_{1})=p_{1}(x_{1})\):_

\[q(x_{0},x_{t_{1}},,x_{t_{N}},x_{1})=p^{W^{}}(x_{t_{1}},,x_ {t_{N}}|x_{0},x_{1})q(x_{0},x_{1})=q(x_{0})_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{ n-1}}),\]

_Then \(q(x_{0},x_{t_{1}},,x_{t_{N}},x_{1})=p^{T^{*}}(x_{0},x_{t_{1}},,x_ {t_{N}},x_{1})\), i.e., it is the finite-dimensional projection of the Schrodinger Bridge \(T^{*}\) to the considered times. Moreover, its joint marginal \(q(x_{0},x_{1})\) at times \(t=0,1\) is the solution to the **static SB** problem (4) between \(p_{0}\) and \(p_{1}\), i.e., \(q(x_{0},x_{1})=p^{T^{*}}(x_{0},x_{1})\)._

Thus, to solve the static SB problem, it is enough to find a Markovian mixture of discrete Brownian bridges. To do so, we propose the Discrete-time Iterative Markovian Fitting (D-IMF) procedure.

### Discrete-time Iterative Markovian Fitting (D-IMF) procedure

Similar to the IMF procedure, our proposed Discrete-time IMF is based on two alternating projections of discrete stochastic processes: reciprocal and Markovian. We start with the reciprocal projection.

**Definition 3.2** (Discrete Reciprocal Projection).: Assume that \(q_{2,ac}(^{D(N+2)})\) is a discrete stochastic process. Then the reciprocal projection \(_{}(q)\) is a discrete stochastic process with the joint distribution given by:

\[_{}(q)(x_{0},x_{t_{1}},,x_{t_{N}},x_ {1})=p^{W^{}}(x_{t_{1}},,x_{t_{N}}|x_{0},x_{1})q(x_{0},x_{1}).\] (8)

This projection takes the joint distribution of start and end points \(q(x_{0},x_{1})\) and inserts the Brownian Bridge for intermediate time moments, see Figure 4. The prop. below justifies the projection's name.

**Proposition 3.3** (Discrete Reciprocal projection minimizes KL divergence with reciprocal processes).: _Under mild assumptions, the reciprocal projection \(_{}(q)\) of a stochastic discrete process \(q_{2,ac}(^{D(N+2)})\) is the unique solution for the following optimization problem:_

\[_{}(q)=*{arg\,min}_{r(N)} (q\|r).\] (9)

Similarly to the discrete reciprocal projection, we introduce discrete Markovian projection.

**Definition 3.4** (Discrete Markovian Projection).: Assume that \(q_{2,ac}(^{D(N+2)})\) is a discrete stochastic process. The Markovian projection of \(q\) is a discrete stochastic process \(_{}(q)_{2,ac}(^{D(N+2)})\) whose joint distribution given by:

\[_{}(q)(x_{0},x_{t_{1}},...,x_{t_{N}},x_{1 })=q(x_{0})_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}}).\] (10)

Despite it is possible to use any discrete stochastic process \(q\) as an input to a discrete markovian projection, in the rest of the paper only discrete reciprocal processes are considered as an input. For such cases, we provide a visualization of the markovian projection in Figure 5.

As with the reciprocal projection, our following proposition justifies the name of the projection.

**Proposition 3.5** (Discrete Markovian projection minimizes KL divergence with Markovian processes).: _Under mild assumptions, the Markovian projection \(_{}(q)\) of a stochastic discrete process \(q_{2,ac}(^{D(N+2)})\) is a unique solution to the following optimization problem:_

\[_{}(q)=*{arg\,min}_{m(N)} (q\|m).\] (11)

Now we are ready to define our D-IMF procedure. For two given distributions \(p_{0}_{2,ac}(^{D})\) and \(p_{1}_{2,ac}(^{D})\) at times \(t=0\) and \(t=1\), respectively, it starts with any discrete Brownian mixture \(p^{W^{}}(x_{t_{1}},,x_{t_{N}}|x_{0},x_{1})q(x_{0},x_{1})\), where \(q(x_{0},x_{1})(p_{0},p_{1})_{2,ac}(^{D 2})\). Then, it constructs the following sequence of discrete stochastic processes:

\[q^{2l+1}=_{}(q^{2l}), q^{2l+2}=_{ }(q^{2l+1}).\] (12)

**Theorem 3.6** (D-IMF procedure converges to the the Schrodinger Bridge).: _Under mild assumptions, the sequence \(q^{l}\) constructed by our D-IMF procedure converges in KL to \(p^{T^{*}}\). In particular, \(q^{l}(x_{0},x_{1})\) convergence to the solution \(p^{T^{*}}(x_{0},x_{1})\) of the static SB. Namely, we have_

\[_{l}(q^{l}\|p^{T^{*}})=0, _{l}(q^{l}(x_{0},x_{1})\|p^{T^{*}}(x_{0},x_{1 }))=0.\]

### Closed form Updates of D-IMF for Gaussian Distributions

In this section, we show that our D-IMF updates (12) can be derived in the closed form for the Gaussian case. Let \(p_{0}=(x_{0}|_{0},_{0})\) and \(p_{1}=(x_{1}|_{1},_{1})\) be Gaussians. Consider any initial discrete Gaussian process \(q_{2,ac}(^{D(N+2)})\) that has joint distribution \(q(x_{0},x_{1})(p_{0},p_{1})\):

\[x_{01}}}{{=}}x_{0}\\ x_{1},_{01}}}{{=}} _{0}\\ _{1},=_{0}&_{} \\ _{}^{T}&_{1}, q(x_{0},x_{1})}}{{=}}(x_{01}|_{01},)\] (13)

where \(^{2D 2D}\) is positive definite and symmetric and \(_{}\) is the covariance of \(x_{0}\) and \(x_{1}\). In this case, the result of updates (12) is always a discrete Gaussian processes with specific parameters. To show this, we introduce two auxiliary matrices \(U^{ND 2D}\) and \(K^{ND ND}\):

\[U}}{{=}}(1-t_{1})I_{D}&t_{1}I_ {D}\\ (1-t_{2})I_{D}&t_{2}I_{D}\\ &\\ (1-t_{N})I_{D}&t_{N}I_{D}, K}}{{=}}t_{1}(1-t_{1})I_{D}&t_{1}(1-t_{2})I_{D}&&t_{1}(1-t_{N})I_{D}\\ t_{1}(1-t_{2})I_{D}&t_{2}(1-t_{2})I_{D}&&t_{2}(1-t_{N})I_{D}\\ &&&\\ t_{1}(1-t_{N})I_{D}&t_{2}(1-t_{N})I_{D}&&t_{N}(1-t_{N})I_{D}\]

Here \(I_{D}\) is an identity matrix with the shape \(D D\). Below we present updates for both projections.

Figure 5: Markovian projection of a reciprocal discrete stochastic process \(q\).

**Theorem 3.7** (Reciprocal projection of a process whose joint marginal distribution is Gaussian).: _Assume that \(q_{2,ac}(^{D(N+2)})\) has Gaussian joint distribution \(q(x_{0},x_{1})\) given by (13). Then_

\[[_{}q](x_{},x_{0},x_{1})=(( x_{}\\ x_{01})|(U_{01}\\ _{01}),_{R}),_{R}}{= }( K+U U^{T}&U\\ (U)^{T}&)\] (14)

**Theorem 3.8** (Markovian projection of a discrete Gaussian process).: _Assume that \(q_{2,ac}(^{D(N+2)})\) is a discrete Gaussian process with \(q(x_{0},x_{1})\) given by (13) and the density_

\[q(x_{},x_{0},x_{1})=((x_{}\\ x_{01})|(_{}\\ _{01}),_{R}),_{in}=(_{t_{1} },,_{t_{N}}),\]

_where \(_{}\) and \(_{R}\) are some parameters of \(q\). Then its Markovian projection is given by:_

\[[_{}q](x_{},x_{0},x_{1})=q(x_{0})_{n=1}^{ N+1}q(x_{t_{n}}|x_{t_{n-1}}), q(x_{t_{n}}|x_{t_{n-1}})=(x_{t_{n}}| _{t_{n}}(x_{t_{n-1}}),_{t_{n}}),\]

\[_{t_{n}}(x_{t_{n-1}})=_{t_{n}}+(_{R})_{t_{n },t_{n-1}}((_{R})_{t_{n-1},t_{n-1}})^{-1}(x_{t_{n-1}}-_{t _{n-1}}),\]

\[_{t_{n}}=(_{R})_{t_{n},t_{n}}-(_{R})_{t_{n},t_{n-1}}((_{R})_{t_{n-1},t_{n-1}})^{-1}( (_{R})_{t_{n},t_{n-1}})^{T}.\]

_In turn, the joint distribution \([_{}q](x_{0},x_{1})\) is given by_

\[_{}q](x_{0},x_{1})=((x_{0} \\ x_{1})|(_{0}\\ _{1}),(_{0}&_{01}\\ (_{01})^{T}&_{1})),_{01}^{T}=_{n =1}^{N+1}(_{R})_{t_{n+1},t_{n}}((_{R})_{t _{n},t_{n}})^{-1}_{0}.\]

_Here \((_{R})_{t_{i},t_{j}}\) is the submatrix of \(_{R}\) denoting the covariance of \(x_{t_{i}}\) and \(x_{t_{j}}\), while \(_{0}\) and \(_{1}\) are covariance matrices of \(x_{0}\) and \(x_{1}\), respectively._

Thus, if we start D-IMF from some discrete process \(q^{0}\) with marginals \(q^{0}(x_{0})=p_{0}(x_{0})\), \(q^{0}(x_{1})=p_{1}(x_{1})\) and Gaussian \(q(x_{0},x_{1})\), then at each iteration of our D-IMF procedure \(q^{l}\) will be discrete Gaussian process with the same marginals and eventually will converge to \(q^{*}\). In SS4.1, we use our derived closed-form to perform an experimental analysis of D-IMF's convergence depending on the number of intermediate time moments \(N\) and the value of coefficient \(\).

### Practical Implementation of D-IMF: ASBM Algorithm

To implement our D-IMF procedure in practice, one should choose the process \(q^{0}\) and implement both discrete Markovian and reciprocal projections. Note that one is usually not interested in the processes' density but only needs the ability to sample endpoints \(x_{1}\) (or trajectories \(x_{0},x_{t_{1}},,x_{t_{N}},x_{1}\)) given a starting point \(x_{0}\) (\(=x_{t_{0}}\)). Thus, to solve SB between \(p_{0}(x_{0})\) and \(p_{1}(x_{1})\) one should choose \(q^{0}\) to have start and end marginals \(q^{0}(x_{0})=p_{0}(x_{0})\) and \(q^{0}(x_{1})=p(x_{1})\) accessible by samples.

**Implementation of the discrete reciprocal projection.** The reciprocal projection (8) of a given discrete process \(q(x_{0},x_{},x_{1})\) is easy if one can sample from \(q(x_{0},x_{1})\). To sample from \(_{}(q)\) it is enough to sample first a pair \((x_{0},x_{1}) q(x_{0},x_{1})\) and then sample intermediate points \(x_{t_{1}},,x_{t_{N}}\) from the Brownian bridge \(p^{W^{}}(x_{t_{1}},,x_{t_{N}}|x_{0},x_{1})\). This can be straightforwardly done using the formula (5) where the involved distributions (6) are simple Gaussians which are easy to sample from.

**Implementation of the discrete Markovian projection via DD-GAN.** To find the Markovian projection (10) of a reciprocal process \(q(N)\), one just needs to estimate the transition probabilities between sequential time moments, i.e., the set \(\{q(x_{t_{n}}|x_{t_{n-1}})\}_{n=1}^{N+1}\) and use the starting marginal \([_{}q](x_{0})=q(x_{0})=p_{0}(x_{0})\). The natural way to find transition probabilities is to set to parametrize all these distributions as \(\{q_{}(x_{t_{n}}|x_{t_{n-1}})\}_{n=1}^{N+1}\) and solve

\[_{}_{n=1}^{N+1}_{q(x_{t_{n-1}})}D_{}q(x _{t_{n}}|x_{t_{n-1}})||q_{}(x_{t_{n}}|x_{t_{n-1}}),\] (15)

where \(D_{}\) is some distance or divergence between probability distributions. In this case, a minimum of such loss is achieved when \(q_{}(x_{t_{n}}|x_{t_{n-1}})=q(x_{t_{n}}|x_{t_{n-1}})\) for each \(n\{1,2,,N+1\}\).

We note that a related setting is considered in the Denoising Diffusion GANs (DD-GAN), see [53, Eq. 4]. The difference is in the nature of \(q\): there \(q\) comes from the standard noising diffusion process, while in our case it is a given reciprocal process. Overall, the authors show that problems like (15)can be efficiently approached via time-conditioned GANs. Therefore, we naturally pick _DD-GAN approach as the backbone_ to learn our discrete Markovian projection and use their best practices.

In short, following DD-GAN, we parameterize \(q_{}(x_{t_{n}}|x_{t_{n-1}})\) via a time-conditioned generator network \(G_{}(x_{t_{n-1}},z,t_{n-1})\). As in DD-GAN, we use the non-saturating GAN loss  as \(D_{}\), which optimizes softened reverse KL-divergence . To optimize this loss, an additional conditional discriminator network \(D(x_{t_{n-1}},x_{t_{n}},t_{n-1})\) is needed. We do not recall technical details here as they are the same as in DD-GAN. For further details on DD-GAN learning, we refer to Appendix D.1.

Note that after learning \(\{q_{}(x_{t_{n}}|x_{t_{n-1}})\}_{n=1}^{N+1}\) the sampling assumes to take sample from \(q_{0}(x_{0})=p(x_{0})\) and then sample from \(\{q_{}(x_{t_{n}}|x_{t_{n}-1})\}_{n=1}^{N+1}\). Hence it is guaranteed that \(q_{0}(x_{0})=p(x_{0})\), but there may be an approximation error in estimating \(q_{1}(x_{1}) p(x_{1})\). This is due to the asymmetry of the definition of Markovian projection, i.e., it can be written in two equivalent ways:

\[_{}(q)(x_{0},x_{t_{1}},...,x_{t_{N}},x_{1} )=q(x_{0})_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}})=q(x_{1})_{n=1}^{N+1}q (x_{t_{n-1}}|x_{t_{n}}).\]

Analogously to the implementation of IMF [47, Algorithm 1], we address this asymmetry in our D-IMF by alternatively learning Markovian projection in forward and reverse directions. To learn Markovian projection in the reverse direction, we just need to use starting marginal \([_{}q](x_{1})=p_{1}(x_{1})\), parametrize \(\{q_{}(x_{t_{n-1}}|x_{t_{n}})\}_{n=1}^{N+1}\) and solve:

\[_{}_{n=1}^{N+1}_{q(x_{t_{n}})}D_{}q(x_ {t_{n-1}}|x_{t_{n}})||q_{}(x_{t_{n-1}}|x_{t_{n}}).\] (16)

In this case \(q(x_{1})=p_{1}(x_{1})\) is guaranteed, while \(q(x_{0}) p_{0}(x_{0})\).

**Implementation of the D-IMF procedure (ASBM algorithm)**. We start with initialization of \(q^{0}\) by the reciprocal process. Depending on the setup we use initialization with the independent coupling, i.e. \(q^{0}(x_{0},x_{1})\) = \(p_{0}(x_{0})p_{1}(x_{1})\) or a minibatch OT coupling , see Appendix D.3 for details.

We follow the best practices of IMF  and in the Markovian projection steps, we alternately learn models in the direction \(p_{0}{}\,p_{1}\) and in the reverse direction \(p_{1}{}\,p_{0}\) by using functionals (15) and (16) respectively to avoid the accumulation of errors due to the asymmetry in the definition of the Markovian projection. For details, see Appendix D.2. At the reciprocal projection steps, we use the model \(q_{}(x_{0},x_{in},x_{1})\) or \(q_{}(x_{0},x_{in},x_{1})\) learned to approximate \(q^{2l+1}\) to sample pair \((x_{0},x_{1})\) and then sample intermediate points from Brownian bridge. We use the term **outer iteration** (\(K\)) for a sequence of two reciprocal projections and two Markovian projections in different directions.

### Relation to Prior Works

There exists a variety of algorithms for learning SB based on different underlying principles: dual form entropic optimal transport algorithms , iterative proportional fitting (IPF) algorithms , bridge matching  and iterative Markovian fitting (IMF) algorithms , adversarial algorithms , etc. We refer to  for a benchmark and to [24, Table 1] for a quick survey of many of them. In turn, in our paper, we specifically focus on the advancement of IMF-type algorithms  as it they are not only theoretically well-grounded but also closely connected to the rectified flow approach  which works well in large-scale generative modeling . Below we discuss the relation of our contributions (SS1) to the prior works in IMF .

**Theory I**. As we detailed in SS2, basic IMF operates with stochastic processes in continuous time and iteratively performs Markovian and reciprocal projections. Our D-IMF procedure (SS3) does the same but in the discrete time, so it might _deceptively_ seem like our D-IMF is just an approximation of IMF. However, this is a misleading viewpoint. Indeed, the Markovian projection in the discrete time, in general, does not match with the continuous time Markovian projection. Still our D-IMF procedure _provably_ converges to SB. Furthermore, D-IMF procedure can theoretically work with just one intermediate time step (when \(N=1\)). Overall, its convergence rate varies depending on the number of intermediate points, see SS4.1. Naturally, we conjecture that in the limit \(N\) (when the time steps \(t_{1},,t_{N}\) densely fill \(\)) our D-IMF behaves the same as IMF since the discrete and continuous Markovian projections start to be close, see discussion in [47, Appendix E].

**Theory II**. In SS3.4, we derive the closed-form expression for our D-IMF updates in the Gaussian case. For the continuous IMF, there exists an analogous result [35, SS6.1]. However, unlike our result,that one is not explicit in the seme that it requires solving the matrix-valued ODE (35, Eq. 39) to get the actual projection. The analytical solution is known only when \(D=1\), i.e., 1-dimensional case, see also (47, Appendix D). In contrast, our Gaussian D-IMF updates work in any dimension \(D\).

**Practice.** Default continuous-time IMF (47; 35) in practice is naturally implemented via the Bridge Matching approach which learns an SDE. In our case, at each D-IMF step we learn several transitional probabilities and do this via also well-established adversarial techniques. In this sense, our practical implementation differs - each approach is based on its own backbone - bridge matching vs. adversarial learning - and naturally inherits the benefits/drawbacks of the respective backbone. They are fairly well stated in the discussion of the generative learning trilemma in (53).

## 4 Experiments

We evaluate our adversarial SB matching (**ASBM**) algorithm, which implements our D-IMF procedure on setups with Gaussian distributions (SS4.1) for which we have closed form update formulas (SS3.4) and real image data distributions (SS4.2). We additionally provide results for an illustrative 2D example in Appendix C.1, results for the Colored MNIST dataset in Appendix C.3, and results on the standard SB benchmark in Appendix C.2. The code for our algorithm and all experiments with it is written in Pytorch, is available in the supplementary materials, and will be made public. We provide all the technical details in Appendix D.

### Gaussian-to-Gaussian Schrodinger Bridge

We analyze the convergence of our D-IMF procedure depending on the number of intermediate time steps \(N 1\) (we use \(t_{n}=n/N+1\)) and the value \(>0\) in the Gaussian case. In this case, the static SB solution \(p^{T^{*}}(x_{0},x_{1})\) is analytically known, see, e.g., (18). This provides us an opportunity to analyse how fast \((q^{l}(x_{0},x_{1})\|p^{T^{*}}(x_{0},x_{1}))\) decreases when \(l\).

We conduct experiments by using our analytical formulas for D-IMF from SS3.4. We follow setup from (12) and consider Schrodinger Bridge problem with the dimensionality \(D=16\) and \(\{1,3,10\}\) for centered Gaussians \(p_{0}=(0,_{0})\) and \(p_{1}=(0,_{1})\). To construct \(_{0}\) and \(_{1}\), sample their eigenvectors from the uniform distribution on the unit sphere and sample their eigenvalues from the log uniform distribution on \([- 2, 2]\). We use the same \(p_{0}\) and \(p_{1}\) for all experiments.

We start our D-IMF procedure from the reciprocal process with \(q^{0}(x_{0},x_{1})=p_{0}(x_{0})p_{1}(x_{1})\), i.e. from the independent joint distribution at times \(t=0,1\). We present the convergence plots in Figures 5(a) and 5(b). In both plots, we use \(10^{-10}\) as a threshold corresponding to the exact matching of distributions to prevent numerical instabilities. We see that our D-IMF procedure empirically shows the exponential rate of convergence in all the cases. As we can see from Figure 5(a), the convergence speed dependence on \(N\) quickly saturates. Thus, even several time moments, e.g., \(N=5\), provide quick convergence speed. From Figure 5(b), we clearly see that the convergence speed is highly influenced by the chosen value of the parameter \(\). For instance, the transition from \(=1\) to \(=10\) requires ten times more D-IMF iterations. Thus, this hyperparameter may be important in practice.

### Unpaired Image-to-image Translation

To test our approach on real data, we consider the unpaired image-to-image translation setup of learning _male \(\) female_ faces of Celeba dataset (33). We use \(10\%\) of _male_ and _female_ images as the test set for evaluation. We train our ABM algorithm based on the D-IMF procedure with \(=1\) and \(=10\). Following the best practices of DD-GAN (53), we use \(N=3\), intermediate times \(t_{1}=,t_{2}=,t_{3}=\) and \(K=5\) outer iterations of D-IMF. We provide qualitative results and the FID metric (14) on the test set in Figures 6(b) and 6(e). Since we use \(N=3\) intermediate time moments, our algorithm requires only \(4\) number of function evaluations (NFE) at the inference stage.

Figure 6: Dependence of convergence of **our** D-IMF procedure on \(N\) and \(\).

We focus our comparison on the DSBM algorithm based on the IMF-procedure  since it is closely related to our method. We train DSBM following the authors  and use \(=100\). As well as for ASBM, we use \(5\) outer iterations of IMF, corresponding to the same number of reciprocal and Markovian projections, but for continuous processes. We use approximately the same number of parameters of neural networks used for models in Markovian projections for ASBM and DSBM (see Appendix D.3). For other details, see Appendix D.4. We present results for DSBM in Figure 6(c) and Figure 6(f). Our algorithm provides better results while using only \(4\) evaluation steps. Further additional results and measurements for ASBM and DSBM algorithms on the Celeba dataset are presented in Appendix E.

Thus, our D-IMF procedure allows us to solve the Schrodinger Bridge efficiently without learning the time-continuous stochastic process, which in turn speeds up inference by an order of magnitude. This aligns with the results obtained in the Gaussian-to-Gaussian setups about exponentially fast convergence of D-IMF even with several intermediate time moments.

## 5 Discussion

**Potential impact.** Beside the pure speed up of the inference of IMF, we want to point to another great advantage of our developed D-IMF framework. In the continuous IMF, one is forced to do Markovian projection via time-consuming learning of continuous-time SDEs (using procedures like bridge matching). In our D-IMF framework, one needs to **learn several transition probabilities**. We do this via adversarial learning , but actually this can be done **using** almost **any other generative modeling technique** (moment matching , normalizing flows [23; 41], energy-based models , score-based models , etc.). We believe that this observation opens great possibilities for ML community to further explore and improve generative modeling algorithms based on Schrodinger Bridges, Markovian projections (bridge matching) and related techniques, e.g., flow matching .

Limitations and broader impact are discussed in Appendix A.

Figure 7: Results of Celeba, _male\(\)female_ translation learned with ASBM (ours), and DSBM learned on Celeba dataset with 128 resolution size for \(\{1,10\}\).