# arXiVeri: Automatic table verification with GPT

Gyungin Shin\({}^{1,4}\)&Weidi Xie\({}^{2,3}\)&Samuel Albanie\({}^{4}\)

\({}^{1}\)Visual Geometry Group, University of Oxford

\({}^{2}\)Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

\({}^{3}\)Shanghai AI Laboratory

\({}^{4}\)CAML, University of Cambridge

###### Abstract

Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of _automatic table verification_ (_AutoTV_), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, _arXiVeri_, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) _table matching_, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) _cell matching_, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large language models (LLMs), we propose simple baselines for table verification. Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark is made publicly available.1

## 1 Introduction

Many areas of scientific research employ numerical data to analyse, summarise and communicate findings. When a researcher proposes a new framework, model or algorithm, it is often informative to compare their contribution with prior work by comparing performance metrics. These performance metrics are typically collated in tables that are interleaved with the body of text contained within scientific manuscripts. In practice, to enable the comparison, it is common for the researcher to manually copy performance metrics from the original manuscript into their own manuscript. While pragmatic, this copying process is susceptible to human error. When errors are introduced, the conclusions drawn from the comparisons are also affected. Given the importance of transferring such data correctly, there is a need for mechanisms that ensure its fidelity, but such tooling is not yet available. In short, we lack a "spell checker" for manually copied scientific data.

On first sight, the problem appears simple--after all, verifying that two numbers are equal is not a mathematically complicated task. However, in practice, it is beset with technical difficulties. Tables in the scientific literature are designed to be readable for a human audience rather than machine parsers. As such, they can vary significantly in layout, design, naming convention and manuscript location. The same numerical data may itself be reported at different levels of precision, using percentages, fractions or decimals and in absolute or relative metrics.

To meet these challenges we propose the task of _automatic table verification_--authenticating the numerical data encapsulated in tables by cross-verifying the referred sources. Specifically, we tackle this task with Large Language Models (LLMs) inspired by their strong performance in many text-based processing tasks . To facilitate evaluation of this task and address the incumbent challenges, we introduce _arXiVeri_, a succinct benchmark composed of tabulardata extracted from open-access academic papers on arXiv. We further propose evaluation metrics for gauging the efficacy of the verification system in two key dimensions: _table matching_ and _cell matching_. The former involves identifying the equivalent source table in a cited document for a given target table, while the latter aims to pair shared cells between a target and source table, and accurately identify their respective indices (see Fig. 1). Our experimental findings underscore the complexity inherent to the task (with frontier models such as GPT-4  struggling in many cases), indicating that there is considerable room for further research progress.

Our contributions can be summarised as follows: (i) We introduce a new and challenging task called _Automatic Table Verification_ (AutoTV), paving the way for advancements in automatic data verification in scientific documents; (ii) To stimulate further research in the AutoTV field, we introduce a benchmark dataset named _arXiVeri_, comprised of 3.8K target-source cell pairs and 158 target-source table pairs, sourced from publicly accessible papers on arXiv; (iii) To facilitate the assessment of AutoTV, we define a set of evaluation metrics for table matching and cell matching sub-tasks. In addition, we provide baselines to underpin future comparisons. (iv) Finally, we conduct a range of ablation studies to evaluate the key components of our approach which bring noticeable performance gains.

## 2 Related work

Our work is connected to large language models (LLMs) for scientific research, table detection and table structure recognition, and automating human labour with LLMs, which we describe next.

**Large language models for scientific research.** LLMs have been adapted for scientific research through various avenues, such as utilising models pretrained on scientific text to enhance performance in scientific NLP tasks [2; 29]. There are also notable advances in the biomedical sector, where specialised LLMs pretrained on biomedical text have demonstrated considerable improvements . Additionally, the compilation of extensive academic paper corpora equipped with metadata and structured full text is proving to be an invaluable resource for academic research and text mining . Alongside these advancements, there are investigations into the consequences of model scaling in scientific applications, evaluating the relationship between model size and performance . Our research further develops this field by presenting a new challenge: automatic table verification in scientific documents, highlighting the essential role of data accuracy and integrity via cross-referencing cited sources.

**Tasks for tables in a single document.** Recent advancements in table-related tasks have primarily focused on detecting tables within documents and understanding their structure within a single document. Early efforts developed practical algorithms for detecting tables in heterogeneous documents , which later evolved with the incorporation of deep learning, specifically using Convolutional Neural Networks (CNNs), to enhance detection in PDF documents by combining visual features with non-visual information . Subsequent research introduced end-to-end deep learning systems capable of not only detecting tables but also recognising their structure in document

Figure 1: (Left) **Table matching**: given a target table from one paper and a list of source tables from another paper cited in the target table, the verifier needs to identify the source table containing numeric data, specifically floating point numbers, that supports the data presented in the target table. (Right) **Cell matching**: given a target table and a source table, the verifier needs to identify and locate cells that hold the same semantic content in both tables, subsequently outputting the respective row and column indices of these matching cells in each table. The cells that are emphasised in red depict the instances of hard negative cases. Best viewed in colour.

images, without the need for metadata or heuristics . Later work tackled both table detection and structure recognition simultaneously . Prior work has also explored dataset construction for table extraction from unstructured documents . However, the central theme of these works has remained the detection and structural understanding of tables within a single document. In contrast, we focus on table verification _across documents_.

**Task automation with LLMs.** LLMs have significantly impacted various automation tasks. For instance, Codex , an LLM fine-tuned on code from GitHub, exhibits proficient Python code generation capabilities, automating a task typically requiring human expertise. In the domain of data annotation, traditionally a labour-intensive task, LLMs like ChatGPT have demonstrated the potential to outperform human crowd-workers in speed, accuracy, and cost-effectiveness . More recently, experiments have been conducted with GPT-4 to assess its ability to assist with Neural Architecture Search (NAS)  and interpreting neurons . Our work also targets automation, offering a novel application of LLMs to automate the intricate task of table verification in scientific documents.

## 3 Automatic Table Verification

In this section, we define the proposed task of _automatic table verification_ (Sec. 3.1) and metrics to evaluate the performance of a verifier on this task (Sec. 3.2). Then we describe our approach to tackle AutoTV (Sec. 3.3).

### Task definition

The high-level objective of Automatic Table Verification (AutoTV) is to confirm that a document, referenced in a table (termed the target table) within a separate document, contains a corroborative table (termed the source table) which supports the cited information. When such a source table exists, AutoTV aims to identify matching cells between the source and target tables.

Our focus is particularly on instances within academic papers, where precise referencing of numeric data (e.g., floating point numbers) in tables is vital for comparative analysis. We observe that such in-table citations in academic literature occur for various reasons, including: attributing a specific approach to its original paper and quoting numerical data from an experimental result. The primary focus of table verification is the latter case, where a verifier is tasked with solving two sub-tasks (see Fig. 1): (i) **Table matching**: detecting a table in the cited document that matches a table in the referring document and if no such table exists, stating that there is no match; (ii) **Cell matching**: identifying correspondences between cells with a floating point number in the source and target tables that share the same semantic meaning. This implies not only identical numeric values, but also similar meanings as suggested by their respective table headers. The process includes pinpointing the location of such cells by providing their respective row and column indices in each table.

We note that these sub-tasks pose distinct challenges. First, there may not be a source table that matches the target table (e.g., the table citation may simply attribute to another document rather than quoting numbers). Second, multiple cells within a table (e.g., source table) can share the same numeric value, making it ambiguous how to pair those cells with ones in another table (e.g., target table). Third, a table can have a complex structure, with a single cell spanning across multiple rows and/or columns or featuring multiple headers, making it difficult to identify cell locations.

### Evaluation metrics

To quantitatively measure performance of a verifier on AutoTV, we define four metrics including _table matching accuracy_ for table matching, _cell matching recall_, _cell matching precision_, and _F-1 score_ for cell matching as follows.

**Table matching accuracy** evaluates the verifier's ability to accurately identify a source table that matches a given target table, or to determine that no such source table exists in the cited document. Formally, given a set of all target tables \(_{t}\), a target table \(t_{t}\) with a set of \(N_{t}\) in-table references \(_{t}=\{r_{i}|1 i N_{t}\}\), a set of candidate source tables \(_{s;r_{i}}\) from a cited document \(r_{i}\), and a verifier \((,prompt;)\), the table detection accuracy (\(Acc.\)) is defined as:

\[Acc.\ =\ _{t}}_{r_{i} _{t}}[s_{r_{i};t}=_{r_{i};t}]}{|_{t}|}, _{r_{i};t}=(t,_{s;r_{i}},prompt;)\] (1)

where \([]\), \(_{r_{i};t}\) and \(s_{r_{i};t}\) denote the Kronecker delta function, the detected source table and the ground-truth source table in the cited document which matches the given target table \(t\), respectively.

**Cell matching recall** quantifies the percentage of target-source cell matches that are accurately identified (i.e., true positives) among a ground-truth set of cell matches across a source table and a target table. Let us denote the ground-truth set of \(N_{r_{i};t}\) paired cells between a target table \(t\) and a source table \(s_{r_{i};t}\) in a cited document \(r_{i}\) as \(_{r_{i};t}=\{(c_{t},c_{r_{i};t})_{j}|1 j N_{r_{i};t}\}\) and a set of \(_{r_{i};t}\) detected cell matches as \(}_{r_{i};t}=\{(_{t},_{r_{i};t})_{j}|1 j _{r_{i};t}\}\) where \(c_{t}\) and \(c_{r_{i};t}\) represent the row and column indices of a cell in the target and source tables, resp. Then, the cell matching recall (\(Recall\)) is defined as:

\[Recall = _{t}}_{r_{i} _{t}}|_{r_{i};t}}_{r_{i};t}|}{ _{t_{t}}_{r_{i}_{t}}|_{r_{i};t}|},\;}_{r_{i};t}=(t,s_{r_{i};t},prompt;)\] (2)

**Cell matching precision** measures how many target-source cell pairs are true positives among all the detected target-source cell pairs. Using the same notation as above, the cell matching precision (\(Prec.\)) is defined as:

\[Prec. = _{t}}_{r_{i} _{t}}|_{r_{i};t}}_{r_{i};t}|}{ _{t_{t}}_{r_{i}_{t}}|}_{r_{i};t}|},\;}_{r_{i};t}=(t,s_{r_{i};t},prompt ;)\] (3)

\(}\) **score** is a harmonic mean of the cell matching recall and precision to encapsulate both the measures in a single metric:

\[F_{1}\;score = 2\] (4)

**Remark.** All four metrics have a fixed range of , with higher values being better. The text prompt, denoted by \(prompt\), provided to the verifier may vary with the task, i.e., table matching and cell matching.

### Baseline methods

To tackle AutoTV, we propose baseline approaches for table matching and cell matching as follows.

**Table matching.** We utilise a text embedding model (e.g., OpenAI's text-embedding-ada-002) to embed a target table alongside a set of candidate source tables from a document cited in the target table, including their respective captions. It is worth mentioning that the tables are in HTML format by default which we extract during the data collection process (detailed in Sec. 4.1). Subsequently, we rank the candidate tables based on their cosine similarities with the target table in the embedding space, selecting the one with the highest similarity score that also shares at least one

  
**Target-source cell matching** \\ 
**Input** & a target table (target\_table), a source table (source\_table) \\ 
**System** & You are a helpful assistant. \\
**User** & Compare the following target and source tables and identify cells that contain floating point numbers with the same meaning present in both tables. Return the matched cells in a Python dictionary with the following format: \\  & \{ \\  & (target\_table\_box, target\_table\_column\_index): \\  & (source\_table\_box, source\_table\_column\_index), \\  &... \\  & \} \\  & Use 0-based indexing, including headers, rowspan, and colspan attributes. Locate as many matching cell pairs as possible. If no matches are found, return an empty dictionary ({}). \\  & The target table and its caption: {target\_table} \\  & The source table and its caption: {source\_table} \\ 
**GPT-4** & Answer \\   

Table 1: **Text prompt used for the cell matching task.** We apply a regular expression to the answer string of the model to ensure the final result follows the specified Python dictionary format.

floating point number with the target table. As the final step, we label the prediction as "no match found" if the chosen table's similarity score falls below a specified threshold.

In addition, we consider weighting each candidate table based on the number of floating point numbers that it shares with the target table before ranking them with their cosine similarity. In essence, we multiply the similarity score for a candidate table by a weight, which is determined by the number of floating-point numbers shared with the target table. Specifically, we sort the candidate tables based on the number of shared floats and assign each table with a weight between 0 and 1 according to their rank such that the table with the most shared floats is assigned with 1. These weights are evenly distributed with intervals of 1 divided by the count of candidate tables that share at least one floating point number with the target table. For tables that do not share any floating points, we assign a weight of 0. We show the effect of the weighting in Sec. 5.2.

Importantly, in all cases, floating point numbers are normalised to account for potential unit differences between the source and target tables.

**Cell matching.** We employ GPT-4 to extract matches between target-source cells containing a floating point number present in both target and source tables. As detailed in Sec. 3.2, each match is depicted as a pair of row and column indices for a cell in the target table and its corresponding cell in the source table. To facilitate this, we direct GPT-4 to generate a string representing a Python dictionary where keys denote cell indices in the target table and values represent indices in the source table as shown in Tab. 1. We then extract this dictionary using a regular expression that conforms to a specified dictionary pattern. For cell matching, we experiment with different types of commonly used table formats including HTML, CSV, or Markdown and show the effect of the table format in Sec. 5.2.

## 4 arXiVeri benchmark

Here, we introduce a benchmark composed of academic papers from arXiv, termed _arXiVeri_, for measuring performance of a verifier on the proposed AutoTV task. We first detail the data collection process (Sec. 4.1) and provide statistics of the arXiVeri benchmark (Sec. 4.2).

Figure 2: **Data collection pipeline for the arXiVeri benchmark.** Top: We randomly select open-access papers under the CC-BY license from arXiv and extract tables with in-table references (i.e. target tables) from an HTML5 version of the selected papers. Then, we repeat the process to retrieve the cited papers and their tables. Bottom left: To identify a _candidate_ source table, which supports a target table, we pick one which has the most cells which are shared with the target table. Bottom right: Given the target and the candidate source table, we manually pair the common cells between them. If no paired cells are identified, we conclude that the candidate source table is a false positive and the source paper does not contain any matching source table for the target table. See the text for the details. Best viewed in colour.

### Data collection

As shown in Fig. 2, our data collection process is composed of three steps: (i) target and source paper retrieval, (ii) target-source table matching, and (iii) manual cell paring, as detailed next.

**Target and source paper retrieval.** We begin by collecting recent arXiv papers published in 2022 under the CC-BY license, using the open-source arXiv API.2 We specifically focus on papers categorised as cs.CV, which had the highest number of submissions on arXiv in 2022. We extract tables along with their captions from each paper's HTML5 format using ar5iv,3 which enables us to isolate tables from other elements such as main text by accessing the appropriate HTML tags (e.g., <table>). Subsequently, we employ an Elasticsearch-based arXiv search system to retrieve papers cited in each table (termed the source papers) with their title available in the "References" section of the referring paper (termed the target paper). As the title of a cited paper in the References section is often presented with irrelevant information (e.g., a url to a code) for the search system, we utilise GPT-3.5-turbo to extract the title from the whole reference information of a cited paper. Importantly, to increase the benchmark's complexity, we omit the cited paper if it does not contain a table sharing at least one cell value with the target table, or if it does not contain more than one table.

**Target-source table matching.** Given a table in a target paper (i.e., the target table) and a source paper which is cited in the target table, we select a table among the set of tables extracted from the source paper (using the same method described above) that supports the target table (i.e., the source table). Specifically, we choose a table that has the highest number of shared floating-point numbers with the target table to be the _candidate_ source table. By iterating through all the references in a target table, we identify a corresponding candidate source table in each cited paper.

It is important to note that a target paper can have multiple tables referring to the same source paper, resulting in several potential table matchings between the target and source papers. In such cases, we choose the matching with the highest number of overlapping floating-point numbers per one target-source paper pair to increase the diversity of papers in the arXiVeri benchmark.

**Manual cell pairing.** In the final step of the collection process, we manually match cells that are commonly found in both target and candidate source tables. To determine a correct cell pair, we compare two cells from the tables and mark them as a match if they meet all of the following conditions:

1. Both cells must represent the same value with an identical meaning, as indicated by their respective row and column headers.
2. Each cell must not contain more than one floating-point number for different metrics, avoiding the use of delimiters such as a comma (',') or a slash ('/').
3. If both cells have the same number of significant digits and the same unit, they must be exactly identical; for example, '12.3' and '12.4' would be treated as an incorrect pair.

The first condition ensures that matched cells have the same meaning as well as value, as determined by their row and column headers. The second condition aims to remove ambiguity during the evaluation step by avoiding cases where a single cell with multiple values is mapped to several cells in another table. The third condition accounts for potential discrepancies in rounding methods or mistakes, requiring matched cells to have the exact values given the same significant digits. If no such cell pairs are found between the target and candidate source tables, we regard the table pair does not have a source table from the source paper for the target table. On the other hand, if there is at least one cell pair, we treat the candidate source table as the source table (for the target table).

**Post-processing.** To ensure that the models used in our experiments can process each table and its caption as input, we filter out tables whose token length, including their captions, exceeds 3,072, as estimated by a tokeniser (i.e., tiktoken4)

### Statistics

We annotate a total of 3.8K cell pairs from 158 target-source table pairs, involving 110 different target papers and 158 distinct source papers. As illustrated in Fig. 3, we make three observations: (i) source papers contain an average of 4.6 tables, with three being the most frequent number of tables in a source paper; (ii) on average, there are 19.5 cell pairs between a target and a source table with the minimum and maximum number of cell pairs being 1 and 84, resp.; (iii) the dimensions of tables in the dataset exhibit a considerable range, with the smallest table measuring 4 by 5 and the largest reaching 20 by 19. On average, tables tend to fall around the size of 15.9 by 8.0.

## 5 Experiments

In this section, we first provide implementation details in Sec. 5.1 and conduct ablation studies in Sec. 5.2 to investigate each component of our approach for the proposed AutoTV task.

### Implementation details

For the task of table matching, we employ four different text embedding models. These include OpenAI's text-embedding-ada-002 which has an output dimension of 1536, as well as three models from Cohere: embed-multilingual-v2.0, embed-english-v2.0, and embed-english-light-v2.0, with respective output dimensions of 768, 4096, and 1024. For the cell matching task, we employ the gpt-4-0314 model with a maximum length of 8,192 tokens and set the temperature parameter \(\) to 0, unless specified otherwise. To further minimize variability in the model's performance, we report the average score obtained by running each model three times across our experiments.

### Ablation study

**Effect of text embedding models on table matching.** To investigate the influence of selecting different text embedding models, we evaluate four different models, as depicted in Tab. 2 (left). Alongside, we measure the performance of two baseline strategies: (i) "random", which selects a table from a candidate set of source tables, including "no match", and (ii) "overlap", which chooses a table that shares the most floating point numbers with the target table. As can be seen, each of the four embedding models significantly outperforms the baseline strategies by a margin of 12.7-15.8%. Among them, the embed-english-light-v2.0 model from Cohere demonstrates the best performance.

**Effect of weighting on table matching.** As described in Sec. 3.3, we further refine our approach by weighting each candidate table based on the number of shared floating point numbers with the target table. Tab. 2 (right) illustrates the impact of this weighting mechanism on the performance of each embedding model. Notably, implementing this weighting strategy improves performance across all four embedding models, underscoring its effectiveness.

**Effect of table format and providing cell indices.** In the cell matching task, we explore three different table formats--HTML, CSV, and Markdown--for feeding tables to GPT-4. We posit that to enable the model to accurately identify a cell's location, providing explicit row and column indices for each cell could be beneficial. To verify this hypothesis, we also assess performance of the model when row and column indices are explicitly specified on the left and top of a table, resp.

From our results in Tab. 3 (left), we can see that the choice of format significantly influences the model's performance with the HTML format yielding the best performance in the absence of cell indices. We conjecture that this is because the HTML format contains more distinctive delimiters such as <tr> and <td> for table rows and table columns compared to CSV or Markdown where the model has to infer a cell location by counting a line break character and a comma ('.'), which can appear in other parts of the input than the actual table (e.g., text prompt and caption). Indeed, when cell indices are provided with an input table, we can observe that both of the CSV and Markdown formats have significant boost in all of the \(Recall\), \(Prec.\), and \(F_{1}\) metrics, outperforming the HTML format. Examples of each format are provided in the supplementary materials.

**Effect of temperature.** In addition, we experiment with the temperature parameter in GPT-4, which modulates the randomness of the model's output. High values (nearing 1) introduce diversity, while low values (tending towards 0) enhance deterministic behavior. As shown in Tab. 3 (right), we

Figure 3: **Data statistics of the arXiveri dataset.** Left: A histogram illustrating the count distribution of tables within source papers. Middle: A histogram representing 3.8K of shared cells between target and source tables. Right: A distribution plot of table dimensions (rows and columns), with colour indicating table size, and the average dimensions marked in red. Best viewed in colour.

[MISSING_PAGE_EMPTY:8]

**Acknowledgements and disclosure of funding.** This work was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (www.csd3.cam.ac.uk), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk). GS would like to thank Vishaal Udandarao for thorough proof-reading and Zheng Fang for the invaluable support. SA would like to acknowledge the support of Z. Novak and N. Novak in enabling his contribution. SA was supported by a Newton Grant.