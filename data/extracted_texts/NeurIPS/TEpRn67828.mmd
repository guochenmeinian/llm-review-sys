# Uniform Convergence with Square-Root Lipschitz Loss

Lijia Zhou

University of Chicago

zlj@uchicago.edu

&Zhen Dai

University of Chicago

zhen9@uchicago.edu

&Frederic Koehler

Stanford University

fkoehler@stanford.edu

&Nathan Srebro

Toyota Technological Institute at Chicago

nati@ttic.edu

Collaboration on the Theoretical Foundations of Deep Learning (deepfoundations.ai)

###### Abstract

We establish generic uniform convergence guarantees for Gaussian data in terms of the Rademacher complexity of the hypothesis class and the Lipschitz constant of the square root of the scalar loss function. We show how these guarantees substantially generalize previous results based on smoothness (Lipschitz constant of the derivative), and allow us to handle the broader class of square-root-Lipschitz losses, which includes also non-smooth loss functions appropriate for studying phase retrieval and ReLU regression, as well as rederive and better understand "optimistic rate" and interpolation learning guarantees.

## 1 Introduction

The phenomenon of "interpolation learning", where a learning algorithm perfectly interpolates noisy training labels while still generalizing well to unseen data, has attracted significant interest in recent years. As Shamir (2022) pointed out, one of the difficulties in understanding interpolation learning is that we need to determine which loss function we should use to analyze the test error. In linear regression, interpolating the square loss is equivalent to interpolating many other losses (such as the absolute loss) on the training set. Similarly, in the context of linear classification, many works (Soudry et al., 2018; Ji and Telgarsky, 2019; Muthukumar et al., 2021) have shown that optimizing the logistic, exponential, or even square loss would all lead to the maximum margin solution, which interpolates the hinge loss. Even though there are many possible loss functions to choose from to analyze the population error, consistency is often possible with respect to only one loss function because different loss functions generally have different population error minimizers.

In linear regression, it has been shown that minimal norm interpolant can be consistent with respect to the square loss (Bartlett et al., 2020; Muthukumar et al., 2020; Negrea et al., 2020; Koehler et al., 2021), while the appropriate loss for benign overfitting in linear classification is the squared hinge loss (Shamir, 2022; Zhou et al., 2022). In this line of work, uniform convergence has emerged as a general and useful technique to understand interpolation learning (Koehler et al., 2021; Zhou et al., 2022). Though the Lipschitz contraction technique has been very useful to establish uniform convergence in classical learning theory, the appropriate loss functions to study interpolation learning (such as the square loss and the squared hinge loss) are usually not Lipschitz. In fact, many papers (Nagarajan and Kolter, 2019; Zhou et al., 2020; Negrea et al., 2020) have shown that the traditional notion of uniform convergence implied by Lipschitz contraction fails in the setting of interpolation learning. Instead, we need to find other properties of the loss function to establish a different type of uniform convergence.

Zhou et al. (2022) recently showed that the Moreau envelope of the square loss or the squared hinge loss is proportional to itself and this property is sufficient to establish a type of uniform convergence guarantee known as "optimistic rate" (Panchenko, 2002; Srebro et al., 2010). Roughly speaking, if this condition holds, then the difference between the _square roots_ of the population error and the training error can be bounded by the Rademacher complexity of the hypothesis class. Specializing to predictors with zero training error, the test error can then be controlled by the square of the Rademacher complexity, which exactly matches the Bayes error for the minimal norm interpolant when it is consistent (Koehler et al., 2021). However, the Moreau envelope of a loss function is not always easy to solve in closed-form, and it is unclear whether this argument can be applied to problems beyond linear regression and max-margin classification. Moreover, considering the difference of square roots seem like a mysterious choice and does not have a good intuitive explanation.

In this paper, by showing that optimistic rate holds for any _square-root Lipschitz_ loss, we argue that the class of square-root Lipschitz losses is the more natural class of loss functions to consider. In fact, any loss whose Moreau envelope is proportional to itself is square-root Lipschitz. Our result also provides an intuitive explanation for optimistic rate: when the loss is Lipschitz, the difference between the test and training error can be bounded by Rademacher complexity; therefore, when the loss is _square-root Lipschitz_, the difference between the _square roots_ of the test and training error can be bounded by Rademacher complexity. In addition to avoiding any hidden constant and logarithmic factor, our uniform convergence guarantee substantially generalize previous results based on smoothness (Lipschitz constant of the derivative, such as Srebro et al., 2010). The generality of our results allows us to handle non-differentiable losses. In the problems of phase retrieval and ReLU regression, we identify the consistent loss through a norm calculation, and we apply our theory to prove novel consistency results. In the context of noisy matrix sensing, we also establish benign overfitting for the minimal nuclear norm solution. Finally, we show that our results extend to fully optimized neural networks with weight sharing in the first layer.

## 2 Related Work

Optimistic rates.In the context of generalization theory, our results are related to a phenomena known as "optimistic rates" -- generalization bounds which give strengthened guarantees for predictors with smaller training error. Such bounds can be contrasted with "pessimistic" bounds which do not attempt to adapt to the training error of the predictor. See for example Vapnik (1982); Panchenko (2003); Panchenko (2002). The work of Srebro et al. (2010) showed that optimistic rates control the generalization error of function classes learned with smooth losses, and recently the works Zhou et al. (2021); Koehler et al. (2021); Zhou et al. (2020) showed that a much sharper version of optimism can naturally explain the phenomena of benign overfitting in Gaussian linear models. (These works are in turn connected with a celebrated line of work in proportional asymptotics for M-estimation such as Stojnic (2013), see references within.) In the present work, we show that the natural setting for optimistic rates is actually square-root-Lipschitz losses and this allows us to capture new applications such as phase retrieval where the loss is not smooth.

Phase retrieval, ReLU Regression, Matrix sensing.Recent works (Maillard et al., 2020; Barbier et al., 2019; Mondelli and Montanari, 2018; Luo et al., 2019) analyzed the statistical limits of phase retrieval in a high-dimensional limit for certain types of sensing designs (e.g. i.i.d. Real or Complex Gaussian), as well as the performance of Approximate Message Passing in this problem (which is used to predict a computational-statistical gap). In the noiseless case, the works Andoni et al. (2017); Song et al. (2021) gave better computationally efficient estimators for phase retrieval based upon the LLL algorithm. Our generalization bound can naturally be applied to any estimator for phase retrieval, including these methods or other ones, such as those based on non-convex optimization (Sun et al., 2018; Netrapalli et al., 2013). For the same reason, they can also naturally be applied in the context of sparse phase retrieval (see e.g. Li and Voroninski, 2013; Candes et al., 2015).

Similarly, there have been many works studying the computational tractability of ReLU regression. See for example the work of Auer et al. (1995) where the "matching loss" technique was developed for learning well-specified GLMs including ReLUs. Under misspecification, learning ReLUs can be hard and approximate results have been established, see e.g. Goel et al. (2017); Diakonikolas et al. (2020). In particular, learning a ReLU agnostically in squared loss, even over Gaussian marginals, is known to be hard under standard average-case complexity assumptions (Goel et al., 2019). Again, our generalization bound has the merit that it can be applied to any estimator. For matrix sensing, we consider nuclear norm minimization (as in e.g. Recht et al. 2010) which is a convex program, so it can always be computed in polynomial time.

Weight-tied neural networks.The work of Bietti et al. 2022 studies a similar two-layer neural network model with weight tying. Their focus is primarily on understanding the non-convex optimization of this model via gradient-based methods. Again, our generalization bound can be straightforwardly combined with the output of their algorithm. The use of norm-based bounds as in our result is common in the generalization theory for neural networks, see e.g. Bartlett (1996) and Anthony, Bartlett, et al. (1999). Compared to existing generalization bounds, the key advantage of our bound is its quantitative sharpness (e.g. small constants and no extra logarithmic factors).

## 3 Problem Formulation

In most of this paper, we consider the setting of generalized linear models. Given any loss function \(f:_{ 0}\) and independent sample pairs \((x_{i},y_{i})\) from some data distribution \(\) over \(^{d}\), we can learn a linear model \((,)\) by minimizing the empirical loss \(_{f}\) with the goal of achieving small population loss \(L_{f}\):

\[_{f}(w,b)=_{i=1}^{n}f( w,x_{i}+b,y_{i}),  L_{f}(w,b)=_{(x,y)}[f( w,x+b,y)].\] (1)

In the above, \(\) is an abstract label space. For example, \(=\) for linear regression and \(=_{ 0}\) for phase retrieval (section 5.1) and ReLU regression (section 5.2). If we view \(x_{i}\) as vectorization of the sensing matrices, then our setting (1) also includes the problem of matrix sensing (section 5.3).

For technical reasons, we assume that the distribution \(\) follows a Gaussian multi-index model (Zhou et al. 2022).

1. \(d\)-dimensional Gaussian features with arbitrary mean and covariance: \(x(,)\)
2. a generic multi-index model: there exist a low-dimensional projection \(W=[w_{1}^{*},...,w_{k}^{*}]^{d k}\), a random variable \(_{}\) independent of \(x\) (not necessarily Gaussian), and an unknown link function \(g:^{k+1}\) such that \[_{i}= w_{i}^{*},x, y=g(_{1},...,_{k},).\] (2)

We require \(x\) to be Gaussian because the proof of our generalization bound depends on the Gaussian Minimax Theorem (Gordon 1985; Thrampoulidis et al. 2014), as in many other works (Koehler et al. 2021; Zhou et al. 2022; Wang et al. 2021; Donhauser et al. 2022). In settings where the features are not Gaussian, many works (Goldt et al. 2020; Mei and Montanari 2022; Misiakiewicz 2022; Hu and Lu 2023; Han and Shen 2022) have established universality results showing that the test and training error are asymptotically equivalent to the error of a Gaussian model with matching covariance matrix. However, we show in section 7 that Gaussian universality is not always valid and we discuss potential extensions of the Gaussian feature assumption there.

The multi-index assumption is a generalization of a well-specified linear model \(y= w^{*},x+\), which corresponds to \(k=1\) and \(g(,)=+\) in (2). Since \(g\) is not even required to be continuous, the assumption on \(y\) is quite general. It allows nonlinear trend and heteroskedasticity, which pose significant challenges for the analyses using random matrix theory (but not for uniform convergence). In Zhou et al. (2022), the range of \(g\) is taken to be \(=\{-1,1\}\) in order to study binary classification. In section 5.2, we allow the conditional distribution of \(y\) to have a point mass at zero, which is crucial for distinguishing ReLU regression from standard linear regression.

## 4 Optimistic Rate

In order to establish uniform convergence, we need two additional technical assumptions because we have not made any assumption on \(f\) and \(g\).

3. hypercontractivity: there exists a universal constant \(>0\) such that uniformly over all \((w,b)^{d}\), it holds that \[[f( w,x+b,y)^{4}]^{1/4}}{[f( w,x +b,y)]}.\] (3)
4. the class of functions on \(^{k+1}\) defined below has VC dimension at most \(h\): \[\{(x,y) 1\{f( w,x,y)>t\}:(w,t)^{k+1} \}.\] (4)

Crucially, the quantities \(\) and \(h\) only depend on the number of indices \(k\) in assumption (B) instead of the feature dimension \(d\). This is a key distinction because \(k\) can be a small constant even when the feature dimension is very large. For example, recall that \(k=1\) in a well-specified model and it is completely free of \(d\). The feature dimension plays no role in equation (3) because conditioned on \(W^{T}x^{k}\) and \(\), the response \(y\) is non-random and the distribution of \( w,x\) for any \(w^{d}\) only depends on \(w^{T}\) and \(w^{T} w\) by properties of the Gaussian distribution.

The hypercontractivity (or "norm equivalence") assumption (3) is one of a few different common assumptions made in the statistical learning theory literature to rule out the possibility of a very heavy-tailed loss function. The reason is that if the loss function has a very tiny chance of being extremely large, it is not possible to state any meaningful learning guarantee from a finite number of samples. Hypercontractivity was used as an assumption already in Vapnik 1982. Another common and incomparable assumption made in the literature is boundedness, but this is not suitable for us because, e.g., the squared loss under the Gaussian distribution will not be bounded almost surely. On the other hand, if \(f\) is the squared loss then (3) holds with \(=\) by standard properties of the Gaussian distribution. We can also state other versions of our main theorem by using other results in statistical learning theory to handle the low-dimensional concentration; in our proof we simply use a result from Vapnik 1982 in a contained and black-box fashion. (See e.g. Mendelson 2017; Vapnik 1982 for more discussion of the different possible assumptions.)

Similar assumptions are made in Zhou et al. (2022) and more background on these assumptions can be found there, but we note that our assumption (C) and (D) are weaker than theirs because we do not require (3) and (4) to hold uniformly over all Moreau envelopes of \(f\).

We are now ready to state the uniform convergence results for square-root Lipschitz losses.

**Theorem 1**.: _Assume that (A), (B), (C), and (D) holds, and let \(Q=I-W(W^{T} W)^{-1}W^{T}\). For any \((0,1)\), let \(C_{}:^{d}[0,]\) be a continuous function such that with probability at least \(1-/4\) over \(x(0,)\), uniformly over all \(w^{d}\),_

\[ w,Q^{T}x C_{}(w).\] (5)

_If for each \(y\), \(f\) is non-negative and \(\) is \(\)-Lipschitz with respect to the first argument, then with probability at least \(1-\), it holds that uniformly over all \((w,b)^{d}\), we have_

\[(1-)L_{f}(w,b)(_{f}(w,b)}+(w)^{2}}{n}})^{2}\] (6)

_where \(=O(})\)._

Since the label \(y\) only depends on \(x\) through \(W^{T}x\), the matrix \(Q\) is simply a (potentially oblique) projection such that \(Q^{T}x\) is independent of \(y\). In the proof, we separate \(x\) into a low-dimensional component related to \(y\) and the independent component \(Q^{T}x\). We establish a low-dimensional concentration result using VC theory, which is reflected in the \(\) term that does not depend on \(d\), and we control the the remaining high-dimensional component using a scale-sensitive measure \(C_{}\).

The complexity term \(C_{}(w)/\) should be thought of as a (localized form of) Rademacher complexity \(_{n}\). For example, for any norm \(\|\|\), we have \( w,Q^{T}x\|w\|\|Q^{T}x\|_{*}\) and the Rademacher complexity for linear predictors with norm bounded by \(B\) is \(B\|x\|_{*}/\)(Shalev-Shwartz and Ben-David 2014). More generally, if we only care about linear predictors in a set \(\), then \(C_{}[_{w} w,Q^{T}x ]\) is simply the Gaussian width (Bandeira 2016; Gordon 1988; Koehler et al. 2021) of \(\) with respect to \(^{}=Q^{T} Q\) and is exactly equivalent to the expected Rademacher complexity of the class of functions (e.g., Proposition 1 of Zhou et al. 2021).

Applications

We now go on to analyze several different problems using optimistic rates and sqrt-lipschitz losses. One of the key insights we want to highlight is the _identification of the consistent loss_ for many of these problems -- in other word, the test loss which is implicitly being minimized. When we consider interpolators/overfit models, this is not obvious because an interpolator simultaneously minimizes many different training losses (and when we look at the test loss, each different loss functional may correspond to a different minimizer). Nevertheless, we are able to analyze phenomena like benign overfitting by identifying the particular sqrt-Lipschitz losses which the interpolator is consistent in, i.e. for which it approaches the minimum of the test loss in the appropriate limit.

### Benign Overfitting in Phase Retrieval

In this section, we study the problem of phase retrieval where only the magnitude of the label \(y\) can be measured. If the number of observations \(n<d\), the minimal norm interpolant is

\[=*{arg\,min}_{w^{d}: i[n], w,x_{i}^{2}=y_{i}^{2}}\|w\|_{2}.\] (7)

We are particularly interested in the generalization error of (7) when \(y_{i}\) are noisy labels and so \(\) overfits to the training set. It is well-known that gradient descent initialized at zero converges to a KKT point of the problem (7) when it reaches a zero training error solution (see e.g. Gunasekar et al., 2018). While is not always computationally feasible to compute \(\), we can study it as a theoretical model for benign overfitting in phase retrieval.

Due to our uniform convergence guarantee in Theorem 1, it suffices to analyze the norm of \(\) and we can use the norm calculation to find the appropriate loss for phase retrieval. The key observation is that for any \(w^{}^{d}\), we can let \(I=\{i[n]: w^{},x_{i} 0\}\) and the predictor \(w=w^{}+w^{}\) satisfies \(| w,x_{i}|=y_{i}\) where

\[w^{}=*{arg\,min}_{w^{d}:\\  i I, w,x_{i}=y_{i}-| w^{},x_{i}| \\  i I, w,x_{i}=| w^{},x_{i}|-y_{i} }\|w\|_{2}.\] (8)

Then it holds that \(\|\|_{2}\|w\|_{2}\|w^{}\|_{2}+\|w^{}\|_{2}\). By treating \(y_{i}-| w^{},x_{i}|\) and \(| w^{},x_{i}|-y_{i}\) as the residuals, the norm calculation for \(\|w^{}\|_{2}\) is the same as the case of linear regression (Koehler et al., 2021; Zhou et al., 2022). Going through this analysis yields the following norm bound (here \(R()=()^{2}/(^{2})\) is a measure of effective rank, as in (Bartlett et al., 2020)):

**Theorem 2**.: _Under assumptions (A) and (B), let \(f:\) be given by \(f(,y):=(||-y)^{2}\) with \(=_{ 0}\). Let \(Q\) be the same as in Theorem 1 and \(^{}=Q^{T} Q\). Fix any \(w^{}^{d}\) such that \(Qw^{}=0\) and for some \((0,1)\), it holds that_

\[_{f}(w^{})(1+)L_{f}(w^{}).\] (9)

_Then with probability at least \(1-\), for some \(+()(}+ )}}++)})\), it holds that_

\[_{w^{d}:\\  i[n], w,x_{i}^{2}=y_{i}^{2}}\|w\|_{2} \|w^{}\|_{2}+(1+)(w^{})}{ (^{})}}.\] (10)

Note that in this result we used the loss \(f(,y)=(||-y)^{2}\), which is 1-square-root Lipschitz and naturally arises in the analysis. We will now show that this is the _correct_ loss in the sense that benign overfitting is consistent with this loss. To establish consistency result, we can pick \(w^{}\) to be the minimizer of the population error. The population minimizer always satisfies \(Qw^{}=0\) because \(L_{f}((I-Q)w) L_{f}(w)\) for all \(w^{d}\) by Jensen's inequality. Condition (9) can also be easily checked because we just need concentration of the empirical loss at a single non-random parameter. Applying Theorem 1, we obtain the following.

**Corollary 1**.: _In the same setting as in Theorem 2, with probability at least \(1-\), it holds that for some_

\[}+(1/ )(}+)}}+ +)}),\]

_the minimal norm interpolant \(\) given by (7) enjoys the learning guarantee:_

\[L(,)(1+)(,b^{})}+\|w^{ }\|_{2}(^{})}{n}})^{2}.\] (11)

Therefore, \(\) is consistent if the same benign overfitting conditions from Bartlett et al. (2020) and Zhou et al. (2022) hold: \(k=o(n),R(^{})=(n)\) and \(\|w^{}\|_{2}^{2}(^{})=o(n)\).

### Benign Overfitting in ReLU Regression

Since we only need \(\) to be Lipschitz, for any 1-Lipschitz function \(:\), we can consider

1. \(f(,y)=(()-y)^{2}\) when \(=\)
2. \(f(,y)=(1-()y)_{+}^{2}\) when \(=\{-1,1\}\).

We can interpret the above loss function \(f\) as learning a neural network with a single hidden unit. Indeed, Theorem 1 can be straightforwardly applied to these situations. However, we do not always expect benign overfitting under these loss functions for the following simple reason, as pointed out in Shamir (2022): when \(\) is invertible, interpolating the loss \(f(,y)=(()-y)^{2}\) is the same as interpolating the loss \(f(,y)=(-^{-1}(y))^{2}\) and so the learned function would be the minimizer of \([( w,x+b-^{-1}(y))^{2}]\) which is typically different from the minimizer of \([(( w,x+b)-y)^{2}]\).

The situation of ReLU regression, where \(()=\{,0\}\), is more interesting because \(\) is _not_ invertible. In order to be able to interpolate, we must have \(y 0\). If \(y>0\) with probability 1, then \(()=y\) is the same as \(=y\) and we are back to interpolating the square loss \(f(,y)=(-y)^{2}\). From this observation, we see that \(f(,y)=(()-y)^{2}\) cannot be the appropriate loss for consistency even though it's 1 square-root Lipschitz. In contrast, when there is some probability mass at \(y=0\), it suffices to output any non-positive value and the minimal norm to interpolate is potentially smaller than requiring \(=0\). Similar to the previous section, we can let \(I=\{i[n]:y_{i}>0\}\) and for any \((w^{},b^{})^{d+1}\), the predictor \(w=w^{}+w^{}\) satisfies \(( w,x_{i}+b^{})=y_{i}\) where

\[w^{}=*{arg\,min}_{w^{d}:\\  i I, w,x_{i}=y_{i}- w^{},x_{i} -b^{}\\  i I, w,x_{i}=-( w^{},x_{i} +b^{})}\|w\|_{2}\] (12)

Our analysis will show that the consistent loss for benign overfitting with ReLU regression is

\[f(,y)=(-y)^{2}& y>0\\ ()^{2}& y=0..\] (13)

We first state the norm bound below.

**Theorem 3**.: _Under assumptions (A) and (B), let \(f:\) be the loss defined in (13) with \(=_{ 0}\). Let \(Q\) be the same as in Theorem 1 and \(^{}=Q^{T} Q\). Fix any \((w^{},b^{})^{d+1}\) such that \(Qw^{}=0\) and for some \((0,1)\), it holds that_

\[_{f}(w^{},b^{})(1+)L_{f}(w^{},b^{}).\] (14)

_Then with probability at least \(1-\), for some \(+()(}+)}}++)})\), it holds that_

\[_{(w,b)^{d+1}:\\  i[n],( w,x_{i}+b)=y_{i}}\|w\|_{2} \|w^{}\|_{2}+(1+)(w^{},b^{})}{ (^{})}}.\] (15)

Since the loss defined in (13) is also 1-square-root Lipschitz, it can be straightforwardly combined with Theorem 1 to establish benign overfitting in ReLU regression under this loss. The details are exactly identical to the previous section and so we omit it here.

### Benign Overfitting in Matrix Sensing

We now consider the problem of matrix sensing: given random matrices \(A_{1},...,A_{n}\) (with i.i.d. standard Gaussian entries) and independent linear measurements \(y_{1},...,y_{n}\) given by \(y_{i}= A_{i},X^{*}+_{i}\) where \(_{i}\) is independent of \(A_{i}\), and \(=0\) and \(^{2}=^{2}\), we hope to reconstruct the matrix \(X^{*}^{d_{1} d_{2}}\) with sample size \(n d_{1}d_{2}\). To this end, we assume that \(X^{*}\) has rank \(r\). In this setting, since the measurement matrices have i.i.d. standard Gaussian entries, the test error is closely related to the estimation error:

\[L(X)=( A,X-y)^{2}=\|X-X^{*}\|_{F}^{2}+^{2}.\]

The classical approach to this problem is to find the minimum nuclear norm solution:

\[=*{arg\,min}_{X^{d_{1} d_{2}}: A _{i},X=y_{i}}\|X\|_{*}.\] (16)

Gunasekar et al. (2017) also shows that gradient descent converges to the minimal nuclear norm solution in matrix factorization problems. It is well known that having low nuclear norm can ensure generalization (Foygel and Srebro, 2011; Srebro and Shraibman, 2005) and minimizing the nuclear norm ensures reconstruction (Candes and Recht, 2009; Recht et al., 2010). However, if the noise level \(\) is high, then even the minimal nuclear norm solution \(\) can have large nuclear norm. Since our result can be adapted to different norms as regularizer, our uniform convergence guarantee can be directly applied. The dual norm of the nuclear norm is the spectral norm, and it is well-known that the spectrum norm of a Gaussian random matrix is approximately \(}+}\)(Vershynin, 2018). It remains to analyze the minimal nuclear norm required to interpolate. For simplicity, we assume that \(\) are Gaussian below, but we can extend it to be sub-Gaussian.

**Theorem 4**.: _Suppose that \(d_{1}d_{2}>n\), then there exists some \(}+d_{2}}\) such that with probability at least \(1-\), it holds that_

\[_{ i[n], A_{i},X=y_{i}}\|X\|_{*}\|X^{ *}\|_{F}+(1+)}{d_{1} d_{2}}}.\] (17)

Without loss of generality, we will assume \(d_{1} d_{2}\) from now on because otherwise we can take the transpose of \(A\) and \(X\). Similar to assuming \(n/R(^{}) 0\) in linear regression, we implicitly assume that \(n/d_{1}d_{2} 0\) in matrix sensing. Such scaling is necessary for benign overfitting because of the lower bound on the test error for _any_ interpolant (e.g., Proposition 4.3 of Zhou et al., 2020). Finally, we apply the uniform convergence guarantee.

**Theorem 5**.: _Fix any \((0,1)\). There exist constants \(c_{1},c_{2},c_{3}>0\) such that if \(d_{1}d_{2}>c_{1}n\), \(d_{2}>c_{2}d_{1}\), \(n>c_{3}r(d_{1}+d_{2})\), then with probability at least \(1-\) that_

\[-X^{*}\|_{F}^{2}}{\|X^{*}\|_{F}^{2}}+d_{2} )}{n}++d_{2})}{n}}\|_{F}}+(}{d_{2}}}+d_{2}})}{\|X^{*}\|_{ F}^{2}}.\] (18)

From Theorem 5, we see that when the signal to noise ratio \(\|_{F}^{2}}{^{2}}\) is bounded away from zero, then we obtain consistency \(-X^{*}\|_{F}^{2}}{\|X^{*}\|_{F}^{2}} 0\) if (i) \(r(d_{1}+d_{2})=o(n)\), (ii) \(d_{1}d_{2}=(n)\), and (iii) \(d_{1}/d_{2}\{0,\}\). This can happen for example when \(r=(1),d_{1}=(n^{1/2}),d_{2}=(n^{2/3})\). As discussed earlier, the second condition is necessary for benign overfitting, and the first consistency condition should be necessary even for regularized estimators. It is possible that the final condition is not necessary and we leave a tighter understanding of matrix sensing as future work.

## 6 Single-Index Neural Networks

We show that our results extend a even more general setting than (1). Suppose that we have a parameter space \(^{p}\) and a continuous mapping \(w\) from \(\) to a linear predictor \(w()^{d}\). Given a function \(f:\) and i.i.d. sample pairs \((x_{i},y_{i})\) drawn from distribution \(\), we consider training and test error of the form:

\[()=_{i=1}^{n}f( w(),x_{i},y_{i}, ) L()=[f( w(),x, y,)].\] (19)We make the same assumptions (A) and (B) on \(\). Assumptions (C) and (D) can be naturally extended to the following:

* there exists a universal constant \(>0\) such that uniformly over all \(\), it holds that \[[f( w(),x,y,)^{4}]^{1/4}}{[ f( w(),x,y,)]}.\] (20)
* bounded VC dimensions: the class of functions on \(^{k+1}\) defined by \[\{(x,y)\{f( w,x,y,)>t\}:(w,t, )^{k+1}\}\] (21) has VC dimension at most \(h\).

Now we can state the extension of Theorem 1.

**Theorem 6**.: _Suppose that assumptions (A), (B), (E) and (F) hold. For any \((0,1)\), let \(C_{}:^{d}[0,]\) be a continuous function such that with probability at least \(1-/4\) over \(x(0,)\), uniformly over all \(\),_

\[ w(),Q^{T}x C_{}(w()).\] (22)

_If for each \(\) and \(y\), \(f\) is non-negative and \(\) is \(}\)-Lipschitz with respect to the first argument, and \(H_{}\) is continuous in \(\), then with probability at least \(1-\), it holds that uniformly over all \(\), we have_

\[(1-)L()(()}+\,C_{}(w())^{2}}{n}})^{2}.\] (23)

Next, we show that the generality of our result allows us to establish uniform convergence bound for two-layer neural networks with weight sharing. In particular, we let \((x)=(x,0)\) be the ReLU activation function and \(=(w,a,b)^{d+2N}\), where \(N\) is the number of hidden units. Consider the loss function

\[f(,y,)=(_{i=1}^{N}a_{i}(-b_{i})-y)^{2 } f(,y,)=(1-_{i=1}^{N}a_{i}( -b_{i})y)_{+}^{2},\] (24)

then \(L()\) is the test error of a neural network of the form \(h_{}(x):=_{i=1}^{N}a_{i}( w,x-b_{i})\). Since our uniform convergence guarantee holds uniformly over all \(\), it applies to networks whose first and second layer weights are optimized at the same time. Without loss of generality, we can assume that \(b_{1}... b_{N}\) are sorted, then it is easy to see that \(\) is \(_{j[N]}|_{i=1}^{j}a_{i}|\) Lipschitz. Applying Theorem 6, we obtain the following corollary.

**Corollary 2**.: _Fix an arbitrary norm \(\|\|\) and consider \(f\) as defined in (24). Assume that the data distribution \(\) satisfy (A), (B), and (E). Then with probability at least \(1-\), it holds that uniformly over all \(=(w,a,b)^{d+2N}\), we have_

\[(1-)L()(()}+|_{i=1}^{j}a_{i}|\,\|w\|(\|Q^{T}x\|_{*}+ ^{})}{})^{2}\] (25)

_where \(\) is the same as in Theorem 6 with \(h=(k+N)\) and \(^{}=O(_{\|u\| 1}\|u\|_{^{}})\)._

The above theorem says given a network parameter \(\), after sorting the \(b_{i}\)'s, a good complexity measure to look at is \(_{j[N]}|_{i=1}^{j}a_{i}|\|w\|\) and equation (25) precisely quantify how the complexity of a network controls generalization.

## 7 On Gaussian Universality

In this section, we discuss a counterexample to Gaussian universality motivated by Shamir (2022). In particular, we consider a data distribution \(\) over \((x,y)\) given by:* \(x=(x_{|k},x_{|d-k})\) where \(x_{|k}(0,_{|k})\) and there exists a function \(h:^{k}\) such that \[x_{|d-k}=h(x_{|k}) z z(0,_{|d-k}) x_{|k}\] (26)
* there exists a function \(g:^{k+1}\) such that \[y=g(x_{|k},)\] (27) where \(_{}\) is independent of \(x\) (but not necessarily Gaussian).

By the independence between \(z\) and \(x_{|k}\), we can easily see that \(x_{|k}\) and \(x_{|d-k}\) are uncorrelated. Moreover, the covariance matrix of \(x_{|d-k}\) is

\[[x_{|d-k}x_{|d-k}^{T}]=[h(x_{|k})^{2}]_{|d-k}\]

which is just a re-scaling of \(_{|d-k}\) and therefore has the same effective rank as \(R(_{|d-k})\). Even though the feature \(x\) in \(\) is non-Gaussian, its tail \(x_{|d-k}\) is Gaussian conditioned on \(x_{|k}\). Therefore, our proof technique is still applicable after a conditioning step. However, it turns out that the uniform convergence bound in Theorem 1 is no longer valid. Instead, we have to re-weight the loss function. The precise theorem statements can be found in the appendix. We briefly describe our theoretical results below, which follow the same ideas as in section 4 and 5.

Uniform Convergence.Under a similar low-dimensional concentration assumption such as (C), if we let \(C_{}\) be such that \( w,z C_{}(w)\) for all \(w^{d-k}\) with high probability, it holds that

\[[)^{2}}](1+o(1 ))(_{i=1}^{n},y_{i})}{h(x_{ |k})^{2}}+(w_{|d-k})}{})^{2}.\] (28)

Note that the distribution of \(z\) is specified in (G) and different to the distribution of \(x_{|d-k}\).

Norm Bound.Next, we focus on linear regression and compute the minimal norm required to interpolate. If we pick \(_{|d-k}\) to satisfy the benign overfitting conditions, for any \(w^{*}_{|k}^{k}\), we have

\[_{w^{d}:\\  i[n], w,x_{i}=y_{i}}\|w\|_{2}^{2} \|w^{*}_{|k}\|_{2}^{2}+(1+o(1))[(_{|k},x_{|k}}{h(x_{|k})})^{2}]}{( _{|d-k})}\] (29)

It is easy to see that the \(w^{*}\) that minimizes the population weighted square loss satisfy \(w^{*}_{|d-k}=0\), and so we can let \(w^{*}_{|k}\) to be the minimizer.

Benign Overfitting.Let \(=_{w^{d}:Xw=Y}\|w\|_{2}\) be the minimal norm interpolant. Plugging in the norm bound (29) into the uniform convergence guarantee (28), we show that

\[[,x-y)^{2}}{h(x_{|k})^{2}}] (1+o(1))\|_{2}^{2}(_{|d-k})}{n} [,x-y)^{2}}{h(x_{|k})^{2}}].\] (30)

which recovers the consistency result of Shamir (2022).

Contradiction.Suppose that Gaussian universality holds, then our optimistic rate theory would predict that

\[[(,x-y)^{2}](1+o(1))\|_{2}^{ 2}[h(x_{|k})^{2}](_{|d-k})}{n}.\] (31)

Combining (30) with (31), we obtain that

\[_{w}\,[( w,x-y)^{2}](1+o(1))_{w}\, [h(x_{|k})^{2}][()})^{2}]\] (32)which cannot always hold. For example, let's consider the case where \(k=1\), \(x_{1}(0,1)\), \(h(x_{1})=1+|x_{1}|\) and \(y=h(x_{1})^{2}\). Then it is straightforward to verify that the left hand side of (32) equals \([h(x_{1})^{4}]\) and the right hand side equals \([h(x_{1})^{2}]^{2}\), but this is impossible because

\[[h(x_{1})^{4}]-[h(x_{1})^{2}]^{2}=(h(x_{1})^{ 2})>0.\]

In the counterexample above, we see that it is possible to introduce strong dependence between the signal and the tail component of \(x\) while ensuring that they are uncorrelated. The dependence will prevent the norm of the tail from concentrating around its mean, no matter how large the sample size is. In contrast, the norm of the tail will concentrate for Gaussian features with a matching covariance -- such discrepancy results in an over-optimistic bound for non-Gaussian data.

## 8 Conclusion

In this paper, we extend a type of sharp uniform convergence guarantee proven for the square loss in Zhou et al. (2021) to any _square-root Lipschitz_ loss. Uniform convergence with square-root Lipschitz loss is an important tool because the appropriate loss to study interpolation learning is usually square-root Lipschitz instead of Lipschitz. Compared to the prior work of Zhou et al. 2022, our view significantly simplify the assumptions to establish optimistic rate. Since we don't need to explicitly compute the Moreau envelope for each application, our framework easily leads to many novel benign overfitting results, including low-rank matrix sensing.

In the applications to phase retrieval and ReLU, we identify the appropriate loss function \(f\) and our norm calculation overcomes the challenge that \(f\) is non-convex and so CGMT cannot be directly applied. Furthermore, we explore new extensions of the uniform convergence technique to study single-index neural networks and suggest a promising research direction to understand the generalization of neural networks. Finally, we argue that Gaussian universality cannot always be taken for granted by analyzing a model where only the weighted square loss enjoys an optimistic rate. Our results highlight the importance of tail concentration and shed new lights on the necessary conditions for universality. An important future direction is to extend optimistic rate beyond Gaussian data, possibly through worst-case Rademacher complexity. Understanding the performance of more practical algorithms in phase retrieval and deriving the necessary and sufficient conditions for benign overfitting in matrix sensing are also interesting problems.

Acknowledgements.F.K. was supported in part by NSF award CCF-1704417, NSF award IIS-1908774, and N. Anari's Sloan Research Fellowship.