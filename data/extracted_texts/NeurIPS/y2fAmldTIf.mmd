# HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning

Yancheng Zhang

University of Central Florida

Mengxin Zheng

University of Central Florida

Yuzhang Shang

Illinois Institute of Technology

Xun Chen

Samsung Research America

Qian Lou

University of Central Florida

###### Abstract

Non-interactive cryptographic computing, Fully Homomorphic Encryption (FHE), provides a promising solution for private neural network training on encrypted data. One challenge of FHE-based private training is its large computational overhead, especially the multiple rounds of forward and backward execution on each encrypted data sample. Considering the existence of largely redundant data samples, pruning them will significantly speed up the training, as proven in plain non-FHE training. Executing the data pruning of encrypted data on the server side is not trivial since the knowledge calculation of data pruning needs complex and expensive executions on encrypted data. There is a lack of FHE-based data pruning protocol for efficient, private training. In this paper, we propose, _HEPrune_, to construct a FHE data-pruning protocol and then design an FHE-friendly data-pruning algorithm under client-aided or non-client-aided settings, respectively. We also observed that data sample pruning may not always remove ciphertexts, leaving large empty slots and limiting the effects of data pruning. Thus, in HEPrune, we further propose ciphertext-wise pruning to reduce ciphertext computation numbers without hurting accuracy. Experimental results show that our work can achieve a \(16\) speedup with only a \(0.6\%\) accuracy drop over prior work. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/Private-Data-Prune.

## 1 Introduction

Machine learning, especially deep neural networks, has been widely applied in various domains, including healthcare , finance , and so forth. Due to the lack of expertise and computational resources, the average user often outsources the training task to the cloud servers in the machine-learning-as-a-service (MLaaS) setting. However, the training data is often highly private and private, and it should not be directly shared because of business, legal, and ethical constraints. Private training enables the cloud server to train machine learning models on encrypted data, where strong privacy guarantees are offered by cryptographic primitives such as FHE .

However, FHE-based private training is significantly hindered by its substantial execution time. For example, the encrypted training can be \(1 3\) orders of magnitudes slower than in the plaintext training . The high execution time primarily stems from the transformation of a plaintext dataset into an encrypted format, along with the substitution of straightforward plaintext computations with more resource-intensive ciphertext operations. Reducing the number of data samples used for training could greatly accelerate the process. To optimize plaintext datasets, methods like dataset pruning are commonly employed, where the model trainer has access to both the dataset and model.

[MISSING_PAGE_FAIL:2]

overhead while achieving generalization performance comparable to training on the full dataset [10; 11]. Several methods have been proposed to identify the most informative data samples. Among these, score-based methods [7; 8; 9; 10] are widely used for their simplicity and effectiveness. These methods compute sample-wise importance scores during training and select the most informative samples based on these scores.

The importance score can be as simple as the entropy loss [8; 10] for each sample. Another commonly used score is the forgetting score , which counts the number of "forgetting events" that occur for each sample during training, where a forgetting event is defined as a change in the model's prediction for the same sample between two consecutive epochs. GraNd  uses the \(_{2}\)-norm of the sample-wise gradient to represent sample importance, while ELN2  approximates GraNd by calculating the \(_{2}\)-norm of the error vector. Additional methods for identifying sample importance include submodularity-based methods like GraphCut  and gradient-matching-based methods like GradMatch . These methods typically start with an empty set and gradually add samples, in contrast to score-based methods, where sample importance scores are computed simultaneously and independently.

Data pruning methods are generally divided into static and dynamic pruning methods. Static pruning refers to data pruning performed once before training [7; 8; 9; 26], while dynamic pruning involves pruning data every few epochs during training [12; 11; 10]. Although these methods were designed with different motivations and target settings, they can be adapted for use in either setting. For instance,  uses entropy loss in a static setting, while [11; 10] apply it in a dynamic setting. Similarly, dynamic pruning methods like [26; 12] can be adapted to static settings, as demonstrated by , without losing effectiveness. In this paper, we focus on dynamic pruning.

**Motivation.** FHE-enabled private training provides a strong privacy guarantee for user data. However, private training remains significantly slower in runtime compared to unencrypted training. One primary reason for this is the large dataset size typically used for training, often comprising tens of thousands of samples. As shown in Figure 1(a), removing a fraction of data during training has minimal impact on model accuracy: even with only \(30\%\) of the data, the accuracy drop is within \(2\%\). However, directly applying plaintext data pruning methods to private training might not only fail to accelerate the process but could actually extend the training time. As shown in Figure 1(b), computing importance scores with existing methods--such as the forgetting score , entropy score [11; 10], GraNd , and EL2N --can introduce prohibitive overhead. This inefficiency arises because FHE natively supports only addition and multiplication, while existing data pruning methods often require a range of non-linear operations. Although these non-linear operations can be implemented in FHE through lookup tables or approximations, these computations are extremely costly in FHE, potentially negating the benefits of data pruning. Furthermore, as illustrated in Figure 1(c), although existing methods can effectively prune samples from datasets, they do not necessarily reduce the number of ciphertexts in private training. This is because, during private training, a single ciphertext can contain multiple samples. Naively applying sample-wise data pruning can lead to a large number of sparse ciphertexts that cannot be excluded from training.

**Threat Model.** We consider a private training scenario where a client outsources model training to a cloud server. Our protocol is designed to allow a client to outsource model training while preserving the privacy of their data and model weights. The client's private dataset and model weights are treated as intellectual property that must not be disclosed to the server at any point. We assume the server is

Figure 1: (a). Test accuracy under various pruning fractions in plaintext dataset (ResNet-18 on CIFAR-10). (b) The encrypted training overhead and accuracy of different data pruning methods. (c) The distribution of data sample numbers in a ciphertext after data pruning. Only a small fraction of ciphertexts are empty with a na√Øve sample-wise pruning.

semi-honest, meaning it will follow the protocol specifications but may attempt to gain unauthorized knowledge, such as the dataset and model weights. Sharing certain meta-information about training, such as the model architecture, dataset size, and early stopping signal, is assumed to be safe . Side-channel attacks are beyond the scope of this paper.

## 3 HEPrune Design

We begin by outlining the pipeline for private training with encrypted data pruning in Section 3.1. In Section 3.2, we provide details of our FHE-based data pruning algorithm. Specifically, we construct an encrypted data pruning baseline to demonstrate how sample importance evaluation and sample removal can be performed using pure FHE operations. We then enhance the efficiency of the data pruning algorithm through FHE-friendly scoring (HEFS) and client-aided masking (CAM). Finally, in Section 3.3, we introduce ciphertext-wise pruning (CWP), which removes sparse ciphertexts and further accelerates the data pruning process.

### Pipeline of Private Training with Encrypted Data Pruning

It has been widely studied how to perform the gradient descent algorithm in the encrypted state during FHE-enabled private training [4; 5; 6]. However, it remains unclear how data pruning can be incorporated into existing private training frameworks. We illustrate our FHE-based encrypted data pruning framework in Figure 2. During private training, \(\) the client first encrypts the dataset with FHE and sends the encrypted dataset to the server. From this point forward, the dataset will never be decrypted. We refer to the encrypted data samples as ciphertexts. To identify the most informative subset of samples, \(\), the server first runs the forward pass algorithm on all ciphertexts to obtain the encrypted prediction vector \(P\). \(\) With \(P\) and the encrypted ground truth label \(Y\), the server computes the sample-wise importance score \(S=(x)\). We propose an HE-friendly importance score in Section 3.2. \(\) With the importance score, the server can decide which samples to prune. This is achieved by sorting the importance score and finding the threshold importance score \(}\) corresponding to the pruning ratio . A pruning mask is generated by comparing the importance score with the threshold as \(M=\{(x)<_{t}\}\). The unimportant samples can be masked by simply multiplying the ciphertexts with the corresponding masks. \(\) To fully reduce the number of ciphertexts, the server performs ciphertext-wise pruning according to the current pruning mask. \(\) The server performs a backpropagation algorithm on the pruned ciphertexts for \(\) epochs, and then \(\) starts a new round of the data pruning algorithm to update the training subset. \(\) At the end of private training, the server sends the encrypted model back to the client, and the client decrypts the model.

We formalize the above workflow into an encrypted data pruning protocol in Figure 5 in Appendix A. Our protocol is the first of its kind to introduce data pruning to private training and is an enhanced private training framework with encrypted data pruning extension over existing works [4; 5; 6]. Additionally, our protocol can work in both the transfer learning setting [5; 6] and training-from-scratch setting . Our protocol is fully compatible with the early stop techniques adopted in . As in , sending meta information during training is allowed in our protocol, such as the logits, early stop signals, and importance scores. The privacy of data and models is strongly guaranteed as they are encrypted and never decrypted by the server. We first instantiate a baseline of encrypted data pruning using only FHE operations in Section 3.2, and then propose HEFS and CAM to optimize the overhead of the encrypted data pruning algorithm. In Section 3.3, we propose CWP to effectively reduce the number of ciphertexts involved in training and thus boost the efficiency of private training.

Figure 2: The overall workflow of private training with encrypted data pruning.

### HE-enabled Data Pruning

While FHE can support arbitrary computation on encrypted data, computing non-linear functions is typically expensive. Applying existing data pruning methods to private training can lead to additional overhead that negates or even exceeds the benefits of performing data pruning. For the entropy-based methods [8; 11; 10], although the entropy loss is typically considered cost-free in the plaintext, it is not explicitly computed during private training. Instead, we compute the gradient directly without calculating the loss itself, as illustrated in Figure 5, step 3(b). Computing the entropy loss in FHE requires approximating the logarithm function, which is computationally expensive. The forgetting score  is simpler to compute, requiring only comparison operations between the current prediction and prediction from the last epoch. However, the forgetting score cannot be easily made dynamic, as it usually requires calculations over the full dataset for multiple epochs. The most viable existing data pruning method is the EL2N , which simply utilizes the prediction \(P\) and label \(Y\). Although originally proposed as a static pruning method, we find that EL2N also remains effective in a dynamic setting, similar to the entropy score . We first instantiate an FHE-enabled data pruning algorithm, detailed in Algorithm 1, based on the dynamic version of EL2N (HE2LN). Subsequently, we demonstrate that even EL2N remains computationally expensive in the encrypted state and propose the HE-friendly score (HEFS) and client-aided masking (CAM) to accelerate the encrypted data pruning.

``` Input :The encrypted dataset \(=\{_{i},_{i}\}_{i=0}^{C-1}\) and the model \(\) with weights \(\), the pruning ration \(p\), the sample number \(N\) and the target class number \(n\). Output :The pruned encrypted dataset \(^{}=\{^{}{}_{i},^{}{}_{i}\}_{i=0}^{C^ {}-1}\), where \(C^{} C\). for\(i 0\)to \(C-1\)do \(_{i}((_{i};))\) ; // Prune.Eval \(_{i}_{i}_{i}\) ; // compute the error vector \(_{i}_{i}_{i}\); \(err_{_{i}}=_{i}\); for\(j 0\)to\(n-1\)do \(err_{i}(err_{_{i}},2^{j})\); \(err_{_{i}} err_{_{i}}_{i}\); \(sc_{i}(err_{_{i}})\) ; // compute the importance score \(s\_score\{\}\); // Prune.Remove \(B N/\)\(C\); for\(i 0\)to\(C-1\)do for\(j 0\)to\(B-1\)do \(s\_score_{iC+j}(sc_{i},j)\) ; // extract sample-wise scores for\(i 0\)to\(N-1\)do \(less(s\_score_{i},s\_score_{j})\); // sort the sample-wise scores \((s\_score_{i})(less s\_score_{j})((1) s\_score_{i})\); \((s\_score_{j})( s\_score_{i})((1 ) s\_score_{j})\); \(threshold\_index N p\); \( s\_{_{threshold\_index}}\); for\(i 0\)to\(C-1\)do \(mask(sc_{i},)\) ; // remove unimportant samples \((_{i},_{i})(mask)((1 )(_{i},_{i}))\); return\(^{}\{^{}{}_{i},^{}{}_{i}\}_{i=0}^{C-1}\) ```

**Algorithm 1**Homomorphic Data Pruning

**HEL2N Baseline.** The EL2N score is defined as \((x)=_{w_{t}}\|p(x;w_{t})-y\|_{2}\), where \(p(x;w_{t})\) is the prediction vector for sample \(x\) and \(y\) the corresponding ground truth label. In essence, the EL2N score is the \(_{2}\)-norm of the error vector, which can be computed via pure FHE operation as shown in Algorithm 1. We first compute the prediction vector \(P\) by encrypted forward pass. The HE.SoftMax in the forward pass is evaluated by polynomial approximation  and domain extension technique . Computing the forward pass in the encrypted state is the same as existing private training frameworks . With the encrypted prediction vector \(P\) and the ground truth label\(Y\), we can compute the EL2N score homomorphically. Specifically, we first compute the sum of squares over the error vector, which requires homomorphic subtraction, multiplication, and rotation. Then we take the square root of sum of squares to compute the \(_{2}\)-norm of the error vector. While HE.5\(\) can be implemented via Newton iterative algorithm , the overhead is prohibitive as the Newton iterative algorithm needs \( 10\) iterations to compute an accurate square root. Additionally, the Newton iterative algorithm in FHE involves considerable ciphertext-ciphertext multiplication. This leads to a large number of additional relinearization and bootstrapping operations. A single square root over a ciphertext can take up to \(2\) minutes.

**HE-friendly Score.** Simple as the EL2N score is, using it to evaluate the importance of data samples in the encrypted state can still incur prohibitive overhead due to its complex non-linearity. We propose an HE-friendly importance score (HEFS) to address this issue. We first derive the formulation of HEFS, and then demonstrate how it can be computed via pure FHE operation. To determine how a single data sample affects the training, we can quantify the importance of a sample by the difference of the gradient before and after removing a sample. Denote the gradient of a sample \((x,y)\) over the weights at time \(t\) as \(g_{t}(x,y)=_{w_{t}}(p(w_{t-1},x),y)\). Given a minibatch of \(B\) samples \(S=\{(x_{i},y_{i})\}_{i=0}^{B-1}\), the importance of a sample can be quantified by the difference of the time derivative of the loss function, \(_{t}\), before and after removing the sample from the minibatch. The difference of the derivative is bounded by the sample's gradient . Specifically, let \(S_{ k}=S\{(x_{k},y_{k})\}\) be the set after removing a certain sample \((x_{k},y_{k})\). For \((x_{i},y_{i}) S\) and \(i k\), it holds that

\[\|_{t}((x_{i},y_{i}),S)-_{t}((x_{i},y_{i}),S_{ k})\|_{2} c \|g_{t}(x_{k},y_{k})\|_{2}\] (1)

EL2N approximates the \(_{2}\)-norm of the gradient by the \(_{2}\)-norm of the error vector. We further streamline the EL2N score using the \(_{1}\)-norm. More formally, we define the HEFS as:

\[(x)=_{w_{t}}\|p(x;w_{t})-y\|_{1}\] (2)

HEFS can be efficiently computed during private training. Specifically, the circuit for HEFS consists of only two FHE subtractions and one max operation, which can be computed as:

\[score=((Y P),(P Y))\] (3)

In the above equation, the homomorphic subtraction \(\) is significantly faster than other FHE operations. \(\) can be effectively computed via HE.Cmp. While the HE.Max operation has the same time complexity as HE.Cmp, the concrete runtime of HE.Max is even more efficient, typically \(1.5 2\) faster under the same parameter setting . The proposed HEFS is a close approximation to the original EL2N score, which guarantees the effectiveness of the HEFS-based data pruning. We show the accuracy of the data pruning using HEFS in Section 4.

**Client-aided Masking.** After evaluating the importance of each data sample, we need to remove the less informative ones from the training set. This requires the server to sort all importance scores homomorphically to determine the threshold of important scores, \(}\). Given a dataset with \(N\) data samples, \(O(N^{2})\) homomorphic comparisons are needed. Since \(N\) is typically large for machine learning model training, such sorting incurs prohibitive overhead in the encrypted state, which can offset the benefits of data pruning or even prolong the total training time. To effectively identify and remove the less important data samples, we propose client-aided masking (CAM) in Algorithm 2. We offer an analysis on the computation, communication overhead, and security implications as follows.

**Efficiency.** In contrast to the heavy server-side homomorphic sorting, finding \(_{t}\) is extremely fast on the client's side, with only \(O(N)\) runtime via the quick select algorithm. In practice, this process takes only \(15\)\(ms\) on the CIFAR-10 dataset. Additionally, the communication overhead is minimal.

For the CIFAR-10 dataset with \(43,750\) training samples, only \(2\) CKKS ciphertexts are needed to transfer the encrypted importance scores when the slots are fully utilized. This incurs only \(4\) MB of communication overhead. For comparison, the early stopping signals used to determine early stopping incur \(18\) MB communication overhead when transferring the encrypted logits .

**Security.** Directly asking the client to decrypt and reveal the model weights and datasets to the server clearly breaches the client's privacy and is therefore prohibited. However, exchanging meta information about training does not directly compromise private information and is typically permitted. For instance, HETAL  allows the server to send the logits of the validation set to the client, who then computes the loss and determines whether to stop training early. Such exchanges of meta information are crucial to ensure the effectiveness of private training. Further details can be found in Appendix B.

### Ciphertext-wise Pruning

While the HEFS and CAM make the encrypted data pruning algorithm much more practical, the resulting ciphertexts remain largely sparse, thus limiting the training time acceleration. To this end, we propose ciphertext-wise pruning to effectively reduce the number of ciphertexts involved in training, as illustrated in Figure 3, which further boosts the efficiency of the private training. We detail the steps of ciphertext-wise pruning in Algorithm 3. Without loss of generality, we refer to all the slots occupied by a single sample within a ciphertext as a sample slot. Once the server obtains the pruning mask, ciphertext-wise pruning can be performed without client involvement.

We denote the number of samples in each ciphertext as \(B\) and \(M_{i}\) is a Boolean array that indicates whether each of the \(B\) samples should be pruned. The server first computes the sparsity of each ciphertext, which is done by simply counting the number of \(0\)-s in each \(M_{i}\). After computing the ciphertext-wise sparsity, the server can sort the ciphertexts along with their corresponding masks. We represent the sorted ciphertexts in a dequeue. To perform ciphertext-wise pruning, the server first identifies two ciphertexts \(ct_{front}\) and \(ct_{back}\) from the queue. After removing the full ciphertexts and empty ciphertexts from the queue via \(()\), we then leverage the most sparse ciphertext \(ct_{back}\) to fill in the empty slots in the least sparse ciphertexts \(ct_{front}\). The key step is to align samples in \(ct_{back}\) with the empty slots in \(ct_{front}\). The function \(()\) returns the index of the first empty sample slot \(slot_{empty}\) in \(ct_{front}\), sets the corresponding mask to \(1\), and decreases the sparsity of \(ct_{front}\) by \(1\). Similarly, \(()\) returns the first non-empty sample slot \(slot_{used}\) in \(ct_{back}\) and sets the corresponding mask and sparsity. By taking the difference of \(slot_{used}\) and \(slot_{empty}\), the server can

Figure 3: Example of ciphertext-wise pruning.

determine how much the \(ct_{back}\) should be rotated to align with \(ct_{front}\). We mask out the other slots in \(ct_{back}\) and obtain \(ct_{align}\) and keep only the first sample slot \(slot_{used}\). This ensures only the empty sample slot in \(ct_{front}\) is filled and the non-empty slots in \(ct_{front}\) will not be corrupted. Then, \(ct_{align}\) is rotated and added to \(ct_{front}\) to fill in the empty slot in \(ct_{front}\). After merging the two ciphertexts, if \(ct_{front}\) is full or there are no remaining ciphertexts to merge, the server adds it to the final set \(^{}\). We defer the details about \(()\), \(()\),\(()\) and \(()\) to the Appendix D. We remark the pruning mask generated by CAM is not encrypted, while enables efficient operations related to the pruning mask without invoking heavy FHE computation.

## 4 Experiments

### Experimental Setup

**Models and Datasets.** To demonstrate the generalizability, we evaluate the proposed encrypted data pruning methods in two settings, the transfer learning setting as in HETAL  and training-from-scratch setting as in FHESGD . For the transfer learning setting, we adopt the same feature extractors as , the pre-trained ViT Base  model and the pre-trained MPNet Base  model. The client first extracts samples into a 768-dimension feature and then encrypts the features. The server performs private training on the encrypted features. For the training-from-scratch setting, the client encrypts the raw dataset directly, and the server performs private training on the encrypted dataset. We use a \(3\)-layer MLP, with two hidden layers of dimension \(128\), which is a widely used structure in the private training setting [4; 21]. We perform encrypted data pruning on four widely used image datasets: MNIST , CIFAR-10 , Face Mask Detection , DermaMNIST  for image classification and one audio dataset SNIPS  for sentiment analysis. We partition the datasets as training, validation, and test set. The size of the training set and validation set is \(7:1\). We defer the number of samples in each set in the Appendix C.

**System Setup and Implementation.** We use the RNS version of the CKKS [36; 37] scheme for homomorphic encryption. We use the bootstrapping method for CKKS in . Our implementation is based on the OpenFHE  library. For HE parameters, we set the cyclotomic ring dimension as \(N=2^{16}\) and ciphertext modulus \(1555\) bits to guarantee a security level of 128-bit under the Homomorphic Encryption Standard . Each ciphertext has \(N/2=32768\) slots and we set the multiplicative depth as \(L=12\). All experiments were conducted using an AMD Ryzen Threadripper PRO 3955WX processor operating at 2.2GHz, equipped with 125GB of memory. We use the Single-Instruction-MultipleData (SIMD) technique  to fully utilize the ciphertext slots and amortize the cost of homomorphic operations. Under SIMD, multiple data samples can be coded into one ciphertext. We adopt the most efficient encoding methods proposed in [6; 42]. For nonlinear operations like SoftMax and ReLU, we adopt approximation-based methods. We report the CPU version of HETAL, as their GPU implementation is not publicly available.

### Results

**End-to-end performance.** As shown in Table 1, we compare the end-to-end training time and accuracy on five datasets with HETAL  and an unencrypted baseline. HETAL is known to be the most efficient full-data private training framework. For fair comparison, we unify the batch size as \(128\). For HETAL, we maintain the security parameters used in their original paper. For the proposed encrypted data pruning method, we set the pruning ratio as \(p=0.9\), i.e., only \(10\%\) of the data remains for training in each epoch. The total training time is reduced by \( 6.6\) across datasets. The accuracy drop is as small as \(0.25\%\) on the Face Mask Detection dataset. With encrypted data pruning, the accuracy is even \(0.14\%\) higher than the unencrypted baseline and HETAL.

   & **MNIST** & **CIFAR-10** & **Face Mask Detection** & **DermaMNIST** & **SNIPS** \\  Unencrypted & Acc(\%) & 95.69\({}_{ 0.02}\) & 96.62\({}_{ 0.02}\) & 95.46\({}_{ 0.06}\) & 75.91\({}_{ 0.11}\) & 94.43\({}_{ 0.05}\) \\   & Acc(\%) & 96.27\({}_{ 0.02}\) & 96.57\({}_{ 0.04}\) & 95.46\({}_{ 0.05}\) & 76.06\({}_{ 0.18}\) & 95\({}_{ 0.08}\) \\  & Runtime(h) & 276.75 & 293.3 & 32.88 & 101.55 & 113.7 \\   & Acc(\%) & 95.54\({}_{ 0.05}\) & 96.31\({}_{ 0.06}\) & 95.21\({}_{ 0.06}\) & 75.86\({}_{ 0.15}\) & 95.14\({}_{ 0.08}\) \\  & Runtime(h) & 41.89 & 44.76 & 5.02 & 15.5 & 17.36 \\  

Table 1: End-to-end comparison across different datasets The pruning ratio is set as \(p=0.9\).

**Effectiveness of the proposed methods.** In Table2, we demonstrate the effectiveness of each proposed technique. We start by training on the full CIFAR-10 dataset using the framework of HETAL. For the encrypted data pruning methods, we fix the pruning ratio as \(p=0.9\). In the prune baseline, we naively apply plaintext data pruning method to HETAL, which prolongs the total training time. Sorting \(43750\) importance score in CIFAR-10 can take more than \(200\) hours, which offsets the benefits of data pruning. By incorporating the client-aided masking, the server can remove the unimportant samples more effectively. The client-aided methods speed up the private training by \(1.49\) with a \(0.41\%\) accuracy drop. The communication cost increases slightly by approximately \(1.2\). Yet, as the EL2N score involves costly square root computation, the speed-up is modest. By incorporating HEFS, we improve the overhead during importance score computation. With HEFS, the private training can be accelerated by \(2.78\). The HEFS also achieves a \( 0.2\%\) higher accuracy. When ciphertext-wise pruning is applied, the runtime speed-up is the most significant, achieving a \(6.5\) speed-up over HETAL with only \(0.2\%\) accuracy drop.

**Ablation on the pruning ratio.** In Figure 4, we demonstrate the effectiveness of the proposed encrypted data pruning method under different data pruning ratios. In Figure 4 (a), we demonstrate the training time and accuracy achieved using different fractions of the CIFAR-10 dataset. Surprisingly, we find that even using only \(1\%\) of the data, the accuracy drop is only around \(0.6\%\), and the training can be sped up by \( 16\). Training with the \(40\% 70\%\) of the data even leads to higher accuracy than training with the full dataset. The same phenomenon is observed in plaintext data pruning . This is because some noisy or low-quality samples are excluded from training while retaining enough informative samples are maintained. We note that we can achieve \( 2.2\) speed up when training with \(40\%\) of the data without any loss of accuracy on the CIFAR-10 dataset. In Figure 4 (b), we experiment with the MNIST dataset and observe a similar trend. On the MNIST dataset, using only \(1\%\) of the data achieves \(91.29\%\) accuracy with \( 15\) speed-up. Yet, using \(5\%\) of the data achieves a significantly higher accuracy of \(95.2\%\), which is only \(0.5\%\) lower than training with the full dataset.

**Training from scratch.** We show the performance of encrypted data pruning in the training from scratch setting in Table 3. We train a \(3\)-layer MLP on the MNIST dataset. We set the pruning

 
**Method** & **Accuracy(\%)** & **Runtime(h)** & **Speedup** & **Communication(MB)** \\  Full Data(HETAL) & 96.57\({}_{ 0.04}\) & 293.3 & \( 1\) & 18.1 \\ Prune Baseline & 95.98\({}_{ 0.12}\) & 488.91 & \( 0.6\) & 18.1 \\ +Client Aided & 96.16\({}_{ 0.07}\) & 196.91 & \( 1.49\) & 22 \\ +HEFS & 96.31\({}_{ 0.06}\) & 105.57 & \( 2.78\) & 22 \\ +Ciphertext-wise Pruning & 96.31\({}_{ 0.06}\) & 44.76 & \( 6.55\) & 22 \\  

Table 2: Effectiveness of the proposed method. We tested different private training methods on the CIFAR-10 dataset, with a pruning ratio \(p=0.9\). Communication is the size of logits and importance score ciphertexts server sends to client for early stopping and data pruning.

Figure 4: Private training time and accuracy with different fractions of data of (a) CIFAR-10 dataset and (b) MNIST dataset

   & \(1\%\) & \(5\%\) & \(10\%\) & \(20\%\) & \(40\%\) & \(50\%\) & \(60\%\) & \(70\%\) & \(80\%\) & \(90\%\) \\   & Acc(\%) & 93.23 & 97.12 & 97.39 & 98.38 & 98.52 & 98.55 & 98.5 & 98.48 & 98.45 & 98.45 \\  & \( Acc.\) & -5.26 & -1.37 & -1.1 & -0.11 & +0.03 & +0.06 & +0.01 & -0.01 & -0.04 & -0.04 \\   & Time(h) & 32.25 & 110.61 & 208.56 & 404.46 & 796.26 & 992.16 & 1188.06 & 1383.94 & 1579.88 & 1775.72 \\  & speed up & \(60.8\) & \(17.2\) & \(9.4\) & \(4.8\) & \(2.5\) & \(1.9\) & \(1.7\) & \(1.4\) & \(1.2\) & \(1.1\) \\  

Table 3: Privately Training an MLP from scratch under different data pruning ratios.

frequency as \(10\) epoch. When training with only \(1\%\) data, the end-to-end training time could be \(60.8\) faster with an accuracy drop of \(5.26\%\). Increasing the training data fraction can effectively improve the test accuracy. When using \(40\%\) and \(50\%\) fraction of data, the training accuracy is \(0.03\% 0.06\%\) higher than training with the full dataset. We remark that warm-start strategy can also improve the test accuracy.

## 5 Discussion

**Broader Impact.** In this paper, we propose a framework that enables encrypted data pruning during confidential training. The proposed techniques can accelerate the private training without sacrificing the model accuracy. By incorporating data homomorphic friendly data pruning techniques, our framework makes the HE-enabled private training more practical while ensuring the data privacy.

**Limitations.** (1) More Model Support. Currently, the proposed methods have only been applied to simple models like MLP. Extending the encrypted data pruning to private training on larger models like CNNs and Transformers can enhance its utility. (2) More Dataset Support. In this paper, we have experimented on relatively small datasets. Extending to larger datasets will enhance its utility.

## 6 Conclusion

The paper presents a novel framework for encrypted data pruning aimed at enhancing private training of deep neural networks. Plaintext data pruning methods offer limited benefits in encrypted settings due to their lack of optimization for cryptographic processes. Our approach introduces crypto-oriented optimizations, including HE-friendly score, client-aided masking, and ciphertext-wise pruning, which effectively harness the potential of data pruning, achieving up to a 16-fold acceleration in training times without compromising accuracy.