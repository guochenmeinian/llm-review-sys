# MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models

Kailai Yang1  Zhiwei Liu1  Qianqian Xie2  Jimin Huang2

Tianlin Zhang1  Sophia Ananiadou1

1 The University of Manchester 2 The Fin AI

{kailai.yang,zhiwei.liu,sophia.ananiadou}@manchester.ac.uk

{xqq.sincere,zhangtianlin668}@gmail.com;jimin@chancefocus.com

###### Abstract

Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose _Meta-Objective Aligner_ (_MetaAligner_), the first policy-agnostic and generalizable method for multi-objective preference alignment. _MetaAligner_ models multi-objective alignment into three stages: (1) **dynamic objectives reformulation** algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) **conditional weak-to-strong correction** paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) **generalizable inference** method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives. Experimental results show that _MetaAligner_ achieves significant and balanced improvements in multi-objective alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU training hours compared to previous alignment methods. The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment. This project is open-sourced here.

## 1 Introduction

The recent advancements in large language models (LLMs) have focused on generating high-quality responses that align with human expectations and values. At the final stage of alignment, LLMs are supervised on human preference data via reinforcement learning from human feedback (RLHF) , where a proxy, directly trained on human preferences data, is leveraged to provide scalar rewards for reinforcement learning (RL) on the target model .

However, human expectations and values include a broad spectrum of heterogeneous and multi-dimensional objectives, which makes scalar supervisions inefficient for aligning diverse and inclusive human preferences . These drawbacks motivate further exploration into multi-objective alignment algorithms. Some intuitive methods extend RLHF into multi-objective RLHF (MORLHF) . Due to its substantial computational cost  and the unstable natureof the proximal policy optimization (PPO) [25; 15; 23] algorithm, other methods seek to bypass the RL paradigm with multi-objective direct preference optimization (MODPO) [39; 10] or supervised fine-tuning (SFT)-based methods [35; 10], which customized prompting strategies to incorporate multiple reward values into queries explicitly.

The above methods for multi-objective alignment bear one commonality: the dependence on the policy model's parameters. This paradigm inevitably brings two key limitations: (1) they require repetition of their high-cost alignment algorithms for each newly-introduced policy model, which is incompatible with the increasing sizes and fast iteration of current foundation models [1; 30; 6; 29]; (2) all target models are statically aligned on pre-determined (e.g. "Helpful", "Harmless", "Honest" [39; 10]) objectives, with currently no efforts in expanding and evaluating their capabilities on unseen objectives. This ignorance leads to poor generalizability of existing multi-objective alignment methods.

In this work, we propose _Meta-Objective Aligner_ (_MetaAligner_), the first policy-agnostic and generalizable method for multi-objective preference alignment. _MetaAligner_ models multi-objective alignment into three stages: (1) the **dynamic objectives reformulation** algorithm reorganizes traditional alignment datasets into dynamic-objective alignment datasets, training _MetaAligner_ to perform flexible alignment across different objectives. It achieves this by incorporating and combining text descriptions of various alignment objectives in a prompt-based manner; (2) the **conditional weak-to-strong correction** paradigm aligns the weak outputs of policy models to approach strong outputs with higher preferences in the corresponding alignment objectives. During training, _MetaAligner_ is stacked onto policy models to perform objective-aware corrections, where parameters of the policy model are fixed and _MetaAligner_ is optimized with an SFT-based three-step training process: warming up, equal-preference alignment, and contrastive-preference alignment. This paradigm enables _MetaAligner_ to perform plug-and-play inferences on any policy models even without access to their parameters, which significantly reduces training costs and facilitates alignment on close-source LLMs; (3) the **generalizable inference** method flexibly adjusts target objectives by updating their text descriptions in the prompts. This method can also adapt _MetaAligner_ to unseen objectives and achieve new alignment strategies via in-context learning , a new feature with rare previous exploration in alignment of language models. The number of aligned objectives also becomes expandable, theoretically leading to unlimited simultaneous alignment objectives. Table 1 compares key features between _MetaAligner_ and previous methods. As shown, conditional weak-to-strong correction of _MetaAligner_ extends _Aligner_ to multi-objective alignment scenarios, which are not directly solvable by _Aligner_ itself. _MetaAligner_ is also the first multi-objective alignment method to achieve policy-agnostic alignment and generalization to unseen objectives, two key advantages over previous methods such as MORLHF, MODPO, and SFT-based methods.

In summary, our main contributions are: (1) we propose _MetaAligner_, the first policy-agnostic method for multi-objective preference alignment. It performs multi-objective alignment efficiently, without tuning the policy models or accessing their parameters. Experimental results show that _MetaAligner_ outperforms previous alignment methods and saves up to 93.63% of GPU training hours; (2) we utilize _MetaAligner_ to exert zero-shot preference alignment for unseen objectives. To our knowledge, this work marks the first attempt at generalizable multi-objective preference alignment. Experimental results show that _MetaAligner_ can simultaneously perform effective alignment for six unseen objectives while maintaining performance on aligned objectives; (3) We examine _MetaAligner_ on three preference alignment datasets. Experimental results show that _MetaAligner_ improves

  
**Algorithm** & **Paradigm** & **Multi-Objective Alignment** & **Policy-Agnostic Alignment** & **Generalizability** \\  RLHF  & PPO & ✗ & ✗ & ✗ \\ MORLHF  & PPO & ✗ & ✗ & ✗ \\ MODPO [10; 39] & SFT, DPO & ✗ & ✗ & ✗ \\ RiC  & SFT & ✗ & ✗ & ✗ \\ _Aligner_ & SFT & ✗ & ✗ & ✗ \\  _MetaAligner_ & SFT & ✗ & ✗ & ✗ \\   

Table 1: Comparisons between previous alignment methods and _MetaAligner_ on different features. “Policy-Agnostic Alignment” means the alignment algorithm is independent of the target policy model parameters, and “Generalizability” denotes zero-shot alignment capability on unseen objectives.

win rates on multiple objectives across 10 policy models, substantially enhancing responses of state-of-the-art foundation models such as GPT-3.5-Turbo  and Claude-3 .

## 2 Multi-Objective Alignment of Language Models

In real-world scenarios, human expectations of high-quality responses from AI agents involve considerable variability, with complex interplays such as contradiction (e.g. "Helpful" and "Harmless") [10; 35] and dependence (e.g. "Correct" and "Informative") . Multi-objective preference alignment tackles this challenge by aiming to optimize multiple objectives simultaneously. For each query-response pair, the reward vector is formalized as: \((q,y)=[r_{1}(q,y),...,r_{N}(q,y)]^{}\), where \(q,y\) denote a query and a corresponding response, \(r_{i}\) denotes the reward values for \(i\)-th objective, which is defined, in most cases of preference alignment, under the Bradley-Terry  model of preferences. Specifically, for the same prompt \(q\) and two responses \((y_{1},y_{2})\) under data distribution \(\), the model assumes:

\[P_{}(y_{1} y_{2}|q,i)=(r_{i}(q,y_{1})-r_{i}(q,y_{2}))\] (1)

where \(\) denotes the logistic function and \(P_{}(y_{1} y_{2})\) denotes the probability that \(y_{1}\) is preferred against \(y_{2}\). MORLHF aims to achieve Pareto optimal among objectives, where the policy model is optimized to maximize a linear scalarization of multiple rewards [26; 19] with a KL-divergence regularization:

\[}{argmax}\,_{q,y_{}( y|q)}[^{}(q,y)]-_{KL} [_{}(y|q)\|_{ref}(y|q)]\] (2)

where \(_{}\) denotes the aligned policy model parameterized by \(\), \(_{ref}\) denotes the reference policy model, \(=[_{1},...,_{N}]\)\(s.t._{i=1}^{N}_{i}=1,_{i} 0\) is the pre-determined heuristic target preference vector. Another paradigm directly built alignment between multiple reward values and their corresponding response by minimizing an SFT loss for the policy model:

\[}{argmin}-_{(q,y)}[log\, _{}(y|q,(q,y))]\] (3)

where objectives and their corresponding reward values are described with text markers and combined into queries with a static prompting template. Compared to MORLHF, SFT-based multi-objective alignment is proven more cost-efficient and training-stable [35; 10].

## 3 Meta-Objective Aligner

Existing methods for multi-objective alignment generally face challenges in increasing training costs with new policy models and generalization to unseen objectives. To tackle these challenges, we introduce _MetaAligner_, which follows a three-stage paradigm: (1) dynamic objectives reformulation for building dynamic multi-objective datasets; (2) conditional weak-to-strong correction for model training; (3) generalizable inference for multi-objective alignment. The paradigm is illustrated in Figure 1.

### Dynamic Objectives Reformulation

We propose a dynamic objectives reformulation algorithm to construct a dynamic multi-objective dataset, which triggers _MetaAligner_'s ability for flexible adjustment of alignment objectives. Specifically, any typical multi-objective preference alignment dataset \(_{m}\) with \(m\) samples and \(N\) objectives

Figure 1: Illustrations of _Meta-Objective Aligner_, which follows a three-stage paradigm.

[MISSING_PAGE_FAIL:4]

where \(_{}\) denotes the _MetaAligner_ module parameterized by \(\), \(t\) depends on the training dataset. Conditional weak-to-strong correction directly trains _MetaAligner_ to align the weak policy model output \(y_{0}\) to the strong target output \(y_{0}\), which has higher preference values in corresponding objectives \(\). We have the standard cross-entropy loss as the training objective:

\[\,(,;) =-_{(q,y,)}[log\,^{*} (y|q)]\] (5) \[=-_{(q,y,)}[log\,_ {}(y|(q,y_{0},,t))]-_{q }[log\,_{}(y_{0}|q)]\]

We fix the parameters of the policy model, thus excluding \(\) from the weight update process. In practice, we use the dynamic multi-objective dataset for supervision, where the weak response in each query-response pair is directly leveraged as samples \(y_{0}\) from unknown policy models. Therefore, we eliminate the second term in Eqn. 5 and simplify the training objective as:

\[\,-_{(q,y_{0},y,)}[log\,_{}(y|(q,y_{0},,t))]\] (6)

The above action poses two advantages: (1) the computation resources required for _MetaAligner_ training is detached from policy model size, which enables policy-agnostic and cost-efficient alignment for large policy models; (2) _MetaAligner_ works only via outputs from the policy models, which allows training and inference for alignment on close-source policy models [1; 21; 2].

#### 3.2.2 Three-Step Model Training

In practice, we utilize an LLM as the base model for _MetaAligner_, which provides domain knowledge and strong reasoning ability to support the conditional weak-to-strong correction process. We propose a three-step paradigm based on the objective function in Eqn. 6: (1) **Warming up**. This stage trains the model in identical response pairs with a warm-up subset, a prelude proven effective in residual correction strategies [11; 12]. We randomly sample a subset of the equal subset \(_{e}\) as the warm-up subset, but set an identical target response for each instance; (2) **Equal-preference alignment**. Due to the contrastive nature of their learning paradigm, most previous preference alignment works focus on modeling the residuals between response pairs and ignore the equal-preference response pairs. However, equal preferences are common in many scenarios [34; 7] and enclose useful information such as the principle components of preference modeling regarding each objective. Based on these intuitions, we introduce a novel equal-preference alignment step to fine-tune the warmed-up _MetaAligner_ on the equal subset \(_{e}\); (3) **Contrastive-preference alignment**. This stage fine-tunes the _MetaAligner_ on the contrastive preference subset \(_{c}\), which instructs the model to perform conditional weak-to-strong correction on the specified objectives.

### Generalizable Inference

During inference, _MetaAligner_ achieves alignment following the sampling process as in Eqn. 4, where unaligned outputs, sampled from the target policy model, are used as the input for conditional weak-to-strong correction. With the prompting-based paradigm, the target objectives for _MetaAligner_ also become expandable and generalizable, a key advantage over previous alignment methods [39; 35; 10]. The generalizability is two-fold: Firstly, users can manipulate the target objectives by adjusting combinations of text descriptions in the objective set \(\). For example, in alignment with objectives 1, 3, and 4, we can flexibly shuffle the corresponding descriptions \( d_{1}\), \( d_{3}\), and \( d_{4}\) as follows: \(= d_{3}; d_{1}; d_{4}\). Secondly, the prompt-based objectives statement enables flexible adjustment of text descriptions for existing objectives and injections of unseen objectives. Following the last example, we have two unseen alignment objectives 5: \( d_{5}^{*}\) and 6: \( d_{6}^{*}\), and an updated text description \( d_{3}\) for aligned objective 3. We can perform zero-shot alignment on the new objectives by adjusting \(\) as follows: \(^{*}= d_{3}; d_{1}; d_{4};  d_{5}^{*}; d_{6}^{*}\). This simple pattern can theoretically lead to unlimited simultaneous alignment objectives. We expect _MetaAligner_ to make generalizable weak-to-strong corrections under these unseen conditions via its in-context learning ability. This advancement marks a new exploration into generalizable multi-objective preference alignment.

## 4 Experiments

### Experimental Settings

Datasets.We transfer the following three alignment datasets into dynamic multi-objective datasets: (1) **HH-RLHF**: a large-scale dataset with 160K prompts and corresponding response pairs.

We follow Yang et al.  and use open-sourced reward models on three objectives: "Harmless", "Helpful", and "Humor" to score the responses; (2) **UltraFeedback**: a multi-aspect alignment dataset with 64K prompts with preferences obtained from GPT-4, including "Instruction following", "Honeset", "Truthful", and "Helpful" objectives; (3) **IMHI**: we create an alignment dataset on the IMHI dataset  targeting interpretable mental health analysis. We invite domain experts to label 7.2K response pairs considering 3 objectives: "Correct", "Informative", and "Professional". Figure 2 shows the objective distributions on two datasets. The objectives display balanced overall distributions across objective set sizes, training _MetaAligner_ to adjust targets dynamically. Most objectives also cover considerable proportions in each column category, alleviating label imbalance problems.

Models.We train _MetaAligner_-(1.1B, 7B, 13B) models based on TinyLLaMA-1.1B  and LLaMA2-(7B, 13B)  foundation models. We utilize _MetaAligner_ to perform multi-objective alignment on the following open-source policy models: LLaMA2-Chat-(7B,13B,70B) , Gemma-instruct-(2B,7B) , and Vicuna-(7B, 13B, 33B) . We also align two advanced close-source foundation models: GPT-3.5-Turbo  and Claude-3-Sonnet , where model parameters are inaccessible.

Evaluation Metric.On each objective, we quantify the alignment performance of model outputs by comparing their _win rates_ against the _ground-truth response_ provided by the benchmark datasets. Considering the large amounts of cluster evaluation tool in previous works [10; 28; 18], we provide a prompt engineering. GPT-4 is required to compare and select the response with higher alignment on the specified objective or indicate a tied performance of the two responses.

More details about the training process, model cards, dataset statistics, IMHI dataset annotation, and evaluation settings are presented in Appendix D.

### Overall Performance

_MetaAligner_-(1.1B, 7B, 13B) performance on 3 alignment datasets are shown in Table 2. According to the results, the _MetaAligner_ models achieve substantial improvement for most objectives and policy models. For example, on UltraFeedback, there is an average of 11.47% advantage for _MetaAligner_-1.1B on "Honest", 34.39% for _MetaAligner_-7B, and 43.79% for _MetaAligner_-13B. These results show the general effectiveness of _MetaAligner_ on various upstream models and the feasibility of plug-and-play multi-objective alignment. On the mental health analysis benchmark IMHI, _MetaAligner_ models also show remarkable win rates on all objectives, proving their effectiveness in performing multi-objective alignment in domain knowledge-intense scenarios. We further evaluate _MetaAligner_ on each IMHI sub-task and the results are shown in Appendix E.

From the policy model scale perspective, _MetaAligner_ provides successful alignments to open-source models with sizes ranging from 2B to 70B, significantly extending the size of _MetaAligner_ itself. In the extreme case, _MetaAligner_-1.1B advances the win rates of LLaMA2-Chat-70B outputs, a policy model with 63\(\) more parameters, by an average of 12.19% on HH-RLHF, 13.08% on UltraFeedback, and 13.26% on IMHI. These results prove _MetaAligner_ as a parameter-efficient alignment strategy compared to previous multi-objective alignment methods, where the policy model weights are updated, leading to an inevitable surge of computation resources as policy model sizes grow. _MetaAligner_ also significantly improves performance on close-source LLMs: GPT-3.5-Turbo and Claude-3-Sonnet. These results prove its potential for application in close-source scenarios and effective multi-objective alignment of state-of-the-art policy models.

Figure 2: Heatmaps of the objective distributions. The columns categorize samples according to the sizes of their objective set. For the lines, ”Overall” shows their distributions in the training data. Other lines show objective-wise distributions across different categories in the columns.

Within most policy model families, we observe a decreasing trend in win-rate advantage as their sizes increase. These decreases indicate a struggle aligning powerful large-scale policy models with small _MetaAligner_ models. Fortunately, _MetaAligner_'s capabilities also show scalability. Increasing the size of its base model leads to a higher win-rate advantage on most policy models. For example, on UltraFeedback, _MetaAligner_-7B outperforms _MetaAligner_-1.1B on all 10 policy models, and _MetaAligner_-13B further surpasses _MetaAligner_-7B by an average of 12.58%. These observations motivate further explorations in model size-performance balance for _MetaAligner_.

### _MetaAligner vs_. Baseline Methods

We compare the performance of _MetaAligner_ with MORLHF, MODPO, SFT-based methods, and Aligner. We implement the linear scalarization method for MORLHF, the CDPO  realization of MODPO, and RiC  realization of the SFT-based method. As Aligner is not suitable for multi-objective alignment, we train Aligner-7B on "Helpful" annotations for HH-RLHF and "IF" for UltraFeedback. We compare these methods on the LLaMA2-Chat-7B policy model. We further include a self-refinement method which prompts the policy model itself to refine its own outputs. We compare self-refinement on the LLaMA2-Chat-70B policy model as it requires strong in-context

    &  &  &  &  \\   & & **Helpful** & **Homvers** & **Ilr** & **Hepful** & **Homover** & **Ilr** & **Hepful** & **Error** & **Ilr** & **Inference** & **Inference** & **Inference** & **Inference** & **Inference** & **Inference** \\   &  &  & **LHAA2-Chat-7B** & +10.01\% & **+20.06\%** & +14.78\% & +11.01\% & +15.09\% & +14.33\% & +9.09\% & +18.33\% & +20.55\% & +31.67\% \\  & & LLAMA2-Chat-7B & +10.75\% & +9.09\% & +13.25\% & +8.66\% & +15.34\% & +16.33\% & +6.75\% & +11.11\% & +8.33\% & +25.09\% & +32.65\% \\  & & LLAMA2-Chat-7B & +6.58\% & +7.42\% & +22.58\% & +6.09\% & +12.67\% & +7.33\% & +16.33\% & +8.33\% & +14.23\% & +17.29\% \\  & & Commine-Miner-7B & +8.58\% & +12.25\% & +13.48\% & +4.16\% & +13.80\% & +4.53\% & +15.55\% & +48.55\% & +83.23\% \\  & & Commine-Miner-7B & +4.09\% & +7.75\% & +23.51\% & +7.09\% & +10.09\% & +14.67\% & +14.06\% & +15.93\% & +34.12\% & +36.11\% \\  & & Vicon-7B & +**11.5** & +10.03\% & +0.33\% & +0.31\% & +13.33\% & +12.33\% & +14.90\% & +17.09\% & +7.22\% & +6.35\% \\  & & Vicon-13B & +7.42\% & +13.09\% & +11.78\% & +11.66\% & +14.43\% & +13.33\% & +10.01\% & +12.72\% & +7.78\% & +3.34\% \\  & & Vicon-33B & +8.59\% & +2.95\% & +23.83\% & +8.09\% & +11.67\% & +6.33\% & +6.67\% & +8.34\% & +4.48\% & +6.12\% \\  & & GFT-5-Tenthe & +1.22\% & +7.95\% & +17.84\% & +5.09\% & +5.09\% & +3.66\% & +1.09\% & +6.78\% & +1.33\% & +9.33\% \\  & & Code-5-50ment & +3.3\% & +15.81\% & +13.73\% & +6.67\% & +2.67\% & +3.09\% & +7.91\% & +2.33\% & +6.66\% \\   &  &  &  &  &  &  &  &  &  &  &  \\  & & LLAMA2-Chat-7B & +25.05\% & +27.9\% & +20.75\% & +36.46\% & +3.07\% & +3.78\% & +23.09\% & +21.67\% & +32.22\% & +43.89\% \\  & & LLAMA2-Chat-7B & +12.87\% & +20.56\% & +18.23\% & +3.04\% & +3.74\% & +23.33\% & +7.66\% & +23.55\% & +5.09\% & +33.89\% \\  & & LLAMA2-Chat-7B & +16.58\% & +14.22\% & +20.08\% & +10.7\% & +27.09\% & +31.33\% & +17.09\% & +20.56\% & +17.23\% & +21.67\% \\  & & Commine-Miner-7B & +20.01\% & +18.75\% & +17.83\% & +13.43\% & +6.07\% & +24.33\% & +13.34\% & +25.05\% & +50.35\% & +51.67\% \\  & & Commine-Miner-7B & +11.01\% & +22.53\% & +26.67\% & +15.73\% & +5.34\% & +3.101\% & +9.09\% & +5.04\% & +6.12\% & +**6.41\%** \\  & & Vicon-7B & +19.5\% & +18.53\% & +27.33\% & +38.90\% & +39.00\% & +37.02\% & +32.34\% & +23.33\% & +22.27\% & +23.33\% \\  & & Vicon-7B & +14.09\% & +20.10\% & +0.56\% & +36.46\% & +40.08\% & +9.66\% & +34.48\% & +25.55\% & +6.09\% & +6.51\% \\  & & Vicon-7B & +28.0\% & +17.09\% & +**30.83\%** & +0.06\% & +7.37\% & +32.39\% & +29.33\% & +11.11\% & +6.11\% & +8.39\% \\  & & GFT-5-Tenthe & +18.99\% & +21.51\% & +22.84\% & +29.99\% & +33.34\% & +21.00\% & +14.34\% & +18.67\% & +6.33\% & +41.22\% \\  & & Cmode-5-50ment & +31.71\% & +20.68\% & +21.71\% & +23.33\% & +21.03\% & +19.21\% & +19.13\% & +19.33\% & +11.33\% \\   &  & \) one-by-one, with 10 aligned objectives in total. Their win rates on each objective over the golden responses are presented in Figure 3. We have the following conclusions:

_MetaAligner performs effective zero-shot alignment for unseen objectives._ With most _MetaAligner_ models, incorporating an unseen objective into the objective set significantly improves its corresponding win rate. For example, _MetaAligner_-7B improves by 25.17% on "Specific", 14.5% on "Factual", and 17.5% on "Readable" compared to each of these objectives unaligned. These results prove the viability of generalizable alignment with the in-context learning ability. However, the win rates

Figure 3: Zero-shot alignment on 6 unseen objectives. In the x-axis, “Aligned Obj.” denotes the 4 supervised objectives (”\(\)” markers), and ”+” denotes _further_ addition of an unseen objective (”\(\)” markers). ”\(\)” denotes the win rates for the unseen objectives before all zero-shot alignments, ”-” lines identify win rate fluctuations before alignment, and solid lines identify fluctuations after alignment.

on supervised objectives ("Instruction following", "Helpful", "Honest", and "Truthful") generally surpass unseen objectives, showing that supervised learning remains more effective in multi-objective preference alignment compared to in-context learning.

_Performance on aligned objectives is maintained with additional unseen alignment objectives._ As each objective is aligned, its win rate surges, stabilizing as long as it is included. On simultaneously aligning 10 objectives, _MetaAligner_-7B outperforms LLaMA2-Chat-70B outputs by an average of 14.25% on unseen objectives. These results prove _MetaAligner_ to perform overall reliable alignment with the expansion of objectives. However, enhancements in one objective can affect performance in certain objectives due to their controversial nature, which is known as the "alignment tax" . For example, aligning on "Fair" (+Fair) with _MetaAligner_-(7B, 13B) benefits its win rates, but harms performance on objectives such as "Readable" and "Factual" compared to when "Fair" is unaligned.

_MetaAligner's generalizability shows scalability._ Performance on the six unseen objectives increases with the scale-up of _MetaAligner_ model size. _MetaAligner_-1.1B provides limited improvement on most unseen objectives, but _MetaAligner_-7B extends the win rates to an average of 48.5%, and _MetaAligner_-13B further reaches 61.25%. _MetaAligner_-13B also more effectively aligns objectives such as "Length", where smaller models perform badly. This scalability is attributed to larger foundation models' growing in-context learning ability, which enables accurate interpretations of the objective descriptions and instructions. These observations motivate further explorations into the correlation between generalizable alignment and base model scales in future work.

### Evaluations of Objective-Wise Alignment

We evaluate the objective-wise performance of _MetaAligner_ by decoupling the target objectives. We utilize _MetaAligner_ to perform six levels of alignments: unaligned, aligning on each objective ("Instruction following", "Helpful", "Honest", and "Truthful"), and full alignment. We leverage GPT-4 to score the responses ranging from 0 to 10. The results are shown in Figure 4. Experimental details and more results are shown in Appendix F. We have the following observations:

_Objective-wise alignment improves performance on the primary target and boosts the performance on other objectives._ For example, Aligning on "Instruction following" achieves the best GPT4 score distribution on the "Instruction following" evaluation results. It also significantly increases GPT4 scores on "Helpful", "Honest", and "Truthful" over the unaligned responses. This tendency holds with other policy models and alignment objectives. These results further prove the complex interplay among objectives, where correlations and contradictions  co-exist.

_Full alignment on all objectives provides balanced performance._ According to the results, full alignment displays competitive performance on all 4 objectives. Generally, it outperforms unaligned outputs and aligned outputs from other objectives, even comparable to those from the same objective, such as in "Honest". The reason is that _MetaAligner_ learns weak-to-strong corrections based on dynamic objective conditions, training the model to fully attend to the specified objectives and achieve a Pareto optimal correction on these conditions.

## 5 Related Work

This paper focuses on advancing multi-objective alignment of language models with human values, which is mainly related to two research areas: (1) **Large Language Models**, including the latest development in close-source AI agents [1; 21; 2] and open-source foundation models [30; 29; 6]. (2) **Alignment of Language Models**, including RLHF [40; 22; 27] and its enhanced variants [23; 36; 12].

Figure 4: Objective-wise kernel density estimates of GPT-4 evaluation scores under different alignment objectives. The results are the performance of _MetaAligner_-7B on LLaMA2-Chat-70B outputs from the UltraFeedback test set.

Multi-objective alignment methods include MORLHF [26; 19; 24], MODPO [39; 10], and SFT-based methods [35; 10]. A detailed review of related work is in Appendix B.

## 6 Discussions

Conclusion.This paper proposed _MetaAligner_, the first policy-agnostic and generalizable method for multi-objective preference alignment. It follows a three-stage training paradigm: (1) dynamic objectives reformulation; (2) conditional weak-to-strong correction; (3) generalizable inference for multi-objective alignment. _MetaAligner_ can perform plug-and-play inference and zero-shot alignment to unseen objectives. Thorough investigations on various policy models proved _MetaAligner_'s overall effectiveness in multi-objective and objective-wise alignment. Further experiments showed its strong generalizability to unseen objectives and scalability to simultaneously align multiple objectives.

Limitations and Future Work.Firstly, stacking _MetaAligner_ module on policy models inevitably leads to increased computational burdens during alignment inference , which affects model deployment, especially for scenarios such as local deployment on mobile devices. Secondly, due to limited resources, we only tested the generalizability of _MetaAligner_ on 6 unseen objectives, which does not provide a clear landscape of its alignment performance on more objectives. In future work, we aim to explore improving _MetaAligner_ in domain-specific alignment scenarios utilizing techniques such as retrieval-augment generation . We will also dive deep into the scalability of _MetaAligner_ to evaluate its impact on alignment performance, including the model scale-performance balance. We will also provide a clearer landscape of their generalizable alignment ability by examining larger base model sizes and aligning on much more unseen objectives (we only expanded to 10 objectives). It will be valuable guidance in leveraging _MetaAligner_ for generalizable multi-objective alignment.