# On Tractable \(\Phi\)-Equilibria in Non-Concave Games

# On Tractable \(\)-Equilibria in Non-Concave Games

 Yang Cai

Yale University

yang.cai@yale.edu

Constantinos Daskalakis

MIT CSAIL

costis@csail.mit.edu

Haipeng Luo

University of Southern California

haipengl@usc.edu

Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

Weiqiang Zheng

Yale University

weiqiang.zheng@yale.edu

###### Abstract

While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when utilities are non-concave - a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents' utilities are computed by neural networks, or both. Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable. To sidestep these challenges, we revisit the classical solution concept of \(\)-equilibria introduced by Greenwald and Jafari , which is guaranteed to exist for an arbitrary set of strategy modifications \(\) even in non-concave games . However, the tractability of \(\)-equilibria in such games remains elusive. In this paper, we initiate the study of tractable \(\)-equilibria in non-concave games and examine several natural families of strategy modifications. We show that when \(\) is finite, there exists an efficient uncoupled learning algorithm that approximates the corresponding \(\)-equilibria. Additionally, we explore cases where \(\) is infinite but consists of local modifications, showing that Online Gradient Descent can efficiently approximate \(\)-equilibria in non-trivial regimes.

## 1 Introduction

Von Neumann's celebrated minimax theorem establishes the existence of Nash equilibrium in all two-player zero-sum games where the players' utilities are continuous as well as _concave_ in their own strategy .1 This assumption that players' utilities are concave, or quasi-concave, in their own strategies has been a cornerstone for the development of equilibrium theory in Economics, Game Theory, and a host of other theoretical and applied fields that make use of equilibrium concepts. In particular, (quasi-)concavity is key for showing the existence of many types of equilibrium, from generalizations of min-max equilibrium  to competitive equilibrium in exchange economies , mixed Nash equilibrium in finite normal-form games , and, more generally, Nash equilibrium in (quasi-)concave games .

Not only are equilibria guaranteed to exist in concave games, but it is also well-established--thanks to a long line of work at the interface of game theory, learning and optimization whose origins can be traced to Dantzig's work on linear programming , Brown and Robinson's work on fictitious play , Blackwell's approachability theorem  and Hannan's consistency theory --that several solution concepts are efficiently computable both centrally and via decentralized learning dynamics. For instance, it is well-known that the learning dynamics produced when the players of a game iteratively update their strategies using no-regret learning algorithms, such as online gradient descent, is guaranteed to converge to Nash equilibrium in two-player zero-sum concave games, and to coarse correlated equilibrium in multi-player general-sum concave games . The existence of such simple decentralized dynamics further justifies using these solution concepts to predict the outcome of real-life multi-agent interactions where agents deploy strategies, obtain feedback, and use that feedback to update their strategies.

While (quasi-)concave utilities have been instrumental in the development of equilibrium theory, as described above, they are also too restrictive an assumption. Several modern applications and outstanding challenges in Machine Learning, from training Generative Adversarial Networks (GANs) to Multi-Agent Reinforcement Learning (MARL) as well as generic multi-agent Deep Learning settings where the agents' strategies are parameterized by deep neural networks or their utilities are computed by deep neural networks, or both, give rise to games where the agents' utilities are _non-concave_ in their own strategies. We call these games _non-concave_, following .

Unfortunately, classical equilibrium theory quickly hits a wall in non-concave games. First, Nash equilibria are no longer guaranteed to exist. Second, while mixed Nash, correlated and coarse correlated equilibria do exist--under convexity and compactness of the strategy sets , which we have been assuming all along in our discussion so far, they have infinite support, in general . Finally, they are computationally intractable; so, a fortiori, they are also intractable to attain via decentralized learning dynamics.

In view of the importance of non-concave games in emerging ML applications and the afore-described state-of-affairs, our investigation is motivated by the following broad and largely open question:

**Question from :**_Is there a theory of non-concave games? What solution concepts are meaningful, universal, and tractable?_

### Contributions

We study Daskalakis' question through the lens of the classical solution concept of \(\)-equilibria introduced by Greenwald and Jafari . This concept is guaranteed to exist for virtually any set of strategy modifications \(\), even in non-concave games, as demonstrated by Stoltz and Lugosi .2 However, the tractability of \(\)-equilibria in such games remains elusive. In this paper, we initiate the study of tractable \(\)-equilibria in non-concave games and examine several natural families of strategy modifications.

  Solution Concept & Incentive Guarantee & Existence & Complexity \\  Nash equilibrium & & & \\  Mixed Nash equilibrium & Global stability & ✓ & NP-hard \\  (Coarse) Correlated equilibrium & & ✓ & : A222 \\  Strict local Nash equilibrium & Local stability & ✗ & \\  Second-order local Nash equilibrium & Second-order stability & ✗ & \\  Local Nash equilibrium & First-order stability & ✓ & PEMD-hard : \\  \(\)**-equilibrium (finite \(||\))** &  Stability against \\ finite deviations \\  & ✓ &  Efficient \(\)-approximation \\ for any \(\) - (Theorem 2) \\  \\  \((())\)**-equilibrium (finite \(|()|\))** & & ✓ &  Efficient \(\)-approximation \\ for \(=(^{2})\) (Theorem 4) \\  \\  \(_{}()\)**-equilibrium** & First-order stability & ✓ &  Efficient \(\)-approximation \\ via GDo/G for \(=(^{2})\) (Theorem 3,9) \\  \\  \(_{}()\)**-equilibrium** & &  When \(=(^{2})\) \\  & ✓ & 
 Efficient \(\)-approximation \\ via no-regret learning for \(=(^{2})\) (Theorem 5) \\  \\  

Table 1: A comparison between different solution concepts in multi-player non-concave games. We include definitions of Nash equilibrium, mixed Nash equilibrium, (coarse) correlated equilibrium, strict local Nash equilibrium, and second-order local Nash equilibrium in B. We also give a detailed discussion on the existence and complexity of these solution concepts in B.

\(\)**-Equilibrium.** The concept of \(\)-equilibrium generalizes (coarse) correlated equilibrium. A \(\)-equilibrium is a joint distribution over \(_{i=1}^{n}_{i}\), the Cartesian product of all players' strategy sets, and is defined in terms of a set, \(^{_{i}}\), of _strategy modifications_, for each player \(i\). The set \(^{_{i}}\) contains functions mapping \(_{i}\) to itself. A joint distribution over strategy profiles qualifies as a \(=_{i=1}^{n}^{_{i}}\)-equilibrium if no player \(i\) can increase their expected utility by using any strategy modification function, \(_{i}^{_{i}}\), on the strategy sampled from the joint distribution. The larger the set \(\), the stronger the incentive guarantee offered by the \(\)-equilibrium. For example, if \(^{_{i}}\) contains all constant functions, the corresponding \(\)-equilibrium coincides with the notion of coarse correlated equilibrium. Throughout the paper, we also consider \(\)-approximate \(\)-equilibria, where no player can gain more than \(\) by deviating using any function from \(^{_{i}}\). We study several families of \(\) and illustrate their relationships in Figure 1.

**Finite Set of Global Deviations.** The first case we consider is when each player \(i\)'s set of strategy modifications, \(^{_{i}}\), contains a finite number of arbitrary functions mapping \(_{i}\) to itself. As shown in , if there exists an online learning algorithm where each player \(i\) is guaranteed to have sublinear \(^{_{i}}\)-regret, the empirical distribution of joint strategies played converges to a \(=_{i=1}^{n}^{_{i}}\)-equilibrium. Gordon, Greenwald, and Marks  consider \(\)-regret minimization but for concave reward functions, and their results, therefore, do not apply to non-concave games. Stoltz and Lugosi  provide an algorithm that achieves no \(^{_{i}}\)-regret in non-concave games; however, their algorithm requires a fixed-point computation per step, making it computationally inefficient.3 Our first contribution is to provide an efficient randomized algorithm that achieves no \(^{_{i}}\)-regret for each player \(i\) with high probability.

**Contribution 1:** Let \(\) be a strategy set (not necessarily compact or convex), and \(\) an arbitrary finite set of strategy modification functions for \(\). We design a randomized online learning algorithm that achieves \(O()\)\(\)-regret, with high probability, for _arbitrary_ bounded reward functions on \(\) (Theorem 2). The algorithm operates in time \(||\) per iteration. If every player in a _non-concave_ game adopts this algorithm, the empirical distribution of strategy profiles played forms an \(\)-approximate \(=_{i=1}^{n}^{_{i}}\)-equilibrium, with high probability, for any \(>0\), after \((,(_{i}|^{_ {i}}|), n)\) iterations.

If players have infinitely many global strategy modifications, we can extend Algorithm 1 by discretizing the set of strategy modifications under mild assumptions, such as the modifications being Lipschitz (Corollary 1). The empirical distribution of the strategy profiles still converges to the corresponding \(\)-equilibrium, but at a much slower rate of \(O(T^{-})\), where \(d\) is the dimension of the set of strategies. Additionally, the algorithm requires exponential time in the dimension per iteration, making it inefficient. This inefficiency is unavoidable, as the problem remains intractable even when \(\) contains only constant functions.

Figure 1: The relationship between different solution concepts in non-concave games. An arrow from one solution concept to another means the former is contained in the latter. The dashed arrow from \((())\)-equilibria to \(_{}\)-equilibria means the former is contained in the latter when \(()=_{}\).

To address the limitations associated with infinitely large global strategy modifications, a natural approach is to focus on local deviations instead. The corresponding \(\)-equilibrium will guarantee local stability. The study of local equilibrium concepts in non-concave games has received significant attention in recent years--see e.g., . However, these solution concepts either are not guaranteed to exist, are restricted to sequential two-player zero-sum games , only establish local convergence guarantees for learning dynamics--see e.g., , only establish asymptotic convergence guarantees--see e.g., , or involve non-standard solution concepts where local stability is not with respect to a distribution over strategy profiles .

We study the tractability of \(\)-equilibrium with infinitely large \(\) sets that consist solely of local strategy modifications. These local solution concepts are guaranteed to exist in general multi-player non-concave games. Specifically, we focus on the following three families of natural deviations.

* **Projection based Local Deviations:** Each player \(i\)'s set of strategy modifications, denoted by \(^{_{i}}_{}()\), contains all deviations that attempt a small step from their input in a fixed direction and project if necessary, namely are of the form \(_{v}(x)=_{_{i}}[x-v]\), where \(\|v\|\) and \(_{_{i}}\) stands for the \(_{2}\)-projection onto \(_{i}\).
* **Convex Combination of Finitely Many Local Deviations:** Each player \(i\)'s set of strategy modifications, denoted by \((^{_{i}}())\), contains all deviations that can be represented as a convex combination of a finite set of \(\)-local strategy modifications, i.e., \(\|(x)-x\|\) for all \(^{_{i}}()\).
* **Interpolation based Local Deviations:** each player \(i\)'s set of local strategy modifications, denoted by \(^{_{i}}_{}()\), that contains all deviations that _interpolate_ between the input strategy and another strategy in \(_{i}\). Formally, each element \(_{,x^{}}(x)\) of \(^{_{i}}_{}()\) can be represented as \((1-)x+ x^{}\) for some \(x^{}_{i}\) and \(/D_{_{i}}\) (\(D_{_{i}}\) is the diameter of \(_{i}\)).

For our three families of local strategy modifications, we explore the tractability of \(\)-equilibrium within a regime we term the _first-order stationary regime_, where \(=(^{2})\)4, with \(\) representing the maximum deviation allowed for a player. An \(\)-approximate \(\)-equilibrium in this regime ensures first-order stability. This regime is particularly interesting for two reasons: (i) Daskalakis, Skoulakis, and Zampetakis  have demonstrated that computing an \(\)-approximate \(\)-local Nash equilibrium in this regime is intractable.5 This poses an intriguing question: can correlating the players' strategies, as in a \(\)-equilibrium, potentially make the problem tractable? (ii) Extending our algorithm, initially designed for finite sets of strategy modifications, to these three sets of local deviations results in inefficiency; specifically, the running time becomes exponential in one of the problem's natural parameters. Designing efficient algorithms for this regime thus presents challenges. Despite these, we show the following:

* **Contribution 2:** For any \(>0\), for each of the three families of infinite \(\)-local strategy modifications mentioned above, there exists an efficient uncoupled learning algorithm that converges to an \(\)-approximate \(\)-equilibrium of the non-concave game in the first-order stationary regime, i.e., \(=(^{2})\).

We present our results for the projection-based local deviation in Theorem3 and Theorem9. Our result for the convex combination of local deviations can be found in Theorem4. Theorem5 contains our result for the interpolation-based local deviations. Similar to the finite case, our algorithms build on the connection between \(\)-regret minimization and \(\)-equilibrium. Given that our strategy modifications are non-standard, it is a priori unclear how to minimize the corresponding \(\)-regret. For instance, to our knowledge, no algorithm is known to minimize \(^{}_{}()\)-regret even when the reward functions are concave, and provably \(^{}_{}()\)-regret is incomparable to external regret (Examples3 and 4). However, via a novel analysis, we show that Online Gradient Descent (GD) and Optimistic Gradient (OG) achieve a near-optimal \(^{}_{}()\)-regret guarantee (Theorem3 and Theorem8). Our results provide efficient uncoupled algorithms to compute \(\)-approximate \(()\)-equilibria in the first-order stationary regime \(=(^{2})\).

Further related work is discussed in AppendixA.

## 2 Preliminaries

A ball of radius \(r>0\) centered at \(x^{d}\) is denoted by \(B_{d}(x,r):=\{x^{}^{d}:\|x-x^{}\| r\}\). We use \(\|\|\) for \(_{2}\) norm throughout. We also write \(B_{d}()\) for a ball centered at the origin with radius \(\). For \(a\), we use \([a]^{+}\) to denote \(\{0,a\}\). We denote \(D_{}\) as the diameter of a set \(\).

**Continuous / Smooth Games.** An \(n\)-player _continuous game_ has a set of \(n\) players \([n]:=\{1,2,,n\}\). Each player \(i[n]\) has a nonempty convex and compact strategy set \(_{i}^{d_{i}}\). For a joint strategy profile \(x=(x_{i},x_{-i})_{j=1}^{n}_{j}\), the reward of player \(i\) is determined by a utility function \(u_{i}:_{j=1}^{n}_{j}\). We denote by \(d=_{i=1}^{n}d_{i}\) the dimensionality of the game and assume \(_{i[n]}\{D_{_{i}}\} D\). A _smooth game_ is a continuous game whose utility functions further satisfy the following assumption.

**Assumption 1** (Smooth Games).: _The utility function \(u_{i}(x_{i},x_{-i})\) for any player \(i[n]\) is differentiable and satisfies_

1. _(G-Lipschitzness)_ \(\|_{x_{i}}u_{i}(x)\| G\) _for all_ \(i\) _and_ \(x_{j=1}^{n}_{j}\)_;_
2. _(L-smoothness) there exists_ \(L_{i}>0\) _such that_ \(\|_{x_{i}}u_{i}(x_{i},x_{-i})-_{x_{i}}u_{i}(x^{}_{i},x_{-i}) \| L_{i}\|x_{i}-x^{}_{i}\|\) _for all_ \(x_{i},x^{}_{i}_{i}\) _and_ \(x_{-i}_{j i}_{j}\)_. We denote_ \(L=_{i}L_{i}\) _as the smoothness of the game._

Crucially, we make no assumption on the concavity of \(u_{i}(x_{i},x_{-i})\).

\(\)**-equilibrium and \(\)-regret.** Below we formally introduce the concept of \(\)-equilibrium and its relationship with online learning and \(\)-regret minimization.

**Definition 1** (\(\)-equilibrium ).: _In a continuous game, a distribution \(\) over joint strategy profiles \(_{i=1}^{n}_{i}\) is an \(\)-approximate \(\)-equilibrium for some \( 0\) and a profile of strategy modification sets \(=_{i=1}^{n}_{i}\) if and only if for all player \(i[n]\), \(_{_{i}}_{x}[u_{i}((x_{i}),x_{-i})] _{x}[u_{i}(x)]+\). When \(=0\), we call \(\) a \(\)-equilibrium._

We consider the standard online learning setting: at each day \(t[T]\), the learner chooses an action \(x^{t}\) from a nonempty convex compact set \(^{m}\) and the adversary chooses a possibly non-convex loss function \(f^{t}:\), then the learner suffers a loss \(f^{t}(x^{t})\) and receives feedback. In this paper, we focus on two feedback models: (1) the player receives an oracle for \(f^{t}()\); (2) the player receives only the gradient \( f^{t}(x^{t})\). The classic goal of an online learning algorithm is to minimize the _external regret_ defined as \(^{T}:=_{x}_{t=1}^{T}(f^{t}(x^{t})-f^{t}(x))\). An algorithm is called _no-regret_ if its external regret is sublinear in \(T\). The notion of \(\)-regret generalizes external regret by allowing more general strategy modifications.

**Definition 2** (\(\)-regret).: _Let \(\) be a set of strategy modification functions \(\{:\}\). For \(T 1\), the \(\)-regret of an online learning algorithm is \(^{T}_{}:=_{}_{t=1}^{T}(f^{t}(x^{t})-f^{ t}((x^{t})))\). An algorithm is called no \(\)-regret if its \(\)-regret is sublinear in \(T\)._

Many classic notions of regret can be interpreted as \(\)-regret. For example, the external regret is \(_{}\)-regret where \(_{}\) contains all constant strategy modifications \(_{x^{*}}(x)=x^{*}\) for all \(x^{*}\). The _swap regret_ on simplex \(^{m}\) is \(_{}\)-regret where \(_{}\) contains all linear transformations \(:^{m}^{m}\). A fundamental result for learning in games is that no-\(\)-regret learning dynamics in games converge to an approximate \(\)-equilibrium .

**Theorem 1** ().: _If each player \(i\)'s \(_{i}\)-regret is upper bounded by \(^{T}_{_{i}}\), then their empirical distribution of strategy profiles played is an \((_{i[n]}^{T}_{_{i}}/T)\)-approximate \(\)-equilibrium._

## 3 Tractable \(\)-Equilibrium for Finite \(\) via Sampling

In this section, we revisit the problem of computing and learning an \(\)-equilibrium in non-concave games when each player's set of strategy modifications \(^{_{i}}\) is finite.

The pioneering work of Stoltz and Lugosi  gives a no-\(\)-regret algorithm for this case where each player chooses a distribution over strategies in each round. This result also implies convergence to \(\)-equilibrium. However, the algorithm by Stoltz and Lugosi  is not computationally efficient. In each iteration, their algorithm requires computing a distribution that is stationary under atransformation that can be represented as a mixture of the modifications in \(\). The existence of such a stationary distribution is guaranteed by the Schauder-Cauty fixed-point theorem , but the distribution might require exponential support and be intractable to find.

Our main result in this section is an efficient \(\)-regret minimization algorithm (Algorithm 1) that circumvents the step of the exact computation of a stationary distribution. Consequently, our algorithm also ensures efficient convergence to a \(\)-equilibrium when adopted by all players.

``` Input:\(x_{}\), \(h 2\), an external regret minimization algorithm \(_{}\) over \(\) Output: A \(\)-regret minimization algorithm for \(\)
1functionNextStrategy() \(p^{t}_{}\).NextStrategy(). Note that \(p_{t}\) is a distribution over \(\). return\(x^{t}(x_{},h,p^{t})\).
2functionObserveReward(\(u^{t}()\))
3 Set \(u^{t}_{}()=u^{t}((x^{t}))\) for all \(\). \(_{}\).ObserveReward(\(u^{t}_{}()\)). ```

**Algorithm 1**\(\)-regret minimization for non-concave reward via sampling

**Theorem 2**.: _Let \(\) be a strategy set (not necessarily compact or convex), \(\) be an arbitrary finite set of strategy modifications over \(\), and \(u^{1}(),,u^{T}()\) be an arbitrary sequence of possibly non-concave reward functions from \(\) to \(\). If we instantiate Algorithm 1 with \(_{}\) being the Hedge algorithm over \(\) and \(h=\), the algorithm guarantees that, with probability at least \(1-\), it produces a sequence of strategies \(x^{1},,x^{T}\) with \(\)-regret at most \(_{}_{t=1}^{T}u^{t}((x^{t}))-_{t=1}^{T}u^{t}(x^{t})  8\). Moreover, the algorithm runs in time \(O(||)\) per iteration._

_If all players in a non-concave continuous game employ Algorithm 1, then with probability at least \(1-\), for any \(>0\), the empirical distribution of strategy profiles played forms an \(\)-approximate \(=_{i=1}^{n}^{_{i}}\)-equilibrium, after \((,(_{i}|^{ _{i}}|),)\) iterations._

**High-level ideas.** We adopt the framework in . The framework contains two steps in each iteration \(t\): (1) the learner runs a no-external-regret algorithm over \(\) which outputs \(p^{t}()\) in each iteration \(t\); (2) the learner chooses a stationary distribution \(^{t}=_{}p^{t}(^{t})\), where we slightly abuse notation to use \((^{t})\) to denote the image measure of \(\) by \(\). However, how to compute the stationary distribution \(^{t}\) efficiently is unclear. We essentially provide a computationally efficient way to carry out step (2) without computing this stationary distribution.

* We first construct an \(\)-approximate stationary distribution by recursively applying strategy modifications from \(\). The constructed distribution can be viewed as a tree. Our construction is inspired by the recent work of Zhang, Anagnostides, Farina, and Sandholm  for concave games. The main difference here is that for non-concave games, the distribution needs to be approximately stationary with respect to a _mixture_ of strategy modifications rather than a single one as in concave games. Consequently, this leads to an approximate stationary distribution with prohibitively high support size \((||)^{}\), as opposed to \(\) in  for concave games.
* Despite the exponentially large support size of the distribution, we utilize its tree structure to design a simple and efficient sampling procedure that runs in time \(\). Equipped with such a sampling procedure, we provide an efficient randomized algorithm that generates a sequence of strategies so that, with high probability, the \(\)-regret for this sequence of strategies is at most \(O()\).

We defer the full proof of Theorem2 to Section3.1. An extension of Theorem2 to infinite \(\) holds when the rewards \(\{u^{t}\}_{t[T]}\) are \(G\)-Lipschitz and \(\) admits an \(\)-cover with size \(N()\). In particular, when \(\) is the set of all \(M\)-Lipschitz functions over \(^{d}\), \(\) admits an \(\)-cover with \( N()\) of the order \((1/)^{d}\). In this case, we have

**Corollary 1**.: _There is a randomized algorithm such that, with probability at least \(1-\), the \(\)-regret is bounded by \(c T^{}(1/)\), where \(c\) only depends on \(G\) and \(M\). The algorithm runs in time \((T,N(T^{-1/(d+2)}))\)._

### Proof of Theorem2

For a distribution \(()\) over strategy space \(\), we slightly abuse notation and define its expected utility as \(u^{t}():=_{x}[u^{t}(x)]\). We define \(()\) the image of \(\) under transformation \(\). In each iteration \(t\), the learner chooses their strategy \(x^{t}\) according to the distribution \(^{t}\). For a sequence of strategies \(\{x^{t}\}_{t[T]}\), the \(\)-regret is \(_{}^{T}:=_{}\{_{t=1}^{T}(u ^{t}((x^{t}))-u^{t}(x^{t}))\}.\) Algorithm1 uses an external regret minimization algorithm \(R_{}\) over \(\) which outputs a distribution \(p^{t}()\). We can then decompose the \(\)-regret into two parts.

\[_{}^{T} =\{_{t=1}^{T}u^{t}((x^ {t}))-_{^{} p^{t}}u^{t}(^{}(x^{t})) \}}_{}+^{T} _{^{} p^{t}}u^{t}(^{}(x^{t})) -u^{t}(x^{t})}_{}.\]

I: Bounding the external regret over \(\).The external regret over \(\) can be bounded directly. This is equivalent to an online expert problem: in each iteration \(t\), the external regret minimizer \(_{}\) chooses \(p^{t}()\) and the adversary then determines the utility of each expert \(\) as \(u^{t}((x^{t}))\). We choose the external regret minimizer \(_{}\) to be the Hedge algorithm . Then we have \(_{}\{_{t=1}^{T}u^{t}((x^{t}))-_{^{ } p^{t}}[u^{t}(^{}(x^{t}))]\} 2\) (Theorem6), where we use the fact that the utility function \(u^{t}\) is bounded in \(\).

II: Bounding error due to sampling from an approximate stationary distribution.We first define a distribution \(^{t}\) over \(\) using a complete \(||\)-ary tree with depth \(h\). The root of this tree is an arbitrary strategy \(x_{}\). Each internal node \(x\) has exactly \(||\) children, denoted as \(\{(x)\}_{}\). The distribution \(^{t}\) is supported on the nodes of this tree. Next, we define the probability for each node under the distribution \(^{t}\). The root node \(x_{}\) receives probability \(\). The probability of other nodes is defined in a recursive manner. For every node \(x=(x_{p})\) where \(x_{p}\) is its parent, \(x\) receives probability \(_{^{t}}[x]=_{^{t}}[x_{p}] p^{t}()\). It is then clear that the total probability of the children of a node \(x_{p}\) is exactly \(_{^{t}}[x\) is \(x_{p}\)'s child\(]=_{}[x_{p}] p^{t}()=[x_{p}]\). Denote the set of nodes in depth \(k\) as \(N_{k}\). We have \(_{^{t}}[x N_{k}]=\) for every depth \(1 k h\). Thus the distribution \(^{t}\) supports on \(-1}{||-1}\) points and is well-defined. By the construction above, we know \(x^{t}\) output by Algorithm2 is a sample from \(^{t}\).

Now we show that the approximation error of \(^{t}\) is bounded by \(O()\). We can evaluate the approximation error of \(^{t}\):

\[_{ p^{t}}u^{t}((^{t}))-u^{ t}(^{t}) =_{ p^{t}}_{k=1}^{h}_{x N_{k} }_{^{t}}[x]u^{t}((x))-[_{k=1}^{h}_{x N_{k} }_{^{t}}[x]u^{t}(x)]\] \[=_{k=1}^{h}_{x N_{k}}_{ p^{t} }_{^{t}}[x]u^{t}((x))-_{^{t}}[x]u^{t}(x) .\]We recall that for a node \(x=(x_{p})\) with \(x_{p}\) being its parent, we have \(_{^{}}[x]=_{^{}}[x_{p}] p^{t}()\). Thus for any \(1 k h-1\), we have

\[_{x N_{h}}_{ p^{t}}_{ ^{}}[x]u^{t}((x))-_{^{}}[x]u^{t}(x) =_{x N_{k}}(_{}p^{t}()_{^{ }}[x]u^{t}((x))-_{^{}}[x]u^{t}(x))\] \[=_{x N_{k+1}}_{^{}}[x]u^{t}(x)-_{x N_{k}} _{^{}}[x]u^{t}(x).\]

Using the above equality, we get

\[_{ p^{t}}u^{t}((^{t}))-u^{ t}(^{t})\] \[=_{k=1}^{h-1}_{x N_{k+1}}_{^{}}[x]u^{t}(x)+ _{x N_{h}}_{}p^{t}()_{^{}}[x]u^{t}((x ))-_{k=2}^{h}_{x N_{k}}_{^{}}[x]u^{t}(x)-_{^{}} [x_{}]u^{t}(x_{})\] \[=_{x N_{h}}_{}p^{t}()_{^{}}[x ]u^{t}((x))-_{^{}}[x_{}]u^{t}(x_{}) ,\]

where in the last inequality we use the fact that \(_{x N_{k}}_{^{}}[x]=\) for all \(1 k h\) and the utility function \(u^{t}\) is bounded in \(\). Therefore, for \(x^{t}^{t}\), the sequence of random variables \(\{_{t=1}^{}(_{ p^{t}}[u^{t}((x^{t}))]-u^{t} (x^{t})-)\}_{ 1}\) is a super-martingale. Thanks to the boundedness of the utility function, we can apply the Hoeffding-Azuma Inequality and get for any \(>0\).

\[[_{t=1}^{T}_{ p^{t}}u^{t}((x^ {t}))-u^{t}(x^{t})-] (-}{8T}).\] (1)

Combining the bounds for 1 and 2 with \(=\) and \(h=\), we have, with probability \(1-\), that \(_{}^{T} 2++ 8\).

**Convergence to \(\)-equilibrium.** If all players in a non-concave continuous game employ Algorithm 1, then we know for each player \(i\), with probability \(1-\), its \(^{_{i}}\)-regret is upper bounded by \(8_{i}}|+(n/))}\). By a union bound over all \(n\) players, we get with probability \(1-\), every player \(i\)'s \(^{_{i}}\)-regret is upper bounded by \(8_{i}}|+(n/))}\). Now by Theorem 1, we know the empirical distribution of strategy profiles played forms an \(\)-approximate \(=_{i=1}^{n}^{_{i}}\)-equilibrium, as long as \(T 64((_{i}|^{_{i}}|)+(n/))/^{2}\) iterations. 

## 4 Approximate \(\)-Equilibria under Infinite Local Strategy Modifications

This section studies \(\)-equilibrium when \(||\) is infinite. It is, in general, computationally hard to compute a \(\)-equilibrium even if \(\) contains all constant deviations. Instead, we focus on \(\) that consists solely of local strategy modifications. We introduce several natural classes of local strategy modifications and provide efficient online learning algorithms that converge to \(\)-approximate \(\)-equilibrium in the first-order stationary regime where \(=(^{2}L)\). These approximate \(\)-equilibria guarantee first-order stability.

**Definition 3** (\(\)-local strategy modification).: _For each agent \(i\), we call a set of strategy modifications \(^{_{i}}\)\(\)-local if for all \(x_{i}\) and \(_{i}^{_{i}}\), \(\|_{i}(x)-x\|\). We use notation \(^{_{i}}()\) to denote a \(\)-local strategy modification set for agent \(i\). We also use \(()=_{i=1}^{n}^{_{i}}()\) to denote a profile of \(\)-local strategy modification sets._

Below we present a useful reduction from computing an \(\)-approximate \(()\)-equilibrium in _non-concave_ smooth games to \(^{_{i}}()\)-regret minimization against _convex_ losses for any \(L}{2}\). The key observation here is that the \(L\)-smoothness of the utility function permits, within a \(\)-neighborhood, a \(L}{2}\)-approximation using a linear function. We defer the proof to Appendix D.

**Lemma 1** (No \(()\)-Regret for Convex Losses to Approximate \(()\)-Equilibrium in Non-Concave Games).: _For any \(T 1\) and \(>0\), let \(\) be an algorithm that guarantees to achieve no more than \(_{^{_{i}}()}^{T}^{_{i}} ()\)-regret for convex loss functions for each agent \(i[n]\). Then_

[MISSING_PAGE_EMPTY:9]

### Convex Combination of Finite Local Strategy Modifications

This section considers \(()\) where \(\) is a finite set of local strategy modifications. The set of infinite strategy modifications \(()\) is defined as \(()=\{_{p}(x)=_{}p()(x):p ()\}\). Our main result is an efficient algorithm (Algorithm 3) that guarantees convergence to an \(\)-approximate \(()\)-equilibrium in a smooth game satisfying Assumption 1 for any \(>^{2}L\). Due to space constraints, we defer Algorithm 3 and the proof to Appendix F.

**Theorem 4**.: _Let \(\) be a convex and compact set, \(\) be an arbitrary finite set of \(\)-local strategy modification functions for \(\), and \(u^{1}(),,u^{T}()\) be a sequence of \(G\)-Lipschitz and \(L\)-smooth but possibly non-concave reward functions from \(\) to \(\). If we instantiate Algorithm 3 with \(_{}\) being the Hedge algorithm over \(()\) and \(K=\), the algorithm guarantees that, with probability at least \(1-\), it produces a sequence of strategies \(x^{1},,x^{T}\) with \(()\)-regret at most \(8(G+)+^{2}LT\). The algorithm runs in time \(||\) per iteration._

_If all players in a non-concave smooth game employ Algorithm 3, then with probability \(1-\), for any \(>0\), the empirical distribution of strategy profiles played forms an \((+^{2}L)\)-approximate \(=_{i=1}^{n}^{_{i}}\)-equilibrium,, after \((,G,(_{i}|^{ _{i}}|),)\) iterations._

### Interpolation-Based Local Strategy Modifications

We introduce a natural set of local strategy modifications and the corresponding local equilibrium notion. Given any set of (possibly non-local) strategy modifications \(=\{:\}\), we define a set of _local_ strategy modifications as follows: for \( D_{}\) and \(\), each strategy modification \(_{,}\) interpolates the input strategy \(x\) with the modified strategy \((x)\): formally,

\[^{}_{,}():=\{_{, }(x):=(1-)x+(x):,/D_{ }\}.\]

Note that for any \(\) and \(\), we have \(\|_{,}(x)-x\|=\|x-(x)\|\), representing the locality constraint. The induced \(^{}_{,}()\)-regret can be written as \(^{T}_{,,}:=_{, }}}_{t=1}^{T}(f^{t}(x^{t})-f^{ t}((1-)x^{t}+(x^{t})))\). To guarantee convergence to the corresponding \(\)-equilibrium, it suffices to minimize \(^{}_{,}()\)-regret against convex losses, which we show further reduces to \(\)-regret minimization against convex losses (Theorem 10 in Appendix G).

**CCE-like Instantiation.** In the special case where \(\) contains only _constant_ strategy modifications (i.e., \((x)=x^{*}\) for all \(x\)), we get a coarse correlated equilibrium (CCE)-like instantiation of local equilibrium, which limits the gain by interpolating with any _fixed_ strategy. We denote the resulting set of local strategy modification simply as \(^{}_{}()\). We can apply any no-external regret algorithm for efficient \(^{}_{}()\)-regret minimization and computation of \(\)-approximate \(_{}()\)-equilibrium in the first-order stationary regime as summarized in Theorem 5. We also discuss faster convergence rates in the game setting in Appendix G.

**Theorem 5**.: _For the Online Gradient Descent algorithm (GD)  with step size \(=}}{G}\), its \(^{}_{}()\)-regret is at most \(2 G\). Furthermore, for any \(>0\) and any \(>L}{2}\), when all players employ the GD algorithm in a smooth game, their empirical distribution of played strategy profiles converges to an \((+L}{2})\)-approximate \(_{}()\)-equilibrium in \(O(1/^{2})\) iterations._

## 5 Discussion and Future Directions

**Lower Bound in the Global Regime** When \(\) equals the diameter of our strategy set, it is NP-hard to compute an \(\)-approximate \(()\)-equilibrium (for \(()=_{}(),_{}( )\)), even when \(=(1)\) and \(G,L=O((d))\). Moreover, given black-box access to value and gradient queries, finding such equilibria requires exponentially many queries in at least one of the parameters \(d,G,L,1/\). These results are presented as Theorem 12 and Theorem 13 in Appendix I.

**More Efficient \(\)-Equilibria** We include the discussion of another natural class of local strategy modifications that is based on beam search, where GD suffers linear regret to Appendix H. This result shows that even for simple local strategy modification sets \(()\), the landscape of efficient local \(()\)-regret minimization is already quite rich and many questions remain open. A fruitful future direction is to identify more classes of \(\) that admit efficient regret minimization.