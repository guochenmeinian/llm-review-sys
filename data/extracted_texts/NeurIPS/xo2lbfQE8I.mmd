# Fitting trees to \(\ell_{1}\)-hyperbolic distances

# Fitting trees to \(_{1}\)-hyperbolic distances

Joon-Hyeok Yim

Anna C. Gilbert

###### Abstract

Building trees to represent or to fit distances is a critical component of phylogenetic analysis, metric embeddings, approximation algorithms, geometric graph neural nets, and the analysis of hierarchical data. Much of the previous algorithmic work, however, has focused on generic metric spaces (i.e., those with no _a priori_ constraints). Leveraging several ideas from the mathematical analysis of hyperbolic geometry and geometric group theory, we study the tree fitting problem as finding the relation between the hyperbolicity (ultrametricity) vector and the error of tree (ultrametric) embedding. That is, we define a vector of hyperbolicity (ultrametric) values over all triples of points and compare the \(_{p}\) norms of this vector with the \(_{q}\) norm of the distortion of the best tree fit to the distances. This formulation allows us to define the average hyperbolicity (ultrametricity) in terms of a normalized \(_{1}\) norm of the hyperbolicity vector. Furthermore, we can interpret the classical tree fitting result of Gromov as a \(p=q=\) result. We present an algorithm HCCRootedTreeFit such that the \(_{1}\) error of the output embedding is analytically bounded in terms of the \(_{1}\) norm of the hyperbolicity vector (i.e., \(p=q=1\)) and that this result is tight. Furthermore, this algorithm has significantly different theoretical and empirical performance as compared to Gromov's result and related algorithms. Finally, we show using HCCRootedTreeFit and related tree fitting algorithms, that supposedly standard data sets for hierarchical data analysis and geometric graph neural networks have radically different tree fits than those of synthetic, truly tree-like data sets, suggesting that a much more refined analysis of these standard data sets is called for.

## 1 Introduction

Constructing trees or ultrametrics to fit given data are both problems of great interest in scientific applications (e.g., phylogeny), algorithmic applications (optimal transport and Wasserstein distances are easier, for example, to compute quickly on trees), data visualization and analysis, and geometric machine learning. An ultrametric space is one in which the usual triangle inequality has been strengthened to \((x,y)\{d(x,z),d(y,z)\}\). A hyperbolic metric space is one in which the metric relations amongst any four points are the same as they would be in a tree, up to the additive constant \(\). More generally, any finite subset of a hyperbolic space "looks like" a finite tree.

There has been a concerted effort to solve both of these problems in the algorithmic and machine learning communities, including  among many others. Indeed, the motivation for embedding into hyperbolic space or into trees was at the heart of the recent explosion in geometric graph neural networks .

As an optimization problem, finding the tree metric that minimizes the \(_{p}\) norm of the difference between the original distances and those on the tree (i.e., the distortion) is known to be NP-hard for most formulations.  showed that it is APX-hard under the \(_{}\) norm. The first positive result also came from , which provided a \(3\)-approximation algorithm and introduced a reduction technique from a tree metric to an ultrametric (a now widely used technique). The current best known result is an \(O(( n n)^{1/p})\) approximation for \(1<p<\), and \(O(1)\) approximation for \(p=1\)recently shown by . Both papers exploit the hierarchical correlation clustering reduction method and its LP relaxation to derive an approximation algorithm.

While these approximation results are fruitful, they are not so practical (the LP result uses an enormous number of variables and constraints). On the more practical side,  provided a robust method, commonly known as _Neighbor Join_ (which can be computed in \(O(n^{3})\) time), for constructing a tree. Recently,  proposed an \(O(n^{2})\) method known as _TreeRep_ for constructing a tree. Unfortunately, neither algorithm provides a guaranteed bound on the distortion.

The main drawback with all of these results is that they assume almost nothing about the underlying discrete point set, when, in fact, many real application data sets are close to hierarchical or nearly so. After all, why fit a tree to generic data only to get a bad approximation? In fact, perhaps with some geometric assumptions on our data set, we can fit a better tree metric or ultrametric, perhaps even more efficiently than for a general data set.

Motivated by both Gromov's \(\)-hyperbolicity  and the work of Chatterjee and Slonam  on average hyperbolicity, we define proxy measures of how _tree-like_ a data set is. We note that ,  provide a simple algorithm and analysis to find a tree approximation for which the maximum distortion (\(_{}\) norm) is bounded by \(O( n)\), where \(\) is the hyperbolicity constant. Moreover, this bound turns out to be the best order we can have. In this paper, we go beyond the simple notion of \(\)-hyperbolicity to define a vector of hyperbolicity values \((d)\) for a set of distance values. The various \(_{p}\) norms of this vector capture how tree-like a data set is. Then, we show that the \(_{q}\) norm of the distortion of the best fit tree and ultrametrics can be bounded in terms of this tree proxy. Thus, we give a new perspective on the tree fitting problem, use the geometric nature of the data set, and arrive at, hopefully, better and more efficient tree representations. The table below captures the relationship between the different hyperbolicity measures and the tree fit distortion. We note the striking symmetry in the tradeoffs.

Our main theoretical result can be summarized as

There is an algorithm which runs in time \(O(n^{3} n)\) which returns a tree metric \(d_{T}\) with distortion bounded by the average (or \(_{1}\)) hyperbolicity of the distances; i.e.,

\[\|d-d_{T}\|_{1} 8\|_{x}(d)\|_{1} 8{n-1 3}(d).\]

Additionally, the performance of HCCRottedTreeFit and other standard tree fitting algorithms on commonly used data sets (especially in geometric graph neural nets) shows that they are quite far from tree-like and are not well-represented by trees, especially when compared with synthetic data sets. This suggests that we need considerably more refined geometric notions for learning tasks with these data sets.

## 2 Preliminaries

### Basic definitions

To set the stage, we work with a finite metric space \((X,d)\) and we set \(|X|=n\), the number of triples in \(X\) as \({n 3}=\), and \(r={n 4}\) the number of quadruples of points in \(X\). In somewhat an abuse of notation, we let \({X 3}\) denote the set of all triples chosen from \(X\) (and, similarly, for all quadruples of points). Next, we recall the notion of hyperbolicity, which is defined via the Gromov product . Given a metric space \((X,d)\), the _Gromov product_ of two points \(x,y X\) with respect to a base point

  
**Hyperbolicity measure** & Value & Distortion \\  Gromovâ€™s \(\)-hyperbolicity & \(=\|_{x}\|_{}\) & \(\|d-d_{T}\|_{}=O( n)=O(\|_{x}\|_{} n)\) \\ Average hyperbolicity & \(=}\|_{x}\|_{1}\) & \(\|d-d_{T}\|_{1}=O( n^{3})=O(\|_{x}\|_{1})\) \\   

Table 1: Connection between \(\)-hyperbolicity and average hyperbolicity and how these quantities determine the distortion of the resulting tree metric.

\(w X\) is defined as

\[gp_{w}(x,y):=(d(x,w)+d(y,w)-d(x,y)).\]

We use the definition of the Gromov product on two points with respect to a third to establish the _four point condition_ and define

\[fp_{w}(d;x,y,z):=_{\,}[(gp_{w}( x, z),gp_{w}( y,  z))-gp_{w}( x, y)]\]

where the maximum is taken over all permutations \(\) of the labels of the four points. Since \(fp_{w}(d;x,y,z)=fp_{x}(d;y,z,w)=fp_{y}(d;x,z,w)=fp_{z}(d;x,y,w)\), we sometimes denote the four point condition as \(fp(d;x,y,z,w)\). Similarly, we define the _three point condition_ as

\[tp(d;x,y,z):=_{\,}[d( x, z)-(d( x, y),d( y, z))]\]

which we use to define ultrametricity.

Following a standard definition of Gromov, a metric space \((X,d)\) is said to be \(\)-hyperbolic with respect to the base point \(w X\), if for any \(x,y,z X\), the following holds:

\[gp_{w}(x,y)(gp_{w}(y,z),gp_{w}(x,z))-.\]

We denote \((d)=\), the usual hyperbolicity constant (similarly, \((d)\), is the usual ultrametricity constant). We note that this measure of hyperbolicity is a worst case measure and, as such, it may give a distorted sense of the geometry of the space. A graph which consists of a tree and a single cycle, for instance, is quite different from a single cycle alone but with a worst case measure, we will not be able to distinguish between those two spaces.

In order to disambiguate different spaces, we define the _hyperbolicity_ vector as the \(\)-dimensional vector of all four point conditions with respect to \(d\):

\[_{w}(d)=[fp(d;x,y,z,w)]x,y,z.\]

Similarly, we define the _ultrametricity_ vector as the \(\)-dimensional vector of all three point conditions with respect to \(d\):

\[(d)=[tp(d;x,y,z)]x,y,z.\]

We use the hyperbolicity and ultrametricity vectors to express more refined geometric notions.

We define _\(p\)-average hyperbolicity_ and _\(p\)-average ultrametricity_.

\[_{p}(d)=(_{x,y,z,w }fp(d;x,y,z,w)^{p})^{1/p}\] \[_{p}(d)=(_{x,y,z }tp(d;x,y,z)^{p})^{1/p}\]

If \(p=1\), then the notions are simply the _average_ (and we will call them so). Also, for clarity, note the usual hyperbolicity and ultrametricity constants \((d)\) and \((d)\) are the \(p=\) case.

**Proposition 2.1**.: We have the simple relations:

1. \((d)=_{x X}\|_{x}(d)\|_{} \|_{x}(d)\|_{}\) for any \(x X\).
2. \((d)=\|(d)\|_{}\).

In the discussion of hierarchical correlation clustering in Section 2.4 and in the analysis of our algorithms in Section 3, we construct multiple graphs using the points of \(X\) as vertices and derived edges. Of importance to our analysis is the following combinatorial object which consists of the set of bad triangles in a graph (i.e., those triples of vertices in the graph for which exactly two edges, rather than three, are in the edge set). Given a graph \(G=(V,E)\), denote \(B(G)\), the set of _bad triangles_ in \(G\), as

\[B(G):=\{(x,y,z)|\,|\{(x,y),(y,z),(z,x)\} E|=2\}.\]

### Problem formulation

First, we formulate the tree fitting problem. Given a finite, discrete metric space \((X,d)\) and the distance \(d(x_{i},x_{j})\) between any two points \(x_{i},x_{j} X\), find a tree metric \((T,d_{T})\) in which the points in \(X\) are among the nodes of the tree \(T\) and the tree distance \(d_{T}(x_{i},x_{j})\) is "close" to the original distance \(d(x_{i},x_{j})\).

While there are many choices to measure how close \(d_{T}\) and \(d\) are, in this paper, we focus on the \(_{p}\) error; i.e., \(\|d_{T}-d\|_{p}\), for \(1 p\). This definition is itself a shorthand notation for the following. Order the pairs of points \((x_{i},x_{j}),i<j\) lexicographically and write \(d\) (overloading the symbol \(d\)) for the vector of pairwise distances \(d(x_{i},x_{j})\). Then, we seek a tree distance function \(d_{T}\) whose vector of pairwise tree distances is close in \(_{p}\) norm to the original vector of distances. For example, if \(p=\), we wish to bound the _maximum_ distortion between any pairs on \(X\). If \(p=1\), we wish to bound the total error over all pairs. Similarly, we define the ultrametric fitting problem.

We also introduce the _rooted_ tree fitting problem. Given a finite, discrete metric space \((X,d)\) and the distance \(d(x_{i},x_{j})\) between any two points \(x_{i},x_{j} X\), and a distinguished point \(w X\), find a tree metric \((T,d_{T})\) such that \(\|d-d_{T}\|_{p}\) is small and \(d_{T}(w,x)=d(w,x)\) for all \(x X\). Although the rooted tree fitting problem has more constraints, previous work (such as ) shows that by choosing the base point \(w\) appropriately, the optimal error of the rooted tree embedding is bounded by a constant times the optimal error of the tree embedding. Also, the rooted tree fitting problem is closely connected to the ultrametric fitting problem.

Putting these pieces together, we observe that while considerable attention has been paid to the \(_{q}\) tree fitting problem for generic distances with only mild attention paid to the assumptions on the input distances. No one has considered _both_ restrictions on the distances and more sophisticated measures of distortion. We define the \(_{p}/_{q}\) tree (ultrametric) fitting problem as follows.

**Definition 2.2** (\(_{p}/_{q}\) tree (ultrametric) fitting problem).: Given \((X,d)\) with hyperbolicity vector \(_{x}(d)\) and \(_{q}(d)\) (ultrametricity vector \((d)\) and \(_{q}(d)\)), find the tree metric \((T,d_{T})\) (ultrametric \((X,d_{U})\)) with distortion

\[\|d-d_{T}\|_{p}_{q}(d) f(n)\|d-d_{U}\|_{p} _{q}(d) f(n)\]

for a growth function \(f(n)\) that is as small as possible. (Indeed, \(f(n)\) might be simply a constant.)

### Previous results

Next, we detail Gromov's classic theorem on tree fitting, using our notation above.

**Theorem 2.3**.: _[_4_]_ _Given a \(\)-hyperbolic metric space \((X,d)\) and a reference point \(x X\), there exists a tree structure \(T\) and its metric \(d_{T}\) such that_

1. \(T\) _is_ \(x\)_-restricted, i.e.,_ \(d(x,y)=d_{T}(x,y)\) _for all_ \(y X\)_._
2. \(\|d-d_{T}\|_{} 2\|_{x}(d)\|_{}_{2 }(n-2)\)_._

In other words, we can bound the _maximum distortion_ of tree metric in terms of the hyperbolicity constant \((d)=\) and the size of our input space \(X\).

### (Hierarchical) Correlation clustering and ultrametric fitting

Several earlier works (, ) connected the correlation clustering problem to that of tree and ultrametric fitting and, in order to achieve our results, we do the same. In the correlation clustering problem, we are given a graph \(G=(V,E)\) whose edges are labeled "+" (similar) or "-" (different) and we seek a clustering of the vertices into two clusters so as to minimize the number of pairs incorrectly classified with respect to the input labeling. In other words, minimize the number of "-" edges within clusters plus the number of "+" edges between clusters. When the graph \(G\) is complete, correlation clustering is equivalent to the problem of finding an optimal ultrametric fit under the \(_{1}\) norm when the input distances are restricted to the values of 1 and 2.

Hierarchical correlation clustering is a generalization of correlation clustering that is also implicitly connected to ultrametric and tree fitting (see , , , ). In this problem, we are given a set of non-negatiweweights and a set of edge sets. We seek a partition of vertices that is both hierarchical andminimizes the weighted sum of incorrectly classified pairs of vertices. It is a (weighted) combination of correlation clustering problems.

More precisely, given a graph \(G=(V,E)\) with \(k+1\) edge sets \(G_{t}=(V,E_{t})\), and \(k+1\) weights \(_{t} 0\) for \(0 t k\), we seek a _hierarchical_ partition \(P_{t}\) that minimizes the \(_{1}\) objective function, \(_{t}|E_{t} E(P_{t})|\). It is _hierarchical_ in that for each \(t\), \(P_{t}\) subdivides \(P_{t+1}\).

Chowdury, et al.  observed that the Single Linkage Hierarchical Clustering algorithm (SLHC) whose output can be modified to produce an ultrametric that is designed to fit a given metric satisfies a similar property to that of Gromov's tree fitting result. In this case, the distortion bound between the ultrametric and the input distances is a function of the ultrametricity of the metric space.

**Theorem 2.4**.: _Given \((X,d)\) and the output of SLHC in the form of an ultrametric \(d_{U}\), we have_

\[\|d-d_{U}\|_{}\|(d)\|_{}_{2}(n-1).\]

### Reductions and equivalent bounds

Finally, we articulate precisely how the tree and ultrametric fitting problems are related through the following reductions. We note that the proof of this theorem uses known techniques from  and  although the specific results are novel. First, an ultrametric fitting algorithm yields a tree fitting algorithm.

**Theorem 2.5**.: _Given \(1 p<\) and \(1 q\). Suppose we have an ultrametric fitting algorithm such that for any distance function \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{U}\) satisfies_

\[\|d-d_{U}\|_{p}_{q}(d) f(n)f(n).\]

_Then there exists a tree fitting algorithm (using the above) such that given an input \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{T}\) satisfies_

\[\|d-d_{T}\|_{p} 2()^{1/q}_{q}(d)  f(n)f(n).\]

Conversely, a tree fitting algorithm yields an ultrametric fitting algorithm. From which we conclude that both problems should have the same asymptotic bound, which justifies our problem formulation.

**Theorem 2.6**.: _Given \(1 p<\) and \(1 q\). Suppose that we have a tree fitting algorithm such that for any distance function \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{T}\) satisfies_

\[\|d-d_{T}\|_{p}_{q}(d) f(n)f(n).\]

_Then there exists an ultrametric fitting algorithm (using the above) such that given an input \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{T}\) satisfies_

\[\|d-d_{U}\|_{p} 3^{}(+2^{q-4})^{1/q} _{q}(d) f(2n)f(n).\]

Proofs of both theorems can be found in the Appendix 6.16.2.

## 3 Tree and ultrametric fitting: Algorithm and analysis

In this section, we present an algorithm for the \(p=q=1\) ultrametric fitting problem. We also present the proof of upper bound and present an example that shows this bound is asymptotically tight (despite our empirical evidence to the contrary). Then, using our reduction from Section 2, we produce a tree fitting result.

### HCC Problem

As we detailed in Section 2, correlation clustering is connected with tree and ultrametric fitting and in this section, we present a hierarchical correlation clustering (HCC) algorithm which _bounds_ the number of disagreement edges by constant factor of number of bad triangles. We follow with a proposition that shows the connection between bad triangle objectives and the \(_{1}\) ultrametricity vector.

**Definition 3.1** (HCC with triangle objectives).: Given a vertex set \(V\) and \(k+1\) edge sets, set \(G_{t}=(V,E_{t})\) for \(0 t k\). The edge sets are hierarchical so that \(E_{t} E_{t+1}\) for each \(t\). We seek a _hierarchical_ partition \(P_{t}\) so that for each \(t\), \(P_{t}\) subdivides \(P_{t+1}\) and the number of disagreement edges \(|E_{t} E(P_{t})|\) is bounded by \(C|B(G_{t})|\) (where \(B\) denotes the set of bad triangles) for some constant \(C>0\).

Note that this problem does not include the weight sequence \(\{_{t}\}\), as the desired output will also guarantee an upper bound on \(_{t}|E_{t} E(P_{t})|\), the usual \(_{1}\) objective.

This proposition relates the \(_{1}\) vector norm of \((d)\), the average ultrametricity of the distances, and the bad triangles in the derived graph. This result is why we adapt the hierarchical correlation clustering problem to include triangle objectives.

**Proposition 3.2**.: Given distance function \(d\) on \(\) (with \(|X|=n\)) and \(s>0\), consider the \(s\)-neighbor graph \(G_{s}=(X,E_{s})\) where \(E_{s}\) denotes \(\{(x,y)|d(x,y) s\}\). Then we have

\[\|(d)\|_{1}=(d)=_{0}^{}|B(G_{s })|ds.\]

### Main results

Our main contribution is that the HCCTriangle algorithm detailed in Section 3.3 solves our modified HCC problem and its output partition behaves "reasonably" on every level. From this partition, we can construct a good ultrametric fit which we then leverage for a good (rooted) tree fit (using the reduction from Section 2.

**Theorem 3.3**.: HCCTriangle _outputs a hierarchical partition \(P_{t}\) where \(|E_{t} E(P_{t})| 4|B(G_{t})|\) holds for every \(0 t=k\). Furthermore, the algorithm runs in time \(O(n^{2})\)._

By Proposition 3.2 and Theorem 3.3, we can find an ultrametric fit using the HCCTriangle subroutine to cluster our points. This algorithm we refer to as HCCUltraFit. Using Theorem 3.3, we have following \(_{1}\) bound.

**Theorem 3.4**.: _Given \(d\), HCCUltraFit outputs an ultrametric \(d_{U}\) with \(\|d-d_{U}\|_{1} 4\|\|_{1}=4_{1}(d)\). The algorithm runs in time \(O(n^{2} n)\)._

In other words, HCCUltraFit solves the HCC Problem 3.1 with constant \(C=4\). The following proof shows why adapting HCCTriangle as a subroutine is successful.

**Proof of Theorem 3.4 from Theorem 3.3:** Suppose the outputs of HCCTriangle and HCCUltraFit are \(\{P_{t}\}\) and \(d_{U}\), respectively. Denote \(d_{i}:=d(e_{i})\) (\(d_{0}=0,d_{k+1}=\)) and, for any \(s>0\), set

\[G_{s}=(X,E_{t}),\ P_{s}=P_{t}\ \ d_{t} s<d_{t+1}.\]

Then, for any \(x,y\), we see that

\[(x,y) E_{s} E(P_{s}) d(x,y) s<d_{U}(x,y)\ \ \ d_{U}(x,y) s<d(x,y).\]

Again, we use the integral notion from Proposition 3.2. Every edge \((x,y)\) will contribute \(|E_{s} E(P_{s})|\) with amount exactly \(|d_{U}(x,y)-d(x,y)|\). Then, by Theorem 3.3,

\[\|d-d_{U}\|_{1} =_{0}^{}|E_{s} E(P_{s})|ds\] \[ 4_{0}^{}|B(G_{s})|ds=4\|(d)\|_{1},\]

as desired. Assuming that HCCTriangle runs in time \(O(n^{2})\), HCCUltraFit runs in time \(O(n^{2} n)\) as the initial step of sorting over all pairs is needed. Thus ends the proof.

By the reduction argument we discussed in Section 2, we can put all of these pieces together to conclude the following:

**Theorem 3.5**.: _Given \((X,d)\), we can find two tree fits with the following guarantees:_

* _Given a base point point_ \(x X\)_, HCCRootedTreeFit outputs a tree fit_ \(d_{T}\) _with_ \(\|d-d_{T}\|_{1} 8\|_{x}(d)\|_{1}\)_. The algorithm runs in_ \(O(n^{2} n)\)* _There exists_ \(x X\) _where_ \(\|_{x}(d)\|_{1}_{1}(d)\)_. Therefore, given_ \((X,d)\)_, one can find a (rooted) tree metric_ \(d_{T}\) _with_ \(\|d-d_{T}\|_{1} 8_{1}(d)\) _in time_ \(O(n^{3} n)\)_._

### Algorithm and analysis

``` functionisHighlyConnected Input: vertex set \(X,Y\) and edge set \(E\)  For \(x X\):  If \(|\{y Y|(x,y) E\}|<\):  return False  For \(y Y\):  If \(|\{x X|(x,y) E\}|<\):  return False return True ```

**Algorithm 1**isHighlyConnected: tests if two clusters are highly connected.

``` functionHCCTriangle Input: \(V=\{v_{1},,v_{n}\}\) and an ordering of all pairs \(\{e_{1},e_{2},,e_{}\}\) on \(V\) so that \(E_{t}=\{e_{1},e_{2},,e_{t}\}\). Desired Output: _hierarchical_ partition \(\{P_{t}\}\) for \(0 t=k\) so that \(|E_{t} E(P_{t})| 4|B(G_{t})|\) holds for every \(t\). Init: \(=\{\{v_{1}\},\{v_{2}\},,\{v_{n}\}\}\) \(P_{0}\)  For \(t\{1,2,,\}\):  Take \(e_{t}=(x,y)\) with \(x C_{x}\) and \(y C_{y}\) (\(C_{x},C_{y}\))  If \(C_{x} C_{y}\):  If \((C_{x},C_{y},E_{t})\) is true:  add \(C=C_{x} C_{y}\) and remove \(C_{x},C_{y}\) in \(\). \(P_{t}\) return\(\{P_{t}\}\) ```

**Algorithm 2**HCCTriangle: Find HCC which respects triangle objectives

In the rest of this subsection, we provide a sketch of the proof that HCCTriangle provides the desired correlation clustering (the detailed proof can be found in Appendix 6.4). We denote the output of HCCTriangle by \(\{P_{t}\}\) and also assume \(E=E_{t}\) and \(P_{t}=\{C_{1},C_{2},,C_{k}\}\). The algorithm HCCTriangle agglomerates two clusters if they are highly connected, adds the cluster to the partition, and iterates. The key to the ordering of the edges input to HCCTriangle is that they are ordered by increasing distance so that the number of edges that are in "disagreement" in

Figure 1: Illustration of highly connectedness condition

the correlation clustering is upper bounded by the number of edges whose distances "violate" a triangle relationship. The proof proceeds in a bottom up fashion. (It is clear that the output partition is naturally hierarchical.) We count the number of bad triangles "within" and "between" clusters, which are lower bounded by the number of disagreement edges "within" and "between" clusters, respectively. The proof uses several combinatorial properties which follow from the highly connected condition. This is the key point of our proof.

**From an ultrametricity fit to a rooted tree fit:** Following a procedure from Cohen-Addad, et al. , we can also obtain a tree fit with the \(_{1}/_{1}\) objective. Note that the algorithm HCCRootedTreeFit takes the generic reduction method from ultrametric fitting algorithm but we have instantiated it with HCCUltraFit. For a self-contained proof, see the Appendix 6.3.

**Proof of Theorem 3.5:** This proof can be easily shown simply by applying Theorem 2.5 with \(p=q=1\) and \(f(n)=4\).

```
1:procedureHCCRootedTreeFit
2:Input: distance function \(d\) on \(\) and base point \(w X\)
3:Output: (rooted) tree metric \(d_{T}\) which fits \(d\) and \(d(x,w)=d_{T}(x,w)\) for all \(x X\)
4:\(M_{x X}d(x,w)\)
5:\(c_{w}(x,y) 2M-d(x,w)-d(y,w)\) for all \(x,y\)
6:\(d_{U}(d+c_{w})\)
7:\(d_{T} d_{U}-c_{w}\)
8:return\(d_{T}\) ```

**Algorithm 4** Find a tree fitting given an ultrametric fitting procedure

**Running Time** Although isHighlyConnected is seemingly expensive, there is a way to implement HCCTriangle so that all procedures run in \(O(n^{2})\) time. Thus, HCCUltraFit can be implemented so as to run in \(O(n^{2} n)\) time. The detailed algorithm can be found in Appendix 6.5.

**Asymptotic Tightness** Consider \((d,X)\) with \(X=\{x_{1},,x_{n}\}\) and \(d(x_{1},x_{2})=d(x_{1},x_{3})=1\) and 2 otherwise. Then we see that \(tp(d;x_{1},x_{2},x_{3})=1\) and 0 otherwise, so that \(\|\|_{p}=1\). One can easily check that for any ultrametric \(d_{U}\), \(|(x_{1},x_{2})|^{p}+|(x_{1},x_{3})|^{p}+|(x_{2},x_{3} )|^{p} 2^{1-p}\) for \(:=d_{U}-d\). When \(p=1\), \(\|d-d_{U}\|_{1}\|(d)\|_{1}=(d)\) holds for any ultrametric \(d_{U}\). While HCCUltraFit guarantees \(\|d-d_{U}\|_{1} 4\|(d)\|_{1}=4(d)\); this shows that our theoretical bound is asymptotically tight.

Examples demonstrating _how_ HCCUltraFit works and related discussion can be found in Appendix 6.7.1.

## 4 Experiments

In this section, we run HCCRootedTreeFit on several different type of data sets, those that are standard for geometric graph neural nets and those that are synthetic. We also compare our results with other known algorithms. We conclude that HCCRootedTreeFit (HCC) is optimal when the data sets are close to tree-like and when we measure with respect to distortion in the \(_{1}\) sense and running time. It is, however, suboptimal in terms of the \(_{}\) measure of distortion (as to be expected). We also conclude that purportedly hierarchical data sets do not, in fact, embed into trees with low distortion, suggesting that geometric graph neural nets should be configured with different geometric considerations. Appendix 6.9 contains further details regarding the experiments.

### Common data sets

We used common unweighted graph data sets which are known to be hyperbolic or close to tree-like and often used in graph neural nets, especially those with geometric considerations. The data sets we used are C-elegan, CS Phd from , and CORA, Airport from . (For those which contain multiple components, we chose the largest connected component.) Given an unweighted graph, we computed its shortest-path distance matrix and used that input to obtain a tree metric. We compared these results with the following other tree fitting algorithms TreeRep (TR) , NeighborJoin (NJ) , and the classical Gromov algorithm. As TreeRep is a randomized algorithm and HCCRootedTreeFit and Gromov's algorithm depends on the choice of a pivot vertex, we run all of these algorithms 100 times and report the average error with standard deviation. All edges with negative weights have been modified with weight 0, as TreeRep and NeighborJoin both occasionally produce edges with negative weights. Recall, both TreeRep and NeighborJoin enjoy _no_ theoretical guarantees on distortion.

First, we examine the results in Table 2. We note that although the guaranteed bound (of average distortion), \(8{n-1 3}(d)/{n 2}\) is asymptotically tight even in worst case analysis, this bound is quite loose in practice; most fitting algorithms perform much better than that. We also see that the \(_{1}\) error of HCCRootedTreeFit is comparable to that of TreeRep, while NeighborJoin performs much better than those. It tends to perform better when the graph data set is known to be _more_ hyperbolic (or tree-like), _despite no theoretical guarantees_. It is, however, quite slow.

Also, we note from Table 3 that Gromov's algorithm, which solves our \(_{}/_{}\) hyperbolicity problem tends to return better output in terms of \(_{}\) error. On the other hand, its result on \(_{1}\) error is not as good as the other algorithms. In contrast, our HCCRootedTreeFit performs better on the \(_{1}\) objective, which suggests that our approach to this problem is on target.

    & C-elegan & CS Phd & CORA & Airport \\  \(O(n^{3} n)\) & HCC & 0.648\(\)0.013 & 3.114\(\)0.029 & 18.125\(\)0.330 & 28.821\(\)0.345 \\ \(O(n^{3})\) & Gromov & 0.055\(\)0.005 & 0.296\(\)0.004 & 2.063\(\)0.033 & 3.251\(\)0.033 \\ \(O(n^{3})\) & TR & 0.068\(\)0.009 & 0.223\(\)0.046 & 0.610\(\)0.009 & 0.764\(\)0.151 \\ \(O(n^{3})\) & NJ & 0.336 & 4.659 & 268.45 & 804.67 \\   

Table 4: Running Time(s). For NJ, we implemented the naive algorithm, which is \(O(n^{3})\). We note that TreeRep produces a tree and not a set of distances; its running times excluded a step which computes the distance matrix, which takes \(O(n^{2})\) time.

  
**Data set** & C-elegan & CS Phd & CORA & Airport \\  HCC & 4.3\(\)0.64 & 23.37\(\)3.20 & 19.30\(\)1.11 & 7.63\(\)0.54 \\ Gromov & 3.32\(\)0.47 & **13.24\(\)**0.67 & **9.23\(\)**0.53 & **4.04\(\)**0.20 \\ TR & 5.90\(\)0.72 & 21.01\(\)3.34 & 16.86\(\)2.11 & 10.00\(\)1.02 \\ NJ & **2.97** & 16.81 & 13.42 & 4.18 \\   

Table 3: \(_{}\) error (i.e., max distortion)In analyzing the running times in Table 4, we notice that HCCRootedTreeFit runs in truly \(O(n^{2} n)\) time. Also, its dominant part is the subroutine HCCTriangle, which runs in \(O(n^{2})\).

### Synthetic data sets

In order to analyze the performance of our algorithm in a more thorough and rigorous fashion, we generate random synthetic distances with low hyperbolicity. More precisely, we construct a synthetic weighted graph from fixed balanced trees. We use \((r,h)\) to indicate a balanced tree with branching factor \(r\) and height \(h\) and Disease from , an unweighted tree data set. For each tree, we add edges randomly, until we reach 500 additional edges. Each added edge is given a distance designed empirically to keep the \(\)-hyperbolicity bounded by a value of 0.2.

Then we measured the \(_{1}\) error of each tree fitting algorithm (and averaged over 50 trials). Note that all rooted tree fitting algorithms use a root node (for balanced trees, we used the apex; for Disease, we used node 0).

For these experiments, we see quite different results in Table 5. All of these data sets are truly _tree-like_. Clearly, NeighborJoin performs considerably worse on these data than on the common data sets above, especially when the input comes from a tree with _high branching factor_ (note that the branching factor of Disease is recorded as 6.224, which is also high). We also note that Gromov's method behaves much worse than all the other algorithms. This is possibly because Gromov's method is known to produce a _non-stretching_ tree fit, while it is a better idea to _stretch_ the metric in this case. The theoretical bound is still quite loose, but not as much as with the common data sets.

## 5 Discussion

All of our experiments show that it is critical to quantify how "tree-like" a data set is in order to understand how well different tree fitting algorithms will perform on that data set. In other words, we cannot simply assume that a data set is generic when fitting a tree to it. Furthermore, we develop both a measure of how tree-like a data set is and an algorithm HCCRootedTreeFit that leverages this behavior so as to minimize the appropriate distortion of this fit. The performance of HCCRootedTreeFit and other standard tree fitting algorithms shows that commonly used data sets (especially in geometric graph neural nets) are quite far from tree-like and are not well-represented by trees, especially when compared with synthetic data sets. This suggests that we need considerably more refined geometric notions for learning tasks with these data sets.

  
**Initial Tree** & \((8,3)\) & \((5,4)\) & \((3,5)\) & \((2,8)\) & Disease \\  \(n\) & 585 & 776 & 364 & 511 & 2665 \\ \(\|_{r}\|_{1}/\) & 0.01887\(\)0.00378 & 0.01876\(\)0.00375 & 0.00150\(\)0.0036 & 0.00098\(\)0.0020 & 0.00013\(\)0.00310 \\  HCC & **0.00443\(\)**0.00098 & 0.01538\(\)0.00733 & **0.04153\(\)**0.01111 & 0.07426\(\)0.02027 & **0.00061\(\)**0.00588 \\ Gromov & 0.18225\(\)0.00237 & 0.44015\(\)0.00248 & 0.17085\(\)0.00975 & 0.16898\(\)0.00175 & 0.18977\(\)0.00196 \\ TR & 0.01360\(\)0.00366 & **0.01103\(\)**0.00365 & 0.06080\(\)0.0874 & 0.0959\(\)0.01857 & 0.00081\(\)0.00059 \\ NJ & 0.03180\(\)0.00767 & 0.06092\(\)0.01951 & 0.04309\(\)0.00321 & **0.04360\(\)**0.00648 & 0.00601\(\)0.00281 \\   

Table 5: Average \(_{1}\) error when \(n_{c}=500\)