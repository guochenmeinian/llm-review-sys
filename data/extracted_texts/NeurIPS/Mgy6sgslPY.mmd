# Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal

Leah Chrestien

Czech Technical University in Prague

leah.chrestien@aic.fel.cvut.cz &Tomas Pevny

Czech Technical University in Prague,

and Gen Digital, Inc.

pevnytom@fel.cvut.cz &Stefan Edelkamp

Czech Technical University in Prague

edelkste@fel.cvut.cz &Antonin Komenda

Czech Technical University in Prague

antonin.komenda@fel.cvut.cz

###### Abstract

In imitation learning for planning, parameters of heuristic functions are optimized against a set of solved problem instances. This work revisits the necessary and sufficient conditions of strictly optimally efficient heuristics for forward search algorithms, mainly A* and greedy best-first search, which expand only states on the returned optimal path. It then proposes a family of loss functions based on ranking tailored for a given variant of the forward search algorithm. Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal \(h^{*}\) is unnecessarily difficult. The experimental comparison on a diverse set of problems unequivocally supports the derived theory.

## 1 Introduction

Automated planning finds a sequence of actions that will reach a goal in a model of the environment provided by the user. It is considered to be one of the core problems in Artificial Intelligence and it is behind some of its successful applications . Early analysis of planning tasks  indicated that optimizing the heuristic function steering the search for a given problem domain towards the goal can dramatically improve the performance of the search. Automated optimization of the heuristic function therefore becomes a central request in improving the performance of planners .

Learning in planning means optimizing heuristic functions from plans of already solved problems and their instances. This definition includes the selection of proper heuristics in a set of pattern databases , a selection of a planner from a portfolio , learning planning operators from instances , learning for macro-operators and entanglements , and learning heuristic functions by general function approximators (e.g. neural networks) .

The majority of research  optimizes the heuristic function to estimate the cost-to-goal, as it is well known that it is an optimal heuristic for many best-first heuristic search algorithms including A* or IDA* . These works optimize the heuristic function to solve _regression problems_. But even a true cost-to-goal \(h^{*}\) does not guarantee that the _forward search_ will find the optimal solution while expanding the minimal number of states. This paper defines a stricter version of _optimally efficiency_ as follows. _Forward search_ (or its heuristic) is called _strictly optimally efficient_ iff it expands _only_ states on one optimal solution path returned by the search, i.e., if this optimal solution path has \(l+1\) states, the search will expand only \(l\) states.

This work focuses exclusively on a _forward search_ with merit functions. It presents theoretical arguments as to why the order (rank) of states provided by the heuristic function is more importantthan the precise estimate of the cost-to-goal and it formalizes the necessary and sufficient condition for strictly optimally efficient search. It also argues why learning to rank is a simpler problem than regression estimating the cost-to-goal from a statistical learning theory point of view. The main motivation of the paper is similar to that in , but the derived theory and implementation are much simpler. The sufficiency of optimizing the rank has been already recognized in  for a beam-search and in  for greedy best-first search (GBFS), though, unlike this work, it cannot guarantee strict optimal efficiency.

We emphasize that this work neither deals with nor questions the existence of a strictly optimally efficient heuristic function with the forward search. It is assumed that the space of possible heuristic functions is sufficiently large, such that there exist one that is sufficiently good. The theoretical results are supported by experimental comparisons to the prior art on eight domains.

The contributions of this paper are as follows.

1. We state necessary and sufficient conditions for strictly optimally efficient heuristic functions for forward search algorithms in deterministic planning.
2. We show that when optimizing a heuristic function, it is better to solve the ranking problem.
3. We argue, why learning to rank is easier than regression used in learning the cost-to-goal.
4. We instantiate it for A* and GBFS searches leading to two slightly different loss functions.
5. We experimentally demonstrate on eight problems (three grid and five PDDL) that optimizing rank is _always_ better.

## 2 Preliminaries

A search problem instance is defined by a directed weighted graph \(=,,w\), a distinct node \(s_{0}\) and a distinct set of nodes \(^{*}\). The nodes \(\) denote all possible states \(s\) of the underlying transition system representing the graph. The set of edges \(\) contains all possible transitions \(e\) between the states in the form \(e=(s,s^{})\). \(s_{0}\) is the initial state of the problem instance and \(^{*}\) is a set of allowed goal states. Problem instance graph weights (alias action costs) are mappings \(w:^{ 0}\).

A path (alias a plan) \(=(e_{1},e_{2},,e_{l})\), \(e_{i}=(s_{i-1},s_{i})\), of length \(l\) solves a task \(\) with \(s_{0}\) and \(^{*}\) iff \(=((s_{0},s_{1}),(s_{1},s_{2}),,(s_{l-1},s_{l}))\) and \(s_{l}^{*}\). An optimal path \(^{*}\) is defined as a path minimizing the cost of a problem instance \(,s_{0},^{*}\), its value as \(f^{*}=w(^{*})=_{i=1}^{l}w(e_{i})\). When cost function \(w(e)=1\) optimal path corresponds to the shortest one. \(_{:i}=((s_{0},s_{1}),(s_{1},s_{2}),,(s_{i-1},s_{i}))\) denotes subplan of a plan \(\) containing its first \(i\) actions. \(^{}=\{s_{i}\}_{i=0}^{l}\) denotes the set of states from the path \(=((s_{0},s_{1}),(s_{1},s_{2}),,(s_{l-1},s_{l})),\) and correspondingly \(^{_{:i}}\) denotes the set of states in a subplan \(_{:i}\).

### Forward search algorithm

To align notation, we briefly review the forward search algorithm, of which A* and GBFS are special cases. As we apply full duplicate detection in all of our algorithms, there is a bijection of search tree nodes to planning states. Forward Search for consistent heuristics, where \(h(s)-h(s^{}) w(s,s^{})\) for all edges \((s,s^{})\) in the \(w\)-weighted state space graph, mimics the working of Dijkstra's shortest-path algorithm . It maintains two sets: the first called _Open list_, \(\), contains generated but not expanded nodes; the second called _Closed list_, \(\), contains already expanded nodes. Parameters \(\) and \(\) of a merit function \(f(s)= g(s)+ h(s)\) used to sort states in Open list allow to conveniently describe different variants of the algorithm as described in detail shortly below. The forward search works as follows.

1. Initialise Open list as \(_{0}=\{s_{0}\}\).
2. Set \(g(s_{0})=0\)
3. Initiate the Closed list to empty, i.e. \(_{0}=\).
4. For \(i 1,\) until \(_{i}=\) 1. Select the state \(s_{i}=_{s_{i-1}} g(s)+ h(s)\)2. Remove \(s_{i}\) from \(_{i-1},\)\(_{i}=_{i-1}\{s_{i}\}\) 3. If \(s_{i}^{*}\), i.e. it is a goal state, go to 5. 4. Insert the state \(s_{i}\) to \(_{i-1},\)\(_{i}=_{i-1}\{s_{i}\}\) 5. Expand the state \(s_{i}\) into states \(s^{}\) for which hold \((s_{i},s^{})\) and for each 1. set \(g(s^{})=g(s_{i})+w(s_{i},s^{})\) 2. if \(s^{}\) is in the Closed list as \(s_{c}\) and \(g(s^{})<g(s_{c})\) then \(s_{c}\) is reopened (i.e., moved from the Closed to the Open list), else continue with (e) 3. if \(s^{}\) is in the Open list as \(s_{o}\) and \(g(s^{})<g(s_{o})\) then \(s_{o}\) is updated (i.e., removed from the Open list and re-added in the next step with updated \(g()\)), else continue with (e) 4. add \(s^{}\) into the Open list
5. Walk back to retrieve the solution path.

In the above algorithm, \(g(s)\) denotes a function assigning an accumulated cost \(w\) for moving from the initial state (\(s_{0}\)) to a given state \(s\). During its execution, it always expands nodes with the lowest \(f(s)= g(s)+ h(s)\). Different settings of \(\) and \(\) give rise to different algorithms: for A*, \(==1\); for GBFS \(=0\) and \(=1\).

_Consistent heuristics_, which are of special interest, are called monotone because the estimated cost of a partial solution \(f(s)=g(s)+h(s)\) is monotonically non-decreasing along the best path to the goal. More than this, \(f\) is monotone on all edges \((s,s^{})\), if and only if \(h\) is consistent as we have \(f(s^{})=g(s^{})+h(s^{}) g(s)+w(s,s^{})+h(s)-w(s,s^ {})=f(s)\). For the case of consistent heuristics, no reopening (moving back nodes from Closed to Open) in A* is needed, as we essentially traverse a state-space graph with edge weights \(w(s,s^{})+h(s^{})-h(s) 0\). For the trivial heuristic \(h_{0}\), we have \(h_{0}(s)=0\) and for perfect heuristic \(h^{*}\), we have \(f(s)=f^{*}=g(s)+h^{*}(s)\) for all nodes \(s\). Both heuristics \(h_{0}\) and \(h^{*}\) are consistent. For GBFS and other variants, reopening is usually neglected. Even if the heuristic is not consistent, best-first algorithms without the reopening, remain complete i.e., they find a plan if there is one. Plans might not be provably optimal but are often good for planning practice .

## 3 Conditions on strictly optimally efficient heuristic

Let \(_{i}\) be an Open list in the \(i^{}\) iteration of the forward search expanding only states on the optimal path \(\). Below definition defines a _perfect ranking heuristic_ as a heuristic function where state on some optimal path in the Open list, \(^{_{,i}}_{i}\), has _always_ strictly lower merit value than other states in the Open list off the optimal path, \(_{i}^{_{,i}}\). The intersection \(^{_{,i}}_{i}\) contains always exactly one state.

**Definition 1** (Perfect ranking heuristic).: _A heuristic function \(h(s)\) is a **perfect ranking** in forward search with a merit function \(f(s)= g(s)+ h(s)\) for a problem instance \(=(,,w,s_{0},^{*})\) if and only if there exists an optimal plan \(=((s_{0},s_{1}),(s_{1},s_{2}),,(s_{l-1},s_{l}))\) such that_

* \(g(s)\) _is the cost from_ \(s_{0}\) _to_ \(s\) _in a search-tree created by expanding only states on the optimal path_ \(;\)__
* \( i\{1,,l\}\) _and_ \( s_{j}_{i}^{_{,i}}\) _we have_ \(f(s_{j})>f(s_{i})\)_._

As an example consider a search-tree in Figure 0(a) with an optimal path \(((s_{0},s_{1}),(s_{1},s_{2}),(s_{2},s_{3}))\) and states \(\{s_{4},s_{5},s_{6},s_{7}\}\) off the optimal path. Then the Open list after expanding \(s_{0}\) is \(_{1}=\{s_{1},s_{4},s_{5}\}\) and that after expanding \(s_{1}\) is \(_{2}=\{s_{2},s_{4},s_{5},s_{6},s_{7}\}\). The set of states on the optimal sub-path are \(^{_{,1}}=\{s_{0},s_{1}\}\), \(^{_{,2}}=\{s_{0},s_{1},s_{2}\}\). States on the optimal path in the Open lists are \(^{_{,1}}_{1}=\{s_{1}\}\) and \(^{_{,2}}_{2}=\{s_{2}\}\). States off the optimal path in the Open lists are \(_{1}^{_{,1}}=\{s_{4},s_{5}\}\) and \(_{2}^{_{,2}}=\{s_{4},s_{5},s_{6},s_{7}\}\). Function \(f\) is perfectly ranking problem instance in Figure 0(a) iff following inequalities holds:

\[f(s_{1})<f(s_{4}), f(s_{2})<f(s_{4}), f(s_{2})<f(s_{6}),\] \[f(s_{1})<f(s_{5}), f(s_{2})<f(s_{5}), f(s_{2})<f(s_{7}).\]

Notice that the heuristic values of states on the optimal path are never compared. This is because if the forward search expands only states on the optimal path, then its Open list always contains only one state from the optimal path. The definition anticipates the presence of multiple optimal solutions and even multiple goals, which requires the heuristic to break ties and have a perfect rank with respect to one of them.

**Theorem 1**.: _The forward search with a merit function \(f(s)= g(s)+ h(s)\) and a heuristic \(h\) is strictly optimally efficient on a problem instance \((,,w,s_{0},^{*})\) if and only if \(h\) is a perfect ranking on it._

Proof.: _Sufficiency:_ If the conditions of a perfect ranking heuristic hold, then there exists an optimal path such that a state on it that is in the Open list always has the strictly lowest heuristic value. Therefore, forward search will never move off the optimal path and is, therefore, strictly optimally efficient.

_Necessity:_ If \(h\) is a strictly optimally efficient heuristic, then the forward search always selects the state on the optimal path, which means that the state has the lowest heuristic value of all the states in the Open list, which is precisely the condition of a perfect ranking heuristic. 

Theorem 1, despite being trivial, allows certifying that the forward search with a given heuristic in a given problem instance is strictly optimally efficient. This property has been discussed in  for Beam search, but the conditions stated there are different because Beam search prunes states in the Open list. Recall that the complexity class of computing a perfect ranking heuristic function is the same as solving the problem because one implies the other. We now remind the reader that the popular cost-to-goal does not always lead to a strictly optimally efficient forward search.

**Example 1**.: _While cost-to-goal \(h^{*}\) is the best possible heuristic for algorithms like A* (up to tie-breaking) in terms of nodes being expanded, for GBFS, \(h^{*}\) does not necessarily yield optimal solutions._

See Figure 2 for a counterexample. The complete proof is in the Appendix. Appendix 8.4 also gives an example of a problem instance, where an optimally efficient heuristic for GBFS returning the optimal solution does not exist.

## 4 Loss functions

### Ranking loss function

Let's now design a loss function minimizing the number of violated conditions in Definition 1 for a problem instance \((,s_{0},^{*})\) and its optimal plan \(\). Assuming a heuristic function \(h(s,)\) with parameters \(\), the number of violated conditions can be counted as

\[_{01}(h,,)=_{s_{i}^{*}}_{s_{j} _{i}^{*,i}}[\![r(s_{i},s_{j},)>0]\!],\] (1)

where

\[r(s_{i},s_{j},)=(g(s_{i})-g(s_{j}))+(h(s_{i},)-h(s_{j}, )),\] (2)\(_{i},\,^{n_{i,i}}\) as used in Definition 1, \([\![]\!]\) denotes Iverson bracket being one if the argument is true and zero otherwise, and \(\) and \(\) are parameters controlling the type of search, as introduced above.

In imitation learning, we assume that we have a training set \(^{}\) consisting of tuples of problem instances and optimal plans \(^{}=\{(_{i},_{i},w_{i} ,s_{0,i},_{i}^{*},_{i})\}_{i=1}^{n}\). To optimize parameters \(\) of a heuristic function \(h(s,)\), we propose to minimize the number of wrongly ranked states over all the problem instances in the training set,

\[_{}_{(,)^{}}_{0}(h,,).\] (3)

In practice, one often wants to solve problem (3) by efficient gradient optimization methods. To do so, the Iverson bracket \([\![]\!]\) (also called 0-1 loss) is usually replaced by a convex surrogate such as the hinge-loss or the logistic loss used in the Experimental section below, in which case Equation (3) becomes

\[_{}_{(,)^{}}_{s_{i}^{*}}_{s_{j}^{i}}(1+ (r(s_{i},s_{j},))).\] (4)

While optimization of the surrogate loss might look as if one optimizes a different problem, it has been shown that for certain classes of functions, optimization of surrogate losses solves the original problem (3) as the number of training samples approach infinity [50; 35].

### Regression loss function

For the sake of comparison, we review a loss function used in the majority of works optimizing the cost-to-goal in imitation learning for planning (some notable exceptions are in the related work section). The optimization problem with \(_{2}\) loss function is defined as

\[_{}_{(,)^{}}_{s_{i}^{*}}(h(s_{i},)-h^{*}(s_{i})^{2},\] (5)

where \(h^{*}(s_{i})\) is the true cost-to-goal. The optimal solution of the optimization is the heuristic function \(h^{*}\). What is important for the discussion below is that the \(_{2}\) loss function uses _only states on the optimal path_ and the heuristic function is optimized to solve the _regression_ problem. Other variants of regression loss, such as asymmetric \(_{1}\) in , do not solve issues of regression losses discussed in this paper.

### Advantages and disadvantages of loss functions

Cost-to-goal provides a false sense of optimalityIt has been already mentioned above that A* with \(h^{*}\) is strictly optimally efficient up to resolving states with the same value of \(g(s)+h^{*}(s)\). This means that for problems with a large (even exponential) number of optimal solutions of equal cost (see  for examples), A* will be very inefficient unless some mechanism for breaking ties is used. Moreover, since learning is inherently imprecise, a heuristic close but not equal to \(h^{*}\) will likely be less efficient than the one close to a perfect-ranking heuristic.

Size of the solution setThe set of all functions satisfying Equation (5) is likely smaller than that satisfying Equation (3), because the solution set of (3) is invariant to transformations by _strictly monotone_ functions, unlike the solution set of (5). From that, we conjecture that finding the solution of (5) is more difficult. The precise theorem is difficult to prove since the solution of (5) is not a solution of (3) due to tie resolution in Definition 1.

Optimizing cost-to-goal does not utilize states off the solution pathAssume a problem instance \((,,w,s_{0},^{*})\) and an optimal solution path \(=((s_{0},s_{1}),(s_{1},s_{2}),,(s_{l-1},s_{l}))\). Loss function optimized in (5) uses only states \(\{s_{i}\}_{i=0}^{l}\) on the optimal path. This means that even when the loss is zero, the search can be arbitrarily inefficient if states \(^{}\) off the optimal path will have heuristic values smaller than \(_{s^{*}}h(s)\). The proposed ranking loss (3) uses states on and off the optimal path and therefore, does not suffer from this problem. This issue can be fixed if the training set is extended to contain heuristic values for all states off the optimal path (states \(\{s_{4},s_{5},s_{6},s_{7}\}\) in Figure 0(a)), as has been suggested in [5; 49]. With respect to the above definitions, this is equal to adding more problem instances to the training set sharing the same graph \(\) and set of goal states \(^{*}\), but differing in initial states. Solving all of them to obtain true cost-to-goal _greatly increases the cost of creating the training set_.

Heuristic value for dead-end statesshould be sufficiently high to ensure they are never selected. An infinity in the \(_{2}\) loss would always lead to an infinite gradient in optimization. In practice, the \(\) is replaced by a sufficiently large value, but too large values can cause large gradients of \(_{2},\) which might negatively impact the stability of convergence of gradient-based optimization methods. Proposed ranking losses do not suffer this problem.

Goal-awarenessThe drawback of the heuristic function \(h(s,)\) obtained by optimizing ranking losses is that they are not goal-aware unlike the heuristic optimizing the estimate of cost-to-goal.

Speed of convergence against the size of training setFrom classical bounds on true error  and , we can derive (see Appendix for details) that the excess error of ranking loss converges to zero at a rate \(}}{}},\) which is slightly faster than that of regression \(}}{}-}},\) where \(n_{s}\) and \(n_{p}\) denotes the number of states and state pairs respectively in the training set. But, the number of state pairs in the training set grows quadratically with the number of states; therefore, the excess error of ranking loss for the number of states \(n_{s}\) can be expressed as \(}}{n_{s}},\) which would be by at least \(}}\) factor faster than that of regression. Note though that these rates are illustrative since the independency of samples (and sample pairs) is in practice, violated since even samples from the same problem instance are not independent.

Conflicts in training setUnlike regression loss, the proposed family of ranking losses is potentially sensitive to conflicts in the training set, which occur when the training set contains examples of two (or more) different solution paths of _the same_ problem instance. This might prevent achieving a zero loss. The effect on the heuristic depends on the composition of the training set. Either solution paths more frequent in the training set will be preferred, or there will be ties, but their resolution do not affect optimally efficiency. Appendix 8.5 provides an illustrative example.

## 5 Related Work

The closest work to ours is [36; 38], which adjusts heuristic value by a policy estimated from the neural network. The theory therein has the same motivation, to minimize the number of expanded states, though it is more complicated than the theory presented here. For efficiency,  needs to modify the GBFS such that search nodes store policy and probability of reaching the node from the start. Moreover, in some variants (used in the experimental comparison below) the neural network has to estimate both heuristic value and policy.

Inspired by statistical surveys of heuristic functions in [57; 58],  proposes to optimize the rank of states preserving order defined by the true cost-to-goal. The formulation is therefore valid only for GBFS and not for A* search. Ties are not resolved, which means the strict optimal efficiency cannot be guaranteed. Similarly, heuristic values for domains where all actions have zero cost cannot be optimized. Lastly and importantly, the need to know the true cost-to-goal for all states in the training set greatly increases the cost of its construction as discussed above. From Bellman's equation  arrives at the requirement that the smallest heuristic value of child nodes has to be smaller than that of the parent node. The resulting loss is regularized with \(_{1}\) forcing the value of the heuristic to be close to cost-to-goal.

In , A* is viewed as a Markov Decision Process with the Q-function equal to the number of steps of A* reaching the solution. This detaches the heuristic values from the cost-to-goal cost, but it does not solve the problem with dead-ends and ties, since the optimization is done by minimizing \(_{2}\) loss.

Symmetry of \(_{2}\) (and of \(_{1}\)) loss are discussed in , as it does not promote the admissibility of the heuristic. It suggests asymmetric \(_{1}\) loss with different weights on the left and right parts, but this does not completely solve the problem of estimating cost-to-goal (for example with ties, dead-ends).

Parameters of potential heuristics  are optimized by linear programming for each problem instance to meet admissibility constraints while helping the search. On contrary, this work focuses on the efficiency of the search for many problem instances but cannot guarantee admissibility in general.

Combining neural networks with discrete search algorithms, such that they become an inseparable part of the architecture is proposed in  and . The gradient with respect to parameters has to be propagated through the search algorithm which usually requires both approximation and the search algorithm to be executed during every inference.

A large body of literature  improves the Monte Carlo Tree Search (MCTS), which has the advantage to suppress the noise in estimates of the solution costs. These methods proposed there are exclusive to MCTS and do not apply to variants of the forward search algorithm. Similarly, a lot of works  investigate architectures of neural networks implementing the heuristic function \(h(s,)\), ideally for arbitrary planning problems. We use  to implement \(h(s,)\), but remind our readers that our interest is the loss function minimized when optimizing \(\).

Finally,  proposes to construct a training set similarly to pattern databases  by random backward movements from the goal state and to adapt A* for multi-core processors by expanding \(k>1\) states in each iteration. These improvements are independent of those proposed here and can be combined.

## 6 Experiments

The experimental protocol was designed to compare the loss functions while removing all sources of variability. It therefore uses imitation learning in planning  and leaves more interesting bootstrap protocol for future work . The goal was not to deliver state-of-the-art results, therefore the classical solvers were omitted, but their results are in the Appendix for completeness.

The proposed ranking losses are instantiated with the logistic loss function as in Equation (4) for A* search by setting \(==1\) in (2), called L*, and for GBFS, by setting \(=0\), \(=1\) in (2), called \(}\). \(}\) is used as defined in Equation (5). Since the heuristics used in this paper are not admissible, A* search will not provide optimal solutions. It was included in the comparison to demonstrate that heuristics optimized with respect to \(}\) might be inferior in A* search. To observe the advantage of ranking over regression, we defined a \(}\) loss function

\[}(h,,)=_{(s_{i-1},s_{i})}(1+(h(s_{i}, )-h(s_{i-1},)))\] (6)

comparing only states on the optimal trajectory. We also compare to the loss function proposed in  defined as \(}(h,,)=_{s^{n}}\{1+_{s^{ }(s)}h(s^{},)-h(s,)\;,0\}+\{0,h^{*}( s)-h(s,)\}+.\{0,h(s,)-2h^{*}(s)\},\) where \((s)\) denotes child-nodes of state \(s\). Furthermore, we compare to the policy-guided heuristic search  (denoted as \(}\)) with GBFS modified for efficiency as described in the paper while simultaneously adapting the neural network to provide the policy and heuristic value.

The heuristic function \(h(s,)\) for a given problem domain was optimized on the training set containing problem instances and their solutions, obtained by SymBA* solver  for Sokoban and Maze with teleports and 1 for Sliding Tile. Each minibatch in SGD contained states from exactly a single problem instance needed to calculate the loss. This means that for \(},\,},\,}\), only states on the solution trajectory were included; for \(}\), \(}\), and \(}\) there were additional states of distance one (measured by the number of actions) from states on the solution trajectory. Optimized heuristic functions were always evaluated within A* and GBFS forward searches, such that we can analyze how the heuristic functions specialize for a given search. To demonstrate the generality, experiments contained eight problem domains of two types (grid and general PDDL) using two different architectures of neural networks described below.

Neural network for grid domainsStates in _Sokoban_, _Maze_ with teleports, and _Sliding-puzzle_ can be easily represented in a grid (tensor). The neural network implementing the heuristic function \(h(s,)\) was copied from . Seven convolution layers, \(P_{1} P_{7}\) of the same shape \(3 3\) with 64 filters are followed by four CoAT blocks (CoAT block contains a convolution layer followed by a multi-head attention operation with 2 heads and a positional encoding layer). Convolution layers in CoAT blocks have size \(3 3\) and have 180 filters. The network to this stage preserves the dimension of the input. The output from the last block is flattened by mean pooling along \(x\) and \(y\) coordinates before being passed onto a fully connected layer (FC) predicting the heuristic. All blocks have skip connections.

Neural network for PDDL domainsFor _Blocksworld_, _Ferry_, _Spanner_, _N-Puzzle_, and _Elevators_, where the state cannot be easily represented as a tensor, we have used Strips-HGN  and we refer the reader therein for details. Strips-HGN was implemented in Julia  using PDDL.jl extending . Importantly, we have used the hyper-graph reduplication proposed in  to improve computational efficiency. Since we did not know the precise architecture (unlike in the grid domain), we performed a grid-search over the number of hyper-graph convolutions in \(\{1,2,3\}\), dimensions of filters in \(\{4,8,16\}\), and the use of residual connections between features of vertices. The representation of vertices was reduced by mean and maximum pooling and passed to a two-layer feed-forward layer with hidden dimension equal to the number of filters in hyper-graph convolutions. All non-linearities were of relu type. The best configuration was selected according to the number of solved problems in the validation set. Forward search algorithms were given 5s to solve each problem instance chosen in a manner to emphasize differences between loss functions. All experiments were repeated 3 times. All experiments on the PDDL domains including development took 17724 CPU hours. Optimizing heuristic for a single problem instance took approximately 0.5 CPU/h except Gripper domain, where the training took about 8h due to large branch factors. The rest of this subsection gives further details about domains.

SokobanThe training set for the Sokoban domain contained 20000 mazes of grid size \(10 10\) with a single agent and 3 boxes. The testing set contained 200 mazes of the same size \(10 10\), but with 3, 4, 5, 6, 7 boxes. Assuming the complexity of to be correlated with the number of boxes, we can study generalization to more difficult problems. The problem instances were generated by .

Maze with teleportsThe training set contained 20000 mazes of size \(15 15\) with the agent in the upper-left corner and the goal in the lower-right corner. The testing set contained 200 mazes of size \(50 50\), \(55 55\), and \(60 60\). Therefore, as in the case of Sokoban, the testing set was constructed such that it evaluates the generalization of the heuristic to bigger mazes. The mazes were generated using an open-source maze generator , where walls were randomly broken and 4 pairs of teleports were randomly added to the maze structure.

Sliding puzzleThe training set contained 20000 puzzles of size \(5 5\). The testing set contained 200 mazes of size \(5 5\). These puzzles were taken from . Furthermore, we tested our approach on puzzles of higher dimensions such \(6 6\) and \(7 7\), all of which were generated with . Thus, in total, the testing set contained 200 sliding tile puzzles for each of the three dimensions.

Blocksworld, N-Puzzle, Ferry, Elevators, and SpannerThe problem instances were copied from  (Blocksworld, N-Puzzle, and Ferry) and from  (elevators-00-strips). Problem instances of Spanner were generated by . Blocksworld contained 732 problem instances of size 3-15, N-Puzzle contained 80, Ferry contained 48, Elevators contained 80, and Spanner contained 440. Table 5 in Appendix shows the fraction of -olved mazes by breadth-first search for calibration of the difficulty. The problem instances were randomly divided into training, validation, and testing sets, each containing 50% / 25% / 25% of the whole set of problems. The source code of all experiments is available at https://github.com/aicenter/Optimize-Planning-Heuristics-to-Rank with MIT license.

### Experimental results

Table 1 shows the fraction of solved mazes in percent for all combinations of search algorithms (A* and GBFS) and heuristic functions optimized against compared loss functions. The results match the above theory, as the proposed ranking losses are _always better_ than the \(_{2}\) regression loss.

For A* search, the heuristic function optimized against the \(^{*}\) is clearly the best, as it is only once inferior to \(_{}\) and \(_{}\) on the Ferry domain. As expected, heuristic functions optimized with respect to \(_{}\) behave erratically in A* search, because they were optimized for a very different search.

For GBFS, heuristic functions optimized against the \(^{*}\) or against \(_{}\) behave the best. It can be shown (the proof is provided in Appendix) that since each action has a constant and positive cost, the heuristic function optimizing \(^{*}\) optimizes \(_{}\) as well, but the opposite is not true.

Heuristic functions optimized against \(_{2}\) estimating cost-to-goal deliver most of the time the worst results. Heuristic functions optimized against other ranking losses \(_{}\) and \(_{}\) frequently perform well, most of the time better than \(_{2}\), but sometimes (Elevators, Sokoban, Sliding puzzle) are much worse than proposed losses. These experiments further confirm the [57; 18; 59] stating that optimizing the exact estimate of cost-to-goal is unnecessary, but they also show the advantage of including states off the optimal path in the loss function. Finally, the method combining policy and heuristic in GBFS, \(_{}\),  performs better that \(_{2}\), but worse than the proposed ranking losses. Since both approaches are derived from the same motivation, we attribute this to difficulties in training neural networks with two heads. Ranking losses optimize just the heuristic which is directly used in the search, which seems to us to be simpler. Additional statistics, namely the average number of expanded states (Table 6) and the average length of plans (Table 8) are provided in the Appendix.

## 7 Conclusion

The motivation of this paper stemmed from the observation that even the cost-to-goal, considered to be an optimal heuristic, fails to guarantee a strictly optimally efficient search. Since a large body of existing research optimizes this quantity, we are effectively lost with respect to what should be optimized. To fill this gap, we have stated the necessary and sufficient conditions guaranteeing the forward search to be strictly optimally efficient. These conditions show that the absolute value of the heuristic is not important, but that the ranking of states in the Open list is what controls the efficiency. Ranking can be optimized by minimizing the ranking loss functions, but its concrete implementation needs to correspond to a variant of the forward search. In case of mismatch, the resulting heuristic can perform poorly, which has been demonstrated when the heuristic optimized for BGFS search was used with A* search. The other benefit of ranking losses is that from the point of view of statistical learning theory, they solve a simpler problem than ubiquitous regression in estimating the cost-to-goal.

The experimental comparisons on eight problem domains convincingly support the derived theory. Heuristic functions optimized with respect to ranking loss \(^{*}\) instantiated for A* search perform almost always the best with A* search (except for one case where it was the second best). In the case of GBFS, heuristic functions optimizing ranking losses \(_{}\) and \(^{*}\) instantiated for GBFS and A* worked the best. This is caused by the fact that the heuristic optimizing \(^{*}\) for A* optimizes \(_{}\) for GBFS as well.

We do not question the existence of a strictly optimally efficient heuristic. Given our experiments, we believe that if the space of heuristic functions over which the loss is optimized is sufficiently rich, the result will be sufficiently close to the optimal for the needs of the search.

  &  &  \\   & complx. & \(^{*}\) & \(_{}\) & \(_{}\) & \(_{2}\) & \(_{}\) & \(^{*}\) & \(_{}\) & \(_{}\) & \(_{2}\) & \(_{}\) & \(_{}\) \\  Blocks & & **100** & **100** & **100** & 99 & **100** & **100** & **100** & **100** & **100** & **100** & 99 \\ Ferry & & 98 & 98 & **100** & 92 & **100** & 98 & **100** & **100** & **100** & 98 & 98 \\ N-Puzzle & & **89** & 87 & 88 & 83 & **89** & **92** & 89 & 89 & 89 & **92** & **88** \\ Spanner & & **100** & 89 & **100** & 84 & 92 & **100** & **100** & **100** & **100** & **100** \\ Elevators & & **91** & 85 & 75 & 36 & 66 & **92** & 85 & 79 & 76 & 67 & 58 \\  Sokoban & 3 boxes & **99** & 98 & 96 & 97 & 92 & 98 & **100** & 94 & 95 & 92 & 98 \\  & 4 boxes & **89** & 89 & 85 & 81 & 82 & 87 & **91** & 84 & 83 & 84 & 84 \\  & 5 boxes & **80** & 75 & 72 & 72 & 73 & **78** & 77 & 74 & 72 & 72 & 73 \\  & 6 boxes & **76** & 69 & 59 & 51 & 53 & **73** & 71 & 56 & 51 & 54 & 64 \\  & 7 boxes & **55** & 49 & 47 & 42 & 45 & **51** & 49 & 48 & 43 & 45 & 49 \\  Maze w. t. & \(50 50\) & **92** & 91 & 88 & 87 & 87 & 89 & **90** & 89 & 84 & 85 & 89 \\  & \(55 55\) & **78** & 75 & 73 & 72 & 74 & 74 & **75** & 74 & 72 & **75** & 74 \\  & \(60 60\) & **49** & 37 & 35 & 32 & 31 & 42 & **48** & 36 & 34 & 32 & 42 \\  Sliding puzzle & \(5 5\) & **88** & 83 & 84 & 80 & 82 & 86 & **87** & 84 & 84 & 85 \\  & \(6 6\) & **51** & 48 & 49 & 45 & 46 & 47 & **49** & 45 & 43 & 46 & 48 \\  & \(7 7\) & **39** & 35 & 36 & 32 & 34 & 35 & **36** & 35 & 32 & 34 & 35 \\  

Table 1: Fraction of solved problem instances in percent from the testing set. The top row denotes the type of the search and the second top row denotes the loss function against which the heuristic function was optimized. The complexity is explicitly shown for grid domains. Standard deviations are most of the times smaller than one percent and are provided in Table in the Appendix.