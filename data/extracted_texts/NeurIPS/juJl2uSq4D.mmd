# RL in Latent MDPs is Tractable:

Online Guarantees via Off-Policy Evaluation

 Jeongyeol Kwon

University of Wisconsin-Madison

jeongyeol.kwon@wisc.edu

Shie Mannor

Technion / NVIDIA AI

shie@ee.technion.ac.il

Constantine Caramanis

University of Texas at Austin

constantine@utexas.edu

Yonathan Efroni

Meta AI

jonathan.efroni@gmail.com

###### Abstract

In many real-world decision problems there is partially observed, hidden or latent information that remains fixed throughout an interaction. Such decision problems can be modeled as Latent Markov Decision Processes (LMDPs), where a latent variable is selected at the beginning of an interaction and is not disclosed to the agent. In the last decade, there has been significant progress in solving LMDPs under different structural assumptions. However, for general LMDPs, even in the tabular case, no algorithm is known to provably match the existing lower bound . We introduce the first sample-efficient algorithm for LMDPs without _any additional distributional assumptions_. Our result builds off a new perspective on the role of off-policy evaluation guarantees and coverage coefficients in LMDPs, a perspective, that has been overlooked in the context of exploration in partially observed environments. Specifically, we establish a novel off-policy evaluation lemma and introduce a new coverage coefficient for LMDPs. Then, we show how these can be used to derive near-optimal guarantees of an optimistic exploration algorithm. These results, we believe, can be valuable for a wide range of interactive learning problems beyond LMDPs, and especially, for partially observed environments.

## 1 Introduction

In Reinforcement Learning (RL) , an agent aims to maximize the long-term cumulative rewards through interactions within an _unknown_ environment. Markov Decision Processes (MDPs) are perhaps the most well-studied and popular framework for this goal. As the name suggests, MDPs heavily rely on the Markovian assumption that requires the state to be fully observable. However, many real-world decision problems involve critical partially observed or latent information, such as sensitive or unknown preference information of users in recommendation systems , undiagnosed illness in medical treatments , and adaptation to uninformed tasks in robotics . Even when such latent factors remain fixed throughout a period of interactions the fundamental Markovian property of MDPs is no longer valid.

A line of work has proposed efficient RL algorithms in the presence of latent contexts  within the framework that we here collectively refer to as Latent Markov Decision Processes (LMDP) following . In LMDPs, nature selects an MDP from a finite set of \(M\) candidate MDP models at the beginning of a period of interactions (a.k.a. episode), and an agent interacts with the chosen MDP for \(H\) time steps of an episode (the horizon). However, the identity of the chosen MDP is not given to the agent. We call this unknown identity the _latent context_.

Most prior work on LMDPs has relied on strict separation assumptions (_e.g.,_). The applicability of these approaches is limited to scenarios where the horizon is sufficiently large and identification of the latent model can be guaranteed, _i.e.,_\(H(SA)\), where \(S\) and \(A\) are the state and action spaces size. Without these explicit horizon requirements, as far we know, all existing algorithms suffer _the curse of horizon_, requiring sample complexity \((A^{H})\) - which frequently arises in the more general framework of Partially Observed MDPs (POMDPs) . Without the ability to identify the underlying latent model, it remains unclear how to address the curse of horizon inherent in partially observed systems .

Recently, a series of works  proposed sample efficient algorithms without separation assumptions when \(M=O(1)\), assuming the transition dynamics of models with different latent context is similar. While this is still a substantial contribution, their results cannot be easily extended to the general LMDP setting with different transition dynamics (see Section 1.1). Consequently, to date, the following question has remained open:

_Can we break the curse of horizon in LMDPs if \(M=O(1)\) without any assumptions?_

In this work we provide the first sample-efficient exploration algorithm for LMDPs without any assumptions. Throughout the paper, we assume that \(H>2M\), and focus on whether we can improve the trivial upper bound that incurs complexity \((A^{H})\). Since a \((SA)^{M}\) lower bound for LMDPs has been established , our goal is to achieve an upper bound of \((S,A)^{M}\) without any assumptions, namely, to get a matching upper bound up to polynomial factors.

### Technical Challenges

Many online RL algorithms follow a similar pattern. They make use of a confidence set - a set of candidate models (hypothesis) that can explain the observed data with high probability - and execute a policy that will shrink the volume of the confidence set is produced and executed . The entirety of the statistical problem is to analyze the decaying rate of confidence sets under proper model class assumptions .

Challenge 1: Limitation of Existing POMDP Algorithms.Existing approaches for online exploration in partially observed systems largely fall into the category of Optimistic Maximum Likelihood Estimation (OmLE) . This class of algorithms often requires an assumption that allows the construction of shrinking confidence sets. These algorithms also assume access to a set of special policies - called _core-tests_ - to be executed to generate trajectories . Without specifying the proper core-tests, the volume of confidence sets may not decay in a desired rate, leading to the curse of horizon \((A^{H})\). Further, existing POMDP approaches require an ability to recover the belief of the underlying model from observations, _e.g.,_ by assuming the distribution of observations when executing the core-tests is invertible to the belief over hidden states. Consequently, existing literature on POMDPs has two limitations: (i) it requires to specify _a priori_ a set of core-tests policies, and (ii) it assumes the full-rankness of the state-observation emission matrix when the core-tests are being executed.

While LMDPs are a special class of POMDPs, neither the existence of a set of core-tests is known _a priori_, nor it is possible to recover the belief over latent contexts from distribution of trajectories (see Section B for details). This creates a fundamental challenge for existing approaches when applied to LMDPs. Further, little is understood on learning a near-optimal policy among "doubly-exponential" number of candidate history-dependent policies without either the visibility of contexts or core-tests. This calls for a new perspective on the question of efficient exploration in LMDPs.

Challenge 2: Limitation of Existing LMDP Algorithms.The work of  suggested an alternative strategy to learn a near-optimal policy in LMDPs: the moment-matching approach for exploration in LMDPs. When all contexts share the same state-transition dynamics, the notion of moments can be defined as the joint distribution of rewards under _a fixed prior_ at a tuple of at most \(d:=2M-1\) state-action pairs \(=(s_{},a_{}),...,(s_{[d]},a_{[d]})\). This in turn suggests that the exploration algorithm must learn how to visit these length-\(d\) state-action tuples simultaneously, _i.e.,_ find a policy that ensures that \(\) appears as a subsequence of the entire trajectory with high enough probability.

When the transition dynamics of different latent contexts is similar, reaching optimally to \(d\) state-action pairs is a minor challenge; _e.g.,_ we can first learn the shared transition kernel with any reward-freeexploration scheme for MDPs , and then execute the policy that maximizes the probability of reaching the \(d\) state-action pairs. However, for general LMDP, when the transition dynamics of different latent contexts may differ, this approach is no longer available since the latent transition dynamics may not be learnable in general. Furthermore, to follow the notion of moments suggested in , the data collected for estimating the correlation tensor must be collected under the same prior (belief) over all latent contexts. Unfortunately, ensuring this for general LMDPs, when the transition dynamics of different latent contexts are not equal, is impossible, since even if we obtain the samples of correlations, different policies may result in different and unknown priors over contexts. These challenges hint we need an alternative approach to solve general LMDPs, when the transition dynamics vary between latent contexts.

Challenge 3: Limitations of Existing Complexity Measures in RL.Numerous studies have examined complexity measures for RL with function approximation or in the rich-observation settings [30; 34; 19]. These studies are based on the Markovian assumption, which does not hold in the LMDP setting where the entire history may be needed to decode the latent state. When defining the effective state as the entire history at each time step, it is unclear how to analyze the complexity measures from these studies without resorting to exponential guarantees in the horizon.

### Overview of Our Contribution

Recent studies have found some fundamental connections between off-policy evaluation (OPE) and online exploration in RL [59; 2; 29; 9; 55; 3; 4]. In this work, we offer a fresh viewpoint, which deviates from existing works, on the connection between OPE and online exploration. This perspective, together with new analysis tools, allows us to provide a sample-efficient algorithm for the LMDP setting. This further showcases the usefulness of OPE for online exploration in POMDPs.

Arguably, the fundamental question in OPE is the following: how much does a behavioral policy \(\) tell about a target policy \(\)? The simplest form of the OPE guarantee in MDPs relies on the notion of _coverage coefficient_ given by:

\[C(;)=_{s,a,t}^{}(s_{t}=s,a_{t}=a)}{^{ }(s_{t}=s,a_{t}=a)}.\]

How would this quantity be related to online exploration? A key observation to start developing intuition is the following: an unbounded coverage coefficient, _i.e._, \(C(;)=\) implies there exists a state-action pair, at some time-steps, that cannot be reached under \(\), but can be reached with \(\).

The algorithmic framework we develop in this work builds off OME. In Section 3, we consider the MDP setting to provide intuition of our analysis. There, OME iteratively tests new policies on models from the confidence set which predict different outcomes, until the trajectory distribution of all policies is reliably estimated. Since the number of new state-action pairs is bounded for MDPs, the number of times the coverage coefficient can be large must be bounded during an interaction. We provide new analysis for the MDP setting based on OPE tools.

To apply this approach for LMDPs, we are required to develop a new notion of coverage coefficient and new OPE tools. We propose a coverage coefficient that can be informally described as follows:

\[C(;)=_{(,)}_{m}^{}(  m)}{^{}( m,\ \ )}\]

Figure 1: Highlevel description of LMDP-OME. In the online phase, we find a new test policy under which models in the confidence set do not agree. Then the exploration policy is constructed with our new notion of _segmentation_ of policies within \(_{}\) that are executed throughout. In the offline phase, we add the batched sample trajectories to dataset and update the confidence set of models.

where \(m\) is the unobserved latent context, \(=(s_{t},a_{t},r_{t})_{t[H]}\) is a sampled trajectory, \(\) is an event of interest, _e.g.,_ visiting length of at most \(d\) tuples of states and actions within an episode, and \(\) is an intervention of interest, _e.g.,_ force an action \(a\) at the \(t^{th}\) time step regardless of \(\) (for the formal definition, see Definition 4.1). Note that the coverage coefficient cannot be measured explicitly, since \(m\) is a latent variable; nevertheless, this concept is central to our analysis and our ability to analyze the sample complexity of the proposed algorithm. Its usefulness lies in an OPE guarantee we develop (see Lemma 4.2):

\[(_{^{*}}^{},_{}^{})() C(;)_{}(_{^{ *}}^{},_{}^{})(),\]

where \((_{1},_{2})()\) is the total-variation (TV) distance between two probability measures \(_{1},_{2}\).

With these tools at hand, we design an iterative online exploration algorithm for the LMDP setting, and prove its sample complexity matches the lower bound, up to polynomial factors. The algorithm, we refer as LMDP-OmLE (see Figure 1 for highlevel illustration), repeats the following: _(i)_ find a policy for which the trajectory distributions between models in the confidence set is large or terminate, or _(ii)_ collect new data with exploration policies constructed with a set of (obtained) test policies and interventions, an exploration strategy for LMDPs that we introduce.

## 2 Preliminaries

We consider an episodic RL with time-horizon \(H\) in LMDPs defined as follows:

**Definition 2.1** (Latent Markov Decision Process (LMDP)): _An LMDP \(\) consists of a tuple \((,,,,H)\) with a state space \(\); action space \(\); reward space \(\), and a finite-time horizon \(H\). \(\) is a model parameter consisting of multiple MDPs in the model \(:=(\{w_{m},T_{m},R_{m}\})_{m=1}^{M}\). In each \(m^{th}\) MDP, \(T_{m}:\) maps a state-action pair and a next state to a probability; \(R_{m}:\) is a probability of rewards; \(\{w_{m}\}_{m=1}^{M}\) are the mixing weights such that at the beginning of every episode the \(m^{th}\) model is chosen with probability \(w_{m}\)._

Without loss of generality, we assume that there exists a null state that represents the starting and terminal state \(s_{0}=s_{H+1}=\), and a null action at the beginning of an episode \(a_{0}=\), even though actual policies do not take any action at the beginning. \(T_{m}(|s_{0},a_{0})\) is the initial state distribution of the \(m^{th}\) MDP. We assume that the number of latent contexts is constant \(M=O(1)\), and the time-horizon is larger than the number of contexts \(H>2M\). Further, we assume the reward values are finite and bounded:

**Assumption 2.2** (Finite and Bounded Reward): _The reward distribution has finite support with (arbitrarily large) cardinality, and each reward is bounded: \(|r| 1\) for all \(r\)._

We also note that this concept can be easily generalized to instantaneous observations that include rewards, and thus, we do not lose much generality due to Assumption 2.2. We consider a policy class \(\) which contains all history-dependent policies \(:(,,)^{*}( [H])()\), where \(\) is the space of independent variables decided at the beginning of execution. As a special case, we consider the class of memoryless policies: \(_{}:([H])()\) We are interested in finding an optimal history dependent policy \(\) that maximizes the expected reward: \(V_{^{*}}^{*}:=_{}_{^{*}}^{}[_{ t=1}^{H}r_{t}],\) where \(^{*}\) is the true model parameter and \(_{^{*}}^{}[]\) is expectation taken over the true LMDP model \(^{*}\) when policy \(\) is executed.

NotationWe use \([n]:=\{1,,n\}\) and \([n]_{+}:=\{0\}[n]\). We define \(d:=2M-1\) and assume \(H>2M\). Let \((H,d)\) be the set of subsequences of \((1,2,...,H)\) with length less than or equal to \(d\), _i.e.,_\((H,d):=\{(_{1},_{2},...,_{q})|q[d],1_{1}<...<_{q} H\}\). We often denote a state-action pair \((s,a)\) as one symbol \(x=(s,a)=()\), and an reward-next state pair \((r,s^{})\) as one symbol \(y=(r,s^{})=()\). We often express the next state at time step \(t\) as either \(s_{t+1}\) or \(s^{}_{t}\), and the pair of instantaneous observation and next state as \(y_{t}=(r_{t},s_{t+1})=(r_{t},s^{}_{t})\). For any segment of a sequence \((z_{1},z_{2},...,z_{H})\) from \(t_{1}\) to \(t_{2}\), we often simplify the notation as \(z_{t_{1}:t_{2}}\). We denote the entire trajectory as \(:=(s,a,r)_{1:H}\), and \(_{1:t}=((s,a,r)_{1:t-1},s_{t})\) for a history of length \(t\). For any set \(\), we define \(^{ k}\) as a short-hand for the \(k\)-times Cartesian power of \(\)```
1:Input:\(n_{},,_{},>0\), \(^{0}=\)
2:Initialize \(k=0\)
3:while there exists \(^{k}_{}\), and \(_{1},_{2}^{k}\) such that \((_{_{1}}^{^{k}},_{_{2}}^{ ^{k}})()>4_{}\)do
4: Generate data \(\{^{k}_{j}\}_{j=1}^{n_{}}\) by executing \(^{k}\), update \(^{k}^{k-1}\{(^{k}_{j},^{k}) \}_{j=1}^{n_{}}\)
5: Refine the confidence set with the dataset: \[^{k+1}=_{(,) ^{k}}_{}^{}()_{ }_{(,)^{k}}_{ }^{}()-}\] (1) \(k k+1\)
6:endwhile
7:Pick any \(^{k}\) and return the optimal policy of \(:=(,,,)\).
```

**Algorithm 1**MDP-OmLE

We define \((,)() ^{||}\) as a valid subsequence of trajectories at time-steps \((H,d)\), _i.e._, if \((x_{},y_{})(,)\), for any \(i\) such that \(_{i}=_{i+1}\), \(y_{_{i}}=(r_{_{i}},s^{}_{_{i}})\) and \(x_{_{i+1}}=(s_{_{i+1}},a_{_{i+1}})\) must have \(s^{}_{_{i}}=s_{_{i+1}}\).

For a tuple of state-action pairs (or states) of length \(q\), we denote \(=(x_{},...,x_{[q]})\) (or \(=(s_{},...,s_{[q]})\) with bracketed indices for each element to distinguish from time steps. We use \(||\) for the length of sequence \(\). We denote the cardinality of the state and action space as \(S:=||\) and \(A:=||\). For any two models \(_{1},_{2}\), we often denote \(_{1}():=_{_{1}}()\) and \(_{2}():=_{_{2}}()\) whenever the context is clear. We denote \(P_{m}()\) for a probability measured conditioned on the context \(m[M]\) over the ground-truth model (\(_{1}\) when we compare \(_{1}\) and \(_{2}\)). We denote \(()\) as the uniform distribution over a set \(\). Let \((_{1},_{2})(X)\) be the total-variation distance between two probability measures \(_{1}(),_{2}()\) over a random variable \(X\).

## 3 New Perspective on \(\): Online Guarantees via Off-Policy Evaluation

In this section, we present our new approach for analyzing the \(\) algorithm, and, for establishing intuition in the Markovian setting. Differently than prior analysis [45; 46] which is based on the generalized eluder-type condition assumption (see , Condition 3.2), we show that a certain type of an OPE guarantee can be used to study the performance of \(\). This alternative perspective is instrumental in designing a sample-efficient algorithm for the LMDP class.

Consider \(\) depicted in Algorithm 1. \(\) is an adaptation of \(\) for the MDP setting with the goal of learning a near-optimal policy. The algorithm iteratively refines the confidence set, i.e., the set of statistically valid models, until it terminates. Specifically, it iteratively repeats the two steps: _(i)_ find a policy for which the TV distance between trajectory distributions of models in the confidence set is sufficiently large, and _(ii)_ collect data with that policy, and use the data to refine the confidence set. To bound the sample complexity of the algorithm we attempt to upper bound the number of iterations, namely, to bound the number of times the TV distance between trajectory distributions can be sufficiently large.

The following OPE lemma is a tool that allows us to bound the number of iterations of \(\). Before discussing its application, we present the result.

**Lemma 3.1** (**Tv Bound via OPE for MDPs**): _For any behavioral and target policies \(,\), let the coverage coefficient be defined by:_

\[C(;)=_{t[H]}_{x}_{^{*}}^ {}(x_{t}=x)}{_{^{*}}^{}(x_{t}=x)}. \]

_For any two models \(,^{*}\), the TV distance between trajectory distributions following a target policy \(\) is bounded as follows:_

\[(_{^{*}}^{},_{}^{})() 2C(;)_{t[H]}(_{^{*}}^{ },_{}^{})(x_{t},y_{t}). \]

How can we use this result to bound the number of iterations of \(\)? Consider the infinite sample regime, when \(\) collects infinite data at each iteration by executing a policy \(^{k}\) on the \(k\)th iteration, _i.e.,_\(n_{}=\). Further, assume the algorithm is at the beginning of its \(k+1\) iteration. In the infinite sample regime all models in the confidence set must have matching event distribution relatively to the underlying model measured when policy \(^{k}\) is tested. Specifically, for all \(^{k}\) and \(t[H]\) it holds that \((^{^{k}}_{^{*}},^{^{k}}_{})=0\). Then Lemma 3.1 implies the following: for all policies \(\) for which \(C(^{k};)<\) it also holds that \((^{}_{^{*}},^{}_{})()=0\). Conversely, assume the condition of the while loop at the beginning of the \(k+1\) iteration holds true, namely, there exists a policy \(\) for which \((^{}_{^{*}},^{}_{ })()>0\). Then Lemma 3.1 also implies that \(C(^{k};)=\), namely, there exists an \(x\) and \(t[H]\) such that \(^{}_{^{*}}(x_{t}=x)>0\) whereas \(^{^{k}}_{^{*}}(x_{t}=x)=0\). Next, recall that \(O\!MLE}\) sets the data collection policy at the \(k+1\) iteration to be \(^{k+1}=\). Hence, the data collection policy at the \(k+1\) iteration will visit some state-action pair at some time step \(^{k}\) did not visit. Hence, in the infinite sample regime, \(O\!MLE}\) halts after at most \(HSA\) iterations, as there are at most \(HSA\) different state-action pairs in different time steps.

The intuition presented above is robust to sampling error, _i.e.,_ when \(n_{}<\). To simplify the discussion, let us temporarily assume that \(^{}_{^{*}}(x_{t}=x)>\) for all \(\) and \(x\) (we do not require this assumption in our final result by analyzing a perturbed MDP). The key intuition on which the finite sample analysis builds upon is formalized in the following lemma:

**Lemma 3.2** (**Coverage Multiplicative Increase**): _For all \(k>0\) in Algorithm 1, there exists at least one \(t[H]\) and \(x\) such that_

\[^{^{k}}_{^{*}}(x_{t}=x) c}}{H}}}{(HSA)}}_{j<k} ^{^{j}}_{^{*}}(x_{t}=x).\]

_with some absolute constant \(c>0\)._

Therefore, by setting the number of samples to be \(n_{}(4H^{2}SA)/(c_{})^{2}\), we ensure that in every iteration \(O\!MLE}\) doubles the coverage of at least one state-action pair at a certain time step. Therefore, the algorithm terminates within at most \(K=O(HSA(1/))\) iterations with high probability. After termination, we are guaranteed that any two models in the confidence set are \(_{}\)-close in TV-distance for any policy, hence we can obtain \(=(H_{})\)-optimal policy. To summarize, we state the following theorem:

**Theorem 3.3**: _Let \(K=O(HSA)(HSA/)\) and \(=(K||/)\). Then, with probability at least, \(1-\), \(O\!MLE}\) terminates after \(K\) iterations with at most \(N\) episodes being generated, where_

\[N O(H^{6}S^{2}A^{2})(HSA/)(K||/)/^{ 2},\]

_and outputs an \(\)-optimal policy with probability at least \(1-\)._

In a typical tabular MDP setting, we take \(O(||)=(SA)\), by discretizing the class of MDPs. Hence the sample complexity of \(O\!MLE}\) is \(N=(H^{6}S^{3}A^{3}/^{2})\). While this upper bound is suboptimal compared to the minimax rate , the appeal of this type of analysis is its ability to bypass the need for analyzing the decaying volume of the constructed confidence sets (Section 1.1, Challenge 1).

## 4 Efficient Exploration in LMDPs

In previous section we presented a new approach to analyze the \(\) algorithm for MDPs. Next, we develop an analogous technique for the LMDP setting and design the \(O\!MLE}\) algorithm. Central to its design and analysis is an OPE lemma and a new coverage coefficient which we now present.

Intuition from moment-exploration algorithm in .Before we dive into our key results, let us provide our intuition on how we construct the OPE lemma for LMDPs. Our construction is inspired by the moment-exploration algorithm proposed in : when state-transition dynamics are identical across latent contexts, _i.e.,_\(T_{1}=T_{2}=...=T_{M}\), we can first learn the transition dynamics with any reward-free type exploration scheme for MDPs , and then set the exploration policy that sufficiently visits some tuples of state-actions \(\) of length at most \(d\). Specifically, they set a memorlyess exploration policy \(_{}\) which sets \(^{}(x_{}=)\) sufficiently large for some \((H,d)\)and \(^{||}\). We note that the same moment-exploration strategy cannot be applied to general LMDPs with different state-transition dynamics since learning the transition dynamics itself involves latent contexts. Nevertheless, the intuition from  suggests that our key statistics are this visitation probabilities to all tuples of state-actions within a trajectory.

### Off-Policy Evaluation in LMDPs

The OPE lemma we derive in this section makes use of a behavior policy of a special form which we refer to as a _segmented policy_, inspired by the notion of moment-exploration in . Let us formally define the key quantities to establish our OPE lemma. A segmented policy, which we denote by \((;,)\), takes as an input a sequence of history-dependent policies, \(=(_{0},...,_{d})\), a sequence of time steps, we call checkpoints, \(=(_{1},...,_{||})(H,d)\), and a sequence of binary numbers \(=(z_{1},...,z_{||})\{0,1\}^{||}\) where \(|| d\), and returns a history-dependent policy.

The segmented policy \((;,)\) switches sequentially between different policies in \(\). The time steps in which the switch occurs are determined by \(\): starting from time step \(_{i}+1\) policy \(_{i}\) will be executed. Finally, the sequence \(\) determines whether an intervention with a random action will occur at the \(_{i}\) time-step. If \(z_{i}=1\) the executed action at time step \(_{i}\) is the uniform action, \(()\), and, otherwise, the policy \(_{i-1}\) is executed. The segmented policy is also denoted by

\[(;,):=_{0}_{(_{1},z_{1})} _{1}_{(_{2},z_{2})}..._{(_{||},z_ {||})}_{||},\]

where "\(_{a}_{(t,z)}_{b}\)" means switch to policy \(_{b}\) at starting from time step \(t+1\), and at time step \(t\) take random action if \(z=1\) and otherwise execute \(_{a}\).

We are now ready to define a coverage coefficient for the LMDP class of models. This new coverage coefficient is central to the analysis and design of LMDP-OmLE.

**Definition 4.1** (LMDP Coverage Coefficient): _The LMDP coverage coefficient of a sequence of policies \(^{(d+1)}\) with respect to a target policy \(\) in is given by:_

\[C(;):=_{(H,d)}_{\{0,1 \}^{||}}_{(,) (,)}_{m[M]}^{}(x_{} =,y_{}=)}{P_{m}^{(;,)}(x_{ {}}=,y_{}=)}. \]

The LMDP coverage coefficient \(C(;)\) between a sequence of policies, \(\), and a target, history-dependent, policy \(\), depends on the worst-case way to generate a segmented policy, \((;,)\) from \(\). Further, it is a worst-case ratio of the probability of a sequence of observations within \(||=d\) different time steps, namely, \(x_{},y_{}\). This is different than the standard coverage coefficient (see equation (2)), that depends on observation from a single time. Fortunately, \(C(;)\) requires only a partial set of observations, instead of using full trajectories. This is crucial towards developingsample complexity guarantees that are not exponential in \(H\). Lastly, observe that the LMDP coverage coefficient depends on the latent context \(m\), and thus, we cannot measure \(C(;)\) from samples.

We are now ready to provide the key OPE lemma, which makes use of the LMDP coverage coefficient.

**Lemma 4.2** (TV Bound via OPE for LMDPs): _Let \(d=2M-1\). For any two models \(,^{*}\), and for any \(\) and \(^{(d+1)}\), let \(C(;)\) be defined as (4) over \(^{*}\). Then the following holds:_

\[(^{}_{^{*}},^{}_{})() M C(;)_{(H,d)}_{ \{0,1\}^{}}(^{(;,)}_{^{*}},^{(;,)}_{})(x_{},y_{}).\]

This result is analogous to the OPE result for MDPs (see Lemma 3.1). It is a tool that allows us to bound the TV distance between trajectory distributions of a history-dependent policy \(\) by a term that depends on a segmented policy \((;,)\) and an LMDP coverage coefficient. Importantly, the term on the RHS that depends on the segmented policy, \((;,)\), is a sum of distributions of partial trajectories of size \(|| d\), which is independent of the horizon length, \(H\).

**Remark 4.3** (Why is single latent-state coverability coefficient not enough?): _One may wonder why it is not sufficient to consider a single latent-state coverability analogous to Lemma 3.1, namely an analogous to (2) defined as:_

\[_{t[H]}_{x}_{m[M]}_{m}(x_{t}=x)}{ P^{}_{m}(x_{t}=x)}.\]

_In Appendix D.5 we provide a counter-example where such single latent-state coverage coefficient is finite, and yet, off-policy evaluation guarantee cannot be established._

### Coverage Doubling via Sufficiency of Memoryless Polices

To convert the OPE guarantee to an online exploration algorithm, we aim to use the coverage-doubling argument. Ideally, we could apply the coverage-doubling argument with the general policy class similarly to the MDP case as presented in Section 3. However, in its current form, Lemma 4.2 requires the behavior policy to be a segmented policy, and is not valid for any general behavioral policy. Hence, it is not obvious on which probabilistic events we can apply the coverage doubling argument. We leave it as future work whether we can obtain an off-policy evaluation lemma with general history-dependent behavioral policies, and its clearer conversion to online guarantees.

In this work, we present an alternative plan to the above issue: we reduce the search space from history-dependent policies to memoryless policies. This allows us to track quantities on a _segmentwise_ coverage. Specifically, we first note that the LMDP coverage coefficient can be bounded (after maximizing over the sequence \(\)) by:

\[C(;)_{(H,d)}_{ ^{||}\\ ^{}^{||-1}}_{m [M]}_{i=0}^{d-1}_{1:_{i}}}P^{}_{m}(s_{ _{i+1}}=s_{[i+1]}|s^{}_{_{i}}=s^{}_{[i]},_{1: _{i}})}{(1/A) P^{(_{i};_{i})}_{m}(s_{_{i+1}}=s_{[i+1]} |s^{}_{_{i}}=s^{}_{[i]})}, \]

where \((_{i};_{i})\) denotes a segmented policy executing \(_{i}\) after the \(_{i}^{th}\) time-step with memory reset, hence ignoring the history up to \(_{i}\) (the conditioning event \(s^{}_{_{0}}=s^{}_{}\) at \(i=0\) can be ignored). Inspired by the form in denominator, we aim to double the following probability defined over a _context-segment_ pair:

\[_{_{}}P^{(;_{1})}_{m}(s_{t_{2}}=s|s^{ }_{t_{1}}=s^{}), \]

for at least one \(m[M],s,s^{}\) and \(t_{1}<t_{2}\). However, another challenge remains: the RHS in equation (5) consists of the maximum over all possible histories in the numerator, whereas in the denominator we force the data collection policy to reset the memory at checkpoints. We still have to side-step this discrepancy to apply the coverage doubling argument.

The restriction to the class of memoryless policies allows us to resolve these issues since

\[_{_{1:t_{1}}}P^{}_{m}(s_{t_{2}}=s|s^{}_{t_{1}}=s^{ },_{1:t_{1}})=P^{}_{m}(s_{t_{2}}=s|s^{}_{t_{1}}=s^{ }),\]\[\ P_{m}^{(;t_{1})}(s_{t_{2}}=s|s^{}_{t_{1}}=s^{})=P_{m} ^{}(s_{t_{2}}=s|s^{}_{t_{1}}=s^{}),\]

if \(,_{}\) since \(P_{m}\) represents the latent Markovian transition dynamics. Thus, we can aim to double up the quantity in equation (6). To apply this argument, we establish our second key lemma, a crucial building block for the coverage doubling argument:

**Lemma 4.4** (Sufficiency of Memoryless Polices for LMDPs): _Suppose the following holds:_

\[_{_{}_{}}\!\!V(_{ ^{*}}^{_{}},_{}^{_{}})( )_{}. \]

_Then for any history-dependent policies \(\), the following holds:_

\[\!\!V(_{1}^{},_{2}^{})() M (2H^{2})^{d}(MSA)^{d}_{}.\]

Therefore, we reduced our goal of learning an optimal policy to finding a set of models which satisfies equation (7) with respect to all memoryless policies. Importantly, upon estimating the trajectory distribution up to accuracy \(_{}>0\) for memoryless policies, we have a bounded TV distance between the trajectory distribution of all history-dependent policies, includes the optimal policy.

### The Lmdp-omle Algorithm

Once the search space is reduced to memoryless policies, we aim to match trajectory distributions for all memoryless policies. At a high-level, LMDP-omle follows similar recipe to MDP-omle, and is summarized in Algorithm 2. It can be described as follows:

1. Find a memoryless policy \(^{k}_{}\) whose prediction on trajectory distributions does not match between two models in the confidence set \(^{k}\). Add \(^{k}\) to the collection of test policies \(^{k}_{}\), that forms each segment of (segmented) exploration policies.
2. Collect new sample trajectories following the new set of segmented policies for exploration, generated by different combinations of collected test policies and switching operations.
3. Update the confidence set \(^{k}\) with Maximum Likelihood Estimation (MLE) on the updated dataset \(^{k}\) by equation (1).

The data collection policy of LMDP-omle (second step above) is inspired and leverages Lemma 4.2 to give upper bounds on the TV distance of untested policies. Next, by Lemma 4.4, we know that when the while loop terminates, any optimal policy of a model contained in the confidence set is a near-optimal policy of the underlying LMDP. We conclude this section with our main theorem on the sample complexity of learning the optimal policy in Latent MDPs:

**Theorem 4.5** (Sample Complexity of Lmdp-omle): _Let \(d=2M-1\) and assume \(H>2M\). After at most \(K=O(MS^{2}H)(MSAH/)\) iterations, LMDP-omle (Algorithm 2) terminates with at most \(N\) episodes being generated where_

\[N(M^{4}S^{6}A^{4}H^{7}(MSAH/))^{d} M^ {4}H^{2}(K||/)/^{2},\]

_and outputs an \(\)-optimal policy with probability at least \(1-\)._

Note that in tabular LMDPs with finite support rewards, we have \((||)=O(S^{2}A||(1/))\). The appeal of \((||)\) dependence is an flexible extension of the same result to parameterized reward distributions. In Section D.1 and D.4 we provide a proof overview and a full proof of Theorem 4.5.

## 5 Conclusion and Future Work

In this work, we presented the first sample-efficient algorithm for LMDPs, resolving an open question of efficient exploration with latent contexts. While our result is specialized to LMDPs, we believe our new perspectives and techniques on deriving online guarantees through the lens of OPE can be useful for a broader range of interactive learning, and, especially, partially observed problems. While resolving the open problem, there are a few remaining questions for the LMDP setting.

Tightness of the Result.The upper bound in Theorem 4.5 scales with \((MSAH(1/))^{O(M)}\), while the existing lower bound is \((SA)^{M}\). Closing this polynomial gap in the exponent, and having a matching upper and lower bounds can be valuable for a deeper understanding of LMDPs and possibly for POMDPs in general.

General OPE lemma and Regret Guarantees for LMDPs.The OPE lemma derived in this work (Lemma 4.2) assumes the behavior policy is a segmented policy with intervention at different checkpoints. While this result allows us to provide guarantees on LMDP-OmLE and prove it learns a near-optimal policy, this result is restrictive, in that it does not provide general guarantees for OPE nor makes it possible to derive regret guarantees. In particular, can we evaluate \(\) without policy-switching or intervention when the behavioral policy is a generic history-dependent policy \(\)? Further, is there an algorithm with provable \((S,A)^{M}\) regret for the general LMDP setting?

Towards Practical Settings.Our result gives a worst-case guarantee. Yet, practical instances may be much simpler under different set of assumptions _e.g.,_ with provided side-information  or additional structural assumptions . Deriving new conditions can be of great importance for real-world applications, _e.g.,_ there could be more practical notion of separation, or the set of instances that allows the notion of coverage-coefficient with a (significantly) shorter length \(d=o(M)\) of state-action tuples. Further, developing practical RL methodologies for the LMDP setting remains an unexplored challenge with significant importance for numerous applications. These are remained to be explored in future works.