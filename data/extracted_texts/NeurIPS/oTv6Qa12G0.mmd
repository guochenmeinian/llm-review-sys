# A Theoretical design of Concept Sets: improving the predictability of concept bottleneck models

Max Ruiz Luyten

University of Cambridge

Cambridge CB3 0WA

mr971@cam.ac.uk

&Mihaela van der Schaar

University of Cambridge

Cambridge CB3 0WA

mv472@cam.ac.uk

###### Abstract

Concept-based learning, a promising approach in machine learning, emphasizes the value of high-level representations called concepts. However, despite growing interest in concept-bottleneck models (CBMs), there is a lack of clear understanding regarding the properties of concept sets and their impact on model performance. In this work, we define concepts within the machine learning context, highlighting their core properties: _expressiveness_ and _model-aware inductive bias_, and we make explicit the underlying assumption of CBMs. We establish theoretical results for concept-bottleneck models (CBMs), revealing how these properties guide the design of concept sets that optimize model performance. Specifically, we demonstrate that well-chosen concept sets can improve sample efficiency and out-of-distribution robustness in the appropriate regimes. Based on these insights, we propose a method to effectively identify informative and non-redundant concepts. We validate our approach with experiments on CIFAR-10 and MetaShift, showing that concept-bottleneck models outperform the foundational embedding counterpart, particularly in low-data regimes and under distribution shifts. We also examine failure modes and discuss how they can be tackled.

## 1 Introduction

Concept Bottleneck Models (CBMs)  offer a compelling trifecta of interpretability , intervenability, and predictability . They are inspired by the power of concepts: the fundamental building blocks of human thought that guide our learning, decision-making, inference, and communication . In a nutshell, CBMs force these high-level representations to inform model predictions. For example, rather than directly predicting a car's action from raw images, a CBM first identifies key concepts such as the presence of a semaphore or a pedestrian crossing the road (\(C:\)), and learns \(:\) to act based on them \(( C)(x)\). Indeed, despite never having seen a Tesla Model X, one could reasonably classify them provided they can identify an SUV, a T logo, gull-wing doors, and the absence of exhaust pipes. This paradigm has found applications across various domains, including automated driving , fine segmentation , and clinical imaging [39; 22].

Despite their promise, the predictability of CBMs, particularly in relation to the properties of the concept set \(\), remains poorly understood. A deeper understanding of how the composition and characteristics of concept sets affect CBM performance is essential to enhance their practical utility and robustness in real-world applications.

In this paper, we address this gap by systematically investigating the properties of concept sets \(=\{c_{1},,c_{m}\}\) and their influence on model performance. We answer the following key questions:1. **How do characteristics of the concept set impact the performance of the CBM?** We identify the size of the concept set \(||\) and the degree of misspecification \(\) as primary drivers of the performance of the CBM, where \(:=||_{f}(f)-_{g _{}}(g C)||\), with \(\{f:\},_{} \{g:\}\) the hypotheses spaces.
2. **What are the optimal conditions for the effectiveness of CBMs?** We find that CBMs are particularly effective in small data regimes and under distribution shifts.

To address these questions, we introduce in Section 3 a theoretical framework that links the properties of concept sets to CBM performance metrics. This framework identifies two core _desiderata_ for concept sets: _expressiveness_ and _model-aware inductive bias_. _Expressiveness_ ensures each concept captures valuable information from inputs, benefiting the expected loss \(( C)\) as the sample size \(n\). _Model-aware inductive bias_ implies a beneficial inductive bias, improving performance in low-sample regimes (\(n 0\)). These properties highlight an implicit assumption in CBMs about the alignment between model outputs and human inductive biases.

In Section 4, we dive into a theoretical analysis of CBMs. Our setting aligns with the recent trend of concept bottleneck models built on multimodal foundational models such as CLIP [30; 39; 41]. We derive a key theorem (Theorem 1) that compares the expected loss of concept-based models (\(( C)\), \(_{g_{}}(g C)\)) with the baseline counterpart (\((),_{f}(f)\)). The theorem demonstrates that concept-based models can outperform baseline models if the concept set is well chosen by minimizing the size of the concept set \(||\) while controlling the misspecification term \(\). This theoretical insight highlights the importance of selecting informative and non-redundant concepts, as proper concept selection can significantly affect sample efficiency and generalization performance.

Based on these insights, we propose a method in Section 5 to generate informative and non-redundant concept sets. We also discuss potential failure modes of CBMs and how existing approaches can mitigate them. Finally, we empirically validate our framework in Section 6 with experiments on CIFAR-10 and MetaShift datasets, showing that appropriately designed CBMs outperform traditional models, particularly in low data and distribution shift scenarios.

Contributions.

1. **Theoretical Framework for Concept Sets**: We introduce a novel theoretical framework for CBMs that elucidates the properties of concept sets, specifically focusing on _expressiveness_ and _model-aware inductive bias_. This framework formalizes how these properties influence model performance, offering new insights into the design of effective concept sets.
2. **Theoretical Analysis and Insights**: We derive theoretical results that compare the expected loss of CBMs with their non-bottleneck counterparts. Our analysis reveals the conditions under which CBMs are most effective, particularly in low-data regimes and under distribution shifts. These results provide a clear understanding of the design choices that enhance CBM performance.
3. **Empirical validation**: We validate our results through experiments on CIFAR-10 and MetaShift datasets, designed to assess different aspects of concept sets and CBMs, and we affirm that appropriately designed concept sets can lead to increased model predictability and efficiency.

## 2 Related Work

Incorporation of human concepts into machine learning algorithms has been considered for a long time as a strategy to improve the explainability of the model through improved representation interpretability [21; 27; 20; 38]. Concept Bottleneck Models (CBMs)  are one of the most promising approaches to concept-based methods and have been adopted in various applications such as fine classification  and medical image analysis [39; 22].

Advances in concept identificationRecent work has focused on increasing the flexibility of CBMs. For example, [30; 41; 40] explore the use of multimodal and large language models for concept identification to eliminate the need for predefined labels. These methods enhance the adaptability of CBMs to various tasks and domains, improving their practical utility.

Performance Enhancement StrategiesRecent works consider relaxing the bottleneck by post hoc concept discernment  or fitting the CBMs' residuals  to improve model accuracy while preserving the other properties of CBMs.

Gaps and Our ContributionDespite the mixed successes of these efforts, there remains a gap in understanding concept bottleneck models and, in particular, how the properties of concept sets affect their performance.

In summary, our contribution is orthogonal to previous work, providing a deeper theoretical understanding of the relationship between concept set properties and model performance.

## 3 What are concepts?

Although the notion of concept has been repeatedly used in the ML literature [23; 30; 10], there is still a lack of a precise definition and delineation of their properties. To make them theoretically tractable entities, we begin by addressing what concepts are.

According to cognition theory, concepts are mental representations that constitute a fundamental key building block in all aspects of cognition . More specifically, they allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives and are necessary for cognitive processes such as categorization, memory, decision-making, learning, and inference. Thus, the formalism should reflect that (1) _concepts are representations_ and (2) _concepts empower inferences for everyday tasks_.

Concepts are representationsLet's denote a hypothetical set of concepts as \(\) and a particular concept as \(c\). As representations of some inputs \(\), concepts can be regarded as mappings from this space. To define the range of these mappings, we emphasize that concept representations indicate'membership' in a class. In modern theories (e.g., Prototype Theory), concepts consist of fuzzy memberships instead of binary ones. For example , humans consider "sink" as borderline a "kitchen utensil", or "octopuses" are just excluded from the concept of "fish". As such, concepts lend themselves to the fuzzy mathematical formulation of membership functions, a generalization of characteristic functions. Thus, the concept can be unequivocally represented as a mapping \(c:\).

Concepts aid in everyday-task inferenceNot all membership functions qualify as concepts; the missing component is their usefulness in task-specific inference. For a new concept to be valuable, it must improve predictability for a subset of tasks relevant to humans. This can happen either by capturing new nuances of inputs (for example, new color tones) or integrating different concepts into a more comprehensive entity (e.g., grouping animals into families).

Consider a task \(\), defined as a tuple1\((,)\), where \(=\{(x_{i},y_{i})_{iid}(,)\}\) is a dataset drawn from the data generation process \((,)\) and \(\) is a loss function. Define the \(n\)-risk of a model class \(f_{}\) on a task \(\) as the expected loss of a model trained on \(n\) samples, \(_{n}(f_{})=_{,x,y}[((x; ()),y)]\). Consider the output hypothesis spaces \(_{}\{g:\}\) complete under permutation symmetry and the learning algorithm invariant under feature permutations. We refer to Appendix B.1 for a technical discussion of this assumption.

**Definition 1**.: _[**Concept set**] Let \((,()):\) be a model trained on the task \(=(,)\), and \(\) a preexisting concept set, potentially void, of size \(m-1\). Given a membership function \(c_{m}\), we denote the conceptualized model \(_{\{c_{m}\}}=_{m}(,_{c})[c_{1}, ,c_{m}]\), such that \(_{m}:\{c_{m}\}\) is trained on the conceptualized input \(\{(c_{j}(x_{i}))_{j[m]}\}_{i[n]}\)._

_Then, \(\{c_{m}\}\) is a concept set if there exists \(N_{0} N_{1}\) such that_

\[_{n}(_{}) >_{n}(_{\{c_{m}\}})N_{0} n\] \[_{n}() >_{n}(_{\{c_{m}\}})n N_{1}\]

This implies that the output model trained on the concept set generalizes better than the vanilla one in the low-sample domain (model-aware inductive bias) and better than the smaller concept set in the large-sample domain (expressiveness)2. We further examine the role of these properties in the Appendix B.2.

Note that for human concept sets, the hypothesis space for the learned functions \(,_{m}\) corresponds to the human model. This definition can be regarded not only as a qualifying condition for a membership function to be considered a concept, but also as a measure of its quality, e.g., through the ratio of risks. From this we could discover iteratively new concepts by greedily maximizing the decrease in risk for a new concept or even take a hierarchical approach by unveiling the most relevant concepts to identify a given set of already established concepts.

The CBM AssumptionAn implicit assumption behind concept-based models that we make explicit is that such human concepts will also have predictive power for a given ML output model. The output model is conventionally linear for interpretability.

Although expressiveness and model-aware inductive bias may not always transfer across different models, it is reasonable to assume that a representation providing valuable inductive bias for one model can be beneficial for others. Additionally, given ML tasks are by design relevant to humans, and on many ML tasks, humans show a non-trivial performance (e.g., image classification), it is reasonable to expect that human _Concept Sets_ can benefit ML models too in these cases. However, an empirical validation (section 6) is required to check that the CBM assumption holds.

Furthermore, since humans have been exposed to a highly varied distribution, concepts are unlikely to be entangled with spurious features, which we also test in our insight experiments.

Model agnostic conceptsIn some cases, it may be useful to evaluate the quality of a concept set without knowing the output model. We propose using an information-theory metric: \(D_{}[(Y|\{c_{m}\}(X))\|(Y| (X))] 0\). A new concept should provide significant information on the output.

In the subsequent sections, we will dive into the theoretical insights derived from this formalism and explore empirical validations through various experiments.

Natural benefits of conceptsUsing a concept representation rather than a typical pre-trained model representation provides direct benefits. One notable advantage is that human concepts allow us to decouple the representation model from the output model. This flexibility means that we can replace the underlying concept identification model with a more advanced version in the future while keeping the output model unchanged. The concept set serves as a stable communication interface with fixed dimensionality and functionality. In addition, concepts can act as common representations across different feature sets, potentially harmonizing datasets and modalities. We expand on this discussion in the appendix B.3.

Theoretical results

We now dive into the effect of concept-based representations and their theoretical advantages. This includes the potential for improved data efficiency, as the concept-based representation space is inherently more structured and informative, thereby reducing the amount of data required to achieve comparable performance. Additionally, we will delve into the mechanisms by which concept-based representations enhance model robustness to distribution shifts. In the process, it will also become apparent in what regimes it is undesirable to bottleneck a model on concepts and potential solutions.

In our analysis, we focus on a recently popular approach to concept-based models, that is, concept bottleneck models based on multi-modal foundational models, such as CLIP .

To that end, we consider the input space \(}\) as the joint embedding space of inputs and concepts from a foundational model, where it is assumed  that concepts can be related to directions, and it is typical to use a linear output layer on top of such representation.

**Definition 2** (Risk).: _Let \(x,x^{(i)}}{{}}()\) for \(i[n]\), and let \(y^{(i)}=f^{*}(x^{(i)})\). Given a loss \(:\), the risk of a predictor \(\) trained on the \(n\) samples \((X,y)\) is given by_

\[()=_{x,X}\{[f^{*}(x), (x)]\}\.\] (1)

Now, since the output layer is conventionally linear, we may write \(f^{*}(x)=^{*}x\) and \((x)=x\). We further consider the \(l_{2}\) loss, where the predictor takes the analytical form \(=YX^{}\), \(X^{}\) representing the Moore-Penrose pseudoinverse of \(X\).

Given concepts are represented by directions in this space, we can write a set of concepts \(\) as a set of dual vectors \(\{c^{(1)},,c^{(k)}\}}^{*}\), we can consequently consider our conceptual representation as \(CX\) with \(C=[c^{(1)},,c^{(k)}]^{}\), and our overall concept model as \(_{C}=_{c} C\).

We now turn to the **Model-aware inductive bias** condition and assess whether there are more explicit conditions that imply it in practice, and thus compute the risk for the concept embedding model and the baseline.

**Theorem 1**.: _Under the above setting and assuming that \(\) is an isotropic distribution on the \(d-\)dimensional input, then the risk of the concept-based model \((_{C})\) is given by_

\[(_{C})=(1-n|}{(d+2)(d-1)}) (_{b})+n(+) ,\]

_where \(=||^{*}C^{}||_{F}^{2}\), and \(C^{}\) stands the orthogonal projection operator onto the orthogonal complement of the concept vector's span._

We note that this result aligns with our intuition. Indeed, even without spurious features or distribution shifts, concepts yield a practical advantage over the baseline foundational embedding (first term), but only if concepts are adequately chosen (second term). Let's dive deeper into each of these two terms.

Let us start with the \(\) term that corresponds to the misspecification of the concept set. Certainly, the larger the component of the actual predictor that does not belong to the subspace captured by the concepts, the lesser the quality of our concept model. Note how the \(\) coefficient grows with the number of examples \(n\), which means that the misspecification term is more influential when more examples are available. Nonetheless, with a proper choice of the concept set, the \(\) can be nullified.

When \( 0\), the remaining terms directly compare the concept and the baseline risks \((_{C}),(_{b})\). In fact, in this situation, we have a strict benefit for using the concept model, that is, \((_{C})<(_{b})\). Interestingly, provided \( 0\), the remaining term suggests we should aim for the minimal set of concepts \(||\), and we obtain a better scaling with the number of examples (i.e., increased sample efficiency).

Now, let us focus on the setting of spurious features due to confounders. As a motivating example, imagine the image classification setting, in which the classes are confounded with an environment property (e.g., outdoor dogs vs indoor cats). In this case, the image setting acts as a confounder \(Z\), manifesting as a spurious feature in the images. We note that such confounders are human concepts, which, per our previous discussion, are embodied by directions in the CLIP embedding \(z}^{*}\). Then,

**Theorem 2**.: _In the presence of confounders \(\{z_{1},z_{2},,z_{k}\}}^{*}\), given a concept \(c}^{*}\), the bias in the ordinary least squares estimator satisfies_

\[||(_{C}|X)||_{2}||( c,z_{1},,(  c,z_{k})||_{2}\]

_In particular, the classifier \(_{\{c\}}\) remains unbiased if and only if \(c(z_{1},z_{2},,z_{k})^{}\)._

Therefore, if the concept set consists only of a handful of non-spurious concepts, that is, semantically distinct from the confounders (which in the CLIP embedding metric translates into a small cosine similarity), we can expect the bias injected by the spurious feature to be significantly reduced.

### Effect of Errors-in-variable

We must admit that our concept identifier can be imperfect, implying our foundational embedding consists of noisy covariates, requiring an errors-in-variables analysis .

Although the effect of noise on multiple covariates is complex, significant noise in any covariate influences the coefficients of all other covariates (see Appendix A.2), degrading the accuracy of the estimator.

In the case of a single noisy feature \(x_{K}=x_{K}^{*}+u\), where \(x_{i}\) represents the observed covariate with noise \(u\), and \(y=x^{*}_{*}+\) is the actual model, the least-squares estimator \(\) is biased. Specifically, the estimated coefficient for this variable is attenuated by \(_{K}=(^{*}}{_{K}^{*}+_{u}^{*}}) _{*K}\). Not only the estimated coefficient \(_{K}\) is increasingly shrunk by the noise towards zero, but all the other coefficients \(_{i}\) for noiseless features are also biased proportionally to the bias in the coefficient of the noisy regressor. This interdependence means that significant noise in any covariate distorts all coefficients.

## 5 Concept sets generation and identification

While Theorem 1 refers to the properties that a concept set must satisfy, to use such a concept set, it is required that we have the concept mappings \(c:^{|C|}\). This can be achieved by collecting a dataset \(\) and training a model in a supervised fashion, but this is a costly procedure. Instead, the appearance of foundational multimodal models in which one of the domains is natural language, such as CLIP , provides a cheap and flexible approach to concept identification . We will focus on the latter approach because of its increased versatility.

From the concept set _desiderata_ in Theorem 1, although powerful, direct foundational model embeddings encode information in a high-dimensional space that likely contains irrelevant information (which can act as spurious features) and noise. For example, CLIP embeddings capture syntactic and semantic nuances and may not necessarily accentuate the conceptual understandings that underpin human reasoning and decision-making processes for a given task. This hinders not only generalization but also the interpretability of the model.

### Concept Identification

First, suppose that we already have a concept set \(C\). Let us explicitly describe how to obtain the concept representation using a multimodal model.

Through the model, concepts (language) and input (e.g., images or language in the case of serialized tabular data) can be embedded into a shared space. We can then use the appropriate metric (or _similarity_ measure) in this space to compute the degree of presence of a concept for each given input.

In the most common scenario, the distance consists of the cosine similarity, so if the textual concepts \(C=\{t_{1},,t_{M}\}\) and the dataset is \(D=\{x_{1},,x_{N}\}\), the concept representation \(c(x)\) can be computed as

\[c_{j}(x)=}(x),E_{T}(t_{j})}{||E_{}(x)||_{2}||E_{T}(t_{j})||_{2}}\,\]

with \(E_{}\) and \(E_{T}\) being the input and text encoders respectively. This is the approach that we use in our experiment section.

### Concept-set Generation

LLMs, especially if endowed with external sources of knowledge (such as ConceptNet  or Wikipedia articles), can be used to sample concepts that are commonly associated with a given task, thereby alleviating the burden of collecting domain expertise for the concept set generation, and potentially keeping the human out of the loop if necessary. See Appendix B.4 for a discussion on the choice of LLMs as base-concept samplers.

As shown in Theorem 1, the properties of the set of concepts are crucial for the effectiveness of the embedding. In particular, we should aim to balance the following objectives: (1) achieve minimal misspecification \(\), which amounts to choosing a complete enough set of concepts to capture as much valuable information in the inputs as possible while also aiming for (2) a set of concepts as small as possible. Additionally, there is an additional practical requirement for the chosen concepts: that the model we use is good enough to identify them to minimize the error-in-variable effect.

Motivated by our theoretical _desiderata_, we propose a multistep approach.

1. Initial generation of a complete concept set that satisfies (1). To achieve that, we propose employing a language model \(L\) to sample a sufficiently expressive set to achieve a sufficiently low misspecification \(\). This can be promoted by conditioning the generation on the context task \(_{}\) so that concepts are more relevant, which could include external knowledge such as ConceptNet and Wikipedia to promote further quality and diversity \(_{}_{}\). That is, \[_{i} L(c|_{})\]
2. For each concept, we must ensure it can be properly identified and its presence is informative for \(\). That is, \(_{i}\) should be discarded if the entropy of the concept for that task \((c_{i}(X))\) is below a given threshold \(_{m}\).
3. Finally, we should remove redundant concepts3. The approach for this step can vary depending on the output model we use. The conceptually most straightforward approach, based on Theorem 1 consists of empirically estimating the risk \(}(_{C})\) by averaging the mean square error \(_{p=1}^{N}MSE[_{C}(;(_{p}))]\) on cross-validation samples \(\{_{p}\}_{p=1}^{N}\). 
All in all, the concept generation procedure can be summarized as follows. Starting from \(C_{0}=\), while \((C_{i})>_{m}\) we increase our concept set by4

\[C_{i}=C_{i-1} c_{i}, c_{i} L(c_{ })\{(c_{i} (X))>_{m}\\ }(_{C_{i}})-}(_{C_{i-1}})>R_{m} .\]

There are settings where we cannot reach values of \(\) below our chosen threshold through any human concept set. For example, this can happen when there is a subset of relevant features for the task about which humans lack expertise.

In such cases, concepts can be leveraged beyond a bottleneck, which might help in any case where the misspecification term dominates. An option is to employ concepts as additional features. Another is to fit the residuals of the CBM by a black box model on the original features to enhance performance . In both cases, one could use concepts that should not influence predictions by including a mutual information penalty to the models trained on the raw input.

Empirical Validation

To empirically validate our insights about concept-based representations, we designed a series of experiments to validate our analysis5. We aim to explore the key insights from our theoretical results, including the ability to effectively leverage scarce datasets and the impact of concept-based representations on robustness to distribution shifts.

DatasetsOur experiments utilize **CIFAR-10** to test the effect of concept representations on data efficiency. This dataset is used because of its extensive use in research: its familiarity enables us to gather insights more effectively. **MetaShift** is a dataset to evaluate distribution shifts in machine learning models. It comprises 12,868 subsets across 410 classes, each representing unique contexts such as "cat with sink" or "cat with fence", using data from Visual Genome . The dataset's graph structure connects subsets by similarity, enabling the quantification shifts and visualization of data conflicts. Thus, we use MetaShift to evaluate model robustness across diverse shifts.

In Appendix D we discuss additional experiments, where we utilized three diverse datasets: CUB-200-2011 , Food-101 , and the Describable Textures Dataset (DTD)  for data efficiency analysis, CINIC-10  for out-of-distribution generalization, and CIFAR-10  to compare with label-free concept bottleneck models.

ModelsWe use VIT-B-32 for the multimodal CLIP  and GPT-4  as the human concept sampler. We refer to the VIT-B-32 embedding as the vanilla embedding, which acts as the natural baseline. We primarily use a linear output layer on top of either the vanilla or the concept embedding. Still, we also assess a variety of output layers to understand which concept-based inductive bias is helpful.

For all experiments, we provide a complete description, including the concept sets, in Appendix C.2.

The effects of the concept and sample sizes.We start by assessing the agreement between theory and practice regarding the effect of the number of training examples and the concept set size on the model's generalization to examples from the same distribution in CIFAR10, as we plot in figure 1.

Figure 1: Scaling laws for the number of examples across different concept set sizes. As predicted by Theorem 1, smaller concept sizes \(n<100\) benefit the small sample regime, whereas larger concept sizes \(n>200\) are superior in the larger sample regime. We plot the mean and \(1-\) bars over ten seeds.

As we concluded from our interpretation of Theorem 1, we expect the first term, \((1-n|}{(d+2)(d-1)})(_{b})\) to dominate the risk in the low data regime \(n 0\), since in that case, the \(\) coefficient \(n(+)\) renders the second term negligible.6 In this domain, an increase in the sample efficiency from the first term directly depends on the concept set size \(||\), favoring smaller concept sets, which is in total agreement with the empirical results shown in figures 1 and 4.

Then, as the number of samples increases substantially \(n\), the risks start to plateau, and the misspecification, embodied by \(\), becomes the dominant term. Thus, we expect larger concept sizes to outperform smaller ones due to a smaller \(\). This is particularly the case given our concept set generation procedure, where smaller concept sets are subsets of the larger ones. Again, this behavior is clearly represented in figures 1 and 4.

Robustness of Concept-embeddings to distribution shiftsWe now evaluate the concept's robustness to distribution shifts. In particular, we follow the approach of  in their evaluation of subpopulation shift in Section 4.2., controlling the degree of imbalance in the dataset to probe its effect on CBMs.

That is, we train and test distributions on mixtures of the same domain but where mixture weights differ between train and test. The domain comprises dog and cat images segregated by indoor and outdoor environments. In this case, pictures of outdoor cats and indoor dogs make up the minority groups, and we control the proportion of such samples while keeping \(|X|\) fixed at \(1700\).

To align with the insights of Theorem 2, we manually verify that the concept set excludes environment-related concepts to reduce the expected confounder bias induced by the shift. Indeed, we observe that the norm of the min-max scaled projection of the concepts "indoor" and "outdoor" in our concept set \(||( c_{i},z_{1},,( c_{i},z_{k})||_{2}\) has an average norm of \(0.80\), small compared to the interconcept similarity, with an average of \(1.65\).

Our results in figure 2 show that the concept representation yields a substantial increase in performance7, with bigger improvements for more pronounced distribution shifts. This effect is also seen for other output models, such as k-nearest neighbors and decision trees, suggesting the concept representation effectively reduces the presence of spurious features and thus confirming that concept representations are more robust to spurious correlations.

Figure 2: Concept representations reduce the effect of distribution shift over using the CLIP embedding directly, as seen by a more pronounced increase in accuracy on more imbalanced datasets. We plot the mean and \(1-\) bars over 10 seeds.

Out-of-distribution generalization of concept embeddingsFinally, we assess the robustness of operating with the concept embedding in terms of out-of-distribution generalization. To that end, we use domain generalization datasets from Section 4.1. in MetaShift.

The datasets we use consist of three tasks: dog vs. cat, bus vs. truck, and elephant vs. horse. Each of these tasks is tested on instances from one of the two classes, in particular, on images of dogs with shelves, trucks with airplanes, and horses with barns. However, for each task, we have four different training datasets, differing by the domain in which the tested class appears. For example, in the dog vs. cat dataset, we have training sets of dogs with bags or boxes, dogs with benches or bikes, dogs with boats or surfboards, and dogs with cabinets or beds (see figure 3 for details on the other three tasks). Importantly, each of the four datasets for each class corresponds to a different degree of out-of-distribution generalization challenge, comprising a varied test bed for robustness (we refer to  for further details).

Figure 3 shows that for all the 12 OOD tasks, the CBM significantly outperforms the non-bottleneck baseline, typically by an accuracy gap above 10% and even surpassing a 50% increase in accuracy. We further verify the robustness of the OOD predictions of CBMs in CINIC-10 in Table 1.

## 7 Conclusion

In this work, we introduced a novel theoretical framework for understanding the properties of concept sets in Concept Bottleneck Models (CBMs). We demonstrated how these properties affect model performance, particularly in low-data regimes and under distribution shifts. Our empirical results validated these theoretical insights, showing that well-chosen concept sets improve sample efficiency and robustness in real-world scenarios. Our findings not only deepen the understanding of CBMs but also pave the way for designing more effective models. We extend the limitations and open avenues for future work discussed throughout the paper in the Appendices E and F, respectively.