# Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics

Guillaume Mahey

INSA Rouen Normandie - Universite Bretagne Sud

LITIS - IRISA

guillaume.mahey@insa-rouen.fr

&Laetitia Chapel

Universite Bretagne Sud - Institut Agro Rennes-Angers

IRISA

laetitia.chapel@irisa.fr

&Gilles Gasso

INSA Rouen Normandie

LITIS

gilles.gasso@insa-rouen.fr

&Clement Bonet

Universite Bretagne Sud

LMBA

clement.bonet@univ-ubs.fr

&Nicolas Courty

Universite Bretagne Sud

IRISA

nicolas.courty@univ-ubs.fr

###### Abstract

Wasserstein distance (WD) and the associated optimal transport plan have proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy for the squared WD, coined min-SWGG, which relies on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics with a pivot measure supported on a line. We notably provide a new closed form of the Wasserstein distance in the particular case where one of the distributions is supported on a line, allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to that of Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and topological properties. Empirical evidences support the benefits of min-SWGG in various contexts, from gradient flows, shape matching and image colorization, among others.

## 1 Introduction

Gaspard Monge, in his seminal work on Optimal Transport (OT) , studied the following problem: how to move with minimum cost the probability mass of a source measure to a target one, for a given transfer cost function? At the heart of OT is the optimal map that describes the optimal displacement as the Monge problem can be reformulated as an assignment problem. It has been relaxed by  by finding a plan that describes the amount of mass moving from the source to the target. Beyond this optimal plan, an interest of OT is that it defines a distance between probability measures: the Wasserstein distance (WD).

Recently, OT has been successfully employed in a wide range of machine learning applications, in which the Wasserstein distance is estimated from the data, such as supervised learning , naturallanguage processing  or generative modelling . Its capacity to provide meaningful distances between empirical distributions is at the core of distance-based algorithms such as kernel-based methods  or \(k\)-nearest neighbors . The optimal transport plan has also been used successfully in many applications where a matching between empirical samples is sought such as color transfer , domain adaptation  and positive-unlabeled learning .

Solving the OT problem is computationally intensive; the most common algorithmic tools to solve the discrete OT problem are borrowed from combinatorial optimization and linear programming, leading to a cubic complexity with the number of samples that prevents its use in large scale applications . To reduce the computation burden, regularizing the OT problem with e.g. an entropic term has led to solvers with a quadratic complexity . Other methods based on the existence of a closed form of OT have also been devised to efficiently compute a proxy for WD, as outlined below.

**Projections-based OT.** The Sliced-Wasserstein distance (SWD) [56; 10] leverages 1D-projections of distributions to provide a lower approximation of the Wasserstein distance, relying on the closed form of OT for 1D probability distributions. Computation of SWD leads to a linearithmic time complexity. While SWD averages WDs computed over several 1D projections, max-SWD  keeps only the most informative projection. These frameworks provide efficient algorithms that can handle millions of samples and have similar topological properties as WD . Other works restrain SWD and max-SWD to projections onto low dimensional subspaces [52; 40] to provide more robust estimation of those OT metrics. Although effective as proxies for WD, those methods do not provide a transport plan in the original space \(^{d}\). To overcome this limitation,  aims to compute transport plans in a subspace which are extrapolated to the original space.

**Pivot measure-based OT.** Other research works rely on a pivot, yet intermediate measure. They decompose the OT metric into Wasserstein distances between each input measure and the considered pivot measure. They exhibit better properties such as statistical sample complexity or computational efficiency [29; 65]. Even though the OT problems are split, they are still expensive when dealing with large sample size distributions, notably when only two distributions are involved.

**Contributions.** We introduce a new proxy for the squared WD that exploits the principles of aforementioned approximations of OT metric. The original idea is to rely on projections and one-dimensional assignment of the projected distributions to compute the new proxy. The approach is well-grounded as it hinges on the notion of Wasserstein generalized geodesics  with pivot measure supported on a line. The main features of the method are as: i) its computational complexity is on par with SW, ii) it provides an optimal transport plan through the 1D assignment problem, iii) it acts as an upper bound of WD, and iv) is amenable to optimization to find the optimal pivot measure. As an additional contribution, we establish a closed form of the WD when an input measure is supported on a line.

**Outline.** Section 2 presents some background of OT. Section 3 formulates our new WD proxy, provides some of its topological properties and a numerical computation scheme. Section 4 builds upon the concept of Wasserstein generalized geodesics to reformulate our OT metric approximation as the Sliced Wasserstein Generalized Geodesics (SWGG) along its optimal variant coined min-SWGG. This reformulation allows deriving additional topological properties and an optimization scheme. Finally, Section 5 provides experimental evaluations.

**Notations.** Let \(,\) be the Euclidean inner product on \(^{d}\) and let \(^{d-1}=\{^{d}\|\|_{2}=1\}\), the unit sphere. We denote \((^{d})\) the set of probability measures on \(^{d}\) endowed with the \(-\)algebra of Borel set and \(_{2}(^{d})(^{d})\) those with finite second-order moment i.e. \(_{2}(^{d})=\{(^{d})_{^{d}}\|\|_{2}^{2}d()<\}\). Let \(_{2}^{n}(^{d})\) be the subspace of \(_{2}(^{d})\) defined by empirical measures with \(n\)-atoms and uniform masses. For any measurable function \(f:^{d}^{d}\), we denote \(f_{\#}\) its push forward, namely for \(_{2}(^{d})\) and for any measurable set \(A^{d}\), \(f_{\#}(A)=(f^{-1}(A))\), with \(f^{-1}(A)=\{^{d}f() A\}\).

## 2 Background on Optimal Transport

**Definition 2.1** (Wasserstein distance).: The squared WD  between \(_{1},_{2}_{2}(^{d})\) is defined as:

\[W_{2}^{2}(_{1},_{2})\ }}{{=}}_{ (_{1},_{2})}_{^{d}^{d}}\|- \|_{2}^{2}d(,)\] (1)with \((_{1},_{2})=\{_{2}(^{d}^{d})\ \ (A ^{d})=_{1}(A)\ \ (^{d} A)=_{2}(A)\), \( A\) measurable set of \(^{d}\}\).

The \(\) of Eq. (1) is referred to as the optimal transport plan. Denoted \(^{*}\), it expresses how to move the probability mass from \(_{1}\) to \(_{2}\) with minimum cost. In some cases, \(^{*}\) is of the form \((Id,T)_{\#}_{1}\) for a measurable map \(T:^{d}^{d}\), _i.e._ there is no mass splitting during the transport. This map is called a Monge map and is denoted \(T^{_{1}_{2}}\) (or shortly \(T^{1 2}\)). Thus, one has \(W_{2}^{2}(_{1},_{2})=_{T\ \ T_{\#}_{1}=_{2}}_{ ^{d}}||-T()||_{2}^{2}d_{1}()\). This occurs, for instance, when \(_{1}\) has a density w.r.t. the Lebesgue measure  or when \(_{1}\) and \(_{2}\) are in \(_{2}^{n}(^{d})\).

Endowed with the WD, the space \(_{2}(^{d})\) is a geodesic space. Indeed, since there exists a Monge map \(T^{1 2}\) between \(_{1}\) and \(_{2}\), one can define a geodesic curve \(^{1 2}:_{2}(^{d})\) as:

\[ t,\ ^{1 2}(t)\ }}{{=}} (tT^{1 2}+(1-t)Id)_{\#}_{1}\] (2)

which represents the shortest path w.r.t. Wasserstein distance in \(_{2}(^{d})\) between \(_{1}\) and \(_{2}\). The Wasserstein mean between \(_{1}\) and \(_{2}\) corresponds to \(t=0.5\) and we simply write \(^{1 2}\).

This notion of geodesic allows the study of the curvature of the Wasserstein space . Indeed, the Wasserstein space is of positive curvature , _i.e._ it respects the following inequality:

\[W_{2}^{2}(_{1},_{2}) 2W_{2}^{2}(_{1},)+2W_{2}^{2}(,_{2})-4 W_{2}^{2}(^{1 2},)\] (3)

for all pivot measures \(_{2}(^{d})\).

Solving and approximating Optimal Transport.The Wasserstein distance between empirical measures \(_{1},_{2}\) with \(n\)-atoms can be computed in \((n^{3} n)\), preventing from the use of OT for large scale applications . Several algorithms have been proposed to lower this complexity, for example the Sinkhorn algorithm  that provides an approximation in near \((n^{2})\) complexity . Notably, when \(_{1}=_{i=1}^{n}_{x_{i}}\) and \(_{2}=_{i=1}^{n}_{y_{i}}\) are 1D distributions, computing the WD can be done by matching the sorted empirical samples, leading to an overall complexity of \((n n)\). More precisely, let \(\) and \(\) two permutation operators s.t. \(x_{(1)} x_{(2)}... x_{(n)}\) and \(y_{(1)} y_{(2)}... y_{(n)}\). Then, the 1D Wasserstein distance is given by:

\[W_{2}^{2}(_{1},_{2})=_{i=1}^{n}(x_{(i)}-y_{(i) })^{2}.\] (4)

Sliced WD.The Sliced-Wasserstein distance (SWD)  aims to scale up the computation of OT by leveraging the closed form expression (4) of the Wasserstein distance for 1D distributions. It is defined as the expectation of 1D-WD computed along projection directions \(^{d-1}\) over the unit sphere:

\[_{2}^{2}(_{1},_{2})\ }}{{=}}\ _{^{d-1}}W_{2}^{2}(P_{\#}^{}_{1},P_{\#}^{}_{2} )d(),\] (5)

where \(P_{\#}^{}_{1}\) and \(P_{\#}^{}_{2}\) are projections onto the direction \(^{d-1}\) with \(P^{}:^{d}\), \(,\) and where \(\) is the uniform distribution over \(^{d-1}\).

Since the integral in Eq. (5) is intractable, one resorts, in practice, to Monte-Carlo estimation to approximate the SWD.

Its computation only involves projections and permutations. For \(L\) directions, the computational complexity is \((dLn+Ln n)\) and the memory complexity is \((Ld+Ln)\). However, in high dimension, several projections are necessary to approximate accurately the SWD and many projections lead to 1D-WD close to 0. This issue is well known in the SW community , where different ways of performing effective sampling have been proposed  such as distributional or hierarchical slicing. In particular, this motivates the definition of max-Sliced-Wasserstein  which keeps only the most informative slice:

\[_{2}^{2}(_{1},_{2})\ }}{{=}}\ _{^{d-1}}W_{2}^{2}(P_{\#}^{}_{1},P_{\#}^{ }_{2}).\] (6)

While being a non convex problem, it can be optimized efficiently using a gradient ascent scheme.

The SW-like distances are attractive since they are fast to compute and enjoy theoretical properties: they are proper metrics and metricize the weak convergence. However, they do not provide an OT plan.

Projected WD.Another quantity of interest based on the 1D-WD is the projected Wasserstein distance (PWD) . It leverages the permutations of the projected distributions in 1D in order to derive couplings between the original distributions.

Let \(_{1}=_{i=1}^{n}_{_{i}}\) and \(_{2}=_{i=1}^{n}_{_{i}}\) in \(_{2}^{n}(^{d})\). The PWD is defined as:

\[_{2}^{2}(_{1},_{2})\ }}{{=}}\ _{^{d-1}}_{i=1}^{n}\|_{_{ }(i)}-_{_{}(i)}\|_{2}^{2}d(),\] (7)

where \(_{},_{}\) are the permutations obtained by sorting \(P_{\#}^{}_{1}\) and \(P_{\#}^{}_{2}\).

As some permutations are not optimal, we straightforwardly have \(W_{2}^{2}_{2}^{2}\). Note that some permutations can appear highly irrelevant in the original space, leading to an overestimation of \(W_{2}^{2}\) (typically when the distributions are multi-modal or with support lying in a low dimensional manifold, see Supp. 7.1 for a discussion).

In this paper, we restrict ourselves to empirical distributions with the same number of samples. They are defined as \(_{1}=_{i=1}^{n}_{_{i}}\) and \(_{2}=_{i=1}^{n}_{_{i}}\) in \(_{2}^{n}(^{d})\). Note that the results presented therein can be extended to any discrete measures by mainly using quantile functions instead of permutations and transport plans instead of transport maps (see Supp. 7.2).

## 3 Definition and properties of min-SWGG

The fact that the PWD overestimates \(W_{2}^{2}\) motivates the introduction of our new loss function coined min-SWGG which keeps only the most informative permutation. Afterwards, we derive a property of distance and grant an estimation of min-SWGG via random search of the directions.

**Definition 3.1** (Swgg and min-SWGG).: Let \(_{1},_{2}_{2}^{n}(^{d})\) and \(^{d-1}\). Denote by \(_{}\) and \(_{}\) the permutations obtained by sorting the 1D projections \(P_{\#}^{}_{1}\) and \(P_{\#}^{}_{2}\). We define respectively SWGG and min-SWGG as:

\[_{2}^{2}(_{1},_{2},) \ }}{{=}}\ _{i=1}^{n}\|_{_{}(i)}-_{_{}(i )}\|_{2}^{2},\] (8) \[_{2}^{2}(_{1},_{2}) \ }}{{=}}\ _{^{d-1}}_{2}^{2}(_{1},_{2},).\] (9)

One shall remark that the function SWGG corresponds to the building block of PWD in eq. (7).

One main feature of min-SWGG is that it comes with a transport map. Let \(^{*}_{2}^{2}(_{1},_{2},)\) be the optimal projection direction. The associated transport map is:

\[T(_{i})=_{_{^{*}}^{-1}(_{^{*}}(i))},  1 i n.\] (10)

In Supp. 7.6 we give several examples of such transport plan. These examples show that the overall structure of the optimal transport plan is respected by the transport plan obtained via min-SWGG.

We now give some theoretical properties of the quantities min-SWGG and SWGG. Their proofs are given in Supp. 7.3.

**Proposition 3.2** (Distance and Upper bound).: Let \(^{d-1}\). \(_{2}(,,)\) defines a distance on \(_{2}^{n}(^{d})\). Moreover, min-SWGG is an upper bound of \(W_{2}^{2}\), and \(W_{2}^{2}_{2}^{2}_{2}^{2}\), with equality between \(W_{2}^{2}\) and min-SWGG\({}_{2}^{2}\) when \(d>2n\).

**Remark 3.3**.: Similarly to max-SW, min-SWGG retains only one optimal direction \(^{*}^{d-1}\). However, the two distances strongly differ: i) min-SWGG is an upper bound and max-SW a lower bound of \(W_{2}^{2}\), ii) the optimal \(^{*}\) may differ (see Supp. 7.4 for an illustration), and iii) max-SW does not provide a transport plan between \(_{1}\) and \(_{2}\).

Solving Eq. (9) can be achieved using a random search, by sampling \(L\) directions \(^{d-1}\) and keeping only the one leading to the lowest value of SWGG.

This gives an overall computational complexity of \((Ldn+Ln n)\) and a memory complexity of \((dn)\). In low dimension, the random search estimation is effective: covering all possible permutations through \(^{d-1}\) can be done with a low number of directions. In high dimension, many more directions \(\) are needed to have a relevant approximation, typically \((L^{d-1})\). This motivates the design of gradient descent techniques for finding \(^{*}\).

## 4 SWGG as minimizing along the Wasserstein generalized geodesics

Solving problem in Eq. (9) amounts to optimize over a set of admissible permutations. This problem is hard since SWGG is non convex w.r.t. \(\) and piecewise constant, thus not differentiable over \(^{d-1}\). Indeed, as long as the permutations remain the same for different directions \(\), the value of SWGG remains constant. When the permutations change, the objective SWGG "jumps" as illustrated in Fig. 1.

In this section, we tackle this problem by providing an alternative formulation of min-SWGG that allows smoothing the different kinks of SWGG, hence, making min-SWGG amenable to optimization. This formulation relies on Wasserstein generalized geodesics we introduce hereinafter.

We show that this alternative formulation brings in computational advantages and allows establishing some additional topological properties and deriving an efficient optimization scheme. We also provide a new closed form expression of the Wasserstein distance \(W_{2}^{2}(_{1},_{2})\) when either \(_{1}\) or \(_{2}\) is supported on a line.

### SWGG based on Wasserstein Generalized Geodesics

Wasserstein generalized geodesics (see Supp. 8 for more details) were first introduced in  in order to ensure the convergence of Euler scheme for Wasserstein Gradient Flows. This concept has been used notably in [29; 44] to speed up some computations and to derive some theoretical properties. Generalized geodesic is also highly related with the idea of linearization of the Wasserstein distance via an \(L^{2}\) space [65; 43], see Supp. 9 for more details on the related works.

Generalized geodesics lay down on a pivot measure \(_{2}^{n}(^{d})\) to transport the distribution \(_{1}\) toward \(_{2}\). Indeed, one can leverage the optimal transport maps \(T^{_{1}}\) and \(T^{_{2}}\) to construct a curve \(t_{g}^{1 2}(t)\) linking \(_{1}\) to \(_{2}\) as

\[_{g}^{1 2}(t)\;}}{{=}}\;((1-t)T^{ _{1}}+tT^{_{2}})_{\#}\,, t.\] (11)

The related generalized Wasserstein mean corresponds to \(t=0.5\) and is denoted \(_{g}^{1 2}\).

Intuitively, the optimal transport maps between \(\) and \(_{i},i=1,2\) give rise to a sub-optimal transport map between \(_{1}\) and \(_{2}\):

\[T_{}^{1 2}\;}}{{=}}\;T^{_{2}}  T^{_{1}}(T_{}^{1 2})_{\#}_{1}= _{2}.\] (12)

One can be interested in the cost induced by the transportation of \(_{1}\) to \(_{2}\) via the transport map \(T_{}^{1 2}\), known as the \(\)-based Wasserstein distance  and defined as

\[W_{}^{2}(_{1},_{2})\;}}{{=}}\; _{^{d}}\|-T_{}^{1 2}()\|_{2}^{2}d_{1}()=2W_{2} ^{2}(_{1},)+2W_{2}^{2}(,_{2})-4W_{2}^{2}(_{g}^{1 2},).\] (13)

Notably, the second part of Eq. (13) straddles the square Wasserstein distance with Eq. (3). Remarkably, the computation of \(W_{}^{2}\) can be efficient if the pivot measure \(\) is chosen appropriately. As established in Lemma 4.6, it is the case when \(\) is supported on a line. Based on these facts, we propose hereafter an alternative formulation of SWGG.

**Definition 4.1** (Pivot measure).: Let \(_{1}\) and \(_{2}_{2}^{n}(^{d})\). We restrict the pivot measure \(\) to be the Wasserstein mean of the measures \(Q_{\#}^{}_{1}\) and \(Q_{\#}^{}_{2}\):

\[_{}^{1 2}\;}}{{=}}\;_{ _{2}^{n}(^{d})}W_{2}^{2}(Q_{\#}^{}_{1},)+ W_{2}^{2}(,Q_{\#}^{}_{2}),\]

Figure 1: (Left) Empirical distributions with examples of 2 sampled lines (Right) that lead to 2 possible values of SWGG when \([0,2]\).

where \(^{d-1}\) and \(Q^{}:^{d}^{d}\), \(,\) is the projection onto the subspace generated by \(\). Moreover \(_{}^{1 2}\) is always defined as the middle of a geodesic as in Eq (2).

One shall notice that \(Q^{}_{\#}_{1}\) and \(Q^{}_{\#}_{2}\) are supported on the line defined by the direction \(\), so is the pivot measure \(=_{}^{1 2}\). We are now ready to reformulate the metric SWGG.

**Proposition 4.2** (SWGG based on generalized geodesics).: Let \(^{d-1}\), \(_{1},_{2}_{2}^{n}(^{d})\) and \(_{}^{1 2}\) be the pivot measure. Let \(_{g,}^{1 2}\) be the generalized Wasserstein mean between \(_{1}\) and \(_{2}_{2}^{n}(^{d})\) with pivot measure \(_{}^{1 2}\). Then,

\[_{2}^{2}(_{1},_{2},)=2W_{2}^{2}(_{1},_{}^{ 1 2})+2W_{2}^{2}(_{}^{1 2},_{2})-4W_{2}^{2}(_{g, }^{1 2},_{}^{1 2}).\] (14)

The proof is in Supp.10.1. From Proposition 4.2, SWGG is the \(_{}^{1 2}\)-based Wasserstein distance between \(_{1}\) and \(_{2}\). This alternative formulation allows establishing additional properties of min-SWGG.

### Theoretical properties

Additionally to the properties derived in Section 3 (SWGG is a distance and min-SWGG is an upper bound of \(W_{2}^{2}\)), we provide below other theoretical guarantees.

**Proposition 4.3** (Weak Convergence).: min-SWGG metricizes the weak convergence in \(_{2}^{n}(^{d})\). In other words, let \((_{k})_{k}\) be a sequence of measures in \(_{2}^{n}(^{d})\) and \(_{2}^{n}(^{d})\). We have:

\[_{k}[k]{,2}_{2}^{2}(_{k}, )[k]{}0,\]

where \([k]{,2}\) stands for the weak convergence of measure i.e. \(_{^{d}}fd_{k}_{^{d}}fd\) for all continuous bounded functions \(f\).

Beyond the weak convergence, min-SWGG possesses the translation property, _i.e._ the translations can be factored out as the Wasserstein distance does (see [53, remark 2.19] for a recall).

**Proposition 4.4** (Translation).: Let \(T^{u}\) (resp. \(T^{v}\)) be the map \(-\) (resp. \(-\)), with \(,\) vectors of \(^{d}\). We have:

\[_{2}^{2}(T^{u}_{\#}_{1},T^{v}_{\#}_{2})=_{ 2}^{2}(_{1},_{2})+\|-\|_{2}^{2}-2-,}-}\]

where \(}=_{^{d}}d_{1}()\) and \(}=_{^{d}}d_{2}()\) are the means of \(_{1}\), \(_{2}\).

This property is useful in some applications such as shape matching, in which translation invariances are sought.

The proofs of the two Propositions are deferred to Supp. 10.2 and 10.3.

**Remark 4.5** (Equality).: min-SWGG and \(W_{2}^{2}\) are equal in different cases. First,  showed that it is the case whenever \(_{1}\) is the shift and scaling of \(_{2}\) (see Supp. 9.1 for a full discussion). In Lemma 4.6, we will state that it is also the case if one of the two distributions is supported on a line.

### Efficient computation of SWGG

SWGG defined in Eq. (14) involves computing three WDs that are fast to compute, with an overall \((dn+n n)\) complexity, as detailed below. Building on this result, we provide an optimization scheme that allows optimizing over \(\) with \((sdn+sn sn)\) operations at each iteration, with \(s\) a (small) integer. We first start by giving a new closed form expression of the WD whenever one distribution is supported on a line, that proves useful for deriving an efficient computation scheme.

New closed form of the WD.The following lemma states that \(W_{2}^{2}(_{1},_{2})\) admits a closed form whenever \(_{2}\) is supported on a line.

This lemma leverages the computation of the WD between \(_{2}\) and the orthogonal projection of \(_{1}\) onto the linear subspace defined by the line. Additionally, it provides an explicit formulation for the optimal transport map \(T^{1 2}\).

**Lemma 4.6**.: Let \(_{1},_{2}\) in \(_{2}^{n}(^{d})\) with \(_{2}\) supported on a line of direction \(^{d-1}\). We have:

\[W_{2}^{2}(_{1},_{2})=W_{2}^{2}(_{1},Q_{\#}^{}_{1})+W_{2}^{2}(Q _{\#}^{}_{1},_{2})\] (15)

with \(Q^{}\) as in Def. 4.1. Note that \(W_{2}^{2}(_{1},Q_{\#}^{}_{1})=\|_{i}-Q^{ }(_{i})\|_{2}^{2}\) and \(W_{2}^{2}(Q_{\#}^{}_{1},_{2})=W_{2}^{2}(P_{\#}^{}_{1},P_{ \#}^{}_{2})\) are the WD between 1D distributions. Additionally, the optimal transport map is given by \(T^{1 2}=T^{Q_{\#}^{}_{1}_{2}} T^{_{1} Q_{\#}^{ }_{1}}=T^{Q_{\#}^{}_{1}_{2}} Q^{}\). In particular, the map \(T^{1 2}\) can be obtained via the permutations of the 1D distributions \(P_{\#}^{}_{1}\) and \(P_{\#}^{}_{2}\). The proof is provided in Supp. 10.4.

Efficient computation of SWGG.Eq. (14) is defined as the Wasserstein distance between a distribution (either \(_{1}\) or \(_{2}\) or \(_{g,}^{1 2}\)) and a distribution supported on a line (\(_{}^{1 2}\)). As detailed in Supp. 10.5, computation of Eq. (14) involves three Wasserstein distances between distributions and their projections: i) \(W_{2}^{2}(_{1},Q_{\#}^{}_{1})\), ii) \(W_{2}^{2}(_{2},Q_{\#}^{}_{2})\), iii) \(W_{2}^{2}(_{g,g}^{1 2},_{}^{1 2})\), and a one dimensional Wasserstein distance \(W_{2}^{2}(P_{\#}^{}_{1},P_{\#}^{}_{2})\), resulting in a \((dn+n n)\) complexity.

Optimization scheme for min-SWGG.The term \(W_{2}^{2}(_{g,}^{1 2},_{}^{1 2})\) in Eq. (14) is not continuous w.r.t. \(\). Indeed, the generalized mean \(_{g,}^{1 2}\) depends only on the transport maps \(T^{_{}^{1 2}_{1}}\) and \(T^{_{}^{1 2}_{2}}\), which remain constant as long as different projection directions \(\) lead to the same permutations \(_{}\) and \(_{}\). Hence, we rely on a smooth surrogate \(^{1 2}}\) of the generalized mean and we aim to minimize the following objective function:

\[_{2}^{2}}(_{1},_{2},)\ }}{{=}}\ 2W_{2}^{2}(_{1},_{}^{1 2})+2W_{2}^{2}(_{ }^{1 2},_{2})-4W_{2}^{2}(^{1 2}},_{ }^{1 2}).\] (16)

To define \(^{1 2}}\), one option would be to use entropic maps in Eq. (11) but at the price of a quadratic time complexity. We rather build upon the blurred Wasserstein distance  to define \(^{1 2}}\) as it can be seen as an efficient surrogate of entropic transport plans in 1D. In one dimensional setting, \(^{1 2}}\) can be approximated efficiently by adding an empirical Gaussian noise followed by a sorting pass. In our case, it resorts in making \(s\) copies of each sorted projection \(P^{}(_{(i)})\) and \(P^{}(_{(i)})\) respectively, to add an empirical Gaussian noise of deviation \(/2\) and to compute averages of sorted blurred copies \(_{^{s}}^{s}\), \(_{^{s}}^{s}\). We finally have \((^{1 2}})_{i}=_{k=(i-1)s+1}^{is}_{ ^{s}(k)}^{s}+_{^{s}(k)}^{s}\).  showed that this blurred WD has the same asymptotic properties as the Sinkhorn divergence.

The surrogate \(}(_{1},_{2},)\) is smoother w.r.t. \(\) and can thus be optimized using gradient descent, converging towards a local minima. Once the optimal direction \(^{*}\) is found, min-SWGG resorts to be the solution provided by \((_{1},_{2},^{*})\). Fig. 2 illustrates the effect of the smoothing on a toy example and more details are given in Supp. 10.6. The computation of \(}(_{1},_{2},)\) is summarized in Alg. 1.

```
0:\(_{1}=_{_{i}}\), \(_{2}=_{_{i}}\), \(^{d-1}\), \(s_{+}\) and \(_{+}\) \(,\) ascending ordering of \((P^{}(_{i}))_{i}\), \((Q^{}(_{i}))_{i}\) \(^{s} s\) copies of \((_{(i)})_{i}\), \(^{s} s\) copies of \((_{(i)})_{i}\) \(^{s},^{s}\) ascending ordering of \((^{s},)+\), \(^{s},+\) for \(_{i}(0,/2)\), \( i sn\) \(a_{i}(\|_{i}-Q^{}(_{i})\|_{2}^{2}+ \|_{i}-Q^{}(_{i})\|_{2}^{2})\)\(\)\(2W_{2}^{2}(_{1},Q_{\#}^{}_{1})+2W_{2}^{2}(_{2},Q_{\#}^{}_{2})\) \(b_{i}P^{}(_{(i)})+P^{}( _{(i)})_{2}^{2}\)\(\)\(2W_{2}^{2}(P_{\#}^{}_{1},P_{\#}^{}_{2})\) \(c(Q^{}(_{(i)})+Q^{ }(_{(i)}))-_{k=(i-1)s+1}^{is}(_{ ^{s}(k)}^{s}+_{^{s}(k)}^{s})_{2}^{2} 4W_{2}^{2}( ^{1 2}},_{}^{1 2})\) Output\(a+b-c\) ```

**Algorithm 1** Computing \(_{2}^{2}}(_{1},_{2},)\)

## 5 Experiments

We highlight that min-SWGG is fast to compute, gives an approximation of the WD and the associated transport plan. We start by comparing the random search and the gradient descent schemes for finding the optimal direction in subsection 5.1. Subsection 5.2 illustrates the weak convergence property of min-SWGG through a gradient flow application to match distributions. We then implement an efficient algorithm for colorization of gray scale images in 5.3, thanks to the new closed form expression of the WD. We finally evaluate min-SWGG in a shape matching context in subsection 5.4. When possible from the context, we compare min-SWGG with the main methods for approximating the WD namely SW, max-SW, Sinkhorn , factored coupling  and subspace robust WD (SRW) . Supp. 11 provides additional results on the behavior of min-SWGG and experiments on other tasks such as color transfer or on data sets distance computation. All the code is available at 1

### Computing min-SWGG

Let consider Gaussian distributions in dimensions \(d\{2,20,200\}\). We first sample \(n=1000\) points from each distribution to define \(_{1}\) and \(_{2}\). We then compute min-SWGG\({}_{2}^{2}(_{1},_{2})\) computed using different schemes, either by random search, by simulated annealing  or by gradient descent. We report the obtained results in Fig. 3 (left). For the random search scheme, we repeat each experiment 20 times and we plot the average value of min-SWGG \(\) 2 times the standard deviation.

For the gradient descent, we select a random initial \(\). We observe that, in low dimension, all schemes provide similar values of min-SWGG. When the dimension increases, optimizing the direction \(\) yields a more accurate approximation of the true Wasserstein distance (see plots' title in Fig. 3). On Fig. 3 (right), we compare the empirical runtime evaluation for min-SWGG with different competitors for \(d=3\) and using \(n\) samples from Gaussian distributions, with \(n\{10^{2},10^{3},10^{4},5 10^{4},10^{5}\}\). We observe that, as expected, min-SWGG with random search is as fast as SW with a super linear time complexity. With the optimization process, it is faster than SRW for a given number of samples. We also note that SRW is more demanding in memory and hence does not scale as well as min-SWGG. We give more details on this experimentation and a comparison with competitors in Supp. 11.2.

Figure 3: (Left) evolution of min-SWGG with different numbers of projections and with the dimension \(d\) in \(\{2,20,200\}\). (Right) Runtimes.

Figure 2: Illustration of the smoothing effect in the same setting as in Fig. 1. (Left) Two sets of generalized Wasserstein means are possible, depending on the direction of the sampled line w.r.t. \(_{1}\) and \(_{2}\), giving rise to 2 different values for SWGG. (Middle) The surrogate provides a smooth transition between the two sets of generalized Wasserstein means as the direction \(\) changes, (Right) providing a smooth approximation of SWGG that is amenable to optimization.

### Gradient Flows

We highlight the weak convergence property of min-SWGG. Initiating from a random initial distribution, we aim to move the particles of a source distribution \(_{1}\) towards a target one \(_{2}\) by reducing the objective min-SWGG\({}_{2}^{2}(_{1},_{2})\) at each step. We compare both variants of min-SWGG against SW, max-SW and PWD, relying on the code provided in  for running the experiment; we report the results on Fig. 4. We consider several target distributions, representing diverse scenarios and fix \(n=100\). We run each experiment 10 times and report the mean \(\) the standard deviation. In every case, one can see that \(_{1}\) moves towards \(_{2}\) and that all methods tend to have similar behavior. One can notice though that, for the distributions in \(d=500\) dimensional space, min-SWGG computed with the optimization scheme leads to the best alignment of the distributions.

### Gray scale image colorization

Lemma 4.6 states that the WD has a closed form when one of the 2 distributions is supported on a line, allowing us to compute the WD and the OT map with a complexity of \((dn+n n)\). This particular situation arises for instance with RBG images (\(_{1},_{2}_{2}^{n}(^{3})\)), where black and white images are supported on a line (the line of grays). One can address the problem of image colorization through color transfer , where a black and white image is the source and a colorful image the target. Our fast procedure allows considering large images without sub-sampling with a reasonable computation time. Fig. 5 gives an example of colorization of an image of size 1280\(\)1024 that was computed in less than 0.2 second, while being totally untractable for the \((n^{3} n)\) solver of WD.

This procedure can be lifted to pan-sharpening  where one aims to construct a super-resolution multi-chromatic satellite image with the help of a super-resolution mono-chromatic image (source) and a low-resolution multi-chromatic image (target). Obtained results are given in the Supp. 11.4.

### Point clouds registration

Iterative Closest Point (ICP) is an algorithm for aligning point clouds based on their geometries . Roughly, its most popular version defines a one-to-one correspondence between point clouds, computes a rigid transformation (namely translation, rotation or reflection), moves the source point clouds using the transformation, and iterates the procedure until convergence. The rigid transformation is the solution of the Procrustes problem _i.e._\(_{(,t) O(d)^{d}}\|(-t)-\|_{2}^{2}\), where \(,\) are the source and the target cloud points and \(O(d)\) the space of orthogonal matrices of dimension \(d\). This Procrustes problem can be solved using a SVD  for instance.

We perform the ICP algorithm with different variants to compute the one-to-one correspondence: nearest neighbor (NN) correspondence, OT transport map (for small size datasets) and min-SWGG

Figure 4: Log of the WD between different source and target distributions as a function of the number of iterations.

Figure 5: Cloud point source and target (left) colorization of image (right).

transport map. Note that SW, PWD, SRW, factored coupling and Sinkhorn cannot be run in this context where a one-to-one correspondence is mandatory; subspace detours  are irrelevant in this context (see Supp. 11.5). We evaluate the results of the ICP algorithm in terms of: i) the quality of the final alignment, measured by the Sinkhorn divergence between the re-aligned and target point cloud; ii) the speed of the algorithm given by the running time until convergence. We consider 3 datasets of different sizes. The results are shown in Table 1 and more details about the setup, can be found in Supp. 11.5. In Supp. 11.5 we give a deeper analysis of the results, notably with different criteria for the final assignment, namely the Chamfer and the Frobenius distance. One can see that the assignment provided by OT-based methods is better than NN. min-SWGG allows working with large datasets, while OT fails to provide a solution for \(n=150000\).

## 6 Conclusion

In this paper, we hinge on the properties of sliced Wasserstein distance and on the Wasserstein generalized geodesics to define min-SWGG, a new upper bound of the Wasserstein distance that comes with an associated transport map. Topological properties of SWGG are provided, showing that it defines a metric and that min-SWGG metrizes the weak convergence of measure. We also propose two algorithms for computing min-SWGG, either through a random search scheme or a gradient descent procedure after smoothing the generalized geodesics definition of min-SWGG. We illustrate its behavior in several experimental setups, notably showcasing its interest in applications where a transport map is needed.

The set of permutations covered by min-SWGG is the one induced by projections and permutations on the line. It is a subset of the original Birkhoff polytope and it would be interesting to characterize how these two sets relates. In particular, in the case of empirical realizations of continuous distributions, the behavior of min-SWGG, when \(n\) grows, needs to be investigated. In addition, the fact that min-SWGG and WD coincide when \(d>2n\) calls for embedding the distributions in higher dimensional spaces to benefit from the greater expressive power of projection onto the line. Another important consideration is to establish a theoretical upper bound for min-SWGG.