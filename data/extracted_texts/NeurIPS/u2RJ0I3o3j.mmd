# Plane: Representation Learning over Planar Graphs

Radoslav Dimitro

Department of Computer Science

University of Oxford

contact@radoslav11.com

&Zeyang Zhao

Department of Computer Science

University of Oxford

zeyzang.zhao@cs.ox.ac.uk

&Ralph Abboud

Department of Computer Science

University of Oxford

ralph@ralphabb.ai

&Ismail likan Ceylan

Department of Computer Science

University of Oxford

ismail.ceylan@cs.ox.ac.uk

This work is largely conducted while these authors were still affiliated with the University of Oxford.Equal contribution.

###### Abstract

Graph neural networks iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations _graph invariants_. On the other hand, it is well-known that graph invariants learned by these class of models are _incomplete_: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for _efficiently_ learning _complete_ invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose Plane as a framework for planar representation learning. PlanE includes architectures which can learn _complete_ invariants over planar graphs while remaining practically scalable. We validate the strong performance of PlanE architectures on various planar graph benchmarks.

## 1 Introduction

Graphs are used for representing relational data in a wide range of domains, including physical , chemical , and biological  systems, which led to increasing interest in machine learning (ML) over graphs. Graph neural networks (GNNs)  have become prominent for graph ML for a wide range of tasks, owing to their capacity to explicitly encode desirable relational inductive biases . GNNs iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph-level function represents a _graph invariant_: a property of graphs which is preserved under all isomorphic transformations.

Learning functions on graphs is challenging for various reasons, particularly since the learning problem contains the infamous graph isomorphism problem, for which the best known algorithm, given in a breakthrough result by Babai , runs in quasi-polynomial time. A large class of GNNs can thus only learn _incomplete_ graph invariants for scalability purposes. In fact, standard GNNs are known to be at most as expressive as the 1-dimensional Weisfeiler-Leman algorithm (1-WL) in terms of distinguishing power . There are simple non-isomorphic pairs of graphs, such as the pair shown in Figure 1, which cannot be distinguished by 1-WL and by a large class of GNNs. Thislimitation motivated a large body of work aiming to explain and overcome the expressiveness barrier of these architectures [1; 5; 7; 9; 11; 15; 37; 42; 43; 45; 50].

The expressiveness limitations of standard GNNs already apply on planar graphs, since, e.g., the graphs \(G_{1}\) and \(G_{2}\) from Figure 1 are planar. There are, however, efficient and complete graph isomorphism testing algorithms for planar graphs [28; 29; 41], which motivates an aligned design of dedicated architectures with better properties over planar graphs. Building on this idea, we propose architectures for _efficiently_ learning _complete_ invariants over planar graphs.

The contributions of this work can be summarized as follows:

* Building on the classical literature, we introduce PlanE as a framework for learning _isomorphism-complete_ invariant functions over planar graphs and derive the BasePlanE architecture (Section 5). We prove that BasePlanE is a _scalable_, _complete_ learning algorithm on planar graphs: BasePlanE can distinguish _any_ pair of non-isomorphic planar graphs (Section 6).
* We conduct an empirical analysis for BasePlanE evaluating its _expressive power_ on three tasks (Section 7.1), and validating its _performance_ on real-world graph classification (Section 7.2) and regression benchmarks (Section 7.3 and 7.4). The empirical results support the presented theory, and also yield multiple state-of-the-art results for BasePlanE on molecular datasets.

The proofs and additional experimental details are delegated to the appendix of this paper.

## 2 A primer on graphs, invariants, and graph neural networks

**Graphs.** Consider simple, undirected graphs \(G=(V,E,)\), where \(V\) is a set of nodes, \(E V V\) is a set of edges, and \(:V\) is a (coloring) function. If the range of this map is \(=^{d}\), we refer to it as a \(d\)-dimensional _feature map_. A graph is _connected_ if there is a path between any two nodes and disconnected otherwise. A graph is _biconnected_ if it cannot become disconnected by removing any single node. A graph is _triconnected_ if the graph cannot become disconnected by removing any two nodes. A graph is _planar_ if it can be drawn on a plane such that no two edges intersect.

**Components.** A _biconnected component_ of a graph \(G\) is a maximal biconnected subgraph. Any connected graph \(G\) can be efficiently decomposed into a tree of biconnected components called the _Block-Cut tree_ of \(G\), which we denote as BlockCut\((G)\). The blocks are attached to each other at shared nodes called _cut nodes_. A _triconnected component_ of a graph \(G\) is a maximal triconnected subgraph. Triconnected components of a graph \(G\) can also be compiled (very efficiently ) into a tree, known as the _SPQR tree_, which we denote as Spqr\((G)\). Given a graph \(G\), we denote by \(^{G}\) the set of all SPQR components of \(G\) (i.e., nodes of Spqr\((G)\)), and by \(^{G}\) the set of all biconnected components of \(G\). Moreover, we denote by \(^{G}_{u}\) the set of all SPQR components of \(G\) where \(u\) appears as a node, and by \(^{G}_{u}\) the set of all biconnected components of \(G\) where \(u\) appears as a node.

**Labeled trees.** We sometimes refer to rooted, undirected, labeled trees \(=(V,E,)\), where the canonical root node is given as one of tree's _centroids_: a node with the property that none of its branches contains more than half of the other nodes. We denote by root\(()\) the canonical root of \(\), and define the _depth_\(d_{u}\) of a node \(u\) in the tree as the node's minimal distance from the canonical root. The _children_ of a node \(u\) is the set \((u)=\{v|(u,v) E,d_{v}=d_{u}+1\}\). The _descendants_ of a node

Figure 1: Two graphs indistinguishable by \(1\)-WL.

Figure 2: Acid anhydride as a planar graph.

\(u\) is given by set of all nodes reachable from \(u\) through a path of length \(k 0\) such that the node at position \(j+1\) has one more depth than the node at position \(j\), for every \(0 j k\). Given a rooted tree \(\) and a node \(u\), the _subtree_ of \(\) rooted at node \(u\) is the tree induced by the descendants of \(u\), which we donote by \(_{u}\). For technical convenience, we allow the induced subtree \(_{u}\) of a node \(u\) even if \(u\) does not appear in the tree \(\), in which case \(_{u}\) is the empty tree.

**Node and graph invariants.** An _isomorphism_ from a graph \(G=(V,E,)\) to a graph \(G^{}=(V^{},E^{},^{})\) is a bijection \(f:V V^{}\) such that \((u)=^{}(f(u))\) for all \(u V\), and \((u,v) E\) if and only if \((f(u),f(v)) E^{}\), for all \(u,v V\). A _node invariant_ is a function \(\) that associates with each graph \(G=(V,E,)\) a function \((G)\) defined on \(V\) such that for all graphs \(G\) and \(G^{}\), all isomorphisms \(f\) from \(G\) to \(G^{}\), and all nodes \(u V\), it holds that \((G)(u)=(G^{})(f(u))\). A _graph invariant_ is a function \(\) defined on graphs such that \((G)=(G^{})\) for all isomorphic graphs \(G\) and \(G^{}\). We can derive a graph invariant \(\) from a node invariant \(^{}\) by mapping each graph \(G\) to the multiset \(\{^{}(G)(u) u V\}\). We say that a graph invariant \(\)_distinguishes_ two graphs \(G\) and \(G^{}\) if \((G)(G^{})\). If a graph invariant \(\) distinguishes \(G\) and \(G^{}\) then there is no isomorphism from \(G\) to \(G^{}\). If the converse also holds, then \(\) is a _complete_ graph invariant. We can speak of (in)completeness of invariants on special classes of graphs, e.g., 1-WL computes an incomplete invariant on general graphs, but it is well-known to compute a complete invariant on trees .

**Message passing neural networks.** A vast majority of GNNs are instances of _message passing neural networks (MPNNs)_. Given an input graph \(G=(V,E,)\), an MPNN sets, for each node \(u V\), an initial node representation \((u)=_{u}^{(0)}\), and iteratively computes representations \(_{u}^{()}\) for a fixed number of layers \(0 L\) as:

\[_{u}^{(+1)}(_{u}^{()}, (_{u}^{()},\{\!\{_{v}^{()}|\;v N_{ u}\}\!\})),\]

where \(\) and \(\) are respectively _update_ and _aggregation_ functions, and \(N_{u}\) is the neighborhood of \(u\). Node representations can be _pooled_ to obtain graph-level embeddings by, e.g., summing all node embeddings. We denote by \(^{(L)}\) the resulting graph-level embeddings. In this case, an MPNN can be viewed as an encoder that maps each graph \(G\) to a representation \(_{G}^{(L)}\), computing a graph invariant.

## 3 Related work

The expressive power of MPNNs is upper bounded by 1-WL [45; 63] in terms of distinguishing graphs, and by the logic \(^{2}\) in terms of capturing functions over graphs , motivating a body of work to improve on these bounds. One notable direction has been to enrich node features with unique node identifiers [42; 65], random discrete colors , or random noisy dimensions [1; 50]. Another line of work proposes _higher-order_ architectures [37; 43; 44; 45] based on higher-order tensors , or a higher-order form of message passing , which typically align with a \(k\)-dimensional WL test4, for some \(k>1\). Higher-order architectures are not scalable, and most existing models are upper bounded by 2-WL (or, _oblivious_ 3-WL). Another body of work is based on sub-graph sampling [7; 9; 11; 56], with pre-set sub-graphs used within model computations. These approaches can yield substantial expressiveness improvements, but they rely on manual sub-graph selection, and require running expensive pre-computations. Finally, MPNNs have been extended to incorporate other graph kernels, i.e., shortest paths [2; 64], random walks [46; 47] and nested color refinement .

The bottleneck limiting the expressiveness of MPNNs is the implicit need to perform graph isomorphism checking, which is challenging in the general case. However, there are classes of graphs, such as planar graphs, with efficient and complete isomorphism algorithms, thus eliminating this bottleneck. For planar graphs, the first complete algorithm for isomorphism testing was presented by Hopcroft and Tarjan , and was followed up by a series of algorithms [10; 19; 28; 52]. Kukluk et al.  presented an algorithm, which we refer to as KHC, that is more suitable for practical applications, and that we align with in our work. As a result of this alignment, our approach is the first _efficient_ and _complete_ learning algorithm on planar graphs. Observe that 3-WL (or, _oblivious_ 4-WL) is also known to be complete on planar graphs , but this algorithm is far from being scalable  and as a result there is no neural, learnable version implemented. By contrast, our architecture learns representations of efficiently computed components and uses these to obtain refined representations. Our approach extends the inductive biases of MPNNs based on structures, such as biconnected components, which are recently noted to be beneficial in the literature .

A practical planar isomorphism algorithm

The idea behind the KHC algorithm is to compute a canonical code for planar graphs, allowing us to reduce the problem of isomorphism testing to checking whether the codes of the respective graphs are equal. Importantly, we do not use the codes generated by KHC in our model, and view codes as an abstraction through which alignment with KHC is later proven. Formally, we can define a code as a string over the alphabet \(\): for each graph, the KHC algorithm computes codes for various components resulting from decompositions, and gradually builds a code representation for the graph. For readability, we allow reserved symbols \("(",")"\) and \(","\) in the generated codes. We present an overview of KHC and refer to Kukluk et al.  for details. We can assume that the planar graphs are connected, as the algorithm can be extended to disconnected graphs by independently computing the codes for each of the components, and then concatenating them in their lexicographical order.

**Generating a code for the the graph.** Given a connected planar graph \(G=(V,E,)\), KHC decomposes \(G\) into a Block-Cut tree \(=(G)\). Every node \(u\) is either a cut node of \(G\), or a virtual node associated with a biconnected component of \(G\). KHC iteratively removes the leaf nodes of \(\) as follows: if \(u\) is a leaf node associated with a biconnected component \(B\), KHC uses a subprocedure BiCode to compute the canonical code \((_{u})=(B)\) and removes \(u\) from the tree; otherwise, if the leaf node \(u\) is a cut node, KHC overrides the initial code \(((\{u\},\{\}))\), using an aggregate code of the removed biconnected components in which \(u\) occurs as a node. This procedure continues until there is a single node left, and the code for this remaining node is taken as the code of the entire connected graph. This process yields a complete graph invariant. Conceptually, it is more convenient for our purposes to reformulate this procedure as follows: we first canonically root \(\), and then code the subtrees in a _bottom-up procedure_. Specifically, we iteratively generate codes for subtrees and the final code for \(G\) is then simply the code of \(_{()}\).

**Generating a code for biconnected components.** KHC relies on a subprocedure BiCode to compute a code, given a biconnected planar graph \(B\). Specifically, it uses the SPQR tree \(=(B)\) which uniquely decomposes a biconnected component into a tree with virtual nodes of one of four types: \(S\) for _cycle_ graphs, \(P\) for _two-node dipole_ graphs, \(Q\) for a graph that has a _single edge_, and finally \(R\) for a triconnected component that is _not_ a dipole or a cycle. We first generate codes for the induced sub-graphs based on these virtual nodes. Then the SPQR tree is canonically rooted, and similarly to the procedure on the Block-Cut tree, we iteratively build codes for the subtrees of \(\) in a bottom-up fashion. Due to the simpler structure of the SPQR tree, instead of making overrides, the recursive code generation is done by prepending a number \((C,C^{})\) for a parent SPQR tree node \(C\) and each \(C^{}(C)\). This number is generated based on the way \(C\) and \(C^{}\) are connected in \(B\). Generating codes for the virtual \(P\) and \(Q\) nodes is trivial. For \(S\) nodes, we use the lexicographically smallest ordering of the cycle, and concatenate the individual node codes. However, for \(R\) nodes we require a more complex procedure and this is done using Weinberg's algorithm as a subroutine.

**Generating a code for triconnected components.** Whitney  has shown that triconnected graphs have only two planar embeddings, and Weinberg introduced an algorithm that computes a canonical code for triconnected planar graphs  which we call TriCode. This code is used as one of the building blocks of the KHC algorithm and can be extended to labeled triconnected planar graphs, which is essential for our purposes. Weinberg's algorithm  generates a canonical order for a triconnected component \(T\), by traversing all the edges in both directions via a walk. Let \(\) be the sequence of visited nodes in this particular walk and write \([i]\) to denote \(i\)-th node in it. This walk is then used to generate a sequence \(\) of same length, that corresponds to the order in which we first visit the nodes: for each node \(u=[i]\) that occurs in the walk, we set \([i]=1+|\{[j] j<i\}|\) if \([i]\) is the first occurrence of \(u\), or \([i]=[\{j[i]=[j]\}]\) otherwise. For example, the walk \(= v_{1},v_{3},v_{2},v_{3},v_{1}\) yields \(= 1,2,3,2,1\). Given such a walk of length \(k\) and a corresponding sequence \(\), we compute the following canonical code: \((T)=(,((\{\},\{\}))),,( [k],((\{[k]\},\{\})))\).

## 5 PlanE: Representation learning over planar graphs

KHC generates a unique code for every planar graph in a hierarchical manner based on the decompositions. Our framework aligns with this algorithm: given an input graph \(G=(V,E,)\), PlanE first computes the BlockCut and Spqr trees (\(-\)), and then learns representations for the nodes, the components, and the whole graph (\(-\)), as illustrated in Figure 3.

Formally, PlanE sets the initial node representations \(_{u}^{(0)}=(u)\) for each node \(u V\), and computes, for every layer \(1 L\), the representations \(}_{C}^{()}\) of Spqr components of \(^{G}\), the representations \(}_{B}^{()}\) of biconnected components \(B^{G}\), the representations \(}_{_{u}}^{()}\) of subtrees in the Block-Cut tree \(=(G)\) for each node \(u V\), and the representations \(_{u}^{()}\) of nodes as:

\[}_{C}^{()} =C,\{(_{u}^{(-1)},u)|\;u C\} \] \[}_{B}^{()} =B,\{(}_{C}^{()},C)|\;C ^{B}\}\] \[}_{_{u}}^{()} =_{u},\{(_{v}^{(-1)},v)|\;v  V\},\{(}_{B}^{()},B)|\;B^{G}\}\] \[_{u}^{()} =_{u}^{(-1)},\{}_{v}^{( -1)}|\;v N_{u}\},\{}_{v}^{(-1)}|\;v V\},\{}_{T}^{()}|\;T_{u}^{G}\},\{}_{B}^{( )}|\;B_{u}^{G}\},}_{_{u}}^{()}\]

where TriEnc, BiEnc, and CutEnc are invariant encoders and \(\) is an Update function. The encoders TriEnc and BiEnc are analogous to TriCode and BiCode of the KHC algorithm. In PlanE, we further simplify the final graph code generation, by learning embeddings for the cut nodes, which is implemented by the CutEnc encoder. For graph-level tasks, we apply a _pooling_ function on final node embeddings to obtain a graph-level embedding \(_{G}^{(L)}\).

There are many choices for deriving PlanE architectures, but we propose a simple model, BasePlanE, to clearly identify the virtue of the model architecture which aligns with KHC, as follows:

**TriEnc.** Given an Spqr component \(C\) and the node representations \(_{u}^{(-1)}\) of each node \(u\), TriEnc encodes \(C\) based on the walk \(\) given by Weinberg's algorithm, and its corresponding sequence \(\) as:

\[}_{C}^{()}=(_{i=1}^{||}(_{[i]}^{(-1)}\|_{[i]}\|_{i}) )\,,\]

where \(_{x}^{d}\) is the positional embedding . This is a simple sequence model with a positional encoding on the walk, and a second one based on the generated sequence \(\). Edge features can also be

Figure 3: Given an input graph (A) PlanE computes the BlockCut (B) and Spqr (C) trees and assigns initial embeddings to SPQR components using TriEnc (D). Then, BiEnc iteratively traverses each Spqr tree bottom-up (numbered), updates Spqr embeddings, and computes biconnected component embeddings (E). CutEnc then analogously traverses the BlockCut tree to compute a graph embedding (F). Finally, the node representation is updated (G).

concatenated while respecting the edge order given by the walk. The nodes of \((G)\) are one of the types \(S\), \(P\), \(Q\), \(R\), where for \(S\), \(P\), \(Q\) types, we have a trivial ordering for the induced components, and Weinberg's algorithm also gives an ordering for \(R\) nodes that correspond to triconnected components.

**BiEnc.** Given a biconnected component \(B\) and the representations \(_{C}^{()}\) of each component induced by a node \(C\) in \(=(B)\), BiEnc uses the SPQR tree and the integers \((C,C^{})\) corresponding to how we connect \(C\) and \(C^{}(C)\). BiEnc then computes a representation for each subtree \(_{C}\) induced by a node \(C\) in a bottom up fashion as:

\[}_{_{C}}^{()}=(}_{C }^{()}+_{C^{}(C)}(}_{ _{C^{}}}^{()}\|_{(C,C^{})})).\]

This encoder operates in a bottom up fashion to ensure that a subtree representation of the children of \(C\) exists before it encodes the subtree \(_{C}\). The representation of the canonical root node in \(\) is used as the representation of the biconnected component \(B\) by setting: \(}_{B}^{()}=}_{_{( )}}^{()}\).

**CutEnc.** Given a subtree \(_{u}\) of a Block-Cut tree \(\), the representations \(}_{B}^{()}\) of each biconnected component \(B\), and the node representations \(_{u}^{(-1)}\) of each node, CutEnc sets \(}_{_{u}}^{()}=^{d()}\) if \(u\) is _not_ a cut node; otherwise, it computes the subtree representations as:

\[}_{_{u}}^{()}=(_{u}^{(- 1)}+_{B(u)}(}_{B}^{()}+ _{v(B)}}_{_{v}}^{()})).\]

The CutEnc procedure is called in a bottom-up order to ensure that the representations of the grandchildren are already computed. We learn the cut node subtree representations instead of employing the hierarchical overrides that are present in the KHC algorithm, as the latter is not ideal in a learning algorithm. However, with sufficient layers, these representations are complete invariants.

**Update.** Putting these altogether, we update the node representations \(_{u}^{()}\) of each node \(u\) as:

\[_{u}^{()}=f^{()}g_{1}^{()} _{u}^{(-1)}+_{v N_{u}}_{v}^{(-1)} g_{2}^{ ()}_{v V}_{v}^{(-1)}\] \[g_{3}^{()}_{u}^{(-1)}+_{C_{u} ^{G}}}_{C}^{()} g_{4}^{()} _{u}^{(-1)}+_{B_{u}^{G}}}_{B}^{()} }_{_{u}}^{()},\]

where \(f^{()}\) and \(g_{i}^{()}\) are either linear maps or two-layer MLPs. Finally, we pool as:

\[_{G}=(_{=1}^{L}_{u V^{G}} {h}_{u}^{()}).\]

## 6 Expressive power and efficiency of BasePlanE

We present the theoretical result of this paper, which states that BasePlanE can distinguish any pair of planar graphs, even when using only a logarithmic number of layers in the size of the input graphs:

**Theorem 6.1**.: _For any planar graphs \(G_{1}=(V_{1},E_{1},_{1})\) and \(G_{2}=(V_{2},E_{2},_{2})\), there exists a parametrization of BasePlanE with at most \(L=_{2}(\{|V_{1}|,|V_{2}|\})+1\) layers, which computes a complete graph invariant, that is, the final graph-level embeddings satisfy \(_{G_{1}}^{(L)}_{G_{2}}^{(L)}\) if and only if \(G_{1}\) and \(G_{2}\) are not isomorphic._

The construction is non-uniform, since the number of layers needed depends on the size of the input graphs. In this respect, our result is similar to other results aligning GNNs with 1-WL with sufficiently many layers [45; 63]. There are, however, two key differences: (i) BasePlanE computes isomorphism-complete invariants over planar graphs and (ii) our construction requires only logarithmic number of layers in the size of the input graphs (as opposed to linear).

The theorem builds on the properties of each encoder being complete. We first show that a single application of TriEnc and BiEnc is sufficient to encode all relevant components of an input graph in an isomorphism-complete way:

**Lemma 6.2**.: _Let \(G=(V,E,)\) be a planar graph. Then, for any biconnected components \(B,B^{}\) of \(G\), and for any SPQR components \(C\) and \(C^{}\) of \(G\), there exists a parametrization of the functions TriEnc and BiEnc such that:_

1. \(}_{B}}_{B^{}}\) _if and only if_ \(B\) _and_ \(B^{}\) _are not isomorphic, and_
2. \(}_{C}}_{C^{}}\) _if and only if_ \(C\) _and_ \(C^{}\) _are not isomorphic._

Intuitively, this result follows from a natural alignment to the procedures of the KHC algorithm: the existence of unique codes for different components is proven for the algorithm and we lift this result to the embeddings of the respective graphs, using the universality of MLPs .

Our main result rests on a key result related to CutEnc, which states that BasePlanE computes complete graph invariants for all subtrees of the Block-Cut tree. We use an inductive proof, where the logarithmic bound stems from a single layer computing complete invariants for all subtrees induced by cut nodes that have at most one grandchild cut node, the induced subtree of which is incomplete.

**Lemma 6.3**.: _For a planar graph \(G=(V,E,)\) of order \(n\) and its associated Block-Cut tree \(=(G)\), there exists a \(L=_{2}(n)+1\) layer parametrization of BasePlanE that computes a complete graph invariant for each subtree \(_{u}\) induced by each cut node \(u\)._

With Lemma 6.2 and Lemma 6.3 in place, Theorem 6.1 follows from the fact that every biconnected component and every cut node of the graph are encoded in an isomorphism-complete way, which is sufficient for distinguishing planar graphs.

**Runtime efficiency.** Theoretically, the runtime of one BasePlanE layer is \((|V|d^{2})\) with a (one-off) pre-processing time \((|V|^{2})\) as we elaborate in Appendix B. In practical terms, this makes BasePlanE linear in the number of graph nodes after preprocessing. This is very scalable as opposed to other complete algorithms based on 3-WL (or, oblivious 4-WL).

## 7 Experimental evaluation

In this section, we evaluate BasePlanE in three different settings. First, we conduct three experiments to evaluate the expressive power of BasePlanE. Second, we evaluate a BasePlanE variant using edge features on the MolHIV graph classification task from OGB . Finally, we evaluate this variant on graph regression over ZINC  and QM9 . We provide an additional experiment on the runtime of BasePlanE in Appendix B, and an ablation study on ZINC in Appendix C. All experimental details, including hyperparameters, can be found in Appendix D5.

### Expressiveness evaluation

#### 7.1.1 Planar satisfiability benchmark: EXP

**Experimental setup.** We evaluate BasePlanE on the planar EXP benchmark  and compare with standard MPNNs, MPNNs with random node initialization and (higher-order) 3-GCNs . EXP consists of planar graphs which each represent a satisfiability problem (SAT) instance. These instances are grouped into pairs, such that these pairs cannot be distinguished by 1-WL, but lead to different SAT outcomes. The task in EXP is to predict the satisfiability of each instance. To obtain above-random performance on this dataset, a model must have a sufficiently strong expressive power (2-WL or more). To conduct this experiment, we use a 2-layer BasePlanE model with 64-dimensional node embeddings. We instantiate the triconnected component encoder with 16-dimensional positional encodings, each computed using a periodicity of 64.

   Model & Accuracy (\%) \\  GCN & \(50.0_{ 0.00}\) \\ GCN-RNI(N) & \(98.0_{ 1.85}\) \\
3-GCN & \(99.7_{ 0.004}\) \\  BasePlanE & \(_{ 0.00}\) \\   

Table 1: Accuracy results on EXP. Baselines are from Abboud et al. .

**Results.** All results are reported in Table 1. As expected, BasePlanE perfectly solves the task, achieving a performance of 100% (despite not relying on any higher-order method). BasePlanE solely relies on classical algorithm component decompositions, and does not rely on explicitly selected and designed features, to achieve this performance gain. This experiment highlights that the general algorithmic decomposition effectively improves expressiveness in a practical setup, and leads to strong performance on EXP, where a standard MPNN would otherwise fail.

#### 7.1.2 Planar 3-regular graphs: P3R

**Experimental setup.** We propose a new synthetic dataset P3R based on 3-regular planar graphs, and experiment with BasePlanE, GIN and 2-WL-expressive PPGN . For this experiment, we generated all 3-regular planar graphs of size 10, leading to exactly 9 non-isomorphic graphs. For each such graph, we generated 50 isomorphic graphs by permuting their nodes. The task is then to predict the correct class of an input graph, where the random accuracy is approximately \(11.1\%\). This task is challenging given the regularity of the graphs.

**Results.** We report all results in Table 2. As expected, GIN struggles to go beyond a random guess, whereas BasePlanE and PPGNs easily solve the task, achieving 100% accuracy.

#### 7.1.3 Clustering coefficients of QM9 graphs: QM9CC

In this experiment, we evaluate the ability of BasePlanE to detect structural graph signals _without_ an explicit reference to the target structure. To this end, we propose a simple, yet challenging, synthetic task: given a subset of graphs from QM9, we aim to predict the graph-level _clustering coefficient (CC)_. Computing CC requires counting triangles in the graph, which is impossible to solve with standard MPNNs .

**Data.** We select a subset QM9CC of graphs from QM9 to obtain a diverse distribution of CCs. As most QM9 graphs have a CC of 0, we consider graphs with a CC in the interval \([0.06,0.16]\), as this range has high variability. We then normalize the CCs to the unit interval \(\). We apply the earlier filtering on the original QM9 splits to obtain train/validation/test sets that are direct subsets of the full QM9 splits, and which consist of 44226, 3941 and 3921 graphs, respectively.

**Experimental setup.** Given the small size of QM9 and the locality of triangle counting, we use 32-dimensional node embeddings and 3 layers across all models. Moreover, we use a common 100 epoch training setup for fairness. For evaluation, we report mean absolute error (MAE) on the test set, averaged across 5 runs. For this experiment, our baselines are (i) an input-agnostic constant prediction that returns a minimal test MAE, (ii) the MPNNs GCNs  and GIN , (iii) ESAN , an MPNN that computes sub-structures through node and edge removals, but which does _not_ explicitly extract triangles, and (iv) BasePlanE, using 16-dimensional positional encoding vectors.

**Results.** Results on QM9CC are provided in Table 3. BasePlanE comfortably outperforms standard MPNNs. Indeed, GCN performance is only marginally better than the constant baseline and GIN's MAE is over an order of magnitude behind BasePlanE. This is a very substantial gap, and confirms that MPNNs are unable to accurately detect triangles to compute CCs. Moreover, BasePlanE achieves an MAE over 40% lower than ESAN. Overall, BasePlanE effectively detects triangle structures, despite these not being explicitly provided, and thus its underlying algorithmic decomposition effectively captures latent structural graph properties in this setting.

**Performance analysis.** To better understand our results, we visualize the predictions of BasePlanE, GIN, and GCN using scatter plots in Figure 4. As expected, BasePlanE follows the ideal regression line. By contrast, GIN and GCN are much less stable. Indeed, GIN struggles with CCs at the extremes of the \(\) range, but is better at intermediate values, whereas GCN is consistently unreliable, and rarely returns predictions above 0.7. This highlights the structural limitation of GCNs, namely its self-loop mechanism for representation updates, which causes ambiguity for detecting triangles.

   Model & Accuracy (\%) \\  GIN & 11.1 \( 0.00\) \\ PPGN & **100**\( 0.00\) \\  BasePlanE & **100**\( 0.00\) \\   

Table 2: Accuracy results on P3R.

   Model & MAE \\  Constant & 0.1627 \( 0.0000\) \\ GCN & 0.1275 \( 0.0012\) \\ GIN & 0.0612 \( 0.0018\) \\ ESAN & \(0.0038 0.0010\) \\ BasePlanE & **0.0023**\( 0.0004\) \\   

Table 3: MAE of BasePlanE and baselines on the QM9CC dataset.

### Graph classification on MolHIV

**Model setup.** We use a BasePlanE variant that uses edge features, called E-BasePlane (defined in the appendix), on OGB  MolHIV and compare against baselines. We instantiate E-BasePlane with an embedding dimension of 64, 16-dimensional positional encodings, and report the average ROC-AUC across 10 independent runs.

**Results.** The results for E-BasePlanE and other baselines on MolHIV are shown in Table 4: Despite not explicitly extracting relevant cycles and/or molecular sub-structures, E-BasePlane outperforms standard MPNNs and the domain-specific HIMP model. It is also competitive with substructure-aware models CIN and GSN, which include dedicated structures for inference. Therefore, E-BasePlane performs strongly in practice with minimal design effort, and effectively uses its structural inductive bias to remain competitive with dedicated architectures.

### Graph regression on QM9

**Experimental setup.** We map QM9  edge types into features by defining a learnable embedding per edge type, and subsequently apply E-BasePlane to the dataset. We evaluate E-BasePlane on all 13 QM9 properties following the same splits and protocol (with MAE results averaged over 5 test set reruns) of GNN-FiLM . We compare R-SPN against GNN-FiLM models and their fully adjacent (FA) variants , as well as shortest path networks (SPNs) . We report results with an 3-layer E-BasePlane using 128-dimensional node embeddings and 32-dimensional positional encodings.

**Results.** E-BasePlane results on QM9 are provided in Table 5. In this table, E-BasePlane outperforms high-hop SPNs, despite being simpler and more efficient, achieving state-of-the-art results on 9 of the 13 tasks. The gains are particularly prominent on the first five properties, where R-SPNs originally provided relatively little improvement over FA models, suggesting that E-BasePlane offers complementary structural advantages to SPNs. This was corroborated in our experimental tuning: E-BasePlane performance peaks around 3 layers, whereas SPN performance continues to improve up to 8 (and potentially more) layers, which suggests that E-BasePlane is more efficient at directly communicating information, making further message passing redundant.

Overall, E-BasePlane maintains the performance levels of R-SPN with a smaller computational footprint. Indeed, messages for component representations efficiently propagate over trees in E-BasePlane, and the number of added components is small (see appendix for more details). Therefore E-BasePlane and the PlanE framework offer a more scalable alternative to explicit higher-hop neighborhood message passing over planar graphs.

   GCN  & \(75.58_{ 0.97}\) \\ GIN  & \(77.07_{ 1.40}\) \\ PNA  & \(79.05_{ 1.32}\) \\  ESAN  & \(78.00_{ 1.42}\) \\ GSN  & \(80.39_{ 0.90}\) \\ CIN  & \(}\) \\  HIMP  & \(78.80_{ 0.82}\) \\  E-BasePlane & \(80.04_{ 0.50}\) \\   

Table 4: ROC-AUC of BasePlane and baselines on MolHIV.

Figure 4: Scatter plots of BasePlane, GIN, and GCN predictions versus the true normalized CC. The red line represents ideal behavior where predictions match the true normalized CC.

### Graph regression on ZINC

**Experimental setup.** We (i) evaluate BasePlanE on the ZINC subset (12k graphs) without edge features, (ii) evaluate E-BasePlanE on this subset and on the full ZINC dataset (500k graphs). To this end, we run BasePlanE and E-BasePlanE with 64 and 128-dimensional embeddings, 16-dimensional positional embeddings, and 3 layers. For evaluation, we compute MAE on the respective test sets, and report the best average of 10 runs across all experiments.

**Results.** Results on ZINC are shown in Table 6: Both BasePlanE and E-BasePlanE perform strongly, with E-BasePlanE achieving state-of-the-art performance on ZINC12k with edge features and both models outperforming all but one baseline in the other two settings. These results are very promising, and highlight the robustness of (E-)BasePlanE.

## 8 Limitations, discussions, and outlook

Overall, both BasePlanE and E-BasePlanE perform strongly across all our experimental evaluation tasks, despite competing against specialized models in each setting. Moreover, both models are isomorphism-complete over planar graphs. This implies that these models benefit substantially from the structural inductive bias and expressiveness of classical planar algorithms, which in turn makes them a reliable, efficient, and robust solution for representation learning over planar graphs.

Though the PlanE framework is a highly effective and easy to use solution for planar graph representation learning, it is currently limited to planar graphs. Indeed, the classical algorithms underpinning PlanE do not naturally extend beyond the planar graph setting, which in turn limits the applicability of the approach. Thus, a very important avenue for future work is to explore alternative (potentially incomplete) graph decompositions that strike a balance between structural inductive bias, efficiency and expressiveness on more general classes of graphs.

    &  &  &  \\  & No & Yes & Yes \\  GCN  & \(0.278 0.003\) & - & - \\ GIN-(E)  & \(0.387 0.015\) & \(0.252 0.014\) & \(0.088 0.002\) \\ PNA  & \(0.320 0.032\) & \(0.188 0.004\) & \(0.320 0.032\) \\  GSN  & \(0.140 0.006\) & \(0.101 0.010\) & - \\ CIN  & \( 0.003\) & \(0.079 0.006\) & \( 0.002\) \\ ESAN  & - & \(0.102 0.003\) & - \\  HIMP  & - & \(0.151 0.006\) & \(0.036 0.002\) \\  (E-)BasePlanE & \(0.124 0.004\) & \( 0.003\) & \(0.028 0.002\) \\   

Table 6: MAE of (E-)BasePlanE and baselines on ZINC.

    &  &  &  &  \\   & base & +FA & base & +FA & \(k=5\) & \(k=10\) & \\  mu & \(2.64 0.11\) & \(2.54 0.09\) & \(2.68 0.11\) & \(2.73 0.07\) & \(2.16 0.08\) & \(2.21 0.21\) & \( 0.03\) \\ alpha & \(4.67 0.52\) & \(2.28 0.04\) & \(4.65 0.44\) & \(2.32 0.16\) & \(1.74 0.05\) & \(1.66 0.06\) & \( 0.01\) \\ HOMO & \(1.42 0.01\) & \(1.26 0.02\) & \(1.48 0.03\) & \(1.43 0.02\) & \(1.19 0.04\) & \(1.20 0.08\) & \( 0.01\) \\ LUMO & \(1.50 0.09\) & \(1.34 0.04\) & \(1.53 0.07\) & \(1.41 0.03\) & \(1.13 0.01\) & \(1.20 0.06\) & \( 0.02\) \\ gap & \(2.27 0.09\) & \(1.96 0.04\) & \(2.31 0.06\) & \(2.08 0.05\) & \(1.76 0.03\) & \(1.77 0.06\) & \( 0.02\) \\ R2 & \(15.63 1.40\) & \(12.61 0.37\) & \(52.39 42.5\) & \(15.76 1.17\) & \(10.59 0.35\) & \(10.63 0.10\) & \( 0.55\) \\ ZPVE & \(12.93 1.81\) & \(5.03 0.36\) & \(14.87 2.88\) & \(5.98 0.43\) & \(3.16 0.06\) & \( 0.13\) & \(2.81 0.16\) \\ U0 & \(5.88 1.01\) & \(2.21 0.12\) & \(7.61 0.46\) & \(2.19 0.25\) & \(1.10 0.03\) & \( 0.05\) & \(0.95 0.04\) \\ U & \(18.71 23.36\) & \(2.32 0.18\) & \(6.86 0.53\) & \(2.11 0.10\) & \(1.09 0.05\) & \( 0.03\) & \(0.94 0.04\) \\ H & \(5.62 0.81\) & \(2.26 0.19\) & \(7.64 0.92\) & \(2.27 0.29\) & \(1.10 0.03\) & \( 0.03\) & \( 0.04\) \\ G & \(5.38 0.75\) & \(2.04 0.24\) & \(6.54 0.36\) & \(2.07 0.07\) & \(1.04 0.04\) & \( 0.05\) & \(0.88 0.04\) \\ Cv & \(3.53 0.37\) & \(1.86 0.03\) & \(4.11 0.27\) & \(2.03 0.14\) & \(1.34 0.03\) & \(1.23 0.06\) & \( 0.06\) \\ Omega & \(1.05 0.11\) & \(0.80 0.04\) & \(1.48 0.87\) & \(0.73 0.04\) & \(0.53 0.02\) & \(0.52 0.02\) & \( 0.01\) \\   

Table 5: MAE of E-BasePlanE and baselines on QM9. Other model results and their fully adjacent (FA) extensions are as previously reported [2; 3].