# Appendix

Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions

 Dongjie Wang

Department of CS

University of Central Florida

dongjie.wang@ucf.edu

&Meng Xiao

CNIC, CAS

University of CAS

shaow@cnic.cn

&Min Wu

Institute for Infocomm Research

A*STAR

wumin@i2r.a-star.edu.sg

&Pengfei Wang

CNIC, CAS

University of CAS

wpf@cnic.cn

&Yuanchun Zhou

CNIC, CAS

University of CAS

zyc@cnic.cn

&Yanjie Fu

School of Computing and AI

Arizona State University

yanjiefu@asu.edu

These authors have contributed equally to this work.Corresponding Author

###### Abstract

Feature transformation aims to generate new pattern-discriminative feature space from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature explosively grows on the basis of combinations of features and operations from low-order forms to high-order forms. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer from large search space. Overly emphasizing efficiency in algorithm design usually sacrifices stability or robustness. To fundamentally fill this gap, we reformulate discrete feature transformation as a continuous space optimization task and develop an embedding-optimization-reconstruction framework. This framework includes four steps: 1) reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding, intending to encapsulate the knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding search, dedicating to uncover potentially superior embeddings within the learned space; 4) transformation operation sequence reconstruction, striving to reproduce the feature transformation solution to pinpoint the optimal feature space. Finally, extensive experiments and case studies are performed to demonstrate the effectiveness and robustness of the proposed method. The code and data are publicly accessible https://www.dropbox.com/sh/imB&cui7va3k5u/AAC10egvx0HuyvYyoQs4WPn?dl=0.

## 1 Introduction

Feature transformation aims to derive a new feature space by mathematically transforming the original features to enhance the downstream ML task performances. However, feature transformation is usually manual, time-consuming, labor-intensive, and requires domain knowledge. These limitations motivate us to accomplish _Automated Feature Transformation_ (**AFT**). AFT is a fundamental task because AFT can 1) reconstruct distance measures, 2) form a feature space with discriminative patterns, 3) ease machine learning, and 4) overcome complex and imperfect data representation.

There are two main challenges in solving AFT: 1) efficient feature transformation in a massive discrete search space; 2) robust feature transformation in an open learning environment. Firstly, it is computationally costly to reconstruct the optimal feature space from a given feature set. Such reconstruction necessitates a transformation sequence that contains multiple combinations of features and operations. Each combination indicates a newly generated feature. Given the extensive array of features and operations, the quantity of possible feature-operation combinations exponentially grows, further compounded by the vast number of potential transformation operation sequences. The efficiency challenge seeks to answer: _how can we efficiently identify the best feature transformation operation sequence?_ Secondly, identifying the best transformation operation sequence is unstable and sensitive to many factors in an open environment. For example, if we formulate a transformation operation sequence as a searching problem, it is sensitive to starting points or the greedy strategy during iterative searching. If we identify the same task as a generation problem, it is sensitive to training data quality and the complexity of generation forms. The robustness challenge aims to answer: _how can we robustify the generation of feature transformation sequences?_

Prior literature partially addresses the two challenges. Existing AFT algorithms can be grouped into three categories: 1) expansion-reduction approaches , in which all mathematical operations are randomly applied to all features at once to generate candidate transformed features, followed by feature selection to choose valuable features. However, such methods are based on random generation, unstable, and not optimization-directed. 2) iterative-feedback approaches , in which feature generation and selection are integrated, and learning strategies for each are updated based on feedback in each iteration. Two example methods are Evolutionary Algorithms (EA) or Reinforcement Learning (RL) with downstream ML task accuracy as feedback. However, such methods are developed based on searching in a massive discrete space and are difficult to converge in comparison to solving a continuous optimization problem. 3) Neural Architecture Search (NAS)-based approaches . NAS was originally to identify neural network architectures using a discrete search space containing neural architecture parameters. Inspired by NAS, some studies formulated AFT as a NAS problem. However, NAS-based formulations are slow and limited in modeling all transformation forms. Existing studies show the inability to jointly address efficiency and robustness in feature transformation. Thus, we need a novel perspective to derive a novel formulation for AFT.

**Our Contribution: A Postfix Expression Embedding and Generation Perspective.** To fundamentally fill these gaps, we formulate the discrete AFT problem as a continuous optimization task and propose a reinforce**M**ent-enhanced ********a**ut**O**regressive ****f**e**A**ture **T**ransformation framework, namely **MOAT**. To advance efficiency and robustness, this framework implements four steps: 1) reinforcement-enhanced training data preparation; 2) feature transformation operation sequence embedding; 3) gradient-steered optimal embedding search; 4) beam search-based transformation operation sequence reconstruction. Step 1 is to collect high-quality transformation operation sequence-accuracy pairs as training data. Specifically, we develop a cascading reinforcement learning structure to automatically explore transformation operation sequences and test generated feature spaces on a downstream predictor (e.g., decision tree). The self-optimizing policies enable agents to collect high-quality transformation operation sequences. The key insight is that when training data is difficult or cost expensive to collect, reinforcement intelligence can be used as an automated training data collector. Step 2 is to learn a continuous embedding space from transformation-accuracy training data. Specifically, we describe transformation operation sequences as postfix expressions, each of which is mapped into an embedding vector by jointly optimizing the transformation operation sequence reconstruction loss and accuracy estimation loss. Viewing feature transformation through the lens of postfix expressions offers the chance to mitigate the challenges associated with an exponentially expanding discrete search problem. By recasting the feature transformation operation sequence in postfix form, the search space becomes smaller, as each generation step involves selecting a single alphabetical letter pertaining to a feature or mathematical operation, rather than considering high-order expansions. Moreover, this postfix form captures feature-feature interaction information and empowers the generation model with the capacity to autonomously determine the optimal number and segmentation way of generated features. Step 3 is to leverage the gradient calculated from the improvement of the accuracy evaluator to guide the search for the optimal transformation operation sequence embedding. Step 4 is to develop a beam search-based generative module to reconstruct feature transformation operation sequences from embedding vectors. Each sequence is used to obtain a transformed feature space, and subsequently, the transformed space that yields the highest performance in the downstream ML task is identified as the optimal solution. Finally, we present extensive experiments and case studies to show the effectiveness and superiority of our framework.

Definitions and Problem Statement

### Important Definitions

**Operation Set.** To refine the feature space for improving the downstream ML models, we need to apply mathematical operations to existing features to generate new informative features. All operations are collected in an operation set, denoted by \(\). Based on the computation property, these operations can be classified as unary operations and binary operations. The unary operations such as "square", "exp", "log", etc. The binary operations such as "plus", "multiply", "minus", etc.

**Cascading Agent Structure.** We create a cascading agent structure comprised of _head feature agent_, _operation agent_, and _tail feature agent_ to efficiently collect quantities of high-quality feature transformation records. The selection process of the three agents will share the state information and sequentially select candidate features and operations for refining the feature space.

**Feature Transformation Operation Sequence.** Assuming that \(D=\{X,y\}\) is a dataset, which includes the original feature set \(X=[f_{1},,f_{N}]\) and predictive targets \(y\). As shown in Figure 1, we transform the existing ones using mathematical compositions \(\) consisting of feature ID tokens and operations to generate new and informative features. \(K\) transformation compositions are adopted to refine \(X\) to a better feature space \(=[_{1},,_{K}]\). The collection of the \(K\) compositions refers to the feature transformation sequence, which is denoted by \(=[_{1},,_{K}]\).

### Problem Statement

We aim to develop an effective and robust deep differentiable automated feature transformation framework. Formally, given a dataset \(D=\{X,y\}\) and an operation set \(\), we first build a cascading RL-agent structure to collect \(n\) feature transformation accuracy pairs as training data, denoted by \(R=\{(_{i},v_{i})\}_{i=1}^{n}\), where \(_{i}\) is the transformation sequence and \(v_{i}\) is the associated downstream predictive performance. We pursue two objectives thereafter: 1) building an optimal continuous embedding space for feature transformation sequences. We learn a mapping function \(\), a reconstructing function \(\), and an evaluation function \(\) to convert \(R\) into a continuous embedding space \(\) via joint optimization. In \(\), each embedding point is associated with a feature transformation sequence and corresponding predictive performance. 2) identifying the optimal feature space. We adopt a gradient-based search to find the optimal feature transformation sequence \(^{*}\), given by:

\[^{*}=(^{*})=_{} ((()(X)),y),\] (1)

where \(\) can reconstruct a feature transformation sequence from any embedding point of \(\); \(\) is an embedding vector in \(\) and \(^{*}\) is the optimal one; \(\) is the downstream ML model and \(\) is the performance indicator. Finally, we apply \(^{*}\) to transform \(X\) to the optimal feature space \(X^{*}\) maximizing the value of \(\).

## 3 Methodology

### Framework Overview

Figure 2 shows the framework of MOAT including four steps: 1) reinforcement-enhanced transformation-accuracy data preparation; 2) deep postfix feature transformation operation sequence embedding; 3) gradient-ascent optimal embedding search; 4) transformation operation sequence reconstruction. In Step 1, a cascading agent structure consisting of two feature agents and one operation agent is developed to select candidate features and operators for feature crossing. The transformed feature sets are applied to a downstream ML task to collect the corresponding accuracy. The data collection process is automated and self-optimized by policies and feedback in reinforcement learning. We then convert these feature transformation operation sequences into postfix expressions. In Step 2, we develop an encoder-evaluator-decoder model to embed transformation operation sequence-accuracy pairs into a continuous embedding space by jointly optimizing the sequence reconstruction loss and performance evaluation loss. In detail, the encoder maps these transformation operation

Figure 1: An example of feature transformation sequence: \(_{(.)}\) indicates the generated feature that is the combination of original features and mathematical operations.

sequences into continuous embedding vectors; the evaluator assesses these embeddings by predicting their corresponding model performance; the decoder reconstructs the transformation sequence using these embeddings. In Step 3, we first learn the embeddings of top-ranking transformation operation sequences by the well-trained encoder. With these embeddings as starting points, we search along the gradient induced by the evaluator to find the acceptable optimal embeddings with better model performances. In Step 4, the well-trained decoder then decodes these optimal embeddings to generate candidate feature transformation operation sequences through the beam search. We apply the feature transformation operation sequences to original features to reconstruct refined feature spaces and evaluate the corresponding performances of the downstream predictive ML task. Finally, the feature space with the highest performance is chosen as the optimal one.

### Reinforcement Training Data Preparation

**Why Using Reinforcement as Training Data Collector.** Our extensive experimental analysis shows that the quality of embedding space directly determines the success of feature transformation operation sequence construction. The quality of the embedding space is sensitive to the quality and scale of transformation sequence-accuracy training data: training data is large enough to represent the entire distribution; training data include high-performance feature transformation cases, along with certain random exploratory samples. Intuitively, we can use random sample features and operations to generate feature transformation sequences. This strategy is inefficient because it produces many invalid and low-quality samples. Or, we can use existing feature transformation methods (e.g., AutoFeat ) to generate corresponding records. However, these methods are not fully automated and produce a limited number of high-quality transformation records without exploration ability. We propose to view reinforcement learning as a training data collector to overcome these limitations.

**Reinforcement Transformation-Accuracy Training Data Collection.** Inspired by [4; 9], we formulate feature transformation as three interdependent Markov decision processes (MDPs). We develop a cascading agent structure to implement the three MDPs. The cascading agent structure consists of a head feature agent, an operation agent, and a tail feature agent. In each iteration, the three agents collaborate to select two candidate features and one operation to generate a new feature. Feedback-based policy learning is used to optimize the exploratory data collection to find diversified yet quality feature transformation samples. To simplify the description, we adopt the \(i\)-th iteration as an example to illustrate the reinforcement data collector. Given the former feature space as \(X_{i}\), we generate new features \(X_{i+1}\) using the head feature \(f_{h}\), operation \(o\), and tail feature \(f_{t}\) selected by the cascading agent structure.

_1) Head feature agent._ This learning system includes: _State:_ is the vectorized representation of \(X_{i}\). Let \(Rep()\) be a state representation method, and the state can be denoted by \(Rep(X_{i})\). _Action:_ is the head feature \(f_{h}\) selected from \(X_{i}\) by the reinforced agent.

_2) Operation agent._ This learning system includes: _State:_ includes the representation of \(X_{i}\) and the head feature, denoted by \(Rep(X_{i}) Rep(f_{h})\), where \(\) indicates concatenation. _Action:_ is the operation \(o\) selected from the operation set \(\).

Figure 2: An overview of our framework. MOAT consists of four main components: 1) transformation-accuracy data preparation; 2) deep feature transformation embedding; 3) gradient-ascent optimal embedding search; 4) transformation sequence reconstruction and evaluation.

_3) Tail feature agent_. This learning system includes: _State:_ includes the representation of \(X_{i}\), selected head feature, and operation, denoted by \(Rep(X_{i}) Rep(f_{h}) Rep(o)\). _Action:_ is the tail feature \(f_{t}\) selected from \(X_{i}\) by this agent.

_4) State representation method, \(Rep()\)._ For the representation of the feature set, we employ a descriptive statistical technique to obtain the state with a fixed length. In detail, we first compute the descriptive statistics (_i.e._ count, standard deviation, minimum, maximum, first, second, and third quantile) of the feature set column-wise. Then, we calculate the same descriptive statistics on the output of the previous step. After that, we can obtain the descriptive matrix with shape \(^{7 7}\) and flatten it as the state representation with shape \(^{1 49}\). For the representation of the operation, we adopt its one-hot encoding as \(Rep(o)\).

_5) Reward function._ To improve the quality of the feature space, we use the improvement of a downstream ML task performance as the reward. Thus, it can be defined as: \((X_{i},X_{i+1})=((X_{i+1}),y)-( (X_{i}),y)\).

_6) Learning to Collect Training Data._ To optimize the entire procedure, we minimize the mean squared error of the Bellman Equation to get a better feature space. During the exploration process, we can collect amounts of high-quality records \((,v)\) for constructing an effective continuous embedding space, where \(\) is the transformation sequence, and \(v\) is the downstream model performance.

### Postfix Expressions of Feature Transformation Operation Sequences

**Why Transformation Operation Sequences as Postfix Expressions.** After training data collection, a question arises: how can we organize and represent these transformation operation sequences in a computationally-tangible and machine-learnable format?

Figure 3 (a) shows an example of a feature transformation operation sequence with two generated features. To convert this sequence into a machine-readable expression, a naive idea is to enclose each calculation in a pair of brackets to indicate its priority (Figure 3(b)). But, the representation of Figure 3(b) has four limitations: (1) _Redundancy._ Many priority-related brackets are included to ensure the unambiguous property and correctness of mathematical calculations. (2) _Semantic Sparsity._ The quantity of bracket tokens can dilute the semantic information of the sequence, making model convergence difficult. (3) _Illegal Transformation._ If the decoder makes one mistake on bracket generation, the entire generated sequence will be wrong. (4) _Large Search Space._ The number of combinations of features and operations from low-order to high-order interactions is large, making the search space too vast.

**Using Postfix Expression to Construct Robust, Concise, and Unambiguous Sequences.** To address the aforementioned limitations, we convert the transformation operation sequence \(\) into a postfix-based sequence expression. Specifically, we scan each mathematical composition \(\) in \(\) from left to right and convert it from the infix-based format to the postfix-based one. We then concatenate each postfix expression by the <SEP> token and add the <SOS> and <EOS> tokens to the beginning and end of the entire sequence. Figure 3(c) shows an example of such a postfix sequence. We denote it as \(=[_{1},,_{M}]\), where each element is a feature ID token, operation token, or three other unique tokens. The detailed pseudo code of this conversion process is provided in Appendix A.

The postfix sequences don't require numerous brackets to ensure the calculation priority. We only need to scan each element in the sequence from left to right to reconstruct corresponding transformed features. Such a concise and short expression can reduce sequential modeling difficulties and computational costs. Besides, a postfix sequence indicates a unique transformation process, thus, reducing the ambiguity of the feature transformation sequence. Moreover, the most crucial aspect is the reduction of the search space from exponentially growing discrete combinations to a limited token set \(\) that consists of the original feature ID tokens, operation tokens, and other three unique tokens. The length of the token set is \(||+|X|+3\), where \(||\) is the number of the operation set, \(|X|\) is the dimension of the original feature set, and \(3\) refers to the unique tokens <SOS>, <SEP>, <EOS>.

Figure 3: Compared to infix-based expressions, postfix-based expressions optimize token redundancy, enhance semantics, prevent illegal transformations, and minimize the search space.

**Data Augmentation for Postfix Transformation Sequences.** Big and diversified transformation sequence records can benefit the learning of a pattern discriminative embedding space. Be sure to notice that, a feature transformation sequence consists of many independent segmentations, each of which is the composition of feature ID and operation tokens and can be used to generate a new feature. These independent segmentations are order-agnostic. Our idea is to leverage this property to conduct data augmentation to increase the data volume and diversity. For example, given a transformation operation sequence and corresponding accuracy \(\{,v\}\), we first divide the postfix expression into different segmentations by <SEP>. We then randomly shuffle these segmentations and use <SEP> to concatenate them together to generate new postfix transformation sequences. After that, we pair the new sequences with the corresponding model accuracy performance to improve data diversity and data volume for better model training and to create the continuous embedding space.

### Deep Feature Transformation Embedding

After collecting and converting large-scale feature transformation training data to a set of postfix expression-accuracy pairs \(\{(_{i},v_{i})\}_{i=1}^{n}\), we develop an encoder-evaluator-decoder structure to map the sequential information of these records into an embedding space. Each embedding vector is associated with a transformation operation sequence and its corresponding model accuracy.

**Encoder \(\):** The Encoder aims to map any given postfix expression to an embedding (a.k.a., hidden state) \(\). We adopt a single layer long short-term memory  (LSTM) as Encoder and acquire the continuous representation of \(\), denoted by \(=()^{M d}\), where \(M\) is the total length of input sequence \(\) and \(d\) is the hidden size of the embedding.

**Decoder \(\):** The Decoder aims to reconstruct the postfix expression of the feature transformation operation sequence \(\) from the hidden state \(\). In MOAT, we set the backbone of the Decoder as a single-layer LSTM. For the first step, \(\) will take an initial state (denoted as \(h_{0}\)) as input. Specifically, in step-\(i\), we can obtain the decoder hidden state \(h_{d}^{d}\) from the LSTM. We use the dot product attention to aggregate the encoder hidden state and obtain the combined encoder hidden state \(h_{i}^{e}\). Then, the distribution in step-\(j\) can be defined as: \(P_{}(_{i}|,_{<i})=(h_{d}^{d}  h_{i}^{e}))}{_{c c}(W_{}(h_{d}^{d} h_{i}^{e}))}\), where \(_{i}\) is the \(i\)-th token in sequence \(\), and \(\) is the token set. \(W\) stand for the parameter of the feedforward network. \(_{<i}\) represents the prediction of the previous or initial step. By multiplying the probability in each step, we can form the distribution of each token in \(\), given as:\(P_{}(|)=_{i=1}^{M}P_{}(_{i}|, _{<i})\). To make the generated sequence similar to the real one, we minimize the negative log-likelihood of the distribution, defined as: \(_{rec}=- P_{}(|)\).

**Evaluator \(\):** The Evaluator is designed to estimate the quality of continuous embeddings. Specifically, we will first conduct mean pooling on \(\) by column to aggregate the information and obtain the embedding \(}^{d}\). Then \(}\) is input into a feedforward network to estimate the corresponding model performance, given as: \(=()\). To minimize the gap between estimated accuracy and real-world gold accuracy, we leverage the Mean Squared Error (MSE) given by: \(_{est}=(v,())\).

**Joint Training Loss \(\):** We jointly optimize the encoder, decoder, and evaluator. The joint training loss can be formulated as: \(=_{rec}+(1-)_{est}\), where \(\) is the trade-off hyperparameter that controls the contribution of sequence reconstruction and accuracy estimation loss.

### Gradient-Ascent Optimal Embedding Search

To conduct the optimal embedding search, we first select top-\(T\) transformation sequences ranked by the downstream predictive accuracy. The well-trained encoder is then used to embed these postfix expressions into continuous embeddings, which later will be used as seeds (starting points) of gradient ascent. Assuming that one search seed embedding is \(\), we search, starting from \(\), toward the gradient direction induced by the evaluator \(\): \(}=+}\), where \(}\) denotes the refined embedding, \(\) is the size of each searching step. The model performance of \(}\) is supposed to be better than \(\) due to \((})()\). For \(T\) seeds, we can obtain the enhanced embeddings \([}_{1},}_{2},,}_{T}]\).

### Transformation Operation Sequence Reconstruction and Evaluation

We reconstruct the transformation sequences by the well-trained decoder \(\) using the collected candidate (i.e., acceptable optimal) embeddings \([}_{1},}_{2},,}_{T}]\). This process can be denoted by: \([}_{1},}_{2},,}_{T}] \{_{i}\}_{i=1}^{T}\). To identify the best transformation sequence, we adopt thebeam search strategy [11; 12; 13] to generate feature transformation operation sequence candidates. Specifically, given a refined embedding \(}\), at step-t, we maintain the historical predictions with beam size \(b\), denoted as \(\{_{<t}^{i}\}_{i=1}^{b}\). For the \(i\)-th beam, the probability distribution of the token identified by the well-trained decoder \(\) at the \(t\)-th step is \(\), which can be calculated as follows: \(P_{t}^{i}()=P_{}(|},_{<t}^{i} )*P_{}(_{<t}^{i}|})\), where the probability distribution \(P_{t}^{i}()\) is the continued multiplication between the probability distribution of the previous decoding sequence and that of the current decoding step. We can collect the conditional probability distribution of all tokens for each beam. After that, we append tokens with top-\(b\) probability values to the historical prediction of each beam to get a new historical set \(\{_{<t+1}^{i}\}_{i=1}^{b}\). We can iteratively conduct this decoding process until confronted with the <EOS> token. We select the transformation sequence with the highest probability value as output. Hence, \(T\) enhanced embeddings may produce \(T\) transformation sequences \(\{_{i}\}_{i=1}^{T}\). We divide each of them into different parts according to the <SEP> token and check the validity of each part and remove invalid ones. Here, the validity measures whether the mathematical compositions represented by the postfix part can be successfully calculated to produce a new feature. These valid postfix parts reconstruct a feature transformation operation sequence \(\{_{i}\}_{i=1}^{T}\), which are used to generate refined feature space \(\{_{i}\}_{i=1}^{T}\). Finally, we select the feature set with the highest downstream ML performance as the optimal feature space \(^{*}\).

## 4 Experiments

This section reports the results of both quantitative and qualitative experiments that were performed to assess MOAT with other baseline models. All experiments were conducted on AMD EPYC 7742 CPU, and 8 NVIDIA A100 GPUs. For more platform information, please refer to Appendix B.2.

### Datasets and Evaluation Metrics

We used 23 publicly available datasets from UCI , LibSVM , Kaggle , and OpenML  to conduct experiments. The 23 datasets involve 14 classification tasks and 9 regression tasks. Table 1 shows the statistics of these datasets. We used F1-score, Precision, Recall, and ROC/AUC to evaluate classification tasks. We used 1-Relative Absolute Error (1-RAE) , 1-Mean Average Error (1-MAE), 1-Mean Square Error (1-MSE), and 1-Root Mean Square Error (1-RMSE) to evaluate regression tasks. We used the Valid Rate to evaluate the transformation sequence generation. A valid sequence means it can successfully conduct mathematical compositions without any ambiguity and errors. The valid rate is the average of all correct sequence numbers divided by the total number of generated sequences. The greater the valid rate is, the superior the model performance is. Because it indicates that the model can capture the complex patterns of mathematical compositions and search for more effective feature transformation sequences.

### Baseline Models

We compared our method with eight widely-used feature generation methods: (1) **RDG** generates feature-operation-feature transformation records at random for generating new feature space; (2) **ERG** first applies operation on each feature to expand the feature space, then selects the crucial features as new features. (3) **LDA** is a matrix factorization-based method to obtain the factorized hidden state as the generated feature space. (4) **AFAT** is an enhanced version of ERG that repeatedly generate new features and use multi-step feature selection to select informative ones. (5) **NFS** models the transformation sequence of each feature and uses RL to optimize the entire feature generation process. (6) **TTG** formulates the transformation process as a graph, then implements an RL-based search method to find the best feature set. (7) **GRFG** uses three collaborated reinforced agents to conduct feature generation and proposes a feature grouping strategy to accelerate agent learning. (8) **DIFER** embeds randomly generated feature transformation records with a seq2seq model, then employs gradient search to find the best feature set. MOAT and DIFER belong to the same setting. We demonstrate the differences between them in Appendix **??**. Besides, we developed two variants of MOAT in order to validate the impact of each technical component: (i) \(^{-d}\) replaces the RL-based data collection component with collecting feature transformation-accuracy pairs at random. (ii) \(^{-a}\) removes the data augmentation component. We randomly split each dataset into two independent sets. The prior 80% is used to build the continuous embedding space and the remaining 20% is employed to test transformation performance. This experimental setting avoids any test data leakage and ensures a fairer transformation performance comparison. We adopted Random Forest as the downstream machine learning model. Because it is a robust, stable, well-tested method, thus,we can reduce performance variation caused by the model, and make it easy to study the impact of feature space. We provided more experimental and hyperparameter settings in Appendix B.1.

### Performance Evaluation

**Overall Performance** This experiment aims to answer: _Can_ MOAT _effectively generate transformation sequence for discovering optimal feature space with excellent performance?_ Table 1 shows the overall comparison between MOAT and other models in terms of F1-score and 1-RAE. We noticed that MOAT beats others on all datasets. The underlying driver is that MOAT builds an effective embedding space to preserve the knowledge of feature transformation, making sure the gradient-ascent search module can identify the best-transformed feature space following the gradient direction. Another interesting observation is that MOAT significantly outperforms DIFER and has a more stable performance. There are two possible reasons: 1) The high-quality transformation records produced by the RL-based data collector provide a robust and powerful foundation for building a discriminative embedding space; 2) Postfix notation-based transformation sequence greatly decreases the search space, making MOAT easily capture feature transformation knowledge. Thus, this experiment validates the effectiveness of MOAT.

**Data collection and augmentation.** This experiment aims to answer: _Is it essential to conduct data collection and augmentation to maintain the performance of_ MOAT? To achieve this goal, we developed two model variants of MOAT: 1) MOAT\({}^{-d}\), which randomly collects feature transformation records for continuous space construction; 2) MOAT\({}^{-a}\), which removes the data augmentation step in MOAT. We select two classification tasks (i.e., Spectf and SpamBase) and two regression tasks (i.e., Openml_616 and Openml_618) to show the comparison results. The results are reported in the Figure 4. Firstly, we found that the performance of MOAT is much better than MOAT\({}^{-d}\). The underlying driver is that high-quality transformation records collected by the RL-based collector build a robust foundation for embedding space learning. It enables gradient-ascent search to effectively identify the optimal feature space. Moreover, we observed that the performance of MOAT\({}^{-a}\) is inferior to MOAT. This observation reflects that limited data volume and diversity cause the construction

   Dataset & Source & C/R & Samples & Features & RDG & ERG & LDA & AFAT & NFS & TTG & GRFG & DIFER & MOAT \\  Higgs Boson & UCICrvine & C & 50000 & 28 & 0.695 & 0.702 & 0.513 & 0.697 & 0.691 & 0.699 & 0.707 & 0.669 & **0.712** \\  Amazon Employee & Kaggle & C & 32769 & 9 & 0.932 & 0.934 & 0.916 & 0.930 & 0.932 & 0.933 & 0.932 & 0.929 & **0.936** \\  Pimaladian & UCICrvine & C & 768 & 8 & 0.760 & 0.761 & 0.638 & 0.756 & 0.749 & 0.745 & 0.754 & 0.760 & **0.807** \\  Specf\({}^{}\) & UCICrvine & C & 267 & 44 & 0.760 & 0.757 & 0.665 & 0.766 & 0.792 & 0.760 & 0.818 & 0.766 & **0.912** \\  SVQuadeG & LedySVM & C & 1243 & 21 & 0.787 & 0.826 & 0.652 & 0.795 & 0.792 & 0.798 & 0.812 & 0.773 & **0.849** \\  German Credit & UCICrvine & C & 1001 & 24 & 0.680 & 0.740 & 0.639 & 0.683 & 0.687 & 0.645 & 0.683 & 0.656 & **0.730** \\  Credit Default & UCICrvine & C & 30000 & 25 & 0.805 & 0.803 & 0.743 & 0.804 & 0.801 & 0.798 & 0.806 & **0.796** & **0.810** \\  Messidev features & UCICrvine & C & 1150 & 19 & 0.624 & 0.669 & 0.475 & 0.665 & 0.638 & 0.655 & 0.692 & 0.660 & **0.749** \\  Wine Quality Red & UCICrvine & C & 999 & 12 & 0.466 & 0.461 & 0.433 & 0.480 & 0.462 & 0.467 & 0.470 & 0.476 & **0.559** \\  Wine Quality White & UCICrvine & C & 4900 & 12 & 0.524 & 0.510 & 0.429 & 0.516 & 0.525 & 0.531 & 0.534 & 0.507 & **0.536** \\  SpamBase & UCICrvine & C & 4601 & 57 & 0.906 & 0.917 & 0.889 & 0.912 & 0.922 & 0.919 & 0.922 & **0.932** \\  AP-onmun-vary & OpenML, & C & 275 & 1096 & 0.832 & 0.814 & 0.658 & 0.830 & 0.832 & 0.758 & 0.849 & 0.833 & **0.885** \\  Lymphography & UCICrvine & C & 148 & 18 & 0.108 & 0.144 & 0.167 & 0.150 & 0.152 & 0.148 & 0.182 & 0.150 & **0.267** \\  Ionosphere & UCICrvine & C & 351 & 34 & 0.912 & 0.921 & 0.654 & 0.928 & 0.913 & 0.902 & 0.933 & 0.905 & **0.985** \\  Housing Boston & UCICrvine & R & 506 & 13 & 0.404 & 0.409 & 0.020 & 0.416 & 0.425 & 0.396 & 0.404 & 0.381 & **0.467** \\  Airfoil & UCICrvine & R & 1503 & 5 & 0.519 & 0.519 & 0.220 & 0.521 & 0.519 & 0.500 & 0.521 & 0.558 & **0.629** \\  Openml\_618 & OpenML & R & 1000 & 50 & 0.472 & 0.561 & 0.052 & 0.472 & 0.473 & 0.467 & 0.562 & 0.408 & **0.692** \\  Openml\_589 & OpenML & R & 1000 & 25 & 0.509 & 0.610 & 0.011 & 0.508 & 0.505 & 0.503 & 0.627 & 0.463 & **0.656** \\  Openml\_616 & OpenML & R & 500 & 50 & 0.700 & 0.193 & 0.024 & 0.149 & 0.167 & 0.156 & 0.372 & 0.076 & **0.526** \\  Openml\_607 & OpenML & R & 1000 & 50 & 0.521 & 0.555 & 0.107 & 0.516 & 0.519 & 0.522 & 0.621 & 0.476 & **0.673** \\  Openml\_620 & OpenML & R & 1000 & 25 & 0.511 & 0.546 & 0.029 & 0.527 & 0.513 & 0.512 & 0.619 & 0.442 & **0.642** \\  Openml\_637 & OpenML & R & 500 & 50 & 0.136 & 0.152 & 0.043 & 0.176 & 0.152 & 0.144 & 0.307 & 0.072 & **0.465** \\  Openml\_586 & OpenML & R & 1000 & 25 & 0.568 & 0.624 & 0.110 & 0.543 & 0.544 & 0.544 & 0.646 & 0.482 & **0.700** \\   

* We reported F1-Score for classification tasks, and 1-RAE for regression tasks.

Table 1: Overall performance comparison. ‘C’ for binary classification, and ‘R’ for regression. The best results are highlighted in **bold**. The second-best results are highlighted in underline. (**Higher values indicate better performance.)

Figure 4: The influence of data collection (MOAT\({}^{-d}\)) and data augmentation (MOAT\({}^{-a}\)) in MOAT.

of embedding space to be unstable and noisy. Thus, this experiment shows the necessity of the data collection and augmentation components in MOAT.

**Beam search.** This experiment aims to answer _What is the influence of beam search for improving the valid rate of generated transformation sequences?_ To observe the impacts of beam search, we set the beam size as 5 and 1 respectively, and add DIFER as another comparison object. We compare the generation performance in terms of valid rate. Figure 5 shows the comparison results in terms of valid rate. We noticed that the 5-beams search outperforms the 1-beam search. The underlying driver is that the increased beam size can identify more legal and reasonable transformation sequences. Another interesting observation is that DIFER is significantly worse than MOAT  and its error bar is longer. The underlying driver is that DIFER collects transformation records at random and it generates transformation sequences using greedy search. The collection and generation ways involve more random noises, distorting the learned embedding space.

**Robustness check.** This experiment aims to answer: _Is MOAT robust to different downstream machine learning models?_ We replaced the downstream ML models with Random Forest (RF), XGBoost (XGB), Support Vector Machine (SVM), K-Nearest Neighborhood (KNN), Ridge, LASSO, and Decision Tree (DT) to observe the variance of model performance respectively. Table 2 shows the comparison results on Spectf in terms of the F1-score. We observed that MOAT keeps the best performance regardless of downstream ML models. A possible reason is that the RL-based data collector can customize the transformation records based on the downstream ML model. Then, the learned embedding space may comprehend the preference and properties of the ML model, thereby resulting in a globally optimal feature space. Thus, this experiment shows the robustness of MOAT.

**Time complexity analysis.** This experiment aims to answer _What is the time complexity of each technical component of_ MOAT? To accomplish this, we selected the SOTA model DIFER as a comparison baseline. Figure 3 shows the time costs comparison of data collection, model training, and solution searching in terms of seconds. We let both MOAT and DIFER collect 512 instances in the data collection phase. We found that the increased time costs for MOAT occur in the data collection and model training phases. However, once the model converges, MOAT's inference time is significantly reduced. The underlying driver is that the RL-based collector spends more time gathering high-quality data, and the sequence formulation for the entire feature space increases the learning time cost for the sequential model. But, during inference, MOAT outputs the entire feature transformation at once, whereas DIFER requires multiple reconstruction iterations based on the generated feature space's dimension. The less inference time makes MOAT more practical and suitable for real-world scenarios.

   & RF & XGB & SVM & KNN & Ridge & LASSO & DT \\  RDG & 0.760 & 0.818 & 0.750 & 0.792 & 0.718 & 0.749 & 0.864 \\  ERG & 0.757 & 0.813 & 0.753 & 0.766 & 0.778 & 0.750 & 0.790 \\  LDA & 0.665 & 0.715 & 0.760 & 0.749 & 0.759 & 0.760 & 0.665 \\  AFAT & 0.760 & 0.808 & 0.722 & 0.759 & 0.723 & 0.770 & 0.844 \\  NPS & 0.792 & 0.799 & 0.732 & 0.792 & 0.744 & 0.749 & 0.864 \\  TTG & 0.760 & 0.819 & 0.656 & 0.750 & 0.716 & 0.749 & 0.842 \\  GRFG & 0.818 & 0.842 & 0.580 & 0.760 & 0.729 & 0.744 & 0.786 \\  DIFER & 0.766 & 0.794 & 0.727 & 0.777 & 0.647 & 0.744 & 0.809 \\  MOAT & **0.912** & **0.897** & **0.876** & **0.916** & **0.780** & **0.844** & **0.929** \\  

Table 2: Robustness check of MOAT with distinct ML models on Spectf dataset in terms of F1-score.

Figure 5: The influence of beam size in MOAT.

  
**Dataset** &  **Model** \\ **Name** \\  &  **Data Collection** \\ **512 instances** \\  &  **Model** \\ **Training** \\  &  **Solution** \\ **Searching** \\  & 
 **Performance** \\  \\  Wine Red & MOAT & 921.6 & 5779.2 & 101.2 & 0.559 \\ Wine White & MOAT & 2764.8 & 7079.6 & 103.4 & 0.536 \\ Openml\_618 & MOAT & 4556.8 & 30786.1 & 111.3 & 0.692 \\ Openml\_589 & MOAT & 4044.8 & 8942.3 & 105.2 & 0.656 \\  Wine Red & DIFER & 323.2 & 11 & 180 & 0.476 \\ Wine White & DIFER & 1315.8 & 33 & 624 & 0.507 \\ Openml\_618 & DIFER & 942.3 & 33 & 534 & 0.408 \\ Openml\_589 & DIFER & 732.5 & 157 & 535 & 0.463 \\  

Table 3: Time complexity comparison between MOAT and DIFERTo thoroughly analyze the multiple characteristics of MOAT, we also analyzed the space complexity (See Appendix C.1), model scalability (See Appendix C.2), and parameter sensitivity (See Appendix C.3). Meanwhile, we provided two qualitative analyses: learned embedding visualization (See Appendix C.4) and traceability case study (See Appendix C.5).

## 5 Related Works

**Automated Feature Transformation (AFT)** can enhance the feature space by automatically transforming the original features via mathematical operations [19; 20]. Existing works can be divided into three categories: 1) expansion-reduction based approaches [1; 3; 21; 2; 22]. Those methods first expand the original feature space by explicitly  or greedily  decided mathematical transformation, then reduce the space by selecting useful features. However, they are hard to produce or evaluate both complicated and effective mathematical compositions, leading to inferior performance. 2) evolution-evaluation approaches [4; 5; 6; 25; 9; 26]. These methods integrate feature generation and selection into a closed-loop learning system. They iteratively generate effective features and keep the significant ones until they achieve the maximum iteration number. The entire process is optimized by evolutionary algorithms or reinforcement learning models. However, they still focus on how to simulate the discrete decision-making process in feature engineering. Thus, they are still time-consuming and unstable. 3) Auto ML-based approaches [7; 8]. Auto ML aims to find the most suitable model architecture automatically [27; 28; 29; 30]. The success of auto ML in many area [31; 32; 33; 34; 35; 36] and the similarity between auto ML and AFT inspire researchers to formulate AFT as an auto ML task to resolve. However, they are limited by: 1) incapable of producing high-order feature transformation; and 2) unstable transformation performance. Recent studies tried to convert the feature transformation problem into a continuous space optimization task to search for the optimal feature space efficiently. _DIFER_ is a cutting-edge method that is comparable to our work in problem formulation. However, the following constraints limit DIFER's practicality: 1) DIFER collects transformation-accuracy data at random, resulting in many invalid training data with inconsistent transformation performances; 2) DIFER embeds and reconstructs each transformed feature separately and, thus, ignores feature-feature interactions; 3) DIFER needs to manually decide the number of generated features, making the reconstruction process more complicated. 4) the greedy search for transformation reconstruction in DIFER leads to suboptimal transformation results. To fill these gaps, we first implement an RL-based data collector to automate high-quality transformation record collection. We then leverage the postfix expression idea to represent the entire transformation operation sequence to model feature interactions and automatically identify the number of reconstructed features. Moreover, we employ beam search to advance the robustness, quality, and validity of transformation operation sequence reconstruction.

## 6 Conclusion Remarks

In this paper, we propose an automated feature transformation framework, namely MOAT. In detail, we first develop an RL-based data collector to gather high-quality transformation-accuracy pairs. Then, we offer an efficient postfix-based sequence expression way to represent the transformation sequence in each pair. Moreover, we map them into a continuous embedding space using an encoder-decoder-evaluator model structure. Finally, we employ a gradient-ascent search to identify better embeddings and then use beam search to reconstruct the transformation sequence and identify the optimal one. Extensive experiments show that the continuous optimization setting can efficiently search for the optimal feature space. The RL-based data collector is essential to keep an excellent and stable transformation performance. The postfix expression sequence enables MOAT to automatically determine the transformation depth and length, resulting in more flexible transformation ways. The beam search technique can increase the validity of feature transformation. The most noteworthy research finding is that the success of MOAT indicates that the knowledge of feature transformation can be embedded into a continuous embedding space to search for better feature space. Thus, it inspires us to regard the learning paradigm as the backbone to develop a large feature transformation model and quickly fine-tune it for different sub-tasks, which is also our future research direction.

## 7 Acknowledgments

This research was partially supported by the National Science Foundation (NSF) via grant numbers: 2040950, 2006889, and 2045567.