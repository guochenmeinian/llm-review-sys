# DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering

**Jiahao Lu\({}^{1}\)** **Jiacheng Deng\({}^{1}\)** **Ruijie Zhu\({}^{1}\)** **Yanzhe Liang\({}^{1}\)**

**Wenfei Yang\({}^{1,2}\)** **Tianzhu Zhang\({}^{1}\)* **Xu Zhou\({}^{3}\)**

Footnote *: Corresponding author

\({}^{1}\)University of Science and Technology of China/Deep Space Exploration Lab,

\({}^{2}\)Jianghuai Advance Technology Center, \({}^{3}\)Sangfor Technologies Inc.

{lujiahao, dengjc, ruijiezhu, yzliang}@mail.ustc.edu.cn,

{yangwf, tzzhang}@ustc.edu.cn, zhouxu@sangfor.com.cn

Dynamic scenes rendering is an intriguing yet challenging problem. Although current methods based on NeRF have achieved satisfactory performance, they still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS) has garnered researchers' attention due to their outstanding rendering quality and real-time speed. Therefore, a new paradigm has been proposed: defining a canonical 3D gaussians and deforming it to individual frames in deformable fields. However, since the coordinates of canonical 3D gaussians are filled with noise, which can transfer noise into the deformable fields, and there is currently no method that adequately considers the aggregation of 4D information. Therefore, we propose Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering (DN-4DGS). Specifically, a Noise Suppression Strategy is introduced to change the distribution of the coordinates of the canonical 3D gaussians and suppress noise. Additionally, a Decoupled Temporal-Spatial Aggregation Module is designed to aggregate information from adjacent points and frames. Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level. Code is available at https://github.com/peoplelu/DN-4DGS.

Figure 1: (a) The visualization results on PlenopticVideo  dataset. (b) The visualization results on HyperNeRF  dataset. The numbers below the images represent PSNR.

## 1 Introduction

Dynamic scene reconstruction from single or multi-view videos is a crucial task in computer vision, with applications such as VR/AR [3; 4], 3D perception [5; 6; 7], movie production , etc. Neural Radiance Fields (NeRF)  offer a promising approach by representing scenes with implicit functions derived from multi-view inputs. By incorporating time as an additional input [10; 1], NeRF enables dynamic scene rendering. However, the original NeRF model suffers from significant training and rendering costs, attributed to the high number of points sampled per camera ray and volume rendering.

Recently, the emerging 3D Gaussian Splitting (3DGS)  has significantly increased rendering speed to a real-time level compared to NeRFs by employing a differentiable rasterizer for 3D Gaussian primitives. 3DGS directly optimizes the parameters of 3D gaussians (position, opacity, anisotropic covariance, and spherical harmonics (SH) coefficients) and renders them through projection and \(\)-blending. Given the explicit expression nature of 3DGS, recent studies [11; 13; 14] represent dynamic scenes by defining a canonical 3D gaussians and deforming it to individual frames in deformable fields. Specifically, during the execution of the deformation field, the coordinates \(xyz\) along with time \(t\) are used as input, and the output corresponds to the changes in Gaussian properties. It is noteworthy that the existing methods, when designing deformable networks, either directly map the 4D coordinates of each input point to the latent space using MLPs [13; 14], or use HexPlane  to interpolate a series of learnable embeddings to obtain the latent of each point .

Both of these approaches have drawbacks that can not be ignored. 1) Canonical 3D gaussians are synthesized from multi-frame images of dynamic scenes. Due to the presence of dynamic regions and the specific design of A (canonical 3D gaussians) + B (deformable network), canonical 3D gaussians exhibit significant noise, as illustrated in Figure 2. This noise is inevitably transferred to the deformable field after the input \(xyz\) is passed through the deformable network. To elaborate on the "Noise": In the canonical + deformable design, we input the canonical Gaussian coordinates \(xyz\) and time \(t\) into the deformable network. The deformable network essentially performs basic operations (addition, subtraction, multiplication, division) on the coordinates \(xyz\) and time \(t\). Since the point-to-point relationships within the canonical Gaussians are chaotic and erroneous, as shown in Figure 2, it is predictable that feeding these erroneous coordinates into the deformable network will transfer this error into the deformation field, introducing inaccuracies in the final deformations \( x, y, z\). 2) There is a lack of feature aggregation for spatial-temporal information, yet due to the presence of noise in canonical 3D gaussians' \(xyz\), direct feature aggregation for spatial information would further amplify noise, affecting the learning of the deformable field. Therefore, spatial aggregation after denoising is very crucial.

To address the aforementioned issues, we propose Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering (DN-4DGS), primarily consisting of two components: the Noise Suppression Strategy (NSS) and the Decoupled Temporal-Spatial Aggregation Module (DTS). To address the initial issue, the design of NSS incorporates two deformation operations. The first deformation operation is a standard deformation. It takes the coordinates \(xyz\) of

Figure 2: **Comparison of our render visualization with 4DGaussian . The results are rendered on HyperNeRF  dataset and use the point cloud provided by HyperNeRF for Gaussian initialization (\(Sparse\)\(Init\)). Image 1: canonical 3D gaussians generated by 4DGaussian. Image 2: deformable 3D gaussians generated by 4DGaussian. Image 3: canonical 3D gaussians generated by our method. Image 4: deformable 3D gaussians after the first stage. Image 5: deformable 3D gaussians after the second stage. Image 6: ground truth. The yellow box emphasizes that through a two-stage deformation process, our method can produce higher-quality rendering results.**the canonical 3D gaussians and time \(t\) as input and outputs corresponding coordinate deformations \( x, y, z\). The second deformation builds upon the first by adding \( x, y, z\) to the original \(xyz\), creating a modified set of coordinates that is then input into a new feature extraction network. The entire process is illustrated by image 3, 4 and 5 in Figure 2. This strategy achieves a successful alteration of the distribution of coordinates \(xyz\) through the initial coordinate deformation, resulting in noise reduction and the generation of a more accurate deformation field. It's worth noting that during the early stage of training, we only perform the first deformation operation. Only after achieving acceptable results with the first deformation operation do we proceed to the second deformation operation to further enhance accuracy. To address the second problem, we design the DTS. The reason we decouple spatial-temporal aggregation is due to the presence of noise in the coordinates of the canonical 3D gaussians. If we directly perform spatial aggregation on the coordinates of the canonical 3D gaussians, the noise information is inevitably amplified after a series of aggregation operations such as k-nearest neighbors (KNN) , significantly affecting the results of the deformation field. Therefore, based on the first design NSS, we conduct spatial aggregation during the second deformation operation. Considering that temporal information is unrelated to the canonical 3D gaussians, temporal aggregation can be directly incorporated into the first deformation operation to enhance feature extraction capabilities. In order to reduce computational overhead and considering that temporal information has already been effectively extracted in the first deformation operation, we do not perform temporal aggregation in the second deformation operation. In conclusion, our main contributions are outlined as follows:

(i) We introduce a novel representation called Denoised Deformable Network with Temporal-Spatial Aggregation for high-fidelity and efficient dynamic scene rendering.

(ii) We promose the Noise Suppression Strategy, which can change the distribution of the coordinates of the canonical 3D gaussians, suppress noise and generate a more precise deformation field.

(iii) We promose the Decoupled Temporal-Spatial Aggregation Module to aggregate information from adjacent points and frames.

(iv) Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level.

## 2 Related Work

**Dynamic NeRF.** Novel view synthesis has been a hot topic in academia for several years. NeRF  models static scenes implicitly using MLPs, and numerous studies [17; 18; 19; 2; 10; 20; 21] have extended its application to dynamic scenes through a canonical 3D grid structure and a deformation field. HyperNeRF  models object topology deformation using higher-dimensional inputs, while DyneRF  employs time-conditioned NeRF to represent a 4D scene. However, these approaches, based on vanilla NeRF, suffer from high computational costs due to ray point sampling and volume rendering. To address this issue, several acceleration methods [22; 23; 24; 25; 26; 27; 15; 28; 29; 30; 31; 32] have been proposed for rendering dynamic scenes. DeVRF  introduces a grid representation, while IBR-based methods [24; 25] utilize multi-camera information for improved quality and efficiency. TensorRF  adopts multiple planes as explicit representations for direct dynamic scene modeling. Recent approaches such as K-Planes , Tensor4D , and HexPlane  have also been proposed. NeRFlayer  introduces a unified streaming representation for both grid-based and plane-based methods, utilizing separate models to differentiate static and dynamic scene components, albeit at the cost of slow rendering times. HyperReel  suggests a flexible sampling network coupled with two planes for dynamic scene representation. Despite the improvements in training and rendering speed achieved by these methods, they still fall short of meeting real-time requirements.

**Dynamic Gaussian Splitting.** Recently, 3D Gaussian Splitting (3DGS)  has garnered increasing attention from researchers due to its superior rendering quality and real-time rendering speed. The method employs a soft point representation with attributes including position, rotation, density, and radiance, and utilizes differentiable point-based rendering for scene optimization. Soon after, several concurrent works [11; 13; 14; 37] have adapted 3D Gaussians for dynamic scenes. These methods represent dynamic scenes by establishing a canonical 3DGS and deforming it to individual frames using deformable fields. Yang et al.  predict per-Gaussian offsets using an additional MLP on canonical 3D gaussians, while Wu et al.  substitute the MLP with multi-resolution HexPlanes  and a lightweight MLP. Our work introduces the Noise Suppression Strategy to change the distribution of the coordinates of the canonical 3D gaussians and generate a more precise deformation field. Additionally, for better aggregation of temporal-spatial information, we propose the Decoupled Temporal-Spatial Aggregation Module to consolidate information from adjacent points and frames.

## 3 Preliminary: 3D Gaussian Splatting

Given images at multiple known viewpoints and timesteps, 3D Gaussian Splatting (3DGS)  optimizes a set of attributes (position, opacity, anisotropic covariance and spherical harmonics) via differentiable rasterization. 3DGS can realize high-fidelity rendering of static 3D scenes in real-time.

Suppose a 3D Gaussian \(G(i)\) has the following attributes: position \(_{i}\), opacity \(_{i}\), covariance matrix \(_{i}\) and spherical harmonics \(h_{i}\). The covariance matrix \(_{i}\) is decomposed as \(_{i}=^{T}^{T}\) for optimization, with \(\) as a rotation matrix represented by a quaternion \(q(3)\), and \(\) as a scaling matrix represented by a 3D vector \(s\). Each Gaussian has an opacity value \(_{i}\) to adjust its influence in rendering and is associated with sphere harmonic (SH) coefficients \(h_{i}\) for view-dependent appearance. The final opacity of a 3D gaussian at any spatial point x can be represented as:

\[_{i}=_{i}e^{-(x-_{i})^{T}_{i}^{-1}(x-_{i})}.\] (1)

To render a 2D image, 3D gaussians are projected to 2D space and aggregating them using fast \(\)-blending. The 2D covariance matrix and center are \(_{i}^{2D}=JW W^{T}J^{T}\) and \(_{i}^{2D}=JW_{i}\). The color \((u)\) of a pixel \(u\) is rendered using the fast \(\)-blending operation:

\[(u)=_{i N}T_{i}_{i}(h_{i},v_{i}),\] (2)

where \(T_{i}=_{j=1}^{i-1}(1-_{j})\), \(\) is the spherical harmonic function and \(v_{i}\) is the view direction.

## 4 Method

### Overview

Our goal is to reconstruct dynamic 3D scenes from single/multi-view videos. Following previous works [11; 13], we represent the geometry and appearance of the dynamic scene using canonical 3D gaussians and model the motion through the deformation fields. An overview of our method is shown in Figure 3. We first describe the details of the Noise Suppression Strategy (NSS) in Section 4.2. Then in Section 4.3, we present the design of the Decoupled Temporal-Spatial Aggregation Module (DTS). Section 4.4 details our optimization process.

### Noise Suppression Strategy

In this section, we attempt to mitigate the terrible noise of the canonical 3D gaussians, as shown in Figure 5. Specifically, the Noise Suppression Strategy comprises two deformation operations. The first deformation operation is a standard deformation. It takes the coordinates \(x,y,z\) of the canonical 3D gaussians and time \(t\) as input and outputs corresponding coordinate deformations \( x, y, z\). To simplify, here we only list the attributes \(x,y,z\), while other attributes are detailed in the subsequent Section 4.3,

\[ x, y, z=(x,y,z,T_{n}),\] (3)

Figure 3: **The overall framework of our method DN-4DGS. Our approach employs a two-stage deformation process. In the first deformation, the well-designed Temporal Aggregation Module is utilized to aggregate temporal information. After the first deformation, the coordinate distribution of 3D gaussians is altered, and noise is suppressed. Subsequently, we proceed with the second deformation, utilizing the Denoised Spatial Aggregation Module to aggregate spatial information.**

where \(\) represents the first deformation operation, \(T_{n}\) represents the set of \(t\)'s neighbors. The details of \(\) are introduced in Section 4.3.1. Next, after obtaining \( x, y, z\), we add them to the original \(x,y,z\).

\[x^{},y^{},z^{}=x+ x,y+ y,z+ z.\] (4)

Following this, the second deformation operation is carried out.

\[ x^{}, y^{}, z^{}=^{}(P_{k},t),\] (5)

where \(^{}\) represents the second deformation operation, \(P_{k}\) represents the set of \(k\) neighbors of \((x^{},y^{},z^{})\). The details of \(^{}\) are introduced in Section 4.3.2. In this deformation, due to the successful alteration of the input coordinate distribution during the first deformation stage, we can obtain more accurate Gaussian positions compared to the canonical Gaussian. As a result, the noise in the input is attenuated.

Overall, Noise Suppression Strategy (NSS) is a strategy that uses two stages of deformation to reduce the impact of noise on the deformable network. During this process, we have two training phases. In the early training phase, we only supervise the first deformation. Once the Gaussian coordinates obtained from the first deformation are relatively accurate, we add the second deformation and shift the supervision to it.

### Decoupled Temporal-Spatial Aggregation Module

Local feature aggregation is very important for 3D point clouds, which can effectively extract local structure information. PointNet++  introduces the set abstraction layer to aggregate information from spatially adjacent points, which has become a fundamental operation in various point cloud tasks [38; 39; 40; 41; 42]. Therefore, to enhance the accuracy of the deformation fields, an intuitive approach is to perform neighbor aggregation for each gaussian.

For 4D gaussians, there are four dimensions of information \(x,y,z,t\) available for aggregation. As discussed in the introduction, performing local aggregation on noisy coordinates would further amplify the noise. Therefore, for \(x,y,z\), spatial aggregation is conducted during the second deformation operation. Since temporal information is unrelated to the canonical 3D gaussians, temporal aggregation can be directly integrated into the first deformation operation to enhance feature extraction capabilities. To reduce computational overhead, and considering that temporal information has already been effectively extracted in the first deformation operation, we omit temporal aggregation in the second deformation operation. The entire process is referred to as decoupled temporal-spatial aggregation.

#### 4.3.1 Temporal Aggregation Module

For each gaussian \(G_{t}(i)\), we first input \(x,y,z,t\) into the Feature Encoding. Regarding Feature Encoding, we can utilize the MLPs from D3DGS  or the HexPlanes from 4DGaussian . After that, we acquire \(F_{t}(i)\) at time \(t\). Next is a critical step, where we repeat the above-mentioned process to obtain the features \(F_{t-1}(i)\) for time \(t-1\) and the features \(F_{t+1}(i)\) for time \(t+1\). It's worth noting that here, the "1" represents one timestep. Next, similar to PointNet++ , we perform the aggregation operation. As illustrated in Figure 4, we merge \(F_{t-1}(i)\), \(F_{t}(i)\), and \(F_{t+1}(i)\) together to form \(F(i)^{3 1 C_{1}}\) and input into a lightweight MLP for channel change. Then, we perform MaxPooling along the first dimension of \(F(i)\) to generate \(F_{}(i)^{1 C_{2}}\). Additionally, we introduce a new attribute \(_{i}\), which is a learnable embedding. This attribute can provide information independent of coordinates without interference due to adjacency. We have also observed a similar design in a recent work E-D3DGS . Finally, \(F_{}(i)\), \(F_{t}(i)\) and \(_{i}\) are concatenated together to generate deformation:

\[F_{t}(i)^{}=[F_{}(i),F_{t}(i),_{i}],\] (6)

\[_{}:F_{t}(i)^{}( x, y, z,  r, s,, h),\] (7)

Figure 4: **The structure of aggregation operation.**

where \(_{}\) is the deformation MLP head, \(r\) is a rotation quaternion, \(s\) is a vector for scaling, \(\) is an opacity, and \(h\) is **SH** coefficients for modeling view-dependent color.

#### 4.3.2 Denoised Spatial Aggregation Module

After obtaining the new \(x^{},y^{},z^{}\), we input them into the denoised spatial aggregation module for spatial aggregation. Concretely, we calculate the k-nearest neighbors for each gaussian based on \(x^{},y^{},z^{}\). Then, we aggregate the features of the k-nearest neighbors to obtain \(F_{n}(i)^{1 K C_{3}}\). MaxPooling is then performed on \(F_{n}(i)\) to get \(F_{nm}(i)\). Finally, \(F_{n}(i)\), \(F_{nm}(i)\) and \(x^{},y^{},z^{}\) are concatenated together to generate the second deformation:

\[F_{n}(i)^{}=[F_{n}(i),F_{nm}(i),x^{},y^{},z^{ }],\] (8) \[^{}_{}:F_{n}(i)^{}(  x, y, z, r, s,, h),\] (9)

where \(_{}\) is the deformation MLP head in the second deformation operation.

Overall, Decoupled Temporal-Spatial Aggregation Module (DTS) is a specific feature aggregation method we propose. Unlike 4DGaussian, D3DGS lacks explicit spatiotemporal aggregation, so we designed DTS to aggregate spatiotemporal information. Considering that inaccurate coordinate relationships would be amplified through spatial aggregation (KNN), we only perform spatial aggregation in the second stage of NSS, which is DSAM. We name the spatial aggregation module "Denoised Spatial Aggregation Module" (DSAM) because the Gaussian coordinates input into DSAM are more accurate (denoised), as shown in the fourth column of Figure 9. Therefore, we prefix the Spatial Aggregation Module with "Denoised". DSAM itself does not have denoising capabilities; it solely performs spatial feature aggregation.

### Optimization

The parameters to be optimized include the deformable network and the attributes of each 3D gaussian \(G(i)\): \(_{i}\), \(_{i}\), \(_{i}\), \(h_{i}\) and \(_{i}\). Following 4DGaussian , we use the reconstruction loss \(_{1}\) and gird-based TV loss [15; 45; 34; 46]\(_{tv}\) to supervise the training process. Additionally, we add a D-SSIM term \(_{ssim}\) to improve structural similarity:

\[=_{1}+(1-)_{ssim}+_ {tv},\] (10)

where \(\) is the hyperparameter. It is worth noting that we employ a two-stage training strategy. During the early stages of training, we exclusively execute the first deformation operation. Once satisfactory results are attained with the initial deformation operation, we then proceed to implement the second deformation operation to further refine accuracy. The reason for this strategy is that only the deformation \( x, y, z\) in the first stage is sufficiently precise to remove a large amount of noise, thereby positively impacting the deformation in the second stage.

## 5 Experiment

### Experimental Setup

**Dataset and Metrics. PlenopticVideo** dataset includes 20 multi-view videos, with each scene consisting of either 300 frames, except for the flame salmon scene, which comprises 1200 frames.

Figure 5: **More rendering images of canonical 3D gaussians.** Here, \(Sparse\)\(Init\) refers to using the point cloud provided by the HyperNeRF  dataset (\(_{}\)) for Gaussian initialization, while \(Dense\)\(Init\) denotes generating a denser point cloud via \(_{}\). In fact, \(Dense\)\(Init\) can produce better rendering quality, but due to the need for regenerating, it consumes more computational resources.

These scenes encompass a relatively long duration and various movements, with some featuring multiple objects in motion. We utilized PlenopticVideo dataset to observe the capability to capture dynamic areas. Total six scenes (coffee martini, cook spinach, cut roasted beef, flame salmon, flame steak, seat steak) are utilized to train and render. Rendering resolution is set to 1352 \(\) 1014 **HyperNeRF** dataset includes videos using two Pixel 3 phones rigidly mounted on a handheld capture rig. We train and render on four scenes (3D printer, Banana, Broom, Chicken) at a resolution downsampled by a factor of two to 540 \(\) 960. **NeRF-DS** dataset consists of seven captured videos (Sieve, Press, Plate, Cup, As, Bell, Basin) with camera pose estimated using colmap . The dataset involves a variety of rigid and non-rigid deformation of various objects. We train and render on the seven scenes. Rendering resolution is set to 480 \(\) 270.

We report the quality of rendered images using PSNR, SSIM , MS-SSIM and LPIPS . Higher PSNR, SSIM and MS-SSIM values and lower LPIPS values indicate better visual quality. To PlenopticVideo dataset, we report PSNR, SSIM and LPIPS (Alex). To HyperNeRF dataset, we report PSNR, SSIM and MS-SSIM. To NeRF-DS dataset, we report PSNR, MS-SSIM and LPIPS (VGG).

**Implementation Details.** We train our model on a single RTX3090. The optimizer we utilize is Adam . The learning rate is initially set at 1.6e-4, gradually decreasing exponentially to 1.6e-6 by the end of the training process. The learning rate for the voxel grid is initialized at 1.6e-3 and exponentially decays to 1.6e-5. For hyperparameters, we tune \(K,\) as 16, 0.9 respectively. More details will be shown in the Appendix.

### Comparison with existing methods.

**Results on PlenopticVideo.** Table 1 reports the results on PlenopticVideo dataset. Refer to the Appendix for per-scene details. Due to the incorporation of the Noise Suppression Strategy, which alters the distribution of canonical 3D gaussians coordinates to suppress noise, as well as the utilization of the Decoupled Temporal-Spatial Aggregation Module for feature aggregation, our approach demonstrates superior reconstruction quality across all metrics compared to the baseline (4DGaussian ). In fact, PSNR and LPIPS are currently the state-of-the-art metrics. As the table shows, despite our method involving two stages of deformation, resulting in a slight weakening in training time and FPS, it overall meets the requirements for rapid training and real-time demands. Regarding storage, due to the presence of a new attribute \(_{i}\), it may slightly exceed the baseline. To vividly illustrate the differences between our method and others, we visualize the qualitative results in Figure 6. From the regions highlighted in red boxes, it is evident that our method can render higher-quality images.

**Results on HyperNeRF.** Table 2 reports the results on HyperNeRF dataset. In this dataset, we present results for both \(Sparse\)\(Init\) and \(Dense\)\(Init\). \(Sparse\)\(Init\) refers to using the point cloud provided by the HyperNeRF dataset (\(_{}\)) for Gaussian initialization, while \(Dense\)\(Init\) denotes generating a denser point cloud via \(_{}\). From Table 2, it can be observed that our method outperforms other Gaussian-based methods under both \(Sparse\)\(Init\) and \(Dense\)\(Init\) settings. Moreover, under the \(Dense\)\(Init\) setting, our method achieves the current state-of-the-art performance. More importantly, we find that 4DGaussian is highly sensitive to the sparsity or density of Gaussian initialization. In contrast, our approach benefits from noise suppression and feature

   Method & PSNR(\(\)) & SSIM(\(\)) & LPIPS(\(\)) & Time(\(\)) & FPS(\(\)) & Storage(MB)(\(\)) \\  DyNeRF  & 29.58 & - & 0.099 & 1344 hours & 0.01 & **28** \\ NeRFlayer  & 30.69 & 0.909 & 0.111 & 6 hours & 0.045 & - \\ HyperReel  & 31.10 & 0.921 & 0.096 & 9 hours & 2.0 & 360 \\ HexPlane-all*  & 31.70 & 0.984 & 0.075 & 12 hours & 0.2 & 250 \\ KPIanes  & 31.63 & 0.964 & - & 1.8 hours & 0.3 & 309 \\ 
4DGS  & 31.19 & 0.940 & 0.051 & 9.5 hours & 19.5 & 8700 \\ E-D3DGS  & 31.31 & 0.945 & 0.037 & 2 hours & 43.1 & 35 \\
4DGaussian  & 31.15 & 0.940 & 0.049 & 40 mins & 30 & 90 \\ Ours & 32.02 & 0.944 & 0.043 & 50 mins & 15 & 112 \\   

Table 1: **Quantitative comparison on PlenopticVideo dataset.** We display the average PSNR/SSIM/LPIPS (Alex) metrics for novel view synthesis on dynamic scenes, with each cell colored to indicate the \(\), \(\), \(\), \(\), and \(\).

aggregation, resulting in a more pronounced performance improvement under the sparse setting. The qualitative results can be observed from Figure 7. From the regions highlighted in red boxes, our method can render higher-quality images, as further supported by PSNR in gray cells.

**Results on NeRF-DS.** Table 4 presents the results on NeRF-DS dataset. Our proposed method achieves better performance compared to previous methods, demonstrating the effectiveness and generalization of our method. More qualitative results are shown in the Appendix.

**Results on D-NeRF.** As illustrated in the Figure 8, we have visualized the canonical results for both 4DGaussian and D3DGS. The results show that D3DGS has less noise compared to 4DGaussian, but noise is still present in moving areas. The quantitative results on D-NeRF dataset, as shown in the Table 3, indicate that our method improves performance on both 4DGaussian and D3DGS, with enhancements on 4DGaussian surpassing those on D3DGS. Regarding the parameters of the deformation networks, in 4DGaussian: Most of the parameters are composed of the HexPlane and deformation head. Our method introduces an additional deformation operation, resulting in a slight increase in the number of parameters compared to the baseline, with an increase of only 0.03M parameters. In D3DGS: The network primarily consists of an MLP-based deformation network. To reduce the computational load, our method halves the number of MLP layers, resulting in fewer overall parameters compared to the original D3DGS.

### Ablation Studies

**Evaluation of the model with different designs.** To evaluate the effectiveness of proposed components, we conduct an ablation study in Table 5 on PlenopticVideo dataset. Here, NSS, TAM, DSAM represents the Noise Suppression Strategy, Temporal Aggregation Module and Denoised

   Method & PSNR(\(\)) & SSIM(\(\)) & MS-SSIM(\(\)) & Time(\(\)) & FPS(\(\)) Storage(MB)(\(\(\))) \\   & 22.23 & - & 0.803 & \(\) hours & \(<\)1 & - \\  & 22.2 & 0.598 & 0.811 & 32 hours & \(<\)1 & - \\  & 24.30 & 0.616 & 0.837 & 30 mins & 1 & 48 \\  D3DGS\(\) & 21.50 & - & - & 2 hours & 10 & 18 \\
4DGaussian\(\) & 21.80 & 0.573 & 0.710 & 50 mins & 38 & 11 \\  & 23.31 (+1.51) & 0.618 (+0.045) & 0.768 (+0.058) & 1.1 hours & 24 & 12 \\  D3DGS* & 23.43 & - & - & 3.5 hours & 7 & 88 \\
4DGaussian* & 25.20 & 0.682 & 0.845 & 1 hour & 34 & 61 \\  & 25.59 (+0.39) & 0.691 (+0.09) & 0.863 (+0.018) & 1.2 hours & 20 & 68 \\   

Table 2: **Quantitative comparison on HyperNeRF dataset. Here, \(\) represents that we train the model based on \(Sparse\)\(Init\). * represents that we train the model based on \(Dense\)\(Init\).**

Figure 6: **Qualitative comparisons on PlenopticVideo Dataset.**

Spatial Aggregation Mouldle respectively. Specifically, the second row shows that with the use of two-stage deformation operations, our model can acquire a certain degree of improvement in quality. This highlights the meaning of noise suppression. The third row demonstrates that with the help of temporal aggregation, a performance gain of 0.41, 0.02, 0.003 has been achieved in PSNR, SSIM and LPIPS. The fourth row demonstrates the effective collaboration between NSS and TAM, resulting in performance improvement. The fifth row indicates that if we do not utilize two-stage training, but instead only perform spatial aggregation on canonical 3D gaussians, it not only fails to bring about an improvement in quality but also leads to a decrease. The sixth row indicates that if we replace TAM with an ordinary deformation network, there is a slight drop in performance compared to the last row. The last row indicates that if we combine all components together, the performance can reach its optimal level.

**Effectiveness of the two-stage deformation operations.** To validate the significance of two-stage deformation operations, we conducted visual experiments. As shown in Figure 9, canonical 3D gaussians exhibit a significant amount of noise, which severely affects the accuracy of the deformation field. To alleviate this issue, we employed two-stage deformation. As depicted in the fourth column of the figure, after the first deformation, there is a significant change in the distribution of coordinates \(xyz\), effectively suppressing the noise. Moreover, due to the design of temporal aggregation, the corresponding PSNR value is even higher than that of 4DGaussian. Finally, by performing the second deformation operation on the basis of the first-stage deformation, the performance is further improved. This is attributed to the cleaner coordinates \(xyz\) and the design of spatial aggregation.

   Method & PSNR(\(t\)) & SSIM(\(t\)) & Parameter (M(\(\)) \\ 
4DGaussian  & 34.05 & 0.9787 & 3.38 \\
4DGaussian-Ours & 34.53(+0.48) & 0.9811(+0.0024) & 3.41(+0.03) \\
3DDG  & 39.51 & 0.9920 & 0.52 \\ D3DGS+Ours & 39.87(+0.36) & 0.9922(+0.020) & 0.39(-0.13) \\   

Table 3: **Quantitative comparison on D-NeRF dataset.** Here, parameter refers to the parameters of the deformation networks corresponding to different baselines.

Figure 8: **The canonical results for both 4DGaussian and D3DGS.**

Figure 7: **Qualitative comparisons on HyperNeRF Dataset.** In the gray cells, the numbers represent PSNR.

### Limitations and Future Work

Although two-stage deformation can alter the coordinate distribution of canonical 3D gaussians and reduce the noise introduced into the deformation field, the lack of simultaneous supervision for both stages [52; 53; 41] poses a challenge. Consequently, during the second stage, due to the lack of supervision in the first-stage deformation, the direction of coordinate deformation becomes uncontrollable to some extent. This, in turn, affects the spatial feature aggregation in the second stage. To address this issue, future work should explore the direction of simultaneous supervision.

## 6 Conclusion

In this paper, we introduce a novel representation called Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering. We promose the Noise Suppression Strategy, which can change the distribution of the coordinates of the canonical 3D gaussians, suppress noise and generate a more precise deformation field. To aggregate information from adjacent points and frames, we promose the Decoupled Temporal-Spatial Aggregation Module. Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level.

## 7 Acknowledgements

This work was partially supported by the National Nature Science Foundation of China (12150007, 62306294), Dreams Foundation of Jianghuai Advance Technology Center (No.2023-ZM01Z019), and the Youth Innovation Promotion Association.

   Method & PSNR(\(\)) & SSIM(\(\)) & LPIPS(\(\)) \\  TiNeuVox  & 21.61 & 0.823 & 0.277 \\ HyperNeRF  & 23.45 & 0.849 & 0.199 \\ NeRF-DS  & 23.60 & 0.849 & 0.182 \\ 
3D-GS  & 20.29 & 0.782 & 0.292 \\ D3DGS  & 23.92 & 0.847 & 0.184 \\ Ours & **24.36** & **0.865** & **0.171** \\   

Table 4: **Quantitative comparison on NeRF-DS dataset.**

Figure 9: **Effectiveness of the two-stage deformation operations. In the gray cells, the numbers represent PSNR.**