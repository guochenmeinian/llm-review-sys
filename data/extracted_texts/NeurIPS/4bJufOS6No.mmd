# On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection

Xiufeng Song

Shanghai Jiao Tong University \({}^{2}\)Michigan State University \({}^{3}\)Shanghai Artificial Intelligence Laboratory

{akikaze, zjc_he, iapple1, zhaiguangtao, xiaohongliu}@sjtu.edu.cn

{guoxia11, liuxm}@cse.msu.edu baisanshi@gmail.com

 Xiao Guo

Shanghai Jiao Tong University \({}^{2}\)Michigan State University \({}^{3}\)Shanghai Artificial Intelligence Laboratory

{akikaze, zjc_he, iapple1, zhaiguangtao, xiaohongliu}@sjtu.edu.cn

{guoxia11, liuxm}@cse.msu.edu baisanshi@gmail.com

 Jiache Zhang

Shanghai Jiao Tong University \({}^{2}\)Michigan State University \({}^{3}\)Shanghai Artificial Intelligence Laboratory

{akikaze, zjc_he, iapple1, zhaiguangtao, xiaohongliu}@sjtu.edu.cn

{guoxia11, liuxm}@cse.msu.edu baisanshi@gmail.com

 Qirui Li

Shanghai Jiao Tong University \({}^{2}\)Michigan State University \({}^{3}\)Shanghai Artificial Intelligence Laboratory

{akikaze, zjc_he, iapple1, zhaiguangtao, xiaohongliu}@sjtu.edu.cn

{guoxia11, liuxm}@cse.msu.edu baisanshi@gmail.com

 Lei Bai

Shanghai Jiao Tong University \({}^{2}\)Michigan State University \({}^{3}\)Shanghai Artificial Intelligence Laboratory

{akikaze, zjc_he, iapple1, zhaiguangtao, xiaohongliu}@sjtu.edu.cn

{guoxia11, liuxm}@cse.msu.edu baisanshi@gmail.com

 Xiaoming Liu

Shanghai Jiao Tong University \({}^{2}\)Michigan State University \({}^{3}\)Shanghai Artificial Intelligence Laboratory

{akikaze, zjc_he, iapple1, zhaiguangtao, xiaohongliu}@sjtu.edu.cn

{guoxia11, liuxm}@cse.msu.edu baisanshi@gmail.com

###### Abstract

Large numbers of synthesized videos from diffusion models pose threats to information security and authenticity, leading to an increasing demand for generated content detection. However, existing video-level detection algorithms primarily focus on detecting facial forgeries and often fail to identify diffusion-generated content with a diverse range of semantics. To advance the field of video forensics, we propose an innovative algorithm named Multi-Modal Detection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the profound perceptual and comprehensive abilities of Large Multi-modal Models (LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's multi-modal space, enhancing its ability to detect unseen forgery content. Besides, MM-Det leverages an In-and-Across Frame Attention (IAFA) mechanism for feature augmentation in the spatio-temporal domain. A dynamic fusion strategy helps refine forgery representations for the fusion. Moreover, we construct a comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF), across a wide range of forgery videos. MM-Det achieves state-of-the-art performance in DVF, demonstrating the effectiveness of our algorithm. Both source code and DVF are available at link.

## 1 Introduction

Recent years have witnessed significant advancements in diffusion generative methods, which have led to the creation of extraordinarily visually compelling content in video generation . Although the latest generated videos impress society with their versatility and stability, synthetic media also poses a risk of malicious attacks, such as counterfeit faces created by deepfakes  and falsifications in business, raising public concerns about information security and privacy. In response to such issues, researchers have made significant progress in forgery detection, addressing problems on image editing manipulation  and CNN-synthesized images . To enhance the trustworthiness and reliability of current detectors in the face of evolving generative video methods, we aim to develop a generalizable detection method for diffusion-based generative videos.

Figure 1: Multi-Modal Detection (MM-Det) leverages features from spatiotemporal (ST) information (), a CLIP encoder  (), and an LMM (). The Fused feature () achieves state-of-the-art performance in our Diffusion Video Forensics (DVF) dataset.

Previously, the video forensics community emphasized more on developing facial forgery detection algorithms , which may struggle to address recent fraudulent videos (_e.g._, sora, pika, etc.). Compared to facial forgery, diffusion-based generated content contains more diverse semantics, making it more challenging to distinguish diffusion forgery contents from real ones. Towards these challenges, a new thread of research on CNN-generated image detection has emerged . These works aim to learn common generation traces in image-level content, but do not design specific mechanisms to capture temporal inconsistencies in videos.

Therefore, previous defensive efforts might not be able to provide a video-level detection algorithm for newly emerged generated videos with diverse manipulation artifacts and visual contexts. Meanwhile, Large Multi-modal Models (LMMs) show unparalleled problem-solving ability , thanks to its powerful multi-modal representations. However, such representations are barely studied in the video forensics task.

Motivated by the limitation of previous work and the unprecedented understanding ability of LMMs, we propose a video-level detection algorithm, named Multi-Modal Detection (MM-Det), to capture forgery traces based on an LMM-based multi-modal representation. MM-Det takes advantage of the perception and reasoning capabilities of LMMs to learn a generalizable forgery feature, as depicted in Fig. 2. To the best of our knowledge, we are the first to use LMMs for video forensic work.

Aside from multi-modal representations, two common sources of generative errors that can be leveraged for video discrimination are spatial artifacts and temporal inconsistencies. Our approach aims to effectively identify these two types of errors as an auxiliary feature in forgery detection. Inspired by the previous work  that shows the effectiveness of reconstruction for detecting diffusion images, we extend this idea into the video domain, amplifying diffusion artifacts both in spatial and temporal information. To capture such artifacts efficiently, we leverage a Vector Quantised-Variational AutoEncoder (VQ-VAE)  for a fast reconstruction process, as detailed in Fig. 3. Moreover, we design a novel **I**n-and-**A**cross **F**rame **A**ttention (IAFA) into a Transformer-based network, which balances frame-level forgery traces with information flow across frames, thus aggregating local and global features.

Although diffusion methods demonstrate strong capabilities in video generation, the lack of public datasets on diffusion videos hinders research efforts in the video forensic community. In light of this,

Figure 2: LMMs detect visual artifacts and anomalies, offering detailed textual reasoning that explains whether the image is generated using Artificial Intelligence (AI) techniques. The powerful representation in the visual domain enables LMMs to understand complex contexts within frames. Furthermore, their advanced language reasoning capability implicitly reveals image authenticity and provenance. For instance, the term like “consistent shape” refers to common features in authentic content, while “unrealistic color” signifies typical artifacts in forged content. This linguistic proficiency stems from the superior perception and comprehension abilities of LMMs, contributing to a generalizable multimodal feature space. By leveraging the visual understanding and textual reasoning abilities of LMMs, we construct a Multi-Modal Forgery Representation (MMFR).

we have established a comprehensive dataset for diffusion-generated videos, named Diffusion Video Forensics (DVF). DVF includes generated content from a variety of diffusion models, featuring rich semantics and high quality, serving as a general benchmark for open-world video forensics tasks.

\(\) We propose a detection method called MM-Det that leverages a **M**ulti-**M**odal Forgery Representation from LMMs to effectively detect diffusion-generated videos with strong generalization capability.

\(\) A powerful and innovative In-and-Across Frame Attention (IAFA) mechanism is introduced to aggregate global and local patterns within forged videos, enhancing the detection of spatial artifacts and temporal inconsistencies.

\(\) We introduce a large-scale dataset, named the Diffusion Video Forensics (DVF) dataset, comprising high-quality forged videos generated using \(8\) diffusion-based methods. The DVF dataset contains diverse forgery types across videos of varying resolutions and durations, effectively serving as a benchmark for forgery detection in real-world scenarios.

\(\) Our MM-Det achieves state-of-the-art detection performance on the DVF dataset. Also, a detailed analysis is provided to showcase the effectiveness of multi-modal representations in detecting forgeries, paving the way for compelling opportunities for using LMMs in future multi-media forensic research.

## 2 Related Works

Frame-level DetectorEarly work [53; 22; 13; 47; 65; 16] observed that forgery traces exist in images generated by AI techniques, and such traces are commonly used as evidence to distinguish diffusion-generated content [40; 6; 7] and attribute if two images are generated by the same method [66; 37]. However, identifying unseen and diverse frequency-based clues in real-world scenarios is challenging. For that, existing frame-level forgery detectors concentrate on improving the generalization ability. For example, some works [36; 8; 17; 26; 29] introduced features from pre-trained CLIP  encoders for the forensic task to help reduce the overfitting issue on specific forgery types and increase the robustness towards detection [36; 8] and localization .  and  proposed proactive methods to protect images from manipulation based on image watermarking and steganography. Also, reconstruction errors through the inversion process of DDIM  are studied by prior works [55; 33; 41; 32] for diffusion generative content detection. Moreover, the previous work [63; 34; 34; 18] develops specific techniques that increase the generalization to unseen forgeries. For example, HiFiNet  proposes a tree structure to model the inherent hierarchical correlation among different forgery methods, NPR  devises a special representation as generative artifacts, and the training set diversity can also contribute to generalization ability . Unlike prior works, our MM-Det leverages multi-modal reasoning to achieve a high level of generalization ability.

Video-level DetectorEarly video-level methods primarily focused on detecting facial forgery. For example,  learned the boundary artifacts between the original background and manipulated faces.  discriminated fake videos from the inconsistency of mouth motion.  designed a multi-attentional detector to capture deepfake features and artifacts. F3Net  captured global and local forgery traces in the frequency domain. [10; 15; 74; 56] explored temporal information and inconsistency from fake videos. Most recently, DD-VQA  formulates deepfake detection as a sentence-generation problem, largely improving the interpretation of deepfake detection. However, these studies are restricted to facial forgery methods, which are insufficient for the current defensive systems that address diverse content produced by diffusion models. Therefore, we develop MM-Det to detect diffusion video content, pushing forward the frontier of forgery video detection.

Figure 3: The residual difference between VQ-VAE  reconstructed images and real ones. Given an encoder \(\) and a decoder \(\) of a VQ-VAE and taking the input video \(\), the reconstructed video \(^{}\) is obtained as \(^{}=(())\). The VQ-VAE reconstruction of real images exhibits obvious edges and visible traces, whereas diffusion-generated ones are reconstructed more effectively, offering residual difference images with fewer visible traces.

Large Multi-modal Models (LMMs)LMMs possess generalizable problem-solving abilities in real-world tasks, including object detection, semantic segmentation and visual question answering. [2; 23; 27; 28] studied feature alignment schemes to bridge visual and textual domains for LMMs. [68; 72; 71] extended the boundaries of LMMs to multi-modal downstream tasks.  aligns multi-modal features for capabilities on cross-domain behaviors.  developed a Large Language Model (LLM)-based feature extractor for cheap-fake detection. Inspired by these studies, we stimulate the powerful perceptual and reasoning ability of an LMM by introducing the multi-modal feature space in video forgery detection.

## 3 Methods

In this section, we introduce the Multi-Modal Detection (MM-Det) framework for diffusion video detection, as depicted in Fig. 4. More formally, Sec. 3.1 details a Large Multi-modal Model (LMM) branch that learns a Multi-Modal Forgery Representation (MMFR). Then Sec. 3.2 reports a Spatio-Temporal (ST) branch that utilizes In-and-Across Frame Attention (IAFA) to capture spatial artifacts and temporal inconsistencies in forged videos. Lastly, a dynamic fusion technique reported in Sec. 3.3 adaptively combines outputs from the LMM branch and ST branch.

### Multi-Modal Forgery Representation

We propose a novel Multi-Modal Forgery Representation (MMFR) from the multi-modal space of LMMs in LMM branch. This representation utilizes the powerful perceptual and reasoning abilities of LMMs in the form of instruction-based conversations.

Specifically, LMM branch is built on the top of LLaVA , one representative LMM, which has two key components: a visual encoder (_e.g._, \(_{v}\)), instantiated by visual encoders from the Contrastive Language-Image Pre-Training (CLIP) , and the large language model \(_{L}\) (_i.e._, Llama 2 ). Let us denote the input video as \(^{N H W C}\) that contains \(N\) frames, where each frame is represented as \(^{H W C}\). First, \(\) is fed to \(_{v}\) to obtain the corresponding visual representation \(_{V}^{Z}\). Such \(_{V}\) not only contains rich semantics but also shows impressive generalization ability and robustness in the forgery detection task [36; 43; 9]. Then, a textual instruction \(\) is sampled from

Figure 4: Multi-Modal Detection network (MM-Det) architecture. Given an input video \(\), the Large Multi-modal Model (LMM) branch takes the frame and instructions to generate Multi-Modal Forgery Representation (MMFR). Hidden states from the visual encoder and large language model are extracted to form the MMFR, denoted as \(_{M}\), which helps capture the forgery traces among different diffusion-generated videos. In the Spatio-Temporal (ST) branch, videos are first reconstructed via a VQ-VAE, and then fed into a CNN encoder, followed by In-and-Across Frame Attention (IAFA) modules detailed in Sec. 3.2. IAFA is introduced to capture features based on spatial artifacts and temporal inconsistencies, termed as \(_{ST}\). At last, a dynamic fusion strategy combines \(_{M}\) and \(_{ST}\) for the final forgery prediction.

pre-defined templates \(\) to guide the LMM on forgery detection reasoning. Both visual representation \(_{V}\) and instruction \(\) are fed to \(_{L}\), which generates enhanced visual representations (_e.g._, \(_{L}\)). We convert \(_{V}\) into a sequence of visual tokens  (_i.e._, \(_{v}=\{_{v,m}\}_{m=1}^{M}^{M D}\)), and \(\) is transformed into textual tokens \(_{t}=\{_{t,o}\}_{o=1}^{O}^{O D}\). Both \(_{v}\) and \(_{t}\) are taken as the input to \(_{L}\), generating \(_{L}^{S D}\) that can be tokenized into the language response providing reasoning (Fig. 2) about the authenticity of the input \(\). This procedure is formulated as

\[_{L}=_{L}(_{t},_{v})=_{L} (,_{V}),\] (1)

where \(\) guides the pre-trained \(_{L}\) in comprehending visual content (_i.e._, \(_{V}\)), discerning the subset information from \(_{V}\). This instruction \(\) enables LMM branch to obtain the multi-modal representation that leverages the generalization ability from the pre-trained large language model Llama 2 (_i.e._, \(_{L}\)), being different to prior work [36; 9] that only relies on \(_{E}\).

Lastly, we retrieve the final MMFR, denoted as \(_{M}^{M Z}\), by concatenating \(_{V}\) and \(_{L}\) after a linear layer(_i.e._, PROJ), as

\[_{M}=(\{(_{V}),(_{L})\}).\] (2)

### Capturing Spatial-Temporal Forgery Traces

Targeting capturing spatiotemporal artifacts in video tasks, we introduce a Spatial-Temporal (ST) branch that learns effective diffusion forgery representation at the video level. Through a reconstruction procedure, we amplify the diffusion traces in the frequency domain, which is then captured by In-and-Across Frame Attention (IAFA) to form an effective video-level feature.

Amplification of Diffusion TracesSimilar to prior studies [55; 33] that discovered specific generative traces of diffusion models through reconstruction on diffusion-generated images, we utilize an Autoencoder to amplify diffusion traces in videos. The reconstruction procedure can be expressed as follows.

More formally, denote the input video as \(^{N H W C}\) that contains \(N\) frames, in which each frame is represented as \(^{H W C}\). We leverage a VQ-VAE  to obtain the reconstructed version of \(\), which is denoted as \(}^{H W C}\). The difference between \(\) and \(}\) makes an effective indicator of showing if the input is generated by diffusion models, as depicted in Fig. 3. Therefore, we jointly proceed \(\) and \(}\) to the following proposed modules for learning an effective representation of discerning forgeries.

It is worth mentioning that the prior approach  also adopts the idea of using the residual difference between original and reconstructed inputs to help forgery detection, but the reconstruction method requires multiple time-step denoising operations, which are computationally infeasible to reconstruct all frames from \(\). In contrast, our VQ-VAE-based reconstruction method only requires one single forward propagation to obtain reconstructed frame \(}\), meanwhile preserving the effectiveness in indicating the discrepancy between real and fake inputs.

Figure 5: (a) In-and-Across Frame Attention (IAFA): Each input frame (or its feature map) is divided into patches that are transformed into tokens, termed P-tokens (). We introduce additional frame-centric tokens FC-tokens () encapsulating the global forgery information of the video frame. In each transformer layer, self-attention is applied alternately among all P-tokens from video frames as well as among the same frame’s P-tokens and its FC-tokens. (b) The dynamic fusion strategy captures that takes \(_{ST}\) and \(_{M}\) as inputs and output channel-wise dependencies, which help refine forgery representations for the fusion.

**Integration of Spatial and Temporal Information** Same as the previous work [3; 35] that utilizes ViT to learn video-level information, we transform each input video frame (_i.e._, \(\)) into \(L\) tokens, and propose IAFA for information aggregation, as depicted in Fig. 5a. Let us denote frame-level tokens as Patch-wise tokens (P-tokens), as they represent local information of one patch from \(\). More formally, the \(i\)th frame, \(_{i}\) is divided into \(L\) patches, and all patches are projected into \(_{i}=\{_{i}^{j}\}_{j=1}^{L}^{L D}\), where \(D\) represents the dimension of each P-token. Also, to capture the global forgery information for each video frame, we introduce additional tokens called Frame-Centric tokens (FC-tokens). The FC-token is denoted as \(_{i}^{D}\) for frame \(_{i}\) and attends other tokens _within_ the same frame \(_{i}\).

During the forward propagation, we conduct IAFA based on a Transformer, with each block containing two self-attentions and consecutively modeling the local and global forgeries at each video frame. Specifically, the first self-attention captures dependencies among P-tokens that restore local forgery clues. This is formulated by Eq. 3 that \(_{i}^{j}^{D}\) attends the token \(_{q}^{p}^{D}\) that represents \(p\)th token from \(q\)th frame \(_{q}\). Consequently, given the \(i\)th frame _i.e._, \(_{i}\), the second self-attention is conducted among the FC-token (_i.e._, \(\{_{i}\}\)) and P-tokens (_i.e._, \(\{_{i}^{0},\,_{i}^{1},\,...\,\,_{i}^{L-1}\}\)) from the same frame, which encapsulates patch-wise forgery information into the global one for learning the more robust representation. We formulate this procedure in Eq. 4.

\[_{i}^{j}=(_{i}^{j},_{q}^{p}) i,j[1,N],\,j,p[1,L],\] (3) \[_{i}=(_{i},_{i}^ {j}) j[1,L],\] (4)

where \(\) refers to the self-attention operation.

### Dynamic Fusion

We devise the dynamic fusion strategy (_i.e._, \(_{f}\)) that combines spatiotemporal information from ST branch and MMFR (_i.e._, \(_{f}\) and \(_{m}\)) for the final prediction, by adjusting their contributions based on forgeries from the input. More formally, \(_{f}\) (Fig. 5b) learns channel-wise dependencies among forgery representations (_e.g._, \(_{ST}\) and \(_{M}\)) via the attention mechanism, generating \(^{N+M}\) as the output. This procedure can be expressed as \(=_{f}(\{_{ST},_{M}\})\). Also, \(\) contains \(_{ST}^{N}\) and \(_{M}^{M}\), representing learned channel-wise weights for \(_{ST}\) and \(_{M}\), respectively. Such channel-wise weights are important in the fusion purpose, as they help emphasize useful information -- we use \(_{ST}\) and \(_{M}\) to refine forgery representations as \(_{ST}^{}=_{ST}_{ST}\) and \(_{M}^{}=_{M}_{M}\). Lastly, \(_{ST}^{}\) and \(_{M}^{}\) are concatenated into the fused representation \(_{0}^{(M+N) D}\), which is used for the final scalar prediction \(s\) via the average pooling (_i.e._, \(\)) and linear layers(_i.e._, \(\)):

\[s=((_{0}))=(( \{_{ST}^{},_{M}^{}\}).\] (5)

## 4 Diffusion Video Forensics (DVF) Dataset

We construct a large-scale dataset for the video forensic task named Diffusion Video Forensics (DVF), as shown in Fig. 6. DVF contains \(8\) diffusion generative methods, including Stable Diffusion , VideoCrafter1 , Zeroscope, Sora, Pika, OpenSora, Stable Video, and Stable Video Diffusion.

To efficiently streamline the collection, we construct an effective automated pipeline that generates forgery videos based on real videos and prompts. Specifically, we start from two real video datasets,

Figure 6: Sampled videos from DVF dataset. DVF contains \(8\) video generation methods, including \(7\) text-to-video methods and \(1\) image-to-video method. Real videos are selected from Internvid-\(10\)M  and Youtube-\(8\)M . [Key: OSOra: OpenSora; VC1: Videocrafter1 ; Zscope: Zeroscope; St. V. D.: Stable Video Diffusion ; St.Diff.: Stable Diffusion ; St. V.:Stable Video]Internvid-\(10\)M  and Youtube-8M . Real videos are sampled for rich semantic content, with their frames and captions used for generation. Fig. 7 a introduces the generation process of DVF. For open-sourced generation methods, a prompt is fed to a text-to-video method(_i.e._ VideoCrafter\(1\), Zeroscope, OpenSora), or a frame is provided to an image-to-video method(Stable Video Diffusion) to generate the corresponding fake video. For commercial and close-sourced datasets(_i.e._ Stable Diffusion, Stable Video, Sora, Pika), forgery videos are collected from official websites and social media. In total, we collect \(3,938\) fake videos and \(2,750\) real videos in DVF. As shown in Fig. 7 b and Fig. 7 c, our dataset contains multiple resolutions and durations. The video number of each dataset varies from \(0.1\)k to \(2.8\)k, with the corresponding frame numbers from \(4.2\)k to \(784\)k. More details about DVF are provided in Appendix 8.3.1.

## 5 Training Strategy

This section details our two-stage training strategy, in which we first finetune the LMM branch via instruction tuning  and then optimize the entire framework in an end-to-end manner.

LMM Branch Instruction TuningWe first adapt LLaVA  to the forgery detection downstream task based on instruction tuning, an empirically effective way for various downstream tasks, which leverages LoRA  to improve the reasoning ability of Large Language Models(LLMs). For that, construct a large image-text paired dataset, named Rich Forgery Reasoning Dataset. Please refer to Appendix 8.3.2 for more details. We use multi-turn conversations to fine-tune the LMM, enhancing its ability to identify and judge the authenticity of input images. Following the instruction tuning strategy of LLaVA , we only fine-tune the projection layers and LLM in LLaVA. More formally, we formulate the objective function as the loss for an auto-regressive model, which is based on answer tokens from the LLM, as:

\[(_{1})=-_{l=1\\ l}^{T}log(p_{_{1}}(s^{t}|s^{i<t})),\] (6)

where \(s^{i}\) refers to the \(i^{th}\) prediction token, \(T\) refers to the length of total prediction tokens, and \(_{1}\) refers to the trainable parameters in the LMM.

End-to-End TrainingAfter fine-tuning LLaVA, we use this model to form LMM branch of MM-Det, and then the entire model is trained in an end-to-end manner. Please note that all parameters in LMM branch are frozen to ensure the optimal multi-modal representation can be obtained. More formally, we denote MM-Det's final prediction scalar and the ground truth as \(s\) and \(y\), respectively, and the model is optimized by the cross-entropy loss \(\) as follows:

\[(_{2})=-(y(v)+(1-y)(1-(v))\] (7)

where \(_{2}\) refers to trainable parameters in both ST branch and dynamic fusion modules.

Figure 7: The overview of DVF dataset: (a) The procedure of forged video generation and collection. Real frames and captions sampled from Internvid-10M  and Youtube-8M  for text-to-video and image-to-video generation. (b) DVF contains videos at various resolutions and durations. (c) The statistics of DVF, measured by the numbers of frames and videos.[Key: VC1: Wideocrafter\(1\); Zscope: Zeroscope; OSOra: OpenSora; St.Diff.: Stable Diffusion ; St. V.: Stable Video; St. V. D.: Stable Video Diffusion ]

## 6 Experiments

### Setup

In the experiment, we use the proposed DVF for the evaluation. In training, \(1,000\) videos from YouTube and \(1,973\) fake videos generated by Stable Video Diffusion serve as the training set, in which \(90\%\) are used for training and the remaining \(10\%\) for validation. Real videos from Internvid-\(10\)M  and fake videos from \(6\) generative methods are used as testing samples. More details on training and testing are provided in Appendix 8.4.

For a fair comparison, we choose the following \(10\) recent detection methods as baselines. CN-NDet  applies a ResNet  as the backbone for forgery detection. F3Net  utilizes frequency traces left in forgery content. HiFi-Net  devise a specific hierarchical fine-grained learning scheme to learn a wide range of forgery traces. Clip-Raising , Uni-FD  takes advantage of a pre-trained CLIP  as a training-free feature space. DIRE  detects diffusion images based on a reconstruction process of DDIM . ViViT , TALL , and TS2-Net  take advantage of spatiotemporal information in various visual tasks. DE-FAKE  adopts visual and textual representations based on a CLIP encoder for image forgery detection. For the measurement, we choose AUC since it is a threshold-independent metric. We computer means and deviations of the performance across 5 runs on different random seeds.

### Video Forgery Detection Performance

In Tab. 1, our proposed MM-Det achieves SoTA performance in detecting diffusion video, surpassing the second-best method, _i.e._, HiFi-Net, by \(7.7\%\) in the average of AUC scores. Specifically, for prior methods that are based on pre-trained CLIP features, such as Raising  and Universal FD , they remain effective on certain types of diffusion content (_i.e._ Stable Diffusion), but fail on most others. Simple structures like CNN  exceed these CLIP-based methods after being fine-tuned on our proposed DVF, reaching an average AUC score of \(78.2\%\), which proves the necessity of such datasets. As for our method, MM-Det outperforms other methods in most datasets. Compared with frequency-based forgery methods, _e.g._, HiFi-Net  and CLIP-based methods [8; 36], our method improves the performance from \(+3.3\%\)(VideoCrafter\(1\)) to \(+15.5\%\)(Stable Diffusion). It is worth mentioning that HiFi-Net makes the second-best performer in our DVF dataset, achieving \(84.3\%\) AUC scores. We believe this indicates the multi-branch feature extractor used in HiFi-Net carries versatile forgery traces at multiple resolutions, enhancing the learning of the forgery invariant. The failure of frequency traces and CLIP features raises the need for more effective features. As for spatiotemporal baselines [3; 62; 31], we outperform them by \(+12.9\%\)(ViViT), \(+22.5\%\)(TALL) and \(+19.9\%\)(TS2-Net), demonstrating the effective features of MMFR and IAFA. At last, our detector improves by \(19.9\%\) to another multi-modal detector , which utilizes visual information and corresponding captions for feature enhancement. It is shown that the introduction of MMFR is more generalizable than a simple combination of visual features and text descriptions in that the powerful perceptual and reasoning abilities of LMMs play a crucial role in discriminating between real and fake content.

   &  &  &  &  &  &  &  \\  &  &  &  &  &  &  \\ 
**CNNDet ** & \(87.4 1.8\) & \(88.8 1.5\) & \(78.0 1.6\) & \(63.8 3.5\) & \(77.3 2.1\) & \(73.5 2.4\) & \(78.9 4.1\) & \(78.2 1.3\) \\ DIRE  & \(55.9 2.2\) & \(61.8 3.3\) & \(53.8 1.8\) & \(60.5 5.5\) & \(65.8 1.7\) & \(62.7 3.6\) & \(69.9 2.5\) & \(62.1 1.8\) \\  Raising  & \(63.8 1.6\) & \(60.7 2.9\) & \(64.1 1.9\) & \(68.3 3.6\) & \(70.7 1.4\) & \(78.2 2.3\) & \(62.8 1.5\) & \(67.0 0.9\) \\ Uni-FD  & \(75.0 3.0\) & \(74.2 3.4\) & \(76.6 2.3\) & \(73.1 1.5\) & \(76.2 2.4\) & \(80.2 1.9\) & \(66.7 2.6\) & \(74.1 1.2\) \\ 
**F3Net ** & \(89.7 1.8\) & \(80.5 2.2\) & \(69.3 1.8\) & \(69.8 89.8\) & \(89.2 3.4\) & \(84.4 2.1\) & \(85.1 1.3\) & \(81.8 1.9\) \\ ViViT  & \(79.1 3.1\) & \(78.4 2.0\) & \(77.7 2.3\) & \(69.4 3.5\) & \(83.1 2.6\) & \(82.1 2.0\) & \(83.6 2.1\) & \(79.1 1.8\) \\  TALL  & \(76.0 1.4\) & \(65.9 1.6\) & \(62.1 1.3\) & \(64.3 1.9\) & \(72.3 2.9\) & \(65.8 2.8\) & \(79.8 2.2\) & \(69.5 1.4\) \\  TS2-Net  & \(61.8 3.9\) & \(70.6 2.8\) & \(75.5 3.4\) & \(78.0 2.9\) & \(78.2 2.8\) & \(62.1 1.3\) & \(78.6 3.0\) & \(72.1 2.8\) \\
**DE-FAKE ** & \(74.7 0.7\) & \(68.2 2.9\) & \(55.8 3.6\) & \(64.1 3.1\) & \(85.6 2.2\) & \(85.4 2.6\) & \(70.6 1.9\) & \(72.1 2.2\) \\ HiFi-Net  & \(90.2 3.0\) & \(89.7 2.9\) & \(80.1 2.6\) & \(70.1 3.8\) & \(87.8 2.9\) & \(89.2 2.5\) & \(83.1 2.2\) & \(84.3 2.4\) \\ 
**MC-Det (Ours)** & \(93.5 8.6\) & \(94.0 2.8\) & \(88.8 2.8\) & \(86.2 1.8\) & \(95.9 2.8\) & \(95.7 2.5\) & \(89.0 2.0\) & \(92.0 2.6\) \\  

Table 1: Video forgery detection performance on the DVF dataset measured by AUC (\(\%\)). [Key: **Best**; Second Best; Stable Diff.: Stable Diffusion; Avg.: Average]

### Ablation Study

Tab. 2 shows the impact of individual modules proposed in MM-Det. Specifically, we use the Hybrid ViT  as the base model and incorporate it with the reconstruction procedure for diffusion trace amplification, which enhances the detection performance by \(+1.7\%\) AUC score. Such a module raises the performance in OpenSora and Sora, revealing that frequency-based information benefits forgery detection on these methods. Detection performance is further increased by using IAFA, which strengthens the learning between in-frame and cross-frame information, increasing a \(+13.0\%\) AUC score to the base model. The rise in performance indicates such temporal information benefits most types of forgery video detection. After that, Tab. 2's line \(4\) indicates the effectiveness of MMFR: a detector purely based on such representation can receive \(88.7\%\) performance in AUC, \(+13.2\%\) higher than the base model. In addition, by merging MMFR (_i.e._, LMM branch) and ST branch, the performance rises by \(+0.4\%\). Finally, with the dynamic fusion strategy, our method receives an impressive \(92.0\%\) AUC score for all generative methods, higher than every single feature. These experiments highlight the necessities of each module in our framework. Moreover, an ablation study on LLMs is detailed in Appendix 8.5 to prove the effectiveness of various LLMs in MM-Det.

### Spatio Temporal Information Anaylsis

In the analysis of IAFA, we visualize feature activation maps from the last layer of ViT in the ST branch based on the L2-norm. We compare our feature maps with another spatiotemporal baseline, ViViT , as depicted in Fig. 8. While attention maps of ViViT are sparse and irregular, the ones of our IAFA have a tendency to concentrate on the segmentation of diffusion-generated objects, indicating that IAFA captures typical spatial forgery regions in frames. The attention mainly focuses on common forgery traces, such as blurred generative patterns and defective parts of objects, signaling that diffusion models might find it difficult to generate delicate content. The concentration of activation on certain informative objects discloses both spatial artifacts of existing generative methods, demonstrating the effectiveness of our proposed ST branch.

   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & Diff. & Video \\    &  &  &  & 68.2\(\)1.8 & 80.1\(\)2.8 & 64.8\(\)2.1 & 59.6\(\)1.9 & 79.2\(\)2.9 & 89.2\(\)2.6 & 87.4\(\)2.3 & 75.5\(\)1.8 \\   &  & & 70.1\(\)1.6 & 7.6\(\)1.8 & 76.1\(\)0.0 & 82.1\(\)0.0 & 83.9\(\)1.5 & 86.1\(\)1.1 & 77.2\(\)1.3 \\   &  & & 90.2\(\)2.8 & 90.1\(\)2.5 & 85.8\(\)1.8 & 87.2\(\)2.8 & 93.9\(\)3.8 & 91.9\(\)2.6 & 85.7\(\)2.2 & 88.5\(\)2.7 \\  & & & 89.7\(\)3.8 & 93.4\(\)3.2 & 84.6\(\)3.2 & 83.2\(\)2.6 & 94.0\(\)2.2 & 89.7\(\)2.0 & 86.2\(\)1.8 & 88.7\(\)1.8 \\   &  & & 92.1\(\)1.1 & 93.8\(\)2.9 & 86.5\(\)2.6 & 85.2\(\)1.9 & 90.1\(\)2.5 & 90.2\(\)2.1 & 87.6\(\)2.9 & 89.1\(\)2.3 \\  & & & **93.5\(\)3.6** & **94.0\(\)2.8** & **88.8\(\)2.8** & **86.2\(\)1.8** & **95.9\(\)2.8** & **95.7\(\)2.5** & **89.9\(\)2.0** & **92.0\(\)2.6** \\  

Table 2: Ablation analysis measured by AUC (\(\%\)). [Key: Best; Avg.: Average; Rec.: Diffusion Reconstruction Procedure; Fus. Dynamic Fusion Strategy].

Figure 8: Visualization of artifacts captured from our IAFA and ViViT . We use activation maps to highlight spatial weights within each frame. All content is generated by VideoCrafter1. Features from the last layer of transformers are extracted for visualization.

### Multi-modal Forgery Representation Analysis

Fig. 9 details the effectiveness of MMFR in LMM branch. First, as depicted in Fig. 9 a, we quantify the detection ability of features from each Transformer-based decoder layer in the LLM. Specifically, we evaluate both pre-trained LLaVA  and its fine-tuned version on the task of distinguishing real and fake frames. These frames are randomly sampled from \(1,000\) real videos and equivalent fake ones from Stable Video Diffusion . Layer-wise outputs from the large language model in LLaVA (_i.e._, Vicuna ) are obtained -- for \(i\) th layer in the LLM, we denote its output features as \(_{i}^{o}\), \(i\). The K-Means clustering algorithm is adopted to evaluate the classification accuracy based on \(_{i}^{o}\). Empirically, we observe that features extracted from a fine-tuned LLaVA show promising classification accuracy in the last few layers, _e.g._, \(22\)-nd or later layers. This phenomenon indicates that specific layers in LLMs indeed generate features that can be used for image forensic tasks. Such features are utilized in MMFR to exhibit high generalization ability towards diverse and unseen forgeries. Secondly, the comparison between pre-trained and fine-tuned LLaVA highlights the importance of downstream task-oriented instruction tuning for LMMs. This conclusion is consistent with the findings of prior works .

In addition, shown in Fig. 9 and 9 c, we analyze features from ST branch and LMM branch through t-SNE . Both features achieve superior performance in separating real and forgery videos. In Fig. 9 b, spatiotemporal information forms a rough boundary between real and fake videos. This feature is effective for VideoCrafter\(1\), Zeroscope, Stable Video, and Pika, whose durations and resolutions are similar to the training set. However, the detection performance might decrease on Sora and OpenSora with overlay in the clusters. We suppose that various resolutions and durations may compromise the generalization ability, magnifying the importance of a comprehensive dataset for these videos. Fig. 9 c demonstrates the more powerful feature from LMM branch. Samples from Zeroscope, Sora, and Pika are compacted into a denser area, indicating the ability of LLMs to conduct generalizable reasoning. Such features provide new insights for detection when spatial and temporal artifacts are not obvious among the latest forgery videos.

## 7 Conclusion

In this work, we develop an effective video-level algorithm termed Multi-Modal Detection (MM-Det) for diffusion-generated video detection. MM-Det leverages a novel generalizable Multi-Modal Forgery Representation (MMFR) that is obtained from multi-modal spaces in LMMs. Specifically, the proposed MM-Det has two major branches: the LMM branch, which incorporates vision and text features from the fine-tuned foundation model, and the ST branch, which concentrates on modeling spatial-temporal information aggregated through In-and-Across Frame Attention. Extensive experiments demonstrate the effectiveness of our proposed detector. In addition, we establish a comprehensive dataset for various diffusion generative videos, which we hope will serve as a general benchmark for real-world video forensic tasks.