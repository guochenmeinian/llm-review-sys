# Revisiting Ensembling

in One-Shot Federated Learning

 Youssef Allouah\({}^{1}\) Akash Dhasade\({}^{1}\)1 Rachid Guerraoui\({}^{1}\) Nirupam Gupta\({}^{2}\)

Anne-Marie Kermarrec\({}^{1}\) Rafael Pinot\({}^{3}\) Rafael Pires\({}^{1}\) Rishi Sharma\({}^{1}\)

\({}^{1}\)EPFL \({}^{2}\)University of Copenhagen

\({}^{3}\)Sorbonne Universite and Universite Paris Cite, CNRS, LPSM

###### Abstract

Federated learning (FL) is an appealing approach to training machine learning models without sharing raw data. However, standard FL algorithms are iterative and thus induce a significant communication cost. One-shot federated learning (OFL) trades the iterative exchange of models between clients and the server with a single round of communication, thereby saving substantially on communication costs. Not surprisingly, OFL exhibits a performance gap in terms of accuracy with respect to FL, especially under high data heterogeneity. We introduce Fens, a novel federated ensembling scheme that approaches the accuracy of FL with the communication efficiency of OFL. Learning in Fens proceeds in two phases: first, clients train models locally and send them to the server, similar to OFL; second, clients collaboratively train a lightweight prediction aggregator model using FL. We showcase the effectiveness of Fens through exhaustive experiments spanning several datasets and heterogeneity levels. In the particular case of heterogeneously distributed CIFAR-10 dataset, Fens achieves up to a \(26.9\%\) higher accuracy over state-of-the-art (SOTA) OFL, being only \(3.1\%\) lower than FL. At the same time, Fens incurs at most \(4.3\) more communication than OFL, whereas FL is at least \(10.9\) more communication-intensive than Fens.

## 1 Introduction

FL is a widely adopted distributed machine learning (ML) approach, enabling clients to _collaboratively train_ a common model over their collective data without sharing raw data with a central server . Clients in FL engage in iterative parameter exchanges with the server over several communication rounds to train a model. While providing high accuracy, this process incurs substantial communication cost . One-shot federated learning (OFL)  has been introduced to address the communication challenges in FL by reducing the exchange of models to a single round. Not surprisingly, this came with a loss of accuracy with respect to FL.

Typical OFL methods execute local training at the clients up to completion and form an ensemble of locally trained models at the server . The ensemble is distilled into a single model, through means of either auxiliary public dataset  or synthetic data generated at the server . While these OFL methods address communication challenges by reducing model exchanges to a single round, they often achieve lower accuracy compared to iterative FL. This is especially true when data distribution across clients is highly heterogeneous as OFL methods typicallyrely on simple prediction aggregation schemes such as averaging [11; 46], weighted averaging [7; 10] or voting .

We introduce Fens, a hybrid of OFL and standard FL. Fens aims to approach both the accuracy of iterative FL as well as the communication cost of OFL. Learning in Fens proceeds in two phases. In the first phase, similar to OFL, clients upload their locally-trained models to the server. Instead of using the traditional OFL aggregation, Fens employs a second phase of FL: the server constructs an ensemble with a prediction _aggregator model_ stacked on top of the locally trained models. This advanced aggregation function is then trained by the clients in a lightweight FL training phase. The overall learning procedure is illustrated in Figure 1, alongside iterative and one-shot FL.

### Our Contributions

We show for the first time, to the best of our knowledge, that a shallow neural network for the aggregator model suffices to satisfactorily bridge the gap between OFL and FL. Leveraging a shallow aggregator model enables two major benefits: first, it induces significantly lower communication cost in the iterative phase, and second, the iterative refinement of this aggregator model significantly improves accuracy over existing OFL methods. By utilizing elements from both OFL and FL in this novel ensembling scheme, Fens achieves the best of both worlds: accuracy of FL and communication efficiency of OFL.

Through extensive evaluations on several benchmark datasets (CIFAR-100, CIFAR-10, SVHN, and AG-News) across different heterogeneity levels, we demonstrate the efficacy of Fens in achieving FL-like accuracy at OFL-like communication cost. We extend our empirical evaluations to the FLamby benchmark , a realistic cross-silo FL dataset for healthcare applications. Our results show that in heterogeneous settings where even iterative FL algorithms struggle, Fens remains a strong competitor. We then conduct an extensive study of different aggregator models and highlight the accuracy vs. communication trade-off. Lastly, we show that Fens maintains high accuracy even with a comparable memory footprint.

To showcase Fens's performance, we compare its accuracy and communication costs against Co-Boosting , a state-of-the-art OFL method, and FedAdam, a popular iterative FL algorithm, as shown in Figure 2. These evaluations are performed on the CIFAR-10 dataset with \(20\) clients across three heterogeneity levels: \(=0.01\) (very high), \(=0.05\) (high), and \(=0.1\) (moderate). Co-Boosting exhibits an accuracy gap of \(13.7-26.9\%\) compared to FedAdam. Fens closes this accuracy gap, being only \(0-3.1\%\) lower than FedAdam. To achieve this, Fens incurs only \(3.8-4.3\) more communication than Co-Boosting whereas FedAdam is \(10.9-22.1\) more expensive than Fens.

### Related Work

**One-shot Federated Learning.** Guha _et al_.  introduced one-shot FL, which limits communication to a single round. They proposed two main methods: _(i)_ heuristic selection for final ensemble clients, and _(ii)_ knowledge distillation (KD) for ensemble aggregation into a single model at the server using an auxiliary dataset. Subsequent methods based on KD [10; 22] require large, publicly available

Figure 1: Fens in comparison to iterative and one-shot federated learning.

Figure 2: Test accuracy and communication cost of OFL, Fens and FL on CIFAR-10 dataset under high data heterogeneity.

datasets similar to local client data for good performance, which are often difficult to obtain . To address this, synthetic data generation using generative adversarial networks (GAN)s has been proposed [7; 46]. The SOTA Co-Boosting algorithm  iteratively generates and refines synthetic data and the ensemble model. In FedCVAE-Ens, clients train variational autoencoders (VAEs) locally and upload decoders to the server, which generates synthetic samples for classifier training. FedOV  trains an open-set classifier at each client to predict "unknown" classes, with the server ensembling these models and using open-set voting for label prediction. Other OFL approaches either do not fully consider data heterogeneity [22; 37], or face difficulties under high data heterogeneity .

Another line of research in OFL focuses on aggregating fully trained client model parameters [42; 45]. PFNM  matches neurons across client models for fully-connected networks, while FedMA extends this to convolutional neural networks (CNNs) and LSTMs. However, the performance of these methods drops with more complex models. Few theoretical works exist, such as , which analyze global model loss for overparameterized ReLU networks. Despite the advances, OFL still exhibits accuracy gap with iterative FL. We show that Fens narrows this accuracy gap while preserving communication efficiency.

**Ensembles in Federated Learning.** Ensembles have been previously studied in FL for a variety of different goals. FedDF  performs robust model fusion of client ensembles to support model heterogeneity. The FedBE algorithm  uses Bayesian Model Ensemble to aggregate parameters in each global round, improving over traditional parameter averaging. Hamer _et al_. propose FedBoost that constructs the ensemble using simple weighted averaging and analyze its optimality for density estimation tasks. However, these works are designed for standard FL and rely on substantial iterative communication. In the decentralized edge setting,  show that collaborative inference via neighbor averaging can achieve higher accuracy over local inference alone. However, they assume a setting where clients can exchange query data during inference and consider only IID data replicated on all edge devices. The idea of learning an aggregator model closely resembles late fusion techniques in multimodal deep learning . The key difference is that Fens focuses on fusing single modality models trained on heterogeneous data under the communication constraints of federated settings.

## 2 Description of Fens

We consider a classification task represented by a pair of input and output spaces \(\) and \(\), respectively. The system comprises \(M\) clients, represented by \([M]=\{1,,M\}\) and a central server. Each client \(i\) holds a local dataset \(_{i}\). For a model \(h_{}:\) parameterized by \(^{d}\), each data point \((x,y)\) incurs a loss of \((h_{}(x),y)\) where \(:\). Denoting by \(:=_{i[M]}_{i}\) the union of all local datasets, the objective is to solve the empirical risk minimization (ERM) problem: \(_{}|}_{(x,y)} (h_{}(x),y)\).

### Federated Learning (FL) and One-shot FL (OFL)

FL algorithms, such as FedAvg , are iterative methods that enable the clients to solve the above ERM problem, without having to share their local data. In each iteration \(t\), the server broadcasts the current model parameter \(_{t}\) to a subset of clients \(S_{t}[M]\). Each client \(i S_{t}\) updates the parameter locally over its respective dataset \(_{i}\) using an optimization method, typically stochastic gradient descent (SGD). Clients send back to the server their locally updated model parameters \(\{_{t}^{(i)},\,i S_{t}\}\). Lastly, the server updates the global model parameter to \(_{t+1}|}_{i S_{t}}_{t}^{(i)}\).

In One-shot Federated Learning (OFL), the iterative exchanges in FL are replaced with a _one-shot_ communication of local models. Specifically, each client \(i\) seeks a model \(^{(i)}\) that approximately solves the ERM problem on their local data: \(_{}_{i}|}_{(x,y)_{i }}(h_{}(x),y)\), and sends \(^{(i)}\) to the server. Upon receiving the local parameter \(^{(i)}\), corresponding to parametric model \(_{i}=h_{^{(i)}}\), the server builds an ensemble model of the form \((x)=_{i[M]}w_{i}_{i}(x)\). This ensemble model is then distilled into a single global model at the server using either a public dataset or synthetic data (generated by the server). Existing OFL algorithms choose weights \(w_{1},,w_{M}\) in three different ways: _(i)_ uniformly at random , _(ii)_ based on local label distributions , and _(iii)_ dynamically adjusted based on generated synthetic data .

### Fens

In Fens, the server builds the ensemble model using a generic aggregator \(f_{}:^{M}\), parameterized by \(^{q}\) to obtain a global model \(:\) defined to be

\[(x) f_{}(_{1}(x),,_{M}(x)).\] (1)

In case of standard aggregators such as weighted averaging, \(q=M\) and \((w_{+}^{M}|_{i=1}^{M}w_{i}=1).\), and \(f_{}(_{1}(x),,_{M}(x))_{i[M]}_{i} _{i}(x)\). In general, \(f_{}\) can be a non-linear trainable model such as a neural network. The overall learning procedure in Fens comprises two phases:

1. **Local training and one-shot communication:** Each client \(i\) does local training to compute \(^{(i)}\), identical to OFL, and sends it to the server.
2. **Iterative aggregator training:** Upon receiving the local parameters \(^{(i)}\), the server reconstructs \(_{i} h_{^{(i)}}\), and obtains \(\) that approximately solves the following ERM problem: \[_{}|}_{(x,y)}(f_{}(_{1}(x),,_{M}(x)),y).\] (2) The above ERM problem is solved using an iterative FL scheme (described above). For doing so, the server transmits the set of local models \(\{_{1},,_{M}\}\) to all the clients. The final model is given by \((x) f_{}(_{1}(x),,_{M}(x))\).

When solving for (2) using iterative FL, only the aggregator parameters are transferred between the server and the clients. As we show through experiments, in the subsequent section, training an aggregator model is much simpler than training the local models \(_{i}\), and a shallow neural network suffices for \(f_{}\). Algorithms 1 and 2 (Appendix C) provide the pseudo for Fens.

**Connection with stacked generalization.** The use of a trainable aggregator corresponds to an instance of stacked generalization  in the centralized ensemble literature, wherein the aggregation function is regarded as _level_\(1\) generalizer, while the clients' models are regarded as _level_\(0\) generalizers. It has been shown that level 1 generalizer serves the role of correcting the biases of level \(0\) generalizers, thereby improving the overall learning performance of the ensemble . While stacked generalization has been primarily studied in centralized settings, through Fens we show that this scheme can be efficiently extended to an FL setting.

## 3 Experiments

We split our evaluation into the following sections: _(i)_ Fens vs OFL in Section 3.2; _(ii)_ Fens vs FL and analysis of when Fens can match FL in Section 3.3; _(iii)_ Fens on real-world cross-silo FLamby benchmark  in Section 3.4; _(iv)_ Fens on language dataset in Section 3.5; _(v)_ dissecting components of Fens in Section 3.6; and _(vi)_ enhancing Fens efficiency in Section 3.7.

### Experimental setup

**Datasets.** We consider three standard vision datasets with varying level of difficulty, including SVHN , CIFAR-10  and CIFAR-100 , commonly used in several OFL works [7; 10; 46] as well as one language dataset AG-News . Vision experiments involve \(20\) clients, except in the scalability study, where client numbers vary; and AG-News uses \(10\) clients. The original training splits of these datasets are partitioned across clients using the Dirichlet distribution \(_{20}()\), in line with previous works [7; 10; 14]. The parameter \(\) determines the degree of heterogeneity, with lower values leading to more heterogeneous distributions (see Appendix A, Figure 9). For our experiments involving the realistic healthcare FLamby benchmark, we experiment with \(3\) datasets: Fed-Camelyon16, Fed-Heart-Disease, and Fed-ISIC2019. Table 5 (Appendix A) presents an overview of the selected tasks. The datasets consist of a natural non independent and identically distributed (non-IID) partitioning across clients. In Fens, each client performs local training using \(90\%\) of their local training data while reserving \(10\%\) for the iterative aggregator training. We observed that by splitting the datasets, we achieve better performance than reusing the training data for aggregator training. For fairness, OFL and FL baselines run with each client using \(100\%\) of their dataset forlocal training. The testing set of each dataset is split (50-50%) for validation and testing. We use the validation set to tune hyperparameters and always report the accuracy on the testing split.

**One-shot FL baselines.** We compare Fens against 6 one-shot baselines: _(i)_ one-round FedAvg; _(ii)_ FedENS, the first one-shot method constituting an ensemble with uniform weights; _(iii)_ FedKD, based on auxiliary dataset; _(iv)_ one-shot version of Fed-ET; _(v)_ the data-free FedCVAE-Ens; and _(vi)_ Co-Boosting, based on synthetic data generation. We use the best-reported hyperparameters in each work for the respective datasets wherever applicable or tune them. Appendix B.1 provides additional details regarding the one-shot baselines.

**Iterative FL baselines.** For comparison with FL, we consider \(6\) algorithms: _(i)_ FedAvg; _(ii)_ FedProx; _(iii)_ FedNova; _(iv)_ Scaffold; _(v)_ FedYogi; and _(vi)_ FedAdam. We tune learning rates for each algorithm. In addition to these baselines, we implement gradient compression with FedAvg STC, following the sparsification and quantization schemes of STC. In particular, we set the quantization precision to 16-bit and sparsity level to \(50\%\), to reduce the communication cost of FedAvg by \(4\) and keep the remaining setup to the same as above baselines. For the FLamby benchmark experiments, we use the reported hyperparameters which were obtained after extensive tuning, except with one difference. The authors purposefully restricted the number of rounds to be approximately the same as the number of epochs required to train on pooled data (see ). Since this might not reflect true FL performance, we rerun all FL strategies to convergence using the reported tuned parameters. Precisely, we run up to \(10\) more communication rounds than evaluated in the FLamby benchmark. We include more details on FL baselines in Appendix B.2.

**Fens.2** For the CIFAR-10 and CIFAR-100 datasets, each client conducts local training for \(500\) epochs utilizing SGD as the local optimizer with an initial learning rate of \(0.0025\). For the SVHN and AG-News datasets, local training extends to \(50\) and \(20\) epochs respectively with a learning rate of \(0.01\). The learning rate is decayed using Cosine Annealing across all datasets. For the FLamby benchmark experiments, each client in Fens performs local training with the same hyperparameters as the client local baselines of FLamby. We experiment with two aggregator models, a 2-layer perceptron with ReLU activations and another that learns per-client per-class weights. We train the aggregator model using the FedAdam algorithm where the learning rate is separately tuned for each dataset (Table 8, Appendix B.3). To reduce the communication costs corresponding to the ensemble download, we employ post-training model quantization at the server from FP32 to INT8. Appendix B.3 provides more details on Fens.

**Configurations.** In line with related work [10; 24; 35], we use ResNet-8  as the client local model for our vision tasks and fine-tune DistilBert  for our language task. Our FLamby experiments use the same models as defined in the benchmark for each task (see Table 5, Appendix A). We report the average results across at least 3 random seeds. For iterative FL baselines, the communication cost corresponds to the round in which the best accuracy is achieved.

### Fens vs Off

To assess Fens's efficacy, we experiment in non-IID settings, varying \(\{0.01,0.05,0.1\}\), and present results across datasets and baselines in Table 1. Our observations reveal challenges for one-shot methods under high heterogeneity, with the optimal baseline differing across settings. FedAvg with one round exhibits the poorest performance. While FedCVAE-Ens maintains consistent accuracy across various heterogeneity levels and datasets, it struggles particularly with CIFAR-10 and CIFAR-100, indicating challenges in learning effective local decoder models. Regarding distillation-based methods, FedKD and Co-Boosting demonstrate similar performance on SVHN and CIFAR-10. However, FedKD outperforms Co-Boosting on CIFAR-100, facilitated by the auxiliary public dataset for KD while Co-Boosting arduously generates its synthetic transfer dataset. Fed-ET improves over FedENS and is also competitive to FedKD. Notably, Fens consistently outperforms the best baseline in each scenario, except for SVHN at \(=0.01\), where FedCVAE-Ens excels. Fens achieves significant accuracy gains, surpassing the best baseline by \(11.4-26.9\%\) on CIFAR-10 and \(8.7-15.4\%\) on CIFAR-100, attributed to its advanced aggregator model.

We chart the client communication costs incurred by all algorithms in Figure 3. The clients in Fens expend \(3.6-4.3\) more than one-shot algorithms owing to the ensemble download and iterative 

[MISSING_PAGE_FAIL:6]

of Fens (details in Appendix B.2). We also show two versions of FedAdam, one achieving the accuracy of Fens and the other with its maximum accuracy to facilitate effective comparison of communication costs.

We observe that Fens with its iteratively trained aggregator significantly closes the accuracy gap between OFL (FedKD) and FL (FedAdam) across all datasets and heterogeneity levels. Remarkably, the boost achieved is sufficient to match FedAdam's accuracy at \(=\{0.01,0.05\}\) on the CIFAR-10 dataset. This comes at only a modest increase in communication costs which are \( 4\) that of OFL across all cases. We observe that FedAdam incurs \(30-80\) more communication than OFL to reach the same accuracy as Fens. Even adding multi-round support to FedKD only marginally improves its performance. While the best accuracy achieved by FedAdam still remains higher, it also comes at significant communication costs of \(47-96\) that of OFL. Furthermore, we observe that communication compression (FedAvg STC) fails to preserve the accuracy of FL under high heterogeneity. Thus Fens achieves the best accuracy vs. communication trade-off, demonstrating accuracy properties of iterative FL while retaining communication efficiency of OFL.

#### 3.3.1 When can Fens match iterative FL?

In this section, we aim to understand when Fens can match iterative FL. The performance of ensembles depends upon _(i)_ the quality of local models; and _(ii)_ data heterogeneity. The quality of local models in turn depends on the amount of local data held by the clients. As local models improve at generalizing locally, the overall performance of the ensemble is enhanced. In contrast, FL struggles to generate a good global model when the local datasets of clients significantly differ. Thus more volume of data does not analogously benefit FL due to high data heterogeneity. However, as heterogeneity reduces, FL excels and benefits significantly from collaborative updates. This suggests that Fens under _sufficiently large local datasets_ and _high heterogeneity_ can match iterative FL's performance. We confirm this intuition through the following experiments on the SVHN dataset.

**Setup.** We study the performance of FL and Fens by progressively increasing the volume of data held by the clients. To this end, we consider the classification task on the SVHN dataset due to the availability of an extended training set of \(604\,388\) samples, _i.e._, \( 10\) bigger than the default SVHN dataset. We then experiment with fractions ranging from \(10\) to \(100\)% of the total training set. Each client locally utilizes \(90\%\) for one-shot local model training and reserves \(10\%\) for iterative aggregator training, similar to previous sections. We then compare Fens with FedAvg (FL baseline) on three

Figure 4: **Fens against iterative FL. The R indicates the number of global rounds, signifying the multi-round version of the OFL baseline. Fens achieves accuracy properties of iterative FL (FedAdam) with a modest increase in communication cost compared to OFL (FedKD). Numerical accuracy results are included in Table 11 (Appendix D).**

levels of heterogeneity: \(=\{0.05,0.1,1\}\), varying from highly non-IID to IID. We tune the learning rate for FedAvg (details in Appendix B.2) and keep the remaining setup as in previous experiments.

**Results.** Figure 5 shows the results and confirms our prior insight behind the effective performance of Fens. Specifically, we observe that the growing volume of training data benefits Fens much more than FedAvg. When the data distribution is homogeneous \((=1)\), the performance of Fens improves faster than FedAvg, but still remains behind. On the other hand, under high heterogeneity (\(=0.01\)), Fens quickly catches up with the performance of FedAvg, matching the same accuracy when using the full training set. We conclude that under regimes of high heterogeneity and sufficiently large local datasets, Fens presents a practical alternative to communication expensive iterative FL.

### Performance on real-world datasets

In this section, we evaluate the performance of Fens on the real-world cross-silo FLamby benchmark . Specifically, we present 5 iterative baselines and 2 one-shot baselines along with the client local baselines (Figure 6). For the one-shot FedAvg and FedProx OFL baselines, we additionally tune the number of local updates. FedKD is infeasible in these settings since it requires a public dataset for distillation, unavailable in the medical setting. FedCVAE-Ens and Co-Boosting are also infeasible due to the difficulty in learning good decoder models or synthetic dataset generators for medical input data, a conclusion supported by their poor performance on the comparatively simpler CIFAR-100 task (Table 1). Figure 6 shows the results with the first row comparing Fens against iterative FL algorithms and the second row against one-shot FL and the client local baselines.

When comparing to iterative FL, we observe that Fens is on par for the Fed-Heart-Disease dataset and performs better for Fed-Camelyon16. The iterative FL performance is affected by high heterogeneity  where the deterioration is more significant for Fed-Camelyon16, which learns on large breast slides (\(10000 2048\)) than for Fed-Heart-Disease, which learns on tabular data. In such scenarios of heterogeneity, Fens can harness diverse local classifiers through the aggregator model to attain good performance. On the Fed-ISIC2019 dataset, however, Fens does not reach the accuracy of iterative FL. Clients in Fed-ISIC2019 exhibit high variance in local data amounts and model performance, with the largest client having \(12\)k samples and the smallest only \(225\) (Table 5, Appendix A). We thus speculate that the Fed-ISIC2019 dataset falls within the low local training fraction regime depicted in Figure 5, exhibiting a larger

Figure 5: Accuracy of Fens for increasing dataset size. Performance of Fens rapidly increases as the data volume increases. At high data heterogeneity, Fens matches iterative FL’s accuracy.

Figure 6: **Fens in FLamby. Fens is on par with iterative FL (row-1), except when local models are weak (Fed-ISIC2019) while remaining superior in the one-shot setting (row-2). Numerical results included in Tables 12 to 17 (Appendix D).**

accuracy gap compared to iterative FL. However, we note that Fens achieves superior performance over one-shot FedAvg and one-shot FedProx, while performing at least as well as the best client local baseline across all datasets. Overall, these observations for FL algorithms have spurred new interest in developing a better understanding of performance on heterogeneous cross-silo datasets . We show that Fens remains a strong competitor in such settings.

### Performance on language dataset

We now study the performance of Fens on the AG-News dataset, comparing it against top-performing baselines FedAdam and FedKD in the iterative and one-shot categories, respectively. Figure 7 shows the results: at \(=0.1\), FedKD achieves \(71.5\%\) accuracy, leaving a gap to FedAdam at \(82.3\%\). Fens effectively closes this gap, reaching \(78.8\%\). As heterogeneity reduces at \(=0.3\), all algorithms achieve higher accuracies. Fens improves upon FedKD from \(79.3\%\) to \(84.5\%\) while FedAdam achieves \(88.3\%\). Thus we observe consistent results on the language task as our vision benchmarks.

### Dissecting Fens

We extensively evaluate various aggregation functions (details in Appendix B.4) on the CIFAR-10 dataset across diverse heterogeneity levels. In particular, we assess static aggregation rules including averaging and weighted averaging, parametric aggregator models including a linear model, and a shallow neural network. We also evaluate an advanced version of voting  which involves computing competency matrices to reach a collective decision. In addition, we evaluate the Mixture-of-Experts (MoE) aggregation rule  where only the gating function is trained via federation. Figure 8 illustrates the accuracy, communication costs, and breakdown for all aggregations. Trained aggregator models outperform static aggregations, incurring additional costs for ensemble download and iterative training. The NN aggregator emerges as the top performer, offering the best accuracy vs. communication trade-off. Notably, the iterative training cost of the NN aggregator model for several rounds is lower than the OFL phase itself. Regarding accuracy, only MoE outperforms NN at \(=0.01\), where extreme heterogeneity induces expert specialization, while the trained gating network accurately predicts the right expert. However, MoE's performance declines as heterogeneity decreases while its communication costs remain higher due to the size of the gating network.

### Enhancing Fens efficiency

The Fens global model comprises the aggregator model stacked atop the ensemble of client local models. Although Fens achieves strong accuracy, the ensemble model can be computationally and memory intensive. We used FP32 to INT8 quantization in our previous experiments which reduces the memory costs by \(4\) (Appendix B.3). In this section, we explore two additional approaches to reduce Fens's overheads.

Figure 8: Accuracy of different aggregation functions on the CIFAR-10 dataset. NN offers the best accuracy vs. communication trade-off, with its iterative training taking up only a fraction of the total cost. Numerical accuracy values are included in Table 9 (Appendix D).

What if we distill Fens into a single model?To enable efficient inference, we can distill the Fens global model into a single model at the server using KD once the training is completed. Specifically, we distill the Fens ensemble model comprising \(20\) ResNet-8 client models and the shallow aggregator neural network into a single ResNet-8 model. Table 3 presents the results on the CIFAR-10 dataset for \(=\{0.01,0.05,0.1\}\) distilled using CIFAR-100 as the auxiliary dataset. We observe a slight accuracy drop arising from the process of distillation, which is standard behavior . While we distill using the standard distillation algorithm , we note that this accuracy gap can be further reduced through the use of more advanced distillation methods [16; 49].

What if we match the memory footprint of Fens to FedAdam?While the above approach enables efficient inference, we still need to mitigate training time costs. We now consider a downsized (DS) version of ResNet-8, where the width of a few layers is reduced so that the total size of the FENS downsized (Fens-DS) model approximately matches the size of the single model in FedAdam. Table 4 presents the results on the CIFAR-10 dataset for various heterogeneity levels. Note that no quantization is considered for Fens and Fens-DS, hence the contrast in values with Table 3. Under a comparable memory footprint, Fens-DS remains competitive with the original FedAdam, with only a slight drop in accuracy compared to Fens. On the other hand, using the downsized model as the global model in FedAdam-DS results in a significant accuracy drop (from \(39.32\%\) to \(29.69\%\)) under high data heterogeneity (\(=0.01\)). Thus, the memory overhead of Fens can be alleviated while retaining its communication benefits without too much impact on accuracy.

## 4 Discussion and Conclusion

**Limitations.** One limitation is the memory required on client devices to store the ensemble model for aggregator training. We explored quantization and downsizing to mitigate this issue. Future work could investigate aggregator models that do not require access to all client models in the ensemble. This memory issue is only present during training; after training, Fens can be distilled into a single global model on the server, enabling efficient inference as shown in Section 3.7. Another limitation is the increased vulnerability to attacks during iterative aggregator training, unlike OFL, which limits the attack surface to one round. However, this only affects the aggregator model, since the client local models are still uploaded in one shot. Privacy can be further enhanced in Fens through techniques such as differential privacy  or trusted execution environments . Specifically, clients can use differentially private SGD  for local training, providing a differentially private local model for the ensemble, while the aggregator training could leverage a differentially private FL algorithm .

**Benefits.** In addition to low communication costs and good accuracy, Fens provides three important advantages. First, it supports model heterogeneity, allowing different model architectures across federated clients . Second, Fens enables rapid client unlearning , towards the goal of the _right to be forgotten_ in GDPR . In Fens, unlearning a client can be achieved by simply re-executing the lightweight aggregator training by excluding the requested clients' model from the ensemble. This is more efficient than traditional FL, where disincorporating knowledge from a single global model can be costly. Lastly, if a small server-side dataset is available, such as a proxy dataset for bootstrapping FL [3; 19], Fens can train the aggregator model on the server. This makes Fens applicable in model market scenarios of OFL [7; 41] where clients primarily offer pre-trained models.

To conclude, we introduce Fens, a hybrid approach combining OFL and FL. Fens emphasizes local training and one-shot model sharing, similar to OFL, which limits communication costs. It then performs lightweight aggregator training in an iterative FL-like fashion. Our experiments on diverse tasks demonstrated that Fens is highly effective in settings with high data heterogeneity, nearly achieving FL accuracy while maintaining the communication efficiency of OFL. Additionally, Fens supports model heterogeneity, rapid unlearning, and is applicable to model markets.

  Algorithm & \(=0.01\) & \(=0.05\) & \(=0.1\) & Memory (MiB) \\  Fens & \(43.32 0.05\) & \(68.22 1.19\) & \(75.61 1.85\) & \(377.03\) \\ Fens-DS & \(43.08 0.01\) & \(44.59 1.72\) & \(72.43 1.76\) & \(17.95\) \\ FedAdam & \(39.32 0.05\) & \(68.74 2.76\) & \(78.73 3.55\) & \(18.85\) \\ FedAdam-DS & \(29.69 1.62\) & \(63.77 0.23\) & \(72.25 1.32\) & \(0.88\) \\  

Table 4: Fens vs FedAdam under similar memory footprint on CIFAR-10. DS stands for downsized.

  Algorithm & \(=0.01\) & \(=0.05\) & \(=0.1\) \\  Fens & \(44.20 2.95\) & \(68.22 1.19\) & \(75.61 1.85\) \\ Fens distilled & \(43.81 2.55\) & \(65.96 2.25\) & \(71.59 1.21\) \\  

Table 3: Accuracy of Fens after distillation on the CIFAR-10 dataset.