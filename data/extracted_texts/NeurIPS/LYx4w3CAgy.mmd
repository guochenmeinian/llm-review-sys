# LLM-Check: Investigating Detection of Hallucinations in Large Language Models

Gaurang Sriramanan

gaurangs@cs.umd.edu

&Siddhant Bharti

sbharti@cs.umd.edu

&Vinu Sankar Sadasivan

vinu@cs.umd.edu

&Shoumik Saha

smksaha@cs.umd.edu

&Priyatham Kattakinda

pkattaki@umd.edu

&Soheil Feizi

sfeizi@cs.umd.edu

###### Abstract

While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations-- outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.

## 1 Introduction

Over the past few years, Large Language Models (LLMs) such as GPT-4  and Llama  have become immensely popular due to their excellent performance on natural language inference, question-answering and summarization tasks. Nonetheless, it has been observed that these models often produce outputs that are fallacious, incorporating fictional or insubtantial details that can be partly misleading or entirely fabricated . Moreover, it is often observed that such model generations seem plausible, appearing tenable before further scrutiny. This phenomenon in LLMs, known as hallucinations, poses a significant challenge to their deployment in applications where accuracy and reliability are critical. Thus, the detection and mitigation of hallucinations in LLMs have been subjects of critical interest. Furthermore, the relative difficulty of hallucination detection can vary significantly in white-box or black-box settings, depending on the extent to which access is available to the original LLM that was utilized for generation.

Uncertainty estimation based on output logit based metrics such as perplexity or logit entropy have been used towards analyzing errors in structured prediction and generations produced by language models (Malinin and Gales, 2021; Kuhn et al., 2023). Furthermore, a more fine-grained approach can be used by considering the relevance of specific tokens in the generated text (Duan et al., 2023), or by directly obtaining quality or score assessments leveraging the zero-shot capabilities of LLMs (Fu et al., 2023), depending upon the task specifications. As an alternative approach, Wang et al. (2023) showed that self-consistency can be incorporated into the decoding strategy to improve upon naive greedy-decoding in the setting of chain-of-thought prompting, and demonstrate notable improvements in arithmetic and common-sense reasoning benchmarks. Specific to the setting of hallucination detection, consistency-based methods like SelfCheckGPT (Manakul et al., 2023) and INSIDE (Chen et al., 2024) proposed to analyze multiple responses of the same model given a common prompt. Indeed, SelfCheckGPT (Manakul et al., 2023) relies upon the fact that when LLMs are trained on a given concept, its output generations tend to be more consistent, while hallucinated information is less likely to be repeatedly generated across multiple stochastically sampled responses. Notably, SelfCheckGPT relies only upon black-box based API access to models such as GPT-3.5 (Brown et al., 2020) to assess hallucinations in a given sample sentence. Similarly, Chen et al. (2024) proposed the INSIDE detection method based on eigenvalue analysis of the covariance matrix over model embeddings generated across multiple responses of the same prompt, enabling hallucination detection at a population level instead of sample-level detection. Another setting studied towards the detection of hallucinations is that of methods using Retrieval Augmented Generation (RAG) (Lewis et al., 2021; Guo et al., 2022; Varshney et al., 2023; Niu et al., 2024), which assume access to a large external database of grounded information, pipelined with a fact-verification system.

A key limitation of these detectors is the assumption that they have access to multiple model generations or a large reference dataset. Consistency-based methods assume hallucinations are rare, and most generations are factually correct, even though a given LLM model often repeats similar errors. Moreover, these techniques induce additional overheads to generate multiple additional responses at inference time, or require large memory overheads to parallelize generations. Retrieval-based techniques rely on strong fact-verification and retrieval techniques, and may not be scalable for large databases due to the considerable inference costs involved overall. Furthermore, incorporating human-level annotation or manual fact checking is often not scalable in this setting.

This leads us to our research question -- Is it possible to identify the presence of hallucinations within a single LLM response in both white-box and black-box settings without incurring additional computational overheads at training or inference time? Towards this, given the practical constraints involved in this problem setting, we seek to analyze the efficacy of hallucination detection using analyses of the internal attention kernel maps, hidden activations and output prediction probabilities of an LLM itself, using an auxiliary substitute model if white-box access is unavailable. Furthermore, we study the effectiveness of different hallucination detection metrics both in the zero-knowledge setting as well as RAG-based settings where ground-truth reference materials are assumed to be available at inference time. We demonstrate that the detection methods proposed are extremely efficient by using teacher-forcing without additional overheads, and is accompanied by significant gains in detection performance relative to baseline approaches. In summary, we make the following contributions1 in this work:

* We analyze hallucination detection within a single LLM response using its corresponding internal attention kernel maps, hidden activations and output prediction probabilities. We utilize these diversified scoring methods from different model components to potentially maximize the capture of hallucinations amongst its various forms without incurring computational overheads at training or inference time.
* We analyze the problem in broad-ranging settings across diverse datasets: from zero-resource detection (FAVA (Mishra et al., 2024)), to the case when multiple model responses are available (SelfCheck (Manakul et al., 2023)), or when ground-truth references are indeed available at inference time (RAGTruth (Niu et al., 2024)).
* We demonstrate that the proposed method LLM-Check is indeed effective over these diverse hallucination detection settings, and achieves notable improvements over existing baselines.
* Furthermore, we demonstrate such performance gains while requiring only a fraction of the computational cost (speedups up to 45x and 450x), as the proposed method does not utilize multiple model generations or extensive external databases.

Related Works

Manakul et al. (2023) introduced SelfCheckGPT, a suite of hallucination detection methods in a zero-resource gray-box/black-box setting to assess the veracity of a LLM response, assuming only access to the output probability distributions. This is highly practical in real-world usage, since API calls often impede access to internal model activations. On the other hand, INSIDE (Chen et al., 2024) measures the self-consistency of the hidden states across multiple independent responses of an LLM, by performing a centered eigen-analysis of the covariance matrix of these hidden states. Notably, INSIDE follows a slightly different evaluation framework from other techniques which generally rely upon human annotations, wherein towards assessing the correctness of a model response INSIDE utilizes either ROUGE-L score or BERT-similarity scores being larger than a threshold with respect to a ground-truth. However, ROUGE-L is an n-gram based method that evaluates the longest common subsequence between the model response and the ground-truth. Thus, several hallucinatory words can be incorporated without a considerable change in the ROUGE-L score overall.

Kadavath et al. (2022) proposed to use the LLM itself to predict the probability of the generated response being True, referred to as Self-Prompt; this is primarily relevant for the context of multiple-choice-questions where the model explicitly has all the options in context. Furthermore, they also train models to predict the probability that a given multiple-choice-question will be correctly answered by calibrating its internal confidence values. Azaria and Mitchell (2023) proposed to train feedforward neural networks on the hidden activations of intermediate layers of LLMs on a True-False dataset of relatively short, simple sentences such as "The zebra uses flying for locomotion." This method is thus a white-box detection technique which also requires supervised training data to train the feedforward network (as noted by Manakul et al. (2023)); in contrast, we do not incorporate training of any sub-network or the LLM itself. Furthermore, it is fairly non-trivial to extend the technique towards more general settings such as when external references or multiple model responses are available at inference time. Yuksekgonul et al. (2024) investigate factual errors made by LLMs by modeling it as a constraint-satisfaction problem. Indeed, the factual query is specified by considering a sequence of Constraint-tokens and using a subsequent Verifier (such as ExactMatch or WikiData Search) to determine the satisfaction for each such constraint. This method is highly pertinent in settings where a fact-checking service is available for ready oracle-access that can provide feedback and annotations in real-time.

Mishra et al. (2024) introduced a fine-grained hallucination detection dataset (FAVA-Bench), wherein a novel hierarchy of hallucination types is identified and analyzed comprehensively. The authors first generate a synthetic hallucination training dataset by inserting specific errors using ChatGPT, and then fine-tune a Llama-2 model (FAVA) to explicitly detect these fine-grained hallucinations. The work also introduces a human-annotated dataset where the specific hallucination types are recorded with specified hallucination spans. We make extensive use of the FAVA datasets so released to compare multiple baselines on a common test-bed. Though retrieval augmented generation (RAG) helps to mitigate hallucination in LLMs, ungrounded generations still persist (Shuster et al., 2021; Li et al., 2024). Recently, Wu et al. (2023) showed how popular LLMs still hallucinate on different tasks even with the integration of RAG, and compiled the RAGTruth dataset with span-level human annotation. Furthermore, they fine-tune an LLM on their training data to detect hallucinations and its span, but thereby incur a significant computational overhead at training time. Due to paucity of space, we extend our discussion of related works further in Section-D of the Appendix.

## 3 Taxonomy and Formalisms for Hallucination Detection

**Notation:** Let \(\) denote the vocabulary of an LLM, and let \(^{*}\) denote the set of all possible sequences obtainable using the same vocabulary. We denote an individual token as \(x\), and a sequence of tokens as \(^{*}\). For an autoregressive model \(f\), let \(p_{f}(|)\) denote the next-token probability distribution based on input \(\). Moreover, given two token sequences \(=(x_{1}x_{2} x_{n})\) and \(}=(_{1}_{2}_{m})\), let \(}=(x_{1}x_{2} x_{n}_{1}_{2} _{m})\) denote their ordered concatenation.

Given a query or prompt \(}=(x_{1}x_{2} x_{n})\), let the LLM response of a model \(f\) be denoted as \(=(x_{n+1} x_{m})\). Thus, the broad goal of detection is to determine the presence or absence of hallucination in the output response \(\), when the input prompt is \(}\). In practice, LLMs such as chat-models require additional system prompts to be included along with the user input; here we assume these are contained within \(}\) for simplicity.

We present a broad taxonomy of the different settings in which hallucination detection is studied in Figure-1. Here, we discuss and analyze this classification in detail:

**I. Without External References:** First, we consider the setting wherein no external references are available to provide additional context to the LLM. Within this scenario, we can make a further classification based on whether multiple model responses can be potentially generated for the same prompt:

**IA. Single Model Response:** Here, only the original prompt \(}\) and fixed model output \(\) is assumed to be available. Thus, in this setting, hallucination detection using LLMs relies upon the tacit assumption that the query topic is covered to some reasonable extent in the pre-training data of the LLM. We comprehensively analyze this setting using the FAVA Human-Annotated dataset to compare different approaches. Moreover, Mishra et al. (2024) provide a fine-tuned LLama-2-chat model, that can be used to annotate specific error types in the given response \(\). In our evaluations, we binarize these outputs to only detect the presence or absence of hallucinations.

**IB. Multiple Model Responses:** Given a prompt \(}\) and a fixed model output \(\), SelfCheckGPT additionally generates stochastically sampled responses \(},},}\) in order to compute different metrics such as BertScore, n-gram, Question-Answer and LLM-prompt score with respect to \(\), to assess the absence or presence of hallucinations in \(\). Notably, these scores can be computed with a black-box model with only external API access. On the other hand, INSIDE only considers the input prompt \(}\), and computes eigen-decomposition of the covariance matrix of hidden activations across multiple generated samples \(},},}\). This is used to then statistically infer the presence of hallucinations within this set of generations, based on a possible lack of self-consistency at a "population level" between samples within this specific set of \(\) samples. Thus, INSIDE performs hallucination detection at a population-level of all the generated samples simultaneously, rather than detection on a given fixed (single) output response \(\).

**II. With External References:** In this setting, a broad-ranging set of data retrieved from external sources are used to affix additional context to assess the validity of the model response \(\). This is a highly pertinent setting, as the retrieved data can consist of up-to-date information from sources deemed reliable or trustworthy. Furthermore, with the inclusion of external data, LLMs can be used on topics not completely covered in its original pretraining data in a straightforward manner, without requiring computationally intensive rounds of additional training or fine-tuning.

**IIA. White-Box:** For a given prompt \(}\), we consider hallucination detection in an output response \(\) that is generated by a given LLM \(f\). If the original LLM \(f\) is available and accessible to compute the internal model activations, the setting is considered to be "white-box". For instance, INSIDE provides the population-level detection using the original source model so used, and thus falls in the family of white-box detection techniques.

**IIA. Black-Box:** If the original LLM \(f\) that generated the response \(\) for prompt \(}\) is no longer available or inaccessible, an auxiliary substitute LLM \(\) can be utilized (such as open-source LLMs like Llama-2) to compute scores with internal activations and attention kernel maps. This can be achieved by using teacher-forcing on the substitute LLM, and this setting is considered to be "black-box".

  
**Method** &  **Train** \\ **Indep.** \\  &  **Single** \\ **Response** \\  &  **Efficient** \\ **Efficient** \\  &  **Sample** \\ **Specific** \\  & 
 **Retrieval** \\ **Indep.** \\  \\  FAVA & \(\) & \(\) & \(\) & \(\) & \(\) \\  SelfCheckGPT & \(\) & \(\) & \(\) & \(\) & \(\) \\  INSIDE & \(\) & \(\) & \(\) & \(\) & \(\) \\  RAGTuth & \(\) & \(\) & \(\) & \(\) & \(\) \\  LLM-Check (ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Overview of Hallucination Detection Methods with FAVA, SelfCheckGPT, INSIDE, and RAGT Truth. Key- Train Indep: does not require model fine-tuning/training, Single Response: does not require multiple LLM responses, Efficient: compute & memory efficient, Sample Specific: sample-wise instead of population-level detection, Retrieval Indep: does not require RAG or reference databases.

Figure 1: Taxonomy for different settings of hallucination detection, and the different datasets and data splits that correspond to each such setting.

## 4 Proposed Method

In this section, we describe our method LLM-Check in detail. Given that we wish to perform hallucination detection without any training or inference time overheads, we propose to analyze all model-related latent and output observables available with a single forward-pass of the LLM. For a broad family of autoregressive models using the Transformer architecture (Vaswani et al., 2017), the input token embeddings are transformed to a sequence of hidden representations at each layer of the model. For an LLM with \(L\) internal layers, let \(_{l}\) denote the hidden representations at layer \(l\{1,,L\}\). The hidden representation at layer \((l+1)\) is then computed as follows:

\[_{l+1}=_{l}+_{l+1}+_{l+1}\]

where \(_{l+1}\) and \(_{l+1}\) denote the attention and MLP contributions respectively. Additional operations such as layer-normalization are commonly carried out, but we omit their details here to be concise. The attention contributions represent the updates to the representation of a token at a given layer by attending to all token representations of the previous layer. In particular, if \(Q,K,V\) denote the Query, Key and Values in the attention mechanism, the attention contribution is given by computing the kernel similarity of Q and K, and masking the values V with this kernel, and is simplified to be written as:

\[=Ker(Q,K)V=Softmax(}{}})V\]

where \(d_{k}\) is the dimension of the keys and queries. Generally, LLMs utilize multi-headed attention, wherein the kernel maps are individually computed over each head and concatenated, and then cast back to the appropriate dimension using Output projection matrices before being aggregated together. We note importantly that for auto-regressive LLMs, the attention kernel maps are lower triangular for each attention head, since a given token can only attend to the internal model representations of previous tokens in the sequence.

While LLMs do hallucinate, they still have a significantly appreciable degree of world-knowledge grounded on truthful facts encountered in the training stage, which is reflected in the fact that hallucinations are absent in some of the autoregressively generated sample outputs when multiple model responses are considered for the same prompt. In this work, we propose to directly analyze the variations in model characteristics between truthful and hallucinated examples by examining the rich semantic representations present in internal LLM representations, since we observe that the LLM can often generate the truthful version, albeit with potentially lower frequency. Thus, we posit that the model is in fact sensitive to the presence of non-factual, hallucinated information in responses which would otherwise be truthful, based on grounded precepts encountered in its training and that this can then be efficiently leveraged to perform hallucination detection within a single model response itself.

**1. Eigenvalue Analysis of Internal LLM Representations**

The differences in model sensitivity to hallucinated or truthful information would reflect in the rich semantic representations present in the hidden activations and the pattern of attention maps across different token representations. To quantitatively capture these variations, we propose to analyze the cross-covariance for hidden representations, and the kernel similarity map of self-attention across different tokens, since these form the foremost salient characteristics of the LLM itself. Given that we wish to do hallucination detection in black-box settings as well, we utilize teacher-forcing to

Figure 2: Schematic of detection pipeline using Eigenvalue Analysis of internal LLM representations.

obtain representations corresponding to \(}\), wherein system prompt tokens delineate the original prompt and model output response. Thus, for a given layer \(l\{1,,L\}\), let \(\) denote the hidden representations for input \(}\). If the token embedding dimension is \(d\) and \(}\) consists of \(m\) tokens, each hidden representation matrix \(\) is of shape \((d m)\). To be highly compute-efficient and enable real-time detection, we distill a simple yet salient scalar quantity - the mean log-determinant - from these variations in hidden representations using eigen-analysis. Theoretically, this is well-motivated as the eigenvalues and singular values capture the interaction in latent space between the different token representations corresponding to hallucinated and truthful sample sequences. We can compute the \((m m)\) covariance matrix \(^{2}\), and compute its log-determinant as follows:

\[^{2}\ =\ H},(^{2 })=_{i=1}^{m}_{i}^{2}=_{i=1}^{m}log\ _{i}^{2}=2_{i=1}^{m}log\ _{i}\]

where \(_{i}\) are the singular values of \(\). In order to remove the explicit dependence on input-length, we propose to utilize the mean log-determinant of \(^{2}\), that is, \(_{i=1}^{m}log\ _{i}\), which we term as Hidden Score. Here, we note crucial differences from the approach of INSIDE, which computes the centered covariance matrix _across multiple model responses_ to check self-consistency of the set of responses, and performs hallucination detection at the population level for a set of responses, rather than a given fixed model response \(\). We repeat this procedure across hidden representations of different layers; please refer to Section-F of the Appendix for detailed layer-wise analysis.

As an alternative approach, we now move towards utilizing internal components of the attention mechanism, namely the kernel similarity maps used within self-attention heads, to analyze hallucinations. If the number of self-attention heads utilized at each layer in the LLM architecture is \(a\), the attention maps can be represented as tensors of the shape \((a m m)\) for an input sequence \(}\) of length \(m\). If \(Ker_{i}\) represents the kernel similarity map corresponding to attention head \(i\), \(Ker_{i}\) is a lower-triangular square matrix of size \((m m)\) for each \(i\{1,,a\}\). Since it is lower-triangular, the eigenvalues of \(Ker_{i}\) are just the values on the principal matrix diagonal, since they are the roots of the characteristic equation \((Ker_{i}- I)=0\). Furthermore, since LLMs use the Softmax kernel, all the eigenvalues are also provably non-negative. We can thus obtain the log-determinant of \(Ker_{i}\) without using SVD or eigen-decomposition:

\[(Ker_{i})\ =\ _{j=1}^{m} Ker_{i}^{jj}\]

where \(Ker_{i}^{jj}\) represents the \((j,j)\) diagonal entry of the square matrix \(Ker_{i}\). Again, we obtain the mean log-determinant for each attention head, and aggregate them to obtain an alternative hallucination measure, which we call Attention Score. Here, we note that computing the Attention Score is extremely efficient, since it does not require any explicit eigen-decomposition. The overall pipeline used for hallucination detection using the Hidden Score and Attention Score is visualized in Figure-2. We further present a detailed illustrative example using the Attention Score in Section-C of the Appendix.

In black-box settings, we propose to utilize an auxiliary autoregressive LLM \(\), where we use teacher-forcing to obtain the scores from the auxiliary LLM instead to perform detection. We also note that since the MLP component of a transformer block is applied to each position separately and identically (Vaswani et al., 2017), it does not capture cross-token interactions, and thus we do not consider this component for detection itself.

**2. Output Token Uncertainty Quantification**

Alongside the internal model representations, we now consider the probability distribution \(p_{f}(|)\) over the output response tokens as induced by the LLM. Given that LLMs are trained with next-token prediction, the probability distribution \(p_{f}(|)\) for a given token can be highly salient toward the relative choices available for completion. Indeed, specifically for factual sentences that are part of topics covered in the training dataset of an LLM, we expect that its completions are such that the model likelihood of the output sequence is high. On the other hand, if the sentences arise from a distribution far different from that encountered in the training regime, the model outputs are more likely to be ungrounded in nature due to the lack of specificity between different tokens in the vocabulary.

We thus present the output token-level analysis techniques for identifying hallucinations in LLM responses. We propose to analyze the use of perplexity of the response \(\), given the prompt \(}\), as a potential measure for detecting hallucinations:

\[()=(-_{i=n}^{m} p_{f}(x _{i}|}_{<i}))\]

Furthermore, at each token position in the output response, we can consider the mean entropy of the probability distribution over all tokens (not just the token finally selected during autoregressive generation). We can further refine logit-entropy-based measures by considering only the top-k tokens rather than all possible tokens since the contribution from tokens of extremely low probability is likely noisy and non-salient. We refer to this as the Logit Entropy score, defined as

\[(,k)=-_{i=n}^{m}_{j=1}^{k}p_ {f}(x_{i}^{j}|}_{<i}) p_{f}(x_{i}^{j}|}_{<i})\]

where \(x_{i}^{j}\) is the token with \(j^{th}\) highest probability at output position \(i\), conditioned on the prefix \(}_{<i}\) However, given that the entire sentence may not be hallucuatory, the length-average scores may not be as salient as we desire to predict hallucinations accurately. Thus, in light of this, we consider the Windowed Logit Entropy score, which computes the logit entropy scores across overlapping token windows and returns the logit entropy of the window with the maximum score. This score is thereby sensitive to short sequences of hallucuatory material and is not diluted by sequence length normalization.

We propose these two distinct lines of analysis towards hallucination detection, which we collectively term as LLM-Check, in order to adequately capture the extremely heterogeneous and diversified forms of hallucination displayed by modern Large Language Models over different domains. Towards this, the Eigen-analysis of internal LLM representations helps highlight the consistent pattern of modifications to the hidden states and the model attention across different token representations in latent space when hallucinations are present in model responses as compared to truthful, grounded responses. On the other hand, the uncertainty quantification of the output tokens helps analyze hallucinations based on the likelihood assigned by the model on the actual tokens predicted at a specific point in the sequence generated auto-regressively.

Before analyzing the quantitative experimental results in the following section, we first present qualitative comparisons of the proposed method with the most pertinent baselines in Table-1. We present various trade-offs and advantages in the table such as to whether the method requires fine-tuning of an LLM, if it inherently requires multiple model responses to detect hallucinations, if the method is computationally efficient, if it performs detection on per-sample basis or at a population level, and whether the method is inherently dependent on retrieval during inference time. We observe that our proposed approach is indeed qualitatively advantageous in all of the aspects so mentioned.

## 5 Experimental Results

**Datasets and Detection Details:** To analyze the efficacy of the proposed hallucination detection measures and perform a comparative analysis with existing baselines, we utilize the taxonomy and datasets presented in Figure-1. For the setting of detection without external references with a single model response, we utilize the FAVA-Annotation (Mishra et al., 2024) dataset, wherein we deem passages containing any form of hallucination to be hallucinated overall. Furthermore, we utilize the fine-grained classification of different forms of hallucination as annotated in the FAVA dataset to analyze the efficacy of the detection measures across these different types. In the setting of detection without external references with multiple model responses, we utilize the SelfCheckGPT dataset (Manakul et al., 2023), consisting of 1908 sentence level annotated samples, alongside 20 stochastically generated responses from GPT 3 (text-davinci-003). Moreover, to fairly compare across different detection methods, we also augment the originally released version of the FAVA-Annotation dataset by generating 20 additional responses from GPT 3 using the original prompt set. By doing so, we are able to obtain new baselines for SelfCheckGPT and INSIDE on the FAVA-Annotation dataset using the multiple responses so generated.

For our evaluations in the setting of hallucination detection with external references assumed available, we primarily consider the RAFTuth dataset (Wu et al., 2023). In detail, we use the Summarization subset which was created by prompting six different LLMs with CNN/Daily Mail dataset and recent news, resulting in more than \(5K\) samples. We use the span-level human annotation so provided in the dataset and considered a sample is hallucinated if there exists any hallucinated span in the response. Additionally, we also consider white-box and black-box detection settings for the RAGTruth dataset, depending on the original LLM model used for generating the responses. Apart from RAGTruth, we also consider the training data-split of FAVA wherein several references are provided, and specific hallucination errors are edited in synthetically to obtain pairs of ground-truth responses with and without hallucinations. We consider a 500-sample subset of the FAVA train-data towards this setting of detection with external references.

**Models and Metrics:** We utilize popular open-source LLM chat-models such as Llama-2-7b Chat , Vicuna  and Llama-3-instruct  as our autoregressive LLMs with their corresponding tokenizers provided by HuggingFace , for both white-box and black-box evaluations. We compute the suite of hallucination detection scores proposed in Section-4, which we collectively term as LLM-Check, and present standard threshold based detection metrics such as Accuracy, Area under the ROC curve (AUROC), True Positive Rate (TPR) at low (5%) False Positive Rate (FPR) and F1 score. In each setting, we consider balanced datasets, with an equal number of samples with and without hallucinations present, except for the SelfCheck dataset where we follow the setup utilized in the original paper.

### Detection Results on Datasets with no External References as Context

First, we analyze the setting of hallucination detection when no grounded references are provided as context. We present the consolidated detection metrics in Table-2 using the FAVA-Annotation dataset. To evaluate methods such as SelfCheckGPT and INSIDE, we make use of the stochastically sampled responses of GPT-3 that we augment to the original dataset. We observe that in this zero-context setting, many of the baselines perform fairly poorly in comparison to LLM-Check. We remark that while entropy-based scores were used for comparisons in the SelfCheckGPT paper, the best-performing variant was seen to be SelfCheckGPT-Prompt using GPT-3 itself, which was used to generate the original hallucinations. Here, we observe that when using Llama2-7B for SelfCheckGPT-Prompt, the results are much worse. This might be due to the fact that SelfCheckGPT is less effective at the passage level after aggregation is performed over the individual sentences. We observe that the Attention Scores are the most effective across different models, and obtains significantly higher detection scores.

   Model & Measure & AUROC & Accuracy & TPR @ 5\% FPR & F1 Score \\  Llama-2-7B & Self-Prompt & 50.30 & 50.30 & - & 66.53 \\ Llama-2-7B & FAVA Model & 53.29 & 53.29 & - & 43.88 \\ Llama-2-7B & SelfCheckGPT-Prompt & 50.08 & 54.19 & - & 67.24 \\ Llama-2-7B & INSIDE & 59.03 & 57.98 & 13.17 & 39.66 \\   \\   & PPL Score & 53.22 & 58.68 & 3.59 & 68.33 \\  & Window Entropy & 56.90 & 56.59 & 2.99 & 42.52 \\  & Logit Entropy & 53.80 & 55.99 & 2.99 & 56.73 \\  & Hidden Score (LY 20) & 58.44 & 58.08 & 11.98 & 59.66 \\  & Attn Score (LY 21) & **72.34** & **67.96** & **14.97** & **69.27** \\   & PPL Score & 53.96 & 56.89 & 3.59 & 64.20 \\  & Window Entropy & 55.24 & 58.38 & 5.99 & 66.02 \\  & Logit Entropy & 52.29 & 55.69 & 1.80 & 57.31 \\  & Hidden Score (LY 15) & 58.22 & 59.28 & 10.18 & **66.99** \\  & Attn Score (LY 19) & **71.69** & **66.47** & **24.55** & 62.00 \\   & PPL Score & 53.22 & 58.68 & 3.59 & 67.40 \\  & Window Entropy & 56.90 & 56.59 & 2.99 & 55.52 \\   & Logit Entropy & 53.80 & 55.99 & 2.99 & 56.27 \\   & Hidden Score (LY 15) & 57.10 & 57.78 & 10.78 & 65.38 \\   & Attn Score (LY 23) & **68.19** & **65.87** & **15.57** & **70.53** \\   

Table 2: Detection results on the FAVA-Annotation Dataset wherein no External References are available. For methods such as INSIDE and SelfCheckGPT-Prompt, we utilize the multiple model responses generated by GPT-3.

We further analyze the layer-wise performance of scores obtained from internal model representations in Section-F of the Appendix. We observe that while the Attention Scores are generally the most effective, particularly around Layer 20, the oscillation in detection performance across different layers can be large, relative to the case of using the Hidden Score which is fairly more stable across layers.

In Table-3, we further analyze the performance of LLM-Check against SelfCheckGPT on the SelfCheckGPT Dataset, where multiple GPT-3 generated responses are available in addition to the original model response. We follow the setup used in the original paper with imbalanced classes, and similarly also report Area under the Precision-Recall curve in-place of AUROC. We again find that the Attention scores provide better detection performance over all three metrics of AUC-PR, Accuracy and TPR@5%FPR, despite not utilizing the additional model responses included in the dataset. Here, we present a Prompt-variant of LLM-Check, where instead of prompting the LLM to output "Yes" or "No" based on the additional model responses and then hard-coding to 0/1 scores as in SelfCheckGPT-Prompt, we compute Attention Scores of modified prompts which include the additionally generated model responses, and then aggregate over them as done in SelfCheckGPT-Prompt. We find that LLM-Check with Attention-Prompt performs similarly well, though the inference time is higher in this setting, due to the iteration over the different model responses.

### Detection Results on Datasets with External References

We now analyze the setting wherein external references are assumed to be available to the model at inference time. In Table-4, we present evaluations obtained using a Llama-2-7b model in both white-box and black-box settings. For the black-box setting, we considered four different models - Llama-2-13b, Llama-2-70b, GPT-4, Mistral-7b. In the white-box setting, we observe that the Hidden Score achieves a higher F1 score compared to all other detection methods. The Attention scores are however better in the overall black-box setting, where we compute a weighted average since different models hallucinate at different frequencies on the same prompt data. Additionally, we also observe that LLM-Check performs better on larger models. For example, with Hidden score, our approach obtains \(54.11\%\), \(59.67\%\), and \(59.31\%\) AUROC on the 7b, 13b, and 70b variants of Llama-2 respectively. Furthermore, the AUROC increases to \(61.87\%\) for hallucinations arising from the GPT-4 model.

Lastly, we present the evaluations on 500 examples of the FAVA Train-split in Table-5. Notably, this dataset differs from RAGTruth in that the hallucinations in the generations are inserted synthetically using GPT-3, and thus potentially induce changes to the joint-distribution of the sequence level probabilities. Indeed, we observe that output-token uncertainty estimates using entropy out-perform even the attention-based scores in this setting. Thus, we observe that different detection methods may prove optimal depending on the problem setup and underlying data distribution.

### Time-Complexity Analysis

We compare the overall runtime cost of the proposed detection scores with other baselines using a Llama-2-7b Chat model on the FAVA-Annotation dataset on a single Nvidia A5000 GPU in Figure-3. We observe that the Logit and Attention scores are indeed very efficient, while the Hidden Score is slightly slower since it uses SVD. We also observe that LLM-Check is considerably faster than most baselines with speedups of up to 45x and 450x, since it only uses model representations with teacher

   Model & Method & AUC-PR & Accuracy & TPR @ 5\% FPR \\  Llama-2 & SelfCheck & 72.84 & 51.44 & 4.81 \\ Llama-3 & SelfCheck & 75.06 & 54.84 & 5.10 \\   \\  Llama-2 & Attn Score & **80.04** & 58.91 & 9.41 \\ Llama-2 & Prompt & 79.46 & **61.21** & 8.76 \\ Llama-3 & Attn Score & 79.96 & 58.92 & **9.48** \\ Llama-3 & Prompt & 78.49 & 58.54 & 7.11 \\   

Table 3: Detection results on the SelfCheckGPT Dataset wherein no External References are available, but when multiple model responses are indeed available. We report the results using the same data-split as the original SelfCheck [Manakul et al., 2023] paper, and thus the number of positive and negative samples are imbalanced in this table: 1392/1908 samples have hallucinations present.

forcing, without additional inference time overheads. This runtime analysis for the Eigenvalue based methods represents the **total time** for computing Attention and Hidden scores from **all 32 layers** of Llama-2-7b. That is, the Attention score computation for all 32 layers takes 0.22 seconds per example, and the Hidden score computation for all 32 layers requires 2.72 seconds per example, when averaged over the FAVA Annotation dataset. Since the overhead to compute scores for all layers is so small, we expect that they can be utilized for real-time detection of hallucinations in model responses.

## 6 Conclusions

In this work, we analyze the problem of detecting hallucinations within a single response of an LLM, and propose LLM-Check, an effective suite of techniques that only rely upon the internal hidden representations, attention similarity maps and logit outputs of an LLM. We demonstrate the efficacy of LLM-Check over broad-ranging settings and diverse datasets: from zero-resource detection to cases where multiple model generations or external databases are made available at inference time, or with varying access restrictions to the original source LLM. Moreover, we observe that LLM-Check obtains considerable improvements over baseline methods in these settings, without requiring fine-tuning or retraining of LLMs. Furthermore, by utilizing only teacher-forcing at inference time without additional overheads, we show that LLM-Check is extremely compute-efficient, requiring only a fraction of the runtime compared to other detection baselines, with speedups of up to 45x and 450x.

    &  &  &  \\   & & Llama-2-7b & Llama-2-13b & Llama-2-70b & GPT-4 & Mistral-7b & Overall \\   & AUROC & 54.11 & 59.67 & 59.31 & 61.87 & 53.68 & 57.24 \\  & Accuracy & 56.33 & 59.66 & 58.42 & 68.52 & 54.15 & 57.62 \\  & TPR@5\%FPR & 8.14 & 12.41 & 9.9 & 3.7 & 5.18 & 8.37 \\  & F1 Score & 61.51 & 50.42 & 66.14 & 67.86 & 32.58 & 47.45 \\   & AUROC & 53.73 & 52.46 & 56.97 & 52.13 & 52.11 & 53.27 \\  & Accuracy & 54.07 & 55.17 & 57.92 & 59.26 & 54.66 & 55.79 \\  & TPR@5\%FPR & 7.69 & 8.97 & 6.93 & 0.00 & 4.15 & 6.01 \\  & F1 Score & 58.7 & 50.57 & 61.26 & 61.02 & 43.23 & 50.45 \\   & AUROC & 52.08 & 55.71 & 56.38 & 55.83 & 52.61 & 54.58 \\  & Accuracy & 53.17 & 56.9 & 57.43 & 59.26 & 53.89 & 55.90 \\  & TPR@5\%FPR & 4.98 & 15.86 & 1.98 & 7.41 & 10.36 & 10.08 \\  & F1 Score & 53.98 & 33.68 & 62.01 & 54.9 & 49.29 & 47.51 \\   & AUROC & 53.95 & 51.18 & 55.14 & 50.34 & 50.43 & 51.68 \\  & Accuracy & 55.43 & 53.79 & 57.43 & 57.41 & 53.89 & 54.83 \\  & TPR@5\%FPR & 7.24 & 9.66 & 4.95 & 0.00 & 6.22 & 6.65 \\  & F1 Score & 53.74 & 15.09 & 66.41 & 60 & 48.41 & 42.62 \\   & AUROC & 54.19 & 60.05 & 60.01 & 63.51 & 55.37 & 58.30 \\  & Accuracy & 54.52 & 59.66 & 60.89 & 66.67 & 56.99 & 59.23 \\   & TPR@5\%FPR & 5.88 & 14.48 & 12.87 & 7.41 & 5.18 & 9.87 \\   & F1 Score & 54.5 & 55.97 & 55.06 & 67.8 & 57.72 & 57.18 \\   

Table 4: Detection on the RAGTtruth Dataset using Llama-2-7b model in white-box and black-box setting, with the “Overall” column presenting the weighted average results for the black-box models.

Figure 3: Averaged runtime analysis of different hallucination detection methods.

   Model & Measure & AUROC & Accuracy & TPR @ 5\% FPR \\   & PPL Score & 74.20 & 70.00 & 26.00 \\  & Window Entropy & **77.00** & **72.00** & **34.00** \\  & Logit Entropy & 74.36 & 71.00 & 26.00 \\  & Hidden Score & 51.44 & 54.00 & 4.00 \\  & Attn Score & 69.57 & 66.60 & 11.60 \\   & PPL Score & 73.48 & 68.80 & 13.20 \\  & Window Entropy & 78.44 & 72.00 & **28.00** \\  & Logit Entropy & **79.24** & **73.60** & 28.00 \\   & Attn Score & 71.91 & 68.20 & 19.60 \\   

Table 5: Detection results on synthetic hallucinations on FAVA Train Split data with External References included.

## 7 Acknowledgments

This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO's Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), an Amazon Research Award and an award from Capital One.