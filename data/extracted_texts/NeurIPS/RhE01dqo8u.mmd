# Feature Selection in the Contrastive Analysis Setting

Ethan Weinberger

Paul G. Allen School of Computer Science

University of Washington

Seattle, WA 98195

ewein@cs.washington.edu

&Ian C. Covert

Department of Computer Science

Stanford University

Stanford, CA 94305

icovert@stanford.edu

&Su-In Lee

Paul G. Allen School of Computer Science

University of Washington

Seattle, WA 98195

suinlee@cs.washington.edu

###### Abstract

Contrastive analysis (CA) refers to the exploration of variations uniquely enriched in a _target_ dataset as compared to a corresponding _background_ dataset generated from sources of variation that are irrelevant to a given task. For example, a biomedical data analyst may wish to find a small set of genes to use as a proxy for variations in genomic data only present among patients with a given disease (target) as opposed to healthy control subjects (background). However, as of yet the problem of feature selection in the CA setting has received little attention from the machine learning community. In this work we present contrastive feature selection (CFS), a method for performing feature selection in the CA setting. We motivate our approach with a novel information-theoretic analysis of representation learning in the CA setting, and we empirically validate CFS on a semi-synthetic dataset and four real-world biomedical datasets. We find that our method consistently outperforms previously proposed state-of-the-art supervised and fully unsupervised feature selection methods not designed for the CA setting. An open-source implementation of our method is available at https://github.com/suinleelab/CFS.

## 1 Introduction

In many scientific domains, it is increasingly common to analyze high-dimensional datasets consisting of observations with many features [1; 2; 3]. However, not all features are created equal: noisy, information-poor features can obscure important structures in a dataset and impair the performance of machine learning algorithms on downstream tasks. As such, it is often desired to select a small number of informative features to consider in subsequent analyses. For example, biologists may seek to measure the expression levels of a small number of genes and use these in a variety of future prediction tasks, such as disease subtype prediction or cell type classification. Measuring only a subset of features may also yield other benefits, including reduced experimental costs, enhanced interpretability, and better generalization for models trained with limited data.

While feature selection is relatively straightforward when supervision guides the process (e.g., class labels), such information is often unavailable in practice. In the unsupervised setting, we can select features for specific objectives like preserving clusters [4; 5] or local structure in the data [6; 7], or we can aim to simply reconstruct the full feature vector [8; 3]. However, these approaches are likely to focus on factors that dominate in the dataset, even if these structures are uninteresting for a specificdownstream analysis. A third setting that lies between supervised and unsupervised is _contrastive analysis_[9; 10; 11], which we describe below.

In the contrastive analysis (CA) setting, we have two datasets conventionally referred to as the _target_ and _background_. The target dataset contains observations with variability from both "interesting" (e.g., due to a specific disease under study) and "uninteresting" (e.g., demographic-related) sources. On the other hand, the corresponding background dataset (e.g., measurements from healthy patients with similar demographic profiles) is assumed to only contain variations due to the uninteresting sources. The goal of CA is then to isolate the patterns enriched in the target dataset and not present in the background for further study. Here, the target-background dataset pairing provides a form of weak supervision to guide our analysis, and carefully exploiting their differences can help us focus on features that are maximally informative of the target-specific variations.

For example, suppose our goal were to measure gene expression in cancer patients for a subset of genes to better understand molecular subtypes of cancer. Identifying factors specific to cancerous tissues is of great importance to the cancer research community, as these intra-cancer variations are essential to study disease progression and treatment response . However, selecting an appropriate set of genes is not straightforward, as unsupervised methods are likely to be confounded by variations due to nuisance factors such as age, ethnicity, or batch effects, which often dominate the overall variation in gene expression and other omics datasets (genomics, transcriptomics, proteomics) [10; 13; 14]. Moreover, fully supervised methods may not be applicable as they require detailed labels that may not be available _a priori_ (e.g., due to the rapid rate of development and testing of new cancer treatments ). Thus, methods that can effectively leverage the weak supervision available in CA may be able to select more appropriate features for a given analysis.

Isolating salient variations is the focus of many recent works on CA, including many that generalize unsupervised representation learning methods to the CA setting [9; 10; 11; 16; 17; 18; 19; 13]. This work aims to develop a principled approach for _contrastive feature selection_, which represents an important use case not considered in prior work.1

The remainder of this manuscript is organized as follows: we begin (Section 2) with an introduction to the contrastive analysis setting and the unique challenges therein, followed by a discussion of related work (Section 3). We then proceed propose CFS, a feature selection method that leverages the weak supervision available in the CA setting (Section 4). CFS selects features that reflect variations enriched in a target dataset but are not present in the background data, and it does so using a novel two-step procedure that optimizes the feature subset within a neural network module. Subsequently, we develop a novel information-theoretic characterization of representation learning in the CA setting to justify our proposed method (Section 5); under mild assumptions about the data generating process, we prove that our two-step learning approach maximizes the mutual information with target-specific variations in the data. Finally, we validate our approach empirically through extensive experiments on a semi-synthetic dataset introduced in prior work as well as four real-world biomedical datasets (Section 6). We find that CFS consistently outperforms supervised and unsupervised feature selection algorithms, including state-of-the-art methods that it builds upon.

## 2 Problem formulation

We now describe the problem of feature selection in the CA setting. Our analysis focuses on an observed variable \(^{d}\) that we assume depends on two latent sources of variation: a salient variable \(\) that represents interesting variations (e.g., disease subtype or progression), and a separate background variable \(\) that represents variations that are not relevant for the intended analysis (e.g., gender or demographic information). Thus, we assume that observations are drawn from the data distribution \(p()= p(,)p(,)dz\), which we depict using a graphical model in Figure 1.

Our goal is to select a small number of features that represent the relevant variability in the data, or that contain as much information as possible about \(\). To represent this, we let \(S[d]\{1,,d\}\) denote a subset of indices and \(_{S}\{_{i}:i S\}\) a subset of features. For a given value \(k<d\), we aim to find \(S\) with \(|S|=k\) where \(_{S}\) is roughly equivalent to \(\), or is similarly useful for downstream tasks (e.g., clustering cells by disease status). We state this goal intuitively here following prior work , but we later frame our objective through the lens of mutual information (Section 5).

What distinguishes CA from the supervised setting is that we lack labels to guide the feature selection process . We could instead perform unsupervised feature selection , but this biases our selections towards variations that dominate the data generating process; thus, in real-world genomics datasets, we may inadvertently focus on demographic features that are irrelevant to a specific disease. Instead, the CA setting provides a form of weak supervision via two separate unlabeled datasets . First, we have a target dataset \(_{t}\) consisting of samples with both sources of variation, or which follow the data distribution \(p()\) defined above (Figure 1). Next, we have a background dataset \(_{b}\) that consists of samples _with no salient variation_; this may represent a single fixed disease status, or perhaps no disease, and we formalize these as samples from \(p(=s^{})\) for a fixed value \(s^{}\). We note that individual samples from the target and background datasets are not paired and we do not assume the same number of samples from the target and background datasets (i.e., \(|_{t}||_{b}|\)). By analyzing both datasets, we hope to distinguish sources of variability specific to \(\) rather than \(\).

The challenge we must consider is how to properly exploit the weak supervision provided by separate target and background datasets. To identify a maximally informative feature set \(_{S}\), we must effectively disentangle the distinct sources of variation in the data, and we wish to do so while allowing for different dataset sizes \(|_{b}|=m\) and \(|_{t}|=n\), greater variability in \(\) driven by irrelevant information \(\) than salient factors \(\), and with no need for additional labeling. Our work does so by building on previous work on contrastive representation learning [10; 16; 18] and neural feature selection [3; 23], while deriving insights that apply beyond the feature selection problem.

## 3 Related work

Contrastive analysisPrevious work has designed a number of representation learning methods to exploit the weak supervision available in the CA setting. Zou et al.  initially adapted mixture models for CA, and recent work adapted PCA to summarize target-specific variations while ignoring uninteresting factors of variability . Other recent works developed probabilistic latent variable models [18; 16; 17; 19], including a method based on VAEs . Severson et al.  experimented with adding a sparsity prior to perform feature selection within their model, but the penalty was designed for fully supervised scenarios: the penalty was manually tuned by observing how well pre-labeled target samples separated in the resulting latent space, which is not suitable for the scenario described in Section 2 where such labels are unavailable. Our work is thus the first to focus on feature selection in this setting, and we provide an information-theoretic analysis for our approach that generalizes to other forms of representation learning in the CA setting.

Feature selectionFeature selection has been extensively studied in the machine learning literature, see [24; 25] for recent reviews. Such methods can be divided broadly into filter [26; 27], wrapper [28; 29] and embedded [30; 31] methods, and they are also distinguished by whether they require labels (i.e., supervised versus unsupervised). Recent work has introduced mechanisms for neural feature selection [32; 3; 23; 33; 34; 35], which learn a feature subset jointly with an expressive model and allow a flexible choice of prediction target and loss function. Our work builds upon these methods, which represent the current state-of-the-art in feature selection. However, unlike prior works that

Figure 1: Data generating process for target and background samples. Observed values \(\) are generated from two latent variables: a salient variable \(\) and a background variable \(\). Both are active in the target dataset, while the salient variable is fixed at \(=s^{}\) in the background data.

focus on either supervised [23; 34] or unsupervised feature selection [3; 36], we focus on contrastive dataset pairs and consider how to properly exploit the weak supervision in this setting.

## 4 Proposed method: contrastive feature selection

We aim to select \(k\) features within \(^{d}\) that provide maximum information about the salient latent variable \(\) in our data generating process (Section 2). Such a procedure must be performed carefully, as standard unsupervised methods may optimize for selecting features that capture much of the overall variance in a dataset but not the (potentially subtle) variations of interest due to \(\).

To do so, we begin with the following intuitive idea: suppose we have _a priori_ a variable \(^{l}\) with \(l<d\) that serves as a proxy for \(\) (i.e., it summarizes all the uninteresting variation in the data). We assume for now that \(\) is already available, and that it should explain a significant portion of \(\)'s variance, so that any remaining variance is due solely to factors of interest. We can then aim to find a complementary set of features that capture the remaining variations in \(\) not explained by \(\), and we do so by learning the feature set in conjunction with a function that reconstructs the full vector \(\). The reconstruction function \(f^{k+l}^{d}\) uses both \(\) and the learned feature set \(_{S}\) as inputs, and we can learn \(f\) and \(S\) jointly as follows:

\[*{arg\,min}_{,|S|=k}*{}_{ _{t}}||f(,\;_{S};)-||^{2}.\] (1)

We call this approach _contrastive feature selection_ (CFS). Notably, eq.1 requires data with both background and salient variations, and thus it uses only the target dataset \(_{t}\). Intuitively, optimizing this objective will select the set of \(k\) features \(S\) that best capture the variations present in the target dataset but which are not captured by the variable \(\).

To implement CFS we must make two choices: (1) how to optimize the feature set \(S[d]\), and (2) how to obtain \(\). First, assuming we already have \(\), we can leverage recent techniques for differentiable feature selection to learn \(S\)[23; 3]. Such methods select features by optimizing continuous approximations of discrete variables at the input layer of a neural network: selected variables are multiplied with a 1 and allowed to pass through the gates, while rejected features are multiplied with a 0. In our implementation of CFS, we use the Gaussian-based relaxation of Bernoulli

Figure 2: Overview of contrastive feature selection (CFS). Our approach consists of a two-step optimization procedure. In the first step (left), we train an autoencoder on a background dataset, which we assume is generated solely from irrelevant variations due to \(\). In the second step (right), we train a feature selector layer using our target dataset. To encourage the selection of features that best reflect salient variations due to \(\), the selector layer’s output is concatenated with the output of the fixed encoder \(g\), and then passed to a learned reconstruction function \(f\). Ideally, the feature set \(_{S}\) chosen by the selector will capture only the factors of variation unique to the target data.

random variables termed _stochastic gates_ (STG), proposed by Yamada et al. .2 Let \(^{d}\) denote a random vector of STG values parameterized by \(^{d}\), so that samples of \(\) are defined as

\[_{i}=(0,(1,_{i}+)),\] (2)

where \(_{i}\) is a learnable parameter and \(\) is drawn from \((0,^{2})\). By multiplying each feature by its corresponding gate value \(_{i}\) and incorporating a regularization term that penalizes open gates (i.e., the number of indices with \(_{i}>0\)), feature selection can be performed in a fully differentiable manner. Leveraging the STG mechanism, we obtain the following continuous relaxation of eq.1:

\[*{arg\,min}_{,}*{}_{ _{t}}||f(,\,;)-||^{2}+ _{i=1}^{d}(}{}),\] (3)

where \(\) denotes the Hadamard product, \(\) is a hyperparameter than controls the number of selected features, and \(\) is the standard Gaussian CDF .

Next, we must decide how to obtain \(\) so that it captures irrelevant variations due to \(\), and ideally not those due to the salient variable \(\). We propose to learn it as a function of \(\), so that \(=g(;)\) and \(g:^{d}^{l}\). Intuitively, this is where we can leverage the background dataset, because it contains only uninteresting variations due to \(\). We learn this background representation function in conjunction with a reconstruction network \(h:^{l}^{d}\) using the following objective:

\[*{arg\,min}_{,}*{}_{ _{b}}||h(g(;);)-||^{2}.\] (4)

Previous representation learning methods for CA, such as the contrastive VAEs of Abid and Zou  and Severson et al. , also considered learning two representations; however, they learned both representations _jointly_ by training simultaneously with target and background data. In our feature selection context, this joint training setup would entail optimizing with the sum of eq.3 and eq.4, so that \(g(;)\) is updated based on both \(_{b}\) and \(_{t}\). This is potentially problematic, because the background representation \(=g(;)\) may absorb salient information that then cannot be learned by \(_{S}\). Indeed, subsequent work observed that such joint training procedures may fail to properly segregate shared and target-specific variations, with target-specific variations being captured by the background representation function and vice versa .

In order to avoid this undesirable "leakage" of salient information into our background representation, we thus separate our objectives into a two-stage optimization procedure. For a given target-background dataset pair, we use the first stage to train an autoencoder network using _only background samples_ (eq.4). In the second stage, we then use the encoder network \(=g(;)\) as our background proxy, freeze its weights, and learn a feature set \(_{S}\) by optimizing eq.3 using _only target samples_. We depict this procedure graphically in Figure2. In the following section, we further motivate this two-stage approach by developing an information-theoretic formulation of representation learning in the CA setting, which illustrates why this approach compares favorably to joint contrastive training [11; 16], as well as fully unsupervised feature selection approaches .

## 5 Information-theoretic analysis

In this section, we motivate our approach from Section4 through a framework of mutual information maximization. To justify learning two representations that are optimized in separate steps, we frame each step as maximizing a specific information-theoretic objective, which we then relate to our ultimate goal of representing the salient variation \(\) via a feature set \(_{S}\).

We simplify our analysis by considering that all variables \((,,)\) are all discrete rather than continuous, which guarantees that they have non-negative entropy . The learned representations are deterministic functions of \(\), and we generalize our analysis by letting the representation from the second step be an arbitrary variable \(\) rather than a feature set \(_{S}\). Thus, our analysis applies even beyond the feature selection setting.

### Analyzing our approach

Before presenting our main results, we state two sets of assumptions required for our analysis. Regarding the data generating process (see Figure 1), we require mild conditions about the latent variables being independent and about each variable being explained by the others.

**Assumption 1**.: _(Assumptions on data distribution.) We assume that the latent variables \(,\) are roughly independent, that \(\) is well-explained by the latent variables, and that each latent variable is well-explained by \(\) and the other latent variable. That is, we assume a small constant \(>0\) such that \(I(;)\), \(H(,)\), and \(H(,)\)._

Intuitively, we require independence so that salient information is not learned in the first step with \(\) and then ignored by \(\); the other conditions guarantee that \(\) can be recovered at all via \(\). We also require an assumption about the learned representations, which is that they do not affect the independence between the latent variables \(,\). This condition is not explicitly guaranteed by our approach, but it also is not discouraged, so we view it as a mild assumption.

**Assumption 2**.: _(Assumptions on learned representations.) We assume that \(,\) remain roughly independent after conditioning on the learned representations \(,\). That is, we assume a small constant \(>0\) such that \(I(;)\) and \(I(;)\)._

With these assumptions, we can now present our analysis. First, we re-frame each step of our approach in an information-theoretic fashion:

* The first step in Section 4 is analogous to maximizing the mutual information \(I(;=s^{})\). We do this for only a single value \(s^{}\) in practice, but for our analysis we consider that this is similar to \(I(;)\), which would be achieved if we optimized \(\) over multiple background datasets with different values. This holds because \(I(;)=_{s}[I(;=s)]\).
* The second step in Section 4 is analogous to maximizing the mutual information \(I(,;)\). However, by treating \(\) as fixed and rewriting the objective as \(I(,;)=I(;)+I(;)\), we see that we are in fact only maximizing \(I(;)\), or encouraging \(\) to complement \(\).
* Finally, our true objective is for \(\) to be equivalent to \(\), which we view as maximizing \(I(;)\). If their mutual information is large, then \(\) is as useful as \(\) in downstream prediction tasks . The challenge in CA is optimizing this when we lack labeled training data.

Now, we have the following result that shows our two-step procedure implicitly maximizes \(I(;)\), which is our true objective in contrastive analysis (proof in Appendix B).

**Theorem 1**.: _Under Assumptions 1 and 2, learning \(\) by maximizing \(I(;)\) and learning \(\) by maximizing \(I(;)\) yields the following lower bound on the mutual information \(I(;)\):_

\[I(;)+I(;)-H()-4 I (;).\] (5)

The result above shows that our true objective is related to \(I(;)\) and \(I(;)\), which represent the two steps of our learning approach. Specifically, the sum of these terms provides a lower bound on \(I(;)\), so maximizing them implicitly maximizes \(I(;)\) (similar to the ELBO in variational inference ). Within the inequality, \(H()\) acts as a rough upper limit on \(I(;)\), and the constant \(\) from Assumptions 1 and 2 contributes to looseness in the bound, which becomes tight in the case where \(=0\). This result requires several steps to show, but it is perhaps intuitive that letting \(\) serve as a proxy for \(\) and then learning \(\) as a complementary variable helps recover the remaining variation, or encourages a large value for \(I(;)\).

### Comparison to other approaches

We now compare our result for our two-stage approach with two alternatives: (1) a simpler one-stage "joint" training approach similar to previously proposed contrastive VAEs [11; 16], and (2) an unsupervised learning approach similar to the concrete autoencoder of Baln et al. .

First, the joint training approach learns \(\) and \(\) simultaneously rather than separately, which we can view as optimizing one objective corresponding to each dataset (background and target). Following the argument in Section 5.1, the background data is used roughly to maximize \(I(;)\). However,the target data is used to maximize \(I(,;)\) with respect to both \(\) and \(\) rather than just \(\). Thus, we can modify the inequality in eq.5 to obtain the following lower bound on \(I(;)\):

\[I(,;)+I(;)-;) }_{}-H()-4 I(;).\] (6)

This follows directly from Theorem1 and the definition of mutual information . By maximizing \(I(,;)=I(;)+I(;)\) rather than just \(I(;)\), we may inadvertently reduce the lower bound through \(I(;)\) and therefore limit the salient information that \(\) is forced to recover. Indeed, our experiments show that the joint training approach results in lower-quality features compared to our proposed two-stage method (Section6).

Next, we consider fully unsupervised methods that learn a single representation \(\). Such methods do not leverage paired contrastive datasets, so we formalize them as maximizing the mutual information \(I(;)\), which is natural when \(\) is used to reconstruct \(\)[3; 40]. Intuitively, if most variability in the data is due to irrelevant variation in \(\), these methods are encouraged to maximize \(I(;)\) rather than \(I(;)\). This can be seen through the following result (see proof in AppendixB).

**Theorem 2**.: _When we learn a single representation \(\) by maximizing \(I(;)\), we obtain the following simultaneous lower bounds on \(I(;)\) and \(I(;)\):_

\[I(;)-H()+I(;)  I(;)\] \[I(;)-H()+I(;)  I(;).\] (7)

The implication of Theorem2 is that if \(\) explains more variation than \(\) in the observed variable \(\), or if we have \(I(;)>I(;)\), then we obtain a higher lower bound for \(I(;)\) than \(I(;)\). This is undesirable, because the mutual information with \(\) roughly decomposes between \(\) and \(\): as we show in AppendixB, Assumptions1 and 2 imply that we have \(|I(;)+I(;)-I(;)| 2\).

We conclude that under this information-theoretic perspective and several mild assumptions, we can identify concrete benefits to a two-stage approach that leverages paired contrastive datasets.

### Caveats and practical considerations

There are several practical points to discuss about our analysis, but we first emphasize the aim of this section: our goal is to justify a two-stage approach that leverages paired contrastive datasets, and to motivate performing the optimization in these steps separately rather than jointly. Our approach provides clear practical benefits (Section6), and this analysis provides a lens to understand why that is, even if it does not perfectly describe how the algorithm works in practice.

The caveats to discuss are the following. (1) In practice, we minimize mean squared error for continuous variables rather than maximizing mutual information; however, the two are related, and AppendixC shows that minimizing mean squared error maximizes a lower bound on the mutual information. (2) Our analysis relates to an arbitrary variable \(\) rather than a feature set \(_{S}\). This is not an issue, and in fact implies that our results apply to more general representation learning and could improve existing methods . (3) The assumptions we outlined may not hold in practice (see Assumptions1 and 2). However, this represents a legitimate failure mode rather than an issue in our analysis: for example, if the latent variables are highly dependent, the second step may not recover meaningful variation from \(\) because this will be captured to some extent by \(\) in the first step.

## 6 Experiments

Here, we empirically validate our proposed CFS approach by using it to select maximally informative features in multiple target-background dataset pairs that were analyzed in previous work. We begin by considering a semi-synthetic dataset, Grassy MNIST , which lets us control the data generation process and the strength of uninteresting variations; we then report results on four publicly available biomedical datasets. For each target-background dataset pair, samples in the target dataset have ground-truth class labels related to the salient variations, which we use to evaluate the selected features. However, distinguishing these classes may be difficult using methods not designed for CA.

To illustrate the benefits of leveraging the weak supervision available in the CA setting, we compared against unsupervised feature selection methods applied to our target datasets. In particular, we considered two recently proposed methods that claim state-of-the-art results in fully unsupervised settings: the concrete autoencoder (CAE)  and differentiable unsupervised feature selection (DUFS) . Moreover, to illustrate that naive adaptations of previous methods designed for fully supervised settings are not sufficient to fully exploit the weak supervision available in the CA setting, we compared against the state-of-the-art fully supervised methods STG  and LassoNet  trained to simply classify samples as target versus background.

Finally, to illustrate the importance of carefully learning CFS's background representation function \(g\) (as discussed in Sections 4 and 5), we experimented with three variations of CFS. First, we performed the two-stage approach proposed in Section 4, where \(g\) is pretrained on background data (CFS-Pretrained). Second, we tried learning \(g\) jointly with our feature set (CFS-Joint), similar to previous work on representation learning for CA . Finally, in an attempt to combine the benefits of our proposed approach (i.e., improved separation of salient and irrelevant variations) with the simplicity of joint training (i.e., the ability to carry out optimization in a single step), we experimented with a modification of the joint training procedure: for target data points, we applied a stop-gradient operator to the output of the background representation function \(g\) (CFS-SG). By doing so, \(g\) is only updated for background data points and thus is not encouraged to pick up target-specific variations.

We refer the reader to Appendix D for details on model implementation and training.3 For all experiments we divided our data using an 80-20 train-test split, and we report the mean \(\) standard error over five random seeds for each method.

### Semi-synthetic data: Grassy MNIST

The Grassy MNIST dataset, originally introduced in Abid et al. , is constructed by superimposing pictures with the "grass" label from ImageNet  onto handwritten digits from the MNIST dataset . For each MNIST digit, a randomly chosen grass image was cropped, resized to be 28 by 28 pixels, and scaled to have double the amplitude of the MNIST digit before being superimposed onto the digit (Figure 2(a)). For a background dataset we used a separate set of grass images with no digits (Figure 2(b)), and our goal here is to select features that best capture the target-specific digit variations.

We began by using our CFS variants with a fixed background dimension \(l=20\) and baseline models to select \(k=20\) features. Qualitatively, the features selected by CFS (Figure 2(c); Figure S1(a)-b) concentrate around the center where digits are found, as desired. On the other hand, features selected by baselines (Figure 2(d)) are undesirably located far from the center where digits are never found (CAE), or are clustered in only a subset of locations where digit-related variations can be found (STG, DUFS), or both (LassoNet). These results illustrate the unsuitability of fully unsupervised methods for CA, as such methods may select features that contain minimal information on the target-specific salient variations. Moreover, these results illustrate that naive adaptations of supervised methods are not suitable for CA, as features that are sufficient to discriminate between target and background samples do not necessarily contain maximal information on the target-specific salient variations.

Figure 3: **a-b**, Example target (**a**) and background (**b**) images from the Grassy MNIST dataset. Our goal here is to select the set of features that best capture the target-dataset-specific variations due to the MNIST digits. **c-d**, \(k=20\) features selected by CFS (**c**) versus unsupervised (CAE , DUFS ) and supervised (STG , LassoNet ) baselines (**d**).

We then evaluated the quality of features selected by each method by training random forest classifiers to predict the digit (i.e., 0-9) in each image, using only the \(k\) selected features. We report the accuracy of the classifiers on a held out test set in Figure 4a for varying values of \(k\). We find that our pretrained and stop-gradient CFS models outperform baseline methods, with a minimal gap between them. On the other hand, CFS-Joint selected lower quality features and did not always outperform baselines, reflecting the pitfalls of joint training discussed in Sections 4-5. We next assessed how CFS's performance varies with respect to the ratio of background noise vs. salient variation by varying the scale of grass noise versus MNIST digit: when selecting \(k=20\) features at each increment, we found that CFS continued to outperform baselines across noise scales (Figure 4b). Finally, we assessed the impact of the background representation size \(l\) for our CFS variants by selecting \(k=20\) features on the original Grassy MNIST dataset with varying values of \(l\), and we found that our results were largely insensitive to this hyperparameter (Figure 4c). We also found that this phenomenon was consistent across varying choices of \(k\) (Figure S3).

### Real-world datasets

We next applied CFS and baseline methods to four real-world biomedical datasets: protein expression levels from trisomic and control mice that were exposed to different experimental conditions (Mice Protein) , single-cell gene expression measurements from mice intestinal epithelial cells exposed to different pathogens (Epithelial Cell) , single-cell gene expression measurements from patients with acute myeloid leukemia before and after a bone marrow transplant (AML Treatment) , and gene expression measurements from a high-throughput single-cell gene knockout perturbation screen (ECCITE-seq) . For each target dataset, we obtained a corresponding background collected as part of the same study. We refer readers to Appendix E for further details on each dataset.

Feature quality was again assessed by training random forest models to classify target samples into known subgroups of target points, and we report classification accuracy on a held-out test set for varying numbers of features in Figure 5. Once again we find that our pretrained and stop-gradient

Figure 4: **a**, Quantitative performance comparison of CFS versus baseline supervised (S) and unsupervised (U) methods on Grassy MNIST for varying numbers of features \(k\). **b**, We vary the relative contribution of grass noise to the dataset and assess each method’s performance when used to select \(k=20\) features. **c**, We select \(k=20\) features on the original Grassy MNIST dataset and vary CFS’s background representation dimension \(l\) to understand the impact of this hyperparameter.

Figure 5: Benchmarking CFS and baseline supervised (S) and unsupervised (U) methods on four real-world biomedical datasets.

CFS variants outperformed baseline fully supervised and unsupervised methods, demonstrating that carefully exploiting the weak supervision available in the CA setting can lead to higher-quality features for downstream CA tasks. Furthermore, we find that CFS-Joint's performance was inconsistent across datasets, and that it sometimes underperformed baseline models. This phenomenon further illustrates the need to carefully train the background function \(g\) as discussed in Sections 4-5 to ensure the selection of features that are maximally informative for CA. Moreover, we found that these results were consistent for other choices of downstream classifier model, including XGBoost  and multilayer perceptrons (Figure S4). Using the mice protein dataset, we also performed further experiments assessing the importance of the background dataset for CFS's performance. In particular, we assessed the performance of our pretrained and gradient stopping CFS variants as the number of background samples varied, and we also compared against a variant of CFS where the background encoder was fixed at a random initialization (i.e., the background encoder was not updated during training). We found that CFS's performance initially improved as the number of background samples increased before saturating (Table S2), and we also found that the randomly initialized CFS variant exhibited significantly worse performance for lower numbers of features (Table S3).

Finally, to further illustrate the utility of our method for exploring real-world biomedical datasets, we performed a deeper analysis of the specific genes selected by CFS on the Epithelial Cell pathogen infection dataset, and we present these results in Appendix F. In short, we found that CFS tended to select genes that exhibited subtle cell-type-specific responses to pathogen infection. On the other hand, features selected by baseline methods were sufficient for distinguishing between control and infected cells, but did not capture these more subtle cell-type-specific effects. This result thus illustrates CFS' potential to facilitate new biological insights.

## 7 Discussion

In this work we considered the problem of feature selection in the contrastive analysis (CA) setting. Our goal is to discover features that reflect salient variations enriched in a target dataset compared to a background dataset, and we tackled the problem by proposing CFS (Section 4), a method we motivated with a new information-theoretic analysis of representation learning in the CA setting (Section 5). Experiments on both simulated and real-world datasets show that features selected by CFS led to superior performance on downstream CA tasks as compared to those selected by previous feature selection methods. CFS represents a promising method to identify informative features when salient variations are subtly expressed in unlabeled contrastive datasets, and our theoretical analysis may prove useful for developing and comparing other learning methods designed for the CA setting.