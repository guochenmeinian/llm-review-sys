# On the Role of Randomization

in Adversarially Robust Classification

 Lucas Gnecco Heredia\({}^{1}\)1  Muni Sreenivas Pydi\({}^{1}\)  Laurent Meunier\({}^{2}\)

**Benjamin Negrevergne\({}^{1}\) Yann Chevaleyre\({}^{1}\) \({}^{1}\)** CNRS, LAMSADE, Universite Paris Dauphine - PSL \({}^{2}\) Payflows

{lucas.gnecco-heredia,muni.pydi}@dauphine.psl.eu

{benjamin.negrevergne,yann.chevaleyre}@lamsade.dauphine.fr

laurent@payflows.io

###### Abstract

Deep neural networks are known to be vulnerable to small adversarial perturbations in test data. To defend against adversarial attacks, probabilistic classifiers have been proposed as an alternative to deterministic ones. However, literature has conflicting findings on the effectiveness of probabilistic classifiers in comparison to deterministic ones. In this paper, we clarify the role of randomization in building adversarially robust classifiers. Given a base hypothesis set of deterministic classifiers, we show the conditions under which a randomized ensemble outperforms the hypothesis set in adversarial risk, extending previous results. Additionally, we show that for any probabilistic binary classifier (including randomized ensembles), there exists a deterministic classifier that outperforms it. Finally, we give an explicit description of the deterministic hypothesis set that contains such a deterministic classifier for many types of commonly used probabilistic classifiers, _i.e._ randomized ensembles and parametric/input noise injection.

## 1 Introduction

Modern machine learning algorithms such as neural networks are highly sensitive to small, imperceptible adversarial perturbations of inputs . In the last decade, there has been a back-and-forth in research progress between developing increasingly potent attacks  and the creation of practical defense mechanisms through empirical design . In particular, probabilistic classifiers (also called stochastic classifiers or randomized classifiers) have been widely used to build strong defenses which can be roughly categorized into two groups: noise injection techniques  and randomized ensembles . The first group includes methods that add noise at inference, usually to the input , an intermediate layer activation  or the weights of a parametric model  like a neural network. Most of this work is experimental, with some theoretical papers that try to justify the use of such methods . The second group, inspired by game theory, create a mixed strategy over base classifiers as a finite mixture . The argument behind this kind of models is that it becomes harder for an attacker to fool multiple models at the same time .

Intuitively, the greater flexibility of probabilistic classifiers implies that they should outperform their deterministic counterparts in adversarially robust classification. For instance, one of the earliest works using randomization to improve robustness to adversarial attacks  claims that _"randomization at inference time makes the network much more robust to adversarial images"_. Dhillon et al.  propose a pruning method that _"is stochastic and has more freedom to deceive the adversary"_.

[MISSING_PAGE_FAIL:2]

Preliminaries

### Adversarially Robust Classification

We consider a classification setting with input space \(^{d}\) and a finite label space \(\). The space \(\) is equipped with some norm \(\|\|\), which is commonly set to be the \(_{2}\) or \(_{}\) norms. Let \(()\) denote the true data distribution, which can be factored as \((x,y)=(y)_{y}(x)\), where \(()\) denotes the marginal probability distribution of \(\) over \(\) and \(_{y}()\) denotes the conditional probability distribution of the data over \(\) given class \(y\).

A _deterministic classifier_ is a function \(h:\) that maps each \(x\) to a fixed label in \(\). The \(0\)-\(1\) loss of \(h\) on a point \((x,y)\) is given by, \(^{01}((x,y),h)=\{h(x) y\}\).

A _probabilistic classifier_ is a function \(:()\) that maps each \(x\) to a probability distribution over \(\). To label \(x\) with \(\), one samples a random label from the distribution \((x)()\). In practice, if \(\) consists of \(K\) classes, \(()\) is identifiable with the \(K\)-simplex \(^{K}\) that consist of vectors \(u^{K}\) such that \(_{i=1}^{K}u_{i}=1\). Therefore, one can think of \((x)\) as a probability vector for every \(x\).

The \(0\)-\(1\) loss of \(\) on \((x,y)\) is given by, \(^{01}((x,y),)=_{(x)}[ \{ y\}]=1-(x)_{y}\). Note that \(^{01}((x,y),)\) is a generalization of the classical \(0\)-\(1\) loss for deterministic classifiers, which can only take values in \(\{0,1\}\).

Given \(x\), we consider a data perturbing adversary of budget \( 0\) that can transport \(x\) to \(x^{} B_{}(x)=\{x^{} d(x,x ^{})\}\), where \(B_{}(x)\) is the closed ball of radius \(\) around \(x\). The adversarial risk of a probabilistic classifier \(\) is defined as follows.

\[_{}()=}_{y}[ _{}^{y}()]=}_{y} }_{x_{y}}[_{x^{} B_{ }(x)}^{01}((x^{},y),)]@note{ footnote}{The measurability of the $0$-1$ loss under attack (inner part of (1)) is non-trivial and depends on various factors like the type of ball considered for the supremum (closed or open) , and the underlying $$-algebra \@@cite[cite]{[\@@bibref{}{Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,B,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,B,Bishop,Bishop,B,Bishop,Bishop,B,Bishop,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,B,Bishop,B,B,Bishop,B,B,B,B,Bishop

### Probabilistic Classifiers Built from a Base Hypothesis Set

In this paper, we study probabilistic classifiers that are constructed from a _base hypothesis set_ (BHS) of (possibly infinitely many) deterministic classifiers, denoted by \(_{b}\). We use the name _mixtures_ for these type of classifiers. In the following, we show that many of the probabilistic classifiers that are commonly used in practice fall under this framework.

Let \(_{b}\) be a BHS of deterministic classifiers from \(\) to \(\), which we assume is identifiable with a Borel subset of some \(^{p}\). Let \((_{b})\) be a probability measure over the measure space \((_{b},(_{b}))\), where \((_{b})\) denotes the Borel \(\)-algebra on \(_{b}\). One can construct a probabilistic classifier \(_{}:()\), built from \(_{b}\), that maps \(x\) to a probability distribution \(_{x}()\), where \(_{x}(y)\ =_{h}(h(x)=y)\). We now instantiate \(_{b}\) and \((_{b})\) for two main types of probabilistic classifiers that are commonly used in practice: randomized ensembles and noise injection classifiers.

For a _randomized ensemble classifier_ (_REC_), \(_{M}(_{b})(_{b})\) where \(_{M}(_{b})\) is the set of all discrete measures on \(_{b}\) supported on a finite set of \(M\) deterministic classifiers, \(\{h_{1},,h_{M}\}\). In this case, \(_{}(x)\) takes the value \(h_{m}(x)\) with probability \(p_{m}=(h_{m})\) for \(m[M]\), where \(_{m[M]}p_{m}=1\). RECs were introduced in  and play the role of mixed strategies in the adversarial robustness game. They are a simple randomization scheme when a finite number of classifiers are at hand, and both training and attacking them represent a challenge .

For a _weight-noise injection classifier_ (_WNC_), \(_{b}=\{h_{w}:w\}\) where \(h_{w}\) is a deterministic classifier with parameter \(w\). In this case, \(\) is taken to be a probability distribution over \(\) with the understanding that each \(w\) is associated with a unique \(h_{w}_{b}\). For example, \(_{b}\) can be the set of all neural network classifiers with weights in the set \(^{p}\). Any probability distribution \(\) on the space of weights \(\) results in a probabilistic classifier \(_{}\). Alternatively, \(_{b}\) can be generated by injecting noise \(z\) sampled from a distribution \(\) to the weights \(w_{0}\) of a fixed classifier \(h_{w_{0}}\). In this case, the probabilistic classifier \(_{}\) maps \(x\) to a probability distribution \(_{x}()\), where \(_{x}(y)=_{z}(h_{w_{0}+z}(x)=y)\). Weight noise injection has been explicitly used in , but there are many other approaches that implicitly define a probability distribution over the parameters of a model  and sample one or more at inference.

For an _input-noise injection classifier_ (_INC_), \(_{b}=\{h_{}:\}\) where \(h_{}(x)=h_{0}(x+)\) for a fixed deterministic classifier \(h_{0}\). In this case, \(\) is taken to be a probability distribution over \(\) (which is unrelated to the data generating distribution), and \(_{}\) maps \(x\) to a probability distribution \(_{x}()\), where \(_{x}(y)=_{}(h(x+)=y)\). Injecting noise to the input has been used for decades as a regularization method . INCs are studied in  as a defense against adversarial attacks, and are closely related to randomized smoothing classifiers .

**Remark 1** (Measurability).: To ensure the well-definedness of the adversarial risk, we need to ensure that the \(0\)-\(1\) loss under attack (inner part of (1)) is measurable. The \(0\)-\(1\) loss for a fixed class \(y\) can now be seen as a function from the product space \(_{b}\) to \(\{0,1\}\):

\[f(x,h)=^{01}((x,y),h)=\{h(x) y\} \]

For a distribution \((_{b})\) and the associated probabilistic classifier \(_{}\), we can rewrite the \(0\)-\(1\) loss of \(_{}\) at \(x\) as

\[^{01}((,y),_{})=}_{h}[ \{h() y\}]=_{_{b}}f(,h)d(h) \]

Note that if \(f\) is Borel measurable in \(_{b}\), then by Fubini-Tonelli's Theorem, the \(0\)-\(1\) loss as a function of \(x\) with an integral over \(_{b}\) shown in (3) is Borel measurable. By [16, Appendix A, Theorem 27], the \(0\)-\(1\) loss under attack is then universally measurable and therefore the adversarial risk, which requires the integral over \(\) of the loss under attack, is well-defined over this universal \(\)-algebra [44, Definition 2.7]. Recent work by Trillos et al.  have also shown the existence of Borel measurable solutions for the _closed ball_ formulation of adversarial risk.

In this work, we assume that the function \(f\) in (2) is Borel measurable in \(_{b}\) for every \(y\), so that (1) is well-defined. This assumption is already satisfied in the settings that are of interest for our work. As an example, it holds when the classifiers at hand are neural networks with continuous activation functions. More details on Appendix A. For a deeper study on the measurability and well-definedness of the adversarial risk in different threat models and settings, see .

From Deterministic to Probabilistic Classifiers

Let \(_{b}\) be a class of deterministic classifiers. In this section, we compare the robustness of probabilistic classifiers built upon \(_{b}\) with the robustness of the class \(_{b}\) itself. Note that if we consider the trivial mixtures \(=_{h}\) for \(h_{b}\), we obtain the original classifiers in \(_{b}\), so it is always true that \(_{(_{b})}_{}(_{ })_{h_{b}}_{}(h)\). We are interested in the situations in which this gap is strict, meaning that mixtures strictly outperform the base classifiers. With this in mind, we introduce the notion of _Matching Penny Gap_ to quantify the improvement of probabilistic classifiers over deterministic ones.

Theoretical Results on Robustness of Probabilistic Classifiers.We begin by presenting a theorem which shows that the adversarial risk of a probabilistic classifier is at most the average of the adversarial risk of the deterministic classifiers constituting it. The proof of this result will be a first step towards understanding the conditions that favor the mixture classifier.

**Theorem 3.1**.: For a probabilistic classifier \(_{}:()\) constructed from a BHS \(_{b}\) using any \((_{b})\), we have \(_{}(_{})_{h}[ _{}(h)]\).

A natural follow-up question to is to ask _what conditions guarantee a strictly better performance of a probabilistic classifier_. We know that this gap can be strict, as can be seen in toy examples like the one shown in Figure 0(b) where \(_{b}\) is the set of all linear classifiers, and there exist two distinct classifiers \(f_{1},f_{2}\) both attaining the minimum adversarial risk among all classifiers in \(_{b}\). Any non-degenerate mixture of \(f_{1},f_{2}\) attains a strictly better adversarial risk, demonstrating a strict advantage for probabilistic classifiers.

From the proof of Theorem 3.1 it is clear that a strict gap in performance between probabilistic and deterministic classifiers is only possible when the following strict inequality holds for a non-vanishing probability mass over \((x,y)\).

\[_{x^{} B_{}(x)}_{h}[ \{h(x^{}) y\}]<_{h}[_{x^{ } B_{}(x)}\{h(x^{}) y\}]. \]

The above condition holds at \((x,y)\) if there exists a subset of vulnerable classifiers \(_{vb}_{b}\) with \((_{vb})>0\)**any of which can be forced to individually misclassify** the point \((x,y)\) by an adversary using (possibly different) perturbations \(x^{}_{h} B_{}(x)\) for \(h_{vb}\), **but cannot be forced to misclassify all at the same time using the same perturbation \(x^{} B_{}(x)\)**. Such a configuration is reminiscent of the game of _matching pennies_ (see Appendix B). If \(_{b}\) is in a _matching penny_ configuration at \((x,y)\), a mixed strategy for classification (i.e. a mixture of \(_{b}\)) achieves a decisive advantage over any pure strategy (i.e. any deterministic base classifier) because the adversary can only force a misclassification on a subset of all vulnerable classifiers. Such a configuration was first noted in  in the context of improved adversarial attacks on RECs, and also in  for computing the adversarial risk of RECs of size \(M=2\). Figure 1 illustrates such a condition on an example REC of size \(M=2\) over a single point (Figure 0(a)), and over a simple discrete distribution (Figure 0(b)). We formalize this intuition with the definition below.

**Definition 1** (Matching penny gap).: The matching penny gap of a data point \((x,y)()\) with respect to a probabilistic classifier \(_{}\) constructed from \(_{b}\) using \((_{b})\) is defined as,

\[_{_{}}(x,y)=(_{vb}(x,y))-^{}(x,y), \]

where \(_{vb}(x,y)_{b}\) denotes the _valnerable subset_ and \(^{}(x,y)\) the _maximal simultaneous vulnerability_ of base classifiers \(_{b}\), defined below.

\[_{vb}(x,y) =\{h_{b}: x^{}_{h} B_{}(x) h(x^{}_{h}) y\},\] \[_{svb}(x,y) =\{^{}_{b}: x^{ } B_{}(x) h^{},h(x^{ }) y\},\] \[^{}(x,y) =_{^{}_{svb}(x,y)}( ^{}).\]

If \(_{_{}}(x,y)>0\), we say that \(_{}\) is in _matching penny configuration_ at \((x,y)\).

For example, in Figure 0(a), \(_{_{}}(x_{0},y_{0})=1-\{(f_{1}),(f_{2})\}=\{(f_ {1}),(f_{2})\}\) where \(_{b}=\{f_{1},f_{2}\}\). The subset \(_{vb}(x,y)\) contains all classifiers that can be individually fooled by an optimal attacker. The collection of subsets \(_{svb}(x,y)\) contains all subsets \(^{}\) of classifiers that can be _simultaneously fooled_. Then, \(^{}(x,y)\) is the maximum mass of classifiers that can be fooled simultaneously. Thus, \(_{_{}}(x,y)\) measures the gap between the mass of classifiers that are individually vulnerable and the maximum mass of classifiers that can be fooled with only one perturbation.

The following theorem strengthens Theorem 3.1 by showing that \(_{}()\) is a strictly convex function if and only if there is a non-zero expected matching penny gap.

**Theorem 3.2**.: For a probabilistic classifier \(_{}:()\) constructed from a BHS \(_{b}\) using any \((_{b})\),

\[_{}(_{})=}_{h}[ _{}(h)]-}_{(x,y)}[_{ _{}}(x,y)]. \]

Proof sketch.: By interchanging expectations over \(h\) and \((x,y)\) and using the fact that \(_{x^{} B_{}(x)}\{h(x^{}) y\}=1\) if and only if \(h_{vb}(x,y)\), we first show the following.

\[}_{h}[_{}(h)]=}_{(x,y)}[(_{vb}(x,y))]. \]

Arguing from the definition of \(^{}(x,y)\), we then show that \(_{x^{} B_{}(x)}}_{h}[ \{h(x^{}) y\}]=^{}(x,y)\). Taking expectation with respect to \((x,y)\), we get,

\[_{}(_{})=}_{(x,y)} [^{}(x,y)]. \]

Combining (8) and (7) yields (6). 

The following corollary shows that a lower bound on the expected matching penny gap is both necessary and sufficient for a mixture to strictly outperform any deterministic classifier in \(_{b}\).

**Corollary 3.1**.: For \(^{}(_{b})\), \(_{}(_{^{}})<_{h_{b}} _{}(h)\) if and only if the following condition holds.

\[}_{(x,y)}[_{_{^{}}}(x,y)]> }_{h^{}}[_{}(h)]-_{h _{b}}_{}(h) \]

Additionally, \(_{(_{b})}_{}(_{ })<_{h_{b}}_{}(h)\) if and only if there exists \(^{}(_{b})\) for which (9) holds.

Figure 1: Toy examples demonstrating a strict gap in adversarial risk between deterministic and probabilistic classifiers (RECs).

**Remark 2** (Multiple optimal classifiers in matching penny configuration).: For finite \(_{b}\), the assumption (9) of Corollary 3.1 holds whenever there exist two distinct optimal classifiers \(h_{1}^{*},h_{2}^{*}_{b}\) with a positive expected matching penny gap. In such a case, the RHS of (9) is zero for any \(\) that is a mixture of \(h_{1}^{*},h_{2}^{*}\). This is indeed the case in Figure 0(b). More generally, assumption (9) holds if there is a subset of classifiers in a matching penny configuration that are "near optimal" on average. More details in Appendix C.3.

The following example depicts the scenario of Remark 2 in the extreme case in which there exist infinitely many distinct optimal classifiers in matching penny configuration, leading to the maximum possible advantage of using probabilistic classifiers.

**Example 1** (Maximum expected matching penny gap of 1).: Consider a discrete data distribution \(=(x=0,y=1)\) and a BHS composed of infinitely many linear classifiers, \(_{b}=\{h:h(x)=\{w^{T}x<1\},\|w\|_{2}=\}\) where \(\|\|_{2}\) is the Euclidean norm. Observe that every classifier in \(_{b}\) is vulnerable at \((x,y)=(0,1)\) and so \(_{}(h)=1\) for all \(h_{b}\). However, no pair is simultaneously vulnerable. Hence, \(_{_{}}(x,y)=1\) for any distribution \((_{b})\) that is continuous over the entire support \(_{b}\). By Theorem 3.2 we get that \(_{}(_{})=0\) for any such \(\). Therefore, any such \(_{}\) outperforms every \(h_{b}\), and we have \(0=_{(_{b})}_{}(_ {})<_{h_{b}}_{}(h)=1\). More details on Appendix C.1.

**Remark 3** (Probabilistic classifiers with zero matching penny gap are not useful).: Suppose the expected matching penny gap \(_{(x,y)}[_{_{^{}}}(x,y)]\) is zero for some \(^{}(_{b})\). Then the left hand side of (9) is zero, whereas the right-hand side \(_{h^{}}[_{}(h)]-_{h_{b}}_{}(h)\) is always non-negative. Hence, (9) does not hold and so \(_{}(_{^{}})_{h_{b} }_{}(h)\). Therefore, any such probabilistic classifier underperforms the best deterministic classifier in the base set \(_{b}\).

The following example illustrates a scenario described in Remark 3, where we examine classifiers with decision boundaries that are parallel sets .

**Example 2** (Minimum expected matching penny gap of \(0\) / Parallel decision boundaries).: Fix any binary classifier \(h:\{0,1\}\) with non-empty decision region \(A_{h}=\{x:h(x)=1\}^{d}\). Let \(_{b}\) be composed of all classifiers whose decision regions are \(r\)-parallel sets of \(A_{h}\) defined as \(A_{h}^{r}=A_{h} B_{r}(0)\) where \(\) denotes the Minkowski sum, i.e., \(_{b}=\{h: r 0\;s.t.\;A_{h}^{r}=\{x:h(x)=1\}\}\). Because of the parallel decision boundaries, whenever two classifiers in \(_{b}\) are vulnerable, they are simultaneously vulnerable, and never exhibit a matching penny configuration. Therefore, \(_{(x,y)}[_{_{}}(x,y)]=0\) for any \((_{b})\). More details on Appendix C.2.

Application to RECs and Links with .In the case of RECs where \(=_{m[M]}p_{m}_{h_{m}}\) i.e., \(_{}\) is a mixture of \(M\) deterministic classifiers in \(_{b}=\{h_{m}\}_{m[M]}\), we can instantiate Theorem 3.2 as follows. As the family \(_{svb}(x,y)\) is finite, the supremum \(^{}(x,y)\) is always attained by some subset of simultaneously vulnerable classifiers \(_{svb}^{max}(x,y)\). We can then write:

\[_{}(_{})=_{m[M]}p_{m}_{ }(h_{m})-p_{m}[}_{(x,y)}\{h _{m}_{vb}(x,y)_{svb}^{max}(x,y)\}] \]

Alternatively, we can use (8) to write \(_{}(_{})=_{m[M]}p_{m} }_{(x,y)}\{h_{m}_{svb}^{max}(x,y)\}\). At each \((x,y)\), testing whether \(h_{m}_{svb}^{max}(x,y)\) reduces to solving a combinatorial optimization problem, as noted in . Any \((x,y)\) falls into one of finitely many configurations, depending on which subset of classifiers are vulnerable or simultaneously vulnerable at \((x,y)\). Dbouk and Shanbhag  use this to derive upper and lower bounds on \(_{}(_{})\). Specifically, [13, Proposition 1] is equivalent to (8) for the special case of RECs. Also, [13, Theorem 1] can be proved as an application of Theorem 3.2 to RECs with \(M=2\). To establish the link, one must note that \(_{(x,y)}[_{_{^{}}}(x,y)]= (\{(x,y) R\})\) where \(R\) indicates the set of all points where \(h_{1},h_{2}\) are in matching penny configuration.

## 4 From Probabilistic to Deterministic Classifiers

In this section, we prove that for any probabilistic binary classifier \(\), there exists a deterministic classifier \(h\) in a "threshold" hypothesis set \(_{T}()\) with at least the same adversarial risk. InSection 4.1 we present the main theorem and in Section 4.2 we apply the theorem to various classes of probabilistic classifiers.

### Reducing a Probabilistic Classifier to a Deterministic One Through Threshold Classifiers

In the case of binary classification, i.e. \(=\{0,1\}\) any distribution \(()\) is uniquely determined by a scalar \(=(y=1)\). Hence, any probabilistic binary classifier is identified with a function \(:\), where \((x)=(y=1|x)\). Accordingly, \(^{01}((x,0),)=(x)\) and \(^{01}((x,1),)=1-(x)\).

Given a probabilistic binary classifier \(:\) and a threshold \(\), the _\(\)-threshold classifier \(h^{}:\{0,1\}\)_ is defined as \(h^{}(x)=\{(x)>\}\), and the _threshold hypothesis set (THS)_ of \(\) is given by \(_{T}()=\{h^{}\}_{}\). In Theorem 4.1 we show that there exists \(h^{_{*}}_{T}()\) such that \(_{}(h^{_{*}})_{}()\). The following lemma plays a crucial role in proving Theorem 4.1.

**Lemma 4.1**.: Let \(:\) be any measurable function. For any \(\{>,\}\), the following inequality holds, and it becomes an equality if \(\) is continuous or takes finitely many values:

\[(_{x^{} B_{}(x)}(x^{ }))}_{x^{} B_{}(x)} \{(x^{})\} \]

Note that Lemma 4.1 is a generalization of the layer-cake representation of \((x)\) given by,

\[(x)=_{0}^{1}\{(x)>\}d=_{0}^{ 1}\{(x)\}d.\]

**Theorem 4.1**.: Let \(:\) be any probabilistic binary classifier. Let \(h^{}\) be the \(\)-_threshold_ classifier. Then the following equation holds:

\[_{}()_{0}^{1}_{}(h^{ })d_{}_{}(h^{}). \]

Further, if \(\) is either continuous or takes finitely many values, the first inequality in (12) becomes an equality.

Note that \(\) takes finitely many values in the case of RECs, and \(\) is continuous in the case of INCs and WNCs whenever the noise distribution admits a density. Hence, \(R_{}()=_{0}^{1}_{}(h^{})d\) in all these cases.

**Remark 4**.: Theorem 4.1 says that in the binary case (K = 2), if one is able to consider complex enough hypotheses sets that contain the \(_{T}()\), then randomization is not necessary because there is a deterministic classifier with equal or better adversarial risk.

It was very recently shown with a toy example [45, Section 5.2] that Theorem 4.1 does not hold in the multi-class case \(K>2\). This example shows that even when the family of probabilistic classifiers considered is very general (all Borel measurable functions), simple data distributions can create a situation in which the optimal classifier is probabilistic, and there is no optimal deterministic classifier. In other words, there is a strict gap in adversarial risk between the best deterministic classifier and the best probabilistic one.

### Applications: Connections to Weighted Ensembles and Randomized Smoothing

In this section, we apply Theorem 4.1 to probabilistic classifiers presented in Section 2.3.

Recs.When \(=_{m[M]}p_{m}_{h_{m}}\) and \(=\{0,1\}\), the REC \(_{}\) can be written as \(_{}(x)=_{m=1}^{M}p_{m}h_{m}(x)\). Let us introduce the constant classifier \(h_{0}(x)=1\) for all \(x\), and \(p_{0}=-\). Then \(h^{}(x)=\{_{m=1}^{K}p_{m}h_{m}(x)>\}= \{_{m=0}^{M}p_{m}h_{m}(x)>0\}\). This shows that \(h^{}\) is a _weighted ensemble_, such as those that the boosting algorithm can learn .

Further, a REC \(\) built with \(M\) base binary classifiers can take at most \(p 2^{M}\) distinct values, corresponding to all possible partial sums of the weights \(p_{m}\). Let \(0~{}=~{}r_{1}~{}~{}~{}~{}r_{p}~{}=~{}1\) be the possible values. Then, for any \([r_{i},r_{i+1})\), \(h^{}=h^{r_{i}}\). Applying Theorem 4.1 yields,

\[_{}()=_{0}^{1}_{}(h^{ })d=_{i=1}^{p-1}_{r_{i}}^{r_{i+1}}_{}(h^{r_{i }})d=_{i=1}^{p-1}(r_{i+1}-r_{i})_{}(h^{r_{i}}) \]

Equation (13) shows that the THS \(_{T}()=\{h^{}\}_{}\) is composed of at most \(p 2^{M}\) distinct classifiers, each of which is in turn a weighted ensemble. In case of uniform weights i.e. \(p_{m}=\), there are only \(M\) distinct weighted ensembles in \(_{T}()\).

INCs.Let \(h_{}:\ \ \ \{0,1\}\) be the deterministic binary classifiers created from a base classifier \(h\), as defined in Section 2.3. The probabilistic classifier \(_{}\) is defined as \(_{}(x)=_{}(h(x\ +\ )\ =\ 1)\). Thus, the \(\)-threshold classifier takes a similar form to the well-known classifier obtained by _randomized smoothing_.

\[h^{}(x)=\{}(h(x+)=1 )>\}. \]

Randomized smoothing was first introduced in  and refined in [8; 27; 42] as a way to create classifiers that are certifiably robust. Given a deterministic classifier \(h:\) and a noise distribution \(()\), the smoothed model will output a prediction according to the following rule:

\[h_{RS()}(x)=*{argmax}_{y}_{ }(h(x+)=y)=\{_{}(h(x+ )=1)>\} \]

In other words, Equation (14) generalizes the randomized smoothing classifier in the binary case by replacing the \(\) threshold by \(\). Theorem 4.1 then states that _for any INC, there exists a deterministic randomized smoothing classifier with threshold \(\) that is at least as robust_.

## 5 Families of Binary Classifiers for Which Randomization Does not Help

In Sections 3 and 4, we discussed two types of hypothesis sets for binary classification: 1) base set \(_{b}\) from which a probabilistic classifier is built using some \((_{b})\), and 2) threshold set \(_{T}()\) which is built from a base probabilistic classifier \(\). Recapitulating this two-step process, one may define the _completion_ of a base set \(_{b}\) of binary classifiers w.r.t. a set of probability distributions \((_{b})\) as, \(_{b}}()=_{}_{ T}(_{})\). Observe that \(_{b}_{b}}((_{b}))\). In the following theorem, we show that if \(_{b}=_{b}}()\), then probabilistic binary classifiers built from \(_{b}\) using any \(\) do not offer robustness gains compared to the deterministic classifiers in \(_{b}\).

**Theorem 5.1**.: If \(_{b}=_{b}}()\) and \(_{h}\) for all \(h_{b}\), then \(_{h_{b}}_{}(h)=_{} _{}(_{})\).

Proof.: \[_{h_{b}}()}_{}(h)= _{}_{h_{T}(_{})}_{}(h)_{}_{}(_{ })_{h_{b}}_{}(h),\] (16)

where the first inequality follows from Theorem 4.1 and the second by considering \(=_{h}\) for any \(h_{b}\). The desired conclusion follows by observing that the first and last terms in (16) are identical from the assumption \(_{b}=_{b}}()\). 

A trivial example where the assumptions of Theorem 5.1 hold is when \(_{b}\) is the set of all measurable functions \(h:\{0,1\}\). Such a \(_{b}\) is commonly used in the study of optimal adversarial risk [2; 40]. In the following corollary, we show that the assumptions of Theorem 5.1 also hold in the case of RECs built using \(_{b}\) that satisfy a "closure" property.

**Corollary 5.1**.: Let \(_{b}\) be any family of deterministic binary classifiers. Let \(=_{M}(_{b})(_{b})\) be the subset of probability measures over \(_{b}\) defining RECs as in Section 2.3. Let \(=\{h^{-1}(1):h_{b}\}\). If \(\) is closed under union and intersection, then

\[_{h_{b}}_{}(h)=_{_{M }(_{b})}_{}(_{}).\]

**Remark 5** (Implications on the optimal adversarial classifier).: Pydi and Jog [40, Theorem 8] show the existence of a binary deterministic classifier attaining the minimum adversarial risk over the space of all Lebesgue measurable classifiers in \(^{d}\). Awasthi et al. [2, Theorem 1] prove a similar result over the space of universally measurable classifiers in \(^{d}\). For both these families the assumptions of Theorem 5.1 hold for \(=(_{b})\), and so probabilistic classifiers offer no additional advantage over deterministic ones. Further, suppose \(^{*}\) is a finite subset of classifiers achieving optimal adversarial risk. Then by Theorem 3.2, for any \((^{*})\), the expected matching penny gap for \(_{}\) must be zero, otherwise we could strictly improve the adversarial risk by considering the mixture built using \(\). In other words, any pair of optimally robust binary deterministic classifiers in the settings of  can only be in a matching penny configuration on a null subset \(E\).

## 6 Conclusion and Future Work

In this paper, we have studied the robustness improvements brought by randomization. First, we studied the situation in which one expands a base family of classifiers by considering probability distributions over it. We showed that under some conditions on the data distribution and the configuration of the base classifiers, such a probabilistic expansion could offer gains in adversarial robustness (See Corollary 3.1), characterized by the _matching penny gap_. These results generalize previous work that focused on RECs . Next, we showed that for any binary probabilistic classifier, there is always another deterministic extension with classifiers of comparable or better robustness. This result is linked with the existence results in . As a direct consequence of this result, we show that in the binary setting, deterministic weighted ensembles are at least as robust as randomized ensembles and randomized smoothing is at least as robust as noise injection.

There are many avenues for future research.

**Improving the Matching Penny Gap.** An interesting direction would be finding tighter bounds on the matching penny gap for particular and widely used probabilistic classifiers like RECs, INCs and WNCs. It would be interesting to establish the conditions under which each method can offer robustness gains, and to quantify those gains in terms of parameters such as the strength of the noise injected or the number of classifiers in the REC.

**Studying the Threshold Hypothesis Sets.** We have seen in Section 4.2 that different probabilistic classifiers induce different THS. In particular, we showed the THS corresponding to RECs and INCs. It would be useful to formally describe the THS induced by other popular families of probabilistic classifiers in the literature. It would also be useful to quantify the complexity gap between the initial \(_{b}\) and the THS to understand the risk-complexity trade-off we would have to incur.

**Multiclass Setting.** The toy example in [45, Section 5.2] shows that randomization might be necessary in the multi-class setting, as it is no longer true that there is always a deterministic optimal classifier, even when considering very general families of deterministic classifiers like all measurable functions. A natural road for future work is to further characterize the situations in which probabilistic classifiers strictly outperform deterministic ones in the multi-class setting.

**Training algorithms for diverse ensembles and RECs.** There are numerous works based on the idea of training diverse classifiers that are not simultaneously vulnerable to create robust ensembles . Most of these approaches try to induce orthogonality in decision boundaries so that gradient-based attacks do not transfer between models. This intuition is validated by Corollary 3.1, since such diversification would tend to increase the matching penny gap. It should be noted that ensembles and RECs have different inference procedures, and attacking them represents different optimization problems. One avenue of research would be to make the link between the matching penny gap and the diversity more formal. In addition, designing training algorithms explicitly optimizing the matching penny gap would be valuable, particularly because existing training algorithms for RECs  have been shown to be inadequate .

## 7 Acknowledgements

This work was funded by the French National Research Agency (DELCO ANR-19-CE23-0016).