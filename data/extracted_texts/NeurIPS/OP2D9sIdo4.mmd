# Suitable is the Best: Task-Oriented Knowledge Fusion in Vulnerability Detection

Jingjing Wang

Institute of Systems Engineering,

Academy of Military Sciences, PLA

jennywangel0163.com

&Minhuan Huang

Institute of Systems Engineering,

Academy of Military Sciences, PLA

darbouxan@126.com

&Yuanpin Nie

Institute of Systems Engineering,

Academy of Military Sciences, PLA

yuanpingnie@nudt.edu.cn

&Xiang Li

Institute of Systems Engineering,

Academy of Military Sciences, PLA

ideal_work@163.com

&Qianjin Du

Department of Computer Science

and Technology, Tsinghua University

dqj20@mails.tsinghua.edu.cn

&Wei Kong

School of Information Science and

Engineering, Zhejiang Sci-Tech University

kong_wei@ieee.org

&Huan Deng

Institute of Systems Engineering,

Academy of Military Sciences, PLA

denghuan6190163.com

&Xiaohui Kuang

Institute of Systems Engineering,

Academy of Military Sciences, PLA

xiaohui_kuang@163.com

Corresponding author

###### Abstract

Deep learning technologies have demonstrated remarkable performance in vulnerability detection. Existing works primarily adopt a uniform and consistent feature learning pattern across the entire target set. While designed for general-purpose detection tasks, they lack sensitivity towards target code comprising multiple functional modules or diverse vulnerability subtypes. In this paper, we present a knowledge fusion-based vulnerability detection method (KF-GVD) that integrates specific vulnerability knowledge into the Graph Neural Network feature learning process. KF-GVD achieves accurate vulnerability detection across different functional modules of the Linux kernel and vulnerability subtypes without compromising general task performance. Extensive experiments demonstrate that KF-GVD outperforms SOTAs on function-level and statement-level vulnerability detection across various target tasks, with an average increase of 40.9% in precision and 26.1% in recall. Notably, KF-GVD discovered 9 undisclosed vulnerabilities when employing on C/C++ open-source projects without ground truth.

## 1 Introduction

According to statistics, 26,447 vulnerabilities were disclosed in 2023, continuing the alarming trend of continuous growth in vulnerability numbers over the past seven years1. Static source codevulnerability detection (VD), as an integral part of the software development lifecycle, plays a crucial role in enhancing system security and building reliable, high-quality software systems. Early static analysis tools required experts to define scanning rules for VD. Machine learning (ML)-based VD methods, on the other hand, required manual predefinition of code features. Currently, deep learning (DL)-based VD methods can achieve automated VD without the need for manual feature definition and have proven effective in identifying potential vulnerability patterns. These studies can be categorized by the target of detection: (1) _Targeting open-source code projects_. Most of the studies like IVDetect  and Reveal assess the performance of VD methods by evaluating their effectiveness across entire open-source projects, such as the Linux kernel, QEMU, and more. (2) _Focusing on specific vulnerability types_. Some studies primarily concentrate on specific commonly occurring vulnerability types in real-world applications, such as buffer overflow, use before initialization, use after free , and so on.

With the prevalent of software applications, the scale and complexity of source code projects increase. On the one hand, the functionalities of various components or modules within a project vary, leading to differences in the vulnerability triggering conditions and types. On the other hand, different triggering mechanisms result in a rich variety of subtypes within the same vulnerability type. Consequently, potential vulnerability patterns that may exist in target objects often closely correlate with specific source code contexts and characteristics of particular vulnerability types. However, the training objective of the current mainstream DL-based VD methods is to learn more comprehensive and effective vulnerability information, thereby obtaining an optimized model with the best generalization performance across the entire target source code dataset. In practical application, when a generlized model is applied to specific scope of source code, the detection of more detailed and distinguishable potential vulnerability patterns becomes difficult, leading to a compromise in the effectiveness of the model. Additionally, the vulnerability uncertainty and complexity of the target detection object, as well as the limitations of available vulnerability data, pose challenges for transfer learning technology in achieving effective knowledge transfer between the source and target domains. As a result, the generalization capability of the transfer model in the target domain will be severely limited. In security practice, scalable models capable of achieving robust VD performance across both the source and target domains are anticipated. To this end, it is essential to design specific pattern learning methods for different target tasks.

In this paper, we propose KF-GVD, a Knowledge Fusion-based GNN model for source code Vulnerability Detection. The object of KF-GVD is to implement the most suitable vulnerability pattern learning and identification strategy for task-oriented VD in a flexible manner, while maintaining a certain degree of generalization of the original model. By incorporating vulnerability knowledge into the feature learning process of the target, KF-GVD promotes the model to learn the vulnerability features that are closely related to the current task more efficiently. Through this way, the biases of the current general-purpose VD methods in target-oriented vulnerability pattern learning can be corrected to some extent. KF-GVD employs a graph self-attention mechanism to capture high-weight nodes that influence model decisions during message propagation. By associating with code statements, it obtains fine-grained vulnerability localization, achieving high transparency and interpretability of the model, thereby assisting developers in understanding the rationality of model decision-making. In summary, our contributions in this paper are:

* We propose a task-oriented knowledge fusion method, which integrates specific vulnerability knowledge in the feature learning process of GNN, it enables the general-purpose VD model to learn vulnerability features related to the current target more effectively and achieve VD tailored to specific tasks.

* We propose the framework KF-GVD, which can be flexibly adapt to target task detection while maintaining the generalization performance for source task. The interpretable KF-GVD achieves effectively vulnerability identification at both the source code function level and statement level.
* Extensive experiments demonstrate the superiority of KF-GVD in VD performance compared to SOTAs when targeting multiple functional modules or diverse vulnerability subtypes. Note that KF-GVD discovered and submitted 9 undisclosed vulnerabilities in the open source C/C++ project without ground truth, proving its effectiveness in real-world applications.

## 2 Motivation

Figure 1 shows the distribution of the number of security commits related to CWE-416 (Use After Free) and CWE-119 (Buffer Overflow) in the Linux kernel over the past decade, collected from NVD. It can be observed that the occurrence proneness of different types of vulnerabilities exhibits a significant module tendency, especially CWE-119. Of the 11 modules in the Linux kernel, more than 80% of the vulnerabilities occur in the first four modules with the highest number of vulnerabilities. Additionally, distinct triggering features for different vulnerability types on the same module are evident. As shown in Figure 2, presenting two typical patch examples in the network module. It can be observed from the code snippet on the left, a race condition caused by pointer-related resource leakage release is a major triggering form of CWE-416 vulnerabilities in the net module. In contrast, CWE-119 in the same module are usually related to restrictive checks lacking critical network protocol fields.

The above examples are presented to demonstrate that in practical applications, the potential vulnerability patterns associated with program behavior differ and have distinct characteristics depending on the context of different detection targets and tasks. In such cases, the two existing types of DL-based VD methods, which employ uniform model optimization and analysis techniques for any target, are difficult to make full use of known information in diverse practical task scenarios to characterize the potential vulnerability characteristics of different target codes. The greatly compromised detection performance will also further burden the code audit in the software development phase. To alleviate the above problems, KF-GVD employs a simple and efficient knowledge guidance form of artificial "prompt", which enhances the rationality of model decision-making by combining the historical information of target tasks and human prior knowledge, corrects the deviation of pattern learning to a certain extent, and makes it more flexible to adapt to downstream tasks. As the first task-oriented GNN-based VD method, KF-GVD strives to achieve more accurate and scalable VD with lower costs.

## 3 The KF-GVD Framework

The overall architecture of KF-GVD is illustrated in Figure 3. KF-GVD consists of two main phases: feature representation and vulnerability detection and interpretation.

### Feature Representation

In the feature representation phase, the source code is initially transformed into Code Property Graphs (CPGs) as the intermediate representation. Subsequently, task-specific vulnerability knowledge

Figure 3: The overall architecture of KF-GVD.

subgraphs are extracted from the CPGs. Finally, node feature vectors are encoded according to the node information of the graph, and combined with the adjacency matrix to form a graph embedding as the input of the model.

#### 3.1.1 Code Property Graph Generation

The concept of CPG proposed by Yamaguchi, et al. (2014)  is a joint data structure that combines abstract syntax tree, control flow graph, and program dependency graph. CPG has been shown to model more diverse types of vulnerabilities than conducting a combination of single or two code properties as an intermediate representation of the source code for static analysis . The key insight behind KF-GVD's adoption of CPG as the intermediate representation is to comprehensively preserve source code information in this stage. This not only enhances the performance of the model in detecting vulnerabilities across the entire project, but also lays the foundation for the model to characterize more detailed vulnerability features in subsequent target tasks.

#### 3.1.2 Task-oriented Vulnerability Knowledge Extraction

KF-GVD extracts the knowledge subgraph by initially screening the codes with the most feature relevance to the current task target, thus affecting the optimization from the source task generalization model to the specialized target expert model. The task-oriented vulnerability knowledge considered by KF-GVD includes: _vulnerable program operations_ and _sensitive functions_ related to the vulnerability type, as well as _customized knowledge for specific tasks_ associated with the target functional scenarios. As for implementation, we first perform a coarse screening of source code statements for specific task objectives based on relevant vulnerability knowledge. Then, we map the identified statements to node sets in the CPG and mask out the remaining nodes weakly related to the task. The left part of the feature representation example in figure 4 shows the extraction process of the task-oriented vulnerability knowledge subgraph. For CWE-119 VD in the file module of the Linux kernel, because the code statement (highlighted in orange) in the code snippet calls the _strlcpy()_ function, which is considered to be a sensitive function that may introduce CWE-119 type vulnerabilities under the current task, the associated node (highlighted in orange) and edges of this statement in CPG is extracted as the knowledge subgraph.

Appendix A provides a more detailed description of the task-oriented vulnerability knowledge considered by KF-GVD.

#### 3.1.3 Graph Embedding

The CPG node information generated by Joern2 comprises two parts: code element types and code statements. Figure 4 shows the corresponding information for nodes \(v_{k1}\) and \(v_{k2}\) in the CPG. KF-GVD adopts two vectors, \(V_{op}\) and \(V_{func}\), to describe the code element types. \(V_{op}\) reflects the operation type of the code associated with the node, such as field access, memory allocation, mathematical operations, and so on. \(V_{func}\) indicates special function calls, code field types, and so on. Both \(V_{op}\) and \(V_{func}\) use one-hot encoding, and when a node contains only one type of information, the remaining vector is zero-encoded. Next, we employ a pretrained Word2Vec model to map tokensgenerated from code statements to fixed-length feature vectors to generate \(V_{semantic}\) that represent the semantic information of the source code corresponding to the node. Finally, the three vectors are concatenated to form the final feature vector of the node. The spatial features of CPG represented by the adjacency matrix and the node feature matrix constitute the graph embedding, serving as the input for the subsequent model stage.

### Vulnerability Detection and Interpretation

In this section, we provide a detailed description of the GNN model adopted in KF-GVD and explain how vulnerability knowledge is leveraged to optimize the model for specific subtasks during VD. Finally, we describe how KF-GVD utilizes attention mechanisms to interpret and locate potential vulnerability source code.

#### 3.2.1 Object

For the source code function \(C\) corresponding to the \(CPG\)\(G=(V,E),G\), we define \(y\) as the function-level label of \(G\) corresponding to \(C\), where \(Y=\{0,1\}\), \(y=1\) indicates that the sample is a vulnerability sample, \(y=0\) otherwise. We define source task O corresponding to dataset \(D_{O}=\{(G_{i},y_{i})|G_{i}_{O},y_{i} Y_{O}\}_{i=1}^{N}\). Target task \(\) corresponds to dataset \(D_{T}=\{(G_{i},y_{i})|G_{i}_{T},y_{i} Y_{T}\}_{i=1}^{M}\), where \(N\) and \(M\) are the total number of samples contained in \(D_{O}\) and \(D_{T}\), and \(N>M\). Assuming that a mapping \(_{D_{O}}:_{O}_{O}\) has been learned on the source task dataset \(D_{O}\), KF-GVD aims to establish a mapping \(f:_{t} Y_{t}\) on the target task \(T\) with the assistance of knowledge \(K_{t}\) relevant to task \(t\). Here, \(d_{t}\) represents the dataset associated with any subtask \(t\) under the target task \(T\). Therefore, the workflow of KF-GVD is mainly divided into two parts: model training on the source task and knowledge fusion on the target task. The workflow of KF-GVD is shown in Figure 5.

#### 3.2.2 Task-oriented Model with Knowledge Fusion

The second phase in Figure 3 shows the general hierarchical structure of the GNN employed by KF-GVD. The model utilizes two layers of Graph Convolution Network (GCN) to calculate the new representation of each node by weighted summation of neighbor node information. Subsequently, based on the self-attention mechanism, the model obtains weights between nodes, calculating and retaining high-weighted nodes during message propagation . The graph pooling layer is then employed to obtain the embedding of the entire graph. Finally, the model obtains function-level vulnerability identification results for the source code based on the classification layer. Furthermore, the training process of the model in the source task stage is similar to the current DL-based VD method. On this foundation, when conducting VD tasks for specific targets, a task-oriented knowledge fusion layer is introduced.

The current state of the graph \(G=(V,E)\), obtained from the GCNs layer, is denoted as \(H_{V}=\{h_{v_{j}}|v_{j} V\},j\{1,..,n\}\), and the corresponding knowledge subgraph \(G_{k}=(V_{k},E_{k})\) on the target task \(t\) is denoted as \(H_{V_{k}}=\{h_{v_{j}}|v_{q} V_{k}\},q\{1,..,m\}\). The calculation of the state \(H_{u}=\{h_{u_{j}}|v_{j} V\},j\{1,...,n\}\) for \(G=(V,E)\) in the fusion layer can be defined as follows:

\[h_{u_{j}}=\{Fusion( h_{v_{j}}, h_{v_{q}}),v_{j} V_{k}\\ h_{v_{j}},v_{j} V_{k}.\] (1)

where \(\) and \(\) are fusion coefficients, reflecting the degree to which knowledge tailored for a specific task \(t\) influences the pattern learning of the model. Assuming that in the source task \(\), the model has been trained on dataset \(_{}\), denoted as \(_{D_{O}}\). The training of model \(f\) for a subtask \(t,t\) includes the following steps:

(1) _Dataset collection_: Define task \(t\) corresponding to dataset \(d_{t}_{}\). To ensure that the fused model \(f\) retains the ability to detect vulnerabilities in \(\) to some extent, the training set \(d_{}\) for \(f\) is derived from \(d_{t}\) and random sampling of \(_{}\), defined as \(d_{o}_{}\), \(d_{}=d_{o}+d_{t}\).

(2) _Initialization_: Initialize the parameters of \(f\) using \(_{D_{O}}\) to achieve knowledge sharing between the source task and the target task.

(3) _Fusion_: During the training phase, we perform the feature fusion operation (Equation 1) only on the data \(d_{t}^{}\) randomly sampled from \(d_{t}\). The fusion function employs a weighted sum of node features.

For implementation, we first evaluate the detection performance of \(_{D_{O}}\) on \(d_{t}\) and consider the misclassified samples in the first round by \(_{D_{O}}\) as the preferred subset of \(d_{t}^{}\) for the fusion operation in the training process of \(f\). We treat this subset of samples as the data subset with understanding bias of \(_{D_{O}}\) in the target task. Overall, the knowledge fusion process employed by KF-GVD is also a process of manual correction and adjustment carried out during the model training phase.

A more detailed computational description of each layer of the model is provided in Appendix B.

#### 3.2.3 Statement-level Interpretation and Location

KF-GVD achieves interpretability through the self-attention mechanism. In the self-attention layer, the top \( n\) nodes with the highest attention scores in \(G\) will be retained as the input for the pooling layer, and the states of the remaining nodes will be masked during subsequent message propagation, where \(\) is the retain rate. The source code statements corresponding to the crucial nodes which have high influence on \(f\) to make vulnerability decisions are regarded as the interpretation of the current sample, that is, the fine-grained vulnerability statement location. In the implementation, we realize the mapping from graph nodes to source code statements through the data files generated by joern.

To improve the performance of KF-GVD in function-level and statement-level VD, we define a graph \(G=(V,E)\) with a labeled node set \(=\{(v_{j},l_{j})|v_{j} V,l_{j} L\}_{j=1}^{n}\), where \(l\) represents the node label. The prediction function \(f\) for the target task \(t\) can be learned by minimizing the following loss function:

\[_{i=1}^{|d_{}|}_{CE}(f(g_{i}),y_{i})+_{j =1}^{n}_{CE}(f(g_{i}),l_{j}|y_{i}))]\] (2)

Here, \(|d_{}|\) represents the size of the training dataset under task \(t\), \(\) is an adjustable hyperparameter.

## 4 Evaluation

We evaluate the superiority of KF-GVD compared to baseline approaches on two source tasks, \(S_{119}\) and \(S_{416}\), as well as their corresponding target tasks, \(T_{m}\) and \(T_{sub}\), in order to answer the following research questions:

_RQ1_: _How does KF-GVD perform compared to other function-level VD methods?_

_RQ2_: _How does the interpretation of KF-GVD compare to other fine-grained VD methods in terms of locating vulnerable statements?_

_RQ3_: _How does KF-GVD perform in detecting and locating vulnerabilities in real-world software products where the presence of vulnerabilities is unknown?_

### Experiment Settings

#### 4.1.1 Dataset

The evaluation dataset consists of two parts: the source task dataset and the target task dataset. Following the current practice of DL-based VD methods, we initially train the model on a widely collected source task dataset to obtain a model with good generalization performance for a specific vulnerability type. Subsequently, the model is applied to the target task dataset for VD.

_Source Task Dataset_: We define two source tasks targeting the detection of CWE-119 and CWE-416 type vulnerabilities, denoted as \(S_{119}\) and \(S_{416}\) respectively. The source task dataset consists of 80% CWE-119 and CWE-416 type vulnerability information extensively collected from 13 real-world C++ projects from NVD3. The remaining 20% is sourced from academic security defects and synthetic data provided by SARD4.

_Target Task Dataset_: Referring to the types of research objects in the current DL-based VD studies (Section 1), our target tasks are divided into two categories: (1) \(T_{m}\): Detection of CWE-119 and CWE-416 type vulnerabilities within various modules in the Linux kernel, denoted as \(T_{m_{119}}\) and \(T_{m_{416}}\) respectively. (2) \(T_{sub}\): Detection of CWE-119 subtypes, specifically CWE-125 and CWE-787.

Both of these vulnerability subtypes are also included in the 2023 CWE Top 25 Most Dangerous Software Weakness5.

#### 4.1.2 Baseline Approaches and Evaluation Metrics

We compare KF-GVD at the function level with four types of VD methods, including: (1)_Rule-based commercial static code analysis tools_: Cppcheck6 and Flawfinder7. (2) _Classical DL-based VD methods_: Sysver  and VulCNN . (3) _Large-scale code models_: Codebert , Code Llama , and Wizardcoder . (4) _GNN-based VD methods_: Devign , ReGVD  and IVDetect . For statement-level localization comparison, we chose the state-of-the-art fine-grained VD methods IVDetect , LineVul  and LineVD  as baselines.

We evaluate the performance of KF-GVD and the baseline methods using Precision (P), Recall (R), and F1-score (F) for both function-level and statement-level comparison. Besides, we also introduce the ranking metric Mean Average Precision (MAP) in order to explore the performance of the method on code statements that it considers as the best interpretation for function prediction. Appendix C provides more specific information on the datasets and experimental settings adopted by KF-GVD.

### Function-level Vulnerability Detection Performance

For ML-based VD methods in addition to large-scale code models, we train and test all models on the source tasks \(S_{119}\) and \(S_{416}\) separately, and then apply them to the corresponding target tasks. Additionally, following the concept of transfer learning, we employed the same GNN model as KF-GVD in the source tasks. For VD in the target tasks, we fine-tuned the GNN model used in KF-GVD, denoted as GVD-ft, instead of employing knowledge fusion operations, so the results of VD on the source task are the same as KF-GVD.

   & \)} & \)} & \)}} & } \\   & &  &  &  &  &  & } \\  & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\  Cppcheck & 45.0 & 55.7 & 49.8 & 33.7 & 50.5 & 40.4 & 32.1 & 45.9 & 37.8 & 44.2 & 40.0 & 42.0 & 23.9 & 35.7 & 28.6 & 24.8 & 50.6 & 33.3 & 29.4 & 35.7 & 32.2 \\ Flawfinder & 27.6 & 50.4 & 35.7 & 15.3 & 57.4 & 24.2 & 25.9 & 48.4 & 32.8 & 37.6 & 42.8 & 40.0 & 29.7 & 56.8 & 39.0 & 12.9 & 37.4 & 19.2 & 18.3 & 33.5 & 23.7 \\  Sysver & 54.8 & 70.6 & 61.7 & 23.6 & 67.2 & 34.9 & 28.3 & 36.2 & 37.6 & 15.7 & 60.9 & 25.0 & 33.0 & 42.6 & 37.2 & 39.7 & 58.4 & 47.3 & 33.4 & 48.6 & 39.6 \\ VulCNN & 63.9 & 77.4 & 70.0 & 35.5 & 50.7 & 41.8 & 27.4 & 46.3 & 39.4 & 58.6 & 47.1 & 22.0 & 43.5 & 29.2 & 16.8 & 29.1 & 21.3 & 17.6 & 33.0 & 23.0 \\  Codebert & 65.2 & 67.9 & 66.5 & 54.7 & 37.9 & 54.9 & 53.7 & 40.0 & 38.7 & 48.8 & 44.1 & 34.6 & 31.5 & 43.8 & 57.6 & 43.4 & 43.7 & 48.6 & 46.0 \\ CodeLlama & 70.0 & 64.6 & 66.9 & 55.7 & 54.9 & 55.3 & 45.8 & 45.7 & 57.2 & 48.2 & 50.2 & 49.3 & 55.1 & 57.5 & 36.3 & 44.3 & 45.0 & 55.8 & 55.1 \\ Wizardcoder & 72.4 & 52.4 & 60.8 & 62.5 & 35.5 & 45.3 & 45.8 & 47.2 & 50.8 & 42.0 & 46.0 & 48.6 & 56.6 & 52.3 & 33.5 & 52.5 & 40.9 & 47.5 & 51.9 & 49.6 \\  Devign & 68.5 & 75.2 & 69.3 & 36.6 & 54.2 & 39.1 & 35.4 & 42.8 & 38.7 & 46.8 & 57.2 & 25.6 & 25.8 & 40.3 & 31.5 & 20.1 & 37.9 & 26.3 & 18.4 & 25.0 & 21.2 \\ ReGVD & 74.1 & 71.2 & 72.6 & 60.8 & 34.2 & 43.8 & 40.9 & 47.4 & 31.8 & 52.1 & 59.5 & 45.4 & 14.0 & 58.4 & 41.8 & 39.2 & 38.4 & 38.4 & 44.9 & 57.2 & 50.3 \\ IVDetect & 79.0 & 83.3 & 81.1 & 46.7 & 33.3 & 38.9 & 33.6 & 47.4 & 46.7 & 50.0 & 57.1 & 40.0 & 46.2 & 42.9 & 31.9 & 55.8 & 38.1 & 46.8 & 52.4 & 43.0 \\ GVD-ft & 82.9 & 90.9 & 86.7 & 73.5 & 58.7 & 65.2 & 66.7 & 78.8 & 97.6 & 76.3 & 58.5 & 64.7 & 57.1 & 61.5 & 59.3 & 49.8 & 60.5 & 54.6 & 66.7 & 61.5 & 64.0 \\ 
**KF-GVD** & 82.9 & 90.9 & 86.7 & 96.1 & 95.2 & 95.7 & 90.0 & 94.7 & 92.3 & 91.7 & 75.0 & 82.5 & 91.7 & 84.6 & 88.0 & 59.2 & 80.0 & 67.9 & 80.0 & 84.2 & 82.1 \\  

Table 1: Comparison of function-level VD of \(T_{m_{119}}\) and \(T_{sub}\) on \(S_{119}\). P: Precision(%); R: Recall(%); F1-score(%)

   & \)} &  &  &  &  &  &  \\  & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\  Cppcheck & 27.7 & 42.6 & 33.6 & 14.8 & 22.7 & 17.9 & 27.0 & 53.6 & 35.9 & 30.7 & 45.9 & 36.8 & 10.3 & 45.9 & 16.8 & 30.2 & 36.5 & 33.1 & 23.6 & 31.8 & 27.1 \\ Flawfinder & 33.4 & 45.9 & 38.7 & 20.6 & 36.6 & 26.4 & 15.9 & 42.6 & 23.2 & 5.6 & 22.4 & 9.0 & 28.5 & 62.8 & 39.2 & 17.8 & 26.7 & 21.4 & 25.0 & 39.7 & 30.7 \\  Sysver & 58.4 & 67.2 & 62.5 & 21.9 & 40.5 & 28.4 & 27.3 & 33.1 & 35.2 & 30.7 & 31.6 & 22.7 & 23.6 & 23.1 & 37.9 & 30.2 & 33.6 & 26.3 & 35.7 & 33.4 \\ VulCNN & 66.9 & 72.8 & 69.7 & 33.4 & 5Table 1 and Table 2 show the comparison of function-level VD performance on different source tasks and their corresponding target tasks. It can be obviously observed that despite all baseline methods being designed to detect the same type of vulnerabilities, they exhibit a noticeable decrease in performance metrics across various specific target tasks. In contrast, KF-GVD consistently performs well. Compared to the best results among baseline methods (indicated by underlines in the table), KF-GVD demonstrates an improvement in precision by 0.6%-44%, recall by 5.8%-29.3%, and an average gain of 22.6% on F1-score. Although GVD-ft achieved relatively better results than other baseline methods in most target tasks due to fine-tuning of the model, GVD-ft shows a 23.3% lower F1 average on \(T_{m119}\), a 31.6% lower F1-score average on \(T_{m416}\), and a 15.7% lower F1-score average on \(T_{sub}\) compared to KF-GVD. This further proves the flexibility and effectiveness of our approach in VD tailored to specific tasks.

Overall, the effectiveness of rule-based static analysis tools is inferior to that of ML-based VD methods. This is due to the limitations of finite and fixed manually predefined static scanning rules when dealing with multiple types of vulnerabilities and detection targets. Moreover, methods that adopt models like LSTM and CNN exhibit an average reduction of 20.6% on F1-score compared to approaches based on GNNs. To some extent, this reflects the powerful spatial feature learning capability of GNNs when dealing with structured languages like source code, as opposed to the flattened feature processing approach of classical DL networks. It is worth noting that the emerging large-scale code models have mediocre performance in dealing with tasks related to code vulnerability detection, and the F1-scores on the corresponding target tasks of \(S_{119}\) and \(S_{416}\) are on average 34.3% and 33.4% lower than those of KF-GVD. We infer that this is because current large language models mainly focus on question and answer, completion and other generative tasks rather than classification tasks.

### Statement-level Vulnerability Detection Performance

Table 3 and Table 4 present the comparison of statement-level vulnerability localization results between KG-GVD and other baseline methods on the target tasks corresponding to \(S_{119}\) and \(S_{416}\). In three different target tasks, KF-GVD exhibits a precision improvement of 5.4% - 52.9% and an average recall enhancement of 2.6% - 73.7% in statement-level vulnerability localization compared to suboptimal detection results. This corresponds to an average gain of 59.7% in precision and 30.9% in recall, showcasing KF-GVD's robust coverage and localization capabilities for vulnerable code during fine-grained detection. It is worth noting that GVD-ft, which

   & \)} \\   &  &  &  &  &  &  \\  & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F \\   & 19.6 & 58.1 & 24.5 & 15.4 & **80.8** & 19.2 & 20.2 & 77.9 & 25.5 & 27.7 & **83.8** & 36.9 & 15.4 & 23.6 & 18.6 & 67.9 & 67.7 & 67.5 \\  LineVul & 24.0 & 98.8 & 31.3 & 16.6 & 55.9 & 25.6 & 17.9 & 75.2 & 23.3 & 15.8 & 27.9 & 21.9 & 12.5 & 16.7 & 14.3 & 48.2 & 49.2 & 48.7 \\  LineVul & 20.9 & 45.3 & 28.6 & 15.3 & 44.0 & 22.7 & 28.2 & 28.3 & 26.9 & 24.9 & 41.3 & 31.2 & 14.1 & 48.0 & 21.8 & 31.7 & 36.4 & 33.9 \\  GVD-ft & 22.7 & 58.6 & 25.3 & 16.7 & 71.3 & 21.8 & 25.3 & 69.9 & 28.0 & 16.4 & 66.5 & 22.4 & 10.8 & 55.3 & 15.2 & 52.9 & 52.2 & 52.4 \\   & 56.3 & 96.3 & 63.8 & 55.9 & 80.8 & 66.0 & 76.5 & 81.1 & 68.1 & 80.6 & 75.9 & 75.1 & 27.4 & 97.3 & 36.1 & 73.3 & 73.1 & 72.6 \\  

Table 4: Comparison of statement-level VD of \(T_{m16}\) on \(S_{416}\). P: Precision(%); R: Recall(%); F1: F1-score(%)

   & \)} & \)} \\   &  &  &  &  &  &  \\  & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F \\   & 32.3 & 36.1 & 34.8 & 10.5 & 63.1 & 15.4 & 36.7 & 20.4 & 26.0 & 9.7 & 74.7 & 16.4 & 2.2 & 17.1 & 3.1 & 16.7 & 10.0 & 12.5 \\  & 39.2 & 27.9 & 32.6 & 11.0 & 87.4 & 16.1 & 37.6 & 21.6 & 26.8 & 17.2 & 53.2 & 26.1 & 4.1 & 24.9 & 5.3 & **33.3** & 20.0 & 25.0 \\  & 33.8 & 45.0 & 38.6 & 10.7 & 24.0 & 14.8 & 22.4 & 28.0 & 24.9 & 16.3 & 44.8 & 23.9 & 6.4 & 13.6 & 8.7 & 29.8 & 19.0 & 23.2 \\  & 32.1 & 55.0 & 34.5 & 11.2 & 66.0 & 16.4 & 18.3 & 10.2 & 13.0 & 9.6 & **85.4** & 16.3 & 7.5 & 51.0 & 10.3 & 2.9 & 18.8 & 22.2 \\   & 82.1 & 58.7 & 66.6 & 38.2 & 81.1 & 49.6 & 74.7 & **65.5** & 66.3 & 54.9 & 84.4 & 65.0 & 31.9 & 55.8 & 38.1 & 29.2 & 67.9 & 31.4 \\  

Table 3: Comparison of statement-level VD of \(T_{m19}\) and \(T_{sub}\) on \(S_{119}\). P: Precision(%); R: Recall(%); F1: F1-score(%)

Figure 6: Statement-level VD comparison on MAP@5.

performs suboptimally at the function level, does not exhibit equally good results at the statement level. This indicates that the fine-tuning approach employed by GVD-ft does not accurately capture the underlying vulnerability patterns of the target task. Moreover, MAP reflects the average precision on the top K confident predictions. Following established research practices [1; 18], we set K=5 and compare the statement-level MAP across all methods, as illustrated in Figure 6. Observably, KF-GVD consistently outperforms other methods in MAP on all target tasks, particularly achieving an average improvement of 42.4% on \(T_{m119}\), which further proving the rationality of the function-level predictions from KF-GVD.

### Case Study

We applied KF-GVD to various C++ open-source libraries commonly targeted for vulnerability detection. For projects without ground truth, we initially gathered information on publicly disclosed vulnerabilities for the target objects (obtained from GitHub) and performed detection for specific unknown vulnerabilities, particularly focusing on several high-risk issues. Taking multiple versions of Assimp (Open Asset Import Library), used for importing various 3D model file formats, as an example, we conducted detection on a total of 312 C++ source code files under the Assimp's assestid directory. The detection revealed 19 vulnerabilities, including three reported and published vulnerabilities (CVE-2022-38528, CVE-2022-45748, CVE-2021-45948), 3 pending confirmation security issues, and 2 undisclosed vulnerabilities that we have submitted to CNNVD and been confirmed. Table 5 presents all the undisclosed vulnerabilities detected by KF-GVD in different C++ open-source objects.

## 5 Disscusion

### Threats to Validity

First, in practical applications, we find that KF-GVD has more obvious advantages in target tasks with more historical version iteration information and knowledge related to specific vulnerability types, which indicates that the performance of the method can be continuously improved with the increase of target task-related vulnerability knowledge. However, without any historical vulnerability information about the target software or knowledge about a specific vulnerability type, method performance degrades to the level of vulnerability detection with the generalized model. Second, we only verify the validity of the method on C/C++ code, not other programming languages, and in principle the method can be extended. Third, we only focus on vulnerability identification and fine-grained localization within functions, and cannot detect source code vulnerabilities that have cross-function or cross-file dependencies.

### Limitations and Future Work

During the data processing stage, generating CPG data from source files accounts for more than 75% of the total processing time, highlighting the need for more efficient data generation tools and processing strategies in the future. Additionally, in the feature embedding stage, we truncate the \(V_{code}\) that exceeds the length threshold of the feature vector, which leads to the loss of semantic information to a certain extent. In the next phase, leveraging the powerful representation and generation capabilities

  ID & Project & File Location & Vul\_line \\  CNNVD-2023-43767151 &  & /.../OpenDDLParser.cpp & 348 \\   CNNVD-2023-12599427 & & /.../FBXParser.cpp & 192 \\   CNNVD-2023-59936877 &  & /.../detail/rapidxml.hpp & 644 \\   CNNVD-2023-23489133 & & /.../basic\_regex\_creator.hpp & 710 \\   CNNVD-2023-20301510 & c-blosc2 & /.../blosc-private.h & 120 \\   CNNVD-2023-76730942 & exiv2 & /.../value.cpp & 13 \\   CNNVD-2023-90736138 & flatbuffers & /.../util.h & 133 \\   CNNVD-2023-83881569 & frr & /.../bgp\_attr.c & 2658 \\   CNNVD-2023-27702356 & harfbuzz & /.../hb-atomic.hh & 172 \\  

Table 5: Undisclosed vulnerabilities detected by KF-GVD in different C++ open-source objects.

of current large-scale code models to obtain more comprehensive and efficient source code features will be a future research direction to enhance vulnerability localization performance. Finally, while KF-GVD has significantly improved statement-level vulnerability localization compared to SOTAs, there remains substantial room for improvement in fine-grained localization precision relative to function-level detection results.

More ablation studies on vulnerability knowledge sensitivity, feature representation, and experiments on cross-domain tasks are provided in Appendix D.

## 6 Related Works

Current DL-based VD methods achieve automated VD, alleviating the manual burden associated with rule-based  and ML-based approaches [20; 21; 22; 23; 24]. VulDeePecker , Sysevr , \(\)VulDeePecker , VulCNN  combined with classical DL models such as LSTM, BGRU, CNN, are employed to perform VD on various open-source projects or specific vulnerability types. The detection granularity of these methods is at the slice or function level. Additionaly, Devign , BGNN4VD , Reveal  and many other studies based on GNNs perform function-level VD on entire projects such as QEMU, FFmpeg, Linux kernel, or mixed datasets.

In recent years, some studies have achieved fine-grained vulnerability localization. VulDeelocator  utilizes intermediate code to define program segments for VD, accommodating semantic information that cannot be conveyed by source code-based representations. By employing the idea of granularity refinement, VulDeelocator outputs a finer granularity than its input, enhancing the precision of the detector. Furthermore, IVDetect  considers vulnerable statements and their surrounding context separately through data dependency and control dependency, enabling the model to better distinguish vulnerable statements. Additionally, IVDetect introduces GNNExplainer  to provide subgraphs in program dependency graph (PDG) as explanations, containing key statements related to the detected vulnerabilities. Besides, LineVD  defines statement-level vulnerability detection as a node classification task, utilizing GNNs to leverage control and data dependency relationships between statements. By resolving conflicts between function-level and statement-level information and learning from both levels, LineVD significantly improves performance.

## 7 Conclusion

In this paper, we propose KF-GVD, a knowledge fusion-based vulnerability detection method. KF-GVD alleviates the limitations of current general-purpose detection methods when applied to the contexts involving multiple functional modules or diverse types of vulnerabilities. By integrating task-oriented vulnerability knowledge, KF-GVD prompts the model to efficiently explore vulnerabilities patterns tailored to specific tasks while still maintaining general task performance. Our empirical evaluations demonstrate the superior performance of KF-GVD tailored for diverse specific tasks in both function-level and statement-level VD. The case study we conducted on real C++ open-source projects further substantiates the practical effectiveness of KF-GVD in real-world applications. Notably, KF-GVD identified 9 undisclosed vulnerabilities when applied to real-world C++ open-source projects, further proving its practicality.