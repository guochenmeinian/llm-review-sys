# Interpretable Image Classification with Adaptive Prototype-based Vision Transformers

Chiyu Ma

Dartmouth College

chiyu.ma.gr@dartmouth.edu

&Jon Donnelly

Duke University

jon.donnelly@duke.edu

&Wenjun Liu

Dartmouth College

wenjun.liu.gr@dartmouth.edu

&Soroush Vosoughi

Dartmouth College

soroush.vosoughi@dartmouth.edu

&Cynthia Rudin

Duke University

cynthia@cs.duke.edu

&Chaofan Chen

University of Maine

chaofan.chen@maine.edu

###### Abstract

We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form "this looks like that." In our model, a prototype consists of _parts_, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful. Our code is available at https://github.com/Henrymachiyu/ProtoViT.

## 1 Introduction

With the expanding applications of machine learning models in critical and high-stakes domains like healthcare , autonomous vehicles , finance  and criminal justice , it has become crucial to develop models that are not only effective but also interpretable by humans. This need for clarity and accountability has led to the emergence of prototype networks. These networks combine the capabilities of deep learning with the clarity of case-based reasoning, providing understandable outcomes in fine-grained image classification tasks. Prototype networks operate by dissecting an image into informative image patches and comparing these with prototypical features established during their training phase. The model then aggregates evidence of similarities to these prototypical features to draw a final decision on classification.

Existing prototype-based models , which are **inherently interpretable** (i.e., the learned prototypes are directly interpretable, and all calculations can be visibly checked), are mainly developed with convolutional neural networks (CNNs). As vision transformers (ViTs)  gain popularity and inspire extensive applications, it becomes crucial to investigate how prototype-based architectures can be integrated with vision transformer backbones. Though a few attempts to develop a prototype-based vision transformer have been made , these methods do not provide inherently interpretable explanations of the models' reasoning because they do not project the learned prototypical features to examples that actually exist in the dataset.

Thus, the **cases** that these models use in their reasoning have no well defined visual representation, making it impossible to know what exactly each prototype represents.

The coherence and clarity of the learned prototypes are important. Most prototype-based models, such as ProtoPNet , TesNet  and ProtoConcept , use spatially rigid prototypical features (such as a rectangle), which cannot account for geometric variations of an object. Such rigid prototypical features can be ambiguous as they may contain multiple features in one bounding box (see top row of Fig. 1 for an example). Deformable ProtoPNet  deforms the prototypes into multiple pieces to adjust for the geometric transformation. However, Deformable ProtoPNet utilizes deformable convolution layers that rely on a continuous latent space to derive fractional offsets for prototypical parts to move around, and thus are not suitable for models like ViTs which output discrete tokens as latent representations. Additionally, the prototypes learned by Deformable ProtoPNet tend to be incoherent (see Fig. 1, middle row).

Addressing the gaps in current prototype-based methods (see Table 1), we propose the prototype-based vision transformer (ProtoViT), a novel architecture that incorporates a **ViT backbone** and can **adaptively** learn **inherently interpretable** and **geometrically variable** prototypes of **different sizes**, without requiring explicit information about the shape or size of the prototypes. We do so using a novel greedy matching algorithm that incorporates an adjacency mask and an adaptive slots mechanism. We provide global and local analysis, shown in Fig. 4, that empirically confirm the **faithfulness** and **coherence** of the prototype representations from ProtoViT. We show through empirical evaluation that ProtoViT achieves state-of-the-art accuracy as well as excellent clarity and coherence.

## 2 Related Work

_Posthoc_ explanations for CNNs like activation maximization [16; 28], image perturbation [17; 20], and saliency visualizations [3; 38; 39] fall short in explaining the reasoning process of deep neural networks because their explanations are not necessarily faithful [33; 2]. In addition, numerous efforts have been made to enhance the interpretability of Vision Transformers (ViTs) by analyzing attention weights [1; 42], and other studies focus on understanding the decision-making process through gradients [18; 19], attributions [9; 53], and redundancy reduction techniques . However, because of their posthoc nature, the outcomes of these techniques can be uncertain and unreliable [2; 25].

  Model & Support ViT Backbone7 & Deformable Proops7 & Coherent Proops7 & Adaptive Stores7 &  \\  ProtoPNet  & Yes & No & Maple & No & Yes \\ Deformable ProtoPNet & No & Yes & No & No & Yes \\ ProtoPNet  & Yes & No & Maple & No & No \\
**ProtoViT (Quars)** & **Yes** & **Yes** & **Yes** & **Yes** & **Yes** \\  

Table 1: Table of contributions of ProtoVit (Ours) compared to existing works.

Figure 1: Reasoning process of how prototype based models identify a test image of a male Ruby-Throated Hummingbird as the correct class. Prototypes are shown in the bounding boxes.

In contrast, prototype-based approaches offer a transparent prediction process through **case-based reasoning**. These models compare a small set of learned latent feature representations called prototypes (the "cases") with the latent representations of a test image to perform classification. These models are inherently interpretable because they leverage comparisons to only well-defined cases in reasoning. The original Prototypical Part Network (ProtoPNet)  employs class-specific prototypes, allocating a fixed number of prototypes to each class. Each prototype is trained to be similar to feature patches from images of its own class and dissimilar to patches from images of other classes. Each prototype is also a latent patch from a training image, which ensures that "cases" are well-defined in the reasoning process. The similarity score from the test image to each prototype is added as positive evidence for each class in a "scoresheet," and the class with the highest score is the predicted class. The Transparent Embedding Space Network (TesNet)  refines the original ProtoPNet by utilizing a cosine similarity metric to compute similarities between image patches and prototypes in a latent space. It further introduces new loss terms to encourage the orthogonality among prototype vectors of the same class and diversity between the subspaces associated with each class. Deformable ProtoPNet  aims to decompose the prototypical parts into smaller sub-patches and use deformable convolutions [12; 54] to capture pose variations. Other works [35; 27; 36] move away from class-specific prototypes, which reduces the number of prototypes needed. This allows prototypes to represent a similar visual concept shared in different classes. Our work is similar to these works that define "cases" as latent patch representations by projecting the trained class-specific prototypes to the closest latent patches.

As an alternative to the closest training patches, ProtoConcept  defines the cases as a group of visualizations in the latent spaces bounded by a prototypical ball. Although they are not projected, the visualizations of ProtoConcept are fixed to the set of training patches falling in each prototypical ball, which establishes well defined cases for each prototype. On the other hand, works such as ProtoPFormer  do not project learned prototypes to the closest training patches because they observed a performance degradation after projection. The degradation is likely caused by the fact that the prototypes are not sufficiently close to any of the latent patches. The design of a "global branch" that aims to learn prototypical features from class tokens also raises concerns about interpretability, as visualizing an arbitrary trained vector (class token) does not offer any semantic connection to the input image. On the other hand, work such as ViTNet  also lack details about how the "cases" are established, yielding concerns about the interpretability of the model. Without a mechanism like _prototype projection_ or _prototypical concepts_ to enable visualizations, prototypes are just arbitrary learned tensors in the latent space of the network, with no clear interpretations. Visualizing nearby examples alone cannot explain the model's reasoning, since the closest patch to a prototype can be arbitrarily far away without the above mechanisms. **Simply adding a prototype layer to an architecture without well defined "cases" in the reasoning process does not make the new architecture more interpretable.**

Our work is also related to INTR , which trains a class specific attention query and inputs it to a decoder to localize the patterns in an image with cross-attention. In contrast, our encoder-only model, through a different reasoning approach, learns patch-wise deformed features that are more explicit, semantically coherent, and provides more detail about the reasoning process than attention heatmaps [21; 48; 6] - it shows how the important pixels are used in reasoning, not just where they are.

## 3 Methods

We begin with a general introduction of the architecture followed by an in-depth exploration of its components. We then delve into our novel training methodology that encourages prototypical features to capture semantically coherent attributes from the training images. Detailed implementations on specific datasets are shown in the experiment sections.

### Architecture Overview

Fig. 2 shows an overview of the ProtoVit architecture. Our model consists of a feature encoder layer \(f\), which is a pretrained ViT backbone that computes a latent representation of an image; a greedy matching layer \(g\), which compares the latent representation to learned prototypes to compute prototype similarity scores; and an evidence layer \(h\), which aggregates prototype similarity scores into a classification using a fully connected layer. We explain each of these in detail below. Let

[MISSING_PAGE_FAIL:4]

to the embedding space as \(_{}^{N d}\), which is then prepended with a trainable class token \(_{}^{d}\), and passed into the Multi-Head Attention layers along with a learnable position embedding \(E_{}^{(N+1) d}\). The ViT backbone then outputs the encoded tokens as \(:=[_{};_{}^{1};_{}^{2};;_{}^{N}]\), where \(^{(N+1) d}\). In this sense, each of the output patch tokens \(_{}^{i}\) is the latent representation of the corresponding unit patch \(_{}^{i}\). The class token can be viewed as a way to approximate the weighted sum over patch tokens, enabling an image-level representation. It is, thus, difficult to visualize the class token and therefore unsuitable for comparisons to prototypes. Drawing inspiration from the idea of focal similarity , which involves calculating the maximum prototype activations minus the mean activations to achieve more focused representations, we take the difference between patch-wise features and the image-level representation. In doing so, we aim to similarly produce more salient visualizations. Specifically, we define the feature token \(_{f}\) by taking the difference between each patch token and the image-level representation. Thus, latent feature tokens can be written as:

\[_{f}:=[_{f}^{1};_{f}^{2};;_{f}^ {N}],_{f}^{i}=_{}^{i}-_{}, _{f}^{i}^{d}.\] (1)

By this design, we not only encode richer semantic information within the latent feature tokens, but also enable the application of prototype layers to various ViT backbones that contains a class token. An ablation study shows that model performance drops substantially when removing the class token from the feature encoding (see Appendix Sec. E.1).

### Greedy Matching Layer

The greedy matching layer \(g\) integrates three key components: a greedy matching algorithm, adjacency masking, and an adaptive slots mechanism, as illustrated in Fig. 2.

Similarity Metric:The greedy matching layer contains \(m\) prototypes \(=\{_{j}\}_{j=1}^{m}\), where each \(_{j}\) consists of \(K\) sub-prototypes \(_{j}^{k}\) that have the same dimension as each latent feature token \(_{f}^{i}\). Each sub-prototype \(_{j}^{k}\) is trained to be semantically close to at least one latent feature token. To measure the closeness of the sub-prototype \(_{j}^{k}\) and latent feature token \(_{f}^{i}\), we use cosine similarity \((_{f}^{i},_{j}^{k})=_{f}^{i} _{j}^{k}}{\|_{f}^{i}\|_{2}\|_{j}^{k}\|_{2}}\), which has range -1 to 1. The overall similarity between prototype \(_{j}\) and the latent feature tokens \(_{f}\) is then computed as the sum across the sub-prototypes of the cosine similarities, which has a range from \(-K\) to \(K\).

Greedy Matching Algorithm:We have not yet described how the token \(_{f}^{i}\) to which \(_{j}^{k}\) is compared is selected. In contrast to prior work, we do not restrict that sub-prototypes have fixed relative locations. Rather, we perform a greedy matching algorithm to identify and compare each of the \(K\) sub-prototypes \(_{j}^{k}\) of \(_{j}\) to _any_\(K\) non-overlapping latent feature tokens. To be exact, for a given prototype \(_{j}\) with \(K\) sub-prototypes, we iteratively identify the sub-prototype \(_{j}^{k}\) and latent feature token \(_{f}^{i}\) that are closest in cosine similarity, "match" \(_{f}^{i}\) with \(_{j}^{k}\), and remove \(_{f}^{i}\) and \(_{j}^{k}\) from the next iteration, until all \(K\) pairs are found. This is illustrated in Fig. 2, and more details can be found in Appendix Sec. C.

Adjacency Masking:Without any restrictions, this greedy matching algorithm can lead to many sub-prototypes that are geometrically distant from each other. Assuming that the image patches representing the same feature are within \(r\) spatial units of one another in horizontal, vertical and diagonal directions, we introduce the Adjacency Mask \(A\) to temporarily mask out latent feature tokens that are more than \(r\) positions away from a selected sub-prototype/latent feature token pair in all directions. Within each iteration \(k\) of the greedy matching algorithm, the next pair can only be selected from the latent feature tokens, \(_{A_{j}^{k}}=A(\{_{j}^{k-1},_{f}^{i}{}^{k-1}\}; _{f};r)\), that are within \(r\) positions of the last selected pair. An example of how the adjacency mask with \(r=1\) works is illustrated in Fig. 2. Incorporating adjacency masking with the greedy matching algorithm, we thus find \(K\) non-overlapping and adjacent sub-prototype-latent patch pairs.

Adaptive Slots Mechanism:Because not all concepts require \(K\) sub-prototypes to represent, we introduce the the adaptive slots mechanism \(S\). The adaptive slots mechanism consists of learnable vectors \(^{m K}\) and a sigmoid function with a hyperparameter temperature \(\). We chose a sufficiently large value for \(\) to ensure that the sigmoid function has a steep slope. The vectors \(\) are sent to the sigmoid function to approximate the indicator function as \(}_{\{_{j}^{k}\}}=( _{j}^{k},)\). Each \(}_{\{_{j}^{k}\}}\) is an approximation of the indicator for whether the \(k\)-th sub-prototype will be included in the \(j\)-th prototype \(_{j}\). More details can be found in Appendix Sec. D.

Computation of Similarity:As described above, we measure the similarity of a selected latent-patch-sub-prototype pair using cosine similarity. We use the summed similarity across sub-prototypes to measure the overall similarity for prototype \(_{j}\) with \(K\) selected non-overlapping latent feature tokens. As pruning out some sub-prototypes for a prototype \(_{j}\) reduces the range of the summed cosine similarity for \(_{j}\), we rescale the summed similarities back to their original range by \(K/_{k}}\{_{j}^{k}\}\). We formally define the reweighted summed similarity function obtained from the greedy matching layer \(g\) as:

\[g_{_{j}}^{}(_{f})=^{K} }_{\{_{j}^{k}\}}}_{k=1}^{K}\{_{_{f}^{i} _{A_{j}^{k}}}(_{f}^{i},_{j}^{k}) \}}_{\{_{j}^{k}\}}.\] (2)

### Training Algorithm

Training of ProtoViT has four stages: (1) optimizing layers before the last layer by stochastic gradient descent (SGD); (2) prototype slots pruning; (3) projecting the trained prototype vectors to the closest latent patches; (4) optimizing the last layer \(h\). Note that a well-trained first stage is crucial to achieve minimal performance degradation after prototype projection. A procedure plot is illustrated in Appendix Fig. 5.

Optimization of layers before last layer:The first training stage aims to learn a latent space that clusters feature patches from the training set that are important for a class near semantically similar prototypes of that class. This involves solving a joint optimization problem over the network parameters via stochastic gradient descent (SGD). We initialize every slot indicator \(}_{\{_{j}^{k}\}}\) as 1 to allow all sub-prototypes to learn during SGD. We initialize the last layer weight similarly to ProtoPNet . The slot indicators and the final layer are frozen during this training stage. The slot indicator functions are not involved in computing cluster loss or separation loss because each sub-prototype should remain semantically close to certain latent feature tokens, regardless of its inclusion in the final computation. Since we deform the prototypes, we propose modifications to the original cluster and separation loss defined in  to:

\[_{Clst}&=-_{ i=1}^{n}_{_{j}_{y_{i}}}_{_{j}^{k} _{j}}_{_{f}^{i}_{A_{j}^{k}}}( _{f}^{i},_{j}^{k});\\ _{Sep}&=_{i=1}^{n}_{ _{j}_{y_{i}}}_{_{j}^{k}_{ j}}_{_{f}^{i}_{A_{j}^{k}}}(_{f}^{i}, _{j}^{k}).\] (3)

The minimization of this new cluster loss encourages each training image to have some unit latent feature token \(_{f}^{i}\) that is close to at least one sub-prototypes \(_{j}^{k}\) of its own class. Similarly, by minimizing the new separation loss, we encourage every latent feature token of a training image to stay away from the nearest sub-prototype from any incorrect class. This is similar to the traditional cluster and separation loss from  if we define the prototypes to consist of only one sub-prototype. We further introduce a novel loss term, called coherence loss, based on the intuition that, if the sub-prototypes collectively represent the same feature (e.g., the feet of a bird), these sub-prototypes should be similar under cosine similarity. The coherence loss is defined as:

\[_{Coh}=_{j=1}^{m}_{_{j}^{k},_ {j}^{s}_{j};_{j}^{s}_{j}^{s}}(1-(_{j}^{k},_{j}^{s}))}_{\{ _{j}^{k}\}}}_{\{_{j}^{s}\}}.\] (4)

Intuitively, the coherence loss penalizes the sub-prototypes that are the most dissimilar to the other sub-prototypes out of the \(K\) slots for prototype \(_{j}\). The slots indicator function is added to the coherence loss term to prune sub-prototypes that are semantically distant from others in a prototype. Moreover, we use the orthogonality loss, introduced in previous work [46; 13], to encourage each prototype \(_{j}\) to learn distinctive features. The orthogonality loss is defined as:

\[_{Orth}=_{l=1}^{C}\|^{(l)}{^{(l)}}^{T}- _{}\|_{}^{2}\] (5)

where \(C\) is the number of classes. For each class \(l\) with \(\) assigned prototypes, \(^{(l)}^{ Kd}\) is a matrix obtained by flattened the prototypes from class \(l\), and \(\) is the number of prototypes \(_{j}\) for each class. \(_{}\) is an identity matrix in the shape of \(\). Overall, this training stage aims to minimize the total loss as:

\[_{total}=_{CE}+_{1}_{Clst}+_{2} _{Sep}+_{3}_{Coh}+_{4}_{Orth}\] (6)

where \(_{CE}\) is the cross entropy loss for classification and \(_{1},_{2},_{3},_{4}\) are hyper-parameters.

Slots pruning:In this stage, our goal is to prune the sub-prototypes \(_{j}^{k}\) that are dissimilar to the other sub-prototypes for each prototype. Intuitively, we aim to remove these sub-prototypes because sub-prototypes that are not similar will lead to prototypes with an inconsistent, unintuitive semantic meaning. We freeze all of the parameters in the model, except the slot indicator vectors \(\). During this stage, we jointly optimize the coherence loss defined in Eq. 4 along with the cross entropy loss. We lower the coherence loss weight to avoid removing all the slots. Since the slot indicators are approximations of step functions using sigmoid functions with a sufficiently high temperature parameter \(\), the indicator values during this phase are fractional but approach binary values close to 0 or 1. The loss in the stage is defined as:

\[_{}=_{CE}+_{5}_{Coh}.\] (7)

Projection:In this stage, we first round the fractional indicator values to the nearest integer (either 1 or 0), and freeze the slot indicators' values. Then, we project each prototype \(_{j}\) to the closest training image patch measured by the summed cosine similarity defined in Eq. 2, as in . Because the latent feature tokens from the ViT encoder correspond to image patches, we do not need to use up-sampling techniques for visualizations, and the prototypes visualized in the bounding boxes represent the exact corresponding latent feature tokens that the prototypes are projected to.

As demonstrated in Theorem 2.1 from ProtoPNet , if the prototype projection results in minimal movement, the model performance is unlikely to change because the decision boundary remains largely unaffected. This is ensured by minimizing cluster and separation loss, as defined in Eq. 3. In practice, when prototypes are sufficiently well-trained and closely clustered to certain latent patches, the change in model performance after the projection step should be minimal. Additionally, this step ensures that the prototypes are inherently interpretable, as they are projected to the closest latent feature tokens, which have corresponding visualizations and provide a pixel space explanation of the "cases" in the model's reasoning process.

Optimization of last layers:Similar to other existing works [10; 46], we performed a convex optimization on the last evidence layer \(h\) after performing projection, while freezing all other parameters. This stage aims to introduce sparsity to the last layer weights \(W\) by penalizing the \(1-\)norm of layer weights \(_{,}\) (initially fixed as -0.5) associated with the class \(b\) for the \(l\)-th class prototype \(^{l}\), where \(l b\). By minimizing this loss term, the model is encouraged to use only positive evidence for final classifications. The loss term is defined as:

\[_{h}=_{CE}+_{6}_{l}^{C}_{b\{0,...,C\}:b  l}\|_{,}\|_{1}.\] (8)

## 4 Experiments

### Case Study 1: Bird Species Identification

To demonstrate the effectiveness of ProtoVit, we applied it to the cropped Caltech-UCSD Birds-200-2011 (CUB 200-2011) dataset . This dataset contains 5,994/5,794 images for training and testing from 200 different bird species. We performed similar offline image augmentation to previous work [10; 46; 26] which used random rotation, skew, shear, and left-right flipping. After augmentation, the training set had roughly 1,200 images per class. We performed prototype projections on only the non-augmented training data. Additionally, we performed ablation studies on the class token (See Appendix Sec. E.1), coherence loss (See Appendix Sec. E.3), and adjacency mask (See Appendix Sec. E.2). The quantitative results for the ablation can be found in Appendix Table. 6. We assigned the algorithm to choose 10 class-specific prototypes for each of the 200 classes. Each of the prototypes is composed of 4 sub-prototypes. Each set of sub-prototypes was encouraged to learn the 'key' features for its corresponding class through only the last layer weighting, and the 4 sub-prototypes were designed to collectively represent one key feature for the class. More discussion on different choices of \(K\) can be found in Appendix F. For more details about hyperparameter settings and training schedules, please refer to Appendix Sec. A and Appendix Sec. B respectively. Details and results of user studies regarding the interpretability of ProtoViT can be found in Appendix. Sec. K.

#### 4.1.1 Predictive Performance Results

The performance of our ProtoViT with CaiT and DeiT backbones is compared to that of existing work in Table 2, including results from prototype-based models using DenseNet161 (CNN) backbones. Integrating ViT (Vision Transformer) backbones with a smaller number of parameters into prototypes based algorithms significantly enhances performance compared to those using CNN (Convolutional Neural Network) backbones, particularly on more challenging datasets. Compared with other prototype-based models that utilize ViT backbones, our model produces the **highest accuracy**, **outperforming** even the black box ViTs used as backbones, while offering **interpretability**. We provide accuracy using an ImageNet pretrained Densenet-161 to match the ImageNet pretraining used in all transformer models.

#### 4.1.2 Reasoning Process and Analysis

Fig. 3 shows the reasoning process of ProtoViT for a test image of a Barn Swallow. Example visualizations for the classes with the top two highest logit scores are provided in the figure. Given this test image \(\), our model compares its latent feature tokens against the learned sub-prototypes \(_{j}^{k}\) through the greedy matching layer \(g\). In the decision process, our model uses the patches that have the most similar latent feature tokens to the learned prototypes as evidences. In the example, our model correctly classifies a test image of a Barn Swallow and thinks the Tree Swallow is the second most likely class based on prototypical features. In addition to this example reasoning process, we conducted local and global analyses (shown in Fig. 4) to confirm the semantic consistency of prototypical features across all training and testing images, where local and global analyses are

  Arch. & Model & CUB Acc.\([\%]\) & Car Acc.\([\%]\) \\   & ProtoPNet (given in ) & 80.1 \(\) 0.3 & 89.5 \(\) 0.2 \\  & Def. ProtoPNet(2x2) (given in ) & 80.9 \(\)0.22 & 88.7 \(\) 0.3 \\ \(\)28.68M params & ProtoPool (given in ) & 80.3 \(\) 0.3 & 90.0 \(\) 0.3 \\  & TesNet (given in ) & **81.5 \(\) 0.3** & **92.6 \(\) 0.3** \\   & Base (given in ) & 80.57 & 86.21 \\  & ViT-Net (given in ) & 81.98 & 88.41 \\  & ProtoPformer (given in ) & 82.26 & 88.48 \\  & **ProtoViT(\(K\)=4, \(r\)=1) (ours)** & **82.92 \(\) 0.5** & **89.02 \(\) 0.1** \\   & Baseline (given in ) & 84.28 & 90.06 \\  & ViT-Net (given in ) & 84.26 & 91.34 \\ \(\)22M params & ProtoPFormer (given in ) & 84.85 & 90.86 \\  & **ProtoViT(\(K\)=4, \(r\)=1) (ours)** & **85.37 \(\) 0.13** & **91.84 \(\) 0.3** \\   & Baseline (given in ) & 83.95 & 90.19 \\  & ViT-Net (given in ) & 84.51 & 91.54 \\ \(\)11.9M params & ProtoPFormer (given in ) & 84.79 & 91.04 \\   & **ProtoViT(\(K\)=4, \(r\)=1) (ours)** & **85.82 \(\) 0.15** & **92.40 \(\) 0.1** \\  

Table 2: Comparison of ProtoViT implemented with DeiT and CaiT backbones to other existing works. Our model is not only inherently interpretable but also superior in performance compared to other methods using the same backbone. We also include models with a CNN backbone (Densenet-161) for reference in the top section. The reported accuracies are the final results after all training stages.

defined as in . The left side of Fig. 4 displays local analysis examples, visualizing the most semantically similar prototypes to each test image. The right side of Fig. 4 shows global analysis examples, presenting the top three nearest training and testing images to the prototypes. Our local analysis confirms that, across distinct classes and prototypes, the comparisons made are reasonable. For example, the first prototype compared to the white breasted kingfisher seems to identify the bird's blue tail, and is compared to the tail of the bird in the test image. Further, our global analysis shows that each prototype consistently activates on a single, meaningful concept. For example, the prototype in the first row of the global analysis consistently highlights the black face of the common yellow throat at a variety of scales and poses. Taken together, these analyses show that **the prototypes of ProtoViT have strong, consistent semantic meanings**. More examples of the reasoning process and analysis can be found in Appendix Sec. H and Sec. J respectively.

#### 4.1.3 Location Misalignment Analysis

Vision Transformers (ViTs) use an attention mechanism that blends information from all image patches, which may prevent the latent token at a position from corresponding to the input token at that

Figure 4: Nearest prototypes to test images (left), and the nearest image patches to prototypes (right). We exclude the nearest training patch, which is the prototype itself by projection.

Figure 3: Reasoning process of how ProtoViT classifies a test image of Barn Swallow using the learned prototypes. Examples for the top two predicted classes are provided. We use the DeiT-Small backbone with \(r\)=1 and \(k\)=4 for the adjacency mask.

position. To assess whether ViT backbones can be interpreted as reliably as Convolutional Neural Networks (CNNs) in ProtoPNets, we conducted experiments using the gradient-based adversarial attacks described in the Location Misalignment Benchmark . As summarized in Table 3, our method, which incorporates a ViT backbone, consistently matched or outperformed leading CNN-based prototype models such as ProtoPNet , ProtoPool , and ProtoTree  across key metrics: Percentage Change in Location (PLC), Percentage Change in Activation (PAC), and Percentage Change in Ranking (PRC), as defined in the benchmark. Lower values for these metrics indicate better performance. This shows that ProtoViT is at least as robust as CNN-based models. Moreover, as observed in the Location Misalignment Benchmark, the potential for information leakage also exists in deep CNNs, where the large receptive fields of deeper layers would encompass the entire image same as attention mechanisms. **While our model is not entirely immune to location misalignment, empirical results indicate that its performance is on par with other state-of-the-art CNN-based models.** Qualitative comparisons that further support this point can be found in Appendix. Sec. G, and more results and discussions on PLC for ablated models can be found in Appendix. Sec. E

### Case Study 2: Cars Identification

In this case study, we apply our model to car identification. We trained our model on the Stanford Car dataset of 196 car models. More details about the implementation can be found in Appendix. Sec. L. The performance with baseline models can be found in Table. 2. Example visualizations and analysis can be found in Appendix L. We find that **ProtoViT again produces superior accuracy and strong, semantically meaningful prototypes on the Cars dataset.**

## 5 Limitations

While we find our method can offer coherent and consistent visual explanations, it is not yet able to provide explicit textual justification to explain its reasoning process. With recent progress in large vision-language hybrid models [23; 11; 29], a technique offering explicit justification for visual explanations could be explored in future work. It is important to note that in some visual domains such as mammography or other radiology applications, features may not have natural textual descriptions - thus, explicit semantics are not yet possible, and a larger domain-specific vocabulary would first need to be developed. Moreover, as discussed in Sec. 4.1.3, our model is not completely immune to location misalignment, meaning the learned prototypical features may be difficult to visualize in local patches. However, this issue is not unique to our approach; CNN-based models face the same challenge. As layers deepen, the receptive field often expands to cover the entire image, leading to the same problem encountered with attention mechanisms in vision transformers.

## 6 Conclusion

In this work, we presented an interpretable method for image classification that incorprates ViT backbones with deformed prototypes to explain its predictions (_this_ looks like _that_). Unlike previous works in prototype-based classification, our method offers spatially deformed prototypes that not only account for geometric variations of objects but also provide coherent prototypical feature representations with an adaptive number of prototypical parts. While offering inherent interpretability, our model empirically outperform the previous prototype based methods in accuracy.

  Method & PLC & PRC & PAC & Acc. Before & Acc. After & AC. \\  ProtoPNet & 24.0 \(\) 1.7 & 13.5 \(\) 3.1 & 23.7 \(\) 2.8 & 76.4 \(\) 0.2 & 68.2 \(\) 0.9 & 8.2 \(\) 1.1 \\ TesNet & 16.0 \(\) 0.0 & 2.9 \(\) 0.3 & 3.4 \(\) 0.3 & 81.6 \(\) 0.2 & 75.8 \(\) 0.5 & 5.8 \(\) 0.6 \\ ProtoPool & 31.8 \(\) 0.8 & 4.5 \(\) 0.9 & 11.2 \(\) 1.3 & 80.8 \(\) 0.2 & 76.0 \(\) 0.3 & 4.8 \(\) 0.1 \\ ProtoTree & 27.7 \(\) 0.3 & 13.5 \(\) 3.1 & 23.7 \(\) 2.8 & 76.4 \(\) 0.2 & 68.2 \(\) 0.9 & 8.2 \(\) 1.1 \\
**ProtoViT(ours)** & **21.68 \(\) 3.1** & **1.28 \(\) 0.1** & **2.92 \(\) 0.1** & **85.4 \(\) 0.1** & **82.8 \(\) 0.3** & **2.6 \(\) 0.4** \\  

Table 3: Experimental results on the Location Misalignment Benchmark. We compare our ProtoViT (Deit-Small) with other prototype-based models with CNN backbones (ResNet34). We found that our model performs similarly to or better than the existing models with CNN backbones in terms of the misalignment metrics and test accuracy.

Acknowledgement

We would like to acknowledge funding from the National Science Foundation under grants HRD-2222336 and OIA-2218063 and the Department of Energy under DE-SC0021358.