# Adaptive \(Q\)-Aid for Conditional Supervised Learning

in Offline Reinforcement Learning

 Jeonghye Kim\({}^{1}\), Suyoung Lee\({}^{1}\), Woojun Kim\({}^{2}\), Youngchul Sung\({}^{1}\)

\({}^{1}\)KAIST \({}^{2}\)Carnegie Mellon University

Correspondence to Youngchul Sung.

###### Abstract

Offline reinforcement learning (RL) has progressed with return-conditioned supervised learning (RCSL), but its lack of stitching ability remains a limitation. We introduce \(Q\)-Aided Conditional Supervised Learning (QCS), which effectively combines the stability of RCSL with the stitching capability of \(Q\)-functions. By analyzing \(Q\)-function over-generalization, which impairs stable stitching, QCS adaptively integrates \(Q\)-aid into RCSL's loss function based on trajectory return. Empirical results show that QCS significantly outperforms RCSL and value-based methods, consistently achieving or exceeding the maximum trajectory returns across diverse offline RL benchmarks. The project page is available at https://beanie00.com/publications/qcs.

## 1 Introduction

Offline reinforcement learning (RL) is a vital framework for acquiring decision-making skills from fixed datasets, particularly when online interactions are impractical. This is especially relevant in fields such as robotics, autonomous driving, and healthcare, where the costs and risks of real-time experimentation are significant.

A promising approach in offline RL is return-conditioned supervised learning (RCSL) [10; 12; 20]. By framing offline RL as sequence modeling tasks, RCSL allows an agent to leverage past experiences and condition on the target outcome, facilitating the generation of future actions that are likely to achieve desired outcomes. This method builds on recent advancements in supervised learning (SL) [41; 9; 11; 30], and thus benefits from the inherent stability and scalability of SL. However, RCSL is significantly limited by its lack of'stitching ability', the ability to combine suboptimal trajectory segments to form better overall trajectories [13; 27; 53; 16; 7; 58]. As a result, its effectiveness is restricted to the best trajectories within the dataset.

Conversely, the \(Q\)-function possesses the ability to stitch together multiple sub-optimal trajectories, dissecting and reassembling them into an optimal trajectory through dynamic programming. Therefore, to address the weakness of RCSL, prior works have attempted to enhance stitching ability through the \(Q\)-function [53; 16]. However, these prior works employ the \(Q\)-function as a conditioning factor for RCSL and do not fully leverage the \(Q\)-function's stitching ability, resulting in either negligible performance improvements or even reduced performance. The primary challenge lies

Figure 1: **Conceptual idea of QCS**: Follow RCSL when learning from optimal trajectories where it predicts actions confidently but the \(Q\)-function may stitch incorrectly. Conversely, refer to the \(Q\)-function when learning from sub-optimal trajectories where RCSL is less certain but the \(Q\)-function is likely accurate.

in the fact that utilizing the \(Q\)-function through conditioning, without managing the conditions for stable \(Q\)-guided stitching, can result in a sub-optimal algorithm.

In this work, we aim to fully synergize the stable and scalable learning framework of RCSL with the stitching ability of the \(Q\)-function. To effectively utilize the \(Q\)-function, it is crucial to identify when it can benefit RCSL and to integrate its assistance, termed \(Q\)-aid. Our contributions to achieving the effective utilization of \(Q\)-aid in RCSL are as follows: (1) We discovered that in-sample \(Q\)-learning on an expert dataset, which predominantly consists of optimal actions with similar \(Q\)-values within a constrained action range, causes the \(Q\)-function to receive improper learning signals and become over-generalized. (2) To prevent errors from this \(Q\)-generalization and to incorporate stitching ability within RCSL's stable framework, we propose \(Q\)-Aided Conditional Supervised Learning (QCS), which adaptively integrates \(Q\)-aid into the RCSL's loss function based on trajectory returns.

Despite its simplicity, the effectiveness of QCS is empirically demonstrated across offline RL benchmarks, showing significant advancements over existing state-of-the-art (SOTA) methods, including both RCSL and value-based methods. Especially, QCS surpasses the maximal dataset trajectory return across diverse MuJoCo datasets, under varying degrees of sub-optimality, as shown in Fig. 2. Furthermore, QCS significantly outperforms the baseline methods in the challenging AntMaze Large environment. This notable achievement underscores the practical effectiveness of QCS in offline RL.

## 2 Preliminaries

We consider a Markov Decision Process (MDP) , described as a tuple \(=(,,,_{0},r,)\). \(\) is the state space, and \(\) is the action space. \(:()\) is the transition dynamics, \(_{0}()\) is the initial state distribution, \(r:\) is the reward function, and \([0,1)\) is a discount factor. The objective of offline RL is to learn a policy \((|s)\) that maximizes the expected cumulative discounted reward, \(_{a_{t}(|s_{t}),s_{t+1}(|s_{t},a_{t} )}[_{t=0}^{}^{t}r(s_{t},a_{t})]\), using a static dataset \(=\{^{(i)}\}_{i=1}^{D}\) comprising a set of trajectories \(^{(i)}\). Each trajectory \(^{(i)}\) consists of transitions over a time horizon \(T\), collected from an unknown behavior policy \(\).

### Value-Based Offline Reinforcement Learning

Offline RL effectively employs off-policy RL techniques, allowing a divergence between the behavior policy \(\) used for data acquisition and the target policy \(\) being optimized [25; 14; 23]. Off-policy methods primarily utilize the \(Q\)-function, which is learned through temporal-difference (TD) bootstrapping. In actor-critic off-policy approaches, both the \(Q\)-function \(_{}\) and the policy \(\) are updated iteratively. This process can cause a shift in the action distribution, leading \(\) to select actions that significantly deviate from those in the training dataset. These deviations can inadvertently result in overestimation errors, especially for out-of-distribution (OOD) actions, as offline RL cannot correct incorrect \(Q\)-values through interactions with the environment.

Unlike actor-critic methods, in-sample learning methods use only in-sample actions to learn the optimal \(Q\)-function, thereby preventing the querying of OOD action Q-values during training [34; 32; 23; 51]. Implicit \(Q\)-Learning (IQL)  is a representative in-sample learning method. It utilizes expectile regression, defined as \(L_{}^{2}(u)=|-(u<0)|u^{2}\) where \([0.5,1)\), to formulate the

Figure 2: Mean normalized return in MuJoCo medium, medium-replay, medium-expert, and AntMaze large. The scores of RCSL, the value-based methods, and the combined methods represent the maximum mean performances within their respective groups. The full scores are in Section 6.2.

asymmetrical loss function for the value network \(V_{}\). Through this loss, \(V_{}\) can approximate the implicit maximum of the TD target, \(_{a}Q_{}(s,a)\). Formally, for a parameterized critic \(Q_{}(s,a)\) with a target critic \(Q_{}(s,a)\), the value loss function is given by

\[_{V}()=*{}_{(s,a)}[L _{}^{2}(Q_{}(s,a)-V_{}(s))].\] (1)

Intuitively, this loss function suggests placing more emphasis when \(Q_{}\) is greater than \(V_{}(s)\). Subsequently, the critic network \(Q_{}\) is updated by treating the learned \(V_{}(s^{})\) as \(_{a^{}(s^{})}Q_{}(s^{},a^{ })\), where \((s^{})\) denotes the in-sample actions for the given state \(s^{}\), i.e., \((s^{},a^{})\):

\[_{Q}()=*{}_{(s,a,s^{}) }[(r(s,a)+ V_{}(s^{})-Q_{}(s,a) )^{2}].\] (2)

We use IQL to pretrain the \(Q\)-function used to aid RCSL, as we found that this method, without conservatism during \(Q\)-function training, can provide good stitching ability when well integrated. A comparison with a different \(Q\)-learning method, CQL , is provided in Appendix H.1.

### Return-Conditioned Supervised Learning (RCSL)

RCSL is an emerging approach to addressing challenges in offline RL. It focuses on learning the action distribution conditioned on _return-to-go_ (RTG), defined as the cumulative sum of future rewards \(_{t}=_{t^{}=t}^{T}r_{t^{}}\) through supervised learning (SL) [10; 12; 20]. Due to the stability of SL, RCSL is capable of learning decision-making by extracting and mimicking useful information from the dataset. In particular, Decision Transformer (DT)  applies the Transformer architecture  to reframe the RL as a sequence modeling problem. It constructs input sequences to the Transformer by using sub-trajectories, each spanning \(K\) timesteps and comprising RTGs, states, and actions: \(_{t-K+1:t}=(_{t-K+1},s_{t-K+1},a_{t-K+1},...,_{t-1},s_{t-1},a _{t-1},_{t},s_{t})\). The model is then trained to predict the action \(a_{t}\) based on \(_{t-K+1:t}\). Recently, Kim et al.  proposed Decision ConvFormer (DC) to simplify the attention module of DT and better model the local dependency in the dataset, yielding performance gains over DT with reduced complexity. These methods have shown effective planning capabilities, but they lack stitching ability, which causes difficulties with datasets that contain many sub-optimal trajectories. This will be discussed in more detail in Section 3.1.

### Neural Tangent Kernel of \(Q\)-Function

The Neural Tangent Kernel (NTK)  provides insightful analysis of function approximation errors of \(Q\)-function, \(Q_{}\), especially those related to generalization. The NTK, denoted as \(k_{}(,,s,a)\), is defined as the inner product of two gradient vectors, \(_{}Q_{}(,)\) and \(_{}Q_{}(s,a)\), i.e., \(k_{}(,,s,a):=_{}Q_{}(,)^{ }_{}Q_{}(s,a)\). The NTK offers a valuable perspective on the impact of parameter updates in function approximation, particularly in gradient descent scenarios. It essentially measures the degree of influence a parameter update for one state-action pair \((s,a)\) exerts on another pair \((,)\). A high value of \(k_{}(,,s,a)\) implies that a single update in the \(Q_{}\) for the pair \((s,a)\) could lead to substantial changes for the pair \((,)\). We guide the readers to Appendix D for a deeper understanding of the NTK.

## 3 When Is \(Q\)-Aid Beneficial for RCSL?

When is it beneficial for RCSL to receive assistance from the \(Q\)-function, denoted as \(Q_{}\), and how should this assistance be provided? To explore this, we trained two policies, RCSL policy and a max-\(Q\) policy that selects the best action according to \(Q_{}\), on two different quality datasets from D4RL  MuJoCo, comparing their performances in Table 1. Note that the performance is not directly linked to the policy's accuracy across all states; even if the agent accurately predicts actions in several states, errors in some states can lead to path deviations and accumulated errors, resulting in a test-time distribution shift and a lower trajectory return. However, these results can provide insight into when \(Q\)-aid might be helpful.

For the RCSL algorithm, we used the Decision Transformer (DT) . To train the max-\(Q\) policy, we first trained the \(Q_{}\) using the in-sample \(Q\)-learning method outlined in Eqs. (1) and (2). Then, we extracted the max-\(Q\) policy to select the action that directly maximizes \(Q_{}(s,)\) for each state \(s\) by using a 3-layer MLP and the loss function \(_{Q}()=*{}_{s} [-Q_{}(s,Q_{}(s))]\).

Observing Table 1, we see that the dataset quality favoring RCSL contrasts with that benefiting the max-\(Q\) policy. RCSL tends to perform well by mimicking actions in high-return trajectory datasets [29; 31]. However, this method is less effective with datasets predominantly containing suboptimal trajectories, even though RTG conditioning helps predict actions that yield higher returns. On the other hand, the max-\(Q\) policy excels with suboptimal datasets but shows notably poor results with optimal datasets. From these observations, a motivating question arises: _"Why does the simple max-\(Q\) policy outperforms RCSL on suboptimal datasets yet fails on optimal datasets? If so, how can we effectively combine the two methods to achieve optimal performance?"_

### How Can Max-\(Q\) Policy Surpass RCSL in Suboptimal Datasets?

We present a toy example demonstrating the limitation of RCSL, as illustrated in Fig. 3. Suppose the dataset is composed of two sub-optimal trajectories. At the initial state \(s^{1}\), the agent has two options: the \(\) action connected to trajectory 2 (the orange trajectory) with an RTG of 5, and the \(\) action connected to trajectory 1 (the purple trajectory) with an RTG of 6. RCSL makes the agent choose the \(\) action with a high RTG and follow the path of trajectory 1, which is not optimal. This example demonstrates that RCSL alone is insufficient for the agent to learn to assemble the parts of beneficial sub-trajectories.

In contrast, a \(Q\)-function can develop stitching ability. Consider the example in Fig. 3 again. We can compute the \(Q\)-values for the actions \(\) and \(\) at state \(s_{1}\) with dynamic programming: \(Q(s^{1},)=3+(Q(s^{2},),Q(s^{2},\,))=7\), \(Q(s^{1},)=1+(Q(s^{3},),Q(s^{3},\,) )=6\).

With the \(Q\)-values, the agent will select the \(\) action at \(s^{1}\) and then the \(\) action at \(s^{2}\). Consequently, using the \(Q\)-function, the agent can select the optimal action that yields the maximum return of 7. Therefore, integrating RCSL with \(Q\)-function in situations with abundant sub-optimal trajectories can be beneficial for developing the stitching ability required for optimal decision-making.

### Why Does Max-\(Q\) Policy Struggle with Optimal Datasets?

Despite the potential advantages of using \(Q\)-values, incorporating values from a learned \(Q\)-function, \(Q_{}\), to aid RCSL can introduce errors due to inaccuracies in learning. These inaccuracies are particularly significant when \(Q_{}\) is learned from optimal trajectories. Suppose we have an optimal policy \(^{}\). Optimal trajectories are visit logs containing actions performed by \(^{}\), yielding the best \(Q\)-value for a given state \(s\). Due to the stochasticity of \(^{}\), multiple similar actions can be sampled from \(^{}\), namely \(a_{1}^{},a_{2}^{},,a_{n(s)}^{}^{}(|s )\) for a given state \(s\). In this case, we have \(Q^{}(s,a_{i}^{}) Q^{}(s,a_{j}^{})\) and \(a_{i}^{} a_{j}^{}\)\( i,j\{1,2,,n(s)\}\) due to the optimality of these actions. When learning \(Q_{}\) from such limited information, where the values at the narrow action points are almost identical for each given state, it is observed that the learned \(Q_{}(s,a)\) tends to be over-generalized to the OOD action region. This means that the nearly identical value at the in-sample actions \(a_{1}^{},a_{2}^{},,a_{n(s)}^{}\) is extrapolated to OOD actions, yielding a nearly flat \(Q\)-value over the entire action space for each given state, i.e., \(Q_{}(s,a_{}) Q_{}(s,a_{1}^{})\) with \(Q_{}(s,a)\) being a function of \(s\) only. This over-generalization makes \(Q_{}\) noise-sensitive, potentially assigning high values to incorrect actions and causing state distribution shifts in the test phase, as shown in Fig. 7.

We present a simple experiment to verify that learning \(Q_{}\) indeed induces over-generalization when trained on optimal trajectories. The experiment consists of an MDP with one-dimensional discrete

   & halfcheetah-e & halfcheetah-m-r & hopper-e & hopper-m-r & walker2d-e & walker2d-m-r \\  DT & 91.4 \(\) 1.7 & 36.6 \(\) 0.8 & 110.1 \(\) 0.9 & 82.7 \(\) 7.0 & 109.2 \(\) 1.5 & 66.6 \(\) 3.0 \\ max-\(Q\) & -4.1 \(\) 1.1 & 52.8 \(\) 0.4 & 1.8 \(\) 1.0 & 92.1 \(\) 2.6 & -0.2 \(\) 0.6 & 91.2 \(\) 1.9 \\  

Table 1: Performance comparison of DT and max-\(Q\) on expert and medium-replay quality datasets in MuJoCo.

Figure 3: An example demonstrating the limit of RCSL: The dataset consists of two trajectories, with a time limit of \(T=3\) and a discount factor \(=1\). The black dashed arrow represents the optimal policy yielding a maximum return of 7.

states and actions, each divided into 500 bins. This environment simulates a car, where the state indicates the agent's position, ranging from -5 to 5, as illustrated in Fig. 4 (a). The action range is between -1 and 1, allowing the agent to move according to the direction and twice the magnitude of the action. The objective is to reach position 0, which grants a reward of 100, while larger actions incur penalties given as \(-30 a^{2}\). Due to its discrete nature, we can compute the true optimal \(Q\)-values through value iteration , which is shown in the bottom row of Fig. 4 (a).

With this environment, we generated two datasets, medium and expert. The medium dataset consisted of actions varying within the range of \(\)0.5 perturbed from the optimal action determined by the true optimal \(Q\)-values, while the expert dataset consisted of actions varying within the range of \(\)0.05 perturbed from the optimal action. (Refer to Fig. 4 (b)) We then adopted a 3-layer MLP as the structure of \(Q_{}\) and performed regression to follow the true \(Q\)-value at each sample point (\(s,a\)) in the trajectories. Note that in-sample \(Q\)-learning can essentially be regarded as regression with the target value obtained from bootstrapping.

The learned \(Q_{}\) with the medium and expert datasets are shown in Fig. 4 (c). Indeed, the learned \(Q_{}\) with the expert dataset, containing nearly-optimal actions, shows that the value is flat over the entire action space for each state. This means that the nearly identical value of in-sample expert actions with a small spread is projected to the entire action space for each state. In contrast, the learned \(Q_{}\) with the medium dataset well estimates the true \(Q\)-function. This is because the medium dataset has diverse actions with diverse values for each state that facilitate the regression process. We additionally present the results from \(Q_{}\) obtained through IQL in Fig. 4 (d), which shows a similar trend to the results from regression.

Figure 4: (a) the view of the environment and true \(Q\) calculated through value iteration, (b) training datasets with color representing the true \(Q\) for each sample, (c) \(Q_{}\) learned through regression with a medium dataset (upper) and an expert dataset (bottom), (d) \(Q_{}\) learned through IQL with a medium dataset (upper) and an expert dataset (bottom).

Figure 5: We present the estimated \(Q_{}(s,)\) for \(\) and the normalized NTK \(k_{}(s,,s,a_{})/\|_{}Q_{}(s,a_{ })\|_{2}^{2}\) across four datasets with a 1D action space for Inverted Double Pendulum and a 3D action space for Hopper. In these figures, we fix the state \(s\) and the fixed reference action \(a_{}\) at zero (marked as \(\)), and sweep over all actions \(\). For Hopper, we use axes for action dimensions and color to represent \(Q\)-values in 3D plots. Additionally, in the NTK plot, we only include the high-NTK regions for values over 0.9. Refer to Appendix E for details.

The over-generalization tendency in \(Q_{}\) with optimal trajectories is not limited to the simple experiment above but also applies to complex RL tasks. We analyze how \(Q_{}(s,)\) varies over the action space in the Gym Inverted Double Pendulum  and MuJoCo Hopper environments [39; 8] trained on expert and medium-quality datasets with IQL. The details of the analysis are in Appendix E. As depicted in the upper row of Fig. 5 (a) and (b), and on the left side of the upper row of (c) and (d), the expert dataset shows concentrated action distribution, while the medium dataset has a broader spread, as expected. The concentration and similarity of true \(Q\)-values of the actions for a given state in the expert dataset cause over-generalization in \(Q_{}(s,)\), yielding constant flat values across the entire action space. This is further supported by the results in Appendix E.2, which visualize the weights of the learned \(Q\)-function.

For a deeper understanding of the over-generalization in \(Q_{}\), we analyze the gradient similarity, captured as the Neural Tangent Kernel (NTK), between an arbitrary action \(\) and the reference action \(a_{}\) for a given state \(s\). A higher NTK value signifies that the gradient update of \(Q_{}(s,a_{})\) has a substantial impact on \(Q_{}(s,)\). This indicates that even when \(a_{}\) and \(\) are dissimilar or unrelated actions, a high NTK value suggests that the \(Q_{}\) network is misjudging the relationship between these actions and over-generalizing. In Fig. 5, \(Q_{}\) trained with the expert dataset shows uniformly high normalized NTK values across actions, indicating that the gradient at one action equally affects all others. In contrast, \(Q_{}\) trained with the medium dataset shows NTK values that are higher near the reference action and decrease with action distance, reflecting more precise generalization. This analysis reveals that datasets consisting of optimal trajectories exhibit more aggressive generalization, which can negatively impact the accuracy of the learned \(Q\)-function in offline RL.

## 4 \(Q\)-Aided Conditional Supervised Learning

According to Section 3, RCSL faces challenges with suboptimal datasets, whereas \(Q_{}\) can effectively serve as a critic for stitching ability, favoring the use of \(Q\)-aid. In contrast, in an optimal dataset, \(Q_{}\) tends to over-generalize, leading to inaccuracies of learned \(Q_{}\), while RCSL excels by mimicking the optimal behavior without requiring external assistance. Recognizing this dynamic, it is crucial to integrate \(Q\)-aid into RCSL adaptively. The following subsections explain how to effectively adjust the level of \(Q\)-aid and facilitate the integration of the two methods, leading to the proposal of \(Q\)-Aided Conditional Supervised Learning (QCS).

### Controlling \(Q\)-Aid Based on Trajectory Returns

Given the complementary relationship, how can we adjust the degree of \(Q\)-aid? Since RCSL's preference for mimicking datasets and the \(Q\)-function's over-generalization issue is tied to trajectory optimality, we can apply varying degrees of \(Q\)-aid based on the trajectory return for each sub-trajectory in RCSL. Therefore, we set the degree of \(Q\)-aid, denoted as the QCS weight \(w(R())\) for a trajectory \(\), as a continuous, monotone-decreasing function of the return of \(\), \(R()\), such that

\[_{1},_{2},\ \ R(_{1})<R(_{2}) w(R(_{1}))  w(R(_{2})),\]

where continuity is imposed for gradual impact change. Among various choices, we find that simple options such as linear decay are sufficient to produce good results, i.e., \(w(R())=(R^{*}-R())\) with some \(>0\), where \(R^{*}\) represents the optimal return of the task. Practically, \(R^{*}\) can be obtained from an expert dataset or from the maximum value in the dataset. For details on how to calculate \(R^{*}\), please refer to Appendix F. Note that \(R()\) differs from RTG \(_{t}\) which is the sum of future rewards after timestep \(t\) and decreases as timestep \(t\) goes, thereby failing to represent the trajectory's optimality accurately.

### Integrating \(Q\)-Aid into the RCSL Loss Function

Instead of using the \(Q\)-function as the conditioning factor for RCSL as in previous works, we propose a more explicit approach by integrating \(Q\)-assistance into the loss function and dynamically adjusting the degree of assistance based on Section 4.1. As a result, the overall policy loss is given as follows:

\[_{}^{}()=_{}[ _{j=0}^{K-1}-_{}(_{t: t+j})\|_{2}^{2}}_{}--R( ))}_{}^{}(s_{t +j},_{}(_{t:t+j}))}_{Q}],\] (3)where \(Q_{}^{}(,)\) denotes the fixed \(Q\)-function pretrained with IQL. \(R()\) is the return of the entire trajectory \(\) containing the sub-trajectory \(_{tt+K-1}\). The overall input to the policy at time \(t\) is the sub-trajectory of context length \(K\) starting from time \(t\), \(_{tt+K-1}=(_{t},s_{t},a_{t},,_{t+K-1},s_{t+K-1} )\).

Our new loss function enables adaptive learning strategies depending on the trajectory's quality to which the subtrajectory belongs. For optimal trajectories, action selection follows RCSL. On the other hand, for suboptimal trajectories \(\) with \(R()<R^{*}\), the \(Q\)-aid term kicks in and its impact increases as \(R()\) decreases. We describe the details of the QCS weight \(w(R())\) and the policy update with the loss function in Appendix J.2 and our full algorithm's pseudocode in Appendix A.

### Implementation

Base Architecture.For implementing \(_{}\), a general RCSL policy can be used. When \(K=1\), meaning only the current time step is considered to estimate the action, we use an MLP network. When \(K 2\), we use a history-based policy network, such as DT  or DC .

Conditioning.We consider two conditioning approaches as proposed by RvS : one for tasks maximizing returns and the other for tasks aiming at reaching specific goals. For return-maximizing tasks, we employ RTG conditioning, and our algorithm is named QCS-R. For goal-reaching tasks, we additionally use subgoal conditioning, and our algorithm is named QCS-G. For subgoal selection, we randomly select a state that the agent will visit in the future. The ablations on conditioning are in Appendix H.2.

## 5 Related Work

Prompting RCSL with Dynamic Programming.Recent studies have recognized the limitations of RCSL in stitching abilities [27; 7; 58]. Our work contributes to the ongoing efforts to imbue RCSL with this capability. Notably, \(Q\)-learning Decision Transformer (QDT)  and Advantage Conditioned Transformer (ACT)  have proposed integrating dynamic programming into RCSL by modifying the RTG prompt to \(Q\)-value or advantage prompt. Our approach, QCS, parallels these efforts by leveraging dynamic programming for action guidance and trajectory stitching. However, unlike these methods, which implicitly incorporate dynamic programming through conditioning, QCS explicitly augments its loss function with the learned \(Q\)-function.

Incorporating RCSL with Stitching Ability.In a distinct vein, recently proposed Critic-Guided Decision Transformer (CGDT)  identifies the gap between target RTG and expected returns of actions as key to RCSL's limited stitching. To mitigate this, it adjusts DT's output with the critic network's Monte-Carlo return predictions and target RTG. In contrast, QCS uses \(Q\)-values learned through dynamic programming to guide actions, enhancing stitching ability explicitly. Another approach, the Elastic Decision Transformer (EDT) , recommends variable context lengths during inference, using longer contexts for optimal trajectories and shorter ones for sub-optimal trajectories to identify optimal paths better. QCS similarly adapts based on trajectory optimality but differentiates itself by modifying its learning approach during training, leveraging the complementary strengths of the \(Q\)-function and RCSL.

Furthermore, POR  integrates imitation learning techniques with stitching ability by generating high-value states using additional networks and value functions. These states are then used as conditions for predicting actions. Unlike QCS, which focuses on action stitching, POR emphasizes state stitching, allowing agents to choose actions that lead to high-value states, albeit with the need for additional networks. By concentrating on action stitching, QCS can avoid the computational demands associated with high-dimensional state prediction.

State-Adaptive Balance CoefficientRegarding the sub-trajectory-adaptive weight used in QCS, FamO2O  employs state-adaptive weight coefficients to balance policy improvement and constraints in the offline-to-online RL framework. Although FamO2O is an offline-to-online method that incorporates additional online samples, we provide a performance comparison with this work in Appendix G.2 to further demonstrate the effectiveness of QCS.

Experiments

In the experiment section, we conduct various experiments across different RL benchmarks to answer the following questions:

* How well does QCS perform in decision-making compared to prior SOTA methods across datasets of varying quality and tasks with diverse characteristics, especially those requiring stitching ability?
* To what extent does the dynamic nature of QCS weights, informed by trajectory return, contribute to effective decision-making, and how robust are these dynamic weights to hyperparameters?
* Can QCS effectively acquire stitching ability while preventing test-time distribution shift?

### Experimental Setup

Baseline Methods.To address a range of questions, we conduct a comprehensive benchmarking against 12 representative baselines that are state-of-the-art in each category. For the value-based category, we assess 4 methods: TD3+BC , IQL , and CQL , SQL . For RCSL, we assess 3 methods: DT , DC , RvS . Additionally, we evaluate 5 advanced RCSL methods proposed to integrate stitching capabilities: QDT , EDT , CGDT , ACT , and POR . For more details on the setup and the baselines, refer to Appendix B.

Benchmarks.We evaluated QCS against various baselines using datasets with diverse characteristics, including tasks focused on return maximization or goal-reaching, and those with dense or sparse rewards and varying sub-optimality levels.

Our primary focus was on the D4RL  MuJoCo, AntMaze, and Adroit domains. The MuJoCo domain [39; 8] features several continuous locomotion tasks with dense rewards. We conducted experiments in three environments: Halfcheetah, Hopper, and Walker2d, utilizing three distinct v2 datasets--medium, medium-replay, and medium-expert--each representing different levels of data quality. AntMaze is a domain featuring goal-reaching environments with sparse rewards, encompassing variously sized and shaped maps. It is an ideal testing bed for evaluating an agent's capability to stitch trajectories and perform long-range planning. We conduct experiments using six v2 datasets: umaze, umaze-diverse, medium-play, medium-diverse, large-play, and large-diverse, where umaze, medium, and large indicate map sizes, and play and diverse refer to data collection strategies. The Adroit domain  comprises various tasks designed to evaluate the effectiveness of algorithms in high-dimensional robotic manipulation tasks. In our experiments, we utilize the human and cloned datasets for the pen task.

Performance results for the MuJoCo and AntMaze domains are presented in Table 2 and Table 3, while results for the Adroit domain are included in Appendix G.

Hyperparameters and Backbone Architecture.We adopted two sets of hyperparameters per domain to determine the gradient of the monotonic decreasing function \(w(R())\). The detailed hyperparameters we used are provided in Appendix J, and the impact of \(\) is detailed in Appendix 6.3. Additionally, we implemented QCS based on DT, DC, and a simple MLP, and compared the performance of each. Detailed results for each architectural choice are provided in Appendix H.2. We observed that the DC-based approach performs best, although the performance gap is minor.

Evaluation Metric.In all evaluations of QCS, we assess the expert-normalized returns  of 10 episodes at each evaluation checkpoint (every \(10^{3}\) gradient steps). Subsequently, we compute the running average of these normalized returns over ten consecutive checkpoints. We report the mean and standard deviations of the final scores across five random seeds.

Figure 6: Views of tasks used in our experiments.

### Overall Performance

As shown in Table 2 and Table 3, QCS significantly outperforms prior value-based methods, RCSL, and combined methods across the datasets. Specifically, QCS outperforms both IQL and DC, upon which it is based, across all datasets, unlike the contradictory results between RCSL and the max-\(Q\) policy shown in Table 1. This empirically confirms that QCS successfully combines the strengths of both RCSL and the \(Q\)-function. A particularly remarkable achievement of QCS is its ability to substantially improve efficiency in goal-reaching tasks, AntMaze, especially in Large environments, where prior RCSL methods exhibited notably low performance. This enhancement is largely attributed to the stitching ability introduced by the \(Q\)-aid of QCS. These results underscore QCS's robustness and superiority in a wide array of offline RL contexts. The training curves for Tables 2 and 3 are shown in Appendix I, demonstrating stable learning curves across all datasets.

### Ablation Studies

To further analyze how each design element influences performance, we conducted additional experiments. More ablation studies are detailed in Appendix H, including the use of \(Q\)-function trained by CQL and the impact of base architecture and conditioning.

**The Importance of Weights Relative to Trajectory Return.**

To assess the impact of dynamically setting the QCS weight \(w(R())\) based on trajectory return, we compare our approach with a constant QCS weight, \(w(R())=c\). We test five constant weights \(c\{1,2.5,5,7.5,10\}\) and report the maximum score among these values in Table 4. The QCS method with the dynamic weight based on trajectory return outperforms the highest scores obtained with various constant weight settings across datasets, as shown in Table 4.

  Dataset & Constant & Dynamic \\ Weight & Weight \\  mujoco-m & 74.7 & **81.2** \\ mujoco-m-r & 75.4 & **82.9** \\ mujoco-m-e & 104.2 & **106.7** \\  

Table 4: Comparison of constant QCS weight and the dynamic weight.

   &  &  &  & Ours \\  Dataset & TD3+BC & IQL & CQL & SQL & DT & DC & RvS-R & QDT & EDT & CQDT & ACT & POR & QCS-R \\  halfcheetah-m & 48.3 & 47.4 & 44.0 & 48.3 & 42.6 & 43.0 & 41.6 & 42.3 & 42.5 & 43.0 & 49.1 & 48.8 & **59.0**\(\) 0.4 \\ hopper-m & 59.3 & 66.3 & 58.5 & 75.5 & 67.6 & 92.5 & 60.2 & 66.5 & 63.5 & **96.9** & 67.8 & 78.6 & **96.4**\(\) 3.7 \\ walker2d-m & 83.7 & 78.3 & 72.5 & 84.2 & 74.0 & 79.2 & 71.7 & 67.1 & 72.8 & 79.1 & 80.9 & 81.1 & **88.2**\(\) 1.1 \\  halfcheetah-m+r & 44.6 & 44.2 & 45.5 & 44.8 & 33.6 & 46.1 & 43.8 & 35.6 & 37.8 & 40.4 & 43.0 & 43.5 & **54.1**\(\) 0.8 \\ hopper-m-r & 60.9 & 94.7 & 95.0 & 99.7 & 82.7 & 94.2 & 73.5 & 52.1 & 89.0 & 93.4 & 98.4 & 98.9 & **100.4**\(\) 1.1 \\ walker2d-m-r & 81.8 & 73.9 & 77.2 & 81.2 & 66.6 & 76.6 & 60.6 & 58.2 & 74.8 & 78.1 & 56.1 & 76.6 & **94.1**\(\) 2.0 \\  halfcheetah-m-e & 90.7 & 86.7 & 91.6 & 94.0 & 86.8 & 93.0 & 92.2 & - & - & 93.6 & **96.1** & 94.7 & 93.3 \(\) 1.8 \\ hopper-m-e & 98.0 & 91.5 & 105.4 & **111.8** & 107.6 & 110.4 & 101.7 & - & - & 107.6 & 111.5 & 90.0 & 110.2 \(\) 2.4 \\ walker2d-m-e & 110.1 & 109.6 & 108.8 & 110.0 & 108.1 & 109.6 & 106.0 & - & - & 109.3 & 113.3 & 109.1 & **116.6**\(\) 2.4 \\  average & 75.3 & 77.0 & 77.6 & 83.1 & 74.7 & 82.2 & 71.7 & - & - & 82.4 & 79.6 & 80.1 & **90.3** \\  

Table 2: Performance of QCS and baselines in the MuJoCo domain. The dataset names are abbreviated as follows: medium to ‘m’, medium-replay to ‘m-r’, medium-expert to ‘m-e’. The boldface numbers denote the maximum score or comparable one among the algorithms.

   &  &  &  & Ours \\  Dataset & TD3+BC & IQL & CQL & SQL & DT & DC & RvS-R & RvS-G & POR & QCS-R & QCS-G \\  antmaze-u & 78.6 & 87.5 & 74.0 & **92.2** & 65.6 & 85.0 & 64.4 & 65.4 & 90.6 & **92.7**\(\) 3.9 & **92.5**\(\) 4.6 \\ antmaze-u-d & 71.4 & 62.2 & **84.0** & 74.0 & 51.2 & 78.5 & 70.1 & 60.9 & 71.3 & 72.3 \(\) 12.4 & 82.5 \(\) 8.2 \\ antmaze-m-p & 10.6 & 71.2 & 61.2 & 80.2 & 4.3 & 33.2 & 4.5 & 58.1 & **84.6** & 81.6 \(\) 6.9 & **84.8**\(\) 11.5 \\ antmaze-m-d & 3.0 & 70.0 & 53.7 & **79.1** & 1.2 & 27.5 & 7.7 & 67.3 & **79.2** & **79.5**\(\) 5.8 & 75.2 \(\) 11.9 \\ antmaze-1-p & 0.2 & 39.6 & 15.8 & 53.2 & 0.0 & 4.8 & 3.5 & 32.4 & 58.0 & 68.7 \(\) 7.8 & **70.0**\(\) 9.6 \\ antmaze-1-d & 0.0 & 47.5 & 14.9 & 52.3 & 0.5 & 12.3 & 3.7 & 36.9 & 73.4 & 70.6 \(\) 5.6 & **77.3**\(\) 11.2 \\  average & 27.3 & 63.0 & 50.6 & 71.8 & 20.5 & 40.2 & 25.6 & 53.5 & 76.2 & 77.6 & **80.4** \\  

Table 3: Performance of QCS and baselines in the AntMaze domain. The dataset names are abbreviated as follows: unaze to ‘u’, unaze-diverse to ‘u-d’, medium-play to ‘m-p’, medium-diverse to ‘m-d’, large-play to ‘l-p’, and large-diverse to ‘l-d’. The boldface numbers denote the maximum score or comparable one among the algorithms.

This demonstrates that our dynamic weight control, grounded in trajectory return, is more effective and robust in integrating \(Q\)-aids.

Impact of the QCS weight \(\).We examined the effect of \(\) by varying it from 0.2 to 1.5. As shown in Table 5, except for the walker2d-medium, we found that even the smallest values achieved with changing \(\) either matched or surpassed the performance of existing value-based methods and RCSL's representative methods, including IQL, CQL, DT, DC, and RvS. This demonstrates QCS's relative robustness regarding hyperparameters. For walker2d-medium, we found that performance begins to decrease when \(\) exceeds the initial setting of 0.5. While increasing the gradient steps from 500K to 1M improves performance at \(=1\), further increasing \(\) to 1.5 leads to greater instability.

Test Time State Distribution Shift.To validate whether QCS effectively acquires stitching ability while preventing a shift in the test-time state distribution, as discussed in Section 3.2, we present Fig. 7. This figure compares the state distributions explored by RCSL, max-\(Q\), and QCS policies during evaluation. RCSL and max-\(Q\), representing QCS's extremes, were trained using specific loss configurations: RCSL loss as QCS loss in Eq. 3 with \(=0\) and max-\(Q\) loss as QCS loss without the RCSL term, i.e., selecting actions as \(*{argmax}_{a}Q_{}^{}(s,a)\). Fig. 7 illustrates RCSL's adherence to dataset states, contrasting with the notable state distribution shift of the max-\(Q\) policy. QCS inherits RCSL's stability but surpasses its performance, indicating an effective blend of transition recombination without straying excessively from the state distribution.

## 7 Conclusion

In conclusion, QCS effectively combines the stability of RCSL with the stitching ability of the \(Q\)-function. Anchored by thorough observation of \(Q\)-function generalization error, QCS adeptly modulates the extent of \(Q\)-assistance. This strategic fusion enables QCS to exceed the performance of existing SOTA methods in both efficacy and stability, particularly in complex offline RL benchmarks encompassing a wide range of optimality.

In addressing our initial motivating question on integrating RCSL and \(Q\)-function, QCS opens up promising future research directions. While we have established a correlation between trajectory return and the mixing weight, we have considered simple linear weights to control the level of \(Q\)-aid. It is also plausible that the mixing weight might be influenced by other dataset characteristics, such as the dimensions of the state and actions. We believe QCS will stand as a motivating work, inspiring new advancements in the field.

Figure 7: t-SNE  analysis of states visited by policies trained with RCSL, max-\(Q\)\((*{argmax}_{a}Q_{}^{}(s,a))\), and QCS losses during evaluation, alongside dataset’s states in walker2d-medium.

   & \(=0.2\) & \(=0.5\) & \(=1\) & \(=1.5\) \\  halfcheetah-medium & 53.7 \(\) 0.4 & 57.7 \(\) 0.3 & **59.0 \(\) 0.4** & **59.0 \(\) 0.2** \\ hopper-medium & 89.4 \(\) 5.6 & **96.4 \(\) 3.7** & **95.7 \(\) 3.5** & 88.8 \(\) 6.2 \\ walker2d-medium & 83.9 \(\) 4.7 & **88.2 \(\) 1.1** & 75.5\(\)7.1 (500K) / **87.6\(\)3.9 (1M) & 60.7 \(\) 11.2 \\ halfcheetah-medium-replay & 52.0 \(\) 0.8 & 52.8 \(\) 0.5 & **54.1 \(\) 0.8** & **54.2 \(\) 0.6** \\ hopper-medium-replay & 98.5 \(\) 2.4 & **100.4 \(\) 1.1 & 99.4 \(\) 2.1 & **100.5 \(\) 0.7** \\ walker2d-medium-replay & 83.3 \(\) 5.7 & 93.2 \(\) 2.5 & **94.1 \(\) 2.0** & 92.3 \(\) 3.7 \\  

Table 5: Performance of QCS in the Mujoco domain with varying \(\) values. The boldface numbers denote the maximum score or a comparable one.

#### Acknowledgments

This work was supported in part by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00469, Development of Core Technologies for Task-oriented Reinforcement Learning for Commercialization of Autonomous Drones, 50%) and in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2021R1A2C2009143 Information Theory-Based Reinforcement Learning for Generalized Environments, 50%).