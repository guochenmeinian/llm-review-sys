# Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification

Haolin Liu

University of Virginia

srs@rh@virginia.edu

&Artin Tajdini

University of Washington

artin@cs.washington.edu

&Andrew Wagenmaker

University of California, Berkeley

ajwagen@berkeley.edu

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

Authors are listed in alphabetical order by last name.

###### Abstract

In linear bandits, how can a learner effectively learn when facing corrupted rewards? While significant work has explored this question, a holistic understanding across different adversarial models and corruption measures is lacking, as is a full characterization of the minimax regret bounds. In this work, we compare two types of corruptions commonly considered: _strong corruption_, where the corruption level depends on the learner's chosen action, and _weak corruption_, where the corruption level does not depend on the learner's chosen action. We provide a unified framework to analyze these corruptions. For stochastic linear bandits, we fully characterize the gap between the minimax regret under strong and weak corruptions. We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level.

Next, we reveal a connection between corruption-robust learning and learning with _gap-dependent misspecification_--a setting first studied by Liu et al. (2023), where the misspecification level of an action or policy is proportional to its suboptimality. We present a general reduction that enables any corruption-robust algorithm to handle gap-dependent misspecification. This allows us to recover the results of Liu et al. (2023) in a black-box manner and significantly generalize them to settings like linear MDPs, yielding the first results for gap-dependent misspecification in reinforcement learning. However, this general reduction does not attain the optimal rate for gap-dependent misspecification. Motivated by this, we develop a specialized algorithm that achieves optimal bounds for gap-dependent misspecification in linear bandits, thus answering an open question posed by Liu et al. (2023).

## 1 Introduction

The real world is rarely truly stochastic--in practice, our observations are often corrupted--and furthermore, rarely are the modeling assumption typically made in theory--that the true data-generating process lives in our model class--met in reality. Therefore, robustly handling these deviations from idealized assumptions is crucial. These challenges are particularly pronounced in interactive decision-making settings, where deviations from idealized assumptions could lead an algorithm to take unsafe or severely suboptimal actions. In this work, we seek to address these challenges, and develop a unified understanding for robust learning in corruption-robust and misspecified settings.

We first consider the corruption-robust learning setting. Robust learning in the presence of corruptions requires designing algorithms whose guarantee have a tight scaling in the corruption level. That is, although some amount of suboptimality is inevitable if our observations are corrupted, we would hope to obtain the minimum amount of suboptimality possible at a given corruption level. While much work has been done on learning with corrupted observations, existing work has failed to yield a tight characterization of this scaling in the corruption level, even in simple settings such as linear bandits. We address this shortcoming, and develop an algorithm which achieves the optimal scaling in the corruption level, and further extend this to a novel corrupted adversarial linear bandit setting, where in addition to corrupted observations, the rewards themselves may be adversarially chosen from round to round. We obtain the first provably efficient bounds in this setting.

Model misspecification, another extensively studied problem in the literature, can be thought of as a form of corruption, where the corruption level is the amount of misspecification between the "closest" model in the model class and the true environment. Standard discussions on misspecification usually assume that the misspecification for every action has a uniform upper bound, and the final regret guarantee scales linearly with the amount of misspecification. The work of Liu et al. (2023) initiated the study on the _gap-dependent misspecification_ setting, where the misspecification level for a given action scales with the suboptimality of that action. They demonstrated that the linear scaling in regret is not necessary in this case. We revisit this problem, and show a general reduction from the gap-dependent misspecified setting to the corruption setting. We utilize this reduction to show that settings previously not known to be learnable--for example, linear MDPs with policy gap-dependent misspecification--are in fact efficiently learnable with existing corruption robust algorithms.

Together, our results present a unified picture of optimally learning in the presence of observation corruption, and (certain types of) model misspecification. We summarize our contributions as follows (see Section 2 and Section 3 for formal definitions of the mentioned quantities):

1. In Section 4, we develop a stochastic linear bandit algorithm with \(}(d+\{dC,C_{}\})\) regret, where \(d\) is the feature dimension, \(T\) is the number of rounds, \(C\) is the strong corruption measure, and \(C_{}\) is the weak corruption measure. These bounds are unimprovable.
2. In Section 5, we initiate the study of adversarial linear bandits with corruptions. We obtain \(}(d+C_{})\) and \(}(T}+dC)\) regret for weak and strong corruptions, respectively.
3. We prove a general reduction that efficiently handles gap-dependent misspecification with corruption-robust algorithms. We apply our reduction to show that linear MDPs with gap-dependent misspecification are efficiently learnable (Section 6).
4. Finally, while the reduction in item 3 is general, it is unable to obtain the tightest possible rate for gap-dependent misspecification. We thus develop a specialized algorithm which, in the linear bandit setting, obtains the optimal rate. This resolves the open problem of Liu et al. (2023).

In Section 2 we present our problem setting, and in Section 3, compare the corruption notions in previous and our work. More related works are discussed in Appendix A. In Section 4-Section 6, we present our main results as outlined above.

## 2 Problem Setting and Preliminaries

We consider the corrupted linear bandit problem. The learner interacts with the environment for \(T\) rounds. The learner is given an action set \(^{d}\). At the beginning of round \(t\), the environment determines a reward vector \(_{t}^{d}\) and a corruption function \(_{t}():[-1,1]\), which are both hidden from the learner. The learner then selects an action \(a_{t}\). Then a reward value \(r_{t}=a_{t}^{}_{t}+_{t}(a_{t})+_{t}\) is revealed to the learner, for some zero-mean noise \(_{t}[-1,1]\)2. We assume that \(\|a\|_{2} 1\), \(\|_{t}\|_{2}\), and \(a^{}_{t}[-1,1]\) for any \(a\) and any \(t=1,2,,T\). We define \(_{t}=_{a}|_{t}(a)|\).

In the stochastic setting, the environment is restricted to choose \(_{t}=^{}\) for all \(t\), while in the adversarial setting, \(_{t}\) can arbitrarily depend on the history up to round \(t-1\). The regret of the learner is defined as

\[_{T}=_{u}_{t=1}^{T}u^{}_{t}-_{t =1}^{T}a_{t}^{}_{t}.\]Note that although the non-stationarity of \(_{t}\) in the adversarial setting captures a certain degree of corruption, this form of corruption is limited to a linear form \(a^{}(_{t}-^{})\), which is not as general as \(_{t}(a)\) that could be an arbitrarily function. Therefore, the corrupted linear bandit problem cannot be reduced to an adversarial linear bandit problem.

Notation.We denote \([n]=\{1,2,,n\}\). Let \(()\) be the set of distribution over \(\). For any \(p()\), define the lifted covariance matrix \(}(p)=_{a p}aa^{}&a\\ a^{}&1^{(d+1)(d+1)}\). For \(A,B^{d d}\), define \( A,B=(AB^{})\). \(_{t}[]\) is the expectation conditioned on history up to \(t-1\).

G-Optimal Design.A G-optimal design over \(\) is a distribution \(()\) such that \(\|a\|_{G^{-1}}^{2} d\) for all \(a\), where \(G=_{a}(a)aa^{}\). Note that such a distribution is guaranteed to exist, and can be efficiently computed (Pukelsheim, 2006; Lattimore and Szepesvari, 2020).

## 3 Two Equivalent Views: On Adversary Adaptivity and Corruption Measure

Previous works have studied corruption with various assumptions on the adaptivity of the adversary and different measures for the corruption level. In this work, we consider both the _strong_ and _weak_ guarantees, which can cover different notions of corruptions studied in previous works. We provide two different viewpoints to understand them. In the first viewpoint, the weak and strong guarantee differ by the _adaptivity_ of the adversary, while in the second viewpoint, the two guarantees differ in the _measure of corruption_. Then we argue that the two viewpoints are equivalent.

Adversary Adaptivity (AA) Viewpoint.In this viewpoint, the corruption is specified only for the _chosen_ action. That is, in each round \(t\), the adversary only decides a single corruption level \(_{t}_{ 0}\) and ensures \(|[r_{t}]- a_{t},^{}\,|_{t}\). We consider two kinds of adversary: strong adversary who decides \(_{t}\)_after_ seeing the chosen action \(a_{t}\), and weak adversary who decides \(_{t}\)_before_ seeing \(a_{t}\). The robustness of the algorithm is measured by how the regret depends on \(_{t=1}^{T}_{t}\).

Corruption Measure (CM) Viewpoint.In this viewpoint, the corruption is individually specified for _every_ action. That is, at each round \(t\), the adversary decides \(_{t}(a)\) for all action \(a\) and ensures \([r_{t}|a_{t}=a]- a,^{}=_{t}(a)\) for all \(a\). The adversary always decides \(_{t}()\)_before_ seeing \(a_{t}\). To evaluate the performance, we consider two different measures of the total corruption: the strong measure \(_{t=1}^{T}|_{t}(a_{t})|\) and the weak measure \(_{t=1}^{T}_{a}|_{t}(a)|\).

We argue that the two viewpoints are equivalent in the sense that the performance guarantee of an algorithm under strong/weak adversary in the AA viewpoint are the same as those under strong/weak measure in the CM viewpoint, respectively. This is by the following observation. A strong adversary in the AA viewpoint who decides the corruption level \(_{t}\) after seeing \(a_{t}\) can be viewed as deciding the corruption \(_{t}(a)\) for all action \(a\)_before_ seeing \(a_{t}\), and set \(_{t}=|_{t}(a_{t})|\) after seeing \(a_{t}\). In other words, \(_{t}(a)\) is the corruption _planned_ (before seeing \(a_{t}\)) by a strong adversary assuming \(a_{t}=a\), and the adversary simply carries out its plan after seeing \(a_{t}\). It is clear that this is equivalent to the CM viewpoint with \(_{t=1}^{T}|_{t}(a_{t})|\) as the corruption measure. See Appendix B for more details. On the other hand, a weak adversary in the AA viewpoint has to decide an upper bound of the corruption level \(_{t}\) no matter which action \(a_{t}\) is chosen by the learner. This can be viewed as deciding the corruption \(_{t}(a)\) for every action \(a\) before seeing \(a_{t}\) with the restriction \(|_{t}(a)|_{t}\) for all \(a\). Therefore, this is equivalent to using \(_{t=1}^{T}_{a}|_{t}(a)|\) to measure total corruption in the CM viewpoint.

In this work, we adopt the CM viewpoint as described in Section2. With the CM viewpoint, for both strong and weak settings, the power of the adversary remains the same as the standard "adaptive adversary" (i.e., deciding the corruption function \(_{t}()\) based on the history up to time \(t-1\)), and we only need to derive regret bounds with different corruption measures. All our results can also be interpreted in the AA viewpoint, as the above argument suggests.

With this unified viewpoint, we categorize in Table1 previous works on linear (contextual) bandits based on the corruption measure, all under the same type of adversary. According to the definitions in Table1, \(C\) and \(C_{}\) correspond to the strong measure and weak measure mentioned above, respectively. It is easy to see that \(C\{C_{},C_{}\} C_{,} C_{}\), where \(C_{}\) and \(C_{}\) are incomparable.

For stochastic linear bandits, considering the relations among different corruption measures, the Pareto frontiers of the existing upper bounds are \(}(d+C_{})\) by Foster et al. (2020) and Takemura et al. (2021), and \(}(d+dC)\) by He et al. (2022). The lower bound frontiers are \((d+C_{})\) by Lattimore et al. (2020) and \((d+dC)\) by Bogunovic et al. (2020). These results imply an \(}(d+dC_{})\) upper bound and an \((d+C_{})\) lower bound, which still have a gap. In this work, we close the gap by showing an \(}(d+C_{})\) upper bound.

For adversarial linear bandits, we are only aware of upper bound \(}(d+C_{,})\) by Liu et al. (2024), and not aware of any upper bounds related to \(C_{}\) or \(C\). In this work, we show \(}(d+C_{})\) and \(}(T}+dC)\) upper bounds. The results are summarized in Table 2. As in most previous work, we assume that \(C_{}\) and \(C\) (or their upper bounds) are known by the learner when developing the algorithms. The case of unknown \(C_{}\) or \(C\) is discussed in Appendix C.

We emphasize that before our work, for both stochastic and adversarial linear bandits, it was unknown how to achieve \(}(d+C_{})\) regret. To see how \(C_{}\) is different from other notions such as \(C_{}\) and \(C_{}\), we observe that for stochastic linear bandits, while \(}(d+C_{})\) can be achieved via deterministic algorithms, it is not the case for \(}(d+C_{})\). The reason is that for deterministic algorithms, the adversary can control \(C_{}\) to be the same as \(C\), for which \((d+dC)\) is unavoidable. We formalize this in Proposition 1, with the proof given in Appendix D. This precludes the possibility of many previous algorithms to actually achieve the \(}(d+C_{})\) upper bound, e.g., Lattimore et al. (2020), Takemura et al. (2021), Bogunovic et al. (2020, 2021), He et al. (2022).

**Proposition 1**.: _For stochastic linear bandits, there exists a deterministic algorithm achieving \(_{T}=}(d+C_{})\), while any deterministic algorithm must suffer \(_{T}=(d+dC_{})\)._

  Measure & Definition & Work \\  \(C_{}\) & \(T_{t,a}|_{t}(a)|\) & Lattimore et al. (2020), Neu and Olkhovskaya (2020) \\  \(C_{,}\) & \((T_{t=1}^{T}_{a}_{t}(a)^{2})^{1/2}\) & Liu et al. (2024) \\  \(C_{}\) & \((T_{t=1}^{T}_{t}(a_{t})^{2})^{1/2}\) & Foster et al. (2020), Takemura et al. (2021) \\  \(C_{}\) & \(_{t=1}^{T}_{a}|_{t}(a)|\) & Li et al. (2019), Bogunovic et al. (2020) \\  \(C\) & \(_{t=1}^{T}|_{t}(a_{t})|\) & Bogunovic et al. (2021, 2022), He et al. (2022) \\  

Table 1: Classification of previous works based on the corruption measure. Foster et al. (2020), Takemura et al. (2021), and He et al. (2022) studied the more general linear _contextual_ bandit setting where the action set can be chosen by an adaptive adversary in every round. Foster et al. (2020) and Takemura et al. (2021) reported their bounds in \(C_{,}\) and \(C_{}\), respectively, though one can make minor modifications to their analysis and show that their algorithms actually ensure the \(C_{}\) bound.

   & \) bound} &  \\   & Stochastic LB & \(d+C_{}\) & \(d+dC\) \\  & & (Algorithm 1) & (He et al., 2022) \\   & Adversarial LB & \(d+C_{}\) \\ (Algorithm 2) \\  & \(T}+dC\\ (Algorithm 3) \\ \) \\   & \(d+C_{}\) \\ (Lattimore et al., 2020) \\ \) & \(d+dC\) \\  

Table 2: Regret bounds under corruption measure \(C\) and \(C_{}\). See Table 1 for their definitions. He et al. (2022) studied the more general linear contextual bandits setting, though it also gives the state-of-the-art \(C\) bound for linear bandits.

```
1Input:\(Z=}\) or \(dC\), action space \(^{d}\), confidence level \(\).
2 Let \(_{1}=\) and \(L=d(||T/)\).
3for\(k=1,2,\)do
4 Compute a G-optimal design (defined in Section 2) \(p_{k}\) over \(_{k}\), and let \(G_{k}=_{a}p_{k}(a)aa^{}\). Define \(_{k}=[(2^{k-1}-1)L+1,(2^{k}-1)L]\) and \(m_{k}=|_{k}|=2^{k-1}L\).
5for\(t_{k}\)do Draw \(a_{t} p_{k}\) and receive \(r_{t}\) where \([r_{t}]=a_{t}^{}^{}+_{t}(a_{t})\).
6 Define reward vector estimator \(_{k}=(m_{k}G_{k})^{-1}_{t_{k}}a_{t}r_{t}\) and active action set: \[_{k+1}=a_{k}:\ \ _{b_{k}}b^{} _{k}-a^{}_{k} 8|T/)}{m_{k}}}+}}.\] (1) ```

**Algorithm 1**Randomized Phased Elimination (for stochastic \(C_{}\) and \(C\) bounds)

## 4 Stochastic Linear Bandits

In this section, we introduce Algorithm 1, which achieves optimal regret for both \(C\) and \(C_{}\).

Algorithm 1 is an elimination-based algorithm. At each epoch \(k\), it samples actions from a fixed distribution \(p_{k}(_{k})\), which is a G-optimal design over the active action set \(_{k}\) (Line 4). At the end of epoch \(k\), only actions that are within the error threshold will be kept in the active action set of the next epoch (Eq. (1)). While previous works by Lattimore et al. (2020) and Bogunovic et al. (2021) have used a similar elimination framework to obtain \(}(d+C_{})\) and \(}(d+d^{}C)\) bounds, respectively, we note that their algorithms only specify the number of times the learner should sample for each action in each epoch. This is different from our algorithm that requires the learner to exactly use the distribution \(p_{k}\) to sample actions in every round in epoch \(k\). As argued in Proposition 1, if their algorithms are instantiated as a deterministic algorithm, then the regret will be at least \((d+dC_{})\). Thus, this subtle difference is important.

Note that to achieve the tight \(C_{}\) (or \(C\)) bound, \(Z=C_{}\) (or \(Z=dC\)) has to be input to the algorithm to decide the error threshold. The guarantee of Algorithm 1 is stated in Theorem 4.1.

**Theorem 4.1**.: _With input \(Z=C_{}\) or \(Z=dC\), Algorithm 1 ensures with probability at least \(1-\) that \(_{T}(d+Z T)\)._

Algorithm 1 can also be shown to ensure that \(_{T}(|T/)}+Z  T)\), which could be smaller than the bound given in Theorem 4.1 when \(||\) is small.

## 5 Adversarial Linear Bandits

In this section, we consider corrupted adversarial linear bandits. Although adversarial linear bandits have been widely studied, robustness under corruption is an under-explored topic: there is no prior work obtaining regret bounds that linearly depends on either \(C_{}\) or \(C\).

### \(C_{}\) bound in Adversarial Linear Bandits

Our algorithm (Algorithm 2) is based on follow-the-regularized-leader (FTRL) with logdet regularizer. Similar to previous works (Foster et al., 2020; Zimmer and Lattimore, 2022; Liu et al., 2024, 2023b) that utilize logdet regularizer, the feasible set \(\) is in \(^{(d+1)(d+1)}\), which is the space of the covariance matrix for distributions over the lifted action space (Line 2-Line 3). At round \(t\), the algorithm obtains a covariance matrix \(_{t}\) by solving the FTRL objective (Eq. (2)). The action distribution \(p_{t}\) is such that the induced covariance matrix is equal to \(_{t}\) (Eq. (3)). After sampling \(a_{t} p_{t}\) and obtaining the reward \(r_{t}\), the algorithm constructs reward vector estimator \(_{t}\) (Line 8) and feeds it to FTRL. The reader may refer to Zimmer and Lattimore (2022) for more details.

In typical corruption-free adversarial linear bandits, the learner would construct an unbiased reward vector estimator. However, in the presence of corruption, the learner can no longer construct an unbiased estimator. To compensate the bias, we adopt the idea of "adding exploration bonus" inspired by previous work on high-probability adversarial linear bandits (Lee et al., 2020; Zimmert and Lattimore, 2022). In the regret analysis, the exploration bonus creates a negative term that cancels the bias of the loss estimator. The bonus is represented by the \(B_{t}\) in Eq. (5).

To decide the form of \(B_{t}\), we first analyze the bias. With the standard construction of the reward estimator, the bias on the benchmark action \(u\) can be calculated as (with \(_{t}:=_{a}|_{t}(a)|\))

\[u^{}(_{t}[_{t}^{-1}a_{t}r_{t}]- _{t})=u^{}_{t}[_{t}^{-1}a_{t} _{t}(a_{t})]_{t}_{t}^{-1} _{t}[a_{t}a_{t}^{}]_{t}^{-1}u}=_{t}\|u\|_{_{ t}^{-1}},\] (6)

where \(_{t}\) is the feature covariance matrix induced by \(p_{t}\) (defined in Eq. (4)). Below, we compare different bonus designs in previous and our work.

Bonus design in previous work.In Zimmert and Lattimore (2022), which is also based on logdet-FTRL but where the goal is only to get a high-probability bound, the bonus introduces an additional regret the form \(-\|u\|_{_{t}^{-1}}^{2}+_{a}p_{t}(a)\|a\|_{_{t}^{- 1}}^{2}\). This can be used to cancel off the bias in Eq. (6):

\[_{t=1}^{T}_{t}\|u\|_{_{t}^{-1}}-_{t=1 }^{T}\|u\|_{_{t}^{-1}}^{2}+_{t=1}^{T}_{a}p_ {t}(a)\|a\|_{_{t}^{-1}}^{2}_{t=1}^{T}^{2}}{ }+ dT,\] (7)

where we use AM-GM. Unfortunately, with the optimal \(\), this only leads to an additive regret \(_{t}^{2}}=C_{,}>C_{}\), which does not meet our goal.

Figure 1: The bonus function and its illustration

Bonus design in our work.To obtain the tighter \(C_{}=_{t}_{t}\) bound, our idea is to construct a positive-definite matrix \(B_{t}\) such that \(B_{t}_{}^{-1}\) for all \([t]\), and add bonus \(B_{t}-B_{t-1}\) at round \(t\). This way, the total negative regret on \(u\) becomes \(-\|u\|_{B_{T}}^{2}\) and the cancellation becomes

\[_{t=1}^{T}_{t}\|u\|_{_{t}^{-1}}-\|u\|_{B_{T}}^{2}+ _{t=1}^{T}_{a}p_{t}(a)\|a\|_{B_{t}-B_{t-1}}^{2} _{t=1}^{T}_{t}^{2}}{}+_{t =1}^{T}_{t},B_{t}-B_{t-1},\] (8)

where we use \(B_{T}_{t}^{-1}\) for all \(t\) and AM-GM. With this, it suffices to find \(B_{t}\) satisfying our condition \(B_{t}_{}^{-1}\) for \( t\), and bound the overhead \(_{t=1}^{T}_{t},B_{t}-B_{t-1}\) by \(}(d)\).

It turns out that there exists a way to inductively construct \(B_{t}\) so that \(B_{t}_{}^{-1}\) for all \( t\) and \(_{t=1}^{T}_{t},B_{t}-B_{t-1} (B_{T})=}(d)\). This is by letting \(B_{t}\) to be a minimal matrix such that \(B_{t} B_{t-1}\) and \(B_{t}_{t}^{-1}\). By induction, this ensures \(B_{t}_{}^{-1}\) for all \( t\). The function \(B_{t}=(B_{t-1},_{t})\) is formally defined in Figure1. The geometric interpretation is finding the minimal ellipsoid that contains both ellipsoids induced by \(B_{t-1}\) and \(_{t}^{-1}\). An illustration figure is given in Figure1.

We adopt the fixed-point formulation in Zimmer and Lattimore (2022) (see their FTRL-FB) that includes the bonus for round \(t\) (i.e., \(B_{t}\)) in the FTRL objective when calculating the policy at round \(t\) (Eq.2). Notice that \(B_{t}\), in turn, depends on the policy at round \(t\) (Eq.5), where \(_{t}\) depends on \(p_{t}\)), and thus this forms a fixed-point problem. In the regret analysis, this avoids the "stability term" of the bonus to appear in the regret bound. While the fixed-point solution always exists, it may not be computationally efficient to find. For completeness, in Algorithm4 (AppendixF), we present a version that does not require solving fixed point but has a suboptimal \(d}\) additive regret. The guarantee of Algorithm2 is stated in Theorem5.1, with its proof deferred to AppendixF.

**Theorem 5.1**.: _Algorithm2 ensures with probability of \(1-\), \(_{T}=}(d+C_{})\), where \(}()\) hides \((T/)\) factors._

### \(C\) bound in Adversarial Linear Bandits

To see how to obtain a \(C\) bound, we perform the bias analysis again. Similar but slightly different from Eq.6, with the standard loss estimator, the bias on action \(u\)'s reward is bounded by

\[u^{}(_{t}[_{t}^{-1}a_{t}r_{t}]-_{t} )=u^{}_{t}[_{t}^{-1}a_{t}_{t }(a_{t})]\|u\|_{_{t}^{-1}}_{t}[\|a_{t}\|_{ _{t}^{-1}}|_{t}(a_{t})]].\] (9)

Unlike in Eq.6, we do not relax \(|_{t}(a_{t})|\) to \(_{t}=_{a}|_{t}(a)|\) because we want the final bound to depend on \(C=_{t}|_{t}(a_{t})|\). The idea to ensure that the sum of Eq.9 over \(t\) can be related to \(C\) is to make \(\|a_{t}\|_{_{t}^{-1}}\) bounded by a constant \((d)\), which allows us to further bound Eq.9 by \((d)\|u\|_{_{t}^{-1}}|_{t}(a_{t})|\). Such a property holds in standard linear bandit algorithms that operate in the continuous action space where \(a_{t}\) is a point in the convex hull of \(\), and utilize a more concentrated action sampling scheme. Algorithms that are of this type include SCRiBLe (Abernethy et al., 2008) and continuous exponential weights (CEW) (Ito et al., 2020).

For SCRiBLe and CEW, the work by Lee et al. (2020) and Zimmer and Lattimore (2022) developed techniques that incorporate bonus terms to get high probability regret bounds. The bonus terms introduced by Zimmer and Lattimore (2022) is similar to that discussed in Eq.7, which only allows us to get a \(C_{}\) bound. The bonus terms introduced by Lee et al. (2020) allows us to obtain a \(C\) bound, but the overhead introduced by the bonus terms is much larger, resulting in a highly sub-optimal regret bound. Indeed, as shown in AppendixJ, adopting their bonus construction results in an additional regret of \(d^{}C\). With several attempts, we are only able to obtain the tight corruption dependency \(dC\) using the bonus in Section5.1. To use that bonus, however, it is necessary to lift the problem to \((d+1)^{2}\)-dimensional space. Unfortunately, existing SCRiBLe and CEW algorithms only operate in the original \(d\)-dimensional space, and as discussed above, we need them to ensure \(\|a_{t}\|_{_{t}^{-1}}(d)\).

In order to combine these two useful ideas (i.e., our bonus design in Section5.1, and the concentrated sampling scheme by SCRiBLe or CEW), we end up with the algorithm that runs CEW over the lifted action space (Algorithm3). In order to simplify the exposition, we assume without loss of generality that \(=()\). The lifted action space is \(=\{}(p):p()\}^ {(d+1)(d+1)}\). The price of the lifting is that the "regularization penalty term" in the regret analysis now grows from \(}(d/)\) to \(}(d^{2}/)\), which gives us the \(T}\) sub-optimal regret.

Note that CEW requires the assumption that the feasible set is a convex body with non-zero volume, but the effective dimension of \(\) is strictly smaller than \((d+1)^{2}\) and thus have zero volume in \(^{(d+1)^{2}}\). To correctly write the algorithm, we introduce an invertible linear transformation \(:^{(d+1)^{2}}^{m}\) that maps an \((d+1)^{2}\)-dimensional action set \(\) to an \(m\)-dimensional one, where \(m\) is the effective dimension of \(\). In Appendix I, we formally define this \(\). The algorithm uses \(\) to map all lifted actions and reward estimators from \(^{(d+1)(d+1)}\) to \(^{m}\).

The exponential weights runs over the space of \(()\) (see Eq. (10)). A point \(h()\) sampled from the exponential weights can be linearly mapped to an action \(a\) according to Eq. (11). We use \(q^{}_{t}\) to denote the exponential weight distribution in \(()\), and use \(p_{t}\) to denote the corresponding distribution in \(\). Instead of sampling \(a_{t}\) from \(p_{t}\), we sample it through rejection sampling that rejects samples with \(\|a_{t}\|_{_{t}^{-1}}>()\) (Eq. (12)). This technique was developed by Ito et al. (2020), and this guarantees \(\|a_{t}\|_{_{t}^{-1}}}()\)--which is our goal as discussed in Eq. (9)--while keeping the clipped distribution \(_{t}\) close enough to the original distribution \(p_{t}\). This last property heavily relies on the log-concavity of the exponential weight distribution (Ito et al., 2020). The definition of the bonus term is similar to that in Algorithm 2 (Eq. (13)). The construction of the reward estimator (Line 7) and the way of lifting (Eq. (10)) are also similar to those in Algorithm 2. Again, we adopt the fixed-point formulation where the calculation of the policy at time \(t\) involves the bonus at time \(t\), which, in turn, depends on the policy at time \(t\). It is unlikely that this algorithm can be polynomial time. As a remedy, we provide a polynomial time algorithm (Algorithm 6) in Appendix J with a much worse regret bound of \(}(d^{3}+d^{}{{2}}}C)\). The regret guarantee of Algorithm 3 is given in the following theorem.

**Theorem 5.2**.: _Algorithm 3 ensures with probability at least \(1-\), \(_{T}=}T}+dC\), where \(}()\) hides \((T/)\) factors._

## 6 Gap-Dependent Misspecification

Intimately related to corrupted settings are _misspecified_ settings, settings where our model class is unable to capture the true environment we are working with. For example, we might consider a stochastic linear bandit problem where the underlying reward function \(f()\) is nearly linear, i.e., there exists some \(\) and \(^{}()\) such that \(|f(a)-a^{}|^{}(a)\) for each \(a\). Indeed, in such settings, playing on our true (nearly linear) environment is equivalent to playing on the environment with reward mean \(a^{}\) and with corruption \(^{}(a)\) at each step. Thus, if we can solve corruption settings, it stands to reason that we can solve misspecified settings.

Here we are particularly interested in obtaining bounds on misspecified decision-making that scale precisely with action-dependent misspecification, \(^{}(a)\). While it is relatively straightforward to obtain bounds on learning in misspecified settings for a uniform level of misspecification \(_{a}^{}(a)\), obtaining bounds on learning with action-dependent misspecification have proved more elusive. To formalize this, we consider, in particular, the following _gap-dependent_ notion of misspecification defined in Liu et al. (2023).

**Assumption 1** (Gap-Dependent Misspecification (Liu et al., 2023)).: _There exists some \(^{d}\) such that some \(>0\), denoting \((a)=_{a^{}}f(a^{})-f(a)\), we have for any \(a\),_

\[|f(a)-a^{}|(a).\]

_We let \(^{}\) denote the original environment with reward function \(f(a)\) (with \(_{T}^{^{}}\) the corresponding regret), and \(_{0}\) the environment with linear reward, \(a^{}\), (with \(_{T}^{_{0}}\) the corresponding regret)._

Assumption 1 allows the reward to be misspecified, but the misspecification level for an action scales with how suboptimal that action is. This could correspond to real-world settings where, for example, significant attention has been given to modeling near-optimal behavior, such that it is accurately represented within our model class, but much less attention has been given to modeling suboptimal behavior. We assume access to a generic corruption-robust algorithm.

**Assumption 2**.: _We have access to a regret minimization algorithm which takes as input some \(C^{}\) and with probability at least \(1-\) has regret bounded on \(_{0}\) as_

\[_{T}^{_{0}}_{1}(,T)+ _{2}(,T)C^{}\]

_if \(C^{} C_{t=1}^{T}^{}(a_{t})\), and by \(T\) otherwise, for \(C\) as defined above and for (problem-dependent) constants \(_{1}(,T),_{2}(,T)\) which may scale at most logarithmically with \(T\) and \(\)._

Assumption 2 is essentially the guarantee of a corruption-robust algorithm in terms of strong corruption measure (defined in Section 3). Note, in particular, that Assumption 2 only needs to obtain a sub-linear regret guarantee in the known-corruption setting, and can have linear regret in the setting where the corruption level is unknown. We then have the following result.

**Theorem 6.1**.: _Assume our environment satisfies Assumption 1 and that we have access to a corruption-robust algorithm satisfying Assumption 2. Then as long as \(\{,_{2}(,T)^{-1}\}\), with probability at least \(1-2\) we can achieve regret bounded as:_

\[_{T}^{^{}} 6_{1}(,T )+4+4.\]

Theorem 6.1 states that, assuming our environment exhibits gap-dependent misspecification with tolerance \(\{,_{2}(,T)^{-1}\}\), then we can achieve regret on the true environment bounded as the leading-order term of our corruption-robust oracle, \(_{1}(,T)\), with additional overhead of only \(()\). This reduction is almost entirely black-box: it requires knowledge of \(_{1}(,T)\) and \(_{2}(,T)\), but does not require knowledge of \(\) or any other facts about the corruption-robust algorithm.

**Remark 1** (Anytime Algorithm).: _The oracle of Assumption 5 must be anytime, achieving the above regret guarantee for any \(T\) not given as an input. Though many existing corruption-robust algorithms take \(T\) as input, the standard doubling trick can convert them into an anytime algorithm._

### Optimal Misspecification Rate for Linear Bandits

We are particularly interested in how stringent a condition on the misspecification level--how small a value of \(\)--Theorem 6.1 requires. As we have shown, Theorem 4.1 obtains the optimal misspecification level of \(dC\). We then have the following corollary.

**Corollary 6.1.1**.: _Assume our environment is a misspecification linear bandit satisfying Assumption 1 with \(()\). Then instantiating Assumption 2 with the algorithm of Theorem 4.1, we can achieve regret bounded with probability \(1-\) as \(_{T}^{^{}}(d)\)._

While the regret bound of Corollary 6.1.1 achieves a scaling of \(}(d)\), which is tight for linear bandits (Lattimore and Szepesvari, 2020), it is unclear its requirement on \(\) of \(}()\) is optimal. The result below shows that it is not optimal because \((})\) suffices for \(}(d)\) regret.

**Theorem 6.2**.: _Assume our environment is a misspecified linear bandit satisfying Assumption 1 with \((})\). Then there exists an algorithm that achieves, w.p. \(1-\): \(_{T}^{^{}}(d)\)._

Theorem 6.2 relies on a specialized algorithm for the gap-dependent misspecification setting, and improves on the best-known bound for gap-dependent misspecification in linear bandits, which requires \(}()\)(Liu et al., 2023a). Moreover, for \(>c_{T}}\) for some logarithmic term \(c_{T}\), adapting the lower-bound instance from Lattimore et al. (2020), we show that achieving sub-linear regret is not possible (Theorem K.2). These results jointly show that \(}\) is the best \(\) we can hope for. This disproves the conjecture of Liu et al. (2023a) that \(=(1)\) is possible.

Note that the reduction in Theorem 6.1 is _not_ able to achieve a tight \(\)--while reducing from gap-dependent misspecification to corruption allows for black-box usage of existing algorithms, it requires more stringent conditions on the misspecification level than specialized algorithms for this setting.

### Gap-Dependent Misspecification in Reinforcement Learning

Theorem 6.1 is a corollary of a more general result, Theorem L.1, which applies to misspecified reinforcement learning, where we there assume a generalized notion of gap-dependent misspecification: for each policy \(\), \(^{^{},}[_{h=1}^{H}_{h}^{} (s_{h},a_{h})](V_{0}^{}-V_{0}^{})\), for \(V_{0}^{}\) the expected reward of policy \(\), and \(_{h}^{}(s,a)\) a measure of the misspecification at step \(h\), state \(s\), and action \(a\). To illustrate this general reduction, we consider the following setting, a generalization of linear MDPs (Jin et al., 2020).

**Assumption 3** (Gap-Dependent Misspecified Linear MDPs).: _Let \((s,a):^{d}\) denote some feature map and \(_{h}^{d}\) some measure which satisfy \(\|(s,a)\|_{2} 1, s,a\), and \(\|_{s}|_{h}(s)\|_{2}\). Assume that the transitions \(P_{h}( s,a)\) on our true environment satisfy:_

\[\|P_{h}( s,a)-(s,a),_{h}()\|_{ }_{h}^{}(s,a)\]

_for some \(_{h}^{}(s,a) 0\) and \(\|P-Q\|_{}\) the total variation distance between \(P\) and \(Q\). Furthermore, assume that for any policy \(\), we have \(^{}[_{h=1}^{H}_{h}^{}(s_{h},a_{h})] (V_{0}^{}-V_{0}^{})\)._

We then have the following result.

**Corollary 6.2.1**.: _Assume our environment satisfies Assumption 3 with \(}()\). Then there exists an algorithm that achieves regret bounded with probability \(1-\) as \(_{T}^{^{}}}( {d^{3}H^{2}T})\)._

To the best of our knowledge, Corollary 6.2.1 is the first result showing that it is possible to efficiently learn in linear MDPs with gap-dependent misspecification. Note that under Assumption 3, our MDP could be far from a linear MDP--we simply assume that if we play a "good" policy, it appears as approximately linear. This result is almost immediate by instantiating our reduction with a known corruption-robust algorithm for linear MDPs (Ye et al., 2023).

## 7 Open Problems

It remains open how to achieve \(d+dC\) regret in corrupted adversarial linear bandits. The tight \(C_{}\) bound for corrupted linear _contextual_ bandits, where the action set can be chosen by an adaptive adversary in every round, also remains open. The best known upper and lower bounds for this setting are \(}(d+dC_{})\) by He et al. (2022) and \((d+C_{})\) by Lattimore and Szepesvari (2020).

With the AA viewpoint in Section 3, our work first shows the separation between the achievable regret under weak adversary and strong adversary in corrupted linear bandits. An interesting future direction is to investigate similar separation in general decision making (Foster et al., 2021).

[MISSING_PAGE_FAIL:11]

* Ito and Takemura (2024) Ito, S. and Takemura, K. (2024). An exploration-by-optimization approach to best of both worlds in linear bandits. _Advances in Neural Information Processing Systems_, 36.
* Jiang et al. (2017) Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR.
* Jin et al. (2020) Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In _Conference on learning theory_, pages 2137-2143. PMLR.
* Jin et al. (2024) Jin, T., Liu, J., Rouyer, C., Chang, W., Wei, C.-Y., and Luo, H. (2024). No-regret online reinforcement learning with adversarial losses and transitions. _Advances in Neural Information Processing Systems_, 36.
* Kong et al. (2023) Kong, F., Zhao, C., and Li, S. (2023). Best-of-three-worlds analysis for linear bandits with follow-the-regularized-leader algorithm. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 657-673. PMLR.
* Lattimore and Szepesvari (2020) Lattimore, T. and Szepesvari, C. (2020). _Bandit algorithms_. Cambridge University Press.
* Lattimore et al. (2020) Lattimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in bandits and in rl with a generative model. In _International conference on machine learning_, pages 5662-5670. PMLR.
* Lee et al. (2020) Lee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. (2020). Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. _Advances in neural information processing systems_, 33:15522-15533.
* Lee et al. (2021) Lee, C.-W., Luo, H., Wei, C.-Y., Zhang, M., and Zhang, X. (2021). Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In _International Conference on Machine Learning_, pages 6142-6151. PMLR.
* Li et al. (2019) Li, Y., Lou, E. Y., and Shan, L. (2019). Stochastic linear optimization with adversarial corruption. _arXiv preprint arXiv:1909.02109_.
* Li and Yang (2024) Li, Y. and Yang, L. (2024). On the model-misspecification in reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2764-2772. PMLR.
* Liu et al. (2023a) Liu, C., Yin, M., and Wang, Y.-X. (2023a). No-regret linear bandits beyond realizability. _arXiv preprint arXiv:2302.13252_.
* Liu et al. (2023b) Liu, H., Wei, C.-Y., and Zimmert, J. (2023b). Towards optimal regret in adversarial linear mdps with bandit feedback. _arXiv preprint arXiv:2310.11550_.
* Liu et al. (2024) Liu, H., Wei, C.-Y., and Zimmert, J. (2024). Bypassing the simulator: Near-optimal adversarial linear contextual bandits. _Advances in Neural Information Processing Systems_, 36.
* Nesterov and Nemirovskii (1994) Nesterov, Y. and Nemirovskii, A. (1994). _Interior-point polynomial algorithms in convex programming_. SIAM.
* Neu and Olkhovskaya (2020) Neu, G. and Olkhovskaya, J. (2020). Efficient and robust algorithms for adversarial linear contextual bandits. In _Conference on Learning Theory_, pages 3049-3068. PMLR.
* Pukelsheim (2006) Pukelsheim, F. (2006). _Optimal design of experiments_. SIAM.
* Seldin and Lugosi (2017) Seldin, Y. and Lugosi, G. (2017). An improved parametrization and analysis of the exp3++ algorithm for stochastic and adversarial bandits. In _Conference on Learning Theory_.
* Seldin and Slivkins (2014) Seldin, Y. and Slivkins, A. (2014). One practical algorithm for both stochastic and adversarial bandits. In _International Conference on Machine Learning_.
* Takemura et al. (2021) Takemura, K., Ito, S., Hatano, D., Sumita, H., Fukunaga, T., Kakimura, N., and Kawarabayashi, K.-i. (2021). A parameter-free algorithm for misspecified linear contextual bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 3367-3375. PMLR.
* Takemura et al. (2020)Wang, R., Salakhutdinov, R. R., and Yang, L. (2020). Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33:6123-6135.
* Wei et al. (2022) Wei, C.-Y., Dann, C., and Zimmert, J. (2022). A model selection approach for corruption robust reinforcement learning. In _International Conference on Algorithmic Learning Theory_, pages 1043-1096. PMLR.
* Wei and Luo (2018) Wei, C.-Y. and Luo, H. (2018). More adaptive algorithms for adversarial bandits. In _Conference On Learning Theory_.
* Ye et al. (2023) Ye, C., Xiong, W., Gu, Q., and Zhang, T. (2023). Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In _International Conference on Machine Learning_, pages 39834-39863. PMLR.
* Zanette et al. (2020) Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020). Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR.
* Zhang et al. (2023) Zhang, W., He, J., Fan, Z., and Gu, Q. (2023). On the interplay between misspecification and sub-optimality gap in linear contextual bandits. In _International Conference on Machine Learning_, pages 41111-41132. PMLR.
* Zimmert and Lattimore (2022) Zimmert, J. and Lattimore, T. (2022). Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits. In _Conference on Learning Theory_, pages 3285-3312. PMLR.
* Zimmert et al. (2019) Zimmert, J., Luo, H., and Wei, C.-Y. (2019). Beating stochastic and adversarial semi-bandits optimally and simultaneously. In _International Conference on Machine Learning_.
* Zimmert and Seldin (2019) Zimmert, J. and Seldin, Y. (2019). An optimal algorithm for stochastic and adversarial bandits. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 467-475. PMLR.

## Appendix A Related Work

* B Equivalence Between AA and CM Viewpoints for Strong Corruption
* C The Case of Unknown \(C_{}\) or \(C\)
* D Proof of Proposition 1
* E Proof of Theorem 4.1
* F Proof of Theorem 5.1
* G Computationally Efficient Algorithm for Adversarial \(C_{}\) Bound
* H Proof of Theorem 5.2
* I Dimension Reduction for Continuous Exponential Weights
* J Computationally Efficient Algorithm for Adversarial \(C\) Bound
* J.1 Preliminaries for Entropic Barrier
* J.2 Auxiliary Lemmas
* J.3 Regret Analysis
* K Gap-dependent Misspecification
* L General Reduction from Corruption-Robust Algorithms to Misspecification
* M Auxiliary LemmasRelated Work

Model MisspecificationTheoretical works on bandits or RL often assume that the underlying world is well-specified by a particular model. Algorithms that are purely built on this assumption are vulnerable to potential misspecifications. Therefore, some works, besides proposing the main results, also discuss the case where the model is misspecified, such as Jiang et al. (2017); Jin et al. (2020); Zanette et al. (2020); Wang et al. (2020); Li and Yang (2024). These discussions, however, usually assume that the amount of misspecification has a uniform upper bound for all actions / states / policies, and the performance degradation is proportional to this uniform upper bound.

For settings like stochastic linear bandits and stochastic linear contextual bandits, it was also found that some widely used algorithm such as LinUCB cannot achieve the tightest guarantee under misspecification (Du et al., 2019). Therefore, a line of work developed better algorithms that have optimal robustness against misspecification, such as Lattimore et al. (2020); Foster et al. (2020); Takemura et al. (2021).

While most work focus on the stochastic setting, Neu and Olkhovskaya (2020) took a first step in studying misspecification in linear contextual bandits with stochastic contexts and adversarial rewards. They established near-optimal regret dependencies on the amount of misspecification.

Gap-dependent MisspecificationGap-dependent misspecification is a setting where the amount of misspecification for an action is bounded by a constant times that action's sub-optimality gap. To our knowledge, this setting is first studied by Liu et al. (2023) for linear bandits. Another related work is Zhang et al. (2023), which assumes that the misspecification is bounded by a constant times the _minimal sub-optimality gap_ among all actions. Although this assumption is more restrictive, they handle the more general linear contextual bandit setting, and derive instance-dependent logarithmic regret bounds.

Corruption-robust BanditsThe guarantees on model misspecification is rather pessimistic in the sense that if the misspecification is time-varying, and large misspecification only appears in a few rounds, then the existing guarantees for misspecification still scale with the largest misspecification. To refine such guarantee, previous works have consider different notions of time-varying corruption, and established more fine-grained regret guarantees. These include \(C_{,},C_{},C_{},\) and \(C\) discussed in Section 3. Among them, \(C_{,}\) and \(C_{}\) are usually studied under the "weak adversary" framework where the adversary decides the corruption before seeing the action chosen by the learner. On the other hand, \(C_{}\) and \(C\) are usually studied under the "strong adversary" framework where the adversary decides the corruption after seeing the action chosen the learner. In Section 3, we provide a unified view for them so that they can both be regarded as weak adversarial setting but with different corruption measure.

The algorithms of Foster et al. (2020) and Takemura et al. (2021) achieved the optimal bound with respect to \(C_{}\) for stochastic linear contextual bandits (i.e., \(d+C_{}\)), and He et al. (2022) showed the optimal bound with respect to \(C\) (i.e., \(d+dC\)). However, it is still unclear whether the tight dependency on \(C_{}\) is \(C_{}\) or \(dC_{}\). In this paper, we answer it for the context-free linear bandit setting, showing that \(d+C_{}\) is achievable. However, the question remains open for linear contextual bandits.

For the adversarial setting, Liu et al. (2024) showed \(d^{2}+C_{,}\) bound for linear contextual bandits with stochastic contexts and adversarial rewards, which can be improved to \(d+C_{,}\) when specialized to adversarial linear bandits. To our knowledge, no \(C_{}\) or \(C\) bound has been shown for adversarial linear bandits, and our work make the first attempts on them.

We remark that for \(A\)-armed adversarial bandits, it is easy to see that \(+C_{}\) bound is achievable simply by running standard adversarial multi-armed bandit algorithm that handles adaptive adversary (e.g., EXP3.P by Auer et al. (2002)). The work of Hajiesmaili et al. (2020) is the only one that we know to obtain \(C\) bound for adversarial bandits. They showed a \(+AC\) bound for \(A\)-armed bandits, which is tight.

Best-of-both-worlds BoundsThe study of the best-of-both-world problem was initiated by Bubeck and Slivkins (2012) and extended by Seldin and Slivkins (2014); Auer and Chiang (2016); Seldinand Lugosi (2017); Wei and Luo (2018); Zimmert and Seldin (2019); Zimmert et al. (2019); Ito and Takemura (2023, 2024); Dann et al. (2023); Kong et al. (2023). The goal of this line of work is to have a single algorithm that achieves a \(}()\) regret when the reward is adversarial and \(( T)\) when the reward is stochastic, without knowing the type of reward in advance. These results should be viewed as refinements of the standard adversarial setting but not the corruption setting considered in our work, though they also used the term "corruption" in their work.

For example, Lee et al. (2021); Ito and Takemura (2024, 2023, 2024); Dann et al. (2023); Kong et al. (2023) studied the best-of-both-world linear bandits problem. The underlying world could be stochastic (\(_{t}=^{}\) for all \(t\)) or adversarial (\(_{t}\)'s are arbitrary). Their algorithm achieves a bound of \((d^{2}(T)/)\) in the former case, where \(\) is the reward gap between the best and the second-best arm, and \(}(d)\) in the latter phase. They also define the _corruption_\(C^{}=_{t}_{a}|a^{}(_{t}-^{})|\) and show that their algorithm achieves a regret of \((d^{2}(T)/+(T)C^{}/})\). Compared to our setting, their corruption is in a more limited form, but their target regret bound in the stochastic setting is tighter than ours.

## Appendix B Equivalence Between AA and CM Viewpoints for Strong Corruption

We show that strong corruption in both definitions is equivalent, that is, for any adversary having strong corruption \(C=_{t}|_{t}|\) from AA viewpoint, there exists an adversary using the equal amount of strong corruption \(_{t}|^{}_{t}(a_{t})|\) from CM viewpoint, where \(|_{t}|=|^{}_{t}(a_{t})|\) for all \(t\), and vice versa.

Assume that \((H_{t-1},a_{t})\) is the function used by an AA strong adversary to decide the corruption at time \(t\), where \(H_{t-1}\) is the history up to time \(t-1\) and \(a_{t}\) is the chosen action at time \(t\). Then we define \(^{}_{t}(a)(H_{t-1},a), a\) for the CM viewpoint, thus \(|_{t}|=|(H_{t-1},a_{t})|=|^{}_{t}(a_{t})|\). Note that the function \(^{}_{t}()\) only depends on the history up to time \(t-1\), so the definition of \(^{}_{t}\) is known to adversary before observing \(a_{t}\). The other direction of this equivalence is achieved by setting the corruption in AA viewpoint as \(_{t}=^{}_{t}(a_{t})\). Note that since \(a_{t}\) is known to a strong adversary in AA viewpoint, \(_{t}\) is also known.

## Appendix C The Case of Unknown \(C_{}\) or \(C\)

In the corrupted stochastic setting, Wei et al. (2022) developed a black-box reduction that can turn any algorithm achieving \(_{1}+_{2}+_{3}C_{}\) regret with the knowledge of \(C_{}\) into an algorithm achieving \((T)(_{1}+_{2}+_{3}C_{})\) regret without knowledge of \(C_{}\). This reduction can be directly applied to our stochastic \(C_{}\) bound result (Theorem 4.1), which allows us to achieve almost the same regret bound without knowledge of \(C_{}\). The idea of Wei et al. (2022) has been extended to the adversarial setting by Jin et al. (2024) (see their Section 4). Similarly, for the adversarial setting, one can turn any algorithm achieving \(_{1}+_{2}+_{3}C_{}\) regret with known \(C_{}\) into one achieving \((T)(_{1}+_{2}+_{3}C_{})\) regret without knowing \(C_{}\). This can be directly applied to our adversarial \(C_{}\) result (Theorem 5.1).

The case of unknown \(C\) is quite different. It has been proven by Bogunovic et al. (2021) that it is impossible to achieve a bound that has linear scaling in \(C\) (e.g., \(_{1}+_{2}+_{3}C\)) for all \(C\) simultaneously if \(C\) is not known by the learner. This is also mentioned in He et al. (2022) again. Hence, almost all previous work studying \(C\) bound assumes knowledge on \(C\). If \(C\) is unknown, simply setting \(=\) as an upper bound of \(C\) yields a bound of \((+C^{2})\)--if \(C\) indeed holds, then \(\) is a correct upper bound, so the regret can be bounded by \((+)=()\); if \(C>\), then simply bound the regret by \(T(C^{2})\).

## Appendix D Proof of Proposition 1

First, we argue that there exists a deterministic algorithm achieving \(}(d+C_{})\) upper bound. The algorithm of Takemura et al. (2021) is such an algorithm, although they only showed an upper bound of \(}(d+C_{})\). To argue the stronger \(}(d+C_{})\) bound, we only need to slightly modify their analysis: In their proof of Lemma 2 (in their Page 6), the original proof bound the per-step regret due to the misspecification as the following (the calculation below uses their original notation):

\[|_{_{t,s}}_{}(i_{})x_{}(i_{ })^{}V_{t-1,s}^{-1}x_{t}(i)| |_{_{t,s}}(x_{ }(i_{})^{}V_{t-1,s}^{-1}x_{t}(i))^{2}},\] \[|x_{t}(i)^{}V_{t-1,s}^{-1}x_{t}( i)}\] \[|c^{-2s}}\] \[}().\] (by their Lemma 1)

We can tighten their analysis by doing the following:

\[|_{_{t,s}}_{}(i_{})x_{}( i_{})^{}V_{t-1,s}^{-1}x_{t}(i)| }_{}(i_{} )^{2})(_{_{t,s}}(x_{}(i_{})^{}V_{t -1,s}^{-1}x_{t}(i))^{2})},\] \[}_{}(i_{} )^{2})x_{t}(i)^{}V_{t-1,s}^{-1}x_{t}(i)}\] \[}_{}(i_{} )^{2})c^{-2s}}\] \[}(|} _{_{t,s}}_{}(i_{})^{2}}).\] (by their Lemma 1)

Since the regret for every step in \(_{t,s}\) can be bounded by this value, when summing the regret over \(_{T+1,s}\), one can get a regret of order

\[}(|_{_{T+1,s}} _{}(i_{})^{2}}).\]

Further summing this over \(s\) (there are logarithmically many different \(s\)) and using that \([T]=_{s}_{T+1,s}\) and using Cauchy-Schwarz, we get a \(^{T}_{t}(i_{t})^{2}}=C_{}\) bound.

To argue that any deterministic algorithm must suffer at least \((d+dC_{})\) regret, we only need to use the lower bound instance of \((d+dC)\). At the beginning of round \(t\), the adversary simply change the corruptions \(_{t}(a)\) to be zero for all \(a a_{t}\) (the adversary knows what \(a_{t}\) since the algorithm is deterministic). This makes \(C=C_{}\), and thus the lower bound \((d+dC_{})\) holds.

## Appendix E Proof of Theorem 4.1

**Lemma E.1**.: _With probability at least \(1-2\), for all \(k\) and for all \(b_{k}\),_

\[| b,_{k}-^{}| 4|T/)}{m_{k}}}+C^{},dC\}}{m_{k }}.\]

Proof.: Let \(_{t}[]\) be the expectation conditioned on the history up to round \(t-1\). We fix \(k\) and \(b\) and consider

\[X_{t}=b^{}(m_{k}G_{k})^{-1}a_{t}r_{t}\]for \(t_{k}\). Notice that \(_{t_{k}}X_{t}=b^{}_{k}\) and

\[_{t_{k}}_{t}[X_{t}] =_{t_{k}}b^{}(m_{k}G_{k})^{-1}_{t} [a_{t}(a_{t}^{}^{}+_{t}(a_{t}))]\] \[=b^{}^{}+b^{}(m_{k}G_{k})^{-1}_{t _{k}}_{t}[a_{t}_{t}(a_{t})].\]

Also, by the definition of \(G_{k}\), we have \(|X_{t}|=|b^{}(m_{k}G_{k})^{-1}a_{t}r_{t}|}\|b\|_{G_{k}^{-1}}\|a_{t}\|_{G_{k}^{-1}} }\). Thus, by Freedman's inequality, with probability at least \(1-|T}\), the following holds:

\[| b,_{k}-^{}| =|_{t_{k}}X_{t}-_{t_{k}} _{t}[X_{t}]|+|b^{}(m_{k}G_{k})^{-1}_{ t_{k}}_{t}[a_{t}_{t}(a_{t})]|\] \[|T}{ })_{t_{k}}_{t}[X_{t}^{2}]} }_{_{1}}+}(|T}{ })+(m_{k}G_{k})^{-1}_{t_{k}}_{t}[a_{t}_{t}(a_{t})]|}_{_{2}}.\]

We bound \(_{1}\) by

\[_{1} =|T/)_{t_{k}} _{t}[b^{}(m_{k}G_{k})^{-1}a_{t}a_{t}^{}(m_{k}G_{k})^{ -1}b]}\] \[=|T/)^{2}}_{t _{k}}b^{}G_{k}^{-1}b}|T/ )}{m_{k}}},\]

and \(_{2}\) by

\[_{2} =}|_{t_{k}}_{a _{k}}p_{k}(a)_{t}(a)b^{}G_{k}^{-1}a|\] \[}_{t_{k}}_{k}}p_{k}(a)_{t}(a)^{2}}_{k}}p_ {k}(a)(b^{}G_{k}^{-1}a)^{2}}\] \[}_{t_{k}}_{a^{}} _{t}(a^{})C^{}}{m_{k}}.\]

or

\[_{2} }_{t_{k}}_{t} [_{t}(a_{t})|b^{}G_{k}^{-1}a_{t}|]\] \[}_{t_{k}}_{t}(a_{t}) |b^{}G_{k}^{-1}a_{t}|+}|T}{})_{t_{k}} _{t}[_{t}(a_{t})^{2}|b^{}G_{k}^{-1}a_{t}|^{2}]}\] \[+(a_{t})b^{}G_{k}^{-1}a_{ t}|}{m_{k}}(|T}{})\] (Freedman's inequality) \[}_{t_{k}}_{t}(a_{t}) +}|T}{} )_{t_{k}}_{t}[b^{}G_{k}^{-1}a_{t }a_{t}^{}G_{k}^{-1}b]}+}(|T}{})\] \[}+}|T}{})_{t_{k}}b^{}G_{k} ^{-1}b}+}(|T}{})\] \[}+|T/ )}{m_{k}}}+}(|T}{ }).\]Thus, for any \(b_{k}\), with probability at least \(1-|T}\),

\[| b,_{k}-^{}|  2|T/)}{m_{k}}}+}(||T/)+}\{C^{},dC\}\] \[ 4|T/)}{m_{k}}}+}\{C^{},dC\}. 28.452756pt(m_{k} d(| |/))\]

Taking a union bound over \(k\) and \(b_{k}\) finishes the proof. 

**Lemma E.2**.: _Let \(a^{}=*{argmax}_{a}a^{}^{}\). Then with probability at least \(1-2\), \(a^{}_{k}\) for all \(k\)._

Proof.: Suppose that the high-probability event in Lemma E.1 holds. For any \(k\), if \(a^{}_{k}\), then for any \(b_{k}\),

\[b^{}_{k}-a^{^{}}_{k}  b^{}^{}-a^{^{}}^{}+|b^ {}(_{k}-^{})|+|a^{^{}}( _{k}-^{})|\] \[ 0+2(4|T/)}{m_{k}}}+ C^{},dC\}}{m_{k}}).\]

By the definition of \(_{k+1}\) in Eq.1, we have \(a^{}_{k+1}\). The lemma is then proven by an induction argument. 

Proof of Theorem4.1.: We first calculate the regret in epoch \(k>1\) assuming that the event in Lemma E.2 holds.

\[_{t_{k}}(_{a}a^{} ^{}-a_{t}^{}^{})\] \[_{t_{k}}(_{a}a^{ }_{k-1}-a_{t}^{}_{k-1})+2m_{k}_ {a_{k}}|a^{}(_{k-1}-^{ })|\] \[ m_{k}(|T /)}{m_{k-1}}}+C^{},dC\}}{m_{k-1}})\] \[=((||T/)}+\{ C^{},dC\}).\]

Summing this over \(k\) and using that \(m_{1}=d(||T/)\), we get

\[_{t=1}^{T}(_{a}a^{}^{}-a_{t}^{} ^{})(|T/ )}+d(||T/)+\{C^{},dC\} T).\]

Notice that without loss of generality we can assume \(d(||T/) T\) (otherwise the right-hand side is vacuous). Using this fact gives the desired bound.

From Exercise 27.6 in Lattimore and Szepesvari (2020), the \(\)-covering number of \(\) is bounded by \(()^{d}\). Let \((,)\) be the \(\)-net of \(\), we then have \(|(,)| T^{d}\). Thus, when \(|| T^{d}\), we can use \((,)\) as \(_{1}\) in Algorithm1 to conduct phase elimination. In that case, following above proof, we have

\[_{t=1}^{T}(_{a(,)}a^{}^{}-a_{t}^{}^{}) (( ,)|T/)}+\{C^{ },dC\} T)\] \[(d+\{C^{ },dC\} T).\]From the definition of covering number, there exists a \(a_{c}^{}(,)\) such that

\[_{a}a^{}^{}-(a_{c}^{})^{}^{ }.\]

We have

\[_{t=1}^{T}(_{a}a^{}^{}- _{a(,)}a^{}^{}) _{t=1}^{T}(_{a}a^{}^{ }-(a_{c}^{})^{}^{})+_{t=1}^{T}((a_{c}^ {})^{}^{}-_{a(,) }a^{}^{})\] \[ 6d.\]

Thus,

\[_{t=1}^{T}(_{a}a^{}^{}-a_{t}^{} ^{}) (d+\{C^{ },dC\} T).\]

## Appendix F Proof of Theorem 5.1

In this section, we use the following notation:

\[}_{t}=0&_{t}\\ _{t}^{}&0,_{t }= B_{t}- B_{t-1}&0\\ 0&0.\]

Algorithm 2 is equivalent to the FTRL update:

\[_{t}=*{argmax}_{} \{,_{s=1}^{t-1}}_{s}+_{s=1}^{t}_{s}-) }{}\}.\] (14)

Algorithm 4 is equivalent to

\[_{t}=*{argmax}_{} \{_{s=1}^{t-1},}_{s}+_{s}-)}{}\}.\] (15)

By the standard analysis for FTRL algorithms (e.g., Theorem 2 in Zimmer and Lattimore (2022)), the regret bounds of Eq. (14) and Eq. (15) are given by the following lemmas, respectively.

**Lemma F.1**.: _The update rule Eq. (14) (Algorithm 2) ensures for any \(\),_

\[_{t=1}^{T}-_{t}, }_{t}\] \[)-_{}G( )}{}-_{t=1}^{T}-_{t},_{t}+_{t=1}^{T}_{ }\{-_{t},}_{t}+_{t}-( ,_{t})}{}\}.\]

**Lemma F.2**.: _The update rule Eq. (15) (Algorithm 4) ensures for any \(\),_

\[_{t=1}^{T}-_{t}, }_{t}\] \[)-_{}G( )}{}-_{t=1}^{T}-_{t},_{t}+_{t=1}^{T}_{ }\{-_{t},}_{t}+_{t}-( ,_{t})}{}\}.\]

We consider an arbitrary comparator \(p_{}()\) with \(u_{}=_{a p_{}}[a]\). Define \(p=(1-)p_{}+\). We have \(p_{}()\), and define \(u=_{a p}[a]\) and \(=}(p)\). The regret with respect to \(p_{}\) can be decomposed as the following: With probability at least \(1-\),

\[_{T}(p_{})\] (16) \[=_{t=1}^{T} u_{}-a_{t},_{t}\] \[=_{t=1}^{T} u-a_{t},_{t}+2 T\] \[_{t=1}^{T} u-x_{t},_{t}+ ()+2 T\] (Azuma's inequality) \[=^{T} u-x_{t},_{t}- _{t}[_{t}]}_{}+ ^{T} u-x_{t},_{t}[ _{t}]-_{t}}_{}+^{T}-_{t},}_{t}}_{}+( + T).\] (17)

By Lemma F.1, the **FTRL** term can further be bounded by

\[)-_{ }G()}{}}_{}- ^{T}-_{t}, _{t}}_{}+^{T }_{}-_ {t},}_{t}-(,_{t})}{}}_{}.\] (18)

In the following five lemmas, we bound the five terms **Bias**, **Deviation**, **Penalty**, **Bonus**, and **Stability**.

**Lemma F.3**.: \[ C_{}_{t}\|u\|_{_{t}^{-1}}+ C_{}.\]

Proof.: \[_{t}[ u-x_{t},-_{t}^{-1}a_{t} _{t}(a_{t})] _{t}[)^{}_{t}^{-1}a_{ t}a_{t}^{}_{t}^{2}(a_{t})_{t}^{-1}(u-x_{t})}]\] \[)^{}_{t}^{-1}}_{t}[a_ {t}a_{t}^{}_{t}^{2}(a_{t})]_{t}^{-1}(u-x_{t})\] \[_{t}\|u-x_{t}\|_{_{t}^{-1}}\] \[_{t}\|x_{t}\|_{_{t}^{-1}}+_ {t}\|u\|_{_{t}^{-1}}\] \[_{t}+_{t}\|u\|_{_{t} ^{-1}}.\] ( \[_{t} x_{t}x_{t}^{}\] )

Thus,

\[ =^{T} u-x_{t},_{t}- _{t}[_{t}^{-1}a_{t}a_{t}^{}_{t}] }_{=0}+_{t}[_{t=1}^{T} u-x_{t},-_{t}^{-1 }a_{t}_{t}(a_{t})]\] \[ C_{}_{t}\|u\|_{_{t}^{-1}}+C_{}.\]

**Lemma F.4**.: _With probability of at least \(1-\), we have_

\[_{t}\|u\|_{_{t}^{-1}}(12 +(T/)}{})+1 2+}\]Proof.: Notice that

\[| u-x_{t},_{t}| |(u-x_{t})^{}_{t}^{-1}a_{t}|\] \[\|u-x_{t}\|_{_{t}^{-1}}\|a_{t}\|_{_{t}^{-1}}\] \[}{}\|u-x_{t}\|_{_{t}^{-1}}.\]

By the strengthened Freedman's inequality (Lemma M.3), with probability at least \(1-\),

\[ =_{t=1}^{T} u-x_{t},_{t}[_{t}]-_{t}\] \[ 3^{T}_{t}[ u-x_{t}, _{t}^{2}](d^{4}T^{4}/)}+2 }{}_{t}\|u-x_{t}\|_{_{t}^{-1}}(d^{4} T^{4}/)\] \[_{t}\|u-x_{t}\|_{_{t}^{-1}}(12+(T/)}{})\] \[_{t}\|u\|_{_{t}^{-1}}(12+(T/)}{})+12+}.\]

**Lemma F.5**.: \[.\]

Proof.: Define \(_{0}=_{a}aa^{}&a\\ a^{}&1\). By the definition of the feasible set \(\), for any \(\), \(_{0}=_{0}\) and \((d+1)_{0}\). Thus, **Penalty** can be upper bounded by

\[)-_{}G()}{} _{0})-G((d+1)_{0} )}{}=(_{0} )}{(_{0})})=.\]

**Lemma F.6**.: \[ 3 d(T)-_{t}\|u\|_{_{t}^{-1}}^{2}.\]

Proof.: Given \(_{a p_{0}}[aa^{}]_{a p_{0}}[a]_{a p_{0}}[a]^{}=ua^{}\), we have

\[_{t=1}^{T},_{t}= _{a p_{0}}[aa^{}], B_{T} uu ^{}, B_{T}=\|u\|_{B_{T}}^{2}.\]

Recall that \(B_{1}=_{1}^{-1}\) and for \(t 2\),

\[_{t}^{-1} =B_{t-1}^{}(_{i=1}^{d}_{ti}v_{ti}v_{ti} ^{})B_{t-1}^{}, \{v_{ti}\}_{i=1}^{d}\] \[B_{t-1} =B_{t-1}^{}(_{i=1}^{d}v_{ti}v_{ti}^{} )B_{t-1}^{}, _{i=1}^{d}v_{ti}v_{ti}^{}=I\] \[B_{t} =B_{t-1}^{}(_{i=1}^{d}\{_{ti},1\}v_ {ti}v_{ti}^{})B_{t-1}^{},\] (19)which ensures \(B_{t} B_{t-1}\) and \(B_{t}_{t}^{-1}\). By induction, it leads to \(B_{T}_{t}^{-1}\) for any \(t\). This implies

\[\|u\|_{B_{T}}^{2}_{t}\|u\|_{_{t}^{-1}}^{2}.\]

Thus, \(_{t=1}^{T},_{t}_{t}\|u \|_{_{t}^{-1}}^{2}\).

Next, we upper bound \(_{t=1}^{T}_{t},_{t}\). First, notice that \(_{1},_{1}=(_{t}B_{t })=(I)= d\). For \(t 2\),

\[_{t},_{t} =(_{t}(B_{t}-B_{t-1}))\] \[=(B_{t-1}^{-}(_{i=1}^{d} _{ti}v_{ti}v_{ti}^{})^{-1}B_{t-1}^{-}B_{t-1}^{ }(_{i=1}^{d}\{_{ti}-1,0\}v_{ti}v_{ti}^{} )B_{t-1}^{})\] (by Eq. ( 19 )) \[=((_{i=1}^{d}_{ti}v_{ti}v_{ti} ^{})^{-1}(_{i=1}^{d}\{_{ti}-1,0\}v_{ti}v_{ti}^{ }))\] \[=_{i=1}^{d}\{1-},0\}\] \[_{i=1}^{d}\{_{ti},0\}.\]

We also have

\[(B_{t})-(B_{t-1})\] \[=(^{})( _{i=1}^{d}\{_{ti},1\}v_{ti}v_{ti}^{})(B_{t-1 }^{})}{(B_{t-1}^{})(_ {i=1}^{d}v_{ti}v_{ti}^{})(B_{t-1}^{})})\] \[=(^{d}\{_{ti},1\}v_ {ti}v_{ti}^{})}{(_{i=1}^{d}v_{ti}v_{ti}^{})})\] \[=_{i=1}^{d}\{_{ti},0\}.\]

Thus,

\[_{t=1}^{T}_{t},_{t} d+ (B_{T})-(B_{1}) d+ (B_{T}).\] (20)

Finally, we bound \((B_{T})\). Since \(_{t}=_{a}p_{t}(a)aa^{}_{a}(a)aa^{}\), by Theorem 3 of Bubeck et al. (2012), we have \(_{t}I\) and \(_{t}^{-1}I\) for all \(t\). Thus, \(B_{1}=_{1}^{-1}I\). Below, we use induction to show that \(B_{t}I\). Assume \(B_{t-1}I\). Then,

\[B_{t} =B_{t-1}^{}(_{i=1}^{d}\{_{ti},1\}v _{ti}v_{ti}^{})B_{t-1}^{}\] \[ B_{t-1}^{}(_{i=1}^{d}(_{ti}+1)v _{ti}v_{ti}^{})B_{t-1}^{}\] \[=_{t}^{-1}+B_{t-1}I+I=I.\]By induction, we get \(B_{T}I\) and \((B_{T}) 2d(T)\) by setting \(=}\). Overall, by Eq. (20), we have

\[_{t=1}^{T}_{t},_{t} 3  d(T).\]

Combining the upper bound for \(_{t=1}^{T}_{t},_{t}\) and the lower bound for \(_{t=1}^{T},_{t}\) finishes the proof. 

**Lemma F.7**.: _With probability at least \(1-\)_

\[(d T+).\]

Proof.: For any \(p\), define \((p)=_{a p}[a]\) and

\[(p)=_{a p}[(a-(p))(a-(p))^{}],}(p)=_{a p}(p)+(p)(p)^ {}&(p)\\ (p)^{}&1.\]

For any \(=H+hh^{}&h\\ h^{}&1\), given \(_{t}=(p_{t})+x_{t}x_{t}^{}&x_{t}\\ x_{t}^{}&1\), we have

\[-_{t},}_{t}-(,_{t})}{2} -_{t},}_{t} --h\|_{(p_{t})^{-1}}^{2}}{2}\] (Lemma M.1) \[= h-x_{t},_{t}--h\|_{(p_{t})^{-1}}^{2}}{2}\] \[\|_{t}\|_{(p_{t})}^{2}\] (AM-GM) \[= r_{t}^{2}a_{t}^{}_{t}^{-1}(p_{t}) _{t}^{-1}a_{t}\] \[\|a_{t}\|_{_{t}^{-1}}^{2}.\] ( \[|r_{t}| 1\]  and \[(p_{t})_{t}\] )

By Freedman's inequality, since \(_{t}[\|a_{t}\|_{_{t}^{-1}}^{2}]=d\), and \(\|a_{t}\|_{_{t}^{-1}}^{2}\), with probability at least \(1-\), we have

\[_{t=1}^{T}\|a_{t}\|_{_{t}^{-1}}^{2} (d T+).\]

Proof of Theorem 5.1.: Using Lemma F.3-Lemma F.7 in Eq. (17) and Eq. (18), we get

\[_{T}\] \[(+ dT+ d(T )++}++C_{}+ T)\] \[+_{t}\|u\|_{_{t}^{-1}}(12+(T/)}{}+C_{})- _{t}\|u\|_{_{t}^{-1}}^{2}\] \[(+ dT+ d( T)++}+.\] \[.+)^{2}}{}++(T/)}{}+C_{}+  T).\] (AM-GM)

Therefore, the choice \(=},=\{}{},\}\) and \(=}\) gives

\[_{T}(d(T/)+C_{}).\]

[MISSING_PAGE_EMPTY:25]

where the last inequality is due to \(_{t}^{-1} B_{t}-B_{t-1}\). Since \(}\), by Lemma M.2, with probability of at least \(1-\), we have

\[  8_{t=1}^{T}(_{t}_{t}_{t} _{t})\] \[ 8^{2}_{t=1}^{T}(_{t}(B _{t}-B_{t-1})_{t}(B_{t}-B_{t-1}))\] \[ 8^{2}d\]

where the last step follows the similar analysis in Lemma F.6. 

Proof of Theorem 5.1 (Option II).: Using Lemma F.3-Lemma G.1 in Eq. (21) and Eq. (22), we get

\[_{T}=(+ dT+d (T)++}+ +^{2}d.\] \[.+)^{2}}{}++(T/)}{}+C_{}+ T)\]

By choosing \(=\{}{},\}\) and \(=\{}{16C_{}},}\}\), and \(=}\), we could ensure \(}\). This gives the final regret \((d(T/)+dC_{})\). The additional \(\) factor comes from the additional condition for the **Stability-2** term. 

## Appendix H Proof of Theorem 5.2

Similar to before, we define

\[}_{t}=0&_{t}\\ _{t}^{}&0,_{t}=  B_{t}- B_{t-1}&0\\ 0&0,\]

and \(x_{t}=_{a p_{t}}[a]\), \(_{t}=_{a_{t}}[a]\). We perform the regret decomposition as the following.

\[_{T}=_{t=1}^{T} u-a_{t},_{t}\] \[=_{t=1}^{T} u-x_{t},_{t}+_ {t=1}^{T} x_{t}-_{t},_{t}+_{t=1}^ {T}_{t}-a_{t},_{t}\] \[=_{t=1}^{T} u-x_{t},_{t}+  T+\] \[=^{T} u-x_{t},_{t}- _{t}[_{t}]}_{}+^{T} u-x_{t},_{t}[_{t}]- _{t}}_{}+^ {T} u-x_{t},_{t}}_{}+  T+.\] (23)The **FTRL** term can be further bounded as the following.

**FTRL** \[=_{t=1}^{T}_{a p_{t}}[ u-a, _{t}]\] \[=_{t=1}^{T}_{ q_{t}}[ -,}_{t}]\] \[ T}{}+_{t=1}^{T} _{ q_{t}}[( ,}_{t})- ,}_{t}-1 ]+_{t=1}^{T}_{ q_{t}}[ -,_{t}]\] (by Theorem 1.2) \[= T}{}+_{t=1}^{T} _{a p_{t}}[( a,_{ t})- a,_{t}-1 ]}_{}+^{T}_{a p_{t }}[\|a\|_{B_{t}-B_{t-1}}^{2}]-_{t=1}^{T}\|u\|_{B_{T}}^{2} }_{}.\] (24)

In the following four lemmas, we bound the four terms **Bias**, **Deviation**, **Bonus**, **Stability**.

**Lemma H.1**.: \[(_{t}\|x_{t}-u\|_{_{t}^{-1}}) (T+2+ C).\]

Proof.: \[ =_{t=1}^{T} u-x_{t},_{t}-_{t}[ _{t}]\] \[=_{t=1}^{T} u-x_{t},_{t}-_{t}^{-1}_{t}[a_{t}a_{t}^{}]_{t}+_{t}^{-1}_{t}[a_{t}_{t}(a_{t})]\] \[=_{t=1}^{T} u-x_{t},_{t} ^{-1}_{t}+_{t=1}^{T} u-x_{t},-_{t}^{-1}_{t}[a_{t}_{t}(a_{t})] (_{t}= I+_{t}[a_{t}a_{t}^{}])\] \[_{t=1}^{T}\|x_{t}-u\|_{_{t}^{-1}} \|_{t}\|_{_{t}^{-1}}+_{t=1}^{T}\|x_{t}-u\|_{ _{t}^{-1}}\,_{t}[\|a_{t}\|_{ _{t}^{-1}}|_{t}(a_{t})]\] \[(_{t}\|x_{t}-u\|_{_{t}^{-1}} )(T+_{t=1}^{T}_{t}[\|a_{t}\|_{ _{t}^{-1}}\,|_{t}(a_{t})]]\] \[(_{t}\|x_{t}-u\|_{_{t}^{-1}} )(T+2+_{t=1}^{T }|_{t}(a_{t})|).\] (Azuma's inequality)

**Lemma H.2**.: \[(_{t}\|u-x_{t}\|_{_{ t}^{-1}}d(T/)).\]Proof.: Notice that

\[| u-x_{t},_{t}| |(u-x_{t})^{}_{t}^{-1}a_{t}|\] \[\|u-x_{t}\|_{_{t}^{-1}}\|a_{t}\|_{ _{t}^{-1}}\] \[\|u-x_{t}\|_{_{t}^{-1}}.\]

By the strengthened Freedman's inequality (Lemma M.3), with probability at least \(1-\),

\[ =_{t=1}^{T} u-x_{t},_{t}[_{t}]-_{t}\] \[(3^{T}_{t}[  u-x_{t},_{t}^{2}](T^{d}/ )}+2_{t}\|u-x_{t}\|_{_{t}^{-1}}( T^{d}/))\] \[(_{t}\|u-x_{t}\|_{_{t }^{-1}}d(T/)).\] (using the assumption

\[d T\]

**Lemma H.3**.: \[ 3 d(T)-_{t}\|u\|_{_{t}^{-1}}^{2}.\]

Proof.: The proof the same as in the logdet case. See the proof of Lemma F.6.

**Lemma H.4**.: \[( dT^{2}T).\]

Proof.: \[=_{t=1}^{T}_{a p_{t}} [( a,_{t})-  a,_{t}-1]\]

Since \(q_{t}^{}\) is a log-concave distribution, so are \(q_{t}\) and \(p_{t}\), which further implies that \( a,_{t}\) follows a log-concave distribution. Furthermore,

\[_{a p_{t}}[^{2} a,_{t}^{2}]_{a p_{t}}[^{2} a_{t}^{}_{t}^{-1}aa^{}_{t}^{-1}a_{t} ] 2^{2}\|a_{t}\|_{_{t}^{-1}}^{2} 2^{2}d ^{2},\]

where we use Lemma J.2 in the second-last inequality. By Lemma 6 of Ito et al. (2020), we have

\[_{t=1}^{T}_{a p_{t}}[ ( a,_{t})-  a,_{t}-1]_{t=1}^{T} _{a p_{t}}[ a,_{t} ^{2}] 2_{t=1}^{T}\|a_{t}\|_{_{t}^{-1}}^{2}  2^{2}dT.\]

Proof of Theorem 5.2.: Combining Eq. (23), Eq. (24), and Lemma H.1, Lemma H.2, Lemma H.3, Lemma H.4, we see that the regret is bounded by

\[}(}{}+ dT+_{t} \|u-x_{t}\|_{_{t}^{-1}}(d+C)+dC+ d )-\|u\|_{B_{T}}^{2}\] \[}(}{}+ dT+  d)+T+dC^{2}}{}\] (AM-GM inequality)

Choosing optimal \(\) and \(\) leads to \(}(T}+dC)\)Dimension Reduction for Continuous Exponential Weights

First, the intrinsic dimension of \(\) can be defined as the following:

**Definition 1**.: _The intrinsic dimension of \(\) is defined as_

\[()=((- )),\]

_where \(-\{x-x^{}:x,x^{}\}\)._

A convex region \(^{n}\) can be translated and rotated so that it entirely lies in \(^{m}\) where \(m=()\) and has non-zero volume in \(^{m}\). We more precisely define this transformation below.

**Definition 2**.: _Let \(^{n}\) be a convex region with \(()=m\). We define \(:^{n}^{m}\) as the following linear transformation:_

\[(x) ZMx,\]

_where \(M^{n n}\) is a rotation matrix (i.e., orthogonal matrix) such that for any \(v-\), \(Mv\) has non-zero elements only in the first \(m\) coordinates (this is always possible by the definition of \(()\) in Definition 1), and_

\[Z=1&0&&0&0&&0\\ 0&1&&0&0&&0\\ &&&&&&\\ 0&0&&1&0&&0^{m n}\]

_extracts the first \(m\) coordinates of a given \(n\)-dimensional vector._

**Lemma I.1**.: _For any \(x\) and any \(^{n}\),_

\[ x,=(x),()+f(,),\]

_where \(f(,)\) is some quantity that only depends on \(\) and \(\) but not \(x\)._

Proof.: Let \(x,x^{}\). By the definition of \(\), we have

\[(x)-(x^{}),()= ZM(x-x^{}), ZM\,.\]

By the choice of \(M\) in Definition 2, \(M(x-x^{})\) only has non-zero elements in the first \(m\) coordinates. Furthermore, since \(Z\) extracts the first \(m\) coordinates, we have

\[ ZM(x-x^{}),ZM =_{i=1}^{m}(M(x-x^{}))_{i}(M)_{i}\] \[=_{i=1}^{n}(M(x-x^{}))_{i}(M)_{i}\] \[= M(x-x^{}),M\] \[= x-x^{},\,. 28.452756pt(M^{}=M ^{-1}M)\]

Thus,

\[ x,-(x),()= x^{}, -(x^{}),()\,,\]

meaning that the value of \( x,-(x),()\) is shared by all \(x\). Defining this value as \(f(,)\) finishes the proof. 

We consider the continuous exponential weight algorithm (Algorithm 5) running on \(()^{m}\):In Algorithm 5, we require that the inverse mapping of \(\) exists. This is true because for any \(x,x^{}\), we have \(\|(x)-(x^{})\|=\|ZM(x-x^{})\|=\|M(x-x^{})\|=\|x-x^{ }\|\), and thus \(\) cannot map \(x,x^{}\) with \(x x^{}\) to the same point.

**Theorem I.2**.: _Let \(q_{t}()\) be the distribution such that \(x q_{t}\) is equivalent to first drawing \(y p_{t}\) and then taking \(x=^{-1}(y)\). Algorithm 5 ensures for any \(x\),_

\[_{t=1}^{T} x,_{t}+b_{t}-_{t=1}^{T} _{x q_{t}}[ x,_{t}+b_{t} ]+_{t=1}^{T}_{x q _{t}}[( x,_{t})-  x,_{t}-1].\]

Proof.: Note that Algorithm 5 is a standard continuous exponential weight algorithm over reward vectors \((_{t})\) and in the space of \(()^{m}\). By the standard analysis (see, e.g., Ito et al. (2020); Zimmert and Lattimore (2022)), we have for any sequence \(_{1},,_{T}\) and any \(y()\),

\[_{t=1}^{T} y,(_{t})+(b_{t})- _{t=1}^{T}_{y p_{t}}[ y,(_{t})+ (b_{t})]\] \[+_{t=1}^{T}_ {y p_{t}}[( y,(_{t}) +_{t})-( y,(_{t})+ _{t})-1].\]

By Lemma I.1, the above implies

\[_{t=1}^{T}^{-1}(y),_{t}+b_{t}- _{t=1}^{T}_{y p_{t}}[^{-1}(y),_ {t}+b_{t}]\] \[+_{t=1}^{T}_ {y p_{t}}[(^{-1}(y),_{t} - f(,_{t})+_{t})-(^{-1}( y),_{t}- f(,_{t})+_{t})-1],\]

which further implies that for any \(x\),

\[_{t=1}^{T} x,_{t}+b_{t}-_{t=1}^{T} _{x q_{t}}[ x,_{t}+b_{t} ]+_{t=1}^{T}_{x q _{t}}[( x,_{t})-  x,_{t}-1]\]

by the definition of \(q_{t}\) and by letting \(_{t}= f(,_{t})\). 

## Appendix J Computationally Efficient Algorithm for Adversarial \(C\) Bound

In this section, we present Algorithm 6, a polynomial-time algorithm that ensures \(}(d^{3}+d^{}C)\) regret. The algorithm is based on the continuous exponential weight algorithm in the original feature space Ito et al. (2020); Zimmert and Lattimore (2022), with the bonus construction similar to Lee et al. (2020).

### Preliminaries for Entropic Barrier

Entropic barrierFor any convex body \(\), the family of exponential distribution is

\[p_{w}(x)=x)\{y\}}{_{} (w^{}y)y}.\]

For any \(x\), there is a unique \(w(x)\) such that \(_{y p_{w(x)}}[y]=x\). The entropic barrier \(F(x)\) is the negative entropy of \(p_{w(x)}\). Namely

\[F(x)= p_{w(x)}(y)(p_{w(x)}(y))y\]

We have \( F(x)=w(x)\) and \(^{2}F(x)=_{y p_{w(x)}}[(y-x)(y-x)^{}]\). We know that \(F(x)\) is a \(d\)-self-concordant barrier on \(\).

The equivalence of mean-oriented FTRL and continuous exponential weightsConsider FTRL with entropic barrier as the regularizer that solves \(x_{t}\) for round \(t[T]\) following

\[x_{t+1}=*{argmax}_{x}\{ x,_{s =1}^{t}_{s}-}\}.\]

This is equivalent to

\[ F(x_{t+1})=_{t}_{s=1}^{t}_{s}.\]

Given that \(_{y p_{w(x_{t+1})}}[y]=x_{t+1}\) and \( F(x_{t+1})=w(x_{t+1})\), playing \(x_{t+1}\) yields the same expected reward as playing according to distribution \(p_{w(x_{t+1})}\) where \(w(x_{t+1})=_{t}_{s=1}^{t}_{s}\). Thus, we have \(p_{w(x_{t+1})}(x)(_{t} x,_{s=1}^{t} _{s})\) for \(x\).

### Auxiliary Lemmas

**Lemma J.1** (Lemma 1 of Ito et al. (2020)).: _If \(x\) follows a log-concave distribution \(p\) over \(^{d}\) and \(_{x p}[xx^{}] I\), we have_

\[[\|x\|_{2}^{2} d^{2}] d(1-).\]

_for arbitrary \(>0\)._

**Lemma J.2**.: _With the choice of \( 4(10dT)\), we have_

\[|_{a p_{t}}[f(a)]-_{a}}[f(a)]| 10d (-)}\]

_for any \(f:[-1,1]\) and_

\[_{a p_{t}}[aa^{}]_{a}}[aa^{}]_{a p_{t}}[aa^{}].\]

Proof.: The proof follows that of Lemma 4 of Ito et al. (2020), with the observation that \(p_{t}\) is a log-concave distribution. 

**Lemma J.3** (Lemma 14 of Zimmert and Lattimore (2022)).: _Let \(f\) be a \(\)-self-concordant barrier for \(^{d}\). Then for any \(u,x\),_

\[\|u-x\|_{^{2}f(x)}-^{} u-x, f(x) +4^{}+2\]

_where \(^{}=}+}}{6}\) (\(^{}\) for \( 1\))._

Minkowsky Functions.The Minkowsky function of a convex body \(\) with the pole at \(w()\) is a function \(_{w}:\) defined as

\[_{w}(u)=\{t>0|\ w+.\}.\] (25)

**Lemma J.4** (Proposition 2.3.2 in Nesterov and Nemirovskii (1994)).: _Let \(f\) be a \(\)-self-concordant barrier on \(^{d}\), and \(u,w()\). Then_

\[f(u)-f(w)((u)}).\]

### Regret Analysis

We perform regret decomposition. For regret comparator \(u^{}\), define \(x^{}=_{x}F(x)\) and \(u=(1-)u^{}+x^{}\). With probability at least \(1-\),

\[_{T} =_{t=1}^{T} u^{}-a_{t},_{t}\] \[=_{t=1}^{T} u-a_{t},_{t}+ {1}{T}_{t=1}^{T} u^{}-x^{},_{t}\] \[=_{t=1}^{T} u-_{t},_{t} +()+2\] (define \[_{t}=_{a_{t}}[a]\] and by Azuma's inequality) \[=_{t=1}^{T} u-x_{t},_{t}+ _{t=1}^{T} x_{t}-_{t},_{t}+()\] \[=^{T} u-x_{t},_{t}- _{t}[_{t}]}_{}+^{T} u-x_{t},_{t}[_{t}]- _{t}}_{}+^{ T} u-x_{t},_{t}+b_{t}}_{}\] \[^{T} u-x_{t},b_{t} }_{}+ T+().\] (26)By standard FTRL analysis, we have

\[}F(x)}{}}_{ }+[_{t=1}^{T}_{x}\{ x-x_{t},_{t}+b_{t}- {}D_{F}(x,x_{t})\}]}_{}.\] (27)

The individual terms **Bias**, **Deviation**, **Bonus**, **Penalty**, **Stability** terms are bounded in Lemma J.6, Lemma J.7, Lemma J.9, Lemma J.10, Lemma J.12.

**Lemma J.5**.: _For any \(t[T]\), if \(a p_{t}\), then with probability of at least \(1-\),_

\[\|a\|_{_{t}^{-1}}( ).\]

Proof.: Define \(y=_{t}^{-}a\). Then \(_{y}[yy^{}]=_{t}^{-}_{a  p_{t}}[aa^{}]_{t}^{-}=I\). Since \(p_{t}\) is a log-concave distribution, and log-concavity is preserved under liner transformation, \(y\) is also log-concave. Applying Lemma J.1 on it leads to

\[[\|a\|_{_{t}^{-1}}^{2} d^{2}]= [\|y\|_{2}^{2} d^{2}] d(1-) 3d (-).\]

Setting \(=3d(-)\), we conclude that with probability at least \(1-\), \(\|a\|_{_{t}^{-1}}^{2} d()^{2}\). 

**Lemma J.6**.: _With probability at least \(1-()\),_

\[(_{t}\|x_{t}-u\|_{_{t}^{-1}} )(T+2+ C).\]

Proof.: The proof is the same as that of Lemma H.1. 

**Lemma J.7**.: \[(_{t}\|u-x_{t}\|_{_{t}^{-1}}d(T/)).\]

Proof.: The proof is the same as that of Lemma H.2. 

**Lemma J.8**.: \[|| d_{2}().\]

Proof.: Our proof is similar to Lemma B.12 in Lee et al. (2020). Let \(\{t_{1},,t_{n+1}\}\) be the rounds such that \(b_{t} 0\). Define \(_{i}=_{j=1}^{i}_{t_{j}}\). For any \(i>1\), since \(_{}(_{t_{i}}-_{i-1})>0\), there exists a vector \(y^{d+1}\) such that \(y^{}_{t_{i}}y>y^{}_{i-1}y\). Thus, \(y^{}_{i}y 2y^{}_{i-1}y\). Let \(z=_{i-1}^{}y\), we have \(z^{}_{i-1}^{-}_{i}_{i-1}^{-}z 2 \|z\|_{2}^{2}\). This implies \(_{}(_{i-1}^{-}_{i}_{i-1}^{-}) 2\). Moreover, we have \(_{}(_{i-1}^{-}_{i}_{i-1}^{-}) 1\) because

\[_{i-1}^{-}_{i}_{i-1}^{-}=_{i-1}^ {-}(_{i-1}+_{t_{i}})_{i-1}^{-} I.\]

Thus, \(_{i})}{(_{i-1})}=(_{i-1}^{- {2}}_{i}_{i-1}^{-}) 2\). By induction, we have \((_{n+1}) 2^{n}(_{1})\). We now give a upper bound for \(_{n+1})}{(_{1})}\). Define \(=a\\ 1\). By AM-GM inequality, we have

\[(_{n+1}_{1}^{-1})=(_{j=1}^{n+1}_ {t_{j}}_{t_{1}}^{-1})((_{j=1 }^{n+1}_{t_{j}}_{t_{1}}^{-1}))^{d}.\]Notice that for any \(t\), \(_{t} I\) and \((_{t})=(I)+(_{t}^{-1})+\|x_{t} \|_{_{t}^{-1}}^{2}\). Thus, we can upper bound the last expression further by

\[((_{j=1}^{n+1}_{t_{j}}))^{ d}()^{d}( )^{d}.\]

Overall, we have \(2^{n}_{n+1})}{(_{1})}(4T/)^{d}\), and thus \(n d_{2}(4T/)\).

**Lemma J.9**.: \[-_{t}\|u-x_{t}\|_{_{ t}^{-1}}+( d^{2} T).\]

Proof.: Let \(=_{t}\|u-x_{t}\|_{_{t}^{-1}}\) and \(t^{*}=*{argmax}_{t}\|u-x_{t}\|_{_{t}^{-1}}\), We discuss two conditions:

* If \(t^{*}\), then \(^{2}_{}\|u-x_{t}\|_{_{}^{ -1}}^{2}\).
* If \(t^{*}\), then \(_{t^{*}}_{}_{}\). Let \(u\\ 1\). This implies \[^{2}=\|u-x_{t^{*}}\|_{_{t^{*}}^{-1}}^{2}=\|\|_{ _{t^{*}}}^{2}_{}\|\|_{_{}}^{2} =_{}\|u-x_{}\|_{_{}^{-1}}^{2},\] where we use the definitions of \(_{t}\) and \(\) in the second and the last equality.

Thus, \(_{t}\|u-x_{t}\|_{_{t}^{-1}}_{} \|u-x_{}\|_{_{}^{-1}}\).

\[_{t=1}^{T} x_{t}-u,b_{t}\] \[=_{} x_{}-u,b_{}\] \[=_{} x_{}-u,- F (x_{})\] \[-}_{}\|u -x_{}\|_{^{2}F(x_{})}+4 d||+||}{^{}}\] (Lemma J.3 and \[F\] is \[d\] -self-concordant barrier) \[-}_{}\|u -x_{}\|_{_{}^{-1}}+( d^{2} T) (^{2}F(x_{})=_{}^{-1} {}_{}^{-1})\] \[-}_{t}\|u-x_{t}\|_{ _{t}^{-1}}+( d^{2} T).\]

**Lemma J.10**.: \[.\]

Proof.: Since \(x^{*}=_{x}F(x)\) and \(_{x^{*}}(u) 1-\) from Eq. (25). We have from Lemma J.4

\[=)}{}.\]

**Lemma J.11** (Lemma 17 in Zimmer and Lattimore (2022)).: _Let \(F\) be the entropic barrier and \(\|w\|_{^{2}F(x_{t})^{-1}}\), then_

\[_{x}\{ x-x_{t},w-D_{F}(x,x_{ t})\} 2\|w\|_{^{2}F(x_{t})^{-1}}^{2}.\]

**Lemma J.12**.: _With probability at least \(1-\),_

\[(^{2}dT+^{2}d^{2} T ).\]

Proof.: Since \(F\) is a \(d\)-self-concordant barrier (Chewi, 2023), we have

\[\|b_{t}\|_{^{2}F(x_{t})^{-1}}=\| F(x_{t})\|_{^{2}F(x _{t})^{-1}}.\]

By Lemma J.2, we have \(_{t}^{-1}_{a_{t}}[aa^ {}]^{-1} 2_{t}^{-1}\), and thus

\[\|_{t}\|_{^{2}F(x_{t})^{-1}}^{2}=\|_ {t}^{-1}a_{t}r_{t}\|_{_{t}}^{2} 2a_{t}^{}_{t} ^{-1}a_{t} 2d^{2}.\]

Thus, \(\|_{t}+b_{t}\|_{^{2}F(x_{t})^{-1}}+ \). If \(+)}\), by Lemma J.11, we have

\[  2_{t=1}^{T}\|_{t}+b_{t}\|_{^{2}F (x_{t})^{-1}}^{2}\] \[ 4_{t=1}^{T}\|_{t}\|_{^{2}F(x_{t })^{-1}}^{2}+4_{}\|b_{}\|_{^{2}F(x_{}) ^{-1}}^{2}\] \[(^{2}dT+^{2}d| |)\] \[(^{2}dT+^{2}d^{2} T)\]

**Theorem J.13**.: _Algorithm 6 ensures with probability at least \(1-\), \(}_{T}=}d^{3}+d^{}C\), where \(}()\) hides \(}(T/)\) factors._

Proof.: Putting Lemma J.6, Lemma J.7, Lemma J.9, Lemma J.10, Lemma J.12 into Eq. (26) and Eq. (27), with \(+)}\) and \(=\), we have with probability at least \(1-()\),

\[}_{t}\|u-x_{t}\|_{_{t}^{-1}}( }(d+C)-)+ }( d^{2}++^{2}d^{2} + dT+).\]

By setting \(=(d+C)\), we have

\[}}(d^{3}+d^{}C++ d^{4}T+ d^{3}C^{2}).\]

Setting \(=+32}\), we get

\[}}(d^{3}+d^{}C).\]

## Appendix K Gap-dependent Misspecification

We consider the same setting as Liu et al. (2023), but remove an assumption for it. Consider bandit learning with general reward function \(f_{0}\) where for any action \(x_{t}^{d}\) at round \(t\), the learner get reward \(y_{t}=f_{0}(x_{t})+_{t}\) where \(_{t}\)s are zero mean, \(\)-sub-Gaussian noise. We assume there exists a linear function \(^{}x\) that could approximate \(f_{0}(x)\) in the following manner.

**Definition 3**.: \[_{x}|x-f_{0}(x)}{f_{0}^{}-f_{0}(x) }|\]

_where \(f_{0}^{}=_{x}f_{0}(x)\) and \(0<1\)._

The algorithm in Liu et al. (2023) only gets \(()\) regret when \(}\) and we improve it to \(}\) by using elimination-based methods in Algorithm 7. For any design \(\) on action set \(\), define

\[G()=_{a}(a)aa^{} g()=_{a}\|a\|_{G()^{-1}}^{2}\]

```
1Input: Action set \(_{1}=\). Initialize \(m_{1}= 64d d(|}{})+16\).
2for\(=1,2,,L\)do
3 Find the approximate G-optimal design \(_{}\) on \(_{}\) with \(g() 2d\) and \(|()| 4d d+16\)
4 Compute \(u_{}(a)= m_{}_{}(a)\) and \(u_{}=_{a_{}}u_{}(a)\)
5 Take each action \(a_{}\) exactly \(u(a)\) times with reward \(y(a)\).
6 Calculate \[_{}=G_{}^{-1}_{a_{}}u(a)ay(a)  G_{}=_{a_{}}u(a)aa^{}\]
7 Update active action set \[_{+1}=\{a A_{}:_{b_{}} _{},b-_{},a }(|}{} )}+}\}\] \[m_{+1} 4m_{}\] ```

**Algorithm 7**Phased Elimination for Misspecification

Define \((x)=f_{}^{}-f_{0}(x)\) as the suboptimal gap at point \(x\). Definition 3 implies the true value function \(f_{0}(x)=^{}x+(x)\) where \(|(x)|(f_{0}^{}-f_{0}(x))=(x)\). We further assume that \(|(x)|(x)\) which captures both standard uniform misspecification and the gap-dependent misspecification. With this assumption, our main result is summarized in Theorem K.1.

**Theorem K.1**.: _For action \(a\), assume \(y(a)=f_{0}(a)+_{a}\) where \(_{a}\) is zero-mean sub-gaussian noise and \(f_{0}(a)=^{}a+(a)\) with \(|(a)|(a)\). If \(}\), with probability of at least \(1-\), we have_

\[_{T}^{^{}}(|/})\]

Proof.: First, with probability of at least \(1-\), for any \(\) and \(b_{}\), we have

\[| b,_{}-| =|b^{}G_{}^{-1}_{a_{}}u(a)ay(a )-b^{}|\] \[=|b^{}G_{}^{-1}_{a_{}}u(a)aa^{ }_{a}+b^{}G_{}^{-1}_{a_{}}u(a)a(a)|\] \[}(|}{ })}+|b^{}G_{}^{-1}_{a_{}}u(a)a (a)|\]

where in the last step, we use standard concentration by Equation (20.2) of Lattimore and Szepesvari (2020) and the apply union bound for all actions.

For the last term, we have

\[|b^{}G_{}^{-1}_{a A_{}}u(a)a| _{c_{}}(c)}u(a))b^{}G_{}^{-1}_{a A_{}}u(a)aa^{} _{}^{-1}b}\] (Cauchy-Schwarz) \[=_{c_{}}(c)^{-1}}^{2}} _{c_{}}(c)}} _{c_{}}(c) 2.\]

Thus, for any \(b_{}\),

\[| b,_{}-|}(|}{})}+2_{a _{}}(a)=}(|}{})}+2_{a_{}} (a)\]

When \(=1\), since \(m_{1}= 256d d(|}{})+16\), we have \(}(|}{})} {1}{2^{4}}\). Moreover, by trivial bound, \(_{a_{1}}(a) 2\) and \(a^{}_{1}\).

We will jointly do two functions. Assume for round \(\), we have \(a^{}_{}\) and \(_{a_{}}(a)}\). We first show \(a^{}_{+1}\). Thus, for any \(b_{}\), given \(}\), since \(m_{}=4^{-1}m_{1}\), we have

\[| b,_{}-|}(|}{})}+2_{a _{}}(a)}}+ }=}\]

From the induction hypothesis, let \(_{}=_{b_{}}(_{},b)\)we have

\[_{}^{}_{}-_{}^{}a^{} ^{}_{}-^{}a^{}+ _{}^{}_{}-^{} _{}}_{}}+a^{ }-_{}^{}a^{}}_{}}\] \[(_{})-f_{0}(a^{})}_{ 0 }+|(_{})|+}\] \[(_{})+} }\]

For \(+1\), the remaining actions \(a_{+1}\) satisfy

\[_{b_{}}(_{},b)-_{},a}(|}{})}+}}+}\]

This implies \(a^{}_{+1}\). Moreover, since \(a^{}_{}\), for \(a_{+1}\), we have

\[(a)=f_{0}^{}-f_{0}(a) =^{}a^{}-^{}a+|(a)|\] \[_{}^{}a^{}-_{ }^{}a+(a)+(-_{})^{}a^{}+( _{}-)^{}a\] \[_{}^{}a^{}-_{ }^{}a+(a)+(-_{})^{}a^{}+( _{}-)^{}a\] \[_{}^{}a^{}-_{}^{}_{}}_{ 0a^{}_{}}+_{}^{}_{ }-_{}^{}}_{}+}}+(a)+}\]

Given \(}\), this implies

\[(a)(}+}+ })(}+}+ })}\]

The above arguments show that as \(\) increases, \(_{a_{}}(a)\) will shrink by \(\) at every step. Since for \(a_{}\), \((a)}=(} (|}{})})\)Finally, given \(L=(T)\), we have

\[=_{=1}^{L}_{a_{}}u_{}(a)(a) _{=1}^{L}m_{}}(|}{})}(|/})\]

When \(|| T^{d}\), we can apply similar covering number arguments as in Appendix E, replacing \(_{1}\) with a \(\)-net of \(\). Combined with Theorem K.1, this yields the result in Theorem 6.2.

Using the hard instance for \(\)-misspecified linear bandits setting in Lattimore et al. (2020), we now show that \(=(})\) for an algorithm to achieve sub-linear regret, proving the above algorithm is optimal in terms of \(\) assumption.

**Theorem K.2**.: _If \(}\) then there exists an instance that \(R_{T}=( T)\)._

Proof.: Using Theorem F.5 in Lattimore et al. (2020), there exist a discrete time-invariant action space \(\{a_{i}^{d}\}_{i=1}^{3T}\) that satisfies these two conditions:

1. \(\|a_{i}\|=1 i\)
2. \( a_{i},a_{j}} i j\)

and let \(^{*}=}a_{i^{*}}\) for some \(i^{*}\), and let misspecification at each round for all non-optimal arms be \(\) to make the true expected reward zero. Defining \(:=(t|i_{s} i^{*} s t)\), we have \([R_{T}]}[]\). Since the observed rewards are independent of \(a_{i^{*}}\) before time \(\), and \(i^{*}\) is chosen randomly, we have \([]\{T,\}\). So,

\[[R_{T}] T}\]

Finally, we have \(-0}}=}\), so choosing \(=(},})\) completes the proof showing linear regret when \(\) is large enough. 

## Appendix L General Reduction from Corruption-Robust Algorithms to Misspecification

In this section, we extend the results of Section 6 to the reinforcement learning setting. We consider episodic MDPs, denoted by a tuple \(=(,,\{P_{h}\}_{h=1}^{H},\{r_{h}\}_{h=1}^{H},s _{1})\) for \(\) the set of states, \(\) the set of actions, \(P_{h}:_{}\) the transition kernel, \(r_{h}:_{}\) the reward, and \(s_{1}\) the starting state. We assume each episode starts in state \(s_{1}\), where the agent takes action \(a_{1}\), transitions to \(s_{2} P_{1}( s_{1},a_{1})\) and receives reward \(r_{1} r_{1}(s_{1},a_{1})\). This proceeds for \(H\) steps at which point the episode terminates and the process resets. We assume that \(_{h=1}^{H}r_{h}\) almost surely (note that the linear bandit setting with rewards in [-1,1] can be incorporated into this with a simple rescaling).

We let \(\) denote a policy, \(_{h}:_{}\), a mapping from states to actions. We denote the value of a policy \(\) on MDP \(\) as \(V_{0}^{,}:=^{,}[_{h=1}^{H}r_{h}]\). We assume access to some function class \(\{\}\). In the MDP setting, we define regret on MDP \(\) as:

\[_{T}^{}:=T_{}V_{0}^{,}-_{t= 1}^{T}V_{0}^{,_{t}}.\]

In the MDP setting, we consider the following notion of misspecification.

**Definition 4** (Misspecification).: _For our environment of interest \(^{}\), there exists some environment \(_{0}\) such that, for each \(f_{h+1}\), \(\), and \((s,a,h)\), we have:_

\[|^{^{},}[r_{h}+f_{h+1}(s_{h+1},a _{h+1}) s_{h}=s,a_{h}=a].\] \[.-^{_{0},}[r_{h}+f_{h+1}(s_{h+ 1},a_{h+1}) s_{h}=s,a_{h}=a]|_{h}^{}(s,a)\]

_and_

\[ f_{h}f_{h}(s,a)=^{_{ 0}}[r_{h}+_{a^{}}f_{h+1}(s_{h+1},a^{}) s_{h}=s,a_{h}=a]\]

_for some \(_{h}^{}(s,a)>0\)._

We make the following assumption on _gap-dependent_ misspecification.

**Assumption 4** (Gap-Dependent Misspecification).: _For any policy \(\), we have_

\[^{^{},}[_{h=1}^{H}_{h}^{ }(s_{h},a_{h})]()\]

_for some \([0,1)\)._

We are interested in relating the above misspecification setting to the corruption-robust setting. In the MDP setting, we allow both the reward and transitions to be corrupted. For some MDP \(\), define the corruption at episode \(t\) and step \(h\) as:

\[_{t,h}(s_{h}^{t},a_{h}^{t}):=_{g\{ [0,H]\}}|(^{h}g-_{b}^{h}g)(s_{h}^{t},a_{h}^{ t})|\]

where

\[^{h}g(s,a):=^{}[r_{h}+_{a^{ }}g(s_{h+1},a^{}) s_{h}=s,a_{h}=a]\]

denotes the Bellman operator, and \(_{b}^{h}\) denotes the corrupted Bellman operator, i.e. \(_{b}^{h}\) denotes the expected reward and next state under the corrupted reward and transition distribution. We denote the total corruption level as

\[C:=_{t=1}^{T}_{h=1}^{H}_{t,h}(s_{h}^{t},a_{h}^{ t}).\]

Note that this definition of corruption encompasses both bandits and RL with function approximation.

Now assume we have access to the following oracle.

**Assumption 5**.: _We have access to a regret minimization algorithm which takes as input \(\) and some \(C^{}\) and with probability at least \(1-\) has regret bounded on \(_{0}\) as_

\[_{T}^{_{0}}_{1}(, T)+_{2}(,T)C^{}\] (28)

_if \(C^{} C\), and by \(HT\) otherwise, for \(C\) as defined above and for (problem-dependent) constants \(_{1}(,T),_{2}(,T)\) which may scale at most logarithmically with \(T\) and \(\)._

Before stating our main reduction from corruption-robust to gap-dependent misspecification, we require the following assumption.

**Assumption 6**.: _For any \(\), we have that there exists some \(f\) such that for all \((s,a,h)\), \(Q_{h}^{_{0},}(s,a)=f_{h}(s,a)\)._

We then have the following result.

**Theorem L.1**.: _Assume our environment satisfies Assumption 4. Then under Assumption 5 and Assumption 6, as long as \(_{2}(,T)}{1-} 1/2\), with probability at least \(1-2\) we can achieve regret bounded as:_

\[_{T}^{^{}} _{1}(,T)+(H {2T(1/)}+H).\]Proof of Theorem L.1.: First, note that by Assumption 4, we can bound

\[_{t=1}^{T}^{^{},_{t}}[_{h=1}^{H} _{h}^{}(s_{h},a_{h})]_{t=1}^{T}(_{t}) _{T}\]

where we abbreviate \(_{T}:=_{T}^{^{}}\). Furthermore, note that under Assumption 4 interacting with \(^{}\) is equivalent to interacting with \(_{0}\) but where the rewards and transitions are corrupted up to level \(_{h}^{}(s,a)\) at \((s,a,h)\).

Relating Regret on \(_{0}\) to \(^{}\).Define the regret on \(_{0}\) as

\[_{T}^{_{0}}:=T_{}^{ _{0},}[_{h=1}^{H}r_{h}]-_{t=1}^{T}^ {_{0},_{t}}[_{h=1}^{H}r_{h}].\]

Under Assumption 4, we have that \(^{^{},^{}}[_{h=1}^{H}_{h}^ {}(s_{h},a_{h})]=0\). Lemma L.2 then implies that

\[^{^{},^{}}[_{h=1}^{H}r_ {h}]=^{_{0},^{}}[_{h=1}^{H}r_{h}]\]

and so

\[^{^{},^{}}[_{h=1}^{H}r_ {h}]_{}^{_{0},}[_{h=1}^{H}r_ {h}].\]

Furthermore, Lemma L.2 also implies

\[|^{_{0},_{t}}[_{h=1}^{H}r_ {h}]-^{^{},_{t}}[_{h=1}^{H}r_{h} ]|^{^{},_{t}}[_{h=1}^{H }_{h}^{}(s_{h},a_{h})].\]

Putting these together we can bound

\[_{T}_{T}^{_{0}}+_{t=1}^{T} ^{^{},_{t}}[_{h=1}^{H}_{h}^{ }(s_{h},a_{h})]_{T}^{_{0}}+ _{T},\]

where the last inequality holds by Assumption 4. Rearranging this gives

\[_{T}_{T}^{_{0}}.\]

Bounding the Regret.Consider running the algorithm of Assumption 5 on \(^{}\) and assume we run with parameter \(C^{}\) which we will choose shortly. From the above observation, this is equivalent to running on \(_{0}\) with corruption level \(_{h}^{}(s,a)\) at \((s,a,h)\). Then by Assumption 5, with probability at least \(1-\) we have regret on \(_{0}\) bounded as

\[_{T}^{_{0}}_{1}(,T) {T}+_{2}(,T)\]

if \(_{t=1}^{T}_{h=1}^{H}_{h}^{}(s_{h}^{t},a_{h}^ {t})\), and by \(HT\) otherwise. Furthermore, by the above argument this then immediately implies a regret bound on \(_{T}\).

Let \(_{1,t}\) denote the event that \(\{_{t^{}=1}^{t}_{h=1}^{H}_{h}^{}(s_{h} ^{t^{}},a_{h}^{t^{}})\}\). Let \(_{2}\) denote the event that for all \(t T\), we have

\[_{t}(_{1}( ,T)+_{2}(,T))+ \{_{1,t}^{c}\},\]

and note that by the above and under Assumption 5 we then have that \(_{2}\) occurs with probability at least \(1-\). For simplicity, for the remainder of the proof we abbreviate \(_{1}:=_{1}(,T)\) and \(_{2}:=_{2}(,T)\).

Note that \(_{h}(s^{t}_{h},a^{t}_{h})[0,H]\) by construction. It follows that, with probability at least \(1-\), via Azuma-Hoeffding,

\[_{t=1}^{T}_{h=1}^{H}_{h}^{}(s^{t}_{h},a^{t}_{h}) _{t=1}^{T}^{_{t}}[_{h=1}^{H}_{h}^{ }(s^{t}_{h},a^{t}_{h})]+H _{T}+H.\]

Denote this event as \(_{3}\).

Now consider choosing

\[=(1-_{2}}{1-})^{-1}(_{1}+H+H)\]

so that

\[=(_{1}+_{2} )+H+H.\]

On \(_{2}_{3}\), assume that

\[<_{T}+H.\] (29)

Let \(t^{}\) denote the minimum time such that

\[_{t=1}^{t^{}}(_{t})+H> _{t=1}^{t^{}-1}(_{t})+H,\]

and note that such a time is guaranteed to exist under (29) and since \( H+H\) by construction so \((_{1})+H H+H\). Furthermore, since \(() H\), we have here that \(_{t=1}^{t^{}-1}(_{t})>- H-H\). We then have

\[_{t^{}-1} =_{t=1}^{t^{}-1}(_{t})\] \[>-H-\] \[=(_{1}+_ {2})\] \[(_{1}-1} +_{2}).\]

However, since by assumption \(_{t=1}^{t^{}-1}(_{t})+H\), on \(_{3}\)\(_{1,t^{}-1}\) holds so on \(_{2}_{3}\) we have that

\[_{t^{}-1}(_{1}-1}+_{2}).\]

This contradicts the above. Therefore, on \(_{2}_{3}\) we must have that \(_{T}+H\), so \(_{1,T}\) holds on \(_{3}\), and so on \(_{2}_{3}\),

\[_{T}(_{1}+ _{2}).\]

From our setting of \(\) we can bound this as

\[_{1}+ _{2}(1-_{2}}{1-})^{-1} (_{1}+H+H).\]

The result follows from some simplification.

**Lemma L.2**.: _For MDPs \(^{},_{0}\) satisfying Definition 4, under Assumption 6 we have_

\[V_{0}^{_{0},}-V_{0}^{^{},}^{ ^{},}[_{h=1}^{H}_{h}^{}(s_{h}, a_{h})].\]Proof.: Let \(r^{}\) denote the reward function on \(_{0}\), and note that under Assumption 6 we have that there exists \(f\) such that \(V_{h}^{_{0},}(s)=f_{h}(s,_{h}(s))\) for all \(,s,h\). Then Lemma E.15 of Dann et al. (2017) gives that

\[V_{0}^{_{0},}-V_{0}^{^{*},} =^{^{*},}_{h=1}^{H}(r_{h}^{ }-r_{h})\] \[+_{h=1}^{H}^{_{0},}[V_{h}^{_{0},}(s_{h+1}) s_{h}]-^{^{*},}[V_{h}^{ _{0},}(s_{h+1}) s_{h}].\]

By Definition 4, we can bound this as

\[^{^{*},}[_{h=1}^{H}_{h}^{}(s_{h},a_{h})].\]

Proof of Corollary 6.2.1.: First, note that under Assumption 3, we have that Assumption 4 holds for \(\) the set of functions linear in \(\), \(=\{(s,a)^{}w\ :\ w^{d}(s,a)^{}w[0,H], s,a\}\), and \(_{h}^{}(s,a)\) of Assumption 4 set to \(H_{h}^{}(s,a)\) for \(_{h}^{}(s,a)\) of Assumption 3. To see this, let \(_{0}\) be the MDP with transitions \((s,a),_{h}()\), and note that this the immediately implies linear realizability on \(_{0}\) (and furthermore that Assumption 6 holds). Furthermore, since the total reward is at most \(H\), it is easy to see that under Assumption 3, we can take \(_{h}^{}(s,a) H_{h}^{}(s,a)\).

Next, note that Theorem 4.2 of Ye et al. (2023) gives an algorithm on \(_{0}\) satisfying Assumption 5 with \(_{1}=}(d^{3}})\) and \(_{2}=}(Hd)\) (assuming that \(_{h=1}^{H}r_{h}\) almost surely). We can then apply Theorem L.1 to obtain the result.

## Appendix M Auxiliary Lemmas

**Lemma M.1** (Lemma 16 of Zimmert and Lattimore (2022)).: _Let \(=X+xx^{}&x\\ x^{}&1\) and \(=Y+yy^{}&y\\ y^{}&1\). Then_

\[D_{G}(,)=D_{G}(X,Y)+\|x-y\|_{Y^{-1}}^{2}\|x-y\|_{Y^{-1}}^{2}.\]

**Lemma M.2** (Lemma 34 of Liu et al. (2023b)).: _Let \(G\) be the log-determinant barrier. For any matrix \(\), if \((_{t}_{t})}\), then_

\[_{}-_{t},- (,_{t})}{} 8(_{t} _{t}).\]

**Lemma M.3** (Strengthened Freedman's inequality (Theorem 9 of Zimmert and Lattimore (2022))).: _Let \(X_{1},X_{2},,X_{T}\) be a martingale difference sequence with a filtration \(_{1}_{2}\) such that \([X_{t}|_{t}]=0\) and \([|X_{t}|_{t}]<\) almost surely. Then with probability at least \(1-\),_

\[_{t=1}^{T}X_{t} 3(,} \}}{})}+2U_{T}(,}\}}{} ),\]

_where \(V_{T}=_{t=1}^{T}[X_{t}^{2}_{t}]\) and \(U_{T}=\{1,_{t[T]}|X_{t}|\}\)._

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]. Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. The claims are validated by detailed proofs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes].

Justification: The paper discuss the limitations of the work when introducing the algorithms. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]. Justification: The paper provides detailed assumptions and proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA]. Justification: This is a theoretical paper. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA]. Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA]. Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA]. Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]. Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical work. There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: The paper poses no such risks. Guidelines:* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: This paper does not use existing assets. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA].

Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.