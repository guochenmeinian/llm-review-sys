# Copycats: the many lives of a publicly available medical imaging dataset

Amelia Jimenez-Sanchez\({}^{1}\) Natalia-Rozalia Avlona\({}^{2}\) Dovile Juodelyte\({}^{1}\) Theo Sourget\({}^{1}\) Caroline Vang-Larsen\({}^{1}\) Anna Rogers\({}^{1}\) Hubert Dariusz Zajac\({}^{2}\) Veronika Cheplygina\({}^{1}\)

\({}^{1}\)IT University of Copenhagen \({}^{2}\)University of Copenhagen {amji,vech}@itu.dk

###### Abstract

Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets' context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.

## 1 Introduction

Datasets are fundamental to the fields of machine learning (ML) and computer vision (CV), from interpreting performance metrics and conclusions of research papers to assessing adverse impacts of algorithms on individuals, groups, and society. Within these fields, medical imaging (MI) datasets are especially important to the safe realization of Artificial Intelligence (AI) in healthcare. Although MI datasets share certain similarities to general CV datasets, they also possess distinctive properties, and treating them as equivalent can lead to various harmful effects. In particular, we highlight three properties of MI datasets: (i) de-identification is required for patient-derived data; (ii) since multiple images can belong to one patient, data splits should clearly differentiate images from each patient; and (iii) metadata containing crucial information such as demographics or hospital scanner is necessary, as models without this information could lead to inaccurate and biased results.

In the past, MI datasets were frequently proprietary, confined to particular institutions, and stored in private repositories. In this particular setting, there is a pressing need for alternative models of data sharing, documentation, and governance. Within this context, the emergence of Community-Contributed Platforms (CCPs) presented a potential for the public sharing of medical datasets. Nowadays, more MI datasets have become publicly available and are hosted on open platforms such as grand-challenges1, or CCP - including companies like Kaggle or HuggingFace.

Although the increasing availability of MI datasets is generally an advancement for sharing and adding public value, it also presents several challenges. First, according to the FAIR (Findable, Accessible, Interoperable, Reusable) guiding principles for scientific data management and stewardship , (meta)data should be released with a clear and accessible data usage license and should be permanently accessible. Second, tracking dataset versions is becoming increasingly difficult, especially when publications use derived versions  or the citation practices are not followed . This hampers the analysis of usage patterns to identify possible ethical concerns that might arise after releasing a dataset , potentially leading to its retraction [89; 60]. To mitigate the harms associated with datasets, ongoing maintenance, and stewardship are necessary . Lastly, rich documentation is essential to avoiding over-optimistic and biased results [15; 68; 32; 125; 86], attributed to a lack of meta-data in MI datasets, such as information linking images to specific patients and their demographics. Documentation needs to reflect all the stages in the dataset development cycle, such as acquisition, storage, and maintenance [51; 40]. Although CCPs offer ways to enhance the redistribution of data's public value and alleviate some of these problems providing structured summaries, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating MI datasets.

In this paper, we investigate MI datasets hosted on CCPs, particularly how they are documented, shared, and maintained. First, we provide relevant background information, highlighting the differences between open MI and CV datasets, especially in the potential for harmful downstream effects of poor documentation and distribution practices (Section 2.1). Second, we present key aspects of data governance in the context of ML and healthcare, specifically affecting MI datasets (Section 2.2). Third, we analyze _access, quality_ and _documentation_ of 30 popular datasets hosted on CCPs (10 medical, 10 computer vision, and 10 natural language processing). We find issues across platforms related to vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata (Section 3). We discuss the limitations of the current dataset management practices and data governance on CCPs, provide recommendations for MI datasets, and conclude with a discussion of limitations of our work and open questions (Section 4).

## 2 Background

### Characteristics of medical imaging datasets

Anatomy of a medical imaging dataset.A MI dataset begins with a collection of images from various imaging modalities, such as X-rays, magnetic resonance imaging (MRI), computed tomography (CT) scans, and others. The scans are often initially captured for a clinical purpose, such as diagnosis or treatment planning, and are associated with a specific patient and their medical data. The scans might undergo various processing steps, such as denoising, registration (aligning different scans together), or segmentation (delineating anatomical structures or pathologies). Clinical experts might then associate the scans with additional information, _e.g_., free text reports or diagnostic labels.

Figure 1: A Medical Imaging (MI) dataset containing images, labels, metadata (patient id, patient sex, etc.), and license (left). After user interaction, on Community-Contributed Platforms we find duplicated data, missing licenses and metadata, which can lead to overoptimistic results (right).

A collection of scans and associated annotations, _i.e._, a MI dataset, might be later used for the purpose of training and evaluating ML models supporting the work of medical professionals . However, before a dataset is "ready" for ML, further steps are required , including cleaning (for example, removing scans that are too blurry), sampling (for example, only selecting scans with a particular disease), and removing identifying patient information. Additional annotations, not collected during clinical practice, may be required to train ML models, _e.g._, organ delineations for patients not undergoing radiotherapy. These annotations might be provided by clinical experts, PhD students, or paid annotators at tech companies.

Not just "small computer vision"!While MI datasets share some similarities with general CV datasets, they also have unique properties. The diversity of image modalities and data preprocessing needed for each specific application is vast. For instance 3D images from modalities like MRI can vary significantly depending on the sequence used. For example, brain MRI sequences (T1-weighted, T2, FLAIR, etc.), are designed to emphasize different brain structures, offering specific physiological and anatomical details. Whole-slide images of histopathology are extremely large (gigapixel) images, making preprocessing both challenging and essential for accurate analysis. A crucial part of this process is stain normalization, which standardizes color variations caused by different staining processes, ensuring consistency across slides for more reliable analysis and comparison . We refer interest readers in knowing more about preparing MI data of different modalities for ML for example to .

Nevertheless, the complexity of medical image data above is often reduced to a collection of ML-library-ready images and labels. Yet treating MI datasets as equivalent to benchmark CV datasets is problematic and leads to harmful effects, also termed _data cascades_ by . _Data cascades_ can lead to degraded model performance, reinforce biases, increase maintenance costs, and reduce trust in AI systems. These problems often stem from poor data quality, lack of domain expertise, and insufficient documentation, which become increasingly difficult to correct once models are deployed.

First, unlike traditional CV datasets, medical images often require de-identification processes to remove personally identifiable data, which are more complex than complete anonymization. Certain attributes like sex and age, need to be preserved for clinical tasks. These attributes are typically included in an "original release" of MI datasets, they might be removed later in a dataset's lifecycle. For example, when medical datasets are shared on CCPs, often only the input desired by ML practitioners remains: inputs (images) and outputs (disease labels), as shown in Figure 1.

Second, MI datasets often include multiple images associated with a single patient. This can occur if a patient has multiple skin lesions, follow-up chest X-rays, or 3D scans split into 2D images. If images from the same patient end up in both training and test data, reported results may be overly optimistic as classifiers memorize patients rather than disease characteristics. Therefore, data splitting at the patient level is crucial to avoid model overfitting. While this practice is common in the MI community, it may be overlooked if datasets are simply shared as a general CV dataset.

Third, MI datasets should contain metadata about patient demographics. Several studies have shown how demographic data may alleviate systematic biases and impact disease classification performance in chest X-rays  and skin lesions . These datasets are often the subject of research on bias and fairness because they include variables for age and sex or gender (typically not described which). However, many MI datasets lack these variables, possibly due to removal in a ML-ifying step rather than actual anonymization. Unlike CV datasets where bias can be identified by annotating individuals in the images based on their gender expression , such information is often unrecoverable from medical images. Additionally, images may be duplicated; see, _e.g._,  for an analysis of the ISIC datasets, with overlaps between versions and duplication of cases between training and test sets.

Finally, MI datasets should include metadata about the origin of scans. Lack of such data may lead to "shortcuts" and other systematic biases. For example, if disease severity correlates with the hospital where the scans were made (general _vs_. cancer clinic), a model might learn the clinic's scanner signature as a shortcut for the disease . In other words, the shortcut is a spurious correlation between an artifact in the image and the diagnostic label. Some examples of shortcuts include patient position in COVID-19 , chest drains in pneumothorax classification , or pen marks in skin lesion classification . High overall performance can hide biases in benchmark evaluations serving underrepresented groups. This cannot be detected without appropriate metadata.

[MISSING_PAGE_EMPTY:4]

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

research outcomes. Besides ISIC, on Kaggle we find other examples of unnecessary duplication of data, see Table 3 of the Supplementary Material for details.

After this finding about ISIC, we examined several other datasets for duplication. On Kaggle, we find 350 datasets related to BraTS (Brain Tumor Segmentation) , and 24 datasets of INBreast , one of them with the rephrased description "I'm just uploading here this data as a backup". Additionally, there are 10 instances of PAD-UFES-20  (also a skin lesion dataset, one instance actually contains data from ISIC). The ACDC (Automated Cardiac Diagnosis Challenge) dataset  consists of MRIs, while ACDC-LungHP (Automatic Cancer Detection and Classification in Lung Histopathology) dataset  contains histopathological images. On Kaggle, we find an example of a dataset titled "ACDC lung" that contains cardiac images.

The lack of documentation for _all_ ML, not just MI datasets, hampers tracking their usage, potentially violates sharing agreements or licenses, and hinders reproducibility. Additionally, due to the characteristics of MI datasets, models trained on datasets missing metadata could result into overoptimistic performance due to data splits mixing patient data, or bias  or shortcuts . We therefore reviewed the documentation on the original websites and related papers for the MI datasets, and found that patient data splits were clearly reported for 6 out of 10 datasets - "clearly reported" means that a field like "patient_id" was provided for each case. However, tracking whether data splits are defined at the subject level for duplicates on CCPs is challenging, as the relevant information is not always in the same location. One must examine the file contents (often requiring downloading the entire dataset) of each duplicated dataset to determine if a field like "patient_id" is available.

Limited implementation of structured summaries.We find that overall HuggingFace follows a much more structured and complete documentation than Kaggle, as reflected in their guides . From our list of MI datasets, on HF we find an instance of MIMIC-CXR with no documentation or reference at all, and other medical datasets (_e.g._ Alzheimer's disease or skin cancer classification) without source citation. We find the lack of source citations for patient-related data deeply concerning. Kaggle automatically computes _usability score_, which is associated with the tag "well-documented" and used for ranking results when searching for a dataset. This score is based on completeness, credibility, and compatibility, we show detailed information about these parameters in Section A.1 of the Supplementary Material. However, we find that even datasets with 100% usability present some issues. For example, based on our observations, the parameter _update frequency_ from maintenance is rarely used. However, an option for this parameter is to set it as "never" while still achieving a high _usability score_. Details about _provenance_ might be filled in on the data card but may be vague, such as "uses internet sources".

We compare the categories analyzed in Kaggle and HuggingFace's data cards with those in Datasheets . Despite making various efforts to integrate dataset documentation, such as the recent inclusion of Croissant , a metadata format designed for ML-ready datasets, we have noticed a prevalent issue:

Figure 2: Representation of the storage size for ISIC (skin lesion) datasets. While the ISIC website hosts a total of 38 GB of data (left), on Kaggle there are a total of 640 datasets related to ISIC (some preprocessed, other with additional annotations), that sum up to 2.35 TB of data (right). Each block on the (right) represents a single instance of ISIC-derived dataset on Kaggle. Block size represents dataset size. Data was retrieved on May 15, 2024.

many of the documentation fields remain empty. While these platforms strive to provide structured summaries, the practical outcome often falls short. Overall, we find composition and collection process are the two fields most represented; motivation of the creation of the dataset is rarely included in the general description of the dataset; information about preprocessing/cleaning/labeling or about uses is usually missing. Only for HuggingFace the field _task_categories_ can point to some possible uses, potentially enabling systematic analysis of a specific task or tag. Kaggle provides a parameter for maintenance of the dataset, although we have already mentioned its limitations. HuggingFace does not provide a specific parameter for maintenance but it is possible to track on their website the history of files and versions. We detail the parameter categorization in Table 2 (Suppl. Material).

## 4 Discussion

Asymmetry between open data and proprietary datasets.Commercial AI systems in clinical settings are unlikely to rely solely on open MI datasets for training. They ensure data quality through agreements or obtaining high-quality medical images . Companies providing proprietary MI datasets or labeling services handle challenges such as licensing, documentation, and data quality, offering greater customization and flexibility. Such proprietary datasets remain unaffected by the mentioned challenges [95; 130]. Similarly,  have shown how regulatory compliance and internal organizational requirements, transverse and often define dataset quality.

This asymmetry between the issues of open data and the value offered by proprietary datasets highlights the shortcomings of publicly available MI data. While open data initiatives like CCPs offer the potential to redistribute data value for the common good and public interest, the current status of MI datasets falls short in reliably training high-performing, equitable, and responsible AI models. Due to these limitations, we suggest rethinking and evaluating open datasets within CCPs through the concepts of _access, quality, and documentation_ drawing upon the FAIR principles . We argue that these concerns need to be accounted for if the MI datasets are to live up to the ideals of open data.

Access to open datasets should be predictable, compliant with open licensing, and persistent.In this paper, we show that a proper dataset infrastructure (both legal and technical) is crucial for their effective utilization. Open datasets must be properly licensed to prevent harm to end-users by models trained on legally ambiguous open data with the potential for bias and unfairness [104; 69]. Moreover, vague licensing pushes the users of open datasets into a legal grey zone .  noticed such a legal gap in the "inappropriate" use of open AI models and pointed out the danger of their possible unrestricted and unethical use. To ensure the responsible use of AI models, they envisioned enforceable licensing. Legal clarity should also span persistent and deterministic storage. The most popular ML datasets are mostly hosted by established academic institutions. However, the CCPs host a plethora of duplicated or altered MI datasets. Instead of boosting the opportunities for AI creators, this abundance may become a hindrance when _e.g._, developers cannot possibly track changes introduced between different versions of a dataset. We argue that open data has to be predictably and persistently accessible under clear conditions and for clear purposes.

Open datasets should be evaluated against the context of real-world use.The understanding of high-quality data for AI training purposes is constantly evolving . After a thorough evaluation focused on real-world use, MI datasets, once considered high-quality [52; 118; 22; 113], were revealed to contain flaws (chest drains, dark corners, ruler markers, etc.) questioning their clinical usefulness [86; 57; 15; 115]. Maintaining open datasets is often an endeavor that is too costly for their creators, resulting in the deteriorating quality of available datasets. Moreover, we showed the prevalence of information about shortcuts and missing metadata in MI datasets hosted on CCPs. These issues can reduce the clinical usefulness of developed systems and, in extreme scenarios, potentially cause harm to the intended beneficiaries. We encourage the MI and other ML communities to expand the understanding of high-quality data by incorporating rich metadata and emphasizing real-world evaluations, including testing to uncover biases or shortcuts [86; 43].

Datasets documentation should be complete and up-to-date.Research has shown that access to large amounts of data does not necessarily warrant the creation of responsible and equitable AI models . Instead, it is the connection between the dataset's size and the understanding of the work that resulted in the creation of a dataset. This connection is the premise behind the creation of proprietary datasets designed for use in private enterprises. When that direct connection is broken, a fairly common scenario in the case of open datasets, the knowledge of the decisions taken during dataset creation is lost. Critical data and data science scholars are concerned about the social and technical consequences of using such undocumented data. Thus, a range of documentation frameworks were proposed . Each documentation method slightly differs, focusing on various aspects of dataset quality. However, their overall goal is to introduce greater transparency and accountability in design choices. These conscious approaches aim to foster greater reproducibility and contribute to the development of responsible AI. Unfortunately, as shown in this paper, the real-world implementation of these frameworks is lacking. Even when a CCP provides a documentation framework, the content rarely aligns with the frameworks' principles. CCPs could take inspiration from PhysioNet , which implements checks on new contributions. Any new submissions are first vetted7 by the editors and may require re-submission if the expected metadata is not provided. When the supplied documentation does not adhere to the frameworks' principles, it fails to fulfill its intended purpose, placing users of open datasets at a disadvantage compared to users of proprietary datasets. We note that while we talk about completeness of documentation and the frameworks provide guidelines on what kind of information that might entail, it is not clear how one would quantify that the documentation is 86% complete in a way that reflects the data stakeholders' needs and is not merely a box-ticking exercise.

CCPs could benefit from commons-based governance.Data governance can help mitigate the issues of accountability, fairness, discrimination, and trust. Inspired by the Wikipedia model , we recommend that CCPs implement norms and principles derived from this commons-based governance model. We suggest incorporating at least the roles of _data administrator_, and _data steward_. We define the role of _data administrator_ as the first-level of data stewardship, a sanctioning mechanism that ensures proper (1) licensing, (2) persistent identifiers, and (3) completeness of metadata for open MI datasets that enter the platform. We define as the second-level of data stewardship, the role of _data steward_, who will be responsible for the ongoing monitoring of the (1) maintenance, (2) storage, and (3) implementation of documentation practices.

Nevertheless, these data stewardship proposals, as a commons-based governance model, need further exploration within a broader community of CCP practitioners. Recognizing the limited resources (monetary and/or human labor) in CCP initiatives, we are very careful in suggesting a complex governance system that would solely rely on the unpaid labor of dataset creators. Instead, we propose this direction for future applied research to enhance the dataset management and stewardship of MI datasets on CCP through commons-based approaches. We sincerely hope that more institutions will support efforts to improve the value of open datasets, which will require additional structural support, such as permanent and paid roles for data stewards .

Initiatives to work on data and improve the data lifecycle.Several fairly recent initiatives aim to address the overlooked role of datasets like the NeurIPS Datasets and Benchmarks Track or the Journal of Data-centric Machine Learning Research (DMLR). New develop platforms, like the data providence explorer , help developers track and filter thousands of datasets for legal and ethical issues, and allow scholars and journalists to examine the composition and origins of popular AI datasets. Other newly born initiative is Croissant , a metadata format for ML-ready datasets, which is currently supported by Kaggle, HuggingFace and other platforms. ML and NLP conferences have started to require ethics statements and various checklists with submissions  for the reviewer use, and even include them in the camera-ready versions of accepted papers  to incentive better documentation. Such checklists typically include questions about data license and documentation, and they could be extended to help develop the norm of not just sharing, but also documenting any new data accompanying research papers, or encourage the use of the 'official' documented dataset versions.

In the MI context, conferences like MICCAI have incorporated a structured format for challenge datasets to ensure high-quality data. Initiatives like Project MONAI  introduce a platform to facilitate collaborative frameworks for medical image analysis and accelerate research and clinical collaboration. Drawing inspiration from CV, benchmark datasets are now emerging in MI, such as MedMNIST  and MedMNIST v2 . These multi-dataset benchmarks have their pros and cons. They are hosted on Zenodo, which facilitates version control, provides persistent identifiers, and ensures proper storage. However, the process of standardizing MI datasets to the CV format means they lack details about patient demographics (such as age, gender, and race), information on the medical acquisition devices used, and other metadata, including patient splits for training and testing. Recent works have investigated data sharing and citations practices at MICCAI and MIDL , and reproducibility and quality of MIDL public repositories .

More insights needed from all people involved.A limitation of our study is that it is primarily based on our quantitative evidence and our subjective perceptions of the fields and practices we describe of a limited number of screened datasets, yet the most cited ones. However, a recent study  has quantitatively and qualitatively confirmed our observations about the lack of documentation for datasets on HuggingFace. However, we did not reach out to Kaggle or HuggingFace. To gain a better understanding of data curation, maintenance, and re-use practices, it would be valuable to do a qualitative analysis with MI and ML practitioners to understand their use of datasets. For example,  is a recent study, based on interviews with researchers from companies and the public health sector, of how several medical datasets were created. It would be interesting to investigate how researchers select datasets to work on, looking beyond mere correlations with popularity and quantitative metrics. We might be able to learn valuable lessons from other communities that we have not explored in this paper, for example neuroimaging (which might appear to be a subset of medical imaging, but in terms of people and conferences is a fairly distinct community), where various issues around open data have been explored .

However, we should not forget that understanding research practices around datasets is not just of relevance to ML and adjacent communities. These datasets have broader importance as these datasets are affecting people who are not necessarily represented at research conferences, so further research should involve these most affected groups . Public participation in data use , alternative data sharing, documenting, and governance models  are crucial to addressing power imbalances and enhancing data's generation of value as a common good . Furthermore, neglecting the importance of recognizing and prioritizing the foundational role of data when working with MI datasets can lead to downstream harmful effects, such as _data cascades_. **In conclusion**, our observations reveal that the existing CCP governance model falls short of maintaining the necessary quality standards and recommended practices for sharing, documenting, and evaluating open MI datasets. Our recommendations aim to promote better data governance in the context of MI datasets to mitigate these risks and uphold the reliability and fairness of AI models in healthcare.