# Conditional independence testing under

misspecified inductive biases

 Felipe Maia Polo

Department of Statistics

University of Michigan

felipemaiapolo@gmail.com &Yuekai Sun

Department of Statistics

University of Michigan

yuekai@umich.edu &Moulinath Banerjee

Department of Statistics

University of Michigan

moulib@umich.edu

###### Abstract

Conditional independence (CI) testing is a fundamental and challenging task in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step; we refer to this class of tests as _regression-based_ tests. Although these methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors of interest, their behavior is less understood when they fail due to misspecified inductive biases; in other words, when the employed models are not flexible enough or when the training algorithm does not induce the desired predictors. Then, we study the performance of regression-based CI tests under misspecified inductive biases. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a regression-based CI test robust against misspecified inductive biases. Finally, we conduct experiments with artificial and real data, showcasing the usefulness of our theory and methods.

## 1 Introduction

Conditional independence (CI) testing is fundamental in modern statistics and machine learning (ML). Its use has become widespread in several different areas, from (i) causal discovery [12; 24; 34; 11] and (ii) algorithmic fairness , to (iii) feature selection/importance [5; 37] and (iv) transfer learning . Due to its growing relevance across different sub-fields of statistics and ML, new testing methods with different natures, from regression to simulation-based tests, are often introduced.

Regression-based CI tests, _i.e._, tests based on supervised learning methods, have become especially attractive in the past years due to (i) significant advances in supervised learning techniques, (ii) their suitability for high-dimensional problems, and (iii) their simplicity and easy application. However, regression-based tests usually depend on the assumption that we can accurately approximate the regression functions or Bayes predictors of interest, which is hardly true if (i) either the model classes are misspecified or if (ii) the training algorithms do not induce the desired predictors, _i.e._, if we have misspecified inductive biases. Misspecified inductive biases typically lead to inflated Type-I error rates but also can cause tests to be powerless. Even though these problems can frequently arise in practical situations, more attention should be given to theoretically understanding the effects of misspecification on CI hypothesis testing. Moreover, current regression-based methods are usually not designed to be robust against misspecification errors, making CI testing less reliable. In this work, we study the performance of three major regression-based conditional independence tests under misspecified inductive biases and propose the Rao-Blackwellized Predictor Test (RBPT), which is more robust against misspecification.

With more details, our main contributions are:

* We present new robustness results for three relevant regression-based conditional independence tests: (i) Significance Test of Feature Relevance (STFR) , (ii) Generalized Covariance Measure (GCM) test , and (iii) REgression with Subsequent Independence Test (RESIT) [42; 24; 12]. Namely, we derive approximations or upper bounds for the testing errors that explicitly depend on the level of misspecification.
* We introduce the Rao-Blackwellized Predictor Test (RBPT), a modification of the Significance Test of Feature Relevance (STFR)  test that is robust against misspecified inductive biases. In contrast with STFR and previous regression/simulation-based1 methods, the RBPT does _not_ require models to be correctly specified to guarantee Type-I error control. We develop theoretical results about the RBPT, and experiments show that RBPT is robust when controlling Type-I error while maintaining non-trivial power.

## 2 Preliminaries

**Conditional independence testing.** Let \((X,Y,Z)\) be a random vector taking values in \(^{d_{X} d _{Y} d_{Z}}\) and \(\) be a fixed family of distributions on the measurable space \((,)\), where \(=()\) is the Borel \(\)-algebra. Let \((X,Y,Z) P\) and assume \(P\). If \(_{0}\) is the set of distributions in \(\) such that \(X Y Z\), the problem of conditional independence testing can be expressed in the following way:

\[H_{0}:P_{0} H_{1}:P_{0}\]

In this work, we also write \(H_{0}:X Y Z\) and \(H_{1}:X Y Z\). We assume throughout that we have access to a dataset \(^{(n+m)}=\{(X_{i},Y_{i},Z_{i})\}_{i=1}^{n+m}\) independent and identically distributed (i.i.d.) as \((X,Y,Z)\), where \(^{(n+m)}\) splits into a test set \(^{(n)}_{te}=\{(X_{i},Y_{i},Z_{i})\}_{i=1}^{n}\) and a training set \(^{(m)}_{tr}=\{(X_{i},Y_{i},Z_{i})\}_{i=n+1}^{n+m}\). For convenience, we use the training set to fit models and the test set to conduct hypothesis tests, even though other approaches are possible.

**Misspecified inductive biases in modern statistics and machine learning.** Traditionally, misspecified inductive biases in statistics have been linked to the concept of model misspecification and then strictly related to the chosen model classes. For instance, if the best (Bayes) predictor for \(Y\) given \(X\), \(f^{*}\), is a non-linear function of \(X\), but we use a linear function to predict \(Y\), then we say our model is misspecified because \(f^{*}\) is not in the class of linear functions. In modern machine learning and statistics, however, it is known that the training algorithm also plays a crucial role in determining the trained model. For example, it is known that training overparameterized neural networks using stochastic gradient descent bias the models towards functions with good generalization [14; 33]. In addition, D'Amour et al.  showed that varying hyperparameter values during training could result in significant differences in the patterns learned by the neural network. The researchers found, for instance, that models with different random initializations exhibit varying levels of out-of-distribution accuracy in predicting skin health conditions for different skin types, indicating that each model learned distinct features from the images. The sensitivity of the trained model concerning different training settings suggests that _even models capable of universal approximation may not accurately estimate the target predictor_ if the training biases do not induce the functions we want to learn.

We present a toy experiment to empirically demonstrate how the training algorithm can prevent us from accurately estimating the target predictor even when the model class is correctly specified, leading to invalid significance tests. We work in the context of a high-dimensional (overparameterized) regression with a training set of \(250\) observations and \(500\) covariates. We use the Generalized Covariance Model (GCM) test2 to conduct the CI test. The data are generated as

\[Z N(0,I_{500}),\ X Z N(_{X}^{}Z,1),\ \ Y X,Z N(_{Y}^{}Z,1),\]

where the first five entries of \(_{X}\) are set to \(20\), and the remaining entries are zero, while the last five entries of \(_{Y}\) are set to \(20\), and the remaining entries are zero. This results in \(X\) and \(Y\) being conditionally independent given \(Z\) and depending on \(Z\) only through a small number of entries. Additionally, \([X Z]=_{X}^{}Z\) and \([Y Z]=_{Y}^{}Z\), indicating that the linear model class is correctly specified. Toperform the GCM test, we use LASSO (\(\|\|_{1}\) penalization term added to empirical squared error) and the minimum-norm least-squares solution to fit linear models that predict \(X\) and \(Y\) given \(Z\). In this problem, the LASSO fitting approach provides the correct inductive bias since \(_{X}\) and \(_{Y}\) are sparse. We set the significance level to \(=10\%\) and estimate the Type-I error rate for \(100\) different training sets. Figure 1 provides the Type-I error rate empirical distribution and illustrates that, despite using the same model class for both fitting methods, the training algorithm induces an undesired predictor in the minimum-norm case, implying an invalid test most of the time. On the other hand, the LASSO approach has better Type-I error control. In Appendix A, we give a similar example but using the Significance Test of Feature Relevance (STFR) .

In this work, we formalize the idea of misspecified inductive biases in the following way. Assume that a training algorithm \(\) is used to choose a model \(^{(m)}=(_{t}^{(m)})\) from the class \(^{(m)}\). We further assume that the sequence \((^{(m)})_{m}\) converges to a limiting model \(g^{*}\) in a relevant context-dependent sense. We use different notions of convergence depending on the specific problem under consideration, which will be clear in the following sections. We say that \(g^{*}\) "carries misspecified inductive biases" if it does not equal the target Bayes predictor or regression function \(f^{*}\). There are two possible reasons for \(g^{*}\) carrying misspecified biases: either the limiting model class is small and does not include \(f^{*}\), or the training algorithm \(\) cannot find the best possible predictor, even asymptotically.

**Notation.** We write \(_{P}\) and \(_{P}\) for the expectation and variance of statistics computed using i.i.d. copies of \((X,Y,Z) P\). Consequently, \(_{P}(A)=_{P}_{A}\), where \(_{A}\) is the indicator of an event \(A\). If \(_{P}\) and \(_{P}\) are conditioned on some other statistics, we assume those statistics are also computed using i.i.d. samples from \(P\). As usual, \(\) is the \(N(0,1)\) distribution function. If \((a_{m})_{m}\) and \((b_{m})_{m}\) are sequences of scalars, then \(a_{m}=o(b_{m})\) is equivalent to \(a_{m}/b_{m} 0\) as \(m\) and \(a_{m}=b_{m}+o(1)\) means \(a_{m}-b_{m}=o(1)\). If \((V^{(m)})_{m}\) is a sequence of random variables, where \(V^{(m)}\) as constructed using i.i.d. samples of \(P^{(m)}\) for each \(m\), then (i) \(V^{(m)}=o_{p}(1)\) means that for every \(>0\) we have \(_{P^{(m)}}(|V^{(m)}|>) 0\) as \(m\), (ii) \(V^{(m)}=_{p}(1)\) means that for every \(>0\) there exists a \(M>0\) such that \(_{m}_{P^{(m)}}(|V^{(m)}|>M)<\), (iii) \(V^{(m)}=a_{m}+o_{p}(1)\) means \(V^{(m)}-a_{m}=o_{p}(1)\), (iv) \(V^{(m)}=o_{p}(a_{m})\) means \(V^{(m)}/a_{m}=o_{p}(1)\), and (v) \(V^{(m)}=_{p}(a_{m})\) means \(V^{(m)}/a_{m}=_{p}(1)\). Finally, let \((V^{(m)}_{P})_{m,P}\) be a family of random variables that distributions explicitly depend on \(m\) and \(P\). We give an example to clarify what we mean by "explicitly" depending on a specific distribution. Let \(V^{(m)}_{P}=_{i=1}^{m}(X_{i}-_{P})\), where \(_{P}=_{P}[X]\). Here, \(V^{(m)}_{P}\) explicitly depends on \(P\) because of the quantity \(_{P}\). In this example, \(X_{i}\)'s outside the expectation can have an arbitrary distribution (unless stated), _i.e._, could be determined by \(P\) or any other distribution. With this context, (i) \(V^{(m)}_{P}=o_{}(1)\) means that for every \(>0\) we have \(_{P}_{P}(|V^{(m)}_{P}|>) 0\) as \(m\), (ii) \(V^{(m)}_{P}=_{p}(1)\) means that for every \(>0\) there exists a \(M>0\) such that \(_{m,P}_{P}(|V^{(m)}_{P}|>M)<\), (iii) \(V^{(m)}_{P}=o_{}(a_{m})\) means \(V^{(m)}_{P}/a_{m}=o_{P}(1)\), and (iv) \(V^{(m)}_{P}=_{p}(a_{m})\) means \(V^{(m)}_{P}/a_{m}=o_{}(1)\).

**Related work.** There is a growing literature on the problem of conditional independence testing regarding both theoretical and methodological aspects3. From the methodological point of view, there is a great variety of tests with different natures. Perhaps, the most important groups of tests are (i) simulation-based tests [5; 3; 4; 32; 35; 20], (ii) regression-based tests [40; 24; 42; 38; 31; 7], (iii) kernel-based tests [10; 8; 34; 30], and (iv) information-theoretic based tests [29; 16; 39]. Due to the advance of supervised and generative models in recent years, regression and simulation-based tests have become particularly appealing, especially when \(Z\) is not low-dimensional or discrete. A related but different line of research is constructing a lower confidence bound for conditional dependence of \(X\) and \(Y\) given \(Z\). In that work, the authors propose a method that relies on computing the conditional expectation of a possibly misspecified regression model, which can be related to

Figure 1: Type-I error rate is contingent on the training algorithm and not solely on the model classes. Unlike the minimum-norm solution, the LASSO fit gives the correct inductive bias in high-dimensional regression, providing better Type-I error control.

our method presented in Section 4. Despite the relationship between methods, their motivations, assumptions, and contexts are different.

Simulation-based tests depend on the fact that we can, implicitly or explicitly, approximate the conditional distributions \(P_{X|Z}\) or \(P_{Y|Z}\). Two relevant simulation-based methods are the conditional randomization and conditional permutation tests (CRT/CPT) [5; 3; 4; 35]. For these tests, Berrett et al.  presents robustness results showing that we can _approximately_ control Type I error even if our estimates for the conditional distributions are not perfect and we are under a finite-sample regime. However, it is also clear from their results that CRT and CPT might not control Type I error asymptotically when models for conditional distributions are misspecified. On the other hand, regression-based tests work under the assumption that we can accurately approximate the conditional expectations \([X Z]\) and \([Y Z]\) or other Bayes predictors, which is hardly true if the modeling and training inductive biases are misspecified. To the best of our knowledge, there are no published robustness results for regression-based CI tests like those presented by Berrett et al. . We explore this literature gap.

## 3 Regression-based conditional independence tests under misspecified inductive biases

This section provides results for the Significance Test of Feature Relevance (STFR) . Due to limited space, the results for the Generalized Covariance Measure (GCM) test  and the REgression with Subsequent Independence Test (RESIT) [42; 24; 12] are presented in Appendix A. From the results in Appendix A, one can easily derive a double robustness property for both GCM and RESIT, implying that not all models need to be correctly specified or trained with the correct inductive biases for Type-I error control.

### Significance Test of Feature Relevance (STFR)

The STFR method studied by Dai et al.  offers a scalable approach for conducting conditional independence testing by comparing the performance of two predictors. To apply this method, we first train two predictors \(_{1}^{(m)}:\) and \(_{2}^{(m)}:\) on the training set \(_{tr}^{(m)}\) to predict \(Y\) given \((X,Z)\) and \(Z\), respectively. We assume that candidates for \(_{2}^{(m)}\) are models in the same class as \(_{1}^{(m)}\) but replacing \(X\) with null entries. Using samples from the test set \(_{tc}^{(n)}\), we conduct the test rejecting \(H_{0}:X Y Z\) if the statistic \(^{(n,m)}^{(n,m)}/^{(n,m)}\) exceeds \(_{}^{-1}(1-)\), depending on the significance level \((0,1)\). We define \(^{(n,m)}\) and \(^{(n,m)}\) as

\[^{(n,m)}_{i=1}^{n}T_{i}^{(m)}^{(n,m)}[_{i=1}^{n}(T_{i}^{(m)})^{2 }-(_{i=1}^{n}T_{i}^{(m)})^{2}]^{1/2}\] (3.1)

with \(T_{i}^{(m)}(_{2}^{(m)}(Z_{i}),Y_{i})-(_{1}^{(m )}(X_{i},Z_{i}),Y_{i})+_{i}\). Here, \(\) is a loss function, typically used during the training phase, and \(\{_{i}\}_{i=1}^{n}}{{}}N(0,^{2})\) are small artificial random noises that do not let \(^{(n,m)}\) vanish with a growing training set, thus allowing the asymptotic distribution of \(^{(n,m)}\) to be standard normal under \(H_{0}:X Y Z\). If the \(p\)-value is defined as \(p(_{te}^{(n)},_{tr}^{(m)})=1-(^{(n,m)})\), the test is equivalently given by

\[_{}^{}(_{te}^{(n)},_{tr}^{(m)}) \{&1,p(_{te}^{(n)},_{tr}^{(m)}) \\ &0,.\] (3.2)

The rationale behind STFR is that if \(H_{0}:X Y Z\) holds, then \(_{1}^{(m)}\) and \(_{2}^{(m)}\) should have similar performance in the test set. On the other hand, if \(H_{0}\) does not hold, we expect \(_{1}^{(m)}\) to have significantly better performance, and then we would reject the null hypothesis. Said that, to control STFR's Type-I error, it is necessary that the risk gap between \(_{1}^{(m)}\) and \(_{2}^{(m)}\), \(_{P}[(_{2}^{(m)}(Z),Y)_{tr}^{(m)}]-_{P}[(_{1}^{(m)}(X,Z),Y)_{tr}^{(m)}]\), under \(H_{0}\) vanishes as the training set size increases. Moreover, we need the risk gap to be positive for the test to have non-trivial power. These conditions can be met if the risk gap of \(g_{1,P}^{*}\) and \(g_{2,P}^{*}\), the limiting models of \(_{1}^{(m)}\) and \(_{2}^{(m)}\), is the same as the risk gap of the Bayes' predictors

\[f_{1,P}^{*}_{f_{1}}_{P}[(f_{1}(X,Z),Y)]f_{2,P}^{*}_{f_{2}}_{P}[(f_{2}(Z),Y)],\]where the minimization is done over the set of all measurable functions4. However, the risk gap between \(_{1}^{(m)}\) and \(_{2}^{(m)}\) will typically not vanish if \(g_{1,P}^{*}\) and \(g_{2,P}^{*}\) are not the Bayes' predictors even under \(H_{0}\). In general, we should expect \(g_{1,P}^{*}\) to perform better than \(g_{2,P}^{*}\) because the second predictor does not depend on \(X\). Furthermore, their risk gap can be non-positive even if \(f_{1,P}^{*}\) performs better than \(f_{2,P}^{*}\). In Appendix A.2, we present two examples in which model misspecification plays an important role when conducting STFR. The examples show that Type-I error control and/or power can be compromised due to model misspecification.

To derive theoretical results, we adapt the assumptions from Dai et al. :

**Assumption 3.1**.: _There are functions \(g_{1,P}^{*}\), \(g_{2,P}^{*}\), and a constant \(>0\) such that_

\[_{P}(_{2}^{(m)}(Z),Y) _{tr}^{(m)}-_{P}(g_{2,P}^{*}(Z),Y)- _{P}(_{1}^{(m)}(X,Z),Y)_{tr}^{(m)} -_{P}(g_{1,P}^{*}(X,Z),Y)\] \[=_{}(m^{-})\]

**Assumption 3.2**.: _There exists a constant \(k>0\) such that_

\[_{P}|T_{1}^{(m)}|^{2+k}_{tr}^{(m)} =_{}(1)m\]

**Assumption 3.3**.: _For every \(P\), there exists a constant \(_{P}^{2}>0\) such that_

\[_{P}[T_{1}^{(m)}_{tr}^{(m)}]-_{P}^{2}=o_{P}(1 )m_{}_{P}^{2}>0\]

Finally, we present the results for this section. We start with an extension of Theorem 2 presented by Dai et al.  in the case of misspecified inductive biases.

**Theorem 3.4**.: _Suppose that Assumptions 3.1, 3.2, and 3.3 hold. If \(n\) is a function of \(m\) such that \(n\) and \(n=o(m^{2})\) as \(m\), then_

\[_{P}[_{}^{}}(_{te}^{(n)}, _{tr}^{(m)})]=1-_{}-^ {2}}}_{P}^{}}+o(1)\]

_where \(o(1)\) denotes uniform convergence over all \(P\) as \(m\) and_

\[_{P}^{}}_{P}[(g_{2,P}^{*}(Z),Y )]-_{P}[(g_{1,P}^{*}(X,Z),Y)]\]

Theorem 3.4 demonstrates that the performance of STFR depends on the limiting models \(g_{1,P}^{*}\) and \(g_{2,P}^{*}\). Specifically, if \(_{P}^{}}>0\), then \(_{P}[_{}^{}}(_{te}^{(n)}, _{tr}^{(m)})] 1\) even if \(H_{0}:X Y Z\) holds. In practice, we should expect \(_{P}^{}}>0\) because of how we set the class for \(_{2}^{(m)}\). In contrast, we could have \(_{P}^{}} 0\), and then \(_{P}[_{}^{}}(_{te}^{(n)}, _{tr}^{(m)})]+o(1)\), even if the gap between Bayes' predictors is positive. See examples in Appendix A.2 for both scenarios. Next, we provide Corollary 3.6 to clarify the relationship between testing and misspecification errors. This corollary formalizes the intuition that controlling Type-I error is directly related to misspecification of \(g_{2,P}^{*}\), while minimizing Type-II error is directly related to misspecification of \(g_{1,P}^{*}\).

**Definition 3.5**.: _For a distribution \(P\) and a loss function \(\), define the misspecification gaps:_

\[_{1,P}_{P}[(g_{1,P}^{*}(X,Z),Y)]-_{P} [(f_{1,P}^{*}(X,Z),Y)]_{2,P}_{P}[(g_{2,P}^{*}(Z),Y)]-_{P}[ (f_{2,P}^{*}(Z),Y)]\]

The misspecification gaps defined in Definition 3.5 quantify the difference between the limiting predictors \(g_{1,P}^{*}\) and \(g_{2,P}^{*}\) and the Bayes predictors \(f_{1,P}^{*}\) and \(f_{2,P}^{*}\), _i.e._, give a misspecification measure for \(g_{1,P}^{*}\) and \(g_{2,P}^{*}\). Corollary 3.6 implies that the STFR controls Type-I error asymptotically if \(_{2,P}=0\), and guarantees non-trivial power if the degree of misspecification of \(g_{1,P}^{*}\) is not large compared to the performance difference of the Bayes predictors \(_{P}\), that is, when \(_{P}-_{1,P}>0\).

**Corollary 3.6** (Bounding testing errors).: _Suppose we are under the conditions of Theorem 3.4. (Type-I error) If \(H_{0}:X Y Z\) holds, then_

\[_{P}[_{}^{}}(_{te}^{(n)}, _{tr}^{(m)})] 1-_{}-^{2}}} _{2,P}+o(1)\]

_where \(o(1)\) denotes uniform convergence over all \(P_{0}\) as \(m\). (Type-II error) In general, we have_

\[1-_{P}[_{}^{}}(_{te}^{(n)}, _{tr}^{(m)})]_{}-^ {2}}}(_{P}-_{1,P})+o(1)\]

_where \(o(1)\) denotes uniform convergence over all \(P\) as \(m\) and \(_{P}_{P}[(f_{2,P}^{*}(Z),Y)]-_{P}[(f_{1,P }^{*}(X,Z),Y)]\)._A robust regression-based conditional independence test

This section introduces the Rao-Blackwellized Predictor Test (RBPT), a misspecification robust conditional independence test based on ideas from both regression and simulation-based CI tests. The RBPT assumes that we can implicitly or explicitly approximate the conditional distribution of \(X Z\) and does not require inductive biases to be correctly specified. Because RBPT involves comparing the performance of two predictors and requires an approximation of the distribution of \(X Z\), we can directly compare it with the STFR  and the conditional randomization/permutation tests (CRT/CPT) [5; 4]. The RBPT can control Type-I error under relatively weaker assumptions compared to other tests, allowing some misspecified inductive biases.

The RBPT can be summarized as follows: (i) we train \(^{(m)}\) that predicts \(Y\) given \((X,Z)\) using \(_{tr}^{(m)}\); (ii) we obtain the Rao-Blackwellized predictor \(h^{(m)}\) by smoothing \(^{(m)}\), _i.e._,

\[h^{(m)}(z)^{(m)}(x,z)P_{X Z=z}(x),\]

then (iii) compare its performance with \(^{(m)}\)'s using the test set \(_{tr}^{(n)}\) and a convex loss5 function \(\) (not necessarily used to train \(^{(m)}\)), and (iv) if the performance of \(^{(m)}\) is statistically better than \(h^{(m)}\)'s, we reject \(H_{0}:X Y Z\). The procedure described here bears a resemblance to the Rao-Blackwellization of estimators. In classical statistics, the Rao-Blackwell theorem  states that by taking the conditional expectation of an estimator with respect to a sufficient statistic, we can obtain a better estimator if the loss function is convex. In our case, the variable \(Z\) can be seen as a "sufficient statistic" for \(Y\) under the assumption of conditional independence \(H_{0}:X Y Z\). If \(H_{0}\) holds and the loss \((,y)\)4 is convex in its first argument, we can show using Jensen's inequality that the resulting model \(h^{(m)}\) has a lower risk relative to the initial model \(^{(m)}\), _i.e._, \(_{p}[(h^{(m)}(Z),Y)_{tr}^{(m)}]-_{p}[ (^{(m)}(X,Z),Y)_{tr}^{(m)}] 0\). Then, the risk gap in RBPT is non-positive 5 under \(H_{0}\) in contrast with STFR's risk gap, which we should expect to be always non-negative given the definition of \(_{2}^{(m)}\) in that case. That fact negatively biases the RBPT test statistic, enabling better Type-I error control.

In practice, we cannot compute \(h^{(m)}\) exactly because \(P_{X Z}\) is usually unknown. Then, we use an approximation \(_{X Z}^{(m)}\), which can be given explicitly, _e.g._, using probabilistic classifiers or conditional density estimators , or implicitly, _e.g._, using generative adversarial networks (GANs) [22; 3]. We assume that \(_{X Z}^{(m)}\) is obtained using the training set. The approximated \(h^{(m)}\) is

\[^{(m)}(z)^{(m)}(x,z)_{X Z=z}^ {(m)}(x)\]

where the integral can be solved numerically in case \(_{X Z}^{(m)}\) has a known probability mass function or Lebesgue density (_e.g._, via trapezoidal rule) or via Monte Carlo integration in case we can only sample from \(_{X Z}^{(m)}\). Finally, for a fixed significance level \((0,1)\), the test \(_{}^{}\) is given by Equation 3.2 where the \(p\)-value is obtained via Algorithm 1.

```
0: (i) Test set \(_{te}^{(n)}=\{(X_{i},Y_{i},Z_{i})\}_{i=1}^{n}\), (ii) initial predictor \(^{(m)}\), (iii) conditional distribution estimate \(_{X Z}^{(m)}\), (iv) convex loss function \(\);
0: p-value \(p\);
1: For each \(i[n]\), get \(^{(m)}(z_{i})=^{(m)}(x,Z_{i})_{X Z=Z_{i} }^{(n)}(x)\);
2: Compute \(^{(n,m)}^{(n,m)}/^{(n,m)}\) where \(^{(n,m)}_{i=1}^{n}T_{i}^{(m)}\) with \[T_{i}^{(m)}(^{(m)}(Z_{i}),Y_{i})-(^{(m)}(X_{i},Z_{i}),Y_{i})\] and \(^{(n,m)}\) being \(\{T_{i}\}\)'s sample std dev (Eq. 3.1).
3: return\(p=1-(^{(n,m)})\). ```

**Algorithm 1**Obtaining \(p\)-value for the RBPT

Before presenting RBPT results, we introduce some assumptions. Let \(Q_{X Z}^{*}\) represent the limiting model for \(_{X Z}^{(m)}\). The conditional distribution \(Q_{X Z}^{*}\) depends on the underlying distribution \(P\), but we omit additional subscripts for ease of notation. Assumption 4.1 defines the limiting models and fixes a convergence rate.

**Assumption 4.1**.: _There is a function \(g_{P}^{*}\), a conditional distribution \(Q_{X|Z}^{*}\), and a constant \(>0\) s.t._

\[_{P}[\|^{(m)}(Z)-g_{P}^{*}(Z)\|_{2}^{2}| _{tr}^{(m)}]=_{}(m^{-})_{P}[d_{}(_{X|Z}^{(m)},Q_{X|Z}^{*}) _{tr}^{(m)}]=_{}(m^{-})\]

_where \(d_{}\) denotes the total variation (TV) distance. Additionally, assume that both \(_{X|Z}^{(m)}\) and \(Q_{X|Z}^{*}\) are dominated by a common \(\)-finite measure which does not depend on \(Z\) or \(m\)._

The common dominating measure in Assumption 4.1 could be, for example, the Lebesgue measure in \(^{d_{X}}\). Next, Assumption 4.2 imposes additional constraints on the limiting model \(Q_{X|Z}^{*}\). Under that assumption, the limiting misspecification level must be uniformly bounded over all \(P\).

**Assumption 4.2**.: _For all \(P\), the chi-square divergence_

\[^{2}(Q_{X|Z}^{*}||P_{X|Z})Q_{X| Z}^{*}}{P_{X|Z}}Q_{X|Z}^{*}-1\]

_is a well-defined integrable random variable and \(_{P}_{P}[^{2}(Q_{X|Z}^{*}||P_{X|Z} )]<\)._

Now, assume \(^{(m)}\) is chosen from a model class \(^{(m)}\). Assumption 4.3 imposes constraints on the model classes \(\{^{(m)}\}\) and loss function \(\).

**Assumption 4.3**.: _Assume (i) \(_{g^{(m)}}_{(x,z) Z}\|g(x,z) \|_{1} M<\), for some real and positive \(M>0\), uniformly for all \(m\), and (ii) that \(\) is a \(L-\)Lipschitz loss function (with respect to its first argument) for a certain \(L>0\), i.e., for any \(,^{},y\), we have that \(|(,y)-(^{},y)| L\|- ^{}\|_{2}\)._

Assumption 4.3 is valid by construction since we choose \(^{(m)}\) and the loss function \(\). That assumption is satisfied when, for example, (a) models in \(_{m}^{(m)}\) are uniformly bounded, (b) \((,y)=\|-y\|_{p}^{p}\) with \(p 1\), and (c) \(\) is a bounded subset of \(^{d_{Y}}\), _i.e._, in classification problems and most of the practical regression problems. The loss \((,y)=\|-y\|_{p}^{p}\), with \(p 1\), is also convex with respect to its first entry and then a suitable loss for RBPT. It is important to emphasize that \(\) does not need to be the same loss function used during the training phase. For example, we could use \((,y)=\|-y\|_{2}^{2}\) in classification problems, where \(y\) is a one-hot encoded class label and \(\) is a vector of predicted probabilities given by a model trained using the cross-entropy loss.

**Theorem 4.4**.: _Suppose that Assumptions 3.2, 3.3, 4.1, 4.2, and 4.3 hold. If \(n\) is a function of \(m\) such that \(n\) and \(n=o(m^{})\) as \(m\), then_

\[_{P}[_{}^{}(_{te}^{(n)}, _{tr}^{(m)})]=1-_{}-^{2}}}_{P}^{}+o(1)\]

_where \(o(1)\) denotes uniform convergence over all \(P\) as \(m\) and \(_{P}^{}=_{P,1}^{}-_{P,2}^{}\) with_

\[_{P,1}^{}_{P}[( g_{P}^{* }(x,Z)Q_{X|Z}^{*}(x),Y)]-_{P}[(  g_{P}^{*}(x,Z)P_{X|Z}(x),Y)]\]

_and_

\[^{}}_{}_{P} (g_{P}^{*}(X,Z),Y)-_{P}[( g_{P} ^{*}(x,Z)P_{X|Z}(x),Y)]\]

When \(H_{0}:X Y Z\) holds and \(\) is a strictly convex loss function (w.r.t. its first entry), we have that \(_{P,2}^{}>0\), allowing6 some room for the "incorrectness" of \(Q_{X|Z}^{*}\). That is, from Theorem 4.4, as long as \(_{P}^{} 0\), _i.e._, if \(Q_{X|Z}^{*}\)'s incorrectness (measured by \(_{P,1}^{}\)) is not as big as Jensen's gap \(_{P,2}\), RBPT has asymptotic Type-I error control. Uniform asymptotic Type-I error control is possible if \(_{P_{0}}_{P}^{} 0\). This is a great improvement of previous work (_e.g._, STFR, GCM, RESIT, CRT, CPT) since there is no need for any model to converge to the ground truth if \(_{P,1}^{}_{P,2}^{}\), which is a weaker condition. See however that a small \(_{P,2}^{}\) reduces the room for \(Q_{X|Z}^{*}\) incorrectness. In the extreme case, when \(g_{P}^{*}\) is the Bayes predictor, and therefore does not depend on \(X\) under \(H_{0}\), we need7\(Q_{X|Z}^{*}=P_{X|Z}\) almost surely. On the other hand, if \(g_{P}^{*}\) is close to the Bayes predictor, RBPT has better power. That imposes an expected trade-off between Type-I error control andpower. To make a comparison with Barrett et al. 's results in the case of CRT and CPT, we can express our remark in terms of the TV distance between \(Q_{X|Z}^{*}\) and \(P_{X|Z}\). It can be shown that if \(_{P}[d_{}(Q_{X|Z}^{*},P_{X|Z})]_{P,2}^{} /(2ML)\), then Type-I error control is guaranteed (see Appendix A.5). This contrasts with Berrett et al. 's results because \(_{P}[d_{}(Q_{X|Z}^{*},P_{X|Z})]=0\) is not needed.

We conclude this section with some relevant observations related to the RBPT.

**On RBPT's power.** Like STFR, non-trivial power is guaranteed if the predictor \(}\) is _good enough_. Indeed, the second part of Corollary 3.6 can be applied for an upper bound on RBPT's Type-II error.

**Semi-supervised learning.** Let \(Y\) denote a label variable. Situations in which unlabeled samples \((X_{i},Z_{i})\) are abundant while labeled samples \((X_{i},Y_{i},Z_{i})\) are scarce happen in real applications of conditional independence testing [5; 4]. RBPT is well suited for those cases because the practitioner can use the abundant data to estimate \(P_{X|Z}\) flexibly. The semi-supervised learning scenario also applies to RBPT2, which we describe next.

**Running RBPT when it is hard to estimate \(P_{X|Z}\): the RBPT2.** There might be situations in which it is hard to estimate the full conditional distribution \(P_{X|Z}\). An alternative approach would be estimating the Rao-Blackwellized predictor directly using a second regressor. After training \(^{(m)}\), we could use the training set, organizing it in pairs \(\{(Z_{i},^{(m)}(Z_{i},X_{i}))\}\), to train a second predictor \(^{(m)}\) to predict \(^{(m)}(Z,X)\) given \(Z\). That predictor could be trained to minimize the mean-squared error. The model \(^{(m)}\) should be more complex than \(^{(m)}\), in the sense that we should hope that the first model performs better than the second under \(H_{0}\) in predicting \(Y\). Consequently, this approach is effective when unlabeled samples are abundant, and we can train \(\) using both unlabeled data and the given training set. After obtaining \(^{(m)}\), the test is conducted normally. We name this version of RBPT as "RBPT2". We include a note on how to adapt Theorem 4.4 for RBPT2 in Appendix A.6.

## 5 Experiments

We empirically8 analyze RBPT/RBPT2 in the following experiments and compare them with relevant benchmarks, especially when the used models are misspecified. We assume \(=10\%\) and \((,y)=(-y)^{2}\). The benchmarks encompass STFR , GCM , and RESIT , which represent regression-based CI tests. Furthermore, we examine the conditional randomization/permutation tests (CRT/CPT) [5; 4] that necessitate the estimation of \(P_{X|Z}\).

**Artificial data experiments.** Our setup takes inspiration from Berrett et al. , and the data is generated as

\[Z N(0,I_{d_{Z}}), X Z N((b^{}Z)^{2},1 ), Y X,Z N(cX+a^{}Z+(b^{}Z)^{2},1).\]

Here, \(d_{Z}\) denotes the dimensionality of \(Z\), \(a\) and \(b\) are sampled from \(N(0,I_{d_{Z}})\), the constant \(c\) determines the conditional dependence of \(X\) and \(Y\) on \(Z\), and the parameter \(\) dictates the hardness of conditional independence testing: a non-zero \(\) implies potential challenges in Type-I error control as there might be a pronounced marginal dependence between \(X\) and \(Y\) under \(H_{0}\). Moreover, the training (resp. test) dataset consists of 800 (resp. 200) entries, and every predictor we employ operates on linear regression. RESIT employs Spearman's correlation between residuals as a test statistic while

Figure 2: Type-I error rates (\(c=0\)). In the first two plots, we set \(=0\) for RBPT, permitting Type-I error control across different \(d_{Z}\) values (Theorem 4.4), while RBPT2 allows Type-I error control for moderate \(d_{Z}\). All the baselines fail to control Type-I errors regardless of \(d_{Z}\). The last two plots illustrate that CRT emerges as the least robust test in this context, succeeded by RBPT and CPT.

CRT and CPT9 deploy STFR's test statistic, all of them with \(p\)-values determined by conditional sampling/permutations executed 100 times (\(B=100\)), considering \(_{X|Z}=N((b^{}Z)^{2}+,1)\). The value of \(\) gives the error level in approximating \(P_{X|Z}\). To get \(\) in the two variations of RBPT, we either use \(_{X|Z}\) (RBPT) or kernel ridge regression (KRR) equipped with a polynomial kernel to predict \(_{1}(X,Z)\) from \(Z\) (RBPT2). We sample generative parameters \((a,b)\) five times using different random seeds, and for each iteration, we conduct 480 Monte Carlo simulations to estimate Type-I error and power. The presented results are the average (\(\) standard deviation) estimated Type-I error/power across iterations.

In Figure 2 (resp. 4) we compare our methods' Type-I error rates (with \(c=0\)) (resp. power) against benchmarks. Regarding Figure 2, we focus on regression-based tests (STFR, GCM, and REST) in the first two plots and on simulation-based tests (CRT and CPT) in the last two plots. Regarding the first two plots, it is not straightforward to compare the level of misspecification between our methods and the benchmarks, so we use this as an opportunity to illustrate Theorem 4.4 and results from Section 3 and Appendix A. Fixing \(=0\) for RBPT, the Rao-Blackwellized predictor \(h\) is perfectly obtained, permitting Type-I error control regardless of the chosen \(d_{Z}\). Using KRR for RBPT2 makes \(\) close to \(h\) when \(d_{Z}\) is not big and permits Type-I error control. When \(d_{Z}\) is big, more data is needed to fit \(\), which can be accomplished using unlabeled data, as demonstrated in Figure 3 and commented in Section 4. On the other hand, Type-I error control is always violated for STFR, GCM, and REST when \(\) grows. Regarding the final two plots, we can more readily assess the robustness of the methods when discrepancies arise between \(_{X|Z}\) and \(P_{X|Z}\) as influenced by varying \(\). Figure 2 illustrates that CRT is the least robust test in this context, succeeded by RBPT and CPT. In Figure 4, we investigate how powerful RBPT and RBPT2 can be in practice when \(d_{Z}=30\). We compare our methods with CPT (when \(=0\)), which seems to have practical robustness against misspecified inductive biases. Figure 4 shows that RBPT2 and CPT have similar power while RBPT is slightly more conservative.

Some concluding remarks are needed. First, RBPT and RBPT2 have shown to be practical and robust alternatives to conditional independence testing, exhibiting reasonable Type-I error control, mainly when employed in conjunction with a large unlabeled dataset, and power. Second, while CPT demonstrates notable robustness and relatively good power, its practicality falls short compared to RBPT (or RBPT2). This is because CPT needs a known density functional form for \(_{X|Z}\) (plus the execution of MCMC chains) whereas RBPT (resp. RBPT2) can rely on conventional Monte Carlo integration using samples from \(_{X|Z}\) (resp. supervised learning).

**Real data experiments.** For our subsequent experiments, we employ the car insurance dataset examined by Angwin et al. . This dataset encompasses four US states (California, Illinois, Missouri, and Texas) and includes information from numerous insurance providers compiled at the ZIP code granularity. The data offers a risk metric and the insurance price levied on a hypothetical customer with consistent attributes from every ZIP code. ZIP codes are categorized as either minority or non-minority, contingent on the percentage of non-white residents. The variables in consideration are \(Z\), denoting the driving risk; \(X\), an indicator for minority ZIP codes; and \(Y\), signifying the insurance price. A pertinent question revolves around

Figure 4: Power curves for different methods. We compare our methods with CPT (when \(=0\)), which seems to have practical robustness against misspecified inductive biases. RBPT2 and CPT have similar power, while RBPT is slightly more conservative.

Figure 3: Making RBPT2 more robust using unlabeled data. With \(d_{Z}=40\), we gradually increase the unlabeled sample size from \(0\) to \(1000\) when fitting \(\). The results show that a larger unlabeled sample size leads to effective Type-I error control. Even though we present this result for RBPT2, the same pattern is expected for RBPT in the presence of unlabeled data.

the validity of the null hypothesis \(H_{0}:X Y Z\), essentially questioning if demographic biases influence pricing.

We split our experiments into two parts. In the initial part, our primary goal is to compare the Type-I error rate across various tests. To ensure that \(H_{0}\) is valid, we discretize \(Z\) into twenty distinct values and shuffle the \(Y\) values corresponding to each discrete \(Z\) value. If a test maintains Type-I error control, we expect it to reject \(H_{0}\) for at most \(=10\%\) of the companies in each state. In the second part, we focus on assessing the power of our methods. Given our lack of ground truth, we qualitatively compare RBPT and RBPT2 findings with those obtained by baseline methods and delineated by Angwin et al. , utilizing a detailed and multifaceted approach. In this last experiment, we aggregate the analysis for each state without conditioning on the firm. We resort to logistic regression for estimating the distribution of \(X Z\) used by RBPT, GCM, CRT, and CPT. For RBPT2, we use a CatBoost regressor  to yield the Rao-Blackwellized predictor. We omit RESIT in this experiment as the additive model assumption is inappropriate. Both CRT and CPT methods utilize the same test metrics as STFR. The first plot10 of Figure 5 shows that RBPT and RBPT2 methods have better control over Type-I errors compared to all other methods, including CPT. The second plot reveals that all methods give the same qualitative result that discrimination against minorities in ZIP codes is most evident in Illinois, followed by Texas, Missouri, and California. These findings corroborate with those of Angwin et al. , indicating that our methodology has satisfactory power while maintaining a robust Type-I error control.

## 6 Conclusion

In this work, we theoretically and empirically showed that widely used regression-based conditional independence tests are sensitive to the specification of inductive biases. Furthermore, we introduced the Rao-Blackwellized Predictor Test (RBPT), a misspecification-robust conditional independence test. RBPT is theoretically grounded and has been shown to perform well in practical situations compared to benchmarks.

**Limitations and future work.** Two limitations of RBPT are that (i) the robustness of RBPT can lead to a more conservative test, as we have seen in the simulations; moreover, (ii) it requires the estimation of the conditional distribution \(P_{X Z}\), which can be challenging. To overcome the second problem, we introduced a variation of RBPT, named RBPT2, in which the Rao-Blackwellized predictor is obtained in a supervised fashion by fitting a second model \(:\) that predicts the outputs of the first model \(:\). However, this solution only works if \(\) is better than \(\) in predicting \(Y\) under \(H_{0}\), which ultimately depends on the model class for \(\) and how that model is trained. Future research directions may include (i) theoretically studying the power of RBPT in more detail and (ii) better understanding RBPT2 from a theoretical or methodological point of view, _e.g._, answering questions on how to choose and train the second model.

## 7 Acknowledgements

This paper is based upon work supported by the National Science Foundation (NSF) under grants no. 1916271, 2027737, 2113373, and 2113364.

Figure 5: Type-I error control and power analysis using car insurance data . The first plot shows that RBPT and RBPT2 have better control over Type-I errors compared to all other methods, including CPT. The second plot reveals that all methods give the same qualitative result, corroborating the findings of Angwin et al. , suggesting that RBPT and RBPT2 can have good power while being more robust to Type-I errors.