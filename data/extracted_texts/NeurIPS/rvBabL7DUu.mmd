# Face2QR: A Unified Framework for Aesthetic, Face-Preserving, and Scannable QR Code Generation

Xuehao Cui\({}^{*}\), Guangyang Wu\({}^{*}\), Zhenghao Gan, Guangtao Zhai, Xiaohong Liu\({}^{}\)

Shanghai Jiao Tong University

{cavosamir, wu.guang.young, ganzhenghao,

zhaiguangtao, xiaohongliu}@sjtu.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Existing methods to generate aesthetic QR codes, such as image and style transfer techniques, tend to compromise either the visual appeal or the scannability of QR codes when they incorporate human face identity. Addressing these imperfections, we present Face2QR--a novel pipeline specifically designed for generating personalized QR codes that harmoniously blend aesthetics, face identity, and scannability. Our pipeline introduces three innovative components. First, the ID-refined QR integration (IDQR) seamlessly intertwines the background styling with face ID, utilizing a unified Stable Diffusion (SD)-based framework with control networks. Second, the ID-aware QR ReShuffle (IDRS) effectively rectifies the conflicts between face IDs and QR patterns, rearranging QR modules to maintain the integrity of facial features without compromising scannability. Lastly, the ID-preserved Scannability Enhancement (IDSE) markedly boosts scanning robustness through latent code optimization, striking a delicate balance between face ID, aesthetic quality and QR functionality. In comprehensive experiments, Face2QR demonstrates remarkable performance, outperforming existing approaches, particularly in preserving facial recognition features within custom QR code designs. Codes are available at https://github.com/cavosamir/Face2QR.

## 1 Introduction

Quick Response (QR) codes, due to their capability to store a substantial amount of data and their ease of accessibility through basic camera devices, have become an exceedingly widespread medium for the representation of information in the digital era . With the wide application of QR codes in social context, there has been increasing needs for customizing QR codes to include **personal identity** and **aesthetic allure**. However, such needs cannot be fulfilled by the dull appearance of common QR codes, which contain only black and white modules.

With the widespread application of QR codes across diverse fields, related technologies are also developing at a rapid pace. While techniques employing image transformation  and style transferring  can partially retain facial features, their perceptual quality and aesthetic adaptability are limited. On the other hand, generative model-based approaches  can produce QR code images of superior quality and diversity, yet they pose challenges in controlling the generated content, particularly in preserving human facial characteristics. To address these limitations and ensure faithful preservation of face identity within a customized and scannable QR code image, we introduce a novel pipeline, named Face2QR. This approach achieves a balanced compromise between face ID preservation, aesthetic appeal, and scannability for QR code images.

The primary challenges lie in effectively integrating three key aspects: face ID, aesthetic quality, and scanable QR pattern, which can be summarized as follows: **(1) Combination of face ID and background.** Achieving a harmonious balance between strict facial ID preservation and diverse customized background styles within a unified pipeline presents a notable challenge. Methods reliant on style transfer  often yield facial textures that appear unnatural, while those based on image transfer  may introduce visible artifacts in the facial region; **(2) Conflict between face ID and QR code pattern.** While prior generative model-based techniques  have demonstrated the ability to control the QR code pattern using QR blueprints, they have struggled to exclude these patterns from the facial region, resulting in unnatural shadows and undesirable artifacts. However, directly removing these patterns from the facial region can make the image unscannable. Thus, achieving a balance between maintaining visual quality in the facial region and ensuring the correctness of the QR pattern presents a formidable obstacle; **(3) Balance between aesthetics and scanability.** As revealed in , generated images often exhibit a tendency towards being unscannable, necessitating enhancements to their scanability through post-processing. However, globally adjusting brightness can compromise the natural appeal of the facial region. Thus, novel region-based enhancement methods are worth considering to address this challenge.

To address these challenges, the proposed Face2QR pipeline offers a solution for generating personalized QR codes that strike a balance between aesthetics, facial ID preservation, and scanability. We propose ID-refined QR integration (IDQR) to seamlessly combine background and face ID, and ID-aware QR ReShuffle (IDRS) to solve the conflict between face ID and QR code pattern. Specifically, IDQR applies a unified SD-based framework to ensure that the generated images have a uniform style. Stable Diffusion (SD) models are guided by two sets of control networks, corresponding to face refinement and QR pattern respectively, to achieve separate control in face region and background. IDRS utilizes the flexibility of QR code encoding and reshuffles the modules to make the QR pattern compatible with face ID. Finally, we use ID-preserved Scannability Enhancement (IDSE) to enhance scan robustness through latent code optimization, achieving a new trade-off between face ID, aesthetics and scanning. Figure 1 shows the QR images generated by Face2QR. It is worth noting that the generated QR images are not only the reprints of the provided references, but also have improved aesthetics to align with the generated background, guided by text prompts (_e.g._, the style and color of clothes have been adjusted accordingly).

The contributions of this work can be summarized as:

\(\) We propose a novel pipeline that holistically integrates aesthetic appealing, facial ID, and scanability to deliver a customized personal representation in QR codes.

\(\) We introduce the ID-refined QR integration (IDQR) for seamlessly integrating face ID with background, the ID-aware QR ReShuffle (IDRS) for solving conflicts between face ID and QR pattern, and the ID-preserved Scannability Enhancement (IDSE) for optimizing scan robustness while maintaining face ID and aesthetic quality.

\(\) Our Face2QR achieves the State-Of-The-Art (SOTA) performance in generating the ID-preserved aesthetic QR codes, compared with previous methods.

Figure 1: Face images (first row) and QR code images (second row) generated by Face2QR. Our QR codes not only faithfully maintain face ID, but also showcase remarkable scanning resilience and aesthetic quality.

Related Works

Quick Response (QR) Code.As QR codes emerging as a key connector between real and virtual worlds, there is increasing interest in enhancing the visual appeal of normally monochromatic QR codes. Halftone QR codes  offers a design where QR code patterns align with a given image in a thematically cohesive manner. QRImage and Artup [13; 46] explore ways to encode colorful imagery within a QR code. Other advances [35; 36] have been made in artistic style transfer to increase aesthetic appearance of QR codes. To further customize QR code and obscure overt QR code markers, Chen et al. [2; 4; 23] crated encoding schemes that consider human visual perception, thus making these patterns less intrusive. TPVM  has gone further to conceal QR codes within video content, exploiting the discrepancies in frame capture rates between human vision and digital screens. Similarly, advancements have sought to keep data imperceptible yet accessible through various stealth mechanisms [10; 9; 37; 16; 42; 17].

Diffusion Based Models.Image manipulation and generation techniques powered by deep learning have made strides in recent years [41; 45; 21; 44; 31; 33; 32], with generative models being at the forefront of this development [51; 24; 28; 26]. Novel diffusion-based models such as GLIDE , DALLE-2 , and Latent Diffusion models  have come into prominence. Notably, the Stable Diffusion model  moves the denoising steps into the latent dimension of a variational autoencoder, which significantly optimizes the generation process in terms of data volume and training time. In parallel, new research has introduced various techniques for modulating the diffusion process. Structural condition interventions have been successfully implemented by ControlNet  and T2I-Adapter . On a different note, BLIP-Diffusion  and SeeCoder  have made progress on steering generative outcomes based on stylistic aspects.

Identity Preserved Generative Models.In the field of ID-preserving image generation, research focuses on maintaining semantic face attributes while generating images that have wide real-world applications. Studies have generally split between techniques requiring test-time fine-tuning, such as Low-Rank Adaptation , and newer optimization-free methods such as Face0 , PhotoMaker , and FaceStudio , which integrate facial embeddings into the generation process in different ways. While techniques like IP-Adapter  strive for identity consistency by using embeddings from recognition models, they face challenges in compatibility with pre-trained models and ensuring facial fidelity. Most recent work like InstantID  use a pluggable module that does not demand fine-tuning and can work seamlessly with available pre-trained diffusion models to achieve high-quality face preservation in generated images.

## 3 Method

The overall structure of Face2QR is shown in Figure 2. The pipeline unfolds through three stages, represented by blue, red and green arrows. Given a user-customized face image \(f\), QR Code \(m\), text prompts \(c\) and random noise \(z_{0}\), the first stage uses the ID-refined QR integration (IDQR) module to generate an initial QR image \(I^{g}\). The IDQR module includes a pre-trained SDXL model (denoted as \(\)), an InstantID  network (denoted as \(C_{id}\)) and a QR Controller  (denoted as \(C_{qr}\)). Stage 1 can be expressed as:

\[I^{g}=(c,z_{0}|_{qr}(m,c,z_{0}),_{id}(f,c,z _{0})).\] (1)

The InstantID network preserves the facial identity information in the generated images, while the QR Controller guides the luminance distribution of the images.

However, as shown in Figure 2, the initial output image from the first stage contains a significant error rate (over 43%). This issue arises from the inherit conflict between two control signals: the foreground face information and the background QR patterns, which are incompatible in the center regions. These conflicts lead to unavoidable QR code errors, presenting a core challenge in our pipeline. To address this, we design the ID-aware QR ReShuffle (IDRS) module to harmonize these conflicts and regenerate the image using a fine-grained QR blueprint \(I_{b}\). As illustrated in Figure 2, this reduces the error rate by more than half. Finally, we use the ID-preserved Scannability Enhancement (IDSE) module to refine the result \(I^{s}\) in latent space, further improving its scanning robustness without compromising the overall visual quality. In the following, we introduce the second and third stages in details.

### ID-Aware QR ReShuffle

As revealed in , a fine-grained QR blueprint can effectively control the generator. To resolve control conflicts in the facial region, we design a novel blueprint that makes facial information and QR patterns compatible. By leveraging the dynamic characteristics of QR code encoding, we can adaptively rearrange the QR modules. Specifically, we maintain the brightness distribution of the facial region and reshuffle the remaining black and white modules accordingly.

First of all, we binarize \(I^{g}^{H W 3}\) into module-wise binary information \(^{n^{2}}\). By dividing \(I^{g}\) into \(n n\) modules each of size \(a a\), and let \(_{j}\) be the set of pixel coordinates of the \(j\)-th module in \(I^{g}\), the extracted information code \(\) is given by:

\[_{j}=0,&(I^{g}(_{j}))<,\\ 1,&(I^{g}(_{j})),\] (2)

where \(()\) denotes the mean pixel value of the squared patch of size \(a a\). The binarization uses a threshold \(\), typically set to 128 for a total of 256 grayscale levels.

As shown in Figure 3 (left), the binarized QR code is un-scannable due to a significant error rate. To address this, we fix the facial and marker region within \(\), then rearrange the remaining codes to align with the encoded information. To locate the facial region, we use a pre-trained face recognition model to obtain the binary facial mask \(M_{f}^{H W}\). Let the set \(_{f}=\{j(M_{f}(_{j}))=1\}\) represent the indices of information codes in \(\) that correspond to the facial region, and let the \(_{m}\) represent the indices of marker codes. Our goal is to obtain a new information code \(}\) which is partially modified from \(\) to make the QR decoder \(\) extract lossless information:

\[|(})-(m)|,\] (3) \[}_{j}=_{j},j _{f}_{m},\] (4)

To ensure the resultant \(}\) can be decoded to the target message, aligning with original QR code \(m\), we re-generate the error correction code  in \(}\).

Afterwards, we expand the binary information of \(}\) to image space. We use adaptive-halftone to combine the texture information of \(I^{g}\) with binary code information of \(}\) in an adaptive manner, resulting in the blueprint \(I_{b}^{H W}\). Note that we leave the facial region unmodified to maintain rich facial features without compromising scanning robustness. The resultant blueprint \(I_{b}\) is then fed

Figure 2: The pipeline of Face2QR is a training-free process for generating ID-consistent and scannable QR code images. Our pipeline has three stages, represented by blue, red, and green arrows. The IDRS module resolves conflicts between human identity and QR patterns during the control process, while the IDSE module reduces coding errors to ensure the output is scannable.

into \(\) for the second generation:

\[I^{s}=(c,z_{0}|_{qr}(I_{b},c,z_{0}),_{id}(f,c, z_{0})).\] (5)

Compared with the first generation in Equation 1, both controllers in stage 2 include facial information to mitigate conflicts. As shown in Figure 2, the result of stage 2 reduces errors by more than half compared to stage 1, while consistently preserving face identity information.

### Scannability Enhancement

The resultant QR image \(I^{s}\) from stage 2 contains a certain QR pattern and consistently reveals face identity, but it is still unscannable by common QR decoders. In this part, we design the ID-Preserved Scannability Enhancement (IDSE) module to achieve the following two goals: 1) minimize modifications to the QR image (especially for facial region) to ensure its scannability; 2) enhance the marker region to better harmonize it without compromising scanning robustness. As illustrated in Figure 3 (right), we first strengthen the finder and alignment pattern of \(I^{s}\), and then refine it using dynamic code loss to reach a harmonious balance between face ID, visual appeal and scannability.

#### 3.2.1 Marker Harmonization

The functional regions of a QR code, especially the finder and alignment patterns, are crucial for the decoder to locate the QR code. Therefore, these patterns on \(I^{s}\) are strengthened to generate \(^{s}\). Specifically, for pixel \(_{k}\) where \(k_{m}\), we have:

\[^{s}()=I^{s}()-(I^{s}( )-(1+),0),&_{k}=1,\\ I^{s}()-(I^{s}()-(1-),0),&_{k}=0, \] (6)

where \((0,1)\) is a hyper-parameter, typically set to 0.8 by default. This threshold-based enhancement helps ensure that the functional regions of the output QR image are easily located.

#### 3.2.2 Spatially Dynamic Loss Function

Inspired by , we use gradient descent to update the latent code of \(^{s}\) to optimize certain loss function. However, instead of using a fixed loss function with constant coefficients, we propose to leverage a spatially dynamic loss function.

Given a pretrained VQ-VAE  with the encoder \(\) and the decoder \(\), the optimization is given by:

\[=*{argmin}_{z}((z),I_{b},^{s}),\] (7)

Figure 3: Illustration of IDRS (left) and IDSE (right). In IDRS, we maintain the information codes within the face and marker regions (red and yellow masks) and remap the remaining modules accordingly. In IDSE, we strengthen the finder and alignment pattern, and update in latent space using adaptive loss to enhance scannability. Visualization \(D\) shows the difference between \(I^{o}\) and \(^{s}\). Compared with uniform loss, adaptive loss modifies face region more gently.

where \(z^{ 4}\) is the latent code. The loss function \(\) consists of an aesthetic content loss \(_{a}\) and a spatially dynamic code loss \(_{c}\):

\[=*{argmin}_{z}\{_{c}((z),I_{b})+ _{a}((z),I^{s})\}.\] (8)

We initialize \(z\) to \((^{s})\), and use Adam  as the optimizer with a learning rate of 0.002 to iteratively update \(z\) until convergence. Finally, the output \(I^{o}=()\) achieves robust scannability and high visual quality.

Adaptive Code Loss.A simulated decoder  using a 2D Gaussian kernel can extract module-wise information consistent with common QR decoders. The variance \(\) of the Gaussian kernel is a key factor in balancing visual quality and scanning robustness. However, in our scenario, we want the facial region to be smooth and the background region to be lossless. Therefore, we propose a spatially dynamic code loss. Let \(Z=(z)\), the loss of \(j\)-th module is calculated by:

\[s_{j}=w(j)\{[Z(_{j})-I_{b}(_{j})] G(j)\},\] (9)

where \(\) denotes the Hadamard product. \(G(j)^{a a}\) is a weighting kernel, and \(w(j)\) is a weighting factor defined by:

\[G(j)=G_{_{f}},&j_{f},\\ G_{_{b}},&;\;w(j)=w_{f},&j _{f},\\ w_{b},&,\] (10)

where \(G_{}\) is a 2D Gaussian kernel with variance \(\). The specific settings for the hyper-parameters \(w_{f}\), \(w_{b}\), \(_{f}\), and \(_{b}\) can be found in the experiments section. Finally, the adaptive code loss is computed by:

\[_{c}(Z,I_{b})=_{j=1}^{n^{2}}w(j)\{[Z(_{j })-I_{b}(_{j})] G(j)\}.\] (11)

Gaussian distribution with bigger \(\) is flatter, which helps equalize the color within the module when updating the latent code. Although this makes modules easier to decode after iterations, bigger \(\) might create unnatural shadow in the face region. On the other hand, Gaussian distribution with smaller \(\) effectively regulates only the central region of a module, making the modules remain unscannable even after updates.

This problem is addressed by utilizing adaptive loss for different regions, _i.e._, applying smaller weight \(w_{f}\) and \(_{f}\) in the face region to prevent distortion on face, and relatively larger \(w_{f}\) and \(_{f}\) in remaining region to maintain balance between scannability and aesthetic quality.

Aesthetic Content Loss.To ensure the retention of aesthetic qualities while preserving face ID and enhancing scannability, we use the aesthetic content loss to retain essential visual characteristics. It is quantified by calculating \(L^{2}\)-Wasserstein distance  (denoted as \(D_{W2}\)) of feature representations between \(Z\) and \(^{s}\) as follows:

\[_{a}(Z,^{s})=_{i}D_{W2}(g_{i}(Z),g_{i}(^{ s})),\] (12)

where \(g_{i}\) is feature representations from a pre-trained VGG-19  network at layer \(i\). The aesthetic content loss reflects the global aesthetic quality of the image. By optimizing both code loss and content loss, IDSE module adeptly balances the aesthetic quality, face-preserving, and scannability and creates optimal customized QR code images.

## 4 Experiments

### Experimental Setup and Configuration

We implemented our pipeline in Python using the PyTorch framework and conducted experiments on an NVIDIA GeForce 4090 GPU. The scannability of QR images is tested using a 27-inch IPS display monitor with a refresh rate of 144Hz. In our experiments, we set control strengths for the InstantID network  and QR Controller at 0.8 and 1.4, respectively. The parameter \(\) in the marker harmonization process defaults to 0.8. The VAE configuration is consistent with the SD model. The face recognition model AntelopeV2 from InsightFace  assists the generation of face mask \(M_{f}\) in IDRE. The VGG-19 architecture, pre-trained on the MS-COCO dataset, facilitates the feature map extraction in IDSE. The Adam optimizer powers the optimization within IDSE, performing 300 iterations at a learning rate of 0.002. Default settings for \(_{f}\), \(_{b}\), \(w_{f}\), and \(w_{b}\) are 1.5, 3.0, 1.0, and 15.0 respectively. We produce QR code in version 5, each with \(37 37\) modules. For clarity, we define \(e\) as the number of error modules in QR image \(I^{o}\) (excluding finder and alignment pattern areas), and \(e_{f}\) as the number of error modules within the face region. Our dataset for comparative analysis contains 200 uniquely stylized QR images, each \(1024 1024\) pixels in size, with diverse visual content and artistic styles. To more accurately assess the preservation of face identity, we define the face feature distance \(d\) as the cosine similarity between the facial features (extracted using ArcFace ) of the generated QR image \(I^{o}\) and the original face image \(f\).

### Qualitative Comparison

Aesthetic Quality.In our comparative study, we evaluate our approach against several state-of-the-art aesthetic QR code generation techniques, including QArt , Halfrone QR code , ArtCoder  and Text2QR , as detailed in Table 1. QArt, Halftone QR and Text2QR take the original face image \(f\) as the primary input, except that Text2QR takes in additional prompt input \(c\). As ArtCoder is based on neural-style transfer technique, we employ \(f\) and \(I^{g}\) to serve as the content reference and the style reference respectively. The results show that Artcoder tends to render the texture of style image to face region, causing unwanted distortion on the face. Text2QR, on the other hand, cannot preserve face ID due to lack of specific control mechanisms for the face region. In contrast, our QR codes are adept at harmoniously integrating face ID, background and QR pattern, thereby achieving superior visual quality as well as scannability.

Identity Preservation.The comparison between original face image \(f\) and the generated image \(I^{o}\) is shown in Table 2. The face ID is well preserved in the final generated QR image \(I^{o}\), with minimal change in haircut or facial expression, which can be further customized by users by adding prompt. The facial region is consistent with the background in style, and the QR pattern is blended seamlessly into the picture. We also compare the generated image \(I^{o}\) with output of the baseline pipeline InstantD  in Table 3, which shows that our pipeline achieves a similar level of identity preservation as the baseline. The outcomes displayed in Table 4 demonstrate that Face2QR consistently generates high-quality images across various poses.

   Input & QArt  & Halftone  & ArtCoder  & Text2QR  & Face2QR \\   

Table 1: Visual comparison of different methods.

### Quantitative Comparison

Scanning Robustness.In this study, we assess the scanning robustness of our QR images using different scanning applications. We first generate a batch of 20 aesthetically pleasing QR codes, each with a dimension of 1,024 \(\) 1,024 pixels. These QR images are then displayed on a high-definition monitor in three standard sizes: 3cm \(\) 3cm, 5cm \(\) 5cm, and 7cm \(\) 7cm. During our controlled test, smartphones are held at a fixed distance of 25cm from the display, and each code is scanned for 3 seconds from different angles. Over a total of 50 trials, the percentage of successful scans is recorded in Table 5. The results reveal an average success rate exceeding 94%, showcasing high reliability of the generated QR images in diverse practical settings. It is also noted that QR images that fail the test in 3s can eventually be scanned if given more time. The scanning success rate is similar to that of Text2QR , as presented in our comparative analysis.

Subjective Study.Figure 4 presents a user study consisting of 30 participants to evaluate 150 QR images (50 for each methods) generated by different methods (the approval from Institutional Review Board is obtained). Participants are asked to choose the better one from a pair of pictures in the aspect of face ID preservation and aesthetic quality. Each pair of QR images are generated by different methods using the same face image as input. The percentages represent how many times users prefer the results of a method over the other. Our results are preferred by most users.

Objective Study.Table 6 shows the statistical performance measured by taking the average of 100 samples. We use the feature distance \(d\), varying from -1 to 1, as a quantifiable measure for the

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]