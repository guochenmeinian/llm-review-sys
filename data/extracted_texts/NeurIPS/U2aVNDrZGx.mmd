# Benchmarking Complex Instruction-Following with Multiple Constraints Composition

Bosi Wen\({}^{1,}\)

Equal contributionWork done when these authors interned at Zhipu AI.

Pei Ke\({}^{3,}\)

Equal contributionWork done when these authors interned at Zhipu AI.

Xiaotao Gu\({}^{2}\)

Lindong Wu\({}^{2}\)

Hao Huang\({}^{2}\)

Jinfeng Zhou\({}^{1}\)

Wenchuang Li\({}^{4,}\)

Binxin Hu\({}^{5,}\)

Work done when these authors interned at Zhipu AI.

Wendy Gao\({}^{2}\)

Jiaxin Xu\({}^{1}\)

Yiming Liu\({}^{1}\)

Jie Tang\({}^{1}\)

Hongning Wang\({}^{1}\)

Minlie Huang\({}^{1,}\)

\({}^{1}\)Tsinghua University \({}^{2}\)Zhipu AI \({}^{3}\)University of Electronic Science and Technology of China

\({}^{4}\)China University of Geosciences \({}^{5}\)Central China Normal University

wbs23@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn

###### Abstract

Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition1.

## 1 Introduction

Large language models (LLMs) have proven their remarkable abilities in addressing various NLP tasks . Among these, instruction following is one of the most crucial requirements for LLM applications as it determines how well LLMs align with human intents . In real-world use of LLMs, almost all the tasks are formulated as instruction following, where human instructions impose different constraints on the model output to specify the requirement of specific tasks .

Hence, how to accurately measure the quality of instruction following has become an essential problem. While early works focused on simple and direct human instructions in traditional NLP tasks, such as translation and text classification [4; 5; 6], recent works have resorted to complex instructions consisting of multiple constraints [3; 7; 8; 9], which are important constituents of LLM's real-world use including role-play  and LLMs as agents . These complex instructionfollowing benchmarks aim to measure whether the generated text can meet every constraint in the input instruction.

However, we argue that existing complex instruction-following benchmarks neglect to model the composition of constraints, causing insufficient evaluation of the LLMs' ability to follow complex instructions. Since composition is a natural phenomenon in language use and a long-standing research problem in the NLP community , it is a necessary ingredient in complex instructions to specify structural combinations of different constraints. In addition, the ignorance of composition leads to issues in both dataset construction and evaluation method design. On dataset construction, existing benchmarks are currently limited to simple composition types such as _And_ which represents coordination between different constraints . As shown in Figure 1, in addition to _And_, complex instructions can also include more intricate composition types of constraints, such as _Chain_ (for sequential completion of constraints) and _Selection_ (for conditional selection of constraints). Regarding evaluation method design, incorporating more complex composition types brings challenges in both constraint / composition evaluation and final score aggregation. First, complex instructions with structural combinations of constraints make it hard to evaluate each constraint / composition type independently with LLMs / rules due to their coupling. Then, simple aggregation methods for each constraint result, such as direct averaging, which is commonly adopted by existing benchmarks neglect the dependency among constraints brought by composition, causing potential biases in evaluation results.

In this paper, we propose ComplexBench, a novel benchmark to comprehensively evaluate the ability of LLMs to follow complex instructions. ComplexBench is manually constructed based on a hierarchical taxonomy of complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, which provide a broad perspective to assess the performance of LLMs in dealing with complex instructions. To precisely measure whether LLMs' generated texts satisfy all these constraints and composition types, we design a yes / no question to verify each constraint and composition type respectively, inspired by the existing works on QA-based evaluation . Then, we propose a new evaluation method for complex instruction-following called rule-augmented LLM-based evaluation. This method first extracts evaluation segments from generated responses for each yes / no question and then solves each question with LLMs or rules. Finally, the answers to each question are aggregated via the dependency structure among these questions, which is built based on the composition types. ComplexBench accompanied by our proposed evaluation method is expected to systematically reveal the deficiencies of existing LLMs on complex instructions and provide insights on the improvement of LLMs when dealing with various constraints and compositions. Our main contributions are as follows:

* We propose a comprehensive hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types. We manually collect

Figure 1: An example instruction of ComplexBench. All constraint dimensions contained in the instruction are marked with underlines and colors, which are categorized into three constraint types in our taxonomy: Format, Semantic, and Utility. Below is the composition structure of the instruction, where these constraint dimensions are combined through three composition types: _And_, _Chain_, and _Selection_.

a high-quality benchmark dataset for complex-instruction following, covering all types of constraints and compositions in our taxonomy.
* We accompany the benchmark with a new automated evaluation method to accurately evaluate the ability of LLMs to follow complex instructions, which integrates the advantages of LLM-based and rule-based methods to verify each constraint and composition type and aggregates the final score via the dependency structure brought by composition types.
* We conduct experiments on the proposed benchmark for a wide range of established LLMs, systematically revealing their deficiencies on various constraints and compositions.

## 2 Related Work

**Evaluation of Instruction-Following.** Instruction following remains one of the most important factors determining the practicality of LLMs . Therefore, numerous studies have attempted to evaluate it from various aspects. Earlier works used to focus on simple human instructions formed with mostly a single constraint, such as semantic [5; 4; 6] and format [19; 22; 23] constraints. Since LLMs have been gradually applied to address complex real-world tasks, users have to form complex instructions, which naturally call for the evaluation of the LLMs' ability in complex instruction following [3; 7]. WizardLM  employs two strategies, _In-Breadth Evolving_ and _In-depth Evolving_, to form complex instructions from simple ones. CELLO  defines complex instructions from task descriptions and input text, and evaluates LLMs with real-world scenarios data. Unlike our work, which includes subjective and objective constraints and combines LLM-based and rule-based evaluations, CELLO focuses only on objective, rule-verifiable constraints and uses rule-based scoring functions for evaluation. Nonetheless, we argue that these benchmarks neglect to model the composition of constraints, which is an important character in complex instructions and brings non-negligible structural complexity that is crucial to assessing LLMs' abilities.

**Compositionality in NLP.** Previous studies have explored compositionality across traditional NLP tasks, including semantic parsing [24; 25; 26], machine translation [26; 27], style transfer , and data-to-text generation . However, in the task of instruction-following, how the LLMs deal with the compositionality in instructions is still under-explored. CompMCTG  investigates the compositionality of multiple control attributes for LLMs, which is a topic neighboring ours. Nevertheless, our work studies more complex composition types beyond simple coordination between different constraints, such as _Chain_ and _Selection_ and their nested structures, which form the basis of many real-world complex tasks for LLMs.

## 3 ComplexBench Framework

### Overview

To comprehensively evaluate the ability of LLMs to follow complex instructions, we propose a hierarchical taxonomy to define constraints and composition types. For constraints, we extend common constraints in controlled text generation tasks to the instruction-following tasks and consider a two-level structure including coarse-grained types and fine-grained dimensions (Section 3.2). As for compositions that indicate structural combinations of constraints, we consider the characteristics of instruction-following tasks to define the composition types according to existing works on compositionality in traditional NLP tasks (Section 3.3).

   Benchmark & Data Size &  Constraint \\ Taxonomy \\  &  \\ _And_ \\  &  _China_ \\  &  _Selection_ \\  &  _Nested_ \\  &  LLM-based \\  &  Rule-based \\  & 
 Aggregation Function \\  \\  WizardLM Testset  & 218 & - & ✓ & - & - & - & ✓ & - & - \\ CELLO  & 523 & 4 & ✓ & ✓ & - & - & ✓ & Average \\ FollowBench  & 820 & 5 & ✓ & - & - & - & ✓ & Average \\ IFEval  & 541 & 25 & ✓ & - & - & - & ✓ & Average \\ InfBench  & 500 & 5 & ✓ & - & - & - & ✓ & Average \\ Col Testset  & 1,068 & - & ✓ & ✓ & - & - & ✓ & - \\ 
**ContainsBrecon (ours)** & **1,150** & **4-19** & ✓ & ✓ & ✓ & ✓ & ✓ & Dependency-based Aggregation \\   

Table 1: Comparisons between ComplexBench and other benchmarks, illustrating the features including dataset sizes, constraint taxonomies, composition types, and evaluation methods. - in Aggregation Function means there is no step to evaluate each constraint and aggregate the final score.

### Constraint

Following existing works on controlled text generation and instruction following , we propose a two-level structure for constraints including 4 constraint types (i.e., Lexical, Format, Semantic, and Utility) and 19 specific constraint dimensions which are further divided from the above types. The distribution of these constraint types and dimensions within ComplexBench is shown in Figure 2. We present the definitions of constraint types in the following and describe the details of the constraint dimensions in Appendix D.

**Lexical Constraint** requires to output specific keywords or phrases or precisely generate texts that are related to specific keywords mentioned in the instructions .

**Format Constraint** specifies the requirements on the output structure (such as JSON, Markdown, and bullet points), length, and patterns of the output, where the patterns include punctuation, content at the beginning or end, and the output templates. Format constraints require LLMs to possess a precise understanding and planning of the output content, which remain challenging for current LLMs .

**Semantic Constraint** specifies the topic , language style , personality , and sentiment  of the output, which are common constraints in the existing works on controlled text generation.

**Utility Constraint** measures the language, helpfulness, supportiveness, consistency, and factuality of generated texts, which are holistic properties. Among these, helpfulness indicates whether the generated text can complete the basic task included in the instruction (such as _Please introduce the following painting_. in Figure 1) regardless of satisfaction of other constraints, while supportiveness means whether the generated text is faithful to the instruction.

### Composition

As shown in Figure 3, we propose 4 composition types that indicate typical structural combinations of constraints.

Figure 3: Composition types in ComplexBench. Each node is a part of an instruction. The purple node may contain other composition types, while the blue node does not. In addition to 4 basic types, the last row also shows a nested selection type.

Figure 2: Constraint distribution of ComplexBench. The Utility constraints helpfulness and factuality possess a high proportion due to their prevalence in various instructions, which are basic requirements for high-quality outputs.

**Single.** The output is required to satisfy a single constraint, with no composition involved.

**And.** The output needs to satisfy multiple constraints simultaneously. This simple composition type commonly appears in most of the existing benchmarks on complex instruction-following .

**Chain.** The output is required to complete multiple tasks in the instruction sequentially, each of which may contain several constraints. Formally, _Chain_ contains \(n\) tasks \(\{T_{1},T_{2},,T_{n}\}\), which need to be completed sequentially. The output of \(T_{k+1}\) may depends on that of \(T_{k}\) (\(k=1,2,,n-1\)).

**Selection.** The output is required to select different branches according to certain conditions, fulfilling the constraints of the corresponding branch. Formally, _Selection_ contains \(m\) branches \(\{B_{1},B_{2},,B_{m}\}\), each of which is a task with expected outputs \(Y_{1},Y_{2},,Y_{m}\) respectively. We denote a selection function as \(S\) with a range \(\{1,2,,m\}\), taking the selection condition _cond_ as input. Finally, the expected output of the instruction is \(Y_{S()}\).

It's worth noting that the above composition types can be nested to construct more complex structures. Each task in _Chain_ and each branch in _Selection_ may also contain other composition types. As shown in the last row of Figure 3, a branch of _Selection_ can also contain _Selection_, thus forming a nested selection composition type.

To verify the necessity and comprehensiveness of the composition types considered in ComplexBench, we analyze the distribution of composition types in real-world scenarios. We collect instructions with high demand and representativeness from an online LLM-based chat service platform that serves more than a million users daily including general and professional instructions. General instructions refer to the instructions used by individual users in routine scenarios, while professional instructions refer to those used by enterprise-level users in business and research scenarios. For each category of instructions, we randomly sample 300 instructions and manually count the number of instructions containing each composition type. We found that the taxonomy of ComplexBench fully covers present composition types. As shown in Figure 4, although the composition types of general instructions are relatively simple and have already been covered by current benchmarks, professional instructions include more complex composition types, such as _Selection_ and nested structures of multiple composition types, which have rarely been considered by current benchmarks. As LLMs have been gradually applied to deal with complex instructions in professional scenarios, it is necessary to evaluate their ability to follow instructions with multiple constraints composition.

## 4 ComplexBench Construction

### Data Collection

We manually construct ComplexBench based on the taxonomy described in Section 3. The detailed construction pipeline consists of four steps, i.e., **Reference Instructions Collection**, **Task Allocation**, **Data Annotation and Validation**, and **Selection Branch Expansion**. We initially used our proposed method to construct Chinese data, while also providing an English version of ComplexBench. More details are in Appendix F, G, and H.

**Reference Instruction Collection.** Considering the difficulty of constructing complex instructions from scratch, annotators are required to create new complex instructions based on provided reference instructions. We collect reference instructions from real-world application scenarios and open-source instruction following benchmarks . We conduct strict desensitization of privacy and carefully filter these instructions using category and quality classifiers.

**Task Allocation.** To ensure comprehensive coverage of each constraint and composition type, we partition the entire dataset construction into multiple annotation tasks. Each annotation task has different requirements for the minimal number of constraint dimensions in each constraint type and composition type. Annotators are required to modify reference instructions to meet the requirements of corresponding tasks. To alleviate the annotation cost, especially when the constraint

Figure 4: Composition type distribution of general and professional instructions.

dimensions in the reference instructions and task requirements are different, we leverage GPT-4  to automatically acquire the constraint dimensions in reference instructions and assign them to corresponding annotation tasks according to minimal editing distance.

**Data Annotation and Validation.** Given reference instructions and corresponding annotation task requirements, annotators are expected to construct new complex instructions and annotate the constraint dimensions and composition types. After the data annotation, newly constructed instructions are cross-validated by other annotators. The process of validation continues until constructed instructions meet the following criteria: (1) Clarity & Reasonableness: The instruction should be easy to understand, unambiguous, and realistic, with at least one reasonable answer. (2) Validity of Constraints: Every constraint within the instruction should substantially influence the output. (3) Complexity & Difficulty: The instruction should be challenging for most LLMs and be capable of distinguishing the complex instruction-following abilities of different LLMs.

**Selection Branch Expansion.** When evaluating the ability of LLMs to follow instructions containing _Selection_, the predisposition toward random selection by LLMs may bring potential bias because most instructions cover only one correct selection branch. But the probability of the correct branch appearing at each position in _Selection_ of data constructed by annotators is unequal 2. To address this issue, in the final stage of instruction construction, we manually modify the selection condition based on the selection function to construct multiple instructions that cover different correct branches, ensuring an equal probability of the correct branch appearing at each position.

### Evaluation Protocol

To conduct a detailed evaluation of how well each constraint and composition type is satisfied, we draw inspiration from previous works that transform text evaluation into multiple question-answering tasks [16; 17; 7]. For each constraint and composition type specified in an instruction, we manually craft a scoring question that can be succinctly answered with either "yes" or "no."

Current mainstream evaluation methods contain LLM-based [18; 3; 7] and rule-based methods [8; 19; 20]. In our preliminary experiments, we find that LLM-based methods are effective at answering open-ended scoring questions, but they demonstrate a significant deficiency in those involving numerical computation, counting, and other objective rule-defined areas, such as keyword inclusion and text length. Simultaneously, rule-based methods perform well in rule-defined areas but are powerless against open-ended scoring problems. To address their limitations, we design a Rule-Augmented LLM-based (RAL) evaluation method to equip LLM evaluators with rules to answer

Figure 5: An exemplar evaluation process of ComplexBench. Given an instruction and its scoring questions, ComplexBench integrates the rule and LLM evaluator to verify each of them and aggregates the final score based on the dependency structure of composition types in the instruction.

scoring questions in both rule-defined and open-ended areas. For the instruction \(I\), the generated response to be evaluated \(o\), and the scoring problem \(q\), if \(q\) is verifiable by rules, we first use the LLM to automatically extract segments \(e\) of \(o\), which is related to scoring question \(q\). Subsequently, we use the rule \(R_{q}\) written for \(q\) to obtain the evaluation result \(r_{q}\{0,1\}\), that is:

\[e=_{ex}(I,q,o)\] (1)

\[r_{q}=R_{q}(e)\] (2)

where \(_{ex}\) indicates the LLM with the prompt used for extraction. Otherwise, if \(q\) cannot be verified by rules, we directly use the LLM to measure the quality of \(o\):

\[r_{q}=_{eva}(I,q,o)\] (3)

where \(_{eva}\) denotes the LLM with the prompt used for evaluation. For composition types, considering that their satisfaction is a prerequisite for satisfying some constraints, we model the dependencies of its scoring questions. Specifically, for _Chain_, all the scoring questions of the subsequent task depend on the answers to those of the preceding task. And for _Selection_, all the scoring questions of the selection branch depend on whether the correct selection branch is selected. If a scoring question is judged as "no", all the scoring questions depending on it will also be directly judged as "no". Formally, we denote the set of scoring questions that \(q\) depends on as \(Dep(q)\). After all scoring questions have been independently verified, _Dependency Aggregation_ will be performed, and the result of \(q\) will be calculated as follows:

\[r_{q}^{{}^{}}=r_{q}_{p Dep(q)}r_{p}\] (4)

Finally, following InfoBench , we calculate Decomposed Requirements Following Ratio (DRFR) as the final score during _Score Aggregation_. Considering a benchmark dataset has \(N\) instructions, the instruction \(i\) has \(m_{i}\) scoring questions, and the result of the \(j\)-th scoring question is \(r_{ij}^{{}^{}}\), the metric is calculated as: \(DRFR=_{i,j}r_{ij}^{{}^{}}/_{i}m_{i}\). Figure 5 shows a framework of our evaluation protocol.

### Benchmark Statistics

ComplexBench contains 1,150 instructions and 5,306 scoring questions, as shown in Table 2. Nesting depth means the maximum depth of composition types. In addition to three basic composition types including _And_, _Chain_, and _Selection_, we adopt a separate category whose instructions simultaneously contain _Chain_ and _Selection_, aiming to use these two challenging types to explore the boundary of LLMs' ability in complex instruction-following3. We also present the task distribution of ComplexBench in Appendix C.

## 5 Experiments

### Agreement Evaluation

To measure the agreement between our evaluation method and manual evaluation, we randomly sample 200 instructions from ComplexBench to construct a meta-evaluation dataset. Five LLMs are involved in this evaluation as generation models. We employ GPT-4-1106  as our primary judge and adopt two metrics to confirm the reliability of our method: (1) **Overall Pairwise Agreement**: Given an instruction, two model responses (denoted as A and B), the human annotators are instructed to compare the quality and choose from 3 options, namely A better than B, tie, B better than A. Subsequently, the automatic evaluation scores for two model responses are converted into pairwise

    &  Nesting \\ Depth \\  } &  &  &  &  \\    & & & & & \\  And & 1 & 475 & 279.39 & 4.09 & 4.14 \\  Chain & 1 & 70 & 352.11 & 4.83 & 4.94 \\  & 2 & 170 & 486.84 & 6.24 & 6.32 \\  Selection & 1 & 80 & 753.15 & 2.91 & 2.06 \\  & 2 & 224 & 664.13 & 4.40 & 3.09 \\  & 2 & 34 & 1409.93 & 5.76 & 3.78 \\  Selection \& 2 & 30 & 440.37 & 4.37 & 3.63 \\ Chain & \(\) 3 & 55 & 398.82 & 6.18 & 5.27 \\  Overall & - & 1150 & 477.51 & 4.61 & 4.19 \\   

Table 2: Statistics of ComplexBench including the number of instructions (**#Inst.**), the average number of characters (**#Len.**), scoring questions (**#Ques.**), and constraints (**#Con.**) per instruction.

comparisons to measure agreement with human annotators. (2) **Question-level Agreement**: Given an instruction and a model response, human annotators are instructed to judge whether each scoring question is satisfied respectively. Then, we calculate the agreement between automatic evaluation results and human-annotated ones.

For the Overall Pairwise Agreement, we sample 500 pairs from the outputs of 5 LLMs. Direct Scoring serves as a baseline, which adopts a scoring prompt  to assign a score to the response with a scale of 1-10. As shown in Table 3, our method can improve the agreement with manual evaluations compared to Direct Scoring with a large margin. _Dependency Aggregation_ also shows its important contribution to our method

For the Question-level Agreement, the scoring questions in the meta-evaluation dataset are categorized into two types: (1) Rule-defined, which can be verified by rules and constitutes 17% of the total, and (2) Open-ended, which is not verifiable by rules. We compare our method with Direct Scoring, which considers a response with a score above 5 to satisfy all scoring questions of an instruction. We also remove rule arguments (w/o rule) to verify its effectiveness. As shown in Table 4, RAL outperforms all the baselines and exhibits an impressive 87.82% agreement with humans at the overall level. The LLM-based evaluator (i.e., RAL w/o rule in Table 4) shows its weakness in rule-defined areas that rule arguments mainly contribute to, supporting our motivation.

### Automatic Evaluation

#### 5.2.1 Setup

We use GPT-4-1106  as our judge to evaluate 15 LLMs: (1) **Closed-source LLMs**: GPT-4-1106, Claude-3-Opus , GLM-4 , ERNIE Bot-4, GPT-3.5-Turbo-1106. (2) **Open-source LLMs**: Qwen1.5-Chat , Llama3-Instruct , InternLM2-Chat , Baichuan2-Chat , Mistral-Instruct , InternLM2-Chat , ChatGLM3-Chat . The sizes of these models vary from 6B to 72B. We use greedy search for reproducibility, and the maximum generation length is 8,192.

#### 5.2.2 Main Results

The main results are shown in Table 5. **Firstly**, the widely recognized powerful GPT-4 still fails to complete 20% of complex instructions, highlighting the necessity of complex instruction evaluation. **Secondly**, as the complexity of composition types within instruction increases, the performance of all LLMs significantly drops, especially on _Selection_ and _Chain_. This aligns with our motivation for constructing complex composition types. **Thirdly**, the performance of most open-source LLMs falls short compared to closed-source LLMs especially on complex composition types, indicating that open-source LLMs still have a large room for improvement in chasing the capabilities of closed-source LLMs.

To dissect the ability of LLMs to follow specific constraint and composition types, we calculate the average accuracy of scoring questions for

  
**Subset** & **Evaluator** & **Agreement between human** \\   & RAL & **95.36\%** \\  & RAL w/o rule & 82.02\% \\  & Direct Scoring & 62.02\% \\   & RAL & **86.28\%** \\  & RAL w/o rule & **86.28\%** \\  & Direct Scoring & 77.83\% \\   & RAL & **87.82\%** \\  & RAL w/o rule & 85.56\% \\   & Direct Scoring & 75.18\% \\   

Table 4: Question-level Agreement with human.

Figure 6: The performance of LLMs on different constraint and composition types.

  
**Evaluation Method** & **Pairwise Agreement** \\  Ours & **0.614** \\ Ours w/o _Dep._ & 0.574 \\ Direct Scoring & 0.512 \\   

Table 3: Overall Pairwise Agreement with human. _Dep._ means _Dependency Aggregation_.

each type. The results are shown in Figure 6. **Firstly**, for constraints, LLMs generally perform better on Semantic and Utility constraints but struggle with the Format and Lexical constraints that have explicit evaluation standards. **Secondly**, for compositions, _Chain_ presents severe challenges while _Selection_ come second. We speculate that the main difficulty in _Selection_ lies not only in choosing the correct branch but in executing it without interference from irrelevant branches. More results and analyses are in Appendix I.

#### 5.2.3 Analysis

**Decomposition of instructions with composition types.** To explore whether decomposing complex instructions and executing them through multi-round interactions can improve the performance of LLMs, we manually decompose ComplexBench instructions based on composition types (e.g., _Chain_ into sequential tasks, _Selection_ into selection and execution branches, while _And_ remains intact) and compare the performance of LLMs between executing decomposed instructions step-by-step and original instructions in one step. The scoring questions of original instructions are split into corresponding decomposed ones with the same dependencies to ensure a fair comparison.

Table 6 shows that GPT-3.5-Turbo-1106 generally performs worse in decomposed instructions, especially as the complexity of composition types within instructions increases. We conjecture that this is due to cumulative errors in multi-round interactions, highlighting that our benchmark is challenging and cannot be simply solved via instruction decomposition.

**The Coherent Test for Selection.** To comprehensively measure the performance of LLMs on different conditions of _Selection_, we merge the instructions with the same branches and selection functions but different conditions into the same task group. For example, the instruction about the Mona Lisa shown in Table 1 and another instruction where everything else remains the same except the final condition "Painting: Mona Lisa" is changed to "Painting: Galloping horse" are merged into the same task group. The two instructions need to execute two different selection branches. We calculate the proportion of instructions with all scoring questions correct (_Original Test_) and group tasks with all scoring questions correct (_Coherent Test_). Formally, considering that there are \(N\) instructions containing Selection, they are divided into \(K\) task groups. Each instruction \(i\) has \(m_{i}\) scoring ques

    &  **Nesting** \\ **Depth** \\  } &  &  &  \\ 
**And** & 1 & 0.845 & 0.845 & 0.000 \\   & 1 & 0.686 & 0.655 & -0.031 \\  & 2 & 0.630 & 0.583 & **-0.047** \\   & 1 & 0.661 & 0.631 & -0.030 \\  & 2 & 0.561 & 0.520 & -0.041 \\  & \( 3\) & 0.475 & 0.411 & **-0.064** \\   & 2 & 0.565 & 0.504 & -0.061 \\  & \( 3\) & 0.482 & 0.415 & **-0.067** \\ 
**Overall** & - & 0.682 & 0.652 & -0.030 \\   

Table 6: The performance of GPT-3.5-Turbo-1106 on original and decomposed instructions.

  
**Category** & **And** &  &  &  &  \\ 
**Nesting Depth** & 1 & 1 & 2 & Avg. & 1 & 2 & \( 3\) & Avg. & 2 & \( 3\) & Avg. & Avg. \\   \\  GPT-4-1106 & 0.881 & **0.787** & 0.759 & 0.766 & **0.815** & **0.772** & **0.694** & **0.765** & 0.802 & 0.626 & 0.675 & **0.800** \\ Claude-3-Opus & **0.886** & 0.784 & **0.779** & **0.780** & 0.764 & 0.749 & 0.592 & 0.724 & 0.695 & 0.576 & 0.609 & 0.788 \\ GLM-4 & 0.868 & 0.763 & 0.739 & 0.745 & 0.768 & 0.739 & 0.626 & 0.724 & **0.809** & **0.647** & **0.692** & 0.779 \\ ERNIEBot-4 & 0.866 & 0.749 & 0.735 & 0.738 & 0.725 & 0.696 & 0.649 & 0.692 & 0.756 & 0.600 & 0.643 & 0.764 \\ GPT-3.5-Turbo-1106 & 0.845 & 0.686 & 0.630 & 0.644 & 0.661 & 0.561 & 0.475 & 0.561 & 0.565 & 0.482 & 0.505 & 0.682 \\   \\  Qwen1.5-72B-Chat & 0.873 & 0.749 & 0.730 & 0.735 & 0.751 & 0.698 & 0.521 & 0.675 & 0.611 & 0.521 & 0.546 & 0.752 \\ Llama-3-70B-Instruct & 0.858 & 0.769 & 0.722 & 0.733 & 0.747 & 0.704 & 0.675 & 0.706 & 0.573 & 0.571 & 0.571 & 0.757 \\  InterLM2-20B-Chat & 0.796 & 0.666 & 0.648 & 0.652 & 0.648 & 0.599 & 0.543 & 0.597 & 0.611 & 0.488 & 0.522 & 0.678 \\ Qwen1.5-14B-Chat & 0.817 & 0.657 & 0.636 & 0.641 & 0.622 & 0.621 & 0.536 & 0.606 & 0.550 & 0.435 & 0.467 & 0.680 \\ Baichuan2-13B-Chat & 0.760 & 0.583 & 0.517 & 0.533 & 0.571 & 0.479 & 0.404 & 0.480 & 0.443 & 0.409 & 0.418 & 0.591 \\  Llama-38B-Instruct & 0.778 & 0.669 & 0.568 & 0.592 & 0.597 & 0.552 & 0.483 & 0.546 & 0.626 & 0.429 & 0.484 & 0.638 \\ Mistral-P-Instruct & 0.737 & 0.574 & 0.556 & 0.560 & 0.554 & 0.493 & 0.411 & 0.488 & 0.534 & 0.374 & 0.418 & 0.592 \\ Qwen1.5-P8B-Chat & 0.802 & 0.598 & 0.611 & 0.608 & 0.519 & 0.564 & 0.570 & 0.558 & 0.634 & 0.491 & 0.531 & 0.658 \\ InterLM2-7B-Chat & 0.755 & 0.633 & 0.598 & 0.607 & 0.532 & 0.568 & 0.525 & 0.555 & 0.432 & 0.465 & 0.634 \\ ChatGLM3-6B-Chat & 0.701 & 0.556 & 0.490 & 0.506 & 0.455 & 0.430 & 0.411 & 0.431 & 0.573 & 0.312 & 0.384 & 0.546 \\   

Table 5: DRFR of LLMs computed by our proposed RAL method. The highest performance among open-source models is underlined, while the highest performance overall is **bold**.

of the \(j\)-th scoring question is \(r^{{}^{}}_{ij}\) (the same definition as Section 4.2). The results of _Original Test_ will be calculated as \(_{i=1}^{N}_{j=1}^{m}r^{{}^{}}_{ij}\), and the results of _Coherent Test_ will be calculated as \(_{k=1}^{K}_{i Group(k)}(_{j=1}^{m}r^{{}^{ }}_{ij})\). Instructions containing Selection are categorized as either single-layer or multi-layer nested, respectively. As shown in Figure 7, for single-layer Selection instructions, LLMs with stronger instruction-following abilities show a smaller performance drop in the coherent test, which better understands the selection structure. For more complex multi-layer nested Selection instructions, even the state-of-the-art LLM, GPT-4, achieves only 14.9% accuracy in the coherent test. At the same time, smaller-scale LLMs can't perfectly follow any group of instructions. The results highlight current LLMs' weaknesses in following multi-layer tree-structured instructions.

Comparisons between Other Capabilities.We compare the performance of representative LLMs across 3 prominent LLM evaluation benchmarks in addition to ComplexBench: IFEval, HumanEval, and MATH, focusing on instruction-following, coding, and mathematical ability, respectively. As shown in Table 7, although the performance of various LLMs on ComplexBench is well correlated with their performance on other benchmarks, the rankings of LLMs on ComplexBench do not entirely correspond with those on the other three benchmarks. For instance, ChatGLM3-6B-Chat demonstrates outstanding coding and mathematical abilities among LLMs of similar scale, but it notably struggles with complex instruction-following. On the other hand, while Llama-3-70B-Instruct surpasses GPT-4-1106 on IFEval and ranks first, it still shows a performance gap with GPT-4-1106 on ComplexBench. This discrepancy is primarily in the areas of instructions with complex constraints composition, which are not covered by IFEval, indicating that ComplexBench can provide a complementary perspective for LLM evaluation.

## 6 Conclusion

In this work, we propose ComplexBench, a systematical benchmark for complex instruction-following. We first propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types. Furthermore, we manually collect a high-quality dataset accordingly. Along with the dataset, we propose a structure-aware automatic evaluation method for complex instruction-following with constraints composition and further enhance the evaluation accuracy by equipping LLM-based evaluators with rules. Finally, we conduct extensive experiments to evaluate the performance of current representative LLMs on complex instruction-following and uncover their significant deficiencies in dealing with complex composition types. In summary, we posit that ComplexBench can serve as a valuable tool for benchmarking the complex instruction-following ability of LLMs, providing a complementary perspective for LLM evaluation and useful insights for further work to improve this ability of LLMs.

  
**Model** & **ComplexBench** & **IFEval** & **HumanEval** & **MATH** \\  GPT-4-1106 & 0.800 & 75.4 & 84.6 & 64.3 \\ GLM-4 & 0.779 & 66.7 & 72.0 & 47.9 \\ Queni-5.72B-Chat & 0.752 & 55.8 & 71.3 & 42.5 \\ Llama-3-70B-Instruct & 0.757 & 78.9 & 81.7 & 50.4 \\ Llama-3-8B-Instruct & 0.638 & 68.6 & 62.2 & 30.0 \\ Mistral-7B-Instruct & 0.592 & 40.5 & 30.5 & 13.1 \\ Queni-5.7B-Chat & 0.658 & 38.8 & 46.3 & 23.2 \\ InternlM2-7B-Chat & 0.634 & 46.5 & 59.8 & 23.0 \\ ChatGLM3-6B-Chat & 0.546 & 28.1 & 64.0 & 25.7 \\  Correlation with ComplexBench & - & 0.814 & 0.715 & 0.895 \\   

Table 7: Model comparison on different abilities. The last row shows the Pearson correlation between the performance of LLMs in ComplexBench and other benchmarks.

Figure 7: The performance variance under the coherent test for _Selection_. The left side represents single-layer _Selection_ instructions, and the right side corresponds to multi-layer _Selection_ instructions.