# Auditing for Human Expertise

Rohan Alur

EECS

MIT

ralur@mit.edu &Loren Laine

School of Medicine

Yale

loren.laine@yale.edu &Darrick K. Li

School of Medicine

Yale

darrick.li@yale.edu &Manish Raghavan

Sloan, EECS

MIT

mragh@mit.edu &Devavrat Shah

EECS, IDSS, LIDS, SDSC

MIT

devavrat@mit.edu &Dennis Shung

School of Medicine

Yale

dennis.shung@yale.edu

###### Abstract

High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts _add value_ which could not be captured by an algorithmic predictor.

We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs ('features'). A rejection of our test thus suggests that human experts may add value to _any_ algorithm trained on the available data, and has direct implications for whether human-AI 'complementarity' is achievable in a given prediction task.

We highlight the utility of our procedure using admissions data collected from the emergency department of a large academic hospital system, where we show that physicians' admit/discharge decisions for patients with acute gastrointestinal bleeding (AGIB) appear to be incorporating information that is not available to a standard algorithmic screening tool. This is despite the fact that the screening tool is arguably _more_ accurate than physicians' discretionary decisions, highlighting that - even absent normative concerns about accountability or interpretability - accuracy is insufficient to justify algorithmic automation.

## 1 Introduction

Progress in machine learning, and in algorithmic decision aids more generally, has raised the prospect that algorithms may complement or even automate human decision making in a wide variety of settings. If implemented carefully, these tools have the potential to improve accuracy, fairness, interpretability and consistency in many prediction and decision tasks. However, a primary challenge in nearly all such settings is that some of the relevant inputs - 'features,' in machine learning parlance - are difficult or even impossible to encode in a way that an algorithm can easily consume. For example, doctors use direct conversations with patients to inform their diagnoses, and sports franchises employ professional scouts to qualitatively assess prospective players. One can think of these experts as incorporating information which is practically difficult to provide to an algorithm, particularly astabular data, or perhaps exercising judgment which is infeasible to replicate with a computational process. Either perspective presents a challenge when deploying predictive algorithmic tools, as any such model will necessarily fail to incorporate at least some of the information that a human expert might consider. Thus, as we seek to use algorithms to improve decision-making, we must answer the following question:

_For a given prediction task, do human experts add value which could not be captured by any algorithmic forecasting rule?_

The answer to this question has significant consequences: if experts are incorporating salient but hard-to-quantify information, we might attempt to somehow ensemble or combine the human and algorithmic predictions; this is commonly referred to as seeking 'complementarity' in the literature on human-machine interaction. On the other hand, if it appears that an expert is not extracting signal beyond whatever is contained in the available features, we might consider whether we can automate the prediction task entirely, or at least reduce the degree to which a human may override algorithmic recommendations.

At this stage it is worth asking - why not simply compare the prediction accuracy of a human expert to that of a particular predictive algorithm? If the human expert performs better than a competing algorithm, we might say the expert adds value which is not captured by the algorithm. However, as the example presented next illustrates, it is possible for the expert to incorporate information that could not be captured by _any_ learning algorithm - even when the expert substantially underperforms a particular algorithm trained to accomplish the same task. Indeed, this is not just a hypothetical: a large body of prior literature (see e.g. Agrawal (2019) for a comprehensive overview) suggests that humans reliably underperform even simple statistical models, and in Section 5, we find exactly this dynamic in real-world patient triage data. Nonetheless, as we highlight next, humans may still add valuable information in a given forecasting task.

**An illustration: experts may add information despite poor predictions.** Let \(Y\) denote the outcome of interest and let \(X,U\) be features that drive the outcome. Specifically, let

\[Y=X+U+_{1},\] (1)

where \(_{1}\) is some exogenous noise. For the purposes of this stylized example, we'll assume that \(X,U,_{1}\) are all zero mean and pairwise independent random variables. Suppose the human expert can observe both \(X\) and \(U\), but only \(X\) is made available to a predictive algorithm. An algorithm tasked with minimizing squared error might then seek to precisely estimate \([Y|X]~{}=~{}X\). In contrast, the expert may instead use simpler heuristics to construct an estimate which can be modeled as

\[=(X)+(U)+_{2},\] (2)

where \(_{2}\) is independent zero-mean noise, and can be thought of as modeling idiosyncracies in the expert's cognitive process1. As discussed in detail in Appendix F, there exist natural distributions over \((X,U,_{1},_{2})\) such that the algorithm performs substantially better than the expert in terms of predictive accuracy. In fact, we show that there exist natural distributions where the algorithm outperforms the expert even under any linear post-processing of \(\) (e.g., to correct for expert predictions which are highly correlated with \(Y\) but perhaps incorrectly centered or scaled). Nonetheless, the expert predictions clearly contain information (cf. \((U)\)) that is not captured by the algorithm.

However, because \(U\) is not recorded in the available data, it is not obvious how to distinguish the above scenario from one in which the expert only extracts signal from \(X\). For example, they might instead make predictions as follows:

\[=(X)+_{2}.\] (3)

While a learning algorithm may outperform the expert in both cases, the expert in scenario (2) still captures valuable information; the expert in (3) clearly does not. The goal of this work will be to develop a test which allows us to distinguish between scenarios like these without the strong modeling assumptions made in this example.

**Contributions.** To understand whether human experts can add value for a given prediction task, we develop a statistical framework under which answering this question becomes a natural hypothesis test. We then provide a simple, data-driven procedure to test this hypothesis. Our proposed algorithm takes the form of a conditional independence test, and is inspired by the Model-X Knockoffs framework of Candes et al. (2016), the Conditional Permutation Test of Berrett et al. (2018) and the 'Model-Powered' test of Sen et al. (2017). Our test is straightforward to implement and provides transparent, interpretable p-values.

Our work is closely related to a large body of literature comparing human performance to that of an algorithm (Cowsgill (2018), Dawes et al. (1989), Grove et al. (2000), among others), and developing learning algorithms which are complementary to human expertise (Madras et al. (2018), Raghu et al. (2019), Mozannar and Sontag (2020), Keswani et al. (2021), Agrawal et al. (2018), Bansal et al. (2020), Rastogi et al. (2022) and Bastani et al. (2021)). However, although similarly motivated, we address a different problem which is in a sense 'upstream' of these works, as we are interested in _testing_ for whether a human forecaster demonstrates expertise which cannot be replicated by any algorithm. Thus, we think of our test as assessing as a necessary condition for achieving human-AI complementarity; success in practice will further depend on the ability of a mechanism designer to actually incorporate human expertise into some particular algorithmic pipeline or feedback system. We discuss these connections further in Appendix A.

We apply our test to evaluate whether emergency room physicians incorporate valuable information which is not available to a common algorithmic risk score for patients with acute gastrointestinal bleeding (AGIB). To that end, we utilize patient admissions data collected from the emergency department of a large academic hospital system. Consistent with prior literature, we find that this algorithmic score is an exceptionally sensitive measure of patient risk - and one that is highly competitive with physicians' expert assessments. Nonetheless, our test provides strong evidence that physician decisions to either hospitalize or discharge patients with AGIB are incorporating valuable information that is not captured by the screening tool. Our results highlight that prediction accuracy is not sufficient to justify automation of a given prediction task. Instead, our results make a case for experts working _with_ a predictive algorithm, even when algorithms might handily outperform their human counterparts.

**Organization.** In Section 2, we formalize the problem of auditing for expertise, and in Section 3 we present our data-driven hypothesis test. Section 4 then examines the theoretical properties of the test. In Section 5 we present our empirical findings from applying this test to real-world patient triage data. Finally, Section 6 provides discussion of our results and directions for future work. We also include a discussion of additional related work in Appendix A, and provide numerical simulations to corroborate our theoretical and empirical results in Appendix F2.

## 2 Setup and question of interest

We consider a generic prediction task in which the goal is to forecast some outcome \(Y\) on the basis of observable features \(X\). The human expert may additionally have access to some auxiliary private information \(U\). For concreteness, let \(=^{d}\) for some \(d 1\).

We posit that the outcome \(Y\) is generated as follows: for some unknown function \(f:\),

\[Y=f(X,U)+_{1},\] (4)

where, without loss of generality, \(_{1}\) represents mean zero idiosyncratic noise with unknown variance.

We are also given predictions by a human expert, denoted as \(\). We posit that the expert predictions \(\) are generated as follows: for some unknown function \(:\),

\[=(X,U)+_{2},\] (5)

where \(_{2}\) also captures mean zero idiosyncratic noise with unknown variance.

We observe \((X,Y,)\) which obey (4)-(5); the private auxiliary feature \(U\) is not observed. Concretely, we observe \(n\) data points \((x_{i},y_{i},_{i}),i[n]\{1,,n\}\).

Our goal is to answer the question "do human experts add information which could not be captured by any algorithm for a given prediction task?" We assume that any competing learning algorithm can utilize \(X\) to predict \(Y\), but it can not utilize \(U\). Thus, our problem reduces to testing whether \(U\) has some effect on both \(Y\) and \(\), which corresponds to the ability of an expert to extract signal about \(Y\) from \(U\). If instead \(U\) has no effect on \(Y\), \(\) or both (either because \(U\) is uninformative about \(Y\) or the expert is unable to perceive this effect), then conditioned on \(X\), \(Y\) and \(\) are independent. That is, if the human expert fails to add any information which could not be extracted from the observable features \(X\), the following must hold:

\[H_{0}:Y\!\!\! X.\] (6)

Intuitively, \(H_{0}\) captures the fact that once we observe \(X\), \(\) provides no _additional_ information about \(Y\) unless the expert is also making use of some unobserved signal \(U\) (whether explicitly or implicitly). In contrast, the rejection of \(H_{0}\) should be taken as an evidence that the expert (or experts) can add value to _any_ learning algorithm trained on the observable features \(X\); indeed, a strength of this framework is that it does not require specifying a particular algorithmic baseline. However, it's worth remarking that an important special case is to take \(X\) to be the prediction made by some _specific_ learning algorithm trained to forecast \(Y\). In this setting, our test then reduces to assessing whether \(\) adds information to the predictions made by this learning algorithm, and can be viewed as a form of feature selection.

**Goal.** Test the null hypothesis \(H_{0}\) using observed data \((x_{i},y_{i},_{i}),i[n]\{1,,n\}\).

To make this model concrete, in Section 5 we use our framework to test whether emergency room physicians incorporate information that is not available to a common algorithmic risk score when deciding whether to hospitalize patients. Accordingly, we let \(X^{9}\) be the inputs to the risk score, \(\{0,1\}\) be a binary variable indicating whether a given patient was hospitalized, and \(Y\{0,1\}\) be an indicator for whether, in retrospect, a patient _should_ have been hospitalized. The risk score alone turns out to be a highly accurate predictor of \(Y\), but physicians take many other factors into account when making hospitalization decisions. We thus seek to test whether physicians indeed extract _signal_ which is not available to the risk score (\(Y\!\!\! X\)), or whether attempts to incorporate other information and/or exercise expert judgement simply manifest as noise (\(Y\!\!\! X\)).

## 3 ExpertTest: a statistical test for human expertise

To derive a statistical test of \(H_{0}\), we will make use of the following elementary but powerful fact about exchangeable random variables.

**A test for exchangeability.** Consider \(K+1\) random variables \((Z_{0},,Z_{K})\) which are exchangeable, i.e. the joint distribution of \((Z_{0},,Z_{K})\) is identical to that of \((Z_{(0)},,Z_{(K)})\) for any permutation \(:\{0,,K\}\{0,,K\}\). For example, if \(Z_{0},,Z_{K}\) are independent and identically distributed (i.i.d.), then they are exchangeable. Let \(F\) be a function that maps these variables to a real value. For any such \(F()\), it can be verified that the order statistics (with any ties broken uniformly at random) of \(F(Z_{0}),,F(Z_{K})\) are uniformly distributed over \((K+1)!\) permutations of \(\{0,,K\}\). That is, \(_{K}\) defined next, is distributed uniformly over \(\{0,1/K,2/K,,1\}\):

\[_{K}=_{k=1}^{K}[F(Z_{0}) F(Z_{k})]\] (7)

where we use definition \([]=1\) if \(<\) and \(0\) if \(>\). If instead \(=\), we independently assign it to be \(1\) or \(0\) with equal probability. Thus, if \((Z_{0},,Z_{K})\) are exchangeable, then \((_{K})+1/(K+1)}{{}}\) and we can reject the hypothesis that \((Z_{0},,Z_{K})\) are exchangeable with p-value (effectively) equal to \(_{K}\).

Observe that while this validity guarantee holds for any choice of \(F()\), the _power_ of the test will depend crucially on this choice; for example, a constant function which maps every argument to the same value would have no power to reject the null hypothesis. We return to the choice of \(F()\) below.

**Constructing exchangeable distributions.** We will leverage the prior fact about the order statistics of exchangeable random variables to design a test of \(H_{0}:Y\!\!\! X\). In particular, we would like to use the observed data to construct \(K+1\) random variables that are exchangeable under \(H_{0}\), but not exchangeable otherwise. To that end, consider a simplified setting where \(n=2\), with \(x_{1}=x_{2}=x\). Thus, our observations are \(Z_{0}=\{(x,y_{1},_{1}),(x,y_{2},_{2})\}\). Suppose we now sample \((_{1},_{2})\) uniformly at random from \(\{(_{1},_{2}),(_{2},_{1})\}\). That is, we _swap_ the observed values \((_{1},_{2})\) with probability \(\) to construct a new dataset \(Z_{1}=\{(x,y_{1},_{1}),(x,y_{2},_{2})\}\).

Under \(H_{0}\), it is straightforward to show that \(Z_{0}\) and \(Z_{1}\) are independent and identically distributed conditioned on observing \((x,x),(y_{1},y_{2})\) and either \((_{1},_{2})\)\(or\)\((_{2},_{1})\). That is, \(Z_{0},Z_{1}\) are exchangeable, which will allow us to utilize the test described above for \(H_{0}\).

Why condition on this somewhat complicated event? Intuitively, we would like to resample \(=(_{1},_{2})\) from the distribution of \(_{ X}\); under the null, \((x,y,)\) and \((x,y,)\) will be exchangeable by definition. However, this requires that we know (or can accurately estimate) the distribution of \( X\), which in turn requires modeling the expert's decision making directly. Instead, we simplify the resampling process by only considering a _swap_ of the observed \(\) values between identical values of \(x\) - this guarantees exchangeable data without modeling \(_{ X}\) at all!

This approach can be extended for \(n\) larger than \(2\). Specifically, if there are \(L\) pairs of identical \(x\) values, i.e. \(x_{2-1}=x_{2}\) for \(1 L\), then it is possible to construct i.i.d. \(Z_{0},,Z_{K}\) for larger \(K\) by _randomly exchanging_ values of \(\) for each pair of data points.

As discussed above, we'll also need to choose a particular function \(F()\) to apply to \(Z_{0}\) and \(Z_{1}\). A natural, discriminatory choice of \(F\) is a _loss function_: for example, given \(D=\{(x_{i},y_{i},_{i}):i 2L\}\), let \(F(D)=_{i}(y_{i}-_{i})^{2}\). This endows \(_{K}\) with a natural interpretation - it is the probability that an expert could have performed as well as they did (with respect to the chosen loss function \(F\)) by pure chance, _without_ systematically leveraging some unobserved \(U\).

Of course, in practice we are unlikely to observe many pairs where \(x_{2-1}=x_{2}\), particularly when \(x\) takes value in a non-finite domain, e.g. \(\) or \(\). However, if the conditional distribution of \( X\) is nearly the same for _close enough_ values of \(X=x\) and \(X=x^{}\), then we can use a similar approach with some additional approximation error. This is precisely the test that we describe next.

**ExpertTest.** Let \(L 1\) be an algorithmic parameter and \(m:_{ 0}\) be some distance metric over \(\), e.g. the \(_{2}\) distance. Let \(F()\) be some loss function of interest, e.g. the mean squared error.

First, compute \(m(x_{i},x_{j}):i j[n]\) and greedily select \(L\) disjoint pairs which are as close as possible under \(m(,)\). Denote these pairs by \(\{(x_{i_{2-1}},x_{i_{2}}):[L]\}\).

Let \(D_{0}=\{(x_{i},y_{i},_{i}):i\{i_{2-1},i_{2}:[L]\}\}\) denote the observed dataset restricted to the \(L\) chosen pairs. Let \(D_{1}\) be an additional dataset generated by independently swapping each pair \((_{i_{2-1}},_{i_{2}})\) with probability \(1/2\), and repeat this resampling procedure to generate \(D_{1} D_{K}\). Next, compute \(_{K}\) as follows:

\[_{K}=_{k=1}^{K}[F(D_{0}) F(D_{k})]\] (8)

Finally, we reject the hypothesis \(H_{0}\) with \(p\)-value \(+1/(K+1)\) if \(_{K}\) for any desired confidence level \((0,1)\). Our test is thus quite simple: find \(L\) pairs of points that are close under some distance metric \(m(,)\), and create \(K\) synthetic datasets by swapping the expert forecasts for each pair independently with probability 1/2. If the expert's loss on the original dataset is "small" relative to the loss on these resampled datasets, this is evidence that the synthetic datasets are not exchangeable with the original, and thus, the expert is using some private information \(U\).

Of course, unlike in the example above, we swapped pairs of predictions for _different_ values of \(x\). Thus, \(D_{0} D_{K}\) are not exchangeable under \(H_{0}\). However, we'll argue that because we paired "nearby" values of \(x\), these datasets are "nearly" exchangeable. These are the results we present next.

Results

We provide theoretical guarantees associated with the **ExpertTest**. First, we demonstrate the validity of our test in a generic setting. That is, if \(H_{0}\) is true, then **ExpertTest** will not reject it with high probability. We then quantify this guarantee precisely under a meaningful generative model.

To state the validity result, we need some notation. For any \((x,)\) and \((x^{},^{})\), define the odds ratio

\[r((x,),(x^{},^{}))== X=x)  Q(=^{} X=x^{})}{Q(=^{ } X=x) Q(= X=x^{})},\] (9)

where \(Q(|)\) represents the density of the conditional distribution of human predictions \(|X\) under \(H_{0}\). For simplicity, we assume that such a conditional density exists.

**Theorem 1** (Validity of ExpertTest): _Given \((0,1)\) and parameters \(K 1,L 1\), the Type I error of_ **ExpertTest** _satisfies_

\[_{K}+1-(1- _{n,L}^{*})^{L}+.\] (10)

_Where \(_{n,L}^{*}\) is defined as follows_

\[_{n,L}^{*}=_{[L]}}, _{i_{2-1}}),(x_{i_{2}},_{i_{2}}))}- .\] (11)

We remark briefly on the role of the parameters \(L\) and \(K\) in this result. To begin with, \(\) is embedded in the type I error, and thus taking the number of resampled datasets \(K\) to be as large as possible (subject only to computational constraints) sharpens the validity guarantee. We also observe that the bound becomes weaker as \(L\) increases. However, observe also that **ExpertTest** is implicitly using an \(L\)-dimensional distribution (or \(L\) fresh samples) to reject \(H_{0}\), which means that increasing \(L\) also provides additional _power_ to the test.

Notice also that the odds ratio (9) is guaranteed to be \(1\) if \(x=x^{}\), regardless of the underlying distribution \(\). This is not a coincidence, and our test is based implicitly on the heuristic that the odds ratio will tend away from \(1\) as the distance \(m(x,x^{})\) increases (we quantify this intuition precisely below). Thus, increasing \(L\) will typically also increase \(_{n,L}^{*}\), because larger values of \(L\) will force us to pair additional observations \((x,x^{})\) which are farther apart under the distance metric.

The type one error bound (10) suggests that we can balance the trade off between validity and power when \(_{n,L}^{*}L 1\) or \(o(1)\), as the right hand side of (10) reduces to \(+o(1)\). Next we describe a representative generative setup where there is a natural choice of \(L\) that leads to \(_{n,L}^{*}L=o(1)\).

**Generative model.** Let \(=^{d}^{d}\). Let the conditional density of the human expert's forecasts \(Q(|x)\) be _smooth_. Specifically, for any \(x,x^{}^{d}\),

\[_{} X=x)}{Q( X=x^{ })} 1+C\|x-x^{}\|_{2},\] (12)

for some constant \(C>0\). Under this setup, Theorem 1 reduces to the following.

**Theorem 2** (Asymptotic Validity): _Given \((0,1)\) and under (12), with the appropriate choice of \(L 1\), the type I error of_ **ExpertTest** _satisfies_

\[_{K}+o(1).\] (13)

_as \(n,K\)._

Intuitively, (12) is intended to model a forecasting rule which is'simple,' in the sense that human experts don't finely distinguish between instances whose feature vectors are close under the \(_{2}\) norm. Importantly, this does not rule out the possibility that predictions for two specific \((x,x^{})\) instances could differ substantially - only that the distributions \( X=x\) and \( X=x^{}\) are similar when \(x x^{}\). We make no such assumption about \(_{Y X}\), the conditional distribution of the true outcomes.

Proofs of theorems 1 and 2 can be found in Appendices B and C respectively. We now illustrate the utility of our test with an empirical study of physician hospitalization decisions.

A case study: physician expertise in emergency room triage

Emergency room triage decisions present a natural real-world setting for our work, as we can assess whether physicians make hospitalization decisions by incorporating information which is not available to an algorithmic risk score. We consider the particular case of patients who present in the emergency room with acute gastrointestinal bleeding (hereafter referred to as AGIB), and assess whether physicians' decisions to either hospitalize or discharge each patient appear to be capturing information which is not available to the Glasgow-Blatchford Score (GBS). The GBS is a standardized measure of risk which is known to be a highly sensitive indicator for whether a patient presenting with AGIB will indeed require hospitalization (findings which we corroborate below). However, despite the excellent performance of this algorithmic risk score, we might be understandably hesitant to automate triage decisions without any physician oversight. As just one example, anticoagulant medications ('blood thinners') are known to exacerbate the risk of severe bleeding. However, whether or not a patient is taking anticoagulant medication is not included as a feature in the construction of the Glasgow-Blatchford score, and indeed may not even be recorded in the patient's electronic health record (if, for example, they are a member of an underserved population and have had limited prior contact with the healthcare system). This is one of many additional factors an emergency room physician might elicit directly from the patient to inform an admit/discharge decision. We thus seek to answer the following question:

_Do emergency room physicians usefully incorporate information which is not available to the Glasgow-Blatchford score?_

We answer in the affirmative, demonstrating that although the GBS provides risk scores which are highly competitive with (and indeed, arguably better than) physicians' discretionary decisions, there is strong evidence that physicians are incorporating additional information which is not captured in the construction of the GBS. Before presenting our results, we first provide additional background about this setting.

**Background: risk stratification and triage for gastrointestinal bleeding.** Acute gastrointestinal bleeding is a potentially serious condition for which 530,855 patients/year receive treatment in the United States alone (Peery et al. (2022)). It is estimated that \(32\%\) of patients with presumed bleeding from the lower gastrointestinal tract (Oakland et al. (2017)) and \(45\%\) of patients with presumed bleeding from the upper gastrointestinal tract (Stanley et al. (2017)) require urgent medical intervention; overall mortality rates for AGIB in the U.S. are estimated at around \(3\) per \(100,000\)(Peery et al. (2022)). For patients who present with AGIB in the emergency room, the attending physician is tasked with deciding whether the bleeding is severe enough to warrant admission to the hospital. However, the specific etiology of AGIB is often difficult to determine from patient presentation alone, and gold standard diagnostic techniques - an endoscopy for upper GI bleeding or a colonoscopy for lower GI bleeding - are both invasive and costly, particularly when performed urgently in a hospital setting. To aid emergency room physicians in making this determination more efficiently, the Glasgow-Blatchford Bleeding Score or GBS (Blatchford et al. (2000)) is a standard screening metric used to assess the risk that a patient with acute upper GI bleeding will require red blood cell transfusion, intervention to stop bleeding, or die within 30 days. It has been also validated in patients with acute lower gastrointestinal bleeding3 to assess need for intervention to stop bleeding or risk of death (Asad Ur-Rahman and Abusaada (2018)); accordingly, we interpret the GBS as a measure of risk for patients who present with either upper or lower GI bleeding in the emergency department.

**Construction of the Glasgow-Blatchford Score.** The Glasgow-Blatchford Score is a function of the following nine patient characteristics: blood urea nitrogen (BUN), hemoglobin (HGB), systolic blood pressure (SBP), pulse, cardiac failure, hepatic disease, melena, syncope and biological sex. The first four are real-valued and the latter five are binary. The GBS is calculated by first converting the continuous features to ordinal values (BUN and HGB to 6 point scales, SBP to a 3 point scale and pulse to a binary value) and then summing the values of the first 8 features. Biological sex is used to inform the conversion of HGB to an ordinal value. Scores are integers ranging from \(0\) to \(23\)with higher scores indicating a higher risk that a patient will require subsequent intervention. US and international guidelines suggest that patients with a score of \(0\) or \(1\) can be safely discharged from the emergency department (Laine et al. (2021); Barkun et al. (2019)), with further investigation to be performed outside the hospital. For additional details on the construction of the GBS, we refer to Blatchford et al. (2000).

**Defining features, predictions and outcomes.** We consider a sample of 3617 patients who presented with AGIB at one of three hospitals in a large academic health system between 2014 and 2018. Consistent with the goals of triage for patients with AGIB, we record an 'adverse outcome' if a patient (1) requires some form of urgent intervention to stop bleeding (endoscopic, interventional radiologic, or surgical; excluding patients who only undergo a diagnostic endoscopy or colonoscopy) while in the hospital (2) dies within 30 days of their emergency room visit or (3) is initially discharged but later readmitted within \(30\) days.4 As is typical of large urban hospitals in the United States, staffing protocols at this health system dictate a separation of responsibilities between emergency room physicians and other specialists. In particular, while emergency room physicians make an initial decision whether to hospitalize a patient, it is typically a gastrointestinal specialist who subsequently decides whether a patient admitted with AGIB requires some form of urgent hemostatic intervention. This feature, along with our ability to observe whether discharged patients are subsequently readmitted, substantially mitigates the selective labels issue that might otherwise occur in this setting. Thus, consistent with clinical and regulatory guidelines - to avoid hospitalizing patients who do not require urgent intervention (Stanley et al. (2009)), and to avoid discharging patients who are likely to be readmitted within 30 days (NEJM (2018)) - we interpret the emergency room physician's decision to admit or discharge a patient as a _prediction_ that one of these adverse outcomes will occur. We thus instantiate our model by letting \(X_{i}^{9}\) be the nine discrete patient characteristics from which the Glasgow-Blatchford Score is computed5; the only transformation we apply is to normalize each feature to lie in \(\). We further let \(_{i}\{0,1\}\) indicate whether that patient was initially hospitalized, and \(Y_{i}\{0,1\}\) indicate whether that patient suffered one of the adverse outcomes defined above.

**Assessing the accuracy of physician decisions.** We first summarize the performance of the emergency room physicians' hospitalization decisions, and compare them with the performance of a simple rule which would instead admit every patient with a GBS above a certain threshold and discharge the remainder (Table 1). We consider thresholds of \(0\) and \(1\) - the generally accepted range for low risk patients (Laine et al. (2021); Barkun et al. (2019)) - as well as less conservative thresholds of \(2\) and \(7\) (the latter of which we find maximizes overall accuracy). For additional context, we also provide the total fraction of patients admitted under each decision rule.

   Decision Rule & Fraction Hospitalized & Accuracy & Sensitivity & Specificity \\  Physician Discretion & 0.86 ± 0.02 & 0.55 ± 0.02 & 0.99 ± 0.00 & \(0.24 0.02\) \\ Admit GBS  0 & 0.88 ± 0.02 & 0.53 ± 0.02 & 0.99 ± 0.00 & \(0.19 0.02\) \\ Admit GBS  1 & 0.80 ± 0.02 & 0.60 ± 0.02 & 0.98 ± 0.00 & \(0.33 0.02\) \\ Admit GBS  2 & 0.73 ± 0.02 & 0.66 ± 0.02 & 0.97 ± 0.00 & \(0.43 0.02\) \\ Admit GBS  7 & 0.40 ± 0.02 & 0.79 ± 0.02 & 0.73 ± 0.02 & \(0.84 0.02\) \\   

Table 1: Comparing the accuracy of physician hospitalization decisions (‘Physician Discretion’) to those made by thresholding the GBS. For example, ‘Admit GBS \(>1\)’ hospitalizes patients with a GBS strictly larger than \(1\). ‘Fraction Hospitalized’ indicates the fraction of patients hospitalized by each rule. ‘Accuracy’ indicates the 0/1 accuracy of each rule, where a decision is correct if it hospitalized a patient who suffers an adverse outcome (as defined above) or discharges a patient who does not. ‘Sensitivity’ indicates the fraction of patients who suffer an adverse outcome that are correctly hospitalized, and ‘Specificity’ indicates the fraction of patients who do not suffer an adverse outcome that are correctly discharged. Results are reported to \( 2\) standard errors.

Unsurprisingly, we find that the physicians calibrate their decisions to maximize sensitivity (minimize false negatives) at the expense of admitting a significant fraction of patients who, in retrospect, could have been discharged immediately. Indeed we find that although \(86\%\) of patients are initially hospitalized, only \( 42\%\) actually require a specific hospital-based intervention or otherwise suffer an adverse outcome which would justify hospitalization (i.e., \(y_{i}=1\)). Consistent with Blatchford et al. (2000) and Chatten et al. (2018), we also find that thresholding the GBS in the range of \(\) achieves a sensitivity of close to \(100\%\). We further can see that using one of these thresholds may achieve overall accuracy (driven by improved specificity) which is substantially better than physician discretion. Nonetheless, we seek to test whether physicians demonstrate evidence of expertise in distinguishing patients with identical (or nearly identical) scores.

**Testing for physician expertise.** We now present the results of running **ExpertTest** for \(K=1000\) resampled datasets and various values of \(L\) (where \(L=1808\) is the largest possible choice given \(n=3617\)) in Table 2. We define the distance metric \(m(x_{1},x_{2})||x_{1}-x_{2}||_{2}\), though this choice is inconsequential when the number of'mismatched pairs' (those pairs \(x,x^{}\) where \(x x^{}\)) is \(0\).

We also observe that, in the special case of binary predictions and outcomes, it is possible to analytically determine the number of swaps which increase or decrease the value of nearly any natural loss function \(F()\). Thus, although we let \(F(D)_{i}1[y_{i}_{i}]\) for concreteness, our results are largely insensitive to this choice; in particular, they remain the same when false negatives and false positives might incur arbitrarily different costs. We elaborate on this phenomenon in Appendix E.

As the results demonstrate, there is _very strong evidence that emergency room physicians incorporate information which is not available to the Glasgow-Blatchford score_. In particular, our test indicates that physicians can reliably distinguish patients who appear identical with respect to the nine features considered by the GBS - and can make hospitalization decisions accordingly - even though simple GBS thresholding is highly competitive with physician performance. This implies that _no_ predictive algorithm trained on these nine features, even one which is substantially more complicated than the GBS, can fully capture the information that physicians use to make hospitalization decisions.

To interpret the value of \(\), observe that for \(L 500\) we recover the smallest possible value \(=1/(K+1)=1/1001\). Furthermore, for all but the final experiment, the number of mismatched pairs is \(0\), which means there is no additional type one error incurred (i.e., \(_{n,L}^{s}\) (11) is guaranteed to be \(0\)). For additional intuition on the behavior of **ExpertTest**, we refer the reader to the synthetic experiments in Appendix F.

## 6 Discussion and limitations

In this work we provide a simple test to detect whether a human forecaster is incorporating unobserved information into their predictions, and illustrate its utility in a case study of hospitalization decisions made by emergency room physicians. A key insight is to recognize that this requires more care than simply testing whether the forecaster _outperforms_ an algorithm trained on observable data; indeed, a large body of prior work suggests that this is rarely the case. Nonetheless, there are many settings in which we might expect that an expert is using information or intuition which is difficult to replicate with a predictive model.

   L & mismatched pairs & swaps that increase loss & swaps that decrease loss & \(\) \\ 
100 & 0 & 5 & 1 & 0.061 \\
250 & 0 & 12 & 1 & 0.003 \\
500 & 0 & 21 & 2 & 0.001 \\
1000 & 0 & 42 & 2 & 0.001 \\
1808 & 265 & 66 & 4 & 0.001 \\   

Table 2: The results of running ExpertTest, where each pair of patients is chosen to be as similar as possible with respect to the nine (discrete) inputs to the Glasgow-Blatchford score. \(L\) indicates the number of pairs selected for the test, of which ‘mismatched pairs’ are not identical to each other. Swaps that decrease (respectively, increase) loss indicates how many of the \(L\) pairs result in a decrease (respectively, increase) in the 0/1 loss when their corresponding hospitalization decisions are exchanged with each other. \(\) is the p-value obtained from running ExpertTest.

An important limitation of our approach is that we do not consider the possibility that expert forecasts might inform decisions which causally effect the outcome of interest, as is often the case in practice. We also do not address the possibility that the objective of interest is not merely accuracy, but perhaps some more sophisticated measure of utility (e.g., one which also values fairness or simplicity); this is explored in Rambachan (2022). We caution more generally that there are often normative reasons to prefer human decision makers, and our test captures merely one possible notion of expertise. The results of our test should thus not be taken as _recommending_ the automation of a given forecasting task.

Furthermore, while our framework is quite general, it is worth emphasizing that the specific algorithm we propose is only one possible test of \(H_{0}\). Our algorithm does not scale naturally to settings where \(\) is high-dimensional, and in such cases it is likely that a more sophisticated test of conditional independence (e.g. a kernel-based method; see Fukumizu et al. (2004), Gretton et al. (2007) and Zhang et al. (2011), among others) would have more power to reject \(H_{0}\). Another possible heuristic is to simply choose some learning algorithm to estimate (e.g.) \([Y X]\) and \([Y X,]\), and examine which of the two provides better out of sample performance. This can be viewed as a form of feature selection; indeed the 'knockoffs' approach of Candes et al. (2016) which inspires our work is often used as a feature selection procedure in machine learning pipelines. However, most learning algorithms do not provide p-values with the same natural interpretation we describe in section 3, and we thus view these approaches as complementary to our own.

Finally, our work draws a clean separation between the 'upstream' inferential goal of detecting whether a forecaster is incorporating unobserved information and the 'downstream' algorithmic task of designing tools which complement or otherwise incorporate human expertise. These problems share a very similar underlying structure however, and we conjecture that - as has been observed in other supervised learning settings, e.g. Kearns et al. (2018) - there is a tight connection between these auditing and learning problems. We leave an exploration of these questions for future work.