# R\({}^{2}\)-Gaussian: Rectifying Radiative Gaussian Splatting

for Tomographic Reconstruction

 Ruyi Zha\({}^{1}\)  Tao Jun Lin\({}^{1}\)  Yuanhao Cai\({}^{2,}\)1 Jiwen Cao\({}^{1}\)

**Yanhao Zhang\({}^{3}\)  Hongdong Li\({}^{1}\)**

\({}^{1}\)The Australian National University \({}^{2}\)Johns Hopkins University

\({}^{3}\)Robotics Institute, University of Technology Sydney

{ruyi.zha, taojun.lin, jiwen.cao, hongdong.li}@anu.edu.au

caiyuanhao1998@gmail.com  yanhao.zhang@uts.edu.au

###### Abstract

3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R\({}^{2}\)-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown _integration bias_ in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency. Crucially, it delivers high-quality results in 4 minutes, which is 12\(\) faster than NeRF-based methods and on par with traditional algorithms. Code and models are available on the project page https://github.com/Ruyi-Zha/r2_gaussian.

## 1 Introduction

Computed tomography (CT) is an essential imaging technique for noninvasively examining the internal structure of objects. Most CT systems use X-rays as the imaging source thanks to their ability to penetrate solid substances . During a CT scan, an X-ray machine captures multi-angle 2D projections that measure ray attenuation through the material. As the core of CT, tomographic reconstruction aims to recover the 3D density field of the object from its projections. This task is challenging in two aspects. Firstly, the harmful X-ray radiation limits the acquisition of sufficient and noise-free projections, making reconstruction a complex and ill-posed problem. Secondly, time-sensitive applications like medical diagnosis require algorithms to deliver results promptly.

Existing tomography methods suffer from either suboptimal reconstruction quality or slow processing speed. Traditional CT algorithms  deliver results in minutes but induce serious artifacts. Supervised learning-based approaches  achieve promising outcomes by learning semantic priors but struggle with out-of-distribution objects. Recently, neural radiance fields (NeRF)  have been applied to tomography and perform well in per-case reconstruction . However, they are very time-consuming (\(>30\) minutes) because a huge amount of points have to be sampled for volume rendering.

Recently, 3D Gaussian splatting (3DGS)  has outperformed NeRF in both quality and efficiency for view synthesis [64; 38; 31] and surface reconstruction [16; 18; 65]. However, attempts to apply the 3DGS technique to volumetric reconstruction tasks, such as X-ray tomography, are limited and ineffective. Some concurrent works [7; 14] empirically modify 3DGS for X-ray view synthesis, but they treat it solely as a data augmentation tool for traditional tomography algorithms. To date, there is no 3DGS-based method for direct CT reconstruction.

In this paper, we reveal an inherent **integration bias** in 3DGS. This bias, despite having a negligible impact on image rendering, critically hampers volumetric reconstruction. To be more specific, we will show in Sec. 4.2.1 that the standard 3DGS overlooks a covariance-related scaling factor when splatting a 3D Gaussian kernel onto the 2D image plane. This formulation leads to inconsistent volumetric properties queried from different views. Besides the integration bias, there are other challenges in applying 3DGS to tomography, such as the difference between natural light and X-ray imaging and the lack of an effective technique to query volumes from kernels.

We propose R\({}^{2}\)-Gaussian (Rectified Radiative Gaussians) to extend 3DGS to sparse-view tomographic reconstruction. R\({}^{2}\)-Gaussian achieves a bias-free training pipeline with three significant improvements. **Firstly**, we introduce a novel radiative Gaussian kernel, which acts as a local density field parameterized by central density, position, and covariance. We initialize Gaussian parameters using the analytical method FDK  and optimize them with photometric losses. **Secondly**, we rectify the 3DGS rasterizer to support X-ray imaging. This is achieved by deriving new X-ray rendering functions and correcting the integration bias for accurate density retrieval. **Thirdly**, we develop a CUDA-based differentiable voxelizer, which not only extracts 3D volumes from Gaussians but also enables voxel-based regularization during training. We evaluate R\({}^{2}\)-Gaussian on both synthetic and real-world datasets. Extensive experiments demonstrate that our method surpasses state-of-the-art (SOTA) methods within 4 minutes, which is \(12\) faster than the most efficient NeRF-based solution, NAF  and comparable to traditional algorithms. It converges to optimal results in 15 minutes, improving PSNR by 0.6 dB compared to SOTA methods. A visual comparison is shown in Fig. 1.

Our contributions can be summarized as follows: (1) We discover a previously unknown integration bias in 3DGS that impedes volumetric reconstruction. (2) We propose the first 3DGS-based tomography framework by introducing new kernels, extending rasterization to X-ray imaging, and developing a differentiable voxelizer. (3) Our method significantly outperforms state-of-the-art methods in both reconstruction quality and training speed, highlighting its practical value.

## 2 Related work

Tomographic reconstructionComputed tomography (CT) is widely used for non-intrusive inspection in medicine [17; 22], biology [12; 39; 24], and industry . Conventional fan-beam CT produces a 3D volume by reconstructing each slice from 1D projection arrays. Recently, the cone-beam scanner has become popular for its fast scanning and high resolution , leading to the demand for 3D tomography, i.e., recovering the volume directly from 2D projection images. Our work focuses on 3D sparse-view reconstruction where less than a hundred projections are captured to reduce radiation exposure. Traditional algorithms are mainly grouped into analytical and iterative methods. Analytical

Figure 1: We compare our method to state-of-the-art NeRF-based methods (IntraTomo , NAF , SAX-NeRF ) in terms of visual quality, PSNR (dB), and training time (minute). Our method achieves the highest reconstruction quality and is significantly faster than other methods.

methods like filtered back projection (FBP) and its 3D variant FDK  produce results instantly (\(<1\) second) by solving the Radon transform and its inverse . However, they introduce serious streak artifacts in sparse-view scenarios. Iterative methods [2; 55; 40; 51] formulate tomography as a maximum-a-posteriori problem and iteratively minimize the energy function with regularizations. They successfully suppress artifacts but take longer time (\(<10\) minutes) and lose structure details. Deep learning methods can be categorized as supervised and self-supervised families. Supervised methods learn semantic priors from CT datasets. They then use the trained networks to inpaint projections [3; 15], denoise volumes [10; 28; 35; 37] or directly output results [19; 63; 1; 32; 33]. Supervised learning methods perform well in cases similar to training sets but suffer from poor generation ability when applied to unseen data. To overcome this limitation, some studies [67; 66; 48; 6; 54] handle tomography in a self-supervised learning fashion. Inspired by NeRF , they model the density field with coordinate-based networks and optimize them with photometric losses. Although NeRF-based methods excel in per-case reconstruction, they are time-consuming (\(>\)30 minutes) due to the extensive point sampling in volume rendering. Our work can be put into the self-supervised learning family, but it greatly accelerates the training process and improves reconstruction quality.

3Dgs3D Gaussian splatting  outperforms NeRF in speed by leveraging highly parallelized rasterization for image rendering. 3DGS represents objects with a set of trainable Gaussian-shaped primitives. It has achieved great success in RGB tasks, including surface reconstruction [16; 18; 65], dynamic scene modeling [60; 34; 61], human avatar [36; 30; 27], 3D generation [57; 62; 9], etc. Some concurrent works have extended 3DGS to X-ray imaging. X-Gaussian  modify 3DGS to synthesize novel-view X-ray projections. Gao et al.  improve X-Gaussian by considering complex noise-inducing physical effects. While they produce plausible 2D X-ray projections, they cannot directly extract 3D density volumes from trained Gaussians. Instead, they first augment projections with 3DGS, and then use traditional algorithms such as FDK for CT reconstruction, which is neither efficient nor effective. Li et al.  represent the density field with customized Gaussian kernels, but they replace the efficient rasterization with existing CT simulators. In comparison, our work can both rasterize X-ray projections and voxelize density volumes from Gaussians.

## 3 Preliminary

### X-ray imaging

A projection \(^{H W}\) measures ray attenuation through the material as shown in Fig. 2. For an X-ray \((t)=+t^{3}\) with initial intensity \(I_{0}\) and path bounds \(t_{n}\) and \(t_{f}\), the corresponding raw pixel value \(I^{}()\) is given with the Beer-Lambert Law  by: \(I^{}()=I_{0}(-_{t_{n}}^{t_{f}}((t))\, dt)\). Here, \(()\) is the isotropic density (or attenuation coefficient in physics) at position \(^{3}\). Tomography typically transforms raw data to the logarithmic space for computational simplicity, i.e.,

\[I()= I_{0}- I^{}()=_{t_{n}}^{t_{f}} ((t))dt,\] (1)

where each pixel value \(I()\) represents the density integral along the ray path. Except otherwise specified, we use the logarithmic projections as inputs. The goal of tomographic reconstruction is to estimate the 3D distribution of \(()\), output as a discrete volume, with X-ray projections \(\{_{i}\}_{i=1,,N}\) captured from \(N\) different angles. Note that real-world projections contain minor anisotropic physical effects such as Compton scattering. Following previous works [13; 2; 55; 67], we do not explicitly model them but treat them as noise during the reconstruction.

### 3D Gaussian splatting

3D Gaussian splatting  models the scene with a set of 3D Gaussian kernels \(^{3}=\{G_{i}^{3}\}_{i=1,,M}\), each parameterized by position, covariance, color, and opacity. A rasterizer \(\) renders an RGB image

Figure 2: A detection plane captures the attenuation of X-rays emitted from different angles.

\(_{rgb}^{H W 3}\) from these Gaussians, formulated as

\[_{rgb}=(^{3})= (^{3}),\] (2)

where \(\), \(\), and \(\) are the transformation, projection, and composition modules, respectively. First, \(\) transforms the 3D Gaussians into the ray space, aligning viewing rays with the coordinate axis to enhance computational efficiency. The transformed 3D Gaussians are then projected onto the image plane: \(^{2}=(^{3})\). The projected 2D Gaussian retains the same opacity and color as its 3D counterpart but omits the third row and column of position and covariance. An RGB image is then rendered by compositing these 2D Gaussians using alpha-blending : \(_{rgb}=(^{2})\). The rasterizer is differentiable, allowing for the optimization of kernel parameters using photometric losses. 3DGS initializes sparse Gaussians with structure-from-motion (SfM) points . During training, an adaptive control strategy dynamically densifies Gaussians to improve scene representation.

## 4 Method

In this section, we first introduce radiative Gaussian as a novel object representation in Sec. 4.1. Next, we adapt 3DGS to tomography in Sec. 4.2. Specifically, we derive new rasterization functions and analyze the integration bias of standard 3DGS in Sec. 4.2.1. We further develop a differentiable voxelizer for volume retrieval in Sec. 4.2.2. The optimization strategy is elaborated in Sec. 4.2.3.

### Representing objects with radiative Gaussians

As shown in Fig. 3, we represent the target object with a group of learnable 3D kernels \(^{3}=\{G_{i}^{3}\}_{i=1,,M}\) that we term as radiative Gaussians. Each kernel \(G_{i}^{3}\) defines a local Gaussian-shaped density field, i.e.,

\[G_{i}^{3}(|_{i},_{i},_{i})=_{i }(-(-_{i})^{}_{i}^{-1}(-_{i})),\] (3)

where \(_{i}\), \(_{i}^{3}\) and \(_{i}^{3 3}\) are learnable parameters representing central density, position and covariance, respectively. For optimization purposes, we follow  to further decompose the covariance matrix \(_{i}\) into the rotation matrix \(_{i}\) and scale matrix \(_{i}\): \(_{i}=_{i}_{i}_{i}^{} _{i}^{}\). The overall density at position \(^{3}\) is then computed by summing the density contribution of kernels:

\[()=_{i=1}^{M}G_{i}^{3}(|_{i},_{i},_{i}).\] (4)

Compared with standard 3DGS, our kernel formulation removes view-dependent color because X-ray attenuation depends only on isotropic density, as shown in Eq. (1). More importantly, we define the density query function (Eq. (4)) for radiative Gaussians, making them useful for both 2D image rendering and 3D volume reconstruction. In contrast, the opacity in 3DGS is empirically designed for RGB rendering, leading to challenges when extracting 3D models such as meshes from Gaussians [16; 8; 65]. Concurrent work  also explores kernel-based representation but uses simplified isotropic Gaussians. Our work employs a general Gaussian distribution, offering more flexibility and precision in modeling complex structures.

Figure 3: We represent the scanned object as a set of radiative Gaussians. We optimize them using real X-ray projections and finally retrieve the density volume with voxelization.

Initialization3DGS initializes Gaussians with SfM points, which is not applicable to volumetric tomography. Instead, we initialize our radiative Gaussians using preliminary results obtained from the analytical method. Specifically, we use FDK  to reconstruct a low-quality volume in less than 1 second. We then exclude empty spaces with a density threshold \(\) and randomly sample \(M\) points as kernel positions. Following , we set the scales of Gaussians as the nearest neighbor distances and assume no rotation. The central densities are queried from the FDK volume. We empirically scale down the queried densities with \(k\) to compensate for the overlay between kernels.

### Training radiative Gaussians

Our training pipeline is shown in Fig. 4. Radiative Gaussians are first initialized from an FDK volume. We then rasterize projections for photometric losses and voxelize tiny density volumes for 3D regularization. Adaptive control is used to densify Gaussians for better representation. After training, we voxelize density volumes of the target size for evaluation.

#### 4.2.1 X-ray rasterization

This section focuses on the theoretical derivation of X-ray rasterization \(\). As discussed in Sec. 3.1, the pixel value of a projection is the integral of density along the corresponding ray path. We substitute Eq. (4) into Eq. (1), yielding

\[I_{r}()=_{i=1}^{M}G_{i}^{3}((t)|_{i},_{i},_{i})dt=_{i=1}^{M} G_{i}^{3}((t)|_{i}, _{i},_{i})dt,\] (5)

where \(I_{r}()\) is the rendered pixel value. This implies that we can individually integrate each 3D Gaussian to rasterize an X-ray projection. Note that \(t_{n}\) and \(t_{f}\) in Eq. (1) are neglected because we assume all Gaussians are bounded inside the target space.

TransformationSince a cone-beam X-ray scanner can be modeled similarly to a pinhole camera, we follow  to transfer Gaussians from the world space to the ray space. In ray space, the

Figure 4: Training pipeline of R\({}^{2}\)-Gaussian. (a) Overall training pipeline. (b) X-ray rasterization for projection rendering. (c) Density voxelization for volume retrieval. (d) Modified adaptive control.

viewing rays are parallel to the third coordinate axis, facilitating analytical integration. Due to the non-Cartesian nature of ray space, we employ the local affine transformation to Eq. (5), yielding

\[I_{r}()_{i=1}^{M} G_{i}^{3}(}|_{i },)}_{}_{i}},_ {i}_{i}^{}_{i}^{}}_{ }_{i}})dx_{2},\] (6)

where \(}=[x_{0},x_{1},x_{2}]^{}\) is a point in ray space, \(}_{i}^{3}\) is the new Gaussian position obtained through projective mapping \(\), and \(}_{i}^{3 3}\) is the new Gaussian covariance controlled by local approximation matrix \(_{i}\) and viewing transformation matrix \(\). Refer to Appendix A for determining \(\), \(_{i}\), and \(\) from scanner parameters.

Projection and compositionA good property of _normalized_ 3D Gaussian distribution is that its integral along one coordinate axis yields a normalized 2D Gaussian distribution. Substitute Eq. (3) into Eq. (6) and we have

\[I_{r}() _{i=1}^{M}_{i}(2)^{}|}_{i}|^{}}|}_{i}|^{}}(-(}-}_{i})^{}}_{i}^{-1}( }-}_{i}))}_{}dx_{2}\] \[=_{i=1}^{M}_{i}(2)^{}|}_{i}|^{}}_ {i}|^{}}(-(}-} _{i})^{}}_{i}^{-1}(}-}_{i}))}_{}\] (7) \[=_{i=1}^{M}G_{i}^{2}(}|}_{i}|}{|}_{i}|}}_{i}}_{ _{i}},}_{i},}_{i}),\]

where \(}^{2}\), \(}^{2}\), \(}^{2 2}\) are obtained by dropping the third rows and columns of their counterparts \(}\), \(}\), and \(}\), respectively. Eq. (7) shows that an X-ray projection can be rendered by simply summing 2D Gaussians instead of alpha-compositing them in natural light imaging.

Integration biasDuring the projection, a key difference between our 2D Gaussian and the original one in 3DGS is the central density (opacity) \(_{i}\). As shown in Eq. (7), we scale the density with a covariance-related factor \(_{i}=(2|}_{i}|/|}_{i}|)^{1/2}\): \(_{i}=_{i}_{i}\), while 3DGS does not. This implies that 3DGS, in fact, learns an integrated density in the 2D image plane rather than the actual one in 3D space. This integration bias, though having a negligible impact on imaging rendering, leads to significant inconsistency in density retrieval. We demonstrate the inconsistency with a simplified 2D-to-1D projection in Fig. 5. When attempting to recover the central density \(\) in 3D space with \(_{i}=_{i}/_{j}\), we find different views (\(_{j}\)) lead to different results. This violates the isotropic nature of \(_{i}\), preventing us from determining the correct value. In contrast, our method assigns the actual 3D density to the kernel and forwardly computes the 2D projection, thus fundamentally solving the issue. While conceptually simple, implementing our idea requires substantial engineering efforts, including reprogramming all backpropagation routines in CUDA.

#### 4.2.2 Density voxelization

We develop a voxelizer \(\) to efficiently query a density volume \(^{X Y Z}\) from radiative Gaussians: \(=(^{3})\). Inspired by voxelizers used in RGB tasks , our voxelizer first partitions the target space into multiple \(8 8 8\) 3D tiles. It then culls Gaussians, retaining those with a \(99\%\) confidence of intersecting the tile. In each 3D tile, voxel values are parallelly computed by summing the contributions of nearby kernels with Eq. (4). We implement the voxelizer and its backpropagation in CUDA, making it differentiable for optimization. This design not only accelerates the query process (\(>100\) FPS) but also allows us to regularize radiative Gaussians with 3D priors.

Figure 5: Density inconsistency in 3DGS.

#### 4.2.3 Optimization

We optimize radiative Gaussians using stochastic gradient descent. Besides photometric L1 loss \(_{1}\) and D-SSIM loss \(_{ssim}\), we further incorporate a 3D total variation (TV) regularization \(_{tv}\) as a homogeneity prior for tomography. At each training iteration, we randomly query a tiny density volume \(_{tv}^{D D D}\) (same spacing as the target output) and minimize its total variation. The overall training loss is defined as:

\[_{total}=_{1}(_{r},_{m})+_{ ssim}_{ssim}(_{r},_{m})+_{tv}_{tv}( _{tv}),\] (8)

where \(_{r}\), \(_{m}\), \(_{ssim}\) and \(_{tv}\) are rendered projection, measured projection, D-SSIM weight, and TV weight, respectively. Adaptive control is employed during training to enhance object representation. We remove empty Gaussians and densify (clone or split) those with large loss gradients. Considering objects such as human organs have extensive homogeneous areas, we do not prune large Gaussians. As for densification, we halve the densities of both original and replicated Gaussians. This strategy mitigates the sudden performance drop caused by new Gaussians and hence stabilizes training.

## 5 Experiments

### Experimental settings

DatasetWe conduct experiments on both synthetic and real-world datasets. For the synthetic dataset, we collect 15 real CT volumes, ranging from organisms to artificial objects. We then use the tomography toolbox TIGRE  to synthesize X-ray projections and add Compton scatter and electric noise. For real-world experiments, we use three cases from the FIPS dataset , each with 721 real projections. Since ground truth volumes are unavailable, we use FDK  to create pseudo-ground truth using all views and then subsample views for sparse-view experiments. We set 75, 50, and 25 views for both synthetic and real-world data as three sparse-view scenarios. Refer to Appendix B for more details of datasets.

Implementation detailsOur R\({}^{2}\)-Gaussian is implemented in PyTorch  and CUDA , and trained with the Adam optimizer  for 30k iterations. Learning rates for position, density, scale, and rotation are initially set as 0.0002, 0.01, 0.005, and 0.001, respectively, and exponentially to 0.1 of their initial values. Loss weights are \(_{ssim}=0.25\) and \(_{tv}=0.05\). We initialize \(M=50\)k Gaussians with a density threshold \(=0.05\) and scaling term \(k=0.15\). The TV volume size is

    &  &  &  \\   & PSNR\(\) & SSIM\(\) & Time\(\) & PSNR\(\) & SSIM\(\) & Time\(\) & PSNR\(\) & SSIM\(\) & Time\(\) \\   \\  FDK  & 28.63 & 0.497 & - & 26.50 & 0.422 & - & 22.99 & 0.317 & - \\ SART  & 36.06 & 0.897 & 4m41s & 34.37 & 0.875 & 3m36s & 31.14 & 0.825 & 1m47s \\ ASD-POCS  & 36.64 & 0.940 & **2m25s** & 34.34 & 0.914 & **1m52s** & 30.48 & 0.847 & **56s** \\ IntraTomo  & 35.42 & 0.924 & 2h7m & 35.25 & 0.923 & 2h9m & 34.68 & 0.914 & 2h19m \\ NAF  & 37.84 & 0.945 & 30m43s & 36.65 & 0.932 & 32m4s & 33.91 & 0.893 & 31m1s \\ SAX-NeRF  & 38.07 & 0.950 & 1h3m & 36.86 & 0.938 & 1h5m & 34.33 & 0.905 & 13h3m \\ Ours (iter=10k) & 38.29 & 0.954 & **2m38s** & 37.63 & 0.949 & **2m35s** & 35.08 & 0.922 & 2m35s \\ Ours (iter=30k) & 38.88 & **0.959** & 8m21s & **37.98** & **0.952** & 8m14s & **35.19** & **0.923** & 8m28s \\   \\  FDK  & 30.03 & 0.535 & - & 27.38 & 0.449 & - & 23.30 & 0.335 & - \\ SART  & 34.42 & 0.845 & 5m11s & 33.61 & 0.827 & **3m28s** & 31.52 & 0.790 & **1m47s** \\ ASD-POCS  & 36.33 & 0.868 & **2m43s** & 34.58 & 0.861 & **1m49s** & 31.32 & 0.810 & **56s** \\ IntraTomo  & 36.79 & 0.858 & 2h25m & 36.99 & 0.854 & 2h19m & **35.85** & **0.835** & 2h18m \\ NAF  & 38.58 & 0.848 & 51m28s & 36.44 & 0.818 & 51m31s & 32.92 & 0.772 & 51m24s \\ SAX-NeRF  & 34.93 & 0.854 & 13h21m & 34.89 & 0.840 & 13h23m & 33.49 & 0.793 & 13h25m \\ Ours (iter=10k) & 38.10 & 0.872 & 3m39s & 37.52 & 0.866 & 3m37s & 35.10 & 0.840 & 3m23s \\ Ours (iter=30k) & 39.40 & **0.875** & 14m16s & 38.24 & 0.864 & 13m52s & 34.83 & 0.833 & 12m56s \\   

Table 1: Quantitative results on sparse-view tomography. We colorize the **best**, **second-best**, and **third-best** numbers.

\(D=32\). Adaptive control runs from 500 to 15k iterations with a gradient threshold of 0.00005. All methods run on a single RTX3090 GPU. We evaluate reconstruction quality using PSNR and SSIM , with PSNR calculated in 3D volume and SSIM averaged over 2D slices in axial, coronal, and sagittal directions. We also report the running time as a reflection of efficiency.

### Results and evaluation

For fairness, we do not compare methods that require external training data but focus on those that solely use 2D projections of arbitrary objects. We compare R\({}^{2}\)-Gaussian with three traditional methods (FDK , SART , ASD-POCS ) and three SOTA NeRF-based methods (IntraTomo , NAF , SAX-NeRF ). Tab. 1 reports the quantitative results on sparse-view tomography. Note that we do not report the running time for FDK as it is instant. R\({}^{2}\)-Gaussian achieves the best performance across all synthetic and most real-world experiments. Specifically, our method delivers a 0.93 dB higher PSNR than SAX-NeRF, on the synthetic dataset, and a 0.95 dB improvement over IntraTomo on the real-world dataset. It is also worth noting that our 50-view results are already on par with the 75-view results of other methods. Regarding efficiency, our method converges to optimal results in 15 minutes, which is 3.7\(\) faster than the most efficient NeRF-based method, NAF. Surprisingly, it takes less than 4 minutes to surpass other methods, which is even faster than the traditional algorithm SART. Fig. 6 shows the visual comparisons of different methods. FDK and SART introduce streak artifacts, while ASD-POCS and IntraTomo blur structural details. NAF and SAX-NeRF are better than other baseline methods but have salt-and-pepper noise. In comparison, our method successfully recovers sharp details, e.g., ovules of pepper, and maintains good smoothness for homogeneous areas, e.g., muscles in the chest.

Figure 6: Colorized slice examples of different methods with PSNR (dB) shown at the bottom right of each image. The first three rows are from the synthetic dataset and the last row is from the real-world dataset. Our method recovers more details and suppresses artifacts.

### Ablation study

Integration biasTo demonstrate the impact of integration bias discussed in Sec. 4.2.1, we develop an X-ray version of 3DGS (X-3DGS) that uses X-ray rendering while retaining the biased 3D-to-2D Gaussian projection. We use the same voxelizer in Sec. 4.2.2 to extract volumes. Before voxelization, we divide the learned density of each Gaussian by the mean scaling factor \(\) of all training views. Tab. 2 shows that rectifying integration bias benefits both 2D rendering (+3.15 dB PSNR) and 3D reconstruction (+17.77 dB PSNR). Fig. 7 visualize rendering and reconstruction results. While X-3DGS renders reasonable 2D projections, its reconstruction quality is significantly worse than ours. Besides, there are notable discrepancies in slices queried from different views. The conflicting 2D and 3D performances indicate that X-3DGS, despite fitting images well, does not accurately model the density field. In contrast, our method learns the actual view-independent density, eliminating inconsistencies and ensuring unbiased object representation.

Component analysisWe conduct ablation experiments to assess the effect of FDK initialization (Init.), modified adaptive control (AC), and total variation regularization (Reg.) on performance. The baseline model excludes these components and uses randomly generated Gaussians for initialization. Experiments are performed under the 50-view condition, evaluating PSNR, SSIM, training time, and Gaussian count (Gau.). Results are listed in Tab. 3. FDK initialization boosts PSNR by \(0.9\) dB. Adaptive control improves quality but prolongs training due to more Gaussians. TV regularization increases SSIM by reducing artifacts and promoting smoothness. Overall, our full model outperforms the baseline, improving PSNR by 1.51 dB and SSIM by 0.018, with training time under 9 minutes.

Parameter analysisWe perform parameter analysis on the number of initialized Gaussians \(M\), TV loss weight \(_{tv}\), and TV volume size \(D\). The results are shown in the last three blocks of Tab. 3. R\({}^{2}\)-Gaussian achieves good quality-efficiency balance at 50k initialized Gaussians. A TV loss weight of \(_{tv}=0.05\) improves reconstruction, but larger values can lead to degradation. The training time increases with TV volume size while the performance peaks at \(D=32\).

Convergence analysisFig. 8 compares the results of NeRF-based methods and our R\({}^{2}\)-Gaussian method at different iterations. Our method, both with and without FDK initialization, converges significantly faster, displaying sharp details by the 500th iteration when other methods still exhibit artifacts and blurriness. Notably, the FDK initialization offers a rough structure before training, which

    &  &  &  \\   & X-3DGS & Ours & X-3DGS & Ours & X-3DGS & Ours \\ 
2D PSNR\(\) & 49.97 & 50.54 & 47.26 & 49.70 & 39.84 & 46.28 \\
2D SSIM\(\) & 0.987 & 0.986 & 0.984 & 0.986 & 0.967 & 0.982 \\
3D PSNR\(\) & 23.40 & 38.86 & 21.24 & 37.98 & 14.07 & 35.17 \\
3D SSIM\(\) & 0.660 & 0.959 & 0.562 & 0.952 & 0.408 & 0.923 \\   

Table 2: Quantitative results of X-3DGS and our method on the synthetic dataset.

Figure 7: Results of X-3DGS and our method with PSNR (dB) indicated on each image. We show slices of X-3DGS queried from three viewing angles. Although X-3DGS can produce plausible X-ray projections, its reconstructed volume lacks view consistency and exhibits poor quality.

further accelerates convergence and enhances reconstruction quality. Finally, our method outperforms others in both performance and efficiency, achieving the highest PSNR of 38.90 dB in 9 minutes.

## 6 Discussion and conclusion

DiscussionR\({}^{2}\)-Gaussian inherits some limitations from 3DGS, such as varying training time across modalities, needle-like artifacts under extremely sparse-view conditions, and suboptimal extrapolation for other tomography tasks. Besides, we have not considered calibration errors regarding the scanned geometry and anisotropic physical effects such as Compton scattering. More details are discussed in Appendix E. Despite these limitations, our method's superior performance and fast speed make it valuable for real-world applications for medical diagnosis and industrial inspection.

ConclusionThis paper presents R\({}^{2}\)-Gaussian, a novel 3DGS-based framework for sparse-view tomographic reconstruction. We identify and rectify a previously overlooked integration bias of standard 3DGS, which hinders accurate density retrieval. Furthermore, we enhance 3DGS for tomography by introducing new kernels, devising X-ray rasterization functions, and developing a differentiable voxelizer. Our R\({}^{2}\)-Gaussian surpasses state-of-the-art methods in both reconstruction quality and training speed, demonstrating its potential for real-world applications. Crucially, we speculate that the newly found integration bias may be pervasive across all 3DGS-related research. Consequently, our rectification technique could benefit more tasks beyond computed tomography.