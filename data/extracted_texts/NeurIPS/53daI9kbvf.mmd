# Jiachen Li\({}^{1}\), Weixi Feng\({}^{1}\), Tsu-Jui Fu\({}^{1}\), Xinyi Wang\({}^{1}\), Sugato Basu\({}^{2}\),

T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback**Wenhu Chen\({}^{3}\), William Yang Wang\({}^{1}\)**

\({}^{1}\)UC Santa Barbara, \({}^{2}\)Google, \({}^{3}\)University of Waterloo

\({}^{1}\){jiachen_li, weixifeng, tsu-juifu, xinyi_wang, william}@cs.ucsb.edu

\({}^{2}\)sugato@google.com \({}^{3}\)venhuchen@uwaterloo.ca

Project Page: https://t2v-turbo.github.io

###### Abstract

Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes. To address the challenge, consistency models have been proposed to facilitate fast inference, albeit at the cost of sample quality. In this work, we aim to break the quality bottleneck of a video consistency model (VCM) to achieve **both fast and high-quality video generation**. We introduce T2V-Turbo, which integrates feedback from a mixture of differentiable reward models into the consistency distillation (CD) process of a pre-trained T2V model. Notably, we directly optimize rewards associated with single-step generations that arise naturally from computing the CD loss, effectively bypassing the memory constraints imposed by backpropagating gradients through an iterative sampling process. Remarkably, the 4-step generations from our T2V-Turbo achieve the highest total score on VBench , even surpassing Gen-2 [Esser et al., 2023] and

Figure 1: By integrating reward feedback during consistency distillation from VideoCrafter2 [Chen et al., 2024], our T2V-Turbo (VC2) can generate high-quality videos with 4-8 inference steps, breaking the quality bottleneck of a VCM . Appendix F includes the corresponding text prompts.

Pika [Pika Labs, 2023]. We further conduct human evaluations to corroborate the results, validating that the 4-step generations from our T2V-Turbo are preferred over the 50-step DDIM samples from their teacher models, representing more than a tenfold acceleration while improving video generation quality.

## 1 Introduction

Diffusion model (DM) [Sohl-Dickstein et al., 2015, Ho et al., 2020] has emerged as a powerful framework for neural image [Betker et al., 2023, Rombach et al., 2022, Esser et al., 2024, Saharia et al., 2022] and video synthesis [Singer et al., 2022, Ho et al., 2022a, He et al., 2022, Wang et al., 2023b, Zhang et al., 2023], leading to the development of cutting-edge text-to-video (T2V) models like Sora [Brooks et al., 2024], Gen-2 [Esser et al., 2023] and Pika [Pika Labs, 2023]. Although the iterative sampling process of these diffusion-based models ensures high-quality generation, it significantly slows down inference, hindering their real-time applications. On the other hand, existing open-sourced T2V models including VideoCrafter [Chen et al., 2023, 2024] and ModelScopeT2V [Wang et al., 2023c] are trained on web-scale video datasets, e.g., WebVid-10M [Bain et al., 2021], with varying video qualities. Consequently, the generated videos often appear visually unappealing and fail to align accurately with the text prompts, deviating from human preferences.

Efforts have been made to address the issues listed above. To accelerate the inference process, Wang et al. [2023a] applies the theory of consistency distillation (CD) [Song et al., 2023, Song and Dhariwal, 2023, Luo et al., 2023a] to distill a video consistency model (VCM) from a teacher T2V model, enabling plausible video generations in just 4-8 inference steps. However, the quality of VCM's generations is naturally bottleneck by the performance of the teacher model, and the reduced number of inference steps further diminishes its generation quality. On the other hand, to align generated videos with human preferences, InstructVideo [Yuan et al., 2023] draws inspiration from image generation techniques [Dong et al., 2023, Clark et al., 2023, Prabhudesai et al., 2023] and proposes backpropagating the gradients of a differentiable reward model (RM) through the iterative video sampling process. However, calculating the full reward gradient is prohibitively expensive, resulting in substantial memory costs. Consequently, InstructVideo truncates the sampling chain by limiting gradient calculation to only the final DDIM step, compromising optimization accuracy. Additionally, InstructVideo is limited by its reliance on an image-text RM, which fails to fully capture the transition dynamic of a video. Empirically, InstructVideo only conducts experiments on a limited set of user prompts, the majority of which are related to animals. As a result, its generalizability to a broader range of prompts remains unknown.

In this paper, we aim to achieve fast and high-quality video generation by breaking the quality bottleneck of a VCM. We introduce T2V-Turbo, which integrates reward feedback from a mixture of RMs into the process of distilling a VCM from a teacher T2V model. Besides utilizing an image-text RM to align individual video frames with human preference, we further incorporate reward feedback

Figure 2: Overview of the training pipeline of our T2V-Turbo. We integrate reward feedback from both an image-text RM and a video-text RM into the VCD procedures by backpropagating gradient through the single-step generation process of our T2V-Turbo.

from a video-text RM to comprehensively evaluate the temporal dynamics and transitions in the generated videos. We highlight that our reward optimization avoids tackling the highly memory-intensive issues associated with backpropagating gradients through an iterative sampling process. Instead, we directly optimize rewards of the single-step generations that arise from computing the CD loss, effectively bypassing the memory constraints faced by conventional methods that optimize a DM (Yuan et al., 2023; Xu et al., 2024; Clark et al., 2023; Prabhudesai et al., 2023).

Empirically, we demonstrate the superiority of our T2V-Turbo in generating high-quality videos within 4-8 inference steps. To illustrate the applicability of our methods, we distill T2V-Turbo (VC2) and T2V-Turbo (MS) from VideoCrafter2 (Chen et al., 2024) and ModelScopeT2V (Wang et al., 2023c), respectively. Remarkably, the 4-step generation results from both variants of our T2V-Turbo outperform SOTA models on the video evaluation benchmark VBench (Huang et al., 2024), even surpassing proprietary systems such as Gen-2 (Esser et al., 2023) and Pika (Pika Labs, 2023) that are trained with extensive resources. We further corroborate the results by conducting human evaluation using 700 prompts from the EvalCrafter (Liu et al., 2023) benchmark, validating that the 4-step generations from T2V-Turbo are favored by human over the 50-step DDIM samples from their teacher T2V models, which represents over tenfold inference acceleration and enhanced video generation quality.

Our contributions are threefold:

* Learn a T2V model with feedback from a mixture of RMs, including a video-text model. To the best of our knowledge, we are the first to do so.
* Establish a new SOTA on the VBench with only 4 inference steps, outperforming proprietary models trained with substantial resources.
* 4-step generations from our T2V-Turbo are favored over the 50-step generation from its teacher T2V model as evidenced by human evaluation, representing over 10 times inference acceleration with quality improvement.

## 2 Preliminaries

**Diffusion models (DMs)**. In the forward process, DMs progressively inject Gaussian noise into the original data distribution \(p_{}() p_{0}(_{0})\) and perturb it into a marginal distribution \(p_{t}(_{t})\) with the transition kernel \(p_{0t}(_{t}|_{0})=(_{t}|(t) _{0},^{2}(t))\) at timestep \(t\). \((t)\) and \((t)\) correspond to the noise schedule. In the reverse process, DMs sequentially recover the data from a noise sampled from the prior distribution \(p_{T}(_{T})(_{T}|,^{2 }(T))\). The reverse-time SDE can be modeled by an ordinary differential equation (ODE), known as the Probability Flow (PF-ODE) (Song et al., 2020):

\[_{t}=[(t)_{t}- (t)^{2} p_{t}(_{t})] t,_{T}(,^{2}(T) ).\] (1)

where \(()\) and \(()\) are the drift and diffusion coefficients, respectively, with the following properties:

\[(t)=(t)}{t},^{2 }(t)=^{2}(t)}{t}-2(t) }{t}^{2}(t).\] (2)

The PF-ODE's solution trajectories, when sampled at any timestep \(t\), align with the distribution \(p_{t}(_{t})\). Empirically, a denoising model \(_{}(_{t},t)\) is trained to approximate the score function \(- p_{t}(_{t})\) via score matching. During the sampling phase, one begins with a sample \(_{T} p_{T}(_{T})\) and follows the empirical PF-ODE below to obtain a sample \(}_{0}\).

\[_{t}=[(t)_{t}+ (t)^{2}_{}(_{t},t) ]t,_{T}(,^{2}(T) ).\] (3)

In this paper, we focus on diffusion-based T2V models, which operate on the video latent space \(\) and train a denoising model \(_{}(_{t},,t)\) conditioned on the text prompt \(\), where \(_{t}\) is obtained by perturbing the image latent \(=(),\) and \(\) is a VAE (Kingma and Welling, 2013) encoder. The T2V models employ Classifier-Free Guidance (CFG) (Ho and Salimans, 2021) to enhance the quality of conditional sampling by substituting the noise prediction with a linear combination of conditional and unconditional noise predictions for denoising, i.e., \(}_{}(_{t},,,t)=(1+ )_{}(_{t},,t)- _{}(_{t},,t)\), where \(\) is the CFG scale. After the completion of the inference process, we can generate a video by \(}_{0}=(_{0})\) with the VAE decoder \(\) corresponding to \(\).

**Consistency Distillation**. Conventional methods [Ho et al., 2020, Song et al., 2020b] generate their samples by solving the PF-ODE sequentially, leading to DM's slow inference speed. To tackle this problem, _consistency models_ (CM) [Song et al., 2023, Song and Dhariwal, 2023] propose to learn a consistency function \(:(_{t},t)_{}\) to directly map any \(_{t}\) on the PF-ODE trajectory to its origin, where \(\) is a fixed small positive number. And thus, the consistency function \(\) has the following _self-consistency_ property

\[(_{t},t)=(_{t}^{},t^{}), t,t^{}[,T],\] (4)

where \(_{t}\) and \(_{t}^{}\) are from the same PF-ODE. We can model \(\) with a CM \(_{}\). When tackling the PF-ODE of a T2V model that operates on the video latent space \(\), we aim to learn a video consistency model (VCM) [Luo et al., 2023a, Wang et al., 2023a]\(_{}:(_{t},,,t)_{ 0}\). To ensure \(_{}(,,,t)=\), we parameterize \(_{}\) as

\[_{}(,,,t)=c_{}(t)+ c_{}(t)F_{}(,,,t),\] (5)

where \(c_{}(t)\) and \(c_{}(t)\) are differentiable functions with \(c_{}()=1\) and \(c_{}()=0\), and \(F_{}\) is modeled as a neural network. We can distill a \(_{}\) from a pre-trained T2V DM by minimizing the _consistency distillation_ (CD) [Song et al., 2023, Luo et al., 2023a] loss as below

\[L_{}(,^{-};)=_{, ,,n}[d(_{}(_{t_{n+k}}, ,,t_{n+k}),_{^{-}}(}_{ t_{n}}^{,},,,t_{n}))],\] (6)

where \(d(,)\) is a distance function. \(^{-}\) is updated by the exponential moving average (EMA) of \(\), i.e., \(^{-}(+(1-)^{-})\). \(}_{t_{n}}^{,}\) is an estimate of \(_{t_{n}}\) obtained by the numerical augmented PF-ODE solver \(\) parameterized by \(\) and \(k\) is the skipping interval

\[}_{t_{n}}^{,}_{t_{n+k}}+(1+ )(_{t_{n+k}},t_{n+k},t_{n},;)-(_{t_{n+k}},t_{n+k},t_{n},;).\] (7)

We follow the LCM paper [Luo et al., 2023a] to use DDIM [Song et al., 2020b] as the ODE solver \(\) and defer the formula of the DDIM solver to Appendix A.

## 3 Training T2V-Turbo with Mixed Reward Feedback

In this section, we present the training pipeline to derive our T2V-Turbo. To facilitate fast and high-quality video generation, we integrate reward feedback from multiple RMs into the LCD process when distilling from a teacher T2V model. Figure 2 provides an overview of our framework. Notably, we directly leverage the single-step generation \(}_{0}=_{}(_{t_{n+k}},, ,t_{n+k})\) arise from computing the CD loss \(L_{}\) (6) and optimize the video \(}_{0}=(}_{0})\) decoded from it towards multiple differentiable RMs. As a result, we avoid the challenges associated with backpropagating gradients through an iterative sampling process, which is often confronted by conventional methods optimizing DMs [Clark et al., 2023, Xu et al., 2024a, Yuan et al., 2023].

In particular, we leverage reward feedback from an image-text RM to improve human preference on each individual video frame (Sec. 3.1) and further utilize the feedback from a video-text RM to improve the temporal dynamics and transitions in the generated video (Sec. 3.2).

### Optimizing Human Preference on Individual Video Frames

Chen et al.  achieve high-quality video generation by including high-quality images as single-frame videos when training the T2V model. Inspired by their success, we align each individual video frame with human preference by optimizing towards a differentiable image-text RM \(_{}\). In particular, we randomly sample a batch of \(M\) frames \(\{}_{0}^{1},,}_{0}^{M}\}\) from the decoded video \(}_{0}\) and maximize their scores evaluated by \(_{}\) as below

\[J_{}()=_{}_{0},}[_{ m=1}^{M}_{}(}_{0}^{m}, )],}_{0}=(_{} (_{t_{n+k}},,,t_{n+k})).\] (8)

### Optimizing Video-Text Feedback Model

Existing image-text RMs [Wu et al., 2023a, Xu et al., 2024a, Kirstain et al., 2024] are limited to assessing the alignment between individual video frames and the text prompt and thus cannot evaluatethrough the temporal dimensions that involve inter-frame dependencies, such as motion dynamic and transitions (Huang et al., 2024; Liu et al., 2023). To address these shortcomings, we further leverage a video-text RM \(_{}\) to assess the generated videos. The corresponding objective \(J_{}\) is given below

\[J_{}()=_{}_{0},}[ _{}(}_{0},)], }_{0}=(_{}(_{t _{n+k}},,,t_{n+k})).\] (9)

### Summary

To this end, we can define the total learning loss \(L\) of our training pipeline as a linear combination of the \(L_{}\) in (6), \(J_{}\) in (8), and \(J_{}\) in (9) with weighting parameters \(_{}\) and \(_{}\).

\[L(,^{-};)=L_{}(,^{-}; )-_{}J_{}()-_{}J_{ }()\] (10)

To reduce memory and computational cost, we initialize our T2V-Turbo with the teacher model and only optimize the LoRA weights (Hu et al., 2021; Luo et al., 2023b) instead of performing full model training. After completing the training, we merge the LoRA weights so that the per-step inference cost of our T2V-Turbo remains identical to the teacher model. We include pseudo-codes for our training algorithm in Appendix B.

## 4 Experimental Results

Our experiments aim to demonstrate our T2V-Turbo's ability to generate high-quality videos with 4-8 inference steps. We first conduct automatic evaluations on the standard benchmark VBench (Huang et al., 2024) to comprehensively evaluate our methods from various dimensions (Sec. 4.1) against a broad array of baseline methods. We then perform human evaluations with 700 prompts from the EvalCrafter (Liu et al., 2023) to compare the 4-step and 8-step generations from our T2V-Turbo with the 50-step generations from the teacher T2V models as well as the 4-step generations from the baseline VCM (Sec. 4.2). Finally, we perform ablation studies on critical design choices (Sec. 4.3).

**Settings**. We train T2V-Turbo (VC2) and T2V-Turbo (MS) by distilling from the teacher diffusion-based T2V models VideoCrafter2 (Chen et al., 2024) and ModelScopeT2V (Wang et al., 2023c), respectively. Similar to both teacher models, we conduct our training using the WebVid10M (Bain et al., 2021) datasets. We train our models on 8 NVIDIA A100 GPUs for 10K gradient steps without gradient accumulation. We set the batch size of training videos to 1 for each GPU device. We employ HPSv2.1 (Wu et al., 2023a) as our image-text RM \(_{}\). When distilling from VideoCrafter2, we utilize the 2nd Stage model of InternVideo2 (InternVid2 S2) (Wang et al., 2024) as our video-text RM \(_{}\). When distilling from ModelScopeT2V, we set \(_{}\) to be ViCLIP (Wang et al., 2023d). To optimize \(J_{}\) (8), we randomly sample 6 frames from the video by setting \(M=6\). For the hyperparameters (HP), we set learning rate \(1e-5\) and guidance scale range \([_{},_{}]=\). We use DDIM (Song et al., 2020b) as our ODE solver \(\) and set the skipping step \(k=20\). For T2V-Turbo (VC2), we set \(_{}=1\) and \(_{}=2\). For T2V-Turbo (MS), we set \(_{}=2\) and \(_{}=3\). We include further training details in Appendix A.

### Automatic Evaluation on VBench

We evaluate our T2V-Turbo (VC2) and T2V-Turbo (MS) on the standard video evaluation benchmark VBench (Huang et al., 2024) to compare against a wide array of baseline methods. VBench is designed to comprehensively evaluate T2V models from 16 disentangled dimensions. Each dimension in VBench is tailored with specific prompts and evaluation methods.

Table 1 compares the 4-step generation of our methods with various baselines from the VBench leaderboard1, including Gen-2 (Esser et al., 2023), Pika (Pika Labs, 2023), VideoCrafter2 (Chen et al., 2023), VideoCrafter2 (Chen et al., 2024), Show-1 (Zhang et al., 2023), LaVie (Wang et al., 2023b), and ModelScopeT2V (Wang et al., 2023c). Table 4 in Appendix further compares our methods with VideoCrafter0.9 (He et al., 2022), LaVie-Interpolation (Wang et al., 2023b), Open-Sora (Open-Sora, 2024), and CoeVideo (Hong et al., 2022). The performance of each baseline method is directly reported from the VBench leaderboard. To obtain the results of our methods, we carefully follow VBench's evaluation protocols by generating 5 videos for each prompt to calculate the metrics. We

further train VCM (VC2) and VCM (MS) by distilling from VideoCrafter2 and ModelScopeT2V, respectively, without incorporating reward feedback, and then compare their results.

VBench has developed its own rules to calculate the **Total Score**, **Quality Score**, and **Semantic Score**. **Quality Score** is calculated with the 7 dimensions from the top table. **Semantic Score** is calculated with the 9 dimensions from the bottom table. And **Total Score** is a weighted sum of Quality Score and Semantic Score. Appendix C provides further details, including explanations for each dimension of VBench. As shown in Table 1, the 4-step generations of both our T2V-Turbo (MS) and T2V-Turbo (VC2) surpass all baseline methods on VBench in terms of Total Score. These results are particularly remarkable given that we even outperform the proprietary systems Gen-2 and Pika, which are trained with extensive resources. Even when distilling from a less advanced teacher model, ModelScopeT2V, our T2V-Turbo (MS) attains the second-highest Total Score, just below our T2V-Turbo (VC2). Additionally, our T2V-Turbo breaks the quality bottleneck of a VCM by outperforming its teacher T2V model, significantly improving over the baseline VCM.

### Human Evaluation with 700 EvalCrafter Prompts

To verify the effectiveness of our T2V-Turbo, we compare the 4-step and 8-step generations from our T2V-Turbo with the 50-step DDIM samples from the corresponding teacher T2V models. We further compare the 4-step generations between our T2V-Turbo and their baseline VCMs when distilled from the same teacher T2V model. We leverage the 700 prompts from the EvalCrafter [Liu et al., 2023] video evaluation benchmark, which are constructed based on real-world user data.

    &  &  &  &  &  &  &  &  &  \\  & **Score** & **Score** &  &  &  &  &  &  &  \\  ModelScopeT2V & 75.75 & 78.05 & 89.87 & 95.29 & 98.28 & 95.79 & 52.06 & **66.39** & 58.57 \\ LaVie & 77.08 & 78.78 & 91.41 & 97.47 & 98.30 & 96.38 & 54.94 & 49.72 & 61.90 \\ Show-1 & 78.93 & 80.42 & 95.53 & 98.02 & 99.12 & 98.24 & 57.35 & 44.44 & 58.66 \\ VideoCrafter1 & 79.72 & 81.59 & 95.10 & 98.04 & 98.93 & 95.67 & 62.67 & 55.00 & 65.46 \\ Pika & 80.40 & **82.68** & 96.76 & **98.95** & **99.77** & 99.51 & 63.15 & 37.22 & 62.33 \\ VideoCrafter2 & 80.44 & 82.20 & 96.85 & 98.22 & 98.41 & 97.73 & 63.13 & 42.50 & 67.22 \\ Gen-2 & 80.58 & 82.47 & **97.61** & 97.61 & 99.56 & **99.58** & **66.96** & 18.89 & 67.42 \\  VCM (MS) & 75.84 & 78.80 & 93.06 & 97.30 & 98.51 & 98.00 & 48.99 & 46.11 & 61.98 \\ Our T2V-Turbo (MS) & 80.62 & 82.15 & 94.82 & 98.71 & 97.99 & 95.64 & 60.04 & **66.39** & 68.09 \\  VCM (VC2) & 73.97 & 78.54 & 94.02 & 96.05 & 99.06 & 98.84 & 54.56 & 42.50 & 52.72 \\ Our T2V-Turbo (VC2) & **81.01** & 82.57 & 96.28 & 97.02 & 97.48 & 97.34 & 63.04 & 49.17 & **72.49** \\    &  &  &  &  &  &  &  &  &  &  \\  & **Score** &  &  &  &  &  &  &  &  \\  ModelScopeT2V & 66.54 & 82.25 & 38.98 & 92.40 & 81.72 & 33.68 & 39.26 & 23.39 & 25.37 & 25.67 \\ LaVie & 70.31 & 91.82 & 33.32 & **96.80** & 86.39 & 34.09 & 52.69 & 23.56 & 25.93 & 26.41 \\ Show-1 & 72.98 & 93.07 & 45.47 & 95.60 & 86.35 & 53.50 & 47.03 & 23.06 & 25.28 & 27.46 \\ VideoCrafter1 & 72.22 & 78.18 & 45.66 & 91.60 & **93.32** & 58.86 & 43.75 & 24.41 & 25.54 & 26.76 \\ Pika & 71.26 & 87.45 & 46.69 & 88.00 & 85.31 & 65.65 & 44.80 & 21.89 & 24.44 & 25.47 \\ VideoCrafter2 & 73.42 & 92.55 & 40.66 & 95.00 & 92.92 & 35.86 & 55.29 & 25.13 & 25.84 & **28.23** \\ Gen-2 & 73.03 & 90.92 & 55.47 & 89.20 & 89.49 & **66.91** & 48.91 & 19.34 & 24.12 & 26.17 \\  VCM (MS) & 63.98 & 83.18 & 24.85 & 87.20 & 85.72 & 31.57 & 42.44 & 23.20 & 23.30 & 24.18 \\ Our T2V-Turbo (MS) & 74.47 & 93.34 & **58.63** & 95.80 & 89.67 & 45.74 & 48.47 & 23.23 & 25.92 & 27.51 \\  VCM (VC2) & 55.66 & 63.97 & 10.81 & 82.60 & 79.12 & 23.06 & 18.49 & **25.29** & 22.31 & 25.15 \\ Our T2V-Turbo (VC2) & **74.76** & **93.96** & 54.65 & 95.20 & 89.90 & 38.67 & **55.58** & 24.42 & 25.51 & 28.16 \\   

Table 1: **Automatic Evaluation on VBench**[Huang et al., 2024]. We compare our T2V-Turbo (VC2) and T2V-Turbo (MS) with baseline methods across the 16 VBench dimensions. A higher score indicates better performance for a particular dimension. We bold the best results for each dimension and underline the second-best result. **Quality Score** is calculated with the 7 dimensions from the top table. **Semantic Score** is calculated with the 9 dimensions from the bottom table. **Total Score** a weighted sum of **Quality Score** and **Semantic Score**. Further details can be found in Appendix C. Both our T2V-Turbo (VC2) and T2V-Turbo (MS) ** surpass all baseline methods with 4 inference steps** in terms of Total Score, including the proprietary systems Gen-2 and Pika.

We hire human annotators from Amazon Mechanical Turk to compare videos generated from different models given the same prompt. For each comparison, the annotators need to answer three questions: Q1) Which video is more visually appealing? Q2) Which video better fits the text description? Q3) Which video do you prefer given the prompt? Appendix D includes additional details about how we set up the human evaluations.

Figure 3 provides the full human evaluation results. We also qualitatively compare different methods in Figure 4. Due to limited space, we include additional qualitative comparison results in Appendix F. Notably, the 4-step generations from our T2V-Turbo are favored by humans over the 50-step generation from their teacher T2V model, representing a 12.5 times inference acceleration with improving performance. By increasing the inference steps to 8, we can further improve the visual quality and text-video alignment of videos generated from our T2V-Turbo, reflected by the fact that our 8-step generations are more likely to be favored by the human compared to our 4-step generations in terms of all 3 evaluated metrics. Additionally, our T2V-Turbo significantly outperforms its baseline VCM, demonstrating the effectiveness of our methods, which incorporate a mixture of reward feedback into the model training.

Figure 4: Qualitative comparisons between the 4-step VCM, 50-step teacher T2V, 4-step T2V-Turbo and 8-step T2V-Turbo generations. **Left**: (VC2), **Right**: (MS).

Figure 3: Human evaluation results with the 700 prompts from EvalCrafter [Liu et al., 2023]. We compare the 4-step and 8-step generations from our T2V-Turbo with their teacher T2V model and their baseline VCM. **Top**: results for T2V-Turbo (VC2). **Bottom**: results for T2V-Turbo (MS).

### Ablation Studies

We are interested in the effectiveness of each RM, and especially in the impact of the video-text RM \(_{}\). Therefore, we ablate \(_{}\) and \(_{}\) and experiment with different choices of \(_{}\). In Appendix E, we further experiment with different choices of \(_{}\).

**Ablating RMs \(_{}\) and \(_{}\).** Recall that the training of our T2V-Turbo incorporate reward feedback from both \(_{}\) and \(_{}\). To demonstrate the effectiveness of each individual RM, we perform ablation study by training VCM (VC2) + \(_{}\) and VCM (VC2) + \(_{}\), which only incorporate feedback from \(_{}\) and \(_{}\), respectively. Again, we evaluate the 4-step generations from different methods on VBench. Results in Table 2 show that incorporating feedback from either \(_{}\) or \(_{}\) leads to performance improvement over the baseline VCM. Notably, optimizing \(_{}\) alone can already lead to substantial performance gains, while incorporating feedback from \(_{}\) can further improve the Semantic Score on VBench, leading to better text-video alignment. In Appendix H, we qualitatively compare the videos generated by our T2V-Turbo and VCM + \(_{}\), corroborating the effectiveness of our mixture of RMs design.

**Effect of different choices of \(_{}\).** We investigate the impact of different choices of \(_{}\) by training T2V-Turbo (VC2) and T2V-Turbo (MS) by setting \(_{}\) as ViCLIP  and the second stage model of Intervideo2 (InternVid2 S2). In terms of model architecture, ViCLIP employs the CLIP  text encoder while InternVid2 S2 leverages the BERT-large  text encoder. Additionally, InternVid2 S2 outperforms ViCLIP in several zero-shot video-text retrieval tasks. As shown in Table 3, T2V-Turbo (VC2) can achieve decent performance on VBench when integrating feedback from either ViCLIP or InternVid2 S2. Conversely, T2V-Turbo (MS) performs better with ViCLIP . Nevertheless, with InternVid2 S2, our T2V-Turbo (MS) still surpasses VCM (MS) + \(_{}\).

    &  &  &  &  &  &  &  &  &  \\  & **Score** & **Score** &  &  &  &  &  &  &  \\  VCM (MS) & 75.84 & 78.80 & 93.06 & 97.30 & **98.51** & **98.00** & 48.99 & 46.11 & 61.98 \\ VCM (MS) + \(_{}\) & 77.28 & 78.76 & 93.24 & 97.67 & 89.49 & 97.27 & 51.70 & 75.00 & 56.40 \\ VCM (MS) + \(_{}\) & 79.51 & 81.81 & **97.64** & **99.59** & **98.46** & 95.83 & **64.69** & 38.33 & **68.66** \\ Our T2V-Turbo (MS) & **80.62** & **82.15** & 94.82 & 98.71 & 97.99 & 95.64 & 60.04 & **66.39** & 68.09 \\  VCM (VC2) & 73.97 & 78.54 & 94.02 & 96.05 & **99.06** & **98.84** & 54.56 & 42.50 & 52.72 \\ VCM (VC2) + \(_{}\) & 77.57 & 80.08 & 95.46 & 96.69 & 98.78 & 98.79 & 58.66 & 25.00 & 65.75 \\ VCM (VC2) + \(_{}\) & 80.42 & **82.59** & **96.52** & **97.31** & 97.50 & 97.29 & **63.08** & 47.50 & **72.91** \\ Our T2V-Turbo (VC2) & **81.01** & 82.57 & 96.28 & 97.02 & 97.48 & 97.34 & 63.04 & **49.17** & 72.49 \\    &  &  &  &  &  &  &  &  &  &  \\  & **Score** &  &  &  &  &  &  &  &  \\  VCM (MS) & 63.98 & 83.18 & 24.85 & 87.20 & 85.72 & 31.57 & 42.44 & 23.20 & 23.30 & 24.18 \\ VCM (MS) + \(_{}\) & 71.35 & 91.14 & 45.64 & 94.60 & 86.97 & 39.74 & **48.55** & 22.90 & 25.91 & 26.81 \\ VCM (MS) + \(_{}\) & 70.32 & 91.30 & 56.10 & 94.80 & 76.45 & **46.04** & **47.56** & 21.30 & 23.47 & 25.98 \\ Our T2V-Turbo (MS) & **74.47** & **93.34** & **58.63** & **95.80** & **89.67** & 45.74 & 48.47 & **23.23** & **25.92** & **27.51** \\  VCM (VC2) & 55.66 & 63.97 & 10.81 & 82.60 & 79.12 & 23.06 & 18.49 & 25.29 & 22.31 & 25.15 \\ VCM (VC2) + \(_{}\) & 67.55 & 87.77 & 30.38 & 93.00 & 86.90 & 28.81 & 39.07 & **25.75** & 24.65 & 27.57 \\ VCM (VC2) + \(_{}\) & 71.70 & 93.13 & 46.20 & 95.00 & 84.12 & 37.78 & 51.34 & 23.65 & 24.62 & 27.75 \\ Our T2V-Turbo (VC2) & **74.76** & **93.96** & **54.65** & **95.20** & **89.90** & **38.67** & **55.58** & 24.42 & **25.51** & **28.16** \\   

Table 2: Ablation studies on the effectiveness of \(_{}\) and \(_{}\). We bold the highest score for each dimension for methods with the same teacher model. While incorporating feedback from \(_{}\) is effective at improving both Quality Score and Semantic Score, integrating reward feedback from \(_{}\) can further improve the semantic score.

    & T2V-Turbo (VC2) & T2V-Turbo (VC2) & T2V-Turbo (MS) & T2V-Turbo (MS) \\  & \(_{}\) = ViCLIP & \(_{}\) = InternVid S2 & \(_{}\) = ViCLIP & \(_{}\) = InternVid S2 \\  Total Score & 80.92 & **

## 5 Related Work

**Diffusion-based T2V Models**. Many diffusion-based T2V models rely on large-scale image datasets for training [Ho et al., 2022a, Wang et al., 2023c, Chen et al., 2023] or inherit weights from pre-trained text-to-image (T2I) models [Zhang et al., 2023, Blattmann et al., 2023, Khachatryan et al., 2023]. The scale of text-image datasets [Schuhmann et al., 2022] is usually more than ten times the scale of open-sourced video-text datasets [Bain et al., 2021, Wang et al., 2023d] and with higher spatial resolution and diversity [Wang et al., 2023c]. For example, Imagen Video [Ho et al., 2022b] discovers that joint training on a mix of image and video datasets improves the overall visual quality and enables the generation of videos in novel styles. Models trained with WebVid-10M [Bain et al., 2021] like ModelScopeT2V [Wang et al., 2023c] or VideoCrafter [Chen et al., 2023] also treat images as a single-frame video, and use them to improve video qualities. LaVie [Wang et al., 2023b] initialize the training with WebVid-10M and LAION-5B and then continue the training with a curated internal dataset of 23M videos. To overcome the data scarcity of high-quality videos, VideoCrafter2 [Chen et al., 2024] proposes to disentangle motion from appearance at the data level so that it can be trained on high-quality images and low-quality videos. The data limitation of high-quality videos and aligned, accurate video captions has been a longstanding bottleneck of current T2V models. In this paper, we propose to combat this challenge by leveraging reward feedback from a mixture of RMs.

**Accelerating inference of Diffusion Models**. Various methods have been proposed to accelerate the sampling process of a DM, including advanced numerical ODE solvers [Song et al., 2020b, Lu et al., 2022a,b, Zheng et al., 2022, Dockhorn et al., 2022, Jolicoeur-Martineau et al., 2021] and distillation techniques [Luhman and Luhman, 2021, Salimans and Ho, 2021, Meng et al., 2023, Zheng et al., 2023]. Recently, Consistency Model [Song et al., 2023, Luo et al., 2023a] is proposed to facilitate fast inference by learning a consistency function to map any point at the ODE trajectory to the origin. Li et al.  proposes to augment consistency distillation with an objective to optimize image-text RM to achieve fast and high-quality image generation. Our work extends it for T2V generation, incorporating reward feedback from both an image-text RM and a video-text RM.

**Vision-and-language Reward Models**. There have been various open-sourced image-text RMs that are trained to mirror human preferences given a text-image pair, including HPS [Wu et al., 2023b,a], ImageReward [Xu et al., 2024a], and PickScore [Kirstain et al., 2024], which are obtained by finetuning a image-text foundation model such as CLIP [Radford et al., 2021] and BLIP [Li et al., 2022], on human preference data. However, to the best of our knowledge, no video-text RMs, e.g., T2VScore [Wu et al., 2024], that mirrors human preference on a text-video pair has been released to the public. In this paper, we choose HPSv2.1 as our image-text RM and directly employ the video foundation models ViCLIP [Wang et al., 2023d] and InterVid S2 [Wang et al., 2024] that are trained for general video-text understanding as our video-text RM. Empirically, we show that incorporating feedback from these RMs can improve the performance of our T2V-Turbo.

**Learning from Human/AI Feedback** has been proven as an effective way to align the output from a generative model with human preference [Leike et al., 2018, Ziegler et al., 2019, Ouyang et al., 2022, Stiennon et al., 2020, Rafailov et al., 2024, Xu et al., 2024b]. In the field of image generation, various methods have been proposed to align a text-to-image model with human preference, including RL [Sutton and Barto, 2018, Li et al., 2020, 2023a] based methods [Fan et al., 2024, Prabhudesai et al., 2023, Zhang et al., 2024] and backpropagation-based reward finetuning methods [Clark et al., 2023, Xu et al., 2024a, Prabhudesai et al., 2023]. Recently, InstructVideo [Yuan et al., 2023] extends the reward-finetuning methods to optimize a T2V model. However, it still employs an image-text RM to provide reward feedback without considering the transition dynamic of the generated video. In contrast, our work incorporates reward feedback from both an image-text and video-text RM, providing comprehensive feedback to our T2V-Turbo.

## 6 Conclusion and Limitations

In this paper, we propose T2V-Turbo, achieving both fast and high-quality T2V generation by breaking the quality bottleneck of a VCM. Specifically, we integrate mixed reward feedback into the VCD process of a teacher T2V model. Empirically, we illustrate the applicability of our methods by distilling T2V-Turbo (VC2) and T2V-Turbo (MS) from VideoCrafter2 [Chen et al., 2024] and ModelScopeT2V [Wang et al., 2023c], respectively. Remarkably, the 4-step generations from both our T2V-Turbo outperform SOTA methods on VBench [Huang et al., 2024], even surpassing theirteacher T2V models and proprietary systems including Gen-2 [Esser et al., 2023] and Pika [Pika Labs, 2023]. Our human evaluation further corroborates the results, showing the 4-step generations from our T2V-Turbo are favored by humans over the 50-step DDIM samples from their teacher, which represents over ten-fold inference acceleration with quality improvement.

While our T2V-Turbo marks a critical advancement in efficient T2V synthesis, it is important to recognize certain limitations. Our approach utilizes a mixture of RMs, including a video-text RM \(_{}\). Due to the lack of an open-sourced video-text RM trained to reflect human preferences on video-text pairs, we instead use video foundation models such as ViCLIP [Wang et al., 2023d] and InternVid S2 [Wang et al., 2024] as our \(_{}\). Although incorporating feedback from these models has enhanced our T2V-Turbo's performance, future research should explore the use of a more advanced \(_{}\) for training feedback, which could lead to further performance improvements.