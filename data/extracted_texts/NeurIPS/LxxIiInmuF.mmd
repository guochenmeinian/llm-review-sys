# Paths to Equilibrium in Games

Bora Yongacoglu

University of Toronto

bora.yongacoglu@utoronto.ca

&Gurdal Arslan

University of Hawaii at Manoa

gurdal@hawaii.edu

&Lacra Pavel

University of Toronto

pavel@control.toronto.edu

&Serdar Yuksel

Queen's University

yuksel@queensu.ca

###### Abstract

In multi-agent reinforcement learning (MARL) and game theory, agents repeatedly interact and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in one period does not switch its strategy in the next period. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for normal-form games. Our analysis reveals a counterintuitive insight that reward deteriorating strategic updates are key to driving play to equilibrium along a satisficing path.

## 1 Introduction

Game theory is a mathematical framework for studying strategic interaction between self-interested agents, called players. In an \(n\)-player normal-form game, each player \(i=1,,n\), selects a strategy \(x^{i}^{i}\) and receives a reward \(R^{i}(x^{1},,x^{n})\), which depends on the collective _strategy profile_\(=(x^{1},,x^{n})=:(x^{i},^{-i})\). Player \(i\)'s optimization problem is to _best respond_ to the strategy \(^{-i}\) of its counterparts, choosing \(x^{i}^{i}\) to maximize \(R^{i}(x^{i},^{-i})\). Game theoretic models are pervasive in machine learning, appearing in fields such as multi-agent systems , multi-objective reinforcement learning , and adversarial model training , among many others.

In multi-agent reinforcement learning (MARL), players use learning algorithms to revise their strategies in response to the observed history of play, producing a sequence \(\{}_{t}\}_{t 1}\) in the set of strategy profiles \(:=^{1}^{n}\). Due to the coupled reward structure of multi-agent systems, each player's learning problem involves a moving target: since an individual's reward function depends on the strategies of the others, strategy revision by one agent prompts other agents to revise their own strategies. Convergence analysis of MARL algorithms can therefore be difficult, and the development of tools for such analysis is an important aspect of multi-agent learning theory.

A strategy profile \((x^{i}_{*})_{i=1}^{n}\) is called a _Nash equilibrium_ if all players simultaneously best respond to one another. Nash equilibrium is a concept of central importance in game theory, and the tasks of computing, approximating, and learning Nash equilibrium have attracted enduring attention in theoretical machine learning . Convergence to equilibrium strategies has long been a predominant, but not unique, design goal in MARL . In this paper, we studymathematical structure of normal-form games with the twin objectives of _(i)_ better understanding the capabilities or limitations of existing MARL algorithms and _(ii)_ producing insights for the design of new MARL algorithms.

A number of MARL algorithms approximate dynamical systems \(\{_{t}\}_{t 1}\) on the set of strategy profiles \(\) in which the next strategy for player \(i\) is selected as \(x_{t+1}^{i}=f^{i}(_{t})\), where \(_{t}=(x_{t}^{1},,x_{t}^{n})\) is the strategy profile in period \(t\). A sampling of such algorithms will be offered shortly. This approach facilitates analysis of the algorithm, as one separately considers the convergence of \(\{_{t}\}_{t 1}\) induced by the update functions \(\{f^{i}\}_{i=1}^{n}\), on one hand, and the approximation of \(\{_{t}\}_{t 1}\) by the algorithm's iterates \(\{}_{t}\}_{t 1}\) on the other. In this work, we consider update functions that satisfy a quasi-rationality condition called _satisficing:_ when an agent is best responding, the update rule instructs the agent to continue using this strategy. That is, if \(x^{i}\) is a best response to \(^{-i}\), then \(f^{i}(x^{i},^{-i})=x^{i}\). This quasi-rationality constraint generalizes the best response update and is desirable for stability of the resulting dynamics, as it guarantees that Nash equilibria are invariant under the dynamics. Moreover, the satisficing condition is only quasi-rational, in that it imposes no constraint on strategy updates when an agent is not best responding, and so allows for exploratory strategy updates. Update rules that incorporate exploratory random search when a strategy is deemed unsatisfactory are common in MARL theory .

Our goal is to better understand the capabilities/limitations of MARL algorithms that use the satisficing principle to select successive strategies, potentially augmented with random exploration when an agent is not best responding. Examples include  and . Instead of studying a particular collection of strategy update functions, we abstract the problem to the level of sequences in \(\), which allows us to implicitly account for experimental strategy updates. A sequence \((_{t})_{t 1}\) of strategy profiles is called a _satisficing path_ if, for each player \(i\) and time \(t\), one has that \(x_{t+1}^{i}=x_{t}^{i}\) whenever \(x_{t}^{i}\) is a best response to \(_{t}^{-i}\). The central research question of this paper is such:

_For a normal-form game \(\) and an initial strategy profile \(_{1}\), is it always possible to construct a satisficing path from \(_{1}\) to a Nash equilibrium of the game \(\)?_

Since many MARL algorithms operate using the satisficing principle (or otherwise approximate processes that involve satisficing update rules, e.g. ), the resolution of this question has implications for the effectiveness of such MARL algorithms. Indeed, the question has been answered in the affirmative for two-player normal-form games by  and for \(n\)-player symmetric Markov games by , and in both classes of games this has directly lead to MARL algorithms with convergence guarantees for approximating equilibria. In addition to removing a theoretical obstacle, positive resolution of this question would establish that _uncoordinated, distributed_ random search can effectively assist Nash-seeking algorithms to achieve last-iterate convergence guarantees in a more general class of games than previously possible.

**Contributions.** We give a positive answer to the question above: for any finite \(n\)-player game \(\) and any initial strategy profile \(_{1}\), there exists a satisficing path beginning at \(_{1}\) and ending at a Nash equilibrium of \(\). This partially answers an open question posed by . We prove this result by analytically constructing a satisficing path from an arbitrary initial strategy profile to a Nash equilibrium. Our approach is somewhat counterintuitive, in that it does not attempt to seek Nash equilibrium by improving the performance of unsatisfied players who are not best responding at a given strategy profile), but by updating strategies in a way that _increases_ the number of unsatisfied players at each round. This tactic leverages the freedom afforded to unsatisfied players to explore their strategy space and avoids the challenge of cyclical strategy revision that occurs when agents attempt to best respond to their counterparts . This insight provides a new approach to MARL algorithm design beyond the well-structured settings considered in prior work.

**Notation.** We let \(_{A}\) denote the set of probability measures over a set \(A\). For \(n\), we let \([n]:=\{1,2,,n\}\). For a point \(x\), the Dirac measure centered at \(x\) is denoted \(_{x}\). When discussing a fixed agent \(i\), the remaining collection of agents are called \(i\)'s counterparts or counterplayers.

Related Work.A vast number of MARL algorithms have been proposed for iterative strategy adjustment while playing a game. The most widely studied class of algorithms of this type involve each player running a no-regret algorithm on its own stream of rewards. The celebrated fictitious play algorithm  and its descendants are special cases of this class. Although the convergence behavior of fictitious play and its variants has been studied extensively, convergence results are typically available only for games exhibiting special structural properties amenable to analysis [25; 29; 4; 45; 46]. Indeed, the convergence properties of fictitious play are intimately connected to those of _best response dynamics_, a full information dynamical system evolving in continuous time where the evolution rule for player \(i\)'s strategy is governed by its best response multi-function. By harnessing such connections, convergence results for fictitious play and a number of other MARL algorithms have been obtained by analyzing the dynamical systems induced by specific update rules [5; 28; 49].

A related line of research considers strategic dynamics defined by strategy update functions, taking the form \(x^{i}_{t+1}=f^{i}(_{t})\) in discrete time or an analogous form in continuous time. In the case of deterministic strategy updates,  studied strategic dynamics in continuous time and showed that if the strategy update functions, analogous to \(f^{i}\) above, satisfy regularity conditions as well as a desirable property called uncoupledness, by which \(f^{i}\) cannot depend on the reward functions of \(i\)'s counterplayers, then the resulting dynamics are not Nash convergent in general. These results were recently generalized by . Additional possibility and impossibility results were presented by , who studied strategic dynamics in a different setting, where players do not observe counterplayer strategies. Under stochastic strategic dynamics, a number of positive results were obtained by incorporating exogenous randomness into one's strategy update, along with finite recall of recent play [23; 19; 20]. In the regret testing algorithm of , players revise their strategies according to whether or not their most recent strategy met a satisfaction criterion: if \(x^{i}_{t}\) performed within \(\) of the optimal performance against \(^{-i}_{t}\), player \(i\) continues using it and picks \(x^{i}_{t+1}=x^{i}_{t}\). Otherwise, player \(i\) experiments and selects \(x^{i}_{t+1}\) according to a probability distribution over \(^{i}\). Conditional strategy updates similar to this have appeared in several other works, such as [12; 10; 11], and the regret testing algorithm has been extended in several ways [20; 1].

A game is said to have the _satisficing paths property_ if every initial strategy profile is connected to some equilibrium by a satisficing path. As we discuss in the next section, satisficing paths can be interpreted as a natural generalization of best response paths. Consequently, the problem of identifying games that have the satisficing paths property is a theoretically relevant question analogous to characterizing potential games  or determining when a game has the fictitious play property [39; 40]. The concept of satisficing paths was first formalized in  in the context of multi-state Markov games, where it was shown that \(n\)-player symmetric Markov games have the satisficing paths property and this fact could be used to produce a convergent MARL algorithm. However, the core idea of satisficing paths appeared earlier, before this formalization: in the convergence analysis of the regret testing algorithm in , it was shown that two-player normal-form games have the satisficing paths property, though this terminology was not used. These earlier works made no claims about the existence of paths in general-sum \(n\)-player games, which is the focus of this paper.

## 2 Normal-form games

A finite, \(n\)-player normal-form game \(\) is described by a list \(=(n,,),\) where \(n\) is the number of players, \(=^{1}^{n}\) is a finite set of action profiles, and \(=(r^{i})_{i[n]}\) is a collection of reward functions, where \(r^{i}:\) describes the reward of player \(i\) as a function of the action profile. The \(i^{}\) component of \(\) is player \(i\)'s action set \(^{i}\).

**Description of play.** Each player \(i[n]\) selects a probability vector \(x^{i}_{^{i}}\) and then selects its action \(a^{i}\) according to \(a^{i} x^{i}\). The vector \(x^{i}\) is called player \(i\)'s mixed strategy, and we denote player \(i\)'s set of mixed strategies by \(^{i}:=_{^{i}}\). Players are assumed to select their actions without observing one another's actions, and the collection of actions \(\{a^{i}:i[n]\}\) is assumed to be mutually independent. The set of mixed strategy profiles is denoted \(:=^{1}^{n}\). After the action profile \(=(a^{1},,a^{n})\) is selected, each player \(i\) receives reward \(r^{i}()\).

Player \(i\)'s performance criterion is its expected reward, defined for each strategy profile \(\) as

\[R^{i}(x^{i},^{-i})=_{}[r^{i }(a^{1},,a^{n})],\]

where \(_{}\) signifies that \(a^{j} x^{j}\) for each player \(j[n]\) and we have used the convention that \(=(x^{i},^{-i})\) and \(^{-i}=(x^{1},,x^{i-1},x^{i+1},,x_{n})\). Since player \(i\)'s objective depends on the strategies of its counterplayers, the relevant optimality notion is that of (\(\)-) best responding.

**Definition 1**.: _A mixed strategy \(x^{i}_{*}^{i}\) is called an \(\)-best response to the strategy \(^{-i}^{-i}\) if_

\[R^{i}(x^{i}_{*},^{-i}) R^{i}(x^{i},^{-i})-  x^{i}^{i}.\]The standard solution concept for \(n\)-player normal form games is that of (\(\)-) Nash equilibrium, which entails a situation in which all players are simultaneously (\(\)-) best responding to one another.

**Definition 2**.: _For \( 0\), a strategy profile \(_{*}=(x^{i}_{*},^{i}_{*})\) is called an \(\)-Nash equilibrium if, for every player \(i[n]\), \(x^{i}_{*}\) is an \(\)-best response to \(^{-i}_{*}\)._

Putting \(=0\) above, one recovers the classical definitions of _best responding_ and _Nash equilibrium_. For any \( 0\), the set of \(\)-best responses to a strategy \(^{-i}\) is denoted \(^{i}_{}(^{-i})^{i}\).

### Satisficing Paths

We now present the concept of satisficing paths as generalized best response paths.

**Definition 3**.: _A sequence of strategy profiles \((_{t})_{t 1}\) in \(\) is called a best response path if, for every \(t 1\) and every player \(i[n]\), we have_

\[x^{i}_{t+1}=x^{i}_{t},&x^{i}_{t}^{i}_{0}( ^{-i}_{t}),\\ x^{i}_{}^{i}_{0}(^{-i}_{t}),&.\]

The preceding definition of best response paths can be relaxed in several ways, and such relaxations are often desirable to avoid non-convergent cycling behavior (see  for an example). A common relaxation involves synchronizing players or incorporating inertia, so that only a subset of players switch their strategies at a given time, which can be help achieve coordination in cooperative settings . Beyond cooperative settings, the use of best response dynamics to seek Nash equilibrium may not be justified. In purely adversarial settings, for instance, best response paths cycle and fail to converge , and some alternative strategic dynamics are needed to drive play to equilibrium. Consider the following generalization of the best response update:

\[x^{i}_{t+1}=x^{i}_{t},&x^{i}_{t}^{i}_{0}( ^{-i}_{t}),\\ f^{i}(x^{i}_{t},^{-i}_{t})&.\]

The update defined above is characterized by a "win-stay, lose-shift" principle , which only constrains the player to continue using a strategy when it is optimal. On the other hand, the player is not forced to use a best response when \(x^{i}_{t}^{i}_{0}(^{-i}_{t})\), and may experiment with suboptimal responses according to a function \(f^{i}:^{i}\).1 Allowing the function \(f^{i}\) to be any function from \(\) to \(^{i}\), one generalizes best response updates and obtains a much larger set of sequences \((_{t})_{t 1}\) and greater flexibility to approach equilibrium from new directions. This motivates the following definition of satisficing paths.

**Definition 4**.: _A sequence of strategy profiles \((_{t})_{t=1}^{T}\), where \(T\{\}\), is called a satisficing path if it satisfies the following pairwise satisfaction constraint for any player \(i[n]\) and any \(t\):_

\[x^{i}_{t}^{i}_{0}(^{-i}_{t}) x^{i}_{t+1}=x ^{i}_{t}.\] (1)

The intuition behind satisficing paths is that they are the result of an iterative search process in which players settle upon finding an optimal strategy (i.e. a best response to the strategies of counterplayers) but are free to explore different strategies when they are not already behaving optimally. Note, however, that the definition above is merely a formal property of sequences of strategy profiles in \(\) and is agnostic to how a satisficing path is produced. The latter point will be important in the coming sections, where we analytically obtain a particular satisficing path as part of an existence proof.

We note that Condition (1) constrains only optimizing players. It does not mandate a particular update for the so-called unsatisfied player \(i\), for whom \(x^{i}_{t}^{i}_{0}(^{-i}_{t})\). In particular, \(x^{i}_{t+1}\) can be any strategy without restriction, and \(x^{i}_{t+1}^{i}_{0}(^{-i}_{t})\) is allowed. In addition to best response paths, constant sequences \((_{t})_{t 1}\) with \(_{t}\) are always satisficing paths, even when \(\) is not a Nash equilibrium. Moreover, since arbitrary strategy revisions are allowed when a player is unsatisfied, if \(_{1}\) is a strategy profile for which all players are unsatisfied, then \((_{1},_{2})\) is a satisficing path for any \(_{2}\).

**Definition 5**.: _The game \(\) has the satisficing paths property if for any \(_{1}\), there exists a satisficing path \((_{1},_{2},)\) such that, for some finite \(T=T(_{1})\), the strategy profile \(_{T}\) is a Nash equilibrium.2_

Satisficing paths were initially formalized in , where it was proved that two-player games and \(n\)-player symmetric games have the satisficing paths property. However, whether general-sum \(n\)-player games have the satisficing paths property was left as an open question. We answer this open question in Theorem 1, presented in the next section.

## 3 Existence of paths in normal-form games

**Theorem 1**.: _Any finite normal-form game \(\) has the satisficing paths property._

**Proof sketch.** Before presenting the formal proof, we describe the intuition of its main argument. In the proof of Theorem 1, we construct a satisficing path from an arbitrary initial strategy \(_{1}\) to a Nash equilibrium by repeatedly switching the strategies of unsatisfied players in a way that grows the set of _unsatisfied_ players after the update. Once the set of unsatisfied players is maximal, we argue that a Nash equilibrium can be reached in one step by switching the strategies of the unsatisfied players. The final point represents the main technical challenge in the proof, as switching the strategies of unsatisfied players changes the objective functions for the previously satisfied players. We address this challenge by showing the existence of a Nash equilibrium on the boundary of a strategy subset in which previously satisfied players remain satisfied.

To give the complete proof, we will require some additional notation, detailed below, and some supporting results, detailed in Appendix A and Appendix B.

**Additional notation.** We require notation for the following sets, defined for any \(\):

\[():=\{i[n]:x^{i}^{i}_{0}(^{-i})\},():=[n] ().\]

A player in \(()[n]\) is called _satisfied_ (at \(\)), and a player in \(()\) is called _unsatisfied_ (at \(\)). For \(\), we also define

\[():=\{:y^{i}=x^{i},\;  i()\}.\]

\(()\) is the subset of strategies that are accessible from strategy \(\), to mean one can obtain strategy \(()\) from \(\) by switching (at most) the strategies of players who were unsatisfied at \(\). We define a subset \(()()\) as

\[() :=\{():( )()\}\] \[=\{()| i (),\,i()\},\]

The set \(()\) consists of strategies \(\) that are accessible from \(\) and also fail to improve the status of players who were previously unsatisfied. The set name \(()\) is chosen to suggest that the players unsatisfied at \(\) are not better off at \(()\), since they are unsatisfied at both \(\) and \(\). We observe \(()\), hence \(()\) is non-empty.

Finally, we define a set \(()()\) as

\[() :=\{():()()\}\] \[=\{()| i ():i()\}.\]

The set \(()\) consists of strategies that are accessible from \(\), that leave all previously unsatisfied players unsatisfied, and flip at least one previously satisfied player to being unsatisfied. In particular, if \(()\), this means \(|()||()|+1\). We observe that \(()\) may be empty, and \(()() ()\).

### Proof of Theorem 1

**Remark 1**.: _In the proof below, we analytically construct a path from \(_{1}\) to a Nash equilibrium. The process of selecting strategies \(_{1},_{2},\) and switching the component strategy of each player is done centrally, by the analyst, and should not be interpreted as a learning algorithm._

Proof.: Let \(_{1}\) be any initial strategy profile. We must produce a satisficing path of finite length terminating at a Nash equilibrium. Equivalently, we must produce a sequence \(_{1},,_{T}\) with \(_{t+1}(_{t})\) for each \(t\) and \(_{T}\) a Nash equilibrium, where the length \(T\) may depend on \(_{1}\). In the trivial case that \(_{1}\) is a Nash equilibrium, we put \(T=1\). The remainder of this proof focuses on the non-trivial case, where \(_{1}\) is not a Nash equilibrium.

To begin, we produce a satisficing path \(_{1},,_{k}\) as follows. We put \(t=1\), and while both \((_{t})\) and \((_{t})\), we arbitrarily fix \(_{t+1}(_{t})\) and increment \(t t+1\). By construction, we have

\[(_{1}) (_{t})(_{t+1})\]

for each non-terminal iteration \(t\), where the inequality holds because \(_{1}\) is not a Nash equilibrium. Thus, the number of unsatisfied players is strictly increasing along this satisficing path. Since the number of unsatisfied players is bounded above by \(n\), and since we have assumed \(|(_{1})| 1\), this process terminates in at most \(n-1\) steps. Letting \(k\) denote the terminal index of this process, we have \(k n-1\).

By the construction of the path \((_{1},,_{k})\), (at least) one of the following holds at index \(k\): either \((_{k})=\) or \((_{k})=\). In other words, either no player is satisfied at \(_{k}\), or there is no accessible strategy that grows the subset of unsatisfied players.

**Case 1:**\((_{k})=\), and all players are unsatisfied at \(_{k}\). In this case, we may switch the strategy of each player \(i[n]\) to any successor strategy. That is, \((_{k})=\). We fix an arbitrary Nash equilibrium \(_{}\), put \(_{k+1}=_{}\), and let \(T=k+1\). Then, \((_{1},,_{T})\) is a satisficing path terminating at equilibrium.

**Case 2:**\((_{k})\) and \((_{k})=\). In this case, there are no accessible strategies that strictly grow the set of unsatisfied players.

Since \((_{k})=\), the following holds: for any strategy \((_{k})\) and any satisfied player \(i(_{k})\), we have that \(i()\). (Otherwise, if \(i()\), then \((_{k})\), since it flipped a satisfied player. But this contradicts the emptiness of \((_{k})\).)

We now argue that there exists a strategy profile \(_{}\) accessible from \(_{k}\) such that all players unsatisfied at \(_{k}\) are satisfied at \(_{}\). That is, there exists an accessible strategy \(_{}(_{k})\) such that

\[(_{k})(_{}).\] (2)

To see that such a strategy \(_{}\) exists, note that fixing the strategies of the \(m\) players satisfied at \(_{k}\) defines a new game, say \(\), with \(n-m\) players, and the new game \(\) admits a Nash equilibrium \(}_{}=(_{}^{i})_{i(_{k})}\). We extend \(}_{}\) to be a strategy profile in the larger game \(\) by putting \(x_{}^{i}=x_{k}^{i}\) for players \(i(_{k})\) while putting \(x_{}^{j}=_{}^{j}\) for players \(j(_{k})\). By construction, we have that \(x_{}^{j}_{0}^{j}(_{}^{-j})\) for each \(j(_{k})\), so (2) holds.

From (2), it is clear that \(_{}(_{k})\), since \((_{k})\) consists of strategies accessible from \(_{k}\) in which unsatisfied agents remain unsatisfied, while the previously unsatisfied agents are satisfied at \(_{}\). We now state a key technical lemma, which asserts that although \(_{}\) does not belong to \((_{k})\), it is a limit point of this set. A proof of Lemma 1 given in Appendix B.

**Lemma 1**.: _If \((_{k})=\), then there exists a sequence \(\{_{t}\}_{t=1}^{}\), with \(_{t}(_{k})\) for each \(t\), such that \(_{t}_{t}=_{}\)._

We will argue that \(_{}\) is a Nash equilibrium for the original game \(\). For each player \(i[n]\), we introduce a function \(F^{i}:\) given by \(F^{i}(x^{i},^{-i})=_{a^{i}^{i}}R^{i}(_{a^{i}}, ^{-i})-R^{i}(x^{i},^{-i}),\) for each \(=(x^{i},^{-i})\). The functions \(\{F^{i}\}_{i=1}^{n}\) have the following useful properties, which are well known , and are summarized in Appendix A. For each player \(i[n]\): (a) \(F^{i}\) is continuouson \(\); (b) \(F^{i}() 0\) for all \(\); (c) for any \(^{-i}^{-i}\), a strategy \(x^{i}\) is a best response to \(^{-i}\) if and only if \(F^{i}(x^{i},^{-i})=0\).

Let \((_{t})_{t=1}^{}\) be a sequence in \((_{k})\) converging to \(_{}\), which exists by Lemma 1. For any previously satisfied player \(i(_{k})\), since \((_{k})=\) and \(_{t}(_{k})\), from a previous observation, we have that \(i(_{t})\). Equivalently, \(x_{k}^{i}^{i}_{0}(_{t}^{-i})\). Re-writing this using the function \(F^{i}\) and the notation \(y_{t}^{i}=x_{k}^{i}\) for satisfied players \(i(_{k})\), we have \(F^{i}(y_{t}^{i},_{t}^{-i})=0\) for all \(t\) and for any \(i(_{k})\). By continuity of \(F^{i}\), we have

\[0=_{t}F^{i}(_{t})=F^{i}(_{t}_{t})=F^{i}(_{}),\]

establishing that player \(i\) is satisfied at \(_{}\), and thus that \((_{k})(_{})\). Then, by (2), we had \((_{k})(_{})\), hence \((_{})=[n]\), and \(_{}\) is a Nash equilibrium accessible from \(_{k}\). We put \(T=k+1\) and \(_{T}=_{}\), which completes the proof, since \((_{1},,_{T})\) is a satisficing path terminating at a Nash equilibrium. 

### Algorithmic insights from the proof of Theorem 1

When coupled with a MARL algorithm that uses an exploratory satisficing strategy update, play will be driven along satisficing paths. Theorem 1 shows that for any starting strategy profile, some such path connects the strategy profile to an equilibrium, and so a sufficiently exploratory strategy update may drive play to equilibrium along a satisficing path. This offers important insights for the design of MARL algorithms. The first takeaway from Theorem 1 is that play can be driven to equilibrium by changing only the strategies of those players who are not best responding. In particular, this means that a satisfied agent does not need to continue updating its strategy after it becomes satisfied. As we will discuss in the next section, this property is helpful in distributed and decentralized multi-agent systems, where agents are able to assess whether they are satisfied but may not be able to assess whether the overall system is at equilibrium.

A second, more subtle takeaway comes from the proof of Theorem 1 and relates to the unorthodox and counterintuitive exploration scheme used to drive play to equilibrium. In the proof, one sees that suboptimal--and perhaps even _reward-deteriorating_--strategic updates were key to driving play to equilibrium along a satisficing path. As we outline below, this construction runs against the conventional approaches to designing MARL algorithms, and it can be used to avoid common pitfalls of MARL algorithms such as cyclical behavior.

At a high level, many existing multi-agent learning algorithms update the strategy parameter in a _reward-improving_ direction at each step. A related approach, described earlier, increments the strategy parameter in a regret-minimizing direction, which has a similar effect. While such algorithms are sensible from the point of view of a single self-interested individual, they may fail to drive play to a Nash equilibrium when all players adopt similar algorithms [36; 18; 37]. To address this non-convergence issue, one recurring algorithmic modification involves manipulating step sizes, either with a mixture of fast agents and slow agents  or with each individual varying its step sizes according to its performance . However, such approaches only come with provable convergence guarantees in select subclasses of games with exploitable structure. In instances where step size manipulation does not (or cannot) yield convergence, the analysis of Theorem 1 may offer an alternative route to algorithm modification.

With these two takeaways in mind, we envision at least two design principles that will be useful for future MARL algorithms. First, strategic updating may incorporate some measure of randomness when a player is not satisfied. This principle has been previously used with some success, but comes with a drawback relating to complexity. A second principle, which we believe to be new, leverages the second takeaway above, involving counterintuitive path construction: players may alternate between reward-improving periods (during which strategy updates are done in a conventional way that improves the agent's reward) and suboptimal periods (during which reward-deteriorating and/or random strategy updates may be used). The timing of such periods or the extent of the randomness in strategic updates may be made to depend on whether cycles in the strategy iterates were detected. By incorporating suboptimal exploration in an adaptive manner, a MARL algorithm can break cycles as needed but rely on conventional algorithms the remainder of the time.

Discussion

### Extension to Markov games

This paper focused on normal-form games with finitely many actions per player due to the central position that normal-form games occupy in game theory. Indeed, insights and intuition developed in normal-form games are helpful for understanding more complex models of strategic interaction. Of special note, finite normal-form games can be generalized to model dynamic strategic environments where rewards and environmental parameters evolve over time according to the history of play. We now describe the extension of Theorem 1 to Markov games, one generalization of finite normal-form games that is a popular model in MARL. Due to space limitations, a formal model for Markov games is postponed to Appendix C.

In an \(n\)-player Markov game, agents interact across discrete time. Each agent \(i[n]\) observes a sequence of state variables \(\{s_{t}\}_{t 1}\) taking values in a finite state space \(\) and selects a sequence of actions \(\{a_{t}^{i}\}_{t 1}\) taking values in a finite action set \(^{i}\). In this dynamic model, player \(i\)'s reward in period \(t\), denoted \(r_{t}^{i}=r^{i}(s_{t},_{t})\), depends on both the action profile \(_{t}\) and also on the state \(s_{t}\). The state process evolves according to a (jointly controlled) transition probability function \(\) as \(s_{t+1}(|s_{t},_{t})\). Rewards are discounted across time using a discount factor \((0,1)\), and player \(i\) attempts to maximize its expected \(\)-discounted return. In this generalization of finite normal-form games, _policies_ (defined as mappings from states to probability distributions over actions) generalize mixed strategies, and the solution concept of _Markov perfect equilibrium_ refines the concept of Nash equilibrium and serves as a popular stability objective for MARL algorithm designers .

Partial results for multi-state Markov games have previously been obtained in special classes of games and used to produce MARL algorithms . The analysis presented in this paper uses a rather different approach that seems promising for extending those results. In the proof of Theorem 1, we used functions \(\{F^{i}\}_{i=1}^{n}\) to characterize best responding in a finite normal-form game. In fact, analogous functions can also be obtained for policies in multi-state Markov games, and these functions satisfy the same desired properties invoked in the proof of Theorem 1 (c.f. [52, Lemmas 2.10-2.13]). For this reason, and due to the central role of continuity in our proof, it seems likely that Theorem 1 can be extended to general-sum Markov games. However, one aspect of the extension remains open, namely the generalization of Lemma 1. In Appendix C, we describe the issue that precludes direct generalization of our normal-form proof of Lemma 1, but we note that this appears to be related only to the proof technique rather than a fundamental obstacle to the generalization.

### On decentralized learning

Multi-agent reinforcement learning algorithms based on the "win-stay, lose-shift" principle characteristic of satisficing paths are especially well suited to decentralized applications, since players are often able to estimate the performance of their current strategy as well as the performance of an optimal strategy, even under partial information. In decentralized problems, coordinated search of the set \(\) of strategy profiles for a Nash equilibrium is typically infeasible, and players must select successor strategies in a way the depends only on quantities that can be locally accessed or estimated.

For instance, consider a trivial coordinated search method, where player \(i\) selects \(x_{t+1}^{i}\) uniformly at random from \(^{i}\) whenever \(_{t}\) was not a Nash equilibrium and selects \(x_{t+1}^{i}=x_{t}^{i}\) only when \(_{t}\) is a Nash equilibrium. This process is clearly ill suited to decentralized applications, because player \(i\)'s strategy update depends on both a locally estimable condition (whether player \(i\) is best responding to \(_{t}^{-i}\)) as well as a condition that cannot be locally estimated (whether another player \(j i\) is best responding to \(_{t}^{-j}\).) The satisfaction (win-stay) constraint plays a key role as a _local_ stopping condition for satisficing paths, and rules out coordinated search of the set \(\) such as the trivial update outlined above. Examples of decentralized or partially decentralized learning algorithms leveraging satisficing paths in their analysis include [19; 33; 1; 52]. The analytic results of this paper suggest that algorithms such as these can be extended to wider classes of games and enjoy equilibrium guarantees under different informational constraints on the players.

### On complexity and dynamics

In Theorem 1, we showed that for any finite \(n\)-player normal-form game \(\) and any initial strategy profile \(_{1}\), there exists a satisficing path \(_{1},,_{T}\) of finite length \(T=T(_{1})\) terminating at a Nash equilibrium \(_{T}\). From the proof of Theorem 1, one makes the following observations. First, the length of such a path can be uniformly bounded above as \(T(_{1}) n\). Second, there exists a collection of strategy update functions \(\{f_{}^{i}:^{i}i[n]\}\) whose joint orbit is the satisficing path described by the proof of Theorem 1. That is, \(f_{}^{i}(_{t})=x_{t+1}^{i}\) for each player \(i[n]\), every \(0 t T-1\), and every \(_{1}\), where \(x_{t}^{i}\) is player \(i\)'s component of \(_{t}\) in the satisficing path initialized at \(_{1}\).

The proof of Theorem 1 is semi-constructive. At each step along the path, we describe how the next strategy profile should be picked (e.g. \(_{t+1}(_{t})\)), but we do not suggest an algorithm for computing it. In at least one place, namely Case 1 where we put \(_{T}:=_{*}\), the path construction involves moving jointly to a Nash equilibrium in one step. The computational complexity of such a step is prohibitive , underscoring that ours is an analytical existence result rather than a computational prescription.

Although we have shown that there exists a discrete-time dynamical system on \(\) that converges to Nash equilibrium in \(n\) steps and can be characterized by update functions \(\{f_{}^{i}\}_{i=1}^{n}\), we note that our possibility result does not contradict the impossibility results of  or . In particular, the functions \(\{f_{}^{i}\}_{i=1}^{n}\) need not be (and usually will not be) continuous, violating the regularity conditions of  and , and furthermore the functions \(\{f_{}^{i}\}_{i=1}^{n}\) depend crucially on the game \(\) in a way that violates the uncoupledness conditions of  and .

### Open questions and future directions

Several interesting questions about satisficing paths remain open. We now briefly describe some that we find especially practical or theoretically relevant.

While this paper dealt with satisficing paths defined using a best responding constraint, the original definition was stated using an \(\)-best responding constraint, according to which a player who was \(\)-best responding was not allowed to switch its strategy. Putting \(=0\), one recovers the definition used here, but one may also select \(>0\), which can be desirable to accommodate for estimation error in multi-agent reinforcement learning applications. The added constraint reduces freedom to switch strategies, and thus makes it more challenging to construct paths starting from a given strategy profile. On the other hand, the collection of Nash equilibria is a strict subset of the set of \(\)-Nash equilibria, and one can attempt to guide the process to a different terminal point in a larger set. At this time, it is not clear to us whether the main result of this paper holds for small \(>0\). It is clear, however, that the proof technique used here will have to be modified, since we have relied on Lemma 1, whose proof involved an indifference condition and invoked the fundamental theorem of algebra, and relaxing to \(>0\) would render such an argument ineffective.

A second interesting question for future work is whether multi-state Markov games with \(n>2\) players have the satisficing paths property. The case with \(n=2\) was resolved by , but the proof technique used there did not generalize to \(n 3\). By contrast, our proof technique readily accommodates any number of players, but is designed for stateless normal-form games. Our proof used multi-linearity of the expected reward functions \(\{R^{i}\}_{i=1}^{n}\), which does not generally hold in the multi-state setting.

In this work, satisficing paths were defined in a way that allowed an unsatisfied player \(i\) to change its strategy to any strategy in its set \(^{i}\), without constraint. This is interesting in many problems where the set of strategies can be explicitly and directly parameterized, but may be unrealistic in games where the set of strategies is poorly understood or in which a player can effectively represent only a subset of its strategies \(^{i}^{i}\). In such games, the question more relevant for algorithm design is whether the game admits satisficing paths to equilibrium within the restricted subset \(^{1}^{n}\). This point was implicitly appreciated by both  and  and explicitly noted in . Some negative results were recently established in  for games admitting pure strategy Nash equilibrium when randomized action selection was not allowed and the constrained set was given by \(^{i}=^{i}\), underscoring the importance of the topology of the sets appearing in the proof of Theorem 1.

Conclusion

Satisficing paths can be interpreted as a natural generalization of best response paths in which players may experimentally select their next strategy in periods when they fail to best respond to their counterplayers. While (inertial) best response dynamics drive play to equilibrium in certain well-structured classes of games, such as potential games and weakly acyclic games , the constraint of best responding limits the efficacy of these dynamics in games with cycles in the best response graph . In such games, best response paths leading to equilibrium do not exist, and multi-agent reinforcement learning algorithms designed to produce such paths will not lead to equilibrium.

In this paper, we have shown that every finite normal-form game enjoys the satisficing paths property. By relaxing the best response constraint for unsatisfied players, one ensures that paths to equilibrium exist from any initial strategy profile. Multi-agent reinforcement learning algorithms designed to produce satisficing paths, rather than best response paths, thus do not face the same fundamental obstacle of algorithms based on best responding. While algorithms based on satisficing have previously been developed for two-player games normal-form games, symmetric Markov games, and several other subclasses of games, the findings of this paper suggest that similar algorithms can be devised for the wider class of \(n\)-player general-sum normal-form games.