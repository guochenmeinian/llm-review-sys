# Sample Selection via Contrastive Fragmentation for Noisy Label Regression

Chris Dongjoo Kim\({}^{1,2}\), Sangwoo Moon\({}^{1}\), Jihwan Moon\({}^{1}\)

Dongyeon Woo\({}^{1}\), Gunhee Kim\({}^{1,2}\)

\({}^{1}\)Seoul National University, \({}^{2}\)LG AI Research

{cdjkim, sangwoo.moon, jihwan.moon, dongyeon.woo}@vision.snu.ac.kr

gunhee@snu.ac.kr

These authors contributed equally to this work.The code is available at [https://github.com/cdjkim/ConFrag](https://github.com/cdjkim/ConFrag)

###### Abstract

As with many other problems, real-world regression is plagued by the presence of noisy labels, an inevitable issue that demands our attention. Fortunately, much real-world data often exhibits an intrinsic property of continuously ordered correlations between labels and features, where data points with similar labels are also represented with closely related features. In response, we propose a novel approach named ConFrag, where we collectively model the regression data by transforming them into disjoint yet contrasting fragmentation pairs. This enables the training of more distinctive representations, enhancing the ability to select clean samples. Our ConFrag framework leverages a mixture of neighboring fragments to discern noisy labels through neighborhood agreement among expert feature extractors. We extensively perform experiments on six newly curated benchmark datasets of diverse domains, including age prediction, price prediction, and music production year estimation. We also introduce a metric called Error Residual Ratio (ERR) to better account for varying degrees of label noise. Our approach consistently outperforms fourteen state-of-the-art baselines, being robust against symmetric and random Gaussian label noise.2.

## 1 Introduction

Regression is an important task in many disciplines such as finance (Zhang et al., 2017; Wu et al., 2020), medicine (de Vente et al., 2021; Tanaka et al., 2022), economics (Zhang et al., 2022), physics (Sia et al., 2020; Doi et al., 2022), geography (Liu et al., 2023) and more. However, real-world regression labels are prone to being corrupted with noise, making it an inevitable problem to overcome in practical applications. In previous research, noisy label regression has been studied much in age estimation with noise incurred from Web data crawling (Rothe et al., 2018; Yiming et al., 2021). Beyond that, the issues of continuous label errors have also been reported in the tasks of object detection (Su et al., 2012; Ma et al., 2022) and pose estimation (Geng and Xia, 2014) as well as measurements in hardware systems (Zhou et al., 2012; Zang et al., 2019).

The vast amount of noisy label learning research has focused more on classification than regression. Some notable approaches include regularization (Wang et al., 2019; Zhang and Sabuncu, 2018), data re-weighting (Ren et al., 2018; Shen and Sanghavi, 2019), training procedures (Jiang et al., 2018), transition matrices (Yao et al., 2020; Xia et al., 2020), contrastive learning (Zhang et al., 2021; Li et al., 2022), refurbishing (Song et al., 2019) and sample selection (Lee et al., 2018; Ostyakov et al., 2018). Particularly, sample selection can be further divided into exploring the memorability of neuralnetworks (Arpit et al., 2017; Zhang et al., 2017) and delineating samples via the loss magnitude (Wei et al., 2020).

To the best of our knowledge, there have been three works that address the noisy label problem for regression with deep learning. Castells et al. (2020) propose a weighted loss correction method based on the small loss assumption. Garg and Manwani (2020) propose an ordinal regression-based loss correction via noise transition matrix estimation. However, they assume that accurate noise rates are known in prior (Patrini et al., 2017), which are hard to attain in practice. Yao et al. (2022) extend MixUp (Zhang et al., 2018) for regression to interpolate the proximal samples in the label space to improve generalization and robustness. Thanks to its regularizing effect, it can aid the noisy label issue.

In this work, we explore the regression problem with noisy labels, surpassing the scope of previous studies both empirically and methodologically. For evaluation, we make three notable contributions. Firstly, recognizing the absence of a standardized benchmark dataset for this task, we take the initiative to curate six balanced real-world datasets. These datasets span diverse domains, encompassing age estimation (Niu et al., 2016; Yiming et al., 2021), music production year estimation (Bertin-Mahieux et al., 2011), and clothing price prediction (Kimura et al., 2021). Secondly, we conduct a comprehensive empirical benchmark exercise, evaluating the performance of fourteen baselines, which are carefully selected from various branches of noisy label research extendable to regression tasks. Lastly, we introduce a performance measure called Error Residual Ratio (ERR), which accounts for the unique property of regression, where labels exhibit varying degrees of noise severity.

Methodologically, we introduce the ConFrag (Contrastive Fragmentation) framework as a novel approach to address label noise in regression. It is rooted in one fundamental characteristic of regression: the continuous and ordered correlation between the label and feature space. In other words, data points similar in the feature space are likely to have similar labels. The framework begins by partitioning the dataset into smaller segments, referred to as fragments, and pairs the most distant fragments in the label space to form what we call _contrastive fragment pairs_. Training an expert network on these contrastive fragment pairs aids in generalization due to the distinctive feature matching and conversion of closed-set noise into open-set noise, which is less detrimental for learning. Next, the framework incorporates neighboring relationships by aggregating and reordering the learned features to detect clean samples. This is accomplished through the design of Mixture (Jacobs et al., 1991) of neighboring fragments. Furthermore, we enhance our approach with neighborhood jittering regularization, which strengthens the selection process by improving the data coverage of each expert. This, in turn, leads to improved agreements among neighboring fragments and serves as an effective tool for mitigating overfitting. Finally, the contributions of this work can be summarized as follows.

1. We propose a novel method named ConFrag (Contrastive Fragmentation) for noisy labeled regression. It leverages the inherent orderly relationship within the label and feature space by employing contrastive fragment pairing and constructs a mixture model based on neighborhood agreement. This is further enhanced by our neighborhood jittering regularization.
2. We perform one of the most thorough empirical investigations into noisy labeled regression up to date. We assemble six well-balanced benchmarks using datasets of AFAD (Niu et al., 2016), IMDB-Clean (Yiming et al., 2021), IMDB-WIKI (Rothe et al., 2018), UTK-Face (Zhifei et al., 2017), SHIFT15M (Kimura et al., 2021), and MSD (Bertin-Mahieux et al., 2011), on which we evaluate fourteen baselines. We design a metric termed ERR (Error Residual Ratio), which accounts for the degree of noise severity within the labels, offering a more comprehensive assessment. Our experiments affirm the superiority of ConFrag over state-of-the-art noisy label learning baselines.

## 2 ConFrag: Contrastive Fragmentation

In the noisy label regression problem, we are presented with a dataset denoted as \(=\{,Y\}\); in each sample \((x,y)\), \(x^{d}\) is an input, and \(y\) is the observed label, which can be possibly noisy. We use \(y^{}\) to denote the groundtruth label. The objective of ConFrag is to sample a _clean_ subset of the data as \(\). By training on \(\), we aim to enhance the performance of the regression model.

An overview of our ConFrag framework is shown in Fig. 3(a). The framework has the following steps. We divide the dataset into what we refer to as _contrastive fragment pairs_ (SS 2.1), which collectively enhance the training of the feature extractors (SS 2.2). We then select clean samples \(\) from dataset \(\) based on neighborhood agreements, utilizing a fragment-based mixture model (SS 2.3). A regression model is trained on the clean samples \(\). We also propose neighborhood jittering as a regularizer for further improved training (SS 2.4). ConFrag is noise rate-agnostic unlike prior methods as it operates without knowing a pre-defined noise rate.

### Contrastive Fragment Pairing

In order to sample a _clean_ data subset \(\), we need to learn a robust feature that can distinguish clean samples from noisy ones. As one theoretical result in (Zhang et al., 2023), the cross-entropy loss used in classification is better for learning high-entropy feature representation than the mean squared loss in regression (see Appendix D.1 for details). Based on this, we start by discretizing the label space into \(F\) continuous fragments, transforming the original regression problem into the multi-class classification one. This transformation harnesses an inherent property of regression: data points with similar labels are also represented with closely related features, as acknowledged in prior studies (Gong et al., 2022; Yang et al., 2022; Yao et al., 2022).

However, instead of training a single feature extractor on the multi-class classification with \(F\) classes, we construct \(F/2\)_maximally contrasting fragment pairs_ and train a smaller expert feature extractor for each pair. The procedure of contrastive fragment pairing is detailed below with an illustration in Fig. 2:

1. Divide the range of continuous labels \(Y\) into \(F\) even number of equal-length fragments. This allows to divide the dataset \(\) into \(F\) disjoint subsets: \(=\{_{1},...,_{F}\}\), where each \(_{i}\) contains the data samples whose \(y\) values are in the \(i\)-th fragment label range.
2. Construct a complete graph \(g=\{,E\}\), where each vertex is a fragment \(_{i}\), and each edge weight \(e_{ij}\) is the distance in the label space between the closest samples of the fragments \((_{i},_{j})\).
3. Compute all possible _perfect matchings_(Monfared and Mallik, 2016; Gibbons, 1985), where every vertex of a graph is incident to exactly one edge in the graph.

Figure 1: (a) **An example of t-SNE illustration of contrastive fragment pairing**. The data with label noise are grouped into six fragments (\(f[1\)-\(6]\)) and formed into three contrastive pairs (\(f,,\)). Contrastive fragment pairing transforms some of closed-set noise (whose ground truth is within the target label set) into open-set noise (whose ground truth is not within the label set). For example, in the  figure, label noise whose ground truth fragment is either 1 or 4 is closed-set noise, and the others are open-set noise. The t-SNE illustration shows that learned features of open-set noises tend to reside outside the feature clusters of the clean samples. (b) The open-set noise is _less harmful_ with much lower errors (MRAE) in the downstream regression. (c) The contrastive pairing \((,,)\) is more effective than using all-fragments together ([1-6]), resulting in much lower MRAE scores. All experiments are based on IMDB-Clean-B with more details in Appendix G.4–G.5.

Figure 2: The contrastive fragment pairing algorithm.

4. Find the perfect matching with the largest minimal edge weight: \(=_{}(())\), where each \(\) is a perfect matching (graph), and \(()\) is the set of edge weights in \(\). Finally, \(=\{(_{i},_{j}),,(_{k}, _{l})\}\) constitutes the _maximally contrasting pairs_ of fragments.

Motivation behind contrastive fragment pairing. Formulating the multi-class classification problem into \(F/2\) binary classification problems via contrastive fragment pairing has the following advantages. Firstly, since the distance between fragments in each contrastive fragment pair is large, the feature extractor trained on each contrastive pair can generalize better (Shawe-Taylor and Cristianini, 1998; Gronlund et al., 2019, 2020). Fig. 1(c) shows the generalization abilities of the expert feature extractors trained on contrastive fragment pairs compared to the single feature extractor trained on all fragments. When using a single feature extractor on all fragments (all-frags), the samples selected by the feature extractor tend to become more noisy as the feature extractor overfits, causing the regressor to perform worse over time. On the other hand, when using multiple feature extractors trained on contrastive pairs, the performance of the regression model consistently improves, indicating that the learned features are more robust and the selected samples are cleaner. The large distance between fragments also explains why contrastive fragment pairing is superior to other fragment pairings, as shown in SS 4.3. The analysis of the prediction depth (Baldock et al., 2021) in Appendix D.3 supports the claim, as it shows that the binary classification on contrastive fragment pairs results in lower prediction depth, leading to better generalization.

Secondly, the contrastive fragment pairing transforms some of _closed-set_ label noise (whose ground truth is within the label set) into _open-set_ label noise (whose ground truth is not within the label set), as shown in Fig. 1(a). Previous works (Wei et al., 2021; Wan et al., 2024) observe that the open-set noise is less harmful than the closed-set noise and may even benefit generalization and robustness against inherent noisy labels. Indeed, in our experiments, we found similar observations where injecting open-set label noise is less harmful than closed-set one, as shown in Fig. 1(b).

The t-SNE visualization in Fig. 1(a) also supports this observation. Let \(f\) and \(f^{}\) be fragments that the observed label \(y\) and the groundtruth label \(y^{}\) respectively belong to. Prior to contrastive fragment pairing, all of the noisy labeled data (\(f f^{}\)) are closed-set noise as their ground truth fragment ids are within the label set (\(f^{}[16]\)) and their features are located in the feature spaces of incorrect classes within the group. After contrastive fragment pairing, much of these noisy labeled data is transformed to open-set noise (\(f^{}\) while \(f\) in case of fragment pair \(\)), and their learned features tend to reside outside the feature clusters of the clean samples, thus mitigating the adverse effects of the noise.

### Training Feature Extractors for Contrastive Pairs

Once we obtain the contrastive fragment pairs \(\), we train \(F/2\) number of expert feature extractors on binary classification \(p(y|x;_{i,j})\) with its respective contrastive pair \((_{i},_{j})\), where \(_{i,j}\) denotes the parameter of an expert. That is, it is trained to predict whether a data \(x\) is in \(_{i}\) or \(_{j}\). Later, the feature extractors play a crucial role in determining whether a sample \((x,y)\) is clean.

### Mixture of Neighboring Fragments

With the learned expert feature extractors, the next step is to perform sample selection. Given a sample \((x,y)\), let \(f\) be a fragment close to \(y\) and \(f^{+}\) be its contrasting pair. Intuitively, we consider a sample clean if the expert trained on \((_{f},_{f^{+}})\) strongly predicts that \(x\) belongs to a fragment \(f\). However, since the expert feature extractor is a binary classifier only trained using a contrasting pair of fragments, we utilize all experts' opinions to obtain a more robust prediction. Specifically, we deem a sample as clean if the experts exhibit a consensus response (**Neighborhood Agreement**) for fragments close to \(y\) (**Fragment Prior**).

Based on this intuition, we formulate Mixture of Experts (MoE) (Jacobs et al., 1991) model, where the sampling probability of a datapoint \((x,y)\) is defined as

\[p(s|x,y,_{1 F};)=_{f}^{F}_{f}(y)_{f}(x; _{1 F},), \]

where \(\) denotes parameters of all feature extractors, \(_{f}\) is the _fragment prior_ (mixture weight), and \(_{f}\) is the _neighborhood agreement_ (a binary vote of whether \(x\) belongs to the fragment \(f\)). Based on the intuition above, \(_{f}(y)\) is large when the fragment \(f\) is close to \(y\), and \(_{f}(x)\{0,1\}\) is \(1\) if \(x\) is likely to belong to the fragment \(f\).

**Fragment Prior.** For a sample \((x,y)\), we compute the prior \(_{f}(y)\) of a fragment \(f\), using a softmax weighting of each fragment \(f\) with respect to its relative distance to \(y\):

\[_{f}(y)=(y))}{_{f^{}}^{F}(g_{f^{}}(y))}, \]

where \(g_{f}(y)=(Y)/(|y-_{f}|)\), \((Y)=(Y)-(Y)\) is the label range, and \(_{f}\) is the mean label value of fragment \(f\). Since \((Y)\) is a constant for a given dataset, \(g_{f}(y)\) rapidly decreases when the mean value of fragment \(f\) is far from \(y\) in the continuous label space. From the MoE perspective, the fragment prior can be regarded as soft gating that depends on \(y\).

**Neighborhood Agreement.** Given a sample \((x,y)\) and a fragment \(f\), we need to determine whether \(x\) belongs to \(f\). The simplest approach is to use the expert trained using \((_{f},_{f^{+}})\) to classify whether \(x\) belongs to \(f\) or \(f^{+}\), where \(f^{+}\) is the contrasting fragment of \(f\). Based on the classification output \(h(x;_{f,f^{+}})\{f,f^{+}\}\), we define self-agreement as:

\[_{f}^{}=[h(x;_{f,f^{+}})=f] \]

where \([A]\) is the Iverson bracket outputting \(1\) if \(A\) is true, and 0 otherwise. Since training with noisy labels often results in suboptimal calibration , we use discrete classification output for \(_{f}^{}\) rather than continuous probabilistic one.

Since the expert \(_{f,f^{+}}\) is only trained to discriminate between \(f\) and its contrasting fragment \(f^{+}\), it is better to utilize other experts to obtain a more robust prediction. For example, consider contrastive fragment pairs \(\{(1,4),(2,5),(3,6)\}\) as in Fig. 2. If \(x\) is more likely to belong to fragment \(2\) than \(5\), then it should be more likely to belong to \(1\) than \(4\) and \(3\) than \(6\). Thus, we consider agreement of neighboring fragments \(f_{L}\) (left) and \(f_{R}\) (right) to obtain neighborhood agreement \(_{f}(x;_{1 F},)\):

\[_{f}(x;_{1 F},)=_{f}^{} _{f}^{},\ \ \ \ _{f}^{}=[_{f_{L}}^{}_{f_{R}}^{ }]. \]

Intuitively, \(_{f}\) is \(1\) if the fragment \(f\) is more likely for \(x\) than \(f^{+}\) (\(_{f}^{}=1\)) and either \(f\)'s left or right fragment is more likely for \(x\) than its respective contrasting fragment (\(_{f}^{}=1\)).

In practice, we implement two variants of the agreements in Eq.(3-4) using the feature extractor's binary classifier and a \(K\)-nearest neighbor classifier on the learned feature space. These two classifiers respectively consider _predictive_ and _representational_ aspects of the expert feature extractor and effectively work as an ensemble, as shown in Appendix G.7. As a result, we compute two versions of sample probability in Eq.(1), and use the union of the sampled _clean_ dataset \(\) for training of the regression model. Algorithm 1 in Appendix summarizes the overall procedure.

Figure 3: **Contrastive Fragmentation framework. (a) The overall sequential process of our framework. (b) Shows the fragmentation of the continuous label space to obtain _contrasting fragment pairs_ (§ 2.1) and train feature extractors on them. (c) Sample Selection by Mixture of Neighboring Fragments obtains the selection probability in both prediction and representation perspectives (§ 2.3). (d) Illustration of Neighborhood Jittering (§ 2.4).**

### Neighborhood Jittering

A potential limitation of mixture models is that the individual expert feature extractor may not fully benefit from the full dataset as they model their own disjoint subsets (Dukler et al., 2023). Our neighborhood jittering mitigates this limitation as a robust regularizer that expands the effective coverage of each contrastive fragment pair during learning. The process is visualized in Fig. 3(d).

We bound the ratio of the jittering buffer range within \([0,]\), where \(F\) is the fragment number. For every epoch, we shift the label coverage of each fragment by randomly sampling the value in this range. Jittering leads to a partially overlapping mixture model (Heller and Ghahramani, 2007; Hinton, 2002) as some data belong to multiple, neighboring fragments and thus the effective coverage per each expert is expanded. Such regularization inhibits feature extractors from overfitting to potentially noisy samples and promotes learning of more robust features, even those that can be generalizable to overlapping parts of neighboring fragments.

Fig. 4(a) shows that with jittering, the feature extractor exhibits higher accuracy on the clean test data due to its regularization effect. In the sample selection stage (Fig. 4(b)), the feature extractor trained without jittering easily overfits the noise, resulting in over-selection and higher ERR (SS 4.2). In contrast, the jittered feature extractor achieves a relatively low selection rate with halved ERR, indicating that the noisier samples are filtered out. Better sample selection due to jittering subsequently leads to significantly better performance in regression (Fig. 4(c)). In Appendix G.9, we compare neighborhood jittering to other regularizations, demonstrating its efficacy.

## 3 Related Works

We review prior works on learning with noisy labels and defer a comprehensive survey to Appendix E. We organize them into those utilizing prediction, representation, and combination of the two.

**Prediction-based Methods**. This approach has been the focus of much existing research and covers a wide array of topics: (i) the small loss selection by exploring the pattern of memorization in neural networks (Han et al., 2018; Arazo et al., 2019), (ii) relying on the consistency of predictions to select or refurbish the samples (Liu et al., 2020; Huang et al., 2020), (iii) estimating the noise distribution (Patrini et al., 2017; Hendrycks et al., 2018), (iv) introducing auxiliary parameters or labels (Pleiss et al., 2020; Hu et al., 2020), (v) using unlabeled data with semi-supervised learning (Li et al., 2020; Bai et al., 2021; Karim et al., 2022), and (vi) designing a noise-robust loss function (Menon et al., 2020; Wang et al., 2019).

**Representation-based Methods**. This approach has seen a recent surge in interest, including (i) clustering based selection (Mirzasoleiman et al., 2020; Wu et al., 2020), (ii) feature eigendecomposition filtering (Kim et al., 2021), (iii) using neighbor information to sample and refurbish with clean validation (Li et al., 2022; Gao et al., 2016), and (iv) generative models of features for sampling (Lee et al., 2019).

**Combination**. Some works have also studied the combination of representation and prediction spaces. Wang et al. (2022) formulate a penalized regression between the network features and the labels for

Figure 4: **Jittering analysis. (a) When trained without jittering, feature extractors easily overfit the noisy training data (yellow-shaded region), while jittering-regularized feature extractors robustly learn from the noisy training data. (b) Overfitted feature extractors (yellow-shaded region) on noisy samples increase their likelihood, leading to a higher selection rate and ERR. It exhibits nearly twice higher ERR (a lower value is better). (c) Most importantly, jittering regularization improves performance in regression.**selection, and Ma et al. (2018) use intrinsic dimensionality and consistent predictions to refurbish. Other important approaches include (i) regularization via MixUp (Zhang et al., 2018) along with its regression version (Yao et al., 2022), (ii) model-based methods that discourage large parameter shifts (Hu et al., 2020), and (iii) importance discrimination of parameter updates (Xia et al., 2021).

The majority of previous works have studied noisy labels for classification. Hence, a large portion of these works may not be directly applicable to regression tasks due to the restricted usage of class-wise information. In SS 4, we empirically compare our method with some of these works that are expandable to the regression task with some or minor technical adaptation.

## 4 Experiments

We compare ConFrag with fourteen strong baselines adapted for noisy label regression. Due to the scarcity of benchmark datasets, we update existing datasets for the study of noisy labels.

### Settings

**Curation of Benchmark Datasets.** We create six benchmark datasets for noisy labeled regression to encompass a sufficient quantity of balanced data, span multiple domains, and present a meaningful level of complexity. (i) _Age Prediction_ from an image is a well-studied regression problem (Li et al., 2019; Shin et al., 2022; Lim et al., 2020). To address this domain, we acquire four datasets of **AFAD**(Niu et al., 2016), **IMDB-Clean**(Yiming et al., 2021), **IMDB-WIKI**(Rothe et al., 2018), and **UTKFace**(Zhifei et al., 2017). Notably, IMDB-WIKI contains real-world label noise stemming from the automatic web crawling process (Yiming et al., 2021). We use a ResNet-50 backbone for all datasets. (ii) _Commodity Price Prediction_ is a vital real-world task (Wen-Huang et al., 2021). We opt for the **SHIFT15M** dataset (Kimura et al., 2021) due to the diversity and scale of this domain. This dataset is provided as the penultimate feature of the ImageNet pre-trained VGG-16 model. Consequently, we use a three-layer MLP architecture for all experiments (Papadopoulos et al., 2022; Kimura et al., 2021). (iii) _Music Production Year Estimation_ uses the tabular **MSD** dataset (Bertin-Mahieux et al., 2011). This dataset is identified as one of the most intricate and challenging datasets, based on the test R2 score (Grinsztajn et al., 2022). We adopt a tabular ResNet proposed by Gorishniy et al. (2021). The suffix "-B" is appended to the dataset name (_e.g._, AFAD-B) to indicate that it is a curated version of the original dataset. To focus on the noisy label problem, we take measures to balance the datasets as elaborated in Appendix F.1.

**Experimental Design.** For all datasets except for IMDB-WIKI-B which contains real-world label noise, we inject symmetric and Gaussian noise into the labels, as done in prior literature (Yao et al., 2022; Yi and Wu, 2019; Wei et al., 2020). These types of noise can simulate a low-cost (human-free) controlled setting. Symmetric noise mimics randomness such as Web crawling or annotator errors, and Gaussian noise is often used for modeling the regression label noise. While Yao et al. (2022) inject a _fixed_ 30% standard deviated Gaussian noise for _every label_, we make it more realistic by _randomizing_ the standard deviation up to 30% or 50% of the domain's range. For our ConFrag experiments, we fix the fragment number (\(F\)) as four. See Appendix F.3 for further training details.

**Baselines.** There are many existing methods of noisy labeled learning for classification. We assess fourteen baselines from the three branches that are naturally adaptable to regression with minor or no updates. (i) Small loss selection: CNLCU-S,H (Xia et al., 2022), Sigua (Han et al., 2020), SPR (Wang et al., 2022), BMM (Arazo et al., 2019), DY-S (Arazo et al., 2019), SuperLoss (Castells et al., 2020). (ii) Regularization: C-mixup (Yao et al., 2022), RDI (Hu et al., 2020), CDR (Xia et al., 2021), D2L (Ma et al., 2018). (iii) Refurbishment: AUX (Hu et al., 2020), Selfie (Song et al., 2019), Co-Selfie (Song et al., 2019). Appendix F.2 comprehensively details these baselines.

### Evaluation Metrics

We mainly report the Mean Relative Absolute Error (MRAE) following prior works. The MRAE is computed as \((e/)-1\), where \(e\) is the model's Mean Absolute Error (MAE) performance under varying conditions (noise type, severity) and \(\) is the noise-free model's MAE. We express MRAEs in percentage for better comprehensibility. The traditional MAE values are also reported in Appendix G.12. In addition, we report the Selection rate (a.k.a prevalence), which is a metric often seen 

[MISSING_PAGE_FAIL:8]

designed for classification and tend to treat all instances of noise as equally severe. Our proposed ERR considers the varying severity of noise and is defined as

\[=|_{c}^{||}|y_{c}-y_{c}^{}| }{1/||_{d}^{||}|y_{d}-y_{d}^{}|}, \]

where \(\) is a set of cleaned (selected or refurbished) samples. The numerator is the average cleaned error that serves as an indicator of the precision of the cleaned data, while the denominator is the average dataset error that normalizes it for standardized assessment. The ERR, along with the selection rate and regression metrics (_e.g._, MSE, MRAE), provides a deeper insight into the model performance. Ideally, a method with a high selection rate coupled with low ERR and regression error can be deemed as closer to the upper bound.

### Results and Discussion

**Overall performance.** Table 1 compares the MRAE values to the noise-free trained model between ConFrag and the baselines. We evaluate six types of noise: four symmetric and two random Gaussian noises. ConFrag and Co-ConFrag achieve the strongest performance in all experiments compared to the fourteen baselines. Notably, Co-ConFrag mixes co-teaching during the regression learning phase by assuming that \(\) still contains 25% noise. The results on UTKFace-B dataset can be found in Appendix G.1.

**Selection/ERR/MRAE comparison.** Fig. 5 compares ConFrag to five selection and refurbishment baselines of CNLCU-H, BMM, DY-S, AUX, Selfie on IMDB-Clean-B using the selection rate, ERR, and MRAE. Ideally, a model should attain a high selection rate and a low ERR. It is worth noting that the relative importance of ERR and selection rate may vary depending on the dataset and the task. ConFrag achieves the lowest ERR while maintaining above-average selection rates, resulting in the best MRAE. Appendix G.10 includes comparison results for all noise types with more baselines.

**Fragment pairing.** Fig. 6(a) compares contrastive pairing to alternative pairings using MRAE as a metric. The contrastive fragment pairing demonstrates superior performance to other pairing methods. Notably, the performance is poorest when both the average and minimum distance between fragments are smallest (\(,\) when \(F=4\), \(,,\) when \(F=6\)). While the pairings of \(,\) and \(,,\) have the same average distance between fragments as the contrastive pairings, their minimum distances between fragments are smaller, resulting in poorer performances than contrastive pairings. This result shows the effectiveness of contrastive fragment pairing for selecting clean samples. See Appendix G.4 for more details.

**Fragment number.** ConFrag introduces a hyperparameter \(F\), the number of fragments. While we simply set \(F=4\) for all experiments, we conduct analysis on the effect of using different \(F\), as shown in Fig. 6(b). On SHIFT1SM-B dataset, the performance is relatively stable across different fragment numbers. On IMDB-Clean-B, a small declining trend in performance is observed as the number of fragments increases. This decrease is likely attributed to a finer division of the training data among feature extractors, ultimately leading to overfitting and reduced generalization capabilities. Appendix G.2 provides further analysis of the fragment number.

**Ablation analysis on mixture of neighboring fragments.** In Table 2, we conduct an ablation analysis of the Mixture of neighboring fragments (SS 2.3). When evaluating neighborhood agreement

Figure 5: **Selection/ERR/MRAE comparison between ConFrag and strong baselines of CNLCU-H, BMM, DY-S, AUX and Selfie on IMDB-Clean-B. We exclude the performance during the warm-up.**

based solely on either the agreement of the current fragment (\(_{f}^{}\)) or the neighboring fragment's agreement (\(_{f}^{}\)), the ablation reveals that relying on the current fragment's agreement alone (\(_{f}^{}\)) exhibited relatively stronger performance. Nevertheless, this approach still fell short of achieving a satisfactory level compared to considering both agreements, as defined in Eq. 4.

Next, as we consider sample selection based on two variants of agreements, the _predictive_ one utilizing the feature extractor's binary classifier and the _representational_ one using \(K\)-nearest neighbors on the learned feature space (referred to as the selected sample sets \(^{p}\) and \(^{r}\) respectively), we conduct an ablation study on these selected sample sets. This involves evaluating the results when determining the final selected sample set (\(\)) either individually, at the intersection, or at the union of \(^{p}\) and \(^{r}\). Overall, in line with ConFrag, the union of sets (\(^{p}^{r}\)) proves to be the most effective strategy.

**Parameter size comparison.** Table 3 compares the number of parameters of ConFrag and baselines on the ResNet-based age prediction datasets. A thorough description of the ConFrag architecture is in Appendix F.3. It is worth noting that each of the ConFrag's feature extractors for noise mitigation employs a much fewer number of parameters than the downstream regression task (_e.g._, 48% in age prediction datasets). The total number of parameters of each method varies, as some share parameters for regression as well as noise mitigation while others, such as ConFrag, do not. Nevertheless, ConFrag uses fewer total parameters than CNLCU-H and RDI.

## 5 Conclusion

To address the problem of noisy labeled regression, we introduce the Contrastive Fragmentation framework (ConFrag). The framework partitions the label space and identifies the most contrasting pairs of fragments, thereby training a mixture of feature extractors over contrastive fragment pairs. This mixture is leveraged for clean selection based on neighborhood agreements. Extensive experiments on six curated datasets on three domains with different levels of symmetric and Gaussian noise demonstrate that our framework performs superior selection and ultimately leads to a better regression performance than fourteen state-of-the-art models. Given its foundation in the Mixture of Experts model, the parameter size of ConFrag linearly grows with an increase in the number of fragments. We acknowledge this as a potential avenue for future research.

    & & & & symmetric & & Gaussian \\ \(_{f}^{}\) & \(_{f}^{}\) & \(\) & 40 & 30 & 50 \\  ✓ & & \(^{p}^{r}\) & 16.97 & 17.53 & 38.18 \\  & ✓ & \(^{p}^{r}\) & 22.22 & 22.77 & 46.36 \\ ✓ & ✓ & \(^{p}\) & 14.18 & 15.84 & 33.07 \\ ✓ & ✓ & \(^{r}\) & 14.06 & 16.94 & 39.85 \\ ✓ & ✓ & \(^{p}^{r}\) & 13.08 & 16.18 & 34.23 \\ ✓ & ✓ & \(^{p}^{r}\) & 12.64 & 15.70 & 33.36 \\   

Table 2: **Ablation of Mixture of Neighboring Fragments.** MRAE on the IMDB-Clean-B dataset (lower is better).

    & regression & noise & total \\  RDI & 23.9M & 47.8M & 47.8M \\  CNLCU & 47.8M & 47.8M & 47.8M \\  “others” & 23.9M & 23.9M & 23.9M \\  ConFrag & 23.9M & 22.8M & 46.7M \\   

Table 3: **Parameter size comparison**. regression: parameters for regression, noise: parameters to mitigate noisy labels, “others”: SPR, CDR, D2L, C-Mixup, Sigua, Selfie, BMM, DY-S, Superloss.

Figure 6: **Analysis** with 40% symmetric noise. (a) Comparison between the proposed contrastive pairing and other pairings on IMDB-Clean-B. (b) Comparison between fragment numbers on SHIFT15M-B and IMDB-Clean-B.