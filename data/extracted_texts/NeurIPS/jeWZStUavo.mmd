# Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer

Ke Xue\({}^{1,2}\), Ruo-Tong Chen\({}^{1,2}\), Xi Lin\({}^{1,2}\), Yunqi Shi\({}^{1,2}\),

Shixiong Kai\({}^{3}\), Siyuan Xu\({}^{3}\), Chao Qian\({}^{*}\)\({}^{1,2}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University

\({}^{2}\)School of Artificial Intelligence, Nanjing University

\({}^{3}\)Huawei Noah's Ark Lab

Correspondence to Chao Qian <qianc@nju.edu.cn>

###### Abstract

In modern chip design, placement aims at placing millions of circuit modules, which is an essential step that significantly influences power, performance, and area (PPA) metrics. Recently, reinforcement learning (RL) has emerged as a promising technique for improving placement quality, especially macro placement. However, current RL-based placement methods suffer from long training times, low generalization ability, and inability to guarantee PPA results. A key issue lies in the problem formulation, i.e., using RL to place from scratch, which results in limits useful information and inaccurate rewards during the training process. In this work, we propose an approach that utilizes RL for the refinement stage, which allows the RL policy to learn how to adjust existing placement layouts, thereby receiving sufficient information for the policy to act and obtain relatively dense and precise rewards. Additionally, we introduce the concept of regularity during training, which is considered an important metric in the chip design industry but is often overlooked in current RL placement methods. We evaluate our approach on the ISPD 2005 and ICCAD 2015 benchmark, comparing the global half-perimeter wirelength and regularity of our proposed method against several competitive approaches. Besides, we test the PPA performance using commercial software, showing that RL as a regulator can achieve significant PPA improvements. Our RL regulator can fine-tune placements from any method and enhance their quality. Our work opens up new possibilities for the application of RL in placement, providing a more effective and efficient approach to optimizing chip design. Our code is available at https://github.com/lamda-bbo/macro-regulator.

## 1 Introduction

In the complex and evolving landscape of modern chip design, placement is a pivotal process that significantly influences the power, performance, and area (PPA) metrics of the final chip . A modern chip typically comprises thousands of macros (i.e., individual building blocks such as memories) and millions of standard cells (i.e., smaller basic components like logic gates). The macro placement result provides a fundamental solution for the subsequent processes (e.g., standard cells placement and routing), thus playing an important role . For example, macro placement influences the placement of standard cells, and poor macro placement might make it challenging to place these cells optimally, leading to an unsatisfactory chip performance . Moreover, an inappropriate macro placement can result in macro blockage in the core center, which harms the overall chip performance by causing unwanted effects such as routing congestion, inferior wirelength, and timing performance issues .

Due to the lengthy and complex workflow of chip design, designers often rely on proxy metrics that can reflect the final results to guide the optimization process [2; 30; 20]. One important proxy metric is half-perimeter wirelength (HPWL), which provides an approximation for the routing wirelength and is widely used to measure the placement quality [2; 13; 28]. Traditional macro placement methods can be divided into two categories. Earlier approaches usually solve macro placement by black-box optimization (BBO) [24; 5; 12; 29]. They often suffer from the poor scalability due to the large-scale search space and high complexity of decoding a solution to a placement. Another type is analytical method [6; 7; 19], which can solve the placement efficiently by approximating HPWL gradients. However, these methods are hard to guarantee the non-overlapping constrain between cells and are easy to be stuck in local optima [16; 34].

Reinforcement learning (RL)  has recently emerged as a promising technique to enhance the macro placement quality [23; 9; 8; 16; 15]. RL's ability to learn policies through interaction with a complex environment offers a novel pathway for addressing various challenges of macro placement. However, the application of RL is currently hindered by several limitations, including the long training time, an inability to guarantee PPA improvements, and the lack of generalization across different chip layouts. In this work, we highlight that a major contributing factor to these issues is the problem formulation, i.e., the conventional RL approach of placing macros from scratch often results in limited state information and inaccurate reward signal throughout the learning process.

To address these challenges, we propose a novel RL approach called MaskRegulate that shifts the focus from initial placement to refining existing placement layouts. The RL policy acts as a regulator rather than a placer, which operates on pre-existing placements, thus allowing for access to comprehensive state information and enabling the acquisition of more precise rewards. This adjustment enhances the efficiency of the learning process and finally improves the final placement results. Furthermore, MaskRegulate introduces the concept of regularity  as a part of input information and a critical reward signal, which has been largely overlooked in previous research despite its significance in ensuring manufacturability and performance. Previous methods often only consider the HPWL metric, suffering from optimizing different metrics effectively. By integrating regularity into the RL framework, our approach aligns more closely with advanced chip requirements.

The effectiveness of the proposed MaskRegulate is comprehensively evaluated on the ICCAD 2015 benchmark , which is is currently one of the largest open-source benchmarks that allows us to evaluate PPA metrics such as congestion and timing slack. We first compare the global

Figure 1: Placement layouts and congestions of (a) MaskPlace and (b) MaskRegulate on the superblue1 from ICCAD 2015 benchmark , where the red points indicate the congestion critical regions. (c): Comparing two crucial PPA metrics, namely Congestion and total negative slack (TNS) between MaskRegulate, DREAMPlace , AutoDMP , WireMask-EA , and MaskPlace , where lower values indicate better performance. These results are obtained using _Cadence Innovus_.

HPWL and regularity of our approach against several competitive methods. Additionally, we use the commercial electronic design automation (EDA) tool _Cadence Innovus_ to evaluate the PPA performance, demonstrating that our proposed MaskRegulate can lead to significant PPA improvements, e.g., the placement layouts and two PPA metrics on superlabel, as shown in Figure 1. Specifically, compared to MaskPlace (an advanced RL placer ; MaskRegulate shares a similar architecture to it), MaskRegulate improves 17.08% on routing wireless, 73.08% and 38.81 % on routed horizontal and vertical congestion overflow respectively, 18.35% on worst negative slack, 37.89% on total negative slack, and 46.17% on the number of violation points.

This work provides a more effective approach for macro placement of modern chips, opening new possibilities for the application of RL in chip design. The contributions of this work are highlighted in three key points:

* **Novel problem formulation**: Innovatively applying RL in the refinement stage of macro placement, which allows for more effective learning from structured state and accurate reward information, significantly enhancing the learning efficiency and effectiveness.
* **Integration of regularity**: Introducing regularity, a critical yet previously overlooked metric in chip design, into the RL training framework, which not only aligns with industry practice but also enhances the chip PPA quality.
* **Impressive PPA improvement and comprehensive analysis**: On the popular ICCAD 2015 benchmark, our proposed MaskRegulate demonstrates significant improvements in PPA metrics, showing the practical applicability and effectiveness of the RL regulator.

## 2 Background

### Placement

The circuit in the placement stage is considered as a graph where vertices model gates. The main input information is the netlist \(=(V,E)\), where \(V\) denotes the information (i.e., height and width) about all macros designated for placement on the chip, and \(E\) is a hyper-graph comprised of nets \(e_{i} E\), which encompasses multiple cells (including both macros and standard cells) and denotes their inter-connectivity in the routing stage. Given a netlist, a fixed canvas layout and a standard cell library, a placement method is expected to determine the appropriate physical locations of movable macros such that the total wirelength can be minimized. A macro placement solution \(=\{(x_{1},y_{1}),,(x_{k},y_{k})\}\) consists of the positions of all the macros \(\{v_{i}\}_{i=1}^{k}\), where \(k\) denotes the total number of macros. One popular objective of macro placement is to minimize the total HPWL of all the nets while satisfying the cell density constraint, which is formulated as,

\[_{}HPWL()=_{}_{e E}HPWL_{e}(),\ \ D(),\] (1)

where \(D\) denotes the density, \(\) is a threshold, and \(HPWL_{e}\) is the HPWL of net \(e\), which is defined as: \(HPWL_{e}()=(_{v_{i} e}x_{i}-_{v_{i} e}x_{i})+(_{v_{i}  e}y_{i}-_{v_{i} e}y_{i})\).

There are three mainstream placement methods, i.e., analytical methods, black-box optimization methods, and learning-based methods. Analytical methods  place macros and standard cells simultaneously, which can be roughly categorized into quadratic placement and nonlinear placement. Quadratic placement [11; 18] iterates between an unconstrained quadratic programming phase to minimize wirelength and a heuristic spreading phase to remove overlaps. Nonlinear placement [6; 20; 7] formulates a nonlinear optimization problem and tries to directly solve it with gradient descent methods. Generally speaking, nonlinear placement can achieve better solution quality, while quadratic placement is more efficient. Recently, there has been extensive attention on GPU-accelerated non-linear placement methods. For example, DREAMPlace [19; 17] transforms the non-linear placement problem in Eq. (1) into a neural network training problem, solves it by classical gradient descent and leverages GPU, enabling ultra-high parallelism and acceleration and producing state-of-the-art analytical placement quality.

Black-box optimization methods for placement have a long history. Earlier methods such as SP  and B\({}^{*}\)-tree  have poor scalability due to the rectangular packing formulation. Recently, some black-box optimization methods have made significant progress by changing the search space. AutoDMP improves DREAMPlace by using Bayesian optimization to explore the configuration space and shows remarkable performance on multiple benchmarks. WireMask-BBO  adopts a wire-mask-guided greedy genotype-phenotype mapping and can be equipped with any BBO algorithm, demonstrating the superior performance over other types of methods.

### RL for Macro Placement

Researchers recently leverage RL-based methods for better placement quality to meet the demands of modern chip design. GraphPlace  first models macro placement as a RL problem. It divides the chip canvas into discrete grids, with each macro assigned discrete coordinates of grids, wherein the agent decides the placement of the current macro at each step. However, no reward is given until all the macros are placed, making the reward sparse and hard to learn. DeepPR  and PRNet  incorporate macro placement, standard cells placement, and routing to achieve better performance than GraphPlace, but may violate the non-overlap constraint. To address this issue, MaskPlace  introduces a dense reward and uses a pixel-level visual representation for circuit modules, which can comprehensively capture the configurations of thousands of pins, enabling fast placement in a full action space on a large canvas size. MaskPlace has many attractive benefits that previous methods do not have, e.g., 0% overlap, dense reward, and high training efficiency. ChipFormer  incorporates an offline learning decision transformer and focuses on improving the generalizability of placer. EfficientPlace  integrates a global tree search algorithm to guide the optimization process, achieving remarkable placement quality within a short time.

However, current RL methods exhibit several shortcomings: 1) Placing from scratch provides insufficient state information and inaccurate reward signals; 2) Most methods focus on minimizing wirelength, which may bring macro blockages and thus harm the final PPA metrics. In this work, we propose a novel RL approach for macro placement: an RL policy acts as a macro regulator rather than a macro placer. Specifically, our learned RL policy is designed to adjust macros based on an existing placement result, rather than placing all macros from scratch. This approach aims to refine and optimize pre-existing layouts, addressing the limitations of traditional RL-based placement methods.

## 3 Method

We present our proposed MaskRegulate here. Section 3.1 introduces our problem formulation and policy architecture, and Section 3.2 describes how to integrate regularity into the method.

### MaskRegulate Framework

**Problem formulation of RL regulator.** In the Markov Decision Process (MDP) formulation of traditional RL placer, a macro is placed at each step [23; 9; 16; 15]. The placement order of macros is determined based on some pre-defined rules, such as the number of nets, the size of macros, and the number of connected modules that have been placed. An episode ends after all macros have been placed. Typically, the state representation includes information about the chip canvas, the macros that have already been placed, and the macro currently being placed. In GraphPlace , the reward is determined only after all macros have been placed, resulting in a sparse reward signal that complicates the training process. Recent works have introduced various methods to densify the reward signal. For instance, WireMask  provides a more continuous reward based on the macros already placed. In contrast to RL placers, our RL regulator focuses on refining an existing placement by adjusting the location of one macro at each step. Unlike the placer, which initiates the placement process from scratch, the regulator benefits from additional information when adjusting each macro. Specifically, the regulator considers not only the macros that have already been placed but also the positions of all other macros. Furthermore, it enhances accuracy by taking into account all macros, even while employing a reward function similar to WireMask.

Due to the advantages mentioned above in the MDP problem formulation, even without considering additional factors (e.g., regularity), RL regulator is able to achieve better results compared to RL placer, as shown in our experiments in Appendix B.1. Furthermore, our main experimental results demonstrate superior performance not only in proxy metrics but also in PPA metrics measured by commercial tools, as shown in Section 4.2. The regulator also exhibits better generalization abilities, as shown in Section 4.3. Intuitively, adjusting an unseen chip is easier for the regulator compared to placing macros from scratch, as the incomplete state information of placer would be even worse in the case of unseen chips, resulting in poorer performance.

**Policy architecture.** Our policy architecture is illustrated in Figure 2. The policy divides the chip canvas into several grids and utilizes visual information as inputs, converting chip information into pixel-level image masks. This approach has demonstrated superior efficiency and performance in RL placer policy learning [16; 15]. The inputs include an image of the current canvas, a PositionMask that identifies all valid positions for placing the current macro, a WireMask  that indicates the approximate wirelength change for placing the current macro at each valid position, and a RegularMask that indicates the change in regularity for placing the current macro at each valid position (which will be detailed in Section 3.2). An illustration of the PositionMask and WireMask is provided in Figure 3. To facilitate broader adjustments, the PositionMask has been modified to consider only macros that have already been adjusted; thus, grids occupied by unadjusted macros are available for placement. In our MaskRegulate, the calculation of the WireMask is based on all macros, allowing its value to either increase or decrease. These values are normalized to the range \([-1,1]\), unlike the \(\) normalization used in . Additionally, our framework introduces the RegularMask to quantify changes in regularity within the state and to encourage improvements in regularity through the reward function, as presented in Section 3.2.

### Integration of Regularity

**Why does regularity matters?** Macro placement has significant impact on subsequent chip design processes, including standard cell placement and routing. If only focusing on minimizing wirelength (which is the case for most current RL placers), certain macros may end up positioned in the middle

Figure 3: Illustration of chip canvas, PositionMask, WireMask and RegularMask. We use the left-bottom corner of the module to denotes its location.

Figure 2: Overview of MaskRegulate. MaskRegulate shares a similar architecture to MaskPlace , except for the MDP formulation and the integration of regularity in the state and reward.

of the chip canvas, resulting in macro blockages . This, in turn, leads to the division of available placement areas into separate and disconnected sub-regions. As a consequence, standard cells that are connected by the same net may be scattered across different placement sub-regions, resulting in increased overall wirelength and the potential routing challenges, which ultimately degrade the timing performance. Thus, a well-established practice among experienced engineers in macro placement is to place macros towards the peripheral regions of the chip to prevent macro blockage. In this work, we aim to integrate regularity in the learning-based placement approach to achieve placement preferences similar to those of experienced engineers.

**RegularMask.** Intuitively, macros closer to the edges tend to have lower regularity. Therefore, we propose a simple and effective way to measure regularity. On a canvas, the regularity of a grid located at \((x,y)\) is calculated as \(\{x,X_{}-x\}+\{y,Y_{}-y\}\), where \(X_{}\) and \(Y_{}\) represent the real length of the horizontal and vertical axes, respectively. Given a macro to be placed, the RegularMask measures the value change in regularity for each valid placement position, as illustrated in Figure 3(d).

**Reward and policy learning.** The reward of MaskRegulate consists of two components: \(r_{wire}\) and \(r_{reg}\), which represent the reduction of HPWL and the improvement in regularity, respectively, after refining the current macro. To mitigate the influence of scale differences on training caused by wirelength and regularity, both \(r_{wire}\) and \(r_{reg}\) are normalized to \(\). The final reward is \(r= r_{wire}+(1-) r_{reg}\), where \(\) is a trade-off coefficient. We will analyze the influence of \(\) in Section 4.4, showing that different \(\) lead to different multi-objective preferences. The detailed information are presented in Appendix A.3. MaskRegulate treats the chip canvas as a grid and divides it into \(N N\) cells, resulting in \(N^{2}\) possible discrete actions. We use the popular proximal policy optimization (PPO) algorithm  to learn the regulator policy.

## 4 Experiment

In this section, we first introduce the basic experimental settings, including the tasks and evaluation metrics in Section 4.1. Then, we try to answer the following three research questions (RQs) in Sections 4.2 to 4.4: 1) How does MaskRegulate perform compared to other methods? 2) How is the generalization ability of MaskRegulate? 3) How do the different parts of MaskRegulate affect the performance? Finally, we provide the visualization of placement results and congestion in Section 4.5.

### Experimental Settings

**Tasks.** We mainly use the ICCAD 2015 benchmark  as our test-bed, which includes sufficient advanced chip information and is currently one of the largest open-source benchmarks that allows us to evaluate congestion, timing and other PPA metrics. The benchmark statistics are listed in Table 3 in Appendix A.1. Although ICCAD 2015 is the benchmark we have found that closely reflects the current practices in the EDA industry, it still has some shortcomings. For example, it allows for a large placement area, resulting in loose placement results that do not adhere to the design principles of advanced modern chips. Note that the "A" in PPA denotes "Area", which is a core metric of chip design and should be minimized [3; 32]. Therefore, we scale down the chip's placement area, presenting further challenges for the compared methods. Besides, we also conduct experiments on ISPD 2005 benchmark , which is also a popular benchmark in AI for chip design but does not have sufficient information for PPA evaluation. Detailed results can be found in Appendix B.

**Proxy evaluation metrics.** We use the following two popular proxy metrics for a quick comparison of different algorithms: 1) Global HPWL. After determining the locations of all the macros, we use DREAMPlace  to place standard cells to obtain the global placement result, and then report the global HPWL (i.e., full HPWL involving both macros and standard cells). Compared to macro HPWL, global HPWL considers the total wirelength, typically on a scale that is two orders of magnitude larger, providing a better estimation of the final real performance of the chip. 2) Regularity: We compute the regularity values for all macros, which serve as a measurement of the overall regularity of the placement result. We run each algorithm for five times and report their mean and variance. We do not consider the rectangular uniform wire density (RUDY) metric  for congestion proxy, as this approximation is sometimes positively correlated with the HPWL metric and is not accurate . Instead, we will evaluate congestion within our PPA evaluation.

**PPA evaluation metrics.** The whole chip design process is lengthy and complex, and proxy metrics may not accurately capture the true performance of the chip. PPA metrics often require the use of commercial EDA tools to obtain precise results with expensive cost. In our experiments, we select the best placement result for PPA evaluation based on global HPWL from multiple runs. After obtaining the global placement results, we use commercial tool _Cadence Innovus_ to proceed the subsequent stages and evaluate their PPA metrics, including routed wirelength, routed vertical and horizontal congestion overflow, worst negative slack, total negative slack, and the number of violation points. These metrics are extremely important measures of chip design and are typically considered to evaluate the quality of a chip comprehensively.

### RQ1: How does MaskRegulate perform compared to other methods?

We consider the following methods to be compared: DREAMPlace : A state-of-the-art analytical placer; AutoDMP : A method that improves DREAMPlace by exploring its configuration space iteratively; WireMask-EA : A state-of-the-art black-box macro placement method with EA as the optimizer; MaskPlace : A representative online RL methods, which shares similar policy architecture, state, HPWL reward with our MaskRegulate.

For the same components, MaskPlace and MaskRegulate use the same settings, e.g., the number of grids, and the learning rate. Detailed information is provided in Appendix A.3. Additionally, in order to demonstrate that the regulator has higher training efficiency than the placer, MaskRegulate and MaskPlace are trained for 1000 and 2000 episodes, respectively. For each chip, MaskRegulate uses DREAMPlace to obtain an initial macro placement result to be adjusted, which takes within few minutes and has relatively low quality.

The overall evaluation results are shown in Table 1. MaskRegulate achieves the best average rank on both proxy and PPA metrics. DREAMPlace has the worst average ranking on wirelength, congestion, and timing. However, after adjustment by MaskRegulate, the obtained placements achieve the best average rank. Compared to MaskPlace, MaskRegulate leads to significant improvements in multiple PPA indicators: improves 17.08% on routing wirelength, 73.08% and 38.81 % on routed horizontal and vertical congestion overflow respectively, 18.35% on worst negative slack, 37.89% on total negative slack, and 46.17% on the number of violation points. By incorporating regularity, MaskRegulate achieves the highest regularity on all the eight chips. We can observe a certain correlation between the proxy metric (global HPWL) and the real metric (rWL), but there still exists a gap, indicating the challenges involved in the placement task. Furthermore, we provide detailed visualizations of placement results in Figure 5, where MaskRegulate shows significant improvements on congestion metrics. Besides, the final placement layouts of MaskRegulate are much regular than all the other methods.

### RQ2: How is the generalization ability of MaskRegulate?

The generalization ability of RL policies is an important question to be investigated. In this section, we pre-train MaskRegulate and MaskPlace on the first four chips (i.e., superblue1, superblue3, superblue4, and superblue5) and test on the remaining four chips. To further validate the ability of MaskRegulate to adjust different initial placement results, we use it to adjust the results obtained by different initial placements on the test chips.

The results are shown in Table 2. On both the global HPWL and regularity metrics, MaskRegulate consistently outperforms MaskPlace, showcasing its stronger generalization capability. An interesting finding is that MaskRegulate performs better on unseen chips than on the chips it was trained on, specifically in terms of global HPWL, such as with superblue16. This may suggest that MaskRegulate has learned some general knowledge during the pre-training process, enabling it to overcome local optima that may arise from direct learning on the target chip.

### RQ3: How do the different parts of MaskRegulate affect the performance?

We investigate the influence of different parts and provide additional analysis in this section.

**Hyperparameters sensitivity analysis: different trade-off coefficient \(\) leads to different multi-objective preferences.** One hyperparameter of RegularMask is the coefficient \(\) between HPWL reward \(r_{wire}\) and regularity reward \(r_{reg}\), where a higher \(\) indicates a preference for optimizing

    &  &  &  \\  & & Global HPWL & Regularity & rWL & rO-H & rO-V & WNS & TNS & NVP \\   & DMP & 8.96 \(\) 0.84 & 4.15 \(\) 0.04 & 154.23 & 17.15 & 4.48 & -119.616 & -2.91 & 3.35 \\  & AutoDMP & 8.13 \(\) 0.17 & 4.99 \(\) 0.08 & 185.60 & 20.99 & 5.73 & -124.572 & -3.72 & 3.46 \\  & WireMask-EA & 8.07 \(\) 0.38 & 4.41 \(\) 0.15 & 149.49 & 7.62 & 0.38 & -67.616 & -3.57 & 2.94 \\  & MaskPlace & 7.93 \(\) 0.06 & 4.40 \(\) 0.06 & 158.59 & 16.28 & 0.64 & -72.070 & -3.98 & 4.41 \\  & MaskRegulate & **5.77 \(\) 0.05** & **3.31 \(\) 0.00** & **116.11** & **1.26** & **0.11** & **-60.532** & **-1.33** & **1.06** \\   & DMP & 12.87 \(\) 1.73 & 4.43 \(\) 0.03 & 232.19 & 40.55 & 19.64 & -96.904 & -2.36 & 2.25 \\  & AutoDMP & 8.13 \(\) 0.69 & 5.49 \(\) 0.17 & 166.15 & 14.471 & 3.39 & **-76.566** & **-1.12** & 1.44 \\  & WireMask-EA & 9.37 \(\) 0.81 & 4.77 \(\) 0.23 & 167.67 & 7.81 & 0.32 & -92.566 & -1.57 & 2 \\  & MaskPlace & 8.90 \(\) 0.17 & 4.77 \(\) 0.06 & 177.25 & 9.16 & 0.64 & -111.041 & -1.77 & 2.02 \\  & MaskRegulate & **7.05 \(\) 0.03** & **3.54 \(\) 0.00** & **142.89** & **1.86** & **0.18** & -83.635 & -1.15 & **0.97** \\   & DMP & 6.81 \(\) 0.23 & 3.06 \(\) 0.01 & 132.16 & 20.62 & 4.87 & -73.192 & -1.63 & 2.42 \\  & AutoDMP & 4.57 \(\) 0.78 & 3.41 \(\) 0.06 & 82.94 & 5.43 & 0.21 & **-48.137** & **-0.64** & 1.08 \\  & WireMask-EA & 5.51 \(\) 0.07 & 3.25 \(\) 0.10 & 110.20 & 8.29 & 0.61 & -83.233 & -1.85 & 1.98 \\  & MaskPlace & 5.28 \(\) 0.03 & 3.22 \(\) 0.03 & 106.36 & 9.71 & 0.31 & -67.995 & -1.47 & 1.9 \\  & MaskRegulate & **4.15 \(\) 0.06** & **2.18 \(\) 0.02** & **81.78** & **0.29** & **0.11** & -49.071 & -0.90 & **0.88** \\   & DMP & 8.78 \(\) 1.47 & 4.84 \(\) 0.06 & 144.64 & 3.75 & 0.46 & **-58.907** & **-0.68** & 1.64 \\  & AutoDMP & 12.67 \(\) 4.09 & 5.79 \(\) 0.32 & 344.14 & 74.75 & 37.32 & -197.175 & -5.83 & 3.55 \\  & WireMask-EA & 10.23 \(\) 0.68 & 5.03 \(\) 0.15 & 189.84 & 4.06 & 0.41 & -75.115 & -1.83 & 2.18 \\  & MaskPlace & 9.81 \(\) 0.03 & 4.86 \(\) 0.04 & 196.79 & 4.79 & 0.37 & -118.122 & -2.98 & 2.62 \\  & MaskRegulate & **6.94 \(\) 0.00** & **4.23 \(\) 0.01** & **137.79** & **0.02** & **0.02** & -74.83 & -0.73 & **1.32** \\   & DMP & 22.70 \(\) 0.91 & 4.24 \(\) 0.02 & 427.71 & 100.32 & 73.18 & -123.310 & -6.55 & 7.02 \\  & AutoDMP & 10.04 \(\) 1.63 & 5.25 \(\) 0.09 & 221.69 & 6.32 & 0.64 & -52.556 & -1.82 & 4.31 \\  & WireMask-EA & 10.05 \(\) 0.34 & 4.31 \(\) 0.13 & 195.36 & 0.82 & 0.35 & -73.070 & -1.78 & 3.45 \\  & MaskPlace & 9.99 \(\) 0.05 & 4.36 \(\) 0.03 & 204.32 & 2.77 & **0.33** & -69.441 & -2.18 & 5.78 \\  & MaskRegulate & **7.90 \(\) 0.03** &HPWL, and vice versa. In this section, we investigate the influence of the trade-off coefficient \(\). We train different MaskRegulate regulators with varying \(\) values (ranging from 0.1 to 0.9) and report the proxy and PPA results in Figure 4. Due to the expensive computational cost of PPA, we select four different trade-offs of MaskRegulate for evaluation. As expected, different \(\) values lead to different multi-objective preferences. In our experiments, we use \(=0.7\) for all the chips as it achieves a relative balance between different objectives.

**Ablation studies.** We consider the following ablations of MaskRegulate. 1) Only changing the problem formulation and purely comparing placer and regulator. We implement Vanilla-MaskRegulate, where the only difference to MaskPlace is the problem formulation, and all the other components (e.g., state and reward) are the same. The results show that Vanilla-MaskRegulate consistently outperforms MaskPlace in terms of Global HPWL. 2) MaskRegulate with or without normalization. Since global HPWL has large scale than regularity, MaskRegulate w/o normalization does not prefer to consider regularity, which is not what we expect. 3) Training regularity-aware RL placer from scratch. We implement MaskPlace + RegularMask and compare it with MaskPlace and MaskRegulate. The results show the advantages of the integration of regularity (between MaskPlace and MaskPlace + RegularMask) and our RL regular formulation (between MaskPlace + RegularMask and MaskRegulate). The above ablation results demonstrate the effectiveness of each component of MaskRegulate. Detailed results and discussions are provided in Appendix B.1 due to space limitation.

### Visualizations of placement results and congestion.

We provide the detail visualizations of placement results of all the methods on all the eight chips from ICCAD 2015. As shown in Figure 5, our proposed MaskRegulate shows significant improvements on congestion metrics. Besides, the placement result of MaskRegulate is much regular than all the other methods.

### Additional results.

We conduct the following additional results to comprehensively show the effectiveness of our MaskRegulate. 1) To verify whether using a better model structure for the RL placer can compare to the regulator, we add comparison with recent proposed ChiPFormer  under a fair setting. 2) To further show the generalization ability of our methods, we conduct generalization experiments on the ISPD 2005 benchmark . 3) To investigate whether MaskRegulate can be used to adjust any initial macro placement solution, we use the pre-trained model to fine-tune other placement results.

    &  &  \\  & Global HPWL (1e8) & Regularity (1e6) & Global HPWL (1e8) & Regularity (1e6) \\  superblue7 & **7.99 \(\) 0.06** & **3.04 \(\) 0.00** & 10.33 \(\) 0.17 & 4.24 \(\) 0.06 \\ superblue10 & **11.55 \(\) 0.27** & **3.25 \(\) 0.00** & 11.88 \(\) 0.72 & 4.73 \(\) 0.05 \\ superblue16 & **5.16 \(\) 0.05** & **1.87 \(\) 0.00** & 5.97 \(\) 0.20 & 3.11 \(\) 0.03 \\ superblue18 & **3.04 \(\) 0.02** & **1.54 \(\) 0.00** & 3.69 \(\) 0.09 & 2.52 \(\) 0.03 \\   

Table 2: Generalization results of proxy metrics on the four chips of ICCAD 2015 benchmarks. The best result of each metric on each chip is **bolded**.

Figure 4: Illustration of MaskRegulate regulators with varying \(\) values (ranging from 0.1 to 0.9).

These results further demonstrate the competitive results of our proposed MaskRegulate. Detailed discussions are provided in Appendix B.2, B.3, and B.4, respectively.

## 5 Final Remarks

**Conclusion.** In this paper, we present a novel RL problem formulation for macro placement, focusing on the development of a macro regulator rather than a placer. Our proposed method, MaskRegulate, demonstrates substantial improvements in chip placement quality by refining existing layouts instead of generating them from scratch. By integrating dense reward signals and emphasizing regularity, our approach effectively addresses the limitations of traditional RL-based placement methods, resulting in superior performance in PPA metrics across various chips. This advancement paves the way for more efficient and effective chip design through RL.

**Limitations and future work.** This study has several primary limitations: it does not consider the impact of module aspect ratio and area factors on placement; it overlooks global wirelength and timing metrics during the training process; and it does not employ advanced transformer architectures  to enhance the generalization of the regulator. Chip design inherently involves different preferences, such as the need for compact size in mobile phone chips and larger sizes for computer chips. Therefore, future research should address these challenges and explore efficient methods to obtain a set of chip placements that accommodate different preferences using multi-objective optimization.

Figure 5: Placement layouts and congestions of different methods on the eight ICCAD 2015 benchmarks. The congestion results are obtained by _Cadence Innovus_, where red points indicate the congestion critical regions.