# Exploring and Interacting with the Set of Good Sparse Generalized Additive Models

Chudi Zhong\({}^{1}\)   Zhi Chen\({}^{1}\)   Jiachang Liu\({}^{1}\)   Margo Seltzer\({}^{2}\)   Cynthia Rudin\({}^{1}\)

\({}^{1}\) Duke University \({}^{2}\) The University of British Columbia

{chudi.zhong, zhi.chen1, jiachang.liu}@duke.edu

mseltzer@cs.ubc.ca, cynthia@cs.duke.edu

Equal Contribution

###### Abstract

In real applications, interaction between machine learning models and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present algorithms to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity, direct editing); and (3) investigating sudden changes in the shape functions. Experiments demonstrate the fidelity of the approximated Rashomon set and its effectiveness in solving practical challenges.

## 1 Introduction

A key ingredient for trust in machine learning models is _interpretability_; it is more difficult to trust a model whose computations we do not understand. However, building interpretable models is not easy; even if one creates a model that is sparse and monotonic in the right variables, it likely is still not what a domain expert is looking for. In fact, domain experts often cannot fully articulate the constraints their problem requires. This leads to an often painful iterative process where a machine learning algorithm, which is typically designed to produce only one model, is now asked to produce another, and possibly many more after that. Hence, the classical machine learning paradigm is broken, because it was not designed to facilitate _choice_ among models. In other words, the classical machine learning paradigm was not designed for the real-world situation in which domain experts see and understand a model, identify shortcomings in it, and change it interactively in a specific way. There is instead an _interaction bottleneck_.

We work in a new paradigm where an _algorithm produces many models from which to choose_, instead of just one . Algorithms in this new paradigm produce or approximate the _Rashomon set_ of a given function class. The Rashomon set is the set of models that are approximately as good as the best model in the class. That is, it is the set of all good models. Rashomon sets for many real problems are surprisingly large , and there are theoretical reasons why we expect many simple, good models to exist . The question we ask here is how to explicitly find the Rashomon set for the class of _sparse generalized additive models_ (sparse GAMs).

GAMs are one of the most widely used forms of interpretable predictive models [7; 8; 9] and have been applied to complex problems such as prediction of health outcomes from medical records [10; 11], where they can provide the same accuracy as the best black box models. GAMs can use sparsity regularization to generalize and to avoid constructing overly complicated models. Different regularization strategies have been used [8; 12; 13], and sparse GAMs have performance that is essentially identical to black box models on most tabular datasets . Thus, an important previously-missing piece of the Rashomon set paradigm is _how to obtain the Rashomon set of sparse GAMs_.

There are two major challenges: (1) Different from regression, the Rashomon set of GAMs in the classification setting has no analytical form; (2) The Rashomon set should lend itself easily to human-model interaction, mitigating the interaction bottleneck. To tackle these challenges, we find the maximum volume ellipsoid inscribed in the true Rashomon set. Our gradient-based optimization algorithm can find such an ellipsoid for a fixed support set. Leveraging the properties of ellipsoids, we can efficiently approximate Rashomon sets of numerous support sets that are subsets of the original support set. We show in Section 5 that this approximation captures most of the Rashomon set while including few models outside of it.

More importantly, using the maximum volume inscribed ellipsoid enables us to solve many practical challenges of GAMs through efficient convex optimization. We first use it to study the importance of variables among a set of well-performing models, called _variable importance range_. This is essentially the Model Class Reliance, which measures the variable importance of models in the Rashomon set [14; 15]; previously, ranges of variable importance have been studied only for linear models  and tree models [2; 16]. We also show how to efficiently use the ellipsoid approximation to search for near-optimal models with monotonicity constraints. The Rashomon set empowers users to be able to arbitrarily manipulate models. As users edit models, their edits are either already in the Rashomon set (which we can check easily) or they can easily be projected back into the Rashomon set using the technique in Section 4.4. Thus, being able to approximate the full Rashomon set for GAMs brings users entirely new functionality, going way beyond what one can do with just a set of diverse models [17; 18; 19; 20; 21; 22; 23]. Also, the ability to sample GAMs efficiently from the approximated Rashomon set allows us to investigate whether the variations or sudden changes observed in a single model are indicative of true patterns in the dataset or simply random fluctuations.

Since our work provides the first method for constructing Rashomon sets for sparse GAMs, there is no direct prior work; this is a novel problem. There has been prior work on related problems, such as constructing Rashomon sets for decision trees , and the Rashomon sets for linear regression are simply described by ellipsoids ; Rashomon sets for linear regression models have been used for decision making , robustness of estimation , and holistic understanding of variable importance [14; 26]. Our work allows these types of studies to generalize to GAMs.

## 2 Background

A sample is denoted as \((,y)\), where \(\) is the \(p\) dimensional feature vector and \(y\) is the target. The \(j^{th}\) dimension of the feature vector is \(x_{j}\). A generalized additive model (GAM)  has the form

\[g(E[y])=_{0}+f_{1}(x_{1})+f_{2}(x_{2})++f_{p}(x_{p})\] (1)

where \(_{0}\) is the intercept, \(f_{j}\)'s are the shape functions and \(g\) is the link function, e.g., the identity function for regression or the logistic function for classification. Each shape function \(f_{j}\) operates only on one feature \(x_{j}\), and thus the shape function can directly be plotted. This makes GAMs interpretable since the entire model can be visualized via 2D graphs. In practice, a continuous feature is usually divided into bins , thereby its shape function can be viewed as a step function, i.e.,

\[f_{j}(x_{j})=_{k=0}^{B_{j}-1}_{j,k}[b_{j,k}<x_{j} b _{j,k+1}],\] (2)

where \(\{b_{j,k}\}_{k=0}^{B_{j}}\) are the bin edges of feature \(j\), leading to \(B_{j}\) total bins. This is a linear function on the binned dataset whose features are one-hot vectors. \(=\{_{j,0},_{j,1},,_{j,B_{j}-1}\}_{j=1}^{p}\) is the weight vector. For simplicity, we use this formulation in the rest of this paper.

Given a dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\), we use the logistic loss \(_{c}(,_{0},)\) as the classification loss. To regularize the shape function, we also consider a weighted \(_{2}\) loss \(_{2}()\) on the coefficients and total number of steps in the shape functions \(_{s}()\) as penalties. Specifically,

\[_{2}():=_{j=1}^{p}_{k=0}^{B_{j}-1}_{j,k}_{ j,k}^{2},\]

where \(_{j,k}=_{i=1}^{n}[b_{j,k}<x_{ij} b_{j,k+1}]\) is the proportion of samples in bin \(k\) of feature \(j\). The weighted \(_{2}\) term not only penalizes the magnitude of the shape function, but also implicitly _centers_ the shape function by setting the population mean to 0: any other offsets would lead to suboptimal \(_{2}()\). This approach is inspired by , which explicitly sets the population mean to 0. To make the shape functions fluctuate less, we penalize the total number of steps in all shape functions, i.e.,

\[_{s}()=_{j=1}^{p}_{k=0}^{B_{j}-1}[w_{j,k} w_{j,k+1}],\]

which is similar to the \(_{0}\) penalty of . Combining classification loss and penalty yields total loss:

\[(,_{0},)=_{c}(, _{0},)+_{2}_{2}()+_{s} _{s}().\] (3)

Following the definition of , we define the Rashomon set of sparse GAMs as follows:

**Definition 2.1**.: (\(\)-Rashomon set) For a binned dataset \(\) with \(n\) samples and \(m\) binary features, where \(^{m}\) defines a generalized additive model. The \(\)-Rashomon set is a set of all \(^{m}\) with \((,_{0},)\) at most \(\):

\[R(,):=\{^{m},_{0} :(,_{0},)\}.\] (4)

We use \((,_{0})\) to represent \((,_{0},)\) and \(R()\) to represent \(R(,)\) when \(\) is clearly defined.

## 3 Approximating the Rashomon Set

### GAM with Fixed Support Set (Method 1)

Suppose we merge all adjacent bins with the same \(_{j}\). The set of bins after merging is called the support set. For a GAM with fixed support set, the \(_{s}\) term is fixed and the problem is equivalent to logistic regression with a weighted \(_{2}\) penalty. For simplicity, in what follows, \(\) also includes \(_{0}\). In this case, the loss \(()\) is a convex function, and the Rashomon set \(R()\) is a convex set. Hence, we find the maximum volume inscribed ellipsoid centered at \(_{c}\) to approximate \(R()\). Specifically, the approximated Rashomon set is:

\[:=\{^{m}:(-_{c})^{T} (-_{c}) 1\},\] (5)

where \(\) and \(_{c}\) are the parameters defining the ellipsoid that we optimize given different \(\)s.

**Gradient-based optimization**: Our goal is to maximize the volume of \(\) while guaranteeing that almost all points in \(\) are also in \(R()\). Mathematically, we define our optimization objective as

\[_{,_{c}}()^{}+C _{(,_{c})}[(()-,0)].\] (6)

The volume of \(\) is proportional to \(()^{-1/2}\), therefore we minimize \(()^{1/(2m)}\) in the objective. \(m\) is included in the exponent to normalize by the number of dimensions. In the second term of Eq (6), we draw samples from \((,_{c})\) and add a penalty if the sample is outside \(R()\); \(C\) is large to ensure this happens rarely. We use gradient descent to optimize the proposed objective function. Note that the "reparameterization trick" can be used to convert the sample \(\) drawn from \(\) into a purely stochastic part and a deterministic part related to \(\) and \(_{c}\), and calculate the gradients accordingly. Details are shown in Appendix B. To facilitate the optimization, we need a good starting point.

**Initialization:** Since the support set is fixed, we treat \(_{s}\) as a constant. We initialize \(_{c}\) using the empirical risk minimizer \(^{*}\) for \(()\) and initialize \(\) by \(/(2(-(^{*})))\), where \(\) is the Hessian. The initialization is motivated by the Taylor expansion on \(()\) at the empirical risk minimizer \(^{*}\):

\[()(^{*})+^{ T}(-^{*})+(-^{*})^{T} (-^{*})+O((-^{*})^{3}).\]Since \(^{*}\) is the empirical risk minimizer, the gradient \(=\). If we ignore the higher order terms, we get a quadratic function, i.e., \(()(^{*})+(-^{*})^{T}(-^{*})\). Plugging this quadratic function into the Rashomon set formula, we get exactly an ellipsoid, i.e.,

\[(-^{*})^{T}(-^ {*})-(^{*}).\]

Therefore, given this approximation, we initialize \(_{c}\) with \(^{*}\) and \(\) with \(/(2(-(^{*})))\), and then optimize through gradient descent.

### GAMs with Different Support Sets (Method 2)

Method 1 can accurately approximate the Rashomon set given a fixed support set. However, if we want the Rashomon set for many different support sets, applying Method 1 repeatedly can be time-consuming. We next present a blocking method, Method 2, to efficiently approximate the Rashomon sets with many smaller support sets after getting an approximated Rashomon set for a large support set by Method 1. Approximating the Rashomon set with an ellipsoid for a large support can be helpful in approximating the Rashomon sets with smaller support sets. Specifically, if the bins of the smaller support set are generated by merging bins in the original support set, it is equivalent to forcing the coefficients of these merged bins to be the same. Formally, this is a hyperplane \(P\) in the solution space defined by a set of linear constraints

\[P:=\{^{m}:_{k}=_{k+1}==_{ }\},\] (7)

where the \(k^{th}\) to the \(^{th}\) bins in the original support set are merged to create a single bin in the new support set. The hyperplane's intersection with any ellipsoid, e.g., \(\), is still an ellipsoid, which can be calculated analytically. Specifically, if we rewrite \(\) in the form of block matrices, i.e.,

\[=[_{1:k-1,1:k-1}&_{1 :k-1,k:}&_{1:k-1,+1:m}\\ _{1:k-1,k:}^{T}&_{k:,k:}&_{+1:m,k:}^{T}\\ _{1:k-1,+1:m}^{T}&_{+1:m,k:}& _{+1:m,+1:m}\\ ],\]

we obtain the quadratic matrix \(}\) for the new support set by summing up the \(k^{th}\) to \(^{th}\) rows and columns, i.e.,

\[}=[_{1:k-1,1:k-1}& _{1:k-1}&_{1:k-1,+1:m}\\ _{1:k-1}^{T}&q_{}&_{k+1:m}^{T}\\ _{1:k-1,+1:m}^{T}&_{k+1:m}&_{+1 :m,+1:m}\\ ],\]

where \(q_{}=_{i=k}^{}_{j=k}^{}Q_{i,j}\), \(_{1:k-1}\) and \(_{+1:m}\) are two column vectors obtained by summing all columns of \(_{1:k-1,k:}\) and \(_{+1:m,k:}\). The linear term (coefficients on the \(\) term) of the original support set is \(:=_{c}\). Similarly, we can obtain the linear term of the new support set \(}\) by summing up the \(k^{th}\) to \(^{th}\) elements of the \(\) vector, i.e., \(}=[_{1},,_{k-1},_{i=k}^{}_{i}, _{+1},,_{m}]^{T}\). With this, we can calculate the new ellipsoid's center, \(}_{c}=}^{-1}}\), and the upper bound of the new ellipsoid \(u=1-_{c}^{T}_{c}+}_{c}^{T} }}_{c}\). This new ellipsoid \(R_{P}:=P\), which satisfies both Equations 5 and 7, is defined as

\[R_{P}:=\{^{m}:(-}_{c})^{T} }(-}_{c}) u\},\]

where if \(u 0\), \(R_{P}\) is empty. If we want to merge multiple sets of bins, the calculations can be slightly modified to summing up sets of rows and columns separately corresponding to the sets of bins to be merged. \(R_{P}\) serves as a simple approximation of the Rashomon set for the smaller support set. Since essentially all of the original ellipsoid \(\) is in the Rashomon set, and \(R_{P}\) is a subset of \(\), every \(\) in \(R_{P}\) is also in the Rashomon set. In fact, since the support set gets smaller, \(_{s}\) becomes smaller too, and the loss is even lower, i.e., for \(\) in \(R_{P}\), \(()-_{0}(-k)\).

**Explore different support sets with size \(\)**: Using the blocking method mentioned above, we can approximate the Rashomon set for many different support sets that are subsets of the original support set. The approximation of the ellipsoid for a smaller support set is more accurate when the size difference between the original support set and the new support set is small. Therefore, instead of using a very large support set at the beginning, we use a sparse GAM algorithm such as FastSparse  with a relatively weak sparsity penalty to get a support set \(S\) whose size \(K\) is moderately largerthan the size of support sets we want to explore, but covers all bins we want to potentially merge to form a new support set. For simplicity of defining the loss threshold of the Rashomon set, here we consider only support sets with size \(\). If we want to explore different \(\), we can repeat this process. Suppose our goal is to explore the Rashomon set with loss threshold \(\). The first step is to use the methods in Section 3.1 to obtain the ellipsoid \(\) approximating the \(\)-Rashomon set on the original support set, where \(=+_{0}(K-)\). Then we can enumerate all possible ways (\(}\) ways in total) to merge bins in the original support set to get any subset \(\) whose size is \(\), and calculate the intersection (i.e., ellipsoid) \(R_{P_{}}:=P_{}\) between \(\) and the hyperplane \(P_{}\) corresponding to merging \(S\) into \(\). All nonempty \(R_{P_{}}\) are valid approximations of the Rashomon set.

## 4 Applications of the Rashomon Set

We now show four practical challenges related to GAMs that practitioners can now solve easily using the approximated Rashomon sets.

### Variable Importance within the Model Class

Variable importance can be easily measured for GAMs. For example, Explainable Boosting Machines [8; 27] measures the importance of each variable by the weighted mean absolute coefficient value of bins corresponding to this variable, i.e., \(VI(_{,j})=_{k=0}^{B_{j}-1}_{j,k}|w_{j,k}|\). This measure is based on one model, but given the Rashomon set, we can now provide a more holistic view of how important a variable is by calculating the range of variable importance among many well-performing models; this is called the Model Class Reliance . Specifically, \(VI_{-}\) and \(VI_{+}\) are the lower and upper bounds of this range, respectively. For example, the range of importance of feature \(j\) is defined as

\[[VI_{-}(_{,j}),VI_{+}(_{,j})]=[_{ R()}VI(_{,j}),_{ R()}VI( _{,j})].\] (8)

A feature with a large \(VI_{-}\) is important in all well-performing models; a feature with a small \(VI_{+}\) is unimportant to every well-performing model. We use \(\) to approximate \(R()\) and estimate \(VI_{-}\) by solving a linear programming problem with a quadratic constraint, and we estimate \(VI_{+}\) by solving a mixed-integer programming problem. Solving these problems gives a comprehensive view of variable importance for the _problem_, not just a single model.

**Lower bound of variable importance**: \(VI_{-}\) of feature \(j\) can be obtained by solving the following linear programming problem.

\[_{[_{j,0},...,_{j,B_{j}-1}]}_{k=0}^{B_{j}-1}_{j,k}| _{j,k}|\ \ \ \ \ (-_{c})^{T}(-_{c}) 1.\] (9)

Since \(_{j,k} 0\), Problem (9) can be solved using an LP solver. The objective minimizes variable importance, and the constraint ensures the solution is in the Rashomon set.

**Upper bound of variable importance**: The maximization problem cannot be solved through linear programming since \([_{j,0},...,_{j,B_{j}-1}]\) can be arbitrarily large. Instead, we formulate it as a mixed-integer program. Let \(\) be a relatively large number (e.g., 200 is large enough for real applications (see Appendix E)) and let \(I_{k}\) be a binary variable.

\[_{[^{}_{j,0},...,^{}_{j,B_{j}-1},I_{1 },...,I_{B_{j}-1}]}_{k=0}^{B_{j}-1}_{j,k}^{}_{j,k}\] (10) \[\ \ (-_{c})^{T}(-_{c}) 1\] \[_{j,k}+M I_{k}^{}_{j,k},\ \ -_{j,k}+M(1-I_{k})^{}_{j,k},\  k\{0,...,B_{j}-1\}\] \[_{j,k}^{}_{j,k},\ \ -_{j,k}^{ }_{j,k},\  k\{0,...,B_{j}-1\}.\]

The last two constraints ensure that \(^{}_{j,k}\) is defined as the absolute value of \(_{j,k}\). However, solving mixed-integer programs is usually time-consuming. We propose another way to solve the maximization problem. Since \(_{j,k}\) can be either positive or negative, we enumerate all positive-negative combinations for \(_{j,k},k\{0,...,B_{j}-1\}\), solve the LP problem with the sign constraint enforced, and choose the maximum value (see Algorithm 2 in the Appendix E).

### Monotonic Constraints

A use-case for the GAM Rashomon set is finding an accurate model that satisfies monotonicity constraints. This can be obtained by solving a quadratic programming problem. For example, if a user wants \(f_{j}(_{j})\) to be monotonically increasing, the optimization problem can be formalized as:

\[_{}(-_{c})^{T}(-_{c})\ \ \ \ \ _{j,k}_{j,k+1},k\{0,...,B_{j}-1\},\] (11)

where we check that the solution is \( 1\); i.e., that solution \(\) is in the approximated Rashomon set.

### Find robustly occurring sudden jumps in the shape functions

Sudden changes (e.g., a jump or a spike) in the shape functions of GAMs are known to be useful signals for knowledge discovery [10; 11] and debugging the dataset . However, due to the existence of many almost equally good models that have different shape functions, it is hard to identify if a change is a true pattern in the dataset or just a random artifact of model fitting. However, with the approximated Rashomon set, we can calculate the proportion of models that have such a change among all near-optimal models and visualize the shape functions (see Section 5.4).

### Find the model within the Rashomon set closest to the shape function requested by users

As discussed, the main benefit of the Rashomon set is to provide users a choice between equally-good models. After seeing a GAM's shape functions, the user might want to make edits directly, but their edits can produce a model outside of the Rashomon set. We thus provide a formulation that projects back into the Rashomon set, producing a model within the Rashomon set that follows the users' preferences as closely as possible. We find this model by solving a quadratic programming problem with a quadratic constraint. Let \(_{req}\) be the coefficient vector that the user requests. The problem can be formulated as follows:

\[_{}\|-_{req}\|_{2}^{2}\ \ \ \ \ (-_{c})^{T}(-_{c})  1.\] (12)

Problem (12) can be easily solved through a quadratic programming solver.

## 5 Experiments

Our evaluation answers the following: 1. How well does our method approximate the Rashomon set? 2. How does the approximated Rashomon set help us understand the range of variable importance in real datasets? 3. How well does the approximated Rashomon set deliver a model that satisfies users' requirements? 4. How does the Rashomon set help us investigate changes in the shape function?

We use four datasets: a recidivism dataset (COMPAS) , the Fair Isaac (FICO) credit risk dataset , a Diabetes dataset , and an ICU dataset MIMIC-II .

### Precision and volume of the approximated Rashomon set

As we mentioned in Section 3, our goal is to create an approximation of the Rashomon set whose volume is as large as possible, while ensuring that most points in the approximated Rashomon set are within the true Rashomon set. To measure precision of our approximation, models within and outside the true Rashomon set \(R()\) are considered as "positive" and "negative" respectively; models within and outside the approximated Rashomon set \(\) are considered as "predicted positive/negative." Accordingly, the precision is defined as the proportion of points \((,_{0})\), equally weighted and within \(\), having \((,_{0})\), i.e., they are in \(R()\). The recall is defined as the proportion of points in \(R()\) that are also in \(\). Note that \(=( R())}{(R())}= ()}{(R())}\). Since \((R())\) is a constant, given the same volume \(()\), recall is proportional to precision.

We will develop some natural baselines to compare to. For fair comparison, we rescale the Rashomon set approximated by baselines to have the same volume as our \(\). Then, we sample 10,000 points from our \(\) and the rescaled baseline Rashomon sets, calculate their loss, and estimate precision.

One baseline is using our Hessian initialization of \(\), i.e., \(/(2(-(^{*})))\), denoted as "hessian" in our comparison. Another baseline is to use the identity matrix to approximate \(\), i.e., the approximated set is a sphere. Another type of baseline comes from fitting GAMs on bootstrap sampled subsets of data. For a fixed support set, we calculate coefficient vectors from two different methods (logistic regression and EBMs ) on many subsets of data. This approach samples only a finite set of coefficient vectors rather than producing a closed infinite set such as an ellipsoid, and therefore its volume is not measurable. Thus, we use optimization to find the minimum volume ellipsoid that covers most coefficient vectors. This is the MVEE problem ; more details are provided in Appendix C. Sampling from the posterior distribution of a Bayesian model is also a way to get many different GAMs. We tried Bayesian logistic regression (using Hamiltonian Monte Carlo), but it is too slow (\(\)6 hours) to converge and produce enough samples to construct the minimum volume ellipsoid. So we did not include this baseline.

Figure 1 compares the precision and volume of our Rashomon set with baselines. Figure 0(a) shows that when the volume of the approximated Rashomon sets are the same between methods, our Rashomon set has better precision than the baselines. In other words, the Rashomon set approximated by our methods has the largest intersection with the true Rashomon set. The results for the hessian is worse than ours but better than other baselines, which means our proposed initialization is already better than other baselines. Also, as \(\) becomes larger, the hessian method becomes better and sometimes close to the result after optimization. Logistic regression and EBM with bootstrapping do not achieve good precision, because coefficient vectors trained by these methods on many subsets of data can be too similar to each other, leading to ellipsoids with defective shapes that do not match the true Rashomon set boundary even after rescaling.

In general, shrinking the approximated Rashomon set leads to higher precision and expanding it leads to better recall. Figure 0(b) shows the performance of different methods under this tradeoff. Our method still dominates others even when we rescaled the volume with different factors.

The previous experiments evaluate the performance when the support set is fixed. To explore many different support sets, we use the blocking method (Method 2) from Section 3.2. Method 2 is much faster than Method 1, so we need to check that it performs similarly, according to

Figure 1: Precision versus the size of the approximated Rashomon set.

both the volume and precision of the approximated Rashomon set. The _volume ratio_, defined by \([k]{(_{1})/(_{2})}\), should be 1 if the two methods perform similarly. The _precision ratio_, the precision of Method 1 over the precision of Method 2, again should be close to 1 if the methods perform similarly.

Figure 2 shows the precision ratio and volume ratio of 100 different new support sets where \(u>0\), i.e., the approximated Rashomon set is non-empty. Clearly, Method 2 will break down if we choose the support size \(\) to be too small. The results from Figure 2 indicate what we expected: precision and volume ratios are close to 1, even for relatively small \(\). Importantly, Method 2 is much more efficient than Method 1. For example, the running time is \(<\)0.001 seconds for Method 2 but ranges from 300 to 1100 seconds for Method 1 on all three datasets (COMPAS, FICO, MIMIC-II). More results are in Appendix C.

### Variable importance range

We solve Problem (9) and (10) to bound the variable importance; results on the Diabetes dataset are shown by dark blue segments in Figure 2(a). The light blue segments show the range with an additional constraint that coefficients of features other than the one we are interested in are fixed. Adding this extra constraint leads to lower \(VI_{+}\) but has negligible impacts on \(VI_{-}\). The "Glucose" feature has dominant \(VI_{-}\) and \(VI_{+}\) compared with other features, indicating that for all well-performing GAMs, this feature is the most important. In fact, the large \(VI_{-}\) indicates that Glucose remains important even for the model that relies least on it in the Rashomon set. This makes sense, because an oral glucose tolerance test indicates diabetes if the plasma glucose concentration is greater than two hours into the test. Features "BMI" and "Age" also have high \(VI_{-}\), indicating both of them are more important than other features.

Figure 3: (a) Variable importance range of the Diabetes dataset and (b) shape functions of “Glucose.” “Not fix”: the shape function is obtained by solving Problem (10). “Fix”: coefficients of all other features except “Glucose” are set to the same values as in \(_{c}\). (\(_{s}=0.001,_{2}=0.001,=1.01^{*}\))

Figure 2: Box plots of the volume ratio and precision ratio of Method 1 over Method 2 across different \(\), for two datasets. It shows the precision and volume ratios are close to 1, indicating that Method 2 performs similarly to Method 1.

Figure 2(b) shows shape functions of \(_{c}\) and the coefficient vectors obtained when the lowest and highest variable importance are achieved for "Glucose." All these shape functions follow the same almost-monotonic trend, which means the higher "Glucose" level, the more likely it is that a patient has diabetes. The variable tends to have higher variable importance when it has a larger range in the predicted logit. Shape functions based on \(_{VI_{+}}\) without fixing other coefficients can deviate more from \(_{c}\); when we do not fix other coefficients, there is more flexibility in the shape functions.

### User preferred shape functions

A significant advantage of the Rashomon set is that it provides the opportunity for users to contribute their domain knowledge flexibly without retraining the model. Note that all models within the approximated Rashomon set predict equally well out of sample (see Table 7 in Appendix), so the user can choose any of them. We use the "BloodPressure" feature in the Diabetes dataset as an example. Figure 4 shows the shape function of "BloodPressure" (in gray), in which a jump occurs when blood pressure is around 60. A potential user request might be to make the shape function monotonically decreasing. By solving Problem (11) with the constraint on coefficients related to "BloodPressure," we get the shape function colored in yellow in Figure 3(a). The jump is reduced by dragging up the left step. Suppose a user prefers to remove the jump by elevating the left step (see inset plot in Figure 3(b)). By solving Problem (12), we find that the specified shape function is not within the Rashomon set, and we find the closest solution in green, which still has a small jump at 60. Another user might prefer to remove the jump by forcing both left and right steps at the same coefficients (see inset plot in Figure 3(c)). This specified shape function is also outside the Rashomon set, and the closest solution keeps the most steps as required with a small step up. Different user-specified shape functions lead to different solutions. The approximated Rashomon set can thus provide a computationally efficient way for users to find models that agree with both their expectations and the data. Our work could be integrated with tools such as GAMchanger  which allows users to directly edit a GAM, but there is no guarantee that the resulting model still performs well.

Figure 4: Example shape functions of “BloodPressure” that satisfy users requirement. (a) The shape function of “BloodPressure” is desired to be monotonically decreasing. But it has a jump at BloodPressure \(\)60. The optimization time to find the monotonically decreasing shape function is 0.04 seconds. (b) Remove the jump by connecting to the right step. The optimization problem is solved in 0.0024 seconds. (c) Removes the jump by connecting to the left step. The time used to get the model is 0.0022 seconds. (\(_{0}=0.001,_{2}=0.001,=1.01^{*}\)).

Figure 5: Different shape functions (in red) of “PFratio” in MIMIC-II sampled from \(\).

### Changes in shape functions

When we see a jump in the shape function, we might want to know whether this jump often exists in other well-performing models. Using the approximated Rashomon set, we can give an answer. For example, the shape function of "PFratio" in the MIMIC-II dataset has a sudden dip around 330 (see the gray curve in Figure 5). The jump is so deep that we might wonder if such a jump is present in other well-performing GAMs. We sample 10,000 points from our \(\) and find that 7012 samples have a downward jump at the same position. We are surprised to find that in some of the remaining 2988 samples, the jump is instead upward. Figure 5 shows four different shape functions of "PFratio" sampled from \(\). The dramatic magnitude change in either direction could be caused by the small number of data points falling in this bin (\(_{}=1.24\)). Since the weight is so small, even a tremendous change leads to only a small impact on the loss. In practice, using our proposed optimization methods in Section 4, users can reduce or amplify the jump in the shape function or even create a different curve, as in Figure 4. Note that our ability to find a diverse set of shape functions illustrates an advantage of the Rashomon set that cannot be easily seen with other approaches.

## 6 Conclusion and Limitations

Our work approximates the Rashomon set of GAMs in a user-friendly way. It enables users to explore, visualize, modify, and gain insight from GAM shape functions. Our work represents a paradigm shift in machine learning, enabling unprecedented flexibility for domain experts to interact with models without compromising performance. Our contributions open a door for enhancing human-model interaction through using the new toolkit provided by the Rashomon set.

One limitation that could be creatively considered in future work is that there are numerous possible support sets, leading to the question of how a human might comprehend them. A carefully-designed interactive visual display might be considered for this task.

## Code Availability

Implementations of our methods is available at https://github.com/chudizhong/GAMsRashomonSet.