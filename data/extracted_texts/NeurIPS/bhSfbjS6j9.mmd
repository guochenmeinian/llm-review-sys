# Multistable Shape from Shading

Emerges from Patch Diffusion

 Xinran Nicole Han

Harvard University

xinranhan@g.harvard.edu &Todd Zickler

Harvard University

zickler@seas.harvard.edu &Ko Nishino

Kyoto University

kon@i.kyoto-u.ac.jp

###### Abstract

Models for inferring monocular shape of surfaces with diffuse reflection--shape from shading--ought to produce distributions of outputs, because there are fundamental mathematical ambiguities of both continuous (e.g., bas-relief) and discrete (e.g., convex/concave) types that are also experienced by humans. Yet, the outputs of current models are limited to point estimates or tight distributions around single modes, which prevent them from capturing these effects. We introduce a model that reconstructs a multimodal distribution of shapes from a single shading image, which aligns with the human experience of multistable perception. We train a small denoising diffusion process to generate surface normal fields from \(16 16\) patches of synthetic images of everyday 3D objects. We deploy this model patch-wise at multiple scales, with guidance from inter-patch shape consistency constraints. Despite its relatively small parameter count and predominantly bottom-up structure, we show that multistable shape explanations emerge from this model for ambiguous test images that humans experience as being multistable. At the same time, the model produces veridical shape estimates for object-like images that include distinctive occluding contours and appear less ambiguous. This may inspire new architectures for stochastic 3D shape perception that are more efficient and better aligned with human experience.

## 1 Introduction

From diaroscuro in Renaissance paintings to the interplay of light and dark in Ansel Adams' photographs, humans are masters at perceiving three-dimensional shape from variations of image intensity--shading--from a single image alone. Even though our visual experience is dominated by everyday objects, our perception of shape from shading generalizes to many synthetic "non-ecological" images invented by vision scientists. Some of these images have ambiguous (e.g., convex/concave) interpretations and lead to multistable perceptions, where one's impression of 3D shape alternates between two or more competing explanations. Figure 1 shows an example adapted from , which is alternately interpreted as an indentation or a protrusion. Both explanations are physically correct because the same image can be generated from either shape under different lighting conditions.

How can a computational model acquire this human ability to capture multiple underlying shape explanations? An algorithmic suggestion comes from Marr's principle of least commitment , which requires not doing anything that may later have to be undone. But this is in contrast to many computer vision methods for shape from shading, including SIRFS  and recent neural feed-forward models , which are deterministic and produce a single, best estimate of shape. These types of models commit to one explanation based on priors that are either designed or learned from a dataset, and they cannot express multiple interpretations of an ambiguous image. They are unlikely to be good models for the mechanisms that underlie multistable perception.

Instead, we approach monocular shape inference as a conditional generative process, and inspired by a long history of shape from shading with local patches [18; 25; 9; 29; 54; 20; 17], we present a bottom-up, patch-based diffusion model that can mimic multistable perception for ambiguous images of diffuse shading. Notably, our model is trained using images of familiar object-like shapes and has no prior experience with the ambiguous images that we use for testing. It is built on a small conditional diffusion process that is pre-trained to predict surface normals from \(16 16\) image patches. When we apply this patch process at multiple scales with inter-patch shape consistency constraints, and when we coordinate the sampling across patches, the model ends up capturing global ambiguities that are very similar to those experienced by humans.

An important attribute of our model is the way it handles lighting. It builds on the mathematical observation that shape perception can precede lighting inference . It also adheres to the philosophy that inferred lighting cannot, and should not, be precise because of spatially-varying effects like global illumination . Our model achieves these aims by guiding its diffusion sampling process with a very weak constraint on lighting consistency, where each patch nominates a dominant light direction and then all patches enact their own concave/convex flips in response to those nominations.

Another critical aspect of our model is a diffusion sampling process that is coordinated across multiple scales. It involves spatially resampling the normal predictions at intermediate diffusion time steps and then adding noise before resuming the diffusion at the resampled resolutions. Our approach is inspired by previous work that uses a "V-cycle" (fine-coarse-fine) to avoid undesirable local extrema during MRF optimization . Our ablation experiments show that multi-scale sampling is crucial for finding good shape explanations that are globally consistent.

We train our patch diffusion model on images of objects like those in Fig. 1(a), and we find experimentally that it can generalize to new objects as well as to images like Fig. 1 which are quite different from the training set and appear multitable to humans. This is in contrast to previous diffusion-based monocular shape models [27; 33] which cannot capture multistability and produce output that is much less diverse. Our model is also extremely efficient, only requiring a small pixel-based diffusion UNet that operates on \(16 16\) patches. Our total model weights require only 10MB of storage, much less than the 2-3GB required by some of the previous (and more general) models we compare to.

## 2 Background

### Ambiguities in Shape from Shading

Shape from shading is a classic reconstruction problem in computer vision. Since being formulated by Horn in the 1970s  there have been many approaches to tackle it, often by assuming diffuse Lambertian shading and uniform lighting from either a single direction (e.g.,[43; 16]) or as low-order spherical harmonics (e.g., [4; 59]). Almost all methods either require the lighting to be known (e.g., for natural illumination ) or try to estimate it precisely via inverse rendering during the optimization process [54; 59; 53], and many approaches rely on a set of priors to constrain the possible

Figure 1: Many shapes (left) can explain the same image (middle) under different lighting, including flattened and tilted versions and convex/concave flips. The concave/convex flip in this example is also perceived by humans, often aided by rotating the image clockwise by 90 degrees. Previous methods for inferring either surface normals (SIRFS , Dereneder3D , Wonder3D ) or depth (Marigold , Depth Anything ) produce a single shape estimate or a unimodal distribution. Ours produces a multimodal distribution that matches the perceived flip. (Image adapted from .)

search space. For instance, SIRFS  uses different lighting priors for natural versus laboratory conditions and a surface normal prior along occluding contours. Recent deep learning approaches like Derender3D  have demonstrated impressive results without being limited to Lambertian reflectance, but they similarly rely on priors internalized from their training sets and have trouble generalizing to new conditions.

The main challenge of shape from diffuse shading comes from the many levels of inherent ambiguity. At a single Lambertian point, when lighting is unknown, there is a multi-dimensional manifold of surface orientations and curvatures that are consistent with the spatial derivatives of intensity at the point . Even when lighting and surface albedo are known at a point, there is a cone of possible normal directions. At the level of a quadratic surface patch, when lighting is unknown, there is a discrete four-way ambiguity corresponding to convex, concave, and saddle shapes . At a global level, when lighting and surface albedo are known, ambiguities arise from interpreting the Lambertian shading equation as a PDE (e.g., ) or as a system of polynomial equations . And when lighting and albedo are unknown, there is an additional three-parameter global ambiguity that corresponds to flattenings and tiltings of the global shape . Finally, when lighting is unknown, a global shape has a discrete counterpart that corresponds to a global convex/concave flip.

It is important to note that all of these mathematical ambiguities are based on certain idealized models for the image formation process, such as exact Lambertian shading, perfectly uniform albedo, and most commonly, perfectly uniform lighting that ignores global illumination effects such as interreflections and ambient occlusion, which in reality have substantial effects . An advantage of a stochastic, learning-based approach, like the one presented here, is the potential to capture all of these ambiguities as well as others that have not yet been discovered or characterized.

### Denoising Diffusion with Guidance

Diffusion probabilistic models (DDPM) generate data by iteratively denoising samples from a Gaussian (or other) pre-determined distribution. We build on a conditional denoising diffusion model, where the condition \(c\) is a grayscale image patch, and the model is designed to approximate the distribution \(q(x_{0}|c)\) on 3-channel normal maps \(x_{0}\) with a tractable model distribution \(p_{}(x_{0}|c)\). A 'forward process' adds Gaussian noise to a clean input \(x_{0}\) and is modeled as a Markov chain with Gaussian transitions for timesteps \(t=0,1,,T\). Each step in the forward process adds noise according to \(q(x_{t}|x_{t-1},c):=(x_{t-1}},_{t})\), where \(\{_{t}\}\) is a predetermined noise variance schedule. The intermediate noisy input \(x_{t}\) can be written as

\[x_{t}=}x_{0}+}, (0,),_{t}:=_{s=1}^{t}(1-_{s})\,.\] (1)

The'reverse process' \(q(x_{t-1}|x_{t},c)\) is modeled by a learned Gaussian transition \(p_{}(x_{t-1}|x_{t},c):=(x_{t-1};_{}(x_{t},t;c), _{t}^{2})\). The mean value \(_{}(x_{t},t;c)\) can be expressed as a combination of the noisy image \(x_{t}\) and a noise prediction \(_{}(x_{t},t;c)\) from a learned model. The noise prediction model

Figure 2: Training patches are cropped from synthetic images of ordinary diffuse objects, and during training, a small diffusion model learns to denoise the normal field \(x_{0}^{u}\) for patch \(u\) from a random sample \(x_{I}^{u}\) conditioned on the patch intensities \(c^{u}\). During inference, the model is applied in parallel to non-overlapping patches, with guidance from inter-patch shape-consistency constraints to minimize the curvature smoothness loss \(_{S}\) and integrability loss \(_{I}\).

\(\) can be trained by minimizing the prediction error

\[L():=_{x_{0},(0,)}||- _{}(x_{t},t;c)||_{2}^{2}],\] (2)

as shown in . To sample noiseless data using the learned model, we start from an initial random Gaussian noise seed and use the learned denoiser \(_{}\) to compute \(x_{t-1}=}}(x_{t}-}{ }}_{}(x_{t},t;c))+_{t}z\) iteratively, where \(z(0,)\), which is a stochastic process.

The denoising diffusion implicit model (DDIM) shows that the reverse procedure can be made _deterministic_ by modeling it as a non-Markovian process with the same forward marginals . This approach helps to accelerate the sampling process by using fewer steps and also provides an estimate of the predicted \(_{0}\) at each timestep with \(x_{t}\). Each denoising step combines noise and a _foreseen_ denoised version

\[x_{t-1}=}f_{}(x_{t},t;c)+} _{}(x_{t},t;c)\,,\] (3)

where

\[f_{}(x_{t},t;c)=_{0}(x_{t})=-}_{}(x_{t},t;c)}{}}\] (4)

is the predicted \(_{0}\) at reverse sampling step \(t\).

The DDIM formulation provides a way to 'guide' the process of sampling \(x_{t-1}\) from \(x_{t}\) by applying additional constraints or losses to the predicted \(_{0}(x_{t})\) at intermediate sampling steps. Previous work has used similar approaches to solve inverse problems [8; 47] or to combine outputs from multiple diffusion models for improved perceptual similarity . Guided denoising is achieved with

\[x_{t}^{}=x_{t}-_{t}_{x_{t}}(_ {0}(x_{t}))\,,\] (5) \[x_{t-1}=}_{0}(x_{t}^{})+}_{}(x_{t},t;c)\,,\] (6)

in each sampling subroutine from time \(t\) to \(t-1\), where \(_{t}\) is the (possibly time-dependent) step size of the guided gradient update. The first step (5) can be repeated multiple times before applying the denoising step (6).

## 3 Multiscale Patch Diffusion with Guidance

Consider a surface represented by a differentiable height function \(h(x,y)\) over image domain \((x,y)\) viewed by parallel projection from above. The image plane is sampled on a square grid (pixels). Image patches have size \(d d\) and are indexed by \(u\), and we denote them as \(c^{u}^{d d}\).

Training occurs on patches extracted from images of everyday objects, as depicted in Fig. 1(a). For each image patch \(c^{u}\) there is a corresponding patch normal field \(x_{0}^{u}[-1,1]^{3 d d}\), whose \((i,j)\)th spatial element represents a surface normal vector via

\[^{u}(i,j)}{\|x_{0}^{u}(i,j)\|}=,-q_{i,j},1)}{\|(-p _{i,j},-q_{i,j},1)\|}=n_{i,j}^{2},\] (7)

where \((p,q)=( h/ x, h/ y)\) are the surface derivatives. Training proceeds as described in Sec. 2.2, with a dataset of patch tuples \((c^{u},x_{0}^{u})\) and a UNet \(_{}(x_{t}^{u},t;c^{u})\) similar to that in  whose four-channel input is the concatenation \(c^{u} x_{t}^{u}\). (Model and training details are in Appendix A.2.)

As depicted in Fig. 1(b), inference occurs over images \(c\) of size \(H W\) which are divided into their collections of non-overlapping patches \(c^{u}\). We assume that the underlying global normal field \(x_{0}[-1,1]^{3 H W}\) is continuous including at most locations on the seams between the patch normal fields \(x_{0}^{u}\). We formulate the prediction of global field \(x_{0}\) as reverse conditional diffusion on an undirected, four-connected graph \((,)\). Each patch \(x_{0}^{u},u\) is a node, and there are edges \(\{u,v\}\) between pairs of patches that are horizontally or vertically adjacent.

To encourage the patch fields to form a globally coherent prediction, we use guidance as described by Eqs. 5 and 6. Our guidance includes two terms:

\[(_{0})=|}_{\{u,v\} }_{S}(_{0}^{u},_{0}^{v})+|}_{v}_{I}(_{0}^{v})\,,\] (8)where \(_{I}\) is a within-patch continuity term that encourages integrability of the normal fields over small pixel loops, and \(_{S}\) is an inter-patch spatial consistency term that encourages constant curvature across the seams between patches. Hyperparameter \((0,1]\) controls the relative weighting of the two terms, and \(_{i} 0\) in Eq. 5 determines the overall guidance strength.

The **integrability** term follows Horn and Brooks  by penalizing deviation from a discrete approximation to the integrability of surface normals, i.e., \( p/ y= q/ x\), over \(2 2\) loops of pixels. We write this as

\[_{I}(_{0}^{u})=_{i,j}(p_{i,j+1}-p_{i+1,j+1}+p_{i,j}-p_{i +1,j}+q_{i,j+1}+q_{i+1,j+1}-q_{i,j}-q_{i+1,j})^{2},\] (9)

where the summation is over the \(i,j\) grid-indexed pixels in patch \(u\), and \(p,q\) are the components of \(_{0}^{u}\) implied by Eq. 7.

The **spatial consistency** term penalizes deviation from constant surface curvature across each seam \(\{u,v\}\) in the direction perpendicular to the seam. Consider four consecutive normals in the perpendicular direction \(n_{1},n_{2},m_{1},m_{2}\) where \(n_{i}\) belong to patch \(u\) and \(m_{i}\) belong to patch \(v\). We penalize the absolute angular difference between \(m_{1}\) in \(v\) and its extrapolated estimate using normals in \(u\), i.e., \(n_{2}+(n_{2}-n_{1})\). Making this symmetric gives

\[\|^{-1}(m_{1}(n_{2}+(n_{2}-n_{1})))\| +\|^{-1}(n_{2}(m_{1}-(m_{2}-m_{1}))) \|,\] (10)

which we sum over the length of the seam to obtain \(_{S}(_{0}^{u},_{0}^{v})\).

### Dominant Global Lighting Constraint

Our guidance so far enforces global coherence, but even globally coherent surfaces can contain regions that independently undergo convex/concave flips without affecting their surround . The top row of Fig. 5 provides a familiar example. One can hide any three of the bumps/dents and then perceive the fourth as being either concave or convex. Yet, when one is allowed to examine the image as a whole and rotate it upside down, instead of perceiving \(2^{4}=16\) interpretations, one sees only two: the bottom-right element is a bump (or dent) and the other three are its opposite. This behavior is explained by lighting. If one expects the dominant light direction to be similar everywhere on the surface, the four flips become tied together.

We can incorporate this notion of dominant light consistency into our model using an additional discrete guidance step, as depicted in the bottom right of Fig. 3. This step can be applied to any global sample \(_{0}\) and has three parts: (\(i\)) patches \(_{0}^{u}\) independently nominate dominant light directions \(^{u}\); (\(ii\)) we identify a single direction \(\) that is most common among these nominations; and (\(iii\)) some patches perform a concave/convex flip to become more consistent with \(\).

Specifically, each patch \(_{0}^{u}\) that is not too close to being planar (i.e., that has non-constant \(_{0}^{u}(i)\)) nominates its dominant light direction by computing the least-squares estimate according to shadowless

Figure 3: _Top_: Illustration of multiscale sampling across two scales in a fine-coarse-fine “V-cycle”, with conditional images omitted for simplicity. In practice, our V-cycle covers more than two scales. _Left_: The \(N\&R\) subroutine injects noise to an earlier timestep \(0<t<T\) and then resumes guided sampling (Fig. 2b) at that scale. _Right_: Optional intermediate guidance comes from lighting consistency (LCG), where each patch nominates a dominant light direction and then some patches flip in response to those nominations. Pseudocode is in the appendix.

Lambertian shading:

\[^{u}=_{l^{3}}_{i}(c^{u}(i)-^{u}_{0}(i),l}{\|^{u}_{0}(i)\|})^{2}\,.\] (11)

We create two clusters in the set \(\{^{u}\}\) using \(k\)-means and choose the center of the majority cluster as the dominant global direction \(\). Each patch \(u\) in the minority cluster undergoes a concave/convex flip \((p,q)(-p,-q)\) by multiplying \((-1)\) with the first two channels of \(^{u}_{0}\). Since the independent flips can cause discontinuities at patch seams, we always follow this discrete lighting guidance step by an _inject Noise & Resume sampling_ (N&R) subroutine, where we add noise to an intermediate timestep via Eq. 1 and resume spatially-guided denoising from that timestep. Pseudocode is provided in Appendix A.1.

This approach to lighting consistency has several advantages. Unlike many previous computational approaches to shape from shading, it does not assume the lighting to be known beforehand. Nor does it require the lighting to be exactly spatially uniform across the surface, which provides some resilience to global illumination effects. It imposes no prior on the dominant light direction (e.g., 'lighting from above'), but one can imagine extending it to do so. And because our patch-based framework can be applied with or without lighting consistency guidance (see Appendix A.3), it may provide a mechanism in the future for modeling the way in which humans selectively enforce lighting consistency across an image [7; 40].

### Multiscale Optimization

Since each local image patch can be explained by either concave or convex shapes, the terms in the spatial guidance energy (Eq. 8) are multimodal, and finding a global minimum is computationally difficult. Our experiments, like the one in Fig. 4, show that optimization at a single scale with random initial noise and gradient-descent guidance often gets trapped in poor local minima. To overcome this, and also to fully leverage shading information from various spatial frequencies and scales, we draw inspiration from work on Markov random fields  and introduce a multiscale optimization scheme. This is possible because our patch diffusion UNet and guidance can be applied to any resampled resolution \((sH)(sW)\) that is divisible by patch size \(d\).

Our multiscale optimization occurs in a "V-cycle", a sequence of fine-coarse-fine resolutions. We begin by applying guided denoising at the highest image resolution. Then, we downsample the predicted global field to a lower resolution before injecting noise and resuming reverse sampling (\(N\&R\)) at that lower resolution. As depicted in the left of Fig. 3, this has the effect of generating a random sample at the lower resolution that is informed by a previous sample at the higher resolution. A similar process occurs when going from coarse to fine, but with the global field being upsampled before applying the \(N\&R\) subroutine.

To further reduce discontinuities at seams, we find it helpful to store and fuse the global field estimates from the final few resolutions of the fine-coarse-fine cycle. We do this by computing their \((p,q)\) fields via Eq.7, resampling them to the highest resolution and averaging them, and then converting the average back to a normal field.

## 4 Experimental Results

The input to our patch UNet is the concatenation of \(c^{u}\) and \(x^{u}_{t}\). It has \(4\) channels and spatial dimension \(d d\) with \(d=16\). We train it using patches of size \(d d\) extracted from rendered images of the 3D objects in  curated from Adobe Stock. We use Lambertian shading from random light directions, with a random albedo in \([0.5,1]\) and without cast shadows or global illumination effects. Our dataset contains around 8000 images (\(256 256\)) of 400 unique objects. Some examples are shown in the left of Fig. 2. The images contain occluding contours, and for empty background pixels \(i,j\) we set \(x_{0}(i,j)=(-1,-1,-1)\). We augment the training data by creating two convex/concave copies of each patch field \(x^{u}_{0}\) that does not contain any background. At inference time, we use the DDIM sampler  with 50 sampling steps and with guidance. Additional details are in the appendix.

For comparison, we consider three deterministic approaches and two stochastic models. SIRFS  and Derender3D  are deterministic inverse-rendering models that estimate lighting and reflectance together with shape. They are among the few recent models that do not rely critically on having input occluding contour masks. Depth Anything  is a recent learning-based deterministic model for monocular depth estimation. It leverages a DINov2 encoder  and a DPT decoder  and is trained for depth regression using 62M images. For comparisons to stochastic models, we include Marigold  which is derived from Stable Diffusion  and is fine-tuned for depth estimation. We also include Wonder3D , which likewise leverages a prior based on Stable Diffusion. Wonder3D is trained to generate consistent multi-view normal estimates on more than \(30\)k 3D objects, and it achieves state-of-the-art results on 3D reconstruction benchmarks .

### Ablation Studies

Figure 4 analyzes the key components of our model using a crop of a shape and image from the lab of James Todd  (the complete image is in Appendix A.5). The left of the figure shows that when each patch is reconstructed independently, the resulting normals are inconsistent, because each patch may choose a different concave/convex mode as well as its various flattenings and tiltings. When spatial consistency guidance is applied at one scale, the global field is more consistent but suffers from discontinuous seams due to poor local minima. With multiscale sampling the seams improve, but separate bump/dent regions can still choose different modes without being consistent with any single dominant light direction. Finally, when lighting consistency is added, the output fields become more concentrated around two global modes--one that is globally convex (lit primarily from below) and another that is globally concave (lit primarily from above).

In the right of the figure, we compare samples from our model to depth profiles that were labeled by humans on the same image . (These results have appeared previously in .) We generate four samples from our model's globally-convex mode and integrate them to depth maps using . Their cross-sections exhibit variations with similar qualitative structure.

Figure 4: Ablations, and comparison to human subjects using image and psychophysics data from . _Left_: Ablations demonstrate the importance of each component. _Right_: Depth cross-sections extracted from four (integrated) samples from the convex mode of our full model exhibit relief-like variations similar to those reported across human subjects. (The dashed line is the depth that was used to render the input image.)

Figure 5: Normals produced by our model for various synthetic test surfaces rendered with directional light sources. For depth maps, brighter is closer: “Reference” depicts the shapes—each with a convex/concave counterpart—that were used to render the input images. We find that our reconstructions are more accurate and diverse than other methods.

### Ambiguous Images

Figure 5 shows results from our full model for images and shapes that we intentionally design to be ambiguous, using insight from . Each one can be perceived as either convex or concave, as shown in the right-most column (Reference). Samples from our model clearly demonstrate the effectiveness of our model in terms of both coverage and accuracy of the possible shapes. In contrast, we find that the two models derived from Stable Diffusion (Wonder3D and Marigold) provide less accuracy on these images, and that, on average, they tend to have a 'lighting from above' prior baked in. For instance, they tend to interpret the third and fourth row as concave, while Depth Anything  interprets them as convex. Additional results are included in the appendix.

Figure 6 visualizes distributions of shape reconstructions as 2D t-SNE plots (with perplexity equal to 30) by sampling 100 random seeds for our model and for Wonder3D. For reference, we also plot the t-SNE embeddings of a frontal plane, \(_{0}(i,j)=(0,0,1)\) and of the two reference shapes. Our model covers both reference shapes whereas Wonder3D either covers only one or is close to a plane. These differences in coverage and accuracy are also apparent in terms of Wasserstein distance.

### Real Images

We evaluate our model using a few different categories of captured images. In each case, we resize the image to \(256 256\) (to accommodate the restrictions of some of the previous models), and we use the multiscale schedule described in Appendix A.10.

Figure 6: t-SNE visualizations of normal field samples produced by our model and by Wonder3D. Plots depict 100 samples from each model, along with the two mathematical possibilities (under directional light) and the normals of a trivial frontal plane. For each model we report the Wasserstein distance (smaller is better) between its samples and the reference distribution, which is uniform over two possibilities. Our model is more accurate and in all cases covers both possibilities.

Figure 7: Sampled reconstructions for real images. (a) For the ‘plates’ image from , regions such as the indicated box can exhibit independent convex/concave flips when lighting consistency is not used; but when lighting consistency is enforced, only two global modes emerge. (b) Sampled reconstructions for some multistable images we captured with illumination from a point or area light. (Rotate them by \(180^{}\) to enhance the alternative experience.) Note that half of the object in the first row was painted matte, and its other half was left glossy. Despite being trained entirely on synthetic data under idealized lighting, the model exhibits some generalization by producing plausible multistable outputs for these captured scenes.

Captured ambiguous images.Inspired by the 'plates' image in , we captured a handful of images that induce multistable perceptions for human observers. We captured these images with a Sony \( 7\)SIII camera under lighting from area or point light sources, and Fig. 6(b) shows some examples. We find that our model's multimodality is qualitatively well aligned with perceptual multistability.

Shape from shading dataset .This dataset contains diffuse objects captured with directional light sources and a dark background, along with ground truth surface normals for measuring accuracy. The right of Fig. 8 shows that our model produces normal maps that are on par with previous methods, even without knowledge of light source directions. Additional quantitative results are in Appendix A.8

Internet and astronomical images.The left of Figure 8 shows that our model can produce a detailed and plausible shape estimate for a tone-mapped sRGB image taken from the web . Appendix A.6 includes two satellite images of the surface of Mars and shows that our model reproduces the so-called crater illusion.

## 5 Related Work

Given the recent success of diffusion models in generating realistic images [22; 49; 28], many works have explored the power of patch-based diffusion, including for generating high-resolution panoramic images [3; 32]. Our method also leverages patch diffusion, but it departs from these work in two key ways. Unlike [3; 32], we do not generate patches auto-regressively or require an anchor patch. Instead, we simultaneously guide all patches (e.g., \(100\) patches for a \(160 160\) image) toward a coherent output using spatial consistency guidance. A second distinction is that we do not provide a global condition such as text to each individual patch. Instead, each patch is conditioned only on the corresponding crop of the input grayscale image, which is why we call it a bottom-up architecture. It shares the same spirit as previous work on inverse lighting , which also uses a bottom-up architecture to produce a variety of explanations that can then be integrated with top-down information.

Recently, Wang et al.  introduced a patch-based diffusion training framework that incorporates patch coordinates to reduce training time and storage cost. Patch-based diffusion has also been used for other tasks. Ozdenizci et al.  use overlapping patch diffusion to restore images in adverse weather, and Ding et al.  use it to synthesize images in higher resolution. All of these works use fairly large patches (e.g., \(64 64\)) and some of their components, such as feature-averaging or noise-averaging, are not appropriate for our shape from shading problem because of its inherent multi-modality. A convex sample and a concave sample cannot simply be averaged to improve the output. These challenges motivate the novel features of our model, including global consistency guidance and multiscale sampling.

## 6 Conclusion

Inspired by the multistable perception of ambiguous images, and by mathematical ambiguities in shape from shading, we introduce a diffusion-based, bottom-up model for stochastic shape inference. It learns exclusively from observations of everyday objects, and then it produces perceptually-aligned multimodal shape distributions for images that are different from its training set and that appear ambiguous to human observers. A critical component of our model is a sampling scheme that

Figure 8: _Left_: Reconstructed normals and integrated depth for an image taken from the web. _Right_: Reconstructions for images in the SfS dataset of  with median angular error from the ground truth normals (lower is better). Our model’s accuracy is on par with the best existing methods. Additional quantitative results are in the appendix.

operates across multiple scales. Our model also provides compositional control: global lighting consistency can be turned on or off, thereby controlling whether regional bumps/dents can each undergo concave/convex flips independently. Our findings motivate the exploration of other multiscale stochastic architectures, for a variety of computer vision tasks. They may also help improve the understanding and modeling of human shape perception.

LimitationsA key limitation is our restriction to textureless and shadowless Lambertian shading. While this restriction is common in theoretical work [29; 54; 20] and useful for creating ambiguous images, it is well-known that many ambiguities disappear in the presence of other cues such as glossy highlights, cast shadows, and repetitive texture. Also, since our model is predominantly bottom-up, it suffers when large regions of an image are covered by cast shadows (e.g., the shoulder region in Fig. 8). These types of regions often require non-local context like object recognition in order to be accurately completed. Incorporating more diverse materials (e.g., as in ) and top-down signals into our model are important directions for future research.

Another limitation of our model stems from its sequential V-cycle approach to multiscale sampling. It scales linearly with the number of resolutions, which is likely to be improved by optimization or parallelization that increases runtime efficiency. Also, since our multiscale approach is training free, it requires a manual search to identify a good schedule. Similar to previous work that restarts the sampling process from intermediate timesteps , ours also require choosing the timestep at which to resume sampling. Overall, further analysis is needed to better understand the structure of our model's latent space, and to discover more efficient and general approaches to multiscale generation.

#### Acknowledgments

We thank Jianbo Shi for helpful discussions and Steven Zucker for suggesting the crater illusion. We also thank James Todd, Benjamin Kunsberg, Steven Zucker and William Smith for kindly sharing their images and perceptual stimuli. This work was in part supported by JSPS 20H05951, 21H04893, and JST JPMJAP2305. It was also in part supported by the NSF cooperative agreement PHY-2019786 (an NSF AI Institute, http://iaifi.org).