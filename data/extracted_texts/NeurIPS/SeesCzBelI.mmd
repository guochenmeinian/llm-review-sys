# Removing Length Bias in RLHF Is Not Enough

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) has become an essential technique for enhancing pretrained large language models (LLMs) to generate responses that align with human preferences and societal values. While RLHF has shown promise, the training of reward models (RMs) still faces the challenge of _reward hacking_, motivating recent works to prevent RMs from finding shortcuts that bypass the intended optimization objectives by identifying simplistic patterns, especially response length. Besides the issue of _length bias_, our work firstly reveal that _prompt-template bias_ learned by RMs can also cause _reward hacking_ when dealing with marginal samples, resulting in LLMs preferring to generate responses in a specific format after RLHF fine-tuning, regardless of the format requested in the prompt. To this end, we propose a low-cost but effective method, namely Prompt Bias Calibration (PBC), to estimate the _prompt-template bias_ term during reward modeling, which can be utilized to calibrate reward scores in the following RL fine-tuning process. Then, we show that our PBC method can be flexibly combined with existing algorithms of removing _length bias_, leading to a further improvement in the aspect of enhancing the quality of generated responses. Experiments results show that the performance of our PBC method and its extensions have significantly surpassed the original implementation of RLHF.

## 1 Introduction

Reinforcement Learning from Human Feedback (RLHF) has become a critical technique to enable pretrained large language models (LLMs) to follow human instructions, understand human intent, and also generate responses that align with human preferences and societal values [1; 2; 3; 4]. Specifically, RLHF usually trains a reward model (RM) to act as the proxy of human preferences, and then utilize online reinforcement learning (RL) algorithms to fine-tune the language models for generating responses that can achieve higher expectation rewards, leading to the success of ChatGPT and also many other AI systems [5; 6]. Although the paradigm of RLHF has simplified human data collection, as acquiring human ratings is much easier than collecting demonstrations for supervised fine-tuning (SFT), it still requires huge amount of human-annotated preference pairs to train well-performing RMs in practice, motivating recent researches to seek novel alignment methods to bypass RM training [2; 3; 4]. However, the pipeline of original RLHF is still the primary choice of most industrial applications, because well-trained RMs can provide a certain level of generalization ability .

Besides the expensive cost of collecting numerous human-annotated preference pairs, another heavily criticized issue of RLHF could be the phenomenon of _reward hacking_, where the over-optimized RMs tend to find some shortcuts to bypass its intended optimization objective, through identifying some simple patterns to distinguish between good and bad responses . The most widely studied pattern in _reward hacking_ could be the sentence (response) length, and these trained RMs can utilize the preference among human raters for longer responses to achieve _reward hacking_, despite the actual quality of response does not improve with the increase of response length . Thus, to mitigate_reward hacking_, recent works has primarily focused on estimating the _length bias_ term in the reward scoring process, so that it can be removed in the subsequent RL fine-tuning procedure to further improve the quality of generated response after RLHF process [11; 12].

Besides the issue of _length bias_, in the practice of applying RLHF to industrial products, we have observed that the original implementation of RLHF tends to make LLMs prefer generating responses in a specific format. This observation motivates us to investigate the underlying causes and seek a cost-effective solution to address this issue. The main contributions are summarized as follows:

* We are the first to reveal the existence of _prompt-template bias_ in RMs trained with the original preference loss, and theoretically analyze the cause of _prompt-template bias_ issue, along with its corresponding potential risks on the entire RLHF process;
* To mitigate the _reward hacking_ caused by _prompt-template bias_, we develop a Prompt Bias Calibration (PBC) method, which will firstly estimate the _prompt-template bias_ term during the reward scoring process, and then remove it in the subsequent RL fine-tuning process;
* We show that the developed PBC method can be flexibly combined with existing methods of removing _length bias_, leading to a further improvement in the aspect of enhancing the quality of generated responses;
* Experimental results show that our developed PCB method and its extensions can achieve promising performance improvements compared to the original implementation of RLHF.

## 2 Preliminary

Reward models (RMs) have become the dominant tool for aligning the LLM's responses with user preferences or task-specific requirements [1; 9]. In this section, we will firstly review the training procedure of reward models in Sec. 2.1, including analyzing the causes of _length bias_ and _prompt bias_ in existing RMs, and also illustrate how these RMs are used for alignment in Sec. 2.2, especially RLHF fine-tuning processes.

### Reward Model Training

The usual optimization goal of a reward model is to minimize the loss under the Bradley-Terry model  on the dataset of pair-wise comparisons of model responses, denoted as \((x,y^{+},y^{-})\) where \(x\) indicates the input prompt, \(y^{+}\) and \(y^{-}\) are the chosen and rejected responses respectively. Then, the objective function can be formulated as

\[^{RM}()=-_{(x,y^{+},y^{-})}[ ((r_{}(x,y^{+})-r_{}(x,y^{-}))]\] (1)

where \(r_{}(x,y)\) denotes the reward model that takes the prompt \(x\) and response \(y\) as input to predict a scalar reward with trainable parameters \(\); \(\) denotes the sigmoid function.

**Length Bias**: Denote \(r_{^{*}}(x,y)\) as the "gold standard" reward model  with the optimal parameters \(^{*}\), it reflects human's intrinsic ranking preferences and can play a role of human rater to provide gold reward signal for each prompt-response pair. However, due to the subjectivity of ranking preferences and flaws in rating criteria, there is a phenomenon where human raters prefer longer responses that appear to be more detailed or better formatted, but their actual quality does not improve . Thus, the "gold standard" reward model for rating preference data can often be biased and thus we can decompose it to disentangle the actual reward from the spurious reward , formulated as

\[r_{^{*}}(x,y)=r_{^{*}}^{Q}(x,y)+r_{^{*}}^{L}(x,y),\] (2)

where \(r_{^{*}}^{Q}(x,y)\) is the actual reward gains brought by improving the quality of response \(y\); \(r_{^{*}}^{L}(x,y)\) is the spurious reward gains of increasing response length, whose patterns are much easier to identify.

Thus, with _length bias_ in the "gold standard" \(r_{^{*}}(x,y)\), during the training of reward model, \(r_{}(x,y)\) can easily find shortcuts to bypass its intended optimization objective, through identifying simple patterns, such as sentence (response) length, to distinguish between good and bad responses, leading to the phenomenon of "reward hacking" caused by _length bias_. Without increasing the cost of rating higher quality preference data, it becomes increasingly important and beneficial to study mitigating the impact of _length bias_ in the process of reward modeling.

**Prompt Bias:** the _prompt bias_ in reward modeling derives from the underdetermination of Bardley-Terry model . For any reward model \(r_{^{{}^{}}}(x,y)\) learned from the preference loss defined in Eq. (1), whose target is optimized to approximate the "gold standard" \(r_{^{{}^{}}}(x,y)\), there always exists an equivalent reward model \(r_{}(x,y)\) that satisfies

\[r_{}(x,y):=r_{^{{}^{}}}(x,y)+C(x)\] (3)

where \(C(x)\) is a prompt-dependent constant referred to as _prompt bias_, leading to the same loss value as \(()=(^{{}^{}})\). Due to the fact that there is no constraint on \(C(x)\) in the original preference loss as defined in Eq. (1), the issue of _prompt bias_ has been criticized in the scenario of reward model ensembles , where different reward models tend to choose different values for \(C(x)\), making the statistics of the set of reward scores meaningless.

As shown in Fig. 1, it has been widely reported that the _prompt bias_ will result in a certain gap in the mean values of the set of prompt-response pairs under different prompts. However, in our research, we find that this gap is more likely caused by the _prompt-template bias_, as discussed in Section 3.1.

### RLHF Fine-tuning

Given the trained reward model \(r_{}(x,y)\) as the proxy of human preferences, Reinforcement Learning from Human Feedback (RLHF) tends to utilize an online reinforcement learning method, typically proximal policy optimization (PPO) , trains a policy language model \(_{}^{RL}\) to maximize expected reward, while staying close to its initial policy \(_{}^{SFT}\), which is finetuned on supervised data (prompt-response pairs). Through measuring the distance from the initial policy with Kullback-Leibler (KL) divergence, the optimization objective of RLHF fine-tuning can be formulated as

\[^{RL}()=_{(x,y)_{_{}^{RL}}} [r_{}(x,y)+[_{}^{RL}(y|x)/^{SFT}(y|x) ]],\] (4)

where \(\) is the hyper-parameter to control the strength of the KL divergence term.

## 3 Method

In this section, we will firstly investigate the cause of _prompt-template bias_ and then theoretically analyze its potential risks when dealing with marginal samples during reward modeling, as shown in Sec. 3.1, and then illustrate our low-cost but effective method to estimate the _prompt-template bias_ term during RM training in Sec. 3.2, which can be utilized to calibrate reward scores in the following RL fine-tuning process. At last, in Sec. 3.3, we show that our Prompt Bias Calibration (PBC) method can be flexibly combined with recent popular methods of removing _length bias_, leading to a further improvement in the aspect of enhancing the quality of generated responses.

Figure 1: Comparison of the RM training process using the original preference loss and our developed PBC method respectively, where the latter employs \(u_{c}(x)\) to approximate the _prompt-template bias_, providing unbiased reward scores with lower variance for the subsequent RL fine-tuning.

### Impact of _prompt-template bias_ on RLHF

In this part, we will first illustrate the cause of _prompt-template bias_ during RM training. Formally, given a set of prompt-response pairs, denoted as \(_{a}=\{x_{a},y_{a}^{(i)}\}_{i=1}^{N_{a}}\), with the same user prompt \(x_{a}\), _e.g._ "_writing an **academic paper** on the field of computer science_", and \(\{y_{a}^{(i)}\}_{i=1}^{N_{a}}\) denoting the set of collected _academic papers_ to satisfy the request of \(x_{a}\), the _prompt bias_ term, specifically \(C(x_{a})\), learned by RMs is supposed to not affect the preference order within \(_{a}\), as discussed in Section 2.1. However, in the practice of RM training, the reward score is usually predicted by a LLM that takes the concatenation of the prompt and response as input, making it challenging for RMs to learn a bias term that focuses solely on the prompt \(x\) while disregarding variations in the subsequent response \(y\). During the training process to order the pairs within \(_{a}\), we find that RMs trained with the original preference loss in Eq. (1) are more likely to introduce a joint bias term across the entire sequence of concatenating the prompt and response, formulated as

\[r_{}(x_{a},y_{a}):=r_{^{}}(x_{a},y_{a})+C(x_{a},_{a}),\ \ \ \ _{a}=}_{i=1}^{N}y_{a}^{(i)},\] (5)

where \(_{a}\) can be considered the average response of the response set \(\{y_{a}^{(i)}\}_{i=1}^{N_{a}}\), and it will embody the common characteristics found within these collected responses, such as the format of _academic paper_; \(C(x_{a},_{a})\) denotes the joint bias on the entire sequence of the prompt \(x_{a}\) associated with the average response \(_{a}\) in the format of _academic paper_; \(r_{}(x_{a},y_{a})\) is still supposed to approximate the "gold standard" provided by \(r_{^{*}}(x_{a},y_{a})\), leading to \(_{_{a}}[r_{^{}}(x_{a},y_{a})] _{_{a}}[r_{^{*}}(x_{a},y_{a})]\).

Considering the average response \(\) can be treated as a standard template of the response to the prompt \(x\), we define the joint bias \(C(x,)\) as _prompt-template bias_. Then, we highlight the properties of _prompt-template bias_ as follows: 1) the original preference loss in Eq. (1) imposes no constraints on \(C(x,)\), because its value will not influence the outcome of the preference loss and also not affect the preference order within the prompt-response pairs collected for the same prompt \(x\); 2) \(C(x,)\) will reduce to the original _prompt bias_\(C(x,-)\) when no common characteristics can be found across all of these collected responses, indicating the diversity of \(\{y^{(i)}\}_{i=1}^{N_{a}}\) is sufficiently high. With these properties in mind, we assume that the _prompt-template bias_\(C(x,)\) can essentially meet most of the properties of the original _prompt bias_\(C(x,-)\) as discussed in Section 2.1. Thus, we suppose \(C(x,)\) can be considered as a broader definition of _prompt bias_ in the actual RM training, because it is more likely to be learned by RMs in practice, given the fact that preference pairs are extremely scarce and the diversity of responses collected for the same prompt is often insufficient.

After defining _prompt-template bias_, we will theoretically investigate the impact of introducing \(C(x,)\) during RM training on the entire RLHF process. Assume that there exist two sets of prompt-response pairs, denoted as \(_{a}=\{x_{a},y_{a}^{(i)}\}_{i=1}^{N_{a}}\) and \(_{b}=\{x_{b},y_{b}^{(i)}\}_{i=1}^{N_{b}}\), where \(x_{a}\) and \(x_{b}\) indicate different categories of prompts, \(e.g.\)\(x_{a}\) requests _"writing an **academic paper** on theme **a**"_ and \(x_{b}\) requests _"writing a **brief** on theme **b**"_, and \(\{y_{a}^{(i)}\}_{i=1}^{N_{a}}\) and \(\{y_{b}^{(i)}\}_{i=1}^{N_{b}}\) denote the collected responses for answering the prompt \(x_{a}\) and \(x_{b}\) respectively. After RM training, due the fact that there is no constraint on \(C(x,)\) in the preference loss defined in Eq. (1), the discrepancies of prompt biases between these two previously mentioned sets of prompt-response pairs, specifically \(_{a}\) and \(_{b}\), could be extremely large, _e.g._\(C(x_{a},_{a})>>C(x_{b},_{b})\), leading to

\[_{(x_{a},y_{a})_{a}}[r_{}(x_{a},y_{a}) ]>>_{(x_{b},y_{b})_{b}}[r_{}(x_{b},y _{b})]\] (6)

where \(r_{}(x_{a},y_{a})=r_{^{}}(x_{a},y_{a})+C(x_{a},_ {a})\) and \(r_{}(x_{b},y_{b})=r_{^{}}(x_{b},y_{b})+C(x_{b},_ {b})\). The unbiased reward distributions, modeling the reward scores \(\{r_{^{}}(x_{a},y_{a}^{(i)})\}_{i=1}^{N_{a}}\) and \(\{r_{^{}}(x_{b},y_{b}^{(i)})\}_{i=1}^{N_{b}}\) respectively, should exhibit similar mean values, _e.g._\(_{_{a}}[r_{^{}}(x_{a},y_{a})] _{_{b}}[r_{^{}}(x_{b},y_{b})]\), and will make little impact on the comparison of expectation terms in Eq. (6). We highlight that the discrepancies of _prompt bias_ terms, specifically the gap between \(C(x_{a},_{a})\) and \(C(x_{b},_{b})\), won't affect preference ordering within categories, but can cause disaster when dealing with some marginal samples, like _"an **academic paper** on theme **b**"_ denoted as \(y_{ab}\), or _"a **brief** on theme **a**"_ denoted as \(y_{ba}\).

To facilitate an intuitive analysis, we take the marginal sample _"an **academic paper** on theme **b**"_, denoted as \(y_{ab}\), as an example, and the reward scores for prompt-response pairs corresponding to the prompt \(x_{b}\) may exhibit the following preference orders:

\[r_{}(x_{b},y_{ab})=r_{^{}}(x_{b},y_{ab})+C(x_{b},_ {a})>r_{^{}}(x_{b},y_{b})+C(x_{b},_{b})=r_{}(x_{b},y _{b}),\] (7)which can be achieved as long as \(r_{^{}}(x_{b},y_{ab}) r_{^{}}(x_{b},y_{b})\) and \(C(x_{b},_{a})>C(x_{b},_{b})\). The first condition \(r_{^{}}(x_{b},y_{ab}) r_{^{}}(x_{b},y_{b})\) can be achieved because both the response \(y_{ab}\) and \(y_{b}\) meet the description of theme \(b\) and are similar on a semantic level. The second inequality is highly likely to be achieved when there is a reward model that has a bias towards preferring the sentence in the format of \(a\) over \(b\), specifically \(C(x_{a},_{a})>>C(x_{b},_{b})\).

Finally, we highlight that the phenomena of inequality in Eq. (7), caused by _prompt-template bias_\(C(x,)\), is commonly encountered in the deployment process of RLHF in real-world applications, especially text creation. For example, if responses are collected solely for the style requested in each prompt during RM training, the reward model can lead to a bias towards particular styles as shown in Fig. 3(a). Then, once such marginal samples, _e.g_\((x_{b},y_{ab})\), are generated by LLMs during the RL fine-tuning process and also satisfy the inequality \(r_{}(x_{b},y_{ab})>r_{}(x_{b},y_{b})\) as shown in Table 1, the entire RL fine-tuning process, typically PPO, will be biased and results in a LLM that only generates responses in a specific format, regardless of the format you request in the prompt.

### Calibrating _prompt-template bias_ in RLHF

To mitigate the impact of the _prompt-template bias_ issue on the RLHF process, the most straightforward solution in industry could be to collect a more diverse set of response candidates for each prompt. However, this approach is time-consuming and may even require a lot of human interventions for response collection, motivating us to develop a low-cost but effective method to alleviate the issue of _prompt-template bias_ during RM training.

The developed Prompt Bias Calibration (PBC) method mainly includes two steps: 1) estimating the _prompt-template bias_ term in the reward scoring process with minimal additional computational cost; 2) removing _prompt-template bias_ in the subsequent RLHF fine-tuning process to ensure that the resulting LLM does not have a tendency to generate responses in a specific format. As shown in Fig. 1, to approximate the _prompt-template bias_ term \(C(x,)\) in Eq. (5), we choose to apply a linear layer on the last token of the prompt sentence to predict _prompt-template bias_, denoted as \(u_{c}(x)\), and then add the following regularization term on the original preference loss, formulated as

\[_{c}^{RM}()=_{(x,y^{+},y^{-})}[ \|r_{}(x,y^{+})-u_{c}(x)\|_{2}^{2}+\|r_{}(x,y^{-})-u_{c}(x)\|_{2} ^{2}],\] (8)

where \(u_{c}(x)\) is supposed to approximate the mean value of reward scores of the prompt-response pairs given the same prompt \(x\). We note that there will be a hyper-parameter \(_{c}\) to be multiplied on the regularization term in the final loss to promise the accuracy of RMs, leading to

\[_{pbc}^{RM}()=^{RM}()+_{c}_{c}^{RM}().\] (9)

The benefits of such a design in the PBC method include the following folds: 1) approximating \(C(x,)\) by adding a linear layer to the last hidden layer of LLMs results in almost no additional

Figure 2: Network architecture design for the RM trained using the LBPC method incorporates a prompt bias head on the last token of the prompt \(x\) designed to predict \(C^{Q}(x,)\) and \(C^{L}(x,)\), and a reward score head on the last token of the response intended to predict \(r_{}^{Q}(x,)\) and \(r_{}^{L}(x,)\).

computational cost; 2) during the autoregressive scoring process of LLM-based RMs, \(C(x,)\) can serve as an intermediate signal guidance of the prompt sequence, thereby enabling RMs to focus more on the differences between chosen/rejected responses in the subsequent reward scoring process; 3) we can use unbiased reward scores to guide the follow RLHF fine-tuning process, formulated as

\[r_{^{}}(x,y)=r_{}(x,y)-u_{c}(x) r_{}(x,y)-C(x, ),\] (10)

which has been proven effective for penalizing reward uncertainty, improving robustness, encouraging improvement over baselines, and reducing variance in PPO fine-tuning .

### Jointly calibrating _length_ and _prompt-template bias_ in RLHF

To simultaneously calibrate _length_ and _prompt-template bias_ in RLHF, the developed PBC method can be flexibly combined with existing methods of removing _length bias_, whose main idea is to separately approximate the "gold standard" reward model after disentangling shown in Eq. (2), formulated as:

\[r_{}(x,y)=r_{}^{Q}(x,y)+r_{}^{L}(x,y),\] (11)

where \(r_{}^{Q}(x,y)\) is supposed to approximate the actual reward \(r_{}^{Q}(x,y)\); \(r_{}^{L}(x,y)\) is used to approximate the spurious reward brought by _length bias_, specifically \(r_{}^{L}(x,y)\). Then, for those methods of removing _length bias_, the original preference loss in Eq. (1) can be equivalently expressed as

\[^{RM}()=-_{(x,y^{+},y^{-})}[ ((r_{}^{Q}(x,y^{+})+r_{}^{L}(x,y^{+})-r_{}^{Q}(x,y ^{-})-r_{}^{L}(x,y^{-}))].\] (12)

where \(r_{}^{Q}(x,y)\) and \(r_{}^{L}(x,y)\) can be modeled with two different LLMs  or two different heads in the same LLM . To remove _length bias_ in Eq. (12), recent work proposes to add constraints on the preference loss to reduce the correlation between the confounding factor, _e.g._ response length, and actual reward \(r_{}^{Q}(x,y)\), while increasing its correlation with spurious reward \(r_{}^{L}(x,y)\), formulated as

\[_{l}^{RM}()=Corr(r_{}^{Q}(x,y),L(x,y))-Corr(r_{}^ {L}(x,y),L(x,y))\] (13)

where the confounding factor \(L(x,y)\) can be either specifically defined as response length \(L(y)\) in , or use Products-of-Experts framework for estimation .

To model the scoring process of the reward model more accurately, which simultaneously considers the concepts of length and prompt bias, we combine the definition of reward model in Eq. (3) and Eq. (11), achieving a more precise definition of reward scoring process, formulated as:

\[r_{}(x,y)=r_{^{}}(x,y)+C(x,)=r_{^{} }^{Q}(x,y)+C^{Q}(x,)+r_{^{}}^{L}(x,y)+C^{L}(x, )\] (14)

where \(C^{Q}(x,)\) and \(C^{L}(x,)\) indicate the component of _prompt-template bias_ in actual and spurious rewards, respectively; the unbiased overall reward \(r_{^{}}(x,y)=r_{^{}}^{Q}(x,y)+r_{^{}}^{L}(x,y)\) and the overall _prompt-template bias_ term \(C(x,)=C^{Q}(x,)+C^{L}(x,)\). Then we can propose Length and Prompt Bias Calibration (LPBC) method, as shown in Fig. 2, which can estimate \(_{l}^{RM}(,)\) with a conditioned correlation method, defined as

\[_{l}^{RM}() =Corr(r_{}^{Q}(x,y)-C^{Q}(x,),L(y;x))-Corr(r_{ }^{L}(x,y)-C^{L}(x,),L(y;x))\] (15) \[=Corr(r_{^{}}^{Q}(x,y),L(y;x))-Corr(r_{^{} }^{L}(x,y),L(y;x))\]

where the confounding factor \(L(y;x):=L(x,y)-L(x)\) can be estimated with the response length.

Through combining the disentangled preference loss in Eq. (12), the prompt-bias regularization term in Eq. (8) and also the length-bias conditional correlation term in Eq. (15), the final loss of LBPC method can be formulated as

\[_{lpbc}^{RM}()=^{RM}()+_{c}_{c}^{RM}()+_{l}_{l}^{RM}(),\] (16)

where \(_{c}\) and \(_{l}\) are hyper-parameters to control the importance of regularization terms, which can be adjusted according to the accuracy of trained RMs on the validation dataset.

[MISSING_PAGE_FAIL:7]

### Experimental Results

**Qualitative Evaluation.** To intuitively evaluate the effectiveness of our method, we exhibit the statistics (mean and standard deviation) of the reward scores predicted by RMs trained with the original preference loss in Eq. (1) and our PBC method in Eq. (9), across different categories of prompt-response pairs in the validation set of the RM-Template dataset. The results depicted in Fig.3(c) demonstrate that calibrating _prompt-template bias_ with the PBC method leads to a gradual reduction in the variance of the mean values of reward distributions across different categories. The most noticeable observation is that the vanilla RM tends to give an extremely high reward score to prompt-response pairs in the format of _tech article_, but the RM trained with the PBC method can calibrate the reward distribution for _tech articles_ to make it more close with that of other categories.

Then, we evaluate the performance of RMs trained with various methods on handling marginal samples defined in Section 3.1. Specifically, given the prompt randomly selected from the validation set of RM-Template dataset, we use GPT-4  to generate responses in various formats according to the theme described in the prompt. Then, we use RMs trained with various preference losses to rank these responses. From the showcase in Table. 1, we can find that the vanilla RM tend to assign a higher reward score to the response in the format of _tech article_, caused by the _prompt-template bias_ issue shown in Fig. d3(a). After removing this bias with our PBC or LPBC methods, the RM can provide a relatively fair ranking for these prompt-response pairs, where LPBC method can even mitigate the affect of _length bias_ during comparing poetry with other categories (the length of poetry is generally shorter than other literary forms). More showcases can be found in Appendix A.6.

**Quantitative Comparison.** For the quantitative comparison in Table 2, we utilize PPO fine-tuning process to align Llama-2-7b with the RMs trained with various methods. From the results, we can find that our developed PBC method can lead to performance improvements compared to the original implementation of RLHF; directly combining PBC with other methods of removing _length bias_, _e.g._ ODIN , can help them to achieve further performance improvement; the well-designed LPBC achieves the best performance and surpasses the rough combination of PBC and ODIN.

To make a comprehensive comparison, we follow the experimental setting described in ODIN , and use GPT-4 as the judge to compare two responses generated by LLMs aligned with RMs trained

   Base Model & Alignment & Length \& Quality Hooks & Prompt Head & Debias Method & MMLU & DROP & BBH & TQA \\  Llama-2-7b & - & - & - & - & 42.27 & 28.10 & 31.27 & 38.75 \\ Llama-2-7b & \(\) & - & - & - & 43.82 & 29.53 & 31.65 & 36.57 \\ Llama-2-7b & \(\) & \(\) & - & ODIN  & 42.29 & 29.82 & 32.01 & 39.43 \\ Llama-2-7b & \(\) & - & \(\) & PBC (9) & 43.84 & 31.61 & 30.99 & 38.50 \\ Llama-2-7b & \(\) & \(\) & \(\) & ODIN  + PBC (9) & 45.56 & **32.04** & 31.32 & **40.80** \\ Llama-2-7b & \(\) & \(\) & \(\) & LPBC (16) & **45.94** & 31.57 & **32.04** & 38.75 \\   

Table 2: Performance comparison of LLMs aligned with RMs trained with various methods.

Figure 4: Win rates comparison (judged by GPT-4) of LLMs aligned with RMs trained with LBPC and other methods.

Figure 3: The comparison of statistics of the reward scores predicted by RMs trained with (a) the original preference loss and (b) our developed PBC method, across different categories of prompt-response pairs in the validation set of the manually constructed RM-Template dataset.

with various methods. Specifically, we take the LLM aligned with LPBC-based RM as model A, and compare it against other LLMs aligned with RM trained with ODIN, PCB, ODIN+PBC, respectively. From the results shown in Fig. 4, we can find that the win rate of LPBC is significantly higher than that of other baseline models, with ODIN+PBC being the most challenging competitor as model B.

### Ablation Studies

To investigate the robustness of our developed LPBC method, we conduct ablation studies on the hyper-parameter settings of LPBC method, specifically \(_{c}\) and \(_{l}\) in Eq. (16). With various settings of \(_{c}\{0.01,0.05,0.1\}\) and \(_{l}\{0.01,0.05,0.1\}\), we can have total 9 RMs trained with various hyper-parameter settings of LPBC methods. From the accuracy curves shown in Fig.5(a), we can find the introducing constraints to the original preference loss indeed affects the performance of RM accuracy, and this performance loss increases with the importance weight of the constraint terms. However, at the limited cost of sacrificing RM accuracy, the performance of the LLM aligned the RM trained with LPBC method has improved to some extent on MMLU and DROP as shown in Fig. 5(b) and 5(c) respectively. Note that the performance of the LPBC method in Table. 2 is not the optimal, as it is achieved with \(_{c}=_{l}=0.05\), demonstrating no cherry-picking of hyperparameters..

## 5 Related Works

The prevalence of _length bias_ in RLHF have been widely criticized as indicative of reward hacking [9; 10], and numerous recent studies have delved into strategies aimed at mitigating the tendency for length increase during the fine-tuning process of RLHF [11; 12; 27]. Typically, Shen et al.  innovatively apply the Productof-Experts (PoE) technique to separate reward modeling from the influence of sequence length, which adopts a smaller reward model to learn the biases in the reward and a larger reward model to learn the true reward. Utilizing similar disentangling ideas, Chen et al.  jointly train two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to focus more on the actual content quality. Ryan et al.  firstly study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping. As for the _prompt bias_ issue, although it has been criticized in the scenario of reward model ensembles , no studies have yet attempted to analyze its cause and influence on RLHF. We emphasize that our work is the first to fill this gap by proposing a low-cost yet effective method to mitigate the reward hacking induced by _prompt-template bias_.

## 6 Conclusion

In this paper, we demonstrate that _prompt-template bias_ in RMs can lead to LLMs, which, after RL fine-tuning, generate responses exclusively in a specific format, irrespective of the variations in the prompt request. Thus, we propose a low-cost but effective PBC method, to estimate the _prompt-template bias_ term during reward modeling, which can be utilized to calibrate reward scores in the following RL fine-tuning process. Then, we show that our PBC method can be flexibly combined with existing algorithms of removing _length bias_, leading to a further improvement in the aspect of enhancing the quality of generated responses. Experimental results show that the performance of PBC method and its extensions have significantly surpassed the original implementation of RLHF.

Figure 5: Ablation studies on the various settings of hyper-parameter \(_{c}\) and \(_{l}\) in LPBC method.