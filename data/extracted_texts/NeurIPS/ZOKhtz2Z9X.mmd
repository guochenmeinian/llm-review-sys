# Encoding Human Behavior in

Information Design through Deep Learning

 Guanghui Yu

Washington University in St. Louis

guanghuiyu@wustl.edu

&Wei Tang

Columbia University

wt2359@columbia.edu

&Saumik Narayanan

Washington University in St. Louis

saumik@wustl.edu

&Chien-Ju Ho

Washington University in St. Louis

chienju.ho@wustl.edu

###### Abstract

We initiate the study of _behavioral information design_ through deep learning. In information design, a _sender_ aims to persuade a _receiver_ to take certain actions by strategically revealing information. We address scenarios in which the receiver might exhibit different behavior patterns other than the standard Bayesian rational assumption. We propose HAIDNet, a neural-network-based optimization framework for information design that can adapt to multiple representations of human behavior. Through extensive simulation, we show that HAIDNet can not only recover information policies that are near-optimal compared with known analytical solutions, but also can extend to designing information policies for settings that are computationally challenging (e.g., when there are multiple receivers) or for settings where there are no known solutions in general (e.g., when the receiver behavior does not follow the Bayesian rational assumption). We also conduct real-world human-subject experiments and demonstrate that our framework can capture human behavior from data and lead to more effective information policy for real-world human receivers.

## 1 Introduction

The problem of information design, where a player with an information advantage (the _sender_) can strategically reveal information to influence another player (the _receiver_) to take certain actions, is ubiquitous in everyday applications. For example, online retailers can highlight a subset of product features to influence buyers to make purchases . Recommendation systems might selectively display other users' ratings to persuade users to follow recommendations . Politicians can influence voters' decisions by designing different policy experiments . There have been various research efforts from economics , machine learning and artificial intelligence , and general computer science  devoted to the study of information design. Among the growing literature on information design, the model of Bayesian persuasion proposed by Kamenica and Gentzkow  is one of the most prominent, and has inspired a rich body of studies.

While Bayesian persuasion offers an elegant framework for formulating the information design problem, it has two limitations. First, the receiver is assumed to be Bayesian rational. This means that the receiver can form a posterior in a Bayesian manner and chooses the action that maximizes his expected utility. 1 However, as consistently observed in empirical studies , humans oftendeviate from being Bayesian or rational. Directly applying the techniques from the information design literature that assume Bayesian rational receivers could lead to suboptimal outcomes. In this work, we address this limitation by proposing a general optimization framework that can integrate a wide range of human behavior, expressed either as traditional analytical closed-form behavioral models or as data-driven models, and design optimal information policies with respect to the provided human behavior.

Second, despite a decade of effort, characterizing the optimal information policy remains notoriously difficult. Dughmi and Xu  have shown that it is #P-hard to compute the optimal expected sender utility, and in multi-receiver settings where each receiver only has binary actions, it is #P-hard to even approximate the optimal sender utility within any constant multiplicative factor . Moreover, most previous works have assumed that the receiver follows the Bayesian rational assumption. When this assumption is relaxed , there are generally no known analytical solutions for finding the optimal information policy yet.

In this work, we initiate the study of automated information design that encodes human behavior into the design process. Inspired by the recent effort in utilizing deep learning for auction design , we propose HAIDNet, an optimization framework that leverages neural-network architectures for information design. Unlike existing works that assume rational human behavior, our optimization framework can adjust to multiple representations of human behavior patterns, including standard behavioral models represented in analytic forms, and data-driven models trained using machine learning approaches. More specifically, We encode receiver behavior as a function and represent the loss in our optimization framework as a function of the receiver's responses to the disclosed information. This approach enables our optimization framework to accommodate different representations of human behavior and can lead to corresponding optimal information policies. Our contributions can be summarized as follows:

* We initiate the study of automated information design that encodes human behavior in the design process. The proposed end-to-end Human behavior encoded neural network mechanism for Automated Information Design, namely HAIDNet, enables us to optimize the sender's information policy based on a given model of human behavior. We incorporate a neural network architecture for information design problems. Moreover, we extend the literature on deep learning for mechanism design to encode realistic human behavior in the design process.
* We evaluate our approach via extensive simulations. In simpler settings with known analytical solutions, we show that HAIDNet can recover the optimal information policies. We also show that HAIDNet can extend to design information policies for settings that are computationally challenging (e.g., multiple receivers involved), or for settings with no known solutions in general (e.g., when the receiver's behavior does not follow the standard Bayesian rationality assumption).
* Through real-world human-subject experiments, we demonstrate that our framework can adapt to scenarios where we do not have access to human models a priori. We demonstrate that our approach can accurately learn a human descriptor from behavioral data, incorporate it in our optimization framework, and result in more effective information design policy in the real world.

**Related Work.** Our work joins a growing line of research that leverages computational tools for automated mechanism design . More recently, deep neural networks have been utilized for optimizing auction design . Our work differs from this line of work in two ways. First, we extend the approach beyond auction design to address the automated information design problem. Second and more importantly, we have incorporated human behavior in our design, while prior works mostly require standard rationality assumptions.

Our information design formulation builds on the seminal work of Bayesian persuasion , which inspired a rich line of research in information design (e.g., see the recent surveys by 38, 4). In particular, given the practical relevance of information design, there is an increasing number of information design studies in the research community in machine learning and artificial intelligence , economics , and operations research . Our work differs from most of the existing works in that we integrate human behavior into the design of information policy. There have been a few works addressing non-Bayesian belief updating  and non-rational receiver behavior  with stylized models in information design. Our work extends previous research by designing an framework that can accommodate both the analytical form and the data-driven form of human behavior. More discussions on related work can be found in Appendix A.

Preliminary - Bayesian Persuasion Basics

In Bayesian persuasion, there are two players: a sender and a receiver. The sender's goal is to design an information disclosure policy that persuades the receiver to take certain actions maximizing the sender's objective. The state of nature \(\) is drawn from a finite set \(\{1,,m\}\) according to a prior distribution \((())_{}()\). The prior is common knowledge to both the sender and the receiver. The receiver's utility \(u^{R}(a,)\) depends on the receiver action \(a\) from an action set \(\) and the state \(\). The sender's utility \(u^{S}(a,)\) also depends on the receiver's action and the state.

The sender can observe the realized state while the receiver cannot, and the sender can utilize this information advantage to persuade the receiver to take the desired action. In particular, before observing the realized state, the sender can commit to an information policy \(\), specifying what signal to present to the receiver conditional on the realized state. More formally, an information policy \(\) consists of a signal space \(\) and a set of conditional probabilities \(\{(|)\}_{}\) where \((|)=((|))_{}()\) and \((|)\) denotes the probability to send signal \(\) given the realized state \(\). This information disclosure policy is known to the receiver and specifies how the sender discloses information to the receiver. When a state \(\) is realized, the sender sends a signal \((|)\) according to the policy.

In Bayesian persuasion, the receiver is assumed to be Bayesian rational in the sense that upon seeing the signal \(\), the receiver forms his posterior belief about the state in a Bayesian manner and takes an action that maximizes his expected utility. Formally, upon seeing the signal realization \(\), the receiver updates his posterior belief over the state of nature, denoted by \(()((|))_{}()\), by applying Bayes' rule: \((|)}(|^{})(^{})}\).

Given the posterior induced from the observed signal \(\), the receiver takes an action \(a^{}()\) that maximizes his expected utility2, namely, \(a^{}()_{a}_{ }(|)u^{R}(a,)\). The sender's information design problem is to find the optimal information policy that maximizes her expected payoff induced by the receiver's action, as follows:

\[_{}\ _{}()_{}( |)u^{S}(a^{}(),)\.\] ( \[^{}\] )

In this work, our goal is to design an automated framework to solve the above bi-level optimization problem while encoding realistic human behavior in the design process (i.e., replacing the Bayesian rational human model \(a^{}()\) with general human behavior).

**Example.** Consider the scenario in which an online retailer (the sender) aims to persuade a buyer (the receiver) to make a purchase. The retailer's products are directly coming from the factory, and the product quality (represented by the binary state \(\)) is drawn from a prior distribution \(\). The buyer's utility \(u^{R}(a,)\) depends on both his binary purchase decision \(a\) and the binary product quality \(\), while the retailer's utility \(u^{S}(a,) u^{S}(a),\) is state-independent and only depends on the buyer's purchase decision. For example, the goal of the retailer is to persuade the buyer to make a purchase, i.e., \(u^{S}(a)=1\) for \(a=1\) and \(u^{S}(a)=0\) for \(a=0\). The buyer only wants to purchase when the product is good, i.e., \(u^{R}(a,)=1\) if \(=a\), and \(u^{R}(a,)=0\) otherwise.

In order to persuade the buyer to purchase, the retailer can commit to performing (noisy) product inspections \((|)\) to reveal information about the product quality. For example, the inspection might signal the product quality is satisfactory with \(80\%\) chance if the quality of the product is indeed satisfactory (i.e., \((=1|=1)=0.8\)) and signal the product quality is unsatisfactory with \(90\%\) chance if the quality is indeed unsatisfactory (i.e., \((=0|=0)=0.9\)). The information design problem for the retailer is to identify an inspection policy that maximizes the probability on selling the product to the buyer.

## 3 HAIDNet: Encoding Human Behavior in Automated Information Design

In this section, we introduce HAIDNet, an optimization framework based on a neural network architecture that can adjust to various forms of human behavior. In the following discussion, we first describe how we modularize human behavior in information design. We then explain the neural network architecture of our proposed HAIDNet that can adapt to different forms of human behavior. Finally, we outline the procedures for optimizing the information policy in HAIDNet.

### Encoding Human Behavior

Bayesian persuasion assumes that the receiver is Bayesian rational. However, in practice, this assumption often does not hold. The receiver may exhibit systematic biases both in belief updating and in decision making. In the following discussion, we formulate the sender's problem on finding the optimal information policy when taking more general human behavior into account.

**Definition 3.1** (Human Behavior Descriptor).: For any receiver utility \(u^{R}\), prior \(\), sender information policy \(\) (and signal space \(\)), a human behavior descriptor is denoted by \(H_{u^{R},,}(,a)\), representing the probability for a human receiver to choose action \(a\) when seeing a realized signal \(\).

When the context is clear, we omit the subscripts and write \(H_{u^{R},,}(,a)\) as \(H(,a)\) for notational simplicity. With the above definition, we can rewrite the sender's information design problem as:

\[_{}_{}()(|)}{}H(,a)u^{S}(a, )\.\] ( \[^{H}\] )

Below we give a few examples of human behavior descriptors.

**Bayesian rational (BR).** In standard Bayesian persuasion, the receiver updates his posterior in a Bayesian manner and takes action that maximizes the expected utility. Following the definition in Section 2, the human descriptor can be written as \(H(,a)=a=a^{}()}\).

**Probability weighting and discrete choice (TH-Model).** We present another human behavior descriptor based on the work by Tang and Ho  (denoted as the TH-model in the description of this work). In particular, they combine probability weighting, assuming the receiver's posterior is distorted based on a function \(():()()\), and discrete choice model, assuming the receiver's action is stochastic, with a higher probability in taking an action with higher expected utility (based on the distorted posterior belief).

Formally, let \((|)\) be the receiver's distorted posterior belief after seeing signal \(\) and \(_{H}\) be a parameter in the discrete choice model that tunes how stochastic the receiver's action is (when \(_{H}\), the discrete choice model reduces to standard expected utility maximization), the human behavior descriptor for this model can be written as:

\[H(,a)=_{H}_{}(| )u^{R}(a,)}{_{a^{}}_{H}_{ }(|)u^{R}(a^{},)}\.\] (1)

**Data-driven human behavior descriptor.** Note that in our formulation, we use the function \(H(,a)\) to represent human behavior. Suppose we have access to sufficient human behavioral data, instead of expressing \(H(,a)\) using a closed-form analytical expression, we can train a machine learning model to approximate this function and utilize the learned model as the human behavior descriptor.

Figure 1: The overall HAIDNet framework. The human descriptor module is given to the optimization module before training. The optimization is performed through back propagation which evaluates the gradient of the loss to update the weights in the neural network structure.

### HAIDNet Framework and Optimization

We now introduce the framework of HAIDNet and explain how we utilize it to optimize the sender's information policy for a given human descriptor.

**HAIDNet framework.** As presented in Figure 1, HAIDNet consists of two modules: the sender's optimization module and the human descriptor. The sender's optimization module is a neural network responsible for optimizing the sender's optimal information policy. It takes the information design problem instances as input, including the prior distribution \(\) over the states and the payoff functions \(u^{S},u^{R}\) for all players. The module outputs an information policy which consists of a set of conditional probabilities \(\{(|)\}_{}\) over the signal space for each state \(\).

The human descriptor can either be model-based (e.g., Bayesian rational model or TH model in Equation (1)), or data-driven (e.g., a neural network modeling the receiver's behavior). The human descriptor is treated as a black box from the perspective of the sender's optimization module, and is fixed before HAIDNet begins training. The input of the descriptor consists of the receiver utility \(u^{R}\), the prior distribution \(\), and the information policy \(\) (i.e. the output of the sender's optimization module), while the output is the receiver's response strategy \(H(,a)=H_{u^{R},,}(,a)\).

**Optimization procedure.** For the sender's optimization, we follow the recent line of research on using deep learning for auction design : we randomly draw problem instances from a pre-specified distribution and perform stochastic gradient descent to minimize the loss function in the training process. The loss function is defined to be the negative of the sender's expected utility, since the goal of the sender is to find the optimal information policy that maximizes her expected utility.

\[_{u^{S},}(,H)=-_{}() _{}(|)_{a}H(,a)u^{S} (a,)\;.\] (2)

Our work differs from previous works in that we incorporate the human behavior descriptor in the definition of the loss function. The requirement is that the human descriptor \(H(,a)\) needs to be differentiable. This requirement is naturally satisfied in many cases, e.g., when the human descriptor follows the model defined in (1) or is a neural-network-based model, the gradient always exists. However, in the Bayesian rational model, since the receiver chooses the action that maximizes his expected utility, this _argmax_ operation makes the human model not differentiable. To overcome this issue, we approximate the Bayesian rational model using softmax instead of argmax with a sufficiently large softmax scale parameter \(\). 3 More concretely, let \(u(a)\) be the expected utility for action \(a\). The softmax operator approximates the receiver's behavior by using \(( u(a))/_{a^{}}( u(a^{}))\) to denote the probability of choosing action \(a\). As a sanity check, when \(\), this expression reduces to argmax, choosing the action maximizing the expected utility.

**Optimization implementation.** To optimize HAIDNet, we train a neural network with 3 fully connected layers employing ReLU activation functions and the Adam optimizer. The model is trained on 100 batches of size 1024, for a total of \(102,400\) uniformly drawn problem instances (i.e., data points for training). Evaluation of the model is conducted on a test set consisting of \(1000\) problem instances. The specification of hyperparameters and implementation details are included in the appendix. We have also included the source code in the supplementary materials.

## 4 Experiments

### Simulations

We have conducted extensive simulations to evaluate HAIDNet. Our results demonstrate that HAIDNet can find the near-optimal information policy in various settings. Specifically, we show its effectiveness in settings where efficient methods exist to obtain the optimal information policy (Section 4.1.1) and in computationally challenging settings where finding the optimal information policy is difficult (Section 4.1.2). Moreover, even in settings where no known solutions exist in general, HAIDNet can generate information policy with good performance (Section 4.1.3). We have conducted additional simulations, including examining the convergence of the training, investigatingthe scalability of the approach, accounting for varying number of receivers, comparing with random policy, and examining empirical run-time. Due to space constraints, these additional simulation results are included in Appendix B.1.

#### 4.1.1 Settings with efficient solutions

We start our evaluations with a simple setting where there exist efficient solutions to find the optimal policy. In this setting, we leverage the efficient solutions as ground truth to examine whether our approach can also identify the optimal information policy.

In particular, we consider the setting with a single Bayesian rational receiver. In this setting, when there are only two actions available for the receiver and there are only two states, there exists a closed-form characterization of the optimal information policy. When the numbers of actions and states are finite constants, the optimal information policy can still be computed efficiently . Therefore, we can evaluate the performance of our approach by comparing the information policy generated by HAIDNet with the optimal policy.

**Binary actions and binary states.** We first examine the simplest setting with binary actions and binary states (a classical setting in Bayesian persuasion ), namely, the action space \(=\{0,1\}\) and the state space \(=\{0,1\}\), and observe whether HAIDNet produces near-optimal information policies. For the sender utility, we adopt a stylized setting where the sender obtains utility \(1\) when the receiver takes action \(1\) and utility \(0\) when the receiver takes action \(0\). The receiver aims to take the action that aligns with the true state, i.e., \(u^{R}(0,1)=u^{R}(1,0)=0\), and we randomly draw each value for \(u^{R}(0,0)\) and \(u^{R}(1,1)\) from \(\). In plain words, the receiver prefers action 1 when the state is 1 and action 0 when the state is 0, and the goal of the sender is to persuade the receiver to take action 1. The prior distribution \(\) is drawn from a Dirichlet distribution. We then simulate data using the setting above and optimize HAIDNet.

We first examine whether the policy generated by HAIDNet matches the known optimal policy. Note that in this simple setting, via revelation principle , an information policy can be characterized by two signals, i.e., \(\{0,1\}\), where each signal corresponds to a recommended action. Moreover, in the optimal policy, we have \(^{*}(=1|=1)=1\), and therefore the optimal policy can be characterized by a single parameter \(^{*}(=1|=0)\). To examine whether HAIDNet generates the same policy as the optimal policy, we compare the value of this parameter on different scenarios.

To showcase our results, we present two settings where we have fixed prior distributions: low prior with \((=0)=0.3\) and medium prior with \((=0)=0.5\). 4 For each prior distribution, we vary the receiver utilities and report the parameter \(^{*}(=1|=0)\) both from the optimal policy and from the output of HAIDNet. As visualized in Figure 2, the policy learned by HAIDNet essentially recovers the optimal information policy in almost all scenarios.

**Multiple actions and multiple states.** To examine whether our approach scales with the size of the problem instances, we increase the number of states and the number of actions5. The performance is measured using the average sender utility. We report both the training performance (e.g., average sender utility for 1,000 instances drawn from instances used for training HAIDNet) and testing performance (e.g., average sender utility for newly drawn 1,000 instances).6 The results, as shown

Figure 2: Comparing the optimal information policy and the policy generated by HAIDNet in the setting with binary actions and binary states.

in Table 1, demonstrate that our approach works well for large-scale problem instances and also generalizes well to instances not used in training.

#### 4.1.2 Settings without efficient solutions

Next, we examine the performance of HAIDNet under the setting where there are no known computationally efficient solutions to characterize the optimal information policy. The goal is to illustrate that HAIDNet performs well even in complicated scenarios and could provide a more efficient approach for settings without analytically tractable solutions.

We consider the setting with multiple receivers and binary actions. The goal is to design a uniform information policy for all receivers (i.e., _public persuasion_). This setting has been shown to be #P-hard to find a policy that approximates the optimal sender utility within any constant multiplicative factor . This means that, unlike the single receiver case, finding the optimal solution for a given problem is practically impossible to solve with a large set of receivers, and we intend HAIDNet to be a new, efficient solver for near-optimal solutions. To examine whether HAIDNet finds the optimal policy, we utilize a brute-force linear-programming approach  (the time complexity is exponential in the number of receivers since the number of constraints in the program grows exponentially) to identify the optimal policy when the number of receivers is small. We then compare the information policy generated by HAIDNet and the optimal policy output from the linear programming approach. The receiver utility and prior distributions are generated in the same way as in the single receiver setting. The sender utility is the fraction of receivers choosing action \(1\), i.e., her utility is given \(\) if there are \(|S|\) receivers choosing action \(1\) out of a total \(K\) receivers.

The simulation results are shown in Table 2. We randomly draw \(1,000\) problem instances from the training/testing set and report the average performance of the optimal policy and the HAIDNet policy. As we can see in the results, the performance of the information policy output from HAIDNet is near-optimal. Moreover, HAIDNet provides a much more efficient approach when the number of receivers is large. As a comparison, solving the exact optimal information policy for each problem instance is time-consuming (e.g., it takes more than 23 hours to solve an instance with 18 receivers). On the other hand, HAIDNet only needs to optimize the model once to generate the optimal information policies for all possible problem instances with the same number of receivers (e.g., training HAIDNet with 18 receivers takes slightly more than 1 hour, and generating information policy for a problem instance takes less than 1 second). The empirical run-time comparison is included in the appendix.

   &  &  \\   & HAIDNet & Optimal & HAIDNet & Optimal \\ 
2 & 0.7409 & 0.7498 & 0.7408 & 0.7451 \\
3 & 0.7737 & 0.7782 & 0.7598 & 0.7669 \\
5 & 0.8171 & 0.8209 & 0.8066 & 0.8225 \\
10 & 0.8495 & 0.8699 & 0.8196 & 0.8686 \\   
   &  &  \\   & HAIDNet & Optimal & HAIDNet & Optimal \\ 
2 & 0.7409 & 0.7498 & 0.7408 & 0.7451 \\
3 & 0.7017 & 0.7214 & 0.7089 & 0.7227 \\
5 & 0.6906 & 0.7113 & 0.6690 & 0.7064 \\
10 & 0.6861 & 0.7084 & 0.6623 & 0.6963 \\  

Table 1: Comparing the average sender utility generated by the optimal policy and the policy from HAIDNet in the setting with a single Bayesian rational receiver.

   &  &  \\   & HAIDNet & Optimal & HAIDNet & Optimal \\ 
2 & 0.7887 & 0.7934 & 0.7756 & 0.7873 \\
3 & 0.7508 & 0.7665 & 0.7379 & 0.7573 \\
5 & 0.7217 & 0.7458 & 0.7209 & 0.7570 \\
10 & 0.6971 & 0.7152 & 0.6790 & 0.6966 \\
15 & 0.6553 & 0.6882 & 0.6621 & 0.6843 \\   
   &  &  \\   & HAIDNet & Optimal & HAIDNet & Optimal \\ 
1 & 0.5043 & 0.5051 & 0.5041 & 0.5060 \\
5 & 0.5512 & 0.5557 & 0.5506 & 0.5559 \\
10 & 0.6045 & 0.6170 & 0.5986 & 0.6168 \\
50 & 0.7002 & 0.7134 & 0.6800 & 0.7081 \\
100 & 0.7187 & 0.7291 & 0.6964 & 0.7179 \\  

Table 2: Comparing the average sender utility generated by the optimal policy and the policy from HAIDNet in the setting with \(K\) Bayesian rational receivers.

  \)} &  &  \\   & HAIDNet & Optimal & HAIDNet & Optimal \\ 
1 & 0.5043 & 0.5051 & 0.5041 & 0.5060 \\
5 & 0.5512 & 0.5557 & 0.5506 & 0.5559 \\
10 & 0.6045 & 0.6170 & 0.5986 & 0.6168 \\
50 & 0.7002 & 0.7134 & 0.6800 & 0.7081 \\
100 & 0.7187 & 0.7291 & 0.6964 & 0.7179 \\  

Table 3: Comparing the average sender utility by the optimal policy and the policy from HAIDNet in the setting with a non-Bayesian-rational receiver parameterized by \(_{H}\).

#### 4.1.3 Settings without known solutions

We now examine the performance of HAIDNet in settings where there are generally no known analytical solutions yet. The goal is to showcase that HAIDNet can be leveraged to address information design problems when we do not have access to solutions.

All our simulations so far have focused on settings which assume that receivers are Bayesian rational. To examine whether HAIDNet works for non-Bayesian-rational receivers, we adopt a relaxation of human behavioral formulation as in Equation (1). While there are no known solutions for identifying the optimal policy in this setting in general, Tang and Ho  derived a solution for the simple setting with binary actions and binary states. Therefore, we compare the performance of the optimal policy and the HAIDNet policy in this simple setting under different choices of \(_{H}\) in the human descriptor in Equation (1). Using the same setup as in previous simulations, we report the results in Table 3, showing that HAIDNet works even for a non-Bayesian-rational receiver.

Next, we would like to examine how HAIDNet performs in scenarios when there are no known solutions (e.g., in settings with more than binary actions/states). To demonstrate the results, we choose the setting with three states and three actions. The lack of an optimal solution means we cannot evaluate the performance of HAIDNet by comparing its performance with the optimal policy as in the simulations above. Instead, we take a different method and provide evidence to support our approach: We evaluate the set of all learned policies \(_{_{H}}\) against each of the human models \(_{H}\).

For each human model \(_{H}=k\), if \(_{k}\) is the best-performing policy, this indicates that our approach generates a reasonably good information policy. Specifically, for each human model, we compute the performance of each policy available, and we then normalize the set of these performances so that the best-performing performance for each human model has value \(1\). If our HAIDNet indeed learns a good information policy, we would expect the best performing HAIDNet to be the one trained on the right human descriptor. The results, as shown in Figure 3, demonstrate this behavior and provides evidence that our HAIDNet generates good information policy even when the receiver is not Bayesian rational.

### Real-World Human-Subject Experiments

In the simulations, we have assumed access to a closed-form behavior model of the receiver. However, in practice, human behavior is complex and there may not exist a single model that can perfectly represent human behavior. Motivated by this practical concern, we conduct human-subject experiments to examine whether HAIDNet adapts to real-world human behavior. The goal is to examine whether we can utilize data-driven approaches to learn human-behavior descriptors and examine whether HAIDNet performs well when it is paired with data-driven behavior descriptors.

**Task description.** In our human-subject experiments, we present the product purchasing example in Section 2 to human participants. Each human participant is asked to make multiple rounds of purchase decisions. In each round, the participant is presented a product with unknown binary quality (good or bad product). The participant is told that a (noisy) inspection has been performed on the product, and is given the conditional distribution associated with the inspection (i.e., the probability to receive a good/bad signal given the product is good/bad). Finally, the participant is given a realization of the inspection signal and is asked to make a binary decision of purchasing or not. The participant's payment depends on both their purchasing decisions and the true product quality. The task interface is included in Appendix C. The experiment is approved by the IRB in our institution.

Figure 3: The performance of HAIDNet in settings when the receiver is not Bayesian rational. We train HAIDNet with non-Bayesian-rational receiver model parameterized by \(_{H}\), then evaluate the learned information policy for all receiver models. The performance is normalized so for each human model, the optimal performance is \(1.0\) among all policies.

**Experiment procedure.** We have recruited 300 workers from Amazon Mechanical Turk. We set the base payment to be \(\$0.50\). Workers could earn additional bonuses depending on their performance. The average hourly rate was around \(\$11\) USD. The experiment contains two phases as described next.

**Learning human behavior descriptors.** The goal of the first phase is to examine whether we could learn accurate human behavior descriptors from worker's response data. In this phase, we recruited 100 workers, and each worker completed 20 rounds of product purchasing decisions. The parameters of each decision (prior, sender utility, receiver utility, and policy) was drawn uniformly at random. We split the collected data into training/test sets, with 80% of the data for training, and 20% for testing. We trained and examined the performance of three different human behavior descriptors.

* Bayesian rational: This descriptor makes the standard assumption that humans are Bayesian rational. There is no training needed for this descriptor.
* TH-Model: We fit the parameters of the TH model, as described in Section 3.1, from data to minimize the least squares error.
* Neural network: We use a 3 fully connected-layer neural network to fit the data in the training set. We further split the training dataset and use 25% of the data as the validation set to implement early-stopping during training.

We then examine how accurately each descriptor predicts human behavior in the test data. The test accuracy is reported in Table 4. As we can see from the results, the data-driven neural network model leads to the best prediction accuracy, and both TH-Model and the data-driven descriptor significantly outperform the Bayesian rational assumption, reaffirming the need to relax this common assumption.

**Evaluating HAIDNet.** In the second phase, we recruited 200 workers to examine the performance of different information policies. In particular, we examine the following four information policies:

* Random: This information policy is drawn from a Dirichlet distribution.
* BR-policy: The optimal policy when the receiver is a Bayesian rational receiver.
* TH-policy: The optimal policy when the receiver behavior follows the TH-Model, as in Section 3.1.
* HAIDNet: The policy by HAIDNet when we use the neural network learned from the first phase as the human model.

When each worker arrives, they are randomly assigned to one of these four policy treatments. They are then presented with 20 rounds of purchase decisions (the parameters of each round are randomly drawn from distributions fixed across all treatments) coupled with the associated information policy in the treatment. We then measure the average sender utility in each treatment. The results, as shown in Figure 4, demonstrate that HAIDNet achieves the best performance. The results showcase the effectiveness of HAIDNet coupled with data-driven human behavior descriptors.

In addition to examining the sender's utility, we also measure the receiver's utility in each treatment (see the full information in Appendix C). We observe that, the policy of HAIDNet leads to an average receiver utility of 0.532, which is the lowest of all four treatments. This creates the concern that when we incorporate the knowledge of receiver behavior to optimize the sender's utility in information design, we are potentially exploiting the knowledge of receiver behavior and hurting the receiver. We offer more discussion on this concern in the discussion section of the paper.

  Model & Bayesian rational & TH-Model & Neural network \\  Test Accuracy & 0.562 & 0.735 & **0.770** \\  

Table 4: Test accuracy of different human behavior descriptors in human-subject experiments.

Figure 4: Average sender utility of different policies in human-subject experiments. The differences between BR-policy and TH-policy and between BR-policy and HAIDNet are statistically significant (\(p<0.01\)).

Conclusion and Discussion

We initiate the study of behavioral information design that encodes human behavior into the design process. We propose HAIDNet, a neural-network-based optimization framework for information design that can adjust to multiple forms of human behavior. Through extensive simulations and human-subject experiments, we demonstrate the effectiveness of HAIDNet in response to different human behavior descriptors. Below we discuss the generalization / limitations and highlight the potential social impacts of this work.

**Generalization and limitations.** While this work has focused on integrating human behavior in automated information design, we believe the methodologies are generalizable to design mechanisms for general human-in-the-loop systems, explicitly encoding realistic human behavior and/or human responses to the system when designing the system. Moreover, our current investigations have adopted the most standard deep learning setup (e.g., full-connected neural networks coupled with stochastic gradient descent). It would be interesting to examine whether the performance could be further improved with carefully crafted network architecture and optimization procedure.

We would like to note the potential limitations of our approach. Our optimization procedure, based on applying stochastic gradient descent on neural networks, does not guarantee to lead to globally optimal solutions in general. Moreover, compared with analytical solutions that are guaranteed to be optimal for all problem instances if the receiver behavior follows the assumption, HAIDNet is a data-driven approach that optimizes the _expected_ utility, which requires training data to be representative to ensure generalizability. While our results suggest that HAIDNet recovers the near-optimal policy (e.g., the results in Figure 2), examining the impacts of different training data distributions and whether the results are robust to distributional shifts are potential important future research directions.

Another limitation pertains to the scalability of our proposed approach. While our method exhibits better scalability than exact solvers that utilize linear programming (more detailed discussion is included in Appendix B.1), our current results primarily focus on discrete action/state spaces. As the number of states and actions expands, so does the input size for HAIDNet. It could require much more training iterations to reach convergence. Furthermore, in scenarios with continuous action/state spaces, our approach is not immediately applicable. While discretization might be employed to address the setting with continuous spaces, such an approach requires additional smoothness assumptions to ensure small discretization errors. Overall, understanding and improving the scalability of HAIDNet is an important next step for increasing its practical applicability.

**Potential negative social impacts.** Finally, we highlight the potential negative social implications of the usage of information design frameworks. In information design, the sender often represents the party in power (e.g., the government, social networking platforms), while the receiver is in a less advantageous position (e.g., the general public, users) due to the asymmetry of information access. While it is possible to use information design for social good, guiding the receiver towards actions that are beneficial for himself or the public, the vast majority of information design literature -- including our work -- focuses on optimizing the sender's utility. When the interests of the sender and receiver are not aligned, optimizing the sender's utility could result in a negative impact on the receivers, who are often the general public. In other words, with an ill-specified objective in information design, the sender could exploit the information advantage and create significant negative social impacts. This concern is further amplified when we obtain more accurate knowledge about the receiver. It is therefore important to consider the impacts and potential regulations on information design.

In light of the concerns raised, to initiate the discussion, we discuss two potential risk mitigation methods. Firstly, on the technical front, we could employ differential privacy techniques [22; 21] to control the amount of private human behavior being incorporated into receiver models. Differential privacy provides a means to balance privacy with utility, typically by introducing controlled noise into the data. This mechanism might be helpful in mitigating the exploitation of marginalized groups, an issue that might be exhibited in our approach. Secondly, from a policy perspective, once we develop a comprehensive understanding of the capabilities of information design with data-driven human models, we, as a society, could and should weigh the utility gains from this method against potential harm. This discussion could then pave the way for the development of regulations and policies for deploying information design. For instance, we might impose constraints ensuring that the deployed information policy does not significantly reduce receiver utility, especially when compared to policies designed assuming standard models such as Bayesian rationality.