# TP\({}^{2}\)Dp\({}^{2}\): A Bayesian Mixture Model of Temporal Point Processes with Determinantal Point Process Prior

TP\({}^{2}\)Dp\({}^{2}\): A Bayesian Mixture Model of Temporal Point Processes with Determinantal Point Process Prior

 Yiwei Dong

Renmin University of China

&Shaoxin Ye

Renmin University of China

&Yuwen Cao

Renmin University of China

&Qiyu Han

Renmin University of China

&Hongteng Xu

Renmin University of China

&Hanfang Yang

Renmin University of China

Corresponding authors.

Renmin University of China

###### Abstract

Asynchronous event sequence clustering aims to group similar event sequences in an unsupervised manner. Mixture models of temporal point processes have been proposed to solve this problem, but they often suffer from overfitting, leading to excessive cluster generation with a lack of diversity. To overcome these limitations, we propose a Bayesian mixture model of **T**emporal **P**oint **P**rocesses with **D**eterminantal **P**oint **P**rocesses Prior (**TP\({}^{2}\)DP\({}^{2}\)**) and accordingly an efficient posterior inference algorithm based on conditional Gibbs sampling. Our work provides a flexible learning framework for event sequence clustering, enabling automatic identification of the potential number of clusters and accurate grouping of sequences with similar features. It is applicable to a wide range of parametric temporal point processes, including neural network-based models. Experimental results on both synthetic and real-world data suggest that our framework could produce moderately fewer yet more diverse mixture components, and achieve outstanding results across multiple evaluation metrics.

## 1 Introduction

As a powerful tool of asynchronous event sequence modeling, the temporal point process (TPP) plays a crucial role in many application scenarios . In practice, event sequences often demonstrate clustering characteristics, with certain sequences showcasing greater similarities when compared with others. For instance, event sequences of patient admissions may exhibit clustering patterns in response to specific medical treatments. Being able to accurately cluster event sequences can bring many benefits, including facilitating healthcare decision making. In recent years, researchers have built mixture models of TPPs to tackle the event sequence clustering problem . However, these models often suffer from overfitting during training, leading to excessive cluster generation with a lack of diversity. Moreover, these methods require either manually setting the number of clusters in advance  or initializing a large number of clusters and gradually removing excessive clusters through hard thresholding . In addition, without imposing proper prior knowledge, the clusters obtained by these models may have limited diversity and cause the identifiability issue.

In this study, we propose a novel Bayesian mixture model of temporal point processes named TP\({}^{2}\)DP\({}^{2}\) for event sequence clustering, imposing a determinantal point process prior to enhance the diversity of clusters and developing a universally applicable conditional Gibbs sampler-based algorithm for the model's posterior inference. As illustrated in Figure 1, TP\({}^{2}\)DP\({}^{2}\) leverages the determinantal point process (DPP) as a repulsive prior for the parameters of cluster components,which contributes to generating TPPs with diverse parameters. To make TP\({}^{2}\)DP\({}^{2}\) applicable for the TPPs with a large number of parameters, we apply Bayesian subnetwork inference , employing Bayesian inference to partially selected parameters while utilizing maximum likelihood estimation for the remaining parameters. For selected parameters, we further categorize them into central and non-central parameters, in which the central parameters mainly determine the clustering structure and thus we apply DPP priors. We design an efficient conditional Gibbs sampler-based posterior inference algorithm, in which the stochastic gradient Langevin dynamics  is introduced into the updating process to facilitate convergence. To our knowledge, TP\({}^{2}\)DP\({}^{2}\) is the first work that explores event sequence clustering based on the TPP mixture model with DPP prior. It automatically identifies cluster numbers,with clustering results more reliable than existing variational inference methods [23; 27].

## 2 Preliminaries

**Temporal Point Processes** TPP is a kind of stochastic process that characterizes the random occurrence of events in multiple dimensions, whose realizations can be represented as event sequences, i.e., \(\{(t_{i},d_{i})\}_{i=1}^{I}\), where \(t_{i}[0,T]\) are time stamps and \(d_{i}=\{1,...,D\}\) are different dimensions (a.k.a. event types). Typically, we characterize a TPP by conditional intensity functions:

\[^{*}(t)=_{d=1}^{D}_{d}^{*}(t),\;\; _{d}^{*}(t)t=[N_{d}(t)_{ t}].\] (1)

Here, \(_{d}^{*}(t)\) is the conditional intensity function of the type-\(d\) event at time \(t\), \(N_{d}(t)\) denotes the number of the occurred type-\(d\) events prior to time \(t\), and \(_{t}\) denotes the historical events happening before time \(t\). Given an event sequence \(=\{(t_{i},d_{i})\}_{i=1}^{I}\), the likelihood function of a TPP can be derived based on its conditional intensity functions:

\[()=\;_{i=1}^{I}_{d_{i}}^{*}(t_{i}) -_{0}^{T}^{*}()d.\] (2)

By maximizing the likelihood in Eq. (2), we can learn the TPP model to fit the observed sequence.

**Mixture Models of TPPs** Given multiple event sequences belonging to different clusters, i.e., \(\{_{n}\}_{n=1}^{N}\), we often leverage a mixture model of TPPs to describe their generative mechanism, leading to a hierarchical sampling process:

1) Determine cluster: \(m(),2)\) Sample sequence: \((_{m}),\) (3)

where \(=[_{1},...,_{M}]^{M-1}\) indicates the distribution of clusters defined on the \((M-1)\)-simplex, \((_{m})\) is the TPP model of the \(m\)-th cluster, whose parameters are denoted as \(_{m}\).

**Determinantal Point Processes** DPP  is a stochastic point process characterized by the unique property that its sample sets exhibit determinantal correlation. The structure of DPP is captured through a kernel function [14; 4]. Denote the kernel function by \(:\), where \(\) represents a sample space. The density function for samples \(x_{1},...,x_{M}\) in one realization of DPP is:

\[p(x_{1},...,x_{M})\{(x_{1},...,x_{M})\},\] (4)

Figure 1: The pipeline of TP\({}^{2}\)DP\({}^{2}\).

where \((x_{1},...,x_{M})=[(x_{i},x_{j})]\) is a \(M M\) Gram matrix corresponding to the samples. Given arbitrary two samples \(x_{i}\) and \(x_{j}\), we have \(p(x_{i},x_{j})=(x_{i},x_{i})(x_{j},x_{j})-(x_{i},x_{j})^{2}=p( x_{i})p(x_{j})-(x_{i},x_{j})^{2} p(x_{i})p(x_{j})\). Therefore, DPP manifests the repulsion between \(x_{i}\) and \(x_{j}\). As such, using DPP as the prior can help enhance the diversity of clustering results.

## 3 Proposed TP\({}^{2}\)DP\({}^{2}\) Model & Corresponding Posterior Inference Algorithm

The mixture model in Eq. (3) reveals that each event sequence \(\) obeys a mixture density, i.e., \(_{m=1}^{M}_{m}(_{m})\), where \(M\) is a random variable denoting the number of clusters, \(=[_{1},...,_{M}]^{M-1}\) specifies the probability of each cluster component (a TPP), and \((_{m})\) is the likelihood of the \(m\)-th TPP parametrized by \(_{m}\). Given \(N\) event sequences \(=\{_{n}\}_{n=1}^{N}\), we denote cluster allocation variables of each sequence \(=[c_{1},...,c_{N}]\{1,...,M\}^{N}\), where each set \(\{_{n} c_{n}=m\}\) contains the sequences assigned to the \(m\)-th cluster. Accordingly, we derive the joint distribution of all variables, i.e., \(p(M,,,,)\), as

\[p(M)p( M)p( M))p( ,)}_{_{n=1}^{N}_{m_{n}}(_{n}}|_{c_{n}})},\] (5)

where \(=\{_{m}\}_{m=1}^{M}^{P}\). \(p(M)\), \(p( M)\) and \(p( M)\) are prior distributions of \(M\), \(\) and \(\), respectively. By Bayes theorem, the posterior \(p(M,,,)\) is proportional to Eq. (5).

The exact sampling from \(p(M,,,)\) is often intractable because the parameters of the TPPs in practice (especially those neural TPPs [15; 8; 26; 29; 16]) are too many to perform full Bayesian posterior calculation. To overcome this issue, we conduct posterior inference only on a subset of model parameters [6; 18; 12] (i.e., the "subnetwork" of the whole model). In particular, we approximate the full posterior of the TPPs' parameters \(\) as

\[p() p(_{S}) (_{R}-}_{R})=p()p( )(_{R}-}_{R}),\] (6)

where we split the model parameters \(\) into two parts, i.e., \(_{S}\) and \(_{R}\), respectively. \(_{S}=\{_{S,m}\}_{m=1}^{M}\) corresponds to the subnetworks of the TPPs in the mixture model, while \(_{R}=\{_{R,m}\}_{m=1}^{M}\) denotes the remaining parameters. In Eq. (6), \(p()\) is decomposed into the posterior of the subnetworks \(p(_{S})\) and a Dirac delta function on the remaining parameters \(_{R}\), in which \(_{R}\) is estimated by their point estimation \(}_{R}=\{}_{R,m}\}_{m=1}^{M}\), e.g., the maximum likelihood estimation achieved by stochastic gradient descent.

Unlike existing work, in Eq. (6), we further decompose the parameters in the subnetworks into two parts, i.e., \(_{S}=\{,\}\), where \(=\{_{m}\}_{m=1}^{M}\) and \(=\{_{m}\}_{m=1}^{M}\), respectively. For the \(m\)-th TPP in the mixture model, \(_{m}\) corresponds to the "central" parameters of their conditional intensity functions, which significantly impacts the overall dynamics of event occurrence (e.g. the base intensity of Hawkes process ). Accordingly, the other "non-central" parameters in each subnetwork are denoted as \(_{m}\), which are contingent upon specific architectures of different models. Imposing the conditional independence on the central and non-central parameters, i.e., \(p(_{S}|M)=p(|M)p(|M)\), we have

\[p(M,,,|) p(M)p(|M)p(|M)p( {}|M)_{n=1}^{N}_{c_{n}}(_{n}|_ {S,c_{n}},}_{R,c_{n}}),\] (7)

where \(}_{R,c_{n}}\) denotes the point estimates of the remaining parameters in the \(c_{n}\)-th TPP. The DPP prior \(p( M)\) is introduced to the central parameter \(\) to mitigate the overfitting problem and diversify the cluster result. The computational method of DPP prior construction is introduced in Appendix. \(p( M)=_{m=1}^{M}p(_{m})\) is the prior of non-central parameters which can be Gaussian. For prior of \(\), instead of directly sampling \(\{_{m}\}_{m=1}^{M}\) from its posterior distribution, we apply the ancillary variable method [3; 1] to make the posterior calculation tractable for the mixture weights \(\{_{m}\}_{m=1}^{M}\). Consider \(=[r_{1},...,r_{M}]\), which consists of i.i.d. positive continuous random variables following the Gamma distribution \((1,1)\), each \(r_{m}\) is independent of \(M\) and \(\) is independent of \(\{,\}\). Defining \(t=_{m=1}^{M}r_{m}\) and \(=[r_{1}/t,...,r_{M}/t]\), we establish a one-to-one correspondence between \(\) and \((,t)\). By introducing an extra random variable \(v(N,1)\), we define the ancillary variable \(u=v/t\), with \(p(u)=}{(N)}_{0}^{}t^{N}e^{-ut}p(t)t\). Introducing \(u\) makesthe posterior computation of \(\) factorizable and gets rid of the sum-to-one constraint imposed on \(\{_{m}\}_{m=1}^{M}\), significantly simplifying the subsequent MCMC simulation process.

In summary, the joint posterior density function becomes

\[p(M,,,,u) p()_{m=1}^{M}p(_{m})p(r_{m})_{n=1}^{N}_{c_{n}}(_{n}_{S,c_{n}},}_{R,c_{n}})},\] (8)

where \(p():=p(M)p( M)\) is the DPP prior.

Since the number of clusters changes dynamically as these algorithms proceed, it is helpful to further partition model parameters into parameters of allocated clusters and those of non-allocated clusters when applying posterior sampling. In particular, we partition \(\) into two sets according to cluster allocations \(\): one comprising cluster centers currently used for data allocation, denoted as \(\{^{(a)}=\{_{c_{1}},,_{c_{n}}\}\), and the other containing cluster centers not involved in the allocation, denoted as \(^{(na)}=^{(a)}\). Note that the product measure \(\) in \(\) lifted by the map \((,)\) results in the measure \(\), so the prior density of \((^{(a)},^{(na)})\) is equivalent to \(p(^{(a)},^{(na)})=p(^{(a)}^{(na)})\), which follows the DPP density. \(\) and \(\) are partitioned in the same way. As \((,,,)\) and \((^{(a)},^{(a)},^{(a)},^{(na)},^{(na)},^{( na)},)\) are in a one-to-one correspondence,we can refer to Eq. (8) and obtain the posterior of \((M,^{(a)},^{(a)},^{(a)},,^{(na)},^{(na)}, ^{(na)},u)\):

\[p(M,^{(a)},^{(a)},^{(a)},,^{(na)}, ^{(na)},^{(na)},u|)\] (9) \[ p(^{(a)}^{(na)})_{m=1}^{k}p(_{ m}^{(a)})p(r_{m}^{(a)})(r_{m}^{(a)})^{n_{m}}_{i:c_{i}=m}(_{i} _{m}^{(a)},_{m}^{(a)},}_{R,m})\] \[_{m=1}^{l}p(_{m}^{(na)})p(r_{m}^{(na)}) p(u t)},\]

where \(n_{m}\) is the number of sequences allocated to the \(m\)-th component, \(k\) denotes the cardinality of allocated clusters, and \(l\) denotes that of non-allocated ones, \(r_{m}^{(a)}\) and \(r_{m}^{(na)}\) denote the allocated and non-allocated unnormalized weight, respectively. \(p(u t)=}{(N-1)!}e^{-ut}t^{N}\).

Our posterior inference algorithm follows the principle of conditional Gibbs sampler . We split all parameters into three groups: an allocated block \((^{(a)},^{(a)},^{(a)})\), a non-allocated block \((^{(na)},^{(na)},^{(na)})\), and remaining parameters \(\{,u\}\), and update them in an alternating scheme. The posterior sampling procedure of TP\({}^{2}\)DP\({}^{2}\) is summarized in Algorithm 1. More detailed derivations are elaborated in Appendix.

## 4 Experiments

Our model is compatible with various TPP backbones, which can detect clusters and fit event sequence data originating from a mixture of hybrid TPPs. To verify our claim, we generate a set of event sequences based on five different TPPs, including 1) Homogeneous Poisson process, 2) Inhomogeneous Poisson process, 3) Self-correcting process, 4) Hawkes process, and 5) Neural Hawkes process . Based on the sequences, we construct three datasets with the number of mixture components ranging from three to five. For each dataset, we learn a mixture model of TPPs and set the backbone of the TPPs to be 1) the classic Hawkes process , 2) the recurrent marked temporal point process (RMTPP) , and 3) the Transformer Hawkes process (THP) , respectively. The learning methods include the variational EM of Dirichlet mixture model (Dirichlet Mixture)  and our TP\({}^{2}\)DP\({}^{2}\). The results in Table 1 show that our method achieves competitive performance. Especially when the backbone is Hawkes process, applying our method leads to notable improvements in purity  and ARI , which means that our method is more robust to the model misspecification issue. In addition, learning RMTPP and THP by our method results in the best performance when \(K_{GT}=5\), showcasing TP\({}^{2}\)DP\({}^{2}\)'s adaptability to complex event sequences. More experiments are in Appendix.

## 5 Conclusion

In this paper, we propose the Bayesian mixture model TP\({}^{2}\)DP\({}^{2}\) for event sequence clustering. It is shown that TP\({}^{2}\)DP\({}^{2}\) could flexibly integrate various parametric TPPs including the neural network-based TPPs as components, achieve satisfying event sequence clustering results and produce more separated clusters. In the future, we plan to study the impact of alternative repulsive priors on event sequence clustering, and develop event sequence clustering methods in high-dimensional and spatio-temporal scenarios.

    &  & =3\)} & =4\)} & =5\)} \\  & &  & ARI &  & ARI &  & ARI \\   & Dirichlet Mixture & 0.678\({}_{ 0.134}\) & 0.622\({}_{ 0.097}\) & 0.620\({}_{ 0.120}\) & 0.564\({}_{ 0.126}\) & 0.574\({}_{ 0.045}\) & **0.545\({}_{ 0.046}\)** \\  & TP\({}^{2}\)DP\({}^{2}\) & **0.884\({}_{ 0.099}\)** & **0.745\({}_{ 0.052}\)** & **0.739\({}_{ 0.004}\)** & **0.626\({}_{ 0.008}\)** & **0.603\({}_{ 0.008}\)** & 0.538\({}_{ 0.013}\) \\   & Dirichlet Mixture & **0.983\({}_{ 0.112}\)** & **0.972\({}_{ 0.124}\)** & 0.751\({}_{ 0.131}\) & **0.712\({}_{ 0.123}\)** & 0.708\({}_{ 0.030}\) & 0.633\({}_{ 0.027}\) \\  & TP\({}^{2}\)DP\({}^{2}\) & 0.974\({}_{ 0.073}\) & 0.971\({}_{ 0.109}\) & **0.753\({}_{ 0.003}\)** & 0.708\({}_{ 0.014}\) & **0.732\({}_{ 0.024}\)** & **0.674\({}_{ 0.017}\)** \\   & Dirichlet Mixture & 0.941\({}_{ 0.093}\) & 0.870\({}_{ 0.201}\) & 0.746\({}_{ 0.007}\) & **0.666\({}_{ 0.088}\)** & 0.610\({}_{ 0.007}\) & 0.559\({}_{ 0.043}\) \\  & TP\({}^{2}\)DP\({}^{2}\) & **0.980\({}_{ 0.035}\)** & **0.897\({}_{ 0.110}\)** & **0.749\({}_{ 0.002}\)** & 0.652\({}_{ 0.007}\) & **0.650\({}_{ 0.007}\)** & **0.600\({}_{ 0.020}\)** \\   

Table 1: Experimental results on synthetic mixture of hybrid point processes datasets.