# Scaling Gaussian Processes for Learning Curve Prediction via Latent Kronecker Structure

Jihao Andreas Lin\({}^{*}\)\({}^{1}\)\({}^{,}\)\({}^{2}\)\({}^{3}\)  Sebastian Ament\({}^{1}\)  Maximilian Balandat\({}^{1}\)  Eytan Bakshy\({}^{1}\)

\({}^{1}\)Meta \({}^{2}\)University of Cambridge \({}^{3}\)Max Planck Institute for Intelligent Systems

Work done during an internship at Meta. Correspondence to: jal232@cam.ac.uk

###### Abstract

A key task in AutoML is to model learning curves of machine learning models jointly as a function of model hyper-parameters and training progression. While Gaussian processes (GPs) are suitable for this task, naive GPs require \((n^{3}m^{3})\) time and \((n^{2}m^{2})\) space for \(n\) hyper-parameter configurations and \((m)\) learning curve observations per hyper-parameter. Efficient inference via Kronecker structure is typically incompatible with early-stopping due to missing learning curve values. We impose _latent Kronecker structure_ to leverage efficient product kernels while handling missing values. In particular, we interpret the joint covariance matrix of observed values as the projection of a latent Kronecker product. Combined with iterative linear solvers and structured matrix-vector multiplication, our method only requires \((n^{3}+m^{3})\) time and \((n^{2}+m^{2})\) space. We show that our GP model can match the performance of a Transformer on a learning curve prediction task.

## 1 Introduction

Most modern machine learning (ML) models are trained with iterative methods, giving rise to learning curves which enable an analysis of the model quality as the training progresses. Being able to predict learning curves accurately based on results from partial training facilitates decisions about whether to continue training or to stop early, such that compute resources can be used more efficiently. This can substantially accelerate model exploration and improve the efficiency of AutoML algorithms.

Existing work on learning curve prediction considered Gaussian processes (GPs) (Swersky et al., 2014; Klein et al., 2020; Wistuba et al., 2022), parametric functions (Domhan et al., 2015; Klein et al., 2017; Kadra et al., 2023), and Transformers (Adriaensen et al., 2023; Rakotoarison et al., 2024) (see Appendix A for details). In this paper, we revisit the idea of a joint GP over hyper-parameters and learning curves. In principle, it is possible to define a GP on the product space of hyper-parameters and learning curves, but, for \(n\) hyper-parameter configurations and \((m)\) learning curve observations per hyper-parameter, this naive approach requires \((n^{3}m^{3})\) time and \((n^{2}m^{2})\) space, which quickly becomes infeasible. An efficient approach is to impose Kronecker structure in the joint covariance matrix (Bonilla et al., 2007; Stegle et al., 2011; Zhe et al., 2019). However, this requires complete learning curves for each hyper-parameter configuration, which is incompatible with early-stopping.

Herein, we leverage _latent Kronecker structure_, which enables efficient inference despite missing entries by obtaining the joint covariance matrix of observed values from the full Kronecker product via lazily evaluated projections. With suitable posterior inference techniques, this reduces the asymptotic time complexity from \((n^{3}m^{3})\) to \((n^{3}+m^{3})\), and space complexity from \((n^{2}m^{2})\) to \((n^{2}+m^{2})\) compared to the naive approach. We demonstrate this superior scalability empirically, and show that our GP with generic stationary kernels and 10 free parameters can match the performance of a specialized Transformer model with 14.69 million parameters on a learning curve prediction task.

## 2 Gaussian Processes with Latent Kronecker Structure

We consider the problem of defining a GP on the product space of hyper-parameter configurations \(^{d}\) and learning curve progression \(t\), namely \(f:^{d}\), where \(f(,k)\) with mean function \(\) and kernel \(k\). Throughout this paper, we suppress the dependence on kernel hyper-parameters for brevity of notation, set \(=0\) without loss of generality, and assume homoskedastic observation noise \(^{2}\). For an introduction to GPs, we refer to Rasmussen and Williams (2006).

The simplest way to address this problem is to define a kernel directly on the product space, which results in a joint covariance \((f(,t),f(^{},t^{}))=k((, t),(^{},t^{}))\). However, this quickly runs into scalability issues. Assuming we evaluate \(n\) hyper-parameter configurations \(:=\{_{i}\}_{i=1}^{n}\) at \(m\) progressions \(=\{t_{1},,t_{m}\}\), observing learning curves \(:=\{_{i}^{m}\}_{i=1}^{n}\), the joint covariance matrix requires \((n^{2}m^{2})\) space, and computing its Cholesky factorization takes \((n^{3}m^{3})\) time.

Latent Kronecker StructureA common way to improve the scalability of GPs on product spaces is to introduce Kronecker structure (Bonilla et al., 2007; Stegle et al., 2011; Zhe et al., 2019). In particular, one may define a product kernel \(k((,t),(^{},t^{}))=k_{1}(,^{})\;k_{2}(t,t^{})\), where \(k_{1}\) only acts on hyper-parameter configurations \(\) and \(k_{2}\) only considers learning curve progressions \(t\). When applied to the observed data, the resulting joint covariance matrix \(\) has Kronecker structure,

\[}=k((),( ))=k_{1}(,) k_{2}(, )=}_{1}},\]

which can be exploited by expressing the eigenvalue decomposition of \(\) in terms of \(_{1}\) and \(_{2}\). This reduces the asymptotic time complexity to \((n^{3}+m^{3})\) and space complexity to \((n^{2}+m^{2})\).

Unfortunately, the joint covariance matrix only has Kronecker structure if each \(_{i}\) is evaluated at every \(t_{j}\), that is, each learning curve must be fully observed, which conflicts with early-stopping. However, _the joint covariance matrix over partially observed learning curves is a submatrix of the latent Kronecker product_, and the former can be selected from the latter using a projection matrix \(\),

\[_{}=\;_{}\; ^{}=(_{1}_{2}) ^{},\]

where \(\) is constructed from the identity matrix by removing rows corresponding to missing values. In practice, projections can be implemented efficiently via slice indexing without instantiating \(\).

Figure 1: Learning curve predictions on the Fashion-MNIST data from the LCBench benchmark (Zimmer et al., 2021). The GP is fit to 16 partially observed learning curves (black). Their ground truth continuations (orange) are contained within the spread of posterior samples (blue). A typical learning curve which is observed close to convergence is predicted with confidence (left). Observing a smaller fraction of the learning curve leads to increased uncertainty in the prediction (left middle). The model also adapts well to less common noisy and spiky learning curves (right).

Figure 2: Selecting the joint covariance matrix using projections of the latent Kronecker product after observing \(\{(_{1},t_{1}),(_{1},t_{2}),(_{2},t_{1}),( _{2},t_{2}),(_{2},t_{3})\}\), two learning curve values from a first hyper-parameter configuration (blue) and three values from a second configuration (orange).

Efficient Inference with Iterative MethodsAs a result of introducing projections, the eigenvalues and eigenvectors of \(_{}\) cannot be expressed in terms of eigenvalues and eigenvectors of \(_{1}\) and \(_{2}\) anymore, which prevents the use of Kronecker structure for efficient matrix factorization. However, despite projections, the Kronecker structure can still be leveraged for fast matrix multiplication,

\[()()= (^{})\ \ ()^{} ()=( ^{-1}(^{}() )^{}),\]

where, in practice, \(\) and \(^{-1}\) correspond to reshaping, \(^{}()\) amounts to zero padding, and \(\) is slice indexing. This facilitates efficient GP inference via _iterative methods_, which only rely on matrix-vector multiplication (MVM) to compute solutions to systems of linear equations (Gardner et al., 2018; Wang et al., 2019). Leveraging latent Kronecker structure and lazy kernel evaluations, iterative methods require \((n^{2}m+nm^{2})\) time and \((n+m)\) space. This is similar to structured kernel interpolation (SKI) (Wilson and Nickisch, 2015), an inducing point approximation that creates Kronecker structure by placing inducing points on regular grids. Compared to SKI, our approach only applies a product structure to separate hyper-parameters \(\) from progressions \(t\), and permits _exact_ MVMs as long as progressions are logged on a shared grid, such as epochs.

Posterior Samples via Matheron's RuleMaddox et al. (2021) proposed to draw posterior samples efficiently by exploiting Kronecker structure via Matheron's rule (Wilson et al., 2020, 2021), which expresses a posterior sample in terms of a transformed prior sample,

\[(f|)(_{},_{t})=f(_{},_{t})+( k_{1}(_{},) k_{2}(_{t},))( _{1}_{2}+^{2})^{-1}(()-f(())-),\]

where \((f|)\) is the posterior sample, \(f\) is the prior sample, \(f(())\) is its evaluation at the training data, and \((,^{2})\). To support _latent_ Kronecker structure, we introduce projections,

\[f(_{},_{t})+(_{},) k_{2}(_{t},))^{}}_{}_{1}_{2}^{ }+^{2})^{-1}(()-f(( ))-)}_{}.\]

In combination with iterative methods, latent Kronecker structure can be exploited to compute the inverse matrix-vector product using fast MVMs. Further, computations can be cached or amortized to accelerate Bayesian optimization or marginal likelihood optimization (Lin et al., 2023, 2024a, b). Drawing a single posterior sample takes \((n^{3}+m^{3}+n_{*}^{3})\) time and \((n^{2}+m^{2}+n_{*}^{2})\) space.

## 3 Experiments

We conducted two empirical experiments. In our first experiment, we illustrate the superior scalability of our Latent Kronecker GP (LKGP) compared to naive Cholesky factorization of the joint covariance matrix. In our second experiment, we predict the final values of partially observed learning curves and evaluate the quality of predictions via mean-square-error (MSE) and log-likelihood (LLH).

Empirical Time and Space ConsumptionWe compared the time and memory requirements for training and prediction of LKGP, which uses iterative methods to exploit latent Kronecker structure, to naive Cholesky factorization of the joint covariance matrix. To this end, we generated random training data with \(n=m\{16,32,...,512\}\) and \(d=10\) (see Appendix C for details).

Figure 3: Time and memory consumption as a function of training data size, where size refers to \(n=m\). Training consists of optimizing noise \(^{2}\) and kernel parameters \(\). Prediction consists of sampling full learning curves for 512 hyper-parameter configurations. Measurements include constant overheads, such as computations performed by the optimizer or memory reserved by CUDA drivers.

Figure 3 visualizes the time and memory consumption as a function of training data size. In general, empirical trends roughly match asymptotic complexities (see Section 2). In particular, LKGP is easily scalable to \(n=m=512\), taking about 6 seconds for training while using less than 2 GB of memory, whereas naive Cholesky factorization already takes more than 3.5 minutes for training at \(n=m=128\) and runs out of memory at \(n=m=256\). Empirically, in terms of time, LKGP seems to scale _better_ than its asymptotic complexity, which is likely due to the iterative solver converging in fewer iterations than would be mathematically required for an exact solution.

Learning Curve Prediction QualityWe replicated the experiment from Rakotoarison et al. (2024), Section 5.1, who used data from LCBench (Zimmer et al., 2021) to define a learning curve prediction task. In particular, the final validation accuracy is predicted given partially observed learning curves. We used the experimental setup and baseline results from Rakotoarison et al. (2024), which are publicly available on GitHub.2 The baselines include DPL (Kadra et al., 2023), a neural network ensemble which makes predictions based on power laws; DyHPO (Wistuba et al., 2022), a Gaussian process model equipped with a deep kernel; FT-PFN (Rakotoarison et al., 2024), a Transformer model which is pre-trained on synthetic learning curves and makes predictions via in-context learning; and FT-PFN (no HPs), a version of FT-PFN which does not consider correlations across hyper-parameters.

Figure 4 illustrates that LKGP achieves better or similar MSE compared to other methods and slightly worse LLH compared to FT-PFN. We consider this an impressive result, because LKGP only has access to the partially observed learning curves which are passed to FT-PFN as context at prediction time, uses basic stationary kernels without any inductive bias for modeling learning curves, and only has 10 parameters; whereas the most competitive model, FT-PFN, is pre-trained on millions of samples drawn from a specifically designed learning curve prior, uses a flexible non-Gaussian posterior predictive distribution, and has 14.69 million parameters (Rakotoarison et al., 2024).

## 4 Conclusion

Motivated by AutoML problems, we proposed a joint Gaussian process model over the product space of hyper-parameters and learning curves, which leverages latent Kronecker structure to accelerate computations and reduce memory requirements in the presence of partial learning curve observations. In contrast to existing Gaussian process models with Kronecker structure, our approach deals with missing entries by combining projections and iterative methods. Empirically, we demonstrated that our method has superior scalability than the naive approach, and that it can match a specialized Transformer model in terms of prediction quality. Future work could investigate specialized kernels and heteroskedastic noise models, which would still be efficient due to the use of iterative methods.

Figure 4: Mean-square-errors (MSE) and log-likelihoods (LLH) of predicted final validation accuracy given partially observed learning curves (mean \(\) standard error over 100 random seeds), where # of training examples refers to the total number of observed values across hyper-parameters and progression. Log-likelihood values for DPL are omitted because they are not competitive enough.