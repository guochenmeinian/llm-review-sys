# Embracing the chaos: analysis and diagnosis of numerical instability in variational flows

Zuheng Xu Trevor Campbell

Department of Statistics

University of British Columbia

[zuheng.xu | trevor]@stat.ubc.ca

###### Abstract

In this paper, we investigate the impact of numerical instability on the reliability of sampling, density evaluation, and evidence lower bound (ELBO) estimation in variational flows. We first empirically demonstrate that common flows can exhibit a catastrophic accumulation of error: the numerical flow map deviates significantly from the exact map--which affects sampling--and the numerical inverse flow map does not accurately recover the initial input--which affects density and ELBO computations. Surprisingly though, we find that results produced by flows are often accurate enough for applications despite the presence of serious numerical instability. In this work, we treat variational flows as dynamical systems, and leverage _shadowing theory_ to elucidate this behavior via theoretical guarantees on the error of sampling, density evaluation, and ELBO estimation. Finally, we develop and empirically test a diagnostic procedure that can be used to validate results produced by numerically unstable flows in practice.

## 1 Introduction

Variational families of probability distributions play a prominent role in generative modelling and probabilistic inference. A standard construction of a variational family involves passing a simple reference distribution--such as a standard normal--through a sequence of parametrized invertible transformations, i.e., a _flow_. Modern flow-based variational families have the representational capacity to adapt to highly complex target distributions while still providing computationally tractable i.i.d. draws and density evaluations, which in turn enables convenient training by minimizing the Kullback-Leibler (KL) divergence  via stochastic optimization methods .

In practice, creating a flexible variational flow often requires many flow layers, as the expressive power of a variational flow generally increases with the number of transformations . Composing many transformations in this manner, however, can create numerical instability , i.e., the tendency for numerical errors from imprecise floating-point computations to be quickly magnified along the flow. This accumulation of error can result in low-quality sample generation , numerical non-invertibility , and unstable training . But in this work, we demonstrate that this is not always the case with a counterintuitive empirical example: a flow that exhibits severe numerical instability--with error that grows exponentially in the length of the flow--but which surprisingly still returns accurate density values, i.i.d. draws, and evidence lower bound (ELBO) estimates.

Motivated by this example, the goal of this work is twofold: (1) to provide a theoretical understanding of how numerical instability in flows relates to error in downstream results; and (2) to provide a diagnostic procedure such that users of normalizing flows can check whether their particular results are reliable in practice. We develop a theoretical framework that investigates the influence of numerical instability using _shadowing theory_ from dynamical systems . Intuitively, shadowing theory asserts that although a numerical flow trajectory may diverge quickly from its exact counterpart,there often exists another exact trajectory nearby that remains close to the numerical trajectory (the _shadowing property_). We provide theoretical error bounds for three core tasks given a flow exhibiting the shadowing property--sample generation, density evaluation, and ELBO estimation--that show that the error grows much more slowly than the error of the trajectory itself. Our results pertain to both normalizing flows [2; 3] and mixed variational flows . We develop a diagnostic procedure for practical verification of the shadowing property of a given flow. Finally, we validate our theory and diagnostic procedure on MixFlow  on both synthetic and real data examples, and Hamiltonian variational flow  on synthetic examples.

Related work.Chang et al.  connected the numerical invertibility of deep residual networks (ResNets) and ODE stability analysis by interpreting ResNets as discretized differential equations, but did not provide a quantitative analysis of how stability affects the error of sample generation or density evaluation. This approach also does not apply to general flow transformations such as coupling layers. Behrmann et al.  analyzed the numerical inversion error of generic normalizing flows via bi-Lipschitz continuity for each flow layer, resulting in error bounds that grow exponentially with flow length. These bounds do not reflect empirical results in which the error in downstream statistical procedures (sampling, density evaluation, and ELBO estimation) tends to grow much more slowly. Beyond the variational literature, Bahsoun et al.  investigated statistical properties of numerical trajectories by modeling numerical round-off errors as small random perturbations, thus viewing the numerical trajectory as a sample path of a Markov chain. However, their analysis was constrained by the requirement that the exact trajectory follows a time-homogeneous, strongly mixing measure-preserving dynamical system, a condition that does not generally hold for variational flows. Tupper  established a relationship between shadowing and the weak distance between inexact and exact orbits in dynamical systems. Unlike our work, their results are limited to quantifying the quality of pushforward samples and do not extend to the evaluation of densities and ELBO estimation. In contrast, our error analysis provides a more comprehensive understanding of the numerical error of all three downstream statistical procedures.

## 2 Background: variational flows

A _variational family_ is a parametrized set of probability distributions \(\{q_{}:\}\) on some space \(\) that each enable tractable i.i.d. sampling and density evaluation. The ability to obtain draws and compute density values is crucial for fitting the family to data via maximum likelihood in generative modelling, as well as maximizing the ELBO in variational inference,

\[(q_{}||p)= q_{}(x)(x)}x,\]

where \(p(x)\) is an unnormalized density corresponding to a target probability distribution \(\). In this work we focus on \(^{d}\) endowed with its Borel \(\)-algebra and Euclidean norm \(\|\|\), and parameter set \(^{p}\). We assume all distributions have densities with respect to a common base measure on \(\), and will use the same symbol to denote a distribution and its density. Finally, we will suppress the parameter subscript \(\), since we consider a fixed member of a variational family throughout.

Normalizing flows.One approach to building a variational distribution \(q\) is to push a reference distribution, \(q_{0}\), through a measurable bijection on \(\). To ensure the function is flexible yet still enables tractable computation, it is often constructed via the composition of simpler measurable, invertible "layers" \(F_{1},,F_{N}\), referred together as a (normalizing) _flow_[1; 2; 4]. To generate a draw \(Y q\), we draw \(X q_{0}\) and evaluate \(Y=F_{N} F_{N-1} F_{1}(X)\). When each \(F_{n}\) is a diffeomorphism,1 the density of \(q\) is

\[ x q(x)=(F_{1}^{-1}  F_{N}^{-1}(x))}{_{n=1}^{N}J_{n}(F_{n}^{-1} F_{N}^{- 1}(x))} J_{n}(x)=|_{x}F_{n}(x)|.\] (1)

An unbiased ELBO estimate can be obtained using a draw from \(q_{0}\) via

\[X q_{0},\,\,E(X)\!=\! p(F_{N} F_{1}(X))\!-\! q_{0} (X)\!+\!_{n=1}^{N} J_{n}(F_{n} F_{1}(X)).\] (2)Common choices of \(F_{n}\) are RealNVP , neural spline , Sylvester , Hamiltonian-based [6; 7], and planar and radial  flows; see Papamakarios et al.  for a comprehensive overview.

MixFlows.Instead of using only the final pushforward distribution as the variational distribution \(q\), Xu et al.  proposed averaging over all the pushforwards along a flow trajectory with identical flow layers, i.e., \(F_{n}=F\) for some fixed \(F\). When \(F\) is ergodic and measure-preserving for some target distribution \(\), Xu et al.  show that averaged flows guarantee total variation convergence to \(\) in the limit of increasing flow length. To generate a draw \(Y q\), we first draw \(X q_{0}\) and a flow length \(K\{0,1,,N\}\), and then evaluate \(Y=F^{K}(X)=F F F(X)\) via \(K\) iterations of the map \(F\). The density formula is given by the mixture of intermediate flow densities,

\[ x, q(x)=_{n=0}^{N}(F^ {-n}x)}{_{j=1}^{n}J(F^{-j}x)}, J(x)=|_{x}F(x)|.\] (3)

An unbiased ELBO estimate can be obtained using a draw from \(q_{0}\) via

\[X q_{0}, E(X)=_{n=0}^{N}X)}{q(F^{n} X)}.\] (4)

## 3 Instability in variational flows and layer-wise error analysis

In practice, normalizing flows often involve tens of flow layers, with each layer coupled with deep neural network blocks [2; 4; 24] or discretized ODE simulations [6; 7]; past work on MixFlows requires thousands of flow transformations . These flows are typically chaotic and sensitive to small perturbations, such as floating-point representation errors. As a result, the final output of a numerically computed flow can significantly deviate from its exact counterpart, which may cause downstream issues during training or evaluation of the flow [12; 14; 27]. To understand the accumulation of error,

Figure 1: MixFlow forward (fwd) and backward (bwd) orbit errors on the Banana and Cross distributions, and on two real data examples. The first column visualizes the exact and numerical orbits with the same starting point on the synthetic targets. For a better visualization, we display every \(2^{}\) and \(4^{}\) states of presented orbits (instead of the complete orbits) in Figs. 0(a) and 0(d), respectively. Figs. 0(b), 0(c), 0(e) and 0(f) show the median and upper/lower quartile forward error \(\|F^{k}x-^{k}x\|\) and backward error \(\|B^{k}x-^{k}x\|\) comparing \(k\) transformations of the forward exact/approximate maps \(F\), \(\) or backward exact/approximate maps \(B\), \(\). Statistics are plotted over 100 initialization draws from the reference distribution \(q_{0}\). For the exact maps we use a 2048-bit BigFloat representation, and for the numerical approximations we use 64-bit Float representation. The “exactness” of BigFloat representation is justified in Fig. 15 in Appendix D.3.

Behrmann et al.  assume that each flow layer \(F_{n}\) and its inverse are Lipschitz and have bounded error, \(_{x}\|F_{n}(x)-_{n}(x)\|\), and analyze the error of each layer individually. This layer-wise analysis tends to yield error bounds that grow exponentially in the flow length \(N\) (see Appendix A.1):

\[\|F_{N} F_{1}(x)-_{N}_{1}(x)\| _{n=0}^{N-1}_{j=n+2}^{N}(F_{j}).\] (5)

Given a constant flow map \(F_{j}=F\) with Lipschitz constant \(\), the bound is \(O(^{N})\), which suggests that the error accumulates exponentially in the length of the flow. In Fig. 1, we test this hypothesis empirically using MixFlows on two synthetic targets (the banana and cross distribution) and two real data examples (Bayesian linear regression and logistic problems) taken from past work on MixFlows . See Appendix D for the details of this test. Figs. 0(b), 0(c), 0(e) and 0(f) confirms that the exponential growth in the error bound Eq. (5) is reasonable; the error does indeed grow exponentially quickly in practice. And Fig. 0(a)-(b) in Appendix D.3 further demonstrate that after fewer than 100 transformations both flows have error on the same order of magnitude as the scale of the target distribution. Naively, this implies that sampling, density evaluation, and ELBO estimation may be corrupted badly by numerical error.

But counterintuitively, in Fig. 2 we find that simply ignoring the issue of numerical error and using the exact formulae yield reasonable-looking density evaluations and sample scatters. These results are of course only qualitative in nature; we provide corresponding quantitative results later in Section 6. But Fig. 2 appears to violate the principle of "garbage in, garbage out;" the buildup of significant numerical error in the sample trajectories themselves does not seem to have a strong effect on the quality of downstream sampling and density evaluation. The remainder of this paper is dedicated to resolving this counterintuitive behavior using shadowing theory .

## 4 Global error analysis of variational flows via shadowing

### The shadowing property

We analyze variational flows as finite discrete-time dynamical systems (see, e.g., ). In this work, we consider a _dynamical system_ to be a sequence of diffeomorphisms on \((,\|\|)\). We define the _forward dynamics_\((F_{n})_{n=1}^{N}\) to be the flow layer sequence, and the _backward dynamics_\((B_{n})_{n=1}^{N}\) to be comprised of the inverted flow layers \(B_{n}=F_{N-(n-1)}^{-1}\) in reverse order. An _orbit_ of a dynamical system starting at \(x\) is the sequence of states produced by the sequence of maps when initialized

Figure 2: Figs. 1(a) and 1(b) respectively show sample scatters produced by the naive application of MixFlow formulae targeting at the banana and cross distributions, without accounting for numerical error. Figs. 1(c) and 1(d) display comparisons of log-densities on both synthetic examples. The true log-target is on the left, the exact MixFlow evaluation (computed via 2048-bit BigFloat representation) is in the middle, and the numerical MixFlow evaluation is on the right.

at \(x\). Therefore, the forward and backward orbits initialized at \(x\) are

\[ x=x_{0}}{}x_{1} }{}x_{2}}{}x_{N}\\ x_{-N}=F_{1}^{-1}}{} x_{-2}=F_{N-1}^{-1}}{}x_{-1}=F_{N}^{-1}}{}x_{0 }=x\]

Given numerical implementations \(_{n} F_{n}\) and \(_{n} B_{n}\) with tolerance \(>0\), i.e.,

\[ x,\|F_{n}(x)-_{n}(x)\|,\|B_{ n}(x)-_{n}(x)\|,\] (6)

we define the _forward_ and _backward pseudo-dynamics_ to be \((_{n})_{n=1}^{N}\) and \((_{n})_{n=1}^{N}\), respectively, along with their _forward_ and _backward pseudo-orbits_ initialized at \(x\):

\[ x=_{0}_{1} F_{1}}{}_{1}_{2} F_{2}}{ }_{2}_{N} F_{N}}{}_{N}\\ _{-N}_{N} F_{1}^{-1}}{} _{-2}_{2} F_{N-1}^{-1}}{} _{-1}_{1} F_{N}^{-1}}{}_{0}=x \]

For notational brevity, we use subscripts on elements of \(\) throughout to denote forward/backward orbit states. For example, given a random element \(Z\) drawn from some distribution, \(Z_{k}\) is the \(k^{}\) state generated by the forward dynamics \((F_{n})_{n=1}^{N}\) initialized at \(Z\), and \(Z_{-k}\) is the \(k^{}\) state generated by the backward dynamics \((B_{n})_{n=1}^{N}\) initialized at \(Z\). Hat accents denote the same for the pseudo-dynamics: for example, \(_{k}\) is the \(k^{}\) state generated by \((_{n})_{n=1}^{N}\) when initialized at \(Z\).

The forward/backward orbits satisfy \(x_{k+1}=F_{k}(x_{k})\) and \(x_{-(k+1)}=B_{k}(x_{-k})\) exactly at each step. On the other hand, the forward/backward pseudo-orbits incur a small amount of error,

\[\|_{k+1}-F_{k}(_{k})\|\|_{ -(k+1)}-B_{k}(_{-k})\|,\]

which can be magnified quickly along the orbit. However, there often exists another _exact_ orbit starting at some other point \(s\) that remains in a close neighbourhood of the numerical orbit. This property, illustrated in Fig. 3, is referred to as the _shadowing property_.

**Definition 4.1** (Shadowing property ).: _The forward dynamics \((F_{n})_{n=1}^{N}\) has the \(\)**-shadowing property** if for all \(x\) and all \((_{n})_{n=1}^{N}\) satisfying Eq. (6), there exists an \(s\) such that_

\[ k=0,1,,N,\|s_{k}-_{k}\|<.\]

An analogous definition holds for the backward dynamics \((B_{n})_{n=1}^{N}\)--where there is a shadowing orbit \(s_{-k}\) that is nearby the pseudo-orbit \(_{-k}\)--and for the joint forward and backward dynamics, where there is a shadowing orbit nearby both the backward and forward pseudo-orbits simultaneously. The key idea in this paper is that, intuitively, if the numerically computed pseudo-orbit is close to

Figure 3: A visualization of a pseudo-orbit and shadowing orbit. Solid arrows and filled dots indicate exact orbits, while dashed arrows and open dots indicate pseudo-orbits (e.g., via numerical computations). Red indicates the exact orbit \((x_{0},,x_{4})\) that one intends to compute. Blue indicates the numerically computed \(\)-pseudo-orbit \((_{0},,_{4})\). Grey indicates the corresponding \(\)-shadowing orbit \((s_{1},,s_{4})\). At the top right of the figure, \(\|_{4}-F_{4}(_{3})\|\) illustrates the \(\) numerical error at each step of the pseudo-orbit. The grey dashed circles demonstrate the \(\)-shadowing window.

_some_ exact orbit (the shadowing orbit), statistical computations based on the pseudo-orbit--e.g., sampling, density evaluation, and ELBO estimation--should be close to those obtained via that exact orbit. We will defer the examination of when (and to what extent) shadowing holds to Section 5; in this section, we will use it as an assumption when analyzing statistical computations with numerically implemented variational flows.

### Error analysis of normalizing flows via shadowing

Sampling.Our first goal is to relate the marginal distributions of \(X_{N}\) and \(_{N}\) for \(X q_{0}\), i.e., to quantify the error in sampling due to numerical approximation. Assume the normalizing flow has the \((,)\)-shadowing property, and let \(_{0}\) be the marginal distribution of the shadowing orbit start point. We suspect that \(_{0} q_{0}\), in some sense; for example, we know that the bounded Lipschitz distance \(_{}(_{0},q_{0})\) is at most \(\) due to shadowing. And \(_{0}\) is indeed an implicit function of \(q_{0}\); it is a fixed point of a twice differentiable function involving the whole orbit starting at \(q_{0}\)[17, Page. 176]. But the distribution \(_{0}\) is generally hard to describe more completely, and thus it is common to impose additional assumptions. For example, past work shows that the Levy-Prokhorov metric \(_{}(_{N},X_{N})\) is bounded by \(\), under the assumption that \(_{0}=q_{0}\). We provide a more general result (Proposition 4.2) without distributional assumptions on \(_{0}\). We control \(_{}(,)\) rather than \(_{}(,)\), as its analysis is simpler and both metrize weak distance.

**Proposition 4.2**.: _Suppose the forward dynamics has the \((,)\)-shadowing property, and \(X{}\,q_{0}\). Then_

\[_{f:|f| U,(f)}|f(X_{N})-f(_{N})|+2UD_{}(_{0},q_{0} ).\]

_In particular, with \(=U=1\), we obtain that \(_{}(X_{N},_{N})+2_{}(_{0},q_{0})\)._

Recall the layerwise error bound from Eq. (5)--which suggests that the difference in orbit and pseudo-orbit grows exponentially in \(N\)--and compare to Proposition 4.2, which asserts that the error is controlled by the shadowing window size \(\). This window size may depend on \(N\), but we find in Section 6 it is usually not much larger than \(\) in practice, which itself is typically near the precision of the relevant numerical representation. We will show how to estimate \(\) later in Section 5.

Density evaluation.The exact density \(q(x)\) follows Eq. (1), while the approximation \((x)\) is the same except that we use the backward _pseudo_-dynamics. For \(x\), a differentiable function \(g:^{+}\), define the _local Lipschitz constant_ for the logarithm of \(g\) around \(x\) as:

\[L_{g,}(x)=_{\|y-x\|}\| g(y)\|.\]

Theorem 4.3 shows that, given the shadowing property, the numerical error is controlled by the shadowing window size \(\) and the sum of the Lipschitz constants along the pseudo-orbit. The constant \(L_{q,}\) occurs because we are essentially evaluating \(q(s)\) rather than \(q(x)\), where \(s\) is the backward shadowing orbit initialization. The remaining constants occur because of the approximation of the shadowing orbit with the nearby, numerically-computed pseudo-orbit in the density formula.

**Theorem 4.3**.: _Suppose the backward dynamics has the \((,)\)-shadowing property. Then_

\[ x,|(x)- q(x)| (L_{q,}(x)+L_{q_{0},}(_{-N})+_{n=1}^{N}L_{J_{ N-n+1},}(_{-n})).\]

ELBO estimation.The exact ELBO estimation function is given in Eq. (2); the numerical ELBO estimation function \((x)\) is the same except that we use the forward _pseudo_-dynamics. The quantity \(E(X)\), \(X q_{0}\) is an unbiased estimate of the exact ELBO, while \((X)\) is biased by numerical error; Theorem 4.4 quantifies this error. Note that for simplicity we assume that the initial state distributions \(q_{0}\), \(_{0}\) described earlier are identical. It is possible to obtain a bound including a \(_{}(q_{0},_{0})\) term, but this would require the assumption that \( p(x)/q(x)\) is uniformly bounded on \(\).

**Theorem 4.4**.: _Suppose the forward dynamics has the \((,)\)-shadowing property, and \(_{0}=q_{0}\). Then_

\[|(q||p)-[(X)]| [L_{p,}(_{N})+L_{q_{0},}(X) +_{n=1}^{N}L_{J_{n,}}(_{n})]X q_{0}.\]

### Error analysis of MixFlows via shadowing

The error analysis for MixFlows for finite length \(N\) parallels that of normalizing flows, except that \(F_{n}=F\), \(B_{n}=B\), and \(J_{n}=J\) for \(n=1,,N\). However, when \(F\) and \(B\) are ergodic and measure-preserving for the target \(\), we provide asymptotically simpler bounds in the large \(N\) limit that do not depend on the difficult-to-analyze details of the Lipschitz constants along a pseudo-orbit. These results show that the error of sampling tends to be constant in flow length, while the error of density evaluation and ELBO estimation grows at most linearly. We say the forward dynamics has the _infinite \((,)\)-shadowing property_ if \((F_{n})_{n=1}^{}\) has the \((,)\)-shadowing property . Analogous definitions hold for both the backward and joint forward/backward dynamics.

Sampling.Similar to Proposition 4.2 for normalizing flows, Proposition 4.5 bounds the error in exact draws \(Y\) and approximate draws \(\) from the MixFlow. The result demonstrates that error does not directly depend on the flow length \(N\), but rather on the shadowing window \(\). In addition, in the setting where the flow map \(F\) is \(\)-ergodic and measure-preserving, we can (asymptotically) remove the total variation term between the initial shadowing distribution \(_{0}\) and the reference distribution \(q_{0}\). Note that in the asymptotic bound, the distributions of \(Y\) and \(\) are functions of \(N\).

**Proposition 4.5**.: _Suppose the forward dynamics has the \((,)\)-shadowing property, and \(X q_{0}\). Let \(Y=X_{K}\), \(=_{K}\) for \(K\{0,1,,N\}\). Then_

\[_{f:|f| U;(f)}|f(Y)-f()|+2U_{}(_{0},q_ {0}).\]

_In particular, if \(=U=1\), we obtain that \(_{}(Y,)+2_{}( _{0},q_{0})\). If additionally the forward dynamics has the infinite \((,)\)-shadowing property, \(F\) is \(\)-ergodic and measure-preserving, and \(_{0},q_{0}\), then_

\[_{N}_{}(Y,).\]

A direct corollary of the second result in Proposition 4.5 is that for \(W\), \(_{N}_{}(W,)\), which results from the fact that \(_{}(W,Y) 0\)[5, Theorem 4.1]. In other words, the bounded Lipschitz distance of the approximated MixFlow and the target \(\) is asymptotically controlled by the shadowing window \(\). This improves upon earlier theory governing approximate MixFlows, for which the best guarantee available had total variation error growing as \(O(N)\)[5, Theorem 4.3].

Density evaluation.The exact density follows Eq. (3); the numerical approximation is the same except that we use the backward _pseudo_-dynamics. We obtain a similar finite-\(N\) result as in Theorem 4.3. Further, we show that given infinite shadowing--where the shadowing window size \(\) is independent of flow length \(N\)--the numerical approximation error asymptotically grows at most linearly in \(N\), in proportion to \(\).

**Theorem 4.6**.: _Suppose the \((,)\)-shadowing property holds for the backward dynamics. Then_

\[ x,|(x)- q(x)| (L_{q,}(x)+_{0 n N}L_{q_{0},}(_{-n})+ _{n=1}^{N}L_{J,}(_{-n})).\]

_If additionally the backward dynamics has the infinite \((,)\)-shadowing property, \(F\) is \(\)-ergodic and measure-preserving, \(L_{q,}(x)=o(N)\) as \(N\), and \(_{0}\), then for \(q_{0}\)-almost every \(x\),_

\[_{N}|(x)- q(x)| [L_{q_{0},}(X)+L_{J,}(X)], X.\]

ELBO estimation.The exact ELBO formula for the MixFlow is given in Eq. (4). Note that here we do not simply substitute the forward/backward pseudo-orbits as needed; the naive approximation of the terms \(q(F^{n}x)\) would involve \(n\) applications of \(\) followed by \(N\) applications of \(\), which do not exactly invert one another. Instead, we analyze the method proposed in , which involves simulating a single forward pseudo-orbit \(_{1},,_{N}\) and backward pseudo-orbit \(_{-1},,_{-N}\) starting from \(x\), and then caching these and using them as needed in the exact formula.

**Theorem 4.7**.: _Suppose the joint forward and backward dynamics has the \((,)\)-shadowing property, and \(_{0}=q_{0}\). Then for \(X q_{0}\),_

\[|(q\|p)-[(X)] |[_{n=0}^{N}L_{p, }(_{n})+L_{q_{0},}( _{n})+L_{J,}(_{n})].\]

_If additionally the joint forward and backward dynamics has the infinite \((,)\)-shadowing property, \(F\) is \(\)-ergodic and measure-preserving, and for some \(1 m_{1}<\) and \(1/m_{1}+1/m_{2}=1\)\(L_{p,},L_{q_{0},},L_{J,} L^{m_{1}}()\) and \(q_{0}}{} L^{m_{2}}()\), then_

\[_{N}|(q\|p)- [(X)]| 2 [L_{q_{0},}(X)+L_{J,}(X)], X.\]

## 5 Computation of the shadowing window size

We have so far assumed the \((,)\)-shadowing property throughout. To make use of our results in practice, it is crucial to understand the shadowing window size \(\). Theorem 5.1 presents sufficient conditions for a finite dynamical system to have the shadowing property, and characterizes the size of the shadowing window \(\). Note that throughout this section we focus on the forward dynamics; the backward and joint dynamics can be treated identically. Let \(\|\|\) denote the spectral norm of a matrix.

**Theorem 5.1** (Finite shadowing theorem).: _Suppose the dynamics \((F_{n})_{n=1}^{N}\) are \(C^{2}\) diffeomorphisms on \(\), and the pseudo-dynamics \((_{n})_{n=1}^{N}\) satisfy Eq. (6). For a given \(x\), define the operator \(A:^{N+1}^{N}\) by_

\[(Au)_{k}=u_{k+1}- F_{k+1}(_{k})u_{k},\;\;u=(u_{0}, u_{1},,u_{N})^{N}, k=0,1,,N-1.\]

_Let_

\[M:=\{_{\|v\| 2}\|^{2}F_{n+1}(_{n}+ v)\|:n=0,1,,N-1\}=_{}(AA^{T})^{-1/2}.\]

_If \(2M^{2} 1\), then the pseudo-orbit starting at \(x\) is shadowed with \(=2\)._

Proof.: This result follows directly by following the proof of [18, Theorem 11.3] with nonconstant flow maps \(F_{n}\), and then using the right-inverse norm from [33, Corollary 4.2]. 

In order to apply Theorem 5.1 we need to (1) estimate \(\), e.g., by examining numerical error of one step of the map in practice; (2) compute \(_{}(AA^{T})\); and (3) estimate \(M\), e.g., by bounding the third derivative of \(F_{n}\). While estimating \(M\) and \(\) are problem-specific, one can employ standard procedures for computing \(_{}(AA^{T})\). The matrix \(AA^{T}\) has a block-tridiagonal form,

\[AA^{T}=D_{1}D_{1}^{T}+I&-D_{2}^{T}\\ -D_{2}&D_{2}D_{2}^{T}+I&-D_{3}^{T}\\ &&&\\ &&-D_{N-1}&D_{N-1}D_{N-1}^{T}+I&-D_{N}^{T}\\ &&-D_{N}&D_{N}D_{N}^{T}+I,\]

where \(D_{k}= F_{k}(_{k-1})^{d d}, k[N]\). Notice that \(AA^{T}\) is a symmetric positive definite sparse matrix with bandwidth \(d\) and so has \(O(Nd^{2})\) entries. The inherent structured sparsity can be leveraged to design efficient eigenvalue computation methods, e.g., the inverse power method , or tridiagonalization via Lanczos iterations  followed by divide-and-conquer algorithms . However, in our experiments, directly calling the eignin function provided in Julia suffices; as illustrated in Figs. 14 and 19, the shadowing window computation requires only a few seconds for low dimensional synthetic examples with hundreds flow layers, and several minutes for real data examples. Hence, we didn't pursue specialized methods, leaving that for future work. It is also noteworthy that practical computations of \(D_{k}\) can introduce floating-point errors, influencing the accuracy of \(\). To address this, one might consider adjusting the shadowing window size. We explain how to manage this in Appendix B.1, and why such numerical discrepancies minimally impact results.

Finally, since \(\) depends on \(_{}(AA^{T})^{-1/2}\), which itself depends on \(N\), the shadowing window size may potentially increase with \(N\) in practice. Understanding this dependence accurately for a general dynamical system is challenging; there are examples where it remains constant, for instance (see Examples B.1 and B.2 in Appendix B.2), but \(\) may grow with \(N\). Our empirical results in next section show that on representative inferential examples with over \(500\) flow layers, \(\) scales roughly linearly with \(N\) and is of a similar order of magnitude as the floating point representation error. For further discussion on this topic, we refer readers to Appendix B.2.

## 6 Experiments

In this section, we verify our error bounds and diagnostic procedure of MixFlow on the banana, cross, and 2 real data targets--a Bayesian linear regression and logistic regression posterior; detailed

Figure 4: MixFlow relative sample average computation error on three test functions: \(_{i=1}^{d}[|x|]_{i},_{i=1}^{d}[(x)+1]_{i}\) and \(_{i=1}^{d}[(x)]_{i}\). The lines indicate the median, and error regions indicate \(25^{}\) to \(75^{}\) percentile from \(20\) runs.

Figure 5: MixFlow relative log-density evaluation error. The lines indicate the median, and error regions indicate \(25^{}\) to \(75^{}\) percentile from \(100\) evaluations on different positions.

Figure 6: MixFlow exact and numerical ELBO estimation over increasing flow length. The lines indicate the averaged ELBO estimates over \(200\) independent forward orbits. The Monte Carlo error for the given estimates is sufficiently small so we omit the error bar.

Figure 7: MixFlow shadowing window size \(\) over increasing flow length. The lines indicate the median, and error regions indicate \(25^{}\) to \(75^{}\) percentile from \(10\) runs.

model and dataset descriptions can be found in Appendix D.1. We also provide a similar empirical investigation for Hamiltonian flow  on the same synthetic targets in Appendix C.2.

We begin by assessing the error of trajectory-averaged estimates based on approximate draws. Fig. 4 displays the relative numerical estimation error compared to the exact estimation based on the same initial draw. Although substantial numerical error was observed in orbit computation (see Fig. 16 in Appendix D.3), the relative sample estimate error was around \(1\%\) for all four examples, suggesting that the statistical properties of the forward orbits closely resemble those of the exact orbit.

We then focus on the density evaluation. For synthetic examples, we assessed densities at 100 evenly distributed points within the target region (Figs. (a)a and (b)b). For real data, we evaluated densities at the locations of 100 samples from MCMC method None-U-turn sampler (NUTS); detailed settings for NUTS is described in Appendix D.2. It is evident from the relative error shown in Fig. 5 that the numerical density closely matches the exact density evaluation, with the relative numerical error ranging between \(0.1\%\) and \(1\%\), which is quite small. The absolute error can be found in Fig. 17 in Appendix D.3, showing that the density evaluation error does not grow as substantially as the orbit computation error (Fig. 16 in Appendix D.3), which aligns with the bound in Theorem 4.6.

Fig. 6 further demonstrates the numerical error for the ELBO estimations (Eq. (4)). In each example, both the averaged exact ELBO estimates and numerical ELBO estimates are plotted against an increasing flow length. Each ELBO curve is averaged over 200 independent forward orbits. Across all four examples, the numerical ELBO curve remain closely aligned with the exact ELBO curve, indicating that the numerical error is small in comparison to the scale of the ELBO value. Moreover, the error does not grow with an increasing flow length, and even presents a decreasing trend as \(N\) increases in the two synthetic examples and the Logistic regression example. This aligns well with the error bounds provided in Theorem 4.7.

Finally, Fig. 7 presents the size of the shadowing window \(\) as the flow length \(N\) increases. As noted earlier, \(\) depends on the flow map approximation error \(\) and potentially \(N\). We evaluated the size of \(\) by calculating the approximation error of a single \(F\) or \(B\) for 1000 i.i.d. draws from the reference distribution. Boxplots of \(\) for all examples can be found in Fig. 18 of Appendix D.3. These results show that in the four examples, \(\) is close to the floating point representation error (approximately \(10^{-14}\)). Thus, we fixed \(\) at \(10^{-14}\) when computing \(\). As shown in Fig. 7, \(\) for both the forward and backward orbits grows roughly linearly with the flow length. This growth is significantly less drastic than the exponential growth of the orbit computation error. And crucially, the shadowing window size is very small--smaller than \(10^{-10}\) for MixFlow of length over \(1000\), which justifies the validity of the downstream statistical procedures. In contrast, the orbit computation errors in the two synthetic examples rapidly reach a magnitude similar to the scale of the target distributions, as shown in Fig. 1.

## 7 Conclusion

This work delves into the numerical stability of variational flows, drawing insights from shadowing theory and introducing a diagnostic tool to assess the shadowing property. Experiments corroborate our theory and demonstrate the effectiveness of the diagnostic procedure. However, the scope of our current analysis is limited to downstream tasks post-training of discrete-time flows. Understanding the impact of numerical instability during the training phase or probing into recent architectures like continuous normalizing flows or neural ODEs , remains an avenue for further exploration. Additionally, while our theory is centered around error bounds that are proportional to the shadowing window size \(\), we have recognized that \(\) can grow with \(N\). Further study on its theoretical growth rate is needed. Finally, in our experiments, we employed a basic method to calculate the minimum eigenvalue of \(AA^{T}\). Given the sparsity of this matrix, a deeper exploration into a more efficient computational procedure is merited.