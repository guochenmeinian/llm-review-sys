# On the impact of activation and normalization in obtaining isometric embeddings at initialization

Amir Joudaki

ETH Zurich

amir.joudaki@inf.ethz.ch &Hadi Daneshmand

MIT/LIDS-FODSI

dhadi@mit.edu &Francis Bach

INRIA-ENS-PSL Paris

francis.bach@inria.fr

###### Abstract

In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results do not extend to layer normalization, which is widely used in transformers, and can not quantitatively characterize the role of non-linear activations. To bridge this gap, we prove that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards the identity matrix at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function.

+
Footnote â€ : Code is available at: https://github.com/ajoudaki/deepnet-isometry

## 1 Introduction

Optimization of deep neural networks is a challenging non-convex problem. Various components and optimization techniques have been developed over the last decades to make optimization feasible. Components such as activation functions (Hendrycks and Gimpel, 2016), normalization layers (Ioffe and Szegedy, 2015), and residual connections (He et al., 2016) have significantly influenced network training and have thus become the building blocks of neural networks. The practical success of these components has inspired extensive theoretical studies on the intricate role of weight initialization (Saxe et al., 2013; Daneshmand et al., 2021), normalization (Yang et al., 2019; Kohler et al., 2019; Daneshmand et al., 2021, 2023; Joudaki et al., 2023) and activation layers (Pennington et al., 2018; Joudaki et al., 2023), on neural network training. For example, the training of large language models hinges on carefully utilizing residual connections, normalization layers, and tailored activations (Vaswani et al., 2017; Radford et al., 2018). Noci et al. (2022) highlight that the absence or improper utilization of these components can substantially slow training.

To delve deeper into the influence of normalization and activation layers on training, one line of research has studied neural networks at initialization (Pennington et al., 2018; de G. Matthews et al., 2018; Jacot et al., 2018; Yang et al., 2019; Li et al., 2022). Several studies have focused on the Gram matrix, which captures the inner products of intermediate representations for a batch of inputs, revealing that Gram matrices become degenerate as the network depth increases (Saxe et al., 2013; Daneshmand et al., 2021; Joudaki et al., 2023). These issue of degeneracy or rank deficiency has been observed in multilayer perceptrons (MLPs) (Saxe et al., 2013; Daneshmand et al., 2020), convolutional networks (Bjorck et al., 2018), and transformers (Dong et al., 2021), posing challenges to the training process (Noci et al., 2022; Pennington et al., 2018; Xiao et al., 2018). Researchindicates that normalization layers can act effectively to circumvent such Gram degeneracy, thereby improving training (Yang et al., 2019; Daneshmand et al., 2020, 2021; Bjorck et al., 2018).

Analyses based of neural networks in the mean-field, i.e., the infinite width regime, have revealed profound insights about initialization by characterizing the local solutions to the Gram dynamics Yang et al. (2019); Pennington et al. (2018). However, these results do not guarantee global convergence towards the mean-field solutions or depend on technical assumptions that are challenging to verify numerically (Daneshmand et al., 2021, 2020; Joudaki et al., 2023). Furthermore, all these theories primarily pertain to the network at initialization, where parameters are typically random and do not necessarily hold during or after the training. In this work, our objective is to bridge these existing gaps.

Contributions.Building upon existing literature that elucidates the spectral properties of the Gram matrix, we introduce the concept of isometry, which quantifies the similarity of the Gram matrix to the identity. Our initial theoretical finding demonstrates that isometry does not decrease under conditions of (batch and layer) normalization. This finding illuminates the bias of normalization layers towards isometry at various stages, namely initialization, during, and post-training.

We subsequently extend our analysis to explore the impact of non-linear activations on the isometry of intermediate representations in MLPs. Within the mean-field regime, we establish that non-linear activations incline the intermediate representations towards isometry at an exponential rate in depth. Our principal contribution is quantifying this rate by utilizing the Hermit polynomial expansion of activations. Intriguingly, our empirical experiments unveil a correlation between this rate and the convergence of stochastic gradient descent in MLPs equipped with layer normalization and standard activations used in practice.

## 2 Related works

A line of research investigates the interplay between signal propagation of the network and training. The existing literature postulates that in order to ensure fast training (Schoenholz et al., 2017; Poole et al., 2016), the network output must be sensitive to input changes, quantified by the spectrum of input-input Jacobean. This hypothesis is employed by Xiao et al. (2018) to train a 10,000-layer CNN using proper weight initialization without stabilizing components such as skip connection or normalization layers. He et al. (2023) demonstrate the critical role of the Jacobean spectra in large language models. In this paper, we analyze the spectrum of Gram matrices that connect to the spectral properties of input-output Jacobean.

Mean-field theory has been extensively used to characterize Gram matrix dynamics in the limit of infinite width. In this setting, the Gram matrix is a fixed point of a recurrence equation that depends on the network architecture (Schoenholz et al., 2017; Yang et al., 2019; Pennington et al., 2018). This fixed-point analysis can provide insights into the structure and spectral properties of Gram matrices in deep neural networks, thereby shedding light on the degeneracy of Gram matrices in networks (Schoenholz et al., 2017; Yang et al., 2019). However, often fixed-points are not unique, and they can be degenerate or non-degenerate Yang et al. (2019). In this paper, we establish a convergence rate to a non-degenerate fixed-point for a family of MLPs.

Batch normalization (Ioffe and Szegedy, 2015) and layer normalization (Ba et al., 2016) layers are widely used in deep neural networks (DNNs) to improve training. Batch normalization ensures that each feature within a layer across a mini-batch has zero mean and unit variance. In contrast, layer normalization centers and divides the output of each layer by its standard deviation. There have been numerous theoretical studies on the effects of batch normalization due to its popularity Yang et al. (2019); Daneshmand et al. (2021); Joudaki et al. (2023). While layer normalization has been the subject of increasing interest due to its application in transformers Xiong et al. (2020), there are relatively fewer studies on its theoretical underpinnings. While we primarily focus on layer normalization, we define and characterize a property that is shared between batch and layer normalization.

A broad spectrum of activation functions such as ReLU (Fukushima, 1969), GeLU (Hendrycks and Gimpel, 2016), SeLU (Klambauer et al., 2017), and Hyperbolic Tangent, and Sigmoid, are used in DNNs. These functions have various computational and statistical consequences in deep learning. Despite this diversity, only the design of SeLU activation is theoretically motivated (Klambauer et al.,2017), while a broader theoretical understanding of activations remains elusive. To address this issue, we develop a theoretical framework to characterize the influence of a broad range of activations on intermediate representations in DNNs.

## 3 Preliminaries

Notation.Let \( x,y\) be the inner product of vectors \(x\) and \(y,\) and \(\|x\|^{2}= x,x\) the squared Euclidean norm of \(x\). For a matrix \(X,\) we write \(X_{i}\) and \(X_{ i}\) for the \(i\)-th row and column of \(X,\) respectively. We use \(W N(,^{2})^{m n}\) to indicate that \(W\) is an \(m n\) Gaussian matrix with i.i.d. elements from \(N(,^{2})\). We denote by \(_{n}\) the zero vector of size \(n\). Given vector \(x^{n}\), \(\) denotes the arithmetic mean of \(_{i=1}^{n}x_{i}\). Lastly, \(I_{n}\) is the identity matrix of size \(n\).

Normalization layers.Let \(:^{d}^{d}\) and \(:^{d n}^{d n}\), denote batch normalization and layer normalization respectively. Table 1 summarizes the definition of normalization layers. In our notations, we separate centering from normalization in layer (batch) normalization. Similarly, we split batch normalization into centering and normalization steps in our definitions. This notation allows us to decouple the effect of normalization from the centering. However, we will not depart from the standard MLP architectures as we include centering in the network architecture defined below.

MLP setup.The subject of our analysis is an MLP with constant width \(d\) across the layers and \(L\) layers, which takes input \(x^{d}\) and maps it to output \(x^{L}^{d},\) with hidden representations as

\[x^{+1}=}(h_{}- _{}),&h_{}=(W^{}x^{}),&=0,,L-1\\ x^{0}:=}(x-),&\] (1)

While the original ordering of layer normalization and activation is different (Ba et al., 2016); Xiong et al. (2020) show that the above ordering is more effective for large language models.

Gram matrices and isometry.Given \(n\) data points \(\{x_{i}\}_{i n}^{d}\), the Gram matrix \(G^{}\) of the feature vectors \(x_{1}^{},,x_{n}^{}^{d}\) at layer \(\) of the network is defined as

\[G^{} :=[ x_{i}^{},x_{j}^{}]_{i,j n }, =0,1,,L.\] (2)

We define the notion of isometry to measure how much \(G^{}\) is close to a scaling factor of the identity matrix.

**Definition 1**.: _Let \(G\) be an \(n n\) positive semi-definite matrix. We define the isometry\((G)\) of \(G\) as the ratio of its normalized determinant to its normalized trace:_

\[(G) :=}{(G)}.\] (3)

\((G^{})\) is a scale-invariant quantity measuring the parallelepiped volume spanned by the feature vectors \(x_{1}^{},,x_{n}^{}\). For example, consider two points on a plane \(x_{1},x_{2}^{2}\) with lengths \(a=|x_{1}|,b=|x_{2}|\) and angle \(=(x_{1},x_{2})\). The ratio is given by \(ab()/(a^{2}+b^{2})\), which is maximized when \(a=b\) and \(=/2\). This relationship between volume and isometry is visually clear \(n=2\) and \(n=3\) feature vectors in Figure 1.

Remarkably, \((M)\) has the following properties (see Lemma A.1 for formal statements and proofs):

  Width & \(d\) & Batch size & \(n\) \\ Depth & \(L\) & Input & \(x^{d}\) \\ Input batch & \(X^{d n}\) & Gaussian weights & \(W^{1},,W^{L} N(0,1)^{d d}\) \\ Activation & \(:\) & Centering & \(x-\) \\ 
**Layer Norm** & \((x)=_{i}^{d}x_{i}^{2}}}\) & **Batch Norm** & \((X)_{ij}=}{_{i}^{d}X_{i}^{2}}}\) \\  

Table 1: Building blocks we consider in this work.

1. _Scaling-invariant:_ For all constants \(c>0\), we have \((G)=(cG)\).
2. _Range:_\(\) where the boundaries \(0\) and \(1\) are achieved for to degenerate and identity matrices respectively.

We also define the **isometry gap** as negative logarithm of isometry \(-(M)\). Based on these properties of isometry, isometry gap lies between \(0\) and \(\), with \(0\) and \(\) indicating the perfect isometry (identity matrix) and degenerate matrices respectively. Isometry allows us to establish the inherent bias of normalization layers in the following section.

## 4 Isometry bias of normalization

This section is devoted to discussing the remarkable property of isometry in the context of normalization. We present a theorem that formalizes this property, followed by its geometric interpretation and implications.

**Theorem 1**.: _Given \(n\) samples \(\{x_{i}\}_{i n}^{d}\{_{d}\}\), their projection onto the unit sphere \(_{i}:=x_{i}/\|x_{i}\|,\) and their respective Gram matrices \(G\) and \(\), the isometry obeys_

\[()(G)(1+_{i}^{n}(a_{i}-)^{2}}{^{2}}),\] (4)

_where \(a_{i}:=\|x_{i}\|,\) and \(:=_{i}^{n}a_{i}\)._

Geometric Interpretation.Isometry can be considered a measurement of the "volume" of the parallelepiped formed by sample vectors, made scale and dimension-independent. The normalization process effectively equalizes the edge lengths of this parallelepiped, enhancing the overall "volume" or isometry, provided there is a variance in the sample norms. Thus, projection onto the unit sphere \(x_{i} x_{i}/\|x_{i}\|\) makes the edge lengths of the parallelepiped equal while leaving the angles between its edges intact. From this geometric perspective, Theorem 1 implies that among parallelepiped with similar angles between their edges and fixed total squared edge lengths, the one with equal edge lengths has the highest volume (and thereby isometry).

The proof of Theorem 1 is intuitive for the special case of vectors forming a cube, where max volume is realized when all edge lengths are equal. This fact that maximum volume is achieved when edge lengths are equal can be deduced from the arithmetic vs geometric mean inequality. Strikingly, the proof for the general case is nearly as simple as this special case. The high-level intuition behind the proof is that the determinant allows us to decouple the role of angles and edge lengths in volume formulation. This fact is evident for \(n=2\) in Figure 1. Since normalization does not modify the angles between edges, the remainder of the proof falls back onto the case where edges form a cube.

Figure 1: A geometric interpretation of isometry: higher volume, in the second row, corresponds to higher value for isometry.

Proof of Theorem 1.: Define \(D:=(a_{1},,a_{n})\). Observe that \(G=DD\), implying \((G)=()(D)^{2}\). Because \(}\)'s have norm \(1\), diagonals of Gram after normalization are constant \(_{ii}=1\), implying \(()=1\). We have

\[()}{(G)} =(G)}{( )})^{1/n}}{(G)^{1/n}}\] (5) \[=_{i}^{n}a_{i}^{2}}{1})^{1/n}}{()^{1/n}(_{i}^{n}a_{i}^{2})^{1/n}}\] (6) \[=_{i}a_{i})^{2}}{(_{i}^{n}a_{i})^{2/n} }_{i}^{n}a_{i}^{2}}{(_{i}a_{i})^{2}} (D)=_{i}^{n}a_{i}^{2}\] (7) \[ 1_{i}^{n}a_{i}^{2}}_{i}^{n}a_{i})^{2}} _{i}^{n}a_{i}(_{i}^{n}a_{i})^{}\] (8) \[=1+_{i}^{n}(a_{i}-)^{2}}{^{2}}, :=_{i}^{n}a_{i}.\] (9)

Theorem 1 further shows a subtle property of normalization: as long as there is some variation in the sample norms, i.e., \(\|x_{i}\|\)'s are not all equal, the post-normalization Gram has strictly higher isometry than the pre-normalization Gram matrix. It further quantifies the improvement in isometry as a function of variation of norms. Intuitively, terms \(\) and \(_{i}^{n}(a_{i}-)^{2}\) can be interpreted as the average and variance of sample norms \(a_{1},,a_{n}\). Thus, a higher variation in the norms \(a_{i}\)'s leads to a larger increase in isometry after normalization.

### Implications for layer (and batch) normalization

Theorem 1 reveals insights into the biases introduced by layer and batch normalization in neural networks, particularly highlighting the improvement in isometry not just limited to initialization but also persistent through the training process.

**Corollary 2**.: _Consider \(n\) vectors before and after layer-normalization \(\{x_{i}\}_{i n}^{d}\{_{d}\}\) and \(\{_{i}\}_{i n},_{i}:=(x_{i})\). Define their respective Gram matrices \(G:=[(_{i},x_{j})]_{i,j n},\) and \(:=[(_{i},_{j})]_{i,j n}\). We have:_

\[()(G)(1+_{i}^{n}(a_{i}-)^{2}}{^{2}}),a_{i}:=\|x_{i}\|,:=_{i}^{n}a_{i}.\]

What makes the above result distinct from related studies (Daneshmand et al., 2021, 2020; Yang et al., 2019) is that the increase in isometry is not limited to random initialization. Thus, layer normalization increases the isometry even during and after training. This calls for future research on the role of this inherent bias in enhanced optimization and generalization performance with batch normalization (Ioffe and Szegedy, 2015; Yang et al., 2019; Lyu et al., 2022; Kohler et al., 2019).

Despite the seemingly vast differences between layer normalization and batch normalization (Lubana et al., 2021), the following corollary shows a link between these two different normalization techniques.

**Corollary 3**.: _Given \(n\) samples in a mini-batch before \(X^{d n},\) and after normalization \(=(X)\) and define covariance matrices \(C:=XX^{}\) and \(:=XX^{}\). We have:_

\[()(C)(1+}_{i}^{d}(a_{i}-)^{2}}{^{2}}), a_{i}:=\|X_{i}\|,:=_{i=1}^{d}a_{i}.\]

Gram matrices of networks with batch normalization have been the subject of many previous studies at network initialization: it has been postulated that BN prevents rank collapse issue (Daneshmandet al., 2020) and that it orthogonalizes the representations (Daneshmand et al., 2021), and that it imposes isometry (Yang et al., 2019). It is straightforward to verify that orthogonal matrices have the maximum isometry. Thus, the increase in isometry links to the orthogonalization of hidden representation characterized by Daneshmand et al. (2021). While all previous results heavily rely on Gaussian random weights to establish this inherent bias, Corollary 3 is not limited to random weights.

### Empirical validation of Corollary 2 in an MLP setup

We can validate Corollary 2 by tracking the isometry of various layers of an MLP with layer normalization. Figure 2 shows the isometry of intermediate representations in an MLP with layer normalization and hyperbolic tangent on CIFAR10 dataset. Shades in the figure mark layers illustrate that the isometry of the Gram matrix is non-decreasing after each layer normalization layer. We can see in Figure 2 that both before (left) and after training (right), the normalization layers maintain or improve isometry. To highlight the fact that the claims of Corollary 2 holds at all times and not only for initialization, Figure 2 tracks isometry of various layers both at initialization (left) and after training (right). This can be verified by the fact that isometry in the normalization layers (shaded blue) is either stable or increased, which validates Corollary 2.

## 5 Isometry bias of non-linear activation functions

So far, our focus was individual normalization layers. In this section, we extend our analysis to all layers when weights are **Gaussian**. Inspired by the isometry bias of normalization, we analyze how other components of neural networks influence the isometry of the Gram matrices, denoted by \((G^{})\) with a specific focus on non-linear activations.

### Hermite expansion of activation functions

Analyzing Gram matrix dynamics for non-linear activation is challenging since even small modifications in the scale or shape of activations can lead to significant changes in the representations. A powerful tool to analyze activations is to express activations in the Hermite polynomial basis. Inspired by previous successful applications of Hermite polynomials in neural network analysis (Daniely et al., 2016; Yang, 2019), we explore their impact on the isometry of activation functions.

**Definition 2**.: _Hermite polynomial of degree \(k,\) denoted by \(_{k}(x),\) is defined as_

\[_{k}(x)(k!)^{-}(-1)^{k}e^{}{2}} {d^{k}}{dx^{k}}e^{-}{2}}.\]

All square-integrable function \(\) with respect to the Gaussian kernel, which obeys \(_{-}^{}(x)^{2}e^{-x^{2}/2}dx<\), can be expressed as a linear combination of Hermite polynomials

Figure 2: **Validation of Corollary 2** Isometry (y-axis) vs different layer of an MLP: Normalization layers (shaded blue) across all layers and configurations maintain or increase isometry both before (left) and after (right) training, validating Corollary 2. _Hyper parameters:_ activation: \(\), depth: \(10\), width: \(1000\), batch-size: \(512\), training SGD on training set of CIFAR10. with \(lr=0.01\).. Layer names are encoded as type-index, where type can be fc: fully connected, norm: LayerNorm, and act: activation.

as \((x)=_{k}c_{k}He_{k}(x)\) with (see section A for more details):

\[c_{k}:=_{x(0,1)}[(x)He_{k}(x)].\]

The subsequent section will discuss how to leverage the Hermite expression of the activation to analyze the dynamics of Gram matrix isometry.

### Non-linear activations bias Gram dynamics towards isometry

In this section, we analyze how \((G^{})\) changes with \(\). We use the mean-field dynamic of Gram matrices subject of previous studies (Yang et al., 2019; Schoenholz et al., 2017; Poole et al., 2016). The mean-field dynamics of Gram matrices is given by

\[G^{+1}_{*}=_{h N(0,G^{}_{*})}[( (h))((h))^{}],[(a)]_{i}:=(a_{i}-a_{i})/a_{i}}.\] (10)

This equation gives the expected Gram matrix for layer \(+1\), based on the Gram matrix \(G^{}_{*}\) from the previous layer, and \(\) is mean-field regime counterpart for layer normalization operator (see section A for more details). The sequence \(G^{}_{*}\) approximates the dynamics of \(G^{},\) and this correspondence becomes exact for infinitely wide MLPs. In the rest of this section, we analyze the above dynamical system. Our theory relies on the notion of _isometry strength_ of the activation function, defined next.

**Definition 3** (Isometry strength).: _Given activation \(\) with Hermite expansion \(\{c_{k}\}_{k 0},\) define its isometry strength \(_{}\) as:_

\[_{}:=2-^{2}}{_{k=1}^{}c_{k}^{2}}\] (11)

We can readily check from the definition that isometry strength \(_{}\) has the following basic properties: (i) it ranges between \(1\) and \(2,\) and (ii) it is \(1\) if and only if the activation is a linear function. Table 2 presents the isometry strength of certain activations in closed form. With this definition, we can finally analyze Gram matrix mean-field dynamics. Interestingly, the negative log of isometry \(-(G^{}_{*})\) can serve as a Lyapunov function for the above dynamics. The following theorem proves non-linear activations also impose isometry similar to normalization layers.

**Theorem 4**.: _Let \(\) be an activation function with a Hermite expansion and a non-linearity strength \(_{},\) (see equation (11)). Given non-degenerate input Gram matrix \(G^{0}_{*},\) then for sufficiently large layer \(_{}^{-1}(-n(G^{0}_{*})+(4n))\), we have_

\[-(G^{}_{*})(-_{}-n (G^{0}_{*})+(4n)).\] (12)

Note that the condition on input being non-degenerate is essential to reach isometry through depth. For example, if the input batch contains a duplicated sample, their corresponding representations across all layers will remain duplicated, implying that all \(G^{}_{*}\)'s will be degenerate.

Theorem 4 reveals the importance of non-linear Hermite coefficients (\(c_{k},k 2\)) in activation function to ensure \(_{}>1\) and obtain isometry in depth. This connection between \(_{}\) and isometry is the rationale for referring to \(_{}\) as the isometry strength. This constant can be computed in closed form for various activations, as shown in Table 2, and for all other activations, it can be computed numerically by sampling.

Figure 3 compares the established bound on the isometry gap with those observed in practice, i.e. \(G^{}\), for three activations. We observe \(_{}\) predicts the decay rate in isometry of Gram matrices \(G^{}.\) While so far, we have only discussed the direct results of our theory for isometry of Gram matrices, in the next section, we will discuss other insights from the above analysis.

   & \(He_{1}(x)\) & \(He_{2}(x)\) & Sine & Exponential & Step & ReLU \\  \(\) & \(x\) & \(x^{2}-1\) & \((x)\) & \((x-2)\) & \([x>0]\) & \((x,0)\) \\ \(_{}\) & 1 & 2 & \(2--1}\) & \(2-\) & \(2-\) & \(\) \\  

Table 2: Isometry strength \(_{}\) (see Definition 3) for various activation functions.

## 6 Implications of our theory

In this section, we elucidate the implications of our theory, beginning with insights into layer normalization through the Hermit expansion of activation functions, followed by an examination of its impact on training.

Layer normalization primarily involves two steps: (i) centering and (ii) normalizing the norms. Through the Hermit expansion of activation functions, we unravel the underlying intricacies of these components and propose alternatives based on insights from the Hermit expansion.

Experimental Setup.Our experiments utilize MLPs with layer normalization and various activation functions for the task of image classification on the CIFAR10 dataset [Krizhevsky et al.]. For training, we use stochastic gradient descent (SGD) with a fixed step size of \(0.01\). Unless stated otherwise, the MLP has constant width of \(d=1000\) is maintained across hidden layers, and a batch size of \(n=10\). Throughout this section, \(a^{}\) and \(x^{}\) respectively denote the post-activation and post-normalization vector for layer \(\).

### Centering and Hermit expansion

One of the key insights of our mean-field theory is that the centering step in layer normalization is crucial in obtaining isometry. In the mean-field regime, pre-activations follow standard Gaussian distributions, and thus the average post-activation \(}=_{i}^{d}a_{i}^{}\) will converge to their expectation \(}=_{z(0,1)}[(X)]=c_{0}\). This insight suggests an alternative way of obtaining isometry by explicitly removing the offset term from activation, i.e., replacing activation \((x)\) by \((x)-c_{0}\). Strikingly, our experiments presented in Figure 4 indicate that such replacement can also impose isometry. This result provides novel insights into the role of centering in layer normalization.

### Normalization and Hermit expansion

Theorem 4 further reveals the importance of normalization of norms in addition to centering to achieve isometry. Figure 5 underlines the importance of the normalization for different activations, where we observe the isometry gap may increase without normalization. Similar to our mean-field analysis of centering, the factor \(_{i=1}^{d}(a_{i}^{}-})^{2}\) in layer normalization converges to variance

Figure 4: Layer vs. mean-field centering in obtaining isometry. Layer centering (orange): \(x^{+1}=(a^{}-})\). Mean-field centering (blue): \(x^{+1}=(a^{}-c_{0})\).

Figure 3: **Validation of Theorem 4.** Solid blue traces show the isometry of MLP with \(n=100\), \(d=5000\), and various activations \(\). Solid lines show the average of \(\#10\) independent runs. The dashed traces are theoretical upper bounds given in Theorem 4, with constant \(C=1\).

\(_{z N(0,1)}((z))=_{k=1}^{}c_{k}^{2}=:(1)\). Thus, as the width increases, the layer normalization operator \((a^{}-})\) will converge to \((a^{}-c_{0})/(1)}\). Figure 6 demonstrates that the constant scaling \(1/(1)}\), achieves comparable isometry to layer normalization for hyperbolic tangent and sigmoid and ReLU, while it is not effective for \(\) function. This observation calls for future research on the link between normalization and activation in deep neural networks.

### Isometry strength correlates with SGD convergence rate in shallow MLPs.

Besides the direct consequences of our theory, we observe a striking correlation between the convergence of SGD and isometry strength in a specific range of neural network hyper-parameters. Figure 7 shows the convergence of SGD is faster for activations with a significantly larger isometry strength \(_{}\) (see Definition 3) for _shallow_ MLPs, e.g., with \(10\) layers or less. We can speculate that this correlation reflects the input-output sensitivity of the networks with higher non-linearity. Surprisingly, this correlation does not extend to deeper networks. This discrepancy between shallow and deep networks regarding SGD convergence may be due to the issue of gradient explosion studied by Meterez et al. (2023). This finding suggests multiple avenues for future research.

Figure 5: Comparing no normalization (blue) \(x^{+1}=(a^{}-c_{0})\) with layer normalization (orange) \(x^{+1}=(a^{}-c_{0})\) in obtaining isometry.

Discussion

In this study, we explored the influence of layer normalization and nonlinear activation functions on the isometry of MLP representations. Our findings open up several avenues for future research.

Self normalized activations.It is worth investigating whether we can impose isometry without layer normalization. Our empirical observations suggest that certain activations, such as ReLU, require layer normalization to attain isometry. In contrast, other activations, which can be considered as "self-normalizing" (e.g., SeLU (Klambauer et al., 2017) and hyperbolic tangent), can achieve isometry with only offset and scale adjustments (see Figure 8). We experimentally show how we can replace centering and normalization by leveraging Hermit expansion of activation. Thus, we believe Hermit expansion provides a theoretical grounding to analyze the isometry of SeLU.

Impact of the ordering of normalization and activation layers on isometry.Theorem 4 highlights that the ordering of activation and normalization layers has a critical impact on the isometry. Figure 8 demonstrates that a different ordering can lead to a non-isotropic Gram matrix. Remarkably, the structure analyzed in this paper is used in transformers (Vaswani et al., 2017).

Normalization's role in stabilizing mean-field accuracy through depth.Numerous theoretical studies conjecture that mean-field predictions may not be reliable for considerably deep neural networks (Li et al., 2021; Joudaki et al., 2023). Mean-field analysis incurs a \(O(1/})\) error per layer when the network width is finite. This error may accumulate with depth, making mean-field predictions increasingly inaccurate with an increase in depth. However, Figure 9 illustrates that layer normalization controls this error accumulation through depth. This might be attributable to the isometry bias induced by normalization, as proven in Theorem 1. Similarly, batch normalization also prevents error propagation with depth by imposing the same isometry (Joudaki et al., 2023). This observation calls for future research on the essential role normalization plays in ensuring the accuracy of mean-field predictions.