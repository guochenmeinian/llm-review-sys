# Revisiting the Integration of Convolution and Attention for Vision Backbone

Lei Zhu

City University of Hong Kong

ray.leizhu@outlook.com

&Xinjiang Wang

Sensetime Research

wangxinjiang@sensetime.com

Wayne Zhang

Sensetime Research

wayne.zhang@sensetime.com &Rynson Lau

City University of Hong Kong

Rynson.Lau@cityu.edu.hk

###### Abstract

Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel **at different granularity levels** instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus enabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named _GLMix_: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (_e.g._, 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at https://github.com/rayleizhu/GLMix.

+
Footnote †: \({}^{}\) Rynson Lau is the corresponding author.

## 1 Introduction

Since the renaissance of deep learning over a decade ago, CNNs had dominated image analysis, until recently when transformers become popular in vision tasks. CNNs and transformers differ in how they model spatial feature interactions: CNNs use convolutions (Convs), while transformers use multi-head self-attentions (MHSAs). Both have their own advantages and limitations. For example, Convs have an inductive bias of translation equivariance, which matches the image property and enables decent performances with less data. They also have a linear complexity w.r.t. pixel number, making them scalable to high-resolution input. However, they have a limited receptive field, which cannot be remedied simply by stacking more layers together . In contrast, MHSAs can modellong-range dependency flexibly, but suffer from a quadratic complexity w.r.t. input resolution and require more data to compensate for the lack of inductive bias. Besides, some discussions  also point out that MHSAs play the role of low-pass filters, while Convs play the role high-pass ones. Hence, they are complementary to each other.

There are indeed some works that use both Convs and MHSAs to build vision backbones. Some of them alternate Convs and MHSAs across different stages/blocks , forming a loose collaboration. Others  integrate Convs and MHSAs tightly in each block. Specifically, they apply Convs and MHSAs in parallel at the same granularity level and fuse their outputs for further processing, as shown in Figure 1(top). With Convs responsible for fine-grained feature extraction, we ask if we still need to apply the heavy MHSAs at the pixel level. Meanwhile, recent vision-language models  have shown that an image can be described as a fixed number of visual tokens regardless of its resolution, possibly stemming from the low-rank property of natural signals. Inspired by these works, we propose a global-local mixing (GLMix) block, which uses Convs and MHSAs _at different granularities_ for different roles: while Convs focus on extracting local features, MHSAs focus on learning global inter-object relations. Specifically, in each block, we represent an image as both a fine-grained regular grid and a coarse-grained set of semantic slots, and then apply Convs to the grid and MHSAs to the slots in parallel. To enable local-global feature fusion, we introduce a pair of conjugated soft clustering and dispatching modules to bridge the grid and set representations. In this way, we achieve highly efficient local-global modeling by using lightweight Convs to extract high-resolution features and heavy MSHAs to process a fixed number of semantic slots.

To verify the performance of the proposed integration scheme for Convs and MHSAs, referred to as _GLMix_, we start by building a **S**win-**T**iny-**L**ayout model, referred to as GLNet-**STL**, based on the GLMix blocks. GLNet-STL achieves 82.5% top-1 accuracy on ImageNet-1k. It surpasses Swin-T (81.3% top-1 accuracy) significantly by 1.2%. Besides, we note that the macro architectural designs are also important factors for the performance of vision backbones. For example, PoolFormer  and ConvNext  reveal that with a deeper architecture, vision backbones can still achieve strong performances with simple token mixers such as average pooling and depth-wise convolution. Hence, we further adopt several macro designs from recent state-of-the-art vision backbones  and scale the model up to derive a family of 3 models: GLNet-4G/9G/16G. As a result, the GLNet-4G/9G/16G models achieve 83.7%/84.5%/85.0% top-1 accuracy, while being more efficient than recent state-of-the-art works (as shown in Figure 2). Evaluations on downstream dense prediction tasks such as object detection, instance segmentation, and semantic segmentation demonstrate the strengths of GLNet consistently. We also observe that a meaningful semantic grouping effect has emerged in the soft clustering module, even with only image-level classification supervision.

Figure 1: Existing integration schemes, _e.g._, ACMix , apply MHSAs and Convs at the same granularity (top). In contrast, we affirm that by offloading the burden of extracting fine-grained features to lightweight Convs, MHSAs can be aggressively applied to coarse semantic slots to make spatial mixing more efficient (bottom).

Figure 2: IN1k top-1 accuracy vs. FLOPs. While several recent state-of-the-art models (_i.e._, MaxViT , CSWin  and SG-Former , SMT ) lie in almost the same Pareto frontier, our GLNet models move the frontier further to the upper-right with a clear margin.

To summarize, our contributions are three-fold:

* We revisit existing integration approaches for Convs and MHSAs, and propose to integrate the two operations _at different granularities_. Such integration combines the strengths of Convs (_e.g._, the inductive bias of translation equivariance) and MSHAs (_e.g._, global interactions, data adaptivity) while avoiding the scalability issue w.r.t. the input resolution.
* We introduce a pair of conjugated, fully differentiable clustering and dispatching modules to bridge the set and grid representations of image features, hence enabling the fusion of the global features extracted by MHSAs and local features extracted by Convs. An advantage of the soft clustering module is that it produces meaningful semantic grouping effects without direct dense supervision.
* Through extensive experiments on various computer vision tasks, such as image classification, object detection, and instance/semantic segmentation, we empirically verify our proposed approach. Specifically, a new family of vision backbones, GLNet, demonstrates a favorable performance-computation trade-off to existing state-of-the-arts, under ImageNet-1k supervised training.

## 2 Related Works

**Efficient Attention Mechanisms.** Vanilla MHSAs Vanilla (2017) have a quadratic complexity w.r.t. the number of input tokens, causing huge computation burden and heavy memory footprints, especially in vision applications where the feature maps are in high-resolution. A large volume of works have been conducted to develop efficient variants to overcome such a limitation. These works can be roughly categorized as sparse approximations Dosovitskiy et al. (2016); He et al. (2017); He et al. (2018), low-rank approximations He et al. (2017); He et al. (2018), and kernel-based methods He et al. (2018); He et al. (2019); He et al. (2019). The global branch in the proposed GLMix block, which is a combination of soft clustering and dispatching modules and an MHSA, can be used independently as a low-rank attention approximation with not only key-value pairs but also queries being down-sampled. However, according to our experiments, such a usage produces poor performance (Table 6), possibly due to losing too many details and the lack of inductive bias. We find that using MSHAs and Convs in a complementary way is crucial to the success of our proposed GLNet family.

**Hybrid Vision Backbones.** Many works indicate that hybrid vision backbones, which use both Convs and MHSAs, can achieve better performances than pure transformers and CNNs. Among these works, some of them use Convs and MHSAs alternatively across different blocks or stages He et al. (2019); He et al. (2019); He et al. (2019); He et al. (2019), forming a loose collaboration between the two operators. Another approach adopted by several recent state-of-the-art works He et al. (2019); He et al. (2019); He et al. (2019); He et al. (2019); He et al. (2019) is to integrate Convs and MHSAs in each block tightly. Different from these works that apply Convs and MHSAs at the same granularity level, we find that by offloading the burden of extracting fine-grained and location-preserving features to lightweight depth-wise Convs, MHSAs can be applied aggressively on coarse semantic slots while achieving compelling performances with higher efficiency.

**Clustering for Representation Learning.** Clustering is a type of unsupervised learning method used to find meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. Existing works, such as He et al. (2019); He et al. (2019); He et al. (2019); He et al. (2019), have explored clustering for representation learning in deep neural networks. However, unlike ClusTR He et al. (2019) and TCFormer He et al. (2019), which use DPC-KNN He et al. (2019) for the clustering, the soft clustering module in our work is fully learnable and does not rely on predefined rules. In comparison with ClusterFormer He et al. (2019) and PaCaViT He et al. (2019), which perform cross-attention between the feature grid and cluster representations/slots, our work performs self-attention over the slots (_i.e._, queries and key-value pairs are both from the slots), making the attention even more lightweight. Besides, our soft clustering module is hardware-efficient because it is designed to be non-iterative and mainly involves a dense matrix multiplication.

## 3 Methodology

Modern vision backbones are usually built by alternating spatial modeling modules (_e.g._, Convs, MHSAs, spatial MLPs) and per-location feed-forward networks (FFNs, _i.e._, embedding MLPs). Much research has been dedicated to developing spatial modeling modules, which is also the primary focus of this work. Specifically, we seek an integration scheme for Convs and MHSAs, which can utilize the strengths of both and scale to high-resolution inputs. Without modifying the standard design of the two basic operators, our key idea is to represent input features twice as both a regular feature grid and a set of _semantic slots_, and then process the feature grid with Convs and the semantic slots with MHSAs (Sec. 3.1). A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the two representations, enabling local-global fusion (Sec. 3.2). Based on such an integration scheme we propose a new family of vision backbones named GLNet (Sec. 3.3).

### Convs and MHSAs at Different Granularities

Image features are usually organized as a regular grid in vision backbones. Such a representation preserves the spatial correspondence between features and the input image, which is necessary for downstream dense prediction tasks (_e.g._, semantic segmentation). Besides, extracting local features with the grid representation is convenient and efficient.

In addition to the grid representation, we also create an intermediate set representation composed of a fixed number of _semantic slots_ to enable efficient global context modeling. The reason is that although global interactions are usually expensive to compute, it is feasible to use a small amount (_e.g._, 64 in our experiment) of semantic slots to summarize an image , as images are natural signals with heavy spatial redundancy . Notably, the set of semantic slots that we use here is different from the sequence of visual tokens in plain ViTs . While each visual token corresponds to a hard-divided regular patch (_e.g._, \(16 16\) pixels), semantic slots are an abstraction of some "soft" irregular semantic regions, as shown in Figure 3.

We apply Convs to the grid representation to extract local features as they are lightweight and thus efficient in processing the fine-grained feature grid. To model global context, we apply MHSAs on semantic slots. This is a natural choice as MHSAs are permutation-equivariant operators, thus naturally suitable for the set representation. The scalability issue w.r.t. input resolution is avoided, as we have only a small number of semantic slots. Hence, the drawback of MHSAs is overcome.

Next, we illustrate how the set and grid representations are bridged by a pair of soft clustering and dispatching modules, so that local and global features can be fused.

### Bridging The Set and Grid Representations

To establish a connection between coarse-grained semantic slots and fine-grained feature grids, we need to create a correspondence between them. Although the classical k-means clustering is applicable for this purpose , it is suboptimal for two reasons. First, it is an iterative algorithm, which is inefficient on GPUs. Second, it is a heuristic approach, which cannot be end-to-end optimized. Hence,

Figure 4: Structure of our GLMix block. At the core is a pair of conjugated soft clustering and dispatching modules to bridge the set and grid representations and enable local-global fusion.

Figure 3: The semantic slots correspond to “soft” irregular semantic regions (left). Compared to using hard-divided regular patches (as adopted by plain ViTs ) on the right, our formulation is closer to tokenization in NLP.

we introduce a simplified and fully differentiable clustering module and the conjugated dispatching module to address these two problems, as shown in Figure 4. We illustrate the process below.

**Clustering (feature grid \(\) semantic slots).** Given an input feature grid, \(^{C H W}\), where \(C\) is the number of channels, \(H\) is the height, and \(W\) is the width, we first initialize \(M\) semantic slots, \(_{init}^{M C}\), via average pooling followed by shape flattening, as:

\[_{init}=(()).\] (1)

We then compute the correspondence logits, \(^{M HW}\), as scaled cosine similarity between the initial semantic slots \(_{init}\) and the flattened input features \(}^{HW C}\), as:

\[=(_{init},})/ .\] (2)

Here, the learnable scale factor \(\) smooths the distribution of \(\), preventing dominance by salient entrances. With the correspondence logits, we perform a 1-step update to derive refined semantic slots, \(^{M C}\), as the weighted sum of flattened features \(}\), as:

\[=()}.\] (3)

The refined semantic slots \(\) are then fed to MHSAs as input.

**Dispatching (semantic slots \(\) feature grid).** After transforming \(\) to propagate global context with an MHSA module, the transformed semantic slots \(^{}\) are dispatched to spatial locations for fusion with local features. Specifically, the dispatched features, \(^{C H W}\), are computed as:

\[=((^{T})^{}).\] (4)

\(\) can be readily fused with the feature grid processed by Convs due to shape compatibility. We follow the Feature Pyramid Network  to use additive fusion, as it is simple/lightweight and provides a regularization that aligns the global and local features in the same semantic space.

**Discussion.** The soft clustering and dispatching operations are highly efficient as the main computations lie in hardware-friendly dense matrix multiplications, and we perform only 1-step instead of iterative updates of the semantic slots. They are fully differentiable as we do not use hard assignments like k-means. The combination is similar to soft-routing in SoftMoE , which aims to build large mixture-of-expert models. However, as we target a better balance between cost/performance, our design has several differences: (1) slots are initialized with a different strategy (_i.e._, per-image average pooling instead of learned parameters shared by all images); (2) the clustering module is placed at a different position (_i.e._, in pair with token mixers instead of FFNs); and (3) significantly fewer slots are used (_i.e._, 64, instead of 4096 which is even more than the number of tokens for an image).

### GLNet

To verify the performance of the GLMix block proposed above, we start by creating a **S**win-**T**iny-**L**ayout architecture, named GLNet-**STL**, which follows the macro architectural designs of the Swin-T  model but with the spatial mixing modules (window attention and shift-window attention) replaced. Specifically, we use the GLMix block in the first three stages of GLNet-STL, where the feature maps are in high-resolution (\(\), \(\), \(\) of the input resolution). At the \(4^{th}\) stage, which is

   Models & Params & FLOPs & Throu. & INK Top-1 \\  & (M) & (M) & (G) & (im/s) & (\%) \\  Swin-T  & 29 & 4.5 & 755.2 & 81.3 \\ DAT-T  & 29 & 4.6 & — & 82.0 \\ Swin-ACM-T  & 30 & 4.6 & — & 81.9 \\ Flatten-Swin-T  & 29 & 4.5 & — & 82.1 \\ NAT-STL  & 29 & 4.5 & — & 81.4 \\ MaxViT-STL  & 28 & 4.5 & 763.4 & 81.8 \\  GLNet-STL (ours) & 30 & 4.4 & 835.9 & **82.5** \\   

Table 1: System-to-system comparison with existing works under the **S**win-**T**iny-**L**ayout protocol . \(\): implemented by us with modules provided by tim .

   Model & \(C\) & \(e\) & \#Blocks & Adv. designs & FLOPs \\  GLNet-STL & 96 & 4 & No & 4.4G \\  GLNet-4G & 64 & 3 & Yes & 4.5G \\ GLNet-9G & 96 & 3 & Yes & 9.7G \\ GLNet-16G & 128 & 3 & Yes & 16.7G \\   

Table 2: Model configurations of the GLNet family. \(C\): base channels (_i.e._, feature channels of the first stage). \(e\): FFN expansion ratio. #Blocks: the 4-stage block numbers. FLOPs are measured at \(224 224\) resolution.

[MISSING_PAGE_FAIL:6]

for model initialization when performing downstream dense prediction tasks. Note that for dense prediction tasks which take high-resolution inputs, we keep the number of semantic slots to 64, which is consistent with that of image classification. We have found that 64 slots are sufficient to achieve state-of-the-art performances while increasing the number does not help. We then visualize the semantic slots to demonstrate that a meaningful semantic grouping effect emerges in the proposed soft clustering module (Sec. 4.4). Finally, we conduct an ablation study on the design choices of the GLMix integration scheme, which is the core of GLNet (Sec. 4.5).

### Image Classification on ImageNet-1k

**Settings.** For a fair comparison with existing works, we conduct image classification experiments on the ImageNet-1k dataset , using the standard training recipe provided by Swin-Transformer  and the advanced distillation recipe provided by LV-ViT . The training details can be found in Appendix B. We then evaluate the models for classification accuracy and benchmark their throughputs with the script provided by the timm library , following the same hardware (a single Tesla V100 32G GPU) and batch size (128) configurations used in Swin-Transformer .

**Results.** In Table 3, we compare GLNet with several closely related methods and/or recent state-of-the-arts. Under the setting of standard supervised training, our GLNet-4G/9G/16G consistently show comparable or superior performances to existing best-performing models across different model scales. With dense distillation supervision, the potential of GLNets is further unleashed compared to standard supervised training. For example, the accuracy of the GLNet-4G model increases from 83.7% to 84.4%, a significant performance improvement of 0.7%. Both GLNet-4G and GLNet-9G provide a more competitive performance-FLOPs trade-off than other distilled models. As FLOPs is an indirect metric for practical inference speed and does not consider the memory access cost, we also plot the performance-throughput curve in Figure 2. The improvements become more pronounced when viewed w.r.t. throughputs. Interestingly, several of the latest vision backbones (_i.e._, SG-Former ,

    &  & FLOPs &  &  \\   & & (M) & (G) & mAP\({}^{b}\) & AP\({}^{b}_{0}\) & AP\({}^{b}_{15}\) & AP\({}^{b}_{17}\) & mAP\({}^{b}_{1}\) & mAP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) & mAP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) & AP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) \\  PVT-Small  & 34 & 226 & 40.4 & 61.3 & 43.0 & 25.0 & 42.9 & 55.7 & 42.2 & 62.7 & 45.0 & 26.2 & 45.2 & 57.2 \\ Swin-T  & 39 & 245 & 41.5 & 62.1 & 44.2 & 25.1 & 44.9 & 55.5 & 43.9 & 64.8 & 47.1 & 28.4 & 47.2 & 57.8 \\ Twins-SVT-S  & 34 & 210 & 43.0 & 64.2 & 46.3 & 28.0 & 46.4 & 57.5 & 45.6 & 67.1 & 48.6 & 29.8 & 49.3 & 60.0 \\ CrossFormer-S  & 41 & 272 & 44.4 & 65.8 & 47.4 & 28.2 & 48.4 & 59.4 & — & — & — & — & — \\ MaxViT-T  & 46 & 263 & 44.7 & 66.3 & 47.7 & 28.0 & 48.3 & 58.9 & — & — & — & — & — \\ BiFormer-S  & 35 & 243 & 45.9 & 66.9 & 49.4 & 30.2 & 49.6 & 61.7 & — & — & — & — & — \\ SMT-S  & 30 & 247 & — & — & — & — & — & — & 47.3 & 67.8 & 50.5 & 32.5 & 51.1 & 62.3 \\ GLNet-4G (ours) & 37 & 214 & **47.1** & **68.6** & **50.5** & **30.8** & **51.1** & **62.9** & **47.9** & **68.8** & **50.8** & **32.7** & **51.6** & **63.5** \\  Swin-S  & 60 & 335 & 44.5 & 65.7 & 47.5 & 27.4 & 48.0 & 59.9 & 46.3 & 67.4 & 49.8 & 31.1 & 50.3 & 60.9 \\ Twins-SVT-B  & 67 & 326 & 45.3 & 66.7 & 48.1 & 28.5 & 48.9 & 60.6 & 46.9 & 68.0 & 50.2 & 31.7 & 50.3 & 61.8 \\ CrossFormer-B  & 62 & 389 & 46.2 & 67.8 & 49.5 & 30.1 & 49.9 & 61.8 & — & — & — & — & — \\ ScalableiT-B  & 85 & 330 & 45.8 & 67.3 & 49.2 & 29.9 & 49.5 & 61.0 & 48.0 & 69.3 & 51.4 & 32.8 & 51.6 & 62.4 \\ MaxViT-S  & 79 & 389 & 46.1 & 68.0 & 49.5 & 28.9 & 50.2 & 61.4 & — & — & — & — & — \\ BiFormer-B  & 67 & 356 & 47.1 & 68.5 & 50.4 & 31.3 & 50.8 & 62.6 & — & — & — & — & — \\ GLNet-9G (ours) & 70 & 292 & **47.7** & **69.0** & **51.6** & **31.8** & **51.6** & **63.5** & **48.8** & **69.6** & **52.5** & **33.5** & **52.9** & **63.9** \\    
    &  & FLOPs &  &  \\   & & (M) & (G) & mAP\({}^{b}\) & AP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) & mAP\({}^{b}\) & AP\({}^{b}_{15}\) & mAP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) & mAP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) & mAP\({}^{b}_{10}\) & AP\({}^{b}_{15}\) \\  SwMaxiViT , and CSwin ) lie in almost the same Pareto frontier, while our GLNet models further move the frontier to the upper-right corner with a clear margin. Such a result demonstrates the superiority of our integration philosophy: by applying the heavy MHSAs at a coarse granularity and light-weight Convs at a fine granularity, spatial modeling can be both effective and efficient.

### Object Detection and Instance Segmentation

**Settings.** We evaluate the backbones for object detection and instance segmentation on COCO 2017 . All experiments are conducted using the MMDetection  toolbox to ensure a fair comparison with existing works. The RetinaNet  framework is used for object detection, and the Mask R-CNN  framework is used for instance segmentation. During training, we initialize the backbone with weights trained on ImageNet-1K while leaving all other layers randomly initialized. Input images are resized by fixing the shorter side to 800 pixels while restricting the longer side to no more than 1,333 pixels. We train the RetinaNet and Mask R-CNN detectors with \(1\) schedule (12 epochs) and \(3\) schedule (36 epochs) provided by MMDetection. More training details are provided in Appendix B. We report the widely used average precision (AP) metric family, such as mean average precision (mAP), average precision at different thresholds (AP\({}_{75}\) and AP\({}_{50}\)), and average precision for objects of different sizes (AP\({}_{S}\), AP\({}_{M}\) and AP\({}_{L}\)). Details of these metrics can be found in MMDetection .

**Results.** We show the results for object detection and instance segmentation in Table 4. Our method achieves the best performances among the compared methods across all metrics and the two model sizes in both cases. These results indicate that local-global modeling with the GLMix block benefits object/instance-level tasks.

### Semantic Segmentation on ADE20K

**Settings**. Our semantic segmentation experiments are conducted on the ADE20K dataset using the MMSegmentation  toolbox. We evaluate our approach using two frameworks - Semantic FPN  and UperNet . In both cases, the backbone is initialized with ImageNet-1k weights, while the other layers are randomly initialized. For a fair comparison, we follow the same setting as PVT  to train the model 80k steps in our Semantic FPN experiments. On the other hand, for our UperNet experiments, we follow the settings used in Swin Transformer  and train the model for 160k iterations. More training details are provided in Appendix B. We report the mean intersection over union (mIoU) metric with no test-time augmentation.

**Results**. Table 5 shows the results of the two different frameworks. Our GLNet-4G/16G achieve 49.6/51.3 mIoU with the Semantic FPN framework, improving the previous best SG-Former-S/M by 0.6/1.2 mIoU. A similar performance gain for the UperNet framework is also observed. The enhancements demonstrate the benefits of utilizing GLNet for high-resolution pixel-wise predictions.

    &  &  \\  & \#Param. & FLOPs mIoU & \#Param. & FLOPs mIoU \\  & (M) & (G) & (\%) & (M) & (G) & (\%) \\  PVT-S  & 28.2 & 161 & 39.8 & — & — & — \\ Swin-T  & 31.9 & 182 & 41.5 & 59.9 & 945 & 44.5 \\ Twins-SVT-S  & 28.3 & 144 & 43.2 & 54.4 & 901 & 46.2 \\ CSwin-T  & 26.1 & 202 & 48.2 & 59.9 & 959 & 49.3 \\ BiFormer-S  & 29.3 & 173 & 48.9 & 55.3 & 930 & 49.8 \\ SG-Former-S  & 25.4 & 205 & 49.0 & 52.5 & 989 & 49.9 \\ SMT-S  & — & — & — & 50.1 & 935 & 49.2 \\ GLNet-4G (ours) & 30.7 & 150 & **49.6** & 56.8 & 907 & **50.6** \\   PVT-M  & 48.0 & 219 & 41.6 & — & — & — \\ Swin-S  & 53.2 & 274 & 45.2 & 81.3 & 1038 & 47.6 \\ Twins-SVT-B  & 60.4 & 261 & 45.3 & 88.5 & 1020 & 47.7 \\ CSwin-S  & 38.5 & 271 & 49.2 & 64.6 & 1027 & 50.4 \\ BiFormer-B  & 60.4 & 282 & 49.9 & 88.5 & 1041 & 51.0 \\ SG-Former-M  & 38.2 & 273 & 50.1 & 68.3 & 1114 & 51.2 \\ SMT-B  & — & — & — & 61.8 & 1004 & 49.6 \\ GLNet-9G (ours) & 63.6 & 230 & **51.3** & 91.7 & 988 & **51.4** \\   

Table 5: Performance comparison of different backbones on the ADE20K segmentation task. We report mIoU with no test-time augmentation. FLOPs are computed at 512 \(\) 2048 resolution.

### Visualization of Semantic Slots

As mentioned in Sec. 3.2, the conjugated clustering and dispatching modules construct a correspondence between the semantic slots and the feature grid. Such a formulation allows us to visualize which regions the semantic slots correspond to. Specifically, we extract the clustering weights in Eq. 3 and split them into M scalar maps of shape H \(\) W. These scalar maps are then pseudo-colored for visualization. In addition, we use the k-medoids algorithm to select four representative slots for a closer look automatically. We find that a meaningful semantic grouping effect emerges in the first block of stage 3, as shown in Figure 5. Note that we use ImageNet-1k trained GLNet-STL for visualization. Hence, the model receives no dense supervision. Visualization for more samples, more blocks and at different epochs can be found in Appendix C.

### Ablation Study

We ablate our GLMix integration scheme using the GLNet-STL model. By default, we use a global branch with 64 semantic slots and a local branch with \(5 5\) depth-wise conv in parallel in the GLMix blocks, as shown in Figure 4. With this default setting, we investigate the effect of **(a)** local-global collaboration, **(b)** the clustering strategy, **(c)** Conv kernel size in the local branch, and **(d)** number of slots in the global branch. Table 6 shows the experimental results. We summarize our findings below.

**Local-global collaboration.** First, using both local and global branches together is crucial. With the global/local branch removed, the model has a significantly degraded accuracy of 81.8%/78.0%, indicating that both coarse-grained inter-object relationship and fine-grained per-pixel local context are important. Second, using global and local branches in parallel instead of sequentially is important. A possible explanation is that the parallel layout provides a regularization for the global branch from the local branch. Otherwise, the global branch is difficult to optimize due to the lack of inductive bias. Finally, using Convs in the local branch is better than window MHSAs, as the latter are heavier and significantly decrease the throughput from 835.9 im/s to 660.9 im/s. This may be because Convs can implicitly bring position information via padding [25; 9] while window MHSAs cannot.

**Clustering strategy.** The soft clustering approach is an important component of the GLMix block. Using the k-means clustering results do not only produce a significantly lower throughput (835.9 im/s \(\) 440.6 im/s) but also incurs unstable training. This can be attributed to the fact that k-means is an iterative, non-differentiable algorithm, as mentioned in Sec. 3.2. We also observe that initializing the semantic slots as learnable parameters decreases the accuracy from 82.5% to 82.1%. This implies that per-image adaptive initialization is better than static initialization. Possibly, there are difficulties to learn diverse contexts for each image with shared parameters as the slot initialization, according to visualizations in Appendix C.

**Convolution kernel size in the local branch.** The model is robust to the convolution kernel size in the local branch. Using a kernel size of 3 or 7 produces a similar accuracy (82.4%) to the kernel size of 5 (82.5%). This is because the global branch has provided a sufficient large receptive field.

Figure 5: Visualization of semantic slots. For each sample, we show the input image (left), assignment maps of all semantic slots (middle), and four representative slots (right). We use the k-medoids algorithm to select the four representative slots automatically.

**Number of semantic slots in the global branch.** Using 64 semantic slots is sufficient to achieve a good performance. Although the accuracy decreases to 82.3% with fewer semantic slots (_e.g_., 9, 25, 36 or 49), increasing the number to 81 also incurs a small performance drop to 82.4%. We hypothesize that this is due to the optimization difficulty caused by too many similar/near-duplicate slots .

## 5 Conclusion

In this paper, we have revisited the existing integration approaches for Convs and MHSAs, and proposed to apply the two operators at _different granularity levels_. We discover that by offloading the task of extracting fine-grained features to the lightweight Convs, the heavy MHSAs can be aggressively applied to a few semantic slots. Such an integration scheme, named GLMix, enables highly efficient local-global modeling to build high-performance vision backbones. A key component of GLMix is a pair of conjugated soft clustering and dispatching modules for bridging the feature grid and the set of semantic slots. Meaningful semantic grouping effects, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches, are observed in the clustering process.

Currently, we only consider using a static number of semantic slots (_i.e_., 64 in our experiments) for all images. This may cause many redundant slots representing the same content, as shown in Figure 5. It may be interesting to design a dynamic slot pruning mechanism for more efficient computation and end-to-end weakly-supervised segmentation. Another drawback of GLNet is that it still incorporates many hardware-inefficient depth-wise convolutions with low arithmetic intensity. Seeking more hardware-friendly alternatives will further improve its throughputs on modern hardware.