# A polar prediction model for learning to represent visual transformations

Pierre-Etienne H. Fiquet\({}^{1}\)  Eero P. Simoncelli\({}^{1,2}\)

\({}^{1}\) Center for Neural Science, New York University

\({}^{2}\) Center for Computational Neuroscience, Flatiron Institute

{pef246, eero.simoncelli}@nyu.edu

###### Abstract

All organisms make temporal predictions, and their evolutionary fitness level depends on the accuracy of these predictions. In the context of visual perception, the motions of both the observer and objects in the scene structure the dynamics of sensory signals, allowing for partial prediction of future signals based on past ones. Here, we propose a self-supervised representation-learning framework that extracts and exploits the regularities of natural videos to compute accurate predictions. We motivate the polar architecture by appealing to the Fourier shift theorem and its group-theoretic generalization, and we optimize its parameters on next-frame prediction. Through controlled experiments, we demonstrate that this approach can discover the representation of simple transformation groups acting in data. When trained on natural video datasets, our framework achieves better prediction performance than traditional motion compensation and rivals conventional deep networks, while maintaining interpretability and speed. Furthermore, the polar computations can be restructured into components resembling normalized simple and direction-selective complex cell models of primate V1 neurons. Thus, polar prediction offers a principled framework for understanding how the visual system represents sensory inputs in a form that simplifies temporal prediction.

## 1 Introduction

The fundamental problem of vision can be framed as that of representing images in a form that is more useful for performing visual tasks, be they estimation, recognition, or motor action. Perhaps the most general "task" is that of temporal prediction, which has been proposed as a fundamental goal for unsupervised learning of visual representations . Previous research along these lines has generally focused on estimating stable representations rather than using them to predict: for example, extracting slow features , or finding sparse codes that have slow amplitudes and phases .

In video processing and computer vision, a common strategy for temporal prediction is to first estimate local translational motion, and then (assuming no acceleration) use this to warp and/or copy previous content to predict the next frame. Such motion compensation is a fundamental component in video compression schemes as MPEG . These video coding standards are the result of decades of engineering effort , and have enabled reliable and efficient digital video communication that is now commonplace. But motion estimation is a difficult nonlinear problem, and existing methods fail in regions where temporal evolution is not translational and smooth: for example, expanding or rotating motions, discontinuous motion at occlusion boundaries, or mixtures of motion arising from semi-transparent surfaces (e.g., viewing the world through a dirty pane of glass). In compression schemes, these failures of motion estimation lead to prediction errors, which must then be adjusted by sending additional corrective bits.

Human perception does not seem to suffer from such failures--subjectively, we can anticipate the time-evolution of visual input even in the vicinity of these commonly occurring non-translational changes. In fact, those changes are often highly informative, revealing object boundaries, and providing ordinal depth and shape cues and other information about the visual scene. This suggests that the human visual system uses a different strategy, perhaps bypassing altogether the explicit estimation of local motion, to represent and predict evolving visual input. Toward this end, and inspired by the recent hypothesis stating that primate visual representations support prediction by "straightening" the temporal trajectories of naturally-occurring input , we formulate an objective for learning an image representation that facilitates prediction by linearizing the temporal trajectories of frames in natural videos.

To motivate the separation of spatial representation and temporal prediction, we first consider the special case of rigidly translating signals in one dimension. In the frequency domain, translation corresponds to phase advance (section 2.1), which reduces prediction to phase extrapolation (section 2.2). We invoke basic arguments from group theory to motivate the search for generalized representations (section 2.3). We propose a neural network architecture that maps individual video frames to a latent space where prediction can be computed more readily and then mapped back to generate an estimated frame (section 3). We train the entire system end-to-end to minimize next frame prediction errors and verify that, in controlled experiments, the learned weights recover the representation of the group used to generate synthetic data. On natural video datasets, our framework consistently outperforms conventional motion compensation methods and is competitive with deep predictive neural networks (section 4). We establish connections between each element of our framework and the modeling of early visual processing (section 4.3).

## 2 Background

### Base case: the Fourier shift theorem

Our approach is motivated by the well-known behavior of Fourier representations with respect to signal translation. Specifically, the complex exponentials that constitute the Fourier basis are the eigenfunctions of the translation operator, and translation of inputs produces systematic phase advances of frequency coefficients. Let \(=[x_{0},,x_{N-1}]^{}^{N}\) be a discrete signal indexed by spatial location \(n[0,N-1]\), and let \(}^{N}\) be its Fourier transform indexed by \(k[0,N-1]\). We write \(^{1^{}}\), the circular shift of \(\) by \(v\), \(x_{n}^{1^{}}=x_{n-v}\) (with translation modulo \(N\)). Let \(\) denote the primitive \(N^{}\) root of unity, \(=e^{12/N}\), and let \(^{N N}\) denote the Fourier matrix, \(F_{nk}=}^{nk}\). Multiplication by the adjoint (i.e. the conjugate transpose), \(^{*}\), gives the Discrete Fourier Transform (DFT), and by \(\) the inverse DFT. We can express the Fourier shift theorem1

\[}}}_{k}=_{n=0}^{N-1}^{-kn}x_{n-v}=_{m= -v}^{N-1-v}^{-kv}^{-km}x_{m}=^{-kv}_{n=0}^{N-1}^{-kn}x_{n} =^{-kv}_{k}.\]

 as: \(^{1^{}}=(_{v})^ {*}\), where \(_{v}=[^{0},^{-v},^{-2v},,^{-(n-1)v}]^{}\) is the vector of phase shift by v. See Appx. A for a consolidated list of notation used throughout this work.

This relationship can be depicted in a compact diagram:

\[_{k}@>{}>{_{k}}>{}}>^{-kv}_{k}\\ @V{}V{^{*}}V@V{}V{}V\\ x_{n}@>{}>{}>x_{n-v}\] (1)

This diagram illustrates how transforming to the frequency domain renders translation a "simpler" operation: phase shift acts as rotation in each frequency subspace, i.e. it is diagonalized.

### Prediction via phase extrapolation

Consider a signal, the \(N\)-dimensional vector \(_{t}\), that translates at a constant velocity \(v\) over time: \(x_{n,t}=x_{n-vt,0}\). This sequence traces a highly non-linear trajectory in signal space, i.e. the vector space where each dimension corresponds to the signal value at one location. In this space, linear extrapolation fails. As an example, Figure 1 shows a signal consisting of a sum of two sinusoidal components in one spatial dimension. Mapping the signal to the frequency domain simplifies the description. In particular, the translational motion now corresponds to circular motion of the two (complex-valued) Fourier coefficients associated with the constituent sinusoids. In polar coordinates, the trajectory of these coefficients is straight, with both phases advancing linearly (at a rate proportional to their frequency), and both amplitudes constant.

### Generalization: representing transformation groups

Streaming visual signals are replete with structured transformations, such as object displacements and surface deformations. While these can not be captured by the Fourier representation, which only handles global translation, the concept of representing transformations in their eigen-basis generalizes. Indeed, representation theory describes elements of general groups as linear transformations in vector spaces, and decomposes them into basic building blocks . However, the transformation groups acting in image sequences are not known a priori, and it can be difficult to give an explicit representation of general group actions. In this work, we aim to find structures that can be modeled as groups in image sequences, and we learn their corresponding representations from unlabeled data.

In harmonic analysis, the Peter-Weyl Theorem (1927) establishes the completeness of the unitary irreducible representations for compact topological groups (an irreducible representation is a subspace that is invariant to group action and that can not be further decomposed). Furthermore, every compact Lie group admits a faithful (i.e. injective) representation given by an explicit complete orthogonal basis, constructed from finite-dimensional irreducible representations . Accordingly, the action of a compact Lie group can be expressed as a rotation within each irreducible representation (an example is the construction of steerable filters  in the computational vision literature).

In the special case of compact commutative Lie groups, the irreducible representations are one-dimensional and complex-valued (alternatively, pairs of real valued basis functions). These groups have a toroidal topology and, in this representation, their action can be described as advances of the phases. This suggests a strategy for learning a representation: seek pairs of basis functions for which phase extrapolation yields accurate prediction of upcoming images in a stream of visual signals.

Figure 1: Straightening translations. **(a)** Three snapshots of a translating signal consisting of two superimposed sinusoidal components: \(x_{n,t}=(2(n-t))+(2 3(n-t))/2\). **(b)** Projection of the signal into the space of the top three principal components. The colored points correspond to the three snapshots in panel (a). In signal space, the temporal trajectory is highly curved—linear extrapolation fails. **(c)** Complex-valued Fourier coefficients of the signal as a function of frequency. The temporal trajectory of the frequency representation is the phase advance of each sinusoidal component. **(d)** Trajectory of one amplitude and both (unwrapped) phases components. The conversion from rectangular to polar coordinates reduces the trajectory to a straight line—which is predictable via linear extrapolation.

Methods

### Objective function

We aim to learn a representation of video frames that enables next frame prediction. Specifically, we optimize a cascade of three parameterized mappings: an analysis transform (\(f_{w}\)) that maps each frame to a latent representation, a prediction in the latent space (\(p_{w}\)), and a synthesis transform (\(g_{w}\)) that maps the predicted latent values back to the image domain. The parameters \(w\) of these mapping are learned by minimizing the average squared prediction error:

\[_{w}_{t}\|_{t+1}-g_{w}(}_{t+1})\|^{2};  28.452756pt}_{t+1}=p_{w}(_{t},_{t-1}),_{t}=f_{w}(_{t}).\] (2)

An instantiation of this framework is illustrated in Figure 2. Here the analysis and synthesis transforms are adjoint linear operators, and the predictor is a diagonal phase extrapolation.

### Analysis-synthesis transforms

Local processingWhen focusing on a small spatial region in an image sequence, the transformation observed as time passes can often be well approximated as a _local_ translation. That is to say, in a spatial neighborhood around position \(n\), \(m N(n)\), we have: \(x_{m,t+1} x_{m-v,t}\). We can use the decomposition described for global rigid translation, replacing the Fourier transform with a learned local convolutional operator , processing each spatial neighborhood of the image independently and in parallel.

At every position in the image (spatial indices are omitted for clarity of notation), each pair of coefficients is computed as an inner product between the input and the filter weights of each pair of channels. Specifically for \(k[0,K]\), where \(K\) is the number of pairs of channels, we have: \(y_{2k,t}=_{2k}^{T}_{t}\) and \(y_{2k+1,t}=_{2k+1}^{}_{t}\). Correspondingly, an estimated next frame is generated by applying the transposed convolution \(g_{w}\) to advanced coefficients (see section 3.3). We use the same weights for the encoding and decoding stages, that is to say the analysis operator is the conjugate transpose of the synthesis operator (as is the case for for the Fourier transform and its inverse). Sharing these weights reduces the number of parameters and simplifies interpretation of the learned solution.

Figure 2: **Polar prediction model**. The previous and current images in a sequence (\(_{t-1}\) and \(_{t}\)) are convolved with pairs of filters (\(^{*}\)), each yielding complex-valued coefficients. For a given spatial location in the image, the coefficients for each pair of filters are depicted in complex planes with colors corresponding to time step. The coefficients at time \(t+1\) are predicted from those at times \(t-1\) and \(t\) by extrapolating the phase (\(_{t}\)). These predicted coefficients are then convolved with the adjoint filters (\(\)) to generate a prediction of the next image in the sequence (\(}_{t+1}\)). This prediction is compared to the next frame (\(_{t+1}\)) by computing the mean squared error (MSE) and the filters are learned by minimizing this error. Notice that, at coarser scales, the coefficient amplitudes tend to be larger and the phase advance smaller, compared to finer scales.

Multiscale processingTransformations such as translation act on spatial neighborhoods of various sizes. To account for this, the image is first expanded at multiple resolutions in a fixed overcomplete Laplacian pyramid ; then learned spatial filtering (see previous paragraph) and temporal processing (see section 3.3) are applied on these coefficients; and finally, the modified coefficients are recombined across scales to generate the predicted next frame (see details in the caption of Figure 3).

### Prediction mechanism

Polar predictorIn order to obtain phases, we group coefficients in pairs, express them as complex-valued: \(z_{k,t}=y_{2k,t}+iy_{2k+1,t}\), and convert to polar coordinates: \(z_{k,t}=a_{k,t}e^{i_{k,t}}\). With this notation, linear phase extrapolation amounts to \(_{k,t+1}=a_{k,t}e^{i(_{k,t}+_{k,t})}\), where the phase advance \(_{k,t}\) is equal to the phase difference over the interval from \(t-1\) to \(t\): \(_{k,t}=_{k,t}-_{k,t-1}\). Note that we assume no phase acceleration and constant amplitudes. The phase-advanced coefficients can be expressed in a more direct way, using complex arithmetic, as:

\[_{k,t+1}=_{k,t}z_{k,t},_{k,t}= }}{|z_{k,t}||z_{k,t-1}|},\] (3)

with \(\) and \(|z|\) respectively denoting complex conjugation and complex modulus of \(z\). This formulation in terms of products of complex coefficients2 has the benefit of handling phases implicitly, which makes phase processing computationally feasible, as previously noted in the texture modeling literature [11; 12]. Optimization over circular variables suffers from a discontinuity if one represents the variable over a finite interval (e.g. \([-,]\)). Alternatively, procedures for "unwrapping" the phase are generally unstable and sensitive to noise.

\[(a+ib)(c+id)=ac-bd+i((a+b)(c+d)-ac-bd).\]

In summary, given a video dataset \(X=[_{1},,_{T}]^{N T}\), the convolutional filters of a polar prediction model are learned by minimizing the average squared prediction error:

\[_{^{N NK}}_{t=1}^{T}|| _{t+1}-(_{t})^{* }_{t}||^{2};\] \[_{t}=(_{t} _{t-1}})(|_{t}||_{t-1}|), _{t}=^{*}_{t}.\] (4)

The columns of the convolutional matrix \(\) contain the \(K\) complex-valued filters, \(_{k}=_{2k}+i_{2k+1}^{N}\) (repeated at \(N\) locations) and multiplication, division, amplitude, and complex conjugation are computed pointwise.

This "polar predictor" (hereafter, **PP**) is depicted in figure 2. Note that the conversion to polar coordinates is the only non-linear step used in the architecture. This bivariate non-linear activation function differs markedly from the typical (pointwise) rectification operations found in convolutional neural networks. Note that the polar predictor is homogeneous of degree one, since it is computed as the ratio of a cubic over a quadratic.

Figure 3: **Laplacian pyramid.** An image is recursively split into low frequency approximation and high frequency details. Given the initial image \(=_{j=0}^{N}\), the low frequency approximation (aka. Gaussian pyramid coefficients) is computed via blurring (convolution with a fixed filter \(B\)) and downsampling (“stride” of 2, denoted \(2_{}\)): \(_{j}=2_{}(B_{j-1}^{2^{-j}N})\), for levels \(j[1,J]\); and the high frequency details (aka. Laplacian pyramid coefficients) are computed via upsampling (put one zero between each sample, \(2^{}\)) and blurring: \(_{j}=_{j}-B(2^{}_{j+1})\). These coefficients, \(\{_{j}\}_{0 j<J}\), as well as the lowpass, \(x_{J}\), can then be further processed. A new image is constructed recursively on these processed coefficients. First by upsampling the lowest resolution, and then by adding the corresponding details until the initial scale \(j=0\) as: \(_{j}=B(2^{}_{j+1})+_{j}\).

Quadratic predictorRather than building in the phase extrapolation mechanism, we now consider a more expressive parametrization of the predictor (\(p_{w}\)) that can be learned on data jointly with the analysis and synthesis mappings. This "quadratic predictor" (hereafter, **QP**) generalizes the polar extrapolation mechanism and can accommodate groups of channels of size larger than two (as for the real and imaginary part in the polar predictor).

First, we rewrite the phase extrapolation mechanism of equation 3 using only real-valued elements:

\[_{2k,t+1}\\ _{2k+1,t+1}=_{k,t}&- _{k,t}\\ _{k,t}&_{k,t}y_{2k,t}\\ y_{2k+1,t},\] (5)

where the \(2 2\) prediction matrix, \(_{k,t}\), is a rotation by angle \(_{k,t}\). Using an elementary trigonometric identity: \((a-b)=(a)(b)+(a)(b)\) and recalling that \(y_{k,t}=_{2k}^{}_{t}=a_{k,t}(_{k,t})\), the elements of this prediction matrix can be made explicit. They are a quadratic function of the normalized response, which we write \(u_{2k,t}\) (for unit vector), as:

\[_{k,t}=u_{2k,t}u_{2k,t-1}+u_{2k+1,t}u_{2k+1,t-1},u_{2k,t}=}{(y_{2k,t}^{2}+y_{2k+1,t}^{2})^{1/2}}.\] (6)

This quadratic function can be expressed in terms of squared quantities (without cross terms) using the polarisation identity: \(=((+)^{2}-(-)^{2})/4\). Specifically:

\[_{k,t}=(u_{2k,t}+u_{2k,t-1})^{2} -(u_{2k,t}-u_{2k,t-1})^{2}+\] \[(u_{2k+1,t}+u_{2k+1,t-1})^{2}-(u_{2k+1,t}-u_{2k+1,t-1})^{2}.\] (7)

An analogous expression can be derived for \(_{k,t}\). These operations are depicted in Figure 4: current and previous pairs of coefficients (\(_{k,t}=[y_{2k,t},y_{2k+1,t},y_{2k,t-1},y_{2k+1,t-1}]^{}^{4}\)) are normalized (\(_{k,t}^{4}\)), then linearly combined (\(_{1}^{4 d}\)), pointwise squared, and linearly combined again (\(_{2}^{4 d}\)) to produce a prediction matrix (\(_{k,t}^{2 2}\)) that is applied to the current coefficients to produce a prediction (\([_{2k,t+1},_{2k+1,t+1}]^{}^{2}\)). These linear combination can be learned jointly with the analysis and synthesis weights by minimizing the prediction error.

In summary, the quadratic predictor is learned as:

\[_{,_{1},_{2}}_{t=1}^{T}|| _{t+1}-_{t}^{}_{t}||^{2};_{t}=(_{1,t}, ,_{K,t}),\] \[_{k,t}=_{2}(_{1}^{}_{ k,t})^{ 2},_{k,t}=_{k,t}}{(_{g}^{} _{k,t}^{2})^{1/2}},_{t}=^{}_{t}.\] (8)

The columns of the convolutional matrix \(^{N gNK}\) contain the groups of \(g\) filters (repeated at N locations). The number of channels in a group is no longer limited to pairs (\(|g| 2\) is a hyper-parameter). The prediction matrices, \(_{k,t}^{g g}\), are computed as a Linear-Square-Linear cascade on normalized activity from group of coefficients at the previous two time points: \(_{k,t}=[u_{k,t},u_{k+1,t},,u_{k+g-1,t}]^{}^{g}\) and \(_{k,t-1}\). The linear combinations are learnable matrices \(_{1}^{2g d}\) and \(_{2}^{g^{2} d}\), where \(d\) is the number of quadratic units and squaring is computed pointwise. Note that in the case of pairs (\(g=2\)), six quadratic units suffice (\(d=6\)).

Figure 4: **Learnable quadratic prediction mechanism.** Groups of coefficients (\(_{k,t}\)) at the previous and current time-step are normalized (\(_{k,t}\)) and then passed through in a Linear-Square-Linear cascade to produce a prediction matrix (\(_{k,t}\)). This matrix is applied to the current vector of coefficients to predict the next one. The linear transforms (\(_{1}\) and \(_{2}\)) are learned. This quadratic prediction module contains phase extrapolation as a special case and handles the more general case of groups of coefficients beyond pairs.

Results

### Recovery of planted symmetries

To experimentally validate our approach, we first verified that both the PP and the QP models can robustly recover known symmetries in small synthetic datasets consisting of translating or rotating image patches. For these experiments, the analysis and synthesis transforms are applied to the entire patch (i.e., no convolution). Learned filters for each of these cases are displayed in Figure 7, Appendix B. When trained on translating image patches, the learned filters are pairs of plane waves, shifted in phase by \(/2\). Similarly, when trained on rotating patches, the learned filters are conjugate pairs of circular harmonics. This demonstrates that theses models can recover the (regular) representation of some simple groups from observations of signals where their transformations are acting. When the transformation is not perfectly translational (e.g. translation with open boundary condition), the learned filters are localized Fourier modes. Note that when multiple kinds of transformations are acting in data (e.g., mixtures of both translations and rotations), the PP model is forced to compromise on a single representations. Indeed, the phase extrapolation mechanism is adaptive but the basis in which it is computed is fixed and optimized. A more expressive model would also allow for adaptation of the basis itself.

### Prediction performance on natural videos

We compare our multiscale polar predictor (**mPP**) and quadratic predictor (**mQP**) methods to a causal implementation of the traditional motion-compensated coding (**cMC**) approach. For each block in a frame, the coder searches for the most similar spatially displaced block in the previous frame, and communicate the displacement coordinates to allow prediction of frame content by translating blocks of the (already transmitted) previous frame. We also compare to phase-extrapolation within a steerable pyramid , an overcomplete multi-scale decomposition into oriented channels (**SPyr**). We also implemented a deep convolutional neural network predictor (**CNN**), that maps two successive observed frames to an estimate of the next frame . Specifically, we use a CNN composed of 20 non-linear stages, each consisting of 64 channels, and computed with \(3 3\) filters without additive constants, followed by half-wave rectification. Finally, we also consider a U-net  which is a CNN that processes images at multiple resolutions (**Unet**). The number of non-linear stages, the number of channels and the filter size match that of the basic CNN. See descriptions in Appendix C for architectures and Appendix D for dataset and training procedures.

Prediction results on the DAVIS dataset  are summarized in Table 1. First, observe that the predictive algorithms considered in this study perform significantly better than baselines obtained by simply copying the last frame, causal motion compensation, or phase extrapolation in a steerable pyramid. Second, the multiscale polar predictor performs better than the convolutional neural networks on test data (both CNN and Unet are overfit). This demonstrates the efficiency of the polar predictor: the PP model has roughly 30 times fewer parameters than the CNN and uses a single non-linearity, while the CNN and Unet contain 20 non-linear layers. Appendix E contains additional a comparison of computational costs for these algorithms: number of trainable parameters, training and inference time. Note that on this dataset, the Unet seems to overfit to the training set. Moreover, the added expressivity afforded by the multiscale quadratic predictor did not result in significant performance gains. A representative example image sequence and the corresponding predictions are displayed in Figure 5. Additional results on a second natural video dataset are detailed in Appendix E and confirm these trends.

  &  \\   & Copy & cMC & SPyr & **mPP** & **mQP** & CNN & Unet \\  train & 21.32 & 23.83 & 25.13 & 25.31 (0.04) & 25.38 (0.11) & 25.78 (0.18) & **26.91** (0.38) \\ test & 20.02 & 22.37 & 23.82 & **24.11** (0.01) & 24.04 (0.06) & 23.58 (0.05) & 23.94 (0.06) \\ 

Table 1: **Performance comparison. Prediction error computed on the DAVIS dataset. Values indicate mean Peak Signal to Noise Ratio (PSNR in dB) and standard deviation computed over 10 random seeds (in parentheses).**

### Biological modeling

The polar prediction model provides a hypothesis for how cortical circuits in the primate visual system might compute predictions of their inputs ("predictive processing") [17; 18; 19; 20]. This framework is agnostic to how predictions are used and is complementary to candidate algorithms for signaling predictions and prediction errors across the visual hierarchy ("predictive coding") [21; 22; 23].

First, the learned convolutional filters of a polar prediction model resemble receptive fields of neurons in area V1 (primary visual cortex). They are selective for orientation and spatial frequency, they tile the frequency domain (making efficient use of limited resources), and filters in each pair have similar frequency selectivity and are related by a shift in spatial phase. Representative filters are displayed in Figure 6.

Second, the quadratic prediction mechanism derived in section 3.3 suggests a qualitative bridge to physiology. Computations for this model are expressed in terms of canonical computational elements of early visual processing in primates. The normalized responses \(u_{2k}(t)\) in equation 6 are linear projections of the visual input divided by the energy of related cells, similar to the normalization behavior observed in simple cells . The quadratic units \(m_{g}\) in equation 7 are sensitive to temporal change in spatial phase. This selectivity for speed in a given orientation and spatial frequency is reminiscent of direction-selective complex cells which are thought to constitute the first stage of motion estimation [25; 26].

## 5 Related work

The polar prediction model is conceptually related to representation learning methods that aim to factorize visual signals. In particular, sparse coding with complex-valued coefficients  aims to factorize form and motion. More generally, several methods adopt a Lie group formalism to factorize _invariance_ and _equivariance_. Since the seminal work that proposed learning group generators from dynamic signals , a polar parametrization was explored in  to identify irreducible representations in a synthetic dataset, and a corresponding neural circuit was proposed in . The Lie group formalism has also been combined with sparse coding [30; 31] to model natural images as points on a latent manifold. More recently, bispectral neural networks  have been shown to learn image representations invariant to a given global transformation. In a related formalism, factored Boltzmann machines have been proposed to learn relational features . The polar prediction model differs in two important ways: (1) unlike the coding approach that operates on _iid_ data, it focuses on predicting, not representing, the signal; (2) the prediction objective does not promote sparsity of either amplitude or phase components and does not rely on explicit regularization. The discontinuity arising from selection of sparse subsets of coefficients seems at odds with the representation of continuous group actions . The polar prediction model relies on a smooth and continuous parameterization to jointly discover and exploit the transformations acting in sequential data. The polar prediction model is convolutional and scales to natural video data, it can adapt to the multiple unknown and noisy

Figure 5: **Example image sequence and predictions. A typical example image sequence from the DAVIS test set. The first three frames on the top row display the unprocessed images, and last five frames show the respective prediction for each method. The bottom row displays error maps computed as the difference between the target image and each predicted next frame on the corresponding position in the first row. All subfigures are shown on the same scale.**transformations that act in different spatial position. The literature on motion microscopy describes temporal processing of local phases in a fixed complex-valued wavelet representation to interpolate between video frames and magnify imperceptible movements . Polar prediction processes phase in a learned representation to extrapolate to future frames.

The polar prediction model is related to other representation learning methods that also rely on temporal prediction. Temporal stability was used to learn visual representations invariant to geometric transformations occurring in image sequences . The idea of learning a temporally straightened representation from image sequences was explored using a heuristic extrapolation mechanism  (specialized "soft max-pooling" and "soft argmax-pooling" modules). A related approach aimed at finding video representations which decompose content and pose and identify "components" in order to facilitate prediction of synthetic image sequences . To tackle the challenge of natural video prediction, more sophisticated architectures have been developed for decomposing images into predictable objects . A recurrent instantiation of predictive coding through time relying on a stacked convolutional LSTM architecture was proposed  and shown to relate to biological vision . In contrast, the polar prediction model scales to prediction of natural videos while remaining interpretable. Another related approach, originating in the fluid mechanics literature, focuses on the Koopman operator . This approach is a dynamical analog of the kernel trick: it lifts a system from its original state-space into a higher dimensional representation space where the dynamics can be linearized (i.e. represented by a fixed dynamics matrix). This formalism has inspired a line of work in machine learning: predictive auto-encoders learn coordinate systems that approximately linearize a system's dynamics . Auxiliary networks have been introduced to adjust the dynamics matrix to velocity . In contrast, the polar prediction model learns a representation that (implicitly) straightens the temporal evolution in an adaptive representation.

## 6 Discussion

We've introduced a polar prediction model and optimized it to represent visual transformations. Using adaptive phase extrapolation in a fixed shiftable basis, the model jointly discovers and exploits the approximate symmetries in image sequences which are due to local content deformation. The basis is optimized to best diagonalize video dynamics by minimizing mean squared prediction error. The phase relationships are exploited implicitly in a bundled computation, bypassing the instabilities and discontinuities of angular phase variables. By starting from mathematical fundamentals and considering an abstract formulation in terms of learning the representation of transformation groups, we formulated a framework that makes three major contributions. First, it provides a method for discovering the approximate symmetries implicit in sequential data and complements methods for imposing known invariants. Second, it achieves accurate next-frame video prediction within a

Figure 6: **Learned filters. A single stage polar predictor was trained to predict videos from the DAVIS dataset. (a) Filters in each pair are displayed side by side and sorted by their norm. (b) Their amplitude spectra are displayed at corresponding locations. Observe that the filters are selective for orientation and spatial frequency, tile the frequency spectrum, and form quadrature pairs.**

principled framework and provides an interpretable alternative to standard architectures. Third, it offers a framework to understand the nonlinear response properties of neurons in primate visual systems, potentially offering a functional explanation for perceptual straightening.

The polar prediction model makes several strong assumptions. First, it is inertial, assuming that amplitude is unchanged and phase evolves linearly with no acceleration. Second, it separates spatial and temporal processing, which seems at odds with the spatiotemporal selectivities of visual neurons , but could enable downstream image based tasks (e.g. object segmentation, heading direction estimation). Third, it acts independently on each coefficient, although the representation does not perfectly diagonalize the dynamics of natural videos. This is analogous to the situation in modern nonlinear signal processing where diagonal adaptive operators in appropriate bases have had a major impact in compression, denoising, and linear inverse problems . Our empirical results demonstrate that these assumptions provide a reasonable description of image sequences. The standard deep networks considered here could in principle have discovered a similar solution, but they seem not to. This exemplifies a fundamental theme in computational vision and machine learning: when possible, let the representation do the analysis. An important limitation of the framework comes from the use of mean squared error, which is minimized by the posterior mean and tends to result in blurry predictions. Since temporal prediction is inherently uncertain, predictive processing should be probabilistic and exploit prior information.

The polar prediction framework suggests many interesting future directions, both for the study of visual perception (e.g. object constancy at occlusion, and object grouping from common fate) and for the development of engineering applications (e.g. building a flow-free video compression standard). To expand expressivity and better represent signal geometry, it may be possible to design a polar prediction architecture that also adapts the basis, potentially extending to the case of noncommutative transformations (e.g. the two dimensional retinal projection of three dimensional spatial rotation). Finally, it is worth considering the extension of the principles described here to prediction at longer temporal scales, which will likely require learning more abstract representations.