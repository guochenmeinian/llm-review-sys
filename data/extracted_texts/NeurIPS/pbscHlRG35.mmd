# Ying Fang\({}^{1}\)**, **Nian Shao\({}^{1}\)**, **Hui Bu\({}^{2}\)**, **Xin Xu\({}^{2}\)**, **Xiaofei Li\({}^{1,3}\)

RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for Dynamic Speech Enhancement and Localization

 Bing Yang\({}^{1,3}\), Changsheng Quan\({}^{1}\), Yabo Wang\({}^{1}\), Pengyu Wang\({}^{1}\), Yujie Yang\({}^{1}\),\({}^{1}\)School of Engineering, Westlake University

\({}^{2}\)Beijing AIShell Technology Co. Ltd

\({}^{3}\)Institute of Advanced Technology, Westlake Institute for Advanced Study

{yangbing, quanchangsheng, wangyabo, wangpengyu, yangyujie,

fangying, shaonian, lixiaofei}@westlake.edu.cn

buhui@aishelldata.com, xuxinwbg@163.com

Corresponding author.

###### Abstract

The training of deep learning-based multichannel speech enhancement and source localization systems relies heavily on the simulation of room impulse response and multichannel diffuse noise, due to the lack of large-scale real-recorded datasets. However, the acoustic mismatch between simulated and real-world data could degrade the model performance when applying in real-world scenarios. To bridge this simulation-to-real gap, this paper presents a new relatively large-scale **Real**-recorded and annotated **M**icrophone **A**rray speech&**N**oise (**RealMAN**) dataset2. The proposed dataset is valuable in two aspects: 1) benchmarking speech enhancement and localization algorithms in real scenarios; 2) offering a substantial amount of real-world training data for potentially improving the performance of real-world applications. Specifically, a 32-channel array with high-fidelity microphones is used for recording. A loudspeaker is used for playing source speech signals (about 35 hours of Mandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for static speaker and 35.4 hours for moving speaker) are recorded in 32 different scenes, and 144.5 hours of background noise are recorded in 31 different scenes. Both speech and noise recording scenes cover various common indoor, outdoor, semi-outdoor and transportation environments, which enables the training of general-purpose speech enhancement and source localization networks. To obtain the task-specific annotations, speaker location is annotated with an omni-directional fisheye camera by automatically detecting the loudspeaker. The direct-path signal is set as the target clean speech for speech enhancement, which is obtained by filtering the source speech signal with an estimated direct-path propagation filter. Baseline experiments demonstrate that i) compared to using simulated data, the proposed dataset is indeed able to train better speech enhancement and source localization networks; ii) using various sub-arrays of the proposed 32-channel microphone array can successfully train variable-array networks that can be directly used to unseen arrays.

Introduction

Microphone array-based multichannel speech enhancement and source localization are two important front-end audio signal processing tasks [1; 2; 3]. Currently, most existing works are deep learning-based and data-driven, for which the amount and diversity of microphone array data are crucial for the proper training of neural networks. Although an unlimited amount of microphone array data can be generated using simulated room impulse response (RIR) (with the image source method, ISM ) and multichannel diffuse noise , there are still significant mismatches between the acoustic properties of the simulated and real-world data, including **1) The directivity of microphones and sound sources, as well as the wall absorption coefficients.** ISM  assumes both microphones and sound sources to be omni-directional, while real microphones (especially when mounted on specific devices) have a certain directivity and human speakers have a frontal (on-axis) directivity . In addition, ISM normally uses frequency-independent wall absorption coefficients, which violates the case of real walls. In , the authors introduced the frequency-dependent microphone directivity, sound source directivity and wall absorption coefficients into ISM, which largely improves the realness of simulated RIRs. **2) Room geometry and furniture.** ISM normally simulates empty shoebox-shaped rooms, while real rooms could have irregular geometry and built-in furniture. For simulating irregular rooms and built-in furniture, more advanced (but less computationally efficient) geometric acoustic (ray-tracing) methods can be used . One extra difficulty is to configure the room geometry and layout, room and furniture materials to conform to the real-world scene semantics. In , this problem is resolved by leveraging scene computer aided design (CAD) models for room design and natural language processing techniques for material setup, which largely improves the realness of simulated RIRs as well. **3) Piece-wise simulation of moving sound source .** To simulate microphone signals from a moving sound source, the continuous trajectory of the source must be discretized into densely-distributed locations. One signal segment is simulated for each location, and signal segments are then connected. If the sound source moves too fast or the discretization is too sparse, the resulting microphone signals may contain clicking noises and other audible artifacts . **4) The spatial correlation of simulated multi-channel noise** is normally determined under the hypothesis of a theoretical diffuse noise field , while the real-world ambient noise is a superposition of a large number of time-varying (partially) diffuse and directional noise sources and its spatial correlation could largely deviate from the theoretical values. Overall, due to the simulation-to-real mismatch, relevant studies on various tasks have shown that models trained with simulated data often perform poorly in real-world scenarios [7; 10; 11; 12].

Training with real-world data can avoid these mismatches. However, existing real-world microphone array data with annotated target clean speech and source location information is limited and lacks diversity. To address this, we collect a new **Real**-recorded and annotated **M**icrophone **A**rray speech&Noise (**RealMAN**) dataset from a variety of real-world indoor, outdoor, semi-outdoor, and transportation scenes. These recordings encompass diverse spatial/room acoustics and noise characteristics. A loudspeaker is used for playing source speech signal (although the used loudspeaker has a similar frontal on-axis directivity as human speakers , note that one fixed loudspeaker cannot account for the various directivities across different human speakers), and about 35 hours of clean Mandarin speech are used as source speech (more speech materials and for other languages will be considered in future recording if copyright can be issued). The dataset consists of 83.7 hours of speech and 144.5 hours of noise, recorded in 32 and 31 different scenes respectively, with both speech and noise recorded in 17 of these scenes. Both static and moving speech sources are included. We provide annotations of direct-path target clean speech, speech transcription, and source location (azimuth angle, elevation angle and distance relative to the microphone array), for speech enhancement (and evaluation of automatic speech recognition, ASR) and source localization. Where the direct-path target clean speech is obtained by filtering the source speech with estimated direct-path impulse responses, and the source locations are annotated with an omni-directional fisheye camera. A 32-channel microphone array is used for recording. End-to-end speech enhancement and source localization models are normally array-dependent, namely the network trained with one specific array can be only used for the same array. However, collecting real-world data for every new array is cumbersome. One solution is to use data from various arrays to train a variable-array network that can generalize to unseen arrays [13; 14; 15]. Our 32-channel array can provide many different sub-arrays for training such variable-array networks. Baseline experiments have been conducted on the proposed dataset, demonstrating that 1) Compared with using simulated data, training with the proposed real data eliminates the simulation-to-real problem and achieves better performancesin speech enhancement and source localization. Thus, the proposed dataset is more suitable for benchmarking new algorithms and evaluating their actual capabilities; 2) the variable-array networks [13; 15] can be successfully trained with our 32-channel array dataset. Hopefully, these networks can be applied directly to real applications involving unseen arrays.

## 2 Related Work

It is challenging to collect a large-scale real-recorded and annotated microphone array dataset. Table 1 and 2 summarize the existing multi-channel speech and noise datasets, respectively.

**Multi-channel speech recording and annotation.** In MIR , BUTReverb , Reverb , DCASE , ACE  and dEchorate , real-world RIRs are measured instead of directly collecting speech recordings. Measuring real-world RIRs offers several advantages: 1) microphone array speech can be generated by convolving RIRs with source signals, which exhibits sufficient data realness; 2) Direct-path speech can be easily obtained by convolving the direct-path impulse response (extracted from RIRs) with source signals, which can be used as the training target signal for speech enhancement; 3) information for source localization, such as the time-difference of arrival, can also be obtained from the multi-channel direct-path impulse responses. However, one significant drawback of RIR measurement is that it is more time-consuming than speech recording. Consequently, existing datasets such as MIR, BUTReverb, Reverb, ACE, and dEchorate offer only a limited range of measured RIRs and scenes. As for moving source, to obtain the RIR at densely-distributed discrete locations along a moving trajectory, the measurement process becomes even more time-consuming.

Other datasets directly provide multi-channel speech recordings. However, annotating the direct-path speech (as the training target signal) for speech enhancement poses challenges. Although some datasets provide close-talking signals, these cannot serve as the target signals since the direct-path speech is essentially an energy-attenuated and time-shifted version of the close-talking signals. To obtain a clean target signal, the CHiME-3 dataset  simulates the time delay of the direct-path speech for training, and also provides the speech signals recorded in a booth for development and test. In addition to evaluating the speech quality, speech enhancement can also be evaluated in terms of ASR performance. LibriCSS , MC-WSJ-AV , CHiME-5/-6/-7 , AMIMeeting , AISHELL-4  and AliMeeting  (see Appendix for details) provide speech transcriptions for evaluating the speech enhancement performance via ASR. Due to the lack of target signal, the speech enhancement network for these datasets can only be trained with simulated data.

The annotation of sound source location is also not easy. VoiceHome-2 , LOCATA  and STARSS22/23 [31; 32] record speech of real human speakers and provide the speaker location. In VoiceHome-2, speakers can only locate at a small number of pre-defined positions, and their coordinates were measured with a laser telemer. In LOCATA and STARSS22/23, speaker locations are obtained through an optical tracking system. However, the difficulty of setting up the optical tracking system limits the number of recording scenes.

**Multi-channel noise recording.** Noise recordings are typically provided either separately , or together with RIRs [21; 17; 18; 20; 19] or speech recordings . However, most of these datasets [21; 17; 18; 20; 22; 19; 33] are limited in both quantity and diversity. Although the DEMAND dataset  offers noise signals recorded in various scenes with a 16-channel microphone array, but the duration of its recording is quite short.

By comparison, **the proposed RealMAN dataset** has the following advantages. _1) Realness._ Speech and noise are recorded in real environments. Direct recording for moving sources avoids issues associated with the piece-wise generation method. Different individuals move the loudspeaker freely to closely mimic human movements in real applications. _2) Quantity and diversity._ We record both speech signals and noise signals across various scenes. Compared with existing datasets, our collection offers greater diversity in spatial acoustics (in terms of acoustic scenes, source positions and states, etc) and noise types. This enables effective training of speech enhancement and source localization networks. _3) Annotation._ We provide detailed annotations for direct-path speech, speech transcriptions and source location, which are essential for accurate training and evaluation. _4) Number of channels._ The number of microphone channels, i.e. 32, is higher than almost all existing datasets, which facilitates the training of variable-array networks. _5) Relatively low recording cost._ The recording, playback, and camera devices are portable and easily transportable to different scenes.

[MISSING_PAGE_FAIL:4]

annotate the position of the loudspeaker. The LED light can emit red or green light, which is visible for the fisheye camera under various light conditions.

### Source speech signals

Source speech signals that are played by the loudspeaker contains nearly 35 hours of clean Mandarin speech (close-sourced), of which about 30 hours are free-talking and 5 hours are reading. For free talk, speakers are encouraged to converse alone. Reading speech entail speakers reading news articles. The topics of speech content spread a wide range of domains including news reports, games, reading experiences, and life trivia. There are 55 speakers in total, including 27 males and 28 females. 17 speakers are recorded in a studio (T60 is smaller than 150 ms) with a high-fidelity microphone, while the rest are recorded in a living room with a bit larger reverberation time (T60 is about 200 ms) using a lower-fidelity microphone. The distance between speaker and microphone is about 0.2 m.

### Speech and noise recording process

**Speech.** The objective of the speech recording process is to mirror the human activities in real speech applications, such as (multi-party) human-robot interaction, smart loudspeaker, meeting, etc. In each scene, the position of both the camera and microphone array are fixed. When playing source speech, the position of the loudspeaker takes on either static or moving states. For the moving case, one person manually moves the loudspeaker carrier with varying but reasonable moving speed. In transportation scenarios, people typically maintain a stationary position, thereby the loudspeaker only takes the static state. The height of the microphone array is always set to 1.40 m. In each scene, the microphone array is located at one position around the scene center part, and sometimes it is put close (not very close) to one side wall for having a large source-to-array distance or not disturbing other people. The center height of the loudspeaker is aligned with the height of the mouth of a standing person, varying randomly between 1.30 m and 1.60 m. Speech data was recorded across 32 distinct environments, including indoor, outdoor, semi-outdoor, and transportation scenarios. We ensure that most speech recordings were conducted under quiet conditions (usually at midnight). Over all scenes, the sound pressure level (SPL) of speech recordings and silent backgrounds are averagely about 68 dB (61 dBA) and 57 dB (36 dBA), respectively.

In different speech applications, speakers could have very different states in terms of facing or not facing the device, static or moving, small-range pacing or large-range walking, and head turning. For example, as for human-robot interaction and smart loudspeaker, the speaker normally faces the

Figure 1: Recording devices.

device and could have some movements. In a meeting application, the speakers normally face each other and have head turning. During speech recording, we do consider these factors to a certain extent. For static speaker, most of the time, the loudspeaker faces towards the microphone array, with a small portion of side-facing (but no back-facing). For moving speaker, movements include a large portion of large-range walking, a small portion of small-range pacing and a smaller portion of head turning. The moving trajectory of large-range walking can be axial, tangential or in between, and can be random walking as well. During movement, the loudspeaker orientation is put either facing (most of the time side-facing) the microphone array, or towards the moving direction. The source-to-array distances are mainly distributed in the range of 0.5 m \(\) 5 m. Overall, in the proposed dataset, a number of speaker states are considered. However, we think it is still far from exhausting the speaker states in various speech applications, and the influence of speaker states on speech enhancement and source localization would be an interesting topic for future research.

The detailed speech recording information for each scene is given in Appendix B.1, including the recording duration, SPLs, room size and reverberation time. The statistics of source locations and several examples of speaker moving trajectory are given in Appendix B.4.

**Noise.** Noise recording is simpler, for which we place the microphone array in various environments to capture the real-world ambient noise. Noise recording is normally conducted in the daytime with active events in each environment. The collected recording clips with noise power lower than a certain threshold are abandoned. Then, an advanced voice activity detection is conducted to further filter out those recording clips including prominent speech signals. Noise is recorded in 31 different scenarios and ultimately retained 144.5 hours of recordings, covering most everyday scenarios. The SPL of noise recordings is averagely about 71 dB (58 dBA) over all scenes. The duration and SPLs of noise recording for each scene are given in Appendix B.1 as well.

### Data annotation

**Direct-path target clean speech**. Deep learning-based speech enhancement methods require a target clean signal for training. Normally, the direct-path speech signal is used [34; 35]. For real-recorded datasets, before, providing direct-path speech has never been solved in the field.

In this paper, we develop a method to estimate the direct-path speech based on the source speech (replayed speech) and microphone recordings. The recording process is formulated in the time domain as \(x(t)=s_{dp}(t)+s_{rev}(t)+n(t)\), \(s_{dp}(t)=h_{dp}(t)*h_{dev}(t)*s(t)=h_{dev}*[As(t-)]\), where \(x(t)\), \(s_{dp}(t)\), \(s_{rev}(t)\) and \(n(t)\) are the microphone recording, direct-path speech, speech reverberation, and noise, respectively. Theoretically, the direct-path speech is the convolution of the played source speech \(s(t)\), the impulse response of the playing and recording devices \(h_{dev}(t)\), and the direct-path impulse response \(h_{dp}(t)\), where the direct-path impulse response \(h_{dp}(t)\) can be formulated as a level attenuation \(A\) and a time shift \(\) of \(s(t)\). Note that \(A\) and \(\) are time-invariant for static source, while time-varying for moving source.

The impulse response of devices (microphone and loudspeaker combined) \(h_{dev}\) is considered to be constant (independent to the direction and orientation of the loudspeaker relative to the microphone) and it is measured in advance for one configuration where the loudspeaker faces toward the microphone with a source-to-microphone distance of 1 m, please refer to Appendix B.3 for more details. This simplified measurement is accurate for the omni-directional microphone (except for a constant time delay and level factor), but inaccurate for the frontal-directional loudspeaker as its impulse response is orientation-dependent. However, in our setting, it is difficult to annotate the loudspeaker orientation and thus to take the loudspeaker directivity into account, which is left for future research.

Then, the estimation of \(s_{dp}(t)\) amounts to the estimation of \(A\) and \(\) according to the known \(x(t)\), \(h_{dev}\), and \(s(t)\). Note that, any constant time delay and level factor, such as the one caused by the measurement of \(h_{dev}\) or by the loudspeaker orientation, will be absorbed into the estimated \(\) and \(A\), respectively. For details of the estimation algorithm, please refer to Appendix C.1. The direct-path target speech can be estimated in the same way for all microphone channels. To keep a small data size, only the direct-path target speech for microphone 0 is included in the released dataset. In the future, we can provide more if there is a high requirement.

Due to the lack of ground truth, it is not straightforward to evaluate the estimation accuracy. We think the credibility of the estimated direct-path speech can be well testified based on the following criteria. i) The estimated direct-path speech should have the same speech quality as source speech to provide an ideal upper-bound for speech enhancement. We have conducted an informal listening test by eight listeners to test whether there is an audible difference between the perceptual quality of the estimated speech and source speech, which shows that there is no audible difference with a 91% confidence. We have also tested the ASR performance, with an established ASR model trained on over 10,000 hours of Mandarin dataset WenetSpeech , where the results are identical for the estimated direct-path speech and source speech. ii) As shown in Appendix C.1, the estimation of \(A\) and \(\) is based on the cross-correlation method. The good properties of intermediate results, such as the sharp correlation peak and the smooth estimation curve for moving source, also give us strong confidence on the estimation accuracy. iii) After all, as long as speech enhancement networks can be successfully trained with the estimated direct-path speech as the target (will be shown in the experiment section), the estimated direct-path speech can be deemed to be sufficiently accurate.

**Sound source location**. The annotation of source/loudspeaker location leverages a fisheye camera and an LED light (placed on top of the loudspeaker). During the speech recording process, the fisheye camera is placed right above the microphone array, and the plane coordinates of the camera and microphone array are aligned. The height of loudspeaker center (varying from 1.3 m to 1.6 m) is manually logged in the recording process. Given the pixel position of the LED light in the camera image, the source location can be calculated in three steps. i) Based on camera calibration, the azimuth angle (relative to both the array and camera) and elevation angle (relative to only the camera) of source can be calculated with the pixel position. ii) With the height and elevation angle (relative to the camera) of source, the horizontal distance between the source and camera/array can be calculated. iii) The elevation angle (relative to the array) of source can be calculated using the height of microphone array, the height and the horizontal distance of source. Note that, the fixed 12 cm height difference between the LED light and loudspeaker center has been taken into account.

To LED light is detected for each video frame with a vision-based detection algorithm. To guarantee the accuracy, all the detection results are manually double-checked. Please refer to Appendix C.3 for examples and pseudocodes of LED detection. According to the frame rate of the fisheye camera, the temporal resolution of source location annotation is set to 100 ms.

### Dataset split and statistics

The recorded speech and noise data are split into training, validation, and test sets for learning-based speech enhancement and source localization, according to the acoustic characteristics of the recording scenes and speaker identities:

**Acoustic characteristics of scenes**. Different scenes have different RIRs and noise characteristics. To make sure that the model can be trained under diverse scenes, there are 40 different scenes (of speech and noise) included in the training set. Various types of acoustic scenes are also provided in the validation and test sets (17 and 21, respectively), such that the algorithms can be fully evaluated under various scenarios. There are 3 scenes that only appeared in the test and validation sets, respectively, to further evaluate the generalization capability on the unseen acoustic scenes. We think the scene diversity of training and test sets are both critical for the full evaluation of general-purpose speech enhancement and source localization methods, so we have to make some scene overlaps across sets, but note that there is no data sample overlap among them.

**Speaker identities**. Following the general speech corpus split, the entire 55 speakers are split into 43, 6, and 6 for the training, validation, and test sets, respectively, without speaker overlap across sets. Please refer to Appendix B.2 for detailed speech duration statistics across speakers.

**Speech and noise matching**. During recording, speech and noise are normally recorded in quiet midnight and noisy daytime, respectively. To make the noisy speech as real as possible, it is better to mix speech and noise from the same scene. This principle is followed for the validation and test sets.

    & Training & Validation & Test \\  Speech duration (hour) & 64.0 & 8.1 & 11.6 \\ - Moving speaker (hour) & 27.1 & 3.5 & 4.8 \\ - Static speaker (hour) & 36.9 & 4.6 & 6.8 \\ Noise duration (hour) & 106.3 & 16.0 & 22.2 \\ Number of scenes & 40 & 17 & 21 \\ Number of female speakers & 22 & 4 & 2 \\ Number of male speakers & 21 & 2 & 4 \\   

Table 3: Statistics of the training, validation, and test sets of RealMAN.

The number of scenes with both speech and noise recordings is 10 out of 17 for validation scenes and 11 out of 21 for test scenes. The same type of indoor environments, such as living rooms or office rooms, have similar noise characteristics, hence noise is not recorded for every scene. For these cases, we mix the speech of one scene with noise from a similar environment. Specifically, speech of all ClassRooms are mixed with ClassRoom1 noise; speech of all OfficeRooms and Library are mixed with OfficeRoom1/3 noise; speech of all LivingRooms are mixed with LivingRoom1 and Laundry noise. As for training, some preliminary experiments show that such match on speech and noise scenes is not required. Instead, a random scene match between speech and noise is more suitable for network training, possibly because of the promotion of data diversity.

The statistics of the dataset are shown in Table 3. The total 83.7 hours of the recorded speech are divided into 64.0, 8.1, and 11.6 hours for training, validation, and test, respectively. The total 144.5 hours of noise data are divided into 106.3, 16.0 and 22.2 hours for training, validation and test, respectively.

## 4 Baseline Experiments

In this section, we benchmark the proposed dataset for speech enhancement and source localization. As presented in Section 3.5, the validation and test sets are generated by mixing speeches and noises from matched scenes, and the signal level of mixed speech and noise are kept unchanged as recorded to maintain their natural loudness. The speech-noise mixed validation and test sets have an average SNR about 0.0 dB and -0.8 dB, respectively. Please refer to Appendix B.5 for the SNR statistics of validation and test sets. The training set is generated by randomly mixing speeches and noises, with a speech-recording-to-noise-recording ratio (SNR) uniformly distributed in [-10, 15] dB. In Appendix D.3, we also provide the experimental results on the recorded speech without adding noise.

In this paper, we only perform single-speaker speech enhancement (denoising and dereverberation) and source localization. Nevertheless, we think the proposed dataset can also be used for the tasks of multi-speaker separation and localization. Multi-speaker signals can be generated by simply mixing the single-speaker signals recorded in the same scene, which will be highly consistent to the simultaneous recording of multiple speakers. One unreal factor is that the background noise in speech recordings will also be mixed, which we think is not problematic, as the SPL of background noise is low compared to the SPL of speech, i.e. 57 dB (36 dBA) versus 68 dB (61 dBA).

### Baseline methods and evaluation metrics

**Speech enhancement.** One popular time-domain network, i.e. FaSNet-TAC , and one recently-proposed frequency-domain network, i.e. SpatialNet , are used for benchmarking the speech enhancement performance. The negative of scale-invariant signal-to-distortion ratio (SI-SDR)  is used as the loss function for training the two networks. For FaSNet-TAC, the best configuration reported in its original paper is used. For SpatialNet, to reduce the computational complexity, a tiny version is used, where the hidden size of the SpatialNet-small version reported in the paper  is further reduced from 96 to 48. SI-SDR (in dB), WB-PESQ , and MOS-SIG, MOS-BAK, MOS-OVR from DNSMOS  are used for measuring the performance of speech enhancement. The ASR performance are evaluated by the WenetSpeech  ASR model trained by over 10,000 hours of Mandarin dataset, implemented in the ESPNet toolkit. Character error rate (CER, in percentage) is taken as the ASR metric.

**Sound source localization.** Azimuth angle localization is performed. We adopt a convolutional recurrent neural network (CRNN) as one baseline system for sound source localization. The baseline CRNN comprises a 10-layer CNN and a 1-layer gated recurrent unit. The kernel size of convolutional layers are all \(3 3\), each convolutional layer is followed by an instance normalization and a rectified linear unit activation function. Max pooling is applied to compress the frequency and time dimensions after every two convolutional layers. This baseline CRNN is very similar to the CRNN network used in many sound source localization methods . The spatial spectrum, with candidate locations of every \(1^{}\) azimuth angle, is used as the learning target . A linear classifier with sigmoid activation is used to predict the spatial spectrum. A recently proposed sound source localization method, i.e. IPDnet , is also used a baseline system. The hidden size of the original IPDnet is reduced from 256 to 128. Candidate locations are also set as every \(1^{}\) azimuth angle. IPD templates are computed with the theoretical time delays from candidate locations to microphones,following . The training target of ground truth direct-path IPDs are computed with the annotated azimuth angles. The localization results are evaluated with i) the Mean Absolute Error (MAE) and ii) Localization Accuracy (ACC) (\(N^{}\)), namely the ratio of frames with the estimation error of azimuth less than \(N^{}\). Please refer to Appendix D.1 for more detailed experimental configurations.

### Benchmark experiments

Benchmark experiments use a 9-channel sub-array (the smallest circle and the center microphone, and the center microphone is used as the reference microphone when necessary). In addition, we also evaluate the effect of the simulation-to-real mismatch on speech enhancement and source localization tasks. Equal amounts of multichannel speech are simulated according to our real-recorded dataset, using the gpuIR toolkit . Specifically, one counterpart utterance is simulated for each real-recorded utterance using the same source speech, room size, T60, and source position/trajectory as the real-recorded utterance. The directivity of microphone and source are always set to be omnidirectional during simulation. Multi-channel noise is simulated with the diffuse noise generator , taking white (code generated), babble and factory noise (from noisex-92 ) as the single-channel source noise. The simulated speech/noise can also be combined with real-recorded noise/speech. For each setting, the SNR for mixing speech and noise is uniformly sampled in [-10, 15] dB.

**Speech Enhancement.** The results of speech enhancement are shown in Table 4. Overall, compared to other settings, training with real speech and real noise achieves the best speech enhancement performance on the real-recorded test set, for both the baseline networks. As for intrusive metrics, i.e. WB-PESQ and SI-SDR, the target clean speech provided in this dataset are used as the reference signals, which may leads to some measurement bias for other settings, therefore these metrics are only presented for reference. The non-intrusive DNS-MOS scores can better reflect the speech quality. It can be seen that, for FaSNet-TAC, training with simulated speech and real noise achieves comparable DNS-MOS performance as the setting of real speech plus real noise, which indicates that speech simulation does not have the simulation-to-real problem for FaSNet-TAC. However, this is not the case for SpatialNet, for which training with simulated speech and real noise achieves the worst performance. Some validation experiments had been conducted to figure out the reasons for this phenomenon, which showed that there might be a slight mismatch between the real and ideal microphone positions. We have designed a new method for resolving the problem of microphone position mismatch, namely disturbing the ideal microphone positions in training, which is shown to be very effective on simulated test data, but only slightly improve the performance on our real test data. This indicates that there are still some unclear mismatches between the simulated and real data. As its name indicates, SpatialNet  mainly learns the RIR-related spatial information, which is possibly more sensitive to those unclear mismatches. Exploring and resolving those mismatches would be an interesting topic for future research.

Training with real speech and real noise consistently outperforms training with simulated noise. The simulated noise lacks diversity in terms of both spectral pattern and spatial correlation. The spatial correlation of real ambient noise could largely deviates from the one of theoretical diffuse noise field. Moreover, the spatial correlation of real noise is also highly time-varying. Please see Appendix D.2 for the detailed analysis of spatial correlation of real noise. The simulation-to-real mismatch of noise leads to the performance degradation. In addition, due to the high complexity and non-stationarity of the spatial correlation of real noise, it is complicated to develop new techniques for simulating real noise. Therefore, we suggest to use real-recorded multi-channel noise for training speech enhancement networks.

    &  &  &  \\   & speech & noise & WB-PESQ & SI-SDR & MOS-SIG & MOS-BAR & MOS-OVR & CER & WB-PESQ & SI-SDR & MOS-SIG & MOS-BA & MOS-OVR & CER \\   & - & 1.14 & -9.9 & 2.01 & 1.73 & 1.52 & 20.1 & 1.10 & -9.0 & 1.80 & 1.54 & 1.37 & 23.7 \\   & sim & sim & 1.39 & -2.4 & 2.71 & 3.00 & 2.18 & 25.4 & 1.34 & -2.5 & 2.62 & 2.92 & 2.10 & 28.2 \\  & semi & real & **1.47** & -1.0 & **2.83** & 3.24 & **2.36** & **20.5** & **1.41** & -0.8 & **2.75** & 3.12 & **2.27** & **23.4** \\  & real & sim & 1.42 & 0.4 & 2.63 & 3.12 & 2.19 & 24.0 & 1.36 & 0.4 & 2.51 & 3.15 & 2.08 & 28.2 \\  & real & real & 1.46 & **2.1** & 2.79 & **3.20** & 2.34 & 21.9 & 1.39 & **1.2** & 2.73 & **3.20** & 2.26 & 25.6 \\   & sim & sim & 1.37 & -5.2 & **3.24** & 2.85 & 2.46 & 17.9 & 1.37 & -4.8 & **3.20** & 2.75 & 2.40 & 21.0 \\  & real & sim & 1.33 & -9.7 & 1.59 & 1.67 & 1.32 & 44.2 & 1.11 & -9.2 & 1.51 & 1.57 & 1.26 & 89.0 \\   & real & sim & 1.96 & 4.7 & 3.16 & 3.13 & 2.52 & 17.3 & 1.80 & 2.7 & 3.08 & 1.05 & 2.42 & 21.2 \\   & real & real & **2.16** & **7.3** & 3.23 & **3.43** & **2.71** & **14.5** & **1.97** & **4.3** & 3.15 & **3.40** & **2.63** & **18.6** \\   

Table 4: Benchmark experiments of speech enhancement.

Overall, the proposed dataset is a difficult one for speech enhancement, due to the large scene diversity, the high realness, and the complex acoustic conditions. The CERs of unprocessed recordings are quite high, i.e. close to or larger than 20%, even though a very strong ASR model (trained with over 10,000 hours data) is used.

**Sound source localization.** The results of the sound source localization are presented in Table 5. The mismatch between simulated RIRs and real recordings causes the performance degradation of sound source localization, which is consistent with the findings in [12; 48; 49]. The simulation-to-real mismatch of noise causes the performance degradation of sound source localization as well.

### Variable-array networks and array generalization

End-to-end speech enhancement and source localization networks are normally array-dependent, which means although the fixed-array networks (presented in the previous section) trained using real speech and real noise achieve better performance for one array, they still cannot be used for other arrays. In this section, we use all the 28-microphone data (microphone 0 \(\) 27) on the horizontal plane to train the variable-array networks, i.e. FaSNet-TAC  for speech enhancement and IPDnet  for source localization, to see whether the trained networks can be directly used to unseen arrays.

We set one test array, namely a 5-channel uniformly-spaced linear array (microphone 11, 3, 0, 7, and 12). The training of variable-array networks uses randomly selected 2 \(\) 8-channel sub-arrays, excluding all 5-channel uniformly-spaced linear arrays. The microphone 0 is always used and taken as the reference channel.

Table 7 and Table 6 present the results of speech enhancement and sound source localization, respectively. It can be seen that there are indeed certain performance losses when compared with the fixed-array networks that are trained using the test array, but the losses are relatively small. This shows that the 32-channel real-recorded microphone array data provided in the proposed dataset can successfully train the variable-array networks, which offers a competitive solution for real-world multi-channel speech enhancement and sound source localization.

## 5 Conclusion

This paper presents a new real-recorded and annotated microphone array speech and noise dataset, named **RealMAN**, for speech enhancement and source localization. Baseline experiments demonstrate that training with our real-recorded data outperforms training with simulated data, by eliminating the simulation-to-real gap. The performance on our dataset can better reflect the capabilities of tested algorithms in real-world applications, providing a more reliable benchmark for speech enhancement and source localization. Additionally, variable-array networks can be successfully trained using various sub-arrays of the proposed 32-channel microphone array, and they have the potential to be applied directly to unseen arrays in real-world applications.

    &  &  \\   & ^{}\)) [\%] MAE [\({}^{}\)] ACC(5\({}^{}\)) [\%] MAE [\({}^{}\)]} & ^{}\)) [\%] MAE [\({}^{}\)]} \\  Fixed-Array & 87.0 & 3.0 & **85.4** & **3.1** \\ Variable-Array & **87.1** & **2.4** & 78.8 & 3.7 \\   

Table 6: IPDnet variable-array experiments for sound source localization.

    &  &  \\   & WB-PESQ & SI-SDR & MOS-SIG & MOS-BAK & MOS-OV & CER & WB-PESQ & SI-SDR & MOS-SIG & MOS-BAK & MOS-OV & CER \\  unprocessed & 1.14 & -9.9 & 2.01 & 1.73 & 1.52 & 20.1 & 1.10 & -9.0 & 1.80 & 1.54 & 1.37 & 23.7 \\  Fixed-Array & 1.41 & **1.5** & **2.76** & 3.25 & **2.28** & **25.5** & **1.36** & **0.5** & **2.72** & 3.18 & **2.23** & **30.0** \\ Variable-Array & **1.42** & 1.0 & 2.70 & **3.33** & **2.28** & 27.4 & **1.36** & 0.3 & 2.64 & **3.27** & 2.20 & 31.4 \\   

Table 7: FaSNet-TAC variable-array experiments for speech enhancement.

   Training Data & Static Speaker & Moving Speaker \\  speech & noise & ACC(5\({}^{}\)) [\%] MAE [\({}^{}\)] ACC(5\({}^{}\)) [\%] MAE [\({}^{}\)] \\  sim & sim & 60.4 & 7.8 & 58.8 & 9.6 \\ sim & real & 52.4 & 22.9 & 48.4 & 21.4 \\ real & sim & 80.0 & 5.7 & 75.8 & 8.4 \\ real & real & **89.2** & **2.2** & **86.7** & **3.1** \\   

Table 5: CRNN benchmark experiments of sound source localization.