# Context-PIPs: Persistent Independent Particles

Demands Spatial Context Features

 Weikang Bian\({}^{1,2}\)1 Zhaoyang Huang\({}^{1}\)1 Xiaoyu Shi\({}^{1}\)

**Yitong Dong\({}^{3}\) Yijin Li\({}^{3}\) Hongsheng Li\({}^{1,2}\)**

\({}^{1}\)CUHK MMLab \({}^{2}\)Centre for Perceptual and Interactive Intelligence \({}^{3}\)Zhejiang University

wkbian@outlook.com, drinkingcoder@link.cuhk.edu.hk, hsli@ee.cuhk.edu.hk

###### Abstract

We tackle the problem of Persistent Independent Particles (PIPs), also called Tracking Any Point (TAP), in videos, which specifically aims at estimating persistent long-term trajectories of query points in videos. Previous methods attempted to estimate these trajectories independently to incorporate longer image sequences, therefore, ignoring the potential benefits of incorporating spatial context features. We argue that independent video point tracking also demands spatial context features. To this end, we propose a novel framework Context-PIPs, which effectively improves point trajectory accuracy by aggregating spatial context features in videos. Context-PIPs contains two main modules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature Aggregation (TAFA) module. Context-PIPs significantly improves PIPs all-sided, reducing 11.4% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD and increasing 11.8% Average Percentage of Correct Keypoint (A-PCK) on TAP-Vid-Kinetics. Demos are available at https://wkbian.github.io/Projects/Context-PIPs/.

## 1 Introduction

Video particles are a set of sparse point trajectories in a video that originate from the first frame (the source image) and move across the following frames, which are regarded as the target images. In contrast to optical flow estimation that computes pixel-wise correspondences between a pair of adjacent video frames, Persistent Independent Particles (PIPs)  or Tracking Any Point (TAP)  is interested in tracking the points in the follow-up frames that correspond to the original query points even when they are occluded in some frames. Video particles provide long-term motion information for videos and can support various downstream tasks, such as video editing  and Structure-from-Motion .

Long-range temporal information is essential for video particles especially when the particles are occluded because the positions of the occluded particles can be inferred from the previous and subsequent frames where they are visible. However, simultaneously encoding long image sequences brings larger computational and memory costs. Previous methods [12; 5] learn to track individual points independently because dense video particles are unnecessary in most scenarios. Inspired by optical flow estimation from visual similarities, they learn to predict point trajectories from the similarities between the query point and the subsequent target images. Specifically, given a query point at the source image, PIPs encodes \(T\) feature maps from \(T\) consecutive target images and builds a \(T H W\) correlation volume by computing the feature similarity between the feature of the query point and the feature maps. The \(T\) particle positions are iteratively refined with the 3D correlation volume through a shared MLP-Mixer . In other words, PIPs trades the spatial context features ofthe particle for longer temporal feature encoding. PIPs achieves great performance on the DAVIS dataset, which contains large movement particles and weak texture images (e.g., fast-moving dogs and black bears).

We argue that independent point tracking still demands spatial context features. Intuitively, although PIPs only accounts for specified query points, spatial context features around them provide informative cues for point trajectory refinement. For example, video particles on the same objects always share similar motions over time. In some video frames where the target particles are occluded, their surrounding particles may be visible and provide guidance for the position estimation of the target particles. However, PIPs only takes correlations and features belonging to the particles while ignoring abundant spatial context features around them. In this work, we propose tracking particles with Context (Context-PIPs) to improve independent point tracking with spatial context features. Context-PIPs contains two key modules for better point trajectory refinement: 1) a source feature enhancement (SOFE) module that learns to adopt more spatial context features in the source image and builds a guidance correlation volume, and 2) a target feature aggregation (TAFA) module that aggregates spatial context features in the target image guided by the correlation information.

In the source image, points that possess similar appearances are supposed to move in similar trajectories in subsequent frames. Such an assumption has also been used in GMA  for optical flow estimation. Given a query point, SOFE computes the correlation between the query point and the source image feature map, which is its self-similarity map. With the guidance of the correlation (self-similarity), SOFE predicts \(M\) offsets centered from the query point, and samples at the corresponding \(M\) auxiliary points to collect source context features. During the iterative point trajectory refinement, the correlation information between the \(M\) auxiliary features and \(T\) target feature maps is injected into the MLP-Mixer, which provides strong guidance and shows evident performance improvement.

Existing methods for optical flow and video particle estimation ignore features in target images for iterative refinement. To better utilize the context of target images, in each iteration, our TAFA module collects target features surrounding the previous iteration's resulting movements. TAFA for the first time shows that context features in target images also benefit correspondence estimation and further improve the point tracking accuracy.

Our contributions can be summarized as threefold: 1) We propose a novel framework, Context-PIPs, to improve independent video particle tracking with context features from both source and target features. Context-PIPs ranks 1st on the four benchmarks and shows clear performance superiority. 2) We design a novel SOurce Feature Enhancement (SOFE) module that builds a guidance correlation volume with spatial context features in the source image, and 3) a novel TAget Feature Aggregation (TAFA) module that extracts context features from target images.

## 2 Related Work

**Optical Flow.** Optical flow estimates the dense displacement field between image pairs and has traditionally been modeled as an optimization problem that maximizes the visual similarity between image pairs with regularizations [13; 1; 2; 34]. It serves as the core module of many downstream applications, such as Simultaneously Localization And Mapping (SLAM) [8; 47; 27; 43; 9], video synthesis [46; 14; 15; 42; 41], video restoration [24; 23], etc. Since FlowNet [6; 20], learning optical flow with neural networks presents superior performance over traditional optimization-based methods and is fast progressing with more training data obtained by the renderer and better network architecture [6; 20; 28; 35; 36; 18; 19; 45]. In recent years, iterative refining flow with all-pairs correlation volume presents the best performance. The most successful network designs are RAFT  and FlowFormer [17; 32], which achieves state-of-the-art accuracy.

Typical optical flow estimation only takes image pairs but longer image sequences can provide more information that benefits optical flow estimation. Kalman filter [4; 7] had been adopted in dealing with the temporal dynamics of motion and estimating multi-frame optical flow. Recent learning-based methods also attempted to exploit multi-frame information and perform multi-frame optical flow estimation. PWC-Fusion  is the first method that learns to estimate optical flow from multiple images. However, it only fuses information from previous frames in U-Net and improves little performance. The "warm start" technique [39; 33; 37] that wrapped the previous flow to initialize the next flow is firstly proposed in RAFT and shows clear accuracy increasing. VideoFlow  takes multi-frame cues better, iteratively fusing multi-frame information in a three-frame and five frame structure, which reveals that longer temporal information benefits pixel tracking. Recently, researchers [25; 10] also tried to learn to estimate optical flow with event cameras.

**Tracking Any Point.** Optical flow methods merely focus on tracking points between image pairs but ignore tracking points across multiple consecutive frames, which is still challenging. Harley _et al._ studied pixel tracking in the video as a long-range motion estimation problem inspired by Particle Video . They propose a new dataset FlyingThings++ based on FlyingThings  for training and Persistent Independent Particles (PIPs) to learn to track single points in consecutive frames with fixed lengths. Doersch _et al._ is a parallel work, which formalized the problem as tracking any point(TAP). They also propose a new dataset Kubric  for training and a network TAP-Net to learn point tracking. Moreover, they provide the real video benchmarks that are labeled by humans, TAP-Vid-DAVIS  and TAP-Vid-Kinetics , for evaluation. PIPs and TAP solve the video particle tracking problem in a similar manner, i.e., recurrently refining multi-frame point trajectory via correlation maps. In this paper, our Context-PIPs follows the training paradigm of PIPs and improves the network architecture design of PIPs. We also take the TAP-Vid-DAVIS and TAP-Vid-Kinetics benchmarks from TAP-Net for evaluation.

## 3 Method

In contrast to optical flow methods [39; 17] that track dense pixel movement between an image pair, the problem of point tracking takes \(T\) consecutive RGB images with a single query point \(_{src}^{2}\) at the first frame as input, and estimates \(T\) coordinates \(=\{_{0},_{1},,_{T-1}\}\) at the video frames where every \(_{t}\) indicates the point's corresponding location at time \(t\). Persistent Independent Particles (PIPs)  is the state-of-the-art network architecture for TAP. It iteratively refines the point trajectory by encoding correlation information that measures visual similarities between the query point and the \(T\) video frames. The query points to be tracked are easily lost when the network only looks at them and ignores spatial context features. We propose a novel framework Context-PIPs (Fig. 1) that improves PIPs with a SOurce Feature Enhancement (SOFE) module and a TArget Feature Aggregation (TAFA) module. In this section, we first briefly review PIPs and then elaborate on our Context-PIPs.

### A Brief Revisit of PIPs

PIPs  processes \(T\) video frames containing \(N\) independent query points simultaneously and then extends the point trajectories to more video frames via chaining rules . Given a source frame with a query point \(_{src}^{2}\) and \(T-1\) follow-up target video frames, PIPs first extracts their feature maps \(_{0},_{1},,_{T-1}^{C H  W}\) through a shallow convolutional neural network and bilinearly samples to obtain the source point feature \(_{src}=_{0}(_{src})\) from the first feature map at the query point \(_{src}\). \(C,H,W\) are the feature map channels, height, and width. Inspired by RAFT , PIPs initializes the point trajectory and point visual features at each frame with the same \(_{src}\) and \(_{src}\):

\[^{0} =\{_{0}^{0},_{1}^{0},,_{T-1} ^{0}|_{t}^{0}=_{src},t=0,,t=T-1\},\] (1) \[^{0} =\{_{0}^{0},_{1}^{0},,_{T-1} ^{0}|_{t}^{0}=_{src},t=0,,t=T-1\},\]

and iteratively refines them via correlation information. \(_{t}^{k}\) and \(_{t}^{k}\) respectively denote the point trajectory and point features in the \(t\)-th frame and \(k\)-th iteration. Intuitively, the point features store the visual feature at the currently estimated query point location in all the \(T\) frames.

Specifically, in each iteration \(k\), PIPs constructs multi-scale correlation maps  between the guidance feature \(\{_{t}^{k}\}_{t=0}^{T-1}\) and the target feature maps \(\{_{t}^{k}\}_{t=0}^{T-1}\), which constitutes \(T\) correlation maps \(^{k}=\{_{0}^{k},_{1}^{k},,_{T-1} ^{k}\}\) of size \(T H W\), and crops correlation information inside the windows centered at the point trajectory: \(^{k}(^{k})=\{_{0}^{k}(_{0}^{k}), _{1}^{k}(_{1}^{k}),,_{T-1}^{k}(_ {T-1}^{k})\}\), where \(_{t}^{k}(_{t}^{k})^{D D}\) denotes that we crop \(D D\) correlations from \(_{t}^{k}\) inside the window centered at \(_{t}^{k}\). The point features \(^{k}\), point locations \(^{k}\), and the local correlation information \(^{k}(^{k})\) are fed into a standard 12-layer MLP-Mixer that produces \(\) and \(\) to update the point feature and the point trajectory:

\[,\; =(^{k},^{k}(^{k}), (^{k}-_{src})),\] (2) \[^{k+1} =^{k}+,\;^{k+1}=^{k} +.\]PIPs iterates \(K\) times for updates and the point trajectory in the last iteration \(^{K}\) is the output.

PIPs achieves state-of-the-art accuracy on point tracking by utilizing longer temporal features. However, the previous method ignores informative spatial context features which are beneficial to achieve more accurate point tracking. Context-PIPs keeps all modules in PIPs and is specifically designed to enhance the correlation information \(^{k}\) and point features \(^{k}\) as \(}^{k}\) and \(}^{k}\) with the proposed SOFE and TAFA.

### Source Feature Enhancement

Given the query point \(_{src}\) and feature map \(_{0}\) of the source image, PIPs simply samples a source feature \(_{src}\) at the query point location to obtain the point visual features \(^{k}\). Although the point features are updated via the iterative refinement, its perceptive field is limited to the single point, easily compromised in harsh scenarios. The correlation maps \(^{k}\) in the \(k\)-th iteration provide vague information when the query point is in a less textured area. Moreover, the correlation map \(^{k}_{t}\) at timestamp \(k\) is ineffective once the particle is occluded at the \(t\)-th frame. To enhance the source feature, as shown in Fig. 1, we propose SOurce Feature Enhancement (SOFE) that accepts spatial context features in the source image as auxiliary features to guide the point trajectory refinement. The MLP-Mixer can infer the point locations via the auxiliary features even when the points are occluded or on less textured regions, which improves the point tracking accuracy and robustness.

Directly adopting all features in the source image brings large computational costs. SOFE learns to sample a small number of auxiliary features to enhance the source feature. Specifically, SOFE improves the point features in three steps. Firstly, SOFE learns to predict \(M\) offsets \(_{0},_{1},,_{M-1} ^{2}\) with an MLP-based sampler to sample \(M\) auxiliary features \(G=\{_{0},_{1},,_{M-1}|_{m}= _{0}(_{src}+_{m})\}\) around the query point \(_{src}\) in the source image. Motivated by GMA that aggregates pixel flows from pixels that are likely to belong to the same object through self-similarity, our proposed sampler also learns the locations of the auxiliary features based on local self-similarities \(^{0}_{0}(_{src})\) which store the correlations cropped from the first frame at the query point location. Secondly, we construct the correlation map \(^{}_{m,t}=<_{m},_{t}>^{H W}\) that measure visual similarities of the \(m\)-th auxiliary feature and the \(t\)-th frame feature map. \(}_{m,t}\) provides additional correlation information to guide the iterative point trajectory refinement. In each iteration \(k\), we crop the additional correlation information \(^{}_{m}(^{k}_{t})\) according to the point locations

Figure 1: Overview of Context-PIPs Pipeline. Our Context-PIPs improves PIPs  with SOurce Feature Enhancement (SOFE) and TAFget Feature Aggregation (TAFA). PIPs iteratively refines the point trajectory \(^{k}\) with an MLP-Mixer with the current point trajectory \(^{k}\), the correlation features \(^{k}\), and the point features \(^{k}\). SOFE and TAFA respectively improves the correlation features and point features, denoted as \(}^{k}\) and \(}^{k}\).

\(_{t}^{k}\) and concatenate them with the original point correlation information \(_{t}^{k}(_{t}^{k})\), where \(_{m}^{}(_{t}^{k})\) denotes the same cropping operation as \(_{t}^{k}(_{t}^{k})\). Finally, for each frame \(t\), we reduce the augmented correlations to a correlation feature vector \(}_{t}\) of length 196 through a correlation encoder CorrEnc.

\[}_{t}^{k}=((_{0}^{ }(_{t}^{k}),_{1}^{}(_{t}^{k}),, _{M-1}^{}(_{t}^{k}),_{t}^{k}(_{t }^{k}))),\] (3)

and inject \(}^{k}=\{}_{0}^{k},}_{1}^{k}, ,}_{T-1}^{k},\}\) into the MLP-Mixer. Compared with PIPs that only adopts \(_{t}^{k}(_{t}^{k}))\), SOFE provides more informative correlations to the MLP-Mixer with spatial context features but does not increase its parameters and computations. The additional auxiliary features from the self-similarity map of the source image enhance the original source point features and significantly improves tracking accuracy.

### Target Feature Aggregation

Inspired by existing optical flow methods, PIPs iteratively refines the point trajectory with correlation information and context features and also iteratively updates the point visual feature \(^{k+1}=^{k}+\) after initializing them with the source feature, which presents benefits for point trajectory refinement. We observe that the input for supporting the point feature updating comes from the correlations \(^{k}\) only. However, such correlations \(^{k}\) are calculated as only cosine distances between the source point visual feature \(^{k}\) and the target features around currently estimated point locations \(^{k}\), which provide limited information on how to conduct visual feature updating. Can we better guide the point feature update with context features in target images?

We, therefore, propose TArget Feature Aggregation (TAFA) to augment point features with target image features nearby the point trajectory. Specifically, for each target frame \(t\), a patch of shape \(D D\) cropped from the corresponding target feature map \(_{t}\) centered at \(_{t}^{k}\) to generate key and value. The augmented correlation features \(}\) in Eq. 3 encode abundant visual similarities. Therefore, we generate a query from it to extract target context features and adopt cross-attention with relative positional encoding to obtain the target context feature \(^{}_{t}^{k}\), which is added to the original source point feature \(}_{t}^{k}=_{t}^{k}+^{}_{t}^{k}\). Finally, such augmented point features \(}^{k}=\{}_{0}^{k},}_{1}^{k}, ,}_{T-1}^{k}\}\) are injected into the MLP-Mixer. Similar to our proposed SOFE, TAFA also keeps the same parameters and computations of MLP-Mixer as PIPs while providing additional target context features and further improving PIPs. Although context features in the source image are used since RAFT , no previous methods adopt context features from target images. TAFA for the first time validates that target images also contain critical context features that benefit point movement refinement. SOFE improves PIPs with auxiliary features in the source image while TAFA absorbs more target image features. Equipping SOFE and TAFA to PIPs constitutes our final model, Context-PIPs.

### Loss Functions

We compute the L1 distance between \(^{k}\) computed in iteration \(k\) and the ground truth \(_{gt}\) and constrain with exponentially increasing weights \(=0.8\).

\[_{TAP}=_{k=1}^{K}^{K-k}||^{k}-_{gt}||_ {1}\] (4)

In addition, we will predict the visibility/occlusion \(\) by a linear layer according to the \(}^{K}\) obtained by the iterative update. And the cross entropy loss is used to supervise \(\) with the ground truth \(_{gt}\).

\[_{Vis}=_{gt}+(1-_{gt})(1- ).\] (5)

The final loss is the weighted sum of the two losses:

\[_{total}=w_{1}_{TAP}+w_{2}_{Vis}.\] (6)

We use \(w_{1}=1\) and \(w_{2}=10\) during training.

[MISSING_PAGE_FAIL:6]

achieves 7.06 ATE-Occ and 4.28 ATE-Vis on the CroHD dataset, 11.4% and 9.5% error reductions from PIPs, the runner-up. On the FlyingThings++ dataset, our Context-PIPs decreases the ATE-Vis and ATE-Occ by 0.96 and 2.18, respectively.

**TAP-Vid-DAVIS and TAP-Vid-Kinetics (first)** A-PCK, the average percentage of correct key points, is the core metric. Context-PIPs ranks 1st in terms of A-PCK on both benchmarks. Specifically, Context-PIPs outperforms TAP-Net by 24.1% on the TAP-Vid-DAVIS benchmark and improves PIPs by 11.8% on the TAP-Vid-Kinetics benchmarks. Context-PIPs also achieves state-of-the-art occupancy accuracy on both datasets.

**TAP-Vid-DAVIS and TAP-Vid-Kinetics (strided)** Tab. 2 compares methods in the "strided" sampling setting. Our Context-PIPs also achieves the best performance on both AJ and A-PCK metrics for the two datasets. Context-PIPs effectively improves PIPs' occupancy accuracy and presents the best performance on the TAP-Vid-Davis dataset.

### Qualitative Comparison

We visualize the trajectories estimated by TAP-Net, PIPs, and our Context-PIPs respectively in Fig 2 to qualitatively demonstrate the superior performance of our method. By incorporating additional spatial context features for point tracking, Context-PIPs surpasses the compared methods in accuracy and robustness. Specifically, the first row shows the case of large-scale variation. The trajectory predicted by TAP-Net deviates considerably from the ground truth. TAP-Net also outputs jitter predictions when the query pixel is on the texture-less area as shown in the second row. Our Context-PIPs generates more accurate results than PIPs in these two hard cases. Furthermore, as depicted in the third row, PIPs struggles to distinguish the front wheel and the rear wheel due to the changing lighting conditions. However, our Context-PIPs achieves consistent tracking, thanks to the rich context information brought by the SOFE and TAFA modules.

### Efficiency Analysis

We train our Context-PIPs and PIPs with different MLP-Mixer depths, i.e., the number of layers in the MLP-Mixer, to show the prominent efficiency and effectiveness of our proposed Context-PIPs. Context-PIPs improves PIPs with SOFE and TAFA, which introduce minor additional parameters and time costs. We show that the accuracy benefits do not come from the increased parameters trivially. As displayed in Tab. 4, we increase the MLP-Mixer depth to 16, which significantly increases the parameters but does not bring performance gain. We also decrease the MLP-Mixer depth in our Context-PIPs. Even with only a 3-layer MLP-Mixer, Context-PIPs achieves better performance than the best PIPs (MLP-Mixer depth=12). Context-PIPs outperforms PIPs with only 40.2% parameters. Moreover, evaluated by the pytorch-OpCounter , PIPs consumes 287.5G FLOPS while Context

    &  &  \\   & AJ & A-PCK & AJ & A-PCK \\  RAFT & 30.0 & 46.3 & 34.5 & 52.5 \\ Kubric-VFS-Like & 33.1 & 48.5 & 40.5 & 59.0 \\ COTR & 35.4 & 51.3 & 19.0 & 38.8 \\ TAP-Net & 38.4 & 53.1 & 46.6 & 60.9 \\ PIPs(Re-imp.) & 45.2 & 59.8 & 42.9 & 58.3 \\ Context-PIPs (Ours) & **48.9** & **64.0** & **49.8** & **64.3** \\   

Table 2: Experiments on TAP-Vid-DAVIS(strided) and TAP-Vid-Kinetics(strided).

    &  &  \\   & TAP-DAVIS & TAP-Kinetics & TAP-DAVIS & TAP-Kinetics \\  TAP-Net & 78.8 & 80.6 & 82.3 & 85.0 \\ PIPs (Re-imp.) & 79.3 & 77.0 & 82.9 & 81.5 \\ Context-PIPs (Ours) & 79.5 & 79.8 & 83.4 & 83.3 \\   

Table 3: Occlusion Accuracy on TAP-Vid-DAVIS and TAP-Vid-Kinetics.

PIPs only consumes 216.4G FLOPS, saving 24.7% computation resources. These numbers reveal the high memory and computation efficiency of our proposed Context-PIPs.

### Ablation Study on Modules

We conduct a module ablation study on the proposed modules as presented in Tab. 5. The errors of Context-PIPs consistently decrease when we sequentially add the SOFE and TAFA modules, which reveals the effectiveness of SOFE and TAFA. To demonstrate the necessity of the cross-attention mechanism used in TAFA, we attempt to predict a matrix of weights corresponding to the feature map shaped with \(r_{a}\) and directly weigh and sum the features to get \( F\). Cross-attention performs better than prediction.

### Ablation Study on Parameters

We conduct a series of ablation experiments (Tab. 6) to demonstrate the significance of each module and explain the rationality of the settings. All ablation experiments are trained on Flyingthings++. Starting from the PIPs baseline, we first add the SOFE module and explore the two related hyper-parameters, i.e., the correlation radius \(r_{c}\) and the number of samples \(M\). Then, we further add the TAFA module and also adjust the attention window radius \(r_{a}\). We additionally conduct a comparison between the prediction and attention mechanisms in TAFA. In the below experiments, we set \(N=64\), learning rate as \(3 10^{-4}\), and train for \(20,000\) steps. Below we describe the details.

**Correlation Radius in SOFE** We crop a multi-scale correlation of size \((2r_{c}+1)(2r_{c}+1)\) from the first correlation map to predict the auxiliary feature offsets in SOFE. The correlation radius

    & MLP-Mixer &  &  &  \\   & & & vis & occ & vis & occ \\   & 6 & 16.06 & 8.00 & 25.30 & 5.04 & 8.15 \\  & 12 & 28.67 & 7.70 & 24.70 & 4.98 & 8.20 \\  & 16 & 37.09 & 7.69 & 24.71 & 4.84 & 8.07 \\   & 3 & 11.54 & 7.37 & 24.19 & 4.54 & 7.79 \\  & 6 & 17.84 & 6.94 & 22.56 & 4.37 & 7.05 \\   & 12 & 30.46 & 6.60 & 22.11 & 4.30 & 6.73 \\   

Table 4: Efficiency analysis for PIPs and Context-PIPs with different MLP-Mixer depth.

Figure 2: Qualitative comparison. In the leftmost column, the green crosses mark the query points and the image is the starting frame. The right three columns show the results of TAP-Net, PIPs, and Context-PIPs. The red and green lines illustrate the predicted and ground truth trajectories.

determines the cropping patch size. We fix \(M=3\), and gradually increase \(r_{c}\) from 1 to 4. The model achieves the best performance when \(r_{c}=2\).

**Number of Samples in SOFE** SOFE learns to sample \(M\) additional auxiliary features to enhance the source feature. Given \(r_{c}=2\), we continued to experiment with the different number of samples \(M\). The model achieves the best performance on both Flyingthings++ and CroHD datasets when \(M=9\).

**Attention Radius in TAFA** TAFA aggregates target features surrounding the currently estimated corresponding point locations to enhance the context feature via cross-attention. The radius of the attention window \(r_{a}\) determines how far the attention can reach. We gradually increase \(r_{a}\) from 1 up to 5, and find that \(r_{a}=3\) performs best.

## 5 Conclusion

We have presented a novel framework Context-PIPs that improves PIPs with spatial context features, including a SOtree Feature Enhancement (SOFE) module and a TAFget Feature Aggregation (TAFA) module. Experiments show that Context-PIPs achieves the best tracking accuracy on four benchmark datasets with significant superiority. This technology has broad applications in video editing and 3D reconstruction and other fields. **Limitations**. Following PIPs, Context-PIPs tracks points in videos with a sliding window. The target point cannot be re-identified when the point is lost. In our future work, we will explore how to re-identify the lost points when the points are visible again.

**Acknowlegement** This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK.

    &  &  &  &  \\   & & \(M\) & \(r_{c}\) & \(r_{a}\) & vis & occ & vis & occ \\  PIPs & Baseline & - & - & - & 14.42 & 37.33 & 6.14 & 9.97 \\   & \))} & 3 & 1 & - & 13.60 & 36.17 & 6.40 & 10.30 \\  & & 3 & 2 & - & **13.02** & **35.60** & 6.48 & **9.69** \\  & & 3 & 3 & - & 13.07 & 35.74 & **6.23** & 10.09 \\  & & 3 & 4 & - & 13.75 & 37.01 & 6.64 & 10.20 \\   &  & 1 & 2 & - & 15.00 & 37.51 & 6.94 & 10.25 \\  & & 3 & 2 & - & 13.02 & 35.60 & 6.48 & 9.69 \\  & & 6 & 2 & - & 13.21 & 35.39 & 6.58 & 9.77 \\  & & 9 & 2 & - & **12.18** & **34.23** & **5.71** & **9.18** \\  & & 12 & 2 & - & 12.87 & 35.27 & 6.20 & 10.17 \\   & \))} & 9 & 2 & 1 & 11.98 & 34.10 & 5.90 & 9.52 \\  & & 9 & 2 & 2 & 11.82 & 33.82 & 5.64 & 9.28 \\   & & 9 & 2 & 3 & **11.67** & **33.38** & **5.53** & 9.19 \\   & & 9 & 2 & 4 & 11.81 & 33.88 & 5.65 & 9.23 \\   & & 9 & 2 & 5 & 11.83 & 33.76 & 5.60 & **9.15** \\   

Table 6: Ablation study. We add one component at a time on top of the baseline to obtain our Context-PIPs. \(r_{c}\), \(M\), and \(r_{a}\) respectively denote the correlation radius and the number of predicted samples in SOFE, and the attention window radius in TAFA. Our final model uses \(M=9,r_{c}=2,r_{a}=3\) to achieve the best performance.

    &  &  &  \\   & & vis & occ & vis & occ \\  PIPs & Baseline & 7.40 & 24.4 & 4.73 & 7.97 \\   & +SOFE & 6.91 & 22.64 & 4.36 & 7.15 \\  & +SOFE+TAFA (attention) & 6.60 & 22.11 & 4.30 & 6.73 \\   & +SOFE+TAFA (prediction) & 7.15 & 23.33 & 4.34 & 7.27 \\   

Table 5: Modules ablation study.