# Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Bayesian optimization (BO) mainly uses Gaussian processes (GP) with a stationary and separable kernel function (e.g., the squared-exponential kernel with automatic relevance determination [SE-ARD]) as the surrogate model. However, such localized kernel specifications are deficient in learning complex functions that are non-stationary, non-separable and multi-modal. In this paper, we propose using Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization (BO) in a \(D\)-dimensional grid with both continuous and categorical variables. Our key idea is to approximate the underlying \(D\)-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, the information from each sample can be shared not only with neighbors but also across dimensions, thus fostering a more global search strategy. Although BKTF no longer has an analytical posterior, we efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC). We conduct numerical experiments on several test functions with continuous variables and two machine learning hyperparameter tuning problems with mixed variables. The results show that BKTF offers a flexible and highly effective approach to characterizing and optimizing complex functions, especially in cases where the initial sample size and budget are severely limited.

## 1 Introduction

For many applications in science and engineering, such as emulation-based studies, experiment design, and automated machine learning, the goal is to optimize a complex black-box function \(f()\) in a \(D\)-dimensional space, for which we have limited prior knowledge. The main challenge in such optimization problems is that we aim to efficiently find the global optima, while the objective function \(f\) is often gradient-free, multimodal and computationally expensive to evaluate. Bayesian optimization (BO) offers a powerful statistical approach to these problems, particularly when the observation budgets are limited [1; 2; 3]. A typical BO framework consists of two components--a surrogate model and an acquisition function (AF)--to balance exploitation and exploration. The surrogate is a probabilistic model that allows us to estimate \(f()\) with uncertainty at a new location \(\), and the AF is used to determine which location to query next.

Gaussian process (GP) regression is the most widely used surrogate for BO [3; 4], thanks to its appealing properties in providing analytical derivations and uncertainty quantification (UQ). The choice of kernel/covariance function is a critical decision in GP models; for multidimensional BO problems, perhaps the most popular kernel is the ARD (automatic relevance determination)--Squared-Exponential (SE) or Matern kernel . Although this specification has certain numerical advantages and can help automatically learn the importance of input variables, a key limitation isthat it implies/assumes that the underlying stochastic process is stationary and separable, and the value of the covariance function between two random points quickly goes to zero with the increase of input dimensionality. These assumptions can be problematic for complex real-world processes with long-range dependencies, because estimating the underlying function with a simple ARD kernel would require a large number of observations. A potential solution to address this issue is to use more flexible kernel structures. The additive kernel, for example, is designed to characterize a more "global" structure by restricting variable interactions . However, in practice using additive kernels requires strong prior knowledge to determine the proper interactions and involves a large number of kernel hyperparameters to learn . Another emerging solution is to use deep GP , such as in [8; 9]; however, learning deep GP often becomes a more challenging task due to the inference of latent layers. In addition, these GP related surrogates can be more deficient to tune when taking into account both continuous and categorical inputs.

In this paper, we propose using _Bayesian Kernelized Tensor Factorization_ (BKTF) as a flexible and adaptive surrogate model for BO in a \(D\)-dimensional Cartesian product space (i.e., grid) when \(D\) is relatively small (say \(D 10\)). BKTF is initially developed for modeling multidimensional spatiotemporal data with UQ, for tasks such as spatiotemporal kriging/cokriging [10; 11]. This paper adapts BKTF to the BO setting, and our key idea is to characterize the multivariate objective function \(f()=f(x_{1},,x_{D})\) for a specific BO problem using the low-rank tensor CANDECOMP/PARAFAC (CP) factorization with random basis functions. Unlike other basis-function models that rely on known/deterministic basis functions , BKTF uses a hierarchical Bayesian framework to achieve high-quality UQ in a more flexible way--GP priors are used to model the basis functions, and hyperpriors are used to model kernel hyperparameters in particular for the lengthscale that characterizes the scale of variation. In addition, BKTF also provides a natural solution for categorical variables, for which we can simply introduce an inverse-Wishart prior on the covariance matrix of the basis functions.

Figure 1 shows the comparison between BKTF and GP surrogates when optimizing a bivariate function (\(D=2\)) that is nonstationary, nonseparable, and multimodal. The details of this function and the BO experiments are provided in Appendix C. This \(2D\) case clearly shows that GP surrogate is limited by the local kernel and becomes ineffective in finding the global solution, while BKTF offers superior flexibility and adaptability to characterize the multidimensional process from limited data. Unlike GP-based surrogate models, BKTF no longer has an analytical posterior; however, efficient inference and acquisition can be achieved through Markov chain Monte Carlo (MCMC) in an element-wise learning way, in which we update basis functions and kernel hyperparameters using Gibbs sampling and slice sampling, respectively . For optimization, we first use MCMC samples to approximate the posterior distribution of the entire tensor and then naturally define the upper confidence bound (UCB) of the posterior as AF. This process is feasible for many real-world applications that can be studied in a discretized tensor product space, such as experimental design.

Figure 1: BO for a 2D function: (a) True function surface, where the global maximum is marked; (b) Comparison between BO models using GP surrogates (with two AFs) and BKTF with 30 random initial observations, averaged over 20 replications; (c) Specific results of one run, including the final estimated mean surface for \(f\), in which green dots denote the locations of the selected candidates, and the corresponding AF surface.

We conduct extensive experiments on both standard optimization and ML hyperparameter tuning tasks. Our results show that BKTF achieves a fast global search for optimizing complex objective functions with limited initial data and budget.

## 2 Bayesian optimization

Let \(f:=_{1}_{D}\) be a black-box function that could be nonconvex, derivative-free, and expensive to evaluate. BO aims to address the global optimization problem:

\[^{}=*{arg\,max}_{}f().\] (1)

BO solves this problem by first building a probabilistic model for \(f()\) (i.e., surrogate model) based on initial observations and then using the predictive distribution to decide where in \(\) to evaluate/query next. The overall goal of BO is to find the global optimum of the objective function using as few evaluations as possible. Most BO models rely on a GP prior for \(f()\) to achieve prediction and UQ:

\[f()=f(x_{1},,x_{D})(m(),k (,^{})),x_{d}_{d},\ d=1, ,D,\] (2)

where \(k(,)\) is a valid kernel/covariance function and \(m()\) is a mean function that can be generally assumed to be 0. Given a finite set of observation points \(\{_{i}\}_{i=1}^{n}\) with \(_{i}=(x_{1}^{i},,x_{D}^{i})^{}\), the vector of function values \(=(f(_{1}),,f(_{n}))^{}\) has a multivariate Gaussian distribution \((,)\), where \(\) is the \(n n\) covariance matrix. For a set of observed data \(_{n}=\{_{i},y_{i}\}_{i=1}^{n}\) with i.i.d. Gaussian noise, i.e., \(y_{i}=f(_{i})+_{i}\) where \(_{i}(0,^{-1})\), GP gives an analytical posterior distribution of \(f()\) at an unobserved point \(^{}\):

\[f(^{})\ _{n}_{^{ }}(+^{-1}_{n})^{-1},\ k(^{ },^{})-_{^{}}(+^{-1} {I}_{n})^{-1}_{^{}}^{},\] (3)

where \(_{^{}}=[k(^{},_{1}),,k( {x}^{},_{n})]^{}\) and \(=(y_{1},,y_{n})^{}\).

Based on the posterior distributions of \(f\), one can compute an AF, denoted by \(:\), for a new candidate \(^{}\) and evaluate how promising \(^{}\) is. In BO, the next query point is often determined by maximizing a selected/predefined AF, i.e., \(_{n+1}=*{arg\,max}_{}(_{n})\). Most AFs are based on the predictive mean and variance; for example, a commonly used AF is the **expected improvement** (EI) :

\[_{}(_{n})=() ()}{()})+|( )|()}{()}),\] (4)

where \(()=()-f_{n}^{}\) is the expected difference between the proposed point \(\) and the current best solution, \(f_{n}^{}=_{\{_{i}\}_{i=1}^{n}}f()\) denotes the best function value obtained so far; \(()\) and \(()\) are the predictive mean and predictive standard deviation at \(\), respectively; and \(()\) and \(()\) denote the probability density function (PDF) and the cumulative distribution function (CDF) of standard normal, respectively. Another widely applied AF for maximization problems is the **upper confidence bound** (UCB) :

\[_{}(_{n},)=()+ (),\] (5)

where \(\) is a tunable parameter that balances exploration and exploitation. The general BO procedure can be summarized as Algorithm 1.

``` Input: Initial dataset \(_{0}\) and a trained surrogate model; total budget \(N\). for\(n=1,,N\)do  Approximate the posterior distribution of \(f\) using the surrogate model based on \(_{n-1}\);  Find next evaluation point \(_{n}\) by optimizing the AF;  Augment data \(_{n}=_{n-1}\{_{n},y_{n}\}\), update the surrogate model. ```

**Algorithm 1**Basic BO process.

## 3 BKTF for Bayesian optimization

### Bayesian hierarchical model specification

Before introducing BKTF, we first construct a \(D\)-dimensional grid space corresponding to the search space \(\), where \(_{d}\) could be continuous, integer-valued, or categorical. We define it on \(D\) sets \(\{S_{1},,S_{D}\}\) and denote the whole grid by \(_{d=1}^{D}S_{d}\):\(\{(s_{1},,s_{D}) d\{1,,D\},s_{d} S_{d}\}\). For dimensions with integer-valued and categorical input, we consider \(S_{d}\) the set of corresponding discrete values. For dimensions with continuous input, the coordinate set \(S_{d}\) is formed by \(m_{d}\) interpolation points \(c_{i}^{d}\) that are distributed over the bounded interval \(_{d}=[a_{d},b_{d}]\), i.e., \(S_{d}=\{c_{i}^{d}\}_{i=1}^{m_{d}}\) with \(c_{i}^{d}_{d}\). The size of \(S_{d}\) becomes \(|S_{d}|=m_{d}\), and size of the entire grid space is \(_{d=1}^{D}|S_{d}|\). Note that we do not restrict \(S_{d}\) to be uniformly distributed.

We assume the underlying function \(f\) as a stochastic process that is zero-centered. We randomly sample an initial dataset including \(n_{0}\) input-output data pairs \(_{0}=\{_{i},y_{i}\}_{i=1}^{n_{0}}\), where \(\{_{i}\}_{i=1}^{n_{0}}\) are located in \(_{d=1}^{D}S_{d}\), and this yields an incomplete \(D\)-dimensional tensor \(}^{|S_{1}||S_{D}|}\) with \(n_{0}\) observed points. BKTF approximates the entire data tensor \(}\) by a kernelized tensor CP decomposition:

\[}=_{r=1}^{R}_{r}_{1}^{r} _{2}^{r}_{D}^{r}+},\] (6)

where \(R\) is a pre-specified tensor CP rank, \(=(_{1},,_{R})^{}\) denote weight coefficients that capture the magnitude/importance of each rank in the factorization, \(_{d}^{r}=[g_{d}^{r}(s_{d}):s_{d} S_{d}]^{|S_{d}|}\) denotes the \(r\)th latent factor for the \(d\)th dimension, entries in \(}\) are i.i.d. white noises following \((0,^{-1})\). It should be particularly noted that both the coefficients \(\{_{r}\}_{r=1}^{R}\) and the latent basis functions \(\{g_{1}^{r},,g_{D}^{r}\}_{r=1}^{R}\) are random variables. The function approximation for \(=(x_{1},,x_{D})^{}\) is:

\[f()=_{r=1}^{R}_{r}g_{1}^{r}(x_{1})  g_{D}^{r}(x_{D})=_{r=1}^{R}_{r}_{d=1}^{D}g_{ d}^{r}(x_{d}).\] (7)

For priors, we assume \(_{r}(0,1)\) for \(r=1,,R\) and use a GP prior on the latent factors for dimension \(d\) with continuous input:

\[g_{d}^{r}(x_{d}) l_{d}^{r}( 0,k_{d}^{r}(x_{d},x_{d}^{};l_{d}^{r})),r=1,,R,\] (8)

where \(k_{d}^{r}\) is a valid kernel function. In this paper, we choose a Matern 3/2 kernel \(k_{d}^{r}(x_{d},x_{d}^{};l_{d}^{r})=^{2}(1+|x_{d}-x_{d}^{}|}{l_{d}^{r}})(-|x_{d }-x_{d}^{}|}{l_{d}^{r}})\). We fix the kernel variance of \(k_{d}^{r}\) as \(^{2}=1\), and only learn the lengthscale hyperparameters \(l_{d}^{r}\), since the variances of the model can be captured by \(\). One can also exclude \(\) but introduce variance \(^{2}\) as a kernel hyperparameter on one of the basis functions; however, learning kernel hyperparameters is computationally more expensive than learning \(\). For simplicity, we can also assume the lengthscale parameters to be identical, i.e., \(l_{d}^{1}==l_{d}^{R}=l_{d}\), for each dimension \(d\). The prior distribution for the corresponding latent factor \(_{d}^{}\) is then a Gaussian distribution: \(_{d}^{r}(,_{d}^{r})\), where \(_{d}^{r}\) is the \(|S_{d}||S_{d}|\) covariance matrix computed from \(k_{d}^{r}\). We place Gaussian hyperpriors on the log-transformed kernel hyperparameters to ensure positive values, i.e., \((l_{d}^{r})(_{l},_{l}^{-1})\). For categorical input, we assume that the corresponding latent basis functions \(_{d}^{r}_{d}(,_{d} ^{-1})\) for \(r=1,,R\), where the precision matrix \(_{d}\) follows a Wishart prior with an identity scale matrix and \(|S_{d}|\) degrees of freedom, i.e., \(_{d}(_{|S_{d}|},|S_{d}|)\). For noise precision \(\), we assume a conjugate Gamma prior \((a_{0},b_{0})\). For dimensions with integer variables, we could model the covariance of the basis functions either using a kernel matrix or with an inverse-Wishart prior, depending on specific situations.

For observations, we assume each \(y_{i}\) in the initial dataset \(_{0}\) follows a Gaussian distribution:

\[y_{i}\{g_{d}^{r}(x_{d}^{i})\},\{_{r}\}, (f(_{i}),^{-1}).\] (9)

### BKTF as a two-layer deep GP

Here we show the representation of BKTF as a two-layer deep GP. The **first** layer characterizes the generation of latent functions \(\{g_{d}^{r}\}_{r=1}^{R}\) for dimension \(d\). For the **second** layer, if we consider \(\{g_{1}^{r},,g_{D}^{r}\}_{r=1}^{R}\) as parameters and rewrite the functional decomposition in Eq. (7) as a linear function \(f(;\{_{r}\})=_{r=1}^{R}_{r}_{d=1}^{D}g_{ d}^{r}(x_{d})\) with \(_{r}}{}(0,1)\), we can marginalize \(\{_{r}\}\) and obtain a fully symmetric multilinear kernel/covariance function for any two data points \(=(x_{1},,x_{D})^{}\) and \(^{}=(x_{1}^{},,x_{D}^{})^{}\):

\[k(,^{};\{g_{1}^{r},,g_{D}^{r}\}_{r=1}^{R})= _{r=1}^{R}_{d=1}^{D}g_{d}^{r}(x_{d})g_{d}^{r}(x_{d}^{ }).\] (10)As can be seen, the second layer has a multilinear product kernel function parameterized by \(\{g_{1}^{r},,g_{D}^{r}\}_{r=1}^{R}\). There are some properties to highlight: (i) the kernel is **nonstationary** since the value of \(g_{d}^{r}()\) is location specific, and (ii) the kernel is **nonseparable** when \(R>1\). Therefore, this specification is very different from traditional GP surrogates, such as:

\[&k(,^{})=_{d=1}^{ D}k_{d}(x_{d},x_{d}^{}),\\ &\]

Additive GP (2nd order): \[ k(,^{})=_{d=1}^{ D}k_{d}^{1}(x_{d},x_{d}^{})+_{d=1}^{D-1}_{e=d+1}^{D}k_{d}^{2} (x_{d},x_{d}^{})k_{e}^{2}(x_{e},x_{e}^{}), \\ &\]

where all kernel functions are stationary with different hyperparameters (e.g., length scale and variance). Compared to the GP-based kernel specification, the multilinear kernel in Eq. (10) has a much larger set of hyperparameters and becomes more flexible and adaptive for the data. From a GP perspective, learning the hyperparameter in the kernel function in Eq. (10) will be computationally expensive; however, we can achieve efficient Bayesian inference of \(\{_{r},g_{1}^{r},,g_{D}^{r}\}_{r=1}^{R}\) under a kernelized tensor factorization framework.

### Model inference

Unlike GP, BKTF no longer enjoys an analytical posterior distribution. Based on the aforementioned prior and hyperprior settings, we adapt the MCMC updating procedure in Ref. [10; 11] to an efficient Gibbs sampling algorithm for model inference. This allows us to accommodate observations that are not located in the grid space \(_{d=1}^{D}S_{d}\). The detailed derivation of the sampling algorithm is given in Appendix A. In terms of computational cost, we note that updating \(_{d}^{r}\) and kernel hyperparameters requires \(\{(n^{3}),(|S_{d}|^{3})\}\) in time. Sparse GP (such as ) could be introduced when \(n,|S_{d}|\) become large. See Appendix A, F for detailed discussion/comparison about computation complexity.

### Prediction and AF computation

In each step of function evaluation, we run the MCMC sampling process \(K\) iterations for model inference, where the first \(K_{0}\) samples are taken as burn-in and the last \(K-K_{0}\) samples are used for posterior approximation. The predictive distribution for any entry \(f^{*}\) in the defined grid space conditioned on the observed dataset \(_{n}\) can be obtained by the Monte Carlo approximation \(p(f^{*}_{n},_{0}) }_{k=K_{0}+1}^{K}p(f^{*}(_{d}^{r})^{(k)}, ^{(k)},^{(k)})\), where \(_{0}=\{_{l},_{l},a_{0},b_{0}\}\) is the set of all parameters used in hyperpriors. Although a direct analytical predictive distribution does not exist in BKTF, we can use MCMC samples to obtain the mean and variance of all points on the grid, thus offering an enumeration-based approach to define AF.

We define a Bayesian variant of the UCB as the AF by approximating the predictive mean and variance (or uncertainty) in ordinary GP-based UCB with the values calculated from MCMC sampling. For every MCMC sample after burn-in, i.e., \(k>K_{0}\), we can estimate an output tensor \(}}^{(k)}\) over the entire grid space using the latent factors \((_{d}^{r})^{(k)}\) and the weight vector \(^{(k)}\): \(}}^{(k)}=_{r=1}^{R}_{r}^{(k)}(_{1 }^{r})^{(k)}(_{2}^{r})^{(k)}(_{D}^{r})^{(k)}\). We can then compute the corresponding mean and variance tensors of the \((K-K_{0})\) samples \(\{}}^{(k)}\}_{k=K_{0}+1}^{K}\), and denote the two tensors by \(}\) and \(}}\), respectively. The approximated predictive distribution at each point \(\) in the space becomes \(()(u(),v())\). Following the definition of UCB in Eq. (5), we define Bayesian UCB (B-UCB) at location \(\) as \(_{}(,,_{d}^{r},)=u()+)}\). The next search/query point can be determined via \(_{}=_{\{_{d=1}^{D}S_{d}-_{n-1 }\}}_{}()\).

We summarize the implementation procedure of BKTF for BO in Appendix B (see Algorithm 2). Given the sequential nature of BO, when a new data point arrives at step \(n\), we can start the MCMC with the last iteration of the Markov chains at step \(n-1\) to accelerate model convergence. The main computational and storage cost of BKTF is to interpolate and save the tensors \(}}^{|S_{1}||S_{D}|}\) over \((K-K_{0})\) iterations for Bayesian AF estimation. This could be prohibitive when the MCMC sample size \(K\) or the dimensionality \(D\) becomes large. To avoid saving the tensors, in practice, we can simply use the maximum values of each entry over the \((K-K_{0})\) iterations through iterative pairwise comparison. The number of samples after burn-in then implies the value of \(\) in \(_{}\). We adopt this simple AF in our numerical experiments.

A critical challenge in BRTF is that tensor size grows exponentially with the number of dimensions. To decrease the computational burden of enumeration-based AF, we also implement BKTF with random discretization--randomly selecting candidate points instead of reconstructing the whole space, denoted as **BKTFrandom**. BKTFrandom can be applied to functions with higher dimensions (e.g., \(D>10\)). We discuss the comparison between BKTF and BKTFrandom in Experiments on test functions, see Section 5.1.

## 4 Related work

The key of BO is to effectively characterize the posterior distribution of the objective function from a limited number of observations. The most relevant work to our study is the _Bayesian Kernelized Factorization_ (BKF) framework, which has been mainly used for modeling large-scale and multidimensional spatiotemporal data with UQ. The key idea is to parameterize the multidimensional stochastic processes using a factorization model, in which specific priors are used to encode spatial and temporal dependencies. Signature examples of BKF include spatial dynamic factor model (SDFM) , variational Gaussian process factor analysis (VGFA) , and Bayesian kernelized matrix/tensor factorization (BKMF/BKTF) [10; 11; 17]. A common solution in these models is to use GP prior to modeling the factor matrices, thus encoding spatial and temporal dependencies. In addition, for categorical dimensions, BKTF uses an inverse-Wishart prior to modeling the covariance matrix for the latent factors. A key difference among these methods is how inference is performed. SDFM and BKMF/BKTF are fully Bayesian hierarchical models and they rely on MCMC for model inference, where the factors can be updated via Gibbs sampling with conjugate priors; for learning the posterior distributions of kernel hyperparameters, SDFM uses the Metropolis-Hastings sampling, while BKMF/BKTF uses the more efficient slice sampling. On the other hand, VGFA uses variational inference to learn factor matrices, while kernel hyperparameters are learned through maximum a posteriori (MAP) estimation without UQ. Overall, BKTF has shown superior performance in modeling multidimensional spatiotemporal processes with high-quality UQ for 2D/3D spaces [11; 17].

The proposed BKTF surrogate models the objective function--as a single realization of a random process--using low-rank tensor factorization with random basis functions. This basis function-based specification is closely related to multidimensional Karhunen-Loeve (KL) expansion  for stochastic (spatial, temporal, and spatiotemporal) processes. Empirical analysis of KL expansion is also known as proper orthogonal decomposition (POD). With a known kernel/covariance function, truncated KL expansion allows us to approximate the underlying random process using a set of eigenvalues and eigenfunctions derived from the kernel function. Numerical KL expansion is often referred to as the Garlekin method, and in practice, the basis functions are often chosen as prespecified and deterministic functions [12; 19], such as Fourier basis, wavelet basis, orthogonal polynomials, B-splines, empirical orthogonal functions, radial basis functions (RBF), and Wendland functions (i.e., compactly supported RBF) (see, e.g., , , , ). However, the quality of UQ will be undermined as the randomness is fully attributed to the coefficients \(\{_{r}\}\); in addition, these methods also require a large number of basis functions (or a large rank) to fit complex stochastic processes. Different from methods with fixed/known basis functions, BKTF uses a Bayesian hierarchical modeling framework to better capture the randomness and uncertainty in the data, in which GP priors are used to model the latent factors (i.e., basis functions are also random processes) on different dimensions, and hyperpriors are introduced on the kernel hyperparameters. Therefore, BKTF becomes a fully Bayesian version of multidimensional KL expansion for stochastic processes with unknown covariance from partially observed data, however, without imposing any orthogonal constraint on the basis functions. Following the analysis in section 3.2, BKTF is also a special case of a two-layer deep Gaussian process [24; 7], where the first layer produces latent factors for each dimension, and the second layer has a multilinear kernel parameterized by all latent factors.

## 5 Experiments

### Optimization for benchmark test functions

We test the proposed BKTF model for BO on seven benchmark functions that are used for global optimization problems , with dimension \(D\) ranging from 2 to 10. All selected standard functions are multimodal; detailed descriptions are given in Appendix D. In fact, we can visually see that the standard Damavandi/Schaffer/Griewank functions in Figure 7 (see Appendix D) indeed have a low-rank structure. For each function, we assume the initial dataset \(_{0}\) contains \(n_{0}=D\) observed data pairs, and we set the total number of query points to \(N=80\) for 4D Griewank and 6D Hartmann function, \(N=200\) for 10D Griewank, and \(N=50\) for others. We rescale the input search range to \(\) for all dimensions and normalize the output data using z-score normalization.

Model configurationWhen applying BKTF to continuous test functions, we introduce \(m_{d}\) interpolation points for the \(d\)th dimension of the input space. The values of \(m_{d}\) used for each benchmark function are predefined and given in Table 1 (see Appendix D). Setting the resolution grid will require certain prior knowledge (e.g., smoothness of the function); and it also depends on the available computational resources and the number of entries in the tensor, which grows exponentially with \(m_{d}\). In practice, we find that setting \(m_{d}=10 100\) is sufficient for most problems. We set the CP rank \(R=2\), and for each BO function evaluation run 400 MCMC iterations for model inference where the first 200 iterations are taken as burn-in. We use Matern 3/2 kernel as the covariance function for all the test functions.

Note that for the 10D Griewank function, the grid-based models do not work, and only models built in continuous space and BKTFrandom can be performed. For BKTFrandom, in each evaluation we randomly select 20k points in the search space as candidates and choose the one with the best AF as the next evaluation location.

BaselinesWe compare BKTF with the following BO methods that use GP as the surrogate model: (1) GP \(_{}\) and (2) GPgrid \(_{}\): GP as the surrogate model and EI as the AF, in continuous space \(_{d=1}^{D}_{d}\) and Cartesian grid space \(_{d=1}^{D}S_{d}\), respectively; (3) GP \(_{}\) and (4) GPgrid \(_{}\): GP as the surrogate model with UCB as the AF where \(=2\), in \(_{d=1}^{D}_{d}\) and \(_{d=1}^{D}S_{d}\), respectively; (5) additive GP: the sum of two 1st-order additive kernels per dimension as the surrogate with EI as the AF, in continuous space. This baseline has the same number of latent functions as BKTF (\(R=2\)) but in a sum-based manner; (6) deepGP: a two-layer fully-Bayesian deep GP with EI as the AF, implemented with the _deepgp_ package1.

Similar as the setting for BKTF, we use Matern 3/2 kernel in all GP models. Given the computational cost, we only compare deepGP on 2D functions . For AF optimization in GP \(_{}\) and GP \(_{}\), we first use the DIRECT algorithm  and then apply the Nelder-Mead algorithm  to further search if there exist better solutions. We also compare with SAASBO (Sparse Axis-Aligned Subspace)  with Hamiltonian Monte Carlo sampling, implemented using BoTorch , on the 6D Hartmann and 10D Griewank functions.

ResultsTo compare the optimization performance of different models on the benchmark functions, we define the absolute error between the global optimum \(f^{}\) and the current estimated global optimum \(^{}\), i.e., \(|f^{}-^{}|\), as the _regret_, and examine how _regret_ varies with the number of function

Figure 2: Optimization on benchmark test functions, where medians with 25% and 75% quartiles of 10 runs are compared.

evaluations. We run the optimization 10 times for every test function with a different set of initial observations. The results are summarized in Figure 2. We see that for the 2D functions Branin and Schaffer, BKTF clearly finds the global optima much faster than GP surrogate-based baselines. For Damavandi function, where the global minimum (\(f(^{*})=0\)) is located in a small sharp area while the local optimum (\(f()=2\)) is located at a large smooth area (see Figure 7 in Appendix D), GP-based models are trapped around the local optima in most cases ( i.e., _regret_ = 2) and cannot jump out. In contrast, BKTF explores the global characteristics of the objective function over the entire search space and reaches the global optimum within 10 iterations of function evaluations. For higher dimensional Griewank and Hartmann functions, BKTF successfully arrives at the global optima under the given observation budgets, while GP-based comparison methods are prone to be stuck around local optima. We compare the _regret_ at the last iteration in Table 2 (Appendix E.2), along with the average cost of evaluations. The enumeration-based GP surrogates, i.e., GPgrid \(_{}\) and GPgrid \(_{}\), perform a little better than direct GP-based search, i.e., GP \(_{}\) and GP \(_{}\) on Damavandi function, but worse on others. This means that the discretization, to some extent, offers possibilities for searching all the alternative points in the space, since in each function evaluation, every sample in the space is equally compared solely based on the predictive distribution. Additive GP is comparable with \(R=2\) BKTF; while the results demonstrate that BKTF can be much more flexible than additive GP. As for BKTFrandom, we see that it can alleviate the curse of dimensionality and be applied for higher-dimensional problems that may not be performed with a grid but at the cost of more evaluation budgets, particularly it costs more iterations for lower-dimensional functions compared with BKTF.

Overall, BKTF reaches the global optimum for every test function and shows superior performance for complex objective functions with a faster convergence rate. To intuitively compare the overall performance of different models across multiple experiments/functions, we further estimate performance profiles (PPs)  (see Appendix E.1), and compute the area under the curve (AUC) for quantitative analysis (see bottom right of Figure 2 and Table 2 in Appendix E.2). Our results show that BKTF obtains the best performance across all functions.

**Interpretable latent space.** The sampled latent functions are interpretable and imply the underlying correlations of the objective function. We illustrate the learned periodic (global) patterns in Appendix E.3. **Effects of hyperpriors.** Since we build a fully Bayesian model, the hyperparameters of the covariance functions can be updated automatically from the data likelihood and hyperprior. Note that in optimization scenarios where the observations are scarce, the prediction performance of BKTF highly depends on the hyperprior settings, i.e., \(_{0}=\{_{l},_{l},a_{0},b_{0}\}\). We discuss the effects of hyperpriors in Appendix E.4. **Effects of rank.** The only predefined model parameter is the model rank, all other model parameters and hyperparameters are sampled with MCMC. We discuss the effects of rank on the 2D nonstationary nonseparable function defined in Introduction (see Figure 1) in Appendix E.5. We see that BKTF is robust to rank specification and can find the global solution efficiently with rank set as 2, 4, 6 and 8.

### Hyperparameter tuning for machine learning

In this section, we evaluate the performance of BKTF for automatic machine learning (ML) tasks. We compare different models to optimize the hyperparameters of two ML algorithms--random forest (RF) and neural network (NN)--on classification for the MNIST database of handwritten digits2 and regression for the Boston housing dataset3. The tuning tasks involve both integer-valued and categorical parameters, and the details are given in the Appendix G. We treat those integer-valued dimensions as continuous and use a Matem 3/2 kernel for the basis functions. Given the size of the hyperparameter space, we perform BKTFrandom for classification and BKTF for regression. We assume that the number of initial observations \(_{0}\) equals the number of tuning hyperparameters. The total budget \(N\) is \(20\) for the classification task and \(50\) for the regression. We implement RF algorithms using the scikit-learn package and construct NN models by Keras with 2 hidden layers. All other model hyperparameters are set as the default values.

Model configurationWe treat all the integer hyperparameters as samples from a continuous space and then generate the corresponding Cartesian product space \(_{d=1}^{D}S_{d}\). One can interpret the candidate values for each hyperparameter as the interpolation points in the corresponding input dimension. The size of the spanned space \( S_{d}\) is \(91 46 64 10 11 2\) and \(91 46 13 10\) for RF classifier and RF regressor, respectively; \(91 49 31 18 3 2\) and \(91 49 31\) for NN classifier and NN regressor respectively. We set the tensor rank \(R=4\) for BKTF, set \(K=400\) and \(K_{0}=200\) for MCMC inference, and use the Matern 3/2 kernel for capturing correlations.

BaselinesIn addition to GP surrogate-based GP \(_{}\) and GP \(_{}\), we also compare with a baseline method: random search (RS), and two non-GP models: particle swarm optimization (PSO)  and Tree-structured Parzen Estimator (BO-TPE) , which are common methods for hyperparameter tuning. We exclude grid-based GP models as sampling the entire grid becomes infeasible.

ResultsWe compare the accuracy for MNIST classification and MSE (mean squared error) for Boston housing regression both in terms of the number of function evaluations and still run the optimization processes ten times with different initial datasets \(_{0}\). The results obtained by different BO models are given in Figure 3, and the final classification accuracy and regression MSE are compared in Table 5 (see Appendix H). For BKTF, we see from Figure 3 that the width between the two quartiles of accuracy and error decreases as more iterations are evaluated, and the median curves present better convergence rates compared to the baselines. Table 5 also shows that the proposed BKTF surrogate achieves the best final mean accuracy and regression error with small standard deviations. The results above demonstrate the advantage of BKTF as a surrogate.

## 6 Conclusion

This paper proposes to use Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization with mixed variables (both discrete/categorical and continuous) when the dimensionality is relatively small (e.g., say \(D<10\)). Compared with traditional GP surrogates, the BKTF surrogate is more flexible and adaptive to data thanks to the Bayesian hierarchical specification, which provides high-quality UQ for BO tasks. The tensor factorization model behind BKTF offers an effective solution to capture global/long-range correlations and cross-dimension correlations. The inference of BKTF is achieved through MCMC, which provides a natural solution for acquisition. Experiments on both test function optimization and ML hyperparameter tuning confirm the superiority of BKTF as a surrogate for BO. Moreover, the tensor factorization framework makes it straightforward to adapt BKTF to handle multivariate and functional output (see e.g., [33; 34]), by directly treating the output coordinates as part of the input. A limitation of BKTF is that we restrict BO to a grid search space in order to leverage tensor factorization; however, we believe that designing a compatible grid space based on prior knowledge is not a challenging task.

There are several directions to be explored to make BKTF more scalable. Scalable GP solutions, such as sparse GP  and Gaussian Markov Random Field (GMRF) , can be introduced to reduce the inference cost when \(|S_{d}|\) becomes large. The current MCMC-based acquisition method requires explicit reconstruction of the whole tensor, which quickly becomes infeasible when \(D\) becomes large (e.g., \(D>20\)). A natural question is whether it is possible to achieve efficient acquisition directly using the basis functions and the corresponding weights without explicitly constructing the tensors. This problem corresponds to finding/locating the maximum entry of a tensor given its low-rank decomposition (see e.g., ).

This work aims to advance the field of probabilistic machine learning, particularly Bayesian optimization. Regardless of the model limitations, it has the potential of misuse for ML algorithms.

Figure 3: Comparison of hyperparameter tuning for ML tasks: (a) classification; (b) regression.