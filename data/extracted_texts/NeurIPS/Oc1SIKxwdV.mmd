# Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors

Thomas Hartvigsen

University of Virginia, MIT

hartvigsen@virginia.edu

&Swami Sankaranarayanan

Sony AI

swami.sankaranarayanan@sony.com

&Hamid Palangi

Microsoft Research

hpalangi@microsoft.com

&Yoon Kim

MIT

yoonkim@mit.edu

&Marzyeh Ghassemi

MIT

mghassem@mit.edu

###### Abstract

Deployed language models decay over time due to shifting inputs, changing user needs, or emergent world-knowledge gaps. When such problems are identified, we want to make targeted edits while avoiding expensive retraining. However, current model editors, which modify such behaviors of pre-trained models, degrade model performance quickly across multiple, sequential edits. We propose GRACE, a _lifelong_ model editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at github.com/thartvigsen/grace.

## 1 Introduction

Large scale pre-trained neural networks are the state-of-the-art for many hard machine learning problems, especially in natural language processing [5; 33] and computer vision [9; 34]. But when deployed, they still make unpredictable errors [2; 43]. For example, Large Language Models (LLMs) notoriously _hallucinate_, _perpetuate bias_, and _factually decay_. Many such errors will arise sequentially in deployment, some of which must be addressed quickly without waiting until new training data is collected [16; 21]. For example, when an LLM generates hate speech or crucial knowledge about the world changes, its behavior must be modified immediately to protect users.

While _retraining_ or _finetuning_ can edit a model's predictions, doing this frequently is often too computationally expensive. LLaMA , for instance, was trained for 21 days on 2,048 A100 GPUs, costing over $2.4M and emitting over 1,000 tons of CO\({}_{2}\). Even repeatedly retraining or finetuning smaller models  quickly costs too much for most practitioners. To enable cheap, targeted updates to big, pre-trained models, we study _lifelong model editing_. Here, we continually make targeted edits sequentially throughout a model's deployment. Success means correcting a model's predictions on a stream of edits without decaying its performance on unrelated and previously-edited inputs.

Two possible approaches to lifelong model editing are _continual learning_ and conventional _model editing_. While continual learning methods can update models sequentially, when used for sequential editing, they suffer from overfitting  and quickly forget previous edits and pre-training data [16; 22]. Alternatively, existing _static_ model editors could also be run sequentially. But the editors thatrely on regularized finetuning [8; 28; 29; 37] succumb to overfitting , while hypernetwork methods require pre-training on hard-to-access data [30; 31]. Further, these existing approaches require large sets of representative training edits, access to the model's pre-training data, or semantically-equivalent inputs. Accessing these data is often unrealistic or impractical, and existing editor performance severely decays the pre-trained model after only a few sequential edits [12; 16].

We study lifelong editing using _only_ singular inputs to edit models, where models are edited immediately upon arrival. This setup extends beyond recent works, which use many inputs for editing, including collected sets of semantically-equivalent inputs, training edits, and the model's pre-training data. Editing in our realistic, more cost-effective setup is hard because each edit must fix the flagged error, while generalizing to similar future inputs without impacting unrelated model behavior.

To address the challenging lifelong model editing setup, we propose General Retrieval Adaptors for Continual Editing, or GRACE. While we study transformers for natural language processing, GRACE is broadly applicable. As illustrated in Figure 1, GRACE edits a model by adding an Adaptor to a chosen layer, while never changing its weights. This Adaptor then modifies layer-to-layer transformations for select inputs. By caching embeddings for input errors and learning values that decode into desired model outputs, GRACE serves as a codebook in which edits are stored, enabling longer sequences of edits than prior works. To encourage generalization, we leverage the models' semantic similarity in its latent space by introducing \(\)-balls around cached edits. GRACE then only applies for inputs near existing keys, differing from more-traditional Adaptors . By managing the \(\)-balls' size over time, GRACE makes immediate edits, remembers previous edits, and leaves correct model behaviors intact, making it parameter efficient. Further, since GRACE codebooks leave model weights unaltered and are fully model-agnostic, they also point towards plug-and-play, cost-effective model editing, especially to make crucial spot-fixes between bigger retraining efforts.

Our contributions are as follows:

1. We establish key metrics and comparisons for lifelong model editing, an important but understudied and challenging problem setting. We introduce two new public benchmarks for lifelong model editing: mitigating LLM hallucination  and addressing label shifts .
2. We develop GRACE, a method for lifelong model editing that avoids expensive retraining or finetuning. Unlike other works, GRACE requires no inputs beyond singular edits. GRACE works with autoregressive and non-autoregressive models alike and achieves parameter efficiency without touching the model's weights.
3. Our experiments show that GRACE outperforms seven alternatives when sequentially editing T5, BERT, and GPT models for question answering, document classification, and language generation. We show that GRACE edits can generalize to new inputs without memorizing while incurring only a small, one-time cost to inference time over long sequences of edits.

## 2 Methods: Lifelong Model Editing with GRACE

### Problem Formulation

The Lifelong Model Editing task is to edit the same model hundreds to thousands of times _in a row_ without forgetting upstream performance or fixes for previous edits. Let \(f_{0}\) denote a model with

Figure 1: Overview of lifelong model editing with GRACE. a) Models make important errors that must be corrected. b) GRACE makes edits by learning, caching, and selectively retrieving new transformations between layers. c) Edits appear sporadically and require quick fixes, so GRACE codebooks are curated over long sequences of edits.

frozen parameters that was pre-trained on dataset \(_{}\). For clarity, we drop the subscript where possible. Assume that \(f\) consists of \(L\) layers, where \(f^{l}()\) computes the hidden state at layer \(l\). In this work, we assume \(f\) is a transformer architecture for natural language, though our principles are general. We then deploy \(f\) on a stream of inputs \([x_{0},x_{1},...]\), where \(x_{t}_{}\), which contains samples observed during deployment. We then monitor the model's predictions \(_{t}=f(x_{t})\) over the stream. Note that the prediction tasks during training and deployment are the same. Over time, the model makes errors such that \(_{t} y_{t}\), where \(y_{t}\) is the true label. To continue safely deploying \(f\), we aim to _edit_\(f\) such that \(f(x_{t})=y_{t}\). After each edit, the updated \(f\) should 1) be successfully edited such that \(f(x_{t})=y_{t}\), 2) retain accuracy on prior edits \(x_{<t}\), and 3) retain its behavior on its training data: \(f(x_{i})=f_{0}(x_{i}) x_{i}_{}\). In contrast to prior works [8; 28; 30; 31; 37], we assume access to only input edits \(x_{t}\) and corrected labels \(y_{t}\), since \(_{}\) is often proprietary or prohibitively large, and collecting training edits or semantically-equivalent inputs is often expensive in practice.

### GRACE: General Retrieval Adaptors for Continual Editing

We propose GRACE, a method for sequentially editing a pre-trained model's behavior _without_ altering its weights, as illustrated in Figure 1. GRACE works by wrapping a chosen layer of any pre-trained model architecture with an Adaptor. A GRACE Adaptor at model \(f\)'s layer \(l\) contains two components: (1) a codebook \(\) and (2) a deferral mechanism to decide whether to use \(\) for a given input.

**GRACE codebook.** A GRACE Adaptor at layer \(l\) maintains a discrete codebook, adding and updating elements over time to edit a model's predictions. The codebook contains three components:

* _Keys_ (\(\)): Set of keys, where each key is a cached activation \(h^{l-1}\) predicted by layer \(l-1\).
* _Values_ (\(\)): Set of values that are randomly initialized and are updated using the model's finetuning loss for edits. Each key maps to a single, corresponding value.
* _Deferral radii_ (\(\)): Each key has a _deferral radius_\(\), which serves as a threshold for similarity matching. The deferral mechanism uses this radius as shown in Algorithm 1. GRACE is activated at layer \(l\)_only_ if the deferral constraint is satisfied. New entries have a default value \(_{}\), which is a hyperparameter.

**Deferral mechanism.** Before editing, GRACE layers are empty. As editing progresses, GRACE adds keys and adapts values and \(\) entries. Conceptually, inference at layer \(l\) with GRACE entails a deferral decision, computing \(h^{l}\) using a similarity search over GRACE's keys:

\[h^{l}=(h^{l-1})&_{i}(d(h^{l-1}, _{i}))<_{i_{*}},\ \ \ i_{*}=_{i}(d(h^{l-1}),_{i}),\\ f^{l}(h^{l-1})&,\] (1)

where \(f^{l}(h^{l-1})\) denotes the _unedted_ model's activation of the \(l\)-th layer. \(h^{l-1}\) can be seen as a _query_ to the codebook, and GRACE(\(h^{l-1}\)) retrieves the value associated with its closest key. \(_{i}^{l}\) and \(_{i}^{l}\) are the influence radius and key \(i\) in layer \(l\), respectively, and \(d()\) is a distance function. We follow related work  and use Euclidean distance for \(d()\) in our experiments--changing this is trivial. Through explicit similarity search, we use the fact that large models encode semantic similarity with respect to their tasks in their latent spaces. This lets GRACE edits to generalize to similar inputs in the future. If a new input is unlike any cached keys, GRACE simply defers to \(f\)'s pretrained weights. This way, GRACE layers limit interference with \(_{}\) by leaving the model weightsunaltered, which helps when input distributions shift .

**Codebook maintenance.** To make an edit, a GRACE layer can perform one of two operations. Each step is described in Algorithm 1. First, if the codebook is empty or the input embedding \(h^{l-1}\) falls _outside_ the deferral radius of all existing keys according to distance function \(d()\), then a new codebook entry is created and added: \(\{(h^{l-1},v,_{init},y)\}\). Thus if \(x_{t}\) were passed into \(f\) again, \(h^{l-1}\) would activate the codebook and value \(v\) would be passed to layer \(l+1\). Training \(v\) is detailed below.

Sometimes, a query \(h^{l-1}\) will be close enough to an existing key that adding a new entry would cause their \(\)-balls to overlap. To avoid this, we compare the edit label \(y\) to the model's prediction for the nearest key and distinguish two cases: 1) If the overlapping key's label is the _same_ as the edit's label, **Expand** that key's \(\) to encompass the query. 2) If the overlapping key's label is _different_ from the edit's label, **Split** these keys by first decreasing the influence radius of the overlapping key, then adding a new codebook entry where the new key is simply the query \(h^{l-1}\). We set both keys' \(\) values to be half their distance apart.

As edits stream over long deployments, by continuously adding and updating the GRACE codebook, layer \(l\)'s latent space is partitioned according to which inputs needed edits. When _not_ performing edits, these codebook maintenance operations are bypassed, and keys are entirely frozen. GRACE thus introduces a new model editing paradigm in which edits can be made sequentially, similar edits are encouraged to be edited similarly, and the ultimate influence of new edits can be controlled and monitored explicitly. \(_{}\) is the sole parameter in GRACE, which sets the initial \(\) value for new codebook entries. Intuitively, using a larger \(_{}\) will create edits with more influence, making edits more general, but increasing the interference with unrelated inputs. In practice, \(_{}\) could be tuned using either exogenous data or GRACE codebooks can periodically be refreshed.

**Training GRACE Values.** When making an edit with GRACE, either a new key-value pair is learned or an existing key-value pair is updated. To ensure that newly-learned values correct the model's behavior, we train them directly using backpropagation through the finetuning loss on the model's prediction given the edit. The learned value \(v\) then replaces \(h^{l}\) for the rest of the forward pass. In our experiments, we train values using 100 gradient descent steps to train the values and ensure the model's behavior is updated.

**GRACE layers with sequential inputs.** For models with different representations per input token, like transformers, we must choose 1) which token should be GRACE's input query, and 2) which token to replace with a retrieved value in the subsequent layer. In practice, we find that broadcasting the value to each token is reasonable for tasks like classification, since values gain strong control over the model's behavior and makes them easy to learn. For autoregressive models however, picking the right tokens for the query and value is more nuanced. We opt for replacing only the final token of an input prompt, which we verify experimentally, since compressing future generated text into tokens is an interesting and burgeoning direction in itself . Upon choosing these tokens, GRACE naturally applies to all popular transformer models.

### Illustrative Example

To garner intuition about GRACE, we provide an illustrative experiment on synthetic data. As shown in Figure 2, we sample 100 instances from two 2D distributions corresponding to classes. We then train a three-layer binary classifier with two 100-dimensional hidden layers and ReLU activations. Next, we introduce edits with flipped labels, simulating local label shift at test time. Using a single

Figure 2: Illustrative example of GRACE. We train a model on separable data in (a), then introduce locally-flipped labels at test time in (b). In (c), the original model unsurprisingly misclassifies these label-flipped instances. In (d), GRACE fixes these labels without impacting other inputs.

key in layer two, GRACE can successfully correct these errors while barely influencing other inputs. Meanwhile, finetuning on these errors alone will clearly break the model.

## 3 Experiments

### Experimental Setup

We evaluate GRACE's capacity to edit models hundreds to thousands of times sequentially. This is in contrast to prior works, which largely evaluate using single edits. Further, much of the model editing literature relies on synthetic edits, often generated by flipping random labels. Beyond unrealistically assessing performance, it is well-established that learning random versus natural labels is vastly different . Instead, we correct authentic mistakes made by models, each time an error is made.

**Baselines.** We compare against continual learning and model editing methods. First, we continually finetune (**FT**)  on streaming errors. To reduce overfitting, we also compare with Elastic Weight Consolidation (**EWC**)  and a simple form of experience replay  where we periodically retrain the model (**Retrain**) on all previous edits. Second, we compare against model editors **MEND** and **Defer**, inspired by SERAC  but altered for our setup. We also compare against **ROME** in our language modeling experiment. Finally, we ablate GRACE by replacing our discrete search with a **Memory** network containing memory module that is indexed by a soft attention mechanism. Details for all comparisons are in Appendix C.

**Datasets and Pre-trained Models.** We evaluate GRACE on three sequential editing tasks with corresponding pre-trained models, as shown in Table 1. 1) We edit a 60-million parameter T5 model  trained for context-free question-answering, as is used in . We extract potential edits from the validation set of **zsRE**, following the editing literature [16; 30]. This model achieves F1 of.72 on NQ and.31 on zsRE prior to editing. Further details are in Appendix B.1. 2) We edit a 110-million BERT classifier trained for a new editing task with label shift using the **SCOTUS** dataset from Fairlex . The prediction task is to categorize U.S. Supreme Court documents over multiple decades into 11 topics. Over time, categorization rules change, so label distributions shift. We train a BERT classifier on the 7.4k training cases from 1946-1982, then make edits on 931 cases from 1991-2009. We also introduce additional, realistic label shifts, as discussed in Appendix B.2. This model achieves Accuracy of 0.99 on the training set and.55 on the edit set. 3) We introduce a new editing task by correcting a GPT language models' **Hallucination**. In , authors prompt GPT-3 to generate 238 wikipedia-style biographies using subjects from WikiBio. They then annotate the factual accuracy of each sentence, recording which are hallucinations. We propose editing inaccurate sentences by replacing them with corresponding sentences in the true wikipedia entries. We include edits for all 238 biographies, creating 1392 sequential edits and 592 already-accurate outputs. We then edit GPT2-XL, which has 1.5B parameters. However, GPT2-XL was not trained on this task, so has high perplexity (PPL) on all sentences. Therefore, we finetune GPT2-XL on these data mixed with sentences from OpenWebText , a public version of GPT2's training data. Our final GPT2-XL model has PPL of 15.98 on OpenWebText (comparable to the original), 8.7 on already-accurate outputs, and 132.7 on intended edits, indicating a need for editing and room for improvement. Further details are available in Appendix B.3.

**Metrics.** To compare each method, we measure three main metrics that align with prior work [16; 28].

1. Edit Success (**ES**): We check whether an edit has been successful using ES: \(m(y,)\), where \(m()\) is a task-specific measure of accuracy: standard F1 for question answering, Accuracy for classification and Perplexity (PPL) for generation.

    &  &  &  \\   & & Dataset & N & Pre-edit & Dataset & N & Pre-edit \\  QA & T5 (60m) & NQ  & 1000 &.72 F1 & zsRE  & 1000 &.31 F1 \\ Clf. & BERT (120m) & SCOTUS\({}_{1992-1991}\) & 914 &.99 Acc & SCOTUS\({}_{1992-2009}\) & 931 & 55 Acc \\ Halluc. & GPT-2 (1.5B) & OpenWebText  & 1000 & 15.98 PPL & SelfCheckGPT & 1392 & 132.7 PPL \\   

Table 1: Dataset statistics for main results. _Test retention_ is the testing set of each model’s training data. \(N\) is the number of samples. _Pre-edit_ is the unedited model’s performance on each dataset.

2. Test Retention Rate (**TRR**): We check how well an edited model retains its performance on its original testing data using TRR: \(_{i=1}^{N}m(f(x_{i}),y_{i})\), where \((x_{i},y_{i})_{}\), \(f\)'s original test set. For T5, \(_{}\) is 1k random samples from NQ, for our BERT model, \(_{}\) is court documents from 1982-1991, and for GPT2-XL, \(_{}\) is WebText, for which we use the first 1k sentences of OpenWebText .
3. Edit Retention Rate (**ERR**): We check how well an edited model retains previous edits through ERR: \(_{i=1}^{N}m(f(x_{i}),y_{i})\) where \((x_{i},y_{i})_{}\).

We also track the number of edits (**#E**), which may vary by editor as different editors will lead to different mistakes in practice. Further implementation details are available in Appendix A.

### Results

#### 3.2.1 Comparisons to existing methods

We first find that GRACE outperforms existing methods after long sequences of edits, as shown in Table 2. We focus here on TRR and ERR, which have a trade-off as each can be achieved in isolation, though all metrics are reported in Appendix G. On zsRE and SCOTUS, we focus on TRR, ERR, and their average. ROME is incomparable on these dataset as it is only proposed for GPT models. On Hallucination, we include Accurate Retention Rate (ARR), which is the edited model's perplexity on sentences on which it was already accurate.

On zsRE and SCOTUS, GRACE's averaged TRR and ERR outperforms its closest competitors by 19% and 9%, respectively. Comparing the average performance across all methods, GRACE's improvement is 86% and 129% due to other methods' inability to balance ERR and TRR. For instance, while Defer achieves the highest TRR on SCOTUS it trades off ERR. In contrast, FT+Retrain achieves the highest ERR on SCOTUS, but trades off TRR. On both datasets, MEND and Defer both struggle to balance TRR and ERR without access to privileged data.

GRACE also outperforms the comparisons on the Hallucination task. We observe that the finetuning methods easily overfit to new edits, leading to competitive ERR but poor TRR. However, their good ERR comes at the expense of ARR in these methods. We also find that ROME and Memory both perform competitively with GRACE, indicating the value of parameter efficiency in lifelong model editing. Finally, we report the average wall-clock time it takes to perform one edit, finding that GRACE makes edits twice as fast as finetuning while competing with other Adaptors.

Excitingly, GRACE can achieve high-quality edits with small codebooks. For example, GRACE uses few keys to edit T5 on zsRE: 1000 edits are made using only 137 keys. Such compression is only feasible by managing the \(\)-balls effectively over time. This is also parameter-efficient: with details in Section 3.2.3, the zsRE codebook contains 210,569 scalar values (.35% of T5's parameters) and

    &  &  & \); PPL \(\))} \\ 
**Method** & TRR & ERR & _Avg._ & _\#E_ & TRR & ERR & _Avg._ & _\#E_ & TRR & ERR & ARR & _\#E_ & time (s) \\  FT  &.56 &.82 & _.69_ & 1000 &.52 &.52 &.52 & 415 & 1449.3 & 28.14 & 107.76 & 1392 &.26 (.07) \\ FT+EWC  &.51 &.82 & _.66_ & 1000 &.67 &.50 &.58 & 408 & 1485.7 & 29.24 & 109.59 & 1392 &.29 (.06) \\ FT+Retrain  &.27 &.99 & _.63_ & 1000 &.67 & **.83** &.75 & 403 & 2394.3 & 35.34 & 195.82 & 1392 & 23.4 (13.2) \\ MEND  &.25 &.27 &.26 & 1000 &.19 &.27 &.23 & 672 & 1369.8 & 1754.9 & 2902.5 & 1392 &.63 (.10) \\ Defer  & **.72** &.31 &.52 & 1000 &.33 &.41 &.37 & 506 & 1833.7 & 133.3 & 10.04 & 1392 &.07 (.02) \\ ROME  & — & — & — & — & — & — & — & — & — & 30.28 & 103.82 & 14.02 & 1392 &.64 (.28) \\ Memory &.25 &.27 &.26 & 1000 &.21 &.20 &.21 & 780 & 25.47 & 79.30 & 10.07 & 1392 &.11 (.02) \\  GRACE & _.69_ & **.96** & _.82_ & 1000 & **.81** &.82 & **.82** & **.83** & **15.84** & **7.14** & **10.00** & 1392 &.13 (.02) \\   

Table 2: Comparison of GRACE to existing methods. Metrics shown are computed after all sequential edits. For Hallucination, we also compute perplexity retention on already-accurate sentences (ARR) and the average time per edit. Parentheses in _time_ denote standard deviation. We also count the number of keys GRACE used. GRACE achieves the best TRR/ERR balance while making edits faster than most comparisons and using small codebooks for zsRE and SCOTUS. A large codebook is required for Hallucination, since each edit label is unique.

only 70,281 are learnable. The number of keys is also lower-bounded by the number of unique edit labels and is related to the complexity of the inputs. On SCOTUS, GRACE also achieves a relatively small codebook, showing a balance between input complexity and generalizability of each edit. Since SCOTUS is a document classification task with 11 classes, there could be as few as 11 keys, but given each document contains many sentences, there is a lower chance that the same keys can be used for many documents. That GRACE's keys represent 1.51 edits on average indicates significant generalization. As expected, the lower-bound on codebook size is evident in the Hallucination task, where GRACE uses a new key for almost every edit. This is because edit labels are unique sentences. Despite a large codebook, such success at selectively inserting tokens that generate full sentences poses an exciting direction for controllable and editable language modeling.

In Figure 3, we show a more-detailed comparison of all methods for the Hallucination editing task over time, focusing on TRR and ES (see Figure 13 for all metrics). As expected, finetuning methods excel at ES, but perform poorly at TRR. On the flip side, ROME and Memory have reasonably-low ES and TRR scores, though both suffer as edits progress. While ROME is competitive early in editing, it especially suffers on ERR (Table 2) and GRACE's TRR remains 2x better after making all edits.

#### 3.2.2 Model Analysis: Memorization vs. Generalization

Next, we study GRACE's memorization vs. generalization performance by editing T5 on zsRE on extremely long sequences of edits while varying \(_{}\) and the edited layer. We split each zsRE question's set of rephrasings into two sets: edits and holdouts. Then, we pass edits through GRACE until 3,000 edits are made. This is extremely large: even 10 edits catastrophically decays performance  and  performs roughly half as many edits. After each edit, we measure TRR, ERR, F1 on the entire Holdout set, and record the number of keys. Since we evaluate GRACE on the entire Holdout set after each edit, the results start low since has seen no rephrasings of held out edits. Therefore, later measures of Holdout performance are more representative than earlier. Figure 4 shows our results for \(_{}=0.1\) and \(_{}=3.0\), while the rest are in Appendix H. We derive the following findings.

**Layer and \(_{}\) choice balance memorization and generalization.** We first find that different blocks lead to different editing performance. Notably, editing Blocks two and four achieve high TRR, high ERR, and strong generalization for both choices of \(_{}\) On the contrary, for Block 6 we see that since each \(_{}\) is small, they do not generalize at all and also lead to poor ERR. As expected, when \(_{}\) is small, TRR performance is optimal, since most edits require a new key. While creating a large codebook can be feasible, the trade-off in Holdout becomes clear, as detailed in the next finding. The relationship between layer choice, performance, and the size of the codebook likely stems from a layer's representational capacity: Layers that map semantically-equivalent inputs near one another will be easier to edit with GRACE.

**GRACE edits generalize to unseen inputs.** Steadily increasing Holdout shows that that GRACE edits can generalize to previously-unseen holdout edits. Excitingly, interior layers appear to generalize better than early and late layers, which is backed up by their use of fewer keys. Larger \(_{}\) values also lead to better generalization, as expected, implying that the semantically-similar inputs indeed land in the same deferral radii. The later layer, Block 6, appears to

**GRACE codebooks stay small and stabilize over time.** Finally, the number of keys over time steadily flattens, indicating that the \(\) values indeed adapt to the data distribution. As we increase \(_{}\)

Figure 3: ES and TRR while editing GPT2-XL on Hallucination. Lower values are better because TRR and ERR measure perplexity. GRACE outperforms the comparisons by making successful edits while maintaining the model’s training knowledge. All metrics are shown in Figure 13.

the resultant codebooks get smaller, indicating control over codebook size. Small codebooks are also important for parameter efficiency and in generalization.

#### 3.2.3 Parameter Efficiency

GRACE's memory requirements are small and straightforward: A new edit requires \(|h^{l-1}|+|h^{l}|+1\) parameters, where \(|h^{l-1}|\) is the dimension of the key, \(|h^{l}|\) is the dimension of the value, and \(_{}\) is a scalar. Further, the key's \(|h^{l-1}|\) parameters are not trained. As detailed in Appendix E, this is comparable to the alternatives when performing thousands of edits, so performance gain is not from parameter count. This intuition is reinforced by the fact that at inference time, if the Adaptor is activated, predictions are only altered using the \(|h^{l-1}|+|h^{l}|+1\) parameters of the chosen entry.

#### 3.2.4 Interpreting GRACE Codebooks

A key benefit of GRACE is that learned codebooks can be detached from the model and inspected. This way, edits can easily be undone without impacting the model and the codebooks can be inspected throughout editing. To demonstrate this advantage, we inspect how keys and their \(\) change throughout editing Block 4 of the T5 QA model. In this experiment, we investigate how well GRACE edits generalize to unseen edits. At each edit in a sequence of 1,000 inputs, we pass the entire holdout set of edits through the newly-edited model. For each holdout instance, we record in which key's \(\)-ball it lands, if any. This way, we track whether a newly-added key generalizes to multiple holdouts successfully. We also track what proportion of the holdouts land inside _any_ key to evaluate generalization over time. In Figure 5, we summarize the results from one such experiment, with more details in Appendix D. We find that the number of holdouts per key stabilizes over time. Interestingly, \(_{}\) values capture too many holdouts per question, which explains the trade-off between generalization and TRR. In the Appendix, we also show that the number of holdouts per key are stable, while others are highly-variable. This implies that our codebook maintenance strategy can have big ripple effects according to the key. Further, some regions of the latent space are likely better than others for performing GRACE edits.

#### 3.2.5 Inference Time

To better understand GRACE's limitations, we compare the inference time of the T5-small QA model before and after editing. We compute the time it takes to run one instance through the model for each of 5,000 edits. We edit T5's block 4 and use a small \(_{}\) of 0.1 to encourage a quickly-growing

Figure 4: Impact of \(_{}\) and block choice for GRACE editing T5 on zsRE for 3000 sequential edits. Other \(_{}\) values are in Appendix H. Along with TRR and ERR, we also measure F1 on a “Holdout” edit set containing unseen rephrasings of all edits. We find that blocks 0 and 6 use more keys and achieve higher TRR, but can lead to lower ERR and generalize worse, given lower holdout values.

codebook. To evaluate the variance in training time, we replicate this experiment ten times, each leading to a codebook of roughly 4500 elements. We then average over 20-timestep windows and show standard deviation across all replications.

We find that inference with a GRACE-edited model was only 1.32x slower than an unedited model on average, as shown in Figure 6. This is a one-time cost that remains fixed even as the codebook grows. Inference time is unchanged because search between one query and a full set of keys can be vectorized. Until the codebook outgrows available memory, this cost remains fixed. That GRACE causes any slowdown is a limitation and an avenue for future work.

## 4 Related Work

**Model editing.** Model editing is a new and active research area where the goal is to make targeted changes to a pre-trained model's behavior. Most methods propose variants of regularized-finetuning via auxiliary data, like training instances from the pre-trained model's training data or by using semantically-equivalent versions of new edits . Access to such data is non-trivial, especially as training data and paradigms are becoming proprietary, and collecting semantically-equivalent inputs is often unlikely. Recent works have retained these requirements, while extending to pretrain hypernetworks that predict edits [8; 30; 31], often decomposing weight updates into low-rank components [28; 29]. Recent approaches all study transformer architectures due to their popularity and high training cost [48; 51]. To ensure targeted edits, recent methods like MEND  and ROME  also draw inspiration from parameter-efficient finetuning . But such methods are known to often require more finetuning steps and are prone to overfit more than regular finetuning [39; 50]. Further, recent works show that edited models are deeply fragile  and methods for picking which parameters to update are surprisingly unreliable .

Unfortunately, nearly all model editing works consider only _static_ edits, making only one edit to a model. While some recent works like MEMIT  indeed perform multiple edits, they do so simultaneously, not over time during deployment. Other works demonstrate that editing performance decays quickly when making multiple edits , though recent works like SERAC  and MEMIT  show burgeoning results in this direction. However, these exciting works still use large amounts of privileged information. Most similar to our work, two recent papers discuss sequential editing. First,  shows that after editing the same model 10 times, editing performance drops dramatically. Second, a concurrent paper  recently proposed a sequential editing setup. However, their method and implementation is architecture-specific and relies on large sources of unrelated inputs.

**Continual Learning.** Continual learning methods are a reasonable approach to lifelong model editing. Most-similar to our setup, recent works have investigated continual finetuning, where large language models are refined over time as new instances arrive. For example,  perform a large benchmark of continual finetuning. They find that regularizing finetuning with continual learning methods like Elastic Weight Consolidation , Experience Replay , and Maximally Interfered Replay , quickly decays performance on prior tasks, though it helps to remember some previous inputs. This implies that _editing_, as opposed to regular continual finetuning, is particularly challenging, since edits are unlikely to be uniformly distributed . One promising avenue for continual learning is key-value methods, stemming from computer vision [26; 34; 42] For example, recent works have demonstrated continual prompt-learning for NLP [44; 45] for applications like text retrieval . Recent works have shown that _discrete_ key-value methods in particular perform well with shifting distributions , with recent works extending to question answering . By caching values, these approaches keep inputs in-distribution for downstream encoders while opening doors to longer-termmemory, resources permitting. We corroborate these advantages in our experiments, where we demonstrate GRACE's robustness to shifting inputs and labels over long sequences of edits.

## 5 Limitations and Ethical Considerations

Lifelong model editing is new and challenging, so our method GRACE has limitations. Understandably, adding similarity search to the layers of a model will slow down inference. Still, while we do not emphasize inference time, accelerating GRACE is natural future step that has been successful in similar methods . Future works may also scale GRACE up to multi-layer edits, a setting unconsidered in this work. While GRACE has already scaled up continual editing to \(\)5k edits, real world scenarios might entail approaches that work at an ever larger editing scale, which presents a promising direction of future work. Another limitation of GRACE-style edits is _implication_: Behaviors are edited in isolation. For example, if we edit a model's knowledge of the _latest pandemic in the U.S._ from Swine Flu to COVID, its knowledge of _the second-to-last pandemic_ will not be updated.

While editing models can improve their behavior, it can surely be used for harm. For example, a bad actor might edit a LLM to _increase_ hate. This limitation is true for all model editors, GRACE included. However, most model editors today directly update the model's weights. This makes it hard to trace back what edits have been made to a model and understand their impact. Transparent editors like GRACE are a promising direction for overcoming this problem. For example, GRACE's codebook can be directly inspected to see what predictions stem from each value and examine properties of the latent space covered by the \(\)-balls.

## 6 Conclusions

Pre-trained models continue to grow and are being applied to a diverse set of downstream tasks. However, they still misbehave in unpredictable ways when deployed. Correcting such behavior is a challenging problem, especially when only some of the model's behavior is problematic. While regularized finetuning or retraining on better data can mitigate this problem somewhat, big models remain too expensive to train for most researchers. We instead build on the model editing literature, and study _Lifelong Model Editing_, an important but understudied problem. With a focus on transformer models for natural language processing, we edit models thousands of times in a row as edits stream during simulated deployments. Our edits only use singular, authentic errors, in contrast to recent works which make synthetic edits and require large amounts of exogeneous data like sets of training edits, semantically-equivalent examples, or pre-training data. We then present GRACE, a plug-in Adaptor for a model's chosen layer that leaves the trained weights untouched. GRACE Adaptors (1) retain the functionality of the original model, while (2) successfully editing model predictions without forgetting previous inputs. Using three real-world datasets, we edit T5, BERT, and GPT models thousands of times and find that GRACE significantly outperforms seven state-of-the-art alternatives. We further investigate GRACE's capacity to make extremely long sequences of edits and show that GRACE can generalize its edits to unseen inputs, avoiding sheer memorization.

## 7 Acknowledgements

We are grateful to all the support received while conducting this research. This project was supported by Quanta, and Marzyeh Ghassemi was supported by the Herman L. F. von Helmholtz Career Development Professorship and the CIFAR Azrieli Global Scholar award. Yoon Kim was supported by MachineLearningApplications@CSAIL.