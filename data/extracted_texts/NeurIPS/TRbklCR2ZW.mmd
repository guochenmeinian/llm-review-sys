# GLOBER: Coherent Non-autoregressive

Video Generation via GLOBal Guided Video DecodER

 Mingzhen Sun \({}^{1,2}\) &Weining Wang \({}^{1}\) &Zihan Qin \({}^{1,2}\)

Jiahui Sun \({}^{1,2}\) &Sihan Chen \({}^{1,2}\) &Jing Liu \({}^{1,2,*}\)

\({}^{1}\)Institute of Automation, Chinese Academy of Sciences (CASIA)

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS)

sunmingzhen2020@ia.ac.cn weining.wang@nlpr.ia.ac.cn qinzihan2021@ia.ac.cn sunjiahui19@mails.ucas.ac.cn sihan.chen@nlpr.ia.ac.cn jliu@nlpr.ia.ac.cn

###### Abstract

Video generation necessitates both global coherence and local realism. This work presents a novel non-autoregressive method GLOBER, which first generates global features to obtain comprehensive global guidance and then synthesizes video frames based on the global features to generate coherent videos. Specifically, we propose a video auto-encoder, where a video encoder encodes videos into global features, and a video decoder, built on a diffusion model, decodes the global features and synthesizes video frames in a non-autoregressive manner. To achieve maximum flexibility, our video decoder perceives temporal information through normalized frame indexes, which enables it to synthesize arbitrary sub video clips with predetermined starting and ending frame indexes. Moreover, a novel adversarial loss is introduced to improve the global coherence and local realism between the synthesized video frames. Finally, we employ a diffusion-based video generator to fit the global features outputted by the video encoder for video generation. Extensive experimental results demonstrate the effectiveness and efficiency of our proposed method1, and new state-of-the-art results have been achieved on multiple benchmarks.

## 1 Introduction

When producing a real video, it is customary to establish the overall information (global guidance), such as scene layout or character actions and appearances, before filming the details (local characteristics) that draw up each video frame. The global guidance ensures a coherent storyline throughout the produced video, and the local characteristics provide the necessary details for each video frame. Similar to real video production, generative models for the video generation task must synthesize videos with coherent global storylines and realistic local characteristics. However, due to limited computational resources and the potential infinite number of video frames, how to achieve global coherence while maintaining local realism remains a significant challenge for video generation tasks.

Inspired by the remarkable performance of diffusion probabilistic models [1; 2; 3], researchers have developed a variety of diffusion-based methods for video generation. When generating multiple video frames, strategies of existing methods can be divided into two categories: autoregression and interpolation strategies. As illustrated in Fig. 1(a), the autoregression strategy [4; 5] first generates an initial video clip, and then employs the last few generated frames as conditions to synthesize subsequent video frames. The interpolation strategy [6; 7], depicted in in Fig. 1(b), generates keyframes first and then interpolates adjacent keyframes iteratively. Both strategies utilize generated video frames as conditions to guide the generation of subsequent video frames, enabling subsequentglobal storylines to be predicted from the context of the given frames. However, since the number of conditional video frames is limited by available computational resources and is generally small, these strategies have a relatively poor capacity to provide global guidance, resulting in suboptimal video consistency. In addition, the generation of local characteristics of subsequent frames refers to previous single frames, which can lead to error accumulation and distorted local characteristics .

In this paper, we present a novel non-autoregression method GLOBER, which first generates 2D global features to serve as global guidance, and then synthesizes local characteristics of video frames based on global features to obtain coherent video generation, as illustrated in Fig. 1(c). To be specific, we propose a video auto-encoder that involves a video encoder and a diffusion-based powerful video decoder. The video encoder encodes each input video into a 2D global feature. The video decoder decodes the storyline from the generated global feature and synthesizes necessary local characteristics for video frames. To achieve maximum flexibility, our video decoder is designed to involve no temporal modules and thus decode multiple video frames in a non-autoregressive manner. In particular, normalized frame indexes are integrated with generated global features to provide temporal information for the video decoder. In this way, we can obtain arbitrary sub video clips with predetermined starting and ending frame indexes. To leverage the success of image generation models, we initialize the video decoder with a pretrained image diffusion model . Then the video auto-encoder can be trained in a self-supervised manner with the target of video reconstruction. Furthermore, we propose a Coherence and Realism Adversarial (CRA) loss to improve the global coherence and local realism in the decoded video frames. For video generation, another diffusion model is used as the video generator to generate global features by fitting the output of video encoder.

Our contributions are summarized as follows: (1) We propose a novel non-autoregression strategy for video generation, which provides global guidance while ensuring realistic local characteristics. (2) We introduce a powerful video decoder that allows for parallel and flexible synthesis of video frames. (3) We propose a novel CRA loss to improve global coherence and local realism. (4) Experiments show that our proposed method obtains new state-of-the-art results on multiple benchmarks.

## 2 Related Work

Current video generation models can be divided into three categories: transformer-based methods that model discretized video signals, generative adversarial networks (GAN) and diffusion probabilistic models (DPM). The first category encodes videos to discrete video tokens and then models these tokens with transformers [9; 10; 11; 12; 13; 14]. MOSO  decomposes video scenes, objects,

Figure 1: Three strategies for multi-frame video generation. (a) Autoregression strategy first generates the starting video frames and then autoregressively predicts subsequent video frames. This approach is prone to accumulating errors , such as the fading of the hula hoop. (b) Interpolation strategy generates video keyframes first and then iteratively interpolates adjacent keyframes. This approach can result in suboptimal video consistency since it is unaware of the global content. (c) Our proposed non-autoregression strategy first generates a global feature to provide global guidance, and then synthesizes local characteristics of video frames in a non-autoregressive manner. \(c\) names the condition input of video description. \(x_{i}\) and \(I_{i}\) denote the \(i\)-th video frame and its index.

and their motions to cover video prediction, generation, and interpolation tasks in an autoregressive manner. TATS  follows the interpolation strategy, and introduces a time-agnostic VQGAN and a time-sensitive hierarchical transformer to capture long temporal dependencies

GAN-base methods [15; 16; 17] excel at generating videos in specific domains. MoCoGAN  proposes to decompose video content and motion by dividing the latent space. DIGAN  explores video encoding with implicit neural representations. StyleGAN-V  extends the image generation model StyleGAN  to the video generation task non-autoregressively. However, StyleGAN-V focuses on the division of content and motion noises and ignores the importance of global guidance. As a result, its content input (i.e., randomly sampled noise) contains limited information, whereas our global features can provide more valuable instructions.

DPM is a recently emerging method for vision generation [4; 19; 5; 20; 21]. VDM  is the first work that applies DPM to video generation by replacing all 2D convolutions with 3D convolutions. VIDM  generates videos in a frame-wise autoregression manner with two individual diffusion models. Both VDM and VIDM employ the autoregression strategy to obtain multiple video frames. In contrast, NUWA-XL  adopts the interpolation strategy but requires significant computational resources to support parallel inference. VideoFusion  targets at dividing the shared and residual video signals of different video frames and modeling them respectively. It employs a non-temporal diffusion model to synthesize video frames based on their indexes within the pixel space. PVDM  utilizes 3D-to-2D projection to encode a video into three 2D latent features for efficient video generation. In PVDM, the video decoder reconstructs video frames with fixed length (i.e. 16 frames) and fixed interval (i.e. predefined FPS). LVDM  encodes videos using 3D CNN through both spatial and temporal downsampling. Compared with previous methods, our method encompass the following advantages. Firstly, GLOBER can take advantage of the powerful generative capability of pretrained image diffusion models (e.g. stable diffusion) to synthesize reconstructed video frames, thus requiring a much smaller dimension of latent features to represent videos. Secondly, GLOBER is more flexible than prior works when decoding video frames from video latents. The video decoder in GLOBER can decode arbitrary video frames without length or interval limitations by taking the normalized indexes of target video frames as inputs. Thirdly, GLOBER is more efficient when training for video generation and synthesizing long videos. Fourthly, GLOBER obtains new state-of-the-art results on multiple benchmarks.

## 3 Method

In this section, we present our proposed method in details. The overall framework of our method is depicted in Fig. 2. A video sample is represented as \(x R^{F H W C}\), where \(F\) denotes the number of video frames, \(H\) is the height, \(W\) is the width, and \(C\) is the number of channels. The \(i\)-th video frame is denoted as \(x_{i} R^{H W C}\).

### Video Auto-Encoder

The video auto-encoder comprises a video encoder and a video decoder. To reduce the computational complexity involved in modeling videos, an auxiliary image auto-encoder, which is known as KL-VAE  and has been validated in previous studies [8; 6], is employed to encode each video frame individually into a low-resolution feature. The video encoder takes video keyframes as input and encodes them into 2D global features. Then the video decoder is used to synthesize each video frame based on the corresponding frame index and the global feature.

Frame EncodingVAEs [25; 26; 27; 8] are widely used models that reduce search space for vision generation. For high-resolution image generation, VAEs typically encode each image into a latent feature and then decode it back to the original input image [10; 27; 28; 29]. To reduce the spatial details of videos in a similar way, we employ a pretrained KL-VAE  to encode each video frame individually. Specifically, the KL-VAE downsamples each video frame \(x_{i}\) by a factor of \(f_{frame}\), obtaining a frame latent feature \(z_{i} R^{H^{} W^{} C^{}}\), where \(H^{}\) and \(W^{}\) are \(}\) and \(}\), respectively, and \(C^{}\) represents the number of feature channels.

Video EncodingAs illustrated in Fig. 2, the video encoder is composed of an embedding feature \(e_{v}\), an input layer, a downsample module with a downsample factor of \(f_{video}\), a mapping module, and an output module. The embedding feature \(e_{v} R^{H^{} W^{} C^{}}\) is randomly initialized and optimized jointly with the entire model. It should be noted that the temporal dimensions of the embedding feature and each frame feature are 1, which are omitted here for brevity. Since content redundancy exists between adjacent video frames , we select \(K\) keyframes from each input video at equal intervals and encode them individually using KL-VAE. This process produces the corresponding frame features \(z_{q_{1}},z_{q_{2}},...,z_{q_{K}}\), where \(q_{k}\) denotes the index of the \(k\)-th selected keyframe. Then, we concatenate the embedding feature \(e_{v}\) with keyframe features along the temporal dimension, obtaining the input feature \([e_{v}:z_{q_{1}}:...:z_{q_{K}}] R^{(K+1) H^{} W^{}  C^{}}\) for the video encoder.

The input layer employs a simple 2D spatial convolution with a kernel size of 3 to expand the available channel maps from \(C^{}\) to \(D\), where \(D\) represents the number of new channel maps. After the input layer, the downsample module processes these keyframe features to capture spatial and temporal information, while the mapping module follows. Specifically, the downsample module is composed of a residual layer, a spatial attention layer, a temporal attention layer, and a downsample convolution, while the mapping module follows a similar structure but replaces the downsample convolution with a temporal split operation. Finally, the output module comprises two spatial convolutions, a group normalization layer, and a SiLU activation function. It takes only the embedding part of the transformed feature as input and produces the mean and standard deviation (std) features of the global feature. Then the global feature can be sampled using the following equation:

\[v=v_{mean}+v_{std}*n\] (1)

where \(n\) is sampled from an isotropic Gauss distribution \((0,)\), \(v_{mean},v_{std} R^{H^{} W^{} C^{ }}\) are the mean and std features, \(H^{}\) and \(W^{}\) are \(}{f_{video}}\) and \(}{f_{video}}\) respectively, and \(C^{}\) is the number of channels. Considering that we are going to model the global features for generation using a

Figure 2: The overall framework of our proposed method GLOBER. During training, the video encoder and decoder are optimized jointly, with the video encoder encoding videos into global features, which are used by the video decoder to synthesize two randomly sampled video frames based on their corresponding frame indexes. We do not draw up the processing of timesteps and video descriptions in the video decoder for conciseness. The synthesized video frames are evaluated by the video discriminator for global coherence and local realism. Then the video generator is trained to generate global features by fitting the outputs of the video encoder. During generation, the video generator generates a novel global feature, which is then decoded by the video decoder to synthesize video frames in a non-autoregressive manner.

diffusion-based model, which will be specified in Sec. 3.2, an additional kl loss  is employed to force the distribution of global features towards an isotropic Gauss distribution:

\[_{kl}=(v_{mean}^{2}+v_{std}^{2}- v_{std}^{2}-1)\] (2)

Video DecodingWe view the synthesis of video frames as a conditional diffusion-based generation task. As illustrated in Fig. 2, we employ UNet  as the backbone of the video decoder, which can be structured into the downsample, mapping, and upsample modules. Each module starts with a residual block and a spatial attention layer, while the downsample and upsample modules additionally include downsample and upsample convolutions respectively. Following , each frame feature \(z_{i}^{0}\) (i.e. \(z_{i}\)) is corrupted by \(T\) steps during the forward diffusion process using the transition kernel:

\[q(z_{i}^{t}|z_{i}^{t-1})=(z_{i}^{t};} z_{i}^{t-1},_{t})\] (3) \[q(z_{i}^{t}|z_{i}^{0})=(z_{i}^{t}; _{t}}z_{i}^{0},(1-_{t}))\] (4)

where \(\{_{t}(0,1)\}_{t=1}^{T}\) is a set of hyper-parameters, \(_{t}=1-_{t}\) and \(_{t}=_{i=1}^{t}_{i}\). Based on Eq. (4), we can obtain the corrupted feature \(z_{i}^{t}\) directly given the timestep \(t\) as follows:

\[z_{i}^{t}=_{t}}z_{i}^{0}+(1-_{t})n_{t}\] (5)

where \(n_{t}\) is a noise feature sampled from an isotropic Gauss distribution \((0,)\). The reverse diffusion process \(q(z_{i}^{t-1}|z_{i}^{t},z_{i}^{0})\) has a traceable distribution:

\[q(z_{i}^{t-1}|z_{i}^{t},z_{i}^{0})=(z_{i}^{t-1}|_{t}(z_ {i}^{t},z_{i}^{0}),_{t})\] (6)

where \(_{t}(z_{i}^{t},z_{i}^{0})=}}(z_{i}^{t}- }{_{t}}}n_{t})\), \(n_{t}(0,)\), and \(_{t}=_{t-1}}{1-_{t}}_{t}\).

Given that the added noise \(n_{t}\) in the \(t\)-th step is the only unknown term in Eq. (6), we train the UNet to predict \(n_{t}\) from \(z_{i}^{t}\) condition on the timestep \(t\), the global feature \(v\), the frame index \(i\), and the video description \(c\) if exists. To achieve this, a timestep and an index embedding layers, additional modules, and a text encoder are employed to process these condition inputs. In particular, the timestep embedding layer first obtain the sinusoidal position encoding  of the diffusion timestep \(t\) and then passes it through two linear functions with a SiLU activation interposed between them. Following previous works [30; 32], the timestep embedding is integrated with intermediate features by the residual block in each UNet module. The index embedding layer embeds the frame index in a similar way with the timestep embedding layer. The additional modules (i.e. downsample, mapping, and upsample modules) are utilized to incorporate the index embedding with the global feature and to extract multi-scale representations of the corresponding video frame. These representations are then added to the outputs of UNet modules after zero-convolution. When encoding video descriptions into text embeddings, a pretrained model, namely CLIP , is utilized as the text encoder. During attention calculation, these text embeddings are concatenated with flattened frame features to provide cross-modal instructions. Finally, the L2 distance between the predicted noise \(n(z_{i}^{t},t,i,v,c)\) and the added noise \(n_{t}\) is calculated as the training target:

\[_{rec}=\|n(z_{i}^{t},t,i,v,c)-n_{t}\|_{2}\] (7)

where \(\|*\|_{2}\) denotes the calculation of the L2 distance. The total training loss is:

\[=_{rec}+_{1}_{kl}+_{2} _{cra}^{G}\] (8)

where \(_{1}\) and \(_{2}\) are hyper-parameters and \(_{cra}^{G}\) is specified in the following paragraph. During generation, the video decoder synthesizes video frames given content features, frame indexes and video descriptions (if exist) by denoising noise features with multiple steps as in [30; 34].

Coherence and Realism Adversarial LossWe propose a novel Coherence and Realism Adversarial (CRA) loss to improve global coherence and local realism of synthesized video frames. To reduce computation complexity, we randomly synthesize two video frames with indexes \(i\) and \(j\), where \(0 i<j F\), using the video decoder. The synthesized video frame \(_{i}\) can be decoded by KL-VAE from the frame feature \(_{i}\), which is obtained directly in each training step with the following formulation:

\[_{i}^{0}=_{t}}}z_{i}^{t}-_{t})}{_{t}}}n(z_{i}^{t},t,i,v,c)\] (9)where \(n(z_{i}^{t},t,i,v,c)\) is the predicted noise of \(n_{t}\). Then the CRA loss is calculated given the synthesized and real video frames using a video discriminator as depicted in Fig. 2.

In our case, we expect the discriminator to provide supervision on global coherence and local realism of synthesized video frames. To ensure the effectiveness of the CRA loss, we formulate it based on the following two guiding principles: (1) the real samples selected for the discriminator must exhibit the desired correlations, whereas the fake samples must deviate from the expected patterns and are often challenging to distinguish; (2) the real samples selected for the video auto-encoder (i.e. the generator) should serve as the fake samples for the discriminator for adversarial training. In particular, to make the discriminator aware of local realism, we select \( x_{i},x_{j}\) as the real sample and \( x_{i},_{j}\), \(_{i},x_{j}\), \(_{i},_{j}\) as fake samples according to the first principle. This is because the latter samples contain at least one synthesized video frame and violate the target pattern of realism. Furthermore, to ensure that the discriminator is aware of the global coherence, we utilize samples that violate temporal relationships like \( x_{j},x_{i}\) and \(_{j},_{i}\) as fake samples of the discriminator. Then, according to the second principle, \(\{_{i},_{j},\)\( x_{i},_{j},\)\(_{i},x_{j}\}\) are chosen as real samples of the video auto-encoder, since these samples contain at least one synthesized video frames. The CRA loss can be formulated as \(_{cra}^{D}\) for the discriminator and \(_{cra}^{G}\) for the generator:

\[_{cra}^{D}=& log(1-(  x_{i},x_{j}))+log( x_{i},_{j})+ log(_{i},x_{j})\\ &+log(_{i},_{j})+log ( x_{j},x_{i})+log(_{j}, {x}_{i})\\ _{cra}^{G}=& log(1-( _{i},_{j}))+log(1-(_{i},x_{j} ))+log(1-( x_{i},_{j}))\] (10)

As illustrated in Fig. 2, the discriminator is built on several spatio-temporal modules that consist of residual blocks and spatio-temporal full attentions. Considering that traditional attention is invariant to the order of input features, two position embeddings \(e_{0}\) and \(e_{1}\) are employed and added to the previous video frame \(x_{i}\) and the subsequent video frame \(x_{j}\) respectively with \(0 i<j F\). These position embeddings are randomly initialized and optimized at the same time with the discriminator.

### Generative Model

As shown in Fig. 2, we can obtain any desired video clip by feeding the frame indexes, global feature, and corresponding video description (if available) to the video decoder. Since the global feature is the only unknown term, we can train a conditional generative model to generate a global feature based on the video description. Considering that global feature are 2D features, the generative model can be relieved from the burden of modeling intricate video details.

In practice, global features typically have a small spatial resolution, and we utilize a transformer-based diffusion model, DiT , to generate them. Specificcally, each 2D global feature \(v R^{H^{} W^{} C}\) is flattened to a 1D feature with length \(H^{} W^{}\). Then the diffusion and reverse diffusion processes of \(v_{l}\) are similar to the procedures outlined in Eq. (3) and Eq. (6), except that the only condition is the video description (if exists) in the training and generation processes.

## 4 Experiments

In this section, we first introduce the experimental setups in Sec. 4.1. Following this, in Sec. 4.2 and Sec. 4.3, we compare the quantitative and qualitative performance of our method and prior methods on four challenging benchmarks: Sky Time-lapse , TaiChi-HD , UCF-101 , and Webvid-10M . Finally, Sec. 4.4 presents the results of ablation studies conducted to analyze the necessity of the CRA loss, and Sec. 4.5 explores the influence of global feature shape on the generation performance.

### Experimental Setups

All experiments are implemented using PyTorch  and conducted on 8 NVIDIA A100 GPUs, with 16-precision adopted for fast training. During the training of the video auto-encoder, pretrained KL-VAEs  were utilized to encode each video frame \(x_{i}\) into a latent feature \(z_{i}\), with a downsample factor of \(f_{frame}=8\) for \(256^{2}\) resolution and \(f_{frame}=4\) for \(128^{2}\) resolution. The latent features were then of resolution \(32^{2}\). The video encoder subsequently encoded the latent features of video keyframes, extracted at fixed frame indexes of \(\) for 16-frame videos, with a downsamplefactor of \(f_{video}=2\) and a number of output channels of \(C^{}=16\), resulting in global features of shape \(16 16 16\). The dimension of \(C^{}\) is 4 for 256x256 resolution and 3 for 128x128 resolution. We first train the video auto-encoder as well as the discriminator jointly until convergence, and then train DiT with the parameters of the video auto-encoder being fixed. The video auto-encoder was trained with a batch size of 40 per GPU for 80K, 40K, and 40K steps on the UCF101, TaiChi-HD, and Sky Time-lapse datasets, respectively. The loss weight \(_{1}\) and \(_{2}\) are set as 1e-6 and 0.1, respectively. When training the video generator, a Transformer-based diffusion model, DiT , was used as the backbone. The batch size was set to 32 per GPU, and the number of training iterations was 200K, 100K, and 100K for the UCF101, TaiChi-HD, and Sky Time-lapse datasets, respectively. When generating videos, we sample each global feature with the number of DDIM steps being 50 and the unconditional guidance scale being 9.0 expect otherwise specified. Then the video decoder synthesizes target video frames parallelly with the number of DDIM steps being 50 and the unconditional guidance scale being 3.0 for the Sky Time-lapse and TaiChi-HD datasets and 6.0 for the UCF101 dataset.

### Quantitative Comparison

Generation QualityTable 1 and Table 2 reports the results of our model trained on the Sky Time-lapse, TaiChi-HD, UCF-101, and Webvid-10M datasets for 16-frame video generation in both unconditional and conditional settings. As shown in Table 1(a), Table 1(b), and Table 1(c), our method achieves comparable performance with prior state-of-the-art models on the Sky Time-lapse, TaiChi-HD, and Webvid-10M datasets, respectively. To comprehensively compare the performance

Table 1: Quantitative comparison against prior methods on UCF-101, Sky Time-lapse and TaiChi-HD datasets for video generation. \(c\) denotes the condition of video descriptions.

Table 2: Quantitative comparison against prior methods on UCF-101 for unconditional video generation with \(256^{2}\) resolution.

on the more challenging UCF-101 dataset, we conduct experiments on both unconditional and conditional video generation at resolutions of \(128^{2}\) and \(256^{2}\). For unconditional video generation, we use the fixed textual prompt "A video of a person" as the video description, while for conditional video generation, we use the class name of each video as the video description. Our proposed method significantly outperforms previous state-of-the-art models, as shown in Table 1(d), Table 1(e), and Table 2. The superior performance of our method can be attributed to two factors. Firstly, we explicitly separate the generation of global guidance from the synthesis of frame-wise local characteristics. As global features are 2D features with relatively small spatial resolution, our generative model can model them effectively and efficiently without imposing a high computational burden. Secondly, the synthesis of frame details can refer to the global feature, which provides global guidance such as scene layout, object appearance, and overall behaviours, making it easier for our method to achieve global-coherent and local-realistic video generation.

Generation SpeedWe quantitatively compare the generation speed among various video generation methods and report the results in Table 3. Our GLOBER method demonstrates remarkable efficiency in generating video frames, thanks to its utilization of the non-autoregression strategy. In contrast, prior methods such as VIDM and VDM, which follow the auto-regression strategy, maintain unchanged memory but require significantly more time for generation. VideoFusion also adopts a non-autoregressive strategy for generating multiple video frames. However, VideoFusion remains slower than our GLOBER, since VideoFusion directly models frame pixels, which brings huge computational burden when generating multiple video frames. TATS employs the interpolation strategy by first generating video keyframes and then interpolating 3 frames between adjacent keyframes. However, it involves autoregressive generating of video keyframes and the interpolation process is not parallelly implemented, thus being slower than our GLOBER. Notably, GPU memory of VideoFusion and our GLOBER increases with the number of video frames due to the parallel synthesis of all video frames.

### Qualitative Comparison

As depicted in Figure 3, we conduct a qualitative comparison of our method with previous approaches on the UCF-101, Sky Time-lapse, and TaiChi-HD datasets. Samples of prior methods are obtained from [22; 19]. The UCF-101 dataset records 101 human actions and is the most challenging and diverse dataset. When applied to this dataset, GAN-based methods such as DIGAN and StyleGAN-V generate video samples that lack distinctiveness. In contrast, TATS, which is built on Transformer and utilizes the interpolation strategy, generates video samples that are more identifiable. In comparison to TATS, diffusion-based methods like VideoFusion  and VIDM  produce samples with more pronounced appearances. However, VideoFusion generates slightly blurred object appearances, and VIDM generates overly trivial object motions. Conversely, our GLOBER generates samples that exhibit both distinct appearances and conspicuous movements. On the Sky Time-lapse dataset, samples generated by DIGAN, StyleGAN-V, and TATS display trivial motions and simplistic objects. VideoFusion and VIDM generate samples with enhanced details and more discernible boundaries, while their motion remains somewhat negligible. In contrast, our GLOBER generates video samples with more dynamic movements and significantly richer visual details. Similarly, on the TaiChi-HD dataset, the human appearances generated by DIGAN, StyleGAN-V, and TATS are noticeably

   Method & VIDM & VIDM & VIDM & VPDM & VideoFusion & TATS & ModelScope & GLOBER \\  &  &  &  &  &  &  & (ours) \\  NAR & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ & ✓ \\ Video Encoding & ✗ & ✗ & ✓ & ✓ & ✗ & ✓ & ✓ \\ NUM\({}_{F=16}\) & - & - & 12288 & 8192 & - & **4096** & 16384 & **4096** \\ NUM\({}_{F=228}\) & - & - & 98304 & 65536 & - & 32768 & 131072 & **4096** \\  Diffusion Steps & 100 & 100 & 50 & - & 50 & - & 50 & 50+50 \\ \(F=16\) & 192s/20G & 125s/11G & 75s/9G & - & 22s/7G & **6s**/16G & 31s/6G & **6s**/7G \\ \(F=32\) & 375s/20G & 234s/11G & 141s/13G & - & 39s/9G & 26s/16G & 48s/8G & **11s**/11G \\ \(F=64\) & 771s/20G & 329s/11G & 288s/20G & - & 76s/13G & 65s/16G & 82s/12G & **21s**/19G \\   

Table 3: Comparison of sampling time/memory using different methods for generating multiple video frames with resolution of \(256^{2}\), batch size of 1, diffusion steps of default settings, and comparable GPU memory on a v100 GPU. \(F\) represents the number of video frames. AR denotes autoregression, IP denotes interpolation and NAR denotes non-autoregression.

distorted. While VideoFusion and VIDM achieve improved human appearances, their movements remain trivial. In contrast, samples generated by our GLOBER exhibit significant movements and distinct object appearances.

### Ablation Study on the CRA Loss

To investigate the effectiveness of our proposed CRA loss, we conducted an ablation study where the CRA loss was removed during the training of the video auto-encoder. The quantitative results are presented in Table 4, which unequivocally demonstrate the effectiveness of our CRA loss on all three benchmarks: UCF-101, Sky Time-lapse, and TaiChi-HD, with a remarkable reduction in FVD scores by 37.0, 20.8, and 31.2, respectively. In addition, we qualitatively compare videos synthesized by models with or without the CRA loss in Fig. 4, which clearly shows that videos generated using the CRA loss exhibit better video consistency and more distinct local details, demonstrating the necessity of our proposed CRA loss. The effectiveness of our CRA loss can be attributed to two key factors. Firstly, by penalizing samples that violate temporal relationships, the CRA loss enhances the overall coherence of the synthesized video frames. Secondly, by utilizing pairwise video frames as inputs for

   Datasets & CRA loss & rFVD \\   & w/o & 106.7 \\   & w/ & **69.7** \\   & w/o & 84.3 \\   & w/ & **63.5** \\   & w/o & 91.3 \\   & w/ & **60.1** \\   

Table 4: Ablation study on the CRA loss.

Figure 4: Visualization of samples synthesized with or without the CRA loss by the video auto-encoder.

Figure 3: Quantitative comparison against previous methods on the UCF-101 (left), Sky Time-lapse (middle) and TaiChi-HD (right) datasets. Our showcased samples on the UCF-101 datasets are produced using the video descriptions ”Apply Lipstick”, ”Soccer Juggling”, and ”Pull Ups”, respectively. Samples on the Sky Time-lapse and TaiChi-HD datasets are generated using fixed video descriptions being ”A time-lapse video of sky” and ”Tai chi”, respectively.

the discriminator, the CRA loss can identify and penalize video frames that exhibit inconsistent or distorted local characteristics, which further enhances the realism of the synthesized videos.

### Impact on the Global Feature Shapes

We experiment the effect of different shapes of the global features on the quality of synthesized video frames. As reported in Table 5, videos generated using global features with a shape of \(8 8 64\) exhibit lower quality than those generated using global features with a shape of \(16 16 16\). And the use of global features with a shape of \(16 16 32\) brings a further improvement of 39.2 in FVD score. It may be due to two reasons. Firstly, our experiments are conducted on the UCF-101 dataset, which contains complex motions that require more channels to be effectively represented. Secondly, by utilizing the KL loss to constrain the distribution of global features toward an isotropic Gaussian, it becomes easier for the generative model to fit the distribution of the global features even with a number of channels, thus increasing the number of channels could bring improved performance.

## 5 Conclusion and Limitations

This paper introduces GLOBER, a novel diffusion-based video generation method that emphasizes the significance of global guidance in multi-frame video generation. Our method offers three distinct advantages. Firstly, it alleviates the computation burden of modeling video generation by replacing 3D video signals with 2D global features. Secondly, it utilizes a non-autoregressive generation strategy, enabling the efficient synthesis of multiple video frames and surpassing the performance of prior methods in terms of efficiency. Lastly, by incorporating global features as guidance, our method generates videos with enhanced coherence and realism, achieving new state-of-the-art results on multiple benchmarks. Nevertheless, our research exhibits several limitations. Firstly, we find GLOBER difficult to process videos with frequent scene changes, as such transitions have the potential to disrupt the video coherence. Furthermore, we have not explored the performance of GLOBER in open-domain video generation tasks due to computational resource constraints. Future works are encouraged to solve the above issues.

## 6 Acknowledgements

This work was supported by the National Key Research and Development Program of China (No.2022ZD0118801), National NaturalScience Foundation of China (U21B2043,62102416, 62206279, 62102419).