# Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods

Tobit Klug, Dogukan Atik, Reinhard Heckel

School of Computation, Information and Technology

Technical University of Munich

tobit.klug, dogukan.atik, reinhard.heckel {@tum.de}

###### Abstract

Supervised training of deep neural networks on pairs of clean image and noisy measurement achieves state-of-the-art performance for many image reconstruction tasks, but such training pairs are difficult to collect. Self-supervised methods enable training based on noisy measurements only, without clean images. In this work, we investigate the cost of self-supervised training in terms of sample complexity for a class of self-supervised methods that enable the computation of unbiased estimates of gradients of the supervised loss, including noise2noise methods. We analytically show that a model trained with such self-supervised training is as good as the same model trained in a supervised fashion, but self-supervised training requires more examples than supervised training. We then study self-supervised denoising and accelerated MRI empirically and characterize the cost of self-supervised training in terms of the number of additional samples required, and find that the performance gap between self-supervised and supervised training vanishes as a function of the training examples, at a problem-dependent rate, as predicted by our theory.

## 1 Introduction

Deep neural networks trained in a supervised fashion to map a noisy measurement to a clean image achieve state-of-the-art performance for image reconstruction tasks including image denoising , image super-resolution  and accelerated magnetic resonance imaging (MRI) .

However, collecting clean training images is sometimes not possible, and is often expensive and time-consuming. For example, to collect clean target images for denoising is difficult since a sensor in a camera only collects noisy images . To collect target images for MRI is difficult as it requires acquiring fully sampled data with long scan times which, introduces difficulties like increased motion artifacts.

This has motivated research on self-supervised methods that enable the training of neural networks from noisy measurements of images only. Self-supervised approaches are generally based on constructing a self-supervised loss. For example, noise2noise  constructs a self-supervised loss based on a noisy image and a second noisy observation of the same image. Noisier2noise , noise2self , and neighbor2neighbor  construct losses based on a single noisy image for denoising. One might expect that a model trained with a self-supervised loss computed from noisy images performs worse than the same model trained in a supervised manner. And indeed, for image reconstruction problems a model trained with many of the self-supervised losses (in particular noisier2noise, noise2self, and neighbor2neighbor) performs worse than a model trained with a supervised loss, even when abundant training data is available.

However, with infinite training data, noise2noise  self-supervised training promises to achieve the performance of supervised training, since the noise2noise loss enables the computation of unbiased estimates of the gradients of the supervised loss.

In this paper, we study a class of self-supervised methods including noise2noise based on constructing unbiased estimates of the gradients of the supervised loss. We characterize the cost of self-supervised training in terms of the number of training examples required to achieve the same performance as supervised training. Our contributions are:

* **Finite sample theory.** We start by viewing the noise2noise loss as enabling computation of unbiased estimates of the gradients of the supervised loss. Based on this view, we characterize the sample complexity of noise2noise like self-supervised training. We find that the risk of a method trained in a noise2noise-like fashion as a function of training examples approaches the optimal risk at the same rate as supervised training (i.e., at rate \(1/N\), where \(N\) is the number of training examples). However, to reach the same performance as with supervised training more training examples are required. The amount of extra training examples required is a function of the self-supervised loss and of the reconstruction problem.
* **Empirical sample complexity of self-supervised denoising.** We empirically characterize the performance of noise2noise self-supervised training for Gaussian and camera-noise image denoising as a function of the training examples, and find that, as predicted by theory, once the training set size becomes sufficiently large noise2noise training yields a denoiser essentially on par with supervised training. For Gaussian denoising, we also characterize the performance of noisier2noise and neighbor2neighbor like training as a function of the number of training examples, and find, again as expected, models trained with a noisier2noise and neighbor2neighbor self-supervised loss perform worse than models trained in a supervised manner, even when abundant training data is available.
* **Empirical sample complexity of self-supervised compressive sensing.** Finally, we characterize the performance of noise2noise-like self-supervised training similar to the approaches from Yaman et al.  and Millard and Chiew  for compressive sensing reconstruction as a function of the training examples. Again the performance gap between models trained with a self-supervised and supervised loss goes to zero as a function of the training examples at a problem dependent rate. Perhaps surprisingly, the performance gap is vanishing already for small training set sizes because the gradients constructed from the self-supervised loss have a similar variance than the gradients constructed from the supervised loss.

Together, our results show that networks trained with a self-supervised loss based on computing unbiased estimates of the gradients of the supervised loss perform as well as model trained in a supervised fashion at the cost of additional training examples required.

Related work.We consider self-supervised learning methods since they enable training neural networks for problems where clean examples are not available or are scarce. We discuss the most related works throughout the paper. Another training technique that is useful when clean data is scarce are data augmentation techniques [7; 10]. Another class of methods that is interesting for the regime, where no clean data is available are zero-shot methods that are not based on any data, such as deep-image-prior based methods [40; 15; 14; 6]. There is also a variety of zero shot methods that rely partly on self-supervised loss functions, such as Yaman et al. 's method for compressive sensing, noise2fast , and zero-shot noise2noise .

## 2 Background on self-supervised learning based on estimates of gradients of the supervised loss

Let \(f_{}^{m}^{n}\) be a neural network with parameters \(\) for estimating an image \(^{n}\) based on a measurement \(^{m}\). Our goal is to find a neural network \(f_{}\) that has a small risk

\[R()=_{(,)}[(f_{} (),)].\] (1)

Here, expectation is over the signal-measurement pairs \((,)\) (for example a clean image \(\) and a noisy measurement \(=+\)), and \(\) is a supervised loss, which we take as the mean-squared error.

Since the data distribution is unknown, we can't compute and minimize the risk. In supervised training, we approximate the risk with an empirical risk computed based on \(N\) pairs of training examples. This requires pairs of ground-truth signal \(\) and associated measurements \(\).

We consider self-supervised training with a self-supervised loss that, unlike the supervised loss, does not require ground-truth images. Instead, it depends on another, randomized measurement of the ground-truth image, denoted by \(^{}\). We are given \(N\) pairs of a randomized measurement and original measurement \((_{1}^{},_{1}),,(_{N}^{}, _{N})\), and train a network to reconstruct a clean image from the original measurement by minimizing

\[_{}()=_{i=1}^{N}_{ }(f_{}(_{i}),_{i}^{}).\] (2)

We consider loss functions \(_{}\), like the noise2noise loss discussed below, with the property that in expectation over the training data, a gradient of the self-supervised loss is also a gradient of the risk \(R()\), and thus with sufficiently many training examples, we expect a network trained with such a self-supervised loss to achieve the same performance as when trained in a supervised manner.

However, the 'noise' induced by relying on randomized measurements of the original image instead of relying on the original image increases the variance of the gradients computed from the self-supervised loss. Therefore, self-supervised training requires more training examples to yield a network on par with supervised training. Next, we discuss two self-supervised losses, one for denoising and one for compressive sensing in the context of MRI.

### Noise2noise loss for denoising

For denoising, our goal is to train a neural network \(f_{}\) to estimate an image \(\) from a noisy observation \(=+\), where \(\) is additive Gaussian, non-Gaussian, or even structured noise. Noise2noise assumes access to pairs of noisy observations \((_{1},_{1}^{}),,(_{N},_{N }^{})\), where \(_{i}=_{i}+_{i}\) and \(_{i}^{}=_{i}+_{i}\). Here, \(_{i}\) is zero-mean noise, independent of but not necessarily of the same distribution as the noise \(_{i}\). Lehtinen et al.  introduced the (noise2noise) self-supervised loss

\[_{}(f_{}(),^{})= \|f_{}()-^{}\|_{2}^{2},\] (3)

which aims at finding a network that predicts one noisy observation of an image based on another one, therefore the name noise2noise.

In expectation, a minimizer of the self-supervised loss is also a minimizer of the associated risk (1), as formalized by the proposition below. Thus, training with the self-supervised loss (3) with infinitely many training examples is as good as supervised training with infinitely many training examples. The proof is in Appendix A.

**Proposition 1**.: _Suppose that a signal \(\) and a corresponding measurement \(\) are drawn from a joint distribution, and let \(^{}=+\) be another randomized measurement of the signal. Assume that the noise \(\) is uncorrelated with the residual, i.e., \(_{(,,)}[(f_{} ()-)^{T}]=0, .\) Then, the minimizer \(\) of the self-supervised risk \(_{(,^{})}[\|f_{}()-^{}\|_{2}^{2}]\) is also a minimizer of the supervised risk \(_{(,)}[\|f_{}( )-\|_{2}^{2}]\)._

Noise2noise self-supervised training has been applied to many domains including biomedical imaging [5; 3; 19], channel estimation  or acoustic sensing .

### Compressive sensing

For accelerated MRI, our goal is to train a network \(f_{}\) to reconstruct an image \(\) from an undersampled measurement \(=\) in the frequency domain. Here, \(^{n n}\) is an undersampling mask (i.e., a diagonal matrix with zeros and ones on its diagonal), and \(^{n n}\) is the Fourier transform.

We do not have access to a ground-truth image \(\). Instead, we are given an undersampled measurement \(=\) as well as a second randomized measurement of the same image, obtained as \(^{}=^{}\), where \(^{}^{n n}\) is a randomized mask which is zero or one on its diagonal, and is one with non-zero probability, so that \(=[^{}]^{-1/2}\) exists. From this measurement, we can compute a self-supervised loss defined as

\[_{}(f_{}(),^{})=\| (^{}f_{}()- ^{})\|_{2}^{2},\] (4)

where \(=[^{}]^{-1/2}\) is a diagonal weighting mask. The loss is evaluated only on the frequencies that are given through the randomized mask \(^{}\) and are weighted by how often a frequency occurs in the randomized mask \(^{}\) (thus, the multiplication with the weighting mask \(\)).

Analogous as for the noise2noise loss for denoising, in expectation, a minimizer of the self-supervised loss is also a minimizer of the associated risk, as formalized by the following proposition. The proof is in Appendix A.

**Proposition 2**.: _Suppose that a signal \(\) and a corresponding measurement \(\) are drawn from some joint distribution (e.g., as \(=\), where \(\) is a mask), and let \(^{}=^{}\) be an independent measurement taken with a randomized mask \(^{}\) with 0's or 1's on it's diagonal and non-zero probability of a one on the diagonal. Then a minimizer \(\) of the self-supervised risk \(_{(,^{})}[\|(^{}f_{}()-^{})\|_{2}^{2}]\) with \(=[^{}]^{-1/2}\) is also a minimizer of the supervised risk \(_{(,)}[\|f_{}()- \|_{2}^{2}]\)._

A variety of works consider training a neural network with a self-supervised loss similar to (4) [24; 45; 42; 17; 16; 52; 29]. However, none of those works studied the sample complexity. Further, Lehtinen et al.  train with a noise2noise loss, but sample new masks \(\) and \(^{}\) in every training epoch, which requires access to fully sampled measurements. Similar to our setup, Yaman et al.  and Millard and Chiew  fix the set of undersampled measurements per image available for creating the masks \(\) and \(^{}\). However, Yaman et al.  and Millard and Chiew  construct a noiser2noise-like self-supervised loss, i.e., the input mask \(\) implements a higher undersampling factor on the training inputs than the undersampling factor used at inference. We assume the mask \(\) to have the same distribution at training and inference, which is important for Proposition 2 to hold.

## 3 Finite sample theory for self-supervised denoising

We start by studying the performance of a denoiser trained with a self-supervised noise2noise loss analytically in the finite sample regime. We measure performance in terms of the risk \(R()=[\|f_{}()- \|_{2}^{2}]\) and learn an estimator \(f_{}\) by minimizing the self-supervised objective function (2), where we take the noise2noise loss (3) as the self-supervised loss.

Finite-sample risk bound for linear denoising.We consider a simple linear denoising problem, where the joint distribution of the image and corresponding measurement is as follows. The signal \(\) is drawn from a \(d\)-dimensional linear subspace according to \(=\), where \(^{n d}\) is an orthonormal basis for the subspace, and where \((0,1/d)\). Thus, for large \(d\), the vector is drawn approximately uniformly from the intersection of the subspace with the unit sphere. We draw a measurement as \(=+\), where \((0,_{z}^{2}/n)\) is Gaussian noise, and then draw a second measurement as \(^{}=+\), where \((0,_{z}^{2}/n)\) is the target noise. With this scaling, the expected SNR of the denoising problem is \(1/_{z}^{2}\).

We consider a linear estimator of the form \(f_{}()=\). Note that the optimal linear estimator (i.e., the estimator that minimizes the risk) is \(^{*}=^{}}^{T}\). The optimal estimator projects the measurement onto the subspace and shrinks the projected measurement by a noise-dependent factor.

We provide a bound on the expected risk of the estimator that is learned by minimizing the self-supervised loss function (2) by running the stochastic gradient method for one epoch. Given a dataset \(=\{(_{1},_{1}^{}),\,,(_{N },_{N}^{})\}\) consisting of pairs of two noisy measurements \(_{i}=_{i}+_{i},_{i}^{}=_{i}+_{i}\) drawn i.i.d. from the distribution specified above, we start the stochastic gradient method at \(_{0}=0\) and update

\[_{k+1}=_{k}-_{k}_{}\| _{k}-_{k}^{}\|_{2}^{2},\]

for \(k=1,,N\). Here, \(_{k}\) is the stepsize.

**Theorem 1**.: _Consider the estimate \(_{N}\) obtained by running the SGM for \(N\) iterations on the training set \(\) with a decaying stepsize \(_{k}=\), where \(c\) is a constant. Then, the expected generalization error, where expectation is over the random training set \(\), obeys_

\[[R(_{N}))] R(^{*})+^{2}/n}{(_{z}^{2}/n)^{2}}(2+(12_{ z}^{2}+_{e}^{2}(1+_{z}^{2}))^{2}).\] (5)

The proof (detailed in Appendix B), is based on an standard convergence analysis of the stochastic gradient method [32; 43; 35]. In a nutshell, from the noise2noise self-supervised loss we compute stochastic gradients of the risk as \(_{}(f_{}(_{i}),_{i}^{ })\), where \((_{i},_{i}^{})\) is a training example. This stochastic gradient is an unbiased estimate of the gradient of the risk, \( R()\). The variance of the stochastic gradient determines the rate on the RHS of equation (5).

Theorem 1 establishes that the risk is upper bounded by a problem-dependent noise floor, which is the risk of the optimal estimator, \(R(^{*})\), plus a term associated with minimizing a self-supervised loss constructed from finitely many training examples. This term decays as \(c/N\), which is exactly the rate we see in the simulations below for this setup. Moreover, the term associated with minimizing the self-supervised loss becomes larger in the noise variance \(_{}^{2}\), and thus reflects that the self-supervised loss is a worse approximation of the supervised loss, as \(_{e}^{2}\) increases.

Numerical results for linear denoising.Figure 1 shows the risk of the linear denoiser \(\) trained on \(N\) examples from the linear subspace model. The linear estimator is learned by applying gradient descent to the self-supervised loss function (2) regularized with early stopping. As predicted by Theorem 1, the performance of the linear estimator \(\) converges to the performance of the optimal estimator \(^{*}\) at the rate \(1/N\) (right plot in Figure 1). Also as predicted by the Theorem, larger levels of target noise \(_{e}\) require more training examples, reflected by the term that multiplies with \(1/N\) increasing in \(_{e}^{2}\).

## 4 Empirical results for self-supervised denoising

In this section, we study the performance of a network trained for denoising with the self-supervised objective function (2) and noise2noise loss (3) as a function of the number of training examples, and compare to the performance of the network trained in a supervised fashion. We consider Gaussian denoising and denosing real-world-camera noise.

Throughout this section, we focus on training a U-net  with 7.4M network parameters. A U-net is a good choice, since it is widely used and gives near SOTA models for image denoising [4; 12; 50; 49]. However, our qualitative results are independent of the network architecture. In the appendix, we present results for SwinIR , an attention-based SOTA architecture, confirming this.

Figure 1: **Subspace denoising.** Simulated risk of a linear estimator learned with early stopped gradient descent for different levels of target noise \(_{e}\), signal dimension \(d=10\), ambient dimension \(n=100\), and input noise level \(_{z}=0.1\). **Left:** The larger the target noise level, the more training examples are required for self-supervised training to approach the optimal risk \(R(^{*})\). **Right:** Plotting the risk minus the optimal risk reveals the convergence rate to be \(1/N\). Error bars show the standard deviation over 5 independent runs.

### Gaussian denoising

Setup.We consider denoising images \(\) from noisy measurements \(=+\) with noise level \(_{z}=25\) (for pixel values in \(\)). For the noisy targets \(^{}=+\), we study noise levels \(_{e}\{0,25,50\}\), where \(_{e}=0\) corresponds to supervised training. The target noise level determines how fast as a function of the training set size self-supervised approaches the performance of supervised training. In all our experiments, the measurement noise \(_{i}\) and target noise \(_{i}\) are only sampled once per clean image \(_{i}\) during training, which corresponds to the practical setup in which we are given two noisy measurements per image. We use cropped patches of size \(128 128\) from ImageNet  to create training sets with 100 to 300k images. We pick the best run out of different runs with independently sampled network initialization. See Appendix C.1.2 for training details.

Results and discussion.Figure 2 shows the performance in PSNR of U-nets trained in a supervised and self-supervised manner for different training set sizes \(N\). We find, as expected from the theory in Section 3, that the gap between noise2noise like self-supervised training and supervised training vanishes as a function of the training examples, at a rate that increases as \(_{e}\) decreases.

At 100 training images the performance gap between supervised and self-supervised training with \(_{e}=25\) (\(_{e}=50\)) is 0.645dB (1.497dB), and at 100k training images the gap is reduced to 0.097dB (0.277 dB). For this specific setup, we can read of the number of additional images required to meet a certain denoising performance relative to what can be achieved with supervised training. For example, to achieve the performance of supervised training on 3k images, self-supervised training with noise on the training targets \(_{e}=25\) (\(_{e}=50\)) requires 10k (30k) images (gray dashed line in Figure 2).

The theory in Section 3 is based on a convergence analysis of the stochastic gradient method, and the rate at which performance improves as a function of the number of training examples, \(N\), is determined by how well the stochastic gradients of the risk, \(_{}(f_{}(_{i}),_{i}^{ })\) (where \((_{i},_{i}^{})\) is a training example), approximate the gradient of the risk, i.e., \( R()\). In Figure 2 (right panel) we show the histograms of the normalized variance of stochastic gradients, i.e., normalized estimates of the MSE \(\|_{}(f_{}(_{i}),_{i }^{})- R()\|_{2}^{2}\) after one epoch of training; see Appendix E for details. It can be seen that the larger the variance of the stochastic gradients, the slower the improvement as a function of the training examples, \(N\).

Self-supervised training with noisier2noise and neighbor2neighbor in general does not yield networks as good as trained in a supervised fashion, even if abundant training data is available. Noisier2noise  and neighbor2neighbor  only require a single noisy measurement per clean image for training. However, noisier2noise adds additional noise to the training inputs, which introduces a mismatch between the problem solved during training and inference. Neighbor2neighbor relies on assumptions on the similarity between neighboring pixels in the clean image that only hold approximately in practice. In Figure 2 we see that noisier2noise and neighbor2neighbor training,

Figure 2: **Gaussian image denoising. Left:** Noise2noise training approaches the performance of supervised training as the number of training images increases at a rate dependent on the target noise level \(_{e}\). Networks trained with noisier2noise and neighbor2neighbor also improve as a function of the training examples, but are far from approaching the performance of networks trained in a supervised fashion. **Right:** Histogram of the variance of the stochastic gradients. The noisier the gradients, the slower the convergence as a function of the training examples, \(N\).

unlike noise2noise, yield methods that perform worse than methods trained in a supervised fashion even if abundant training data is available. For training details see Appendices C.1.3 and C.1.4.

For the results in Figure 2 we consider a U-net of fixed size. The performance of a network trained in a supervised and self-supervised manner depends on the network architecture and size. However, since we investigate training schemes, we expect our qualitative findings for relative performance differences to continue to hold for other architectures and network sizes. In Appendix C.1.1 we provide results for U-nets of varying sizes and in Appendix C.1.5 we provide results for SwinIR , a recent state-of-the-art network for image denoising. The results are as expected analogous to the ones for the U-net presented here.

### Real-world camera noise

We now study self-supervised denoising for real-world camera noise from the Smartphone Image Denoising Dataset (SIDD) . The noise in the SID dataset is structured and not Gaussian.

Setup.We train U-nets on increasing amounts of patches of size \(128 128\) from the training set, validate on the first 10 scenes, and report the test performance on the remaining 30 scenes from the validation set. The dataset consists of 150 noisy images per scene of which 2 are used as input and target for self-supervised training. Ground truth images for supervised training are estimated from all 150 noisy images. Training and testing is done in the raw-RGB space; see Appendix C.2 for details.

Results and discussion.In Figure 3 we see that performance improves steeper as a function of the training set size compared to Gaussian denoising in Figure 2, which we attribute to the high variety of noise levels and lighting conditions present in the SID dataset. However, analogous to the Gaussian case the model trained in a self-supervised manner closely approaches the performance of the model trained in a supervised manner as the number of training examples grows large.

## 5 Empirical results for self-supervised compressive sensing

We now study the performance as a function of the training set size of a network trained for compressive sensing reconstruction by minimizing the self-supervised objective (2) with the noise2noise-like loss (4), relative to the performance achieved by the same network with supervised training.

We perform two sets of experiments: First we experiment on natural images obtained from ImageNet, since this allows us to explore the compressive sensing problem over a wider range of training set sizes than existing medical image datasets allow. Then, we investigate accelerated MRI reconstruction on real world multi-coil measurements from the fastMRI dataset . We consider a U-net in the main body, but again our qualitative results are independent of the network, and we demonstrate this with experiments on the VarNet, a state-of-the-art architecture, in the appendix.

Figure 3: **Real-world camera noise denoising on SIDD. Left: Similar to our results for Gaussian denoising a model trained with noisy targets in a noise2noise manner approaches the performance of a model trained with ground truth targets in a supervised manner as the number of training patches gets large. Right: Histogram of the variance of the stochastic gradients. The stochastic gradients of the self-supervised loss are slightly noisier than the ones of the supervised loss.**

Sampling scheme.The setup from Section 2.2 requires a pair of source measurement \(\) (that can be arbitrarily obtained) and a target measurement that is sampled independently of the source measurement as \(^{}=^{}\), where \(^{}\) is a random mask with a non-zero probability of selecting each frequency. While in clinical MRI protocols, we can acquire two independently undersampled measurements per image, we typically acquire only one. We therefore slightly adapt our sampling scheme as follows to generate two measurements \(\) and \(^{}\) from one undersampled measurement \(}\) that is undersampled with factor \(\). See Figure 4 for an illustration of the scheme.

Our goal is to train a network to perform reconstruction based on a measurement \(\) undersampled with factor \(p<\). We consider Cartesian undersampling, where the frequency domain (also called k-space) is sampled column-wise. Since the low-frequency components of an image contain the largest portion of the signal energy, we follow the common practice to sample a fraction \(=0.08\) of the center columns in the k-space with probability 1, and assign the center frequencies to both measurements \(\) and \(^{}\). We then add a random fraction \(\) of the non-center columns in \(}\) to \(\), which is now a measurement undersampled with factor \(p\) by design. The target \(^{}\) is constructed by adding the remaining frequencies that are in \(}\) but not in \(\), and an additional fraction \(p^{}q\) of the non-center columns in \(\), where \(p^{}=\). The rationale behind this approach is that if the masks \(\) and \(^{}\) sample fractions \(p^{}\) and \(q\) independently from all non-center frequencies, then the expected overlap of frequencies selected by both masks is \(p^{}q\) with \(q=\).

We compute the diagonal weight mask from equation (4) as \(=[^{}]^{-1/2}\), which yields 1's at entries corresponding to center frequencies and \(1/\) otherwise. In our experiments we set \(=0.08\), \(p=0.25\) and \(\{0.28,0.33\}\) resulting in \(q\{0.05,0.11\}\).

### Compressive sensing for natural images

We start with compressive sensing experiments on natural images, because the largest existing MRI dataset for image reconstruction research (fastMRI) only has 55k images, and we want to vary the training set size far beyond that. In the next section we conduct experiments on fastMRI.

Setup.We use cropped images of size \(100 100\) from ImageNet  as ground truth images \(\) and obtain undersampled complex-valued measurements in the frequency domain as \(=\) and \(^{}=^{}\). We draw random training sets with 50 to 1M patches from a fixed pool of 1M images.

For all training set sizes, we train a U-net with about 1M network parameters and pick the best run out of different runs with independently sampled training set and network initialization. The network takes as input a coarse reconstruction obtained as \(^{-1}\), where \(\) is the zero-filled measurement (i.e., \(f_{}()=_{}(^{-1})\). The measurements are complex-valued, and we take one channel of the U-net as the imaginary part and one as the real part. The reconstructed image is obtained from the complex-valued network output by taking the absolute value. See Appendix D.1 for training details.

Figure 4: **Self-supervised training scheme for compressive sensing MRI. The overlap between constructed measurements \(,^{}\) is indicated in green and stems from both masks sampling center frequencies with probability 1 and an expected overlap of non-center frequencies of size \(p^{}q\). The scheme depicts single-coil measurements for conciseness.**

Results and discussion.Figure 5 shows the performance in SSIM as a function of the training set size for supervised and self-supervised training with fractions \(=0.33\) and \(=0.28\) of all frequencies available for self-supervised training. Note that the fraction of frequencies in the target plays the same role as the noise variance for noise2noise denoising: The larger the fraction, the better in general. Since we keep the undersampling factor \(p=0.25\) of the training inputs \(\) fixed, a smaller \(\) implies a smaller undersampling factor \(q\) (less frequencies) of the training targets \(^{}\).

Networks trained in a self-supervised manner with sufficiently many training examples perform as well as the same network trained in a supervised manner (but with fewer examples). The fewer measurements are available, i.e., the smaller \(\), the more training examples are required to achieve the same performance as the same network trained in a supervised manner. This parallels our theoretical and empirical findings for denoising where noise2noise training approaches the performance of supervised training as a function of the training set size and at a rate dependent of the training targets.

Figure 5, right panel, depicts histograms of the variance of the stochastic gradients. The noisier the gradients, the slower the performance increase as a function of the training examples.

The performance of the zero-filled baseline in Figure 5 is computed between the network inputs \(^{-1}\) and ground truth images \(\) in the test set. As we can see self-supervised training requires at least 250 training images to outperform this trivial baseline.

### Compressive sensing MRI

Next, we discuss the performance of noise2noise self-supervised and supervised training for multi-coil MRI reconstruction. MRI scanners use parallel imaging, where a single scan simultaneously collects measurements \(_{1},,_{C}\) of the same object with \(C\) coils, where \(_{j}=_{j}\) with complex valued images \(^{n}\) and diagonal coil specific sensitivity maps \(_{j}^{n n}\), which we estimate from the center of the k-space using ESPIRiT .

Setup.We conduct our experiments on a subset of the fastMRI brain dataset . We design fixed training sets with 50 to 50k images. We train U-nets with 31M network parameters. To demonstrate that our results for comparing supervised and self-supervised training schemes translate qualitatively to other network architectures, we further train end-to-end variational networks (E2E-VarNet) , a state-of-the-art architecture, of size 9M parameters. We pick the best run out of different runs with independently sampled network initialization. See Appendix D.2 for training details.

The network inputs are computed through SENSE-1 coil combination  of the coil measurements as \(_{j=1}^{C}_{j}^{*}^{-1}_{j}\), where \(_{j}^{*}\) denotes the complex conjugate of the sensitivity map. Ground truth images for evaluation are computed analogously as \(|_{j=1}^{C}_{j}^{*}^{-1}_{j,}|\), where \(_{j,}\) is the fully sampled coil k-space provided by the fastMRI dataset. To compute the training loss in the k-space between given and predicted coil measurements, the set of predicted coil measurements is obtained from the network output as \(_{1}f_{}(),,_{C}f_{ }()\).

Figure 5: **Compressive sensing for natural images.****Left:** The larger the fraction of frequencies in the target, \(\), the faster the network’s performance improves as a function of the number of training examples, \(N\). As the training set size becomes large, self-supervised performance approaches supervised performance. **Right:** Histogram of the variance of the stochastic gradients. The noisier the gradients (i.e., the smaller \(\)), the slower the convergence as a function of the training examples.

We follow Yaman et al.  and make best use of the given data by re-sampling the input mask \(\) in every epoch for supervised training. For self-supervised training the given frequencies \(}\) are fixed, but we can re-sample the split into measurements \(,^{}\). As the network sees more combinations of input/target pairs, the performance of supervised and self-supervised training improves.

Results and discussion.Figure 6 shows the performance of U-nets trained in a supervised and self-supervised way for multi-coil MRI as a function of the training set size. We report the performance in SSIM, since a high score in SSIM was shown to correlate well with a high rating by radiologists in the fastMRI challenge . We find that already for small training set sizes the performance gap between supervised and self-supervised training vanishes to approximately 0.002 in SSIM.

Figure 6, right panel, depicts the histogram of the variance of the stochastic gradients. The difference in variance of the supervised and self-supervised gradients is very small, which explains the small performance gap between supervised and self-supervised training for this setup. The performance of the zero-filled baseline in Figure 6 is computed between the magnitude of the network inputs \(|_{j=1}^{C}_{j}^{*}^{-1}_{j}|\) and ground truth images in the test set.

In Appendix D.2.2 we provide additional results for the E2E-VarNet. As expected, the VarNet outperforms the U-net by a significant margin for every training set size, especially for small training set sizes. However, the relative performance gap between VarNets trained in a supervised and self-supervised way is very small, analogous as for the U-net.

## 6 Conclusion

We characterized the sample complexity of deep learning based image reconstruction models trained with a class of self-supervised losses including noise2noise based on computing unbiased estimates of the gradients of the supervised loss. Our work shows that if sufficiently many training examples are available, it is as good to work with pairs of independent measurements as with training data with ground-truth images. In particular for accelerated MRI, training networks only based on undersampled data is sufficient for peak performance.

One advantage of supervised training over self-supervised training that is not reflected in our results is that supervised training has more flexibility in choosing the loss function, and choosing a loss function such as SSIM might give better visual image quality than training with the MSE. Our results of self-supervised training approaching supervised training pertain to training with the MSE.

We hasten to note that the class of methods that enable the computation of unbiased estimates of gradients of the supervised loss studied in this work, are in practice limited to problem setups in which obtaining two independent measurements of the same underlying signal is feasible.

ReproducibilityThe repository at https://github.com/MLI-lab/sample_complexity_ss_recon contains the code to reproduce all results in the main body of this paper.

Figure 6: **Compressive sensing MRI Left:** Already at small training set sizes the performance gap between supervised and self-supervised training vanishes. **Right:** The small performance gap on the left is explained by a small difference in the variances of the supervised and self-supervised gradients.