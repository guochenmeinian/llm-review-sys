# The Power of Hard Attention Transformers on Data Sequences: A Formal Language Theoretic Perspective

Pascal Bergstrafser

RPTU Kaiserslautern-Landau

67663 Kaiserslautern, Germany

bergstraesser@cs.uni-kl.de &Chris Kocher

MPI-SWS

67663 Kaiserslautern, Germany

ckoecher@mpi-sws.org &Anthony Widjaja Lin

MPI-SWS and RPTU Kaiserslautern-Landau

67663 Kaiserslautern, Germany

awlin@mpi-sws.org &Georg Zetzsche

MPI-SWS

67663 Kaiserslautern, Germany

georg@mpi-sws.org

###### Abstract

Formal language theory has recently been successfully employed to unravel the power of transformer encoders. This setting is primarily applicable in Natural Language Processing (NLP), as a token embedding function (where a bounded number of tokens is admitted) is first applied before feeding the input to the transformer. On certain kinds of data (e.g. time series), we want our transformers to be able to handle _arbitrary_ input sequences of numbers (or tuples thereof) without _a priori_ limiting the values of these numbers. In this paper, we initiate the study of the expressive power of transformer encoders on sequences of data (i.e. tuples of numbers). Our results indicate an increase in expressive power of hard attention transformers over data sequences, in stark contrast to the case of strings. In particular, we prove that Unique Hard Attention Transformers (UHAT) over inputs as data sequences no longer lie within the circuit complexity class AC0 (even without positional encodings), unlike the case of string inputs, but are still within the complexity class TC0 (even with positional encodings). Over strings, UHAT without positional encodings capture only regular languages. In contrast, we show that over data sequences UHAT can capture non-regular properties. Finally, we show that UHAT capture languages definable in an extension of linear temporal logic with unary numeric predicates and arithmetics.

## 1 Introduction

Recent years have witnessed the success of transformers  in different applications, including natural language processing , computer vision , speech recognition , and time series analysis . In the quest to better understand the ability and limitation of transformers, theoretical investigations have actively been undertaken in the last few years. Among others, _formal language theory_ has been successfully applied to reveal deep insights into the expressive power of transformers, e.g., see the recent survey  and . In particular, relevant questions pertain to the power of various attention mechanisms, bounded/unbounded precision, positional encoding functions, and interplay between encoders and decoders, among many others.

One common assumption in the formal language theoretic approach to transformers is that the input sequence ranges over a _finite_ set \(\) (called alphabet), which is then to be fed into a transformer after applying a token embedding function of the form \(f:^{d}\). As a by-product, the number of tokens is finite. In certain applications (e.g. time series forecasting ), we want our transformers tobe able to handle _arbitrary_ input sequences of numbers (or tuples thereof) without _a priori_ limiting the values of these numbers. Moreover, numbers could be compared using arithmetic and (in)equality, which is not the case for elements of alphabets considered in formal language theory. For this reason, we propose to investigate the expressive power of _transformers over data sequences_, which takes us to the setting of formal language theory over the alphabet \(=^{d}\), for some \(d_{>0}\), e.g., see . That is, _what properties of a sequence of (tuples of) numbers can be recognized by transformers?_

**Connections to circuit complexity.** Existing work has revealed intimate connections between transformers and circuit complexity. In particular, let us consider the following class of transformer encoders that has been the main focus of many recent papers: _Unique Hard Attention Transformers (UHAT)_. Among others, UHAT allows arbitrary positional encoding and an attention mechanism that picks a vector at a unique minimum position in the sequence that maximizes the attention score. It is known that the class of formal languages recognized by UHAT is a strict subset of the circuit complexity class \(^{0}\) (cf. ), i.e., each UHAT can be simulated by a family of boolean circuits of constant depth. More concretely, this entails among others that UHAT cannot compute the parity (even/oddness) of the number of occurrences of any given letter \(a\) in the input string (for strings over an alphabet containing at least two letters).

**Our first contribution** is that UHAT over data sequences (even without positional encodings) is _no longer_ contained in \(^{0}\), unlike the case of finite number of tokens. Instead, we show that UHAT can be captured by the circuit complexity class \(^{0}\), which extends \(^{0}\) circuits with majority gates.

**Theorem 1**.: _UHAT with positional encoding over data sequences is in \(^{0}\) but not in \(^{0}\)._

This complexity upper bound allows us to deduce the expressive power of UHAT over data sequences by using complexity theory. For example, since UHAT accepts only \(^{0}\) languages, successfully constructing a UHAT (e.g. through learning) for

\[:=\{(a_{1},b_{1}),,(a_{n},b_{n})|_{i=1}^{ n}}_{i=1}^{n}},(a_{i},b_{i})_{>0}_{>0},.\},\]

would constitute a major breakthrough in complexity theory (cf. ), i.e., showing that \(\) is in the complexity class \(^{0}/\). A byproduct of our proof is that for each length, the set of accepted sequences is a semialgebraic set. This implies, e.g., that the graph \(\{(x,e^{x}) x\}^{2}\) of \(x e^{x}\) (viewed as a set of length-\(1\) sequences) is not accepted by UHAT.

**Connection to regular languages over data sequences.** Recent results have revealed surprising connections between regular languages and formal languages recognizable by transformer encoders. In particular, it was proven (cf. ) that languages recognizable by UHAT (even _with no positional encodings_) form a strict subset of regular languages, namely those that are "star-free" or, equivalently, definable in First-Order Logic (FO), or Linear Temporal Logic (LTL). With positional encodings, similar connections hold, by extending the logics with _unary numerical predicates_ (cf. ).

To investigate whether such connections extend to data sequences, we bring forth _formal languages theory over infinite alphabets_ (cf., ), which has been an active research field in the last decade or so with applications to programming languages and databases (to name a few), e.g., see . **Our second contribution** is a language over data sequences recognizable by UHAT without positional encodings that lies beyond existing formal models over infinite alphabets (in particular, "regular" ones). This shows the strength of UHAT over data sequences even without positional encodings, in stark contrast to the case of finite alphabets.

**Theorem 2**.: _There is a non-regular language over \(=^{d}\) that is accepted by masked UHAT with no positional encoding._

Finally, to better understand languages over data sequences recognizable by UHAT, **our third contribution** is to show how UHAT can recognize languages definable by the so-called _Locally Testable LTL_ (\(()^{2}\)), which extends LTL with unary numerical predicates and _local arithmetic tests_ for fixed-size windows over the input sequence. Using \(()^{2}\), we can see _at a glance_ what can be expressed by UHAT over data sequences. For one, this includes the well-known _Simple Moving Averages_. As another example, using \(()^{2}\) it can be easily shown that UHAT can capture linear recurrence sequences considered in the famous Skolem problem and discrete linear dynamical systems , i.e., sequences of the form \(,A,,A^{n}\) such that \(n 0\) is minimal with \(A^{n}=0\) where \(^{1 d}\) and \(A^{d d}\) are fixed and \(^{d}\).

**Theorem 3**.: _Every \((LT)^{2}L\)-definable language is accepted by UHAT with positional encoding._

**Technical challenges.** Obtaining our results poses several challenges. First, for the TC\({}^{0}\) upper bound, we need to use Boolean (constant depth) circuits to simulate UHATs, in which real constants can occur (in affine transformations or positional encodings). While in TC\({}^{0}\), it is known that majority gates can be used to perform multiplication of rationals , arithmetic with reals requires infinite precision and cannot be done with Boolean circuits. To this end, we compute rational approximations of reals accurate enough to preserve the acceptance condition for inputs up to a particular length \(n\).

Here, a naive attempt would be to replace each real occurring in the UHAT in affine transformations and the positional encoding by some rational approximation. However, this is not possible, meaning _any_ rational approximation would change the behavior on input sequences of length \(n=3\), even for low-dimensional vectors with entries in \(\{0,1\}\). Indeed, there is a UHAT involving real numbers \(,\) that accepts a simple sequence of \(\{0,1\}\)-vectors if and only if \(=2\) and \(=\), i.e. \(==\). Thus, \(\) and \(\) cannot be replaced by rationals, even for very short inputs (see Appendix A for details).

Instead, we show that a UHAT can be translated into a small Boolean combination of polynomial inequalities. This format has the advantage that--as we show using convex geometry--the real coefficients of those polynomials _can_ be replaced by suitably chosen rational numbers. In turn, the layer-by-layer construction of these polynomial inequalities requires a carefully chosen data structure to encode the function computed by a sequence of transformer layers. For example, we show that the resulting Boolean combinations of polynomial inequalities have a bound on the number of alternations between conjunctions and disjunctions, which is crucial for constructing TC\({}^{0}\)-circuits.

Another key challenge occurs in the translation from \(()^{2}\) to UHATs: In the inductive construction, we need to represent truth values using reals in \(\). To implement negation, we use a UHAT gadget (with positional encodings) that can normalize these truth values to \(\{0,1\}\).

**Notation.** In the sequel, we assume some background from computational complexity, in particular circuit complexity (see the book ). In particular, we use the circuit complexity class AC\({}^{0}\), which defines a class of problems that are computable by a nonuniform family of constant-depth boolean circuits, where each gate permits an unbounded fan-in (i.e. arbitrary many inputs). Similarly, the complexity class TC\({}^{0}\) is an extension of AC\({}^{0}\), where majority gates are allowed. It is well-known that AC\({}^{0}\) TC\({}^{0}\). Assuming _uniformity_, both AC\({}^{0}\) and TC\({}^{0}\) are contained in the class of problems solvable in polynomial-time. For _nonuniformity_, these classes are contained in the complexity class P/poly, which admits (nonuniform) polynomial-size circuits. It is a long-standing open problem whether numerical analysis (e.g. square-root-sum) is in \(/\), e.g., see .

## 2 Transformer encoders and their languages

In the following, we adapt the setting of formal language theory (see ) to data sequences. For a vector \(=(a_{1},,a_{d})\) we write \([i,j]:=(a_{i},,a_{j})\) for all \(1 i j d\) and if \(i=j\), we simply write \([i]\). For a set \(S\) we denote the set of (potentially empty) sequences of elements from \(S\) by \(S^{*}\). We write \(S^{+}\) for the restriction to non-empty sequences. We consider languages \(L\) over the infinite alphabet \(=^{d}\), for some integer \(d>0\). That is, \(L\) is a set of sequences of \(d\)-tuples over rational numbers. We will define a UHAT (similarly as in previous papers that study formal language theoretic perspectives) as a length preserving map \((^{d})^{*}(^{e})^{*}\).

**Standard encoder layer with unique hard attention.** A standard encoder layer is defined by three affine transformations \(A,B^{d}^{d}\) and \(C^{2d}^{e}\). For a sequence \(_{1},,_{n}^{d}\) with \(n 1\) we define the _attention vector_ at position \(i[1,n]\) as \(_{i}:=_{j}\) with \(j[1,n]\) minimal such that the _attention score_\( A_{i},B_{j}\) is maximized. The layer outputs the sequence \(C(_{1},_{1}),,C(_{n},_{n})\).

**ReLU encoder layer.** A ReLU layer for some \(k[1,d]\) on input \(_{1},,_{n}^{d}\) applies the ReLU function to the \(k\)-th coordinate of each \(_{i}\), i.e. it outputs the sequence \(^{}_{1},,^{}_{n}\) where \(^{}_{i}:=(_{i}[1,k-1],\{0,_{i}[k]\},_{i}[k+1,n])\). [Equivalently, one could instead allow a feed-forward network at the end of an encoder layer (see ).]

**Transformer encoder.** A _unique hard attention transformer encoder_ (UHAT) is a repeated application of standard encoder layers with unique hard attention and ReLU encoder layers. Clearly, using analternation of standard layers and ReLU layers, we can assume that the output of a UHAT layer is an arbitrary composition of affine transformations and component-wise ReLU application. In particular, these compositions may use the functions \(\) and \(\).

**Languages accepted by UHATs.** The notion of "languages" accepted by a UHAT (i.e. a set of accepted sequences) can be defined, depending on whether a _positional encoding_ is permitted. If it is permitted, a language \(L(^{d})^{+}\) is accepted by a UHAT \(T\) if and only if there exists a positional encoding function \(p^{s}\) and an acceptance vector \(^{c}\) such that on every sequence

\[(p(1,n+1),_{1}),,(p(n,n+1),_{n}),(p(n+1,n+1),)\] (1)

\(T\) outputs a sequence \(^{}_{1},,^{}_{n+1}^{e}\) with \(,^{}_{1}>0\) if and only if \((_{1},,_{n}) L\). Note that if \(T_{1}\) and \(T_{2}\) are UHATs with positional encoding that realize functions \(f_{1}(^{d})^{*}(^{c})^{*}\) and \(f_{2}(^{c})^{*}(^{c})^{*}\), then there is a UHAT \(T_{2} T_{1}\) with positional encoding that realizes the composition \(f_{2} f_{1}\) by using a positional encoding that combines the positional encodings of \(T_{1},T_{2}\).

In the above definition we appended an additional zero vector to the end of the input. Over finite alphabets it is often assumed that the input sequence is extended with a special unique end-of-input marker (e.g. see ). When the input is a sequence of (tuples of) numbers, if we allow positional encoding, then the zero vector at the end of the input can be turned into a unique vector marking the end of the input (see Section 5). Without positional encoding, however, we have to explicitly make the zero vector at the end of the input unique. That is, a UHAT without positional encoding is initialized with the sequence \((1,_{1}),,(1,_{n}),^{d+1}\) instead; this ensures, among others, that the end-of-input marker does not appear in the actual input.

In the definition of a standard encoder layer the attention vector at position \(i[1,n]\) can be any vector in the sequence \(_{1},,_{n}\). Using _masking_, one can restrict the attention vector to vectors of certain positions. A UHAT with _past masking_ restricts the attention vector \(_{i}\) at position \(1 i<n\) to be contained in the subsequence \(_{i+1},,_{n}\) and at position \(n\) to \(_{n}:=_{n}\).

## 3 UHAT and TC\({}^{0}\)

In this section, we prove Theorem 1. First, we show that all languages of UHAT (even with positional encoding) belong to the class TC\({}^{0}\). Then, we show that there is a UHAT (even without positional encoding) whose language is TC\({}^{0}\)-hard under AC\({}^{0}\)-reductions. We begin with the proof that all UHAT languages belong to TC\({}^{0}\).

Input encodingA language \(L^{*}\) over a finite alphabet \(\) belongs to TC\({}^{0}\) if for every input length \(n\), there is a circuit of size polynomial \(n\), such that the circuit consists of input gates (for each input position \(i\), and each letter \(a\), there is a gate that evaluates to "true" if and only if position \(i\) of the input words carries an \(a\)), Boolean gates (computing the AND, OR, or NOT function) and majority gates (evaluating to true if more than half of their input wires carry true). Here, AND, OR, and majority gates can have arbitrary fan-in. In order to define what it means that a language \(L(^{d})^{+}\) belongs to TC\({}^{0}\), we need to specify an encoding as finite-alphabet words. To this end, we encode a sequence \(_{1},,_{n}\) with \(_{i}^{d}\) as a string \(v_{1}\#\#v_{n}\), where \(v_{i}=p_{1}/q_{1} p_{d}/q_{d}\) with \(p_{j},q_{j}\{-,0,1\}^{*}\). Here, \(v_{i}\{-,/,,0,1\}^{*}\) represents the vector \(_{i}^{d}\) such that \(_{i}[j]=}{b_{j}}\), \(a_{j},b_{j}\), and \(p_{j},q_{j}\{-,0,1\}^{*}\) are the binary expansions of \(a_{j}\) and \(b_{j}\).

**Remark 4**.: _The main challenge in proving Theorem 1 is that the constants appearing in a UHAT can be real numbers. These can in general not be avoided: There are UHAT languages over \(=\) (even without positional encoding) that cannot be accepted by UHAT with rational constants (even with positional encoding). For example, for every real number \(r>0\), one can1 construct a UHAT for the language \(L_{r}\) of all sequences over \(=\) where the first letter is \(>r\). Note that \(L_{r} L_{s}\) for any \(r,s>0\), \(r s\). However, there are clearly only countably many languages over \(=\) accepted by UHAT with rational constants where membership only depends on the first letter2. Thus, there are uncountably many real \(r>0\) such that \(L_{r}\) is not accepted by a UHAT with rational constants._

The construction of TC\({}^{0}\) circuits comprises three steps. In Step I, we show that the set of accepted length-\(n\) sequences can be represented by a Boolean combination of polynomial inequalities. Importantly, (i) this representation, called "polynomial constraints" is polynomial-sized in \(n\), and (ii) the number of alternations between conjunction and disjunction is bounded (i.e. independent of \(n\)). The polynomials in this representations can still contain real coefficients. In Step II, we show that if we restrict the input not only to length-\(n\) sequences, but to rational numbers of size \( m\), then we can replace all real coefficients of our polynomials by rationals of size polynomial in \(m\) and \(n\), without changing the language (among vectors of size \( m\)). In Step III, we implement a TC\({}^{0}\) circuit. Here, it is important that the number of alternations between conjunctions and disjunctions in our polynomial constraints is bounded, because the depth of the circuit is proportional to this number of alternations.

Step I: UHAT as polynomialsWe first consider a formalism to describe a set of sequences over \(^{d}\). We consider such sequences \((_{1},,_{n})\) of length \(n\), where \(_{i}^{d}\) for each \(i\). In this case, we also abbreviate \(}=(_{1},,_{n})\). A _polynomial constraint_ (PC) is a positive Boolean combination (i.e., without negation) of constraints of the form \(p(})>0\) or \(p(}) 0\), where \(p[X_{1},,X_{d n}]\) is a polynomial with real coefficients. Here, plugging \(}(^{d})^{n}\) into \(p\) is defined by assigning the \(d n\) rational numbers in \(}\) to the \(d n\) variables \(X_{1},,X_{d n}\). The PC \(\)_accepts_ a sequence of vectors \(}(^{d})^{n}\), if the Boolean formula evaluates to true when plugging \(}\) into the polynomials \(p\) in \(\). The set of accepted sequences is denoted by \([]\). Now let \(a\). A PC has _a alternations_ if the positive Boolean combination has \(a\) alternations between disjunctions and conjunctions.

In the following, a _constrained polynomial representation (CPR)_ can be used to compute from a sequence of inputs \((_{1},,_{n})\) with \(_{1},,_{n}^{d^{}}\) a new sequence of outputs \((_{1},,_{n})\) with \(_{1},,_{n}^{d}\). Formally, a CPR comprises for each \(i\{1,,n\}\) a sequence \((_{1},D_{1}),,(_{s_{i}},D_{s_{i}})\) of pairs \((_{j},D_{j})\), where each pair \((_{j},D_{j})\) is a "conditional assignment": each \((_{j},D_{j})\) tells us that if the condition \(_{j}\) is satisfied, then we return \(D_{j}(})\). More precisely: (i) each \(_{j}\) is a polynomial constraint where all polynomials have degree \( 2\), (ii) for any \(j m\), the constraints \(_{j}\) and \(_{m}\) are mutually exclusive, and (iii) each \(D_{j}^{d n}^{d}\) is an affine transformation. Because of their role as conditional assignments, we also write \(_{j} D_{j}\) for such pairs. For \(a\), we say that the CPR is _\(a\)-alternation-bounded_ if each of the formulas \(_{j}\) has at most \(a\) alternations. A CPR as above computes a function \(^{d^{} n}^{d n}\): Given \(}=(_{1},,_{n})\) with \(_{1},,_{n}^{d^{}}\), it computes the sequence \((_{1},,_{n})\) if for every \(i\{1,,n\}\), we have \(_{i}=D_{j}(})\), provided that \(j\) is the (in case of existence uniquely determined) index for which \(_{j}(})\) is satisfied. The _size_ of PCs and CPRs are their bit lengths (see Appendix B for details).

**Proposition 5**.: _Fix a UHAT with positional encoding and \(\) layers. For any given sequence length \(n\), there exists a polynomial-sized PC \(\) with \(O()\) alternations such that \([]\) equals the set of accepted sequences of length \(n\)._

Note that Proposition 5 implies that the set of sequences of each length \(n\) is a semialgebraic set . The proof is by induction on the number of layers, which requires a slight strengthening:

**Lemma 6**.: _Fix a UHAT with positional encoding and \(\) layers. For any given sequence length \(n\), one can construct in polynomial time an \(O()\)-alternation-bounded CPR computing the function \(^{d(n+1)}^{e(n+1)}\) computed by the UHAT._

Proof.: We prove the statement by induction on the number of layers. First, we consider the positional encoding \(p^{d}\) as some affine transformations \(P_{i}^{d(n+1)}^{d}\) mapping the input sequence \(}\) to \(_{i}+p(i,n+1)\). Then we obtain a CPR with \( P_{i}\) for each \(1 i n+1\). Now, suppose the statement is shown for \(\) layers and consider a UHAT with \(+1\) layers. Suppose that the first \(\) layers of our UHAT compute a function \(^{d^{}(n+1)}^{d(n+1)}\), and the last layer computes a function \(^{d(n+1)}^{e(n+1)}\). By induction, we have a polynomial size CPR consisting of conditional assignments \(_{i,k} D_{i,k}\) for every \(i\{1,,n+1\}\) and \(1 k s_{i}\). Here, each \(D_{i,j}\) is an affine transformation \(^{d^{}(n+1)}^{d}\).

Let us first consider the case that the last layer of our UHAT is a standard encoder layer. For each \((i,I,j,J)\{1,,n+1\}^{4}\), we build the conditional assignment using the formula \(_{i,I,j,J}\):

\[_{m=1}^{j-1}(_{M=1}^{s_{m}}_{m,M} p_{i,I,j,J, m,M}(})>0)_{m=j+1}^{n+1}(_{M=1}^{s_{m}} _{m,M} p_{i,I,j,J,m,M}(}) 0)\]where \(p_{i,I,j,J,m,M}(})\) is the polynomial \( AD_{i,I}},\ BD_{j,J}}-BD_{m,M}}\). Then, the conditional assignment is \(_{i,I}_{j,J}_{i,I,j,J} C(D_{i,I}},D_ {j,J}})\). Here, the idea is that (i) \(_{i,I}\) expresses that the \(I\)-th conditional assignment was used to produce the \(i\)-th cell in the previous layer, (ii) \(_{j,J}\) says the \(J\)-th conditional assignment was used to produce the \(j\)-th vector in the previous layer, and (iii) \(_{i,I,j,J}\) says the vector \(_{j}\) yields the maximal attention score for the input \(_{i}\), meaning (iii-a) for all positions \(m<j\), \(_{m}\) has a lower score than \(_{j}\) (left parenthesis), and (iii-b) for all positions \(m>j\), \(_{m}\) has at most the score of \(_{j}\) (right parenthesis). In (iii-a) and (iii-b), we first find the index \(M\) of the conditional assignment used to produce the \(m\)-th cell in the previous layer. Note that then indeed, all the PCs \(_{i,I,j,I}\) are mutually exclusive. Moreover, the polynomials \( AD_{i,I}},\ BD_{j,J}}-BD_{m,M}}\) have indeed degree \(2\) and are of size polynomial in \(n\). Furthermore, if the assignments \(_{i,k} D_{i,k}\) had at most \(a\) alternations, then the new assignments have at most \(a+3\) alternations. Finally, the case of ReLU layers is straightforward (see Appendix B). 

Finally, the proof of Proposition 5 is straightforward: from the constructed CPR in Lemma 6 we obtain the polynomial constraint \(_{J=1}^{s_{1}}_{1,J}(,D_{1,J}})>0\) with a bounded number of alternations.

Step II: Replace real coefficients by rationalsIn our proof, the key step is to replace the real coefficients in the PC by rational coefficients so that the rational PC will define the same set of rational sequences, up to some given size. Let us make this precise. We denote by \(_{ m}=\{a\|a\|_{2} m\}\) the set of all rational numbers of size at most \(m\). A polynomial constraint is _rational_ if all the polynomials occurring in it have rational coefficients.

**Proposition 7**.: _For every \(m\) and every PC \(\) with polynomials having \(n\) variables, there exists a rational PC \(^{}\) of polynomial size such that \(_{ m}^{n}=^{} _{ m}^{n}\)._

Proving Proposition 7 requires the following technical lemma, for which we introduce some notation. For two vectors \(,^{t}\) and \(m\), we write \(_{m}\) if for every \(^{t}\) and every \(z\) with \(_{2}, z_{2} m\), we have (i) \(, z\) if and only if \(,>z\). In other words, we have \(_{m}\) if and only if \(\) and \(\) satisfy exactly the same inequalities with rational coefficients of size at most \(m\).

**Lemma 8**.: _For every \(^{t}\) and \(m\), there is a \(^{}^{t}\) with \(^{}_{2}(mt)^{O(1)}\) and \(_{m}^{}\)._

**Remark 9**.: _For proving Lemma 8, it is not sufficient to pick a rational \(^{}\) with \(^{}-<\) for some small enough \(\). For example, note that if in some coordinate, \(\) contains a rational number of size \( m\), then in this coordinate, \(^{}\) and \(\) must agree exactly for \(_{m}^{}\) to hold._

Before we prove Lemma 8, let us see how to deduce Proposition 7: in a PC \(\) we understand each polynomial \(p(X_{1},,X_{n})\) as a scalar product \(,\) where \(\) constrains only variables and \(\) consists of all coefficients. Then Lemma 8 yields a vector \(_{2m}\) containing only rationals with the same behavior as \(\). From this we finally obtain polynomials having only rational coefficients, which also proves Proposition 7. A detailed proof of Proposition 7 can be found in Appendix B.

In the proof of Lemma 8, we use the following fact about solution sizes to systems of inequalities.

**Lemma 10**.: _Let \(A^{k n}\), \(A^{}^{ n}\), \(^{k}\), \(^{}^{}\) with \( A_{2}, A^{}_{2},_{2}, ^{}_{2} m\). If the inequalities \(A\) and \(A^{}^{}\) have a solution in \(^{n}\), then they have a solution \(^{n}\) with \(_{2}(mn)^{O(1)}\)._

We prove Lemma 10 in the appendix. The proof idea is the following. By standard results about polyhedra, the set of vectors \(\) satisfying \(A\) and \(A^{}^{}\) can be written as the convex hull of some finite set \(X=\{_{1},,_{s}\}\), plus the cone generated by some finite set \(\{_{1},,_{t}\}\). Here, the vectors in \(X Y\) are all rational and of polynomial size. By the Caratheodory Theorem, the real solution \(^{n}\) to \(A\) and \(A^{}^{}\) belongs to the convex hull of \(n\) elements of \(X\), plus a conic combination of \(n\) elements of \(Y\). We then argue that by taking the barycenter of those \(n\) elements of \(X\), plus the sum of the \(n\) elements of \(Y\) gives a rational vector \(^{n}\) with \(A\) and \(A^{}^{}\). The full proof of Lemma 10 is in Appendix B.4. To prove Lemma 8, given \(^{n}\), we set up a system of (exponentially many) inequalities of polynomial size so that the solutions are exactly the vectors \(\) with \(_{m}\). The solution provided by Lemma 10 is the desired \(^{}\) (see Appendix B.5).

Step III: Constructing \(TC^{0}\) circuitsIt is now straightforward to translate a polynomial-sized CPR with rational coefficients and bounded alternations into a TC\({}^{0}\) circuit:

**Proposition 11**.: _Every language accepted by a UHAT with positional encoding is recognized by a family of circuits in TC\({}^{0}\)._

We now show that the TC\({}^{0}\) upper bound is tight: There is a UHAT whose language is TC\({}^{0}\)-hard under AC\({}^{0}\) reductions. In particular, this language is not in AC\({}^{0}\), since AC\({}^{0}\) is strictly included in TC\({}^{0}\).

**Proposition 12**.: _There is a TC\({}^{0}\)-complete language that is accepted by a UHAT, even without positional encoding and masking, but is not recognized by any family of circuits in AC\({}^{0}\)._

Proof.: As shown by Buss [6, Corollary 3], the problem of deciding whether \(ab=c\) for given binary encoded integers \(a,b,c\) is TC\({}^{0}\)-complete under AC\({}^{0}\)-reductions. Since \(ab=c\) if and only if \(ab>c-1\) and \(-ab>-(c+1)\), the problem of deciding \(ab>c\) is also TC\({}^{0}\)-complete. We exibit a UHAT such that the problem of deciding \(ab>c\) can be AC\({}^{0}\)-reduced to membership in the language.

It suffices to define a UHAT \(T\) that accepts a language \(L(^{2})^{+}\) such that for all \(r,s\) we have that \((r,s) L\) if and only if \(r>s\). Then we can reduce the test \(ab>c\) to checking whether \((a,) L\). Note that formally, \(\) is represented as a string containing the binary encodings of \(c\) and \(b\) separated by a special symbol. The UHAT \(T\) is by definition initialized with the sequence \((1,r,s),(0,0,0)^{3}\) since we only have to consider the accepted language restricted to sequences of length \(1\). It can directly check that \(r-s>0\) using the acceptance vector \(:=(0,1,-1)\). 

## 4 UHAT and regular languages over infinite alphabets

It was shown by Angluin et al.  that UHATs with no positional encoding on binary input strings accept only regular languages, even if masking is allowed. We show that UHATs with masking over data sequences can recognize "non-regular" languages over infinite alphabet (Theorem 2). More precisely, a standard notion of regularity over the alphabet \(=^{d}\) is that of _symbolic automata_ (see the CACM article ), since it extends and shares all nice closure and algorithmic properties of finite automata over finite alphabets, while at the same time permitting arithmetics. Intuitively, a transition rule in a symbolic automaton is of the form \(p_{}q\), where \(\) represents the (potentially infinite) set \(S^{d}\) of solutions to an arithmetic constraint \(\) (e.g. \(2x=y\) represents \(\{(n,2n):n\}\)). The meaning of such a transition rule is: move from state \(p\) to state \(q\) by reading any \(a S\).

To prove Theorem 2, we define the language

\[:=\{(r_{1},,r_{n})^{n} n 12r_{i}<r_{i+1}1 i<n\}\]

of sequences of rational numbers where each number is more than double the preceding number.

**Lemma 13**.: _UHAT with past masking and without positional encoding can recognize Double._

Proof.: Given an input sequence \(_{1},,_{n+1}=(1,r_{1}),,(1,r_{n}),(0,0)^{2}\), we need to check that for all pairs \(1 i<j<n+1\), we have \(2 r_{i}<r_{j}\). To this end, a first standard encoder layer uses the differences \(2r_{i}-r_{j}\) as attention scores--except for \(j=n+1\), where the attention score will be \(0\). This is achieved by setting the attention score for positions \(i,j\) to \(2_{i}_{j}-_{j}\). Indeed, this evaluates to \(2r_{i}-r_{j}\) for \(i<j<n+1\), and to \(0\) for \(i<j=n+1\). In particular, for a position \(i[1,n]\), the attention score is maximized at \(j=n+1\) if and only if \(2r_{i}<r_{j}\) for all \(j[i+1,n]\).

The output vector \(_{i}\) at position \(i\) is then set to \(_{i}\), where \(_{i}\) is the attention vector at position \(i\). Thus, the output vector has dimension \(1\), and for each \(i[1,n+1]\), we have \(_{i}=0\) if and only if \(2 r_{i}<r_{j}\) holds for all \(j[i+1,n]\).

In a second standard encoder layer we now check whether all \(_{i}\) have value \(0\). To this end, we choose for \(i<j n+1\) the attention score \(_{j}\). Let \(_{i}\) denote the attention vector at position \(i\). Then \(_{i}=0\) iff \(_{i+1},,_{n}\) are all \(0\). We then output \(_{i}=1-(_{i}+_{i})\), which is positive if and only if \(_{i}==_{n+1}=0\). Finally, with the acceptance vector \(=1\) we accept if and only if \(_{1}>0\), which is equivalent to \(_{1}==_{n+1}=0\). As we saw above, the latter holds if and only if \(2r_{i}<r_{j}\) for all \(i,j\) with \(1 i<j<n+1\). 

The proof of non-regularity of Double is easy (see Appendix C). One could also easily show that Double cannot be recognized by other existing models in the literature of formal language theory over infinite alphabets, e.g., register automata , variable/parametric automata ,and data automata variants [5; 17]. For example, for register automata over \((;<)\) (see the book ), one could use the result therein that data sequences accepted by such an automaton are closed under any order-preserving map of the elements in the sequence (e.g., if \(1,2,3\) is accepted, then so is \(10,11,20\)), which is not satisfied by Double.

## 5 Logical languages accepted by UHAT

In this section we show that an extension of linear temporal logic (LTL) with linear rational arithmetic (LRA) and unary numerical predicates is expressible in UHAT over data sequences (Theorem 3). A formula of dimension \(d>0\) in _locally testable LTL_ (\(L}\)) has the following syntax:

\[}=_{k}(_{1},,_{k+1})  U\]

Here, \(_{k}\) for \(k 0\) is an atom in LRA over the \(d\)-dimensional vectors of variables \(_{i}\) of the form \(,(_{1},,_{k+1})+b>0\) where \(^{d(k+1)}\) and \(b\). Intuitively, \(_{k}\) allows one to check the values in the sequence with \(k\) "lookaheads". Furthermore, \(\) is a _unary numerical predicate_, i.e. a family of functions \(_{n}\{1,,n\}\{0,1\}\) for all \(n 1\). We define the satisfaction of an \(L}\) formula \(\) over a sequence \(}=(_{1},,_{n})\) of vectors in \(^{d}\) at position \(i[1,n]\), written \((},i)\), inductively as follows (omitting negation and disjunction):

* \((},i)_{k}(_{1},,_{k+1})\) iff \(i n-k\) and \(_{k}(_{i},,_{i+k})\)
* \((},i)\) iff \(_{n}(i)=1\)
* \((},i) X\) iff \(i<n\) and \((},i+1)\)
* \((},i) U\) iff there is \(j[i,n]\) with \((},j)\) and \((},k)\) for all \(k[i,j-1]\)

We write \(L()}=\{}(^{d})^{+}( {},1)\}\) for the language of \(\).

**Example 14**.: _Consider sequences of the form \(,A,A^{2},,A^{n}\) such that \(A^{n}=0\) and \(n 0\) is minimal with this property where \(^{1 d}\) and \(A^{d d}\) are fixed and \(^{d}\). Theorem 3 implies that this language is accepted by a UHAT since it is defined by the \((LT)^{2}L\) formula \(G[( Last(_{1} 0 A_{1}=_{2}))( Last_{1}=0)]\), where \(Last}= X\)._

**Example 15**.: _Take the standard notion of 7-day Simple Moving Average (7-SMA); this can be generalized to larger sliding windows of 50-days, or 100 days, which are often used in finance. Using \((LT)^{2}L\), it is easy to show that the following notion of "uptrend" can be captured using UHAT: sequences of numbers such that the value at each time \(t\) is greater than the 7-SMA value at time \(t\). The formula for this is:_

\[G(X^{7}(x_{1},,x_{7}))\]

_where \(()\) is the formula \(7x_{7}_{i=1}^{7}x_{i}\). Note here that \(G\) means (as usual for LTL) "globally" \(\), which can be written as \(( U)\). Similarly, \(X^{i}\) means that \(X\) is repeated \(i\) times._

We assume UHATs with positional encoding and a zero vector at the end of the input sequence (see Section 2). In the following we always assume that the components from the positional encoding are implicitly given and are not changed by any UHAT. So we write the sequence in Eq. (1) as \(_{1},,_{n},\). We use the following results from  that also hold for UHATs over data sequences.

**Lemma 16**.: _Let \(d>0\) and \([1,d]\)._

1. _For every_ \(b\{0,1\}\) _there is a UHAT with positional encoding that on every sequence_ \(_{1},,_{n}^{d}\) _with_ \(v_{i}[]\{0,1\}\) _for all_ \(i[1,n]\) _outputs the sequence_ \(_{1},,_{n-1},(_{n}[1,-1],b,_{n}[+1,d])\)_._
2. _There is a UHAT layer with positional encoding that on every sequence_ \(_{1},,_{n}^{d}\) _and for every_ \(i[1,n-1]\) _picks attention vector_ \(_{i}=_{i+1}\)_._
3. _There is a UHAT layer with positional encoding that on every sequence_ \(_{1},,_{n}^{d}\)_, for every_ \([1,d]\) _with_ \(_{1}[],,_{n-1}[]\{0,1\}\) _and_ \(_{n}[]=0\)_, and for every_ \(i[1,n]\) _picks attention vector_ \(_{i}=_{j}\) _with minimal_ \(j[i,n]\) _such that_ \(_{j}[]=0\)_._

Here, 2) and 3) directly follow from . For 1) we remark that in  only the case \(b=0\) is shown. On input \(_{1},,_{n}\) as in 1), the UHAT uses positional encoding function \(p(i,n)}=(i,n)\)and a composition of affine transformations and ReLU to output at position \(i[1,n]\) the vector \((_{i}[1,-1],b_{i},_{i}[+1,d])\) where \(b_{i}:=\{_{i}[],n-i\}\) if \(b=0\) and \(b_{i}:=\{_{i}[],i-n+1\}\) if \(b=1\).

Using Lemma16, we show that a UHAT can transform rational values \(>0\) to 1 and values \( 0\) to 0. This will be used to evaluate inequalities by outputting 1 for true and 0 for false.

**Lemma 17**.: _Let \(d>0\) and \([1,d]\). There is a UHAT with positional encoding that on every sequence \(_{1},,_{n+1}^{d}\) outputs \(^{}_{1},,^{}_{n+1}^{d}\) with \(^{}_{i}:=(_{i}[1,-1],b_{i},_{i}[+1,d])\) for all \(i[1,n+1]\) where \(b_{i}:=1\) if \(_{i}[]>0\) and \(b_{i}:=0\) otherwise._

Proof.: On input \(_{1},,_{n+1}^{d}\), the first layer outputs at position \(i[1,n+1]\) the vector \(_{i}:=(_{i}[1,-1],r_{i},_{i}[+1,d])\) where \(r_{i}:=\{_{i}[],0\}\). Thus, \(r_{i}=0\) if \(_{i}[] 0\) and \(r_{i}>0\) otherwise. The second layer turns the sequence \(_{1},,_{n+1}\) into \((0,_{1}),,(0,_{n+1})\). We then apply 1) of Lemma16 to obtain the sequence \((0,_{1}),,(0,_{n}),(1,_{n+1})\), i.e. the last vector has first component 1, and all other vectors have first component 0. Let \(_{1},,_{n+1}^{d+1}\) be the resulting sequence. The final layer uses attention score \( A_{i},B_{j}\) for all \(1 i,j n+1\) where the affine transformations \(A,B^{d+1}^{d+1}\) yield \(A_{i}=(_{i}[],0,,0)\) and \(B_{j}=(_{j},0,,0)\). Let \(_{i}\) be the attention vector of position \(i[1,n+1]\). Since \(_{i}[] 0\), we have \(_{i}=0\) if \(_{i}[]=0\) and \(_{i}=1\) if \(_{i}[]>0\). The layer outputs \(^{}_{i}:=(_{i}[2,],_{i},_{i}[+2,d+1])\) at position \(i[1,n+1]\). 

We now prove Theorem3. We claim that for every \(()^{2}\) formula \(\) of dimension \(d\) and every \(m d\) there exists a UHAT \(T_{,m}\) with positional encoding that on every sequence \(_{1},,_{n},^{m}\) outputs a sequence \(^{}_{1},,^{}_{n},^{m+1}\) such that for all \(i[1,n]\) we have \(^{}_{i}[1,m]=_{i}\) and \(^{}_{i}[m+1]=1\) if \((},i)\) and \(^{}_{i}[m+1]=0\) otherwise, where \(}:=(_{1}[1,d],,_{n}[1,d])\). Then the theorem follows since for every \(()^{2}\) formula \(\) of dimension \(d\) the UHAT \(T_{,d}\) outputs on every sequence \(}=(_{1},,_{n})\) of vectors in \(^{d}\) extended with the vector \(^{d}\) a sequence \(^{}_{1},,^{}_{n},^{d+1}\) such that \(^{}_{1}[d+1]>0\) if and only if \((},1)\). Thus, \(T_{,d}\) accepts \(L()\) by taking the acceptance vector \(:=(0,,0,1)^{d+1}\).

We prove the claim by induction on the structure of \(()^{2}\) formulas. If the formula is a unary numerical predicate \(\), then we can use the positional encoding \(p(i,n+1):=_{n}(i)\) for all \(i[1,n]\) and \(p(n+1,n+1):=0\) to output on every sequence \(_{1},,_{n},^{m}\) the sequence \((_{1},p(1,n+1)),,(_{n},p(n,n+1)),(,p(n+1,n+1)) ^{m+1}\).

If the formula is an atom \(_{k}(_{1},,_{k+1})\) of the form \(,(_{1},,_{k+1})+b>0\), the UHAT \(T_{_{k},m}\) adds in its first layer a component that is set to 1 to the top of every vector, outputting on every sequence \(_{1},,_{n},^{m}\) the sequence \((1,_{1}),,(1,_{n}),(1,)\). Then we apply 1) of Lemma16 to turn this sequence into \((1,_{1}),,(1,_{n}),\). Next, \(T_{_{k},m}\) uses \(k\) layers to allow each position to gather the first \(d+1\) components of its \(k\) right neighbors. More precisely, the \(\)-th layer, for \([1,k]\), on sequence \(_{1},,_{n+1}\) uses 2) of Lemma16 to get for every position \(i[1,n]\) the attention vector \(_{i}=_{i+1}\) and the attention vector \(_{n+1}\) is arbitrary. Note that if \(=1\), then \(_{n}=\). Then it applies an affine transformation to output at position \(i[1,n]\) the vector \((_{i}[1,d+1],_{i})\) and using 1) at position \(n+1\) the vector \((0,_{n+1}[2,d+1],_{n+1})\). Let \(_{1},,_{n+1}^{k(d+1)+m+1}\) be the output of the \(k\)-th of those layers. We add another layer that using a composition of ReLU and affine functions outputs at every position \(i[1,n+1]\) the vector \(^{}_{i}:=(_{i}[k(d+1)+2,k(d+1)+m+1],r_{i})^{m+1}\) where \(r_{i}:=\{_{i},(,}_{i})+b\}\) and \(}_{i}:=(_{i}[2,d+1],_{i}[d+3,2(d+1)],,_{i}[ kd+k+2,(k+1)(d+1)])\),

which contains the first \(d\) components of the initial input vector and its \(k\) right neighbors. That is, for all \(i[1,n]\) we have that \(r_{i}=0\) if \(,}_{i}+b 0\) or \(i+k>n\) since \(_{i}\) can only be 0 if it was gathered from the vector at position \(n+1\) using attention. Furthermore, \(r_{i}>0\) if \(,}_{i}+b>0\) and \(i+k n\). Note that \(^{}_{i}[1,m]\) is equal to the input vector \(_{i}\) from the beginning if \(i[1,n]\) and \(\) if \(i=n+1\). Finally, we apply Lemma17 followed by 1) to output at position \(i[1,n]\) the vector \(^{ the cases \(\), \(\), and \(X\) we refer to Appendix D. For \( U\) define the UHAT \(T_{ U,m}\) that first applies \(T_{,m+1} T_{,m}\) outputting a sequence \(_{1},,_{n},^{m+2}\). Observe that \((},i) U\) for \(i[1,n]\) and \(}:=(_{1}[1,d],,_{n}[1,d])\) if and only if for the minimal \(j[i,n]\) with \((},j)\) we have \((},j)\) and such a \(j\) exists. Equivalently, for the minimal \(j[i,n+1]\) with \(\{_{j}[m+1],1-_{j}[m+2]\}=0\) it holds that \(_{j}[m+2]=1\). To check this, we first add a layer that outputs at position \(i[1,n+1]\) the vector \(^{}_{i}:=(_{i},\{_{i}[m+1],1-_{i}[m+2]\}) ^{m+3}\). Finally, we add a layer that uses 3) of Lemma 16 to get attention vector \(_{i}=^{}_{j}\) with \(j[i,n+1]\) minimal such that \(^{}_{j}[m+3]=0\). The layer then outputs at position \(i[1,n+1]\) the vector \(^{}_{i}:=(^{}_{i}[1,m],_{i}[m+2])^{ m+1}\).

## 6 Concluding remarks

We initiated the study of the expressive power of transformers, when the input is a sequence of (tuples of) numbers, which is the setting for applications like time series analysis/forecasting. Our results indicate an increased expressiveness of transformers on such input data, in comparison to the previous formal language theoretic setting (see survey ), i.e., when a token embedding function (with a bounded number of tokens) is first applied before feeding the input to a transformer. More precisely, this represents for Unique Hard Attention Transformers (UHAT) a jump from the complexity class \(^{0}\) to \(^{0}\) (since \(^{0}^{0}\)), and the jump from regular to non-regular languages (when position encoding is not allowed). On the positive side, we successfully developed an expressive class of logical languages recognized by UHAT in terms of a logic called locally testable LTL, which extends previously identified logic for UHAT for strings over finite alphabets [2; 3].

**Limitations.** While we follow the standard formalization of transformer encoders in Formal Languages and Neural Networks (e.g. [3; 20; 21; 40]), limitations of the models are known (see  for a thorough discussion). For example, used real numbers could be of _unbounded_ precision, which allow one to precisely represent values of \(\) and \(\) functions (actually used in practice for positional encoding). In addition, the positional encoding used by the model could be _uncomputable_. Three answers can be given. First, an _upper bound complexity_ on the model with unbounded precision and arbitrary positional encodings (e.g. in \(^{0}\)) still applies in the case of bounded precision. Second, limiting the power of UHAT (e.g. allow only rational numbers, and assuming efficient (i.e. uniform \(^{0}\)) computability of the positional encoding \(p:^{d}\)), our proof in fact yields _uniformity_ of our \(^{0}\) upper bound. Third, our lower bound for non-regularity of UHAT (cf. Theorem 2) holds even with only rational numbers and no positional encodings. Finally, to alleviate these issues, we have always made an explicit distinction between UHAT with and without positional encodings.

**Future directions.** Our paper opens up a plethora of research avenues on the expressive power of transformers on data sequences. In particular, one could consider other transformer encoder models that have been considered in the formal language theoretic setting to transformers (see the survey ). For example, instead of unique hard attention mechanism, we could consider the expressive power of transformers on data sequences using _average hard attention_ mechanism. Similar question could be asked if we use a softmax function instead of a hard attention, which begs the question of which numerical functions could be computed in different circuit complexity classes like \(^{0}\). Another important question concerns a logical characterization for UHAT over sequences of numbers. This is actually still an open question even for the case of finite alphabets. Barcelo et al.  showed that first-order logic (equivalently, LTL) with monadic numerical predicates (called LTL(Mon)) is subsumed in UHAT with arbitrary position encodings, i.e., the transformer model that we are generalizing in this paper to sequences of numbers. There are UHAT languages (e.g. the set of palindromes) that are not captured by this. As remarked in , LTL(Mon) can be extended with arbitrary linear orders on the positions (parameterized by lengths), which then can define the palindromes. [An analogous extension for \(()^{2}\) can define palindromes over an infinite alphabet.] However, it is possible to show that the resulting logic is still not expressive enough to capture the full generality of UHAT. That said, although our logic does not capture the full UHAT, it can still be used to see at a glance what languages can be recognized by UHAT (e.g. Simple Moving Averages). There could perhaps be a hope of obtaining a precise logical characterization if we restrict the model of UHAT. The recent paper  showed that LTL(Mon) captures precisely the languages of masked UHAT with position encodings with "finite image". It is interesting to study similar restrictions for UHAT in the case of sequences of numbers.

#### Acknowledgments

Funded by the European Union (ERC, LASD, 101089343 and FINABIS, 101077902). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.