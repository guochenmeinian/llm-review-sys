# TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion

Preetha Vijayan\({}^{1,}\), Prashant Bhat\({}^{2,3,*}\), Bahram Zonooz\({}^{2,3,}\), Elahe Arani\({}^{2,}\)

\({}^{1}\)NavInfo Europe \({}^{2}\)Eindhoven University of Technology (TU/e) \({}^{3}\)TomTom

preetha.vijayan@navinfo.eu, {p.s.bhat, e.arani, b.zonooz}@tue.nl

Equal contribution. \({}^{}\)Equal advisory role.Code is available at https://github.com/NeurAI-Lab/TriRE

###### Abstract

Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses _retaining_ the most prominent neurons for each task, _revising_ and solidifying the extracted knowledge of current and past tasks, and actively promoting less active neurons for subsequent tasks through _rewinding_ and relearning. Across CL settings, TriRE significantly reduces task interference and surpasses different CL approaches considered in isolation.1

## 1 Introduction

Continual learning (CL) over a sequence of tasks remains an uphill task for deep neural networks (DNNs) due to catastrophic forgetting of older tasks, often resulting in a rapid decline in performance and, in the worst-case scenario, complete loss of previously learned information . Several approaches, such as parameter isolation , weight regularization , and experience rehearsal  have been proposed in the literature to address the problem of catastrophic forgetting in DNNs. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings. Parameter isolation approaches suffer from capacity saturation and scalability issues in longer task sequences, while weight regularization approaches cannot discriminate classes from different tasks, thus failing miserably in scenarios such as class-incremental learning (Class-IL) . In scenarios where buffer size is limited due to memory constraints (e.g., edge devices), rehearsal-based approaches are prone to overfitting on the buffered data . As these research directions have rarely crossed paths, there is a need for an integrated approach to leverage the advantages of competing methods to effectively mitigate catastrophic forgetting in CL.

Catastrophic forgetting is a direct consequence of a more general problem, namely the stability-plasticity dilemma : the extent to which the CL model must be plastic to accommodate newly acquired knowledge and stable to not interfere with previously learned information . In starkcontrast to DNNs, biological systems manage this dilemma better and are able to learn continually throughout their lifetime with minimal interference. CL in the brain is administered by a rich set of neurophysiological processes that encompass different kinds of knowledge, and conscious processing that integrates them coherently . Empirical studies suggest that metaplasticity  and experience replay play a prominent role in memory consolidation in the brain [40; 15]. In addition, neurogenesis in the brain is crucial for the growth and restructuring necessary to accommodate new skills . Neuromodulatory systems facilitate swift learning and adaptability in response to contextual changes induced by new stimuli or shifts in motivation . Whereas context-dependent gating  and active forgetting  improve the separation between the representations of patterns belonging to different tasks. By simultaneously leveraging these processes, the brain exploits task similarity and exhibits positive forward transfer, rarely resulting in catastrophic forgetting.

Inspired by the biological underpinnings of the CL mechanisms in the brain, we propose _'REtain, REvise & REwind' (TriRE)_, a novel CL paradigm to mitigate catastrophic forgetting. Specifically, TriRE involves experience rehearsal, scalable neurogenesis, selective forgetting, and relearning to effectively mitigate catastrophic forgetting. Within each task, the proposed method consists of three stages: (i) _Retain_, where the most active neurons and their corresponding most important weights of the task are extracted and retained to avoid task interference and drastic weight changes, (ii) _Revise_, where the extracted network is finetuned to revise and solidify the current task as well as the joint distribution of the past tasks, (iii) _Rewind_, where the free neurons undergo active forgetting and relearning to promote the less active neurons back into the learning circuit for the next task as illustrated in Figure 1. TriRE effectively combines multiple mechanisms and leverages the advantages offered by different CL approaches.

We find that TriRE significantly reduces task interference and surpasses the aforementioned CL approaches across various CL scenarios. Specifically, TriRE outperforms rehearsal-based approaches in Seq-TinyImageNet for Class-IL scenario by almost 14%, even under low-buffer regimes, by promoting generalization through weight and function space regularization. Experience rehearsal enables discrimination of classes belonging to different tasks in TriRE, resulting in at least twice as good performance over weight regularization methods for the same dataset. Unlike parameter isolation approaches, TriRE is scalable and produces at least a 7% relative improvement compared to parameter isolation approaches in Seq-CIFAR100 Task-IL setting without requiring access to task identity at inference time.

## 2 Related works

**Rehearsal-based Approaches:** Prior works attempted to address the problem of catastrophic forgetting by explicitly storing and replaying previous task samples, akin to experience rehearsal in the brain. Experience rehearsal (ER) approaches [41; 42] maintain a fixed capacity memory buffer to

Figure 1: _TriRE_ consists of a three-phase learning paradigm that reduces task interference and drastic weight changes by using task modularity. In the _Retain_ stage, the method selects and preserves the most active neurons and weights in a mask \(_{t}\), which is used in the subsequent _Revise_ stage to finetune the joint distribution of current and past tasks along with a cumulative subnetwork mask \(\). The _Rewind_ stage is responsible for reintroducing less active neurons to the learning process for future tasks by actively forgetting and relearning the non-sparse subnetwork.

store data sampled from previous task distributions. Several approaches are built on top of ER to better preserve the previous task information: GCR  proposed a coreset selection mechanism that approximates the gradients of the data seen so far to select and update the buffer. DER++  and CLS-ER  enforce consistency in predictions using soft targets in addition to ground-truth labels. DRI  uses a generative model to further support experience rehearsal in low buffer regimes. More recent works like TARC , ER-ACE  and Co\({}^{2}\)L  focus on reducing representation drift right after task switch to mitigate forgetting through asymmetric update rules. Under low-buffer regimes and longer task sequences, however, these approaches suffer from overfitting on the buffered samples.

**Weight Regularization Approaches:** Catastrophic forgetting mainly emanates from large weight changes in DNNs when learning a new task. Therefore, weight-regularization methods seek to penalize the sudden changes to model parameters that are crucial for the previous tasks. Depending on the type of regularization, these approaches can be broadly categorized into prior- and data-focused approaches. Prior-focused approaches, such as elastic weight consolidation (EWC) , online EWC (oEWC) , memory-aware synapses (MAS) , and synaptic intelligence (SI)  employ prior information on model parameters and estimate the importance of parameters associated with previous tasks based either on the gradients of the learned function output or through Fisher's information matrix. On the other hand, data-focused methods, such as Learning without Forgetting (LwF)  instead perform knowledge distillation from models trained on previous tasks when learning on new data. Although weight regularization approaches do not require a memory buffer and are scalable, they only impose a soft penalty, thus failing to entirely prevent forgetting of previous task information.

**Parameter Isolation Approaches:** Parameter isolation has been predominantly done in two ways: either within a fixed capacity or by growing in size. In the former, dynamic sparse methods such as PackNet , CLNP , PAE , and NISPA  make use of DNN's over-parameterization to learn multiple tasks within a fixed model capacity. Similar to the brain, these models simultaneously learn both connection strengths and a sparse architecture for each task, thereby isolating the task-specific parameters. However, these methods suffer from capacity saturation in longer task sequences, limiting their ability to accommodate new tasks. In contrast, the latter methods, such as PNNs , Expert-gate  and DEN  expand in size, either naively or intelligently, to accommodate new tasks while minimizing forgetting. Although these approaches are extremely efficient in mitigating catastrophic forgetting, they do not scale well with longer task sequences, rendering them inapplicable in real-world scenarios.

Contrary to DNNs, the brain simultaneously exploits multiple neurophysiological processes, including neurogenesis , active forgetting , metaplasticity , experience rehearsal , and context-dependent gating  to continually acquire, assimilate, and transfer knowledge across tasks without catastrophic forgetting . Inspired by how the brain exploits multiple mechanisms concurrently, we propose a novel CL paradigm, TriRE, that leverages the advantages of multiple aforementioned mechanisms to effectively mitigate catastrophic forgetting in CL.

## 3 Method

CL problems typically comprise \(t\{1,2,..,T\}\) sequential tasks, with \(c\) classes per task, and data that appear gradually over time. Each task has a task-specific data distribution associated with it \((x_{t},y_{t}) D_{t}\). We take into account two well-known CL scenarios, class-incremental learning (Class-IL) and task-incremental learning (Task-IL). Our working model consists of a feature extractor network \(f_{}\) and a single head classifier \(g_{}\) that represents all classes of all tasks.

Sequential learning through DNNs has remained a challenging endeavor, since learning new information tends to dramatically degrade performance on previously learned tasks. As a result, to better retain information from past tasks, we maintain a memory buffer \(D_{m}\) that contains data from tasks previously viewed. Considering the desiderata of CL, we assume that the model does not have infinite storage for previous experience and thus \(|D_{m}||D_{t}|\). To this end, we use _loss-aware balanced reservoir sampling_ to maintain the memory buffer. We update the working model, \(_{}=g_{}(f_{}())\), using experience rehearsal at each iteration by sampling a mini-batch from both \(D_{t}\) and \(D_{m}\) as follows:

\[=}_{(x_{i},y_{i}) D_{t}}[_{ce }((_{}(x_{i})),y_{i})]\ +}_{(x_{j},y_{j}) D_{m}}[_{ce}( (_{}(x_{j})),y_{j})],\] (1)where \(_{ce}\) is the cross-entropy loss, \((.)\) is the softmax function, \(_{t}\) is the task-wise loss and \(_{er}\) is the rehearsal-based loss. The objective in Eq. 1 encourages plasticity through the supervisory signal from \(D_{t}\) and increases stability through \(D_{m}\). However, as CL training advances, model predictions carry more information per training sample than ground truth . Hence, soft targets can be utilized in addition to ground-truth labels to better preserve the knowledge of the earlier tasks. Traditional methods to enforce consistency in predictions include using an exponential moving average (EMA) of the weights of the working model  or holding previous predictions in a buffer . As the former result in better knowledge consolidation and decision boundaries, we use EMA of the working model weights to ensure consistency in predictions:

\[_{cr},y_{j}) D_{m}}{}\| _{_{EMA}}(x_{j})-_{}(x_{j})\|_{F}^{2},\] (2)

where \(_{_{EMA}}\) is the EMA of the working model \(_{}\) and \(\|\|_{F}\) is the Frobenius norm. We update the EMA model as follows:

\[_{EMA}=\,_{EMA}+(1-)\;,& (0,1)\\ _{EMA},&\] (3)

where \(\) is the decay parameter and \(\) is the update rate. Finally, the EMA model acts as a self-ensemble of models with distinct task specializations for inference rather than the working model.

**Notations:** Let \(_{t}\) be the extracted subnetwork mask corresponding exclusively to the current task and \(\) be the cumulative dynamic network mask corresponding to the tasks learned so far. At the end of the training, \(\) would contain the most active neurons and the best corresponding weights across all tasks. In the following, we describe in detail various components of TriRE learning paradigm.

### Retain

In CL, typically, models from previous tasks are seen as initialization and are "washed out" by new updates from the current task, which causes CF . However, the brain uses context-dependent gating  to selectively filter neural information based on the context in which it is presented, allowing the development of specialized modules that can be added or removed from the network without disrupting previously learned skills [37; 50]. Inspired by this, the _Retain_ phase induces modularity in the model by training a hyper-network first and then extracting a subnetwork that is equivalently representational of the current task knowledge. This extracted subnetwork not only helps in creating task-wise specialized modules, but also helps the model preserve capacity for future tasks. Retention of this subnetwork is done using heterogeneous dropout of activations and weight pruning.

The _Retain_ stage appears at the beginning of each task. At this stage, initially \(\{f_{}\}\) is trained using a mini-batch of \(D_{t}\) and \(\{f_{}\}\) is trained using a mini-batch of \(D_{m}\). This is to ensure that the weights not in the cumulative network learn the new task to maintain plasticity, while the weights in the cumulative network learn a combination of old and new tasks to maintain stability. At the convergence of this training, we perform activation pruning followed by weight pruning to extract \(_{t}\) as shown in Figure 2.

**Activation Pruning:** This involves identifying and extracting neurons that contribute the most to the overall activation or output of the network. We monitor the frequency of neuron activations when a network is trained on a task. In essence, each neuron is given an activation counter that increases when a neuron's activation is among the top-k activations in its layer. We use these activation counts to extract the k-winner activations and retain them as the knowledge base for the current task. Heterogeneous dropout  is used to map the activation counts of each neuron to a Bernoulli variable, indicating whether the said neuron is extracted or dropped. This, essentially, leaves the less activated neurons free to learn the next tasks.

**Weight Pruning:** After retaining the most activated neurons for the task, we prune the less important connections corresponding to these neurons. In contrast to conventional methods, which only leverage weight magnitude or Fisher information for pruning, our method also takes into account the significance of weights with respect to data saved in the rehearsal buffer. Continual Weight Importance (CWI)  criteria ensure that we maintain: (1) weights of greater magnitude for output stability, (2) weights significant for the current task for learning capacity, and (3) weights significant for past data to prevent catastrophic forgetting.

For the working model, the CWI of weight \(\) is defined as follows,

\[CWI()=\|\|_{1}+\|_{ce}}(D_{t}; )}{}\|_{1}+\|_{ce}(D_{m};) }{}\|_{1}\] (4)

where \(\) and \(\), are coefficients that regulate the weight of current and buffered data, respectively. In addition, \(_{ce}}\) denotes the single-head form of the cross-entropy loss, which only takes into account the classes relevant to the current task by masking out the logits of other classes.

At the end of this stage, we end up with a mask of the most activated neurons and their most important weights for the current task, \(_{t}\).

### Revise

Although context-dependent gating is a helpful phenomenon for CL in tackling CF, there are other biological phenomena such as neuromodulation, metaplasticity, and neurogenesis among others which also contribute to the brain's ability to combat CF . Neuromodulation, for instance, is used to decide whether a new feature is novel and unfamiliar (that is, creates a new memory) or common and familiar (that is, consolidates into an existing memory). Taking the cue from that, _Revise_ stage mainly focuses on revising and solidifying the knowledge that the extracted subnetwork of the current task, \(_{t}\), and the cumulative subnetwork of past tasks, \(\), currently possess. It is clear that \(_{t}\) is specialized on the current task and \(\) is specialized on the past tasks. However, finetuning these two networks jointly can improve the knowledge overlap, and the features thus generated would be a better approximation of all the seen classes.

Firstly, \(\{f_{}(_{t})\}\) is trained with a mini-batch of \(D_{t}\) so that a part of the extracted network, \(_{t}\), can be utilized to regain the performance for the current task. Subsequently, \(\{f_{}(_{t})\}\) is trained with a mini-batch of \(D_{m}\) to preserve forward transfer and knowledge overlap between the past and the current tasks. The optimization learning rate is also considerably reduced at this stage compared to the _Retain_ stage to prevent drastic weight changes in subnetworks, which in turn decreases the forgetting. This could be seen as an adaptation of the metaplasticity in the brain [26; 20] which refers to the ability to adjust the amount of plasticity in the network based on the current and future needs of the organism. At the end of this finetuning, the currently extracted subnetwork \(_{t}\) is merged with the cumulative extracted mask from past tasks, \(\). The merging of the subnetworks can be seen as a process of integrating new neurons (\(_{t}\)) into the existing neural network (\(\)), similar to the way neurogenesis allows for the integration of new neurons into existing neural circuits to accommodate new memories.

### Rewind

Evidence implies that the brain uses active forgetting as a tool for learning due to its limited capacity [47; 29]. Studies also suggest that although learning and memory become more difficult to access during the forgetting process, they still exist [54; 48]. There are still memory remnants in the brain that can be reactivated . In this work, we define the rewinding of weights to an earlier state as active forgetting. We also make sure that rather than reinitializing the model back to random or very

Figure 2: Schematic representation of extraction of subnetwork at the end of _Retain_ stage. The dense network is first pruned using k-WTA criteria, resulting in a subnetwork of the most activated neurons. This subnetwork is then pruned using CWI criteria, resulting in a final extracted subnetwork, \(_{t}\).

early weights, it is rewound to a point where the model has learned some features and has a generic perception of the objective closer to convergence (but not absolute convergence). To aid this, the weights from a later epoch \(k\) from the _Retain_ phase are saved to be used in the _Rewind_ stage.

Specifically, after the _Retain_ and _Revise_ steps, we rewind the weights belonging to non-cumulative subnetwork \(\{f_{}\}\) back to the epoch \(k\) weights. Then the rewinded weights are finetuned for a few epochs using a mini-batch of \(D_{t}\). This is helpful because studies  show that in the human brain, less active neurons follow a '_use-it-or-lose-it_' philosophy. Therefore, forgetting and relearning act as a warm-up for these less active neurons making them relevant again for the learning circuit and making them more receptive to learning the next task.

In summary, TriRE (Retatin-Revise-Rewind) involves iteratively applying the three phases mentioned above to each task within a lifelong learning setting. Our method effectively combines multiple biological phenomena and harnesses the advantageous characteristics provided by popular CL approaches. The step-by-step procedure is given in Algorithm 1.

```
1:Input: Data streams \(_{t}\), working model \(_{}=g_{}(f_{}(.))\), EMA model \(_{_{EMA}}=g_{_{EMA}}(f_{_{EMA}}(.))\), sparsity factor \(\), learning rates \(^{}\), Retain epochs \(E_{1}\), Revise epochs \(E_{2}\), Rewind epochs \(E_{3}\).
2:\(\{\}\), \(\{\}\)
3:for all tasks \(t\{1,2,..T\}\)do
4:for epochs \(e_{1}\{1,2,...E_{1}\}\)do\(\)Retain
5:for minibatch \(\{(x_{i},y_{i})\}_{i=1}^{B}_{t}\) and \(\{(x_{m},y_{m})\}_{m=1}^{B}\)do
6: Update \(\{f_{}\},g_{}\) with \(\) on \(\{(x_{i},y_{i})\}_{i=1}^{B}\) using Eq. 1 (\(_{t}\))
7: Update \(\{f_{}\},g_{}\) with \(\) on \(\{(x_{m},y_{m})\}_{m=1}^{B}\) using Eqs. 1 (\(_{er}\)) and 2
8:if\(e==k\)then
9: Save the weights \(_{k}\)
10: Update \(_{EMA}\) using Eq. 3
11: Extract new subnetwork \(_{t}\) with \(\) sparsity based on CWI from Eq. 4
12:for epochs \(e_{2}\{1,2,...E_{2}\}\)do\(\)Revise
13:for minibatch \(\{(x_{i},y_{i})\}_{i=1}^{B}_{t}\) and \(\{(x_{m},y_{m})\}_{m=1}^{B}\)do
14: Finetune \(\{f_{}(_{t})\},g_{}\) with \(^{}\) on \(\{(x_{i},y_{i})\}_{i=1}^{B}\)
15: Finetune \(\{f_{}(_{t})\},g_{}\) with \(^{}\) on \(\{(x_{m},y_{m})\}_{m=1}^{B}\)
16: Update \(_{EMA}\)
17: Update cumulative set \(=_{t}\)
18: Reinitialize non-cumulative weights \(\{f_{}\}\) with \(_{k}\)
19:for epochs \(e_{3}\{1,2,...E_{3}\}\)do\(\)Rewind
20:for minibatch \(\{(x_{i},y_{i})\}_{i=1}^{B}_{t}\)do
21: Update \(\{f_{}\},g_{}\) with \(\) on \(\{(x_{i},y_{i})\}_{i=1}^{B}\)
22: Update \(_{EMA}\)
23: Update buffer \(\)
24:return model \(_{}\), model \(_{EMA}\) ```

**Algorithm 1** Proposed Approach - TriRE

## 4 Results

**Experimental Setup:** We expand the Mammoth CL repository in PyTorch . On the basis of Class-IL and Task-IL scenarios, we assess the existing CL techniques against the proposed one. Although the training procedure for Class-IL and Task-IL is the same, during inference, Task-IL has access to the task-id. We consider a number of rehearsal-based, weight regularization, and parameter-isolation approaches as useful baselines because TriRE necessitates experience rehearsal and model modularity. We use ResNet-18  as the feature extractor for all of our investigations. In order to reduce catastrophic forgetting, we additionally offer a lower bound SGD without any support and an upper bound Joint where the CL model is trained using the full dataset.

**Experimental Results:** We compare TriRE with contemporary rehearsal-based and weight regularization methods in Class-IL and Task-IL settings. As shown in Table 1, TriRE consistently outperformsrehearsal-based methods across most datasets, highlighting the significance of dynamic masking in CL. Although methods like Co\({}^{2}\)L and ER-ACE excel on simpler datasets, they struggle with more challenging ones. The same applies to methods like DRI and GCR, which augment memory buffers through core-set and generative replay. Their performance, for instance, lags behind TriRE in Seq-TinyImageNet, where the buffer-to-class ratio is low. Retaining task-wise dynamic subnetworks and revising extracted subnetworks to preserve past knowledge significantly reduces task interference in TriRE. In essence, TriRE boosts generalization through weight and function space regularization, selective forgetting, and relearning, yielding superior performance across tasks.

As evident from Table 1, weight regularization methods such as LWF, oEWC, and SI perform miserably in both Class-IL and Task-IL settings across datasets. The reason being, these approaches encounter classes solely from the current task at any point in CL training. Therefore, they fail to discriminate between classes from different classes resulting in subpar performance. On the other hand, TriRE leverages samples from previous classes through experience rehearsal to learn discriminatory features across tasks. In addition, TriRE entails forming subnetworks through _Retain_ and _Revise_ resulting in modularity and reduced task interference between tasks.

Parameter isolation approaches minimize task interference in CL by creating distinct sub-networks either within a given model capacity or by dynamically growing the network. Figure 3 illustrates a comparison between methods trained on Seq-CIFAR100 with 20 tasks i.e., it depicts final accuracies

    Buffer \\ size \\  } &  &  &  &  \\   & & Class-IL & Task-IL & Class-IL & Task-IL & Class-IL & Task-IL \\   & SGD & 19.62\(\)0.05 & 61.02\(\)3.33 & 17.49\(\)0.28 & 40.46\(\)0.99 & 07.92\(\)0.26 & 18.31\(\)0.68 \\  & Joint & 92.20\(\)0.15 & 98.31\(\)0.12 & 70.56\(\)0.28 & 86.19\(\)0.43 & 59.99\(\)0.19 & 82.04\(\)0.10 \\   & LwF & 19.61\(\)0.05 & 63.29\(\)2.35 & 18.47\(\)0.14 & 26.45\(\)0.22 & 8.46\(\)0.22 & 15.85\(\)0.58 \\  & oEWC & 19.49\(\)0.12 & 68.29\(\)3.92 & - & - & 7.58\(\)0.10 & 19.20\(\)0.31 \\  & SI & 19.48\(\)0.17 & 68.05\(\)5.91 & - & - & 6.58\(\)0.31 & 36.32\(\)0.13 \\   & ER & 44.79\(\)1.86 & 91.19\(\)0.94 & 21.40\(\)0.22 & 61.36\(\)0.35 & 8.57\(\)0.04 & 38.17\(\)2.00 \\  & DER++ & 64.88\(\)1.17 & 91.92\(\)0.60 & 29.60\(\)1.14 & 62.49\(\)1.02 & 10.96\(\)1.17 & 40.87\(\)1.16 \\  & CLS-ER\({}^{}\) & 61.88\(\)2.43 & **93.59\(\)**0.87 & 43.38\(\)1.06 & **72.01\(\)**0.97 & 17.68\(\)1.65 & 52.60\(\)1.56 \\  & ER-ACE & 62.08\(\)1.44 & 92.20\(\)0.57 & 35.17\(\)1.17 & 63.09\(\)1.23 & 11.25\(\)0.54 & 44.17\(\)1.02 \\  & Co\({}^{2}\)L & 65.57\(\)1.37 & 93.43\(\)0.78 & 31.90\(\)0.38 & 55.02\(\)0.36 & 13.88\(\)0.40 & 42.37\(\)0.74 \\  & GCR & 64.84\(\)1.63 & 90.81\(\)1.05 & 33.69\(\)1.40 & 64.24\(\)0.83 & 13.05\(\)0.91 & 42.11\(\)1.01 \\  & DRI & 65.16\(\)1.13 & 92.87\(\)0.71 & - & - & 17.58\(\)1.24 & 44.28\(\)1.37 \\  & TriRE & **68.17\(\)**0.33 & 92.45\(\)0.18 & **43.91\(\)**0.18 & 71.66\(\)0.44 & **20.14\(\)**0.19 & **55.95\(\)**0.78 \\   

Table 1: Comparison of prior methods across various CL scenarios. We provide the average top-1 (\(\%\)) accuracy of all tasks after training. \({}^{}\) Results of the single EMA model.

Figure 3: Comparison of TriRE against evolving architectures in terms of Task-IL accuracy on Seq-CIFAR100 dataset divided into 20 tasks. The graph reports the accuracy of individual tasks at the end of CL training.

on \(1^{st}\), \(2^{nd}\).. \(20^{th}\) task after training. Upon completing all tasks, TriRE achieves an average accuracy of 80.85%, surpassing the performance of the baselines considered. TriRE leverages the benefits of experience rehearsal, weight regularization, and function space regularization to learn compact and overlapping subnetworks, resulting in reduced task interference while maintaining scalability. In line with biological principles, TriRE incorporates selective forgetting and relearning mechanisms to activate less active neurons and enhance their receptiveness to learning subsequent tasks, thereby mitigating the risk of capacity saturation.

## 5 Model Analysis

**Task Interference:** Figure 4 shows the changes in neuronal activity for the subset of neurons in the last convolutional layer (top) and the last shortcut layer (bottom) of ResNet-18 trained on 5 tasks in Seq-CIFAR100 dataset. The neurons that are blacked out are the most active neurons for each particular task after the _Retain_ phase. For each task, it is observed that there are exclusive subnetworks forming on their own (horizontally) that capture task-specific information. However, with CL training, several neurons become generalizable by capturing information that can be shared across tasks. Therefore, there is neuron overlap between the extracted neurons across tasks (vertically). More so in the shortcut layer, since the number of parameters in these layers is low. TriRE optimally manages the model capacity vs. task modularity trade-off better by re-using neurons that can be shared across tasks while maintaining the modularity of knowledge in each task intact.

**How Much to Rewind?** In Figure 5, we examine Class-IL accuracy when the model is rewound to different points in the _Retain_ stage in order to comprehend how rewinding affects the accuracy of the model. We observe that the accuracy of the inference model decreases if the model forgets too much and is rewound to an early stage in the training. This is in alignment with the observations made by  that rewinding to extremely early stages is not recommended for DNNs because the network has

Figure 4: (Top) depicts the distribution of neuron activations across tasks in the last convolutional layer of the feature extractor and (Bottom) depicts the same for the last shortcut layer. The black cubes represent the extracted ones after _Retain_ stage.

Figure 5: The effect of rewinding on Class-IL accuracy for all three datasets. The region from 70% to 90% of all epochs gives the best results consistently across datasets.

not learned enough meaningful features by then to regain the lost accuracy. Additionally, we notice that accuracy also suffers when the model is rewound to a point extremely close to the end of training time. Rewinding to a very late point in the training phase close to convergence is not ideal because there is not enough time for relearning. Our experiments indicate that rewinding to between 70% and 90% of the training time results in the best accuracy.

**Ablation Study:** As explained previously, TriRE employs a three-stage learning paradigm to reduce task interference and improve weight reuse in CL. We seek to uncover how each of _Retain_, _Revise_, and _Rewind_ in TriRE influence Class-IL and Task-IL accuracies in Seq-CIFAR100 and Seq-TinyImageNet datasets through Table 2. It can be seen that although _Retain_ alone can extract the subnetworks containing the most active neurons and decrease task interference, it falls short in aspects of forward transfer and weight reuse. Similarly, _Retain_ and _Revise_ together can solidify the knowledge extracted from current and past tasks, but such a model suffers from capacity issues without the reactivation of less active neurons for future tasks. Likewise, _Retain_ and _Rewind_ together can encourage task-wise delimitation of knowledge and promote efficient usage of available networks, but lose out on the forward transfer introduced by the learning of joint distributions. Finally, analogous to the brain, it is evident that the harmony of all components is what achieves the best results in both datasets.

**Memory and Computational Cost:** We conduct a comparative analysis of the computational and memory overhead of TriRE in contrast to related works. Table 3 provides an analysis of the learnable parameters and memory required by TriRE in contrast to those of DER++, EWC, and PNNs, (Opting for one method from each individual family of CL methods). Firstly, similar to DER++ and EWC, TRiRE does not add any learnable parameters to the model. However, it is evident that PNNs have an infeasible amount of learnable parameters which gets progressively worse with longer task sequences. Secondly, the observed increase in memory consumption in TriRE can be attributed to several factors: (1) the application of multiple masking mechanisms for parameter isolation; (2) the incorporation of the Rewind phase necessitating weight retention from a previous epoch; and (3) the utilization of the Exponential Moving Average (EMA) model to enhance knowledge consolidation. All of these factors hold memory but do not add any learnable parameter to the training.

## 6 Conclusion

We introduced TriRE, a novel biologically inspired CL paradigm that entails experience rehearsal, scalable neurogenesis, and selective forgetting and relearning to effectively mitigate catastrophic forgetting in CL. Within each task, TriRE entails retaining the most prominent neurons for each task, revising the extracted knowledge of current and past tasks, and actively promoting less active neurons for subsequent tasks through rewinding and relearning. Analogous to multiple neurophysiological mechanisms in the brain, TriRE leverages the advantages of different CL approaches, thus significantly lowering task interference and surpassing different CL approaches when considered in isolation. For Seq-TinyImageNet, TriRE outperforms the closest rival in rehearsal-based baselines by 14%,

   &  &  &  &  \\   & & &  &  &  &  \\  ✓ & ✗ & ✗ & 38.01 & 66.23 & 11.54 & 40.22 \\ ✓ & ✓ & ✗ & 33.08 & 60.03 & 8.44 & 31.90 \\ ✓ & ✗ & ✓ & 43.03 & **72.09** & 16.25 & 48.89 \\ ✓ & ✓ & ✓ & **43.91** & 71.66 & **20.14** & **55.95** \\  

Table 2: Comparison of the contribution of each phase in TriRE. Note that the combination of _Revise_ alone or _Revise & Rewind_ has not been considered, as it is not feasible without the _Retain_ phase.

   &  &  \\   & 5 Tasks & 10 Tasks & 20 Tasks & 5 Tasks & 10 Tasks & 20 Tasks \\  DER ++ & \(1\)x & \(1\)x & \(1\)x & \(1\)x & \(1\)x & \(1\)x \\ EWC & \(1\)x & \(1\)x & \(1\)x & \(3\)x & \(3\)x & \(3\)x \\ TriRE & \(1\)x & \(1\)x & \(1\)x & \(6\)x & \(6\)x & \(6\)x \\ PNNs & \(27\)x & \(79\)x & \(240\)x & \(27\)x & \(79\)x & \(240\)x \\  

Table 3: Relative number of learnable parameters and corresponding memory footprint in Seq-CIFAR100 with varying number of task sequence.

surpasses the best parameter isolation baseline by 7%, and nearly doubles the performance of the best weight regularization method. Extending our method to CL scenarios oblivious to task boundaries and to few- and zero-shot learning settings are some of the future research directions for this work.

## 7 Limitations and Future Work

We proposed TriRE, a novel paradigm that leverages multiple orthogonal CL approaches to effectively reduce catastrophic forgetting in CL. As orthogonal CL approaches may not always be complementary, the selection of such approaches needs careful consideration in TriRE. In addition, having multiple objective functions naturally expands the number of hyperparameters, thereby requiring more tuning to achieve optimal performance. Therefore, additional computational complexity and memory overhead due to the staged approach and extensive hyperparameter tuning are some of the major limitations of the proposed method. For the same reason, we highlight that TriRE is not directed toward compute-intensive architectures such as vision transformers.

TriRE involves different stages of training within each task, requiring knowledge of task boundaries. In line with state-of-the-art methods in CL, each task entails a non-overlapping set of classes, and data within each task is shuffled to guarantee i.i.d. data. However, in the case of online learning where data streams and the distribution gradually shift, TriRE cannot be applied in its current form. Therefore, additional measures such as task-boundary approximation and modification to learning objectives are necessary to enable TriRE to work in such scenarios. Furthermore, traditional CL datasets considered in this work entail independent tasks and data points without intrinsic cumulative structure. As TriRE does not leverage structures learned in previously encountered tasks, structure learning forms another limitation of this proposed method. Reducing computational and memory overhead, extending to task-free CL scenarios with recurring classes, and leveraging intrinsic structures within underlying data are some of the future research directions for this work.

## 8 Broader Impacts

Inspired by the multifaceted learning mechanisms of the brain, we propose TriRE, which replicates the brain's ability to leverage multiple mechanisms for CL, enhancing the generalization capabilities of CL models across tasks. Its success not only encourages the exploration of neuro-inspired methods for deep neural networks, but also opens up opportunities to augment existing CL approaches by leveraging the advantages of competing strategies. By enabling models to learn continuously and adapt to new tasks, TriRE contributes to the responsible and ethical deployment of AI technologies, as models can improve and update their knowledge without requiring extensive retraining. This advancement has significant implications for various real-world applications and promotes the development of AI systems that can continually improve and adapt their performance.

Acknowledgement:The work was conducted while all the authors were affiliated with NavInfo Europe, Eindhoven, The Netherlands.