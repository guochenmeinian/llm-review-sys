# Are Language Models Actually Useful for

Time Series Forecasting?

 Mingtian Tan

University of Virginia

wtd3gz@virginia.edu

&Mike A. Merrill

University of Washington

mikeam@cs.washington.edu

&Vinayak Gupta

University of Washington

vinayak@cs.washington.edu

&Tim Althoff

University of Washington

althoff@cs.washington

&Thomas Hartvigsen

University of Virginia

hartvigsen@virginia.edu

###### Abstract

Large language models (LLMs) are being applied to time series forecasting. But are language models actually useful for time series? In a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance--in most cases, the results even improve! We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters.1

## 1 Introduction

Time series analysis is a critical problem across many domains, including disease propagation forecasting , retail sales analysis , healthcare [26; 17] and finance . A great deal of recent work in time series analysis (constituting repositories with more than 1200 total stars on GitHub) has focused on adapting pretrained large language models (LLMs) to classify, forecast, and detect anomalies in time series [15; 50; 22; 4; 5; 32; 14; 44; 16]. These papers posit that language models, being advanced models for sequential dependencies in text, may generalize to the sequential dependencies in time series data. This hypothesis is unsurprising given language models are now pervasive in machine learning research. However, direct connections between language modeling and time series forecasting remain largely undefined. So to what extent is language modeling _really_ beneficial for traditional time series tasks?

Our claim is simple but profound: **popular LLM-based time series forecasters perform the same or worse than basic LLM-free ablations, yet require orders of magnitude more compute**. Derived from extensive ablations, this reveals a worrying trend in contemporary time series forecasting literature. Our goal is not to imply that language models will never be useful for time series. In fact, recent works point to many exciting and promising ways that language and time series interact, like time series reasoning [25; 7; 45; 42; 37], social understanding  and financial reasoning [36; 20]. Rather, we aim to highlight surprising findings that existing methods do very little to use the innate reasoning power of pretrained language models on established time series tasks.

We substantiate our claim by performing three ablations of three popular and recent LLM-based forecasting methods [50; 15; 22] using eight standard benchmark datasets from reference methodsand another five datasets from MONASH . First, we successfully reproduce results from the original publications. Then, we show that replacing language models with simple attention layers, basic transformer blocks, randomly-initialized language models, and _even removing the language model entirely_, yields comparable or better performance. The same performance was observed on another five datasets that were not studied by the reference methods.

Next, we compare the training and inference speed of these methods against their ablations, showing that these simpler methods reduce training and inference time by up to three orders of magnitude while maintaining comparable performance. Then, to investigate the source of LLM forecaster's performance, we further explore time series encoders. We find that a simple linear model with an encoder composed of patching and attention can achieve forecasting performance similar to that of LLMs. Next, we test whether the sequence modeling capabilities of LLMs transfer to time series by shuffling input time series and find no appreciable change in performance. Finally, we show that LLMs do not even help forecasting in few-shot settings with 10% of the training data. We discuss the implications of our findings and suggest that time series methods that use large language models are better left to multimodal applications [4; 12; 38] that require textual reasoning.

The key contributions we make in this paper are as follows:

* We propose three straightforward ablation methods for methods that pass time series into LLMs for forecasting. We then ablate three top-tier methods on thirteen standard datasets and find that LLMs fail to convincingly improve time series forecasting. However, they significantly increase computational costs in both training and inference.
* We study the impact of an LLM's pretraining by re-initializing their weights prior to forecasting. We find that this has no impact on forecasting performance. Additionally, in shuffling input time series, we find no evidence the LLMs successfully transfer sequence modeling abilities from text to time series and no indication they help in few-shot settings.
* We find a very simple model, with patching and attention as encoder, can achieve performance similar to LLMs. This suggests a massive gap between the benefits LLMs pose and the time series forecasting problem, despite a rapid rush to adopt LLMs.

## 2 Related Work

Here, we summarize the key works relevant to LLM-based time series models. They can be broadly classified into three sections: (i) time series forecasting using LLMs; (ii) encoders in LLM time series models; and (iii) smaller and efficient neural models for time-series.

**Time Series Forecasting Using LLMs.** Recently, with the development of Large Language Models (LLMs) [10; 29; 34] and their demonstrated multi-modal capabilities, more researchers have successfully applied LLMs to time series forecasting tasks [14; 16; 5; 4]. Chang _et al.,_ used finetuning the transformer module and positional encoding in GPT-2 to align pre-trained LLMs with time series data for forecasting tasks. Zhou et al.  proposed a similar finetuning method, named "OneFitsAll", for time series forecasting with GPT-2. Additionally, Jin et al.  introduced a reprogramming method to align LLM's Word Embedding with time series embeddings, showing good representation of time series data on LLaMA . Similarly, CALF  and Test  adapted word embeddings to enable LLMs to forecast time series data effectively. In addition to time-series forecasting models, Liu et al.  show that these models can be extended to classifying health-time series, such as heart-rate and daily-footsteps. These models have also been shown to outperform supervised neural models in few-shot settings.

**Encoders in LLM Time Series Models.** In order for an LLM to learn from text it must first be discretized and encoded as word tokens which are \(1 d\) vectors [10; 29; 34]. Similarly, LLM-based methods for time series learn discrete time series tokens. One method is to segment the time series into overlapping patches, which effectively shortens the time series while retaining its features [15; 50; 5; 4; 28; 27; 11]. Other methods decompose time series into trend, seasonal components, and residual components [4; 28]. Lastly, Liu et al.  feed the multivariate time series using a Transformer to enable different channels to learn the dynamics of other channels. These embedding procedures are followed by a linear neural network layer that projects the time series encoding to the same dimensions used by the pre-trained LLM.

**Small and Efficient Neural Forecasters.** In addition to LLMs, there has been a large body of research on smaller yet efficient frameworks that outperform their bulky counterparts in time series forecasting [19; 47; 33; 24; 2]. For example, Zeng et al.  present DLinear, an incredibly simple model that combines decomposition techniques and achieves better forecasting performance than state-of-the-art transformer-based time series architectures at the time, such as Informer , FEDformer , and Autotiormer . Furthermore, Xu et al.  introduces a lightweight model with only 10k parameters, which captures both amplitude and phase information in the time-series to outperform transformer-based models.

## 3 Experimental Setup

We use three state-of-the-art methods for time series forecasting and propose three ablation methods for LLMs: (i) "**w/o LLM**"; (ii) "**LLM2Attn**"; (iii) and "**LLM2Trsf**". To evaluate the effectiveness of LLMs in time series forecasting, we test these methods on eight standard datasets.

### Reference Methods for Language Models and Time Series

We experiment with three recent methods for time series forecasting using LLMs. All models were published between December 2023 and May 2024 and are popular, with their GitHub repositories collectively amassing 1,245 stars. These methods are summarized in Table 2, and use either GPT-2  or LLaMA  as base models, with different alignment and fine-tuning strategies.

* **OneFitsAll **: OneFitsAll, sometimes called GPT4TS, applies instance norm and patching to the input time series and then feeds it into a linear layer to obtain a input representation for the language model. The multi-head attention and feed forward layers of the language model are frozen while the positional embeddings and layer norm are optimized during training. A final linear layer is used to transform the language model's final hidden states into a prediction.
* **Time-LLM **: In Time-LLM the input time series is tokenized via patching and aligned with a low-dimensional representation of word embeddings using multi-head attention. The outputs of this alignment, combined with the embeddings of descriptive statistical features, are passed to a frozen pre-trained language model. The output representations of the language model are then flattened and passed through a linear layer to obtain a forecast.
* **CALF **: CALF embeds the input time series by treating each channel as a token. One half of the architecture is a "textual branch" which uses cross attention to align the time series representation with a low dimensional representation of the language model's word embeddings. This representation is then passed through a pretrained, frozen language model to obtain a "textual prediction". Simultaneously, a "temporal" branch learns a low-rank adapter for a pretrained language model based on the input time series to produce a "temporal prediction" which is used for inference. The model includes additional loss terms that enforce similarity between these representations.

**Reproducibility Note.** While experimenting with each model, we tried to replicate the conditions of their original papers. We used the original hyper-parameters, runtime environments, and code, including model architectures, training loops, and data-loaders. To ensure a fair comparison, we have included error metrics from the original papers alongside our results wherever possible.

  
**Dataset** & **ETTh1 \& ETH2** & **ETTm1 \& ETH2** & **Traffic** & **Electricity** & **Weather** & **Illness** \\  Channels & 7 & 7 & 862 & 321 & 21 & 7 \\ Sampling-Rate & 1 Hour & 15 Min. & 1 Hour & 1 Hour & 10 Min. & 1 Week \\ Timesteps & 17,420 & 69,680 & 17,544 & 26,304 & 52,696 & 966 \\   

Table 1: Statistics for all datasets used in reference methods [50; 22; 15].

  
**Method** & Base & Learnable & Positional & Align Word & \\  & Model & LM Parameters & Embeddings & Embeddings & Multimodal \\  OneFitsAll  & GPT-2 & Add\&Norm & Fine-Tune & \(\) & \(\) \\ Time-LLM  & LLaMA & None & Freeze & \(\) & \(\) \\ CALF  & GPT-2 & LoRA & Fine-Tune & \(\) & \(\) \\   

Table 2: Three popular methods for time series forecasting with Large Language Models.

### Proposed Ablations

To isolate the influence of the LLM in an LLM-based forecaster, we propose three ablations: removing the LLM component or replacing it with a simple block. Specifically, for each of the three methods we make the following three modifications:

* **w/o LLM** (Figure 1 (b)). We remove the language model entirely, instead passing the input tokens directly to the reference method's final layer.
* **LLM2Attn** (Figure 1 (c)). We replace the language model with a single randomly-initialized multi-head attention layer.
* **LLM2Trsf** (Figure 1 (d)). We replace the language model with a single randomly-initialized transformer block.

In the above ablations, we keep left parts of the forecasters unchanged (trainable). For example, as shown in Figure 1 (a), after removing the LLM, the input encodings are passed directly to the output projection. Alternatively, as shown in Figure 1 (b) or (c), after replacing the LLM with attention or a transformer, they are trained along with the remaining structure of the original method.

### Datasets and Evaluation Metrics

**Benchmark Datasets.** We evaluate on the following real-world datasets: (1) **ETT**: encompasses seven factors related to electricity transformers across four subsets: ETTh1 and ETTh2, which have hourly recordings, and ETTm1 and ETTm2, which have recordings every 15 minutes; (2) **Illness**: includes the weekly recorded influenza illness among patients from the Centers for Disease Control, which describes the ratio of patients seen with influenza-like illness to the total number of patients; (3) **Weather**: local climate data from 1,600 U.S. locations, between 2010 and 2013, and each data point consists of 11 climate features; (4) **Traffic**: is an hourly dataset from California transportation department, and consists of road occupancy rates measured on San Francisco Bay area freeways; (5) **Electricity**: contains the hourly electricity consumption of 321 customers from 2012 to 2014. The train-val-test split for ETT datasets is 60%-20%-20%, and for Illness, Weather, and Electricity datasets is 70%-10%-20% respectively. The statistics for all datasets is given in Table 1. We highlight that these datasets, with the same splits and size, have been extensively used to evaluate time-series forecasting ability of LLM-based and other neural models for time-series data [48; 50; 4; 15; 5; 46; 40; 49]. (6) **Exchange Rate**: collected between 1990 and 2016, it contains daily exchange rates for the currencies of eight countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore). (7) **Covid Deaths**: contains daily statistics of COVID-19 deaths in 266 countries and states between January and August 2020. (8) **Taxi (30 min)**: contains taxi ridges from 1,214 locations in New York City between January 2015 and January 2016. The data is collected every 30 minutes, with an average of 1,478 samples. (9) **NN5 (Daily)**: contains daily cash withdrawal data from 111 ATMs in the UK, with each ATM having

Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15; 14], while in others, they undergo fine-tuning [50; 22; 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer.

[MISSING_PAGE_FAIL:5]

nominal performance in mind. The language models in our reference methods use hundreds of millions and sometimes billions of parameters to perform time series forecasting. Even when the parameters of the language models are frozen they still contribute to substantial overhead during training and inference. For instance, Time-LLM has 6642 M parameters and takes 3003 minutes to train on the Weather dataset whereas ablation methods have only 0.245 M parameters and take 2.17 minutes on average. Information about training other methods on ETTh1 and Weather datasets are shown in Table 4. In the case of inference time, we divide by the maximum batch size to give an estimate of inference time per example. Time-LLM, OneFitsAll, and CALF take, on average, **28.2**,

    &  &  &  &  &  &  \\  & **Dataset \(\)** & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\   & ETTh1 & 0.432 & 0.417 & **0.419** & **0.405** & 0.437 & 0.422 & 0.439 & 0.429 & 0.423 & 0.408 \\  & ETTh2 & 0.396 & 0.360 & **0.333** & **0.345** & 0.389 & 0.353 & 0.394 & 0.359 & 0.383 & 0.334 \\  & ETTh1 & 0.377 & 0.356 & **0.371** & **0.350** & 0.376 & 0.356 & 0.377 & 0.359 & 0.371 & 0.329 \\  & ETTh2 & 0.315 & 0.260 & **0.307** & **0.252** & 0.314 & 0.259 & 0.310 & 0.253 & 0.329 & 0.250 \\  & Illness & 0.894 & 2.017 & 0.924 & 1.956 & 0.849 & **1.798** & **0.837** & 1.795 & 0.801 & 1.435 \\  & Weather & 0.270 & 0.243 & 0.272 & 0.243 & 0.254 & **0.224** & **0.254** & 0.226 & 0.257 & 0.225 \\  & Traffic & 0.281 & 0.421 & 0.295 & 0.428 & 0.276 & 0.416 & **0.275** & **0.416** & 0.263 & 0.387 \\  & Electricity & 0.259 & 0.164 & 0.269 & 0.171 & 0.260 & 0.167 & **0.254** & **0.161** & 0.252 & 0.158 \\  & Exchange Rate & 0.448 & 0.422 & **0.413** & **0.384** & 0.432 & 0.403 & 0.442 & 0.422 & - & - \\  & Covid Deaths & 0.089 & 1.898 & 0.080 & 0.198 & 0.058 & 0.086 & **0.054** & **0.079** & - & - \\  & Taxi (30 Min) & 0.277 & 0.163 & 0.286 & 0.176 & 0.269 & 0.157 & **0.255** & **0.141** & - & - \\  & NNS Daily & 0.432 & 0.402 & 0.425 & 0.437 & 0.379 & 0.411 & 0.364 & **0.401** & **0.347** & - & - \\  & FRED-MD & 0.0004 & 5e-7 & **0.0002** & **3e-7** & 0.0046 & 2.53e-5 & 0.0008 & 2.6e-6 & - & - \\   &  &  &  &  \\
**\#Parameters** & 6651.82M & 0.55M & 0.55M & 0.66M & & & \\    **Model \(\)** \\ **Dataset \(\)** \\  } &  &  &  &  &  \\  & **Dataset \(\)** & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\   & ETTh1 & 0.431 & 0.431 & **0.428** & 0.436 & 0.430 & **0.428** & 0.430 & 0.430 & 0.428 & 0.432 \\  & ETTh2 & 0.383 & 0.351 & 0.383 & 0.352 & **0.382** & **0.349** & 0.383 & 0.350 & 0.382 & 0.349 \\  & ETTh1 & 0.391 & 0.396 & 0.390 & 0.397 & 0.390 & 0.396 & **0.390** & **0.394** & 0.390 & 0.395 \\  & ETTh2 & 0.323 & 0.283 & 0.322 & 0.282 & 0.321 & **0.281** & **0.320** & 0.281 & 0.321 & 0.281 \\  & Illness & 0.869 & 1.699 & 0.861 & 1.639 & 0.892 & 1.748 & **0.860** & **1.630** & - & \\  & Weather & **0.273** & **0.251** & 0.277 & 0.257 & 0.279 & 0.258 & 0.277 & 0.255 & 0.274 & 0.250 \\  & Traffic & 0.284 & 0.443 & 0.278 & 0.439 & 0.275 & 0.430 & **0.271** & **0.426** & 0.281 & 0.439 \\  & Electricity & 0.266 & 0.175 & 0.262 & 0.174 & 0.264 & 0.175 & **0.261** & **0.172** & 0.265 & 0.175 \\  & Exchange Rate & 0.417 & 0.388 & 0.409 & **0.367** & 0.417 & 0.389 & **0.409** & 0.367 & - & - \\  & Covid Deaths & 0.084 & 0.163 & **0.066** & 0.115 & 0.131 & 0.431 & 0.066 & **0.106** & - & - \\  & Taxi (30 Min) & **0.258** & **0.142** & 0.264 & 0.147 & 0.267 & 0.150 & 0.267 & 0.150 & - & - \\  & NNS Daily & 0.403 & 0.362 & **0.386** & **0.336** & 0.463 & 0.445 & 0.415 & 0.381 & - & - \\  & FRED-MD & 0.0012 & 2.9e-6 & **0.0011** & **2.7e-6** & 0.0015 & 4.9e-6 & 0.0017 & 4.5e-6 & - & - \\   &  &  &  &  \\
**\#Parameters** & 1802.52M & 0.817M & 10.5M & 13.68M & & & \\    **Model \(\)** \\ **Dataset \(\)** \\  } &  &  &  &  &  \\  & **Dataset \(\)** & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\   & ETTh1 & **0.420** & 0.417 & 0.422 & **0.417** & 0.452 & 0.465 & 0.474 & 0.525 & 0.426 & 0.427 \\  & ETTh2 & **0.388** & **0.353** &

**2.3**, and **1.2** times longer than the modified models. Examples can be seen in Figure 3, where the green marks (ablation methods) are typically below the red one (LLM) and are positioned towards the left of the axis, indicating a lower computational costs and better forecasting performance. Other datasets and MSE metric refer to Figure 7 and Figure 8 in Appendix. In conclusion, the computational intensity of LLMs in time series forecasting tasks does not result in a corresponding performance improvement.

### Does language model pretraining help performance on forecasting tasks? (RQ3)

Our evaluation in this section indicates that **pretraining with language datasets is unnecessary for time series forecasting.** To test whether the knowledge learned during pretraining meaningfully improves forecasting performance we experimented with different combinations of pretraining and finetuning CALF's  language model on time series.

* **Pretrain + Finetune (Pre+FT)**. This is the original method, wherein a pretrained language model is finetuned on time series data. In the case of CALF, the base language model is frozen and low rank adapters (LoRA) are learned.
* **Random Initialization + Finetune (woPre+FT)**. Does the textual knowledge from pretraining aid time series forecasting? In this method we randomly initialize the weights of the language model (thereby erasing any effect of pretraining) and train the LLM from scratch.
* **Pretrain + No Finetuning (Pre+woFT)**. How much does finetuning on time series improve prediction performance? For this baseline we again leave the language model frozen and forgo

    &  &  &  &  \\  & & \# Param (M) & Time (min) & \# Param (M) & Time (min) & \# Param (M) & Time (min) \\   & **w/ LLM** & 6652 & 181 & 85 & 7.36 & 180 & 3.28 \\  & **w/o LLM** & 0.198 & 0.99 & 3 & 0.27 & 8 & 0.35 \\  & **LLM2Attn** & 0.202 & 1.41 & 5 & 0.70 & 10 & 0.37 \\  & **LLM2Trsf** & 0.336 & 0.84 & 8 & 0.64 & 13 & 0.40 \\   & **w/ LLM** & 6642 & 3003 & 86 & 152 & 180 & 12 \\  & **w/o LLM** & 0.198 & 1.91 & 4 & 16 & 8 & 2.32 \\   & **LLM2Attn** & 0.202 & 2.22 & 7 & 21 & 10 & 2.14 \\   & **LLM2Trsf** & 0.336 & 2.38 & 10 & 24 & 13 & 1.89 \\   

Table 4: In time series tasks, LLM (LLaMA and GPT-2) significantly increases training time. The table shows the number of model parameters (in millions) and total training time (in minutes) for three methods predicting over a length of 96 on ETTh1 and Weather data. Compared with original method “**w/ LLM**” are “**w/o LLM**”, “**LLM2Attn**” and “**LLM2Trsf**”.

Figure 3: Ablation methods consume less time for inference while providing better forecasting performance. The figure above shows the inference time and prediction accuracy of Time-LLM, OneFitsAll, and CALF on ETTm2, Traffic, and Electricity datasets, averaged across prediction lengths. For more datasets and MSE metrics refer to Figure 7 and Figure 8 in the Appendix.

learning LoRAs. Results from this model are therefore indicative of the base language model's performance without additional guidance on processing time series.
* **Random Initialization + No Finetuning (woPre+woFT)**. This baseline is effectively a random projection from the input time series to a forecasting prediction and serves as a baseline comparison with the other methods.

Overall, as shown in Table 5, across 8 datasets using MAE and MSE metrics, the "Pretraining + Finetune" method performed the best 3 times, while "Random Initialization + Finetune" achieved this 8 times. This indicates that language knowledge offers very limited help for forecasting. However, "Pretrain + No Finetuning" and the baseline "Random Initialization + No Finetuning" performed the best 5 times and 0 times, respectively, suggesting that Language knowledge does not contribute meaningfully during the finetuning process. Detailed results refer to Table 20 in Appendix.

In summary, textual knowledge from pretraining provides very limited aids for time series forecasting.

### Do LLMs represent sequential dependencies in time series? (RQ4)

Most time series forecasting methods that use LLMs finetune the positional encoding to help understand the position of time steps in the sequence . We would expect a time series model with good positional representations to show a significant drop in predictive performance when the input is shuffled . We applied three types of shuffling to the time series: shuffling the entire sequence randomly ("sf-all"), shuffling only the first half of the sequence ("sf-half"), and swapping the first and second halves of the sequence ("ex-half"). As shown in Table 6, **LLM-based methods were no more vulnerable to input shuffling than their ablations.** This implies that LLMs do not have unique capabilities for representing sequential dependencies in time series.

    &  &  \\ Input Ablation & Sf-all. & Sf-half. & Ex-half & Masking & Sf-all. & Sf-half. & Ex-half & Masking \\  Time-LLM & 51.8\% & 5.6\% & 79.6\% & 32.5\% & 99.0\% & 33.6\% & 34.9\% & 64.6\% \\
**w/o LLM** & 56.0\% & 4.5\% & 89.7\% & 39.5\% & 76.5\% & 20.9\% & 18.4\% & 53.0\% \\
**LLM2Attn** & 53.8\% & 3.3\% & 92.2\% & 33.8\% & 72.7\% & 20.4\% & 13.1\% & 44.6\% \\
**LLM2Trsf** & 50.3\% & 3.4\% & 89.2\% & 34.8\% & 74.5\% & 23.0\% & 14.3\% & 49.3\% \\  OneFitsAll & 62.1\% & 6.1\% & 16.6\% & 31.3\% & 86.2\% & 30.9\% & 36.7\% & 77.5\% \\
**w/o LLM2Attn** & 58.6\% & 6.1\% & 19.2\% & 36.1\% & 68.9\% & 13.0\% & 17.3\% & 43.5\% \\
**LLM2Attn** & 68.5\% & 9.0\% & 15.0\% & 34.4\% & 108.3\% & 39.8\% & 44.2\% & 74.2\% \\
**LLM2Trsf** & 58.0\% & 7.8\% & 12.6\% & 30.2\% & 90.8\% & 27.4\% & 40.3\% & 60.6\% \\  CALF & 50.5\% & 9.6\% & 5.6\% & 8.5\% & 113.0\% & 47.4\% & 24.4\% & 22.9\% \\
**w/o LLM** & 56.2\% & 12.1\% & 6.1\% & 10.4\% & 118.0\% & 50.4\% & 45.8\% & 28.9\% \\
**LLM2Attn** & 51.9\% & 10.8\% & 5.8\% & 7.3\% & 87.3\% & 42.4\% & 35.1\% & 25.8\% \\
**LLM2Trsf** & 50.3\% & 8.5\% & 5.5\% & 7.0\% & 102.6\% & 56.2\% & 32.6\% & 26.0\% \\   

Table 6: For the input shuffling/masking experiments on ETH1 (predict length is 96) and Illness (predict length is 24), the impact of shuffling the input on the degradation of time series forecasting performance does not change significantly before and after model modifications. Results of other predict lengths refer to table 21 in Appendix.

  
**Methods** & **Pre+FT (GPT-2)** & **woPre+FT** & **Pre+woFT** & **woPre+woFT** \\  & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\  ETH1 & 0.4312 & **0.4313** & 0.4284 & 0.4362 & **0.4267** & 0.4342 & 0.4365 & 0.4474 \\ ETH2 & 0.3838 & 0.3510 & 0.3839 & **0.3508** & **0.3830** & 0.3514 & 0.3872 & 0.3554 \\ ETH1 & 0.3910 & 0.3963 & 0.3933 & 0.4013 & **0.3898** & **0.3954** & 0.3949 & 0.4028 \\ ETTm2 & 0.3230 & 0.2831 & **0.3221** & 0.2852 & 0.3221 & **0.2827** & 0.3224 & 0.2829 \\ Illness & 0.8691 & 1.6996 & **0.8523** & **1.6146** & 0.8742 & 1.6640 & 0.8663 & 1.6381 \\ Weather & **0.2377** & **0.2510** & 0.2760 & 0.2520 & 0.2771 & 0.2535 & 0.2776 & 0.2582 \\ Traffic & 0.2844 & 0.4438 & **0.2771** & **0.4409** & 0.2820 & 0.4446 & 0.2863 & 0.4483 \\ Electricity & 0.2660 & 0.1758 & **0.2597** & **0.1669** & 0.2635 & 0.1730 & 0.2663 & 0.1784 \\  
**\# Wins:** & **3** & **8** & **5** & **0** & \\   

Table 5: Randomly initializing LLM parameters and training from scratch (woPre) achieved better results than using a pretrained (Pre) model. “woFT” and “FT” refer to whether the LLM parameters are frozen or trainable.

### Do LLMs help with few-shot learning in forecasting? (RQ5)

In this section, our evaluation demonstrates that LLMs are still **not meaningfully useful in few-shot learning scenarios.**

While our results indicate that LLMs are not useful for time series forecasting, it is nonetheless possible that knowledge encoded in pretrained weights could help performance in few-shot settings where data are scarce. To evaluate whether this is the case we trained models and their ablations on 10% of each dataset. Specifically, we evaluated LLaMA in Time-LLM methods. The results for LLaMA, shown in Table 7, compared LLaMA with completely removing the LLM (**w/o LLM**). There was no difference, with each performing better in 8 cases. We conducted similar experiments with CALF, a GPT-2-based method. Our results in Table 8 indicate that our ablations can perform better than LLMs in few-shot scenarios.

### Where does the performance come from? (RQ6)

In this section, we evaluate common encoding techniques used in LLM time series models. We find that combining **patching with one-layer attention is a simple and effective choice.**

In subsection 3.2, we found that simple ablations of LLM-based methods did not decrease performance. To understand why such simple methods work so well we selected some popular techniques used for encoding in LLM time series tasks, such as patching [50; 15; 5; 32; 22; 11], decomposition [4; 28]. A basic transformer block also can be used to aid in encoding .

The specific results, shown in Table 18 in the Appendix, indicate that a structure combining patching and attention, named "PAttn", performs better than most other encoding methods on small datasets (with time stamps less than 1 million) and is even comparable to LLM methods. Its detailed structure, as shown in Figure 4, involves applying "instance norm" to the time series, followed by patching and projection. Then, one-layer

  
**Model** &  &  &  &  \\  & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\  ETH1 & **0.522** & **0.555** & 0.543 & 0.621 & 0.547 & 0.611 & 0.566 & 0.656 \\ ETH2 & **0.394** & **0.371** & 0.432 & 0.407 & 0.437 & 0.416 & 0.433 & 0.412 \\ ETTm1 & 0.426 & 0.404 & **0.403** & **0.393** & 0.428 & 0.440 & 0.438 & 0.457 \\ ETTm2 & 0.323 & 0.277 & **0.319** & **0.269** & 0.355 & 0.316 & 0.351 & 0.311 \\ Weather & 0.273 & **0.234** & **0.271** & 0.241 & 0.275 & 0.241 & 0.271 & 0.239 \\ Traffic & 0.306 & **0.429** & 0.303 & 0.431 & 0.302 & 0.432 & **0.298** & 0.432 \\ Electricity & **0.270** & **0.175** & 0.275 & **0.175** & 0.273 & 0.179 & 0.274 & 0.181 \\ 
**\# Wins:** &  &  &  &  \\   

Table 7: In few-shot scenarios (10% dataset), LLaMA (Time-LLM) performs similarly to the ablation methods. LLaMA and “**w/o LLM**” each outperformed the other 8 times. Note that the results of Time-LLM is from the original paper .

Figure 4: PAttn Model.

  
**Model** &  &  &  &  \\  & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\  ETH1 & 0.543 & 0.649 & **0.542** & **0.642** & 0.545 & 0.646 & 0.544 & 0.643 \\ ETTh2 & 0.433 & 0.434 & **0.429** & **0.427** & 0.433 & 0.431 & 0.432 & 0.434 \\ ETTm1 & 0.500 & 0.574 & **0.499** & **0.572** & 0.503 & 0.581 & 0.507 & 0.586 \\ ETTm2 & 0.339 & 0.304 & **0.338** & **0.303** & 0.340 & 0.305 & 0.340 & 0.305 \\ Weather & **0.286** & **0.263** & 0.287 & 0.265 & 0.293 & 0.270 & 0.286 & 0.264 \\ Traffic & 0.369 & 0.571 & 0.337 & 0.517 & 0.341 & 0.518 & **0.333** & **0.510** \\ Electricity & 0.301 & 0.220 & **0.287** & **0.205** & 0.292 & 0.208 & 0.290 & 0.206 \\ 
**\# Wins:** &  &  &  &  \\   

Table 8: In few-shot scenarios (10% dataset), Ablation methods perform much better than GPT-2 (CALF). Without LLMs, 12 out of 14 cases showed better performance.

attention enables feature learning between patches. For larger datasets, such as Traffic (\(\)15 million) and Electricity (\(\)8 million), a model named "LTrsf," using the encoder from CALF , performs better. In those methods, finally, time series embedding will be projected with a single linear layer to forecast. Details of other encoders is in Appendix subsection D.3.

Overall, patching plays a crucial role in encoding. Additionally, basic Attention and Transformer blocks also effectively aid in encoding.

## 5 Conclusion

In this paper we showed that despite the recent popularity of LLMs in time series forecasting they do not appear to meaningfully improve performance. We experimented with simple ablations, showing that they maintain or improve the performance of the LLM-based counterparts while requiring considerably less compute. Once more, our goal is not to suggest that LLMs have no place in time series analysis. To do so would likely prove to be a shortsighted claim. Rather, we suggest that the community should dedicate more focus to the exciting tasks could be unlocked by LLMs at the interface of time series and language such as time series reasoning [25; 7; 45; 37], or social understanding .

## 6 Acknowledgements

We thank the University of Virginia's Research Computing team for maintaining the excellent high-performance computing resources that allowed us to conduct this research. This research was supported in part by NSF IIS-1901386, NSF CAREER IIS-2142794, NIH R01MH125179, the Bill & Melinda Gates Foundation (INV-004841), the Office of Naval Research (#N00014-21-1-2154), and the National Security Data & Policy Institute, Contracting Activity #2024-24070100001.