# Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective

Zhiding Liu1,2, Mingyue Cheng1,2, Zhi Li3, Zhenya Huang1,2, Qi Liu1,2,

**Yanhu Xie4, Enhong Chen1,2**

###### Abstract

Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to **SAN**, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available2.

## 1 Introduction

Time series forecasting is becoming increasingly prevalent in real-world scenarios. Various applications have been facilitated by the advancement of forecasting, such as energy consumption planning , clinical healthcare analysis , financial risk assessment  and cloud resource allocation . Recently, deep learning-based methods have largely advanced forecasting and other tasks due to their powerful capacity to capture sequence dependence .

Nevertheless, it is still challenging to make accurate predictions for time series forecasting due to the rapid evolution of time series points over time (a.k.a. non-stationarity of time series) . Suchnon-stationarity can lead to discrepancies between different time spans and hinder the generalization of deep learning models. To alleviate the impact of the non-stationary nature, removing these dynamic factors from the original data through normalization has been proposed as a feasible solution .

Recently, some pioneering efforts have been devoted to this research topic [17; 25; 10]. Although these normalization approaches have significantly improved the prediction performance, we identify two limitations in existing solutions. On the one hand, most existing methods overlook the distribution discrepancy between the input series and the horizon series, and simply adopt the statistical properties of the input series to denormalize the output results. Furthermore, previous studies assume that all time points within the same instance share the same statistical properties during the normalization processing, and a global instance normalization is widely applied. Such coarse-grained settings are not appropriate since time series points rapidly change over time [6; 20], particularly in long-term forecasting scenarios where both input and horizon series may span a considerable duration. For example, there may be sudden changes in data distribution due to events like holidays or temperature spikes for electricity consumption data.

We plot a forecasting sample in Fig. 1 to better illustrate our opinion. Though temporally related, the input series' mean differs from the horizon's significantly (from 0.75 to 1.5), indicating a potentially universal distribution discrepancy. Moreover, such a distribution shift can happen rapidly at a more fine-grained slice level, violating the basic assumption of existing normalization methods. Therefore, these approaches risk damaging instinct patterns of each slice of the input sequence by _normalizing with improper statistics_ while also causing a prediction shift in final forecasting results due to _poor estimation of future statistics_.

To overcome these limitations, we propose a general normalization framework for non-stationary time series forecasting named **S**licing **A**daptive **N**ormalization (SAN). **SAN models the non-stationarity in the fine-grained _temporal slices_, or patches, which are more informative than single data points and can be regarded as fundamental units of the time series data[4; 14]. To be specific, the input sequence is first split into non-overlap equally-sized slices, which are then normalized according to their statistics and fed into the forecasting model. Meanwhile, we use a statistics prediction module to predict the distributions of future slices based on the statistics of the input. Finally, the non-stationary information is restored to the output of the forecasting model with well-estimated statistics. By modeling the slice-level characteristic, SAN is able to remove the non-stationarity in a local region. Besides, with the statistics prediction module independently modeling the evolving trends of statistical properties, SAN adopts more precise statistics for adaptive denormalization. Consequently, the non-stationary forecasting task is actually simplified by being split into statistic prediction and stationary forecasting. Moreover, SAN is a model-agnostic framework and can be applied to arbitrary forecasting models. Sufficient experiments have been conducted in a widely used benchmark dataset. The results demonstrate that SAN surpasses advanced normalization approaches by boosting the performance of various kinds of mainstream forecasting models by a large margin.

In summary, our main contributions are as follows:

* We propose SAN, a general normalization framework for non-stationary time series forecasting tasks which distinguishes by modeling the non-stationary nature from a temporal slice perspective. In this way, SAN can better remove the non-stationary factors in input sequences while keeping their distinct patterns.

Figure 1: An illustration of a forecasting instance in energy consumption along with its daily mean (Mean-ByDay). We also plot the input series’ mean and the horizon series’ mean in the figure.

* We design a flexible statistics prediction module for SAN which independently models the evolving trends of statistical properties. By explicitly learning to estimate future distributions, SAN can simplify the non-stationary forecasting task through divide and conquer.
* We conduct sufficient experiments on nine real-world datasets. Results show that SAN can be applied to various mainstream forecasting models and boost the performance by a large margin. Moreover, the comparison between SAN and state-of-the-art normalization methods demonstrates the superiority of our proposed framework.

## 2 Related Works

### Time Series Forecasting

Time series forecasting has been extensively studied in recent decades. Originally, ARIMA [1; 40] builds an auto-regressive model and forecasts in a moving average fashion. Though theoretical guarantees are achieved, such traditional methods usually require data with ideal properties, which is inconsistent with the real-world scenario. With the increasing data availability and computing power, numerous deep-learning-based models have emerged, which always follow a sequence-to-sequence paradigm. Recurrent neural networks (RNNs) are first utilized to capture the temporal dependence by summarizing the past information in time series [30; 32; 37]. Such architectures naturally suffer from a limited reception field and an error accumulation issue caused by the recursive inference schema, both dragging down forecasting precision. To further boost the performance of the final prediction, many advanced architectures have been introduced to capture the long-range dependencies, such as the self-attention mechanism and the convolutional networks [19; 21; 24]. Besides, to leverage the characteristics of the time series data, recent works also integrate traditional analysis methods like trend-seasonal decomposition and time-frequency conversion into neural networks [36; 44]. In addition, a recent study points out that a simple linear network enhanced with decomposition also achieves competitive performance. Furthermore, slice-based methods demonstrate superior accuracy in the long time series forecasting tasks [26; 41].

### Non-stationary Time Series Forecasting

Most time series forecasting methods prioritize designing powerful architectures that can effectively capture temporal dependencies, but often overlook the non-stationary nature of the data. Considering the basic assumption of deep-learning-based models that the data in both training and test sets follow the same distribution, such a discrepancy will definitely drag down the precision of the model for future time prediction. Moreover, the distribution differences among instances in the training set may introduce noise, making the learning task harder to converge. To address these challenges, various stationarization methods have been explored.

In detail, DDG-DA predicts the evolving data distribution in a domain adaptation fashion. Du et al. propose an adaptive RNN to alleviate the impact of non-stationary factors by distribution characterization and distribution matching. Besides, normalization-based approaches have also gained popularity as they aim to remove non-stationary factors from original data and normalize all data to a consistent distribution. DAIN  introduces a non-linear network to learn how to normalize each input instance adaptively and ST-norm  proposes two normalization modules from both temporal and spatial perspectives. Later researchers point out that non-stationary factors are essential in accurate forecasting and simply removing them may result in poor prediction. Therefore, they propose RevIN , a symmetric normalization method that first normalizes the input sequences and then denormalizes the model output sequences through instance normalization . Based on the similar structure, Non-stationary Transformers  presents de-stationary attention that incorporates the non-stationary factors in self-attention, resulting in significant improvements over Transformer-based models. Moreover, a recent study  identifies the intra- and inter-space distribution shift in time series, and proposes to relieve these issues by learning the distribution coefficients.

Despite the effectiveness of existing normalization methods, they inappropriately assume that all time points within the same instance share the same statistical properties during the normalization processing. Different from them, our proposed approach focuses on further thinking of the nature of the data, i.e., the distribution is inconsistent across compact time slices and such inconsistency is not just on a per-instance basis.

Proposed Method

We propose a general model-agnostic normalization framework for time series forecasting called Slicing Adaptive Normalization (SAN) to address the inconsistency mentioned above. Considering an input set of time series \(X=\{^{i}\}_{i=1}^{N}\) and their horizon series \(Y=\{^{i}\}_{i=1}^{N}\), SAN is expected to remove the non-stationary factors and assist the forecasting models to predict more accurately based on the observed input series. In this section, we will present the detailed workflow of the entire framework and explain how it works with non-stationary time series data. To provide better clarity, we summarize the key notations in Table 1 and the whole framework can be referred to in Fig. 2.

### Normalization

Similar to existing normalization methods for non-stationary time series forecasting , SAN first normalizes the input sequence to remove the non-stationary factors and later restores them to the output sequence by denormalization. Differently, SAN applies such operation on a per-slice basis instead of the whole input sequence. Such a localized operation can better maintain the instinct pattern of each slice than global instance normalization. The framework first splits the input \(^{i}\) into \(M\) non-overlapping slices \(\{^{i}_{j}\}_{j=1}^{M}\) based on \(T\). Then the mean and standard deviation for each slice is computed as:

\[^{i}_{j}=_{t=1}^{T}^{i}_{j,t},(^{i}_{j})^{2}= _{t=1}^{T}(^{i}_{j,t}-^{i}_{j})^{2},\] (1)

where \(^{i}_{j},^{i}_{j} R^{V*1}\) and \(^{i}_{j,t}\) is the value of slice \(^{i}_{j}\) at \(t\)-th time step. Later, SAN normalizes every slice of the original input sequence by their individual statistics as:

\[}^{i}_{j}=_{j}+}(^{i}_{j}- ^{i}_{j}).\] (2)

Here we use \(\) to denote the element-wise product and \(\) is a small constant. Finally, SAN restores all the slices in their original chronological order and lets the processed series without non-stationary factors \(}^{i}\) be the new input of the forecasting models.

### Statistics Prediction

As illustrated in Fig. 2, SAN introduces a unique statistics prediction module, \(f_{}(*)\), to better estimate future distributions in addition to the backbone forecasting model, \(g_{}(*)\). Unlike existing works that denormalize the entire output of backbone models with the statistics of original input sequences, SAN faces a natural challenge of per-slice normalization: how to **estimate the evolving distributions** for each future slice. To simplify and improve efficiency, we use a two-layer perceptron network with an appropriate activation function (e.g., Relu() for standard deviation to ensure non-negativity) that learns to predict future distributions based on input statistics and stationarized sequence.

The quality of statistics predictions determines the overall performance of SAN since we depend on an accurate estimation of future distribution to restore the non-stationary nature of each instance. In

   Notation &  \\  \(N\) & the number of instances \\ \(V\) & the number of variables \\ \(T\) & the given slicing time span \\ \(L_{in}\),\(L_{out}\) & the sequence length of input/target sequence \\ \(M,K\) & the number of slices of input/target sequence, \(M=}{T},K=}{T}\) \\ \(^{i},^{i}\) & the i-th input/target series for the whole framework \\ \(}^{i},}^{i}\) & the i-th input/target series for forecasting models \\ \(*^{i}_{j}\) & the property of j-th slice in the i-th series, determined by * \\ \(,\) & the mean and the standard deviation value \\ \(\) & the predicted value, determined by * \\   

Table 1: The Key Mathematical Notations.

our modeling of distribution, the mean determines the approximate scale of a given slice and the standard deviation represents the degree of dispersion, where the scale of a small slice may be more important in the task of forecasting. Therefore, we aim to further refine the modeling approach for the mean component on the basis of analyzing its properties.

In detail, we believe that the overall mean of the input sequence \(^{i}=}_{1}^{L_{in}}^{i} R^{V*1}\) is a _maximum likelihood estimation_ of the target sequence's mean \(^{i}=}_{1}^{L_{out}}^{i} R^{V*1}\) since they are temporally related. That is, \(^{i}^{i}\). Such property is widely accepted in existing works [17; 25] as they denormalize the output using the statistics of the whole input sequence. Based on the above assumption, we introduce a _residual learning_ technique in our method, letting the module learn the difference between the future slice mean \(}^{i}\) and the overall input mean \(^{i}\), instead of predicting the specific values. This approach reduces the difficulty in modeling means with prior knowledge about future trends. Additionally, to account for different variables exhibiting distinct patterns in scale changes, we further use two learnable vectors \(_{1},_{2} R^{V}\) initialized as ones-vector to present the _individual preference_ weights for each variable, making the prediction computed in a weighted-sum manner. The statistics prediction procedure can be formulated as:

\[}^{i}=_{1}*(^{i}-^{i},}^{i }-^{i})+_{2}*^{i},}^{i}=(^ {i},}^{i}).\] (3)

Here \(^{i}=[^{i}_{1},^{i}_{2}...^{i}_{M}] R^{V*M}\) denotes all the mean values of \(M\) slices of the input, and \(}^{i} R^{V*K}\) stands for the predicted mean of future \(K\) slices. The same notation works for the standard deviation. The mean squared error (MSE) between predicted statistics and ground truth is utilized as the loss function (\(l_{sp}\)) to train the network through backpropagation.

In this work, we mainly focus on proposing and modeling the non-stationary nature of time series from a slicing perspective. The challenge of how to design powerful deep models for statistics prediction is left for future explorations.

### Denormalization

Simultaneous with statistic prediction, SAN feeds the normalized sequence into the forecasting model, which is responsible for producing internal output \(}^{i}\). Finally, SAN denormalizes the output given by the backbone, restoring the non-stationary factors for an accurate forecasting result.

Figure 2: The illustration of the proposed SAN framework. SAN is a model-agnostic symmetrical normalization framework that removes and restores the non-stationary factors in the time series data from the perspective of slicing. SAN trains in a two-stage manner. It first optimizes the statistics prediction module into convergence (\(l_{sp}\)), which learns to predict future statistics based on sliced input mean \(^{i}\) and standard deviation \(^{i}\). The second stage is the traditional procedure of training forecasting models (\(l_{fc}\)), where the statistics prediction module is frozen and functions as a plugin.

Symmetrically, SAN performs on a per-slice basis as illustrated in Fig. 2. For the internal output \(}^{i}\), we first split it into \(K\) non-overlapping slices \(\{}^{i}_{j}\}_{j=1}^{K}\). Then the denormalization operation for an arbitrary slice based on our predicted statistics can be defined as the following formula:

\[}^{i}_{j}=}^{i}_{j}*(}^{i}_{j}+) +}^{i}_{j}.\] (4)

Finally, by restoring all the slices in their chronological order, we can get the final prediction \(}^{i}\) of the whole framework, which will be later used for loss computation (\(l_{fc}\)) and performance evaluation.

### Two-stage Training Schema

Though the overall framework is simple and clear, we find that the training process needs to be carefully deliberated. Since the normalization flow of SAN functions as a constraint to the backbone model, the overall learning procedure is actually a _bi-level optimization problem_. The goal of the upper level is the performance of time series forecasting while the goal of the lower level is the distribution similarity between denormalized output and ground truth. Formally, the original overall training process can be described as:

\[_{}&_{(^{i}, ^{i})}l_{fc}(,^{*},(^{i},^{i})),\\ &^{*}=_{}_{(^{ i},^{i})}l_{sp}(,,(^{i},^{i})).\] (5)

Here we omit the transformation process of the data and only keep the original input required for the calculation for brevity.

We propose a two-stage training paradigm for SAN by simplifying the lower-level optimization objective so that it can focus on estimating the future distribution, instead of reducing the distribution discrepancy between the denormalized output of a certain model and the ground truth. Specifically, we optimize \(^{*}=_{}_{(^{i},^{i})}l_{sp}(,(^{i}, ^{i}))\) using stochastic gradient descent. This decouples the original non-stationary forecasting task into a statistic prediction task and a stationary forecasting task. In practice, the statistics prediction module is first trained into convergence, which is then frozen and treated as a plugin during the second stage of training the forecasting model. The training algorithm is provided in the Appendix C.2.

Such a solution has some desirable qualities: The first is simplicity. The two-stage schema allows for a concise and easy-to-follow design of the model architecture and training process. The second is effectiveness. The statistics prediction module is expected to produce reliable predictions on future distribution since it is optimized on the whole training set into convergence. Therefore the forecasting model can handle the simpler task of learning the scale-free pattern in the normalized data. These two advantages greatly meet our ultimate goal of designing a concise yet effective framework for non-stationary time series forecasting tasks. The third and the most important is flexibility. Though there exist many advanced methods for the bi-level optimization problem [5; 11], their upper and lower objectives are always highly related. In contrast, our proposal completely decouples these parts, making SAN a model-agnostic framework that can migrate to various scenarios without special design and further tuning.

## 4 Experiments

In this section, we conduct sufficient experiments within a widely used benchmark dataset compared to state-of-the-art methods to evidence the effectiveness of our proposed SAN framework.

### Experimental Setup

DatasetsWe use nine datasets in our experiment and here are brief descriptions of them. (1) \(\)3 records the oil temperature and load features of the electricity transformers from July 2016 to July 2018. It is made up of 4 sub datasets where ETThs are sampled per hour and ETTms are sampled every 15 minutes. (2) **Electricity4** contains the electricity consumption data of 321 clients from July 2016 to July 2019. (3) **Exchange5** collects the daily exchange rates of 8 countries from 1990 to 2016. (4) **Traffic6** includes the hourly traffic load of San Francisco freeways recorded by 862 sensors from 2015 to 2016. (5) **Weather7** is made up of 21 indicators of weather, including air temperature and humidity collected every 10 minutes in 2021. (6) **ILF8** records the weekly ratio of influenza-like illness patients versus the total patients by the Centers for Disease Control and Prevention of the United States from 2002 to 2021. The detailed information about these datasets are listed in the Table. 2. We also report the ADF test (Augmented Dickey-Fuller Test)  results in the table, which evaluate the stationarity of a time series. Following the standard protocol, we split each dataset into training, validation and testing sets according to the chronological order. The split ratio is 6:2:2 for ETT dataset and 7:1:2 for the other datasets . Also, we apply a z-score normalization on them based on the statistics of training data as preprocessing to measure different variables on the same scale. Note that z-score normalization is unable to handle non-stationary time series since the statistics are fixed during normalization .

Backbone modelsSAN is a model-agnostic framework that can be applied to arbitrary time series forecasting models. To evidence the effectiveness of the framework, we select some mainstream models based on different architectures and evaluate their performance under both multivariate and univariate settings: Linear model based **DLinear**, Transformer based **Autoformer** and **FEDformer**, and dilated convolution based **SCINet**. We follow the implementation and settings provided in the official code of DLinear9 and SCINet10 to implement these models.

Experiments detailsWe use ADAM  as the default optimizer across all the experiments and report the mean squared error (MSE) and mean absolute error (MAE) as the evaluation metrics. A lower MSE/MAE indicates a better performance. For the statistics prediction module in SAN, we use a simple two-layer perceptron network with a hidden size the same as the embedding size of the backbone model for simplicity. The detailed implementation of the statistics prediction module can be referred to in Appendix C.1. All the experiments are implemented by PyTorch  and are conducted for three runs with a fixed random seed on a single NVIDIA RTX 3090 24GB GPU.

Slicing lengthRegarding the selection of slicing length for each dataset, we adopt a heuristic idea that real-world time series data exhibit similar changing patterns within artificially defined or actual periods (daily, weekly, etc.). Combing the frequencies of benchmark datasets, we establish a range of \(\{6,12,24,48\}\) as slicing lengths such that most settings cover a meaningful time span. For example, we selected a slicing length of 24 for datasets such as ETTh1, Electricity and Traffic with a frequency of 1 hour. This ensures that each time slice contained data within a day and guarantees optimal performance among candidates. Here we admit that one limitation of our method is that the current design cannot handle indivisible length such that we set the slicing length to 6 which approximately represents a weekly period instead of 7 in the Exchange dataset. We present the ablation study on the effect of slicing length in Appendix B.5.

   Dataset & Variables & Sampling Frequency & Length & Slicing Length & ADF\({}^{*}\) \\  Electricity & 321 & 1 Hour & 26,304 & 24 & -8.44 \\ Exchange & 8 & 1 Day & 7,588 & 6 & -1.90 \\ Traffic & 862 & 1 Hour & 17,544 & 24 & -15.02 \\ Weather & 21 & 10 Minutes & 52,696 & 12 & -26.68 \\ ILI & 7 & 1 Week & 966 & 6 & -5.33 \\ ETTh1\&ETTh2 & 7 & 1 Hour & 17,420 & 24 & -5.918-4.13 \\ ETTm1\&ETTm2 & 7 & 15 Minutes & 69,680 & 12 & -14.98\&-5.66 \\  ^{*}\)A smaller ADF test result indicates a more stationary time series data} \\   

Table 2: The Statistics of Each Dataset.

### Main Results

We report the multivariate forecasting results in Table 3. The ILI dataset has a forecasting horizon of \(L_{out}\{24,36,48,60\}\) while the others have a forecasting horizon of \(L_{out}\{96,192,336,720\}\). As for the input sequence length, we follow the traditional protocol and fix \(L_{in}=96\) for Autoformer, FEDformer and SCINet with respect to all datasets (\(L_{in}=36\) for ILI dataset) and extend it to 336 (96 for ILI dataset) for DLinear. Full benchmarks of ETT datasets and univariate results are provided in the Appendix.

As shown in the table, we clearly find that our proposed SAN framework can boost these models by a large margin in most cases of the benchmark dataset. We attribute this improvement to two aspects. Firstly, SAN mitigates the impact of non-stationary factors, as demonstrated by the performance on three typical non-stationary datasets (Exchange, ILI and ETTh2, determined by ADF test results). Specifically, under all experimental forecasting lengths with DLinear, SAN achieves an average MSE reduction of **7.67%** in the Exchange dataset, **11.13%** in the ILI dataset and **21.29%** in the ETTh2 dataset. This conclusion applies to other backbone models as well and the enhancement is even more pronounced. Secondly, even in long-term forecasting scenarios where the difficulty of forecasting increases significantly with the length of the forecast, SAN imposes constraints on

    &  &  \\  & +SAN & +RevIN & +NST & +Dish-TS & IMP(\%) & +SAN & +RevIN & +NST & +Dish-TS & IMP(\%) \\  Electricity & **0.191** & 0.200 & 0.198 & 0.203 & 3.54 & **0.204** & 0.219 & 0.213 & 0.231 & 4.23 \\ Exchange & **0.298** & 0.474 & 0.480 & 0.704 & 37.13 & **0.297** & 0.495 & 0.494 & 1.008 & 39.88 \\ Traffic & **0.572** & 0.647 & 0.649 & 0.652 & 11.59 & **0.594** & 0.666 & 0.664 & 0.677 & 10.54 \\ Weather & 0.279 & 0.268 & **0.267** & 0.398 & -4.49 & 0.305 & **0.290** & **0.290** & 0.433 & -5.17 \\ ILI & **2.467** & 2.962 & 3.084 & 2.846 & 13.32 & **2.562** & 3.151 & 3.235 & 3.180 & 18.69 \\ ETTh1 & **0.447** & 0.463 & 0.456 & 0.461 & 1.97 & **0.518** & 0.519 & 0.521 & 0.521 & 0.19 \\ ETTh2 & **0.404** & 0.465 & 0.481 & 1.004 & 13.12 & **0.411** & 0.489 & 0.465 & 1.175 & 11.61 \\ ETTm1 & **0.377** & 0.415 & 0.411 & 0.422 & 8.27 & **0.406** & 0.562 & 0.535 & 0.567 & 24.11 \\ ETTm2 & **0.287** & 0.310 & 0.315 & 0.759 & 7.42 & **0.311** & 0.325 & 0.331 & 0.894 & 4.31 \\   

Table 4: Comparison between SAN and existing normalization approaches. The best results are highlighted in **bold**.

    &  &  &  &  & Autoformer &  &  &  \\  & Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & 96 & 0.140 & 0.237 & **0.137** & **0.234** & 0.185 & 0.300 & **0.164** & **0.272** & 0.195 & 0.309 & **0.172** & **0.281** & 0.213 & 0.316 & **0.152** & **0.256** \\  & 192 & 0.153 & 0.250 & **0.151** & **0.247** & 0.196 & 0.310 & **0.179** & **0.286** & 0.215 & 0.325 & **0.195** & **0.300** & 0.224 & 0.329 & **0.163** & **0.266** \\  & 336 & 0.168 & 0.267 & **0.166** & **0.

backbone models to produce more reliable results using a novel statistical prediction module. For instance, when predicting for a length of 720 time steps, SCINet accompanied by SAN achieves a **70.37%** reduction in MSE on the ETTh2 dataset and a **20.77%** reduction on the Electricity dataset. These improvements make SCINet comparable to other forecasting models and suggest that SAN can help stabilize outputs in long-term forecasting scenarios.

### Comparison With Normalization Methods

In this section, we compare SAN with three state-of-the-art normalization methods for non-stationary time series forecasting: **RevIN**, Non-Stationary Transformers (**NST**)  and **Dish-TS**. Following the same experimental settings in Section 4.2, we report the average MSE evaluation of Autoformer and FEDformer over all the forecasting lengths for each dataset and the relative improvements in Table 4. Other models are not as involved backbone since NST can only suit Transformer-based models, while the rest methods are much more flexible to be applied for arbitrary forecasting models on the contrary.

It can be concluded that SAN achieves the best performance among existing normalization methods. The improvement is significant with an average MSE decrease of **10.71%** by FEDformer. SAN consistently performs better than baseline models except for Weather and the improvement is more pronounced in typical non-stationary datasets like Exchange and ILI (determined by the ADF test). The comparison reveals that SAN may be more effective at removing non-stationary factors from a time-slicing perspective rather than considering the entire instance. Additionally, the proposed two-stage training schema is crucial as it enables SAN to outperform Dish-TS by a large margin, which ignores the bi-level optimization nature. However, this exceptional ability of SAN may lead to an over-stationarization problem , resulting in decreased performance on the Weather dataset. The detailed results of all cases and further discussions are provided in Appendix B.6.

### Qualitative Evaluation

The quality of prediction results in time series forecasting is crucial, in addition to the accuracy of metrics. Figure 3 displays a sample forecast on the ETTm2 dataset using FEDformer as the backbone with SAN, RevIN, NST or Dish-TS enhancements. The input length is 96 and the forecasting length is set to 336. It's evident that SAN produces more realistic predictions while its counterparts even fail to capture the scale of future data. We guess the poor quality of RevIN and NST is caused by their coarse way of denormalizing. Although the mean value of an input sequence can be considered a maximum likelihood estimation for future data, it's likely that non-stationary datasets' distribution will change significantly in comparison to inputs. Therefore, simply denormalizing output from backbone models with input sequence statistics may lead to mismatches like those seen in RevIN and NST forecasts where both scales are similar. As for Dish-TS, though the method tries to learn future distribution, it ignores the bi-level optimization nature and its entangled learning schema limits the estimation accuracy of statistics and finally leads to poor performance. On the opposite, SAN models the dynamic nature of time series from a slicing perspective and introduces an independent statistics prediction module to learn to predict the future distribution for denormalizing by a two-stage training schema. In this way, we adaptively adjust the scale and bias of forecasting results based on the statistic predictions, capturing the tendency of future data. As a result, though the average value of the input is rather low, SAN still produces higher predictions that are consistent with ground truth.

Figure 3: Visualization of long-term forecasting results of a sample of ETTm2 dataset given by FEDformer enhanced with different normalization methods.

Conclusion

In this study, we focused on alleviating the non-stationary property of time series data using a novel slice view. We proposed the SAN framework for time series forecasting, which is a model-agnostic approach that normalizes the input by removing non-stationary factors and restores them to the output through denormalization on a per-slice basis. Additionally, with the help of a novel statistics prediction module, SAN simplifies non-stationary forecasting by dividing it into two subtasks to improve forecasting model performance. To demonstrate the superiority of SAN, we conducted experiments on a widely used benchmark dataset and found that SAN significantly improves mainstream forecasting models and outperforms state-of-the-art normalization methods. We hope that SAN can serve as a foundation component for time series forecasting, and stimulate further research on modeling time series from a slice perspective.

## 6 Acknowledgement

This research was partially supported by grants from the National Natural Science Foundation of China (Grant No. U20A20229). This work also thanked to the support of funding MAI2022C007. We furthermore thanked the anonymous reviewers for their constructive comments.