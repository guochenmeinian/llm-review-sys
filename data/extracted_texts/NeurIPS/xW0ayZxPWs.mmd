# Fair Graph Distillation

Qizhang Feng\({}^{1}\), Zhimeng Jiang\({}^{1}\), Ruiquan Li\({}^{2}\), Yicheng Wang\({}^{1}\), Na Zou\({}^{1}\), Jiang Bian\({}^{3}\), Xia Hu\({}^{4}\)

\({}^{1}\)Texas A&M University, \({}^{2}\)University of Science and Technology of China,

\({}^{3}\)University of Florida, \({}^{3}\)Rice University

###### Abstract

As graph neural networks (GNNs) struggle with large-scale graphs due to high computational demands, graph data distillation promises to alleviate this issue by distilling a large real graph into a smaller distilled graph while maintaining comparable prediction performance for GNNs trained on both graphs. However, we observe that GNNs trained on distilled graphs may exhibit more severe group fairness issues than GNNs trained on real graphs for vanilla and fair GNNs training. Motivated by these observations, we propose _fair graph distillation_ (FGD), an advanced graph distillation approach to generate fair distilled graphs. The challenge lies in the deficiency of sensitive attributes for nodes in the distilled graph, making most debiasing methods (e.g., regularization and adversarial debiasing) intractable for distilled graphs. We develop a simple yet effective bias metric, named coherence, for distilled graphs. Based on the proposed coherence metric, we introduce a framework for fair graph distillation using a bi-level optimization algorithm. Extensive experiments demonstrate that the proposed algorithm can achieve better prediction performance-fairness trade-offs across various datasets and GNN architectures.

## 1 Introduction

Real-world data, like chemical molecules, social networks, and transportation networks, can be represented as graphs . Graph neural networks (GNNs) excel at capturing structural information but struggle with large-scale graphs due to memory consumption and computational expense caused by the neighborhood explosion problem. This cost becomes unaffordable in situations requiring repeated GNN training, such as neural architecture search and continual learning . Dataset distillation is a promising solution to address computation challenges by generating small, informative distilled data for neural network training in downstream tasks . Techniques like dataset condensation  can significantly reduce training data size without major performance degradation in the image and graph domains. However, focusing solely on prediction performance may introduce fairness issues, as sensitive information can be condensed into distilled data for prediction. A natural question is raised: _Is the model trained on the distilled graph fair, and if not, how can we achieve fair graph distillation?_

In this work, we focus on the group fairness1 for node classification tasks under binary sensitive attribute setting. We discover that GNNs trained on distilled small graphs exhibit more severe group fairness issues than those on real graphs. In other words, graph distillation can even _amplify_ graphdata bias, which challenges the applicability of graph distillation in high-stake applications (Mehrabi et al., 2021; Suresh and Guttag, 2019). To this end, we propose a fair graph distillation framework to offer a significantly reduced graph size and also better utility-fairness trade-off while maintaining predictive performance.

Many debias methods explicitly use sensitive attributes, but these are inherently missing in distilled graphs because they are excluded from the data attributes and the meaning of the attributes may change during the optimization process. In this paper, we point out the relationship between the space of real graphs and the space of distilled graphs and develop a simple estimator of sensitive attributes and introduce a bias measurement called consistency. We then propose a bi-level optimization algorithm for fair graph distillation: the outer loop generates a fair and informative distilled graph using gradient matching and coherence loss, while GNNs train on distilled graphs in the inner loop. In a nutshell, the contributions can be summarized as follows:

* To our knowledge, this is the first paper to identify group fairness issues in conventional graph distillation methods with binary sensitive attributes, motivating the formulation of a fair graph distillation problem in node classification tasks.
* We discover the relationship between the space of real graphs and the space of distilled graphs. We develop a bias metric called coherence for distilled graphs and propose a bi-level optimization framework using this metric to achieve fair graph distillation.
* We perform extensive experiments on various real-world datasets and GNN architectures to validate the effectiveness of the proposed FGD algorithm. Results demonstrate that FGD achieves a better accuracy-fairness trade-off compared to vanilla graph distillation methods and numerous baselines.

## 2 Preliminaries

### Notations

We consider node classification tasks given a graph dataset \(=\{,,,\}\) with \(N\) nodes. Here, \(\{0,1\}^{N N}\) is the adjacency matrix, and \(A_{ij}=1\) represents there exists an edge between node \(i\) and \(j\). \(^{N D}\) is the node feature matrix, where \(D\) is non-sensitive feature dimension for each node. \(\{0,1,,C-1\}^{N}\) denotes the node labels over \(C\) classes. For simplicity, we consider a binary sensitive attribute2\(\{0,1\}^{N}\). \(^{s}\) is the sensitive membership diagonal matrix. \(^{s}_{ii}=1\) if and only if \(i\)-th node belongs to sensitive group \(s\). The distilled small graph dataset is marked as \(^{}=\{^{},^{},^{}\}\) which contains \(N^{}\) nodes and \(N^{} N\). Note that elements of the distilled adjacency matrix satisfy \(^{}_{ij}\) and no sensitive attributes exist in \(^{}\). The latent node representation of real graph is \(\), and the span space of it is \(()_{i=1}^{N}w_{i}_{i}|1 i N,w_{i}}\). Similar definition of \(^{}\) and \((^{})\) for distilled graph.

### Graph Distillation via Gradient Matching

The purpose of graph distillation is to generate a _distilled_ graph \(^{}\) such that the GNN model, denoted as \(_{}\) with parameters \(\), trained on distilled graph performs _comparably_ to the model trained on the real graph \(\). The objective can be formulated as the following bi-level optimization problem:

\[_{^{}}(_{^{^{ }}}(,),)\ \ \ \ \ ^{^{}}=( _{}(^{},^{}),^{})\] (1)

where \(^{^{}}\) denotes the optimal parameter trained on distilled small graph \(^{}\), and \((,)\) denotes the loss function. To tackle the above optimization problem, the gradient matching method Zhao et al.

Figure 1: AUC and \(_{DP}\) of the GNN trained on real graph data and distilled graph data. Both utility and fairness performance deteriorates after vanilla graph distillation.

[2021a] is proposed. The intuition is to let the GNN parameters \(^{^{}}\) trained on distilled graph follow a similar path to the GNN parameters \(^{}\) trained on the real graph during model optimization. The gradient of the GNN parameters is forced to be the same over the real and distilled graphs:

\[_{^{}}[_{t=0}^{T-1}D(_{}(),_{}(^{}))],\] (2)

where \(D(,)\) is a distance function, \(T\) is the number of steps of model parameters trajectory, and \(_{t}^{},_{t}^{^{}}\) denotes the model parameters trained on \(\) and \(^{}\) at time step \(t\), respectively. The gradient calculated on \(\) and \(^{}\) is denoted as \(_{}()_{} (_{_{t}}(,),)\) and \(_{}(^{})_{} (_{_{t}}(^{},^{} ),^{})\), respectively.

## 3 Bias Measurement for Distilled Graph

In this section, we empirically demonstrate the fairness issue in the distilled graph. Motivated by this, we pursue fair graph distillation. Although distilled graphs lack sensitive attributes \(^{}\), we observe that between the node representations of the real and distilled graphs: their barycenter remain consistent, and their spaces are also consistent. We leverage this phenomenon to develop a simple, effective bias measurement for distilled graphs.

### Is Graph Distillation Really Fair?

Our empirical investigation assesses the fairness of graph distillation across various datasets and architectures. We compare the utility (AUC) and fairness (demographic parity (\(_{DP}\)) [Beutel et al., 2017]) of GNNs trained on real graphs and those trained on distilled graphs created by the vanilla graph distillation method. The utility and fairness performance are shown in Figure 1. We can find that: For datasets like Pokec-n, German, and Credit, distilled graph-based GNNs have higher \(_{DP}\) and lower AUC performance, suggesting a compromise in fairness and prediction performance. We also notice that, for Pokec-z and Recidivism datasets, these GNNs exhibit lower \(_{DP}\) and significantly lower AUC performance (shown in Table 1), indicating a trade-off between improved fairness and reduced prediction performance. We observe similar results when using other fair GNN models. More details can be found in Appendix E. Motivated by these observations, we aim to find a better prediction performance and fairness trade-off via chasing the fair graph distillation method.

### Geometric Connections in Data Distillation

The distilled data can be generated via minimizing gradient distance in Equation 2. To simplify the analysis, we consider \(D(,)\) as Euclidean distance and those model parameters during optimization trajectory satisfying \(_{}\), where \(_{}\) is certain but unknown parameters' distribution. Therefore, the objective can be transformed as

\[_{^{}}_{_{}} ||_{}()-_{}( ^{})||^{2}.\] (3)

We consider three assumptions of the model parameters' distribution and the convergence for loss minimization.

**Assumption 3.1** (Model parameters' distribution).: We assume that each model parameter in the last softmax layer satisfies the same distribution.

**Assumption 3.2** (Loss minimization).: We assume that exists at least one distilled dataset that minimizes Equation 3.

**Theorem 3.3** (Consistent Span Space).: _We empirically show that \((^{})()\) via calculating the principle angles between them. We also provides the rigorous proof of \(^{}()\) under distribution matching in Appendix D._

**Theorem 3.4** (Consistent Geometric Barycenters).: _Under Assumptions 3.1 and 3.2, the barycenter of the last representation for the optimal distilled graph and the real graphs are consistent, i.e. \(_{i=1}^{N}_{i}=}_{i=1}^{N^{}} _{i}^{}\). Please see proof in Appendix B._

Figure 2: Geometric intuition of sensitive attribute estimation. The projection distance indicates the extent to which the node belongs to the sensitive group. (a) Unfair node representations have a large coherence bias. (b) Fair node representations have a small coherence bias.

### Sensitive Attribute Estimation

The consistent span space and geometric barycenter suggest that we can estimate sensitive attributes from the representations of both distilled and real graphs. We frame sensitive attribute estimation as a classification problem: Given a data representation \(^{}^{}\), what is the probability that \(^{}\) belongs to the sensitive group?

Ridge regression for distance measurement.Notice that the representation of each sensitive group for the real graph is known, we define \(_{0}\) and \(_{1}\) as the representation matrix for sensitive group \(s=0\) and \(s=1\). To measure the probability that \(^{}\) belongs to these two sensitive groups, we first find the closest vector \(^{}_{proj}=^{}_{s}(Z_{s})\) to approximate the representation \(^{}\), and then use the norm of \(^{}-^{}_{proj}\) to measure the distance between \(^{}\) and sensitive group \(_{s}\). Specifically, we adopt ridge regression to find the optimal coefficient vector \(_{*}\), which can be formulated as

\[Dist(^{},_{s})=\|^{}-^{}_ {s}^{*}\|_{2}\] (4) \[^{*}=*{arg\,min}_{}\| ^{}-^{}_{s}\|_{2}^{2}+\|\|_{2}^{2},\] (5)

where \(\) is the hyperparameter for ridge regression. For the optimal \(q^{*}\), we have

\[^{s}=^{}-^{}_{s}^{*}=^{}- ^{}_{s}(+_{s}^{}_{s})^{-1}_{s}^{},\] (6)

where \(\) represents matrix transpose, \(^{s}\) is approximately the projection onto the orthogonal complement of the subspace \((_{s})\). The proof is in Appendix C.

Sensitive attribute soft estimation.Since \(^{s}=^{}-^{}_{s}(+_{s} ^{}_{s})^{-1}_{s}^{}\) can be viewed as approximately the projection of \(\) onto the orthogonal complement of sensitive group \(_{s}\), \(\|^{s}\|_{2}\) is small if \(\) is in sensitive group \(_{s}\) and large otherwise. The probability of the given data representation \(\) belongs to the sensitive group \(_{s}\) can further be inferred via a softmax function:

\[^{s}(^{})=^{s}\|_{2})}}{_{s= 0}^{1}^{s}\|_{2})}}\,,\] (7)

where \(\) is the temperature hyperparameter. The sensitive attribute probability of \(^{}\) for distilled graph can be estimated as probability distribution \([^{s=0}(^{}),^{s=1}(^{})]\), where \(^{s=0}(^{})+^{s=1}(^{})=1\).

### Bias Measurement

Given the estimated sensitive attribute probability for \(^{}\) of each distilled node, how can we measure the bias for them? For a fair representation, we can not distinguish which representation is more likely to be a specific sensitive group. Therefore, we adopt a simple surrogate bias measurement, named coherence, the variance of the estimated sensitive group membership. Given the whole distilled data representation \(^{}=[_{1}...,_{N^{}}]^{}\), the bias can be defined as:

\[Coh^{s}(^{})=(^{s}(^{}) )=}_{n=1}^{N^{}}(^{s}(^ {}_{n})-}_{n=1}^{N^{}}^{s}(^ {}_{n}))\]

Note that \(Coh^{s=0}(^{})=Coh^{s=1}(^{})\), and we adopt abbreviation \(Coh(^{})\)3.

Geometric intuition.The intuition of sensitive attribute estimation, as illustrated in Figure 2, can be grasped from a geometric standpoint. In a toy example with a two-dimensional data representation, \(^{}^{2}\), we consider two demographic groups for a binary sensitive attribute. The subspace spanned by the data representations from these groups is denoted by \(^{0}\) and \(^{1}\). Data representations to be estimated are \(^{}_{0}\) and \(^{}_{1}\). \(^{0}_{0}\), \(^{0}_{1}\) and \(^{1}_{0}\), \(^{1}_{1}\) is the projection of \(^{}_{0}\) and \(^{}_{1}\) onto the orthogonal complement of \(^{0}\) and \(^{1}\). As for fair data representation, zero coherent encourages all representations aligned with a "line" so that all representations are with the same normalized similarity with sensitive groups. Figure 2 (a) shows the case in which the data representation is biased where \(^{}_{0}\) and \(^{}_{1}\) can be easily distinguished. Figure 2 (b) shows that fairer data representation as they are less separable.

## 4 Methodology

### Problem Statement

Based on the proposed coherence metric, we argue that if \(Coh(^{})\) is reduced, bias in the distilled graph can be mitigated. As a result, if GNNs are trained on such distilled graphs, the bias issues in downstream tasks could also be alleviated. The problem is formally defined as: Given an undirected attributed network \(=\{,,,\}\), our goal is to obtain an debiased distilled graph \(^{}=\{^{},^{},^{}\}\) via reducing \(Coh\), so that the fairness issues of GNNs trained on \(^{}\) is mitigated. Hence the overall objective goal for generating a fair and condensed graph is:

\[_{^{}}_{GM}+_{Coh }(_{^{^{}}}(^{},^{}),_{^{^{}}}(, {X}))\] \[^{^{}}=*{arg\,min}_{ }_{CE}(_{}(^{},^{}),^{})\] (8)

### Fair Graph Distillation Loss

Gradient Matching Loss.We adopt gradient matching, as shown in equation (2), for graph distillation to distill useful information for node classification tasks. However, treating both \(^{}\) and \(^{}\) as learnable parameter 4 and directly optimizing \(^{}\) is unaffordable due to \(O(N^{2})\) computation complexity. Following previous work Jin et al. (2021), we parameterize \(^{}\) as a function of \(^{}\):

\[^{}_{i,j}=(_{}([^{ }_{i};^{}_{j}])+_{}([^{}_{j};^{}_{i}])}{2}),\] (9)

where \(^{}_{i,j}\) is \(i\)-th row, \(j\)-th column of \(^{}\), \(_{}\) is a multi-layer neural network parameterized with \(\) and \([;]\) denotes concatenation. Note that \(^{}\) is controlled to be symmetric since \(^{}_{i,j}=^{}_{j,i}\). Sigmoid function pushes \(^{}\) close to \(0\) or \(1\) to encourage its sparsity. For simplicity, we denote the parameterized adjacency matrix as \(^{}_{}\). In this way, we can reduce the complexity to \(O(N)\).

The distance metric \(D\) measures the similarity of gradients over the real graph and distilled graph. We adopt the summation of the gradient distance over all layers as the final gradient distance:

\[D(_{}(),_{}( )^{})=_{i}(1- ()_{i}_{}()^{}_{i}}{ \|_{}()_{i}\|\|_{}( )^{}_{i}\|})\] (10)

Figure 3: An overview of the proposed framework. The synthesizer generates the attribute matrix and adjacency matrix of the distilled small graph \(^{}\). The Cross-Entropy loss \(_{CE}\) guides the update of GNNs model during the inner optimization loop. Gradient matching loss \(_{GM}\) and coherence loss \(_{Coh}\) guide the update of the synthesizer during the outer optimization loop for utility and fairness.

where \(_{}()_{i}\) is the i-th column vectors of the gradient matrices. Hence the loss objective for the graph distillation module is given by:

\[_{GM}=_{ P_{0}}[_{t=0}^{T-1}D( _{}(),_{}( ^{}))]\] (11)

where \(^{}=\{^{},^{},^{}\}\), \(t\) is the training epoch, and \(_{t}\) is well-trained GNNs model parameters. To reduce the impact of parameter initialization, the initial model parameters \(_{0}\) are sampled from a distribution of random initialization.

Coherence loss.In Section 3.4, we introduce coherence as a bias metric for distilled data. To mitigate bias, we use coherence bias as a regularization for fair synthesis. This calculation employs real graph node attribute \(\) and distilled node attribute \(^{}\) to estimate sensitive group memberships but overlooks structural bias in graph data. Given the GNN propagation mechanism, bias can exist in both node attributes and graph structure Dong et al. (2022). Even without attribute bias, node representation may still be biased if structural bias is present.

Work by Dong et al. (2022) suggests that structural bias can be measured through graph representation bias. Leveraging this, we aim for low coherence in node attributes and representations to fully remove bias from our distilled graph. Specifically, we introduce attribute and structural coherence to decrease attribute and structural bias, respectively, by minimizing the variance in sensitive group membership estimation for node attributes and representations. Given a real graph data \(=\{,,,\}\) and a distilled graph \(^{}=\{^{},^{},^{}\}\), we feed them into a \(L\)-layer GNN, where the \(l\)-th layer latent representation in the GNN is denoted as \(_{l}\). The latent representation for node attribute after \(l\)-hop propagation contains both attribute bias as well as structural bias. Note the node attribute \(\) and \(^{}\) before propagation as \(_{0}\) and \(_{0}^{}\), we get a set of latent presentation \(\{_{0},_{1},...,_{L}\}\) for \(\) and \(\{_{0}^{},_{1}^{},...,_{L}^{}\}\) for \(^{}\). The objective to measure bias of \(_{l}^{}\) is:

\[Coh(_{l}^{})=(^{j}(_{l}^{}) )=(^{j}_{l }^{}\|_{2})}{_{j}(-\|^{j} _{l}^{}\|_{2})}),\] (12)

where \(^{j}=_{j}(+_{j}_{l}^{j}_{l}^ {T})^{-1}\). \(^{j}\) is introduced in Sec 4.1. Since we consider the binary sensitive attribute, \(j\) is set as \(0\) without losing generality and is omitted in the notations as \(^{j}():=()\). After considering all the latent representations, the coherence loss objective is defined as the summation of all coherence over all layers, i.e.,

\[_{Coh}=_{l=0}^{L}Coh(_{l}^{})=_{l=0}^{L} ((_{l}^{})).\] (13)

Prediction loss for GNN training.The GNNs model is trained on distilled graph \(^{}=\{^{},^{},^{}\}\) with prediction loss. We adopt \(L\)-layer GNNs model, where \(\) is the parameter of the GNN. We also adopt cross-entropy loss by default:

\[_{CE}=(_{}(^{}, ^{}),^{}),\] (14)

### Final Objective and Training Algorithm

Outer loop optimization.In the outer loop, we optimize the fair graph synthesizer with gradient matching loss and coherence loss:

\[_{^{},^{}}_{GM}+_{Coh},\] (15)

where \(\) is a hyperparameter to regularize the debiasing intensity. The distilled node attribute \(^{}\) and the distilled node label \(^{}\) are initialized with the nodes uniformly sampling from real graph data \(\).

Inner loop optimization.The GNN parameter \(\) is optimized in the inner loop:

\[_{}_{CE}(_{}(^{}, ^{}),^{}).\] (16)

Instead of using the real graph data \(\) to calculate the loss, we use the distilled graph \(^{}\). It empirically shows good performance and better efficiency. But the adversarial training baseline uses \(\) as it needs the sensitive attribute for discriminator training.

[MISSING_PAGE_FAIL:7]

### Debiasing distilled Graph

In response to **RQ.1**, we assess FGD's bias mitigation and prediction performance across various GNN architectures and datasets, as shown in Table 1. We compare the \(_{DP}\) and \(_{EO}\) values of GNNs trained on real graphs (_Real_), vanilla distilled graphs (_Vanilla_), and debiased distilled graphs via FGD (_FGD_). Key findings include: (1) Models trained on _Real_ graphs consistently outperform _Vanilla_ and _FGD_ in utility, though _FGD_'s utility matches or surpasses _Vanilla_. (2) FGD consistently yields lower bias than _Vanilla_, and outperforms _Real_ on 4 out of 5 datasets, excluding Poken-n.

We compare the coherence bias of distilled graphs generated by _Vanilla_ and _FGD_ methods across five real-world datasets with the GCN architecture (Table 2). Our analysis reveals that FGD reduces unfairness, reflected in lower coherence bias in the distilled graphs. This consistency confirms the effectiveness of coherence bias as a measure of distilled graph bias.

### Trade-Off Comparison

In response to **RQ.2**, we compare the trade-off between model utility and bias mitigation against other baselines using the GCN architecture. We utilize the Pareto frontier Ishizaka and Nemeny (2013) to evaluate our approach's utility-fairness trade-off, using different hyperparameters. The Pareto frontier graphically represents optimal trade-offs in multi-objective optimization. We use AUC as the utility metric and \(_{DP}\) and \(_{EO}\) as fairness metrics. Higher AUC and lower \(_{DP}\)/\(_{EO}\) are preferred, so models with Pareto frontier curves closer to the bottom right corner (AUC on the horizontal axis and \(_{DP}\)/\(_{EO}\) on the vertical) have better trade-off performance.

Figure 4 shows the results for models trained on the real graph, the distilled graph debiased by baseline methods (vanilla graph distillation, FairGNN, and EDITS5) and the distilled graph debiased by FGD. We can observe: (1) From a model utility perspective, FGD performs comparably to other baselines, like vanilla graph distillation, FairGNN, and EDITS 6, suggesting it preserves sufficient information for node classification. (2) In terms of bias mitigation, all baselines show effectiveness, with FGD exhibiting the best results. (3) When considering the utility-fairness trade-off, FGD's Pareto front curve lies at the bottom right corner of all baselines, signifying it offers the best balance. Thus, FGD outperforms other baselines in balancing model utility and bias mitigation.

### Add-on Module

   & Vanilla & FGD \\  Pokec-z & 0.009468 & \(-77.57\%\) \\  Pokec-n & 0.004464 & \(-90.32\%\) \\  German & 0.012772 & \(-72.68\%\) \\  Credit & 0.011864 & \(-75.84\%\) \\  Recidivism & 0.000098 & \(-61.22\%\) \\  

Table 2: Coherence bias comparison between vanilla distilled graph (denoted as Vanilla) and fair distilled graph (denoted as FGD). The lower, the better. The best ones are marked in bold. The architecture model is GCN.

Figure 4: Trade-off comparison between FGDand other baselines for five real-world graph datasets.

In addition to its superior trade-off performance, our method, FGD, can enhance other debiasing baselines like FairGNN and EDITS by acting as an add-on debias module. This compatibility is due to the fact that these baselines can replace the cross-entropy loss in the GNN training module. To answer **RQ.3**, we conducted experiments on the Credit dataset comparing FairGNN/EDITS performance with and without FGD. As shown in Figure 5, FairGNN/EDITS coupled with FGD delivers better utility-fairness trade-off, demonstrating FGD's potential to boost other debias methods.

## 6 Related Work

Dataset Distillation \(\&\) Knowledge Distillation.Dataset Distillation (DD) and Knowledge Distillation (KD) are methods to improve the efficiency of training deep neural networks. DD synthesizes a small dataset encapsulating the knowledge of a larger one, achieving comparable model performance . It employs a bi-level optimization approach, with dataset condensation (DC) speeding up the process via gradient matching of model parameters. DD also helps with repeated training or privacy applications like continual learning, neural architecture search, and privacy-preserving scenarios. Meanwhile, graph data condensation methods have been developed for node and graph classification tasks . KD, on the other hand, enhances computational efficiency through model compression and acceleration. It trains a compact student model using the knowledge from a larger teacher model Gou et al. To address the scarcity and high complexity of labeled data in GNNs, knowledge distillation (KD) was introduced to enhance existing GNNs Liu et al. , Wang et al. , also for fairness problem Dong et al. . While KD focuses on model compression, DD targets data compression, each improving efficiency from model-centric and data-centric perspectives.

Fair Graph Learning.Fairness in machine learning has attracted many research efforts. Many technologies are introduced in graph neural networks to achieve fair graph learning in node classification tasks, including optimization with regularization , rebalancing , adversarial learning  and graph rewiring . For link prediction, dyadic fairness and corresponding graph rewiring solutions are also developed in . Another line of work focuses on solving the individual fairness problem on the graph data Song et al. , Dong et al. , Kang et al. .

## 7 Conclusion

Despite the ability of graph distillation to condense valuable graph data, this study finds that the vanilla method can worsen fairness issues. Therefore, we introduce a fair graph distillation process to generate fair distilled graph data. As the distilled graph lacks the nodes' sensitive attributes, conventional fair methods cannot be directly applied. However, we identify a consistent geometric phenomenon in graph distillation to estimate these sensitive attributes. We also introduce a new bias metric, coherence, and propose a bi-level optimization framework, FGD, for fair graph distillation. Experimental results validate FGD's effectiveness in mitigating bias while maintaining model utility across various GNN architectures and datasets. Future work will focus on addressing individual fairness issues and non-binary sensitive attribute conditions, among other aspects, as discussed in Appendix H.