# Block Coordinate Descent Methods for Optimization under J-Orthogonality Constraints with Applications

**Anonymous Author(s)**

Affiliation

Address

email

###### Abstract

The J-orthogonal matrix, also referred to as the hyperbolic orthogonal matrix, is a class of special orthogonal matrix in hyperbolic space, notable for its advantageous properties. These matrices are integral to optimization under J-orthogonal constraints, which have widespread applications in statistical learning and data science. However, addressing these problems is generally challenging due to their non-convex nature and the computational intensity of the constraints. Currently, algorithms for tackling these challenges are limited. This paper introduces **JOBCD**, a novel Block Coordinate Descent method designed to address optimizations with J-orthogonality constraints. We explore two specific variants of **JOBCD**: one based on a Gauss-Seidel strategy (**GS-JOBCD**), the other on a variance-reduced and Jacobi strategy (**VR-J-JOBCD**). Notably, leveraging the parallel framework of a Jacobi strategy, **VR-J-JOBCD** integrates variance reduction techniques to decrease oracle complexity in the minimization of finite-sum functions. For both **GS-JOBCD** and **VR-J-JOBCD**, we establish the oracle complexity under mild conditions and strong limit-point convergence results under the Kurdyka-Lojasiewicz inequality. To demonstrate the effectiveness of our method, we conduct experiments on hyperbolic eigenvalue problems, hyperbolic structural probe problems, and the ultrahyperbolic knowledge graph embedding problem. Extensive experiments using both real-world and synthetic data demonstrate that **JOBCD** consistently outperforms state-of-the-art solutions, by large margins.

## 1 Introduction

A matrix \(^{n n}\) is a J-orthogonal matrix if \(^{}=\), where \(=[_{p}&\\ &-_{n-p}]\), and \(_{p}\) is a \(p p\) identity matrix. Here, \(^{n n}\) is the signature matrix with signature \((p,n-p)\). In this paper, we mainly focus on the following optimization problem under J-orthogonality constraints:

\[_{^{n n}}f() _{i=1}^{N}f_{i}(),\,\,\,\,^{ }=.\] (1)

Here, \(f()\) could have a finite-sum structure, each component function \(f_{i}()\) is assumed to be differentiable, and \(N\) is the number of data points. For brevity, the J-orthogonality constraint \(^{}=\) in Problem (1) is rewritten as \(\).

We impose the following assumptions on Problem (1) throughout this paper. (\(\)-i) For any matrices \(\) and \(^{+}\), we assume \(f_{i}:^{n n}\) is continuously differentiable for some symmetric positive semidefinite matrix \(^{nn nn}\) that:

\[f_{i}(^{+}) f_{i}()+^{+}-, f_{i}()+\|^{+}-\|_{ }^{2},\] (2)

for all \(i[N]\), where \(\|\| L_{f}\) for some constant \(L_{f}>0\) and \(\|\|_{}^{2}()^{} \,()\). This further implies that: \(\| f_{i}()- f_{i}(^{+})\|_{} L _{f}\|-^{+}\|_{}\) for all \(i[N]\). Importantly, the function \(f()=(^{} )=\|\|_{}^{2}\) with \(=\) satisfies the equality\(,^{+},f(^{+})=(^{+}; )\) in (2), where \(^{n n}\) and \(^{n n}\) are arbitrary symmetric matrices. (A-ii) The function \(f_{i}()\) is coercive for all \(i N\), that is, \(_{\|\|_{}}f_{i}()=,\, i\).

Problem (1) defines an optimization framework that is fundamental to a wide range of models in statistical learning and data science, including hyperbolic eigenvalue problem [6; 43; 40], hyperbolic structural probe problem [20; 7], and ultrahyperbolic knowledge graph embedding . Additionally, it is closely related to machine learning in hyperbolic spaces, including Lorentz model learning [35; 50; 8] and ultrahyperbolic neural networks [27; 54; 42]. It also intersects with hyperbolic linear algebra [3; 21], addressing problems such as the indefinite least squares problem, hyperbolic QR factorization, and indefinite polar decomposition.

### Related Work

**Block Coordinate Descent Methods**. Block Coordinate Descent (BCD) is a well-established iterative algorithm that sequentially minimizes along block coordinate directions. Its simplicity and efficiency have led to its widespread adoption in structured convex applications . Recently, BCD has gained traction in non-convex problems due to its robust optimality guarantees and/or excellent empirical performance in areas including optimal transport , matrix optimization , fractional minimization , deep neural networks [5; 53; 32], federated learning, black-box optimization , and optimization with orthogonality constraints [51; 14]. To our knowledge, this is the first application of BCD methods to optimization under J-orthogonality constraints, with a focus on analyzing their theoretical guarantees and empirical efficacy.

**Minimizing Smooth Functions under J-Orthogonality Constraints**. The J-orthogonal matrix belongs to a subset of generalized orthogonal matrices [16; 36; 23]. However, projecting onto the J-orthogonality constraint poses challenges, complicating the extension of conventional optimization algorithms to address optimization problems under these constraints [1; 16]. This contrasts with computing orthogonal projections using methods such as polar or SVD decomposition, or approximating them via QR factorization. Existing methods for addressing Problem (1) can be categorized into three classes. (_i_) CS-Decomposition Based Methods. These approaches involve parameterizing four orthogonal matrices (as described in Proposition 2.2) and subsequently minimizing a smooth function over these matrices in an alternating fashion. The involvement of \(3 3\) block matrices makes the implementation of these methods very challenging. Consequently, the work of  focuses on optimizing a reduced subspace of the CS decomposition parameters, albeit at the expense of losing some degrees of freedom. (_ii_) Unconstrained Multiplier Correction Methods [31; 13; 14]. These methods leverage the symmetry and explicit closed-form expression of the Lagrangian multiplier at the first-order optimality condition. Consequently, they address an unconstrained problem, resulting in efficient first-order infeasible approaches. (_iii_) Alternating Direction Method of Multipliers . This method reformulates the original problem into a bilinear constrained optimization problem by introducing auxiliary variables. It employs dual variables to handle bilinear constraints, iteratively optimizing primal variables while keeping other primal and dual variables fixed, and using a gradient ascent strategy to update the dual variables. This approach has become widely adopted for solving general nonconvex and nonsmooth composite optimization problems. Notably, all the aforementioned methods solely identify critical points of Problem (1).

**Finite-Sum Problems via Stochastic Gradient Descent**. The finite-sum structure is prevalent in machine learning and statistical modeling, facilitating decomposition into smaller, more manageable components. This property is advantageous for developing efficient algorithms for large-scale problems, such as Stochastic Gradient Descent (SGD). Reducing variance is crucial in SGD because it can lead to more stable and faster convergence. Various techniques, such as mini-batch SGD, momentum methods, and variance reduction methods like SAGA , SVRG , SARAH , SPIDER [11; 44], SNVRG , and PAGE , have been developed to address this issue. Additionally, SGD for minimizing composite functions has also been investigated by the authors [15; 24; 29].

### Contributions

This paper makes the following contributions. (_i_) Algorithmically: We introduce the **JOBCD** algorithm, a novel Block Coordinate Descent method specifically designed to tackle optimizations constrained by J-orthogonality. We explore two specific variants of **JOBCD**, one based on a Gauss-Seidel strategy (**GS-JOBCD**), the other on a variance-reduced and Jacobi strategy (**VR-J-JOBCD**). Notably, **VR-J-JOBCD** incorporates a variance-reduction technique into a parallel framework to reduce oracle complexity in the minimization of finite-sum functions (See Section 2). (_ii_) Theoretically: We provide comprehensive optimality and convergence analyses for both 

[MISSING_PAGE_FAIL:3]

**Proposition 2.2**.: _(Hyperbolic CS Decomposition ) Let \(\) be J-orthogonal with signature \((p,n-p)\). Assume that \(n-p p\). Then there exist vectors \(,^{n-p}\) with \(-=\), and orthogonal matrices \(_{1},_{1}^{p p}\) and \(_{2},_{2}^{(n-p)(n-p)}\) such that: \(=[_{1}&0\\ 0&_{2}][_{ 2}()&0&_{2}()\\ 0&_{2}()&0&_{2}()]\)\([_{1}^{}&0\\ 0&_{2}^{}]\)._

Applying Proposition 2.2 with \(n=2\), \(p=1\), and \(_{1}=_{2}=_{1}=_{2}= 1\), \(}^{2}-}^{2}=1\) with \(},}\), we parametrize \(\) as: \(=( 1&0\\ 0& 1)(}&}\\ }&})(  1&0\\ 0& 1)\), where we denote \(}\) as \(()\), \(}\) as \(()\), and \(}\) as \(()\)for some \(\), for simplicity of notation. It is not difficult to show that Problem (6) reduces to the following one-dimensional search problem:

\[_{}()^{} }()+, ,\,\,\{(}&}\\ }&}),( }&-}\\ -}&}),( -}&-}\\ }&-}),( }&-}\\ }&-})\}.\] (7)

We apply a breakpoint search method to solve Problem (7). For simplicity, we provide an analysis only for the first case. A detailed discussion of all four cases can be found in Appendix Section B.1. For the case where \(=(}&}\\ }&})\), Problem (7) reduces to the following problem:

\[_{,}}a\,}+b\,} +c\,}^{2}+d\,}\,}+e\, }^{2},\] (8)

where \(a=_{11}+_{22}\), \(b=_{12}+_{21}\), \(c=(}_{11}+}_{41}+}_{14 }+}_{44})\), \(d=(}_{21}+}_{31}+}_{12 }+}_{42}+}_{13}+}_{43}+}_{24}+}_{34})\), and \(e=(}_{22}+}_{32}+}_{23 }+}_{33})\). Then we perform a substitution to convert Problem (8) into an equivalent problem that depends on the trigonometric functions: (\(}\)) \(^{2}=}^{2}}\); (\(}\)) \(}^{2}=}^{2}}{1-}^{2}}\); (\(}\)) \(}=}}{}}\). The following lemma provides a characterization of the global optimal solution for Problem (8).

**Lemma 2.3**.: _(Proof in Section C.2) We let \((,) a+b+c^{2}+d +e^{2}\). The optimal solution \(\) to Problem (8) can be computed as: \([(),()]_{[c,s]}(c,s),\, \,[c,s]\{[_{+})^{2}}},_{+}}{_{+})^{2}}}],[_{-})^ {2}}},_{-}}{_{-})^{2}}}]\}\), where \(_{+}_{t}p(t)}{^{2}}}+^{2}}\); \(_{-}_{t}(t)^{2}}}+^{2}}\). Here \(w=c+e\)._

We now describe how to find the optimal solution \(_{+}\), where \(_{+}_{t}p(t)^{2}}}+ ^{2}}\); this strategy can naturally be extended to find \(_{-}\). Initially, we have the following first-order optimality conditions for the problem: \(0= p(t)=\)\([b(1-t^{2})+(a+bt)t]^{2}}+[d(1-t^{2})+(w+dt)(2t)] dt ^{2}+2wt+d=-[b+at]^{2}}\). Squaring both sides yields the following quartic equation: \(c_{4}t^{4}+c_{3}t^{3}+c_{2}t^{2}+c_{1}t+c_{0}=0\), where \(c_{4}=d^{2}+a^{2}\), \(c_{3}=4wd+2ab\)\(c_{2}=4w^{2}+2d^{2}-a^{2}+b^{2}\), \(c_{1}=4wd-2ab\), \(c_{0}=d^{2}-b^{2}\). This equation can be solved analytically by Lodovico Ferrari's method , resulting in all its real roots \(\{_{1},_{2},,_{j}\}\) with \(1 j 4\).

For the second and third cases, Problem (6) essentially boils down to optimization under orthogonality constraints. The work of  derives a breakpoint search method for finding the optimal solution for Problem (6) with \(_{}\{(1&0\\ 0&1),(-1&0\\ 0&-1)\}\) using the Givens rotation and Jacobi reflection matrices.

### Variance-Reduced Jacobi Block Coordinate Descent Algorithm

This subsection proposes the **VR-J-JOBCD** algorithm, a randomized block coordinate descent method derived from **GS-JOBCD**. Importantly, by leveraging the parallel framework of a Jacobi strategy [17; 9], **VR-J-JOBCD** integrates variance reduction techniques [39; 30; 18] to decrease oracle complexity in the minimization of finite-sum functions. This makes the algorithm effective for minimizing large-scale problems under J-orthogonality constraints.

**Notations**. We assume \(n\) is an even number in this paper. We create \((n/2)\) pairs by non-overlapping grouping of the numbers in any arbitrary combination, with each pair containing two distinct numbers from the set \([n]\). It is not hard to verify that such grouping yields \(_{J}=(n!)/(2^{n/2}!)\) possible combinations. The set of these combinations is denoted as \(\{}_{i}\}_{i=1}^{_{J}} \{}_{1},}_{2},,}_ {_{J}}\}\)1.

\(\)**Variance Reduction Strategy**. We incorporate state-of-the-art variance reduction strategies from the literature [30; 5] into our algorithm to solve Problem (1). These methods iteratively generate a stochastic gradient estimator as follows:

\[}^{t}=\{_{i_{+}^{t}} f_{i}(^{t}),&\ p;\\ }^{t-1}+}_{i_{*}^{t}}(  f_{i}(^{t})- f_{i}(^{t-1})),&\ 1-p..\] (9)

Here, \(\{_{+}^{t},_{+}^{t}\}\) are uniform random minibatch samples with \(|_{+}^{t}|=b\), \(|_{*}^{t}|=b^{}\), and \(}^{0}=_{i_{+}^{0}} f_{i}( ^{0})\). We drop the superscript \(t\) for \(\{_{+}^{t},_{*}^{t}\}\) as \(t\) can be inferred from context. We only focus on the default setting that [30; 5]: \(b=N\), \(b^{}=\) and \(p=}{b+b^{}}\).

\(\)**Jacobi Block Coordinate Descent Method**. The proposed algorithm is built upon the parallel framework of a Jacobi strategy. In each iteration \(t\), we randomly and uniformly (with replacement) select a coordinate set \(^{t}\{_{(1)}^{t},_{(2)}^{t},, _{()}^{t}\}\) from the set \(\) with \(^{t}^{ 2}\) and \(_{(i)}^{t}^{2}\).

For all \(t\), we have: \(_{(i)}^{t}_{(j)}^{t}=\) and \(_{i=1}^{n/2}(_{(i)}^{t})=[n]\). We drop the superscript \(t\) if \(t\) can be inferred from context.

The following lemma shows how to choose a suitable matrix \(\) so that the Jacobi strategy can be applied.

**Lemma 2.4**.: _(Proof in Section C.3) We let \(^{t}\{_{(1)}^{t},_{(2)}^{t},, _{()}^{t}\}\) for all \(t\). We let \(=_{4}\), where \(\) is some suitable constant with \( L_{f}\). For any \(_{(i)}^{t}\) and \(_{(j)}^{t}\) with \(i j\), their corresponding objective functions as in Equation (3) are independent._

We consider the following block coordinate update rule in **VR-J-JOBCD**: \(^{t+1}}_{}^{t}(_{ :})^{t}+[_{i=1}^{n/2}_{_{(i)}}( _{i}-_{2})_{_{(i)}}^{}]^{t}\). The following lemma provides properties of this rule.

**Lemma 2.5**.: _(Proof in Section C.4) We let \(\), \(_{i}_{_{(i)}}\), \(\), and \(i[]\). We define \(^{+}}_{}(_{:}) +[_{i=1}^{n/2}_{_{(i)}}(_{i}-_{2})_{_{(i)}}^{}]\). We have: **(a)**\(_{i=1}^{n}\|_{_{(i)}}(_{i}-_{2}) _{_{(i)}}^{}\|_{}^{2}=\|_{i=1}^ {n}_{_{(i)}}(_{i}-_{2})_{ _{(i)}}^{}\|_{}^{2}\). **(b)**\(\|^{+}-\|_{}^{2}\|\|_{}^{2} _{i=1}^{n/2}\|_{i}-_{2}\|_{}^{2}\). **(c)**\(\|^{+}-\|_{}^{2}_{i=1}^{n/2}\| _{i}-_{2}\|_{}^{2}\) with \(=_{4}\). **(d)** For all \(}^{n n}\), it follows that: \(2_{i=1}^{n/2}_{i}-_{2},[( f()- {})^{}]_{_{(i)}_{(i)}} \|\|_{}^{2}_{i=1}^{n/2}\|_{i}-_{2}\|_{}^{2}+\|[ f()-}]\|_{ }^{2}\).

\(\)**The Main Algorithm**. Using the update rule above, we consider the following iterative procedure: \(^{t+1}}_{}^{t}(_{ :})\), where \(}_{:}^{t}_{}\), \(f(}_{}^{t}(_{:}))\). We establish the majorization function for 

[MISSING_PAGE_FAIL:6]

\(_{J}f(}) f(})-[ f(})]^{}}\). The associated Lagrangian multiplier can be computed as \(=^{} f(})\). (_b_) The critical point condition is equivalent to the requirement that the matrix \( f(})^{}\) is symmetric, which is expressed as \(^{}=[^{} ]^{}\).

**Remarks**. While our results in Lemma 3.1 show similarities to existing works focusing on problems under orthogonality constraints , this study marks the first investigation into the first-order optimality condition for optimization problems under J-orthogonality constraints.

The following definition is useful in our subsequent analysis of the proposed algorithms.

**Definition 3.2**.: (Block Stationary Point, abbreviated as BS-point) Let \(>0\). A solution \(}\) is termed as a block stationary point if, for all \(\{_{1},_{2},, _{_{n}^{2}}\}\), the following condition is satisfied: \(_{2}_{_{}}( ;},)\).

The following theorem shows the relation between critical points and BS-points.

**Theorem 3.3**.: _(Proof in Section D.2) Any BS-point is a critical point, while the reverse is not necessarily true._

## 4 Convergence Analysis

This section provides a convergence analysis for **GS-JOBCD** and **VR-J-JOBCD**.

For **GS-JOBCD**, the randomness of output \((}^{t},^{t+1})\) for all \(t\) are influenced by the random variable \(^{t}(^{1};^{2};;^{t})\). For **VR-J-JOBCD**, the randomness of output \((}^{t}_{:},^{t+1})\) are influenced by the random variables \(^{t}(^{1},^{1}_{+},^{1}_{*}; ^{2},^{2}_{+},^{2}_{*};;^{t}, ^{t}_{+},^{t}_{*})\).

We denote \(}\) as the global optimal solution of Problem (1). To simplify notations, we define: \(u^{t}=\|}^{t}- f(^{t})\|_{}}^{2}\), and \(_{i}=f(^{i})-f(})\).

We impose the following additional assumptions on the proposed algorithms.

**Assumption 4.1**.: There exists constants \(\{},}\}\) that: \(\|^{t}\|_{}}\), and \(\|^{t}\|_{}}\) for all \(t\).

**Assumption 4.2**.: There exists a constant \(}\) that: \(\| f(^{t})\|_{}}\), and \(\|}^{t}\|_{}}\) for all \(t\).

**Assumption 4.3**.: For any \(^{n n}\), \(_{i}[\| f_{i}(^{t})- f(^{t})\|_{ }^{2}]^{2}\), where \(i\) is drawn uniformly at random from \([N]\).

**Remarks**. (_i_) Assumption 4.1 is satisfied as the function \(f_{i}()\) is coercive for all \(i\). (_ii_) Assumption 4.2 imposes a bound on the (stochastic) gradient, a fairly moderate condition frequently employed in nonconvex optimization . (_iii_) Assumption 4.3 ensures that the variance of the stochastic gradient is bounded, which is a common requirement in stochastic optimization [30; 5].

### Global Convergence

We define the \(\)-BS-point as follows.

**Definition 4.4**.: (\(\)-BS-point) Given any constant \(>0\), a point \(}\) is called an \(\)-BS-point if: \((})\). Here, \(()\) is defined as \(()}^{2}}_{i=1}^{C_{ }^{2}}(_{2},_{} (;,_{i}))^{2}\) for For **GS-JOBCD**, while it is defined as \(()}}_{i=1}^{C_{ }^{2}}_{^{t}}[(_{2},_{ }(_{:};,}_{i}))^{2}]\) for **VR-J-JOBCD**, where the expectation is with respect to the randomness inherent in the algorithm .

We have the following useful lemma for **VR-J-JOBCD**.

**Lemma 4.5**.: _(Proof in Section E.1) Suppose Assumption 4.3 holds, then the variance \(_{^{t}}[u_{k}]\) of the gradient estimators \(\{}^{t}\}\) of Algorithm 2 is bounded by: \(_{^{t}}[u^{t}]^{2}+(1-p)_{^{t-1}}[u^{t-1}]+}^{2}}^{2}(1 -p)}{b^{}}_{^{t-1}}[_{i=1}^{n/2}\|_{i}^{t-1}- _{2}\|_{}^{2}]\)_

The following two theorems establish the iteration complexity (or oracle complexity) for **GS-JOBCD** and **VR-J-JOBCD**.

**Theorem 4.6**.: _(Proof in Section E.2)_ **GS-JOBCD** finds an \(\)-BS-point of Problem (1) within \((N}{})\) arithmetic operations.

**Theorem 4.7**.: _(Proof in Section E.3) Let \(b=N\), \(b^{}=\), and \(p=}{b+b^{}}\)._ **VR-J-JOBCD** _finds an \(\)-BS-point of Problem (1) within \((nN+}{})\) arithmetic operations._

**Remark.** Theorems 4.6 and 4.7 demonstrate that the arithmetic operation complexity of **GS-JOBCD** is linearly dependent on \(N\), while **VR-J-JOBCD** is linearly dependent on \(\). Therefore, **VR-J-JOBCD** reduces the iteration complexity significantly.

### Strong Convergence under KL Assumption

We prove algorithms achieve strong convergence based on a non-convex analysis tool called Kurdyka-Lojasiewicz inequality.

We impose the following assumption on Problem (1).

**Assumption 4.8**.: (Kurdyka-Lojasiewicz Property). Assume that \(f^{}()=f()+_{}()\) is a KL function. For all \(\,f^{}\), there exists \([0,1),(0,+]\) a neighborhood \(\) of \(\) and a concave and continuous function \((t)=ct^{1-},c>0\), \(t[0,)\) such that for all \(^{}\) and satisfies \(f^{}(^{})(f^{}(),f^{}( )+)\), the following holds: \((, f^{}(^{}))^{ }(f^{}(^{})-f^{}()) 1\).

We establish strong limit-point convergence for **VR-J-JOBCD** and **GS-JOBCD**.

**Theorem 4.9**.: _(Proof in Section E.5, a Finite Length Property). The sequence \(\{^{t}\}_{t=0}^{}\) of_ **GS-JOBCD** _has finite length property that: \( t,_{i=1}^{t}_{^{t}}[\|^{t+1}-^{t} \|_{}]((_{1}))<+\), where \(()\) is the desingularization function defined in Proposition 4.8._

**Theorem 4.10**.: _(Proof in Section E.4, a Finite Length Property). Choosing \(b=N\), \(b^{}=\) and \(p=}{b+b^{}}\), then the sequence \(\{^{t}\}_{t=0}^{}\) of_ **VR-J-JOBCD** _has finite length property that: \( t,_{i=1}^{t}_{^{t}}[\|^{t+1}-^{t }\|_{}]()}{N^{1/4}})<+\), where \(()\) is the desingularization function defined in Assumption 4.8._

## 5 Applications and Numerical Experiments

This section demonstrates the effectiveness and efficiency of **JOBCD** on three optimization tasks: _(i)_ the hyperbolic eigenvalue problem, _(ii)_ structural probe problem, and _(iii)_ Ultra-hyperbolic Knowledge Graph Embedding problem. We provide experiments for the last problem in Section F.2.

\(\)**Application to the Hyperbolic Eigenvalue Problem (HEVP)**. The hyperbolic eigenvalue problem refers to the generalized eigenvalue problem in hyperbolic spaces . This problem is a fundamental component in machine learning models, such as Hyperbolic PCA [43; 6]. Given a data matrix \(^{m n}\) and a signature matrix \(\) with signature \((p,n-p)\), HEVP can be formulated as the following optimization problem: \(_{}~{}-(^{}^{}),~{}~{}^{} =\).

\(\)**Application to the Hyperbolic Structural Probe Problem (HSPP)**. The Structure Probe (SP) is a metric learning model aimed at understanding the intrinsic semantic information of large language models . Given a data matrix \(^{m n}\) and its associated Euclidean distance metric matrix \(^{m m}\), HSPP employs a smooth homeomorphic mapping function \(()\) to project the data \(\) into ultra-hyperbolic space. Subsequently, it seeks an appropriate linear transformation \(^{n n}\) constrained within a specific structure \(\), such that the resulting transformed data \(()^{m n}\) exhibits similarity to the original distance metric matrix \(\) under the ultra-hyperbolic geodesic distance \(d_{}(_{i:},_{j:})\), expressed as \(_{i,j} d_{}(_{i:},_{j:})\) for all \(i,j[m]\), where \(_{i:}\) is \(i\)-th row of the matrix \(^{m n}\). This can be formulated as the following optimization problem: \(_{}}_{i,j m}(_{i,j}-d_{}( _{i:},_{j:}))^{2},~{}~{} ()\), \(\). For more details on the functions \(()\) and \(d_{}(,)\), please refer to Appendix Section F.1.

\(\)**Datasets**. To generate the matrix \(^{m n}\), we use 8 real-world or synthetic data sets for both HEVP and HSPP tasks: 'Cifar', 'CnnCaltech', 'Gisette', 'Mnist', 'randn', 'Sector', 'TDT2', 'w1a'. We randomly extract a subset from the original data sets for the experiments.

\(\)**Compared Methods**. We compare **GS-JOBCD** and **VR-J-JOBCD** with 3 state-of-the-art optimization algorithms under J-orthogonality constraints. _(i)_ The CS Decomposition Method (**CSDM**) . _(ii)_ Standard ADMM (**ADMM**) . **UMCM**: Unconstrained Multiplier Correction Method [31; 13].

[MISSING_PAGE_EMPTY:9]