# Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations

Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations

 Piotr Indyk

MIT

indyk@mit.edu

&Haike Xu

MIT

haikexu@mit.edu

###### Abstract

Graph-based approaches to nearest neighbor search are popular and powerful tools for handling large datasets in practice, but they have limited theoretical guarantees. We study the worst-case performance of recent graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG and DiskANN. For DiskANN, we show that its "slow preprocessing" version provably supports approximate nearest neighbor search query with constant approximation ratio and poly-logarithmic query time, on data sets with bounded "intrinsic" dimension. For the other data structure variants studied, including DiskANN with "fast preprocessing", HNSW and NSG, we present a family of instances on which the empirical query time required to achieve a "reasonable" accuracy is linear in instance size. For example, for DiskANN, we show that the query procedure can take at least \(0.1n\) steps on instances of size \(n\) before it encounters _any_ of the \(5\) nearest neighbors of the query.

## 1 Introduction

The nearest neighbor search (NN) problem is defined as follows: given a set of \(n\) points \(P\) in a metric space \((X,D)\), build a data structure that, given any query point \(q X\), returns \(p P\) closest to \(q\). More generally, given a parameter \(k\), the data structure should report \(k\) points in \(P\) that are closest to \(q\). Often, though not always, the metric space is a \(d\)-dimensional vector space with the distance function induced by the Euclidean norm. Since its introduction in the influential book by Minsky and Papert in the 1960s , the problem has found a tremendous number of applications in machine learning and computer vision . In most of those applications, the underlying metric is induced by a set of points in a high-dimensional space. Since in this setting worst-case efficient nearest neighbor algorithms are unlikely to exist (see e.g., ), various _approximate_ formulations of the problem have been studied extensively. One popular formulation allows the algorithm to return any point \(p^{} P\) whose distance to the query \(q\) is at most \(c\) times the distance between \(q\) and its true nearest neighbor in \(P\); such point \(p^{}\) is called a _\(c\)-approximate nearest neighbor_. In practice, the accuracy of an approximate data structure is often evaluated by estimating the recall, i.e., the average fraction of the true \(k\) nearest neighbors returned by the data structure.

Over the last few decades, many nearest neighbor data structures have been proposed, both exact and approximate. The most popular classes of data structures are tree algorithms (e.g., kd-trees ); locality sensitive hashing ; learning-to-hash and product quantization [35; 36] ; and metric data structures. The last class includes algorithms that work for point-sets in _arbitrary_ metrics, not just those in \(d\)-dimensional vectors normed spaces. This category can be further subdivided into algorithms based on metric trees [6; 32], divide and conquer algorithms [24; 5; 10], and methods based on greedy search in proximity graphs [3; 28; 15; 21]. See the surveys [8; 27] for further overview of these classes of algorithms.

Several algorithms in the latter class, such as HNSW , NSG  and DiskANN , are widely used in practice.1 They have been recently shown empirically to provide excellent tradeoffs between accuracy and query speed . Their design is based on approximating theoretical concepts such as Delaunay or Relative Neighborhood Graphs. However, their worst-case performance is not well understood, especially when the dimension is high. For example, the authors of  point out that "Further analytical evidence is required to confirm whether the resilience of Delaunay graph approximations generalizes to higher dimensional spaces." This stands in contrast to the older algorithms, e.g., those based on the divide and conquer approach, which come with worst-case performance guarantees. Those algorithms are typically analyzed under the assumption that the input point set \(P\) has low _doubling dimension_ (a measure of the intrinsic dimensionality of the point-set)2 and provide near-linear space and logarithmic query time bounds, with the big-Oh constants depending on the doubling dimension. This raises the question whether similar bounds can be obtained for the newer algorithms. On the practical side, investigating the worst-case behavior of data structures is important to understand their benefits and limitations.

Our resultsIn this paper we initiate the study of the worst-case performance of the recent metric data structures based on proximity graphs. As in , we mostly focus on three popular algorithms and their implementations: HNSW, NSG and DiskANN. (In addition, we present similar results for other graph-based algorithms in the supplementary material section.) We present both upper and lower bounds on their worst-case search times. Our specific contributions are as follows:

* **Upper bounds**: For one of the data structures studied, namely DiskANN version with "slow preprocessing", we are able to show a provable worst-case upper bound on its performance. Specifically, we show that (a) the greedy search procedure returns an \((+)\)-approximate neighbor in \(O(_{})\) steps and (b) each step takes at most \(O((4)^{d})\) time. This implies that the overall running time is poly-logarithmic in \(\) when \(d\) is constant. Here \(>1\) denotes a parameter of the DiskANN algorithm (described in Preliminaries, typically set to \(2\)), \(d\) denotes the doubling dimension, while \(\) denotes the _aspect_ ratio of the input set \(P\), i.e., the ratio between the diameter and the distance of the closest pair 3. We also show that our approximation bound is tight, and that the logarithmic dependence of the query time bound on \(\) cannot be removed. * **Lower bounds**: For the other data structure variants studied (NSG, HNSW and DiskANN with "fast preprocessing") we present a family of point sets of size \(n\) for all \(n\) large enough, such that for each \(n\), the _empirical_ query time required to achieve "reasonable" accuracy is linear in \(n\). For example, for DiskANN, we show that the query procedure can take at least \(0.1n\) steps before it encounters any of the 5 nearest neighbors of the query. Remarkably, the point sets are relatively simple: they live in a \(2\)_-dimensional_ Euclidean space, and therefore have a _constant doubling dimension_ (see Preliminaries). We use implementations provided by the authors, publicly available on GitHub . Our hard instance examples are available on GitHub at .

To the best of our knowledge, these are the first worst-case upper bounds and lower bounds for these data structures.

We emphasize that our results do not contradict the empirical evaluations given in the original papers, or in summary studies such as . Indeed, these algorithms have been demonstrated to be very useful and can be highly effective in practice. Nevertheless, we believe that our results provide important information about the behavior of these algorithms. For example, they demonstrate the importance of validating the quality of answers reported by the algorithms when applying them to new data sets. They also shed light on the types of data sets which result in suboptimal performance of the algorithms.

Related workApproximate nearest neighbor search has been studied extensively; the references in the introduction provide some of the main milestones of that rich body of research. In the context of this paper, we also mention [25; 33], which studied theoretical properties of modern graph-based nearest neighbor data structure. However, their focus is on the average-case performance, i.e., under the assumption that the data is generated according to some well-defined distribution, like uniform over the sphere. In contrast, this work is focused on the worst-case behavior of those algorithms, and show that their empirical running time can be high even for relatively simple 2-dimensional data sets.

## 2 Preliminaries

We denote the underlying metric space by \((X,D)\). For any point \(p X\) and radius \(r>0\), we use \(B(p,r)\) to denote a ball of radius \(r\) centered at \(p\), i.e., \(B(p,r)=\{q X:D(p,q) r\}\).

Consider a set of points \(P\). We say that \(P\) has the _doubling constant_\(C\) if for any ball \(B(p,2r)\) centered at some \(p P\), the set \(P B(p,2r)\) can be covered using at most \(C\) balls of radius \(r\), and \(C\) is the smallest number with this property. Doubling constant is a popular measure of "intrinsic dimensionality" of high dimensional point sets, see e.g., [17; 24; 5]. The value \(_{2}C\) is called the _doubling dimension_ of \(P\). Doubling dimension is often used as a measure of the "intrinsic dimensionality" of a data set. It generalizes the "standard" (topological) dimension: for any data set \(P^{d}\) equipped with a metric \(D(p_{1},p_{2})=\|p_{1}-p_{2}\|_{p}\), the doubling dimension of \(P\) is at most \(O(d)\). However, the doubling dimension of \(P^{D}\) could be much lower than \(D\), e.g., if points in \(P\) lie on a low-dimensional manifold. The doubling dimension can be viewed as a finite version of the fractal Hausdorff dimension. Empirical studies (e.g., ) showed that the fractal dimension of real data sets is often smaller than their ambient dimension \(D\).

The following fact is standard and follows from the definition.

**Lemma 2.1**.: _Consider a set of points \(P\) with doubling dimension \(d\). For any ball \(B(p,r)\) centered at some \(p P\) and a constant \(k\), the set \(B(p,r) P\) can be covered by a set of \(m O(k^{d})\) balls with diameter smaller than \(r/k\), i.e. \(B(p,r) P_{i=1}^{m}B(p_{i},r/k)\)._

We use \(d\) to denote the doubling dimension of the point set \(P\), \(=}{D_{min}}\) to denote the aspect ratio of the input set \(P\), where \(D_{max}\) (\(D_{min}\)) is the maximal (minimal) distance between any pair of vertices in the point set. For two points \(x_{u},x_{v}\) in \(P=\{x_{1},...x_{n}\}\), we use \(D(x_{u},x_{v})\) to denote the distance between them, or sometimes \(D(u,v)\) for simplicity.

## 3 Analysis of DiskANN

In this section we show bounds on the performance of DiskANN with slow preprocessing.

### DiskANN recap

In this section we give an overview of the DiskANN procedures. For the full description the reader is referred to the original paper , or section A (supplementary material).

The DiskANN data structure is based on a directed graph \(G\) over the set \(P\), i.e., the set of vertices \(V\) of \(G\) are associated with the set of points \(P\). After the graph is constructed, to answer a given query \(q\), the algorithm performs search starting from some vertex \(s\). In what follows we describe the search and insertion procedure in more detail.

The search procedure, \(GreedySearch(s,q,L)\), has the following parameters: the start vertex \(s\), the query point \(q\), and the queue size \(L\). It performs a best-first-search using a queue of with a bounded length \(L\), until the \(L\) vertices \(v\) with the smallest value of \(D(v,q)\) seen so far are all scanned. Upon completion, it returns a list of vertices in an increasing distance from \(q\) where the first vertex (or the first \(k\) vertices) are answers for the query. Note that as long as the graph is connected, the procedure runs for at least \(L\) steps. The total running time of the procedure is bounded by the number of steps times the out-degree bound of the graph \(G\).

The construction of the graph \(G=(V,E)\) is done by a repeated invocation of a procedure called \(RobustPruning\). For any vertex \(v\), a set of vertices \(U\) (specified later), and parameters \(>1\) and \(R\), \(RobustPruning(v,U,,R)\) proceeds as follows. First, the set \(U\) is sorted in the increasing order of the distance to \(v\). The algorithm traverses this sequence in order. After encountering a new vertex \(u\), the algorithm deletes all other vertices \(w\) from \(U\) such that \(D(u,w)<D(v,w)\). Finally, the node \(v\) is connected to all vertices in \(U\) that have not been pruned.

The starting point of the DiskANN data structure construction algorithm4 is the following simple procedure: for each vertex \(v\), execute \(RobustPruning(v,U,,R)\) with \(U=V\) and \(R=n\). That is, robust pruning is applied to _all_ vertices in the graph. We refer to this procedure as _slow preprocessing_, as it can be seen that a naive implementation of this method takes time \(O(n^{3})\). Although the construction time is slow, we show that this construction method provably constructs a graph whose degree depends only logarithmically on the aspect ratio of the graph (assuming constant doubling dimension), and guarantees that the greedy search procedure has polylogarithmic running time. We note that this result is inspired by an observation in  about convergence of greedy search in a logarithmic number of steps, though to obtain our result we also need to bound the degree of the search graph and analyze the approximation ratio.

Since the slow-preprocessing-algorithm is too slow in practice, the authors of  propose a faster heuristic method to construct the graph \(G\), which we call _fast preprocessing_ method. At the beginning, the graph \(G\) is initialized to be a random \(R\)-regular graph. Then the construction of the graph \(G=(V,E)\) is done incrementally. The construction algorithms make two passes of the point set in random order. For each vertex \(v\) met, the algorithm computes a set of vertices \(U=GreedySearch(s,x_{v},L)\) (for some starting vertex \(s\)) and then calls the pruning procedure on \(U\), not \(V\). That is, it executes \(RobustPruning(v,U,,R)\). After pruning is performed, the insertion procedure adds both edges \((v,u)\) and \((u,v)\) for all vertices \(u U\) output by the pruning procedure. Finally, if the degree of any of \(u U\) exceeds a threshold \(R\), then the set of neighbors of \(u\) is pruned via \(RobustPruning(u,N_{out}(u),,R)\) as well. This construction method is implemented and evaluated in the paper.

### Analysis: preprocessing

For the sake of simplicity, we let \(RobustPruning(p,V,)\) be the no-degree-limit version of edge selection, i.e., where \(R=n\). We first analyze the property of graph constructed by the no-degree-limit pruning, and then show that setting \(R=O((4)^{d})\) yields equivalent results.

**Definition 3.1** (\(\)-shortcut reachability).: Let \( 1\). We say a graph \(G=(V,E)\) is \(\)-shortcut reachable from a vertex \(p\) if for any other vertex \(q\), either \((p,q) E\), or there exists \(p^{}\) s.t. \((p,p^{}) E\) and \(D(p^{},q) D(p,q)\). We say a graph \(G\) is \(\)-shortcut reachable if \(G\) is \(\)-shortcut reachable from any vertex \(v V\).

First, we show that the slow preprocessing algorithm constructs a graph that is \(\)-reachable.

**Lemma 3.2** (\(\)-shortcut reachable).: _For each vertex \(p\), if we connect \(p\) to the output of \(RobustPruning(p,V,)\), then the graph formed is \(\)-shortcut reachable._

Proof.: By Definition 3.1, we only need to prove that the constructed graph is \(\)-shortcut reachable from each vertex. Suppose that, for some vertex \(p\), vertex \(q\) is not connected to \(p\). Then, in \(RobustPruning(p,V,)\), there must have existed a vertex \(p^{} V\) connected to \(p\) s.t. \(D(p^{},q) D(p,q)/\). 

Next, we show that the graph produced by no degree limit \(RobustPruning\) is actually sparse.

**Lemma 3.3** (sparsity).: _For any vertex \(p\), let \(U=RobustPruning(p,V,)\), then \(|U| O((4)^{d})\) where \(d\) is the doubling dimension of the point set \(P\)._

Proof.: We use \(Ring(p,r_{1},r_{2})\) to denote the set of vertices that lie in \(B(p,r_{2})\) but not in \(B(p,r_{1})\). We use \(D_{max}\) (\(D_{min}\)) to denote the maximal (minimal) distances between a pair of vertices in the point set \(P\) and by definition \(=}{D_{min}}\). For each \(i[_{2}]\), we consider \(Ring(p,D_{max}/2^{i},D_{max}/2^{i-1})\) separately. We cover \(Ring(p,D_{max}/2^{i},D_{max}/2^{i-1})\) using balls with radius \(D_{max}/ 2^{i+1}\) by Lemma 2.1. The number of balls required is bounded by \(O((4)^{d})\). Because every two points in the same ball have distance at most \(D_{max}/ 2^{i}\) from each other, and this value is \(\)times smaller than their distance lower bound to \(p\), at most one of them will remain after performing \(RobustPruning(p,V,)\). Therefore, the number of vertices remain after performing \(RobustPruning(p,V,)\) is upper bounded by \(O((4)^{d})\). 

### Analysis: query procedure

We now show that if the graph is constructed using the slow indexing algorithm analyzed in the previous section, then \(GreedySearch(s,q,1)\) starting from any vertex \(s\) returns an \(()\)-approximate nearest neighbor from query \(q\) in a logarithmic number of steps.

**Theorem 3.4**.: _Let \(G=(V,E)\) be an \(\)-shortcut reachable graph constructed using the slow preprocessing method. Consider \(GreedySearch(s,q,L)\) starting with any vertex \(s V\) and \(L=1\) (i.e., the algorithm performs no back-tracking) answering query \(q\). The algorithm finds an \((+)\)-approximate nearest neighbor in \(O(_{})\) steps._

Proof.: Let \(a\) be the nearest neighbor of \(q\), \(v_{i}\) be the \(i\)-th scanned vertex, \(d_{i}=D(v_{i},q)\), with approximate ratio \(c_{i}=}{D(a,q)}\). We use \(=}{D_{min}}\) to denote the aspect ratio of the vertex set.

We know that the distance between \(v_{i}\) and \(a\) is no more than \(d_{i}+D(a,q)\) by triangle inequality, and because \(G\) is \(\)-shortcut reachable, each \(v_{i}\) is either connected to \(a\), or \(v_{i}\) is connected to a vertex \(v^{}\) whose distance to \(a\) is shorter than \(+D(a,q)}{}\). In best-first-search, the next scanned vertex \(v_{i+1}\) should have distance from \(q\) no farther than \(v^{}\), which is at most \(+D(a,q)}{}+D(a,q)\). By induction:

\[d_{i}}+D(a,q)\] (1)

Consider the following three cases, depending on the value of \(D(a,q)\):

Case (1): \(D(s,q)>2D_{max}\). In this case we have \(D(a,q)>D(s,q)-D(a,s)>D(s,q)-D_{max}>D(s,q)/2\). Plugging this into inequality 1 gives us \(c_{i}=}{D(a,q)}D(a,q)}+}+\). Therefore, for any \(>0\), we get a \((+)\)-approximate nearest neighbor in \(_{}\) steps.

Case (2): \(D(s,q) 2D_{max}\) and \(D(a,q)D_{min}\). By inequality 1, the algorithm reaches an \((+)\)-approximate nearest neighbor when \(}< D(a,q)\). Substituting the value of \(D(s,q)\) and \(D(a,q)\) with corresponding upper and lower bound, we obtain that the number of steps needed is \(_{} O(_{ })\)

Case (3): \(D(s,q) 2D_{max}\) and \(D(a,q)<D_{min}\). Suppose at step \(i\), \(d_{i}>d(a,q)\) is not the nearest neighbor. Here we obtain a new lower bound for \(d_{i}\). We know \(d(v_{i},a)>D_{min}\), \(d(v_{i},q)>d(a,q)\), and \(d(a,q)<D_{min}\). By triangle inequality, we have \(d_{i}=d(v_{i},q)>d(v_{i},a)/2>D_{min}/2\). Combing this with inequality 1, we obtain that if \(v_{i}\) is not the exact nearest neighbor, it must satisfy \(}{2} d_{i}}+D(a,q)}{^{i}}+}{4}\). This can only happen when \(i_{}8\). Therefore, the algorithm reaches the exact nearest neighbor in \(O(_{})\) steps. 

We note that similar neighbor selection strategies in the greedy search procedure are also used in HNSW  and NSG . However, we cannot prove query performance guarantees similar to the ones above, as those algorithms use \(=1\).

### A tight convergence rate lower bound for DiskANN

In this section we show that the logarithmic dependence of the query time bound on \(\) is unavoidable, i.e., we cannot replace \(\) with \( n\). The proof is deferred to appendix B.

**Theorem 3.5**.: _For any choice of \(>1\), there exists a set of \(n=2k-1\) points on a one dimensional line with aspect ratio \(=O(^{n})\), and a query \(q\), such that after the slow preprocessing version of DiskANN, greedy search must scan at least \(()\) or \((n)\) vertices before reaching an \(O(1)\)-approximate nearest neighbor of \(q\)._

### A tight approximation lower bound for DiskANN

In Theorem 3.4, we know that the slow preprocessing version of DiskANN algorithm can asymptotically get to an \(\)-approximate nearest neighbor. Here, we provide a simple instance showing that this approximation ratio is tight.

**Theorem 3.6**.: _For any \(>1\), there exists a set of \(n+2\) points in the two dimensional plane under \(l_{1}\) distance where the slow preprocessing version of DiskANN will scan at least \(n\) vertices before getting to a vertex with an approximation ratio smaller than \(\)-approximate nearest neighbor._

We draw the instance in Figure 1. The whole instance can be embedded in a two dimensional grid using \(l_{1}\) distance. Note that in the figure, grids are drawn to label distances and highlight the layout structure. The vertex set \(V\) consists of a point set \(P\) and two single points \(p^{}\) and \(a\). The point set \(P\) further consists of a \(\) (we assume that \(n\) is a perfect square) square grid with grid length \(}\) where each grid point is associated with a point. We denote the upper-rightmost point in \(P\) by \(p_{0}\). Some important distances are \(D(p_{0},p^{})=2\), \(D(p^{},a)=\), \(D(p_{0},a)=\), \(D(a,q)=1+\), \(D(p_{0},q)=+1-\), where \(<0.01\).

**Lemma 3.7**.: _Some properties regarding the graph \(G=(V,E)\) built on the instance in Figure 1 using the slow preprocessing version of DiskANN:_

1. _For any_ \(p P\)_,_ \((p,p^{}) E\) _and_ \((p,a) E\)__
2. _The subgraph of_ \(G\) _induced by point set_ \(P V\) _is strongly connected._
3. _The starting vertex_ \(s\) _is in_ \(P\)_._

Proof.: (1): According to the \(l_{1}\) distance, we can see that for any point \(p P\), \(D(p,p^{})>D(p_{0},p^{})=2\) and \(D(p,a)>D(p_{0},a)=2+=\), and \(D(p^{},a)=\). Therefore, in the procedure \(RobustPruning(p,V,)\), the edge \((p,p^{})\) remains and edge \((p,a)\) is pruned.

(2): Because the point set \(P\) is a uniform grid, we know that for each vertex \(p\), the distances between \(p\) and its four adjacent vertices in the grid are smaller than to all other vertices and must remain after \(RobustPruning(p,V,)\). Therefore the subgraph induced by the vertex set \(P\) in the final graph \(G\) is strongly connected.

(3): In the implementation of DiskANN, the starting point is the closest vertex to the centroid of the whole vertex set, which lies in \(P\). 

Proof of Theorem 3.6.: Now we analyze the behavior of running \(GreedySearch(s,q,L)\) on the instance above on the graph constructed by DiskANN's slow version. We will show that the first \(n\) vertices entering the queue are all vertices in the set \(P\). Additionally, the only neighbor of set \(P\), \(p^{}\), is further from \(q\) than any vertex in the set \(P\). If \(L n\), \(GreedySearch(s,q,L)\) terminates before getting to \(p^{}\) or \(a\). Consequently, the nearest vertex returned by \(GreedySearch(s,q,L)\) has distance at least \(+1-\) from \(q\), which is not a \(\)-approximate nearest neighbor when \(\) approaches \(0\)

Figure 1: \(\)-approximation ratio lower bound instance for slow preprocessing version of DiskANN. The whole instance can be embedded in a two-dimensional grid using \(l_{1}\) distance, and therefore has a constant doubling dimension. Black dots (solid or hollow) are points in the database, the red dot is the query point. **Grids are only used to show the layout structure.** Please refer to Section 3.5 for detailed instance description.

The first vertex in the queue is \(s P\). According to property (1) in Lemma 3.7, no vertex in \(P\) is connected to \(a\), any vertex \(p P\) has distance \(D(p,q)<D(p^{},q)\) thus has higher priority to be scanned than \(p^{}\), and the subset \(P\) is strongly connected, so in the first \(n\) steps (recall that \(|P|=n\)), \(GreedySearch(s,q,L)\) will always scan vertices in \(P\). 

## 4 Experiments

In our experiments, we test the performance of DiskANN, NSG, and HNSW, on two of our constructed hard instances. We run each algorithm on 20 different data sizes \(n\{10^{5},2 10^{5},,2 10^{6}\}\). Each data set consists of \(n\) points in the two-dimensional plane. We plot Recall@5 rates (Figures 3 and 5) and approximation ratios (Figure 6) for answering the query with queue length \(L\) equal to \(m\) where the percentage \(p\) is enumerated from the set \(1\%,2\%,,12\%,15\%,18\%,20\%,30\%,40\%,50\%\). Note that for all graph-based nearest neighbor search algorithms, the value \(L\) lower bounds the number of vertices scanned (and therefore the running time) of the algorithm.

Our results show that, unlike on standard benchmarks, the algorithms scan at least \(10\%\) of the points in our instances before finding quality nearest neighbors. All experiments were ran on Google Cloud. Since our experiments only use combinatorial measures of algorithm complexity (i.e., the number of vertices searched), the results do not depend on the exact details of the computer architecture.

### Hard instance for DiskANN

Though we provide \(()\)-approximate ratio upper bound for _slow_ preprocessing version of DiskANN algorithm, in the experiments, we show that there exists a family of instances where the DiskANN algorithm with _fast_ preprocessing cannot reach any top \(5\) nearest neighbor before scanning \(10\%\) of the vertices. We test our constructed instance using the authors' DiskANN code (fast preprocessing) on GitHub . For a comparison, we also test our constructed instance on our implementation of the slow preprocessing DiskANN algorithm.

In Figure 2, we draw our constructed instance for the DiskANN algorithm for \(n=10^{6}\). It is easy to mimic this instance for other data sizes. The parameters used in our experiment are \(R=70\), \(L=125\) (suggested in the original DiskANN paper , section 4.1, for in-memory search experiments), \(=2\), \(num\_threads=1\).

In the left plot of Figure 3, we can see that in most cases, DiskANN (fast preprocessing version) cannot achieve non-zero Recall@5 unless scanning at least \(10\%\) of the vertex set. In the right plot of Figure 3, we can see that DiskANN (slow preprocessing version) generates very sparse graph data structure given our hard instance. Furthermore, after preprocessing, greedy search is very efficient: it finds the exact nearest neighbor in **only 2 steps:** Overall, it can be seen that, on the hard instances proposed in this paper, the greedy search procedure for query answering is highly efficient after slow preprocessing, and slow (linear time) after fast preprocessing.

Figure 2: Our constructed hard instance for DiskANN for \(n=10^{6}\). The instance lives in a two-dimensional Euclidean space, and therefore has a constant doubling dimension. Black dots (solid or hollow) are points in the database, the red dot is the query point. **Grids are only used to show the layout structure.** See section 4.1 for detailed description and supplementary materials for its implementation.

Description of instance in Figure 2The instance lives in a 2-dimensional plane under the \(l_{2}\) distance, so in what follows, we give the coordinates for each point, which defines the distances between all points. In Figure 2, \(n=10^{6}\), the vertex set \(V\) consists of three sets of points \(M,P\) and \(P^{}\), of sizes \(0.8n\), \(0.1n\), \(0.1n\) respectively, and a single answer point \(a\). Let \(l=0.01*n=10^{4}\). \(M\) is a \(\) square grid with grid side length 1 whose bottom-right corner is \(m=(-1.2l,1.2l)\). \(P\) is a \(\) square grid with grid side length 1 whose upper-right corner is \(p=(-l,0)\). \(P^{}\) is a \(|}|}\) square grid with grid side length 1 and whose bottom-left corner is \(p=(0,l)\). (We assume that \(|M|,|P|,|P^{}|\) are perfect squares.) The query point is \(q=(-0.4l,0)\), whose nearest neighbor is \(a=(0,0.1l)\). Since our experiments measure \(Recall@5\), we add another 4 points very close to \(a\), which is not shown in the figure for simplicity.

IntuitionFirst, the start point \(s\) should lie in the point set \(M\). The three key properties we want to maintain in the construction of the graph are that (1) \(s\) is always connected to at least one vertex in the point sets \(P\) and \(P^{}\). (2) Except for the points randomly connected to \(a\) at initialization, only the points in the set \(P^{}\) can still have edges to \(a\) at the end of the construction. (3) In the final graph, at least \(L\) vertices in set \(P\) are reachable from start point \(s\) without passing through any vertex not in \(P\). If these three properties hold, \(GreedySearch(s,q,L)\) scans only the vertices in \(P\) (except for the start point \(s\)) until it finds a vertex \(p_{i} P\) which is randomly connected to \(a\) at initialization. In expectation, this requires scanning \((n/R)\) vertices, and DiskANN won't reach the nearest neighbor \(a\) before that. The actual running time of the code is even slower.

### Hard instance for NSG and HNSW

A variant of the hard instance from the previous section works for NSG  and HNSW  algorithms as well. In Figure 4, we draw this instance for \(n=10^{6}\). It is easy to mimic this instance for other data sizes.

The parameters of NSG are as follows. We set \(K=400\), \(L=400\), \(iter=12\), \(S=15\), \(R=100\) for EFANNA  to construct the KNN graph. We set \(L=60\), \(R=70\), \(C=500\) for constructing NSG from KNN graph. These parameters were used by the authors when testing on GIST1M dataset, whose size is similar to ours. The parameters of HNSW are as follows. \(ef_{construction}=200\) (as used in their example code), \(M=64\) (maximal degree limit in their suggested range), \(num\_threads=1\). We test our constructed instance using the authors' code available at  (for HNSW) and at 

Figure 3: Results for both variants of the DiskANN algorithm on the instance in Figure 2. **Left figure (fast preprocessing DiskANN):** the horizontal axis represents the data size \(n\), in multiples of \(10^{5}\) points. The vertical axis represents the size of the search queue length \(L\) in terms of the percentage of the data size. Each pixel represents the average value of \(Recall@5\) over 10 runs of the algorithm. Note the sharp transaction of the recall (from \(0\) to \(1\)) for \(L 10\%\). **Right figure (slow preprocessing DiskANN):** as indicated by Theorem 3.4, slow preprocessing DiskANN constructs a very efficient data structure. In the experiment, it finds the exact nearest neighbor **in at most 2 steps** of greedy search, with \(L=1\). In the figure we plot the maximum degree of the constructed graph (on the vertical axis) vs. the data size \(n\) in multiples of \(10^{4}\) points (on the horizontal axis). The plot ends at \(n=10^{5}\), as the slow preprocessing algorithm takes several hundred hours for larger values of \(n\).

(for NSG). We note that both implementations are randomized, but their random seeds are hardwired into the code. To obtain more informative results, we generate and use different random seeds for each algorithm execution, and plot the average recall.

In Figure 5, we can see that, for most values of \(n\), both NSG and HNSW algorithms cannot achieve good Recall@5 unless scanning at least \(10\%\) of the vertex set.

Description of instance in Figure 4This instance is based on the last instance in Figure 2. The main difference is that we use a few chains of points to connect the three subsets \(M\), \(P\), and \(P^{}\). Specifically, on the dotted line, we add points uniformly spaced out, separated by a distance of 5 in the horizontal and/or vertical directions. For \(n=10^{6}\) and \(l=0.01n=10^{4}\), the instance consists of \(400(0.04\%)\) points on the diagonal (from \(m=(-1.2l,-1.2l)\) to \((-l,-l)\)), \(2000(0.2\%)\) on the horizontal line (from \((-l,l)\) to \(p^{}=(0,l)\)) and \(2000(0.2\%)\) points on the vertical line (from \((-l,l)\) to \(p=(-l,0)\)). In total, there are \(4400\) new added points on the chain compared with the previous instance, occupying \(0.44\%\) of the vertex set.

Intuition for the instance in Figure 4The new added vertex chains are there to make KNN graph connected for those nearest neighbor algorithms using KNN as part of their constructions. Thanks to these chains, most of the vertices in subset \(P\) and \(P^{}\) are reachable from the start point. The construction algorithm will only add edges from the vertices in the set \(P^{}\) to \(a\), but not from \(P\). Then,

Figure 4: Our constructed hard instance for NSG and HNSW algorithms for data size \(n=10^{6}\). The instance lives in a two-dimensional Euclidean space, and therefore has a constant doubling dimension. Black dots (solid or hollow) are points in the database, the red dot is the query point. **Grids are only used to show the layout structure.** See Section 4.2 for a detailed description and supplementary materials for its implementation.

Figure 5: Results for running NSG and HNSW algorithm on instance in Figure 4. The horizontal axis represents the data size \(n\), in multiples of \(10^{5}\) points. The vertical axis represents the size of the search queue length \(L\) in terms of the percentage of the data size. Each pixel represents the average value of \(Recall@5\) over 10 runs of the algorithm. Since the algorithm is randomized, each run generates and uses a different random seed.

\(GreedySearch\) on query \(q\) will first traverse the whole subset \(P\) before going to \(P^{}\). Therefore, \(GreedySearch\) cannot get to the nearest neighbor if the queue length limit \(L\) is smaller than \(|P|\).

### Cross-comparisons

We also evaluated DiskANN on hard examples for NSG and HNSW, and vice versa. For \(n=10^{6}\) the results are similar to the ones reported in earlier sections: none of the algorithms can achieve non-zero recall unless \(L\) exceeds \(10\%\) of the data set size. We did not experiment with other values of \(n\), as two different families of instances are anyway helpful for other algorithms, as outlined in the next section and described in detail in supplementary material.

### Evaluating approximation ratio of the algorithms

In addition to \(Recall@5\), we also measure the _average approximation ratio_ of DiskANN (fast preprocessing), NSG and HNSW on slightly modified hard instances from Figure 2 and 4. The modifications only involve scaling of the entire instance and bringing points \(a\) and \(q\) closer, to amplify the approximate ratio for other points. As per Figure 6, the average ratio is very high unless \(L 0.1n\). Note that we do not include the plot for DiskANN (slow preprocessing) here because, as shown in Figure 3, that algorithm (with \(L=1\)) can find the exact nearest neighbor in two steps.

### Experiments on other popular approximate nearest neighbor search algorithms

We also tested other popular approximate nearest neighbor search algorithms covered in the survey : NGT , SSG , KGraph , DPG , NSW , SPTAG-KDT  and EFANNA . We ran them on our hard instances, with \(n\) ranging over \(10^{5},2 10^{5} 2 10^{6}\) and the queue length \(L=0.1n\). The table depicts average Recall@5 rates over all values of \(n\) and 5 or 10 repetitions. One can see that all algorithms achieve sub-optimal recall. See the appendix for more details.

## 5 Conclusions

In this paper we study the worst-case performance of popular graph-based nearest neighbor search algorithms. We demonstrate empirically that almost all of them suffer from linear query times on carefully constructed instances, despite being fast on benchmark data sets . The exception is DiskANN with slow-preprocessing, for which we bound the approximation ratio and the running time. However, its super-linear preprocessing time makes it difficult to use for large data sets.

An important question raised by our work is whether there is a fast preprocessing algorithm, and a query answering procedure, that are _empirically fast_ (e.g., as fast as for DiskANN) while having _worst case_ query time and approximation guarantees. Another interesting direction is to investigate whether it is possible to replicate our findings for _real_ data sets, using adversarially selected queries.

  DiskANN & NSG & HNSW & NGT & SSG & KGraph & DPG & NSW & SPTAG-KDT & EFANNA \\ 
0.0 & 0.27 & 0.1 & 0.05 & 0.16 & 0.42 & 0.37 & 0.02 & 0.02 & 0.12 \\  

Table 1: Average Recall@5 for the 10 algorithms surveyed in , on our constructed instances.

Figure 6: Average approximation ratio results for running DiskANN (fast preprocessing), NSG, and HNSW algorithm on instances in Figure 2 and 4. The horizontal axis depicts the data size \(n\), in multiples of \(10^{5}\). The vertical axis depicts the size \(L\) of the search queue, as a ratio to \(n\). Each pixel represents the average approximation ratio over 10 runs of the algorithm. Since the algorithms are randomized, each run generates and uses a different random seed.

Acknowledgement:This work was supported by the Jacobs Presidential Fellowship, the NSF TRIPODS program (award DMS-2022448), Simons Investigator Award and MIT-IBM Watson AI Lab.