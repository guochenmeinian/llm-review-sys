# OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step

Owen Dugan

Department of Physics

Massachusetts Institute of Technology

Cambridge, MA

odugan@mit.edu

&Donato M. Jimenez-Beneto

Department of Physics

Massachusetts Institute of Technology

Cambridge, MA

donatojb@mit.edu

&Charlotte Loh

Department of EECS

Massachusetts Institute of Technology

Cambridge, MA

cloh@mit.edu

&Zhuo Chen

Department of Physics

Massachusetts Institute of Technology

Cambridge, MA

chenzhuo@mit.edu

&Rumen Dangovski

Department of EECS

Massachusetts Institute of Technology

Cambridge, MA

rumenrd@mit.edu

&Marin Soljacic

Department of Physics

Massachusetts Institute of Technology

Cambridge, MA

soljacic@mit.edu

Equal contributionCorresponding author

###### Abstract

Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations. However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in _a single autoregressive step_, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100% accuracy on single arithmetic operations (\(+,-,,,,,,,\)), outperforming GPT 4o with and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models. Code is available at https://github.com/druidowm/OccamLLM.

## 1 Introduction

Since the release of GPT 3, Large Language Models (LLMs) have dramatically improved in their text generation and reasoning capabilities. This has enabled success in downstream applications including machine translation , sentiment analysis , and interactive dialogue generation, with language models even surpassing human experts on some academic benchmarks that require reading comprehension, reasoning and coding . However even industry-leading LLMs such as GPT 4 cannot reach 100% accuracy on simple arithmetic , limiting their ability to perform basic mathematical tasks. This hinders potential applications of LLMs ranging from chat-bot physics tutors to LLM-powered automated research that could accelerate scientific discovery and technological innovation. The poor arithmetic performance of LLMs is particularly acute for small LLM agents, limiting their usage in smartphone or in multi-agent applications.

To enable accurate calculations, language model systems often resort to running code written by a LLM. However, this comes at the cost of speed; the model must perform multiple autoregressive steps to generate code that performs the appropriate arithmetic operations. This increased decoding time may negatively impact applications such as multi-agent workflows [9; 10] where speed is essential. At the same time, code-based LLM arithmetic mechanisms may increase system vulnerability by providing a mechanism for arbitrary LLM-generated code execution.

We propose an alternative, a framework which enables exact and interpretable LLM arithmetic in _a single autoregressive step_, providing faster and more secure arithmetic capabilities in LLM systems. Our framework uses the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Although our method can in principle work with any symbolic architecture, in this paper we use an interpretable neurosymbolic architecture known as OccamNet [11; 12] because of its interpretability and scalability. Therefore, we term our method OccamLLM, or OccamLlama when using a Llama model as the LLM.

Our core contributions are as follows:

1. We develop a framework for exact and interpretable LLM arithmetic in a single autoregressive step without catastrophic forgetting  or vulnerability from code generation. We explore how to train OccamLlama, including data generation, decoder architecture, and loss function.
2. We benchmark OccamLlama on arithmetic tasks, demonstrating that OccamLlama achieves 100% accuracy on arbitrary single arithmetic operations (\(+\), \(-\), \(\), \(\), \(,,,\), \(\)), more than double the accuracy of GPT 4o. OccamLlama performs slightly better than GPT 4o with Code Interpreter while answering in on average more than 50x fewer generation tokens.
3. We benchmark on mathematical problem solving tasks, showing that OccamLlama can sustain long generations. OccamLlama outperforms both GPT 4o and GPT 4o with code interpreter on average across the benchmarks we tested.

## 2 Related Work

Arithmetic Performance in LLMs.Prior research has trained models on synthetic data, finding that such models can achieve near-perfect accuracy on addition [14; 15], subtraction , multiplication [14; 15], division , and raising to powers . These prior models have been tested only on arithmetic datasets, so their generality has not been assessed. Other work focuses on finetuning LLMs which are already trained on large amounts of general-purpose data on math datasets. Both full-parameter [16; 17] and parameter-efficient (PEFT)  finetuning strategies have been applied. However, finetuning on a single dataset carries the risk of catastrophic forgetting of an LLM's previously acquired linguistic skills . While PEFT techniques have been shown to partially mitigate this effect, this area is still one of active research [20; 21].

    &  &  & \\  & Single Pass &  & Code Execution & Interpretable \\  Fine Tuning & \(\) & \(\) & \(\) & \(\) & \(\) \\ Tool Use & \(\) & \(\) & \(\) & \(\) \\
**OccamLLM** & \(\) & \(\) & \(\) & \(\) \\   

Table 1: OccamLLM is the only approach to improving the arithmetic capabilities of a pretrained LLM which 1) enables single-pass arithmetic, 2) does not risk catastrophic forgetting from finetuning, 3) does not require arbitrary code execution, and 4) provides an interpretable process.

LLMs with Tool Use.Another thrust of prior research has focused on LLM tool use, which we believe is most directly related to our methods. _Calc-X_ introduces a technique to offload arithmetic computations to an external tool like a calculator. The authors curated a large dataset of arithmetic problems and trained a language model that learns to interact with a calculator through the use of tags to signify the calling of the external tool. Several other works [23; 24; 25] follow a similar idea, using crowd workers to annotate tool calls and using this data to train language models to interact with external tools such as a web searching tool, a calculator, or a translation system. These approaches can be prohibitively expensive in annotation costs; _Toolformer_ overcomes this cost by using in-context learning and a language model to generate datasets containing the necessary 'API' tool calls via a self-supervised loss. Further, the above methods all require finetuning of the LLM, placing the LLM at risk of losing generality and its original language modelling abilities through catastrophic forgetting. In contrast, our approach does not involve training the language model. Our 'external tool' is a symbolic model which can be trained to correctly use the hidden states of the language model to perform the required arithmetic computations. The language model is kept frozen throughout this process. Unlike other tool-calling approaches, where the cost of data annotation to train for tool-calling interaction can be prohibitively expensive, in our method, each task only requires manually annotating tens of prompts, a high annotation efficiency. Other prior methods leverage prompt engineering to improve arithmetic performance of LLMs; this is done either through chain-of-thought , or to encourage LLMs to use a code interpreter [28; 29; 30]. Contrary to these methods, our approach does not use code kernels; this provides several advantages: 1) it enables tool use without expending compute on autoregressive steps for token generation, and 2) it avoids running potentially incorrect or malicious code generated by language models.

## 3 Methods

### OccamLLM: Combining a Language Model with a Symbolic Model

In short, the OccamLLM system combines a language model with a symbolic model, namely OccamNet, that can perform arithmetic operations like addition and subtraction. For each token, the corresponding internal hidden states of the language model are fed into a decoder module which initializes the symbolic model such that it executes the operation required by the task described in the input text. A string parser feeds the necessary numbers from the text into OccamNet, which evaluates the desired expression. Finally, a decoder determines whether to use the language model output or the OccamNet output for generating the next token.

Figure 1: The OccamLLM system. For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet. The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights. The decoder then determines whether to use the LLM output or the OccamNet output.

In the example shown in Figure 1, a decoder determines how to initialize OccamNet from the language model hidden states, choosing to have OccamNet perform addition. The text parser then feeds the numbers \(6\) and \(7\) into OccamNet, which adds the numbers, returning \(13\). Finally, a decoder decides to use the OccamNet output instead of the language model output, so the system outputs 13. The new sentence, including the 13, is tokenized and fed back to the LLM to continue autoregressive generation. The language model might later generate "Since she ate two apples, she now has," at which point the switch will again trigger OccamNet, this time implementing \(13-2\) and returning \(11\).

In the subsections below, we describe the OccamLLM system which from our experiments we find to be most performant, even outperforming GPT 4o in several benchmarks. For an analysis of alternate architectures and losses, see Appendix D.

#### 3.1.1 OccamNet

OccamNet is a symbolic architecture that provides an interpretable way of parametrizing probability distributions over a space of functions . We leave a more thorough explanation of OccamNet to  and Appendix E, describing only the relevant components here.

An \(l\)-layer OccamNet with primitives \(\) and \(n\) inputs is an architecture that defines a probability distribution over the space of functions representable as compositions of the primitives in \(\) up to depth \(l\). For example, a two-layer OccamNet with primitives \(=\{,\}\) and one input represents a probability distribution over the set

\[=\{x,(x),(x),((x)),((x)),((x)), ((x))\}.\]

OccamNet has the structure of an \(n\)-input, \(l\)-internal-activation-layer multilayer perceptron with the biases removed and the activations in each layer replaced by the primitives \(\), as shown in Figure 2a. Activation functions may have multiple inputs. We rename the linear layers _softmax layers_, denote the weights of the \(i\)th softmax layer as \(^{(i)}\), and denote the combined weights of OccamNet as \(\).

We define the probability distribution which OccamNet parametrizes by specifying how to sample from it. For each softmax layer output node (shown in red in Figure 2), we select a single connection to that node from a softmax layer input node by sampling from the distribution given by the softmax of the weights of the connections to the different inputs. This process produces a directed acyclic graph (DAG) defining a computational path through the OccamNet activations, such as the one shown in Figure 2b. In this way, each DAG represents a function on the inputs of OccamNet.

To ensure that OccamNet can represent all possible compositions of functions in \(\) up to depth \(l\), we include the following modifications to the OccamNet architecture: 1) for each softmax layer, we concatenate its inputs with the previous softmax layer's inputs to enable the representation of functions with fewer than \(l\) compositions, and 2) we repeat primitives in the \(i\)th activation layer \(A^{l-i}\) times, where \(A\) is the maximum number of inputs of any of the primitives, to ensure that a sufficient number of each primitive is available at each layer. We refer to this modified architecture as _complete OccamNet_ as it can represent the complete set of desired functions. The resulting architecture is shown in Figure 7 in the appendix.

In principle, OccamLLM can work with any symbolic model, i.e., any model that can parameterize a set of symbolic functions or a distribution over such functions. We choose OccamNet as opposed to, for example, a transformer  or recurrent neural network , for two reasons: 1) OccamNet

Figure 2: a) A schematic of the OccamNet architecture, with softmax layers in grey and their outputs in red. b) A Directed Acyclic Graph (DAG) (with edges not connected to the output removed for clarity) formed by sampling from OccamNet. This DAG corresponds to the function \(((x_{1})(x_{0}))\). Modified from .

is interpretable, which we hypothesize makes controlling OccamNet an easier task for a decoder to learn, and 2) OccamNet is parallelizable over multiple samples, allowing for scalable training.

#### 3.1.2 OccamLLM Decoder

The OccamLLM decoder takes the hidden states of a language model and outputs an initialization for OccamNet. This gives the LLM control over which function to apply on the inputs. The decoder acts on each input token separately, producing a different OccamNet initialization for each. Therefore, the arithmetic operations predicted may change along an input sequence, allowing OccamNet's use for different computations in a single multi-token generation. This is crucial in multi-step reasoning scenarios where OccamNet is employed several times for different purposes.

Many decoder architectures are possible. We choose to parameterize the weights of each softmax layer of OccamNet independently, as \((^{(1)},,^{(l)})=(_{1}(), ,_{l}()),\) where \(\) are the hidden states of the language model. We choose

\[_{i}()=_{i}(_{j=1}^{L}w_{i,j} _{j})+^{*(i)}\] (1)

where \(_{j}\) are the hidden states of the \(j\)th layer of the language model, \(w_{i,j}\) are trainable weights, \(_{i}\) are two-layer multilayer perceptrons (MLPs), and \(^{*(i)}\) are untrained weights which initialize all functions to have approximately equal probabilities according to the initialization scheme described in  and explained in Appendix E.4.

#### 3.1.3 OccamLLM Switch

We similarly train a decoder for a switch that, for each input token, is fed the hidden states of the language model and selects whether to use the output of OccamNet or the output of the language model. The decoder outputs a single number from 0 to 1, where all numbers less than or equal to 0.5 correspond to using the output of the language model and all numbers greater than 0.5 correspond to using the output of OccamNet. We choose the following architecture for the switch decoder:

\[_{}()=(_{ }(_{j=1}^{L}w_{,j}_{j}) ).\] (2)

### Data Generation

We create synthetic datasets to train the OccamLLM decoders, which contain instruction prompts for diverse arithmetic tasks. To generate datasets of arbitrary size, we create prompts with placeholders for numbers. Each prompt includes a question with number placeholders, the sampling value range for each number, and a function that computes the answer to the query given the sampled input numbers. The prompts fall into two main categories: purely arithmetic tasks and reasoning problems.

Purely arithmetic prompts are formed by expressions including only symbols, without any natural language added, such as "\(3+85=\)." We create prompts using the following operations: \(+(,)\), \(-(,)\), \((,)\), \((,)\), \(()\), \((,)\), \(_{e}()\), \(()\), \(()\), and \(()\).

We also include word problems that require one or two reasoning steps. We generated 150 single step word problems and 40 multi-step reasoning problems which we modified from examples in the MultiArith training dataset .

#### 3.2.1 OccamNet Decoder Training Data

For training the decoder that controls the weights of OccamNet, we created two types of examples, single queries and concatenated queries. For single queries, we select a single prompt from the problems generated as discussed in Section 3.2. We use the Llama 3 Instruct chat template and fill in the query as the user input and the result as the assistant response, prepending "Answer = " to the later in randomly selected samples (see Appendix A.1.1 for further details). For the concatenated queries of examples, we select a random number of prompts and concatenate the query-response pairs without using the Llama 3 Instruct chat template. The OccamNet decoder is trained to predict only the results of the last query in the sequence. This strategy helps OccamLLM to learn which operation to perform without becoming confused by earlier text, which is useful for continuous generation. To create the training dataset, each example is sampled by first randomly selecting whether to create a single or concatenated query, then randomly selecting the type(s) of prompt(s) used, and finally randomly sampling the input values from the range corresponding to each selected prompt.

#### 3.2.2 OccamLLM Switch Training Data

To train the switch, we generate examples of possible LLM outputs for given input expressions and label the outputs with sequences of 0s or 1s corresponding to whether the language model or the OccamNet output should be used for the next token. Some examples correspond to the prompts described in Section 3.2. For such examples, the LLM output is set to "The answer is" or "Answer = " and the label sequence is all 0s with a 1 at the last token to indicate the system should use OccamNet only to compute the answer. We also manually created and labeled several other examples for diverse scenarios to explicitly teach the system in which cases it should or should not use OccamNet (see Appendix A.1.2 for further details).

To create the training dataset, we concatenate a random number of the above user input - assistant output pairs in a conversational fashion, using the Llama 3 Instruct chat template.

### OccamLLM Training

We train the OccamLLM decoder and the switch separately, as they do not share weights. In all cases, the weights of the LLM are kept frozen. In the first step, we train the system to predict the answer to examples generated by the method explained in Section 3.2.1. The OccamNet decoder processes the hidden states corresponding to the last token of the response and sets the weights of OccamNet such that the correct arithmetic expression is sampled. In this step, we use a rescaled REINFORCE  loss, which can also be interpreted as a Monte-Carlo estimate of the cross-entropy loss (see Appendix D.2):

\[(x,y;W)=-}R(f(x),y) p_{W}[f]}{_{f  p_{W}}R(f(x),y)},\] (3)

where \(p_{W}[f](f;_{W}((x)))\) is the probability distribution represented by the decoder-initialized OccamNet.

Minimizing this loss steers the decoder towards assigning higher probabilities to the functions that maximize the reward \(R(f(x),y)\), which measures the similarity between the correct answer \(y\) and the prediction of OccamNet \(f(x)\). We find setting \(R(f(x),y)=1\) if \(f(x)=y\), and 0 otherwise, most effective. We discuss the OccamNet loss in more detail in Appendix D.

The second step involves training the decoder to route the outputs to OccamNet when needed. We train the switch decoder alone, freezing the weights of the OccamNet decoder of the previous step and minimizing the binary cross-entropy loss between the switch output and the desired output for each token. The OccamLLM switch decoder learns when to route the output to OccamNet in diverse contexts.

## 4 Experiments

For all OccamLLM results, we use Llama 3 8B Instruct and Llama 3 70B Instruct  as the underlying language models. As such, we call our models OccamLlama 8B and OccamLlama 70B, respectively. We use a 1 layer Complete OccamNet with primitives

\[=\{+(,),-(,),(,),(, ),(),(,),_{e}(),( ),(),()\}.\]

This single layer OccamNet can be invoked by the LLM several times during generation to perform complex arithmetic operations accurately. To use the trained OccamLlama for inference, we sample the highest probability function from OccamNet as described in Appendix E.3.

We benchmark our methods against unmodified Llama 2 7B Chat (Llama 2 7B) , unmodified Llama 3 8B Instruct (Llama 3 8B) , unmodified Llama 3 70B Instruct (Llama 3708B) , gpt-3.5-turbo-0125 (GPT 3.5 Turbo) , gpt-4o-2024-05-13 (GPT 4o) , and gpt-4o-2024-05-13 with Code Interpreter (GPT 4o + Code) . To reduce costs, for GPT 4o with Code Interpreter, we test a random subset of 200 datapoints for each dataset.

To determine if a model output is correct, we parse all numbers in the model output and if one of them "matches" the correct answer, we determine that the result is correct. We mark each correct result as 100% accuracy and each incorrect result as 0% accuracy. For each model on each dataset, we report the mean accuracy and the standard error of the mean. To determine if a number matches the result, we first determine how many places after the decimal \(d\) the number should be accurate to. If the number is an integer, we set \(d\) to 2. Otherwise, we set \(d\) to the number of places after the decimal in the model output, clipped between 2 and 5. Finally we state that a number "matches" the result if the number and the result differ by less than \(10^{-4}\). We present further experiment details, including additional experiments, hyperparameters, and prompts in Appendix A.

### Simple Arithmetic Problems

To evaluate OccamLlama and the baselines on purely arithmetic expressions, we create several synthetic datasets. For each of the operations in \(\{+,-,,\}\), the inputs are random 7-digit positive or negative integers. For \(\), the inputs are random 7-digit positive integers. For the logarithms, the examples are log-uniformly sampled in the interval \((10^{-10},10^{10})\); for the exponentials, they are uniformly sampled in the interval \((-10,10)\), and for sines and cosines they are uniformly sampled in the interval \((-2,2)\).

The results of these evaluations are shown in Table 2. More detailed results, including relative error and results for 3- and 5-digit arithmetic, are shown in Appendix A.5.

Both OccamLlama 8B and 70B have \(100.0 0.0\)% accuracy on all tasks, missing 0 out of 9000 problems. On the other hand, we tested GPT 4o with Code Interpreter on fewer problems to save cost, and it missed 3 out of the 1800 problems it faced, achieving an accuracy of \(99.8 0.1\)%.

Furthermore, GPT 4o with Code Interpreter generates on average more than 54 tokens to answer these problems, whereas our model uses OccamNet on the first forward pass. This means that, barring advanced decoding techniques such as speculative decoding , GPT 4o would need to be more than 50x faster than OccamLlama per forward pass to be comparable in answer generation speed on these tasks.

Table 2 demonstrates that arithmetic with LLMs is still challenging; state-of-the-art proprietary language models like GPT 4o achieve less than 40% accuracy on 7-digit division and fail to perform any 7-digit multiplications correctly. Open source LLMs fall farther behind, with Llama 3 8B achieving below 50% on relatively simple tasks such as 7-digit addition.

### Mathematical Problem Solving

To test the performance of OccamLlama on more general mathematical problem solving tasks, we evaluate our method and baselines on the following six benchmarks: AddSub , GSM8K , MultiArith , MATH401 , Single Eq , and SVAMP . All but MATH401 are word

    & OccamLlama & Llama 2 & Llama 3 & GPT 3.5 & GPT 4o & GPT 4o \\  & 8B / 70B & 7B Chat & 8b Instruct & Turbo & & Code \\  Addition & **100.0**\(\)0.0 & 19.2\(\)1.2 & 44.9\(\)1.6 & 65.2\(\)1.5 & 95.7\(\)0.6 & **100.0**\(\)0.0 \\ Subtraction & **100.0**\(\)0.0 & 8.7\(\)0.9 & 34.4\(\)1.5 & 59.8\(\)1.6 & 85.6\(\)1.1 & 99.5\(\)0.5 \\ Multiplication & **100.0**\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 99.0\(\)0.7 \\ Division & **100.0**\(\)0.0 & 2.8\(\)0.5 & 35.3\(\)1.5 & 10.7\(\)1.0 & 38.6\(\)1.5 & **100.0**\(\)0.0 \\ Square Root & **100.0**\(\)0.0 & 0.0\(\)0.0 & **0.0\(\)**0.0 & 0.9\(\)0.3 & 18.6\(\)1.2 & **100.0**\(\)0.0 \\ Exponential & **100.0**\(\)0.0 & 0.3\(\)0.2 & 3.1\(\)0.5 & 12.5\(\)1.0 & 23.2\(\)1.3 & **100.0**\(\)0.0 \\ Logarithm & **100.0**\(\)0.0 & 0.1\(\)0.1 & 0.0\(\)0.0 & 17.1\(\)1.2 & 21.3\(\)1.3 & **100.0**\(\)0.0 \\ Sine & **100.0**\(\)0.0 & 7.6\(\)0.8 & 7.0\(\)0.8 & 13.4\(\)1.1 & 39.3\(\)1.5 & **100.0**\(\)0.0 \\ Cosine & **100.0**\(\)0.0 & 0.8\(\)0.3 & 1.5\(\)0.4 & 6.7\(\)0.8 & 32.8\(\)1.5 & **100.0**\(\)0.0 \\  Average & **100.0**\(\)0.0 & 4.4\(\)0.2 & 14.0\(\)0.4 & 20.7\(\)0.4 & 39.5\(\)0.5 & 99.8\(\)0.1 \\   

Table 2: Accuracy on arithmetic tasks, in percentages. The OccamLlama column corresponds to the results of both OccamLlama 8B and OccamLlama 70B. Higher is better. Bold indicates best performance for each row.

problems requiring longer generation and a mix of reasoning and arithmetic capabilities. MATH401 also includes multistep arithmetic problems which require more than one call to OccamLlama. We selected these datasets (including the MultiArith Float dataset described below) before testing any methods on them to ensure unbiased selection of benchmarks.

Because many of the arithmetic operations required in these datasets are relatively simple, we also create MultiArith Float, a modification of MultiArith in which we select problems which are arithmetically more challenging, while requiring similar levels of reasoning. To this end, we select prompts having input numbers that can be replaced with floats. For instance, \(3.5\) feet or \(\$39.95\) are reasonable but \(3.5\) people is not. Furthermore, we sample input values from ranges larger than those appearing in the MultiArith dataset, in cases where it is reasonable. Float operations and larger additions and multiplications are more difficult for the baseline LLMs but do not make a difference for OccamLLM, so this dataset is particularly useful to show the advantages of the system we propose. Figure 3 shows the results of these evaluations. More detailed results are shown in Appendix A.5.

OccamLlama 70B outperforms both GPT 4o and GPT 4o + Code on average across the benchmarks, demonstrating OccamLlama's strong mathematical problem solving capability. We also note that GPT 4o + Code does not outperform GPT 4o on average, suggesting that existing implementations of LLMs with code generation may not help with mathematical problem solving.

We now consider the performance of OccamLlama 8B, the smaller OccamLlama model. On MultiArith Float and MATH401, two datasets requiring challenging arithmetic, OccamLlama 8B outperforms not only Llama 3 8B but also GPT 4o and GPT 4o + Code. At the same time, most other datasets in this benchmark do not involve challenging arithmetic, meaning that Llama 3 8B is well suited to solve these tasks without assistance; most of the difficulty of these tasks lies in the reasoning rather than in the arithmetic computations. This is further supported by the fact that GPT 4o with Code Interpreter never substantially outperforms and sometimes underperforms GPT 4o on these tasks. As such, it is remarkable that OccamLlama 8B can achieve comparable accuracy to Llama 3 8B even when it is trained on very different data and evaluated on tasks without challenging arithmetic.

The only datasets for which OccamLlama 8B performs noticeably worse than Llama 3 8B are GSM8K and Single Eq, but we believe this results from an imperfect OccamLlama switch, likely stemming from text which is outside of the switch training distribution (see Section 4.3). Fortunately, in Appendix C, we find that the OccamNet decoder is quite robust to out of distribution data and that both the OccamNet and switch decoders generalize well to unseen languages. This suggests that, with relatively little data, it should be possible to teach the switch to handle these unseen cases, something we leave for future work.

In Figure 4, we show example generations from OccamLlama 8B for both arithmetic and reasoning tasks. These generations demonstrate how the OccamLlama switch learns to balance OccamNet

Figure 3: Accuracy of OccamLlama and baselines on mathematical problem solving tasks. Higher is better. OccamLlama 8B achieves accuracy comparable to Llama 3 8B on benchmarks with simple arithmetic, higher accuracy than GPT 4o and GPT 4o + Code on tasks with challenging arithmetic, and accuracy above Llama 3 8B and similar to GPT 3.5 Turbo on average. OccamLlama 70B outperforms GPT 4o and GPT 4o + Code on average.

outputs with LLM outputs, effectively distributing the work between a reasoner (Llama) and a calculator (OccamNet). Because the language model is unaware of the OccamLlama system, its generations behave as if it possesses an interior calculator even though it is actually using a tool. In this way, we combine the benefits of a language model finetuned on arithmetic with the benefits of a language model finetuned to use code for arithmetic, all _without any finetuning_.

### Limitations

In our experiments, we use a single-layer OccamNet as the symbolic network, enabling evaluation of _single-operation_ arithmetic problems. This sometimes poses a challenge on reasoning problems when the base language model generates compound expressions requiring more than one operation to evaluate, such as \(3+5+7=\). A single-layer OccamNet cannot evaluate these expressions. We attempted to overcome this by prompting Llama to break down compound expressions into multiple steps, but we find it difficult to coerce Llama to follow these instructions. Another challenge is that Llama often generates expressions in fractions or percentages, which also constitute compound expressions that are not properly handled by the OccamLLM system. Fortunately, we observed that these compound expressions were typically simple enough for the LLM to evaluate without OccamNet. Therefore, in our experiments, we trained the OccamLLM switch to avoid using OccamNet for compound operations, largely mitigating this issue. Future work could explore other solutions such as integrating a two-layer OccamNet as the symbolic network. We found that these issues are particularly acute in the GSM8K and Single Eq datasets, where the expressions generated by Llama are not prevalent in the switch training data, causing it to sometimes incorrectly trigger OccamNet and degrade performance, as discussed more in Appendix A.5.

Furthermore, we found that the language model sometimes appends further digits to OccamLlama outputs, defeating the purpose of OccamLlama generations. To address this issue, we append "unn." to every number computed with OccamNet, emulating the usual behavior of Llama.

These techniques demonstrate a design paradigm of OccamLlama: by tuning the behaviors of OccamNet and the switch, we can often avoid finetuning the LLM.

Figure 4: Examples from Llama 3 8B Instruct and OccamLlama 8B on (top) an arithmetic problem and (bottom) a mathematical reasoning problem from the MultiArith Float dataset. In OccamLlama, the LLM performs reasoning, the switch predicts when to use OccamNet, and OccamNet performs arithmetic operations. OccamNet’s inputs and outputs are highlighted in purple and green, respectively.

Discussion

We presented OccamLLM, a system enabling exact and interpretable language model arithmetic in a single autoregressive step. Our method does not require modifying the weights of the underlying language model, thereby avoiding risks of catastrophic forgetting. Furthermore, our method avoids security risks arising from running code generated by a language model while outperforming top LLM code generation methods (GPT 4o + Code) on average across our benchmarks.

We benchmarked our method on challenging arithmetic tasks, achieving 100% accuracy where GPT 4o achieves only 40% performance on average. We also benchmarked our method on mathematical problem solving tasks, demonstrating that the OccamLlama switch can accurately balance the LLM for reasoning and OccamNet for arithmetic, outperforming even GPT 4o and GPT 4o with Code Interpreter on average.

Our work could enable smaller LLMs to be as performant as much larger LLMs in arithmetic. Moreover, integrating OccamLLM with larger LLMs like GPT 4o could further improve their arithmetic abilities without requiring a code interpreter. Furthermore, at present, OccamLLM may not integrate with more advanced decoding techniques such as speculative decoding . We hope to explore these avenues in future work.

## 6 Broader Impact

We believe that, in addition to enabling fast, safe, and interpretable arithmetic, OccamLLM demonstrates a new paradigm for tool use. As a proof of concept for more complex tool use, we further train OccamLlama 8B with a two layer Complete OccamNet with the primitives

\[=\{(,),(,),(,),(,)\},\]

which enables OccamLlama to perform up to three arithmetic operations (e.g., \(2 7+3/2\)) in a single autoregressive step. We find that this two-layer OccamLlama can reach near 100% accuracy, even when performing three arithmetic operations in a single autoregressive step, as shown in Table 3. This demonstrates that OccamLLM can be used to perform more complex operations, including composing multiple different tools.

For future work, we plan to explore integrating other tools beyond calculators through a similar technique. This is facilitated by the fact that there are no restrictions on OccamNet's activations; in principle, tools could be placed inside activations of OccamNet, enabling OccamNet to serve as a sort of a mixture of experts for tools. While some tools, like querying a search engine, may still be most effective when integrated into language model systems through language, we believe this work demonstrates that some tools are more effective when they can be more tightly integrated into the language model.