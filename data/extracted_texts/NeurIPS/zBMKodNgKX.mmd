# FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction

Ziwei Li, Xiaoqi Wang, Hong-You Chen, Han-Wei Shen, Wei-Lun Chao

The Ohio State University

{li.5326, wang.5502, chen.9301, shen.94, chao.209}@osu.edu

###### Abstract

Federated learning (FL) has rapidly evolved as a promising paradigm that enables collaborative model training across distributed participants without exchanging their local data. Despite its broad applications in fields such as computer vision, graph learning, and natural language processing, the development of a data projection model that can be effectively used to visualize data in the context of FL is crucial yet remains heavily under-explored. Neighbor embedding (NE) is an essential technique for visualizing complex high-dimensional data, but collaboratively learning a joint NE model is difficult. The key challenge lies in the objective function, as effective visualization algorithms like NE require computing loss functions among pairs of data. In this paper, we introduce FedNE, a novel approach that integrates the FedAvg framework with the contrastive NE technique, without any requirements of shareable data. To address the lack of inter-client repulsion which is crucial for the alignment in the global embedding space, we develop a surrogate loss function that each client learns and shares with each other. Additionally, we propose a data-mixing strategy to augment the local data, aiming to relax the problems of invisible neighbors and false neighbors constructed by the local \(k\)NN graphs. We conduct comprehensive experiments on both synthetic and real-world datasets. The results demonstrate that our FedNE can effectively preserve the neighborhood data structures and enhance the alignment in the global embedding space compared to several baseline methods.

## 1 Introduction

Federated Learning (FL) has emerged as a highly effective decentralized learning framework in which multiple participants collaborate to learn a shared model without sharing the data. In recent years, FL has been extensively studied and applied across various domains, including image and text classifications , computer vision tasks , and graph learning problems . Despite the growing interest in FL, the area of dimensionality reduction within this framework has received limited investigation. However, visualizing and interpreting data from distributed sources is important, as real-world applications often generate large volumes of complex datasets that are stored locally by each participant. For example, different hospitals collect high-dimensional electronic health records (EHRs)  and securely store this patient data within their local systems. As each hospital might only collect limited data or focus on particular diseases, conducting data visualization on a combined dataset can substantially improve disease diagnosis and provide deeper insights. However, sharing sensitive patient information is restricted due to privacy protection. Thus, developing a shared dimensionality reduction model in the FL setting is crucial for facilitating collaborative analysis while maintaining data on local sites.

Dimensionality reduction (DR) refers to constructing a low-dimensional representation from the input data while preserving the essential data structures and patterns. Neighbor embedding (NE) , a family of DR techniques, is widely utilized to visualize complex high-dimensional data due to its ability to preserve neighborhood structures and handle non-linear relationships effectively. Essentially,NE methods (e.g., t-SNE [44; 43] and UMAP ) operate on an attraction-repulsion spectrum , balancing attractive forces that pull similar data points together and repulsive forces that push dissimilar points apart. Defining the objective function requires access to any pairs of data points.

Such a need to access pairwise data for determining the attraction and repulsion terms, however, poses critical challenges to the FL framework. As data are distributed across different clients, computing their pairwise distances becomes non-trivial, making it difficult to recover the centralized objective in an FL setting. Specifically, the _absence of inter-client repulsive forces_ complicates the separation of dissimilar data points. Moreover, within a client, due to the unavailability of others' data, _defining a faithful attraction term based on the top neighbors is challenging_, often resulting in the inaccurate grouping of distant data points. This contrasts with traditional FL tasks, such as image classification, where learning objectives can be decomposed over individual training instances, allowing each client to calculate the loss independently and optimize the model based solely on its local dataset.

To the best of our knowledge, only a few existing works address the problem of decentralized data visualization. Both dSNE  and F-dSNE  methods require publicly shared data that serves as a reference point for aligning the embeddings from different clients. This setup introduces additional assumptions that may not be feasible in real-world applications, and the quantity and representativeness of the reference data can significantly impact the resulting embeddings.

To this end, we proposed a novel Federated neighbor embedding technique, called FedNE, which follows the widely used FedAvg pipeline. It trains a shared NE model that is aware of the global data distribution, without requiring any shareable data across participants. To compensate for the lack of much-needed inter-client repulsive force, besides training a local copy of the NE model, each client additionally learns a surrogate model designed to summarize its local repulsive loss function. During global aggregation, this surrogate model will be sent back to the server along with the local NE model, which other clients can use in the next round of local training. In detail, for a client \(m\), its local surrogate model is designed to approximate the repulsion loss from an arbitrary point to its local data points. By sending the surrogate model to other clients, another client \(m^{}\) can incorporate it into its local loss function for training the NE model. Additionally, to handle the difficulty of estimating the neighborhood, we introduce an intra-client data mixing strategy to simulate the presence of potential neighbors residing on the other clients. This approach augments the local data to enhance the training of the NE model.

To showcase the effectiveness of FedNE, we conduct comprehensive experiments using both synthetic and real-world benchmark datasets used in the field of dimensionality reduction under various FL settings. Both qualitative and quantitative evaluation results have demonstrated that our method outperforms the baseline approaches in preserving the neighborhood data structures and facilitating the embedding alignment in the global space.

**Remark**. It is worth discussing that we understand privacy-preserving is an important aspect to address in the FL framework. However, we want to reiterate the main focus of this paper is identifying the unique challenges in the federated neighbor embedding problem and proposing effective solutions rather than resolving all the FL challenges at once. We discuss the privacy considerations and our further work in section 6.

## 2 Related Work

**Federated learning.** FL aims to train a shared model among multiple participants while ensuring the privacy of each local dataset. FedAvg  is the foundational algorithm that established the general framework for FL. Subsequent algorithms have been proposed to further improve FedAvg in terms of efficiency and accuracy. Some of the work focuses on developing advanced aggregation techniques from various perspectives such as distillation [46; 39], model ensemble [30; 40], and weight matching [45; 52] to better incorporate knowledge learned by local models. Moreover, to minimize the deviation of local models from the global model, many works focus on enhancing the local training procedures [21; 2; 51; 29]. FedXL  was proposed as a novel FL problem framework for optimizing a family of risk optimization problems via an active-passive decomposition strategy. Even though FedXL deals with the loss decomposition for pairwise relations, our main focus and application are very different.

**Neighbor embedding.** Neighbor embedding (NE) is a family of non-linear dimensionality reduction techniques that rely on \(k\)-nearest neighbor (\(k\)NN) graphs to construct the neighboring relationshipswithin the dataset . The key of NE methods lies in leveraging the interplay between attractive forces which bring neighboring data points closer and repulsive forces which push uniformly sampled non-neighboring data pairs further apart. t-SNE  is a well-known NE algorithm. It first converts the data similarities to joint probabilities and then minimizes the Kullback-Leibler divergence between the joint probabilities of data pairs in the high-dimensional space and low-dimensional embedding space. Compared to t-SNE, UMAP  is better at preserving global data structure and more efficient in handling large datasets. A later study has analyzed the effective loss of UMAP and demonstrated that the negative sampling strategy indeed largely reduces the repulsion shown in the original UMAP paper, which explains the reasons for the success of UMAP. Our federated NE work is built upon a recent work that has theoretically connected NE methods with contrastive loss [12; 19]. Their proposed framework unifies t-SNE and UMAP as a spectrum of contrastive neighbor embedding methods.

**Decentralized dimensionality reduction.** As nowadays datasets are often distributively stored, jointly analyzing the data from multiple sources has become increasingly important especially when the data contains sensitive information. SMAP  is a secure multi-party t-SNE. However, as this framework requires data encryption, decryption, and calculations on the encrypted data, SMAP is very time-consuming and thereby it can be impractical to run in real-world applications. dSNE was proposed for visualizing the distributed neuroimaging data . It assumes that a public neuroimaging dataset is available to share with all participants. The shareable data points act as anchors for aligning the local embeddings. To improve the privacy and efficiency of dSNE, Faster AdaCliP dSNE (F-dSNE)  was proposed with differential privacy to provide formal guarantees. While their goal is not to collaboratively learn a global predictive DR model and thus does not follow the formal FL protocols [22; 8; 34] defined in the literature. Both methods require a publicly available dataset to serve as reference gradients communicating across central and local sites. However, since a public dataset may not be available in most real-world scenarios, our FedNE is designed without any requirements for the shareable data.

## 3 FL Framework for Neighbor Embedding

In this section, we first provide background information on neighbor embedding techniques. We then formulate the problem within the context of FL and outline the unique challenges.

### Contrastive Neighbor Embedding

The goal of general NE techniques is to construct the low-dimensional embedding vectors \(_{1},...,_{N}^{d}\) for input data points \(_{1},...,_{N}^{D}\) that preserve pairwise affinities of data points in the high-dimensional space. The neighborhood relationships are defined via building sparse k-nearest-neighbor (\(k\)NN) graphs over the entire dataset with a fixed \(k\) value. Contrastive NE  is a unified framework that establishes a clear mathematical relationship among a range of NE techniques including t-SNE , UMAP , and NCVis , via contrastive loss. For parametric NE, an encoder network \(f_{}\) is trained to map an input data point \(x\) to a low-dimensional representation \(\), i.e., \(=f_{}()\).

In general, the contrastive NE algorithms first build \(k\)NN graphs to determine the set of neighbors \(p_{i}\) for each data point \(x_{i}\) in the high-dimensional space. A numerically stable Cauchy kernel is used for converting a pairwise low-dimensional Euclidean distance to a similarity measure: \((_{i},_{j})=_{i}-_{j}\|_{2}^{2}}\). Then, the contrastive NE  loss is optimized via the negative sampling strategy:

\[()=-}_{ij p_{i}}((f_{ }(_{i}),f_{}(_{j})))}_{}- }{ij}(1-(f_{}(_{i}),f_{ }(_{j})))}_{},\] (1)

where \(p_{i}\) denotes the set of neighboring data points of \(_{i}\).

### Problem Formulation

In general federated learning with one central server and \(M\) clients, each client holds its own training data \(_{m}=\{_{i}\}_{i=1}^{|_{m}|}\) and we denote the collective global data as \(_{}\). The clients' datasets are disjoint which cannot be shared across different local sites, i.e., \(D_{m} D_{m^{}}=\) for \(\ m,m^{}[M],\) and \(\ m m^{}\). Our goal is to learn a single neighbor embedding model such that the high-dimensional affinities of the global data can be retained in a global low-dimensional embedding (2D) space.

It is natural to consider employing the FedAvg  framework since the clients can collaborate by communicating their parametric NE models. Then, the learning objective can be formulated with \(_{i},_{j}_{m}\) as following:

\[^{*}=*{arg\,min}_{}\ }()=_{m=1}^{M}_{m}|}{||} _{m}(),\] (2) \[\ \ _{m}()=- }_{ij p_{i}}((f_{}(_ {i}),f_{}(_{j})))}_{}-}_{ij}(1-(f_{}(_{i}),f_{}(_{j})))\\ .\] (3)

As the client data cannot leave its own device, Equation 2 cannot be solved directly. The vanilla FedAvg relaxes Equation 2 through \(T\) iterations of local training and global model aggregations. The fundamental procedures are defined below,

**Local:** \[_{m}^{(t)}=*{arg\,min}_{} _{m}(),\ \ }^{(t-1)};\] \[\ \ }^{(t)}_{m=1}^{M} _{m}|}{||}_{m}^{(t)}.\] (4)

During local training, each participating client \(m\) updates its model parameter \(_{m}\) for only a few epochs based on the aggregated model \(}^{(t-1)}\) received from the server.

### Challenges of Federated Neighbor Embedding

However, besides the challenges posed by the non-IID data, simply decomposing the problem into Equation 3 indeed _overlooks the pairwise data relationships existing across different clients_. The major difference between the existing FL studies, e.g., image classification, and Federated neighbor embedding is that the objective function of the former problems is instance-based, where their empirical risk is the sum of the risk from each data point:

\[_{m}()=_{m}|}_{i}^{|_{m}|}(_{i},_{i};).\] (5)

As a result, the FL objective in Equation 2, i.e., \(_{m=1}^{M}_{m}|}{||}_{m}()\), is exactly the one as if all the clients' data were gathered at the server.

In the context of Federated neighbor embedding, Equation 3 only considers \(_{j}\) to come from the same client as \(_{i}\). Thus, simply adopting the vanilla FedAvg framework can result in losing all the attractive and repulsive terms that should be computed between different clients.

Therefore, we redefine the FL objective of the contrastive neighbor embedding problem to be

\[()=^{M}_{(i,j) D_{ m}}[(_{i},_{j};)]}_{}+^{M}_{m =1\\ m^{} m}^{M}_{(i,j)(D_{m},D_{m^{}}) }[(_{i},_{j};)]}_{},\] (6)

where the _pairwise_ empirical risk \((_{i},_{j};)\) can be further defined as

\[(_{i},_{j};)=- p_{i}]((f_{ }(_{i}),f_{}(_{j})))}_{}- (_{i}),f_{}(_{j})))}_{ }.\] (7)

Nonetheless, since the inter-client pairwise distances are unknown, Equation 7 cannot be solved directly when \(_{i}\) and \(_{j}\) come from different clients. Specifically, this decentralized setting brings two technical challenges: (1) _Biased repulsion loss_: Negative sampling requires selecting non-neighbor pairs uniformly across the entire data space. Under the FL setting, it is difficult for a client to sample from outside of its local dataset. (2) _Incorrect attraction loss_: Each client only has access to its local data points. This partitioning can result in an incomplete \(k\)NN graph, leading to incorrect \(p_{i}\), as some true neighbors of a data point might reside on other clients.

## 4 Federated Neighbor Embedding: FedNE

To address the aforementioned challenges in applying the FL framework to the neighbor embedding problem, we develop a learnable surrogate loss function1 trained by each client and an intra-client data augmentation technique to tackle the problems in repulsion and attraction terms separately. The two components can be smoothly integrated into the traditional FedAvg pipeline shown in Figure 1.

### Surrogate Loss Function

The repulsive force plays an important role in ensuring separation among dissimilar data points, contributing to the global data arrangement in the embedding space. In the centralized scenario, each data point can uniformly sample its non-neighbors across the entire dataset. In contrast, in the federated setting, as each client can only access its local data, the dissimilar points residing in other clients are invisible, and all the repulsion will be estimated within its own data space. In particular, under severe non-IID situations, where the data distributions across different clients vary significantly , the non-neighboring samples selected to repel are very likely to come from the same clusters in high-dimensional space. As showcased in Figure 2 (a), without explicitly taking care of the repulsion between dissimilar data pairs across clients, those points may still overlap with each other in the embedding space.

At a high level, we seek to learn a function \(f^{}_{m;w}:d R\) for each client \(m\) such that \(f^{}_{m;w} b(1-(f_{}(_{i}),f_{}( _{j})))\) to estimate the repulsion, where \(x_{i}\) and \(x_{j}\) come from different clients. This surrogate model, once shared, enables other clients to input their local data points and obtain a pre-estimated repulsive loss to data points from the originating client.

**Surrogate model training.** We learn \(f^{}_{m;w}\) via supervised learning at each round of FedAvg. To do so, we generate a set of low-dimensional query points as inputs and pre-compute their corresponding repulsive loss to client \(m\)'s data points based on the current projection model. We choose to sample a set of points \(Z_{q}\) within 2D space for the following two reasons. Firstly, as non-neighboring points are uniformly selected across the data space, query points are not required to maintain any specific affinity with \(_{m}\). Second, because the high-dimensional space is often much sparser than 2D space, generating sufficient high-dimensional samples to comprehensively represent the data distributions of all other clients is impractical. Therefore, each client employs a grid sampling strategy at every round, using a predefined step size and extensive ranges along the two dimensions. This procedure is informed by client \(m\)'s current embedding positions, ensuring a more manageable and representative sampling process within the embedding space.

In sum, given the sampled inputs \(Z_{q}=\{z_{q_{1}},z_{q_{2}},,z_{q_{N_{q}}}\}\), we prepare the training targets by computing the repulsive loss between each \(z_{q_{i}}\) and \(b\) random data points in \(_{m}\), i.e., \(l^{}_{q_{i}}=-_{j}^{b}(1-(z_{q_{i}},z_{m}^{(j)}))\). Then, the dataset for supervised training the surrogate repulsion function \(f^{}_{m;w}\) is constructed as \(_{q}=\{(z_{q_{i}},l^{}_{q_{i}})\}_{i=1}^{|_{q _{i}}|}\).

Figure 1: An illustration of one round of FedNE. Besides the general steps in FedAvg: \(\), our local surrogate model training (\(\)) can be smoothly integrated into the whole pipeline. Then, each client conducts its local training (\(\)) using the augmented data and the surrogate models received from all the other clients (\(\)).

**Implementation details.** After building the training dataset, each client trains its surrogate model \(f^{}_{m;w}\) using an MLP with one hidden layer to learn the mapping between the input embedding vectors and their corresponding repulsive loss measured within the client data by minimizing the mean squared error (MSE). The training objective is formulated as follows:

\[w^{*}=*{arg\,min}_{w}|}_{i=1}^{|D_{q}|}(f^ {}_{q_{i}}-f^{}_{m;w}(z_{q_{i}}))^{2}\] (8)

### Neighboring Data Augmentation

To mitigate the limitations of biased local \(k\)NN graphs and ensure better neighborhood representation, we propose an intra-client data mixing strategy. This approach generates additional neighboring data points within each client, thereby enhancing the local data diversity. To be specific, locally constructed \(k\)NN graphs can be biased by the client's local data distribution. As the associated data pairs for computing the attractive loss are distributed across multiple clients, the local neighborhood within each client can be very sparse. Consequently, data points within a client may miss some of their true neighbors (i.e., _invisible neighbors_) considered in the global space. Moreover, when the local data is extremely imbalanced compared to the global view, constructing the \(k\)NN graph with a fixed \(k\) value may result in incorrect neighbor connections between very distant data points (i.e., _false neighbors_). As demonstrated in Figure 2 (b), since data is partitioned across different clients, with a fixed \(k\) value, each local \(k\)NN graph can be even more sparse and erroneously connect very distinct data points.

**Intra-client data mixing.** To address these problems, we employ a straightforward yet effective strategy, intra-client data mixing, to locally generate some data within a client by interpolating between data points and their neighbors. Our method shares a similar spirit to the mixup augmentation [53; 36]. In detail, given a data point \(_{i}\) and the set of its \(k\) nearest neighbor \(_{k}(_{i})\), a new data point is generated by linearly interpolating \(_{i}\) and a random sample in \(_{k}(_{i})\) denoted as \(_{j}\):

\[}=_{i}+(1-)_{j},\] (9)

where \(\) is the weight sampled from the Beta distribution i.e., \(\).

### Overall Framework

Once each client has received the surrogate loss functions of all the other participants (i.e., step 4 in Figure 1), it proceeds to its local training. By combining the original NE loss with the surrogate loss function on the augmented local training data, the new objective for client \(m\) can be formulated as:

\[_{m}(_{m}};)= ((f_{}(_{i}^{m}), f_{}(_{j}^{m})))}_{}-_{m}|}{||}_{ij}(1-(f_{ }(_{i}^{m}),f_{}(_{j}^{m})))}_{}\] \[+_{m^{} m}_{m^{ }}|}{||}_{i}f^{rep}_{m^{};w}(f_{}(_{i}^{m}))}_ {m^{}},\] (10)

where \(_{i}^{m},_{j}^{m}_{m}}\) i.e., the augmented training set. For simplicity, we use \(p\) to represent the neighbor set constructed within \(_{m}}\). \(f^{rep}_{m^{};w}\) is the surrogate model received from another client \(m^{}\).

**Computation.** At first glance, FedNE may seem to introduce heavy computational overhead compared to the original FedAvg framework, as it requires additional surrogate model training at every round. Moreover, a client needs to use multiple received surrogate models to do inference using its own local data. Nevertheless, we want to emphasize that the surrogate model contains only one hidden layer and takes _2D data points_ as inputs. Therefore, training and using them is manageable. We conducted experiments using MNIST with 20 clients on a server with 4 NVIDIA GeForce RTX 2080 Ti GPUs. Compared to FedAvg, our FedNE takes \(35\%\) more GPU time to complete one round of training. For future speed-up, we may consider applying strategies such as clustered FL and we leave this for future work.

## 5 Experiments

### Experimental Settings

**Datasets.** We conduct experimental studies on four benchmark datasets that have been widely used in the field of dimensionality reduction [35; 55]: MNIST , Fashion-MNIST , mouse retina single-cell transcriptomes , and CIFAR-10 . Their statistical information and general settings are summarized in Table 1. For CIFAR-10, since the Euclidean distances in the pixel space of a natural image dataset are not meaningful to preserve , we adopted ImageNet-pretrained ResNet-34  to extract a set of 512D feature vectors as input data. The resulting vectors still retain category-relevant structures that can be suitable for the Euclidean metric.

**Non-IID data partition.** We consider two partitioning strategies to simulate the heterogeneous client distributions: (1) _Dirichlet_: For a class \(c\), we sample \(q_{c}\) from \(Dir()\) and assign data samples of that class \(c\) to a client \(m\) proportionally to \(q_{c}[m]\). The hyperparameter \(\) controls the imbalance level of the data partition where a smaller \(\) indicates a more severe non-IID condition [27; 18]. (2) _Shards_: each client holds data from \(C\) classes, and all samples from the same class are randomly and equally divided among all clients .

**Evaluation metrics.** We assess the quality of data embeddings by analyzing the input high-dimensional data points and their corresponding 2D positions . First, to evaluate the preservation of neighborhood structures, we compute trustworthiness and continuity scores. Trustworthiness quantifies the quality of a low-dimensional embedding by checking whether neighbors in the high-dimensional space remain the same as the ones in the embedded low-dimensional space. Conversely, continuity verifies whether the neighbors in the embedded space correspond to neighbors in the original input space. We use \(k\)NN classification accuracy to measure the effectiveness in preserving the nearest neighbors in the embedded space, where higher scores indicate better class discrimination. We fix \(k=7\) for all the neighborhood metrics. Additionally, we employ steadiness and cohesiveness metrics to evaluate the reliability of the global inter-cluster structures . Steadiness assesses the presence of false groups, while cohesiveness checks for the existence of any missing groups.

**Implementation and training details.** We employ a fully connected neural network with three hidden layers for MNIST, Fashion-MNIST, and CIFAR-10 datasets and a network with two hidden layers for the RNA-Seq dataset. In all experiments, we use Adam optimizer  with learning rate annealing and a batch size of 512 where the batch size refers to the number of edges in the constructed \(k\)NN graphs. Furthermore, we assume full participation during the federated learning and each client performs one epoch of local training (\(E=1\)). In addition, we set \(=0.2\) to perform the intra-client data augmentation in our study.

**Baselines.** We consider four approaches to compare with our FedNE. (1) LocalNE: each client trains the NE model using only its local data without any communication. Two baseline methods:

Figure 2: Toy examples for illustrating the major challenges in solving Federated NE. Color denotes the client identity, and different shapes represent the true categories of the data points (just for demonstration purposes). (a) Without repelling the dissimilar data from other clients, the projected data points from different clients may overlap with each other in the global embedding space. (b) Biased local \(k\)NN graphs may incorrectly connect distant data pairs as neighbors.

(2) FedAvg+NE and (3) FedProx+NE are implemented by applying the widely used FL frameworks [34; 28] to NE model training. (4) GlobalNE: the NE model trained using aggregated global data, serving as the upper bound for performance comparison. Moreover, we want to emphasize that we do not include dSNE and F-dSNE for comparison. Although, at first glance, their titles might imply that they tackled a similar problem, their method is built upon the _non-parametric_ t-SNE and _heavily_ relies on the shareable reference dataset. Thus, they are not comparable with our framework.

### Results

We conduct comprehensive experiments under various non-IID conditions and then evaluate on the global test data of each four datasets. For the highly imbalanced scRNA-Seq dataset, we only consider the Dirichlet partitions. The results of partitions under Dirichlet distributions are summarized in Table 2. Overall, our FedNE outperforms the LocalNE by \(2.62\%\), \(6.12\%\), \(14.31\%\)\(12.69\%\), and \(7.31\%\) on average under the five metrics (i.e., conti., trust., \(k\)NN acc., stead., and cohes.) respectively. In addition, the results of the Shards setting can be found in the appendix, i.e., Table 9.

**Improved preservation of true neighbors.** Both FedNE and the baseline approaches achieved relatively high continuity scores, indicating that the models can easily learn how to pull the data points that are similar in the high-dimensional space closer in the 2D space. However, the lower trustworthiness scores observed with the two baselines, FedAvg+NE and FedProx+NE, imply that without properly addressing incorrect neighborhood connections and separation of data points across different clients, the resulting embeddings may contain false neighbors. Consequently, points that are positioned closely in the 2D space might not be neighbors in the original high-dimensional space.

**Enhanced class discrimination in the embedding space.** Our method has significantly improved the \(k\)NN classification accuracy compared to the baseline results. This improvement highlights the limitations of locally constructed \(k\)-NN graphs, which may incorrectly pull distant data pairs closer in the embedding space. In particular, if two data points from different classes are mistakenly treated as neighbors, class separation will be largely reduced even when inter-client repulsion is considered. Our intra-client data mixing method is specifically designed to relax this problem, and when combined with our surrogate loss function, it ensures an enhanced class separation. For instance, the embedding visualization of FedAvg+NE in Figure 3 under the \(Dir(0.1)\) setup shows a significant overlap between points from different labels. In contrast, FedNE effectively separates the top groups of features in the visualization.

**Better preservation of global inter-cluster structures.** Furthermore, we observe large improvements in preserving the clustering structures according to measures of steadiness and cohesiveness. Specifically, higher steadiness achieved by FedNE indicates that the clusters identified in the projected space better align with the true clusters in the original high-dimensional space. The higher cohesiveness scores imply that the clusters in the high-dimensional space in general can be retained in the projected space, i.e., not splitting into multiple parts. Overall, even though FedNE is not explicitly designed to improve feature clustering, it can produce relatively reliable inter-cluster structures.

### Ablation Study

To verify the effect of our design choices, we conduct ablation studies on removing one of the proposed technical components from the FedNE pipeline in Figure 1. First, we remove the data augmentation by intra-client data mixing technique and only keep the surrogate repulsion model. We then remove the surrogate model and only augment the local data using the intra-client data mixing approach. The comparison results under the setup of \(Dir(0.1)\) with 20 clients are shown in Table 3. With any of the components removed, our FedNE can still outperform the baseline FedAvg+NE. However, we cannot simply conclude which component impacts the most on the final embedding results since the data characteristics and client partitions are very different across different setups. Further studies on our design choices are included in the appendix.

  Dataset & \#Class & \#Training & \#Test & \#Clients(\(M\)) & \#Dimension \\  MNIST & 10 & 60K & 10K & 20/100 & \(784\) \\ Fashion-MNIST & 10 & 60K & 10K & 20/100 & \(784\) \\ scRNA-Seq & 12 & 30K & 4.4K & 20/50 & \(50\) \\ CIFAR-10 & 10 & 50K & 10K & 20/100 & \(512\) \\  

Table 1: Dataset statistics and learning setups.

    & &  &  &  &  \\  Metric & Method & \(M=20\) & \(M=100\) & \(M=20\) & \(M=100\) & \(M=20\) & \(M=50\) & \(M=20\) & \(M=100\) \\   & & \(0.1\) & \(0.5\) & \(0.1\) & \(0.5\) & \(0.1\) & \(0.5\) & \(0.1\) & \(0.5\) & \(0.1\) & \(0.5\) & \(0.1\) & \(0.5\) & \(0.1\) & \(0.5\) \\   & LocalNE & 0.91 & 0.95 & 0.93 & 0.95 & 0.96 & 0.98 & 0.97 & 0.98 & 0.95 & 0.97 & 0.96 & 0.97 & 0.87 & 0.92 & 0.87 & 0.91 \\  & FedAvgNE & **0.97** & **0.98** & **0.96** & **0.97** & 0.98 & **0.99** & **0.99** & **0.99** & **0.97** & **0.98** & **0.97** & **0.98** & **0.93** & **0.94** & 0.93 & 0.94 \\  & FedProNE & **0.97** & **0.98** & **0.96** & **0.97** & **0.99** & **0.99** & 0.98 & **0.99** & **0.97** & **0.98** & **0.97** & **0.98** & **0.93** & **0.94** & **0.95** \\  & FedNE & **0.97** & 0.97 & 0.97 & 0.99 & **0.99** & **0.99** & **0.99** & **0.99** & **0.97** & **0.98** & **0.97** & **0.98** & **0.93** & **0.94** & 0.93 & 0.94 \\  & GlobalNE & 0.97 & 0.97 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.97 & 0.98 & **0.97** & **0.98** & 0.93 & **0.94** & 0.93 & 0.94 \\   & LocalNE & 0.75 & 0.84 & 0.74 & 0.81 & 0.89 & 0.94 & 0.89 & 0.93 & 0.80 & 0.86 & 0.79 & 0.86 & 0.74 & 0.81 & 0.73 & 0.79 \\  & FedAvgNE & 0.78 & 0.91 & 0.74 & 0.88 & **0.95** & **0.97** & **0.89** & **0.96** & 0.85 & 0.90 & 0.84 & 0.89 & 0.82 & 0.86 & 0.78 & 0.84 \\  & FedProNE & 0.78 & 0.91 & 0.75 & 0.88 & **0.95** & **0.97** & 0.89 & **0.96** & 0.84 & 0.90 & 0.83 & 0.89 & 0.81 & **0.86** & **0.80** & **0.85** \\  & FedNE & **0.85** & **0.93** & **0.82** & **0.90** & **0.95** & **0.97** & **0.95** & **0.96** & **0.87** & **0.91** & **0.85** & **0.91** & **0.83** & **0.86** & **0.80** & **0.85** \\  & GlobalNE & 0.94 & 0.94 & 0.97 & 0.97 & 0.93 & 0.93 & 0.87 & & 0.87 & & & & & & & & & & & & \\   & LocalNE & 0.44 & 0.66 & 0.43 & 0.58 & 0.53 & 0.64 & 0.53 & 0.60 & 0.81 & 0.89 & 0.80 & 0.89 & 0.44 & 0.58 & 0.43 & 0.55 \\  & FedAvgNE & 0.48 & 0.76 & 0.43 & 0.67 & 0.60 & **0.70** & 0.55 & 0.66 & 0.85 & 0.94 & 0.84 & 0.93 & 0.55 & 0.72 & 0.48 & 0.68 \\  & FedProNE & 0.49 & 0.75 & 0.44 & 0.68 & 0.60 & 0.69 & 0.64 & 0.66 & 0.83 & 0.94 & 0.83 & 0.93 & 0.55 & 0.71 & 0.50 & 0.69 \\  & FedNE & **0.72** & **0.89** & **0.65** & **0.78** & **0.66** & **0.70** & **0.66** & **0.67** & **0.90** & **0.96** & **0.88** & **0.95** & **0.63** & **0.77** & **0.54** & **0.73** \\  & GlobalNE & 0.93 & 0.73 & & & & & & & & & & & & & & & & \\   & LocalNE & 0.45 & 0.60 & 0.46 & 0.56 & 0.64 & 0.76 & 0.63 & 0.72 & 0.55 & 0.68 & 0.55 & 0.51 & 0.57 & 0.65 & 0.56 & 0.64 \\  & FedAvgNE & 0.54 & 0.73 & 0.43 & 0.69 & 0.79 & **0.84** & 0.62 & **0.81** & 0.54 & 0.79 & 0.68 & **0.78** & **0.67** & 0.71 & 0.63 & 0.69 \\  & FedProNE & 0.51 & 0.72 & 0.43 & 0.68 & 0.79 & **0.84** & 0.62 & **0.81** & 0.59 & 0.79 & 0.66 & **0.78** & 0.67 & 0.71 & 0.63 & **0.71** \\  & FDNE & **0.63** & **0.74** & **0.57** & **0.73** & **0.81** & 0.83 & **0.81** & **0.73** & **0.81**Discussion

**Privacy Preserving.** As introduced in section 4, FedNE incorporates the proposed surrogate models into the traditional FedAvg framework where the surrogate models only take the _low-dimensional_ randomly sampled data as inputs. After training, each surrogate model contains much-compressed information about the corresponding client. Thus, we consider the privacy concerns to be alleviated as one cannot directly reconstruct the original high-dimensional client data. To further enhance privacy protection, our framework can be integrated with various privacy-preserving techniques at different stages. For example, Gaussian mechanisms (GM) can be applied to the parameters of the surrogate model before it is sent to the server.

**Scalability and Computational Complexity.** To our knowledge, the field of dimensionality reduction (DR) focuses on relatively smaller-scale datasets, compared to the studies of classification problems. This is because computational complexity is never a trivial problem even for many outstanding DR techniques, particularly for non-linear methods such as Isomap and t-SNE which have non-convex cost functions . In our experiments, we have included the most widely used benchmarks in the DR literature. Moreover, we have considered more participants and larger scales of data compared to prior work . While, unlike the other FL studies focused on classification, our experiments have not yet included much larger datasets or with increased numbers of clients, we expect our approach to be applicable in real-world settings, for example, cross-silo settings with manageable amounts of clients. In terms of computation, as discussed in section 4, our approach requires only 35% additional GPU time compared to FedAvg, and we expect such overhead to remain similar when going to larger datasets. When the client number increases, we may optionally drop a portion of surrogate models in local training.

## 7 Conclusion

In this paper, we develop a federated neighbor embedding technique built upon the FedAvg framework, which allows collaboratively training a data projection model without any data sharing. To tackle the unique challenges introduced by the pairwise training objective in the NE problem, we propose to learn a surrogate model within each client to compensate for the missing repulsive forces. Moreover, we conduct local data augmentation via an intra-client data mixing technique to address the incorrect neighborhood connection within a client. We compare FedNE to four baseline methods and the experiments have demonstrated its effectiveness in preserving the neighborhood data structures and clustering structures.