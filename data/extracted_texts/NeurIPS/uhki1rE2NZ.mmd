# Parameter Symmetry and Noise Equilibrium

of Stochastic Gradient Descent

 Liu Ziyin

Massachusetts Institute of Technology,

NTT Research

ziyinl@mit.edu

&Mingze Wang

Peking University

mingzewang@stu.pku.edu.cn

&Hongchao Li

The University of Tokyo

lhc@cat.phys.s.u-tokyo.ac.jp

&Lei Wu

Peking University

leiwu@math.pku.edu.cn

###### Abstract

Symmetries are prevalent in deep learning and can significantly influence the learning dynamics of neural networks. In this paper, we examine how exponential symmetries - a broad subclass of continuous symmetries present in the model architecture or loss function - interplay with stochastic gradient descent (SGD). We first prove that gradient noise creates a systematic motion (a "Noether flow") of the parameters \(\) along the degenerate direction to a unique initialization-independent fixed point \(^{*}\). These points are referred to as the _noise equilibria_ because, at these points, noise contributions from different directions are balanced and aligned. Then, we show that the balance and alignment of gradient noise can serve as a novel alternative mechanism for explaining important phenomena such as progressive sharpening/flattening and representation formation within neural networks and have practical implications for understanding techniques like representation normalization and warmup.

## 1 Introduction

Stochastic gradient descent (SGD) and its variants have become the cornerstone algorithms used in deep learning. In the continuous-time limit, the algorithm can be written as [19; 13; 21; 32; 9]:

\[_{t}=- L(_{t})\,t+ (_{t})}\,W_{t},\] (1)

where \(()\) is the covariance matrix of gradient noise (Section 3) with the prefactor \(^{2}=/(2S)\) modeling the impact of a finite learning rate \(\) and batch size \(S\); \(W_{t}\) denotes the Brownian motion. When \(=0\), Eq. (1) corresponds to gradient descent (GD)1. However, SGD and GD can exhibit significantly different behaviors, often converging to solutions with significantly different levels of performance [31; 39; 44; 22; 49]. Notably, even when \(^{2} 1\), where we expect a close resemblance between SGD and GD over finite time , their long-time behaviors still differ substantially . These observations indicate that gradient noise can bias the dynamics significantly, and revealing its underlying mechanism is thus crucial for understanding the disparities between SGD and GD.

Contribution.In this paper, we study the how of SGD noise biases training through the lens of symmetry. Our key contributions are summarized as follows. We show that

1. when symmetry exists in the loss function, the dynamics of SGD can be precisely characterized and is different from GD along the degenerate direction;2. the treatment of common symmetries, including the rescaling and scaling symmetry in deep learning, can be unified in a single theoretical framework that we call the exponential symmetry;
3. for any \(\), every exponential symmetry implies the existence of a unique and attractive fixed point along the degenerate direction for SGD;
4. symmetry and balancing of noise can serve as novel mechanisms for important deep learning phenomena such as progressive sharpening/flattening and latent representation formation.

See Figure 1 for an illustration of how symmetry leads to a systematic flow of SGD. This work is organized as follows. We discuss the most relevant works in Section 2. The main theoretical results are presented in Section 4. We apply our theory to understand specific problems and present numerical results in Section 5. The last section concludes this work. All the proofs are presented in the Appendix.

## 2 Related Works

The dynamics of SGD in the degenerate directions of the loss landscape is a poorly understood problem. There are two closely related prior works. Ref.  studies the dynamics of SGD when there is a simple rescaling symmetry and applies it to derive the stationary distribution of SGD for linear networks. Our result is more general because rescaling symmetry is the simplest case of exponential symmetries2. Another related work is Ref. , which studies a different special case of exponential symmetry, the scale invariance, and in the presence of weight decay. Their analysis assumes the existence of the fixed point of the dynamics, which we proved to exist. Also related is the study of conservation laws under gradient flow [34; 18; 24; 43; 40], which we will discuss more closely in Section 4. However, these works do not take the stochasticity of training into account. Comparing with these results that assume no stochasticity, our result could suggest that SGD converges to initialization-independent solutions, whereas the GD finds solutions are strongly initialization-dependent. In addition, Section D extends our main result to discrete-time SGD.

## 3 Preliminaries

Setup and Notations.Let \(:\) denote the per-sample loss, with \(\) and \(\) denoting the parameter and sample space, respectively. Here, \(z\) includes both the input and label and accordingly. We use \(_{z}=\) to denote the expectation over a given training set. Therefore, \(L()=_{z}[(,z)]\) is the empirical risk function. The covariance of gradient noise is given by

\[()=_{z}[(,z)(,z)^{ }]- L() L()^{}.\]

Additionally, we use \(_{v}():=_{z}[_{v}(,z)_{v}( ,z)^{}]-_{v}L()_{v}L()^{}\) to denote the covariance of gradient noise impacting on the subset of parameters \(v\). Denote by \(_{t}_{t 0}\) the trajectory of SGD or GD. For any \(h:\), we write \(h_{t}=h(_{t})\) and \((_{t})=}{t}h(_{t})\) for brevity. When the context is clear, we also use \(()\) to denote \((,z)\).

Symmetry.The per-sample loss \((,)\) is said to possess the \(Q\)-symmetry if

\[(,z)=(Q_{}(),z),,\] (2)

where \((Q_{})_{}\) is a set of continuous transformation parameterized by \(\). Without loss of generality, we assume \(Q_{0}=\). The most common symmetries exist within the model \(f\), namely \(f_{}\) is invariant under certain transformations of \(\). However, our formalism is slightly more general in the sense that it is also possible for the model to be variant while the per-sample loss remains unchanged, which appears in self-supervised learning , for example.

Figure 1: An example of a 2d loss function with scale invariance: \(()=()\) for a scalar \(\) and \(^{2}\). Because of the symmetry, the gradient \(\) must be _tangential_ to the circles whose center is the origin. This implies that the norm \(||\) does not change during gradient flow training. However, when the training is stochastic or discrete-time, SGD must move outward. If the model starts at \(_{t}\), it must move to a larger circle. As an illustrative example, this loss function has a unique and attractive fixed point: \(||=\). SGD will diverge after training under scale invariance. Also, see Remark 4.4 for a discussion of the difference between discrete-time and continuous-time dynamics.

## 4 Continuous Symmetry and Noise Equilibria

Taking the derivative with respect to \(\) at \(=0\) in Eq. (2), we have

\[0=_{}(,z) J(),\] (3)

where \(J()=Q_{}()}{}_{=0}\). Denote by \(C\) be the antiderivative of \(J\), that is, \( C()=J()\). Then, taking the expectation over \(z\) in (3) gives the following conservation law for GD solutions \((_{t})_{t 0}\):

\[(_{t})=0.\] (4)

Essentially, this is a consequence of Noether's theorem , and \(C\) will be called a "Noether charge" in analogy to theoretical physics. The conservation law (4) implies that the GD trajectory is constrained on the manifold \(\{:C()=C(_{0})\}\). We refer to Ref.  for a study of this type of conservation law under the Bregman Lagrangian .

### Noether Flow in Degenerate Directions

In this paper, we are interested in how \(C(_{t})\) changes, if it changes at all, under SGD. By Ito's lemma, we have the following _Noether flow_ (namely, the flow of the Noether charge):

\[(_{t})=^{2}[(_{t})^{2}C (_{t})],\] (5)

where \(^{2}C\) denotes the Hessian matrix of \(C\). The derivation is deferred to Appendix B. By definition, \((_{t})\) is always positive semidefinite (PSD). Thus, we immediately have: if \(^{2}C\) is PSD throughout training, \(C(_{t})\) is a monotonically increasing function of time. Conversely, if \(^{2}_{}C\) is negative semidefinite (NPD), \(C(_{t})\) is a monotonically decreasing function of time.

The existence of symmetry implies that (with suitable conditions of smoothness) any solution \(\) resides within a connected, loss-invariant manifold, defined as \(_{}:=\{Q_{}():\}\). We term directions within this manifold as "degenerate directions" since movement along them does not change the loss value. Notably, the biased flow (5) suggests that SGD noise can drive SGD to explore within this manifold along these degenerate directions since the value of \(C()\) for \(_{}\) can vary.

### Exponential symmetries

Now, let us focus on a family of symmetries that is common in deep learning. Since the corresponding conserved quantities are quadratic functions of the model parameters, we will refer to this class of symmetries as _exponential symmetries_.

**Definition 4.1**.: \((Q_{})_{}\) is said to be a exponential symmetry if \(J():=}{}Q_{}()|_{=0}=A\) for a symmetric matrix \(A\).

This implies when \( 1\), \(Q_{}=+ A+o()\). In the sequel, we also use the words "\(A\)-symmetry" and "\(Q\)-symmetry" interchangeably since all properties of \(Q_{}\) we need can be derived from \(A\). This definition applies to the following symmetries that are common in deep learning:

* _Rescaling symmetry_: \(Q_{}(a,b)=(a(+1),b/(+1))\), which appears in linear and ReLU networks [7; 48]. In this symmetry, \(A=(I_{a},-I_{b})\), where \(I\) is the identity matrix with dimensions matching that of \(a\) and \(b\).
* _Scaling symmetry_: \(Q_{}=(+1)\), which exists whenever part of the model normalized using techniques like batch normalization , layer normalization , or weight normalization . In this case, \(A=I\).
* _Double rotation symmetry:_ This symmetry appears when parts of the model involve a matrix factorization problem, where for an arbitrary invertible matrix \(B\)\(=(UW)=(UBB^{-1}W)\). Writing the exponential symmetry for this case is a little cumbersome. We need first to view \(U\) and \(W\) as a single vector, and the exponential transformation is given by a block-wise diagonal matrix \((B,...,B,B^{-1},...,B^{-1})\). See Section 5.1 for more detail.

It is possible for only a subset of parameters to have a given symmetry. Mathematically, this corresponds to the case when \(A\) is low-rank. It is also common for \(\) to have multiple exponential symmetries at once, often for different (but not necessarily disjoint) subsets of parameters. For example, a ReLU network has a different rescaling symmetry for every hidden neuron.

It is obvious that under this \(Q\) symmetry, the Noether charge has a simple quadratic form:

\[C()=^{}A.\] (6)

Moreover, the interplay between this symmetry and weight decay can be explicitly characterized in our framework. To this end, we need the following definition.

**Definition 4.2**.: For any \(\), we say \(_{}(,x):=(,x)+||^{2}\) has the \(Q\) symmetry as long as \((,x)\) has the \(Q\) symmetry.

For the SGD dynamics that minimizes \(L_{}()=_{x}[_{}(,x)]\), it follows from (5) that

\[(_{t})=-4 C(_{t})+^{2}[( _{t})A]=:G(_{t}).\] (7)

Thus, a positive \(\) always causes \(|C(_{t})|\) to decay, and the influence of symmetry is determined by the spectrum of \(A\). Denote by \(A=_{j}_{j}n_{j}n_{j}^{}\) the eigendecomposition of \(A\). Then,

\[[(_{t})A]=_{i:_{i}>0}_{i}n_{i}^{} (_{t})n_{i}+_{j:_{j}<0}_{j}n_{j}^{}( _{t})n_{j}.\]

This gives a clear interpretation of the interplay between SGD noise and the exponential symmetry: the noise along the positive directions of \(A\) causes \(C(_{t})\) to grow, while the noise along the negative directions causes \(C(_{t})\) to decay. In other words, the noise-induced dynamics of \(C(_{t})\) is determined by the competition between the noise along the positive- and negative-eigenvalue directions of \(A\).

Time Scales.The above analysis implies that the dynamics of SGD can be decomposed into two parts: the dynamics that directly reduce loss, and the dynamics along the degenerate direction of the loss, which is governed by Eq (5). These two dynamics have essentially independent time scales. The first part is independent of the \(^{2}\) in expectation, whereas the time scale of the dynamics in the degenerate directions depends linearly on \(^{2}\).

The first time scale \(t_{}\) is due to the dynamics of empirical risk minimization. The second time scale \(t_{}\) is the time scale for Eq. (5) to reach equilibrium, which is irrelevant to direct risk minimization. When the parameters are properly tuned, \(t_{}\) is of order \(1\), whereas \(t_{}\) is proportional to \(^{2}=/(2S)\). Therefore, when \(^{2}\) is large, the parameters will stay close to the equilibrium point early in the training, and one can expect that \((_{t})\) is approximately zero after \(t_{}\). In line with Ref. , this can be called the fast-equilibrium phase of learning. Likewise, when \(^{2} 1\), the approach to equilibrium will be slower than the actual time scale of risk minimization, and the dynamics in the degenerate direction only take off when the model has reached a local minimum. This can be called the slow-equilibrium phase of learning.

### Noise Equilibrium and Fixed Point Theorem

It is important and practically relevant to study the stationary points of dynamics in Eq. (7). Formally, the stationary point is reached when \(- C()+[()A]=0\). Because we make essentially no assumption about \(()\) and \(()\), one might feel that it is impossible to guarantee the existence of a fixed point. Remarkably, we prove below that a fixed point exists and is unique for every connected degenerate manifold.

To start, consider the exponential maps generated by \(A\):

\[e^{ A}:=_{ 0}(I+ A+o())^{/},\]

which applies the symmetry transformation to \(\) for \(/\) times. Then, it follows that if we apply \(Q_{}\) transformation to \(\) infinitely many times and for a perturbatively small \(\),

\[()=(e^{ A}).\] (8)

Thus, the exponential symmetry implies the symmetry with respect to an exponential map, a fundamental element of Lie groups . Note that exponential-map symmetry is also an exponential symmetry by definition. For the exponential map, the degenerate direction is clear: for any \(\), \(\) connects to \(e^{ A}\) without any loss function barrier. Therefore, the degenerate direction for any exponential symmetry is unbounded. Now, we prove the following fixed point theorem, which shows that for every exponential symmetry and every \(\), there is one and only one corresponding fixed point in the degenerate direction.

**Theorem 4.3**.: _Let the per-sample loss satisfy the \(A\)-exponential symmetry and \(_{}:=[ A]\). Then, for any \(\) and any \( 0\),3_

1. \(G(_{})\) _(Eq. (_7_)) and_ \(-C(_{})\) _are monotonically decreasing functions of_ \(\)_;_
2. _there exists a_ \(^{*}\{\}\) _such that_ \(G(_{^{*}})=0\)_;_
3. _in addition, if_ \(G(_{}) 0\)_,_ \(^{*}\) _is unique and_ \(G(_{})\) _is strictly monotonic;_
4. _in addition to (_3_), if_ \(()\) _is differentiable,_ \(^{*}()\) _is a differentiable function of_ \(\)_._

_Remark 4.4_.: It is now worthwhile to differentiate gradient flow (GF), GD, SGD, and stochastic gradient flow (SGF). Technically, one can prove that the same result holds for discrete-time GD and SGD in expectation, and GF is the only of the four algorithms that do not obey this theorem (See Section D), and so one could argue that the discrete step size is the essential cause of noise balance. Mathematically, the SGF can be seen as a model of the leading order effect of having a finite step size and thus also share this effect (remember that the Ito Lemma contains a second-order term in \(d\)). That being said, there is a practical caveat: in practice, we find it much easier for models to reach these fixed points with SGD than with GD, and so it is fair to say that this effect is the most dominant when gradient noise is present.

Part (1), together with Part (2), implies that the unique stationary point is essentially attractive. This is because \(\) decreases with \(\) while \(C\) increases with it. Let \(C^{*}=C(_{^{*}})\). Thus, \(C()-C^{*}\) always have the opposite sign of \(^{*}\), while \(C()\) will have the same sign. Conceptually, this means that \(C\) will always move to reduce its distance to \(C^{*}\). Assuming that \(C^{*}\) is a constant in time (or close to a constant, which is often the case at the end of training), Part (1) implies that \((C()-C^{*})-(C()-C^{*})\), signaling a convergence to \(C()=C^{*}\). In other words, SGD will move to restore the balance if it is perturbed away from \(^{*}=0\). If the matrix \( A\) is well-behaved, one can indeed establish the convergence to the fixed point in the relative distance even if \(C^{*}\) is mildly divergent due to diffusion. Because this part is strongly technical and our focus is on the fixed points, we leave the formal statement and its discussion to Appendix B.3.

**Theorem 4.5**.: _(Informal) Let \(C^{*}\) follow a drifted Brownian motion and \( A\) satisfy two well-behaved conditions. Then, either \(C-C^{*} 0\) in \(L_{2}\) or \((C-C^{*})^{2}/(C^{*})^{2} 0\) in probability._

Parts (2) and (3) show that a unique fixed point exists. We note that it is more common than not for the conditions of uniqueness to hold because there is generally no reason for \([()A]\) or \([^{}A]\) to vanish simultaneously, except in some very restrictive subspaces. One major (perhaps the only) reason for the first trace to vanish is when the model is located at an interpolation minimum. However, interpolation minima are irrelevant for modern large-scale problems such as large language models because the amount of available text for training far exceeds the size of the largest models. Even when the interpolation minimum exists, the unique fixed point should still exist when the training is not complete. See Figure 1. Part (4) means that the fixed points of the dynamics is well-behaved. If the parameter \(\) has a small fluctuation around a given location, \(C\) will also have a small fluctuation around the fixed point solution. This justifies approximating \(C\) by a constant value when \(\) changes slowly and with small fluctuation.

Fixed point as a Noise Equilibrium.Let \(^{*}\) be a fixed point of (7). It must satisfy

\[4 C(^{*})=^{2}[(^{*})A].\] (9)

Hence, a large weight decay leads to a small \(|C(^{*})|\), whereas a large gradient noise leads to a large \(|C(^{*})|\). When there is no weight decay, we get a different equilibrium condition: \([(^{*})A]=0\), which can be finite only when \(A\) contains both positive and negative eigenvalues. This equilibrium condition is equivalent to \(_{i:_{i}>0}_{i}n_{i}^{}(^{*})n_{i}=-_{j:_{j} <0}_{j}n_{j}^{}(^{*})n_{j}\). Namely, the overall gradient fluctuation in the two different subspaces specified by the symmetry \(A\) must balance. We will see that the main implication of this result is that the gradient noise between different layers of a deep neural network should be balanced at the end of training. Conceptually, Theorem 4.3 suggests the existence of a special type of fixed point for SGD, which the following definition formalizes.

**Definition 4.6**.: \(\) is a _noise equilibrium_ for a nonconstant function \(C()\) if \(()=0\) under SGD.

## 5 Applications

Now, we analyze the noise equilibria of a few important problems. These examples are prototypes of what appears frequently in deep learning practice and substantiate our arguments with numericalexamples. In addition, an experiment with the scale invariance in normalized tanh networks is presented in Appendix A.1.

### Generalized Matrix Factorization

Exponential symmetry is also observed when the model involves a (generalized) matrix factorization. This occurs in standard matrix completion problems  or within the self-attention of transformers through the key and query matrices . For a (generalized) matrix factorization problem, we have the following symmetry in the objective:

\[(U,W,^{})=(UA,A^{-1}W,^{})\] (10)

for any invertible matrix \(A\) and symmetry-irrelevant parameters \(^{}\). We consider matrices \(A\) that are close to identity: \(A=I+ B+O(^{2})\), and \(A^{-1}=I- B+O(^{2})\). Therefore, for an arbitrary symmetric \(B\), we have a conserved quantity for GD: \(C_{B}()=[UBU^{}]-[W^{}BW]\). This conservation law can also be written in the matrix form, which is a well-known result for GD [8; 24]:

\[(W_{t}W_{t}^{}-U_{t}^{}U_{t})=(W_{0}W_{0}^{}-U_{0}^{ }U_{0}).\] (11)

For SGD, applying (5) gives the following proposition.

**Proposition 5.1**.: _Suppose the symmetry (10) holds. Let \(U=(_{1},,_{d_{2}})^{}^{d_{2} d}\), \(W=(_{1},,_{d_{0}})^{d d_{0}}\), where \(_{i},_{j}^{d}\). Let \(C_{B}()=[UBU^{}]-[W^{}BW]\) for any symmetric matrix \(B^{d d}\). Then, for SGD, we have_

\[_{B}(_{t})=^{2}(_{i=1}^{d_{2}}[ _{_{i}}(_{t})B]-_{j=1}^{d_{0}}[ _{_{j}}(_{t})B]).\]

This dynamics is analytically solvable when \(U^{1 d}\) and \(W^{d 1}\). In this case, taking \(B=E_{k,l}+E_{l,k}\) where \(E_{i,j}\) denotes the matrix with entries of \(1\) at \((i,j)\) and zeros elsewhere. For this choice of \(B\), we obtain that \(C_{B}()=W_{k}W_{l}-U_{k}U_{l}\), and applying the results we have derived, it is easy to show that for some random variable \(r\): \(_{B}(_{t})=-[r(_{t})]C_{B}(_{t})\), which signals an exponential decay. For common problems, \([r(_{t})]>0\). Since the choice of \(B\) is arbitrary, we have that \(W_{k}W_{l} U_{k}U_{l}\) for all \(k\) and \(l\). The message is rather striking: SGD automatically converges to a solution where all neurons output the same sign (\((U_{i})=(U_{j})\)) at an exponential rate.

### Balance and Stability of Matrix Factorization

As a concrete example, let us consider a two-layer linear network (this can also be seen as a variant of standard matrix factorizations):

\[_{}=\|UWx-y\|^{2}+(\|U\|_{F}^{2}+\|W\|_{F}^{2}).\] (12)

where \(x^{d_{x}}\) is the input data, and \(y=y^{}+^{d_{y}}\) is a noisy version of the label. The ground truth mapping is linear and realizable: \(y^{}=U^{*}W^{*}x\). The second moments of the input and noise are denoted as \(_{x}=[xx^{}]\) and \(_{}=[^{}]\), respectively. Note that this problem is essentially identical to a matrix factorization problem, which is not only a theoretical model of neural networks but also an important algorithm frequently in use for recommender systems . The following theorem gives the fixed point of Noether flow.

Figure 2: Comparison between GD and SGD for matrix factorizations. **Left**: Example of a learning trajectory. The convergence speed is almost exponential-like in experiments. **Mid**: evolution of \(10\) individual elements of \(_{ij}:=(U^{}_{U}U-W_{W}W^{})_{ij}\). As the theory shows, they all move close to zero and fluctuate with a small variance. **Right**: Converged solutions of SGD agree with the prediction of Theorem 5.2, but are an order of magnitude away from the solution found by GD, even if they start from the same init.

**Theorem 5.2**.: _Let \(r=UWx-y\) be the prediction residual. For all symmetric \(B\), \(_{B}=0\) if_

\[W_{W}W^{}=U^{}_{U}U,\] (13)

_where \(_{W}=[\|r\|^{2}xx^{}]+2 I\), \(_{U}=[\|x\|^{2}rr^{}]+2 I\)._

See Figure 8 for the convergence of SGD to this solution under different learning rate, batch size and width. The equilibrium condition takes a more suggestive form when the model is at the global minimum, where \(U^{*}W^{*}x-y=\). Assuming that \(\) and \(x\) are independent and that there is no weight decay, we have:

\[W_{x}W^{}=U^{}_{}U\] (14)

Here, the bar over the matrices indicates that they have been normalized by their traces: \(=/[]\). The matrices \(_{W}\) and \(_{U}\) simplifies because at the global minimum, \(r_{i}=_{i}\) and so \([\|x\|^{2}rr^{}]=[_{x}]_{}\) and \([\|r\|^{2}xx^{}]=[_{}]_{x}\). This condition should be compared with the alignment condition for GD in Eq. (11), where the alignment is entirely determined by the initialization and perfect alignment is achieved only if the initialization is perfectly aligned. This condition simplifies further if both \(_{x}\) and \(_{}\) are isotropic, where the equation simplifies to \(WW^{}/d_{x}=U^{}U/d_{y}\). Namely, the two layers will be perfectly aligned, and the overall balance depends only on input and output dimensions. Figure 2-Left shows an experiment that shows that the two-layer linear net is perfectly aligned after training. Here, every point corresponds to the converged solution of an independent run with the same initialization and training procedures but different values of \(_{}\). In agreement with the theory, the two layers are aligned according to Theorem 5.2 under SGD, but not under GD. In fact, GD finds solutions that are more than an order of magnitude away from SGD.

Noise Driven Progressive Sharpening and Flattening.This result implies a previously unknown mechanism of progressive sharpening and flattening, where, during training, the stability of the algorithm steadily improves (during flattening) or deteriorates (during sharpening) . To see this, we first derive a metric of sharpness for this model.

**Proposition 5.3**.: _For the per-sample loss (12), let \(S():=[^{2}L()]\). Then, \(S()=d_{y}\|W_{x}^{1/2}\|_{F}^{2}+\|U\|_{F}^{2}[_ {x}]\)._

The trace of the Hessian is a good metric of the local stability of the GD and SGD algorithm because the trace upper bounds the largest Hessian eigenvalue. Let us analyze the simplest case of an autoencoding task, where the model is at the global minimum. Here, \(_{x} I_{d_{x}}\), \(_{} I_{d_{y}}\). For a random Gaussian initialization with variance \(_{W}^{2}\) and \(_{U}^{2}\), the trace at initialization is, in expectation, \(S_{}=d_{y}d[_{x}](_{W}^{2}+_{U}^{2})\). At the end of the training, the model is close to the global minimum and satisfies Proposition 5.3. Here, the rank of \(U\) and \(W\) matters and is upper bounded by \((d,d_{x})\), and at the global minimum, \(U\) and \(W\) are full-rank (equal to \((d,d_{x})\)), and all the singular values are \(1\). Thus,

\[S_{}=d_{x}d(_{U}^{2}+_{W}^{2})[_{x}],\\ S_{}=2(d,d_{x})[_{x}].\] (15)

Figure 3: A two-layer linear network after training. Here, the problem setting is the same as Figure 8. The theoretical prediction is computed from Theorem 5.2. **Left**: balance of the norm is only achieved when \(_{x}=1\), namely, when the data has an isotropic covariance. We also test SGD with a small weight decay (\(10^{-4}\)), which is sufficiently small that the solution we obtained for SGD without SGD still holds approximately. In contrast, training with GD + WD always converges to a norm-balanced solution. **Right**: the sharpness of the converged model trained with SGD. We see that for some data distributions, SGD converges to a sharper solution, whereas it converges to flatter solutions for other data distributions. This flattening and sharpening effect are both due to the noise-balance effect of SGD. Here, we find that the systematic error between experiment and theory is due to the use of a finite learning rate and decreases as we decrease \(\).

The change in the sharpness during training thus depends crucially on the initialization scheme. For Xavier init, \(_{U}^{2}=(d_{y}+d)^{-1}\) and \(_{W}^{2}=(d+d_{x})^{-1}\), and so \(S_{ init} S_{ end}\) (but \(S_{ init}\) is slightly smaller). Thus, for the Xavier init., the sharpness of loss experiences a small sharpening during training. For Kaiming init., \(_{U}^{2}=1\) and \(_{W}^{2}=d_{x}^{-1}\). Therefore, it always holds that \(S_{ init} S_{ end}\), and so the stability improves as the training proceeds. The only case when the Kaiming init. does not experience progressive flattening is when \(d=d_{x}=d_{y}\), which agrees with the common observation that training is easier if the widths of the model are balanced . See Figure 4 for an experiment. In previous works, the progressive sharpening happens when the model is trained with GD ; our theory suggests an alternative mechanism for it.

A practical technique that the theory explains is using warmup to stabilize training in the early stage. This technique was first proposed in Ref.  for training CNNs, where it was observed that the training is divergent if we start the training at a fixed large learning rate \(_{ max}\). However, this divergent behavior disappears if we perform a warmup training, where the learning rate is increased gradually from a minimal value to \(_{ max}\). Later, the same technique is found to be crucially useful for training large language models . Our theory shows that the gradient noise can drive Kaiming init. to a stabler status where a larger learning can be applied.

Flat or Sharp?Prior works have often argued that SGD prefers flatter solutions to sharper ones (e.g., see Ref. ). The exact solution we found, however, implies a subtle picture: for some datasets, SGD prefers sharper solutions, while for others, SGD prefers flatter solutions. Therefore, there is no causal relationship between SGD training and the sharpness of the found solution. See Figure 3 for the dependence of the flatness on the data distribution. A related question is whether SGD noise creates a similar effect as weight decay training. The answer is also negative: weight decay always prefers smaller norms and, thus, norm-balanced solutions, which are not necessarily noise-aligned solutions. Figure 3 shows that SGD can also lead to unbalanced solutions, unlike weight decay.

### Noise-Aligned Solution of Deep Linear Networks

Here, we apply our result to derive the exact solution of an arbitrarily deep and wide deep linear network, which has been under extensive study due to its connection in loss landscape to deep neural networks [47; 38; 4; 5; 17; 23]. Deep linear networks have also been a major model for understanding the implicit bias of GD . The per-sample loss for a deep linear network can be written as:

\[()=\|W_{D}...W_{1}x-y\|^{2},\] (16)

where \(W_{i}\) is an arbitrary dimensional matrix for all \(i\). The global minimum is realizable: \(y=Vx+\), for i.i.d. noises \(\). Because there is a double rotation symmetry between every two neighboring matrices, the Noether charge can be defined with respect to every such pair of matrices. Let \(B_{i}\) be a symmetric matrix; we define the charges to be \(C_{B_{i}}=W_{i}^{T}B_{i}W_{i}\). The noise equilibrium solution is given by the following theorem.

**Theorem 5.4**.: _Let \(W_{D}...W_{1}=V\). Let \(V^{}=}V}\) such that \(V^{}=LS^{}R\) is its SVD and \(d={ rank}(V^{})\). Then, for all \(i\) and all \(B_{i}\), a noise equilibrium for \(C_{B_{i}}\) at the global minimum is_

\[}W_{D}=L_{D}U_{D-1}^{},\;W_{i}=U_{i}_{ i}U_{i-1}^{},\;W_{1}}=U_{1}_{1}R,\] (17)

_for \(i=2,,D-1\). \(U_{i}\) are arbitrary matrices satisfying \(U_{i}^{T}U_{i}=I_{d d}\), and \(_{i}\) are diagonal matrices such that_

\[_{1}=_{D}=(S^{}})^{(D-2)/2D} },\;_{i}=(S^{}}{d})^{1/D} I_{d d}.\] (18)

Figure 4: Dynamics of the stability condition \(S\) during the training of a rank-1 matrix factorization problem. The solid lines show the training of SGD with Kaiming init. When the learning rate (\(=0.008\)) is too large, SGD diverges (orange line). However, when one starts training at a small learning rate (\(0.001\)) and increases \(\) to \(0.008\) after 5000 iterations, the training remains stable. This is because SGD training improves the stability condition during training, which is in agreement with the theory. In contrast, the stability condition of GD and that of SGD with a Xavier init increases only slightly. Also, note that both Xavier and Kaiming init. under SGD converges to the same stability condition because the equilibrium is unique.

This solution has quite a few striking features. Surprisingly, the norms of all intermediate layers are balanced:

\[[_{1}^{2}]=[_{i}^{2}]=(S^{})^{ 2/D}d^{1-2/D}.\] (19)

All intermediate layers are thus rescaled orthogonal matrices aligned with the neighboring matrices and the only two matrices that process information are the first and the last layer. See Figure 5 for an illustration of this effect. This explains an experimental result first observed in Ref. , where the authors showed that the neural networks find similar solutions when the model is initialized with the standard init., where there is no alignment at the start, and with the aligned init. Thus, the balance and alignment between different layers in the neural networks can be attributed to the rescaling symmetry between each pair of matrices.

### Approximate Symmetry and Bias of SGD

Lastly, let us consider what happens if the loss function only has an approximate symmetry. As a minimal model, let us consider the following loss function: \(=_{1}(,x)+_{2}()\). Here, \(_{1}\) has the \(A\)-symmetry, whereas \(_{2}()\) has no symmetry nor randomness and so \(_{2}\) does not affect \(\) at all. \(\) determines the relative strength between the two terms. In totality, \(\) no longer has the \(A\)-symmetry.

As before, let \(C_{A}=^{}A\). Then, \(_{A}()=-(_{2})^{}A^{*}+^{2} [()A]\), whose fixed point is

\[(_{2})^{}A^{*}=^{2}[() A].\] (20)

This equilibrium condition thus depends strongly on how large \(\) is. When \(\) is small, we see that SGD still favors the fixed point given by Theorem 4.3, but with a first-order correction in \(\).

Conversely, if \(\) is large and \(^{2}\) is small, we can expand around a local minimum of the loss function \(^{*}\), and so the fixed point becomes

\[(-^{*})^{}H(^{*})A^{*}=^{2} [(^{*})A]+O(^{2}\|-^{*}\|+\|-^{*}\|^ {2}),\] (21)

where \(H\) is the Hessian of \(_{2}\). Certainly, this implies that SGD will stay around a point that deviates from the local minimum by an \(O(^{2})\) amount. This stationary point potentially has many solutions. For example, one class of solution is when \(-^{*}\) is an eigenvector of \(H\) with eigenvalue \(h^{*}>0\) and eigenvector \(n\), we can denote \(s=(-^{*})^{}n\) and obtain a direct solution of \(s\):

\[s=[(^{*})A]}{ h^{*}n^{}A ^{*}}.\] (22)

This deviation disappears in the limit \(^{2} 0\). Therefore, this implicit regularization effect is only a consequence of SGD training and is not present under GD. With this condition, one can obtain a clear expression of the deviation of the quantity \(C\) from its local minimum value \(C^{*}:=(^{*})^{}A^{*}\). We have that

\[C()=C^{*}+2(-^{*})^{}A^{*}+O(\|-^{*} \|^{2})=C^{*}+2}{ h^{*}}[ A].\] (23)

Thus, our results in the previous section still apply. The quantity \(C\) will be systematically larger than the local minimum values of \(C\) if the approximate symmetry matrix \(A\) is PD. It is systematically smaller if \(A\) is ND. When \(A\) contains both positive and negative eigenvalues, the deviation of

Figure 5: Norms of weights of multilayer deep linear network during training on MNIST without weight decay. We see that the intermediate layers converge to the same norm during training, whereas the input and output layers are different because they are determined by the input and output noise. This effect is robust against different initializations. This agrees with our analysis for deep linear nets (**Theorem** 5.4). **Left**: initializing all layers with the same norm. **Right**: initializing all layers at randomly different norms.

depends on the local gradient fluctuation balancing condition. When the smallest eigenvalue of \(H\) is close to zero (which is true for common neural networks), the dominant factor that biases \(C\) occurs in this space. Therefore, it is not bad to approximate the deviation as \(C() C^{*}+2^{2}[ A]/h_{},\) where \(h_{}\) is the smallest eigenvalue of the Hessian at the local minimum. In reality, \(\) is neither too large nor too small, and one expects that the solution favored by SGD is an effective interpolation between the true local minimum and the fixed point favored by the symmetries.

A set of experiments is shown in Figure 6, where we compare the latent representation of a two-layer tanh net with the prediction of 5.2. This is a natural example because fully connected networks are believed to be approximated by deep linear networks because they have the same connectivity patterns. We thus compare the prediction of Theorem 5.2 with the experimental results of nonlinear networks. Here, the task is a simple autoencoding task, where \(x^{40}\) and \(y=x+\). \(x\) is sampled from an isotropic Gaussian, and \(\) is an independent non-isotropic (but diagonal) Gaussian noise such that \([_{1}]=5\) and \([_{i}]=1\) for \(i 1\). We train with SGD or GD for \(10^{4}\) iterations. The experimental results show that if trained with SGD, the learned representation agrees with the prediction of Theorem 5.2 well, whereas under GD, the model learned a completely different representation. This suggests that our result may be greatly useful for understanding the structures of latent representations of trained neural networks because the quantity \(W_{x}W\) has a clean interpretation as the normalized covariance matrix of pre-activation hidden representation. Also, this result is not a special feature of tanh networks. Appendix A.4 also shows that the same phenomenon can be observed for swish , ReLU, and leaky-ReLU nets.

## 6 Conclusion

In this work, we have studied how continuous symmetries affect the learning dynamics and fixed points of SGD. The result implies that SGD converges to initialization-independent solutions at the end of training, in sharp contrast to GD, which converges to strongly initialization-dependent solutions. We constructed the theoretical framework of exponential symmetries to study the special tendency of SGD to stay close to a special fixed point along the constant directions of the loss landscape. We proved that every exponential symmetry leads to a mapping of every parameter to a unique and essentially attractive fixed point. This point also has a clean interpretation: it is the point where the gradient noises of SGD in different subspaces _balance_ and _align_. Because of this property, we termed these fixed points the "noise equilibria." The advantage of our result is that it only relies on the existence of symmetries and is independent of the particular definitions of model architecture or data distribution. A limitation of our work is that we only focus on the problems that exponential symmetries can describe. It would be important to extend the result to other types of symmetries in the future. Another interesting future direction is to study these noise equilibria of more advanced models, which may deepen both our understanding of deep learning and neuroscience.