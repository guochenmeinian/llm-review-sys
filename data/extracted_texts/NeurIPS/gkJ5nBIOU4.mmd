# Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity

Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity

Kaja Gruntkowska

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

Alexander Tyurin

KAUST

AIRI

Skoltech

AIRI

Skoltech

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

###### Abstract

Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analysis demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression: we introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. Theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms.

## 1 Introduction

In federated learning (McMahan et al., 2017; Konecny et al., 2016) and large-scale machine learning (Ramesh et al., 2021; OpenAI, 2023), a typical environment consists of multiple devices working together to train a model. Facilitating this collaborative process requires the transmission of substantial information (e.g., gradients, current model) between these devices. In the centralized framework, communication takes place via a server. As a result, practical challenges arise due to the large size of machine learning models and network speed limitations, potentially creating a communication bottleneck (Kairouz et al., 2021; Wang et al., 2023). One possible strategy to reduce this communication burden is to use _lossy compression_(Seide et al., 2014; Alistarh et al., 2017). Our paper focuses on this research direction.

We consider the following nonconvex distributed optimization task:

\[_{x^{d}}\{f(x):=_{i=1}^{n}f_{i}(x)\},\] (1)

where \(x^{d}\) is the vector of parameters of the model, \(n\) is the number of workers and \(f_{i}\,:\,^{d}\), \(i[n]:=\{1,,n\}\) are smooth nonconvex functions. We investigate the scenario where the functions \(f_{i}\) are stored on \(n\) distinct workers, each directly connected to the server via some communication port (Kairouz et al., 2021). At present, we operate under the following generic assumptions:

**Assumption 1.1**.: The function \(f\) is \(L\)-smooth, i.e., \(\| f(x)- f(y)\| L\|x-y\| x,y ^{d}\).

**Assumption 1.2**.: There exists \(f^{*}\) such that \(f(x) f^{*}\; x^{d}\).

In the nonconvex world, our goal is to find a (possibly) random point \(\) such that \([\| f()\|^{2}]\). We refer to such a point an \(\)-stationary point.

### Related Work

Before we discuss more advanced optimization methods, let us consider the simplest baseline: the gradient descent (GD) (Lan, 2020), which iteratively performs updates \(x^{t+1}=x^{t}- f(x^{t})=x^{t}-/n_{i=1}^{n} f_{i}( x^{t})\). In the distributed setting, the method can be implemented as follows: each worker calculates \( f_{i}(x^{t})\) and sends it to the server where the gradients are aggregated, after which the server takes the step and broadcasts \(x^{t+1}\) back to the workers. With step size \(=}{{L}}\), GD finds an \(\)-stationary point after \((L}}{{}})\) steps, where \(^{0}:=f(x^{0})-f^{*}\) for a starting point \(x^{0}\). Since at each step the workers and the server send \((d)\) coordinates/bits, the worker-to-server (w2s, uplink) and server-to-worker (s2w, downlink) communication costs are

\[(L}{}).\] (2)

**Definition 1.3**.: The _worker-to-server (w2s)_ and _server-to-worker (s2w)_ communication complexities of a method are the expected number of coordinates/floats that a worker sends to the server and that the server sends to a worker, respectively, to find an \(\)-solution. The _total communication complexity_ is the sum of these complexities.

**Unbiased compressors.** In this work, to perform lossy compression, we employ mappings from the following family:

**Definition 1.4**.: A stochastic mapping \(\;:\,^{d}^{d}\) is an _unbiased compressor_ if there exists \( 0\) such that

\[[(x)]=x,\,[\|( x)-x\|^{2}]\|x\|^{2}\, x ^{d}.\] (3)

We denote the family of such mappings by \(()\). A canonical example is the Rand\(K(}{{K}}-1)\) sparsifier, which preserves \(K\) random coordinates of a vector scaled by \(}{{K}}\)(Beznosikov et al., 2020). More examples can be found in Wangni et al. (2018); Beznosikov et al. (2020); Szlendak et al. (2021); Horvath et al. (2022). A larger family of compressors, called _biased compressors_, also exists (see Section B). In this paper, we implicitly assume that compressors are mutually independent _across iterations_ of algorithms.

**Worker-to-server compression scales with \(n\).** Many previous works ignore the s2w communication costs and focus solely on w2s compression, assuming that _broadcasting is free_. For nonconvex objective functions, the current state-of-the-art w2s communication complexities are achieved by the MARINA and DASHA methods (Gorbunov et al., 2021; Szlendak et al., 2021; Tyurin and Richtarik, 2023b). Here, two additional assumptions are needed:

**Assumption 1.5**.: The function \(f_{i}\) is \(L_{i}\)-smooth. We define \(^{2}:=_{i=1}^{n}L_{i}^{2}\) and \(L_{}:=_{i[n]}L_{i}\).

**Assumption 1.6**.: For all \((),\) all calls of \(\) are mutually independent.4

Under Assumptions 1.1, 1.2, 1.5, 1.6, and considering the Rand\(K\) compressor with \(K}{{}}\) as an example, the w2s communication complexity of both methods is

\[}_{} (}{}(L+}) )}_{}=(}{ }),\] (4)where we use the facts that \(L\) and \(=}{{K}}-1\) for \(K\). The key observation is that when comparing (2) and (4), one sees that (4) can be \(\) times smaller if \( L\). Consequently, the communication complexity of \(/\) scales with the number of workers \(n\), and can _provably_ improve the _worker-to-server_ communication complexity \((d^{^{0}L}}{{}})\) achieved by GD.

**Server-to-worker compression does not scale with \(n\).** In certain applications, the significance of s2w communication cannot be ignored. In 4G LTE and 5G networks, w2s and s2w communication speeds can be almost the same (Huang et al., 2012) or differ by at most a factor of \(10\)(Narayanan et al., 2021). Although important, this issue is often overlooked and that is why it is the s2w communication that this work places a central emphasis on.

There exist many papers using communication compression techniques to reduce the s2w communication (Zheng et al., 2019; Liu et al., 2020; Philippenko and Dieuleveut, 2021; Fatkhullin et al., 2021; Gruntkowska et al., 2023; Tyurin and Richtarik, 2023a). However, to the best of our knowledge, under Assumptions 1.1, 1.2, 1.5, and 1.6, in the worst case, all previous theoretical s2w communication guarantees _are greater or equal_ to (2). As an example, let us consider the result from Gruntkowska et al. (2023)[Theorem E.3]. If the server employs operators from \(()\) and we ignore w2s compression, the method from Gruntkowska et al. (2023) converges in \((L}}{{}})\) iterations. Thus, with \(K\), the s2w communication complexity is \((KL}}{{}} )=(d^{^{0}L}}{{}})\). Another method, called CORE, proposed by Yue et al. (2023), achieves s2w and w2s communication complexities equal to \(((f)^{0}L}}{{}})\), where \(r_{1}(f)\) is a uniform upper bound of the trace of the Hessian. When \(r_{1}(f) dL\), CORE can improve on GD. However, this complexity does not scale with \(n\) and requires an additional assumption about the Hessian of \(f\).

## 2 Contributions

In our work, we aim to investigate whether the _server-to-worker and total communication complexities_ (2) of the vanilla GD method can be improved. We make the following contributions:

1. We start by proving the impossibility of devising a method where the server communicates with the workers using unbiased compressors \(()\) (or biased compressors from Section B) and achieves an _iteration rate_ faster than \((}}{{}})\) (Theorem 3.1) under Assumptions 1.1, 1.2 and 1.6. This result provides a lower bound for any method that applies such compressors to vectors sent from the server to the workers in every iteration. Moreover, we prove a more general iteration lower bound of \((}}{{}})\) for all methods where the server zeroes out a coordinate with probability \(}{{(+1)}}\) (see Remark 3.2).

2. In view of this result, it is clear that an extra assumption is needed to break the lower bound \((}}{{}})\). In response, we introduce a novel assumption termed "Functional \((L_{A},L_{B})\) Inequality" (see Assumption 4.2). We prove that this assumption is relatively weak and holds, for instance, under the local smoothness of the functions \(f_{i}\) (see Assumption 1.5).

3. We develop a new method for downlink compression, \(/\), and show that under our new assumption, along with Assumptions 1.1 1.2, and 1.6, it can achieve the iteration rate of

\[(L}{{}}+L_{A}( +1)}{{}}+L_{B}(+1)}{})\]

(see Theorem D.1 with \(p=}{{(+1)}}\) + Lemma A.5). Notably, when \(L_{A}\) is small and \(n 1\), this complexity is _provably_ superior to \(((L}}{{}}+}{{}})\) and the complexities of the previous compressed methods. In this context, \(L_{A}\) serves as a measure of the similarity between the functions \(f_{i}\), and can be bounded by the "variance" of the Hessians of the functions \(f_{i}\) (see Theorem 4.8). Thus, \(/\)_is the first method whose iteration complexity can provably improve with the number of workers \(n\)_.

4. Moreover, \(/\) can achieve the s2w communication complexity of

\[(L}{n{}}+L_{A}} {{}}).\]

When \(L_{A}\) is small and \(n 1\), this communication complexity is provably superior to (2) and the communication complexities of the previous compressed methods.

Our theoretical improvements can be combined with techniques enhancing the _w2s communication complexities_. In particular, by combining MARINA-P with MARINA(Gorbunov et al., 2021) and adding the crucial momentum step, we develop a new method, M3, that guarantees a _total communication complexity_ (s2w + w2s) of

\[(L_{}}{n^{1/3}}+L_{A}}{}).\]

When \(n 1\) and in the close-to-homogeneous regime, i.e., when \(L_{A}\) is small, this complexity is better than (2) and the complexities of the previous bidirectionally compressed methods.

6. Our theoretical results are supported by numerical experiments (see Section F).

## 3 Lower Bound under Smoothness

Let us first investigate the possibility of improving the iteration complexity \((L}}{{}})\) under Assumptions 1.1,1.2 and 1.6. In Section G, we consider a family of methods that include those proposed in Zheng et al. (2019); Liu et al. (2020); Philippenko and Dieuleveut (2021); Fatkhullin et al. (2021); Gruntkowska et al. (2023), where the server communicates with workers using unbiased/biased compressors, and establish that

**Theorem 3.1** (Slightly Less Formal Reformulation of Theorem G.5).: _Under Assumptions 1.1, 1.2 and 1.6, all methods in which the server communicates with clients using different and independent unbiased compressors from \(()\) and sends one compressed vector to each worker cannot converge before \((}}{{}})\) iterations._

_Remark 3.2_.: The theorem remains applicable to biased compressors \(()\) (see Section B) with a lower bound of \((L}}{{}})\). This is because if \(()\), then \((+1)^{-1}((+1)^{-1}).\) We also establish a more general result (Theorem G.4): "all methods in which the server zeroes out a coordinate with probability \( p\) independently _across iterations_ cannot converge before \((L}}{{p}})\) iterations."

This lower bound is tight up to a constant factor. For instance, under exactly the same assumptions, the EF21-P mechanism from Gruntkowska et al. (2023) converges after \((}}{{}})\) iterations. Unlike (4), this convergence rate does not scale with \(n,\) and Theorem 3.1 leaves no room for improvement. Consequently, breaking the lower bound requires an additional assumption about the structure of the problem. Before presenting our candidate assumption, we first introduce the ingredients needed to leverage it to the fullest extent: our novel downlink compression method and the type of compressors we shall employ.

## 4 The MARINA-P Method

Let us first recall the MARINA method (Gorbunov et al., 2021; Szlendak et al., 2021):

\[x^{t+1} =x^{t}- g^{t}, c^{t}(p)\] (5) \[g_{i}^{t+1} = f_{i}(x^{t+1})&c^{t}=1,\\ g^{t}+_{i}^{t}( f_{i}(x^{t+1})- f_{i}(x^{t}))&c^{t}=0 i[n],\] \[g^{t+1} =_{i=1}^{n}g_{i}^{t+1},\]

where \(g^{0}= f(x^{0})\). Motivated by MARINA, we design its primal counterpart, MARINA-P (Algorithm 1), operating in the primal space of the model parameters, as outlined in (6).

At each iteration of MARINA-P, the workers calculate \( f_{i}(w_{i}^{t})\) and transmit it to the server. The server then averages the gradients and updates the global model \(x^{t}\). Subsequently, with some (typically small) probability \(p\), the master sends the non-compressed vector \(x^{t+1}\) to all workers. Otherwise, the \(i^{}\) worker receives a compressed vector \(_{i}^{t}(x^{t+1}-x^{t})\). Each worker then uses the received message to compute \(w_{i}^{t+1}\) locally. Importantly, \(_{1}^{t}(x^{t+1}-x^{t}),,_{n}^{t}(x^{t+1}-x^{t})\)_can differ_, and this distinction will form the basis of our forthcoming advancements.

The MARINA-P Method:

Initialize vectors \(x_{0},w_{0}^{0},,w_{n}^{0}^{d}\), step size \(>0\), probability \(0<p 1\) and compressors \(_{1}^{t},,_{n}^{t}(_{P})\) for all \(t 0\). The method iterates

\[g^{t} =_{i=1}^{n} f_{i}(w_{i}^{t}),\] (6) \[x^{t+1} =x^{t}- g^{t},\] \[c^{t} (p),\] \[w_{i}^{t+1} =x^{t+1}&c^{t}=1,\\ w_{i}^{t}+_{i}^{t}(x^{t+1}-x^{t})&c^{t}=0\] for all \(i[n]\).

We denote \(w^{t}:=}{{n}}_{i=1}^{n}w_{i}^{t}\). See the implementation in Algorithm 1.

Comparing (5) and (6), MARINA-P and MARINA are dual methods: both learn control variables (\(w_{i}^{t}\) and \(g_{i}^{t}\)), compress the differences (\(x^{t+1}-x^{t}\) and \( f_{i}(x^{t+1})- f_{i}(x^{t})\)), and with some probability \(p\) send non-compressed vectors (\(x^{t+1}\) and \( f_{i}(x^{t+1})\)). However, unlike MARINA, which compresses vectors sent _from workers to server_ and operates in the _dual_ space of gradients, MARINA-P compresses messages sent _from server to workers_ and operates in the _primal_ space of arguments.

Let us take \(K(}{{K}}-1)\) as an example. If we set \(p=(+1)^{-1}=}{{d}}\) to balance heavy communications of \(x^{t+1}\) and light communications of \(_{i}^{t}\) in (6), MARINA-P averages sending \(pd+(1-p)K 2K\) coordinates per iteration. Then, the lower bound from Theorem G.4 implies that at least \((L}}{{e}})\) iterations of the algorithm are needed.

At first glance, it seems that MARINA-P does not offer any extra benefits compared to previous methods, and that is true - we could not expect to break the lower bound. However, as we shall soon see, under an extra assumption, MARINA-P can achieve a communication complexity that improves with \(n\).

### Three ways to compress

Existing algorithms performing s2w compression share a common characteristic: at each iteration, the server broadcasts _the same_ message to all workers (Zheng et al., 2019; Liu et al., 2020; Fatkullin et al., 2021; Gruntkowska et al., 2023; Tyurin and Richtarik, 2023a).5 In contrast, in w2s compression methods, each worker sends to the server a _different_ message, specific to the data stored on that particular device. An analogous approach can be taken in the s2w communication: intuitively, sending \(n\) distinct messages would convey more information, potentially leading to theoretical improvements. This indeed proves to be the case. While the usual approach of the server broadcasting the same vector to all clients does not lead to an improvement over (2), allowing these vectors to differ enables a well-crafted method to achieve communication complexity that improves with \(n\) (see Corollary D.4).

In Appendix A we provide a detailed discussion of the topic and compare the theoretical complexities of MARINA-P when the server employs three different compression techniques: a) uses _one compressor_ and sends the same vector to all clients, b) uses a _collection of independent compressors_, or c) uses a _collection of correlated compressors_. We now turn to presenting the technique that gives the best theoretical s2w communication complexity out of these, namely the use of a set of _correlated compressors_.

### Recap: permutation compressors Perm\(K\)

Szlendak et al. (2021) propose compressors that will play a key role in our new theory. For clarity of presentation, we shall assume that \(d n\) and \(n|d\).6

**Definition 4.1** (Perm\(K\) (for \(d n\) and \(n|d\))).: Assume that \(d n\) and \(d=qn\), where \(q_{>0}\). Let \(=(_{1},,_{d})\) be a random permutation of \(\{1,,d\}\). For all \(x^{d}\) and each \(i\{1,2,,n\}\), we define

\[_{i}(x):=n_{j=q(i-1)+1}^{qi}x_{_{j}}e_{_{j}}.\]

Unpacking this definition: when the server compresses a vector using a Perm\(K\) compressor, it randomly partitions its coordinates across the workers, so that each client receives a sparse vector containing a random subset of entries of the input vector. Like Rand\(K\), Perm\(K\) is also a sparsifier. However, unlike Rand\(K\), it does not allow flexibility in choosing \(K\), as it is fixed to \(}{{n}}\). Furthermore, it can be shown (Lemma A.6) that \(_{i}(n-1)\) for all \(i[n]\).

An appealing property of Perm\(K\) is the fact that

\[_{i=1}^{n}_{i}(x)=x\] (7)

for all \(x^{d}\) deterministically. Here, it is important to note that by design, compressors \(_{i}\) from Definition 4.1 are _correlated_, and do not satisfy Assumption 1.6. This correlation proves advantageous - Szlendak et al. (2021) show that MARINA with Perm\(K\) compressors performs provably better than with i.i.d. Rand\(K\) compressors.

### Warmup: homogeneous quadratics

We are finally ready to present our first result showing that the s2w communication complexity can scale with the number of workers \(n\). To explain the intuition behind our approach, let us consider the simplest (and somewhat impractical) choice of functions \(f_{i}\) - the homogeneous quadratics:

\[f_{i}(x)=x^{}x+b^{}x+c, i[n],\] (8)

where \(^{d d}\) is a symmetric but not necessarily positive semidefinite matrix, \(b^{d}\) and \(c\). We now investigate the operation of MARINA-P with Perm\(K\) compressors. With probability \(p\), we have \(w^{t+1}=x^{t+1}\). Otherwise \(w^{t+1}=w^{t}+_{i=1}^{n}_{i}^{t}(x^{t+1}-x^{t}) )}}{{=}}x^{t+1}+(w^{t}-x^{t}).\) Hence, if we initialize \(w_{i}^{0}=x^{0}\) for all \(i[n],\) an inductive argument shows that \(w^{t}=x^{t}\) deterministically for all \(t 0\). Then, substituting the gradients of \(f_{i}\) to (6), one gets

\[g^{t}=_{i=1}^{n}(w_{i}^{t}+b)=w^{t }+b=x^{t}+b= f(x^{t})\]

for all \(t 0\). Therefore, MARINA-P with Perm\(K\) compressor in this setting is essentially a smart implementation of vanilla GD! Indeed, for \(p}{{n}}\), MARINA-P with Perm\(K\) sends on average \(}{{n}}\) coordinates to each worker, so the s2w communication complexity is

\[L}{ }}_{}=(L}{n} ),\]

which is \(n\) times smaller than in (2)!

### Functional \((L_{a},l_{b})\) Inequality

From the discussion in Section 3, we know that to improve (2), an extra assumption about the structure of the problem is needed. Building on the example from Section 4.3, we introduce the _Functional \((L_{A},l_{B})\) Inequality_.

**Assumption 4.2** (Functional \((L_{A},L_{B})\) Inequality).: There exist constants \(L_{A},L_{B} 0\) such that

\[\|_{i=1}^{n}( f_{i}(x+u_{i})- f_{i}(x))\|^ {2} L_{A}^{2}(_{i=1}^{n}\|u_{i}\|^{2})+ L_{B}^{2}\|_{i=1}^{n}u_{i}\|^{2}\] (9)

for all \(x,u_{1},,u_{n}^{d}\).

_Remark 4.3_.: A similar assumption, termed "Heterogeneity-driven Lipschitz Condition on Averaged Gradients", is proposed in Wang et al. (2023b). Our assumption aligns with theirs when \(L_{B}=0\). However, our formulation proves to be more powerful. The possibility that \(L_{B}>0\) becomes instrumental in driving the enhancements we introduce.

Assumption 4.2 is defined for all functions together, and intuitively, it tries to capture the similarities between the functions \(f_{i}\). For \(n=1\), inequality (9) reduces to

\[\| f(x)- f(y)\|^{2}(L_{A}^{2}+L_{B}^{2} )\|x-y\|^{2} x,y^{d},\]

equivalent to standard \(L\)-smoothness (Assumption 1.1) with \(L^{2}=L_{A}^{2}+L_{B}^{2}\). The Functional \((L_{A},L_{B})\) Inequality is reasonably weak also for \(n>1\), as the next theorem shows.

**Theorem 4.4**.: _For all \(i[n],\) assume that the functions \(f_{i}\) are \(L_{i}\)-smooth (Assumption 1.5). Then, Assumption 4.2 holds with \(L_{A}=L_{}\) and \(L_{B}=0\)._

Therefore, Assumption 4.2 holds whenever the functions \(f_{i}\) are smooth, which is a standard assumption in the literature. Now, returning to the example from Section 4.3,

**Theorem 4.5**.: _For all \(i[n],\) assume that the functions \(f_{i}\) are homogeneous quadratics defined in \(()\). Then, Assumption 4.2 holds with \(L_{A}=0\) and \(L_{B}=\|\|.\)_

Under Assumption 1.5, no information about the similarity of the functions \(f_{i}\) is available, yielding \(L_{B}=0\) and \(L_{A}>0\) in Theorem 4.4. However, once we have some information limiting heterogeneity, \(L_{A}\) can decrease. Notably, \(L_{A}=0\) for homogeneous quadratics. As we shall see in Section 4.5, the values \(L_{A}\) and \(L_{B}\) significantly influence the s2w communication complexity of MARINA-P, with lower \(L_{A}\) values leading to greatly improved performance.

### The Convergence Theory of MARINA-P with Perm\(K\)

We are ready to present our main convergence result, focusing on the Perm\(K\) compressor from Section 4.2. This choice simplifies the presentation, but our approach generalizes to a much larger class of compression operators. The full theoretical framework, covering all unbiased compressors, is detailed in Appendix D.

    &  \\ 
**Method** & **Complexity** & **Method** & **Complexity** \\  GD & & GD \\ and other compressed methods\({}^{(n)}\) & & and other compressed \\  CORE & & method\({}^{(n)}\) \\ (Vie et al., 2023) & & CORE & \(u}{}\) \\  MARINA-P & & & \\ with independent Rand\({}^{(K)}\) & & \(u}{}\|_{i}\|+_{i[n]}\|_{i}-\|}{}\) & \(\) \\ (Courley \(\))-D & & & \\  MARINA-P with Perm\(K^{(n)}\) & & \(\|_{i}\|}{n}+_{i[n]} \|_{i}-\|}{}\) & \(\) \\ (Chevena \(\)-1) & & & \\   

Table 1: **The worst case _communication complexities_ to find an \(\)-stationary point. For simplicity, we compare the complexities with non-homogeneous quadratics: \(f_{i}(x)=x^{}_{i}x+b_{i}^{}x+c_{i},\) where \(_{i}^{d d}\) is symmetric but not necessarily positive semidefinite, \(b_{i}^{d}\) and \(c_{i}\) for \(i[n]\). We denote \(=_{i=1}^{n}_{i}\).

**Theorem 4.6**.: _Let Assumptions 1.1, 1.2 and 4.2 be satisfied. Set \(w_{i}^{0}=x^{0}\) for all \(i[n].\) Take \(K\) as \(_{i}^{t}\) and \(=(L+L_{A}(}{{p}}-1)})^{-1},\) where \(_{P}=n-1\) (Lemma A.6). Then, \(\) finds an \(\)-stationary point after_

\[(}{}(L+L_{A}}}{{p}}}))\]

_iterations._

**Corollary 4.7**.: _Let \(p=}{{d}}}{{n}}\). Then, in the view of Theorem 4.6, the average s2w communication complexity of \(\) with \(K\) compressor is_

\[(L}{ne}+L_{A}}{ }).\] (10)

The key observation is that (10) is independent of \(L_{B},\) and only depends on \(L_{A}\). This particular property is specific to _correlated compressors_ with parameter \(=0\) (defined in Appendix A), such as \(K\). A similar result holds for _independent_\(K\) compressors (see Corollary D.4), but the convergence rate is worse and depends on \(L_{B}\). Nevertheless, this dependence improves with \(n\).

When \(L_{A}=0\), which is the case for homogeneous quadratics, the step size bound from Theorem 4.6 simplifies to \(}{{L}}\), the standard GD stepsize (recall that in this case our method reduces to GD). Most importantly, (10) scales with the number of workers \(n!\) Even when \(L_{A}>0,\) for sufficiently big \(n,\) (10) can improve (2) to \((L_{A}}}{}).\)

Let us now investigate how the constants \(L_{A}\) and \(L_{B}\) change in the general case.

### Estimating \(L_{a}\) and \(L_{b}\) in the General Case

It is clear from Corollary 4.7 that \(\) with \(K\) shines when \(L_{A}\) is small. To gain further insights into what values \(L_{A}\) may take, we now provide an analysis based on the Hessians of the functions \(f_{i}\).

**Theorem 4.8**.: _Assume that the functions \(f_{i}\) are twice continuously differentiable, \(L_{i}\)-smooth (Assumption 1.5), and that there exist \(D_{i} 0\) such that_

\[_{z_{1},,z_{n}^{d}}\|^{2}f_{i}(z_{i})- _{j=1}^{n}^{2}f_{j}(z_{j})\| D_{i}\] (11)

_for all \(i[n]\). Then, Assumption 4.2 holds with \(L_{A}=_{i[n]}D_{i} 2_{i[n]}L_{i}\) and \(L_{B}=(_{i=1}^{n}L_{i}).\)_

Intuitively, (11) measures the similarity between the functions \(f_{i}\). The above theorem yields a more refined result than Theorem 4.4: it is always true that \(_{i[n]}D_{i} 2_{i[n]}L_{i}\), and, in fact, \(_{i[n]}D_{i}\) can be much smaller, as the next result shows.

**Theorem 4.9**.: _Assume that \(f_{i}(x)=x^{}_{i}x+b_{i}^{}x+c_{i},\) where \(_{i}^{d d}\) is symmetric but not necessarily positive semidefinite, \(b_{i}^{d}\) and \(c_{i}\) for \(i[n].\) Define \(=_{i=1}^{n}_{i}.\) Then, Assumption 4.2 holds with \(L_{A}=_{i[n]}\|_{i}-\|\) and \(L_{B}=(_{i=1}^{n}\|_{i}\| ).\)_

Thus, \(L_{A}\) is less than or equal to \(_{i[n]}\|_{i}-\|,\) which serves as a measure of similarity between the matrices. The smaller the values of \(\|_{i}-\|\) (indicating greater similarity among the functions \(f_{i}\)), the smaller the \(L_{A}\) value.

In the view of this theorem, the s2w communication complexity of \(\) with \(K\) on non-homogeneous quadratics is

\[(\|\|}{ne}+_{i[n]}\|_{i}-\|}{ }).\] (12)

Since the corresponding complexity of GD is

\[(\|\|}{} ),\] (13)

in the close-to-homogeneous regimes (i.e., when \(_{i[n]}\|_{i}-\|\) is small), the complexity (12) can be _provably_ much smaller than (13). The same reasoning applies to the general case when thefunctions \(f_{i}\) are not quadratics: MARINA-P improves with the number of workers \(n\) in the regimes when \(D_{i}\) are small (see Theorem 4.8).

Let us note that there is another method, CORE, by Yue et al. (2023), that can also provably outperform GD, achieving the s2w communication complexity of \((}}{{e}})\) on non-homogeneous quadratics. Neither their method nor ours universally provides the best possible communication guarantees. Our method excels in the close-to-homogeneous regimes: for example, if we take \(_{i}=L_{i}\) for all \(i[n]\), and define \(L=}{{n}}_{i=1}^{n}L_{i}\), then the complexity of CORE is \((}}{{e}}),\) while ours is \((}{{n}{e}}+_{i [n]}}|L_{i}-L|}{{e}}).\) Hence, our guarantees are superior in regimes where \(_{i[n]}|L_{i}-L| L\). One interesting research direction is to develop a universally better method combining the benefits of both approaches.

## 5 M3: A New Bidirectional Method

In the previous sections, we introduce a new method that provably improves the _server-to-worker_ communication, but ignores the _worker-to-server_ communication overhead. Our aim now is to treat MARINA-P as a starting point for developing methods applicable to more practical scenarios, by combining it with techniques that compress in the opposite direction. Since the theoretical state-of-the-art w2s communication complexity is obtained by MARINA (see Section 1.1), our next research step was to combine the two and analyze "MARINA + MARINA-P", but this naive approach did not yield communication complexity guarantees surpassing (2) in any regime. It became apparent that some "buffer" step between these two techniques is needed, and this step turned out to be the momentum. Our new method, M3 (Algorithm 2), is described in (14).

M3 combines (5), (6), and the momentum step \(z_{i}^{t+1}= w_{i}^{t+1}+(1-)z_{i}^{t}\), which is the key to our improvements. A similar technique is used to reduce the variance in Fatkhullin et al. (2023). Let us explain how M3 works in practice. First, the server calculates \(x^{t+1}\). Depending on the first probabilistic decision, it sends either \(x^{t+1}\) or \(_{i}^{t}(x^{t+1}-x^{t})\) to the workers, who then calculate \(w_{i}^{t+1}\) locally. Next, the workers compute \(z_{i}^{t+1},\) and depending on the second probabilistic decision, they send either \( f_{i}(z_{i}^{t+1})\) or \(_{i}^{t}( f_{i}(z_{i}^{t+1})- f_{i}(z_{i}^{t}))\) back to the server. The server aggregates the received vectors and calculates \(g^{t+1}\). As in MARINA, \(p_{P}\) and \(p_{D}\) are chosen in such a way that the non-compressed communication does not negatively affect the communication complexity. Therefore, the method predominantly transmits compressed information, with only a marginal probability of sending uncompressed vectors.

### The Convergence Theory of M3

For simplicity, we consider \(K\) in the role of \(_{i}^{t}\) and \(K\) in the role of \(_{i}^{t}\). The general theory for all unbiased compressors is presented in Section E.

**Theorem 5.1**.: _Let Assumptions 1.1, 1.2, 1.5 and 4.2 be satisfied. Take \(=(L+34(nL_{A}+n^{2/3}L_{B}+n^{2/3}L_{}))^{-1}\), \(p_{D}=p_{P}=}{{n}}\), \(=n^{-2/3}\), \(w_{i}^{0}=z_{i}^{0}=x^{0}\) and \(g_{i}^{0}= f_{i}(x^{0})\) for all \(i[n]\). Then \(\) with \(_{i}^{t}=\)Perm\(K\) and \(_{i}^{t}=\) Rand\(K\) with \(K=}{{n}}\) finds an \(\)-stationary point after \(\!(}{}(n^{2/3}L_{}+nL_{A} )\,)\) iterations. The total communication complexity is_

\[(L_{}}{n^{1/3}}+L_{A}}{}).\] (15)

Once again, we observe improvement with the number of workers \(n\), and the obtained complexity (15) can be provably smaller than (2). Indeed, in scenarios like federated learning, where the number of workers (e.g., mobile phones) is typically large (Kairouz et al., 2021; Chowdhery et al., 2023), the first term can be significantly smaller than \(d^{0}L/\). The second term can also be small in close-to-homogeneous regimes (see Section 4).

## 6 Experimental Highlights

This section presents insights from the experiments, with further details and additional results in Appendix F. The experiment aims to empirically test the theoretical results from Section 4. We consider a quadratic optimization problem, where the functions \(f_{i}\) are as defined in Theorem 4.9 and \(_{i}^{300 300}\). We compare GD, \(\) sending the same message compressed using a single Rand\(K\) compressor to all workers ("SameRand\(K\)" from Appendix A), \(\) with independent Rand\(K\) compressors, \(\) with \(K\) compressors, and \(\) with \(K\) compressor. We consider \(n\{10,100,1000\}\) and fine-tune the step size for each algorithm. The results, presented in Figure 1, align closely with the theory, with \(\) using \(K\) compressors consistently performing best. Moreover, the convergence rate of \(\) with \(K\) and independent \(K\) compressors improves with \(n\). Since this is not the case for \(\), even though it outperforms \(\) with independent Rand\(K\) compressors for \(n=10\), it falls behind for \(n\{100,1000\}\).

Figure 1: Experiments on the quadratic optimization problem from Section 6. We plot the norm of the gradient w.r.t. # of coordinates sent from the server to the workers.