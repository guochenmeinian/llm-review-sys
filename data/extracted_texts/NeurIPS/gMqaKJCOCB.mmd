# Understanding the Gains from

Repeated Self-Distillation

 Divyansh Pareek Simon S. Du Sewoong Oh

Paul G. Allen School of Computer Science and Engineering

University of Washington, Seattle, WA

{dpareek,ssdu,sewoong}@cs.washington.edu

###### Abstract

Self-Distillation is a special type of knowledge distillation where the student model has the same architecture as the teacher model. Despite using the same architecture and the same training data, self-distillation has been empirically observed to improve performance, especially when applied repeatedly. For such a process, there is a fundamental question of interest: How much gain is possible by applying multiple steps of self-distillation? To investigate this relative gain, we propose studying the simple but canonical task of linear regression. Our analysis shows that the excess risk achieved by multi-step self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by a factor as large as \(d\), where \(d\) is the input dimension. Empirical results on regression tasks from the UCI repository show a reduction in the learnt model's risk (MSE) by up to \(47\)%.

## 1 Introduction

Knowledge distillation  was initially proposed as a way to transfer the knowledge learnt by a larger teacher model to a smaller student model, which can then be deployed in limited resource settings. The process is as follows: Train a teacher (\(T\)) model using ground-truth labels, then use its predictions to supervise the training of a student (\(S\)) model via a combined per-sample loss,

\[_{T},y_{S}()+(1-) y,y_{S}()\;,\] (1)

where \(\) denotes the loss function, \(y\) is the ground-truth label, \(_{T}\) denotes the teacher's prediction, and \(y_{S}()\) denotes the student's prediction, parameterized by the learnable \(\). The extra hyperparameter \(\) is called the imitation parameter , generally restricted to \(\). It gives additional freedom to the student to balance importance between labels and teacher's predictions. The student trained via this distillation objective (i.e., utilizing the teacher's predictions through \( 0\)) has been widely observed to generalize better than when trained only on the labels (i.e., \(=0\)). This gain has been attributed to 'dark knowledge' that is \((i)\) impossible to be directly extracted from the training data by the small model, but \((ii)\) easily learnt by the large model and transferred to the small model.

Challenging this interpretation, Li et al.  and Furlanello et al.  empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set \(T\) and \(S\) to have the _same architecture_, and \(S\) trained with the objective in Eq. (1) outperforms \(T\). This is referred to as Born-Again Networks (BANs) or _Self-Distillation_ (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [10; 36; 44]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1). For such _multi-step self-distillation_, there is a fundamental question of interest: _How much more gain can we get by repeatedly applying self-distillation?_

Recently, Das and Sanghavi  provided theoretical understanding of the original one-step self-distillation. For the canonical task of fixed design linear regression, considering the standard ridgeestimator as both the teacher and student model,  showed that there is indeed a regime of problem instances in which the _optimal_ student (i.e., with optimally tuned ridge parameter \(\) and imitation parameter \(\)) can provably achieve a strictly lower test error than the _optimal_ teacher (i.e. with optimally tuned \(\)). However, the amount of this gain has not been characterized in closed form, and can only be numerically evaluated for a given problem instance. Inspired by this work, we aim to study the performance gains from multi-step self-distillation under linear regression.

**Contributions.** We summarize our contributions below.

* Under the fixed design linear regression defined in Section 3.1, we show that the _optimal_ multi-step self-distilled model (i.e., each \(\) value at each step is optimized for the validation accuracy of the final multi-step self-distilled model) can achieve a test error that is a factor of \(d\) smaller than the _optimal_ one-step self-distillation (Theorem 1), under certain assumptions on the problem parameters (Assumption 2). Here, \(d\) is the dimension of the input. Our analysis in Theorem 1 suggests that the sequence of \(\) parameters provides additional freedom that can control the spectrum of eigenvalues of the linear estimator. Optimally choosing these \(\) parameters can significantly reduce the variance of the estimator, leading to a factor of (up to) \(d\) difference in the overall test errors of multi-step SD compared to \(1\)-step SD. We note that Das and Sanghavi  also observed a bias-variance tradeoff associated with the \(\) parameter for \(1\)-step SD compared to the ridge, which was the reason behind \(1\)-step SD strictly outperforming the ridge.
* We demonstrate the necessity of the main assumption (Assumption 2) both theoretically (Theorems 2 and 3) and numerically (Figure 3). Further, we provide a lower bound for the test error that any repeated SD can achieve, and show that only \(r\) steps of SD (with optimally chosen \(\) at each step) are sufficient to achieve this, when the input data matrix has rank \(r\) (Theorem 4).
* By capturing the functional form of the test error in \(\) (Theorem 5), we also show a method to practically select the \(\) parameters for real-world regression tasks. In Section 5, we empirically show that this theoretical insight leads to selecting effective \(\) values, which can indeed achieve a lower test error on real-world regression tasks.

## 2 Related Work

**Knowledge distillation and self-distillation.** Hinton et al. , Ba and Caruana  proposed knowledge distillation to transfer knowledge learnt by large teacher models into smaller student models without any substantial performance drop (e.g., [30; 31; 14; 8; 33; 24; 32] and surveys in [11; 13]). Distillation also provides interpretability , robustness to adversarial examples , and defense against backdoor attacks [39; 21; 35], although stronger backdoor attacks have been proposed that bypass distillation defense . Perhaps surprisingly, empirical observations show that performance _improves_ when a teacher model is distilled into a student model with the same architecture on the same training data (_self-distillation_). Performance gains with one-step self-distillation of the form Eq. (1) were first demonstrated by Li et al.  for AlexNet on YFCC100M. Further gains can be achieved by repeating self-distillation, as shown for the DenseNet architecture family on CIFAR10 and CIFAR100 [10, Table 2]. To empirically explain such gains, Zhang and Sabuncu  measured prediction uncertainty on the same multi-step experiments and offered an interpretation that soft labels capture sample-level uncertainties. Yang et al.  also reproduced the same experiments and explained the gains as knowledge refinement on the class similarities. We will analytically study the gains that can be achieved with such repeated self-distillation.

Many variations of self-distillation have also been proposed. Snapshot Distillation  tries to treat previous checkpoints (snapshots) of the same model as the teacher. Zhang et al.  employ a group of collaborative students with no teacher. Zhang et al. [42; 41] use it for model self-improvement, and DINO  adopts self-distillation for self-supervised learning. Knowledge distillation is also popular for transfer learning, where the student model is trained on a different dataset than the teacher model [38; 40; 1], which is not a setting we address. With the recent scaling of data, ,  are relevant works using a teacher model for either label editing or data reweighing.

**Theory of distillation and self-distillation.** Theoretical understanding of distillation started with Phuong and Lampert  studying linear student networks. Mobahi et al.  studied self-distillation theoretically in the restricted setting of \(=1\) (i.e. only teacher supervision, no ground-truth labels), showing that in this setting, the SD process acts as a regularizer, with a few steps of SD helping, but further steps hurting model performance. We study the setting where \(\) is not restricted to \(1\)and show a different conclusion. In particular, we observe that more steps of SD always provide an improvement, if the \(\) parameters are chosen optimally. Allen-Zhu and Li  analyzed a stylized setting, where a different view of the data is learned by different models, and show how ensemble methods can combine the views, achieving improved test accuracy. This framework is used to show how self-distillation can also improve model accuracy by implicitly performing ensembling. Menon et al.  theoretically studied distillation in the classification setting, and also observed a bias-variance tradeoff underlying teacher supervision. Das and Sanghavi  theoretically studied one-step self-distillation for fixed design linear regression and binary classification, and showed that the student can provably achieve a lower test error than the teacher. We take inspiration from them and study the multi-step SD to characterize this performance gain, showing that the multi-step SD can outperform one-step SD by a large factor. Borup and Andersen  also studied multi-step SD and obtained an analytical form for the \(k\)-step SD similar to ours [4, Theorem 4.1]. The crucial difference is the freedom of the \(\) parameters being different at each step of self-distillation. Whereas [4, Lemma 4.2 and Theorem 4.3] assume the \(\) values at each step are equal (similar to ), concluding that after a point, more steps of SD will result in a poorer performing model (similar to ); our main result (Theorem 1) is different as it says that subsequent steps of SD strictly provide more freedom, and that the best multi-step SD can outperform the best \(1\)-step SD by an \((r)\) factor. Similar to us, Jeong and Chung  also take inspiration from ,  however aim to provide understanding of multi-step self-distillation in the multi-class classification setting.

## 3 Problem formulation and background on self-distillation

Focusing on the simple but canonical task of linear regression, we investigate the performance gain from applying repeated self-distillation.

### Linear regression

For the observed response \(Y\) and the covariate \(X^{d}\), the following assumption is standard in linear regression, e.g., .

**Assumption 1**.: _There exist \(^{}^{d}\) and \(>0\) such that (i) \([Y|X]=^{},X\); (ii) \([Y|X]=^{2}\) for all \(X^{d}\); and (iii) \((Y-[Y|X]) X\), i.e. the label noise is independent of \(X\)._

The training set of size \(n\) is denoted by \(^{d n}\), the collection of covariates, and \(^{n}\), the responses; \(=^{}^{}+\), with \(\) satisfying \([]=0\), \([^{}]=^{2}_{n}\). The problem instance is defined by its parameters \((,^{},^{2})\), treating \(=[X_{1},X_{2}, X_{n}]\) as fixed but \(\) as random. The training set \((,)\) is one occurrence of the random noise \(^{n}\). In this _fixed design_ setup, the excess risk of an estimator \(\) is defined using the standard \(_{n}\)-norm, \(\|v\|_{_{n}}=\|_{n}^{1/2}v\|_{2}\), as

\[() := _{}[\|-^{} \|_{_{n}}^{2}]\;,\] (2)

where \(_{n}:=(}{{n}})^{}\) is the covariance matrix, and the expectation is over the randomness in \(\). Measuring the error in the \(_{n}\)-norm ensures that the signal-to-noise ratio is uniform in all directions. The popular ridge estimator serves as a baseline, using a single hyperparameter \(>0\):

\[()\ :=\ _{^{d}}(\|- ^{}\|^{2}+\|\|^{2})\ =\ (^{}+_{d})^{-1} \;.\] (3)

We use \(_{}:=^{}+_{d}\) throughout. We consider only \(>0\), but surprisingly, Kobak et al.  showed that the optimal penalty \(^{}\) (one with the lowest risk) can indeed be negative. However we will largely work in the non-overparameterized case (\(n>d\)), where \(^{}>0\) holds.

### Self-distillation

Applying the self-distillation loss in Eq. (1) to linear regression with hyperparameters \(\) and \(\),

\[(,) :=\ _{^{d}}(\,\|^{ }()-^{}\|^{2}+(1-)\,\|- ^{}\|^{2}+\,\|\|^{2})\] (4) \[=\ (^{}+_{d})^ {-1}^{}() +(1-))}_{}\] (5) \[=\ _{d}+ _{}^{-1}^{}\}}_{}_{}^{-1} }_{()}\;,\] (6)where \(\) is not restricted to the conventional \(\) interval. This additional freedom is meaningful since it can result in a strictly better solution, as noted by Das and Sanghavi  both theoretically (Remark 3.6) and empirically (Table 1). It is worth noting that the optimization problem in eq.4 remains convex for any \(\), since its hessian evaluates to \( 2^{}+(1-) 2^{}=2 ^{} 0\). On the other hand, the teacher and student use the same ridge penalty \(\) for simplicity.

We call this estimator \(1\)_-step self-distillation_. This can be interpreted as (\(i\)) assigning new labels that combine the ground-truth labels with the teacher's predictions, or (\(ii\)) pre-multiplying the usual ridge estimator with a pre-conditioner. Note that \(=0\) recovers ridge. Das and Sanghavi [9, Theorem 3.8] show that under a certain condition, 1-step self-distillation strictly dominates ridge, i.e.,

\[_{ 0,}_{}[\|(,)-^{}\|_{2}^{2}]\ <\ _{ 0}_{}[\|()-^{}\|_{2}^{2}]\,\] (7)

where the risk is measured in the non-standard Euclidean norm. The same strict inequality can be shown under the standard \(_{n}\)-norm under a slightly modified condition stated in PropositionB.1. This naturally leads to a fundamental question: _How much more gain can we get by repeatedly applying self-distillation?_

### Repeated self-distillation

The standard repeated application of self-distillation starts with the teacher model, \(T\) (which we also refer to as the zeroth model, \(S_{0}\)), and applies self-distillation sequentially for \(k\) steps. At each step \(i\), Eq.1 is applied with the \((i-1)^{th}\) model, \(S_{i-1}\) as the teacher, and the \(i^{th}\) model, \(S_{i}\) as the student, with an imitation parameter \(_{i}^{(k)}\), i.e., \(*{arg\,min}_{}_{i}^{(k)}_{S_{i-1}},y_{S_{i}}()+(1-_{i}^{(k)})y,y_{S_{i}}()}\) for \(i[k]\). The collection of parameters is denoted by \(^{(k)}=[_{1}^{(k)},_{2}^{(k)},,_{k}^{(k)}]^{k}\).

This repeated self-distillation has been studied, for example, theoretically in  and empirically in . We aim to understand its gain under linear regression, where we prove that

\[(,}_{^{k}})\ =\ ^{k}_{i}^{(k)} )_{d}+_{i=1}^{k}_{i}^{(k)}(_{ }^{-1}^{})^{i}\}}_{(,^{(k)})}_{}^{-1} }_{\,()}\,,\] (8)

with \(_{i}^{(k)}:=(1-_{k-i}^{(k)})_{l=k-i+1}^{k}_{l}^{(k)}\) for each \(i[k]\), and we let \(_{0}^{(k)}=0\). The proof that repeated SD with \(^{(k)}^{k}\) results in Eq.8 is provided in AppendixC.2. Here \(^{(k)}^{k}\) denote the imitation parameters, \(\) denotes the ridge coefficient for all the models, and \(^{(k)}^{k}\) is a _reparametrization_ of \(^{(k)}^{k}\) (details in AppendixC.2). We call this _\(k\)-step self-distillation_. Note the increasing flexibility in the pre-conditioner matrix. The increasing powers of \(_{}^{-1}^{}\) in the above expression are still numerically stable, since, for \(>0\), \(_{}^{-1}^{}\) is PSD with all eigenvalues in \(\). As an aside, one can also consider a version of SD where the \(i^{th}\) model receives supervision from all \(S_{<i}\) instead of just \(S_{i-1}\). AppendixC.1 shows that this version provides no extra representational capacity over the repeated version presented above, when all \(k\) entries of \(^{(k)}\) are optimized as free parameters. Hence, the procedure in Figure1 suffices for analysis.

## 4 Main results for linear regression

The main object of our study is to theoretically demonstrate the gains from repeated self-distillation. Concretely, we aim to show that there can be a significant multiplicative separation between the

Figure 1: The standard \(1\)-step self-distillation defined in Eq.1 with parameter \(\) and \(k\)-step self-distillation that repeatedly applies Eq.1 with parameter \(^{(k)}=[_{1}^{(k)},_{2}^{(k)},,_{k}^{(k)}]^{k}\).

excess risk achieved by \(r\)-step SD (Self-Distillation), where \(r\) is the rank of the input \(\); compared to the ridge estimator, as well as the \(1\)-step SD (Section 4.1). The necessity of the two main assumptions is shown in Section 4.2. The sufficiency of \(r\) steps of SD is shown in Section 4.3. In Section 4.4, we provide an exact characterization of the excess risk achieved by \(k\)-step SD (for any \(k\)).

### The \(r\)-step self-distillation significantly improves upon the \(1\)-step self-distillation

We show the desired separation under the following assumption (and two more mild technical assumptions specified fully in Appendix E).

**Assumption 2**.: _Assume the following two conditions hold on the problem instance \((,^{},^{2}):\)_

1. _No two non-zero singular values of_ \(\) _collide, i.e._ \(s_{1}>s_{2}>>s_{r}>0\)_, where_ \(\{s_{j}\}_{j=1}^{r}\) _denote the non-zero singular values of the input data matrix_ \(\) _whose rank is denoted by_ \(r\)_._
2. _For a_ \([0,1)\)_, there exists an index_ \(j[r]\) _such that_ \(^{},_{j}^{2}(1-)\|^{ }\|^{2}\)_; where_ \(\{_{j}\}_{j=1}^{d}\) _denote the eigenvectors of_ \(^{}\)_,_ \(_{1}\) _being the leading one._

Assumption 2 is needed to show that \(r\)-step SD achieves a small excess risk in Eq. (9). The quantity \(\) captures the similarity of the (unknown) \(^{}\) to the eigenbasis directions. The case of \(=0\) entails perfect alignment of \(^{}\) with one of \(_{j},j[r]\). As we will see in the result in Theorem 1, the gains provided by multi-step SD are large when \(\) is small. In general, both these conditions are somewhat necessary for the separation, as we show in Theorems 2 and 3. We now state our main result. We show that under the above assumption, there exists a family of problem instances, \((,^{},^{2})\), such that the excess risk achieved by \(r\)-step SD is a factor of \(r:=()\) smaller than that of the ridge estimator _and_ the \(1\)-step SD.

**Theorem 1**.: _Under the fixed design linear regression in Assumption 1, there exists a family of problem instances satisfying Assumption 2 such that for any instance \((,^{},^{2})\) in the family, it holds that_

\[>0,^{(r)}^{r}, ((,^{(r)})) }{n}(1+\|^{2}s_{1}^{2}}{ ^{2}})\;,\] (9) \[>0,, ((,)) (})(1-)}{n}\;,\; {and}\] (10) \[>0, (()) 0.98( )^{2}\;}{n}\;,\] (11)

_where \(r:=()\), \(n\) is the number of samples, \((,^{(r)})\) and \((,)\) are the \(r\)-step and \(1\)-step SD estimators defined in Eqs. (8) and (4) respectively, and \(()\) is the ridge estimator defined in Eq. (3)._

This theorem captures a general result for the gains of multi-step SD. In particular, the special case of \(=0\) (i.e. \(^{}\) being completely aligned with one of the eigenvectors \(_{j},j[r]\)) presents an \((r)\) multiplicative separation between the excess risk of \(r\)-step SD and \(\{1,0\}\)-step SD. We provide precise conditions and a proof in Appendix E. Since each \(k\)-step SD includes \((k-1)\)-step SD as a special case with the proper choice of \(^{(k)}\), the hyperparameter-tuned excess risk of repeated SD is monotonically non-increasing. However, it is perhaps unexpected that the multiplicative separation between \(r\)-step SD and \(1\)-step SD can be as large as \((r)\), demonstrating the gains of repeated SD. Figure 2 illustrates this \((r)\) multiplicative separation on a synthetic family of problems. Note that \((d)\) separation can be achieved by choosing the problem instance to have rank \(r=d\), at the cost of requiring many more steps of SD. This \((d)\) factor is the largest multiplicative separation possible with self-distillation, as shown by the fundamental lower bound in Theorem 4 for any pre-conditioning based approach. In general, if \(=O(}{\|^{}\|^{2}s_{1}^{2}})\) (akin to the inverse signal-to-noise ratio), then there exists an \((r)\) multiplicative separation between \(r\)-step SD and \(\{1,0\}\)-step SD. In our particular construction of the problem instance for Theorem 1, the additional technical condition (i.e. Condition #2 in the detailed theorem statement in Appendix E) effectively translates this to \(=O(}{{r}})\).

**Remark 4.1**.: _SD significantly outperforms ridge by primarily reducing the variance. For the lower bound on ridge's excess risk, i.e., Eq. (11), we ignored the bias term and only used the variance term. The repeated SD (Eq. (9)) primarily reduces the variance to improve the excess risk over Eq. (11)._

### Necessity of Assumption 2

In Figure 3, we empirically show on synthetic tasks how violating Assumption 2.1 or 2.2 leads to higher excess risks, even for the \(r\)-step SD (\(r=4\) in the example). This supports the necessity of both assumptions, which we analytically investigate in the following.

**Necessity of Assumption 2.1 on \(\)**. We assume that the non-zero singular values of \(\) are unique. This allows us to tightly upper bound the excess risk achieved by \(r\)-step SD in Eq. (9) via Theorem 4. We show in the following that some version of Assumption 2.1 is also _necessary_. For a more detailed explanation of why we need Assumption 2.1, we refer the reader to Remark 4.2.

**Theorem 2**.: _Under the hypotheses of Theorem 1 except for Assumption 2.1, if the singular values of \(\) satisfy \(s_{1}==s_{r}=1\), where \(r=()\), for all \(k 1\), \(>0\), and \(^{(k)}^{k}\), we have_

\[((,^{(k)}))\; \;}{n}(1+}{_{j=1}^{r} ^{},_{j}^{2}})^{-1}\;.\] (12)

_Furthermore, there exists \(>0\) such that the ridge, \(()\), achieves this lower bound with equality._

We provide a proof in Appendix F. This implies that when there is no gap in the singular values of the input \(\), there is no separation between ridge and SD estimators (repeated or not). Intuitively, if the \(s_{j}\) are all equal, the pre-conditioner for ridge (i.e., \(_{}^{-1}\)) and the pre-conditioner for the repeated SD, both are restricted to have all eigenvalues to be equal. (Repeated) SD has no degrees of freedom to deviate from this. However, \(s_{j}\)'s being unequal provides the freedom for the \(^{(k)}\) to control the SD's pre-conditioner matrix, and reduce the excess risk. This is also why in Remark 4.2, we hypothesize that numerically, the optimal \((^{(k)})^{}\) depends inversely on the min-gap of the singular values. Figure 4 demonstrates this increasing relationship of the magnitude of the optimal \(\) parameters w.r.to the decreasing singular gap.

**Necessity of Assumption 2.2 on \(^{}\)**. Theorem 1 shows that there is a large separation in the performance of repeated SD over ridge when Assumption 2.2 holds with a small \(\). In general, this translates to \(^{}\) being highly aligned with _any one_ of the eigenvectors of \(^{}\) (not necessarily the leading eigenvector). We show next that if \(^{}\) is equally (mis)aligned with all the eigenvectors \(\{_{j}\}_{j=1}^{r}\) of \(^{}\), then again there is no separation between ridge and repeated SD.

Figure 2: On a synthetic problem family with dimension \(d=100\), noise variance \(=0.1\), and \(^{}=_{1}\) (agreement with Asmp. 2.2); we set the singular values of \(\) with a power law from \(s_{1}=1\) to \(s_{r}=\{0.8,0.5\}\) (left and right panels) and vary \(r=()\). Both plots show a linear increase of the relative gain of \(r\)-step self-distillation in excess risk, i.e. the ratio \(}{{B}}\) where \(A:=_{>0,^{(r)}^{r}} (,^{(r)})\); demonstrating that \(r\)-step SD outperforms ridge by a factor of \((r)\), with the constant inside the \(\) (i.e. slope of the line) changing with the effective condition number, \(}}{{s_{r}}}\).

**Theorem 3**.: _Under the hypotheses of Theorem 1 except for Assumption 2.2, if the true parameter \(^{}\) satisfies \(^{},_{j}^{2}=z\) for all \(j[r]\), it holds that for all \(z>0\), \(k 1,>0\), and \(^{(k)}^{k}\),_

\[((,^{(k)}))\ \ }{n}_{j=1}^{r}(1+}{zs_{j}^{2}} )^{-1}\.\] (13)

_Furthermore, there exists \(>0\) such that the ridge, \(()\), achieves this lower bound with equality._

We provide a proof in Appendix G. Similar conditions are needed when analyzing \(1\)-step SD in  as well; [9, Eq. (9) in Theorem 3.8] is required for the \(1\)-step SD to strictly outperform ridge. Observe that Eq. (9) is violated when either (\(i\)) \(s_{j}\)'s are all equal or (\(ii\)) \(^{},_{j}^{2}\)'s are all equal.

### \(r\) steps of self-distillation are sufficient

For a given problem instance \((,^{},^{2})\), the excess risk achieved by the \(k\)-step SD with parameter \(^{(k)}\) can be exactly characterized (Theorem 5), but it is complicated and can only be evaluated numerically in general. On the other hand, we show that there exists a fundamental lower bound that holds for a linear family of estimators including all repeated SD, and this lower bound has a simple characterization (Lemma 4.1). Furthermore, we show that under a mild assumption on the eigenvalues of \(^{}\) in Assumption 2.1, the \(r\)-step SD achieves the lower bound (Theorem 4). This allows a precise characterization of the performance of \(r\)-step SD.

Figure 4: On the synthetic problem from Figure 2(a), we fix \(=0.125\) and set the singular values of \(\) as \(s_{j}=\{1-(j-1)\}\), i.e. consecutive values are separated by \(\). For \(k\)-step SD with \(k=\{1,2,3\}\), we plot \((^{(k)})^{}()\) (i.e. optimal values of the \(\) parameters) by varying \(\{0.2,0.1,0.05,0.02,0.01\}\). The magnitude of \(^{(k)}_{k}\) values increases as the singular gap \(\) decreases, verifying Remark 4.2.

Figure 3: On a synthetic task (explained in Section 5.1), \(\) has rank \(4\) with (a) \(^{}=_{1}\) and distinct \(s_{j}\)’s; (b) \(s=[1,1,}{{2}},}{{3}}]\); (c) \(^{}=0.5(_{1}+_{2}+_{3}+_{4})\). Each additional step of SD with optimal choice of \(^{(k)}\) reduces \(((,(^{(k)})^{}))\) for any choice of \(\) on the \(x\)-axis. Panel (a) satisfies Asmp. 2 and hence \(4\)-step SD is necessary to achieve the optimal excess risk. This is no longer true when Asmp. 2.1 is violated (b) or Asmp. 2.2 is violated (c). Excess risk achieved by \(4\)-step SD (i.e. the green lines) in panels (a) and (c) exactly match the numerical value given by RHS of eq. (14), i.e. the fundamental lower bound for any SD estimator. But this is not the case in panel (b) [which has the same lower bound from eq. (14) as panel (a)], because Asmp. 2.1 is violated.

**Theorem 4**.: _Under the fixed design linear regression in Assumption 1, the excess risk of any \(k\)-step SD estimator on an instance \((,^{},^{2})\), is lower bounded for all \(k 1\), \(>0\), and \(^{(k)}^{k}\) by_

\[((,^{(k)}))\ \ }{n}_{j=1}^{r},_{j} ^{2}}{(^{},_{j}^{2}+}{s_{j}^{2}})}\,\] (14)

_where \((s_{j},_{j})\) is the \(j^{}\) eigenvalue and eigenvector of \(\) and \(r:=()\). Furthermore, if Assumption 2.1 holds then there exists \(>0\) and \(^{(r)}^{r}\) such that the equality is achieved by the \(r\)-step SD estimator \((,^{(r)})\)._

**Proof sketch.** (_Proof in Appendix H).: The lower bound in Eq. (14) is an instantiation of Lemma 4.1, since \((,^{(k)})\) is a specific linear family estimator with \(=(,^{(k)})\ _{ }^{-1}\) defined in Eq. (8). To show achievability, we need to show that \((,^{(k)})\ _{}^{-1}=^{}\) for some value of \(k\), \(\), and \(^{(k)}\). This holds when the below system of \(r\) linear equations admits a solution for the \(k\) parameters (i.e. \(^{(k)}\)), with an _extra_ free parameter \(>0\). We show that with \(k=r\) and under Assumption 2.1, there exists \(>0\) that will ensure the existence of a solution for this system of equations.

\[(1-_{i=1}^{k}_{i}^{(k)}\{1-(^{2}}{ +s_{j}^{2}})^{i}\})\ ^{2}}{+s_{j}^{2}}\ =\ ,_{j}^{2}}{^{}, _{j}^{2}+}{s_{j}^{2}}} j[r]\] (15)

**Remark 4.2** (Necessity of Assumption 2.1).: _This assumption is required for (15). Otherwise, the LHS would be the same for indices \(j\) and \(j+1\) if \(s_{j}=s_{j+1}\), but the RHS could still be different as \(^{},_{j}^{},_{j+1}\) generally. If Assumption 2.1 does not hold, there might not be any \(^{(k)}\) satisfying the set of equations for a general \(^{}^{d}\). Further, the system of linear equations in Eq. (15) becomes more ill-conditioned as the singular values \(s_{j},j[r]\) get closer to each other. Capturing this dependence explicitly is outside the scope of this paper._

**Lower bound for a linear family.** Consider a linear family of estimators of the form \(():=\), for \(:=_{d}_{d}^{}\), whose eigenspace coincides with that of \(^{}\) (i.e., \(_{d}=[_{1},,_{d}]\)) and has \(d\) degrees of represented by the eigenvalues \(=[_{1},,_{d}]\). This is a generic form of any linear estimator, albeit with the restriction of the eigenvectors matching the underlying \(_{d}\). In particular, \(k\)-step SD is an instantiation of this with \(=(,^{(k)})\ _{}^{-1}\) (refer to Eq (8)).

**Lemma 4.1**.: _The Excess Risk for \(()=\) where \(:=_{d}_{d}^{}\), satisfies_

\[(())\ \ }{n}_{j=1}^{r},_{j}^{2}}{(^{}, _{j}^{2}+}{s_{j}^{2}})}\,\] (16)

_with equality achieved at \(=^{}=_{d}^{}_{d}^{}\), given by_

\[_{j}^{}\ =\ ,_{j} ^{2}}{(^{},_{j}^{2}s_{j}^{2}+ ^{2})}\,&j r\ \ (s_{j}>0)\\ \,&j r+1\ (s_{j}=0)\.\] (17)

**Proof sketch.** (_Proof in Appendix H.1)._ One can expand the excess risk for \(()\), which is a quadratic expression in \(_{j},j[d]\). Completing the squares gives the result immediately.

The excess risk for the \(k\)-step SD estimator is quadratic in \(^{(k)}^{k}\)

We give an explicit formula for the excess risk achieved by for the \(k\)-step SD estimator from Eq. (8). Since \((,^{(k)})\) is linear in each of \(_{i}^{(k)},i[k]\) (recall that \(^{(k)}\) is a reparametrization of \(^{(k)}\)), the overall excess risk is _quadratic_ in \(^{(k)}\) as shown below. Appendix I provides a proof and the expressions for \(M^{(k)},\,m^{(k)}\), and \(c\). This quadratic form will be especially useful in experiments.

**Theorem 5** (Informal version of Theorem 7 in Appendix I).: _Under the fixed design linear regression in Assumption 1, the excess risk achieved by the \(k\)-step SD is quadratic in \(^{(k)}^{k}\):_

\[((,^{(k)}))\ =\ (^{(k)})^{} _{}(^{(k)})}_{^{k  k}}+2(^{(k)})^{}_{}}_{ ^{k}}+c\.\] (18)From the detailed expressions given in Appendix I, we note that \(M^{(k)}\) is a sum of \(r\) symmetric rank-\(1\) matrices, which means it can have a maximum rank of \(r\). This implies that \(M^{(k)}^{k k}\) for \(k>r\) is rank-deficient (causing no additional decrease in the excess risk if the \(^{(r)}^{r}\) were chosen optimally to minimize the excess risk). This indicates that \(r\) steps of SD might be sufficient to achieve the optimal excess risk, which is indeed what we observe in Theorem 4.

## 5 Experiments

In this section, we empirically show that multi-step SD can outperform the ridge and single-step SD. We first present a synthetic setting (section 5.1) to validate our theory. In section 5.2, we discuss a strategy to select \(\) parameters based on the theoretical insight from section 4.4. In section 5.3, we implement that strategy on real-world regression tasks and show that it can indeed select performant \(\) values that provide multi-step SD estimators that achieve a smaller test risk.

### Synthetic Experiments

We validate our theoretical results with a fixed design synthetic experiment. We consider a problem with \(d=r=4\), and set problem parameters \((,^{},^{2})\). Namely, \(\)'s singular values are set as \(s_{j}:=}{{j}}\) for \(j\), and \(^{}:=_{1}\) as in Theorem 1. Figure 3 shows the result for \(=0.125\), along with two more settings that validate the necessity of our assumptions (validating Theorems 2 and 3). Figure 2(a) confirms that repeated steps of SD do provide a reduction in the excess risk, since the _lowest point_ of the curve for each \(k\) reduces as \(k\) increases. Also note that the optimal \(\) for each \(k\) (one that produces lowest excess risk estimator) is different. Appendix J presents some more settings, including \(^{}:=}{{}}(_{1}+_{2})\) for comparison with . An interesting phenomenon in Figure 3 is that local maxima in \(k\)-step SD's curve coincide with local minima in \((k-1)\)-step SD's curve, which was proven for \(k=1\) in , and we observe empirically for all values of \(k\).

**Explanation of Figures 3, 5**. For the fixed design synthetic experiment in Figure 3 and the random design real-world experiment in Figure 5 (section 5.3), the curves plotted are with the optimal \((^{(k)})^{}\) for each \(\). Hence, the curve of \(k\)-step SD will point-wise be lower/equal to the curve of \((k-1)\)-step SD, since more steps of SD only provide more freedom. We say \(k\)-step SD _strictly_ dominates \((k-1)\)-step SD when the minimum value of \(k\)-step SD's excess risk is strictly lower than that of \((k-1)\)-step SD. For Figure 3, the optimal \((^{(k)})^{}\) is found analytically from the problem parameters. For real-world datasets in Figure 5, we use the strategy in section 5.2 to find \((^{(k)})^{}\).

### Choosing the hyperparameters \(\) for real-world datasets

We have shown that at the cost of introducing additional hyperparameters _and_ setting them to their optimal values, one can extract a large (upto \((d)\)) performance gain. However, how does one select these \(\)'s for real-world datasets? The standard practice is to use a validation set, and perform a grid search. But this becomes infeasible for \(k\)-step SD for larger values of \(k\), since performing a search over parameters in \(^{k+1}\) (i.e \(k\) values of \(^{(k)}\) and \(1\) value of \(\)) quickly becomes impractical. However, our theoretical analysis provides an insight that can be used to directly compute the optimal \(^{(k)}^{k}\) (for a chosen \(\)) given a few evaluations on the validation set with certain chosen \(^{(k)}\) values. Namely, Theorem 5 tells us that the \(\) is quadratic in \(^{(k)}\) (the reparameterized version). Now the coefficients of the quadratic depend on unknown quantities (like \(^{},^{2}\)), however just knowing the quadratic nature of the functional, we can use the validation set to estimate these coefficients. To estimate the coefficients for \(k\)-step SD, we need \(}{{2}}+1\) evaluations on the validation set. Appendix K provides more discussion, and a detailed illustration of the above process for \(k=1,2\). Note that this is feasible when the cost/time needed for a single training run of a \(k\)-step SD is small (since we need to perform it \(O(k^{2})\) times), which holds true for linear regression.

### Real-world regression experiments

We implement multi-step SD for real-world regression tasks from the UCI repository , and demonstrate that \(2\)-step SD can outperform ridge and \(1\)-step SD. Note that for this section, the test set will contain fresh samples of \(X^{d}\), i.e. random design linear regression instead of fixed design. Our metric for an estimator's performance will now be mean squared error (MSE) on a test set of unseen examples, which is the empirical version of total risk (i.e. excess risk translated by the unknown noise variance \(^{2}\)). Using the training and validation splits, we compute (\(i\)) Optimal ridge: \((_{0}^{})\), (\(ii\)) Optimal \(1\)-step SD: \((_{1}^{},^{})\), and (\(iii\)) Optimal \(2\)-step SD: \(_{2}^{},(_{1}^{},_{2}^{})\). The procedure is to plot the MSE on the validation set for a grid of \(\) values for all three estimators, and choose the \(\) that achieves the lowest error for each one (for any given \(\), the optimal \(^{}()\) is chosen by the strategy described in Section 5.2). Finally, we evaluate the MSE of all three selected estimators (i.e. with the chosen optimal hyperparameters) on the test set, which serves as our performance metric (refer to Table 1). Appendix L explains the overall methodology in greater detail. We apply this methodology on three datasets (dataset descriptions in Appendix L.1).

Table 1 describes the results we observe. For two of the three datasets, \(2\)-step SD can outperform both ridge and \(1\)-step SD. For the Air Quality dataset, \(2\)-step SD significantly outperforms both ridge and \(1\)-step SD, reducing the MSE by \(47.2\)% compared to the optimal ridge. In contrast, for the AEP dataset, we observe that the SD process cannot improve upon the ridge at all. The MSE curves in Figure 5 also shed light on these observations. Notice how Figures 4(a), 4(b) show a gap in the ridge and \(2\)-step SD (similar to Figure 2(a)), whereas Figure 4(c) shows no such gap (similar to Figure 2(c)).

In Appendix L.2, we explain the lack of gains from self-distillation on the AEP dataset from the lens of Assumption 2.2. Recall that Theorem 1 says that the gains of repeated SD are most prominent with a small \(\) in Assumption 2.2. Figure 10 shows that the \(^{}\) for the AEP dataset is at most \( 35\%\) aligned with any of the eigenbasis directions, corresponding to a \( 0.65\). The same alignment for the other two datasets is \( 80\%\), corresponding to a much smaller \( 0.2\). In Appendix L.3, we also verify that the strategy described in section 5.2 indeed selects performant \(\) values.

## 6 Conclusion and Broader Impacts

In this paper, we theoretically studied the multi-step self-distillation for fixed design linear regression, with the goal of characterizing its performance compared to the single-step SD. Perhaps surprisingly, we demonstrated that the optimal multi-step SD can outperform the optimal single-step SD by a factor as large as \(d\) in the estimator's excess risk, where \(d\) is the input dimension of the regression. Our analysis is limited by the fixed design assumption, and it would be useful to study the case of random design linear regression as well. We empirically demonstrated the gains from using \(2\)-step SD on simple linear regression tasks. Larger scale empirical studies of multi-step SD, especially leveraging the insights of Section 5.2 on hyperparameter search, remain as a direction of future work.

Our contributions are largely on the theoretical understanding of multi-step self-distillation, and its potential performance gains. At a high-level, self-distillation can use data more effectively, since it allows us to extract more knowledge from the same training dataset. In today's age with data being one of the most important resources, this has positive potential impacts through more judicious use of data. On the other hand, we propose to use multiple steps of self-distillation, requiring more compute and potentially contributing to higher environmental costs.

  
**Dataset** & **Optimal ridge** & **Optimal \(1\)-step SD** & **Optimal \(2\)-step SD** \\   & Optimality hyperparameters & \(_{0}^{}=10^{2}\) & \(_{1}^{},^{}=10^{3},-4.1\) & \(_{2}^{},(_{1}^{},_{2}^{})=10^{3},(-0.9,-16.2)\) \\  & Test set MSE & 2.01 & 1.99 & **1.06** \\   & Optimality hyperparameters & \(_{0}^{}=10^{2}\) & \(_{1}^{},^{}=10^{0},66.5\) & \(_{2}^{},(_{1}^{},_{2}^{})=10^{3},(-1.8,-7.8)\) \\  & Test set MSE & 1.34 & 1.22 & **1.19** \\   & Optimality hyperparameters & \(_{0}^{}=10^{2.5}\) & \(_{1}^{},^{}=10^{2.5},0.1\) & \(_{2}^{},(_{1}^{},_{2}^{})=10^{2.5},(-2.4,-2.3)\) \\  & Test set MSE & **0.62** & 0.62 & 0.63 \\   

Table 1: Chosen hyperparameter values and the achieved test set MSE for ridge and \(1,2\)-step SD.

Figure 5: Validation set MSE vs \(\) for three estimators: Ridge, \(1\)-step SD and \(2\)-step SD.