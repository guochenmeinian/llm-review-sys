# Can Learned Optimization Make Reinforcement Learning Less Diffic!

Alexander D. Goldie\({}^{*}\)\({}^{12}\) Chris Lu\({}^{1}\) Matthew T. Jackson\({}^{12}\)

Shimon Whiteson\({}^{12}\) Jakob N. Foerster\({}^{}\)\({}^{1}\)

\({}^{1}\)FLAIR, University of Oxford \({}^{2}\)WhiRL, University of Oxford

Correspondence to goldie@robots.ox.ac.uk.Equal supervision.Open-source code is available here.

###### Abstract

While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned **O**ptimization for **P**lasticity, **E**xploration and **N**on-stationarity (Open\({}^{1}\)), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, Open outperforms or equals traditionally used optimizers. Furthermore, Open shows strong generalization characteristics across a range of environments and agent architectures.

## 1 Introduction

Reinforcement learning [1, RL] has undergone significant advances in recent years, scaling from solving complex games  towards approaching real world applications . However, RL is limited by a number of difficulties which are not present in other machine learning domains, requiring the development of numerous hand-crafted workarounds to maximize its performance.

Here, we take inspiration from three _difficulties_ of RL: non-stationarity due to continuously changing input and output distributions ; high degrees of plasticity loss limiting model capacities ; and exploration, which is needed to ensure an agent does not converge to local optima prematurely . Overcoming these challenges could enable drastic improvements in the performance of RL, potentially reducing the barriers to applications of RL in the real-world. Thus far, approaches to tackle these problems have relied on human intuition to find _hand-crafted_ solutions. However, this is fundamentally limited by human understanding. _Meta-RL_ offers an alternative in which RL algorithms themselves are learned from data rather than designed by hand. Meta-learned RL algorithms have previously demonstrated improved performance over hand-crafted ones .

On a related note, _learned optimization_ has proven successful in supervised and unsupervised learning (e.g., VeLO ). Learned optimizers are generally parameterized update rules trained to outperform handcrafted algorithms like gradient descent. However, current learned optimizers perform poorly in RL . While some may argue that RL is simply an out-of-distribution task , the lack ofconsideration for, and inability to _target_, specific difficulties of RL in naive learned optimizers leads us to believe that they would still underperform even if RL were in the training distribution.

We propose an algorithm, shown in Figure 1, for meta-learning optimizers specifically for RL called Learned **O**ptimization for **P**lasticity, **E**xploration and **N**onstationarity (Open). Open meta-learns update rules whose inputs are rooted in previously proposed solutions to the difficulties above, and whose outputs use _learnable stochasticity_ to boost exploration. Open is trained to maximize final return _only_, meaning it does not use regularization to mitigate the difficulties it is designed around.

In our results (Section 6), we benchmark Open against handcrafted optimizers (Adam , RMSProp ), open-source discovered/learned optimizers (Lion , VeLO ) and baseline learned optimizers trained for RL (Optim4RL , '_No Features_'). We show that Open can fit to single and small sets of environments and also generalize in- and out-of-support of its training distribution. We further find that Open generalizes better than Adam  to gridworlds and Craftax-Classic . Therefore, our novel approach of _increasing_ optimizer flexibility via extra inputs and an exploration-driven output, rather than enforcing more structured updates, enables significant performance gains.

## 2 Background

Reinforcement LearningThe RL problem is formulated as a _Markov Decision Process_[1, MDP] described by the tuple \(,,P,R,,\). At discrete timestep \(t\), the agent takes action \(a_{t}\) sampled from its (possibly stochastic) policy, \((|s_{t})\), which conditions on the current state \(s_{t}\) (where \(s_{0}\)). After each action, the agent receives reward \(R(s_{t},a_{t})\) and the state transitions to \(s_{t+1}\), based on the transition dynamics \(P(s_{t+1}|s_{t},a_{t})\). An agent's objective is to maximize its _discounted expected return_, \(J^{}\), corresponding to a discount factor \([0,1)\), which prevents the agent making _myopic_ decisions. This is defined as

\[J^{}_{a_{0:},s_{0},s_{1:} }[_{t=0}^{}^{t}R_{t}].\] (1)

Proximal policy optimization [22, PPQ] is an algorithm designed to maximize \(J^{}\). It uses advantage, \(A^{}(s,a)\), which is calculated from the _state value function_, \(V^{}=_{}[_{t=0}^{}^{t}R_{t}|S_{t}=s]\), and _state-action value function_, \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}R_{t}|S_{t}=s,A_{ t}=a]\). It measures the improvement of a specific action over the current policy and takes the form

\[A^{}(s,a)=Q^{}(s,a)-V^{}(s).\] (2)

PPO introduces a loss for optimizing the policy, parameterized by \(\), that prevents extreme policy updates in gradient _ascent_. This uses _clipping_, which ensures there is no benefit to updating \(\) beyond where the policy _probability ratio_, \(r_{t}()=|s_{t})}{_{d_{t}}(a_{t}|s_{t})}\), exceeds the range \([1]\). The clipped loss is

\[L^{CLIP}()=[(r_{t}()A^{}(s_{t},a_{t}), {clip}(r_{t}(),1)A^{}(s_{t},a_{t}))].\] (3)

PPO is an actor-critic  method, where the policy and value functions are modeled with different neural networks, or separate heads of the same neural network, conditioning on state. The PPO objective to maximize combines the clipped loss, a value function error, and an entropy bonus into

\[L_{t}()=[L_{t}^{CLIP}()-c_{1}L_{t}^{VF}()+c_{2}S [_{}](s_{t})].\] (4)

Figure 1: A visualization of Open. We train \(N\) agents, replacing the handcrafted optimizer of the RL loop with ones sampled from the meta-learner (i.e., evolution). Each optimizer conditions on gradient, momentum and additional inputs, detailed in Section 5.3, to calculate updates. The final returns from each loop are output to the meta learner, which improves the optimizer before repeating the process. A single inner loop step is described algorithmically in Appendix B.1.

Meta-Learning Optimizers in RLAlgorithms like Adam  or RMSprop  are designed to maximize an objective by updating the parameters of a neural network in the direction of _positive gradient_ with respect to the function. They are often applied with augmentations such as _momentum_ or _learning rate schedules_ to better converge to optima. Learned optimizers offer an alternative: parameterized _update rules_, conditioning on more than just gradient, which are trained to maximize an objective [23; 16; 24]. For any parameterized optimizer \(_{}\), which conditions on a set of inputs \(\), the update rule to produce update \(u\) can be described as a function, \(_{}() u\).

We treat learning to optimize as a meta-RL problem . In meta-RL, the goal is to maximize \(J^{}\) over a _distribution_ of MDPs \(P()\). For our task, an optimizer trained to maximize \(()=_{}[J^{}|_{}]\) yields the optimal meta-parameterization \(_{}\).

Evolution StrategiesEvolution algorithms (EA) are a _backpropagation-free_, black-box method for optimization  which uses a population of _perturbed_ parameters sampled from a distribution (here, \((,^{2}I)\)). This population is used to maximize a _fitness_\(F()\). EA encompasses a range of techniques (e.g., evolution strategies (ES) [26; 27], genetic algorithms  or CMA-ES ).

Natural evolution strategies (NES)  are a class of ES methods that use the population fitness to estimate a natural gradient for the mean parameters, \(\). This can then be optimized with typical gradient ascent algorithms like Adam . Salimans et al.  introduce _OpenAI ES_ for optimizing \(\) using the estimator

\[_{}_{ N(0,I)}F(+)= _{ N(0,I)}\{F(+) \},\] (5)

which is approximated using a population average. In practice, we use antithetic sampling (i.e., for each sampled \(\), evaluating \(+\) and \(-\)) . Antithetic _task_ sampling enables learning on a task _distribution_, by evaluating and ranking each antithetic pair on _different_ tasks .

Historically, RL was too slow for ES to be practical for meta-training. However, PureJaxRL  recently demonstrated the feasibility of ES for meta-RL, owing to the speedup enabled by vectorization in Jax . We use the implementation of _OpenAI ES_ from evosax .

## 3 Related Work

### Optimization in RL

Wilson et al.  and Reddi et al.  show that adaptive optimizers struggle in highly stochastic processes. Henderson et al.  indicate that, unlike other learning regimes, RL is sufficiently stochastic for these findings to apply, which suggests RL-specific optimizers could be beneficial.

The idea that optimizers designed for supervised learning may not perfectly transfer to RL is reflected by Bengio et al. , who propose an amended momentum suitable for temporal difference learning . This is related to work by Sarigul and Avci , who explore the impact of different types of momentum on RL. While these works motivate designing optimization techniques _specifically_ for RL, we take a more expressive approach by replacing the whole optimizer instead of just its momentum calculation, and using meta-learning to fit our optimizer to data rather than relying on potentially suboptimal human intuition.

### Meta-learning

Discovering RL AlgorithmsRather than using handcrafted algorithms, a recent objective in meta-RL is _discovering_ RL algorithms from data. While there are many successes in this area (e.g., Learned Policy Gradient [13; LPG], MetaGenRL , Active Adaptive Perception  and Learned Policy Optimisation [15, LPO]), we focus on meta-learning a replacement to the _optimizer_ due to the outsized impact a learned update rule can have on learning. We also use _specific_ difficulties of RL to guide the _design_ of our method, rather than simply applying end-to-end learning.

Jackson et al.  learn temporally-aware versions of LPO and LPG. While their approach offers inspiration for dealing with non-stationarity in RL, they also rely on Adam , an optimizer designed for stationarity that is suboptimal for RL . Instead, we propose replacing the _optimizer_ itself with an expressive and dynamic update rule that is not subject to these problems.

Learned OptimizationLearning to optimize [23; 42; 43; L2O] strives to learn replacements to handcrafted gradient-based optimizers like Adam , generally using neural networks (e.g., [44; 24; 45; 46]). While they show strong performance in supervised and unsupervised learning [16; 47], previous learned optimizers do not consider the innate difficulties of RL, limiting transferability. Furthermore, while VeLO used 4000 TPU-months of compute , Open requires on the order of one GPU-month.

Lion  is a symbolic optimizer discovered by evolution that outperforms AdamW  using a similar update expression. While the simplistic, symbolic form of Lion lets it generalize to RL better than most learned optimizers, it cannot condition on features additional to gradient and parameter value. This limits its expressibility, ability to target the difficulties of RL and, therefore, performance.

Learned Optimization in RLLearned optimization in RL is significantly more difficult than in other learning domains due to the problems outlined in section 4. For this reason, SOTA learned optimizers fail in transfer to RL . Optim4RL  attempts to solve this issue by learning to optimize directly in RL. However, in their tests and ours, Optim4RL fails to consistently beat handcrafted benchmarks. Also, it relies on a heavily constrained update expression based on Adam , and it needs expensive learning rate tuning. Instead, we achieve much stronger results with a completely _black-box_ setup inspired by preexisting methods for mitigating specific difficulties in RL.

## 4 Difficulties in RL

We believe that fundamental differences exist between RL and other learning paradigms which make RL particularly difficult. Here, we briefly cover a specific set of prominent difficulties in RL, which are detailed with additional references in appendix A. Our method takes inspiration from handcrafted heuristics targeting these challenges (Section 5.3). We show via thorough ablation (Section 7) that explicitly formulating our method around these difficulties leads to significant performance gains.

(Problem 1) Non-stationarityRL is subject to non-stationarity over the training process  as the updating agent causes changes to the training distribution. We denote this _training non-stationarity_. Lyle et al.  suggest optimizers designed for stationary settings struggle under nonstationarity.

(Problem 2) Plasticity lossPlasticity loss, or the inability to fit new objectives during training, has been a theme in recent deep RL literature [9; 49; 50; 10]. Here, we focus on dormancy , a measurement tracking inactive neurons used as a metric for plasticity loss [51; 52; 10; 53]. It is defined as

\[s_{i}^{l}=_{x D}|h_{i}^{l}(x)|}{}_{k h }_{x D}|h_{k}^{l}(x)|},\] (6)

where \(h_{i}^{l}(x)\) is the activation of neuron \(i\) in layer \(l\) with input \(x D\) for distribution \(D\). \(H^{l}\) is the total number of neurons in layer \(l\). The denominator normalizes average dormancy to \(1\) in each layer.

A neuron is \(\)-dormant if \(s_{i}^{l}\), meaning the neuron's output makes up less than \(\) of its layer's output. For ReLU activation functions, \(=0\) means a neuron is in the saturated part of the ReLU. Sokar et al.  find that dormant neurons generally _stay_ dormant throughout training, motivating approaches which try to reactivate dormant neurons to boost plasticity.

(Problem 3) ExplorationExploration is a key problem in RL . To prevent premature convergence to local optima, and thus maximize final return, an agent must explore uncertain states and actions. Here, we focus on parameter space noise for exploration , where noise is applied to the parameters of the agent rather than to its output actions, like \(\)-greedy .

## 5 Method

There are three key considerations when doing learned optimization for RL: what architecture to use; how to train the optimizer; and what inputs the optimizer should condition on. In this section, we systematically consider each of these questions to construct Open, with justification for each of our decisions grounded in our core difficulties of RL.

### Architecture and Parameterization

To enable conditioning on history, which is required to express behavior like momentum, for example, Open uses a gated recurrent unit [54, GRU]. This is followed by two fully connected layers with LayerNorm , which we include for stability. All layers are small; this is important for limiting memory usage, since the GRU stores separate states for _every_ agent parameter, and for maintaining computational efficiency . We visualize and detail the architecture of Open in Appendix B.

Update ExpressionWe split the calculation of the update in Open into three stages, each of which serves a different purpose. The first stage, which follows Metz et al. , is

\[_{i}=_{1}m_{i}_{2}e_{i},\] (7)

where \(m_{i}\) and \(e_{i}\) are optimizer outputs for parameter \(i\), and \(_{(1,2)}\) are small scaling factors used for stability. This update can cover many orders of magnitude without requiring large network outputs.

(P3)Secondly, we augment the update rule for the _actor_ only to increase exploration; there is no need for the critic to explore. We take inspiration from parameter space noise for exploration , since it can easily be applied via the optimizer. To be precise, we augment the actor's update as

\[_{i}^{}:=_{i}^{}+_{3} _{i}^{}.\] (8)

Here, \(_{3}\) is a small, stabilizing scaling factor, \(_{i}^{}\) is a third output from the optimizer, and \((0,1)\) is sampled Gaussian noise. By multiplying \(_{i}^{}\) and \(\), we introduce a random walk of learned, per-parameter variance to the update which can be used for exploration. Since \(_{i}^{}\) depends on the optimizer's inputs, this can potentially learn complex interactions between the noise schedule and the features outlined in Section 5.3. Unlike Plappert et al. , who remove and resample noise between rollouts, Open adds permanent noise to the parameters. This benefits plasticity loss (i.e., (P2)), in principle enabling the optimizer to reactivate dormant neurons in the absence of gradient.

Unfortunately, naive application of the above updates can cause errors as, even without stochasticity, agent parameters can grow to a point of numerical instability. This causes difficulty in domains with _continuous_ action spaces, where action selection can involve exponentiation to get a non-negative standard deviation. Therefore, the _final_ update stage stabilizes meta-optimization by zero-meaning, as

\[=}-[}].\] (9)

While this limits the update's expressiveness, we find that even traditional optimizers tend to produce nearly zero-mean updates. In practice, this enables learning in environments with continuous actions without harming performance for discrete actions. Parameter \(i\) is updated as \(p_{i}^{(t+1)}=p_{i}^{(t)}-u_{i}\).

### Training

We train our optimizers with _OpenAI ES_, using _final return_ as the fitness. We apply the commonly-used rank transform, which involves mapping rankings over the population to the range \([-0.5,0.5]\), to the fitnesses before estimating the ES gradient; this is a form of fitness shaping [30; 26], and makes learning both easier and invariant to reward scale.

For training on multiple environments simultaneously (i.e., multi-task training, Section 6), we evaluate _every_ member of the population on _all_ environments. After evaluation, we: 1) Divide by the return Adam achieves in each environment; 2) Average the scores over environments; 3) Do a rank transform. Normalizing by Adam maps returns to a roughly common scale, enabling comparison between diverse environments. However, this biases learning to environments where Adam underperforms Open. We believe finding better curricula for multi-task training would be highly impactful future work.

### Inputs

Carefully selecting which inputs to condition Open on is crucial; they should be sufficiently expressive without significantly increasing the computational cost or meta-learning sample requirements of the optimizer, and should allow the trained optimizers to surgically target specific problems in RL. To satisfy these requirements, we take inspiration from prior work addressing our focal difficulties of RL. In spirit, we distill current methods to a 'lowest common denominator' which is cheap to calculate. We provide additional details of how these features are calculated in Appendix B.2.

(P1) Two training timescalesMany learned optimizers for stationary problems incorporate some version of progress through training as an input [16; 45; 46]. Since PPO learns from successively collected, stationary batches of data , we condition Open on how far it is through updating with the current batch (i.e., _batch proportion_). This enables behavior like learning rate scheduling, which has proved effective in stationary problems (e.g., [57; 58]), and bias correction from Adam to account for inaccurate momentum estimates .

Inspired by Jackson et al. , who demonstrate the efficacy of learning dynamic versions of LPG  and LPO , we also condition Open on how far the current batch is through the total number of batches to be collected (i.e., _training proportion_). This directly targets _training non-stationarity_.

(P2) **Layer Proportion**Nikishin et al. [59; 60] operate on higher (i.e., closer to the output) network layers in their attempts to address plasticity loss in RL. Furthermore, they treat intervention depth as a hyperparameter. To replicate this, and enable varied behavior between the different layers of an agent's network, we condition Open on the relative position of the parameter's layer in the network.

(P2) **Dormancy**As Sokar et al. _reinitialize_\(\)-dormant neurons, we condition Open _directly_ on dormancy; this enables similar behavior by allowing Open to react as neurons become more dormant. In fact, in tandem with learnable stochasticity, this enables Open to reinitialize dormant neurons, just like Sokar et al. . Since dormancy is calculated for _neurons_, rather than _parameters_, we use the value of dormancy for the neuron _downstream_ of each parameter.

## 6 Results

In this section, we benchmark Open against a plethora of baselines on large-scale training domains.

### Experimental Setup

Due to computational constraints, we meta-train an optimizer on a single random seed without ES hyperparameter tuning. This follows standard evaluation protocols in learned optimization (e.g., [16; 17; 47]), which are also constrained by the high computational cost of meta-learned optimization. We provide the cost of experiments in Appendix J, including a comparison of runtimes with other optimizers. We detail hyperparameters in Appendix C.5. We define four evaluation domains, based on Kirk et al. , in which an effective learned optimization framework _should_ prove competent:

Single-Task TrainingA learned optimizer must be capable of fitting to a single environment, and being evaluated in the _same_ environment, to demonstrate it is expressive enough to learn _an_ update rule. We test this in five environments: Breakout, Asterix, Space Invaders and Freeway from MinAtar [62; 63]; and Ant from Brax [64; 65]. This is referred to as'singleton' training in Kirk et al. .

Multi-Task TrainingTo show an optimizer is able to perform under a wide input distribution, it must be able to learn in a number of environments _simultaneously_. Therefore, we evaluate performance from training in all four environments from MinAtar [62; 63]2. We evaluate the average normalized score across environments with respect to Adam.

In-Distribution Task GeneralizationAn optimizer should generalize to unseen tasks within its training distribution. To this end, we train Open on a _distribution_ of gridworlds from Jackson et al.  with antithetic task sampling , and evaluate performance by sampling tasks from the _same distribution_. We include details in Appendix D.

Out-Of-Support Task GeneralizationCrucially, an optimizer unable to generalize to new settings has limited real-world usefulness. Therefore, we explore _out-of-support_ (OOS) generalization by testing Open on specific gridworld distributions defined by Oh et al. , and a set of mazes from minigrid  which are not in the training distribution, with unseen agent parameters. Furthermore, we test how Open performs _0-shot_ in Craftax-Classic , a Jax reimplementation of Crafter .

BaselinesWe compare against open-source implementations  of Adam , RMSProp , Lion  and VeLO [16; 45]. We also _learn_ two optimizers for the _single_- and _multi_-task settings: _'No Features'_, which only conditions on gradient and momentum; and Optim4RL  (using ES instead of meta-gradients, as in Lan et al. ). Since Optim4RL is initialized close to \(()\), and tuning its learning rate is too practically expensive, we set a learning rate of \(0.1_{}\) based on Lion  (which moves from AdamW to \(()\)). The optimizer's weights can be scaled to compensate if this is suboptimal. We primarily consider interquartile mean (IQM) of final return with 95% stratified bootstrap confidence intervals . All hyperparameters can be found in Appendix C.

### Single-Task Training Results

Figure 2 shows the performance of Open after single-task training. In three MinAtar environments, Open significantly outperform all baselines, far exceeding previous attempts at learned optimization for RL. Additionally, Open beat both learned optimizers in _every_ environment. Overall, these experiments show the capability of Open to learn highly performant update rules for RL.

Return curves (Appendix E) show that Open does not always achieve high returns from the start of training. Instead, Open can sacrifice greedy, short-term gains in favor of long-term final return, possibly due to its dynamic update rule. We further analyze the optimizers' behavior in Appendix F.

### Multi-Task Training Results

Figure 3 shows each optimizer's ability to fit to multiple MinAtar environments [63; 62], where handcrafted optimizers are tuned _per-environment_. We normalize returns with respect to Adam, and aggregate scores over environments to give a single performance metric. Here, we increase the size of the learned optimizers, with details in Appendix B.5. In addition to IQM, we consider the _mean_ normalized return to explore the existence of outliers (which often correspond to asterix, where Open strongly outperforms Adam), and optimality gap, a metric from Agarwal et al.  measuring how close to optimal algorithms are. Unlike single-task training, where optimizers are trained until convergence, we run multi-task experiments for a fixed number of generations (300) to limit compute.

Open drastically outperforms **every** optimizer for multi-task training. In particular, Open produces the only optimizer with an aggregate score higher than Adam, demonstrating its ability to learn highly expressive update rules which can fit to a range of contexts; no other learned optimizers get close to Open in fitting to multiple environments simultaneously. As expected, Open prioritizes optimization in asterix and breakout, according to the return curves (Appendix G); we believe better curricula would help to overcome this issue. Interestingly, Optim4RL seems to perform better in the multi-task setting than the single-task setting. This may be due to the increased number of meta-training samples.

### Generalization

Figure 4 shows Open's ability to generalize in a gridworld setting. We explore _in-distribution_ generalization on the left, and _OOS_ generalization on the right, where the top row (_rand_dense_, _rand_sparse_ and _rand_long_) are from LPG [13; 66] and the bottom row are 3 mazes from Minigrid [67; 66]. We explore an additional dimension of OOS generalization in the agent's network hidden size; Open only saw agents with network hidden sizes of 16 in training, but is tested on larger and smaller agents. We include similar tests for OOS training lengths in Appendix H. We normalize Open against Adam, which was tuned for the same distribution and agent that Open learned for.

Figure 3: Mean, IQM and optimality gap (smaller = better), evaluated over 16 random seeds _per environment_ for the aggregated, Adam-normalized final returns after multi-task training on MinAtar [62; 63]. We plot 95% stratified bootstrap confidence intervals for each metric.

Figure 2: IQM of final returns for the five single-task training environments, evaluated over 16 random environment seeds. We plot 95% stratified bootstrap confidence intervals for each environment.

Open learns update rules that consistently outperform Adam _in-distribution_ and _out-of-support_ with regards to both the agent and environment; in _every_ OOS environment and agent size, Open outperformed Adam. In fact, in all but rand_dense, its generalization **improves** compared to Adam for some different agent widths, increasing its normalized return. In combination with our findings from Section 6.3, which demonstrate our approach can learn expressive update rules for hard, diverse environments, these results demonstrate the effectiveness of Open: if trained on a wide enough distribution, Open has the potential to generalize across a wide array of RL problems.

Finally, in Figure 5, we test how the optimizer trained above (i.e., for Figure 4) transfers to Craftax-Classic, an environment with completely different dynamics, _0-shot_. We compare against two baselines: Adam (0-shot), where we tune the hyperparameters of Adam on the distribution of gridworlds that Open was trained on; and Adam (Tuned), where the Adam and PPO hyperparameters are tuned directly in Craftax-Classic. The latter gives an idea of the maximum achievable return.

We find that Open transfers significantly better than Adam when both are tuned in the same distribution of environments, and performs only marginally worse than Adam tuned directly in-distribution. These results exemplify the generalization capabilities of Open and lay the foundation for learning a large scale 'foundation' optimizer which could be used for a wide range of RL problems.

## 7 Ablation Study

In our ablation study, we explore how each constituent of Open contributes to improved performance. Firstly, we focus on two specific, measurable challenges: plasticity loss and exploration. Subsequently, we verify that Open can be meta-trained for an RL algorithm besides PPO.

### Individual Ablations

We ablate each individual design decision of Open in Figure 6. For each ablation, we train 17 optimizers in a shortened version of Breakout [62; 63] and evaluate performance after 64 PPO training runs per optimizer. Further details of the ablation methodology are in Appendix I.

Figure 4: IQM of return, normalized by Adam, in seven gridworlds, with 95% stratified bootstrap confidence intervals for 64 random seeds. On the left, we show performance in the distribution Open and Adam were trained and tuned in. On the right, we show OOS performance: the top row shows gridworlds from Oh et al. , and the bottom row shows mazes from Chevalier-Boisvert et al. . We mark Hidden Size \(=16\) as the in-distribution agent size for Open and Adam.

Figure 5: A comparison of OPEN and Adam with and without hyperparameter tuning in Craftax-Classic. We plot mean return over 32 seeds. Standard error is negligible (< 0.06).

While measuring plasticity loss is important, dormancy alone is not an appropriate performance metric; a newly initialized network has near-zero dormancy but poor performance. Instead, we include dormancy here as one _possible_ justification for why some elements of Open are useful.

(P1) Ablating _training proportion_ directly disables the optimizer's ability to tackle _training_ nonstationarity. Similarly, removing _batch proportion_ prevents dynamic behavior within a (stationary) batch. The corresponding decreases in performance show that both timescales of non-stationarity are beneficial, potentially overcoming the impact of non-stationarity in RL.

(P2) Removing _dormancy_ as an input has a **drastic** impact on the agent's return, corresponding to a large increase in plasticity loss. While dormancy plays no _direct_ role in the optimizer's meta-objective, including it as an input gives the optimizer the _capability_ to react as neurons grow dormant, as desired.

(P2) Not conditioning on layer proportion has a negative effect on final return. The increase in dormancy from its removal suggests that allowing the optimizer to behave differently for each layer in the network has some positive impact on plasticity loss.

(P2)/(P3) Stochasticity benefits are two-fold: besides enhancing exploration (Section 7.2), it notably lowers dormancy. Though limited to the actor, this reduction likely also contributes to improve return.

### Exploration

(P3) We verify the benefits of _learnable stochasticity_ in Deep Sea, an environment from bsuite  designed to analyze _exploration_. This environment returns a small penalty for each right action but gives a large positive reward if right is selected for _every_ action. Therefore, agents need to explore beyond the local optimum of 'left' at each timestep' to maximize return. Deep Sea's size can be varied, such that an agent needs to take more consecutive right actions to receive reward.

Naively applying Open to Deep Sea struggles; we posit that optimizing _towards_ the gradient can be detrimental to the actor since the gradient leads to a local optimum, whereas in the critic the gradient always points to (beneficially) more accurate value functions. Therefore, we augment Open to learn different updates for the actor and critic ('_separated_'). In Figure 7, we show the disparity between a separated optimizer trained _with_ and _without_ learnable stochasticity after training for sizes between \(\) in a small number of generations (48), noting that larger sizes are OOS for the optimizer. We include full results in Appendix I.2.

The optimizer with learnable stochasticity consistently achieves higher return in large environments compared to without, suggesting significant exploration benefits. In fact, our analysis (Appendix F.5) finds that, despite being unaware of size, stochasticity increases in larger environments. In other words, the optimizer promotes _more_ exploration when size increases. However, stochasticity does

Figure 6: IQM of mean final return for 17 trained optimizers _per ablation_, evaluated on 64 random seeds each, alongside mean \(=0\) dormancy for optimizers in the interquartile range. We show 95% stratified bootstrap confidence intervals.

Figure 7: IQM performance improvement of an optimizer with learnable stochasticity over one without. We plot 95% stratified bootstrap confidence intervals over 128 random seeds.

marginally reduce performance in smaller environments; this may be due to the optimizer promoting exploration even after achieving maximum reward, rather than converging to the optimal policy.

### Alternative RL Algorithm

To ensure that Open can learn for algorithms beyond PPO, and thus is applicable to a wider range of scenarios, we explore learning an optimizer for PQN  in Figure 8. PQN is a vectorized and simplified alternative to DQN . We include analysis and the return curve in Appendix I, and hyperparameters in Appendix C.

We find that, in our single meta-training run, Open is able to outperform Adam by a significant margin. Therefore, while PPO was the principle RL algorithm for our experiments, our design decisions for Open are applicable to a wider class of RL algorithms. While exploring the ability of Open to learn for other common RL algorithms would be useful, Figure 8 provides a confirmation of the versatility of our method.

## 8 Limitations and Future Work

While Open demonstrates strong success in learning a multi-task objective, our current approach of normalizing returns by Adam biases updates towards environments where Adam underperforms learned optimizers. We believe developing better curricula for learning in this settings, akin to unsupervised environment design [66; 74], would be highly impactful future work. This may unlock the potential for larger scale experiments with wider distributions of meta-training environments. Additionally, the Open framework need not be limited to the specific difficulties we focus on here. Exploring ways to include other difficulties of RL which can be measured and incorporated into the framework (e.g., sample efficiency or generalization capabilities of a policy) could potentially elevate the usefulness of Open even further. Finally, while we show that Open can learn for different RL algorithms, further testing with other popular algorithms (e.g., SAC , A2C ) would be insightful. Furthermore, training Open on many RL algorithms simultaneously could unlock a truly generalist learned optimization algorithm for RL. Successfully synthesizing these proposals with additional scale will potentially produce a universal and performant learned optimizer for RL.

## 9 Conclusion

In this paper, we set out to address some of the major difficulties in RL by meta-learning update rules directly for RL. In particular, we focused on three main challenges: non-stationarity, plasticity loss, and exploration. To do so, we proposed Open, a method to train parameterized optimizers that conditions on a set of inputs and uses learnable stochasticity in its output, to specifically target these difficulties. We showed that our method outperforms a range of baselines in four problem settings designed to show the expressibility and generalizability of learned optimizers in RL. Finally, we demonstrated that each design decision of Open improved the performance of the optimizer in an ablation study, that the stochasticity in our update expression significantly benefited exploration, and that Open was sufficiently versatile to learn with different RL algorithms.

## Author Contributions

**AG** implemented the code, designed and performed all experiments and made the architectural and training design decisions of Open. **AG** also provided all analysis and wrote all of the paper.

**CL** proposed the initial project, contributed the initial implementation framework and provided assistance throughout the project's timeline.

**MJ** contributed to experimental and ablation design and helped with editing the manuscript.

**SW** and **JF** provided advice throughout the project, and in writing the paper.

Figure 8: IQM of final return for Adam and OPEN after training PQN in Asterix. We plot 95% stratified bootstrap confidence intervals over 64 seeds.

## Funding

**AG** and **MJ** are funded by the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems EP/S024050/1. **MJ** is also sponsored by Amazon Web Services. **JF** is partially funded by the UKI grant EP/Y028481/1 (originally selected for funding by the ERC). **JF** is also supported by the JPMC Research Award and the Amazon Research Award. Our experiments were also made possible by a generous equipment grant from NVIDIA.