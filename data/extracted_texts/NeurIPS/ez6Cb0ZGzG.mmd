# Continual Learning for Instruction Following from Realtime Feedback

Alane Suhr

University of California, Berkeley

suhr@berkeley.edu

&Yoav Artzi

Cornell University

yoav@cs.cornell.edu

Work done while at Cornell University.

###### Abstract

We propose and deploy an approach to continually train an instruction-following agent from feedback provided by users during collaborative interactions. During interaction, human users instruct an agent using natural language, and provide realtime binary feedback as they observe the agent following their instructions. We design a contextual bandit learning approach, converting user feedback to immediate reward. We evaluate through thousands of human-agent interactions, demonstrating 15.4% absolute improvement in instruction execution accuracy over time. We also show our approach is robust to several design variations, and that the feedback signal is roughly equivalent to the learning signal of supervised demonstration data.

## 1 Introduction

The dynamics that arise in situated language interactions between human users and automated agents expose a plethora of language learning signals. A prime example of such a signal is explicit feedback: when users convey their intent via natural language instructions for an agent to follow, they are well positioned to provide feedback, for example, through a stream of binary signals as the agent acts.

Learning from this type of signal has significant potential. It shifts the burden of learning from annotated data to learning through interaction with users, which not only reduces data costs, but also enables continual2 improvement through interaction with users. This signal also fundamentally differs from gold-standard annotated data: it directly targets the current agent behavior, rather than reflecting optimal human behavior that may be of less relevance to the agent's current policy. This approach stands in contrast with most methods for learning to follow instructions, where the use of demonstration data entails high data costs, and the clear separation between training and deployment passes over opportunities for learning through interaction with users [e.g., 3, 1, 38].

In this paper, we study the problem of continually3 improving an automated instruction-following agent4 by learning from user feedback in human-agent interactions. We situate the interaction in a collaborative environment, which provides conditions necessary for this type of learning: users are incentivized to improve the agent's capabilities by providing feedback, and are continuously present as the agent executes their instructions. We use the CerealBar scenario , where two participants collaborate towards a common goal in a shared world, using natural language instructions to coordinate their actions. During interaction, human users write instructions for an agent, and provide feedback to the agent as it executes them via a stream of binary signals. Figure 1 illustratesthe setup and our learning process. To the best of our knowledge, our work is the first to demonstrate continual learning from human feedback for natural language instruction following.

A key challenge is the complexity of the learning signal. Users provide feedback at their discretion and under time pressure as the agent executes their instructions, resulting in an unpredictable, and often noisy, learning signal. While the concurrency of agent actions and user feedback provides a weak alignment, delays in human response and lack of clarity about which action or actions they critique make it challenging to attribute a learning signal to specific actions. Further complicating learning is the feedback loop created by the continually changing agent behavior and constant adaptation by human users. These factors create a scenario that is particularly difficult for complex methods as they often make various assumptions about the learning signal and the interaction dynamics . We opt for a simple approach with minimal assumptions, and formulate learning as a contextual bandit scenario. We alternate between deployment, where the agent interacts with users that provide feedback, and training, where we compute immediate rewards from feedback signals, and weigh examples using the current policy to account for the continually changing agent behavior.

We experiment with our approach through interactions with humans users, where we repeatedly deploy, train, and re-deploy the agent. We study the effects of human adaptation and process design decisions on the efficacy of learning. We design our experiments to tease apart genuine agent improvements from human user adaptation, and observe dramatic improvements in agent behavior beyond what user adaptation explains, 15.4% absolute improvement in instruction execution accuracy in our longer running experiment. Our code and data is released under the MIT license 2023.4

## 2 Technical Overview

**Interaction Scenario** We situate user-agent interactions in CerealBar, a collaborative scenario where two participants, a leader and a follower, collect sets of matching cards together in a shared, 3D environment. The two participants collaborate towards a shared goal, act in the world, and coordinate using natural language. The leader plans what the pair should do, acts according to the plan, and describes the follower's part of the plan using natural language instructions. The follower's role is to follow the instructions. The pair receives a point for each set of cards they successfully collect. Their goal is to maximize their score. We add to the original CerealBar setting the ability for the leader to provide binary feedback signals to the follower as they execute instructions. In our study, the agent controls the follower and the leader is always a human user. We use CerealBar as it is the only research environment designed to support the real-time collaborative interaction scenario we study: alternatives (e.g., Thor ) are not designed to deploy at scale for model-user interaction studies, and would require extensive modifications to support multi-agent collaborative interactions.

**Task** The agent's task is to map natural language instructions and observations to follower actions. Actions include moving FORWARD or BACKWARD, turning LEFT or RIGHT, and completing instruction execution with STOP. The agent observation is a partial view of the world state from the

Figure 1: Illustration of our continual learning process.2 The process progresses in rounds \(\), each including deployment and training. In deployment, we sample and execute actions \(a\) from a policy \(\) parameterized by \(_{}\), conditioned on user-written instruction \(\) and agent observation \(o\). Concurrently, users provide binary feedback as they observe instruction execution. We convert this feedback to rewards, and estimate parameters \(_{+1}\) of next roundâ€™s policy with a contextual bandit objective.

first-person perspective of the follower. Formally,5 given an instruction \(\) and an initial observation \(o_{1}\), our goal is to generate a sequence \((o_{1},a_{1}),,(o_{m},a_{m})\), where \(o_{i}\) an agent observation, \(a_{i}\) is the action taken, and \(a_{m}=\).

**Inference and Learning** We define the follower agent as a policy \((,o;)\) parameterized by \(\) that maps instruction \(\) and state observations \(o\) to a distribution over actions.6 During interaction with users, for each user-written instruction \(\), we sample and execute actions \(a_{i}(,o_{i};)\) until we sample the instruction completion action. Concurrently, the user may provide positive and negative feedback signals \(f\). Feedback signals are given in realtime, and are not timed to coincide with a specific action. Between any two agent steps, there could be any number of feedback signals.

We optimize parameters through rounds of continual learning (Figure 1). At each round \(\), we deploy the policy with parameters \(_{}\) to interact with users, and then estimate parameters \(_{+1}\). During deployment, we collect agent instruction executions and user feedback. The execution at instruction \(\) during interaction creates a trace made of two sequences \(((o_{i},a_{i},w_{i}^{a})_{i=1}^{m},(f_{j},w_{j}^{f}) _{j=1}^{n})\), where each action \(a_{i}\) and feedback signal \(f_{j}\) are paired with wall times \(w_{i}^{a}\) and \(w_{j}^{f}\). We compute reward from the feedback to construct training examples \((,o,a,r)\), where \(r\) is a reward. We optimize policy parameters by solving a contextual bandit problem, maximizing immediate expected reward.

**Evaluation** Our main metric is instruction execution accuracy for user-written instructions. Because we only have access to agent instruction executions, and not ground-truth demonstrations, we cannot compute accuracy automatically. We use crowdsourcing to acquire human judgments of follower accuracy after each round of deployment. We also evaluate trends in user feedback and ratings.

## 3 Continual Learning

We estimate the policy parameters from user feedback during interaction with users. The process, illustrated in Algorithm 1, progresses in rounds. Each round \(\) includes: (a) deploying the agent policy parameterized by \(_{}\) to interact with users (Section 3.1), (b) computing rewards from user feedback to construct training data \(_{}\) (Section 3.2), and (c) optimizing the policy parameters using all data observed so far \(_{^{}=0}^{}_{^{}}\) to compute \(_{+1}\) (Section 3.3). We initialize the process with a policy parameterized by \(_{1}\) estimated on human demonstration data \(_{0}\) (Algorithm 1, Line 1).

### Deployment Interactions

During deployment in round \(\), users collaborate with the agent, and delegate to it tasks by typing natural language instructions. For each instruction \(\), we sample a sequence of actions from the policy \(a_{i}(,o_{i};_{})\) (Lines 9-15). Executing an action in the environment results in a new observation \(o_{i+1}\) (Line 11). For each action, we record the wall time \(w_{i}^{a}\) as the time when the user starts seeing the follower starting to execute the action (i.e., the action animation starts) (Line 12). We terminate instruction execution when the stop action STOP is sampled (Line 15).

At any point in time during the agent's instruction execution, the user may provide binary (positive or negative) feedback signals, creating a stream \((f_{j},w_{j}^{f})_{j=1}^{n}\) where \(w_{j}^{f}\) is the user's wall time when a feedback button is pressed (Line 13). We also allow the user to reboot the follower at any point during an instruction execution (omitted from Algorithm 1). This is meant to terminate very bad executions before they damage the interaction, for example by leading the follower too far astray. Rebooting adds a negative feedback signal to the end of the stream, and removes all queued instructions.7

The execution of an instruction by the agent and the feedback given by the user are combined together to create a trace made of two sequences \((o_{i},a_{i},w_{i}^{a})_{i=1}^{m}\) and \( f_{j},w_{j}^{f})_{j=1}^{n}\).

### Dataset Construction

We use all traces collected in round \(\) to construct a training dataset \(_{}\). Each example in \(_{}\) is a tuple \((,o,a,r)\), where \(\) is an instruction written by the user in round \(\), \(o\) is the agent observation when action \(a\) was selected during the execution \(\) in the interaction, and \(r\) is a numerical reward we compute from the feedback stream. For each instruction written during round \(\) we may create no examples if no feedback was given, or multiple examples if computing the rewards from feedback resulted in attributing rewards to multiple actions.

**Simple Reward** We consider the value of each positive feedback as \(f=+1\) and each negative feedback as \(f=-1\). We compute the reward \(r_{i}\) for an action \(a_{i}\) as the sign of sum of feedback signals that occur between it and the next action after correcting for human response delay time :

\[r_{i}=\{f_{j} w_{i}^{a}<w_{j}^{f}-d w_{i+1}^{a} \},\] (1)

where \(w_{i}^{a}\) is the wall time of action \(a_{i}\), \(w_{j}^{f}\) is the wall time of feedback signal \(f_{j}\), and \(d\) is a human response delay constant (\(d\) = 0.2 seconds). If the set is empty with no feedback signals within the time window, or if \(r_{i}\) = 0, we do not set a reward or create an example for the corresponding action.

**Heuristically Propagated Reward** As users are not required to follow any specific feedback schedule, the simple reward computation is likely to result in relatively sparse reward assignment, with many actions not receiving any learning signal and thus essentially thrown out as training data. Indeed, in our experiments, we observe that roughly only 63.1% of actions receive feedback. However, human feedback may not be about the most recent action only, but can potentially be attributed to multiple recent actions. For example, the user may not be certain if to discourage a behavior, such as turning and going in a certain direction, until the trajectory is clear after several steps.

We address this by taking inspiration from the use of eligibility traces for credit assignment , and create heuristics to propagate the reward to actions that otherwise receive no reward. If an action is assigned no reward, we propagate to it a reward from a later action by assigning to it the reward of the next action that is assigned one, if one exists within the next eight actions. The only exception is that we do not propagate reward from a STOP action that receives a negative reward.

We additionally prevent noisy feedback from being propagated by considering the scenario dynamics. We remove all training examples for actions following an action that both results in a card selection (or de-selection) and receives a negative reward. If an action with positive reward results in an invalid set (i.e., the selected cards do not match), we remove it from the training data and do not propagate its award. These scenario-specific heuristics were developed by identifying common feedback errors in the data. Following propagation, we observe that 82.2% of the actions have reward assigned to them, up from 63.1, without increasing the reward error rate of about 7.0%.8

### Parameter Optimization

The complexity of human feedback complicates reward attribution. We adopt a simple approach and maximize the immediate reward (i.e., a discount factor of zero).9 This creates a contextual bandit scenario. Because of the costs of human interaction and our aim to rapidly improve through few interactions, we emphasize simplicity and use a REINFORCE-style policy gradient objective . At the end of each round, we train from scratch using all the data observed so far \(_{^{}=0}^{}_{^{}}\). We process the initial human demonstration data \(_{0}\) to the same form as the reward data we get from user interactions, so we can use it seamlessly with the data we get from the rounds. For each action \(a\) and input instruction \(\) and observation \(o\) in \(_{0}\), we create an example \((,o,a,+1)\); we consider all demonstration data as gold-standard, and set the action reward to \(+1\).

During round \(\), the gradient for an example \((,o,a,r)\) from dataset \(_{^{}}^{}\), \(^{}\) is:

\[ =c(a,,o,^{})r(a ,o;)\] (2) \[c(a,,o,^{}) =(1,,o; )}{(a,o;_{^{}})} )&^{}>0\\ 1&^{}=0\]

where \(c()\) computes a clipped inverse propensity scoring (IPS) coefficient of the policy gradient , and \(_{^{}}\) are parameters with which the examples in \(_{^{}}\) were generated. IPS is commonly used for debiasing policies during off-policy learning. We also use IPS to avoid the problem of exploding gradients while training on negative examples (\(r<0\)) . We clip the value of the IPS coefficient to a maximum of 1 to avoid overfitting on positive examples .

## 4 Experimental Setup

**Initialization Data** Except when specified otherwise, the demonstration training dataset \(_{0}\) includes 8,790 instructions from 456 randomly-sampled human-human interactions from Suhr et al. . Pilots studies showed estimating \(_{1}\) using this data provides a good balance of human-human data amount and initial system performance for interaction with human users.

**Model** We implement the policy \(\) as a neural network with parameters \(\). The network design is based on the design of Suhr et al. , except that we account for partial observability. Roughly, the instruction \(\) and observation \(o\) are embedded independently. We use instruction-conditioned convolutions on the embedded observation to mix features from both inputs, and a modified LingUNet generates a distribution over actions. Appendix B provides formal model details.

**Deployment** In each round of deployment, we collect up to a fixed number of interactions per user. We instruct users to use the reboot button sparingly, and only when they anticipate the follower will make a mistake that cannot be easily corrected. We do not enforce any specific feedback patterns. Users are aware that their instructions and feedback will be used to train the agent for future rounds.

**Evaluation** The main measure of performance is the agent's instruction execution accuracy during interactions with human users. Because we have no human demonstration or annotation for this data, we perform post-hoc manual evaluation for each round. We randomly sample instructions from each round. We only sample instructions the agent completed (i.e., executed the STOP action) to annotate.

We assume all instructions rebooted by the user are failures, and adjust down the accuracy as estimated from the annotations according to the ratio of rebooted instructions. Appendix D formally defines the evaluation metric. We show the instruction, an animation of the agent's execution, and a list of cards that were reached by the follower during execution. We ask whether the follower correctly executed the instruction, and whether the follower made any number of errors in a pre-defined list. For performance measurements and statistics, we report the mean and confidence interval,10 except where noted. When emphasizing the relationship between the amount of training data and each measure, we plot the metric by the number of observed instructions rather than by round index.

**Crowdsourcing** We use MTurk for both user-agent interactions and post-hoc instruction evaluation. We recruit workers from English-majority locales using a qualification quiz and tutorial. Appendix E contains crowdsourcing management and compensation details. We maintain novice and expert groups of workers. Expert workers have higher compensation, and access to the post-hoc instruction accuracy tasks. To become an expert, workers must show they understand the task and annotation expectations during interactions in the novice pool. We qualify 108 workers, including 65 expert workers. During deployment, we balance the number of interactions between workers by limiting each worker to about eight interactions per agent and round.

## 5 Results and Analysis

We conduct two experiments: an 11-round experiment to observe long-term learning and user behavior trends, and a shorter 5-round experiment to compare several learning design decisions.

### Long-Term Experiment

We evaluate over eleven rounds of deployment and training. This experiment uses the heuristically propagated reward, with the goal of obtaining more training examples compared to the simple reward (Section 3.2). In total, we collect 3,368 games and 46,573 instructions, at a cost of $15,944.45 USD.

**Agent Accuracy** Figure 2 (left) shows instruction execution accuracy (estimated with 800 instructions per round) and number of points across the eleven rounds. Accuracy improves from 66.7 \(\) 1.5 in the initial round to 82.1 \(\) 1.1 in the final round. The continual learning process also results in increased

Figure 2: _Left_: Mean estimated instruction execution accuracy and game scores across 11 rounds. The \(x\)-axis shows the number of instructions observed. We mark with x the accuracies and scores from the post-hoc study of user adaptation for \(_{1}\) and \(_{11}\). Dotted lines illustrate the share of the improvement due to user adaptation. _Right_: Proportion of actions receiving positive and negative feedback from users over 11 rounds, and the ratio of frequency between positive and negative feedback. The x-axis shows the number of instructions observed. In both plots, dashed lines show round boundaries.

interaction success: the game score increases from 3.3 \(\) 0.2 to 5.3 \(\) 0.2 over time. Performance begins to plateau after the sixth round, which we hypothesize is due to model performance limitations. Appendix F provides additional evaluation on a static, held-out set of human demonstration data.

**Influence of User Adaptation on Performance** User adaptation over time is a confounding factor, as improvements in agent performance may be due to users adapting what tasks they delegate to the follower agent, or how they phrase their instructions. We conduct an additional deployment with both the initial agent (with parameters \(_{1}\)) and the last deployed agent (\(_{11}\)). The agents are deployed concurrently in a randomized experiment. Users do not know which agent they are collaborating with. We collect 571 interactions, with a total of 7,784 instructions. Points on the righthand side of Figure 2 (left) show the performance of both models during this deployment. Execution accuracy (estimated with 400 instructions) for the initial model \(_{1}\) improves from an average of 66.7 to 70.4 from user adaptation alone, while the final agent achieves an accuracy rate of 83.6 during this side-by-side comparison. This demonstrates that while roughly 20% of the improvement is due to user adaptation, genuine agent improvement through continual learning is extremely effective.

**User Perception of Agent Behavior** User perception of the agent improves over time. In-game feedback (Figure 2, right) improves: the rate of positive feedback per action increases from 44.9 to 60.3% and the negative feedback rate decreases from 10.1 to 4.7%. The reboot rate per instruction also decreases from 15.2 to 8.7%. After each interaction, we ask the leader for their agreement with several statements. Figure 3 (left) shows the distribution of these post-interaction Likert ratings, which improve significantly over the 11 rounds. For example, the rate of agreement with _The follower completed all the tasks I instructed them to do_ increases from 27.7 to 59.8% of interactions.

**Error Analysis** Figure 3 (right) shows trends of seven agent error types across rounds in the long-term experiment. Error types are annotated by workers as part of accuracy estimation. All but one error type decreases significantly over time; the rate of inefficient executions remains stable (Error 6). Manual analysis shows that more than a third of remaining missed-card errors (Error 1) could be attributed to poorly-written instructions, and most extra-card errors (Error 2) occur when the incorrect card shares at least one property with the target, or is closer to the starting position than the target.

Figure 3: _Left_: Distribution over interactions of post-hoc user agreement with three statements about agent performance. _Right_: Instruction execution errors during 11 rounds for seven common categories. Center columns show prevalence of each error type as a proportion of analyzed instructions, for the initial (\(=1\)) and final (\(=11\)) agents; the rightmost column shows trend in prevalence over time.

**Language Change** Users are likely to change their language over time, as observed in human-human interactions, including in CerealBar. In our study, instruction length remains relatively stable over time at an average of 8.6 tokens per utterance. This contrasts with existing findings in reference games, where convention formation facilitates reduction in utterance length over time [24; 9], and shows that interaction dynamics and incentives shape the dynamics of language change. We observe changes in instruction content over time, using heuristics to estimate the number of target cards specified in each instruction. There is a decrease in the rate of instructions that specify no cards to interact with (i.e., cards to select or deselect) from 12.0 in the first round to 7.7% in the last. The rate of instructions specifying a single card increases from 81.5 to 88.7%, while the rate of instructions specifying multiple cards decreases from 6.5 to 3.7%. This is potentially because no-card instructions are relatively inefficient, while multi-card instructions are relatively hard and have a higher risk of cascading errors that require corrections (i.e., deselecting a wrongly selected card). We also find that the rate of instructions containing a reference to an object (e.g., a tree, a pond) decreases over time from 17.9 to 13.5% of instructions. We find that users increasingly shift labor to the agent over time: the average number of steps per set taken by the follower increased from 14.8 to 15.3, while steps taken by the leader decreased from 10.0 to 8.9 steps. These changes illustrate the impact of users developing game strategies and, potentially, a model of the follower's instruction following ability.

### Comparison of Learning Design Choices

We conduct a second deployment experiment to study the impact of the amount of initial demonstration data, negative feedback, and reward propagation heuristics. We also compare the feedback learning signal to supervised demonstration data. We design and deploy five system variations:

**RewardProp**: uses the exactly same process as in the long-term experiment (Section 5.1), including reward propagation (Section 3.2).
**SimpleReward**: measures the effect densifying and de-noising user feedback with heuristics by using only the simple reward (Section 3.2).
**NoNegative**: simulates using positive feedback only. We discard all negative feedback, including reboots, from the data, and then apply the reward propagation heuristics (Section 3.2).11

**FewerDemo**: uses only 144 human-human interactions (2,114 instructions) for initialization, about 25% of the supervised data used in RewardProp.
**SupOnly**: compares training with human demonstrations vs. feedback data given the same number of interactions. SupOnly does not use data from human-agent interactions, though we deploy it for evaluation. Instead, after each round, we add a fixed number of additional human-human interactions to the training set, equivalent to the amount of data collected with the other variants. Due to the limited amount of demonstration data, we only deploy this approach for three rounds.

We deploy the five variations for five rounds. In each round, all variations are deployed concurrently, each for 200 user interactions. To ensure that each worker interacts with all agents and to reduce ordering bias, we assign each worker a unique, consistent, and random ordering of agents; as they play multiple games, they cycle through agents in this order. Because all variants except FewerDemo are pre-trained on the same initial data, in the first round only, we deploy two agents: one agent for RewardProp, SimpleReward, NoNegative, and SupOnly, and another for FewerDemo. After the first round, we deploy five agents, one for each variation. This experiment cost $16,769.46.

Figure 4 shows results for the five agent variants over five rounds. Overall, learning from feedback is robust to the different design choices. Reward propagation (RewardProp) has minor benefit relative to the simple reward (SimpleReward). RewardProp shows slightly faster learning, achieving significantly higher accuracy (80.3 \(\) 1.0) than SimpleReward (78.1 \(\) 1.0) in Round 3, which means SimpleReward gives a poorer user experience early on, though it catches up quickly.12

We observe that negative feedback is important: compared to RewardProp and SimpleReward, NoNegative learns slower, receives less positive feedback, and more negative feedback and reboots.

This difference does not show in early rounds, but appearing around round three. It particularly stands out in round four, when it shows an accuracy of 76.8 \(\) 1.0, compared to 81.0 \(\) 0.9 of RewardProp.

The experiment illustrates that learning can start with a much weaker agent. FewerDemo uses roughly a quarter of the demonstration data available to the other variants, and indeed starts with much lower performance, 34.8 \(\) 1.8 accuracy, compared to 70.6 \(\) 1.1 for the others. Even this weak model is sufficient for effective learning: it leads to a much steeper learning curve, improving to 64.2 \(\) 1.5 accuracy after five rounds. However, this comes at cost to the initial user experience: in the first round, only 2% of interactions with FewerDemo were rated positively by users.

Finally, we observe the feedback data is roughly equivalent to supervised data as a learning signal, showing similar accuracy and feedback trends. RewardProp achieves equivalent performance to SupOnly with less than half the amount of demonstration data (i.e., as used for initialization). This shows the benefits of training through user-agent interaction: not only do learning signals directly target current agent behavior, but the training process is significantly less expensive, requiring no human-human demonstrations, but instead just user-system interactions.

## 6 Related Work

Learning for instruction following commonly relies on various levels of supervision, such as gold-standard demonstrations (e.g., 7; 43; 32; 1; 5; 6; 41; 8; 38), or goal annotations (e.g., 3; 33; 40). We collapse the distinction between learning and deployment, shifting training and evaluation into human-agent interactions, relying only on a small amount of demonstrations data to initialize.

There has been limited work on continual learning for language-related tasks. A major challenge studying this problem is designing a scenario that allows for scalable non-expert feedback and iteratively improving a model and deploying it. CerealBar is unique in supporting real-time interaction with situated natural language between agents and human users. Alternatively, continual learning has been studied via synthetic deployment by simulating feedback using supervised datasets (e.g., 34; 29; 12). We focus on studies with real human instructors, which create dramatically different dynamics and data cost constraints. For example, this line of work usually uses significant amount of supervised data, while we emphasize rapid learning from limited interactions. Kojima et al. (22) studied the problem of continual learning for instruction generation by observing human following behavior, including deployment in an iterative study. Similar to us, they used CerealBar because of

Figure 4: Results for five agent variants over five rounds. _Left_: mean estimated instruction execution accuracy. _Right_: rate of actions with positive (\(\)) and negative (\(\)) feedback, and instruction reboot rate (\(\)). Across five rounds, all variants exhibit increased execution accuracy, decreased reboot rates, and increasingly positive (and decreasingly negative) action-level feedback.

its ability for scalable deployment and real-time model-human interaction. Beyond this environment choice, our work differs significantly: we study human feedback and the task of instruction following.

Human feedback through annotation or selection of intended output has been used for semantic parsing [45; 18]. This is related to soliciting partial annotations [2; 44; 48] or post-interaction feedback  in dialogue. Recent work has also employed preference-based user judgments for reward model learning to fine-tune language models using RL [RLHF; 26; 49; 39; 35]. In contrast to this existing work, we focus on sequential execution of instructions in embodied interactions, using realtime feedback provided by human users to continually train our language-understanding agent. While work in this line showed improving a learned model once, it did not study iteratively improving and deploying a model multiple times, a core focus of our work.

Continual learning has been studied more extensively outside of a language context, e.g., for robotics. Our work is inspired by TAMER [20; 21; 46] and COACH [31; 4], where humans provide binary feedback to an agent acting in a world. While these methods were developed in the context of an agent learning a single task, we focus on generalization to previously unseen natural language instructions.

## 7 Discussion

We propose an approach for continually learning to follow instructions through interaction with human users that provide realtime feedback. We demonstrate its effectiveness through multiple rounds of training and deployment, including thousands of interactions. Experiments with various learning design decisions show our approach is robust, and can start from a weak initial agent.

Our work sets the stage for several avenues of further study. First, future work could reconsider a number of our design choices, including the simple contextual bandit objective and the neural architecture. Application of a user simulator  could further improve performance; however, this approach fails to capture dynamics that arise in real interactions through user adaptation. An important direction for future work is incorporating additional observational interaction signals (e.g., by observing the instructor behavior) [10; 22]. Such orthogonal signals can accelerate learning. Another promising avenue for research is more expressive feedback mechanisms, such as through natural language and even complete bi-directional dialogue. Although reasoning about such signals significantly complicates both reasoning and learning, they are also much more informative. A potential direction is to use simple feedback signals like ours to bootstrap the language capabilities of the system, and then gradually switch to more complex natural language feedback.