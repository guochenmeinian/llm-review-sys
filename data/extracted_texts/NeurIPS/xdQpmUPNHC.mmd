# Efficient Symbolic Policy Learning with Differentiable Symbolic Expression

Jiaming Guo1  Rui Zhang1  Shaohui Peng2  Qi Yi1,3,4  Xing Hu1,5

**Ruizhi Chen2  Zidong Du1,5  Xishan Zhang1,4  Ling Li2,6  Qi Guo1  Yunji Chen1,6 1**

Corresponding Author.1SKL of Processors, Institute of Computing Technology, CAS, Beijing, China

2Intelligent Software Research Center, Institute of Software, CAS, Beijing, China

3University of Science and Technology of China, USTC, Hefei, China

4Cambricon Technologies

5Shanghai Innovation Center for Processor Technologies, SHIC, Shanghai, China

6University of Chinese Academy of Sciences, UCAS, Beijing, China

{guojiaming, zhangrui}@ict.ac.cn, pengshaohui@iscas.ac.cn, yiqi@mail.ustc.edu.cn

huxing@ict.ac.cn, ruizhi@iscas.ac.cn {duzidong,zhangxishan}@ict.ac.cn

liling@iscas.ac.cn, {guoqi,cyj}@ict.ac.cn

###### Abstract

Deep reinforcement learning (DRL) has led to a wide range of advances in sequential decision-making tasks. However, the complexity of neural network policies makes it difficult to understand and deploy with limited computational resources. Currently, employing compact symbolic expressions as symbolic policies is a promising strategy to obtain simple and interpretable policies. Previous symbolic policy methods usually involve complex training processes and pre-trained neural network policies, which are inefficient and limit the application of symbolic policies. In this paper, we propose an efficient gradient-based learning method named Efficient Symbolic Policy Learning (ESPL) that learns the symbolic policy from scratch in an end-to-end way. We introduce a symbolic network as the search space and employ a path selector to find the compact symbolic policy. By doing so we represent the policy with a differentiable symbolic expression and train it in an off-policy manner which further improves the efficiency. In addition, in contrast with previous symbolic policies which only work in single-task RL because of complexity, we expand ESPL on meta-RL to generate symbolic policies for unseen tasks. Experimentally, we show that our approach generates symbolic policies with higher performance and greatly improves data efficiency for single-task RL. In meta-RL, we demonstrate that compared with neural network policies the proposed symbolic policy achieves higher performance and efficiency and shows the potential to be interpretable.

## 1 Introduction

With the development of deep neural networks as general-purpose function approximators, deep reinforcement learning (DRL) has achieved impressive results in solving sequential decision-making tasks [1; 2]. In DRL, the policies are commonly implemented as deep neural networks which involve tremendous parameters and thousands of nested non-linear operators. Despite the excellent representation ability, the neural network (NN) is very complex, making it difficult to understand, predict the behavior and deploy with limited computational resources.

With the high academic and industrial interest in interpretable and simple RL policy, some works [3; 4; 5] propose to learn the symbolic policy which is a symbolic expression composing variables, constants, and various mathematical operators. The symbolic policy has a succinct form and low complexity which is considered to be more interpretable and easily deployable in real-world settings.  and  approximate a symbolic policy with genetic programming but are limited to simple tasks with provided or learned world model and suffered from performance decrease compared with NN policies. DSP , the state-of-the-art symbolic policy learning method, removes some limitations by employing a recurrent neural network(RNN) as an agent to generate the symbolic policy and trains the RNN with reinforcement learning. However, this method has low data efficiency and requires hundreds of times of environment interactions compared with traditional reinforcement learning algorithms. For environments with multidimensional action spaces, they need a pre-trained NN policy as the anchor model, which brings additional complexity and may limit the final performance. In addition, the complexity of these algorithms makes it difficult to apply them to complex reinforcement learning tasks, e.g. meta-reinforcement learning.

In this paper, we propose an efficient gradient-based learning method called ESPL (Efficient Symbolic Policy Learning) for learning the symbolic policy from scratch in an end-to-end differentiable way. To express the policy in a symbolic form, the proposed ESPL consists of a symbolic network and a path selector. The symbolic network can be considered as a full set of candidate symbolic policies. In the symbolic network, the activation functions are composed of various symbolic operators and the parameters can be regarded as the constants in the symbolic expression. The path selector chooses the proper compact symbolic form from the symbolic network by adaptively masking out irrelevant connections. We design all these modules to be differentiable and represent the policy with a differentiable symbolic expression. Then we can efficiently train the symbolic policy in an off-policy manner and the symbolic policy is directly updated via gradient descent without an additional agent. Experimentally, on several benchmark control tasks, our algorithm is able to produce well-performing symbolic policy while requiring thousands of times fewer environmental interactions than DSP.

Meta-reinforcement learning (meta-RL) is one of the most important techniques for RL applications, which improves the generalization ability on unseen tasks by learning the shared internal structure across several tasks. We raise the question: is it possible to exploit the benefit of the symbolic policy and meta-RL to generate symbolic policies for unseen tasks? While previous symbolic policy methods are too complex to be combined with meta-RL, we combine the proposed ESPL with context-based meta-RL and develop the contextual symbolic policy (CSP). Context-based meta-RL [6; 7; 8; 9] is the most promising meta-RL method which forces the policy to be conditional on context variables that are formed by aggregating experiences. In the CSP framework, the path selector decides the symbolic form based on the context variables. We also involve a parameter generator to generate the constants of the symbolic policy based on the context variables. We build the training process on top of the context-based meta-RL method PEARL . The proposed CSP can generate symbolic policies for unseen tasks given a few trajectories. We find that compared with neural network policies, contextual policies produced by CSP achieve higher generalization performance, efficiency, and show the potential to be interpretable.

The contributions of this paper are three-fold. First, we introduce a novel gradient-based symbolic policy learning algorithm named ESPL that learns the symbolic policy efficiently from scratch. Next, with ESPL we develop the contextual symbolic policy for meta-RL, which can produce symbolic policies for unseen tasks. Finally, we summarize our empirical results which demonstrate the gain of ESPL both in single-task RL and meta-RL. Importantly, we find empirically that contextual symbolic policy improves the generalization performance in PEARL.

## 2 Related Works

### Symbolic Policy

The emergence of symbolic policy is partly credited to the development of symbolic regression which is applicable in wide fields, e.g. discovering physics lows  and automated CPU design . Symbolic regression aims to find symbolic expressions to best fit the dataset from an unknown fixed function. A series of methods [12; 13; 14] employ genetic programming (GP) to evolve the symbolic expressions. With the development of neural network and gradient descent, some methods [15; 16; 17] involve deep learning for symbolic regression. Some works employ symbolic regression methods to obtain symbolic policies for efficiency and interpretability.  and  aim to approximate a symbolic policy with genetic programming but require a given dynamics equations or a learned world model. DSP , following the symbolic regression method DSR , employs a recurrent neural network to generate the symbolic policy. They use the average returns of the symbolic policies as the reward signal and train the neural network with risk-seeking policy gradients. However, for environments with multidimensional action spaces, they need a pre-trained neural network policy as the anchor model. Besides, in this framework, a single reward for reinforcement learning involves many environmental interactions, which is inefficient and makes it hard to combine the symbolic policy with meta-RL. Recently, some works [18; 19] attempt to distill an interpretable policy from a pre-trained neural network policy but have a problem of objective mismatch . Different from the above-mentioned methods, we propose an efficient gradient-based framework to obtain the symbolic policy without any pre-trained model.

### Meta-Reinforcement Learning

Meta-RL extends the notion of meta-learning [20; 21; 22] to the context of reinforcement learning. Some works [23; 24; 25] aim to meta-learn the update rule for reinforcement learning. We here consider another research line of works that meta-train a policy that can be adapted efficiently to a new task. Several works [26; 27; 28] learn an initialization and adapt the parameters with policy gradient methods. However, these methods are inefficient because of the on-policy learning process and the gradient-based updating during adaptation. Recently, context-based meta-RL [6; 7; 29] achieve higher efficiency and performance. For example, PEARL  proposes an off-policy meta-RL method that infers probabilistic context variables with experiences from new environments. Hyper  proposes a hypernetwork where the primary network determines the weights of a conditional network and achieves higher performance. Most of the subsequent context-based meta-RL methods [30; 31; 32] attempt to achieve higher performance by improving the context encoder or the exploration strategy. In this paper, we combine the symbolic policy with meta-RL to form the CSP and consequently improve the efficiency, interpretability and performance of meta-RL. As far as we know, we are the first to learn the symbolic policy for meta-RL.

Our method is also related to some neural architecture search methods and programmatic RL methods. We provide an extended literature review in Appendix E.

## 3 Gradient-based Symbolic Policy Learning

This section introduces the structure of the proposed ESPL, an end-to-end differentiable system. The proposed ESPL consists of two main components: 1) the Symbolic Network, which expresses the policy in a symbolic form, and 2) the Path Selector, which selects paths from the symbolic network to form compact symbolic expressions.

Figure 1: Example network structures for the symbolic network. **Left**: the plain structure. **Middle**: a symbolic work with dense connections. **Right**: a symbolic network with dense connections and arranged operators.

### Densely Connected Symbolic Network

To construct a symbolic policy in an end-to-end differentiable form, we propose the densely connected symbolic network \(\) as the search space for symbolic policies. Inspired by previous differentiable symbolic regression methods [17; 33], we employ a neural network with specifically designed units, which is named symbolic network. We now introduce the basic symbolic network named plain structure which is illustrated in Figure 1. The symbolic network is a feed-forward network with \(L\) layers. Different from traditional neural networks, the activation functions of the symbolic network are replaced by symbolic operators, e.g. trigonometric and exponential functions. For the \(l_{th}\) layer of the symbolic network, we denote the input as \(x_{l-1}\) and the parameters as weights \(w_{l}\) and biases \(b_{l}\). These parameters serve as the constants in a symbolic expression. We assume that the \(l_{th}\) layer contains \(m\) unary functions \(\{g_{1}^{1},,g_{m}^{1}\}\) and \(n\) binary functions \(\{g_{1}^{2},,g_{n}^{2}\}\). Firstly, the input of the \(l_{th}\) layer will be linearly transformed by a fully-connected layer:

\[y_{l}=F_{l}(x_{l-1})=w_{l}x_{l-1}+b_{l}.\] (1)

The fully-connected layer realizes the addition and subtraction in symbolic expressions and produces \(m+2n\) outputs. Then the outputs will go through the symbolic operators and be concatenated to form the layer output:

\[G_{l}(y_{l})=[g_{1}^{1}(y_{l}^{1}),,g_{m}^{1}(y_{l}^{m}),g_{1}^{2}(y_{l} ^{m+1},y_{l}^{m+2}),,g_{n}^{2}(y_{l}^{m+2n-1},y_{l}^{m+2n})]\] (2)

Then the \(l_{th}\) layer of the symbolic network can be formulated as \(_{l}:x_{l}=G_{l}(F_{l}(x_{l-1}))\). Following the last layer, a fully-connected layer will produce a single output. For multiple action dimensions, we construct a symbolic network for each dimension of action.

**Symbolic operator.** The symbolic operators are selected from a library, e.g. \(\{sin,cos,exp,log,,\}\) for continuous control tasks. For the plain structure, we include an identical operator which retains the output of the previous layer to the next layer in the library. To find the symbolic policy via gradient descent, it is critical to ensure the numerical stability of the system. However, this is not natural in a symbolic network. For example, the division operator and the logarithmic operator will create a pole when the input goes to zero and the exponential function may produce a large output. Thus, we regularize the operators and employ a penalty term to keep the input from the "forbidden" area. For example, the logarithmic operator \(y=log(x)\) returns \(log(x)\) for \(x>bound_{log}\) and \(log(bound_{log})\) otherwise and the penalty term is defined as \(_{log}=max(bound_{log}-x,0)\). The division operator \(c=a/b\) returns \(a/b\) for \(b>bound_{div}\) and 0 otherwise. The penalty term is defined as \(_{div}=max(bound_{div}-b,0)\). The details of all regularized operators can be found in the Appendix. To ensure the numerical stability, we involve a penalty loss function \(_{penalty}\) which is the sum of the penalty terms of all \(N\) regularized operators in symbolic networks:

\[_{penalty}=_{i=1}^{i=N}_{g_{i}}(x_{i}).\] (3)

**Dense connectivity.** We introduce dense connections  in the symbolic network, where inputs of each layer are connected to all subsequent layers. Consequently, the \(l_{th}\) layer of the symbolic network will receive the environment state \(s\) and the output of all preceding layers \(x_{1},,x_{l-1}\): \(x_{l}=G_{l}(F_{l}([s,x_{1},,x_{l-1}]))\). The dense connections improve the information flow between layers and benefit the training procedure. Besides, with these dense skip connections across layers, the combination of symbolic operators becomes more flexible, making the symbolic network more likely to contain good symbolic policies. In addition, we can flexibly arrange the position of operators. For example, if we only arrange the \(sin\) operator in the last layer but the oracle expression contains terms like \(sin(s_{0})\), the input of the \(sin\) operator can still be from the original state because of the dense connections. We give an example of arranged operators in Figure 1 which we use for all tasks in the experiments. In this symbolic network, we heuristically involve more multiplication and division operators at shallow layers to provide more choice of input processed by simple operators for complex operations such as sines and cosines.

### The Path Selector

The symbolic network serves as a full set of the search space of symbolic expressions. To select the proper paths from the symbolic network to produce a compact symbolic policy, we reduce thenumber of paths involved in the final symbolic policy then proper paths remain and redundant paths are removed. This can be naturally realized by minimizing the \(L_{0}\) norm of the symbolic network weights. As the \(L_{0}\) norm is not differentiable, some methods [17; 33] employ \(L_{1}\) norm instead of \(L_{0}\) norm. However, \(L_{1}\) will penalize the magnitude of the weights and result in performance degradation. Inspired by the probability-based sparsification method [35; 36; 37], we propose a probabilistic path selector which selects paths from the network by multiplying a binary mask on the weights of the symbolic network \(\). The binary mask \(m_{i}\) is sampled from the Bernoulli distribution: \(m_{i} Bern(p_{i})\), where \(p_{i}\) serves as the probability. Then the final weights of the symbolic network are \(}=\), where \(\) is the element-wise multiply operation. Consequently, to get a compact symbolic expression, we only need to minimize the expectation of the \(L_{0}\) norm of the binary mask \(_{ Bern(|)}\|\|_{0}= p_{i}\), without penalizing the magnitude of the weights. During the process of collecting data or testing, we can directly sample the binary mask from the Bernoulli distribution. Then we can obtain the symbolic policy \(_{sym}\) by removing paths with zero weight and simplifying the symbolic expression.

However, the sampling process does not have a well-defined gradient. Thus, for the training process we build up our sampling function with the gumbel-softmax trick . As the mask \(\) is binary categorical variables, we replace the _softmax_ with _sigmoid_ and named the sampling function as _gumbel sigmoid_. The _gumbel sigmoid_ function can be formulated as:

\[}=sigmoid(}{1-})+}-}} {}),\] (4)

where \(}\) and \(}\) are i.i.d samples drawn from \(Gumbel(0,1)\). \(\) is the temperature annealing parameter. Note that \(}\) is still not a binary mask. To obtain a binary mask but maintain the gradient, we employ the Straight-Through (ST) trick: \(=_{ 0.5}(})+}-}}\), where \(_{ 0.5}(x)\{0,1\}^{n}\) is the indicator function and the overline means stopping the gradient. During training, we do not remove paths with zero weight and directly use symbolic network \((})\) as the policy.

We also involve a loss function \(_{select}\) to regularize the sum of probabilities \(\) which is the expectation of the \(L_{0}\) norm of the binary mask \(\). To limit the minimum complexity of symbolic policies, we involve the minimum \(L_{0}\) norm defined as \(l_{min}\). Then the loss function can be defined as:

\[_{select}=max( p_{i}-l_{min},0).\] (5)

### Implementation

In practice, we build our off-policy learning framework on top of the soft actor-critic algorithm (SAC) . We employ the neural network \(Q(s,a)\) parameterized by \(_{Q}\) as the critic (state-action-value function). To construct a stochastic policy, we also employ a small neural network \(F(s)\) parameterized by \(_{F}\) to output the standard deviation. Note that \(Q(s,a)\) and \(F(s)\) are only used during training. We optimize the weights \(\), the biases \(\) of the symbolic network, the probabilities \(\) in the path selector, and \(_{F}\) with the combination of actor loss from SAC, \(_{penalty}\) and \(_{select}\). We update \(_{Q}\) with the critic loss from SAC. During training, we decrease the temperature parameter \(\) of _gumbel sigmoid_ linearly and decrease the \(l_{min}\) from the count of the original parameters \(\) to a target value with a parabolic function. We summarize the training procedure and give the pseudo-code in Appendix C.

## 4 Contextual Symbolic Policy for Meta-RL

### Background

In the field of meta-reinforcement learning (meta-RL), we consider a distribution of tasks \(p()\) with each task \( p()\) modeled as a Markov Decision Process(MDP). In common meta-RL settings, tasks share similar structures but differ in the transition and/or reward function. Thus, we can describe a task \(\) with the 6-tuple (\(,,_{},_{0},r_{},\)). In this setting, \(^{n}\) is a set of n-dimensional states, \(^{m}\) is a set of m-dimensional actions, \(_{}:} \) is the state transition probability distribution, \(_{0}:\) is the distribution over initial states, \(r_{}:\) is the reward function, and \((0,1)\) is the per timestep discount factor. Following the setting of prior works [7; 8], we assume there are \(M\) meta-training tasks \(\{_{m}\}_{m=1,,M}\) sampled from the training tasks distribution \(p_{train}()\). For meta-testing, the tasks are sampled from the test tasks distribution \(p_{test}()\). The two distributions are usually the same in most settings but can be different in out-of-distribution(OOD) settings. We denote context \(c_{T}=\{(s_{1},a_{1},s^{}_{1},r_{1}),,(s_{T},a_{T},s^{}_{T},r_ {T})\}\) as the collected experiences. For context-based meta-RL, the agent encodes the context into a latent context variable \(z\) with a context encoder \(q(z|c)\) and the policy \(\) is conditioned on the current state and the context variable \(z\). During adaptation, the agent first collects experiences for a few episodes and then updates the context variables. Then the contextual policy is able to adapt to new tasks according to the context variables. The meta-RL objective can be formulated as \(}_{ p()}[ }_{c_{T}}[R(,,q(z|c_{T}))]]\), where \(R(,,q(z|c_{T}))\) denotes the expected episode return.

### Incorporating the Context Variables

To quickly adapt to new tasks, we need to incorporate the context variables \(z q(z|c_{})\) to the symbolic network and produce different symbolic policies for different tasks \(\) sampled from the task distribution \(p()\). To condition the parameters of the symbolic expression on the context variable, we propose a parameter generator: \(,=(z)\) which is a neural network to produce the parameters of symbolic networks for all action dimensions based on the context variables. We also involve a neural network to generate the probabilities of the path selector: \(=(z)\). Then the contextual symbolic network can generate different symbolic expression forms according to the context variables.

  
**Environment** & **ESPL** \\  CartPole & \(a_{1}=17.17s_{3}+1.2s_{4}\) \\  MountainCar & \(a_{1}=8.06sin(9.73s_{2}-0.18)+1.26\) \\  Pendulum & \(a_{1}=-(4.27s_{1}+0.62)(1.9s_{2}+0.42s_{3})\) \\  InvDoublePend \(a_{1}=12.39s_{5}-4.48sin(0.35s_{2}+4.51s_{5}+1.23s_{6}+7.97s_{8}+1.23s_{9}+0. 08)+0.34\) \\  InvPendSwingup \(a_{1}=4.33sin(0.17s*s_{1}+0.14s_{2}+0.49s_{3}+1.76s_{4}+0.33s_{4}-0.29)-0.65\) \\   & \(a_{1}=(0.14-2.57s_{4})(0.48-0.68log(0.5s_{2}))-1.44\) \\  & \(a_{2}=-5.72s_{3}+4.42sin(2.54s_{5}+0.03)-0.4--2.13cos(0.78sin 1.15s_{1}-(1.05)+0.51s_{0})-0.98}{4717s_{7}+0.77}\) \\   & \(a_{1}=-0.32s_{12}-1.46s_{8}-0.83s_{10}-0.11sin(0.26s_{11}-5s_{13}-2.57s_{5}+0. 38)-0.92\) \\  & \(a_{2}=-0.52s_{12}-3.63s_{4}-4.58s_{8}+0.68exp(-7.31s_{11}-2.51s_{13})+0.58+ +3.89s_{4}-4.7}{1.33-4.45s_{4}}\) \\  & \(a_{3}=0.83+-0.47-0.1exp(1.05s_{1}-1.76s_{11}+1.65)(2s_{21}-1.88 s_{14}+1.32)(5.59s_{4}-0.08)}{2.93420s_{1}(0.05s_{1}-1.76s_{11}+1.48)(2.52s_{12}-1.8 s_{14}+1.32)(5.59s_{4}+0.8)}\) \\   & \(a_{1}=1.45-2.94cos(-0.73s_{5}+(0.106s_{3})-(1.38s_{2}-0.28s_{6}+0.41)+1.32)\) \\  & \(a_{2}=7.53exp(0.4s_{1}-0.13s_{6}-0.52sin(1.5s_{7}-0.24))-11.1\) \\  & \(a_{3}=-+0.41}{4.56s_{9}+(0.2-0.31s_{12}^{2}-1.8s_{14}-0.01s_{1} ^{2}-0.54)+0.55}{3.65s_{9}+(0.2-0.31s_{12}^{2}-1.8s_{14}^{2}+0.29s_{2}-2.66} +0.55\) \\  & \(a_{4}=-0.28++5.61s_{4}+0.29s_{2}s_{2}-2.66}{3.26s_{1}-4.5}\) \\   

Table 1: Symbolic policies produced by ESPL.

  
**Environment** & **DDPG** & **TRPO** & **A2C** & **PPO** & **ACKTR** & **SAC** & **TD3** & **Regression** & **DSP** & **ESPL** \\  Cartpole & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 211.82 & 1000 & 1000 \\ Mountaincat & 95.36 & 93.6 & 93.97 & 93.76 & 93.79 & 94.68 & 93.87 & 95.16 & 99.11 & 94.02 \\ Pendulum & -155.6 & -145.49 & -157.59 & -160.14 & -201.57 & -154.82 & -155.06 & -1206.9 & -155.4 & -151.72 \\ InvDoublePend & 9347.1 & 9188.43 & 9359.81 & 9356.59 & 9359.06 & 9359.92 & 9359.52 & 637.2 & 9149.9 & 9359.9 \\ InvPendSwingup & 8914.88 & 892.9 & 254.71 & 890.11 & 890.12 & 891.32 & 892.25 & -19.21 & 891.9 & 890.36 \\ LunarLander & 266.05 & 265.26 & 238.51 & 269.65 & 271.53 & 276.92 & 272.13 & 56.08 & 261.36 & 283.56 \\ Hopper & 1678.84 & 2593.56 & 2104.98 & 2586.56 & 2583.88 & 2613.6 & 2743.9 & 47.35 & 2122.4 & 2442.48 \\ BipedalWalker & 209.42 & 312.14 & 291.79 & 287.43 & 309.57 & 308.31 & 314.24 & -110.77 & 311.78 & 309.43 \\  Worst Rank & 9 & 10 & 9 & 9 & **6** & 7 & 10 & 9 & **6** \\ Average Rank & 5.5 & 4.125 & 6.25 & 6.125 & 5.375 & 3 & **2.875** & 9.125 & 4.625 & 3.5 \\   

Table 2: Performance comparison of symbolic policies and neural policies for seven different DRL algorithms.

### Training Schedule

We train the CSP in an off-policy manner. For meta-training epoch \(t\), the agent first collects experiences of different training tasks into the corresponding buffer \(_{_{i}}\) for several iterations. At the beginning of each data collection iteration, we sample context \(c_{T}\) from buffer \(_{_{i}}\) and sample context variables \(z q(z|c_{T})\) as PEARL  does. The difference is that we also sample the symbolic policy with \((z)\) and \((z)\) and use the sampled policy for the following steps of the iteration. For training, we sample RL batch and context from the buffer and optimize the context encoder \(q(z|c_{T})\) to recover the state-action value function. For each training step, we sample a new symbolic policy. We employ the soft actor-critic to optimize the state-action value function. For the parameter generator and the path selector, we employ \(_{select}\) and \(_{penalty}\) in addition to the SAC loss. During training, we decrease the temperature parameter \(\) and \(l_{min}\) just like single-task RL. More details and the pseudo-code can be found in Appendix C.

## 5 Experiment

### Experimental Settings

**Environment.** For single-task RL, we evaluated our method on benchmark control tasks which are presented in DSP: (1) CartPole; (2) MountainCar; (3) Pendulum; (4) InvertedDoublePendulum; (5) InvertedPendulumSwingup; (6) LunarLander; (7) Hopper; (8) BipedalWalker. For meta-RL, we evaluate the CSP on several continuous control environments which are modified from the environments of OpenAI Gym  to be meta-RL tasks similar to [7; 9; 30]. These environments require the agent to adapt across dynamics (random system parameters for Hopper-params, Walker2d-params, Lunarlander-params, InvDoublePend-params, different force magnitude and pole length for Cartpole-fl-ood) or reward functions (target velocity for Cheetah-vel-ood).

**Methods.** In the single-task RL experiments, for the neural network policies, we compare our method with seven state-of-the-art DRL algorithms: DDPG, TRPO, A2C, PPO, ACKTR, SAC, and TD3 [41; 42; 43; 44; 45; 39; 46]. The results are obtained with the tuned pretrained policies from an open-source repository Zoo . For symbolic policies, we include the Regression method and DSP. The Regression policies are produced with two steps: 1) generate a dataset of observation action trajectories from the best pre-trained policy from Zoo; 2) perform deep symbolic regression  on the dataset and select the expression with the lowest error for each action. DSP first trains a recurrent neural network with reinforcement learning to produce symbolic policies and then optimizes the constant with several methods such as Bayesian Optimization .

Figure 2: Comparison for different kinds of contextual policies on meta-RL tasks. We show the mean and standard deviation of returns on test tasks averaged over five runs.

**Evaluation.** Following DSP, we evaluate the proposed ESPL by averaging the episodic rewards across 1,000 episodes with different environment seeds. The evaluation for all the baselines is also in accordance with this protocol. For the DSP and regression method, we use the results from the DSP paper. DSP performs 3n independent training runs for environments with n-dimension action and select the best symbolic policy. For a fair comparison, we perform three independent runs and select the best policy for ESPL and DRL methods. For meta-RL, we run all environments based on the off-policy meta-learning framework proposed by PEARL  and use the same evaluation settings. We compare CSP with PEARL which concatenates the observation and context variables as the input of policy and Hyper  which generate the parameters of policy with a ResNet model based on the context variables. Note that the original Hyper also modifies the critic, but we build all the critics with the same network structure for consistency. More details of the experimental settings can be found in Appendix D.

### Comparisons for Single-task RL

**Performance.** In Table 2, we report the average episode rewards of different algorithms. Among symbolic policies, the proposed ESPL achieves higher or comparable performance compared with DSP while the regression method performs poorly in most environments. To directly compare ESPL with other algorithms across different environments, we rank the algorithms and calculate the average rank and worst-case rank. We find that TD3 and SAC outperform other algorithms. For symbolic policy methods, the proposed ESPL achieves superior performance compared to DSP. ESPL is also comparable with the better-performing algorithms of DRL.

**Data efficiency.** In Table 3, we report the number of environment episodes required for learning the symbolic policy in different algorithms. The proposed ESPL requires fewer environmental interactions to learn the symbolic policies in most environments. Compared with DSP, the proposed ESPL uses thousands of times fewer environment episodes.2 Although the regression method requires a similar number of environment episodes as ESPL, it fails to find meaningful policies in most environments according to Table 2. Besides, the proposed ESPL is trained from scratch while the regression method and DSP need pretrained neural network policies. In conclusion, the proposed ESPL greatly improves the data efficiency of symbolic policy learning.

### Comparisons for Meta-RL

**Performance.** For meta-RL tasks, we report the learning curves of undiscounted returns on the test tasks in Figure 2. We find that CSP achieves better or comparable performance in all the environments compared with previous methods. In Hopper-params, Walker2d-params and InvDoublePend-params, CSP outperforms PEARL and Hyper during the whole training process. In Lunarlander-params, CSP achieves better final results. In Cartpole-fl-ood, CSP adapts to the optimal more quickly. In the out-of-distribution task Cheetah-vel-ood, we find the performance of PEARL and Hyper decrease during training because of over-fitting. But our CSP is less affected. Thus, expressing the policy in the symbolic form helps improve the generalization performance.

**Deploying efficiency.** We also evaluate the deploying efficiency of contextual policies. We first calculate the flops of each kind of policy per inference step. Then we consider an application scenario

  
**Environment** & **Regression** & **DSP** & **ESPL** \\  CartPole & 1000 & 2M & 500 \\ MountainCar & 1000 & 2M & 500 \\ Pendulum & 1000 & 2M & 500 \\ InvDoublePend & 1000 & 2M & 500 \\ InvPendSWingup & 1000 & 2M & 500 \\ LunarLander & 1000 & 0.4M & 1000 \\ Hopper & 1000 & 0.4M & 3000 \\ BipedalWalker & 1000 & 0.4M & 2000 \\   

Table 3: The number of environment episodes used for learning symbolic policies.

that the algorithm control five thousand simulated robots with the Intel(R) Xeon(R) Gold 5218R @ 2.10GHz CPU and record the elapsed time per inference step. We report the results in Table 4. Compared to PEARL, CSP reduces the FLOPs by 60-45000x and reduces the inference time by up to 600x. Compared to Hyper, CSP reduces the flops by 2-450x and reduces the inference time by up to 200x. Thus, compared with pure NN policies, the CSP has a significant advantage in computational efficiency.

### Analysis of symbolic policies

**Interpretability.** For single-RL tasks, we report the symbolic policies found by ESPL for each environment in Table 1. The policies in the symbolic form are simple and we can directly glean insight from the policies by inspection, or in other words, interpretability. For example, the goal of LunarLander is to land a spacecraft on a landing pad without crashing. The action \(a_{1}\) controls the main engine and \(a_{2}\) controls the orientation engines. In our symbolic policy, \(a_{1}\) is a function of \(s_{1}\) (the height) and \(s_{4}\) (the vertical velocity). The main engine turns on to counteract downward motion when the height is low. Action \(a_{2}\) is a combination of a term about \(s_{3}\)(the horizontal velocity), a term about \(s_{5}\)(the angle), and a highly nonlinear term about \(s_{6}\) (the angular velocity), \(s_{1}\) (the horizontal position) and \(s_{7}\) (whether the leg has landed). Thus, the policy adjusts the orientation engines based on the incline and horizontal motion to move the spacecraft to the center of the landing pad. In meta-RL, the CSP also shows the potential to be interpretable. We take the Cartpole-fl-ood environment as an example and illustrate the Cartpole system in Figure 3. The form of the symbolic policies produced by CSP is \(action=c1*+c_{2}*+b\), where \(c1\) and \(c2\) are the positive coefficients and \(b\) is a small constant which can be ignored. Then the policy can be interpreted as pushing the cart in the direction that the pole is deflected or will be deflected. To analyze the difference between policies for different tasks, we uniformly set the force magnitude and the length of the pole. Then we generate the symbolic policy with CSP and record the coefficients. As Figure 4 shows, \(c1\) and \(c2\) tend to increase when the force magnitude decrease and the length increase, which is in accord with our intuition. We also provide the human study results of the interpretability in Appendix F.3.

**Complexity.** we compare the length of the symbolic policies and define \(length=^{i=n}N_{o}^{i}+N_{o}^{i}+N_{o}^{i}}{n}\), where \(n\) is the dimension of the action, \(i\) is the index of the action dimension, \(N_{o}^{i}\) is the number of operators, \(N_{c}^{i}\) is the number of constant terms, \(N_{v}^{i}\) is the number of variable terms. We give a comparison of the length of the symbolic policies in Table 5.

In the benchmark environments used in the literature, in some environments ESPL produces longer symbolic policies than DSP, in others ESPL produces similar or shorter symbolic policies than DSP. In general, symbolic policies produced by ESPL are only slightly longer than the symbolic policies produced by DSP.

### Ablation

Finally, we carry out experiments by ablating the features of the proposed ESPL. We change the structure of the symbolic network and replaced the path selector with the \(L_{1}\) norm minimization. We

  
**Environment** & **CSP** & **PEARL** & **Hyper** \\  Walker2d-params & **3.11/20.9** & 189.3/27.0 & 5.64/22.6 \\ Hopper-params & **0.51/4.13** & 186.9/26.6 & 4.1/17.2 \\ InvDoublePend-params & **0.039/0.37** & 186.0/25.1 & 3.59/12.3 \\ Cartpole-fl-ood & **0.004/0.042** & 183.9/23.9 & 1.79/0.8 \\ Lunarlander-g & **0.015/0.14** & 185.4/23.4 & 3.08/12.3 \\ Cheetah-vel-ood & **0.53/4.9** & 190.2/28.4 & 7.18/24.2 \\   

Table 4: FLOPs (k)/Inference time (ms) of different contextual policies.

    & **Average** & **CarlPole** & **MountainCar** & **Pendulum** & **InvDoublePend** & **InvPendSwingup** & **LunarLander** & **Hopper** & **BipedalWalker** \\ 
**ESPL** & 12.91 & 3 & 6 & 7 & 15 & 13 & 16.5 & 24.6 & 17 \\
**DSP** & 8.25 & 3 & 4 & 8 & 1 & 19 & 6.5 & 12 & 12.5 \\   

Table 5: Comparison of the length of the symbolic policies.

report the average episode rewards for single-task RL in Table 6. As the experiment results show, without the path selector or the dense connections, the performance degrades, especially in Hopper and BipedalWalker. With the path selector or the dense connections, ESPL\({}_{}\) is able to perform well for all the environments while we observe that the arranged symbolic operators can further improve the performance. We also provide the ablation study results for meta-RL in Appendix F.2.

## 6 Conclusions

In this paper, we introduce ESPL, an efficient gradient-based symbolic policy learning method. The proposed ESPL is able to learn the symbolic policy from scratch in an end-to-end way. The experiment results on eight continuous control tasks demonstrate that the approach achieves comparable or higher performance than both NN-based policies and previous symbolic policies while greatly improving the data efficiency compared with the previous symbolic policy method. We also combine our method with meta-RL to generate symbolic policies for unseen tasks. Empirically, compared with neural network policies, the proposed symbolic policy achieves higher performance and efficiency and shows the potential to be interpretable. We hope ESPL can inspire future works of symbolic policy for reinforcement learning or meta-reinforcement learning. Besides, as the symbolic policy is a white box and more dependable, the proposed ESPL may promote applications of reinforcement learning in industrial control and automatic chip design.

**Limitations and future work.** In this paper, we focus on continuous control tasks with low-dimensional state space. The proposed ESPL and CSP can not directly generate a symbolic policy for tasks with high-dimensional observation like images. A possible method is to employ a neural network to extract the environmental variables and generate symbolic policy based on these environmental variables. We leave this in the future work. Symbolic policies generally have good interpretability. However, when the task is too complex, the symbolic policy is also more complex, making the interpretability decrease. Solving this problem is also an important direction for future work. For application, we will further learn a symbolic policy for automated CPU design based on this framework to optimize the performance/power/area (PPA) of the CPU.

 
**Environment** & **ESPL** & **ESPL\({}_{}\)** & **ESPL\({}_{}\)** & **ESPL\({}_{}\)** \\  CartPole & **1000** & **1000** & **1000** & **1000** \\ MountainCar & 94.02 & 93.69 & 93.83 & **94.17** \\ Pendulum & -151.72 & -183.16 & **-144.08** & -163.54 \\ InvDoublePend & **9359.9** & 9357.6 & 9197.44 & 8771.18 \\ InvPendSwingup & **890.36** & 844.84 & 890.01 & 865.38 \\ LunarLander & **283.56** & 263.95 & 277.36 & 271.69 \\ Hopper & **2442.48** & 2003.24 & 2316.54 & 1546.35 \\ BipedalWalker & **309.43** & -11.63 & 298.50 & 6.81 \\  

Table 6: Ablation results for single-task RL. ESPL\({}_{}\) and ESPL\({}_{}\) means replacing the symbolic network with a plain structure and a densely connected structure respectively. ESPL\({}_{_{1}}\) means replacing the path selector with the \(L_{1}\) norm minimization.

Figure 3: The Cartpole system to be Figure 4: The coefficients of symbolic policies for Cartpole environments.

## 7 Acknowledgement

This work is partially supported by the National Key R&D Program of China (under Grant 2021ZD0110102), the NSF of China (under Grants 61925208, 62102399, 62222214, 62002338, U22A2028, U19B2019), CAS Project for Young Scientists in Basic Research (YSBR-029), Youth Innovation Promotion Association CAS and Xplore Prize.