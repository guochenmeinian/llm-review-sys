# Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses

Jacob Granley

Department of Computer Science

University of California, Santa Barbara

jgranley@ucsb.edu

&Tristan Fauvel

Institut de la Vision, Sorbonne Universite

17 rue Moreau, F-75012 Paris, France

Now with Quinten Health

t.fauvel@quinten-health.com

&Matthew Chalk

Institut de la Vision, Sorbonne Universite

17 rue Moreau, F-75012 Paris, France

matthew.chalk@inserm.fr

&Michael Beyeler

Department of Computer Science

Department of Psychological & Brain Sciences

University of California, Santa Barbara

mbeyeler@ucsb.edu

###### Abstract

Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian optimization strategy utilizes this encoder to optimize patient-specific parameters for a new patient, using a minimal number of pairwise comparisons between candidate stimuli. We demonstrate the viability of this approach on a novel, state-of-the-art visual prosthesis model. We show that our approach quickly learns a personalized stimulus encoder, leads to dramatic improvements in the quality of restored vision, and is robust to noisy patient feedback and misspecifications in the underlying forward model. Overall, our results suggest that combining the strengths of deep learning and Bayesian optimization could significantly improve the perceptual experience of patients fitted with visual prostheses and may prove a viable solution for a range of neuroprosthetic technologies.

## 1 Introduction

Sensory neuroprostheses are devices designed to restore or enhance perception in individuals with sensory deficits. They often interface with the nervous system by electrically stimulating neural tissue in order to provide artificial sensory feedback to the user . For instance, visual prostheses have the potential to restore vision to people living with incurable blindness by bypassing damaged parts of the visual system and directly stimulating the remaining cells in order to evoke visual percepts (phosphenes) . However, patient outcomes with current technologies are limited. Patientsrequire extensive training to learn to interpret the evoked percepts, which are typically described as "fundamentally different" from natural vision . Moreover, phosphene appearance varies widely across patients , making personalized stimulus optimization a major outstanding challenge .

Much work has gone into developing computational models that can predict the neuronal or perceptual response to an electrical stimulus [8; 10; 11] (often called forward models). Once the forward model is known, a deep neural network can approximate its inverse, thereby identifying the required stimulus to elicit a desired percept [12; 13; 14]. However, these inverse models typically assume perfect knowledge of the patient-specific mapping from stimulation to perception (which is not practically feasible) and are heavily reliant on the forward model's accuracy over the entire stimulus space.

Alternatively, Bayesian optimization has been successful in personalizing stimulation strategies for many existing neural interfaces [15; 16]. However, this approach is limited in practice because it requires the stimulus dimension to be small (typically \(<30\), which is orders of magnitudes smaller than the number of stimulus parameters in current implants), and optimization must be repeated for every stimulus. Moreover, visual prosthesis users can typically only give indirect feedback (e.g., verbal phosphene descriptions), unsuitable for traditional Bayesian optimization.

To address these challenges, we propose a novel framework that integrates deep learning-based stimulus inversion into a preferential Bayesian optimization strategy to learn a patient-specific stimulus encoder (Fig. 1). First, a deep stimulus encoder (DSE) is trained to optimize stimuli assuming perfect knowledge of a set of patient-specific parameters (Fig. 1, _left_). Second, we embed the DSE within a human-in-the-loop optimization (HILO) strategy based on preferential Bayesian optimization, which iteratively learns the ground-truth patient-specific parameters through a series of 'duels', where the patient is repeatedly asked their preference between two candidate stimuli. The resulting DSE can then be deployed as a personalized stimulation strategy.

To this end, we make the following contributions:

* We introduce a forward model for retinal implants that achieves state-of-the-art response predictions. Unlike previous models, this allows us to train a deep stimulus encoder to predict optimal stimuli across 13 dimensions of patient-specific parameters.
* We propose a personalized stimulus optimization strategy for visual prostheses, where a human-in-the-loop optimization (HILO) Bayesian optimization algorithm iteratively learns the optimal patient-specific parameters for a deep stimulus encoder.

Figure 1: _Left_: Deep stimulus encoder (DSE). A forward model (\(f\)) is used to approximate the perceptual response to electrical stimuli, subject to patient-specific parameters \(\). An encoder (\(f^{-1}\)) is then learned to minimize the perceptual error between predicted and target percept. _Right_: Human-in-the-loop optimization (HILO). Patient-specific parameters \(\) of the DSE are optimized with user preferences: the patient performs a series of binary comparisons between percepts evoked with different encoders. New pairs of parameters to compare are adaptively selected so as to efficiently find the parameters maximizing the patientâ€™s preference. The target changes each iteration.

* We demonstrate the viability of our approach by conducting a comprehensive series of evaluations on a population of simulated patients. We show HILO quickly learns a personalized stimulus encoder and leads to dramatic improvements in the quality of restored vision, outperforming existing encoding strategies. Importantly, HILO is resilient to noise in patient feedback and performs well even when the forward model is misspecified. Code for the forward model, DSE, and HILO algorithm is available at https://github.com/bionicvisionlab/2023-NeurIPS-HILO.

## 2 Background and Related Work

Visual NeuroprosthesesNumerous groups worldwide are pursuing a visual prosthesis that stimulates viable neuronal tissue in the hope of restoring a rudimentary form of vision to people who are blind (Fig. 2, _left_) [3; 4; 5; 6]. Analogous to cochlear implants, these devices electrically stimulate surviving cells in the visual pathway to evoke visual percepts (phosphenes). Existing devices generally provide an improved ability to localize high-contrast objects and perform basic mobility tasks.

Much work has focused on characterizing phosphene appearance as a function of stimulus and neuroanatomical parameters [2; 10; 18; 19; 20; 21]. In epiretinal implants, phosphenes often appear distorted due to inadvertent activation of nerve fiber bundles in the optic fiber layer of the retina , causing elongated percepts (Fig. 2, _center_). In addition, the exact brightness and shape of the elicited percepts depends on the applied stimulus  and differs widely across patients (Fig. 2, _right_). Granley _et al._ captured these individual differences by including a set of patient-specific parameters in their phosphene model, denoted by \(\), which includes both neuroanatomical (e.g., implant location) and stimulus-related parameters (e.g., how brightness scales with amplitude).

Deep Stimulus EncodingMany works attempt to mitigate distortions in prosthetic vision, but do not describe comprehensive stimulation strategies [22; 23; 24]. Those that describe strategies in detail typically require simplification  or strong assumptions  to be used in practice. Due to the complexities of optimization, deep learning-based stimulus encoders have risen in popularity [12; 13; 14]. In , authors proposed an innovative approach where the latent representations of an autoencoder are treated as stimuli and decoded with a phosphene model. However, they used an unrealistic binary phosphene model. Their approach has since been adapted for cortical models , and for non-differentiable forward models . Granley _et al._ generalized the approach, showing it could work with realistic forward models across a small range of patients without needing to retrain.

Given a forward (phosphene) model \(f\) (mapping stimuli to percepts given \(\)), it is straightforward to show that the optimal stimulus encoder (mapping target images to stimuli) is the pseudoinverse of \(f\). However, to account for the wide range of individual differences in phosphene perception, most realistic forward models are highly nonlinear and not analytically invertible. Thus, previous works have proposed to use the forward model [12; 14] as a fixed decoder within a deep autoencoder trained to minimize the reconstruction error between target images and the predicted percepts. After training, the encoder can be extracted and used to encode target visual inputs in real time. Deep

Figure 2: _Left_: Visual prosthesis. Incoming target images are transmitted from a camera to an implant in the retina, which encodes the image as an electrical stimulus pattern. _Center_: Electrical stimulation (red disc) of a nerve fiber bundle (gray lines) leads to elongated tissue activation (gray shaded region) and a phosphene (bottom). _Right_: The same stimulus parameters may lead to widely varying visual perceptions in different patients. Adapted with permission from .

stimulus encoders trained using this approach produce high quality stimuli, but assume knowledge of \(\). Additionally, if the forward model \(f\) is not extremely accurate over the whole stimulus space, then the encoder network might learn to exploit inaccuracies in the model, producing stimuli that don't generalize to real patients . We utilize an enhanced variant of this approach in our experiments.

Preferential Bayesian OptimizationPreferential Bayesian optimization (PBO) is an efficient method for optimizing expensive black-box functions based on binary comparisons [28; 29]. Since the subject's response to stimulation cannot be directly observed, PBO instead builds a Bayesian model of the subject's preferences, \(g\), typically modeled using a Gaussian process. An approximate inference algorithm (expectation propagation; [30; 31]) is used to infer the posterior distribution of the preference function given binary comparison data, \(p(g|)\), which is then used to select new configurations for the next trial according to an acquisition rule. The acquisition rule must balance the exploration-exploitation trade-off inherent to any black-box optimization problem .

PBO was previously used to tune BCI stimulation parameters for transcranial  and spinal cord stimulation . However, these works directly optimized only a handful of stimulation parameters and cannot translate to visual prostheses, where complex and varying visual inputs have to be mapped to high-dimensional stimuli. To this end, Fauvel & Chalk  reduced optimization complexity by inverting a model of phosphene perception, then used PBO to generate encodings preferred by sighted subjects viewing simulated prosthetic vision. However, a linear approximation was used to invert the perception model, which is unrealistic for real-world applications.

SummaryWe identify 3 main limitations of previous work that this study aims to address:

* **Generalizability of deep stimulus encoders.** Autoencoder-like deep stimulus encoders can accurately optimize stimuli, but require perfect knowledge of patient-specific parameters , which can be difficult or impossible to determine in practice [8; 11]. Further, these approaches heavily rely on the accuracy of the forward model [13; 14], while real patients will likely deviate from the forward model. We overcome this limitation by optimizing the learned stimulus encoder based on patients' preferences, which we show is not bounded by a misspecified forward model.
* **Applicability of Bayesian optimization.** Bayesian optimization is ideally suited for optimizing stimulation parameters based on limited, noisy measurements, but can only optimize a small number of parameters . We train a deep stimulus encoder to output optimal stimuli for any specific patient with known parameters, reducing the search space from the entire stimulus space (hundreds or thousands of parameters) to the low-dimensional space of patient-specific parameters (13 parameters with our model), enabling Bayesian optimization.
* **Simplistic models of perception.** Most previous approaches use overly simplified forward models that do not match empirical data [8; 19]. More accurate models  are too computationally expensive to support deep stimulus optimization over a wide range of patients. We overcome this by developing a new phosphene model that matches patient data better than existing models, with significantly reduced memory and time complexity

## 3 Methods

General FrameworkWe consider a system attempting to optimize stimuli for a new patient, specified by a set of (unknown) parameters \(\). The goal of optimization is a patient-specific stimulus encoder mapping target perceptual responses \(\) (e.g., visual percepts) to stimuli \(\).

We assume there exists a forward model \(f\) which predicts the patient's perceptual response to stimulation: \(}=f(;)\). It follows that the optimal stimulus encoder is the (pseudo)inverse of \(f\). The inverse can be approximated using an autoencoder-like deep neural network, with \(f\) as the fixed decoder and \(^{-1}\) as the learned stimulus encoder . The encoder's weights \(w\) are updated using gradient descent to minimize the reconstruction error, measured by some distance function \(d\), between the target \(\) and the predicted response \(}\) averaged across patients and a dataset of targets (Eq. 1):

\[w *{arg\,min}_{w}*{}_{ },}d(,})\] (1) \[} =f(_{w}^{-1}(,);)\] (2)Once trained, the encoder can accurately predict stimuli, but requires knowledge of the patient-specific parameters \(\). For a new patient, Bayesian optimization is used to optimize \(\) based on user feedback, thereby learning a personalized DSE. Since the patient's response cannot be directly measured for visual prostheses, the user is presented with a 'duel', i.e. a binary comparison, where they are asked to decide which of two candidate stimuli they prefer . A Gaussian process model of preferences is updated based on the patient's response, and an acquisition function is used to generate new candidate stimuli. The process can be repeated to iteratively tune the DSE to the patient's preferences.

Phosphene ModelThe phosphene model is a differentiable approximation of the underlying biological system (also called a forward model ), which maps an electrical stimulus to a visual percept. Although phosphene models exist for visual prostheses, current models either do not match patient data well , or are too computationally expensive .

Thus, we developed a new phosphene model for epirical prostheses (See Appendix A.1 for full details). The model takes in a stimulus vector \(^{n_{e} 3}\) specifying the frequency, amplitude, and pulse duration of a biphasic pulse train on each of \(n_{e}\) electrodes. The output phosphene for each electrode is a Gaussian blob centered over the electrode's location \(_{e}()\) with covariance matrix \(}(,)\) constructed so that the resulting percept will have area \(_{e}(,)\), eccentricity \(_{e}(,)\) and orientation \(_{e}()\). These functions allow phosphene properties to vary locally with stimulus (e.g., current spread) and anatomical parameters (e.g., electrode location, underlying axon nerve fiber bundle trajectory). The percept for an electrode located at \(_{e}\), is a multivariate Gaussian blob, renormalized to have maximum brightness \(b_{e}(,)\):

\[b(x,y)=2 b_{e}(})\,([x,y]^{ }|_{e},}),\] (3)

where \(b_{e}\), \(_{e}\), and \(_{e}\) are implicitly parametrized by \(\) and \(\). The covariance matrix \(}=}^{T}\) is calculated from the eigenvalue matrix \(}\) and a rotation matrix \(\):

\[_{0}=_{e}^{2}&0\\ 0&_{e}^{2}, 14.226378ptR= _{e}&-_{e}\\ _{e}&_{e}.\]

The eigenvalues \(_{e}\) and \(_{e}\) depend on the intended phosphene area (\(_{e}\)) and elongation \((_{e})\):

\[_{e}^{2}=-^{2}}}{2}, 14.226378pt _{e}^{2}=-}{2^{2}}}.\]

Blobs from individual electrodes are summed into a global percept . Although the sum across electrodes is linear, modulating the size and eccentricity of phosphenes with stimulus parameters makes the final result a nonlinear function of stimulus parameters, preventing analytic inversion. Motivated by previous studies, we used a square \(15 15\) array of 150\(\)m electrodes, spaced 400\(\)m apart . In total, the model is parameterized by 13 patient specific parameters, shown in Table 1. The ranges for each parameter were chosen to conservatively encompass all observed patients, centered on the mean value across patients .

Deep Stimulus InversionA deep stimulus encoder (DSE) is a deep neural network responsible for approximately inverting the forward model to produce the optimized stimulus for a target image and a specific patient (\(_{}=^{-1}(,)\)) . We used a network (45M parameters) consisting of blocks, each containing 3 fully connected layers, batch normalization, and a residual connection. The architecture is illustrated in Appendix B.1. The flattened target image and the patient specific parameters were passed separately through one block each, concatenated, and passed through another block, after which the amplitude is predicted. The amplitudes were concatenated to the prior intermediate representation, fed through a final block, after which frequency and pulse duration were predicted. The output layers use ReLU activation; all others use leaky ReLU. During training, \(\) were randomly sampled from the range of allowed parameters (Table 1). Tensorflow 2.12, an NVIDIA RTX 3090, Adam optimizer, and batch size of 256  were used to train the network.

    & & & & & & & & & & & & Implant Parameters \\   & \(\) (dva) & \(\) & \(\) & \(a_{0}\) & \(a_{1}\) & \(a_{2}\) & \(a_{3}\) & \(a_{4}\) & \(OD_{x}\) (\(\)m) & \(OD_{y}\) (\(\)m) & x (\(\)m) & y (\(\)m) & rot (deg) \\  Lower & 1.5 &.45 &.9 &.27 &.42 &.005 &.2 & -0.5 & 3700 & 0 & -500 & -500 & -500 \\ Upper & 8 &.98 & 1.1 &.57 &.62 &.025 &.7 & -0.1 & 4700 & 1000 & 500 & 500 & 30 \\   

Table 1: Patient-Specific Parameters \(\)Human-in-the-Loop OptimizationWe propose using preferential Bayesian optimization (PBO) to optimize the patient-specific parameters \(\) of the pretrained DSE. Given two sets of patient-specific parameters, \(_{1}\) and \(_{2}\), we assume that the probability of a subject preferring \(_{1}\) to \(_{2}\) (returning a response \(_{1}_{2}\)) depends on a preference function \(g()\), modeled using a Gaussian process model:

\[P(_{1}_{2}|g)=g(_{1})-g(_{2}),\] (4)

where \(\) is the normal cumulative distribution . The larger the value of \(g(_{1})\) relative to \(g(_{2})\), the higher the likelihood that the subject reports preferring \(_{1}\) over \(_{2}\).

We used the Maximally Uncertain Challenge  to select new comparisons to query, although other popular acquisitions performed similarly (Appendix C.3). Searching within the bounds in Table 1, this acquisition function selects a 'champion', \(_{1}\), which maximizes the expectation of \(g\), and a 'challenger', \(_{2}\), for which subjects' preferences are most uncertain:

\[_{1} *{arg\,max}_{}_{p(g| )}[g()],\] (5) \[_{2} *{arg\,max}_{}_{p(g| )}[(g()-g(_{1}))],\] (6)

where \(\) denotes the variance. This algorithm is designed to balance exploitation (values of \(\) that maximize \(g\)) and exploration (values of \(\) for which the response is uncertain).

The performance of PBO crucially depends on the Gaussian process kernel and its hyperparameters, which encode our prior assumptions about the latent preference function. Inferring the kernel's hyperparameters online would slow down the algorithm and could lead to overfitting. Thus, we adopted a transfer learning strategy, which could also be applied to real-life patients (see Appendices C.1 and C.2 for full details and discussion of alternatives). In brief, for each of 10 patients (with parameters different from those used in the following PBO experiment), we simulated 600 random duels and fit candidate hyperparameters for each of 4 commonly used kernels. We then selected the kernel and hyperparameters that generalized best to the other 9 patients (measured using Brier score  on a held-out test set). The 5/2 Matern kernel performed best, and was used for all subsequent experiments.

Simulated Patients_In silico_ experiments on simulated patients were used to demonstrate the viability of our approach. Each patient was assigned a set of patient-specific parameters \(\), uniformly sampled from the ranges specified in Table 1. When challenged with a duel between two candidate stimuli \(_{_{1}}\) and \(_{_{2}}\), the simulated patient ran each stimulus through the phosphene model (using ground-truth patient-specific parameters), obtaining the predicted percepts \(}_{_{1}}=f(_{_{1}};)\) and \(}_{_{2}}=f(_{_{2}};)\). The users' preferences were modeled with a Bernoulli distribution, with probability \(p\) modulated by the difference in reconstruction error between each percept and the target image:

\[p=(d(}_{_{2}},) -d(}_{_{1}},)))}\] (7)

Here, \(\) is a configurable parameter that scales the width of the sigmoid, introducing noise into the response. We set \(\) to be \(0.01\), chosen empirically based on a conservative estimate: when the error difference was greater than 0.01 it was obvious which percept was better to human observers.

Data and MetricsWe used MNIST images as target visual percepts throughout the experiments. Images were resized to be the same size as the output of \(f\) (\(49 49\) pixels), and scaled to have a maximum brightness of 2 (aligned with range(\(f\))). Inspired by , we used a perceptual similarity metric designed to capture higher-level differences between images . Let \(v_{l}()\) be a function that extracts the downstream representations of target \(\) input to a VGG19 network pretrained on ImageNet . The perceptual metric is then given by equation 8.

\[d(,})=(||-}||_ {2}^{2}+||v_{l}()-v_{l}(})||_{2}^{2})\] (8)

This metric was used by the deep stimulus encoder as a training objective, by the simulated patient to choose a duel winner, and throughout HILO as an evaluation metric. \(=2.5\)e-5 was selected via cross-validation (see  App. B). To aid in interpretability, we also report a secondary metric based on how identifiable the predicted percepts were. We first pretrained a separate deep net to 99% test accuracy on MNIST classification. We then measured the accuracy of this classifier on the predicted percepts at every iteration of HILO.

## 4 Results

### Phosphene Model

To verify that our phosphene model's predictions line up with observed results from real prosthesis users, we repeated analyses from previous state-of-the-art models, evaluating how phosphene appearance changes with electrode location  and stimulus parameters [10; 11; 19; 37]. We used the same datasets, consisting of thousands of phosphene drawings and brightness and size ratings collected across multiple epiretinal prosthesis  patients over several years. To evaluate phosphene appearances with electrode location, we calculated the correlation between predicted and observed phosphene (\(R_{i}\)) for three shape descriptors: area, eccentricity, and orientation. The final score reported is \(1-_{i}R_{i}^{2}\). To evaluate how phosphene appearance was modulated by stimulus parameters, we calculated the mean squared error between the size and brightness of predicted percepts and patient ratings as amplitude, frequency, and pulse duration were varied. The reported values correspond to Figures 3(a)-3(c) and 5 in .

Evaluation results are presented in Table 2. Our model significantly outperforms all baseline on the Beyeler _et al._ evaluation, and matches SOTA on the Granley _et al._ evaluations. Additionally, our model is on average 45x faster, and uses 120x less memory than . A detailed description of evaluation methods and additional analysis can be found in Appendix A.2.

### Deep Stimulus Encoder

We trained a deep stimulus encoder (DSE) to invert our phosphene model (decoder). The encoder was trained across 13 patient-specific parameters, randomly sampled at every epoch, including for the first time implant position and rotation. This is in contrast to previous DSEs, which either require retraining for every new patient [12; 13; 27], or can only vary two patient-specific parameters .

We compared the performance of the DSE to a traditional ('naive') encoder  currently used by retinal prostheses , illustrated in Fig. 3. The DSE achieved a test perceptual loss of 0.05 and a MNIST accuracy of 95.6%, significantly outperforming the naive encoder (5.68 and 51% respectively). Note that this performance is when the true patient-specific parameters are known.

    &  &  \\  Model & S1 & S2 & S3 & 4A & 4B & 4C & 5 \\  Nanduri _et al._ & 37.3 & 19.5 & 294.8 & 11.1 & 72.9 &.2 & 160.9 \\ Beyeler _et al._ & 2.43 & 7.07 & 1.15 & 215.6 & 108.7 & 5.52 & 190.2 \\ Granley _et al._ & 2.43 & 7.07 & 1.15 & 0.9 & **2.1** & 0.16 & 49.5 \\ Proposed & **0.28** & **0.57** & **0.38** & **0.73** & 2.3 & **0.1** & **48.6** \\   

Table 2: Evaluation of Phosphene Model

Figure 3: Percepts resulting from a naive encoder and the trained DSE for two example target images across 25 randomly selected patients.

This performance was slightly better than the values reported in  (L1 loss of.108 vs.12 in ) despite training across 11 additional patient-specific parameters.

### Human-in-the-Loop Optimization

We ran deep learning-based HILO for 100 randomly sampled simulated patients. After every duel, we evaluated the DSE parameterized by the current prediction of patient-specific parameters on a subset of the MNIST test set. The performance of the learned encoder over time ('HILO') is illustrated in Figure 4, which plots the joint perceptual loss (Figure 4.B) and MNIST accuracy (Fig. 4C).

As baselines for comparison we used a naive encoder, a non-personalized DSE where the patient-specific parameters are guessed (DSE-\(_{Guess}\)), and an ideal DSE using the true \(\) (DSE-\(_{True}\)). To guess \(\), we consider two approaches, one which selects the midpoint from the ranges in Table 1, and another where performance was averaged across random selections of \(\) from the same ranges. In real patients, we estimate that the performance of a deep stimulus encoder without patient-specific optimization would likely fall somewhere between these two methods, since the distribution of real patients might deviate slightly from Table 1. We therefore plot the region bounded by the performance of a DSE with either of these approaches for guessing \(\). Example percepts after optimization are shown in Fig. 4D.

The HILO encoder starts with random predictions, but, after a short initial exploration period, quickly surpassed the baselines. After about 75 iterations, performance approached the ideal DSE encoder, however the HILO encoder still resulted in high-quality percepts after as few as 20 iterations. Averaged across patients, the final reconstruction error of the HILO encoder was.071 \(\).0031 (SEM)

Figure 4: Human-in-the-loop optimization of a deep stimulus encoder. _A_: Two example duels, from which patient preferences are learned. _B_: Reconstruction error throughout optimization across 100 simulated patients. Insets show the predicted percept resulting from stimulation with various encoders. Note the y axis is on a log scale. _C_: MNIST accuracy of a pretrained classifier on reconstructed phosphenes. Both plots show smoothed median (window size of 3), with error bars denoting IQR. _D_: Example percepts after optimization for Naive, DSE without HILO, and HILO encoders for 6 random patients. Note, brightness is capped for display for the Naive encoder.

and MNIST accuracy was 92% \(\) 1.0%. DSE-\(_{Guess}\) had an error of between.25 and 1.1 and MNIST accuracy between 58.6% and 78.3%, and the DSE with true \(\) had an error of.05 \(\).001 and accuracy of 95.5% \(\).1%.

### Robustness

In reality, it is likely that a patient's perceptions will not be perfectly captured by the phosphene model. Further, patient responses for visual prostheses are notoriously noisy [7; 46]. To test HILO's resiliency to these variations, we conducted additional robustness experiments, each with the same 25 simulated patients (Figure 5). First, we varied the noise parameter \(\) in simulated patients' decision making (Figure 5A). Next we constructed various'misspecified' forward models, where the ground-truth model used to decode stimuli differed from the forward model assumed by the DSE. First, we varied the trajectories of the simulated axon bundles , thereby changing the orientation of phosphenes (Figure 5C). Second, threshold amplitudes for stimulation are notoriously hard to predict, and have been shown to drift by up to 300% over time . Therefore, we tested a variant where the threshold assumed by the encoder was incorrect by up to 300% (Figure 5B). Lastly, we used the same forward model, but with patient-specific parameters outside the ranges in 1 (Figure 5D).

At \(\)=1e-4, the patient response was noiseless. For \(\) equal to.005,.01, and.02, HILO performed similarly to the noiseless model, despite the patient on average making 'random' (\(p[0.35,0.65]\)) decisions in 26%, 38%, and 48% of duels. At \(\)=0.05, the decision was 'random' 2/3 of the time, and HILO performed similarly or slightly better than the baseline DSE-\(_{Guess}\). The DSE itself is very resilient to misspecifications in axon trajectory, so HILO performs similarly for this misspecification to the original patients. When thresholds varied, HILO still outperformed the baselines, but converged to slightly worse encodings than without misspecification. Further, HILO surpassed the DSE encoded with the ground-truth \(\). This demonstrates that even when the forward model assumed by the DSE is incorrect, HILO still tends to converge to a set of patient-specific parameters that work well for the

Figure 5: Reconstruction error through optimization for noisy patient responses (_upper left_) and for various misspecifications in the forward model assumed by the DSE. Noise level denotes the percentage of duels where the decision was essentially random (\(p[0.35,0.65]\)), corresponding to \(\) of 1e-4, 0.005, 0.01, 0.02, and 0.05, respectively. All y axes are on log scales. Naive encoders and some error bars omitted for clarity.

misspecified patient. For out-of-distribution \(\), HILO again outperformed both the baseline and true DSEs, but performed worse than in-distribution patients.

## 5 Discussion

Our experiments show that HILO optimization of a deep stimulus encoder led to high-quality, personalized stimulation strategies that outperformed previous state-of-the-art techniques. HILO led to an increase in percept quality compared to using a non-personalized DSE for 99 out of 100 simulated patients, demonstrating the viability of our approach. To enable our HILO algorithm, we also developed a new phosphene model, which is computationally simpler and matches patient data better than previous models, and trained a new DSE, which is able to produce high-quality encodings across all 13 dimensions of patient-specific variations included in our phosphene model. Together, these significantly advance state-of-the-art in patient-specific stimulus encoding, and are important steps towards practically-feasible personalized prosthetic vision in real patients.

The proposed framework combining Bayesian optimization and deep stimulus encoding offers significant improvements over both components in isolation. Use of a DSE allows us to incorporate prior information, reducing the dimensionality of the Bayesian optimization search space from the large stimulus space to the much smaller model parameter space. Our results demonstrate that even when the DSE's predictions are incorrect, this parameterization is still useful for Bayesian optimization based on patient preferences. Additionally, DSEs are able to invert highly nonlinear forward models, enabling encoder-parameterized Bayesian optimization to be applied to a much larger set of problems. Lastly, the learned encoder can be applied for any target percept, without needing additional optimization. On the other hand, without adaptive feedback from HILO, deep stimulus encoders have no method for learning the individual differences of a new patient, which we show leads to suboptimal stimuli. DSEs rely on the accuracy of their assumed forward model over the entire stimulus space. We show that our approach produces stimuli that work well for the patient, even when the forward model is misspecified, or when the patient's responses are noisy.

This approach is practical for stimulus optimization in the wild. The encoder learned during optimization is lightweight, and once deployed, can predict individual stimuli in less than \(5\,\) on CPU, allowing for high frame rates for prosthetic stimulation. During HILO, updating the Gaussian process model and producing new stimuli on average took 3 seconds, meaning that stimulus optimization could be performed in a matter of minutes. A HILO strategy could be bundled with future visual prostheses, allowing for patients to periodically re-calibrate their devices when they feel the device is not performing adequately, without requiring a clinical professional.

Broader ImpactsAlthough we demonstrate this approach in the context of visual prostheses, our framework is general and could be applied to a variety of sensory devices. Our approach is applicable when the stimulus search space is large and there exists a forward model mapping stimuli to responses. Forward models [49; 50; 51; 52] and deep stimulus encoders [53; 54; 55] have been successfully used across multiple sensory modalities, and could potentially be adapted for personalization with HILO.

LimitationsAlthough promising, our approach is not without limitations. We assumed that the preference of patients for different stimuli is related to the distance metric used to measure perceptual similarity, which may not be true in practice. However, results by  suggest that PBO is robust to a mismatch between the distance metric used to invert the forward model and the preference of patients. Another limitation is that evaluation of our approach was only performed on simulated patients with a simulated perceptual model. However, this is mitigated by the fact that HILO showed robustness to model inaccuracies. Still, since it is difficult to predict the behavior of deep learning models, using a deep stimulus encoder in real patients could raise safety concerns. It may be possible for a deep encoder to produce unconventional stimuli, potentially leading to adverse effects. However, most devices come with firmware responsible for ensuring stimuli stay within FDA-approved safety limits.

In conclusion, our results suggest that combining the strengths of deep learning and Bayesian optimization could significantly improve the perceptual experience of patients fitted with visual prostheses and may prove a viable solution for a range of neuroprosthetic technologies.

Acknowledgements

Research reported in this publication was supported by the National Library of Medicine of the National Institutes of Health under Award Number DP2LM014268. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.