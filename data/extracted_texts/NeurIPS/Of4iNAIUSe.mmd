# Resource-Aware Federated Self-Supervised Learning

with Global Class Representations

 Mingyi Li

Shandong University

&Xiao Zhang

Shandong University

&Qi Wang

Shandong University

&Tengfei Liu

Hong Kong University of

Science and Technology

&Ruofan Wu

Coupang

&Weiqiang Wang

Shanghai Jiaotong University

&Fuzhen Zhuang

Institute of Artificial Intelligence,

Beihang University

Zhongguancun Laboratory

&Hui Xiong

Thrust of AI, HKUST(Guangzhou)

Dep. of Com. Sci. and Eng.,

HKUST

Xiao Zhang and Dongxiao Yu are corresponding authors. Email:{xiaozhang, dxyu}@sdu.edu.cn

###### Abstract

Due to the heterogeneous architectures and class skew, the global representation models training in resource-adaptive federated self-supervised learning face with tricky challenges: _deviated representation abilities_ and _inconsistent representation spaces_. In this work, we are the first to propose a multi-teacher knowledge distillation framework, namely _FedMKD_, to learn global representations with whole class knowledge from heterogeneous clients even under extreme class skew. Firstly, the adaptive knowledge integration mechanism is designed to learn better representations from all heterogeneous models with deviated representation abilities. Then the weighted combination of the self-supervised loss and the distillation loss can support the global model to encode all classes from clients into a unified space. Besides, the global knowledge anchored alignment module can make the local representation spaces close to the global spaces, which further improves the representation abilities of local ones. Finally, extensive experiments conducted on two datasets demonstrate the effectiveness of _FedMKD_ which outperforms state-of-the-art baselines 4.78% under linear evaluation on average.

## 1 Introduction

The federated self-supervised learning (Fed-SSL) has emerged as a highly promising paradigm due to the extremely limited labeled data in real-world scenarios . The Fed-SSL mechanism can learn common representations collaboratively across all the clients without labeled data , which could enable the aggregation of knowledge from diverse unlabeled data sources and overcome the limitations caused by the high cost and scarcity of labeled data .

Traditional Fed-SSL methods usually assume that each client should train the identical architecture model, such as FedU , FedEMA , FedCA . But it would not be easy in resource-limited scenarios, especially for the existing arsing large-scale models . As shown in Fig.1, _client A_, _client B_ and _client C_ might train heterogeneous representation models due to the varying system resources. In addition, real-world data often exhibits skewed class distributions across clients.

Therefore, how to learn global class representations under the heterogeneous architectures and class skew in resource-aware Fed-SSL paradigm is challenging, particularly comparing with existing \(FedU^{2}\), _FedX_ with identical architectures.

Although both _Hetero-SSFL_ and _FedFoA_ consider the heterogeneous client models, they can not learn a global representation model. In order to aggregate the knowledge from the clients to form global class representations, some tricky challenges arise. (1) _Deviated representation abilities_. Even for the same data samples, the different models might encode them into different latent spaces with _deviated representation abilities_. For example, _client A_ and _client B_ all have images with _dog, cat, tiger_, but _client model A_ can encode _cat, tiger_ well into different clusters, _client model B_ can only learn better representations of _dog_. So _how could global representation models take advantage of the best of both client models?_ (2) _Inconsistent representation spaces_. The skewed class distributions across clients lead to inconsistent representation spaces. For example in Fig. 1, comparing with _client A_, _client C_ has different kinds of images with _dog, cat, airplane_. Thus _how to make global representation models encode the whole classes from all the clients well in a unified space?_ Therefore, different from the existing works, our goal is to break the gaps caused by the hybrid heterogeneity, which can learn the high-quality global representation model in federated self-supervised learning.

Along this line, we propose _FedMKD_, a multi-teacher knowledge distillation based resource-adaptive Fed-SSL framework, which can learn global representations over all classes from heterogeneous clients. First, an adaptive knowledge integration module is introduced to learn high-quality representations from all the heterogeneous models with deviated representation abilities. Then in order to encode all classes from clients in a unified space, the global model uses the weighted combination of self-supervised loss and distillation loss to update. Besides, the global knowledge anchored alignment module is applied within the server to eliminate the inconsistency in representation spaces and reduce the burden on the clients. It uses global knowledge to additionally update the local models, which can not only make the local representation spaces close to the global space but also improve the representation capability of both local models and the global ones. Code is available at https://github.com/limee-sdu/FedMKD. The main contributions of this paper can be summarized as follows.

* In resource-aware Fed-SSL, we are the first to delve into global class representation learning through revealing the deviated representation abilities and inconsistent representation spaces caused by the heterogeneous architectures and class skew.
* We design a multi-teacher knowledge distillation framework, namely _FedMKD_, to adaptively aggregate positive knowledge from heterogeneous models with deviated representation abilities. Through combining the self-supervised loss and the distillation loss, _FedMKD_ can encode skewed classes into a unified space.
* Extensive experiments conducted on _CIFAR-10_ and _CIFAR-100_ show the representation abilities over all classes of the _FedMKD_ perform better than state-of-the-art baselines. Our algorithm can improve 4.22% and 5.31% separately on the two chosen datasets.

## 2 Related work

The federated self-supervised learning aims to learn high-quality representations from clients without large labeled datasets . From the beginning, several works [12; 23] simply combine federated learning with self-supervised methods. Besides, _FedU_ designs a communication-efficient mech

Figure 1: Illustrations of main challenges in resource-aware Fed-SSL.

anism by only aggregating the online encoders under non-IID data. _FedUTN_ is proposed to use the aggregated online networks for the target network updating in the self-supervised framework. _L-DAWA_ proposes the layer-wise divergence aware weight aggregation to mitigate the influence of client bias. _FedEMA_ considers the divergence-aware moving average updating in clients, measuring the divergence between local models and global model. _FedX_ proposes a unsupervised federated learning framework to learn representations through a two-sided distillation method. However, all the above works intuitively gain the global model through parameters average due to the identical client models, which can not be applied in the heterogeneous clients setting directly. In addition, although _FedCA_ address the misaligned and inconsistent representation challenges by gathering features from clients, inducing potential privacy problems. _FLPD_[27; 29] introduces distillation method based similarity between prototypes from a labeled public dataset to update the local model. \(FedU^{2}\) focuses on mitigating representation collapse entanglement and obtaining unified representation spaces.

Considering heterogeneous client models in federated self-supervised learning, _Hetero-SSFL_ introduces linear-CKA to align lower-dimensional representations between the local model and global model without architectural constraints. _FedFoA_ designs a factorization-based method to extract the cross-feature relation matrix from the local representations for aggregation. However, both _Hetero-SSFL_ and _FedFoA_ can not learn a global representation model considering the hybrid heterogeneity, which is the main focus of our work. The comparison details are shown in Table 1.

## 3 Preliminaries

The goal of federated unsupervised learning is to learn the generalized representation for some downstream tasks from several distributed unlabeled data sources. A federated learning setting consists of a central server and \(N\) clients. Each client \(k\) contains a local unlabeled dataset \(_{k}\), and the server contains a public unlabeled dataset \(_{pub}\). The local objective at \(k\)-th client is

\[_{_{k}}F(_{k})=_{_{k}_{k}}[ _{k}(_{k},_{k})],\] (1)

to minimize the expected local loss of client \(k\) on local dataset \(_{k}\) and \(_{k}\) is the unlabeled data. In traditional FL settings, it's assumed that \(\{_{k}\}\) are identical and gain the global model using \(=_{k=1}^{N}p_{k}_{k}\), where \(p_{k}\) is the weight of \(k\)-th client. But in real-world cross-device scenarios, each client might have a unique model and the architecture of the model might be different, which means that traditional aggregation methods are not available. We assume that \(_{k}\) is not similar to others, and use \(\{_{k}\}\) to collaborate in training the larger global model \(\) in server. Here we define the global update function is \(_{t}=(_{t-1};_{1},,_{N})\). Our final aim is to optimize the global goal

\[_{}F_{global}()=_{_{global}}[ (,)],\] (2)

where \((,)\) is global loss function in server and \(\) is the unlabeled data sampled from global dataset.

## 4 Designed FedMKD Method

We propose a multi-teacher knowledge distillation based federated self-supervised learning framework _FedMKD_, which is shown in Fig. 2. In _FedMKD_, besides the local self-supervised learning (Sec. 4.1),

  
**Method** &  **Global** \\ **model** \\  &  **Global** \\ **size** \\  &  **Model** \\ **Heterogenicity** \\  &  **Deviated** \\ **representation ability** \\  &  **Inconsistent** \\ **representation space** \\  & 
 **Theoretical** \\ **Analysis** \\  \\  FedU  & ✓ & = Client & ✗ & ✗ & ✗ & ✗ \\ FedEMA  & ✓ & = Client & ✗ & ✗ & ✗ \\ L-DAWA  & ✓ & = Client & ✗ & ✗ & ✗ & ✗ \\ FedX  & ✓ & = Client & ✗ & ✗ & ✗ & ✗ \\ FedCA  & ✓ & = Client & ✗ & ✗ & ✓ & ✗ \\ FLPD  & ✓ & = Client & ✓ & ✗ & ✗ & ✗ \\ \(FedU^{2}\) & ✓ & = Client & ✗ & ✗ & ✓ & ✓ \\ FedFoA  & ✗ & - & ✓ & ✗ & ✓ & ✗ \\ Hetero-SSFL  & ✗ & - & ✓ & ✗ & ✓ & ✓ \\
**FedMKD(ours)** & ✓ & \(\) Client & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of federated self-supervised learning methods.

we design a multi-teacher adaptive knowledge integration distillation module to adaptive determine the weight of the representations from heterogeneous local models with deviated representation abilities. The distilled loss combined with the global self-supervised loss, we can gain the weighted combined loss to update the global model, so that the global model can encode all classes from clients in a unified space (Sec. 4.2). And the global knowledge anchored alignment could improve the representation capability of clients and further benefit the global model training (Sec. 4.3). In addition, we provide the theoretical analysis of our algorithm in Appendix B.

### Self-supervised model training

Each client performs self-supervised contrastive learning using an asymmetric Siamese network, inspired by BYOL . The model \(\) comprises an online encoder \(\) and a target encoder \(\), both sharing the same architecture, with the online network incorporating an additional predictor \(p\). That is \(=\{p(()),()\}\). Given an unlabeled image \(x\), we can obtain two augmented views, \(v\) and \(v^{}\), serving as inputs to online and target networks, respectively. The loss function is defined as follows:

\[_{self}=\|-}{ \|r^{}\|}\|^{2},\] (3)

where \(r=(v)\) and \(r^{}=(v^{})\). This loss encourages the online network to produce representation \(p(r)\) that is similar to the positive sample generated by the target network \(r^{}\). We then exchange the views, feeding \(v^{}\) to the online network and \(v\) to the target network, to compute \(^{}_{self}\). At each training step, we use stochastic gradient descent to minimize \(}_{self}=_{self}+^{}_{self}\) to update the online network \(\) alone,

\[-_{}}_{self}.\] (4)

The target network helps to provide regression targets to train the online network. Choosing \(\) as the target decay rate, we employ the exponential moving average (EMA) of the online network to update \(\):

\[+(1-).\] (5)

Using this self-supervised training method, the model learns intricate representations from unlabeled data, capturing high-level features and patterns inherent in the dataset.

### Multi-teacher adaptive knowledge integration distillation.

In contrast to homogeneous federated learning, the presence of model heterogeneity poses a challenge: direct aggregation of local models into a global model is not feasible. To overcome this, we design a multi-teacher knowledge distillation mechanism to transfer local knowledge to the server.

Figure 2: The overall framework of _FedMKD_. Clients initialize the model architecture based on the local resource, then self-supervised train the local model using unlabeled local data. The server uses the multi-teacher adaptive knowledge integration distillation to aggregate positive local knowledge to train the global model and then updates local models again according to the alignment module.

Given a batch of data \(\), the representation from the teacher model is denoted as \(r_{t}\) and that from the student model as \(r_{s}\), the knowledge distillation loss is defined as follows:

\[L_{distill}=-log,r_{t,i})/)}{exp(sim(r_{s,i},r_{t,i})/ )+_{k\{-i\}}exp(sim(r_{s,i},r_{s,k})/)},\] (6)

where \(\) is the temperature parameter controlling entropy and \(sim()\) is the similarity function between two representations.

Then we extend this knowledge distillation learning method to multi-teacher. Although the data is heterogeneous, the knowledge of each local model is valuable, each local model captures the unique characteristics of local data. Our goal is to integrate the positive knowledge of all clients to guide the global model in learning a general representation of unlabeled data. We design a multi-teacher adaptive knowledge integration distillation that can adaptively weigh the representations from clients.

Given a sample \(x_{i}\), representation from the \(n\)-th local model is \(R_{n,i}^{d}\) where \(d\) is the dimension of the representation. Following , a fully connected layer is employed to project the representation into a lower-dimensional space, enhancing the discriminate power of the learned representations. So, we map the representation from the global model \(R_{s,i}\) into the same lower latent space, obtaining

\[r_{s,i}=g(R_{s,i}),r_{n,i}=g(R_{n,i}),\] (7)

where \(r_{n,i},r_{s,i}^{k}\), \(k\) is the dimension of the new latent space and \(g()\) is the projector.

In addition, we introduce an adapter module to learn instance-level teacher importance weights for knowledge integration. After getting \(r_{n,i}\), an attention block is used to generate the weighted sum of them. In this context, representation from global model \(r_{s,i}\) is treated as the _query_, while those from local models \(=[r_{1,i},r_{2,i},,r_{N,i}]^{T}\) is treated as the _key_ and _value_. Treating representations from the global model as _query_ ensures consistency in knowledge transfer. The attention mechanism computes attention scores to understand the relevance of each local model's representation to the global model's query. The aggregated representation is:

\[_{i}=Attn(r_{s,i},)=softmax(}{ {k}}),\] (8)

where \(_{i}\) means the aggregated representation, and \(Attn()\) denotes the attention block.

Returning to the knowledge distillation for unlabeled data proposed earlier, we treat the aggregate representation as the positive sample, and the remaining samples in the batch as the negative sample. The adaptive weight multi-teacher knowledge distillation loss function is expressed as follows:

\[L_{distill}=-log,_{i})/)}{exp(sim(r_{s,i}, _{i})/)+_{j i}exp(sim(r_{s,i},r_{s,j})/)}.\] (9)

Above all, the weighted combined loss for the global model is presented as:

\[L_{server}=L_{self}+ L_{distill},\] (10)

where \(\) is a hyper-parameter controlling the weight of the distillation process.

### Global knowledge anchored alignment

As we mentioned, the representations from different models are inconsistent and the representation abilities of models are also deviated. So we introduce the global knowledge anchored alignment mechanism that each local model uses the global model as an anchor. It ensures that the local representation spaces are closer to the global ones.

Unlike methods such as FedX  and MOON , which align local models to the global model locally, our approach aims to train a better global encoder tailored for resource-constrained federated learning scenarios. Those methods are not available when clients cannot afford to store or infer the global model locally. So we transfer this alignment process to the server.

After finishing the global model training, we construct local twin models in the server to realize the alignment under global view. Here, we use the global online network and local online network to construct a new asymmetric siamese network called the twin of the original local model. The local online network \(_{n}\) is the online model, and the global online network \(_{s}\) is the target model, that is,

\[\{^{}_{n},^{}_{n}\}\{_{n},_{s}\}\] (11)

and \(=\{^{}_{n},^{}_{n}\}\). The training loss is updated to

\[_{align}=-log_{n}(i),^{}_{ n}(i))/)}{_{i B}exp(sim(^{}_{n}(i),^{}_{n}(i))/ )}.\] (12)

According to the idea of contrastive learning, the representations learned by the online network become more consistent with the knowledge captured by the target network. This global knowledge anchored contrastive learning suggests that the global model's knowledge is used as a positive example for the local model to train itself, thus making it more consistent with the target global network. Aligning local models with the global view representation helps create a comprehensive understanding of the overall data distribution. The process refines the knowledge acquired locally, ensuring that it contributes meaningfully to the overall federated learning process. Then, we use stochastic gradient descent to minimize \(_{align}\) to update the \(^{}_{n}\),

\[^{}_{n}^{}_{n}-_{align}.\] (13)

Once the global model anchored alignment is finished, the server will send the online network of the local twin network \(^{}\) to the corresponding client to update the local model. The local online network \(_{n}\) is replaced by the server-updated network \(^{}_{n}\). The target network is not replaced to retain more local knowledge and stabilize model training:

\[\{_{q,n},_{q,n}\}\{^{}_{q-1,n},_{q-1,n}\}\] (14)

so that, the local model can benefit from the alignment process and align to the representation under global view, which can further use the local data to train the model.

## 5 Experiments

In this section, we evaluate the representations learned from our proposed global model _FedMKD_ on _CIFAR-10_ and _CIFAR-100_. We first describe the experimental setup and baselines, and then analyze the performance in comparison to other methods. Due to the space limitation, further hyperparameter analysis and communication cost analysis are represented in Appendix D.

### Experimental setup

We use _CIFAR-10_ and _CIFAR-100_ datasets to train all the models. Both of them contain 50,000 training images and 10,000 testing images. To construct the public dataset, we sample 4000 data samples from the training set, then divide the remaining data into \(N\) partitions to simulate \(N\) clients.

Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data distribution of clients is IID.

To assess the validity of the public dataset, we use two sampling methods to construct it. First, we use a random sampling method over all classes to generate public dataset 'IID'. And for the public dataset 'Partial', data is selected randomly from 40% classes in two datasets.

We utilize three settings to simulate heterogeneous data distributions among all the clients. For the IID setting, each client contains the same number of samples from all classes. For the class setting, each client only has \(10/N\) and \(100/N\) classes on two datasets and the classes between clients have no overlap. For the non-IID setting, data heterogeneity levels are described by the Dirichlet distribution Dir(\(\)) , where smaller \(\) represents stronger heterogeneity levels, here we choose \(=0.5\).

Regarding the self-supervised learning framework design within each client, we use ResNet18  and VGG9  as the encoder network and Multi-Layer Perception (MLP) as the predictor. In order to construct the model heterogeneous setting, 2 clients train the Resnet18 encoder while 3 clients use the VGG9. And for the global representation model, Resnet18 is selected as the encoder in server.

### Baselines and evaluation methods

Firstly, we select several federated knowledge distillation frameworks _FedMD_, _FedDF_, _FedET_, _MOON_, _MOON-KL_ that use unlabeled public dataset for distillation. We then replaced the local model with a self-supervised model to evaluate the process of knowledge distillation in our method. And several federated self-supervised learning frameworks _FedU_, _FedEMA_, _HeterOSFL_ are also chosen as baselines. To verify the client's knowledge can improve the global model, we also train the global model separately on the public dataset, denoted as _Std. ResNet18_. Following _FedEMA_, we evaluate the performance of learned representations using linear and semi-supervised evaluation. Due to limited space, please refer to Appendix C for more details.

### Performance Evaluation

Table 2 and 3 shows the linear evaluation results and semi-supervised evaluation results of _FedMKD_ compared with all the baselines on _CIFAR-10_ and _CIFAR-100_. We can gain the following observation.

On the whole, our _FedMKD_ outperforms all baselines under different public dataset settings and different data heterogeneity level settings on both two datasets. Compared to the second-best results, _FedMKD_ achieves significant improvement. On average, our model improves _CIFAR-10_ by 4.22% and _CIFAR-100_ by 5.31% under linear evaluation and gain 3.66% and 2.07% improvement on two dataset under semi-supervised evaluation.

The effectiveness of our multi-teacher adaptive knowledge integration distillation can be approved when compared with _FedMD_, _FedDF_, _MOON-KL_ and _MOON_. Although these methods all designed

   &  &  &  \\   & & **Class** & **Dir(\(\)=0.5)** & **IID** & **Class** & **Dir(\(\)=0.5)** & **IID** \\  Std. ResNet18 &  & \(51.09 0.04\) & \(46.21 0.02\) & \(23.25 0.05\) & \(22.46 0.04\) & \(23.20 0.03\) \\ FedMD & & \(46.94 0.04\) & \(48.04 0.02\) & \(48.74 0.08\) & \(23.07 0.03\) & \(27.73 0.03\) & \(21.57 0.01\) \\ MOON-KL & & \(44.93 0.05\) & \(45.84 0.03\) & \(46.51 0.03\) & \(21.26 0.03\) & \(21.34 0.01\) & \(21.82 0.02\) \\ MOON & IID & \(53.35 0.03\) & \(53.71 0.04\) & \(55.14 0.02\) & \(27.82 0.01\) & \(26.84 0.03\) & \(26.70 0.03\) \\ FedET & \(56.42 0.02\) & \(59.38 0.03\) & \(61.43 0.02\) & \(29.11 0.03\) & \(26.98 0.01\) & \(24.48 0.02\) \\ FedU/FedEMA & - & - & - & - & - & - \\ Hetero-SSFL & \(59.13 0.02\) & \(64.04 0.04\) & \(65.61 0.07\) & \(30.84 0.10\) & \(29.63 0.06\) & \(28.89 0.06\) \\ FedMKD & **64.81 \(\) 0.02** & **66.98 \(\) 0.06** & **69.07 \(\) 0.04** & **36.33 \(\) 0.01** & **35.95 \(\) 0.07** & **35.94 \(\) 0.02** \\  Std. ResNet18 &  & \(50.15 0.02\) &  &  &  &  \\ FedMD & & \(47.16 0.03\) & \(46.39 0.02\) & \(45.93 0.03\) & \(23.95 0.05\) & \(23.14 0.03\) & \(22.47 0.01\) \\ FedDF & & \(52.59 0.05\) & \(53.50 0.03\) & \(54.17 0.05\) & \(27.21 0.02\) & \(27.31 0.04\) & \(27.05 0.04\) \\ MOON-KL & & \(46.41 0.03\) & \(46.81 0.03\) & \(45.89 0.01\) & \(21.73 0.01\) & \(20.97 0.03\) & \(22.27 0.04\) \\ MOON & Par. & \(54.31 0.04\) & \(54.54 0.02\) & \(52.94 0.04\) & \(27.00 0.04\) & \(27.27 0.01\) & \(28.26 0.02\) \\ FedET & & \(57.75 0.01\) & \(57.08 0.01\) & \(58.59 0.01\) & \(29.38 0.01\) & \(28.12 0.02\) & \(29.61 0.01\) \\ FedU/FedEMA & - & - & - & - & - & - \\ Hetero-SSFL & \(63.20 0.08\) & \(61.93 0.04\) & \(61.15 0.07\) & \(30.94 0.05\) & \(29.92 0.03\) & \(29.56 0.03\) \\ FedMKD & & **66.39 \(\) 0.09** & **67.60 \(\) 0.04** & **65.88 \(\) 0.03** & **35.82 \(\) 0.02** & **35.55 \(\) 0.05** & **34.38 \(\) 0.02** \\  

Table 2: Top-1 accuracy comparison under linear probing on CIFAR datasets with best model performance in bold and second-best results with underlines. ’-’ means this method is not suitable for the experiment setting.

new federated knowledge distillation frameworks based on unlabeled public dataset, since the original local model is supervised, they prefer using the class information from logits to distill. When observing the result of _FedET_, we find that although it also designs a larger global model in server which improves the model a lot, the final result is not satisfied. This is also because it designs a distillation method based on knowledge of probability distribution over classes. Next, compared with the federated self-supervised method, our _FedMKD_ also achieves better performance. _FedU_ and _FedEMA_ are not applicable in model heterogeneity setting, so we cannot evaluate their effectiveness. _Hetero-SSFL_ gain the best performance among all baselines but is worse than ours. That's because it aims to train personalised client models. Only the alignment module cannot hold the inconsistent representation space perfectly. But our global model can directly generate representation from the global model, it avoids using the representation from inconsistent clients.

Apart from the client data heterogeneity, we consider the influence of the public dataset distribution. Here, we construct two public datasets, one is 'IID' to the whole data distribution and the other only has partial classes. In both two settings, our _FedMKD_ also gets the best performance. The overall performance of 'IID' public dataset is better than 'Partial' setting. That's because the global model adapts the self-supervised learning on public dataset, and the diversity of the sample is important, which can help the model explore a wide range of features and patterns present in the data. And it's observed that when the public dataset is 'IID', the performance increases with the decrease of the data heterogeneity level for _CIFAR-10_, but it doesn't apply to the _CIFAR-100_. It's because there are too many classes in _CIFAR-100_ and the number of samples in each class is not efficient.

In order to evaluate the effectiveness of _FedMKD_, we use the dimensionality reduction algorithm t-sne to visualize the representation on the test dataset of _CIFAR-10_ from different encoders. As shown in Fig. 3 (b)(c)(d), the global models in _FedMKD_ trained on both 'IID' and 'Partial' public datasets both achieve better clustering results than Standalone training and _MOON_. These results further verify our model can gain better generalized representations although the representation spaces of clients are inconsistent. There's also an averaged global model in _MOON_, but it cannot tackle the problem of inconsistent spaces well using the average method, so it only gains a poor clustering performance. Additionally, the performance on 'IID' public dataset is better than 'partial' ones from the observation of cluster performance. This suggests that the number of classes seen by the global model also affects how well the global model can encode all classes in a unified space. Comparing the class distributions in Fig. 3 (c) and (d), we can find that although these two global models are trained on different public datasets, the final cluster layout is similar, which can further prove that our global model can encode all classes from clients even if it never sees some classes during the training.

    &  &  &  \\   & & **Class** & **Dir(\(\)=0.5)** & **IID** & **Class** & **Dir(\(\)=0.5)** & **IID** \\  Std. ResNet18 &  & \(44.02 0.18\) & \(44.66 0.20\) & \(15.88 0.17\) & \(14.94 0.19\) & \(15.34 0.12\) \\ FedMD & & \(43.60 0.44\) & \(44.13 0.16\) & \(44.80 0.40\) & \(14.39 0.20\) & \(13.06 0.14\) & \(12.90 0.07\) \\ MOON-KL & & \(45.42 0.26\) & \(46.61 0.21\) & \(46.72 0.15\) & \(16.25 0.06\) & \(17.22 0.25\) & \(16.07 0.04\) \\ MOON & IID & \(49.96 0.24\) & \(50.21 0.10\) & \(51.78 0.28\) & \(19.23 0.12\) & \(17.21 0.13\) & \(17.07 0.18\) \\ FedET & \(52.37 0.24\) & \(56.57 0.17\) & \(57.44 0.13\) & \(19.70 0.08\) & \(16.82 0.20\) & \(15.68 0.18\) \\ FedU/FedEMA & - & - & - & - & - & - \\ Hetero-SSFL & \(54.30 0.15\) & \(58.73 0.54\) & \(60.50 0.12\) & \(20.04 0.40\) & \(19.19 0.17\) & \(18.82 0.17\) \\ FedMKD & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Std. ResNet18 &  &  &  &  &  \\ FedMD & & \(44.54 0.26\) & \(43.61 0.13\) & \(42.52 0.19\) & \(17.32 0.17\) & \(16.47 0.14\) & \(16.31 0.07\) \\ FedDF & & \(48.14 0.27\) & \(48.74 0.12\) & \(48.56 0.18\) & \(17.01 0.01\) & \(17.14 0.01\) & \(16.95 0.01\) \\ MOON-KL & & \(46.76 0.05\) & \(46.92 0.05\) & \(46.49 0.22\) & \(16.21 0.27\) & \(16.10 0.12\) & \(16.94 0.12\) \\ MOON & Par. & \(50.43 0.18\) & \(51.99 0.34\) & \(49.86 0.19\) & \(18.64 0.21\) & \(18.92 0.25\) & \(19.29 0.08\) \\ FedET & & \(52.75 0.07\) & \(52.61 0.03\) & \(54.64 0.14\) & \(18.49 0.12\) & \(18.16 0.13\) & \(18.01 0.22\) \\ FedU/FedEMA & - & - & - & - & - & - \\ Hetero-SSFL & \(59.95 0.34\) & \(58.31 0.50\) & \(58.35 0.20\) & \(20.72 0.14\) & \(20.30 0.32\) & \(19.62 0.08\) \\ FedMKD & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Top-1 accuracy comparison on 1% of labeled data for semi-supervised learning on CIFAR datasets with best model performance in bold and second-best results with underlines. ’-’ means this method doesn’t apply for the experiment setting.

### Improvement of clients

In _FedMKD_, the global knowledge anchored alignment module is used to align the client model in the server which can further transfer the global knowledge to the client and incentivize the clients to participate in the federated learning. To evaluate the improvement of the clients, the local models which are standalone training locally are compared with our local models. As shown in Fig. 4, the client performance in our _FedMKD_ framework is better than local standalone training, regardless of the architecture of the local model. Especially for clients with ResNet18, it improves 6.73% on average. It's concluded that clients benefit from federated training by contributing to global training.

### Validation of inconsistent representation spaces

As we mentioned, the representation spaces between different clients are inconsistent because of data heterogeneity. To validate this opinion, we use Linear Discriminant Analysis (LDA) to reduce dimensionality in order to visualize the distribution of the representations. Here the data distribution of clients is Class. In Fig. 5 left, '\(\)' and '\(\)' denote representations of Client A and Client B, respectively. And different colors denote different classes. We can observe that in Fig. 5 left the classes 'cat' and 'dog' almost overlap while they are from different clients, which verifies that the inconsistent representation spaces did exactly exist. And Fig. 5 right shows the visualization result of the representation from the global model. It's clear that the classes 'cat' and 'dog' are embedded in different positions in global space, which proves that although the local representations are inconsistent, our global model can learn a good representation.

### Ablation experiment

In order to investigate the effectiveness of different parts of _FedMKD_, we design these comparison experiments:

* _Standalone training_: The global model is trained alone using the public dataset, without the knowledge from client models.
* _FedMKD\({}_{KL}\)_: The global distillation function is replaced by the KL-divergence function to measure the similarity of the aggregated representation \(\) and global representation \(r_{s}\).
* _FedMKD\({}_{w/o\ adaptive}\)_: The adaptive knowledge integration module is removed, each local representation has the same weight to generate the aggregated representation.

    &  &  \\  & **Class** & **IID** & **Class** & **IID** \\  Standalone training &  &  \\ FedMKD\({}_{KL}\) & \(43.88 0.19\) & \(46.24 0.08\) & \(19.56 0.18\) & \(15.45 0.05\) \\ FedMKD\({}_{w/o\ adaptive}\) & \(46.14 0.05\) & \(47.29 0.08\) & \(21.77 0.02\) & \(22.76 0.03\) \\ FedMKD\({}_{w/o\ alignment}\) & \(61.85 0.04\) & \(62.34 0.09\) & \(34.97 0.04\) & \(29.91 0.02\) \\ FedMKD & \(\) & \(\) & \(\) & \(\) \\   

Table 4: Experimental results on ablation studies of FedMKD with best model performance in bold.

Figure 4: Improvement of clients after involving our proposed _FedMKD_.

Figure 5: LDA visualizations of hidden vectors from different models on CIFAR-10.

* _FedMKD\({}_{w/o\ alignment}\)_: The global knowledge anchored alignment module is removed from _FedMKD_.

Above experiment is conducted on the both _CIFAR-10_ and _CIFAR-100_ dataset, using the IID public dataset. The results of the ablation study experiment are shown in Table 4. When comparing _FedMKD_ with _FedMKD\({}_{KL}\)_, significant performance drop can be observed when the knowledge distillation function is replaced by the KL-divergence function. Thus we can conclude that for self-supervised learning, we prefer performing knowledge distillation based on representation, but the KL-divergence cannot capture the distribution characteristics from them. Therefore, the appropriate distillation method is critically important in self-supervised learning due to the lack of labels. And a worse performance on both two datasets can be observed when we use the equal weight instead of the adaptive weight. Because the client models are heterogeneous, their representation capabilities are different and the representation spaces are also inconsistent, so intuitively average representations may reduce the information contained in the representation. Finally, when we remove the alignment module, the performance under each setting all decreases, which demonstrates that the alignment is not only beneficial to the local models, but also improves the whole training process.

### Scalability of algorithm

In order to explore the scalability of our proposed algorithm _FedMKD_, we add the experiment that the number of clients is 5, 10, 30 on _CIFAR-10_, and 40% clients use the VGG model and 60% use ResNet18. And we repartition the data for each client under Dir(\(=0.5\)) and set the public dataset distribution as 'IID'. The results are shown in Table 5. We can find that as the number of clients increasing, the performance decreases. The reason is that the total number of data is fixed, if the number of clients increases, the number of data in each client will decrease, which further affect the performance of the local model.

## 6 Conclusion

In this work, we focused on how to solve the deviated representation abilities and inconsistent representation spaces caused by the heterogeneous architectures and class skew in federated self-supervised learning. We proposed a multi-teacher knowledge based federated self-supervised learning framework _FedMKD_ to learn a global model. Firstly, the adaptive knowledge integration module could learn high-quality representation knowledge from heterogeneous models. And the combination of the self-supervised loss and the distillation loss enabled the global model to encode all classes from clients in a unified space. Then a global knowledge anchored alignment module improved the local representation models in server and fed it back to corresponding clients. The experiments conducted on two datasets demonstrated that our proposed _FedMKD_ was state-of-the-art and outperformed existing methods.