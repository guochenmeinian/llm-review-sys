# Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets

Wolfgang Boettcher\({}^{1}\) Lukas Hoyer\({}^{2}\) Ozan Unal\({}^{2}\) Jan Eric Lenssen\({}^{1}\) Bernt Schiele\({}^{1}\)

\({}^{1}\) Max Planck Institute for Informatics, Saarland Informatics Campus, Germany

\({}^{2}\) ETH Zurich, Switzerland

{wolfgang.boettcher, jlenssen, schiele}@mpi-inf.mpg.de

{lhoyer, ouenal}@vision.ee.ethz.ch

###### Abstract

In this work, we introduce _Scribbles for All_, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, _Scribbles for All_ provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All.

## 1 Introduction

Semantic segmentation is one of the most crucial tasks for computer vision research and a key component to scene understanding in many applications. While substantial advancements have been made in designing highly accurate segmentation architectures [25; 6; 8; 7; 17], those models heavily rely on detailed labels. The crafting of such dense labels constitutes a laborious and resource-intensive process. This limitation impedes the availability of specialized datasets and the practical deployment of vision algorithms in real-world scenarios. It is particularly pronounced in self-driving applications, which demand immense amounts of training data [2; 32; 9] or in domains with fine-grained classes [24; 54]. Those settings necessitate models capable of dealing with complex inter-object relations, varying shapes, and scales. Even in the current era of large pre-trained foundation models [27; 19], fine-tuning these models on custom, labeled data still remains a necessity for many applications [48; 4; 42].

One popular approach to addressing the issue of label cost is weakly supervised semantic segmentation (WSSS) [3; 49; 43] which has made significant progress in recent years. WSSS methods use incomplete labelling in the form of image level labels  or bounding boxes [18; 26; 50] to labelobjects. Similarly, Sparsely Annotated Semantic Segmentation (SASS) is defined by labelling a subset of the image pixels through coarse labels [11; 10], labelling every object through points  or drawing scribbles [23; 37]. Previous research has shown that especially scribbles are a promising means of attaining cheap yet powerful labels [23; 40]. While they take only slightly longer for the annotator to label than points, they have been shown to enable stronger segmentation results [45; 21]. Annotating full segmentation images takes several minutes for object centric datasets like Pascal  or even hours for more complex driving scenes [9; 30] while labelling ScribbleSup  took on average about 25 seconds. Moreover, state-of-the-art methods gain 7-10% mIoU on PascalVOC  for scribble labels over point labels [31; 45; 21]. This success has lead to a series of promising training methods for SASS with scribbles [13; 39; 44].

However, currently, there exists only one popular segmentation dataset with scribble labels, namely ScribbleSup, introduced by Lin et al.  for the PascalVOC dataset. An example image of it is shown in Fig. 2. Two challenges arise for the research area of scribble-supervised segmentation methods. Firstly, generalization of methods to other datasets cannot be verified. Secondly, PascalVOC is too easy to serve as the sole benchmark for scribble-supervised methods as visualized in Fig. 1. It consists mostly of images with one object class and the background class. By learning precise class boundaries of the dominant background class, a model can already achieve high performance while the challenge of learning object-to-object boundaries is less relevant. In contrast, modern semantic segmentation faces additional challenges such as small object instances (e.g. poles in Cityscapes) or a large number of semantic classes (e.g. 150 classes in ADE20K), which cannot be properly benchmarked with PascalVOC (see Fig. 1).

We present an algorithm that derives scribble labels from fully labeled datasets which are functionally equivalent to hand-drawn scribbles allowing us to bring scribble-supervision to a variety of popular and challenging segmentation datasets. Our datasets enable future research on segmentation methods trained or fine-tuned on scribble labels. Explicitly, we extend the applicability of scribble-based segmentation methods to the broad range of available datasets such as Cityscapes , ADE20K , KITTI360 , and more. Our main contributions can be summarised as follows:

* We present an automatic scribble generator for any fully labeled segmentation dataset, enabling future research into state-of-the-art models for scribble-supervised segmentation.
* We introduce s4Pascal, s4KITTI360, s4Cityscapes and s4ADE20K, four automatically generated datasets with scribble labels.
* We benchmark state-of-the-art segmentation methods on our datasets, showing that models trained with our scribbles perform on-par with models trained on manually created scribbles.

Figure 1: **Visual difference in scribble-supervised performance – While predictions from scribble supervised models are almost identical to fully supervised models for PascalVOC, the quality of segmentation for scribble supervised Cityscapes models is visibly poorer (see dotted boxes), highlighting the greater complexity of the dataset and the need for further research.**

## 2 Related Work

In this section, we provide a structured overview of the research that is of particular relevance for our work and the current state-of-the-art with respect to scribble labelled datasets.

**Weakly Annotated Semantic Segmentation (WASS)** Methods for WASS have been trained using image-level labels [29; 49; 43] or bounding-box labels [18; 26]. While image-level labels are fast to obtain, they suffer from the lack of pixel-level information, which renders them unsuitable for complex scenes. Bounding-boxes offer spatial supervision on individual objects but fail to deal with overlapping, not box-aligned objects. In contrast to WASS labels, scribbles provide a better supervision signal and are cheaper to obtain than bounding boxes . The latter suffer from added challenges such as the overlap of boxes.

**Sparsely Annotated Semantic Segmentation (SASS)** Related to supervision with weak labels, SASS is concerned with using labeled pixel subsets, allowing direct supervision on sparse regions of the image. The two main label types for SASS are point labels  and scribbles , which led to several follow-up representations . Depending on the requirements and domain, those other labelling strategies are also used, such as coarse annotation [11; 10]. The latter comes with the benefit of being more expressive but also requires more effort. Scribbles have been demonstrated to be a Pareto-optimal choice between labelling effort and segmentation quality. Unal et al.  introduce ScribbleKITTI for SemanticKITTI and demonstrate that scribbles can lead to approximately supervised performance in the 3D domain. Scribbles, traditionally, require a human annotator, which entails the need for further resources to provide the respective labels, hindering model research and development. In this work, we propose a method to automatically generate scribble labels from dense 2D annotations, providing excellent benefits for the development of future SASS methods.

**Training SASS models.** In contrast to a dense semantic label, the labels used for SASS do not provide information on the object outline, leading to challenges in class boundary estimation . Several methods utilise auxiliary tasks such as edge detection  or saliency  to improve performance. However, those methods tend to suffer from model errors in the auxiliary task, which limit their prediction capabilities. Other approaches [34; 33; 37] use regularised losses that model interdependencies of the labeled and unlabeled pixels. Often, these are combined with CRFs , which are adapted for region growing on the pseudo labels generated by the model and for overall refinement of the predictions [49; 41; 44]. For example, URSS  addresses the inherent model uncertainty in semi-supervised training by applying random walks and consistency losses in a self-training framework. Similarly, Valvano et al.  uses self-training but with multi-scale consistency while TEL  achieve performance increases by introducing a similarity prior through a novel tree-based loss. Most recently, SASformer  utilises the attention mechanisms in transformer architectures, using global dependencies to achieve more accurate segmentation results, marking the trend away from auxiliary tasks and two-stage approaches towards end-to-end trainable frameworks. To validate our automatic scribble labels, we train and evaluate a subset of these recent methods on our datasets and show performance comparative to manual labelling.

**Scribble Annotation Datasets** We present the second scribble image dataset so far, only preceded by ScribbleSup . ScribbleSup only contains labels for the older PascalVOC  dataset. In contrast, we provide labels for a larger set of currently relevant datasets and, additionally, a method for the automatic generation of scribbles for any fully-annotated dataset.

## 3 Automated Scribble Generation

This section introduces our automatic scribble generation method, before the individual generated datasets are described in Sec. 4. We begin by shortly outlining design objectives in Sec. 3.1, before describing the detailed algorithm in Sec. 3.2.

Figure 2: **Overview of label types – Left to right: Full PascalVOC semantic label, scribble labels created by our scribble generation algorithm for s4Pascal, hand-drawn scribble labels from ScribbleSup.**

### Design Objectives

The presented scribble generation algorithm takes an image with corresponding dense segmentation labels as input and produces a single scribble, represented as a set of points, for each object in the image. We formulate the following design objectives:

1. **Mimic human annotations.** The generated scribbles should approximately resemble scribbles that human annotators draw. Specifically, they are supposed to be more coarse for larger, simple geometries and more precise for detailed objects, as would be the case with hand-crafted labels. Also, the scribble is expected to go roughly through the centre part of an object and not to come too close to its margins for a large portion of its length.
2. **Probabilistic generation.** The generation of labels should occur in a probabilistic fashion to prevent mean collapse when confronted with similar shapes, maintaining enough variance in the labelling process.
3. **No boundary violation.** We also apply hard constraints that prevent scribbles from violating any class boundaries.

We design the algorithm described in Sec. 3.2 to reach these objectives.

### Scribble Generation Algorithm

This section describes the scribble generation algorithm, as detailed in Alg. 1 and visualized in Fig. 3.

**Preprocessing.** The algorithm commences by separating the semantic mask \(GT_{mask}\) of an image by \(C\). Then, we apply class-wise connected component analysis to obtain separate masks for each instance of the respective class. Each object mask \(b B\) of \(J\) total objects masks is subsequently subjected to morphological erosion in an amount \(_{1}\) depending on the object area to comply with design objective 1 as shown in Fig. 3a). If an object is separated into multiple masks at this stage, each mask will be considered an individual object for further processing. Through this, we ensure that objects with complex non-convex shapes are properly labeled as well by generating multiple scribbles. If two objects overlap in a single connected component, they are considered as a single instance. This splitting procedure is only applied in the first erosion step. After the following ones (see below), smaller separated objects are removed instead.

**Polynomial fitting.** After the preprocessing procedure, the algorithm fits a curve through each obtained mask by iteratively repeating the following process until a valid scribble annotation is found or the blob is completely eroded: A fixed-rate \(_{2}\) binary erosion is applied. After that, the edge image of the blob \(e\) and its centre-of-mass (COM) \(c_{M}\) is calculated. In the case of strongly non-convex shapes, it can occur that the COM is not part of the image. If that happens, the object is subjected to a skeletonisation operation in the style introduced by Zhang et al.  and the closest point in terms of \(L_{2}\)-distance of the skeleton wrt. to the true COM is used as a COM substitute.

Hand-drawn scribbles usually follow the object's predominant direction up to some human noise factor. To imitate this behaviour, we randomly sample \(Q\) points on the object edges and select a pair \((p_{1},p_{2})\) with the maximum distance \(\|p_{1}-p_{2}\|_{2}\) by farthest point sampling. The pair spans the furthest distance of the object most of the time while retaining a chance of suboptimal solutions and

Figure 3: **Scribble Generation – a) Size dependent erosion, b) COM in red, sampling of points on the edge in green, determination of the approx. farthest pair in darker green and tentative scribble in blue c) Sampling of two extra points along the tentative scribble d) Fitting final scribble through points e) Scribble overlayed on initial segmentation map.**variance like in the case of a human annotator. Random sampling allows the algorithm to explore an extended solution space if the scribble generation takes multiple iterations for the object. The returned point pairs and the COM with added noise \(_{M}\) are utilised to obtain a second-order polynomial \(F_{2}\) as depicted in Fig. 2(b)), by solving the linear least-squares problem. We choose \(x\) as the coordinate with the largest distance between \(p_{1}\) and \(p_{2}\).

The next step improves the label variance of the scribble shapes and satisfies design objective 2. For this, two points \(p_{3},p_{4}\) are sampled from \(F_{2}\), adding area-dependent Gaussian noise. As the final step, \(_{M},p_{1},...,p_{4}\) are used to find a 4th-order polynomial \(F_{4}\) as shown in Fig. 2(d)).

If the curve is entirely within the blob and does not violate any class boundaries (objective 3), the label is considered valid. Otherwise, the process of sampling from the edge and fitting curves is repeated up to \(N\)-times. If this does not yield a valid scribble, the algorithm is restarted.

``` Input:\(GT_{mask}_{0}^{m n}\) \(C_{mask}\)separatClass(\(GT_{mask}\)) \(\)\(C_{mask}_{2}^{C n}\) \(B\)componentAnalysis(\(C_{mask}\)) \(\)\(B_{2}^{m n}\) for\(b B\)do\(\)\(b_{2}^{m n}\) \(\)binaryErosion(\(b\), \(_{1}\)(area(b))) \(\)\(_{2}^{m n}\) while\(\)\(\)do\(\)binaryErosion(\(\), \(_{2}\)) \(e*)^{2}+(S_{y}*)^{2}}\)\(\)\(S\) - Sobel operator, \(e_{2}^{m n}\) \(c_{M}\)enterOfMass(\(\)) \(\)\(c_{M}_{2}^{2}\) if\(c_{M}\)then \(c_{M}\) skeletonCOM(\(,c_{M}\)) \(\) Closest pt. in \(L_{2}\)-dist. to COM in skeleton of \(\) endif for\(i N\)do \(\)\(Q_{+}\), \(P_{+}^{Q 2}\) \((p_{1},p_{2})\)furthest-pair(\(P\), \(L_{2}\)) \(\) max. dist. by \(L_{2}\)-norm, \(p_{1},p_{2}_{+}^{2}\) \(_{M}\)\(c_{M}+\), \((0,_{com})\) \(\)\(c_{M}_{+}^{2}\) \(_{0},_{1},_{2}_{_{0}, _{1},_{2}}||_{j\{p_{1},p_{2},_{M}\}}j_{y}-(_{0}+ _{1}j_{x}+_{2}j_{x}^{2})||^{2}\) \(F_{2}(x)=_{0}+_{1}x+_{2}x^{2}\) \(\)area(\(\))/\(\) \((p_{3},p_{4})\)sample(\(F_{2}(x),2\)) + \(\), \((0,)\) \(\) Sample from polynomial \(F_{2}\) \(_{0},_{1},...,_{4}_{_{ 0},...,_{4}}||_{j\{p_{1},...,4_{M}\}}j_{y}-(_{0}+...+ _{4}j_{x}^{4})||^{2}\) \(F_{4}(x)=_{0}+_{1}x+...+_{4}x^{4}\) if\(F_{4}(x),x[p_{1},p_{2}]\)then break endif endfor endfor return\(F_{4}(x),x[p_{1},p_{2}]\) ```

**Algorithm 1** Scribble Generation

## 4 Automatic Scribble Datasets

We now describe the datasets we created with the algorithm presented in Sec. 3.2. All datasets are publicly available and can be used to advance research in the area of scribble-supervised models.

**s4Pascal Dataset** The Pascal VOC 2012 dataset  is a widely used benchmark dataset in the field of object detection, image classification and semantic segmentation. It consists of images collected from various sources and covers 20 object classes and the background class. In terms of size, the Pascal VOC 2012 dataset for semantic segmentation contains 10,582 training images and 1,449 validation images when using the augmented version introduced by Hariharan et al. . The dataset's semantic mask includes a "do not care" label that is applied between class boundaries and fine-grained structures of the same class.

The scribble labels for PascalVOC were introduced by Lin et al.  as the _ScribbleSup_ dataset. Fig. 2 depicts a segmentation map and scribble labels from ScribbleSup on the right. Given that ScribbleSup is the only available dataset with hand-crafted scribble labels and provides dense semantic maps as well, it is the suitable reference to evaluate the synthetic scribble generation algorithm described in Sec. 3. Dense segmentation maps are required since those serve as the input for the synthetic scribble generation. The new synthetic scribble dataset is created using the dense labels of the Pascal VOC 21-class dataset. It is from now on referred to as _s4Pascal_. Label generation includes all object classes as well as the background class. The "do not care" areas of the segmentation maps are omitted. Therefore, those are also not valid points for the scribble generation algorithms, while the hand-crafted labels appear to have no clear policy on whether scribbles are allowed to intersect "do not care" regions. Overall, s4Pascal adds synthetic scribbles to the PascalVOC dataset, allowing for model training with three different types of segmentation labels.

In general, the scribble labels of ScribbleSup and s4Pascal are statistically very similar. As shown in Tab. 1 both contain approximately the same share of labeled pixels and exhibit similar closeness of scribbles very close to the class boundaries (\(_{10px}\)). The same is true for the overall spatial distribution of scribbles within the objects the label (\(_{20px}\), \(_{30px}\)) corresponds to. Being this similar, their visual appearance is close as well, as depicted in Fig. 2. Likewise, the class-wise distribution of scribbles depicted in Fig. 4 shows no significant aberrations when comparing the two label sets. Consequently, the scribbles generated by our scribble algorithm mimic the human-annotated labels closely. The only significant difference lies in the average number of scribbles used to label each image as shown in Tab. 1. This behaviour is explained by the human scribbles partially drawing over the don't care areas of the dataset which is prohibited for the algorithm and for more complex objects, the automatically generated labels may be broken down into multiple scribbles while the human may draw a single more complex line.

Importantly, as can be seen from Tab. 2 and as discussed in Sec. 5.2, the results obtained with ScribbleSup and s4Pascal are similar for various segmentation algorithms, further highlighting the similarity of our generated scribbles to human scribbles.

**Further Automatic Scribble Labeled Datasets** The lack of different scribble labeled datasets impedes thorough benchmarking of scribble-supervised methods under different domains. To alleviate this, we apply the scribble generation algorithm to a selection of popular segmentation datasets. Labeling these provides the foundation for benchmarking SOTA methods in the next step. We introduce _s4Cityscapes_ which is a set of scribble labels for the Cityscapes dataset  that is known for the broad range of object scales and object sizes it requires the segmentation model to learn. Due to the high level of detail in the data, the scribble algorithm is parameterized such that also small objects are labeled. Furthermore, we provide _s4KITTI360_ which contains scribble labels for KITTI360  which like Cityscapes is a self-driving domain dataset. In contrast to the latter, it contains a notably higher number of labeled images but a lower level of detail in ita annotations. Like Pascal, those datasets only contain a small number of object classes. It is also important to asses how models can cope with fine-grained classes. Thus, we further provide scribble labels _s4ADE20K_ for the ADE20K dataset , which consists of 150 classes.

The different properties of these datasets translate into different dataset statistics for the automated scribbles. While the number of scribbles per scene is similar for ADE20K and KITTI360, the share of labeled pixels is higher for ADE20K due to the lower image size as shown in Tab. 1. The dominance of furniture-related classes, doors, windows and other convex geometric objects further leads to s4ADE20K having scribbles with a relatively high distance to class boundaries. When looking at the two self-driving datasets, the higher level of detail and small objects in Cityscapes becomes apparent as the average number of scribbles per image is more than double that of KITTI360. The higher prevalence of slim objects such as poles and traffic-lights/signs moreover entails greater closeness to class boundaries. Further details on the class distribution are visualised in Fig. 5.

The datasets generated in this work are chosen to demonstrate the versatility and usefulness of the proposed scribble generator. The algorithm can be applied to all pre-existing datasets that contain dense segmentation maps, making it universally applicable. Further information on the algorithm runtime per dataset conversion is included in App. A.3.

## 5 Experiments

In this section, we perform evaluations on the proposed datasets. The section begins with providing implementation details in Sec. 5.1, before presenting segmentation experiments in Sec. 5.2, a scribble length ablation in Sec. 5.3, and a discussion about limitations in Sec. 5.4.

    & & &  & \\  & & & _SegFormer-B4 _ & & & _DeepLabV3+ _ \\  & Ours & _Sup._ & EMA & SASformer & _Sup._ & TEL  & AGMM  \\  ScribbleSup  & & \(86.6\) & \(79.2\) & \(}\) & \(84.6^{*}\) & \(76.4\) & \(76.8\) \\ s4PASCAL & ✓ & \(86.6\) & \(78.8\) & \(\) & \(84.6^{*}\) & \(76.8\) & \(73.8\) \\  s4Cityscapes & ✓ & \(83.8^{*}\) & \(\) & \(65.8\) & \(78.3^{*}\) & \(64.5\) & \(56.6\) \\ s4KITTI360 & ✓ & \(66.6\) & \(57.4\) & \(49.7\) & \(64.8^{*}\) & \(\) & \(49.6\) \\ s4ADE20K & ✓ & \(51.1^{*}\) & \(37.7\) & \(\) & \(46.8^{*}\) & \(39.6\) & \(35.4\) \\    & & & \\  & _Sup._ & EMA & SASformer & _Sup._ & TEL & AGMM \\  ScribbleSup & & - & \(91.5\) \% & \(91.8\) \% & - & \(90.3\) \% & \(90.7\) \% \\ s4PASCAL & ✓ & - & \(91.0\) \% & \(91.2\) \% & - & \(90.8\) \% & \(87.2\) \% \\  s4Cityscapes & ✓ & - & \(80.8\) \% & \(78.5\) \% & - & \(82.4\) \% & \(72.2\) \% \\ s4KITTI360 & ✓ & - & \(86.2\) \% & \(74.6\) \% & - & \(92.1\) \% & \(83.1\) \% \\ s4ADE20K & ✓ & - & \(73.8\) \% & \(80.4\) \% & - & \(84.6\) \% & \(69.3\) \% \\   

Table 2: **Quantitative comparison of SOTA methods on s4-datasets – Alongside the absolute performance expresssed in mIoU, we also list the segmentation results with respect to the fully supervised model (Sup.). The methods are grouped by their respective encoder backbone. The * marks values taken from literature.**

### Implementation Details

We evaluate the s4-scribble datasets using three current SOTA methods, namely Tree Energy Loss (TEL) , AGMM-SASS  and SASformer . Both ScribbleSup and s4Pascal are trained according to the information provided by the authors and the published code. Hyperparameters for s4ADE20K and s4Cityscapes were also kept at the values described in TEL and AGMM-SASS. For training ADE20K and Cityscapes on SASformer, we used the same learning rates as in TEL. For KITTI360 we found the hyperparameters for Cityscapes to be permissive. All models were trained on four RTX 8000 GPUs. All models were trained in a single-stage process without postprocessing such as applying CRFs. More information is provided in App. A.5.

Additionally, we also train on a simple mean-teacher [51; 47; 35] setup with a Segformer-B4  backbone to provide a naive WSSS baseline that does not make any specific prior assumptions like specialised losses, TEL, or architecture dependencies, SASformer. The training procedure was kept as published for SegFormer with additional augmentations for the student through CutOut  and AugMix . The loss is composed of an equally weighted supervised loss from the scribbles and a KL-divergence loss informed by the teacher.

### Baseline Scribble Datasets

As revealed by Tab. 2, the majority of three of the four methods used for comparing the ScribbleSup and s4Pascal datasets leads to very similar segmentation results differing 0.5 % mIoU or less. While EMA, and SAS perform better on ScribbleSup, TEL performs better on s4Pascal. The only method where the two label sets lead to different results is AGMM-SASS which shows a decline of approx. 3 % when trained with s4Pascal showing that some SASS methods are more susceptible to changes in label distribution than others. In conclusion, these results validate our observation from Sec. 4 that our scribble generation algorithms produce labels that are almost equivalent to human-created scribble labels. For class-wise evaluation, refer to App. A.4. Comparing scribble-supervised with fully-supervised models, the two methods with a SegFormer-B4 backbone show about 92 % relative performance, while the methods with ResNet101/DeepLabv3+ architectures both reach about 90 % relative performance as listed in Tab. 2. There is no relevant performance difference on ScribbleSup between the different methods that share the same backbone architecture, giving further evidence that the current benchmark is saturated.

This insight is supported by the results obtained from experiments on the s4ADE20K, s4Cityscapes and s4KITTI360 datasets. The typical relative performance drops to approx. 80 % as shown in Tab. 2. The more challenging datasets also lead to reduced mIoU values in absolute terms which is perceivable

Figure 6: **Qualitative performance on the s4-datasets and ScribbleSup – Shown are the input image overlaid with the corresponding scribbles, the EMA-model prediction and the ground truth. Color legends can be found in App. A.1.**on the examples shown in Fig. 6. Additionally, clear disparities between method performances can be observed as well as different best relative performances depending on the dataset. For instance, the high number of training images enables TEL to reach more than 90 % relative performance on s4KITTI360 while the best value for the similar but smaller dataset s4Cityscapes is approx. 10 % mIoU lower. We therefore hope that our datasets can facilitate research into scribble-labeled semantic segmentation models towards closing this performance gap.

### Scribble Length Ablations

Shrinking the initial scribbles to different proportions leads to varying deterioration of prediction results depending on the applied methods. Fig. 7 illustrates that TEL is the most robust towards reductions in scribble length dropping by less than 2 % mIoU as the scribble length is reduced to a tenth of the original size. More sensitive is SASformer with a decline of about 7 % mIoU. While not being the best-performing SOTA method for full scribble lengths and small reductions, TEL leads to better results for stronger degradations. The naive mean teacher shows similar behaviour to SASformer though less pronounced. These results illustrate the importance of scribble length ablations when benchmarking methods to aid in obtaining a more thorough understanding of how the developed methods react to label variations.

### Limitations

The proposed automatic scribble generation algorithm requires a dataset with existing semantic segmentation ground truth. As the purpose of this paper is a more broad evaluation of scribble-supervised methods on common segmentation scenarios, feasible basis datasets are available for many domains. However, for new or custom use cases, this assumption might not hold and manual scribble annotations can be necessary.

## 6 Conclusion

We presented new scribble annotations for four popular semantic segmentation datasets and a generally applicable method to generate those. Our work has shown that using more complex datasets reveals a widening gap between full supervision and scribble SOTA methods, compared to previous datasets. Furthermore, we demonstrated that different methods cope differently well with challenges such as shorter scribbles. Therefore, we suggest expanding the evaluation procedures for scribble-supervised segmentation to multiple datasets and also providing scribble-length ablations to show the robustness of the methods. We hope that our s4-datasets will drive a robust benchmarking of future scribble-supervised methods to close the gap to densely-supervised segmentation training. In particular, we see a strong potential in adapting vision foundation models with scribbles to custom applications.

Figure 7: **Effect of scribble length on prediction performance – The different methods exhibit varying robustness with respect to changes in the scribble length, evaluated here for the s4ADE20K dataset. The TEL method is less strongly affected than the naive EMA-model, while the effects on SASformer are the most severe. This highlights the importance of scribble length ablations for comprehensive method benchmarking.**