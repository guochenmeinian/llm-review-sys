# Causal Imitability Under Context-Specific Independence Relations

Fateme Jamshidi

EPFL, Switzerland

fateme.jamshidi@epfl.ch

&Sina Akbari

EPFL, Switzerland

sina.akbari@epfl.ch

&Negar Kiyavash

EPFL, Switzerland

negar.kiyavash@epfl.ch

###### Abstract

Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature. However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored. An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts. We consider the problem of causal imitation learning when CSI relations are known. We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard. Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient. Finally, we propose a sound algorithmic approach for causal imitation learning which takes both CSI relations and data into account.

## 1 Introduction

Imitation learning has been shown to significantly improve performance in learning complex tasks in a variety of applications, such as autonomous driving , electronic games [17; 41], and navigation . Moreover, imitation learning allows for learning merely from observing expert demonstrations, therefore circumventing the need for designing reward functions or interactions with the environment. Instead, imitation learning works through identifying a policy that mimics the demonstrator's behavior which is assumed to be generated by an expert with near-optimal performance in terms of the reward. Imitation learning techniques are of two main flavors: _behavioral cloning_ (BC) [42; 30; 24; 25], and _inverse reinforcement learning_1 (IRL) [26; 1; 37; 44]. BC approaches often require extensive data to succeed. IRL methods have proved more successful in practice, albeit at the cost of an extremely high computational load. The celebrated generative adversarial imitation learning (GAIL) framework and its variants bypass the IRL step by occupancy measure matching to learn an optimal policy .

Despite recent achievements, still in practice applying imitation learning techniques can result in learning policies that are markedly different from that of the expert [9; 7; 22]. This phenomenon is for the most part the result of a distributional shift between the demonstrator and imitator environments [11; 31; 12]. All aforementioned imitation learning approaches rely on the assumption that the imitator has access to observations that match those of the expert. An assumption clearly violated when unobserved confounding effects are present, e.g., because the imitator has access only to partial observations of the system. For instance, consider the task of training an imitator to drive a car, the causal diagram of which is depicted in Figure 0(a). \(X\) and \(Y\) in this graph represent the action taken by the driver and the latent reward, respectively. The expert driver controls her speed (\(X\)) based on the speed limit on the highway (denoted by \(S\)), along with other covariates such as weather conditions,brake indicator of the car in front, traffic load, etc. We use two such covariates in our example: \(Z\) and \(T\). An optimal imitator should mimic the expert by taking actions according to the expert policy \(P(X|S,Z,T)\). However, if the collected demonstration data does not include the speed limit (\(S\)), the imitator would tend to learn a policy that averages the expert speed, taking only the other covariates (\(Z\) and \(T\)) into account. Such an imitator could end up crashing on serpentine roads or causing traffic jams on highways. As shown in Figure 0(a), the speed limit acts as a latent confounder2.

One could argue that providing more complete context data to the imitator, e.g. including the speed limit in the driving example, would resolve such issues. While this solution is not applicable in most scenarios, the fundamental problem in imitation learning is not limited to causal confounding. As demonstrated by  and , having access to more data not only does not always result in improvement but also can contribute to further deterioration in performance. In other words, important issues can stem not merely from a lack of observations but from ignoring the underlying causal mechanisms. The drawback of utilizing imitation learning without considering the causal structure has been recently acknowledged, highlighting terms including causal confusion , sensor-shift , imitation learning with unobserved confounders , and temporally correlated noise .

Incorporating causal structure when designing imitation learning approaches has been studied in recent work. For instance,  proposed performing targeted interventions to avoid causal misspecification.  characterized a necessary and sufficient graphical criterion to decide the feasibility of an imitation task (aka _imitability_). It also proposed a practical causal imitation framework that allowed for imitation in specific instances even when the general graphical criterion (the existence of a \(\)-backdoor admissible set) did not necessarily hold.  proposed a method based on instrumental variable regression to circumvent the causal confounding when a valid instrument was available.

Despite recent progress, the potential benefits of further information pertaining to the causal mechanisms remain unexplored. An important type of such information is _context-specific independence_ (CSI) relations, which are generalizations of conditional independence. Take, for instance, the labor market example shown in Figure 0(d), where \(W\), \(E\), and \(U\) represent the wage rate, education level, and unemployment rate, respectively. The wage rate \(W\) is a function of \(E\) and \(U\). However, when unemployment is greater than \(10\%\), the education level does not have a significant effect on the wage rate, as there is more demand for the job than the openings. This is to say, \(W\) is independent of \(E\) given \(U\), only when \(U>10\%\). This independence is shown by a label \(U>0.1\) on the edge \(E W\). Analogously, in our driving example, the imitator would still match the expert's policy if there was heavy traffic on the route. This is because, in the context that there is heavy traffic, the policy of the expert would be independent of the speed limit. This context-specific independence between the speed limit \(S\) and the action \(X\) given \(T=1\) (heavy traffic) is indicated by a label (\(T=1\)) on the edge \(S X\) in Figure 0(c). This label indicates that the edge \(S X\) is absent when \(T=1\). The variables \(T\) in Figure 0(c) and \(U\) in Figure 0(d) are called _context variables_, i.e., variables which induce conditional independence relations in the system based on their realizations.

CSIs can be incorporated through a refined presentation of Bayesian networks to increase the power of inference algorithms . The incorporation of CSI also has ramifications on causal inference. For instance,  showed that when accounting for CSIs, Pearl's do-calculus is no longer complete for causal identification. They proposed a sound algorithm for identification under CSIs. It is noteworthy

Figure 1: Example of learning to drive a car. (a) Imitation is impossible, as the imitator has no access to the speed limit. (b) The car has automatic cruise control, which makes the expert actions independent of the speed limit. (c) Driver actions are independent of speed limit only in the context of heavy traffic. (d) A simple example of a CSI relation in economics.

that unlike conditional Independence (CI) relations that can be learned from data and are widely used to infer the graphical representation of causal mechanisms [35; 6; 10; 4; 19; 18], not all CSI relations can be learned from mere observational data. However, several approaches exist for learning certain CSIs from data [21; 16; 32; 23]. In this paper, we investigate how CSIs allow for imitability in previously non-imitable instances.

After presenting the notation (Section 2), we first prove a hardness result, namely, that deciding the feasibility of imitation learning while accounting for CSIs is NP-hard. This is in contrast to the classic imitability problem , where deciding imitability is equivalent to a single d-separation test. We characterize a necessary and sufficient graphical criterion for imitability under CSIs under a structural assumption (Section 3) by leveraging a connection we establish between our problem and classic imitability. Next, we show that in certain instances, the dataset might allow for imitation, despite the fact that the graphical criterion is not satisfied (Section 4). Given the constructive nature of our achievability results, we propose algorithmic approaches for designing optimal imitation policies (Sections 3 and 4) and evaluate their performance in Section 5. The proofs of all our results appear in Appendix A.

## 2 Preliminaries

Throughout this work, we denote random variables and their realizations by capital and small letters, respectively. Likewise, we use boldface capital and small letters to represent sets of random variables and their realizations, respectively. For a variable \(X\), \(_{X}\) denotes the domain of \(X\) and \(_{X}\) the space of probability distributions over \(_{X}\). Given two subsets of variables \(\) and \(\) such that \(\), and a realization \(_{}\), we use \(()_{}\) to denote the restriction of \(\) to the variables in \(\).

We use structural causal models (SCMs) as the semantic framework of our work . An SCM \(M\) is a tuple \(,,P^{M}(),\) where \(\) and \(\) are the sets of exogenous and endogenous variables, respectively. Values of variables in \(\) are determined by an exogenous distribution \(P^{M}()\), whereas the variables in \(\) take values defined by the set of functions \(=\{f_{V}^{M}\}_{V}\). That is, \(V f_{V}^{M}((V)_{V})\) where \((V)\) and \(_{V}\). We also partition \(\) into the observable and latent variables, denoted by \(\) and \(\), respectively, such that \(=\). The SCM induces a probability distribution over \(\) whose marginal distribution over observable variables, denoted by \(P^{M}()\), is called the _observational distribution_. Moreover, we use \(do(=)\) to denote an intervention on \(\) where the values of \(\) are set to a constant \(\) in lieu of the functions \(\{f_{X}^{M}: X\}\). We use \(P_{}^{M}() P^{M}(=|do( =))\) as a shorthand for the post-interventional distribution of \(\) after the intervention \(do(=)\).

Let \(\), \(\), \(\), and \(\) be pairwise disjoint subsets of variables. \(\) and \(\) are called _contextually independent_ given \(\) in the context \(_{}\) if \(P(|,,)=P(|, )\), whenever \(P(,,)>0\). We denote this context-specific independence (CSI) relation by \(\!\!\!|,\). Moreover, a CSI is called _local_ if it is of the form \(X\!\!\! Y|\) where \(\{Y\}(X)\).

A directed acyclic graph (DAG) is defined as a pair \(=(,)\), where \(\) is the set of vertices, and \(\) denotes the set of directed edges among the vertices. SCMs are associated with DAGs, where each variable is represented by a vertex of the DAG, and there is a directed edge from \(V_{i}\) to \(V_{j}\) if \(V_{i}(V_{j})\). Whenever a local CSI of the form \(V_{i}\!\!\! V_{j}|\) holds, we say \(\) is a label for the edge \((V_{i},V_{j})\) and denote it by \(_{(V_{i},V_{j})}\). Recalling the example of Figure 1c, the realization \(T=1\) is a label for the edge \((S,X)\), which indicates that this edge is absent when \(T\) is equal to \(1\). Analogous to , we define a labeled DAG (LDAG), denoted by \(^{}\), as a tuple \(^{}=(,,)\), where \(\) denotes the labels representing local independence relations. More precisely, \(=\{_{(V_{i},V_{j})}:_{(V_{i},V_{j})} (V_{i},V_{j})\}\), where

\[_{(V_{i},V_{j})}=\{_{^{}}|^{}(V_{j})\{V_{i}\},V_{i}\!\!\! V_{j}|\}.\]

Note that, when \(=\), \(^{}\) reduces to a DAG. That is, every DAG is a special LDAG with no labels. For ease of notation, we drop the superscript \(\) when \(=\). Given a label set \(\), we define the context variables of \(\), denoted by \(()\), as the subset of variables that at least one realization of them appears in the edge labels. More precisely,

\[()\!=\!\!V_{i}| V_{j},V_{k} V_{i}, ^{},:(V_{k},V_{j})\!\!,V_{i}^{ },_{^{}}_{(V_{k},V_{j})} },\] (1)where \(^{}\) is an arbitrary subset of nodes containing \(V_{i}\). The argument is that if there exists some arbitrary subset \(^{}\) of nodes containing \(V_{i}\) such that a realization \(l\) of this subset results in independence (e.g., of some \(V_{j}\) and \(V_{k}\)), then \(V_{i}\) is considered as a context variable. We mainly focus on the settings where context variables are discrete or categorical. This assumption can be relaxed under certain considerations (See Remark 3.11). We let \(_{}\) denote the class of SCMs compatible with the causal graph \(^{}\). For a DAG \(\), we use \(_{}\) and \(_{}\) to represent the subgraphs of \(\) obtained by removing edges incoming to and outgoing from vertices of \(\), respectively. We also use standard kin abbreviations to represent graphical relationships: the sets of parents, children, ancestors, and descendants of \(\) in \(\) are denoted by \(()\)\(()\), \(()\), and \(()\), respectively. For disjoint subsets of variable \(\), \(\) and \(\) in \(\), \(\) and \(\) are said to be d-separated by \(\) in \(\), denoted by \(|\), if every path between vertices in \(\) and \(\) is blocked by \(\) (See Definition 1.2.3. in 28). Finally, solid and dashed vertices in the figures represent the observable and latent variables, respectively.

## 3 Imitability

In this section, we address the decision problem, i.e., whether imitation learning is feasible, given a causal mechanism. We first review the imitation learning problem from a causal perspective, analogous to the framework developed by . We will use this framework to formalize the causal imitability problem in the presence of CSIs. Recall that \(\) and \(\) represented the observed and unobserved variables, respectively. We denote the action and reward variables by \(X\) and \(Y\), respectively. The reward variable is commonly assumed to be unobserved in imitation learning. Given a set of observable variables \(^{}(X)\), a policy \(\) is then defined as a stochastic mapping, denoted by \((X|^{})\), mapping the values of \(^{}\) to a probability distribution over the action \(X\). Given a policy \(\), we use \(do()\) to denote the intervention following the policy \(\), i.e., replacing the original function \(f_{X}\) in the SCM by the stochastic mapping \(\). The distribution of variables under policy \(do()\) can be expressed in terms of post-interventional distributions (\(P(|do(x))\)) as follows:

\[P(|do())=_{x_{X},^{}_{^{}}}P(|do(x),^{})(x|^{ })P(^{}),\] (2)

where \(\) is a realization of an arbitrary subset \(^{}\). We refer to the collection of all possible policies as the _policy space_, denoted by \(=\{:_{^{}}_{X}\}\). Imitation learning is concerned with learning an optimal policy \(^{*}\) such that the reward distribution under policy \(^{*}\) matches that of the expert policy, that is, \(P(y|do(^{*}))=P(y)\). Given a DAG \(\) and the policy space \(\), if such a policy exists, the instance is said to be _imitable_ w.r.t. \(^{},\). The causal imitability problem is formally defined below.

**Definition 3.1** (Classic imitability w.r.t. \(,\) 43).: _Given a latent DAG \(\) and a policy space \(\), let \(Y\) be an arbitrary variable in \(\). \(P(y)\) is said to be imitable w.r.t. \(,\) if for any \(M_{}\), there exists a policy \(\) uniquely computable from \(P()\) such that \(P^{M}(y|do())=P^{M}(y)\)._

Note that if \(Y(X)\), the third rule of Pearl's do calculus implies \(P(y|do(x),^{})=P(y|^{})\), and from Equation (2), \(P(y|do())=P(y)\) for any arbitrary policy \(\). Intuitively, in such a case, action \(X\) has no effect on the reward \(Y\), and regardless of the chosen policy, imitation is guaranteed. Therefore, throughout this work, we assume that \(X\) affects \(Y\), i.e., \(Y(X)\). Under this assumption,  proved that \(P(y)\) is imitable w.r.t. \(,\) if and only if there exists a \(\)-backdoor admissible set \(\) w.r.t. \(,\).

**Definition 3.2** (\(\)-backdoor, 43).: _Given a DAG \(\) and a policy space \(\), a set \(\) is called \(\)-backdoor admissible set w.r.t. \(,\) if and only if \(^{}\) and \(Y X|\) in \(_{}\)._

The following lemma further reduces the search space of \(\)-backdoor admissible sets to a single set.

**Lemma 3.3**.: _Given a latent DAG \(\) and a policy space \(\), if there exists a \(\)-backdoor admissible set w.r.t. \(,\), then \(=(\{X,Y\})(^{})\) is a \(\)-backdoor admissible set w.r.t. \(,\)._

As a result, deciding the imitability reduces to testing a d-separation, i.e., whether \(\) defined in Lemma 3.3 d-separates \(X\) and \(Y\) in \(_{}\), for which efficient algorithms exist [14; 38]. If this d-separation holds, then \((X|)=P(X|)\) is an optimal imitating policy. Otherwise, the instance is not imitable.

It is noteworthy that in practice, it is desirable to choose \(\)-admissible sets with small cardinality for statistical efficiency. Polynomial-time algorithms for finding minimum(-cost) d-separators exist [2; 38].

### Imitability with CSIs

Deciding the imitability when accounting for CSIs is not as straightforward as the classic case discussed earlier. In particular, as we shall see, the existence of \(\)-backdoor admissible sets is not necessary to determine the imitability of \(P(y)\) anymore in the presence of CSIs. In this section, we establish a connection between the classic imitability problem and imitability under CSIs. We begin with a formal definition of imitability in our setting.

**Definition 3.4** (Imitability w.r.t. \(^{},\)).: _Given an LDAG \(^{}\) and a policy space \(\), let \(Y\) be an arbitrary variable in \(^{}\). \(P(y)\) is called imitable w.r.t. \(^{},\) if for any \(M_{^{}}\), there exists a policy \(\) uniquely computable from \(P()\) such that \(P^{M}(y|())=P^{M}(y)\)._

For an LDAG \(^{}\), recall that we defined the set of context variables \(()\) by Equation (1). The following definition is central in linking the imitability under CSIs to the classic case.

**Definition 3.5** (Context-induced subgraph).: _Given an LDAG \(^{}=(,,)\), for a subset \(()\) and its realization \(_{}\), we define the context-induced subgraph of \(^{}\) w.r.t. \(\), denoted by \(^{}_{}\), as the LDAG obtained from \(^{}\) by keeping only the labels that are compatible with \(\)3, and deleting the edges that are absent given \(=\), along with the edges incident to \(\)._

Consider the example of Figure 2 for visualization. In the context \(Z=1\), the label \(Z=0\) on the edge \(U X\) is discarded, as \(Z=0\) is not compatible with the context (see Figure 1(b).) Note that edges incident to the context variable \(Z\) are also omitted. On the other hand, in the context \(Z=1,T=0\), the edge \(X Y\) is absent and can be deleted from the corresponding graph (refer to Figure 1(c).) Edges incident to both \(T\) and \(Z\) are removed in this case. Equipped with this definition, the following result, the proof of which appears in Appendix A, characterizes a necessary condition for imitability under CSIs.

**Lemma 3.6**.: _Given an LDAG \(^{}\) and a policy space \(\), let \(Y\) be an arbitrary variable in \(^{}\). \(P(y)\) is imitable w.r.t. \(^{},\) only if \(P(y)\) is imitable w.r.t. \(^{}_{},\) for every realization \(_{}\) of every subset of variables \(()\)._

For instance, a necessary condition for the imitability of \(P(y)\) in the graph of Figure 1(a) is that \(P(y)\) is imitable in both 1(b) and 1(c). Consider the following special case of Lemma 3.6: if \(=()\), then \(^{}_{}=_{}\) is a DAG, as \(_{}=\) for every \(_{}\). In essence, a necessary condition of imitability under CSIs can be expressed in terms of several classic imitability instances:

**Corollary 3.7**.: _Given an LDAG \(^{}\) and a policy space \(\), let \(Y\) be an arbitrary variable in \(^{}\). \(P(y)\) is imitable w.r.t. \(^{},\) only if \(P(y)\) is imitable w.r.t. \(_{},\), i.e., there exists a \(\)-backdoor admissible set w.r.t. \(_{},\), for every \(_{()}\)._

It is noteworthy that although the subgraphs \(_{}\) in Corollary 3.7 are defined in terms of realizations of \(()\), the number of such subgraphs does not exceed \(2^{||}\). This is due to the fact that \(_{}\)s share the same set of vertices, and their edges are subsets of the edges of \(^{}\).

Figure 2: Two examples of context-induced subgraphs (Definition 3.5).

Although deciding the classic imitability is straightforward, the number of instances in Corollary 3.7 can grow exponentially in the worst case. However, in view of the following hardness result, a more efficient criterion in terms of computational complexity cannot be expected.

**Theorem 3.8**.: _Given an LDAG \(^{}\) and a policy space \(\), deciding the imitability of \(P(y)\) w.r.t. \(^{},\) is NP-hard._

This result places the problem of causal imitability under CSI relations among the class of NP-hard problems in the field of causality, alongside other challenges such as devising minimum-cost interventions for query identification  and discovering the most plausible graph for causal effect identifiability . Although Theorem 3.8 indicates that determining imitability under CSIs might be intractable in general, as we shall see in the next section taking into account only a handful of CSI relations can render previously non-imitable instances imitable. Before concluding this section, we consider a special yet important case of the general problem. Specifically, for the remainder of this section, we assume that \((())()\). That is, the context variables have parents only among the context variables. Under this assumption, the necessary criterion of Corollary 3.7 turns out to be sufficient for imitability as well. More precisely, we have the following characterization.

**Proposition 3.9**.: _Given an LDAG \(^{}\) where \((())()\) and a policy space \(\), let \(Y\) be an arbitrary variable in \(^{}\). \(P(y)\) is imitable w.r.t. \(^{},\) if and only if \(P(y)\) is imitable w.r.t. \(_{},\), for every \(_{()}\)._

```
1:functionimitate1 (\(^{},,X,Y\))
2: Compute \(:=()\) using Equation (1)
3:for\(_{}\)do
4: Construct a DAG \(_{}\) using Definition 3.5
5:ifFindSep (\(_{},,X,Y\)) Fails then
6:return FAIL
7:else
8:\(_{}\) (\(_{},,X,Y\))
9:\(_{}(X|^{}) P(X|(^{})_{ _{}})\)
10:\(^{*}(X|^{})\!\!_{_{ }}\!\!\{(^{})_{}\!\!=\! \}_{}(X|^{})\)
11:return\(^{*}\) ```

**Algorithm 1** Imitation w.r.t. \(^{},\)

The proof of sufficiency, which is constructive, appears in Appendix A. The key insight here is that an optimal imitation policy is constructed based on the imitation policies corresponding to the instances \(_{},\). In view of proposition 3.9, we provide an algorithmic approach for finding an optimal imitation policy under CSIs, as described in Algorithm 1. This algorithm takes as input an LDAG \(^{}\), a policy space \(\), an action variable \(X\) and a latent reward \(Y\). It begins with identifying the context variables \((=())\), defined by Equation (1) (Line 2). Next, for each realization \(_{}\), the corresponding context-induced subgraph (Def. 3.5) is built (which is a DAG). If \(P(y)\) is not imitable in any of these DAGs, the algorithm fails, i.e., declares \(P(y)\) is not imitable in \(^{}\). The imitability in each DAG is checked through a d-separation based on Lemma 3.3 (for further details of the function \(FindSep\), see Algorithm 3 in Appendix B.) Otherwise, for each realization \(_{}\), an optimal policy \(_{c}\) is learned through the application of the \(\)-backdoor admissible set criterion (line 9). If such a policy exists for every realization of \(\), \(P(y)\) is imitable w.r.t. \(^{},\) due to Proposition 3.9. An optimal imitating policy \(^{*}\) is computed based on the previously identified policies \(_{}\). Specifically, \(^{*}\), the output of the algorithm, is defined as

\[^{*}(X|^{})_{_{ }}\{(^{})_{}=\}_{ }(X|^{}).\]

**Theorem 3.10**.: _Algorithm 1 is sound and complete for determining the imitability of \(P(y)\) w.r.t. \(^{},\) and finding the optimal imitating policy in the imitable case, under the assumption that \((())()\)._

**Remark 3.11**.: _Algorithm 1 requires assessing the d-separation of line 5 in the context-induced subgraphs \(_{}\). Even when the context variables \(\) are continuous, the domain of these variables can be partitioned into at most \(2^{m}\) equivalence classes in terms of their corresponding context-induced subgraph, where \(m\) denotes the number of labeled edges. This holds since the number of context-induced subgraphs cannot exceed \(2^{m}\). It is noteworthy, however, that solving the equation referred to in line 10 of Algorithm 2 for continuous variables may bring additional computational challenges._

**Remark 3.12**.: _Under certain assumptions, polynomial-time algorithms can be devised for deciding imitability. One such instance is when \(_{()}=(|V|)\). Another analogous case is when the number of context-specific edges is \((|V|)\). Both of these lead polynomially many context-induced subgraphs in terms of \(|V|\), which in turn implies that Alg. 1 runs in polynomial time._

## 4 Leveraging causal effect identifiability for causal imitation learning

Arguably, the main challenge in imitation learning stems from the latent reward. However, in certain cases, there exist observable variables \(\) such that \(P(|do())=P()\) implies \(P(y|do())=P(y)\) for any policy \(\). Such \(\) is said to be an imitation surrogate for \(Y\). Consider, for instance, the graph of Figure 2(a), where \(X\) represents the pricing strategy of a company, \(C\) is a binary variable indicating recession (\(C=0\)) or expansion (\(C=1\)) period, \(U\) denotes factors such as demand and competition in the market, \(S\) represents the sales and \(Y\) is the overall profit of the company. Due to Proposition 3.9, \(P(y)\) is not imitable in this graph. On the other hand, the sales figure (\(S\)) is an imitation surrogate for the profit (\(Y\)), as it can be shown that whenever \(P(S|do())=P(S)\) for a given policy \(\), \(P(y|do())=P(y)\) holds for the same policy. Yet, according to do-calculus, \(P(S|do())\) itself is not identifiable due to the common confounding \(U\). On the other hand, we note that the company's pricing strategy (\(X\)) becomes independent of demand (\(U\)) during a recession (\(C=0\)), as the company may not have enough customers regardless of the price it sets. This CSI relation amounts to the following identification formula for the effect of an arbitrary policy \(\) on sales figures:

\[P(s|do())=_{x,c}P(s|x,C=0)(x|c)P(c),\] (3)

where all of the terms on the right-hand side are known given the observations. Note that even though \(S\) is a surrogate in Figure 2(a), without the CSI \(C=0\), we could not have written Equation (3). Given the identification result of this equation, if the set of equations \(P(s|do())=P(s)\) has a solution \(^{*}\), then \(^{*}\) becomes an imitation policy for \(P(y)\) as well . It is noteworthy that solving the aforementioned linear system of equations for \(^{*}\) is straightforward, for it boils down to a matrix inversion4. In the example discussed, although the graphical criterion of Proposition 3.9 does not hold, the data-specific parameters could yield imitability in the case that these equations are solvable. We therefore say \(P(y)\) is imitable w.r.t. \(^{},,P()\), as opposed to 'w.r.t. \(^{},\)'. Precisely, we say \(P(y)\) is imitable w.r.t. \(^{},,P()\) if for every \(M_{^{}}\) such that \(P^{M}()=P()\), there exists a policy \(\) such that \(P^{M}(y|do())=P^{M}(y)\).

The idea of surrogates could turn out to be useful to circumvent the imitability problem when the graphical criterion does not hold. In Figure 2(b), however, neither graphical criteria yields imitability nor any imitation surrogates exist. In what follows, we discuss how CSIs can help circumvent the problem of imitability in even in such instances. Given an LDAG \(^{}\) and a context \(=\), we denote the _context-specific_ DAG w.r.t. \(^{},\) by \(^{}()\) where \(^{}()\) is the DAG obtained by deleting all the spurious edges, i.e., the edges that are absent given the context \(=\), from \(^{}\).

Figure 3: Two examples of leveraging CSI relations to achieve imitability.

**Definition 4.1** (Context-specific surrogate).: _Given an LDAG \(^{}\), a policy space \(\), and a context \(\), a set of variables \(\) is called context-specific surrogate w.r.t. \(^{}(),\) if \(X Y|\{,\}\) in \(^{}()\) where \(^{}()\) is a supergraph of \(^{}()\) by adding edges from \(^{}\) to \(X\)._

Consider the LDAG of Figure 2(b) for visualization. The context-specific DAGs corresponding to contexts \(C=0\) and \(C=1\) are shown in Figures 2(c) and 2(d), respectively. \(S\) is a context-specific surrogate with respect to \(^{}(C=0),\), while no context-specific surrogates exist with respect to \(^{}(C=1),\). The following result indicates that despite the absence of a general imitation surrogate, the existence of context-specific surrogates could suffice for imitation.

**Proposition 4.2**.: _Given an LDAG \(^{}\) and a policy space \(\), let \(\) be a subset of \(^{}()\). If for every realization \(_{}\) at least one of the following holds, then \(P(y)\) is imitable w.r.t. \(^{},,P()\)._

* _There exists a subset_ \(_{}\) _such that_ \(X Y|\{_{},\}\) _in_ \(^{}()\)_, and_ \(P(_{}|do(),=)=P(_{}|=)\) _has a solution_ \(_{}\)_._
* _There exists_ \(_{c}^{}\) _such that_ \(X Y|\{_{c},\}\) _in_ \(^{}()_{}\)_._

As a concrete example, defining \(_{1}=\), \(X Y|\{_{1},C\}\) holds in the graph of Figure 2(d). Moreover, \(_{0}=\{S\}\), is a context-specific surrogate w.r.t. \(^{}(C=0),\). As a consequence of Proposition 4.2, \(P(y)\) is imitable w.r.t. \(^{},,P()\), if a solution \(_{0}\) exists to the linear set of equations

\[P(S|do(),C=0)=P(S|C=0),\]

where \(P(s|do(),C=0)=_{x,t}P(s|x,T=0,C=0)(x|t,C=0)P(t|C=0)\), analogous to Equation (3).

To sum up, accounting for CSIs has a two-fold benefit: (a) _context-specific surrogates_ can be leveraged to render previously non-imitable instances imitable, and (b) identification results can be derived for imitation surrogates that were previously non-identifiable.

In light of Proposition 4.2, an algorithmic approach for causal imitation learning is proposed, summarized as Algorithm 2. This algorithm calls a recursive subroutine, \(SubImitate\), also called \(SI\) within the pseudo-code. It is noteworthy that Proposition 4.2 guarantees imitability if the two conditions are met for any arbitrary subset of \(()}^{}\). As we shall see, Algorithm 2 utilizes a recursive approach for building such a subset so as to circumvent the need to test all of the possibly exponentially many subsets.

The subroutine \(SI\) is initiated with an empty set (\(=\)) as the considered context variables at the first iteration. At each iteration, the realizations of \(\) are treated separately. For each such realization \(\), if the second condition of Proposition 4.2 is met through a set \(}\), then \(P(X|},=)\) is returned as the context-specific imitating policy (lines 3-6). Otherwise, the search for a context-specific surrogate begins. We utilize the \(FindMinSep\) algorithm of  to identify a minimal separating set \(}\) for \(X\) and \(Y\), among those that necessarily include \(\) (lines 7-8). We then use the identification algorithm of  under CSI relations to identify the effect of an arbitrary policy on \(}\), conditioned on the context \(\). This algorithm is built upon CSI-calculus, which subsumes do-calculus5. Next, if the linear system of equations \(P(}|do(_{}),)=P(}|,)\) has a solution, then this solution is returned as the optimal policy (lines 9-12). Otherwise, an arbitrary variable \(V()}^{}\) is added to the considered context variables, and the search for context-specific policies proceeds while taking the realizations of \(V\) into account (lines 17-24). If no variables are left to add to the set of context variables (i.e., \(=()}^{}\)) and neither of the conditions of Proposition 4.2 are met for a realization of \(\), then the algorithm stops with a failure (lines 14-15). Otherwise, an imitating policy \(^{*}\) is returned. We finally note that if computational costs matter, the CSI-ID function of line 9 can be replaced by the ID algorithm of . Further, the minimal separating sets of line 8 might not be unique, in which case all such sets can be used.

**Theorem 4.3**.: _Given an LDAG \(^{}\), a policy space \(\) and observational distribution \(P()\), if Algorithm 2 returns a policy \(^{*}\), then \(^{*}\) is an optimal imitating policy for \(P(y)\) w.r.t. \(^{},,P()\). That is, Algorithm 2 is sound6._

## 5 Experiments

Our experimental evaluation is organized into two parts. In the first part, we address the decision problem pertaining to imitability. We evaluate the gain resulting from accounting for CSIs in rendering previously non-imitable instances imitable. In particular, we assess the classic imitability v.s. imitability under CSIs for randomly generated graphs. In the second part, we compare the performance of Alg. 2 against baseline algorithms on synthetic datasets (see Sec. D for further details of our experimental setup). Python implementation are accessible at https://github.com/SinaAkbarii/causal-imitation-learning/.

### Evaluating imitability

We sampled random graphs with \(n\) vertices and maximum degree \(=\) uniformly at random. Each variable was assumed to be latent with probability \(\). We chose 3 random context variables

    &  \\   & Expert & Naive 1 & Naive 2 & Algorithm 2 \\  \([Y]\) & \(1.367\) & \(1.194\) & \(1.193\) & \(1.358\) \\ \(D_{KL}(P(Y)||P(Jo(_{ALG}))\) & \(0\) & \(0.0217\) & \(0.0219\) & \(0.0007\) \\ \(D_{KL}(_{ALG}(X|T=0)||_{ALG}(X|T=0))\) & \(NA\) & \(2.3 10^{-5}\) & \(4.4 10^{-6}\) & \(1.3 10^{-3}\) \\ \(D_{KL}(_{ALG}(X|T=1)||_{ALG}(X|T=1))\) & \(NA\) & \(2.3 10^{-5}\) & \(4.8 10^{-4}\) & \(1.3 10^{-3}\) \\   

Table 1: Results pertaining to the model of Figure 2(a).

such that the graphical constraint \((())()\) is satisfied. Labels on the edges were sampled with probability \(0.5\). We then evaluated the graphical criterion of classic imitability (existence of \(\)-backdoor) and imitability with CSIs (Corollary 3.7). The results are depicted in Figure 4, where each point in the plot is an average of 100 sampled graphs. As seen in this figure, taking into account only a handful of CSI relations (in particular, 3 context variables among hundreds of variables) could significantly increase the fraction of imitable instances.

### Performance evaluation

In this section, we considered the graph of Figure 2(b) as a generalization of the economic model of Figure 2(a), where the reward variable \(Y\) can be a more complex function. As discussed earlier, this graph has neither a \(\)-backdoor admissible set nor an imitation surrogate. However, given the identifiability of \(P(S|do(),C=0)\), Algorithm 2 can achieve an optimal policy. We compared the performance of the policy returned by Algorithm 2 against two baseline algorithms: Naive algorithm 1, which mimics only the observed distribution of the action variable (by choosing \((X)=P(X)\)), and Naive algorithm 2, which takes the causal ancestors of \(X\) into account, designing the policy \((X|T)=P(X|T)\). Naive algorithm 2 can be thought of as a feature selection followed by a behavior cloning approach. The goal of this experiment was to demonstrate the infeasibility of imitation learning without taking CSIs into account. A model with binary observable variables and a ternary reward was generated. Let \(_{ALG}\) represent the policy that the algorithm would have learned with infinite number of samples, and \(_{ALG}\) the policy it learns with the given finite sample size. As can be seen in Table 1, Algorithm 2 was able to match the expert policy both in expected reward and KL divergence of the reward distribution. The naive algorithms, on the other hand, failed to get close to the reward distribution of the expert. Since the algorithms were fed with finite observational samples, the KL divergence of the estimated policies with the nominal policy is also reported. Notably, based on the reported measures, the undesirable performance of the Naive algorithms does not stem from estimation errors.

## 6 Concluding remarks

We considered the causal imitation learning problem when accounting for context-specific independence relations. We proved that in contrast to the classic problem, which is equivalent to a d-separation, the decision problem of imitability under CSIs is NP-hard. We established a link between these two problems. In particular, we proved that imitability under CSIs is equivalent to several instances of the classic imitability problem for a certain class of context variables. We showed that utilizing the overlooked notion of CSIs could be a worthwhile tool in causal imitation learning as an example of a fundamental AI problem from a causal perspective. We note that while taking a few CSI relations into account could result in significant achievable results, the theory of CSIs is not yet well-developed. In particular, there exists no complete algorithm for causal identification under CSIs. Further research on the theory of CSI relations could yield considerable benefits in various domains where such relations are present.

Figure 4: The fraction of imitable instances (in the classical sense) vs those that are imitable considering CSIs.