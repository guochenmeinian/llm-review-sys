# Bayesian Optimisation with

Unknown Hyperparameters:

Regret Bounds Logarithmically Closer to Optimal

 Juliusz Ziomek\({}^{,}\), Masaki Adachi\({}^{,}\), Michael A. Osborne\({}^{}\),

\({}^{}\)Machine Learning Research Group, University of Oxford

\({}^{}\) Toyota Motor Corporation

\({}^{}\) Corresponding Author

{juliusz, masaki, mosb}@robots.ox.ac.uk

###### Abstract

Bayesian Optimization (BO) is widely used for optimising black-box functions but requires us to specify the length scale hyperparameter, which defines the smoothness of the functions the optimizer will consider. Most current BO algorithms choose this hyperparameter by maximizing the marginal likelihood of the observed data, albeit risking misspecification if the objective function is less smooth in regions we have not yet explored. The only prior solution addressing this problem with theoretical guarantees was A-GP-UCB, proposed by Berkenkamp et al. (2019). This algorithm progressively decreases the length scale, expanding the class of functions considered by the optimizer. However, A-GP-UCB lacks a stopping mechanism, leading to over-exploration and slow convergence. To overcome this, we introduce Length scale Balancing (LB)--a novel approach, aggregating multiple base surrogate models with varying length scales. LB intermittently adds smaller length scale candidate values while retaining longer scales, balancing exploration and exploitation. We formally derive a cumulative regret bound of LB and compare it with the regret of an oracle BO algorithm using the optimal length scale. Denoting the factor by which the regret bound of A-GP-UCB was away from oracle as \(g(T)\), we show that LB is only \( g(T)\) away from oracle regret. We also empirically evaluate our algorithm on synthetic and real-world benchmarks and show it outperforms A-GP-UCB, maximum likelihood estimation and MCMC.

## 1 Introduction

Bayesian Optimisation (BO)  has proven to be an efficient solution for black-box optimisation problems, finding applications across science, engineering and machine learning [12; 18; 23]. As a model-based optimisation technique, BO constructs a surrogate model of the black box function, which is typically a Gaussian Process (GP) . However, to construct this surrogate, we need to specify our expectations about the smoothness of the black-box function. In the case of GP, the choice of smoothness is reflected in the selection of an appropriate length scale value for the kernel function. Selecting smaller length scales allows us to model less smooth functions and, as such, expands the class of all possible black-box functions the optimiser will consider. At the same time, it makes the convergence of the algorithm slower, due to an increase in the number of possible 'candidate' functions, the algorithm has to explore the space much more. As such, we wish to consider the smallest possible class of functions that still contains the black-box function we wish to solve. This translates to selecting some optimal length scale value, which is neither too short nor too long.

Appropriate selection of the length scale parameter can be challenging. Typical practice is to fit the length scale value by maximum likelihood estimation (MLE) on the observed data we have collected thus far. However, it is entirely possible that the function changes less smoothly in the regions we have not explored yet, as shown in Figure 1. As such, we cannot guarantee that maximising the likelihood of the limited, observed data will find a length scale value such that the black-box function will lie in the space of considered functions. A previously proposed algorithm called A-GP-UCB  approached this issue by progressively decreasing the length scale value, and as such increasing the class of functions considered by the optimiser. As a consequence, at some point, it must contain the black-box function we are trying to optimise. However, the algorithm has no mechanism for stopping and as such the length scale value will decrease indefinitely, inexorably expanding the class of considered functions. This causes over-exploration, making the convergence much slower compared to an optimiser that knows the optimal length scale value.

A-GP-UCB is suboptimal because it never returns to previously trialled, longer length scales. Observe that if trying a shorter length scale value does not improve function discovery, opting for a longer scale is a safer choice, preventing excessive exploration. To build an algorithm following this intuition, we could have a number of base optimisers, each utilising a different length scale value, and aggregate them into a single'master' optimiser. By knowing how explorative each of the base optimisers is, the'master' optimiser could select the most suitable one at each iteration, so as to balance exploration and exploitation. Within the literature of multi-armed bandit problems, a number of rules for aggregating base algorithms have been proposed , however, the performance of those'master' algorithms worsens with the number of base algorithms. This prohibits us from directly applying'master' algorithms to the unknown hyperparameter problem, as the length scale is a continuous parameter and has infinitely many possible values.

Within this work, we extend one such aggregation scheme, called regret-balancing, to handle infinitely many learners, so that it could tackle the problem of BO with unknown hyperparameters. We propose an algorithm called Length scale Balancing GP-UCB (LB-GP-UCB), which aggregates a number of base optimisers with different length scale values and gradually introduces new base optimisers, equipped with smaller length scales. Instead of permanently decreasing the length scale value, as done by A-GP-UCB, LB-GP-UCB occasionally introduces new base learners with smaller length scale values, while still maintaining base learners with longer ones. As such, if one of the longer length scales is optimal, we will be able to recover performance close to the one of the oracle optimiser utilising that optimal length scale. Denoting the factor by which the regret bound of A-GP-UCB was away from oracle as \(g(T)\), we show that LB-GP-UCB is only \( g(T)\) away from oracle regret. We also conduct empirical evaluation and show LB-GP-UCB obtains improved regret results on a mix of synthetic and real-world benchmarks com

Figure 1: An objective function (proposed by ) that illustrates the importance of length scales to BO. The blue line shows a GP fit with shaded regions representing one standard deviation. The length scale value was set to the optimal value on the left and was selected by MLE on the right, based on five points represented by dots. While the optimiser with the MLE of length scale persistently selects a suboptimal value of \(x=1\), the optimiser with the optimal length scale can spot the hidden peak leading to finding the maximum at \(x^{*}=0.3\).

Figure 2: Histogram showing how often (as a proportion of iterations) each algorithm selected a given length scale value while optimising the Michalewicz function over ten seeds. \(^{*}\) corresponds to an estimate of optimal length scale value. See ยง5 for details.

pared to A-GP-UCB, MLE and MCMC. We show the histogram of length scale values selected by each method on one of the benchmark problems in Figure 2. \(^{}\) represents an estimate of optimal length scale value on this problem (see SS5 for details). We can see that the length scale values selected by LB-GP-UCB are close to the estimated optimal value, whereas MLE and A-GP-UCB miss this value by respectively over and under-estimating, matching our predictions. We summarise our contributions below.

* We propose LB-GP-UCB and show that compared to A-GP-UCB its regret bound is logarithmically closer to the bound of an oracle optimiser knowing the optimal length scale.
* We extend our algorithm to also handle the case of unknown output scale (function norm) alongside the length scale
* We show the empirical superiority of our algorithm compared to MLE, MCMC and A-GP-UCB on a mix of synthetic and real-world problems, and conduct an ablation study showing increased robustness of our method.

## 2 Problem Statement and Preliminaries

We consider the problem of maximising an unknown black-box function \(f:\) on some compact set \(^{d}\). At each time step \(t\), we are allowed to query a function at a selected point \(_{t}\) and observe noisy feedback \(y_{t}=f(_{t})+_{t}\), where \(_{t}(_{N}^{2})\). We wish to find the optimum \(^{}=_{}f()\). We define the instantaneous regret to be \(r_{t}=f(^{})-f(_{t})\) and cumulative regret as \(R_{T}=_{t=1}^{T}r_{t}\), and we wish to minimise it. We assume we are given some kernel function \(k^{}(,^{})=k(,^{ }}{})\) parametrised by a length scale value \(^{+}\) and we denote its associated Reproducing Kernel Hilbert Space (RKHS) as \((k^{})\). We assume that at least for certain values of \(\), the black-box function \(f\) belongs to this RKHS, i.e. \(_{^{+}}f(k^{})\). Concretely, we will consider two popular types of kernels: RBF and \(\)-Matern, defined below for completeness:

\[k^{}_{}(,^{}) =(--^{}\|_{2}^{2}}{^{2} })\] \[k^{}_{}(,^{}) =}{()}(- ^{}\|}{})^{}K_{}( -^{}\|}{}),\]

where \(()\) is the Gamma function and \(K_{}()\) is the modified Bessel function of the second kind of order \(\). Without the loss of generality, we assume \(k^{}(,) 1\) for all \(^{+}\). In the rest of the paper, if we do not specify the type of the kernel, it means the result is applicable to both types of kernels. If we fit GP model with a kernel \(k^{}(,^{})\) to the data so far, \(_{t-1}=\{(_{},y_{})\}_{=1}^{t-1}\), we obtain the following mean \(_{t-1}^{}()\) and variance \((_{t-1}^{})^{2}()\) functions:

\[_{t-1}^{}()=_{t-1}^{}()^{T}(_{t-1} ^{}+_{N}^{2})^{-1}_{t-1}\]

\[(_{t-1}^{})^{2}()=k^{}(,)-_{t-1}^{ }()^{T}(_{t-1}^{}+_{N}^{2})^{-1} _{t-1}^{}(),\]

where \(_{t-1}^{t-1}\) with elements \(()_{i}=y_{i}\), \(k^{}()^{t-1}\) with elements \(^{}()_{i}=k^{}(,_{i})\) and similarity \(_{t-1}^{}^{t-1 t-1}\) with entries \((_{t-1}^{})_{i,j}=k^{}(_{i},_{j})\), and \(_{N}^{2}\) is the regulariser factor  with identity matrix \(\). If we were to use any length scale value such that \(f(k^{})\), then we can obtain certain guarantees about the predictions made by the GP model, as stated next.

**Theorem 2.1** (Theorem 2 of ).: _Let \(f(k^{})\), such that \(\|f\|_{k^{}} B\) and set \(_{}^{,B}=B+_{N}_{t-1}(k^{})+1+ (1/_{A}))}\), where \(_{T}(k^{})\) is an upper bound \( I+_{N}^{-2}_{T}^{^{}} _{T}(k^{})\), which depends on the kernel and length scale choice. Then, with probability at least \(1-_{A}\), for all \(\) and \(t=1,,T\):_

\[ f()-_{t-1}^{}()_{t}^{ ,B}_{t-1}^{}().\]

Note that the Theorem 2.1 relies on the quantity \(_{T}(k^{})\), also called maximum information gain (MIG). The next proposition, proven in Appendix A, provides bounds on \(_{T}(k^{})\) for RBF and \(\)-Matern kernel and shows the explicit dependence on length scale hyperparameter \(\).

**Proposition 2.2**.: _We have that \(_{T}(k^{})(_{T}(k^{}))\):_

* _For an RBF kernel_ \(_{T}(k^{})=}(T)^{d+1}\)__
* _For_ \(\)_-Matern kernel_ \(_{T}(k^{})=}T^{}( T)^{}\)__

Based on the GP model, a typical BO algorithm constructs an acquisition function, which tells us how 'promising' a given point is to try next. We will focus on the commonly used Upper Confidence Bound (UCB), defined as \(_{t}^{,B}()=_{t-1}^{}()+_{t}^{,B}_{t-1}^{}()\). The GP-UCB algorithm  fits a GP model and utilises the UCB criterion to select new points to query. Such an algorithm admits a high-probability regret bound as stated by the next Theorem.

**Theorem 2.3** (Theorem 2 in ).: _Let us run a GP-UCB utilising a GP with a kernel \(k^{}\) and the exploration bonus of \(_{t}^{,B}=B+_{N}_{T}(k^{}))+1+( 1/_{A}))}\) on a black-box function \(f(k^{})\) such that \(\|f\|_{k^{}} B\). Then, with probability at least \(1-_{A}\), it admits the following bound on its cumulative regret \(R_{T}(R^{,B}(T))\), where \(R^{,B}(T)=(B(k^{})}+_{T}(k^{ }))\)._

In our notation, note the distinction between the regret of an algorithm \(R_{T}\) and the scaling of its bound \(R^{,B}(T)\). As the maximum regret we can possibly suffer at any time step, while optimising a function with property \(\|f\|_{k^{}} B\), is bounded as \(r_{t} 2B\)1, we are going to assume the bound obeys the property \(R^{,B}(t+1)-R^{,B}(t) 2B\) for all \(t=1,,T-1\), as otherwise the bound can be trivially improved.

In order for the bound of Theorem 2.1 to hold, we need to know the length scale \(\) and an upper bound on the RKHS norm \(B\) of the black-box function for the given kernel \(k^{}\). Inspecting the regret bound together with Proposition 2.2, we see that selecting the smallest \(B\) (i.e. the tightest bound) and the longest length scale \(\) results in the smallest \(R^{,B}(T)\). Note that the same function \(f()\), can have different RKHS norms under kernels with different length scale values. As such, to obtain the optimal scaling of the regret bound, one needs to jointly optimise for \(\) and \(B\). The optimal hyperparameters are thus \(^{},B^{}=_{,B^{+}}R^{,B}(T)\) such that \(\|f\|_{k^{^{}}} B^{}\). We assume we are given some initial \(_{0}^{}\) and \(B_{0} B^{}\). As explained in the introduction, in practice, those initial values could be found by maximising the marginal likelihood for a small number of initial data points. We now notice one interesting property. In the case of RBF and \(\)-Matern kernels, if we change the length scale value from \(_{0}\) to \(\) and the norm bound from \(B_{0}\) to \(B\), we get that the regret bound with those new hyperparameters scales as follows:

\[R^{,B}(T)=((})(}{})^{d/2}B_{0}(k^{_{0}})}+(}{})^{d}_{T}(k^{_{0}})).\]

Since \(_{T}(k^{})\) is increasing in \(T\), for large enough \(T\) we have \(B<(k^{})}\) and any \(B<(}{})^{d/2}B_{0}\) does not affect the bound's order dependence. As such, whenever we decrease lengthscale to \(\), we can increase norm bound by \((}{})^{d/2}\) essentially "for free". As such, we are going to use \(\)-dependent norms in the form of \(B(,N)=(}{})^{d/2}N\), where \(N\) is the norm bound under \(_{0}\) and becomes the new hyperparameter we wish to select, instead of \(B\). The optimal values of hyperparameters under this new parameterization are thus \(^{},N^{}= R^{,B(,N)}(T)\) subject to \(\|f\|_{k^{^{}}} B(^{},N^{})\). Notice that \(_{(0,_{0}]}B(,N^{})=N^{}\) and as such it does not make sense to try values of \(N\) smaller than \(B_{0}\). Using this new parameterization brings an important benefit, as stated next.

**Lemma 2.1** (Consequence of Lemma 4 in ).: _In case of RBF and Matern kernels, for any \(<^{}\) and \(N>N^{}\), we have that \(\|f\|_{k^{}} B(,N)\)._

We will thus refer to any pair \((,N)\), such that \(^{}\) and \(N N^{}\), as _well-specified_ hyperparameters, as the GP-UCB admits a provable regret bound when they are used (albeit that bound might not be optimal). For simplicity, we are now going to assume that \(N^{}\) is known and proceed with solving the problem of only one unknown hyperparameter \(^{}\). We will thus be writing \(_{t}^{}=_{t}^{,B(,N^{})}\), \(_{t}^{}()=_{t}^{,B(,N^{})}()\) and \(R^{}()=R^{,B(,N^{})}()\). However, we would like to emphasise that the algorithm we will propose throughout this paper can be extended to the case when \(B^{}\) is also an unknown hyperparameter, which we do in Appendix F.

Length scale Balancing

Aggregation schemes describe a set of rules that a master algorithm should follow while coordinating a number of base algorithms. One such scheme is regret-balancing with elimination . This scheme assumes each of the base algorithms comes with a _suspected regret bound_, which is a high-probability bound on its regret that holds if the algorithm is well-specified for the given problem, but might not hold if the learner is misspecified. The scheme always selects the base algorithm that currently has the smallest cumulative regret according to its suspected bound. This ensures that the regret of the master algorithm will not be too far from the regret of the best well-specified candidate. It also removes base algorithms that underperform, compared to others, by more than their suspected bound, as this means their bounds do not hold and, with a high probability, are misspecified.

Our idea is to use regret balancing with elimination while having each base algorithm be a GP-UCB algorithm with a different value of the length scale hyperparameter. Let us now discuss how to identify candidates for the length scale values. We propose the usage of a candidate-suggesting function \(q():^{+}\), such that the \(i\)th candidate length scale value to consider is given by \(q(i)\).

**Definition 3.1**.: Let us define the length scale candidate-suggesting function \(q():^{+}\) as a mapping for each \(i\) of form:

\[q(i)=_{0}e^{-i/d}.\]

We want to ensure that one of the candidates we will eventually introduce will be close to \(^{}\). Let us denote \(=_{i\,;\,q(i)^{}}q(i)\) to be the largest length scale suggested by our candidate-suggesting function that is still smaller than \(^{}\). Observe that \(\|f\|_{k^{}} B(,N^{})\) and thus \(\) is the largest well-specified length scale value among suggested candidates. As such, the regret bound of the best base learner is \(R^{}(T)\) and we hope that the regret bound of a master algorithm aggregating this learner with others will be close to \(R^{}(T)\). Comparing with the regret bound of the GP-UCB algorithm utilising the true optimal length scale value \(R^{^{}}(T)\), we get the result stated by the following Lemma 3.1, proven in Appendix C

**Lemma 3.1**.: _In the case of both RBF and \(\)-Matem kernel, we have that:_

\[}(T)}{R^{^{}}(T)}=(1).\]

This Lemma shows that the regret bound of the best of our base algorithms is only a constant factor away from the bound of the algorithm using the optimal length scale value. However, as for any \(i\), we have \(q(i)>0\), and the candidate-suggesting function \(q()\) introduces infinitely many candidates. As we can only aggregate a finite number of base algorithms, we thus propose to gradually introduce new optimisers equipped with new candidate length scale values. Observe that if we stopped our quantisation at some lower bound \(_{L}\), then we would create a maximum of \(q^{-1}(_{L})\) candidates, that is, \(d(}{_{L}})\). However, this would require us to know a sure lower bound on the optimal length scale value. Since we do not have this knowledge, we could employ a mechanism similar to A-GP-UCB, where we progressively decrease the _suspected lower bound value_\(_{L}(t)=}{g(t)}\), based on some growth function \(g(t)\). Observe that since \((}{_{L}(t)})=(g(t))\), the number of candidate values grows only logarithmically with the growth function \(g(t)\). Same as for A-GP-UCB, this growth function needs to be specified by the user, and we describe how this choice can be made in SS5. However, we would like to emphasise that, unlike A-GP-UCB which simply sets its length scale value to \(_{L}(t)\), we instead introduce new learners with shorter length scale values, while still keeping the old learners with longer values. This strategy is thus more robust to the choice of the growth function, which is reflected in better scaling of the regret bound we derive later. In Algorithm 1, we present LB-GP-UCB, an algorithm employing this mechanism. We now briefly explain the logic behind its operations.

```
1:\(_{L}(t)=_{L}(t)\)
2:\(_{L}(t)=_{L}(t)\)
3:for\(i\)do
4:\(_{L}(t)=_{L}(t)\)
5:for\(i\)do
6:\(_{L}(t)=_{L}(t)\)
7:for\(i\)do
8:\(_{L}(t)=_{L}(t)\)
9:\(_{L}(t)=_{L}(t)\)
10:\(_{L}(t)=_{L}(t)\)
11:endfor
12:return\(_{L}(t)\) ```

**Algorithm 1** Algorithm 1

The algorithm starts in line 1 by initialising the set of candidates to just the upper bound \(_{0}\). Later on, in lines 14-16, new candidates are introduced using the candidate-suggesting function \(q()\) at a pace dictated by the growth function \(g(t)\). Typically in aggregation schemes, each one of the base algorithms is run in isolation. However, there is nothing preventing us from making them share the data and as such, selecting a base algorithm in our case simply amounts to choosing the length scale value we will use to fit the GP model at a given time step \(t\), which is done in line 3. This choice is done by the regret-balancing rule \(_{t}=_{_{t}}R^{}(|S_{t}^{}|+1)\), with \(R^{}()\) defined as in Theorem 2.3 and \(S_{t}^{}\) being the set of iterations before \(t\) at which length scale value \(\) was chosen. Note that we only need to know the scaling of the bound up to a constant. This rule implies that lower length scale values will be selected less frequently than higher values, as their regret bounds grow faster. After that, in line 4, the algorithm utilises the acquisition rule dictated by a model fitted with the selected length scale value to find the point to query next, \(_{t}\). The idea is that, occasionally, \(_{t}\) will be set to one of the smaller values from \(_{t}\) and, if that results in finding significantly better function values, then the rejection mechanism in lines 9-12 will remove longer length scales from the set of considered values, \(_{t}\). Otherwise, we will keep all of the length scales and try again after some number of iterations. We now proceed to derive a regret bound for our developed algorithm.

```
0: initial length scale value \(_{0}\); suspected regret bounds \(R^{}()\); growth function \(g()\); confidence parameters \(\{_{t}\}_{t=1}^{T}\) and \(\{_{t}^{}\}_{t=1}^{T}\)
1: Set \(_{0}=\), \(_{1}=\{_{0}\}\), \(S_{0}^{}=\) for all \(\), length scale counter \(l=1\)
2:for\(t=1,,T\)do
3: Select length scale \(_{t}=_{_{t}}R^{}(|S_{t-1}^{}|+1)\)
4: Select point to query \(_{t}=_{}_{t-1}^{ _{t}}()\)
5: Query the black-box \(y_{t}=f(_{t})+_{t}\)
6: Update data buffer \(_{t}=_{t-1}(x_{t},y_{t})\)
7: For each \(_{t}\), set \(S_{t}^{}=\{=1,,t:_{}=\}\)
8: Initialise length scales set for new iteration \(_{t+1}:=_{t}\)
9:if\(_{_{t}}|S_{t}^{}| 0\)then
10: Define \(L_{t}()=(^{}|}_{ S_{t}^{}}y_ {}-}{|S_{t}^{}|}})\)
11: {# Eliminate underperforming length scale values}
12:\(_{t+1}=\{_{t}:L_{t}()+^{ }|}_{ S_{t}^{}}_{}^{}_{-1}^{}( _{})_{^{}_{t}}L_{t}(^{})\}\)
13:endif
14:if\(q(l+1)}{g(t)}\)then
15:\(_{t+1}:=_{t+1}\{q(l+1)\}\) {# Add shorter length scales}
16:\(l:=l+1\)
17:endif
18:endfor ```

**Algorithm 1** Length scale Balancing GP-UCB (LB-GP-UCB)

## 4 Regret Bound and Proof Sketch

We now state the formal regret bound of the proposed algorithm, provide a brief sketch of the proof and discuss the result.

**Theorem 4.1**.: _Let us use confidence parameters of \(_{t}=2_{N}^{2}(d(g(t))^{2}t^{2})- 3\) and \(_{t}^{}=B(,N^{})+_{N}^{ }+1+(2/))}\), then with probability at least \(1-\), the cumulative regret \(R_{T}\) of the Algorithm 1 admits the following bound:_

\[R_{T}=((t_{0}+)B^{}+(R^{^{ }}(T)+})((}{^{}} )^{d}d}{^{}}+)),\]

_where \(t_{0}=g^{-1}(e^{-1/d}_{0}/^{})\) and \(=d g(T)\)._

Proof.: (sketch) We provide a sketch of the result here and defer the proof to Appendix D.

Let us denote by \(t_{0}\) the iteration at which the first well-specified length scale (\(^{}\)) is added to the candidate set in line 15. This will happen at the first iteration after \(g^{-1}(}{^{}})\), where the condition in line 14 will trigger. Given the ratios between consecutive candidates suggested by \(q()\), we get that \(t_{0}= g^{-1}(}{^{*}}e^{-1/d})\). On iterations up to \(t_{0}\), we can potentially suffer the highest as possible, thus the cumulative regret can be bounded as:

\[R_{T}=_{t=1,,t_{0}-1}r_{t}+_{t=t_{0},,T}r_{t} 2B^{}t_{0}+ _{T},\]

where \(_{T}\) is the regret of the algorithm after \(t_{0}\). Let us define by \(\) the set of iterations, where we reject at least one length scale value in line 12. We thus have:

\[_{T}=_{t}r_{t}+_{t}r_{t} 2 B^{}||+_{t}r_{t} 2B^{}q^{-1}( }{g(T)})+_{t}r_{t},\]

where the second inequality comes from the fact that we cannot reject more candidates than we have introduced in total. The remaining thing to do is to bound \(_{t}r_{t}\). This expression is the cumulative regret of the iterations, where no candidates are rejected and where at least one of the well-specified candidates has been introduced. We can bound this term using a similar strategy as in . First, we show that, with a probability of at least \(1-\), the well-specified candidate introduced at \(t_{0}\) will not be rejected. Second, since no other candidates are rejected at iterations \(t\), it means that the function values achieved at those iterations cannot be too different from the ones achieved when using \(\). Using this fact, we arrive at a statement:

\[_{t}r_{t}(R^{}(|S_{t}^{ }|)+})(_{_{0}}^{}|}{|S_{t}^{}|}}+q^{-1}(}{ g(T)})),\]

where \(_{0}\) is the set of misspecified length scale values that were chosen at least once after \(t_{0}\). The rest of the proof consists of bounding \(^{}|}{|S_{t}^{}|}\), which can be done due to the selection rule in line 3.

**Optimality** In Appendix H, we show that for a fixed choice of growth function, we get \(R_{T}/R^{^{*}}(T)=(d g(T))\). This is an improvement compared to A-GP-UCB achieving \(R_{T}/R^{^{*}}(T)=(g(T)^{d})\). As such the bound of our algorithm is significantly closer to the optimal bound than the one of A-GP-UCB. The faster \(g()\) is increasing, the quicker we will be able to find the first well-specified candidate, which will decrease the term \(t_{0}B^{}\) in the bound. At the same time, it will increase all the terms depending on \(g(T)\), but as our bound only scales with \(d g(T)\), we are able to select much more aggressive growth functions than A-GP-UCB, whose bound scales with \(g(T)^{d}\). In the Experiments section we compare the performance of LB-GP-UCB and A-GP-UCB using different growth functions \(g(t)\) and show that the former algorithm is much more robust to the choice of \(g(t)\).

**Extension to unknown \(N^{}\)** As we discussed before, LB-GP-UCB requires us to know the initial RKHS norm \(N^{}\). However, we can easily extend the algorithm to handle the case of unknown \(N^{}\), which we do in Appendix F. In Algorithm 3 we present Length scale and Bound Balancing (LNB) -- an algorithm, which in addition to having candidates for \(\) also maintains a number of candidates for \(N^{}\). As such, it requires us to specify another growth function \(b(t)\) for exploring new norm values as well as the initial RKHS norm \(B_{0}\). We prove its cumulative regret bound in Theorem F.1. We can similarly derive the suboptimality gap for our algorithm in this case. We display it in Table 1 together with the gap of A-GP-UCB. We can see that in this setting, we also achieve an improvement.

   Algorithm & Optimality & \(R_{T}/R^{}(T)\) \\  & Unknown \(\) & Unknown \(\) and \(B\) \\  A-GP-UCB  & \((g(T)^{d})\) & \((b(T)g(T)^{d})\) \\ LB-GP-UCB / LNB-GP-UCB (ours) & \((d g(T))\) & \((d g(T) b(T))\) \\   

Table 1: Comparison of optimality for A-GP-UCB and LB-GP-UCB for fixed functions \(g()\) and \(b()\). \(R^{}(T)\) refers to the scaling of the regret bound of an oracle optimiser, knowing the optimal hyperparameters. See Appendix H for more details.

Experiments

We now evaluate the performance of our algorithm on multiple synthetic and real-world functions. To run experiments we used the compute resources listed in Appendix I.1 and implemented based on the codebase of , which uses the BoTorch package [5; 34]. We open-source our code2. We used the UCB acquisition function and we compared different techniques for selecting the length scale value. For all experiments, we used isotropic \(\)-Matern kernel with \(=2.5\). We standardise the observations before fitting the GP model and as such keep the kernel outputscale fixed to \(1.0\). The first baseline we compare against is MLE, where the length scale value is optimised using a multi-start L-BFGS-B method  (the default BoTorch optimiser ) after each timestep by maximising the marginal likelihood for the data collected so far. The next baseline is MCMC, where we employ a fully Bayesian treatment of the unknown length scale value using the NUTS sampler , which we implemented using Pyro . We use BoTorch's default hyperprior (\((3,6)\)) and to select a new point we optimise the expected acquisition function under the posterior samples as described by . We also compare against A-GP-UCB. To achieve a fair comparison, we used the same growth function \(g(t)=\{t_{0},\}\) for both LB-GP-UCB and A-GP-UCB across all experiments, where \(t_{0}\) was selected so that at least 5 candidates are generated for \(g(1)\). We study the impact of this choice in the ablation section. We used 10 initial points for each algorithm unless specified otherwise. To select upper bound \(_{0}\) for A-GP-UCB and LB-GP-UCB we fitted a length scale to initial data points with MLE (and we did not use MLE after that). We present the results in Figure 3 below. We show running times in Table 2 in Appendix I.2. We now describe each benchmark problem in detail.

**Berkenkamp Toy Problem** We start with a one-dimensional toy problem proposed by the same paper that proposed the A-GP-UCB algorithm . We showed a plot of this one-dimensional function in Figure 1. On the right side of the domain, the function appears to be smoother than on the left side. In this problem, we only use three initial points, to benchmark the ability of algorithms to escape from the local optimum on the right side of the domain. MLE and MCMC can be easily misled towards too-long length scale values, which causes them to get stuck in the local optimum. Both A-GP-UCB and LB-GP-UCB quickly find the optimal solution, however, due to over-exploration, the cumulative regret of A-GP-UCB grows faster than that of LB-GP-UCB.

**Michalewicz Synthetic Function** As a next benchmark, we evaluate our algorithm on the five-dimensional Michalewicz synthetic function, which has been designed to be challenging while using MLE for fitting hyperparameters, because it exhibits different degrees of smoothness throughout its domain. In the histogram in Figure 2, we compare the selected length scale value with an estimate of the optimal length scale \(^{*}\). We produce this estimate by sampling ten thousand points uniformly through the domain and fitting a length scale value by maximum likelihood to the points with the

Figure 3: Regret results of the proposed algorithm and baselines on synthetic and real-world tasks. We ran 20 seeds on Berkenkamp and AGNP and 10 seeds on Michalewicz and Crossedbarrel problems. Shaded areas correspond to standard errors.

top 1% of objective values. In this way, we are able to capture the length scale value that produces a good model for the Michalewicz function around the optimum, where it is least smooth. We can see LB-GP-UCB selects lengthscale value closer to \(^{*}\) and as a result outperforms other baselines in terms of both cumulative and best regret metrics.

**Material Design Problems** We utilise material design tasks proposed by  and  - the 4-dimensional CrossedBarrel and 5-dimensional AGNP tasks. At each time step, the algorithm can choose which material configuration to try, and observe the objective value, which corresponds to a given material optimisation criterion. As material design problems are known to exhibit a needle-in-a-haystack behaviour , on both benchmarks MLE and MCMC get stuck at a suboptimal solution and their best regret does not fall beyond a certain value. A-GP-UCB is able to quickly find low-regret solutions on the AGNP benchmark, but struggles on the Crossedbarrel problem and underperforms in terms of cumulative regret. On the contrary, LB-GP-UCB performs well across both benchmark problems and across both regret metrics.

**Ablation on \(g(t)\)** To test robustness of LB-GP-UCB, we evaluate it on Michalewicz function together with A-GP-UCB for different choices of \(g(t)\). We try functions of form \(g(t)=(t_{0},t^{a})\) for \(a(0.25,0.5,0.75)\). In Figure 4 we plot the final performance of the algorithms after \(N=250\) steps as well as the distribution of selected length scale values. We see that A-GP-UCB is very sensitive to the selection of growth function \(g(t)\), whereas our algorithm selects similar length scale values regardless of \(g(t)\), which results in consistently good best regret results. We can also see that LB-GP-UCB typically selects values around \(^{*}\) for different growth functions, whereas A-GP-UCB decreases its length scale beyond \(^{*}\), resulting in slower convergence.

## 6 Related Work

We already mentioned the work proposing A-GP-UCB , which so far has been the only work providing the guarantees on BO with unknown hyperparameters, where only an upper bound on the optimal length scale value is known. The work of  addresses the problem, where, in addition, a lower bound on the optimal length scale is known, however, the regret bound of the algorithm they propose scales with the \(_{T}(k^{_{L}})\) of the smallest possible length scale \(_{L}\), making it no better than a naive algorithm always selecting \(_{L}\).  studied the problem of solving BO, when the kernel function is misspecified, however, provided no method for finding the well-specified kernel function.  proved a lower bound on the algorithm's regret in the case when the regularity of RKHS is unknown (which corresponds to an unknown \(\) hyperparameter in the case of Matern kernel), compared to their work we focused on different unknown hyperparameters, such as the length scale. There have also been a number of works [13; 21; 27; 32] that tackled the problem of BO with unknown hyperparameters but did not provide a theoretical analysis of the used algorithm. Some of earlier works [15; 38] viewed BO with unknown hyperparameters as meta-learning or transfer learning problem, where a large dataset is available for pre-training. In our problem setting, we do not assume access to any such pre-training data. Within this work, we considered a frequentist problem setting, where the black-box function is arbitrarily selected from some RKHS. While there are no guarantees

Figure 4: Ablation study of the choice of growth function \(g(t)\). \(t_{0}\) is chosen so that at least 5 candidates are generated at \(g(1)\). See beginning of ยง5 for details.

for the consistency of MLE in such a setting, if we were to assume a Bayesian setting and put a GP prior on the black box, statistical literature derived asymptotic consistency results [4; 22; 24; 28] for MLE of kernel hyperparameters, including length scale. Under such a Bayesian setting,  studied the problem of BO with unknown prior, when we are given a finite number of candidate priors.  derived predictive guarantees for GP in a Bayesian setting with unknown hyperparameters, provided that hyperpriors on those hyperparameters are known. However, the authors do not provide any BO algorithm based on their results.

## 7 Conclusions

Within this work, we addressed the problem of BO with unknown hyperparameters. We proposed an algorithm with a cumulative regret bound logarithmically closer to optimal than the previous state of the art and showed that our algorithm can outperform existing baselines in practice. One limitation of our work is that we only showed how to handle the isotropic case, i.e. where the same length scale value is applied for every dimension. This limitation is because our algorithm requires the knowledge of the regret bounds of an optimiser utilising a given length scale value, which in turn requires the knowledge of the bounds on MIG \(_{T}(k^{})\) for the used kernel. To the best of our knowledge, within the existing literature, no work has yet derived those bounds in non-isotropic cases. However, we believe that if such bounds were obtained, one could easily extend our algorithm to the non-isotropic case, in the same way as we extended our base algorithm to handle unknown norm and length scale simultaneously. This constitutes a promising direction of future work.

Another limitation of our work is the assumption of known noise magnitude \(_{N}\). The problem of simultaneously not knowing kernel hyperparameters and noise magnitude is extremely challenging, as large variations in observed function values can be a result of either short lengthscale value or large noise magnitude. To the best of our knowledge, previous work did not tackle this setting and it remains an open problem.

Our algorithm relies on the standard GP model, which can result in poor scalability to large datasets and high-dimensional spaces. Extending our work to sparse GPs [29; 31] and kernels specifically designed for a high number of dimensions [14; 42] is another possible direction of future work.