# SuperDeepFool: a new fast and accurate minimal adversarial attack

Alireza Abdollahpoorrostam

EPFL

Lausanne, Switzerland

alireza.abdollahpoorrostam@epfl.ch

&Mahed Abroshan

Imperial College, London, UK

m.abroshan23@imperial.ac.uk

&Seyed-Mohsen Moosavi-Dezfooli

Apple

Zurich, Switzerland

smoosavi@apple.com

###### Abstract

Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal \(_{2}\) adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal \(_{2}\) adversarial perturbations.

## 1 Introduction

Deep learning has achieved breakthrough improvement in numerous tasks and has developed as a powerful tool in various applications, including computer vision  and speech processing . Despite their success, deep neural networks are known to be vulnerable to adversarial examples, carefully perturbed examples perceptually indistinguishable from original samples . This can lead to a significant disruption of the inference result of deep neural networks. It has important implications for safety and security-critical applications of machine learning models.

Our goal in this paper is to introduce a parameter-free and simple method for accurately and reliably evaluating the adversarial robustness of deep networks in a fast and geometrically-based fashion. Most of the current attack methods rely on general-purpose optimization

Figure 1: The average number of gradient computations vs the mean \(_{2}\)-norm of perturbations. It shows that our novel fast and accurate method, SDF, outperforms other minimum-norm attacks. SDF finds significantly smaller perturbations compared to DF, with only a small increase in computational cost. SDF also outperforms other algorithms in optimality and speed. The numbers are taken from Table 5.

techniques, such as Projected Gradient Descent (PGD)  and Augmented Lagrangian , which are oblivious to the geometric properties of models. However, deep neural networks' robustness to adversarial perturbations is closely tied to their geometric landscape . Given this, it would be beneficial to exploit such properties when designing and implementing adversarial attacks. This allows to create more effective and computationally efficient attacks on classifiers. Formally, for a given classifier \(\) and input \(\), we define an adversarial perturbation as the minimal perturbation \(\) that is sufficient to change the estimated label \(()\):

\[(;):=_{}\|\|_{2}(+)().\] (1)

DeepFool (DF)  was among the earliest attempts to exploit the "excessive linearity"  of deep networks to find minimum-norm adversarial perturbations. However, more sophisticated attacks were later developed that could find smaller perturbations at the expense of significantly greater computation time.

In this paper, we exploit the geometric characteristics of minimum-norm adversarial perturbations to design a family of fast yet simple algorithms that achieves a better trade-off between computational cost and accuracy in finding \(_{2}\) adversarial perturbations (see Fig. 1). Our proposed algorithm, guided by the characteristics of the optimal solution to Eq. (1), enhances DF to obtain smaller perturbations, while maintaining simplicity and computational efficiency that are only slightly inferior to those of DF. Our main contributions are summarized as follows:

* We introduce a novel family of fast yet accurate algorithms to find minimal adversarial perturbations. We conduct a comprehensive evaluation of our algorithms against state-of-the-art (SOTA) adversarial attack methods across multiple scenarios. Our findings demonstrate that our algorithm identifies minimal yet accurate perturbations with significantly greater efficiency than competing SOTA approaches (4).
* Our algorithms are developed in a systematic and well-grounded manner, based on theoretical analysis (3).
* We further improve the robustness of state-of-the-art image classifiers to minimum-norm adversarial attacks via adversarial training on the examples obtained by our algorithms (4.3).
* We significantly improve the time efficiency of the state-of-the-art Auto-Attack (AA)  by adding our proposed method to the set of attacks in AA (4.3).
* We revisit the importance of minimal adversarial perturbations as a proxy to demystify deep neural network properties (Appendix G, Appendix O).

**Related works.** It has been observed that deep neural networks are vulnerable to adversarial examples . To exploit this vulnerability, a range of methods have been developed for generating adversarial perturbations for image classifiers. These attacks occur in two settings: white-box, where the attacker has complete knowledge of the model, including its architecture, parameters, defense mechanisms, etc.; and black-box, where the attacker's knowledge is limited, mostly relying on input queries to observe outputs . Further, adversarial attacks can be broadly categorized into two categories: bounded-norm attacks (such as FGSM  and PGD ) and minimum-norm attacks (such as DF and C\(\&\)W ) with the latter aimed at solving Eq. (1). In this work, we specifically focus on white-box minimum \(_{2}\)-norm attacks.

The authors in  studied adversarial examples by solving a penalized optimization problem. The optimization approach used in  is complex and computationally inefficient; therefore, it cannot scale to large datasets. The method proposed in  applied a single-step of the input gradient to generate adversarial examples efficiently. DF was the first method to seek minimum-norm adversarial perturbations, employing an iterative approach. It linearizes the classifier at each step to estimate the minimal adversarial perturbations efficiently. C\(\&\)W attack  transform the optimization problem in  into an unconstrained optimization problem. C\(\&\)W leverages the first-order gradient-based optimizers to minimize a balanced loss between the norm of the perturbation and misclassification confidence. Inspired by the geometric idea of DF, FAB  presents an approach to minimize the norm of adversarial perturbations by employing complex projections and approximations while maintaining proximity to the decision boundary. By utilizing gradients to estimate the local geometry of the boundary, this method formulates minimum-norm optimization without the need for tuning a weighting term. DDN  uses projections on the \(_{2}\)-ball for a given perturbation budget \(\). FMN extends the DDN attack to other \(_{p}\)-norms. By formulating (1) with Lagrange's method, ALMA  introduced a framework for finding adversarial examples for several distances.

**Why does \(_{2}\) white-box adversarial robustness matter?** The reasons for using \(_{2}\) norm perturbations are manifold. We acknowledge that \(_{2}\) threat model may not seem particularly realistic in practical scenarios (at least for images); however, it can be perceived as a basic threat model amenable to both theoretical and empirical analyses, potentially leading insights in tackling adversarial robustness in more complex settings. The fact that, despite considerable advancements in AI/ML, we are yet to solve adversarial vulnerability, motivates part of our community to return to the basics and work towards finding fundamental solutions to this issue [9; 25; 34]. In particular, thanks to their intuitive geometric interpretation, \(_{2}\) perturbations provide valuable insights into the geometry of classifiers. They can serve as an effective tool in the "interpretation/explanation" toolbox to shed light on what/how these models learn. Moreover, it has been demonstrated that [19; 38], \(_{2}\) robustness has several applications beyond security (for more details on the necessity of robustness to \(_{p}\) norms, please refer to Appendix O).

## 2 DeepFool (DF) and Minimal Adversarial Perturbations

In this section, we first discuss the geometric interpretation of the minimum-norm adversarial perturbations, i.e., solutions to the optimization problem in Eq. (1). We then examine DF to demonstrate why it may fail to find the optimal minimum-norm perturbation. Then in the next section, we introduce our proposed method that exploits DF to find smaller perturbations.

Let \(f:^{d}^{C}\) denote a \(C\)-class classifier, where \(f_{k}\) represents the classifier's output associated to the \(k\)th class. Specifically, for a given datapoint \(^{d}\), the estimated label is obtained by \(()=_{k}f_{k}()\), where \(f_{k}()\) is the \(k^{}\) component of \(f()\) that corresponds to the \(k^{}\) class. Note that the classifier \(f\) can be seen as a mapping that partitions the input space \(^{d}\) into classification regions, each of which has a constant estimated label (i.e., \((.)\) is constant for each such region). The decision boundary \(\) is defined as the set of points in \(^{d}\) such that \(f_{i}()=f_{j}()=_{k}f_{k}()\) for some distinct \(i\) and \(j\). Additive \(_{2}\)-norm adversarial perturbations are inherently related to the geometry of the decision boundary. More formally, Let \(^{d}\), and \(^{*}()\) be the minimal adversarial perturbation defined as the minimizer of Eq. (1). Then:

_Properties of minimal adversarial perturbation \(^{*}()\)_:

1 It is orthogonal to the decision boundary of the classifier \(\).

2 Its norm, i.e., \(\|^{*}()\|_{2}\) measures the Euclidean distance between \(\) and \(\), that is \(+^{*}\) lies on \(\).

We aim to investigate whether the perturbations generated by DF satisfy the aforementioned two conditions. Let \(_{}\) denote the perturbation found by DF for a datapoint \(\). We expect \(+_{}\) to lie on the decision boundary. Hence, if \(\) is the minimal perturbation, for all \(0<<1\), we expect the perturbation \(\) to remain in the same decision region as of \(\) and thus fail to fool the model.

Fig. 2 illustrates the two conditions discussed in Section 2. In the figure, \(n_{1}\) and \(n_{2}\) represent two orthogonal vectors to the decision boundary. The optimal perturbation vector \(^{*}\) aligns parallel to \(n_{2}\). On the other hand, a non-optimal perturbation \(_{}\) forms an angle \(\) with \(n_{1}\).

In Fig. 3 (left), we consider the fooling rate of \(\,_{}\) for \(0.2<<1\). For a minimum-norm perturbation, we expect an immediate sharp decline for \(\) close to one. However, in Fig. 3 (top-left) we cannot observe such a decline (a sharp decline happens close to \(=0.9\), not 1). This is a confirmation that DF typically finds an overly perturbed point. One potential reason for this is the fact that DF stops when a misclassified point is found, and this point might be an overly perturbed one within the adversarial region, and not necessarily on the decision boundary.

Now, let us consider the other characteristic of the minimal adversarial perturbation. That is, the perturbation should be orthogonal to the decision boundary. We measure the angle between the found

Figure 2: Illustration of the optimal adversarial example \(+^{*}\) for a binary classifier \(f\); the example lies on the decision boundary (set of points where \(f()=0\)) and the perturbation vector \(^{*}\) is orthogonal to this boundary.

perturbation \(_{}\) and the normal vector orthogonal to the decision boundary (\( f(+_{})\)). To do so, we first scale \(_{}\) such that \(+_{}\) lies on the decision boundary. It can be simply done via performing a line search along \(_{}\). We then compute the cosine of the angle between \(_{}\) and the normal to the decision boundary at \(+_{}\) (this angle is denoted by \(()\)). A necessary condition for \(_{}\) to be an optimal perturbation is that it must be parallel to the normal vector of the decision boundary. In Fig. 3 (right), we show the distribution of cosine of this angle. Ideally, we wanted this distribution to be accumulated around one. However, it clearly shows that this is not the case, which is a confirmation that \(_{}\) is not necessarily the minimal perturbation.

## 3 SuperDeepFool: Efficient Algorithms to Find Minimal Perturbations

In this section, we propose a new class of methods that modifies DF to address the aforementioned challenges in the previous section. The goal is to maintain the desired characteristics of DF, i.e., computational efficiency and the fact that it is parameter-free while finding smaller adversarial perturbations. We achieve this by introducing an additional projection step which its goal is to steer the direction of perturbation towards the optimal solution of Eq. (1). Let us first briefly recall how DF finds an adversarial perturbations for a classifier \(f\). Given the current point \(_{i}\), DF updates it according to the following equation:

\[_{i+1}=_{i}-_{i})}{|| f(_{i})||_{2}^{2}}  f(_{i}).\] (2)

Here the gradient is taken w.r.t. the input. The intuition is that, in each iteration, DF finds the minimum perturbation for a linear classifier that approximates the model around \(_{i}\). The below proposition shows that under certain conditions, repeating this update step eventually converges to a point on the decision boundary.

**Proposition 1**: _Let the binary classifier \(\)1: \(^{d}\) be continuously differentiable and its gradient \(\) is \(\)-Lipschitz. For a given input sample \(_{0}\), suppose \((_{0},)\) is a ball centered around \(_{0}\) with radius \(\), such that there exists \(^{}(_{0},)\) that \(f(^{})=0\). If \(\|\|_{2}\) for all \(\) and \(<()^{2}\), then DF iterations converge to a point on the decision boundary._

_Proof: We defer the proof to the Appendix._

Notice while the proposition guarantees the perturbed sample to lie on the decision boundary, it does not state anything about the orthogonality of the perturbation to the decision boundary.

To find perturbations that are more aligned with the normal to the decision boundary, we introduce an additional projection step that steers the perturbation direction towards the optimal solution of

Figure 4: Histogram of the cosine angle between the normal to the decision boundary and the perturbation vector obtained by C\(\&\)W and FMN.

Figure 3: **(Left)** we generated 1000 images with one hundred \(\) between zero and one, and the fooling rate of the DeepFool and SuperDeepFool is reported. This experiment is done on the CIFAR10 dataset and ResNet18 model. **(Right)** histogram of the cosine angle between the normal to the decision boundary and the perturbation vector obtained by DeepFool and SuperDeepFool has been showed.

Eq. (1). Formally, the optimal perturbation, \(^{*}\), and the normal to the decision boundary at \(_{0}+^{*}\), \( f(_{0}+^{*})\), should be parallel. Equivalently, \(^{*}\) should be a solution of the following maximization problem:

\[_{}^{} f(_{0}+)}{\| f(_ {0}+)\|\|\|},\] (3)

which is the cosine of the angle between \(\) and \( f(_{0}+)\). A necessary condition for \(^{*}\) to be a solution of Eq. (3) is that the projection of \(^{*}\), i.e, (\(_{}\)) on the subspace orthogonal to \( f(_{0}+^{*})\) should be zero. Then, \(^{*}\) can be seen as a fixed point of the following iterative map:

\[_{i+1}=T(_{i})=_{i}{}^{} f(_{0}+_{i})}{\| f(_{0}+_{i})\|}_{0}+ {r}_{i})}{\| f(_{0}+_{i})\|}.\] (4)

The scalar multiplier on the right-hand side of Eq. (4) represents the norm of the projection of the vector \(_{i}\) along the gradient direction. The following proposition shows that this iterative process can converge to a solution of Eq. (3).

**Proposition 2**: _For a differentiable \(f\) and a given \(_{0}\), \(_{i}\) in the iterations Eq. (4) either converge to a solution of Eq. (3) or a trivial solution (i.e., \(_{i} 0\))._

_Proof: We defer the proof to the Appendix._

Intuitively, by the geometrical properties of a decision boundary (\(\)), a small portion of the boundary can be enclosed between two affine parallel hyperplane. The following proposition from () states that the angle between \( f()\) and the optimal direction \( f(+^{*})\) can be bounded in a neighborhood of the boundary \(\).

**Proposition 3**: _() Given a radius \(>0\) and \(_{}\) is the set of all samples whose distance from the decision boundary \(\) is less than \(\). For each angle \(||(0,)\), there exists a distance \(}_{()}\), such that, for all \(_{}_{()}}\), the following inequality holds:_

\[)^{} f(_{}())}{\|  f()\|\| f(_{}())\|}>( ),\] (5)

_where \(_{}\) is the unique projection of \(\) on the \(\)._

_Proof: We defer the proof to the Appendix._

### A Family of Adversarial Attacks

``` Input: image \(_{0}\), classifier \(f\), \(m\), and \(n\). Output: perturbation \(\)
1 Initialize: \(_{0}\) while\((f())=(f(_{0}))\)do repeat\(m\)times
2\(-)\|}{\| f()\|_{2}^{2}} f ()\)
3 end whilerepeat\(n\)times
4\(_{0}+-_{0})^{} f()}{\|  f()\|^{2}} f()\)
5 end while
6repeat\(n\)times
7\(_{0}+-_{0})^{} f()}{\|  f()\|^{2}} f()\)
8 end while
9
10 end while return\(=-_{0}\) ```

**Algorithm 1**SDF (\(m\),\(n\)) for binary classifiers

Finding minimum-norm adversarial perturbations can be seen as a multi-objective optimization problem, where we want \(f(+)=0\) and the perturbation \(\) to be orthogonal to the decision boundary. So far we have seen that DF finds a solution satisfying the former objective and the iterative map Eq. (4) can be used to find a solution for the latter. A natural approach to satisfy both objectives is to _alternate_ between these two iterative steps, namely Eq. (2) and Eq. (4). We propose a family of adversarial attack algorithms, coined SuperDeepFool, by varying how frequently we alternate between these two steps. We denote this family of algorithms with \((m,n)\), where \(m\) is the number of DF steps Eq. (2) followed by \(n\) repetition of the projection step Eq. (4). This process is summarized in Algorithm 1. One interesting case is \((,1)\) which, in each iteration, continues DF steps till a point on the decision boundary is found and then applies the projection step.

``` Input: image \(_{0}\), classifier \(f\), \(m\), and \(n\). Output: perturbation \(\)
1 Initialize: \(_{0}\) while\((f())=(f(_{0}))\)do
2repeat\(m-)\|}{\| f()\|^{2}} f()\)
3 end while
4repeat\(n-)\|}{\| f()\|^{2}} f()\)
5 end while
6repeat\(n-)\|}{\| f()\|^{2}} f()\)
7 end while
8
9 end while
10
11 end while return\(=-_{0}\) ```

**Algorithm 2**Algorithm 3**DFS (\(m\),\(n\)) for binary classifiers

This particular case has a resemblance with the strategy used in  to find black-box adversarial perturbations. This algorithm can be interpreted as iteratively approximating the decision boundary with a hyperplane and then analytically calculating the minimal adversarial perturbation for a linear classifier for which this hyperplane is the decision boundary. It is justified by the observation that the decision boundary of state-of-the-art deep networks has a small mean curvature around data samples . A geometric illustration of this procedure is shown in Figure 5.

### SDF Attack

We empirically compare the performance of \((m,n)\) for different values of \(m\) and \(n\) in Section 4.1. Interestingly, we observe that we get better attack performance when we apply several DF steps followed by a single projection. Since the standard DF typically finds an adversarial example in less than four iterations for state-of-the-art image classifiers, one possibility is to continue DF steps till an adversarial example is found and then apply a single projection step. We simply call this particular version \((,1)\) of our algorithm SDF, which we will extensively evaluate in Section 4.

SDF can be understood as a generic algorithm that can also work for the multi-class case by simply substituting the first inner loop of Algorithm 1 with the standard multi-class DF algorithm. The label of the obtained adversarial example determines the boundary on which the projection step will be performed. A summary of multi-class SDF is presented in Algorithm 2. Compared to the standard DF, this algorithm has an additional projection step. We will see later that such a simple modification leads to significantly smaller perturbations.

Table 1 demonstrates that SDF family outperforms DF in finding more accurate perturbations, particularly \((,1)\) which significantly outperforms DF at a small cost.

Like any other gradient-based optimization method tackling a non-convex problem, providing a definitive explanation for why one algorithm outperforms others is not straightforward. We have the following speculation on why \((,1)\) consistently outperforms the other configurations: Note that each projection step reduces the perturbation, while each DF step moves the perturbation nearer to the boundary. So when projection is repeated multiple times (\(n>1\)), it might undo the progress made by DF, potentially slowing down the algorithm's convergence. On the other hand, by first reaching a boundary point through multiple DF steps and then applying the projection operator just once, we at least ensure that the algorithm has reached intermediate adversarial examples. Each subsequent outer loop is hoped to incrementally move the adversarial example closer to the optimal point (see 5).

``` Input: image \(_{0}\), classifier \(f\). Output: perturbation \(\)
1 Initialize: \(_{0}\)
2while\(()=(_{0})\)do
3\(}()\)
4\( f_{(})}(})-  f_{(_{0})}(})\)
5\(_{0}+}-_{0})^{}} {\|\|^{2}}\)
6 end while return\(=-_{0}\) ```

**Algorithm 2**SDF for multi-class classifiers

## 4 Experimental Results

In this section, we conduct extensive experiments to demonstrate the effectiveness of our method in different setups and for several natural and adversarially trained networks. We first introduce our experimental settings, including datasets, models, and attacks. Next, we compare our method with state-of-the-art \(_{2}\)-norm adversarial attacks in various settings, demonstrating the superiority of our simple yet fast algorithm for finding accurate adversarial examples. Moreover, we add SDF to the collection of attacks used in AutoAttack, and call the new set of attacks AutoAttack++. This setup

   Attack & Median-\(_{2}\) & Grads \\  DF & \(0.15\) & \(\) \\ SDF (1,1) & \(0.13\) & \(22\) \\ SDF (1,3) & \(0.14\) & \(26\) \\ SDF (3,1) & \(0.11\) & \(30\) \\  \((,1)\) & \(\) & \(32\) \\   

Table 1: Comparison of \(_{2}\)-norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in  studies.

meaningfully speeds up the process of finding norm-bounded adversarial perturbations. We also demonstrate that a model adversarially training using the SDF perturbations becomes more robust compared to the models2 trained using other minimum-norm attacks. Please refer to Appendix B for details of the experimental setup and metrics.

### Comparison with DeepFool (DF)

In this part, we compare our algorithm in terms of orthogonality and size of the \(_{2}\)-norm perturbations especially with DF. Assume \(\) is the perturbation vector obtained by an adversarial attack. First, we measure the orthogonality of perturbations by measuring the inner product between \( f(+)\) and \(\). As we explained in Section 2, a larger inner product between \(\) and the gradient vector at \(f(+)\) indicates that the perturbation vector is closer to the optimal perturbation vector \(^{*}\). We compare the orthogonality of different members of the SDF family and DF.

The results are shown in Table 2. We observe that DF finds perturbations orthogonal to the decision boundary for low-complexity models such as LeNet, but fails to perform effectively when evaluated against more complex ones. In contrast, attacks from the SDF family consistently found perturbations with a larger cosine of the angle for all three models.

Verifying optimality conditions for SDF.We validate the optimality conditions of the perturbations generated by SDF using the procedure outlined in Section 2. Comparing Fig. 3 DF and SDF, it becomes evident that our approach effectively mitigates the two issues we previously highlighted for DF. Namely, the alignment of the perturbation with the normal to the decision boundary and the problem of over-perturbation. We can see that unlike DF, the cosine of the angle for SDF is more concentrated around one, which indicates that the SDF perturbations are more aligned with the normal to the decision boundary. Moreover, Fig. 3 shows a sharper decline in the fooling rate (going down quickly to zero) when \(\) decreases. This is consistent with our expectation for an accurate minimal perturbation attack.

### Comparison with minimum-norm attacks

We now compare SDF with SOTA minimum \(_{2}\)-norm attacks: C&W, FMN, DDN, ALMA, and FAB. For C&W, we use the same hyperparameters as in . We use FMN, FAB, DDN, and ALMA with budgets of \(100\) and \(1000\) iterations and report the best performance. For a fair comparison, we clip the pixel-values of SDF-generated adversarial images to \(\), consistent with the other minimum-norm attacks. We report the average number of gradient computations per sample, as these operations are computationally intensive and provide a consistent metric unaffected by hardware differences. We also provide a runtime comparison (Appendix Table 19).

We evaluate the robustness of the IBP model, which is adversarially trained on the MNIST dataset, against SOTA attacks in Table 3. We choose this robust model as it allows us to have a more nuanced comparison between different adversarial attacks. SDF and ALMA are the only attacks that achieve a \(100\%\) percent fooling rate against this model, whereas C&W is unsuccessful on most of the data samples. The fooling rates of the remaining attacks also degrade when evaluated with \(100\) iterations. For instance, FMN's fooling rate decreases from \(89\%\) to \(67.8\%\) when the number of iterations is reduced from \(1000\) to \(100\). This observation shows that, unlike SDF, selecting the _necessary number of iterations_ is critical for the success of _fixed-iteration_ attacks. Even for ALMA which can achieve

    &  \\   & LeNet & RN18 & WRN-28-10 \\  DF & \(0.89\) & \(0.14\) & \(0.21\) \\ SDF (1,1) & \(0.90\) & \(0.63\) & \(0.64\) \\ SDF (1,3) & \(0.88\) & \(0.61\) & \(0.62\) \\ SDF (3,1) & \(\) & \(0.70\) & \(0.72\) \\  SDF (\(,1\)) & \(\) & \(\) & \(\) \\   

Table 2: The cosine similarity between the perturbation vector(\(\)) and \( f(+)\). We performed this experiment on three models trained on CIFAR10.

   Attack & FR & Median-\(_{2}\) & Grads \\  DF & \(93.4\) & \(5.31\) & \(43\) \\ ALMA (1000) & \(\) & \(\) & \(100\) \\ DDN (1000) & \(99.27\) & \(1.46\) & \(1000\) \\ FAB (1000) & \(99.98\) & \(3.34\) & \(10000\) \\ FMN (1000) & \(89.08\) & \(1.34\) & \(1000\) \\ C\&W & \(4.63\) & \(-9000\) \\ SDF & \(\) & \(\) & \(\) \\   

Table 3: We evaluate the performance of iteration-based attacks on MNIST using IBP models, noting the iteration count in parentheses. Our analysis focuses on the best-performing versions, highlighting their significant costs when encountered powerful robust models.

a nearly perfect FR, decreasing the number of iterations from \(1000\) to \(100\) causes the median norm of perturbations to increase fourfold. In contrast, SDF is able to compute adversarial perturbations using the fewest number of gradient computations while still outperforming the other algorithms, except ALMA, in terms of the perturbation norm. However, it is worth noting that ALMA requires twenty times more gradient computations compared to SDF to achieve a marginal improvement in the perturbation norm.

Table 4 compares SDF with SOTA attacks on the CIFAR10 dataset. The results show that SOTA attacks have a similar norm of perturbations, but an essential point is the speed of attacks. SDF finds more accurate adversarial perturbation very quickly rather than other algorithms.

We also evaluated all attacks on an adversarially trained model for the CIFAR10 dataset. SDF achieves smaller perturbations with half the gradient calculations than other attacks. SDF finds smaller adversarial perturbations for adversarially trained networks at a significantly lower cost than other attacks, requiring only \(20\%\) of FAB's cost and \(50\%\) of DDN's and ALMA's (See Tables 11, 19 in the Appendix).

Table 5 demonstrates the performance of SDF on a naturally and adversarially trained models on ImageNet dataset. Unlike models trained on CIFAR10, where the attacks typically result in perturbations with similar norm, the differences between attacks are more nuanced for ImageNet models.

In particular, FAB, DDN, and FMN performance degrades when the dataset changes. In contrast, SDF achieves smaller perturbations at a significantly lower cost than ALMA. This shows that the geometric interpretation of optimal adversarial perturbation, rather than viewing (1) as a non-convex optimization problem, can lead to an efficient solution. On the complexity aspect, the proposed approach is substantially faster than the other methods. In contrast, these approaches involve a costly minimization of a series of objective functions. We empirically observed that SDF converges in less than \(5\) or \(6\) iterations to a fooling perturbation; our observations show that SDF consistently achieves SOTA minimum-norm perturbations across different datasets, models, and training strategies, while requiring the least number of gradient computations. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large datasets.

### SDF Adversarial Training (AT)

In this section, we evaluate the performance of a model adversarially trained using SDF against minimum-norm attacks and AutoAttack. Our experiments provide valuable insights into the effectiveness of adversarial training with SDF and sheds light on its potential applications in building more robust models. Adversarial training requires computationally efficient attacks, making costly options such as C\(\&\)W unsuitable. Therefore, an attack that is parallelizable (both on batch size and gradient computation) is desired for successful adversarial training. SDF possesses these crucial properties, making it a promising candidate for building more robust models.

We adversarially train a WRN-\(28\)-\(10\) on CIFAR10. Similar to the procedure followed in , we restrict \(_{2}\)-norms of perturbation to \(2.6\) and set the maximum number of iterations for SDF to \(6\).

   &  &  \\   & Mean & Median & Mean & Median \\  DDN & \(1.09\) & \(1.02\) & \(0.86\) & \(0.73\) \\ FAB & \(1.12\) & \(1.03\) & \(0.92\) & \(0.75\) \\ FMN & \(1.48\) & \(1.43\) & \(1.47\) & \(1.43\) \\ ALMA & \(1.17\) & \(1.06\) & \(0.84\) & \(\) \\  SDF & \(\) & \(\) & \(\) & \(0.73\) \\  

Table 6: The comparison between \(_{2}\) robustness of our adversarial trained model and  model.

   & FR & Median-\(_{2}\) & Grads \\  DF & \(100\) & \(0.26\) & \(\) \\ ALMA & \(100\) & \(0.10\) & \(100\) \\ DDN & \(100\) & \(0.13\) & \(100\) \\ FAB & \(100\) & \(0.11\) & \(100\) \\ FMN & \(97.3\) & \(0.11\) & \(100\) \\ C\&W & \(100\) & \(0.12\) & \(90\,000\) \\  SDF & \(100\) & \(\) & \(25\) \\  

Table 4: Performance of attacks on the CIFAR-10 dataset with naturally trained WRN-\(28\)-\(10\).

   &  &  \\   & FR & Median-\(_{1}\) & Grads & FR & Median-\(_{2}\) & Grads \\  DF & \(99.1\) & \(0.21\) & \(\) & \(98.8\) & \(1.36\) & \(\) \\ ALMA & \(\) & \(0.10\) & \(100\) & \(100\) & \(0.85\) & \(100\) \\ DDN & \(99.9\) & \(0.17\) & \(1,000\) & \(99.7\) & \(1.10\) & \(1,000\) \\ FAB & \(99.3\) & \(0.10\) & \(900\) & \(100\) & \(0.81\) & \(900\) \\ FMN & \(99.3\) & \(0.10\) & \(1,000\) & \(99.9\) & \(0.82\) & \(1,000\) \\ CAw & \(\) & \(0.21\) & \(82,667\) & \(99.9\) & \(1.17\) & \(52,000\) \\ SDF & \(\) & \(\) & \(37\) & \(\) & \(\) & \(39\) \\  

Table 5: Performance comparison of SDF with other SOTA attacks on ImageNet dataset with natural trained RN-50 and adversarially trained RN-50.

We train the model on clean examples for the first \(200\) epochs, and we then fine-tune it with SDF generated adversarial examples for \(60\) more epochs. Since a model trained using DDN-generated samples  has demonstrated greater robustness compared to a model trained using PGD , we compare our model with that one (for more details about AT please refer to Appendix O). Our model reaches a test accuracy of \(90.8\%\) while the model by  obtains \(89.0\%\). SDF adversarially trained model does not overfit to SDF attack because, as Table 6 shows, SDF obtains the smallest perturbation. It is evident that SDF adversarially trained model can significantly improve the robustness of model against minimum-norm attacks up to \(30\%\). In terms of comparison of these two adversarially trained models with AA, our model outperformed the  by improving about \(8.4\%\) against \(_{}\)-AA, for \(=8/255\), and \(0.6\%\) against \(_{2}\)-AA, for \(=0.5\).

Furthermore, compared to a network trained on DDN samples, our adversarially trained model has a smaller input curvature (Table 7). The second column shows the average spectral-norm of the Hessian w.r.t. input, \(\|^{2}f()\|_{2}\), and the third column shows the average of the same quantity normalized by the norm of the input gradient, \(_{f}()=\|^{2}f()\|_{2}/\| f( )\|_{2}\). The standard deviation is denoted by numbers enclosed in brackets.

This observation corroborates the idea that a more robust network will exhibit a smaller input curvature .

**AutoAttack++**

Although it is not the primary focus of this paper, in this section we notably enhance the time efficiency of the AA  by incorporating SDF method into the set of attacks in AA.

We introduce a new variant of AA by introducing AutoAttack++ (AA++). AA is a reliable and powerful ensemble attack that contains three types of white-box and a strong black-box attacks. AA evaluates the robustness of a trained model to adversarial perturbations whose \(_{2}\)/\(_{}\)-norm is bounded by \(\). By substituting SDF with the attacks in the AA, we significantly increase the performance of AA in terms of _computational time_. Since SDF is an \(_{2}\)-norm attack, we use the \(_{2}\)-norm version of AA as well. We restrict maximum iterations of SDF to \(10\). If the norm of perturbations exceeds \(\), we renormalize the perturbation to ensure its norm stays \(\). In this context, we have modified the AA algorithm by replacing APGD\({}^{}\) with SDF due to the former's cost and computation bottleneck in the context of AA (See Appendix F.1 for more details). Our decision to replace APGD\({}^{}\) with SDF was primarily motivated by the former being a computational bottleneck in AA. As it is shown in Table 8, AA and AA++ achieve similar fooling rates, with AA++ being notably faster. We compared the sets of points that were fooled or not fooled by SDF/APGD\({}^{}\) across 1000 samples (\(=0.5\)). The results indicate that both algorithms fool approximately the same set of points, differing only in a handful of samples for this epsilon value. Therefore, the primary benefit of using SDF is the reduction in computation time. We compare the fooling rate and computational time of AA++ and AA on the models from the RobustBench. In Table 8, we observe that AA++ is up to _three_ times faster than AA. In an alternative scenario, we added the SDF to the beginning of the AA set, resulting in a version that is up to two times faster than the original AA, despite now containing five attacks (See Appendix F). This outcome highlights the efficacy of SDF in finding adversarial examples. These experiments suggest that leveraging efficient _minimum-norm_ and _non-fixed iteration_ attacks, such as SDF, can enable faster and more reliable evaluation of the robustness of deep models.

## 5 Conclusion and Future Works

In this work, we have introduced a family of parameter-free, fast, and parallelizable algorithms for crafting optimal adversarial perturbations. Our proposed algorithm, SDF, _consistently_ finds smaller

   Model & \(_{}\|^{2}f()\|_{2}\) & \(_{}_{f}()\) \\  Standard & \(600.06\) (29.76) & \(73.99\) (6.62) \\ DDN AT & \(2.86\) (1.22) & \(4.32\) (2.91) \\ SDF AT (Ours) & \(\) (0.08) & \(\) (0.86) \\   

Table 7: Average input curvature of AT models. According to the measures proposed in .

    &  &  \\   & Clean acc. & Robust acc. & Grads & Robust acc. & Grads \\  R1  & \(95.7\%\) & \(82.3\%\) & \(1259.2\) & \(\) & \(\) \\ R2  & \(90.3\%\) & \(76.1\%\) & \(1469.1\) & \(76.1\%\) & \(\) \\ R3  & \(89.4\%\) & \(63.4\%\) & \(1240.4\) & \(\) & \(\) \\ R4  & \(88.6\%\) & \(\) & \(933.7\) & \(68.4\%\) & \(\) \\ RS  & \(88.0\%\) & \(68.4\%\) & \(846.3\) & \(\) & \(\) \\ R6  & \(88.02\%\) & \(67.6\%\) & \(721.4\) & \(\) & \(\) \\ Natural & \(94.7\%\) & \(0.00\%\) & \(208.6\) & \(0.00\) & \(\) \\   

Table 8: Analysis of robust accuracy for various defense strategies against AA++ and AA with \(=0.5\) for six adversarially trained models on CIFAR10. All models are taken from the RobustBench library .

norm perturbations on various networks and datasets with only a small additional computation cost compared to DF (which is still significantly faster than all SOTA attacks). Furthermore, we have shown that adversarial training using the examples generated by SDF builds more robust models. While our primary focus in this work has been on minimal \(_{2}\) attacks, there exists potential for extending SDF families to other threat models, including general \(_{p}\)-norms and targeted attacks. In the Appendix, we have demonstrated straightforward modifications that highlight the applicability of SDF to both targeted and \(_{}\)-norm attacks. However, a more comprehensive evaluation remains a direction for future work. Moreover, further limitations of our proposed method are elaborated upon in Appendix N. In the end, by revisiting the necessity of \(_{p}\)-norm robustness and characterizing a toy example on robustness-free phenomena, we underscore the pivotal role of minimum-norm attacks in ensuring secure AI systems.

## 6 Acknowledgments

We want to thank Kosar Behnia and Mohammad Azizmalayeri for their helpful feedback. We are very grateful to Fabio Brau and Jerome Rony for providing code and models and answering questions on their papers.