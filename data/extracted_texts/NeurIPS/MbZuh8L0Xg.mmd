# DiffPhyCon: A Generative Approach to Control Complex Physical Systems

Long Wei\({}^{1}\)1  Peiyan Hu\({}^{2}\)1  Ruiqi Feng\({}^{1}\)1  Haodong Feng\({}^{1}\)  Yixuan Du\({}^{3}\)2  Tao Zhang\({}^{1}\)

**Rui Wang\({}^{4}\)2  Yue Wang\({}^{5}\)  Zhi-Ming Ma\({}^{2}\)  Tailin Wu\({}^{1}\)2 \({}^{1}\)School of Engineering, Westlake University, \({}^{2}\)Academy of Mathematics and Systems Science, Chinese Academy of Sciences, \({}^{3}\)Jilin University, \({}^{4}\)Fudan University, \({}^{5}\)Microsoft AI4Science {weilong,hupeiyan,fengruiqi,wutailin}@westlake.edu.cn**

Equal contribution. 2Work done as an intern at Westlake University. 3Corresponding author.

###### Abstract

Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon.

## 1 Introduction

Modeling the dynamics of complex physical systems is an important class of problems in science and engineering. Usually, we are not only interested in predicting a physical system's behavior but also injecting time-variant signals to steer its evolution and optimize specific objectives. This gives rise to the complex physical control problem, a fundamental task with wide applications, such as controlled nuclear fusion , fluid control , underwater devices  and aviation , among others.

Despite its importance, controlling complex physical systems efficiently presents significant challenges. It inherits the fundamental challenge of simulating complex physical systems, which are typically high-dimensional and highly nonlinear, as specified in Appendix A.1. Furthermore, observed control signals and corresponding system trajectories for optimizing a control model are typically far from the optimal solutions of the specific control objective. This fact poses a significant challenge of dilemma: _How to explore long-term control sequences beyond its training distribution_to seek near-optimal solutions while making the resulting system trajectory faithful to the dynamics of the physical system_?

To tackle physical systems control problems, various techniques have been proposed, yet they fall short of addressing the above challenges. Regarding traditional control methods, the Proportional-Integral-Derivative (PID) control , is efficient but only suitable for a limited range of problems. Conversely, Model Predictive Control (MPC) , despite a wider range of applicability, suffers from high computational costs and challenges in global optimization. Recent advances in supervised learning (SL) [22; 24] and reinforcement learning (RL) [15; 46; 52], trained on system trajectories and control signals data, have demonstrated impressive performance in solving physical systems control problems. However, existing SL and model-based RL methods either fall into myopic failure modes  that fail to achieve long-term near-optimal solutions, or produce adversarial state trajectories  that violate the physical system's dynamics. The main reason may be that they treat the continuous evolution of dynamics from an iterative view, both lacking long-term vision and struggling in global optimization. See Appendix A.2 for more related work on physical system control.

In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a _new class_ of method to address the physical systems control problem. We take an energy optimization perspective over system trajectory and control sequences across the whole horizon to implicitly capture the constraints inherent in system dynamics. We accomplish this through diffusion models, which are trained using system trajectory data and control sequences. In the inference stage, DiffPhyCon integrates simulation and control optimization into a unified energy optimization process. This prevents the generated system dynamics from falling out of distribution, and offers an enhanced perspective over long-term dynamics, facilitating the discovery of control sequences that optimize the objectives.

An essential aspect of physical systems control lies in its capacity to generate near-optimal controls, even when they may deviate significantly from the training distribution. We address this challenge with the key insight that the learned generative energy landscape can be _decomposed_ into two components: a prior distribution representing the control sequence and a conditional distribution characterizing the system trajectories given the control sequence. Based on this insight, we develop a _prior reweighting_ technique to subtract the effect of the prior distribution of control sequences, with adjustable strength, from the overall joint generative energy landscape during inference.

In summary, we contribute the following: **(1)** We develop DiffPhyCon, a novel generative method to control complex physical systems. By optimizing trajectory and control sequences jointly in the entire horizon by diffusion models, DiffPhyCon facilitates global optimization of long-term dynamics and helps to reduce myopic failure modes. **(2)** We introduce the prior reweighting technique to plan control sequences that are superior to those in the training set. **(3)** We demonstrate the effectiveness of our method on 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control tasks. On all three tasks, our method outperforms widely applied classical control methods and is also competitive with recent supervised learning and strong reinforcement learning baselines, particularly demonstrating advantages of DiffPhyCon in scenarios with partial observations and partial/indirect control. Notably, DiffPhyCon reveals the intriguing fast-close-slow-open pattern exhibited by the jellyfish, aligning with findings in the field of fluid dynamics . **(4)** We generate a dataset of 2D jellyfish based on a CFD (computational fluid dynamic) software to mimic the movement of a jellyfish under control signals of its flapping behavior. To advance research in controlling complex physical systems, we release our dataset as a benchmark.

## 2 Background

### Problem Setup

We consider the following complex physical system control problem:

\[^{*}=*{argmin}_{}(, )(,)=0.\] (1)

Here \((t,):[0,]^{d_{ }}\) is the system trajectory \(\{(t,),t[0,]\}\) defined on time range \([0,]\) and spatial domain \(^{D}\), and \((t,):[0,]^{d_{ }}\) is the external control signal for the physical system with dimension \(d_{}\). \((,)\) denotes the control objective. For example, \(\) can be designed to measure the control performance towards a target state \(^{*}\) with cost constraints: \(\|-^{*}\|^{2} t+\|\|^{2}t\). \((,)=0\) denotes the physical constraints. \((,)=0\) can be specified either _explicitly_ by a PDE dynamic which describes how the control signal \((t,)\) drives the trajectory \((t,)\) to evolve under boundary and initial conditions, or _implicitly_ by control sequences and trajectory data collected from observation of the physical system. In the latter case, we may even have access to only partial trajectory data or engage in partial control. These situations collectively pose significant challenges to the physical system control task.

### Preliminary: Diffusion Models

Diffusion models  are a class of generative models that learn data distribution from data. Diffusion models consist of two opposite processes: the forward process \(q(_{k+1}|_{k})=(_{k+1};}_{k},(1-_{k}))\) to corrupt a clean data \(_{0}\) to a Gaussian noise \(_{K}(,)\), and the reverse parametrized process \(p_{}(_{k-1}|_{k})=(_{k-1}; _{}(_{k},k),_{k})\) to denoise from standard Gaussian \(_{K}(,)\), where \(\{_{k}\}_{k=1}^{K}\) is the variance schedule. To train diffusion models,  propose the DDPM method to minimize the following training loss for the denoising network \(_{}\), a simplification of the evidence lower bound (ELBO) for the log-likelihood of the data:

\[=_{k U(1,K),_{0} p(x), (,)}[\|-_{ }(_{k}}_{0}+_{k}},k)\|_{2}^{2}],\] (2)

where \(_{k}:=_{i=1}^{k}_{i}\). \(_{}\) estimates the noise to be removed to recover data \(_{0}\). During inference, iterative application of \(_{}\) from a Gaussian noise could generate a new sample \(_{0}\) that approximately follows the data distribution \(p()\). See Appendix A.3 for related work on diffusion models.

**Notation**. We use \(_{[n,m]}=[_{n},,_{m}]\) to denote a sequence of variables. We use \(_{[0,T-1],k}\) to denote the hidden variable of \(_{[0,T-1]}\) in a diffusion step \(k\). For simplicity, we abbreviate \(_{[0,T-1]}\), \(_{[1,T]}\) as \(\), \(\). Concatenation of two variables is denoted via _e.g._, \([,]\).

## 3 Method

In this section, we detail our method DiffPhyCon. In Section 3.1, we introduce our method including its training and inference. In Section 3.2, we further propose a prior reweighting technique to improve DiffPhyCon. The overview of DiffPhyCon is illustrated in Figure 1.

### Generative Control by Diffusion Models

DiffPhyCon takes an energy optimization perspective to solve the problem Eq. (1), where PDE constraints can be modeled as a parameterized energy-based model (EBM) \(E_{}(,,)\) which characterizes the distribution \(p(,|)\) of \(\) and \(\) conditioned on conditions \(\) by the correspondence \(p(,|)(-E_{}(,,))\). Lower \(E_{}(,,)\), or equivalently higher \(p(,|)\), means better satisfaction of the PDE constraints. Then the problem Eq. (1) can be converted to:

Figure 1: **Overview of DiffPhyCon**. The figure depicts the training (top), inference (bottom left), and evaluation (bottom right) of DiffPhyCon. Orange and blue colors respectively represent models learning the joint distribution \(p_{}(,)\) and the prior distribution \(p_{}()\). Through prior reweighting and guidance, DiffPhyCon is capable of generating superior control sequences.

\[^{*},^{*}=*{argmin}_{,}[E_ {}(,,)+(, )],\] (3)

where \(\) is a hyperparameter. This formulation optimizes \(\) and \(\) of all physical time steps simultaneously. The first term encourages the generated \(\) and \(\) to satisfy the PDE constraints \((,)=0\). The second term guides optimization towards optimal objectives. The advantage of this optimization framework is that it tasks a global optimization on \(\) and \(\) of all times steps, which may obtain better solutions that are faithful to the dynamics of the physical system. Details about the effect of the hyperparameter \(\) are provided in Appendix L.

**Training.** To train \(E_{}\), we exploit the diffusion model to estimate the gradient of \(E_{}\). For convenience, we introduce a new variable \(\) to represent the concatenation of \(\) and \(\) as \(=[,]\). We use a denoising network \(_{}\), which approximates \( E_{}(,)\), to learn the noise that should be denoised in each diffusion step \(k=1,,K\). The training loss of \(_{}\) is:

\[=_{k U(1,K),(,) p(,),(,)}[ \|-_{}(_{k }}+_{k}},,k)\|_ {2}^{2}],\] (4)

where \(_{}\) is conditioned on \(\). Regarding training datasets, they could be generated by designing the conditions \(\) and control sequences followed by a simulation when an explicit PDE form is available. Otherwise, they should be collected from observed pairs of control sequences and trajectories.

**Control optimization.** After the denoising network is trained, the Eq. (3) can be optimized by the Langevin sampling procedure as follows. We start from an initial sample \(_{K}(,)\), and iteratively run the following process

\[_{k-1}=_{k}-(_{}(E_{}( _{k},)+(}_{k}))+, ,_{k}^{2},\] (5)

where \(_{k}^{2}\) and \(\) correspond to noise schedules and scaling factors used in the diffusion process, respectively. Here \(}_{k}\) is the approximate noise-free \(_{0}\) estimated from \(_{k}\) by:

\[}_{k}=(_{k}-_{k}}_{}(_{k},,k))/_{k}}.\] (6)

We calculate \(\) in (5) based on \(}_{k}\) instead of directly using \(_{k}\) because otherwise noise in \(_{k}\) could bring errors to \(\). Then \( E_{}\) can be replaced by our trained denoising network \(_{}\) as follows:

\[_{k-1}=_{k}-(_{}( _{k},,k)+_{}(}_{k}))+,,_ {k}^{2}\] (7)

Iteration of this denoising process for \(k=K,K-1,...,1\) yields a final solution \(_{0}=\{_{[1,T],0},_{[0,T-1],0}\}\) for the optimization problem Eq. (3).

**Guidance conditioning.** In addition to the above introduced explicit guidance, conditioning is also widely used to guide sampling in diffusion models . When the control objective can be naturally expressed in a conditioning form, e.g., the generated trajectory \(\) is required to coincide with a desired target \(^{*}\), we can include \(=^{*}\) as a condition in \(\) such that the sampled trajectory \(\) from diffusion models automatically satisfying \(=^{*}\).

Overall, in our proposed DiffPhyCon framework, the control objective \(\) can be optimized either using the explicit guidance \(\) or guidance conditioning depending on the specific control objectives.

### Prior Reweighting

**Motivation.** In physical systems control, a critical challenge lies in obtaining control sequences superior to those in training datasets, which often deviate significantly from achieving the optimal control objective. Although guidance of the control objective is incorporated in our diffusion model, generated control sequences are still highly influenced by the prior distribution of control sequences in training datasets. This inspires us to explore strategies to mitigate the effect of this prior, aiming to generate near-optimal control sequences.

We address this challenge with the key insight that the energy-based model \(E(,,)\) can be decomposed into two components: one is \(E^{(p)}(,)\) derived from the prior distribution \(p(|)\) of control sequences, and the other \(E^{(c)}(,,)\) representing the conditional probability distribution \(p(|,)\) of trajectories with respect to given control sequences. This decomposition has the following form

\[E(,,)=E^{(p)}(,)+E^{(c)}( ,,),\] (8)

by the corresponding decomposition of distribution \(p(,|)=p(|)p(| ,)\). We propose a _prior reweighting_ technique, which introduces an adjustable hyperparameter \(>0\) as an exponential of \(p(|)\), allowing for the tuning of the influence of this prior distribution. Then we have a reweighted version of \(p(,|)\) as \(p_{}(,|)=p(|)^{}p( |,)/Z\), which is also a probability distribution and can be further transformed to

\[p_{}(,|)=p(|)^{-1 }p(,|)/Z,\] (9)

where \(Z\) is a normalization constant. In particular, when \(0<<1\), this approach is advantageous as it flattens the distribution \(p(,|)\), thereby increasing the likelihood of sampling from low probability region of \(p(,|)\), where the optimal solutions of the problem Eq. (3) probably lie.

Integrating Eq. (9) into Eq. (8), we have

\[E^{()}(,,)=(-1)E^{(p)}(, )+E_{}(,,)- Z,\] (10)

where \(E^{()}(,,)=-(p_{}(,|))+const\) is the reweighted energy-based model associated with \(E_{}(,,)\) in Eq. (3), relying on the hyperparameter \(\). Then the optimization problem Eq. (3) can be transformed to

\[^{*},^{*}=*{argmin}_{,} [E^{()}(,,)+ (,)].\] (11)

Optimization of this problem encourages sampling from the low likelihood region of \(p(|)\) while minimizing the control objective, which possesses the capability to generate control sequences that are more likely to be near-optimal than its degenerate version \(=1\) in the original optimization problem Eq. (3). The intuition of prior reweighting is illustrated in Figure 2.

**Training**. To learn the reweighted energy \(E^{()}(,,)\), we parameterize its gradient as a summation of two parts by taking the gradient of both sides of Eq. (10):

\[ E^{()}_{,}(,,) =(-1) E^{(p)}_{}(,)\] \[+ E_{}(,,),\]

where \(E^{(p)}_{}(,)\) parameterizes the energy based model \(E^{(p)}(,)\) corresponding to \(p(|)\). Note that \( Z\) vanishes here because it is a constant. Notice that \( E_{}(,,)\) has already been trained by Eq. (4). \( E^{(p)}_{}(,)\) can be trained similarly by the following loss function

\[=_{k U(1,K),(,) p(,),(,)}[\|-_{}(_{t}}+_{k}},,k)\|_{2}^{2}],\] (12)

where \(_{}\) is the conditional denoising network that approximates \( E^{(p)}_{}(,)\).

**Control optimization**. With both \(_{}\) and \(_{}\) trained, Eq. (11) can be optimized by running:

\[_{k-1} =_{k}-(_{}(_{k}, ,k)+_{}(}_{k}))+ _{1},_{1}0,_{k}^{2}\] (13) \[_{k-1} =_{k-1}-(-1)_{}( _{k},,k)+_{2},_{2}0,_{k}^{2} ,\] (14)

iteratively, where \(_{k}=[_{k},_{k}]\). The difference between this iteration scheme and Eq. (7) is that it uses an additional step to update \(_{k}\) based on the predicted noise of \(_{}\). This guides \(_{k}=[_{k},_{k}]\) to move towards the reweighted distribution \(p_{}(,|)\) while aligning with the direction to decrease the objective by its guidance in the iteration of \(_{k}\). The complete algorithm is present in Algorithm 1. Detailed discussion and results about how to set the hyperparameter \(\) are provided in Appendix L.

**Theoretical Analysis.** Consider a pair \([,]\) sampled using the prior reweighting technique with hyperparameter \(\): \([,] p_{}(,)=p(| )p^{}()/C_{}\). Denote \(^{*}\) as the global minimum

Figure 2: **Intuition of Prior Reweighting**. The top surface illustrates the landscape of \((,)\), where the high-dimensional variables \(\) and \(\) are represented using one dimension. The middle and lower planes depict probability heatmaps for the reweighted distribution \(p^{}()p(|)/Z\). Adjusting \(\) from \(=1\) (middle plane) to \(0<<1\) (lower plane), a better minimal of \(\) (red dot in the lower plane) gains the chance to be sampled. This contrasts with the suboptimal red point in the middle plane highly influenced by the prior \(p()\).

of the control objective \(\). Define \(Q()\) to be the "\(\)-optimal" solution set of \([,]\) such that \((,)-^{*}()\), whose complement set is \(Q()^{c}\), and denote \(_{Q()}(,)\) as its indicator function, i.e. \(_{Q()}(,)=1\) if \([,] Q()\); otherwise \(0\). Define \(Y\) to be the random variable of "whether use \(\) as a guidance for sampling", namely,

\[p(Y|,)=e^{-(, )}/Z,&Y=1\\ \\ 1-e^{-(,)}/Z,&Y=0.\] (15)

Consider \(E()=_{(,) p,(,)} [_{Q()}(,)|Y=1]\), which indicates the expectation of getting an \(\)-optimal solution by using the prior reweighting technique with \(\) under the guidance of \(\). Define

\[F()=_{,}[_{Q( )}(,)p(Y=1|,)(p())]}{ _{,}[_{Q()}(, )p(Y=1|,)]}-_{, }[_{Q()^{c}}(,)p(Y=1| ,)(p())]}{_{,} [_{Q()^{c}}(,)p(Y=1|, )]},\]

then we have the following theorem (please refer to Appendix B for its proof):

**Theorem 3.1**.: _Assume \(E()\) is a smooth function, then the following hold:_

* _If_ \(F(1)<0\)_, there exists a_ \(_{-}<1\)_, s.t.,_ \(E(_{-})>E(1)\)_;_
* _If_ \(F(1)>0\)_, there exists a_ \(_{+}>1\)_, s.t.,_ \(E(_{+})>E(1)\)_._

**Remark**: Here \(F()\) can be interpreted as some kind of difference between "entropies" in \(Q()^{c}\) and \(Q()\). When \(F(1)<0\), it means that \(Q()^{c}\) has higher "entropies", implying that the training trajectories are far from optimal. As a result, we may need to flatten the distribution of training trajectories, which corresponds to using the prior reweighting technique with \(<1\). Since this is the most common case in real scenarios, we usually set \(<1\).

**DiffPhyCon-lite.** The introduction of the prior reweighting technique in DiffPhyCon involves training and evaluation of two models, thus bringing in additional computational cost. It is gratifying to note that we can balance the control performance and computational overhead of DiffPhyCon by adjusting the parameter \(\). When \(=1\), the model \(_{}\) is not needed, and we denote this simplified version of DiffPhyCon as DiffPhyCon-lite.

## 4 Experiments

In this section, we aim to answer the following questions: (1) Can DiffPhyCon present superiority over traditional, supervised learning, and reinforcement learning methods for physical systems control? (2) Does the proposed prior reweighting technique help achieve better control objectives? (3) Could answers to (1) and (2) be generalized to more challenging partial observation or partial control scenarios? To answer these questions, we conduct experiments on three vital and challenging problems: 1D Burgers' equation, 2D jellyfish movement control, and 2D smoke control problems.

The following state-of-the-art control methods are selected as baselines. For the 1D Burgers' equation, we use (1) the classical and widely used control algorithm Proportional-Integral-Derivative (PID)  interacting with our trained surrogate model of the solver; (2) Supervised Learning method (SL) ; RL methods including (3) Soft Actor-Critic (SAC)  with offline and surrogate-solver versions; (4) Behaviour Cloning (BC) ; and (5) Behavior Proximal Policy Optimization (BPPO). Specifically, the surrogate-solver version of SAC interacts with our trained surrogate model of the solver, while the offline version only uses given data. BC and BPPO are also in offline versions. For 2D jellyfish movement control, baselines include SL, SAC (offline), SAC (surrogate-solver), BC, BPPO, and an additional classical multi-input multi-output algorithm Model Predictive Control (MPC) . PID is inapplicable to this data-driven task . Detailed descriptions of baselines are provided in Appendix H and Appendix I.

### 1D Burgers' Equation Control

**Experiment settings.** The Burgers' equation is a governing law occurring in various physical systems. We consider the 1D Burgers' equation with the Dirichlet boundary condition and external force \(w(t,x)\), which is also studied in [24; 42].

\[=-u+u}{ x^{2}}+w(t,x)&[0,T]\\ u(t,x)=0&[0,T]\\ u(0,x)=u_{0}(x)&\{t=0\}.\] (16)

Here \(\) is the viscosity parameter, and \(u_{0}()\) is the initial condition. Subject to Eq. (16), given a target state \(u_{d}(x)\), the objective of control is to minimize the control error \(_{}\) between \(u_{T}\) and \(u_{d}\), while constraining the energy cost \(_{}\) of the control sequence \(w(t,x)\):

\[_{}_{}|u(T,x)-u_{d}(x)|^{2}x,\ \ _{}_{[0,T]}|w(t,x)|^{2} tx.\] (17)

To make the evaluation challenging, we select three different experiment settings that correspond to different real-life scenarios: partial observation, full control (PO-FC), full observation, partial control (FO-PC), and partial observation, partial control (PO-PC), which are elaborated on in Appendix D.2 and illustrated in Appendix C.1. These settings are challenging for classical control methods such as PID since they require capturing the long-range dependencies in the system dynamics. Note that the reported metrics in different settings are not directly comparable. In this experiment, DiffPhyCon uses the guidance conditioning (Section 3.1) to optimize \(_{}\) and explicit guidance to optimize the energy cost, which is elaborated in Appendix D.3.

**Results.** In Table 1, we report results of the control error \(_{}\) of different methods. It can be observed that DiffPhyCon delivers the best results compared to all baselines. Specifically, DiffPhyCon decreases \(_{}\) of the best baseline by 30.1%, 52.6%, and 44.6% in the PO-FC, FO-PC, and PO-PC

    & PO-FC & FO-PC & PO-PC \\  PID (surrogate-solver) & - & 0.09115 & 0.09631 \\ SL & 0.09752 & 0.00078 & 0.02328 \\ SAC (surrogate-solver) & 0.01577 & 0.03426 & 0.02149 \\ SAC (offline) & 0.03201 & 0.04333 & 0.03328 \\ BC & 0.02836 & 0.00856 & 0.00952 \\ BPPO & 0.02771 & 0.00852 & 0.00891 \\ 
**DiffPhyCon-lite (ours)** & 0.01139 & **0.00037** & **0.00494** \\
**DiffPhyCon (ours)** & **0.01103** & **0.00037** & **0.00494** \\   

Table 1: **Best \(_{}\) achieved in 1D Burgers’s equation control.** Bold font denotes the best model, and underline denotes the second best model.

Figure 3: **Pareto frontier of \(_{}\) vs. \(_{}\) of different methods for 1D Burgers’ equation.**

settings respectively. From Table 1, DiffPhyCon and DiffPhyCon-lite show little performance gap. This is because, the prior distribution of \(w\) that our DiffPhyCon-lite learned is conditioned on both \(u_{0}\) and \(u_{T}\), which fully determines the optimal \(w\). Therefore, \(p(w|u_{0},u_{T})\) is intrinsically the optimal distribution and thus DiffPhyCon-lite can already deliver satisfactory performance.

To compare the ability of different methods to optimize \(_{}\) with constrained energy cost \(_{}\), we compare the Pareto frontiers of different methods in Figure 3. We vary the hyperparameter \(\) to control the tradeoff between \(_{}\) and the energy cost since most baselines have this hyperparameter. As can be observed in Figure 3, the Pareto frontiers of DiffPhyCon are consistently among the best, achieving the _lowest_\(_{}\) for most settings of the energy budget. Although SL performs well in full observation setting (b) where the system dynamics can be more easily predicted, it encounters difficulty in partial observation scenarios (a)(c). The results demonstrate DiffPhyCon's ability to generate near-optimal control sequences compared to baselines. More visualization results are provided in Appendix C.1. More results of evaluation are presented in Appendix K.1. For efficiency evaluation of training and test phases, please refer to Table 23 in Appendix K.4.

### 2D Jellyfish Movement Control

**Experiment settings.** This task is to control the movement of a flapping jellyfish with two wings in a 2D fluid field where fluid flows at a constant speed. The jellyfish is propelled by the fluid when its wings flap. Its moving speed and efficiency are determined by the mode of flapping. This task is an important source of inspiration for the design of underwater and aerial devices, and its challenges come from complex vortice behavior and fluid-solid coupling dynamics [54; 28]. For this task, fluid dynamics follows the 2D incompressible Navier-Stokes Equation:

\[}{ t}+ -^{2}+ p=0\\ =0\\ (0,)=_{0}(),\] (18)

where \(\) represents the 2D velocity of the fluid, and \(p\) represents the pressure, constituting the PDE state \(=(,p)\). The initial velocity condition is \(_{0}()\) and the kinematic viscosity is \(\). We assume that each wing is rigid, so the jellyfish's boundary can be parameterized by the opening angle \(_{t}\) of wings. Long-term movement of jellyfish usually presents a periodic flapping mode. Consequently, the control objective is to maximize its average moving speed \(\) determined by the pressure of the fluid, under the energy cost constraint \(R()\) and the periodic constraint \(d(_{T},_{0})\) of the movement:

\[=-+ R()+d(_{T},_{0}),\] (19)

subject to Eq. (18) and the boundary condition that the velocity of fluid vanishes near the boundary. The hyperparameter \(\) is set to be 1000. We evaluate in two settings: full observation, where the full state \(=(,p)\) is observed; and partial observation, where only pressure is observed. This task is very challenging due to the complicated dynamics of fluid-solid interactions and vortices behaviour [54; 28], especially in the scenario of partial observation where the missing of \(\) in Eq. (18) restricts the information available to generate well-informed control signals. Details of the experiment are provided in Appendix F.

**Open-source Dataset Description.** We use the Lily-Pad simulator  to generate a dataset describing the movement of jellyfish under the control of its flapping behavior. Statistics about the dataset are listed in Table 2. The feature of this dataset is that it contains a rich variation of vortices behavior and fluid-solid interaction dynamics, determined by violent changes in opening angles of wings and open-close phase ratio. It would serve as an important benchmark for studying complex physical system control problems. Details about the dataset are provided in Appendix E.

**Results.** Evaluation results are presented in Table 3. It can be seen that our method outperforms the baselines by a large margin in optimizing the control objective \(\). At a cost of slightly increasing the control cost \(R()\), control sequences generated by our method achieve a much faster average speed

   training trajectories & test trajectories & resolution & \#fluid features & trajectory length \\ 
30000 & 1000 & 128\(\)128 & 3 & 40 \\   

Table 2: **Jellyfish movement dataset outline.**than baselines. In the full observation setting, the control objective achieved by DiffPhyCon is -53.18 lower than the best baseline BPPO, while the average speed exhibits an increment of 172.2 over it. This demonstrates that diffusion models are effective for this challenging control task by performing global optimization of trajectory and control sequences in a generative approach. Comparison between DiffPhyCon-lite and DiffPhyCon reveals that by flattening the prior distribution of control sequences, another significant improvement is further achieved. Even in the more challenging partial observation setting, DiffPhyCon still exhibits substantial advantages over existing methods. This reflects our method has a strong control capability under inadequate information. Configuration of the hyperparameter \(\) in DiffPhyCon and performance with respect to varying \(\) is presented in Figure 17 in Appendix L.1. Details about the hyperparameter \(\) can be found in Table 33 in Appendix L.2.

Figure 4 visualizes generated opening angle curves of different methods on three test jellyfish. Opening angle curves of DiffPhyCon-lite show an obvious fast-close-slow-open shape, which is proven to produce high speed in jellyfish movement . The reason is that the fast closing of wings leads to a thrust from fluid in the early state, resulting in a long-term high speed, and followed by a slow opening to reduce resistance. While this mode of movement appears rarely in the training dataset, DiffPhyCon-lite could generate such control sequences for most test samples. This reflects that diffusion models under guidance are effective in optimizing the control objective. Furthermore, DiffPhyCon makes this mode of movement more aggressive, with sharper change of the opening angle in the beginning and end stage within a period. This provides strong evidence that reweighting the prior distribution of control sequences allows for flexible sampling over a flattened distribution, thus control sequences with low prior but good objectives are more likely to be sampled. The movement and the resulting fluid field of the jellyfish corresponding to the middle subfigure of Figure 4 controlled by DiffPhyCon is illustrated in Figure 5 and more examples are provided in Figure 10 in Appendix C.2. Conversely, opening angles obtained by baselines are inferior. The reason may be that they predict opening angles sequentially and hard to globally optimize the objective with three conflicting terms: average speed, \(R()\), and \(d(_{T},_{0})\). We further study such myopic failure mode of SAC in Appendix K.3. More comparisons about the variation of the weight \(\) are listed in Table 20 and Table 21 in Appendix K.2. For efficiency evaluation of training and test phases, please refer to Table 24 in Appendix K.4. We also extend this experiment to a high-dimensional control signal setting, where the wings of the jellyfish are assumed to be soft. We find that our method is still competitive with baselines. Details about this evaluation are provided in Appendix M.

    &  &  \\  & \(\) & \(R()\) & \(\) & \(\) & \(R()\) & \(\) \\  MPC & 25.72 & 0.0112 & 109.17 & -150.51 & 0.1791 & 329.59 \\ SL & -76.94 & 0.1286 & 205.57 & -102.98 & 0.1188 & 221.79 \\ SAC (surrogate-solver) & -166.96 & 0.0069 & 18.14 & -153.09 & 0.0057 & 158.82 \\ SAC (offline) & -158.66 & 0.0069 & 165.58 & -206.21 & 0.0058 & 211.96 \\ BC & 30.48 & 0.0629 & 32.44 & 20.08 & 0.0556 & 35.48 \\ BPPO & 107.67 & 0.0867 & -20.93 & 54.83 & 0.0518 & -3.02 \\ 
**DiffPhyCon-lite (ours)** & 95.04 & 0.0746 & -20.47 & 2.92 & 0.0779 & 74.97 \\
**DiffPhyCon (ours)** & **279.87** & 0.2058 & **-74.11** & **150.21** & 0.1269 & **-23.32** \\   

Table 3: **2D jellyfish movement control results.** Bold font denotes the best model, and underline denotes the second best model.

Figure 4: **Comparison of generated control curves of three test jellyfish.** The resulting control objective \(\) for each curve is presented.

### 2D Smoke Indirect Control

**Experiment settings.** This task is to control smoke control in an incompressible fluid environment, following a similar (but more challenging) setup of . Control forces were applied within a \(64 64\) grid flow field, excluding a semi-enclosed region, to minimize the smoke failing to pass through the top middle exit (seven exits in total). For illustration of our settings, please refer to Figure 14 in Appendix G. This high-dimensional indirect control problem involves managing 2D forces at approximately 1,700 grid points every time step, resulting in about 100,000 control variables across 32 time steps, making it highly challenging.

**Results.** Evaluation results are presented in Table 4. Our method still has significant advantages over baselines in minimizing the control objective. Furthermore, the prior reweighting technique achieves extra improvement over DiffPhyCon-lite. One test sample of smoke density and fluid field dynamics is illustrated in Figure 6. More visualization results of our method are presented in Figure 11 in Appendix G. These results demonstrate that DiffPhyCon is capable of controlling high dimensional physical systems even when control signals are indirectly applied to the system.

## 5 Conclusion

In this work, we have introduced DiffPhyCon, a novel methodology for controlling complex physical systems. It generates control sequences and state trajectories by jointly optimizing the generative energy and control objective. We further introduced prior reweighting to enable the discovery of control sequences that diverge significantly from training. Through comprehensive experiments, we demonstrated our method's superior performance compared to classical, deep learning, and reinforcement learning baselines in challenging physical systems control tasks. We discuss limitation and future work in Appendix N and state social impact in O.

## 6 Acknowledgment

We thank Yuchen Yang for insightful discussions on theoretical analysis. We thank the anonymous reviewers for providing valuable feedback on our manuscript. We also gratefully acknowledge the support of Westlake University Research Center for Industries of the Future and Westlake University Center for High-performance Computing. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.

   Method & \(\) \\  BC & 0.3085 \\ BPPO & 0.3066 \\ SAC (surrogate-solver) & 0.3212 \\ SAC(offline) & 0.6503 \\ 
**DiffPhyCon-lite (ours)** & 0.2324 \\
**DiffPhyCon (ours)** & **0.2254** \\   

Table 4: **2D smoke movement control results.** Bold font denotes the best model, and underline denotes the second best model.

Figure 5: **Visualization of jellyfish movement and fluid field controlled by DiffPhyCon as in the middle subfigure of Figure 4.**

Figure 6: **Visualization of smoke density and fluid field dynamics controlled by DiffPhyCon.**