# Embarrassingly Simple Dataset Distillation

Yunzhen Feng \({}^{*}\)\({}^{}\)  Ramakrishna Vedantam \({}^{*}\)  Julia Kempe \({}^{*}\)\({}^{*}\) Center for Data Science, New York University

\({}^{}\) Courant Institue of Mathematical Sciences, New York University

\({}^{}\) yf2231@nyu.edu

###### Abstract

Training of large-scale models in general requires enormous amounts of training data. Dataset distillation aims to extract a small set of synthetic training samples from a large dataset with the goal of achieving competitive performance on test data when trained on this sample, thus reducing both dataset size and training time. In this work, we tackle dataset distillation at its core by treating it directly as a bilevel optimization problem. Re-examining the foundational back-propagation through time method, we study the pronounced variance in the gradients, computational burden, and long-term dependencies. We introduce an improved method: Random Truncated Backpropagation Through Time (RaT-BPTT) to address them. RaT-BPTT incorporates a truncation coupled with a random window, effectively stabilizing the gradients and speeding up the optimization while covering long dependencies. This allows us to establish new dataset distillation state-of-the-art for a variety of standard dataset benchmarks.

## 1 Introduction

_Dataset Distillation_, introduced by , aims to condense a given dataset into a small synthetic version such that when neural networks are trained on this distilled version, they achieve good performance on the original distribution. The distilled datasets thus speed up model-training  by using less data and training steps, and have found numerous applications including protecting privacy , continual learning , federated learning , and neural architecture search .

Dataset distillation is an instance of bilevel optimization  where one optimization output (in this instance, the learning algorithm trained on the small dataset) is fed into another optimization problem (the generalization error on the target set) which we intend to minimize. In general, this problem is intractable, as the inner loop involves a multi-step computation with a large number of steps. Early works  tackle this problem via back-propagation through time (BPTT), the go-to method for bilevel optimization . BPTT unrolls the inner loop for a certain number of steps and calculate the meta-gradient for the distilled dataset. However, long unrolling introduces large computational and memory requirements, limiting performance. Numerous follow-up works turn to replacing the inner loop with closed-form differentiable _surrogates_ or modify the outer loop objective using _proxy training-metrics_ (see the appendix for related work).

In this paper, we refine BPTT and achieve state-of-the-art performance across a vast majority of the CIFAR10, CIFAR100, CUB and TinyImageNet benchmarks. For dataset distillation, the inner problem presents unique challenges - the pronounced non-convex nature when training a neural network from scratch on the distilled data. One has to use long unrolling to encapsulate the long dependencies inherent in the inner optimization. However, this results in BPTT suffering from slow optimization and huge memory demands, a consequence of backpropagating through all intermediate steps. This is further complicated by considerable instability in meta-gradients, emerging from the multiplication of Hessian matrices during long unrolling. Therefore, the performance is limited.

To address these challenges, we integrate the concepts of randomization and truncation with BPTT, leading to the Random Truncated Backpropagation Through Time (RaT-BPTT) method. The refined approach unrolls within a randomly anchored smaller fixed-size window along the training trajectory and aggregates gradients within that window (see Fig. 1 for a cartoon illustration). The random window design ensures that the RaT-BPTT gradient serves as a random subsample of the full BPTT gradient, covering the entire trajectory, while the truncated window design enhances gradient stability and alleviates memory burden. Consequently, RaT-BPTT provides expedited training and superior performance compared to BPTT.

Overall, our method is _embarrassingly_ simple - we show that a careful analysis and modification of backpropagation lead to results exceeding the current state-of-the-art, without resorting to various approximations, a pool of models in the optimization, or additional heuristics. Since our approach does not depend on large-width approximations, it works for any architecture, in particular commonly used narrower models, for which methods that use inner-loop approximations perform less well. Moreover, our method can be seamlessly combined with prior methods on dataset re-parameterization , leading to further improvements. To our knowledge, we are the first to introduce _truncated_ backpropagation through time  to the dataset distillation setting, and to combine it with _random_ positioning of the unrolling window.

## 2 Methods

Denote the original training set as \(\) and the distilled set as \(\). With an initialization \(_{0}\) for the inner-loop learner \(\), we perform the optimization for \(T\) steps to obtain \(_{T}()\) with loss \((_{T}(),)\). We add \(()\) to denote its dependence on \(\). The dataset distillation problem can be formulated as

\[_{}\ (_{T}(),)\ _{T}()=(_{0},,T)\  \]

When the inner-loop learner \(\) is gradient descent with learning rate \(\), one could leverage the chain rule to get the gradient of BPTT with respect to the distilled data:

\[_{BPTT}=-(_{T}(), )}{}_{i=1}^{T-1}_{j=i+1}^{T-1}[1- (_{j}(),)}{ ^{2}}](_{i}(), )}{ U} \]

This computation indicates that the meta-gradient is divided into \(T-1\) segments. Each part represents a matrix product \([1- H]\) where each \(H\) matrix corresponds to a Hessian matrix. Yet, computing the meta-gradient demands the storage of all intermediate states to backpropagate through, and thus is less computationally efficient.

To circumvent these challenges, the prevalent strategy is to adopt the _truncated_ BPTT (T-BPTT) method [56; 39], which unrolls the inner loop for the same \(T\) steps but only propagates backwards through a smaller window of \(M\) steps. Therefore, in the T-BPTT gradient, the sum in Eq. (2) starts at \(T-M\). This technique omits the initial \(T-M-1\) terms, each being a product of more than \(M\) Hessian matrices. Assuming the inner loss function is strongly convex, T-BPTT aligns well with BPTT . The convexity assumption implies positive eigenvalues of the Hessians, causing the term \([1- H]\) to vanish as the number of factors increases, allowing for good performance of T-BPTT with less memory requirement and faster optimization time. However, in our scenario, the task involves training a random neural network on distilled data; it is thus inherently non-convex, with multiple minima.

We visualize the training curve and the norm of meta-gradients through outer-loop optimization steps in Fig. 3 and Fig. 2, respectively. All experiments are on CIFAR10 with IPC (image per class) 10. A comparison between BPTT and T-BPTT reveals that: 1) meta-gradients from T-BPTT show more

Figure 1: **Illustration of bilevel optimization of the outer loss when training for 2 steps. We show BPTT (left), Truncated BPTT (middle) and our proposed RaT-BPTT (right). RaT-BPTT picks a window in the learning trajectory (randomly) and tracks the gradients for the chosen window, as opposed to T-BPTT that uses a fixed window, and BPTT that uses the entire trajectory.**

stability than from BPTT, largely due to the excluded \(T-M+1\) gradient terms. This highlights the non-convexity of the inner problem, marked by Hessian matrices having negative eigenvalues. The impact of these eigenvalues intensifies the variance, resulting in unstable gradients. However, once gradients stabilize, T-BPTT displays swift early progress. 2) BPTT ends up with higher accuracy than T-BPTT, suggesting T-BPTT might overlook vital initial phase details, crucial since neural network optimization often peaks early in the inner loop. The challenge thus is how to merge the good performance of BPTT with the computational speedup of T-BPTT.

To this end, we propose the _Random_ Truncated BPTT (RaT-BPTT) in Alg. 1, which randomly places the truncated window along the inner unrolling chain. The gradient of RaT-BPTT is

\[_{RaT-BPTT}=-(_{N}( ),)}{}_{i=N-M}^{N-1}_{j=i+1}^{N-1}[1- (_{j}(),)}{ ^{2}}](_{i}( ),)}{ U} \]

Looking at the gradients, RaT-BPTT differs by randomly sampling M consecutive parts in \(_{BPTT}\) and leaving out the shared Hessian matrix products. Therefore, RaT-BPTT is a subsample version of BPTT, spanning the entire learning trajectory. Moreover, the maximum number of Hessians in the product is restricted to less than M. It thus inherits the benefits of both the accelerated performance and gradient stabilization from T-BPTT. As illustrated in Fig. 3, RaT-BPTT consistently outperforms other methods throughout the optimization process. We also examine performing full unrolling along trajectories of randomly sampled lengths (R-BPTT) as a sanity check. The gradients are similarly unstable and the performance is worse than full unrolling with BPTT.

## 3 Experimental Results

In this section, we present an evaluation of our method, RaT-BPTT, comparing it to a range of SOTA methods across multiple benchmark datasets.

**Datasets** We run experiments on four standard datasets, CIFAR-10 (10 classes, \(32 32\)), CIFAR-100 (100 classes, \(32 32\), ), Caltech Birds 2011 (200 classes, CUB200, \(32 32\), ) and Tiny-ImageNet (200 classes, \(64 64\),  ). We distill datasets with 1, 10, and 50 images per class for the first two datasets and with 1 and 10 images per class for the last two datasets.

**Baselines** We compare our methods to two lines of SOTA methods 1) _Inner-loop surrogates_: BPTT (the non-factorized version of LinBa in ), Neural Tangent Kernel (KIP) , Random Gaussian Process (RFAD) , and empirical feature kernel (FRePO) , and reparameterized convex implicit gradient (RCIG)

Figure 3: Test Accuracy during distillation with BPTT, T-BPTT, R-BPTT, and our RaT-BPTT.

Figure 2: Meta-gradient norm in the first 500 steps. BPTT: unroll 120 steps). T-BPTT: unroll 120 steps and backpropagate 40 steps. RaT-BPTT: we randomly place the backpropagation window for each epoch (25 steps)

[MISSING_PAGE_FAIL:4]