# Online Estimation via Offline Estimation:

An Information-Theoretic Framework

 Dylan J. Foster

dylanfoster@microsoft.com

Yanjun Han

yanjunhan@nyu.edu

Jian Qian

jianqian@mit.edu

Alexander Rakhlin

rakhlin@mit.edu

###### Abstract

The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design ("offline estimation"), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates ("online estimation"). Motivated by connections between estimation and interactive decision making, we ask: _is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion?_ We investigate this question from an information-theoretic perspective by introducing a new framework, _Oracle-Efficient Online Estimation_ (OEOE), where the learner can only interact with the data stream indirectly through a sequence of _offline estimators_ produced by a black-box algorithm operating on the stream. Our main results settle the statistical and computational complexity of online estimation in this framework.

1. _Statistical complexity._ We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.
2. _Computational complexity._ We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms.

Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making.

## 1 Introduction

Consider a general framework for statistical estimation specified by a tuple \((,,,,)\), which we will show encompasses classification, regression, and conditional density estimation. The learner is given a parameter space \(\) (typically a function class), where each parameter \(f\) is a map from the space of _covariates_\(\) to the space of _values_\(\). For an integer \(T 1\), the learner is given a dataset \((x^{},y^{}),,(x^{},y^{})\), where \(x^{},,x^{}\) are _covariates_ and \(y^{},,y^{}\) are _outcomes_ generated via \(y^{}( f^{}(x^{}))\), where \(f^{}\) is an unknown target parameter that the learner wishes to estimate; here \(\) is a probability kernel that assigns to each value \(z\) a distribution \(( z)\) on the space of outcomes \(\). We adopt the shorthand \((z)=( z)\) throughout.

The classical theory of statistical estimation typically assumes that the _covariates_\(x^{},,x^{}\) are an arbitrary fixed design, and is concerned with estimating the target parameter \(f^{}\) well _in-distribution_. Formally, for a _loss function_\(:_{ 0}\) on the space of values \(\), the goal of the learner is to output an estimator \(\) based on \((x^{},y^{}),,(x^{},y^{})\) such that the in-distribution error

\[_{}^{}(T):=_{t=1}^{T} (x^{}),f^{}(x^{})\] (1)

is small; we refer to this as an _offline estimation_ guarantee. Canonical examples include:* Classification (i.e., distribution-free PAC learning [38; 63]), where \(==\{0,1\}\), \((f^{}(x))=_{f^{}(x)}\),1 and \(_{0/1}(x),f^{}(x)=1\{(x)  f^{}(x)\}\) is the \(0/1\)-loss.
* Regression with a well-specified model [62; 66], where \(==\), \((f^{}(x))=(f^{}(x),^{2})\), and \(_{}(x),f^{}(x)=((x)-f^{}(x))^{2}\) is the square loss.
* Conditional density estimation , where \(\) is an arbitrary alphabet, \(=()\), \((f^{}(x))=f^{}(x)\), and \(_{}^{2}(,)\) is squared Hellinger distance; see Appendix C for details.

In parallel to statistical estimation, the contemporary theory of online learning [17; 49] provides estimation error algorithms that support _adaptively chosen_ sequences of covariates, a meaningful form of _out-of-distribution_ guarantee. Here, the examples \((x^{},y^{})\) arrive one at a time. For each step \(t[T]\), the learner produces an estimator \(^{}:\) based on the data \((x^{},y^{}),,(x^{-1},y^{-1})\) observed so far. The covariate \(x^{}\) is then chosen in an arbitrary fashion, and the outcome is generated via \(y^{}(f^{}(x^{}))\) and revealed to the learner. The quality of the estimators is measured via2

\[_{}^{}(T):=_{t=1}^{T}^{}(x^{}),f^{}(x^{}).\] (2)

We refer to this as an _online estimation_ guarantee; classical examples include online classification in the mistake-bound model , online regression , and online conditional density estimation . Online estimation provides a non-trivial out-of-distribution guarantee, as it requires (on average) that the learner achieves non-trivial estimation performance on covariates \(x^{}\) that can be arbitrarily far from the previous examples \(x^{1},,x^{-1}\). This property has many applications in algorithm design, notably in the context of _interactive decision making_, where it has recently found extensive use for problems including contextual bandits [25; 58; 24], reinforcement learning [27; 28], and imitation learning [56; 55].

In this paper, we investigate the relative power of online and offline estimation through a new information-theoretic perspective. It is well known that any algorithm for online estimation can be used _as-is_ to solve offline estimation through _online-to-batch conversion_, a standard technique in learning theory and statistics [3; 9; 16; 61; 36; 8]. The converse is less apparent, as online estimation requires non-trivial algorithm design techniques that go well beyond classical estimators like least-squares or maximum likelihood . In the case of regression with a finite class \(\), least squares achieves optimal offline estimation error \(_{}^{}(T) O([])\),3 and while it is possible to achieve a similar rate \(_{}^{}(T) O([])\) for online estimation, this requires Vovk's aggregating algorithm or exponential weights ; directly applying least squares or other standard offline estimators leads to vacuous guarantees. This leads us to ask: _Is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion?_

Computationally speaking, this question has practical significance, since online estimation algorithms are typically far less efficient than their offline counterparts (the classical exponential weights algorithm maintains a separate weight for every \(f\), which is exponentially less memory-efficient than empirical risk minimization). In fact, at first glance this seems like a _purely_ computational question: if the learner has access to an offline estimator, nothing is stopping them (information-theoretically) from throwing the estimator away and using the data to run an online estimation algorithm.4 Yet, for aforementioned applications in interactive decision making [25; 58; 24; 27; 28; 56; 55], estimation algorithms--particularly online estimators--play a deeper information-theoretic role, and can be viewed as compressing the data stream into a succinct, operational representation that directly informs downstream decision making. With these applications in mind, the first contribution of this paper is to introduce a new protocol, _Oracle-Efficient Online Estimation_, which provides an _information-theoretic_ abstraction of the role of online versus offline estimation, analogous to the framework of information-based complexity in optimization [43; 60; 48; 2] and statistical query complexity in theoretical computer science [13; 37; 21; 22].

### Our protocol: oracle-efficient online estimation

In the Oracle-Efficient Online Estimation (OEOE) framework, the aim is to perform _online_ estimation in the sense of Eq. (2), with the twist that the learner does not directly observe the outcomes \(y^{},,y^{}\); rather, they interact with the environment _indirectly_ through a sequence of _offline_ estimators produced by a black-box algorithm operating on the historical data. We formalize this black-box algorithm as an _estimation oracle_\(_{}=\{_{}^{} \}_{t=1}^{T}\) (e.g., Foster and Rakhlin ), which is a mapping from histories to estimators that enjoy bounded offline estimation error.

**Definition 1.1** (Offline estimation oracle).: _An offline estimation oracle\(_{}=\{_{}^{} \}_{t=1}^{T}\) for a statistical estimation instance \((,,,,)\) and loss \(\) is a mapping \(_{}^{}:()^{t-1} ()\) such that for any sequence \((x^{},y^{}),,(x^{},y^{})\) with \(y^{}(f^{}(x^{}))\), the sequence of estimators \(^{}=_{}(x^{},,x ^{-1},y^{},,y^{-1})\) satisfies \(_{}^{}(t):=_{s=1}^{t-1} ^{}(x^{}),f^{}(x^{}) _{}\) for all \(t[T]\) almost surely; we allow \(x^{}\) to be selected adaptively based on \(y^{},,y^{-1}\) and \(^{},,^{-1}\). We refer to \(_{} 0\) as the offline estimation parameter._

This definition simply asserts that the estimators \(^{}\) produced by the offline estimation oracle satisfy the guarantee in Eq. (1), even when the covariates are selected adaptively. Examples include standard algorithms like least-squares for regression and maximum likelihood for conditional density estimation, which guarantee \(_{} O(||)\) with high probability whenever \(\) is a finite class; see Appendix C.1 for further background.5 Throughout the paper, we assume for simplicity that \(_{}>0\) is known in advance.

With this definition, we present the Oracle-Efficient Online Estimation protocol in Protocol 1. In the protocol, a learner aims to perform online estimation, but at each step \(t\), the only information available is the covariates \(x^{},,x^{-1}\) and the estimators \(^{},,^{}\) generated by an offline estimation oracle satisfying Definition 1.1; the outcomes \(y^{},,y^{}\) are not directly observed. Based on this information, the learner produces a new estimator \(^{}\) such that the online estimation error \(_{}^{}(T)=_{t=1}^{T}_{^{}}(x^{}),f^{ }(x^{})\) in Eq. (2) is minimized.6 An algorithm is termed _oracle-efficient_ if it attains low online estimation error (2) in the OEOE framework. Note that while the learner cannot directly observe the outcomes \(y^{},,y^{}\), the covariates \(x^{},,x^{}\) are observed; we prove that without this ability, it is impossible to achieve non-trivial estimation performance (Section 3).

The OEOE framework abstracts away the property that oracle-efficient algorithms implicitly interact with the environment through a compressed, potentially lossy channel (the estimation oracle \(_{}\)). We believe this property merits deeper investigation: it is shared by essentially all algorithms from recent research that reduces interactive decision making and reinforcement learning to estimation oracles , yet the relative power of offline oracles and analogously defined online oracles is poorly understood in this context. By providing an information-theoretic abstraction to study oracle-efficiency, the OEOE framework plays a role similar to information-based complexity in optimization  and statistical query complexity in theoretical computer science , both of which provide rich frameworks for designing and evaluating iterative algorithms that interact with the environment in a structured fashion. We expect that this abstraction will find broader use for more complex domains (e.g., decision making and active learning) as a means to guide algorithm design and prove lower bounds against natural classes of algorithms.

Let us first build some intuition. Familiar readers may recognize that the classical _halving algorithm_ for binary classification (e.g., Cesa-Bianchi and Lugosi ) can be viewed as oracle-efficient in our framework. Specifically, for binary classification with \(==\{0,1\}\) and loss function \(_{0/1}(x),f^{}(x)=1( x) f^{}(x)}\), the halving algorithm can use any offline oracle with \(_{}=0\) to achieve \(_{}^{}(T)=O(||)\), which is optimal. However, little is known for noisy oracles with \(_{}>0\), or more general outcome spaces and loss functions (e.g., regression or density estimation). In addition, the halving algorithm--while oracle-efficient--is computationally inefficient, as it requires maintaining an explicit version space. This leads us to restate our central question formally, in two parts:

1. _Can we design oracle-efficient algorithms with near-optimal online estimation error (2), up to polynomial factors (for general instances \((,,,,)\) and \(_{}>0\))?_
2. _Can we do so in a computationally efficient fashion?_

### Contributions

For a general class of losses \(\), referred to as _metric-like_, we settle the statistical and computational complexity of performing online estimation via black-box offline estimation oracles up to mild gaps, answering questions (1) and (2) above.

Statistical complexity.Our first result concerning statistical complexity focuses on finite classes \(\), where the optimal rates for offline and online estimation with standard losses \((,)\) both scale as \((||)\). For this setting, we show (Theorem 3.1) that there exists an oracle-efficient online estimation algorithm that achieves \(_{}^{}(T)=O((_{}+1)\{| |,||\})\) in the OEOE framework, and that this is optimal (Theorem 3.2). This provides an affirmative answer to question (1), and characterizes the statistical complexity of oracle-efficient online estimation with finite classes \(\).

In the general OEOE framework, the learner can use the entire history of offline estimators \(^{1},,^{t}\) and covariates \(x^{1},,x^{t-1}\) to produce the online estimator \(^{t}\) for step \(t\). As a secondary result, we study a restricted class of _memoryless_ oracle-efficient algorithms that choose \(^{t}\) only based on the most recent offline estimator \(^{t}\), and show (Theorem 3.3) that it is impossible for such algorithms to achieve low online estimation error.

Lastly, we give a more general approach to deriving oracle-efficient reductions (Theorem D.1) that is based on _delayed online learning_. Using this result, we give a characterization of learnability with _infinite classes_ for binary classification in the OEOE framework (Theorem D.2), proving that finite Littlestone dimension is necessary and sufficient for oracle-efficient learnability.

Computational complexity.On the computational side, we provide a negative answer to question (2), showing (Theorem 4.1) that under standard conjectures in computational complexity, there do not exist polynomial-time algorithms with non-trivial online estimation error in OEOE framework. In spite of this negative result, we provide a fine-grained perspective for the statistical problem of _conditional density estimation_, a general task that subsumes classification and regression and has immediate applications to reinforcement learning and interactive decision making . Here we show, perhaps surprisingly (Theorem 4.2), that online estimation in the OEOE framework is no harder computationally than online estimation with arbitrary, unrestricted algorithms. This result is salient in light of the applications we discuss below.

Implications for interactive decision making.As the preceding discussion has alluded to, our interest in studying oracle-efficient online estimation is largely motivated by a connection to the problem of _interactive decision making_. Foster et al. , Foster and Rakhlin  propose a general framework for interactive decision making called _Decision Making with Structured Observations_ (DMSO), which subsumes contextual bandits, bandit problems with structured rewards, and reinforcement learning with general function approximation. They show that for any decision making problem in the DMSO framework, there exists an algorithm that, given access to an online estimation algorithm (or, "oracle") for conditional density estimation for an appropriate class \(\), it is possible to achieve _near-optimal regret_. The results above critically make use of _online estimation_ oracles, as they require achieving low estimation error for adaptively chosen sequences of covariates, and it is natural to ask whether similar guarantees can be achieved using only offline estimation oracles. However, positive results are only known for certain special cases , with scant results for reinforcement learning in particular. In this context, our results have the following implication (Corollary E.1): _Information-theoretically, it is possible to achieve near-optimal regret for any interactive decision making problem using an algorithm that accesses the data stream only through offline estimation oracles._

Additional results._Due to space constraints, the following results are deferred to the appendix: (i) detailed examples for our statistical estimation framework (Appendix C); (ii) additional results concerning statistical complexity of the OEOE framework (Appendix D); and (iii) detailed results for our application to interactive decision making (Appendix E)._

## 2 Preliminaries

Unless otherwise stated, our results assume the loss function \(\) has _metric-like_ structure.

**Definition 2.1** (Metric-like loss).: _A loss function \(:\) is metric-like on the set \(\) if it is symmetric and satisfies (i) \((z_{1},z_{2}) 0\) for any \(z_{1},z_{2}\) and \((z,z)=0\) for all \(z\); and (ii) \((z_{1},z_{2}) C_{}((z_{1},z_{3})+ (z_{3},z_{2}))\) for all \(z_{1},z_{2},z_{3}\), for an absolute constant \(C_{} 1\)._

Throughout the paper, we focus on three canonical applications, outlined in the introduction: Classification with the indicator loss \(_{0/1}\) (\(C_{}=1\)), regression with the square loss \(_{}\) (\(C_{}=2\)), and conditional density estimation with squared Hellinger distance \(_{}^{2}\) (\(C_{}=2\)). See Appendix C for detailed examples and discussion. (omitted for space).

Finite versus infinite classes.The majority of our results focus on finite classes \(\). We believe this captures the essential difficulty of the problem, but we expect that most of our sample complexity results (which typically scale with \(||\)) can be extended to infinite classes by combining our techniques with appropriate notions of complexity for the function class (Littlestone dimension for classification, sequential Rademacher complexity, and sequential covering numbers [50; 41; 52]). For the canonical settings of classification, regression, and conditional density estimation, there exist algorithms that achieve \(_{}^{}(T)=O(||)\) and \(_{}^{}(T)=O(||)\) for arbitrary finite classes; see Appendix C for details.

We defer additional notation and related work to Appendices A and B

## 3 Statistical complexity of oracle-efficient online estimation

This section presents our main results concerning the statistical complexity of oracle-efficient online estimation. In Section 3.1, we focus on finite classes \(\) and present an oracle-efficient algorithm that achieves near-optimal online estimation error (Theorem 3.1). We then provide a lower bound that shows that our reduction is near optimal (Theorem 3.2). In Section 3.2, we turn our attention to memoryless oracle-efficient algorithms, proving strong impossibility results (Theorem 3.3).

### Minimax sample complexity for oracle-efficient algorithms

In this section, we present our main statistical conclusion for the OEOE framework: _For any finite class \(\), it is possible to transform any black-box offline estimation algorithm into an online estimation algorithm with near-optimal error_ (up to a logarithmic factor that we show is unavoidable).

Algorithm and minimax upper bound.Our results are achieved through a new algorithm, _Version Space Averaging_, described in Algorithm 1. At each round \(t\), the algorithm uses estimators \(^{},,^{}\) produced by an offline estimation oracle \(_{}\), along with the previous covariates \(x^{1},,x^{t-1}\), to construct a _version space_\(_{t}\) in Eq. (3). Informally, \(_{t}\) consists of all \(f\) that are _consistent_ with the estimators \(^{1},,^{}\) in the sense that for all \(s[t]\), the offline estimation error relative to \(^{}\) is small; as long as the offline estimation oracle \(_{}\) has offline estimation error \(_{}\) (Definition 1.1), it follows immediately that the construction in Eq. (3) satisfies \(f^{}_{t}\). Given the version space \(_{t}\), Algorithm 1 predicts by uniformly sampling: \(^{}^{}:=(_{t})\), then proceeds to the next round.7 The main guarantee for Algorithm 1 is stated in Theorem 3.1.

**Theorem 3.1** (Main upper bound for OEOE).: _For any instance \((,,,,)\), any metric-like loss \(\), and any offline estimator \(_{}\) with parameter \(_{} 0\), Algorithm 1 is oracle-efficient and achieves_

\[_{}^{}(T) O(C_{}(_{ }+1)\{||,|| T\}).\]

Most notably, Algorithm 1 achieves \(_{}^{}(T) O(C_{}(_{ }+1)||)\); that is, up to a \(O(||)\) factor, the reduction achieves online estimation rates in the OEOE framework that are no worse than the minimax rate for offline estimation. For classification, regression, and density estimation with generic finite classes \(\) (Appendix C), the best possible offline estimation error rate is \(_{}=O(||)\), so this shows that price of oracle-efficiency is at most quadratic.

Minimax lower bound.Next, we show that the upper bound in Theorem 3.1 is nearly tight, giving a lower bound that matches up to logarithmic factors.

**Theorem 3.2** (Main lower bound for OEOE).: _Consider the binary classification setting with \(==\{0,1\}\) and loss \(_{0/1}(,)\). For any \(N\) and \(_{}>0\), there exists an instance \((,,,,)\) with \(||=||=N\) such that for any oracle-efficient algorithm, there is a sequence of covariates \((x^{1},,x^{r})\) and offline oracle with parameter \(_{}\) such that \(_{}^{}(T)( \{(_{}+1)N,T\})\)._

This result states that for a generic finite class \(\) and offline estimation oracle \(_{}\), any oracle-efficient online estimator must have

\[_{}^{}(T) (\{(_{}+1)||,(_{}+1) ||,T\})\]

in the worst case. This implies that the \(||\) factor we pay for offline to online conversion is unavoidable, and that Theorem 3.1 is optimal up to a \( T\) factor, giving a near-optimal characterization for the minimax rate for online estimation in the OEOE framework. We conclude with two remarks: (i) The \((_{}+1)\) scaling (as opposed to say, \(_{}\)) in Theorem 3.1 is unavoidable, as witnessed by the optimality of the halving algorithm for noiseless binary classification ; (ii) if the space \(\) and the loss \(\) are convex, then we can change Algorithm 1 to output a deterministic prediction by using the average of all parameters in \(_{t}\) rather than the uniform distribution on \(_{t}\). See Lemma G.1 for details.

General reductions and infinite classes.Algorithm 1 is somewhat specialized to finite classes. In Appendix D (deferred to the appendix for space), we provide a more general approach to designing oracle-efficient algorithms based on _delayed online learning_ (Theorem D.1), and use it to derive a characterization of oracle-efficient learnability for classification with infinite classes \(\) (Theorem D.2).

Full memory vs. finite memory.Algorithm 1 requires full memory of all past offline estimators. The more general approach proposed in Appendix D can use \(N\) most recent offline estimators to obtain an estimation error bound of \(O(C_{}(N+_{}T/N)+N||)\) (Corollary D.1) for any integer \(N>0\).

### Impossibility of memoryless oracle-efficient algorithms

In the general OEOE framework, the learner can use the entire history of estimators \(^{1},,^{i}\) and covariates \(x^{1},,x^{r-1}\) to produce the online estimator \(^{i}\) for step \(t\); notably the Version Space Averaging algorithm with which our upper bounds in the prequel are derived uses the entire history. In this section, we show that for _memoryless_ oracle-efficient algorithms (Definition 3.1) that select the estimator \(^{i}\) only as a function of the most recent offline estimator \(^{i}\), similar guarantees are impossible.

**Definition 3.1** (Memoryless algorithm).: _An online estimation algorithm is memoryless if there exists a map \(F^{}()\) such that we can write \(^{}=F^{}(^{})\), where \(^{}=_{}^{}(x^{1},,x^{-1},y^{1},,y^{-1})\) and \(^{}\) is the randomization distribution for the online estimator \(^{}\).8_

Memoryless algorithms are more practical than arbitrary algorithms, since they do not require storing past estimators or covariates in memory. Our motivation for studying memoryless algorithms arises from recent work in interactive decision making [25; 27], which shows that there exist near-optimal algorithms for contextual bandits and reinforcement learning that use estimation oracles in memoryless fashion. We show that unfortunately, it is not possible to convert offline estimators into memoryless online estimation algorithms with non-trivial error.

**Theorem 3.3** (Impossibility of memoryless algorithms for OEOE).: _Consider the binary classification setting with \(==\{0,1\}\) and loss \(_{0/1}(,)\). For any \(N\) and \(_{} 0\), there exists an instance \((,,,,)\) with \(||=||=N\) such that for any memoryless oracle-efficient algorithm, there exists a sequence of covariates \((x^{1},,x^{r})\) and a (potentially improper) offline oracle \(_{}\) with parameter \(_{}\) such that \([_{}^{}(T)]( \{N(_{}+1),T\})\). This conclusion still holds when the online estimation algorithm remembers \(^{1},,^{-1}\) but not \(x^{1},,x^{-1}\)._

This result shows that in the worst case, any memoryless oracle-efficient algorithm must have

\[_{}^{}(T) ((_{}+1)\{||,||\}).\]

This precludes an online estimation error bound scaling with \((_{}+1)||\) as in Theorem 3.1, and shows that the gap between general and memoryless oracle-efficient algorithms can be exponential.

Interestingly, the lower bound in Theorem 3.3 holds even if the online estimation algorithm is allowed to remember \(^{1},,^{-1}\), but not \(x^{1},,x^{-1}\). The intuition here is that without covariate information, it is not possible to aggregate the predictions of previous estimators or otherwise use them to reduce uncertainty. This provides post-hoc motivation for our decision to incorporate covariate memory into the OEOE protocol in Section 1.1.

The proof of Theorem 3.3 uses that the estimators \(^{}\) produced by the offline estimation oracle may be _improper_ (i.e., \(^{}\)). We defer a variant of the result that holds even if the estimation oracle is _proper_ under additional assumptions as well as the complementary upper bound to Appendix D.2

## 4 Computational complexity of oracle-efficient online estimation

In this section, we turn our attention to the computational complexity of oracle-efficient online estimation in the OEOE framework. In Section 4.1, we show (Theorem 4.1) that in general, it is not possible to transform black-box offline estimation algorithms into online estimation algorithms in a computationally efficient fashion. Then, in Section 4.2, we provide a more fine-grained perspective, showing (Theorem 4.2) that for conditional density estimation, online estimation in the OEOE framework is no harder computationally than online estimation with unrestricted algorithms.

### Computational hardness of oracle-efficient estimation

Our main upper bounds (Section 3.1) show that online estimation error \(_{}^{}(T) O((_{}+1) ||)\) can be achieved in an oracle-efficient fashion for any finite class \(\), but the algorithm (Algorithm 1) is not computationally efficient. We now show that this is fundamental: There exist classes \(\) for which offline estimation can be performed in polynomial time, yet no oracle-efficient algorithm running in polynomial-time algorithm can achieve sublinear online estimation error.

Computational model.To present our results, we must formalize a computational model for oracle-efficient online estimation, and in particular, define a notion of _input length_ for oracle-efficient online algorithms. To do so, we restrict our attention to noiseless binary classification, and consider a sequence of classification instances indexed by \(n\), with \(==\{0,1\}\), \(_{n}:=\{0,1\}^{n}\), \((z)=_{z}\), and indicator loss \(_{0/1}(,)\). We consider a sequence of classes \(_{n}\) that have polynomial description length.e. \(|_{n}|\) is polynomial in \(n\), so that \(f_{n}\) can be described in \((n)\) bits. In particular, we assume that \(f_{n}\) is represented as a Boolean circuit of size \((n)\) so that \(f(x)\) can be computed in \((n)\) time for \(x_{n}\); we refer to such sequences as _polynomially computable_.

To allow for offline estimators that are improper, we assume that for all \(t\) and all sequences \((x^{i},y^{i}),,(x^{r},y^{r})\), the output \(^{i}:\{0,1\}^{n}\{0,1\}\) returned by \(_{}^{}(x^{1},,x^{t-1},y^{1},,y^{t-1})\) is a Boolean circuit of size \((n)\); we refer to such oracles as having \((n)\)-_output description length_.9 Likewise, to allow the online estimation algorithm itself to be improper and randomized, we restrict to algorithms for which computing \(^{i}(x)\) for \(^{i}^{i}\) can be implemented as \(^{i}(x,r)\) for a random bit string \(r(\{0,1\}^{B})\), where \(B=(n)\); we refer to the online estimator as having \((n)\)-output description length if \(^{i}(,)\) is itself a Boolean circuit of size \((n)\). We refer to the online estimation algorithm _polynomial time_ if it runs in time \((n)\) for any sequence of inputs \(t\), \(x^{1},,x^{t-1}\), and \(^{1},,^{t-1}\), and has \((n)\)-output description length.10

Main lower bound.Our main computational lower bound is as follows.

**Theorem 4.1** (Computational lower bound for OEOE).: _Assume the existence of one-way functions.11 There exists a sequence of polynomially computable classes \((_{1},_{2},,_{n},)\), along with a sequence of \((n)\)-output description length offline oracles with \(_{}=0\) associated with each \(_{n}\), such that for any fixed polynomials \(p,q:\) and all \(n\) sufficiently large, any oracle-efficient online estimation algorithm with runtime bounded by \(p(n)\) must have \([_{}^{n}(T)] T/4\) for all \(1 T q(n)\). At the same time, there exists an inefficient algorithm that achieves \([_{}^{n}(T)] O()\) for all \(T\)._

Informally, Theorem 4.1 shows that there exist a class \(\) and offline estimation oracles \(_{}\) for which no oracle-efficient online estimation algorithm that runs in time

\[((),(), _{t}(^{i}),T)\]

can achieve sublinear estimation error, where \(()\) and \(()\) denote the number of bits required to describe \(x\) and \(f\), and \((^{i})\) denotes the size of the circuit required to compute \(^{i}(x)\). Yet, low online estimation error _can_ be achieved by an inefficient algorithm, and there exist efficient offline estimators with \(_{}=0\) as well. The result is essentially a corollary of Blum ; we refer to Appendix H.1 for the detailed proof. We mention in passing that Hazan and Koren  also give lower bounds against reducing online learning to offline oracles, but in a somewhat different computational model; see Appendix B for detailed discussion.

Theorem 4.1 is slightly disappointing, since one of the main motivations for studying oracle-efficiency is to leverage offline estimators as a computational primitive. Combined with our results in Section 3, Theorem 4.1 shows that even though it is possible to be oracle-efficient information-theoretically, it is not possible to achieve this computationally. Nonetheless, we are optimistic that our abstraction can (i) aid in designing computationally efficient algorithms for learning settings beyond online estimation, and (ii) continue to serve as a tool to formalize lower bounds against natural classes of algorithms, as we have done here; see Section 5 for further discussion.

**Remark 4.1**.: _Theorem 4.1 relies on the fact that the offline estimator may be improper (i.e., \(^{i}\)). An interesting open problem is whether one can attain \((|_{n}|) o(T)\) online estimation error with runtime \((|_{n}|)\) given a proper offline estimation oracle with parameter \(_{}=0\). \(\)_

### Conditional density estimation: computationally efficient algorithms

In spite of the negative result in the prequel, which shows that efficient computation in the OEOE framework is not possible in general, we can provide a more fine-grained perspective on the computational complexity of oracle-efficient estimation for the problem _conditional density estimation_, a general task which subsumes classification and regression, and has immediate applications to reinforcement learning and interactive decision making [27; 28].

Conditional density estimation.Recall that conditional density estimation [71; 12] is the special case of the online estimation framework in Section 1 in which \(\) and \(\) are arbitrary, \(=()\), and the kernel is \((z)=z\); that is sampling \(y(f^{}(x))\) is equivalent to sampling \(y f^{}(x)\). For the loss, we use squared Hellinger distance: \(_{}^{2}f(x),f^{}(x)= -(y x)}^{2}\); with online estimation error given by \(}_{}^{}(T)=_{t=1}^{T} _{}^{2}^{}(x^{t}),f^{}(x^{t}) \).

**Base algorithm.** Our result is based on a reduction. We assume access to a base algorithm \(_{}\) for online estimation in the Conditional Density Estimation (CDE) framework, which is _unrestricted_ in the sense that it is not necessarily oracle-efficient. That is, \(_{}\) can directly use the full data stream \((x^{},y^{}),,(x^{-1},y^{-1})\) at step \(t\). For parameters \(R_{}(T)\) and \(C_{} 1\), we assume that for any \(f^{}\), the base algorithm \(_{}\) ensures that for all \((0,e^{-1})\), with probability at least \(1-\),

\[}_{}^{}(T) R_{}(T)+C_{}(1/).\] (4)

We define the total runtime for \(_{}\) across all rounds as \((,T)\).

Main result.Our main result shows that any algorithm \(_{}\) satisfying the guarantee above can be transformed into an oracle-efficient algorithm with only polynomial blowup in runtime. For technical reasons, we assume that \(V:=e\ \ _{f,f^{},x,y} (y x)}\) is bounded; our sample complexity bounds scale only logarithmically with respect to this parameter. In addition, we assume that all \(f\) and \(x\) have \(O(1)\) description length, and that one can sample \(y f(x)\) in \(O(1)\) time.

**Theorem 4.2**.: _Let \(_{}\) be an arbitrary (unrestricted) online estimation algorithm that satisfies Eq. (4) and has runtime \((,T)\). Then for any \(N\), there exists an oracle-efficient online estimation algorithm that achieves estimation error_

\[}_{}^{}(T) (C_{} V_{}T/N+N (R_{}(T)+C_{} V))\]

_with runtime \(((,T),||,| |,T)\), where \(_{} 0\) is the offline estimation parameter. The distributions \(^{1},,^{T}\) produced by the algorithm have support size \((||,||,T)\). As a special case, if the online estimation guarantee for the base algorithm holds with \(R_{}(T) C_{}^{} T\) for some problem-dependent constant \(C_{}^{} 1\), then by choosing \(N\) appropriately, we achieve \(}_{}^{}(T) (C_{}(C_{}+C_{}^{})_{})^{1/2} V T^{1/2}+(C_{}+C_{ }^{}) V\)._

Note that the estimation error bound in Theorem 4.2 is sublinear whenever the rate \(R_{}(T)\) is. This implies that for squared Hellinger distance, online estimation in the OEOE framework is no harder computationally than online estimation with arbitrary, unrestricted algorithms.

The proof of Theorem 4.2 is algorithmic, and is based on several layers of reductions. The main reason why the result is specialized to conditional density estimation is as follows: If we have an estimator \(\) for which \(_{}^{2}(x),f^{}(x)\) is small for some \(x\), we can simulate \(y f^{}(x)\) up to low statistical error by sampling \(y(x)\) instead, as \(\) and \(f^{}\) are close in distribution. This allows us to implement a scheme based on simulating outcomes and feeding them to the base algorithm.

## 5 Discussion

Our work introduces the Oracle-Efficient Online Estimation protocol as an information-theoretic framework to study the relative power of online and offline estimators and gives a nearly complete characterization of the statistical and computational complexity of learning in this framework. In what follows, we discuss broader implications for our information-abstraction of oracle-efficiency.

Oracle-efficient learning as a general framework for analysis of algorithms.One of the most important contributions of this work is to formalize oracle-efficient algorithms as mappings that act upon a sequence of estimators but do not directly act on historical outcomes. While the computational lower bounds we provide for oracle-efficient learning are somewhat disappointing, we are optimistic that--similar to statistical query complexity in TCS and information-based complexity in optimization--our abstraction can (i) aid in designing computationally efficient algorithms for learning settings beyond online estimation, and (ii) continue to serve as a tool to formalize lower bounds against natural classes of algorithms, for estimation and beyond. That is, we envision _oracle-efficient learning_ as a more general framework to study oracle-based algorithms in any type of interactive learning problem. We remark that one need not restrict to offline oracles; it is natural to study oracle-efficient algorithms based on online estimation oracles or other types of oracles through our information-theoretic abstraction as well. For concreteness, let us mention a couple of natural settings where our information-theoretic abstraction can be applied.

_Oracle-efficient interactive decision making_. For interactive decision making problems like bandits and reinforcement learning (more broadly, the DMSO framework described in Appendix E.1), it is natural to formalize _oracle-efficient_ algorithms as algorithms that do not directly observe rewards (bandits) or trajectories (reinforcement learning), and instead must select their decision based on an (online or offline) estimator (e.g., regression for bandits or conditional density estimation for RL). Foster and Rakhlin (2015) and Foster et al. (2017) et seq. provide algorithms with this property for contextual bandits and RL, respectively, but the power of offline oracles in this context is not well understood.

_Oracle-efficient active learning_. For active learning, it is natural to consider algorithms that decide whether to query the label for a point in an oracle-efficient fashion (e.g., Krishnamurthy et al. (2018)). For concreteness, consider pool-based active learning (Krishnamurthy et al., 2018). Suppose the learner is given a pool \(=\{x_{1},...,x_{n}\}\) of covariates and a parameter space \(\). The learner can repeatedly choose \(x^{t}\) and call the offline oracle to obtain an estimator \(^{i}\) such \(_{}^{}(t):=_{i<t}( ^{i}(x^{i}),f^{}(x^{i}))_{}\) (in contrast to an unrestricted algorithm that observes \(y^{t}=f^{}(x^{t})\)). The aim is to learn a hypothesis with low classification error using the smallest number of queries possible. Can we design oracle-efficient algorithms that do so with near-optimal label complexity?

### Further Directions

We close with some additional directions for future research.

Refined notions of estimation oracles.This work considers generic offline estimation algorithms that satisfy the statistical guarantee in Definition 1.1 but can otherwise be arbitrary. Understanding the power of offline estimators that satisfy more refined (e.g., problem-dependent) guarantees is an interesting direction for future research.

Open questions for proper versus improper learning.Our results leave some interesting gaps in the power of proper versus improper oracles. First, the computational lower bounds in Section 4.1, leave open the possibility of attaining \((|_{n}|) o(T)\) online estimation error with runtime \((|_{n}|)\) given access to a _proper_ offline estimation oracle with parameter \(_{}=0\). Second, our results in Section 3.2 leave open the possibility of bypassing the \((||(_{}+1))\) lower bound for memoryless algorithms under the assumption that the offline oracle is proper.