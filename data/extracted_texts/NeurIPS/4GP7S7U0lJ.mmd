# Learnability Matters: Active Learning for Video Captioning

Yiqian Zhang\({}^{1}\), Buyu Liu\({}^{2}\), Jun Bao\({}^{2}\), Qiang Huang\({}^{3}\), Min Zhang\({}^{2}\), Jun Yu\({}^{2}\)

\({}^{1}\)Hangzhou Dianzi University

\({}^{2}\)Harbin Institute of Technology (Shenzhen)

\({}^{3}\)National University of Singapore

yiqian.zyq@gmail.com,{buyu.liu, baojun}@hit.edu.cn,

huangq@comp.nus.edu.sg, zhangmin2021@hit.edu.cn, zju.yujun@gmail.com

Corresponding author

###### Abstract

This work focuses on the active learning in video captioning. In particular, we propose to address the learnability problem in active learning, which has been brought up by collective outliers in video captioning and neglected in the literature. To start with, we conduct a comprehensive study of collective outliers, exploring their hard-to-learn property and concluding that ground truth inconsistency is one of the main causes. Motivated by this, we design a novel active learning algorithm that takes three complementary aspects, namely learnability, diversity, and uncertainty, into account. Ideally, learnability is reflected by ground truth consistency. Under the active learning scenario where ground truths are not available until human involvement, we measure the consistency on estimated ground truths, where predictions from off-the-shelf models are utilized as approximations to ground truths. These predictions are further used to estimate sample frequency and reliability, evincing the diversity and uncertainty respectively. With the help of our novel caption-wise active learning protocol, our algorithm is capable of leveraging knowledge from humans in a more effective yet intellectual manner. Results on publicly available video captioning datasets with diverse video captioning models demonstrate that our algorithm outperforms SOTA active learning methods by a large margin,\(e\)._g_.we achieve about \(103\%\) of full performance on CIDEr with \(25\%\) of human annotations on MSR-VTT.

## 1 Introduction

Video captioning, which aims to understand videos in the form of describing them in natural language Abdar et al. (2023), becomes a heated task Lin et al. (2022) with the emergence of a large amount of data Li et al. (2023) as well as transformer-based models Liu et al. (2022). Despite the superior performance, existing methods suffer heavily from the need for time-consuming and labor-intensive human annotations Chan et al. (2020) because of their learning-based nature.

Approaches such as active learning Tharwat and Schenck (2023), semi-supervised learning Yang et al. (2023), and domain adaptation Yu et al. (2023) are proposed to address the above-mentioned data issue. This paper takes the active learning approach where we assume there exists a small amount of labelled data together with a large number of unlabelled ones. Our goal is to select the most informative samples from the unlabelled set and have them annotated by humans such that the video captioning model can achieve the best performance with the minimum human effort. Though uncertainty and diversity have been exploited to investigate the properties of unlabelled data, thepotential problem from human annotations is largely neglected in active learning literature as these annotations are not available during the selection process.

In this paper, we first and foremost propose to analyze problems of human annotations in video captioning tasks (see Fig. 1.(b)). With the help of Dataset Maps, we are able to observe the problem, or collective outliers, in human annotations, and conclude its severity by the amount of outliers. Such an amount will further aggravate the learning process due to their hard-to-learn nature, thus worth exploring. We observe that inconsistency in human annotations is one of the main causes of collective outliers, which can be divided into the abstraction and granularity inconsistency (refer to Fig. 1.(a)). The former highlights instances where humans offer different abstractions for videos with complex contents, while the latter reveals that humans may describe the same object at varying semantic levels.

To this end, we incorporate our observations into active learning frameworks. We re-phrase the problem introduced by collective outliers as learnability and aim to address it in our active learning design. Due to the lack of access to human annotations in active learning, directly identifying collective outliers is implausible. Nevertheless, we turn to their main property, or inconsistency, as our breakthrough and propose to estimate the abstraction and granularity inconsistencies in unlabelled data. Concretely, existing image-based large Vision-Language Models (LVLMs) are used to approximate human annotations. Instead of directly using predictions from LVLMs as pseudo ground truths, which would deteriorate the overall performance because of domain gaps, we propose to measure the consistency not only internally among per-frame predictions but externally between predictions from a video captioning model and those per-frame predictions. The internal measurement captures the abstraction consistency and the external one counts on the granularity consistency. Combining both provides us an estimation of sample **learnability**, or how likely this sample belongs to collective outliers. **Diversity** and **uncertainty** are also leveraged in our active learning scheme where we rank samples based on their frequency in the entire dataset and reliability respectively. Our algorithm is able to select reliable, diverse yet learnable samples, and achieves SOTA trade-offs between accuracy and human efforts. Motivated by the causes of collective outliers, we propose a caption-wise active learning protocol such that only a limited number of human-annotated captions are acquired if one video is selected by our active learning algorithm. Our protocol is capable of avoiding inconsistency and providing a more intellectual way to allocate human effort to more diverse videos. In all, our contribution can be summarized as follows:

* To the best of our knowledge, we are the very first in terms of exploring collective outliers on video captioning tasks and providing comprehensive studies on them.
* A novel active learning algorithm that explicitly takes learnability, diversity, and uncertainty into account. Specifically, the learnability is designed to tackle collective outliers, inspired by their abstraction and granularity inconsistency.
* A novel caption-wise active learning protocol that effectively leverages knowledge from humans.
* State-of-the-art performances on video captioning datasets under diverse model backbones.

Figure 1: We provide examples of collective outliers as well as their causes in (a). (b) illustrates the Dataset Maps based on the SPICE score for the MSR-VTT training set, with the y-axis representing the average SPICE score over training epochs and the x-axis showing their variability. The consistency score is also measured on ground truth captions. The entire space is divided into four sub-regions based on x,y coordinates, with examples in each region demonstrating different levels of learnability.

Related Work

Visual Captioning Based on Deep LearningVisual captioning Sharma et al. (2023) is a task that automatically generates textual grammatical and semantically appropriate descriptions for a given visual content. While this task is easy for humans, it is extremely difficult for deep learning models. To perform visual captioning, models are required to have multiple capabilities: including but not limited to detection and recognition capabilities to extract objects in visual content, visual semantic understanding capabilities to determine the attributes and relationships of objects, and enriching language knowledge to describe information with grammatically correct sentences. Visual captioning can be classified into image captioning Ming et al. (2022) and video captioning Abdar et al. (2023). Deep-learning-based image captioning starts with the encoder-decoder framework combining convolutional neural networks and recurrent neural networks Kiros et al. (2014); Vinyals et al. (2015). It is further developed by extracting fine object region features Anderson et al. (2018); Karpathy and Fei-Fei (2015), attribute features Yao et al. (2017), semantic relation features Yao et al. (2018); Yang et al. (2022) and introducing attention mechanisms Xu et al. (2015); Pan et al. (2020); Yu et al. (2020). With breakthroughs in vision-and-language pre-training approaches Radford et al. (2021); Zhang et al. (2021); Alayrac et al. (2022); Li et al. (2023a), applications in image captioning Li et al. (2020); Mokady et al. (2021); Li et al. (2023c) have emerged in recent years, benefit from the power of LVLMs (_e.g_.rich knowledge and strong recognition ability). The recently proposed BLIP2 Li et al. (2023a) has achieved superior performance on major image-text tasks with a simple pre-training method. Unfortunately, LVLMs focusing on video-text-related tasks have not yet come out. Video captioning still faces many challenges. Video captioning is more difficult than image captioning due to the many frames the video contains, which carry massive amounts of information. This raises the necessity of abstracting semantic information from the temporal dimension and describing content from the spatial dimension with different granularity. Video captioning started with the encoder-decoder framework Venugopalan et al. (2015). Subsequent researchers continued to design new encoders Wang et al. (2019); Pan et al. (2016) and decoders Jin et al. (2020); Guo et al. (2016). Some work is also actively trying to introduce scene graphs Hou et al. (2020); Zhang et al. (2020), text corpus Shi et al. (2023); Zhang et al. (2021), and pre-training models Li et al. (2023b); Seo et al. (2022); Tang et al. (2021). Among them, Lin _et al_.proposed the end-to-end video captioning method Lin et al. (2022) for the first time and achieved significant performance improvement. Unlike existing video captioning methods, we focus on underlying data issues. Unlike existing video captioning methods, our work focuses on underlying data issues. We are committed to exploiting knowledge to measure video consistency scores for active learning.

Active LearningDue to the high cost of manually annotating data, active learning Tharwat and Schenck (2023) has attracted widespread attention from academia and industry. Active learning aims to use as few, high-quality samples as possible to achieve the best possible performance of the model. Active learning has many mature works in closed tasks such as image classification Parvaneh et al. (2022), object detection Wu et al. (2022), semantic segmentation Xie et al. (2022), and natural language processing Zhang et al. (2022). Among them, Coreset Sener and Savarese (2018) can minimize the distance between an example in the unlabeled pool to its closest labeled example, and can effectively capture the diversity in the dataset. However, active learning for video captioning remains to be explored. Chan et al. (2020) tries to migrate conventional active learning methods to video captioning, and has proposed a new method based on ensemble and clustering. Besides, active learning has been observed to fail on open-ended tasks such as visual question answering due to the presence of collective outliers Karamcheti et al. (2021). There is currently no literature that attempts to address the impact of collective outliers. In this work, we focus on the problem of collective outliers in video captioning. We point out possible causes of collective outliers and attempt to mitigate their impact on active learning methods.

## 3 Method

As the very first work that explores data learnability in active learning for video captioning, we start our method with a comprehensive analysis of their causes and various forms in Sec. 3.1. According to our analysis, we then propose a novel active learning scheme that in particular considers the learnability during the unlabelled data selection process in Sec. 3.2, together with novel designs of uncertainty and diversity. Our overall active learning method can be found in Fig. 2.

### Data Learnability in Video Captioning

_Motivation_ The order of training samples is shown to be important in curriculum learning matiisen2020exploiting. Specifically, it found that feeding models with examples of successively increasing difficulty produces better performances than providing full examples immediately. Though posing a more severe problem in active learning as selecting far more difficult samples at early stages is a waste of human efforts, measuring sample difficulty is largely neglected in the literature as neither ground truth nor the definition itself can be easily obtained. To this end, our goal is to first identify difficult examples in video captioning task, and then perform analysis on them so that our observations can be beneficial for our active learning selection scheme.

_Methodology_ Collective outliers are a group of data objects that fall extremely far from well-defined norms of a data set or given concepts of expected behavior in data-mining lai2021exploiting, which are much harder to detect as they are more generic yet harder to identify as individuals. Not surprisingly, one important property of collective outliers is learnability, e.g., they are more likely to be hard-to-learn samples compared to normal data Karamcheti et al. (2021). Concretely, we exploit Dataset Maps to diagnose datasets with training dynamics swayamdipta2020exploiting; dagan2013exploiting; Sakaguchi et al. (2020). Compared to conventional Dataset Map that exploits two model-specific metrics (_i.e_.average confidence assigned to the correct answer and the variance of these values) to measure the _learnability_ of training examples, we propose to utilize SPICE Anderson et al. (2016) to approximate the confidence score on the correct answer. On the one hand, the confidence score is not a reliable criterion as it can be largely affected by random-length text sequences generated by autoregressive models zhang2020exploiting. On the other hand, SPICE measures the F1 score between scene graphs generated by predictions and ground truths. It has the useful property of being defined over tuples that are easy to subdivide into meaningful categories, providing a more semantic-centric evaluation in a human-interpretable manner by abstracting away most of the lexical and syntactic idiosyncrasies of natural language compared to CIDEr Vedantam et al. (2015).

_Observation_ We provide an example of Dataset Map of SwinBERT lin2022exploiting trained on MSR-VTT Xu et al. (2016) in Fig. 1.(b), where the y-axis and x-axis plots the average SPICE score over training epochs and their variability respectively. Clearly, samples that fall into the upper left area of this figure are easy-to-learn ones whose SPICE scores are consistently high. While the lower left area is occupied with hard-to-learn samples where no observable improvements can be found with longer training time. Unlike conventional close-end tasks where only a small proportion of samples are hard to learn, we observe that a noticeable amount of samples fall into the lower left area.

_Analysis_ Taking a closer look at collective outliers, we observe that the video captioning model struggles when learning from samples with inconsistent human annotations. As shown in Fig. 1.(a), these inconsistencies can be divided into two categories, abstraction and granularity. The inconsistency in abstraction summarizes cases where humans tend to provide not only the description of video contents, but also high-level reasoning behind it. For instance, _a boy is walking to the beach to surf_ versus _a movie about a man who likes to surf_. On the other hand, inconsistency in granularity consists of samples with human annotations at different graininess,_e.g_.people might coarsely describe the video as _a cartoon character talking on the phone_ while a fine-grained description summarizes the same video with _Krabs talks on the phone while SpongeBob freaks out_. We argue that collective outliers are not impossible to learn. Instead, they can be exploited with more consistent descriptions or learned at a later stage of the training process when general principles from easier examples are already learned by models. Our observations and analysis align well with the philosophy of curriculum learning, encouraging our active learning design in the following paragraphs.

### Our Active Learning Scheme

Denoting the labeled set as \(=\{V_{m},_{m}\}_{m=1}^{M}\), consisting of \(M\) video sequences and their human annotated captions \(_{m}\), our video captioning model \(f\) is initially trained with \(\). We further denote another set of \(N\) unlabelled videos as \(=\{V_{n}\}_{n=1}^{N}\). Mathematically, \(=\). Our goal of active learning is to select a subset \(S_{t}\) at each selecting step \(t\{1,,T\}\) such that the overall performance of \(f\) can be maximally boosted once annotations on \(S_{t}\) are obtained from humans.

As stated above, collective outliers can be regarded as a group of hard-to-learn samples in an active learning framework where inconsistencies in human annotations are one of the main causes. Due to the lack of access to these annotations when selecting from \(\), we propose to mimic them to measure those inconsistencies on the mimicked annotations. In particular, we exploit LVLMs since they provide powerful tools to generate human-like captions at the same granularity. On the one hand, one can approximate the abstraction inconsistency among predictions from LVLMs. On the other hand, by comparing the predictions from \(f\) and LVLMs, granularity inconsistency can be further approached. Rather than leveraging foundation models for videos, we utilize the predicted per-frame captions from image-based LVLMs for the following three reasons. Firstly, the image-based LVLMs are more mature and well-exploited compared to the former. Moreover, inconsistency in abstraction occurs more frequently when video samples are complex, e.g., contents vary a lot temporally. Such variety can be better captured by measuring the consistency between frame-wise predictions. Lastly, our design provides a lower bound when measuring the inconsistency in granularity as frame-wise predictions from LVLMs and video captions predicted by \(f\) look at videos from different perspectives to the maximum extent, leading to more reliable approximations.

To this end, we uniformly sample \(I=32\) frames from \(V_{n}\) and refer to the \(i\)-th frame as \(V_{n}^{i}\). Then we apply BLIP2 Li et al. (2023a) on all sampled frames, leading to a set of \(\{b_{n}^{i}\}_{i=1}^{I}\). \(b_{n}^{i}\) is the predicted caption on \(V_{n}^{i}\). Meanwhile, we can obtain the video captions on \(V_{n}\) by \(f(V_{n})\) and further denote the one with the highest confidence as \(b_{n}^{*}\). Inspired by SPICE Anderson et al. (2016), we convert captions to scene graphs and measure the consistency over graphs. Specifically, we denote \(G_{n}^{i}=\{O_{n}^{i},A_{n}^{i},R_{n}^{i}\}\) as the scene graph generated from \(b_{n}^{i}\), which consists of objects \(O_{n}^{i}\), attributes \(A_{n}^{i}\), and their relations \(R_{n}^{i}\). Then \(_{n}=\{G_{n}^{i}\}_{i=1}^{I}\) is the set of all scene graphs generated from \(\{b_{n}^{i}\}_{i=1}^{I}\). Similarly, we can obtain scene graph \(G_{n}^{*}\) from \(b_{n}^{*}\). We further denote \(_{n}=_{i=1}^{I}O_{n}^{i}\), \(_{n}=_{i=1}^{I}A_{n}^{i}\) and \(_{n}=_{i=1}^{I}R_{n}^{i}\). In the next few paragraphs, we will introduce our active learning scheme that aims to capture the **learnability**, **diversity** and **uncertainty** in video captioning.

Our scheme is composed of four terms. The first two of them focus on **learnability**, namely abstraction and granularity inconsistencies. The first term \(L_{n}^{1}\) focuses on the scene graphs generated by per-frame BLIP2, or \(_{n}\), and measures their internal prediction consistency. Our \(L_{n}^{1}\) is defined as:

\[L_{n}^{1}=_{n}}H_{k}(_{n})}{|_{n }|}+_{n}}H_{k}(_{n})}{|_{n}|}, \]

where \(H_{k}(_{n})\) counts the number of times that an object, an attribute, or a relationship \(k\) appears in \(_{n}\). \(|_{n}|\) and \(|_{n}|\) capture the number of unique objects and attributes in \(_{n}\), respectively. Intuitively, Eq. 7 prefers \(\{b_{n}^{i}\}_{i}\) when they agree with each other. In other words, the higher the \(L_{n}^{1}\) is, the more

Figure 2: Our method explicitly introduces learnability to reflect the collective outliers in video captioning. Together with uncertainty and diversity, our active learning scheme ranks unlabelled videos and parses them to humans. Our caption-wise protocol further provides an intellectual yet effective way to allocate human efforts, leading to 103\(\%\) full performances at 25\(\%\) human annotations.

consistent \(b^{i}_{n}\) should be w.r.t. predictions from other frames. We purposely neglect relations \(R^{i}_{n}\) in experiments as the relationship is less reliable in current LVLMs (e.g. BLIP2).

Our second term \(L^{2}_{n}\) focuses on granularity inconsistency where human annotations are at different graininess. To this end, we approximate the potential granularity inconsistency between \(_{n}\) and \(G^{*}_{n}\) as predictions from image-based LVLMs and that from video captioning model \(f\) tend to capture different aspects of \(V_{n}\) to the maximum extent. In particular, granularity inconsistency can be measured by SPICE between two types of predictions, where a lower SPICE value indicates higher inconsistency. Instead of relying solely on SPICE, which tends to select well-learned samples and undermines the active learning scheme, we focus on the time-variant changes in SPICE for each \(V_{n}\) to simulate the expected changes in granularity consistency. Denoting the absolute distance between the SPICE on an unlabelled sample \(V_{n}\) at time step \(t-2\) and \(t-1\) as \( s_{n}\), we have \(L^{2}_{n}=g( s_{n}) g( s_{n})\), where \(g\) is a min-max normalization function over all \(N\) samples. Our \(L^{2}_{n}\) prefers samples with moderate changes, based on our observation that large and small changes are associated with less informative and hard-to-learn examples, respectively.

The third term \(L^{3}_{n}\) is designed for **diversity**. Specifically, we prefer unlabelled samples in which contents are beyond the current model \(f\) yet of great importance. In practice, we apply the concept of the Term Frequency Inverse Document Frequency (TF-IDF) Robertson (2004) to measure the importance of video content, in which we further incorporate our observation that longer descriptions tend to co-occur more with diverse videos by re-weighting the TF with long captions. Let's denote \(^{*}\) as the set that includes predictions of the highest confidence on \(\), or \(^{*}=\{G^{*}_{n},G^{*}_{n}\}_{n=1,m=1}^{N,M}\). Similarly, the BLIP2 predictions on full dataset is denoted as \(=\{_{j}\}_{j=1}^{M+N}=\{G^{i}_{j}\}_{i=1,j=1}^{T,M+N}\). Mathematically, \(L^{3}_{n}=F(_{n})+F(_{n})\) where \(F()\) is defined as:

\[F(x)=_{k x}[k^{*}](H_{k}(_{n}) ^{N+M}[H_{k}(_{j})>0]}) \]

where \([]\) is a binary indicator function and equals to 1 iff \(\) is valid. Mathematically, \(L^{3}_{n}\) focuses only on contents beyond the current model. Meanwhile, it values distinctive samples with important contents. Overall, a higher \(L^{3}_{n}\) indicates greater diversity in \(V_{n}\).

**Uncertainty** is captured by our last term \(L^{4}_{n}\). Intuitively, inaccurate predictions provide valuable information in active learning. Therefore, we introduce \(L^{4}_{n}=|O^{*}_{n}_{n}|+|A^{*}_{n}_{n}|+|R^{*}_ {n}_{n}|\). Specifically, \(L^{4}_{n}\) counts shared objects, attributes, and relationships between \(G^{*}_{n}\) and \(_{n}\), where \(_{n}\) serves as human annotations to measure how well the prediction of highest confidence \(G^{*}_{n}\) is. A higher \(L^{4}_{n}\) reflects more certainty in \(V_{n}\).

Finally, the overall active learning score on sample \(V_{n}\) is defined as \(L_{n}=-_{1}L^{1}_{n}+_{2}L^{2}_{n}-_{3}L^{3}_{n}+L^{4}_{n}\), where \(_{1},_{2},_{3}\) are hyper-parameters. At the \(t\)-th step of the active learning algorithm, the top unlabelled samples \(S_{t}\) from \(\) are selected w.r.t. \(L_{n}\), where the lower \(L_{n}\) signifies greater informativeness. These samples are then fed to human annotators to acquire annotations \(_{S_{t}}\). Then we update our labelled and unlabelled set with \(\{S_{t},_{S_{t}}\}\) and \( S_{t}\). Later on, our video captioning model \(f\) is re-trained with the updated \(\). Our overall active learning algorithm iterates until either the maximum time stamp \(T\) or human annotation effort is reached.

### Caption-wise Selection Protocol

Conventional captioning-related active learning algorithms are video-based where all the human annotations from one video sequence are acquired if this video is selected at step \(t\). In practice, there are multiple annotations associated with one video sequence, e.g., we have 20 captions for one video sequence in MSR-VTT dataset Xu et al. (2016). We argue that the current video-based selection protocol is prone to collective outliers, leading to inferior active learning performance. Instead, we propose a caption-wise selection scheme such that not all annotations of one video sequence are acquired if this video has been selected. In practice, we acquire at most 2 2 captions for each selected video at the \(t\)-th step. Such a design reflects two of our observations. Firstly, the fewer annotations are acquired from humans, the less likely they are inconsistent with each other, leading to fewer collective outliers. Moreover, including more videos with fewer annotations rather than fewer videos with more annotations boosts the diversity of \(S_{t}\). The superiority of our protocol is shown in Sec. 4.

Experiment

We validate our ideas with various backbones on two publicly available datasets MSVD Chen and Dolan (2011) and MSR-VTT Xu et al. (2016), and demonstrate SOTA performances on both.

### Dataset and Experimental Setup

**Datasets** We conduct our experiments on two datasets, MSVD Chen and Dolan (2011) and MSR-VTT Xu et al. (2016). Specifically, MSVD consists of 1970 open-domain videos collected from a commercial video search engine, each of which is associated with about 41 human-annotated captions. Similarly, there are 10K open-domain videos in MSR-VTT and each of them has 20 human-labelled annotations. For each dataset, we follow their standard splits and report our active learning performance on their test sets. To mimic the learning process, we initialize \(\) with \(5\%\) of data randomly selected from the training set, including both videos and their annotations. Consequently, \(\) is composed of videos from the remaining \(95\%\) training set. At each selecting step, the annotation budget \(\|_{S_{t}}\|\) is set to \(5\%\) captions of the full training set. More details can be found in the appendix.

**Baselines** Active learning for video captioning is under-explored in literature. We follow the existing work Chan et al. (2020) and compare our algorithms with the baselines described below:

* **Random Sampling** serves as a very competitive baseline in open-ended tasks. Basically, it performs random sampling in \(\) to obtain \(S_{t}\).
* **Maximum Entropy**Chan et al. (2020) is a conventional uncertainty-based active learning algorithm where samples with the highest entropy will be selected. In video captioning tasks, the entropy of sample \(V_{n}\) is approximated by the averaged entropy over multiple predictions in \(f(V_{n})\). For each prediction, its entropy is computed over word output distributions at each new word.
* **Minimum Likelihood**Chan et al. (2020) is another uncertainty-based active learning algorithm. Typically, samples with the lowest log-likelihood will be selected. In practice, the averaged log-likelihood over multiple predictions in \(f(V_{n})\) is used to rank each unlabelled video \(V_{n}\).
* **Core-Set Selection**Sener and Savarese (2018) is a classical diversity-based active learning algorithm. Specifically, it works on the representation space and aims to select samples that are spread out in the feature space. In practice, features from Liu et al. (2022) are used to select samples that minimize the distance between the unlabelled pool to its closest labeled sample.
* **Clustered-Divergence**Chan et al. (2020) ensembles multiple models and computes the KL-divergence between the conditional distributions of the ensemble members to measure sample uncertainty. Diversity is also implicitly considered as it already ensembles a clustering-based active learning regularization method.

**Implementation Details** In our experiment, we employ SwinBERT Lin et al. (2022) and CoCap Shen et al. (2023) as our \(f\) as they provide good trade-offs between accuracy and efficiency. In contrast, other SOTA video captioning methods or video-based foundation models, such as COSA Chen et al. (2023) and mPLUG-2 Xu et al. (2023), require extensive pre-training or finetuning when adapting to downstream tasks to achieve SOTA video captioning performances. To further speed up the training process, each video is uniformly sampled with 32 frames and these frames are parsed to \(f\). The batch size is set to 4. We refer to the performance of \(f\) that trained with the entire training set on the test set as _full performance_. And we are able to achieve comparable performance compared to the official release. As for BLIP2 Li et al. (2023), we choose an open-sourced version 3. All experiments are conducted on 4 RTX 3090 GPUs and 4 RTX 4090 GPUs with Pytorch Paszke et al. (2019), Huggingface transformers Wolf et al. (2020). \(T\) is set to 4. For other hyper-parameters, we keep the same configuration as in Das et al. (2022). More details can be found in the appendix.

_Evaluation Metrics_ We provide detailed comparisons using a diverse set of performance metrics, including BLEU4 Papineni et al. (2002), METEOR Banerjee and Lavie (2005), ROUGE-L Lin and Och (2004), CIDEr Vedantam et al. (2015), and SPICE Anderson et al. (2016).

### Main Active Learning Performances

We report the active learning performances of all methods with either SwinBERT or CoCap on MSVD in Fig. 3. To ensure more reliable results, all methods were conducted three times. This figure reports both the average performance and the variance.

There are several interesting observations. First and foremost, it is noteworthy that our method consistently outperforms all other baselines across all five evaluation metrics with two backbones, highlighting our superiority. Predictably, random sampling usually ranks as the second-best option, aligning with findings from other open-ended tasks Karamcheti et al. (2021). More importantly, we observe that our method surpasses the full performance of \(f\) trained with 100\(\%\) annotations with SwinBERT out of 4 evaluation metrics, as indicated by the dotted horizontal line, with less than 25\(\%\) of annotations. For example, we achieve full performance with about 10\(\%\) and 15\(\%\) of annotations when measured by BLEU4 and ROUGE-L, respectively. Regarding CIDEr, our method attains 103.65\(\%\) of the full performance with 25\(\%\) of annotations. Even with CoCap, we can almost always achieve more than 95\(\%\) of full performance with 25\(\%\) annotations. This observation supports our concerns about the negative impacts of collective outliers during the training process. Clearly, our method effectively reduces the impact of collective outliers, leading to significant performance improvements over existing methods and even surpassing full performance. Results on MSR-VTT are reported in the appendix. In summary, our method significantly outperforms the SOTA methods and achieves an averaged 103\(\%\) full performance with 25\(\%\) annotations on all metrics.

**Cross-dataset Performance** To simulate the scenario where people tend to exploit a large unannotated dataset to benefit a small annotated dataset, we use the small annotated dataset MSVD (1,200 videos with 40 captions per video) and treat MSR-VTT as the large unannotated dataset, which includes 6,513 videos paired with 20 captions each. The results using the official SwinBERT implementation are provided in Tab. 1.

We report the overall performance on the MSVD test set. "Data Per." is the percentage of human annotations on MSR-VTT. We also report the performance of directly selecting 20\(\%\) of MSR-VTT (Row 4) and iteratively adding 5\(\%\) of MSR-VTT four times (Rows 5-8). As shown in the table, our AL algorithm significantly improves the overall performance of MSVD and is a more effective choice compared to random selection. Furthermore, directly selecting 20\(\%\) of data is slightly less effective than iterative selection, demonstrating the benefits of curriculum learning. Notably, the overall performance peaks at two iterations, or 10\(\%\) of human annotations on MSR-VTT, according to CIDEr and SPICE. Beyond this point, the performance saturates and slightly declines. This is expected, as 20\(\%\) of MSR-VTT includes 26K captions and at least 1.3K videos, which is comparable to the original training set of MSVD. Adding more data from a different dataset can degrade performance, as the training may deviate from the original dataset.

   Method & Data Per. & BLEU\_4 & METEOR & ROUGE\_L & CIDEr & SPICE \\  Starting Point & 0 & 55.71 & 39.70 & 75.73 & 109.39 & 6.97 \\  Random & 20 & 62.15 & 42.58 & 78.66 & 123.26 & 7.63 \\ Ours & 20 & 63.70 & 43.69 & 79.86 & 127.41 & 7.80 \\  Ours & 5 & 63.68 & 43.34 & 79.73 & 126.32 & 7.57 \\ Ours & +5 & 63.10 & 43.64 & 79.98 & 130.96 & 7.89 \\ Ours & +5 & 63.51 & 43.80 & 79.81 & 129.63 & 7.93 \\ Ours & +5 & 64.88 & 44.25 & 80.43 & 129.08 & 7.77 \\   

Table 1: Starting from fully-annotated MSVD training samples, exploiting data from MSR-VTT with our AL algorithm can further boost the performances on the MSVD test set.

Figure 3: Active learning performances on the MSVD. Ours significantly outperforms other methods.

### Ablation Study on Learnability, Uncertainty, and Diversity

To demonstrate the effectiveness of individual components of our design, we conduct a thorough ablation study by incrementally integrating various components into our active learning scheme. The steps are as follows: 1) \(L_{n}^{4}\) (uncertainty); 2) +\(L_{n}^{3}\) (diversity); 3) +\(L_{n}^{2}\) (learnability); 4) +\(L_{n}^{1}\) (learnability); 5) + Caption-wise Protocol (CP); Specifically, we report the Area-Under-Curve(AUC) score over CIDEr and SPICE curve in Tab. 2. Evidently, incorporating any of these components will enhance overall active learning performance, demonstrating their effectiveness. For example, our designed terms outperform Random Sampling significantly, indicating the effectiveness of our active learning scheme without CP. Notably, we observe a significant performance boost after integrating the CP, which is reasonable as it directly reduces the impact of collective outliers and improves overall diversity.

To further demonstrate that learnability, uncertainty, and diversity each reflect distinct aspects of active learning, we report the percentage of overlapped selections in \(S_{1}\) under active learning schemes focused on individual aspects, as shown in Fig. 4. As expected, our design targeting learnability, uncertainty, and diversity addresses various unlabelled samples, leading to limited overlap in their output \(S_{1}\). Together with Tab. 2, we conclude that all aspects are complimentary and mutually informative.

### More Analysis

**Are we celebrating from more annotations per video?** As summarized in Sec. 3.1, ground truth inconsistency is one of the main causes of collective outliers. We further showcase in Fig. 3 that fewer human annotations bring in better performance. Then a natural question to ask is whether we are celebrating from more annotations per video.

To answer this question, we conduct a caption-wise random sampling experiment with SwinBERT on MSR-VTT. Specifically, SwinBERT is trained with all training videos on MSR-VTT where each video is associated with \(k\{1,3,5,7,9\}\) captions that are randomly sampled from a full set of 20 human annotations. We visualize the CIDEr and SPICE score on the test set of MSR-VTT in Fig. 5. As shown in the figure, full performance is achieved with approximately 35\(\%\) of the annotations (equivalent to \(k\)=7). Overall performance improves with up to nine annotations. We argue that the performance degradation with the full training set is not due to overfitting, as techniques such as regularization, data augmentation, and early stopping have been applied to mitigate it. Instead, we hypothesize that it is related to collective outliers, where inconsistency increases with the number of annotations. Additionally, we observe that caption-wise random sampling performs worse than ours with the same amount of human annotations, highlighting the effectiveness of the \(L_{n}\) design.

**Can we truly reduce the impact of collective outliers?** Though we leverage the knowledge from LVLMs to approximate human annotations and thus identify collective outliers, it remains unknown how well such approximation or identification is. To address this, we first divide unlabelled samples into different groups w.r.t. their learnabilities, and then we obtain the distribution of \(S_{t}\) according to these groups. Ideally, our method should select fewer samples that belong to collective outliers. Specifically, we divide unlabelled samples into four discretized groups according to their \(x,y\) coordinates in Dataset Maps in Fig. 1. (b). Easy samples are those whose \(y>0.07\) and \(x<0.03\)

    & \)} & \)} \\  Random &.574 &.560 \\  \(L_{n}^{4}\) &.583 &.578 \\ \(+L_{n}^{3}\) &.589 &.585 \\ \(+L_{n}^{2}\) &.594 &.592 \\ \(+L_{n}^{1}\) &.603 &.600 \\  \(+\)CP (Ours) &.738 &.678 \\   

Table 2: Ablation study on MSVD.

Figure 4: The percentage of overlapped selections in \(S_{1}\).

Figure 5: The performance of SwinBERT when trained on samples equipped with different numbers \(k\) of annotation captions.

Moderate ones fall into the region with \(y>0.07\) and \(x 0.03\). For these samples whose \(y 0.07\) and \(x>0.17\), we call them hard samples. The remaining samples are then collective outliers.

We report the data distribution of \(S_{1}\) of all methods in Fig. 6. Again, results are obtained on MSR-VTT with SwinBERT as \(f\). First and foremost, we observe that our method is able to select the least collective outliers compared to other baselines, which fulfills our goal as expected. Another interesting observation is that our method prefers to select more easy samples at \(S_{1}\). It is an effective strategy to select easy and moderate samples for learning in the early stage of model training, which greatly improves the efficiency and effect of training. By avoiding collective outliers and more reliable samples, it's no wonder ours achieves SOTA performance. We refer the readers to the appendix for more analysis.

**Can we exploit the knowledge from LVLMs more?** Another way to utilize the predictions from LVLMs is to directly apply them as pseudo ground truths to update \(f\) in a semi-supervised learning setup. Specifically, this simple approach achieves 0.31 and 0.07 for CIDEr and SPICE on MSR-VTT, which are worse than the starting points. Adding additional control, _e.g_.including only videos when predictions from LVLMs are highly consistent, achieves 0.5185 and 0.0712 for CIDEr and SPICE, which is worse than our algorithm. We refer the readers to the appendix for more details.

**Are inconsistencies in ground truths our illusions?** To validate our hypothesis that inconsistency in human annotations genuinely exists and is not merely due to subjective judgments, we utilize ChatGPT Radford et al. (2019) as an objective tool. We observe that ChatGPT believes that 49\(\%\) of all captions are inconsistent on average. Such inconsistency rate increases when moving from easy to collective outlier sub-regions. More details can be found in the appendix.

**Does the reduction of human annotation costs justify the extensive computational cost?** Our additional computational costs arise from the active learning algorithm, primarily due to applying BLIP2 to the unlabelled dataset and generating scene graphs from its predictions. This process occurs only once on the unlabelled set. On our hardware, consisting of 4 RTX 4090 graphics cards with a power capacity of 2000 kWh, it takes no more than 9 minutes to run BLIP2 and 5 minutes for scene graph generation on the MSVD training dataset. In contrast, annotating the full dataset in 2010 required hundreds of annotators, around 2 months, and less than 5000 USD in total Chen and Dolan (2011b). Therefore, we argue that active learning is more efficient in terms of both time and cost.

**Limitations** There are several limitations that we believe are worth further exploration. Firstly, our paper only briefly touches on the relationship between curriculum learning and learnability. Beyond provoking the design of the learnability term, we believe curriculum learning can enhance the interpretability of learnability terms and even active learning. Secondly, we found that current evaluation metrics, such as CIDEr, do not always align with human evaluations. More human analysis is needed for video captioning tasks. Thirdly, we made some preliminary attempts to combine knowledge from LVLMs in a semi-supervised learning manner. Although we did not see a significant improvement, we believe further efforts are warranted. Lastly, our experiments with ChatGPT-4 can be improved with more refined designs. We will include these limitations in our final version.

## 5 Conclusion

In this work, we propose a novel active learning algorithm for video captioning tasks, which effectively leverages learnability, diversity, and uncertainty. To the best of our knowledge, our algorithm is the very first one that targets collective outliers in video captioning and further proposes to reduce their impacts by measuring sample learnability, as well as introducing a caption-wise protocol. Results on two datasets demonstrate the superiority of our algorithm over SOTA methods, _e.g_.we can achieve \(103\%\) of full performance with \(25\%\) of human annotations on MSR-VTT.

Figure 6: The distribution of samples in \(S_{1}\) on MSR-VTT.

Acknowledgement

This work was supported in part by the National Natural Science Foundation of China (No. 62125201, 62020106007).