# BayesDAG: Gradient-Based Posterior Inference for Causal Discovery

Yashas Annadani\({}^{}\)\({}^{*}\)\({}^{1,3,4}\)  Nick Pawlowski\({}^{2}\)  Joel Jennings\({}^{2}\)  Stefan Bauer\({}^{3,4}\)

**Cheng Zhang\({}^{2}\)  Wenbo Gong\({}^{*}\)\({}^{2}\)**

\({}^{1}\) KTH Royal Institute of Technology, Stockholm \({}^{2}\) Microsoft Research

\({}^{3}\) Helmholtz AI, Munich \({}^{4}\) TU Munich

Equal contribution. Work done during internship at Microsoft Research. Correspondence to wenbogong@microsoft.com

###### Abstract

Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on a combination of stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and Variational Inference (VI) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluation on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.

## 1 Introduction

The quest for discovering causal relationships in data-generating processes lies at the heart of empirical sciences and decision-making [56; 59; 68]. Structural Causal Models (SCMs)  and their associated Directed Acyclic Graphs (DAGs) provide a robust mathematical framework for modeling such relationships. Knowledge of the underlying SCM and its corresponding DAG permits predictions of unseen interventions and causal reasoning, thus making causal discovery - learning an unknown SCM and its associated DAG from observed data - a subject of extensive research [54; 60].

In contrast to traditional methods that infer a single graph or its Markov equivalence class (MEC) [14; 60], Bayesian causal discovery [21; 33; 65] aims to infer a posterior distribution over SCMs and their DAGs from observed data. This approach encapsulates the epistemic uncertainty, degree of confidence in every causal hypothesis, which is particularly valuable for real-world applications when data is scarce. It is also beneficial for downstream tasks such as experimental design [2; 4; 49; 64].

The central challenge in Bayesian causal discovery lies in inferring the posterior distribution over the union of the exponentially growing (discrete) DAGs and (continuous) function parameters. Prior works have used Markov Chain Monte Carlo (MCMC) to directly sample DAGs or bootstrap traditional discovery methods [14; 49; 65], but these methods are typically limited to linear models which admit closed-form marginalization over continuous parameters. Recent advances have begun to utilize gradient information for more efficient inference. These approaches are either: (1) DAG regularizer-based methods, e.g. DIBS , which use continuous relaxation of adjacency matrices together with DAG regularizer . But DIBS formulation fails to model edge co-dependencies and suffer from inference quality due to its inference engine (Stein variational gradient descent) [27; 29]. Additionally, all DAG regularizer based methods cannot guarantee DAG generation; (2) permutation-based DAG learning, which directly infers permutation matrices and guarantees to generate DAGs. However, existing works focus on using only variational inference [11; 16], which may suffer from inaccurate inference quality [28; 61; 66] and is sometimes restricted to only linear models .

In this work, we introduce BayesDAG, a gradient-based Bayesian causal discovery framework that overcomes the above limitations. Our contributions are:

1. We prove that an augmented space of edge beliefs and node potentials \((,)\), similar to NoCurl , permits equivalent Bayesian inference in DAG space without the need for any regularizer. (Section 3.1)
2. We derive an equivalence relation from this augmented space to permutation-based DAG learning which provides a general framework for gradient-based posterior inference. (Section 3.2)
3. Based on this general framework, we propose a scalable Bayesian causal discovery that is model-agnostic for linear and non-linear cases and also offers improved inference quality. We instantiate our approach through two formulations: (1) a combination of SG-MCMC and VI (2) SG-MCMC with a continuous relaxation. (Section 4)
4. We demonstrate the effectiveness of our approach in providing accurate Bayesian inference quality and superior causal discovery performance with comprehensive empirical evaluations on various datasets. We also demonstrate that our method can be easily scaled to \(100\) variables with nonlinear relationships. (Section 6)

## 2 Background

Causal Graph and Structural Causal ModelConsider a data generation process with \(d\) variables \(^{d}\). The causal relationships among these variables is represented by a Structural Causal Model (SCM) which consists of a set of structural equations  where each variable \(X_{i}\) is a function of its direct causes \(^{i}}}\) and an exogenous noise variable \(_{i}\) with distribution \(P_{_{i}}\):

\[X_{i} f_{i}(^{i}}},_{i})\] (1)

These equations induce a causal graph \(=(,)\), comprising a node set \(\) with \(||=d\) indexing the variables \(\) and a directed edge set \(\). If a directed edge \(e_{ij}\) exists between a node pair \(v_{i},v_{j}\) (i.e., \(v_{i} v_{j}\)), we say that \(X_{i}\) causes \(X_{j}\) or \(X_{i}\) is the parent of \(X_{j}\). We use the binary adjacency matrix \(\{0,1\}^{d d}\) to represent the causal graph, where the entry \(G_{ij}=1\) denotes \(v_{i} v_{j}\). A standard assumption in causality is that the structural assignments are acyclic and the induced causal graph is a DAG [9; 52], which we adopt in this work. We further assume that the SCM is causally sufficient i.e. all variables are measurable and exogenous noise variables \(_{i}\) are mutually independent. Throughout this work, we consider a special form of SCM called Gaussian additive noise model (ANM):

\[X_{i} f_{i}(^{i}}})+_{i}\ \ _{i}(0,_{i}^{2})\] (2)

If the functions are not linear or constant in any of its arguments, the Gaussian ANM is structurally identifiable [34; 55].

Bayesian Causal DiscoveryGiven a dataset \(=\{^{(1)},,^{(N)}\}\) with i.i.d observations, underlying graph \(\) and SCM parameters \(\), they induce a unique joint distribution \(p(,,)=p(|,)p(,)\) with the prior \(p(,)\) and likelihood \(p(|,)\). Under finite data and/or limited identifiability of SCM (e.g upto MEC), it is desirable to have accurate uncertainty estimation for downstream decision making rather than inferring a single SCM and its graph (for e.g. with a maximum likelihood estimate). Bayesian causal discovery therefore aims to infer the posterior \(p(,|)=p(,,)/p()\). However, this posterior is intractable due to the super-exponential growth of the possible DAGs \(\) and continuously valued model parameters \(\) in nonlinear functions. VI  or SG-MCMC [24; 45] are two types of methods developed to tackle general Bayesian inference problems, but adaptations are required for Bayesian causal discovery.

NoCurl CharacterizationInferring causal graphs is challenging due to the DAG constraint. Previous works [22; 25; 40; 43; 71] directly infer adjacency matrix with the DAG regularizer . However, it requires an annealing schedule, resulting in slow convergence, and no guarantees on generating DAGs. Recently,  introduced NoCurl, a novel characterization of the **weighted DAG** space. They define a potential \(p_{i}\) for each node \(i\), grouped as potential vector \(^{d}\). Further, a gradient operator on \(\) mapping it to a skew-symmetric matrix is introduced:

\[()(i,j)=p_{i}-p_{j}\] (3)

Based on the above operation, a mapping that directly maps from the augmented space \((,)\) to the DAG space \((,):^{d d}^{d}^{d  d}\) was proposed:

\[(,)=()\] (4)

where \(()\) is the ReLU activation function and \(\) is a skew-symmetric **continuously weighted** matrix. This formulation is complete (Theorem 2.1 in ), as any continuously weighted DAG can be represented by a \((,)\) pair and vice versa. NoCurl translates the learning of a single weighted DAG to a corresponding \((,)\) pair. However, direct gradient-based optimization is challenging due to a highly non-convex loss landscape, which leads to the reported failure in .

Although NoCurl appears suitable for our purpose, the failure in directly learning suggests non-trivial optimizations. We hypothesize that this arises from the continuously weighted matrix \(\). In the following, we introduce our proposed parametrization inspired by NoCurl to characterize the **binary** DAG adjacency matrix.

## 3 Sampling the DAGs

In this section, we focus on the Bayesian inference over binary DAGs through a novel mapping, \((,)\), a modification of NoCurl. We establish the validity of performing Bayesian inference within \((,)\) space utilizing \(\) (Section 3.1). However, \(\) yields uninformative gradient during backpropagation, a challenge we overcome by deriving an equivalent formulation based on permutation-based DAG learning, thereby enabling the use of relaxed gradient estimators (Section 3.2).

### Bayesian Inference in \(W,p\) Space

The NoCurl formulation (Equation (4)) focuses on learning _a single weighted_ DAG, which is not directly useful for our purpose. We need to address two key questions: (1) considering only binary adjacency matrices without weights; (2) ensuring Bayesian inference in \((,)\) is valid.

We note that the proposed transformation in NoCurl \(\) (Equation (4) can be hard to optimize for the following reasons: (i) \(()\) gives a fully connected DAG. The main purpose of \(\) matrix therefore is to disable the edges. Continuous \(\) requires thresholding to properly disable the edges, since it is hard for a continuous matrix to learn exactly \(0\) during the optimization; (ii) \(()\) and \(\) are both continuous valued matrices. Thus, learning of the edge weights and DAG structure are not explicitly separated, resulting in complicated non-convex optimizations2. Parameterizing the search space in terms of binary adjacency matrices significantly simplifies the optimization complexity as the aforementioned issues are circumvented. Therefore, we introduce a modification \(:\{0,1\}^{d d}^{d}\{0,1\}^{d d}\):

\[(,)=()\] (5)

where we abuse the term \(\) for binary matrices, and replace \(()\) with \(()\). \(\) acts as mask to disable the edge existence. Thus, due to the \(,\) can only output a binary adjacency matrix.

Next, we show that performing Bayesian inference in such augmented \((,)\) space is valid, i.e., using the posterior \(p(,|)\) to replace \(p(|)\). This differs from NoCurl, which focuses on a single graph rather than the validity for Bayesian inference, requiring a new theory for soundness.

**Theorem 3.1** (Equivalence of inference in \((,)\) and binary DAG space).: _Assume graph \(\) is a binary adjacency matrix representing a DAG and node potential \(\) does not contain the same values, i.e. \(p_{i} p_{j}\  i,j\). Then, with the induced joint observational distribution \(p(,)\), dataset \(\), and a corresponding prior \(p()\), we have_

\[p(|)= p_{}(,|)\,(=( ,))dd\] (6)

_if \(p()= p_{}(,)\,(=(,) )dd\), where \(p_{}(,)\) is the prior, \(()\) is the indicator function, and \(p_{}(,|D)\) is the posterior distribution over \(,\)._

Refer to Appendix B.1 for detailed proof.

This theorem guarantees that instead of performing inference directly in the constrained space (i.e. DAG space), we can apply Bayesian inference in a less complex \((,)\) space where \(\{0,1\}^{d d}\) and \(^{d}\) without explicit constraints.

For inference of \(\), we adopt a sampling-based approach, which is asymptotically accurate . In particular, we consider SG-MCMC (refer to Section 4), which avoids the expensive Metropolis-Hastings acceptance step and scales to large datasets. We emphasize that any other suitable sampling algorithms can be directly plugged in, thanks to the generality of the framework.

However, the mapping \(\) does not provide meaningful gradient information for \(\) due to the piecewise constant \(()\) function, which is required by SG-MCMC.

### Equivalent Formulation

In this section, we address the above issue by deriving an equivalence to a permutation learning problem. This alternative formulation enables various techniques that can approximate the gradient of \(\).

IntuitionThe node potential \(\) implicitly defines a topological ordering through the mapping \((())\). In particular, \(()\) outputs a skew-symmetric adjacency matrix, where each entry specifies the potential difference between nodes. \((())\) zeros out the negative potential differences (i.e. \(p_{i} p_{j}\)), and only permits the edge direction from higher potential to the lower one (i.e. \(p_{i}>p_{j}\)). This implicitly defines a sorting operation based on the descending node potentials, which can be cast as a particular \(\) problem  involving a permutation matrix.

Alternative formulationWe define \(\{0,1\}^{d d}\) as a matrix with lower triangular part to be \(1\), and vector \(=[1,,d]\). We propose the following formulation:

\[=()()^{T}\] where \[()=*{arg\,max}_{^{}_{d}}^{T}(^{})\] (7)

Here, \(_{d}\) represents the space of all \(d\) dimensional permutation matrices. The following theorem states the equivalence of this formulation to Equation (5).

**Theorem 3.2** (Equivalence to NoCurl formulation).: _Assuming the conditions in Theorem 3.1 are satisfied. Then, for a given \((,)\), we have_

\[=(\,)= ()()^{T}\]

_where \(\) is a DAG and \(()\) is defined in Equation (7)._

Refer to Appendix B.2 for details.

This theorem translates our proposed operator \((())\) into finding a corresponding permutation matrix \(()\). Although this does not directly solve the uninformative gradient, it opens the door for approximating this gradient with the tools from the differentiable permutation literature . For simplicity, we adopt the Sinkhorn approach , but we emphasize that this equivalence is general enough that any past or future approximation methods can be easily applied.

Sinkhorn operatorThe Sinkhorn operator \(()\) on a matrix \(\) is defined as a sequence of row and column normalizations, each is called Sinkhorn iteration.

 showed that the non-differentiable \(\) problem

\[=*{arg\,max}_{^{}_{d}}< ^{},>\] (8)

can be relaxed through an entropy regularizer with its solution being expressed by \(()\). In particular, they showed that \((/t)=_{^{}_{d}}< ^{},>+th(^{})\), where \(h()\) is the entropy function. This regularized solution converges to the solution of Equation (8) when \(t 0\), i.e. \(_{t 0}(/t)\). Since the Sinkhorn operator is differentiable, \((/t)\) can be viewed as a differentiable approximation to Equation (8), which can be used to obtain the solution of Equation (7). Specifically, we have

\[*{arg\,max}_{^{}_{d}}^{T}( ^{})=*{arg\,max}_{^{} _{d}}^{},^{T}=_{t  0}(^{T}}{t})\] (9)

In practice, we approximate it wth \(t>0\), resulting in a doubly stochastic matrix. To get the binary permutation matrix, we apply the Hungarian algorithm . During the backward pass, we use a straight-through estimator  for \(\).

Some of the previous works [11; 16] have leveraged the Sinkhorn operator to model variational distributions over permutation matrices. However, they start with a full rank \(\), which has been reported to require over **1000** Sinkho m iterations to converge . However, our formulation, based on explicit node potential \(^{T}\), generates a rank-1 matrix, requiring much fewer Sinkhorn steps (around **300**) in practice, saving two-thirds of the computational cost.

## 4 Bayesian Causal Discovery via Sampling

In this section, we delve into two specific methodologies that are derived from the proposed framework. The first one, which will be our main focus, combines SG-MCMC and VI in a Gibbs sampling manner. The second one, which is based entirely on SG-MCMC with continuous relaxation, is also derived, but we include its details in Appendix A due to its inferior empirical performance.

### Model Formulation

We build upon the model formulation of , which combines the additive noise model with neural networks to describe the functional relationship. Specifically, \(X_{i} f_{i}(_{^{}^{i}})+_{i}\), where \(f_{i}\) adheres to the adjacency relation specified by \(\), i.e. \( f_{i}()/ x_{j}=0\) if no edge exists between nodes \(i\) and \(j\). We define \(f_{i}\) as

\[f_{i}()=_{i}(_{j=1}^{d}G_{ji}l_{j}(x_{j})),\] (10)

where \(_{i}\) and \(l_{i}\) are neural networks with parameters \(\), and \(\) serves as a mask disabling non-parent values. To reduce the number of neural networks, we adopt a weight-sharing mechanism: \(_{i}()=(_{i},)\) and \(l_{i}()=l(_{i},)\), with trainable node embeddings \(_{i}\).

Likelihood of SCMThe likelihood can be evaluated through the noise \(=-(;)\).  showed that if \(\) is a DAG, then the mapping from \(\) to \(\) is invertible with a Jacobian determinant of 1. Thus, the observational data likelihood is:

\[p(|)=p_{}(-(;))=_{i=1}^ {d}p_{_{i}}(x_{i}-f_{i}(_{^{}_{O}}))\] (11)

Prior designWe implicitly define the prior \(p()\) via \(p(,)\). We propose the following for the joint prior:

\[p(,,)(;,) (;,)(;,) (-_{s}\|(,)\|_{F}^{2})\]

where \(\) controls the initialization scale of \(\) and \(_{s}\) controls the sparseness of \(\).

### Bayesian Inference of \(W,p,\)

The main challenge lies in the binary nature of \(\{0,1\}^{d d}\), which requires a discrete sampler. Although recent progress has been made , these methods either involve expensive Metropolis-Hasting (MH) steps or require strong assumptions on the target posterior when handling batched gradients. To address this, we propose a combination of SG-MCMC for \(,\) and VI for \(\). It should be noted that our framework can incorporate any suitable discrete sampler if needed.

We employ a Gibbs sampling procedure , which iteratively applies (1) sampling \(, p(,|,)\) with SG-MCMC; (2) updating the variational posterior \(q_{}(|,) p(|,,)\).

We define the posterior \(p(,|,)(-U(,,))\), where \(U(,,)=- p(,,,)\). SG-MCMC in continuous time defines a specific form of Ito diffusion that maintains the target distribution invariant  without the expensive computation of the MH step. We adopt the Euler-Maruyama discretization for simplicity. Other advanced discretization can be easily incorporated .

Preconditioning techniques have been shown to accelerate SG-MCMC convergence . We modify the sampler based on , which is inspired by Adam . Detailed update equations can be found in Appendix C.

The following proposition specifies the gradients required by SG-MCMC: \(_{,}U(,,)\).

**Proposition 4.1**.: _Assume the model is defined as above, then we have the following:_

\[_{}U=-_{} p()-_{} p(| ,(,))\] (12)

_and_

\[_{}U=-_{} p()-_{} p(|,(,))\] (13)

Refer to Appendix B.5 for details.

Variational inference for \(\)We use the variational posterior \(q_{}(|)\) to approximate the true posterior \(p(|,,)\). Specifically, we select an independent Bernoulli distribution with logits defined by the output of a neural network \(_{}()\):

\[q_{}(|)=_{ij}Ber(_{}()_{ij})\] (14)

To train \(q_{}\), we derive the corresponding _evidence lower bound_ (ELBO):

\[()=_{q_{}(|)}[ p(, ,|)]-D_{}[q_{}(| )\|p()]\,.\] (15)

where \(D_{}\) is the Kullback-Leibler divergence. The derivation is in Appendix B.6. Algorithm 1 summarizes this inference procedure.

SG-MCMC with continuous relaxationFurthermore, we explore an alternative formulation that circumvents the need for variational inference. Instead, we employ SG-MCMC to sample \(}\), a continuous relaxation of \(\), facilitating a fully sampling-based approach. For a detailed formulation, please refer to Appendix A. We report its performance in Appendix E.3, which surprisingly is inferior to SG-MCMC+VI. We hypothesize that coupling \(,\) through \(_{}\) is important since changes in \(\) results in changes of the permutation matrix \(()\), which should also influence \(\) accordingly during posterior inference. However, through sampling \(}\) with few SG-MCMC steps, this change cannot be immediately reflected, resulting in inferior performance. Thus, we focus only on the performance of SG-MCMC+VI for our experiments.

Computational complexityOur proposed SG-MCMC+VI offers a notable improvement in computational cost compared to existing approaches, such as DIBS . The computational complexity of

Figure 1: Graphical model of the inference problem.

our method is \(O(BN_{p}+N_{p}d^{3})\), where \(B\) represents the batch size and \(N_{p}\) is the number of parallel SG-MCMC chains. This former term stems from the forward and backward passes, and the latter comes from the Hungarian algorithm, which can be parallelized to further reduce computational cost. In comparison, DIBS has a complexity of \(O(N_{p}^{2}N+N_{p}d^{3})\) with \(N B\) being the full dataset size. This is due to the kernel computation involving the entire dataset and the evaluation of the matrix exponential in the DAG regularizer . As a result, our approach provides linear scalability w.r.t. \(N_{p}\) with substantially smaller batch size \(B\). Conversely, DIBS exhibits quadratic scaling in terms of \(N_{p}\) and lacks support for mini-batch gradients.

## 5 Related Work

Bayesian causal discovery literature has primarily focused on inference in linear models with closed-form posteriors or marginalized parameters. Early works considered sampling directed acyclic graphs (DAGs) for discrete [15; 46; 33] and Gaussian random variables [21; 65] using Markov chain Monte Carlo (MCMC) in the DAG space. However, these approaches exhibit slow mixing and convergence [18; 32], often requiring restrictions on number of parents . Alternative exact dynamic programming methods are limited to low-dimensional settings .

Recent advances in variational inference  have facilitated graph inference in DAG space, with gradient-based methods employing the NOTEARS DAG penalty . samples DAGs from autoregressive adjacency matrix distributions, while  utilizes Stein variational approach  for DAGs and causal model parameters.  proposed a variational inference framework on node orderings using the gumbel-sinkhorn gradient estimator . [17; 51] employ the GFlowNet framework  for inferring the DAG posterior. Most methods, except are restricted to linear models, while  has high computational costs and lacks DAG generation guarantees compared to our method.

In contrast, _quasi-Bayesian_ methods, such as DAG bootstrap , demonstrate competitive performance. DAG bootstrap resamples data and estimates a single DAG using PC , GES , or similar algorithms, weighting the obtained DAGs by their unnormalized posterior probabilities. Recent neural network-based works employ variational inference to learn DAG distributions and point estimates for nonlinear model parameters [11; 22].

## 6 Experiments

In this section, we aim to study empirically the following aspects: (1) posterior inference quality of BayesDAG as compared to the true posterior when the causal model is identifiable only upto Markov Equivalence Class (MEC); (2) posterior inference quality of BayesDAG in high dimensional nonlinear causal models (3) ablation studies of BayesDAG and (4) performance in semi-synthetic and real world applications. The experiment details are included in Appendix D.

Baselines.We mainly compare BayesDAG with the following baselines: Bootstrap GES (**BGES**) [14; 20], **BCD** Nets , Differentiable DAG Sampling (**DDS**) and **DIBS**.

### Evaluation on Synthetic Data

Synthetic data.We evaluate our method on synthetic data, where ground truth graphs are known. Following previous work, we generate data by randomly sampling DAGs from Erdos-Renyi (ER)  or Scale-Free (SF)  graphs with per node degree 2 and drawing at random ground truth parameters for linear or nonlinear models. For \(d=5\), we use \(N=500\) training, while for higher dimensions, we use \(N=5000\). We assess performance on 30 random datasets for each setting.

MetricsFor \(d=5\) linear models, we compare the approximate and true posterior over DAGs using Maximum Mean Discrepancy (MMD) and also evaluate the expected CPDAG Structural Hamming Distance (SHD). For higher-dimensional nonlinear models with intractable posterior, we compute the expected SHD (\(\)**-SHD**), expected orientation F1 score (**Edge F1**) and negative log-likelihood of the held-out data (**NLL**). Our synthetic data generation and evaluation protocol follows prior work . All the experimental details, including how we use cross-validation to select hyperparameters is in Appendix D.

#### 6.1.1 Comparison with True Posterior

Capturing equivalence classes and quantifying epistemic uncertainty are crucial in Bayesian causal discovery. We benchmark our method against the true posterior using a 5-variable linear SCM with unequal noise variance (identifiable upto MEC ). The true posterior over graphs \(p()\) can be computed using the BGe score . Results in Figure 2 show that our method outperforms DIBS and DDS in both ER and SF settings. Compared to BCD, we perform better in terms of MMD in ER but worse in SF. We find that BGES performs very well in low-dimensional linear settings, but suffers significantly in more realistic nonlinear settings (see below).

#### 6.1.2 Evaluation in Higher Dimensions

We evaluate our method on high dimensional scenarios with nonlinear relations. Our approach is the first to attempt full posterior inference in nonlinear models using permutation-based methods. Results for \(d=30\) variables in Figure 3 demonstrate that BayesDAG significantly outperforms other _permutation-based approaches_ and DIBS in most of the metrics. For \(d=50\), BayesDAG performs comparably to DIBS in ER but a little worse in SF. However, our method achieves better NLL on held-out data compared to most baselines including DIBS for \(d=30,50\), ER and SF settings. Only DDS gives better NLL for \(d=30\) ER setting, but this doesn't translate well to other metrics and settings. We additionally evaluate on \(d\{70,100\}\) variables (Table 1). We find that our method consistently outperforms the baselines with \(d=70\) and in terms of \(\)-SHD with \(d=100\). Full results are presented in Appendix E.2. Competitive performance for \(d>50\) in nonlinear settings further demonstrates the applicability and computational efficiency of the proposed approach. In contrast, the only fully Bayesian nonlinear method, DIBS, is not computationally efficient to run for \(d>50\).

  & \(d=70\) & \(d=100\) \\  BGES & 355.77 \(\) 18.02 & 563.02 \(\) 27.21 \\ BCD & 217.05 \(\) 9.58 & 362.66 \(\) 29.18 \\ DIBS & N/A & N/A \\ BaDAG & **143.70 \(\) 11.61** & **295.92 \(\) 24.67** \\ 

Table 1: \(\)-SHD (with \(95\%\) CI) for ER graphs in higher dimensional nonlinear causal models. DIBS becomes computationally prohibitive for \(d>50\).

Figure 2: Posterior inference on linear synthetic datasets with \(d=5\). Metrics are computed against the true posterior. \(\) denotes lower is better.

### Ablation Studies

We conduct ablation studies on our method using the nonlinear ER \(d=30\) dataset.

Initialized \(\) scaleFigure 3(a) investigates the influence of the initialized scale of \(\). We found that the performance is the best with \(=0.01\) or \(10^{-5}\), and deteriorates with increasing scales. This is because with larger initialization scale, the absolute value of the \(\) is large. Longer SG-MCMC updates are needed to reverse the node potential order, which hinders the exploration of possible permutations, resulting in the convergence to poor local optima.

Number of SG-MCMC chainsWe examine the impact of the number of parallel SG-MCMC chains in Figure 3(b). We observe that it does not have a significant impact on the performance, especially with respect to the \(\)-SHD and Edge F1 metrics.

Injected noise level for SG-MCMCIn Figures 3(c) and 3(d), we study the performance differences arising from various injected noise levels for \(\) and \(\) in the SG-MCMC algorithm (i.e. \(s\) of the SG-MCMC formulation in Appendix C). Interestingly, the noise level of \(\) does not impact the performance as much as the level of \(\). Injecting noise helps improve the performance, but a smaller noise level should be chosen for \(\) to avoid divergence from optima.

### Application 1: Evaluation on Semi-Synthetic Data

We evaluate our method on the SynTReN simulator . This simulator creates synthetic transcriptional regulatory networks and produces simulated gene expression data that approximates real experimental data. We use five different simulated datasets provided by  with \(N=500\) samples each. Table 2 presents the results of all the methods. We find that our method recovers the true network much better in terms of \(\)-SHD as well as Edge F1 compared to baselines.

### Application 2: Evaluation on Real Data

We also evaluate on a real dataset which measures the expression level of different proteins and phospholipids in human cells (called the Sachs Protein Cells Dataset) . The data corresponds to a network of protein-protein interactions of 11 different proteins with 17 edges in total among them. There are 853 observational samples in total, from which we bootstrap 800 samples of \(5\) different datasets. It is to be noted that this data does not necessarily adhere to the additive noise and DAG assumptions, thereby having significant model misspecification. Results in Table 2 demonstrate that our method performs well as compared to the baselines even with model misspecification, proving the suitability of the proposed framework for real-world settings.

Figure 3: Posterior inference of both graph and functional parameters on synthetic datasets of nonlinear causal models with \(d=30\) and \(d=50\) variables. BayesDAG gives best results across most metrics and outperforms other permutation based approaches (BCD and DDS). We found DDS to perform significantly worse in terms of \(\)-SHD and thus has been omitted for clarity. \(\) denotes lower is better and \(\) denotes higher is better.

## 7 Discussion

In this work, we propose BayesDAG, a novel, scalable Bayesian causal discovery framework that employs SG-MCMC (and VI) to infer causal models. We establish the validity of performing Bayesian inference in the augmented \((,)\) space and demonstrate its connection to permutation-based DAG learning. Furthermore, we provide two instantiations of the proposed framework that offers direct DAG sampling and model-agnosticism to linear and nonlinear relations. We demonstrate superior inference accuracy and scalability on various datasets. Future work can address some limitations: (1) designing better variational networks \(_{}\) to capture the complex distributions of \(\) compared to the simple independent Bernoulli distribution; (2) improving the performance of SG-MCMC with continuous relaxation (Appendix A), which currently does not align with its theoretical advantages compared to the SG-MCMC+VI counterpart.

Acknowledgements.The authors would like to thank Colleen Tyler, Maria Defante, and Lisa Parks for conversations on real-world use cases that inspired this work. YA and SB are thankful for the Swedish National Computing's Berzelius cluster for providing resources that were helpful in running some of the baselines of the paper. In addition, the authors would like to thank the anonymous reviewers for their feedback.