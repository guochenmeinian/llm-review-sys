# Efficient Adversarial Training in LLMs with Continuous Attacks

Sophie Xhonneux

Mila, Universite de Montreal

lpxhonneux@gmail.com&Alessandro Sordoni

Microsoft Research, Mila

alsordon@microsoft.com&Stephan Gunnemann

Technical University of Munich,

Munich Data Science Institute

s.guennemann@tum.de&Gauthier Gidel

Mila, Universite de Montreal

Canada AI CIFAR Chair

gidelgu@mila.quebec

Leo Schwinn

Technical University of Munich,

Munich Data Science Institute

l.schwinn@tum.de

###### Abstract

Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (CAT) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce CAPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.

## 1 Introduction

As large language models (LLMs) become increasingly integrated into various applications, ensuring their safety and robustness is crucial. The seminal work of Zou et al.  highlighted substantial vulnerabilities in even the most advanced proprietary models, demonstrating that adversarial attacks can effectively disable safety mechanisms. More recently, adaptive attacks have been shown to achieve nearly a \(100\%\) success rate on widely used models, underscoring the severity of this issue .

Adversarial training, which involves online augmenting the training data of a neural network with adversarial attacks, has consistently proven to enhance robustness against adversaries [3; 4]. Yet, initial attempts at adversarial training for LLMs have shown ineffective . Unlike _continuous_ adversarial training (AT) algorithms in other domains, AT for LLMs usually involves _discrete_ attacks, where tokens in the prompt are either substituted, injected, or appended as suffixes [1; 6]. Recently, Mazeika et al.  proposed R2D2, the first AT algorithm that successfully improves robustness against various attacks in LLMs. The authors use Greedy Coordinate Gradient (GCG) to generate discrete adversarial suffixes in natural language. However, GCG requires extensive computational resources, employing hundreds of thousands of model evaluations to compute a single attack. This leads to considerable overhead for R2D2 despite additional optimisations.

Continuous adversarial attacks have recently demonstrated higher success rates and significantly faster computation times than their discrete counterparts in LLMs [7; 8]. Moreover, continuous attacks have proven effective in adversarial training algorithms for encoder-decoder models, such as BERT [9; 10]. Thus, we argue that continuous attacks could be an efficient alternative to discrete attacks within LLM adversarial training algorithms. We ask the following research question:

_Does adversarial training with continuous attacks in the token embedding space of an LLM extrapolate and provide robustness to discrete natural language attacks?_

We positively answer this research question using two novel adversarial training algorithms. We propose CAT, an efficient continuous AT algorithm, combining training on an adversarial behaviour dataset with fine-tuning on utility data. We further introduce _continuous_ adversarial preference optimisation (CAPO), an adversarial variant of identity preference optimisation (IPO)  that does not require utility data for adversarial alignment. We surpass the robustness-utility trade-offs of the discrete R2D2 AT algorithm , achieving up to \(100\%\) attack robustness while requiring over \(299\) times less computing resources. Additionally, we identify a failure mode in previous evaluation protocols: the models are tested with their chat template for safety evaluations but without it for utility evaluations. This protocol is unrealistic as the chat template is not enabled or disabled based on the prompt the user enters. By enabling the chat template for standard queries, we demonstrate that R2D2 overfits the safety objective and grammar of the harmful dataset. Thus, it often refuses to respond to benign inputs, thereby hurting its usefulness. In contrast, models trained with CAT and CAPO show substantially fewer refusals.

## 2 Related Work

Adversarial AttacksAdversarial attacks and defenses have been extensively studied in the literature [1; 3; 4; 12; 13; 14; 15; 16; 17; 18; 19]. More recently, LLMs have been shown to be vulnerable to exploitation by adversarial attacks, and several threat models, such as suffix attacks  and jailbreaking , have been proposed. Zou et al.  present the Greedy Coordinate Gradient (GCG) suffix attack, which generates adversarial examples transferable from small open-source models to large proprietary

Figure 1: We propose continuous adversarial training (AT) to address the large computational requirements of existing discrete AT approaches . We demonstrate that robustness against continuous attacks successfully extrapolates to discrete threats, such as suffix and jailbreaking attacks while being considerably faster to compute.

models. Huang et al.  find that just varying generation strategies, such as adjusting decoding hyper-parameters and sampling methods, can trigger harmful behaviour in LLMs. Geisler et al.  introduce a novel discrete attack strategy that leverages continuous embedding space optimisation. In the area of continuous adversarial attacks, Fort  explore scaling laws for continuous adversarial attacks on language model activations. Further, Schwinn et al. [7; 8] showcase the potential of continuous adversarial attacks as a threat model to compromise safety alignment and unlearning.

An alternative threat model involves jailbreaks, a form of prompt engineering with the goal of circumventing safety alignment. Deng et al.  fine-tune an LLM with jailbreak examples and demonstrate that the fine-tuned LLM can generate strong attacks, which transfer between different models. Similarly, Chao et al.  found that LLMs could be leveraged to create jailbreaks for other LLMs, even without fine-tuning. They introduced the Prompt Automatic Iterative Refinement (PAIR) algorithm, which uses an attacker algorithm to iteratively query a target LLM, optimising the jailbreak prompt. Liu et al.  developed a hierarchical genetic algorithm to generate high-perplexity jailbreaks that can bypass the safety alignments of LLMs.

Adversarial TrainingPrevious work on _continuous_ adversarial training (AT) on token embeddings has mostly focused on encoder-decoder models, such as BERT [9; 10; 23; 24; 25; 26]. Jiang et al.  use adversarial attacks to promote smoothness in the embedding space of the model and show that this approach improves generalisation. Similarly, Zhu et al.  enforce invariance in the embedding space through adversarial attacks. He et al.  combine a disentangled attention mechanism with continuous AT and demonstrate improved generalisation for BERT and RoBERTa models on multiple downstream tasks. Other works apply continuous adversarial perturbation to word embeddings to increase performance in different NLP tasks [23; 25; 26]. Robey et al.  propose improving the robustness of autoregressive LLMs by a randomised smoothing-inspired approach.

Concurrent to this work, Casper et al.  use continuous attacks for the purpose of AT. They propose latent adversarial training (LAT), a method that finds perturbations in the network's hidden layer representations and applies them to several tasks including text generation. For text generation, they demonstrate that fine-tuning for desirable behaviour with LAT makes the model more likely to forget triggers from data poisoning in some cases. Contrary to our work, they set up the adversarial training in an untargeted manner, i.e. the attack they apply does not aim to produce a particular harmful output but uses the standard AT objective. In contrast, our work focuses on the challenge of making LLMs robust against discrete attacks and jailbreaks while maintaining their helpfulness. To do so, we propose novel algorithms and loss functions that make use of the harmful targets of discrete attacks. Moreover, we thoroughly evaluate across multiple benchmarks and adversarial attacks to ensure a good robustness-utility trade-off.

Adversarial Data AugmentationSeveral works [29; 18] have developed adversarial attack generators against LLMs and then used the generated adversarial attacks to create a dataset on which to perform supervised fine-tuning (SFT) to improve adversarial robustness. This kind of adversarial robustness training is based on dataset augmentation and does not adapt the model online to worst-case attacks. Thus, we consider these approaches orthogonal to our work.

## 3 Method

In this section, we introduce our adversarial training (AT) algorithms: Continuous-Adversarial UL (CAT) and Continuous-Adversarial IPO (CAPO). We begin by reviewing the standard AT regime from Madry et al.  (SS 3.1). We then explain differences between attacks in the standard AT setting and unique aspects of adversarial attacks in LLMs (SS 3.2). From there, we derive the Unlikelihood loss for--CAT (SS 3.3). Next, we introduce an adversarial IPO formulation--CAPO (SS 3.5). Finally, we discuss key design decisions in the above AT algorithm (SS 3.6).

### Adversarial Training

AT is generally defined as a minimax optimisation problem as follows :

\[_{}_{(x,y)}[_{ T(x)} (f_{}(x+),y)],\] (1)where \(\) is the loss function, \(f_{}\) is a neural network with parameters \(\), \(\) is the dataset, \(T(x)\) is the set of perturbations around \(x\) allowed by the threat model. In computer vision, \(x^{d}\) is an image, \(T(x)=\{_{p},\,x+ ^{d}\}\) and \(\) is a classification loss such as cross-entropy.

### Attack Perturbation Sets in LLMs

For LLMs with a token vocabulary \(\), \(x\) is a prompt and a common perturbation set \(T\) are discrete manipulations of the input space, such as suffix attacks . For suffix attacks, the set of acceptable perturbations \(\) is defined to be in the set of sequences of tokens of length \(m\) that can be appended to the input prompt. In other words, the adversarial attack \(x+\) is of the form \(x;\), where \(\) is a fixed number of tokens the attacker has full control over and ; means concatenation. However, computing the best \(\) from this perturbation set \(T_{}(x)=\{ x+^{n+m}\}\) is computationally expensive, as the optimisation turns into a discrete combinatorial problem with exponentially many solutions. Arguably, it is too expensive to use during training, especially for large datasets.

Thus, we propose a different perturbation set \(T\) based on continuous embedding attacks . This perturbation set allows the modification of the embeddings of the tokens in the prompt under some \(\)-ball as measured under the \(_{p}\) norm. \(E\) is a function from tokens \(v\) to embeddings \(E(v)^{k}\). We abuse notation and for a sequence \(x=v_{1};v_{2};;v_{n}\) we say that \(E(x)=E(v_{1});E(v_{2});;E(v_{n})\). Our perturbation set allows for a \(_{i}^{k}\) around each token embedding. Therefore, the modified prompt after the attack \(x+\) is \(E(v_{1})+_{1};;E(v_{n})+_{n}\), where \(^{n k}\) and \(T_{}(x)=\{ i.\,_{i} _{p},x+^{n k}\}\), as in the standard AT setting. Schwinn et al.  proposes to find the perturbation \(\) with signed gradient descent as in :

\[^{t+1}=^{t}+( f(y|x+ ^{t})).\] (2)

### Adversarial Training in LLMs

As described in Eq. 1, the inner loop of standard AT involves finding the worst-case perturbation by maximising the loss with respect to the ground truth prediction in an _untargeted_ way. In contrast, the goal of attacks on LLMs is to induce a specific harmful continuation \(\) given a harmful prompt \(x\). This exemplifies adversarial training under a _targeted attack_. Mazeika et al.  propose a loss that encourages the model to _i)_ increase the likelihood of a "safe" continuation \(y\) (e.g. "I am sorry,..."), and _ii)_ decrease the likelihood of the unsafe continuation \(\), given the targeted adversarial perturbation of \(x\). This yields:

\[_{}-_{(x,y,)}(y|x+(x,))}_{}-(|x+(x,))}_{},\] (3)

where \((x,)=*{arg\,min}_{^{} T(x)}(f(|x+^{}))\) is the targeted attack on \(x\). Contrary to standard AT , we are not maximising the loss of the safe answer, but specifically minimising towards a particular harmful continuation \(\). As discussed in the previous section, \(\) naturally depends on the choice of \(T,f,\), but we leave that out of the notation for clarity. Losses of the form of Equation 3 have been referred to as "unlikelihood" losses (UL) [30; 31]. Note that the dataset \(\) contains harmful prompts \(x\) under which we want to give a safe answer \(y\) rather than an unsafe answer \(\).

In addition to the two terms in Equation 3, Mazeika et al.  propose to add an additional loss term that maximises the utility of the model, i.e. given an utility dataset \(_{}\), it optimises:

\[_{}-_{(x,y,)}(y|x+(x,))}_{}-(|x+(x,))}_{}-_ {(x,y)_{}}(y|x)}_{ },\] (4)

Mazeika et al.  found this loss necessary to avoid degenerate behaviours such as refusing to answer all prompts by producing some often generic refusal answer \(y\).

### Continuous-Adversarial Unlikelihood

The primary difference between Mazeika et al.  and our method is the choice of perturbation set used during AT. Mazeika et al.  choose **discrete** suffix attacks \(T_{}\) and employ the GCG algorithm along with several tricks to mitigate the computational cost to find a GCG attack. Oneoptimisation they introduce is to only update the attack after every \(k\) training steps. In contrast, we employ \(T_{}\) with **continuous** attacks as introduced by Schwinn et al. , which are orders of magnitude (\( 299\)) more efficient (see Table 1). Consequently, we do not require any additional tricks to further reduce computational costs. In the Unlikelihood loss (Eq 3) we add cut-off values for the toward and away loss to prevent over-optimising either. Given a loss \(^{}\) before, we implement the cutoff as \(=[^{}>c]0.999c+([^ {}>c]0.001+[^{} c])^{}\), where \(c\) is the cutoff value chosen.

### Continuous-Adversarial IPO

Equation 3 has a similar form to DPO , which maximises the likelihood of a preferred answer while decreasing the likelihood of a dispreferred answer, given a prompt \(x\). This motivates us to present the following loss function, which we will call Continuous-Adversarial IPO (CAPO):

\[_{}-_{(x,y,)}[_{} ((y|x+(x,))}{f_{_{0}}(y|x)}- (|x+(x,))}{f_{_{0}}(|x)} )],\] (5)

where \(_{}(h)\) would be the \(( h)\) in the original DPO, but we use the loss proposed in Azar et al.  called IPO, i.e. \(_{}(h)=(h-)^{2}\), because it is less prone to overfitting. This loss implicitly minimises the Kullback-Leibler divergence w.r.t. the original model distribution \(f_{_{0}}(y|x)\), which prevents the model to collapse to degenerate behaviors leading to refuse all prompts with the refusal answer \(y\). As a result, we are able to omit the utility dataset for CAPO.

### Design Decisions

A few design decisions worth discussing are:

1. The adversarial attack in the toward loss optimises \(\) such that the harmful output \(\) becomes more likely. An alternative that we leave for future work would be to formulate the attack for the toward loss such that \(y\) becomes less likely, i.e. \((x,y)=_{^{} T(x)}-(f(y|x+^{}))\). It might even make sense to compute two separate attacks, one for \(y\) and one for \(\), and use them for the positive and negative cross-entropy loss terms, respectively. However, this would induce additional computational overhead.
2. Importantly, we do not use the attack \(\) on the input for the reference model (\(f_{_{0}}\) in Equation 5). Empirically we found that this makes training unstable in the DPO setting. We hypothesize that this is because the reference model represents roughly desirable log probability values of the safe answer \(y\). Note that the original DPO paper  reports a similar observation and proposes to do SFT on the chosen continuation \(y\) to make sure that these reference values are on-policy.
3. Mazeika et al.  suggests to optimise \((1-f_{}(|x+(x,)))\) instead of \(- f_{}(|x+(x,))\) for the away loss. We explored this and found that it yielded a considerably worse robustness/safety trade-off. We were unable to find a model that is robust and maintains some level of utility.

## 4 Experimental Details

The main goal of this paper is to assess if robustness against continuous attacks extrapolates to discrete attacks in natural language (see Figure 2). For additional hyperparameters see App. A.

DatasetsFor all AT experiments, we utilise the AT dataset from HarmBench  with the safe answer \(y\) always being Sorry, I can't do that. As a utility dataset for CAT, we employ UltraChat200k [32; 33], which has been successfully used in both the discrete AT algorithm Zephyr + R2D2  and general fine-tuning . For robustness evaluations, we use the first 40 samples of the HarmBench test set. Due to the substantial computational cost associated with LLM adversarial attacks, such as GCG , we limit our evaluation to these samples instead of the full test set.

Moreover, we measure the utility of trained models using common benchmarks, including MMLU , Arc-E and Arc-C , and MT-Bench . To reduce the computational demand, we evaluate \(100\) questions for each category for MMLU. Finally, we introduce Harmless which consists of 40 harmless queries (e.g. Tell me a story, see App. I for full list) that are written in the same grammatical style as the Harmbench behaviour. We query the models with their chat template and report the number of refusals (checked manually). Note that only MT-Bench and Harmless use the model's chat template.

ModelsIn our experiments, we adversarially fine-tuned four different open-source models Gemma , Phi-3-Mini , Mistral-7B , Zephyr-7B , and Llama2-7B  with increasing parameter counts--2B, 3.8B, 7B, 7B, and 7B, respectively. We chose instruction-tuned models for all of them. We additionally include Zephyr + R2D2 in our evaluations, which is the Mistral-7B base model fine-tuned with the R2D2 AT algorithm . This results in a diverse set of instruction-tuned models of different sizes. For more details, refer to App. A.2.

Continuous adversarial trainingWe investigate two novel continuous AT algorithms in this work CAT and CAPO. Due to the computational complexity of fine-tuning LLMs, we do not perform full model fine-tuning for both methods but use LoRA  on all linear layers of the transformer architectures. Additionally, we use \(4\)-bit quantization for all training runs to further reduce the memory overhead. We use \(_{2}\) norm perturbations and set the size of the attack \(\) relatively to the average magnitude of the token embeddings of the respective model. For all models, we use \(10\) attack iterations. We set \(=0.1\) for Gemma and Phi-3-Mini. For Mistral-7B, Llama-7B, and Zephyr-7B, we set \(=0.05\), \(=0.05\), and \(=0.075\), respectively. For a full list of AT hyperparameters, see App. A.1.

Robustness evaluationWe use three diverse adversarial attacks for the robustness evaluation. GCG, which has shown to achieve one of the highest average attack success rates (ASR) among other state-of-the-art attacks on several models . Since GCG is a suffix attack, we further use AutoDAN and PAIR, which generate more diverse jailbreaks. Finally, we also evaluate against Adaptive Attacks  and ICL  (see Table 5 and Table 6). Furthermore, PAIR has shown high ASR against previous AT approaches in LLMs . To evaluate the ASR, we use the harmfulness classifier from , which was shown to align well with human judgement.

Computational costGiven the constrained computational resources, we prioritised getting evidence to answer our main research question regarding the extrapolation of adversarial robustness. We want to emphasize that better trade-offs between utility and robustness might be obtained with more exhaustive hyperparameter search.

HardwareAll experiments were performed on an internal cluster of either V100, 40GB A100, or 80GB A100 GPUs. All conducted experiments required at least \(1904\) GPU hours.

## 5 Results

In the following, we illustrate the computational benefit of continuous AT compared to existing discrete methods. Subsequently, we show improved robustness against state-of-the-art discrete attacks by using continuous adversarial training (AT).

Why do we need continuous adversarial training?In Table 1, we compare the combined number of forward and backward passes used by the discrete AT algorithm RD2D  with CAT and CAPO. Computing a single adversarial example with R2D2 is \( 128.5\) times more expensive than for CAT and CAPO, while the whole training is \(299\) times more costly. This illustrates the considerable compute advantage of continuous AT approaches compared to discrete methods.

LLM adversarial training with utility dataWe first explore robustness extrapolation from continuous AT to discrete attacks for the CAT algorithm, which utilises additional utility data to maintain model performance. Figure 2 summarises the evaluation results. For all models, CAT

   Algorithm & R2D2 & CAT & CAPO \\  F/B & 2565/5 & 101/0 & 101/0 \\ Iterations & 2000 & 780 & 360 \\ Batch size & 256 & 64 & 64 \\ F/B (total) & 165,632,000 & 234,000 & 552,960 \\ Time (sec) & 1567.8 & 3.2 & 3.2 \\ Type & Discrete & Continuous & Continuous \\   

Table 1: The combined number of forward (F) and backward (B) passes to compute a single adversarial example for different AT types. The total number of F&B for the whole training and the number of training iterations and batch size are are shown. Time is the wallclock time for a single batch weight update (measured on 1 A100 with Mistral).

considerably increases the average robustness against discrete adversarial attacks. For the Gemma and Zephyr models, robustness increases for all attacks. For Phi-3-Mini and Mistral-7B, PAIR still achieves high attack success rates (ASR). In terms of utility, we observe similar degradations for all CAT trained models. All models still show considerable utility after fine-tuning.

Compared to the Zephyr + R2D2 model, which was trained with discrete AT, CAT exhibits marginally worse utility on standard utility benchmarks while providing substantially improved robustness against discrete attacks. For, Zephyr + R2D2, PAIR achieves an ASR of \(40\%\), while it achieves \(10\%\) ASR for CAT. We note a substantial difference in the Harmless benchmark, where CAT massively outperforms Zephyr + R2D2 showing that our method has not overfitted the safety objective or the patterns in the Harmbench behaviours. Note that the Harmless score of R2D2 demonstrates that it can not simultaneously achieve non-trivial utility and robustness, which are heavily dependent on not using or using the chat template, respectively.

LLM adversarial training without utility dataWe further investigate if adversarial variations of proven alignment methods, such as IPO, can be used to align models in an adversarially robust manner (see Figure 2). For this purpose, we fine-tune Gemma and Phi-3-Mini using the proposed CAPO algorithm. Figure 2, illustrates differences between the base model, CAT, and CAPO. Despite using no utility dataset within CAPO to retain helpfulness, the algorithm does not introduce larger utility decreases on common benchmarks than CAT. Moreover, CAPO achieves considerably higher robustness against the jailbreaking method PAIR, demonstrating generalisation to diverse threat models. The Phi-3-Mini-IPO model achieves \(100\%\) attack robustness for all conducted attacks.

Figure 2: **Trade-off between utility and robustness for CAT (Eq. 4), CAPO (Eq. 5), and R2D2 , compared to their non-adversarially fine-tuned models. The objective is a small loss in utility and a large improvement in attack robustness. Larger is better for MMLU, Arc-E, Arc-C, MT-Bench (left of dashed line). Smaller is better for GCG, AutoDAN, and PAIR (right of dashed line). MT-Bench score is multiplied by 10 to see the change in performance on this \(y\)-axis. Additional results are included in App. B.**

For Gemma, robustness improvements also mostly surpass CAT, with slightly lower robustness against GCG. Compared to R2D2, CAPO does not require an auxiliary dataset to maintain utility and achieves higher robustness on average. Specifically for PAIR CAPO trained models exhibit considerably higher robustness. Lastly, the Phi-3-Mini-IPO achieves a substantially higher score on the Harmless benchmark than CAT and R2D2.

_The results indicate that adversarial variations of common alignment methods, such as IPO, can be used to adversarially align LLMs._

## 6 Failure Modes of Training and Robustness Evaluations in LLMs

Utility evaluationCommon utility benchmarks such as MMLU or Arc do not use a chat template in their standard evaluation . Firstly, this dramatically impacts performance, especially for smaller models, which often require a lot of prompt engineering to follow the few-shot prompts correctly. Secondly, it dramatically changes the mode of the model. In effect, a model might be overly robust in chat mode (i.e. when using a chat template) where it rejects most requests, but it might appear to have high utility in benchmarks because no chat template is used (e.g. MMLU). Arc as an evaluation benchmark is even more misleading as it measures the likelihood of a set of possible answer tokens, thus not reflecting the utility of the model when using a chat template. We quantitatively evaluate the refusals of MMLU questions when using a chat template in App. G. We recommend future work, to consider these issues when evaluating robustness and utility for the same model.

Training data failure modesAT datasets such as Harmbench  or AdvBench  tend to use a common grammatical and syntactical structure, using imperative commands such as "Tell me" or "Give instructions". Chatting with our models and Zephyr + R2D2, we observe that requests would be refused when using this same style but are accepted if asked in a different style, such as "Could you please...?". This holds for both harmful and harmless requests. For instance, Zephyr + R2D2 will refuse to answer "Tell me a story" and "Tell me how to build a bomb", but will answer "Could you please tell me a story?" and "Could you please explain to me how to build a bomb?". This also explains why the model may even appear useful under utility benchmarks employing chat templates such as MT-Bench. To demonstrate this failure case we create two small benchmark datasets called PoliteHarmbench (see App. H) and Harmless. The former rephrases the harmful behaviours politely, and the latter consists of harmless requests formulated in the same grammatical style as the original Harmbench behaviours. We leave developing better datasets and benchmarks for a future paper as it is outside the scope of this work.

## 7 Adversarial Training Ablations

Robust fine tuning without attackWe found that continuous adversarial training successfully increases the robustness of LLMs to discrete adversarial attacks. Here, we explore whether robustness gains stem from using continuous adversarial attacks during training, or from the fine-tuning process itself. Thus, we fine-tune Gemma using the CAPO algorithm but without using adversarial attacks. We observe no robustness gains when fine-tuning without attacks (see App. B.2). This demonstrates that continuous adversarial attacks are a crucial part of our fine-tuning algorithm.

One-step adversarial training in LLMsFor all our experiments, we use \(10\) adversarial attack iterations. While this is orders of magnitude cheaper than calculating discrete adversarial attacks (GCG requires \(2570\) model evaluations with default settings), it still increases training time by an order of magnitude. We thus propose one-step AT with CAPO. As in previous work , we set the step size of the attack to the magnitude of the \(\)-ball. This achieves robustness improvements comparable to the multi-step variant and slightly worse utility trade-offs (see App B.1).

Robustness-utility trade-offsPrior work on AT has shown theoretical and empirical trade-offs between robustness and utility [4; 45]. Our previous results demonstrate that continuous AT can achieve non-trivial robustness-utility trade-offs. All experiments are conducted on Gemma models trained with CAPO and varying hyperparameters. Specifically, we sample \([0.00125,0.3]\), and \([0,0.5]\) and fine-tune \(7\) different models. In Figure 3(b), we depict the GCG loss of the trained models (as a proxy for robustness) on the \(y\)-axis in logarithmic scale against the MMLU score on the \(x\)-axis (as a proxy for utility). Clear trade-offs between robustness and utility can be observed, ranging from models with high robustness and no utility to models showing less robustness than the standard non-robust models and slightly higher utility.

Moreover, we analyse hyperparameter choices that affect the robustness-utility trade-off for CAPO in more detail. This includes the strength of the adversarial attacks defined by the \(\) magnitude and the IPO \(\) value. Figure 3 illustrates that for both hyperparameters, we obtain intuitive robustness-utility trade-offs, where larger epsilon values and smaller \(\) values are associated with increased robustness and reduced utility. A detailed analysis can be found in App C.

Correlation between continuous attack loss and GCG lossWe additionally investigated the relationship between training-time robustness to continuous adversarial attacks and inference-time robustness to discrete attacks. This is illustrated in Figure 3(a). The observed strong Pearson correlation (\(r=0.99\), \(p=0.0075\)) indicates that models robust to continuous attacks during training are also robust to discrete attacks at inference. This suggests continuous AT can be a reliable proxy for AT with discrete attacks. Thus, demonstrating the potential use of continuous attacks to reduce the computational burden of evaluating adversarial robustness .

## 8 Conclusion

We answer our research question about the extrapolation of robustness under the continuous attack threat model to robustness under discrete attacks in the affirmative. We propose an efficient continuous adversarial training algorithm (CAT), combining training on an adversarial behaviour dataset with fine-tuning on utility data. Additionally, we introduce an adversarial variant of IPO (CAPO) that does not require additional utility data. Our algorithms achieve up to \(100\%\) robustness against a set of state-of-the-art attacks (Phi-3-Mini-CAPO), surpassing robustness utility trade-offs in previous work  while requiring at least \(299\) times less compute. In future work, we will further analyse settings where continuous robustness does not extrapolate (e.g. novel attacks) and possible ways to address this, such as larger and more diverse training data. Additionally, the objectives of preventing harmful output and machine unlearning are closely related as such the applicability of our method for machine unlearning would be an interesting angle for further exploration.

We further show that great care is required in the evaluation of the robustness and utility of adversarially trained models. We demonstrate that previous work overfits the safety objective, refusing

Figure 4: Gemma-IPO used for both plots: (a) Correlation between GCG loss and continuous attack loss. (b) GCG loss vs MMLU score for a variety of \(\) and \(\) values.

Figure 3: Ablating how changing \(\) or \(\) affect GCG loss vs MMLU score on Gemma-IPO

to answer benign queries. Further, we exemplify that both the chat template and the grammatical structure of prompts need to be carefully controlled to prevent a misleading evaluation.

LimitationsOur method relies on the quality and breadth of the harmful dataset, while we are less prone to overfit than Zephyr + R2D2, we may still see improvements from augmented adversarial training datasets . An additional limitation is the number of hyperparameters introduced that require careful selection. We expect future work to achieve considerably better robustness-utility trade-offs through better hyperparameter selection alone. Furthermore, our proposed method CAT requires a utility dataset to retain helpfulness, which may shift the predictions of the model on unrelated tasks, a limitation we try to address with the CAPO method. Finally, due to limited compute we were not able to apply our method to much larger LLMs in the 70B parameter and larger regime, we leave this to future work.

Broader impactThis work aims to enable scalable adversarial training for LLMs to be robust against adversarial attacks. The positive impact is that this will reduce the amount of harmful content produced by LLMs if adopted as many attacks will no longer work. In addition, the lower computation cost should hopefully reduce the carbon footprint of training robust and safe LLMs. However, this may lead to overconfidence in the safety of LLMs, thus necessitating more extensive red teaming. Another possible negative impact of our work is that adversarial training may be used to prevent LLMs saying things the model operator does not want regardless of the harmfulness of the content. Our contributions on the failure modes of robustness evaluation should hopefully lead to more rigorous and trustworthy evaluation protocols. These are crucial to accurately assess the state of robustness in LLMs. Note, it may be that further failure modes exist we did not yet find.