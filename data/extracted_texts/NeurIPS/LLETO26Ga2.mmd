# Disentanglement via Latent Quantization

Kyle Hsu\({}^{}\) Will Dorrell\({}^{}\) James C. R. Whittington\({}^{@sectionsign}\) Jiajun Wu\({}^{}\) Chelsea Finn\({}^{}\)

\({}^{}\)Stanford University \({}^{}\)University College London \({}^{@sectionsign}\)Oxford University

{kylehsu,jiajunwu,cbfinn}@cs.stanford.edu {dorrellvec,jcrwhittington}@gmail.com

###### Abstract

In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards encoding to and decoding from an organized latent space. Concretely, we do this by (i) quantizing the latent space into discrete code vectors with a separate learnable scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the latent space design forces the encoder to combinatorially construct codes from a small number of distinct scalar values, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this approach by adding it to both basic data-reconstructing (vanilla autoencoder) and latent-reconstructing (InfoGAN) generative models. For reliable evaluation, we also propose InfoMEC, a new set of metrics for disentanglement that is cohesively grounded in information theory and fixes well-established shortcomings in previous metrics. Together with regularization, latent quantization dramatically improves the modularity and explicitness of learned representations on a representative suite of benchmark datasets. In particular, our quantized-latent autoencoder (QLAE) consistently outperforms strong methods from prior work in these key disentanglement properties without compromising data reconstruction.

Figure 1: Motivating observations and illustration of the inductive bias of latent quantization. The mapping between true sources (c) and data (b) is simpler than most other possible generative functions from unstructured spaces (a). To help recover the sources, we use quantized latent codes (d)—continuous codes (dots) are mapped (via arrows) to their nearest discrete codes (black circles), each of which is constructed combinatorially from per-dimension scalar codebooks (turquoise and orange ticks). One way this design’s inductive bias manifests is in how the codes change from previous values (grayed) as a result of codebook optimization: the combinatorially defined codes move in lockstep, changing the quantized representations of many datapoints. In contrast, naively quantizing into individual vector embeddings (e) would result in codebook optimization having a comparatively local effect.

Introduction

Our increasing reliance on black-box methods for processing high-dimensional data underscores the importance of developing techniques for learning human-interpretable representations. To name but a few possible benefits, such representations could foster more informed human decision-making [62; 27], facilitate efficient model debugging and improvement [16; 44], and streamline auditing and regulation [51; 14]. In this context, disentangled representation learning serves as a worthwhile scaffolding: loosely speaking, its goal is for a model to tease apart a dataset's underlying sources of variation and represent them independently of one another.

Accomplishing this, however, has proven difficult. Conceptually, the field lacks a formal problem statement that resolves fundamental ambiguities [30; 47] without overly restrictive assumptions (reviewed in Section 6); methodologically, evaluation metrics have been found to be sensitive to hyperparameters, ad hoc, and/or sample inefficient ; and empirically, there remains a need for an inductive bias that enables consistently good performance in the purely unsupervised setting.

In this work, we answer the call for a better inductive bias for disentanglement. Our solution is motivated by observing that many datasets of interest are generated from their sources in a compositional manner, which entails a neatly organized source space (Figure 1). This distinguishing property of realistic generative processes applies to real world physics as well as human approximations thereof (e.g., rendering). Hence, our broad strategy to uncover the true underlying sources is to bias the model towards encoding to and decoding from a similarly structured latent space.

We manifest this inductive bias by drawing from two common ideas in the machine learning literature: discrete representations  and model regularization. Specifically, we propose (i) quantizing a model's latent representation into learnable discrete values with a separate scalar codebook per dimension and (ii) applying strong regularization via an unusually high weight decay . Intuitively, forcing the model to use a small number of scalar values to combinatorially construct many latent codes encourages it to assign a consistent meaning to each value, an outcome that weight decay explicitly incentivizes by regularizing towards parsimony.

A side benefit of models with quantized latents is that they sidestep one issue that has hindered evaluation in previous works: they enable the use of simpler, more robust distribution estimation techniques for discrete variables. As a further methodological contribution, we present InfoMEC, a new set of metrics for the modularity, explicitness, and compactness of (both continuous and discrete) representations that is cohesively grounded in information theory and fixes other well-established shortcomings in existing disentanglement metrics.

We demonstrate the broad applicability of latent quantization by adding it to both basic data-reconstructing (vanilla autoencoder) and latent-reconstructing (InfoGAN) generative models. Together with regularization, this is sufficient to dramatically improve the modularity and explicitness of the learned representations of a representative suite of four disentangled representation learning datasets with image observations and ground-truth source evaluations [9; 20; 53]. In particular, our quantized-latent autoencoder (QLAE, pronounced like clay) consistently outperforms strong methods from prior work without compromising data reconstruction. We think of QLAE as a minimalist implementation of a combinatorial representation in neural networks, suggesting that our recipe of latent quantization and regularization could be broadly useful to other areas of machine learning.1

## 2 Preliminaries

In order to properly contextualize our proposed inductive bias, methodological contributions, and experiments, we first devote some attention to explaining the problem of disentangled representation learning. We also discuss prior disentangled representation learning methods we build upon.

### Nonlinear ICA and Disentangled Representation Learning

We begin by considering the standard data-generating model of nonlinear independent components analysis (ICA) , a problem very related to but more conceptually precise than disentanglement:

\[p()=_{i=1}^{n_{s}}p(_{i}),\ \ =g(),\] (1)where \(=(_{1},,_{n_{s}})\) comprise the \(n_{s}\) mutually independent source variables (sources); \(\) is the observed data variable; and \(g:\) is the nonlinear data-generating function. The nonlinear ICA problem is to recover the underlying sources given a dataset \(\) of samples from this model. Specifically, the solution should include an approximate inverting function \(^{-1}:\) such that, assuming latent variables (latents) \(=(_{1},,_{n_{s}})\) are matched correctly to the sources, each source is perfectly determined by its corresponding latent. A typical technical phrasing is that \(^{-1} g\) should be the composition of a permutation and a dimension-wise invertible function.

As stated, this problem is _nonidentifiable_ (or underspecified). Given \(\), one may find many sets of independent latents (and associated nonlinear generators) that fit the data despite being non-trivially different from the true generative sources . As such, reliably recovering the true sources from the data is impossible. Much recent work in nonlinear ICA has focused on proposing additional problem assumptions so as to provably pare down the possibilities to a unique solution. These theoretical assumptions can then be transcribed into architectural choices or regularization terms. Such approaches have shown promise in increasing our understanding of what assumptions are required to disentangle; we review these works in Section6.

While identifiability is conceptually appealing, achieving it under sufficiently generalizable assumptions that apply to non-toy datasets has proven hard. The field of disentangled representation learning has taken a more pragmatic approach, focusing on empirically evaluating the recovery of each dataset's designated source set. Given this empirical focus, robust performance metrics are vital. Unfortunately, there is a plethora of approaches in use, and subtle yet impactful issues arise even in the most common choices . To address these concerns, in Section4 we propose new metrics for three existing, complementary notions of disentanglement [17; 59]. They measure the following three properties: **modularity**--the extent to which sources are encoded into disjoint sets of latents; **explicitness**--how _simply_ the latents encode each source; and **compactness**--the extent to which latents encode information about disjoint sets of sources. We frame these three in a cohesive information-theoretic framework that we name InfoMEC.

The disentangled representation learning problem statement considered in this work is as follows. Given a dataset of paired source-data samples \(\{(s,x=g(s))\}\) from the nonlinear ICA model (1), learn an encoder \(^{-1}:\) and decoder \(:\) solely using the data \(\{x\}\) such that (i) the InfoMEC as estimated from samples \(\{(s,z=^{-1} g(s))\}\) from the joint source-latent distribution is high, while (ii) maintaining an acceptable level of reconstruction error between \(x\) and \(^{-1}(x)\).

### Autoencoding and InfoGAN as Data and Latent Reconstruction

We will apply our proposed latent quantization scheme to two foundational approaches for disentangled representation learning: vanilla autoencoders (AEs) and information-maximizing generative adversarial networks (InfoGANs) . Here, we provide a brief overview of the two and defer complete implementation details to AppendicesA andC. Both approaches involve learning an encoder \(^{-1}\) and decoder \(\). An autoencoder takes a datapoint \(x\) as input and produces a reconstruction \(^{-1}(x)\) that is optimized to match the input:

\[_{}(^{-1},;):= _{x}[- p(x^{-1}(x)) ].\] (2)

An InfoGAN instead takes a latent code \(z\) as input. The decoder (aka generator) maps \(z\) to the data space, and from this the encoder produces a reconstruction of the latent:

\[_{}(^{-1},):=_{z  p()}[- p(z^{-1}(z))].\] (3)

Unlike the data reconstruction loss, this is clearly insufficient for learning as the dataset \(\) isn't even used. InfoGAN can be thought of as grounding latent reconstruction by making the marginal distribution of generated datapoints, \(()\), indistinguishable from the empirical data distribution. A concrete measure of this is provided by an additional binary classifier (aka discriminator)  or value model (aka critic)  trained alongside but in opposition to the decoder. While InfoGAN was originally motivated as maximizing a variational lower bound on the mutual information between the latent and the generated data, we find the above interpretation to be unifying.

## 3 Latent Quantization

Our goal is to encourage our model to disentangle by biasing it towards using an organized latent space. Why would this mitigate the nonidentifiability of nonlinear ICA? Our key motivation is that generative processes for realistic data are compositional and hence necessarily use highly organized source spaces. We discuss connections to related works in Section 6.

To build our desired inductive bias, we propose latent quantization, a modification of vector quantization . In the latter, the latent representation of a datapoint is partitioned into component vectors, each of which is quantized to the nearest of a discrete set of vector embeddings (Figure 1(a)). We modify vector quantization to better enable the model to encode and decode with a consistent interpretation of each component of the discrete code. Concretely, we do this by specifying the set of latent codes to be the Cartesian product of \(n_{z}\) distinct scalar codebooks: \(Z=V_{1} V_{n_{z}}\), where each codebook \(V_{j}\) is a set of \(n_{v}\) reals. The complete transformation comprises, first, the encoder network mapping the data to a continuous latent space \(^{-1}:^{n_{z}}\) (with a slight abuse of notation), followed by quantization onto the nearest code (Figure 1(b)). This nearest neighbor calculation can be done elementwise, which is highly efficient. Formally, latent quantization is:

\[z_{j}=*{arg\,min}_{v_{jk} V_{j}}|^{-1}(x)_{j}-v_{jk}|, \ \ j=1,,n_{z}.\] (4)

We represent \(Z\) by its constituent discrete values, concretely as a learnable two-dimensional array \(^{n_{z} n_{v}}\), the \(j\)-th row of which stores the elements of codebook \(V_{j}\) in an arbitrary order.

A lesson from nonlinear ICA is that, given a flexible enough model, data can be mapped to and from latent spaces in many convoluted ways. We motivate the use of strong model regularization with the conjecture that, of all the possible mappings from organized latent space to data, the most parsimonious will be the true generative model or something close enough to it. We operationalize this by using a high weight decay on both the encoder and decoder networks. Ablation studies (Section 5) show that both quantized latents and weight decay are necessary to disentangle well.

To train a quantized-latent model, we use the straight-through gradient estimator  and co-opt the quantization and commitment losses proposed for vector quantization:

\[_{}=\|(^{-1}(x))-z\|_{2}^ {2},_{}=\|^{-1}(x)-(z )\|_{2}^{2}.\] (5)

The straight-through gradient estimator facilitates the flow of gradients through the nondifferentiable quantization step. \(_{}\) pulls the discrete values constituting \(z\) onesidedly towards the pre-quantized continuous output of the encoder. This is needed to optimize \(\), since straight-through gradient estimation disconnects \(\) from the computation graph. Conversely, the commitment loss prevents the pre-quantized representation, which does see gradients from downstream computation, from straying too far from the codes. While this is a significant failure mode for vector quantization, we find that the use of scalars instead of high-dimensional vectors alleviates this issue, allowing us to drastically downweight the quantization and commitment losses while maintaining training stability. This gives the model much-needed flexibility to reorganize the discrete latent space. Finally, while using a shared global codebook like in vector quantization is certainly feasible, we find it better to maintain dimension-specific codebooks to enable the stable optimization of each individual value. Algorithm 1 contains pseudocode for latent quantization and computing the quantization and

Figure 2: Two ways to quantize into one of \({n_{v}}^{n_{z}}\) (\(5^{2}\)) discrete codes. (a) Vector quantization  splits a continuous representation of size \(n_{z}d\) (4) into \(n_{z}\) parts (black dots), each of which is quantized to the nearest of a global codebook of \(n_{v}\) vectors of size \(d\) (turquoise circles). (b) Latent quantization specifies \(d=1\) and quantizes a continuous representation of size \(n_{z}\) (black dot) onto a regular grid parameterized by dimension-specific codebooks (orange and turquoise ticks), each of size \(n_{v}\). Unlike vector quantization, latent quantization ties the decoder input space to the discrete code space.

commitment losses. Appendix A presents pseudocode for training a quantized-latent autoencoder (QLAE) in Algorithm 2 and a quantized-latent InfoWGAN-GP [12; 1; 23] in Algorithm 3.

```
1:function LatentQuantization(datapoint \(x\), encoder \(^{-1}\), discrete value array V)
2:\(z_{c}^{-1}(x)\)
3:\(z_{v Z} z_{c}-v_{1},\;\;v_{j} V_{j},\; \;_{j=1}^{n_{z}}V_{j}=\)\(\) implement via (4)
4:\(_{}(z_{c})-z _{2}^{2}\)\(\) from VQ-VAE 
5:\(_{} z_{c}-(z) _{2}^{2}\)\(\) ibid.
6:\(z z_{c}+(z-z_{c})\)\(\) straight-through gradient estimator 
7:return\(z,_{},_{}\) ```

**Algorithm 1** Latent quantization and computation of codebook losses.

## 4 InfoMEC: Information-Theoretic Metrics for Disentanglement

In this section, we derive InfoMEC, metrics for modularity, explicitness, and compactness, building upon and otherwise taking inspiration from several prior works [58; 17; 11; 34; 72; 45; 10]. We take care to motivate our design decisions, and while we do not expect this to be the final word on disentanglement metrics, we hope our presentation enables others to clearly understand InfoMEC and propose further improvements.

### Modularity and Compactness

Nonlinear ICA asks for the latents to recover the sources up to a permutation and dimension-wise invertible transformation. The mutual information between an individual source and latent,

\[I(_{i};_{j}):=D_{}(p(_{i},_{ j}) p(_{i})p(_{j})),\] (6)

is a granular measure of the extent to which they are deterministic functions of each other. Unlike other measures such as correlation (used in MCC ), LASSO weights (used in linear DCI ), or linear predictive accuracy (used in SAP ), mutual information takes into account arbitrary nonlinear dependence between its two arguments, making it invariant within the nonlinear ICA equivalence class for any candidate solution.

When both arguments are discrete, estimating the mutual information is simple via the empirical joint distribution, but if either is continuous, estimation becomes non-trivial. Previous works bin a continuous variable and pretend it is discrete [47; 10], but this is sensitive to the binning strategy . Instead, for evaluating continuous latents, we choose the celebrated \(k\)-nearest neighbor based KSG estimator [40; 19], in particular a variant  designed to handle a mix of discrete and continuous arguments. We use \(k=3\). See Appendix B for experimental vignettes demonstrating the severe sensitivity of binning-based estimation to the binning strategy (Figure 5) and the robustness of KSG-based estimation to \(k\) (Figure 6). We remark that latent quantization enables reliable evaluation using the discrete-discrete estimator.

To facilitate aggregation, we desire a normalization to the interval \(\). To this end, note that the identity \(I(_{i};_{j})=H(_{i})-H(_{i} _{j})\) and the nonnegativity of entropy imply \(I(_{i};_{j}) H(_{i})\) for discrete sources. Following , we define a normalized mutual information as

\[(_{i},_{j}):=_{i};_{j})}{H(_{i})}.\] (7)

We prefer this normalization scheme over others  since i) it is the proportion of a source's entropy reduced by conditioning on a latent and thus scales consistently to \(\) for any model, and ii) it avoids the scale-dependent (and possibly negative) differential entropy of a continuous latent. We gather all evaluations of \((_{i},_{j})\) into a 2-dimensional array \(^{n_{z} n_{z}}\). We remove inactive latents (columns of \(\)), which are those with zero range (over the evaluation sample) for discrete latents. For continuous latents, zero is too strict, so we heuristically define the threshold to be \(}{{20}}\), applied after dividing the ranges by their maximum. See Figure 3 for examples of \(^{}\).

Modularity is the extent to which sources are separated into disjoint sets of latents. Perfect modularity occurs when each latent is informative of only one source, i.e. when every column of \(\) has only one nonzero element. This has been measured as the _gap_ between the two largest entries in a column, or the _ratio_ of the largest entry in the column to the column sum. We prefer the ratio since the gap is agnostic to the smallest \(n_{s}-2\) values in the column, but these values matter and should influence the measure . Since the possible range of values for this ratio is \([}{{n_{s}}},1]\), we re-normalize to \(\). Finally, we define InfoModularity (InfoM) as the average of this quantity over latents:

\[:=(}_{j=1}^{n_{s}}_{ij}}{_{i=1}^{n_{s}}_{ij}}-}) (1-}).\] (8)

Compactness complements modularity; it is the extent to which latents only contain information about disjoint sets of sources. We therefore define InfoCompactness (InfoC) analogously to InfoM, but considering rows of \(\) instead of columns, and averaging over sources instead of latents, etc.:

\[:=(}_{i=1}^{n_{s}}_{ij}}{_{j=1}^{n_{s}}_{ij}}-}) (1-}).\] (9)

We advocate for this terminology since previous names such as "mutual information gap"  and "mutual information ratio"  are ambiguous, and indeed the former of these works considered solely compactness and the latter solely modularity, with neither mentioning the distinction. We remark that when \(n_{z}>n_{s}\) (after pruning inactive latents), it is impossible to achieve both perfect modularity and perfect compactness. Of the two, modularity should be prioritized [58; 10] and indeed has been referred to as disentanglement itself .

### Explicitness

Modularity and compactness are measured in terms of mutual information, so they are agnostic to _how_ this information is encoded. Our third metric, explicitness, measures the extent to which the relationship between the sources and latents is simple (e.g., linear [41; 17; 59; 18]). Since previous explicitness metrics have been rather ad hoc, we propose a formalism using the framework of predictive \(\)-information, a generalization of mutual information that specifies an allowable function class, denoted \(\), for the computation of information . We first estimate the predictive \(\)-information of each source \(_{i}\) given _all_ latents \(\):

\[I_{}(_{i}):=H_{}( _{i})-H_{}(_{i}).\] (10)

This requires estimating the predictive conditional \(\)-entropy

\[H_{}(_{i}):=_{f}_{s p(),z p(|)}[- p(s_{i} f(z))]\] (11)

and the marginal \(\)-entropy of the source

\[H_{}(_{i}):=_{f}_{s p()}[- p(s_{i} f())],\] (12)

Figure 3: Visualization of \(^{}\) for Shapes3D models. Inactive latents (red font) are removed from InfoM and InfoC computation. The low values in the \(^{}\) of AE indicate that individual latents are not highly informative of individual sources. Adding latent quantization results in QLAE achieving near-perfect InfoM (\(0.99\)): each latent is only informative of one source (rows are sparse). This induces a trade-off with InfoC, in which \(\)-TCVAE scores higher (\(0.53\) vs. \(0.62\)). See Appendix D for qualitative studies on the fidelity of \(\) entries as judged by decoded latent interventions.

where \(\) is an uninformative constant. The predictive conditional \(\)-entropy measures how well a source, \(_{i}\), can be predicted by mapping the latents, \(\), through a function in function class \(\). Note that this estimation uses the best _in-sample_ negative log likelihood. We choose \(\) to be the space of linear models (though one could pick \(\) to fit particular needs) and so use logistic regression (linear regression) for discrete (continuous) sources. We use no regularization. We compute the marginal \(\)-entropy \(H_{}(_{i})\) in the same way, but substituting a universal constant for all inputs. We propose a simple normalization analogous to the one done for \(\):

\[_{}(_{i}):=}(_{i})}{H_{}(_{i})},\] (13)

which can be interpreted as the relative reduction in the \(\)-entropy of a source achieved by knowing the latents, and is in \(\): for classification negative log likelihood is the cross-entropy, and for regression we leverage Propositions 1.3 and 1.5 from Xu et al.  to argue that \(_{}(_{i})=R^{2}\), the coefficient of determination. We can now compute explicitness as:

\[:=}_{i=1}^{n_{s}}_{}( _{i}).\] (14)

### Summary and Comparison to Nonlinear DCI

We have derived three metrics for evaluating the modularity, explicitness, and compactness of a representation. Each metric has a straightforward information-theoretic interpretation and all share a range of \(\). We order them in decreasing importance and collectively refer to them as InfoMEC \(:=(,,)\).

Nonlinear DCI , a widely used three-pronged framework that measures similar disentanglement properties, suffers several practical drawbacks in comparison to InfoMEC from being defined in terms of relative counts of decision tree splits: determining this requires considering all latents _jointly_ while fitting \(p(_{i})\). This results in a cumbersome computational footprint that is exacerbated by highly sensitive hyperparameters such as tree depth [17; 10], the tuning of which has even seen omission in prior work . These drawbacks worsen with increased latent space dimensionality. In contrast, InfoMEC avoids these issues as it isolates InfoM and InfoC from the choice of predictive function class and only computes pairwise interactions between individual sources and latents. (InfoE also fits \(p(_{i})\), but does so with function classes of severely limited capacity for which fitting procedures scale well.) See Appendix B for experimental vignettes demonstrating the hyperparameter sensitivity of nonlinear DCI (Figure 7) and the robustness of InfoMEC (Figure 6).

## 5 Experiments

**Experimental design.** We design our experiments to answer the following questions: Does latent quantization improve disentanglement? How does it compare against the strongest known methods that operate under the same assumptions? And, finally, which of our design choices were critical? We benchmark on four established datasets: Shapes3D , MPI3D , Falcor3D , and Isaac3D . Each consists of RGB image observations generated (near-)noiselessly from categorical or discretized numerical sources. Shapes3D is toyish, but the others are chosen for their difficulty [54; 20]. In particular, we use the complex shapes variant of MPI3D collected on a real world robotics apparatus. See Appendix C.1 for further dataset details. Aside from baseline AE and InfoGAN (specifically InfoWGAN-GP, a Wasserstein GAN  with gradient penalty ), we compare to \(\)-VAE , \(\)-TCVAE , and BioAE , the strongest methods from prior work that obey our problem assumptions and make design decisions mutually exclusive with latent quantization. We also compare to VQ-VAE with \(d=64\) and \(n_{v}=512\) (Figure 2). We ablate weight decay, scalar codebooks, and dimension-specific codebooks from QLAE and weight decay from QLIinfoWGAN-GP. We quantify modularity, explicitness, and compactness using both InfoMEC and nonlinear DCI. We qualitatively inspect representations via decoded latent interventions (Figure 4 and Appendix D).

**Select experimental details.** The choice of decoder architecture is known to impose inductive biases relevant for disentanglement [33; 43]. We use an expressive architecture (Appendix C.3) based on StyleGAN [33; 54] for all methods and datasets. We downsample the observations to \(64 64\) (if necessary). We follow prior work  in considering a statistical learning problem rather than a machine learning one: we train on the entire dataset then evaluate on \(10{,}000\) i.i.d. samples. We fix the number of latents in all methods to twice the number of sources. For quantized-latent models, we fix \(n_{v}=10\) discrete values per codebook. We tune one key regularization hyperparameter per method per dataset with a thorough sweep (Table 11, Appendix C.2). We use the best performing configurations over 2 seeds and rerun with 5 more seeds. Despite our modest list of methods and ablations, just the last stage took over 1000 GPU-hours.

**Effect of latent quantization.** Adding latent quantization to AE and InfoWGAN-GP results in consistent and dramatic increases in modularity and compactness under both InfoMEC and DCI evaluation (Table 1). For explicitness, the improvement is more significant under (random forest) I than under (linear) InfoE. Qualitatively, decoded latent interventions demonstrate that the QLAE latent space is highly interpretable, corroborating the gains in modularity (Figure 4 and Appendix D).

**Comparison of QLAE with prior methods.** QLAE significantly outperforms all prior methods on all four datasets in modularity under both InfoM and D (Table 1), with the exception of \(\)-VAE and \(\)-TCVAE on Falcor3D under InfoM. There is also significant improvement in explicitness under (random forest) I, but less so for (linear) InfoE. The objectives in \(\)-VAE and \(\)-TCVAE contain a term that explicitly minimizes the total correlation (aka multiinformation), amongst the latent variables . Since this essentially optimizes for compactness, we should expect compactness metrics to rank \(\)-VAE and \(\)-TCVAE ahead of QLAE; InfoC does so, but C does not. Corresponding metrics from InfoMEC and DCI have Spearman rank correlations of \(=0.80,p<1 10^{-11}\) for modularity, \(=0.77,p<1 10^{-9}\) for explicitness, and \(=0.59,p<1 10^{-5}\) for compactness.

   model &  &  &  &  &  \\   &  \\ AE & (0.40 & **0.81** & 0.26) & (0.41 & **0.98** & 0.28) & (0.37 & **0.72** & 0.36) & (0.39 & 0.74 & 0.20) & (0.42 & **0.80** & 0.21) \\ \(\)-VAE & (0.59 & 0.81 & 0.55) & (0.59 & **0.99** & 0.49) & (0.45 & **0.71** & 0.51) & (**0.71** & 0.73 & 0.70) & (0.60 & 0.80 & 0.51) \\ \(\)-TCVAE & (0.58 & 0.72 & 0.59) & (0.61 & 0.82 & 0.62) & (0.51 & 0.60 & 0.57) & (**0.66** & 0.74 & 0.71) & (0.54 & 0.70 & 0.46) \\ BioAE & (0.54 & 0.75 & 0.36) & (0.56 & **0.98** & 0.44) & (0.45 & **0.66** & 0.36) & (0.54 & 0.73 & 0.31) & (**0.63** & 0.65 & 0.33) \\ VQ-VAE & (0.58 & 0.81 & 0.39) & (0.72 & **0.97** & 0.47) & (0.43 & 0.57 & 0.22) & (0.61 & **0.83** & 0.42) & (0.57 & 0.87 & 0.45) \\ QLAE (ours) & (**0.76** & **0.84** & 0.50) & (**0.95** & **0.99** & 0.59) & (**0.61** & 0.63 & 0.51) & (**0.71** & **0.77** & 0.44) & (**0.78** & **0.97** & 0.49) \\  InfoWGAN-GP & (0.50 & **0.57** & 0.29) & (0.61 & **0.78** & 0.41) & (0.43 & 0.40 & 0.20) & (0.44 & **0.60** & 0.30) & (0.53 & 0.51 & 0.24) \\ QLInfoWGAN-GP (ours) & (**0.63** & **0.59** & 0.47) & (**0.73** & **0.75** & 0.48) & (**0.62** & **0.51** & 0.37) & (**0.54** & **0.53** & 0.56) & (**0.63** & **0.58** & 0.49) \\    
    &  \\ AE & (0.12 & 0.81 & 0.10) & (0.11 & 0.82 & 0.08) & (0.15 & **0.83** & 0.14) & (0.08 & **0.76** & 0.07) & (**0.13** & **0.85** & 0.10) \\ \(\)-VAE & (0.37 & 0.89 & 0.30) & (**0.61** & **0.99** & 0.47) & (**0.31** & **0.83** & 0.27) & (0.32 & 0.84 & 0.28) & (0.23 & 0.88 & 0.19) \\ \(\)-TCVAE & (0.31 & 0.87 & 0.27) & (0.46 & 0.99 & 0.38) & (0.22 & 0.77) & (0.21) & (0.36 & 0.90 & 0.33) & (**0.19** & 0.84 & 0.16) \\ BioAE & (0.29 & 0.86 & 0.23) & (0.33 & 0.93 & 0.25) & (0.24 & **0.79** & 0.19) & (0.21 & 0.81 & 0.17) & (0.38 & 0.91 & 0.31) \\ VQ-VAE & (0.28 & 0.79 & 0.27) & (0.40 & 0.84 & 0.34) & (0.09 & 0.63 & 0.14) & (0.30 & **0.79** & 0.29) & (0.33 & 0.89 & 0.31) \\ QLAE (ours) & (**0.59** & **0.95** & 0.47) & (**0.81** & **0.99** & 0.61) & (**0.36** & **0.85** & 0.36) & (**0.50** & **0.96** & 0.38) & (**0.69** & **0.99** & 0.54) \\  InfoWGAN-GP & (0.14 & 0.72 & 0.12) & (0.23 & 0.80 & 0.18) & (0.09 & 0.63 & 0.09) & (0.11 & **0.74** & 0.08) & (0.13 & **0.71** & 0.11) \\ QLInfoWGAN-GP (ours) & (**0.26** & **0.77** & 0.26) & (**0.38** & **0.85** & 0.29) & (**0.24** & **0.71** & 0.25) & (**0.20** & **0.73** & 0.24) & (**0.24** & **0.79** & 0.25) \\ QLInfoWGAN-GP w/o w.d. & (0.19 & 0.73 & 0.19) & (0.16 & 0.71 & 0.13) & (**0.28** & **0.74** & 0.23) & (0.14 & **0.72** & 0.17) & (**0.20** & **0.77** & 0.23) \\   

Table 1: Main disentanglement results measured in InfoMEC and nonlinear DCI. Modularity is the key property, followed by explicitness, with compactness (grayed) a distant third. AE and InfoGAN variants are presented and bolded separately as all AEs are filtered for near-perfect data reconstruction, whereas InfoGANs are generally more lossy. For confidence intervals and data reconstruction results, see Appendix E.

   model &  &  &  &  &  \\   &  \\ QLAE (ours) & (**0.76** & **0.84** & 0.50) & (**0.95** & **0.99** & 0.55) & (**0.61** & **0.63** & 0.51) & (**0.71** & **0.77** & 0.44) & (**0.78** & **0.97** & 0.49) \\ QLAE w/ global codebook & (0.68 & 0.80 & 0.44) & (**0.96** & **0.99** & 0.48) & (**0.54** & **0.62** & 0.45) & (0.59 & 0.74 & 0.3

**Ablations on latent space design and regularization.** We observe that ablating dimension-specific codebooks, weight decay, and scalar codebooks from QLAE each causes a significant drop in InfoM and D (Table 2), verifying the importance of these design decisions. The effect of ablating weight decay from QLInfoWGAN-GP is less pronounced. We note that while VQ-VAE w/ weight decay performs somewhat closely to QLAE in terms of InfoMEC, this is only because we use the categorical codes (as opposed to the high-dimensional vector representation) for evaluation. In addition, the scalar codebook design of QLAE enables meaningful interpolation between discrete values, whereas this is not supported by vector quantization.

Figure 4: Decoded Isaac3D latent interventions for QLAE and BioAE, the prior method with highest InfoM. In each image block, a single data sample is encoded into its latent representation. In the \(j\)-th column, the \(j\)-th latent is intervened on with a linear interpolation of its range across the dataset. The resulting representation is then decoded. For reference, the true sources for this dataset are object shape, robot \(x\), robot \(y\), camera height, object scale, lighting intensity, lighting direction, object color, and wall color. QLAE’s representation is highly interpretable: going down each column corresponds to one source (sometimes two) changing in a consistent manner, with all other sources remaining unchanged. In contrast, the BioAE’s representation varies more sporadically, and its decoder often generates low-quality samples from post-intervention latents. For qualitative results for multiple data samples from all datasets alongside \(\) heatmaps, see Appendix D.

Related Work

**Nonlinear ICA and disentangled representation learning.** There is a long history of trying to build interpretable representations that separately represent the sources of variation in a dataset. This goes back to classic work on (linear) ICA [13; 29], and has been known in deep learning as disentanglement . Without further assumptions, nonlinear ICA and its relatives in disentangled representation learning are provably underspecified [30; 47]. Approaches to resolve this indeterminacy include labeling a small number of datapoints  or showing pairs of datapoints in which only one or a few sources differ [63; 48]. More in line with our work are approaches that assume additional structure in the data generative process and disentangle by ensuring the representation reflects this structure . Assumptions and methods include: factorized priors [25; 11; 36; 61], biologically inspired activity constraints , sparse source variation over time [65; 38], structurally sparse source to pixel influence [57; 31; 78; 52; 8], geometric assumptions on the source to image mapping [64; 77; 26; 22], sparse underlying causal graphs between sources , piecewise linearity , and hierarchical generation . Many of these ideas are in principle compatible with latent quantization, and we leave discovery of fruitful combinations to future work.

**Factorized latent spaces.** VAEs that specify an isotropic Gaussian latent prior regularize the marginalized variational distribution (aka aggregate posterior) towards being a factorized distribution . Roth et al.  bias latents to have pairwise factorized support via a Hausdorff set distance regularization. For linear ICA, Whittington et al.  prove that regularizing latents to be nonnegative and energy-minimizing results in them having factorized support. Differently from all of these works, latent quantization imbues a model with factorized structure by construction, instead of relying on the optimization of regularized objectives to manifest this structure. The favorable disentanglement that QLAE yields over \(\)-TCVAE  and BioAE  suggests that this strategy is more effective.

**Discrete representation learning.** Oord et al.  first demonstrated the feasibility of discrete neural representation learning at scale, and their techniques have since been broadly applied, e.g., to videos [70; 76], audio [2; 15; 68; 6], and anomaly detection . The following works design discrete representations similarly to how we do, though for purposes other than unsupervised disentanglement. Several works use one scalar codebook per latent dimension to achieve high efficiency in retrieval [3; 67; 32; 73]. Kobayashi et al.  disentangle normal and abnormal features in medical images into separate vector codebooks via pixel-space supervision. Liu et al.  and Trauble et al.  use multiple codebooks with separately parameterized key and value vectors and investigate the effect of discretization in systematic generalization and continual learning, respectively.

## 7 Discussion

We have proposed to use latent quantization and model regularization to impose an inductive bias towards disentanglement that enables our models to outperform strong prior methods. Ablations verify that our main design decisions are critical. We have also synthesized previously proposed ideas for evaluation into InfoMEC, three information-theoretic disentanglement metrics that rectify or sidestep key drawbacks in existing approaches.

While our results are promising, one concern might be that we have overfit our inductive bias to existing disentanglement benchmarks, in which, just like our model, the sources are discrete and the generative process is (near-)noiseless. Our experiments have already demonstrated the ability of latent quantization to represent sources that have more values (up to 40) than the per-dimension codebook size (fixed to 10) via allocating multiple latent dimensions. Future work should strive to construct disentanglement benchmarks that better reflect realistic conditions, e.g. continuous sources.

Beyond the intuitions and connections to related works we have presented, we do not understand why our method performs as well as it does. It may be fruitful to tackle this empirically, e.g. by probing how QLAE distributes data around its latent space, and how weight decay changes this. Achieving satisfactory understanding would enable the field to better position latent quantization within the ongoing body of work that aims to develop generalizable conditions for successful disentanglement.

Lastly, we hope this method, its future versions, and other methods the field develops are able to deliver on the original motivation for disentangled representation learning--to learn human-interpretable representations in complex, real-world situations, and to leverage the interpretability to empower human decision-making. This will require methods that can disentangle out-of-distribution data samples, that work for generic data types, and that can learn compositionally from sparse interactions with data. We suspect that latent quantization may have a role to play in these directions.