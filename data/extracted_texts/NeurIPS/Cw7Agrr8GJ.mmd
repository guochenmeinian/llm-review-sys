# Large Language Models-guided Dynamic Adaptation

for Temporal Knowledge Graph Reasoning

 Jiapu Wang\({}^{1}\), Kai Sun\({}^{1}\), Linhao Luo\({}^{2}\), Wei Wei\({}^{3}\), Yongli Hu\({}^{1}\), Alan Wee-Chung Liew\({}^{4}\), Shirui Pan\({}^{4}\)

\({}^{1}\)Beijing University of Technology, China, \({}^{2}\)Monash University, Australia

\({}^{3}\)University of Hong Kong, China, \({}^{4}\)Griffith University, Australia

{jpwang, sunkai}@emails.bjut.edu.cn, linhao.luo@monash.edu, weiwei1206cs@gmail.com

{huyongli, ybc}@bjut.edu.cn, {a.liew, s.pan}@griffith.edu.au

Equally ImportantCorresponding authorsCode and data are available at: [https://github.com/jiapuwang/LLM-DA.git](https://github.com/jiapuwang/LLM-DA.git)

###### Abstract

Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a **L**arge **L**anguage **M**odels-guided **D**ynamic **A**daptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks1.

## 1 Introduction

Temporal Knowledge Graphs (TKGs)  are the structured representations of the real world, which incorporate the temporal dimension to analyze how relations between entities evolve over time. Temporal Knowledge Graph Reasoning (TKGR) focuses on leveraging historical information within TKGs to forecast future events. Prior research  on TKGR has primarily relied on temporal logical rules  or deep learning algorithms, such as graph neural networks  and reinforcement learning techniques . However, the deep learning-based TKGRs often suffer from the lack ofinterpretability  and are difficult to dynamically update to accommodate new data in TKGs. While rule-based methods offer great interpretability and flexibility, effectively learning temporal logical rules and adapting them to new knowledge remains a huge challenge.

Large Language Models (LLMs) , pretrained on large-scale text corpora, have exhibited extensive knowledge and reasoning ability. LLMs is able to effectively grasp intricate semantic and logical relationships within natural language, making them show remarkable performance across a wide range of tasks . Recently, LLMs have also demonstrated surprising ability in temporal reasoning . By utilizing their powerful contextual processing and pattern recognition abilities, LLMs can extract meaningful temporal patterns and complex temporal dependencies from historical data within TKGs , thereby significantly enhancing their temporal reasoning capabilities. Thus, leveraging the capabilities of LLMs  holds great promise for enhancing the performance of TKGR tasks.

Previous research on LLMs for TKGRs has primarily focused on prompting the LLMs with historical events and asking the LLMs to infer new facts . Despite these accomplishments, LLMs are known to be black boxes, leaving it unclear which temporal patterns contribute to the reasoning results. Besides, LLMs suffer from the issue of hallucinations , which further undermines the trustfulness of the results. Moreover, it is impractical to promptly update LLMs to incorporate the evolving knowledge within TKGs for reasoning.

Due to the evolving nature, the knowledge within TKGs would continuously update over time, which results in a temporal distribution shift from the initial observations to the future facts . As illustrated in Figure 1, the distribution of the relations in TKG changes dramatically over longer intervals. Despite LLMs possessing abundant knowledge via pre-training, it is still essential to accommodate up-to-date knowledge for reasoning. However, continually updating LLMs is highly impractical due to the intensive resources required for fine-tuning . Additionally, TKGs usually contain significant noise, necessitating an efficient process to extract relevant information for LLMs to discern the underlying temporal patterns.

To address these challenges, this paper proposes a **L**arge **L**anguage **M**odel-guided **D**ynamic **A**daptation (LLM-DA) method for TKGR tasks, which dynamically adapts to the new knowledge and conduct interpretable reasoning powered by LLMs. Specifically, LLM-DA leverages the capabilities of LLMs to analyze historical data and extract temporal logical rules, unveiling temporal patterns and facilitating interpretable reasoning. To efficiently adapt to the new distribution of TKGs, LLM-DA introduces an innovative dynamic adaptation strategy. This strategy iteratively updates the rules generated by LLMs instead of updating the LLMs themselves with the latest events. The extracted rules are dynamically updated and ranked to ensure they consistently incorporate the most recent knowledge and improve predictions for future events, all without the resource-intensive process of LLM fine-tuning.

Figure 1: A brief description of LLM-DA. Specifically, LLM-DA harnesses LLMs to formulate general rules on historical data. Subsequently, LLM-DA dynamically guides the LLMs to update these rules based on current data, ensuring they more accurately reflect the objective distribution.

In order to facilitate the rule generation and update, LLM-DA employs a contextual relation selector to meticulously filter the relations in TKG. The selector identifies the top-\(k\) most important relations for each rule head based on their semantic similarities. For example, given a rule head "president_of", the relevant relations might be "occupation_of" and "politician_of". These selected relations are fed into the LLMs as context to ensure LLMs are aligned with the temporal data, enhancing their abilities in uncovering the underlying temporal patterns.

The main contributions of this paper are summarized as follows:

* This paper attempts to harness the ability of Large Language Models (LLMs) for rule-based Temporal Knowledge Graph Reasoning (TKGR) to unveil temporal patterns and facilitate interpretable reasoning;
* This paper proposes an innovative dynamic adaptation strategy that iteratively updates the LLM-generated rules with the latest events, allowing for better adaptation to the constantly changing dynamics within TKGs;
* This paper introduces the contextual relation selector to identify the top \(k\) relevant relations, ensuring higher contextual relevance and enhancing the ability of LLMs to understand complex temporal patterns;
* Experimental results on several widely used datasets show that LLM-DA significantly enhances TKGs reasoning accuracy without requiring fine-tuning LLMs.

## 2 Related Work

### Temporal Knowledge Graph Reasoning

Temporal Knowledge Graph Reasoning (TKGR) [28; 29; 30; 31; 32; 33; 34] aims to leverage historical information within TKGs to forecast future events, which can be roughly categorized into two groups: Rules-based TKGR methods and Deep learning-based TKGR methods.

**Rules-based TKGR methods** enhance TKGs inference by leveraging temporal logical rules to accurately predict future events. TLmod  introduces a sophisticated pruning strategy to derive rules and selects the high-confidence rules for TKGR tasks. TLogic  learns temporal logical rules from TKGs based on temporal random walks, and subsequently feeds these rules into a symbolic reasoning module for predicting future events. TILP  proposes a differentiable framework for temporal logical rule learning, utilizing constrained random walks to enhance the learning process. TFLEX  advances beyond learning simplistic chain-like rules by proposing a temporal feature-logic embedding framework. Although temporal logical rules can reveal hidden temporal patterns within TKGR, effectively extracting these rules from TKG still remains a significant challenge.

**Deep learning-based TKGR methods**[39; 40; 41; 42] employ the deep learning techniques to capture the hidden temporal patterns to predict the future events in TKGs. RE-NET  uses a recurrent event encoder and a neighborhood aggregator to encode historical facts and model their connections, enhancing future event predictions in TKGs. Based on RE-NET, RE-GCN  further employs RGCN and GRU to aggregate neighboring messages and model the temporal dependency. CyGNet  employs a copy-generation mechanism for capturing global repetition frequencies. TiRGN  integrates both local and global historical data to capture the sequential, repetitive, and cyclical patterns inherent in historical data. However, the deep neural networks adopted by these methods often lack interpretability, making it difficult to verify the predictions.

### Large Language Models for TKGR

Large Language Models (LLMs) for TKGR generally leverage the sufficient knowledge and reasoning ability of LLMs to conduct reasoning on TKGs. TIMEBENCH  proposes a comprehensive hierarchical temporal reasoning benchmark to provide a thorough evaluation for investigating the temporal reasoning capabilities of LLMs. Luo _et al._ performs fine-tuning on known data and then leverage a sequence of established factual information to predict and generate the subsequent event in the series. PPT  converts the TKGC task into a masked token prediction task using a Pre-trained Language Model and designs specific prompts for various types of intervals between timestamps to enhance the extraction of semantic information from temporal data. GPT-NeoX  implements a method that utilizes GPT for TKGs forecasting through in-context learning without any fine-tuning. Similarly, Mistral-8x7B-CoH  also does not require fine-tuning and adopts a "chain-of-history" reasoning method to effectively generate high-order historical information step-by-step. Nevertheless, existing methods only prompt the LLMs with historical events from TKG, which are limited by the quality of input data and interpretability of LLMs.

Different from the aforementioned LLM for TKGRs, the proposed LLM-DA is LLMs for rule-based TKGR method. By utilizing explicit rules, LLM-DA ensures the interpretability of the LLM-generated processes and dynamically updates the rules to adapt to new data, thereby addressing a major limitation of LLMs-enhanced deep learning-based TKGRs.

## 3 Preliminary

**Temporal Knowledge Graph (TKG).** TKG \(=\{,~{},~{},~{}\}\) is a collection of entity set \(\), relation set \(\) and timestamp set \(\). Specifically, each quadruplet is denoted as \((e_{s},~{}r,~{}e_{o},~{}t)\), where \(e_{s},~{}e_{o}\) represent the entities, \(r\) denotes the relation and \(t\) is the timestamp.

**Temporal Logical Rule.** Temporal logical rules \(\) define the relation between two entities \(e_{s}\) and \(e_{o}\) at timestamp \(t_{l}\),

\[: =r(e_{s},e_{o},t_{l})_{i=1}^{l-1}r^{*}(e_{s},~{ }e_{o},~{}t_{i}), \]

where the left-hand side denotes the rule head with relation \(r\) that can be induced by (\(\)) the right-hand rule body. The rule body is represented by the conjunction (\(\)) of a series of body relations \(r^{*}\{r_{1},...,r_{l-1}\}\).

**Different Data Types.**_Historical data_ refers to data that have occurred in the past, reflecting the state of things in the past period of time. _Current data_ reflects the latest state of things in the present, from a more recent point in time. _Future data_ refers to data that will occur, reflecting the possible future trend of things. Specifically, the historical data, current data and future data correspond to the training, validation, and test datasets of prior research [46; 22].

## 4 Methodology

In this section, we propose a novel **L**arge **L**anguage **M**odel-guided **D**ynamic **A**daptation (LLM-DA) method for TKGR tasks. LLM-DA contains four main stages: **Temporal Logical Rules Sampling** explores the constrained Markovian random walks to extract temporal logical rules from the historical data; **Rule Generation** utilizes the powerful generative capabilities of LLMs to extract

Figure 2: The Framework of LLM-DA. Specifically, LLM-DA first analyzes historical data to extract temporal rules and utilizes the powerful generative capabilities of LLMs to generate general rules. Subsequently, LLM-DA updates these rules using current data. Finally, the updated rules are applied to predict future events.

meaningful temporal patterns and complex temporal dependencies from historical data within TKGs; **Dynamic Adaptation** leverages LLMs to update the LLM-generated general rules using current data; **Candidate Reasoning** combines rules-based reasoning and graphs-based reasoning to generate candidates. The whole framework is illustrated in Figure 2.

### Temporal Logical Rules Sampling

Temporal logical rules sampling is a constrained Markovian random exploration process, which explores the _Timestamp-Entity_ joint level weighted temporal random walks. Generally, the constrained Markovian random exploration process is not only strictly constrained by the graph structure but also deeply influenced by the temporal dimension when choosing the next node.

**Constrained Markovian Random Walks.** Compared to traditional Markov random walks, constrained Markovian random walks primarily reflect in two key factors such as temporal order and temporal intervals when choosing the next node. As shown in Figure 3, edges of the next state are selected based on the temporal order, and edges are weighted by the temporal interval. Specifically, edges with shorter interval receive higher weights, thus making the random walk more inclined to choose nodes that are temporally closer.

To maximize the performance of random walks, LLM-DA needs to ensure that the random walks simultaneously satisfy the Markov property and additional constraints. Given the edge \(r(e_{x},e_{y},t_{l})\), LLM-DA employs the Markovian random walks to search the closed temporal paths, further obtaining the set of candidates \(_{r}\). To ensure the efficiency of our framework, LLM-DA introduces the filtering operator \((t)\), which explores the temporal order to filter these candidate paths. The filtering operator \((t)\) can be denoted as:

\[(t)=\{1,&\:t<t_{l},\\ 0,&. \]

After filtering these candidate paths, LLM-DA further introduces the temporal interval as the another constraint. This constraint selects the next node based on the transition probability \(P\). For a temporal logical rule, the edge \(r(e_{x},e_{y},t)\) represents a connection from node \(e_{x}\) to node \(e_{y}\) at timestamp \(t\). Thus, the transition probability of the constrained Markovian random walks is expressed as:

\[P_{xy}(t)=(-(t-T))}{_{(e_{x},e_{y},t^{}) }(-(t^{}-T))}, \]

where \(P_{xy}(t)\) is the probability of transitioning from node \(e_{x}\) to node \(e_{y}\) at time \(t\), and \(w(t)=(-(t-T))\) denotes an exponential decay function that weights the edges based on the time difference, \(T\) is the current time, \(t\) is the timestamp of the edge, and \(\) is the decay rate parameter, which controls the weight given to more recent times. In this way, more recent times (i.e., \(t\) closer to \(T\)) are assigned greater weight. The detailed theoretical analysis is shown in Appendix B. Through the constrained Markovian random walks on the historical data, LLM-DA extracts the temporal rules \(\) from the sampled temporal paths.

### Rule Generation

Rule generation typically utilizes the powerful generative capabilities of LLMs to improve the insufficient coverage and low quality of the extracted temporal rules \(\). Specifically, LLM-DA first employs the contextual relation selector to identify the top \(k\) relevant relations. Additonally, LLMs

Figure 3: The constraints of the constrained Markovian random walks. “\(\)” denotes this path does not exist, “\(P_{i}\)” indicates the transition probability.

[MISSING_PAGE_FAIL:6]

_Low-Quality Rules._ As TKGs evolve over time, the LLM-generated rule set \(_{g}\) may become increasingly difficult to fit to the current data, eventually turning into low-quality rules. Thus, LLM-DA applies the _Confidence_ metric to score each temporal rule \(\) on the current data. _Confidence_ generally measures the reliability of the temporal rule \(\), which can be defined as the proportion of temporal fact pairs that satisfy the rule body \(rule\_body()\) and also satisfy the whole \(rule()\):

\[c_{}=rule\_{ body()}}{rule()}, \]

where \(c_{}\) denotes the confidence of rule \(\). The higher the confidence, the greater the reliability of the rule. In other words, when the rule body is true, the likelihood of the rule head being true is higher. Subsequently, we select the subset of rules with low confidence \(_{g()}=\{_{g} c_{}<\}\), where \(\) denotes the threshold of the low confidence.

_Extracted Rules from Current Data._ To address the issue of the broad range of rules generated by LLMs, LLM-DA explores constrained Markovian random walks to extract temporal logical rules from current data, which serves as a standard to constrain the scope of dynamic adaptation. Through iteratively invoking LLMs, the accuracy of LLMs in predicting future events can be enhanced. The detailed prompt for _Dynamic Adaptation_ refers to Appendix A.2.

Finally, LLM-DA updates these LLMs-generated low-quality rules through the extracted rules from current data, further obtaining the rules set \(_{d}\).

### Candidate Reasoning

Candidate reasoning aims to infer potential answers for the query by integrating the above LLMs-generated rules and GNNs-based predictions. Specifically, LLM-DA mainly consists of two key modules: **Rule-based Reasoning** and **Graph-based Reasoning**.

**Rule-based Reasoning.** Rule-based reasoning typically utilizes the above LLMs-generated high-scoring rules to conduct in-depth logical reasoning within TKGs, deducing new entities as potential answers . Given a query \((e_{s},\ r,\?,\ t_{l})\), LLM-DA scores the rule through the Equation 7, and then select the high-scoring rules:

\[^{}_{d}=\{ c_{}>,_{d}\}, \]

where \(_{d}\) is the rule set obtained after the _Dynamic Adaptation_ process, \(^{}_{d}\) is the high-confidence rule set, in which the score \(c_{}\) of the rule \(\) is greater than the threshold \(\). Following the rule \(^{}_{d}\), we can find the reasoning paths and further derive the entity \(e^{}_{o}\):

\[(e_{s},\ r,\ e^{}_{o},\ t_{l})\ \ _{i=1}^{l-1}(e_{s},\ r_{i},\ e^{}_{o},\ t_{i}), \]

where \(e^{}_{o}\) is the candidate derived based on the rule \(\), and \(t_{l-1} t_{1}\). Considering the time decay property of temporal data, we further select candidates that are most relevant to the query:

\[_{(,e^{}_{o})}=_{ R^{}_{i}\ body(r)(e_{s},e^{}_{o},t_{l}) }(c_{}+(-(t_{l}-t_{o}))), \]

where \(_{(,e^{}_{o})}\) indicates the score of the candidate \(e^{}_{o}\) obtained through the searched path in TKGs based on rule \(\) at the time point \(t_{o}\), \(c_{}\) denotes the confidence of the temporal rule \(\), \(\) represents the decay rate, and \(body()(e_{s},e^{}_{o},t_{l})\) denotes the path in TKGs that satisfies the rule body.

**Graph-based Reasoning.** Due to inconsistent data distribution, candidates generated solely based on rules may not fully match all query answers. Thus, we introduce the graph-based reasoning function \(f_{g}(Query)\) to further predict the candidates of the query, and the score can be computed through the inner product operation:

\[_{(graph,e^{}_{o})}= f_{g}(Query),e^{}_{o}. \]

Since the candidates of _Rule-based Reasoning_\(_{(,e^{}_{o})}\) and _Graph-based Reasoning_\(_{(graph,e^{}_{o})}\) have the overlap and difference, we assign the score \(_{(,e^{}_{o})}\) as \(0\) where \(e^{}_{o}_{(graph,e^{}_{o})}\), \(e^{}_{o}_{(,e^{}_{o})}\), and vice versa. The whole score of the candidate can be computed as follows:

\[_{f}=_{(,e^{}_{o})}\ +\ (1- )_{(graph,e^{}_{o})}, \]

where \(_{f}\) represents the final score of the candidate \(e^{}_{o}\) from rule-based reasoning and graph-based reasoning modules, and \(\) is used to assign weights to different scores. Finally, LLM-DA aggregates all candidates, and then sorts these candidates to select those that best meet the query requirements.

## 5 Experiments

### Experimental Settings

**Datasets.** ICEWS14  and ICEWS05-15  are the subset of _Integrated Crisis Early Warning System (ICEWS)_, which is a TKG of international political events and social dynamics. ICEWS14 contains events that occurred in 2014, while ICEWS05-15 contains events that occurred between 2005 and 2015. Details of datasets can be referred to Appendix C.1.

**Baselines.** The proposed LLM-DA is compared with several classic TKGR methods, including 1) TKGR methods: RE-NET , RE-GCN , TiRGN  and TLogic ; 2) LLMs-based TKGRs: GPT-NeoX , Llama-2-7b-CoH, Vicuna-7b-CoH , Mixtral-8x7B-CoH  and PPT . Here, LLM-DA selects RE-GCN and TiRGN as the graph-based reasoning function \(f_{g}(Query)\). The detail of each baseline is described in Appendix C.2.

**Parameter Setting.** The proposed LLM-DA uses the ChatGPT4 as the LLM for _Rules Generation_ and _Dynamic Adaptation_. LLM-DA chooses _Mean Reciprocal Rank_ (MRR) and _Hit@\(N\)_(\(N=1,3,10\)) as evaluation metrics, and presents the _filtered_ results (Appendix C.3). Additionally, LLM-DA sets the decay rate \(\) in _Temporal Logical Rules Sampling_ and _Candidate Generation_, the threshold \(\) in _Dynamic Adaptation_, the min-confidence \(\) and the parameter \(\) in _Candidate Generation_ on both datasets as follows: \(=0.1\), \(=0.01\), \(=0.9\) and \(=0.01\), except for \(=0.8\) on ICEWS05-15. The number of iterations for the _Dynamic Adaptation_ is set as \(5\). All experiments are implemented on a NVIDIA RTX 3090 GPU with i9-10900X CPU.

### Performance Comparison

The link prediction experimental results are displayed of ICEWS14 and ICEWS05-15 in Table 1, and the ICEWS18 is shown in Appendix C.4. The experimental analyses are listed as follows:

(1) Experimental results indicate that even without fine-tuning, the proposed LLM-DA can still surpass all LLM-based TKGR methods. This phenomenon demonstrates that the dynamic adaptation strategy can effectively update LLMs-generated general rules with latest events to capture the evolving nature of TKGs, thereby significantly improving the accuracy of future event predictions. Additionally, we present the visualization experiment in Appendix C.6, which validates the superiority of the dynamic adaptation strategy.

(2) Some LLMs-based TKGC methods such as PPT  and GPT-NeoX , are not always superior to traditional TKGR methods. This is primarily because the rules generated by LLMs are too broad and sometimes fail to precisely adapt to specific data. However, the proposed LLM-DA outperforms the existing state-of-the-art benchmarks on all metrics. This phenomenon proves that LLM-DA can effectively guide LLMs in adjusting rules to the target distribution.

   Type & Models & Train &  &  \\   & & & MRR & Hit@1 & Hit@3 & Hit@10 & MRR & Hit@1 & Hit@3 & Hit@10 \\   & RE-NET & \(\) & 0.388 & 0.290 & 0.436 & 0.576 & 0.441 & 0.332 & 0.512 & 0.650 \\  & RE-GCN & \(\) & 0.425 & 0.320 & 0.476 & 0.627 & 0.478 & 0.371 & 0.535 & 0.682 \\  & TiRGN & \(\) & 0.441 & 0.341 & 0.497 & 0.650 & 0.495 & 0.389 & 0.559 & 0.703 \\  & TiLogic & \(\) & 0.390 & 0.295 & 0.437 & 0.573 & 0.459 & 0.360 & 0.518 & 0.646 \\   & PPT & \(\) & 0.384 & 0.289 & 0.425 & 0.570 & 0.389 & 0.286 & 0.434 & 0.586 \\  & Llama-2-7b-CoH & \(\) & – & 0.349 & 0.470 & 0.591 & – & 0.386 & 0.541 & 0.699 \\  & Vicuna-7b-CoH & \(\) & – & 0.328 & 0.457 & 0.656 & – & 0.392 & 0.546 & 0.707 \\  & GPT-NeoX & \(\) & – & 0.334 & 0.460 & 0.565 & – & – & – & – \\  & Mixtral-8x7B-CoH & \(\) & 0.439 & 0.331 & 0.496 & 0.649 & 0.497 & 0.380 & 0.564 & 0.713 \\   & LLM-DA (RE-GCN) & \(\) & 0.461 & 0.356 & 0.515 & 0.662 & 0.501 & 0.394 & 0.568 & 0.710 \\  & LLM-DA (TiRGN) & \(\) & **0.471** & **0.369** & **0.526** & **0.671** & **0.521** & **0.416** & **0.586** & **0.728** \\   

Table 1: Link prediction results on ICEWS14 and ICEWS05-15. The best results are in bold and - means the result is unavailable. \(\) denotes the TKGR methods, \(\) represents the LLMs-based TKGR, and LLM-DA (\(\)) indicates replacing the graph-based reasoning module \(f_{g}(Query)\) with TKGRs (\(\)).

(3) GPT-NeoX  is an important baseline as it also incorporates GPT as an LLM in TKGC tasks. However, LLM-DA shows significant improvement. This phenomenon indicates that the dynamic adaptation strategy can effectively update LLMs-generated rules to adapt to future data.

(4) Furthermore, replacing the graph-based reasoning module \(f_{g}(Query)\) with RE-GCN ("LLM-DA (RE-GCN)") and TiRGN ("LLM-DA (TiRGN)"), the MRR performance shows a slight variation. This variation highlights the importance of incorporating graph-based reasoning function in enhancing the ability to predict future events.

### Analysis of Dynamic Adaptation

In LLM-DA, dynamic adaptation aims to continuously update the generated rules to capture temporal patterns and facilitate future predictions. To further investigate its impact, we aim to answer the following questions: **RQ1:** Can the dynamic adaptation better extract the temporal patterns from TKGR for reasoning? **RQ2:** Can the dynamic adaptation adapt to different distributions over time? **RQ3:** Can the iterative dynamic adaptation improve the performance?

**RQ1:** The ablation study on different data without dynamic adaptation aims to evaluate the impact results are in bold. "LLM-DA \(w\) H" indicates "only using the historical data", "LLM-DA \(w\) C" is "only using the current data", and "LLM-DA \(w\) H+C" denotes "using the historical and current data".

**RQ1:** The ablation study on different data without dynamic adaptation aims to evaluate the impact of dynamic adaptation on performance. Dynamic adaptation typically employs the current data to update the rules generated by LLMs based on historical data. To demonstrate the superiority, we compare three variations, including "_LLM-DA w H_", "_LLM-DA w C_" and "_LLM-DA w H+C_", and the experimental results are shown in Table 2. Specifically, the "_LLM-DA w H+C_" outperforms "_LLM-DA w H_" and "_LLM-DA w C_", indicating that large-scale historical data can provide general knowledge, while current data offers relevant knowledge. The combination of both enhances the prediction of future events. Furthermore, compared to "_LLM-DA w H+C_", LLM-DA shows a significant improvement. This phenomenon demonstrates that the dynamic adaptation strategy can effectively integrate historical data and current data, leveraging the current data to update the rules generated by LLMs on historical data.

**RQ2:** The temporal data has the time decay property, causing the issue of distributional shift. Specifically, the longer the time interval between the data, the more pronounced the shift becomes. To verify whether the dynamic adaptation can adapt to different distributions over time, we conduct the time interval segmented prediction experiment, which typically segments the future data into multiple time intervals based on chronological order, and then conducts the link prediction experiment for each time interval. As shown in Figure 4, the proposed LLM-DA exhibits a significant performance improvement in the MRR metric over RE-GCN and TiRGN in each time interval. This indicates that the proposed LLM-DA can accurately capture the temporal dependencies in TKGs and adapt to the continuously changing temporal data. Furthermore, in Appendix C.5, we conduct long-term horizontal link prediction to forecast events occurring at future time points.

**RQ3:** To further verify whether the number of iterations of the dynamic adaptation module affects the performance, we conduct the different numbers of iterations experiment on both datasets. As shown in Figure 5, the MRR performance exhibits an increasing trend as the number of iterations

   &  &  \\   & MRR & Hit@1 & Hit@3 & Hit@10 & MRR & Hit@1 & Hit@3 & Hit@10 \\  LLM-DA (TiRGN) \(w\) H & 0.450 & 0.345 & 0.502 & 0.656 & 0.503 & 0.395 & 0.563 & 0.709 \\ LLM-DA (TiRGN) \(w\) C & 0.454 & 0.350 & 0.507 & 0.657 & 0.508 & 0.400 & 0.570 & 0.714 \\ LLM-DA (TiRGN) \(w\) C & 0.457 & 0.352 & 0.510 & 0.659 & 0.511 & 0.402 & 0.573 & 0.718 \\ LLM-DA (TiRGN) & **0.471** & **0.369** & **0.526** & **0.671** & **0.521** & **0.416** & **0.586** & **0.728** \\  

Table 2: Ablation study on different data without dynamic adaptation on both datasets. The best results are in bold. “LLM-DA \(w\) H” indicates “only using the historical data”, “LLM-DA \(w\) C” is “only using the current data”, and “LLM-DA \(w\) H+C” denotes “using the historical and current data”.

Figure 4: Time interval segmented prediction: MRR performance on both datasets compared to different baselines.

increases. These observations indicate that the dynamic adaptation strategy can continuously update the LLMs-generated rules with the latest events through iterations, thereby better adapting to the dynamic changes of TKGs. This further demonstrates the effectiveness and necessity of the dynamic adaptation strategy in handling the evolving nature of TKGs. Moreover, we conduct the parameter analysis experiment to validate the impact of the weight \(\) in Appendix C.7.

## 6 Conclusion

In this paper, we propose a novel **L**arge **L**anguage **M**odel-guided **D**ynamic **A**daptation (LLM-DA) method to enhance TKGR tasks. Specifically, LLM-DA leverages a contextual relation selector to identify the top \(k\) most relevant relations, thereby selecting pertinent contextual information. Subsequently, LLM-DA harnesses the generative capabilities of LLMs to analyze historical data and derive general rules. Furthermore, LLM-DA proposes a dynamic adaptation strategy to update the LLM-generated rules with latest events, further capturing the evolving nature of TKGs. Experimental results on several datasets unequivocally demonstrate that LLM-DA achieves competitive performance compared to state-of-the-art methods. Appendix D further analyzes the limitations of LLM-DA and provides an outlook for future work.