# PAC-Bayesian Spectrally-Normalized Bounds for

Adversarially Robust Generalization

 Jiancong Xiao\({}^{1,}\)1, Ruoyu Sun\({}^{2,3,4,}\)2, Zhi-Quan Luo\({}^{2,4,}\)3

\({}^{1}\)University of Pennsylvania, PA, USA

\({}^{2}\)Scool of Data Science, The Chinese University of Hong Kong, Shenzhen, China

\({}^{3}\)Shenzhen International Center for Industrial and Applied Mathematics

\({}^{4}\)Shenzhen Research Institute of Big Data

jcxiao@upenn.edu, {sunruoyu,luozq}@cuhk.edu.cn

###### Abstract

Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a different perspective on understanding robust generalization: The mismatch terms between standard and robust generalization bounds shown in previous studies do not contribute to the poor robust generalization. Instead, these disparities solely due to mathematical issues. Finally, we extend the main result to adversarial robustness against general non-\(_{p}\) attacks and other neural network architectures.

## 1 Introduction

Even though deep neural networks (DNNs) have impressive performance on many machine learning tasks, they are often highly susceptible to adversarial perturbations imperceptible to the human eye (Goodfellow et al., 2015; Madry et al., 2018). They have received enormous attention in the machine learning literature over recent years and a large number of defense algorithms (Gowal et al., 2020; Rebuffi et al., 2021) are proposed to improve the robustness in practice. Nonetheless, it still fails to deliver satisfactory performance. One major challenge stems from adversarially robust generalization. For example, Madry et al. (2018) demonstrated that the robust generalization gap can extend up to 50% on CIFAR-10. In contrast, the standard generalization gap is notably small in practical settings. Hence, a theoretical question arises: Why is there a huge difference between standard generalization and robust generalization? This paper focuses on norm-based generalization analysis.

In classical learning theory, one of the most well-known findings is that the generalization bound for neural networks depends on the norms of their layers (Bartlett, 1998). To further explore the generalization of deep learning, a series of work aimed at improving the norm-based bound (Bartlett &Mendelson, 2002; Neyshabur et al., 2015; Golowich et al., 2018), mainly using tools of Rademacher complexity. The tightest bound is given by Bartlett et al. (2017), using a covering number approach. Neyshabur et al. (2017) gave a different and simpler proof based on PAC-Bayes analysis, presented an almost equally tight bound. The key step involves bounding the change in output of the predictors in response to slight variations in the predictor parameters. In particular, considering \(f_{}()\) as the predictor parameterized by \(\), the crucial component for providing the generalization bound lies in bounding the gap \(|f_{}()-f_{^{}}()|\), where \(\) and \(^{}\) are close. The weight perturbation bound, which addresses this aspect, is presented in Lemma 2 of Neyshabur et al. (2017).

To comprehend the limited robust generalization capabilities of deep learning, a line of research endeavors to extend the norm-based bounds into robust settings. However, this has proven to be a challenging mathematical problem, as researchers have attempted the mentioned approaches including the Rademacher complexity (Khim and Loh, 2018; Yin et al., 2019; Awasthi et al., 2020), covering number (Gao and Wang, 2021; Xiao et al., 2022; Mustafa et al., 2022), and the PAC-Bayes analysis (Farnia et al., 2018), yet a satisfactory solution remains elusive. For more details, see Section 2.

We use the PAC-Bayesian approach as an example to illustrate the mathematical challenge. The weight perturbations in adversarial settings differ from those in standard settings. When considering two predictors \(f_{}()\) and \(f_{^{}}()\), the adversarial examples against these predictors are distinct, leading to a gap referred to as robust weight perturbation (defined later in Problem 1). It remains unclear how to establish a bound for robust weight perturbation. The combined changes in input and weights can potentially cause a significant alteration in the function value. The main challenge is illustrated in Figure 1, the details of which will be provided in Section 6.2. As a result, Farnia et al. (2018) introduced additional assumption to control this gap and provide bounds in adversarial settings. However, the assumption imposed limitations on the effectiveness of the bounds due to two reasons: Firstly, the assumption of sharp gradients throughout the domain is a strong requirement. Secondly, without this assumption, the bounds become unbounded (=\(+\)). Similarly, other existing norm-based bounds also depend on additional assumptions or involve higher-order terms in certain factors.

Given that the existing robust generalization bounds are much larger than standard generalization bounds, these results suggest a possible hypothesis: The significant disparity between standard and robust generalization in practical scenarios could potentially be attributed to the mismatch terms between the standard bounds and the robust bounds. However, verifying this hypothesis is challenging because it remains unclear whether the existence of these terms or assumptions is due to mathematical issues. Therefore, the current bounds are insufficient to address the main theoretical question.

In this paper, we address this problem and present a PAC-Bayes spectrally-normalized robust generalization bound without additional assumptions. Our robust generalization bound is as tight as the standard generalization bound, with an additional factor representing the perturbation intensity \(\). Furthermore, our bound is strictly smaller than the previous generalization bounds proposed in adversarial robustness settings. To provide an initial overview of the main result, we begin by defining the _spectral complexity_ of a \(d\)-layer neural network \(f_{}\) as follows:

\[(f_{})=_{i=1}^{d}\|W_{i}\|_{2}^{2}_{i=1}^{d}( \|W_{i}\|_{F}^{2}/\|W_{i}\|_{2}^{2}),\] (1)

where \(W_{i}\) is the weights of \(f_{}\) in each of the \(d\) layers.

**Theorem** (Informal).: _Let \(m\) be the number of samples and the training samples \(x\) is bounded by \(B\). \(\) is the attack intensity. Let \(f_{}:^{k}\) be a \(d\)-layer feedforward network. Then, with high probability, we have_

\[((f_{ })/m}).\]

Figure 1: Demonstration of the main challenge of providing robust generalization bound. The weight perturbation bound (Neyshabur et al., 2017) seems hard to extend to adversarial settings.

When \(=0\), the bound reduces to the standard generalization bound presented by Neyshabur et al. (2017). Our results give a different perspective from existing bounds. The additional factors or assumptions are solely due to mathematical considerations. Our findings suggest that the implicit difference of the spectral complexity \((f_{})\) likely contributes to the significant disparity between standard and robust generalization.

Technical Proof.It is shown that the robust weight perturbation is not controllable without additional assumptions. Therefore, existing tools are not sufficient to derive the bounds. The main technical tools to derive the bounds are two folds. Firstly, we introduce a crucial inequality to address this problem, which is the preservation of weight perturbation bound under \(_{p}\) attack. Secondly, we restructure the proof by (Neyshabur et al., 2017) in terms of the margin operator. This modification enables the application of the aforementioned inequality. To further extend the bound to more general settings, we establish a framework that allows us to derive a robust generalization bound from its corresponding standard generalization bound. The framework's demonstration is presented in Figure 2, and detailed information regarding Figure 2 will be provided in Section 6.3.

Furthermore, we extend the results to encompass general settings. Firstly, although \(_{p}\) adversarial attacks are widely used, real-world attacks are not always bounded by the \(_{p}\) norm. Hence, we extend the results to cover general attacks. Secondly, as the current state-of-the-art robust performance is achieved with WideResNet (Rebuffi et al., 2021; Croce et al., 2021), we demonstrate that the results can be extended to other DNN structures, such as ResNet.

The contributions are listed as follows:

1. Main result: We provide a PAC-Bayesian spectrally-normalized robust generalization bound without any additional assumption. The derived bound is as tight as the standard generalization bound and tighter than the existing robust generalization bound.
2. Our results give a different perspecitve from existing bounds. The significant disparity between standard and robust generalization in practical scenarios is not attributed to the mismatch terms between the standard bound and the robust bound. The implicit difference of the spectral complexity \((f_{})\) possibly contributes to the significant disparity.
3. We provide a general framework for robust generalization analysis. We show how to obtain a robust generalization bound from a given standard generalization bound.
4. We extend the result to general adversarial attacks and other neural networks architectures.

## 2 Related Work

Adversarial Attack.Adversarial examples were first introduced in (Szegedy et al., 2014). Since then, adversarial attacks have received enormous attention (Papernot et al., 2016; Moosavi-Dezfooli et al., 2016; Carlini and Wagner, 2017). Nowadays, attack algorithms have become sophisticated and powerful. For example, Autoattack (Croce and Hein, 2020) and Adaptive attack (Tramer et al., 2020). Therefore, we consider theoretical analysis on robust margin loss (defined later in Eq. (4)) against any norm-based attacks. Real-world attacks are not always norm-bounded (Kurakin et al., 2018). Therefore, we also consider non-\(_{p}\) attacks (Lin et al., 2020; Xiao et al., 2022) in Sec. 7.

Adversarially Robust Generalization.Even enormous algorithms were proposed to improve the robustness of DNNs (Madry et al., 2018; Tramer et al., 2018; Gowal et al., 2020; Rebuffi et al., 2021), the performance was far from satisfactory. One major issue is the poor robust generalization, or robust overfitting (Rice et al., 2020). A series of studies (Xing et al., 2021; Xiao et al., 2022; Ozdaglar et al., 2022) have delved into the concept of uniform stability within the context of adversarial training.

Figure 2: Demonstration of the framework: perturbation bound of robustified function. Under this framework, a standard generalization bound directly implies a robust generalization bound.

However, these analyses focused on general Lipschitz functions, without specific consideration for neural networks.

Rademacher Complexity.Rademacher complexity can provide similar spectral norm generalization bound as PAC-Bayesian bound (Theorem 2). Rademacher complexity was extended to adversarial settings for linear classifier (Khim and Loh, 2018; Yin et al., 2019) and two-layers neural networks (Awasthi et al., 2020). As for DNNs, they found that it was mathematically difficult and provided some discussions on surrogate losses rather than the adversarial loss.

Covering Number.Rademacher complexity can be bounded in terms of the covering number of the function class, as discussed in (Bartlett et al., 2017). Nevertheless, calculating the covering number for an adversarial function class is also shown to be a challenging problem. Gao and Wang (2021) considered adversarial loss against FGSM attacks, employing similar assumptions to those of (Farnia et al., 2018), resulting in a bound similar to Theorem 3. Additionally, Xiao et al. (2022) and Mustafa et al. (2022) introduced two different methods, respectively, to compute the covering number for adversarial function classes. However, the bounds obtained through these methods remain notably larger when compared to those in standard settings. The related research on Rademacher complexity and covering number help proves the difficulty of the problem we are addressing.

PAC-Bayes Analysis.We mainly compare our results to the previous PAC-Bayesian spectrally-normalized bounds (Neyshabur et al., 2017; Farnia et al., 2018), which we have already discussed in the introduction. We will provide more details later. The workshop version of this paper is presented in (Xiao et al., 2023). Other PAC-Bayes frameworks for tackling adversarial robustness also exist. Viallard et al. (2021) explored a distinct adversarial attack targeting the loss of the \(Q\)-weighted majority vote over the posterior distribution \(Q\). Mustafa et al. (2023) introduced a non-vacuous PAC-Bayes bound designed for stochastic neural networks.

## 3 Preliminaries

### Notations

We mainly follow the notations of (Neyshabur et al., 2017). Consider the classification task that maps the input \(\) to the label \(y^{k}\). The output of the model is a score for each of the \(k\) classes. The class with the maximum score will be the prediction of the label of \(\). A sample dataset \(S=\{(_{1},y_{1}),,(_{m},y_{m})\}\) with \(m\) training samples is given. The \(l_{2}\) norm of each of the samples \(x_{i}\) is bounded by \(B\), _i.e._, \(\|x_{i}\|_{2} B\), \(i=1,,m\). Let \(\|W\|_{F}\) and \(\|W\|_{2}\) denote the Frobenius norm and the spectral norm of the weights \(W\), respectively.

Fully-Connected Neural Networks.Let \(f_{}():^{k}\) be the function computed by a \(d\)-layer feed-forward network for the classification task with parameters \(=(\{W_{i}\}_{i=1}^{d})\), \(f_{}()=W_{d}\,(W_{d-1}\,(....(W_{1})))\), here \(\) is the ReLU activation function. Let \(f_{}^{i}()\) denote the output of layer \(i\) before activation and \(h\) be an upper bound on the number of output units in each layer. We can then define fully-connected feed-forward networks recursively: \(f_{}^{1}()=W_{1}\) and \(f_{}^{i}()=W_{i}(f_{}^{i-1}())\). In Section 7, we extend the results to ResNet (He et al., 2016), since the state-of-the-art robust performance is built on WideResNet (Rebuffi et al., 2021; Croce et al., 2021).

### Standard Margin Loss and Robust Margin Loss

Standard Margin Loss.For any distribution \(\) and margin \(>0\), the expected margin loss is defined as follows:

\[L_{}(f_{})=_{(,y)}[f_ {}()[y]+_{j y}f_{}( )[j]].\] (2)

Let \(_{}(f_{})\) be the empirical estimate of the above expected margin loss. Since setting \(=0\) corresponds to the classification loss, we will use \(L_{0}(f_{})\) and \(_{0}(f_{})\) to refer to the expected loss and the training loss. The loss \(L_{}\) defined this way is bounded between 0 and 1.

Robust Margin Loss.Adversarial examples are usually crafted by an attack algorithm. Let \(^{adv}_{}()\) be an algorithm output and \(^{*}_{}()\) be the maximizer of the following maximization problem

\[_{\|\|}(f_{}(+),y),\] (3)where \(\) is the loss function of the predicted label and true label. Without explicit specification, \(\|\|\) refers to the \(_{2}\) norm. The robust margin loss is defined as follows:

\[ R_{}(f_{})=&_ {(,y)}[^{}_{ }^{p}(),f_{}(^{})[y]+ _{j y}f_{}(^{})[j]]\\ =&_{(,y)}[f_ {}(+_{}^{*}())[y]+ _{j y}f_{}(+_{}^{*}())[j ]].\] (4)

Let \(_{}(f_{})\) be the empirical estimate of the above expected robust margin loss. The robust margin loss requires the whole norm ball around the original example \(\) to be labelled correctly, which is the goal of norm-based adversarial robustness. By replacing \(_{}^{*}()\) by \(_{}^{adv}()\) in the above definition, we denote \(R_{}^{adv}(f_{})\) as the margin loss against attacks \(adv\). The work of (Farnia et al., 2018) consider three attacks: fast gradient sign method (FGSM or FGM), projected gradient method (PGM), and wasserstein risk minimization (WRM), _i.e., \(adv\)_ = FGSM, PGM, and WRM. They provided three different bounds for these adversarial attacks respectively. However, methods for generating these adversarial examples are becoming significantly more sophisticated and powerful. For example, Autoattack (Croce and Hein, 2020) in default settings is a collection of four attacks to find adversarial examples. Therefore, a bound of robust margin loss against a single attack provides a limited robustness guarantee to a machine learning model. In fact, Autoattack collects different attacks to attempt and to provide a close lower estimation of \(R_{0}(f_{})\). Therefore, this paper focuses on the robust margin loss.

## 4 Robust Generalization Bound

In this section, we will first provide our main result of robust generalization.

**Theorem 1** (**Main Result:** Robust Generalization Bound).: _For any \(B,d,h,>0\), let \(f_{}:^{k}\) be a \(d\)-layer feedforward network with ReLU activations. Then, for any \(,>0\), with probability \( 1-\) over a training set of size \(m\), for any \(\), we have:_

\[R_{0}(f_{})-_{}(f_{})( d^{2}h(dh)(f_{})+}{^{2}m}}),\]

_where \((f_{})=_{i=1}^{d}\|W_{i}\|_{2}^{2}_{i=1}^{d} \|_{2}^{2}}{\|W_{i}\|_{2}^{2}}\) is the **spectral complexity of \(f_{}\)**._

Remark.Theorem 1 is presented under \(_{2}\) attacks to simplify the notation. For other \(_{p}\) attacks, suppose all the samples \(x_{i}\) have \(_{p}\) norm bounded by \(B\) and \(\|\|_{p}\), the robust generalization bound is to replace \((B+)\) by \(\{1,n^{-}\}(B+)\) in Theorem 1, where \(n\) is the dimension of the samples \(x_{i}\).

Theorem 1 provides the first PAC-Bayesian bound in adversarial robustness settings without introducing new assumptions. Fixing other factors, the generalization gap goes to 0 as \(m\).

**Theorem 2** (Standard Generalization Bound (Neyshabur et al., 2017b)).: _For any \(B,d,h>0\), let \(f_{}:^{k}\) be a \(d\)-layer feedforward network with ReLU activations. Then, for any \(,>0\), with probability \( 1-\) over a training set of size \(m\), for any \(\), we have:_

\[L_{0}(f_{})-_{}(f_{}) (d^{2}h(dh)(f_{})+} {^{2}m}}),\]

_where \((f_{})=_{i=1}^{d}\|W_{i}\|_{2}^{2}_{i=1}^{d} \|_{2}^{2}}{\|W_{i}\|_{2}^{2}}\)._

Comparison with Existing Standard Generalization Bounds.Comparing the robust generalization bound in Theorem 1 with the standard generalization bound in Theorem 2, the only difference is a factor of the attack intensity \(\), which is unavoidable in adversarial settings. In other words. \(B\) and \(B+\) are the magnitudes of the clean and adversarial examples, respectively. Therefore, our main result is as tight as the standard generalization bound in Theorem 2.

**Theorem 3** (Robust Generalization Bound (Famia et al., 2018)).: _For any \(B,d,h>0\), let \(f_{}:^{k}\) be a \(d\)-layer feedforward network with ReLU activations. Consider an FGM attack with noise power \(\) according to Euclidean norm \(\|\|_{2}\). Assume that \(\|_{}(f_{}(),y)\|\), \(\)\(\)-close to \(X\). Then, for any \(,>0\), with probability \( 1-\) over a training set of size \(m\), for any \(\), we have:_

\[R_{0}^{adv}(f_{})-_{}^{adv}(f_{}) (d^{2}h(dh)^{fgm}(f_{ })+}{^{2}m}}),\]

\[^{fgm}(f_{})=_{i=1}^{d}\|W_{i}\|_{2}^{2}(1+C^{ fgm})_{i=1}^{d}\|_{F}^{2}}{\|W_{i}\|_{2}^{2}}, C^{fgm}=(_{i=1}^{d}\|W_{i}\|_{2})( _{i=1}^{d}_{j=1}^{i}\|W_{j}\|_{2}).\]

Remark:For robust generalization bounds of PGM or WRM adversarial attacks, the bounds have similar forms as in Theorem 3, with different constants \(C^{pgm}\) and \(C^{wrm}\).

Comparison with Existing Robust Generalization Bounds.Comparing Theorem 1 and Theorem 3, the difference of the upper bounds is the difference of \(\) and \(^{fgm}\), where \(^{fgm}\) contains an additional term \(C^{fgm}\). Therefore, our bound is tighter. Moreover, the robust generalization gap is much larger than the FGSM generalization gap based on the observation in practice. We provide a tighter upper bound for a larger generalization gap.

Additionally, the term \(C^{fgm}\) could be very large. Notice that Theorem 3 requires \((f_{}(),y)\) to be sharp w.r.t. \(\) for all \(\). It is hard to verify and \(\) could be small. Therefore, if we remove the additional assumption \(\|_{}(f_{}(),y)\|\), we have \(C^{fgm}+\) as \( 0\) and the upper bound in Theorem 3 goes to infinity.

It is also worth noting that our bound is tighter than other norm-based robust generalization bounds derived in Rademacher complexity and covering number approaches, since these bounds are larger than their standard counterpart, the bound given by (Bartlett et al., 2017).

## 5 Analysis of Adversarially Robust Generalization

As mentioned in the introduction, the robust generalization gap is much larger than the standard generalization gap in practical scenarios. What factors contribute to such a significant difference? Previous norm-based bounds might lead to the following hypothesis: The significant disparity could potentially be attributed to the additional terms or assumptions between the standard bound and the robust bound. Our result provides a different perspective: They are solely due to mathematical considerations. The following three factors are (implicitly) different in Theorem 1 and Theorem 2 and possibly contribute to the significant disparity.

Clean Sample and Adversarial Example (\(B\) and \(B+\)).The only difference between the bounds in Theorem 1 and Theorem 2 lies in the factor \(\). In this context, \(B\) represents the magnitude of clean samples, while \(B+\) signifies the magnitude of adversarial examples. This factor holds less significance in improving robust generalization, as it is unlikely to be controlled during the training of DNNs.

Standard Margin and Robust Margin (\(\)).The margin \(\) remains consistent in both of these two bounds, but it is implicitly different in the definitions of standard margin loss and robust margin loss. The robust margin is smaller due to the smaller distance between two adversarial examples. As it is discussed in (Neyshabur et al., 2017), \(\) is usually considered to normalize the spectral complexity discussed below.

Standard-Trained and Adversarially-Trained Parameters (\((f_{})\)).The spectral complexity \((f_{})\) is implicitly different because the weights \(\) of the standard-trained and adversarially-trained models are distinct. The spectral complexity \((f_{})\) induced by adversarial training is significantly larger. We conducted experiments training MNIST, CIFAR-10, and CIFAR-100 datasets on VGG networks, see Appendix C. See also the work of (Xiao et al., 2022) for more discussion about the experiments of weights norm of adversarially-trained models. The margin-normalized spectral complexity \((f_{})\) likely contributes to the huge difference between standard generalization and robust generalization.

Main Challenge of Robust Generalization Bound and Proof Sketch

### PAC-Bayesian Framework

The PAC-Bayesian framework (McAllester, 1999) provides generalization guarantees for randomized predictors drawn from a learned distribution \(Q\) (as opposed to a single predictor) that depends on the training data set. In particular, let \(f_{}\) be a predictor parameterized by \(\). We consider the distribution \(Q\) over predictors of the form \(f_{+}\), where \(\) is a random variable and \(\) is considered to be fixed. Given a prior distribution \(P\) over the set of predictors that is independent of the training data, the PAC-Bayes theorem states that with probability at least \(1-\), the expected loss of \(f_{+}\) can be bounded as follows

\[_{}[L_{0}(f_{+})]_{ }[_{0}(f_{+})]+2+\|P)+)}{m-1}}.\] (5)

To get a bound on the margin loss \(L_{0}(f_{})\) for a single predictor \(f_{}\), we need to relate the expected loss, \(_{}[L_{0}(f_{+})]\) over a distribution \(Q\), with the loss \(L_{0}(f_{})\) for a single model. The following lemma provides this relation.

**Lemma 4** (Neyshabur et al. (2017)).: _Let \(f_{}():^{k}\) be any predictor (not necessarily a neural network) with parameters \(\), and \(P\) be any distribution on the parameters that is independent of the training data. Then, for any \(,>0\), with probability \( 1-\) over the training set of size \(m\), for any \(\), and any random perturbation \(\) s.t. \(_{}[_{}|f_{+ }()-f_{}()|_{}< ]\), we have:_

\[L_{0}(f_{})_{}(f_{})+4+\|P)+}{m-1}}.\]

As it is discussed in (Neyshabur et al., 2017), the KL-divergence is evaluated for a fixed \(\) and \(\) is random. Lemma 4 is not specific to neural networks and generally holds for any functions. Providing Lemma 4, it is left to provide a bound of \(\|f_{+}()-f_{}()\|_{2}\) to obtain the final generalization bound.3 This framework can be directly extended to adversarially robust settings by replacing \(\|f_{+}()-f_{}()\|_{2}\) by \(\|f_{+}(+_{+}^{adv}( ))-f_{}(+_{}^{adv}()) \|_{2}\)(Farnia et al., 2018). For more details, see Appendix B.

### Main Challenge

Based on Lemma 4, to provide an upper bound of robust margin loss is to solve the following problem:

**Problem 1**.: _How to provide a bound of_

\[\|f_{+}(+_{+}^{adv}( ))-f_{}(+_{}^{adv}()) \|_{2}?\] (6)

We refer to the gap in Eq. (6) as robust weight perturbation. To the best of our knowledge, it remains unclear how to establish a bound for robust weight perturbation. In standard settings, when we perturb the weights from \(\) to \(+\), the input \(\) remains the same. The change in function values is solely attributable to the change in weights. However, the situation becomes much more complex in adversarial settings. If we perturb the weights from \(\) to \(+\), the adversarial attacks also vary from \(_{}^{adv}()\) to \(_{+}^{adv}()\). The combined changes in input \(\) and weights \(\) may result in a substantial change in function values. The challenge of Problem 1 can be observed in previous studies.

Farnia et al. (2018) introduced additional assumptions to bound Eq. (6). For instance, for FGSM and PGM attacks, they assumed \(|_{}(f_{}(),y)|\) for all \(\)\(\)-close to \(\). This parameter \(\) appears in the bound of Eq. (6) as well as in the final generalization bound. To the best of our knowledge, there has been no attempt at \(_{}^{*}()\). It is not because such research is unimportant (as mentioned in Sec. 3), but rather due to the challenge presented by Problem 1. In this case, it remains unclear what assumptions can be made to bound Eq. (6). The related work on Rademacher complexity analysis demonstrates the difficulty, as researchers have found it challenging to bound robust margin loss and have instead resorted to bounding robust loss against sodel attack with additional assumptions. Further discussion on this topic can be found in Sec. 2.

Our solution to this problem consists of two steps. Step 1: We recognize that a general and reasonable bound for Eq. (6) without additional assumptions may not exist. To address this, we establish a bound for a similar expression, namely the weight perturbation of margin operator, without requiring any additional assumptions. To develop this bound, we introduce a generalization framework called "Perturbation Bounds of Robustified Function", which can be further extended to analyze other neural network structures. Step 2: We modify Lemma 4 to incorporate the weight perturbation bound that we have introduced. By combining these two steps, we are able to address the challenges and provide a robust generalization bound.

### Perturbation Bounds of Robustified Function

In this section, we consider functions \(g_{}()\) parameterized by the weights of a neural network. We mainly consider scalar value functions \(g_{}():\). For example, \(g_{}()\) can be the \(i^{th}\) output of a neural network \(f_{}()[i]\), the margin operator \(f_{}()[y]-_{j y}f_{}()[j]\), or the robust margin operator.

**Definition 1** (Local Perturbation Bounds).: Given \(\), we say \(g_{}()\) has a \((L_{1},,L_{d})\)-local perturbation bound w.r.t. \(\), if

\[|g_{}()-g_{^{}}()|_{i= 1}^{d}L_{i}\|W_{i}-W_{i}^{}\|,\] (7)

where \(L_{i}\) can be related to \(\), \(^{}\) and \(\).

Eq. (7) controls the change of the output of functions \(g_{}()\) given a slight perturbation on the weights of DNNs. The following Lemma is the key Lemma to estimate perturbation bounds of the robustified function, which is defined as \(_{\|-^{}\|}g_{}( ^{})\). The reason why we require \(g_{}()\) to be scalar functions is that we can define their corresponding robustified functions.

**Lemma 5** (**Key Lemma)**.: _if \(g_{}()\) has a \((A_{1}||,,A_{d}||)\)-local perturbation bound, i.e.,_

\[|g_{}()-g_{^{}}()|_{i= 1}^{d}A_{i}||\|W_{i}-W_{i}^{}\|,\]

_the robustified function \(_{\|-^{}\|}g_{}(^{})\) has a \((A_{1}(||+),,A_{d}(||+))\)-local perturbation bound._

Proof: Let \(()=_{\|-^{}\| }g_{}(^{}),(^{ })=_{\|-^{}\|}g_{^{}}(^{}),\) Then,

\[|_{\|-^{}\|}g_{}(^{})-_{\|-^{}\|}g_{^ {}}(^{})|\{|g_{}(() )-g_{^{}}(())|,|g_{}((^{}))-g_{^{}}((^{}) )|\}.\]

It is because \(g_{}(())-g_{^{}}(( ^{})) g_{}((^{}))-g_{ ^{}}((^{}))\) and \(g_{^{}}((^{}))-g_{}( ()) g_{^{}}(())-g_ {}(())\). Therefore,

\[|_{\|-^{}\|}g_{}(^{})-_{\|-^{}\|}g_{^ {}}(^{})|_{i=1}^{d}A_{i}|() \|W_{i}-W_{i}^{}\|_{i=1}^{d}A_{i}(||+)\|W_{i} -W_{i}^{}\|.\]

Lemma 5 shows that the local perturbation bound of the robustified function \(_{\|-^{}\|}g_{}(^{})\) can be estimated by the local perturbation bound of the function \(g_{}()\), which is the key to provide robust generalization bounds.

### Perturbation Bounds of Margin Operator

It should be noted that Lemma 5 is unable to provide a bound for Problem 1. In order to utilize Lemma 5, we shift our focus to the margin operator, which is a scalar function.

Margin Operator.Following the notation of (Bartlett et al., 2017), we define the margin operator of the true label \(y\) given \(\) and of a pair of two classes \((i,j)\) as

\[M(f_{}(),y)=f_{}()[y]-_{j y}f_{ }()[j],\ M(f_{}(),i,j)=f_{}( )[i]-f_{}()[j].\]Robust Margin Operator.Similarly, we define the robust margin operator of the true label \(y\) and of a pair of two classes \((i,j)\) given \(\) as

\[RM(f_{}(),y)=_{\|-^{}\| }(f_{}(^{})[y]-_{j y}f_{}( ^{})[j]),\]

\[RM(f_{}(),i,j)=_{\|-^{}\| }(f_{}(^{})[i]-f_{}(^{})[j]),\]

respectively. Based on Lemma 5, it is left to provide the form of \(A_{i}\) for the margin operator.

**Lemma 6**.: _Let \(f_{}\) be a \(d\)-layer neural networks with Relu activation. The following local perturbation bounds hold._

1. _Given_ \(\) _and_ \(i,j\)_, the margin operator_ \(M(f_{}(),i,j)\) _has a_ \((A_{1}||,,A_{d}||)\)_-local perturbation bound w.r.t._ \(w\)_, where_ \(A_{i}=2e_{l=1}^{d}\|W_{l}\|_{2}/\|W_{i}\|_{2}.\) _And_ \[|M(f_{+}(),i,j)-M(f_{}(),i,j)| 2eB_{l=1}^{d}\|W_{l}\|_{2}_{i=1}^{d} \|_{2}}{\|W_{i}\|_{2}}.\] (8)
2. _Given_ \(\) _and_ \(i,j\)_, the robust margin operator_ \(RM(f_{}(),i,j)\) _has a locally_ \((A_{1}(||+),,A_{d}(||+))\)_-local perturbation bound w.r.t._ \(w\)_. And_ \[|RM(f_{+}(),i,j)-RM(f_{}(),i,j)| 2e(B+)_{l=1}^{d}\|W_{l}\|_{2} _{i=1}^{d}\|_{2}}{\|W_{i}\|_{2}}.\] (9)

The proof of Lemma 6.1 is adopted from Lemma 2 in (Neyshabur et al., 2017), and the proof of Lemma 6.2 is a combination of Lemma 5 and Lemma 6.1. It is important to note that Eq. (9) provides a bound for a similar but different form of robust weight perturbation compared to Eq. (6), indicating that Problem 1 has not been fully resolved. However, we are fortunate that the subsequent lemma demonstrates that Eq. (9) is sufficient to yield the final robust generalization bound.

**Lemma 7**.: _Let \(f_{}():^{k}\) be any predictor with parameters \(\), and \(P\) be any distribution on the parameters that is independent of the training data. Then, for any \(,>0\), with probability \( 1-\) over the training set of size \(m\), for any \(\), and any random perturbation \(\) s.t._

1. \(_{}[_{i,j[k],}|M(f_{ +}(),i,j)-M(f_{}(),i,j)|< ]\)_, we have:_ \[L_{0}(f_{})_{}(f_{})+4+\|P)+}{m-1}}.\]
2. \(_{}[_{i,j[k],}|RM(f_{ +}(),i,j)-RM(f_{}(),i,j)|< ]\)_, we have:_ \[R_{0}(f_{})_{}(f_{})+4+\|P)+}{m-1}}.\]

Remark:Lemma 7 shows that we can replace the robust weight perturbation (Eq. (6)) by the weight perturbation of the robust margin operator. The proof is deferred to the Appendix.

Now that we have established the complete framework of the _perturbation bound of robustified function_ to derive the robust generalization bound, we are ready to prove Theorem 1. By following the proof of (Neyshabur et al., 2017), we can replicate the standard generalization bound by combining Lemma 6.1 and 7.1. Similarly, we can obtain the robust generalization bound by combining Lemma 6.2 and 7.2. The flowchart illustrating this process is presented in Figure 2. Additionally, Lemma 5 serves as a crucial link between the robust margin operator and the margin operator, thus establishing the connection between the robust generalization bound and the standard generalization bound.

Extension of the Main Result

The provided framework allows us to extend the result to 1) general non-\(_{p}\) adversarial attacks and 2) other neural network structures.

Extension to Non-\(_{p}\) Adversarial Attacks.Even though most of the adversarial robustness studies focused on norm-bounded attacks, real-world attacks are not restricted in the \(_{p}\)-ball. We consider the following general adversarial attack problem:

\[_{^{} C()}(f_{}(^{ }),y),\]

where \(C()\) can be any reasonable constraint given the original example \(\). Assume that \(_{x S}_{^{} C()}|^{}|=D\). In words, the norm of the adversarial examples is bounded by \(D\).

**Theorem 8** (Robust Generalization Bound for non-\(_{p}\) attack.).: _For any \(D,d,h\), let \(f_{}:^{k}\) be a \(d\)-layer feedforward network with ReLU activations. Then, for any \(,>0\), with probability \( 1-\) over a training set of size \(m\), for any \(\), we have:_

\[R_{0}^{nl}(f_{})-_{}^{nl}(f_{}) (d^{2}h(dh)(f_{})+}{^{2}m}}),\]

_where \((f_{})=_{i=1}^{d}\|W_{i}\|_{2}^{2}_{i=1}^{d} \|_{F}^{2}}{\|W_{i}\|_{2}^{2}}\) and nl stands for non-\(_{p}\) adversarial attacks._

The proof is based on a slight modification of Lemma 5.

Extension to Other Neural Networks Structure.The framework we have established enables us to extend the PAC-Bayesian generalization bound from standard settings to robust settings, provided that the standard generalization bound is also obtained using this framework. Importantly, this extension is independent of the structure of the neural networks.

ResNet.Consider a neural network: \(f_{}^{1}()=W_{1}\) and \(f_{}^{i}()=W_{i}(f_{}^{i-1}())+f_ {}^{i-1}()\). ResNet in practice could be complicated. We use this structure for illustration.

**Theorem 9** (Robust Generalization Bound for ResNet).: _For any \(D,d,h\), let \(f_{}:^{k}\) be a \(d\)-layer ResNet with ReLU activations. Then, for any \(,>0\), with probability \( 1-\) over a training set of size \(m\), for any \(\), we have:_

\[R_{0}(f_{})-_{}(f_{})( d^{2}h(dh)(f_{})+}{^{2}m}}),\]

_where \((f_{})=_{i=1}^{d}(\|W_{i}\|_{2}+1)^{2}_{i=1}^{ d}\|_{2}^{2}}{(\|W_{i}\|_{2}+1)^{2}}\)._

## 8 Conclusion

Limitation.The primary limitation lies in the fact that norm-based bounds tend to be excessively large in practical scenarios. As illustrated in Table 1, the bounds for VGG networks surpass \(10^{9}\) in the experiments on CIFAR-10 dataset. The challenge at hand is how to achieve smaller norm-based bounds in practical contexts, not only in adversarial settings but also in standard settings. This remains an open problem.

In this paper, we introduce a PAC-Bayesian spectrally-normalized robust generalization bound. The proof is constructed based on the framework of the perturbation bound of the robustified function. This established framework enables us to extend the generalization bound from standard settings to robust settings, as well as to generalize the results to encompass various adversarial attacks and DNN architectures. The simplicity of this framework makes it a valuable tool for analyzing robust generalization in machine learning.