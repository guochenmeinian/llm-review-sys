# Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training

Cheng Luo

California Institute of Technology

chengluo@caltech.edu

&Jiawei Zhao

Meta FAIR

jwzhao@meta.com

&Zhuoming Chen

Carnegie Mellon University

zhuominc@andrew.cmu.edu

&Beidi Chen

Carnegie Mellon University

beidic@andrew.cmu.edu

&Anima Anandkumar

California Institute of Technology

anima@caltech.edu

###### Abstract

We introduce Mini-Sequence Transformer (MsT), a simple and effective methodology for highly efficient and accurate LLM training with extremely long sequences. MsT partitions input sequences and iteratively processes mini-sequences to reduce intermediate memory usage. Integrated with activation recomputation, it enables significant memory savings in both forward and backward passes. In experiments with the Llama3-8B model, with MsT, we measure no degradation in throughput or convergence even with 12x longer sequences than standard implementations. MsT is fully general, implementation-agnostic, and requires minimal code changes to integrate with existing LLM training frameworks. Integrated with the huggingface library, MsT successfully extends the maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.

## 1 Introduction

The development of Transformer  has been a remarkable journey, with each iteration pushing the boundaries of what is possible regarding model size, performance, and efficiency. One of the critical challenges in this journey has been managing the memory requirements of these models, particularly during training. As Transformers have significantly grown in size and complexity , the memory demand has increased exponentially, necessitating innovative solutions to optimize memory usage while maintaining performance.

A significant milestone in this journey was the introduction of multi-query attention . This technique dramatically reduced the size of the KV-cache during inference, which uses multiple query heads but single key and value heads. The idea was first adopted in the large-scale training of PaLM , then adopted and empirically tested in LLaMA . As the field progressed, multi-query attention evolved into grouped query attention (GQA) , which relaxes the single key and value head restriction to multiple heads, and each head is coupled with a group of queries. It significantly improves the quality and is adopted by Llama2-70B  and Mistral-7B .

To further improve model quality, Llama3  introduced a tokenizer with a vocabulary of 128K tokens, enabling more efficient language encoding than Llama2's 32K vocabulary. Additionally, Llama3 increased its MLP intermediate size from 11k to 14k. These changes reflect a trend toward more extensive vocabulary and intermediate sizes for better quality. Meanwhile, Llama3 maintains its hidden size of 4k for inference efficiency. This trend is also reflected in the Microsoft development of Phi-3  compared with Phi-2 .

These advancements have also brought about new memory challenges, particularly in the intermediate value of linear layers of multilayer perception (MLP) and language modeling head (LM-Head). The substantial increase in intermediate variables, which can be nearly ten times larger than the input variables, has severely limited the network's ability to expand sequence length and batch size. This limitation has made it difficult to train large models without restricting sequence length to 8K or relying on gradient accumulation or distributed systems to expand batch size.

**Our Approach:** Recognizing these challenges, we introduce Mini-Sequence Transformer (MsT), a simple and effective methodology for enabling highly efficient and highly accurate LLM training with extremely long sequence lengths by reducing intermediate memory overhead. MsT introduces a per-layer mini-sequence where the input partitions work for each MLP and LM-Head block. MsT partitions individual samples along the sequence dimension and iteratively processes each mini-sequence, combining all mini-sequence results to recover full-sequence outputs for these blocks. Our work also adopts activation recomputation . We find no degradation in throughput or convergence even with sequences up to \(12\) compared to a standard implementation of Llama3-8B, as shown in Figure 1(c).

To summarize, we make the following contributions to advance the long-sequence training:

* MsT trains \(12-24\) longer sequence lengths than existing systems on a single A100 GPU with no degradation in throughput and convergence of training.
* Fully general and implementation agnostic: MsT supports most parameter-efficient training as it works independently with attention layers.
* Support for large-scale distributed training: MsT works together with DeepSpeed-Ulysses  to support linear scaling sequence length by the number of GPUs.
* Easy-to-use and portable, requiring minimal code changes to the existing training frameworks like Huggingface . The details can be referred to Appendix G.

In subsequent sections, we provide background and related work, a detailed discussion of Mini-Sequence Transformer (MsT) design, Hardware-efficient analysis, experimental evaluation, and comparison with existing work. This work is open-source under an MIT license on https://github.com/wdlctc/mini-s.

Figure 1: (a) Standard Transformer architecture. MLP’s and LM-Head’s activation sequence length is annotated with \(S\). (b) Mini-Sequence Transformer is used to replace MLP blocks and LM-Head block, which splits the input sequence \(S\) into \(M\) mini-sequences with sequence length \(S/M\), where \(M=2\) on this figure. (c) Max sequence size for training Llama2/Llama3 on A100-80GB GPU, with no degradation of throughput or convergence using our approach.

Background and Related Work

This section briefly overviews the performance characteristics of long sequence transformers on modern hardware(e.g., GPUs). We also describe some backgrounds of mini-batch training and activation recomputation, which inspire our work.

### Transformer Architecture

Figure 1(a) is a sketch of the building blocks of a typical Transformer architecture . It consists of input sequences \(S\) sent into \(L\) repeated block with attention and MLP, then computed output loss with LM-Head block. The inputs and outputs of each block are typically a 3D tensor of size \((B,S,d)\) where \(B\) is micro batch size, \(S\) is sequence length, and \(d\) is hidden dimension. The intermediate value includes the \(Q,K,V\) tensors of size \((B,S,d)\) within the attention block, the \(I\) tensor of size \((B,S,I)\) within the MLP block, and the logits tensor of size \((B,S,V)\) of within LM-Head block. Here, \(I\) represents the intermediate size of MLP, and \(V\) represents the vocabulary size.

### Hardware Performance of Long Sequence Training

**Memory Hierarchy.** GPUs have a memory hierarchy with larger but slower global GPU memory (high bandwidth memory; HBM) and smaller but faster-shared memory (SRAM). Transformers' high memory demand originates from the quadratic complexity of self-attention operations, where the memory needed to store attention scores for each token increases quadratically as the sequence length grows. This dramatic increase in memory demand can quickly overwhelm the capacity of the HBM, leading to OOM issues. Flashattention  uses kernel fusion to effectively mitigate the memory overheads associated with the quadratic growth in sequence length, and Xformer  deploys optimized memory access patterns that achieve linear memory scaling. Our work is partly inspired by memory optimization technologies, where our optimization targets are MLP and LM-Head.

**Occupancy.** GPUs have many threads executed in parallel; threads are grouped into thread blocks, which execute on streaming multiprocessors (SMs). Modern hardware has specialized units like tensor cores on NVIDIA GPU to accelerate mammals. In long sequence training scenarios where the sequence size tends to be long (>10k), parallelizing over the sequence dimension usually enables high GPU occupancy.

**Performance characteristics.** GPU operators can be classified as either compute-bound or memory-bound, which is determined by the time spent in arithmetic operations and the time spent accessing HBM. Typical self-attention with long sequence, MLP with the long intermediate size is a compute-bound operator because their core operators are matrix-multiply with a large inner dimension of sequence length. Then, cross-entropy with reduction is memory-bound.

### Mini-Batch Training

Our work is inspired by Mini-Batch Training algorithms, also known as gradient accumulation. Mini-batch training algorithms [17; 40] can support large batch size by processing the training batch in smaller mini-batches, which allows the model to be trained on a subset of the data at a time, accumulating gradients over several mini-batch and only updating the parameter with accumulated gradient. This reduces the memory requirements compared to batch gradient descent , which enables training bigger batch sizes than GPU memory constrain. We are inspired by the idea and adapt it to train long sequences instead of large batch sizes.

### Activation Recomputation

Activation recomputation , also known as gradient checkpointing, is a memory-saving technique for training large neural networks. This method trades computation for memory by discarding intermediate activations during the forward pass and recomputing them as needed during the backward pass. In standard training, all activations must be stored to compute gradients, which can lead to significant memory usage for large models or long sequences. Activation recomputation is orthogonal with our MsT, and we integrate this method for better optimizing intermediate value. We analyze the memory efficiency of activation recomputation and its integration with MsT on Sec 3.2.

## 3 Mini-Sequence Transformer (MsT): Algorithm, Analysis, and Distributed Extensions

We present our Mini-Sequence Transformer (MsT) mechanism to partition the input sequence into \(M\) mini-sequences. We show how to compute the exact transformer block by gradient accumulation during the backward pass. Then, we analyze its memory efficiency and IO complexity, showing that our method is memory-efficient and throughput-equalized compared to the standard transformer. Based on the analysis, we found the optimal implementation of MsT by selecting the best hyperparameters. We further show how MsT can work on distributed settings by integrating with DeepSpeed .

We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.

### Algorithms: Optimizing Intermediate Memory With Mini-Sequence Processing

Our idea arises from the observation of large intermediate values from transformer blocks. Given the inputs \(X^{N d}\) in HBM, attention blocks and MLP blocks compute the output \(O^{N d}\) and LM-head block computes the output \(loss^{1}\), \(N\) equals to sequence size \(S\) here. We observe that the intermediate values are always larger than the input \(X\) and output \(O\), \(loss\), illustrated in Table 1. Attention has intermediate values \(,,^{N d}\), which is \((1+2 d)/G\) larger than input size, where \((1+2 d/G=1.5)\) in Llama3 setting. \(G\) refers to the number of grouped query attention (GQA). MLP has intermediate value \(I_{up},I_{gate}^{N I}\), where \(2 I/d=7\) in Llama3 setting. LM-Head has \(logits^{V d}\), where \(V/d=32\) in Llama3 setting. The detail setting of Llama3-8B is listed in Appendix C

As flash attention and group query attention have minimized the intermediate value of attention, we put our focus on the MLP block and LM-Head block. Therefore, our implementation of MsT is general enough to work with any attention: self-attention , cross-attention , causal attention , their sparse counterparts , and their various optimized kernels such as different versions of FlashAttention . Our implementation adopts FlashAttention2  for the experiments.

Input Partition.We apply the mini-sequence technique to overcome the technical challenge of large intermediate values occupying HBM memory. We describe this in Algorithms 1, and 2, which represent MLP blocks and LM-Head from Llama serials. Their MLP block consists of three linear layers and SiLU function , and their LM-Head block consists of one linear layer and CrossEntropyLoss function. The corresponding backward implementations can be referred to in Appendix B for more details. The main idea is to partition the input \(X\) into mini-sequence \(X_{i}\) as Algorithm 1 line 1 and Algorithm 2 line 1, then compute the output with respect to those mini-sequences. We get the exact same result as standard implementation by contacting all mini-sequence outputs.

Gradient Accumulation.One of our goals is to reduce intermediate values for backward passes. The backward pass typically requires the matrices \(X^{N d}\), \(I^{N I}\), \(logits^{N V}\) to compute the gradients with respect to weights. However, by input partition the \(X^{N_{m} d}\), we can reduce the intermediate value as \(I^{R_{m} I}\), \(logits^{N_{m} V}\) by \(M\) in the backward pass in HBM. With gradient accumulation for all mini-sequences, all gradients are generated in the same way as standard implementation by introducing more memory loading time. However, as MLP is the standard computation-bound operator and LM-Head occupies only a small amount of total training time, MsT would not affect the whole training speed with a significant reduction in memory overhead.

 Transformer Blocks & Input/Output Size & Peak Intermediate Value Size & Intermediate/Input Ratio 1  \\  Attention & \((B,S,d)/(B,S,d)\) & \((B,S,d)+2(B,S,d/G)\) & \((1+2 d/G) 1.5\) \\ MLP & \((B,S,d)/(B,S,d)\) & \(2(B,S,I)\) & \((2 I)/d 7\) \\ LM-Head & \((B,S,d)/1\) & \((B,S,V)\) & \(V/d 32\) \\ 

Table 1: Intermediate value size analysis for transformer blocks```
0: Matrices \(X^{N d}\), MLP block, \(W_{down},\ ^{I d},\) Weights of three linear layers \(W_{gate},W_{up}^{d I}\), \(W_{down}^{I d}\)
1: Partition matrices \(X\) into \(M\) blocks \(X_{1},,X_{m}\) of size \(N_{m} d\), where \(N_{m}=N/M\)
2:for\(1 i M\)do
3: Compute \(^{}_{i}=MLP(X_{i},W_{gate},W_{up},W_{down})\), \(_{i}^{N_{m} d}\)
4:endfor
5: Contact \(=\{^{}_{i},,^{}_{m}\} ^{N d}\)
6: Return \(\). ```

**Algorithm 1** Mini-Sequence MLP

### Analysis: Memory Efficiency of Mini-Sequence Transformer (Mst)

We analyze the memory efficiency of Mst. MST can reduce intermediate value by \(M\) while maintaining the same throughput performance.

**Theorem 1**.: _Let \(S\) be the sequence length, \(W_{mem}\) be the weight memory occupation, including weights, gradient, and optimizer. \(A_{mem}\) be the activation memory occupation per sequence, \(I_{mem}\) be the intermediate memory occupation per sequence. The peak memory of the standard transformer is achieved by \(M=W_{mem}+S(I_{mem}+L A_{mem})\). Note that \(L A_{mem}>>I_{mem}\) for standard transformer, as \(A_{mem}\) lasts for all \(L\) layers, but \(I_{mem}\) only lasts for one layer._

**Theorem 2**.: _With OpenAI's activation recomputation, the \(L A_{mem}\) could be reduced to \(sqrt(L) A_{mem}\). Therefore the peak memory is reduced to \(M=W_{mem}+S(I_{mem}+sqrt(L) A_{mem})\). For models with a large vocabulary and MLP intermediate, \(sqrt(L) A_{mem}<I_{mem}\)._

**Theorem 3**.: _MST can reduce intermediate value by \(M\), so the memory occupation becomes \(M=W_{mem}+S(I_{mem}/M+sqrt(L)\ timesA_{mem})\). For GPU with maximum memory \(M_{max}\), the maximum sequences length is contained by \(S_{max}=-W_{mem})}{(I_{mem}/M+sqrt(L) A_{mem})}\). This sequence length would be much longer than the standard implementation with \(S_{max}=-W_{mem})}{(I_{mem}+L A_{mem})}\)._

### Analysis: IO Complexity and Memory of Mini-Sequence Transformer (Mst)

We analyze the IO complexity of Mst, compared with consistent compute complexity, which can affect its compute-bound or memory-bound performance characteristics.

**Theorem 4**.: _Let \(S\) be the sequence length, \(d\) be the hidden dimension, \(I\) be the intermediate size, and \(V\) be the voice size. Standard MLP returns \(O=act((XW_{gate})*(X_{i}W_{up}))*W_{down}\) with \(O(SdI)\) FLOPS and Mst MLP returns \(O(SdI/M*M)=O(SdI)\) FLOPS. Standard LM-Loss returns \(loss=crossentropyloss(XW,L)\) with \(O(SdV+SV)\) FLOPS, and Mst LM-Loss returns \(O((SdV+SV)/M*M)=O(SdV+SV)\) FLOPS._

**Theorem 5**.: _Standard MLP requires \((Sd+SI+dI)\) HBM accesses, while Mst (1) requires \((Sd+SI+dIM)\) HBM accesses. Standard LM-Head requires \((Sd+SV+dV)\) HBM accesses, while Mst (2) requires \((Sd+SV+dVM)\) HBM accesses._

For Llama3 values of \(d\) (4096), \(I\) (14336) and \(V\) (128256), \(SI\), \(Sv\) is many time larger than \(Sd\). For long sequence cases, the compute complexity and IO complexity are dominated by \(SI\) and \(SV\), where Mst is close to standard implementation. However, for small sequence cases where \(S<<d\), the compute complexity and IO complexity are dominated by \(dI\) and \(dV\) while Mst needs \(dIM\) and \(dVM\). Therefore, Mst would cause throughput downgrades for small sequence lengths.

### Chunk-based Mini-Sequence Transformer (MsT)

We present an optimized implementation of chunk-based MsT designed to mitigate throughput reductions when training with small sequence data. The fundamental approach involves partitioning sequences \(S\) into equally sized chunks of size \(C\) (when possible), resulting in \(M=S/C\) mini-sequences.

Our IO complexity analysis indicates that the number of mini-sequences \(M\) influences the HBM accesses as \((Sd+SI+dIM)\) and \((Sd+SV+dVM)\). However, the HBM accesses remain stable at \((SI)\) and \((SV)\) provided that \(dIM SI\) and \(dVM SV\). It means \(d S/M\).

Therefore, by setting the chunk size to \(C=S/M d\), MST avoids throughput downgrades for small sequences. Intuitively, when the sequence size is smaller than the chunk size, MST does not split the input, thereby preventing any performance loss.

We apply chunk-based MsT exclusively to MLP blocks by setting a constant chunk size \(C\) equal to the hidden dimension, \(C=d\). For LM-head blocks, we maintain a constant mini-sequence size of \(M=V/d\), as these blocks contribute minimally to the overall training time of transformers.

### Extension: Distributed Mini-Sequence Transformer (MsT)

We extend Mini-Sequence Transformer (MsT) to the distributed setting: we propose MsT + SP, which can effectively scale the transformer using sequence parallelism(SP). In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements. It is worth noting that our proposed approach is orthogonal to most sequence parallelism, such as Megatron-LM , Deepspeed-Ulysses , Sequence parallelism , and Ring Attention . Here, we take Deepspeed-Ulysses as an example of how they work together.

Figure 2 shows the design of extending MsT with DeepSpeed-Ulysses. As with the transformers architecture, the design consists of an attention block with DeepSpeed-Ulysses, MLP, and LM-Head with MsT's mini-sequence technology. The design consists of input sequences \(S\) partitioned across available devices and mini-sequences. Each attention block Matrices \(\), \(\), \(\) are communicated through all-to-all collectives before and after the attention computation. The remaining modules of MLP and LM-Head use the sequence parallel and mini-sequence together. As DeepSpeed-Ulysses's main change is working on attention block and MsT is working on MLP and LM-Head, it is straightforward to make them work together to scale sequence length.

## 4 Experiment

We evaluate the impact of using chunk-based Mini-Sequence Transformer (MsT) on Llama3, a state-of-the-art model for many NLP tasks. We also evaluate Qwen , Mistral , and Gemma-2  for context length improvements. We validate our claims about scaling sequence length, reporting training time, and memory overhead. Distributed Extension results can be found in appendix E, which confirms that the sequence length of MsT can scale linearly with the number of GPUs.

* **Maximum Sequence Length.** MsT can train Llama3-8B with context length 60k and Llama3-7B with context length 84k on a single A100 GPU, outperforming the standard implementation by \(12\). Also, it achieves \(12-24\) than the standard implementation of Qwen, Mistral, and Gemma-2.
* **Training throughput.** MsT maintains the same training throughput compared with standard long-sequence training. Moreover, the throughput can be slightly improved with a large batch size supported by MsT.

Figure 2: Distributed Mini-Sequence Transformer.

### Longer Sequence Length with Mini-Sequence Transformer (Mst)

Llama3 and Llama2.We train a Llama3-8B Mst and Llama2 models Mst by exploring the sequence length on a single A100 GPU with lossless training strategies, such as activation recomputation, fusing the backward operation with the optimizer update  and Mst. Table 2 compares our maximum sequence and training time to the PyTorch standard implementation and Huggingface PEFT with activation recomputation. Our implementation trains \(4\) longer sequence LLAMA-3 compared with activation recomputation and \(12\) longer sequence compared with standard implementation. Also, our implementation trains \(1.8\) longer sequence compared with activation recomputation and \(12\) longer sequence compared with standard implementation.

Qwen, Mistral, and Gemma-2.We've extended our evaluation to include Mistral-7B, Qwen2-7B, and Gemma-2-9B, demonstrating significant increases in maximum sequence length (\(12\) for Mistral-7B, \(18\) for Qwen2-7B, \(24\) for Gemma-2-9B) across these architectures. Among these models, Mst provide best sequence extension for Gemma-2 of \(24\). The critical observation here is that gamma-2 uses the largest vocal size (256k) than Mistral-7B (32k) and Qwen2(152k).

Combination with gradient accumulation.Gradient Accumulation has been used during training Llama2 and Llama3, which helps them train larger batch sizes given limited available GPU memory. However, in Gradient Accumulation, instead of updating the model parameters after processing each batch of training data, the gradients are accumulated over multiple batches before updating. This means that the memory usage for gradients would occupy the memory used for activation. Therefore, using gradient accumulation during training would constrain the maximum sequence size.

Table 4 summarizes the maximum sequence length with gradient accumulation. The activation recomputation technology can train up to 8K sequences. Then Mst can train up to 30k sequence length, which is \(4\) longer sequence length than activation recomputation, and \(21\) longer than vanilla. For Llama2-7B, Mst can also train up to 55k sequence length.

Comparison and Combination with Lossy Methods.We've comprehensively compared Mst with quantization methods and the combinations between Mst and quantization on Table 5. All lossy methods are HuggingFace official implementations. This comparison demonstrates Mst's

 Model Implementation with gradient accumulation & Maximum Sequence Length (K) \\  Llama3-8B-hf vanilla & 1.5 \\ Llama3-8B-hf Activation recomputation & 8 \\ Llama3-8B-hf MstT & 32 \\  Llama2-7B-hf vanilla & 4 \\ Llama2-7B-hf activation recomputation & 38 \\ Llama2-7B-hf MstT & 55 \\  

Table 4: Maximum sequence length training with gradient accumulation.

 Llama3-8B-hf Implementation & Maximum Sequence Length (K) \\  Llama3-8B-hf vanilla & 5 \\ Llama3-8B-hf activation recomputation & 14 \\ Llama3-8B-hf Mst & 60 \\  Llama2-7B-hf vanilla & 7 \\ Llama2-7B-hf activation recomputation & 45 \\ Llama2-7B-hf Mst & 84 \\  

Table 2: Maximum sequence length of Llama3-8B and Llama2-7B.

 Model Implementations & Maximum Sequence Length (K) \\  Mistral-7B vanilla & 5 \\ Mistral-7B activation recomputation & 42 \\ Mistral-7B MST & 70 \\  Qwen2-7B vanilla & 4 \\ Qwen2-7B activation recomputation & 13 \\ Qwen2-7B MST & 74 \\  gamma-2-9b vanilla & 1.5 \\ gamma-2-9b activation recomputation & 5 \\ gamma-2-9b MST & 36 \\  

Table 3: Maximum sequence length of various models.

superiority in enabling longer sequences for Llama3 training on a single A100 GPU. Mst alone (60K tokens) outperforms these lossy approaches (4bit 28k). When combined with quantization techniques, Mst achieves even more impressive results: MstT + 8-bit reaches 110K tokens (a \(22\) improvement over standard 8-bit), while MstT + 4-bit pushes the boundary to 140K tokens. We did not evaluate the effect of quantization on training loss.

### Faster Long Sequence Training with Mini-Sequence Transformer (Mst)

We evaluate the training performance of MST on Llama3-8B with 8k sequence and Llama2-7B with 4k sequence using a single A100 80G GPU. Table 6 compares the training time per step and TFLOPS achieved by Mst with the vanilla PyTorch implementation and activation recomputation technique.

For Llama3-8B, the vanilla implementation runs out of memory (OOM) with a batch size of 1. Activation recomputation allows training with a batch size of 2, achieving 3271.42 TFLOPS and a training time of 5.01 seconds per step. Mst, with the same batch size of 2, achieves a comparable 3194.90 TFLOPS with a slightly longer training time of 5.13 seconds per step. However, Mst's memory efficiency allows scaling the batch size to 8, resulting in an improved 3386.13 TFLOPS and a training time of 19.35 seconds per step.

In the case of Llama2-7B, the vanilla implementation can train with a batch size of 1, achieving 3290.88 TFLOPS and a training time of 1.24 seconds per step. For the same batch size, Mst without activation recomputation achieves 3115.03 TFLOPS with a training time of 1.31 seconds per step, demonstrating a 16% speedup over activation recomputation (\(2684.67\) TFLOPS) and only a 5% slowdown compared to vanilla PyTorch. Mst further increases the batch size to 16, maintaining a similar 3656.17 TFLOPS with a training time of 17.92 seconds per step.

### Better Models with Longer Sequences

Language Modeling with Long Context.The memory efficiency of Mst allows us to increase the context length of llama by \(4\) than activation recomputation. Table 7 shows that training Llama3-8B with 30K context length achieved a \(2.7\) improvement in perplexity compared to the 8K baseline. We train a Llama3-8B Mst on the LongAlpaca dataset. The training lasts for two epochs and 10k steps for demonstration. For all implementation, we use the AdamW optimizer . We use a weight decay of 0.001, gradient clipping of 1.0, and a constant learning rate of 1e-4. All batch sizes equal 16, with a gradient accumulation step of 16. The bf16 precision is also deployed.

 Model Implementation & Batch Size & Training Time Per Step (s) & TFLOPS \\  Llama3-8B-hf vanilla & 1 & OOM & OOM \\ Llama3-8B-hf activation recomputation & 2 & 5.01 & 3271.42 \\ Llama3-8B-hf MstT & 2 & 5.13 & 3194.90 \\ Llama3-8B-hf MstT & 8 & 19.35 & 3386.13 \\  Llama2-7B-hf vanilla & 1 & 1.24 & 3290.88 \\ Llama2-7B-hf activation recomputation & 1 & 1.52 & 2684.67 \\ Llama2-7B-hf MstT without activation recomputation & 1 & 1.31 & 3115.03 \\ Llama2-7B-hf activation recomputation & 8 & 8.85 & 3703.48 \\ Llama2-7B-hf MstT & 8 & 9.33 & 3511.39 \\ Llama2-7B-hf MstT & 16 & 17.92 & 3656.17 \\  

Table 6: Training performance using Mst on single A100 80G GPU.

 Llama3-8B-hf Implementation & Context length & LongAlpaca-12k (ppl) & loss & Training Time \\  Activation Recputation & 8k & 9.34 & 2.23 & 25.6 hours \\ Mst & 8k & 7.41 & 2.00 & 26.5 hours \\ Mst & 16k & 3.53 & 1.26 & 62.5 hours \\ Mst & 30k & 3.45 & 1.23 & 233 hours \\  

Table 7: LLAMA3-8b with Mst, with \(4times\) larger context length compared to activation recomputation.

 Llama3 Implementations & Maximum Sequence Length (K) \\ 
8-bit & 5 \\ 4-bit & 10 \\  MST & 60 \\ MST + 8-bit & 110 \\ MST + 4-bit & 140 \\  

Table 5: Maximum sequence length training with lossy method 

## 5 Ablation Study:

### Memory Optimization of Mini-Sequence Transformer (MsT)

MsT introduces a series of memory optimizations to reduce the memory overhead of long-sequence training. To understand the effectiveness of MsT memory optimizations, we perform an ablation study that incrementally turns off these optimizations (mini-sequence, activation recomputation) and measures the memory requirements. We consider three options: vanilla (standard Pytorch with BF16), activation recomputation only, and MST with activation recomputation.

Figure 3 shows the results. We analyze the peak memory usage of Llama3-8B and Gemma2-9B, with a sequence length of 20k. For sequence length 20k of Llama3-8B and Gemma2-9B, only MsT can make the model fit into A100 GPU. The rest of the memory consumption is estimated based on its model architectures and theoretical activation amount. For Llama3, activation recomputation can reduce the memory overhead of activation by \(3\), and MST can further reduce \(4\) memory overhead based on activation recomputation. For Gemma2-9B, MST achieves \(24\) longer sequence than vanilla and \(8\) longer sequence than activation recomputation. This improvement from \(12\) to \(24\) is due to Gemma2-9B's higher intermediate/input ratio (8 for MLP and 72 for the LM head) compared to Llama3 (7 for MLP and 32 for the LM head, as shown in Table 1). Further details on the memory ablation study can be found in Appendix D.

### How many mini-sequences are needed during training

We observe that increasing \(M\), the number of mini-sequences, can enhance memory efficiency; however, this enhancement has a certain upper limit. Specifically, increasing \(M\) can also affect throughput performance. Appendix F provides details regarding these limitations and their effects. This observation allows us to identify the optimal configuration for memory optimization and achieve the best balance between memory performance, consistent with our analysis in Sec 3.2 and 3.3.

We found that the best balance for memory and throughput is achieved by the optimal values of \(C\) for chunk-based MLP \(C=d,M=S/d\), where \(d\) is the hidden size. For the LM-Head, the original MST is employed for memory saving, and the optimal setting for \(M\) is determined by \(M=V/d\), specifically 32 for Llama3 and 64 for Gemma-2. This value provides the best memory efficiency.

## 6 Limitations and Future Directions

We discuss the limitations and future directions. Related work is also given in Appendix A.

**Compiling to CUDA.** Our current approaches are built on Pytorch implementation. This may constrain performance and low-level memory savings. It can be improved by fused kernel and cuda optimization, which can be our next step.

**Combination with memory optimization.** Our goal is to increase sequence length while maintaining performance and accuracy. Relaxing these requirements, MST can be combined with activation offload to extend sequence length as \(S_{max}=-W_{mem})}{(I_{mem}/M+A_{mem})}\), or with quantization to extend sequence length as \(S_{max}=-W_{mem})}{(I_{mem}/M+I A_{ mem})}\). This combination can be explored in future research.

Figure 3: Memory consumption of pre-training Llama3-8B and Gemma2-9B models with a batch size of 1 on a single A100 device, with activation recomputation and MST. Note that long-sequence training gradients overlap with activation, so gradients are not shown in bars.