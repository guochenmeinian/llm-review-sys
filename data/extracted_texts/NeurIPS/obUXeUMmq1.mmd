# Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective

Haixiang Sun

ShanghaiTech University

sunhx@shanghaitech.edu.cn &Ye Shi

ShanghaiTech University

shiye@shanghaitech.edu.cn

Corresponding author.

###### Abstract

Deep Equilibrium Model (DEQ), which serves as a typical implicit neural network, emphasizes their memory efficiency and competitive performance compared to explicit neural networks. However, there has been relatively limited theoretical analysis on the representation of DEQ. In this paper, we utilize the Neural Collapse (\(\)) as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions. \(\) is an interesting phenomenon in the neural network training process that characterizes the geometry of class features and classifier weights. While extensively studied in traditional explicit neural networks, the \(\) phenomenon has not received substantial attention in the context of implicit neural networks. We theoretically show that \(\) exists in DEQ under balanced conditions. Moreover, in imbalanced settings, despite the presence of minority collapse, DEQ demonstrated advantages over explicit neural networks. These advantages include the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions, highlighting DEQ's superiority in handling imbalanced datasets. Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios.

## 1 Introduction

Recently, there has been significant research on _implicitly-defined layers_ in neural networks [1; 2; 3; 6; 9; 20; 21; 49], where the output is implicitly mapped from the input under certain conditions. These layers embed interpretability and introduce inductive bias  into black-box neural networks, demonstrating superior performance compared to existing explicit layers.

Among these implicit networks, the Deep Equilibrium Model (DEQ) is a memory-efficient architecture that represents all hidden layers as the equilibrium point of a nonlinear fixed-point equation. Due to the absence of a closed-form solution in its forward process, DEQ can be viewed as having an infinite number of layers during iteration as long as the threshold is set low enough, enhancing its ability to fit input data. Consequently, its representational capacity is relatively stronger compared to a single-layer network structure. This phenomenon explains why DEQ has achieved state-of-the-art results in classification tasks compared to existing architectures like ResNet. For instance, it has been successfully applied to language tasks and image classification tasks, reaching state-of-the-art performance. Additionally, DEQ can be applied in various domains and integrated with numerous other models, including inverse problems , Neural ODEs , diffusion models [24; 43], Gaussian processes , and more.

However, recent research reveals a phenomenon called Neural Collapse (\(\)) concerning the learned deep representations across datasets in image classification tasks . Under the \(\) regime, the lastlayer feature of each sample in neural networks collapses to their within-class mean, and the classifier vector converges to a simplex Equiangular Tight Frame (ETF). Theoretical analyses [10; 40; 51] indicate that under the Unconstrained Features Mode (UFM) condition, specific features \(^{0}\) can be isolated from the entire network, known as the layer-peeled model . In this scenario, Neural Collapse (\(\)) is observed under certain conditions, suggesting that \(\) is agnostic to the backbone of feature extraction. Moreover, since \(\) measures the degree of proximity between features of the same category, an imbalanced dataset can exert a more negative influence on the performance of \(\). For instance, classes with fewer samples may not separate well and could converge in the same direction, leading to what is known as _Minority Collapse_. Thus, the \(\) metric serves as a valuable indicator of a model's behavior in the context of imbalanced datasets.

The reasons behind the superior performance of DEQ still lack theoretical proof and comprehensive quantitative analysis. Additionally, to the best of our knowledge, no prior work has integrated DEQ with imbalanced scenarios. In our study, we integrate DEQ with layer-peeled models, add constraints with respect to weights \(_{}\), and consider the results of fixed-point iteration as the output of DEQ. Therefore, we analyze the performance of \(\) in DEQ by continuously deriving the lower bound of the loss function under certain constraints, allowing us to assess how \(\) manifests in the training performance of the network. Similarly, we apply the same operations to explicit neural networks for comparison. Our results show that DEQ performs similarly to explicit neural networks under balanced settings. We further extend the dataset to imbalanced conditions and analyze the \(\) performance in DEQ, explaining why DEQ tends to outperform explicit neural networks under mild conditions. We systematically analyze performance in terms of feature convergence, distance to the Simplex ETF, and the parallel relationship between extracted features and classifier weights. These analyses uncover the reasons behind the superior performance of DEQ compared to explicit neural networks during training. Additionally, the experimental results in both balanced and imbalanced scenarios validate our theoretical analyses.

Our main contributions are:

* We systematically analyzed the representation of DEQ from the \(\) perspective and compared their performance with explicit neural networks. Our theoretical analysis shows that both DEQ and explicit neural networks exhibit the \(\) phenomenon in balanced datasets.
* Under imbalanced settings, we theoretically proved the convergence of extracted features to the vertices of a simplex ETF and alignment with classifier weights under certain conditions, demonstrating DEQ's advantages over explicit neural networks under some mild conditions.
* Experimental results on Cifar-10 and Cifar-100 validated our theoretical findings for distinguishing the differences between DEQ and explicit neural networks.

## 2 Background and related works

We consider a classification task with \(K\) classes. Let \(n_{k}\) denote the number of training samples in each class \(k\), and \(N=_{k=1}^{K}n_{k}\) represent the total number of training samples. A traditional neural network can be expressed as a mapping:

\[()=()+,\] (1)

where \(():^{ N}^{D N}\) is the feature extraction, \(^{K D}\) and \(^{K}\) are the classifiers and bias in the last layer, respectively. For simplicity, we consider the bias-free case and omit the term \(\). Besides, we will denote \(=()\) in later sections.

### Deep Equilibrium Models

There have been numerous neural network architectures designed for various practical tasks from different perspectives [17; 32; 38; 39; 48]. DEQ, a typical implicit network [13; 52], incorporates unrolling methods [12; 41], which are devised for training arbitrarily deep networks by integrating all the network layers into one [3; 4; 5; 35; 36; 58].

Let \(f_{}(,)\) represent a DEQ layer with input \(\) parameterized by \(\). When \(z^{}\) reaches the equilibrium point, it satisfies:

\[g_{}(^{},) f_{}(^{},)- ^{}=0.\] (2)

The forward procedure mostly employs the Broyden solver  for iterative solving:

\[_{t+1}=_{t}-_{t}^{-1}g_{}(_{t},),\] (3)

where \(_{t}^{-1}\) refers to the approximation of inverse matrix \(_{}^{-1}g_{}(_{t},)\), as well as the same parameter \(\) shared across iterations. However, the solution can be quite unstable, and efforts have been made to enhance stability and robustness [34; 44; 55; 56]. Especially, regarding the computation of the inverse matrix, it can be expanded in the form of a Neumann series [18; 60]. Besides, accelerating and stabilizing the backward procedure is also an important issue in DEQ .

### Neural Collapse \(\)

The phenomenon of \(\) was initially uncovered by , which is considered an intriguing regularity in neural network training with many elegant geometric properties [50; 61; 66]. When the model is at the terminal phase of training (TPT), or more precisely, achieves zero training error, the within-class means of features and the classifier vectors converge to the vertices of a simplex Equiangular Tight Frame (ETF) on a balanced dataset.

**Definition 2.1**.: (Simplex Equiangular Tight Frame) A collection of points \(_{i}^{D}\), \(i=1,2,,K\), is said to be a simplex equiangular tight frame if

\[=}_{K}-_{K} _{K}^{T},\] (4)

where \(\) is a non-zero scalar, \(=[_{1},,_{k}]^{D K}\), \(_{K}^{K K}\) is the identity matrix, \(_{K}\) is the ones vector, and \(^{D K}(D K)\) is a partial orthogonal matrix such that \(^{T}=_{K}\).

\(\) incorporates the following four properties of the last-layer features and classifiers in deep learning training on balanced datasets:

\(1\)**: **Variability collapse:** The feature within-class converges to a unique vector, _i.e._, for any sample \(i\) in the same class \(k\), its feature \(_{k,i}\) satisfies \(\|_{k,i}-}_{k}\| 0,k[k]\), with the training procedure.

\(2\)**: **Convergence to simplex ETF:** The mean value \(^{}\) of optimal features for each class collapses to the vertices of the simplex ETF.

\(3\)**: **Convergence to self-duality:** The class means and the classifier weights mutually converge: \(^{}}{\|\|}=^{}}{\|\|}\).

\(4\)**: **Nearest Neighbor:** The classifier determines the class based on the Euclidean distances among the feature vector and the classifier weights.

### Layer-peeled model under balanced and imbalanced conditions

Current studies often focus on the case where only the last-layer features and classifier are learnable without considering the layers in the backbone network under the assumption of Unconstrained Features Mode (UFM) , which can also be referred to as the Layer-peeled Model [14; 28]. First, we define the feasible set of parameters:

\[=\{_{k},h_{k,i}\ |\ _{k=1}^{K}\|_{k} \|^{2} E_{W},_{k=1}^{K}}_{i=1}^{n_{k}}\| _{k,i}\|^{2} E_{H}\}.\] (5)

**Definition 2.2**.: (Layer-peeled Model) When \(\) and \(\) are the last layer classifier and weights respectively, then the optimization process of the neural network can be reformulated as:

\[_{,}\ \ \ _{k=1}^{K}_{i=1}^{n_{k}}( _{k,i},_{k})\ \ \ \ \ _{k},_{k,i},\] (6)

where \(E_{H}\) and \(E_{W}\) are two predefined values, \(N\) refers to the total number of samples.

It should be noted that all the loss functions \(\) analyzed in our study are cross-entropy, as most current research focuses on this widely used deep learning classification loss function [23; 31]. And though the optimization program is nonconvex; however, it can generally be mathematically tractable for analysis. Besides, experiments with unregularized loss function and randomly initialized gradient descent typically converge to non-collapse global minimizers .

Under UFM, most \(\) studies are based on 1-2 conventional layers of weights, However, there is also work [10; 51] that extends it to analyze \(M\) linear layers. Additionally, various studies have revealed additional characteristics of \(\), such as its impact on generalization [16; 25; 27; 61], its influence on feature learning , global optimality of the network [64; 66] and others. Therefore, \(\) is a very efficient tool to analyze the performance of neural networks.

**Imbalanced learning** However, \(\) will not occur under imbalanced settings generally. This phenomenon arises due to the imbalance in sample quantities, leading to challenges in adequately fitting features for certain classes. This is commonly referred to as _minority collapse_[8; 14]. As the degree of imbalance increases, it is expected that classifiers for minority classes converge. When Minority Collapse occurs, the neural network predicts equal probabilities for all minority classes, regardless of the input.

To enhance learning performance in imbalanced scenarios  and mitigate the effects of minority collapse, several methods have been proposed.  introduced convex relaxation, modifying a loss function , and incorporating a regularization term . The reweighted approach is also widely applied, with some studies measuring it based on sample quantities [47; 59]. Additionally, adaptive techniques such as AutoBalance  have been introduced, which incorporates a bilevel optimization framework, along with logit balance [46; 54; 63; 65].

## 3 Comparison under balanced setting

In this section, we first analyze the \(\) phenomenon in DEQ under balanced settings. As illustrated in Figure 1, after completing the initial feature extraction, we further examine the feature \(\) obtained respectively by explicit neural networks and DEQ to reveal the \(\) phenomenon.

### \(\) in Explicit Neural Networks

Building upon (6), we analyze \(\) in explicit neural networks by considering the following constrained optimization problem during training:

\[_{,_{},^{0}}& _{k=1}^{K}_{i=1}^{n}(_{ }^{0}_{k,i},_{k})\\ &\|_{}\|_{F} E_{H}; _{k},_{k,i},\] (7)

Figure 1: Illustration of feature extraction. After extracting feature maps \(^{0}\), further features \(\) or \(^{}\) can be obtained by passing through an explicit neural network or DEQ. The final step involves the classifier to obtain predicted logits. To ensure a fair comparison, we standardize the backbone network and its output \(^{0}\) across all conditions.

where each \(n_{k}\) is set to \(n\) under the balanced setting, \(_{}\) represents the subsequent network weights. For ease of comparison with DEQ, we assume that the final feature is represented as \(=_{}^{0}\). Traditional neural network structures are nonconvex, making them challenging to analyze due to their highly interactive nature. Employing the layer-peeled model alleviates the difficulty of \(\) analysis.

### \(\) in Deep Equilibrium models

Building upon recent investigations into the \(\) phenomenon, we embrace the layer-peeled model, where the last-layer features \(=()\) (equilibrium points in DEQ \(^{}\)) as unconstrained optimization variables. Accordingly, we add the following constraints to enforce \(\) in DEQ:

\[_{}^{},_{}| \ ^{}=f(^{0};_{}),\ \ \|_{}\|_{F} E_{H}}.\] (8)

Compared to explicit layers, the active parameter in Deep Equilibrium models is \(W_{}\), hence imposing restrictions on it to align with the same feasible space. Then the formulation of DEQ with \(\) becomes:

\[_{,_{},^{},^{0}} _{k=1}^{K}_{i=1}^{n_{k}}(^{},_{k})\] (9) s.t. \[_{k},_{k,i};^{},_{ {DEQ}}_{}.\]

No matter whether under DEQ or explicit neural networks, these constraints must be imposed. This is because when these constraints are satisfied and the loss function reaches its lower bound, the \(\) phenomenon is guaranteed. In our theoretical analysis, we assume that the DEQ is linear, that is, \(^{}=(f_{}(),)=_{i=0 }^{}_{}^{i}\). Detailed analysis incorporating these constraints is provided in Appendix B.

The following theorem elucidates the specific scenarios in which the \(\) phenomenon occurs. For a fair comparison, we assume that the extracted features \(^{0}\) of the image encoder are the same in the derivation.

**Theorem 3.1**.: _(Feature collapse of explicit fully connected layers and implicit deep equilibrium models under balanced setting) Suppose (7) and (9) reaches its minimal, then \(1\): For \(\ k=1,2,,K\) and \(\ i=1,2,,n\):_

\[_{}_{k,i}^{0}=_{}_{k}^{0},\]

_where \(_{k}^{0}=_{i(k)}_{k,i}^{0}\). Similarly, if the model is DEQ, then_

\[f(_{k,i}^{0};_{})=f(_{k}^{0};_{}).\]

\(2\)_: The classifier aligns to the Simplex ETF, regardless of whether explicit neural network and DEQ are applied:_

\[^{T} =/E_{H}}_{}^{0}\] \[=/E_{H}}f(^{0};_{})\] \[=}{K-1}(_{K}-_{K}_{ K}^{T}).\]

\(3\)_: For \(\ k=1,2,,K\), the feature aligns to the weights:_

\[_{}_{k}^{0}_{k}.\]

_In DEQ cases:_

\[f(_{k}^{0};_{})_{k}.\]

The theorem demonstrates that when the network training reaches its limit, i.e., when the loss function reaches its minimum, the \(\) phenomenon emerges regardless of whether the chosen network is DEQ or explicit neural network. Besides, in certain scenarios, the lower bound of the loss function for DEQ is relatively smaller compared to explicit neural networks. More detailed proofs are in Appendix Section B.

Comparison under imbalanced setting

In this section, we analyze the performance differences between DEQ and explicit neural network on imbalanced datasets. We observe that, unlike in balanced scenarios, as long as certain conditions are met, the advantages of DEQ over explicit neural network become more pronounced on imbalanced datasets. And we provide theoretical evidence to support this phenomenon.

Suppose the total number of classes is \(K\), with \(K_{A}\) being the number of majority classes and \(K_{B}=K-K_{A}\) being the number of minority classes. Each majority class has \(n_{A}\) samples, and each minority class has \(n_{B}\) samples. The total number of samples is given by \(N=K_{A}n_{A}+K_{B}n_{B}\). Note that \(n_{A}>n_{B}\) with no requirement for \(K_{A}\) to be greater than \(K_{B}\). We first start with the loss function, which can be partitioned into two components as follows:

\[&_{,},^{0}} n_{A}}{N}_{k=1}^{K_{A}}_{i=1}^{n_{A}}( }^{0},_{k})+n_{B}}{N}_{k=K_{A}+1 }^{K_{B}}_{i=1}^{n_{B}}(}^{0}, _{k}),\\ &\ \ \ }\{_{}\ \ _{}\}\,,\ \ _{k},_{k,i},\] (10)

where \(}\) represents the weights of Deep Equilibrium Models \(_{}\) and explicit neural network \(_{}\). To analyze the \(\) phenomenon, we present the results in the following theorem:

**Theorem 4.1**.: _(Neural Collapse under imbalanced settings on explicit neural networks and deep equilibrium models)_

_When the loss function reaches the minimum, then \(1\): For \(\ k=1,2,,K\) and \(\ i=1,2,,n\):_

\[_{}_{k,i}^{0}=_{}_{k}^{0},\]

_where \(_{k}^{0}=_{i(i)}_{k,i}^{0}\). Similarly, if the model is DEQ, then_

\[f(_{k,i}^{0};_{})=f(_{k}^{0};_{}).\]

\(2\)_: Not exists, but the results of explicit neural network and DEQ can be compared: Here we denote \((_{k}^{0})^{T}_{k^{}}^{0}=_{k,k^{}}\) and \(\) is a \(K\)-Simplex ETF, if_

\[E_{H}<2_{ij}-_{ij}<}\]

_is satisfied, the following inequality_

\[\|(_{}^{0})^{T}(_{} ^{0})-\|_{F}>\|f^{T}(^{0};_{})f(^{0};_{})-\|_{F}\]

_holds. \(3\)_: Similarly as \(2\), though it does not exist, the results can still be compared, when_

\[}{E_{w}+E_{H}}+E_{H}(1-E_{H})<2\]

_is satisfied, then the cosine distance satisfies:_

\[(f(_{k};_{}),_{k})/(_{}_{k},_{k})>1.\]

The detailed proof is in Appendix Section C.

Besides, the conclusion regarding the loss function is quite similar to that of Theorem B.3 under balanced settings. As analyzed in (43) and (44) in the Appendix, the lower bound of the loss function in DEQ is still lower than that in explicit neural network, where the performance of learned features is more evident in Figure 2, where we use t-SNE  and Gram matrix of features to describe the performance of two models. Although the phenomenon of \(2\) and \(3\) does not exist, we have discovered in Theorem 4.1 that under mild conditions, DEQ is superior in terms of \(\) compared to explicit neural network. Notably, the conditions are easy to satisfy since \(E_{H}\) is generally very small in practice.

A crucial insight is that since DEQ undergoes multiple rounds of parameter adjustments for learning, it can be viewed as having an infinite number of layers, thus possessing greater representational capacity. As the network deepens, the iterative process of forward fixed-point may not necessarily reach the lowest threshold. Therefore, DEQ exhibits a certain degree of generalization for features in the minority class. Given the substantial feature differences among classes under an imbalanced dataset, the learned features by DEQ may demonstrate better adaptability to unseen categories. Consequently, compared to explicit neural network, DEQ tends to enhance performance.

Besides, due to the repeated iterations in solving the fixed-point iteration for some samples in the minority class with a small sample size, the model somewhat engages in multiple learning iterations for the features of samples in this class. This mitigates the impact of imbalanced samples to some extent. However, despite some improvements compared to the explicit neural network, DEQ still faces the issue of minority collapse. This conclusion is further validated in our subsequent experiments. Besides, to further discuss the situation of the dataset in terms of the degree of imbalance, we derived the following proposition:

**Proposition 4.2**.: _Denote \(R=K_{A}n_{A}/N\). When the number of samples in the majority class becomes extremely large, i.e., \(R 1\), the features of the two kinds of classes will become:_

_Majority classes:_

\[_{}_{k,i}^{0} =_{}_{k}^{0},\] \[f(_{k,i}^{0};_{}) =f(_{k}^{0};_{}),\]

_where \(1 k K_{A}\) and \(i(k)\). Each feature collapses to \(K_{A}\)-simplex ETF._

_Minority classes:_

\[_{k} =,\] \[_{}_{k,i}^{0} =f(_{k,i}^{0};_{})=,\]

_where \(K_{A}+1 k K\) and \(i(k)\)._

_Here, \((k)\) refers to the samples that belong to the class \(k\)._

This situation is equivalent to having a balanced dataset in the majority class, while the minority class, due to its extremely small sample size, contributes almost nothing. In such an extreme scenario, the \(\) performance of DEQ and the fully connected layer is nearly indistinguishable similar to Theorem 3.1. Both collapse on majority classes, resulting in a lack of learning features from minority classes meeting the results of (51) and (52). This aligns with the findings in , where they provide more specific bounds on the ratio \(K_{A}/K_{B}\) in their Theorem 5.

## 5 Experiments

In this section, we empirically conducted experiments to validate the correctness of the proposed theorems. Initially, we implemented DEQ on a balanced dataset and compared its \(\) performance

Figure 2: Under the imbalanced setting for CIFAR-10 with \(K_{A}=3\) and \(R=10\), the disparity in the learned features between Explicit Neural Networks (left) and DEQ (right).

with that of ResNet. Subsequently, for imbalanced datasets, we tested varying degrees of imbalance by manipulating the quantities of \(n_{A}\) and \(n_{B}\), as well as \(K_{A}\) and \(K_{B}\). The experimental results showed that, on imbalanced datasets, DEQ outperformed Explicit Neural Networks. This finding is consistent with the results reported in . All experiments were implemented using PyTorch on NVIDIA Tesla A40 48GB.

### Experiment setup

Without loss of generality, since any traditional neural network can be formulated as a DEQ, we use ResNet18  as the backbone architecture here. As discussed earlier, to utilize the fixed point \(^{}\) learned by DEQ as the extracted feature, we formulate the last ResNet block into a DEQ format, while maintaining the remaining structure identical to ResNet. As mentioned in , training with DEQ can lead to instability issues. This is especially noticeable as training progresses, where some samples struggle to converge to a fixed point. To address this, in accordance with their setting, we implement the solver with a threshold \(\) set to \(10^{-3}\) and introduce an early stopping mechanism. If convergence is not achieved within \(T>20\) iterations, we terminate the fixed-point iteration. Additionally, when facing problematic samples during fixed-point solving, we skip them to ensure training stability. During training, we set the learning rate to \(1 10^{-4}\) and utilize stochastic gradient descent with a momentum of \(0.9\) and weight decay of \(5 10^{-4}\). Both \(E_{W}\) and \(E_{H}\) are set to \(0.01\). The training phase for each network consists of 100 epochs, with a batch size of \(128\). In this context, accuracy is assessed by averaging the results from the last 10 epochs and computing their standard deviation.

### Performance under balanced conditions

By using the settings in (7) and (9), we compared the performance of DEQ and Explicit NN on Cifar-10  and Cifar-100  for validation, as shown in Figure 3(a). Their \(\) performances remain comparable, i.e., DEQ achieves results similar to Explicit NN, corroborating the findings of Theorem 3.1. As for accuracy, from the results in the first column of Table 1, it can be observed that DEQ's accuracy is higher than that of the explicit layer, which aligns with Theorem B.3. However, the increase is only marginal due to the fact that the coefficients \(E_{H}\) and \(E_{W}\) act as scaling factors. Therefore, compared to explicit neural network, DEQ finds it challenging to achieve a significantly lower loss and, consequently, a substantial improvement. Moreover, Explicit NN performs well in fitting balanced datasets, so the accuracy of DEQ does not experience a significant boost in this context.

   & Cifar-10 & Cifar-100 \\  Explicit NN & \(93.05 0.17\) & \(64.35 0.20\) \\ DEQ & \(93.23 0.13\) & \(64.77 0.36\) \\  

Table 1: Comparison of accuracy under balanced settings of Cifar-10 and Cifar-100

Figure 3: Comparison of accuracy and \(\) phenomenon in training Cifar-10 dataset

Here, we manually set the number of epochs to \(100\) to avoid potential instability issues with DEQ as training deepens. This is because DEQ can be challenging to reach the TPT (Terminal Phase of Training). As the number of parameters increases, achieving fixed-point convergence becomes more difficult, and even parameter explosion may occur. Under the current vanilla design, it is challenging to avoid such instability. Therefore, for a fair comparison, we apply the same training settings to both the implicit DEQ and the explicit neural network. The results in Figure 5 indicate that the test performance at 100 epochs is not significantly different from that at TPT. Since DEQ shares the same backbone as the corresponding explicit neural network, it can still demonstrate better \(\) performance after reaching TPT in these cases.

### Performance under imbalanced conditions

We conducted experiments with varying configurations with different numbers of majority and minority classes and imbalance degrees. Assume the numbers of majority and minority classes are \((K_{A},K_{B})\) with corresponding sample sizes \((n_{A},n_{B})\), the imbalance degree is denoted as \(R=n_{A}/n_{B}\).We considered different setups for majority and minority class quantities, such as \((3,7)\), \((5,5)\), and \((7,3)\). Additionally, we varied the ratio of sample quantities \(R\) between majority and minority classes with values of \(10\), \(50\) and \(100\). We also tested the phenomenon of \(\) and accuracy on the Cifar-10 and Cifar-100 datasets, which own a total of \(5000\) images for each class. Specifically, when \(R=100\) and \((K_{A},K_{B})=(3,7)\) for Cifar-10, the number of samples for all classes is \((5000,5000,5000,50,50,50,50,50,50,50)\).

The results for \((K_{A},K_{B})=(3,7)\) are shown in Table 2, where the test dataset owns the same distribution as the training dataset. We use "overall", "majority", and "minority" to represent the results across all categories, the majority class, and the minority class, respectively. We contrasted the difference in the training outcomes between the Explicit Neural Network and DEQ, and the superior performance of DEQ compared to Explicit Neural Network confirms DEQ's higher learning potential. This suggests that DEQ can achieve a lower bound on its loss function. The experimental results indicate that DEQ consistently outperforms explicit neural network in accuracy during imbalanced training, aligning with Theorem 4.1. Specifically, we present the outcomes for \((K_{A},K_{B})=(5,5)\) with \(R=100\) are depicted in Figure 3(b). The results strongly corroborate Theorem 4.1, affirming DEQ exhibits the same \(1\) phenomenon as an explicit neural network under these conditions. However, DEQ outperforms the explicit neural network in terms of \(2\) and \(3\). Additional experimental results with different parameters are detailed in Appendix Section D.

In addition to the stability considerations discussed in Section 5.2, we refrain from training for an extensive number of epochs due to the imbalance in the samples of the training set. This is because excessive learning rounds might cause the network parameters to predominantly capture information from the majority class, resulting in overfitting its features. This, in turn, diminishes the generalization of learning features from other classes, leading to marginal improvements in accuracy on the test set. As depicted in Figure 3(b), the model has already converged at this point. Moreover, limiting the number of training epochs helps to avoid the gradual instability in the learning process of DEQ.

## 6 Conclusion

In this study, we have systematically analyzed the representation of Deep Equilibrium Models (DEQ) and explicit neural networks under both balanced and imbalanced conditions using the phenomenon of Neural Collapse (\(\)). Our theoretical analysis demonstrated that \(\) is present in DEQ under

   & &  &  \\   & \(R\) & 10 & 50 & 100 & 10 & 50 & 100 \\   & overall & 72.57\(\)0.25 & 44.32\(\)0.23 & 32.14\(\)0.81 & 41.41\(\)0.56 & 28.18\(\)0.42 & 23.43\(\)0.92 \\  & majority & 96.40\(\)0.32 & 96.80\(\)0.29 & 91.67\(\)0.61 & 73.03\(\)0.62 & 74.53\(\)0.55 & 73.46\(\)0.56 \\  & minority & 62.36\(\)0.12 & 21.83\(\)0.20 & 6.64\(\)0.99 & 27.86\(\)0.39 & 8.31\(\)0.38 & 1.99\(\)1.06 \\   & overall & 73.84\(\)0.72 & 46.08\(\)1.06 & 34.18\(\)1.28 & 43.72\(\)0.60 & 30.46\(\)1.27 & 24.78\(\)1.93 \\  & majority & 96.68\(\)0.87 & 96.63\(\)0.98 & 93.33\(\)1.36 & 74.16\(\)0.82 & 73.63\(\)0.95 & 74.89\(\)0.88 \\   & minority & 64.06\(\)0.66 & 24.42\(\)1.32 & 8.83\(\)1.08 & 30.67\(\)0.53 & 11.96\(\)1.66 & 3.31\(\)2.45 \\  

Table 2: Test Accuracy on Cifar-10 and Cifar-100 Dataset with \(K_{A}=3\)balanced conditions. Furthermore, in imbalanced settings, DEQ exhibited notable advantages over explicit neural networks, such as the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions. These findings highlight the superior performance of DEQ in handling imbalanced datasets. Our experimental results in both balanced and imbalanced scenarios validate the theoretical insights. The current analysis is limited to simple imbalanced scenarios and the linear structure of DEQ models. Future work will expand on this foundation by exploring more general imbalanced scenarios and extending the analysis to more complex forms of DEQ models.