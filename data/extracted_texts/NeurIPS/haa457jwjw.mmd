# Piecewise-Stationary Bandits with Knapsacks

Xilin Zhang

Department of ISEM

National University of Singapore

Singapore, 117578

zhangxilin@u.nus.edu &Cheung Wang Chi

Department of ISEM

National University of Singapore

Singapore, 117578

isecwc@nus.edu.sg

###### Abstract

We propose a novel inventory reserving algorithm which draws new insights into Bandits with Knapsacks (Bwk) problems in piecewise-stationary environments. Suppose parameters \(_{},_{}(0,1]\) respectively lower and upper bound the ratio between the reward earned and the resources consumed in a round. Our algorithm achieves a provably near-optimal competitive ratio of \(O((_{}/_{}))\), with a matching lower bound provided. Our performance guarantee is based on a _dynamic benchmark_ that upper bounds the optimum, different from existing works on adversarial Bwk Immorlica et al. (2019); Kesselheim and Singla (2020) who compare with the stationary benchmark. Different from existing non-stationary Bwk work Liu et al. (2022), we do not require a bounded global variation.

## 1 Introduction

In a bandits with knapsack (Bwk) problem, each action \(a\) in the action set \(\) is associated with a latent and random amount of reward earned, \(R_{t}(a)\), and resource consumed, \(C_{t}(a)\), in each round \(t=1,,T\). A decision maker (DM) selects an action \(a_{t}\) in round \(t\), and observes bandit feedback \((R_{t}(a_{t}),C_{t}(a_{t}))\). The DM targets at maximizing the total reward \(_{t=1}^{T}R_{t}(a_{t})\), while satisfying the hard capacity constraint \(_{t=1}^{T}C_{t}(a_{t}) B\). Bwk has many real-life applications such as dynamic pricing Babaioff et al. (2015), resource allocation Zhalechian et al. (2022), online auction Balseiro and Gur (2019) and assortment planning Agrawal et al. (2019). Stochastic Bwk is first introduced by Badanidiyuru et al. (2018), followed by generalizations to concave reward with convex constraints Agrawal and Devanur (2014), combinatorial bandits Sankararaman and Slivkins (2018) and contextual bandits Badanidiyuru et al. (2014); Agrawal and Devanur (2016). In stochastic Bwk problems, the expected feedback \([(R_{t}(a),C_{t}(a))]=(r(a),c(a))\) is stationary for all \(a,\;t\{1,,T\}\), and a sublinear-in-\(T\) regret is achievable. Nevertheless, the stationary model could be too ideal in many applications.

Adversarial Bwk is firstly considered in Immorlica et al. (2019) where \((r_{t},c_{t})=\{(r_{t}(a),c_{t}(a))\}_{a}\) can change arbitrarily over the horizon. They achieve a competitive ratio (CR) of \(O(d(T))\) with respect to a _static benchmark_ when there are \(d\) budget constraints. A static benchmark picks a fixed optimal action (or a fixed optimal distribution over arms), and applies the same action (or distribution) in all \(T\) rounds. Kesselheim and Singla (2020) further improve the CR to \(O((d)(T))\). Other papers consider different regimes such as unlimited rounds (Rangi et al. (2018)), large budget \(B=(T)\) (Castiglioni et al. (2022)), strict feasibility (Castiglioni et al. (2022)) and approximate stationarity (Fikioris and Tardos (2023)). All these works compare with static benchmarks (see Appendix A.1). Moreover, adversarial Bwk could be too conservative in certain real-life scenarios. For instance, sales patterns could be stationary for a duration of time, but only change during periods of hot seasons/promotions/new trends, which fits into our piecewise-stationary Bwk regime.

An abundance of existing works explore adversarial online knapsack problems with full feedback, where \((r_{t},c_{t})\) can change arbitrarily but their realized value of \(\{(R_{t}(a),C_{t}(a))\}_{a}\) is observed before choosing \(a_{t}\)(Karp et al. (1990); Mehta et al. (2007); Zhou et al. (2008)). Many of these works compare their accrued rewards with dynamic benchmarks (stronger than the static benchmarks used in adversarial Bwk), where the DM picks different optimal actions \(a_{t}^{*}\) in different rounds. However, the dynamic benchmarks considered in the above papers are _best single arm_ benchmark, which only allows pulling a single arm in each round; while our benchmark is a _best distribution over arms_ benchmark (see Appendix A.2 for a more detailed elaboration). Further, the ability of observing \(\{(R_{t}(a),C_{t}(a))\}_{a}\) before selecting \(a_{t}\) is crucial in the algorithm designs in Karp et al. (1990); Mehta et al. (2007); Zhou et al. (2008). Their algorithm design cannot be readily generalized to the bandit setting, where the DM only observes \((R_{t}(a_{t}),C_{t}(a_{t}))\) after selecting \(a_{t}\).

Another line of recent research investigates non-stationary online knapsack problems with either full feedback Jiang et al. (2020); Balseiro et al. (2022) or bandit feedback Liu et al. (2022). These works quantify the scale of non-stationarity in terms of both the local variation \(=_{t=1}^{T-1}((r_{t+1},c_{t+1}),(r_{t},c_{t}))\) and the global variation \(=_{t=1}^{T}(_{t=1}^{T}(r_{t},c_{t})/T,(r_{t},c_{ t}))\), where \(\) is a certain metric. Assuming \(\{,\}\) grows sublimently in \(T\), they achieve sublinear-in-\(T\) regret bounds compared to the dynamic benchmark. However, \(\) growing sublinearly in \(T\) is rather strong assumption. We could have \(\) linear in \(T\), even with one change point (see Remark A.3 for more detail). In this work, we consider piece-wise stationary models that allow a higher degree of non-stationarity, which is yet to be studied in all aforementioned works.

**Our contributions.** Firstly, on **modeling** (see Section 2), the DM does not know the number of change points and when changes happen. We formulate our model as a single-resource problem, and extend to \(d\)-resource problems in Appendix B.5 with an extra multiplicative factor of \(d\) on the competitve ratio. Secondly, on **algorithm design** (see Sections 3.1 and 4.1), we propose novel algorithms which are natural, intuitive and easy to implement. Our idea of reserving inventory based on the reward-consumption ratio provides new insights into the problem. Thirdly, on **performance guarantee** (see Sections 3.2 and 4.2), we achieve a provably near-optimal competitive ratio with respect to a _best distribution over arms_ benchmark without requiring a bounded \(\), which distinguishes our work from the existing literature. Specifically, suppose there exists parameters \(_{},_{}(0,1]\) such that \(_{} r_{t}(a),c_{t}(a)_{}\) for all \(t,a\). Our algorithms achieve a competitive ratio of \(O((_{}/_{}))\), which requires a novel analysis. We prove the tightness of our competitive ratio by providing a matching lower bound (see Section 4.4). We also run some illustrative **numerical experiments** (see Section 5) to compare our algorithm performance with Immorlica et al. (2019) and Zhou et al. (2008) under the piecewise-stationary settings.

## 2 Model

### Problem formulation

**Problem dynamics.** The online model involves \(T\) rounds, indexed as \(t=\{1,2,,T\}\). We index an arm as \(a\). Additionally, we define the null arm \(a_{}\), where no allocation is made when \(a_{}\) is chosen. In round \(t\), the DM chooses an arm \(a_{t}\{a_{}\}\), and observes a noisy outcome vector \((R_{t}(a_{t}),C_{t}(a_{t}))^{2}\) as the bandit feedback, where \(R_{t}(a_{t})\) and \(C_{t}(a_{t})\) are the reward and the resource consumption in round \(t\) respectively. We set \(R_{t}(a_{})=C_{t}(a_{})=0\) with certainty for all \(t\). The DM is endowed with \(B T\) units of the resource. The DM's goal is to maximize the total reward, with the constraint that the total resource consumption is at most \(B\) with certainty. Denote \(r_{t}=\{r_{t}(a)\}_{a}=\{[R_{t}(a)]\}_{a}\) and \(c_{t}=\{c_{t}(a)\}_{a}=\{[C_{t}(a)]\}_{a}\). We consider a piece-wise stationary setting, where the planning horizon \(\) is partitioned into \(L\) stationary pieces \(\{t_{0}=1,,t_{1}\},\{t_{1}+1,,t_{2}\},,\{t_{L-1}+1,,t_{L }=T\}\). On each stationary piece \(l\), we have \((r_{t},c_{t})=(r^{(l)},c^{(l)})\) for all \(t\{t_{l-1}+1,,t_{l}\}\).

The DM does _not_ know the number of rounds \(T\), the number of stationary pieces \(L\), the rounds \(t_{1},,t_{L-1}\) where changes happen, the values of \(\{(r^{(l)},c^{(l)})\}_{l\{1,,L\}}\) and their realized outcomes.

**Goal and benchmark.** Our goal is to develop an online algorithm that maximize the expected total reward \([_{t=1}^{T}R_{t}(a_{t})]\) while satisfying the inventory constraint \(_{t=1}^{T}C_{t}(a_{t}) B\), which can be formulated as the following dynamic program DP. In DP, \(X_{t}=\{X_{t}(a)\}_{a}\) is a binary decision vari able indicating whether to pick arm \(a\) in round \(t\) (i.e., \(X_{t}(a)=1\)) or not (i.e., \(X_{t}(a)=0\)). An online algorithm is non-anticipatory in the sense that \(X_{t}\) depends only on \(B\) and \(\{(R_{s}(a_{s}),C_{s}(a_{s}),X_{s})\}_{s=1}^{t-1}\).

DP \[:=_{X_{t}}[_{t=1}^{T}_{a }R_{t}(a)X_{t}(a)]\] FA \[:=\ _{l=1}^{L}(t_{l}-t_{l-1})_{a}r^{(l)}(a )x_{l}(a)\] s.t. \[_{l=1}^{L}(t_{l}-t_{l-1})_{a}c^{(l)}(a)x_{l }(a) B\] \[_{a}X_{t}(a) 1 t _{a}x_{t}(a) 1 l=1,,L\] \[X_{t}(a)\{0,1\} a,\ t. x_{l}(a) 0 a,\ l=1,,L.\]

In non-stationary bandits without resource constraints, the performance bound of an online algorithm is in the form of \(_{t=1}^{T}R_{t}(a_{t})_{t=1}^{T}r_{t}(a_{t}^{*})-\) Reg, where \(_{t=1}^{T}r_{t}(a_{t}^{*})\) is the optimal expected reward obtained by choosing the best arm in each round, and Reg is a sublinear-in-\(T\) regret characterizing the reward loss. In our Bwk setting, due to the inventory constraint, achieving a sublinear-in-\(T\) regret is impossible without assuming a bounded global variation (see Appendix A.3). We denote \(()\) as the optimum of an optimization problem P. We aim for a performance guarantee of the form \(_{t=1}^{T}R_{t}(a_{t})}()- \), where opt(DP) is the optimum of DP, CR is a competitive ratio, and Reg is a sublinear-in-\(T\) regret. Unfortunately, DP is hard to solve. Therefore, we define a fluid approximation FA of DP, where \(R_{t}\) and \(C_{t}\) are replaced by their respective expectations, and the decision variables are fractional. In the following Lemma 2.1 (proved in Appendix C.1), we justify that opt(FA) can serve as a benchmark for our algorithms' performance since

\[_{t=1}^{T}R_{t}(a_{t})}()- }()-,\]

and we aim to derive performance guarantees of the form \(_{t=1}^{T}R_{t}(a_{t})}()- \).

**Lemma 2.1**.: _opt(FA)\(\)opt(DP)._

### Assumptions, limitations and discussions

Compared with the adversarial Bwk literature, our piecewise-stationary setting has two limitations. The first is on \(L\), the number of change points. When \(L\) is not known, our result is meaningful only when \(L=o(})\). When \(L\) is known, our result is meaningful when \(L=o(T_{})\) (see Theorem 4.2). In contrast, existing works on adversarial Bwk generally allow \(L=T\). The second is on the value range of non-null actions:

**Assumption 2.2**.: For all \(a,l\{1,,L\}\), there exists known constants \(_{},_{}(0,1]\) such that \(_{} r^{(l)}(a),c^{(l)}(a)_{}\).

While \(_{}\) can be as small as \(0\) generally, we argue that this assumption is mild. Assumption 2.2 holds in many real-life scenarios. For instance, in portfolio management, an investor allocates a limited budget among different investment options (arms) to maximize the overall return. The investor has assessments on lower and upper ranges of the expected returns for each investment option. The lower range is usually strictly positive, since the investor would not consider investment options with 0 or negative expected return. In applications such as dynamic pricing, assortment planning, network resource allocation and energy management, expected profits and consumer demands are usually within a known positive value range. We further justify that Assumption 2.2_theoretically_ in the following Lemma 2.3 (proved in Appendix C.5).

**Lemma 2.3**.: _For any online algorithm, there exist an instance for which \(0 r_{t}(a),c_{t}(a) 1\) for all \(a,t\), and that CR\(>((_{}/_{}))\)._

Additionally, in Appendix C.5 we demonstrate that knowing the values of \(_{},_{}\) is _necessary_ for achieving CR\(=O((_{}/_{}))\).

### High-level idea of our algorithm

**Decomposing opt(FA) in terms of reward-consumption ratios.** Throughout our paper, we fix an optimal solution \(\{x_{l}^{*}\}_{l=1}^{L}\) to FA. We define the set \(=\{l\{1,,L\}:_{a}x_{l}^{*}(a)>0\}\), which indexes stationary pieces where non-null allocations are made under the optimal solution \(\{x_{l}^{*}\}_{l=1}^{L}\) to FA. For each \(l\), we define

\[^{(l)*}=}r^{(l)}(a)x_{l}^{*}(a)}{_ {a}c^{(l)}(a)x_{l}^{*}(a)}[}{_{} },}{_{}}], B_{l}^{*}=(t_{l}-t_{l-1})_{ a}c^{(l)}(a)x_{l}^{*}(a).\] (1)

\(^{(l)*}\), whose value range follows from Assumption 2.2, is the optimal expected reward earned per unit of consumed resource under \(\{x_{l}^{*}\}_{l=1}^{L}\). We call \(^{(l)*}\) the optimal expected _reward-consumption ratio_ of stationary piece \(l\). \(B_{l}^{*}\) represents the optimal expected amount of resources assigned for stationary piece \(l\). To aid our algorithm design, we define the following linear program:

\[(,,):= _{a}(a)x(a)\] s.t. \[_{a}(a)x(a)\] \[_{a}x(a) 1\] \[x(a) 0 a.\]

Then, we can express opt(FA) in terms of \((,,)\) and \(^{(l)*}\), \(B_{l}^{*}\) as follows:

\[=_{l}(t_{l}-t_{l-1})( (r^{(l)},c^{(l)},B_{l}^{*}/(t_{l}-t_{l-1})))=_{ l}^{(l)*} B_{l}^{*}.\] (2)

The first equation in (2) can be verified by noting that \(x_{l}^{*}\) is feasible to \((r^{(l)},c^{(l)},B_{l}^{*}/(t_{l}-t_{l-1}))\) for each \(l\{1,,L\}\), and the concatenation of the optimal solutions of \(\{(r^{(l)},c^{(l)},B_{l}^{*}/(t_{l}-t_{l-1})\}_{l}\) forms a feasible solution to FA. The second equation in (2) holds, by the definitions of \(^{(l)*}\), \(B_{l}^{*}\) and the fact that \(((r^{(l)},c^{(l)},B_{l}^{*}/(t_{l}-t_{l-1})))=_{a }r^{(l)}(a)x_{l}^{*}(a)\).

**Algorithm design.** Fix an arbitrary constant \(>1\) (we set \(=e\) by default, but our results hold for any constant \(>1\)). We define \(M=[_{}(_{}/_{})]\) and partition \([_{}/_{},_{}/_{}]\) into \(2M\) intervals \([^{-M},^{-M+1}]\{(^{m},^{m+1}]\}_{m=-M+1}^{M-1}\). For each stationary piece \(l\), we denote \(m_{l}^{*}\{-M,,M-1\}\) as the interval such that \(^{(l)*}(^{m_{l}^{*}},^{m_{l}^{*}+1}]\). In the forthcoming discussion, with some abuse of notation, we sometimes write interval \([^{-M},^{-M+1}]\) as \((^{-M},^{-M+1}]\), and we refer to interval \((^{m},^{m+1}]\) as reward-consumption ratio interval \(m\), or "interval \(m\)" in short. Then we can decompose opt(FA) regarding reward-consumption ratio intervals:

\[=(2) =_{m=-M}^{M-1}_{l}^{(l)*} (^{(l)*}(^{m},^{m+1}]) B_{l}^{*}\] \[=_{m=-M}^{M-1}_{l}^{(l)*} (m_{l}^{*}=m) B_{l}^{*}.\] (3)

The _key intuition_ of our algorithm is to achieve a reward guarantee for each interval \(m\) regarding the reward-consumption ratio \((m_{l}^{*}=m)^{(l)*}\) and the resource consumption \(_{l}(m_{l}^{*}=m) B_{l}^{*}\), which is done by performing two tasks: (a) for each \(l\), we guess the value of \(m\) such that \(^{(l)*}(^{m},^{m+1}]\). We guarantee that for at least \(1/(M+1)\) fraction of requests on each \(l\), our guessed ratio interval are close to the correct interval \(m_{l}^{*}\); (b) for each interval \(m\), we "reserve" \(B/2M\) resource units. That is, we reserve an inventory of \(B/2M\) resource units to satisfy requests with a guessed reward-consumption ratio interval \(m\). When the inventory reserved for interval \(m\) is depleted, the DM rejects (by choosing \(a_{}\)) all future requests with a guessed interval \(m\).

By accomplishing task (a), we ensure that for each \(l\), at least \((m_{l}^{*}=m) B_{l}^{*}/(M+1)\) requested resources are served by resources reserved for interval \(m\), generating reward at a ratio of at least\(^{m}\). Then, by accomplishing task (b), if the reserved inventory for interval \(m\) are not depleted by round \(T\), our algorithm earns a reward of at least \(^{m}(m_{l}^{*}=m) B_{l}^{*}/(M+1)\) during stationary piece \(l\). Else, if the reserved \(B/2M\) resource units for interval \(m\) are depleted by round \(T\), then the DM earns a reward of at least \(^{m} B/2M^{m}_{l}(m _{l}^{*}=m) B_{l}^{*}/(2M)\) from resources reserved for interval \(m\), since \(_{l}(m_{l}^{*}=m) B_{l}^{*} B\). By judiciously analyzing the relationship between stationary pieces and reward-consumption ratio intervals, for each interval \(m\), we ensure that a reward of

\[_{l}^{m}(m_{l}^{*}=m ) B_{l}^{*}\]

is accrued, which is \(1/O(M)\) of the benchmark resources consumed on all stationary pieces \(l\) whose \(^{(l)*}(^{m},^{m+1}]\). By summing over \(m\{-M,,M-1\}\), we achieve \(1/O(M)\) fraction of reward (3).

## 3 Warm-up: Full-feedback deterministic outcome setting

In this section, we introduce the main idea of our algorithm on the bandit model by relaxing the model uncertainty assumptions and specializing to the full-feedback deterministic setting: \((R_{t},C_{t})=(r^{(l)},c^{(l)})\) with certainty for each \(t\{t_{l-1}+1,,t_{l}\}\). Thus, \((r^{(l)},c^{(l)})\) is observed at the start of the stationary piece \(l\). The DM does not know \(L\) and \(\{t_{1},,t_{L-1}\}\) before the online process begins. The DM's decision can be fractional, which means on each stationary piece \(l\), a decision can take the form of \(x_{l}\{x^{||}:_{a}x(a) 1,x(a)  0\  a\}\), resulting in a reward of \(_{a}r^{(l)}(a)x_{l}(a)\) and resource consumption of \(_{a}c^{(l)}(a)x_{l}(a)\) in a round.

### Inventory REServing (IRES) algorithm

Upon observing \((R_{t},C_{t})=(r^{(l)},c^{(l)})\), guessing \(^{(l)*}\) (see Section 2.3) is equivalent to guessing \(x_{l}^{*}\), which is equivalent to guessing \(B_{l}^{*}/(t_{l}-t_{l-1})\), since \(\{x_{l}^{*}\}_{l=1}^{L}\) is an optimal solution to \((r^{(l)},c^{(l)},B_{l}^{*}/(t_{l}-t_{l-1}))\). In this section, when we say "Line xx", we refer to a line of Algorithm 1, which displays IRES. At the start, the IRES _reserves_\(B/2M\) units of resource to each interval \(m\{-M,,M-1\}\), and each resource unit is reserved by exactly one interval. In Line 5, for each stationary piece \(l\), we firstly solve \((r^{(l)},c^{(l)},_{}^{q})\) for each \(q\{0,,M\}\), and get an optimal solution \(x_{l}^{(q)*}=\{x_{l}^{(q)*}(a)\}_{a}\). Each \(_{}^{q}\) is a guess of \(B_{l}^{*}/(t_{l}-t_{l-1})\). For each \(q\{0,,M\}\), we define

\[_{l}^{(q)}:=}r^{(l)}(a)x_{l}^{(q)*}(a)} {_{a}c^{(l)}(a)x_{l}^{(q)*}(a)}\]

as a guess of \(^{(l)*}\). Claim 3 in Appendix B.3 shows that, by guessing a \(q\) such that \(_{}^{q}\) is within a factor of \(\) from \(B_{l}^{*}/(t_{l}-t_{l-1})\), we also have \(_{l}^{(q)}\) to be at most a factor of \(\) from \(^{(l)*}\). As time progresses on \(l\), we go round-robin on the choices of \(q_{t}\{0,,M\}\) for each round \(t\) (Lines 7, 15). In round \(t\{t_{l-1}+1,,t_{l}\}\), we identify \(m_{t}\{-M,,M-1\}\) such that \(_{l}^{(q_{t})}(^{m_{t}},^{m_{t}+1}]\) (Line 8). If there remains enough reserved resource units for interval \(m_{t}\) (Line 10), the DM fulfils the \(t\)-th request by selecting fractional action \(x_{t}=x_{l}^{(q_{t})*}\) (Line 11), which consumes resources reserved for \(m_{t}\) (Lines 9, 11). Otherwise, the DM selects \(a_{}\) and rejects the request (Line 13). By Line 9, we have \(_{t}^{(m)}=\{s\{1,,t\}:m_{s}=m\}\), which consists of rounds in \(\{1,,t\}\) when the DM attempts to fulfil a request with resources reserved for interval \(m\).

### Performance guarantee of IRES

We provide a performance guarantee to IRES in Theorem 3.1.

**Theorem 3.1**.: _For any given \(>1\), IRES achieves a reward of at least_

\[(_{}/_{})+1}{B}) }{6^{2}_{}(_{}/_{} )},\]

_under mild requirements that \(t_{l}-t_{l-1} M+1\  l\). In particular, IRES achieves a competitive ratio of \(O(_{}(_{}/_{}))\) if \(B(_{}(_{}/_{}))\).__Remark 3.2_ (Comparing with online knapsack problems).: Our deterministic setting resembles online knapsack problems with adversarial \((r_{t},c_{t})\) revealed in each round Zhou et al. (2008), but our \((r_{t},c_{t})\) remains the same for an unknown number of rounds. Assuming \(B(_{})\), Zhou et al. (2008) achieve a competitive ratio of \(2(_{}/_{})+1\) and they provide a nearly-matching lower bound. We recover their competitive ratio with an extra \(3^{2}\) multiplicative factor in a piece-wise stationary setting and a stricter requirement on \(B\).

### Analysis

Denote \(_{T}^{(m)}=\{^{(m)}(1),^{(m)}(2),\}\) where \(^{(m)}(1)<^{(m)}(2)<\), and \(}^{(m)}\) is the prefix of \(_{T}^{(m)}\) satisfying

\[}^{(m)}=\{^{(m)}(n)_{T}^{(m)}:_{ s=1}^{n}_{a}c_{^{(m)}(s)}(a)x_{^{(m)}(s)}(a)-1\}.\]

That is, \(}^{(m)}\) consist up to the last round assigned to interval \(m\) such that the reserved inventory is not fully consumed. It is evident that if \(_{s_{T}^{(m)}}c_{s}(a_{s}) B/(2M)-1\), then \(}^{(m)}=_{T}^{(m)}\).

Define \(_{l}=\{t_{l-1}+1,,t_{l}\}\), the time interval of the \(l\)-th piece. The reward achieved by IRES is \(=_{t}_{a}r_{t}(a)x_{t}(a)\), which can be decomposed as \(=_{m=-M}^{M-1}^{(m)}\) where

\[^{(m)}=_{l}(m_{l}^{*}=m)_{t( _{n=-M}^{M-1}}^{(n)})_{l}}_{a }r_{t}(a)x_{t}(a).\]

The set \((_{n=-M}^{M-1}}^{(n)})_{l}\) consists of rounds in stationary piece \(l\), which requests are not rejected due to shortage in reserved resource units. The summation \(_{l}(m_{l}^{*}=m)\) yields the reward accrued on pieces where \(m_{l}^{*}=m\). By the summation \(_{m=-M}^{M-1}^{(m)}\), we obtain the total reward accrued with resources reserved for \(2M\) intervals.

Similarly, we decompose the benchmark opt(FA) \(=_{m=-M}^{M-1}^{(m)}\) where

\[^{(m)}=_{l}(m_{l}^{*}=m)^{(l)*} B_{l}^{*},\]

To prove Theorem 3.1, it suffices to show \(^{(m)}M}^{(m)}\) for each interval \(m\), as in the following Claim 1 and Claim 2. Then Theorem 3.1 can be established by summing over \(m\{-M,,M-1\}\).

**Claim 1**.: For any interval \(m\{-M,,M-1\}\), if for all \(n\{\{m-1,-M\},m\}\) we have \(}^{(n)}=_{T}^{(n)}\), then \(^{(m)}^{(m)}\).

**Claim 2**.: For any interval \(m\{-M,,M-1\}\), if for at least one element \(n\{\{m-1,-M\},m\}\) we have \(}^{(n)}_{T}^{(n)}\), then \(^{(m)}M}^{(m)}\).

**Sketch proofs of Claims 1, 2**.: Claims 1, 2 are proved in Appendices C.2, C.3 respectively. We first show in Claim 3 in Appendix B.3 that on a stationary piece \(l\), there exists a "correct" \(q_{l}^{*}\{0,,M\}\), such that when selecting decision \(x_{t}=x_{l}^{(q_{l}^{*})*}\) (the optimal solution to the \((r^{(l)},c^{(l)},_{}^{q_{l}^{*}})\)), our guess \(_{l}^{(q_{l}^{*})}\) on the ground-truth \(^{(l)*}(^{m_{l}^{*}},^{m_{l}^{*}+1}]\) satisfies

\[_{l}^{(q_{l}^{*})}(^{m_{l}^{*}-1},^{m_{l}^{*}+1}]= (^{m_{l}^{*}-1},^{m_{l}^{*}}](^{m_{l}^{*}},^{m_{l} ^{*}+1}].\] (4)

When taking fractional action \(x_{l}^{(q_{l}^{*})*}\), we consume resources reserved for reward-consumption ratio intervals \(m_{l}^{*}-1\) or \(m_{l}^{*}\). Therefore by our round-robin design, on each stationary piece \(l\) such that \(m_{l}^{*}=m\), at least \((t_{l}-t_{l-1})/(M+1)\) requests are assigned to intervals \(m-1\) or \(m\), under decision \(x_{l}^{(q_{l}^{*})*}\). It remains to analyze how many requests are fulfilled by resources reserved for the correct interval \(m_{l}^{*}=m\) at the correct reward-consumption ratio \(_{l}^{(q_{l}^{*})}\), as discussed in Section 2.3.

For an interval \(m\) where \(}^{(m)}=_{T}^{(m)},}^{(m-1)}= _{T}^{(m-1)}\) (Claim 1 case), there are still remaining resources reserved for intervals \(m-1,m\) at the end of the horizon. Hence, for each stationary piece \(l\) such that \(m_{l}^{*}=m\), at least \((t_{l}-t_{l-1})/(M+1)\) requests (consuming \(B_{l}^{*}/(M+1)\) resource units) are indeed fulfilled by resources reserved for interval \(m-1\) or \(m\), accruing reward at the reward-consumption ratio of at least \(^{(l)*}/\) according to (4). Summing over all \(l\) such that \(m_{l}^{*}=m\), we have \(^{(m)}_{l}(^{(l)*}/) (m_{l}^{*}=m) B_{l}^{*}/(M+1)\) and Claim 1 is validated. For an interval \(m\) where there exists some \(n\{\{m-1,-M\},m\}\) such that \(}^{(n)}_{T}^{(n)}\) (Claim 2 case), the \(B/2M\) resource units reserved for interval \(n\) are depleted before the end of the horizon. In this case, some requests on stationary piece \(l\) where \(m_{l}^{*}=m\) may be rejected, but the \(B/(2M)\) resource units reserved for interval \(n\) have been consumed, generating reward at a reward-consumption ratio of at least \(^{n}^{m-1}\). Since the total resources that should be consumed w.r.t. interval \(m\) under the optimal FA solution is \(_{l}(m_{l}^{*}=m) B_{l}^{*} B\), we have \(^{(m)}^{m-1} B/(2M)_{l}( {Ratio}^{(l)*}/^{2})(m_{l}^{*}=m) B_{l}^{*}\), and Claim 2 is validated.

## 4 Bandit-feedback stochastic outcome setting

In this section, we consider the original piece-wise stationary Bwk model, where the DM receives bandit feedback on outcomes \((R_{t},C_{t})\), and decisions are randomized.

### Inventory REServing with change monitoring (IRES-CM) Algorithm

In this section, when we say "Line xx", we refer to a line of Algorithm 2, which displays IRES-CM. In the bandit-feedback setting, guessing \(^{(l)*}\) requires estimating \((r^{(l)},c^{(l)})\). To do so, we adaptively partition \(\) into exploration rounds and exploitation rounds. In each round \(t\), we conduct exploration with probability \(_{t}=M|(1/)((||)+1)}/\) (reflected in a Bernoulli random variable \(U(t)\) in Line 7), where \((0,1)\) is a confidence parameter and \(N\) is defined in (5). In an exploration round \(t\) (Lines 9-13), we uniformly sample an arm \(a\) and pull it for \(N\) consecutive rounds. We update an estimate \((_{t}(a),_{t}(a))\) on \((r_{t}(a),c_{t}(a))=(r^{(l)}(a),c^{(l)}(a))\) using the \(\{(R_{s}(a),C_{s}(a))\}_{s_{s}^{s}(a)}\) information, where \(_{s}^{s}(a)\) denotes the set of the most recent \(N\) exploration rounds before round \(t\) when arm \(a\) is pulled. That is, we set \(_{t}^{}(a)=\{\{t-s_{t},,t-1\}:a_{}=a\}\) where \(s_{t}=_{s}\{_{=t-s_{t}}^{t-1}(a_{}=a)=N\}\). We define

\[N=)^{2}_{}}, _{t}(a)=_{s}^{s}(a)}R_{s}(a)}{N},_{ t}(a)=_{s}^{s}(a)}C_{s}(a)}{N}.\] (5)

The estimates \(_{t},_{t}\) have two sources of error: error due to random noise, which decreases with \(N\); and error due to non-stationarity, which increases with \(N\). We set \(N\) according to (5) to balance these two errors. We let \(_{t}^{}\) denote the set of exploration rounds.

In an exploitation round \(t\) (Lines 17-25), we take turns to pull arms according to decision \(_{t}^{(q_{t})*}\), which is very similar to Algorithm 1 with \((_{t},_{t})\) in place of \((r_{t},c_{t})\). We define

\[}_{t}^{(q_{t})}=}_{t}(a) _{t}^{(q)*}(a)}{_{a}_{t}(a)_{t}^{(q)*}( a)},()_{t}=\{\{}_{t}^{(q_{t})}, ^{M}\},^{-M}\},\]

which can both be interpreted as a guess of \(^{(l)*}\) at any round \(t\) during stationary piece \(l\). We reserve \(B/(2M)\) units of resources for each interval \(m\{-M,,M-1\}\). In round \(t\), we serve request \(t\) using resources reserved for interval \(_{t}\) such that \(()_{t}(^{_{t}},^{_{t}+t}]\). If interval \(_{t}\) has remaining reserved inventory, then we pull arm \(a_{t}=a\) with probability \(_{t}^{(q_{t})*}(a)\), or \(a_{t}_{t}^{(q_{t})*}\) in short. We let \(_{t}^{(m)}\) denote the set of exploitation rounds using resources reserved for interval \(m\). We finally highlight that the major performance difference between IRES and IRES-CM is due to estimating \((r_{t},c_{t})\) by \((_{t},_{t})\), which is detailed in Section 4.3.

```
1:Input: resource capacity \(B\), rate \(\), bounding parameters \(_{},_{}\).
2: Set \(_{t}^{}=\) for all \(t\) and \(_{t}^{(m)}=\) for all \(m,t\).
3: Pull each arm \(a\) for \(N\) times, get \(_{t}(a),_{t}(a)\) as in (5).
4: Set \(t=N||+1\).
5:while\(t T\)do
6: Solve \((_{t},_{t},_{}^{q})\)\( q\{0,1,,M\}\) for optimal \(_{t}^{(q)*}=\{_{t}^{(q)*}(a)\}_{a}\).
7: Sample \(U(t)(_{t})\).
8:if\(U(t)=1\)then
9: Pick arm \(a()\), pull arm \(a_{s}=a\).
10: Set \(U(s)=1\) for \(s\{t,,t+N-1\}\).
11: Set \(_{s}^{}=_{t-1}^{}\{t,,s\}\) for \(s\{t,,t+N-1\}\).
12: Set \(t=t+N\), \((_{t},_{t})=(_{t-N},_{t-N})\).
13: Update \(_{t}(a)\), \(_{t}(a)\) as in (5).
14:else
15:for\(q=0,,M\)do
16: Set \(q_{t}=q\).
17: Determine \(_{t}\{-M,,M-1\}\) such that \(()_{t}(^{_{t}},^{_{t}+1}]\).
18: Set \(_{t}^{(_{t})}=_{t-1}^{(_{t})}\{t\}\).
19:if\(_{s_{t-1}^{(_{t})}}C_{s}(a_{s})-1\)then
20: Pick arm \(a_{t}_{t}^{(q_{t})*}\).
21:else
22: Pick arm \(a_{t}=a_{}\).
23:endif
24: Set \(t=t+1\), \((_{t},_{t})=(_{t-1},_{t-1})\).
25:endfor
26:endif
27:endwhile ```

**Algorithm 2** Inventory REServing with Change Monitoring (IRES-CM)

### Performance guarantee of IRES-CM

We impose the following assumption on the ranges of \(B\), opt(FA).

**Assumption 4.1**.: \(\{B,\}(L|NT}),\) where \(()\) hides multiplicative factors in terms of \(_{}(_{}/_{}),(1/),((||)+1)\).

The performance of IRES-CM is as follows:

**Theorem 4.2**.: _For any given \(>1\), with probability at least \(1-2||(_{}(_{}/_{})L+T)\), IRES-CM achieves a reward of at least_

\[_{}(_{}/_{})} -(L|NT})\]under Assumption 4.1, where \(o()\) hides multiplicative factors in terms of \(\) and \(()\) hides multiplicative factors in terms of \(_{}(_{}/_{}),(1/)\), \(((||)+1)\). In particular, IRES-CM achieves a competitive ratio of \(O(_{}(_{}/_{}))\) as long as \(L=o(})\)._

The proof of Theorem 4.2 can be found in Appendix C.4. We provide a thorough comparison of our performance guarantee with existing works on adversarial and non-stationary Bwk in Appendix A.

_Remark 4.3_ (Improved performance with known \(L\)).: If the DM knows \(L\), Assumption 4.1 can be relaxed to

\[\{B,\}(|NT}).\]

Furthermore, in our performance guarantee in Theorem 4.2, the deductive term \((L|NT})\) from opt(FA) can be improved to \((|NT})\) by setting the exploration parameter \(_{t}=M|(1/)((||)+1)}/\) in IRES-CM. Without prior knowledge of \(L\), the deductive term \((L|NT})=o(T)\) if \(L=o(})\); with prior information of \(L\), the deductive term \((|NT})=o(T)\) if \(L=o(T_{})\).

_Remark 4.4_ (Deterministic setting with bandit feedback).: In our full-feedback deterministic setting, since \((r^{(l)},c^{(l)})\) is given at the beginning of each stationary piece, our performance guarantee is independent on \(||,L\). In a bandit-feedback deterministic setting, IRES-CM can be applied by setting \(N=1\). In this case, under Assumption 4.1, IRES-CM achieves a reward of at least

\[_{}(_{}/_{} )}(-(L|T})).\]

### Analysis

We denote \(_{t}(a)=\{s:s_{t}^{}(a)\}\) as the 1st element in \(_{t}^{}(a)\). We partition the exploitation round set \(_{T}^{l(m)}\) into two sets \(}^{1(m)}\) and \(}^{1(m)}\), i.e., \(_{T}^{l(m)}=}^{1(m)}}^{ 1(m)}\), \(}^{1(m)}}^{1(m)}=\). A time index \(t_{T}^{l(m)}\) belongs to the set \(}^{1(m)}\) (referred to as "successful exploitation rounds regarding interval \(m\)") if and only if the following condition is satisfied for all \(a\):

\[\{(r_{s}(a),c_{s}(a))\}_{a}=\{(r_{_{t}(a)}(a),c_{_{ t}(a)}(a))\}_{a}, s\{_{t}(a),,t\}.\] (6)

For \(t}^{1(m)}\) (referred to as "failed exploitation rounds regarding interval \(m\)"), inequality (6) is violated for at least one \(a\). We denote \(}^{1(m)}=\{^{I(m)}(1),^{I(m)}(2),\}\) where \(^{I(m)}(1)<^{I(m)}(2)<\). We let \(}^{1(m)}\) be a prefix of \(}^{1(m)}\) satisfying

\[}^{1(m)}=\{^{(m)}(n)} ^{1(m)}:_{s=1}^{n}_{a}C_{^{(m)}(s)}(a_ {^{I(m)}(s)})-1\}\]

which consists up to the last exploitation round satisfying (6) for interval \(m\), such that the reserved resource is adequate. If \(_{s_{T}^{1(m)}}C_{s}(a_{s}) B/(2M)-1\), then \(}^{1(m)}=}^{1(m)}\).

**Sketch proof of Theorem 4.2.** Recall that the performance guarantee of our algorithms is in the form of \(_{t=1}^{T}R_{t}(a_{t})}-\). The proof consists of mainly two steps: (a) we derive the CR\(=O(M)\) by bounding two different cases of interval \(m\) in a similar manner to Claim 1 and Claim 2 (see Appendix C.4), with \(}^{1(m)}\) (successful exploitation rounds in IRES-CM) in place of \(_{T}^{(m)}\) (all rounds in IRES); (b) we derive the Reg\(=(L|NT})\) by bounding the number of exploration rounds \(|_{T}^{}|\) and failed exploitation rounds \(|_{m=-M-2}^{M}}^{1(m)}|\) (see Appendix B.7).

**Comparing performance of IRES and IRES-CM.** We highlight that the major performance difference between IRES and IRES-CM is the loss caused by estimating \((r_{t},c_{t})\), reflected in the following aspects: (i) reward loss caused by exploration (upper bounding \(|_{T}^{}|\)); (ii) \(_{t}^{}(a)\) contains change points, causing failed estimation of \((r_{t},c_{t})\) (upper bounding \(|_{m=-M-2}^{M}}^{1(m)}\)); (iii) \(_{t}^{}(a)\) does not contain change points, but the discrepancy between \((r_{t},c_{t})\) and \((_{t},_{t})\) results in assigning Ratio\({}^{(1)*}\) (estimated by \(}_{t}^{(q)}\)) to the wrong interval. We remark that the losses due to (i, ii) are accounted for in Reg, while (iii) is accounted for in the CR.

### A lower bound on competitive ratio

We complement our analysis by showing the tightness of our CR (see Appendix C.6 for proof).

**Theorem 4.5**.: _Consider a fixed but arbitrary \(>1\), and set \(_{}=^{-3},_{}=1\) for an arbitrary \(_{>0}\). For any online algorithm, there exist an instance for which \(_{} r_{t}(a),c_{t}(a)_{}\) for all \(a,t\), and that \(_{t=1}^{T}R_{t}(a_{t})/(1/_{}(_{ }/_{}))\)._

## 5 Numerical Experiments

We run numerical experiments on a single-resource problem where \(L=2\), \(T=20000\) (each stationary piece has \(10000\) rounds), \(=\{1,2\}\), \(B=9360\) and we set \(=\) for our algorithms. The rewards and resource consumption in all rounds are uniformly distributed within a \([-0.2,+0.2]\) range from their mean values. We compare the performance of IRES-CM with Immorlica et al. (2019)'s algorithm and Zhou et al. (2008)'s algorithm. Recall that Immorlica et al. (2019) focus on an adversarial Bwk problem and achieves a CR w.r.t. a static benchmark. Zhou et al. (2008) study a full-feedback adversarial setting and achieves a CR w.r.t. a single best arm benchmark. In Figure 1, each curve represents the average cumulative reward over \(10\) simulations, and the shaded area around each curve marks the variance over the simulations. We provide Zhou et al. (2008)'s algorithm with extra information of \((r_{t},c_{t})\) before making decisions in each round, and compare the performance of algorithms with the linear program benchmark FA (dotted curves in Figure 1).

In Figure 1(a), we set \(r^{(1)}(1)=r^{(1)}(2)=0.5,c^{(1)}(1)=c^{(1)}(2)=1\) for stationary piece 1; and set \(r^{(2)}(1)=1,r^{(2)}(2)=0.5,c^{(2)}(1)=0.5,c^{(2)}(2)=1\) for stationary piece 2. In Figure 1(b), we switch the values of \(r^{(2)}(1)\) and \(r^{(2)}(2)\). i.e., setting \(r^{(2)}(1)=0.5,r^{(2)}(2)=1\). Observe that IRES-CM outperforms Immorlica et al. (2019)'s algorithm in both cases. This is mainly because Immorlica et al. (2019)'s algorithm is designed for a more general adversarial Bwk setting. In contrast, we utilize the extra information that \(_{}=0.5\). Therefore, Immorlica et al. (2019)'s algorithm is significantly more conservative than IRES-CM in reserving inventories for future customers. Zhou et al. (2008)'s algorithm outperforms IRES-CM in Figure 1(a), but performs worse than IRES-CM in Figure 1(b). This is because that in Figure 1(a), the optimal solution of the benchmark FA chooses a single arm on each stationary piece, which aligns with Zhou et al. (2008)'s single best arm benchmark. Zhou et al. (2008)'s algorithm performs well with the extra information of \((r_{t},c_{t})\) before making decisions. In Figure 1(b), the optimal solution of the benchmark FA chooses mixed arms on the second stationary piece, where \(x_{2}^{*}(1)=0.128,x_{2}^{*}(2)=0.872\). The numerical results are consistent with the theoretical results that Zhou et al. (2008) achieve sub-optimal rewards compared with a _best distribution over arms_ benchmark, while our IRES-CM performs well. Finally, our experiments are run on a Surface Pro 7 with an i5-1035G4 processor. All results can be produced within 30 minutes.

Figure 1: Performance comparison of algorithms for piecewise-stationary Bwk