# Aligning Individual and Collective Objectives

in Multi-Agent Cooperation

 Yang Li

The University of Manchester

yang.li-4@manchester.ac.uk

&Wenhao Zhang

Shanghai Jiao Tong University

wenhao_zhang@sjtu.edu.cn

&Jianhong Wang

INFORMED-AI Hub

University of Bristol

jianhong.wang@bristol.ac.uk

&Shao Zhang

Shanghai Jiao Tong University

shaozhang@sjtu.edu.cn

&Yali Du

King's College London

yali.du@kcl.ac.uk

&Ying Wen

Shanghai Jiao Tong University

ying.wen@sjtu.edu.cn

&Wei Pan

The University of Manchester

wei.pan@manchester.ac.uk

Corresponding authors. \(\) Jianhong Wang is a visiting researcher at the University of Manchester.

###### Abstract

Among the research topics in multi-agent learning, mixed-motive cooperation is one of the most prominent challenges, primarily due to the mismatch between individual and collective goals. The cutting-edge research is focused on incorporating domain knowledge into rewards and introducing additional mechanisms to incentivize cooperation. However, these approaches often face shortcomings such as the effort on manual design and the absence of theoretical groundings. To close this gap, we model the mixed-motive game as a differentiable game for the ease of illuminating the learning dynamics towards cooperation. More detailed, we introduce a novel optimization method named **A**ltruistic **G**radient **A**djustment (**_A_g_A) that employs gradient adjustments to progressively align individual and collective objectives. Furthermore, we theoretically prove that AgA effectively attracts gradients to stable fixed points of the collective objective while considering individual interests, and we validate these claims with empirical evidence. We evaluate the effectiveness of our algorithm AgA through benchmark environments for testing mixed-motive collaboration with small-scale agents such as the two-player public good game and the sequential social dilemma games, Cleanup and Harvest, as well as our self-developed large-scale environment in the game StarCraft II.

## 1 Introduction

Multi-agent cooperation primarily focuses on learning how to promote collaborative behavior in shared environments. In general, multi-agent cooperation research is categorized into two prominent areas: pure-motive cooperation and mixed-motive cooperation . Recent progress in cooperative Multi-Agent Reinforcement Learning (MARL) has been primarily focusing on pure-motive cooperation, also known as common payoff games. This game models situations where each agent's individual goal fully aligns with the collective objectives . Nevertheless, mixed-motive cooperationis more widespread across real-world situations. It is usually defined by imperfect alignment between individual and collective rationalities (Rapoport, 1974; McKee et al., 2020).

Recent studies in mixed-motive cooperative MARL have largely employed hand-crafted designs to promote collaboration. One popular approach is to align objectives as per existing mechanisms in cooperative games, such as reputation (Anastassacos et al., 2021), norms (Vinitsky et al., 2023), and contracts (Hughes et al., 2020). Another prevalent method leverages intrinsic motivation to align individual and collective objectives, enhancing altruistic collaboration by integrating heuristic knowledge into the incentive function. Conventionally, some works accumulate individual rewards with the group to promote altruistic conduct (Hotsallero et al., 2020; Peyaskhovich and Lerer, 2018; Apt and Schafer, 2014; Roesch et al., 2024). Furthermore, some studies derive more sophisticated preference signals from the rewards of other agents (Hughes et al., 2018; McKee et al., 2020). Additionally, several approaches aim to learn the potential influences of an agent's actions on others (Yang et al., 2020; Jaques et al., 2019; Lu et al., 2022). Most of these algorithms rely heavily on carefully crafted designs, necessitating significant human expertise and detailed domain knowledge. On the other hand, several studies leverage Nash equilibria and related game theory concepts, such as the price of anarchy, to automatically modify rewards by learning additional weights that adjust the original objectives (Gemp et al., 2022; Kwon et al., 2023). However, finding Nash equilibria in nonconvex games presents a greater challenge compared to identifying minima in neural networks (Letcher et al., 2019).

In this study, we introduce the differentiable mixed-motive game (DMG), an effective framework for analyzing learning dynamics at both individual and collective levels. Furthermore, we derive the Altruistic Gradient Adjustment (AgA) algorithm, which aligns individual and collective objectives by modifying the gradient. We theoretically prove that AgA, with an appropriately chosen sign for the adjustment term, can successfully guide the gradient towards stable fixed points of the collective objective while considering individual interests. Additionally, empirical evidence from optimization trajectory visualizations and ablation studies validates our claims.

We also conduct comprehensive experiments to verify the effectiveness of the proposed AgA algorithm. First, optimization trajectory analysis and ablation studies validate our theoretical conclusions. Next, a series of experiments conducted in various environments demonstrate that the AgA algorithm outperforms related baselines in both gradient adjustment and mixed-motive cooperation areas. In addition to commonly used testbeds like the public goods matrix game and sequential social dilemma games (Cleanup and Harvest) (Leibo et al., 2017), which are limited in terms of agent scale, action space, and task complexity, we introduce a more complex mixed-motive environment called Selfish-MMM2, an adaptation of the MMM2 map from the StarCraft II game (Samvelyan et al., 2019). Selfish-MMM2 offers the following significant improvements over other mixed-motive games: it supports large-scale scenarios with 10 heterogeneous controlled agents facing 11 enemies, features a large action space with a size of \(18^{10}\), which vastly exceeds the action spaces in Cleanup \(9^{5}\) and Harvest \(8^{5}\) that are limited to 5 homogeneous agents. Selfish-MMM2 also has a larger action space and greater task complexity than the 10-player PD testbed (with a size of \(2^{10}\)) used in D3C (Gemp et al., 2022), which claims to solve the large-scale problem in mixed-motive games.

In summary, the contributions of this paper are as follows: (1) We are the first work to model the mixed-motive game as a differentiable game (to the best of our knowledge) and propose the AgA algorithm to align individual and collective objectives from a gradient perspective. (2) We theoretically prove that, in the neighborhood of fixed points, AgA could pull the gradient toward stable fixed points of the collective objectives and push the gradient away from unstable fixed points. (3) We introduce Selfish-MMM2, a novel large-scale mixed-motive cooperation environment, and conduct comprehensive experiments across multiple settings that verify our theoretical claims and demonstrate the superior performance of the AgA algorithm.

## 2 Related Work

Mixed-motive cooperation.Mixed-motive cooperation refers to scenarios where the group's objectives are sometimes aligned and at other times conflicted (Philip S. Gallo and McClintock, 1965). Recently, there has been a surge in academic interest in the Sequential Social Dilemma (SSD) (Leibo et al., 2017), which expands the concept from its roots in matrix games (Macy and Flache, 2002) to Markov games. Prosocial (Peysakhovich and Lerer, 2018) improves collective performance by blending individual rewards to redraft the agent's overall utility. Inequity aversion further integrates the concept into Markov games by adding envy (disadvantageous inequality) and guilt (advantageous inequality) rewards to the original individual rewards (Hughes et al., 2018). To further promote cooperation, the gifting mechanism--a crucial strategy in mixed-motive cooperation (Lupu and Precup, 2020)--allows agents to influence each other's reward functions through peer rewarding. Besides, PED-DQN (Hostallero et al., 2020) introduces an automatic reward-shaping MARL method that gradually adjusts rewards to shift agents' actions from their perceived equilibrium towards more cooperative outcomes. Social Value Orientation (SVO) strategy introduces a unique shared rewards-based compensation approach, encouraging behavior modifications in line with interdependence theory (McKee et al., 2020). The LIO strategy bypasses the need to modify extrinsic rewards by empowering an agent to directly influence its partner's actions (Yang et al., 2020). Meanwhile, other studies introduce new cooperative mechanisms such as incorporating a reputation model (McKee et al., 2021), social norms (Vinitsky et al., 2023). Recently, Roesch et al. (2024) proposed a selfishness level method that incorporates social welfare into individual rewards to enhance altruistic cooperation. Additionally, several works have explored automatically modifying rewards online. D3C (Gemp et al., 2022) learns to mix rewards to improve efficiency in a Nash equilibrium. Kwon et al. (2023) extended D3C by addressing the problem of automatically modifying individual agent objectives to optimize a desired global objective. Despite these advances, many current studies lack both cost efficiency in design and theoretical analysis of alignment and convergence. To address this, our study applies a gradient perspective to align goals and further investigates the learning dynamics of our proposed method.

Gradient-based Methods.Our proposed AgA is fundamentally a gradient adjustment methodology, making gradient-based optimization methods highly relevant to the context of this paper. Methods based on gradient have been developed to find stationary points, such as the Nash equilibrium or stable fixed points. Optimistic Mirror Descent leverages historical data to extrapolate subsequent gradients (Daskalakis et al. (2018), while Gidel et al. (2019) extends this concept by advocating averaging methodologies and variants of extrapolation techniques. Consensus gradient adjustment, or consensus optimization, is a technique that embeds a consensus agreement term within the gradient to ensure its convergence (Mescheder et al., 2017). Learning with Opponent-Learning Awareness (LOLA) utilizes information from other players to compute one player's anticipated learning steps (Foerster et al., 2018). Subsequently, Stable Opponent Shaping (SOS) (Letcher et al., 2019) and Consistent Opponent-Learning Awareness (COLA) (Willi et al., 2022) methods have enhanced the LOLA algorithm, targeting convergence assurance and inconsistency elimination, respectively. Symplectic Gradient Adjustment (SGA) alters the update direction towards the stable fixed points based on a novel decomposition of game dynamics (Balduzzi et al., 2018; Letcher et al., 2019). Recently, Learning to Play Games (L2PG) ensures convergence towards a stable fixed point by predicting updates to players' parameters derived from historical trajectories (Chen et al., 2023). However, these methods, while focusing on zero-sum or general sum games, could potentially act counter to their individual interests, as they may prioritize stability over minimizing personal loss. Our research specifically targets mixed-motive setting with the aim of reconciling individual and collective objectives.

## 3 Preliminaries

### Differential Game

The theory of differential games was initially proposed by Isaacs (1965), aiming to expand the scope of sequential game theory to encompass continuous-time scenarios. Through the lens of machine learning, we formalize the differential game, as shown in Definition 3.1.

**Definition 3.1** (Differential Game (Balduzzi et al., 2018; Letcher et al., 2019)).: _A differential game could be defined as a tuple \(\{,,\}\), where \(=\{1,,n\}\) is the set of players. The parameter set \(=[_{i}]^{n}^{d}\) is defined, each with \(_{i}^{d_{i}}\) and \(d=_{i=1}^{n}d_{i}\). Here, \(=\{_{i}:^{d}\}_{i=1}^{n}\) represents the corresponding losses. These losses are assumed to be at least twice differential. Each player \(i\) is equipped with a policy, parameterized by \(_{i}\), aiming to minimize its loss \(_{i}\)._

We write the _simultaneous gradient_\(()\) of a differential game as \(()=(_{_{1}}_{1},,_{_{n}}_{ n})^{d},\) which is the gradient of the losses with respect to the parameters of the respective players. Furthermore, _Hessian matrix_\(\) mentioned in a differential game is the Jacobian matrix of the simultaneous gradient.

The _learning dynamics_ of differential game often refers to the process of sequential updates over \(\). The _learning rule_ for each player is defined as the operator, \(-\), where \(\) is a step size (learning rate) to determine the distance to move for each update.

### Gradient Adjustment Optimization

The _stable fixed point_, a criterion initiated from stability theory, exhibits its stability (robustness) to minor perturbations of environments, making it applicable to many real-world scenarios.

**Definition 3.2**.: _A point \(^{}\) is a fixed point if \((^{})=0\). If \((^{}) 0\) and \((^{})\) is invertible, the fixed point \(^{}\) is called stable fixed point. If \((^{}) 0\), the point is called unstable._

A naive idea to steer the dynamic towards convergence at fixed points involves minimizing \(\|()\|^{2}\). Assuming the Hessian \(()\) is invertible, then \((\|()\|^{2})=^{T}=0\) holds true if and only if \(=0\). However, it could converge to unstable fixed points (Mescheder et al., 2017). Hence, the consensus optimization method has been proposed, incorporating gradient adjustment (Mescheder et al., 2017), shown as follows: \(}=+\|()\|^{2 }=+^{T}\). For simplicity, we will refer to the _consensus gradient adjustment_ as _CGA_. While CGA proves effective in certain specific scenarios, such as two-player zero-sum games, it unfortunately falls short in general games (Balduzzi et al., 2018). To address this shortage, _symplectic gradient adjustment_ (_SGA_) (Balduzzi et al., 2018; Letcher et al., 2019) is introduced to find the stable fixed point in general sum games, such that \(}=+^{T}\). Herein, \(\) represents the antisymmetric component of the generalized Helmholtz decomposition of \(\), \(()=()+()\), where \(()\) denotes the symmetric component.

## 4 Method

In this section, we first define differentiable mixed-motive games (DMG) and analyzes the alignment dilemma faced by existing methods, which is described in Section 4.1. We then propose the Altruistic Gradient Adjustment (AgA) algorithm in Section 4.2 to align the individual and collective objectives by modifying the gradient. Finally, a case study demonstrating AgA's effectiveness in a toy two-player DMG is presented in Section 4.3.

### Differentiable Mixed-motive Game

We first formulate the mixed-motive game as a differentiable game. Specifically, the _differentiable mixed-motive game (DMG)_ is defined as a tuple \((,,)\), where \(_{i}\) is at least twice the differentiable loss function for the agent \(i\). Differentiable losses exhibit the _mixed motivation property_: minimization of individual losses can result in a conflict between individuals or between individual and collective objectives (e.g., maximizing individual stats and winning the game are often

Figure 1: Trajectories of optimization in a two-player DMG (as delineated in Example 4.1). Fig.0(a) displays the trajectories over the collective reward landscape - deeper orange equates to higher rewards. Remarkably, only Simul-Co and AgA make successful strides towards the social optimum. Fig.0(b) and Fig.0(c) delineate trajectories on the individual reward contour, _underscoring Simul-Co’s neglect for Player 1’s interests as it navigates through the crests and troughs of its reward_. Conversely, our AgA optimizes along the summit of Player 1’s reward while also maximizing the collective reward, _demonstrating successful alignment_.

conflict in basketball matches). In addition to the simultaneous gradient \(()\) of individual losses with respect to the parameters of the respective players, we use \(_{c}()\) to refer to the gradient of the collective loss: \(_{c}()=(_{_{1}}_{c},,_{_{n} }_{c}).\)

**Alignment Dilemma in DMG.** Direct optimization of individual losses and collective loss are straightforward ideas to solving DMG problems. However, minimizing only the individual loss of each agent is unlikely to achieve a collective optimum (Leibo et al., 2017; McKee et al., 2020). Conversely, optimizing the collective loss might produce better overall outcomes but risks neglecting individual interests (Roesch et al., 2024). Additionally, the local convergence of gradient descent in singular collective functions is not always guaranteed (Balduzzi et al., 2018).

Example 4.1 gives a two-player differentiable mixed-motive game. We provide a visual representation of optimization trajectories using a series of methods to investigate the learning dynamics involved in resolving the example, as shown in Fig. 1.

**Example 4.1**.: _Consider a two-player DMG with \(_{1}(a_{1},a_{2})=-sin(a_{1}a_{2}+a_{2}^{2})\) and \(_{2}(a_{1},a_{2})=-[cos(1+a_{1}-(1+a_{2})^{2})+a_{1}a_{2}^{2}]\), where \(a_{i}\) represents the action of the player \(i\) (\(i=1,2\)), and \(a_{i}\). The rewards for the two players are the negation of their respective losses._

We first implement simultaneous optimization of individual losses (Simul-Ind) with respect to each parameter, defined by the learning rule \(_{i}=_{i}-_{i}\), for \(i\{1,2\}\), where \(_{i}=_{_{i}}_{i}\). Simultaneous optimization of collective loss (Simul-Co) replaces the individual gradient with the collective gradient \(_{c}\) of collective loss \(_{c}=_{1}+_{2}\). Furthermore, in order to investigate the learning dynamics of prevalent gradient modification optimization approaches such as SGA and CGA for the two-player differentiable mixed-motive game, we modify the learning rule by integrating the respective adjusted gradient as delineated in Section 3.2.

Fig. 0(a) shows the optimization paths on the collective reward landscapes, with every path starting from the front-bottom of the landscape. The collective reward is defined as social welfare, i.e., the sum of individual rewards. As shown in the figure, Simul-Ind, CGA, and SGA converge to unstable points or local maxima. Simul-Co effectively navigates towards the apex of the collective reward landscape as depicted in Fig. 0(a). However, Simul-Co is ineffective in aligning individual and collective objectives, leading to the neglect of individual interests. As depicted in Fig. 0(b), the update trajectory navigates through the crests and troughs of Player 1's reward contour. The trajectory suggests a disregard for Player 1's preferences, signifying that the updates are predominantly driven by the overarching collective goal.

### Altruistic Gradient Adjustment

To tackle the challenge of aligning objectives in DMG, we propose the Altruistic Gradient Adjustment (AgA) method. Unlike existing gradient adjustment techniques (Mescheder et al., 2017; Balduzzi et al., 2018; Letcher et al., 2019; Chen et al., 2023), which primarily aim at achieving stable fixed

Figure 2: **Left figure a: Illustration of Corollary 4.3. In case 1, within an unstable fixed point’s neighborhoods, an appropriate selection of the \(\) sign push AgA to evade the unstable fixed point and pull towards a stable fixed point in its neighborhoods as shown in case 2. Right figure b: Alignment Effectiveness of AgA. The comparison between AgA (shown in red) and AgA without sign alignment (AgA-Sign, in purple) trajectories spans 40 steps, marked at every tenth step. Norm gradients are represented with blue arrows. Starting from the 14th step, sign alignment pulls the gradient toward the steepest direction, resulting in AgA reducing the number of steps by approximately 15% compared to AgA-Sign by the end of the trajectory.**

points for individual objectives, our AgA method simultaneously considers both individual and collective objectives when seeking stable fixed points for the collective objective. Specifically, AgA is defined as follows.

**Definition 4.2** (Altruistic Gradient Adjustment).: _Altruistic gradient adjustment (AgA) extends the gradient term in the learning dynamic as_

\[}:=_{c}+_{adj}=_{c}+( +_{c}^{T}_{c}),\] (1)

_where \(\) is alignment parameter, \(_{adj}\) is called adjustment term. In \(_{adj}\), \(_{c}\) and \(_{c}\) is the gradient vector and Hessian matrix of the game about collective loss._

Note that the Hessian matrix \(_{c}\) is symmetric. For brevity, we denote \(_{c}\) as \(_{}(\|_{c}\|^{2})\), which simplifies to \(_{c}^{T}_{c}\). In our AgA method, the formula \(_{c}+(+_{c}^{T}_{c})\) includes the Hessian matrix, but we do not compute it directly. Instead, we calculate Hessian-vector products \(_{c}^{T}_{c}\), which suffice for determining the adjustment gradient and the sign of \(\). This approach reduces computational complexity, enhancing the effectiveness of the AgA method. **The cost for computing Hessian-vector products \(_{c}^{T}_{c}\) is \((n)\) for \(n\) weights**(Pearlmutter, 1994). Refer to Section 6 for a detailed analysis.

While AgA introduces additional complexity, we theoretically demonstrate that an appropriate choice of the sign of \(\) ensures that AgA pulls the gradient towards a stable fixed point through the adjustment term in the neighborhood of fixed points. Conversely, when dealing with an unstable fixed point, AgA will push the gradient away from these unstable points.

Before introducing the corollary, we first provide some basic notations. The inner product of vectors \(\) and \(\) are denoted by \(,\). If the Hessian matrix \(\) is non-negative-definite, \(_{c}, 0\) for a non-zero \(_{c}\). Analogously, if \(\) is negative-definite, \(_{c},<0\) for a non-zero \(_{c}\). Lastly, the angle between the two vectors \(\) and \(\) is denoted by \((,)\). Then, we state the corollary as follows:

**Corollary 4.3**.: _In the neighborhood of fixed points of the collective objective, AgA will **pull** the gradient **toward** stable fixed points, which means \((},_{c})(_{c}, _{c})\), and **push away** from unstable ones, indicated by \((},_{c})(_{c}, _{c})\), if \(\) satisfies \(_{c},_{c}(, _{c}+\|_{c}\|^{2}) 0\)._

The proof is provided in Appendix B. Fig. 1(a) presents an illustrative visualization of Corollary 4.3. The figure depicts two scenarios: the neighborhoods of stable and unstable fixed point (denoted by a star), respectively. In the neighborhood of an unstable fixed point of the collective objective, as depicted in Case 1, our AaA gradient \(}\) is pushed out of the region, resulting in a faster escape compared to the original collective gradient \(_{c}\). This behavior is exemplified by the green arrow. Conversely, Case 2 illustrates the scenario of a stable fixed point, wherein our AaA gradient \(}\) is pulled towards the stable equilibrium, exemplified by the red arrow.

AgA could be easily integrated into any centralized training with decentralized execution (CTDE) framework within MARL. Detailed statements on the implementation of AgA, including the pseudocode, are provided in Appendix C.

### Alignment Effectiveness of AgA: A Toy Experiment

To address the two-player DMG as outlined in Example 4.1, we incorporate the AgA method into the fundamental gradient descent algorithm. The optimization trajectories in both the collective reward landscape and the individual player reward contour are represented in Fig. 0(a), Fig. 0(b), and Fig. 0(c). In contrast to the objective misalignment exhibited by Simul-Co, AgA successfully aligns the agents' interests, as evidenced in Fig. 0(b) and Fig 0(c). Fig. 0(b) depicts the trajectory of AgA meticulously carving its course along the summit of the individual reward contour. Furthermore, AgA is also shown to improve Player 2 reward (as shown in Fig. 0(c)), despite a slower convergence rate than Simul-Co.

In addition, Fig. 1(b) presents a critical comparison between the trajectories of AgA (in red) and AgA without sign alignment (AgA-Sign, depicted in purple), as outlined in Corollary 4.3. This side-by-side comparison covers 40 steps and features highlighted points every tenth step. It provides a visual illustration of the effectiveness introduced by sign alignment starting from the 14th step in the trajectories of AgA and AgA without sign alignment. Remarkably, norm gradients are represented by blue arrows, indicating the direction of the fastest updates. As depicted in the figure, AgA with sign selection is aligned toward the steepest update direction, resulting in a reduction of approximately 15% in the number of steps compared to AgA-Sign by the end of the trajectory.

## 5 Experiments

To assess the effectiveness of the proposed AgA algorithm, we perform comprehensive experiments across various environments, ranging from simple to complex, and from small to large scales. The initial environment involves a two-player public goods game (see Section 5.1), followed by sequential social dilemma environments (see Section 5.2): Cleanup and Harvest, involving 5 homogeneous players. The final test is conducted in our specially developed selfish-MMM2 environment (see Section 5.3), which is both more complex and larger in scale, featuring approximately 10 controlled agents of 3 distinct types, competing against 11 adversaries. Comparative experiments employ commonly used baseline approaches, such as simultaneous optimization with individual and collective losses (termed Simul-Ind and Siml-Co), CGA (Mescheder et al., 2017), SGA (Balduzzi et al., 2018), SVO (McKee et al., 2020), and the selfishness level approach (termed SL) (Roesch et al., 2024).

### Two-Player Public Goods Game

A two-player public goods matrix game \(\) is widely utilized to study cooperation in social dilemmas. The game involves players \(\{1,2\}\) with parameters \(\{_{1},_{2}\}\) and payoffs \(\{p_{1},p_{2}\}\). The social welfare, denoted as \(SW=p_{1}+p_{2}\). Each player, i.e., \(i\{1,2\}\), contributes an amount \(a_{i}\) within a budgeted range \([0,b]\), and the host evenly distributes these contributions as \((a_{1}+a_{2})\), where \(1<c 2\). Consequently, each player's payoff \(p_{i}(a_{1},a_{2})\) is estimated as \(b-a_{i}+(a_{1}+a_{2})\). In our experiments, we set the budget \(b\) to \(1\) and weight \(c\) to \(1.5\), with the social optimum of the game at \((1,1)\).

**Results.** Table 1 presents a comparison of individual rewards and the collective outcome, derived from 50 random trials, each limited to 100 steps. The rows \(r_{1}\), \(r_{2}\), \(SW\), and \(E\) represent the individual rewards for each player, the group's total welfare (SW), and the equality metric (E), respectively. The equality metric is based on the Gini coefficient (\(G\)) (David, 1968), with \(E\) defined as \(E=1-G=1-}_{i=1}^{n}i(p_{i}-)\), where \(\) is the mean of the ranked payoff vector \(p\), and \(n\) is the total number of players. A higher \(E\) value indicates greater equality among the players.

The social optimum of the game occurs when all players allocate their entire budget, achieving the highest possible social welfare score of \(3\). Among all algorithms, AgA achieves the closest social welfare score to this optimum, with a value of 2.90. Furthermore, AgA exhibits the greatest degree of fairness, with two players receiving nearly identical rewards and the highest equality value, in contrast to the baseline algorithms. Furthermore, Fig. 4 in Appendix D illustrates the action distributions of these algorithms, showing that AgA actions are more tightly clustered around the social optimum actions \((1,1)\). This further indicates that AgA approaches the social optimum more effectively.

### Sequential Social Dilemma: Cleanup and Harvest

We then verify the AgA algorithm in two widely used sequential social dilemma games with 5 homogeneous agents: Harvest and Cleanup (Hughes et al., 2018). In the Cleanup scenario, agents predominantly gather rewards by harvesting apples in an orchard, where the yield is affected by the river's pollution levels. Neglecting rising pollution levels ultimately ceases apple production, thus setting up a trade-off between individual gains and communal welfare. In contrast, the Harvest

  
**Metrics** & **Simul-Ind** & **CGA** & **SGA** & **SVO** & **Simul-Co** & **SL** & **AgA** \\  \(}\) & 1.133 \(\) 0.063 & 1.156 \(\) 0.060 & 1.175 \(\) 0.062 & 1.104 \(\) 0.054 & 1.433 \(\) 0.056 & 1.314 \(\) 0.062 & **1.4433 \(\) 0.042** \\  \(}\) & 1.184 \(\) 0.065 & 1.150 \(\) 0.057 & 1.137 \(\) 0.063 & 1.060 \(\) 0.051 & 1.381 \(\) 0.065 & 1.371 \(\) 0.057 & **1.459 \(\) 0.041** \\ 
**SW** & 2.316\(\) 0.039 & 2.306 \(\) 0.039 & 2.312\(\) 0.044 & 2.164 \(\) 0.026 & 2.814 \(\) 0.033 & 2.684 \(\) 0.049 & **2.903 \(\) 0.023** \\ 
**E** & 0.923\(\) 0.014 & 0.929\(\) 0.012 & 0.924 \(\) 0.013 & 0.930 \(\) 0.011 & 0.941 \(\) 0.014 & 0.940 \(\) 0.011 & **0.960 \(\) 0.008** \\   

Table 1: The comparison of the average individual rewards (denoted as \(r_{1},r_{2}\)), social welfare (denoted as \(SW\)), and equality metric (denoted as \(E\)) on two- player public goods game. We show the mean of value and 95% confidence interval utilizing 50 random runs.

scenario compensates agents for apple collection, with the regeneration of apples being ideally dependent on the proximity to other apples. Here, the communal challenge manifests itself as over-harvesting apples, which diminishes their regrowth rate and, consequently, the aggregate rewards for the group. Our research assessed AgA against various benchmarks, including Simul-Ind, Simul-Co, CGA, SVO, and SL. Simul-Ind was implemented using the IPPO algorithm (de Witt et al., 2020), while the remaining benchmarks employed the common parameters of the IPPO framework. The formula for calculating the collective loss for CGA and AgAs, which aims to improve both the performance of the group and the equitable distribution between agents, is expressed as \(_{i}(r_{i}-(1-(r_{j}}{r_{i}} ))\), where \(\) is a constant. Further details on the experimental setups and the algorithms used are provided in Appendix E.1.

**Varying \(\) values.** Fig. 2(a) and Fig. 2(b) explore the social welfare outcomes from AgA under three distinct \(\) values: 0.1, 1, 100, and 1000. Among these, \(=100\) demonstrates superior performance in both the Cleanup and Harvest scenarios. Therefore, in the subsequent experiments involving sign alignment and comparing primary results with baseline models, we will employ \(=100\) as the default parameter.

**The effectiveness of sign alignment.** In the second row of Fig. 3, Fig. 2(d) and Fig. 2(e) illustrate the social welfare comparisons between AgA and AgA without sign alignment (AgA-Sign) during their training phases. Referencing Corollary 4.3, near a fixed point, sign alignment helps direct the gradient towards stable fixed points and away from unstable ones. During the latter half of the training sessions shown in Fig. 2(d) and Fig. 2(e), it is observed that the social welfare (\(SW\)) for AgA-Sign does not improve to the same extent as it does for AgA, highlighting the effectiveness of sign alignment.

**Baseline Comparison.** As illustrated in Fig. 2(g) and Fig. 2(h), AgA demonstrates a notable improvement in social welfare over Simul-Ind, Simul-Co, CGA, SVO, and SL. Particularly, within the Cleanup scenario, AgA recorded an average social welfare of 105.15, marking approximately a 56% increase over the runner-up method SL. In the Harvest scenario, AgA shows an average improvement of 33.55 in social welfare over the next best method SVO. Table 2 compares the mean and standard deviation of the equality metric achieved by different methods in the Harvest and Cleanup environments. A value closer to 1 signifies more equal rewards among agents. As shown in the table, AgA outperforms the baseline methods, demonstrating its ability to effectively balance the interests of all team members.

### Selfish-MMM2

While sequential social dilemma games are frequently used to study mixed-motive problems, they are relatively simplistic compared to other experimental environments in MARL, particularly when considering real-world applications. To address this gap, we introduce a sophisticated mixed-motive testbed called Selfish-MMM2, which is adapted from the MMM2 map in the StarCraft II game (Samvelyan et al., 2019). Unlike the standard SMAC framework, which employs a shared reward structure, our methodology assigns individual objectives to each agent, thereby enhancing the complexity of mixed motives. Specifically, we have redesigned the reward systems so that each agent's personal gain is directly tied to the damage they inflict on adversaries, diverging from traditional approaches where collective damage is evenly distributed among all agents. Additionally, penalties for agent elimination emphasize the importance of self-preservation and highlight agents' tendencies towards self-interested behavior. In comparison to widely utilized multi-agent environments such as Cleanup (Hughes et al., 2018), Harvest (Hughes et al., 2018), and the 10-player Prisoner's Dilemma (PD) for large-scale evaluations (Gemp et al., 2022), Selfish-MMM2 is distinguished by two notable

    &  &  &  &  &  &  \\   & & & & & & \(=0.1\) & \(=100\) & \(=1000\) \\   & 0.973 & 0.975 & 0.974 & 0.950 & 0.972 & 0.981 & **0.988** & 0.982 & 0.980 \\  & \(\) 0.005 & \(\) 0.006 & \(\) 0.007 & \(\) 0.051 & \(\) 0.005 & \(\) 0.006 & \(\) **0.003** & \(\) 0.012 & \(\) 0.006 \\   & 0.841 & 0.948 & 0.902 & 0.903 & 0.946 & 0.940 & 0.956 & **0.959** & 0.905 \\  & \(\) 0.071 & \(\) 0.013 & \(\) 0.019 & \(\) 0.034 & \(\) 0.016 & \(\) 0.017 & \(\) 0.007 & \(\) **0.011** & \(\) 0.022 \\   

Table 2: The comparison of the average equality metric (denoted as \(E\)) on Harvest and Cleanup. We show the mean of equality value and standard deviation utilizing three random runs.

features: 1) Large-Scale Agents: Selfish-MMM2 manages the interactions of 10 heterogeneous agents, categorized into three types: seven Marines, two Marauders, and one Medivac, confronting 11 enemy units. This configuration is in stark contrast to Cleanup and Harvest, which each utilize five homogeneous agents. 2) Extensive Action Space: The action space in Selfish-MMM2, sized at \(18^{10}\), is vastly larger than that of Cleanup and Harvest, which are limited to \(9^{5}\) and \(8^{5}\), respectively. It also surpasses the action space of the 10-player PD, which is \(2^{10}\).

In evaluating the Selfish-MMM2 environment, we use the average win rate against the built-in AI bot in SMAC to measure the collective performance. The experimental setup employs the IPPO algorithm (de Witt et al., 2020) for the Simul-Ind algorithm, while the MAPPO (Yu et al., 2022) is utilized for the Simul-Co, SVO, CGA, SL and AgA algorithms. More details on the Selfish-MMM2 environment and implementation specifics are available in the Appendix E.2.

**Varying \(\) Values and the effectiveness of Sign Alignment:** Fig. 2(c) presents the average win rates in various settings of the align parameter \(\) in the AgA scenario. The findings indicate that increased \(\) values are associated with faster convergence in the training period. Among these, a \(\) value of 100 demonstrates a higher overall effectiveness compared to other values tested. Based on these findings, we have set a default value \(\) of 100 for subsequent experiments. Furthermore, Fig. 2(f) in the second row illustrates a comparable conclusion in sequential social dilemma games. During the

Figure 3: **The first row** displays results comparing different values of the alignment parameter \(\) across three environments: Cleanup and Harvest (measurement of social welfare, SW) and Selfish-MMM2 (focusing on win rate). **The second row** examines the performance differences between the proposed AgA and AgA without sign alignment (AgA-Sign) on the three testbeds. The bold lines indicate the mean social welfare calculated in three seeds, while the surrounding shaded areas represent the 95% confidence interval. **The third row** compares the AgA method with baseline approaches on these testbeds. Each bar represents the mean collective results of each method and the error bars indicate the 95% confidence interval.

initial training phase, the convergence rates of AgA and AgA-Sign are nearly identical. However, as training progresses, AgA exhibits convergence at points of superior performance, highlighting the critical role of sign alignment in achieving convergence near fixed points.

**Baseline Comparison.** As depicted in Fig. 2(i), the bar chart displays a comparison of the average win rates along with 95% confidence intervals for AgA versus other baseline approaches. Our AgA approach attains the highest team performance, securing a 92.64% win rate with the built-in bot, which is 23.61 percent more than the next best win rate achieved by the SL method.

## 6 Discussion

**Computational Cost of AgA.** The altruistic gradient adjustment is expressed as \(}=_{c}+(+_{c}^{T}_{c})\), where the key computational cost comes from the Hessian-vector product, \(_{c}^{T}_{c}\). The cost for computing Hessian-vector products, \(_{c}^{T}_{c}\), is \((n)\) for \(n\) weights . This introduces added complexity compared to standard methods in mixed-motive MARL, making AgA's running time generally more than twice as long as gradient-based methods. Table 3 provides a comparison of the average running time between AgA and baseline methods in a two-player public goods game, including total duration, timesteps, time per step, and the time ratio relative to Simul-Ind. Simul-Ind, Simul-Co, and SL are standard gradient-based methods, while CGA and SGA modify gradients. Our results show that AgA takes 2-3 times longer per step but remains the most efficient, requiring only 1389 steps over 50 runs, compared to around 4000 steps for the baselines.

**Limitations.** Despite conducting a series of experiments to verify the proposed AgA algorithm, including using a new large-scale mixed-motive cooperation testbed, these environments remain somewhat removed from real-world applications. Although we strive to align the objectives of both individuals and the collective, our primary focus in this paper is on converging towards stable fixed points of the collective objective. For future research, we plan to delve deeper into the interaction of individual objectives in mixed-motive games, with an increased emphasis on understanding their dynamics in real-world mixed-motive cooperation scenarios.

## 7 Conclusion

In this paper, we propose AgA, an optimization method specifically designed to align individual and collective objectives through gradient adjustments in mixed-motive cooperation scenarios. To achieve this, we first model the mixed-motive game as a differentiable game, offering a powerful tool for analyzing learning dynamics at both individual and collective levels. Furthermore, we theoretically demonstrate that AgA can effectively attract gradients to stable fixed points and support our claims with empirical evidence. To evaluate the effectiveness of AgA in complex and large-scale scenarios, we introduce a new mixed-motive environment called Selfish-MMM2, which features heterogeneous large-scale agents, a larger action space, and increased task complexity. Comprehensive experiments, ranging from simple public goods games to Harvest, Cleanup, and Selfish-MMM2, show AgA's superior performance, consistently outperforming existing baselines across multiple evaluation metrics. These results validate our theoretical claims and highlight the effectiveness of AgA in achieving cooperative behavior in multi-agent systems.

  
**Metrics** & **Simul-Ind** & **Simul-Co** & **SL** & **CGA** & **SGA** & **AgA** \\ 
**Duration (ms)** & 1165.79 & 910.15 & 1149.97 & 3041.84 & 3007.77 & 1034.69 \\
**Steps** & 4272 & 3252 & 3887 & 4478 & 4179 & 1389 \\
**Step Time (ms)** & 0.27 & 0.28 & 0.30 & 0.68 & 0.72 & 0.74 \\
**Ratio** & 1.00 & 1.04 & 1.11 & 2.52 & 2.67 & 2.74 \\   

Table 3: Comparison of the average running time between baseline methods and AgA in the two-player public goods game, including total duration, timesteps, time per step, and the time per step ratio relative to Simul-Ind.