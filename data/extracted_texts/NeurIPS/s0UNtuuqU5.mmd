# GeoMFormer: A General Architecture for Geometric Molecular Representation Learning

Tianlang Chen\({}^{1}\), &Shengjie Luo\({}^{2*,}\), &Di He\({}^{2,}\), &Shuxin Zheng\({}^{3}\), &Tie-Yan Liu\({}^{3}\), &Liwei Wang\({}^{2,}\),\({}^{4,}\)

\({}^{1}\)School of EECS, Peking University \({}^{2}\)National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University \({}^{3}\)Microsoft Research

\({}^{4}\)Center for Machine Learning Research, Peking University

tlchen@pku.edu.cn, luosj@stu.pku.edu.cn, dihe@pku.edu.cn

{shuz, tyliu}@microsoft.com, wanglu@pku.edu.cn

Equal contributions.

###### Abstract

Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed _cross-attention_ modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales. Code and models will be made publicly available.

## 1 Introduction

Deep learning approaches have emerged as a powerful tool for a wide range of tasks . Recently, researchers have started investigating whether the power of neural networks could help solve problems in physics and chemistry, such as predicting the property of molecules with 3D coordinates and simulating how each atom moves in Euclidean space . These molecular modeling tasks require the learned model to satisfy general physical laws, such as the invariance and equivariance conditions: The model's prediction should react _physically_ when the input coordinates change according to the transformation of the coordinate system, such as rotation and translation.

A variety of methods have been proposed to design neural architectures that intrinsically satisfy the invariance or equivariance conditions . To satisfy the invariant condition, several approaches incorporate invariant features, such as the relative distance between each atom pair, into classic neural networks . However, this may hinder the model from effectively extracting the molecular structural information. For example, computing dihedral angles from coordinates is straightforward but requires much more nonlinear operations using relative distances. To satisfy the equivariant condition, several works design neural networks with equivariant operation only, such as tensor product between irreducible representations  and vector operations . However, the number of such operations is limited, and they are either costly to scale or lead to fairly complex network architecture designs to guarantee sufficient expressive power. More importantly, manyreal-world applications require a model that can effectively perform both invariant and equivariant prediction with strong performance at the same time. While some recent works study this direction [50; 55], most proposed networks are designed heuristically and lack general design principles.

We argue that developing a general and flexible architecture to effectively learn both invariant and equivariant representations is essential. In this work, we introduce GeoMFormer to achieve this goal. GeoMFormer uses a standard Transformer-based architecture  but with two streams. An invariant stream learns invariant representations, and an equivariant stream learns equivariant representations. Each stream consists of invariant/equivariant self-attention and feed-forward layers. The key design in GeoMFormer is to use _cross-attention mechanisms_ between the two streams, letting each stream incorporate the information from the other and enhance itself. In each layer of the invariant stream, we develop an _invariant-to-equivariant_ cross-attention module, where the invariant representations are used to query key-value pairs in the equivariant stream. An _equivariant-to-invariant_ cross-attention module is similarly designed for the equivariant stream. We show that the design of all self-attention and cross-attention modules is flexible and how to satisfy the invariant/equivariant conditions effectively.

Our proposed architecture has several advantages compared to previous works. GeoMFormer decomposes the invariant/equivariant representation learning through self-attention and cross-attention modules. By interacting the two streams using cross-attention modules, the invariant stream receives more structural signals (from the equivariant stream), and the equivariant stream obtains more non-linear transformation (from the invariant stream), which allows simultaneously and completely modeling interatomic interactions within/across feature spaces in a unified manner. Furthermore, we demonstrate that the proposed decomposition is general by showing that many existing methods can be regarded as special cases in our framework. For example, PaiNN and TorchMD-NET can be formulated as a special instantiation by following the design philosophy of GeoMFormer and using proper instantiations of key building components. From this perspective, we believe our architecture can offer many different options in different scenarios in real applications.

We evaluate our architecture on diverse datasets with both invariant and equivariant targets. On the Open Catalyst 2020 (OC20) dataset , which contains large atomic systems composed of an adsorbate and a catalyst, our architecture accurately predicts energy (invariant) and relaxed structure (equivariant). Additionally, t achieves state-of-the-art performance in predicting the homo-lomo energy gap (invariant) on PCQM4Mv2  and Molecule3D  datasets, both comprising molecules from chemical databases[41; 44]. Moreover, we conduct an N-body simulation experiment, wherein our architecture precisely forecasts particle positions (equivariant) governed by physical rules. All the empirical results highlight the generality and effectiveness of our approach.

## 2 Related Works

**Invariant Representation Learning.** Recently, invariance has been recognized as one of the key principles guiding the development of molecular models. To describe the properties of a molecular system, the model's prediction should remain unchanged if we conduct any rotation or translation actions on the coordinates of the whole system. Previous works typically used relative structural information from coordinates to inherently maintain invariance. In SchNet , interatomic distances were encoded via radial basis functions, serving as weights for the continuous-filter convolutional layers. PhysNet  similarly incorporated both atomic features and interatomic distances in its interaction blocks. Graphormer-3D  employs a Transformer-based model, encoding relative distances as attention bias terms, which shows strong performance on large-scale datasets .

Beyond the interatomic distance, other works further incorporate high-order invariant signals. Based on PhysNet, DimeNet  and DimeNet++  additionally encode the bond angle information using Fourier-Bessel basis functions. Moreover, GemNet  and GemNet-OC  carefully studied the connections between spherical representations and directional information, which inspired to leverage the dihedral angles, i.e., angles between planes formed by bonds. SphereNet  and ComENet  consider the torsional information to augment the molecular models. During the development in the literature, more complex features are incorporated due to the lossy structural information when purely learning invariant representations, while largely increasing the costs. Moreover, these invariant models are generally unable to directly perform equivariant prediction tasks.

**Equivariant Representation Learning.** Rather than focusing solely on invariant blocks, various works aim to learn equivariant representations. In real-world applications, numerous molecular tasks require equivariant predictions, such as predicting forces, positions, velocities, and other tensorized properties in dynamic simulations. When rotating positions, these properties should rotate correspondingly. One classical approach [56; 13; 3; 43] to encoding the equivariant constraints is using irreducible representations (irreps) via spherical harmonics . Equivariant convolutions based on tensor products between irreps enable models to preserve equivariance. However, these models do not always significantly outperform invariant models on invariant tasks. Besides, their operations are in general costly [50; 48; 12], hindering deployment in large-scale molecular systems.

On the other hand, several recent works maintain both invariant and equivariant representations. EGNN  proposed a simple framework. Its invariant representations encode type information and relative distance, and are further used in vector scaling functions to transform the equivariant representations. PaiNN  extended EGNN's framework to include the Hardamard product operation to transform the equivariant representations. Based on the operations of PaiNN, TorchMD-Net  further proposed a modified version of the self-attention modules to update invariant representations and achieved better performance on invariant tasks. Allegro  used tensor product operations to update equivariant features and interacted equivariant and invariant features by using weight-generation modules. In contrast, our GeoMFormer is developed based on a general design philosophy to learn both invariant and equivariant representations, enabling simultaneous and complete modeling of interatomic interactions within/across feature spaces in a unified manner, as introduced in Section 4.1.

## 3 Preliminary

### Notations & Geometric Constraints

We denote a molecular system as \(\), which consists of a collection of atoms held together by attractive forces. Let \(^{n d}\) denote the atoms with features, where \(n\) is the number of atoms, and \(d\) is the feature dimension. Given atom \(i\), we use \(_{i}^{3}\) to denote its cartesian coordinate in the three-dimensional Euclidean space. We define \(=(,R)\), where \(R=\{_{1},...,_{n}\}\).

In nature, molecular systems are subject to physical laws that impose geometric constraints on their properties and behaviors. For instance, if the position of each atom is translated by a constant vector in Euclidean space, the system's total energy remains unchanged. If a rotation is applied to each position, the direction of the force on each atom will rotate correspondingly. Mathematically, these geometric constraints are closely tied to the principles of invariance and equivariance in group theory. [9; 8; 52].

Formally, let \(:\) denote a function mapping between vector spaces. Given a group \(G\), let \(^{}\) and \(^{}\) denote its group representations. A function \(:\) is said to be equivariant/invariant if it satisfies the following conditions respectively:

\[}&^{ }(g)[(x)]=(^{}(g)[x])g G,x\\ }&(x)=(^{ }(g)[x])g G,x\] (1)

Intuitively, an equivariant function transforms the output predictably in response to input transformations, while an invariant function produces an unchanged output when transformations are applied to the input. For additional background on group theory, please refer to the appendix of [56; 1; 13].

Molecular systems are naturally located in the three-dimensional Euclidean space, and the group related to translations and rotations is known as \(SE(3)\). For each element \(g\) in the \(SE(3)\) group, its representation on \(^{3}\) can be parameterized by pairs of translation vectors \(^{3}\) and orthogonal transformation matrices \(^{3 3},()=1\), i.e., \(g=(,)\). Given a vector \(x^{3}\), we have \(^{^{3}}(g)[x]:=x+\). For molecular modeling, it is essential to learn molecular representations that encode the rotation equivariance and translation invariance constraints. Formally, let \(V_{}\) denote the space of molecular systems, for each atom \(i\), we define equivariant representation \(^{E}\) and invariant representation \(^{I}\) if \(\;g=(,) SE(3),=(,R) V_{ }\), the following conditions are satisfied:

\[&^{E}:V_{}^{3  d}, 56.905512pt^{E}(,\{_{1},..., _{n}\})=^{E}(,\{_{1},...,_{n}\})\\ &^{E}:V_{}^{3 d},  56.905512pt^{E}(,\{_{1},...,_{n}\})= ^{E}(,\{_{1}+,...,_{n}+ \})\\ &^{I}:V_{}^{d}, 56.905512pt ^{I}(,\{_{1},...,_{n}\})=^{I}(,\{_{1}+,...,_{n}+\})\] (2)

### Attention module

The attention module lies at the core of the Transformer architecture , and it is formulated as querying a dictionary with key-value pairs, e.g., \((Q,K,V)=(}}{})V\), where \(d\) is the hidden dimension, and \(Q\) (Query), \(K\) (Key), \(V\) (Value) are specified as the hidden representations of the previous layer. The multi-head variant of the attention module is widely used, as it allows the model to jointly attend to information from different representation subspaces. It is defined as follows:\[(Q,K,V) =(_{1},,_{H})W^{O}\] (3) \[_{k} =(QW_{k}^{Q},KW_{k}^{K},VW_{k}^{V}),\]

where \(W_{k}^{Q}^{d d_{H}},W_{k}^{K}^{d d_{H}}, W_{k}^{V}^{d d_{H}}\), and \(W^{O}^{Hd_{H} d}\) are learnable matrices, \(H\) is the number of heads. \(d_{H}\) is the dimension of each attention head.

Serving as a generic building block, the attention module can be used in various ways. On the one hand, the self-attention module specifies Query, Key, and Value as the same hidden representation, thereby extracting contextual information for the input. It has been one of the key components in Transformer-based foundation models across various domains [10; 5; 11; 38; 64; 28]. On the other hand, the cross-attention module specifies the hidden representation from one space as Query, and the representation from the other space as Key-Value pairs, e.g. encoder-decoder attention for sequence-to-sequence learning. As the cross-attention module bridges two representation spaces, it has been also widely used beyond Transformer for information fusion and improving representations [33; 24; 27; 26].

## 4 GeoMFormer

In this section, we introduce GeoMFormer, a novel Transformer-based molecular model for learning invariant and equivariant molecular representations. We begin by elaborating on the key designs of GeoMFormer, which form a general framework to guide the development of geometric molecular models (Section 4.1), Next we thoroughly discuss the implementation details of GeoMFormer (Section 4.2).

### A General Design Philosophy

As previously mentioned, several existing works learned invariant representations using invariant features, such as distance information, which may have difficulty in extracting other useful structural signals. Some other works developed equivariant models via equivariant operations, which are either heuristic or costly. Instead, we aim to develop a general design principle, which guides the development of a model instance that addresses the disadvantages aforementioned in both invariant and equivariant representation learning.

We call our model GeoMFormer, which is a two-stream Transformer model to encode invariant and equivariant information. Each stream is built up using stacked Transformer blocks, each of which consists of a self-attention module and a cross-attention module, followed by a feed-forward network. For each atom \(k[n]\), we use \(_{k}^{I}^{d}\) and \(_{k}^{E}^{3 d}\) to denote its invariant and equivariant representations respectively. Let \(^{I}=[_{1}^{I},...;_{n}^{I}]^{n d}\) and \(^{E}=[_{1}^{E};...;_{n}^{E}]^{n  3 d}\), the invariant (colored in red) and equivariant (colored in blue) representations are updated in the following manner:

\[}\ \{^{{}^{I,I}, }&=^{{}^{I,I}}\ +(^{{}^{I,I}},^{{}^{I,I}},^{ {}^{I,I}})\\ ^{{}^{I,I},}&=^{{}^{I,I},}\ +( ^{{}^{I,I}},^{{}^{I,E,I}},^{{}^{I,E,I}})\\ ^{{}^{I,I},}&=^{{}^{I,I},}\ +(^{{}^{I,I}})\\ .\] (4) \[}\{^{{}^{ I,E},}&=^{{}^{E,I}}\ +(^{{}^{E,I}},^{{}^{E,I}},^{ {}^{E,I}})\\ ^{{}^{I,E},}&=^{{}^{I,E},}\ +(^{{}^{E,I}},^{{}^{E,I}},^{ {}^{E,I},})\\ ^{{}^{E,I},}&=^{{}^{}{}^{E,I}}+(^{{}^{}{}^{E,I}})\\ .\]

where \(l\) denotes the layer index. In this framework, the self-attention modules and feed-forward networks are used to iteratively update representations in each stream. The cross-attention modules use representations from one stream to query Key-Value pairs from the other stream. By using this mechanism, a bidirectional bridge is established between invariant and equivariant streams. Besides the contextual information from the invariant stream itself, the invariant representations can freely attend to more geometrical signals from the equivariant stream. Similarly, the equivariant representations can benefit from using more non-linear transformations in the invariant representations. With the cross-attention modules, the expressiveness of both invariant and equivariant representation learning is largely improved, which allows simultaneously and completely modeling interatomic interactions within/across feature spaces in a unified manner. In this regard, as highlighted by different colors, the Query, Key, and Value in the self-attention modules (\(,\)) and the cross-attention modules (\(\),\(\)) are differently specified, which should carefully encode the geometric constraints mentioned in Section 3.1, as introduced below.

**Desiderata for Invariant Self-Attention.** Given the invariant representation \(^{I}\), the Query, Key and Value in \(\)-\(\)-\(\) are calculated via a function mapping \(^{I}:^{n 3 d}^{n 3 d}\), i.e., \(^{I}=_{Q}^{I}(^{I}),^{I}=_{K}^{I}( ^{I}),\,^{I}=_{V}^{E}(^{I})\). Essentially, the attention module linearly transforms the Value \(^{I}\), with the weights being calculated from the dot product between the Query and Key (i.e., attention scores). In this regard, if both \(^{I}\) and the attention scores preserve the invariance, then the output satisfies the invariant constraint, i.e., \(^{I}\) is required to be invariant. Under this condition, it is easy to check the output representation of this module keeps the invariance, which is proved in the appendix.

**Desiderata for Equivariant Self-Attention.** Similarly, given the equivariant input \(^{E}\), the Query, Key and Value in \(\)-\(\)-\(\) are calculated via a function mapping \(^{E}:^{n 3 d}^{n 3  d}\), i.e., \(^{E}=_{Q}^{E}(^{E}),^{E}=_{K}^{E}( ^{E}),\,^{E}=_{V}^{E}(^{E})\). Similarly, \(^{E}\) is required to be equivariant. However, this still cannot guarantee the module to be equivariant if standard attention is used. We modified \(_{ij}=_{k=1}^{d}^{E_{[i,:,k]}^{E_{[j,:,k]}}}^{}\), where \(^{E_{[i,:,k]}}^{3}\) denotes the \(k\)-th dimension of the atom \(i\)'s Query. It is straightforward to check the equivariance is preserved, which is proved in the appendix.

**Desiderata for Cross-attentions between the two Streams.** In each stream, the cross-attention module is used to leverage information from the other stream. We call the cross attention in the invariant stream _invariant-cross-equivariant_ attention, and call the cross attention in the equivariant stream _equivariant-cross-invariant_ attention, i.e., \(\)-\(\)-\(\) and \(\)-\(\)-\(\). The difference between the two cross attention lies in how the query, key, value are specified:

\[}& ^{I\_E}=_{Q}^{I}(^{I}),^{I\_E}=_{K} ^{I_E}(^{I},^{E}),\,^{I\_E}=_{V}^{I_E}(^{I},^{E})\\ }& ^{E\_I}=_{Q}^{E}(^{E}),^{E_{ I}}= _{K}^{E_{ I}}(^{I},^{E_{ I}}),^{ E_{ I}}=_{V}^{E_{ I}}(^{E},^{I})\] (5)

First, for Query \(^{I\_E}\) and \(^{E\_I}\), the requirement to \(^{I}\) and \(^{E}\) remains the same as previously stated. Moreover, as distinguished by different colors, the Key-Value pairs and the Query are calculated in different ways, for which the requirement should be separately considered. Note that both \(^{I\_E}\) and \(^{E\_I}\) are still linearly transformed by the cross-attention modules. If \(^{I\_E}\) preserves the invariance and \(^{E\_I}\) preserves the equivariance, then the remaining condition is to keep the invariance of the attention score calculation. That is to say, for the \(\)-\(\)-\(\), both \(^{I}\) and \(^{I\_E}\) are required to be invariant. It is similar to the \(\)-\(\)-\(\) that both \(^{E}\) and \(^{E\_I}\) are required to be equivariant. In this way, the outputs of both cross-attention modules are under the corresponding geometric constraints, which is proved in the appendix.

**Discussion.** The carefully designed blocks outlined above provide a general design philosophy for encoding the geometric constraints and bridging the invariant and equivariant molecular representations, which lie at the core of our framework. Note that the translation invariance can be easily preserved by encoding relative structure signals of the input. It is also worth pointing out that we do not restrict the specific instantiation of each component, and various design choices can be adopted as long as they meet the requirements mentioned above. Moreover, we prove that our framework can include many previous models as an instantiation, e.g., PaiNN  and TorchMD-Net , can be extended to encode additional geometric constraints , which are presented in the appendix. In this work, we present a simple yet effective model instance that implements this design philosophy, which we will thoroughly introduce in the next subsection.

### Implementation Details of GeoMFormer

Following the design guidance in Section 4.1, we propose **Ge**ometric **M**olecular Transformer (GeoMFormer). The overall architecture of GeoMFormer is shown in Figure 1, which is composed of stacked GeoMFormer blocks (Eqn.(5)). We introduce the instantiations of the self-attention, cross-attention and FFN modules below and prove the properties they satisfy in the appendix. We also incorporate widely used modules like Layer Normalization  and Structural Encodings  for better empirical. Due to the space limits, we refer readers to the appendix for further details.

**Instantiation of Self-Attention.** In GeoMFormer, the linear function is used to implement both \(^{I}:^{n d}^{n d}\) and \(^{E}:^{n 3 d}^{n 3 d}\):

\[^{I}=_{Q}^{I}(^{I})= ^{I}W_{Q}^{I},^{I}=_{K}^{I}(^{I})=^{I }W_{K}^{I},^{I}=_{V}^{I}(^{I})=^{I }W_{V}^{I}\\ ^{E}=_{Q}^{E}(^{E})=^{E}_{Q}^{E},^{E}=_{K}^{E}(^{E})=^{E}W_{K}^{E}, ^{E}=_{V}^{E}(^{E})=^{E}W_{V}^{E}\] (6)

where \(W_{Q}^{I},W_{K}^{I},W_{V}^{I},W_{Q}^{E},W_{K}^{E},W_{V}^{E}^{d d _{H}}\) are learnable parameters.

**Instantiation of Cross-Attention.** As previously stated, both \(^{I\_E}\) and \(^{E\_I}\) in the cross-attention modules fuse representations from different spaces (invariant & equivariant) into target spaces. In the_Invariant-cross-Equivariant_ attention module (\(\)-\(\)-\(\)), to obtain the Key-Value pairs, the equivariant representations are mapped to the invariant space. For the sake of simplicity, we use the dot-product operation \(<,>\) to instantiate \(^{I\_E}\). Given \(X,Y^{n 3 d}\), \(Z=<X,Y>^{n d}\), where \(Z_{[i,k]}=X_{[i_{:},i_{:}]}_{[i_{:},k]}\). Then the Key-Value pairs in \(\)-\(\)-\(\) are calculated as:

\[^{I\_E}=^{I\_E}_{K}(^{I},^{E})=<^ {E}W^{I\_E}_{K,1},^{E}W^{I\_E}_{K,2}>,^{I\_E}=^{I \_E}_{V}(^{I},^{E})=<^{E}W^{I\_E}_{V,1},^{E}W^{I\_E}_{V,2}>\] (7)

where \(W^{I\_E}_{K,1},W^{I\_E}_{K,2},W^{I\_E}_{V,1},W^{I\_E}_{V,2}^{d  d_{H}}\) for Key and Value are learnable parameters. On the other hand, the invariant representations are mapped to the equivariant space in the _Equivariant-cross-Invariant_ attention module (\(\)-\(\)-\(\)). To achieve this goal, we use the scalar product \(\) to instantiate \(^{E\_I}\). Given \(X^{n 3 d},Y^{n d}\), \(Z=X Y^{n 3 d}\), where \(Z_{[i_{:},j_{:}]}=X_{[i_{:},j_{:}]} Y_{[i_{:},k]}\). Using this operation, the Key-Value pairs in \(\)-\(\)-\(\) are calculated as:

\[^{E\_I}=^{E\_I}_{K}(^{E},^{I})=^{E }W^{E\_I}_{K,1}^{I}W^{E\_I}_{K,2},^{E\_I}= ^{E\_I}_{V}(^{E},^{I})=^{E}W^{E\_I}_{V,1} ^{I}W^{E\_I}_{V,2}\] (8)

where \(W^{E\_I}_{K,1},W^{E\_I}_{K,2},W^{E\_I}_{V,1},W^{E\_I}_{V,2}^{d  d_{H}}\) are learnable parameters.

**Instantiation of Feed-Forward Networks.** Feed-forward networks (FFN) also play important roles in refining contextual representations. In the invariant stream, the FFN is kept unchanged from the standard Transformer model, i.e., \(\)-\((^{^{I}})=(^{ ^{I}}W^{I}_{1})W^{I}_{2}\), where \(W^{I}_{1}^{d r},W^{I}_{2}^{r d}\) and \(r\) denotes the hidden dimension of the FFN layer. In the equivariant stream, it is worth noting that commonly used non-linear activation functions break the equivariant constraints. In our GeOMFormer, we use the invariant representations as a gating function to non-linearly activate the equivariant representations, i.e., \(\)-\((^{^{E}})=(^{ ^{E}}W^{E}_{1}(^{^{I}}W^{I}_{2}) )W^{E}_{3}\), where \(W^{E}_{1},W^{I}_{1}^{d r},W^{E}_{2}^{r d}\).

**Input Layer.** Given a molecular system \(=(,R)\), we set the invariant representation at the input as \(^{I,0}=\), where \(_{i}^{d}\) is a learnable embedding vector indexed by the atom \(i\)'s type. For the equivariant representation, we set \(^{E,0}_{i}=}^{I}_{i}g(||^{}_{i}||) ^{}^{3 d}\), where we consider both the direction \(}^{i}_{i}^{3}\) and the scale \(g(||^{}_{i}||)^{d}\) of the each atom's mean-centered position \(^{}_{i}\). \(g:^{d}\) is instantiated by the Gaussian Basis Kernel, i.e., \(g(||^{}_{i}||)=_{i}W\), \(_{i}=[^{1}_{i};...;^{d}_{i}]^{}\), \(^{k}_{i}=-|^{k}|}(-( ||^{}_{i}||+_{i}-^{k}}{|^{k}|} )^{2}),k=1,...,d\), where \(W^{d d}\) is learnable, \(_{i},_{i}\)

Figure 1: An illustration of our GeoMFormer model architecture.

[MISSING_PAGE_FAIL:7]

### PCQM4MV2 Performance (Invariant)

PCQM4MV2 is one of the largest quantum chemical property datasets from the OGB Large-Scale Challenge (). The task involves predicting the HOMO-LUMO energy gap of a molecule's equilibrium structure, evaluating the model's capacity for invariant prediction. This property holds significance in real applications such as reactivity. Ground-truth labels are derived from DFT calculations. The total number of training samples is around 3.37 million.

In a practical setting, the DFT-calculated equilibrium geometric structure of each training sample is provided, but only initial structure is available for each validation sample. In this regard, we adopt one recent approach (UniMol+ ) to handle this task. During training, the model receives efficient but inaccurate RDKit-generated structures as input, and predicts both the HOMO-LUMO energy gap and the equilibrium structure by using both invariant and equivariant representations. After training, the model can be used to predict the HOMO-LUMO gap target by only using the initial structure, which meets the requirement of the settings. We compare to various baselines in the leaderboard. More details are presented in the appendix.

From Table 4, our GeoMFormer achieves the lowest MAE among the quadratic models, specifically, 6.7\(\%\) relative MAE reduction compared to the previous best model. Besides, compared to the best model Uni-Mol+ , our GeoMFormer achieves competitive performance while keeping the efficiency (\((n^{2})\) complexity), which can be more applicable to large molecular systems. Overall, the results further verify the effectiveness of GeoMFormer on invariant representation learning.

### N-Body Simulation Performance (Equivariant)

Simulating dynamical systems consisting of a set of geometric objects interacting under physical laws is crucial in many applications, e.g. molecular dynamic simulation. Following [13; 48], we use a synthetic n-body system simulation task as an extension of molecular modeling tasks. It requires the model to predict positions of a set of particles and evaluates the model's ability for equivariant predictions. The simulated system consists of 5 particles, and each carries a positive or negative charge and has an initial position and velocity. The system adheres to physical rules involving attractive and repulsive forces. The dataset contains 3000 trajectories for training, and 2000 trajectories for validation and testing respectively. We compare several competitive baselines following . Detailed descriptions of data generation, training settings, and baselines can be found in the appendix. The results are shown in Table 5. Our GeoMFormer achieves the best performance compared to all baselines. In particular, the significant 33.8\(\%\) MSE reduction indeed demonstrates the GeoMFormer's superior ability on learning equivariant representations.

## 6 Conclusion

In this paper, we propose a general and flexible architecture, called GeoMFormer, for learning geometric molecular representations. Using the standard Transformer backbone, two streams are developed for learning invariant and equivariant representations respectively. In particular, the cross-attention mechanism is used to bridge these two streams, letting each stream leverage contextual information from the other stream and enhance its representations. This simple yet effective design significantly boosts both invariant and equivariant modeling. Within the newly proposed framework, many existing methods can be regarded as special instances, showing the generality of our method. We conduct extensive experiments covering diverse tasks, data and scales. All the empirical results show that our GeoMFormer can achieve strong performance in different scenarios. The potential of our GeoMFormer can be further explored in a broad range of applications in molecular modeling.

  Model & Complexity & \# param. & Valid MAE \(\) \\  MLP-Fingerrprint  & & 16.1M & 0.1735 \\ GINE-V8 [4; 18] & & 13.2M & 0.1167 \\ GCN-V8 [30; 18] & (n)\)} & 4.9M & 0.1153 \\ GIN-V8 [26; 18] & & 6.7M & 0.1083 \\ DeeperGCN-V8 [34; 18] & & 25.5M & 0.1021* \\  TokenGT  & & 48.5M & 0.0910 \\ EGT  & & 89.3M & 0.0869 \\ GRPE  & & 46.2M & 0.0867 \\ Graphormer [64; 53] & (n^{2})\)} & 47.1M & 0.0864 \\ GraphGPS  & & 19.4M & 0.0858 \\ GPS+  & & 44.3M & 0.0778 \\ Transformer-M  & & 47.1M & 0.0787 \\  GEM-2  & (n^{3})\)} & 32.1M & 0.0793 \\ Uni-Mol+  & & 52.4M & 0.0708* \\  GeoMFormer (ours) & \((n^{2})\) & 54.5M & 0.0734* \\  

Table 4: Results on PCQM4MV2. The evaluation metric is the Mean Absolute Error (MAE). We report the official results of baselines. \(*\) indicates the best performance achieved by models with the same complexity (\(n\) denotes the number of atoms).

  Model & MSE \(\) \\  SE(3) Transformer  & 0.0244 \\ Tensor Field Network  & 0.0155 \\ Graph Neural Network  & 0.0107 \\ Radial Field  & 0.0104 \\ EGNN  & 0.0071 \\  GeoMFormer (ours) & **0.0047** \\  

Table 5: Results on N-body Simulation experiment. We report the official results of baselines.