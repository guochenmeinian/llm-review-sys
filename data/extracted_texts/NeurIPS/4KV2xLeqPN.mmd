# On the Variance, Admissibility, and Stability of Empirical Risk Minimization

Gil Kur

EECS

MIT

gilkur@mit.edu

&Eli Putterman

Mathematics Department

Tel Aviv University

putterman@mail.tau.ac.il

&Alexander Rakhlin

BCS & IDSS

MIT

rakhlin@mit.edu

###### Abstract

It is well known that Empirical Risk Minimization (ERM) may attain minimax suboptimal rates in terms of the mean squared error (Birge and Massart, 1993). In this paper, we prove that, under relatively mild assumptions, the suboptimality of ERM _must_ be due to its bias. Namely, the variance error term of ERM (in terms of the bias and variance decomposition) enjoys the minimax rate. In the fixed design setting, we provide an elementary proof of this result using the probabilistic method. Then, we extend our proof to the random design setting for various models. In addition, we provide a simple proof of Chatterjee's admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that in the fixed design setting, ERM cannot be ruled out as an optimal method, and then we extend this result to the random design setting. We also show that our estimates imply _stability_ of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes. Finally, we highlight the somewhat irregular nature of the loss landscape of ERM in the non-Donsker regime, by showing that functions can be close to ERM, in terms of \(L_{2}\) distance, while still being far from almost-minimizers of the empirical loss.

## 1 Introduction

Maximum Likelihood and the method of Least Squares are fundamental procedures in statistics. The study of the asymptotic consistency of Maximum Likelihood has been central to the field for almost a century (Wald, 1949). Along with consistency, failures of Maximum Likelihood have been thoroughly investigated for nearly as long (Neyman and Scott, 1948; Bahadur, 1958; Ferguson, 1982). In the setting of non-parametric estimation, the seminal work of (Birge and Massart, 1993) provided _sufficient_ conditions for minimax optimality (in a non-asymptotic sense) of Least Squares while also presenting an example of a model class where this basic procedure is sub-optimal. Three decades later, we still do not have necessary and sufficient conditions for minimax optimality of Least Squares when the model class is large. While the present paper does not resolve this question, it makes several steps towards understanding the behavior of Least Squares -- equivalently, Empirical Risk Minimization (ERM) with square loss -- in large models.

Beyond intellectual curiosity, the question of minimax optimality of Least Squares is driven by the desire to understand the current practice of fitting large or overparametrized models, such as neural networks, to data (cf. (Belkin et al., 2019; Bartlett et al., 2020)). At the present moment, there is little theoretical understanding of whether such unregularized data-fitting procedures are optimal, and the study of their statistical properties may lead to new methods with improved performance.

In addition to minimax optimality, many other important properties of Least Squares on large models are yet to be understood. For instance, little is known about its _stability_ with respect to perturbations of the data. It is also unclear whether _approximate_ minimizers of empirical loss enjoy similar statisticalproperties as the exact solution. Conversely, one may ask whether in the landscape of possible solutions, a small perturbation of the minimizer output by Least Squares itself is a near-optimizer of empirical loss.

The contribution of this paper is to provide novel insights into the aforementioned questions for convex classes of functions in a quite generic setting. In detail, we show the following:

1. We prove that in the fixed design setting, the variance term of ERM is upper bounded by the minimax rate of estimation; thus, if the ERM is minimax suboptimal, this must be due to the bias term in the bias-variance decomposition. Then, we extend this result to random design, and provide an upper bound for the variance error term under a uniform boundedness assumption on the class. This bound also implies that under classical assumptions in empirical process theory, the variance error term is minimax optimal.
2. We show that under an isoperimetry assumption on the noise, the expected conditional variance error term of ERM is upper bounded by the "lower isometry" remainder, a parameter that was introduced in Bartlett et al. (2005); Mendelson (2014). Next, under an additional isoperimetry assumption on the covariates, we prove that the variance of ERM is upper bounded by the lower isometry remainder on any robust learning architecture (namely, a class consisting of functions which are all \(O(1)\)-Lipschitz) which almost interpolates the observations (cf. Bubeck and Sellke (2023)).
3. It is known that ERM is always admissible in the fixed design setting (Chatterjee, 2014; Chen et al., 2017); that is, for any convex function class, there is no estimator having a lower error than ERM (up to a multiplicative absolute constant) on _every_ regression function. We provide a short proof of this result for fixed design via a fixed-point theorem. Using the same method, we also prove a somewhat weaker result in the random design case, generalizing the main result of Chatterjee (2014).
4. We show that ERM is stable, in the sense that all almost-minimizers (up to the minimax rate) of the squared loss are close in the space of functions. This result is a non-asymptotic analogue of the asymptotic analysis in Caponnetto and Rakhlin (2006), and extends its scope to non-Donsker classes.
5. While any almost-minimizer of the squared loss is close to the minimizer with respect to the underlying population distribution, the converse is incorrect. We prove that for any non-Donsker class of functions, there exists a target regression function such that, with high probability, there exists a function with high empirical error near the ERM solution; this means that the landscape of near-solutions is, in some sense, irregular.

ConclusionsOur results show that the ERM enjoys an optimal variance error term in two distinct regimes: the classical regime (van de Geer, 2000), where the function class is fixed and the number of samples is increasing, and the "benign overfitting" setting (Belkin et al., 2019; Bartlett et al., 2020), in which the "capacity" of the class is large compared to the number of samples. In both these settings, our work implies that the minimax optimality of ERM is only determined by its bias error term (or its implicit bias). For models with "few" parameters, _computationally efficient_ bias correction methods do exist and are commonly used in practice (cf. (Efron and Tibshirani, 1994)), however, these methods fail over large function classes, in which the bias causes the statistical sub-optimality. Our work reveals the importance of inventing computationally efficient debiasing methods for rich function classes, including non-parametric models and high-dimensional models. The main message of our work is that such methods, if discovered, may significantly improve the statistical performance of ERM over large models in practice.

### Prior Work

Stability of ERMThe stability of learning procedures, which was an active area of research in the early 2000's, has recently seen a resurgence of interest because of its connections to differential privacy and to robustness of learning methods with respect to adversarial perturbations. In the interest of space, we only compare present results to those of Caponnetto and Rakhlin (2006). In the latter paper, the authors showed that the \(L_{1}\)-diameter of the set of almost-minimizers of empirical error (with respect to any loss function) asymptotically shrinks to zero as long as the perturbation is \(o(n^{-1/2})\) and the function class is Donsker. The analysis there relies on passing from the empirical process to the associated Gaussian process in the limit, and studying uniqueness of its maximum using anti-concentration properties. While the result there holds without assuming that the class is convex, it is limited by (a) its asymptotic nature and (b) the assumption that the class is not too complex. In contrast, the present paper uses more refined non-asymptotic concentration results, at the expense of additional assumptions such as convexity and minimax optimal lower and upper lower isometry remainders. Crucially, the present result, unlike that of Caponnetto and Rakhlin (2006), holds for non-Donsker classes -- those for which the empirical process does not converge to the Gaussian process.

Shape-constrained regressionThe term "shape-constrained regression" refers to function classes consisting of functions with a certain "shape" property, such as convexity or monotonicity (Samworth and Sen, 2018). In these problems, a common theme is that the statistical behavior of the class exhibits a phase transition when the dimension \(d\) of the domain reaches a certain value. For instance, in convex (Lipschitz) regression, the transition happens at \(d=5\): when \(d<5\), the ERM procedure is minimax optimal (Seijo and Sen, 2011; Han and Wellner, 2016; Kim and Samworth, 2016; Seijo and Sen, 2011; Guntuboyina, 2012), but in higher dimensions, it is known to be suboptimal (Kur et al., 2020, 2020). In other shape-constrained models, however, the ERM is minimax optimal even in high dimensions, such as _isotonic regression_ and _log-concave density estimation_(Han et al., 2019; Kur et al., 2019; Carpenter et al., 2018). Our results show that the sub-optimality of ERM in shape-constrained regression can only be due to the high bias of ERM. These results also align with the empirical observation that for the problem of estimation of convex sets, the ERM has a bias towards "smooth" convex sets (Soh and Chandrasekaran, 2019; Ghosh et al., 2021).

High-dimensional statisticsIn classical statistics, the Maximum Likelihood (MLE) typically has a low bias compared to its variance, and the standard approach is to introduce bias into the procedure in order to reduce the variance, overall achieving a better trade-off (see (Sur and Candes, 2019, SS1) and references therein). In contrast, in high-dimensional models, the MLE may suffer from high bias even in tasks such as logistic regression and sparse linear regression (cf. Candes and Sur (2020); Javanmard and Montanari (2018)). Our results align with this line of work, showing that the problem of high bias may also arise in the task of regression over rich function classes.

## 2 Main Results

In SS2.1, we present the setting of our model and the all required preliminaries, and in the remaining sub-sections, we present our results. In details, in SS2.2-2.3, we present our results on the variance of ERM in fixed and random designs respectively, and in SS3 we provide sketches of some of our proofs. For lack of space, we presents our admissibility and our landscape results on ERM in the supplementary material (SS4.1-4.2).

### Preliminaries

Let \(\) be some fixed domain, \(\) be a class of functions from \(\) to \(\), and \(f^{*}\) an unknown target regression function. We are given \(n 2\) data points \(X_{1},X_{n}\) and \(n\) noisy observations

\[Y_{i}=f^{*}(X_{i})+_{i}, i=1,,n\] (1)

which we denote by \(:=\{(X_{i},Y_{i})\}_{i=1}^{n}\), \(:=(_{1},,_{n})\) is the random noise vector.

In the _fixed design_ setting, the observations \(X_{1}=x_{1},,X_{n}=x_{n}\) are arbitrary and fixed, and we denote the uniform measure on this set of points by \(^{(n)}\). In the _random design_ setting, the data points \(:=(X_{1},,X_{n})\) are drawn i.i.d. from a probability distribution over \(\), denoted by \(\), and the noise vector \(\) is drawn independently of \(\). Note that this model is general enough to cover the high-dimensional setting, as both the function class \(\) and the distributions of \(\) and \(X\) are allowed to depend on the number of samples \(n\).

An estimator for the regression task is defined as a measurable function \(_{n}:\{\}\), that for any realization of the input \(\), outputs some real-valued measurable function on \(\). The risk of \(_{n}\) is defined as

\[(_{n},,):=_{f^{*}} _{}(_{n}-f^{*})^{2}d,\] (2)

where \(=\) in the random design case, and \(=^{(n)}\) in the fixed design case. Note that in fixed design, the expectation \(_{}\) is taken over the noise \(\), while in random design the expectation \(_{}\) is taken both over the random data points \(\) and noise \(\). The minimax rate is defined via

\[(n,,):=_{_{n}}(_{n}, ,).\] (3)

In the fixed design setting, we also denote the minimax rate by \((,^{(n)})\), as the dependence in \(n\) is already present in \(^{(n)}\).

The most natural estimation procedure is the Least Squares (LS) or Empirical Risk Minimization (ERM) with squared loss, defined as

\[_{n}*{argmin}_{f}_{i=1}^{n}(f(X_{ i})-Y_{i})^{2}.\] (4)

When studying fixed design, we will abuse notation and treat \(_{n}\) as a vector in \(^{n}\). We emphasize that many of our results hold for many other estimators, including various _regularized_ ERM procedures (see the relevant remarks below).

In both the fixed and random design settings, we shall assume the following:

**Assumption 1**.: \(\) _is a closed convex subset of \(L_{2}()\), where \(\{^{(n)},\}\)._

The convexity of \(\) means that any \(f,g\) and \(\): \( f+(1-)g\); closedness means that for any sequence \(\{f_{n}\}_{n=1}^{}\) converging to \(f\) with respect to the norm of \(L_{2}()\), the limit \(f\) lies in \(\). The closedness ensures that \(_{n}\) is well-defined.

Assumption 1 is standard in studying the statistical performance of the ERM (cf. Lee et al. (1996); Bartlett et al. (2005); Mendelson (2014)). In particular, under this assumption, the values of \(_{n}\) at the observation points \(X_{1},,X_{n}\) is uniquely determined for any \(\). Note that in general, the values of a function \(f\) at the points \(X_{1},,X_{n}\) does not uniquely identify \(f\) among all functions in the class.

In addition to \(_{n}\), we analyze properties of the set of \(\)-approximate minimizers of empirical loss, defined for \(>0\) via

\[_{}:=\{f:_{i=1}^{n}(Y_{i}- f(X_{i}))^{2}_{i=1}^{n}(Y_{i}-_{n}(X_{i}))^{2}+ \}.\] (5)

Note that \(_{}\) is a random set, in both fixed and random designs.

It is well-known that the squared error of any estimator, in particular that of LS, decomposes into variance and bias components:

\[_{}(_{n}-f^{*})^{2}d= {_{}(_{n}-_{} {f}_{n})^{2}d}_{V(_{n})}+_{ }_{n}-f^{*})^{2}d}_{B^{2}(_{n})},\] (6)

where \(=^{(n)}\) in the fixed design setting and \(=\) in the random design setting. Also, for simplicity of the presentation of our results, we denote the maximal variance error term of \(_{n}\) by \((_{n},,)\), i.e.

\[(_{n},,):=_{f^{*}}V( _{n})=_{}(_{n}-_{} _{n})^{2}d\]

In the random design setting, we also have the law of total variance:

\[V(_{n})=_{}_{} [(_{n}-_{}[_{n}| ])^{2}\,d]}_{V(_{n} |)}+_{}[(_{ }[_{n}|]-_{, }_{n})^{2}d]}_{V((_{n} |))}.\] (7)

We refer to the two terms as the expected conditional variance and the variance of the conditional expectation, respectively. We conclude this introductory section with a bit of notation and a definition.

NotationWe use the notation of \(,,\) to denote equality/inequality up to an absolute constant. We use \(\|\|=\|\|_{L_{2}()}\) to denote the \(L_{2}()\) norm, and \(\|\|_{n}\) to denote the \(L_{2}(^{(n)})\) norm (that is equal to the Euclidean norm scaled by \(1/\)). Finally, given a function \(f:S T\) between metric spaces, we define its Lipschitz constant as \(\|f\|_{Lip}=_{a,b T,a b}(f(a),f(b))}{d_{S}(a,b)}\), and we say "\(f\) is \(L\)-Lipschitz" when its Lipschitz constant is at most \(L\). Finally, we denote by \(_{}()\) the \(L_{2}()\) diameter of a set of functions \(\).

**Definition 1**.: _Let \( 0\), \(\{\}\) and \(d(,)\) a pseudo-metric on \(\). We call a set \(S\) an \(\)-net of \(\) with respect to \(d\) if for any \(f\) there exists \(g S\) with \(d(f,g)\). We denote by \((,,d)\) the \(\)-covering number of \(\) with respect to \(d\), that is, the minimal positive integer \(N\) such that \(\) admits an \(\)-net of cardinality \(N\)._

### Variance of ERM in fixed design setting

In this part, we consider some fixed \((,^{(n)})\) and assume that the noise is standard normal.

**Assumption 2**.: _The noise vector \(\) is distributed as an isotropic Gaussian, i.e. \( N(0,I_{n n})\)._

Our first result provides an exact characterization of the variance (up to a multiplicative absolute constant) under Assumptions 1-2. In order to state it, for a fixed \(f^{*}\), we define the following set:

\[_{}:=\{f:\|f-_{} _{n}\|_{n}^{2} 4 V(_{n})\}.\] (8)

In words, when the underlying function \(f^{*}\) is fixed, we consider the ERM as a random vector (depending on the noise), whose expectation we denote by \(_{}_{n}\). \(_{*}\) is then just a neighborhood around the expected ERM with a radius of order the square root of the variance error term of \(_{n}\), when the underlying function is \(f^{*}\).

We can now state our first result, which uses the notion of the set \(_{}\) of \(\)-approximate minimizers from (5).

**Theorem 1**.: _Under Assumptions 1-2, the following holds:_

\[V_{}(_{n})(_{*},^{(n)}),\]

_and in particular \((_{n},,^{(n)})( ,^{(n)})\). Furthermore, for \(:=(f^{*},n)(_{*},^{(n)})\), the event_

\[_{f_{}}(f-_{}_{n })^{2}d^{(n)}(_{*},^{(n)}).\] (9)

_holds with probability at least \(\{1-2(-cn(_{*},^{(n)})),0.9\}\), where \(c(0,1)\) is an absolute constant._

Theorem 1 establishes our first claim: the variance of ERM is bounded above (up to a multiplicative absolute constant) by the minimax rate of estimation on \(_{*}\). Since \(_{*}\) is contained in \(\), its minimax rate is at most that of \(\); in particular, the variance of ERM is bounded by \((,^{(n)})\), and hence, any sub-optimality of ERM must arise from its bias. The theorem also incorporates a stability result: not only is the ERM close to its expected value \(_{}_{n}\) with high probability, but any approximate minimizer (up to an excess error of \(^{2}\)) is close to \(_{}_{n}\) as well.

Our next result complements Theorem 1 above, providing a lower bound on \((_{n},,^{(n)})\):

**Theorem 2**.: _Under Assumptions 1-2, the following holds:_

\[(_{n},,^{(n)})( ,^{(n)})^{2}.\]

Note that there is a multiplicative gap of order \((,^{(n)})\) between the bounds of Theorems 1 and 2. We leave it as an open problem whether the bound of Theorem 2 can be improved under these general assumptions.

### Variance of ERM in random design setting

We now turn our attention to the random design setting. Here, we establish similar results to the previous sub-section, albeit under additional assumptions, and with significantly more effort. Unlike the fixed design case, we cannot provide an exact characterization of the variance of ERM. We shall use two different approaches to estimate the variance error term. In the first approach, we use classical tools of empirical process theory together with assumptions that are commonly used in M-estimation (van de Geer, 2000). The second approach, which is inspired by our fixed-design approach, relies heavily on isoperimetry and concentration of measure (cf. Ledoux (2001)).

Throughout this part, we assume for simplicity of presentation that the \(L_{2}()\)-diameter of the function class is independent of \(n\).

**Assumption 3**.: _There exist absolute constants \(C,c>0\) such that \(c_{}() C\)._

The classical work of Yang and Barron (1999) provides a characterization of the minimax rate \((n,,)\) under appropriate assumptions (such as normal noise, and uniform boundedness of \(\), and richness of \(\)). They proved that the minimax rate is the square of the solution of the following (asymptotic) equation

\[(,,) n^{2},\] (10)

where \((,,)\) is the \(\)-covering number of \(\) in terms of \(L_{2}()\) metric (see Def. 1 above). We denote this point by \(_{*}=_{*}(n)\), and even under less restrictive assumptions, \(_{*}^{2}\) also lower bounds the minimax rate, up to a multiplicative factor of \(O( n)\). Also, we remark that it is well known that ERM may not achieve this optimal rate (Birge and Massart, 1993) for large (so-called non-Donsker) function classes.

We also introduce the following additional notations and definitions: First, \(_{n}\) denotes the (random) uniform measure over \(=(X_{1},,X_{n})\). Next, following Bartlett et al. (2005), we define the lower and upper isometry remainders of \((,)\) for a given \(n\). These remainders measure the discrepancy between \(L_{2}()\) and a "typical" \(L_{2}(_{n})\), here "typical" means for most of realizations of \(\). These remainders first emerged in the field of metric embeddings, specifically in the definition of quasi-isometries (cf. Ostrovskii (2013)).

In order to introduce these isometry remainders, we first define for each realization of the input \(\), the constants \(_{L}()\) and \(_{U}()\) as the minimal numbers \(A_{X},B_{X} 0\), respectively, such that the following holds:

\[ f,g:\;4^{-1}(f-g)^{2}d-A_{X}(f-g)^ {2}d_{n} 4(f-g)^{2}d+B_{X}.\]

Note that as \(A_{X}\) and \(B_{X}\) increase, the geometry of \(L_{2}(_{n})\) and \(L_{2}()\) over \(\) becomes less similar. For example, in the extreme case of \(A_{X}=B_{X}=0\), it implies the \(L_{2}()\) and \(L_{2}(_{n})\) induce the same topology over \(\). In words, the lower isometry is the minimal threshold that satisfies the following: all \(f,g\) that are \((_{L}())\) far from each other in \(L_{2}()\), must be _at least_\((\|f-g\|)\) far in \(L_{2}(_{n})\). The upper isometry remainder implies the converse. To provide further intuition on these remainders, for instance, observe that \(_{L}()\) upper bounds on diameter in \(L_{2}()\) of possible solutions of ERM; namely, one has

\[_{f^{*},^{n}}_{ }(\{f:(f(X_{1}),,f(X_{n}))=(_{n}(X_{1} ),,_{n}(X_{n}))\})^{2} 4_{L}().\]

Finally, the isometry remainders \(_{L}(n)\), \(_{U}(n)\) are defined as the "typical" values of \(_{L}()\), \(_{U}()\):

**Definition 2**.: _The lower and upper isometry remainders \(_{L}(n),_{U}(n)\) are defined as the minimal constants \(A_{n},B_{n} 0\) (respectively) such that_

\[_{}(_{L}() A_{n}) 1-n^{-1}_{ }(_{U}() B_{n}) 1-n^{-1}.\]

In the classical regime (van de Geer, 2000), it is considered to be a standard assumption that \(\{_{L}(n),_{U}(n)\}_{*}^{2}\). However, in the high dimensional setting, it may happen that the lower isometry remainder is significantly smaller than the upper isometry remainder, e.g., \(_{L}(n)_{*}^{2}\) and\(_{U}(n)_{*}^{2}\) (cf. Liang et al. (2020); Mendelson (2014)). We discuss these remainders further in Remark 10 below.

Finally, we remind the reader that \(_{n}\) is uniquely defined on the data points \(\) when \(\) is a convex closed function class, but it _may not_ be unique over the entire \(\) (as multiple functions in \(\) may take the same values at \(X_{1},,X_{n}\)). In SS2.3.1, the results hold for any possible solution of \(_{n}\) over \(\), whereas in SS2.3.2, we (implicitly) assume that \(_{n}\) is equipped with a selection rule such that it is also unique over the entire \(\) (e.g., choosing the minimal norm solution (Hastie et al., 2022; Bartlett et al., 2020)); i.e, \(_{n}:\).

#### 2.3.1 The empirical processes approach

Here, we assume that the function class and the noise are uniformly bounded.

**Assumption 4**.: _There exist universal constants \(_{1},_{2}>0\) such that \(\) is uniformly upper-bounded by \(_{1}\), i.e. \(_{f}\|f\|_{}_{1}\); and the components of \(=(_{1},,_{n})\) are i.i.d. zero mean with variance one and are almost surely bounded by \(_{2}\)._

_Remark 1_.: The uniform boundedness assumption on the noise is taken to simplify the proof (which uses of Talagrand's inequality). This can be relaxed to the assumption that the noise is i.i.d. sub-Gaussian, at the a price of a multiplicative factor of \(O( n)\) in the error term in Theorem 3 below.

**Definition 3**.: _Set \(_{U}:=\{_{*},\}\), where \(\) is the solution of_

\[_{U}(n)(,,) n ^{4}.\] (11)

Note that when \(_{U}(n)_{*}^{2}\), \(_{U}_{*}\), while if \(_{U}(n,)_{*}^{2}\) then \(_{U}_{*}\). The following is our main result in this approach to the random design setting:

**Theorem 3**.: _Set \(_{V}^{2}:=\{_{U}^{2},_{L}(n)\}\), then under Assumptions 1,3,4 the following holds with probability of at least \(1-n^{-1}\):_

\[_{f_{_{n}}}(f-_{} _{n})^{2}d_{V}^{2},\]

_where \(_{n}=O(_{V}^{2})\); and in particular \((_{n},,)_{V}^{2}\)._

Theorem 3 is a generalization of Theorem 1 to the random design case, and its proof uses the strong convexity of the loss and Talagrand's inequality. In SS5.1 below, we discuss this bound in the context of "distribution unaware" estimators. We remark that this Theorem extends the scope of Caponnetto and Rakhlin (2006) to non-Donsker classes.

An immediate and useful corollary of this result is that if we have sufficient control of the upper and lower isometry remainders, the variance will be minimax optimal:

**Corollary 1**.: _Under Assumptions 1,3,4 and \(\{_{L}(n),_{U}(n)\}_{*}^{2}\), the following holds:_

\[(_{n},,)_{*}^{2}.\]

In the classical regime, the assumption that \(\{_{L}(n),_{U}(n)\}_{*}^{2}\) is considered to be standard in the empirical process and shape constraints literature (cf. van de Geer (2000) and references therein); it holds for many classical models (see Remark 11 below).

_Remark 2_.: Note that Corollary 1 may also be derived directly from Theorem 1 if the noise is assumed to be standard Gaussian. Yet, this corollary holds for any isotropic sub-Gaussian noise - which is significantly more general.

#### 2.3.2 The isoperimetry approach

In order to motivate this part, we point out that just requiring that \(_{L}(n)_{*}^{2}\) is considered to be a mild assumption (see Remark 10 below). However, the upper bound of Theorem 3 depends on the upper isometry remainder as well; we would like to find some conditions under which this dependency can be removed. Moreover, note that the isometry remainders are connected to the geometry of \((,)\) and not directly to the stability properties of the _estimator_. Using a different approach, based on isoperimetry, we will upper-bound the variance of ERM based on some "interpretable" stability parameters of the estimator itself. These stability parameters will be data-dependent relatives of the lower isometry remainder. Differently from the previous part, we do not assume that the function class \(\) is uniformly bounded by a constant independent of the sample size \(n\).

First, we introduce the definition of Lipschitz Concentration Property (LCP):

**Definition 4**.: _Let \(=(Z_{1},,Z_{m})\) be a random vector taking values in \(^{ m}\). \(\) satisfies the LCP with constant \(c_{L}>0\), with respect to a metric \(d:(^{ m},^{ m})^{+}\), if for all \(F:^{m}\) is \(1\)-Lipschitz, the following holds:_

\[(|F()-F()| t) 2(-c_{L}t^{2}).\] (12)

The LCP property is also known as the isoperimetry condition (cf. (Bubeck and Sellke, 2023, SS1.3)), and it is stronger than being sub-Gaussian (Boucheron et al., 2013), and yet it is significantly less restrictive than requiring normal noise (in which case \(c_{L}=1/2\)(Ledoux, 2001)); for further details see Remark 9 below. Now, we state our first assumption:

**Assumption 5**.: \(\) _is an isotropic random vector satisfying (12) with constant \(c_{L}=(1)\), with respect to the Euclidean norm in \(^{n}\)._

Recall that \(_{*}\) is defined as the stationary point of \(n^{2}(,,)\), and that the conditional variance of \(_{n}\), which is a function of the realization \(\) of the input, is defined as

\[V(_{n}|):=_{}[\|_{n }-_{}[_{n}|]\|^{2}];\]

that is, we fix the data points \(\) and take expectation over the noise.

The formulation of the following definition involves a yet-to-be-defined (large) absolute constant \(M>0\), which will be specified in the proof of Theorem 4 (see SS3.2 below).

**Definition 5**.: _For each realization \(\) and \(f^{*}\), let \(_{S}(,f^{*})\) be defined as the minimal constant \((n)\) such that_

\[_{}^{n}: ^{} B_{n}(,M_{*}):\|_{n}(,^{})-_{n}(, )\|^{2}(n)}(-c_{2}n_{*}^{2}).\] (13)

_where \(B_{n}(,r)=\{^{}^{n}:\| -^{}\|_{n} r\}\), and \(c_{2}>0\) is an absolute constant._

We also set \(_{S}():=_{f^{*}}_{S}(,f^{*})\). \(_{S}()\) measures the optimal radius of stability (or "robustness") of \(_{n}\) to perturbations of the noise _when the underlying function and data points \(\) are fixed_. This is a weaker notion than the lower isometry remainder; in fact, one can verify that \(_{S}()\{_{L}(),_{*}^{2}\}\) for every realization \(\) (see Lemma 12 for completeness). Now, we are ready to present our first theorem:

**Theorem 4**.: _Under Assumptions 1,3,5, the following holds for every realization \(\) of the data:_

\[V_{}(_{n}|)\{_{S}(, f^{*}),_{*}^{2}\},\]

_and in particular \(_{f^{*}}_{}V_{}(_{n }|)\{_{L}(n),_{*}^{2}\}\)._

Note that if \(_{L}(n)_{*}^{2}\) - a very mild assumption - then we obtain that the expected conditional variance is minimax optimal. However, we believe that it is impossible to bound the total variance via the lower isometry remainder alone. Intuitively, \(_{n}\) only observes a given realization \(\), and in general, the geometry of \(\) may "look different" under different realizations if \(_{L}(n)\) is large (see the discussion in SS5.1 for further details).

In our next result, we identify a model under which we can bound the _total_ variance of \(_{n}\) by the lower isometry remainder. To state the next assumption, we fix a metric \(d:^{+}\) on \(\), and denote by \(d_{n}\) the metric on \(^{n}\) given by \(d_{n}(,^{})^{2}= d(X_{i},X_{i}^{})^{2}\).

**Assumption 6**.: \(^{ n}\) _satisfies (12) with respect to the metric \(d_{n}(,)\), and with constant \(c_{X}\) that only depends on \(\)_Note that it is insufficient to assume that \(X\) satisfies an LCP, since this does not imply that \(\) satisfies an LCP with a constant independent of \(n\) (w.r.t. to \(d_{n}(,)\)). However, if \(X\) satisfies a concentration inequality which tensorizes "nicely," such as a log-Sobolev or \(W_{2}\)-transportation cost inequality (cf. (Ledoux, 2001, SS5.2, SS6.2)), then \(^{ n}\) does satisfy this LCP property.

Next, we assume that with high probability, \(_{n}\) is close to interpolating the observations:

**Assumption 7**.: _There exist absolute constants \(c_{I},C_{I}>0\), such that the following holds:_

\[_{}(_{i=1}^{n}(_{n}(X_{i})-Y_{i})^{2} C _{I}_{*}^{2} n) 1-(-c_{I}n_{*}^{2}).\]

This assumption is quite common in the study of "rich" high dimensional models (i.e. when the function class \(\) depends on \(n\); see, e.g., Belkin et al. (2019); Liang and Rakhlin (2020)) which are prominent in the recent benign overfitting literature.

Finally, we introduce another stability notion. Recall the random set \(_{}\) of almost-minimizers of the empirical loss, as defined in (5) above; note that, in the random design setting, \(_{}\) depends on both \(\) and \(\). The random variable \(_{}(_{})\) can be thought of as measuring the stability of the ERM with respect to imprecision in the minimization algorithm (cf. (Caponnetto and Rakhlin, 2006)). The formulation of the following definition involves another yet-to-be-defined (large) absolute constant \(M^{}>0\), which will be specified in the proof of Theorem 5 (see SS6.6 below), as well as the constant \(c_{I}\) from Assumption 7.

**Definition 6**.: \(_{}(n,,f^{*})\) _is defined as the smallest \((n) 0\) such that_

\[_{}(_{}(_{M^{} _{*}^{2}})) 2(-c_{I}n_{*}^{2}),\] (14)

_where \(c_{I} 0\) is the same absolute constant defined in Assumption 7._

In order to understand the relation between this and the previous stability notions, note that under Assumption 7 and the event \(\) of Definition 6, we have that on an event of nonnegligible probability, \(_{S}(,f^{*})_{}(n,,f^{*})\); in addition, \(_{}(n,,f^{*})\{_{L}(n), _{*}^{2}\}\) (see Lemma 13 below). Under these additional two assumptions and the last definition, we state our bound for the total variance of \(_{n}\):

**Theorem 5**.: _Under Assumptions 1,3,5-7, the following holds:_

\[V_{}(_{n}) c_{X}^{-1}_{f^{*} }\|f^{*}\|_{Lip}\{_{*}^{2},_{}(n, ,f^{*})\},\]

_and in particular one has \((_{n},,) c_{X}^{-1} _{f^{*}}\|f^{*}\|_{Lip}\{_{*}^{2},_{L}(n)\}\)._

Note that when \(\) is a robust learning architecture (i.e. \(\{:\|f\|_{Lip}=O(1)\}\)), our bound is optimal. Interestingly, the assumptions of Theorem 5 coincide with those of the model considered in the recent paper of Bubeck and Sellke (2023). Also note that the last theorem connects the total variance of \(_{n}\) to a "probabilistic" threshold for the \(L_{2}()\)-diameter of the _data-dependent_ set of \((_{*}^{2})-\)approximating solutions of \(_{n}\) - two parameters which at first sight are unrelated.

_Remark 3_.: One may suspect that the assumptions of almost interpolation and robustness are incompatible, which would render our theorem vacuous. However, perhaps counter-intuitively, in the high-dimensional setting these assumptions can coexist. For example, interpolation with \(O(1)\)-Lipschitz functions may be possible when the "intrinsic" dimension of \(\) is \(((n))\) (depending on the richness of \(\)), though it is generally impossible when the dimension is \(o((n))\) (this follows from the behaviour of the entropy numbers of the class of Lipschitz functions; cf. Dudley (1999)).

_Remark 4_.: Using Assumptions 1,3,5-6, one may prove the same bound as in Theorem 3, i.e. that \((_{n},,)_{V}^{2}\), without requiring the noise or the function class to be uniformly bounded. The idea is to obtain the crucial concentration bounds in the proof of Theorem 3 by using the LCP properties of \(\) and \(X\) along with the robustness of \(\), rather than via Talagrand's inequality.

Proof sketches

In this section, we sketch the proofs of less-technical results to give the reader a flavor of our methods. The full proofs are given in the next section. For the proofs we introduce some additional notations. For \(m\), we set \([m]:=\{1,,m\}\). The inner products in \(L_{2}(),L_{2}(^{(n)})\) are denoted by \(,,,_{n}\), respectively.

### Sketch of proof of Theorem 1

Here, we sketch a simple proof of a weaker version of our result, namely \(V_{}(_{n})(,^{(n)})\), under the stronger assumption that

\[(,^{(n)})_{*}^{2},\] (15)

where \(_{*}\) solves \((,,^{(n)}) n^{2}\). (This holds under reasonable assumptions on \(\), but can be dispensed with; see Lemma 1 for the exact characterization.) In SS6.1.1, we fill in the details of this sketch, and in SS6.1.2, we give the full proof of Theorem 1.

The proof uses the probabilistic method (Alon and Spencer, 2016). Let \(f_{1},,f_{N}\) be centers of a minimal \(_{*}\)-cover of \(\) with respect to \(L_{2}(^{(n)})\), per Definition 1. First, since for any \(i[N]\), the map \(\|_{n}()-f_{i}\|_{n}\) is \(1\)-Lipschitz, (12) and a union bound ensure that with probability at least \(1-\), for all \(i[N]\),

\[_{}\|_{n}-f_{i}\|_{n}-\|_{n}- f_{i}\|_{n}}.\]

On the other hand, by the pigeonhole principle, there exists at least one \(i^{*}[N]\) such that with probability at least \(1/N\), \(\|_{n}-f_{i^{*}}\|_{n}_{*}\). Hence, there exists at least one realization of \(^{n}\) for which both bounds hold, and thus, _deterministically_,

\[_{}\|_{n}-f_{i^{*}}\|_{n} _{*}+}_{*}\]

where we used the balancing equation (10). Another application of (12) and integration of tails yields

\[V(_{n})=_{}\|_{n}-_{ }_{n}\|_{n}^{2}_{}\|_{n}-f_{i^{*}}\|_{n}^{2}_{*}^{2},\]

implying that the variance of ERM is minimax optimal.

### Sketch of proof of Theorem 4

As is well-known, the Lipschitz concentration condition (12) is equivalent to an isoperimetric phenomenon: for any set \(A^{n}\) with \(_{}(A) 1/2\), its \(t\)-neighborhood \(A_{t}=\{^{n}:_{x A}\|x-\|_ {n} t\}\) satisfies

\[_{}(A_{t}) 1-2(-nt^{2}/2).\] (16)

One sees quickly that this implies that if \(A\) has measure at least \(2(-nt^{2}/2)\), then \(A_{2t}\) has measure \(1-2(-nt^{2}/2)\).

Let \(\) be the event of Definition 5. As in SS3.1 above, one obtains via the pigeonhole principle and the definition of \(_{*}\) that there exists some \(f_{c}\) such that

\[_{}(_{n} B(f_{c},_{*}) \}}_{A}|)}()}{(_{*},,)}(-C_{3}n_{*} ^{2}).\]

By isoperimetry, \(_{}(A_{2t}) 1-2(-nt^{2}/2)\), where \(t=M_{*}/2\) and \(M\) is chosen such that \((M/2)^{2} 2C_{3}\); this fixes the value of the absolute constant \(M\) used in (13).

Applying (13) yields that if \( A\) and \(\|^{}-\|_{n} M_{*}=2t\), \(\|_{n}()-_{n}(^{}) \|_{S}(,f^{*})\) and so \(\|_{n}(^{})-f_{c}\|_{*}+_{S} (,f^{*})\). This implies

\[_{}(\{_{n} B(f_{c},_{*}+_{S}( ,f^{*}))\}|)_{}(A_{2t}|)  1-2(-nt^{2}/2),\]

which implies via conditional expectation that \(V(_{n}|)\{_{S}(),_{*}^{ 2}\}\), as desired (where we used that \(_{*}^{2}(n)/n\) (see Lemma 3 below), and therefore \((-nt^{2})=O(_{*}^{2})\)).

Acknowledgements:This work was supported by the Simons Foundation through Award 814639 for the Collaboration on the Theoretical Foundations of Deep Learning, the ERC under the European Union's Horizon 2020 research and innovation programme (grant agreement No 770127), and the NSF (awards DMS-2031883, DMS-1953181). Part of this work was carried out while the first two authors authors were in residence at the Institute for Computational and Experimental Research in Mathematics in Providence, RI, during the Harmonic Analysis and Convexity program; this residency was supported by the NSF (grant DMS-1929284). Finally, the first two authors also wish to acknowledge Prof. Shiri Artstein-Avidan for introducing them to each other.