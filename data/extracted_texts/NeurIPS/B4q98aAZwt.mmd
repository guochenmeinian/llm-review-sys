# Genetic-guided GFlowNets

for Sample Efficient Molecular Optimization

 Hyeonah Kim\({}^{1}\)1  Minsu Kim\({}^{1}\)  Sanghyeok Choi\({}^{1}\)  Jinkyoo Park\({}^{1,2}\)

\({}^{1}\)Korea Advanced Institute of Science and Technology (KAIST), \({}^{2}\)OMELET

###### Abstract

The challenge of discovering new molecules with desired properties is crucial in domains like drug discovery and material design. Recent advances in deep learning-based generative methods have shown promise but face the issue of sample efficiency due to the computational expense of evaluating the reward function. This paper proposes a novel algorithm for sample-efficient molecular optimization by distilling a powerful genetic algorithm into deep generative policy using GFlowNets training, the off-policy method for amortized inference. This approach enables the deep generative policy to learn from domain knowledge, which has been explicitly integrated into the genetic algorithm. Our method achieves state-of-the-art performance in the official molecular optimization benchmark, significantly outperforming previous methods. It also demonstrates effectiveness in designing inhibitors against SARS-CoV-2 with substantially fewer reward calls.

## 1 Introduction

Discovering new molecules is one of the fundamental tasks in the chemical domain, with applications in drug discovery  and material design . Particularly, _de novo_ molecular design focuses on generating novel molecules with desired properties from scratch. In this context, deep learning-based generative methods have emerged, showing promising results (e.g., ). However, these methods still face a key challenge: the reward function is computationally expensive (e.g., assessing binding affinity through docking simulations), while the molecule space is combinatorially large.

Sample-efficient molecular optimization is thus crucial for discovering high-reward molecular structures with limited reward calls, especially for real-world applicability. The recently proposed benchmark, Practical Molecular Optimization (PMO) , has extensively assessed the sample efficiency of various algorithms, including reinforcement learning , active learning , variational autoencoders , generative flow networks (GFlowNets) , and classical optimization methods like Bayesian optimization  and genetic algorithms . Interestingly, the PMO benchmark has revealed a shift in algorithm rankings, with classical algorithms often outperforming recently proposed methods such as GFlowNets when the sample efficiency is considered.

Recent investigations , including those highlighted by the PMO benchmark , indicate that the classical frameworks, especially genetic algorithms (GA), still exhibit competitive performances compared to recently proposed deep learning methods. These studies underscore that GAs effectively navigate the chemical space using domain-specific genetic operators. In contrast, deep learning methods usually do not leverage domain-specific knowledge, relying instead on deep networks to autonomously learn these insights. It can lead to inefficient training processes due to the lack of expert guidance . To address this limitation, Genetic Expert Guided Learning (GEGL)  has been introduced, which enhances deep learning by distilling GA-generated samples into a deep generative policy using maximum likelihood estimation. This approach enables the deep generative policy toimplicitly utilize domain-specific knowledge from the GA as a form of guidance. However, despite its successes, GEGL may face challenges in generalizing to unexplored regions and in learning a peaky distribution from samples, as it maximizes likelihood equally across all high-reward samples without adequately configuring the reward landscape.

To address this, we propose a novel molecular optimization algorithm that integrates domain-specialized genetic algorithms into GFlowNets, an off-policy training method that trains policy to sample proportional to their rewards. As illustrated in Fig. 1, we first generate diverse candidates using the current policy and refine the candidates into higher-reward samples using GAs. Subsequently, we fine-tune the policy with a GFlowNet using the collected samples. Unlike the MLE training, which can be stuck in local modes when the reward landscape is peaky, GFlowNet trains the policy to sample molecules _proportional to rewards_. To enhance sample efficiency, we perform unsupervised pretraining on chemical datasets and regularize the GFlowNets policy with KL-divergence to align generative probability with the dataset distribution, focusing on the compact valid space.

Our contributions can be interpreted from both perspectives of GFlowNets and GAs:

**GA increases the exploitation power of GFlowNets.** The proposed algorithm incorporates an effective off-policy exploration into GFlowNets based on domain-specialized GA. This approach aligns with recent studies that utilize off-policy explorations to guide toward the high-reward regions . Our key contribution lies in explicitly leveraging domain knowledge about chemical structures and effectively distilling it into GFlowNets, which enable exceptional performance in real-world tasks beyond small-scale molecular generations. This contribution is crucial for the field of GFlowNets, which have struggled with sample-efficient molecular optimization tasks, even when using proxy reward models for active learning [10; 8]; see Section 4.2.

**GFlowNets increases population diversity of GA.** The proposed algorithm generates diversified samples using GFlowNets, enabling GA to effectively improve samples; our method can be regarded as a new genetic algorithm with deep generative policy-based population resetting. Our GFlowNet policy, parameterized over the whole space, periodically resets the population by diversely sampling individuals proportional to rewards, mitigating premature convergence of GA [19; 20]. Experiments show that ours outperforms recent GAs in the PMO benchmark; see Section 5.1.

Our extensive experiments demonstrate the effectiveness and practical applicability of the proposed method. First, our method achieves the highest total score of 16.213 across 23 oracles in the Practical Molecular Optimization benchmark , outperforming all other baselines. Second, we conduct in _in silico_ experiments for designing SARS-CoV-2 inhibitors. The proposed method successfully generates inhibitors with ten times fewer reward calls. Moreover, our method effectively balances optimization and diversity, achieving higher scores with increased diversity compared to previously top-ranked methods.

## 2 Background

### Sample efficient _de novo_ molecular optimization

Molecular design is the process of proposing new molecules likely to exhibit desirable outcomes. Compared to traditional virtual screening approaches, which identify suitable molecules from virtual

Figure 1: Overview of Genetic GFN. Our generative policy is trained to sample molecules proportional to rewards, and the genetic search refines them to higher-reward samples.

libraries with a large number of molecules known a priori, de novo approaches seek to generate molecule structures anew. The desired properties can be measured using score functions \(\), called oracle. Formally, molecular design can be formulated as \(*{arg\,max}_{x}(x)\), where \(x\) is a molecule, and \(\) denotes the chemical space which comprises all possible molecules.

The publication of standard benchmarks and datasets has facilitated the assessment of _de novo_ design methods (e.g., GuacaMol , Therapeutics Data Commons (TDC) ). The score functions are designed to consider various properties, such as the presence and absence of substructures, similarity, isomers, structural features, physicochemical properties, biological activity, and binding affinity (i.e., docking score). Notably, the PMO benchmark  offers a unified framework that comprehensively evaluates the sample efficiency of a range of molecular design methods.

### Generative flow networks

Generative flow networks (GFlowNets)  are introduced as a new class of probabilistic models to sample a discrete compositional object \(x\) from the target distribution, i.e., \(P(x) e^{-(x)}\). In general, direct sampling from the target distribution is challenging since the partition function \(Z=_{x}e^{-(x)}\) is intractable when the sample space is combinatorially large. Hence, GFlowNets sample an object from an unnormalized distribution as a constructive generative process, where discrete _actions_ iteratively modify a _state_ -- a partially constructed object. We define a trajectory as \(=(s_{0},,s_{T})\), where \(s_{T}\) is a terminal state corresponding to a fully constructed object \(x\).

A GFlowNet models flow \(F\) of particles along a directed acyclic graph (DAG). The source and sink nodes of the DAG correspond to the initial state \(s_{0}\) and terminal states \(s_{T}\), respectively. The _trajectory flow_\(F()\) is defined as a flow through the trajectory \(\), and the _state flow_\(F(s)\) is defined as the sum of trajectory flows that include the state \(s\), i.e., \(F(s)=_{s}F()\). The _edge flow_\(F(s s^{})\) the sum of trajectory flows through the edge from state \(s\) to \(s^{}\), i.e., \(F(s s^{})=_{(s,s^{})}F()\).

From the flow function \(F\), we derive two policy distributions. The _forward policy_\(P_{F}(s^{}|s)\) is the probability of transiting from state \(s\) to its child state \(s^{}\), defined as the edge flow \(F(s s^{})\) normalized by the state flow \(F(s)\), i.e., \(P_{F}(s^{}|s)=F(s s^{})/F(s)\). Similarly, the _backward policy_\(P_{B}(s|s^{})\) is the probability of moving from state \(s^{}\) to its parent state \(s\), defined as \(P_{B}(s|s^{})=F(s s^{})/F(s^{})\). Utilizing these forward and backward policies, GFlowNets can derive an optimal sampler \(P(s_{T})= P_{F}(s_{t}|s_{t-1})=R(s_{T})/Z\) if balance conditions (e.g., [24; 25; 26; 27; 6]) are satisfied.

Trajectory balance loss .One of the most popular conditions is _trajectory balance (TB)_, which directly parameterizes \(P_{F}\), \(P_{B}\), and flow of initial state (i.e., partition function) \(Z\) to satisfy the following trajectory balance condition:

\[Z_{t=1}^{n}P_{F}(s_{t}|s_{t-1})=R(s_{T})_{t=1}^{n}P_{B}(s_{t-1}|s_ {t}).\]

Then, this equation is converted into a loss function to be minimized along sampled trajectories, i.e.,

\[_{}(;)=(_{t=1}^{ n}P_{F}(s_{t}|s_{t-1};)}{R(x)_{t=1}^{n}P_{B}(s_{t-1}|s_{t};)} )^{2}.\] (1)

In GFlowNet training, employing exploratory behavior policies or replay training is allowed since GFlowNet can be trained in an off-policy manner, which is a key advantage [23; 24; 28; 18].

## 3 Genetic-guided GFlowNets

This section describes how the desired molecules are discovered with Genetic GFN. We model the generation process of molecules as a string-based constructive process. First, we pretrain the policy to learn the distribution of valid molecules. During the optimization phase, we iteratively generate molecules and update the policy with GFlowNet training using high-reward molecules. Particularly, we introduce graph-based genetic search to refine generated samples.

### Factorized string-based generative policy and unsupervised pretraining

Building on insights from previous works [3; 17], we employ a string-based representation strategy, simplifying the molecular generation process by reducing it to a one-directional sequence generation. We adopt a sequence generative policy using a string-based assembly strategy, especially the simplified molecular-input line-entry system (SMILES) . Motivated by REINVENT , we parameterize the policy using a recurrent neural network architecture . Then, the probability \(_{}()\) of generating a molecule, can be factorized to \(_{t=1}^{n}_{}(x_{t}|x_{1},,x_{t-1})\), where \(x_{1},,x_{n}\) are characters of SMILES representation of \(\).

As demonstrated in previous studies, including [3; 17; 8], pretraining is inevitable since training the generative policy from scratch is excessively sample-inefficient. Therefore, our policy is pre-trained to maximize the likelihood of valid molecules on existing chemical datasets \(_{}\); note that pretraining _does not require oracle information_. Precisely, the policy is pretrained to minimize the following:

\[_{}()=-_{t=1}^{n}_{}(x_{t}|x_{1}, ,x_{t-1}).\] (2)

### GFlowNet training of the generative policy with graph-based genetic search

To generate desirable molecules with limited reward calls, we iteratively generate samples using two distinct strategies (Section 3.2.1) and fine-tune the policy, initialized with \(_{}\), using a GFlowNet by replaying collected samples (Section 3.2.2). The overall procedure is described in Algorithm 1.

```
1:Set \(_{}_{}\), \(\)
2:while\(||\)do
3:\(\{,()\}\), where \(_{}()\)\(\) SMILES generation with policy
4: Initialize population \(_{}\) from \(\)\(\) Graph-based genetic search
5:for\(n=1\)to\(\)do
6:\((_{1},_{2}),\) where \((_{1},_{2})_{}\)
7:\(^{}()\)
8:\(\{^{},(^{ })\}\), \(_{}_{}\{^{},(^{})\}\)
9:\(_{}(_{} _{})\)
10:endfor
11:for\(k=1\)to\(\)do\(\) Updating the policy with GFlowNet training
12: Get \(\) from \(\) with rank-based sampling (Eq. (4))
13: Update \(\) to minimize \(|}_{}_{}+ (_{}()||_{}())\)
14:endfor
15:endwhile ```

**Algorithm 1** Genetic GFN training with limited reward calls

#### 3.2.1 Molecule generation strategies in Genetic GFN

We employ two distinct molecule generation strategies, SMILES generation with our training policy and graph-based genetic search. These two strategies are synergized to generate diversified and high-reward samples, efficiently searching the vast chemical space.

**SMILES generation with policy.** The training policy \(_{}\) generates SMILES sequences. Since the policy is trained using the trajectory balance loss (see the following subsection), it is the same as sampling \(\) from \(_{t=1}^{T}P_{F}(s_{t}|s_{t-1}) R(s_{T}=)\), where \(s_{t-1}\) is represented by previously collected SMILES token, and \( R()=-()\) with an inverse temperature \(\).

**Graph-based genetic search.** To effectively search the higher-reward region, we employ a genetic algorithm that iteratively evolves populations through _crossover_, _mutation_, and _selection_. We adopt the operations of the graph-based genetic algorithm, Graph GA , which has proven to effectively search the molecule space with finely designed genetic operations; please refer to the original paper for details. The genetic search is performed as follows:1. **Initialize a population**\(_{}\): The initial population is selected from the whole buffer \(\), consisting of samples from the policy and previous genetic search.
2. **Generate offspring**\(_{}\): A child is generated from randomly chosen two parent molecules by combining the fragments (_crossover_). Then, the child is randomly modified (_mutation_).
3. **Select a new population**\(^{}_{}\): Sample from \(_{}_{}\), and go back to 2.

One key advantage is that offspring can have a large distance from the parents in the 1D string space, even if the molecule distances are small, which is beneficial to avoid being stuck in local optima; see the experimental results in Table 2(b).

#### 3.2.2 Updating the generative policy with GFlowNets training

Using the generated samples, the policy is fine-tuned using the trajectory balance loss. The off-policy property of GFlowNet losses enables the utilization of refined samples from the genetic search. In particular, for better sample efficiency, we employ replay training with a rank-based reweighed buffer.

TB loss with KL-divergence penalty.The generative policy is trained using the trajectory balance loss in Eq. (1). Note that we set \(P_{B}\) to 1 for simplicity since SMILES generations are conducted in one direction. To ensure that the policy does not deviate excessively from the pretrained policy during training, we introduce a Kullback-Leibler (KL) divergence penalty inspired by the works in language model fine-tuning [31; 32]. Thus, our model is updated to minimize the following loss function:

\[=_{}(;)+(_{} (x)||_{}(x)),\] (3)

where \(_{}\) denotes the the pre-trained policy. As a result, \(_{}\) is trained to generate desired (by \(_{}\)) and valid (by \(_{}\)) molecules. Note that trajectories on which the proposed loss is minimized are sampled from the experience buffer.

Rank-based reweighed experience buffer.The rank-based reweighting biases the samples towards high-reward candidates by assigning greater weight to trajectories with higher ranks, thereby enhancing the focus on more promising solutions [33; 34]. The weight is computed as follows:

\[|+_{,}() )^{-1}}{_{}(k||+_{ ,}())^{-1}}.\] (4)

Here, \(k\) is a weight-shifting factor, and \(_{,}()\) is a relative rank of value of \(()\) in the dataset \(\). Note that we also utilize rank-based sampling in the genetic search (steps 1 and 3).

## 4 Related works

### Genetic algorithms for molecular optimization

Genetic algorithm (GA) is a representative meta-heuristic inspired by the evolutionary process. This subsection focuses on discussing the application of GA in molecular optimization. As one of the seminal works, a graph-based GA (Graph GA) was proposed with sophisticatedly designed operations based on chemical knowledge . Note that our method also adopts Graph GA operations in the genetic search. Various strategies for molecular assembly, not limited to graphs, have been utilized in GA [35; 14; 36]. A recent contribution by  introduces an enhanced version of Graph GA. They introduce quantile-uniform sampling to bias the population towards containing higher reward samples while maintaining diversity. Experimental results from Mol GA demonstrate the effectiveness of GAs as strong baselines, achieving state-of-the-art (SOTA) performance in the PMO benchmark.

### GFlowNets for molecular optimization

Generative Flow Networks (GFlowNets or GFN) [23; 6] have drawn significant attention in scientific discovery , particularly in molecular optimization and biological sequence design [6; 10; 38; 18; 39; 40; 11; 41]. GFlowNets, which are off-policy variational inference methods , are closely related to value-based reinforcement learning within soft Markov Decision Processes (soft MDPs) [43; 44], focusing on learning maximum entropy agents [45; 46]. This allows GFlowNets to generate 

[MISSING_PAGE_FAIL:6]

#### 5.1.1 Main results in the official benchmark of PMO

As baselines, we employ Top-8 methods from the PMO benchmark since they recorded the best AUC Top-10 in at least one oracle. The baseline methods include various ranges of algorithms and representation strategies. First, REINVENT  is an RL method that tunes the policy with adjusted likelihood. Graph GA , STONED , SMILES GA , and SynNet  are genetic algorithms that utilize different assembly strategies; they use fragment-based graphs, SELFIES, SMILES, and synthesis, respectively. Additionally, a hill climbing method (SMILES-LSTM-HC ) and Bayesian optimization (GP BO ) are included. SMILES-LSTM-HC iteratively generates samples and imitates high-reward samples, while GP BO uses a surrogate model with the Gaussian process (GP) and Graph GA to optimize the GP acquisition functions in the inner loop.

Moreover, we adopt additional methods, Mol GA  and GEGL . Mol GA is an advanced version of Graph GA and outperforms other baselines in the PMO benchmark. On the other hand, GEGL is an ablated version of our approach that utilizes imitation learning with a reward-priority queue instead of GFlowNet training with rank-based sampling. For both, we adopt the original implementations23 with hyperparameters searches following the guidelines; see Appendix C.

The main results in Table 1 report the AUC score of Top-10 candidates with independent five runs with different seeds. In addition, Fig. 2 visually presents the Top-10 average score across the computational budget, i.e., the number of oracle calls, providing a concise overview of the results. Due to the lack of space, the best five results are provided; please check Appendix G.2 for the rest of the results. As shown in Table 1, Genetic GFN outperforms the other baselines with a total of 16.213 and attains the highest AUC Top-10 values in 14 out of 23 score functions. The results of diversity and SA score for each oracle are presented in Appendix G.5.

#### 5.1.2 Controllability of the scores-diversity trade-off.

**Controllability of the scores-diversity trade-off.**

In the benchmark, we found a pronounced trade-off between attaining high evaluation scores within a limited budget and generating diverse molecular candidates. This section demonstrates the controllability of the score-diversity trade-off through adjustments in the inverse temperature \(\). Decreasing the inverse temperature gives more diverse candidates. The results in Fig. 3 demonstrate adjustments of \(\) can control the trade-off between score and diversity, achieving Pareto-frontier to other baselines in the benchmark. Notably, Genetic GFN with \(=30\) achieves a higher AUC Top-10 with a greater diversity compared to the SOTA GA method (Mol GA: 15.686 with a diversity of 0.465) and RL method (REINVENT: 15.185 with a diversity of 0.468). Similarly, the weight-shifting factor \(k\) in rank-based sampling can control the trade-off; see Appendix G.1.

**Ablation studies.** The ablation studies investigate the essential components of our framework: the genetic search (GS) and the KL-divergence penalty. To assess the effectiveness of the genetic search, we also compare its performance against the exploration strategy used in previous GFlowNet studies

    & Genetic &  & KL-divergence \\  & & \(\{\)GS\(\}\) & - \(\{\)GS\(\}\) + \(\{\)\(\)-greedy\(\}\) & penalty \\  AUC Top-1 & **16.530 \(\) 0.198** & 16.070 \(\) 0.290 & 15.966 \(\) 0.085 & 16.251 \(\) 0.440 \\ AUC Top-10 & **16.213 \(\) 0.173** & 15.738 \(\) 0.274 & 15.626 \(\) 0.082 & 15.928 \(\) 0.426 \\ AUC Top-100 & **15.516 \(\) 0.127** & 15.030 \(\) 0.322 & 14.939 \(\) 0.147 & 15.188 \(\) 0.297 \\   

Table 2: Ablation studies. In the GS ablation study (-\(\{\)GS\(\}\)), the generative policy solely generates samples, while \(\)-greedy samples from \(P_{F}\) mixed with a uniform distribution. The **bold** text indicates the best value.

Figure 3: Average of Top-10 score and diversity. Note that the fragment-based GFlowNet achieves 10.957 with a diversity of 0.816.

[6; 10]. It samples actions from a GFlowNet sampler mixed with a uniform distribution, similar to \(\)-greedy in RL. The results, shown in Table 2, reveal that the removal of either component results in a decline in performance, underscoring the importance of employing a suitable exploration strategy. Detailed results, including statistical analysis, are provided in Appendix G.4.

#### 5.1.3 Comparisons with GFlowNets variants

We compare Genetic GFN with the graph-based GFlowNet , GFlowNet-AL , and the local search GFlowNet (LS-GFN)  using SMILES representations. LS-GFN utilizes Monte Carlo Markov Chain (MCMC) techniques, incorporating partial backtracking and reconstructing solution trajectories with the training policy as the proposal distribution . The experiments are conducted on the PMO benchmark, and we implement LS-GFN with SMILES by replacing our genetic search with a local search. Note that while the original LS-GFN employs the prepend-append MDP, which does not directly apply to SMILES, we use the same one-directional SMILES generation as ours.

As shown in Table 2(a), Genetic GFN outperforms other GFlowNet variants. Notably, generating SMILES is significantly more advantageous than generating graph-based fragments. The performance gap between Genetic GFN and LS-GFN highlights the importance of a proper exploratory policy. To further analyze, we measure the distance between samples before and after searches in GSK3\(\) and JNK3. The normalized Levenshtein distances for SMILES and Tanimoto similarity for molecules are reported in Table 2(b). The results show that the local search may be inefficient in effectively searching in moderate-scale chemical spaces because its capabilities heavily depend on the current policy, leading to suboptimal search performance. In contrast, our approach leverages a domain-specialized genetic search within the molecule graph space, working as an effective off-policy exploration -- the samples with SMILES representation are used to train the string-based generative policy.

#### 5.1.4 Sample efficient multi-objective molecular optimization

According to Zhu et al. , we apply our method to multi-objective tasks: GSK3\(\)+JNK3 and GSK3\(\)+JNK3+QED+SA. Notably, GSK3\(\) and JNK3 are potential targets of Alzheimer's Disease treatments . We use a linear combination of each objective with given coefficients, and the performance is measured by hypervolumes with 1K evaluations. We obtained the results from five independent trials using different seeds. Even though Genetic GFN is not designed for multi-objective molecular optimization, it demonstrates notable performance using proper scalar-valued score functions; please see Appendix E for details.

#### 5.1.5 Further analysis

**Active learning with Genetic GFN.** Similar to GFlowNet-AL, ours can work as a generative model in multi-round active learning. We compare Genetic GFN-AL with other model-based and active learning methods; please refer to Appendix D.

**Genetic GFN with SELFIES representation.** Genetic GFN with SELFIES generation achieves the improved sample efficiency to other SELFIES-based methods; see Appendix G.6.

    & GSK3\(\) + JNK3 & GSK3\(\) + JNK3 \\  & + QED + SA \\  HierVAE+qParEGO & 0.205 \(\) 0.015 & 0.186 \(\) 0.009 \\ HierVAE+eHV1 & 0.341 \(\) 0.072 & 0.211 \(\) 0.006 \\ LaMOO & 0.279 \(\) 0.090 & 0.190 \(\) 0.069 \\ Graph GA & 0.368 \(\) 0.020 & 0.335 \(\) 0.021 \\ MARS & 0.418 \(\) 0.095 & 0.273 \(\) 0.020 \\ HN-GFN & 0.669 \(\) 0.061 & 0.416 \(\) 0.023 \\  Genetic GFN & **0.718 \(\) 0.138** & **0.642 \(\) 0.053** \\   

Table 4: Average and standard deviation of hypervolumes (\(\)) for each task. The baseline results are directly from the HN-GFN paper . The **bold** text indicates the best value.

    & SMILES &  &  & SMILES & Molecule \\   & Genetic GFN & LS-GFN  & GFN  & GFN-AL  \\  AUC Top-1 & **16.530 \(\) 0.198** & 15.514 \(\) 0.269 & 10.957 \(\) 0.033 & 11.032 \(\) 0.016 \\ AUC Top-10 & **16.213 \(\) 0.173** & 15.230 \(\) 0.026 & 9.918 \(\) 0.027 & 9.928 \(\) 0.027 \\ AUC Top-100 & **15.516 \(\) 0.127** & 14.619 \(\) 0.027 & 8.416 \(\) 0.024 & 8.064 \(\) 0.005 \\        & Fragment-based &  & SMILES & Molecule \\   & Genetic GFN & 0.240 & 0.528 \\ LS-GFN & 0.374 & 0.494 \\    
    & SMILES & Molecule \\   & Genetic GFN & 0.706 & 0.536 \\ LS-GFN & 0.403 & 0.512 \\   

Table 3: Comparison with GFlowNet variants. Notably, samples from the GS have larger SMILES distances than LS, leading to better sample efficiency. The **bold** text indicates the best value.

**Sensitivity analysis.** We provide the experimental results by varying the hyperparameters, such as the offspring size and the number of training loops; please refer to Appendix G.8.

### Designing inhibitors against SARS-CoV-2 targets

In this subsection, we conduct drug discovery experiments for Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-Cov-2), known as the novel coronavirus. One desired property is maximizing the binding affinity to the target protein. The binding affinity is measured with a docking score, which is calculated based on the energies of the interaction between the ligand and the receptor. Typically, the computation of docking scores is expensive since it involves predicting the spatial orientation and binding affinity of the molecule in the active site of the target protein. We employ Quick Vina 2  docking software to assess generated molecules.

Additionally, QED (Quantitative Estimate of Drug-likeness) and SA (Synthetic Accessibility) are considered to quantify the drug-likeness and difficulties of synthesizing. The higher QED, which ranges , and the lower SA, which ranges , are desired. Therefore, we define the score function \(s(x)\) for designing SARS-Cov-2 inhibitors as a linear combination of normalized scores according to the previous work . Following  and , the target proteins are selected: PLPro_7JIR, a critical enzyme in the life cycle of SARS-CoV-2, and RdRp_6YYT, which is essential for the replication and the transcription of genes.

The experiments are conducted with up to 1000 update steps with 128 batch size . As shown in Table 5, ours achieves the highest Top-100 average scores only with 100 steps, which is 10 times fewer than others. Note that the score is recalculated based on the normalized score function in Eq. (5) using average values in the MoIRL-MGPT paper ; the full results and a more detailed experimental setup are provided in Appendix F. We also report the best candidates of 100 steps in Fig. 4 and in Fig. 5. The final molecules correspond to the Top-1 score molecules from 3 independent runs.

## 6 Discussion

This paper introduces a Genetic-guided GFlowNet (Genetic GFN), which integrates a domain-specific genetic algorithm to guide the GFlowNet policy toward higher-reward samples. The method employs off-policy training with a rank-based reweighted buffer, enhancing the policy as a powerful amortized inference sampler for chemical discovery. Extensive experiments demonstrate that Genetic GFN effectively generates desirable molecules within the high-dimensional chemical space, including long chemical structure sequences (e.g., \( 100\)). On the other hand, our approach can be considered as

Figure 4: The final candidates for the PLPr_7JIR target with 100 steps.

Figure 5: The final candidates for the RdRp_6YYT target with 100 steps.

    & PLPro & RdRp \\  JT-VAE & 0.272 & 0.216 \\ GFlowNet & 0.326 & 0.280 \\ Graph GA & 0.723 & 0.786 \\ REINVENT & 0.717 & 0.799 \\ MoIRL-MGPT & 0.772 & 0.854 \\  Genetic GFN (100) & 0.891 & 0.873 \\ Genetic GFN (1000) & **0.925** & **0.902** \\   

Table 5: Average Top-100 scores (\(\)). Ours outperforms baselines with 10 times fewer steps. The **bold** denotes the best scores.

a novel population reinitialization strategy for genetic algorithms using GFlowNets, which sample diverse objects proportional to rewards.

**Limitations and future works.** Our method assumes the existence of effective genetic algorithms, which is valid for the molecular design domain. However, designing domain-specific operators for genetic algorithms can be challenging in other fields. One possible future work is to enhance genetic algorithms using the neural policy similar to recent studies . Another direction is to extend our approach to other domains, such as combinatorial optimization. For instance, we could utilize a powerful GA, hybrid genetic search , to design a GFlowNet-based solver for routing problems.

**Broader Impact.** This paper introduces a new generative model, significantly enhancing sample efficiency in molecular optimization. This advance is likely to hold substantial promise for this field, potentially accelerating the development of new therapies and advanced materials. Our research is currently focused on _in-silico_ experiments. The potential safety concerns of discovered molecules are further examined in the subsequent processes, such as _in-vitro_ experiments and pre-clinical tests.