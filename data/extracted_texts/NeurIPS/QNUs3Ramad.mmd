# Adversarial Self-Training Improves Robustness and Generalization for Gradual Domain Adaptation

Lianghe Shi

Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

Correspondence to: Weiwei Liu <liuwiewei863@gmail.com>.

###### Abstract

Gradual Domain Adaptation (GDA), in which the learner is provided with additional intermediate domains, has been theoretically and empirically studied in many contexts. Despite its vital role in security-critical scenarios, the adversarial robustness of the GDA model remains unexplored. In this paper, we adopt the effective gradual self-training method and replace vanilla self-training with adversarial self-training (AST). AST first predicts labels on the unlabeled data and then adversarially trains the model on the pseudo-labeled distribution. Intriguingly, we find that gradual AST improves not only adversarial accuracy but also clean accuracy on the target domain. We reveal that this is because adversarial training (AT) performs better than standard training when the pseudo-labels contain a portion of incorrect labels. Accordingly, we first present the generalization error bounds for gradual AST in a multiclass classification setting. We then use the optimal value of the Subset Sum Problem to bridge the standard error on a real distribution and the adversarial error on a pseudo-labeled distribution. The result indicates that AT may obtain a tighter bound than standard training on data with incorrect pseudo-labels. We further present an example of a conditional Gaussian distribution to provide more insights into why gradual AST can improve the clean accuracy for GDA.

## 1 Introduction

The key assumption of classical machine learning--that training and test data come from the same distribution--may not always hold in many real-world applications . A data distribution typically evolves due to changes in conditions: for example, changing weather in vehicle identification , sensor aging in sensor measurement , the evolution of road conditions in self-driving , etc. To address this problem, Unsupervised Domain Adaptation (UDA) has been developed to train a model that performs well on an unlabeled target domain by leveraging labeled data from a similar yet distinct source domain.

Various works in UDA  theoretically demonstrate that the generalization error can be controlled by the domain discrepancy. Hence, the domain shift between the source domain and the target domain is expected to be small . However, in some applications, the domain shift is substantial, leading to a sharp drop in the performance of the UDA method . Furthermore, since changes in real-world data are more often gradual than abrupt , there are many intermediate domains between the source domain and the target domain. To get a better solution to the gradually shifting data, some recent works have focused on the Gradual Domain Adaptation (GDA) problem, where the learner is additionally provided with unlabeled intermediate domains. The large gap between the source and target domains is then divided up into multiple small shifts between theintermediate domains. Recently, the celebrated work  proposes gradual self-training, which iteratively applies the self-training method to adapt the model along the intermediate domains. Empirically, the gradual self-training method greatly improves the target domain's accuracy compared to the traditional direct domain adaptation. After that, remarkable theoretical  and algorithmic [10; 1] advances have been achieved in GDA. However, the adversarial robustness of the GDA model remains unexplored, despite its vital role in security-critical scenarios. Adversarial robustness refers to the invariance of a model to small perturbations of its input , while adversarial accuracy refers to a model's prediction accuracy on adversarial examples generated by an attacker. Numerous works [33; 41; 49] have shown that very small perturbations, even those imperceptible to humans, can successfully deceive deep neural network (DNN) models, resulting in erroneous predictions.

One popular and effective method of improving robustness is Adversarial Training (AT), which adds adversarial examples to the training data. Due to the lack of labels in the target domain, it is difficult to directly generate adversarial examples. Accordingly, this paper replaces self-training with Adversarial Self-Training (AST), which first generates pseudo-labels on the unlabeled data and then adversarially trains the model based on these pseudo-labeled data. We conduct comprehensive experiments to validate the effectiveness of gradual AST in Section 3. Compared to the gradual self-training method, the proposed gradual AST method delivers a great improvement in adversarial accuracy, from \(6.00\%\) to \(90.44\%\) on the Rotating MNIST dataset. More interestingly, we find that the clean accuracy of gradual AST on the target domain also increases from \(90.06\%\) to \(97.15\%\). This is a surprising result, since many prior works [52; 38] demonstrate that AT may hurt generalization; in other words, there is a trade-off between clean accuracy and adversarial accuracy. We then empirically investigate the reason why AST improves clean accuracy in GDA and find that the adversarially trained model has better clean accuracy than the standardly trained model when the generated pseudo-labels contain a proportion of incorrect labels.

In Section 4, we present novel generalization error bounds for the proposed gradual AST. The results show that if we have a model \(f\) with low adversarial margin loss on the source domain and the distribution shifts slightly, gradual AST can generate a new model \(f^{}\) that is also adversarially robust on the target domain. To explain why gradual AST achieves better clean accuracy than vanilla gradual self-training, we use the optimal value of the Subset Sum Problem  to bridge the standard error on a real distribution and the adversarial error on a pseudo-labeled distribution. The result shows that AT may yield a tighter generalization guarantee than standard training on pseudo-labeled training data. We further provide an example of a conditional Gaussian distribution to illustrate why AT outperforms standard training when a proportion of the labels in the training data are incorrect.

We summarize our contributions as below:

* We are the first to apply the AST method in GDA. The proposed gradual AST method can not only improve the adversarial robustness but also the clean accuracy for GDA, which is an appealing and nontrivial result, considering that many prior works demonstrate that adversarial training may hurt generalization.
* We empirically explore the reason for the improvements in clean accuracy and find that adversarial training performs better than standard training when the pseudo-labeled training set contains a proportion of incorrect labels.
* From the theoretical perspective, we provide the generalization error bounds for gradual AST, which explain why the trained model is adversarially robust on the target domain. We provide an error bound and a toy example of Gaussian distribution to provide some intuitions for the improvements in clean accuracy.

## 2 Preliminaries

### Multiclass Classification Learning Framework

Let \(=\) be a measurable instance space, where \(\) and \(\) denote the input space and label space, respectively. The input space \(\) is a subset of a \(d\)-dimensional space, \(^{d}\). In this work, we focus on multiclass classification, and the label space \(\) is \(\{1,,k\}\), where \(k\) is the number of classes. Following the notations used in , a domain is envisioned as a tuple \( P,h_{P}\), consisting of a distribution \(P\) on input space \(\) and a labeling function \(h_{P}:\). In practice, the true distribution \(P\) is unknown to the learner, which has access only to the training data \(S\) drawn independent and identically distributed (i.i.d.) according to the true distribution \(P\). We use \(\) to denote the empirical distribution of \(P\) according to the training data \(S\). Moreover, following the notations used in , we consider the class \(=\{f:^{k}\}\) of scoring functions \(f\). We use \(f_{y}(x)\) to indicate the output of \(f\) associated with the data point \(x\) on the \(y\)-th dimension; the output indicates the confidence of the prediction. The label with the largest score is the predicted label of \(x\). We use \(_{}=\{h_{f}()=}{} \!f_{y}():f\}\) to denote the labeling function class induced by \(\). The expected risk and the empirical risk of a classifier \(h\) with respect to a labeling function \(h^{}\) on distribution \(P\) are defined as \(R_{P}(h,h^{})}[h(x)  h^{}(x)]\) and \(R_{}(h,h^{})}{} [h(x) h^{}(x)]\), respectively, where \(\) is the indicator function. We use \(R_{P}(h)\) and \(R_{}(h)\) to abbreviate \(R_{P}(h,h_{P})\) and \(R_{}(h,h_{P})\) respectively. Furthermore, the 0-1 loss is non-differentiable and cannot be minimized directly. Thus, a margin theory for multiclass classification was developed by , that replaces the 0-1 loss with the margin loss. The margin of a scoring function \(f\) for labeled data \((x,y)\) is defined as \(_{f}(x,y) f_{y}(x)- y}{}\!f_{y^{ }}(x)\). And the expected margin loss of a scoring function \(f\) with respect to another scoring function \(f^{}\) on distribution \(P\) is defined as \(R_{P}^{()}(f,f^{})}_{ }_{f}(x,h_{f^{}}(x))\), where \(_{}(m)=\{1,\{0,1-m/\}\}\) is the ramp loss. Similarly, we use \(R_{P}^{()}(f)\) and \(R_{}^{()}(f)\) to abbreviate \(R_{P}^{()}(f,f_{P})\) and \(R_{}^{()}(f,f_{P})\) respectively. Note that \(R_{P}(h_{f},h_{f^{}}) R_{P}^{()}(f,f^{})\), since the 0-1 loss is upper bounded by the margin loss.

### Gradual Domain Adaptation

Under the standard GDA settings [26; 45], the learner is sequentially trained on \(T+1\) domains with gradual shifts. The corresponding data distributions are \(P_{0},P_{1},,P_{T}\), where \(P_{0}\) is the distribution of the source domain, \(P_{T}\) is the distribution of the target domain, and \(P_{1},,P_{T-1}\) are the distributions of the intermediate domains. For simplicity, we assume that the number of data points in each domain is the same: namely, each domain \(t\) has a set of \(n\) data drawn i.i.d. from \(P_{t}\), denoted as \(S_{t}\). Recently,  proposes a gradual self-training method for GDA, which successively applies self-training to each of the intermediate domains. The self-training method adapts a pre-trained model trained on the previous domain to the current domain using unlabeled data drawn from the current distribution. Specifically, given a pre-trained model \(f\) and an unlabeled data set \(S\), the model first predicts the labels of the unlabeled data, then trains \(f\) with empirical risk minimization (ERM) over these pseudo-labeled data, i.e.,

\[f^{}=ST(f,S)=}{}R_{}^{ ()}(f^{},f)=}{} _{x_{i} S}_{}_{f^{}}(x_{i},h_{f}(x_{i})).\]

In gradual self-training, the model is first pre-trained on the labeled source domain and then successively self-trained on the unlabeled dataset of each of the intermediate domains, i.e.,

\[f_{t}=ST(f_{t-1},S_{t}),t\{1,,T\}.\]

Since the domain shifts between the consecutive domains are assumed to be small [10; 26; 45], the accuracy of the pseudo-labels is expected to be high at each step. The gradual self-training method aims to output a final trained classifier \(h_{f_{T}}\) with low expected risk in the target domain.

### Adversarial Self-Training

Given a classifier \(h_{f}\) and a data point \((x,y)\), we can generate the corresponding adversarial example \((x^{adv},y)\) by adversarially perturbing \(x\) in a small neighborhood \(B_{}(x)\) of \(x\), as follows: \(x^{adv}=_{x^{} B_{}(x)}_{}_{f}(x^{ },y)\). In this paper, we focus on the \(_{q}\) adversarial perturbation \(B_{}(x)\{x^{}:\|x^{}-x\|_{q} ,q 1\}\), which is referred to as \(_{q}\)-attack in the AT context and has been widely studied in existing works. Given a vector \(x^{d}\), we define the \(_{q}\)-norm of \(x\) as \(\|x\|_{q}(_{i=1}^{d}|x_{(i)}|^{q})^{1/q}\), for \(q[1,)\), where \(x_{(i)}\) is the \(i\)-th dimension of \(x\); for \(q=\), we define \(\|x\|_{}|x_{(i)}|\). Similar to the definition of the standard risk \(R_{P}(h,h^{})\), we define the adversarial risk on distribution \(P\) for any two classifiers \(h,h^{}\) as follows: \(_{P}(h,h^{})}}{}[h(x+) h^{}(x)]\). We also define the expected adversarial margin loss for any two scoring functions \(f,f^{}\) on distribution \(P\) as follows:

\[_{P}^{()}(f,f^{})}}{}_{}_{f}(x+ ,h_{f^{}}(x)).\] (1)

Although the expected adversarial margin loss is asymmetrical, it satisfies the triangle inequality (see proof in Appendix A.1). The empirical adversarial risk and the empirical adversarial margin loss can be defined similarly. We use \(_{P}^{()}(f)\) and \(_{}^{()}(f)\) to abbreviate \(_{P}^{()}(f,f_{P})\) and \(_{}^{()}(f,f_{P})\) respectively. To train a robust model with resistance to the adversarial examples, AT aims to find a classifier \(h_{f}\) that yields the minimal adversarial margin loss, i.e., \(}{}_{}^{()}(f)\). However, we cannot adversarially train the model directly on the unlabeled target domain because the conventional AT method requires labeled data. Instead, we adapt the self-training method to the adversarial self-training (AST) method by generating adversarial examples over the pseudo-labels, i.e.,

\[f^{}=AST(f,S)=}{}_{}^{()}(f^{},f)=}{ }_{x_{i} S}\|_{q} }{}_{}_{f^{}}(x_{i}+_{i},h_{f}(x_{i})).\]

## 3 Empirical Exploration

In this section, we propose a gradual AST method to improve adversarial robustness. Surprisingly, we find that the gradual AST improves not only adversarial robustness but also clean accuracy. Our code is available at https://github.com/whustone007/AST_GDA.

**Datasets.** We run experiments on two datasets that are widely used in GDA [26; 45]. **Rotating MNIST** is a semi-synthetic dataset generated by rotating each MNIST image at an angle between \(0\) and \(60\) degrees. The \(50{,}000\) training set images are divided into three parts: a source domain of \(5000\) images (\(0\)-\(5\) degrees), \(21\) intermediate domains of \(42{,}000\) images (\(5\)-\(60\) degrees), and a set of validation data. The rotating degree gradually increases along the domain sequence. The \(10{,}000\) test set images are rotated by \(55\)-\(60\) degrees, representing the target domain. **Portraits** is a real dataset consisting portraits of American high school students across a century. The model aims to classify gender. The dataset is split into a source domain of the first \(2000\) images, seven intermediate domains of the next \(14{,}000\) images, and a target domain of the next \(2000\) images.

**Methods.** The standard gradual self-training method  successively adapts the model to the next domain via self-training. To improve the adversarial robustness, we replace the vanilla self-training with AST. We implement multiple gradual AST methods with different starting domains \(\{0,,T+1\}\) where we begin to use AST, i.e.,

\[f_{t}=\{ST(f_{t-1},S_{t}),&1 t-1\\ AST(f_{t-1},S_{t}),& t T.\]

In particular, if \(=0\), then the model is adversarially trained throughout; if \(=T+1\), then the model is standardly trained throughout. Note that \(T=21\) in Rotating MNIST and \(T=7\) in Portraits.

**Implementation Details.** We implement our methods using PyTorch  on two Nvidia GeForce RTX 3090 Ti GPUs. Following , we use a \(3\)-layer convolutional network with dropout (\(0.5\)) and BatchNorm on the last layer. We use mini-batch stochastic gradient descent (SGD) with momentum \(0.9\) and weight decay \(0.001\). The batch size is \(32\) and the learning rate is \(0.001\). We train the model for \(40\) epochs in each domain. We set the radius of the bounded perturbation to be \(0.1\) for Rotating MNIST and \(0.031\) for Portraits. Following , we use PGD-\(20\) with a single step size of \(0.01\) as the adversarial attacker.

### Results

The results of the different methods on Rotating MNIST are shown in Figure 1(a). The numbers in the abscissa represent different starting domains \(\) of various methods. From Figure 1(a), we can see that the vanilla gradual self-training method (\(=22\)) achieves a clean accuracy of \(90.06\%\) and adversarial accuracy of \(6.00\%\) on the target domain. Notably, the gradual AST method that uses ASTthroughout (\(=0\)) improves the adversarial accuracy to \(83.54\%\) (\(+77.54\%\)). More interestingly, we find that the clean accuracy is also slightly improved by \(0.53\%\). Furthermore, the results of varying the starting domains \(\) indicate that the performance will be further improved if we use the vanilla self-training method for the first few domains and then use AST from an intermediate domain. We utilize the validation set of the target domain to choose the optimal starting time \(\). Experimentally, the best choice of starting time for Rotating MNIST is \(=9\), which achieves clean accuracy of \(96.66\%\) (\(+6.6\%\)) and adversarial accuracy of \(88.86\%\) (\(+82.86\%\)). We find that AST not only improves adversarial robustness but also clean accuracy with a large step. The results of the different methods on Portraits are shown in Figure 1(b). We find similar results to those on Rotating MNIST: the vanilla gradual self-training method (\(=22\)) achieves a clean accuracy of \(82.03\%\) and adversarial accuracy of \(40.23\%\), while the gradual AST method (\(=0\)) improves the clean accuracy by \(2.73\%\) and the adversarial accuracy by \(37.40\%\). We provide more experimental results of different \(\), domain numbers, and neural networks in Appendix B.

### Why Does Gradual AST Improve Clean Accuracy in GDA?

It is widely believed that AT would hurt generalization [52; 38] and decrease clean accuracy; thus it is surprising that the proposed gradual AST method improves the clean accuracy of the trained model in the GDA setting. In this section, we speculate on the reasons for this. Recently, some works [39; 42] have revealed that an adversarially pre-trained model tends to improve the target model's accuracy in transfer learning. We accordingly investigate whether the adversarially trained model has better clean accuracy on the next domain and thus generates more accurate pseudo-labels. However, by comparing the gradual AST methods with \(=0\) and \(=1\) on Rotating MNIST, we find that the model standardly trained on the source domain achieves a clean accuracy of \(97.30\%\) on the next domain, while the model adversarially trained on the source domain achieves a clean accuracy of \(96.95\%\) on the next domain. On the other hand, given that the pseudo-labels contain a small portion of incorrect labels, we wonder whether the adversarially trained model has better clean performance than the standardly trained model if trained on such a noisy training set. Since we set a fixed random seed for all gradual AST methods with varying starting domains, given a \(_{0}\{0,,T\}\), the gradual AST method with \(=_{0}\) and that with \(=_{0}+1\) have the same training processes before domain \(_{0}\) and the same pseudo-label set on domain \(_{0}\). The difference between the gradual AST method with \(=_{0}\) and that with \(=_{0}+1\) is whether we adversarially train the model on the domain \(_{0}\). We compare the clean accuracy of these two methods on domain \(_{0}\) in Figure 1(c). As we can observe from the figure, the adversarially trained model (\(=_{0}\)) has higher clean accuracy on domain \(_{0}\) than the standardly trained model (\(=_{0}+1\)). Note that there are two exceptions in which AT hurts clean performance when the accuracy of pseudo-labels is very high (\(_{0}=0,1\)), which explains why using AST from an intermediate domain is better than using AST throughout. Besides, we find that the adversarially trained model predicts more accurate pseudo-labels in the next domain than the standardly trained model when the current training set contains a proportion of incorrect labels. We then conclude that AT benefits the clean accuracy of GDA from two perspectives:

* For the intermediate domains, AT improves the accuracy of the pseudo-labels in the next domain. So the learner can fine-tune the model on a more accurate dataset.
* For the target domain, when the pseudo-labels contain a proportion of incorrect labels, the adversarially trained model has higher clean accuracy on the target domain than the standardly trained model.

### Sensitivity of Filtration Ratio \(\)

Following the standard experimental strategy [26; 45], for each step of self-training, we filter out a portion of images for which the model's prediction is least confident. In Figure 1, the ratio \(\) is set to \(0.1\) as in the work of . We further conduct more experiments with varying \(\). With smaller \(\), more data will be included for the next domain's training, but the accuracy of the pseudo-labels will decrease. Due to the page limit, we attach the detailed results of methods with different values of \(\) and \(\) in Appendix B. We find that when \(\) decreases, the performance of the standard gradual self-training also decreases. However, the gradual AST method prefers smaller \(\) even though more incorrect pseudo-labels are included in the training set. For example, when we set \(=0.05\), the gradual AST with the optimal starting domain \(=9\) (chosen using the validation set) achieves clean accuracy of \(97.15\%\) (\(+7.09\%\)) and adversarial accuracy of \(90.44\%\) (\(+84.44\%\)) on Rotating MNIST.

[MISSING_PAGE_FAIL:6]

show the error correction effect of AT on the incorrect labels. Our theory thus supports the excellent performance of gradual AST shown in Section 3.

### Generalization Error Bounds for Gradual AST

We introduce the Margin Disparity Discrepancy (MDD) to measure the domain shifts, as below.

**Definition 4.1** (Mdd ).: Let \(P\) and \(Q\) be two distributions over \(\). Given a scoring function class \(\) and a specific scoring function \(f\), the MDD between \(P\) and \(Q\) is defined as

\[d^{()}_{f,}(P,Q)_{f^{}}( R^{()}_{Q}(f^{},f)-R^{()}_{P}(f^{},f)).\] (2)

When the domain shift is slight, the MDD between the domains is small. The MDD is well-defined and can be empirically estimated by the training data , where the estimation error can be bounded by Rademacher complexity. We next present the definition of Rademacher complexity, which is widely used in generalization theory to measure the complexity of a hypothesis class.

**Definition 4.2** (Rademacher Complexity ).: Let \(\) be a set of real-valued functions defined over \(\). For a fixed collection \(S\) of \(n\) data points, the empirical Rademacher complexity of \(\) is defined as

\[}_{S}()}[_{g}_{x_{i} S}_{i}g(x_{ i})].\]

The expectation is taken over \(=(_{1},,_{n})\), where \(_{i},i\{1,,n\}\), are independent uniform random variables taking values in \(\{-1,+1\}\).

We next introduce an adversarial margin hypothesis class \(}}\).

**Definition 4.3** (Adversarial Margin Hypothesis Class).: Given a scoring function class \(\), the adversarial margin hypothesis class is defined as

\[}}\{x_{\|\| _{q}}_{f^{}}(x+,h_{f}(x)):f,f^{}\}.\]

Having defined the Rademacher complexity and the MDD, we now focus on a pair of arbitrary consecutive domains. The following result shows that AST returns a new model \(f^{}\) with bounded adversarial margin loss if the domain shifts gradually. The proof can be found in Appendix A.2.

**Theorem 4.4** (Adversarial Error Bound for AST).: _Let \( P,f_{P}\) and \( Q,f_{Q}\) be two domains with gradual shifts. Suppose we have a scoring function \(f\) pre-trained on domain \( P,f_{P}\), and a data set \(S\) of \(n\) unlabeled data points drawn \(i.i.d.\) according to distribution \(Q\). \(f^{}\) is the adapted scoring function by generated by the AST algorithm over \(S\), i.e., \(f^{}=AST(f,S)\). Then, for any \( 0\), the following holds with probability of at least \(1-\) over data points \(S\):_

\[^{()}_{Q}(f^{})^{()}_{P}(f)+ ^{*}+^{*}+d_{f,}(P,Q)+ }_{S}(}})+6}{2n}},\]

_where \(^{*}=_{f^{}}^{()}_ {Q}(f^{},f)\) and \(^{*}=_{f}R^{()}_{P}(f)+R^{()}_{Q}(f) }\)._

_Remark 4.5_.: Theorem 4.4 indicates that if we have a model \(f\) with low adversarial margin loss on the domain \( P,f_{P}\) and the distribution shifts slightly, AST generates a new model \(f^{}\) that is also adversarially robust on the domain \( Q,f_{Q}\). The \(^{*}\) term is widely used in domain adaptation theory  to implicitly characterize the conditional distribution shift. In the setting of GDA, we assume that the domain shifts only slightly, i.e., \(^{*}\) and \(d_{f,}(P,Q)\) are relatively low.

Based on the error bounds for AST, we can apply this argument inductively and sum these error differences along the domain sequence to obtain the following corollary. See proofs in Appendix A.3.

**Corollary 4.6** (Error Bounds for Gradual AST).: _Given a sequence of domains \( P_{t},f_{P_{t}},t\{0,,T\}\) with gradual shifts, each intermediate domain has an unlabeled data set \(S_{t}\) drawn \(i.i.d.\) from \(P_{t}\). The model is successively trained by the AST method, i.e., \(f_{t}=AST(f_{t-1},S_{t}),t\{0,,T\}\)._\(\{1,,T\}\). Then, for any \( 0\), the following holds with probability of at least \(1-\) over data points \(\{S_{t}\}_{t=1}^{T}\):_

\[_{P_{T}}(h_{f_{T}})_{P_{0}}^{()}(f_{0})+ _{t=1}^{T}_{t}+}_{S})}+6T}{2n}},\]

_where \(_{t}=d_{f_{t-1},}(P_{t-1},P_{t})+_{f} _{P_{t}}^{()}(f,f_{t-1})+_{f}\{R_{P}^{ ()}(f)+R_{Q}^{()}(f)\}\)._

_Remark 4.7_.: Our bound indicates that the adversarial risk on target domain can be controlled by the adversarial margin loss on the source domain and the discrepancy  between the intermediate domains. We also consider the standard risk and find that the standard risk of AST can be bounded similarly. We provide the bounds in Appendix A.4.

**Comparison with Previous Work.** Both of the works of  and  focus on binary classification and standard self-training, while our results non-trivially extend the error bounds into the setting of adversarial GDA and multiclass classification. Furthermore,  sets linear hypotheses as their training models, while we consider a more general hypothesis class that can be implemented by neural networks.  makes an assumption that the new model can always fit the pseudo-labels without error. As we can observe from the experiments, the training error tends to be small but never equal to \(0\). Compared to , we don't need the assumption, so our setting is more consistent with real experiments.

### Adversarial Training Improves Clean Accuracy for Self-Training

AT is generally considered harmful to the clean accuracy of the model [52; 38]. However, the experimental results in Section 3 indicate that AT unexpectedly improves both the clean accuracy and adversarial accuracy of the model trained on the pseudo-labels. In this section, we provide some theoretical insights showing that when the training data contains a small portion of incorrect labels, the adversarially trained model may have a tighter generalization guarantee than the standardly trained model. Our theorem is based on the following optimization problem.

**Definition 4.8**.: Given two fixed vectors \(p\) and \(p^{}\) with all positive coordinates and a fixed \(N\)-dimensional \(0\)-\(1\) vector \(e\), i.e., \(e\{0,1\}^{N}\). It is a fixed subset of \(\{1,,N\}\). The variant of the Subset Sum Problem  can be defined as follows:

\[_{\{0,1\}^{N}}|p^{T}-{p^{}}^{T}e |,\ \ s.t.\ \ _{i}=e_{i}, i\{1,,N\}.\]

We use \(^{*}(p^{},p,e,)\) to denote the optimal value of the problem. Given two fixed vectors \(p^{}\) and \(e\), the value of the term \({p^{}}^{T}e\) is therefore constant. The optimization problem can be viewed as one of selecting a subset of \(p\) such that the sum of its elements is as close as possible to \({p^{}}^{T}e\). Intuitively, the set \(\) adjusts the difficulty of this optimization problem. For two sets \(_{1}_{2}\), we have \(^{*}(p^{},p,e,_{1})^{*}(p^{},p,e,_{2})\) since we can optimize \(\) on more coordinates.

With a little abuse of notations, in this subsection, we consider a discrete real distribution \(P\) and a discrete noisy pseudo-labeled distribution \(P_{}\) over \(\). We denote the corresponding probability mass functions as \(p\) and \(p_{}\) respectively. \(p\) and \(p_{}\) can be viewed as two vectors, i.e., \(p_{i}=p(x_{i},y_{i}),(x_{i},y_{i}),i\{1, ,||\}\). The standard risk on the real data distribution is defined as \(R_{P}(h)=_{(x,y)}p(x,y)[h(x) y]\) and the adversarial risk on the pseudo-labeled data distribution is defined as \(_{P_{}}(h)=_{(x,y)}p_{ }(x,y)_{\|\|_{q}}[h(x+) y]\).

The next result uses the optimal value \(^{*}(p,p_{},e,)\) to bound the difference between these two risks. The proof can be found in Appendix A.5.

**Theorem 4.9**.: _Given two distributions \(P\) and \(P_{}\) over \(\) with the corresponding probability mass vectors \(p\) and \(p_{}\), the following bound holds for any classifier \(h_{}\):_

\[R_{P}(h)_{P_{}}(h)+^{*}(p,p_{},e,_{e}),\]

_where \(e\) is the risk vector \(e_{i}=[h(x_{i}) y_{i}]\), and \(_{e}=\{i:\|_{i}\|_{q},h(x_{i}+_{ i}) h(x_{i})\}\)._By setting \(=0\), we can derive the error bound for standard training: \(R_{P}(h) R_{P_{}}(h)+^{*}(p,p_{},e,_{0})\). By the definition of the set \(_{}\), the larger the perturbation radius \(\), the larger the size of \(_{}\), i.e., \(_{0}_{}\). We then have \(^{*}(p,p_{},e,_{})^{*}(p,p_{},e,_{0})\). Theorem 4.9 implies that if the adversarial risk \(_{P_{}}(h)\) on the pseudo-labeled distribution is well controlled by AT, then AT may yield a tighter generalization guarantee than standard training.

### A Toy Example of a Conditional Gaussian Distribution

We further provide an example of a conditional Gaussian distribution to show the efficacy of AT under incorrect pseudo-labels.

**Example 4.10** (Conditional Gaussian Distribution).: We consider the input space \(=\) and the label space \(=\{-1,+1\}\). For simplicity, we assume that the two classes have symmetrical mean parameters \(\{-,+\}\) and the same standard deviations \(\). The \((,)\)-conditional Gaussian distribution \(P\) can then be defined as a distribution over \(=\{-1,+1\}\), where the conditional distribution of \(X\) given \(Y\) is \(X|Y=y(y,^{2})\) and the marginal distribution of \(Y\) is \(P_{Y}(\{-1\})=P_{Y}(\{+1\})=1/2\). We consider a threshold hypothesis class, i.e., \(\{x(x-b):b\}\), where sgn is the sign function. We denote the hypothesis with threshold \(b\) as \(h_{b}\). In the setting of self-training, we have a labeling function \(h_{w}\), which is the model trained on the previous domain. The threshold \(w 0\), since there is a distribution shift. The function \(h_{w}\) assigns \(y=-1\) to the data points where \(x<w\) and assigns \(y=+1\) to the data points where \(x>w\).

We denote the pseudo-labeled data distribution over \(\) as \(P_{}\). We use \(R_{P_{}}(h)\) and \(_{P_{}}(h)\) to respectively denote the standard \(0\)-\(1\) risk and adversarial \(0\)-\(1\) risk on the pseudo-labeled data distribution \(P_{}\). We assume \(|w|\) and \(\), since the domain shift is slight and the adversarial perturbation is subtle. We also assume \(>\), which means that most samples can be well separated. The learner is provided with infinite samples. Under these assumptions, we provide the following results to demonstrate the efficacy of AT. The proof can be found in Appendix A.6.

**Theorem 4.11**.: _For the real data distribution \(P\) defined in Example 4.10, the optimal classifier that minimizes the standard risk \(R_{P}(h)\) and adversarial risk \(_{P}(h)\) is \(h_{0}()=()\). For the pseudo-labeled distribution \(P_{}\) defined in Example 4.10, the standardly trained classifier \(h_{std}\) that minimizes the standard risk \(R_{P_{}}(h)\) is \(h_{std}=h_{w}\); the adversarially trained classifier \(h_{adv}\) that minimizes the adversarial risk \(_{P_{}}(h)\) has the following corresponding threshold \(b_{adv}\)_

\[b_{adv}=\{w+,&w<-\\ 0,&-<w<\\ w-,&w>.\]

_Remark 4.12_.: Theorem 4.11 shows that the adversarially trained model \(h_{adv}\) consistently moves the threshold close to the optimal threshold \(b=0\), leading to a smaller generalization error. We visually represent the adversarially trained model in Figure 2.

## 5 Related Work

**Self-Training.** A series of works have achieved significant progress using self-training in semi-supervised learning [29; 40] and domain adaptation . The recent work  has proposed robust

Figure 2: The visualization of the adversarially trained classifier for a conditional Gaussian distribution. AT consistently moves the threshold \(b_{adv}\) close to the optimal threshold \(b=0\), leading to better generalization on the real data distribution. Best viewed in color.

self-training (RST) to leverage additional unlabeled data for robustness [2; 36]. While RST  focuses on utilizing additional unlabeled data drawn from the same distribution to improve adversarial robustness, our work investigates the different effects of vanilla self-training and adversarial self-training on clean accuracy.

**Gradual Domain Adaptation.** Unlike the UDA problem, GDA has intermediate domains between the source domain and target domain of UDA. Some works [22; 17; 48] propose various algorithms to handle these evolving domains in a computer vision context. The recent work  adopts the self-training method and proposes gradual self-training, which iteratively adapts the model along the domain sequence. Moreover,  provides the first learning theory for GDA and investigates when and why the gradual structure helps.  further improves the generalization error bound in  and reveals the influence of the number of intermediate domains on the error bound.  investigates a more difficult setting in which the learner is provided with a set of unordered intermediate data. The authors propose TDOL, a framework that generates synthetic domains by indexing the intermediate data. However, the aforementioned works pay insufficient attention to the adversarial robustness of the model in a GDA context. In the field of UDA, the work of  constructs synthetic intermediate domains by creating Grassmannian manifolds between the source and the target. Another line of work instead uses a generative network, such as a cycle generative adversarial network , to generate the intermediate data.

**Adversarial Training.** After  shows that DNNs are fragile to adversarial attacks, a large amount of works have proposed various attack methods [33; 27; 8] and defense methods [47; 19; 51; 31; 20; 24; 30]. Adversarial training  is one of the popular and effective methods that improves adversarial robustness by adding adversarial examples to the training dataset. From the theoretical perspective, some works focus on the sample complexity [50; 54; 12] and the generalization of adversarial training.  investigates the trade-off between robustness and fairness. [46; 55] study adversarial robustness under self-supervised learning.

## 6 Conclusion

In this work, we empirically demonstrate that gradual AST improves both the clean accuracy and the adversarial accuracy of the GDA model. We reveal that the reason for this performance improvement is that AT outperforms standard training when the training data contains incorrect pseudo-labels. We first provide generalization error bounds for gradual AST in a multiclass setting. We then construct the Subset Sum Problem to connect the adversarial error and the standard error, providing theoretical insights into the superiority of AST in GDA. The example of conditional Gaussian distribution is further provided to give additional insights into the efficacy of AT on pseudo-labeled training data.