# Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions

Tongxin Li

School of Data Science

CUHK-SZ, China

litongxin@cuhk.edu.cn

&Yiheng Lin

Computing + Mathematical Sciences

Caltech, USA

yiheng1@caltech.edu

Shaolei Ren

Electrical & Computer Engineering

UC Riverside, USA

shaolei@ucr.edu

&Adam Wierman

Computing + Mathematical Sciences

Caltech, USA

adamw@caltech.edu

###### Abstract

We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.

## 1 Introduction

Machine-learned predictions and hand-crafted algorithmic advice are both crucial in online decision-making problems, driving a growing interest in _learning-augmented algorithms_[1; 2] that exploit the benefits of predictions to improve the performance for typical problem instances while bounding the worst-case performance [3; 4]. To this point, the study of learning-augmented algorithms has primarily viewed machine-learned advice as potentially untrusted information generated by black-box models. Yet, in many real-world problems, additional knowledge of the machine learning models used to produce advice/predictions is often available and can potentially improve the performance of learning-augmented algorithms.

A notable example that motivates our work is the problem of minimizing costs (or maximizing rewards) in a single-trajectory Markov Decision Process (MDP). More concretely, a value-based machine-learned policy \(\) can be queried to provide suggested actions as advice to the agent at each step [5; 6; 7]. Typically, the suggested actions are chosen to minimize (or maximize, in case of rewards) estimated cost-to-go functions (known as Q-value predictions) based on the current state.

Naturally, in addition to suggested actions, the Q-value function itself can also provide additional information (e.g., the long-term impact of choosing a certain action) potentially useful to the design of a learning-augmented algorithm. Thus, this leads to two different designs for learning-augmented algorithms in MDPs: _black-box_ algorithms and _grey-box_ algorithms. A learning-augmented algorithm using \(\) is black-box if \(\) provides only the suggested action \(\) to the learning-augmented algorithm,whereas it is value-based (a.k.a., grey-box) if \(\) provides an estimate of the Q-value function \(\) (that also implicitly includes a suggested action \(\) obtained by minimizing \(\)) to the learning-augmented algorithm.

Value-based policies \(\) often perform well empirically in stationary environments in practice [5; 6]. However, they may not have performance guarantees in all environments and can perform poorly at times due to a variety of factors, such as non-stationary environments [8; 9; 10; 11], policy collapse , sample inefficiency , and/or when training data is biased . As a consequence, such policies often are referred to as "untrusted advice" in the literature on learning-augmented algorithms, where the notion of "untrusted" highlights the lack of performance guarantees. In contrast, recent studies in competitive online control [15; 16; 17; 18; 19; 20; 21] have begun to focus on worst-case analysis and provide control policies \(\) with strong performance guarantees even in adversarial settings, referred to as _robustness_, i.e., \(\) provides "trusted advice." Typically, the goal of a learning-augmented online algorithm [1; 3] is to perform nearly as well as the untrusted advice when the machine learned policy performs well, a.k.a., achieve _consistency_, while also ensuring worst-case robustness. Combining the advice of an untrusted machine-learned policy \(\) and a robust policy \(\) naturally leads to a tradeoff between consistency and robustness. In this paper, we explore this tradeoff in a time-varying MDP setting and seek to answer the following key question for learning-augmented online algorithms:

_Can Q-value advice from an untrusted machine-learned policy, \(\), in a **grey-box** scenario provide more benefits than the **black-box** action advice generated by \(\) in the context of **consistency and robustness tradeoffs** for MDPs?_

### Contributions

We answer the question above in the affirmative by presenting and analyzing a unified projection-based learning-augmented online algorithm (PROjection Pursuit policy, simplified as PROP in Algorithm 1) that combines action feedback from a trusted, robust policy \(\) with an untrusted ML policy \(\). In addition to offering a consistency and robustness tradeoff for MDPs with black-box advice, our work moves beyond the black-box setting. Importantly, by considering the grey-box setting, the design of PROP demonstrates that the _structural information_ of the untrusted machine-learned advice can be leveraged to determine the trust parameters dynamically, which would otherwise be challenging (if not impossible) in a black-box setting. To our best knowledge, PROP is the first-of-its-kind learning-augmented algorithm that applies to general MDP models, which allow continuous or discrete state and action spaces.

Our main results characterize the tradeoff between consistency and robustness for both black-box and grey-box settings in terms of the ratio of expectations, \(\), built upon the traditional consistency and robustness metrics in [3; 22; 23; 4] for the competitive ratio. We show in Theorem 5.2 that for the black-box setting, PROP is \((1+((1-)))\)-consistent and \((+())\)-robust where \(0 1\) is a hyper-parameter. Moreover, for the black-box setting, PROP cannot be both \((1+o())\)-consistent and \((+o((1-)))\)-robust for any \(0 1\) where \(\) is the diameter of the action space. In sharp contrast, by using a careful design of a robustness budget parameter in PROP with Q-value advice (grey-box setting), PROP is \(1\)-consistent and \((+o(1))\)-robust.

Our result highlights the benefits of exploiting the additional information informed by the estimated Q-value functions, showing that the ratio of expectations can approach the better of the two policies \(\) and \(\) for any single-trajectory time-varying, and even possibly adversarial environments -- if the value-based policy \(\) is near-optimal, then the worst-case \(()\) can approach \(1\) as governed by a consistency parameter; otherwise, \(()\) can be bounded by the ratio of expectations of \(\) subject to an additive term \(o(1)\) that decreases when the time horizon \(T\) increases.

A key technical contribution of our work is to provide the first quantitative characterization of the consistency and robustness tradeoff for a learning-augmented algorithm (PROP) in a general MDP model, under both standard black-box and novel grey-box settings. Importantly, PROP is able to leverage a broad class of robust policies, called _Wasserstein robust_ policies, which generalize the well-known contraction principles that are satisfied by various robust policies  and have been used to derive regrets for online control [19; 25]. A few concrete examples of Wasserstein robust policies applicable for PROP are provided in Table 1(Section 3.1).

### Related Work

**Learning-Augmented Algorithms with Black-Box Advice.** The concept of integrating black-box machine-learned guidance into online algorithms was initially introduced by .  coined terms "robustness" and "consistency" with formal mathematical definitions based on the competitive ratio. Over the past few years, the consistency and robustness approach has gained widespread popularity and has been utilized to design online algorithms with black-box advice for various applications, including ski rental [3; 22; 23], caching [27; 28; 29], bipartite matching , online covering [31; 32], convex body chasing , nonlinear quadratic control . The prior studies on learning-enhanced algorithms have mainly focused on creating meta-strategies that combine online algorithms with black-box predictions, and typically require manual setting of a trust hyper-parameter to balance consistency and robustness. A more recent learning-augmented algorithm in  investigated the balance between competitiveness and stability in nonlinear control in a black-box setting. However, this work limits the robust policy to a linear quadratic regulator and does not provide a theoretical basis for the selection of the trust parameters.  generalized the black-box advice setting by considering distributional advice.

**Online Control and Optimization with Structural Information.** Despite the lack of a systematic analysis, recent studies have explored the usage of structural information in online control and optimization problems. Closest to our work,  considered a related setting where the Q-value function is available as advice, and shows that such information can be utilized to reduce regret in a tabular MDP model. In contrast, our analysis applies to more general models that allow continuous state/action spaces. In , the dynamical model and the predictions of disturbances in a linear control system are shown to be useful in achieving a near-optimal consistency and robustness tradeoff. The predictive optimization problem solved by MPC [35; 36; 16; 37] can be regarded as a special realization of grey-box advice, where an approximated cost-to-go function is constructed from structural information that includes the (predicted) dynamical model, costs, and disturbances.

**MDP with External Feedback.** Feedback from external sources such as control baselines [38; 39], visual explanations , and human experts [41; 42; 43] is often available in MDP. This external feedback can be beneficial for various purposes, such as ensuring safety , reducing variance , training human-like chatbots , and enhancing overall trustworthiness , among others. The use of control priors has been proposed by  as a way to guarantee the Lyapunov stability of the training process in reinforcement learning. They used the Temporal-Difference method to tune a coefficient that combines a RL policy and a control prior, but without providing a theoretical foundation. Another related area is transfer learning in RL, where external Q-value advice from previous tasks can be adapted and utilized in new tasks. Previous research has shown that this approach can outperform an agnostic initialization of Q, but these results are solely based on empirical observations and lack theoretical support [46; 47; 48].

## 2 Problem Setting

We consider a finite-horizon, single-trajectory, time-varying MDP with \(T\) discrete time steps. The state space \(\) is a subset of a normed vector space embedded with a norm \(\|\|_{}\). The actions are chosen from a convex and compact set \(\) in a normed vector space characterized by some norm \(\|\|_{}\). Notably, \(\) can represent either continuous actions or the probability distributions used when choosing actions from a finite set.1 The diameter of the action space \(\) is denoted by \(_{u}\|u\|_{}\). Denote \([T]\{0,,T-1\}\). For each time step \(t[T]\), let \(P_{t}:_{}\) be the transition probability, where \(_{}\) is a set of probability measures on \(\). We consider time-varying costs \(c_{t}:_{+}\), while rewards can be treated similarly by adding a negative sign. An initial state \(x_{0}\) is fixed. This MDP model is compactly represented by \((,,T,P,c)\).

The goal of a policy in this MDP setting is to minimize the total cost over all \(T\) steps. The policy agent has no access to the full MDP. At each time step \(t[T]\), only the incurred cost value \(c_{t}(x_{t},u_{t})\) and the next state \(x_{t+1} P_{t}(|x_{t},u_{t})\) are revealed to the agent after playing an action \(u_{t}\). We denote a policy by \(=(_{t}:t[T])\) where each \(_{t}:\) chooses an action \(u_{t}\) when observing\(x_{t}\) at step \(t[T]\). Note that our results can be generalized to the setting when \(_{t}\) is stochastic and outputs a probability distribution on \(\). Given \((,,T,P,c)\), we consider an optimization with time-varying costs and transition dynamics. Thus, our goal is to find a policy \(\) that minimizes the following expected total cost:

\[J()_{P,}_{t[T]}c_{t}(x_{t},_{t }(x_{t}))\] (1)

where the randomness in \(_{P,}\) is from the transition dynamics \(P=(P_{t}:t[T])\) and the policy \(=(_{t}:t[T])\). We focus our analysis on the expected dynamic regret and the ratio of expectations, defined below, as the performance metrics for our policy design.

**Definition 1** (Expected dynamic regret).: _Given \((,,T,P,c)\), the (expected) dynamic regret of a policy \(=(_{t}:t[T])\) is defined as the difference between the expected cost induced by the policy \(\), \(J()\) in (1), and the optimal expected cost \(J^{}_{}J()\), i.e., \(() J()-J^{}\)._

Dynamic regret is a more general (and often more challenging to analyze) measure than classical static regret, which has been mostly used for stationary environments [49; 50]. The following definition of the ratio of expectations [51; 52] will be used as an alternative performance metric in our main results.

**Definition 2** (Ratio of expectations).: _Given \((,,T,P,c)\), the ratio of expectations of a policy \(=(_{t}:t[T])\) is defined as \(() J()/J^{}\) where \(J()\) and \(J^{}\) are the same as in Definition 1._

Dynamic regret and the ratio of expectations defined above also depend on the error of the untrusted ML advice; we make this more explicit in Section 3.2. Next, we state the following continuity assumption, which is standard in MDPs with continuous action and state spaces [53; 54; 55]. Note that our analysis can be readily adapted to general Holder continuous costs with minimal modifications.

**Assumption 1** (Lipschitz costs).: _For any time step \(t[T]\), the cost function \(c_{t}:_{+}\) is Lipschitz continuous with a Lipschitz constant \(L_{C}<\), i.e., for any \(t[T]\), \(|c_{t}(x,u)-c_{t}(x^{},u^{})| L_{C}(\|x-x^{}\|_{ }+\|u-u^{}\|_{})\). Moreover, \(0<c_{t}(x,u)<\) for all \(t[T]\), \(x\), and \(u\)._

## 3 Consistency and Robustness in MDPs

Our objective is to achieve a balance between the worst-case guarantees on cost minimization in terms of dynamic regret provided by a robust policy, \(\), and the average-case performance of a valued-based policy, \(\), in the context of \((,,T,P,c)\). In particular, we denote by \( 1\) a ratio of expectation bound of the robust policy \(\) such that the worst case \(()\). In the learning-augmented algorithms literature, these two goals are referred to as consistency and robustness [3; 1]. Informally, robustness refers to the goal of ensuring worst-case guarantees on cost minimization comparable to those provided by \(\) and consistency refers to ensuring performance nearly as good as \(\) when \(\) performs well (e.g., when the instance is not adversarial). Learning-augmented algorithms seek to achieve consistency and robustness by combining \(\) and \(\), as illustrated in Figure 1.

Figure 1: _Left_: Overview of settings in our problem. _Right_: consistency and robustness tradeoff, with \(\) and \(\) defined in Definition 2 and Equation (4).

Our focus in this work is to design robust and consistent algorithms for two types of advice: black-box advice and grey-box advice. The type of advice that is nearly always the focus in the learning-augmented algorithm literature is black-box advice -- only providing a suggested action \(_{t}\) without additional information. In contrast, on top of the action \(_{t}\), grey-box advice can also reveal the internal state of the learning algorithm, e.g., the Q-value \(_{t}\) in our setting. This contrast is illustrated in Figure 1.

Compared to black-box advice, grey-box advice has received much less attention in the literature, despite its potential to improve tradeoffs between consistency and robustness as recently shown in . Nonetheless, the extra information on top of the suggested action in a grey-box setting potentially allows the learning-augmented algorithm to make a better-informed decision based on the advice, thus achieving a better tradeoff between consistency and robustness than otherwise possible.

In the remainder of this section, we discuss the robustness properties for the algorithms we consider in our learning-augmented framework (Section 3.1), and introduce the notions of consistency in our grey-box and black-box models in Section 3.2.

### Locally Wasserstein-Robust Policies

We begin with constructing a novel notion of robustness for our learning-augmented framework based on the Wasserstein distance as follows. Denote the robust policy by \(:=(_{t}:t[T])\), where each \(_{t}\) maps a system state to a deterministic action (or a probability of actions in the stochastic setting). Denote by \(_{t_{1}:t_{2}}()\) the joint distribution of the state-action pair \((x_{t},u_{t})\) at time \(t_{2}[T]\) when implementing the baselines \(_{t_{1}},,_{t_{2}}\) consecutively with an initial state-action distribution \(\). We use \(\|\|_{}\|\|_{}+\| \|_{}\) as the included norm for the product space \(\). Let \(W_{p}(,)\) denote the Wasserstein \(p\)-distance between distributions \(\) and \(\) whose support set is \(\):

\[W_{p}(,)(_{J(,)}\|(x,u)-(x^{ },u^{})\|_{}^{p}J((x,u),(x^{},u^{})))^{1/p}\]

where \(p[1,)\) and \((,)\) denotes a set of all joint distributions \(J\) with a support set \(\) that have marginals \(\) and \(\). Next, we define a robustness condition for our learning-augmented framework.

**Definition 3** (\(r\)-locally \(p\)-Wasserstein robustness).: _A policy \(=(_{t}:t[T])\) is \(r\)**-locally \(p\)-Wasserstein-robust** if for any \(0 t_{1} t_{2}<T\) and any pair of state-action distributions \(,^{}\) where the the \(p\)-Wasserstein distance between them is bounded by \(W_{p}(,^{}) r\), for some radius \(r>0\), the following inequality holds:_

\[W_{p}(_{t_{1}:t_{2}}(),_{t_{1}:t_{2}}(^{}))  s(t_{2}-t_{1})W_{p}(,^{})\] (2)

_for some function \(s:[T]_{+}\) satisfying \(_{t[T]}s(t) C_{s}\) where \(C_{s}>0\) is a constant._

Our robustness definition is naturally more relaxed than the usual contraction property in the control/optimization literature  -- if any two different state-action distributions converge exponentially with respect to the Wasserstein \(p\)-distance, then a policy \(\) is \(r\)_-locally \(p\)-Wasserstein-robust_. This is illustrated in Figure 2. Note that, although the Wasserstein robustness in Definition 3 well captures a variety of distributional robustness metrics such as the total variation robustness defined on finite state/action spaces, it can also be further generalized to other metrics for probability distributions.

As shown in Appendix A (provided in the supplementary material), by establishing a connection between the Wasserstein distance and the total variation metric, any policy that induces a regular

   Model & **Robust Baseline**\(\) & RoE \\   Time-varying MDP (Our General Model) & Wasserstein Robust Policy (Definition 3) & ROB \\  Discrete MDP (Appendix A.2) & Any Policy that Induced a Regular Markov Chain & â€” \\ Time-Varying LQR (Appendix A.1) & MPC with Robust Predictions (Algorithm 3) & \((1)\) \\   

Table 1: Examples of models covered in this paper and the associated control baselines. For the right column, bounds on the ratio of expectations RoE are exemplified, where ROB is defined in Section 3 and \(\) omits inessential constants.

Markov chain satisfies the fast mixing property and the state-action distribution will converge with respect to the total variation distance to a stationary distribution . A more detailed discussion can be found in Appendix A.2. Moreover, the Wasserstein-robustness in Definition 3 includes a set of contraction properties in control theory as special cases. For example, for a locally Wasserstein-robust policy, if the transition kernel \(P\) and the baseline policy \(\) are deterministic, then the state-action distributions become point masses, reducing Definition 3 to a state-action perturbation bound in terms of the \(_{2}\)-norm when implementing the policy \(\) from different starting states [35; 19].

The connections discussed above highlight the existence of several well-known robust policies that satisfy Definition 3. Besides the case of discrete MDPs discussed in Appendix A.2, another prominent example is model predictive control (MPC), for which robustness follows from the results in  (see Appendix A.1 for details). The model assumption below will be useful in our main results.

**Assumption 2**.: _There exists a \(\)-locally \(p\)-Wasserstein-robust baseline control policy (Definition 3) \(\) for some \(p 1\), where \(\) is the diameter of the action space \(\)._

### Consistency and Robustness for RoE

In parallel with the notation of "consistency and robustness" in the existing literature on learning-augmented algorithms [3; 1], we define a new metric of consistency and robustness in terms of RoE. To do so, we first introduce an optimal policy \(^{*}\). Based on \((,,T,P,c)\), let \(^{*}_{t}=(^{*}_{t}:t[T])\) denote the optimal policy at each time step \(t[T]\), whose optimal Q-value function is

\[Q^{*}_{t}(x,u)_{}_{P,}[_{=t}^{T-1}c _{}(x_{},u_{})x_{t}=x,u_{t}=u],\]

where \(_{P,}\) denotes an expectation with respect to the randomness of the trajectory \(\{(x_{t},u_{t}):t[T]\}\) obtained by following a policy \(\) and the transition probability \(P\) at each step \(t[T]\). The Bellman optimality equations can then be expressed as

\[Q^{*}_{t}(x,u)= (c_{t}+_{t}V^{*}_{t+1})(x,u), V^{*}_{t}(x)=_{v}Q^{*}_{t}(x,v), V^{*}_{t}(x)=0\] (3)

for all \((x,u)\), \(t[T]\) and \(t[T]\), where we write \((_{t}V^{*})(x,u)_{x^{} P_{t }(|x,u)}[V^{*}(x^{})]\). This indicates that for each time step \(t[T]\), \(^{*}_{t}\) is the greedy policy with respect to its optimal Q-value functions \((Q^{*}_{t}:t[T])\). Note that for any \(t[T]\), \(Q^{*}_{t}(x,u)=0\). Given this setup, the value-based policies \((_{t}:t[T])\) take the following form. For any \(t[T]\), a value-based policy \(_{t}:\) produces an action \(_{t}_{v}_{t}(x_{t},v)\) by minimizing an estimate of the optimal Q-value function \(Q_{t}\).

We make the following assumption on the machine-learned untrusted policy \(\) and the Q-value advice.

**Assumption 3**.: _The machine-learned untrusted policy \(\) is value-based. The Q-value advice \(_{t}:\) is Lipschitz continuous with respect to \(u\) for any \(x\), with a Lipschitz constant \(L_{Q}\) for all \(t[T]\). Moreover, \(_{t}(x,u)-Q^{}_{t}(x,u)=o(T)\) for all \((x,u)\) and \(t[T]\)._

We can now define a consistency measure for Q-value advice \(_{t}\), which measures the error of the estimates of the Q-value functions due to approximation error and time-varying environments, etc. Let \(p(0,]\). Fix a sequence of distributions \(=(_{t}:t[T])\) whose support set is \(\) and let \(_{t}\) be the marginal distribution of \(_{t}\) on \(\). We define a quantity representing the error of the Q-value

Figure 2: An illustration of an _\(r\)-locally \(p\)-Wasserstein-robust_ policy.

advice

\[(p,)_{t[T]}(\|_{t}-Q_{t}^{ }\|_{p,_{t}}+_{v}_{t}- _{v}Q_{t}^{}\|_{p,_{t}})\] (4)

where \(\|\|_{p,}(||^{p}\,)^{1/p}\) denotes the \(L_{p,}\)-norm. A policy with Q-value functions \(\{Q_{t}:t[T]\}\) is said to be \((,p,)\)_-consistent_ if there exists an \(\) satisfying (4). In addition, a policy is \((0,)\)-consistent if \(_{t}\) is a Lebesgue-measurable function for all \(t[T]\) and \((,)\)-consistent if the \(L_{}\)-norm satisfies \(_{t[T]}\|_{t}-Q_{t}^{}\|_{}\). The consistency error of a policy in (4) quantifies how the Q-value advice is close to optimal Q-value functions. It depends on various factors such the function approximation error or training error due to the distribution shift, and has a close connection to a rich literature on value function approximation [57; 58; 59; 60; 61]. The results in  generalized the worst-case \(L_{}\) guarantees to arbitrary \(L_{p,}\)-norms under some mixing assumptions via policy iteration for a stationary Markov decision process (MDP) with a continuous state space and a discrete action space. Recently, approximation guarantees for the average case for parametric policy classes (such as a neural network) of value functions have started to appear [57; 58; 60]. These bounds are useful in lots of supervised machine learning methods such as classification and regression, whose bounds are typically given on the expected error under some distribution. These results exemplify richer instances of the consistency definition (see (4)) and a summary of these bounds can be found in .

Now, we are ready to introduce our definition of consistency and robustness with respect to the ratio of expectations, similar to the growing literature on learning-augmented algorithms [3; 22; 23; 4]. We write the ratio of expectations \(()\) of a policy \(\) as a function of the Q-value advice error \(\) in terms of the \(L_{}\) norm, defined in (4).

**Definition 4** (Consistency and Robustness).: _An algorithm \(\) is said to be \(k\)**-consistent** if its worst-case (with respect to the MDP model \((,,T,P,c)\)) ratio of expectations satisfies \(() k\) for \(=0\). On the other hand, it is \(l\)**-robust** if \(() l\) for any \(>0\)._

## 4 The Projection Pursuit Policy (PROP)

In this section we introduce our proposed algorithm (Algorithm 1), which achieves near-optimal consistency while bounding the robustness by leveraging a robust baseline (Section 3.1) in combination with value-based advice (Section 3.2). A key challenge in the design is how to exploit the benefits of good value-based advice while avoiding following it too closely when it performs poorly. To address this challenge, we propose to judiciously project the value-based advice into a neighborhood of the robust baseline. By doing so, the actions we choose can follow the value-based advice for consistency while staying close to the robust baseline for robustness. More specifically, at each step \(t[T]\), we choose \(u_{t}=_{}_{t}}(_{t})\) where a projection operator \(_{}_{t}}()\) is defined as

\[_{}_{t}}(u)*{arg\,min} _{v}\|u-v\|_{}\|v-_{t}(x_{t})\|_{} R_{t},\] (5)

corresponding to the projection of \(u\) onto a ball \(}_{t}\{u:\|u-_{t}(x_{t})\|_{} R_{t}\}\). Note that when the optimal solution of (5) is not unique, we choose the one on the same line with \(_{t}(x_{t})-u\).

The PROjection Pursuit policy, abbreviated as PROP, can be described as follows. For a time step \(t[T]\), let \(_{t}:\) denote a policy that chooses an action \(_{t}\) (abitrarily choose one if there are multiple minimizers of \(_{t}\)), given the current system state \(x_{t}\) at time \(t[T]\) and step \(t[T]\). An action \(u_{t}=_{}_{t}}(_{t}(x_{t}))\) is selected by projecting the machine-learned action \(_{t}(x_{t})\) onto a norm ball \(}_{t}\) defined by the robust policy \(\) given a radius \(R_{t} 0\). Finally, PROP applies to both black-box and grey-box settings (which differ from each other in terms of how the radius \(R_{t}\) is decided). The results under both settings are provided in Section 5, revealing a tradeoff between consistency and robustness.

The radii \((R_{t}:t[T])\) can be interpreted as _robustness budgets_ and are key design parameters that determine the consistency and robustness tradeoff. Intuitively, the robustness budgets reflect the trustworthiness on the value-based policy \(\) -- the larger budgets, the more trustworthiness and hence the more freedom for PROP to follow \(\). How the robustness budget is chosen differentiates the grey-box setting from the black-box one.

[MISSING_PAGE_FAIL:8]

## 5 Main Results

We now formally present the main results for both the black-box and grey-box settings. Our results not only quantify the tradeoffs between consistency and robustness formally stated in Definition 4 with respect to the ratio of expectations, but also emphasize a crucial role that additional information about the estimated Q-values plays toward improving the consistency and robustness tradeoff.

### Black-Box Setting

In the existing learning-augmented algorithms, the untrusted machine-learned policy \(\) is often treated as a black-box that generates action advice \(_{t}\) at each time \(t[T]\). Our first result is the following general dynamic regret bound for the black-box setting (Section 4.1). We utilize the Big-O notation, denoted as \(()\) and \(o()\) to disregard inessential constants.

**Theorem 5.1**.: _Suppose the machine-learned policy \(\) is \((,)\)-consistent. For any MDP model satisfying Assumption 1,2, and 3, the expected dynamic regret of PROP with the Black-Box Procedure is bounded by \(()\{()+((1- ) T),((+-1)\,T)\}\) where \(\) is defined in (4), \(\) is the diameter of the action space \(\), \(T\) is the length of the time horizon, \(\) is the ratio of expectations of the robust baseline \(\), and \(0 1\) is a hyper-parameter._

When \(\) increases, the actual action can deviate more from the robust policy, making the dynamic regret potentially closer to that of the value-based policy. While the regret bound in Theorem 5.1 clearly shows the role of \(\) in terms of controlling how closely we follow the robust policy, the dynamic regret given a fixed \(\) grows linearly in \((T)\). In fact, the linear growth of dynamic regret holds even if the black-box policy \(\) is consistent, i.e., \(\) is small. This can be explained by noting the lack of dynamically tuning \(\) to follow the better of the two policies -- even when one policy is nearly perfect, the actual action still always deviates from it due to the fixed choice of \(\).

Consider any MDP model satisfying Assumptions 1,2, and 3. Following the classic definitions of consistency and robustness (see Definition 4), we summarize the following characterization of PROP, together with a negative result in Theorem 5.3. Proofs of Theorem 5.1, 5.2, and 5.3 are detailed in Appendix C.

**Theorem 5.2** (Black-Box Consistency and Robustness).: PROP _with the Black-Box Procedure is \((1+((1-)))\)-consistent and \((+())\)-robust where \(0 1\) is a hyper-parameter._

**Theorem 5.3** (Black-Box Impossibility).: PROP _with the Black-Box Procedure cannot be both \((1+o((1-)))\)-consistent and \((+o())\)-robust for any \(0 1\)._

### Grey-Box Setting

To overcome the impossibility result in the black-box setting, we dynamically tune the robustness budgets by tapping into additional information informed by the estimated Q-value functions using the Grey-Box Procedure (Section 4.2). By setting the robustness budgets in (7), an analogous result of Theorem 5.1 is given in Appendix D, which leads to a dynamic regret bound of PROP in the grey-box setting (Theorem D.1 in Appendix D). Consider any MDP model satisfying Assumptions 1,2, and 3. Our main result below indicates that knowing more structural information about a black-box policy can indeed bring additional benefits in terms of the consistency and robustness tradeoff, even if the black-box policy is untrusted.

**Theorem 5.4** (Grey-Box Consistency and Robustness).: PROP _with the Grey-Box Procedure is \(1\)-consistent and \((+o(1))\)-robust for some \(>0\)._

Theorem 5.3 implies that using the Black-Box Procedure, PROP cannot be \(1\)-consistent and \((+o(1))\)-robust, while this can be achieved using the Grey-Box Procedure. On one hand, this theorem validates the effectiveness of the PROP policy with value-based machine-learned advice thatmay not be fully trusted. On the other hand, this sharp contrast between the black-box and grey-box settings reveals that having access to information of value function can improve the tradeoff between consistency and robustness (see Definition 4) non-trivially. A proof of Theorem 5.4 can be found in Appendix D. Applications of our main results are discussed in Appendix A.

## 6 Concluding Remarks

Our results contribute to the growing body of literature on learning-augmented algorithms for MDPs and highlight the importance of considering consistency and robustness in this context. In particular, we have shown that by utilizing the _structural information_ of machine learning methods, it is possible to achieve improved performance over a black-box approach. The results demonstrate the potential benefits of utilizing value-based policies as advice; however, there remains room for future work in exploring other forms of structural information.

**Limitations and Future Work.** One limitation of our current work is the lack of analysis of more general forms of black-box procedures. Understanding and quantifying the available structural information in a more systematic way is another future direction that could lead to advances in the design of learning-augmented online algorithms and their applications in various domains.