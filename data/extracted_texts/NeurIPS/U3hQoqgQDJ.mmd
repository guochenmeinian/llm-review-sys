# Context for Image

[MISSING_PAGE_FAIL:1]

## 1 Introduction

With the exhilarating progress in foundation models across the vision and language domains, such as GPT4(V) [(30)], DALLE-3 [(31)], SAM [(19)], and LLaMA [(38)], _etc._, we have reached a stage where deep learning models achieve remarkable performances on both vision and language domains [(5; 22)]. Specifically, models like GPT-4(V) [(30)] have showcased human-level perception and reasoning skills [(46)].

Despite their impressive capabilities in information memorization, processing, and reasoning, these models tend to be specialized for specific output types. However, their output types are limited to language for GPT, images for DALLE, masks for SAM, _etc._ In this work, we aim to leverage the privileged properties of foundation models' embeddings to expand their output space (e.g., extend to pixel-level outputs), unlocking their potential for interleaved understanding and reasoning.

To accomplish this, we introduce an INterface for Foundation models' embeDdings (FIND), which utilizes the pre-trained foundational model embeddings to jointly handle downstream tasks of varying granularities (from pixel to image) in an interleaved manner. As illustrated in Fig.2.1, the _FIND_ interface processes embeddings from vision and language foundation models, and outputs segmentation, grounding, and retrieval results.

As all vision-language tasks are trained uniformly in _FIND_, an interleaved shared embedding space is created where vision and language references can be interchanged and augmented. For example, in Fig.2.2, during mapping an interleaved representation loosens the single-modality constraint on the source and target domain. And during reasoning, interleaved sequences enhance information exchange between vision and language compared to multimodal sequences.

To effectively align and evaluate the interleaved embedding space, we construct a new dataset named FIND-Bench. This dataset uses COCO images and includes new annotations for integrated grounding and segmentation. These annotations are generated by GPT-4, which, despite not processing visual input, can directly link specific image segments and annotation IDs with generated descriptions (e.g., <id>(the golden retriever)...). This unique capability enables the creation of training and evaluation datasets for retrieval and grounding in an interleaved context.

In summary, we claim the following contributions:

* We introduce the _FIND_ interface that is is generalizable, flexible, and extendable to various downstream tasks and foundation models.
* Through the effective training scheme of _FIND_, an interleaved shared embedding space is created interfacing foundation models.
* We propose a new Benchmark, _FIND_-Bench, which includes new training and evaluation ground truths for interleave segmentation and retrieval.
* Our model achieves SoTA performance on interleave retrieval and grounding and shows better or comparable performance on generic, interactive, grounded segmentation and image-text retrieval.

## 2 Related Work

**Foundation Models.** Recent years have seen a speedy evolution of foundation models in diverse areas such as computer vision [(47)], natural language processing [(39; 10; 4; 30)], and their interactions [(1;

Figure 2: (1) The concept of interfacing foundation models embedding, the black arrow means active attached modules and the gray arrow means the option that it can switch to. On the right, we show the difference of Multimodal and Interleave (2.a) in the context of embeddings matching; (2.b) in the context of embeddings interaction for reasoning and generation.

23; 44). For example, GPT-3 (4) heralds breakthroughs in natural language understanding and generation tasks, As a vision foundation model, Florence [47; 42] can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, etc.Flamingo (1) bridges powerful pre-trained vision-only and language-only models by token fusion with cross-attention. BLIP-2 (23) proposes an efficient pretraining strategy that bootstraps vision-language pre-training with a lightweight Q-Former in two stages. Different from previous multi-modal approaches, such as Flamingo (1), LLaVA (26) and Q-Former (BLIP-2) (23) that feed the vision foundation model output into a language decoder and use the LLM as an interpreter, our goal is to interface foundation model embeddings so that LLMs and vision models can be unified in the embedding space.

**Interleaved Image-Text Understanding.** Previous works have explored interleaved visual understanding in the context of visual question answering, visual dialogue, image captioning, and interleaved image retrieval (20; 13; 1). In addition, recent works (48) explore contextual detection that associates phrases with visual content in a sentence. We notice that these earlier works, though reveal interleaved capabilities for image understanding, lack an evaluation benchmark, as well as a complete training dataset. [(51; 21; 2)] propose a new benchmark on interleaved generation and understanding of image and document level, while there is no benchmark available for the interleaved tasks between interactive image parts and phrases. To this end, we introduce the interleaved segmentation and interleaved retrieval tasks with our carefully designed benchmark _FIND_-Bench, which we believe to be essential for the field.

**Image Understanding.** Vision Transformers [(16; 37; 40; 36; 41; 12; 15; 49; 33; 34)] have dominated a wide range of key image understanding tasks, such as image retrieval, detection, and segmentation. Some multimodal methods [(7; 24; 50)] have shown good performance for retrieval tasks. On the other hand, open-vocabulary segmentation methods have recently drawn much attention, including generic segmentation [(6; 53; 11)], interactive segmentation [(14; 19)] that separates objects by actively integrating user inputs, and grounded segmentation [(53; 52)] that grounds object segments from language descriptions. We notice that there is currently no available work that achieves image-level retrieval, pixel-level segmentation, and interleaved vision-language understanding in a single model. In this work, we propose _FIND_ as a unified interface that can support all the above tasks, while maintaining good performance, and further enabling two new tasks of interleaved segmentation and interleaved retrieval. We unify these tasks by interfacing foundation models' embeddings.

## 3 Method

Foundation models such as CLIP (32), SAM (19), LLaMA (38), etc. can process vision or language inputs for reasoning, understanding, and generation. The embeddings generated by these models contain rich and structured information [(35; 3)], making them extremely well-suited for understanding tasks. Aligned with the Platonic Representation Hypothesis (17), we believe foundation models can easily communicate with each other. Therefore, we designed the FIND interface to project vision and language embeddings from foundation models into a unified space. The created space enhances both multimodal and interleaved understanding.

Since no prior benchmark exists for interleave understanding, we believe it is meaningful to formally define the interleave retrieval and segmentation problems and create a dataset for benchmarking them.

### _Find_ Benchmark

Our new benchmark supports two tasks: interleave retrieval and interleave grounding. It evaluates both dataset-level and image-level interleave alignment, focusing on reasoning and matching capabilities. Additionally, we created training and evaluation datasets to further enhance interleave understanding.

#### 3.1.1 Task Definition

**Interleave Retrieval 1.** An interleave entry (\(E\)) consists of a sequence of images (I), texts (T), and connections (C), and can be represented as \(E= N_{1},N_{2},,N_{n} N_{i}\{I,T,C\}\), where \(\) is an ordered sequence. The bottom part of the Table. 1 clearly illustrates an example of an interleave entry. We denote the source domain (\(_{s}\)) of interleave retrieval as \(_{s}=\{E_{1},E_{2},,E_{n}\}\), as shown in Fig. 3.1 (Left), and the target domain (\(_{t}\)) as \(_{t}=\{I_{1},I_{2},,I_{n}\}\), as shown in Fig. 3.1 (Right). The task of interleave retrieval is to find the closest entry \(I_{*}_{t}\) for each \(E_{s}\), excluding itself. Formally, we define this as \( E_{s}, I_{*}=_{I_{t},\,I E} (E,I)\).1. **GT**: Ground Truth image caption labeled by human.
2. **PD**: Pseudo image Description generated by VLM model.
3. **Box**: All Ground truth bounding box labeled by human.
4. **SI**: Segment Information for each box area including index, bbox, category, descriptions, etc.
5. **SP**: Segment Proposal for the generated description.

**Prompt for GPT4 Engine**

":Generate image captions with grounded entities and attributes with the following information:

ground truth image captions: <[ ]>,

pseudo image description: <[ ]>,

ground truth bounding boxes (\(\{x_{0},y_{0},w,h\}\): \((x_{0},y_{0})\) is the top-left corner; \((w,h)\) is box size);

segment_info: <[ ] >, and segment_proposal: <[ ]>.

An example output format would be: _"[index]<A woman> sitting next to [index]<a handsome man>, with their hands holding together under [index]<the blue sky>."_, where _[index]_ and _<xxxx>_ are associated with the ground truth bounding boxes.

Generated caption constraints: _(1-6) Please refer to appendix.""_.format(**GT**, **PD**, **Box**, **SI**, **SP**)

**Retrieve Visual Sample with SEEM**

Given the search dataset (**Q**) with the segments in all images denoted as (**SD**), we compute all embeddings **S** representing each segment using SEEM (53) with \(=()^{n d}\).

Given the similarity matrix \(W=^{}\), where \(W_{ij}\) represents the similarity between segment \(i\) and segment \(j\), the index of the closest segment for segment \(i\) is \((i)=_{j i}W_{i}\) where \((i)\) returns the index \(j\) that has the highest similarity to segment \(i\).

**Integrated Response of GPT4 and SEEM**

 
**Interleave Grounding**2. An image contains a sequence of objects or segments (\(O\)) represented as \(I=\{O_{1},O_{2},,O_{n}\}\). We provide an example of objects in the bakery image in Fig. 3.2 upper part. These objects form the target domain \(_{t}=I=\{O_{1},O_{2},,O_{n}\}\) for interleave grounding. Unlike interleave retrieval, where interleave entries constitute the source domain, interleave grounding focuses on each component of the interleave entry, with the entities (\(N\)) in the interleave entry forming the source domain. Specifically, \(_{s}=\{N_{1},N_{2},,N_{n} N_{i}\{I,T\}\} E\). We show an example of interleave entry decomposition in the lower part of Fig. 3.2. The task of interleave grounding is to find the closest entry \(O_{*}_{t}\) for each \(N_{s}\), excluding itself. Formally, we define this as \( N_{s}, O_{*}=_{O_{t},O N }(N,O)\). \\  

Table 1: Pseudo code for Data Engine. We show the pipeline to create the _FIND_-Bench from data preparation, text prompting using GPT4, visual prompting with SEEM to integrated result.

associated with the COCO annotation ID  and a similar playing field (marked in blue) in another image. In this way, the data engine can generate comprehensive interleaved descriptions for each image in the COCO dataset. This is sufficient to build \(_{s}\) and \(_{t}\) for the interleave retrieval and grounding tasks introduced in Sec. 3.1.1.

### _Find_ Approach

With benchmarks introduced in Sec. 3.1 to evaluate the model's interleaved visual understanding capability, we now present our approach for interfacing foundation models' embeddings on multimodal and interleave understanding. We begin with the preliminaries on task unification and terminology.

#### 3.2.1 Preliminary

**Task Unification.** In this work, we focus on retrieval, grounding, and segmentation in both multimodal and interleaved manners. In Fig. 3, we demonstrate four example tasks: interleave retrieval, interleave grounding, interactive segmentation, and generic segmentation. From an abstract perspective, we can regard all visual understanding tasks as the problem of matching candidates from the source domain to the target domain. Formally, we define the source domain as \(_{s}\) and the target domain as \(_{t}\). Example elements in \(_{s}\) or \(_{t}\) includes interleaved entry \(E\), an image \(I\), an object or segment \(O\), texts \(T\). For each visual understanding task \((_{s},_{t})\), the goal is to find the closest \(Y_{t}\) for each \(X_{s}\). Formally we write:

\[ X_{s}, Y^{*}=_{Y_{t}}(X,Y)\]

where \(\), and \(\) are base element of \(_{s}\), and \(_{t}\) respectively, and \((X,Y)\) denotes the similarity between \(X\) and \(Y\). For example, in generic segmentation (Fig. 3.4), \(_{s}\) is the set of all objects (segments) in the image: \(_{s}=\{O_{1},,O_{n_{s}}\}\), and \(_{t}\) is the set of category names: \(_{t}=\{T_{1},,T_{n}\}\). For each object \(\) in \(_{s}\), we will find the corresponding category \(_{t}\).

**Terminology.** Here we will introduce important model terminology, including prompts (\(P\)) and queries (\(Q\)). Our model supports three kinds of inputs: vision (I), language (T), and interleaved vision-language (E). The vision and language foundation models predict the embeddings for those inputs. As shown in Fig. 4.1, by sampling the embeddings, we obtain vision prompts (\(P_{I}\)), language prompts (\(P_{T}\)), and interleave prompts (\(P_{E}\)). Additionally, trainable queries initialized with random parameters will accumulate information from the prompts. For example, in generic segmentation, object queries (\(Q_{O}\)) gather information from visual prompts. Interestingly, queries just act like "buckets" accumulating "water" (prompts) in the _FIND_ interface, as shown in Fig. 4.1.

#### 3.2.2 Model Pipeline

Our model is designed to interface with a pair of arbitrary vision and language foundation models. **Prompts and Queries Preparation.** Given image (I), text (T), and interleave (E) inputs, the vision encoder (\(_{v}\)) and language encoder (\(_{l}\)) will encode these inputs to sequences of embeddings \(M\):

\[M_{I}=_{v}(I),\ \ M_{T}=_{l}(T),\ \ M_{E}=\{_{v}, _{l}\}(E)\] (1)

where, \(M^{n d}\), and \(n,d\) is the embedding number and dimension respectively. Similar to SEEM (53), we use an embedding sampler to sample customized prompts for downstream tasks. Example sampling strategies include downsampling, ROI pooling for the region, and rearrangement of embeddings for interleave prompt. The sampling procedure does not alter the embedding distribution. After sampling, we obtain \(\{P_{E},P_{T},P_{I},\}=(M_{I},M_{T},M_{E})\). Additionally, the embedding sampler is responsible for sampling queries (\(\{Q_{E},Q_{T},Q_{I},\}\)) from the pool of learnable queries. We allow duplication in the sampling procedure of learnable queries. These queries

Figure 3: Task Unification for retrieval, grounding, and segmentation. The corresponding components are labeled with the same color or connected with a line or arrow.

and prompts are the inputs of _FIND_ interface. Technically, the embedding sampler is usually an interpolation or grid sample layer in PyTorch.

_FIND Interface._ The _FIND_ interface primarily consists of two operations: content attention \(_{t}\) and conditional attention \(_{d}\), as shown in Fig. 4.3. Content attention allows queries to accumulate information from the corresponding prompts, while conditional attention enables prompts and queries to reason internally (e.g. self-attention on object queries to avoid duplication). With initial prompts \(^{0}=\{P_{E}^{0},P_{T}^{0},P_{I}^{0},\}\), and initial learnable queries \(^{0}=\{Q_{E}^{0},Q_{T}^{0},Q_{I}^{0},\}\), content attention and conditional attention are formally defined as:

\[^{l+1}=_{t}(^{l},^{l};[^{l} ^{l}]),\ \ \ ^{l+1},^{l+1}=_{d}(^{l},^{l};[ ^{l}^{l}],[^{l} ^{l}])\] (2)

where \(^{l}\{^{l},^{l}\}\) is a subset of queries and prompts, \(\) represents the attention mask. For example, \([]\) means that \(\) is able to attend \(\) during the attention. In this way, prompts act as the information source, and queries act as the bucket. In Fig. 4.2, we unfold the prompts and queries for some tasks supported by _FIND_ interface.

**Projection** The outputs of the _FIND_ interface are a sequence of queries: \(^{L}=\{Q_{E}^{L},Q_{T}^{L},Q_{I}^{L},Q_{E}^{L},\}\). We then project the queries using linear layers, \(_{s}\) and \(_{p}\), for semantic and pixel projection, respectively. The semantic and pixel queries are computed as \(Q^{s}=_{s}(^{L})^{n_{t} d}\) and \(Q^{p}=_{p}(^{L})^{n_{t} d}\), where \(n_{t}\) is the total instance number, and \(d\) is the embedding dimension. The semantic outputs are used for retrieval, category mapping, etc., while the pixel outputs are used for mask prediction.

**Task Head** With the projected queries, as illustrated Sec. 3.2.1 each understanding task can be represented as a similarity mapping procedure. Formally, segmentation result (**Mask**) can be computed given initial image embedding \(M_{I}^{n_{p} d}\), where \(n_{p}\) is the pixel number. The similarity scores (**Score**) can be computed directly from \(Q^{s}\). The outputs for each task is a subset of \(\{\}\).

\[=Q^{p} M_{I}^{}^{n_{t} n_{p}},\ \ \ =Q^{s} Q^{s}^{n_{t} n_{t}}\] (3)

**Loss _FIND_ is trained with a linear combination of losses for panoptic segmentation, grounded segmentation, interactive segmentation, image-text retrieval, interleave retrieval with visual entities from the same image, and interleave grounding. We demonstrate the loss details in the Appendix.

## 4 Experiments

**Datasets.** We use COCO (25) as our main training and evaluation dataset, which spans diverse annotation types. We make use of the annotations from COCO-panoptic, Ref-COCO (45; 28; 29), COCO-Karpathy (18), and the new datasets generated with the data engine in _FIND_-Bench. We generate two sets of new annotations, including COCO-Entity and COCO-Paragraph, the detailed statistics are shown in the table below:

   &  &  &  &  \\  & Images & Captions & Entire & Images & Captions & Entire & Mask & Private & Visual & Entire/Image \\  COCO-Entity & 118189 & 353219 & 1104907 & 4950 & 4950 & 13305 & & & & 4 \\ COCO-Paragraph & - & - & - & 4981 & 4981 & 22569 & ✓ & ✓ & ✓ & ✓ & 7 \\  

**Settings.** We benchmark our method on three different model sizes: Tiny (FocalNet), Base (Davit-d3), and Large (Davit-d3). The vision backbone is fixed and reuses the X-Decoder pre-trained weights

Figure 4: (a) Preliminaries on the terminology of prompts and queries. (b) _FIND_ approach pipeline. The shape of different polygons represents different embedding types, and the color (vision, language) of the polygons represents input modality. (c) Detailed architecture of the _FIND_ Interface.

unless specified as SAM. The language backbone is a fixed LLaMA-7B, unless specified as UniCL. During training, we train the FIND-Interface jointly on all the tasks unless specified.

**Metrics.** We evaluate all the tasks with their standard evaluation metrics. For the newly proposed interleave retrieval, we use IR@5 and IR@10 (Interleave-to-image Retrieval accuracy at rank 5/10). For interleave grounding, we evaluate based on cIoU (pixel-wise IoU), and mIoU (image-wise IoU) between the predicted interleave masks and the ground truth masks.

**Baselines.** We use ImageBind (13), FROMAGe (20), BLIP2 (23) as baselines for the interleave retrieval task; Grounding-SAM (27), SEEM (53) for interleave grounding. We claim to make every effort to design the baseline evaluation protocol to achieve the best possible performance.

### Main Results

In the main experiments, we focus on evaluating _FIND_ on Generalizable, Interleavable, and Extendable capabilities as claimed in the abstract.

**(1)** Generalizable to **Segmentation, Grounding, and Retrieval.** Table 2 compares _FIND_ with strong baselines on generic segmentation tasks including panoptic segmentation, instance segmentation, and semantic segmentation. In addition, we demonstrate the segmentation capability in both referring segmentation (RefCOCO-g: one sentence is associated with one instance) and grounded segmentation (COCO-Entity and COCO-Paragraph: one sentence is associated with multiple instances) settings. Moreover, we also benchmark _FIND_'s performance in image-text retrieval on three different ground truth types on COCO, where the average sentence length for the splits (Karpathy, Entity, and Paragraph) gradually increases. Below are the takeaways:

_The instance segmentation result stands out:_ Our approach with a large vision encoder outperforms similar models like Mask2Former, X-Decoder, and SEEM, achieving a performance 2.2 points higher than Mask2Former (L), which additionally uses deformable convolution. Notably, the segmentation training data is identical for both Mask2Former and _FIND_. The performance gain likely results from our unified segmentation and grounding pipeline, which mutually benefits from the semantic ground truth of each domain.

_Mutual benefits of grounded and referring segmentation:_ In _FIND_, we unify grounded and referring segmentation using queries and prompts. As shown in Table 2, our model achieves state-of-the-art performance on COCO-Entity and COCO-Paragraph and outperforms strong baselines on the Ref-COCOg dataset.

_Interactive segmentation performance is preserved in the unified settings._ Unlike SEEM which is only trained on image-only tasks, _FIND_ is trained also on image-text tasks, such as image-text retrieval. With the smart design of queries, prompts, and attention mechanisms, training interactive segmentation and image-text retrieval does not interfere. Thus, it enables our approach to achieve competitive performances (i.e. _FIND_ 88.5/89.5/77.4 vs. SEEM 88.5/89.6/76.5).

_Less optimal image-text retrieval results:_ The sub-optimal performance in image-text retrieval is due to batch size during fine-tuning. Pilot experiments with X-Decoder showed that different resolutions (e.g., 1024 for images and 224 for language) do not generalize well across tasks. Thus, _FIND_ is trained with the same resolution for all tasks. In Table 2, models are either 384x384 with batch size 384 or 1024x1024 with batch size 192 for all tasks. Other tables show results with a 640x640 training resolution and a 192 batch size.

  } & } & 
   &  &  &  \\  &  &  &  &  &  \\   &  & PQ & mAP & mIoU & PQ & mAP & mIoU & cIoU & 1-IoU & IR@1 & TR@1 \\  X-Decoder (T) (52) & UniCL (43) & 48.5 & 39.0 & 61.4 & 12.4 & 20.7 & 18.9 & 61.3 & 82.6 & 40.4 & 54.0 \\ X-Decoder (T) (52) & LLMaMa (38) & 48.5 & 38.9 & 61.2 & 19.5 & 30.2 & 35.5 & 61.6 & 82.5 & 40.2 & 52.2 \\ SAM (B) (19) & UniCL (43) & 42.5 & 37.6 & 53.6 & 4.5 & 17.7 & 17.9 & 64.9 & 81.6 & 29.1 & 39.5 \\ SAM (B) (19) & LLMaMa (38) & 42.5 & 36.9 & 53.0 & 6.1 & 15.6 & 16.6 & 58.9 & 81.5 & 27.0 & 35.5 \\  

Table 4: Ablation study on different foundation model architectures.

   &  &  &  \\  & COCO-Paragraph & COCO-Paragraph & COCO-Paragraph & COCO-Paragraph & COCO-Paragraph &  &  &  the grounding task decreases entity-based grounding performance. Since interleave grounding is related to interactive segmentation, removing it also reduces interleave segmentation performance. Finally, training only panoptic segmentation yields similar performance to other settings, indicating the unified interface's consistency with basic task training.

_Varying the feature embeddings layer for LLM:_ LLMs process language tokens, with embeddings near input and output layers being less semantic. We hypothesize that intermediate layers align better with vision embeddings. Table 5 shows performance across tasks using emebddings from layers -1 (output) to -30 (input). Layer -12 emebddings perform best, while top and bottom layers perform worse for image-text retrieval on COCO-Karparthy splits. Thus, we use layer -12 emebddings for LLaMA throughout the paper.

### Demonstration Results

Interleave Album Search.The queries in our _FIND_ approach support linear complexity interleave album search. Given an image, interleave, or text input, our model can retrieve and segment all the photos in the album. Below, we show an example using the COCO validation set as the search space.

Interleave Video Localization.We can formulate the video frame localization problem as an image-text retrieval task. This allows us to reason about and identify corresponding objects based on given instructions, as illustrated below. We believe _FIND_ is useful for robot navigation.

3D Feature Field.Foundation model embeddings are utilized to create a 3D feature field for robot manipulation, localization, and reasoning. We believe that the interleave embedding space, with its pixel-level understanding capabilities, has significant potential in the 3D feature field. Below, we compare a scene trained with FIND embeddings versus CLIP embeddings.

**Conclusions and Future Work.** This work introduces the _FIND_ Interface, a generalized interface for aligning foundation models' embeddings, along with the _FIND_ Benchmark for training and evaluation. In Sec. 4.3, we demonstrate potential applications such as interleave album search, video localization, and 3D feature fields. These examples clearly illustrate the potential of our model for personalized foundation models and robotics.

**Limitations.** Our model is only trained and evaluated on the COCO dataset. With the limitation of data quantity, we mention that the method may not be well adapted to the in-the-wild settings.

**Broader Impact.** Our proposed approach inherits ethical or social issues (e.g. bias amplification, privacy risks, energy consumption) of foundational models.

   &  &  &  &  &  &  \\   & &  &  &  &  &  &  &  &  &  &  &  &  \\   & All & 48.5 & 39.0 & **61.4** & **61.3** & 73.0 & 82.6 & **40.4** & **54.0** & **50.8** & **51.9** \\  & - Retrieval & 48.5 & 39.0 & 61.1 & 60.6 & **73.2** & **82.8** & - & - & 44.3 & 44.8 \\  & - Grounding & 48.6 & 39.1 & 61.3 & - & 40.9 & 28.8 & - & - & 45.3 & 46.2 \\  & - Interactive & 48.6 & 38.8 & 61.0 & - & 36.5 & - & - & - & 31.4 & 33.4 \\  & - Interactive & **48.9** & **39.3** & 61.0 & - & - & - & - & - & - & - \\   & [-1] & 48.3 & 39.1 & 61.2 & 61.3 & 73.0 & 82.6 & 38.9 & 52.2 & 50.3 & 50.8 \\  & [-6] & 47.8 & 38.8 & 60.4 & 60.3 & 72.9 & 81.3 & 38.1 & 49.9 & 48.1 & 47.5 \\  & [-12] & **48.5** & **39.0** & **61.4** & 61.3 & **73.0** & **82.6** & **40.4** & **54.0** & **50.8** & **51.9** \\  & [-18] & 48.2 & 39.0 & 61.1 & 62.2 & 72.6 & 82.2 & 40.1 & 52.7 & 50.6 & 50.5 \\  & [-24] & 48.5 & 38.8 & 61.5 & **61.6** & 72.9 & 82.6 & 40.2 & 52.2 & 50.5 & 51.3 \\  & [-30] & 48.1 & 39.2 & 61.1 & 60.1 & 73.3 & 82.4 & 37.9 & 49.3 & 49.4 & 50.0 \\  

Table 5: Ablate on each training task and language encoder feature level.

**Acknowledgement.** This work was supported in part by NSF CAREER IIS2150012, NASA 80NSSC21K0295, the Institute of Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration). This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.