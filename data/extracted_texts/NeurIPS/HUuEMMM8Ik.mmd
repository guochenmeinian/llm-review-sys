# Detecting Hidden Confounding In Observational Data Using Multiple Environments

Rickard K.A. Karlsson

Department of Intelligent Systems

Delft University of Technology

The Netherlands

r.k.a.karlsson@tudelft.nl

&Jesse H. Krijthe

Department of Intelligent Systems

Delft University of Technology

The Netherlands

j.h.krijthe@tudelft.nl

###### Abstract

A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify this assumption from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate & dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.

## 1 Introduction

Estimating the causal effect of a treatment on an outcome is a fundamental challenge in many areas of science and society. While this is straightforwardly done using data from randomized studies, using observational data for this task is appealing since they are often more feasible to collect while also being more representative of the population of interest . To identify causal effects using such data it is often assumed there is no hidden confounding. When this _untestable_ assumption is violated we run the risk of confusing causal relationships with spurious correlations. This can have serious consequences such as unknowingly suggesting a non-effective or, even worse, potentially harmful treatment. Therefore, detecting the presence of hidden confounding is an important problem.

Data collected from different sources often tend to be heterogeneous due to e.g. changing circumstances or time shifts. In this work, we show how such heterogeneity can be exploited to make hidden confounding testable _solely_ from observational data. We consider a setting where observational data has been collected from different environments \(E\). In each environment, we observe the same treatment \(T\) and outcome \(Y\), as well as covariates \(X\) that are known confounders between \(T\) and \(Y\). Further, we assume that the data is heterogeneous across these environments under the principle of _Independent Causal Mechanisms_, which states that a causal system consists of autonomous modules that do not inform or influence each other. The question we ask is whether there exists further hidden confounding between \(T\) and \(Y\) after having adjusted for \(X\). If that is the case, the causal effect of \(T\) on \(Y\) is not identifiable in general. Perhaps surprisingly, we demonstrate a way to decide if the causal effect would be identifiable by derivingtestable implications for whether hidden confounding is present or not. We achieve this by exploiting the hierarchical structure of the problem, shown in Figure 1.

As an illustration of a setting where this might be applied, there are many existing multi-level studies in which individuals are nested in clusters and non-randomly assigned to a treatment/control on an individual level. For instance, we can have pre-defined clusters in a multi-level observational study that investigate a specific treatment and outcome from multiple hospitals (Goldstein et al., 2002) or schools (Leite et al., 2015) that care for patients/pupils from different demographics. Here the clusters constitute different environments. Often we might suspect the existence of potential individual-level confounders such as socio-economic status. Now, if these confounding factors have different distributions at each cluster our work proposes a way to statistically test the presence of confounding between the treatment and outcome - even when we do not observe the confounding factors directly.

Another example where our method is suitable is when there are multiple observational studies where no randomized control trials are available, a problem area where systematic procedures are still lacking (Mueller et al., 2018). In particular, individual participant data meta-analyses are a type of analysis that uses all individual-level data from multiple studies instead of aggregating summary statistics (Riley et al., 2010; Di Angelantonio et al., 2016). With this information and given that we observe the same treatment and outcome across all studies, our proposed algorithm can also be used to detect if there is common hidden confounding among the studies.

ContributionsWe prove that there exists, under the principle of independent causal mechanisms, testable independencies that are only violated in the presence of unobserved confounding between treatment and outcome (Sec. 4, Theorem 1). Further, we explore the effect of changes and violations of our assumptions - while most assumptions are necessary, we find that some can be relaxed (Sec. 4.1). We then introduce a statistical testing procedure that uses any suitable conditional independence test to detect the presence of hidden confounding in observational datasets from multiple environments (Sec. 4.2). Lastly, we perform an empirical finite-sample analysis of it using both synthetic and semi-synthetic data generated with real-world covariates from the Twins

Figure 1: We have multiple observations \(i=1,,N_{k}\) of treatment \(T_{i}^{(k)}\), outcome \(Y_{i}^{(k)}\) and confounder \(X_{i}^{(k)}\) in different environments \(E^{(k)}\). The dashed bi-directed edge between \(X_{i}^{(k)}\) and \(U_{i}^{(k)}\) allows for any causal relationship, or lack thereof, between the observed and hidden confounder. **(a)**: The hierarchical structure of the multi-environment data; the causal mechanisms are unobserved but we know the indicator \(E^{(k)}\) for what environment observations belong to. **(b)**: By unrolling the graph in (a) we see that dependencies exist between any pairs of observations \((i,j)\) from the same environment (when not conditioning on the mechanisms). This can be exploited to detect the presence of the hidden confounder.

dataset (Almond et al., 2005; Louizos et al., 2017). We observe that our proposed procedure correctly predicts the presence of hidden confounding in most cases, particularly when the confounding bias is large (Sec. 5).

## 2 Problem setting

We start with some preliminaries of the causal terminology used in this paper.

**Definition 1** (Causal Graphical Model (CGM)).: _A causal graphical model \(M=(,P)\) over \(d\) random variables \(=(V_{1},V_{2},,V_{d})\) comprises (i) a directed acyclic graph (DAG) \(\) with vertices \(\) and edges \(V_{i} V_{j}\) iff \(V_{i}\) is a direct cause of \(V_{j}\), and (ii) a joint distribution \(P\) such that it has the following Markov or causal factorization over \(\):_

\[P(V_{1},V_{2},,V_{d})=_{i=1}^{d}P(V_{i}(V_{i}))\] (1)

_where \((V_{i})\) denotes the parents (direct causes) of \(V_{i}\) in \(\) and \(P(V_{i}(V_{i}))\) is the causal mechanism of \(V_{i}\)._

The DAG \(\) encodes various conditional independencies between the variables - also known as d-separations in the DAG, see Pearl (1988, Chapter 3.3) - which we write as \(_{d}\) over some disjoint sets of variables \(,\) and \(\). We shall assume that conditional independencies in \(\) imply the same conditional independencies in \(P\), and vice versa:

We consider a setting with the following variables in our causal graphical model: a one-dimensional treatment \(T\) and outcome \(Y\), in addition to some observed covariates \(X\) and unobserved covariates \(U\). We do not restrict the dimensionality of \(X\) and \(U\). Additionally, in this setting, the environment \(E\) has a direct effect on all other variables, making it a root node in \(\). We say that a variable is a confounder between \(T\) and \(Y\) if it is a cause of both \(T\) and \(Y\) in \(\). We assume that \(X\) is a known confounder between \(T\) and \(Y\), while the relationship between \(U\) and the other variables is unknown. Hence, \(U\) could be an unobserved hidden confounder (as illustrated in Figure 2) or, for instance, completely unrelated to the other variables.

Expressed in the framework of Pearl (2009), the goal of causal inference is to estimate the probability \(P(Y do(T=t))\) where \(do(T=t)\) represents an intervention on the treatment. Without any further assumptions, \(P(Y do(T=t))\) is not identifiable from an observational dataset; that is data where we have observed the choice of treatment without influencing it (Pearl, 2009). In particular, in the setting we consider here, the interventional effect remains unidentifiable if the unobserved \(U\) is a confounder between \(T\) and \(Y\). 1. Unfortunately, there is no way to check whether such unobserved confounders are present in a single dataset. We will show, however, that things are different when we have access to observational datasets from multiple environments. In this setting, we present a way to detect confounding even if it is not observed, hence demonstrating a novel and valuable approach for verifying an essential prerequisite for causal inference from observational data. In the rest of this section, we present the main assumptions that enable us to do this.

First, we have data from multiple environments \(E\) with different joint distributions \(P(T,Y,X,U E)\). We shall use \(P_{E}()\) to denote \(P( E)\), and use small letters for the random variables whenever they take particular values. In our setting, we have datasets \(D_{k}=\{t_{i}^{(k)},y_{i}^{(k)},x_{i}^{(k)}\}_{i=1}^{N_{k}}\)

Figure 2: The setting where we want to detect the presence of a hidden confounder \(U\) in \(\).

from multiple environments \(e^{(1)},e^{(2)},,e^{(K)}\); each has \(N_{k}\) observations which are assumed to be i.i.d. within the environment. \(N_{k}\) is fixed but can be different for each environment. The environments are related to each other through the following assumption.

**Assumption 2** (Shared Causal Graph).: _All environments share the same underlying causal DAG \(\)._

Next, we specify how changes in \(P_{E}(T,Y,X,U)\) arise between the different environments. We shall assume that the conditional probabilities in (1) - which we refer to as causal mechanisms - vary independently per environment. This is known as the independent causal mechanism principle.

**Assumption 3** (Independent Causal Mechanism (ICM) Principle (Peters et al., 2017)).: _The causal generative process of a system's variables is composed of autonomous modules that do not inform or influence each other. In the probabilistic case, this means that the conditional distribution of each variable given its causes (i.e., its parents in the causal graph) does not inform or influence the other mechanisms._

The above assumption covers two aspects: one concerning _informing_ and the other about _influencing_. That the mechanisms do not _inform_ each other can be interpreted as that knowing the conditional probability of one variable does not tell us anything about the conditional probabilities of other variables (Janzing and Scholkopf, 2010; Guo et al., 2022). Further, we assume that changing (or performing an intervention upon) one mechanism has no _influence_ on other mechanisms (Scholkopf et al., 2012). While this notion of independence between mechanisms can be described through a non-stochastic, algorithmic mutual information (Janzing and Scholkopf, 2010), we focus in this work explicitly on statistically independent mechanisms.

To model changes between environments with independent causal mechanisms, we parameterize each causal mechanism with \(_{V}_{V}\) for \(V\{T,Y,X,U\}\). In each environment, these parameters are fixed and determine the distribution

\[P_{E}(T,Y,X,U)=_{V\{T,Y,X,U\}}P_{_{V}}(V(V))\;.\] (2)

Note that while we need to know which observations come from which environment, we do not assume to know the particular values of the individual parameters \((_{T},_{Y},_{X},_{U})\) in any environment. We shall assume that environments are randomly sampled from a _distribution over mechanisms_ by defining non-degenerate probability measures for each causal mechanism.

**Assumption 4** (Non-degenerate Probabilistic Independent Causal Mechanisms).: _The independent causal mechanisms are non-degenerate random variables with probability measures \(P(_{V})\) for all \(V\{T,Y,X,U\}\) such that \(_{T}\), \(_{Y}\), \(_{X}\) and \(_{U}\) are pairwise independent random variables._

With the above assumption, when we now say _independent_ causal mechanisms, we refer to statistical independence between them. We argue that the above assumption is not particularly strong if we already have Assumption 3; the mechanisms are now allowed to change across environments in a probabilistic manner. Guo et al. (2022) proved the existence of such probability measures for the causal mechanisms when the data comprises an infinitely exchangeable sequence of random variables, drawing parallels to de Finetti's theorem (de Finetti, 1937).

As a final note on the assumptions we have made: these assumptions should not be taken for granted and it is crucial to also understand how violations of them will influence our theory. For this reason, we will cover this topic in Section 4.1.

Hierarchical model of the environmentsUsing these assumptions, we can now express the distribution of the datasets \(\{D_{k}\}_{k=1}^{K}\) as a hierarchical model (Gelman et al., 2013; Chapter 5), wherein we first sample the mechanisms i.i.d. \(_{V}^{(k)} P(_{V})\) for \(k=1,,K\) and \(V\{T,Y,X,U\}\) and then, for each environment \(k\), obtain \((T_{i}^{(k)},Y_{i}^{(k)},X_{i}^{(k)},U_{i}^{(k)})\) by repeatedly sampling \(N_{k}\) times according to (2). Using plate notation, we can compactly represent this hierarchical model in the augmented DAG \(^{*}\) shown in Figure 0(a). The edges in \(^{*}\) between \((T_{i}^{(k)},Y_{i}^{(k)},X_{i}^{(k)},U_{i}^{(k)})\) are the same as those between \((T,Y,X,U)\) in \(\) and \(_{V}^{(k)}(V_{i}^{(k)})\) where \(V\{T,Y,X,U\}\), for all \(i\) and \(k\).

In the next parts of the paper, we will prove how the structure of \(^{*}\) implies novel observable constraints in the multi-environment data distribution that can be exploited to statistically test the presence of hidden confounding between \(T\) and \(Y\) after having adjusted for \(X\). But first, we discuss the main literature related to our work.

## 3 Related work

This paper contributes to the growing body of research based on the principle of _Independent Causal Mechanisms_(Peters et al., 2017) which has inspired further research on integrating machine learning and causality (Scholkopf et al., 2012; Peters et al., 2016; von Kugelgen et al., 2020; Scholkopf et al., 2021). Multiple works have demonstrated how the independent causal mechanism principle could improve causal structure learning when data comes from heterogeneous environments that share the same causal model (Zhang et al., 2017; Ghassami et al., 2018; Guo et al., 2022). In particular, Guo et al. (2022) demonstrated how independent causal mechanisms imply independence constraints similar to ours when the data is exchangeable - but they assume there exist no unobserved latent variables in contrast to our work where we detect the presence of such variables.

Detecting hidden confounding is hard, and often we can only reason about the plausibility of having unmeasured confounders using some sort of sensitivity analysis (Rosenbaum and Rubin, 1983; VanderWeele and Ding, 2017; Cinelli et al., 2019). Other approaches check whether a treatment effect estimate is robust to changes in our assumptions by varying the adjustment set (Lu and White, 2014; Oster, 2019; Su and Henckel, 2022). However, the guarantees are elusive for whether this type of robustness implies unconfoundedness. Similarly, one could test for heterogeneity of the treatment effect estimates from multiple environments and conclude that if they are different, then it is due to unobserved confounding; this idea bears resemblance to the pseudo-treatment approach discussed by Imbens and Rubin (2015) for assessing unconfoundedness. But testing heterogeneity to detect confounding only works if the treatment effect is assumed to be fixed across all environments, which excludes many real-world settings. Lastly, Janzing and Scholkopf (2018) proposed a method to detect hidden confounding which is restricted to settings with linear models.

In the setting with data from multiple environments, various approaches have been proposed to deal with hidden confounding, typically by combining both experimental and observational data (Bareinboim and Pearl, 2016; Kallus et al., 2018; Athey et al., 2020; Hatt et al., 2022; Ilse et al., 2022; Imbens et al., 2022). In contrast, we consider a setting combining _only_ observational data from multiple environments. Some works make parametric assumptions in this case, such as Huang et al. (2020), assuming linearity with non-Gaussian noise. Since we want to avoid strong parametric assumptions, we consider approaches that avoid these assumptions. The principled _Joint Causal Inference_(JCI) framework (Mooij et al., 2020) is one such approach. It demonstrates how to apply traditional constraint-based methods for causal discovery (Glymour et al., 2019) with multi-environment data. In the simpler setting with observed variables \((T,Y,E)\) excluding \(X\), the JCI framework informs us that \(Y_{P}E T\) is violated in the presence of a hidden confounder \(U\) if \(E\) is an instrumental variable. But this means, once again, that the treatment effect is fixed across environments as we assume \(E\) has no direct effect on \(Y\). Variants of this type of test have also been mentioned by others, for instance Athey et al. (2020, Lemma 3) and Dahabreh et al. (2020). We demonstrate the limitations of using this approach in our experiments, and provide a more in-depth explanation using graph-based arguments in Appendix C. Our contribution is a more general non-parametric test that works even if \(E\) is an invalid instrument that can influence any of the other variables.

## 4 Detecting hidden confounding in multi-environment data

Our goal is to detect the presence of hidden confounding between treatment \(T\) and outcome \(Y\) after having adjusted for some observed confounders \(X\). Graphically, this corresponds to detecting the existence of both edges \(U T\) and \(U Y\) in the causal DAG \(\). In this section, we demonstrate testable conditional independencies between the observed variables that are _only_ violated when both those edges exist - hence providing testable implications for hidden confounding.

While we do not assume to know the complete causal DAG \(\) between our variables, we put two restrictions on it: (i) that \(Y\) is not an ancestor of \(T\) and (ii) that \(X\) is a confounder to both \(T\) and \(Y\) in contrast to, for instance, being a mediator or only a cause to either one of them. These restrictions are relatively weak as (i) holds in all practical causal inference settings as a treatment \(T\) happens before outcome \(Y\) in time and (ii) can sometimes be verified by checking that both \(T\) and \(Y\) depend on \(X\). Under this setting, we prove the following.

**Theorem 1**.: _Let \(^{(k)}=(T_{1}^{(k)},,T_{N_{k}}^{(k)})\) be the vector of all observed treatments in environments \(E^{(k)}\); define \(^{(k)}\), \(^{(k)}\), and \(^{(k)}\) similarly. We consider the data distribution \(P(^{(k)},^{(k)},^{(k)},^{(k)})\) with \(N_{k} 2\) under assumption 1,2, 3 and 4. Furthermore, assume an underlying causal DAG \(\) where \(Y\) is not an ancestor of \(T\), and that \(X\) is a known common cause to \(T\) and \(Y\). Then, for any \(k=1,,K\), there exists hidden confounding between \(T\) and \(Y\) in \(\) if and only if_

\[T_{j}^{(k)}_{P}Y_{i}^{(k)} T_{i}^{(k)},X_{i}^{(k)},X_{j}^{(k)}\ \  i,j=1,,N_{k}:i j\;.\] (3)

Proof sketch.: To prove the statement, we look at d-separations in the extended causal graphical model \(^{*}\) and show that (3) only is true for corresponding graphs \(\) where the unobserved \(U\) is a confounder between \(T\) and \(Y\). Figure 0(b) illustrates how open paths may exist between pairs of observations \((i,j)\) going through \(_{T}^{(k)},_{Y}^{(k)},_{X}^{(k)}\) or \(_{U}^{(k)}\) by unrolling the augmented graph \(^{*}\). These paths are open because of Assumption 4. This technique resembles the twin network method used for counterfactual inference  but the results we obtain from using this approach are distinctly different. The complete proof can be found in the Appendix. 

The variables \(T_{j}^{(k)}\) and \(Y_{i}^{(k)}\) are the treatment and outcome of two different observations in the same environment. Intuitively, the theorem states that after having adjusted for \((T_{i}^{(k)},X_{i}^{(k)},X_{j}^{(k)})\), we would expect under the ICM principle that \(T_{j}^{(k)}\) to not provide any information about how \(Y_{i}^{(k)}\) behaves. Thus, if it still does, then this can only be due to unobserved confounding. Testing this independence hence provides us with a testable implication in our observed data distribution on whether the unobserved \(U\) is a confounder or not.

Two-variable case without observed confoundersWe can drop the observed confounder \(X\) in Theorem 1 and, interestingly, in that case, obtain even stronger results for detecting the presence of a hidden confounder. This setting is interesting as even the two-variable case is notoriously difficult in causal discovery . Unlike in the more general setting, we no longer need to know the direction of the causal relationship between \(T\) and \(Y\).

**Theorem 2**.: _Let \(^{(k)}=(T_{1}^{(k)},,T_{N_{k}}^{(k)})\) be the vector of all observed treatments in environments \(E^{(k)}\); define \(^{(k)}\) and \(^{(k)}\) similarly. We consider the data distribution \(P(^{(k)},^{(k)},^{(k)})\) without any observed confounders and \(N_{k} 2\) under assumption 1,2, 3 and 4. Then, for any \(k=1,,K\), there exists hidden confounding between \(T\) and \(Y\) in \(\) if and only if_

\[(i)\ \ T_{j}^{(k)}_{P}Y_{i}^{(k)} T_{i}^{(k)}\ \ \ \ \ \ (ii)\ \ T_{j}^{(k)}_{P}Y_{i}^{(k)} Y_{j}^{(k)}\ \  i,j=1,,N_{k}:i j\;.\] (4)

Guo et al.  studied a similar setting to Theorem 2 and demonstrated how to decide the causal direction between \(T\) and \(Y\) in this case when there is no latent variable. Our results extend theirs as we now also show how to exclude the possibility of a latent common cause in this setting. The proof is similar to that of Theorem 1, but the conditional independencies are different. Firstly, we have \(T_{j}^{(k)}_{P}Y_{i}^{(k)} T_{i}^{(k)}\) which is the conditional independence in Theorem 1 without conditioning on \(X_{i}^{(k)}\) and \(X_{j}^{(k)}\). Secondly, we have \(T_{j}^{(k)}_{P}Y_{i}^{(k)} Y_{j}^{(k)}\). This one is necessary as we no longer assume anything about the ancestral relationship between treatment and outcome. If we had assumed that \(T\) could not be a descendant of \(Y\), we can show that only condition (i) in the theorem is necessary. Similarly, condition (ii) is only necessary when \(Y\) could not be a descendant of \(T\).

### Influence of the assumptions

Our theory shows how to test for hidden confounding, but it now relies on other untestable assumptions: namely non-degenerate independent causal mechanisms and the faithfulness & causal Markov property. Due to this, we investigate the necessity of these assumptions and identify various failure cases when they are violated. On a more positive note, we also demonstrate that the assumption of non-degenerate mechanisms can be weakened. We present here the main conclusions regarding violations on two of the assumptions while more elaborate explanations can be found in Appendix D, together with a demonstration of how our procedure can fail due to faithfulness violations as well as a discussion on assumptions about positivity and selection bias.

Violation of Assumption 3: dependent causal mechanismsWhat happens if any of the pair-wise independencies between \(_{T},_{Y},_{X}\) or \(_{U}\) are violated? To investigate this, we go through the same procedure for proving Theorem 1 where we allow any of these mechanisms to be dependent. We find that \(T_{j}^{(k)}_{P}Y_{i}^{(k)} T_{i}^{(k)},X_{i}^{(k)},X_{j}^{(k)}\) can be violated even when there is no confounding in all but one case with dependent mechanisms, meaning that it no longer works for detecting hidden confounding. The only case where our theory still works is when \(_{X}_{P}_{U}\) - i.e. the mechanisms of the observed and unobserved confounders are allowed to co-vary across environments.

Violation of Assumption 4: degenerate causal mechanismsWhat happens if one or more of the distributions \(P(_{T})\), \(P(_{Y})\), \(P(_{X})\) and \(P(_{U})\) are degenerate, meaning that some mechanisms are fixed across all environments? In the most extreme case, if all mechanisms are fixed then the distribution \(P_{E}\) would be identical in each environment. We investigate these scenarios by first adding \(_{T}\), \(_{Y}\), \(_{X}\) and/or \(_{U}\) to the conditioning set of the independence in Theorem 1. Then, we check whether this independence still is violated in the presence of hidden confounding using the same procedure used for proving the theorem. We find that the theorem fails only when we condition on both \(_{T}\) and \(_{U}\). In other words, it is only strictly necessary for our theory that changes in \(P_{_{T}}(T(T))\) or \(P_{_{U}}(U(U))\) occur between environments.

**Remark 1**.: _We may now identify a more conservative interpretation of our proposed procedure. First, one can verify the assumption of non-degenerate causal mechanisms by checking from data whether \(P_{E}(T X)\) varies across environments; if it does, then that is likely because \(_{T}\) and/or - through potential downstream effects - \(_{U}\) are non-degenerate. Next, we would run our proposed procedure. Now if the null is rejected then we can be conservative by concluding that this is either because we have hidden confounding and/or dependent mechanisms. But in the case of no rejection, it can only be interpreted as having no hidden confounders present. This is because having dependent causal mechanisms (violation of assumption 3) can only cause false positives._

### Testing the independence

Here, we explain how test the conditional independence \(T_{j}^{(k)}_{P}Y_{i}^{(k)} T_{i}^{(k)},X_{i}^{(k)},X_{j}^{(k)}\) from Theorem 1 using multi-environment data; the full procedure is summarized in Algorithm 1 where we have defined \(t_{2i-1}^{_{2i}}:=\{t_{2i-1}^{(k)}\}_{k_{2i}}\) and similarly for \(y_{2i}^{_{2i}},t_{2i}^{_{2i}},x_{2i-1}^{_{2i}}\) and \(x_{2i}^{_{2i}}\).

To test \(T_{j}^{(k)}_{P}Y_{i}^{(k)} T_{i}^{(k)},X_{i}^{(k)},X_{j}^{(k)}\), we need to simulate sampling from the joint distribution \(P(T_{i}^{(k)},Y_{i}^{(k)},X_{i}^{(k)},T_{j}^{(k)},Y_{j}^{(k)},X_{j}^{(k)})\). Note here that we do not condition on \(E^{(k)}\). The idea is as follows: we select two different observations \(i\) and \(j\) from all environments such that we get a vector of observed treatments \(t_{i}=(t_{i}^{(1)},t_{i}^{(2)},,t_{i}^{(K)})\); outcomes \(y_{i}=(y_{i}^{(1)},y_{i}^{(2)},,y_{i}^{(K)})\); and so on for \(x_{i},t_{j}\) and \(x_{j}\). Then, we can use any suitable method for conditional independence testing with \(t_{i},y_{i},x_{i},t_{j}\) and \(x_{j}\). Note that the choice of observations within each environment is arbitrary as long as we do not pick the same observation for \(i\) and \(j\), this is a consequence of observations being i.i.d. within each environment.

Increasing power of test with Fisher's methodIn essence, we perform a conditional independence test where the "sample size" of the test is the number of available environments. Thus, for a smallnumber of environments, our test might have low power (probability of detecting hidden confounding when it is present). To alleviate this issue, we recognize that we can perform this test multiple times if we have many samples \(N_{k}\) per environment. Then, we select new observations from every environment for each hypothesis test until all observations have been used up. It is important to note that we only select from environments where there still are observations that have not yet been used for the hypothesis testing. Since each hypothesis test is independent and has the same null, we can aggregate the p-values from all tests using Fisher's method to obtain a global hypothesis test (Fisher, 1925). As we show in our experiments, using Fisher's method drastically improves the power of our method, thus reducing the number of environments needed to detect the presence of hidden confounding. Having a different number of samples per environment \(N_{k}\) also necessitates specifying the hyperparameter \(K_{}\), which determines the minimum observations required in each hypothesis test. This parameter should be chosen to ensure that the used independent testing method works properly if it is provided with at least \(K_{}\) samples.

## 5 Experiments

To evaluate and investigate the theory for testing hidden confounding in multi-environment data, we perform a series of simulation studies with synthetic data in addition to experiments with semi-synthetic data generated using the Twins dataset (Almond et al., 2005; Louizos et al., 2017).2. As we want to evaluate our method's ability to detect confounding, we use data where the ground-truth causal graph is known. Unless otherwise stated, each experiment is repeated 50 times where we use a significance level \(=0.05\). Depending on the variable types in the experiment, we state what suitable conditional independence testing method is used by our algorithm.

### Synthetic data

For the synthetic data experiments, we generate data as follows: we have the confounder \(U_{i}^{(k)}(_{U}^{(k)},1)\); treatment \(T_{i}^{(k)}((U_{i}^{(k)}+_{T}^{(k)}))\); and outcome \(Y_{i}^{(k)}(( U_{i}^{(k)}+T_{i}^{(k)}+ _{Y}^{(k)}))\). Note \((x)=1/(1+e^{-x})\) is the logistic function and \(_{V}^{(k)}(0,_{_{V}}^{2})\) for \(V\{T,Y,U\}\). Unless otherwise stated, we use \(_{_{T}}=_{_{U}}=_{_{Y}}=1\). We control the strength of confounding by varying \(\), where \(=0\) corresponds to no confounding.

Larger confounder effect sizes increase the probability of detectionWe investigate how the effect size of the confounding variable influences our proposed testing procedure. We vary \(\) between 0 (no confounding) and 10 while also varying the number of environments. We perform this experiment with \(N_{k}=2\) for all \(k\) and use the G-test for conditional independence testing (McDonald, 2014). The results are shown in Figure 2(a). We note two things: the probability of detection grows for larger confounder effect size and it also grows when the number of environments is increased.

The growth rate in detection depends on the number of environmentsWe investigate the probability of detecting confounding when varying both the number of environments and the number of samples per environment for a fixed confounding strength \(=5\). We use a permutation-based method for the conditional independence test (Tsamardinos and Borboudakis, 2010) as we do not want to rely only on asymptotic validity (such as in the G-test) due to the limited number of environments. The results show that the performance of the testing procedure is highly dependent on the number of environments \(K\), see 2(b). The probability of detection grows as we increase the number of samples. Noticeably, the rate of growth increases with the number of environments \(K\).

Robustness to environmental changesWe compare our proposed procedure to the alternative approach of testing \(Y} 1.0pt}}$}}_{P}E T\) to detect hidden confounding, the latter being valid when \(E\) is an instrumental variable (Mooij et al., 2020). Here we test the sensitivity to violating one of its conditions, namely that \(P_{E}(Y T)\) is fixed under the null. We vary \(_{_{Y}}\) between 0 and \(\) when there is no confounding by setting \(=0\) with \(N=100\) and \(K=500\), and we use the G-test for conditional independence testing (McDonald, 2014). As shown in Figure 2(c), the probability of false detection using \(Y} 1.0pt}}$}}_{P}E T\) increases when \(_{_{Y}}\) starts to increase. Meanwhile, the false detection rate (type 1 error) remains bounded by \(=0.05\) for our procedure as desired. In Appendix F, we

Figure 4: **Twins dataset – (a): Effect of bias from omitting confounders on the test statistic of our hypothesis test. (b), (c): Performance when adjusting for observed confounders with either a total of 3 or 5 confounders in the data. The different curves correspond to combining numbers of hypothesis tests. The black dashed line corresponds to the rejection threshold / desired type 1 error \(=0.05\) in all figures, and the error bars / shaded area shows standard deviation (figure a) or standard error (figures b and c) from 50 repetitions.**

Figure 3: **Synthetic data – (a): Detecting confounding with \(N_{k}=2\) across a range of confounder effect sizes and numbers of environments \(K\). (500 repetitions) (b): Simulations with fixed confounding strength \(=5\) for \(N_{k}>2\) with a small number of environments \(K\). (c): Comparing the proposed procedure and an alternative testing procedure by varying the standard deviation of \(_{Y}\) in the absence of confounding. The black dashed line corresponds to the desired type 1 error control \(=0.05\). The shaded area shows the standard error from 50 repetitions.**

also include the same comparison when confounding is present to confirm that our method is able to detect confounding in this case.

### Twins dataset

We use data from twin births in the USA between 1989-1991 (Almond et al., 2005; Louizos et al., 2017) to construct an observational dataset with continuous treatment/outcome and non-linear relationships. Here the environments are different states, and a notable element of our dataset is that all variations between environments stem solely from the real-world distribution shifts of the covariates between birth states. The strength of confounding is controlled by a parameter \(\), where \(=0\) corresponds to no confounding. The full procedure for data generation is described in Appendix E. For the following experiments, we use the Kernel Conditional Independence Test (Zhang et al., 2012) in our algorithm due to having continuous variables and, unless otherwise stated, combine 50 hypothesis tests using Fisher's method.

Detection rate increases with bias from unobserved confoundingWe perform an experiment having \(p=5\) unobserved confounders, where we vary confounding strength \(\) between 0 and 5. We compute the bias from omitting the unobserved confounders when estimating the average treatment effect of \(T\) on \(Y\) in each environment. We then compare the average bias to the test statistic computed by our algorithm averaged over multiple iterations. As observed in Figure 3(a), the test statistic increases together with the bias. The black dashed line in the figure represents the rejection threshold at \(=0.05\), hence we can see that for sufficient bias the method will detect it.

Adjusting for observed confoundersIn the last experiments, we attempt to detect hidden confounding while also adjusting for observed confounders. We go from observing none to all confounders while having a confounding strength of \(=5\). We do this for the case with either a total of \(p=3\) or \(p=5\) confounders, shown in Figure 3(b) and 3(c), respectively. In addition, we investigate the influence of combining multiple hypothesis tests (\(n_{c}\) denotes the number of tests) using Fisher's method. We observe first that adjusting for more confounders leads to a decrease in detection rate, and that our desired type 1 error of \(=0.05\) is controlled when we have adjusted for all confounders. Secondly, the performance deteriorates when the total number of confounders increases, as indicated by the detection rate, which is lower when adjusting for 4 confounders when \(p=5\) than adjusting for 2 confounders when \(p=3\). This is likely because the conditional independence test loses power as the conditioning set becomes larger (Zhang et al., 2012). Thirdly, we see that the combination of multiple hypothesis tests using Fisher's method does improve the power of our algorithm. We did, however, not see any significant benefit in combining more than 50 hypothesis tests in these experiments.

## 6 Discussion

In this work, we studied a setting where observational data has been collected from different heterogeneous environments in which the same treatment \(T\), outcome \(Y\), and covariates \(X\) have been observed. We showed that assuming independent causal mechanisms, there exist testable conditional independencies that are violated in the presence of hidden confounders, for which we also proposed a statistical procedure to test these independencies from observed data. In many cases, with a sufficient number of environments, we show that we are able to detect confounding when it is present. While our main goal was to derive testable implications of hidden confounding, open questions remain on how to improve sample efficiency and tackle loss of power when adjusting for many observed confounders. Addressing these can lead to better tools for researchers to validate their causal assumptions and move towards making safer causal inferences.

Societal impactCausal inference has a big influence on real-world decision-making as it lies at the core of many sciences, ranging from medicine to public policy. While our work has the potential to improve the soundness and safety of causal inference methodology, this research is still in its infancy and we caution careful use of this work, particularly in high-stakes settings.