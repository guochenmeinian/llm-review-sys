# Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation

Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation

Guojun Xiong, Jian Li

Stony Brook University

{guojun.xiong,jian.li.3}@stonybrook.edu

###### Abstract

Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale. Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear. This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error. Combing these provide Neural-Q-Whittle with \((1/k^{2/3})\) convergence rate, where \(k\) is the number of iterations.

## 1 Introduction

We consider the restless multi-armed bandits (RMAB) problem , where the decision maker (DM) repeatedly activates \(K\) out of \(N\) arms at each decision epoch. Each arm is described by a Markov decision process (MDP) , and evolves stochastically according to two different transition kernels, depending on whether the arm is activated or not. Rewards are generated with each transition. Although RMAB has been widely used to study constrained sequential decision making problems , it is notoriously intractable due to the explosion of state space . A celebrated heuristic is the Whittle index policy , which computes the Whittle index for each arm given its current state as the cost to pull the arm. Whittle index policy then activates the \(K\) highest indexed arms at each decision epoch, and is provably asymptotically optimal .

However, the computation of Whittle index requires full knowledge of the underlying MDP associated with each arm, which is often unavailable in practice. To this end, many recent efforts have focused on learning Whittle indices for making decisions in an online manner. First, model-free reinforcement learning (RL) solutions have been proposed , among which  developed a Whittle index based Q-learning algorithm, which we call Q-Whittle for ease of exposition, and provided the first-ever rigorous asymptotic analysis. However, Q-Whittle suffers from slow convergence since it only updates the Whittle index of a specific state when that state is visited. In addition, Q-Whittle needs to store the Q-function values for all state-action pairs, which limits its applicability only to problems with small state space. Second, deep RL methods have been leveraged to predict Whittle indices via training neural networks . Thoughthese methods are capable of dealing with large state space, there is no asymptotic or finite-time performance guarantee. Furthermore, training neural networks requires to tuning hyper-parameters. This introduces an additional layer of complexity to predict Whittle indices. Third, to address aforementioned deficiencies,  proposed Q-Whittle-LFA by coupling Q-Whittle with linear function approximation and provided a finite-time convergence analysis. One key limitation of Q-Whittle-LFA is the unrealistic assumption that all data used in Q-Whittle-LFA are sampled i.i.d. from a fixed stationary distribution.

To tackle the aforementioned limitations and inspired by the empirical success of deep Q-learning in numerous applications, we develop Neural-Q-Whittle, a Whittle index based Q-learning algorithm with _neural network function approximation under Markovian observations_. Like , the updates of Q-function values and Whittle indices form a two-timescale stochastic approximation (2TSA) with the former operating on a faster timescale and the later on a slower timescale. Unlike , our Neural-Q-Whittle uses a deep neural network with the ReLU activation function to approximate the Q-function. However, Q-learning with neural network function approximation can in general diverge , and the theoretical convergence of Q-learning with neural network function approximation has been limited to special cases such as fitted Q-iteration with i.i.d. observations , which fails to capture the practical setting of Q-learning with neural network function approximation.

In this paper, we study the non-asymptotic convergence of Neural-Q-Whittle with data generated from a Markov decision process. Compared with recent theoretical works for Q-learning with neural network function approximation , our Neural-Q-Whittle involves a two-timescale update between two coupled parameters, i.e., Q-function values and Whittle indices. This renders existing finite-time analysis in  not applicable to our Neural-Q-Whittle due to the fact that  only contains a single-timescale update on Q-function values. Furthermore,  required an additional projection step for the update of parameters of neural network function so as to guarantee the boundedness between the unknown parameter at any time step with the initialization. This in some cases is impractical. Hence, a natural question that arises is

_Is it possible to provide a non-asymptotic convergence rate analysis of Neural-Q-Whittle with two coupled parameters updated in two timescales under Markovian observations without the extra projection step?_

The theoretical convergence guarantee of two-timescale Q-learning with neural network function approximation under Markovian observations remains largely an open problem, and in this paper, we provide an affirmative answer to this question. Our main contributions are summarized as follows:

\(\) We propose Neural-Q-Whittle, a novel Whittle index based Q-learning algorithm with neural network function approximation for RMAB. Inspired by recent work on TD learning  and Q-learning  with linear function approximation, our Neural-Q-Whittle removes the additional impractical projection step in the neural network function parameter update.

\(\) We establish the first finite-time analysis of Neural-Q-Whittle under Markovian observations. Due to the two-timescale nature for the updates of two coupled parameters (i.e., Q-function values and Whittle indices) in Neural-Q-Whittle, we focus on the convergence rate of these parameters rather than the convergence rate of approximated Q-functions as in . Our key technique is to view Neural-Q-Whittle as a 2TSA for finding the solution of suitable nonlinear equations. Different from recent works on finite-time analysis of a general 2TSA  or with linear function approximation , the nonlinear parameterization of Q-function in Neural-Q-Whittle under Markovian observations imposes significant difficulty in finding the global optimum of the corresponding nonlinear equations. To mitigate this, we first approximate the original neural network function with a collection of local linearization and focus on finding a surrogate Q-function in the neural network function class that well approximates the optimum. Our finite-time analysis then requires us to consider two Lyapunov functions that carefully characterize the coupling between iterates of Q-function values and Whittle indices, with one Lyapunov function defined with respect to the true neural network function, and the other defined with respect to the locally linearized neural network function. We then characterize the errors between these two Lyapunov functions. Putting them together, we prove that Neural-Q-Whittle achieves a convergence in expectation at a rate \((1/k^{2/3})\), where \(k\) is the number of iterations.

\(\) Finally, we conduct experiments to validate the convergence performance of Neural-Q-Whittle, and verify the sufficiency of our proposed condition for the stability of Neural-Q-Whittle.

## 2 Preliminaries

**RMAB.** We consider an infinite-horizon average-reward RMAB with each arm \(n\) described by a unichain MDP \(_{n}:=(,,P_{n},r_{n})\), where \(\) is the state space with cardinality \(S<\), \(\) is the action space with cardinality \(A\), \(P_{n}(s^{}|s,a)\) is the transition probability of reaching state \(s^{}\) by taking action \(a\) in state \(s\), and \(r_{n}(s,a)\) is the reward associated with state-action pair \((s,a)\). At each time slot \(t\), the DM activates \(K\) out of \(N\) arms. Arm \(n\) is "active" at time \(t\) when it is activated, i.e., \(A_{n}(t)=1\); otherwise, arm \(n\) is "passive", i.e., \(A_{n}(t)=0\). Let \(\) be the set of all possible policies for RMAB, and \(\) is a feasible policy, satisfying \(:_{t}^{N}\), where \(_{t}\) is the sigma-algebra generated by random variables \(\{S_{n}(h),A_{n}(h): n,h t\}\). The objective of the DM is to maximize the expected long-term average reward subject to an instantaneous constraint that only \(K\) arms can be activated at each time slot, i.e.,

\[_{}\ _{T}_{} (_{t=0}^{T}_{n=1}^{N}r_{n}(t)),\ _{n=1}^{N}A_{n}(t)=K, t.\] (1)

**Whittle Index Policy.** It is well known that RMAB (1) suffers from the curse of dimensionality . To address this challenge, Whittle  proposed an index policy through decomposition. Specifically, Whittle relaxed the constraint in (1) to be satisfied on average and obtained a unconstrained problem: \(_{}_{T}_{}_{t=1}^{T} _{n=1}^{N}\{r_{n}(t)+(1-A_{n}(t))\}\), where \(\) is the Lagrangian multiplier associated with the constraint. The key observation of Whittle is that this problem can be decomposed and its solution is obtained by combining solutions of \(N\) independent problems via solving the associated dynamic programming (DP): \(V_{n}(s)=_{a\{0,1\}}Q_{n}(s,a), n,\) where

\[Q_{n}(s,a)+=ar_{n}(s,a)+_{s^{}}p_{n}(s^{}|s,1)V_{n }(s^{})+(1-a)r_{n}(s,a)++_{s^{}}p_{n}(s^ {}|s,0)V_{n}(s^{}),\] (2)

where \(\) is unique and equals to the maximal long-term average reward of the unichain MDP, and \(V_{n}(s)\) is unique up to an additive constant, both of which depend on the Lagrangian multiplier \(\). The optimal decision \(a^{*}\) in state \(s\) then is the one which maximizes the right hand side of the above DP. The Whittle index associated with state \(s\) is defined as the value \(_{n}^{*}(s)\) such that actions \(0\) and \(1\) are equally favorable in state \(s\) for arm \(n\)[3; 23], satisfying

\[_{n}^{*}(s):=r_{n}(s,1)+_{s^{}}p_{n}(s^{}|s,1)V_{n}(s^ {})-r_{n}(s,0)-_{s^{}}p_{n}(s^{}|s,0)V_{n}(s^{}).\] (3)

Whittle index policy then activates \(K\) arms with the largest Whittle indices at each time slot. Additional discussions are provided in Section B in supplementary materials.

**Q-Learning for Whittle Index.** Since the underlying MDPs are often unknown,  proposed Q-Whittle, a tabular Whittle index based Q-learning algorithm, where the updates of Q-function values and Whittle indices form a 2TSA, with the former operating on a faster timescale for a given \(_{n}\) and the later on a slower timescale. Specifically, the Q-function values for \( n\) are updated as

\[Q_{n,k+1}(s,a):=Q_{n,k}(s,a)+_{n,k}_{\{S_{n,k}=s,A_{n,k}=a\}}r_{n}(s,a)+(1-a)_{n,k}(s)\\ +_{a}Q_{n,k}(S_{n,k+1},a)-I_{n}(Q_{k})-Q_{n,k}(s,a),\] (4)

where \(I_{n}(Q_{k})=_{s}(Q_{n,k}(s,0)+Q_{n,k}(s,1))\) is standard in the relative Q-learning for long-term average MDP setting , which differs significantly from the discounted reward setting [45; 1]. \(\{_{n,k}\}\) is a step-size sequence satisfying \(_{k}_{n,k}=\) and \(_{k}_{n,k}^{2}<\).

Accordingly, the Whittle index is updated as

\[_{n,k+1}(s)=_{n,k}(s)+_{n,k}(Q_{n,k}(s,1)-Q_{n,k}(s,0)),\] (5)

with the step-size sequence \(\{_{n,k}\}\) satisfying \(_{k}_{n,k}=\), \(_{k}_{n,k}^{2}<\) and \(_{n,k}=o(_{n,k})\). The coupled iterates (4) and (5) form a 2TSA, and  provided an asymptotic convergence analysis.

## 3 Neural Q-Learning for Whittle Index

A closer look at (5) reveals that Q-Whittle only updates the Whittle index of a specific state when that state is visited. This makes Q-Whittle suffers from slow convergence. In addition, Q-Whittleneeds to store the Q-function values for all state-action pairs, which limits its applicability only to problems with small state space. To address this challenge and inspired by the empirical success of deep Q-learning, we develop Neural-Q-Whittle through coupling Q-Whittle with neural network function approximation by using low-dimensional feature mapping and leveraging the strong representation power of neural networks. For ease of presentation, we drop the subscript \(n\) in (4) and (5), and discussions in the rest of the paper apply to any arm \(n\).

Specifically, given a set of basis functions \(_{}:,=1, ,d\) with \(d SA\), the approximation of Q-function \(Q_{}(s,a)\) parameterized by a unknown weight vector \(^{md}\), is given by \(Q_{}(s,a)=f(;(s,a)),\  s ,a\), where \(f\) is a nonlinear neural network function parameterized by \(\) and \((s,a)\), with \((s,a)=(_{1}(s,a),,_{d}(s,a))^{}\). The feature vectors are assumed to be linearly independent and are normalized so that \(\|(s,a)\| 1, s,a\). In particular, we parameterize the Q-function by using a two-layer neural network 

\[f(;(s,a)):=}_{r=1}^{ m}b_{r}(_{r}^{}(s,a)),\] (6)

where \(=(b_{1},,b_{m},_{1}^{},, _{m}^{})^{}\) with \(b_{r}\) and \(_{r}^{d 1}, r[1,m]\). \(b_{r}, r\) are uniformly initialized in \(\{-1,1\}\) and \(w_{r}, r\) are initialized as a zero mean Gaussian distribution according to \((,_{d}/d)\). During training process, only \(_{r}, r\) are updated while \(b_{r}, r\) are fixed as the random initialization. Hence, we use \(\) and \(_{r}, r\) interchangeably throughout this paper. \((x)=(0,x)\) is the rectified linear unit (ReLU) activation function1.

Given (6), we can rewrite the Q-function value updates in (4) as

\[_{k+1}=_{k}+_{k}_{k} _{}f(_{k};(S_{k},A_{k} )),\] (7)

with \(_{k}\) being the temporal difference (TD) error defined as \(_{k}:=r(S_{k},A_{k})+(1-A_{k})_{k}(s)-I(_{k})+ _{a}f(_{k};(S_{k+1},a))-f( {}_{k};(S_{k},A_{k}))\), where \(I(_{k})=_{s}[f(_{k};(s,0))+f(_{k};(s,1))]\). Similarly, the Whittle index update (5) can be rewritten as

\[_{k+1}(s)=_{k}(s)+_{k}(f(_{k}; {}(s,1))-f(_{k};(s,0))).\] (8)

The coupled iterates in (7) and (8) form Neural-Q-Whittle as summarized in Algorithm 1, which aims to learn the coupled parameters \((^{},^{}(s))\) such that \(f(^{},(s,1))=f(^{ },(s,0)), s\).

**Remark 1**.: _Unlike recent works for Q-learning with linear  or neural network function approximations , we do not assume an additional projection step of the updates of unknown parameters \(_{k}\) in (7) to confine \(_{k}, k\) into a bounded set. This projection step is often used to stabilize the iterates related to the unknown stationary distribution of the underlying Markov chain, which in some cases is impractical. More recently,  removed the extra projection step and established the finite-time convergence of TD learning, which is treated as a linear stochastic approximation algorithm.  extended it to the Q-learning with linear function approximation.__However, these state-of-the-art works only contained a single-timescale update on Q-function values, i.e., with the only unknown parameter \(\), while our Neural-Q-Whittle involves a two-timescale update between two coupled unknown parameters \(\) and \(\) as in (7) and (8). Our goal in this paper is to expand the frontier by providing a finite-time bound for Neural-Q-Whittle under Markovian noise without requiring an additional projection step. We summarize the differences between our work and existing literature in Table 1._

## 4 Finite-Time Analysis of Neural-Q-Whittle

In this section, we present the finite-time analysis of Neural-Q-Whittle for learning Whittle index \((s)\) of any state \(s\) when data are generated from a MDP. To simplify notation, we abbreviate \((s)\) as \(\) in the rest of the paper. We start by first rewriting the updates of Neural-Q-Whittle in (7) and (8) as a nonlinear two-timescale stochastic approximation (2TSA) in Section 4.1.

### A Nonlinear 2TSA Formulation with Neural Network Function

We first show that Neural-Q-Whittle can be rewritten as a variant of the nonlinear 2TSA. For any fixed policy \(\), since the state of each arm \(\{S_{k}\}\) evolves according to a Markov chain, we can construct a new variable \(X_{k}=(S_{k},A_{k},S_{k+1})\), which also forms a Markov chain with state space \(:=\{(s,a,s^{})|s,(a|s) 0,p(s^{}|s,a )>0\}\). Therefore, the coupled updates (7) and (8) of Neural-Q-Whittle can be rewritten in the form of a nonlinear 2TSA :

\[_{k+1}=_{k}+_{k}h(X_{k},_{k},_ {k}),_{k+1}=_{k}+_{k}g(X_{k},_{k},_ {k}),\] (9)

where \(_{0}\) and \(_{0}\) being arbitrarily initialized in \(^{md}\) and \(\), respectively; and \(h()\) and \(g()\) satisfy

\[h(X_{k},_{k},_{k}) :=_{}f(_{k};(S_{k},A_{k})) _{k},_{k}^{md},_{k},\] (10) \[g(X_{k},_{k},_{k}) :=f(_{k};(s,1))-f(_{k};(s,0)),_{k}^{md}.\] (11)

Since \(_{k}_{k}\), the dynamics of \(\) evolves much faster than those of \(\). We aim to establish the finite-time performance of the nonlinear 2TSA in (9), where \(f()\) is the neural network function defined in (6). This is equivalent to find the root2\((^{*},^{*})\) of a system with _two coupled_ nonlinear equations \(h:^{md}^{md}\) and \(g:^{md}\) such that

\[H(,):=_{}[h(X,,)]=0, G (,):=_{}[g(X,,)]=0,\] (12)

where \(X\) is a random variable in finite state space \(\) with unknown distribution \(\). For a fixed \(\), to study the stability of \(\), we assume the condition on the existence of a mapping such that \(=y()\) is the unique solution of \(G(,)=0\). In particular, \(y()\) is given as

\[y()=r(s,1)+_{s^{}}p(s^{}|s,1)_{a}f( ;(s^{},a))-r(s,0)-_{s^{}}p(s^{}|s,0)_{a}f( ;(s^{},a)).\] (13)

 
**Algorithm** & **Noise** & **Approximation** & **Timescale** & **Whittle index** \\   Q-Whittle  & _i.i.d._ & ✗ & _two-timescale_ & ✓ \\ Q-Whittle-LFA [60; 1; 1] & _i.i.d._ & linear & _two-timescale_ & ✓ \\ Q-Learning-LFA [60; 1; 10; 11] & _Markovian_ & linear & _single-timescale_ & ✗ \\ Q-Learning-NFA [13; 15; 22; 62] & _Markovian_ & _neural network_ & _single-timescale_ & ✗ \\ TD-Learning-LFA  & _Markovian_ & linear & _single-timescale_ & ✗ \\
2TSA-IID [19; 21] & _i.i.d._ & ✗ & _two-timescale_ & ✗ \\
2TSA-Markovian  & _Markovian_ & ✗ & _two-timescale_ & ✗ \\   Neural-Q-Whittle **(this work)** & _Markovian_ & _neural network_ & _two-timescale_ & ✓ \\  

Table 1: Comparison of settings in related works.

### Main Results

As inspired by , the finite-time analysis of such a nonlinear 2TSA boils down to the choice of two step sizes \(\{_{k},_{k}, k\}\) and a Lyapunov function that couples the two iterates in (9). To this end, we first define the following two error terms:

\[}_{k}=_{k}-^{*},_{ k}=_{k}-y(_{k}),\] (14)

which characterize the coupling between \(_{k}\) and \(_{k}\). If \(}_{k}\) and \(_{k}\) go to zero simultaneously, the convergence of \((_{k},_{k})\) to \((^{*},^{*})\) can be established. Thus, to prove the convergence of \((_{k},_{k})\) of the nonlinear 2TSA in (9) to its true value \((^{*},^{*})\), we can equivalently study the convergence of \((}_{k},_{k})\) by providing the finite-time analysis for the mean squared error generated by (9). To couple the fast and slow iterates, we define the following weighted Lyapunov function

\[M(_{k},_{k}):=}{_{k}}\|}_{k}\|^{2}+\|_{k}\|^{2}=}{_{k}}\|_{k}-^{*}\|^{2}+\|_{k}-y(_{k})\|^{2},\] (15)

where \(\|\|\) stands for the the Euclidean norm for vectors throughout the paper. It is clear that the Lyapunov function \(M(_{k},_{k})\) combines the updates of \(\) and \(\) with respect to the true neural network function \(f(;(s,a))\) in (6).

To this end, our goal turns to characterize finite-time convergence of \([M(_{k},_{k})]\). However, it is challenging to directly finding the global optimum of the corresponding nonlinear equations due to the nonlinear parameterization of Q-function in Neural-Q-Whittle. In addition, the operators \(h(),g()\) and \(y()\) in (10), (11) and (13) directly relate with the convoluted neural network function \(f(;(s,a))\) in (6), which hinders us to characterize the smoothness properties of theses operators. Such properties are often required for the analysis of stochastic approximation [15; 19; 21].

To mitigate this, **(Step 1)** we instead approximate the true neural network function \(f(,(s,a))\) with a collection of local linearization \(f_{0}(;(s,a))\) at the initial point \(_{0}\). Based on the surrogate stationary point \(_{0}^{*}\) of \(f_{0}(;(s,a))\), we correspondingly define a modified Lyapunov function \((_{k},_{k})\) combining updates of \(\) and \(\) with respect to such local linearization. Specifically, we have

\[(_{k},_{k}):=}{_{k}}\|_{k}-_{0}^{*}\|^{2}+\|_{k}-y_{0}(_{k})\|^{2},\] (16)

where \(y_{0}()\) is in the same expression as \(y()\) in (13) by replacing \(f()\) with \(f_{0}()\), and we will describe this in details below. **(Step 2)** We then study the convergence rate of the nonlinear 2TSA using this modified Lyapunov function under general conditions. **(Step 3)** Finally, since the two coupled parameters \(\) and \(\) in (9) are updated with respect to the true neural network function \(f(;(s,a))\) in (6) in Neural-Q-Whittle, while we characterize their convergence using the approximated neural network function in Step 2. Hence, this further requires us to characterize the approximation errors. We visualize the above three steps in Figure 1 and provide a proof sketch in Section 4.3. Combing them together gives rise to our main theoretical results on the finite-time performance of Neural-Q-Whittle, which is formally stated in the following theorem.

**Theorem 1**.: _Consider iterates \(\{_{k}\}\) and \(\{_{k}\}\) generated by Neural-Q-Whittle in (7) and (8). Given \(_{k}=}{(k+1)},_{k}=}{(k+1)^{4/3}}\), we have for \( k\)_

\[[M(_{k+1},_{k+1})|_{k- }][(_{},_{})]}{( k+1)^{2}}+^{3}}{_{0}}+\|}_{0}\|)^{2}+(2C_{1}+\|_{0}\|)^{2}}{(k+1)^{2/3}}\] \[+c_{0}^{2}}{_{0}(1-)^{2}}\|span(_{ }f(^{*})-f(^{*}))\|^{2}+}+2\,^{3}(\|_{0}\|+| _{0}|+1)^{3}}{m^{1/2}},\] (17)

_where \(C_{1}:=c_{1}(\|_{0}\|+\|_{0}\|+1)\) with \(c_{1}\) being a proper chosen constant, \(c_{0}\) is a constant defined in Assumption 3, \(\) is the mixing time defined in (22), \(span\) denotes for the span semi-norm , and \(_{}\) represents the projection to the set of \(\) containing all possible \(f_{0}(;(s,a))\) in (18)._

The first term on the right hand side (17) corresponds to the bias compared to the Lyapunov function at the mixing time \(\), which goes to zero at a rate of \((1/k^{2})\). The second term corresponds to the accumulated estimation error of the nonlinear 2TSA due to Markovian noise, which vanishes at the rate \((1/k^{2/3})\). Hence it dominates the overall convergence rate in (17). The third term captures the distance between the optimal solution \((^{*},^{*})\) to the true neural network function \(f(_{k};(s,a))\) in(6) and the optimal one \((_{0}^{*},y_{0}(_{0}^{*}))\) with local linearization \(f_{0}(_{k};(s,a))\) in (18), which quantifies the error when \(f(^{*})\) does not fall into the function class \(\). The last term characterizes the distance between \(f(_{k};(s,a))\) and \(f_{0}(_{k};(s,a))\) with any \(_{k}\). Both terms diminish as \(m\). Theorem 1 implies the convergence to the optimal value \((^{*},^{*})\) is bounded by the approximation error, which will diminish to zero as representation power of \(f_{0}(_{k};(s,a))\) increases when \(m\). Finally, we note that the right hand side (17) ends up in \((1/k^{2})+(1/k^{2/3})+c\), where \(c\) is a constant and its value goes to \(0\) as \(m\). This indicates the error bounds of linearization with the original neural network functions are controlled by the overparameterization value of \(m\). Need to mention that a constant step size will result in a non-vanishing accumulated error as in .

**Remark 2**.: _A finite-time analysis of nonlinear 2TSA was presented in . However,  required a stability condition that \(_{k}(_{k},_{k})=(^{*},^{*})\), and both \(h\) and \(g\) are locally approximated as linear functions. [19; 60] relaxed these conditions and provided a finite-time analysis under i.i.d. noise. These results were later extended to Markovian noise  under the assumption that \(H\) function is strongly monotone in \(\) and \(G\) function is strongly monotone in \(\). Since  leveraged the techniques in , it needed to explicitly characterize the covariance between the error caused by Markovian noise and the parameters' residual error in (14), leading to the convergence analysis much more intrinsic.  exploited the mixing time to avoid the covariance between the error caused by Markovian noise and the parameters' residual error, however, it only considered the single timescale \(Q\)-learning with linear function approximation. Though our Neural-Q-Whittle can be rewritten as a nonlinear 2TSA, the nonlinear parameterization of \(Q\)-function caused by the neural network function approximation makes the aforementioned analysis not directly applicable to ours and requires additional characterization as highlighted in Figure 1. The explicit characterization of approximation errors further distinguish our work._

### Proof Sketch

In this section, we sketch the proofs of the three steps shown in Figure 1 as required for Theorem 1.

#### 4.3.1 Step 1: Approximated Solution of Neural-Q-Whittle

We first approximate the optimal solution by projecting the Q-function in (6) to some function classes parameterized by \(\). The common choice of the projected function classes is the local linearization of \(f(;(s,a))\) at the initial point \(_{0}\)[13; 62], i.e., \(:=\{f_{0}(;(s,a)),\}\), where

\[f_{0}(;(s,a))=}_{r=1}^{m}b_{r} {1}\{_{r,0}^{}(s,a)>0\}_{r}^{}(s,a).\] (18)

Then, we define the approximate stationary point \(_{0}^{*}\) with respect to \(f_{0}(;(s,a))\) as follows.

**Definition 1**.: _[_13; 62_]_ _A point \(_{0}^{*}\) is said to be the approximate stationary point of Algorithm 1 if for all feasible \(\) it holds that \(_{,,}[(_{0}_{}f_{0}( ;(s,a)))^{}(-_{0}^{*})]  0,,\) with \(_{0}:=[r(s,a)+(1-a)^{*}-I_{0}()+_{a^{}}f_{0 }(;(s^{},a))-f_{0}(;(s,a))]\), where \(I_{0}()=_{s}[f_{0}(;(s,0))+f_{0}(;(s,1))]\)._

Figure 1: Neural-Q-Whittle operates w.r.t. true neural function \(f()\) with its finite-time performance given in Theorem 1 (indicated in dashed lines). Our proofs operate in three steps: (i) Step 1: Obtain local linearization \(f_{0}()\) and define Lyapunov function \(()\) w.r.t. \(f_{0}()\). (ii) Step 2: Characterize the finite-time performance w.r.t. \(()\) using Lyapunov drift method. Since Neural-Q-Whittle is updated w.r.t. \(f()\), we need to characterize the gap between \(f()\) and \(f_{0}()\). (iii) Step 3: Similarly, we characterize the approximation errors between \(M()\) and \(()\).

Though there is a gap between the true neural function (6) and the approximated local linearized function (18), the gap diminishes as the width of neural network i.e., \(m\), becomes large .

With the approximated stationary point \(_{0}^{*}\), we can redefine the two error terms in (14) as

\[}_{k}=_{k}-_{0}^ {*},_{k}=_{k}-y_{0}(_{k}),\] (19)

using which we correspondingly define a modified Lyapunov function \((_{k},_{k})\) in (16), where

\[y_{0}()\!=\!r(s,1)\!+\!_{s^{}}\!p(s^{}|s,1) _{a}\!f_{0}(;(s^{},a))\!-\!r( s,0)\!\!_{s^{}}p(s^{}|s,0)_{a}\!f_{0}(;(s^{},a)).\] (20)

#### 4.3.2 Step 2: Convergence Rate of \((_{k},_{k})\) in (16)

Since we approximate the true neural network function \(f(;(s,a))\) in (6) with the local linearized function \(f_{0}(;(s,a))\) in (18), the operators \(h()\) and \(g()\) in (10)-(11) turn correspondingly to be

\[h_{0}(X_{k},_{k},_{k})=_{ }f_{0}(_{k};(S_{k},A_{k}))_{k,0},\ g_{0}( _{k}):=f_{0}(_{k};(s,1 ))-f_{0}(_{k};(s,0)),\] (21)

with \(_{k,0}:=r(S_{k},A_{k})+(1-A_{k})_{k}-I_{0}(_{ k})+_{a}f_{0}(_{k};(S_{k+1},a))-f_{0}( _{k};(S_{k},A_{k})).\)

Before we present the finite-time error bound of the nonlinear 2TSA (9) under Markovian noise, we first discuss the mixing time of the Markov chain \(\{X_{k}\}\) and our assumptions.

**Definition 2** (Mixing time ).: _For any \(>0\), define \(_{}\) as_

\[_{}=\!\{k 1:\|[h_{0}(X_{k},, )|X_{0}=x]-H_{0}(,)\|(\| {}-_{0}^{*}\|+\|-y_{0}(_{0 }^{*}\|)\|)\}.\] (22)

**Assumption 1**.: _The Markov chain \(\{X_{k}\}\) is irreducible and aperiodic. Hence, there exists a unique stationary distribution \(\), and constants \(C>0\) and \((0,1)\) such that \(d_{TV}(P(X_{k}|X_{0}=x),) C^{k}, k 0,x,\) where \(d_{TV}(,)\) is the total-variation (TV) distance ._

**Remark 3**.: _Assumption 1 is often assumed to study the asymptotic convergence of stochastic approximation under Markovian noise ._

**Lemma 1**.: _The function \(h_{0}(X,,)\) defined in (21) is globally Lipschitz continuous w.r.t \(\) and \(\) uniformly in \(X\), i.e., \(\|h_{0}(X,_{1},_{1})\!-\!h_{0}(X, _{2},_{2})\| L_{h,1}\|_{1}-_ {2}\|+L_{h,2}\|_{1}-_{2}\|, X\), and \(L_{h,1}=3,h_{h,2}=1\) are valid Lipschitz constants._

**Lemma 2**.: _The function \(g_{0}()\) defined in (21) is linear and thus Lipschitz continuous in \(\), i.e., \(\|g_{0}(_{1})-g_{0}(_{2})\| L_{g} \|_{1}-_{2}\|\), and \(L_{g}=2\) is a valid Lipschitz constant._

**Lemma 3**.: _The function \(y_{0}()\) defined in (20) is linear and thus Lipschitz continuous in \(\), i.e., \(\|y_{0}(_{1})-y_{0}(_{2})\| L_{y} \|_{1}-_{2}\|\), and \(L_{y}=2\) is a valid Lipschitz constant._

**Remark 4**.: _The Lipschitz continuity of \(h_{0}\) guarantees the existence of a solution \(\) to the ODE \(}\) for a fixed \(\), while the Lipschitz continuity of \(g_{0}\) and \(y_{0}\) ensures the existence of a solution \(\) to the ODE \(\) when \(\) is fixed. These lemmas often serve as assumptions when proving the convergence rate for both linear and nonlinear 2TSA ._

**Lemma 4**.: _For a fixed \(\), there exists a constant \(_{1}>0\) such that \(h_{0}(X,,)\) defined in (10) satisfies_

\[[}^{}h_{0}(X,, )]-_{1}\|}\|^{2}.\]

_For fixed \(\), there exists a constant \(_{2}>0\) such that \(g_{0}(X,,)\) defined in (11) satisfies_

\[[g_{0}(X,,)]-_{2}\| \|^{2}.\]

**Remark 5**.: _Lemma 4 guarantees the stability and uniqueness of the solution \(\) to the ODE \(}\) for a fixed \(\), and the uniqueness of the solution \(\) to the ODE \(\) for a fixed \(\). This assumption can be viewed as a relaxation of the stronger monotone property of nonlinear mappings , since it is automatically satisfied if \(h\) and \(g\) are strong monotone as assumed in ._

**Lemma 5**.: _Under Assumption 1 and Lemma 1, there exist constants \(C>0\), \((0,1)\) and \(L=(3,_{X}h_{0}(X,_{0}^{*}),y_{0}(_{0}^{*}))\) such that_

\[_{}.\]

**Remark 6**.: \(_{}\) _is equivalent to the mixing time of the underlying Markov chain satisfying \(_{ 0}_{}=0\). For simplicity, we remove the subscript and denote it as \(\)._

We now present the finite-time error bound for the Lyapunov function \((_{k},_{k})\) in (16).

**Theorem 2**.: _Consider iterates \(\{_{k}\}\) and \(\{_{k}\}\) generated by_ Neural-Q-Whittle _in (7) and (8). Given Lemma 1-4, \(_{k}=}{(k+1)},_{k}=}{(k+1)^{4/3}}\), \(_{1}:=c_{1}(\|_{0}\|+\|_{0}\|+1)\) with a constant \(c_{1}\),_

\[[(_{k+1},_{k+1})| _{k-}][(_{},_{})]}{(k+1)^{2}}+^{3}}{_{0 }}+\|}_{0}\|)^{2}+(2C_{1}+\| _{0}\|)^{2}}{(k+1)^{2/3}}\\ +c_{1}^{3}(\|_{0}\| \!+\!|_{0}|\!+\!1)^{3}m^{-1/2}}{(k+1)^{2/3}}, k.\] (23)

3.3 Step 3: Approximation Error between \(M(_{k},_{k})\) and \((_{k},_{k})\)

Finally, we characterize the approximation error between Lyapunov functions \(M(_{k},_{k})\) and \((_{k},_{k})\). Since we are dealing with long-term average MDP, we assume that the total variation of the MDP is bounded .

**Assumption 2**.: _There exists \(0<<1\) such that \(_{(s,a),(s^{},a^{})}\|p(|s,a)-p(|s^{},a^{ })\|_{TV}=2\)._

Hence, the Bellman operator is a span-contraction operator , i.e.,

\[span(f_{0}(_{0}^{*})-f(^{*}))\;span(f_{0}(_{0}^{*})-f( {}^{*})).\] (24)

**Assumption 3**.: \(\|_{0}^{*}-^{*}\| c_{0}\|span(f_{0} (_{0}^{*})-f(^{*}))\|\)_, with \(c_{0}\) being a positive constant._

**Lemma 6**.: _For \(M(_{k},_{k})\) in (15) and \((_{k},_{k})\) in (16), with constants \(c_{1}\) and \(c_{0}\) (Assumption 3),_

\[M(_{k},_{k}) 2(_{k}, _{k})+c_{0}^{2}}{_{k}(1-)}\|span(_{ }f(^{*})-f(^{*}))\|+2 ^{3}(\|_{0}\|\!+\!|_{0} |\!+\!1)^{3}}{m^{1/2}}.\]

## 5 Numerical Experiments

We numerically evaluate the performance of Neural-Q-Whittle using an example of circulant dynamics . The state space is \(=\{1,2,3,4\}\). Rewards are \(r(1,a)=-1,r(2,a)=r(3,a)=0\), and \(r(4,a)=1\) for \(a\{0,1\}\). The dynamics of states are circulant and defined as

\[P^{1}=0.5&0.5&0&0\\ 0&0.5&0.5&0\\ 0&0&0.5&0.5\\ 0.5&0&0&0.5P^{0}=0.5&0&0&0.5\\ 0.5&0.5&0&0\\ 0&0.5&0.5&0\\ 0&0&0.5&0.5.\]

This indicates that the process either remains in its current state or increments if it is active (i.e., \(a=1\)), or it either remains the current state or decrements if it is passive (i.e., \(a=0\)). The exact value of Whittle indices  are \((1)=-0.5,(2)=0.5,(3)=1,\) and \((4)=-1\).

In our experiments, we set the learning rates as \(_{k}=0.5/(k+1)\) and \(_{k}=0.1/(k+1)^{4/3}\). We use \(\)-greedy for the exploration and exploitation tradeoff with \(=0.5\). We consider a two-layer neural network with the number of neurons in the hidden layer as \(m=200\). As described in Algorithm 1, \(b_{r}, r\) are uniformly initialized in \(\{-1,1\}\) and \(w_{r}, r\) are initialized as a zero mean Gaussian distribution according to \((,_{d}/d)\). These results are carried out by Monte Carlo simulations with 100 independent trials.

Figure 2: Convergence of Neural-Q-Whittle.

**Convergence to true Whittle index.** First, we verify that Neural-Q-Whittle convergences to true Whittle indices, and compare to Q-Whittle, the first Whittle index based Q-learning algorithm. As illustrated in Figure 1(a), Neural-Q-Whittle guarantees the convergence to true Whittle indices and outperforms Q-Whittle  in the convergence speed. This is due to the fact that Neural-Q-Whittle updates the Whittle index of a specific state even when the current visited state is not that state.

Second, we further compare with other other Whittle index learning algorithms, i.e., Q-Whittle-LFA , WIQL  and QWIC in Figure 3. As we observe from Figure 3, only Neural-Q-Whittle and Q-Whittle-LFA in  can converge to the true Whittle indices for each state, while the other two benchmarks algorithms do not guarantee the convergence of true Whittle indices. Interestingly, the learning Whittle indices converge and maintain a correct relative order of magnitude, which is still be able to be used in real world problems . Moreover, we observe that Neural-Q-Whittle achieves similar convergence performance as Q-Whittle-LFA in the considered example, whereas the latter has been shown to achieve good performance in real world applications in . Though this work focuses on the theoretical convergence analysis of Q-learning based whittle index under the neural network function approximation, it might be promising to implement it in real-world applications to fully leverage the strong representation ability of neural network functions, which serves as future investigation of this work.

**Convergence of the Lyapunov function defined in (15).** We also evaluate the convergence of the proposed Lyapunov function defined in (15), which is presented in Figure 1(b). It depicts \([M(_{k},_{k})]\) vs. the number of iterations in logarithmic scale. For ease of presentation, we only take state \(s=4\) as an illustrative example. It is clear that \(M(_{k},_{k})\) converges to zero as the number of iterations increases, which is in alignment with our theoretical results in Theorem 1.

**Verification of Assumption 3.** We now verify Assumption 3 that the gap between \(_{0}^{*}\) and \(^{*}\) can be bounded by the span of \(f_{0}(_{0}^{*})\) and \(f(^{*})\) with a constant \(c_{0}\). In Figure 4, we show \(c_{0}\) as a function of the number of neurons in the hidden layer \(m\). It clearly indicates that constant \(c_{0}\) exists and decreases as the number of neurons grows larger.

## 6 Conclusion

We presented Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation. We proved that Neural-Q-Whittle achieves an \((1/k^{2/3})\) convergence rate, where \(k\) is the number of iterations when data are generated from a Markov chain and Q-function is approximated by a ReLU neural network. By viewing Neural-Q-Whittle as 2TSA and leveraging the Lyapunov drift method, we removed the projection step on parameter update of Q-learning with neural network function approximation. Extending the current framework to two-timescale Q-learning (i.e., the coupled iterates between Q-function values and Whittle indices) with general deep neural network approximation is our future work.

Figure 4: Verification of Assumption 3 w.r.t the constant \(c_{0}\).

Figure 3: Convergence comparison between Neural-Q-Whittle and benchmark algorithms.