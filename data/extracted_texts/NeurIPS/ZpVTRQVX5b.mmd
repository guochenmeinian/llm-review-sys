# TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation

Chenyang Le\({}^{1,2}\)1  Yao Qian\({}^{2}\)2  Dongmei Wang\({}^{2}\)  Long Zhou\({}^{2}\)  Shujie Liu\({}^{2}\)  Xiaofei Wang\({}^{2}\)

Midia Yousefi\({}^{2}\)  Yanmin Qian\({}^{1}\)  Jinyu Li\({}^{2}\)  Sheng Zhao\({}^{2}\)  Michael Zeng\({}^{2}\)

\({}^{1}\)Shanghai Jiao Tong University, China

\({}^{2}\)Microsoft, USA

Work done during an internship at Microsoft Azure AI. nethermanpro@sjtu.edu.cn

###### Abstract

There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speech translation. However, most end-to-end models struggle to outperform cascade models, i.e., a pipeline framework by concatenating speech recognition, machine translation, and text-to-speech models. The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data. In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separate encoders to preserve the speaker's voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing. Our experiments on the French-English language pair demonstrate that our model outperforms the current state-of-the-art speech-to-speech translation model.

Figure 1: Overview of our speech-to-speech translation framework: 1) Joint encoder-decoder model for translating speech into the target text, and coarse-grained speech tokens, \(C_{0}\); 2) Non-autoregressive acoustic model for acoustic details, \(C_{0:16}\); 3) Codec model to convert discrete speech tokens back to the waveform. Abbreviation: S/A/I(Semantic/Acoustic/Isochrony Information), \(C_{0}\)/\(C_{0:16}\)(Codec layer 0/0-15), S/A-Enc(Semantic/Acoustic Encoder), ICM(Isochrony Control Module).

Introduction

In recent years, speech translation (ST) has shifted from loosely coupled cascaded systems to more integrated, and even end-to-end systems . Traditionally, cascaded systems comprised separate components for automatic speech recognition (ASR), machine translation (MT), and optionally, text-to-speech (TTS). Recent studies [2; 3; 4; 5] have successfully integrated ASR and MT into a unified end-to-end speech-to-text translation (S2TT) system. Furthermore, the integration of TTS to form a speech-to-speech translation (S2ST) system has become an increasingly researched area.

The primary challenge in developing end-to-end S2ST systems lies in performance issues. Both S2TT and TTS involve high variability with multiple reasonable outputs. Simultaneously performing these tasks increases the complexity of learning exponentially. Additionally, there is a scarcity of end-to-end S2ST data. Most available data are weakly supervised, obtained through synthesis  or internet parsing , further complicating the task. Another significant challenge is preserving speaker identity, as obtaining large-scale datasets with ground-truth paired speech spoken by the same speaker in two languages is nearly impossible. In practical applications like video dubbing, there is also a demand for controlling the length of the generated target speech, ensuring that it closely matches the length of the source speech. This capability of isochrony control, however, is absent in the majority of existing S2ST systems.

To address these issues, we propose TransVIP, a speech-to-speech **Trans**lation framework with **V**oice and **I**sochrony **P**reservation. TransVIP employs a consecutive generation approach, simplifying the complex S2ST task into two sequential tasks while maintaining an end-to-end framework. The generation of the target speech is conditioned not only on the semantic information, as in conventional S2ST models, but also on the isochrony and acoustic information derived from the source speech. The corresponding overview of TransVIP is demonstrated in Figure 1. We evaluate the performance of TransVIP for French-English mutual translation using a subset of the CVSS-T test set . The results demonstrate that TransVIP outperforms the publicly available SOTA models such as a larger Seamless Expressive model. The generated audio samples are available at https://aka.ms/transvip. The training code and script are available at https://github.com/nethermanpro/transvip. The main contributions of the paper can be summarized as follows:

1. We introduce a framework for speech-to-speech translation tasks that employ a consecutive generation with joint inference. This method efficiently utilizes a variety of datasets through multi-task learning to overcome the challenge of scarce paired data during the training phase, while preserving the end-to-end nature during inference.
2. We propose to disentangle various information required to learn in the training stage by employing separated encoders. It can enhance the transfer of voice characteristics and isochrony temporal alignment from the source to the target speech in the translation process. Additionally, it facilitates the design of lightweight modules for more effective individual information learning.
3. We advance the SpeechTokenizer  technology for multi-lingual tasks by distilling the semantic information from a large-scale self-supervised model to the latest high-performing codec model. This advancement allows us to employ a textless non-autoregressive model for learning fine codec code generation without text labels, which is impractical in the conventional codec-based speech generation methods such as VALL-E .
4. We propose a method that refines the decoding process by incorporating a sampling mechanism within the Layer Beam Search framework, thereby enhancing the efficiency and effectiveness of non-autoregressive model decoding.

## 2 Related Works

Speech QuantizationThe task of speech quantization is to transform continuous speech features into discrete tokens. The quantization module is usually trained by self-supervised learning (SSL) methods. The speech tokens can be divided into two categories: semantic tokens and acoustic tokens. The semantic tokens[12; 13; 14; 15; 16; 17] are usually learned by context prediction task. This kind of token is rich in context information and is suitable for downstream tasks like recognition. The acoustic tokens[18; 19; 20; 21; 22], also called the neural codec, is usually trained by reconstruction task through a discrete information bottleneck. This codec is suitable for audio compression and audio generation. SpeechTokenizer  combines the semantic token and acoustic token by semantic distillation. The latest works propose attribute disentanglement of neural codec for better generation task support. FAcodec further disentangle codec into content, prosody, acoustic, and timbre through supervision and reversed gradient.

Speech Language ModelWith the recent advance in speech quantization and the strong in-context learning capability of the language model, the speech-language model has shown great potential in speech generation. The GSLM family[24; 25; 26] use semantic tokens to train language models on speech continuation. VoxLM  integrates text tokens with semantic tokens to perform speech recognition, synthesis, and continuation in a single causal language model. AudioLM uses both semantic tokens and acoustic tokens for high-quality speech continuation. VALL-E  extends this idea to zero-shot TTS and proposes auto-regressively generating the first layer and non-autoregressively the rest layers of the codec codes. Later work[28; 29; 30] have enhanced the functionality of zero-shot TTS systems, such as building cross-lingual synthesis and accurate alignment between text control signal and acoustic tokens.

End-to-End S2STAn end-to-end approach has been an ultimate goal for S2ST research. Some previous works[31; 32] have tried a direct end-to-end approach for S2ST. However, by far such methods still suffer great performance issues. A solution is to include semantic tokens(text, phoneme, or semantic speech tokens) as an intermediate result. Some recent works[33; 6; 34; 35] applied speech-to-unit translation(S2UT) that translate speech input into semantic speech tokens and then generate speech through a unit vocoder like Hifi-GAN. And UnitY use separate speech-to-text and text-to-unit models that can be optimized together. However traditional vocoder based on semantic tokens like Hifi-GAN cannot generate high-quality speech. SeamlessExpressive addresses this issue using a well-designed PRETSSEL vocoder. PolyVoice and AudioPaLM employees use two causal models to first translate speech into semantic tokens, then generate acoustic tokens based on semantic tokens, and then convert acoustic tokens to output speech. VioLA and MSLM-S2ST adopt a similar approach but use a single causal model in a multi-tasking way. However, these approaches perform several separate auto-regressive generations during the inference process. This, to a certain degree, contradicts the principle of end-to-end processing.

## 3 TransVIP

### Overview

Inspired by the recent advance in codec-based zero-shot TTS such as VALL-E , TransVIP contains three major parts: 1) A codec model for speech quantization and reconstruction, which is essential for auto-regressive speech generation (section 4.1). 2) An auto-regressive joint translation model to translate the input speech into coarse-grained speech (Section 3.1). 3) A textless non

Figure 2: The illustration of the training framework of the Joint Enc-Dec Model. During the training, the losses from the target speech clip, i.e., a sub-part of the whole target speech, which serves as a prompt, are not aggregated when computing the Cross-Entropy (CE) loss. The corresponding codec labels are masked in the implementation. The semantic encoder and the auto-regressive decoder are initialized by a SeamlessM4T X2T model 4. The semantic encoder is frozen during training. In inference, all the target speech input are replaced by source speech input.

autoregressive acoustic model to replenish acoustic details into the output speech (Section 4.2). The overall framework for generating target speech is shown in Figure 1.

In this section, we will present a detailed introduction to our joint translation model, which is the key component of our translation system. The training procedure of the joint model is illustrated in Figure 2. In section 4, we will discuss the codec and NAR model.

### Consecutive Generation with Joint Inference

Directly generating target translation speech \(Y\) from input speech \(X\) presents a considerable challenge. Therefore, the primary objective of this model is to produce a coarse-grained speech \(Y^{}\) represented by a first-layer codec sequence. Instead of optimizing the direct probability distribution of \(P(Y^{}|X)\), our optimizing target is the joint probability \(P(Y^{},T|X)\) to preserve semantic accuracy in the text outputs alone, as \(P(T|X)\). Hence, our approach focuses on modeling and maximizing the conditional expression:

\[P(Y^{},T|X)=P(Y^{}|X,T)P(T|X)\] (1)

where \(T\) represents the translated target text. This modeling is achieved through a consecutive generation framework and optimized using the beam search algorithm. Specifically, the decoder initially generates text tokens, followed by codec tokens. We employ a unique separation token to delineate the end of text tokens and the start of codec tokens. Consequently, the codec is conditioned on both the input speech and the generated text. In practical terms, exploring the entire space of \(T\) is infeasible. In beam search generation, we optimize the following approximate form:

\[Y^{},T=*{arg\,max}_{Y^{},T}P(Y^{ }|X,T)P(T|X)\] (2)

where \(\) represents a subset of \(T\) with the highest speech-to-text translation probability selected by the beam search algorithm.

### Feature Disentanglement

In the real world, an ideal training dataset for speech-to-speech generation is not readily available, where the source and target speeches are roughly equal in length and uttered by the same speaker. A real dataset comprises paired speech, either spoken by different speakers or synthesized. Thus, we cannot guarantee voice and duration preservation with these corpora.

To address this issue, we decouple the input feature into three parts: semantic information(\(S\)), acoustic information(\(A\)), and isochrony information(\(I\)). Formally, \(X=(S,A,I)\). During training the inputs to the model are \(S\) from source speech, and \((A,I)\) information from reference target speech. While during inference, \((A,I)\) are derived from source speech. In this way the input feature can always match the output speech in terms of voice characterizes \(A\) and isochrony \(I\).

Semantic Information \(S\)\(S\) is extracted using a pre-trained speech encoder from an encoder-decoder speech/text-to-text translation model, designed to provide minimal acoustic or speaker information. We also include the text encoder of the pre-trained model to enable text input and perform a knowledge distillation loss on the text output part. We freeze two pre-trained encoders during the training process.

Acoustic information \(A\)The acoustic information that we extract relates to the distinctive acoustic characteristics of a speaker's voice. We assume that the translated text \(T\) is independent of \(A\). The relationship can be modeled as in Equation 3. We also take advantage of the one-direction mask in the auto-regressive decoder to model such a relation explicitly.

\[ P(Y^{}|X,T)P(T|X)&=P(Y^{ }|S,A,I,T)P(T|S,A,I)\\ &=P(Y^{}|S,A,I,T)P(T|S,I)\] (3)

We use an acoustic encoder, comprising transformer blocks that learn from scratch, to extract the acoustic information. This information is sum pooled along the time dimension into a single embedding, which then replaces the embedding of a separation token at the input of the auto-regressive decoder. This acoustic embedding will not affect text generation due to the causal property. It is compatible with both gradients passing during training and beam search during inference. In training, the input to the acoustic encoder is a speech prompt, i.e., a sub-part of the whole target speech. We zero out the loss in the same sub-part of the label to prevent information leakage. This combined with the sum pooling forms an information bottleneck to prevent complex semantic information from passing, allowing the encoder to learn meaningful acoustic information for voice preservation. Like Classifier-free Guidance, this replacement operation is not performed with a certain probability(in our experiment \(p=0.5\)) during training.

Isochrony information \(I\)We define a frame length, in our case 160ms, to quantify the duration of speech. Our Isochrony control module contains three embeddings: a regular position embedding, a reversed position embedding, and a voice activity detection (VAD) 0-1 embedding. The reversed position embedding provides the model with duration information on the number of additional tokens to be generated in each step of the inference process , and the VAD embedding identifies whether each frame is voice-active or not. Isochrony information is then represented by the sum of the three embedding sequences, which is then concatenated with the semantic feature \(S\) before being fed into the decoder through cross-attention. This setup enables our model to align with both the total length and the voice-active regions of the input speech, which is especially beneficial for translating long speeches with pauses.

### Multi-task Training

We claim that this translation model is a cascaded trainable end-to-end model. Because the distribution \(P(Y^{}|S,A,I,T)\) and \(P(T|S,I)\) can be fitted both separately and jointly using various datasets. It significantly enhances data accessibility, which is crucial given the scarcity of S2ST datasets. We utilize these datasets during training as follows:

* **S2ST Dataset** comprises quadruple of (\(X\), \(T_{s}\), \(T_{t}\), \(Y\)), where \(T_{s}\) is the source text and \(T_{t}\) is the target text. We either use \(X\) or \(T_{s}\) as input and consecutive \(T_{t}\) and \(Y\) as target labels, or we can reverse the roles of source input and target labels.
* **ST Dataset** is a triplet of (\(X\), \(T_{s}\), \(T_{t}\)). In our model, the missing codec label does not impede the training of speech-to-text translation. We can use \(X\) as the input and treat \(T_{t}\) as an incomplete label that terminates with a separation token. In this setup, the isochrony feature is omitted and does not concatenate with the semantic feature. And, alternatively, \(T_{t}\) can serve as the input with consecutive \(X\) and \(T_{s}\) as the label, mirroring the approach in the S2ST dataset.
* **ASR Dataset**, which is highly accessible, consists of pairs (\(X\), \(T_{s}\)). Unlike most other speech translation works, we use ASR data mainly for TTS training, fitting \(P(Y^{}|S,A,I,T)\). Here, \(T_{s}\) is used as the input with consecutive \(T_{s}\) and \(X\) as the output labels, and the loss is not applied to the part of the label corresponding to \(T_{s}\). We didn't train on the recognition task because our semantic encoder is frozen, making this task ineffective.

## 4 Codec and Acoustic Modeling

### Nerual Codec with Semantic Distillation

Our neural codec is designed to provide high reconstruction quality and compatibility with language model predictions. We refer to our codec as the **S**emantic-**A**ware **S**pecch **Codec** (SASCodec), which integrates two existing methodologies: DAC  and SpeechTokenizer. DAC employs innovative factorized and L2-normalized codes in vector quantization and finely tuned hyperparameters to achieve state-of-the-art reconstruction quality. SpeechTokenizer, on the other hand, uses a Hubert encoder as a teacher and performs semantic distillation between it and the first-layer Residual Vector Quantization (RVQ) codes. This process enriches the RVQ codes with additional semantic information, facilitating downstream language modeling tasks.

We have adopted SpeechTokenizer as our foundational model and made improvements. Originally trained with limited monolingual data, we have expanded the training to include multilingual datasets. Furthermore, we replaced the Hubert encoder with an MMS encoder to enhance multilingual semantic distillation. To further improve the efficacy of semantic distillation, we use a fixed SpeechTokenizer model as the teacher and apply L1 mel loss to the audios reconstructed using only the first quantization layer.

To enhance reconstruction quality, we have integrated the codec with factorized and L2-normalized codes. The discriminator, loss configuration, and training hyperparameters align with those used in DAC.

### Textless Non-autoregressive Acoustic Modeling

Similar to previous work, the function of our non-autoregressive model is to enhance the acoustic details of generated speech by predicting the subsequent layers of Residual Vector Quantization (RVQ) codes based on the initial layer. We utilize a standard transformer equipped with adaptive layer normalization for this task. The model inputs the summation of embeddings from the first \(n\) layers of RVQ codes to predict the \((n+1)\)th layer, where \(n=1,2,...,N\) and \(N\) represent the total number of RVQ layers. A prompt is concatenated with the input embeddings along the time dimension. The prompt is the summation of full \(N\) layers of embeddings of the prompt speech. During the training phase, the prompt and the part to be generated are in the same language. However, in the inference phase, they are in two distinct languages for the translation task. This creates a certain gap. Fortunately, our model trained using a multilingual dataset can effectively generalize and bridge this gap.

Our approach diverges from prior studies by excluding textual or phonetic inputs, rendering it a completely textless model. Previous research indicated that omitting semantic information such as text or phonemes typically significantly decreases intelligibility. However, our experiments demonstrate that the textless model either matches or surpasses the performance of models with text inputs, owing to semantic distillation within the SASCodec.

The textless model offers two significant advantages: Firstly, it improves data accessibility, as the acoustic model can now be trained entirely with unsupervised data. Unlike supervised data, which requires aligned text labels and precise speech clip boundaries, unsupervised data can be arbitrarily segmented, effectively augmenting the dataset. Secondly, traditional non-autoregressive models that utilize phoneme inputs depend on accurate transcriptions to establish alignment and initiate generation. This requirement can be challenging, especially in zero-shot speech translation tasks where users might not provide transcriptions. Our textless model addresses this problem by eliminating the need for transcription.

Layer Beam Search based on SamplingIn previous work, the decoding of non-autoregressive models is typically performed greedily, as the decoding techniques used in auto-regressive models cannot be directly applied. Greedy decoding often leads to an "early decision error" problem where the model cannot revise previous outputs once generated. To address this, we propose a method called Layer Beam Search (LBS), which adapts the beam search algorithm for non-autoregressive models.

Consider a beam size of \(B\) and a vocabulary size of \(V\). In a typical beam search algorithm, a beam maintains \(B\) hypotheses. At each decoding step, each hypothesis calculates the probability distribution over \(V\), yielding \(B V\) candidates. These candidates are then ranked by their aggregated scores, and the top \(B\) are selected to form the new beam. This approach is impractical in non-autoregressive models due to the enormous vocabulary size at each decoding step. For example, for a speech segment containing \(L\) tokens, and a codec codebook size of \(C\), the effective vocabulary size becomes \(|V|=C^{L}\), which is too large to enumerate.

To tackle this, we employ a sampling method to choose a reasonable number of candidates. Rather than sampling from the entire vocabulary space \(V\), we sample from the top-K candidates at each token, thus creating a reduced vocabulary subspace \(V^{}\) of size \(k^{L}\). We sample \(N\) times from \(V^{}\) for each candidate in the original beam to form a total of \(B*N\) new candidates. We then rank and form a new beam of size \(B\) from these sampled candidates. The pseudo code for the Layer Beam Search is provided in appendix D.

## 5 Experiments and Results

### Experimental Settings

In this section, we introduce the implementation, training, and evaluation of the TransVIP system. We conducted our experiment in the English-French mutual translation setting, as it had relatively more data than other language pairs. Appendix A and B include more details about our models and metrics.

ImplementationAll three models within the system are trained using 32 NVIDIA V100 32G GPUs. We utilize Fairseq2 library5 for model building and the PyTorch Lightning framework6 for distributed data parallel (DDP) training. The training time for each of the three models is around one week.

EvaluationFor speech-to-speech translation translation evaluation, we use a subset of CVSS-T  fr-en test set containing 300 utterances. It is the first three hundred rows in the original table7 for test set provided by CoVoST 2 . We use source speech as the prompt. All the similarity metrics are also calculated with the source speech as the reference.

We use at most, a 10-second prompt for the joint translation model and a 5-second prompt for the NAR acoustic model. If the source speech is shorter than that limitation we just use the whole utterance as the prompt. For baseline, we compared our model with a textless S2UT model and two Seamless models. The textless S2UT model is the All-to-English XM Transformer8 release by SpeechMatrix. For seamless models, We compared with two versions, the medium version, and the expressive version. The medium version uses the same speech-to-text translation model as the one we use to initialize our joint translation model, while the expressive version has larger model size and a stronger text-to-speech model. For codec evaluation, we employ LibriSpeech  test-clean, which is widely adopted for codec and TTS evaluation. We compare our SASCodec with Encodec , Speechtokenizer  and DAC. We evaluate S2ST performance on translation(BLEU), speaker&prosody similarity(SIM & AutoPCP), Isochrony control(Rate, Pause & SLC), and naturalness. Please refer to Appendix A for the details of evaluation matrices.

### S2ST Evaluation Result

In this section, we will show the evaluation result of TransVIP in translation performance, voice preservation, isochrony control, and speech naturalness.

    &  &  &  &  &  &  & _{p}\)} &  \\  & & Speech & Text & & & & & & & 0.2 & 0.4 & \\   \\ Source audio & - & - & - & - & - & - & - & - & - & - & 2.62 \\ CVSS-T target & - & - & - & 0.205 & 2.30 & 0.42 & 0.47 & 0.56 & 0.88 & 3.50 \\   \\ ST + StyleTTS* & - & 33.57 & 35.26 & 0.173 & 2.74 & 0.33 & 0.51 & 0.56 & 0.85 & 3.25 \\ Textless S2UT XM & 1.2B & 15.45 & - & 0.035 & 1.96 & - & - & 0.52 & 0.79 & 3.18 \\ SeamlessM4T(M) & 1.0B & 28.95 & 34.69 & 0.037 & 2.27 & 0.16 & 0.52 & 0.43 & 0.79 & 3.08 \\ SeamlessExpressive & 1.7B & 30.85 & 35.26 & 0.256 & 2.74 & 0.22 & 0.51 & 0.50 & 0.79 & 2.91 \\ TransVIP & 1.1B & **32.60** & 35.34 & **0.320** & 2.49 & **0.55** & 0.44 & **0.70** & **0.91** & **3.19** \\   \\ ST + ValleX* & - & 22.50 & 34.89 & 0.418 & 2.87 & 0.27 & 0.54 & 0.65 & 0.89 & 3.32 \\ SeamlessM4T(M) & 1.0B & 21.06 & 32.41 & 0.033 & 2.31 & 0.16 & 0.53 & 0.51 & 0.89 & 3.20 \\ SeamlessExpressive & 1.7B & 27.39 & 34.89 & 0.335 & 2.53 & 0.30 & 0.52 & 0.58 & 0.92 & 3.57 \\ TransVIP & 1.1B & 27.28 & 33.02 & **0.395** & **2.67** & **0.45** & **0.65** & **0.81** & **0.99** & 3.40 \\   

Table 1: The subjective evaluation results of our model and baseline model. In all scores, higher values are better. The results of baseline models are inferred from official checkpoints. We have also conducted a statistical significance test between TransVIP and SeamlessExpressive at a 0.05 significance level. Any results that are statistically significantly better are highlighted in bold. Abbreviation: A.PCP(AutoPCP), Nat.(Natureness) * ST + TTS cascaded systems use the ST results from Seamless Expressive.

Translation PerformanceWe evaluated the translation performance using BLEU and ASR-BLEU metrics as presented in Table 1. 1) By comparing our models, TransVIP and the Seamless baseline, with the state-of-the-art (SOTA) textless S2UT model and analyzing the generated samples, we observed that both our models significantly outperform the textless S2UT model. Notably, the textless model exhibits severe issues with repetition and "illusion" (generating audio that seems plausible but is unrelated to the input), which have been substantially mitigated in both the Seamless model and our TransVIP. This confirms our hypothesis that including an intermediate text is crucial for successful S2ST generation. 2) Our TransVIP demonstrates highly competitive results compared to the Seamless baselines. In the French-to-English translation direction, our model achieved a BLEU score of 32.60, 3.65 points higher than the SeamlessM4T medium and 1.75 points above the larger SeamlessExpressive. In the English-to-French direction, our model surpasses the SeamlessM4T medium by 6.22 points. Additionally, the ASR-BLEU performance of our model is comparable to that of SeamlessExpressive, with only a slight difference of -0.11 points despite a 1.87-point gap in the text BLEU score. The disparity in text BLEU scores is attributed to differences in model size, and our limited S2TT data precludes rectifying this issue.

Voice PreservationWe assess speaker similarity in the SIM column of Table 1. The Textless S2UT model and SeamlessM4T Medium do not effectively preserve speaker identity, as evidenced by their near-zero similarity scores. In contrast, our TransVIP model achieves scores of 0.320 and 0.395 in the fr-en and en-fr directions, respectively, slightly outperforming the SeamlessExpressive baseline, which scores 0.256 and 0.335. TransVIP also exceeds the fr-en target speech from the CVSS-T dataset, used in our training, by 0.115 points. This indicates that our model's similarity performance transcends the limitations of its training dataset. Regarding prosody similarity, TransVIP's AutoPCP score surpasses Textless S2UT and SeamlessM4T Medium. Compared to SeamlessExpressive, TransVIP performs 0.25 points lower in the fr-en direction but 0.14 points higher in en-fr. It is significant to note that, unlike TransVIP, SeamlessExpressive explicitly incorporates a prosody encoder.

Isochrony ControlWe assess isochrony control through several metrics: speech length compliant(SLC), speech rate compliant, and pause number compliant. 1) TransVIP demonstrates superior length control, achieving an improvement of no less than 0.18 in Fr-En and 0.23 in En-Fr for SLC\({}_{0.2}\) compared to baseline models without a length control strategy. 2) TransVIP achieves the highest speech rate compliance among all baselines, benefiting from an acoustic encoder that incorporates rate information into the model. 3) Regarding pause number, TransVIP scores best in En-Fr and slightly underperforms compared to SeamlessExpressive in Fr-En. Upon reviewing the samples, we attribute this to TransVIP's addition of new pauses in the speech to adjust the total duration.

Speech NaturalnessWe observed that the quality of the conditioning speech prompt significantly influences the naturalness of the generated speech, as measured by the NISQA score. For instance, when processing low-quality real French data (Fr-En), SeamlessExpressive registers the lowest score at 2.91. Conversely, with high-quality synthesized English data (En-Fr) as input, SeamlessExpressive achieves the highest score of 3.57. Meanwhile, our TransVIP model demonstrates consistent performance, scoring 3.19 in the Fr-En direction and 3.40 in the en-fr direction.

### Codec Evaluation Result

We show the evaluation result in Table 2 and group the result by bandwidth(BW). The SpeechTokenizer that has undergone semantic distillation performs well in the NISQA score, producing a clean and natural voice. On the other hand, DAC performs well in similarity and WER, showing a strong capability of a faithful reconstruction of the original speech. Finally, our SASCodec successfully combines the strength of two codecs, showing a very competitive result in all four metrics.

## 6 Ablation Study

### Ablation on Acoustic Embedding

In the process of semantic distillation within the codec, a significant amount of acoustic information was disentangled from the first-layer codec. This raises the question of whether the acoustic encoder is still necessary in the joint translation model. To investigate this, we leveraged the model's capability to perform inference without the acoustic embedding. We conducted such inferences and comparedthe outcomes, as shown in Table 3. Without the acoustic embedding, speaker similarity scores decreased by 0.040 in the French-English (Fr-En) translations and by 0.032 in the English-French (En-Fr) translations. Additionally, there was a slight decline in the AutoPCP scores. These findings suggest that residual acoustic information remains within the first-layer codec, underscoring the necessity of including a prompt embedding.

### Ablation on Isochrony Control method

We compared our proposed Isochrony control method with and without using other strategies. We presented the results in Table 4, where a) No Isochrony Control (No IC). b) Isochrony Control on the Decoder (Dec IC). This involves adding the Isochrony embedding to the input of the encoder as another positional embedding. We implemented the method from  in our system. c) Isochrony Control on the Decoder with Future Pause Information (Dec IC + FPI). This is an improvement over (b). In addition to the distance to the global end and VAD information, two extra pieces of information are encoded: the distance to the next pause and the number of pauses in the future. We implemented the method from  in our system.

It demonstrates that our proposed method reaches the best isochronic control. The results also show that this approach can improve the ASR-BLEU score compared to Isochrony control on the decoder, meaning the model is more confident and accurate in the generation and makes fewer errors like repetition and truncation.

    & ASR-BLEU & Overlap & SLC\({}_{0.2}\) & SLC\({}_{0.4}\) \\  No IC & 30.81 & 0.689 & 0.63 & 0.87 \\ Dec IC & 30.51 & 0.748 & 0.75 & 0.90 \\ Dec IC+ FPI & 30.45 & 0.766 & 0.77 & 0.91 \\ Enc IC (Proposed) & 30.62 & 0.784 & 0.82 & 0.95 \\   

Table 4: Ablation study on the isochrony control strategy.

    & } & } & } & } & } & } & } \\  SpeechTokenizer* & 8 & 50 & 4 kbps & 0.847 & 0.558 & **3.714** & 1.44 \\ SASCodec & 8 & 50 & 4 kbps & **0.871** & **0.576** & 3.425 & **1.34** \\  Encodec* & 8 & 75 & 6 kbps & 0.887 & 0.582 & 3.139 & 1.30 \\ DAC* & 8 & 75 & 6 kbps & 0.900 & 0.590 & 3.387 & 1.31 \\  DAC\(\) & 16 & 50 & 8 kbps & 0.936 & 0.622 & 3.667 & 1.11 \\ SASCodec & 16 & 50 & 8 kbps & **0.939** & **0.624** & **3.785** & 1.11 \\  Encodec* & 16 & 75 & 12 kbps & 0.931 & 0.618 & 3.281 & 1.16 \\ DAC* & 16 & 75 & 12 kbps & **0.967** & **0.646** & 3.636 & 1.07 \\ SASCodec & 24 & 50 & 12 kbps & 0.950 & 0.631 & **3.786** & **1.06** \\   

Table 2: Codec re-synthesize evaluation result. The codecs are grouped by bandwidth. * means the result is inferred from the official checkpoint. \(\)means it is reproduced result using the official recipe. Abbreviation: NQ(number of quantizers), FR(frame rate), BW(Bandwidth), Nat.(Natureness)

    & ASR-BLEU & BLEU & SIM & A.PCP & Nat. \\   &  & \\  & 32.60 & 35.34 & 0.320 & 2.49 & 3.19 \\ -A\_emb & 32.47 & 35.18 & 0.280 & 2.45 & 3.23 \\   &  & \\  & 27.28 & 33.02 & 0.395 & 2.67 & 3.40 \\ -A\_emb & 26.84 & 33.15 & 0.362 & 2.45 & 3.46 \\   

Table 3: The ablation study of the acoustic embedding in the joint translation model.

### Ablation Study on the NAR Acoustic Model

In this section, we conduct two comparisons. Firstly, we contrast the performance of a textless non-autoregressive acoustic model with that of a model trained using text transcriptions (BPE) as input. Secondly, we assess the inference results with and without the utilization of the Layer Beam Search (LBS) algorithm to determine its impact on performance enhancement.

The textless model is trained on a combination of datasets including Librilight, VoxPopuli French subset, SeamlessAlign, and Common Voice English and French subsets. As some datasets do not provide text transcriptions, we substitute the LibriHeavy dataset in the model trained with text input. Additionally, in the textless model, long audio segments are randomly clipped, whereas in the model with text input, clips strictly adhere to timestamps provided in the metadata to align with text transcriptions. We compare the Fr-En uni-directional setup to mitigate the impact of the absence of a large-scale French dataset on the model with text input. Despite the apparent data discrepancy, textless modeling demonstrates superior performance, underscoring one of its key data accessibility advantages. All other hyperparameters, including training steps, remain consistent across both models.

The results are presented in Table 5. The textless model consistently outperforms the model with text input across all metrics of ASR-BLEU, speaker similarity, and naturalness. Moreover, employing LBS yields superior results compared to greedy decoding.

## 7 Conclusion, Limitations and future works

In this paper, we develop a speech-to-speech translation framework that preserves speaker voice and isochrony, generating high-quality translated speech suitable for both daily communication and automatic video dubbing. Our framework can infer in an end-to-end manner, jointly considering text and speech probability during the inference phase and making full use of various data during the training phase. In the Fr-En direction, TransVIP surpasses the current state-of-the-art model with comparable translation performance and significantly improved voice and isochrony preservation.

**Limitations and future works:** 1) Performance & Language Support: Due to current resource and data limitations, we have only trained our model on the Fr-En language pair. We used approximately 5k hours of audio to train the translation model, compared to over 50k hours in previous works[7; 40; 38]. This suggests that there is potential for performance improvement if we scale up the data. Additionally, we aim to extend our framework to many-to-many settings through large-scale multilingual training, similar to Seamless. 2) More Detailed Attribute Control: Upon reviewing the test samples, we found that TransVIP occasionally alters the tone of the speech, such as translating interrogative sentences into declarative ones. It is reasonable to hypothesize that more detailed control of attributes like intonation or emotion could help address this issue.

**Broader Impact:** TransVIP was developed to improve cross-lingual communication and assist with automated video dubbing workflows. However, it carries potential risks, including the misuse of the model for impersonating specific speakers by inputting targeted text and speaker prompts. Additionally, as a generative model, there is a possibility it may produce toxic or biased outputs, although such occurrences were not observed in our test samples.