# Unravelling in Collaborative Learning

Aymeric Capitaine\({}^{1}\)

Etienne Boursier\({}^{2}\)

Antoine Scheid\({}^{1}\)

Eric Moulines\({}^{1}\)

Michael I. Jordan\({}^{3}\)

El-Mahdi El-Mhamdi\({}^{1}\)

Alain Durmus\({}^{1}\)

###### Abstract

Collaborative learning offers a promising avenue for leveraging decentralized data. However, collaboration in groups of strategic learners is not a given. In this work, we consider strategic agents who wish to train a model together but have sampling distributions of different quality. The collaboration is organized by a benevolent aggregator who gathers samples so as to maximize total welfare, but is unaware of data quality. This setting allows us to shed light on the deleterious effect of _adverse selection_ in collaborative learning. More precisely, we demonstrate that when data quality indices are private, the coalition may undergo a phenomenon known as _unravelling_, wherein it shrinks up to the point that it becomes empty or solely comprised of the worst agent. We show how this issue can be addressed without making use of external transfers, by proposing a novel method inspired by probabilistic verification. This approach makes the grand coalition a Nash equilibrium with high probability despite information asymmetry, thereby breaking unravelling.

## 1 Introduction

Collaborative learning is a framework in which multiple agents share their data and computational resources to address a common learning task . A significant challenge arises when the quality of these distributions is unknown centrally and agents are strategic. Indeed, participants may be tempted to withhold or misrepresent the quality of their data to gain a competitive advantage. These strategic behaviors and their consequences have been studied extensively in the literature on information economics . In particular, information asymmetry is known to result in _adverse selection_, whereby low-quality goods end up dominating the market. In the current paper we study collaborative learning from the perspective of information economics.

A vivid illustration of adverse selection is found in Akerlof's seminal work on the market for lemons [second-hand cars of low quality, 1]. Because buyers cannot properly assess the quality of cars on the second-hand market, their inclination to pay decreases. As a consequence, sellers with high-quality cars withdraw from the market, since the proposed price falls below their reservation price. This in turn lowers the buyers' expectation regarding the average quality of cars on the market, so their willingness to pay decreases even more, which _de facto_ crowds out additional cars. Themarket may therefore enter a death spiral, up to the point where only low-quality cars are exchanged in any competitive Nash equilibrium. This phenomenon is known as _unravelling_. The insurance market serves as another poignant example of this effect (Rothschild and Stiglitz, 1976; Einav and Finkelstein, 2011; Hendren, 2013). In insurance, information asymmetry arises due to the fact that insurees possess private knowledge about their individual risk profiles, which insurers lack. Individuals with higher risk are more inclined to purchase policy, while those with lower risk opt out. Consequently, insurers are left with a pool of policyholders skewed towards higher risk, leading to increased premiums to cover potential losses. This, in turn, prompts low-risk individuals to exit the market, exacerbating adverse selection further--a cycle reminiscent of the unravelling described by Akerlof.

We study the problem of whether collaborative learning could also fall victim to unravelling. We consider strategic agents who have access to sampling distributions of varying quality and wish to jointly train a model. They delegate the training process to a central authority who collects samples so as to maximize total welfare. We ask whether adverse selection can arise when data quality is private information. In particular, the presence of a low-quality data owner may harm the model, prompting high-quality owner to leave the collaboration and train a model entirely on their own. Their departure would decrease the average data quality even more, and create a vicious circle. In the worst case, the coalition of learners would reduce to the lowest data quality owner alone. This question is of prime importance from a practical point of view, because unravelling could jeopardize the long-run stability of collaborative models deployed at large scale.

Our contribution is threefold:

1. We provide a rigorous framework for analyzing collaborative learning with strategic agents having data distributions of varying quality. On the one hand, we leverage tools from domain adaptation to capture a notion of data quality formally. On the other hand, we model collaboration as a principal-agent problem, where the principal is an aggregator in charge of collecting samples so as to maximize social welfare. This setup allows us to derive the benchmark welfare-maximizing collaboration scheme when data quality is public information.
2. We show that when data quality is private, a naive aggregation strategy which consists in asking agents to declare their quality type and applying the optimal scheme results in a complete unravelling. More precisely, the set of agents willing to collaborate is either empty or made of the lowest-quality data owner alone at any pure Nash equilibrium.
3. We present solutions to unravelling. When transfers are allowed, the VCG mechanism suffices to re-establish optimality. When transfers are not possible, we leverage probabilistic verification techniques to design a mechanism which breaks unravelling. More precisely, we ensure that the optimal, grand coalition ranks with high probability among the Nash equilibria of the game induced by our mechanism. We demonstrate how to implement our mechanism practically in the setting of classification.

Related work.The issue of information asymmetry in machine learning has been an area of recent activity. Several learning settings have been considered, including bandits (Wei et al., 2024), linear regression (Donahue and Kleinberg, 2021, 2021), classification (Blum et al., 2021) and empirical risk minimization (Dorner et al., 2023; Liu et al., 2023) in a federated context (Tu et al., 2022).

Most of these studies focus on the sub-problem of _moral hazard_, where agents take actions that are unobserved by others. This situation usually results in under-provision of effort and inefficiency at the collective scale (Laffont and Martimort, 2001). This issue appears naturally in federated learning, because model updates are performed locally. For instance, Karimireddy et al. (2022) show that heterogeneity in sampling costs results in total free-riding without a proper incentive scheme. Huang et al. (2022) show that under-provision of data points in federated learning arises from privacy concerns. Yan et al. (2023) consider a federated classification game where agents can reduce the noise in their data distribution but incur a costly effort to do so. In the same vein, Huang et al. (2023) study the case of agents who are interested in different models, and may skew their sampling measure accordingly. Saig et al. (2023) and Ananthakrishnan et al. (2023) study hidden actions when a principal delegates a predictive task to another agent, and show that thresholds or linear contracts are able to approximate the optimal contracts.

_Adverse selection_ is another type of information asymmetry, where preferences of agents are unobserved rather than actions. This issue also naturally arises in collaborative learning, because the data distributions from which agents sample or about which they care may not be public. Whiledata heterogeneity is a widely explored topic in federated learning (see for instance Gao et al. 2022 for a general survey, and Fu et al. 2023 for the specific problem of _client selection_), the strategic aspect has been rarely considered. Most studies doing so focus on hidden sampling costs (see, e.g., Karimireddy et al. 2022, or Wei et al. 2024 in a bandit context), but few address the fundamental problem of distribution shift. Ananthakrishnan et al. (2023) mentions the issue of adverse selection when a principal delegates a predictive task to an agent, and provide qualitative insights about the optimal contract. However, they consider a single agent and leave aside the question of participation. Finally, Werner et al. (2024) and Tsoy and Konstantinov (2024) study the question of the stability of collaborative learning between competing agents. However, their analysis relies on ad-hoc market structures rather than information asymmetry per se. As such, our work is the first to demonstrate the effect of imperfect information on the sustainability of collaborative learning.

Organization.Section 2 presents our model and assumptions. Section 3 studies a full information benchmark, which allows us to derive the welfare-maximizing contribution scheme. In Section 4, we turn to the more realistic case where data quality is private information. We first show that in this case, a naive aggregation method leads to total unravelling. Second, we introduce a mechanism which breaks unravelling by inducing a game where the grand coalition is a pure strategy Nash equilibrium with high probability.

## 2 Model

Statistical framework.Let \((,)\) and \((,)\) be two measurable spaces and denote by \(\) a family of probability measures on \((,)\). We consider \([J]=\{1,,J\}\) agents who aim to perform a prediction task associated with a hypothesis class \(\{g:\}\), a loss function \(:\) and a probability measure \(P_{0}\). Each agent seeks to minimize \(g_{P_{0}}(g)\) where for any probability distribution \(P\), \(_{P}(g)=(g(x),y)P(x,y)\) is the risk associated to \(g\) with respect to \(P\).

We leverage tools and results from statistical learning theory. Denote by \(\{(X_{i},Y_{i})\}_{i 1}\) the canonical process on \(\) and denote by \(_{P}\) and \(_{P}\) the canonical probability and expectation under which \(\{(X_{i},Y_{i})\}_{i 1}\) are i.i.d. random variables with distribution \(P\). With this notation, we can introduce our assumptions on \(\) and \(\).

**H1**.: _For \((0,1)\), there exist \(_{}>0\), \(>0\) and \(>0\) such that for any distribution \(P\), hypothesis \(g\) and \(n 1\)_

\[_{P}_{P}(g)-_{i=1}^{n} (g(X_{i}),Y_{i})}{(1+n)^{}}+  1-\;.\]

This assumption covers a wide range of situations. For instance in the classification case where \(=\{-1,1\}\) and \(:(y,)_{\{y 0\}}\), \(_{}=\), \(=2()\) and \(=1/2\) where \(()\) is the empirical Rademacher complexity of \(\)(Bousquet et al., 2003). Similarly, the Bayesian PAC approach in the linear regression context with bounded loss leads to \(_{}=(\|)+(1/)\), \(=\|\|_{}^{2}/8\) and \(=1\) where \(\) and \(\) are any distributions on \(\)(Shalaeva et al., 2019, Corollary 4).

For ease of notation, we let \(_{j}(g)\) serve as a shorthand for \(_{P_{j}}(g)\). It is moreover assumed that agent \(j[J]\) cannot directly sample from \(P_{0}\), but has instead access to a distribution \(P_{j}\) which deviates from \(P_{0}\) according to the \(\)-divergence:

**H2**.: _For any \(j[J]\), \(P_{j}\) has finite \(\)-divergence: \(_{j}=_{g}|_{j}(g)-_{0}(g)|<+ \;\;.\)_

Intuitively, for any \(j[J],\,_{j} 0\) models the bias incurred by having access to samples from \(P_{j}\) instead of the target distribution \(P_{0}\). More precisely, the risk excess associated with empirical risk minimization (ERM) based on samples from \(P_{j}\) is in the worst case at least \(_{j}\). A poor sampling distribution \(P_{j}\)--which corresponds to a high \(_{j}\) in the previous expression--might be the consequence of low-quality sensors or degraded experimental conditions resulting in noisier data points.

The class of discrepancies appearing in **H2** has been considered in the domain adaptation literature (see, e.g., Ben-David et al., 2010, Kifer et al., 2004, Konstantinov and Lampert, 2019). It provides a natural framework to analyze the behavior of models trained on diverse distributions, and is practically appealing since it can be easily estimated in the context of classification (Ben-David et al., 2010).

We make the following assumptions on the quality indexes \((_{1},,_{J})\), hereafter referred to as _types_.

**H3.**_There exists \((,)_{+}^{2}\) such that \(_{1}<_{2}<<_{J} {}\)._

**H3** The ordering assumption is just for ease of exposition but is neither used by the aggregator nor the agents. Our condition that types are strictly different is for convenience in simplifying the proofs.

Collaborative learning framework.We further suppose that agent \(j[J]\) can fit a model \(g\) based on i.i.d. samples \(\{(X_{1}^{j},Y_{1}^{j}),,(X_{n_{j}}^{j},Y_{n_{j}}^{j})\}\) of size \(n_{j} 0\) from \(P_{j}\) in one of two ways: they can compute on their own, or collaborate. This is captured by the two following options:

**Option 1**: Agent \(j\) performs ERM on their own samples:

\[_{j}=*{argmin}_{g} }_{j}(g)\;,}_{j}(g)=n_{j}^{-1}_{i=1}^{n_{j}} (g(X_{i}^{j}),Y_{i}^{j})\;. \]

This non-collaborative procedure is referred to as the _outside option_.

**Option 2**: Agent \(j\) can take part in a _coalition_ orchestrated by a central data aggregator, encoded as \(=(B_{1},,B_{J})\{0,1\}^{J}\), where \(B_{j}=1\) means that agent \(j\) is member of the coalition. We also write \(=\{j[J]:\,B_{j}=1\}\). In exchange for their samples, agent \(j\) gains access to the collaborative model trained over the concatenation of samples:

\[_{} =*{argmin}_{g}}_{ }(g)\;, \] \[}_{}(g) =N^{-1}_{j[J]}B_{j}_{i=1}^{n_{j}}(g(X_{i}^{j}),Y_{ i}^{j}) N=_{j[J]}B_{j}n_{j}\;.\]

Agent utilities.We assume that agents incur a unitary cost for sampling from their distribution and dislike statistical risk. Therefore, a baseline model for measuring the preferences associated with a model \(g\) and a number of samples \(n\) is based on a linear map: \((g,n)-a_{0}(g)-cn\) for \(a,c>0\). In practice, however, \(_{0}(g)\) is typically unknown so agents instead can use a PAC bound of the form \((_{0}(_{n})_{0}^{*}+ ) 1-\) to a assess a model \(_{n}\) trained over their samples, where \(>0\), \((0,1)\) and \(_{0}^{*}=_{g}_{0}(g)\). Our next result shows that **H**1 and **H**2 allow each agent to pin down such an \(>0\), under either **Option**1) or **Option**2). Assuming **H**1, define the function \(\) for any \((,n)_{+}\) by

\[(,n)=2_{}(1+n)^{-}++ \;. \]

**Lemma 1**.: _Assume **H**1 and **H**2._

1. _Any agent_ \(j[J]\) _picking the outside_ **Option**1) obtains a model_ \(_{j}\) _achieving_ \[_{0}(_{j})_{0}^{}+ (_{j},n_{j})1-\;.\]
2. _Any coalition_ \(\{0,1\}^{J}\) _drawing_ \(=(n_{1},,n_{J})_{+}^{J}\)_, samples obtains a model_ \(_{}\) _achieving_ \[_{0}(_{})_{0}^{}+ (,N)1-\;,\]

_where \(N\) is the total of samples and \(\) is the weighted average type within the coalition:_

\[N(B,)=_{j[J]}B_{j}n_{j}\;,(, )=N^{-1}_{j[J]}B_{j}n_{j}_{j}\;. \]

When the context is clear, we write \((,)=((,),N(,))\) to lighten notation. Based on Lemma 1, we define the utility of agent \(j[J]\) as

\[u_{j}:(,)-a[\,_{0}^{}+(1-B_{j}) (_{j},n_{j})+B_{j}(,)\,]-cn_{ j}\;. \]Note that for any \(j[J]\), we take \(n_{j} 0\) to be a real number for ease of presentation. In (5), \(a/c>0\) captures the extent to which individuals are willing to trade off model quality against sampling cost.

For any \(j[J]\) and \(_{-j}=(B_{1},,B_{j-1},B_{j+1},,B_{J})\{0,1\}^{J-1}\), we denote by a slight abuse of notation \((B,_{-j})=(B_{1},,B_{j-1},B,B_{j+1},,B_{J})\) for any \(B\{0,1\}\). Similarly for \(_{-j}=(n_{1},,n_{j-1},n_{j+1},,n_{j})_{+}^{ J-1}\), we write \((n,_{-j})=(n_{1},,n_{j-1},n,n_{j+1},,n_{j})\) for any \(n 0\). We can then characterize the optimal behavior of any agent picking the outside option as follows.

**Proposition 1**.: _Assume 1 and 2. For any \(j[J]\), \(_{-j}\{0,1\}^{J-1}\) and \(_{-j}_{+}^{J-1}\), the optimal number of samples to draw under Option 1) is \(*{argmax}_{n 0}u_{j}((0,_{-j}),(n,_{-j} )\,;\,_{j})=\) where_

\[=(2ac^{-1}_{})^{1/(+1)}-1\;. \]

In what follows, we assume \(2a/c>(_{})^{-1}\) to exclude the pathological case where no agent is willing to sample data points. From now on, we denote by

\[_{j}=u_{j}((0,_{-j}),(,_{-j})) \]

the best achievable utility under the outside option. Note that \(\) does not depend on \(_{j}\) (but \(_{j}\) does) so all agents outside of the coalition draw a same number of data points \(>0\). This result, which may be surprising at first glance, comes from the fact that all agents have the same accuracy-to-sampling-cost ratio \(a/c\) in their utility.

Aggregator.We finally assume that the aggregator acts benevolently to set up a Pareto-optimal collaboration, by maximizing the total welfare under individual rationality. In other words, they solve:

\[ W:\;(,)\{0,1\}^{J} _{+}^{J}_{j[J]}u_{j}(,) \] \[_{j[J]}u_{j}(,) -_{j} 0\;.\]

In the Social Choice literature, \(W\) is referred to as the utilitarian social welfare function. The participation constraint ensures that no agent within the coalition finds it beneficial to switch to their outside option. Note that \(_{+}^{J}\) is required to have non-negative entries, which prevents the aggregator from giving away data points to agents.

## 3 Full-Information Benchmark: First-Best Collaboration

In this section, we assume that the profile of types \((_{1},,_{J})^{J}\) is public, and study how the aggregator can implement an optimal collaboration among agents under this most-favorable scenario.

Exact solution.We are looking for a solution to the aggregator's problem (8). For any \(\{0,1\}^{J}\) and \(_{+}^{J}\), denote by

\[_{j}(,) =\{n 0:\,u_{j}((1,_{-j}),(n, _{-j})\,;\,_{j})_{j}\}\] \[=-(a/c)[(,)- (_{j},)]\;, \]

the maximum number of samples that agent \(j\) can be asked to provide within the coalition under its participation constraint, where \(\) is defined as in (6), and \(_{j}\) as in (7). With this notation, problem (8) rewrites

\[ W(,) _{j}_{j}(,)-n_{j} 0\;. \]

**Theorem 1**.: _Assume 1, 2, 3. Problem (8) admits a unique solution \((^{},^{}()) \{0,1\}^{J}_{+}^{J}\). Moreover,_

1. \(^{}==(1,,1)\)_,__._
2. _Denoting_ \(^{}()=(n_{1}^{}(),,n_{J}^{}())\)_, there exists_ \(L^{}[J]\) _such that for any_ \(j[J]\)_,_ \[n_{j}^{}() &=_{j}(,^{}( ))&&j<L^{},\\ &[0\,,\,_{j}(,^{}( ))\,]&&j=L^{},\\ &=0&&.\] (11)

The couple \((^{},^{})\) is referred to as the _optimal contribution scheme_. Although implicit, the condition (11) provides insights about the optimal scheme. The aggregator makes everyone enter the coalition, but only asks the \(L^{}>0\) first-best agents to contribute. This allows to obtain the best possible collaborative model while sparing any sampling cost to other agents. Moreover, the number of required samples \(n_{j}^{}()\) slightly differs from \(\) according to the relative performance of the collaborative model with respect to agent \(j\)'s one: if the agent gets a better accuracy by collaborating, the aggregator can ask them for more data; if on the other hand the agent gets a worse model by collaborating (i.e., they are a contributor with very high quality data), the aggregator can only ask less data because of the participation constraint.

Relaxed solution.Working with the optimal scheme \((^{},^{})\) is difficult because \(_{j}(,^{}())\) has no explicit expression. To make the analysis tractable, we slightly simply the optimal contribution scheme in Theorem 1 and consider the simplified optimal contribution scheme \((^{},^{})\) where \(^{}=^{}=(1,,1)\) and for any \(j[J]\),

\[n_{j}^{}()=_{\{j L^{} \}}_{j}(,^{}())  L^{}=\{j[J]:\,_{k j} _{k}(,^{}()) \}\;, \] \[=(+1)J^{}-1\;.\]

Note that \((^{},^{})\) only differs from \((^{},^{})\) in two ways. First, in \((^{},^{})\) all contributors' participation constraint bind, while in \((^{},^{})\) the \(L^{}\)-the one could be slack. Second, the total number of data points required from the coalition is fixed and equal to \(=(J^{})\). The quantity \(\) comes from a natural relaxation of the original problem Equation (10) where we leave aside an intricate term of the objective function. This relaxation, which is formally described in Appendix A, provides a good approximation of the exact solution in reasonable settings. Indeed, the following result establishes that applying \((^{},^{})\) instead of \((^{},^{})\) comes at a negligible welfare cost when types are sufficiently evenly spaced.

**Lemma 2**.: _Assume **H**1, **H**2 and \(_{j}-_{j-1}=(1/J)\) for any \(j\{2,,J\}\). Then,_

\[W(^{},^{}())=W( ^{},^{}())+(J^{ })\;.\]

Moreover, the following proposition shows that \(^{}()\) admits a workable expression.

**Corollary 1**.: _Assume **H**1, **H**2 and **H**3. Then \(L^{}=(J^{})\) and for any \(j[J]\),_

\[n_{j}^{}()=_{\{j L^{} \}}}{L^{}}+_{j}- {L^{}}_{=1}^{L^{}}_{}\;.\]

Since \((^{},^{})\) correctly approximates the optimal scheme while being more tractable, we work with it in the remainder to lighten proofs.

**H4**.: _The aggregator applies the simplified contribution scheme \((^{},^{})\)._

## 4 Hidden information

The welfare-maximizing contribution scheme described in (12) depends explicitly on \(^{J}\), so it is implementable only if types are public. This often unrealistic, either for legal or competitive reasons. We therefore turn to the problem of setting up a collaboration when types are private.

### Naive aggregation and unravelling

A naive solution to coping with the private nature of \(^{J}\) is for the aggregator to ask agents to disclose their types, and apply the simplified optimal contribution scheme defined in (12). In this setting, however, agents may declare a type \(_{j}\) different from their true type \(_{j}\).

This approach corresponds to a direct-revelation mechanism \(:(,})^{*}(})\) which unfolds as follows.

1. Any agent \(j[J]\) declares a tuple \((B_{j},_{j})\{0,1\}\{\}\). If \(B_{j}=1\), then agent \(j\) picks **Option**2), and enters the coalition with type \(_{j}\). If \(B_{j}=0\), then agent \(j\) picks **Option**1), their declared type \(_{j}\) is \(\) by convention.
2. Setting \(=(B_{1},,B_{J})\) and \(}(\{\})^{J}\), then the aggregator applies the contribution scheme defined in (12), so the vector of number of contributions within the coalition is \(^{*}(})\).

\(\) induces a direct revelation game \(([J],^{J},(v_{j})_{j[J]})\) where the action space is \(=\{(1,)\,:\,\}\{( 0,)\}\) and payoffs are for any \(j[J]\) and \(^{J}\),

\[v_{j}(s_{j},_{-j})=u_{j}(,^{*}(}))=B_{j}-a_{0}^{*}+(, ^{*}(}))-cn_{j}^{*}(})+(1-B_{j})_{j}\;.\]

This mechanism is obviously vulnerable to strategic manipulation, since it disregards incentive compatibility. This has severe consequences for the coalition, as shown by the following proposition.

**Theorem 2** (Unravelling).: _Assume **H1**, **H2**, **H3**, and **H4**. Let \(^{J}\) be the set of pure-strategy Nash equilibria of the game induced by \(\). We have_

1. \(\)__
2. _at any_ \(s^{*}\)_,_ \(=(0,,0)\) _or_ \(=(0,,0,1)\;.\)__

Theorem 2 shows that under \(\), the coalition undergoes a full unravelling: it is either empty or comprised solely of the worst agent in any Nash equilibrium. Thus, collaborative learning is not immune to adverse selection, and may suffer from unravelling as any market characterized by information asymmetry.

Sketch of proof.: The profile of actions \(((0,),,(0,))\) corresponding to \(=(0,,0)\) is a pure Nash equilibrium, since forming a lone coalition cannot bring more utility than picking the outside option. Conversely, consider a pure-strategy Nash equilibrium \(\) such that \((0,,0)\). Denote by \(=\{j[J]:\,B_{j}=1n_{j}^{*}(})>0\}\) the set of contributors under this equilibrium. It can be shown that (i) for any \((j,k)^{2}\), \(_{j}-_{j}=_{k}-_{k}\), and (ii) For any \(j\), \((1,_{j})\) with \(_{j}>\) is strictly dominated by \((1,)\) so \(_{j}=\) at the equilibrium. As a consequence, \(_{j}=_{k}\) for any \((j,k)^{2}\), which implies by **H3** that \(||=1\). From the definition of the contribution scheme (12), we can deduce that \(_{j[J]}B_{j}=||=1\), because \(||>1\) would entail \(||>1\). Finally, \(B_{J}=1\) because \(v_{J}((1,),_{-J})>v_{J}((0,),_{-J})\) and \(\) is a Nash equilibrium. This leads to \(=(0,,0,1)\). 

### Breaking unravelling

The previous results motivate the design of a more sophisticated aggregation scheme that addresses adverse selection. In this section, we discuss how to design such a procedure.

Is VCG available in our framework?Unravelling occurs under \(\) because agents do not find it beneficial to declare their true type and eventually opt for their outside option. This could be avoided by modifying \(\) to make it

1. _individually rational_, that is \(v_{j}((1,_{j}),_{-j}) v_{j}((0,),_{-j})\) for any \(j[J]\), \(_{-j}^{J-1}\),
2. and _incentive compatible_, that is \(v_{j}((1,_{j}),_{-j}) v_{j}((1,_{j} ),_{-j})\) for any \(_{j}\).

Under these conditions, the truthful, optimal profile of actions \(((1,_{1}),,(1,_{J}))\) would emerge as a Nash equilibrium. Since the aggregator seeks to minimize the utilitarian function \(W\), one option would be to rely on the VCG mechanism (Vickrey, 1961; Clarke, 1971; Groves, 1973), which is the direct-revelation mechanisms fulfilling these desiderata (Green and Laffont, 1977; Holmstrom, 1979). Formally, the VCG mechanism writes \(^{}:}(^{}( }),(}))\) where \((})=(t_{1}(),,t_{J}())^{J}\) is a set of transfers satisfying for any \(j[J]\):

\[t_{j}(})=_{k j}u_{k}((0,_{-j}), ^{}(}))-_{k j}u_{k}(, ^{}(}))\;.\]

It re-establishes truthfulness as a dominant strategy by aligning individual payoffs \(v_{j}^{}(})=u_{j}(,^{ }(}))-t_{j}(})\) with total social welfare. Unfortunately, the VCG approach is unavailable in our framework, because of the following observation.

**Lemma 3**.: _There exists \(j[J]\) such that \(-t_{j}(})>0\)._

Lemma 3 shows that some agent would need to receive a strictly positive transfer. This is impossible without a monetary payment-utility can only be decreased by the aggregator, for instance through _accuracy shaping_(Karimireddy et al., 2022)-, which we exclude here.

A probabilistic verification-based mechanism.We now show how to design a mechanism that recovers the optimal collaboration as a Nash equilibrium in high probability without the need for transfers. Inspired by the probabilistic verification approach (Ferraioli and Ventre, 2018; Ball and Katwinkel, 2019), we assume that the aggregator can approximately estimate \(_{j}\) with few samples from \(P_{j}\) for any \(j[J]\):

**H5**.: _There exists a decreasing function \(_{}:\;^{}_{+}^{}_{+}\), with \((0,1)\) defined in **H1**, such that for any \(j[J]\) and i.i.d samples \((X_{1}^{j},Y_{1}^{j}),,(X_{J}^{j},Y_{J}^{j})\) of size \(q>0\) from \(P_{j}\), there exists a \((X_{1}^{j},Y_{1}^{j}),,(X_{q}^{j},Y_{q}^{j})\)-measurable estimator \(_{j}\) satisfying_

\[(|_{j}-_{j}|_{}(q))  1-\;.\]

In Section 4.2, we show how the aggregator can design such estimators. **H5** allows us to consider a new mechanism \(:\;()\) as follows:

1. any agent \(j[J]\) declares \(B_{j}\{0,1\}\). If \(B_{j}=1\), the principal asks for \(-2(a/c)(-)\) i.i.d samples from \(P_{j}\) and estimates types as \(}=(_{j})_{j}\) following **H5**.
2. Based on the estimated types \(}=(_{j})_{j}\), the aggregator asks for \([\,n^{}_{j}(}+_{j})-\,]_{+}\) additional samples from \(P_{j}\), where \(^{}(\,\,)\) is defined as in (12) and \[_{j}=_{/J}()-2_{j}_{ /J}(),_{j}=(0,,0,1,0,,0)^{}\;.\] Thus, the number of draws required from agent \(j\) is \([\,,\,n^{}_{j}(}+_{j})]\).
3. The aggregator keeps \(m_{j}(})=\{n^{}_{j}(}+ _{j})>0\}[\,,\,n^{}_{j}(}+ _{j})]\) samples from agent \(j\), and trains a collaborative model with these pooled samples.

\(\) induces a game \(([J],\{0,1\}^{J},(_{j})_{j[J]})\) where any agent \(j[J]\) has a payoff function

\[_{j}:(B_{j},_{-j}) B_{j}-a^{ }_{0}+(,(}))-c [\,,\,n^{}_{j}(}+_{j})] +(1-B_{j})_{j}\;.\]

The rationale behind this mechanism is fairly intuitive: since \(}\) is a correct estimate of \(\), \(n^{}_{j}(}+_{j}) n^{}_{j}( {})\) correctly approximates the optimal contribution \(n^{}_{j}()\) for any contributor \(j\). Note that type estimates are purposely biased by \(_{j}\) when asking for samples. This is a safeguard against over-estimated types, which would lead to asking to many data points and could deter agents from participating in the coalition.

Critically, \((})\) does not depend on declared type, so agents are no longer able to strategically manipulate the mechanism. Moreover, \(\) does not require agents to know their own types, which would be an unrealistic assumption. Finally, observe that the number of data points asked to estimate types \(\) is low enough to never deter agents from participating in the coalition.

**Theorem 3**.: _Assume **H1**, **H2**, **H3**, **H4** and **H5**. \(^{}=(1,,1)^{}\) is a Nash equilibrium under \(\) with probability \(1-\)._

Theorem 3 shows that the optimal coalition is a sustainable equilibrium under \(\), which effectively breaks unravelling: the set of (approximate) Nash equilibria is no more reduced to profiles of actions where the coalition is empty, or reduced to the worst agent.

Practical implementation.We now explain how to practically implement \(\) in a collaborative learning setting. This requires defining a collection of estimators \((_{j})_{j}\) satisfying **H5**. To this end, we assume that few samples from the target distribution are available.

**H6**.: _There are \(q^{}>0\) i.i.d samples \(\{(X_{1}^{0},Y_{1}^{0}),,(X_{q^{}}^{0},Y_{q^{}}^{0})\}\) from \(P_{0}\) available to the aggregator and agents._

Under **H6**, define \(}_{0}(g)=q^{-1}_{i=1}^{q^{}}(g(X_{i}^ {0}),Y_{i}^{0})\) for \(g\). This allows us to devise suitable estimators \(_{j}\) as follows.

**Proposition 2**.: _Assume **H1**, **H2** and **H6**. For any \(j[J]\) the estimator_

\[_{0,j}^{}}=_{g}| }_{j}(g)-}_{0}(g)|\;,\]

_satisfies **H5** with_

\[_{}(q)=_{/4}(q+1)^{-}+(q^{}+1)^{- }+2\;. \]

Proposition 2 shows that the empirical version of the \(\)-divergence defined in **H2** correctly estimate types. Note that the tighter the PAC bound in **H1**, the better the approximation term in (13). The type estimator \(_{0,j}^{}}\) defined in **H5** can easily be computed in the classification case, as shown with the following example.

**H7** (Classification setting).: \(=\{-1,1\}\)_,_ \(=_{0,1}:(y,y^{})_{ \{yy^{}<0\}}\)_, and_ \(\) _is a symmetric (_\(g\) _if and only if_ \(-g\)_) class of classifiers._

**Example 4**.: _Assume **H1**, **H2**, **H6** and **H7**._

1. _Denoting_ \(}_{j^{-}}(g)=n_{j}^{-1}_{i=1}^{n_{j}}_{0,1}(g(X_{ i}^{j}),-Y_{i}^{j})\)_, we have_ \[_{0,j}^{}}=1-_{g}\{ }_{0}(g)+}_{j^{-}}(g)\}\;.\]
2. _In_ **H1**_, assume_ \(_{}=(1/)^{1/2}\)_,_ \(=2}()\) _and_ \(=1\)___[_10_]__. With_ \(_{0,j}^{}}\) _defined in Proposition_ 2_, we have_ \[_{/J}(q)=(4J/)^{1/2}[(1+q)^{-}+(1+q^{})^{- }]+2}()\;.\]

Example 4 shows that in the classification case, it is sufficient to flip the labels of the data received from each contributor, merge these samples with those from \(P_{0}\), and perform an empirical risk minimization to compute \(_{0,j}^{}}\). The approximation error grows no more than logarithmically with the number of agents, while decreasing at rate \(\) with the number of samples used in the estimation.

## 5 Conclusion

In this work, we show that information asymmetry has deleterious consequences when strategic agents try to learn a collaborative model. More precisely, under a naive aggregation procedure, the ignorance of others' data quality leads the coalition of learners to be either empty or reduced to the lowest-quality agent. We introduce a transfer-free mechanism based on estimation of types. This effectively counteracts unravelling by letting the grand coalition ranks among the approximate Nash equilibria with high probability.

Several possible extensions can be considered. First, it would be interesting to relax the assumption that all agents have the same ratio \(a/c\) in their utility, and see how heterogeneity affects the results.

Second, the mechanism presented in Section 4 aims for individual rationality. A more desirable, yet difficult to achieve, property would be core stability, to ensure that no group of agents would benefit from a coordinated deviation, i.e., forming an alternative coalition. Finally, it would be interesting to check whether there exist mechanisms where the optimal collaboration not only emerges as a Nash equilibrium, but as a dominant equilibrium under imperfect information.