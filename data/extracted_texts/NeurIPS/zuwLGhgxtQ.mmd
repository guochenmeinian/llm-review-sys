# A Separation in Heavy-Tailed Sampling:

Gaussian vs. Stable Oracles for Proximal Samplers

 Ye He

Georgia Institute of Technology

yhe367@gatech.edu &Alireza Mousavi-Hosseini

University of Toronto, and Vector Institute

mousavi@cs.toronto.edu &Krishnakumar Balasubramanian

University of California, Davis

kbala@ucdavis.edu &Murat A. Erdogdu

University of Toronto, and Vector Institute

erdogdu@cs.toronto.edu

###### Abstract

We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only \(((1/))\) versus \(((1/))\) iterations to output a sample which is \(\)-close to the target in \(^{2}\)-divergence. Our results are presented for proximal samplers that are based on Gaussian versus stable oracles. We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. We also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved.

## 1 Introduction

The task of sampling from heavy-tailed targets arises in various domains such as Bayesian statistics , machine learning , robust statistics , multiple comparison procedures , and study of geophysical systems . This problem is particularly challenging when using gradient-based Markov Chain Monte Carlo (MCMC) algorithms due to diminishing gradients, which occurs when the tails of the target density decay at a slow (e.g. polynomial) rate. Indeed, canonical algorithms like Langevin Monte Carlo (LMC) have been empirically observed to perform poorly  when sampling from such heavy-tailed targets.

Several approaches have been proposed in the literature to overcome these limitations of LMC and related algorithms. The predominant ones include (i) transformation-based approaches, where a diffeomorphic (invertible) transformation is used to first map the heavy-tailed density to a light-tailed one so that a light-tailed sampling algorithm can be used , (ii) discretizing general Ito diffusions with non-standard Brownian motion that have heavy-tailed densities as their equilibrium density , and (iii) discretizing stable-driven stochastic differential equations . However, the few theoretical results available on the analysis of algorithms based on approaches (i) and (ii) provide only low-accuracy heavy-tailed samplers; such algorithms require \((1/)\) iterations to obtain a sample that is \(\)-close to the target in a reasonable metric of choice. Furthermore, quantitative complexity guarantees for the sampling approach used in (iii) are not yet available; thus, existing comparisons are mainly based on empirical studies.

In stark contrast, when the target density is light-tailed it is well-known that algorithms like proximal samplers based on Gaussian oracles and the Metropolis Adjusted Langevin Algorithm (MALA) have high-accuracy guarantees; these algorithms require only \((1/)\) iterations to obtain a sample which is \(\)-close to the target in some metric. See, for example, the works by .

WSC22a, CCSW22, CG23]. Specifically,  analyzed the proximal sampling algorithm to sample from a class of strongly log-concave densities and obtained high-accuracy guarantees.  established similar high-accuracy guarantees for the proximal sampler to sample from target densities that satisfy a certain functional inequality, covering a range of light-tailed densities with exponentially fast tail decay (e.g. log-Sobolev and Poincare inequalities). However, it is not clear if the proximal sampler achieves the same desirable performance when the target is not light-tailed.

In light of existing results, in this work, we first consider the following question:

**Q1.** _What are the fundamental limits of proximal samplers under the Gaussian oracle when sampling from heavy-tailed targets?_

To answer this question, we construct lower bounds showing that Gaussian-based samplers necessarily require \((1/)\) iterations to sample from a class of heavy-tailed targets. These results complement the lower bounds on the complexity of sampling from heavy-tailed densities using the LMC algorithm established in . With this lower bound in hand, we next consider the following question:

**Q2.** _Is it possible to design high-accuracy samplers for heavy-tailed targets?_

We answer this in the affirmative by constructing proximal samplers that are based on stable oracles (see Definition 1 and Algorithm 2) by leveraging the fractional heat-flow corresponding to a class of stable-driven SDEs. We analyze the complexity of this algorithm when sampling from heavy-tailed densities that satisfy a fractional Poincare inequality, and establish that they require only \((1/)\) iterations. Together, our answers to **Q1** and **Q2** provide a clear separation between samplers based on Gaussian and stable oracles. Our contributions can be summarized as follows.

* _Lower bounds for the Gaussian oracle_: In Section 2, we focus on **Q1** and establish in Theorems 1 and 2 respectively that the Langevin diffusion and the proximal sampler based on the Gaussian oracle necessarily have a fundamental barrier when sampling from heavy-tailed densities. Our proof technique builds on , and provides a novel perspective for obtaining algorithm-dependent lower bounds for sampling, which may be of independent interest.
* _A proximal sampler based on the stable oracle:_ In Section 3, we introduce a proximal sampler based on the \(\)-stable oracle, which fundamentally relies on the exact implementations of the fractional heat flow that correspond to a stable-driven SDE. Here, the parameter \(\) determines the allowed class of heavy-tailed targets which could be sampled with high-accuracy. In Theorem 3 and Proposition 1, we provide upper bounds on the iteration complexity that are of smaller order than the corresponding lower bounds established for the Gaussian oracle. We provide a rejection-sampling based implementation of the \(\)-stable oracle for the case \(=1\) and prove complexity upper bounds in Corollary 3. Finally, in Theorem 4, considering a sub-class of Cauchy-type targets, we prove lower bounds showing that our upper bounds cannot be fundamentally improved.

An illustration of our results for Cauchy target densities, \(_{}(1+|x|^{2})^{-(d+)/2}\) where \(\) is the degrees of freedom, is provided in Table 1. We specifically consider the practical version of the stable proximal sampler with \(=1\) (i.e., Algorithm 2 with the stable oracle implemented by Algorithm 3), and show that it always outperforms the Gaussian proximal sampler (Algorithm 1). Indeed, when \( 1\), the separation between these algorithms is obvious. In the case \((0,1)\), Algorithm 2 & 3 has a \((1/)\) complexity, nevertheless, it still improves the complexity of the Gaussian proximal sampler by a factor of \(\). We also show via lower bounds (in Section 3.4) that the \((1/)\) complexity for Algorithm 2 & 3, when \((0,1)\), can only be improved up to certain factors. We remark that for the ideal proximal sampler (Algorithm 2), the upper bound when \((0,1)\) is also \(((1/))\). These results demonstrate a clear separation between Gaussian and stable proximal samplers.

**Related works.** We first discuss works analyzing the complexity of heavy-tailed sampling as characterized by a functional inequality assumption.  analyzed the connection between sampling

   &  &  \\  Oracle & Gaussian (Alg. 1) & Stable (Alg. 2 \& 3) & Gaussian (Alg. 1) & Stable (Alg. 2 \& 3) \\  Complexity & \((^{-})\) (Cor. 2) & \(((^{-1}))\) (Cor. 5) & \((^{-})\) (Cor. 2) & \(}(^{-+1})\) (Cor. 5) \\  

Table 1: **Separation for Proximal Samplers: Gaussian vs. practical Stable oracles (\(=1\)): Upper and lower iteration complexity bounds to generate an \(\)-accurate sample in \(^{2}\)-divergence from the generalized Cauchy target densities with degrees of freedom \(\), i.e. \(_{}(1+|x|^{2})^{-(d+)/2}\). Here, \(\), \(}\) hide constants depending on \(\) and \((d,1/)\). For the proximal sampler with a general \(\)-Stable oracle (Algorithm 2), the upper bound for \((0,1)\) is \(((1/))\) when \(=\). The lower bounds are from Corollary 2 via \(2^{2}^{2}\).**algorithms for a class of \(s\)-concave densities satisfying a certain isoperimetry condition related to weighted Poincare inequalities.  undertook a mean-square analysis of discretization of a specific Ito diffusion that characterizes a class of heavy-tailed densities satisfying a weighted Poincare inequality.  and  analyzed the complexity of pseudo-marginal MCMC algorithms and the random-walk Metropolis algorithm respectively, under weak Poincare inequalities. As mentioned before,  showed lower bounds for the LMC algorithm when the target density satisfies a weak Poincare inequality.  and  analyzed a transformation based approach for heavy-tailed sampling under conditions closely related to the same functional inequality. This transformation methodology is also used to demonstrate asymptotic exponential ergodicity for other sampling algorithms like the bouncy particle sampler and the zig-zag sampler, in the heavy-tailed settings . These works provide only low-accuracy guarantees for heavy-tailed sampling and do not consider the use of weak Fractional Poincare inequalities.

Recent years have witnessed a significant focus on (strongly) log-concave sampling, leading to an extensive body of work that is challenging to encapsulate succinctly. In the context of (strongly) log-concave or light-tailed distributions, a plethora of non-asymptotic investigations have been conducted on LMC variations, including advanced integrators , underdamped LMC , and MALA . Outside the realm of log-concavity, the dissipativity assumption, which regulates the growth of the potential, has been used in numerous studies to derive convergence guarantees .

While research on upper bounds of sampling algorithms' complexity has advanced considerably, the exploration of lower bounds is still nascent.  explored the query complexity of sampling from strongly log-concave distributions in one-dimensional settings.  established lower bounds for LMC in sampling from strongly log-concave distributions.  presented lower bounds for sampling from strongly log-concave distributions with noisy gradients.  focused on lower bounds for estimating normalizing constants of log-concave densities. Contributions by  and  provide lower bounds in the metropolized algorithm category, including Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally,  contributed to lower bounds in Fisher information for non-log-concave sampling.

## 2 Lower Bounds for Sampling with the Gaussian Oracle

In this section, we focus on **Q1** for both the Langevin diffusion (in continuous time) and the proximal sampler (in discrete time), where both procedures have the target density as their invariant measures. Our results below illustrate the limitation of the Gaussian oracle1 for heavy-tailed sampling in both continuous and discrete time, showing that the phenomenon is not because of the discretization effect, but is inherently related to the use of Gaussian oracles.

**Langevin diffusion.** We first start with the overdamped Langevin diffusion (LD):

\[X_{t}=- V(X_{t})t+B_{t}.\] (LD)

LD achieves high-accuracy "sampling" in continuous time, i.e. a \((1/)\) convergence rate in the light-tailed setting. We make the following dissipativity-type assumption.

**Assumption 1**.: _The target density is given by \(^{X}(x)(-V(x))\), where \(V:^{d}\) satisfies_

\[ x^{d},)|x|^{2}}{1+|x|^{2}}  x, V(x))|x|^{2}}{1+|x|^{2}} {for some }_{2}_{1} 0.\]

**Remark 1**.: _The upper bound on \( x, V(x)\) ensures that \(V\) grows at most logarithmically in \(|x|\). Consequently, \(^{X}\) is heavy-tailed and in fact does not satisfy a Poincare inequality. The lower bound on \( x, V(x)\) is only needed for deriving the dimension dependency in our guarantees. If one is only interested in the \(\) dependency, this condition can be replaced with \(0 x, V(x)\)._

A classical example of a density satisfying the above assumption is the generalized Cauchy density with degrees of freedom \(=_{1}=_{2}>0\), where the potential is given by

\[V_{}(x)(1+|x|^{2}).\] (1)

The following result, proved in Appendix A, provides a lower bound on the performance of LD.

**Theorem 1**.: _Suppose \(^{X}(-V)\) satisfies Assumption 1. Let \(X_{t}\) be the solution of the Langevin diffusion, and \(_{t}(X_{t})\). Then, for any \(>0\),_

\[(^{X},_{t}) C_{_{1},_{2}}d^{-_{2}}{2 }(1+)}(C_{}(_{0})+_{}t)^{- (1+)}{2}},\]

_where \(_{} 1}(1+)}{(d+ _{2})}\), \(C_{}(_{0})}[(1+|X_{0}|^{2})^{ }]^{1/}\) with \(=_{}(d+_{2})/2\), and \(C_{_{1},_{2}}\) is a constant depending only on \(_{1}\) and \(_{2}\)._

If we assume \(|X_{0}|()\) for simplicity, then by choosing \(= t}-_{1})  d}\), we obtain

\[(^{X},_{t})_{_{1},_{2}}(d^{-_{2}}{2}}t^{-}{2}}).\]

Thus, LD requires at least \(T=_{_{1},_{2}}d^{-_{2}}{2}}(1/ )^{2/_{2}}\) to reach \(\) error in total variation. While this bound may be small in high dimensions when \(_{2}>_{1}\), for the canonical model of Cauchy-type potentials with \(_{2}=_{1}=\), it will be independent of dimension, as stated by the following result. Note that Assumption 1 can also cover a general scaling by replacing \(|x|\) with \(c|x|\) for some constant \(c\), which would introduce a multiplicative factor of \(1/c^{2}\) for the lower bound on \(T\). This is expected as e.g., mixing to the Gibbs potential \(c^{2}|x|^{2}\) can be faster than mixing to \(|x|^{2}\) by a factor of \(1/c^{2}\).

**Corollary 1**.: _Consider the generalized Cauchy density \(^{X}_{}(-V_{})\) where \(V_{}\) is as in (1). Let \(X_{t}\) be the solution of the Langevin diffusion, and \(_{t}(X_{t})\). For simplicity, assume the initialization satisfies \(|X_{0}|()\). Then, achieving \((^{X}_{},_{T})\) requires \(T_{}^{-}\)._

The above lower bound implies that LD is a low-accuracy "sampler" for this target density in the sense that it depends polynomially on \(1/\); this dependence gets worse with smaller \(\) as the tails get heavier. It is worth highlighting the gap between the upper bound of [11, Corollary 8], which is \(}1/^{4/}\), and the lower bound in Corollary 1.

**Gaussian proximal sampler.** In the remainder of this section, we prove that the Gaussian proximal sampler, described in Algorithm 1, also suffers from a \((1/)\) rate when the target density is heavy-tailed. In each iteration of Algorithm 1, the first step involves sampling a standard Gaussian random variable \(y_{k}\) centered at the current iterate \(x_{k}\) with variance \( I\); this is a one-step isotropic Brownian random walk. Alternatively, since the Fokker-Planck equation of the standard Brownian motion is the classical heat equation, this step could also be interpreted as an exact simulation of the heat flow; see, for example,  and . Specifically, the density of \(y_{k}\) is the solution to the heat flow at time \(\) with the initial condition being the density of \(x_{k}\). The second step is called the restricted Gaussian oracle (RGO) as coined by ; under which \((x_{k},y_{k})\) is a reversible Markov chain whose stationary density has \(x\)-marginal \(^{X}\).

**Assumption 2**.: _For some \(_{2}_{1} 0\), the target \(^{X}(x)(-V(x))\) with \(V:^{d}\) satisfies_

\[ x^{d})|x|^{2}}{1+|x|^{2}} x, V(x),| V(x)|)|x|}{1+|x|^{2}},  V(x))^{2}}{1+|x|^{2}}.\]

The first condition above also appears in Assumption 1 and the second condition implies the upper bound of Assumption 1; thus, the above assumption is stronger. Note that the generalized Cauchy measure (1) satisfies this assumption with \(_{1}=_{2}=\). Under Assumption 2, we state the following lower bound on the Gaussian proximal sampler and defer its proof to Appendix A.

**Theorem 2**.: _Suppose \(^{X}(-V)\) satisfies Assumption 2. Let \(x_{k}\) denote the \(k^{}\) iterate of the Gaussian proximal sampler (Algorithm 1) with step \(\) and let \(^{X}_{k}(x_{k})\). Then, for any \(>0\),_

\[(^{X},^{X}_{k}) C_{_{1},_{2}}d^{-_ {2}}{2}(1+)}(C_{}(_{0})+_{} k)^{- (1+)}{2}},\]

_where \(_{}\), \(C_{}(_{0})\), and \(C_{_{1},_{2}}\) are defined in Theorem 1._Above, assuming \(|X_{0}|()\) with the same choice of \(\) as in Theorem 1 yields \((_{}^{X},_{k}^{X})_{_{1},_{2}} -_{2}}{2}\). Note that in order for the RGO step to be efficiently implementable, we need to have a sufficiently small \(\). The state-of-the-art implementation of RGO requires a step size of order \(=}(1/(Ld^{1/2}))\) when \(V\) has \(L\)-Lipschitz gradients . With this choice of step size, the above lower bound requires at least \(N=_{_{1},_{2}}Ld^{1/2+(_{1}-_{2})/_{2}} (1/)^{2/_{2}}\) iterations. The assumptions in Theorem 2 once again cover the canonical examples of generalized Cauchy densities, where we have \(L=d+\), which simplifies the lower bound as follows.

**Corollary 2**.: _Consider the generalized Cauchy density \(_{}^{X}(-V_{})\) where \(V_{}\) is as in (1). Let \(x_{k}\) denote the \(k^{}\) iterate of the Gaussian proximal sampler, and define \(_{k}^{X}(x_{k})\), and choose the step size \(=}(1/(Ld^{1/2}))\). If we assume \(|X_{0}|()\) for simplicity, then achieving \((_{}^{X},_{N}^{X})\) requires \(N_{}d^{}^{-} \) iterations._

We emphasize that the above lower bound is of order \((1/)\) as advertised. Thus, the RGO-based proximal sampler can only yield a low-accuracy guarantee in this setting.

## 3 Stable Proximal Sampler and the Restricted \(\)-Stable Oracle

Having characterized the limitations of Gaussian oracles for heavy-tailed sampling, thereby answering **Q1**, in what follows, we will focus on **Q2** and construct proximal samplers based on the \(\)-stable oracle, and prove that they achieve high-accuracy guarantees when sampling from heavy-tailed targets. First, we provide a basic overview of \(\)-stable processes and fractional heat flows.

**Isotropic \(\)-stable process.** For \(t 0\), let \(X_{t}^{()}\) be the isotropic stable Levy process in \(^{d}\), starting from \(x^{d}\), with the index of stability \((0,2]\), defined uniquely via its characteristic function \(_{x}e^{i(,X_{t}^{()}-x)}=e^{-t||^{}}\). When \(=2\), \(X_{t}^{(2)}\) is a scaled Brownian motion, and when \(0<<2\), it becomes a pure Levy jump process in \(^{d}\). The transition density of \(X_{t}^{()}\)is then given by

\[p^{()}(t;x,y)=p_{t}^{()}(y-x) p_{t}^{( )}(y)=(2)^{-d}_{^{d}}(-t||^{})e^{-i(,y )},\] (2)

where the second equation above is the inverse Fourier transform of the characteristic function, thus returns the density. The transition kernel and the density in (2) have closed-form expressions for the special cases \(=1,2\). In particular, when \(=1\), \(p_{t}^{(1)}\) reduces to a Cauchy density with degrees of freedom \(=1\), i.e. \(p_{t}^{(1)}(y)(|y|^{2}+t^{2})^{-(d+1)/2}\). We finally note that the isotropic stable Levy process \(X_{t}^{()}\) displays self-similarity like the Brownian motion; the processes \(X_{at}^{()}\) and \(a^{1/}X_{t}^{()}\) have the same distribution. This property is crucial in the development of the stable proximal sampler.

**Fractional heat flow.** The equation \(_{t}u(t,x)=-(-)^{/2}u(t,x)\) with the condition \(u(0,x)=u_{0}(x)\) is an extension of the classical heat flow, and is referred to as the fractional heat flow. Here, \(-(-)^{/2}\) is the fractional Laplacian operator with \((0,2]\), which is the infinitesimal generator of the isotropic \(\)-stable process. For \(=2\), it reduces to the standard Laplacian operator \(\).

**Stable proximal sampler.** Let \((x,y)\) be a joint density such that \((x,y)^{X}(x)p^{()}(;x,y)\), where \(^{X}\) is the target and \(p^{()}(;x,y)\) is the transition density of the \(\)-stable process, introduced in (2). It is easy to verify that (i) the \(X\)-marginal of \(\) is \(^{X}\), (ii) the conditional density of \(Y\) given \(X\) is \(^{Y|X}(|x)=p^{()}(;x,)\), (iii) the \(Y\)-marginal is \(^{Y}=^{X} p_{}^{()}\), i.e. \(^{Y}\) is obtained by evolving \(^{X}\) along the \(\)-fractional heat flow for time \(\), and (iv) the conditional density of \(X\) given \(Y\) is \(^{X|Y}(|y)^{X}()p^{()}(;,y)\). Based on these, we introduce the following stable oracle.

**Definition 1** (Restricted \(\)-Stable Oracle).: _Given \(y^{d}\), an oracle that outputs a random vector distributed according to \(^{X|Y}(|y)\), is called the Restricted \(\)-Stable Oracle (R\(\)SO)._

Note that when \(=2\), the R\(\)SO reduces to the RGO of . The Stable Proximal Sampler (Algorithm 2) with parameter \(\) is initialized at a point \(x_{0}^{d}\) and performs Gibbs sampling on the joint density \(\). In each iteration, the first step involves sampling an isotropic \(\)-stable random vector \(y_{k}\) centered at the current iterate \(x_{k}\), which is a one-step isotropic \(\)-stable random walk. This could also be interpreted as an exact simulation of the fractional heat flow. Indeed, due to the relation between the fractional heat flow and the isotropic stable process, the density of \(y_{k}\) is exactly the solution to the \(\)-fractional heat flow at time \(\) with the initial condition being the density of \(x_{k}\)When \(=2\), the first step reduces to an isotropic Brownian random walk and a simulation of the classical heat flow. The second step calls the R\(\)SO at the point \(y_{k}\).

### Convergence guarantees

We next provide convergence guarantees for the stable proximal sampler in \(^{2}\)-divergence assuming access to the R\(\)SO. Similar results for a practical implementation are presented in Section 3.2. To proceed, we introduce the fractional Poincare inequality, first introduced in  to characterize a class of heavy-tailed densities including the canonical Cauchy class.

**Definition 2** (Fractional Poincare Inequality).: _For \((0,2)\), a probability density \(\) satisfies a \(\)-fractional Poincare inequality (FPI) if there exists a positive constant \(C_{()}\) such that for any function \(:^{d}\) in the domain of \(_{}^{()}\), we have_

\[_{}() C_{()}_{}^{( )}().\] (FPI)

_where \(_{}^{()}\) is a non-local Dirichlet form associated with \(\) defined as_

\[_{}^{()}():=c_{d,}_{\{x y\}} }{|x-y|^{(d+)}}x(y)y c_{d,}=((d+)/2)}{^{d/2}| (-/2)|}.\]

**Remark 2**.: _FPI is a weaker condition than Assumption 2. In fact, any density satisfying the first 2 conditions in Assumption 2 satisfies \(\)-FPI for all \(<_{1}\)[13, Theorem 1.1]. In Proposition 2, we show that as \( 2^{-}\), FPI becomes equivalent to the standard Poincare inequality._

In the sequel, \(_{k}^{X}\) denotes the law of \(x_{k}\), \(_{k}^{Y}\) denotes the law of \(y_{k}\), and \(_{k}=_{k}^{X,Y}\) is the joint law of \((x_{k},y_{k})\). We provide the following convergence guarantee under an FPI, proved in Appendix B.2.

**Theorem 3**.: _Assume that \(^{X}\) satisfies the \(\)-FPI with parameter \(C_{()}\) for \((0,2)\). For any step size \(>0\) and initial density \(_{0}^{X}\), the \(k^{}\) iterate of Algorithm 2, with parameter \(\), satisfies_

\[^{2}(_{k}^{X}|^{X})(-k(C_{() }+)^{-1})^{2}(_{0}^{X}|^{X}).\]

As a consequence of Remark 2 and Proposition 2, we recover the result in [10, Theorem 4], by letting \( 2^{-}\). While our results in Theorem 3 are based on Algorithm 2 which requires exact calls to R\(\)SO, the next result, proved in Appendix B.3, shows that even with an inexact implementation of R\(\)SO, the error accumulation is at most linear, and Algorithm 2 still converges quickly.

**Proposition 1**.: _Suppose the R\(\)SO in Algorithm 2 is implemented inexactly, i.e. there exists a positive constant \(_{}\) such that \((_{k}^{X|Y}(|y),_{k}^{X|Y}(|y)) _{}\) for all \(y^{d}\) and \(k 1\), where \(_{k}^{X|Y}(|y)\) is the density of the inexact R\(\)SO sample conditioned on \(y\). Let \(_{k}^{X}\) be the density of the output of the \(k^{}\) step of Algorithm 2 with the inexact R\(\)SO and \(_{k}^{X}\) be the density of the output of \(k^{}\) step Algorithm 2 with the exact R\(\)SO. Then, for all \(k 0\),_

\[(_{k}^{X},_{k}^{X})(_{0 }^{X},_{0}^{X})+k\,_{}.\]

_Further, if \(_{0}^{X}=_{0}^{X}\), for any \(K K_{0}\), we get \((_{k}^{X},^{X})\), if \(_{}/2K\), where the constant \(K_{0}=(1+C_{()}^{-1})(^{2}(_{0}^ {X}|^{X})/^{2})\) with \(C_{()}\) being the \(\)-FPI parameter of \(^{X}\)._

### A practical implementation of R\(\)SO

In the sequel, we introduce a practical implementation of R\(\)SO when \(=1\). For this, we consider the case when the target density \(^{X} e^{-V}\) satisfies the \(1\)-FPI with parameter \(C_{(1)}\). A more thorough implementation of R\(\)SO for other values of \(\) will be investigated in future work.

**Assumption 3**.: _There exist constants \(,L>0\) such that for any minimizer \(x^{*}_{y^{d}}V(y)\) and for all \(x^{d}\), \(V\) satisfies \(V(x)-V(x^{*}) L|x-x^{*}|^{}\)._Algorithm 3 provides an exact implementation of R\(\)SO for \(=1\) via rejection sampling. Inputs to this algorithm are the intermediate points \(y_{k}\) in the stable proximal sampler (Algorithm 2). Note that Algorithm 3 requires a global minimizer of \(V\), which is always assumed to exist, which guarantees that the acceptance probability is non-trivial. It generates proposals with density \(p^{(1)}(;,y)\) and utilizes that \(p^{(1)}\) is a Cauchy density and Cauchy random vectors can be generated via ratios between a Gaussian random vector and square-root of a \(^{2}\) random variable. Finally, the accept-reject step ensures that the output \(x\) has density \(^{X|Y}(|y) e^{-V}p^{(1)}(;,y)\). This makes Algorithm 3 a zeroth-order algorithm requiring only access to function evaluations of \(V\). Under Assumption 3, by choosing a small step-size, we can control the expected number of rejections in Algorithm 3. We now state the iteration complexity of our stable proximal sampler with this R\(\)SO implementation in the following result, whose proof is provided in Appendix B.3.

**Corollary 3**.: _Assume \(V\) satisfies Assumption 3. If we choose the step-size \(=(d^{-}L^{-})\), then Algorithm 3 implements the R\(\)SO with \(=1\), with the expected number of zeroth-order calls to \(V\) of order \([(L|y_{k}|^{})]\). Further assume \(^{X}\) satisfies \(1\)-FPI with parameter \(C_{(1)}\). Suppose we run Algorithm 2 with R\(\)SO implemented for with \(=1\) by Algorithm 3. Then, to return a sample which is \(\)-close in \(^{2}\)-divergence to the target, the expected number of iterations required by Algorithm 2 is_

\[C_{(1)}d^{}L^{}(^{2} (_{0}^{X}|^{X})/).\]

Note that the above result provides a high-accuracy guarantee for the implementable version of the stable proximal sampler (Algorithm 3) for a class of heavy-tailed targets, overcoming the fundamental barrier established in Theorem 2 for the Gaussian proximal sampler (i.e., Algorithm 1). A numerical illustration of this improvement is provided in Appendix D by sampling from student-t distributions.

**Remark 3**.: _(1) Finding a global minimizer of the potential \(V\) can be hard, which could be avoided if a lower bound on the potential \(V\) is available; see Appendix B.3. (2) A trivial bound for \([(L|y_{k}|^{})]\) is \((LM)\) for \(M=_{^{X}}[|X|^{}]+^{2}(_{0}^{X}|^{X})_{ ^{X}}[|X|^{2}]^{}\). Since our main focus is high vs low accuracy samplers, deriving a sharper bound is beyond the scope of the current paper._

### Illustrative examples

To illustrate our results, we now apply the proximal algorithms to sample from Cauchy densities and discuss the complexity of both the ideal sampler (Algorithm 2) in which we can choose any \((0,2)\) and the implementable version with \(=1\) (Algorithm 3). For the ideal sampler, we can choose \(\) for any degrees of freedom \(>0\), and apply Theorem 3 since \(_{}\) satisfies a \(\)-FPI .

**Corollary 4**.: _For any \(>0\), consider the generalized Cauchy target \(_{}(-V_{})\) with \(V_{}\) defined in (1). For the stable proximal sampler with parameter \((0,2)\) and \(\) (i.e., Algorithm 2), suppose we set the step-size \((0,1)\) and draw the initial sample from the standard Gaussian density. Then, the number of iterations required by Algorithm 2 to produce an \(\)-accurate sample in \(^{2}\)-divergence is \((C_{()}^{-1}(d/))\), where \(C_{()}\) is the \(\)-FPI parameter of \(_{}\)._

For the implementable sampler, since the parameter \(\) is fixed to be \(1\), whether a suitable FPI is satisfied or not depends on the degrees of freedom \(\). Specifically, when \( 1\), \(1\)-FPI is satisfied and Corollary 5 applies. When \((0,1)\), on the other hand, \(1\)-FPI is not satisfied. To tackle this issue, we prove convergence guarantees for the proximal sampler under a weak fractional Poincare inequality; the next corollary, proved in Appendix B.4, summarizes these results.

**Corollary 5**.: _For the Cauchy target \(_{}(-V_{})\) where \(V_{}\) is defined in (1), we consider Algorithm 2 with \(=1\), a standard Gaussian initialization, and R\(\)SO implemented by Algorithm 3._

1. _When_ \( 1\)_, if we set the step-size_ \(=d^{-}(d+)^{-4}\)_, the expected number iterations required by Algorithm_ 2 _to output a sample which is_ \(\)_-close in_ \(^{2}\)_-divergence to the target is of order_ \(C_{(1)}d^{}(d+)^{4}(d/) \)_, where_ \(C_{(1)}\) _is the_ \(1\)_-FPI parameter of_ \(_{}\)2. _When_ \((0,1)\)_, if we set the step-size_ \(=d^{-}(d+)^{-}\)_, the expected number of iterations required by Algorithm_ 2_, to output a sample which is_ \(\)_-close in_ \(^{2}\)_-divergence to the target is of order_ \(}c^{}d^{+},cd^{+}^{- {}+1}}\)_, where_ \(c\) _is the positive constant given in (_16_). Here,_ \(}\) _hides the polylog factors on_ \(d\) _and_ \(1/\)_._

The stable proximal sampler (Algorithm 2) is a high accuracy sampler for the class of generalized Cauchy targets, as long as \(\), meaning that it achieves log(\(1/\)) iteration complexity. The improvement from poly(\(1/\)) to log(\(1/\)) separates the stable proximal sampler and the Gaussian proximal sampler in the task of heavy-tailed sampling. When we use the rejection-sampling implementation with parameter \(=1\) (Algorithm 3), iteration complexity goes through a phase transition as the tails get heavier. When the generalized Cauchy density has a finite mean (\(>1\)), we achieve a high-accuracy sampler with log(\(1/\)) iteration complexity. However, without a finite mean (i.e., \((0,1)\)), the algorithm becomes a low-accuracy sampler with poly(\(1/\)) complexity. Even in this low-accuracy regime, the implementable stable proximal sampler outperforms the Gaussian one, as originally highlighted in Table 1. Last, we claim that the poly(\(1/\)) complexity of Algorithms 2 and 3 is not due to a loose analysis, as we show \((1/)\) lower bounds in the following section.

### Lower bounds for the stable proximal sampler

We now study lower bounds on the stable proximal sampler to sample from the class of target densities satisfying Assumption 2, which includes the generalized Cauchy target. Recall that Assumption 2 implies the FPI used in Theorem 3. The result below, proved in Appendix C, complements Theorem 3, showing the impossibility of achieving \((1/)\) rates for a sufficiently large \(\).

**Theorem 4**.: _Suppose \(^{X}(-V)\) with \(V\) satisfying Assumption 2 and \((d+_{2})}{d+_{1}}< 2\). Let \(x_{k}\) denote the \(k^{}\) iterate of Algorithm 2 with parameter \(\) and step size \(\), and let \(_{k}^{X}(x_{k})\). Then for any \((d+_{2})}{d+_{1}},\), and \(g(d,_{1},_{2},)=_{2}/\{(d+_{1})-_{2}(d+_{2})\}\), we have_

\[(^{X},_{k}^{X}) C_{_{1},_{2},}d^{)g(d,_{1},_{2},)}{2}}[(1+|x_{0}|^{2})^{ }]+m_{}^{()}k^{+1}^{}^{-(d+_{2})g(d,_{1},_{2},)},\]

_where \(C_{_{1},_{2},}\) is a constant depending only on \(_{1},_{2},\), and \(m_{}^{()}\) is the \(^{}\) absolute moment of the \(\)-stable random variable with density \(p_{1}^{()}\) defined in (2)._

**Remark 4**.: _The parameter \(\) in Theorem 4 can be chosen arbitrarily close to \(\). Specifically, if we assume \(|X_{0}|()\), then with the choice of \(=-)} {(^{-1})}\), we have_

\[(^{X},_{k}^{X})_{_{1},_{2},} d^{)g(d,_{1},_{2},)}{2}}d^{ }+m_{}^{()}k^{+1}^{-(d+_{2})g(d,_{1},_{2},)},\]

_where \(\) hides \((d/)\) factors._

The \(^{}\) absolute moment of the \(\)-stable random variable depends on the choice of \(\) and the dimension \(d\). It is hard to find an explicit formula of \(m_{}^{()}\) in general. An explicit formula is only available in some special cases, such as \(=1,2\). Specializing Theorem 4 for the generalized Cauchy potential (i.e., \(_{1}=_{2}\)) we obtain the following explicit result.

**Corollary 6**.: _Let \((0,2]\). Suppose \(_{}(-V_{})\) where \(V_{}(x)\) is as in (1) for some \((0,)\). Let \((x_{k})_{k 0}\) be the output of Algorithm 2 with parameter \(\) and step-size \(>0\), and \(_{k}^{X}(x_{k})\) for all \(k 0\). Then for any \((,)\),_

\[(_{k}^{X},_{}) C_{,}d^{}[(1+|x_{0}|^{2})^{}]+m_{}^{()}k ^{+1}^{}^{-}.\]

_where \(m_{}^{()}\) is the \(^{}\) absolute moment of the \(\)-stable random variable with density \(p_{1}^{()}\) as in (2)._

For the rejection sampling implementation in Algorithm 3, \(=1\) and \(m_{}^{(1)}=(d^{})\) for all \(<1\) (see Appendix B.1). Notice that to implement the R\(\)SO in the Stable proximal sampler efficiently, we need a sufficiently small step-size \(\). When the target potential satisfies Assumption 3, i.e. \(V\) is \(\)-Holder continuous with parameter \(L\), we require \(=(d^{-}L^{-})\) to ensure R\(\)SO can be implemented with \((1)\) queries. Therefore, if we choose \(=(d^{-}L^{-})\), the minimum number of iterations we need to get an \(\)-error in \(\) is

\[_{,}^{-}d^{}L^{}.\]For the generalized Cauchy potential with \((0,1)\), we have \(=/4\) and \(L=(d+)/\), which leads to the following corollary.

**Corollary 7**.: _Suppose \(_{}^{X}(-V_{})\) is the generalized Cauchy density with \((0,1)\). Let \(x_{k}\) denote the \(k\)-th iterate of the stable proximal sampler with \(=1\) (Algorithm 3), and \(_{k}^{X}(x_{k})\). If we choose the step size \(=(L^{-}d^{-})\) where \(L=\) is the \(/4\)-Holder constant of \(V_{}\), and assume, for simplicity, \(|x_{0}|()\), then, \((_{}^{X},_{N}^{X})\) requires \(N_{,}d^{}^{-}\), for any \((,1)\). Further, by choosing \(=(,1-)\), we obtain_

\[N_{}d^{}^{-},(_{}^{X},_{N}^{X}) .\]

The above result shows that when implementing the R\(\)SO in Algorithm 2 with Algorithm 3, to sample from generalized Cauchy targets with \((0,1)\), we can at best have an iteration complexity of order \((1/)\), matching the upper bounds in Corollary 5 up to certain factors.

## 4 Overview of Proof Techniques

**Lower bounds.** We build on the techniques developed in . Let \(_{t}\) denotes the law of LD along its trajectory. To proceed, we need some \(G:^{d}\) for which we can upper bound \(_{t}(G) G_{t}\), and some \(f:^{d}\) that satisfies \(^{X}(G y) f(y)\) for all \(y_{+}\). After finding the candidates \(G\) and \(f\), Lemma 1 in Appendix A guarantees \((^{X},_{t})_{y_{+}}f(y)-_{t}(G)/y\). This technique relies on choosing \(G\) such that it has heavy tails under \(^{X}\) leading to a large \(f(y)\), while having light tails along the trajectory, thus small \(_{t}(G)\). By picking \(G=( V)\) with \( 1\), one can immediately observe that \(^{X}(G)=\), thus \(G\) indeed has heavy tails under \(^{X}\).

To control \(_{t}(G)\) along the trajectory, one can use the generator of LD to bound \(_{t}_{t}(G)\). Recall the generator of LD, \(_{}()=()- V,\). Therefore, with a choice of \(G=( V)\), controlling \(_{t}_{t}(G)\) requires bounding the first and second derivatives of \(V\). To avoid making extra assumptions for \(V\) in the analysis of LD, we instead construct \(G\) based on a surrogate potential \((x)=}{2}(1+|x|^{2})\), which is an upper bound to the potential \(V\). We then estimate \(f\) based on this surrogate potential in Lemma 2, and control the growth of \(_{t}(G)\) in Lemma 3. Combined with Lemma 1, this leads to the proof of Theorem 1, with the details provided in Appendix A.

For the Gaussian proximal sampler, bounding \(_{k}^{X}(G)\) requires controlling the expectation of \(G\) along the forward and backward heat flow. For the particular choice of \(G=( V)\), we show in Lemma 4 that the growth of \(_{k}^{X}(G)\) can be controlled only by considering a forward heat flow with the corresponding generator \(_{}=\). Therefore, given additional estimates on the second derivatives of \(V\), we bound the growth of \(_{k}^{X}(G)\) in Lemma 5. Once this bound is achieved, we can invoke Lemma 1 to finish the proof of Theorem 2.

**Upper bounds.** Our upper bound analysis builds on that by  in the specific ways discussed next. We consider the change in \(^{2}\) divergence when we apply the two operations to the law \(_{k}^{X}\) to the iterates and the target \(^{X}(i)\) evolving the two densities along the \(\)-fractional heat flow for time \(\) and \((ii)\) applying the R\(\)SO to the resulting densities. For the step (i), it is required to show that the solution along the fractional heat flow of the stable proximal sampler at any time, satisfies FPI. To show this, \((a)\) the convolution property of the FPI is proved in Lemma 6, and \((b)\) the FPI parameter for the stable process follows from [1, Theorem 23]. In Proposition 3, it is then shown that the \(^{2}\) divergence decays exponentially fast along the fractional heat flow under the assumption of FPI. The aforementioned results enable us to prove the exponential decay of \(^{2}\) divergence along the fractional heat flow under FPI in Proposition 3. To deal with the step (ii) above, we use the data processing inequality; see Proposition 3. These two steps together, enable us to derive the stated upper bounds for the stable proximal sampler.

## 5 Discussion

We showed the limitations of Gaussian proximal samplers for high-accuracy heavy-tailed sampling, and proposed and analyzed stable proximal samplers, establishing that they are indeed high-accuracy algorithms. We now list a few important limitations and problems for future research: (i) It is important to develop efficiently implementable versions of the stable proximal sampler for all values of \((0,2)\), and characterize their complexity in terms of problem parameters, (ii) Gaussian proximal samplers can be interpreted as a proximal point method for approximating the entropic regularized Wasserstein gradient flow of the KL objective . This leads to the question, _can we provide a variational interepretation of the stable proximal sampler?_ A potential approach is to leverage the results by  on gradient flow interpretation of jump processes corresponding to the fractional heat equation, (iii) It is possible to use a non-standard Ito process in the proximal sampler (in place of the \(\)-stable diffusion); see, for example, . With this modification, it is interesting to examine the rates under weighted Poincare inequalities that also characterize heavy-tailed densities. There are two difficulties to overcome here: \((a)\) How to generate an exact non-standard Ito process? \((b)\) How to implement the corresponding Restricted non-standard Gaussian Oracle, which requires the zeroth order information of the transition density of the Ito process? In certain cases, non-standard Ito diffusion can be interpreted as a Brownian motion on an embedded sub-manifold; thus, the approach in  might be useful.