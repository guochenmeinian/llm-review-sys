# Generative Modeling of

Molecular Dynamics Trajectories

 Bowen Jing11Hannes Stark11Tommi Jaakkola1Bonnie Berger12

1CSAIL, Massachusetts Institute of Technology

2Dept. of Mathematics, Massachusetts Institute of Technology

{bjing, hstark}@mit.edu, tommi@csail.mit.edu, bab@mit.edu

Equal contribution.

###### Abstract

Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.

## 1 Introduction

Numerical integration of Newton's equations of motion at atomic scales, known as _molecular dynamics_ (MD), is a widely-used technique for studying diverse molecular phenomena in chemistry, biology, and other molecular sciences (Alder and Wainwright, 1959; Rahman, 1964; Verlet, 1967; McCammon et al., 1977). While general and versatile, MD is computationally demanding due to the large separation in timescales between integration steps and relevant molecular phenomena. Thus, a vast body of literature spanning several decades aims to accelerate or enhance the sampling efficiency of MD simulation algorithms (Ryckaert et al., 1977; Darden et al., 1993; Sugita and Okamoto, 1999; Laio and Parrinello, 2002; Anderson et al., 2008; Shaw et al., 2009). More recently, learning surrogate models of MD has become an active area of research in deep generative modeling (Noe et al., 2019; Zheng et al., 2023; Klein et al., 2024; Schreiner et al., 2024; Jing et al., 2024). However, existing training paradigms fail to fully leverage the rich dynamical information in MD training data, restricting their applicability to a limited set of downstream problems.

In this work, we propose MDGen, a novel paradigm for fast, general-purpose surrogate modeling of MD based on _direct generative modeling of simulated trajectories_. Different from previous works, which learn the autoregressive transition density or equilibrium distribution of MD, we formulate end-to-end generative modeling of full trajectories viewed as _time-series_ of 3D molecular structures. Akin to how image generative models were extended to videos (Ho et al., 2022), our framing of the problem augments single-structure generative models with an additional time dimension, opening the door to a larger set of forward and inverse problems to which our model can be applied. Whenprovided (and conditioned on) the initial "frame" of a given system, such generative models serve as familiar surrogate forward simulators of the reference dynamics. However, by providing other kinds of conditioning, these "molecular video" generative models also enable highly flexible applications to a variety of inverse problems not possible with existing surrogate models. In sum, we formulate and showcase the following novel capabilities of MDGen:

* _Forward simulation_--given the initial frame of a trajectory, we sample a potential time evolution of the molecular system.
* _Interpolation_--given the frames at the _two endpoints_ of a trajectory, we sample a plausible path connecting the two. In chemistry, this is known as _transition path sampling_ and is important for studying reactions and conformational transitions.
* _Upsampling_--given a trajectory with timestep \( t\) between frames, we upsample the "frame" by a factor of \(M\) to obtain a trajectory with timestep \( t/M\). This infers fast motions from trajectories saved at less frequent intervals.
* _Inpainting_--given part of a molecule and its trajectory, we _generate the rest of the molecule_ (and its time evolution) to be consistent with the known part of the trajectory. This ability could be applied to design molecules to scaffold desired dynamics.

These tasks are conceptually illustrated in Figure 1. While the forward simulation task aligns with the typical modeling paradigm of approximating the data-generating process, the others represent novel capabilities on scientifically important inverse problems _not straightforward to address even with MD itself_. As such, our framework presents a new perspective on how to unlock value from MD simulation with machine learning towards diverse downstream objectives. We highlight further exciting possibilities opened up by our framework in Section 5.

We demonstrate our framework on MD simulations of tetrapeptides (i.e., length-4 peptides), with preliminary extensions to full-sized protein monomers. To do so, we parameterize molecular trajectories in terms of sidechain torsions and residue offsets with respect to conditioning _key frames_, obtaining a generative modeling task over a 2D array of \(SE(3)\)-invariant tokens rather than residue frames or point clouds. In this parameterization, we can then employ a Scalable Interpolant Transformer (SiT) (Ma et al., 2024) as our flow-based generative backbone, avoiding the more restrictive geometric architectures commonly used for molecular structure. Furthermore, by replacing the time-wise attention in SiT with the long-context architecture Hyena (Poli et al., 2023), we provide proof-of-concept of scaling up to trajectories of 100k frames, enabling a wide range of timescales and dynamical processes to be captured with a single model generation.

We evaluate MDGen on the forward simulation, interpolation, upsampling, and inpainting tasks on tetrapeptides in a _transferable_ setting (i.e., unseen test peptides). Our model accurately reproduces free energy surfaces and dynamical content such as torsional relaxation and Markov state fluxes, provides realistic transition paths between arbitrary pairs of metastable states, and recovers fast dynamical phenomena below the sampling threshold of coarse-timestep trajectories. In preliminary

Figure 1: (_Left_) Tasks: generative modeling of MD trajectories addresses several tasks by conditioning on different parts of a trajectory. (_Right_) Method: We tokenize trajectories of \(T\) frames and \(L\) residues into an \((T L)\)-array of SE(3)-invariant tokens encoding roto-translation offsets from _key frames_ and torsion angles. Using _stochastic interpolants_, we generate arrays of such tokens from Gaussian noise.

steps toward dynamics-scaffolded design, we show that molecular inpainting with MDGen obtains much higher sequence recovery than inverse folding methods based on one or two static frames. Finally, we evaluate MDGen on simulation of proteins and find that it outperforms MSA subsampling with AlphaFold (Del Alamo et al., 2022) in terms of recovering ensemble statistical properties.

## 2 Background

**Molecular dynamics.** At a high level, the aim of molecular dynamics is the integrate the equations of motion \(M_{i}}_{i}=-_{_{i}}U(_{1} _{N})\) for each particle \(i\) in a molecular configuration \((_{1}_{N})^{3N}\), where \(M_{i}\) is the mass and \(U\) is the potential energy function (or _force field_) \(U:^{3N}\). However, these equations of motion are often modified to include a _thermost_ in order to model contact with surroundings at a given temperature. For example, the widely-used _Langevin thermostat_ transforms the equations of motion into a stochastic diffusion process:

\[d_{i}=_{i}/M_{i}\,dt, d_{i}=-_{ _{i}}U\,dt-_{i}\,dt+ kT}\,d\] (1)

where \(_{i}\) are the momenta. By design, this process converges to the _Boltzmann distribution_ of the system \(p(_{1}_{N}) e^{-U/kT}\). To incorporate interactions with solvent molecules--ubiquitous in biochemistry--one includes a box of surrounding solvent molecules as part of the molecular system (explicit solvent) or modifies the force field \(U\) to model their effects (implicit solvent). In either case, only the positions \(_{i}\) of non-solvent atoms are of interest, and their time evolution constitutes (for our purposes) the _MD trajectory_.

**Deep learning for MD.** An emerging body of work seeks to approximate the distributions over configurations \(=(_{1}_{N})\) arising from Equation 1 with deep generative models. Fu et al. (2023), Timewarp (Klein et al., 2024), and ITO (Schreiner et al., 2024) learn the _transition density_\(p(_{t+ t}_{t})\) and emulate MD trajectories via simulation rollouts of the learned model. On the other hand, _Boltzmann generators_(Noe et al., 2019; Kohler et al., 2021; Garcia Satorras et al., 2021; Midgley et al., 2022, 2024) directly approximate the stationary Boltzmann distribution, forgoing any explicit modeling of dynamics. In particular, Boltzmann-targeting diffusion models trained with frames from MD trajectories have demonstrated promising scalability and generalization to protein systems (Zheng et al., 2023; Jing et al., 2024). However, these works have focused exclusively on forward simulation and have not explored joint modeling of entire trajectories \((_{t}_{t+N t})\) or the inverse problems accessible under such a formulation.

**Stochastic interpolants.** We build our MD trajectory generative model under the _stochastic interpolants_ framework: Given a continuous distribution \(p_{1} p_{}\) over \(^{n}\), stochastic interpolants (Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022), provide a method for learning continuous flow-based models \(d=v_{}(,t)\,dt\) transporting a prior distribution \(p_{0}\) (e.g., \(p_{0}(0,)\)) to the data \(p_{1}\). To do so, one defines intermediate distributions \(_{t} p_{t},t(0,1)\) via \(_{t}=_{t}_{1}+_{t}_{0}\) where \(_{0} p_{0}\) and \(_{1} p_{1}\) and the interpolation path satisfies \(_{0}=_{1}=0\) and \(_{1}=_{0}=1\). A neural network \(v_{}:^{n}^{n}\) is trained to approximate the time-evolving flow field

\[v_{}(_{t},t) v(_{t},t)_{ _{0},_{1}|_{t}}[_{t}_{1} +_{t}_{0}]\] (2)

which satisfies the transport equation \( p_{t}/ t+(p_{t}v_{t})=0\). Hence, at convergence, noisy samples \(_{0} p_{0}\) can be evolved under \(v_{}\) to obtain data samples \(_{1} p_{1}\). When parameterized with transformers (Vaswani et al., 2017), stochastic interpolants are state-of-the-art in image generation (Esser et al., 2024). In particular, we adopt the notation, architecture, and training framework of Scalable Interpolant Transformer (SiT) (Ma et al., 2024), to which we refer for further exposition.

## 3 Method

### Tokenizing Peptide Trajectories

Given a chemical specification of a molecular system with \(N\) atoms, our aim is to learn a generative model over time-series \([_{1},_{T}]\) of corresponding molecular structures \(_{i}^{3N}\) for some trajectory length \(T\). In this work, we specialize to MD trajectories of short peptides (Sections 4.1-4.4) or single-chain proteins (4.4). Thus, our chemical specifications are amino acid sequences\(A=\{1 20\}^{L}\), and we adopt an \(SE(3)\)-based parameterization of peptide structures (Jumper et al., 2021; Yim et al., 2023). In this parameterization, the all-atom coordinates of each amino acid residue are _implicitly_ described by a roto-translation (i.e., element of \(SE(3)\)) corresponding to the rigid body motion of the residue, and seven torsion angles describing its conformation:

\[_{t}^{l}=[g,_{1},_{7}], g SE(3),,([SE(3)^{7}]^{L})^{T}\] (3)

Throughout, subscripts indicate time and superscripts residue indices. The undefined torsion angles can be randomized and are unsupervised for residues with fewer than seven torsion angles.

Traditionally, equivariant architectures have been required for geometry-aware processing of polypeptide structures. However, to learn a scalable generative model over this space of roto-translations and torsion angles, we seek to represent each \(_{t}^{l}\) in terms of an \(SE(3)\)-invariant feature vector--a _token_ suitable for processing by vanilla transformers. To obtain such a vector, we leverage the fact that we are concerned with _conditional trajectory generation_--meaning that there always exists at least one frame in the trajectory with un-noised roto-translations, which we do not need to generate and can reference in the modeling process. Inspired by analogy to video compression, we refer to such frames as _key frames_. We can then obtain \(SE(3)\)-invariant tokens by parameterizing the roto-translations of remaining structures _relative_ to the key frames.

In more detail, given \(K\) key frames at times \(t_{1} t_{K}\) we tokenize residue \(j\) in frame \(t\) as:

\[_{t}^{j}=[([g_{t_{1}}^{j}]^{-1}g_{t}^{j}),, ([g_{t_{K}}^{j}]^{-1}g_{t}^{j}),([_{t}^{j}]_{1}), ([_{t}^{j}]_{7})]^{7K+14}\] (4)

where \(g_{t}^{j} SE(3)\) represents the roto-translation and \([_{t}^{j}]_{i}\) the torsion angles of residue \(j\) at frame \(t\). Here, \(:SE(3)^{7}\) parameterizes an element of \(SE(3)\) in terms of a unit quaternion and translation vector, and \(:^{2}\) converts torsion angles to points on the unit circle. We thus obtain a (\(7K+14\))-dimensional array for each residue in every frame. Because the relative roto-translations and torsion angles are both \(SE(3)\)-invariant, in this manner we can represent a polypeptide molecular trajectory as an \((T L)\)-array of \(SE(3)\)-invariant tokens.

To _untokenize_ a generated trajectory of tokens to all-atom coordinates \(_{t}^{3N}\), we first convert each predicted quaternion and translation vector to a relative roto-translation and apply it to the key frame(s), obtaining absolute roto-translations. We then read off the torsion angles from the unit circle and assemble the all-atom coordinates as implemented in Jumper et al. (2021), averaging the reconstructions from different key frames if needed.

### Flow Model Architecture

Our base modeling task is to generate a distribution over \(^{T L(7K+14)}\) conditioned on roto-translations of one or more key frames \(g_{t_{1}} g_{t_{K}}\), and (in most settings) amino acid identities \(A\). To do so, we learn a flow-based model via the stochastic interpolant framework described in SiT (Ma et al., 2024) and parameterize a velocity network \(v_{}( g_{t_{1}} g_{t_{K}},A):^{T L (7K+14)}^{T L(7K+14)}\). To condition on the key frames and amino acids, we first provide the sequence embedding to several IPA layers (Jumper et al., 2021) that embed the key frame roto-translations; these conditioning representations (which are \(SE(3)\)-invariant) are broadcast across the time axis and added to the input embeddings. The main trunk of the network consists of alternating attention blocks across the residue index and across time, with the construction of each block closely resembling DiT (Peebles and Xie, 2023). Sidechain torsions and roto-translation offsets, when available, are directly provided to the model as conditioning tokens. Further details are provided in Appendix A.1.

In the molecular inpainting setting where we also _generate_ the amino acid identities, we additionally require a generative framework over these discrete variables. While several formulations of discrete diffusion or flow-matching are available (Hoogeboom et al., 2021; Austin et al., 2021; Campbell et al., 2022, 2024), we select Dirichlet flow matching (Stark et al., 2024) as it is most compatible with the continuous-space, continuous-time stochastic interpolant framework used for the positions. Specifically, we place the amino acid identities on the 20-dimensional probability simplex (one per amino acid), augment the token representations with these variables, and regress against a \(T L(7K+14+20)\)-dimensional vector field. Further details are provided in Appendix A.2.

### Conditional Generation

We present the precise specifications of the various conditional generation settings in Table 1. Depending on the task, we choose the key frames to be the first frame \(g_{1}\) or the first and last frames \(g_{1},g_{T}\). Each conditional generation task is further characterized by providing the ground-truth tokens of known frames or residues as additional inputs to the velocity network. Meanwhile, mask tokens are provided for the unknown frames and residues that the model generates. For example, in the upsampling setting, we provide ground-truth tokens every \(M\) frames, while mask tokens are provided for all other frames. We note that in the inpainting setting, the model accesses the roto-translations \(g\) of designed residues at the trajectory endpoints via the key frames, constituting a slight departure from the full inpainting setting. However, these residues are not observed for intermediate frames, and their identities are never provided to the model.

## 4 Experiments

We evaluate MDGen on its ability to learn from MD simulations of training molecules and then sample trajectories for unseen molecules. We focus on _tetrapeptides_ as our main molecule class for evaluation as they provide nontrivial chemical diversity while remaining small enough to tractably simulate to equilibrium (Klein et al., 2024). Sections 4.1-4.3 thoroughly evaluate our model on forward simulations, interpolation / transition path sampling, and trajectory upsampling on test peptides. Section 4.4 provides proof-of-concept and preliminary exploration of additional tasks--namely, inpainting for dynamics-conditioned design, long trajectories with Hyena (Poli et al., 2023), and scaling to simulations of protein monomers. Separate models are trained for each setting.

To obtain tetrapeptide MD trajectories for training and evaluation, we run implicit- and explicit-solvent, all-atom simulations of \(\)3000 training, 100 validation, and 100 test tetrapeptides for 100 ns. For proteins, we use explicit-solvent, all-atom simulations from the ATLAS dataset (Vander Meersche et al., 2024), which provides three 100 ns trajectories for each of 1390 structurally diverse proteins. Unless otherwise specified, models are trained with trajectory timesteps of \( t=10\) ps. Our default baselines consist of _replicate MD simulations_ ranging from 10 ps to 100 ns, with additional comparisons in each section as appropriate.

Our experiments make extensive use of Markov State Models (MSMs), a widely used coarse-grained representation of molecular dynamics (Prinz et al., 2011; Noe et al., 2013). We obtain an MSM to represent a system by discretizing its MD trajectory (parameterized with torsion angles) into 10 metastable states and estimating the transition probabilities between them. Appendix B provides further details on constructing MSMs and other experimental settings. Additional results, including structural validations and further comparisons with related methods, can be found in Appendix C.

### Forward Simulation

In the _forward simulation_ setting, we train a model to sample 10 ns trajectories conditioned on the first frame. By chaining together successive model rollouts at inference time, we obtain 100 ns trajectories for each peptide to compare with ground-truth simulations. We evaluate if these sampled trajectories (1) match the structural distribution of trajectories from MD, (2) accurately capture the dynamical content of MD, and (3) traverse the state space in less wall-clock time than MD.

**Distributional similarity.** We report the Jensen-Shannon divergence (JSD) between the ground-truth and emulated trajectories along various collective variables shown in Figure 2 and Table 2. The first

   Setting & Key frames & Generate & Conditioned on & Token dim. \\  Forward simulation & \(g_{1}\) & \(g_{1 T},_{1 T}\) & \(g_{1},_{1},A\) & 21 \\ Interpolation & \(g_{1},g_{T}\) & \(g_{1 T},_{1 T}\) & \(g_{1,T},_{1 T},A\) & 28 \\ Upsampling & \(g_{1}\) & \(g_{1 T},_{1 T}\) & \(g_{1+\{1,2,\}M},_{1+\{1,2,\}M},A\) & 21 \\ Inpainting & \(g_{1},g_{T}\) & \(g_{1 T},A\) & \(g_{1 T}^{}\) & 7 (+20) \\   

Table 1: Conditional generation settings. \(g\): roto-translations, \(\): torsions, \(A\): residue identities \(M\): upsampling factor. Superscripts indicate residue index and subscripts indicate frame (time) index. For inpainting, we find that excluding identities and torsions reduces overfitting.

set of these are the individual torsion angles (backbone and sidechains) in each tetrapeptide. The second set of variables are the top independent components obtained from _time-lagged independent components analysis_ (TICA), representing the slowest dynamic modes of the peptide. By each of these collective variables, MDGen demonstrates excellent distributional similarity to the ground truth, approaching the accuracy of replicate 100-ns simulations. To more stringently assess the ability to locate and populate modes in the joint distribution over state space, we build Markov State Models (MSMs) for each test peptide using the MD trajectory, extract the corresponding metastable states, and compare the ground-truth and emulated distributions over metastable states. Our model captures the relative ranking of states reasonably well and rarely misses important states or places high mass on rare states (Figure 2D).

**Dynamical content.** We compute the dynamical properties of each tetrapeptide in terms of the _decorrelation time_ of each torsion angle from the MD simulation and from our sampled trajectory. Intuitively, this assesses if our model can discriminate between slow- and fast-relaxing torsional barriers. The correlation between true and predicted relaxation timescales is plotted in Figure 2F, showing excellent agreement for sidechain torsions and reasonable agreement for backbones. To assess coarser but higher-dimensional dynamical content, we compute the _flux matrix_ between all pairs of distinct metastable states using ground-truth and sampled trajectories and find substantial Spearman correlation between their entries (mean \(=0.67 0.01\); Figure 8). Thus, our simulation rollouts can accurately identify high-flux transitions in the peptide conformational landscape.

**Sampling speed.** Averaged across test peptides, our model samples 100 ns-equivalent trajectories in \(\)60 GPU-seconds, compared to \(\)3 GPU-hours for MD. To quantify the speedup more rigorously,

   C.V. & Ours & 10 ns & 1 ns & 100 ps & 100 ns \\  Torsions (bb) & **.130** &.145 &.212 &.311 &.103 \\ Torsions (sc) & **.093** &.111 &.261 &.403 &.055 \\ Torsions (all) & **.109** &.125 &.240 &.364 &.076 \\  TICA-0 & **.230** &.323 &.432 &.477 &.201 \\ TICA-0,1 joint & **.316** &.424 &.568 &.643 &.268 \\  MSM states & **.235** &.363 &.493 &.527 &.208 \\  Runtime & 60s & 1067s & 107s & 11s & 3h \\   

Table 2: JSD between sampled and ground-truth distributions, with replicate simulations as baselines. 100 ns represents oracle performance.

Figure 2: **Forward simulation evaluations on test peptides. (A) Torsion angle distributions for the six backbone torsion angles from MD trajectories (orange) and sampled trajectories (blue). (B, C) Free energy surfaces along the top two TICA components computed from backbone and sidechain torsion angles. (D) Markov State Model occupancies computed from MD trajectories versus sampled trajectories, pooled across all test peptides (\(n=1000\) states total). (E) Wall-clock decorrelation times of the first TICA component under MD versus our model rollouts. (F) Relaxation times of torsion angles computed from MD versus sampled trajectories, pooled across all test peptides—508 backbone (blue) and 722 sidechain (orange) torsions in total. (G) Torsion angles in the tetrapeptide AAAA colored by the decorrelation time computed from MD (top) and from rollout trajectories (bottom).**

we compute the decorrelation wall-clock times along the slowest independent component from TICA, capturing how quickly the simulation traverses the highest barriers in state space. These times are plotted in Figure 2E, showing that our model achieves a speedup of 10x-1000x over the MD simulation for 78 out of 100 peptides (the other 22 peptides did not fully decorrelate).

### Interpolation

In the _interpolation_ or _transition path sampling_ setting, we train a model to sample 1 ns trajectories conditioned on the first and last frames. For evaluation, we identify the two most well-separated states (i.e., with the least flux between them) for each test peptide and sample an ensemble of 1000 transition paths between them. Figure 3 shows an example of such a sampled path, which passes through several intermediate states on the free energy surface to connect the two endpoints.

To evaluate the accuracy of these sampled transitions, we cannot directly compare with MD trajectories since, in most cases, there are zero or very few 1-ns transitions between the two selected states (by design, the transition is a _rare event_). Thus, we instead discretize the trajectory over MSM metastable states and evaluate the _path likelihood_ under the transition path distribution from the reference MSM (details in Appendix B.3). We also report the fraction of valid paths (i.e., non-zero probability) and the JSD between the distribution of visited states from our path distribution versus the transition path distribution of the reference MSM. For baselines, we sample transition paths from MSMs constructed from replicate MD simulations of varying lengths and compute the same metrics for these (discrete) path ensembles under the reference MSM.

As shown in Figure 3, our paths have higher likelihoods than those sampled from any replicate MD MSM shorter than 100ns, which is the length of the reference MD simulation itself. Moreover, MDGen's ensembles have the best JSDs to the distribution of visited states of the reference MD MSM and the highest fraction on valid non-zero probability paths. Hence, our model enables zero-shot sampling of trajectories corresponding to arbitrary rare transitions for unseen peptides.

### Upsampling

Molecular dynamics trajectories are often saved at relatively long time intervals (10s-100s of picoseconds) to reduce disk storage; however, some molecular motions occur at faster timescales and

Figure 3: **Transition path sampling results. (Top) Intermediate states of one of the 1-nanosecond interpolated trajectories between two metastable states for the test peptide IPGD. (Bottom Left) The corresponding trajectory on the 2D free energy surface of the top two TICA components (more examples in Figure 9). (Bottom Right) Statistics averaged over 100 test peptides and 1000 paths for each of them. Shown are JSD, fraction of drawn paths that are valid transition paths, and average path likelihood of our discretized transitions under the reference MSM compared to discrete transitions drawn from the reference MSM or alternative MSMs built from replica simulations of varying lengths.**

would be missed by downstream analysis of the saved trajectory. In the _upsampling_ setting, we train MDGen to upsample trajectories saved with timestep 10 ps to a finer timestep of 100 fs, representing a 100x upsampling factor. To evaluate if the upsampled trajectories accurately capture the fastest dynamics, we compute the _autocorrelation function_\((_{t}-_{t+ t})\) of each torsion angle in the test peptides as a function of lag time \( t\) ranging from 100 fs to 100 ps.

Representative examples of ground truth, subsampled, and reconstructed autocorrelation functions for two test peptides are shown in Figure 4 (further examples in Figure 10). We further compute the _dynamical content_ as the negative derivative of the autocorrelation with respect to log-timescale, which captures the extent of dynamic relaxations occurring at that timescale (Shaw et al., 2009). These visualizations highlight the significant dynamical information absent from the subsampled trajectory and which are accurately recovered by our model. In particular, our model distinctly recovers the _oscillations_ of certain torsion angles as seen in the non-monotonicity of the autocorrelation function at sub-picosecond timescales; these features are completely missed at the original sampling frequency.

### Additional Tasks

**Inpainting Design.** We aim to sample trajectories conditioned on the dynamics of the two flanking residues of the tetrapeptide; in particular, the model determines the identities and dynamics of the two inner residues. We focus on _dynamics scaffolding_ as one possible higher-level objective of inpainting: given the conformational transition of the observed residues, we hope to design peptides that support flux between the corresponding Markov states. Thus, for each test peptide, we select a 100-ps transition between the two most well-connected Markov states, mask out the inner residue identities and dynamics, and inpaint them with our model. To evaluate the designs, we compute the fraction of

Figure 4: **Recovery of fast dynamics via trajectory upsampling for peptide GTLM. (_Left_) Autocorrelations of each torsion angle from (—) the original 100 fs-timestep trajectory, (\(\)) the subsampled 10 ns-timestep trajectory, and ( \(\)) the reconstructed 100 fs-timestep trajectory (all length 100 ns). (_Right_) Dynamical content as a function of timescale from the upsampled vs. ground truth trajectories, stacked for all torsion angles (same color scheme). The subsampled trajectory contains only the shaded region and our model recovers the unshaded region. Further examples in Figure 10.**

Figure 5: Autocorrelation functions of MDGEN sidechain torsion angles computed from a 10-ns MD trajectory (_left_) versus a single **100k-frame model sample** with Hyena (_right_), capturing dynamics spanning four orders of magnitude.

   Method & High & Random \\  & Flux & Path \\  MDGen & **52.1\%** & **62.0\%** \\ DynMPNN & 17.4\% & 24.5\% \\ S-MPNN & 16.3\% & 13.5\% \\   

Table 3: Sequence recovery for the inner two peptides when conditioning on the partial trajectory (MDGen), the two terminal frames (DynMPNN), or a single frame (S-MPNN).

generated residue types that are identical to the tetrapeptide in which the target transition is known to occur. We compare MDGen with a bespoke inverse folding baseline that is provided the two terminal states (i.e., two fully observed MD frames), and thus designs peptides that support the two modes (rather than additionally a partially-observed _transition_ between them). We call this baseline DynMPNN, and it otherwise has the same architecture and settings as MDGen (more details in Appendix B.3). We find (Table 3) that MDGen recovers the ground-truth peptide substantially more often than DynMPNN when conditioned on a high-flux path or (as a sanity check) a random path from the reference simulation.

**Scaling to Long Trajectories.** Although Section 4.1 showed that our model can emulate long trajectories, this was limited to rollouts of 1000 frames at a time with coarse 10 ps timesteps, potentially missing faster dynamics or disrupting slower dynamics. Thus, we investigate generating extremely long consistent trajectories that capture timescales spanning several orders of magnitude _within a single model sample_. To do so, we replace the time attention in our baseline SiT architecture with a non-causal Hyena operator (Poli et al., 2023), which has \(O(N N)\) rather than \(O(N^{2})\) overhead. We overfit on 100k-frame, 10-ns trajectories of the pentapeptide MDGEN and compare the torsional autocorrelation functions computed from a _single_ generated trajectory with a _single_ ground truth trajectory (Figure 5). Although not yet comparable to the main set of forward simulation experiments due to data availability and architectural expressivity reasons, these results demonstrate proof-of-concept for longer context lengths in future work.

**Protein Simulation.** To demonstrate the applicability of our method for larger systems such as proteins, we train a model to emulate all-atom simulations of proteins from the ATLAS dataset (Vander Meersche et al., 2024) conditioned on the first frame (i.e., forward simulation). We follow the same splits as Jing et al. (2024). Due to the much larger number of residues, we generate samples with 250 frames and 400 ps timestep, such that a single sample emulates the 100 ns ATLAS reference trajectory. The difficulty of running fully equilibrated trajectories for proteins prevents the construction of Markov state models used in our main evaluations. Instead, we compare statistical properties of forward simulation ensembles following Jing et al. (2024). Our ensembles successfully emulate the ground-truth ensembles at a level of accuracy between AlphaFlow and MSA subsampling while being orders of magnitude faster per generated structure than either (Table 4; Figure 6).

## 5 Discussion

**Limitations.** Our experiments have validated the model and architecture for peptide simulations; however, a few limitations provide opportunities for future improvement. Due to the reliance on key frames, the model is not capable of unconditional generation or inpainting of residue rototranslations. The weaker performance on protein monomers relative to peptides suggests that scaling to larger systems will likely require additional data or methodological innovations. Fine-tuning of single structure models for co-generation of the key frames and trajectory tokens, similar to the content-frame decomposition of video diffusion models (Yu et al., 2024), may provide improvement. Since our tokenization scheme is specific to polypeptides, alternative strategies will be needed to model all-atom trajectories of more general systems, such as organic ligands, materials, or explicit solvent. More ambitious applications (see below) may require the ability to model trajectories not of a predefined set of atoms but over a region of space in which atoms may enter and exit. As such, we anticipate advancements in tokenization and architecture to be a fruitful direction of future work.

**Opportunities.** Similar to the foundational role of video generative models for understanding the macroscopic world (Yang et al., 2024), MD trajectory generation could serve as a multitask, unifying paradigm for deep learning over the microscopic world. Interpolation can be more broadly framed as _hypothesis generation_ for mechanisms of arbitrary molecular phenomena, especially when only partial information about the end states is supplied. Molecular inpainting could be a general technique to design molecular machinery by scaffolding more fine-grained and complex dynamics, for example, redesigning proteins to enhance rare transitions observed only once in a simulation or (with _ab initio_ trajectories) _de novo_ design of enzymatic mechanisms and motifs. Other types of conditioning not explored in this work may lead to further applications, such as conditioning over textual or experimental descriptors of the trajectory. Future availability of significantly more ground truth MD trajectory data for diverse chemical systems could be a chief enabler of such work. Lastly, considerations unique to molecular trajectories, such as equilibrium vs non-equilibrium processes, Markovianity, and the reversibility of the microscopic world contrasted with the macroscopic world (e.g., the missing arrow of time), could provide ripe areas for theoretical exploration.