# SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization

Hao Dong Ismail Nejjar Han Sun Eleni Chatzi Olga Fink

###### Abstract

In real-world scenarios, achieving domain generalization (DG) presents significant challenges as models are required to generalize to unknown target distributions. Generalizing to unseen multi-modal distributions poses even greater difficulties due to the distinct properties exhibited by different modalities. To overcome the challenges of achieving domain generalization in multi-modal scenarios, we propose _SimMMDG_, a simple yet effective multi-modal DG framework. We argue that mapping features from different modalities into the same embedding space impedes model generalization. To address this, we propose splitting the features within each modality into modality-specific and modality-shared components. We employ supervised contrastive learning on the modality-shared features to ensure they possess joint properties and impose distance constraints on modality-specific features to promote diversity. In addition, we introduce a cross-modal translation module to regularize the learned features, which can also be used for missing-modality generalization. We demonstrate that our framework is theoretically well-supported and achieves strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code and HAC dataset are available at https://github.com/donghao51/SimMMDG.

## 1 Introduction

Domain generalization (DG) has received significant attention in the research community . In real-world scenarios such as autonomous driving [13; 26], robotics , action recognition , and fault diagnosis , it is crucial that models trained on limited source domains perform well across novel target domains. To tackle distribution shift problems, numerous DG algorithms have been proposed, including domain-invariant feature learning , feature disentanglement , data augmentation , and meta-learning . However, most of these algorithms are designed for unimodal data, such as images  and time series data . More recently, the emergence of large-scale multi-modal datasets [16; 7] has highlighted the need to address multi-modal DG across multiple modalities, such as audio-video [36; 73], image-language [58; 33], and LiDAR-camera [44; 18]. However, to date, only RNA-Net  has focused on the multi-modal DG problem, with a relative norm alignment loss proposed to balance the feature norms of audio and video.

Multi-modal DG is closely related to multi-modal representation learning . Traditional multi-modal contrastive learning frameworks [58; 43] aim to project the features of different modalities into a common embedding space. However, this approach may not be optimal as different modalities consist of both shared information that is consistent across modalities and unique information that is specific to each modality. Consequently, attempting to align all modalities together can be challenging and impractical. For instance, consider descriptions of the same object that use entirely different modalities, such as video, audio, and optical flow, as illustrated in Fig. 1 (a). All modalities share someinformation since they describe the same object, but each modality further provides modality-specific information. For example, videos typically convey visual appearance information and sequential relations between frames, while audio provides tone information that is closely linked to sentiment and frequency information for different sounds. Optical flow, on the other hand, offers more intuitive information on motions and associated velocities. Therefore, simply mapping features from different modalities into the same embedding space would preserve only modality-shared information while overlooking modality-specific details. This could result in the loss of modality-specific information and a decline in downstream task performance.

Effective multi-modal DG frameworks that can address the challenges outlined above are urgently needed. To this end, we propose _SimMMDG_ - a **Sim**ple yet effective **M**ulti-**M**odal **DG** framework that can be applied to two or more modalities. We first propose to split the feature embedding of each modality into modality-specific and modality-shared parts, ensuring that complementary information from different modalities is retained. We apply supervised contrastive learning on modality-shared features and incorporate distance constraints for modality-specific features to ensure that the learned representations are informative. Moreover, we introduce a cross-modal translation module to further regularize the learned features and facilitate missing-modality generalization. Some theoretical insights from both the multi-modal representation learning perspective and the domain generalization perspective are provided to demonstrate that our approach is well-motivated in theory. Finally, we release a novel **H**uman-**A**nimal-**C**artoon (HAC) dataset, to promote research in multi-modal domain generalization, as shown in Fig. 1 (b). Our contributions can be summarized as:

* We propose a universal framework for multi-modal domain generalization that demonstrates both simplicity and effectiveness and outperforms existing methods on several challenging datasets. We also demonstrate its efficacy in general multi-modal classification setups.
* We address the missing-modality generalization problem and propose a cross-modal translation module as a solution, which is robust even in scenarios where multiple modalities are missing.
* We provide theoretical insights in support of the efficacy of our proposed approach.
* We introduce a new multi-modal dataset that includes three modalities and large domain shifts, which can serve as a challenging benchmark for future research on multi-modal DG.

## 2 Related Work

**Domain Generalization** refers to the process of training a model on multiple source domains in order to generalize to unseen target domains. This task is more challenging than domain adaptation (DA)  as target domain data cannot be accessed during training. Prior research has identified three main categories of domain generalization methods , including data manipulation, representation learning, and learning strategies. Data manipulation techniques aim to improve generalization performance by increasing the diversity of training data. For example, Tobin _et al._ use simulated environments for additional data generation, while Zhou _et al._ train a domain transformation network using adversarial training to synthesize data from previously unseen domains. Mixup  generates new instances by performing linear interpolations between data and labels. Representation

Figure 1: (a). Different modalities possess shared information, while simultaneously containing unique information exclusive to each modality. Inspired by this, we propose to split the feature of each modality into modality-specific and modality-shared parts in our framework. (b) Our new multi-modal DG dataset consists of three domains and three modalities. For each domain, the actor of different actions (opening door in this case) could be either a human, animal, or cartoon figure.

learning methods seek to learn domain-invariant representations through the use of domain-adversarial neural networks [25; 42], explicit feature distribution alignment , and instance normalization . Other approaches utilize learning strategies to improve the generalization performance. Examples of such approaches include meta-learning , gradient operation , and self-supervised learning .

**Multi-modal Representation Learning** aims to learn robust representations through two or more modalities, which can then be applied to various downstream tasks. While unified models [68; 4] tokenize various input modalities into sequences  and employ a single Transformer  for joint learning, methods such as CLIP  and ALIGN  use separate encoders for each modality and leverage contrastive loss to align features. However, these methods align features from different modalities into the same embedding space and may only preserve modality-shared information. In contrast, we propose splitting the features of each modality into modality-specific and modality-shared components to ensure that complementary information from different modalities is preserved. A recent work  also proposes to split the features into different components in multi-modal representation learning and provide an information-theoretical analysis. However, they only deal with two modalities and without considering missing-modality cases. They also don't use the label information within a batch in contrastive learning.

**Multi-modal DA and DG.** Several approaches exist for multi-modal DA. For instance, Munro and Damen  propose a self-supervised alignment approach along with adversarial alignment for multi-modal DA, while Kim _et al._ leverage cross-modal contrastive learning to align cross-modal and cross-domain representations. Zhang _et al._ propose an audio-adaptive encoder and an audio-infused recognizer to address domain shifts. Notably, RNA-Net  is the only method known to address multi-modal DG problem, by introducing a relative norm alignment loss to balance audio and video feature norms.

## 3 Methodology

### Problem Setting: Multi-modal Domain Generalization

We commence by presenting the definition of the multi-modal domain generalization problem based on the definition of unimodal DG, as described in . Let \(\) represent a nonempty input space, and \(\) denote an output space. A domain, denoted as \(\), consists of data sampled from a distribution, \(=\{(_{j},y_{j})\}_{j=1}^{n} P_{XY}\), where \(P_{XY}\) denotes the joint distribution of input samples and output labels. \(X\) and \(Y\) represent the corresponding random variables.

**Definition 1** (Multi-modal domain generalization).: _In multi-modal domain generalization, we are given \(D\) training domains \(_{train}=\{^{i} i=1,,D\}\), where \(^{i}=\{(_{j}^{i},y_{j}^{i})\}_{j=1}^{n_{i}}\) denotes the \(i\)-th domain with \(n_{i}\) data instances. Each data instance \(_{j}^{i}=\{(_{j}^{i})_{k} k=1,,M\}\) is comprised of \(M\) different modalities and \(y_{j}^{i}\) denotes the label. The joint distributions between each pair of domains are different: \(P_{XY}^{i} P_{XY}^{j},1 i j D\). The goal of multi-modal domain generalization is to learn a robust and generalizable predictive function \(f:\) from \(D\) training domains and \(M\) data modalities to achieve a minimum prediction error on an unseen test domain \(_{test}\) (i.e., \(_{test}\) cannot be accessed during training and \(P_{XY}^{test} P_{XY}^{i}\) for \(i\{1,,D\}\)):_

\[_{f}\ _{(,y)_{test}}[(f( ),y)],\] (1)

_where \(\) is the expectation and \((,)\) is the loss function._

### SimMMDG

We present the _SimMMDG_ framework for addressing the multi-modal DG problem, as depicted in Fig. 2. Our approach involves splitting the feature embedding of each modality into modality-specific and modality-shared components. We aim to map modality-shared embeddings of data samples with the same label, whether they belong to the same or different modalities, to be as close as possible (and vice versa for samples with different labels). For modality-specific features within each modality, our objective is to maximize the distance from their modality-shared features to capture unique and complementary information. Additionally, we incorporate a cross-modal translation module to regularize learned features and address the missing-modality generalization problem, which is essential for real-world testing scenarios where one or more modalities may be absent due to unpredictable factors.

#### 3.2.1 Within-modal Feature Splitting

Traditional multi-modal learning frameworks  aim to map features from various modalities into a common embedding space using contrastive learning. However, this approach may not be optimal because different modalities often contain a mix of both homogeneous and heterogeneous information, making it challenging and impractical to align them all together. For instance, consider the language and visual modality . Both modalities can describe people, objects, actions, and gestures, which reflect shared information between them. However, images can provide additional information such as texture, depth, and visual appearance that are not available in language data. Similarly, language can provide syntactic structure, vocabulary, and morphology, which are not present in image data. These are specific pieces of information that are unique to each modality.

Thus, the simple mapping of features from different modalities into the same embedding space may result in the loss of modality-specific information and consequently lead to decreased performance in downstream tasks. Guided by this intuition, we propose a novel approach that involves the separation of the feature embedding of each modality into modality-specific and modality-shared components. For example, given a unimodal embedding \(\), we denote it as \(=[_{s};_{c}]\), where \(_{s}\) is modality-specific feature and \(_{c}\) is modality-shared feature.

**Multi-modal Supervised Contrastive Learning.** We expect modality-shared features \(_{c}\) to possess characteristics that are shared among distinct modalities. If data instances from different modalities have the same label, we expect their modality-shared features to be as close as possible in the embedding space. To achieve this objective, we leverage the availability of labels for each data instance and employ supervised contrastive loss  for effective guidance. For a set of \(N\) randomly sampled label pairs, \(\{_{j},y_{j}\}_{j=1,...,N}\), the corresponding batch used for training consists of \(M N\) pairs, \(\{}_{k},_{k}\}_{k=1,...,M N}\), where \(}_{M j}\), \(}_{M j-1}\),..., \(}_{M j-M+1}\) are data instances from \(M\) different modalities in \(_{j}\) (\(j=1,...,N\)) and \(_{M j}=...=_{M j-M+1}=y_{j}\).

Let \(i I\{1,...,M N\}\) be the index of an arbitrary unimodal sample within a batch. We define \(A(i) I\{i\}\), \(P(i)\{p A(i):_{p}=_{i}\}\) as the set of indices of all positive samples in the batch which share the same label as \(i\). The cardinality of \(P(i)\) is denoted as \(|P(i)|\). Then, the multi-modal supervised contrastive learning loss can be written as follows:

\[_{con}=_{i I}_{p P(i)}_{i}_{p}/)}{_{a A(i)} (_{i}_{a}/)},\] (2)

with \(_{k}=Proj(g(}_{k}))^{D_{P}}\), where \(g()\) is the feature extractor, that maps \(\) to modality-specific and modality-shared features, \(=[_{s};_{c}]=g()\), where \(_{s},_{c} R^{D_{E}}\), and \(Proj()\) is the

Figure 2: Overview of _SimMMDG_. We split the features of each modality into modality-specific and modality-shared parts. For the modality-shared part, we use supervised contrastive learning to map the features with the same label to be as close as possible. For modality-specific features, we use a distance loss to encourage them to be far from modality-shared features, promoting diversity within each modality. Additionally, we introduce a cross-modal translation module that regularizes features and enhances generalization across missing modalities.

projection network that maps \(_{c}\) to a vector \(=Proj(_{c})^{D_{P}}\). The inner product between two projected feature vectors is denoted by \(\), and \(^{+}\) is a scalar temperature parameter.

**Feature Splitting with Distance.** To ensure that the modality-specific features \(_{s}\) carry unique and complementary information, we aim to maximize their dissimilarity from the corresponding modality-shared features \(_{c}\). To achieve this goal, we utilize negative \(_{2}\) distance and formulate a distance loss on \(_{s}\) and \(_{c}\) as:

\[_{dis}=_{i=1}^{M}||_{s}^{i}-_{ c}^{i}||_{2}^{2},\] (3)

where \(M\) is the number of modalities, \(_{s}^{i}\) and \(_{c}^{i}\) are the modality-specific and modality-shared features of the \(i\)-th modality.

#### 3.2.2 Cross-modal Translation

Simply increasing the distance between \(_{s}\) and \(_{c}\) may not yield optimal results. The proposed cross-modal translation module aims to ensure the meaningfulness of modality-specific features by exploiting the implicit relationships and approximate translation mappings that exist between the \(M\) modalities within the same data instance. This is achieved through a multi-layer perceptron (MLP)  that translates the feature embedding \(\) across modalities. For instance, \(_{t}^{j}=MLP_{^{i}^{j}}(^ {i})\) implies that we translate the embedding \(^{i}=[_{s}^{i};_{c}^{i}]\) of the \(i\)-th modality to the \(j\)-th modality using \(MLP_{^{i}^{j}}\), resulting in the translated embedding \(_{t}^{j}\). More intuitions behind the cross-modal translation module are discussed in the appendix. To ensure that the translated embedding \(_{t}^{j}\) is a meaningful representation of the \(j\)-th modality, we aim to minimize its \(_{2}\) distance from the real embedding of the \(j\)-th modality, \(^{j}\) and the cross-modal translation loss is defined as:

\[_{trans}=_{i=1}^{M}_{j i}||MLP_{^{i}^{j}}(^{i})-^{j}||_{2}^{2}.\] (4)

### Final Loss

The final loss is obtained as the weighted sum of the previously defined losses:

\[=_{cls}+_{con}_{con}+_{dis} _{dis}+_{trans}_{trans},\] (5)

where \(_{cls}\) is the cross-entropy loss for classification, and where \(_{}\), \(_{}\), and \(_{}\) are hyperparameters that control the relative importance of the contrastive learning, dissimilarity, and cross-modal translation terms, respectively.

### Missing-modality Generalization

During the training phase, we have access to data from all modalities. However, in the testing phase, one or more modalities may be absent due to unpredictable reasons, such as sensor malfunction or loss of communication. In such cases, the system's robustness toward potential missing modalities becomes critical. To address the missing-modality scenario, we utilize our cross-modal translation module. In normal circumstances, the output of the network is defined as:

\[y=f(x)=h(g(x))=h([^{1},...,^{i},...,^{M}]),\] (6)

where \(g()\) is the feature extractor and \(h()\) is the classifier. If the \(i\)-th modality is missing during testing, one solution is to replace its embedding with zero. The output is then given as:

\[y=h([^{1},...,,...,^{M}]).\] (7)

However, simply replacing the embeddings with null entries may have an adverse effect on the network's performance. To address this issue, we propose using our cross-modal translation module to predict and substitute the missing modality's embedding with information from available modalities, resulting in a more robust output for the network. The output of the network is then given as:

\[y=h([^{1},...,_{t}^{i},...,^{M}]),\] (8)where

\[_{t}^{i}=_{j i}^{M}MLP_{^{j} ^{i}}(^{j}).\] (9)

In cases where multiple modalities are missing during testing, we can use the same strategy to utilize available modalities to predict and substitute the missing ones, similar to Eq. (9). The benefits of our approach are demonstrated in the subsequent experiments.

## 4 Theoretical Insights

### Multi-modal Representation Learning Perspective

We first generalize Theorem 3.1 in  to the case of \(M\) modalities. Let \(p\) be the joint distribution of \((x_{1},...,x_{M},y)\), where \(x_{i}\) is the \(i\)-th modality and \(y\) is the target. We define the _information gap_ as \(_{p}:=\{I(x_{i};y);i=1,...,M\}-\{I(x_{i};y);i=1,...,M\}\), where \(I(x_{i};y)\) is the mutual information between modality \(x_{i}\) and the target variable \(y\). The information gap serves to characterize the effectiveness of the modalities in predicting the target \(y\). We use the cross-entropy loss, denoted by \(_{}\), and the prediction function, denoted by \(f\).

**Theorem 1**.: _For \(M\) feature extractors \(g^{i}()\) (\(i=1,...,M\)), if the multi-modal features \(^{i}=g^{i}(x_{i})\) are perfectly aligned in the feature space, i.e., \(^{1}=...=^{i}=...=^{M}\), then \(_{f}_{p}[_{}(f(^{1},...,^{M} ),y)]-_{f^{}}_{p}[_{}(f^{}(x_{1},...,x_ {M}),y)]_{p}\)._

The proof of this theorem can be found in the appendix. Theorem 1 indicates that the optimal prediction error, achieved with perfectly aligned features, is at least \(_{p}\) larger than when using the raw input modalities. In practical scenarios where one modality is less informative for prediction, the information gap \(_{p}\) tends to be substantial. Consequently, this leads to a notable increase in prediction errors in downstream tasks. Furthermore, achieving perfect modality alignment enforces the aligned features to exclusively contain predictive information present in all input modalities, potentially causing the loss of modality-specific information. By splitting the feature embedding for each modality, our model has access to both modality-shared features with predictive information present in both modalities, and modality-specific features that contain predictive information unique to each individual modality. This enables our model to effectively capture the distinct information provided by each modality, while simultaneously leveraging the shared information across modalities. Consequently, our approach has the potential to enhance generalization capabilities and achieve better performance on downstream tasks.

### Domain Generalization Perspective

**Theorem 2**.: _Let \(\) be a space and \(\) be a class of hypotheses corresponding to this space. Let \(\) and the collection \(\{_{i}\}_{i=1}^{K}\) be distributions over \(\) and let \(\{_{i}\}_{i=1}^{K}\) be a collection of non-negative coefficient with \(_{i}_{i}=1\). Let \(\) be a set of distributions s.t. \(\), the following holds_

\[_{i}_{i}d_{}(_{i},) _{i,j}d_{}(_{i},_{j}).\] (10)

_Then, for any \(h\),_

\[_{}(h)_{}+_{i}_{i} _{_{i}}(h)+_{}d_{ }(,)+_{i,j}d_{ }(_{i},_{j}),\] (11)

_where \(_{}\) is the error of an ideal joint hypothesis, \(_{}(h)\) is the error for a hypothesis \(h\) on a distribution \(\), and \(d_{}(,)\) is \(\)-divergence which measures differences in distribution ._

The proof of this theorem is provided in . Here, \(\) corresponds to the unseen out-of-distribution target domain and \(\{_{i}\}_{i=1}^{K}\) correspond to source domains. \(_{}\) is small in reality and often neglected. The term \(_{i}_{i}_{_{i}}(h)\) is minimized by cross-entropy loss with class labels as supervision. The term \(_{i,j}d_{}(_{i}, _{j})\) measures the maximum differences among source domains. This corresponds to multi-modal supervised contrastive learning in our approach, where we map the modality-shared features of different modalities from all source domains to be as close as possible if they have the same label. The term \(_{}_{ }(,)\) demonstrates the importance of diverse source distributions , so that the unseen target \(\) might be "near" to \(\). Therefore, to enlarge the range of \(\), we split the feature embedding of each modality into modality-specific and modality-shared parts by maximizing the \(_{2}\) distance to preserve diversity.

## 5 Experiments

### Experimental Setting

**Dataset.** We use the EPIC-Kitchens dataset  and introduce a novel HAC dataset in this paper, which will be made publicly accessible for further research. We follow the experimental protocol used for the EPIC-Kitchens dataset in . The EPIC-Kitchens dataset includes eight actions ('put', 'take', 'open', 'close', 'wash', 'cut','mix', and 'pour') recorded in three different kitchens, forming three separate domains D1, D2, and D3. Our HAC dataset consists of seven actions ('sleeping', 'watching tv', 'eating', 'drinking','swimming', 'running', and 'opening door') performed by humans, animals, and cartoon figures, forming three different domains H, A, and C. We collect \(3381\) video clips from the internet with around \(1000\) samples for each domain. We provide three modalities in our dataset: video, audio, and pre-computed optical flow. Some examples of the HAC dataset are shown in Fig. 1 (b) and more details are provided in the supplementary material.

**Implementation Details.** In our framework, we perform experiments on three modalities: video, audio, and optical flow. We adopt the MMAction2  toolkit for experiments. To encode the visual information, we use SlowFast network  initialized with Kinetics-400  pre-trained weights. For the audio encoder, we use ResNet-18  and initialize the weights from the VGGSound pre-trained checkpoint . Similarly, we use the SlowFast network with slow-only pathway and also Kinetics 400  pre-trained weights for the optical flow encoder. The dimensions of the unimodal embedding \(\) for video, audio, and optical flow are \(2304\), \(512\), and \(2048\) correspondingly. For the projection network \(Proj()\) in supervised contrastive learning, we instantiate it as a multi-layer perceptron with two hidden layers of size \(2048\) and output vector of size \(D_{P}=128\). We use a multi-layer perceptron with two hidden layers of size \(2048\) to instantiate the cross-modal translation \(MLP_{^{}}\). After obtaining the feature embedding from the encoder, we split the embedding into modality-shared features (the first half of the embedding) and modality-specific features (the remaining half). We use the Adam optimizer  with a learning rate of \(0.0001\) and a batch size of \(16\). The scalar temperature parameter \(\) is set to \(0.1\). Additionally, we set \(_{con}=3.0\), \(_{dis}=0.7\), and \(_{trans}=0.1\). We also analyze the sensitivity of different \(\) in the appendix. Finally, we train the network for \(15\) epochs on an RTX 2080 Ti GPU which takes about \(20\) hours and select the model with the best performance on the validation dataset. We report the Top-1 accuracy for all experiments.

### Results

**Multi-modal Multi-source DG.** Tab. 1 and Tab. 2 illustrate the results of _SimMMDG_ under the multi-modal multi-source DG setting, where we train on multiple source domains and test on one target domain. We first conduct experiments using video and audio modalities for experiments, as in . We re-implement our framework employing the same I3D  and BN-Inception  backbone for video and audio as in RNA-Net  to ensure fair comparisons. The DeepAll approach implies that we feed all the data from source domains to the network without any domain generalization strategies. Tab. 1 shows that our _SimMMDG_ significantly outperforms all the baselines. When we replace the backbone with SlowFast  and ResNet-18 ,

  
**Method** & D2, D3 \(\) D1 & D1, D3 \(\) D2 & D1, D2 \(\) D3 & _Mean_ \\  
**I3D backbone** & & & & \\ DeepAll & 43.19 & 39.35 & 51.47 & 44.67 \\ BN-Net  & 44.46 & 49.21 & 48.97 & 47.55 \\ Gradient Blending  & 41.97 & 48.40 & 51.43 & 47.27 \\ TBN  & 42.35 & 47.45 & 49.20 & 46.33 \\ AVSA  & 42.78 & 47.38 & 51.79 & 47.32 \\ Co-Attention  & 40.87 & 43.57 & 54.88 & 46.44 \\ SENet  & 42.82 & 42.81 & 51.07 & 45.56 \\ Non-Local  & 45.72 & 43.08 & 49.49 & 46.10 \\ MM-SADA  & 39.79 & 52.73 & 51.87 & 48.13 \\ RNA-Net  & 45.65 & 51.64 & 55.88 & 51.06 \\ SynMMDG (ours) & **54.25** & **58.67** & **57.28** & **56.73** \\ 
**SlowFast backbone** & & & & \\ DeepAll & 47.13 & 55.73 & 57.17 & 53.34 \\ MM-SADA  & 49.20 & 60.40 & 59.14 & 56.25 \\ RNA-Net  & 52.18 & 59.47 & 60.88 & 57.51 \\ SimMMDG (ours) & **57.93** & **65.47** & **66.32** & **63.24** \\   

Table 1: Multi-modal **multi-source DG** on EPIC-Kitchens dataset using video and audio.

the results further improve by a large margin (with an average improvement of up to \(5.73\%\)) compared to the baselines. In the following experiments, we adopt SlowFast and ResNet-18 as our default backbones. To verify the generalization of our framework to different modalities, we conduct experiments by combining any two modalities, as well as all three modalities, and present the results in Tab. 2. Our _SimMMDG_ outperforms all the baselines by a significant margin in all cases, with improvements of up to \(9.58\%\). Notably, when we combine all three modalities, the performance further improves and surpasses that of any two modalities. In contrast, the baseline methods cannot achieve better results with more modalities, indicating that they fail to fully leverage the complementary information between modalities. Finally, we validate the performance of our framework on the HAC dataset, and the results are consistent with those obtained on the EPIC-Kitchens dataset, as demonstrated in Tab. 2. Our _SimMMDG_ outperforms all the baselines by a significant margin in all cases, with improvements of up to \(7.73\%\).

**Multi-modal Single-source DG.** The domain labels are not required in our _SimMMDG_ framework. This feature makes our method readily applicable to single-source DG without modifications. In this setup, we train the model on a single source domain and test it on multiple target domains. Tab. 3 presents the results of _SimMMDG_ under the multi-modal single-source DG setting using video and audio modalities. Despite being trained only on data from a single domain, our model demonstrates robust generalization to unseen domains, with an average improvement of up to \(5.71\%\). In comparison, other baseline methods perform even worse than the DeepAll baseline, highlighting their limitations in the single-source DG setting. We present additional results obtained by exploring different combinations of modalities in the appendix.

**Missing-modality DG.** In real-world deployment scenarios, we cannot always guarantee that all modalities will be available. Hence, the network should be capable of handling missing-modality cases. One simple approach is to set the embedding of the missing modality to zero. We compare this with our proposed cross-modal translation replacement in Sec. 3.4. We retrain the cross-modal translation \(MLP_{^{i}^{j}}\) for \(10\) epochs using \(_{trans}\) in Eq. (4) and keep other parameters fixed. Tab. 4 reports on the comparison results on the EPIC-Kitchens dataset, where our solution yields significant benefits (up to \(10.47\%\) performance improvement) compared to zero-filling. By replacing the missing modality features with the translation ones, our approach also outperforms the unimodal model in most cases. In contrast, zero-filling hurts the network performance in some cases and is even worse than the unimodal model. The last six rows in Tab. 4 demonstrate that our approach is _robust_

    &  &  &  \\ 
**Method** & Video & Audio & Flow & D2, D3 \(\) D1 & D1, D3 \(\) D2 & D1, D2 \(\) D3 & _Mean_ & A, C \(\) H, H, C \(\) A & H, A \(\) C & _Mean_ \\  DeepAll & ✓ & ✓ & & 47.13 & 55.73 & 57.17 & 53.34 & 66.55 & 72.85 & 45.77 & 61.72 \\ MM-SADA  & ✓ & ✓ & & 49.20 & 64.00 & 59.14 & 56.25 & 65.47 & 72.52 & 44.30 & 60.76 \\ RNA-Net  & ✓ & ✓ & & 52.18 & 59.47 & 60.88 & 57.51 & 60.20 & 73.95 & 48.90 & 61.02 \\ SIMMDG (ours) & ✓ & ✓ & **57.93** & **64.47** & **64.32** & **63.24** & **74.77** & **77.81** & **33.68** & **68.75** \\  DeepAll & ✓ & ✓ & & 55.17 & 62.93 & 60.37 & 59.49 & 76.78 & 70.64 & 49.63 & 65.68 \\ MM-SADA  & ✓ & ✓ & 47.13 & 57.60 & 59.34 & 54.69 & 69.79 & 69.76 & 49.45 & 63.00 \\ RNA-Net  & ✓ & ✓ & 54.71 & 61.87 & 58.21 & 58.26 & 77.14 & 79.44 & 42.00 & 64.69 \\ SimMMDG (ours) & ✓ & ✓ & **59.31** & **63.33** & **62.73** & **61.79** & **79.31** & **77.04** & **51.29** & **69.21** \\  DeepAll & ✓ & ✓ & 45.28 & 56.40 & 57.08 & 52.92 & 50.04 & 59.71 & 38.97 & 49.97 \\ MM-SADA  & ✓ & ✓ & 47.36 & 53.47 & 60.27 & 53.70 & 46.58 & 61.81 & 39.15 & 49.18 \\ RNA-Net  & ✓ & ✓ & 45.74 & 57.73 & 56.47 & 53.31 & 52.05 & 64.13 & 40.35 & 52.18 \\ SimMMDG (ours) & ✓ & ✓ & **56.09** & **67.33** & **61.50** & **61.64** & **59.63** & **64.24** & **44.85** & **56.24** \\  DeepAll & ✓ & ✓ & ✓ & 55.63 & 59.20 & 58.01 & 57.61 & 69.07 & 71.30 & 51.47 & 63.95 \\ MM-SADA  & ✓ & ✓ & ✓ & 51.72 & 58.40 & 59.34 & 56.49 & 72.53 & 72.19 & 55.51 & 66.74 \\ RNA-Net  & ✓ & ✓ & 52.41 & 57.20 & 60.16 & 56.59 & 69.00 & 73.40 & 51.65 & 64.68 \\ SimMMDG (ours) & ✓ & ✓ & ✓ & **63.68** & **70.13** & **67.76** & **67.19** & **77.45** & **79.03** & **56.62** & **71.10** \\   

Table 2: Multi-modal **multi-source DG with different modalities on EPIC-Kitchens and HAC datasets.**

    &  &  \\   &  &  &  &  &  &  &  \\   & & D2 & D3 & D1 & D3 & D1 & D2 & _Mean_ & A & C & H & C & H & A & _Mean_ \\  DeepAll & 52.27 & 51.75 & 44.60 & 54.11 & 48.05 & 56.67 & 51.24 & 66.11 & 43.01 & 63.45 & 37.68 & 46.86 & 58.94 & 52.68 \\ MM-SADA  & 50.67 & 50.10 & 51.49 & 56.57 & 42.99 & 54.00 & 50.97 & 65.89 & 37.22 & 57.75 & 40.90 & 49.82 & 62.91 & 52.42 \\ RNA-Net  & 45.60 & 46.30 & 43.68 & 57.39 & 49.66 & 55.87 & 49.75 & 65.67 & 42.92 & 61.72 & 36.89 & 47.66 & 61.9 & 53.04 \\ SIMMDG (ours) & **54.00** & **52.26** & **51.49** & **60.88** & **49.88** & **69.53** & **54.84** & **66.67** & **44.21** & **68.42** & **46.05** & **54.43** & **72.73** & **58.75** \\   

Table 3: Multi-modal **single-source DG on EPIC-Kitchens and HAC datasets using video and audio.**even when two modalities out of three are missing_. We present more results on HAC dataset in the appendix.

### Ablation Studies

**Ablation on each proposed module.** We conducted extensive ablation studies to investigate the role of each module of _SimMMDG_ on EPIC-Kitchens dataset, as shown in Tab. 5. Incorporating the supervised contrastive learning loss alone resulted in noticeable improvements. However, the mean accuracy decreased when we integrated feature splitting without imposing any constraints. The results were further enhanced when we added the distance loss to promote diversity, and even more so when we incorporated the cross-modal translation module. Although using only supervised contrastive learning and cross-modal translation yielded satisfactory results, their performance is on average \(4.15\%\) lower than the complete approach with feature splitting and distance loss. These findings highlight the significance of segregating the feature embedding of each modality into modality-specific and modality-shared components. Although _SimMMDG_ without supervised contrastive learning is already better than most baselines, it still has a performance gap compared with the whole framework, which means the cross-modal translation is helpful but cannot replace contrastive learning. The effect of contrastive learning is similar to explicit feature alignment. It aligns the modality-shared features of different modalities from different source domains with the same label to be as close as possible in the embedding space, while pushing away features with different labels, to make the embedding space more distinctive.

**Comparison against unimodal DG.** Tab. 6 presents the results in comparison to unimodal DG algorithms that exclusively rely on either video, audio, or optical flow inputs. We choose RSC , Mixup , and Fishr  as our baselines. By leveraging information from multiple modalities, our multi-modal DG framework delivers significant improvements (up to \(7.74\%\)) in terms of performance as compared to unimodal DG methods.

**Combine _SimMMDG_ with other training strategies._SimMMDG_ can be seamlessly combined with other training strategies due to its generality and simplicity. We first combine _SimMMDG_ with Gradient Blending , a strategy to improve multi-modal learning. The results are further improved compared to the original _SimMMDG_ as shown in Tab. 7. We also combine _SimMMDG_ with a Domain-adversarial Neural Network (DANN)  to align the features of source domains using

    & Video & Audio & Flow & D2, \(\) D1 & D1, D3 \(\) D2 & D1, D2 \(\) D3 & _Mem_ \\  DesEMM (Video-only) & ✓ & & & 51.03 & 59.87 & 56.57 & 55.82 \\ SimMMDG (zero-filling) & ✓ & ✗ & & 55.40 & 64.00 & 58.32 & 59.24 \\ SimMMDG (translation) & ✓ & ✗ & & **57.24** & **65.07** & **59.65** & **60.65** \\ SimMMDG (zero-filling) & ✓ & ✗ & & 53.10 & 60.93 & 61.09 & 58.37 \\ SimMMDG (translation) & ✓ & ✗ & **58.17** & **61.33** & **61.70** & **59.40** \\  DeepAll (Audio-only) & ✓ & & 32.87 & 42.27 & 45.17 & 40.10 \\ SimMMDG (zero-filling) & ✗ & ✓ & & 29.20 & 37.33 & 45.69 & 37.41 \\ SimMMDG (translation) & ✗ & ✓ & & **37.70** & **46.40** & **52.05** & **45.38** \\ SimMMDG (zero-filling) & ✓ & ✗ & & 28.51 & 34.27 & 42.81 & 35.20 \\ SimMMDG (translation) & ✓ & ✗ & **39.77** & **46.00** & **51.23** & **45.67** \\  DeepAll (Bioseonly) & ✓ & & 54.25 & 61.33 & 55.95 & 57.18 \\ SimMMDG (zero-filling) & ✗ & ✓ & 56.32 & 60.67 & 56.57 & 57.85 \\ SimMMDG (translation) & ✗ & ✓ & & **56.32** & **62.13** & **56.98** & **58.48** \\ SimMMDG (zero-filling) & ✗ & ✓ & & 52.41 & 62.13 & 51.35 & 55.29 \\ SimMMDG (translation) & ✗ & ✓ & **55.40** & **63.73** & **56.57** & **58.57** \\  SimMMDG (zero-filling) & ✗ & ✓ ✓ & 53.10 & 62.40 & 64.48 & 59.99 \\ SimMMDG (translation) & ✗ & ✓ & & **55.86** & **68.27** & **64.58** & **62.90** \\ SimMMDG (zero-filling) & ✗ & ✓ & & 62.76 & 66.80 & 57.49 & 62.35 \\ SimMMDG (translation) & ✓ & ✗ & ✓ & **63.22** & **67.20** & **59.24** & **63.22** \\ SimMMDG (zero-filling) & ✓ & ✗ & & 60.92 & **68.66** & 60.96 & 63.52 \\ SimMMDG (translation) & ✓ & ✓ & ✗ & **62.53** & 68.13 & **61.60** & **64.09** \\  SimMMDG (zero-filling) & ✓ & ✗ & ✗ & 59.08 & 62.27 & 52.98 & 58.11 \\ SimMMDG (translation) & ✓ & ✗ & ✗ & **59.31** & **64.00** & **56.57** & **59.96** \\ SimMMDG (zero-filling) & ✗ & ✗ & & 33.39 & 57.44 & 44.56 & 38.61 \\ SimMMDG (translation) & ✗ & ✓ & ✗ & **38.39** & **44.53** & **47.74** & **43.55** \\ SimMMDG (zero-filling) & ✗� & ✓ & 35.17 & 57.07 & 48.46 & 53.37 \\ SimMMDG (translation) & ✗� & ✓ & **56.32** & **64.27** & **56.57** & **59.05** \\   

Table 4: Multi-modal multi-source DG with **missing modalities** on EPIC-Kitchens dataset. ✗ means the modality is available during training, but is missing in test time.

domain labels and observe a similar improvement as with Gradient Blending. This indicates that our _SimMMDG_ can be easily combined with other training strategies to get even better results.

_SimMMDG_ **as a general framework for multi-modal classification.** Here, we evaluate our framework in a more general multi-modal classification setup without DG. We first evaluate our framework on the EPIC-Kitchens dataset without incorporating the DG setup. To do so, we aggregate data from three domains, we partition the training, validation, and testing data to ensure that no apparent domain shifts exit between the training and testing data. We compare our method with DeepAll, TBN , and Gradient Blending . The results, as presented in Tab. 8 demonstrate that _SimMMDG_ also exhibits significant advantages in the general multi-modal classification setup.

We further evaluate our framework on two multi-modal datasets: MUSTARD  and UR-FUNNY , both available in MultiBench . These datasets pertain to human sentiment analysis and encompass language, video, and audio modalities. We conduct our experiments using the MultiBench codebase and implement _SimMMDG_ within that environment. MultiBench treats human sentiment analysis as a regression task, whereas our framework is tailored for classification. To align the codebase with our classification task, we made the necessary modifications. For all baselines, we apply the same backbone model and solely change the fusion paradigms. In our comparisons, we evaluate our method against Late Fusion, Low-rank Tensor Fusion (LMF) , and Multimodal Transformer Fusion (MULT) . The results reveal that our _SimMMDG_ exhibits clear advantages, outperforming the baselines with an average improvement of \(7.3\%\) and \(1.7\%\). This suggests that our _SimMMDG_ serves as a versatile framework for multi-modal classification tasks and is compatible with various combinations of modalities, such as video+audio+flow and language+video+audio.

## 6 Conclusion

In this paper, we propose the _SimMMDG_ framework for multi-modal DG. Our approach involves splitting the features of each modality into modality-specific and modality-shared parts and enforcing constraints on each part using supervised contrastive learning, distance loss, and cross-modal translation. The cross-modal translation module can also be applied in the case of missing-modality generalization. Our experiments on different datasets demonstrate the effectiveness of _SimMMDG_. Furthermore, we introduce a new challenging multi-modal dataset that can serve as a benchmark and guide future research in multi-modal DG problems.

**Limitations.** Currently, the number of cross-modal translation MLP in our framework is \((|M|^{2})\) and will be complex with the increase of modalities. In future work, the encoder-decoder network proposed in  can be used to reduce the complexity to \((|M|)\).

    &  \\   & 50.11 & 62.53 & 58.73 & 57.12 \\  & 36.55 & 43.73 & 48.15 & 42.81 \\  & 55.17 & 63.33 & 59.65 & 59.38 \\  & 49.20 & 59.73 & 59.96 & 56.30 \\  & 35.17 & 40.80 & 45.07 & 40.35 \\  & 56.32 & 65.60 & 54.62 & 58.85 \\  & 53.79 & 63.47 & 61.09 & 59.45 \\  & 37.47 & 44.80 & 47.43 & 43.23 \\  & 54.25 & 63.87 & 59.14 & 59.09 \\  &  & **63.68** & **70.13** & **67.76** & **67.19** \\   

Table 6: Comparison with single-modal DG methods on EPIC-Kitchens dataset. V: video, A: audio, F: optical flow.

    &  &  &  \\  SimMMDG & 57.93 & 65.47 & 66.32 & 63.24 \\ +Gradient Blending & 59.31 & 68.40 & 66.63 & **64.78** \\ +DANN & 60.69 & 66.69 & 64.58 & **64.07** \\   

Table 7: Combine SimMMDG with other training strategies on EPIC-Kitchens dataset.

    &  &  &  \\   & 50.11 & 62.53 & 58.73 & 57.12 \\  & 36.55 & 43.73 & 48.15 & 42.81 \\  & 55.17 & 63.33 & 59.65 & 59.38 \\  & 49.20 & 59.73 & 59.96 & 56.30 \\  & 35.17 & 40.80 & 45.07 & 40.35 \\  & 56.32 & 65.60 & 54.62 & 58.85 \\  & 53.79 & 63.47 & 61.09 & 59.45 \\  & 37.47 & 44.80 & 47.43 & 43.23 \\  & 54.25 & 63.87 & 59.14 & 59.09 \\  &  & **63.68** & **70.13** & **67.76** & **67.19** \\   

Table 8: Evaluation under multi-modal classification setup.

    &  &  &  \\   & 50.11 & 62.53 & 58.73 & 57.12 \\  & 36.55 & 43.73 & 48.15 & 42.81 \\  & 55.17 & 63.33 & 59.65 & 59.38 \\  & 49.20 & 59.73 & 59.96 & 56.30 \\  & 35.17 & 40.80 & 45.07 & 40.35 \\  & 56.32 & 65.60 & 54.62 & 58.85 \\  & 53.79 & 63.47 & 61.09 & 59.45 \\  & 37.47 & 44.80 & 47.43 & 43.23 \\  & 54.25 & 63.87 & 59.14 & 59.09 \\  &  & **63.68** & **70.13** & **67.76** & **67.19** \\   

Table 5: Ablations of each proposed module on EPIC-Kitchens dataset. CL: supervised contrastive learning, FS: feature splitting, DL: distance loss, CT: cross-modal translation.