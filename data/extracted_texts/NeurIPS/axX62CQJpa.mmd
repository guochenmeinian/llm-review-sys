# Streaming Long Video Understanding with

Large Language Models

 Rui Qian\({}^{1}\) Xiaoyi Dong\({}^{1,2}\) Pan Zhang\({}^{2}\) Yuhang Zang\({}^{2}\) Shuangrui Ding\({}^{1}\)

**Dahua Lin\({}^{1,2,3}\) Jiaqi Wang\({}^{2}\)\({}^{}\)**

\({}^{1}\) The Chinese University of Hong Kong

\({}^{2}\) Shanghai AI Laboratory

\({}^{3}\) HKGAI under InnHK

Work done during an internship in Shanghai AI Laboratory.Corresponding Author.

###### Abstract

This paper presents **VideoStreaming**, an advanced vision-language large model (VLLM) for video understanding, that capably understands arbitrary-length video with a constant number of video tokens streamingly encoded and adaptively selected. The challenge of video understanding in the vision language area mainly lies in the significant computational burden caused by the great number of tokens extracted from long videos. Previous works rely on sparse sampling or frame compression to reduce tokens. However, such approaches either disregard temporal information in a long time span or sacrifice spatial details, resulting in flawed compression. To address these limitations, our VideoStreaming has two core designs: Memory-Propagated Streaming Encoding and Adaptive Memory Selection. **The Memory-Propagated Streaming Encoding architecture** segments long videos into short clips and sequentially encodes each clip with a propagated memory. In each iteration, we utilize the encoded results of the preceding clip as historical memory, which is integrated with the current clip to distill a condensed representation that encapsulates the video content up to the current timestamp. This method not only incorporates long-term temporal dynamics into the streaming encoding process but also yields a fixed-length memory as a global representation for arbitrarily long videos. After the encoding process, the **Adaptive Memory Selection strategy** selects a constant number of question-related memories from all the historical memories, and feeds them into the LLM to generate informative responses. The question-related selection reduces redundancy within the memories, enabling efficient and precise video understanding. Meanwhile, the disentangled video extraction and reasoning design allows the LLM to answer different questions about a video by directly selecting corresponding memories, without the need to encode the whole video for each question. Through extensive experiments, our model achieves superior performance and higher efficiency on long video benchmarks, showcasing precise temporal comprehension for detailed question answering.

## 1 Introduction

The evolution of Large Language Models (LLMs) has significantly advanced artificial intelligence, encompassing text generation and reasoning in complex language environments . Later, the community extends LLMs to multi-modal domains, demonstrating promising results in captioning and question-answering tasks that integrate diverse visual signals . Yet, within the domain of video understanding, long video sequences pose a formidable challenge.

Incorporating such long visual contents into LLMs requires a substantial number of tokens, which not only amplifies computational demands but also risks early contextual information loss .

Among the recent works on general video understanding with LLMs [48; 42; 44; 88; 50; 43; 66; 62], a prevalent strategy is using sparse temporal sampling [44; 86] or spatio-temporal pooling [50; 48] to reduce tokens. Unfortunately, this paradigm explicitly loses substantial information in the long time span. To address this limitation, [43; 42; 88] develop frame-wise compression, with LLaMA-VID  as a typical example. It compresses each frame into only two tokens but overlooks the inter-frame temporal dynamics which are vital in compressing temporal redundancy within videos. Besides, its question-dependent compression pipeline limits the ability to produce a general representation that can handle diverse instructions. Another line of works employ memory banks [77; 7] to store history information [66; 22]. Whereas, these methods rely on explicit timestamps to recall the historical details, limiting the ability to generate comprehensive responses without specific time indicators.

In this work, we propose VideoStreaming, a novel Memory-Propagated Streaming Encoding architecture with Adaptive Memory Selection to sequentially encode a long video into condensed memories and generate responses referring to relevant timestamps. The core idea behind the memory-propagated streaming encoding is to preserve representative spatial cues and temporal dynamics while reducing temporal redundancy in videos. To achieve this goal, we segment the long video into multiple short clips and sequentially encode each clip. When encoding each clip, we first refer to the encoded results of its preceding clip as historical memory, then concatenate it with the current clip features and feed them into a small decoder-only language model . Due to its autoregressive nature, the information of the sequence naturally accumulates to the last few tokens [38; 34]. Consequently, we take these last few tokens as an updated memory that encapsulates the video information up to the current timestamp. Through this streaming encoding, we explicitly take long-term temporal relations into consideration and maintain a fixed-length memory to represent an arbitrarily long video.

However, this fixed-length memory inevitably loses detailed information, especially in early contexts. To address this problem, we store the historical memories of all clips and select a constant number of subsets that are closely related to the question. To accomplish this, when streaming encoding each clip, we additionally append a summary token at the end of the sequence as a clip indicator that summarizes the clip contents within one token. Then, given a specific question, we concatenate the condensed memory from the final iteration with the question and pass it through the same small language model used in streaming encoding. We take the final token as the question indicator and calculate its similarity with all historical clip indicators, the clip indicator with higher similarity means its corresponding memory is more related to the question. Finally, we feed the adaptively selected memories into the LLM for detailed question answering.

In practice, we realize our VideoStreaming with a carefully designed two-stage progressive training process and long-video data construction strategy. In the first stage, we empower a small language model with the single-clip encoding capability by a specialized prefix task. In the second stage, it serves as the streaming encoder and we jointly train it with the LLM for long video understanding. Due to the lack of long video QA data, we manually constructed a set of long video QA pairs in two ways. On the one hand, we concatenate short videos from existing datasets [76; 81] into longer ones, where the original questions correspond to different segments. On the other hand, we curate a subset of Panda-70M  which includes captions for segmented clips as well as the original long videos, and use this to create multi-round long video QA pairs with explicit timestamps. These long video QA data not only optimize the responses from the LLM but also guide accurate memory selection.

In summary, our contributions are as follows: (1) We analyzed the challenge of long video understanding in the vision language area, and pointed out that the problem of current methods lies in the inefficient video encoding. (2) In response to the challenges, we propose two efficient designs: Memory-Propagated Streaming Encoding and Adaptive Memory Selection, which result in our advanced video understanding model VideoStreaming. (3) The extensive experiments demonstrate that our model achieves precise temporal grounding with respect to specific questions, attains superior performance, and exhibits higher inference efficiency on long video benchmarks.

## 2 Related Work

**Large Language Models** (LLMs) have revolutionized natural language processing. Early works establish encoder-decoder models with masked language modeling [16; 61], while later decoder-onlymodels like GPT  showcase remarkable performance and scalability. Recent groundbreaking works, such as PaLM , LLaMA  and GPT-4 , have pushed the boundaries by developing significantly larger models with billions of parameters. To harness the full potential of LLMs, a series of works [55; 57; 13] adopt supervised instruction tuning  to guide models towards generating more natural and contextually relevant responses. Inspired by the powerful reasoning capacities of LLMs, we explore using LLMs for challenging long video understanding.

**Vision Language Models** like CLIP  employ contrastive learning on image-text pairs to formulate a unified embedding space [59; 28; 40]. Later, [46; 39; 56; 90; 3; 4; 85] integrate image features into LLMs and achieve promising visual reasoning. To process more complex video data, [50; 48; 44; 25; 86] use sparse sampling or simple temporal pooling to obtain compact video tokens for LLMs. [42; 88] employ Q-Former  to project frame-wise features into the textual space. To handle longer videos, [31; 75] utilize token merging  to reduce redundancy and alleviate computational burden. LLaMA-VID  proposes an instruction-aware compression strategy to represent each frame with only two tokens, but it overlooks the temporal relations in the compression step. [66; 22] develop memory banks to accumulate information in long videos and excel in global video comprehension. However, these methods struggle with moment-specific questions without explicit time indicators. To address these limitations, we propose a memory-propagated streaming encoding architecture with adaptive memory selection, which effectively reduces temporal redundancy and accurately selects relevant information for detailed question answering.

**Long Video Understanding** is a challenging task in computer vision. The most prevalent strategy is to maintain a memory bank to store history information in long videos [77; 7; 78; 54; 12; 71; 21]. Typically, MemDPC  formulates a memory bank shared across the whole dataset as an external knowledge reference for future prediction, but it is not explicitly designed for long-term understanding. To facilitate long video content analysis and ensure computation efficiency, MC-ViT , MovieChat  and StreamingCaption  rely on handcrafted rules like clustering to compress the history into a finite-length memory and iteratively update it in video streams. A potential drawback is that the memory cannot be optimized through the extensive video caption data. It is potentially more promising to learn comprehensive memory consolidation in a data-driven manner. More recently, [32; 27; 87] use language as a bridge for long-term video understanding. They first divide a long video into short clips, generate textual descriptions for each clip, and then employ an LLM to aggregate the short captions for long video analysis. However, this architecture cannot be trained end-to-end, and the long video understanding quality depends on the short clip captions. In contrast, we employ a trainable small language model to iteratively encode short clips into compact memories, which can be jointly optimized with the subsequent LLM for long video understanding.

## 3 VideoStreaming

In this section, we introduce VideoStreaming, a streaming long video understanding framework with LLM. As illustrated in Fig. 1a, given a long video input, VideoStreaming segments it into multiple short clips and iteratively encodes each clip into compact historical memory. To enhance the reasoning ability to specific questions, we design an adaptive memory selection strategy to select a subset of relevant memories and feed them into an LLM to produce detailed responses.

### Single Clip Encoding

To effectively distill the information within a sequence into a compact set of tokens, we take inspiration from recent advanced decoder-only language models [2; 70; 13; 5; 29] and employ a comparatively small language model, Phi-2 , for efficient encoding. Due to the causal attention and autoregressive nature, it is intuitive to utilize a language model to aggregate the sequence information onto the last few tokens [38; 34], which naturally serve as a compact representation that provides a high-level summary of the input sequence.

Mathematically, given a \(T\)-frame video clip, we first use a pre-trained CLIP ViT-L  to extract frame-wise features and concatenate every four spatially adjacent visual tokens along channel dimension to reduce the number of tokens by 75%. The resulting clip features are denoted as \(^{TN C}\), where \(N\) denotes the per-frame spatial token number, and \(C\) is the channel dimension. To produce the condensed representations, we initialize a set of summarization tokens \(^{TP C}\) by adaptively pooling each frame into \(P\) tokens, where \(P N\). Intuitively, \(\) can be regarded as a with the modified attention mask, the \(TT\) text tokens can only get video-related information from the \(TP\) summarization tokens to predict the next token. This encourages the summarization tokens to extract more video information from previous \(TN\) video clip tokens, _ie._ learns better video encoding.

### Memory-Propagated Streaming Long Video Encoding

Till this point, we have obtained an encoder capable of distilling short video clips into condensed representations. The next step is to comprehensively consider the long-term temporal relations within the complete videos, leveraging the historical information from previous clips to facilitate the encoding of subsequent segments as depicted in Fig. 0(b).

To accomplish this objective, we divide a long video into \(K\) clips, each containing \(T\) frames, and propose a memory-propagated streaming encoding mechanism to iteratively encode each clip in

Figure 1: Fig. 0(a) shows an overview of VideoStreaming, where we segment a long video into short clips and iteratively encode each clip into compact memories. Then, according to specific questions, we select a constant number of subsets of relevant memories as input to an LLM to produce responses. The \(\) and \(\) respectively denote selected and unselected memories. Fig. 0(b) illustrates the detailed process of each streaming encoding iteration. We encode current clip features with reference to specific timestamps and historical memory from the preceding clip into a condensed representation. coarse encapsulation of the given clip, making it well-suited to serve as the summarization tokens for consolidating the clip information. To this end, we concatenate \(\) with \(\) and feed them into the encoder \(g()\), which consists of an MLP projector and a language model Phi-2. We utilize the output of the last \(T P\) tokens as the condensed representation of the given clip:

\[=g([])^{TP D},\] (1)

where \(\) denotes concatenation operation, \(D\) is the channel dimension of Phi-2.

To reinforce the visual consolidation ability, we design a prefix task to train the encoder on visual captioning and question-answering tasks. In particular, to guarantee that the clip information is distilled into the summarization tokens, we enforce the language model to generate the response only with reference to these few tokens. To achieve this goal, a straightforward way is to modify the attention mask in each Transformer decoder layer. As depicted in Fig. 2, we take a sequence covering \(TN\) clip feature tokens, \(TP\) summarization tokens, and \(TT\) text response tokens as an example. Based on the standard causal attribute, the binary attention mask \(\) is modified as shown in Figure 3:

Figure 2: Illustration of the prefix task format. Figure 3: Modified attention mask \(\).

sequence. In each iteration, we employ the encoded results from the last iteration as historical memory and integrate them with current clip features to produce an updated memory for subsequent encoding. Specifically, given the \(k\)-th clip, we denote the current clip features as \(_{k}^{T{N} C}\), the summarization tokens as \(_{k}^{TP C}\), and an additional global token as \(}_{k}^{1 C}\). This global token, initialized by global average pooling on the clip features \(_{k}\), is expected to summarize the entire clip contents and serve as a clip indicator for memory selection in the next subsection. To enrich the temporal contexts, we refer to the encoded representations from the previous clip \(_{k-1}^{TP D}\) to provide historical information. Then we jointly feed them into the streaming encoder to produce the condensed representation \(_{k}^{TP D}\) and the clip indicator \(}_{k}^{1 D}\) of the \(k\)-th clip:

\[_{k},}_{k}=g([_{k-1}_{k}_{k} }_{k}]).\] (2)

Note that for the first clip encoding, the historical memory is not used. Through this streaming encoding process, \(_{k}\) not only encompasses the current clip information but encapsulates the overall video content up to the \(k\)-th clip. To this end, we manage to maintain a fixed length of memory to represent arbitrarily long videos.

**Discussion.** In this architecture, we use a language model for video encoding, which has the unique advantage that we can flexibly provide the encoder with diverse prompts to guide the encoding process. Hence, the summarization tokens capture not only the core content but also additional contextual information. Typically, the explicit timestamp is an important cue in videos . As shown in Fig. 1b, we incorporate a text prompt indicating the specific timestamps of each clip and historical memory to enhance temporal awareness. Besides, this prompt-based approach also allows the user to tailor the condensed output to better suit the needs of downstream tasks, going beyond a purely extractive summarization.

Another noteworthy point is that in the language model, the feature space of the final decoder layer is designed for the next token prediction, which may not perfectly align with the objective of producing condensed video representations. Considering that we modify the attention masks in each decoder layer to encourage information consolidation, this allows us to leverage the intermediate outputs from partial attention layers as the encoded results. Similar to the techniques in vision domain [82; 46; 17], this strategy potentially enables the model to capture a richer set of semantic and contextual features as the condensed representations, bridging the gap between the language model's original training objective and the requirements for video encoding.

### Adaptive Memory Selection

Through the streaming video encoding, it is feasible to use the encoded results from the final iteration, i.e., \(_{K}\), as a compact global memory that concludes the entire video. However, this fixed-length memory inevitably loses details, especially the information from early segments. Hence, this global memory alone is insufficient for comprehensive long video understanding.

To address this limitation, we make use of the encoded results of all historical clips of the input video, i.e., \(=\{_{1},_{2},...,_{K}\}\). Given a specific question or instruction, we first generate an adaptive indicator that summarizes relevant video content for that particular instruction. We accomplish this by reusing the language model in the streaming encoder, where we concatenate the global memory from the final iteration, \(_{K}\), and the instruction texts, then pass the sequence into the model. We employ the output of the final token as the instruction indicator, denoted as \(}_{Q}^{1 D}\). Thereafter, we calculate the cosine similarity between this instruction indicator and all historical clip indicators \(\{}_{1},}_{2},...,}_{K}\}^{K  D}\) and obtain the similarity distribution \(^{K}\). To achieve a differentiable discrete selection, we adopt Gumbel-Topk technique  to produce a binary index \(\) that activates a subset of \(V\) out of \(K\) positions with the highest similarities:

\[=(,V)\{0,1\}^{K}.\] (3)

Based on \(\), we select the corresponding encoded results from \(\) to formulate a subset of memories that are related to the instruction:

\[}=\{_{k}_{k}|_{k}=1\},\] (4)

where \(_{k}\) denotes the selected indexes. We concatenate the selected memories \(}\) in temporal order, resulting in a sequence consisting of \(V T P\) tokens. Then, we feed the sequence with instruction texts into an LLM for comprehensive reasoning.

Our adaptive memory selection allows the model to dynamically access historical memories relevant to specific instructions, which mitigates the information loss inherent in the streaming encoding process. By drawing upon fine-grained details across the full video duration, the LLM can provide detailed and informative responses, while preserving high computational efficiency.

### Short-to-Long Training

To train VideoStreaming, we design a progressive two-stage paradigm. First, we train single clip encoding on image and short video understanding tasks. Next, we train memory-propagated streaming encoding and adaptive memory selection as well as the LLM for long video understanding.

**Single Clip Training.** In this stage, both image- and video-text pairs are used to train the encoder to handle general visual signals. Following [44; 50; 88; 43], we employ 790K image and short video caption data [64; 6] to train the MLP projector for modality alignment. After that, we employ 763K image and video instruction data from [46; 50; 45] to finetune the small language model. For video input, we uniformly sample \(T=16\) frames with spatial resolution \(224 224\) and use a frozen CLIP ViT-L/14  to extract frame-wise features. After adjacent token merging, we obtain \(16 64=1024\) tokens as the clip feature representation. Then, the encoder, a two-layer MLP and a small language model Phi-2 2.7B , distills each frame into \(P=4\) tokens, resulting in \(16 4=64\) tokens as the condensed representation with a compression ratio of \(16:1\). For image-text pairs, we regard the images as single-frame clips and encode each into \(4\) tokens. We use standard next token prediction to consolidate visual contents into compact summarization tokens as illustrated in Fig. 2.

**Streaming Long Video Training.** In the second stage, we use long video QA pairs to finetune the whole architecture, including ViT, the streaming encoder, and the LLM, as shown in Fig. 0(a). The long video QA data encompasses three parts. (1) We adopt 25K movie QA pairs from [43; 24; 66]. (2) We curate a subset from Panda-70M , which provides the original long videos and the captions of segmented clips. Based on this subset, we create 300K multi-round long video QA pairs with explicit timestamps. (3) We synthesize 20K long videos by concatenating short videos from existing QA datasets [81; 76], and the original QA pairs correspond to different segments in the synthesized long videos. For each video, we extract 16-frame clips at 1 FPS, and the number of clips varies with the video duration. In streaming encoding, we employ the intermediate outputs from the first \(16\) layers of Phi-2 as the condensed memories. Finally, we select \(V=4\) most relevant timestamps and feed the selected memories of \(V T P=256\) tokens into the LLM, Vicuna-7B , for long video reasoning. Since our curated long video data could provide pseudo temporal grounding labels of specific questions, we utilize 30K QA pairs to warm up memory selection via a KL divergence loss. Subsequently, we use the rest 315K QA pairs to optimize the responses from the LLM and guide memory selection in a weakly-supervised manner. More training details are included in Appendix A.

## 4 Experiments

### Datasets

We evaluate our model on long video QA datasets and present the statistics on the temporal duration of individual datasets in Table. 1. Among them, Next-QA , Next-GQA  and VideoChatGPT  encompass minute-long videos with thousands of frames. EgoSchema  contains over 5K three-minute videos with multiple-choice questions. Each question has a long temporal certificate, requiring more than 100 seconds within a video to produce a correct answer. MovieChat-1K  and MovieNetQA  consist of around ten-minute-long or even hour-long movies, posing significant challenges for the model to comprehend the visual contents across such long time spans.

   Dataset & Duration \\  Next-QA  & 42.23 sec \\ Next-GQA  & 39.60 sec \\ VideoChatGPT  & 1.81 min \\ EgoSchema  & 3.00 min \\ MovieChat-1K  & 7.66 min \\ MovieNet-QA  & 108.26 min \\   

Table 1: The statistics of the average video duration time of each evaluation dataset.

   Method & Params & CI & DO & CU & TU & CO \\  Video-LLMA  & 7B & 1.96 & 2.18 & 2.16 & 1.82 & 1.79 \\ VideoChat  & 7B & 2.23 & 2.50 & 2.53 & 1.94 & 2.24 \\ VideoChatGPT  & 7B & 2.40 & 2.52 & 2.62 & 1.98 & 2.37 \\ MovieChat  & 7B & 2.76 & 2.93 & 3.01 & 2.24 & 2.67 \\ LongVLM  & 7B & 2.76 & 2.86 & 3.34 & 2.39 & 3.11 \\ LLAMA-VID  & 13B & 3.07 & 3.05 & 3.60 & 2.58 & 2.63 \\ PLLaVA  & 13B & 3.27 & 2.99 & 3.66 & 2.47 & 3.09 \\ Ours & 7B+1.3B & **3.33** & **3.27** & **3.73** & **2.74** & **3.15** \\   

Table 2: Results on VideoChatGPT benchmark .

### Main Results

In this section, we present the results of our 8.3B model (half of Phi-2 2.7B in streaming encoder and Vicuna-7B as the LLM). We omit the comparisons to proprietary LLMs.

**VideoChatGPT.** Table 2 presents the results on VideoChatGPT  in terms of Correctness of Information (CI), Detailed Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU) and Consistency (CO). Our model outperforms LLM-based video understanding methods on all five metrics, with a significant advantage in temporal understanding. It can be attributed to the memory-propagated streaming encoding architecture that explicitly captures temporal dynamics.

**EgoSchema.** In Table 3, we report the _zero-shot_ performance on the fullset test split of EgoSchema . MC-ViT  consolidates a long-term memory to memorize long contexts but requires finetuning on related dataset . LLM-based methods  curate answers from the captions of segmented video clips. However, these short-term captions cannot be optimized end-to-end and inevitably lose some detailed information. In contrast, we use a trainable streaming encoder to produce memory embeddings in long videos and feed them into an LLM to generate responses. Our model outperforms all zero-shot methods and is comparable to the finetuned MC-ViT, demonstrating the effectiveness of our streaming architecture for long-term temporal modeling.

**Next-QA.** In Table 4, we perform _zero-shot_ evaluation on the validation split of Next-QA  covering 5K multiple-choice questions. We respectively report the accuracy on Causal (C), Temporal (T) and Descriptive (D) subsets. Our method consistently surpasses all zero-shot counterparts. Typically, compared to LangRepo  with Mixtral-8\(\)7B , our 8.3B model improves the causal, temporal, and descriptive accuracy by 0.7%, 10.8%, 9.0% with considerably fewer model parameters.

**Next-GQA.** Besides the evaluation of the generated responses, we also assess the temporal grounding ability on Next-GQA . We calculate the Intersection of Prediction (IoP) and Intersection of Union (IoU), and use Acc@GQA to measure the accuracy of the correctly grounded predictions. According to the comparisons in Table 5, our simple similarity score based selection achieves the highest IoP and comparable IoU to SeViLA  with a specialized grounding module. Moreover, the highest Acc@GQA demonstrates the comprehensive capacity for grounding and high-level understanding.

**MovieChat-1K.** Table 6 shows the results on MovieChat-1K , including a global mode for overall long-term understanding and a breakpoint mode for detailed analysis of specific moments.

   Method & Params & C & T & D & All \\  _finetuned_ & & & & & \\ ML-ViT-L  & 424M & 44.4 & LLMa-VQA  & 7B & 72.7 & 69.2 & 75.8 & 72.0 \\  _zero-shot_ & & & & & & \\ FrozenBiLM  & 890M & 26.9 & InternVideo  & 478M & 43.4 & 48.0 & 65.1 & 49.1 \\ SeViLA  & 4B & 61.3 & 61.5 & 75.6 & 63.6 & 63.6 \\ Mistral  & 7B & 51.0 & 48.1 & 57.4 & 51.1 \\ LLoVi  & 7B & 55.6 & 47.9 & 63.2 & 54.3 \\ LangRepo  & 7B & 57.8 & 45.7 & 61.9 & 54.6 \\ LangRepo  & 8\(\)7B & 64.4 & 51.4 & 69.1 & 60.9 \\ Ours & 7B+1.3B & **65.1** & **62.2** & **78.1** & **66.2** \\   

Table 4: Results on the validation set of Next-QA . C, T, D denotes causal, temporal and descriptive splits.

   Method & Params & mIoP & IoP@0.5 & mIoU & mIoU@0.5 & Acc@GQA \\  _w/ specialized grounding module_ & & & & & \\ TempCILP  & 130M & 25.7 & 25.5 & 12.1 & 8.9 & 16.0 \\ SeViLA  & 4B & 29.5 & 22.9 & 21.7 & 13.8 & 16.6 \\  _w/o specialized grounding module_ & & & & & \\ LLoVi  & 7B & 20.7 & 20.5 & 8.7 & 6.0 & 11.2 \\ LangRepo  & 7B & 20.3 & 20.0 & 8.7 & 6.0 & 11.2 \\ LangRepo  & 8\(\)7B & 31.3 & 28.7 & 18.5 & 12.2 & 17.1 \\ Ours & 7B+1.3B & **32.2** & **31.0** & **19.3** & **13.3** & **17.8** \\   

Table 5: Results on Next-GQA . Acc@GQA is defined as the percentage of questions that are both correctly answered and visually grounded with IoP \( 0.5\).

In breakpoint mode, [66; 67] manually extract segments according to the timestamps in questions, while our model adaptively selects the related historical memories. Fig. 4 reveals that our selected timestamps are close to the ground-truths, and the higher breakpoint accuracy validates our adaptive selection effectively gathers the desired information from long contexts. Meanwhile, we reach significantly superior results in global mode, with the model's selection concentrated at the beginning and ending parts. On the one hand, the beginning of a movie often contains hints of global information while the middle comprises redundant details. On the other hand, the condensed memories near the end of the video encapsulate the entire video, making them quite suitable for global understanding.

**MovieNet-QA.** Finally, we show the results on MovieNet-QA  consisting of 100 hour-long movies. Inspired by , we use GPT-3.5 to produce scores in range 0-5 to evaluate the performance in overview, plot, and temporal understanding in Table 7. Specifically, LLaMA-VID  compresses each frame into two tokens, which are then combined with movie subtitles as input to an LLM. MovieLLM  further incorporates more generated data in training. These approaches largely rely on the texts for movie understanding, and only using visual frames leads to dramatic performance drop. Moreover, its frame-wise compression is dependent on specific questions. The model has to reprocess the entire movie to extract visual features for different questions, resulting in a high inference latency of over 10 seconds per question. Conversely, our architecture requires only once streaming encoding to obtain a general condensed representation and adaptively selects significantly fewer tokens as input to LLM to answer specific questions. Therefore, we achieve a higher inference speed of 5.32 seconds per question and attain promising movie understanding without using subtitles.

**Qualitative Results.** We also present qualitative examples in Fig. 5. Typically, in Fig. 4(a), our model accurately captures the detailed descriptions in the question, and precisely selects the relevant segments that contain the corresponding character. Moreover, in Fig. 4(b), given a two-hour long movie and a high-level question on the movie plot, without relying on subtitles, VideoStreaming can comprehend the intent of the question and select relevant scenes from the lengthy video. In particular, the model selects the scenes of tightrope walk, team disputes, and equipment setup, clearly illustrating the protagonist's challenges, thereby contributing to a comprehensive answer generation.

### Ablation Study

We first explore four different settings on the memory and selection design of our model in Table 8. The details are as follows: (1) No propagated memory and no temporal selection, the model degenerates into uniform sampling 64 frames for video understanding. (2) Without temporal selection, we adopt the memories of final 4 clips as the video-level representation. (3) Without propagated memory, we concatenate time indicators with questions to produce the question indicator to retrieve relative temporal segments. (4) The full architecture of VideoStreaming.

    &  &  \\   & Acc. & Score & Acc. & Score \\  VideoChat  & 57.8 & 3.00 & 46.1 & 2.29 \\ Video-LLaMA  & 51.7 & 2.67 & 39.1 & 2.04 \\ VideoChatGPT  & 47.6 & 2.55 & 48.0 & 2.45 \\ MovieChat  & 62.3 & 3.23 & 48.3 & 2.57 \\ MovieChat+  & 71.2 & 3.51 & 49.6 & 2.62 \\ Ours & **90.4** & **4.42** & **54.9** & **2.80** \\   

Table 6: Results on MovieChat-1K  global and breakpoint mode accuracy (Acc.) and score.

   Method & Text & Vision & Tokens & Latency & Overview & Plot & Temporal \\  LLaMA-VID  & ✓ & ✓ & 18430 & 16.03 sec & 3.09 & 3.31 & 2.02 \\ MovieLLM  & ✓ & ✓ & 18430 & 16.48 sec & 3.22 & 3.38 & 2.18 \\  LLaMA-VID  & ✗ & ✓ & 5477 & 10.47 sec & 2.28 & 2.88 & 1.46 \\ MovieLLM  & ✗ & ✓ & 5477 & 10.43 sec & 2.36 & 2.97 & 1.58 \\ Ours & ✗ & ✓ & 256 & 5.32 sec & **2.65** & **3.13** & **1.88** \\   

Table 7: Results on MovieNet-QA . We present the used modality, the average number of tokens input to LLM and the average inference latency per question for comprehensive comparison.

**Historical Memory.** In terms of the propagated memory in the streaming encoding process, i.e., \(_{k-1}\) in Eq 2, we report the fullset accuracy on EgoSchema  as well as global and breakpoint accuracy on MovieChat-1K  in Table 8. Typically, the historical memory significantly improves global understanding. This verifies our intuition that leveraging historical memory enables the model to produce a global representation that summarizes the entire video. Meanwhile, since we select a small portion of the encoded results from the long video as input to LLM, the propagated memory across clips increases tolerance for imperfect temporal selection, as it preserves previous contexts. Without memory, there would be a strict requirement on temporal selection accuracy to avoid completely losing necessary details, thus degrading the performance.

**Memory Selection.** We also validate the effects of our memory selection strategy. Comparing the results in Table. 8 with respect to temporal selection, we have two observations. First, for breakpoint mode needing detailed understanding of specific moments, the lack of temporal selection leads to dramatic performance drop. It is crucial to select the related clips otherwise the LLM cannot catch the necessary details. Second, the historical memories in streaming encoding process enable the encoded results from the final iterations to provide coarse summarization of the entire video. Whereas, as shown in Fig. 4, the beginning of the movie contains crucial cues for global understanding. Directly feeding the memories of final clips without temporal selection into LLM still results in information loss. These phenomena demonstrate the necessity of our adaptive selection for gathering detailed information over the long time span, which facilitates more accurate and informative responses.

**Sampling Strategy.** In default, we use clip-based sampling to segment a video into clips each consisting of a fixed number of frames. Alternatively, it is also feasible to directly sample at a moderate FPS, e.g., 1 FPS, and streamingly process the frames. Table 9 showcases the comparison between clip-based sampling and frame-based sampling at 1 FPS, and presents the average number of sampled frames on each dataset. The overall performance is close, which verifies that our model can well generalze to different scenarios. And on the longer MovieNet-QA videos, the frame-based sampling performs slightly better, as the clip-based approach limits the maximum number of clips, resulting in fewer total sampled frames for very long videos.

   Memory & Selection & Fullset & Global Acc. & Break. Acc. \\  ✗ & ✗ & 34.4 & 52.5 & 21.6 \\ ✓ & ✗ & 37.3 & 69.1 & 23.0 \\ ✗ & ✓ & 38.4 & 43.8 & 39.1 \\ ✓ & ✓ & 44.1 & 90.4 & 54.9 \\   

Table 8: Ablation studies on the effects of memory selection and historical memory in streaming encoding.

Figure 5: Examples of question answering and the selected timestamps based on specific instructions.

**Language Reasoning in Streaming Encoding.** Besides, we ablate the necessity of using a language model for memory consolidation. We compare our streaming encoder instantiated with partial layers of Phi-2.7B  with a conventional ViT-based approach, MC-ViT . We compare the results on EgoSchema fullset  and Next-QA , Global and Breakpoint Accuracy on MovieChat-1K , as well as the number of encoder layers and parameters in Table 10. It is clear that using language reasoning to extract video memories exceeds the vision-based strategy even with fewer parameters. The reasons are twofold. On the one hand, MC-ViT updates memory according to handcrafted rules like clustering. In contrast, using language model as the streaming encoder allows us to leverage extensive video caption/QA data to guide memory consolidation in an end-to-end manner. This data-driven strategy results in more comprehensive memories that facilitate general video understanding. On the other hand, since the memory is fed into a subsequent LLM for reasoning, it is easier to align the memory generated by a language model with the input space of LLM, thus improving performance.

**Streaming Encoder Architecture.** Additionally, we also explore two alternative language models as the streaming encoder. (1) A comparatively a small language model, Phi-2.7B , with different partial layers. (2) The previous layers of the large language model, Vicuna-7B . Interestingly, using comparatively fewer layers of the language model leads to better results. We conjecture this is because the language model is originally trained for next token prediction. Its feature space of the final Transformer decoder layer might not align with the objective of visual content condensation. Similar to [82; 46; 17], the shallower layers might produce feature embeddings that encode richer information and serve as more comprehensive condensed video representations. Besides, under the same parameters, the smaller language model retains more layers and consistently outperforms the previous layers of Vicuna. Meanwhile, the prefix task for training the streaming encoder requires first training the entire model for next token prediction, before using partial layers in later stages. Therefore, using Phi as the streaming encoder brings another advantage in reduced computation.

More ablation studies on temporal grounding supervision, the number of summarization tokens and selected timestamps, the time prompts, and the similarity measurement are included in Appendix D.

## 5 Conclusion

In this paper, we introduce a novel approach to tackle the complexities of long video understanding with large language models (LLMs). Our proposed memory-propagated streaming encoding architecture segments long videos into short clips and iteratively encodes each clip in sequence. By leveraging historical memory from preceding clips, we incorporate temporal dynamics into the encoding process and produce a fixed-length memory to encapsulate arbitrarily long videos. To further augment the detailed information for handling specific questions, we develop adaptive memory selection that selects relevant timestamps based on given instructions. This approach ensures that the most pertinent historical memories are utilized for question answering, thereby facilitating detailed and informative responses. Our model achieves superior performance with substantially fewer tokens and higher efficiency on extensive long video benchmarks. We demonstrate that memories from the streaming encoding significantly enhance global video understanding, while adaptive selection results in accurate temporal grounding with respect to specific questions.

   Encoder & Layers & Params & Fullset & Next-QA & Global Acc. & Break Acc. \\  MC-ViT & 24 & 0.4B & 32.3 & 53.1 & 71.2 & 40.4 \\ Phi & 4 & 0.3B & 36.4 & 59.6 & 77.3 & 46.2 \\ Phi & 8 & 0.7B & 39.8 & 63.2 & 84.3 & 49.2 \\ Phi & 12 & 1.0B & 42.5 & 65.1 & 87.4 & 51.2 \\ Phi & 16 & 1.3B & 44.1 & 66.2 & 90.4 & 53.7 \\ Phi & 24 & 2.0B & 43.8 & 66.0 & 90.0 & 53.7 \\ Phi & 32 & 2.7B & 41.3 & 64.8 & 87.2 & 51.5 \\ Vicuna & 3 & 0.7B & 39.8 & 63.2 & 84.3 & 49.2 \\ Vicuna & 6 & 1.3B & 39.5 & 64.1 & 85.5 & 50.1 \\ Vicuna & 12 & 2.7B & 40.2 & 64.4 & 85.7 & 50.1 \\   

Table 10: Ablation studies on the streaming encoder architecture.