# Improving Deep Learning Speed and Performance through Synaptic Neural Balance

Antonios Alexos

Department of Computer Science

University of California Irvine

Irvine, CA 92697

aalexos@uci.edu

&Ian Domingo

Department of Computer Science

University of California Irvine

Irvine, CA 92697

idomingo@uci.edu

&Pierre Baldi

Department of Computer Science

University of California Irvine

Irvine, CA 92697

pfbaldi@uci.edu

###### Abstract

We present experiments and their corresponding theory, demonstrating that synaptic neural balancing can significantly enhance deep learning speed, accuracy, and generalization, particularly on non-traditional compute paradigms. Given an additive cost function (regularizer) of the synaptic weights, a neuron is in balance if the total cost of its incoming weights equals that of its outgoing weights. For various networks, activation functions, and regularizers, neurons can be balanced using scaling operations without altering their functionality, associated with a strictly convex optimization problem. In our simulations, we systematically observe that: (1) Fully balancing before training results in better performance as compared to several other training approaches; (2) Interleaving partial (layer-wise) balancing and stochastic gradient descent steps during training results in faster learning convergence and better overall accuracy (with \(L_{1}\) balancing converging faster than \(L_{2}\) balancing; and (3) When given limited training data, neural balanced models outperform plain or regularized models. and this is true both for both feedforward and recurrent networks. These balancing operations are entirely local, making them viable for biological or neuromorphic systems. This positions synaptic neural balancing as a promising approach for leveraging the unique characteristics of emerging AI accelerators, advancing the efficiency and sustainability of machine learning.

## 1 Introduction

Broadly speaking, neural balance refers to the idea of achieving or keeping a certain equilibrium in a neural network during training or after training, whereby such equilibrium may facilitate better information flow, or lower energy expenditure Shwartz-Ziv (2022). As such, there are different notions of neural balance including, for example, the notion of balance between excitation and inhibition in biological neural networks (Froemke, 2015; Field et al., 2020; Howes and Shatalina, 2022; Kim and Lee, 2022; Shirani and Choi, 2023). Here we develop the concept of synaptic neural balance which refers to any systematic relationship between the input and output synaptic weights of individual neurons, or layers of neurons. Specifically, we consider the case where the cost of the input weights is equal to the cost of the output weights, where the cost is defined by some regularizer. One of the most basic examples of such a relationship, described below, is when the sum of the squares ofthe input weights of a neuron is equal to the sum of the squares of its output weights. In this work, we briefly describe the theory of synaptic neural balance and demonstrate its applications to deep learning regularization. We now describe the base case of synaptic neural balance.

**Base Case:** Consider a neuron with a ReLU activation function inside a network trained to minimize a regularized error function \(=E+R\), where \(E\) is the data-dependent error (typically the negative log-likelihood of the data) and \(R\) is the regularizer (typically \(L_{2}\) regularizer). If we multiply the incoming weights of the neuron by some \(>0\) (including the bias) and divide the outgoing weights of the neuron by the same \(\), it is easy to see that this scaling operation does not affect in any way the contribution of the neuron to the rest of the network. Thus, the error \(E\) which depends only on the input-output function of the network is unchanged. However, the value of the \(L_{2}\) regularizer changes continuously with \(\), and the corresponding contribution is given by:

\[_{i IN}( w_{i})^{2}+_{i OUT}(w_{i}/)^{2}=^ {2}A+}B\] (1)

where \(IN\) and \(OUT\) denote the set of incoming and outgoing weights respectively, \(A=_{i IN}w_{i}^{2}\), and \(B=_{i OUT}w_{i}^{2}\). When \(\) moves away from 1, the contribution increases in one direction and decreases in the other. In the direction where it decreases, we can solve for the value \(^{*}\) associated with the mimimal cost. Without taking derivatives, we note that the product of the two terms on the right-hand side of Equation 1 is equal to \(AB\) and does not depend on \(\). Thus, the minimum is achieved when these two terms are equal, which yields: \((^{*})^{4}=B/A\) for the optimal \(^{*}\). The corresponding new set of weights, \(v_{i}=^{*}w_{i}\) for the input weights and \(v_{i}=w_{i}/^{*}\) for the outgoing weights, must be balanced: \(_{i IN}v_{i}^{2}=_{i OUT}v_{i}^{2}\). This is because the optimal scaling factor for the optimal synaptic weights can only be \(^{*}=1\). Thus, we can define two operations that can be applied to the incoming and outgoing weights of a neuron: scaling and balancing. In between, we can also consider favorable scaling, or partial balancing, where \(\) is chosen to reduce the cost without necessarily minimizing it.

There have been isolated previous studies of this kind of synaptic balance (Du et al., 2018; Stock et al., 2022) under special conditions. For instance, in Du et al. (2018), it is shown that if a deep network is initialized in a balanced state with respect to the sum of squares metric, and if training progresses with an infinitesimal learning rate, then balance is preserved throughout training. However, using an infinitesimal learning rate is not practical. Furthermore, there are many intriguing questions that can be raised. For instance: Why does balance occur? Does it occur only with ReLU neurons? Does it occur only with \(L_{2}\) regularizers? Does it occur only in fully connected feedforward architectures? Does it occur only at the end of training? What happens if we iteratively balance neurons at random in a large network? And can partial or full balancing, before or during learning, be used as an effective regularization technique? All these questions, but the last one, are addressed by the theory of synaptic neural balance that we have developed and briefly describe in the next section. The last question, on using balancing as a learning regularizer, is the main topic of this paper and is addressed by the experiments presented in the following sections. Unless otherwise specified, throughout the paper, terms like "balancing" or "neural balancing" refer to "synaptic neural balancing".

## 2 The Theory of Synaptic Neural Balance

We present a brief summary of the main point of the theory. The complete theory is described in the Appendix with the detailed proofs of all the theorems. The first key point is that the base case described in the Introduction, can be extended in three main directions in terms of the activation functions, the regularizers, and the network architectures.

**Theorem:** (Balance and Regularizer Minimization) _Consider a neural network with BiLU activation functions in all the hidden units and overall error function of the form:_

\[=E(W)+R(W) R(W)=_{w}g_{w}(w)\] (2)

_where each function \(g_{w}(w)\) is continuously differentiable, depends on the magnitude \(|w|\) alone, and grows monotonically from \(g_{w}(0)=0\) to \(g_{w}(+)=+\). For any setting of the weights \(W\) and any hidden unit \(i\) in the network and any \(>0\) we can multiply the incoming weights of \(i\) by \(\) and the outgoing weights of \(i\) by \(1/\) without changing the overall error \(E\). Then, for any neuron, there exists at least one optimal value \(^{*}\) that minimizes \(R(W)\). Any optimal value must be a solution of the consistency equation:_

\[^{2}_{w IN(i)}wg^{}_{w}( w)=_{w OUT(i)}wg^{ }_{w}(w/)\] (3)

_Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance equation:_

\[_{w IN(i)}wg^{}(w)=_{w OUT(i)}wg^{}(w)\] (4)

_In particular, if \(g_{w}(w)=|w|^{p}\) for all the incoming and outgoing weights of neuron \(i\), then the optimal value \(^{*}\) is unique and equal to:_

\[^{*}=|w|^{p}}{_{w IN(i)}|w|^{p}} ^{1/2p}=}{||IN(i)||_{p}}^{1/2}\] (5)

_The decrease \( R 0\) in the value of the \(L_{p}\) regularizer \(R=_{w}|w|^{p}\) is given by:_

\[ R=_{w IN(i)}|w|^{p}^{1/2}-_{w  OUT(i)}|w|^{p}^{1/2}^{2}\] (6)

_After balancing neuron \(i\), its new weights satisfy the generalized \(L_{p}\) balance equation:_

\[_{w IN(i)}|w|^{p}=_{w OUT(i)}|w|^{p}\] (7)

_Proof:_ The proof is given in the Appendix. We use the optimal value \(^{*}\), which we proved how to find in the Appendix, for our experiments in the next Section.

_Network Architectures:_ It is easy to see that the reasoning behind the base case can be applied to any BiLU neurons inside any architecture, such a fully connected feedforward, locally connected feedforward, or recurrent. Again this is because scaling does not change the effect of the neuron on the rest of the network and therefore we can always scale the neuron in a way that minimizes a particular cost function or regularizer. It is even possible to train a network with a certain regularizer and balance it with respect to a different regularizer. This brings us to the main result of the theory which is related to balancing algorithms. Imagine that we have a neural network containing BiLU (e.g. ReLU) neurons, with a fixed set of weights \(W\). These could be the weights before learning has started, during learning (i.e. at a particular epoch), or after learning has finished. Imagine that we start balancing the BiLU neurons one after the other, in some regular order or, more generally, even in a stochastic order. Balancing the weights of a neuron may break the balance of another neuron. So while the value of the regularizer always decreases after each balancing operation, it is not clear what happens to the weights of the network, whether they converge to a stable value, and if so whether this value is unique. The main theorem of the theory is the proof that indeed not only the regularizer converges, but the weights themselves must converge and, most interestingly, they must converge to a unique point, which depends only on the initial set of weights \(W\). The limit does _not_ depend on the order in which the balancing operations are applied.

## 3 Related Work

Yang et al. (2022) proposed to replace the \(L_{2}\) regularization term in the loss with the sum of products of l2 norms of the input and output weights. Stock et al. (2022) proposed a new local heterosynaptic learning rule by adding a kind of reconstruction loss term in which neurons try to balance themselves. Du et al. (2018) proved that gradient descent with infinitesimal step size effectively conserves the differences between squared norms of inputs and outputs weights of each layer without explicit regularization. Related results are also described in Arora et al. (2018). Saul (2023) computesmultiplicative rescaling factors--one at each hidden unit-- to balance the weights of neural networks. Neyshabur et al. (2015a) shows that training with stochastic gradient descent does not work well in highly unbalanced neural networks, so they proposed a rescaling-invariant solution Neyshabur et al. (2015c). Others have proposed that learning in neural networks can be accelerated with rescaling transformations Zhao et al. (2022), Armenta et al. (2023) without mentioning balancing the weights though. In our case we present both theoretical results on neural synaptic balance, including the existence and uniqueness of a globally balanced state (given an initial set of weights \(W\)), and experimental results showing that balancing neurons can expedite learning convergence and improve learning performance. This is also the first work experimenting with neural balance in both feedforward and recursive neural networks.

## 4 Experiments and Results

In our experiments, we train and compare various neural network architectures using full neural balancing, partial balancing, and \(L_{1}\) or \(L_{2}\) regularization. The term "plain" is used to refer to training of neural networks without balancing or regularizers. Full balance is obtained by iteratively balancing all BiLU neurons in the network until convergence is achieved. Partial balance is implemented by balancing the neurons in a layer-wise fashion, starting from the input layer and moving towards the output layer or vice-versa (no significant differences are observed). Due to the gradual nature of partial balance, the periodicity of the balancing operation is key to its implementation. In partial balance, the balancing operation can be performed up to once per epoch. Through the use of partial balancing during training, it has been observed that the ratio of the norms of a neuron's output to input weights tends to equalize, irrespective of the periodicity of epochs that we perform partial balancing operations. We have also observed that partial balancing helps the network converge faster and achieve a balanced state as is expected in a fully-trained network, same is in full balancing. The balancing operations for each neuron in each layer take place in parallel so they do not impose a bottleneck during training. Our results suggest that neural balancing is effective in training various types of neural networks with limited data. Furthermore, this approach proves beneficial in reducing overfitting and enhancing generalization in data-scarce environments.

To ensure reproducibility and fairness, experiments comparing training methodologies use the same range of seeds, learning rates, and train/test splits. Every experiment was run with 8 different seeds and the result reported is the average of them. A more detailed description of our experimental setup can be found in the Appendix. The roadmap of our experiments is organized as follows: first, we present experiments with the full dataset on both FCNs and RNNs. Then, we move onto data-scarce environments, amplifying the complexity of the experiments. For every experiment we deploy FCNs and RNNs ranging from smaller to larger sizes. The term FCN refers to Feedforward-layered networks with full connectivity between the layers.

### Assessment of Full Balance Before Training

In table 1, we assess the use of the full balancing operation before the commencement of training. Compared to a standard initialization, the application of full balancing results in faster convergence, and higher overall accuracy when using the same model architecture, hyperparameters, and training methodologies. Partial balancing at every epoch after a full balance results in the least change due to the fundamentally similar nature of the full balancing operation to the partial balancing operation,

  Type &  &  \\   & Plain & L1 Reg. & L2 Reg. & Plain & L1 Reg. & L2 Reg. \\ 
2 Layer FCN & 90.09\% & 90.05\% & 90.062\% & **91.22\%** & **93.96\%** & **91.18\%** \\ 
3 Layer FCN & 89.594\% & 89.67\% & 89.70\% & **90.83\%** & **93.47\%** & **90.79\%** \\ 
5 Layer FCN & 89.09\% & 87.85\% & 90.3\% & **91.37\%** & **95.50\%** & **91.59\%** \\  

Table 1: Test accuracy during training of Plain, L1 Regularized, and L2 Regularized Fully Connected Networks trained on MNIST, comparing full balancing before training with no full balance before training. Full balancing before training results in faster convergence, as well as universally higher attained test accuracy.

hence its omission from the plots. Repeated partial balancing results in the same outcome weights when using the same seed, albeit, over time since those weights aren't balanced from the start. Larger model sizes tend to exhibit a stronger correlation between the use of neural balancing, and the model's rate of convergence. These observations are especially exhibited in the normally trained models, where the use of full balancing at the start results in much faster convergence, as well as a higher final accuracy achieved by the model. table 1 displays the comparison between full balancing and no balancing performed on an FCN on MNIST before training. Each square in the grid represents a combination of a model size and a training methodology, where in every case, neural balancing results in an improvement in the rate of convergence and model accuracy.

### Partial Balance with FCNs on MNIST

We test all forms of neural balancing on the MNIST handwritten digit dataset exclusively through FCNs. To fully capture the regularization capability of neural balancing, we test on a range of model architectures. From table 2, we observe that neural partial balancing results in faster convergence and test accuracy across all model sizes.

### Full Balance with RNNs on IMBD

We continue our assessment of neural balancing with experiments performed on the RNN architecture. We train a 3-layered RNN on the IMDB sentiment analysis dataset, once again assessing full neural balancing with a 'plain', and regularized models. table 3 demonstrates that when full balancing is performed before training, the model has a better final accuracy when compared to equivalent, non-balanced methodologies.

### Neural Balance in Limited-data Environments

In data-scarce environments, models employing neural partial balancing techniques demonstrate accelerated convergence compared to unmodified models. These experiments are executed by stratifying samples equally according to their class labels to maintain a balanced distribution of classes within the training data. We observe that neural balancing results in higher accuracy and faster convergence, which could be attributed to better performance in data-scarce environments.

Further, we continue this study with an assessment of neural balancing on an RNN trained with a fraction of the original IMDB dataset. fig. 1 contains the comparison between training methodologies using a 3 layer RNN trained on the IMDB sentiment analysis dataset with 5% of the available training data. Partial balancing has higher accuracy on average, as well as a faster convergence, hinting at a characteristic of being able to generalize without a lot of training data.

  Type & Plain & L2 NB & L1 NB & L2 1e-5 & L1 1e-5 \\ 
2-FCN & 91.22\% & 91.19\% & **94.542\%** & 91.18\% & 93.96\% \\ 
3-FCN & 90.84\% & 90.86\% & **93.94\%** & 90.79\% & 93.47\% \\ 
5-FCN & 91.37\% & 91.63\% & **96.26\%** & 91.59\% & 95.48\% \\  

Table 2: Test accuracy across training comparisons of partial balancing, L2 Regularization, and Plain Accuracy for FCNs of varying sizes on MNIST. We observe that L1 partial balancing outperforms the other training methodologies on all model sizes

  Type & Plain & L1 Regularization & L2 Regularization \\  No NB at Start & 88.26\% & 88.23\% & 88.22\% \\  NB at Start & **88.64\%** & 88.24\% & 88.57\% \\  

Table 3: Test accuracy for a Recurrent Neural Network trained on the IMDB sentiment analysis dataset, comparing Plain, L1 Regularized, and L2 Regularized models with and without a full balance at the start of training. A full balance before the commencement of training universally results in a higher test accuracy during training.

### Discussion

Summing up our experiments we observe the following quantitative results. In FCNs, Neural Balance yields a notable improvement in model performance and convergence speed. Specifically, this method results in a 3-5% performance increase over plain models, and more than a 1% improvement over optimally L1-regularized models. Additionally, L1 neural balancing facilitates convergence at a rate 1.5 to 10 times faster, contingent on model size. When trained on limited datasets (1% of the full data), L1 neural balancing enhances performance by 3-10% compared to plain models, and by 1-5% relative to models regularized with L1 and L2 techniques. Moreover, it achieves up to a 10-fold increase in convergence speed, depending on model size. In RNNs, L1 neural balancing contributes to a 2-5% increase in convergence speed, with the application of L2 neural balancing leading to a more than 15% acceleration in convergence when training on 5% of the data. These findings underscore the efficacy of L1 neural balancing in optimizing both performance and training efficiency across different model architectures. We have extended our experiments due to page limits in the Appendix.

## 5 Conclusions

Synaptic balancing provides a novel approach to regularization that is supported by an underlying theory. Synaptic balancing is very general in the sense that it can be applied with all usual cost functions, including all \(L_{p}\) cost functions. Synaptic balancing can be carried in full or in partial manner, due to the convexity connection provided by the main theorem. It can be applied at any time during the learning process: at the start of learning, at the end of learning, or during learning, by alternating balancing steps with stochastic gradient steps. Given, neural balance has some limitations; as mentioned earlier it can be applied only to neurons with specific activation functions (BiLU or slightly more general activation functions as shown in the Appendix). Another limitation is that it cannot be applied to neurons in Convolution layers due to the nature of the convolution operation with the kernels. Simulations show that these approaches can improve learning in terms of speed (fewer epochs), accuracy or generalization abilities. Thus, in short, balancing is a novel effective approach to regularization that can be added to the list of tools available to regularize networks, like dropout, and other regularization tools. Finally, a neuron can balance its weights independently of all

  Type &  &  \\   & 2 Layer FCN & 3 Layer FCN & 5 Layer FCN & 2 Layer FCN & 3 Layer FCN & 5 Layer FCN \\  Plain & 84.15\% & 73.49\% & 88.9\% & **91.39\%** & **91.42\%** & **90.86\%** \\  L1 NB & 91.25\% & 90.57\% & 92.87\% & **93.26\%** & **93.3\%** & **92.92\%** \\  L2 NB & 87.99\% & 84.53\% & 91.06\% & **91.94\%** & **91.37\%** & **90.59\%** \\  L1 Reg. & 83.92\% & 72.03\% & 11.35\% & **85.99\%** & **81.33\%** & **19.79\%** \\  L2 Reg. & 83.35\% & 75.21\% & 86.81\% & **91.16\%** & **90.86\%** & **88.78\%** \\  

Table 4: A comparison of test accuracy during training of various methodologies, using 1% of the MNIST dataset to simulate a limited data environment. The use of full balancing at the start of training not only increases the rate of convergence of every training methodology, but also allows for a higher attainable overall accuracy.

Figure 1: A comparison between partial balancing, Plain Accuracy, and L2 Regularization as performed on a 3 Layer RNN using 5% of the available dataset. Neural balancing reports the best overall performance.

other neurons in the network. The knowledge required for balancing is entirely _local_ and available at each neuron. In this sense, balancing is a natural algorithm for distributed asynchronous architectures and physical neural systems, and as such it may find applications in neuromorphic chip designs or brain studies.