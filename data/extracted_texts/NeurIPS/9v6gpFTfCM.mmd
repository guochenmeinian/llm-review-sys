# Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?

Yutong He

Peking University

yutonghe@pku.edu.cn

&Xinmeng Huang

University of Pennsylvania

xinmengh@sas.upenn.edu

Equal Contribution. Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Laboratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.

Kun Yuan

Peking University

kunyuan@pku.edu.cn

Equal Contribution. Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Laboratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.

###### Abstract

Communication compression is a common technique in distributed optimization that can alleviate communication overhead by transmitting compressed gradients and model parameters. However, compression can introduce information distortion, which slows down convergence and incurs more communication rounds to achieve desired solutions. Given the trade-off between lower per-round communication costs and additional rounds of communication, it is unclear whether communication compression reduces the total communication cost.

This paper explores the conditions under which unbiased compression, a widely used form of compression, can reduce the total communication cost, as well as the extent to which it can do so. To this end, we present the first theoretical formulation for characterizing the total communication cost in distributed optimization with unbiased compressors. We demonstrate that unbiased compression alone does not necessarily save the total communication cost, but this outcome can be achieved if the compressors used by all workers are further assumed independent. We establish lower bounds on the communication rounds required by algorithms using independent unbiased compressors to minimize smooth convex functions and show that these lower bounds are tight by refining the analysis for ADIANA. Our results reveal that using independent unbiased compression can reduce the total communication cost by a factor of up to \((})\) when all local smoothness constants are constrained by a common upper bound, where \(n\) is the number of workers and \(\) is the condition number of the functions being minimized. These theoretical findings are supported by experimental results.

## 1 Introduction

Distributed optimization is a widely used technique in large-scale machine learning, where data is distributed across multiple workers and training is carried out through worker communication. However, dealing with a vast number of data samples and model parameters across workers poses a significant challenge in terms of communication overhead, which ultimately limits the scalability of distributed machine learning systems. To tackle this issue, communication compression strategies  have emerged, aiming to reduce overhead by enabling efficient yet imprecise message transmission. Instead of transmitting full-size gradients or models, these strategies exchange compressed gradients or model vectors of much smaller sizes in communication.

There are two common approaches to compression: quantization and sparsification. Quantization  maps input vectors from a large, potentially infinite, set to a smaller set of discrete values. In contrast, sparsification  drops a certain amount of entries to obtain a sparse vector for communication. In literature , these compression techniques are often modeledas a random operator \(C\), which satisfies the properties of unbiasedness \([C(x)]=x\) and \(\)-bounded variance \(\|C(x)-x\|^{2}\|x\|^{2}\). Here, \(x\) represents the input vector to be compressed, and \(\) is a fixed parameter that characterizes the degree of information distortion. Besides, part of the compressors can also be modeled as biased yet contractive operators .

While communication compression efficiently reduces the volume of vectors sent by workers, it suffers substantial information distortion. As a result, algorithms utilizing communication compression require additional rounds of communication to converge satisfactorily compared to algorithms without compression. This adverse effect of communication compression has been extensively observed both empirically  and theoretically . Since the extra rounds of communication needed to compensate for the information loss may outweigh the saving in the per-round communication cost from compression, this naturally motivates the following fundamental question:

_Q1. Can unbiased compression alone reduce the total communication cost?_

By "unbiased compression alone", we refer to the compression that solely satisfies the assumptions of unbiasedness and \(\)-bounded variance without any additional advanced properties. To address this open question, we formulate the total communication cost as the product of the per-round communication cost and the number of rounds needed to reach an \(\)-accurate solution to distributed optimization problems. Using this formulation, we demonstrate the decrease in the per-round communication cost from unbiased compression is completely offset by additional rounds of communication. Therefore, we answer Q1 by showing unbiased compression alone _cannot_ ensure a lower total communication cost, even with an optimal algorithmic design, see Sec. 3 for more details. This negative conclusion drives us to explore the next fundamental open question:

_Q2. Under what additional conditions and how much can unbiased compression provably save the total communication cost?_

Fortunately, some pioneering works  have shed light on this question. They impose _independence_ on unbiased compressors, _i.e._, the compressed vectors \(\{C_{i}(x_{i})\}_{i=1}^{n}\) sent by workers are mutually independent regardless of the inputs \(\{x_{i}\}_{i=1}^{n}\). This independence assumption enables an "error cancellation" effect, producing a more accurate compressed vector \(n^{-1}_{i=1}^{n}C_{i}(x_{i})\) and hence incurring fewer additional rounds of communication compared to dependent compressors. Consequently, the decrease in the per-round communication cost outweighs the extra communication rounds, reducing the total communication cost.

However, it remains unclear how much the total communication cost can be reduced _at most_ by independent unbiased compression and whether we can develop algorithms to achieve this optimal reduction. Addressing this question poses significant challenges as it necessitates a study of the optimal convergence rate for algorithms using independent unbiased compression.

This paper provides the _first_ affirmative answer to this question for convex problems by: (i) establishing lower bounds on convergence rates of distributed algorithms employing independent unbiased compression, and (ii) demonstrating the tightness of these lower bounds by revisiting ADIANA  and presenting novel and refined convergence rates nearly attaining the lower bounds. Our results reveal that compared to non-compression algorithms, independent unbiased compression can save the total communication cost by up to \((})\)-fold, where \(n\) is the number of workers and \([1,+]\) is the function condition number. Figure 1 provides a simple empirical justification. It shows independent compression (ADIANA i.d.) reduces communication costs compared to no compression (Nesterov's Accelerated algorithm), while dependent compression (ADIANA s.d.) does not, which validates our theory.

### Contributions

Specifically, our contributions are as follows:

Figure 1: Performance of ADIANA using random-\(s\) sparsification compressors with shared (s.d.) or independent (i.d.) randomness against distributed Nesterov’s accelerated algorithm with no compression in communication. Experimental descriptions are in Appendix F.1

* We present a theoretical formalization of the total communication cost in distributed optimization with unbiased compression. With this formulation, we demonstrate that unbiased compression alone is insufficient to save the total communication cost, even with an optimal algorithmic design. This is because any reduction in the per-round communication cost is fully offset by the additional rounds of communication required due to the presence of compression errors.
* We prove lower bounds on the convergence complexity of distributed algorithms using independent unbiased compression to minimize smooth convex functions. Compared to lower bounds when using unbiased compression without independence , our lower bounds demonstrate significant improvements when \(n\) and \(\) are large, see the first two lines in Table 1. This improvement highlights the importance of independence in unbiased compression.
* We revisit ADIANA  by deriving an improved rate for strongly-convex functions and proving a novel convergence result for generally-convex functions. Our rates nearly match the lower bounds, suggesting their tightness and optimality. Our optimal complexities reveal that, compared to non-compression algorithms, independent unbiased compression can decrease total communication costs by up to \((})\)-fold when all local smoothness constants are constrained by a common upper bound.
* We support our theoretical findings with experiments on both synthetic data and real datasets.

We present the lower bounds, upper bounds, and complexities of state-of-the-art distributed algorithms using independent unbiased compressors in Table 1. With our new and refined analysis, ADIANA nearly matches the lower bounds for both strongly-convex and generally-convex functions.

### Related work

**Communication compression.** Two main approaches to compression are extensively explored in literature: quantization and sparsification. Quantization coarsely encodes input vectors into fewer discrete values, \(e\)._g_., from 32-bit to 8-bit integers . Schemes like Sign-SGD  use 1 bit per entry, introducing unbiased random information distortion. Other variants such as Q-SGD , TurnGrad , and natural compression  quantize each entry with more effective bits. In contrast, sparsification either randomly zeros out entries to yield sparse vectors , or transmits only the largest model/gradient entries .

  
**Method** & **GC** & **SC** \\ 
**Lower Bound** & \((()+(1+ })}{})\) & \(((+(1+}))())\) \\ Lower Bound \({}^{}\) & \(((1+)}{})\) & \(((1+)())\) \\  CGD \({}^{}\) & \(((1+)})\) & \(}((1+)())\) \\ ACGD \({}^{}\) & \(((1+)}{})\) & \(}((1+)())\) \\ DIANA  & \(((1++}{n+}))\) & \(}((+(1+}) ()).\) \\ EF21 \({}^{}\) & \(}((1+))\) & \(}((1+)())\) \\ ADIANA  & — & \(}((+(1+}{n^{1/4}}+ }))())\) \\ CANITA \({}^{}\) & \((}}{}+(1+ }{n^{1/4}}+})}{})\) & — \\ NEOLITHIC \({}^{}\) & \(}((1+)}{})\) & \(}((1+)())\) \\ 
**ADIANA (Thm. 3)** & \((}{}+(1+})}{})\) & \(}((+(1+}))())\) \\    \({}^{}\) Results obtained in the single-worker setting and cannot be extended to the distributed setting.

\({}^{}\) The rate is obtained by correcting mistakes in the derivations of . See details in Appendix E.

\({}^{}\) Results hold without assuming independence across compressors.

Table 1: Lower and upper bounds on the number of communication rounds for distributed algorithms using unbiased compression to achieve an \(\)-accurate solution. Notations \(,n,L,\) (\( L/ 1\)) are defined in Section 2. \(\) is a parameter for unbiased compressors (Assumption 2). \(}\) and \(\) hides logarithmic factors independent of \(\). GC and SC denote generally-convex and strongly-convex functions respectively.

**Error compensation.** Recent works [52; 59; 55; 4; 49] propose error compensation or feedback to relieve the effects of compression errors. These techniques propagate information loss backward during compression, thus preserving more useful information. Reference  uses error compensation for 1-bit quantization, while the work  proposes error-compensated quantization for quadratic problems. Error compensation also reduces sparsification-induced errors  and is studied for convergence in non-convex scenarios . Recently, the work  introduces EF21, an error feedback scheme that compresses only local gradient increments with improved theoretical guarantees.

**Lower bounds.** Lower bounds in optimization set a limit for the performance of a single or a class of algorithms. Prior works have established numerous lower bounds for optimization algorithms [1; 15; 6; 43; 7; 21; 64; 17; 44]. In the field of distributed optimization with communication compression, reference  provides an algorithm-specific lower bound for strongly-convex functions, while the work  establishes the bit-wise complexities for PL-type problems, which reflect the influence of the number of agents \(n\) and dimension \(d\), but not the condition number \(\) and the compression \(\). In particular,  characterizes the optimal convergence rate for all first-order and linear-spanning algorithms, in the stochastic non-convex case, which is later extended by  to convex cases.

**Accelerated algorithms with communication compression.** There is a scarcity of academic research on compression algorithms incorporating acceleration, as evident in a limited number of studies [32; 33; 48]. References [32; 33] develop accelerated algorithms with compression in the strongly-convex and generally-convex cases, respectively. For distributed finite-sum problems, accelerated algorithms with compression can further leverage variance-reduction techniques to expedite convergence .

**Other communication-efficient strategies.** Other than communication compression studied in this paper, there are a few different techniques to mitigate the communication overhead in distributed systems, including decentralized communication and lazy communication. Notable examples of decentralized algorithms encompass decentralized SGD [12; 34; 30; 64], D2/Exact-Diffusion [56; 62; 61], gradient tracking [47; 60; 29; 2], and their momentum variants [35; 63]. Lazy communication allows each worker to either perform multiple local updates as opposed to a single communication round [38; 54; 41; 24; 14; 22], or by adaptively skipping communication [13; 36].

## 2 Problem setup

This section introduces the problem formulation and assumptions used throughout the paper. We consider the following distributed stochastic optimization problem

\[_{x^{d}} f(x)=_{i=1}^{n}f_{i}(x), \]

where the global objective function \(f(x)\) is decomposed into \(n\) local objective functions \(\{f_{i}(x)\}_{i=1}^{n}\), and each local \(f_{i}(x)\) is maintained by node \(i\). Next, we introduce the setup and assumptions.

### Function class

We let \(^{}_{L,}\) (\(0 L\)) denote the class of convex and smooth functions satisfying Assumption 1. We define \( L/[1,+]\) as the condition number of the functions to be optimized. When \(>0\), \(^{}_{L,}\) represents strongly-convex functions. Conversely, when \(=0\), \(^{}_{L,}\) represents generally-convex functions with \(=\).

**Assumption 1** (Convex and smooth function).: _We assume each \(f_{i}(x)\) is \(L\)-smooth and \(\)-strongly convex, i.e., there exists constants \(L 0\) such that_

\[\|y-x\|^{2} f_{i}(y)-f_{i}(x)- f_{i}(x),y-x \|y-x\|^{2}\]

_for any \(x,y^{d}\) and \(1 i n\). We further assume \(\|x^{0}-x^{}\|^{2}\) where \(x^{}\) is one of the global minimizers of \(f(x)=_{i=1}^{n}f_{i}(x)\)._

### Compressor class

Each worker \(i\{1,,n\}\) is equipped with a potentially random compressor \(C_{i}:^{d}^{d}\). We let \(_{}\) denote the set of all \(\)-unbiased compressors satisfying Assumption 2, and \(^{}_{}\) denote the set of all _independent_\(\)-unbiased compressors satisfying both Assumption 2 and Assumption 3.

**Assumption 2** (Unbiased compressor).: _We assume all compressors \(\{C_{i}\}_{i=1}^{n}\) satisfy_

\[[C_{i}(x)]=x,[\|C_{i}(x)-x\|^{2}]\|x\|^{2}, \,x^{d} \]

_for constant \( 0\) and any input \(x^{d}\), where the expectation is taken over the randomness of the compression operator \(C_{i}\).2_

**Assumption 3** (Independent compressor).: _We assume all compressors \(\{C_{i}\}_{i=1}^{n}\) are mutually independent, i.e., outputs \(\{C_{i}(x_{i})\}_{i=1}^{n}\) are mutually independent random variables for any \(\{x_{i}\}_{i=1}^{n}\)._

### Algorithm class

Similar to , we consider centralized and synchronous algorithms in which first, every worker is allowed to communicate only directly with a central server but not between one another; second, all iterations/communications are synchronized, meaning that all workers start each of their iterations simultaneously. We further require algorithms to satisfy the so-called "linear-spanning" property, which appears in  (see formal definition in Appendix C). Intuitively, this property requires each local model \(x_{i}^{k}\) to lie in the linear manifold spanned by the local gradients and the received messages at worker \(i\). The linear-spanning property is satisfied by all algorithms in Table 1 as well as most first-order methods .

Formally, this paper considers a class of algorithms specified by Definition 1.

**Definition 1** (Algorithm class).: _Given compressors \(\{C_{i}\}_{i=1}^{n}\), we let \(_{\{C_{i}\}_{i=1}^{n}}\) denote the set of all centralized, synchronous, linear-spanning algorithms admitting compression in which compressor \(C_{i}\), \(\,1 i n\), is applied for the messages sent by worker \(i\) to the server._

For any algorithm \(A_{\{C_{i}\}_{i=1}^{n}}\), we define \(^{k}\) and \(x_{i}^{k}\) as the output of the server and worker \(i\) respectively, after \(k\) communication rounds.

### Convergence complexity

With all the interested classes introduced above, we are ready to define our complexity metric for convergence analysis. Given a set of local functions \(\{f_{i}\}_{i=1}^{n}_{L,}^{}\), a set of compressors \(\{C_{i}\}_{i=1}^{n}\) (\(=_{}^{}\) or \(_{}\)), and an algorithm \(A_{\{C_{i}\}_{i=1}^{n}}\), we let \(_{A}^{t}\) denote the output of algorithm \(A\) after \(t\) communication rounds. The convergence complexity of \(A\) solving \(f(x)=_{i=1}^{n}f_{i}(x)\) under \(\{(f_{i},C_{i})\}_{i=1}^{n}\) is defined as

\[T_{}(A,\{(f_{i},C_{i})\}_{i=1}^{n})=\{t:[f(_{A}^{t})]-_{x}f(x)\}. \]

This measure corresponds to the number of communication rounds required by algorithm \(A\) to achieve an \(\)-accurate optimum of \(f(x)\) in expectation.

**Remark 1**.: _The measure in (3) is commonly referred to as the communication complexity in literature . However, we refer to it as the convergence complexity here to avoid potential confusion with the notion of "communication complexity" and "total communication cost". This complexity metric has been traditionally used to compare communication rounds used by distributed algorithms . However, it cannot capture the total communication costs of multiple algorithms with different per-round communication costs, e.g., algorithms with or without communication compression. Therefore, it is unable to address the motivating questions Q1 and Q2._

**Remark 2**.: _The definition of \(T_{}\) can be independent of the per-round communication cost, which is specified only through the degree of compression \(\) (i.e., choice of compressor class). However, to be precise, we may further assume these compressors are non-adaptive with the same fixed per-round communication cost. Namely, the compressors output compressed vectors that can be represented by a fixed and common number of bits. Notably, such hypothesis of non-adaptive cost is widely adopted for practical comparison of communication costs and is valid when input \(x\) is bounded or can be encoded with finite bits ._

## 3 Total communication cost

### Formulation of total communication cost

This section introduces the concept of Total Communication Cost (TCC). TCC can be calculated at both the level of an individual worker and of the overall distributed machine learning system comprising all \(n\) workers. In a centralized and synchronized algorithm where each worker communicates compressed vectors of the same dimension, the TCC of the entire system is directly proportional to the TCC of a single worker. Therefore, it is sufficient to use the TCC of a single worker as the metric for comparing different algorithms. In this paper, we let TCC denote the total communication cost incurred by each worker in achieving a desired solution when no ambiguity is present.

Let each worker to be equipped with a non-adaptive compressor with the same fixed per-round communication cost, _i.e._, the compressor outputs compressed vectors of the same length (size), the TCC of an algorithm \(A\) to solve problem (1) using a set of \(\)-unbiased compressors \(\{C_{i}\}_{i=1}^{n}\) in achieving an \(\)-accurate optimum can be characterized as

\[_{}(A,\{(f_{i},C_{i})\}_{i=1}^{n}):=(\{C_{i}\}_{i=1}^{n}) T_{}(A,\{(f_{i},C_{i})\}_{i=1}^{n}). \]

### A tight lower bound for per-round cost

The per-round communication cost incurred by \(\{C_{i}\}_{i=1}^{n}\) in (4) will vary with different \(\) values. Typically, compressors that induce less information distortion, _i.e._, associated with a smaller \(\), incur higher per-round costs. To illustrate this, we consider random-\(s\) sparsification compressors, whose per-round cost corresponds to the transmission of \(s\) entries, which depends on parameter \(\) through \(s=d/(1+)\) (see Example 1 in Appendix A). Specifically, if each entry of the input \(x\) is numerically represented with \(r\) bits, then the random-\(s\) sparsification incurs a per-round cost of \(rd/(1+)\) bits up to a logarithm factor.

The following proposition, motivated by the inspiring work , establishes a lower bound of TCC when using any compressor satisfying Assumption 2.

**Proposition 1**.: _Let \(x^{d}\) be the input to a compressor \(C\) and \(b\) be the number of bits needed to compress \(x\). Suppose each entry of input \(x\) is numerically represented with \(r\) bits, i.e., errors smaller than \(2^{-r}\) are ignored. Then for any compressor \(C\) satisfying Assumption 2, the per-round communication cost of \(C(x)\) is lower bounded by \(b=_{r}(d/(1+))\) where \(r\) is viewed as an absolute number in \(_{r}()\) (See the proof in Appendix B)._

Proposition 1 presents a lower bound on the per-round cost of an arbitrary compressor satisfying Assumption 2. This lower bound is tight since the random-\(s\) compressor discussed above can achieve this lower bound up to a logarithm factor. Since \(d\) only relates to the problem instance itself and \(r\) is often a constant absolute number in practice, _e.g._, \(r=32\) or \(64\), both of which are independent of the choices of compressors and algorithm designs, they can be omitted from the lower bound order. As a result, the TCC in (4) can be lower bounded by

\[_{}=((1+)^{-1}) T_{}(A,\{(f_{i},C _{i})\}_{i=1}^{n}). \]

Notably, when no compression is employed (i.e., \(=0\)), \(_{}=(1) T_{}(A,\{(f_{i},C_{i})\}_{i=1}^{ n})\) is consistent with the convergence complexity.

## 4 Unbiased compressor alone cannot save total communication cost

With formulation (5), given the number of communication rounds \(T_{}\), the total communication cost can be readily characterized. A recent pioneer work  characterizes a tight lower bound for \(T_{}(A,\{(f_{i},C_{i})\}_{i=1}^{n})\) when each \(C_{i}\) satisfies Assumption 2.

**Lemma 1** (, Theorem 1, Informal).: _Relying on unbiased compressibility alone, i.e., \(\{C_{i}\}_{i=1}^{n}_{}\), without leveraging additional property of compressors such as mutual independence, the fewest rounds of communication needed by algorithms with compressed communication to achieve an \(\)-accurate solution to distributed strongly-convex and generally-convex optimization problems are lower bounded by \(T_{}=((1+))\) and \(T_{}=((1+))\), respectively._

Substituting Lemma 1 into our TCC lower bound in (5), we obtain \(_{}=((1/))\) or \(()\) in the strongly-convex or generally-convex case, respectively, by relying solely onunbiased compression. These results do not depend on the compression parameter \(\), indicating that _the lower per-round cost is fully compensated by the additional rounds of communication incurred by compressor errors_. Notably, these lower bounds are of the same order as optimal algorithms without compression such as Nesterov's accelerated gradient descent [43; 44], leading to the conclusion:

**Theorem 1**.: _When solving convex optimization problems following Assumption 1, any algorithm \(A_{\{C_{i}\}_{i=1}^{n}}\) that relies solely on unbiased compression satisfying Assumption 2 cannot reduce the total communication cost compared to not using compression. The best achievable total communication cost with unbiased compression alone is of the same order as without compression._

Theorem 1 presents a _negative_ finding that unbiased compression alone is insufficient to reduce the total communication cost, even with an optimal algorithmic design.3 Meanwhile, it also implies that to develop algorithms that provably reduce the total communication cost, one must leverage compressor properties beyond \(\)-unbiasedness as defined in (2). Fortunately, mutual independence is one such property which we discuss in depth in later sections.

## 5 Independent unbiased compressor provably saves communication

### An intuition on why independence can help

A series of works [40; 32; 33] have shown theoretical improvements in the total communication cost by imposing independence across compressors, _i.e._, \(\{C_{i}\}_{i=1}^{n}_{}^{}\). The intuition behind the role of independence among worker compressors can be illustrated by a simple example where workers intend to transmit the same vector \(x\) to the server. Each worker \(i\) sends a compressed message \(C_{i}(x)\) that adheres to Assumption 2. Consequently, the aggregated vector \(n^{-1}_{i=1}^{n}C_{i}(x)\) is an unbiased estimate of \(x\) with variance

\[[\|_{i=1}^{n}C_{i}(x)-x\|^{2}]= }(_{i=1}^{n}[\|C_{i}(x)-x\|^{2}]+_{i j }[ C_{i}(x)-x,C_{j}(x)-x]) \]

If the compressed vectors \(\{C_{i}(x)\}_{i=1}^{n}\) are further assumed to be independent, _i.e._, \(\{C_{i}\}_{i=1}^{n}_{}^{}\), then the cancellation of cross error terms leads to the following equation:

\[[\|_{i=1}^{n}C_{i}(x_{i})-x\|^{2} ]=}_{i=1}^{n}[\|C_{i}(x)-x\|^{2}]\|x\|^{2}. \]

We observe that the mutual independence among unbiased compressors leads to a decreased variance, which corresponds to the information distortion, of the aggregated message. Remarkably, this reduction is achieved by a factor of \(n\) compared to the transmission of a single compressor. Therefore, the independence among the compressors plays a pivotal role in enhancing the accuracy of the aggregated vector, consequently reducing the number of required communication rounds.

On the contrary, in cases where independence is not assumed and no other properties of compressors can be leveraged, the use of Cauchy's inequality only allows us to bound variance (6) as follows:

\[[\|_{i=1}^{n}C_{i}(x)-x\|^{2}] _{i=1}^{n}[\|C_{i}(x)-x\|^{2}]\|x\|^{2}. \]

It is important to note that the upper bound \(\|x\|^{2}\) can only be attained when the compressors \(\{C_{i}\}_{i=1}^{n}\) are identical, indicating that this bound cannot be generally improved further. By comparing (7) and (8), we can observe that the variance of the aggregated vector achieved through unbiased compression with independence can be \(n\) times smaller than the variance achieved without independence.

### Convergence lower bounds with independent unbiased compressors

While mutual independence can boost the unbiased worker compressors, it remains unclear how much the total communication cost can be reduced _at most_ by independent unbiased compression and how to develop algorithms to achieve this optimal reduction. The following subsections aim to address these open questions.

Following the formulation in (5), to establish the best achievable total communication cost using _independent unbiased compression_, we shall study tight lower bounds on the number of communication rounds \(T_{}\) to achieve an \(\)-accurate solution, which is characterized by the following theorem.

**Theorem 2**.: _For any \(L 0\), \(n 2\), the following results hold. See the proof in Appendix C._

* _Strongly-convex:_ _For any_ \(>0\)_, there exists a constant_ \(c_{}\) _only depends on_ \( L/\)_, a set of local loss functions_ \(\{f_{i}\}_{i=1}^{n}^{}_{L,>0}\)_, independent unbiased compressors_ \(\{C_{i}\}_{i=1}^{n}^{}_{}\)_, such that the output_ \(\) _of any_ \(A_{\{C_{i}\}_{i=1}^{n}}\) _starting from_ \(x^{0}\) _requires_ \[T_{}(A,\{(f_{i},C_{i})\}_{i=1}^{n})=((+(1+ }))())\] _rounds of communication to reach_ \([f()]-_{x}f(x)\) _for any_ \(0< c_{}\)_._
* _Generally-convex:_ _For any_ \(>0\)_, there exists a constant_ \(c=(1)\)_, a set of local loss functions_ \(\{f_{i}\}_{i=1}^{n}^{}_{L,0}\)_, independent unbiased compressors_ \(\{C_{i}\}_{i=1}^{n}^{}_{}\)_, such that the output_ \(\) _of any_ \(A_{\{C_{i}\}_{i=1}^{n}}\) _starting from_ \(x^{0}\) _requires at least_ \[T_{}(A,\{(f_{i},C_{i})\}_{i=1}^{n})=(()+(1+})()^{})\] _rounds of communication to reach_ \([f()]-_{x}f(x)\) _for any_ \(0< cL\)_._

**Consistency with prior works.** The lower bounds established in Theorem 2 are consistent with the best-known lower bounds in previous literature. When \(=0\), our result reduces to the lower bound for distributed first-order algorithms established by Y. Nesterov in . When \(n=1\), our result reduces to the lower bound established in  for the single-node case.

**Independence improves lower bounds.** A recent work  establishes lower bounds for unbiased compression without the independence assumption, listed in the second row of Table 1. Compared to these results, our lower bound in Theorem 2 replaces \(\) with \(/\), showing a reduction in order. This reduction highlights the role of independence in unbiased compression. To better illustrate the reduction, we take the strongly-convex case as an example. The ratio of the number of communication rounds \(T_{}\) under unbiased compression with independence to the one without independence is:

\[)}{(1+)}=+(}+})=(,\} }). \]

Clearly, using independent unbiased compression can allow algorithms to converge faster, by up to a factor of \((})\) (attained at \(}\)), in terms of the number of communication rounds, compared to the best algorithms with unbiased compressors but without independence.

**Total communication cost.** Substituting Theorem 2 into the TCC formulation in (5), we can obtain the TCC of algorithms using independent unbiased compression. Comparing this with algorithms without compression, such as Nesterov's accelerated algorithm, and using the relations in (9), we can demonstrate that independent unbiased compression can reduce the total communication cost. Such reduction can be up to \((})\) by using compressors with \(}\), _e.g._, random-\(s\) sparsification with \(s d/}\).

### ADIANA: a unified optimal algorithm

By comparing existing algorithms using independent unbiased compression, such as DIANA, ADIANA, and CANITA, to our established lower bounds in Table 1, it becomes clear that there is a noticeable gap between their convergence complexities and our established lower bounds. This gap could indicate that these algorithms are suboptimal, but it could also mean that our lower bounds are loose. As a result, our claim that using independent unbiased compression reduces the total communication cost by up to \((})\) times is not well-grounded yet. In this section, we address this issue by revisiting ADIANA  (Algorithm 1) and providing novel and refined convergence results in both strongly- and generally-convex cases.

**Algorithm 1**: ADIANA

**Input:** Scalars \(\{_{1,k}\}_{k=0}^{T-1}\), \(_{2}\), \(\), \(\), \(\{_{k}\}_{k=0}^{T-1}\), \(\{_{k}\}_{k=0}^{T-1}\), \(p\).

Initialize \(w^{0}=x^{0}=y^{0}=z^{0}=h^{0}=h_{i}^{0}\), \(\,1 i n\).

**for**\(k=0,,T-1\)do

**On server:**

 Update \(x\): \(x^{k}=_{1,k}z^{k}+_{2}w^{k}+(1-_{1,k}-_{2})y^{k}\) and broadcast to all workers;

**On all workers in parallel:**

 Compress the increment of local gradient \(m_{i}^{k}=C_{i}( f_{i}(x^{k})-h_{i}^{k})\) and send to the server;

 Compress the increment of local gradient \(c_{i}^{k}=C_{i}( f_{i}(w^{k})-h_{i}^{k})\) and send to the server;

 Update local shift \(h_{i}^{k+1}=h_{i}^{k}+ c_{i}^{k}\);

**On server:**

 Aggregate received compressed message \(g^{k}=h^{k}+_{i=1}^{n}m_{i}^{k}\);

 Update shift \(h^{k+1}=h^{k}+_{i=1}^{n}c_{i}^{k}\);

 Apply gradient descent \(y^{k+1}=x^{k}-_{k}g^{k}\);

 Update \(z\): \(z^{k+1}= z^{k}+(1-)x^{k}+}{_{k}}(y^{k+1}-x^{k})\);

 Update \(w\): \(w^{k+1}=y^{k},&p,\\ w^{k},&1-p;\)

**Output:**\(=w^{T}\) if \(f(w^{T}) f(y^{T})\) else \(=y^{T}\).

In the strongly-convex case, we refine the analysis of  by: (i) adopting new parameter choices where the initial scalar \(_{2}\) is delicately chosen instead of being fixed as \(_{2}=1/2\) in , (ii) balancing different terms in the construction of the Lyapunov function. While we do not modify the algorithm design, our technical ingredients are necessary to obtain an improved convergence rate. In the generally-convex case, we provide the _first_ convergence result for ADIANA, which is missing in literature to our knowledge. In both strongly- and generally-convex cases, our convergence results (nearly) match the lower bounds in Theorem 2. This verifies the tightness of our lower bounds for both the convergence complexity and the total communication cost. In particular, our results are:

**Theorem 3**.: _For any \(L 0\), \( 0\), \(n 1\), and precision \(>0\), the following results hold. See the proof in Appendix D._

* _Strongly-convex:_ _If_ \(>0\)_, by setting parameters_ \(_{k}=n_{2}/(120 L)\)_,_ \(_{1,k}_{1}=1/(3)\)_,_ \(=p=1/(1+)\)_,_ \(_{k}=/(2_{1}+)\)_,_ \(=2_{1}/(2_{1}+)\)_, and_ \(_{2}=1/(3+3n/)\)_, ADIANA requires_ \[((+(1+}))())\] _rounds of communication to reach_ \([f()]-_{x}f(x)\)_._
* _Generally-convex:_ _If_ \(=0\)_, by setting parameters_ \(=1/(1+)\)_,_ \(=1\)_,_ \(p=_{2}=1/(3(1+))\)_,_ \(_{1,k}=9/(k+27(1+))\)_,_ \(_{k}=_{k}/(2_{1,k})\)_, and_ \[_{k}=\{(1+27(1+))L},}{1+})^{6}\). Our refined rates for ADIANA are state-of-the-art among existing algorithms using independent unbiased compression.

## 6 Experiments

In this section, we empirically compare ADIANA with DIANA , EF21 , and CANITA  using unbiased compression, as well as Nesterov's accelerated algorithm  which is an optimal algorithm when no compression is employed. We conduct experiments on least-square problems (strongly-convex) with synthetic datasets as well as logistic regression problems (generally-convex) with real datasets. In all experiments, we measure the total communicated bits sent by a single worker, which is calculated through _communication rounds to acheive an \(\)-accurate solutions \(\) per-round communicated bits_. All curves are averaged over \(20\) trials with the region of standard deviations depicted. Due to the space limit, we only provide results with random-\(s\) compressors here. More experimental results can be found in Appendix F.2.

**Least squares.** Consider a distributed least-square problem (1) with \(f_{i}(x):=\|A_{i}x-b_{i}\|^{2}\), where \(A_{i}^{M d}\) and \(b_{i}^{M}\) are randomly generated. We set \(d=20\), \(n=400\), and \(M=25\), and generate \(A_{i}\)'s by randomly generating a Gaussian matrix in \(^{nM d}\), then modify its condition number to \(10^{4}\) through the SVD decomposition, and finally distribute its rows to all \(A_{i}\). We use independent random-\(1\) compressors for communication compression. The results are depicted in Fig. 2 (left) where we observe ADIANA beats all baselines in terms of the total communication cost. We do not compare with CANITA since it does not have theoretical guarantees for strongly-convex problems.

**Logistic regression.** Consider a distributed logistic regression problem (1) with \(f_{i}(x):=_{m=1}^{M}(1+(-b_{i,m}a_{i,m}^{}x))\), where \(\{(a_{i,m},b_{i,m})\}_{1 i n,1 m M}\) are datapoints in a9a and w8a datasets from LIBSVM . We set \(n=400\) and choose independent random-\( d/20\) compressors for algorithms with compressed communication. The results are as shown in Fig. 2 (middle and right). Again, we observe that ADIANA outperforms all baselines.

**Influence of independence in unbiased compression.** We also construct a delicate quadratic problem to validate the role of independence in unbiased compression to save communication, see Fig. 1. Experimental details are in Appendix F.1. We observe that ADIANA with independent random-\(s\) compressors saves more bits than Nesterov's accelerated algorithm while random-\(s\) compressors of shared randomness do not. Furthermore, more aggresive compression, _i.e._, a larger \(\), saves more communication costs in total. These observations are consistent with our theories implied in (9).

## 7 Conclusion

This paper clarifies that unbiased compression alone cannot save communication, but this goal can be achieved by further assuming mutual independence between compressors. We also demonstrate the saving can be up to \((})\). Future research can explore when and how much biased compressors can save communication in non-convex and stochastic scenarios.

## 8 Acknowledgment

This work is supported by NSFC Grant 12301392, 92370121, and 12288101.

Figure 2: Convergence results of various distributed algorithms on a synthetic least squares problem (left), logistic regression problems with dataset a9a (middle) and w8a (right). The \(y\)-axis represents \(f()-f^{}\) and the \(x\)-axis indicates the total communicated bits sent by per worker.