# DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation

Sunghyeon Woo\({}^{1}\)\({}^{}\) &Baesung Park\({}^{2}\)\({}^{*}\) &Byeongwook Kim\({}^{2}\) &Minjung Jo\({}^{2}\)

&Se Jung Kwon\({}^{2}\) &Dongsuk Jeon\({}^{1}\) &Dongsoo Lee\({}^{2}\)

Seoul National University\({}^{1}\) &NAVER Cloud\({}^{2}\)

###### Abstract

Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5\(\), and enable training with a sequence length 6.2\(\) larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100 GPU and 117% on an Intel Gaudi2 HPU. The code is available at https://github.com/WooSunghyeon/dropbp.

## 1 Introduction

Since the advent of the transformer architecture , the field of language modelling has experienced dramatic advancements. Especially, following the scaling laws , the development of Large Language Models (LLMs)  continues with the aim of achieving or outperforming human capabilities. However, tremendously increasing the size of the model results in significant costs for training from scratch. An alternative approach for developing high-capability LLMs without the costly pretraining on extensive datasets is instruction tuning . This method fine-tunes well-trained foundation models on relatively small instruction-following datasets, enabling the models to better understand and follow prompts.

While fine-tuning Large Language Models (LLMs) on instruction-following datasets is more cost-effective than training from scratch, it still requires substantial memory for parameters and activations, along with significant floating-point operations (FLOPs). In this context, Parameter-Efficient Fine-Tuning (PEFT) techniques  effectively reduce the memory required for parameter gradients and optimizer states by freezing pretrained weights and selectively training newly added modules. Moreover, when combined with quantization methods , these techniques can further significantly decrease the memory requirements for parameters.

While Parameter-Efficient Fine-Tuning (PEFT) has successfully reduced memory associated with parameters, two significant challenges remain for efficient fine-tuning: computational cost and activation memory, both of which are linked to the backpropagation . First, fine-tuning Large Language Models (LLMs) using a backpropagation requires substantial floating-point operations (FLOPs). Specifically, the backpropagation algorithm necessitates forward propagation to calculate outputs and backward propagation to compute gradients for inputs and parameters. Notably, backward propagation demands twice the computational operations compared to forward propagation, thus becoming the primary bottleneck. Second, all intermediate outputs (i.e., activations) generated during forward propagation must be stored for compute in backward propagation. This activations consume considerable memory, which becomes especially critical when training LLMs on long sequence contexts [22; 23].

In this paper, we introduce Dropping Backward Propagation (DropBP), an efficient fine-tuning algorithm for LLMs that significantly reduces computational costs and activation memory. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. As a result, these undropped layers no longer require FLOPs and activation memory during backward propagation. Additionally, DropBP calculates the sensitivity of each layer, an indicator of its impact on the total training process, to adjust drop rate. This careful calibration of drop rate according to layer sensitivity ensures more stable training. This DropBP algorithm can be seamlessly integrated with any PEFT algorithm, operating orthogonally by simply dropping layers during backward propagation.

We implemented DropBP as an easy-to-integrate PyTorch library , requiring only minimal changes to the existing training codes. In experiments, DropBP successfully reduces training time as shown in Fig. 0(a), maintaining comparable accuracy on the MMLU  and commonsense reasoning tasks [26; 27; 28; 29; 30]. The DropBP also accelerated the convergence of the same perplexity by 1.5\(\) in LLMaMA2-70B . Moreover, DropBP substantially decreases activation memory, increasing an available maximum sequence length by up to 6.2 \(\) in LLaMA2-70B on a single NVIDIA A100 GPU , as shown in Fig. 0(b). Finally, our DropBP increases training throughput by up to 79% and 117% on a single NVIDIA A100 GPU and Intel Gaudi2 HPU , respectively, when fully fine-tuning LLaMA3-8B . In summary, the main contributions of our paper are:

* We propose DropBP, an efficient fine-tuning algorithm that randomly drops backward propagation based on layer sensitivity.

Figure 1: Performance enhancements in fine-tuning large language models using DropBP when the \(p\) represents the target average drop rate for backward propagation: (a) Training time per sample for fine-tuning LLaMA2-7B with DropBP, at a sequence length of 512 and a micro batch size of 2. (b) Available max sequence length for fine-tuning LLaMA2-70B with DropBP, at a micro batch size of 1 on an NVIDIA-A100 GPU.

* We implemented DropBP as a user-friendly PyTorch extension with a straightforward API for ease of use, making it easily applicable to existing training codes.
* DropBP reduces training time by 44% with comparable accuracy, increases convergence speed by 1.5\(\), increases the available maximum sequence length up to 6.2\(\), and enhances training throughput up to 117%.

## 2 Background & Motivation

BackpropagationBackpropagation , a core algorithm for training deep neural networks, involves both forward and backward propagation. Specifically, the training process in the linear layer is represented as follows:

\[_{out}= _{in}\] (1) \[_{in}=^{T} _{out}\] (2) \[=_{out}_{in}^{T}\] (3)

where **H** and **W** represent the activations and parameters, respectively, with '\(\)' indicating matrix multiplication operation. The gradients of **H** and **W** are denoted by \(\) and \(\). The computational costs during forward propagation primarily arises from matrix multiplication for computing output activations by Eq. 1. In backward propagation, the computational burden is primarily due to matrix multiplication for calculating input gradients by Eq. 2 and parameter gradients by Eq. 3. Note that the computational costs of these equations are almost equal. Consequently, the FLOPs required for backward propagation including Eqs. 2 and 3 are approximately 2\(\) as large as the FLOPs needed for forward propagation by Eq. 1. Furthermore, the activations of all layers (\(_{in}^{T}\)) must be stored in memory for use in backward propagation computations in Eq. 3. Therefore, focusing on reducing the computations during backward propagation is crucial for decreasing both the overall computational costs and the activation memory.

Interpretation the model with residual connectionsResidual connections are one of the widely used methods to address the issue of vanishing gradients . Transformer  also incorporates residual connections that bypass multi-head attention and feedforward networks. Networks utilizing these residual connections can be interpreted as ensembles of several submodules . For example, if we expand the model with three residual connections as shown in Fig. 1(a), it can be represented as a combination of eight submodules, as depicted in Fig. 1(b) From this perspective, a network with \(n\) layers can be interpreted as an ensemble of \(2^{n}\) submodules .

## 3 Methodology

### Dropping Backward propagation

In Section 2, we observed that the backpropagation algorithm consumes a significant amount of FLOPs and activation memory, particularly for the backward propagation. To reduce this overhead,

Figure 2: Interpreting the model with residual connections as a combination of multiple submodules.

we propose a straightforward approach: Dropping Backward Propagation (DropBP). DropBP is designed to drop layers exclusively during backward propagation, effectively reducing both FLOPs and activation memory for the dropped layers, as demonstrated in following equations:

\[_{imm} =_{in}+_{p_{i}}(f_{ATTN}(f_{LN}(_{ in})))\] (4) \[_{out} =_{imm}+_{p_{i+1}}(f_{FFN}(f_{LN}(_{imm})))\] (5)

Here, \(_{in}\), \(_{out}\), and \(_{imm}\) represent the input, output, and immediate activation between the attention layer and feedforward network in a transformer block, respectively. \(f_{ATTN}\), \(f_{FFN}\), and \(f_{LN}\) denote the attention layer, feedforward network, and layer normalization of the transformer block. The DropBP layer, defined as \(_{p}()\), skips backward propagation in the input **X** with a probability of \(p\), while not dropping forward propagation. Following to Eqs. 4 and 5, backward propagation in the attention layer and feedforward network is dropped with probabilities \(p_{i}\) and \(p_{i+1}\), respectively, as shown in Fig. 2(a).

We can view the transformer as a collection of a lot of submodules composed of various modules (i.e., \(f_{ATTN} f_{LN}\) and \(f_{FFN} f_{LN}\)) with residual connections, as described in Section 2. When the transformer block contains \(n\) units, each including multi-head attention and a feedforward network, the model can be interpreted as an ensemble of \(2^{2n}\) submodules. From this perspective, DropBP can be interpreted as training only certain shallow submodules. For example, as shown in Fig. 2(b), if the \(F_{2}\) layer is dropped, only the shallow submodule composed of the remaining layers is trained during backward propagation. Therefore, if the overall drop rate is \(p\), DropBP can be interpreted as training shallow submodules with the depth of \(2n(1-p)\) or less, since \(2np\) layers are dropped. We anticipate that training these smaller modules alone can effectively fine-tune well-pretrained LLMs, based on the analysis that shallow submodules have a significant impact on the overall training process as detailed in Appendix A.

### Sensitivity-based Drop Rate Allocation

In Section 3.1, we introduce DropBP, which selectively drops layers during backward propagation and trains only certain shallow submodules. In addition, we hypothesized that the significance of individual layers and the submodules encompassing these layers varies in their impact on the overall training process. Therefore, we assign different drop rate to each layer based on sensitivity, which is defined by defined by how much each layer and its encompassing submodules affect the overall training process in terms of parameter gradient. Specifically, we calculate the sensitivity of a layer by the variance in L2-norm of parameter gradients between when the backward propagation of that layer

Figure 3: The overview of DropBP.

is skipped or not, inspired by GradNormVar , a memory efficient gradient variance approximation, as below:

\[S_{l}=_{i}(||_{i}||_{2}-||_{i}^{(l)}||_{2})^{2}\] (6)

where \(S_{l}\) denotes of the \(l\)-th layer. Here, \(_{i}\) represents the parameter gradient of the \(i\)-th layer when no layers are dropped, while \(_{i}^{(l)}\) denotes the parameter gradient of the \(i\)-th layer when the \(l\)-th layer is dropped during backward propagation. After calculating the sensitivity for each layer, we aim to minimize the expected sensitivities across all layers-essentially the expected gradient variance caused by DropBP-under a given FLOPs as follow:

\[_{i}(1-p_{i})S_{i},\ \ _{i}(1-p_{i})F_{i} F_{t}\] (7)

where \(p_{i}\) denotes the drop rate, and \(F_{i}\) denotes the FLOPs of the \(i\)-th layer during backward propagation. \(F_{t}\) represents the target FLOPs, derived from the target average drop rate \(p_{avg}\) (i.e. \(F_{t}=(1-p_{avg})_{i}F_{i}\)). In other words, we determine the drop rates across all layers that minimize the expected sensitivities of the model, while satisfying a given FLOPs budget, by solving Problem 7. In practice, DropBP addresses the Prob. 7 using a simple greedy algorithm, as detailed in Section 4.1.

## 4 Evaluation

### Implementation and Settings

DropBP aims to decrease the training costs in fine-tuning based on backpropagation, consequently enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation. To facilitate practical implementation, we developed a user-friendly DropBP library in PyTorch , as demonstrated in Fig. 4. In detail, we implemented a DropBP layer that internally drops backward propagation in the input direction according to a given drop rate as shown in Fig. 3(a). The DropBP layer designed to initially receive the FLOPs of the layers that would be skipped (\(F_{i}\)) as initial value to solve Prob. 7.

Additionally, we developed a DropBPHandler that automatically solve Prob. 7 by assigning varying drop rate to each layer using a simple greedy algorithm, as demonstrated in Fig. 3(b). Specifically, the process begins by setting the drop rate (\(p_{i}\)) of all layers to 0 and then gradually increases them to

Figure 4: Code implementation for integrating DropBP.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

We first measured the maximum sequence length that could be trained without an Out Of Memory (OOM) on a single NVIDIA A100 GPU. The results in Table 4 indicate that our DropBP considerably increases the maximum sequence length, by up to 6.2\(\) the baseline when the drop rate was 0.875. This is because DropBP allows skipping certain layers during backward propagation, eliminating the need to store activations required for calculating parameter gradients of those skipped layers. We believe that this property of DropBP will be particularly useful for fine-tuning LLMs with long-context data [22; 23].

We also evaluated training throughput when full fine-tuning LLaMA3-8B using BF16 precision on a single NVIDIA A100 GPU and an Intel Gaudi2 HPU, increasing the batch size up to the point of OOM errors. As shown in Fig. 6, applying DropBP allows for an increase in batch size per iteration by up to 3.3\(\) on the NVIDIA A100 GPU and 5.2\(\) on the Intel Gaudi2 HPU, ensuring high hardware utilization and scalability. Furthermore, DropBP demonstrates a sustained increase in throughput over the baseline at an identical batch size. Ultimately, with a drop rate of 0.875, DropBP achieves a throughput of 16.4 sentences/s on the NVIDIA A100 GPU and 28.4 sentences/s on the Intel Gaudi2 HPU, increasing by 79% and 117% over the baseline, respectively.

### Ablation Study

Impact of the Number of SubmodulesWe conducted an ablation study to investigate the impact of the number of trainable submodules on the fine-tuning of LLMs. This study compared DropBP, which trains varying submodules randomly at each iteration, with layer freezing, which trains submodules composed of only upper layers. Here, the skip rate \(p\) denotes the drop rate in DropBP and the proportion of layers that are frozen in the layer freezing.

First, we analyzed the number of submodules trained by layer freezing and DropBP. In the case of layer freezing, the lower \(2np\) layers are frozen and only the remaining \(2n(1-p)\) layers are trained. In this case, the number of trainable upper submodules is \(2^{2n(1-p)}\). In contrast, DropBP randomly drops \(2np\) layers at each iteration, allowing it to train all submodules with a depth of \(2n(1-p)\) or less without the restriction of training only the submodules composed of the upper layers. In this scenario, since the number of different submodules at depth \(i\) in the entire network is \({}_{2n}C_{i}\), DropBP can train \(_{i=0}^{2n(1-p)}\)\({}_{2n}C_{i}\) submodules.

As shown in Table 5, when fine-tuning LLaMA2-7B using layer freezing or DropBP with a high skip rate of 0.875, we observed a significant 1.8% decrease in accuracy with layer freezing compared to

Figure 6: Throughput (sentences/s) on a single NVIDIA A100 GPU on a single NVIDIA A100 GPU and Intel Gaudi2 HPU when fine-tuning LLaMA3-8B with a sequence length of 512.

Figure 7: Validation perplexity (PPL) for fine-tuning LLaMA2-7B through LoRA with layer freezing or DropBP on the Alpaca dataset.

the baseline, while DropBP exhibited a relatively smaller accuracy decrease of 1.0%. Furthermore, as illustrated in Fig. 7, the convergence speed to the same validation PPL on the downstream task is much slower for layer freezing compared to DropBP, especially at high skip rates, where it converges even more slowly than the baseline. We believe this is due to the ability of DropBP to train a relatively larger number of submodules (\(_{i=0~{}64}^{8}C_{i}\)), compared to the fewer submodules trained by layer freezing (\(2^{8}\)). Moreover, when fine-tuning LLaMA2-70B, DropBP resulted in a 0.5% increase in MMLU 5-shot accuracy compared to the baseline, despite a high skip rate of 0.875. This improvement is due to the large number of layers in LLaMA2-70B, enabling DropBP to train deeper and more numerous submodules (\(_{i=0~{}160}^{20}C_{i}\)) even with a high skip rate of 0.875.

Impact of Sensitivity-based Drop RateWe also conducted an ablation study to analyze the effectiveness of sensitivity-based drop rate allocations. First, we identified the sensitivity of different layers by calculating Eq. 6 during the training of LLMs in various scenarios, as illustrated in Fig. 7(a) and Fig. 7 in Appendix D. While the distribution varies slightly depending on the number of parameters, fine-tuning approach, and target average drop rate, there is a consistent tendency to assign importance to both the initial and final layers. Consequently, drop rates for these layers are allocated to be lower by a simple greedy algorithm, as explained in Section 4.1

Additionally, we fine-tuned the LLaMA2-7B and 13B using DropBP on Alpaca datasets, comparing sensitivity-based allocated drop rates with uniform drop rate. In detail, we compared the average accuracy of commonsense reasoning tasks when fine-tuning the models with a learning rate of 1e-4 and 3e-4, as shown in Table 6. Note that the PPL for fine-tuning LLaMA2-7B in Fig. 7(b) corresponds to a learning rate of 3e-4. The results indicate that sensitivity-based drop rates achieved a 1.6% higher

  
**LLaMA2** &  &  \\
**LR** & **1e-4** & **3e-4** & **1e-4** & **3e-4** \\  LoRA & 65.7 & 66.0 & 68.2 & 68.0 \\  \(+\)DropBP (uniform) & 66.4 & 63.1 & 66.6 & 65.8 \\ \(+\)DropBP (sens) & 66.6 & 64.7 & 67.7 & 67.3 \\   

Table 6: Test accuracy on the 0-shot commonsense reasoning tasks when fine-tuning LLaMA2-7B and 13B through LoRA with DropBP at uniform or sensitivity-based drop rate on the Alpaca datasets. The target average drop rate is 0.875.

    &  &  \\ 
**Method** & **LoRA** & **LoRA+Freeze** & **LoRA+DropBP** & **OLoRA** & **OLoRA+DropBP** \\ 
**Drop Rate** & - & 0.875 & 0.875 & - & 0.875 \\ 
**\# of Submodules** & \(2^{64}\) & \(2^{8}\) & \(_{i=0~{}64}^{8}C_{i}\) & \(2^{160}\) & \(_{i=0~{}160}^{20}C_{i}\) \\
**Accuracy (\%)** & 44.7 & 42.9 (-1.8) & 43.7 (-1.0) & 68.3 & 68.8 (+0.5) \\   

Table 5: The number of submodules being trained and test accuracy on the 5-shot MMLU tasks with layer freezing or DropBP on the Alpaca datasets.

Figure 8: Distribution of drop rates and the validation PPL when fine-tuning LLaMA2-7B through LoRA with DropBP at uniform or sensitivity-based drop rate on Alpaca datasets.

accuracy compared to uniform drop rates with a relatively high learning rate of 3e-4, while there was no significant difference in accuracy when the learning rate was set to 1e-4 in LLaMA2-7B. Fig. 8b also shows that sensitivity-based drop rates consistently stabilized the convergence of validation loss, whereas uniform drop rates occasionally diverged when the learning rate was set to 3e-4 in LLaMA2-7B. This phenomenon is even more pronounced with LLaMA2-13B, resulting in a 1.1% increase in accuracy through sensitivity-based drop rate allocation, even with a low learning rate of 1e-4. In other words, sensitivity-based drop rate allocation helps stabilize the training process, especially in the case of large learning rates or larger models.

## 5 Related Works

Parameter-efficient fine-tuningWhen fine-tuning LLM, substantial amount of memory is required to store parameters, gradients, and optimizer states. LoRA  successfully reduces the memory allocated to gradients and optimizer states by inserting trainable rank decomposition matrices into the linear layers of the model while keeping the original LLM parameters frozen. LLaMA-Adapter  and LLaMA-Adapter V2  significantly reduce training memory using trainable adoption prompts and zero-initialized attention mechanisms. Some studies attempt to reduce not only the memory footprint of gradients and optimizer states but also that of parameters by considering quantization. PEQA , for instance, quantizes the original LLM parameters into a low-bit format and fine-tunes only the scale factor, thus saving memory for parameters during training. QLoRA  and QA-LoRA , built upon LoRA, also employ quantization on the original LM parameters, significantly reducing parameter memory during training. Our DropBP is orthogonal and easily combinable with these PEFT techniques, enabling memory and computationally efficient fine-tuning.

ParallelismParallelism techniques are widely used to accelerate training LLM using multiple GPU efficiently. Data parallelism  is a technique that involves dividing data along the batch dimension for training across multiple GPUs, which still requires sufficient memory to load the entire model on each GPU. Conversely, tensor parallelism [43; 44; 45] partitions the model across GPUs, dividing matrix multiplication operations column-wise and row-wise. Pipeline parallelism [46; 47; 48] involves partitioning the model depth-wise across GPUs, which enables efficient pipeline scheduling. The Zero Redundancy Optimizer (ZeRO)  and Fully Sharded Data Parallelism (FSDP)  shard parameters, gradients, and optimizer states across multiple GPUs, retrieving parameters when needed to restore their non-partitioned form, enabling the overlapping of computation and communication during training. While these parallelism techniques are designed to efficiently manage the massive computational costs across multiple GPUs, our DropBP specifically aims to reduce the inherent computational costs required for training process.

Layer droppingStochastic Depth , the first approach to randomly drop layers during neural network training, reduces overfitting and costs in image recognition. Layerdrop  randomly drops layers during training and selectively uses some layers during inference, accelerating both processes for transformers. Progressive Layer Dropping (PLD)  progressively increases the drop rate across depth and iterations, improving training speed without accuracy degradation in transformers. These techniques speed up pretraining of small transformer models like BERT  by dropping layers during the entire training process, whereas DropBP, specific to fine-tuning LLMs, drops layers only during backward propagation. Consequently, as detailed in Appendix E, our DropBP achieves higher performance compared to these layer dropping when fine-tuning LLMs.

## 6 Conclusion

We propose DropBP, an effective algorithm that accelerates the fine-tuning of LLMs by randomly dropping layers during backward propagation, which can be orthogonally integrated into both full-fine tuning and parameter-efficient fine-tuning. We developed the DropBP library as a user-friendly PyTorch extension to facilitate easy integration into existing training codes. Experimental results demonstrate that DropBP significantly accelerates training speed during the fine-tuning of LLMs, achieving comparable accuracy to baseline fine-tuning. Furthermore, DropBP reduces activation memory, enabling long-context training and increasing batch size on limited resources. Consequently, applying DropBP enables a 79% higher throughput on an NVIDIA A100 GPU and a 117% higher throughput on an Intel Gaudi2 HPU.