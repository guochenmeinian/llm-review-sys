# Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback

Yang Cai

Yale University

yang.cai@yale.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

&Weiqiang Zheng

Yale University

weiqiang.zheng@yale.edu

###### Abstract

We revisit the problem of learning in two-player zero-sum Markov games, focusing on developing an algorithm that is _uncoupled_, _convergent_, and _rational_, with non-asymptotic convergence rates to Nash equilibrium. We start from the case of stateless matrix game with bandit feedback as a warm-up, showing an \((t^{-})\) last-iterate convergence rate. To the best of our knowledge, this is the first result that obtains finite last-iterate convergence rate given access to only bandit feedback. We extend our result to the case of irreducible Markov games, providing a last-iterate convergence rate of \((t^{-})\) for any \(>0\). Finally, we study Markov games without any assumptions on the dynamics, and show a _path convergence_ rate, a new notion of convergence we define, of \((t^{-})\). Our algorithm removes the coordination and prior knowledge requirement of , which pursued the same goals as us for irreducible Markov games. Our algorithm is related to  and also builds on the entropy regularization technique. However, we remove their requirement of communications on the entropy values, making our algorithm entirely uncoupled.

## 1 Introduction

In multi-agent learning, a central question is how to design algorithms so that agents can _independently_ learn (i.e., with little coordination overhead) how to interact with each other. Additionally, it is desirable to maximally reuse existing single-agent learning algorithms, so that the multi-agent system can be built in a modular way. Motivated by this question, _decentralized_ multi-agent learning emerges with the goal to design decentralized systems, in which no central controller governs the policies of the agents, and each agent learns based on only their local information - just like in a single-agent algorithm. In recent years, we have witnessed significant success of this new decentralized learning paradigm. For example, _self-play_, where each agent independently deploys the same single-agent algorithm to play against each other without further direct supervision, plays a crucial role in the training of AlphaGo  and AI for Stratego .

Despite the recent success, many important questions remain open in decentralized multi-agent learning. Indeed, unless the decentralized algorithm is carefully designed, self-play often falls short of attaining certain sought-after global characteristics, such as convergence to the global optimum or stability as seen in, for example, .

In this work, we revisit the problem of learning in two-player zero-sum Markov games, which has received extensive attention recently. Our goal is to design a decentralized algorithm that resemblesstandard single-agent reinforcement learning (RL) algorithms, but with an additional crucial assurance, that is, _guaranteed convergence_ when both players deploy the algorithm. The simultaneous pursuit of independence and convergence has been advocated widely , while the results are still not entirely satisfactory. In particular, all of these results rely on assumptions on the dynamics of the Markov game. Our paper takes the first step to remove such assumptions.

More specifically, our goal is to design algorithms that simultaneously satisfy the following three properties (the definitions are adapted from ):

* **Uncoupled**: Each player \(i\)'s action is generated by a standalone procedure \(_{i}\) which, in every round, only receives the current state and player \(i\)'s own reward as feedback (in particular, it has no knowledge about the actions or policies used by the opponent). There is no communication or shared randomness between the players.
* **Convergent**: The policy pair of the two players converges to a Nash equilibrium.
* **Rational**: If \(_{i}\) competes with an opponent who uses a policy sequence that converges to a stationary one, then \(_{i}\) converges to the best response of this stationary policy.

The uncoupledness and rationality property capture the independence of the algorithm, while the convergence property provides a desirable global guarantee. Interestingly, as argued in , if an algorithm is uncoupled and convergent, then it is also rational, so we only need to ensure that the algorithm is uncoupled and convergent. Regarding the notion of convergence, the standard definition above only allows _last-iterate_ convergence. Considering the difficulty of achieving such convergence, in the related work review (Section 2) and in the design of our algorithm for general Markov games (Section 6), we also consider weaker notions of convergence, including the _best-iterate_ convergence, which only requires that the Cesaro mean of the duality gap is convergent, and the _path_ convergence, which only requires the convergence of the Cesaro mean of the duality gap _assuming minimax/maximin policies are followed in future steps_. The precise definitions of these convergence notions are given at the end of Section 3.

### Our Contributions

The main results in this work are as follows (see also Table 1 for comparisons with prior works):

* As a warm-up, for the special case of matrix games with bandit feedback, we develop an uncoupled algorithm with a last-iterate convergence rate of \((t^{-})\) under self-play (Section 4). To the best of our knowledge, this is the first algorithm with provable last-iterate convergence rate in the setting.
* Generalizing the ideas from matrix games, we further develop an uncoupled algorithm for irreducible Markov games with a last-iterate convergence rate of \((t^{-})\) for any \(>0\) under self-play (Section 5).
* Finally, for general Markov games without additional assumptions, we develop an uncoupled algorithm with a path convergence rate of \((t^{-})\) under self-play (Section 6).

Our algorithms leverage recent advances on using entropy to regularize the policy updates  and the Nash-V-styled value updates . On the one hand, compared to , our algorithm has the following advantages: 1) it does not require the two players to exchange their entropy information, which allows our algorithm to be fully uncoupled; 2) it does not require the players to have coordinated policy updates, 3) it naturally extends to general Markov games without any assumptions on the dynamics (e.g., irreducibility). On the other hand, our algorithm inherits appealing properties of Nash-V , but additionally guarantees path convergence during execution.

## 2 Related Work

The study of two-player zero-sum Markov games originated from , with many other works further developing algorithms and establishing convergence properties . However, these works primarily focused on _solving_ the game with full knowledge of its parameters (i.e., payoff function and transition kernel). The problem of _learning_ in zero-sum games was first formalized by . Designing a _provably_ uncoupled, rational, and convergent algorithm 

[MISSING_PAGE_FAIL:3]

which guarantee that the player's long-term payoff is at least the minimax value . Interestingly, as shown in , if the player is paired with an optimistic best-response opponent (instead of using the same algorithm), the first player's strategy can converge to the minimax policy.  developed another coupled learning framework to handle exploration, but with symmetric updates on both players. In each round, the players need to jointly solve a general-sum equilibrium problem due to the different exploration bonus added by each player. Hence, the execution of these algorithms is more similar to the Nash-Q algorithm by .

So far, exploration has been handled through coupled approaches that are also not rational. To our knowledge, the first uncoupled and rational algorithm that handles exploration is the Nash-V algorithm by . Nash-V can output a nearly-minimax policy through weighted averaging ; however, it is not provably convergent during execution. A major remaining open problem is whether one can design a natural algorithm that is provably rational, uncoupled, and convergent with exploration capability. Our work provides the first progress towards this goal.

### Other works on last-iterate convergence

Uncoupled Learning dynamics in normal-form games with provable last-iterate convergence rate receives extensive attention recently. Most of the works assume that the players receive gradient feedback, and convergence results under bandit feedback remain sparse. Linear convergence is shown for strongly monotone games or bilinear games under gradient feedback  and sublinear rates are proven for strongly monotone games with bandit feedback . Convergence rate to strict Nash equilibrium is analyzed by . For monotone games that includes two-player zero-sum games as a special case, the last-iterate convergence rate of no-regret learning under gradient feedback has been shown recently . With bandit feedback,  showed an impossibility result that certain algorithms with optimal \(()\) regret do not converge in last-iterate. To the best of our knowledge, there is no natural uncoupled learning dynamics with provable last-iterate convergence rate in two-player zero-sum games with bandit feedback.

## 3 Preliminaries

Basic NotationsThroughout the paper, we assume for simplicity that the action set for the two players are the same, denoted by \(\) with cardinality \(A=||\).1 We usually call player 1 the \(x\)-player and player \(2\) the \(y\)-player. The set of mixed strategies over an action set \(\) is denoted as \(_{}:=\{x:_{a}x_{a}=1;0 x_{a} 1,  a\}\). To simplify notation, we denote by \(z=(x,y)\) the concatenated strategy of the players. We use \(\) as the entropy function such that \((x)=-_{a}x_{a} x_{a}\), and KL as the Kullback-Leibler (KL) divergence such that \((x,x^{})=_{a}x_{a}}{x^{} _{a}}\). The all-one vector is denoted by \(=(1,1,,1)\).

Matrix GamesIn a two-player zero-sum matrix game with a loss matrix \(G^{A A}\), when the \(x\)-player chooses action \(a\) and the \(y\)-player chooses action \(b\), the \(x\)-player suffers loss \(G_{a,b}\) and the \(y\)-player suffers loss \(-G_{a.b}\). A pair of mixed strategy \((x^{},y^{})\) is a _Nash equilibrium_ for \(G\) if for any strategy profile \((x,y)_{}_{}\), it holds that \((x^{})^{}Gy(x^{})^{}Gy^{} x^{}Gy^{}\). Similarly, \((x^{},y^{})\) is a Nash equilibrium for a two-player zero-sum game with a general convex-concave loss function \(f(x,y):_{}_{}\) if for all \((x,y)_{}_{}\), \(f(x^{},y) f(x^{},y^{}) f(x,y^{})\). The celebrated minimax theorem  guarantees the existence of Nash equilibria in two-player zero-sum games. For a pair of strategy \((x,y)\), we use _duality gap_ defined as \((G,x,y)_{y^{}}x^{}Gy^{}-_{x^{ }}x^{}Gy\) to measure its proximity to Nash equilibria.

Markov GamesA generalization of matrix games, which models dynamically changing environment, is _Markov games_. We consider infinite-horizon discounted two-player zero-sum Markov games, denoted by a tuple \((,,(G^{s})_{s},(P^{s})_{s}, )\) where (1) \(\) is a finite state space; (2) \(\) is a finite action space for both players; (3) Player 1 suffers loss \(G^{s}_{a,b}\) (respectively player 2 suffersloss \(-G_{a,b}^{s}\)) when player 1 chooses action \(a\) and player 2 chooses action \(b\) at state \(s\); (4) \(P\) is the transition function such that \(P_{a,b}^{s}(s^{})\) is the probability of transiting to state \(s^{}\) when player 1 plays \(a\) and player 2 plays \(b\) at state \(s\); (5) \([,1)\) is a discount factor.

A stationary policy for player 1 is a mapping \(_{}\) that specifies player 1's strategy \(x^{s}_{}\) at each state \(s\). We denote \(x=(x^{s})_{s}\). Similar notations apply to player 2. We denote \(z^{s}=(x^{s},y^{s})\) as the concatenated strategy for the players and \(z=(x,y)\). The value function \(V_{x,y}^{s}\) denotes the expected loss of player 1 (or the expected payoff of player 2) given a pair of stationary policy \((x,y)\) and initial state \(s\):

\[V_{x,y}^{s}=_{t=1}^{}^{t-1}G_{a_{t},b_{t}}^{ s_{t}}|s_{1}=s,a_{t} x^{s_{t}},b_{t} y^{s_{t}},s_{t+1} P_{a_{t},b_{t}} ^{s_{t}}(), t 1.\]

The _minimax game value_ on state \(s\) is defined as \(V_{}^{s}=_{x}_{y}V_{x,y}^{s}=_{y}_{x}V_{x,y}^{s}\). We call a pair of policy \((x_{},y_{})\) a _Nash equilibrium_ if it attains minimax game value of a state \(s\) (such policy pair necessarily attains the minimax game value over all states). The _duality gap_ of \((x,y)\) is \(_{s}(_{y^{}}V_{x,y^{}}^{s}-_{x^{}}V_{x^{ },y}^{s})\). The \(Q\)-function on state \(s\) under policy pair \((x,y)\) is defined via \(Q_{x,y}^{s}(a,b)=G_{a,b}^{s}+_{s^{} P_{a,b}^{s} ()}[V_{x,y}^{s^{}}]\), which can be rewritten as a matrix \(Q_{x,y}^{s}\) such that \(V_{x,y}^{s}=x^{s}Q_{x,y}^{s}y^{s}\). We denote \(Q_{}^{s}=Q_{x,y}^{s}\), the \(Q\)-function under a Nash equilibrium \((x_{},y_{})\). It is known that \(Q_{}^{s}\) is unique for any \(s\) even when multiple equilibria exist.

Uncoupled Learning with Bandit FeedbackWe assume the following uncoupled interaction protocol: at each round \(t=1,,T\), the players both observe the current state \(s_{t}\), and then, with the policy \(x_{t}\) and \(y_{t}\) in mind, they independently choose actions \(a_{t} x_{t}^{s_{t}}\) and \(b_{t} y_{t}^{s_{t}}\), respectively. Both of them then observe \(_{t}\) with \([_{t}]=G_{a_{t},b_{t}}^{s_{t}}\), and proceed to the next state \(s_{t+1} P_{a_{t},b_{t}}^{s_{t}}()\). Importantly, they do not observe each other's action.

Notions of ConvergenceFor Markov games with the irreducible assumption (Assumption 1), given players' history of play \((s_{t},x_{t},y_{t})_{t[T]}\), the _best-iterate_ convergence rate is measured by the average duality gap \(_{t=1}^{T}_{s,x,y}(V_{x_{t},y}^{s}-V_{x,y_{t}}^{s})\), while the stronger _last-iterate_ convergence rate is measured by \(_{s,x,y}(V_{x_{T},y}^{s}-V_{x,y_{T}}^{s})\), i.e., the duality gap of \((x_{T},y_{T})\). For general Markov games, we propose the _path_ convergence rate, which is measured by the average duality gap at the visited states with respect to the optimal \(Q\)-function: \(_{t=1}^{T}_{x_{t},y}(x_{t}^{s_{t}^{}}Q_{}^{s_ {t}}y^{s_{t}}-x_{t}^{}Q_{}^{s_{t}}y_{t}^{s_{t}})\). We remark that the path convergence guarantee is weaker than the counterpart of the other two notions of convergence in general Markov games, but still provides meaningful implications (see detailed discussion in Section 6.1 and Appendix F).

## 4 Matrix Games

In this section, we consider two-player zero-sum matrix games. We propose Algorithm 1 for decentralized learning of Nash equilibria. We only present the algorithm for the \(x\)-player as the algorithm for the \(y\)-player is symmetric.

```
1:Define:\(_{t}=t^{-k_{}}\), \(_{t}=t^{-k_{}}\), \(_{t}=t^{-k_{}}\) where \(k_{}=\), \(k_{}=\), \(k_{}=\). \(_{t}=\{x_{}:x_{a}},\, a \}\).
2:Initialization:\(x_{1}=\).
3:for\(t=1,2,\)do
4: Sample \(a_{t} x_{t}\), and receive \(_{t}\) with \([_{t}]=G_{a_{t},b_{t}}\).
5: Compute \(g_{t}\) where \(g_{t,a}=[a_{t}=a]_{t}}{x_{t,a}+_{t}}+ _{t} x_{t,a}, a\).
6: Update \(x_{t+1}*{argmin}_{x_{t+1}}\{x^{}g_{t}+ }(x,x_{t})\}.\)
7:endfor ```

**Algorithm 1** Matrix Game with Bandit Feedback

The algorithm is similar to the Exp3-IX algorithm by  that achieves a high-probability regret bound for adversarial multi-armed bandits, but with several modifications. First (and most importantly), in addition to the standard loss estimators used in , we add another negative term \(_{t} x_{t,a}\) to the loss estimator of action \(a\) (see Line 5). This is equivalent to the entropy regularization approach in, e.g., , since the gradient of the negative entropy \(-(x_{t})\) is \(( x_{t,a}+1)_{a}\) and the constant \(1\) takes no effect in Line 6. Like , the entropy regularization drives last-iterate convergence; however, while their results require full-information feedback, our result holds in the bandit feedback setting. The second difference is that instead of choosing the players' strategies in the full probability simplex \(_{}\), our algorithm chooses from \(_{t}\), a subset of \(_{}\) where every coordinate is lower bounded by \(}\). The third is the choices of the learning rate \(_{t}\), clipping factor \(_{t}\), and the amount of regularization \(_{t}\). The main result of this section is the following last-iterate convergence rate of Algorithm 1.

**Theorem 1** (Last-iterate Convergence Rate).: _Algorithm 1 guarantees with probability at least \(1-()\), for any \(t 1\),_

\[_{x,y_{A}}(x_{t}^{}Gy-x^{}Gy_{t})=(^{3/2}(At/)t^{-}).\]

Algorithm 1 also guarantees \((t^{-})\) regret even when the other player is adversarial. If we only target at an _expected_ bound instead of a high-probability bound, the last-iterate convergence rate can be improved to \((^{3/2}(At)t^{-})\). The details are provided in Appendix C.

### Analysis Overview

We define a regularized zero-sum game with loss function \(f_{t}(x,y)=x^{}Gy-_{t}(x)+_{t}(y)\) over domain \(_{t}_{t}\), and denote by \(z_{t}^{*}=(x_{t}^{*},y_{t}^{*})\) its unique Nash equilibrium since \(f_{t}\) is strongly convex-strongly concave. The regularized game is a slight perturbation of the original matrix game \(G\) over a smaller domain \(_{t}_{t}\), and we prove that \(z_{t}^{*}\) is an \((_{t})\)-approximate Nash equilibrium of the original matrix game \(G\) (Lemma 9). Therefore, it suffices to bound \((z_{t}^{*},z_{t})\) since the duality gap of \(z_{t}\) is at most \(((z_{t}^{*},z_{t})}+_{t})\).

Step 1: Single-Step AnalysisWe start with a single-step analysis of Algorithm 1, which shows:

\[(z_{t+1}^{*},z_{t+1})(1-_{t}_{t})(z_{t}^{*},z_{t})+^{2}A^{2}(At)+2_{t}^{2}A_{t}}_{ }+_{t}+_{t}_{t}}_{}+v_{t}\]

where we define \(v_{t}=(z_{t+1}^{*},z_{t+1})-(z_{t}^{*},z_{t+1})\) (see Appendix B for definitions of \(_{t},_{t},_{t}\)) The instability penalty comes from some local-norm of the gradient estimator \(g_{t}\). The estimation error comes from the bias between the gradient estimator \(g_{t}\) and the real gradient \(Gy_{t}\). We pay the last term \(v_{t}\) since the Nash equilibrium \(z_{t}^{*}\) of the regularized game \(f_{t}\) is changing over time.

Step 2: Strategy Convergence to NE of the Regularized GameExpanding the above recursion up to \(t_{0}\), we get

\[(z_{t+1}^{*},z_{t+1})^{ t}w_{t}^{i}_{i}^{2}}_{_{1}}+^{t}w_{t}^{i} _{i}^{2}_{i}}_{_{2}}+^{t}w_{t}^{i} _{i}_{i}}_{_{3}}+^{t}w_{t}^{i}_{i} _{i}}_{_{4}}+^{t}w_{t}^{i}v_{i}}_{ _{5}},\] (1)

where \(w_{t}^{i}_{j=i+1}^{t}(1-_{j}_{j})\). To upper bound \(_{1}\)-\(_{4}\), we apply careful sequence analysis (Appendix A.1) and properties of the Exp3-IX algorithm with changing step size (Appendix A.2). The analysis of \(_{5}\) uses Lemma 13, which states \(v_{t}=(z_{t+1}^{*},z_{t+1})-(z_{t}^{*},z_{t+1})((At)\|z_{t+1}^{*}-z_{t}^{*}\|_{1})=((At)}{t})\) and is slightly involved as \(_{t}\) and \(_{t}\) are both changing. With these steps, we conclude that with probability at least \(1-()\), \((z_{t}^{*},z_{t})=(A^{3}(At/)t^{-})\).

## 5 Irreducible Markov Games

We now extend our results on matrix games to two-player zero-sum Markov games. Similarly to many previous works, our first result makes the assumption that the Markov game is _irreducible_ with bounded travel time between any pair of states. The assumption is formally stated below:

**Assumption 1** (Irreducible Game).: _We assume that under any pair of stationary policies of the two players, and any pair of states \(s,s^{}\), the expected time to reach \(s^{}\) from \(s\) is upper bounded by \(L\)._

We propose Algorithm 2 for uncoupled learning in irreducible two-player zero-sum games, which is closely related to the Nash-V algorithm by , but with additional entropy regularization. It can also be seen as players using Algorithm 1 on each state \(s\) to update the policies \((x_{t}^{s},y_{t}^{s})\) whenever state \(s\) is visited, but with \(_{t}+ V_{t}^{s_{t+1}}\) as the observed loss to construct loss estimators. Importantly, \(V_{1}^{s},V_{2}^{s},\) is a slowly changing sequence of value estimations that ensures stable policy updates . Note that in Algorithm 2, the updates of \(V_{t}^{s}\) only use players' local information (Line 8).

```
1:Define:\(_{t}=(1-)t^{-k_{}},\,_{t}=t^{-k_{}},\,_{t}= t^{-k_{}}\), \(_{t}=t^{-k_{}}\) with \(k_{},k_{},k_{},k_{}(0,1)\), \(_{t}=\{x_{A}:x_{a}},\, a \}\).
2:Initialization:\(x_{1}^{s}\), \(n_{1}^{s} 0,\,\,\,V_{1}^{s},\,\,  s\).
3:for\(t=1,2,,\)do
4:\(=n_{t+1}^{s_{t}} n_{t}^{s_{t}}+1\) (the number of visits to state \(s_{t}\) up to time \(t\)).
5: Draw \(d_{t} x_{t}^{s_{t}}\), observe \(_{t}\) with \([_{t}]=G_{a_{t},b_{t}}^{s_{t}}\), and observe \(s_{t+1} P_{a_{t},b_{t}}^{s_{t}}()\).
6: Compute \(g_{t}\) where \(g_{t,a}==a)(_{t}+ V_{t}^{s_{t+1}})}{x_{t}^ {s_{t}}+_{s}}+_{} x_{t,a}^{s_{t}},\,\, a \).
7: Update \(x_{t+1}^{s_{t}}*{argmin}_{x_{+1}}\{x ^{}g_{t}+}(x,x_{t}^{s_{t}})\}\).
8: Update \(V_{t+1}^{s_{t}}(1-_{})V_{t}^{s_{t}}+_{}( _{t}+ V_{t}^{s_{t+1}})\).
9: For all \(s s_{t}\), \(x_{t+1}^{s} x_{t}^{s}\), \(n_{t+1}^{s} n_{t}^{s}\), \(V_{t+1}^{s} V_{t}^{s}\).
10:endfor ```

**Algorithm 2** Irreducible Markov Game

Comparison to Previous WorksAlthough Algorithm 2 shares similarity with previous works that also use entropy regularization, we believe that both the design and the analysis of our algorithm are novel and non-trivial. To the best of our knowledge, all previous entropy regularized two-player zero-sum Markov game algorithms are coupled (e.g., ), while ours is the first that achieves uncoupledness under entropy regularization. We further discuss this by comparing our algorithm to those in , highlighting the new technical challenges we encounter.

The entropy-regularized OMWU algorithm in  is tailored to the full-information setting. Moreover, in the value function update step both players need to know the entropy value of the other player's policy, which is unnatural. Indeed, the authors explicitly present the removal of this information sharing as an open question. We answer this open question affirmatively by giving a fully decentralized algorithm for zero-sum Markov games with provable last-iterate convergence rates. In Algorithm 2 (Line 8), the update of the value function \(V\) is simple and does not require any entropy information: \(V_{t+1}^{s_{t}}(1-_{})V_{t}^{s_{t}}+_{}( _{t}+ V_{t}^{s_{t+1}})\). This modification results in a discrepancy between the policy update and the value update. While the policy now incorporates a regularization term, the value function does not. Such a mismatch is unprecedented in earlier studies and necessitates a non-trivial approach to resolve. Additionally, Algorithm 2 operates on bandit feedback instead of full-information feedback, presenting further technical challenges.

```
1:Define:\(_{t}=(1-)t^{-k_{}},\,_{t}=t^{-k_{}},\,_{t}= t^{-k_{}}\), \(_{t}=t^{-k_{}}\) with \(k_{},k_{},k_{}(0,1)\), \(_{t}=\{x_{A}:x_{a}},\, a \}\).
2:Initialization:\(x_{1}^{s}}\), \(n_{1}^{s} 0,\,\,\,V_{1}^{s}}\), \( a\).
3:for\(t=1,2,,\)do
4:\(=n_{t+1}^{s_{t}} n_{t}^{s_{t}}+1\) (the number of visits to state \(s_{t}\) up to time \(t\)).
5: Draw \(d_{t} x_{t}^{s_{t}}\), observe \(_{t}\) with \([_{t}]=G_{a_{t},b_{t}}^{s_{t}}\), and observe \(s_{t+1} P_{a_{t},b_{t}}^{s_{t}}()\).
6: Compute \(g_{t}\) where \(g_{t,a}==a)(_{t}+ V_{t}^{s_{t+1}})}{x_{t}^ {s_{t}}+_{s}}+_{} x_{t,a}^{s_{t}},\,\, a\).
7: Update \(x_{t+1}^{s_{t}}*{argmin}_{x_{+1}}\{x ^{}g_{t}+(x,x_{t}^{s_{t}})\}\).
8: Update \(V_{t+1}^{s_{t}}(1-_{})V_{t}^{s_{t}}+_{}( _{t}+ V_{t}^{s_{t+1}})\).
9: For all \(s s_{t}\), \(x_{t+1}^{s} x_{t}^{s}\), \(n_{t+1}^{s} n_{t}^{s_{t}} n_{t}^{s_{t}}\), \(V_{t+1}^{s} V_{t}^{s}\).
10:endfor ```

**Algorithm 2** Irreducible Markov Game

Algorithm 2 also offers improvement over the uncoupled algorithm of . The algorithm of  requires coordinated policy update where the players interact with each other using the current policy for several iterations to get an approximately accurate gradient (the number of iterations required depends on \(L\) as defined in Assumption 1), and then simultaneously update the policy pair on all states. We do not require such unnatural coordination between the players or prior knowledge on \(L\).

Our main result is the following theorem on the last-iterate convergence rate of Algorithm 2.

**Theorem 2** (Last-Iterate Convergence Rate).: _For any \(,>0\), Algorithm 2 with \(k_{}=\), \(k_{}=\), \(k_{}=\), and \(k_{}=\) guarantees, with probability at least \(1-()\), for any time \(t 1\),_

\[_{s,x,y}V_{x_{t},y}^{s}-V_{x,y_{t}}^{s} ^{4+1/}(SAt/)^{1/}(t/( 1-))}{(1-)^{2+1/}} t^{-}.\]

### Analysis Overview

We introduce some notations for simplicity. We denote by \(_{s^{} P^{s}}[V_{t}^{s^{}}]\) the \(A A\) matrix such that \((_{s^{} P^{s}}[V_{t}^{s^{}}])_{a,b}=_{s^{ } P^{s}_{a,b}}[V_{t}^{s^{}}]\). Let \(t_{}(s)\) be the \(\)-th time the players visit state \(s\), and define \(_{}^{s}=x_{t_{}(s)}^{s}\) and \(_{}^{s}=y_{t_{}(s)}^{s}\). Then, define the regularized game for each state \(s\) via the loss function \(f_{}^{s}(x,y)=x^{}(G^{s}+_{s^{} P^{s}}[V_{t_ {}(s)}^{s^{}}])y-_{}(x)+_{}(y)\). Furthermore, let \(_{}^{s}=(_{}^{s},_{}^{s})\) be the equilibrium of \(f_{}^{s}(x,y)\) over \(_{}_{}\). In the following analysis, we fix some \(t 1\).

Step 1: Policy Convergence to NE of Regularized GameUsing similar techniques to Step 1 and Step 2 in the analysis of Algorithm 1, we can upper bound \((_{+1}^{s},_{+1}^{s})\) like Eq. (1) with similar subsequent analysis for \(_{1}\)-\(_{4}\). The analysis for \(_{5}\) where \(v_{i}^{s}=(_{i+1}^{s},_{i+1}^{s})-(_{i}^{s},_{i+1}^{s})\) is more challenging compared to the matrix game case since here \(V_{t_{i}(s)}^{s}\) is changing between two visits to state \(s\). To handle this term, we leverage the following facts for any \(s^{}\): (1) the irreducibility assumption ensures that \(t_{i+1}(s)-t_{i}(s)(L(St/))\) thus the number of updates of the value function at state \(s^{}\) is bounded; (2) until time \(t_{i}(s) i\), state \(s^{}\) has been visited at least \(()\) times thus each change of the value function between \(t_{i}(s)\) and \(t_{i+1}(s)\) is at most \((()^{-k_{}})\). With these arguments, we can bound \(_{5}\) by \((^{4}(SAt/)L^{-k_{}+k_{}+2k_{}})\). Overall, we have the following policy convergence of NE of the regularized game (Lemma 17): \((_{}^{s},_{}^{s})(A ^{4}(SAt/)L^{-k_{}})\), where \(k_{}=\{k_{}-k_{},k_{}-k_{},k_{}-k_{ }-2k_{}\}\).

Step 2: Value ConvergenceUnlike matrix games, policy convergence to NE of the regularized game is not enough for convergence in duality gap. We also need to bound \(|V_{t}^{s}-V_{}^{s}|\) since the regularized game is defined using \(V_{t}^{s}\), the value function maintained by the algorithm, instead of the minimax game value \(V_{}^{s}\). We use the following weighted regret quantities as a proxy: \(^{s}_{x,y}(_{i=1}^{}_{}^{i} (f_{i}^{s}(_{i}^{s},_{i}^{s})-f_{i}^{s}(x^{s},_{i}^{s })),_{i=1}^{}_{}^{i}(f_{i}^{s}(_{i}^{s},y_ {i}^{s})-f_{i}^{s}(_{i}^{s},_{i}^{s})))\), where \(_{}^{i}=_{i}_{j=i+1}^{}(1-_{j})\). We can upper bound the weighted regret \(^{s}_{}\) using a similar analysis as in Step 1 (Lemma 19). We then show a contraction for \(|V_{t_{}(s)}^{s}-V_{}^{s}|\) with the weighted regret quantities: \(|V_{t_{}(s)}^{s}-V_{}^{s}|_{i=1}^{}_{}^{i} _{s^{}}|V_{t_{i}(s)}^{s^{}}-V_{}^{s^{}}|+}(_{}+^{s}_{})\). This leads to the following convergence of \(V_{t}^{s}\) (Lemma 20):\(|V_{t}^{s}-V_{}^{s}|}(t^{-k_{}})\), where \(k_{}=\{k_{},k_{},k_{}-k_{},k_{}\}\).

Obtaining Last-Iterate Convergence RateFix any \(t\) and let \(\) be the number of visits to \(s\) before time \(t\). So far we have shown (1) policy convergence of \((_{}^{s},_{}^{s})\) in the regularized game; (2) and value convergence of \(|V_{t}^{s}-V_{}^{s}|\). Using the fact that the regularized game is at most \((_{}+|V_{t}^{s}-V_{}^{s}|)\) away from the minimax game martrix \(Q^{}\) and appropriate choices of parameters proves Theorem 2.

## 6 General Markov Games

In this section, we consider general two-player zero-sum Markov games without Assumption 1. We propose Algorithm 3, an uncoupled learning algorithm that handles exploration and has path convergence rate. Compared to Algorithm 2, the update of value function in Algorithm 3 uses a bonus term \(_{}\) based on the optimism principle to handle exploration.

```
1:\(_{}^{s}=_{x,y}(x_{}^{s_{}^{}}Q_{}^{s_{ }}y^{s_{}}-x_{}^{s_{}^{}}Q_{}^{s_{}}y_{}^{s_{ }})=(t^{-})\)

[MISSING_PAGE_POST]

### Path Convergence

Path convergence has multiple meaningful game-theoretic implications. By definition, It implies that frequent visits to a state bring players' policies closer to equilibrium, leading to both players using near-equilibrium policies for all but \(o(T)\) number of steps over time.

Path convergence also implies that both players have no regret compared to the game value \(V_{*}^{s}\), which has been considered and motivated in previous works such as . To see this, we apply the results to the _episodic_ setting, where in every step, with probability \(1-\), the state is redrawn from \(s\) for some initial distribution \(\). If the learning dynamics enjoys path convergence, then \([_{t=1}^{T}x_{t}^{s_{t}^{}}G^{s_{t}}y_{t}^{s_{t}}]=(1- )_{s}[V_{*}^{s}]T o(T)\). Hence the one-step average reward is \((1-)_{s}[V_{*}^{s}]\) and both players have no regret compared to the game value. A more important implication of path convergence is that it guarantees stability of players' policies, while cycling behaviour is inevitable for any FTRL-type algorithms even in zero-sum matrix games . We defer the proof and more discussion of path convergence to Appendix F.

Finally, we remark that our algorithm is built upon Nash V-learning , so it inherits properties of Nash V-learning, e.g., one can still output near-equilibrium policies through policy averaging , or having no regret compared to the game value when competing with an arbitrary opponent . We demonstrate extra benefits brought by entropy regularization regarding the stability of the dynamics.

### Analysis Overview of Theorem 3

For general Markov games, it no longer holds that every state \(s\) is visited often, and thus the analysis is much more challenging. We first define two regularized games based on \(_{t}^{s}\) and the corresponding quantity \(_{t}^{s}\) for the \(y\)-player. Define \(t_{}(s)\), \(_{*}^{s}\), \(_{*}^{s}\) the same way as in the previous section. Then define \(f_{}^{s}(x,y) x^{}(G^{s}+_{s^{} P ^{s}}[_{t_{}(s)}^{s^{}}])y-(x)+( y)\), \(_{}^{s}(x,y) x^{}(G^{s}+_{s^{}  P^{s}}[_{t_{}(s)}^{s^{}}])y-(x)+ (y)\) and denote \(J_{t}=_{x,y}(x_{t}^{s_{}^{}}(G^{s_{t}}+_{s^{ } P^{s_{t}}}[_{t}^{s^{}}]y^{s_{t}}-x_{t}^{s_{}^{ }}(G^{s}+_{s^{} P^{s_{t}}}[_{t}^{s^ {}}])y_{t}^{s_{t}})\). We first bound the "path duality gap" as follows

\[_{x,y}x_{t}^{s_{}^{}}Q_{}^{s_{}}y^{s}-x_{t}^{s_{ }^{}}Q_{}^{s_{}}y_{t}^{s} J_{t}+ _{s^{}}V_{}^{s^{}}-_{t}^{s^{}},_{t}^{s^{}}-V_{}^{s^{}}.\] (3)

Value Convergence: Bounding \(V_{t}^{s}-V_{}^{s}\) and \(V_{}^{s}-_{t}^{s}\)This step is similar to Step 2 in the analysis of Algorithm 2. We first show an upper bound of the weighted regret (Lemma 23): \(_{i=1}^{}_{}^{i}(f_{i}^{s}(_{i}^{s},_{i}^{s})- _{i}^{s}(x^{s},_{i}^{s}))_{}\), where \(_{}^{i}=_{i}_{j=i+1}^{}(1-_{j})\). Note that the value function \(_{t}^{s}\) is updated using \(_{t}+ V_{t}^{s_{t}+1}-_{}\). Thus when relating \(|V_{t}^{s}-V_{}^{s}|\) to the regret, the regret term and the bonus term cancel out and we get \(_{t}^{s} V_{}^{s}+()}{1-})\) (Lemma 26). The analysis for \(V_{}^{s}-_{t}^{s}\) is symmetric. By proper choice of \(\), both terms are bounded by \(u\). Combiningthe above with Eq. (3), we can upper bound the left-hand side of the desired inequality Eq. (2) by \(_{t=1}^{T}[J_{t}u]\), which is further upper bounded in Eq. (29) by

\[_{s}_{=1}^{n_{T+1}(s)}[_{y} {f}_{}^{s}(_{}^{s},y^{s})-_{}^{s}(_{} ^{s})]+_{s}_{=1}^{n_{T+1}(s)}[ _{}^{s}(_{}^{s})-_{x}_{}^{s}( x^{s},_{}^{s})]\] \[+_{t=1}^{T}[x_{t}^{s_{t}^{}}( _{s^{} P^{s_{t}}}[_{t}^{s^{}}- _{t}^{s^{}}])y_{t}^{s_{t}}].\] (4)

Policy Convergence to NE of Regularized GamesTo bound the first two terms, we show convergence of the policy \((_{}^{s},_{}^{s})\) to Nash equilibria of both games \(f_{}^{s}\) and \(_{}^{s}\). To this end, fix any \(p\), we define \(f_{}^{s}=p_{}^{s}+(1-p)_{}^{s}\) and let \(_{}^{s}=(_{}^{s},_{}^{s})\) be the equilibrium of \(f_{}^{s}(x,y)\). The analysis is similar to previous algorithms where we first conduct single-step analysis (Lemma 22) and then carefully bound the weighted recursive terms. We show in Lemma 27 that for any \(0<^{} 1\): \(_{s}_{=1}^{n_{T+1}(s)}[(_{ }^{s},_{}^{s})^{}]( A^{5}(SAT/)}{^{2}^{}(1-) ^{3}})\). This proves policy convergence: the number of iterations where the policy is far away from Nash equilibria of the regularized games is bounded, which can then be translated to upper bounds on the first two terms.

Value Convergence: Bounding \(|_{t}^{s}-_{t}^{s}|\)It remains to bound the last term in Eq. (4). Define \(c_{t}=[x_{t}^{s_{t}}(_{s^{} P^{s_{t}}}[ _{t}^{s}-_{t}^{s^{}}])y_{t}^{s_{t}} ]\) where \(=\). Then we only need to bound \(C_{t=1}^{T}c_{t}\). We use the weighted sum \(P_{T}_{t=1}^{T}c_{t}\). On the other hand, in Lemma 25, by recursively tracking the update of the value function and carefully choosing \(\) and \(\), we upper bound \(P_{T}\) by \(}{2}+((AST/)}{( 1-)^{3}})\). Combining the upper and lower bound of \(P_{T}\) gives \(C((AST/)}{ u(1-)^{3}})\) (Corollary 2). Plugging appropriate choices of \(\), \(\), and \(\) in the above bounds proves Theorem 3 (see Appendix E).

## 7 Conclusion and Future Directions

In this work, we study decentralized learning in two-player zero-sum Markov games with bandit feedback. We propose the first uncoupled and convergent algorithms with non-asymptotic last-iterate convergence rates for matrix games and irreducible Markov games, respectively. We also introduce a novel notion of path convergence and provide algorithm with path convergence in Markov games without any assumption on the dynamics. Previous results either focus on average-iterate convergence or require stronger feedback/coordination or lack non-asymptotic convergence rates. Our results contribute to the theoretical understanding of the practical success of regularization and last-iterate convergence in multi-agent reinforcement learning.

Settling the optimal last-iterate convergence rate that is achievable by uncoupled learning dynamics is an important open question. The following directions are promising towards closing the gap between current upper bounds \((T^{-1/8})\) and \(O(T^{-1/(9+)})\) and lower bound \((T^{-})\), The impossibility result by  demonstrates that certain algorithms with \(()\) regret diverge in last-iterate. Their result indicates that the current \((})\) lower bound on convergence rate may not be tight. On the other hand, our algorithms provides insights and useful templates to potential improvements on the upper bound. For instance, instead of using EXP3-IX update, adapting optimistic policy update or other accelerated first-order methods to the bandit feedback setting is an interesting future direction.

AcknowledgementWe thank Chanwoo Park and Kaiqing Zhang for pointing out a mistake in our previous proof. We also thank the anonymous reviewers for their constructive feedback. HL is supported by NSF Award IIS-1943607 and a Google Research Scholar Award.