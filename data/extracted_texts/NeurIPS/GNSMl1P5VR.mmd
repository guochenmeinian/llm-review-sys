# Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models

Yushi Hu\({}^{*}\)

Weijia Shi\({}^{*}\)

Xingyu Fu

University of Washington

Allen Institute for AI

Dan Roth

University of Pennsylvania

Mari Ostendorf

University of Washington

Allen Institute for AI

Luke Zettlemoyer

University of Washington

Allen Institute for AI

Noah A. Smith

Ranjay Krishna

###### Abstract

Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. Sketchpad can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment on a wide range of math tasks (including geometry, functions, graph, chess) and complex visual reasoning tasks. Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a new state of the art on all tasks, including \(V^{*}\)Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%).All codes and data are in https://visualsketchpad.github.io/.

## 1 Introduction

Sketching is a fundamental human activity, serving as a versatile tool for communication , ideation , and problem-solving . Unlike written language, sketches have the advantage of conveying visuo-spatial ideas directly, for example by using spatial relations on paper to convey spatial relations or other more abstract relationships in the world. This may explain their ubiquity; maps  and architectural plans  have been found incised in stone, etched on leather, impressed in clay, and drawn on paper in diverse cultures scattered across the world . Sketches are so fundamental that we use them to teach school children how to solve geometry problems by drawing support lines, to aid engineers conveying prototypes, to support architects creating blueprints, and to allow scientists like us to convey scientific contributions (see Figure 1).

As multimodal language models (LMs)  have begun to mature, we now expect them to solve tasks like the ones mentioned above, i.e., ones where people draw intermediate sketches to simplify reasoning. Popular benchmarks now include questionsabout geometry (e.g., Geometry3K ) and complex math problems (e.g., IsoBench ). In these benchmarks, models are given images of diagrams and asked questions requiring symbolic grounding and spatial understanding, where intermediate sketches like auxiliary lines can enhance reasoning. Even benchmarks in computer vision now have a similar flavor. Specialist vision models can be viewed as sketching on natural images. For example, object detection is plotting bounding boxes around objects; depth estimation is drawing colormaps according to depth. The recently proposed BLINK benchmark  would benefit significantly from such intermediate visual sketches. Similarly, the \(V\)*Bench benchmark  could focus reasoning on image crops to find answers. Unfortunately, current LMs lack a scaffold for using sketch-based reasoning when solving tasks.

In this paper, **we introduce Visual Sketchpad: a framework that provides multimodal LMs with the tools necessary to generate intermediate sketches to reason over tasks.** Inspired by textual chain-of-thought reasoning in LMs , Sketchpad prompts the underlying visual LM to produce visual artifacts as part of a chain of mixed textual, programmatic, and visual reasoning. For example, to prove that the angles of triangles sum up to 180 degrees in Figure 1 (a), Sketchpad enables agents to modify the diagram by introducing a new auxiliary line. This new line, along with new annotated angles, provides the critical information to solve the geometry task. Similarly, Sketchpad improves models' spatial reasoning for computer vision. To determine if there are cookies stacked on top of other cookies in the image (Figure 0(b)), the model first produces an intermediate depth estimate. By analyzing the depth estimate, which reveals cookies overlapping at different depths, the model is able to correctly answer that the cookies are indeed stacked.

We demonstrate the effectiveness of visual Sketchpad across a wide range of mathematics and computer vision tasks. For math, we tackle problems including (1) geometry , (2) mathematical functions, (3) graph algorithms, and (4) strategy games . For geometry questions, Sketchpad enables models to generate Matplotlib code with auxiliary lines and variables, given the diagram input and questions (Figure 0(a)). Notably, even when the input is solely language-based, such as mathematical functions, Sketchpad enables models to plot the functions and reason about their properties, using only the mathematical function expression as input (Figure 0(b)). These results

Figure 1: Sketchpad **equips GPT-4 with the ability to generate intermediate sketches to reason over tasks.** Given a visual input and query, such as proving the angles of a triangle equal 180\({}^{}\), Sketchpad enables the model to draw auxiliary lines which help solve the geometry problem. The examples are from . For all these examples, without Sketchpad, GPT-4o fails to get the correct answer, while Sketchpad + GPT-4o achieves the correct solution.

highlight the ability of Sketchpad to aid reasoning, even in tasks with purely language-based inputs.

**Across all four categories of mathematical tasks, Sketchpad consistently improves the baseline GPT-4o performance, yielding an average gain of 11.2%.** For computer vision, we tackle diverse tasks including (1) depth, (2) spatial reasoning, (3) jigsaw, (4) visual correspondence, (5) semantic correspondence, as well as questions from (6) the MMVP and (7) the \(V^{}\)Bench benchmarks [9; 44; 51]. For this domain, Sketchpad enables models to generate segmentation masks, crop images, draw bounding boxes, zoom into image regions, overlay images, etc. Similar to math, **Sketchpad shows consistent improvements across all seven types of computer vision tasks**. For example, GPT-4o, augmented with Sketchpad, sees 14.3% improvement on \(V^{}\)Bench, 12.1%, and 9.7% improvements on BLINK's depth and semantic correspondence tasks, setting a new state of the arts across all tasks. Finally, we analyze the effectiveness of Sketchpad by comparing the plans generated by our model with human-created plans, showing that they are well-aligned and exhibit similar reasoning patterns. We hope Sketchpad opens up new research opportunities toward more capable and interpretable multimodal intelligence.

## 2 Related Work

Sketchpad generalizes recent work on multimodal tool-use and visual prompting. We also place our work within the larger sphere exploring LMs as agents.

**Visual programming and tool-use.** With the advancement of LMs [4; 35; 42; 45; 13], researchers have demonstrated the possiblity of decomposing complex vision tasks into simpler substeps that can each be solved using vision tools [57; 60; 18; 17]. Among them, the most relevant to us are Visprog  and ViperGPT . They use LMs to generate Python code, which sequencially invokes specialized vision tools. These methods share a common problem that the multimodal modules follow a pre-defined plan outlined by the LM. By contrast, Sketchpad allows LMs to change their plan according to the intermediate visual artifacts, yielding better performance and robustness when solving complex multimodal tasks.

**Visual prompting.** Recent work shows that multimodal models can be augmented by visual prompts added to natural images . For example, SoM  shows that adding labeled segmentation masks on images unleashes GPT-4V's visual grounding ability. Prior work also reports similar findings in 3D  and Robotics . Sketchpad is a generalized framework for all these methods, allowing LMs to decide what visual prompting to use as part of the multimodal reasoning process.

**LMs as agents.** Recent work has started to treat LMs as agents that can both reason and act [58; 33; 52; 36; 59]. Researchers have applied this idea to software engineering [20; 61; 15], robotics , vision [29; 57], and GUI navigation [54; 23; 53]. Sketchpad can also be viewed as an agent that accepts multimodal inputs and outputs. One big difference is that Sketchpad can create visual artifacts to facilitate reasoning, while prior LM agents only generate text during reasoning.

## 3 Visual Sketchpad

We introduce visual Sketchpad, a general framework that enables multimodal LMs to draw sketches as intermediate reasoning steps and to use these sketches to facilitate further reasoning. Figure 2 shows examples of how Sketchpad works. Given a multimodal query, Sketchpad agent generates a sketching plan to address the query (_Thought_), and then synthesizes a program to create visual sketches (_Action_). By analyzing the resulting sketches (_Observation_), which serve as a visual representation of the reasoning process, the model generates a final response to the query.

Our framework requires no finetuning or training. Multimodal LMs, out of the box, can be prompted to sketch using our framework. Our implementation is based on the AutoGen  framework. We give the overview of our Sketchpad framework in SS3.1, and delve deep into how it integrates sketching into the reasoning process in SS3.2.

### Overview of Sketchpad

The Sketchpad agent solves tasks by engaging in an iterative interaction process with an environment. Given a multimodal query \(q\) that includes both visual and textual components, the model generates a series of thoughts, actions, and observations to gather the information needed to answer the query. At each time step \(t\), the model performs three key steps:

**Thought:** The model analyzes the current context \(c_{t}\), which includes the query, previous thoughts, actions, and observations, to generate a thought plan \(p_{t}\) for the next action. For example, given the query \(q-\)_"find the \( BIC\)"_ in Figure 1(a), the model's thought plan \(p_{1}\) is to draw an auxiliary line \(IX\) parallel to \(BD\) serving as a _visual sketch_ to help solve the problem.

**Action:** Based on the thought plan, the model executes an action \(a_{t}\), which can manipulate both visual and textual content. In the geometry example, to realize the proposed thought of drawing the auxiliary line, the model generates Python code to modify the original geometry diagram. The generated code is then compiled and executed.

**Observation:** Based on the action \(a_{t}\), Sketchpad's environment returns a new observation \(o_{t+1}\), such as a new diagram with the auxiliary line drawn in the geometry example. The multimodal context is then updated to \(c_{t+1}=(c_{t},p_{t},a_{t},o_{t+1})\).

The multi-turn interaction process continues until time step \(T\), when the model determines that it has gathered enough information from the context \(c_{T}\) to answer the query. At this point, it generates a special **Terminate** action and provides the answer.

Different from prior work , where LMs primarily generate and manipulate text-based observations and actions, Sketchpad enables the model to work with **multimodal observations \(o_{t}\) and actions \(a_{t}\), manipulating both visual and textual content.** This allows the model to plan and reason with the visual sketches it has drawn, enhancing its problem-solving capabilities.

### Sketching via Code Generation

The core component of Sketchpad is sketching, which enables the LM to generate visual sketches by synthesizing programs that call different specialist vision models or Python plotting packages.

**Program Generation.** Similar to recent works like ViperGPT and VPD , Sketchpad enables LMs to sketch through code generation. The LM is provided, through a prompt, with a detailed description of the available tools that can generate multimodal content (an example prompt and description can be found in SSC). The prompt includes Python function signatures and docstrings  for these modules, but does not contain their full implementation. The LM generates Python code in a code block, using the provided tools, which, when executed, generates new image and text outputs. A special _display_ function allows the LM to **visualize** the sketch image in the next observation \(o_{t+1}\).

**Modules for sketching.** Sketchpad uses various tools to facilitate the sketching process, depending on the task at hand. For mathematical tasks, Sketchpad uses common Python packages like

Figure 2: **Overview of Sketchpad. Given a multimodal query, the Sketchpad agent generates a sketching plan to address the query (_Thought_), and then synthesizes a program to create visual sketches (_Action_). By analyzing the resulting sketches (_Observation_), which serve as a visual representation of the reasoning process, the model generates a final response to the query.**matplotlib and networkx for plotting (see SS4). For vision tasks, the LM leverages **specialist vision models** during the sketching process. These models include detection tools that draw bounding boxes on the image, as well as segmentation and marking tools (inspired by SoM ) that draw colorful masks on the image and use numbers to label segments. We find these specialists possess useful perception skills for visual reasoning tasks, and Sketchpad is an effective way to combine them into a multimodal LM (see SS5.1).

## 4 Sketching to Solve Math Problems

In this section, we experiment with Sketchpad on four complex mathematical tasks : (1) geometry, (2) mathematical functions, (3) graph algorithms, and (4) game strategies. We demonstrate that incorporating sketching capabilities into LMs significantly improves their performance on these mathematical problems, setting new state-of-the-art results (SS4.1).

Details of our evaluation tasks and the tools employed for visual reasoning are as follows:

**Geometry Problems.** Drawing auxiliary lines in geometry diagrams is often helpful for problem-solving. For example, in Figure 2 (a), when asked to find \( EIC\), the LM plans to draw an auxiliary line \(IX\) parallel to \(BD\), allowing it to use the properties of parallel lines to determine \( EIC\). To evaluate the effectiveness of Sketchpad, we use the problems from the Geometry3K dataset .

To realize the line drawing process, Sketchpad takes a geometry diagram and its corresponding matplotlib code as input. The model then proposes and modifies the code to generate auxiliary lines, and executes the modified code to visualize the updated diagram with the added lines.

**Mathematical functions.** Understanding mathematical functions is crucial for various applications in science, engineering, and economics. We focus on two tasks related to mathematical functions from the IsoBench datasets :

* _Classifying parity_ aims to determine whether a function is even, odd, or neither. Even functions satisfy \(f(-x)=f(x)\) for all \(x\), while odd functions satisfy \(f(-x)=-f(x)\).
* _Identifying convexity/concavity_ aims to determine whether a function is convex or concave.

Existing LMs can only analyze functions and attempt to prove their properties analytically. 1 However, Sketchpad enables them to visually sketch functions to solve problems more efficiently. For instance, to determine the convexity of the function in Figure 0(b), Sketchpad allows the model to plot the function using matplotlib, and visually inspect its overall shape.

**Graph algorithms.** Many real-world problems, such as those related to computer networks and transportation systems, can be formulated as graph problems. We evaluate Sketchpad on three graph problems from IsoBench :

* _Graph connectivity_ determines whether there exists a path between two vertices in a graph.
* _Maximum flow_ aims to find the maximum amount of flow that can be sent through a network from a source vertex to a sink vertex, subject to capacity constraints on the edges.
* _Graph isomorphism_ tests whether two graphs are structurally equivalent.

Given an adjacency matrix of a graph like in Figure 2(b), Sketchpad can draw the actual graph structure, using using Python's networkx library, enabling direct visual reasoning about graph properties and relationships.

**Game strategies.** Chess games can be represented in various formats, including visual board states and textual move notations. Given only the textual move notations, Sketchpad can draw the visual representations of the chess board to analyze positions and formulate strategies. We evaluate the performance of Sketchpad on the wimber identification task from the IsoBench datasets  that aims to find the outcome of a chess game (win for White, win for Black, or draw) based on the final board state. To create the graphical board, Sketchpad uses Python's chess library to draw the board from the Forsyth-Edwards Notation (FEN) of chess.

### Results

We evaluate the performance of Sketchpad on multimodal LMs with API access, including gpt-4-turbo-2024-04-29 and gpt-4o-2024-05-13. We compare these results to baselines without the Visual Sketchpad and other notable closed-source models, such as Claude 3 and Gemini-Pro, as well as open-source models like Mistral  and LLaMA-2 70B .

**Main results.** As shown in Table 1, Sketchpad consistently improves base model performance across all tasks, with an average improvement of 11.2% for GPT-4o and 23.4% for GPT-4 Turbo. In particular, we observe large gains on graph algorithms such as maximum flow and connectivity. For instance, GPT-4o with Sketchpad achieves an accuracy of 66.3% on the maximum flow problem, improving over the base model by 41.3%. Similarly, Sketchpad substantially improves the performance on mathematical functions, with GPT-4 Turbo achieving over 90% accuracy and GPT-4o over 88% accuracy on convexity and parity classification tasks. Furthermore, we observe gains (3% \(\) 10%) on game strategies, demonstrating that drawn game boards drawn can improve reasoning about game strategies. Overall, these results highlight the effectiveness of Sketchpad in enhancing the reasoning capabilities of multimodal language models across diverse domains.

## 5 Sketching to Solve Computer Vision Tasks

In this section, we experiment with Sketchpad on complex visual reasoning tasks. Recent work (BLINK)  finds that many core visual perception abilities are still missing from existing multimodal LMs--even though many computer vision specialist models possess such abilities. Also, SoM  shows that drawing segmentation masks on images unleashes the strong visual grounding ability of GPT-4V. We generalize these ideas with Sketchpad, allowing LMs to use **specialist vision models** to sketch. Details of these modules are in SS5.1. Sketchpad enhances multimodal LMs' visual reasoning abilities and establishes new SOTAs on all 7 tasks (SS5.2).

**Tasks.** We experiment with a wide range of complex visual reasoning tasks: (1) \(V^{*}\)**Bench**. This benchmark contains questions about small items in an image. (2) **MMVP** benchmark from _Eyes Wide Shut_. This benchmark contains visual questions specially designed to reveal the visual shortcomings of CLIP-based multimodal LMs. (3) **BLINK**. This benchmark contains visual perception tasks that are easy for humans, but post significant challenge for multimodal LMs. Specifically, we experiment with relative depth, spatial reasoning, jigsaw puzzle, visual correspondence, and semantic correspondence tasks. More details of each task are in SSD.

### Vision Specialists as Sketching Tools in Sketchpad

LMs can use the following modules to sketch and manipulate images. We wrap these modules into Python functions that the LMs can call. Refer to SSC for the function definitions.

**Detection.** This module takes an image and a simple text query (e.g., "cat") as input. We run the Grounding-DINO  open-vocabulary objection detection model and plot the detected bounding boxes (together with a number label) on the image. It also returns the bounding box coordinates.

    & **Geometry** &  &  &  \\  Model & Geometry & Maxflow & Isomorphism & Connectivity & Convexity & Parity & Winner ID \\   _Prior ILMs without visual Inputs_ & & & & & & \\  Gemini-Pro &  & 15.6 & 47.7 & 50.0 & 87.9 & 48.2 & 8.1 \\ Claude 3 OPUS &  & 56.3 & 50.0 & 82.0 & 93.0 & 77.6 & 74.4 \\ Mistral 8x7B  &  & 8.6 & 50.0 & 62.5 & 69.1 & 41.7 & 7.4 \\ LLaMA-2-70B  &  & 18.0 & 50.0 & 50.0 & 74.2 & 33.3 & 12.4 \\   _Latest multimodal ELMs + Visual Sketchpad_ & & & & & & \\  GPT-4 Turbo & 37.5 & 32.8 & 62.5 & 66.0 & 57.0 & 80.5 & 50.4 \\ + Sketchpad & 45.8 & 96.8 & 97.6 & 97.6 & 77.3 & 71.5 & 64.2 \\ +8.3 & +64.0 & +35.1 & +31.6 & +20.3 & -9.0 & +13.8 \\  GPT-4o & 62.5 & 25.0 & 50.8 & 96.1 & 87.2 & 84.4 & 61.1 \\ + Sketchpad & **66.7** & **66.3** & **65.3** & **98.4** & **94.9** & **94.7** & **64.6** \\ +4.2 & +41.3 & +14.5 & +2.3 & +7.7 & +10.3 & +3.5 \\   

Table 1: Accuracy scores on geometry problems, graph algorithms, mathematical functions, and game. Sketchpad yields large performance gains on most tasks and outperform all baselines.

**Segmentation.** This module takes an image as input and returns an image with colorful segmentation masks on it. Each mask also has a number label. We follow the implementation of SoM . The underlying segmentation models are SegmentAnything  and Semantic-SAM .

**Depth estimation.** This module takes an image as input and returns a depth map. The underlying model is DepthAnything .

**Visual search via sliding window.** This module mimics how humans search for small items on an image. It takes a text query as input and runs a sliding window over the image. The window size is 1/3 of the image size, and the step size is 2/9 of the image size (so an image will have \(4 4=16\) windows). It returns a sequence of image patches in which the query is detected.

**Other image manipulation modules.** Other modules include (1) **zoom-in and crop**, which takes an image and a bounding box as input and returns the image patch inside the box; (2) **Overlay images**, which takes two images and alpha values as input, and returns the overlayed image.

### Results

We experiment with the same multimodal LMs as in SS4 on complex visual reasoning tasks. We compare the performance with and without Sketchpad, as well as other notable multimodal LMs, including Gemini , Claude 3 , and the open-source LLaVA 1.5 , LLaVA-NeXT .

**Main results.** Table 2 shows the performance of our Sketchpad and baselines. Sketchpad consistently improves base model performance across all tasks. GPT-4o with Sketchpad sets the new state-of-the-art results on all tasks. Sketchpad is particularly effective on \(V^{*}\)Bench, yielding 18.5% accuracy improvement for GPT-4 Turbo and 14.3% improvement for GPT-4o, surpassing the previous state of the art SEAL  which used a visual search model specifically trained for this task. On BLINK tasks, Sketchpad on average yields 6.6% absolute accuracy gain for GPT-4 Turbo and 9.0% gain for GPT-4o. Interestingly, despite the fact that all modules in Sketchpad work on a single image, the LMs also get substantial improvement on multi-image tasks, including jigsaw puzzles, visual correspondence, and semantic correspondence. Finally, GPT-4o, the LM with stronger multimodal ability than GPT-4 Turbo, benefits more from Sketchpad. For example, on the relative depth task, GPT-4o gets 12.1% accuracy improvement, while GPT-4 Turbo only gets 2.4%, showing that GPT-4o is better at understanding the depth map Sketchpad generated. Overall, our experiments show that Sketchpad is an effective way to improve multimodal LMs' performance on visual reasoning tasks.

**How many times is each vision specialist used?** We count the number of times each vision specialist is used in each task, as shown in Figure 4. Here we choose the four tasks that achieve the largest improvement: \(V^{*}\)Bench, relative depth, spatial reasoning, and semantic correspondence.

Figure 3: Examples of Sketchpad applied to vision tasks. The figure shows actual outputs generated by Sketchpad. By contrast, the baseline GPT-4o model cannot answer these questions correctly. Note that for demonstration purposes, the “A” and “B” marks in (a) are different from the actual images in the experiments.

We observe that (1) **the use of vision specialist is task-dependent, and the two LMs analyzed utilize similar tools.** For example, for \(V^{*}\), which needs to locate small objects, the LMs mainly use detection, sliding window search, and zoom-in, similar to how people would search. For the relative depth task, both models rely on depth estimation. For spatial reasoning, the LMs use detection and segmentation to facilitate visual reasoning. (2) **GPT-do likes to use more tools.** GPT-4o uses the vision specialists more often than GPT-4 Turbo. Also, the two LMs behave differently for the semantic correspondence tasks. GPT-4o uses the segmentation module for \(40\%\) of the task instances, while GPT-4 Turbo uses the detection module for less than \(20\%\) of times, and rarely uses the segmentation module. This difference may explain the performance gap between the two LMs (58.3% v.s. 42.4%) on this task.

**Comparison with visual prompting and tool-use frameworks.** In Table 3, we compare Sketchpad with the visual prompting framework **SoM** and the LLM tool-use framework **Visprog**. Details of these methods can be found in SS2. For a fair comparison, we make the following adaptations: (1) we find that prompting LMs with SoM images can hurt performance, likely because the visual prompts confuse the model. To make a stronger baseline, we prompt the LM with both the original image and the SoM image (full prompt in SSC), which we refer as "SoM + orig." (2) We replace the LM and VQA modules in Visprog with the corresponding GPT-4 model. (3) Since baseline methods are developed on single-image tasks, we compare Sketchpad on such tasks. From Table 3, we can see that **Sketchpad is the only framework that yields consistent improvement on all tasks.** SoM can boost spatial reasoning ability, as the authors reported. However, it can hurt the

   Model & \(V^{*}\)Bench & MMVP & Depth & Spatial & Jigsaw & Vis. Corr. & Sem. Corr. \\    \\  LLaVA-1.5-7B  & 48.7 & - & 52.4 & 61.5 & 11.3 & 25.6 & 23.0 \\ LLaVA-1.5-13B  & - & 24.7 & 53.2 & 67.8 & 58.0 & 29.1 & 32.4 \\ LLaVA-NeXT-34B  & - & - & 67.7 & 74.8 & 54.7 & 30.8 & 23.7 \\ Claude 3 OPUS  & - & - & 47.6 & 58.0 & 32.7 & 36.6 & 25.2 \\ Gemini-Pro  & 48.2 & 40.7 & 40.3 & 74.8 & 57.3 & 42.4 & 26.6 \\ GPT-AV-preview  & 55.0 & 38.7 & 59.7 & 72.7 & 70.0 & 33.7 & 28.8 \\ Previous state of the art & 75.4  & 49.3  & 67.7  & 76.2  & 70.0  & 42.4  & 33.1  \\    \\  GPT-4 Turbo & 52.5 & 71.0 & 66.1 & 68.5 & 64.7 & 48.8 & 30.9 \\ + Sketchpad & 71.0 & 73.3 & 68.5 & 80.4 & 68.5 & 52.3 & 42.4 \\ +18.5 & +2.3 & +2.4 & +11.9 & +3.8 & +3.5 & +11.5 \\  GPT-4o & 66.0 & 85.3 & 71.8 & 72.0 & 64.0 & 73.3 & 48.6 \\ + Sketchpad & **80.3** & **86.3** & **83.9** & **81.1** & **70.7** & **80.8** & **58.3** \\  & +14.3 & +1.0 & +12.1 & +9.1 & +6.7 & +7.5 & +9.7 \\   

Table 2: Accuracy on complex visual reasoning tasks. **Sketchpad enhances both GPT-4 Turbo and GPT-4o performance, establishing new SOTA performance levels on all the tasks.**

Figure 4: Percentage of times GPT-4o and GPT-4 Turbo use a visual module in Sketchpad when solving \(V^{*}\)Bench, relative depth, spatial reasoning, and semantic correspondence tasks.

performance on other tasks, even in the "SoM + orig." setting. Visprog performs worse than the base LM on all the tasks. As prior work [21; 18] suggests, one possible reason is that the vision modules themselves have errors, and the error propagates when the modules are composed by a program.

## 6 Analysis and Discussion

Why does Sketchpad work?First, **vision is a versatile and informational interface that complements language**. Dense information like depth and segmentation cannot be described easily through language . In a broader perspective, humans have developed many visualization techniques that are direct, efficient, and informational. Sketchpad provides LMs the opportunity to use them. Second, in Sketchpad, multimodal LMs can **plan and reason based on the intermediate visual artifacts** they created. In contrast, in prior modular vision work [14; 38; 55], multimodal modules follow a predefined plan by either humans or code. Sketchpad is much more flexible and robust to errors. For example, suppose object detection makes an error. The LM can (in principle) find the error by viewing the bounding boxes, and change its following plans, but prior methods cannot. Third, as discussed next, **the plans of multimodal LMs are similar to human plans**, and therefore likely benefit from the fact that the underlying LMs have seen data with similar reasoning patterns.

Do LMs have the same plans as humans?We conduct a human study on all geometry problems and 10 problems on each vision task. On geometry, humans draw the same auxiliary line as GPT-4o 80% of the time. On vision, we show 2 human subjects the full plan of GPT-4o, which they rate is valid in 92.8% of instances. Most errors are caused by failures in the vision specialists (e.g., fail to detect an object) and mistakes in simple visual question answering, rather than planning.

Experiments on open-source models.Can sketches like diagrams, plots, and auxiliary lines facilitate existing open-source multimodal LMs? To answer this question, we conduct the experiments in Table 4. We use the visual artifacts generated in the last action of GPT-4o + Sketchpad experiment as the image input for open-source LLaVA-NEXT models . We can see that this oracle Sketchpad brings consistent improvement to math tasks and boosts mathematical reasoning.

## 7 Conclusion

We present Visual Sketchpad, a framework that provides multimodal LMs with the tools necessary to generate intermediate sketches to reason over tasks. For complex mathematical reasoning tasks, Sketchpad yields large performance gains, by visualizing auxiliary lines, math functions, graphs, and games during reasoning. For visual reasoning tasks, we add vision specialists to Sketchpad. The LM can call these specialists during reasoning, observing the visualization of these specialists' predictions (e.g., bounding boxes from the object detection model; masks from the segmentation model), and then conduct further planning and reasoning. Experiments show that Sketchpad enhances the LMs' performance across all tasks, and sets new state-of-the-art results. Ultimately, Sketchpad represents a step toward endowing LMs with more human-like multimodal intelligence, leveraging the complementary strengths of language and vision to tackle increasingly complex reasoning challenges.

Limitations and future directions.First, Sketchpad requires more computing resources than directly outputting language tokens. We discuss more about computing costs in E. Second, this work focuses on existing off-the-shelf LMs. Future work may explore the training side of Sketchpad. For example, recent multimodal models like Unified-IO 2  and Chameleon  are natively multimodal and can output both text and images. Sketchpad may emerge as a new paradigm for instruction tuning these models. Finally, Sketchpad can be applied in more areas. For example, for robotics, we can apply Sketchpad to search for small things in a crowded space, highlight the object of interest, and zoom the camera for a better view or use depth estimation to help navigation.

   Model & Geometry & Maxflow & Convexity & Winner ID \\  LLaVA-NeXT-13B & 11.1 & 7.8 & 50.39 & 5.8 \\  + oracle Sketchpad & 22.2 & 10.2 & 50.0 & 36.7 \\  LLaVA-NeXT-34B & 26.1 & 0.8 & 81.6 & 49.0 \\  + oracle Sketchpad & 28.3 & 14.1 & 87.1 & 49.4 \\   

Table 4: Open-source LLaVA models’ performance on math tasks. The oracle Sketchpad uses the visual artifact generated in the last action of GPT-4o + Sketchpad as inputs.