# Deep Stochastic Processes

via Functional Markov Transition Operators

 Jin Xu\({}^{1}\)  Emilien Dupont\({}^{1}\)  Kaspar Martens\({}^{2}\)  Tom Rainforth\({}^{1}\)  Yee Whye Teh\({}^{1}\)

\({}^{1}\) Department of Statistics, University of Oxford, UK.

\({}^{2}\) Big Data Institute, University of Oxford, UK.

Corresponding author: <jin.xu@stats.ox.ac.uk>ED is now at Google DeepMind; this work was done while ED was at Oxford.

###### Abstract

We introduce Markov Neural Processes (mnps), a new class of Stochastic Processes (sps) which are constructed by stacking sequences of neural parameterised Markov transition operators in function space. We prove that these Markov transition operators can preserve the exchangeability and consistency of sps. Therefore, the proposed iterative construction adds substantial flexibility and expressivity to the original framework of Neural Processes (nps) without compromising consistency or adding restrictions. Our experiments demonstrate clear advantages of mnps over baseline models on a variety of tasks.

## 1 Introduction

Stochastic Processes (sps) are widely used in many scientific disciplines, including biology , chemistry , neuroscience , physics  and control theory . They are formed by a (typically infinite) collection of random variables and can be used to model data by considering the conditional distribution of target variables given observed context variables. In machine learning, sps in the form of Bayesian nonparametric models--such as Gaussian Processes (gps)  and Dirichlet processes --are used in tasks such as regression, classification, and clustering. sps parameterised by neural networks have also been used for meta-learning  and generative modelling .

With the increasing amount of data available, and the complex patterns arising in many applications, more flexible and scalable sp models with greater learning capacity are required. The Neural Process (np) family  meets this demand by parameterising sps with neural networks, and enjoys greater flexibility and computational efficiency compared to traditional nonparametric models.

Unfortunately, the original version of nps lacks expressivity and often underfits the data in practice . Various extensions have therefore been proposed--such as Attentive Neural Processes (anps) , Convolutional Neural Processes (convnps)  and Gaussian Neural Processes (gnps) --to improve expressivity. However, these models--which we refer to as _predictive_sps--are based around directly constructing mappings from contexts to predictive distributions, and therefore forgo the fully generative nature of the original np formulation. This can be problematic as it means that they are no longer _consistent_ under conditioning: their predictive distributions no longer correspond to the conditional distribution of an underlying sp prior. In turn, this can cause a variety of issues, such as conflicts or inconsistencies between predictions and miscalibrated uncertainties.

To address these issues, we propose an alternative mechanism to extend the np family and provide increased expressivity while maintaining their original fully generative nature and consistency. Webegin by generalising the marginal density functions of nps. The generalised form can be viewed as Markov transition operators with functions as states. This lays the foundation for stacking these operators to construct more powerful generative sp models that we call Markov Neural Processes (mnps). mnps can be seen as Markov chains in _function_ space, parameterised by neural networks: they make iterative transformations from simple initial sps into more flexible, yet properly defined, sps (as illustrated in Figure 1), without compromising consistency or introducing additional assumptions.

To empirically demonstrate the value of our proposed approach, we first benchmark mnps against baselines on 1D function regression tasks, and conduct ablation studies in this controlled setting. We then show that they can be used as a high-performing surrogate for contextual bandit problems. Finally, we apply them to geological data, for which they demonstrate encouraging performance.

## 2 Background

A sp is a (typically infinite) collection of random variables defined on a common probability space. We can consider a sp as a random function \(F:\) where inputs can be regarded as indexing the output random variables. With a relaxed use of notation, we employ \(p(f)\) in denoting a sp, where \(f\) maps inputs \(x\) to outputs \(y\). Kolmogorov's Consistency Theorem shows that a sp can be _indirectly_ defined via a collection of marginal distributions, \(\{p_{x_{1:n}}(y_{1:n})\}_{x_{1:n}^{n}}\) (we drop \(^{n}\) from here on for conciseness) if they, for any permutation \(\) and all possible sets of inputs \(x_{1:n}^{n}\), satisfy the _exchangeability_ condition:

\[p_{x_{1:n}}(y_{1:n})=p_{(x_{1:n})}((y_{1:n})):=\ p_{x_{(1)},,x_ {(n)}}(y_{(1)},,y_{(n)}),\] (1)

and the _(marginal) consistency_ condition:

\[p_{x_{1:m}}(y_{1:m})= p_{x_{1:n}}(y_{1:n})\,y_{m+1:n}\ \ \ \  1 m<n.\] (2)

If none of the random variables in a sp are observed, we call it a _prior_ sp. For any two distinct subsets of datapoints \(=\{(x_{i},y_{i})\}_{i=1}^{m}\) (the context) and \(=\{(x_{i},y_{i})\}_{i=m+1}^{n}\) (the target), one can use this prior sp to compute the conditional density \(p_{x_{m+1:n}|x_{1:m}}(y_{m+1:n}|y_{1:m})\) via Bayesian inference. If inference is exact, it can be proved that the collection of conditional distributions \(\{p_{x_{m+1:n}|x_{1:n}}(y_{m+1:n}|y_{1:m})\}_{x_{m+1:n}}\) also satisfy exchangeability and consistency, and hence define a valid _posterior_ sp, \(p(f|)\). We discuss the impact of approximate inference in Section 4.3.

Based on a conditional version of de Finetti's Theorem, a np defines a sp by providing a collection of exchangeable and consistent marginal densities parameterized by neural networks. Specifically, they set up their marginal densities to have the form:

\[p_{x_{1:n}}(y_{1:n};)= p_{}(z)_{i=1}^{n}(y_{i}|_{}(x_{i},z),_{}^{2}(x_{i},z))z,\] (3)

where \(z\) is a latent variable which captures dependencies across different input locations, \(y_{i}:=f(x_{i})\), \(_{}\) and \(_{}\) are deep neural networks, and \(p_{}(z)\) is a, typically Gaussian, prior distribution on the latents. Note that this form is more general than the one in  where both \(p_{}\) and \(_{}\) are not learnt.

Figure 1: **mnps construct expressive sps via iterative transformations. Here we use mnps to model distributions over monotonic functions. We start with trivial initial sps and gradually transform them into more complex sps conditioned on observed context points marked as black crosses. The blue lines represent sampled mean functions and the shaded region indicates one standard deviation.**

## 3 Generative versus predictive stochastic process models

Before introducing our mnp approach, we first delve into the distinctions between conventional, consistent, sp models like nps, and the predictive sp models corresponding to popular np extensions, such as anps, convnps, and gnps, highlighting some of the potential drawbacks with the latter.

We introduce the term _generative_ sp model to refer to the standard case where one first specifies a prior sp, \(p(f)\), and then relies on Bayesian inference to compute posterior a sp, \(p(f|)\). The context and the target are only distinguished for inference, and they are treated equally in model specification. Traditional nonparametric models such as gps, Student-t processes and the original nps belong to this category. Since posterior sp are inferred under the same prior sp, under the condition of exact inference, for two different contexts \(=\{(x_{i},y_{i})\}_{i}\) and \(^{}=\{(x_{i},y_{i})\}_{i^{}}\), we have

\[ p(f|)p_{xc}(y_{})y_{}= p(f| ^{})p_{x_{^{}}}(y_{^{}}) y_{^{}},\] (4)

where \(x_{}:=\{x_{i}\}_{i}\) and \(x_{^{}}:=\{x_{i}\}_{i^{}}\) are taken as fixed, and \(p_{x_{}}(y_{})\) and \(p_{x_{^{}}}(y_{^{}})\) are defined through the prior sp\(p(f)\). We call this property _conditional consistency_ to set apart from marginal consistency defined in Equation (2). Note that conditional consistency is a prerequisite for marginal consistency to hold for the prior \(p(f)\): if the two integrals in Equation (4) are not equal, at least one must be different to the prior distribution \(p(f)\).

By contrast, predictive sp models directly construct mappings from a context \(\) to a predictive sp\(p(f;)\) and make a clear distinction between the context and the target in model specification. Consequently, for these models \(p(f;)\) no longer corresponds to a posterior derived from a certain prior \(p(f)\), so they no longer need to satisfy conditional consistency. As such, they _no longer form a valid conditioning of a_ sp: though they are typically constructed to ensure \(p(f;)\) is itself a valid sp for new evaluations of \(f\) with \(\) held fixed, this sp is no longer itself derived as the conditional of a prior sp. As such, predictive sp models are not consistent under updating and no longer update their uncertainties in a Bayesian manner as new data is observed. In short, they are _no longer treating the data itself as being drawn from an_ sp.

Though predictive sp models have proven to be effective tools for meta-learning tasks, such as 1-D regression, image regression, and few-shot image classification [4; 18; 30; 60], this lack of consistent updating can be problematic in scenarios where the context is not fixed, such as when performing sequential updating. In particular, their uncertainties will be mismatched with how the model is updated in practice as new data is observed. This has the potential to be problematic for a wide variety of possible applications involving sequential decision-making, such as Bayesian experimental design [7; 46], active learning [2; 27; 51], Bayesian optimization [19; 23], and contextual bandit problems [35; 50]. There is also the potential for such models to fall foul of so-called Dutch Book scenarios , wherein the inconsistencies of the model can lead to conflicting predictions.

## 4 Markov Neural Processes

To correct the shortfalls of nps while maintaining their conditional consistency, we now introduce a more expressive family of generative sp models termed Markov Neural Processes (mnps). Our starting point is to extend np density functions into a generalised form that can be viewed as a transition operator which transforms a trivial sp to a more flexible one. mnps are then formed by stacking sequences of these transition operators to form a highly expressive and flexible model class. The training and inference procedure for mnp mirrors that of np, but we also introduce a novel inference model that allows for efficient learning in our scenario.

### A more general form of Neural Process density functions

Recall the form of the np marginal density functions from Equation (3). One can draw joint samples from \(p_{x_{1:n}}(y_{1:n};)\) via reparameterization using:

\[y^{(0)}_{1:n}(,), z p_{}(z), y_{i}=_{}(z,x_{i}) y^{(0)}_{i}+_{}(z,x_{i}).\] (5)

The key starting point for mnps is to show that this can be generalised to the case where \(y^{(0)}_{1:n}\) is drawn from any given sp of its own, \(p(f^{(0)})\), and each \(y_{i}\) is any invertible transformation, \(F_{}\), of\(y_{i}^{(0)}\), parameterized by \(x_{i}\) and \(z\). Specifically, we introduce the following result (see Appendix A for proof).

**Proposition 4.1**.: _Let \(F_{}(;x,z):\) denote an invertible transformation between outputs, parameterized by the input and latent. If \(\{p_{x_{1:n}}(y_{1:n}^{(0)})\}_{x_{1:n}}\) is a valid sp (i.e. it satisfies Equation (1) and Equation (2)) and_

\[y_{1:n}^{(0)}\{p_{x_{1:n}}(y_{1:n}^{(0)})\}_{x_{1:n}}, z p_{ }(z), y_{i}=F_{}(y_{i}^{(0)};x_{i},z).\] (6)

_then \(\{p_{x_{1:n}}(y_{1:n})\}_{x_{1:n}}\) is also a valid sp._

Our next step is to realise that Equation (6) can be viewed as a Markov transition in function space, denoted as \(p(f|f^{(0)})\), which transforms a simpler sp\(p(f^{(0)})\) to a more expressive one \(p(f)\). We can thus generalise things further by stacking sequences of Markov transition operators in function spaces (rMTOs), denoted by \(p(f^{(1)}|f^{(0)})\),..., \(p(f^{(T)}|f^{(T-1)})\), together to form a Markov chain \(f^{(0)} f^{(T)}\) in function space. This will be the basis of mnps.

### Markov chains in function space

Analogously to defining a sps through its marginals, we indirectly specify rMTOs using a collection of marginal Markov transition operators (mMTOs), denoted by \(\{p_{x_{1:n}}(y_{1:n}|y_{1:n}^{(0)})\}_{x_{1:n}}\), where each mMTO is just Markov transition operator over a finite sequence of function outputs.

To adapt the definitions of consistency and exchangeability for sps to the transition operator setting, we say that the mMTOs are consistent if and only if, for any \(1<m<n\) and sequence \(x_{1:n}^{n}\),

\[ p_{x_{1:n}}(y_{1:n}|y_{1:n}^{(0)})\,y_{m+1:n}=p_{x_{1:m}}(y_{1:m }|y_{1:n}^{(0)})=p_{x_{1:m}}(y_{1:m}|y_{1:m}^{(0)}).\] (7)

Similarly, mMTOs are exchangeable if, and only if, for all possible permutations \(\),

\[p_{x_{1:n}}(y_{1:n}|y_{1:n}^{(0)})=p_{(x_{1:n})}((y_{1:n})|(y_{1:n}^{ (0)})).\] (8)

Note that, if we consider \((x_{i},y_{i}^{(0)})\) as inputs, and \(y_{i}\) as function outputs, the transition operator \(p_{x_{1:n}}(y_{1:n}|y_{1:n}^{(0)})\) can be seen as the finite marginals of a random function \(F^{}:\) whose distribution is a sp, and the conditions (7) and (8) correspond to (2) and (1).

Provided that these mMTOs are consistent and exchangeable, the rMTOs in function space will also be well-defined indirectly--i.e. the transition produces a well-defined sp given input random functions from a sp--as per the following result (see Appendix A for proof):

**Proposition 4.2**.: _If the collection of marginals before transition \(\{p_{x_{1:n}}(y_{1:n}^{(0)})\}_{x_{1:n}}\) is consistent and exchangeable (as per Equations (1) and (2)) and the collection of mMTOs \(\{p_{x_{1:n}}(y_{1:n}|y_{1:n}^{(0)})\}_{x_{1:n}}\) is also consistent and exchangeable (as per Equations (7) and (8)), then the collection of marginals after transition \(\{p_{x_{1:n}}(y_{1:n})\}\) is also consistent and exchangeable, hence defining a valid sp._

Furthermore, a Markov chain in function space can be constructed by a sequence of fMTOs \(p(f^{(1)}|f^{(0)})\),..., \(p(f^{(T)}|f^{(T-1)})\) where \(p(f^{(t)}|f^{(t-1)})\{p_{x_{1:n}}(y_{1:n}^{(t)}|y_{1:n}^{(t-1)})\}_{x_ {1:n}}\). With repeated applications of Proposition 4.2, if the initial state \(\{p_{x_{1:n}}(y_{1:n}^{(0)})\}_{x_{1:n}}\) is exchangeable and consistent, \(\{p_{x_{1:n}}(y_{1:n}^{(T)})\}_{x_{1:n}}\) at time \(T\) is also exchangeable and consistent, hence defining a sp\(p(f^{(T)})\).

Equation (6) then provides a valid construction of consistent and exchangeable mMTOs by introducing an auxiliary latent variable \(z\). The transition operator is written as

\[p_{x_{1:n}}(y_{1:n}|y_{1:n}^{(0)};)= p_{}(z)_{i=1}^{n} (y_{i}-F_{}(y_{i}^{(0)};x_{i},z))\,z,\] (9)

where both \(p_{}\) and \(F_{}\) in Equation (9) can be parameterised by neural networks. Critically, we can extend Proposition 4.2 to cover these auxiliary settings in Equation (9) as well, as per the following result (see Appendix A for proof):

**Proposition 4.3**.: mMTOs _in the form of Equation (9) are consistent and exchangeable._

### Parameterisation, inference and training

We can now define a mnp as a sequence of neural fMTOs, with each fMTO indirectly specified via a collection of mMTOs in the form of Equation (9). If we specify a distribution over the sequence of auxiliary latent variables \(p_{}(z^{(1:T)})\) along with an initial sp with marginals \(p_{x_{1:n}}(y_{1:n}^{(0)})\), we can write down the marginal distribution over function outputs \(y_{1:n}:=y_{1:n}^{(T)}\) for a sequence of inputs \(x_{1:n}\) under the mnp model as (see Figure 1(a) for illustration and Appendix A for derivation):

\[p_{x_{1:n}}(y_{1:n};) = p_{}(z^{(1:T)})p_{x_{1:n}}(y_{1:n}^{(0)})_{t=1}^{ T}_{i=1}^{n}p_{}^{(t)}(y_{i}^{(t)}\,|\,y_{i}^{(t-1)},x_{i},z^{(t)}) y_{1:n}^{(0:T-1)}z^{(1:T)}\] (10) \[= p_{}(z^{(1:T)})p_{x_{1:n}}(y_{1:n}^{(0)})_{t=1}^ {T}_{i=1}^{n}^{(t)}(y_{i}^{(t- 1)};x_{i},z^{(t)})}{ y_{i}^{(t-1)}}z^{(1:T)}\]

According to Propositions 4.2 and 4.3, \(\{p_{x_{1:n}}(y_{1:n};)\}_{x_{1:n}}\) defines a valid sp parameterised by \(\). The initial sp can be arbitrarily chosen, as long as we can evaluate its marginals \(p_{x_{1:n}}(y_{1:n}^{(0)})\). In our experiments, we use a trivial sp where all the outputs are i.i.d. standard normal distributed. We adopt normalising flows [16; 43; 49] to parameterise the invertible transformations \(F_{}^{(t)}\).

To integrate over latent variables \(z^{(1:T)}\) in Equation (10), we introduce a posterior approximation \(q_{x_{1:n}}(z^{(1:T)}|y_{1:n};)\). For many applications, we need to learn and query the conditional distributions of a target \(=\{(x_{i},y_{i})\}_{i=m+1}^{n}\) given context \(=\{(x_{i},y_{i})\}_{i=1}^{m}\). To better reflect the desired model behaviour at test time, similar to nps, we train the model by maximising the following approximation of the conditional log-likelihood w.r.t. both model \(\) and variational \(\) parameters:

\[_{,}(y_{1:n};x_{1:n}):=\] (11) \[_{q_{x_{1:n}}(z^{(1:T)}\,|\,y_{1:n};)}[ p _{x_{m+1:n}}(y_{m+1:n}\,|\,z^{(1:T)},)+}(z^{(1:T)}\, |\,y_{1:m};)}{q_{x_{1:n}}(z^{(1:T)}\,|\,y_{1:n};)}]\]

where \( p_{x_{m+1:n}}|_{x_{1:m}}(y_{m+1:n}\,|\,y_{1:m};)_{, }(y_{1:n};x_{1:n})\). Here \(q_{x_{1:m}}(z^{(1:T)}\,|\,y_{1:m};)\) can be seen as an approximate prior conditioned on the context \(\{(x_{i},y_{i})\}_{i=1}^{m}\), while \(q_{x_{1:n}}(z^{(1:T)}\,|\,y_{1:n};)\) is the approximate posterior after the target \(\{(x_{i},y_{i})\}_{i=m+1}^{n}\) is observed. Thus the prior and the posterior share the same inference model with parameters \(\). The training objective is optimised by stochastic gradient descent, and the gradients \(_{,}_{,}(y_{1:n};x_{1:n})\) can be efficiently estimated with low variance using the reparameterisation trick .

However, different from nps, the latent variables \(z^{(1:T)}\) for mnps are a sequence. Therefore, we propose a different inference model which shares parameters with the generative model above. Firstly, we write \(q_{x_{1:m}}(z^{(1:T)}\,|\,y_{1:m};)\) in a factorised form:

\[q_{x_{1:n}}(z^{(1:T)}\,|\,y_{1:n};)\!=\!_{t=1}^{T}q_{x_{1:n}}^{(t)}(z^ {(t)}|z^{(t+1)},y_{1:n}^{(t)};)\] (12)

where \(\) are variational parameters and we set \(z^{(T+1)}=\), \(y_{1:n}=y_{1:n}^{(T)}\). In our implementation, we use a Gaussian distribution for each factor \(q_{x_{1:n}}^{(t)}\), with mean \(_{}^{(t)}(z^{(t+1)},y_{1:n}^{(t)},x_{1:n})\) and variance \(_{}^{(t)}(z^{(t+1)},y_{1:n}^{(t)},x_{1:n}))\), Here \(_{}^{(t)}\) and \(_{}^{(t)}\) are parameterised functions invariant to the permutation of data points \(\{(x_{i},y_{i})\}_{i=1}^{n}\), i.e.

\[_{}^{(t)}(z^{(t+1)},y_{1:n}^{(t)},x_{1:n})=_{}^{(t)}(z^{(t+1)},y_ {(1:n)}^{(t)},x_{(1:n)})\]

\[_{}^{(t)}(z^{(t+1)},y_{1:n}^{(t)},x_{1:n})=_{}^{(t)}(z^{(t+ 1)},y_{(1:n)}^{(t)},x_{(1:n)}).\]

We can parameterise them by first having

\[r^{(t)}=(y_{1:n}^{(t)},x_{1:n}))\] (13)

where SetEncoder could be a Set Transformer , Deep Sets , then taking

\[_{}^{(t)}(z^{(t+1)},y_{1:n}^{(t)},x_{1:n})=_{}(z^{(t+1)},r^{ (t)})\] (14)

\[_{}^{(t)}(z^{(t+1)},y_{1:n}^{(t)},x_{1:n})=_{}(z^{(t+1)},r^ {(t)}).\] (15)Figure 2: (a) **Consistent generative models on finite sets**. Observed variables are shown in white, and hidden variables are shaded. Stochastic paths are indicated with dashed lines while deterministic paths are solid lines. Conditioned on the function inputs \(x_{1:n}\) and the auxiliary latent variables \(z^{(1:T)},\) the function outputs are transformed independently using instance-wise conditional normalising flows (CNFs). If the initial states \(\{(x_{i},y_{i})\}_{i=1:n}\) come from a sp, the construction will ensure that the collection of marginals \(p(y^{(t)}_{1:n}|x_{1:n})\) are consistent under marginalisation for any \(t\). (b) **Permutation-invariant inference models**. Auxiliary latent variables \(z^{(1:T)}\) are inferred in reverse order in our inference model. We reuse the parameters of CNFs in the generative model to compute function values \(y^{t}_{1:n}\) at all time steps, where each step has a different colour. We parameterise the conditional distribution \(q^{(t)}_{x_{1:n}}(z^{(t)}|z^{(t+1)},y^{(t)}_{1:n};)\) with permutation-invariant neural networks.

In Equation (12), the intermediate function outputs \(y_{1:n}^{(1:T-1)}\) are not observed. However, we can sample them autoregressively by iterating between sampling \(z^{t+1}\) and calculating \(y_{i}^{(t)}=(F_{}^{(t+1)})^{-1}(y_{i}^{(t+1)};x_{i},z^{(t+1)})\) for each \(i\) (see Figure 2b). Therefore, we share the parameters of the normalising flows \(F_{}^{(1:T)}\) between the generative and inference models, leading to better performance. This idea was originally proposed by Cornish et al. , except that our normalising flows are applied to a sequence of function outputs \(y_{1:n}\) given the inputs \(x_{1:n}\).

In practice, the inference model \(q_{x_{1:n}}(z^{(1:T)} y_{1:n};)\) provides an approximate prior/posterior. Ideally, if it gives the exact posterior, conditional consistency would hold perfectly. However, when the inference model is approximate, the degree of conditional consistency depends on the discrepancy between the inference model and the true posterior. Predictive sp models also have a stochastic mapping conditioned on the context, represented as \(q_{x_{1:m}}(z^{(1:T)}|y_{1:m};)\). However, it is important not to confuse this with the inference model, as they do not serve to approximate the posterior.

## 5 Related work

Bayesian nonparametric modelsBayesian nonparametric models such as gps[24; 48] and Student-t processes [5; 10; 52] provide common classical approaches for modelling distributions over functions. Under these models, any conditional distribution of a target given a context can be directly evaluated, and both marginal/conditional consistency is guaranteed by construction (if all computation is exact). However, they can be too restrictive for some applications, e.g. any conditional or marginal distribution of a gp is also Gaussian. Further, the evaluation of conditional densities is typically computationally intensive (cubic w.r.t. the context due to matrix inversion). Deep gps[13; 59] use gps as building blocks to construct deep architectures, designed to be flexible enough to model a wide range of complex systems. However, only the hyperparameters for each GP layer are tunable, which means they can still be restrictive when modelling highly non-Gaussian data.

**Copula-based processes** In multivariate statistics, a copula function refers to a multivariate function that describes the dependence between random variables and links the marginal distributions of each variable to the joint distribution of all the variables [28; 41]. Similarly, a copula process  describes the dependency between infinitely many random variables independent of their marginal distributions. [32; 33; 38] exploited this independence to specify the more flexible Copula-Based Processes (cBps) by combing the copula processes of existing sps with flexible models of marginal distributions based on normalising flows [16; 43; 49]; the consistency of cBps is guaranteed as long as the base sps are consistent. However, cBps are still restrictive in terms of modelling relationship between variables because the underlying copula processes still come from known sps. For example, BRUNOs [32; 33] have the same underlying copula processes as gps, so they cannot be used to model data with non-Gaussian dependencies.

**Neural process family** nps are generative sp models which specify a prior sp and rely on Bayesian inference to compute conditional densities. To improve expressivity of the original nps, [4; 18; 30] explore predictive sp models that directly learn mappings from context to predictive sps. More specifically, anps incorporate cross-attention modules to model the interaction between context and target. convnps produce a functional representation with a convolutional architecture which parameterizes a stationary predictive SP over the target, given the context. In gps, predictive sps are modelled by gps where the mean/kernel functions are directly produced by neural networks conditioned on the context. However, simply setting the context to an empty set in these models does not yield more expressive generative sp models. In the absence of context, anps and gps become standard nps and gps respectively, without any additional expressivity. convnps can indeed be adjusted for generative sp models where the latent variables are _random functions_. However, it remains unclear how to perform Bayesian inference in function space . Conditional np families, e.g. Conditional Neural Processes (cnps), Convolutional Conditional Neural Processes (convcnps) are another category of predictive sp models, which make a strong assumption that all targets are independent given the context. Recently, Transformer Neural Processes (tnps), Neural Diffusion Processes (ndps) provide an alternative to the np models above, showing promising predictive performance. However, both marginal and conditional consistency are sacrificed in these models.

## 6 Experiments

Our experiments aim to answer the questions: 1) Can the proposed framework of mnps offer better expressivity than nps? 2) Compared to Bayesian nonparametric models, do neural parameterised sps have advantages, especially on highly non-Gaussian data? 3) How well do mnps perform on scientific problems? All experiments are performed using PyTorch . For details about datasets, please see Appendix B.1. For additional experimental details such as hyperparameters and architectures, please refer to Appendix B.2 and our reference implementation at https://github.com/jinxu06/mnp.

### 1D function regression

We first consider 1D function regression problems to perform controlled comparisons between gps, cbps, nps, and mnps. Table 1 shows results across a range of datasets, including samples from the GP prior (we consider three different kernels) as well as more challenging non-GP data.

For datasets sampled from gps, we also include the performance of oracle gps with the right hyperparameters for generating the data. As shown, gps and cbps are close to the oracle except for samples generated using a periodic kernel, where cbps struggle to learn the right kernel hyperparameters due to the difficulty of optimisation. For neural parameterised models, the performance gaps between the mnps and the oracle gps are much smaller than for nps.

For non-GP samples such as monotonic functions, convex functions and samples from certain SDEs, mnps perform the best across all models. cbps obtain better marginal log-likelihood than gps because the pointwise marginals are transformed using normalising flows, but their underlying copula processes (which capture the dependence between data points) are still Gaussian, restricting their ability to model highly non-Gaussian data compared to neural parameterised models e.g. mnps, nps.

### Contextual bandits

Contextual bandits are a class of problems where agents repeatedly choose from a set of actions based on context and receive rewards. The goal is then to learn a policy that maximizes expected cumulative reward over time. Contextual bandits find applications in real-time decision making problems such as resource allocation and online advertising.

    &  &  \\ Model & RBF Kernel &  &  &  &  &  \\  Oracle & \(2.846 0.012\) & \(2.709 0.013\) & \(0.641 0.006\) & — & — & — \\ GPs & \(\) & \(\) & \(0.419 0.013\) & \(0.633 0.059\) & \(2.976 0.224\) & \(1.719 0.034\) \\ CBPs & \(2.628 0.016\) & \(2.604 0.015\) & \(0.169 0.022\) & \(1.776 0.088\) & \(4.268 0.035\) & \(1.842 0.024\) \\ NPs & \(0.935 0.019\) & \(1.115 0.021\) & \(0.356 0.020\) & \(1.823 0.006\) & \(1.956 0.004\) & \(1.621 0.009\) \\ MNPs & \(2.491 0.024\) & \(2.290 0.021\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: **Estimated marginal log-likelihood of sps on 1D function regression problems**. We compare (a) Oracle models (when available). (b) gps with optimised hyperparameters for additive kernels with three component kernels including an RBF kernel, a Matern kernel and a periodic kernel. (c) cbps which combine Gaussian copula processes with neural spline flows . (d) nps. (e) mnps with \(7\) transition steps. Each experiment is repeated \(5\) times and we report the mean and standard errors of the marginal log-likelihood normalised by the number of points in the target. When latent models such as mnps, nps are used, we obtain estimations of marginal log-likelihoods on test data using the IWAE objective  with \(20\) latent samples.

Figure 3: Wheel contextual bandits with varying exploration parameter \(\). The optimal actions are \(a_{1},,a_{5}\) when the context point are in blue, yellow, red, green and black regions respectively.

Similarly to Garnelo et al. , we use the wheel bandit problem  (see Appendix B.1) to evaluate our approach: a unit circle is partitioned into 5 regions (see Figure 3) whose sizes are controlled by an exploration parameter \(\). There are 5 actions \(a_{1},a_{2},a_{3},a_{4},a_{5}\), and their associated reward depend on a 2D contextual coordinate \(X=(X_{1},X_{2})\) uniformly sampled from within the circle. If \(||X||\), \(a_{1}\) is the optimal action with reward sampled from \((1.2,0.01^{2})\), and taking any other action would yield a reward \(r(1.0,0.01^{2})\). If \(||X||>\), the optimal action depends on which of the remaining \(4\) region \(X\) falls into and choosing it would yield a reward \(r(50,0.01^{2})\). Under this circumstance, any other action yield a reward \((1.0,0.01^{2})\) except that \(a_{1}\) receives \(r(1.2,0.01^{2})\). We follow the experimental setup from Garnelo et al.  and only include models with a pre-training procedure (see Appendix B.2 for details). As can be seen in Table 2, mnps significantly outperform baselines (taken from the results of ) across different exploration rates \(\).

### Geological inference

In geostatistics, one is often interested in inferring the geological structure of an area given only a sparse set of measurements. This problem has traditionally been tackled with variants of gp regression

   \(\) & **0.5** & **0.7** & **0.9** & **0.95** & **0.99** \\   \\ Uniform & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) \\ MAML & \(2.95_{ 0.12}\) & \(3.11_{ 0.16}\) & \(4.84_{ 0.22}\) & \(7.01_{ 0.33}\) & \(22.93_{ 1.57}\) \\ NPs & \(1.60_{ 0.06}\) & \(1.75_{ 0.05}\) & \(3.31_{ 0.10}\) & \(5.71_{ 0.24}\) & \(22.13_{ 1.23}\) \\ MNPs & \(_{ 0.00}\) & \(_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.05}\) \\   \\ Uniform & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) & \(100.0_{ 0.00}\) \\ MAML & \(2.49_{ 0.12}\) & \(3.00_{ 0.35}\) & \(4.75_{ 0.48}\) & \(7.10_{ 0.77}\) & \(22.89_{ 1.41}\) \\ NPs & \(_{ 0.06}\) & \(_{ 0.21}\) & \(2.90_{ 0.35}\) & \(5.45_{ 0.47}\) & \(21.45_{ 1.3}\) \\ C-BRUNO & \(1.32_{ 0.06}\) & \(1.43_{ 0.07}\) & \(3.44_{ 0.13}\) & \(6.17_{ 0.21}\) & \(21.52_{ 0.41}\) \\ MNPs & \(_{ 0.06}\) & \(_{ 0.08}\) & \(_{ 0.16}\) & \(_{ 0.16}\) & \(_{ 0.38}\) \\   

Table 2: **Results on wheel contextual bandit problems**. We use an increasing value of \(\), where more exploration is needed with higher \(\). We report the mean and standard deviation of both cumulative and simple regrets (a performance measure of the final policy, estimated by the mean cumulative regrets in the last \(500\) steps) over \(100\) trials. Results are normalised by the performance of a uniform agent which chooses actions with equal probability.

Figure 4: **Inferred geology conditioned on measurements**. Given a small number of physical measurements (red pixels indicate positive measurements while blue pixels indicate negative ones), we show the predictive mean (first row in each panel) and standard deviation (second row in each panel). Different columns correspond to different context sets. As can be seen, mnps make predictions that are consistent with the data while showing larger uncertainty when fewer context points are available (or when a prediction is made far from a context point).

 (often referred to as Kriging in this context). However, several geological patterns (e.g. fluvial patterns) are highly complex and cannot be properly captured by these methods. To address this, several geostatistical models use a single training image to extract geological patterns and match these to the measurements [53; 63]. However, these approaches generally fail to produce realistic patterns capturing the variability of real geology.

More recently, deep learning approaches have been applied to this problem. [8; 14; 62] train GANs on large geological datasets and use this as a prior for inferring geological structure given sparse measurements. However, the resulting models do not provide any meaningful uncertainty estimates, which are crucial for decision making in several applications. As MNPs provide uncertainty estimates they may therefore be a compelling approach for this problem.

To evaluate MNPs on this problem, we introduce the GeoFluvial dataset, containing more than 25k simulations of fluvial patterns as 128\(\)128 grayscale images. The dataset was generated using the open source meanderpy package, which itself simulates the geological patterns as sps (see Appendix B.1 for details). We train our model on a training set of 20k simulations and evaluate it on a test set of 5k simulations. We test our trained model on a variety of context point configurations, corresponding to geological measurements taken at various spatial locations. Specifically, we consider uniformly sampled measurements (at 20, 40, 80, and 160 locations) as well as scenarios where measurements are spatially restricted to a certain area (top, left, and in the center of the square). Quantitative results are shown in Table 3 and qualitative results in Figure 4. As can be seen, our model achieves better likelihoods than NPs on this problem for all context point configurations, while also generating patterns that match the geological context. Further, as can be seen in Figure 4, the uncertainty estimates of the model are consistent with expectations, i.e. the model is more uncertain far from measurement locations or when there is ambiguity in the direction of the fluvial pattern.

## 7 Discussion

Limitations and future workBecause we apply only a finite number of Markov transitions, the entire computational graph containing intermediate states is held in memory for backpropagation. To alleviate this, an interesting direction for future work would be to consider continuous-time Markov transitions, i.e. stochastic differential equations in function space, which require less memory by using adjoint methods [9; 37]. While Equation (9) provides a valid general construction of exchangeable and consistent MMTOs with latent variables, it may be feasible to investigate other forms of MMTOs that do not necessitate latent variables, thereby eliminating the need for variational approaches.

There are often inherent symmetries in data (e.g. translational symmetries for stationary sps) and it may be possible to improve empirical performance of mNPs using (group equivariant) convolutions that preserve certain symmetries [18; 25; 29]. These improvements are, however, orthogonal to our contributions and combining them is an exciting direction for future work.

ConclusionsWe have introduced Markov Neural Processes (mnps), a framework for constructing flexible neural generative sp models. mnps use neural-parameterized Markov transition operators on function spaces to gradually transform a simple initial sp into a flexible one. We proved that our proposed neural transitions preserve exchangeability and consistency necessary to define valid sps. Empirical studies demonstrate that we can obtain expressive models of sps with promising performance on contextual bandits and geological data.

    &  &  \\ \(||\) & \(N=20\) & \(40\) & \(80\) & \(160\) & Top & Left & Center \\  NPs & \(-0.35 0.30\) & \(-0.31 0.29\) & \(-0.28 0.27\) & \(-0.25 0.27\) & \(-39.63 304.08\) & \(-22.03 107.25\) & \(-1.18 4.20\) \\ MNPs & \( 0.31\) & \( 0.23\) & \( 0.18\) & \( 0.15\) & \( 0.59\) & \( 0.61\) & \( 0.39\) \\   

Table 3: **Test marginal log-likelihood on geology data**. Uniform context points are uniformly sampled from within the 2D square, while regional context includes the top/left half or the central square with half the size.