# EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views

Yuhang Yang\({}^{1}\), Wei Zhai\({}^{1}\), Chengfeng Wang\({}^{1}\), Chengjun Yu\({}^{1}\), Yang Cao\({}^{1,2}\), Zheng-Jun Zha\({}^{1}\)

\({}^{1}\) University of Science and Technology of China

\({}^{2}\) Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

https://yywhang.github.io/EgoChoir

###### Abstract

Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI. For the egocentric HOI, in addition to perceiving semantics _e.g._, "what" interaction is occurring, capturing "where" the interaction specifically manifests in 3D space is also crucial, which links the perception and operation. Existing methods primarily leverage observations of HOI to capture interaction regions from an exocentric view. However, incomplete observations of interacting parties in the egocentric view introduce ambiguity between visual observations and interaction contents, impairing their efficacy. From the egocentric view, humans integrate the visual cortex, cerebellum, and brain to internalize their intentions and interaction concepts of objects, allowing for the pre-formulation of interactions and making behaviors even when interaction regions are out of sight. In light of this, we propose harmonizing the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, jointly inferring 3D human contact and object affordance from egocentric videos. To achieve this, we present **EgoChoir**, which links object structures with interaction contexts inherent in appearance and head motion to reveal object affordance, further utilizing it to model human contact. Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios. Moreover, 3D contact and affordance are annotated for egocentric videos collected from EgoExo4D and GIMO to support the task. Extensive experiments on them demonstrate the effectiveness and superiority of EgoChoir.

Figure 1: EgoChoir takes egocentric frames and head motion from head-mounted devices, along with the 3D object, to capture 3D interaction regions, including human contact and object affordance. The human motion is just visualized for intuitive observation of contact, yet it is not utilized by EgoChoir.

## 1 Introduction

Human-object interaction (HOI) understanding aims to excavate co-occurrence relations and interaction attributes between humans and objects [91; 103]. For egocentric interactions, in addition to capturing interaction semantics like what the subject is doing or what the interacting object is [5; 25], knowing where the interaction specifically manifests in space _e.g._, human contact [9; 80] and object affordance [16; 24] is also crucial. The precise delineation of spatial regions constitutes a pivotal component in numerous applications, like interacting with the scene in embodied AI [17; 71], interaction modeling in graphics [28; 95], robotics manipulation [52; 115], and AR/VR .

Most existing methods isolate the human and object to estimate contact or affordance regions [9; 29; 63; 80; 96; 101], capturing one aspect of interaction regions but neglecting the synergistic nature of interaction regions between interacting parties . They delineate the region where objects should be operated without specifying the region of subjects intended for executing such operations, and vice versa. This oversight limits their efficacy in shaping final interactions. Some studies explore correlations between interacting parties to jointly estimate interaction regions for both the subject and object [34; 100; 102], in which observations of the interacting parties are quite crucial, whether appearances within exocentric visuals or compatible structures formed by geometries of the subject and object. However, the egocentric view possesses incomplete observations of interacting parties, for instance, when sitting on a chair or interacting with hands accompanied by head rotation, the interacting parties are only partially visible or even completely invisible. This leads to **ambiguity** between visual observations and interaction contents, which undermines the effectiveness of these methods, resulting in gaps when directly applied to egocentric scenarios.

Studies in cognitive science illustrate that humans make egocentric behaviors through coordination of the visual cortex, cerebellum, and brain to correlate visual observations, self-movement, and conceptual understanding, thereby revealing complementary interaction clues that link their embodiment and surroundings [1; 23; 66]. This motivates us to ponder: what clues could drive machines to capture effective interaction contexts and infer interaction regions from the egocentric view? Analogous to humans, in this paper, we propose harmonizing the visual appearance, head motion, and 3D object to infer 3D human contact and object affordance from egocentric videos (Fig. 1). Normally, objects are designed to fulfill certain human needs, the linkage between their functionalities and structures reveals their interaction concepts, implying the intention and interaction regions. When engaging in interactions with specific objects, visual observation synergistically changes with head movement, conveying the interaction intention . The subject intention and object concept formulate an interaction "body image" [72; 75], with it, the region humans intend to contact, and the region that objects afford for such interactions could be pre-envisioned during forming the interaction (Fig. 2). This guides the estimation of interaction regions even when interacting parties move out of sight, eliminating the ambiguity between egocentric visual observations and interaction contents.

To consolidate the above insight, we present EgoChoir, a novel framework that integrates the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, collaboratively capturing 3D interaction regions. EgoChoir first links the semantic functionality and structures of the object by correlating interaction contexts within the appearance and motion with object geometry, thus mining the object interaction concept. Specifically, the appearance and motion features are mapped into interaction clues, and the object geometric feature, along with a semantic token that represents the functionality, queries these clues to calculate the 3D affordance through a parallel cross-attention. With affordance, the appearance feature is taken to

Figure 2: The subject intention, conveyed through synergistic visual appearances and head movements, along with the object interaction concept revealed by its structure and functionality, pre-formulate an interaction body image, which enables interaction regions to be envisioned.

query complementary interaction clues from head motion and 3D affordance in parallel, excavating the subject intention and modeling the contact representation. Despite the framework being heuristic, egocentric interaction scenarios are quite distinct _e.g._, with hand or body, which leads to varying effects of multiple interaction clues on modeling interaction regions in different scenarios. To adapt to this variability, EgoChoir employs modulation tokens to adjust gradients of specific layers that map interaction clues in the parallel cross-attention, endowing the model to adopt appropriate interaction clues for robustly estimating interaction regions across various egocentric scenarios.

Furthermore, we collect egocentric videos including \(12\) types of interactions with \(18\) different objects, and over \(20K\) corresponding 3D object instances. 3D human contact and object affordance are also annotated for the collected data, which could serve as the first test bed for estimating 3D human-object interaction regions from egocentric videos. The key contributions are summarized as follows:

* We propose harmonizing the visual appearance, head motion, and 3D object to infer human contact and object affordance regions in 3D space from egocentric videos. It furnishes essential spatial representations for egocentric human-object interactions.
* We present EgoChoir, a framework that correlates complementary interaction clues to mine the object interaction concept and subject intention, thereby modeling the object affordance and human contact through parallel cross-attention with gradient modulation.
* We construct the dataset that contains paired egocentric interaction videos and 3D objects, as well as annotations of 3D human contact and object affordance. It serves as the first test bed for the task, extensive experiments on it demonstrate the effectiveness and superiority of EgoChoir.

## 2 Related Work

**Embodied Perception.** Embodied perception emphasizes actively understanding the surroundings and facilitates intelligent agents in learning and improving human-like skills through interactions . This involves perceiving various attributes of the scene, _e.g._, object functionality [18; 22; 58; 89; 98; 101; 109], scene semantics or geometry [15; 44; 36; 60; 61; 86], and sound [6; 7; 8; 21; 76]. Meanwhile, perceiving the embodied subject is also crucial, which involves anticipating the intention of the interacting subject [30; 83; 99] and the way to interact with objects [38; 82; 97; 115] or scene [28; 39; 50; 110]. These methods achieve significant progress in perceiving a certain side of the embodiment or surroundings. However, when embodied agents interact with their surroundings, the interaction manifests in both the interacting subject and the facing object. Capturing synergistic interaction between the interacting parties is crucial. EgoChoir aims to explore the synergy perception of interaction regions from egocentric videos, including human contact and object affordance.

**Egocentric Interaction Understanding.** So far, methods have made significant progress in several proxy tasks for understanding egocentric interactions, such as action recognition [33; 65; 77; 88], anticipation [53; 70; 93], moment query [41; 73], semantic affordance detection [47; 104; 106], and temporal localization [105; 107]. They endow machines to understand the semantic ("what") and temporal ("when") aspects of the interaction. Despite their importance in egocentric interaction understanding, the lack of spatial perception ("where") makes it challenging to form interactions in the physical world. Some methods explore grounding spatial interaction regions at the instance-level [2; 40; 114] or part-level [48; 49; 57], but only in 2D space, resulting in gaps when extrapolating to the real 3D environment. In contrast, EgoChoir captures the spatial aspect of egocentric interactions, and jointly estimates object affordance and human contact in 3D space.

**Perceiving Interaction Regions in 3D Space.** For 3D interaction regions, dense human contact  and 3D object affordance [16; 24] recently get much attention in the field. Methods estimate them typically follow two paradigms, one of which is to directly establish a mapping between geometries and semantics [28; 55; 56; 89; 96; 111], _e.g._, "sit" links the seat of chairs, as well as the buttocks and thighs of humans. This paradigm establishes category-level connections between semantics and geometric regions, but it possesses limited generalization to unseen categories. Another paradigm explores correlations between geometries and interaction contents in 2D visuals _e.g._, exocentric images [29; 62; 74; 80; 101; 102], taking correlations to guide the estimation. This endows the model to actively anticipate based on interaction contents, which generalizes better in unseen cases. However, incomplete observations in the egocentric view lead to ambiguous visual appearances for modeling the correlation, which affects their effectiveness. EgoChoir mitigates this influence by harmonizing multiple interaction clues that could provide effective interaction contexts.

## 3 Method

The pipeline of EgoChoir is shown in Fig. 3, including extracting modality-wise features (Sec. 3.2), modeling the object affordance and human contact (Sec. 3.3), and the gradient modulation that enables to adopt appropriate clues to estimate interaction regions across various scenarios (Sec. 3.4).

### Preliminaries

Given the inputs \(\{,,\}\), where \(^{T H W 3}\) indicates a video clip with \(T\) frames of size \(H W\), \(^{T 12}\) denotes the translation vectors and rotation matrixes of head poses. \(^{N 3}\) is an object point cloud with \(N\) points. The goal is to learn a model \(f\) that outputs temporal dense human contact \(_{c}^{T 6890 1}\), 3D object affordance \(_{a}^{N 1}\), along with an interaction category \(_{s}\), expressed as: \(_{c},_{a},_{s}=f(,,)\). \(6890\) is the number of SMPL  vertices.

### Modality-wise feature extraction

Employing video backbones that are pre-trained by specific tasks like action recognition or contrastive learning  on egocentric datasets [13; 26] is a candidate approach to encode \(\). However, we find that they tend to homogenize features across a sequence in our task (Sec. 4.3), this is detrimental to estimating temporally dynamic interaction regions. Thus, referring to HPS from videos [32; 35], EgoChoir adopts the paradigm that correlates per-frame features. Specifically, per-frame features are extracted through a pre-trained HRNet (\(f_{i}\)) , then, the joint space-time attention (\(f_{st}\)) is applied to establish temporal and spatial correlations among features, expressed as: \(_{}=f_{st}(f_{i}()),^{TH_{1}W_{1}  C}\), where \(H_{1},W_{1}\) are height and width, \(C\) is the feature dimension.

The relative change in head poses is a crucial clue for providing interaction contexts . Thus, the relative head pose difference between each frame and the first frame is calculated, including translation difference \(\) and rotation difference \(\). It could be formulated as: \(_{i}=t_{i}-t_{0},_{i}=R_{0}^{-1}R_{i}\),

where \(t_{0},t_{i}^{1 3}\) and \(R_{0},R_{i}^{3 3}\) indicate the head translations and rotations at the first frame and \(i\)-th frame, \(i[1,T]\). The calculated \(\) and \(\) are concatenated into the relative head motion \(}\). Despite calculating relative changes in head pose, a motion encoder capable of encoding the variation is still needed. EgoChoir achieves this by associating the feature discrepancy between encoded motion features with the discrepancy in visual appearance features . In detail, appearance features \(_{}^{j},_{}^{k}\) are extracted from two random frames in \(\) by the frozen \(f_{i}\), where \(j,k\) means \(j\)-th, \(k\)-th frame and \(j<k\). Then, the corresponding \(j\)-th, \(k\)-th head poses are selected from \(}\) and encoded by \(f_{}\) that is composed of MLP layers, obtaining motion features \(_{}^{j},_{}^{k}\). The \(f_{}\) is trained by minimizing KL divergences calculated by \(_{}^{j},_{}^{k}\) and \(_{}^{j},_{}^{k}\), the loss can be formulated as:

\[_{m}=||_{C}_{}^{i}log(+_{}^{i}}{+_{}^{j}})-_{ H_{1}W_{1}}_{C}_{}^{i}log(+_{ }^{i}}{+_{}^{j}})||_{2},\] (1)

where \(\) is a regularization constant. By constraining the distance between the visual discrepancies and motion discrepancies in feature space, the variation in appearance features moderately transmitter to

Figure 3: **Method. EgoChoir first employs modality-wise encoders to extract features, in which the motion encoder is pre-trained by minimizing the distance between visual disparity and motion disparity. Then, it takes them to excavate the object interaction concept and subject intention, modeling the affordance and contact through parallel cross-attention with gradient modulation.**motion features, allowing \(f_{}\) to extract motion features \(_{}^{T C}\) with variations and associate with appearances. The object geometric feature \(_{}^{N C}\) is extracted through the DGCNN . Each encoder is further fine-tuned during the optimization of affordance and contact estimation.

### Modeling object affordance and human contact

**Object interaction concept.** With the modality-wise features, EgoChoir correlates \(_{},_{}\) with \(_{}\) to link the object functionality and structure, revealing the object interaction concept and calculating the affordance feature through parallel cross-attention. In specific, a semantic token \(_{f}^{1 C}\) representing the functionality is concatenated with \(_{}\) as the query, while \(_{},_{}\) are used as two parallel key-value pairs. In the parallel cross-attention, \(_{},_{}\) are scaled by learnable modulation tokens \(_{v},_{m}^{C}\). This modulates gradients of mapping layers and enables the model to extract effective interaction contexts from appropriate interaction clues across various scenarios, which is clarified in Sec. 3.4. The cross-attention is employed to model correlations among the query and key-value pairs parallelly, expressed as: \(}_{a}=_{a}([_{f},_{} ],_{v}_{},_{m}_{} ),^{(N+1) C}\), where \(_{a}\) denotes the transformer with parallel cross-attention, shown in Fig. 3, the fusion is composed of concatenation and MLP layers, \(\) indicates the concatenation, "-" is the hadamard product. \(}_{a}\) is split into the affordance feature \(_{a}^{N C}\) and semantic feature of the functionality \(_{sf}^{1 C}\).

**Subject interaction intention.** As a manifestation of the object interaction concept, affordance implies the subject intention and assists in modeling intention semantics and human contact. With it, the \(_{}\) queries complementary interaction clues from the motion feature \(_{}\) and affordance feature \(_{a}\) to derive the subject intention, and calculate the human contact and intention semantic features. Analogous to the affordance extraction, it can be expressed as: \(}_{c}=_{c}([_{i},_{} +pe_{t}],_{o}_{a},_{m}(_{}+ pe_{t}))^{(TH_{i}W_{1}+1) C}\), where \(_{c}\) is similar with \(_{a}\), \(_{i}^{1 C}\) is a token that represents the intention semantics, \(pe_{t}^{T C}\) is a temporal position encoding which introduces temporal dynamics into human contact and it is expanded to \(^{TH_{i}W_{1} C}\). \(}_{c}\) is split into semantic feature of the intention \(_{si}^{1 C}\) and contact feature \(_{c}^{TH_{i}W_{1} C}\). Furthermore, to maintain the synergy between contact and affordance, \(_{c}\) is then mapped to key-value features, and \(_{a}\) queries the synergistic interaction regions from them through a cross-attention \(f_{ca}\).

**Decoder.** The semantics of functionality and intention are correlated, thus, the \(_{sf},_{si}\) are concatenated to the semantic feature \(_{s}\), then \(_{s}\) is decoded into the categorical logits \(_{s}^{n}\) through MLP layers, \(n\) is the number of interaction category. The affordance feature \(_{a}\) is decoded in the feature dimension and projected to object affordance \(_{a}^{N 1}\). For the \(_{c}\), in addition to decoding the feature dimension, the spatial dimension is mapped to the sequence of SMPL vertices. The human contact \(_{c}^{T 6890 1}\) is output through two shallow MLP layers that decode the feature and spatial dimension. The overall loss is formulated as: \(=_{a}+_{c}+_{s}\), where \(_{s}\) is a cross-entropy loss, it constrains synergistic interaction semantics of the human and object. \(_{a}\) and \(_{c}\) optimize the affordance and contact respectively, both are a focal loss  plus a dice loss .

### Gradient modulation

Egocentric interaction scenarios exhibit differences, _e.g._, with hands or body, which affect the effectiveness of distinct interaction clue features for extracting interaction contexts in the parallel cross-attention. Assuming sitting down or operating with hands in the egocentric view, the former hardly observes interaction regions, in which the variation of head motion is a more effective clue for extracting interaction contexts. In contrast, the latter has less head movement but can derive rich contexts from the object interaction concept and visual appearances. Vanilla cross-attention presents limitations for adapting to diverse egocentric interactions.

Our goal is to enable the model to adopt appropriate interaction clues for modeling interaction regions across various egocentric scenarios. Some methods [20; 67; 87] balance information from distinct modalities by calculating the discrepancy of a consistent output (_e.g._, category logits), they compute a scaling factor \(\) from logits output by different modalities and take the \(\) to modulate gradients in each modal branch, thereby adjusting the weight to balance each modality, it can be simplified as:

\[_{t+1}_{t}-} {_{t}},=((x_{1})}{f_{2}(x_{2})}),\] (2)

where \(\) is the optimize parameter, \(\) is the learning rate, \(\) is the loss function. \(x_{1},x_{2}\) represent inputs of distinct modalities, \(f_{1},f_{2}\) indicates layers and calculations to get the logits, and \(\) denotes

[MISSING_PAGE_FAIL:6]

 through MeshLab , corresponding to the human contact in exocentric frames. For data in Ego-Exo4D, we select the best exocentric perspective and initially annotate the contact for \(150\) video clips, the process is shown in Fig. 5 (a). In detail, we select frames with a stride of \(16\) to manually annotate the contact and the remaining frames are consistent with the adjacent annotated frames. Next, annotators check per-frame annotations and refine those with slight changes. Then, We fine-tune LEMON  through the annotated contact, the human body needed by LEMON is obtained by SMPLer-X . The remaining data is divided into groups for every \(200\) clips, and the fine-tuned model is used to predict human contact along with the manual refinement. Multiple rounds are conducted to obtain the final annotations. Please note that the annotations for each round are accumulated, and LEMON is fine-tuned each round, it takes exocentric frames as the input.

For data in GIMO, we first set a distance threshold  to calculate the contact between the human body and the 3D scene, the threshold is set to \(2cm\). However, we find that there is a deviation in the accuracy of human-scene alignment, which makes it hard to calculate all contacts using a unified threshold, shown in Fig. 5 (b). Besides, the scanned geometry cannot reflect deformation, which also affects the contact annotation. Therefore, we locate key frames of the interaction and visualize human bodies in the 3D scene for these frames, then manually refine the calculated contacts.

For 3D object affordance, shown in Fig. 5 (c), we annotate a high probability interaction region (red) and an adjacent propagable region (blue) on a 3D object, and calculate the 3D affordance annotation \(S\) through a symmetric normalized laplacian matrix , formulated as:

\[S=(I-(D^{-0.5}WD^{-0.5})^{-1})Y, W=0.5(A+A^{T}), A_{ij}=\{ \|_{i}-_{j}\|_{2}, _{j} NN_{k}(_{i})\\ 0,.\] (4)

where \(Y\{0,1\}\) is the one-hot label vector and \(1\) indicates positive label, \(\) is a hyper-parameter controlling decreasing speed, set to \(0.995\). \(A\) represents the adjacency matrix of sampled points in a KNN graph, \(W\) is the symmetric matrix and \(D\) is the degree matrix. \(v\) is the \(xyz\) spatial coordinate of the point in the red region and \(NN_{k}\) denotes the set of \(k\) nearest neighbors in the blue region.

**Metrics and baselines.** Referring to advanced work in estimating interaction regions , the object affordance is evaluated through AUC, aIOU, and SIM. Precision, Recall, F1, and geodesic errors (geo.) are used to evaluate human contact estimation. Since there is no existing method to estimate 3D human contact and object affordance from the egocentric view, the constructed dataset is utilized to retrain methods that estimate interaction regions based on observations for comparison, including DECO , LEMON , etc. Note: some methods require certain modifications to their raw frameworks, details of metrics and each comparison method are provided in the appendix.

### Experimental results

**Quantitative results.** Tab. 1 shows that our method outperforms baselines across all metrics in both human contact and object affordance estimation from egocentric videos. The gap between visual

Figure 5: **Annotation of 3D human contact and object affordance.****(a)** Annotate contact for data in Ego-Exo4D. **(b)** Contact annotation for GIMO dataset, including calculations and manual refinement. **(c)** 3D object affordance annotation, with the red region denoting that with higher interaction probability, while the blue region indicates the adjacent propagable region.

appearance and interaction content, caused by incomplete observations of interacting parties, hinders the performance of methods that rely on visual cues, _e.g._, BSTRO, DECO, O2O, and IAG, leading to suboptimal results for both contact and affordance. LEMON gets moderate results owing to modeling human-object geometric correlations. However, due to the parallel architecture of visual appearances and geometries in its framework, the incomplete appearance in the egocentric view diminishes its performance. In contrast, EgoChoir correlates interaction regions by linking the object interaction concept and subject intention, thereby bridging the gap and achieving better results. Semantics are primarily used to constrain the synergy of interaction regions and are not included in the main evaluation. The comparison of category prediction accuracy is reported in the appendix.

**Qualitative results.** Fig. 6 presents a qualitative comparison of contact and affordance estimated by our method and LEMON. As can be seen, our method yields more precise results and captures the temporal variation of contact, _e.g._, playing the piano with two hands or one. Besides, in cases where the interaction regions are invisible (the below row), LEMON gets poor results due to the ambiguous guidance provided by visual observations. Our method adopts appropriate interaction clues to extract interaction contexts under different interaction scenarios and still infers plausible results.

### Ablation study

We conduct a thorough ablation study to validate the effectiveness of the framework design and some implementation mechanisms, both quantitative and qualitative results are provided.

**Framework design.** The metrics when detaching certain framework designs are recorded in Tab. 2. The head motion \(}\) and affordance \(_{a}\) are crucial interaction clues to excavate the subject intention

Figure 6: **Qualitative Results. Contact vertices are colored yellow, and 3D object affordance are colored red, with the depth of red representing the affordance probability. Note: for intuitive visualization, the contact GT of body interactions are visualized on posed humans (last row) from GIMO . Please zoom in for a better visualization and refer to the Sup. Mat. for video results.**

and object interaction concept, the performance significantly declines without them. The gradient modulation \(\) enables the model to robustly adapt to various interaction scenarios, the absence of this mechanism also impacts performance. The semantic feature \(_{s}\) and the \(f_{ca}\) establish semantic and regional synergy between interacting parties, the metrics drop when detaching any of them. The temporal position encoding \(pe_{t}\) introduces disparity in temporal dimension and eliminates some false positives, removing it decreases the precision and aIOU.

Additionally, qualitative results are provided for further analysis. Fig. 7 (a) demonstrates the results \(\) and \(/\) head motion, as observed, the model can hardly anticipate interaction regions without the head motion, particularly for body interactions. The ablation of \(_{a}\) in modeling human contact is shown in Fig. 7 (b), which shows that the 3D affordance constrains the contact scope and maintains the temporal coherence of contact. Even if the object disappears in some frames, the model still plausibly infers based on the interaction concept provided by 3D affordance. The effectiveness of gradient modulation is illustrated in Fig. 7 (c). As can be seen, the gradients of layers mapping different interaction clues in the parallel cross-attention exhibit significant differences across inputs with distinct interactions, indirectly reflecting that the modulation endows the model to adopt appropriate clues for interaction context modeling and generalize to various interaction scenarios.

**Implementation mechanisms.** The metrics of some implementation mechanisms are also shown in Tab. 2. Randomly initializing the motion encoder makes it difficult to capture variations in motion features, adversely affecting the extraction of interaction contexts and resulting in a performance decline. For the extraction of video features \(_{V}\), we also test video backbones such as SlowFast , Lavila  pre-trained on egocentric datasets [13; 26]. The precision and recall of contact estimation reveal that they tend to predict consistent results across all frames (see qualitative results in appendix), leading to lower precision. Divided space-time attention  is also implemented to replace the joint one, while the joint space-time attention demonstrates superior performance.

### Performance analysis

Here, we outline several heuristic attributes of the model that can robustly reason interaction regions from egocentric videos, and provide insights for further improving the model performance.

  
**Metrics** & **Ours** & \(}\,}\) & \(}_{a}\) & \(}\,\) & \(}\,_{s}\) & \(}\,f_{ca}\) & \(}\,pe_{t}\) & **ri.**\(f_{}\) & **S.F.** & **La.** & **d.**\(f_{st}\) \\ 
**Prec.** & \(0.78\) & \(0.68\) & \(0.71\) & \(0.72\) & \(0.74\) & \(0.72\) & \(0.75\) & \(0.73\) & \(0.67\) & \(0.70\) & \(0.76\) \\
**Recall** & \(0.79\) & \(0.73\) & \(0.64\) & \(0.71\) & \(0.71\) & \(0.75\) & \(0.78\) & \(0.69\) & \(0.77\) & \(0.79\) & \(0.75\) \\
**F1** & \(0.76\) & \(0.69\) & \(0.66\) & \(0.71\) & \(0.73\) & \(0.72\) & \(0.74\) & \(0.70\) & \(0.72\) & \(0.74\) & \(0.75\) \\
**geo.** & \(12.62\) & \(19.86\) & \(19.13\) & \(17.68\) & \(15.53\) & \(15.73\) & \(13.43\) & \(14.57\) & \(21.37\) & \(19.22\) & \(13.04\) \\ 
**AUC** & \(78.02\) & \(74.36\) & \(75.21\) & \(75.34\) & \(76.12\) & \(76.61\) & \(77.75\) & \(76.05\) & \(76.35\) & \(76.62\) & \(77.88\) \\
**aIOU** & \(14.94\) & \(11.75\) & \(12.05\) & \(12.36\) & \(13.04\) & \(13.63\) & \(12.86\) & \(12.92\) & \(12.52\) & \(13.10\) & \(14.62\) \\
**SIM** & \(0.436\) & \(0.403\) & \(0.410\) & \(0.413\) & \(0.422\) & \(0.425\) & \(0.429\) & \(0.423\) & \(0.422\) & \(0.427\) & \(0.431\) \\   

Table 2: **Quantitative Ablations. Metrics when detaching the head motion \(}\), affordance \(_{a}\) in \(_{c}\), gradient modulation \(\), the \(_{s},f_{ca}\) for semantics and region synergy, and \(pe_{t}\). As well as ablations of several implementations, including randomly initialize (ri.) \(f_{}\) without pre-train, video extractors _e.g._, SlowFast (S.F.) and Lavila (La.), divided space-time attention (d. \(f_{st}\)), \(}\) means without.**

Figure 7: **Qualitative Ablations. (a) Results of the human contact and object affordance \(/\) and \(\) head motion, along with the visualized head motion. (b) The lack of 3D affordance leads to over-prediction and temporal inconsistency of human contact. (c) Gradients of layers mapping different interaction clues under distinct input interactions during sampled \(30\) training epochs.**

**Dynamic region.** Human contact and object affordance regions vary as the interaction evolves. We conduct an experiment to validate whether the model captures this attribute. Fig. 8 (a) shows the results of changing object affordances and Fig. 8 (b) demonstrates the estimated changing human contact, unlike methods that distinguish left-right hands by masks or boxes, EgoChoir leverages the head motion _e.g._, rotations, for inference. This provides a way to get rid of intermediary models.

**Multiplicity.** The multiplicity is another crucial attribute of the interaction, encompassing interacting with multiple objects and instances. This requires the model to differentiate interactions with distinct objects and generalize across various instances. As shown in Fig. 8 (c), our method infers credible interaction regions with different objects and instances, which indicates that our model effectively captures interaction contexts with specific objects. It completes the estimation through the interaction contexts rather than merely mapping to specific categories or instances.

**Whole-body motion.** Recently, significant progress has been made in estimating human pose from the egocentric view [12; 37; 84], facilitating the capture of egocentric whole-body motion. We test replacing motion features in the existing framework with global geometric features derived from a sequence of human bodies (SMPL vertices), and find that the performance continues to improve, shown in Tab. 3. This validates the effectiveness of harmonizing multiple clues for estimating interaction regions and suggests the potential for boosting performance in the future.

## 5 Discussion and conclusion

We propose harmonizing the visual appearance, head motion, and 3D object to infer 3D human contact and object affordance regions from egocentric videos. It furnishes spatial representation of the interaction to facilitate applications like embodied AI and interaction modeling. Through the constructed data and annotations, we train EgoChoir, a novel framework that mines the object interaction concept and subject intention, to estimate object affordance and human contact by correlating multiple interaction clues. With the gradient modulation in parallel cross-attention, it adopts appropriate clues to extract interaction contexts and achieves robust estimation of interaction regions across diverse egocentric scenarios. Extensive experiments show that EgoChoir could infer dynamic and multiple egocentric interactions, as well as its superiority over existing methods. EgoChoir offers fresh insights into the field and facilitates egocentric 3D HOI understanding.

**Limitations and future work.** Currently, EgoChoir may estimate the interaction region slightly before or after the exact contact frame, possibly due to a lack of spatial relation perception between interacting parties. Future work could consider incorporating 3D scene conditions and estimated whole-body motion [37; 108] to better constrain the spatial relation, achieving more fine-grained estimation of interaction regions and promoting egocentric human-scene interaction modeling.

**Precision** & **Recall** & **F1** & **geo.** & **AUC** & **aIOU** & **SIM** \\  \(0.80\) & \(0.82\) & \(0.79\) & \(11.24\) & \(78.54\) & \(15.46\) & \(0.448\) \\   

Table 3: Metrics when using whole-body motion.