# VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks

Yang Li

Dept. of Computer Science

Georgia State University

Atlanta, GA 30303

yli93@student.gsu.edu &Shaobo Han

Optical Networking and Sensing

NEC Laboratories America

Princeton, NJ 08540

shaobo@nec-labs.com &Shihao Ji

School of Computing

University of Connecticut

Storrs, CT 06269

shihao.ji@uconn.edu

Part of the work was done while the author was affiliated with Georgia State University.

###### Abstract

As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a "divide-and-share" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing parameters globally via a _vector bank_. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites _all_ the low-rank matrices of LoRA from a shared _vector bank_ with a differentiable top-\(k\) admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results. Our source code is available at https://github.com/leo-yangli/VB-LoRA. This method has been merged into the Hugging Face PEFT package2.

## 1 Introduction

Parameter-efficient fine-tuning (PEFT) casts a new paradigm that leverages strong prior knowledge built in foundation models and adapts them to a wide range of downstream tasks by updating a small amount of trainable parameters [He et al., 2021]. Compared to prefix/prompt tuning [Li and Liang, 2021, Lester et al., 2021] or in-context learning [Brown et al., 2020], fine-tuning a large-scale pre-trained model yields better domain specialization dictated by high-quality datasets [Brown et al., 2020, Liu et al., 2022, Zhao et al., 2023]. This process can be repeated to suit the needs of ever-changing deployment scenarios and personalizations. However, the sheer volume of parameter space across a multitude of instantiations [Sheng et al., 2023] poses challenges for storage, transmission, and computation, especially for low-resource hardware and consumer-grade networks [Borzunov et al., 2024].

To mitigate these challenges, various PEFT methods have been proposed by adding or adapting a small amount of trainable parameters per task without sacrificing performance [Houlsby et al., 2019,

Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters.

Karimi Mahabadi et al., 2021; Ding et al., 2023). These methods exploit the dependencies among model parameters to reduce the redundancy. For example, Hu et al. (2021) propose the low-rank adaptation (LoRA) to approximate the accumulated gradient update for self-attention modules, and induces the intra-matrix parameter coupling. Renduchintala et al. (2024) further study the options of allowing the inter-matrix parameter sharing via weight tying across all the layers. In both cases, the number of trainable parameters is reduced significantly. These two methods stand at the two extremes of spectrum in deciding the range of model components reuse (locally or across-layers) and designating which low-rank matrices needs to be shared and updated. However, as the model size increases and the demand for user-customized models across various services rises, the expense of storing and transmitting the customizations for each combination escalates and emerges as a critical issue. Hence, investigating PEFT methods with significantly smaller number of trainable parameters has attracted a flurry of research interests (Kopiczko et al., 2024; Renduchintala et al., 2024).

This paper introduces VB-LoRA, extreme parameter-efficient fine-tuning with _vector banks_ based on a simple yet effective "divide-and-share" paradigm. We push the limits of LoRA parameter efficiency by breaking the two barriers of low-rank decomposition: (1) locally within each module and each layer, and (2) only across the two original matrix dimensions (without division; see Sec. 3.2 for details). We argue that the parameters across different modules and layers can be shared, and thus the redundancy in parameters can be further reduced. In addition, by partitioning rank-one component vectors into sub-vectors, we introduce "virtual" dimensions such that deep structure in the parameter space can be represented by a highly compressed matrix factorization.

VB-LoRA draws inspirations from previous line of work on quantized tensor networks (Oseledets, 2010; Cichocki, 2014) in breaking the constraint of physical dimension for extreme parameter compression. Specifically, VB-LoRA reparameterizes LoRA's low-rank adaptation by a rank-one decomposition and then divides the resulting vectors into sub-vectors of the same size. A _global_ sharing mechanism is then learnt based on a sparse top-\(k\) admixture module. The same sized sub-vectors allows parameters to be shared across modules and layers at the sub-vector level. Moreover, compared to the post-hoc matrix compression methods (Oseledets, 2010; Khoromskij, 2011), VB-LoRA is end-to-end differentiable, and therefore the fine-tuning process is aware of the compressed form, enabling task-oriented compression. Figure 1 illustrates the parameter efficiency of VB-LoRA as compared with state-of-the-art PEFT methods. Our contributions are summarized as follows:

1. We introduce a "divide-and-share" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing parameters _globally_ via a vector bank.
2. We reparameterize LoRA's low-rank decomposition by a rank-one decomposition, and divide the resulting vectors further into sub-vectors of the same size, enabling extreme parameter efficiency at the sub-vector level.

Figure 2: **Left**: The model parameters can be represented as a composition of vectors from a _vector bank_, which is shared across sub-vectors, modules and layers. **Right**: Architecture of VB-LoRA. We use a top-\(k\) softmax function to select \(k\) vectors from the vector bank. The selected vectors are then pooled into a sub-vector, which is arranged at a desired position, forming the parameters of LoRA.

3. We propose a sparse top-\(k\) module based on the admixture model to learn a global sharing mechanism, making our framework end-to-end differentiable and compression-aware.
4. Our method achieves extreme parameter efficiency while maintaining comparable or better empirical performance compared to the state-of-the-art PEFT methods on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks.

## 2 Related Work

Exploit Global Redundancy for Enhanced Parameter EfficiencyThe parameters of deep neural networks (DNNs) can be naturally divided by layers, heads, or types (MHA or FFN). While LoRA (Hu et al., 2021) only exploits the _intra-matrix_ dependency, Tied-LoRA (Renduchintala et al., 2024) employs a simple weight tying scheme on the low-rank matrices \(A\) and \(B\) across layers to reduce the _inter-matrix_ redundancy. When \(A\) and \(B\) are randomly initialized, frozen, and shared across all layers, Tied-LoRA degenerates to VeRA (Kopiczko et al., 2024), which only requires two scaling vectors to be updated, leading to impressive parameter efficiency. A concurrent work, LoRA-XS (Balazy et al., 2024), further improves the parameter efficiency of LoRA by introducing small trainable matrices between frozen LoRA projection matrices, which are initialized using Singular Value Decomposition (SVD) of the pretrained module weights. Our VB-LoRA pushes the limits of LoRA parameter efficiency by sharing parameters globally across modules and layers at the sub-vector level.

On the low-dimensional reparameterization, Aghajanyan et al. (2020) empirically show that there exists a low-dimensional reparameterization that is as effective for fine-tuning as the full parameter space. The actualization of the random projection is achieved through the Fastfood transform (Le et al., 2013) for large-scale pre-trained language models. To make it structure-aware, a set of layer-wise scaling parameters are included as part of the training parameters. Following this intuition, we study the lightweight fine-tuning within LoRA based on the customized reparameterization that arises from the rank-one matrix decomposition.

Moreover, tensor decomposition has been leveraged for PEFT in ViT models (Jie and Deng, 2023) based on classical formats, such as tensor-train or Tucker (Kolda and Bader, 2009). We find that forcing multilinear decomposition across multiple modes results in a higher rank number, which is detrimental to the objective of parameter compression. An indirect comparison of VB-LoRA to Jie and Deng (2023) can be conducted by referring the compression rate to LoRA. From this perspective, our VB-LoRA can be viewed as a customized tensor format endowed with a convex geometry structure, which is enabled by the sparse top-\(k\) admixture model we proposed.

Compared to the deep fusion approach (Mazzawi et al., 2024) where LLM parameters are split and initialized using pre-trained smaller networks under a designed network growth mechanism, our parameter division operates on the rank-one component vectors. Sub-vector division allows for similar extensions to leverage pre-trained vector bank initializations from smaller models and distributed training using model parallelism.

Parameter Modeling based on Sparse Admixture ModelsAdmixture models have been widely used in population genetics (Pritchard et al., 2000), topic modeling (Reisinger et al., 2010; Inouye et al., 2014), and hyperspectral unmixing (Li and Bioucas-Dias, 2008; Fu et al., 2015) to extract archetypal (or endmember) components from observed data. The archetypal components can be relaxed to have mixed sign (Ding et al., 2008) with identifiability guarantees (Lin et al., 2015). Conventionally, parameters estimation are conducted based on linear programming (Chan et al., 2009) or combinatorial algorithms (Arora et al., 2013). However, an involved integer programming problem arises when incorporating an extra top-\(k\) constraint into the mixing weights that is especially challenging for the large-scale language models. In this work, we propose learning archetypal vector banks not from observed data but from model parameters of LLMs. By modifying the sparse top-\(k\) module (Shazeer et al., 2016) commonly used in Mixture-of-Expert models (Jiang et al., 2024), the mixing weights and vector banks are optimized by back-propagation under the objective of downstream fine-tuning tasks. The proposed top-\(k\) admixture model is model-agnostic in the sense that it can be readily integrated into any neural network parameters or accumulated gradient updates.

Proposed Method

### Preliminaries: Transformer Architecture and LoRA Adapters

The transformer architecture (Vaswani et al., 2017) consists of \(L\) layers, each containing two types of blocks: Multi-Head Attention (MHA) and Feed-Forward Network (FFN). We denote the query, key, value, and output matrices of MHA at layer \(\) as \(}_{t}^{}=\{_{t}^{i}\}_{i=1}^{N_{h}}\), \(t\{q,k,,o\}\), where \(_{t}^{i}^{d d}\), and \(N_{h}\) is the number of heads. Given \(()=_{}(_{})\) with \(^{d}\), viewing FFN as a multi-head operation, we further divide \(_{}^{d d}\) and \(_{}^{d cd}\) into \(c\) matrices of size \(d d\), denoted by \(}_{}^{}=\{_{}^{,i}\}_{i=1}^{c}\) and \(}_{}^{}=\{_{}^{,i}\}_{i=1 }^{c}\). \(c=4\).

Given a pre-trained matrix \(_{0}^{m n}\), LoRA (Hu et al., 2021) constrains the weight increments \(\) as a low-rank decomposition \(=\), where \(^{m r}\), \(^{r n}\) are trainable parameters, with \(r(m,n)\). VeRA (Kopiczko et al., 2024) further limits the trainable parameters to two scaling vectors \(b\) and \(d\), which form the diagonal elements of two diagonal matrices \(_{b}\) and \(_{d}\). Hence, VeRA can be expressed as \(=_{b}_{d}\), where \(\) and \(\) are randomly initialized, frozen and shared across layers.

Collectively, we denote the model parameters of transformer as \(=\{\{}_{q}^{},}_{k}^{},}_{}^{},}_{o}^{}\}\{}_{}^{},}_{}^{}\}\}_{=1}^{L} ^{12L d d}\). In the sequel, we propose a _global_ reparameterization on the weight increments of \(\) based on the LoRA decomposition \(=\). we will show how extreme parameter efficiency can be achieved by (1) parameter sharing across matrix dimensions of \(\) and \(\) based on a rank-one decomposition and sub-vector partitions (Sec. 3.2), and (2) across modules and layers regardless of the index or matrix type (Sec. 3.3).

### Divide-and-Share: a New Paradigm for Parameter Sharing

The low rank decomposition of LoRA can be _equivalently_ expressed in a rank-one form as follows:

\[==_{k=1}^{r}_{k}_{k}= _{k=1}^{r}_{i=1}^{2}_{k}^{(i)},_{k}^{(1)} =_{k},_{k}^{(2)}=_{k},\] (3.1)

where \(\) denotes the outer product operator and \(_{k}^{(i)}\) is a vector of size \(d_{i}\).

DivideBased on the rank-one decomposition above, we further represent each component vector \(_{k}^{(i)}\) as a concatenation of a set of sub-vectors,

\[_{k}^{(i)}=(_{k,1}^{(i)},_{k,2}^{(i)},, _{k,d_{i}^{}}^{(i)}),_{k,j}^{(i)}^{b},  j\{1,,d_{i}^{}\},\] (3.2)

where \(\{d_{i}\}_{i=1,2}\) represents the size of the matrix dimension of \(\). In general, \(\{d_{i}\}_{i=1,2}\) are not equal across \(\) and \(\), and we choose \(b\) as a common factor of \(d_{i}\) such that \(d_{i}^{}=d_{i}/b\) and \(d_{i}^{}\).

ShareTo facilitate parameter sharing across model dimensions, we assume each sub-vector \(_{k,j}^{(i)}\) as a top-\(k\) admixture of basic elements from vector bank \(=\{_{1},,_{h}\}\), where \(_{i}^{b}\) for \(i\{1,,h\}\), and is defined as follows (with the subscripts omitted for clarity):

\[=_{s=1}^{h}w_{s}()_{s},()=((,k)),\] (3.3)

where \((,k)_{i}=_{i}\) if \(_{i}\) is among the top-\(k\) of \(\) and \((,k)_{i}=-\) otherwise. For each sub-vector \(\), we introduce logits \(^{h}\) as its learnable parameters. We call the model expressed in Eq. 3.3 as the _top-\(k\) admixture module_ (TKAM), which is differentiable. This design enables the joint learning of vector bank \(\) and logits \(\) in an end-to-end manner, which is amenable for model fine-tuning to the downstream tasks.

The TKAM module promotes sparsity by selecting \(k\) vectors of the largest logits from the vector bank. By setting \(k h\), we restrict the sub-vector \(\) to be sparse. That is, in each iteration, the updates to the vector bank remain locally dominated - with at most \(k\) basis vectors \(\) affected by the backpropagation through \(\) - in the hope that the learnt vectors can be more specialized and the knowledge encapsulated in the vector bank can be activated and updated sparsely.

Noise-free Top-\(k\) moduleThe Noisy Top-\(k\) Gating module [Shazeer et al., 2016] has been widely used to replace the fully connected layers with the Mixture of Experts (MoE) layers in large language models [Jiang et al., 2024]. In contrast, we use Eq. 3.3 to learn the selective sharing scheme across the rank-one component vectors without changing the original model. Due to the decomposition, we find that the cumulative gradient parameter updates are more sensitive than the original model parameters during the training process. This may be related to the training instability issues observed in hypernetworks [Ortiz et al., 2024], where parameters are generated by another parameterized model as well. Therefore, keeping zero noise in the gating function can help make the learning more efficient and stable. An ablation study of different vector selection methods, including Gumbel-softmax, is provided in Sec. 4.5.

### Breaking Boundaries of LoRA for Global Parameter Sharing

While LoRA only applies the low rank decomposition to each individual weight increment, the boundary can be broken by the _divide-and-share_ scheme we proposed in Sec. 3.2. Our divide-and-share approach can be interpreted as hierarchical and constrained tensor decomposition, which facilitates efficient global parameter sharing that goes beyond LoRA's low-rank representation of matrices.

The divide operator was first introduced in Quantized Tensor Train (QTT) for super compression of large-scale matrices [Oseledets, 2010, Cichocki, 2014]. For example, dyadic division reshapes a vector of length \(L=2^{p}\) into a \(p\)-dimensional array which facilitates the efficient Tensor Train decomposition to be used. Our divide operator instead applies to the rank-one component vectors \(_{k}^{(i)}\), and the resulting hierarchical tensorial representation of \(\) can be viewed as a Canonical Polyadic Decomposition (CPD) [Kolda and Bader, 2009] with component vectors \(_{k}^{(i)}\) folded into \(2\)-dimensional arrays with sub-vectors \(_{k,j}^{(i)}\) as columns. Each sub-vector \(_{}\) is composed from a _globally_ shared vector bank \(\) via TKAM, where \(=[,]\) is a multi-index including physical indices \(\), such as module, layer, head, and left/right decomposed matrix, and virtual indices \(\) (created from vector partition).

The share operator (TKAM module) can be viewed as a factor model with simplex constraints on the mixing weight (e.g., \(k=2\), the sub-vector \(\) has on the edges of the simplex) and common factors stored in \(\). Let \(^{b}\) and \(=_{s=1}^{h}_{s}w_{s}\), where \(_{s}\) is the \(s\)-th factor, and \(\) is the factor score for the sub-vector \(\). We consider the following options for \(\): (1) Admixture (convex combination): \(^{h}\) and \(_{s=1}^{h}w_{s}=1\), which is commonly used in various communities. (2) Sparse Admixture (TKAM): \(^{h}\) and \(_{s=1}^{h}w_{s}=1\) with only \(k h\) non-zero elements allowed. It's worth mentioning that adding the multi-index information to the vector selection mechanism can make the TKAM model structure-aware, potentially yielding additional benefits. One possibility is to make the logits of vector selection conditional on the embeddings of the layer, module, and matrix type, which can be implemented through a hypernetwork [Mahabadi et al., 2021]. However, we leave this for future work.

In summary, LoRA provides a _local_ low-rank factorization for each \(d_{1} d_{2}\) matrix \(\) independently. In contrast, our VB-LoRA introduces a _global_ low-rank factorization on a \(b|\{\}|\) matrix composed of partitioned rank-one vectors, where \(|\{\}|\) denotes the cardinality of the index set including both physical and virtual indices. As we will see below, this differentiation can better leverage the redundancy in the cumulative gradients, leading to extreme parameter efficiency.

Figure 2 overviews our method. The left section demonstrates the high-level idea of VB-LoRA: the vector bank is shared across sub-vectors, modules, and layers. The right section details its architecture. To form each sub-vector, we use a top-\(k\) softmax function to select \(k\) vectors from the vector bank, which are then pooled into a sub-vector. These sub-vectors are arranged in the desired positions, forming the parameters for LoRA with negligible computational overhead. Algorithm 1 provides the PyTorch-like pseudocode for VB-LoRA, which can be seamlessly integrated into the PyTorch framework.

### Parameter Count

In full fine-tuning, the number of trainable parameters is equal to the model size, i.e., \(LMd^{2}\), where \(L\) is the number of layers, \(M\) is the number of fine-tuned modules, and \(d\) is hidden dimension.

LoRA reduces this number to \(2LMdr\), while VeRA further reduces it to \(LM(d+r)\). The trainable parameters of LoRA and VeRA are the same as the parameters they need to store.

In VB-LoRA, the trainable parameters consist of two parts: the parameters of the vector bank \(\) and the parameters of logits \(\). However, at the end of training, the logit parameters can be discarded and only the \(k\) selected indices and the top-\(k\) admixture weights need to be stored. Therefore, the stored parameters can be represented by a triplet \(=\{,,\}\), where \(^{h b}\) is a vector bank containing \(h\) vectors of \(b\)-dimensional, \(^{2 L M r(d/b) k}\) is the top-\(k\) indices of the vectors in \(\) for all sub-vectors, and \(^{2 L M r(d/b)(k-1)}\) is the top-\(k\) admixture weights used to composite the sub-vectors from the bank. It is worth noting that the top-\(k\) admixture weights have only \(k-1\) degrees of freedom since they must be summed to 1. Additionally, depending on the size of the vector bank \(h\), the indices \(\) can be efficiently stored as unsigned integers (e.g., uint8 when \(h 256\)), and hence, we count the number of parameters as the float32-equivalent size for a fair comparison. When we use \(k=2\) and uint8 for indices, the number of stored parameters of VB-LoRA is \(hb+3LMr(d/b)\). Unlike LoRA and VeRA, the number of parameters in VB-LoRA does not increase linearly with the model size (determined by \(L\) and \(d\)) or the number of fine-tuned modules, i.e., \(M\). While the second term of VB-LoRA's parameters is a linear function of \(LMd\), the coefficient is \(3r/b\), which is typically very small. For example, in our experiments, the typical values are \(r=4\) and \(b=256\), leading to a coefficient of 0.04, whereas the coefficient is \(2r\) for LoRA and 1 for VeRA. Most of the parameters in VB-LoRA reside within the shared vector bank, whose size does not increase linearly with the model size or number of fine-tuned modules.

## 4 Experiments

In this section, we conduct a comprehensive evaluation of our method through a series of experiments. We begin by comparing VB-LoRA to the state-of-the-art PEFT methods: LoRA, VeRA, and Tied-LoRA on the GLUE benchmark. Next, we extend our analysis to natural language generation tasks using GPT-2, instruction tuning tasks on the Llama2, as well as mathematical reasoning tasks on Mistral and Gamma models. All our experiments were conducted on a server equipped with 8 NVIDIA A100 GPUs. For reproducibility, we provide detailed hyperparameters and specifications of computing resources for each experiment in the appendix. The source code is available at https://github.com/leo-yangli/VB-LoRA.

### Natural Language Understanding

We adopt the General Language Understanding Evaluation (GLUE) benchmark3[Wang et al., 2018] to assess the performance of VB-LoRA across various natural language understanding tasks, includingsimilarity, paraphrase, and inference tasks. Following Kopiczko et al. (2024), we focus on six tasks from GLUE: CoLA Warstadt et al. (2019) (linguistic acceptability), SST-2 Socher et al. (2013) (sentiment analysis), MRPC Dolan and Brockett (2005) (paraphrase detection), STS-B Cer et al. (2017) (semantic textual similarity), QNLI Rajpurkar et al. (2018) (inference), and RTE (inference).

Our experiments are performed with RoBERTabase and RoBERTalarge Liu et al. (2019). While LoRA and VeRA only finetune the query and value modules, we explore two fine-tuning strategies: query and value only (VB-LoRAqu), and all linear modules (VB-LoRAall), including \(_{q},_{k},_{v},_{o}\), \(_{}\), and \(_{}\). We create a vector bank of 90 vectors of a length of 256, initialized with a uniform distribution \((-0.02,0.02)\). The logits are initialized with a normal distribution \((0,0.01)\). The learning rates for the vector bank and logit parameters are set to 0.001 and 0.01, respectively. We set the rank to 4 and \(k=2\) for all our experiments.

Table 1 reveals that VB-LoRA achieves competitive or superior performance compared to VeRA and Tied-LoRA, while being more parameter efficient. For example, when fine-tuning the query and value modules on the RoBERTalarge model, our method reduces the stored parameters to less than 40% of those required by VeRA or Tied-LoRA, while outperforming them across all tasks. These results suggest that model performance depends not only on the quantity of trainable parameters but also on how they are composed.

Moreover, the results consistently indicate that fine-tuning all modules, beyond just the query and value modules, enhances performance for all the methods. However, LoRA, VeRA and Tied-LoRA requires 2-4 times of the parameters in this case because their parameter counts increase linearly with the number of fine-tuned modules. In contrast, our method uses only 37.5% additional parameters as we maintain the same vector bank size but add additional parameters for indices and top-\(k\) weights. Thus, with only 12.8% of the parameters compared to \(\) (4% compared to \(_{}\)), our method achieves the best average performance.

### Natural Language Generation

For natural language generation experiments, we fine-tune the GPT-2 Medium and Large models Radford et al. (2019) on the E2E dataset4Novikova et al. (2017), which contains approximately 42,000 training examples, 4,600 validation examples, and 4,600 test examples from the restaurant domain. We use a vector bank of size 256 for GPT-2 Medium and 350 for GPT-2 Large. The vector length is set to 256 and the rank is set to 4 for both models. To achieve the best performance, we fine-tune all attention layers and FFN layers. As shown in Table 2, our approach achieves competitive performance compared to VeRA, while requiring about 20% less stored parameters for both models.

    & Method & \# Params & SST-2 & MRPC & CoLA & QNLI & RTE & STS-B & Avg. \\    } & FT & 125M & 94.8 & 90.2 & 63.6 & 92.8 & 78.7 & 91.2 & 85.2 \\  & LoRAv & 0.295M & 95.1\(\)0.2 & 89.7\(\)0.7 & 63.4\(\)1.2 & 93.3\(\)0.3 & 86.6\(\)0.7 & 91.5\(\)0.2 & 86.6 \\  &   } & VeRA\({}_{}\) & 0.043M & **94.6\(\)**0.1 & **89.5\(\)**0.5 & **65.6\(\)**0.8 & 91.8\(\)0.2 & 78.7\(\)0.7 & 90.7\(\)0.2 & 85.2 \\  & & Tied-LoRAv & 0.043M & 94.4\(\)0.5 & 88.5\(\)0.1 & 61.9\(\)0.1 & 92.0\(\)0.1 & 76.2\(\)1.0 & 89.8\(\)0.3 & 83.8 \\  & \(_{}\) _(**Ours)** & **0.023M** & 94.4\(\)0.2 & **89.5\(\)**0.5 & 63.3\(\)0.7 & **92.2\(\)**0.2 & **82.3\(\)**1.3 & **90.8\(\)**0.1 & **85.4** \\   & & VeRAall & 0.157M & **95.1\(\)**0.4 & 88.7\(\)0.5 & 64.5\(\)1.0 & 92.3\(\)0.2 & 81.9\(\)1.4 & 90.2\(\)0.3 & 85.5 \\  & & Tied-LoRAall & 0.109M & 94.7\(\)0.2 & 88.5\(\)0.8 & **64.7\(\)**0.8 & **92.4\(\)**0.1 & 76.5\(\)1.3 & 90.3\(\)0.1 & 84.5 \\  & & VB-LoRAall _(**Ours)** & **0.027M** & 95.0\(\)0.2 & **89.7\(\)**0.2 & 64.3\(\)1.4 & 92.3\(\)0.2 & **82.3\(\)**0.9 & **90.7\(\)**0.2 & **85.7** \\    } & LoRAv & 0.786M & 96.2\(\)0.5 & 90.2\(\)1.0 & 68.2\(\)1.9 & 94.8\(\)0.3 & 85.2\(\)1.1 & 92.3\(\)0.5 & 87.8 \\  & & VRAAv & 0.061M & **96.1\(\)**0.1 & 90.9\(\)0.7 & 68.0\(\)0.8 & 94.4\(\)0.2 & 85.9\(\)0.7 & 91.7\(\)0.8 & 87.8 \\  & & Tied-LoRAqv & 0.066M & 94.8\(\)0.6 & 89.7\(\)1.0 & 64.7\(\)1.2 & 94.1\(\)0.1 & 81.2\(\)0.1 & 90.8\(\)0.3 & 85.9 \\  & & VB-LoRAqv _(**Ours)** & **0.024M** & 96.1\(\)0.2 & **91.4\(\)**0.6 & **68.3\(\)**0.7 & **94.7\(\)**0.5 & **86.6\(\)**1.3 & **91.8\(\)**0.1 & **88.2** \\   & & VeRAall & 0.258M & **96.6\(\)**0.5 & 90.9\(\)0.8 & 68.5\(\)1.4 & **94.4\(\)**0.8 & 85.9\(\)1.2 & **92.2\(\)**0.2 & 88.1 \\  & & Tied-LoRAall & 0.239M & 94.8\(\)0.3 & 90.0\(\)0.4 & 66.8\(\)0.1 & 94.1\(\)0.1 & 82.3\(\)0.9 & 91.6\(\)0.2 & 86.6 \\  & & VB-LoRAall _(**Ours)** & **0.033M** & 96.3\(\)0.2 & **91.9\(\)**0.9 & **69.3\(\)**1.5 & **94.4\(\)**0.2 & **87.4\(\)**0.7 & 91.8\(\)**0.2 & **88.5** \\   

Table 1: Results with RoBERTabase and RoBERTalarge on the GLUE benchmark. The best results in each group are shown in **bold**. We report Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqv are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.

### Instruction Tuning

Instruction tuning is a process of fine-tuning model with a set of instructions or prompts to enhance its performance on specific instructions (Ouyang et al., 2022). We first experiment on a general instruction tuning dateset. We use the Cleaned Alpaca Dataset 5, which improves the data quality of the original Alpaca dataset (Taori et al., 2023). We evaluate the fine-tuned models on the MT-Bench6(Zheng et al., 2024), which contains 80 multi-turn questions.

Following Kopiczko et al. (2024), we fine-tune the Llama2 model (Touvron et al., 2023) within the QLoRA (Dettmers et al., 2023) framework7, which aims to reduce memory usage when fine-tuning large language models on a single GPU. We utilize the quantization strategy provided by QLoRA, including 4-bit NormalFloat for storage data, BFloat16 for computation parameters, double quantization and paged optimizers to train it on a single GPU. Our fine-tuned models generate responses to these questions, and subsequently, GPT-4 is employed to review and evaluate the generated answers, assigning a quantitative score on a scale of 10. Note that aligning with VeRA, we report the score of the first turn of the conversation. Following Kopiczko et al. (2024), we apply VB-LoRA to all linear layers except the top one. For Llama2 7B, we use a vector bank of 2,048 vectors, each with a length of 256, and the rank is set to 4, resulting in a total of 0.8M stored parameters. For Llama2 13B, we use the same-sized vector bank but increase the rank to 6, leading to 1.1M stored parameters. For all the experiments, we train for one epoch.

The results are reported in Table 3. Notably, we report two sets of LoRA results for each experiment: one from our implementation and the other from Kopiczko et al. (2024), due to a noticeable discrepancy between the scores. Since we closely follow the experimental settings of Kopiczko et al. (2024), we speculate that the difference is due to changes in the GPT-4 model over time. However, comparing the relative improvements of VeRA and VB-LoRA with their respective implementations of LoRA remains fair. VB-LoRA achieves higher scores than LoRA while using only 0.5% (Llama2 7B) and 0.4% (Llama2 13B) of the stored parameters. While VeRA can reach similar scores with their implementation of LoRA, it requires more than twice of parameters compared to VB-LoRA.

### Mathematical Reasoning

To evaluate mathematical reasoning capabilities, we fine-tune the Mistral-7B-v0.1 and Gemma-7B models on the MetaMathQA8(Yu et al., 2023) dataset and test them on GSM89(Cobbe et al., 2021) and MATH10(Hendrycks et al., 2021) datasets. We compare our results with the concurrent work LoRA-XS (Balazy et al., 2024), following its experimental configuration. The result is shown in Table 4. Our method outperforms all baselines on GSM8K, with Mistral-7B utilizing only 0.4% of

    & Method & \# Params & BLEU & NIST & METEOR & ROUGE-L & CIDEr \\    } & FT & 354.92M & 68.2 & 8.62 & 46.2 & 71.0 & 2.47 \\  & LoRA & 0.35M & 68.9 & 8.69 & 46.4 & 71.3 & 2.51 \\  & VeRA & 0.098M & **70.1** & **8.81** & **46.6** & **71.5** & 2.50 \\  & VB-LoRA _(Ours)_ & **0.076M** & 70.0 & **8.81** & **46.6** & **71.5** & **2.52** \\    } & FT & 774.03M & 68.5 & 8.78 & 46.0 & 69.9 & 2.45 \\  & LoRA & 0.77M & 70.1 & 8.80 & 46.7 & 71.9 & 2.52 \\  & VeRA & 0.17M & **70.3** & 8.85 & **46.9** & 71.6 & **2.54** \\  & VB-LoRA _(Ours)_ & **0.13M** & **70.3** & **8.86** & 46.7 & **72.2** & **2.54** \\   

Table 2: Results with GPT-2 Medium and GPT-2 Large on the E2E benchmark. The results for FT and LoRA are taken from Hu et al. (2021), and the results for VeRA are taken from Kopiczko et al. (2024). We report the mean of 3 runs using different random seeds.

the parameters compared to LoRA, and Gemma-7B using just 0.3%. Compared with LoRA-XS, our method outperforms on both evaluation datasets while using 70% (Mistral-7B) and 83% (Gemma-7B) of LoRA-XS parameters.

### Ablation Study

We conduct an ablation study to examine the impact of each individual component of VB-LoRA. The experiments are performed on RoBERTa-large, fine-tuning only the query and value modules.

Vector Selection MethodsBesides the top-\(k\) admixture module (abbreviated as Top-\(k\) below), there exist several commonly used discrete optimization methods for vector selection, including Noisy Top-\(k\)[Shazeer et al., 2016], Gumbel-Softmax (GS), and Straight-Through Gumbel-Softmax [Jang et al., 2017; Maddison et al., 2016]. For Top-\(k\) and Noisy Top-\(k\), we evaluate the impact of different \(k\) to the performances on the CoLA dataset. For GS and Straight-Through GS, we set the temperature \(=1/3\) during training and use Top-1 and Top-2 Softmax for inference. Additionally, we explore "Select All", a special case of Top-\(k\) with \(k\) equals to the vector bank size \(h\). As shown in Table 5, Noisy Top-\(k\), GS, and Straight-Through GS significantly underperform Top-\(k\) and "Select All". We hypothesize that random noise injected by these methods likely disrupts the parameters of vector bank, leading to instability in the learning process.

We further investigate the impact of \(k\) to the training dynamics and performance of VB-LoRA. As discussed in Sec. 3.4, the choice of \(k\) affects not only the model's performance but also the number of parameters to be stored. Hence, a smaller \(k\) is generally preferred for improved parameter efficiency. Table 5 shows that \(k=2\) yields the best result on CoLA, whereas \(k=1\) performs significantly worse. To explain this, we delve into the training dynamics of VB-LoRA. As shown in Figure 3 (a), when \(k=1\), the selected vectors remain largely unchanged during training. In contrast, when \(k>1\), the model actively explore the vector bank as illustrated in Figure 3 (b) and (c), i.e., different vectors are selected and updated actively during the training process. Additionally, we observed that this vector exploration primarily occurs in the early stages of training, with updates becoming progressively sparser in later stages, as shown in Figure 5 in the appendix. This suggests that the vectors become increasingly specialized for specific sub-vectors as training progresses.

Sub-vector Length \(b\)VB-LoRA introduces a new virtual dimension that divides the original dimensions of LoRA matrices into sub-vectors of length \(b\). Note that \(b\) must be a common factor of all hidden dimensions to ensure compatibility across the entire model. However, the optimal value of \(b\) is task-specific and requires tuning as a hyperparameter. Theoretically, with a fixed vector bank budget, a larger \(b\) reduces the number of vectors in the vector bank, potentially making each vector less specialized. On the other hand, a smaller \(b\) increases the number of trainable parameters and complicates the vector selection process. As shown in Table 6, a moderate \(b=256\) yields the best performance on the CoLA task.

   Model & Method & \# Parameters & Score \\   & w/o FT & - & 4.79 \\   & LoRA\({}^{}\) & 159.9M & 5.19 \\  & VoRA & 1.6M & 5.08 \\   & LoRA\({}^{}\) & 159.9M & 5.63 \\  & VB-LoRA (_Ours_) & **0.8M** & **5.71** \\   & w/o FT & - & 5.38 \\   & LoRA\({}^{}\) & 250.3M & 5.77 \\    & VoRA & 2.4M & 5.93 \\    & LoRA\({}^{}\) & 250.3M & 6.13 \\   & VB-LoRA (_Ours_) & **1.1M** & **6.31** \\   

Table 4: Results with Mistral-7B and Gemma-7B models on the GSM8K and MATH Benchmarks. Specifically, in VB-LoRA, we use a vector bank size of 2,048 with \(b=256\), set the rank to 4, and train with a batch size of 128 for 2 epochs. The warm-up ratio is 0.02, and training uses a cosine learning rate scheduler, with an initial learning rate of 0.001 for the vector bank and 0.01 for the logits. The baseline results are taken from Balazy et al. .

   Model & Method & \# Parameters & Score \\   & w/o FT & - & 4.79 \\   & LoRA\({}^{}\) & 159.9M & 5.19 \\  & VoRA & 1.6M & 5.08 \\   & LoRA\({}^{}\) & 159.9M & 5.63 \\  & VB-LoRA (_Ours_) & **0.8M** & **5.71** \\   & w/o FT & - & 5.38 \\   & LoRA\({}^{}\) & 250.3M & 5.77 \\    & VoRA & 2.4M & 5.93 \\    & LoRA\({}^{}\) & 250.3M & 6.13 \\   & VB-LoRA (_Ours_) & **1.1M** & **6.31** \\   

Table 3: Results with Llama2 on MT-Bench, scored by GPT-4 out of 10. LoRA\({}^{}\) and VeRA are sourced from Kopiczko et al. . LoRA\({}^{}\) and VB-LoRA are from our implementations. The discrepancy between LoRA\({}^{}\) and LoRA\({}^{}\) may be due to changes in the GPT-4 model over time.

## 5 Conclusion

This paper introduces a "divide-and-share" paradigm and a differentiable top-\(k\) admixture module for extreme parameter-efficient fine-tuning with vector banks. Our proposed VB-LoRA achieves the competitive or higher accuracy while using significantly smaller number of stored parameters compared to the state-of-the-art PEFT methods, including LoRA, VeRA, and Tied-LoRA. In addition, VB-LoRA is model-agnostic and applicable to other PEFT methods (Ding et al., 2023), including inserted adapters (Karimi Mahabadi et al., 2021), prompt tuning (Qin et al., 2021), and BitFit (Ben Zaken et al., 2022). Although VB-LoRA focuses on reducing the storage and transmission costs for LLM fine-tuning, we believe the proposed scheme can be extended to memory-efficient fine-tuning and parameter-efficient pre-training. We leave these for future exploration.

Fine-tuning a pre-trained model requires making design choices about which layers of the model should be frozen or updated. Multitask fine-tuning adds extra complexity about which parameters should be shared or task-specific. Along this line of work, Polytropon (Ponti et al., 2022) jointly learns a small inventory of LoRA adapters and a routing function that selects a variable-sized subset of adapters for few-shot adaptation. Caccia et al. (2023) emphasize the importance of routing granularity and further propose a finer-grained mixing across multiple heads. Following these works, it would be interesting to explore a finer-grained parameter transfer across tasks, heads, types, and layers at the sub-vector level for multitask fine-tuning.

**Limitations and broader impacts** Our experiments are limited to monomodal (text-based), monolingual (English), and LoRA-only settings. Additionally, our exploration of the vector bank is somewhat limited, as we only examine a small range of configurations for bank size and vector length. In terms of broader impacts, VB-LoRA reduces the storage and transmission costs of LLM adapters and demonstrates improved memory-efficiency, making customized LLMs more accessible. We do not foresee any negative societal impact beyond those generally associated with LLMs.