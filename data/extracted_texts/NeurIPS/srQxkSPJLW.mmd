# DI-MaskDINO: A Joint Object Detection and

Instance Segmentation Model

Zhixiong Nan

College of Computer Science, Chongqing University, Chongqing, China.

Xianghong Li

College of Computer Science, Chongqing University, Chongqing, China.

Tao Xiang

Corresponding author.

Jifeng Dai

Department of Electronic Engineering, Tsinghua University, Beijing, China.

nanzx@cqu.edu.cn, lixianghong@stu.cqu.edu.cn, txiang@cqu.edu.cn, daijifeng@tsinghua.edu.cn

###### Abstract

This paper is motivated by an interesting phenomenon: the performance of object detection lags behind that of instance segmentation (i.e., _performance imbalance_) when investigating the intermediate results from the beginning transformer decoder layer of MaskDINO (i.e., the SOTA model for joint detection and segmentation). This phenomenon inspires us to think about a question: will the _performance imbalance_ at the beginning layer of transformer decoder constrain the upper bound of the final performance? With this question in mind, we further conduct qualitative and quantitative pre-experiments, which validate the negative impact of _detection-segmentation imbalance_ issue on the model performance. To address this issue, this paper proposes **DI-MaskDINO** model, the core idea of which is to improve the final performance by alleviating the _detection-segmentation imbalance_. **DI-MaskDINO** is implemented by configuring our proposed _De-Imbalance (DI)_ module and _Balance-Aware Tokens Optimization (BATO)_ module to MaskDINO. _DI_ is responsible for generating balance-aware query, and _BATO_ uses the balance-aware query to guide the optimization of the initial feature tokens. The balance-aware query and optimized feature tokens are respectively taken as the _Query_ and _Key&Value_ of transformer decoder to perform joint object detection and instance segmentation. **DI-MaskDINO** outperforms existing joint object detection and instance segmentation models on COCO and BDD100K benchmarks, achieving **+1.2**\(AP^{box}\) and **+0.9**\(AP^{mask}\) improvements compared to SOTA joint detection and segmentation model MaskDINO. In addition, **DI-MaskDINO** also obtains **+1.0**\(AP^{box}\) improvement compared to SOTA object detection model DINO and **+3.0**\(AP^{mask}\) improvement compared to SOTA segmentation model Mask2Former.

## 1 Introduction

Object detection and instance segmentation are two fundamental tasks in the computer vision community. Intuitively, the two tasks are closely-related and mutually-beneficial. However, in the current time, specialized detection or segmentation gains more focuses, and the amount of works studying the specialized task is significantly larger than that for joint tasks. One typical explanatory is that the multi-task training even hurts the performance of the individual task.

Confronting current research situations, we think about whether there are some essential cruxes that are ignored in previous works and these cruxes hinder the cooperation of object detection and instancesegmentation tasks, which further constrains the breakthrough of the performance upper bound. This paper reveals one of cruxes is the imbalance of object detection and instance segmentation. As shown in Fig. 1, when investigating the results at the first layer of transformer decoder of MaskDINO model , an interesting phenomenon is found that there exists the performance imbalance between object detection and instance segmentation, as qualitatively illustrated in Fig. 0(a) and quantitatively illustrated in the first bar of Fig. 0(c). After considering the imbalance issue, the performance gap at the first layer is narrowed as illustrated in the first bar of Fig. 0(d), and the final performance upper bounds are improved (i.e., 28.1 to 29.5 for detection and 25.3 to 25.7 for segmentation) as illustrated in the second bar of Fig. 0(d). The qualitative results are also optimized, the detection bounding boxes closely surround segmentation masks, as illustrated in Fig. 0(b).

According to the above analysis, we can find the detection-segmentation imbalance at the beginning layer is one of essential cruxes that hinders the cooperation of object detection and instance segmentation. Therefore, we reviewed the previous works to investigate whether there are works that have been aware of this issue. The idea of many classical and excellent methods [16; 2; 5; 11] is combining two tasks by adding a segmentation branch to an object detector. These detect-then-segment methods make the performance of segmentation task to be limited by the performance of the object detector. Thanks to the thriving of transformer  and DETR , recent research attention has been geared towards transformer-based methods, which make giant contributions to the community. For example, [10; 50; 25] use the unified query representation for object detection and instance segmentation tasks based on transformer architecture.

However, to our best knowledge, there is no existing work to solve the above mentioned detection-segmentation imbalance issue. Factually, the imbalance issue naturally exists, which is determined by the **individual characteristics** of detection and segmentation tasks and also derived from the **supervision manners** for the two tasks. **Firstly**, segmentation is a pixel-level grouping and classification task [16; 46], thus local detailed information is important for this task. In contrast, detection is a region-level task to locate and regress the object bounding box [13; 38], which requires global information focusing on the complete object. The query at the beginning decoder layer conveys relatively local features, which is more beneficial for the segmentation task, thus the detection task tends to achieve lower performance at the beginning layer. **Secondly**, supervision manners for detection and segmentation are distinctly different. The segmentation is densely supervised by all pixels of the GT mask, while detection is sparsely supervised by a 4D vector (i.e., x, y, w, and h) of GT bounding box. The dense supervision provides richer and stronger information than the sparse supervision during the optimization procedure. Therefore, the optimization speeds of the two tasks are not synchronous, which will lead to the imbalance issue.

Based on two above-analyzed reasons for the detection-segmentation imbalance issue, it is straightforward that the performance of detection task will lag behind at the beginning layer. Considering existing

Figure 1: Qualitatively, (a) shows that the detection bounding boxes predicted by the query/feature at the first decoder layer of MaskDINO do not fit well with segmentation masks, and (b) exhibits that the corresponding results of **DI-MaskDINO** are optimized and the detection bounding boxes closely surround segmentation masks. Quantitatively, (c) displays that there exists a significant performance gap between detection and segmentation at the first decoder layer of MaskDINO, and (d) demonstrates **DI-MaskDINO** not only alleviates the performance imbalance at the first layer but also improves the performance upper bound.

methods share a unified query for detection and segmentation tasks, the performance of a task will be negatively affected by another poorly-performed task, leading to that the multi-task joint training even hurts the performance of the individual task. Therefore, addressing the detection-segmentation imbalance issue is significant for designing a joint object detection and instance segmentation model. To address the detection-segmentation imbalance issue, we propose **DI-MaskDINO** model, which is implemented by configuring our proposed _De-Imbalance (DI)_ module and _Balance-Aware Tokens Optimization (BATO)_ module to MaskDINO. _DI_ module is responsible for generating balance-aware query. Specifically, _DI_ module strengthens the detection at the beginning decoder layer to balance the performance of the two tasks, and the core of _DI_ module is our proposed _residual double-selection_ mechanism. In this mechanism, the _token interaction_ based _double-selection_ structure learns the global geometric, contextual, and semantic patch-to-patch relations to update initial feature tokens, and high-confidence tokens are selected to benefit the detection task since the selected tokens have learned global semantics during the _token interaction_ procedure. In addition, this mechanism makes use of initial feature tokens by the _residual_ structure, which is the necessary compensation for the information loss occurring during _double-selection_. Apart from _DI_ module, we also design _BATO_ module, which uses the balance-aware query to guide the optimization of initial feature tokens. The balance-aware query and optimized feature tokens are respectively taken as the _Query_ and _Key&Value_ of transformer decoder to perform joint object detection and instance segmentation.

The contributions of this paper are as follows: _i_) to our best knowledge, this paper for the first time focuses on the detection-segmentation imbalance issue and proposes _DI_ module with the _residual double-selection_ mechanism to alleviate the imbalance; _ii_) **DI-MaskDINO** outperforms existing SOTA joint object detection and instance segmentation model MaskDINO (**+1.2**\(AP^{box}\) and **+0.9**\(AP^{mask}\) on COCO, using ResNet50 backbone with 12 training epochs), SOTA object detection model DINO (**+1.0**\(AP^{box}\) on COCO), and SOTA segmentation model Mask2Former(**+3.0**\(AP^{mask}\) on COCO).

## 2 Related Work

**Object Detection.** Classical object detection methods [13; 38; 27; 37; 5; 43; 42] have achieved significant success. In recent years, transformer-based methods such as DETR  make a giant contribution to object detection by introducing the concept of object queries and the one-to-one matching mechanism. The success of DETR has sparked a boom in query-based end-to-end detectors, and numerous excellent variants are proposed [56; 33; 52; 28; 23; 6; 47; 51; 24; 32; 21; 54; 18]. Specifically, to enhance the convergence speed of DETR, Deformable DETR  proposes deformable attention mechanism that learns sparse feature sampling and aggregates multi-scale features accelerating model convergence and improving performance. From the interpretability of object queries, DAB-DETR  formulates the queries as 4D anchor boxes and dynamically updates them in each decoder layer.

**Instance Segmentation.** CNN-based instance segmentation methods are categorized into top-down and bottom-up paradigms. The top-down paradigm [16; 2; 11; 19; 7; 4; 1] firstly generates bounding boxes by object detectors, and then segments the masks. The bottom-up paradigm [35; 9; 29; 12] treats instance segmentation as a labeling-clustering problem, where pixels are firstly labeled as a class or embedded into a feature space and then clustered into each object. Recently, many transformer-based instance segmentation methods [8; 14; 20; 53; 17; 10; 50; 25] are proposed. The transformer-based methods treat the instance segmentation task as a mask classification problem that associates the instance categories with a set of predicted binary masks.

**Joint Object Detection and Instance Segmentation.** The goal of joint object detection and instance segmentation is to carry out the two tasks simultaneously [45; 34; 41; 36]. The traditional joint object detection and instance segmentation methods [16; 2; 5; 11] are usually implemented by adding a mask branch to a strong object detector. For example, the classical model Mask RCNN  achieves joint object detection and instance segmentation by adding a mask branch to Faster RCNN . Recently, the proposal of the transformer-based framework has promoted the development of joint object detection and instance segmentation. Following DETR , SOLQ  proposes a unified query representation for simultaneous detection and segmentation tasks. The recent MaskDINO  achieves optimal performance with the unified query representation on both detection and segmentation tasks.

## 3 Proposed Method

In response to the naturally-existing but commonly-ignored imbalance issue between object detection and instance segmentation, we propose **DI-MaskDINO** model, which is based on MaskDINO . To better understand our proposed **DI-MaskDINO**, we firstly review MaskDINO (SS 3.1), and then introduce **DI-MaskDINO** (SS 3.2).

### Preliminaries: MaskDINO

MaskDINO is a unified object detection and segmentation framework, which adds a mask prediction branch on the structure of DINO . MaskDINO (grey shaded part in Fig. 2) is composed of a backbone, a transformer encoder, a transformer decoder, and detection and segmentation heads (i.e., "Prediction" in Fig. 2). Position embeddings and the flattened multi-scale features (extracted by backbone) are inputted to the transformer encoder to generate the initial feature tokens (\(_{i}\)). Note that in MaskDINO, the top-ranked feature tokens selected from \(_{i}\) directly serve as the _Query_ of transformer decoder, while we design _De-Imbalance_ module with a _residual double-selection_ mechanism to firstly alleviate the detection-segmentation imbalance and then obtain the balance-aware query \(_{bal}\) to serve as the _Query_ of transformer decoder. In addition, we design _Balance-Aware Tokens Optimization_ module to optimize \(_{i}\) and generate the balance-aware feature tokens \(_{bal}\) to serve as the _Key&Value_ of transformer decoder. Token and query are specialized terms, and their explanations are provided in Appendix A.

### Our Method: DI-MaskDINO

Fig. 2 illustrates the overview of **DI-MaskDINO**, which consists of four modules, including _Feature Tokens Extractor_ (_FTE_), _De-Imbalance_ (_DI_), _Balance-Aware Tokens Optimization_ (_BATO_), and _Transformer Decoder_ (_TD_), _FTE_ extracts the initial feature tokens \(_{i}\) from the input image using backbone and MaskDINO encoder. The goal of _DI_ is to generate the balance-aware query \(_{bal}\), which is implemented by applying our proposed _residual double-selection_ mechanism on the initial feature tokens \(_{i}\). _BATO_ utilizes \(_{bal}\) to optimize \(_{i}\), generating the balance-aware feature tokens \(_{bal}\). _TD_ takes \(_{bal}\) as the _Key&Value_ and \(_{bal}\) as the _Query_ to perform joint object detection and instance segmentation.

Figure 2: The overview of **DI-MaskDINO** model based on MaskDINO (grey shaded), with the extensions (green shaded) of _De-Imbalance_ and _Balance-Aware Tokens Optimization_. For simplicity, content token and position token are merged in _De-Imbalance_ (i.e., \(_{i}\), \(_{s1}\), \(_{s2}\), and \(_{bal}\) contain both content and position token) in presentation. GTG is short for guiding token generation.

#### 3.2.1 Feature Tokens Extractor

Given an image \(^{H W 3}\), the backbone (e.g., ResNet ) firstly extracts multi-scale features, which are then flattened and concatenated to serve as the input of transformer encoder comprising six multi-scale deformable attention layers , obtaining the initial feature tokens \(_{i}\) that is composed of \(_{i=3}^{6}{(}})}\) tokens, where each token expresses the feature of the corresponding patch in \(\).

#### 3.2.2 De-Imbalance

There exists the detection-segmentation imbalance at the beginning layer of transformer decoder, which might constrain the upper bound of model performance. To handle this issue, we design \(\) module to alleviate the imbalance, instead of directly providing \(_{i}\) to the transformer decoder as MaskDINO does. Specifically, detection-segmentation imbalance means that the performance of object detection lags behind that of instance segmentation at the beginning layer of transformer decoder. Therefore, we propose the _residual double-selection_ mechanism to strengthen the performance of object detection.

The _double-selection_ consists of the first selection and the second selection. In the first selection, we select top-\(k_{1}\) ranked feature tokens in \(_{i}\), based on their category classification scores:

\[_{s1}=(_{i},k_{1}),\] (1)

where \(_{s1}\) represents the firstly-selected feature tokens, \(\) denotes the selection operator. The first selection mainly filters out most of the tokens conveying background information, making \(_{s1}\) focus on the objects.

Before the second selection, a _token interaction_ network comprising two self-attention layers is applied on \(_{s1}\):

\[_{s1}^{sa}=(_{s1}),\] (2)

where MHSA is Multi-Head Self-Attention and \(_{s1}^{sa}\) indicates the feature tokens processed by MHSA.

The _token interaction_ is the key point to make sure that the secondly-selected tokens are beneficial for detection, we explain its rationality as follows. As we know, each token actually corresponds to a patch (remarkably smaller than an object in most cases) in the image . The bounding box of an object is regressed by integrating the multiple patches (belonging to the same object) that have global patch-to-patch spatial relations, thus it is really needed for the detection task to learn the interaction relation between patches. In contrast, the dense all-pixel supervision for the segmentation task mainly focuses on local pixel-level similarity with GT mask , hence the segmentation task is not particularly depend on the patch-to-patch relation as the detection task. By self-attention layers, different tokens representing the patches (belonging to the same object) can interact with each other to learn the global geometric, contextual, and semantic patch-to-patch relations, benefiting the perception of object bounding boxes. Therefore, executing _token interaction_ before the second selection makes \(\) module to be more beneficial for detection. In addition, verified by some studies (e.g., ), the tokens with higher category scores correspond to higher IOU scores of object bounding boxes. Therefore, the second selection further strengthens the object detection and alleviates the detection-segmentation imbalance.

In the second selection, we select the top-\(k_{2}\) ranked feature tokens in \(_{s1}^{sa}\) to obtain the secondly-selected feature tokens \(_{s2}\):

\[_{s2}=(_{s1}^{sa},k_{2}).\] (3)

The _residual double-selection_ is inspired by the _residual_ idea in , and the _residual_ is the necessary compensation for _double-selection_ since the information loss occurs in the selection procedures. The formulation of this mechanism is combining \(_{i}\) with \(_{s2}\) by the Multi-Head Cross-Attention network (MHCA, a self-attention layer and a FFN layer are omitted here), generating \(_{bal}\):

\[_{bal}=(_{s2},_{i}).\] (4)

\(_{bal}\) conveys the balance-aware information, thus it is named as balance-aware query. Subsequently, \(_{bal}\) is fed to \(\) to guide the optimization of initial feature tokens \(_{i}\). It is noted that the tokens in \(_{bal}\) have become significantly different from the tokens in \(_{i}\). Through Eq. 1-4, the tokens in \(_{bal}\) have obtained larger receptive field and considered the interaction with other tokens, thus they are better understood as object/instance candidates. Correspondingly, the denotation has been changed from \(\) to \(\).

#### 3.2.3 Balance-Aware Tokens Optimization

In MaskDINO, initial feature tokens \(_{i}\) directly serve as the _Key&Value_ of _TD_. Instead, we design _BATO_ module that makes use of both balance-aware query \(_{bal}\) and \(_{i}\) to generate the _Key&Value_ of _TD_. \(_{i}\) contains a large number (\(\) 20k) of tokens conveying detailed local information for both background and objects/instances, while \(_{bal}\) consists of a small number (=300) of high-confidence tokens mainly focusing on objects/instances. In addition, benefiting from the _token interaction_ (i.e., Eq. 2), \(_{bal}\) has learned rich semantic and contextual interaction relations. Therefore, \(_{bal}\) is used to guide the optimization of \(_{i}\). The optimized feature tokens (denoted as \(_{bal}\)) is taken as the _Key&Value_ of _TD_.

Firstly, to provide guidance for both detection and segmentation, the mask network and box network are separately applied on \(_{bal}\) to generate mask guiding token \(_{g}^{mask}\) and box guiding token \(_{g}^{box}\):

\[_{g}^{mask} =_{mask}(_{bal}),\] (5) \[_{g}^{box} =_{box}(_{bal}),\] (6)

where \(_{mask}\) and \(_{box}\) indicate the mask network and box network, respectively. Both \(_{mask}\) and \(_{box}\) consist of a \(mlp\) network.

Then, the overall guiding token \(_{g}\) is obtained by fusing \(_{g}^{mask}\) and \(_{g}^{box}\) :

\[_{g}=_{g}^{mask}+_{g}^{box}.\] (7)

Finally, \(_{g}\) guides the optimization of the initial feature tokens \(_{i}\) through a Multi-Head Cross-Attention. The motivation is straightforward. Same with \(_{bal}\), each token in \(_{g}\) corresponds to an object/instance candidate. When \(_{i}\) interacts with \(_{g}\), the tokens (in \(_{i}\)) that belong to the same object/instance will be aggregated, enhancing the foreground information. For a better comprehension, a token in \(_{g}\) could be assumed as the center of a "cluster", and the tokens (in \(_{i}\)) belonging to the same object/instance could be assumed as the points in the "cluster". The points move towards the "cluster" center, realizing the optimization of \(_{i}\). This procedure is formulated as follows:

\[_{bal}=(_{i},_{g}),\] (8)

generating balance-aware feature tokens \(_{bal}\) (also called optimized feature tokens), which serve as the _Key&Value_ of _TD_.

#### 3.2.4 Transformer Decoder

_TD_ is responsible for the predictions of instance mask, object box, and class. _TD_ consists of decoder layers and each layer contains a self-attention, a cross-attention, and a FFN. The inputs of _TD_ are \(_{bal}\) (in Eq. 8) and \(_{bal}\) (in Eq. 4). \(_{bal}\) interacts with \(_{bal}\) in the decoder layers, continuously refining the query:

\[_{ref}=_{de}(_{bal},_{bal}),\] (9)

where \(_{ref}\) is the refined query, and \(_{de}\) denotes the transformer decoder network.

Subsequently, we follow the detection head and segmentation head structures of MaskDINO to perform object detection and instance segmentation. For object detection, \(_{ref}\) is used to predict the categories \(\) and bounding boxes \(\):

\[\{,\}=_{det}(_{ref}),\] (10)

where \(_{det}\) denotes the detection head network. For instance segmentation, \(_{ref}\), \(_{i}\), and the 1/4 resolution CNN feature \(_{cnn}\) are used to predict the instance masks \(\):

\[=_{seg}(_{ref},_{i},_{cnn}),\] (11)

where \(_{seg}\) denotes the segmentation head network.

## 4 Experiments

### Settings

We conduct extensive experiments on COCO  and BDD100K  datasets using ResNet50  backbone pretrained on ImageNet-1k  as well as SwinL  backbone pretrained on ImageNet-22k. NVIDIA RTX3090 GPUs are used when the backbone is ResNet50. Due to the large memoryrequirement of SwinL, NVIDIA RTX A6000 GPUs are used when the backbone is SwinL. More implementation details are in Appendix B.

### Comparison Experiments

MaskDINO is the SOTA model for joint object detection and instance segmentation, thus we mainly compare our model with MaskDINO under different backbones (ResNet50 and SwinL). Additionally, our model is compared with some classical (i.e., Mask RCNN ) and recently-proposed (i.e., HTC  and SOLQ ) joint object detection and instance segmentation models. Furthermore, our model is compared with SOTA model that is specifically designed for object detection (i.e., DINO ) and instance segmentation (i.e., Mask2Former ). The comparison results on COCO dataset are summarized in Tab. 1. It is noted that the experiments with the Swin-L backbone are conducted on the A6000 GPUs with the batch size of 4 (the maximum batch size that 4 A6000 GPUs supports). The batch size is smaller than that in MaskDINO paper (i.e., batch size = 16) and the 4 A6000 GPUs present weaker computation power than 4 A100 GPUs, thus the results we reproduced are lower than those in the original MaskDINO paper. We can observe that **1)** our model surpasses MaskDINO under different training conditions (epoch = 12, 24, and 50). Notably, our model presents more significant advantage with the training condition of epoch = 12, which potentially reveals that our model reaches the convergence with a faster speed; **2)** under the Swin-L backbone, **DI-MaskDINO** exhibits significant superiority over MaskDINO, further confirming the effectiveness of our model; **3)** the performance of our model on individual detection and segmentation tasks is simultaneously higher than that of SOTA models specifically designed for detection (i.e., DINO) and segmentation (i.e., Mask2Former) when they are fully trained (epoch = 36 or 50), which is really hard-won since the single-task model usually designs the specific module for the specific task (e.g., DINO uses the tailored query formulation to improve the detection performance and Mask2Former proposes tailored masked attention module to improve the segmentation performance).

   Methods & Epochs & \(AP^{box}\) & \(AP^{box}_{S}\) & \(AP^{box}_{M}\) & \(AP^{box}_{L}\) & \(AP^{mask}\) & \(AP^{mask}_{S}\) & \(AP^{mask}_{M}\) & \(AP^{mask}_{L}\) & FPS \\ 
**ResNet50 backbone** & & & & & & & & & & \\  Mask RCNN  & 36 & 41.0 & 24.9 & 43.9 & 53.3 & 37.2 & 18.6 & 39.5 & 53.3 & 21.0 \\ HTC  & 36 & 44.9 & - & - & - & 39.7 & 22.6 & 42.2 & 50.6 & 5.5 \\ SOLQ  & 50 & 48.5 & 30.1 & 51.6 & 64.8 & 40.1 & 20.9 & 43.7 & 59.4 & 7.0 \\  DINO  & 36 & 50.9 & 34.6 & 54.1 & 64.6 & - & - & - & - & 6.8 \\ Mask2Former  & 50 & - & - & - & - & 43.7 & 23.4 & 47.2 & 64.8 & 5.3 \\  MaskDINO  & 12 & 45.7 & - & - & - & 41.4 & 21.1 & 44.2 & 61.4 & 8.0 \\ DI-MaskDINO (Ours) & 12 & **46.9 (+1.2)** & 28.8 & 49.5 & 62.9 & **42.3 (+0.9)** & 22 & 44.8 & 62.8 & 7.7 \\ MaskDINO (C2) & 24 & 48.4 & - & - & - & 44.2 & 23.9 & 47.0 & 64.0 & 8.0 \\ DI-MaskDINO (Ours) & 24 & **49.6 (+1.2)** & 31.7 & 52.6 & 65.3 & **44.8 (+0.6)** & 24.3 & 47.8 & 65.0 & 7.7 \\ MaskDINO (25) & 50 & 51.7 & 34.2 & 54.7 & 67.3 & 46.3 & 26.1 & 49.3 & 66.1 & 8.0 \\ DI-MaskDINO (Ours) & 50 & **51.9 (+0.2)** & 36.3 & 54.7 & 66.7 & **46.7 (+0.4)** & 27.5 & 49.8 & 66.2 & 7.7 \\ 
**Swinl backbone** & & & & & & & & & & \\  MaskDINO  & 12 & 52.2 & 34.8 & 55.6 & 69.9 & 47.2 & 26.3 & 50.3 & 69.1 & 3.4 \\ DI-MaskDINO (Ours) & 12 & **53.3 (+1.1)** & 36.7 & 56.7 & 70.4 & **47.9 (+0.7)** & 27.7 & 51.1 & 69.3 & 3.0 \\  MaskDINO  & 50 & 56.8 & 40.2 & 60.2 & 72.3 & 51.0 & 31.3 & 54.1 & 71.2 & 3.4 \\ DI-MaskDINO (Ours) & 50 & **57.8 (+1.0)** & 41.5 & 61.2 & 73.9 & **51.8 (+0.8)** & 31.8 & 55.1 & 72.2 & 3.0 \\   

Table 1: Comparison with other methods on the COCO validation set.

   Methods & Epochs & \(AP^{box}\) & \(AP^{box}_{S}\) & \(AP^{box}_{M}\) & \(AP^{box}_{L}\) & \(AP^{mask}\) & \(AP^{mask}_{S}\) & \(AP^{mask}_{M}\) & \(AP^{mask}_{L}\) & FPS \\ 
**ResNet50 backbone** & & & & & & & & & & \\  Mask RCNN  & 50 & 25.5 & 15.7 & 32.8 & 56.1 & 20.7 & 15.7 & 26.6 & 49.1 & 20.1 \\ HTC  & 50 & 26.9 & 15.7 & 35.4 & 55.3 & 21.1 & 11.0 & 26.7 & 46.4 & 4.8 \\ SOLQ  & 50 & 27.0 & 16.5 & 35.3 & 45.6 & 19.6 & 10.1 & 25.8 & 37.4 & 6.3 \\  DINO  & 36 & 28.9 & 18.0 & 37.0 & 48.1 & - & - & - & - & 6.1 \\ Mask2Former  & 50 & - & - & - & - & 19.6 & 8.4 & 25.9 & 41.0 & 4.7 \\  MaskDINO  & 68 & 28.1 & 17.4 & 36.1 & 47.9 & 25.3 & 14.2 & 31.8 & 48.1 & 6.7 \\ DI-MaskDINO (Ours) & 68 & **29.5 (+1.4)** & 18.0 & 37.4 & 50.4 & **25.7 (+0.4)** & 14.5 & 32.1 & 48.1 & 6.4 \\ 
**Swinl backbone** & & & & & & & & & & \\  MaskDINO & 68 & 30.2 & 19.0 & 37.5 & 48.6 & 27.0 & 15.4 & 32.6 & 50.5 & 3.2 \\ DI-MaskDINO (Ours) & 68 & **31.4 (+1.2)** & 19.4 & 40.4 & 48.7 & **27.9 (+0.9)** & 16.6 & 34.1 & 51.2 & 2.8 \\   

Table 2: Comparison with other methods on the BDD100K validation set.

Existing joint detection and segmentation models like [5; 10; 50] only conduct the experiments on COCO dataset. In this paper, to further verify the robustness and generalization of our model, additional experiments are conducted on more complex traffic scene dataset BDD100K  using ResNet50 and Swin-L backbones, and the results we reproduce are shown in Tab. 2. Due to the complexity of traffic scenes, the overall performance is lower than the performance on COCO dataset, and the model asks for more training epochs (epoch = 68) to reach the convergence. It can be observed that our model still exhibits superiority over MaskDINO, DINO, and Mask2Former, which presents the robustness and generalization of our model. It should be noted that the performance of MaskDINO on detection task is lower than that of the specialized object detection model DINO, indicating that DINO still exhibits the advantage in complex traffic scene datasets. In contrast, our model improves DINO by 0.6 \(AP^{box}\), further demonstrating the effectiveness of our model.

### Diagnostic Experiments

#### 4.3.1 Imbalance Tolerance Test

There exists the natural imbalance between object detection and instance segmentation, and we are interested in how will a model perform if the imbalance condition is worsened. Therefore, we conduct the imbalance tolerance test by designing two severe imbalance conditions: **1)**_loss weight constraint_, which is implemented by constraining the weight of detection loss to 1/10 of the default value while the weight of segmentation loss remains unchanged; **2)**_position token constraint_, position token conveys important cues of object locations, thus constraining position token will generate disturbing location information to confuse detection task. The position token constraint is implemented by randomly initializing position token of \(_{bal}\) (composed of position token and content token) in Eq. 4. _DI_ module is mainly responsible for alleviating imbalance issue, thus the imbalance tolerance test on **DI-MaskDINO** only enables _DI_ module. The experiments are conducted on more complex BDD100K dataset, because the results on the more complex dataset can better reflect the performance of imbalance tolerance. Considering the imbalance issue is severe at the beginning decoder layer, thus the experiments utilize models configured with 3 decoder layers.

The results of imbalance tolerance test are summarized in Tab. 3, and the percentage of performance drops (compared with standard condition) is highlighted in colors. We can observe that **1)** the imbalance between detection and segmentation has remarkable affect on the upper bound of model performance, potentially indicating the significance of our work; **2)** the effects of imbalance conditions on detection task are larger than that on segmentation task, because the two imbalance conditions are implemented to mainly constrain the detection task to simulate the natural detection-segmentation imbalance; **3)** even the performance of SOTA model MaskDINO is largely affected by the imbalance conditions (e.g., **-21.8%**\(AP^{box}\) degradation on the condition of position token constraint), which potentially reflects that _De-Imbalance_ deserves the research focus; **4)** compared with MaskDINO, the performance degradation of our model is slighter (i.e., **-10.2%** v.s. **-2.9%** and **-21.8%** v.s. **-14.7%** on the \(AP^{box}\) metric, -3.0% v.s. +1.2% and -7.6% v.s. **-4.8%** on the \(AP^{mask}\) metric), which demonstrates the effectiveness of our model; **5)** from a comprehensive perspective, we think the standard condition is still a detection-segmentation imbalance condition (which is commonly treated as a regular condition in previous works), and we claim the imbalance is one of the cruxes that limit the upper bound of model performance, hence it should be further studied.

#### 4.3.2 Diagnostic Experiments on Main Modules

To test the effects of main modules in our model (i.e., _DI_ and _BATO_), we test the performance of our model under four configurations: **#1** exclusion of both _DI_ and _BATO_; **#2** exclusion of _BATO_; **#3** exclusion of _DI_; **#4** inclusion of both _DI_ and _BATO_. To make the results solid, the experiments are conducted on both BDD100K and COCO datasets, and the results are reported in Tab. 4.

    &  &  \\   & \(AP^{box}\) & \(AP^{mask}\) & \(AP^{box}\) & \(AP^{mask}\) \\  standard & 27.5 & 23.7 & 27.9 & 24.9 \\ loss weight constraint & 24.7 (**-10.2\%**) & 23 (-3.0\%) & 27.1 (**-2.9\%**) & 25.2 (+1.2\%) \\ position token constraint & 21.5 (**-21.8\%**) & 21.9 (**-7.6\%**) & 23.8 (**-14.7\%**) & 23.7 (**-4.8\%**) \\   

Table 3: Imbalance tolerance test comparison of MaskDINO and **DI-MaskDINO**.

In comparison with **#1**, the model under the configuration of **#2** or **#3** yields higher performance on both datasets, and the optimum results are achieved when both _DI_ and _BATO_ are enabled (**#4**). These results demonstrate the effectiveness of _DI_ and _BATO_. The results are explainable. _DI_ module alleviates the imbalance between detection and segmentation, generating balance-aware query \(_{bal}\), which is then fed to _BATO_ to further make use of balance-aware information, contributing to performance improvement.

#### 4.3.3 Diagnostic Experiments on _Di_ Module

The core of our model is _DI_, which improves the model performance by mitigating the imbalance between detection and segmentation. _DI_ is realized by applying the _residual double-selection_ mechanism on \(_{i}\), generating \(_{s1}\) (firstly-selected tokens), \(_{s2}\) (secondly-selected tokens), and \(_{bal}\) (balance-aware query). To analyze _DI_ module, we design the fine-grained ablation experiments by respectively using \(_{i}\), \(_{s1}\), \(_{s2}\), and \(_{bal}\) as the guidance for _BATO_ (i.e., \(_{:}=_{i}\), \(_{:}=_{s1}\), \(_{:}=_{s2}\), and \(_{:}=_{bal}\)) and examine the corresponding performance.

The experiment results on BDD100K and COCO datasets are reported in Tab. 5. \(_{:}=_{i}\) actually represents the situation when _DI_ module is disabled, which serves as the baseline for other situations. Firstly, we can observe \((_{:}=_{s2})>(_{:}=_{s1})> (_{:}=_{i})\) where \((*)\) denotes the performance of the model under the configuration \(*\), demonstrating our _double-selection_ mechanism is effective. The reason is intuitive, by applying _double-selection_ mechanism, the tokens with high confidence are selected, and high-confidence tokens indicate the location of objects more clearly than other tokens, thus benefiting the object detection task (i.e., mitigating the imbalance between detection and segmentation). Secondly, the highest performance is achieved when \(_{:}=_{bal}\), validating the effectiveness of our _residual double-selection_ mechanism. In _DI_ module, apart from the secondly-selected tokens \(_{s2}\), the initial feature tokens \(_{i}\) is also used to compute \(_{bal}\), which could be coarsely formulated as \(_{bal}=_{i}+(_{i})\). This residual structure enables the model to make use of the information in both the initial feature tokens and the selected feature tokens, hence reaching the optimal performance.

#### 4.3.4 Diagnostic Experiments on _Bato_ Module

_BATO_ targets to use the balance-aware query \(_{bal}\) to guide the optimization of the initial feature tokens \(_{i}\). The effectiveness of _BATO_ has been validated in Tab. 4. We further conduct experiments to validate the effect of the proposed guiding token generation (GTG). The GTG is designed to provide guidance for both detection and segmentation, generating mask guiding token and box guiding token that are closely related to mask instances and object boxes through the mask network and box network, respectively. These guiding tokens can provide more global and semantic guiding information for the optimization of the initial feature tokens \(_{i}\). As shown in Tab. 6, the model with GTG performs better, which demonstrates the effect of GTG.

    &  &  \\   & \(AP^{box}\) & \(AP^{mask}\) & \(AP^{box}\) & \(AP^{mask}\) \\  w/o GTG & 28.6 & 25.4 & 46.5 & 42.2 \\ w/ GTG & **29.5** & **25.7** & **46.9** & **42.3** \\   

Table 6: The results of diagnostic experiments on _BATO_ module.

    &  &  &  &  \\   & & & \(AP^{box}\) & \(AP^{mask}\) & \(AP^{box}\) & \(AP^{mask}\) \\ 
**\#1** & - & - & 27.8 & 24.4 & 45.6 & 41.2 \\
**\#2** & _笨点 & - & 28.8 & 25.2 & 46.4 & 42.1 \\
**\#3** & - & _笨点 & 28.3 & 24.9 & 46.2 & 41.8 \\
**\#4** & _笨点 & _笨点 & **29.5** & **25.7** & **46.9** & **42.3** \\   

Table 4: The results of diagnostic experiments on main modules. The experiments are conducted on the BDD100K dataset with 68 training epochs and on COCO dataset with 12 training epochs. The results in Tab. 5 and Tab. 6 are also obtained under the same experiment settings.

    &  &  \\   & \(AP^{box}\) & \(AP^{mask}\) & \(AP^{box}\) & \(AP^{mask}\) \\  \(_{:}=_{i}\) & 28.3 & 24.9 & 46.2 & 41.8 \\ \(_{:}=_{s1}\) & 28.5 & 24.9 & 46.6 & 42.0 \\ \(_{:}=_{s2}\) & 28.9 & 25.6 & 46.7 & 42.2 \\ \(_{:}=_{bal}\) & **29.5** & **25.7** & **46.9** & **42.3** \\   

Table 5: The results of diagnostic experiments on _DI_ module. \(gui\). denotes the **guidance** in Fig. 2.

Conclusion

In this paper, we investigate the naturally-existing but commonly-ignore detection-segmentation imbalance issue. The imbalance means that the performance of object detection lags behind that of instance segmentation at the beginning layer of transformer decoder, which is one of cruxes that hurt the cooperation of object detection and instance segmentation tasks and might constrain the break-through of the performance upper bound. To address the issue, we propose **DI-MaskDINO** model with the _residual double-selection_ mechanism to alleviate the imbalance, achieving significant performance improvements compared with SOTA joint object detection and instance segmentation model MaskDINO, SOTA object detection model DINO, and SOTA segmentation model Mask2Former.

**Limitations.** This paper focuses on the task of joint object detection and instance segmentation, thus the model is not applicable for other segmentation tasks such as semantic segmentation and panoptic segmentation.