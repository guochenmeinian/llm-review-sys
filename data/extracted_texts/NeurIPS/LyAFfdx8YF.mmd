# PEAC: Unsupervised Pre-training for

Cross-Embodiment Reinforcement Learning

 Chengyang Ying Zhongkai Hao Xinning Zhou Xuezhou Xu\({}^{1}\)

Hang Su\({}^{1,2}\) Xingying Zhang Jun Zhu\({}^{1,2}\)

\({}^{1}\)Department of Computer Science & Technology, Institute for AI, BNRist Center,

Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University

\({}^{2}\)Pazhou Lab (Huangpu), Guangzhou, China

ycy21@mails.tsinghua.edu.cn

Corresponding author

###### Abstract

Designing generalizable agents capable of adapting to diverse embodiments has achieved significant attention in Reinforcement Learning (RL), which is critical for deploying RL agents in various real-world applications. Previous Cross-Embodiment RL approaches have focused on transferring knowledge across embodiments within specific tasks. These methods often result in knowledge tightly coupled with those tasks and fail to adequately capture the distinct characteristics of different embodiments. To address this limitation, we introduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which leverages unsupervised learning to enable agents to acquire embodiment-aware and task-agnostic knowledge through online interactions within reward-free environments. We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP) and systematically analyze CEURL's pre-training objectives under CE-MDP. Based on these analyses, we develop a novel algorithm Pre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating an intrinsic reward function specifically designed for cross-embodiment pre-training. PEAC not only provides an intuitive optimization strategy for cross-embodiment pre-training but also can integrate flexibly with existing unsupervised RL methods, facilitating cross-embodiment exploration and skill discovery. Extensive experiments in both simulated (e.g., DMC and Robosuite) and real-world environments (e.g., legged locomotion) demonstrate that PEAC significantly improves adaptation performance and cross-embodiment generalization, demonstrating its effectiveness in overcoming the unique challenges of CEURL. The project page and code are in https://yingchengyang.github.io/ceurl.

## 1 Introduction

Cross-embodiment reinforcement learning (RL) involves designing algorithms that effectively function across various physical embodiments. The fundamental goal is to enable agents to apply skills and strategies learned from some embodiments to other embodiments, which may own different physical dynamics, action-effectors, shapes, and so on . This capability significantly enhances the generalization of RL agents, reducing the necessity for embodiment-specific training. By adeptly adapting to new and shifting embodiment, cross-embodiment RL ensures that agents maintain reliable performance in unpredictable real-world scenarios, thereby benefiting the deployment process and reducing the need for extensive data collection for each new embodiment.

One of the primary challenges in this area is the transfer of knowledge across embodiments that have vastly different physical dynamics and environmental interactions. This requires the agent to abstract knowledge in a way that is not overly specialized to a single embodiment or some downstream tasks. However, directly training cross-embodiment agents under some given tasks will cause the learned knowledge highly related to these tasks rather than only to embodiments themselves.

Inspired by the transformative effects of unsupervised learning in natural language processing and computer vision [6; 21], which has demonstrated efficiency in extracting generalized knowledge independent of downstream tasks, we propose a natural question: _Can we pre-train cross-embodiment agents in an unsupervised manner, i.e., online cross-embodiment pre-training in reward-free environments, to capture generalized knowledge only related to embodiments?_ Existing unsupervised RL techniques, including exploration [45; 38] and skill discovery [10; 26] ones, typically involve pre-training agents by engaging a single embodiment within a controlled Markov Decision Process (MDP) that lacks extrinsic reward signals. These pre-trained agents are then expected to quickly fine-tune to any downstream tasks characterized by extrinsic rewards using this specific embodiment. This approach of unsupervised RL fosters the development of policies that are not overly specialized to specific tasks or reward structures but are rather driven by intrinsic motivations of embodiments, which shows the potential for discovering more generalized knowledge across different embodiments.

In this work, we adapt the unsupervised RL paradigm to the cross-embodiment setting, introducing the concept of Cross-Embodiment Unsupervised RL (CEURL). This setting involves pre-training with a distribution of embodiments in reward-free environments, followed by fine-tuning to handle specific downstream tasks through these embodiments. These embodiments may own similar structures so that we can abstract generalized knowledge from them. To analyze CEURL and design corresponding algorithms, we formulate it as a Controlled Embodiment Markov Decision Process (CE-MDP), which comprises a distribution of controlled MDPs, each defined by its unique embodiment context. Compared to the traditional single-embodiment setting, the CE-MDP framework addresses the additional complexity caused by the inherent variability among embodiments. We then extend the information geometry analyses of the controlled MDP  to better explain the complexity of CE-MDP. Our findings indicate that skill vertices within CE-MDP may no longer be simple deterministic policies and the behaviors across different embodiments can display substantial variability.

To address the complexities of CE-MDP, we undertake an in-depth analysis of the pre-training objective in CE-MDP. We aim to enable our pre-trained agent to quickly fine-tune for any downstream tasks denoted as \(_{}\), especially under the worst-case reward scenarios. Thus, our pre-training objective involves minimizing across \(_{}\) while maximizing the fine-tuned policy \(^{*}\), leading to a complex min-max problem (Eq. 3). We further introduce a novel Pre-trained Embodiment-Aware Control (PEAC) algorithm to optimize this objective and handle CE-MDP, which improves the agent's robustness and adaptability across various embodiments by employing a cross-embodiment intrinsic reward \(_{}\). This reward is complemented by an embodiment discriminator, which distinguishes between different embodiments. During fine-tuning, the pre-trained policy is further enhanced under the extrinsic reward, \(_{}\) with limited timesteps. Moreover, PEAC can integrate flexibly with existing single-embodiment unsupervised RL methods to achieve cross-embodiment exploration and skill discovery, resulting in two combination algorithm examples PEAC-LBS and PEAC-DIAYN.

Figure 1: **Overview of Cross-Embodiment Unsupervised Reinforcement Learning (CEURL). The left subfigure illustrates the cross-embodiment setting with various possible embodiment changes. Directly training RL agents across embodiments under given tasks may result in task-aware rather than embodiment-aware knowledge. CEURL pre-trains agents in reward-free environments to extract embodiment-aware knowledge. The center subfigure shows the Pre-trained Embodiment-Aware Control (PEAC) algorithm, using our cross-embodiment intrinsic reward function \(_{}()\). The right subfigure demonstrates the fine-tuning phase, where pre-trained agents fast adapt to different downstream tasks, improving adaptation and generalization.**

To verify the versatility and effectiveness of our algorithm, we extensively evaluate PEAC in both simulated and real-world environments. In simulations, we choose state-based / image-based Deep-Mind Control Suite (DMC) environments extending Unsupervised RL Benchmark (URLB)  and different robotic arms in Robosuite . Under these settings, PEAC demonstrates superior few-shot learning ability to downstream tasks, and remarkable generalization ability to unseen embodiments, surpassing existing state-of-the-art unsupervised RL models. Besides, we have evaluated PEAC in real-world Aliengo robots by considering practical joint failure settings based on Isaacgym , verifying PEAC's strong adaptability on different joint failures and various real-world terrains.

In summary, the main contributions are as follows:

* We propose a novel setting CEURL to enhance agents' adaptability and generalization across diverse embodiments, and then we introduce the Pre-trained Embodiment-Aware Control (PEAC) algorithm for handling CEURL.
* We integrate PEAC with existing exploration and skill discovery techniques, designing practical methods and facilitating efficient cross-embodiment exploration and skill discovery.
* Extensive experiments show that PEAC not only excels in fast fine-tuning but also effectively generalizes across new embodiments, outperforming current SOTA unsupervised RL models.

## 2 Related Work

Cross-Embodiment RL.Designing generalizable agents simultaneously controlling diverse embodiments has achieved significant attention in RL. A common strategy involves using expert trajectories [70; 49; 5; 66; 60], internet-scale human videos [3; 58; 13], or offline datasets [29; 8; 41] to train a generalist agent that can handle various tasks across different embodiments. However, these methods are often limited by the need for large-scale, costly datasets and the availability of expert trajectories. Additionally, the discrepancy between open-loop training and closed-loop testing may lead to distribution shifts , adversely affecting the final performance. An alternative line of research [28; 36; 64; 4; 66; 52; 12; 68] focuses on training general agents through online interaction across diverse environments. However, these methods treat the embodiment and task as a unified training environment and overlook the role of proprioception, i.e., the internal understanding of an agent's embodiment, which has recently proven to be beneficial for representation learning and optimization in RL [23; 15]. Thus these methods may not fully capture the intrinsic properties of different embodiments by linking knowledge to specific tasks. Emerging research suggests the potential of decoupling the training of embodiment characteristics from task execution, aiming to develop a unified cross-embodiment model. This involves unsupervised pre-training across a variety of embodiments, followed by task-aware fine-tuning, enabling a single agent to adeptly manage both roles effectively.

Unsupervised RL.Unsupervised RL leverages interactions with reward-free environments to extract useful knowledge, such as exploratory policies, diverse skills, or world models [17; 18]. These pre-trained models are utilized to fast adapt to downstream tasks within specific embodiments and environments. Unsupervised RL methods can be categorized into two main types: exploration and skill discovery. Exploration methods aim to maximize state coverage, typically through intrinsic rewards that encourage uncertainty [45; 7; 46; 51; 38; 40; 67] or state entropy [30; 47; 35; 34]. The resulting exploratory trajectories benefit pre-training actor-critic or world models, thereby enhancing fine-tuning efficiency . Skill discovery methods focus on learning an array of distinguishable skills, often by maximizing the mutual information between states and acquired skills [10; 54; 20; 55; 25; 26; 72; 24; 61]. This approach benefits from theoretical insights into the information geometry of skill state distributions, emphasizing the importance of maximizing distances between different skills [11; 22; 42; 43; 44]. Recent efforts also explore incremental skill learning in dynamic environments [53; 31]. Unlike these methods generally focus on single embodiments, we aim to develop generalizable models capable of handling downstream tasks across a variety of embodiments.

## 3 Cross-Embodiment Unsupervised RL

In this section, we analyze the cross-embodiment RL in an unsupervised manner, which is formulated as our Controlled Embodiment MDP. Then we propose a novel algorithm PEAC to optimize CE-MDP.

### Controlled Embodiment Markov Decision Processes

Cross-embodiment RL can be formulated by contextual MDP  with a distribution of Markovian decision processes (MDPs) of \(\{_{}\}\). Cross-embodiment RL hopes to learn shared knowledge from this distribution of MDPs, which is crucial for enhancing the adaptability or generalization of agents across embodiments. However, directly optimizing agents by online interacting with \(\{_{}\}\) or utilizing offline datasets sampled from \(\{_{}\}\) may learn knowledge not only related to these embodiments but also highly related to these task reward functions in \(\{_{}\}\). This phenomenon may have negative impacts on learning the general knowledge across embodiments or improving the agent's generalization ability. For example, as the agent is required to handle \(\{_{}\}\), it will less explore the trajectories with low rewards. These trajectories, although not optimal for the embodiment in this task, might also include embodiment knowledge and be useful for other tasks. Without extrinsic task rewards, the agent is encouraged to learn embodiment-aware and task-agnostic knowledge, which can effectively adapt to any downstream task across embodiments.

In this paper, we propose to pre-train cross-embodiment agents in reward-free environments to ensure that the agent can learn knowledge only specialized in these embodiments themselves. In other words, we introduce unsupervised RL into cross-embodiment RL as a novel setting: _cross-embodiment unsupervised RL_ (CEURL). As shown in Fig. 1, in CEURL, we first pre-train a general agent by interacting with the reward-free environment through varying embodiments sampled from an unknown embodiment distribution. Given any downstream task represented by the extrinsic reward \(_{}\), the pre-trained agent is subsequently fine-tuned to control these embodiments, and other unseen embodiments from the distribution, to complete this task within limited steps (like one-tenth of the pre-training steps). Formally, we formulate CEURL as the following controlled embodiment MDP (CE-MDP):

**Definition 3.1** (**Controlled Embodiment MDP (CE-MDP)**).: _A CE-MDP includes a distribution of controlled MDPs defined as \(_{}^{c}=(_{},_{},_{},)\), where \(\) and \(\) represents the embodiment distribution. Each embodiment may have different state spaces \(_{}\) and action spaces \(_{}\). \(_{}:_{}_{}( _{})\) denoting the transition dynamics for embodiment \(\) and \(\) is the discount factor. We define the state space \(=_{}_{}\) and adopt a unified action embedding space \(\) with corresponding action projectors \(_{}:_{}\), which can be fixed or learnable._

Thus we can establish a unified policy \(:()\) across all embodiments. For any embodiment \(\), we sample an action \(\) from \((|)\) for a state \(_{}\) and execute the projected action \(_{}()\). Without loss of generality, we assume \(_{}\) is fixed and focus our analysis on the policy \(\). To explain the complexities of CE-MDP with varying embodiment contexts, we extend the single-embodiment information geometry analyses  into our cross-embodiment setting. First, we consider the discount state distribution of \(\) within \(_{}^{c}\) at state \(\) as \(d_{}^{}()=(1-)_{t=0}^{}[^{t}_{}(_{t}=)]\). It is well known that the trajectory return of the state-based reward function can be computed as

\[J_{_{}^{c},_{}}()_{_{}^{c},}[_{}() ]=_{s d_{}^{c}}[_{ }()].\] (1)

Thus, the properties of \(d_{}^{c}\) are significant in determining useful initializations for downstream tasks. We consider the set \(^{}=\{d_{}^{e}()\}\), which includes all feasible \(d_{}^{}\) over the probability simplex. As shown in , for each \(\), \(^{}\) is a convex set, and any useful policy, which can be optimal for certain downstream tasks under embodiment \(\), must be a vertex of \(^{}\), typically corresponding to deterministic policies.

However, in the context of CEURL, the unknown embodiment context \(e\) introduces partial observability  and significant differences in the corresponding points of the same skill across different embodiments. In CE-MDP, we consider the entire embodiment space and define \(d_{}^{}()=_{}[d_{}^ {}()]\), with \(^{}=\{d_{}^{}() \}\). The primary challenge lies in the high variability of embodiments, which complicates the process of learning a policy that generalizes well across different embodiments. We demonstrate that the vertices of \(^{}\) may no longer correspond to deterministic policies, as they need to handle all embodiments in the distribution. This significantly heightens the challenge of the pre-training process in CE-MDP, making it more difficult to find useful cross-embodiment skills (proofs and discussion in Appendix A.1).

To solve CEURL under the paradigm of CE-MDP, the agent will collect reward-free trajectories \(=(_{0},_{0},_{1},...)\) with probability \(p_{_{}^{c},}()=_{}(_{0})_ {t=0}(_{t}|_{t})_{}(_{t+1}|_{t}, _{t})\) via somesampled embodiments \(\) during the pre-training. These trajectories are then used in CEURL methods to design intrinsic rewards \(_{}\) for pre-training agents. During fine-tuning, we will sample several embodiments \(\) from \(\) and combine \(_{}^{c}\) with a downstream task represented by extrinsic rewards \(_{}\), and agents are required to maximize the task return over all embodiments, i.e., \(_{}[J_{_{}^{c},_{}}()]\), within limited steps (like one-tenth or less of the pre-training steps).

### Pre-trained Embodiment-Aware Control

We primarily focus on the pre-training objective of CEURL, specifically determining the optimal pre-trained policy \(\) for CEURL. In the fine-tuning stage, given any downstream task characterized by extrinsic reward \(_{}\), the pre-trained policy \(\) will be optimized into the fine-tuned policy \(^{*}\) with _limited_ steps to handle \(_{}\) via some RL algorithms like PPO . Consequently, it is widely assumed that \(^{*}\) will remain close to \(\) during fine-tuning due to constraints on limited interactions with the environment . Our _cross-embodiment fine-tuning objective_ thus combines _policy improvement_ under \(_{}\) and a _policy constraint_ evaluated via KL divergence

\[(,^{*},_{},) {_{_{_{}^{c},^{*}}()}[ _{}()]-_{_{_{}^{c},}( )}[_{}()]}_{}-}(p_{_{}^{c},^{*}}()\|p_{_{},}())}_{},\] (2)

where \(>0\) is the unknown trade-off parameter related to the fine-tuning steps (when fine-tuning steps tend towards infinity, \(\) tends to 0 and this objective converges to the original RL objective), and \(}\) represents the "average embodiment MDP" satisfying that \(p_{,}()=_{}[p_{_{}^{c},}()]\). During fine-tuning, we hope to optimize \(^{*}\) by maximizing \(\), i.e., the fine-tuned result is \(_{p_{_{}^{c},^{*}}()}(,^{*}, _{},)\). As the pre-trained policy \(\) needs to handle _any_ downstream task, we consider the worst-case extrinsic reward function across the embodiment distribution, and our _cross-embodiment pre-training objective_ can be formally represented as maximizing

\[(,)_{}[ _{_{}()\,p_{_{}^{c},^{*}}( )}(,^{*},_{},)].\] (3)

This objective is a min-max problem that is hard to optimize. Fortunately, we can simplify it as below

**Theorem 3.2** (Proof in Appendix A.2).: _The pre-training objective Eq. (3) of \((,)\) satisfies_

\[(,)=_{}[- D_ {}(p_{_{}^{c},}()\|p_{}, }())]=_{}_{  p_{_{}^{c},}()}[)}{p_{ }(|)}].\] (4)

Here \(p()\) and \(p_{}(|)\) are embodiment prior and posterior probabilities, respectively. This result simplifies our pre-trained objective as a form easy to calculate and optimize. Also, although \(\) is an unknown parameter, the optimal pre-trained policy is independent of \(\). Based on these analyses, we propose a novel algorithm named Pre-trained Embodiment-Aware Control (PEAC). In PEAC, we first train an embodiment discriminator \(q_{}(|)\) to approximate \(p_{}(|)\), which can learn the embodiment context via historical trajectories. For cross-embodiment pre-training, PEAC then utilizes our cross-embodiment intrinsic reward, which is defined following Eq. (4) as

\[_{}() p()- q_{}(| ).\] (5)

Assuming the embodiment prior \(p()\) is fixed, \(_{}\) encourages the agent to explore the region with low \( q_{}(|)\). In these trajectories, the embodiment discriminator is misled, where the agent may not have explored enough or different embodiment posteriors are similar. Thus, the embodiment discriminator can boost itself from these trajectories and learned embodiment-aware contexts that can effectively represent different embodiments, which benefit generalizing to unseen embodiments.

In practice, \(_{}\) needs to be calculated for each state \(\) rather than the whole trajectory \(\), also, the embodiment discriminator needs to classify the embodiment context for every state. For RL backbones that encode historical information as the hidden state \(\) like Dreamer [17; 18; 64], we directly train \(q_{}(|,)\) as the discriminator and further calculate \(_{}\). For RL algorithms with Markovian policies like PPO , we encode a fixed length historical state-action pair to the hidden state \(h\) and also train \(q_{}(|,)\), following . For a fair comparison, our policy still uses Markovian policy and does not utilize encoded historical messages. PEAC's pseudo-code is in Appendix C.

## 4 Cross-Embodiment Exploration and Skill Discovery

As shown above, PEAC pre-trains the agent for the optimal initialization to few-shot handle downstream tasks across embodiments. Besides, although PEAC does not directly explore or discover skills, it is flexible to combine with existing unsupervised RL methods, including exploration and skill discovery ones, to achieve cross-embodiment exploration and skill discovery. Below we will discuss in detail the specific combination between PEAC and these two classes respectively, exporting two practical combination algorithms, PEAC-LBS and PEAC-DIAYN, as examples.

Embodiment-Aware Exploration.Existing exploration methods mainly encourage the agent to explore unseen regions. As PEAC suggests the agent explores the region where the embodiment discriminator is wrong, it is natural to directly combine \(_{}\) and exploration intrinsic rewards to achieve cross-embodiment exploration, i.e., balancing embodiment representation learning and unseen state exploration. As an example, we take LBS , of which the intrinsic reward is the KL divergence between the latent prior and the approximation posterior, as the PEAC-LBS. As \(_{}\) and \(_{}\) are both related to some KL divergence, we can directly add up these two intrinsic rewards with the same weight in PEAC-LBS, of which the detailed pseudo-code is in Appendix C.

Embodiment-Aware Skill Discovery.Single-embodiment skill-discovery mainly maximizes the mutual information between trajectories \(\) and skills \(\) as \((;)=D_{}(p(,)\|p()p())\), which has been shown as optimal initiation to some skill-based adaptation objective . We combine it and our cross-embodiment fine-tuning objective Eq. (2) to propose a unified _cross-embodiment skill-based adaptation objective_ as

\[_{s}(,^{*},_{},)&_{p_{_{}^{c}, ^{*}()}}[_{}()]-_{^{*}}_{ p_{_{}^{c},}(|^{*})}[_{}( )]\\ -& D_{}(p_{_{}^{c}, ^{*}}()\|p_{},}()).\] (6)

Similar to Theorem 3.2, we can define our pre-training objective and simplify it as

\[&_{s}(,)_{ e}_{_{}()}_{p_{_{}^{c}, ^{*}()}}_{s}(,^{*},_{},)\\ =&-_{}_{p(|_{}^{c})}[_{ p_{_{},}} (|)}{p_{}()}+D_{}(p_{}(,| _{}^{c})\|p_{}(|_{}^{c})p_{}( |_{}^{c}))].\] (7)

The proof of Eq. (7) is in Appendix A.3, where we also show it is a general form of Theorem 3.2 and the single-embodiment skill-discovery result . The result of Eq. (7) includes two terms for handling cross-embodiment and discovering skills respectively. In detail, the first term is the same as the objective in Eq. (4), thus we can directly optimize it via PEAC. As the second term is similar to the classical skill-discovery objective \((;)\) but only embodiment-aware, we can extend existing skill-discovery methods into an embodiment-aware version for handling it.

We take DIAYN  as an example, resulting in PEAC-DIAYN. Overall, In the pre-training stage, given a random skill \(\) and an embodiment \(\), we will sample trajectories with the policy \(_{}(|,,)\) that is conditioned on \(z\) and the predicted embodiment context. Then we will train a neural network \(p(,|)\) to jointly predict the current skill and the embodiment. For training the policy, we combine \(_{}\) and \(_{}\) as the intrinsic reward. During fine-tuning, we utilize the embodiment discriminator, mapping observed trajectories to infer the embodiment context. We then train an embodiment-aware meta-controller \((|,)\), which inputs the state and predicted context and then outputs the skill. It extends existing embodiment-agnostic meta-controller  and directly chooses from skill spaces rather than complicated action spaces. The pseudo-code of PEAC-DIAYN is in Appendix C.

Figure 2: **Benchmark environments, including DMC , Robosuite , Isaacgym .**

## 5 Experiments

We now present extensive empirical results to answer the following questions:

* Does PEAC enhance the cross-embodiment unsupervised pre-training for handling different downstream tasks? (Sec. 5.2)
* Can CEURL benefit cross-embodiment RL and effectively generalize to unseen embodiments? (Sec. 5.3)
* Does CEURL advantage to real-world cross-embodiment applications? (Sec. 5.4)

### Experimental Setup

To fully evaluate PEAC in CEURL, we choose extensive benchmarks (Fig. 2), including state-based / image-based Deepmind Control Suite (DMC)  in URLB , Robosuite [73; 69] for robotic manipulation, and Isaacgym  for simulation as well as real-world legged locomotion. Below we will introduce embodiments, tasks, and baselines for these settings, with more details in Appendix B.

State-based DMC & Image-based DMC.These two benchmarks extend URLB , classical single-embodiment unsupervised RL settings. Based on basic embodiments, we change the mass or damping to conduct three distinct embodiment distributions: walker-mass, quadruped-mass, and quadruped-damping, following previous work with diverse embodiments [28; 65]. All downstream tasks follow URLB. These two settings take robot states and images as observations respectively.

In state-based DMC, we compare PEAC with 5 exploration and 5 skill-discovery methods: ICM , RND , Disagreement , ProtoRL , LBS , DIAYN , SMM , APS , CIC , and BeCL , which are standard and SOTA for this setting. For all methods, we take DDPG  as the RL backbone, which is widely used in this benchmark . In image-based DMC, we take 5 exploration baselines: ICM, RND, Plan2Explore , APT , and LBS; as well as 4 skill-discovery baselines: DIAYN, APS, LSD , and CIC. Also, we choose a SOTA baseline Choregmber , which combines exploration and skill discovery. For all methods, we take DreamerV2  as the backbone algorithm, which has currently shown leading performance in this benchmark .

Robosuite.We further consider embodiment distribution with greater change: different robotic arms for manipulation tasks from Robosuite . We pre-train our agents in robotic arms Panda, IIWA, and Kinova3. Besides, we take robotic arm Jaco for evaluating generalization. Following , we take DrQ  as the RL backbone and choose standard task settings: Door, Lift, and TwoArmPegInHole.

Figure 4: Aggregate metrics  in **image-based DMC**. Each statistic for every algorithm has 36 runs (3 embodiment settings \(\) 4 downstream tasks \(\) 3 seeds).

Figure 3: Aggregate metrics  in **state-based DMC**. Each statistic for every algorithm has 120 runs (3 embodiment settings \(\) 4 downstream tasks \(\) 10 seeds).

[MISSING_PAGE_FAIL:8]

**Robosuite.** Besides, we validate PEAC in a more challenging setting Robosuite where different embodiments own different robotic arms (subfigures 3-6 in Fig. 2). As shown in Table 1, PEAC still significantly outperforms all baselines in both training and testing embodiments, demonstrating its powerful cross-embodiment ability and better generalization ability. The detailed results of each robotic arm are in Table 12 of Appendix B.6.

**Ablation studies.** We do several ablation studies in image-based DMC to clarify the contribution of PEAC better. First, we evaluate the effectiveness of pre-trained steps in fine-tuned performance. We pre-train agents for 100k, 500k, 1M, and 2M steps and then fine-tune them for 100k steps. As shown in Fig. 6, all algorithms improve with pre-training timesteps increasing, indicating that cross-embodiment pre-training effectively benefits fast handling downstream tasks. PEAC-LBS becomes the best-performing method from 1M steps on and PEAC-DIAYN significantly exceeds skill discovery methods. This suggests that PEAC excels at handling cross-embodiment tasks with increased pre-training steps. Additional results are in Appendix B.7. Besides pre-training steps, we also do more ablations studies of different components in PEAC to verify their effectiveness in Appendix B.8. For example, we evaluate the stability of PEAC-LBS in DMC-image under different \(\) (we set it as 1.0 in all main experiments), which is the trade-off parameter for balancing the policy improvement term and the policy constraint term. Moreover, we also do an ablation study on our embodiment discriminator to verify the contribution of each component in our PEAC. More results and analyses are in Appendix B.8.

### Generalization to Unseen Embodiments

To answer the second question, we further assess the generalization ability of PEAC to unseen embodiments. First, we directly leverage pre-trained agents to zero-shot sample trajectories with different unseen embodiments and then visualize results through t-SNE  in Fig. 5, where different colored points represent states sampled via different embodiments. As shown in Fig. 5, PEAC-LBS can distinguish different embodiments' states more effectively compared to LBS, which is difficult to distinguish them (more results are in Appendix B.9). Furthermore, we evaluate the generalization ability of fine-tuned agents for all methods by zero-shot evaluating them with unseen embodiments and the same downstream task. In Table 2, we report the detailed generalization results of all 3 domains about state-based DMC and image-based DMC. The results demonstrate that the fine-tuned agents of PEAC can successfully handle the same downstream task with unseen embodiments, which illustrates that PEAC effectively learns cross-embodiment knowledge. Detailed results for each downstream task are in Appendix B.10 (Table 16-17).

### Real-World Applications

To validate CEURL in more realistic settings, we conduct results based on legged locomotion in Isaacym, which is widely used for real-world applications. First, we present simulation results of A1-disabled in Table 1, with 100M pre-train timesteps and 10M fine-tune timesteps. As shown in Table 1, PEAC effectively establishes a good initialization model across embodiments with different joint failures and quickly adapts to downstream tasks, especially for challenging climb and leap tasks.

Besides, we have deployed PEAC fine-tuned agents in real-world Aliengo-disabled robots, i.e., Aliengo robots with different failure joints. As shown in Fig. 7, due to joint failure, the movement ability of the robot is limited compared to normal settings, but the robot still demonstrates strong adaptability on various terrains not seen in simulators. More images and videos of real-world applications are in Appendix B.12.

Figure 6: Ablation studies on pre-training timesteps. Figure 7: Real-world results.

### Limitations and Discussion

In terms of limitations, we assume that different embodiments may own similar structures so that we can pre-train a unified agent for them. As a result, it might be challenging for PEAC to handle extremely different embodiments. Also, existing unsupervised RL methods still struggle to handle more challenging downstream tasks. In Appendix B.11, we take the first step to evaluate several more challenging downstream tasks and more different embodiment distributions, of which the results show that PEAC can still perform better than baselines. Designing more efficient cross-embodiment unsupervised algorithms for these more difficult and practical settings are interesting future directions. The Broader Impact os this work is discussed in Appendix D.

## 6 Conclusion

In this work, we propose to analyze cross-embodiment RL in an unsupervised RL perspective as CEURL, i.e., pre-training in an embodiment distribution. We formulate it as CE-MDP, with some more challenging properties than the single-embodiment setting. By analyzing the optimal cross-embodiment initialization, we propose PEAC with a principled intrinsic reward function and further show that PEAC can flexibly combine with existing unsupervised RL. Experimental results demonstrate that PEAC can effectively handle downstream tasks across embodiments for extensive settings, ranging from image-based observation, state-based observation, and real-world legged locomotion. We hope this work can encourage further research in developing RL agents for both task generalization and embodiment generalization, especially in real-world control.