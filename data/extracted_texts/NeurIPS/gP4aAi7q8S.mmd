# Contents

_CausalChaos!_ Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes

 Paritosh Parmar\({}^{1,2}\), Eric Peh\({}^{1,2}\), Ruiru Chen\({}^{1}\), Ting En Lam\({}^{3}\),

&Yuhan Chen\({}^{4}\), Elston Tan\({}^{5}\), Basura Fernando\({}^{1,2}\)

\({}^{1}\)Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore

\({}^{2}\)Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore

\({}^{3}\)Nanyang Technological University, \({}^{4}\)National University of Singapore, \({}^{5}\)Singapore Polytechnic

https://github.com/LUNAPProject22/CausalChaos

###### Abstract

Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. Cartoons use the principles of animation that allow animators to create expressive, unambiguous causal relationships between events to form a coherent storyline. Utilizing these properties, along with thought-provoking questions and multi-level answers (answer and detailed causal explanation), our questions involve causal chains that interconnect multiple dynamic interactions between characters and visual scenes. These factors demand models to solve more challenging, yet well-defined causal relationships. We also introduce hard incorrect answer mining, including a causally confusing version that is even more challenging. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling & joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field.

## 1 Introduction

Understanding the intricate _motivations_ behind human actions is paramount in developing sophisticated systems capable of nuanced behavior analysis. In real-world scenarios, actions are shaped by a _multitude of factors_, including personal experiences, emotions, social contexts, and cultural backgrounds. This complexity necessitates a _comprehensive_ approach to unraveling the "_why_" behind actions, fostering empathy, effective communication, and robust decision-making. _Causal video question answering_ aims to decipher the answers behind characters' actions. Despite the growing interest in causal video QA, existing datasets often fall short, requiring only: **1)**_surface-level understanding_; or **2)**_involve more of simple word substitution in the QA pairs, rather than causal reasoning_ (_e.g._, "Q. Why are the cars on the street not moving? A. Parked."). Recognizing this gap, we embark on the development of a rigorous and challenging _causal Video-QA dataset_. Our goal is to provide a high-quality resource that rigorously evaluates and advances causal video QA models.

Drawing on the established benefits of cartoons with cognitive processes among children , we leverage the renowned series  to _create a novel and demanding causal Why-QA dataset_ called **CausalChaos!**. The key characteristics of CausalChaos! are as follows:* As demonstrated in Figure 1, 2, we formulate _thought-provoking questions_ (**Q**), where the _answer_ (**A**) and its _detailed explanation_ (**E**) aim to enhance the model's understanding of the causal chains of visual events in a given video clip. A causal chain is a sequence of events/actions in which each step influences or leads to the next, creating a cause-and-effect relationship. It illustrates how one event/action produces a subsequent event/outcome. Compared to existing datasets, CausalChaos! presents **longer causal chains**, as highlighted in Figure 2 and discussed in section 2.
* Video clips in our dataset feature frequent **scene** and **shot changes**. Here, a scene change or shot change refers to the transition from one visual setting or perspective to another. It typically involves shifting focus to a new location, action, or character within the storyline. The _links_ and _causes of the causal chains are often dispersed across various scenes_. Consequently, models are challenged to exert greater cognitive effort in _connecting multiple events_ (scenes) and identifying intermediate causes to comprehend the "why" questions related to the clips.
* Despite the complexity and length of the causal chains, they are distinctly _delineated_, **unambiguous**, and **effectively communicated** using _principles of animation_ like staging and exaggeration. This deliberate design allows models to _focus on deciphering causal relationships_.
* CausalChaos! introduces an added layer of complexity by necessitating the modeling of actions at various levels of granularity--ranging from sweeping, **large-scale movements** to nuanced, **finegrained actions**, such as interpreting emotional cues through facial expressions.
* Our dataset demands a **diverse range of reasoning skills**, encompassing deductive, spatial, emotional reasoning, and more, as outlined in Figure 2 and discussed in section 3.
* We introduce **challenging incorrect options**, including the CausalConfusion set, to prevent models from relying on shortcuts, such as object-noun or action-verb matching in vision-language spaces, and instead require them to understand causal relationships.

Upon evaluating various state-of-the-art VideoQA models including the recent multimodal instruction tuning models, we found that our dataset remains one of the most challenging causal QA dataset. Particularly, we observed that models often: **1)**_jumped to conclusions_ based on _partial evidence_, rather than considering the full set of evidence; **2)**_failed to engage in true causal reasoning_, opting instead for shortcuts like object/action-noun/verb matching to select answers. Based on this, we identified _more advanced/explicit causal relationship modeling and jointly modeling vision and language_ as the immediate areas for future efforts to focus upon. Further, we show that similar to how cartoons help children better connect cause and effects, they can help VideoQA models as well. We _incorporated our dataset with the NextQA_, a _real-world dataset_ and found that it _brings some improvements in why questions_. What is more, _incorporating our dataset_ brings improvement on _non-Why questions_ as well.

Figure 1: (**Left) Examples of causal questions about characters’ actions from our CausalChaos! dataset— based on Tom & Jerry cartoon series.** Q: question; A: answer; E: explanation. _Please view in Adobe Reader to play the embedded videos for better explanation_. (**Middle) Illustration of causal chain, scene changes.** Linking multiple clues/cues embedded in different scenes to solve causal relationships pose a challenge for VideoQA models. (**Right)** Animators leverage **Principles of Animation** to _stylize_ the _visuals & motions_ to _disentangle/highlight key content_ of the scene to create _well-defined/unambiguous and effectively communicated cause-and-effect relationships_. The interplay of these factors allow models to focus on solving complex, yet, well-defined, unambiguous causal relationships. We have provided enlarged figures in Appendix.

Overall, we believe our CausalChaos! dataset presents challenges spanning the entire VideoQA pipeline, from _deciphering intricate videos_ to _processing complex questions_ and _discerning nuanced answers_, stimulating research in _video processing_, _causal reasoning_, _language modeling_, and _joint modeling_. Along with the other complementary datasets, such as [49; 17; 19; 21; 48], our new challenging dataset will pave the way for these developments in the field.

## 2 Related Work

To drive the progress in VideoQA, researchers have developed various datasets [27; 50; 10; 60; 58; 63; 13; 14; 56; 57; 9; 48] with distinct focuses and contributions--see the survey . In the following, we mainly compare our work with prior art focusing on causal QA literature. We provide a detailed discussion on various VideoQA datasets and models in the Appendix. Our work is inspired from a few datasets: _CLEVRER_[54; 29], _NextQA_, _CausalVidQA_, _IntentQA_ that cover causal reasoning. However, they have limitations that our dataset aims to fill:

* _Limitation in scope_: _e.g._, [54; 29] only consider _collision events of simple inanimate objects_ such as cuboids; as a result, _lack: actors_, their _characteristics_ such as _emotions_, _intentions_. Our dataset has actors whose actions are shaped by _emotions_, _intentions_, _context_, etc.
* _Lack of precise temporal annotations_ (e.g., [49; 17]) _inadvertently_ involves _temporal localization_ & may create a _false sense of difficulty_, while the questions may _not be challenging_. We alleviate this problem by providing precise temporal annotations and focusing on designing _truly complex_ and challenging QA.
* _Lack of complexity in questions_ (_e.g._[49; 17; 19]). Based on human studies, we found that a majority of causal QA in these datasets were flagged as _not requiring any notable reasoning_ (Figure 2(a)). We also attempted to _quantify_ the _complexity in reasoning_ by computing the _lengths of causal chains_ involved in their QAs vs. ours using GPT-4o . Causal chains in [49; 17; 19] are shorter (Figure 2(b)). In comparison, our dataset has _longer causal chains_--average length 1.9--posing a challenge for models in _connecting multiple events_ or cues. Our dataset also demands _wide spectrum of types of reasoning_ (Figure 2(a)).
* Scenes in [54; 29; 49; 17; 19] are _less dynamic_, mostly involving a _single scene_. In comparison, our dataset averages about _four scenes_. _Rapid scene changes_ & _dynamic interactions_ challenge VideoQA models to link _context_ & _cues across different scenes_, modeling cause-and-effect relationships over _longer causal chains_ (more details in subsection 3.2). While similar challenges have been acknowledged in other computer vision problems , they remain unexplored in VideoQA.

Figure 2: **(a) Types of reasoning demanded by our CausalChaos! dataset. Reasoning types: DR-_deductive_ reasoning; IR-_inductive_; SR-_spatial_; CR-_causal_; CT-_critical thinking_; ER-_emotional_; AR-_abductive_; TR-_temporal_; None-no reasoning required as per the human subjects. None is undesirable, and tend to indicate that questions are less challenging. **(b) Comparison among CausalChaos! and existing causal videoQA datasets.** MA-multilevel answers; CCL-causal chain length; NOS-no. of scenes; RS-reasoning spectrum; MGA-multigranular actions. **(c) Qualitative comparison between CausalChaos! and NextQA (Why-QA) datasets.** CausalChaos! Answers and Explanations give detailed information regarding the actual cause-and-effect relationships, motivations, emotions covering wide range of reasoning types. Note that, we have temporally cropped videos to retain only the relevant parts from NextQA dataset videos; otherwise, raw videos are longer resulting in unintended problem of temporal localization for models.

* _Lack of hard negatives._ Current datasets may not emphasize on including hard incorrect options. Due to this, models may not be required to do causal reasoning, but rather they can exploit _shortcuts like object/action-noun/verb matching_ in vision-language spaces to select correct answer. To address this limitation, we develop _hard negative selection strategies_.
* _No hierarchical answers with different level of explanation._ These datasets contain a _single-level answer to 'Why'_-questions. On the other hand, we richly annotate our dataset to provide _two-level answers to 'Why'-questions--1) direct/immediate cause; 2) deeper explanation. Quality of answers is further enhanced, and are _more informative_ as a consequence of the _more complex questions_.

## 3 CausalChaos! Dataset

This section details the construction of our VideoQA dataset designed to challenge causal reasoning, covering its _video source_, _annotation process_, & _quality checks_ to ensure high-quality annotations. We then discuss the _unique attributes_ that make our dataset valuable for causal VideoQA tasks.

### Dataset Construction

Video source.To focus on _visual reasoning_, we selected the timeless _silent_ cartoon series, "_Tom & Jerry_" (1940)1, spanning over **6**_seasons_ and **161**_episodes_. Silent cartoons, which lack other modalities like dialogue, foster visual reasoning in children. Similarly, we hypothesize that using silent cartoons for VideoQA will enhance visual reasoning. The Tom & Jerry series is ideal for a causal reasoning dataset, offering _abundant segments with diverse cause-and-effect relationships_. Concurrent work  on causal _image_ generation also leverages Tom & Jerry.

Annotations.Each dataset sample includes the following annotations: {_Question_, _Answer_, _Explanation_, _start frame_, _end frame_}. Details in the following.

\(\)**Questions** are crafted to capture the _why_ or reasoning behind the _actions_ of characters in Tom & Jerry cartoons. To cultivate comprehensive & deeper video understanding capabilities in models, we formulated _thought-provoking_ causal questions where cause & effect are connected by _longer causal chains_. Questions also extend _beyond explicit_ visual cues, encompassing gestures & expressions, to delve into the _characters' underlying intentions_ & _goals_. Annotators generated questions while watching the video for the first time to mimic how models assess unseen clips. They then rewatched the videos multiple times to create more critical thinking questions. To focus on visual reasoning, annotators watched the videos without audio, ensuring no audio cues were included in the dataset.

\(\)**Multi-level Answers**. The first-level or the Primary answer, represents a _literal_ or _direct_ cause or form of response. It is accompanied by a deeper form of Explanation, which considers the _broader context_ of the scene, the _thoughts_, _feelings_, _intentions_ of characters, & their actions. This deeper explanation also takes into account potential consequences & provides further reasoning to support the primary answer. It includes reasons & additional details to comprehensively address the question. Examples shown in Figure 1 & Appendix. **Guiding principle for consistency:** our dataset's multi-level answer structure is designed to explore causality from direct causes (Primary answers) to deeper, contextual reasoning (Explanations). To ensure consistency, each Explanation must extend the causal chain from the Primary answer with at least one additional causal link, providing the next logical step in causal chain without unnecessary detail. For instance, if the Primary answer addresses an immediate reaction, the Explanation would delve into underlying motivations, intentions, or consequences, systematically adding depth. **Commonsense knowledge:** understanding causal relationships in complex scenes, such as those in Tom & Jerry episodes, often requires more than just visual cues--it requires the integration of commonsense knowledge. When commonsense knowledge is needed to connect cause-and-effect, annotators articulate this understanding within the Explanation.

\(\)**Temporal annotations**. For each QAE set, the _Start_ and _End frame numbers_ are recorded to cover the entire scene, including contextual frames that support the reasons behind the actions.

Quality Check.To ensure dataset quality and reliability, we implemented a two-stage quality check process. First, quality checkers assessed episodes they did not annotate. Then, they reviewed episodes they did not check in the first stage. Multiple quality checkers carefully reviewed and verified curated question and answer sets for logical fallacies, timestamp inconsistencies, grammatical errors, and spelling mistakes. They also excluded annotations containing audio or text aspects that the model cannot comprehend. To prevent direct overwriting, edits were flagged in a different font color for further discussion. Flagged inconsistencies or errors were resolved through discussions and consensus among checkers and annotators. The team referred to episode synopses from Tom and Jerry Wiki , an online encyclopedia, as a third-party opinion and for fact-checking.

Dataset Statistics.Annotators reviewed all **161** episodes across **6** seasons, generating **4,945** detailed Question-Answer-Explanation sets. This comprehensive dataset covers various scenes, characters, and events, providing a solid foundation for understanding and answering questions about the series. Additional statistics and word clouds for answers and explanations are available in the Appendix.

### Unique Characteristics of our CausalChaos! Dataset

In the following, we delve into the untapped unique properties of our dataset, which both _complement existing video understanding datasets_ and present _novel challenges to VideoQA_, particularly from a _causal reasoning perspective_.

1. **Multi-level answers** offer a _richer and more nuanced perspective_, which leads to _deeper insights_--considering not only the _immediate circumstances_ but also underlying _psychological_, _social_, and _personal_ factors that contributed to _characters' decisions_. By employing _multi-level answers alongside thought-provoking questions_, we can cultivate _deeper reasoning_ and _analysis of characters' actions_, enabling the training and evaluation of models on complex question-answer pairs that demand a _comprehensive understanding_ of _longer causal chains_. Further elaborated discussion on the importance of multi-level answers to 'Why'-questions included in Appendix.
2. **Grounded in diverse _Visual & Motion_ information.** The Tom & Jerry cartoon series is _visually rich_ and _dynamic_, with _intricate scenes_, _actions_, and _character interactions varying across episodes_. We had annotators craft questions where the _answer is grounded in the video_. This grounding compels the _model to analyze the video_ for a _broader range of details_ and _clues_ to provide _meaningful_ answers. For example, consider Figure 3(a), same question appears in different episodes, but their answers/context are completely different. This demonstrates that it is crucial for VideoQA models to understand visual information to answer correctly on our dataset. Multimodal nature of VideoQA task is enhanced by the presence of _unusual situations_ in cartoons--typically not found in real-world datasets as shown in Figure 3(b,c). Since the answers are embedded in videos, models cannot rely on _correlations/biases_ in their training data; they must thoroughly process and understand the content to answer correctly. This is crucial even with large language models (LLMs) and embeddings, which are also susceptible to _frequency-based biases_ and hallucinations . For example, _Lion_ is often associated with _killing_ in LLM embeddings. In a qualitative analysis, we found that models might choose an answer involving _'killing'_ Jerry as the _motive_ for the Lion-character's action, while the Lion-character was actually trying to _quiet_ Jerry Figure 3(b-left). Revealing such biases has been considered in other computer vision problems , but is yet to be introduced in VideoQA. _Our dataset offers a valuable opportunity to address this challenge_. Additionally, understanding Tom

Figure 4: **Traditional temporal modeling vs Dynamic scene linking. Notice the abrupt scene change, which causes disruption in visual flow, resulting in large amplitude and widespread optical flow.**

Figure 3: **Grounded in diverse visual information.**

& Jerry cartoons requires modeling of _details_ and _actions at varying levels of granularity_--from _sweeping movements_ to _subtle emotional cues_ via facial expressions.

## 3 Focus on causal reasoning in visually dynamic scenes

Our dataset focuses on causal reasoning in Tom and Jerry cartoons, characterized by dynamic and rapidly changing scenes. Characters and objects may appear, disappear, and reappear, limiting the model's access to partial observations. Video understanding models struggle to track and understand context. To answer causal questions, these models must link events to form a causal chain despite rapid scene changes and dynamic interactions. We term this Dynamic Scene Linking (DSL), distinct from traditional temporal modeling, which typically focuses on the gradual transitions within a scene as shown in Figure 7 (more details in Appendix). Related challenges are noted in other computer vision problems  but not yet in VideoQA. We hypothesize that humans understand cartoons by forming a mental "world model" of the scene, which helps bridge gaps between discontinuous scenes and provides a coherent overall context. Current VideoQA datasets often lack coverage of such complex scenarios, unlike our dataset.

## 4 More challenging and requiring cognitive effort

We used GPT-4o  to compute the lengths of causal chains of _100 randomly chosen QAE pairs_ from _our dataset_ and _existing causal(-Why) VideoQA datasets_. Results are presented in Figure 2. We observed that the QAE pairs in _our dataset have longer causal chains than existing QA datasets_, potentially suggesting our QAs are more complex. Identifying and solving _longer causal chains_ in the process of answering causal (why) questions involve linking together multiple cues/clues, which requires significant cognitive efforts on the part of VideoQA models, especially, when the clues/cues are embedded in different scenes (described above), as in our dataset.

## 5 Leveraging principles of animation

 (shown in Figure 1) such as _timing_, _squash and stretch_, _anticipation_, _staging_, and _exaggeration_ can be conceptualized as **Data Design-driven Visual Prompting**, as they aid in: 1) "_highlighting_" _key movements_, _emotions_, & _storvelling_; 2) Consequently, greatly aid in _establishing clear cause-and-effect relationships_ and _effectively communicating them_. These principles serve as _spatiotemporal_ counterparts of _caricatures_, which exaggerate and manipulate facial and bodily features, and these have been shown to improve facial recognition rates . In other domains of computer vision, some works  acknowledge the advantages and devise methods to _disentangle the area of interest_, enabling more _targeted_ processing and _comprehension of image contents_. Similarly, we hypothesize that the _principles of animation_ can provide "_hints/guidance_", but the models still need to be able to _leverage this guidance to solve causal relationships_. Overall, _causal relationships in our dataset are complex, longer_, but at the same time they are _unambiguous/well-defined_ using principles of animation, allowing models to _focus on deciphering causal relationships_.

## 6 Wide spectrum of types of reasoning required

Our dataset demands various types of reasoning like 1) _Deductive_; 2) _Inductive_; 3) _Spatial_; 4) _Causal_; 5) _Critical thinking_; 6) _Emotion_; 7) _Abductive_; and 8) _Temporal_. We have visualized examples demonstrating these reasoning types in Figure 9. It is also possible that a question may involve more than one type of reasoning. We conducted _human studies_ to determine the types of reasoning required by our dataset and NextQA and _observed that our dataset demands a wider range of reasoning than NextQA_ (results in Figure 1(a)). More information and definitions of types of reasoning and human studies are provided in Appendix.

### Benchmark Design

Question-types/Tasks.Our dataset comprises two types of questions. The first is Multiple Choice Question Answering (**MCQA**). VideoQA models are provided with the question, the associated video

Figure 5: **Examples of various types of reasoning required by our dataset.**_Please zoom in & view in AdobeReader to play the embedded videos._ Further types of reasoning visualized in Appendix.

footage, **1**_correct_ answer, and **4**_incorrect_ answers; the task for the models is to pick the correct answer. The same format is followed for Answers and Explanations. The second is Open-ended Answer Generation (**OEAG**). The models are provided only with the question and the video footage, and the models have to generate the Answers and Explanations in natural language.

Incorrect Answer/Option Mining.In designing **MCQA** sets, answer options should be _distinct_ from the correct answer & each other but _semantically similar enough to challenge reasoning beyond commonsense_. This prevents models from taking _shortcuts_ like simple object/action matching in vision-language spaces. Using _randomly chosen answers_ as incorrect options is _unproductive_ and makes the correct answer _too obvious_, so we avoid this strategy. Instead, we introduce these strategies:

1. **Vanilla Hard Negative mining.** A pre-trained Sentence-Bert measures semantic similarity. For each question, we sample the top-10 other questions based on cosine similarity of Sentence-Bert embeddings. From these, we sample associated answers and again apply cosine similarity between the correct and candidate answers. Using the equation \( cossim(Q_{i},Q_{n}) cossim(A_{i},A_{n})\), with adjustable weights \(\) and \(\), we select the top 4 answer candidates. To avoid overly similar answers, we penalize high similarity scores by setting \(\) to 0.001, reducing the candidate's overall score.

2. **Causal-Confusion Negative generation. Causal relationship modeling** is _crucial_ for causal VideoQA, but maybe given less emphasis in existing datasets/benchmarks. We hypothesize that, _relationships might not be adequately modeled_ by existing models, instead models may be _selecting answers based on basic object/actor matching_ in vision-language spaces. To test our hypothesis & underscore the relationship modeling, we introduce _hard negatives_ in which the _objects/actors remain unchanged_, but the _relationships between them_ are **altered** or alternate, but plausible scenarios are created. For instance, the _effect is inverted_ (_e.g._"Tom was hitting Jerry"\(\)"Tom was _not_ hitting Jerry") or the _causal agents are swapped_ (_e.g._"Tom_ was hitting _Jerry_"\(\)"_Jerry_ was hitting _Tom_"). We leverage LLMs to generate _causally-incorrect_ options. Examples can be found in the Appendix.

Design of Train-Val-Test splits.We typically partition the dataset into **70**% for training, and **15**% each for validation and testing. Additionally, we employ specific strategies for training and testing.

1. **Uniformly Distributed _Seen_-episode testing (UD).** Dataset samples are randomly divided into train-val-test splits without constraints on which episodes or parts are included in each split. Consequently, uniformly distributed chunks of **all** episodes from **all** seasons are likely seen during training, potentially giving VideoQA models _sparse storylines_.

2. **Consecutive Partially _Seen_-episode testing (PS).** The _train_ set is drawn from the **first 70**% of each episode, the _val_ set from the **middle 70-85**%, & the _test_ set from the **last 15**%. This setup allows the model to have partially "seen" episodes during training & make "educated guesses" during testing.

3. _Unseen_-episode testing (UN).** In this case, a certain **70**% of the _episodes_ are reserved for _train_ set; **15**% of the remaining episodes are reserved for _val_ set; and the _remaining_**15**% of the episodes are used to prepare the _test_ set. So, _testing_ is done on entirely novel, _unseen episodes_ and _storylines_.

## 4 Experiments

We benchmark SOTA VideoQA methods on our new causal action QA dataset for two tasks: 1) **MCQA** and 2) **OEAG**. Then, we explore if our dataset aids real-world QA.

### Benchmarking on MCQA

Baselines.Following prior work , we benchmark the performance of these models on our causal action QA dataset: _BlindQA_, _EVQA_, _CoMem_, _HME_, _HCRN_, and _HGA_. We also evaluate recent vision-language models like _MIST_, _BLIP-2_, and multimodal instruction tuning models _Video-LAMA_ and _VideoChat2_, _GPT-4o_, _VILA_ which excel on vision-language tasks. Further details on each model are provided in Appendix.

Evaluation Protocols.Models are evaluated under these protocols, with **Accuracy** as the metric:

\({}^{}\)**Protocol 1.** Models' ability to select the _correct Answer(A)_ is only evaluated. Their ability to select the correct explanation is not taken into consideration. The chance accuracy in this protocol is \(1/5\). Results from this protocol are presented under columns marked as **A** in Table 1.

\({}^{}\)**Protocol 2.** Models' ability to select the _correct Answer(A) as well as the correct Explanation (E)_ is evaluated. If the model can select the correct answer, but not the correct explanation, then the model is considered to have failed. In this protocol, the chance accuracy becomes \(1/5 1/5=1/25\). Results from this protocol are presented under columns marked as **A+E** in Table 1.

Quantitative results.The performances of the baseline models for both protocols are presented in Table 1. In general, we observe that the performances of most of the baseline approaches are low. Most of the models gain only around 11-12% improvement over the chance accuracy for both protocols. MIST seems to be doing exceptionally well compared to other models. We hypothesize that this could be because MIST being geared for long-form video understanding, is better able to handle long video contexts, which are frequent in our dataset. **Analysis**: we observed the following **failure modes** for models: **1) Limited Evidence Consideration**: Models often focus on a small subset of evidence, neglecting other relevant cues distributed throughout the sequence, which is problematic for datasets with long causal chains like ours. **2)** Models failed because they tried to exploit **shortcuts** like _object-noum_ or _action-verb matching_ in video-language spaces instead of focusing on causal relationship modeling--such _shortcuts can lead to wrong predictions on our dataset due to our hard negative mining_, correct & incorrect answers would likely contain these nouns/verbs. The most effective way to discriminate on our dataset is by inferring the causal relationship. We have discussed _further shortcomings_ & **Qualitative Results** in Appendix. To mitigate these shortcomings, we design a MIST-based multitask model, **MIST-CC**, which, in addition to doing VideoQA, also learns to generate the causal chain from Video-Question features (groundtruth causal chains are generated by GPT-4 from QAE pairs). We found that this simple modification mildly boosts the performance. We have provided further details on MIST-CC in Appendix. Moving to VLM/MLLMs, we observe that they perform better than traditional models (), except MIST. However, GPT-4o released only in late May 2024, outperformed all the models. Although, it is closed source model, we hypothesize its vision and language capabilities maybe significantly better than other VLMs. It was able to incorporate small details like facial expressions. To glean insights into it, we also asked it to give us its reasoning, and found that it analyzes each option individually and then selects the most likely answer. This is close to (or at least mimicking) how humans would approach this task. We believe this, at least on surface, seems to be going beyond just correlation-based answer picking as in non-VLM/MLLMs. In comparison, we found that **humans** performed significantly better than all the models. We established the human baseline on our dataset (MCQA) using five human subjects who are fluent in English and have at least an undergraduate level of education. This baseline was based on a subset of our dataset, consisting of 100 randomly chosen samples. We also provide results for the recent VLM, VILA  (CVPR 2024), _fine-tuned_ and evaluated on our dataset. These results were obtained following our initial paper submission.

**Comparing the two protocols**, we find that Protocol 2, where the model has to select both the correct answer and the correct explanation is significantly more difficult than selecting just the correct answer for all the models. **Comparing splits**, We observe that UD is the easiest split across all models, followed by PS, with UN being the most difficult. This intuitive order suggests that understanding past events or storylines may aid in reasoning about current events or actions.

We further conducted an experiment where we tested the best performing models, MIST, MIST-CC, & GPT-4o, on our **Causal-Confusion** set, where incorrect answer choices have the same objects and actors as the correct answer option, but cause-and-effect relationships are reversed or altered. We observed a _significant drop_ in performance of all models as shown in Table 2. This could potentially be due to _causal relationships being not modeled adequately_ even by such advanced VideoQA models. Performance of MIST-CC dropped relatively less, potentially because the auxiliary task of generating causal chains may have enhanced the understanding of causal relationships. Interestingly, we also found the S-BERT similarity score to be above 90% for an action

    &  &  &  \\ 
**Model** & **A** & **A+E** & **A** & **A+E** & **A** & **A+E** \\   & 20.00 & 04.00 & 20.00 & 04.00 & 20.00 & 04.00 \\  &  & & \\ 
**BiladQA** & 29.38 & 13.87 & 26.51 & 11.54 & 25.13 & 11.02 \\
**EVQA** & 29.38 & 13.48 & 31.32 & 13.32 & 27.82 & 14.65 \\
**CoMem** & 32.08 & 13.88 & 26.10 & 09.89 & 23.12 & 09.27 \\
**HME** & 32.35 & 14.02 & 29.53 & 12.36 & 25.13 & 10.22 \\
**HCN** & 32.48 & 16.98 & 32.01 & 14.97 & 25.67 & 12.23 \\
**IIGA** & 31.40 & 15.36 & 29.40 & 13.19 & 28.23 & 13.84 \\
**MIST** & 62.22 & 44.88 & 62.22 & 42.86 & 55.91 & 37.90 \\
**MIST-CC\({}^{}\)** & 63.34 & 46.80 & 62.50 & 43.54 & 56.18 & 39.25 \\
**VILA** & **77.22** & **62.80** & - & - & - \\  &  & & \\ 
**BLIP-2** & 43.67 & 23.32 & 45.88 & 24.18 & 46.64 & 26.48 \\
**Video-LLM** & 35.00 & 11.73 & 35.16 & 9.34 & 29.62 & 9.68 \\
**Video-MCLM** & 38.14 & 15.36 & 38.91 & 15.93 & 40.83 & 15.99 \\
**GPT-4o** & **63.64** & **48.17** & **63.84** & **49.79** & **64.82** & **52.23** \\ 
**Ihumans** & **94.80** & **93.40** & - & - & - \\   

Table 1: **MCQA Results on our dataset.’We design multitask version of MIST that learns to generate causal chains as an auxiliary task.**

  
**Negative type** & **MIST** & **MIST-CC** & **GPT-4o** \\  Vanilla Hard & 62.22 & 63.34 & 63.64 \\ Causal-Confusion & 55.80 & 58.76 & 54.95 \\   

Table 2: **Impact of causal-confusion.**(_e.g._, Tom is running after Jerry) and its Causal-Confusion version (_e.g._, Tom is _not_ running after Jerry); while these sentences would be opposite/different in terms of human perception. We believe that _Causality might be more overlooked than we think in various fields, not just in computer vision_.

### Benchmarking on Open-ended Answer Generation (OEAG)

CapsMIX Performance metric.We measure the performance of OEAG in terms of BLEU-1,2,3 , METEOR , ROUGE , SPICE , CIDEr  & Sentence-BERT  scores by comparing with the ground truth answers as done by captioning & QA literature. The wide range of metrics complicates model comparison, so we introduce Caps-MIX (Captioning Metrics Integration eXpert), which normalizes and integrates all scores into a single metric, simplifying comparisons and combining the unique strengths of individual metrics.

Baselines.Following prior work , we benchmarked the performance of _EVQA_; _UATT_; _HME_; _HGA_, _VILA_ on OEAG task. We also report the zeroshot performance of recent multimodal video understanding models _BLIP-2_, _Video-LLaMA_, _VideoChat2_, _GPT-4o_.

We evaluated the baselines for generating answers and explanations. The performances of various models are summarized in Table 3. Here we have presented unnormalized performances. For full results, and normalized version, see Appendix. Overall, we observe that models struggle significantly with open-ended generation, including some recent VLMs and MLLMs. Despite GPT-4o's strong performance on MCQA, it also falters on OEAG. These models likely perform better on MCQA by _eliminating incorrect choices_, but they fail to genuinely understand videos and perform causal reasoning for standalone answer generation. VideoChat2 seem to be doing well, perhaps, because it was specifically trained on various video understanding tasks and datasets, including causal reasoning task. Comparing split-type wise, we found that as in the case of MCQA, UD was the easiest split.

Inspired by the success of LLMs, we experimented with leveraging GPT-2 , a publicly available LLM, as our natural language answer generator. We found that a pre-trained version out of the box did not work well--it generated mostly random, unrelated words. However, upon simple training on the train set, it performed significantly better than the baselines we considered as shown in Table 3 (BlindGPT-2). However, it is unclear if these LLMs have "seen" Tom and Jerry scripts during their pretraining stage. If so, then pretraining on relevant scripts followed by finetuning on our dataset could potentially be a reason for GPT-2's good performance. Although this is less likely. What is more likely to be the reason behind this good performance is the language modeling/generating capability of GPT-2/LLMs.

In the next step, we integrated visual information into LLM. Taking inspiration from , we learn a projector network to align the visual features with GPT-2 representations (VisualGPT-2 in Table 3. We observed an improvement in the performance. However, we believe that this might not be a very efficient way to integrate visual information with LLMs. We expect the performance to boost considerably through better and more sophisticated joint modeling vision and language. Overall, we believe shallower networks might not have the capacity to do inference over longer, complex causal chains, and as such it might not be the best option to invest future efforts into; LLMs have the potential to excel at causal reasoning; this view is also supported in a concurrent survey . We have also provided results of VILA  finetuned on our dataset. These results were obtained following our initial paper submission. Note that none of the models discussed are performing well. This becomes clearer when we compare their performances with the upper bound (see the normalized version in Table 6). This shows that there is a significant room for improvement on OEAG task.

### Does our dataset help with real-world cases?

To evaluate the direct impact of our dataset, we combine our training set with NextQA's  training set and measure model performance on the NextQA  test set. For comparison, we also measure model performance without incorporating our dataset. Despite being a _synthetic/cartoon_ dataset, and

  
**Model** & **CapsMIX** \\  _Fineuteland_ & \\ 
**BlindQA** & 2.7646 \\
**UATT** & 3.3928 \\
**HME** & 3.0975 \\
**HGA** & 3.7872 \\
**PhilotGPT-2** & 6.6006 \\
**VisionGPT-2** & 6.7582 \\
**VILA** & **8.0320** \\  _Zero-shot_ & \\ 
**BLIP-2** & 1.8931 \\
**Video-LLaMA** & 2.4464 \\
**VideoChat2** & **3.9524** \\
**GPT-4o** & 2.9851 \\   

Table 3: **OEAG results on our dataset (UD split).**much smaller in size_, our dataset boosted the performance on a real-world dataset on both MCQA and OEAG (Table 4).

We observed _improvement in identifying the correct causes by breaking the reliance on shortcuts & focus on causal effects_; a more comprehensive analysis of the situation/interaction, rather than jumping to a conclusion. Qualitative results provided in Appendix. What is more, improvements were not limited to 'causal-why' questions but also extended across other question types. We hypothesize that this can attributed to the _wider range of reasoning_ involved in our dataset. To the best of our knowledge, this is the first time a synthetic VideoQA dataset has shown immediate improvement on a real-world dataset.

Although modest, the improvements are meaningful because: **1)** improvements held across models showing broad applicability; **2)** improvements beyond why-questions suggest that the models' enhanced causal reasoning skills and learnt representations generalize beyond the specific questions seen during training, potentially improving overall video comprehension; **3)** Cross-domain learning often carries a high risk of negative transfer, where irrelevant or misleading patterns from one domain can harm performance in another. The fact that the cartoon dataset did not degrade performance across multiple models is a good indication that it introduces valuable, complementary information.

However, we noticed a slight drop in the performance on location-type questions. Thus, we believe that it might be better to: **1)**_transfer reasoning skills acquired from challenging synthetic datasets like ours_; **2)** leverage our synthetic dataset to _inform the model designing process_, as it better reflects the challenges VideoQA models may face in the real world, such as longer causal chains and frequent scene changes in visual streams. Nonetheless, we _do not suggest naively deploying whole models/weights trained on our dataset to real-world scenarios or applications_.

## 5 Conclusion

We introduced CausalChaos!, a challenging dataset for causal action question-answering tasks based on the classic Tom and Jerry cartoon, richly annotated with critical thinking questions requiring extensive reasoning from Video QA models. Questions come with multi-level answers and explanations covering the entire video context. We also provide the novel CausalConfusion test set to challenge causal relationship modeling in Video QA models. Our experiments show that while existing models perform well on causal action QA tasks, there is significant room for improvement in causal relationship modeling and generating detailed open-ended answers. LLMs show promise, but integrating visual information with LLMs or joint modeling of vision and language is crucial. We hope our dataset fosters such developments and will release it and the codes to support future efforts. Lastly, we demonstrated improvements in real-world datasets. Our dataset, derived from cartoons, should inform model design, reflecting real-world challenges like longer causal chains and frequent scene changes. However, we do not suggest deploying models trained solely on our dataset in real-world scenarios.

Material Acknowledgement and Disclaimer.Tom and Jerry is a material of Turner Entertainment Company (Warner Bros. Entertainment Inc.). All rights reserved. We do not claim any ownership of or rights to the Tom and Jerry material. All other trademarks, service marks, trade names and any other material referenced in this document are the property of their respective owners.

Acknowledgements.This research/project is supported by the National Research Foundation, Singapore, under its NRF Fellowship (Award# NRF-NRFF14-2022-0001). This research is also supported by funding allocation to B.F. by the Agency for Science, Technology and Research (A*STAR) under its SERC Central Research Fund (CRF), as well as its Centre for Frontier AI Research (CFAR). We sincerely thank all the reviewers for their valuable feedback and suggestions.