# DAGER: Exact Gradient Inversion for Large Language Models

Ivo Petrov\({}^{*1}\), Dimitar I. Dimitrov\({}^{*1,2}\), Maximilian Baader\({}^{2}\),

Mark Niklas Muller\({}^{2,3}\), Martin Vechev\({}^{1,2}\)

\({}^{1}\) INSAIT, Sofia University "St. Kliment Ohridski"

\({}^{2}\) ETH Zurich

\({}^{3}\) LogicStar.ai

{ivo.petrov, dimitar.iliev.dimitrov}@insait.ai \({}^{1}\)

{mbaader, mark.mueller, martin.vechev}@inf.ethz.ch \({}^{2}\)

Equal contribution.

###### Abstract

Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, _the first algorithm to recover whole batches of input text exactly_. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).

## 1 Introduction

While large language models (LLMs) have demonstrated exceptional potential across a wide range of tasks, training them requires large amounts of data. However, this data is sensitive in many cases, leading to privacy concerns when sharing it with third parties for model training. Federated learning (FL) has emerged as a promising solution to addressing this issue by allowing multiple parties to collaboratively train a model by sharing only gradients computed on their private data with the server instead of the data itself. In particular, FL has been used to finetune LLMs while protecting private data  in privacy-critical domains, such as law  and medicine .

Gradient Inversion AttacksUnfortunately, recent work has shown that this private data can be recovered from the shared gradients using so-called gradient inversion attacks, raising concerns about the privacy guarantees of federated learning . While most prior work on gradient inversion attacks has focused on image data , first works have demonstrated that text can also be recovered . However, as these approaches are optimization-based, the discrete nature of text data poses a major challenge by inducing much harder optimization problems and limiting them to approximate recovery of small batch sizes and short sequences. Therefore, applying existing attacks methods on modern LLMs would be computationally infeasible or yield subpar reconstructions.

This Work: Exact Recovery of Large Batches and Long SequencesTo overcome these limitations, we propose DAGER (**D**iscreteness-Based **A**ttack on **G**radients for **E**xact **R**ecovery), the first exact gradient inversion attack for (transformer-based) LLMs in the challenging honest-but-curious setting. Our key insight is that while discrete inputs pose a challenge for optimization-based attacks, they can be leveraged in combination with the low-rank structure of gradients to enable exact recovery via search-based attacks. Crucially, we show that the gradients of self-attention projection matrices in transformers are i) typically low-rank and ii) linear combinations of input embeddings. This allows us to check whether a given input embedding lies within the span of the gradient and was thus part of the input sequence. We use this to first recover the set of input tokens and then reconstruct the full sequences. For decoder architectures DAGER leverages their causal attention masks for to derive an efficient greedy recovery, while for encoder architectures, DAGER uses several heuristics to make exhaustive search tractable. As DAGER only requires propagating inputs through the first transformer block instead of full gradient computations, it scales to very large models. In fact, the higher internal dimension of these models even allows DAGER to recover more information as our low-rankness assumptions hold for larger batch sizes and longer sequences. We note that this approach is applicable both to the easier next-token prediction and to the harder classification setting.

EvaluationWe demonstrate in an extensive evaluation that DAGER enables the exact recovery of long sequences and large batch sizes for both encoder- and decoder-based architectures, beating prior attacks in terms of speed (20x at same batch sizes), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99). In particular, we show this for GPT-2 , LLLaMa-2 , and BERT  across CoLA, SST-2 , Rotten Tomatoes  and ECHR , for batch sizes up to 128. Additionally, we demonstrate that DAGER is versatile and can be applied to a wide range of settings, including FedAvg , LoRA  finetuning and model quantization .

Key ContributionsOur main contributions are:

* We show how the low-rankness of self-attention layer gradients can be leveraged to check whether specific discrete inputs were present in the input (Sec. 4).
* We leverage this key insight to propose DAGER, the first exact gradient inversion attack for transformers (Sec. 5).
* We conduct an extensive empirical evaluation demonstrating that DAGER is not only able to reconstruct inputs exactly but also scales to much larger batch sizes, longer input sequences, and larger models than prior attacks, while also being significantly faster to mount (Sec. 6).
* We provide an efficient GPU implementation of DAGER, that can be publicly accessed at https://github.com/insait-institute/dager-gradient-inversion.

## 2 Related Work

Gradient leakage attacks, first introduced by Zhu et al. , generally fall into two categories -- honest-but-curious attacks [6; 22; 23; 7; 8; 24; 9; 10; 11; 25; 26; 27; 28], where the attacker passively observes the client's federated learning updates and tries to recover the data solely based on them, and malicious server attacks [29; 30; 31; 32; 33] where the attacker is further allowed to modify the federated learning model shared with the client. In this work, we focus on the harder to attack and more realistic honest-but-curious setting. A large body of the gradient leakage literature in this setting focuses on image data [7; 8; 24; 9; 27]. Differently, gradient leakage in the text domain remains successful only in the case of a malicious adversary [31; 32; 34]. In the honest-but-curious setting, the results either remain limited to short sequences and small batch sizes \(B\)[6; 10; 11], require large number of gradient updates , or cannot recover the order of tokens in client sequences Xu et al. . Further, state-of-the-art attacks require strong data priors [11; 25], and do not scale to realistic decoder-based LLMs. In contrast, DAGER, works on large batches and sequences for both encoder- and decoder-based transformers, including LLaMa-2 . Additionally, unlike prior work, our attack works on both token prediction tasks and the harder setting of sentiment analysis  where label recovery methods, such as , are not applicable. Finally, DAGER has no requirements for the state of the model training. In instance,  exploits model memorization of the data, unlike DAGER, which can handle the more realistic setting of being applicable at any point in time. Further, in contrast to [26; 25; 31], we do not require the gradient of the embedding layer, making our setting significantly harder.

While most prior honest-but-curious attacks leverage optimization methods to approximately recover the client inputs [6; 23; 7; 8; 24; 9; 10; 11], several works have shown that exact reconstruction is possible for batch size \(B=1\) under various conditions for different architectures [22; 26; 27]. Crucially, Dimitrov et al.  recently showed that \(B>1\) exact reconstruction from gradients of fully-connected layers is also possible. Our work, builds upon this result to show that exact gradient leakage is also possible for transformer-based LLMs.

## 3 Background and Notation

In this section, we introduce the background and notation required to understand our work. To this end, we first recall the basic operation of the transformer architecture in the context of LLMs, and then describe the result, first introduced in Dimitrov et al.  for linear layers, in the context of a self-attention layer showing that the gradients of its linear transformations have a low-rank structure. The notations used throughout this paper are summarized in Table 1 for clarity and ease of reference.

### Transformers

In this paper, we consider LLMs based on both encoder and decoder transformer architectures trained using the FedSGD  protocol and a loss function \(\). While we mainly focus on the harder-to-attack binary-classification loss typically used for sentiment analysis, we demonstrate that DAGER is equally applicable to the next-token prediction loss in Sec. 6, which contains more gradient information, as suggested by prior work Zhu et al. . We denote the transformer's context length with \(P\), its hidden dimension with \(d\), the number of transformer blocks with \(L\), the vocabulary of the tokenizer with \(\), and its size with \(V\). We present our approach for single-headed self-attention but it can be directly extended to multi-head self-attention, and we experimentally apply DAGER in this context.

Transformer InputsWe consider inputs batches consisting of \(B\) sequences of tokens, where \(n_{j}\) is the length of the \(j^{}\) batch element. Sequences with length \(<n=_{j}(n_{j})\) are padded. We denote the total number of non-padding tokens in a batch with \(b=_{j=1}^{B}n_{j}\).

Token EmbeddingsThe discrete input tokens are usually embedded via a function \(f^{0}[V][P]^{d}\) mapping a token's vocabulary index \(v\) and its position \(i\) in the sequence to an embedding vector \(^{ij}=f^{0}(v,i)\). These embeddings \(^{ij}\) are then stacked row-wise to form the input \(_{1}^{b d}\) to the first self-attention layer. Note that \(f^{0}\) is known to the server, as it is part of the model. Further, while \(f^{0}\) differs between models, typically it maps token indices to embedding vectors before optionally adding a positional encoding and applying a LayerNorm. Crucially, \(f^{0}\) is applied per-token.

 
**Symbol** & **Definition** & **Symbol** & **Definition** \\  \(B\) & Batch size & \(\) & Loss function used for training \\ \(P\) & Transformer context length & \(d\) & Hidden(embedding) dimension \\ \(L\) & Number of transformer blocks & \(\) & Vocabulary set \\ \(V\) & Vocabulary size \(||\) & \(n_{j}\) & Token length for the \(j\)-th sequence \\ \(n\) & \(_{j}b_{j}\) - the length of the & \(b\) & \(_{j=1}^{B}n_{j}\) - the total number of non-padding tokens \\  & longest sequence & & padding tokens \\ \(f^{0}\) & Embedding function & & The \(j\)-th entry of the \(i\)-th position \\ \(f^{0}\) & (maps tokens to embeddings) & \(z^{ij}\) & The \(j\)-th entry of the \(i\)-th position \\ \(^{l}\) & Input to the \(l\)-th attention layer & \(\) & The attention mask \\ \(_{l}^{\{Q,K,V\}}\) & Query/key/value projection weights & & The query/key/value embeddings \\  & for the \(l\)-th attention layer & & in the \(l\)-th attention layer \\ \(f_{l}^{l}\) & The \(i\)-th token embedding after & \(_{i}^{}\) & The set of client tokens at position \(i\) \\  & the \(l\)-th transformer block & & The set of batch sequences up to \\ \(_{i}^{*}\) & position \(i\) & \(s_{1},s_{2},...,s_{P}\) & A sample sequence of \(P\) tokens \\ \(S_{best}^{*}\) & The set of the best reconstructed sequences & \(^{}\) & The set of distances to the span for \\ \(^{ank}\) & The singular value threshold for determining the rank of the l-th layer & \(_{l}\) & The distance threshold for filtering \\  

Table 1: Table of notations used in the technical description of DAGER.

Self-AttentionThe stacked embeddings \(_{1}\) are then passed through a series of self-attention layers. We denote the input to the \(l^{}\) self-attention layer as \(_{l}^{b d}\), for \(1 l L\). A self-attention layer is a combination of three linear layers: The query \(_{l}=_{l}_{l}^{Q}\), key \(_{l}=_{l}_{l}^{K}\), and value \(_{l}=_{l}_{l}^{V}\) layer, which are then combined to compute the self-attention output:

\[(_{l},_{l},_{l})=(_{l}_{l}^{T}}{})_{l},\]

where \(\) is the binary self-attention mask, \(\) is the element-wise product, and the softmax is applied row-wise. \(\) is chosen to ensure that padding tokens do not affect the layer's output. Further, for decoders, \(\) ensures that only preceeding tokens are attended. For notational convenience, we denote as \(f_{i}^{l}^{P}^{d}\) the function that maps any sequence of input tokens to the \(i^{}\) input embedding at the \(1 l L\) self-attention layer. Note that \(f_{i}^{l}\) is part of the model and, thus, known to the attacker.

### Low-Rank Decomposition of Self-Attention Gradients

For a linear layer \(=+(||)^{T}\) with a weight matrix \(^{n m}\), a bias \(^{m}\), and batched inputs \(^{b n}\) and outputs \(^{b m}\), Dimitrov et al.  show that:

**Theorem 3.1** (Adapted from Dimitrov et al. ).: _The network's gradient w.r.t. the weights \(\) can be represented as the matrix product:_

\[}{}=^{T}}{}.\] (1)

_Further, when the batch size \(b n,m\), the rank of \(}{}\) is at most \(b\)._

The rank limit follows directly from the dimensionalities of \(}{}^{b m}\) and \(^{b n}\) in Eq. 1.

In this work, we apply Theorem 3.1 to the linear projection matrices \(_{l}^{\{Q,K,V\}}^{d d}\). As long as the total number of tokens \(b<d\), it states that the gradients \(}{_{l}^{Q}}\), \(}{_{l}^{K}}\), and \(}{_{l}^{V}}\) are rank-deficient. Without loss of generality, for the rest of the paper we use \(}{_{l}^{Q}}=_{l}^{T}}{_{l}}\) to explain our method.

## 4 Overview of DAGER

In this section, we provide a high-level overview of our method DAGER, illustrated in Fig. 1. DAGER is an attack that recovers the client input sequences from the shared gradients of a transformer-based LLM. DAGER works for both encoder and decoder-based LLMs, however, for simplicity here we focus on decoder-only LLMs. While, in theory, one could enumerate all possible batches of input sequences, and check whether they produce the desired gradients, this is infeasible in practice as it requires computing \(V^{P B}\) different gradients. We reduce the search space by leveraging the rank-deficiency of \(}{_{l}^{Q}}\), discussed in Sec. 3.2, combined with the finite number of possible inputs to each self-attention corresponding to one of the \(V^{P B}\) gradients above. For the rest of the section, we assume rank-deficiency of \(}{_{l}^{Q}}\), that is \(b<d\). This assumption is in practice satisfied for reasonable input lengths and batch sizes.

Leveraging the Rank DeficiencyAs the gradient matrix \(}{_{l}^{Q}}\) is rank-deficient, i.e. \(b<d\), the columns of \(}{_{l}^{Q}}\) form a subspace of \(^{d}\) of dimension \(b\). Further, under mild assumptions

Figure 1: Overview of DAGER. DAGER first recovers the sets of client tokens \(_{i}^{*}\) at each position \(i\) by testing each token in the vocabulary \(\) via a span check based on the client gradients of the first self-attention. Then it recursively combines them into partial client sequences \(_{i}\) with length up to \(i\), filtered to obtain the correct sequences \(_{i}^{*}\) via the gradients of the second self-attention.

(see Theorem 5.1), the embedding vectors forming \(_{l}\) are linear combinations of the columns of \(}{_{l}^{Q}}=_{l}^{T}}{_{l}}\). It is unlikely that any incorrect embedding vector, part of one of the \(V^{P B}\) incorrect inputs \(_{l}\), is part of \((}{_{l}^{Q}}) ^{d}\), as the hypervolume of this subspace is \(0\).

Filtering Incorrect EmbeddingsWe can efficiently filter out all incorrect client embeddings at any layer \(l\) without computing their gradient, simply by checking if they are in \((}{_{l}^{Q}})\). However, applying this procedure naively still requires us to check all \(V^{P B}\) different client batches. Instead, we leverage this filtering in a two-stage recovery algorithm that first recovers the client tokens \(_{i}^{*}\) at position \(i\) using the rank deficiency of \(}{_{1}^{Q}}\) (Token Recovery in Fig. 1), and then recovers the client batch of sequences \(^{*}\) based on \(_{i}^{*}\) and the rank deficiency of \(}{_{2}^{Q}}\) (Sequence Recovery in Fig. 1).

Token RecoveryOur token recovery method relies on the observation that \(f^{0}\) is computed per-token. Therefore, the input embeddings in \(_{1}\) are always part of the set \(\{f^{0}(v,i)|v[V],i[P]\}\). We apply our span check above to this set for the first layer gradients \(}{_{1}^{Q}}\) to filter the incorrect embeddings and their corresponding client tokens \(v\) at position \(i\), thus, constructing the set of correct client tokens \(_{i}^{*}\) at position \(i\).

Sequence RecoveryIn our sequence recovery, we leverage the fact that \(f_{i}^{1}\) is computed per-sequence and that the decoder mask \(\) ensures that the second layer input embeddings at position \(i\) do not depend on tokens with position \(>i\), i.e., \(f_{i}^{1}(s_{1},,s_{P})=f_{i}^{1}(s_{1},,s_{i})\), for any sequence of tokens \(s_{1},,s_{P}\). Crucially, for a correct client partial sequence \(s_{1},,s_{i-1}\) of length \(i-1\) this allows us to find the correct next token in \(_{i}^{*}\) by simply extending it with all possible token \(_{i}_{i}^{*}\) and then checking which of the resulting embedding vectors \(f_{i}^{1}(s_{1},,s_{i-1},_{i})\) is correct, i.e, is in \((}{_{2}^{Q}})\). We apply this procedure iteratively starting with the single token sequences \(_{1}^{*}=_{1}^{*}\), extending them one token at a time to produce the partial sequence reconstructions \(_{i}^{*}\), until the sequences cannot be extended anymore and return the result.

## 5 DAGER: Exact Sequence Recovery for Transformers

In this section, we present the technical details of DAGER. Specifically, we first theoretically derive of our filtering procedure based on the rank-deficiency of \(}{_{l}^{Q}}\) in Sec. 5.1. We then describe how we apply it on the gradients of the first and second self-attention layers to respectively recover the client tokens (Sec. 5.2) and sequences (Sec. 5.3).

### Efficient Embedding Filtering

Below, we discuss the technical details of our filtering procedure, outlined in Sec. 4, and prove its correctness. We first show that, under mild assumptions, the embedding vectors forming \(_{l}\) are linear combinations of the columns of \(}{_{l}^{Q}}\), restating this in terms of \((_{l})\) and \((}{_{l}^{Q}})\):

**Theorem 5.1**.: _If \(b<d\) and the matrix \(}{_{l}}\) is of full rank (rank \(b\)), then \((_{l})=(}{ _{l}^{Q}})\)._

Note that the assumption that \(}{_{l}}\) is full-rank holds in practice, as shown empirically in Dimitrov et al. , and that further \(b<d\) is almost always satisfied, i.e., that the total number of tokens in the input is smaller than the internal dimensionality of the model, for practical LLMs. We discuss the assumptions in further detail in App. B.2. The latter then directly implies the rank-deficiency of \(}{_{l}^{Q}}\), which we leverage to show:

**Theorem 5.2**.: _When \(b<d\), the probability of a random vector \(^{d}\) to be part of \((}{_{l}^{Q}})\) is almost surely \(0\)._Combining Theorems 5.1 and 5.2, we arrive at our main result stating that, if \(b<d\), an embedding vector \(\) that is part of the client self-attention inputs \(_{l}\) belongs to \((}{_{l}^{Q}})\), while random embedding vectors that are not part of \(_{l}\) almost surely do not.

Span Check ImplementationWhile the above result holds under real, i.e., infinite precision, arithmetic, for our method to work in practice, we require an implementation that is both fast and robust to numerical errors caused by floating-point arithmetic. We, therefore, introduce the metric \(d\), the distance between a candidate embedding vector \(\) and its projection on the \((}{_{l}^{Q}})\):

\[d(,l)=\|-(,(}{_{l}^{Q}}))\|_{2}.\] (2)

Intuitively, the closer \(d(,l)\) is to \(0\), the more likely \(\) is part of the span. To allow for efficient computation of the projection in Eq. 2, we first pre-compute an orthonormal basis for \(}{_{l}^{Q}}\) using an SVD and truncating the eigenvalues below a chosen threshold \(_{l}^{}\). We can then trivially compute this projection, as the sum of projections onto each basis vector. Finally, we say that a vector \(\) is in \((}{_{l}^{Q}})\), if the distance \(d(,l)<_{l}\) is below a chosen per-layer threshold \(_{l}\).

### Recovering Token Sets

We now describe how DAGER leverages the above filtering procedure to recover the input tokens exactly. To this end, we consider the set of all tokens in the model's vocabulary \(v[V]\) at every possible position \(i[P]\) and compute their input embeddings at the first layer via the per-token embedding function \(f^{0}\). We then filter out token-position tuples \((v,i)\) whose embedding vectors \(f^{0}(v,i)\) do not lie in \((}{_{l}^{Q}})\) to obtain the set input tokens (across batch elements) at position \(i\):

\[_{i}^{*}=\{v[V] d(f^{0}(v,i),1)<_{1}\}.\] (3)

We formalize this process in Algorithm 1, where we simply enumerate all token position tuples \((v,i)\). Additionally, we compute the length of the longest input sentence \(n\) as the largest position \(i\) of any recovered tuple \((v,i)\) (Line 7). If \(f^{0}\) is position-independent, e.g. when rotary instead of absolute positional embeddings are used, we recover the set of all input tokens \(^{*}=_{i}_{i}^{*}\) for every position. Our algorithm handles this at the sequence recovery stage at the price of slightly higher computational costs (see Theorem B.4).

```
1:functionGetTok(\(}{_{1}^{Q}},V,P,f^{0},_{1}\))
2:\(n 0\)
3:\(_{i}^{*}\{\}\), \(_{i}^{*}\{\}\)
4:for\(v,i[V][P]\)do
5:\( d(f^{0}(v,i),1)\)
6:if\(<_{1}\)then
7:\(n(n,i+1)\)
8:\(_{i}^{*}_{i}^{*}+\{v\}\)
9:\(_{i}^{*}_{i}^{*}+\{\}\)
10:return\(n,\{_{i}^{*}\}_{i=0}^{n},\{_{i}^{*}\}_{i=0}^{n}\) ```

**Algorithm 1** Recovering Individual Tokens

While conceptionally simple, this approach is exceptionally effective and robust to the distance threshold \(_{1,2}\), as we demonstrate in Fig. 2 for the GPT-2 model  (\(d=768\)) and a batch of \(B=32\) sequences consisting of \(b=391\) tokens. Despite the total number of tokens \(b\) exceeding half the model dimensionality \(d\), even our single layer (L1) filtering approach narrows the set of possible starting tokens, which are independent of the rest of the input, down to less than \(300\) from GPT-2's vocabulary of \(V 50K\) across a wide range of thresholds. Adding a second filtering stage using second-layer filtering, described next, allows DAGER to recover the exact \(32\) starting tokens. This is used to exactly narrow down the first token for each sequence, allowing us to inductively reconstruct the whole sentence for decoder-only models.

### Recovering Sequences

Given the set of input tokens, recovered above, we now describe how to recover input sequences by applying our filtering procedure to the inputs of the second self-attention layer \(_{2}\). We first define the set \(=_{1}^{*}_{P}^{*}\) of all sequences formed using the recovered token sets \(_{i}^{*}\). As the

Figure 2: Effect of L1 and L2 Filtering

second layer input embeddings \(_{2}=f^{1}()\) are computed independently for each sequence \(\), one can naively enumerate all \(\), compute their second layer embedding vectors \(f^{1}()\) and apply the span check for every position \(i\) to obtain the true set of client sequences:

\[^{*}=\{ d(f^{1}_{i}(),2)<_{2},\  i [P]\}.\]

Unfortunately, this naive approach requires \((B^{P})\) span checks. To alleviate this issue, we first show that the causal attention mask of decoder architectures allows us to greedily recover the exact sequences in polynomial time, before discussing heuristics that make an exhaustive search tractable for encoder-based architectures.

Recovering Decoder SequencesDue to the causal attention mask \(\) in decoder architectures, the \(i^{}\) input of the second-self attention layer \(f^{1}_{i}()\) depends only on the first \(i\) tokens of the input sequence \(\). We can thus apply a span check on the results of \(f^{1}_{i}\) to check arbitrary sequence prefixes of length \(i\). We leverage this insight in Algorithm 2 to iteratively recover the sets \(^{*}_{i}\) (Line 10) of input sequence prefixes

\[^{*}_{i}=\{^{*}_{i-1}^{*}_{i}  d(f^{1}_{i}(),2)<_{2}\},\]

starting from the set of empty sequences \(_{0}\) and extending them one token at a time (Line 6) until none of our sequences can be extended any further (Line 12).

For models with a small internal dimension \(d\), or batches with a large number of total tokens \(b\), the weight gradient \(}{^{Q}}\) might become full rank and all embeddings would pass our span check. To avoid this we set a maximum rank threshold, \(=(b,d-_{b})\) for the orthonormal basis computed via SVD (See Sec. 5.1). We visualize the effect of different \(_{b}\) on GPT-2, in Fig. 3, and observe that \(_{b}=20\) offers the best trade-off between stability and accuracy, yielding almost perfect reconstruction even for very large inputs with \(b\) very close to \(d\).

Recovering Encoder SequencesFor encoders, all second-layer embeddings \(f^{1}_{i}()\) depend on all input tokens. We thus cannot use the greedy reconstruction discussed above but have to enumerate all sequences in \(\). To make this search tractable, we leverage the following heuristics. We can determine the positions \(i\) of end-of-sequence (EOS) tokens in the input to determine the input sequence lengths \(n_{j}\). This allows us to recover input sequences by increasing length and eliminate the tokens constituting the recovered sequences from the token sets \(^{*}_{i}\). Additionally, we truncate the proposal token sets \(^{*}_{i}\) to the batch size \(B\) token closest to \((}{^{Q}_{1}})\). Finally, we always consider at most 10M sequences from \(\) before returning the ones closest to \((}{^{Q}_{2}})\).

We demonstrate the effectiveness of our heuristics in Fig. 4 for the base BERT model and different batch sizes \(B\) and note that for \(B=1\) we can still recover inputs perfectly, as \(|^{*}_{i}|=1\). We provide more details on DAGER for encoder-architectures in App. B.6.

Figure 4: Encoder Ablation Study

Figure 3: Encoder Ablation Study

Experimental Evaluation

We now describe our extensive experimental evaluation of DAGER. Our results demonstrate significant performance improvements compared to prior methods, on a variety of settings. We also present ablation studies for isolating the effects of each DAGER component.

### Experimental Setup

We evaluate DAGER on both encoder- and decoder-based models including BERT , GPT-2 , and variations of LLaMa [13; 37]. We consider three sentiment analysis datasets - CoLA , SST-2 , and Rotten Tomatoes (RT) , featuring sequences of varying lengths between typically 4 and 27 words. Additionally, we consider the ECHR  dataset, which contains sentences exceeding 1000 words to demonstrate the scalability of our approach in sequence length. We provide a more detailed description of our architectures and datasets in App. C. Following previous work, we report the mean and error of the ROUGE-1/2  scores, i.e., the overlap rate of unigrams and bigrams, respectively, over 100 batches, excluding padding tokens. We report the error as the 95% confidence interval given by twice the standard error. Wherever we cannot assume normality of the mean's distribution, we estimate the interval by generating \(10\,000\) random samples by bootstrapping. Additional details regarding computational requirements and hyperparameters can be found in App. C.

Comparison against baselinesFirst, we compared our performance against the state-of-the-art algorithms TAG  and LAMP  with batch sizes ranging between \(B=1\) and \(B=8\). We run the two attacks for 2500 iterations per experiment, making use of the hyperparameters described in their respective studies. As Balunovic et al.  provide two variations of the LAMP algorithm based on different objective functions -- LAMP\({}_{L2\,+\,L1}\) and LAMP\({}_{}\), we only report results from the variation with the higher ROUGE-1 score. In Table 2, we show results on GPT-2BASE and BERTBASE, assessing the performance on decoder-based and encoder-based models respectively.

The results indicate that for decoder-based models, such as GPT2, DAGER achieves near-perfect reconstructions across all datasets and batch sizes, significantly outperforming the baseline algorithms in every setting. Importantly, as further elaborated in App. C.1, DAGER achieves that while also being significantly more efficient -- 100 batches of size 8 on RT took 3.5 hours vs TAG and LAMP which required \( 10\) and 50 hours, respectively. Additionally, we confirm the claims made by Balunovic et al.  that LAMP outperforms TAG in the majority of settings. Examples of reconstructed sentences can be seen in Table 9 in App. C.3. We note that while we observe non-perfect ROUGE-2 scores on the SST-2 dataset, this is entirely due to an artifact of our metric library that assigns ROUGE-2 score of \(0\) to the SST-2's single-word sequences. We kept this behaviour to avoid having to rerun the baseline experiments, that also relied on this.

Further, Table 2 shows a significant improvement over prior work on encoder-based models like BERT, with near-perfect reconstruction for \(B=1,2\), and an average of 43% more tokens recovered for larger batch sizes. A significant advantage of DAGER over the baselines is its ability to more accurately recover the sentence structure, as evidenced by the much higher ROUGE-2 scores.

Main experimentsWhile prior attacks' performances become very poor for batch sizes as little as \(8\), we now demonstrate that DAGER is only limited by the embedding dimension of the model. To this end, in Table 3 we compare two decoder-only models, GPT-2BASE with \(d=768\), and LLaMa-2 7B with \(d=4096\) on \(B\) as large as 128.

The results are consistent with our claims that DAGER produces almost perfect reconstructions in all cases when the total number of client tokens is not extremely close to the embedding dimension \(d\). Further, while on LLaMa-2 DAGER achieves near-perfect reconstructions even up to a batch size of 128, for GPT-2 DAGER shows partial or complete failure for \(B=64,128\). This suggests that despite the significant computational costs of \(>2\) hours per batch for \(B=128\) on LLaMa-2, larger models have the potential to leak significantly more information. This is especially concerning given the current trend of ever-increasing model sizes. Finally, we observe the effect of attempting a best-effort reconstruction by establishing a rank threshold, as described in Sec. 5.3, when the gradients are of full rank. This allows DAGER to achieve a ROUGE-1 score of 30.3 (instead of 0) for GPT-2 on CoLA \(B=128\). A thorough ablation study on the advantage of this heuristic can be found in App. C.2.

Reconstruction under FedAvgThe FedAvg algorithm  is among the most widely used protocols in federated learning. It features \(E\) training epochs on minibatches of size \(B_{mini}<B\) with a fixed learning rate \(\). Despite featuring multiple low-rank gradient updates, this setting it remains vulnerable to our attack, as we elaborate in App. B.3. We show in Table 4 that FedAvg is susceptible to gradient leakage under DAGER for a range of reasonable learning rates and number of epochs.

Effect of Fine-tuning MethodsWe further demonstrate DAGER's versatility across a range of pretraining paradigms, including quantized models and Low-Rank Adaptation (LoRA)  finetuning. For both LLaMa-3 70B with \(B=1\) and LLaMa-3.1 BB with \(B=32\) at 16-bit quantization, we observed excellent ROUGE-1 and ROUGE-2 scores (>99%) (see Table 11 in App. C.5). We also present near-exact reconstructions under LoRA training, as DAGER can be directly applied to the decomposed weight matrix, with further technical specifics detailed in App. B.4. With LoRA updates of rank \(r=256\), which is standard for the LLaMa-2 model as noted by Biderman et al. , we observe ROUGE-1 and ROUGE-2 scores in the region of \(94-95\%\), given in Table 11. These results reaffirm that DAGER is applicable to common fine-tuning methods.

Effect of Model Size and Training on ReconstructionPrior work [11; 10] suggests that the size of a model, as well as, the degree of pre-training significantly affects the amount of leaked client information. To this end, in Table 5 we evaluate DAGER on the larger (GPT-2\({}_{}\)) and

    &  &  &  &  \\   &  &  &  &  &  &  \\   & GPT-2 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & LLaMa-2 (7B) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & GPT-2 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & LLaMa-2 (7B) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & GPT-2 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & LLaMa-2 (7B) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: Experiments on the FedAVG setting on the GPT-2 model with a batch size of 16 on the Rotten Tomato dataset. We use default set of hyperparameters of \(E=10\) epochs, learning rate \(=10^{-4}\) and mini-batch size \(B_{mini}=4\). R-1 and R-2 denote ROUGE-1 and ROUGE-2 respectively.

    & &  &  &  &  \\   & &  &  &  &  &  &  &  &  \\   & &  &  &  &  &  &  &  &  \\   & &  & \(7.0 2.5\) & \(0.54 0.54\) & \(8.0 2.0\) & \(1.4 1.3\) & \(7.8 1.2\) & \(0.8 0.5\) & \(5.3 0.7\) & \(0.4 0.2\) \\  & &  & \(73.3 4.5\) & \(43.3 7.0\) & \(26.8 2.8\) & \(11.0 3.0\) & \(13.4 1.4\) & \(3.9 1.2\) & \(8.9 1.2\) & \(1.9 0.6\) \\  & &  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & TAG &  &  & \) & \(6.0 1.7\) & \(0.5 0.4\) & \(6.1 1.2\) & \(0.6 0.6\) & \(4.4 0.6\) & \(0.2 6.4\) \\  & &  & \(62.9 6.9\) & \(18.8 8.4\) & \(21.4 3.1\) & \(9.2 3.1\) & \(9.8 2.0\) & \(9.7 1.3\) & \(8.1 1.1\) & \(0.7 0.4\) \\  & &  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & & 

pre-trained for 2 epochs (GPT-\(2_{}\)) variants of GPT-2 on the RT dataset for batch sizes up to 128. We observe very little difference in performance. In fact, the GPT-\(2_{}\)'s larger embedding dimension allows us to approximately reconstruct more tokens at larger batch sizes. We further note that a larger vocabulary does not negatively impact DAGER, as can be seen from the applications on LLaMa3.1-8B and LLaMa3.1-70B (see Table 11), which feature a vocabulary size of 128,256 tokens.

Reconstruction under Next-Token PredictionAdditionally, we evaluate our model on the next-token prediction task to demonstrate DAGER's efficacy under different contexts. We again achieve near-perfect results with ROUGE-1/2 scores of \(>99\). DAGER does not reach perfect scores because the last token in each client sequence only acts as a target and it is, thus, masked out from the input.

Reconstruction of Long SequencesFinally, to demonstrate our robustness to long sequences, we conducted a single experiment with \(B=1\) on the ECHR dataset truncated to 512 tokens. We obtain a perfect score of \(\) for ROUGE-1 and ROUGE-2 on GPT-\(2_{}\), emphasizing the general applicability of DAGER. In contrast, in the same setting LAMP achieves a ROUGE-1 of \(10.1 2.3\).

## 7 Limitations

As discussed in Sec. 5 and demonstrated in Sec. 6, the performance of DAGER on decoder-based models is only constrained by the embedding dimension \(d\). While an exact reconstruction for a number of tokens \(b>d\) is unachievable, we showed that the attack's effectiveness decreases only gradually with \(b\). Given our robust performance in an undefended setting, an interesting avenue for future work is to improve DAGER against different defense mechanisms, including but not limited to using the Differential Privacy SGD optimization process (DPSGD).

On the other hand, applying DAGER on encoder-based architectures for larger batches (\(B>>8\)) becomes challenging due to the high-order polynomial growth of the search space volume with respect to the batch size. These computational constraints make comprehensive exploration of the search space nearly impossible, thereby reducing the likelihood of achieving a feasible reconstruction. This issue extends to longer sequences, where the size of the search space expands exponentially with the maximum sequence length. To mitigate these effects, we propose that future research could focus on exploring further heuristics to efficiently reduce the search space.

## 8 Conclusion

We introduced DAGER, the first gradient inversion attack for transformers able to recover large batches of input text exactly. By exploiting the rank-deficiency of self-attention layer gradients and discreteness of the input space, we devised a greedy algorithm and a heuristic search approach for decoder-based and encoder-based architectures, respectively. Our results show that DAGER achieves exact reconstruction for batch sizes up to \(128\) and sequences up to \(512\) tokens. We further demonstrate DAGER's effectiveness across model sizes, architectures, degrees of pre-training, and federated learning algorithms, establishing the widespread applicability of our attack.

Our work demonstrates that recent decoder-based LLMs are particularly vulnerable to data leakage, allowing adversaries to recover very large batches and sequences in the absence of a robust defense mechanism. This underlying vulnerability highlights the need for increased awareness and development of effective countermeasures in privacy-critical applications. We hope this paper can facilitate further research into creating reliable frameworks for effective and private collaborative learning.

    &  &  &  &  \\   & R-1 & R-2 & R-1 & R-2 & R-1 & R-2 & R-1 & R-2 \\  GPT-\(2_{}\) & \(\) & \(_{-0.3}}\) & \(98.0 1.7\) & \(97.8 1.8\) & \(2.8 1.1\) & \(1.1 0.4\) & \(0.0 0.0\) & \(0.0 0.0\) \\  GPT-\(2_{}\) & \(\) & \(99.8^{+0.1}_{-0.3}\) & \(96.4 2.3\) & \(96.0 2.5\) & \(0.84 0.6\) & \(0.2^{+0.2}_{-0.1}\) & \(0.0 0.0\) & \(0.0 0.0\) \\ GPT-\(2_{}\) & \(99.9 0.0\) & \(99.7^{+0.2}_{-0.3}\) & \(99.6^{+0.3}_{-0.9}\) & \(99.4^{+0.3}_{-0.9}\) & \(2.3 0.8\) & \(0.5^{+0.3}_{-0.2}\) & \(0.0 0.0\) & \(0.0 0.0\) \\ GPT-\(2_{}\) & \(\) & \(99.8^{+0.3}_{-0.3}\) & \(\) & \(_{-0.2}}\) & \(\) & \(\) & \(0.0 0.0\) & \(0.0 0.0\) \\   

Table 5: Experiments on GPT-2 variations in different settings on the Rotten Tomatoes dataset. R-1 and R-2 denote the ROUGE-1 and ROUGE-2 scores respectively.

#### Acknowledgments

This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure).

This work has been done as part of the EU grant ELSA (European Lighthouse on Secure and Safe AI, grant agreement no. 101070617). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the European Commission can be held responsible for them.

The work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI).