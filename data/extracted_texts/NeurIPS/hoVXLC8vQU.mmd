# Convergence of \(\log(1/\epsilon)\) for Gradient-Based Algorithms in Zero-Sum Games without the Condition Number: A Smoothed Analysis

Convergence of \((1/)\) for Gradient-Based Algorithms in Zero-Sum Games without the Condition Number: A Smoothed Analysis

Ioannis Anagnostides

Carnegie Mellon University

ianagnos@cs.cmu.edu

&Tuomas Sandholm

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

###### Abstract

Gradient-based algorithms have shown great promise in solving large (two-player) zero-sum games. However, their success has been mostly confined to the low-precision regime since the number of iterations grows polynomially in \(1/\), where \(>0\) is the duality gap. While it has been well-documented that linear convergence--an iteration complexity scaling as \((1/)\)--can be attained even with gradient-based algorithms, that comes at the cost of introducing a dependency on certain condition number-like quantities which can be exponentially large in the description of the game.

To address this shortcoming, we examine the iteration complexity of several gradient-based algorithms in the celebrated framework of _smoothed analysis_, and we show that they have _polynomial smoothed complexity_, in that their number of iterations grows as a polynomial in the dimensions of the game, \((1/)\), and \(1/\), where \(\) measures the magnitude of the smoothing perturbation. Our result applies to optimistic gradient and extra-gradient descent/ascent, as well as a certain iterative variant of Nesterov's smoothing technique. From a technical standpoint, the proof proceeds by characterizing and performing a smoothed analysis of a certain _error bound_, the key ingredient driving linear convergence in zero-sum games. En route, our characterization also makes a natural connection between the convergence rate of such algorithms and perturbation-stability properties of the equilibrium, which is of interest beyond the model of smoothed complexity.

## 1 Introduction

We consider the fundamental problem of computing an _equilibrium_ strategy for a (two-player) zero-sum game

\[_{^{}}_{^{}}, ,\] (1)

where \(^{d+1}\{_{ 0}^{d+1}:^{} _{d+1}=1\}\) represents the \(d\)-dimensional probability simplex and \(^{n m}\) is the payoff matrix of the game. Tracing all the way back to Von Neumann's celebrated minimax theorem (von Neumann, 1928), zero-sum games played a pivotal role in the early development of game theory (von Neumann and Morgenstern, 1947) and the crystallization of linear programming duality (Dantzig, 1951). Indeed, in light of the equivalence between zero-sum games and linear programming (Adler, 2013; von Stengel, 2023; Brooks and Reny, 2023), many central optimization problems can be cast as (1).

State of the art algorithms for solving zero-sum games can be coarsely classified based on the desired accuracy of a feasible solution \((,)\), measured in terms of the _duality gap_

\[(,)_{^{}^{m}}, {A}^{}-_{^{}^{n}}^{ },.\] (2)

In the so-called low-precision regime, where one is content with a crude solution \((^{},^{})\) such that \((^{},^{})=: 0\), the best available algorithms typically revolve around the framework of _regret minimization_, both in practice (Farina et al., 2021; Brown and Sandholm, 2019; Zinkevich et al., 2007; Tang et al., 2023) and in theory (Carmon et al., 2020, 2019, 2024; Grigoriadis and Khachiyan, 1995; Clarkson et al., 2012; Alacaoglu and Malitsky, 2022)--in conjunction with other techniques to speed up the per-iteration complexity, such as variance reduction, data structure design, and sparsification (Zhang and Sandholm, 2020; Farina and Sandholm, 2022). Such algorithms have been central to landmark results in practical computation of equilibrium strategies even in enormous games (Brown and Sandholm, 2018; Bowling et al., 2015; Moravcik et al., 2017; Perolat et al., 2022).

The high-precision regime, where \((nm)}\), has turned out to be more elusive, with current LP-based techniques struggling to scale favorably in large instances. This deficiency can be in part attributed to the relatively high per-iteration complexity of LP-based approaches, such as interior-point methods or the ellipsoid algorithm, as well as their intense memory requirements. A promising antidote is to instead rely on iterative gradient-based methods that have a minimal per-iteration cost. Indeed, in a line of work pioneered by Tseng (1995), it is by known well-documented that _linear convergence_--an iteration complexity scaling only as \((1/)\)--can been achieved even with such methods (Tseng, 1995; Gilpin et al., 2012; Wei et al., 2021; Applegate et al., 2023; Fercoq, 2023). There is, however, a major caveat to those results: the number of iterations no longer grows polynomially with the dimensions of the game \(n\) and \(m\), but instead depends on certain condition number-like quantities that could be exponentially large in the description of the problem; it is thus unclear how to interpret those results from a computational standpoint.

To address those shortcomings, in this paper we work in the celebrated framework of _smoothed analysis_ pioneered by Spielman and Teng (2004). Namely, our goal is to characterize the iteration complexity of certain gradient-based algorithms in zero-sum games when the payoff matrix \(}\) is subjected to small but random perturbations, as formally introduced below.

**Definition 1.1** (Zero-sum games under Gaussian perturbations).: Let \(}}[-1,1]^{n m}\). We assume that the payoff matrix is given by \(}}}+}\), where each entry of \(}\) is an independent (univariate) Gaussian random variable with zero mean and variance \(^{2} 1\).

Randomness here is only injected into the payoff matrix and not the set of constraints (that is, the probability simplex), which is the natural model; after applying the perturbation, the problem should still be a zero-sum game in the form of (1). Under this model, we investigate the convergence of the following gradient-based algorithms.1 (Their formal description is given later in Appendix B.)

1. _optimistic gradient descent/ascent (OODA)_(Popov, 1980);
2. _optimistic multiplicative weights update (OWWU)_(Syrgkanis et al., 2015; Chiang et al., 2012; Rakhlin and Sridharan, 2013);
3. _extra-gradient descent/ascent (EGDA)_(Korpelevich, 1976); and
4. an iterative variant of Nesterov's _smoothing technique (IterSmooth)_(Gilpin et al., 2012; Nesterov, 2005).

Smoothed complexity allows interpolating between worst-case analysis--when the variance of the noise \(^{2}\) is negligible--and average-case analysis--when the noise dominates over the underlying input. An average-case analysis is often unreliable since--as Edelman (1993) convincingly argued--a fully random matrix does not necessarily capture typical instances encountered in practice. Spielman and Teng (2004) put forward the framework of smoothed analysis as an attempt to explain the performance of algorithms in realistic scenarios; to understand how brittle worst-case instances really are. They famously proved that the simplex algorithm, under a certain pivoting rule, enjoys _polynomial smoothed complexity_, meaning that its running time is bounded by some polynomial in the size of the input and \(1/\). Smoothed analysis is by now a well-accepted algorithmic framework with a tremendous impact in the analysis of algorithms. We also argue that it is particularly well-motivated from a game-theoretic perspective: there is often misspecification or noise when modeling a game, so smoothed analysis offers a compelling way of bypassing pathological instances that are perhaps artificial in the first place.

Nevertheless, we are not aware of any prior work operating in the smoothed complexity model per Definition 1.1 in the context of zero-sum games. To clarify this point, it is important to stress here that although zero-sum games can be immediately reduced to linear programs, that reduction is less clear in the smoothed complexity model. In particular, one set of constraints in the induced linear program takes the form \( v_{n}=:\), where \(_{n}^{n}\) is the all-ones vector. According to the usual model of smoothed complexity in the context of linear programs, randomness has to be injected into both \(\) and \(\), but that clearly disturbs the validity of the equivalence. More broadly, reductions in the smoothed complexity model are quite delicate (Blaser and Manthey, 2015); as a further example, even reductions involving solely linear transformations can break in the smoothed complexity model since independence--a crucial assumption in this framework--is not guaranteed to carry over. Relatedly, one interesting direction arising from the work of Spielman and Teng (2003) is to perform smoothed analysis in linear programs which are guaranteed to be feasible and bounded, no matter the perturbation; zero-sum games under Definition 1.1 constitute such a class. Besides the point above, different algorithms designed for the same problem can have entirely different properties, not least in terms of their smoothed complexity. The class of algorithms we consider in this paper is quite distinct from the ones shown to have polynomial smoothed complexity in the context of linear programs (described further in Appendix A). In many ways, gradient-based methods are simpler and more natural, which partly justifies their tremendous practical use. As a result, understanding their smoothed complexity is an important question.

### Our results

Our main contribution is to show that, with the exception of OMWU, the other gradient-based algorithms mentioned above (Items 1, 3 and 4) have polynomial smoothed complexity with high probability--that is to say, with probability at least \(1-(nm)}\).

**Theorem 1.2**.: _With high probability over the randomness of \(^{n m}\) (Definition 1.1), OGDA, EGOA and IterSmooth converge to an \(\)-equilibrium after \((n,m,1/)(1/)\) iterations._

The main takeaway of this result is that, modulo pathological instances, certain gradient-based algorithms are reliable solvers in zero-sum games even in the high-precision regime. Similarly to earlier endeavors in the context of linear programs (Spielman and Teng, 2004; Blum and Dunagan, 2002), a dependency of \((1/)\) (as in Theorem 1.2) is what we should expect; the one exception is the class of interior-point methods whose running time grows as \((1/)\), but those algorithms are (weakly) polynomial even in the worst case. We further remark that the polynomial dependency on \(n\) and \(m\) in Theorem 1.2 can almost certainly be improved, and we made no effort to optimize it.

Regarding OMWU, which is not covered by Theorem 1.2, we also obtain a significant improvement in the iteration complexity compared to the worst-case analysis of Wei et al. (2021), but our bound is still not polynomial. As we explain further in Appendix C.3, the main difficulty pertaining to OMWU is that the analysis of Wei et al. (2021) gives (at best) an exponential bound _no matter the geometry of the problem_. With that mind, our result is essentially the best one could hope for without refining the worst-case analysis of OMWU, which is not within our scope here. We anticipate that our characterization herein will prove useful in conjunction with future developments in the worst-case complexity of OMWU, as well as in the analysis of other iterative methods.

The error boundThe central ingredient that enables gradient-based algorithms to exhibit linear convergence is a certain _error bound_, given below as Definition 1.3. For compactness in our notation, we let \(:=^{n}\) and \(^{m}\). We then let \((,)\), \(\), and \(^{}:=^{}^{}\), where \(^{}\) and \(^{}\) represent the (convex) set of equilibria for Player \(x\) and Player \(y\), respectively.

**Definition 1.3** (Error bound).: Let \(()\) denote the duality gap as introduced in (2). We say that the zero-sum game (1) satisfies an _error bound_ with modulus \(_{>0}\) if

\[()\|-_{^{}}( )\|.\] (3)Above, \(_{^{}}()\) denotes the (Euclidean) projection operator; the set of games with a unique equilibrium has measure one, so we can safely replace \(_{^{}}()\) by the unique equilibrium \(^{}^{}\). It has been known at least since the work of Tseng (1995) that affine variational inequalities indeed satisfy (3). Nevertheless, it should come to no surprise that, even in \(3 3\) games, \(\) can be arbitrarily small (Proposition 3.1), which in turn means that, linear convergence notwithstanding, the number of iterations prescribed by an analysis revolving around (3) can be arbitrarily large. In fact, with the exception of OMMU, which is to be discussed further below, Definition 1.3 suffices to establish linear convergence (essentially) based on existing results.2 Our main result pertaining to Definition 1.3 is that the modulus \(\) is likely to be polynomial in the smoothed complexity model:

**Theorem 1.4**.: _With high probability over the randomness of \(\) (Definition 1.1), the error bound per Definition 1.3 is satisfied for any sufficiently small \((,1/(nm))\)._

To establish this result, the first step is to lower bound \(\) in terms of certain natural geometric features of the problem (Theorem 3.6), which is discussed further in Section 3.1. Establishing Theorem 1.4 then reduces to analyzing each of those quantities under Definition 1.1. It turns out that bounding those quantities also suffices for characterizing OMMU, whose existing analysis due to Wei et al. (2021) involves some further ingredients besides the error bound of Definition 1.3.

Further implicationsOur characterization of the error bound given in Theorem 3.6 has some further important implications. First, a well-known vexing issue regarding computing equilibria even in zero-sum games is that a solution with small duality gap can still be relatively far from the equilibrium in the geometric sense, a phenomenon further exacerbated in multi-player games (Etessami and Yannakakis, 2007). Therefore, results providing guarantees in terms of the duality gap are not particularly informative when it comes to computing strategies close to the equilibrium in a geometric sense. At the same time, there are ample reasons why the latter guarantee is more appealing (Etessami and Yannakakis, 2007). Theorem 1.4 implies that such concerns can be alleviated in the smoothed complexity model:

**Corollary 1.5**.: _With high probability over the randomness of \(\) (Definition 1.1), any point \(\) with \(()\) satisfies \(\|-^{}\|(n,m,1/)\)._

Beyond smoothed analysis, Theorem 3.6 applies to any non-degenerate game (Definition 3.2), and can be thereby used to parameterize the rate of convergence of gradient-based algorithms based on natural and interpretable game-theoretic quantities of the underlying game, which has eluded prior work. In particular, we make a natural connection between the complexity of gradient-based algorithms and _perturbation stability_ properties of the equilibrium. In light of misspecifications which are often present in game-theoretic modeling, focusing on games with perturbation-stable equilibria is well-motivated and has already received ample of interest in prior work (Balcan and Braverman, 2017; Awasthi et al., 2010); more broadly, perturbation stability is a common assumption in the analysis of algorithms beyond the worst-case model (Makarychev and Makarychev, 2021). There are different natural ways of defining perturbation-stable games; here, we assume that any perturbation with magnitude below \(>0\), in that \(\|^{}-\|_{2}\), maintains the support of the equilibrium and the non-degeneracy of the game; we call such games \(\)_-support-stable_ (Definition 4.1). In this context, we show the following result.

**Corollary 1.6**.: _For any \(\)-support-stable zero-sum game, 0000, EGDA and IterSmooth converge to an \(\)-equilibrium after \((n,m,1/)(1/)\) iterations._

That is, games in which \(\) is not too close to \(0\) are more amenable to gradient-based algorithms, which is a quite natural connection. Corollary 1.6 is shown by relating each of the quantities involved in Theorem 3.6 to parameter \(\) defined above.

## 2 Notation

Before we proceed with our technical content, we first take the opportunity to streamline our notation; further background on smoothed analysis and a description of the algorithms referred to earlier (Items 1 to 4) is given later in Appendix B, as it is not important for the purpose of the main body.

We use boldface letters, such as \(,,,\), to represent vectors in a Euclidean space. For a vector \(^{n}\), we access its \(i\)th coordinate via a subscript, namely \(_{i}\). Superscripts (together with parantheses) are typically reserved for the (discrete) time index. We denote by \(\|\|\) the Euclidean norm, \(\|\|:=^{n}_{i}^{2}}\), the \(_{}\) norm by \(\|\|_{}:=_{1 i n}|_{i}|\), and the \(_{1}\) norm by \(\|\|_{1}:=_{i=1}^{n}|_{i}|\). For \(,^{}^{n}\), we let \((,^{}):=\|-^{}\|\). \(()\) represents the linear space spanned by a given set of vectors. For \(^{n}\) and a subset \(B[n]\), we denote by \(_{B}^{B}\) the subvector of \(\) induced by \(B\). We let \(_{n}^{n}\) be the all-ones vector of dimension \(n\); we will typically omit the subscript when it is clear from the context. For vectors \(^{n}\) and \(^{m}\), we write \((,)^{n+m}\) to denote their concatenation. Throughout this paper, we use \(\) and \(\) to denote the strategy of Player \(x\) and Player \(y\), respectively.

To represent matrices, we use boldface capital letter, such as \(,\). It will sometimes be convenient to use \(^{}^{nm}\) to represent a vectorization of \(^{n m}\). We overload notation by letting \(\|\|\) be the spectral norm of \(\). For a matrix \(^{n m}\) and subsets \(B[n],N[m]\), we denote by \(_{B,N}^{B N}\) the submatrix of \(\) induced by \(B\) and \(N\). \(_{i,}\) and \(_{,j}\) represent the \(i\)th row and \(j\)th column of \(\), respectively. The singular values of a matrix \(^{d d}\) are denoted by \(_{1}()_{2}()_{d}( ) 0\) (not to be confused with our notation for the variance \(^{2}\)). To be more explicit, we may also use \(_{}()_{1}()\) and \(_{}()_{d}()\).

## 3 Smoothed analysis of the error bound

In this section, we perform a smoothed analysis of the error bound--as introduced earlier in Definition 1.3--in (two-player) zero-sum games. It is first instructive to point out why smoothed analysis is useful in the first place: the modulus \(\) can be arbitrarily close to \(0\) even when \(n=m=3\) (that is, \(3 3\) games); this is detrimental as the iteration complexity of algorithms such as GODA grows as a polynomial in \(1/\).

**Proposition 3.1**.: _There exists a \(3 3\) zero-sum game such that \(\) per Definition 1.3 is arbitrarily close to \(0\)._

In proof, it is enough to consider the ill-conditioned diagonal matrix

\[=&0&0\\ 0&2&0\\ 0&0&1,\] (4)

where \(0< 1\). The (unique) equilibrium of (4) reads \(^{}=^{}=(2,1,2)^{3}\). Now, considering \(=(1,0,0)\) and \(=(0,0,1)\), for the duality gap we have \((,)=\), while the distance of \((,)\) from the optimal solution \((^{},^{})\) is at least \(\). In turn, by Definition 1.3, this means that \( 2\). So, Proposition 3.1 follows by taking \( 0\).3

Proposition 3.1 exposes one type of pathology that can decelerate gradient-based algorithms, which is evidently related to the poor spectral properties of the payoff matrix. This intuition is quite helpful when equilibria are fully supported--as is the case in (4)--but has to be significantly refined more broadly, as we formalize in the sequel.

To sidestep such pathological examples, we thus turn to the smoothed analysis framework of Definition 1.1.

### Overview

The most natural approach to analyze the error bound in the smoothed complexity model is to rely on an existing (worst-case) analysis proving that a positive \(\) exists, and then attempt to refine that analysis. Yet, at least based on such prior results we are aware of, that turns out to be challenging. As an example, let us consider the recent analysis of Wei et al. (2021). As we explain in more detail in Appendix C.3, Wei et al. (2021) relate the modulus \(\) of the error bound to the (inverse of the) norm of a solution to a certain feasible linear program; the existence of a legitimate \(>0\) then follows readily from feasibility. Now, this reduction seems quite promising: Renegar (1994) has shown that the norm of a solution to a linear program can be bounded in terms of its _condition number_--the distance to infeasibility in our case, and Dunagan et al. (2011) later proved that the condition number of linear programs is polynomial in the smoothed complexity model. Nevertheless, there are some difficulties in materializing that argument. First, the induced linear program involves terms depending on both the payoff matrix and the geometry of the constraints (the probability simplex in our case). Consequently, the analysis of Dunagan et al. (2011) does not carry over since randomness is only injected into the payoff matrix. The second and more important obstacle is that the induced linear program depends on the optimal solution, which in turn depends on the randomness of the payoff matrix; this significantly entangles the underlying distribution. As there are exponentially many possible configurations, we cannot afford to argue about each one separately and then apply the union bound. This difficulty is in fact known to be the crux in performing smoothed analysis (Spielman and Teng, 2004).4

To address those challenges, we provide a new characterization of the error bound in terms of some natural quantities of the underlying game (Theorem 3.6), which in some sense capture the difficulty of the problem. We are then able to use a technique due to Spielman and Teng (2004), exposed in Section 3.3, to bound the probability that each of the involved quantities is close to \(0\) (Propositions 3.8 to 3.10), even though the underlying distribution is quite convoluted. The resulting analysis follows the one given by Spielman and Teng (2003) in the context of termination of linear programs, but still has to account for a number of structural differences.

In what follows, we structure our argument as follows. First, in Section 3.2, we relate the modulus \(\) to some natural quantities capturing key geometric features of the problem. Section 3.3 then proceed by analyzing those quantities in the smoothed analysis framework.

### Characterization of the error bound

Our first goal is to characterize the error bound in terms of certain natural quantities, which will then enable us to provide polynomial error bounds in the smoothed complexity model. Our only assumption here is that the zero-sum game is _non-degenerate_, in the sense of Definition 3.2 below; this can always be met with the addition of an arbitrarily small amount of noise (Lemma C.1). As such, our characterization here has an interest beyond the smoothed analysis framework, casting the error bound in terms of more interpretable game-theoretic quantities; for example, a concrete implication is given in Section 4.

Let us denote by \(v\) the _value_ of game (1), that is,

\[v=_{}_{},=_{}_{} ,,\]

which is a consequence of the minimax theorem (von Neumann, 1928). We are now ready to state the formal definition of a non-degenerate game.

**Definition 3.2** (Non-degenerate game).: A zero-sum game described with a payoff matrix \(\) and value \(v\) is said to be _non-degenerate_ if it admits a unique equilibrium \((^{},^{})\), and \(^{}\) and \(^{}\) make tight exactly \(n\) of the inequalities \(\{_{i} 0\}_{i[n]}\{,_{:,j}  v\}_{j[m]}\) and \(m\) of the inequalities \(\{_{j} 0\}_{j[m]}\{,_{i,:,j}  v\}_{i[n]}\), respectively.

In the sequel, we will make constant use of the fact that the set of degenerate games has measure zero under the law induced by Definition 1.1 (Lemma C.1).

In this context, we let \(B(^{})\{i[n]:^{}_{i}>0\}\) denote the _support_ of \(^{}\) (corresponding to Player \(x\)), and similarly \(N(^{})\{j[m]:^{}_{j}>0\}\) for the support of Player \(y\). The strict complementarity theorem (Ye, 2011) tells us that \(B\) indexes exactly the set of tight inequalities \(\{,_{i,:,j} v\}_{i[n]}\), and symmetrically, \(N\) indexes exactly the set of tight inequalities \(\{,_{:,j} v\}_{j[m]}\). In particular, this implies that \(|B|=|N|\) with probability \(1\). It will also be convenient to define \([n] B\) and \([m] N\).

Now, at a high level, one can split solving a zero-sum game into two subproblems: i) identifying the support of the equilibrium, and ii) solving the induced _linear system_ to specify the exact probabilitieswithin the support. It will be helpful to have that viewpoint in mind in the upcoming analysis, and in particular in the proof of Theorem 3.6. Roughly speaking, thinking of \(\) as a measure of the problem's difficulty, we will relate \(\) to i) the difficulty of identifying the support of the equilibrium, and ii) the difficulty of solving the induced linear system. To be clear, those two subproblems are only helpful for the purpose of the analysis, and they are certainty intertwined when using algorithms such as OODA.

Staying on the latter task, we will make use of a certain transformation so as to eliminate one of the redundant variables. Namely, for any \(}_{B}(B)\) and \(}_{N}(N)\), let us select a fixed pair of coordinates \((i,j) B N\) (for example, the ones with the smallest index). Using the fact that \(}_{B},=1\) and \(}_{N},=1\), we can eliminate \(}_{i}\) and \(}_{j}\) by writing

\[}_{B},_{B,N}}_{N}= },}-}, -},+d,\] (5)

where \(}_{ 0}^{},} _{ 0}^{}\) (for \( B\{i\}\) and \( N\{j\}\)) coincide with \(}_{B}\) and \(}_{N}\) on all coordinates in \(\) and \(\), respectively, and \(_{B,N}^{}=(^{},,,d)\) for a (non-singular) linear transformation \(^{(BN)(BN)}\). (We spell out the exact definition of \(\) later in Appendix C.1, as it is not important for our purposes here; it follows by simply writing \(}_{i}=1-},\) and \(}_{j}=1-},\).) The point of transformation (5) is that, by eliminating one of the redundant variables, there is a convenient characterization of the equilibrium (Claim C.3); namely, \(^{}=\) and \(^{}^{}=\).

We are now ready to introduce the key quantities upon which our characterization relies on. It turns out that those are analogous to the ones considered by Spielman and Teng (2003) in the context of analyzing the termination of linear programs; this is not coincidental, as our analysis was especially targeted to do so.

**Definition 3.3**.: Let \(\) be the payoff matrix of a non-degenerate game, \((^{},^{})\) the unique equilibrium, and \(B[n],N[m]\) the support of \(^{}\) and \(^{}\) respectively. We introduce the following quantities.

1. \(_{P}()_{i B}(_{i}^{})\) and \(_{D}()_{j N}(_{j}^{})\);
2. \(_{P}()_{j}(v-_{B}^{ },_{B,j})\) and \(_{D}()_{i}(_{i,N },_{N}^{}-v)\); and
3. \(_{P}()_{j}(_{,j}, (_{,-j}))\) and \(_{D}()_{i}(_{i,}, (_{-i,}))\), where we use the shorthand notation \(-i\{i\}\) (\(-j\{j\}\)), and \(=()\) is defined in (5).

(Above, we adopt the convention that if a minimization problem is with respect to an empty set, the minimum is to be evaluated as \(1\).)

Item 3 above will enable us to control the norm of solutions to any linear system induced by \(\), as we explain in the sequel. Our proof will actually rely on a slightly different matrix, which we call \(}\); the lemma below relates the geometry of \(}\) to \(\), and reassures us that the condition number of \(}\) cannot be far from that of \(\) so long as \(1-_{j}_{j}^{}_{D}()\) (by Item 1) is not too close to \(0\). (A symmetric statement holds when focusing on Player \(y\).)

**Lemma 3.4**.: _Let \(=}^{}=_{j} }_{j}^{}_{,j}\), and suppose that \(}^{}\) is such that its \(j\)th column is equal to \(_{,j}-\). Then,_

\[_{j}(_{,j},( _{,-j}))(1+|}{1-_ {j}_{j}^{}})_{j}(}_{,j},(}_{ ,-j})).\]

Next, we recall a fairly standard bound relating the magnitude of a solution to a linear system \(}=\) with the smallest singular value of a full-rank matrix \(\).

**Lemma 3.5**.: _Let \(^{d d}\) be a full-rank matrix. For any \(}^{d}\) there is \(^{d}\) with \(\|\|()}\|}\|\) such that_

\[}==_{j=1}^{d}_{j}_{,j}.\]Moreover, to connect Lemma 3.5 with \(_{P}()\), we observe that the smallest singular value can also be lower bounded in terms of the smallest distance of a column from the linear space spanned by the rest of the columns--which now matches the expression of Item 3 we saw earlier. In particular, we will make use of the so-called negative second moment identity (Tao et al., 2010) (Proposition C.4), which implies that

\[_{}(})} ^{-2}(}_{:,j},(}_{:,-j}))}}|}}_{j }(}_{:,j},(}_{:,-j})).\] (6)

Proposition C.4 also implies that \(_{D}()}_{P}()\), and so it will suffice to lower bound \(_{P}()\) in the sequel. We are now ready to proceed with the main result of this subsection. Below, we use the notation "\(\)" to suppress lower-order terms and absolute constants.

**Theorem 3.6**.: _Let \(\) be a non-degenerate payoff matrix, and suppose that \((_{P}(),_{D}())\), \((_{P}(),_{D}())\) and \((_{P}(),_{D}())\) are as in Definition 3.3. Then, the error bound (Definition 1.3) is satisfied for any sufficiently small modulus_

\[^{}\|_{}}} \{(_{D}())^{2}_{D}()_{P}( ),(_{P}())^{2}_{P}()_{D}( )\}.\]

It is enough to explain how to lower bound \(>0\) such that \(_{^{}},^{ }-v\|-_{^{}}()\|= \|-^{}\|\) for any \(\). In a nutshell, our argument is divided based on the magnitude \(\|_{B}\|\), which can be thought of as a measure of closeness from the support of the equilibrium. When \( 1\), which means that \(\) is still far from the support of the equilibrium, \(_{^{}},^{ }-v\) is governed by \(_{D}()\). In the contrary case, our basic strategy revolves around showing that the error bound can be treated as in the unconstrained case, which would then relate the modulus \(\) to the smallest singular value of the underlying matrix (essentially by Lemma 3.5)--and subsequently to \(_{P}()\) due to (6). Indeed, this turns out to be possible by working with matrix \(}\), as defined earlier in Lemma 3.4. We defer the precise argument to Appendix C.1.

### Smoothed analysis

Having established Theorem 3.6, our next step is to show that each of the quantities introduced in Definition 3.3 is unlikely to be too close to \(0\) in the smoothed complexity model, which would then imply Theorem 1.4. The main difficulty lies in the fact that each configuration that may arise depends on the support of the equilibrium, which in turn depends on the underlying randomization of \(\), thereby significantly complicating the underlying distribution. Further, one cannot afford to argue about each configuration separately and then apply the union bound as there are too many possible configurations. To tackle this challenge, we follow the approach put forward by Spielman and Teng (2003).

In particular, given that all quantities of interest in Theorem 3.6 depend on the support of the equilibrium, it is natural to proceed by partitioning the probability space over all possible supports, and then bound the worst possible one--that is, the one maximizing the probability we want to minimize. In doing so, the challenge is that one has to condition on the equilibrium having a given support (formally justified by Proposition C.5). To argue about the induced probability density function upon such a conditioning, it is convenient to perform a change of variables from \(\) to a new set of variables that now contains the equilibrium \((^{},^{})\) (Lemma C.6). The basic idea here is that since the event we condition on concerns the equilibrium, it is helpful to have that equilibrium being part of our set of variables. The induced probability density function is now quite complicated, but can still be analyzed using the following lemma.

**Lemma 3.7** (Spielman and Teng, 2003).: _Let \(\) be the probability density function of a random variable \(X\). If there exist \(>0\) and \(c(0,1]\) such that_

\[0 t t^{})}{(t)} c,\] (7)

_then_

\[[X X 0].\]In words, random variables whose density is smooth--in the sense of (7)--are unlikely to be too close to \(0\). Gaussian random variables certainly have that property (Lemma C.8), but it is not confined to the Gaussian law; the analysis of Spielman and Teng (2003)--and subsequently our result--is not tailored to the Gaussian case.

We are now ready to state our main results in the smoothed complexity model; the proofs are deferred to Appendix C.2. We commence with \(_{P}()\), which is the easiest to analyze. In particular, the following result is a consequence of an anti-concentration bound with respect to a conditional Gaussian random variable (Lemma C.7).

**Proposition 3.8**.: _Let \(_{P}()\) be defined as in Item 2. For any \( 0\),_

\[_{}[_{P}()^{}\|_{}}]}{ ^{2}}.\]

The analysis of \(_{P}()\) is more challenging, and makes crucial use of Lemma 3.7. As we alluded to earlier, a key step is to change variables from \(_{B,N}\) to \((,,,)\)--in accordance with (5)--and then to \((,^{},^{},)\) based on \(}^{}=\), \(^{}}^{}=\). It is important to note that \(\) no longer contains independent random variables even though \(_{B,N}\) is (by Definition 1.1); this stems from the presence of a redundant variable in \(^{}_{B}\) (since \(^{}_{B},=1\)). Nevertheless, we can still overcome this issue using Lemma 3.7, leading to the following bound.

**Proposition 3.9**.: _Let \(_{P}()\) be defined as in Item 3. For any \( 0\),_

\[_{}[_{P}()}\|_{:,j}\|+20\|^{}\|_{}+3} ]}{^{2}}.\]

Similar reasoning, albeit with some further complications, provides a bound for \(_{P}()\), which is given below.

**Proposition 3.10**.: _Let \(_{P}()\) be defined as in Item 1. For any \( 0\),_

\[_{}[_{P}()^{}\|_{}+1)^{2}}]mn(n, m)}{^{2}}.\]

Armed with Propositions 3.8 to 3.10 and Theorem 3.6, we can establish Theorem 1.2 by suitably leveraging existing results, as we formalize in Appendix C.3.

## 4 Parameterized results for perturbation-stable games

Another important implication of our characterization in Theorem 3.6 is that it enables connecting the convergence rate of gradient-based algorithms to natural and interpretable game-theoretic quantities. In particular, here we highlight a connection with perturbation-stable games, in the following formal sense.

**Definition 4.1** (Perturbation-stable games).: Let \(\) be the payoff matrix of a non-degenerate game. We say that the game is _\(\)-support-stable_, with \(>0\), if for any \(^{}\) with \(\|-^{}\|\) it holds that \(^{}\) is a non-degenerate game whose equilibrium has the same support as \(\).

Perhaps the simplest example of a support-stable game with a favorable parameter \(>0\) arises when \(\) is the \(2 2\) identity matrix. Indeed, as long as the perturbation parameter \(\) remains below a certain absolute constant, the perturbed game still admits a unique full-support equilibrium. To see this, suppose for the sake of contradiction that the perturbed game has an equilibrium such that Player \(x\) plays one of the two actions with probability \(1\). Player \(y\) would then obtain a utility of at least \(1-O()\). But the value of the original game was \(1/2\), which in turn implies that the value of the perturbed game is \(1/2()\); for a sufficiently small \(\) this leads to a contradiction. Similar reasoning applies with respect to Player \(y\). (The previous argument carries over more broadly to diagonally dominant \(2 2\) payoff matrices.)

As we have highlighted already, games with perturbation-stable equilibria--albeit under different notions of stability--have already received attention in the literature (Balcan and Braverman, 2017; Awasthi et al., 2010)(_cf._Cohen (1986)), and are part of a broader trend in the analysis of algorithms beyond the worst case (for further background, we refer to the excellent book edited by Roughgarden (2021)). Our goal here is to make the following natural connection.

**Theorem 4.2**.: _Any \(\)-support-stable game (per Definition 4.1) satisfies the error bound for any sufficiently small modulus_

\[(,,).\]

By virtue of our discussion in Appendix C.3, Theorem 4.2 immediately implies Corollary 1.6. Indeed, we observe that all parameters involved in Theorem 3.6 can be lower bounded in terms of the stability parameter of Definition 4.1, as we formalize in Appendix C.4.

## 5 Conclusions and future research

In conclusion, we performed the first smoothed analysis with respect to a number of well-studied gradient-based algorithms in zero-sum games. In particular, we showed that OQDA, EGDA and IterSmooth all enjoy polynomial smoothed complexity, meaning that their iteration complexity grows as a polynomial in the dimensions of the game, \(1/\), and \((1/)\); for OMWU, our analysis reveals a significant improvement over the worst-case bound due to Wei et al. (2021), but it still remains superpolynomial. We also made a connection between the rate of convergence of the above algorithms and a natural perturbation-stability property of the equilibrium, which is interesting beyond the model of smoothed complexity.

A number of interesting avenues for future research remain open. First, is it the case that OMWU has polynomial smoothed complexity or is there an inherent separation with the other algorithms we studied? Answering this question in the positive would necessitate significantly improving the worst-case analysis of OMWU due to Wei et al. (2021) (_cf._ Cai et al. (2024) for a recent development concerning the last-iterate convergence of OMWU). Beyond OMWU, our results could also prove useful for establishing polynomial bounds for other natural dynamics in the smoothed analysis framework. Moreover, our characterization of the error bound in Theorem 3.6 assumes that the game is non-degenerate. This is an innocuous assumption in the smoothed complexity model, as it holds with probability \(1\), but nevertheless it would be interesting to generalize it to any game. Doing so could shed some light into whether Theorem 4.2 holds with respect to other, perhaps more natural notions of perturbation stability beyond Definition 4.1. It would also be interesting to investigate other models of smoothed complexity that account for dependencies between the entries of the payoff matrix (Blaskara et al., 2024). Moreover, our focus has been on zero-sum games under simplex constraints, but we suspect that more general positive results should be attainable under polyhedral constraint sets; perhaps the most notable such candidate is the class of _extensive-form games_(Romanovskii, 1962; von Stengel, 1996). Even beyond (two-player) zero-sum games, Theorem 1.2 could apply to (multi-player) _polymatrix_ zero-sum games (Cai et al., 2016). It is less clear whether the model of smoothed complexity can be informative when it comes to convergence to _coarse correlated equilibria_ in multi-player games.