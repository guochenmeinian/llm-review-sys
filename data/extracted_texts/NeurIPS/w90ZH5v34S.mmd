# Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning

Jifan Zhang\({}^{1}\)1, Lalit Jain\({}^{2}\)1, Yang Guo\({}^{1}\)1, Jiayi Chen\({}^{1}\)2, Kuan Lok Zhou\({}^{1}\)2,

**Siddharth Suresh\({}^{1}\), Andrew Wagenmaker\({}^{2}\), Scott Sievert\({}^{1}\), Timothy Rogers\({}^{1}\), Kevin Jamieson\({}^{2}\), Robert Mankoff\({}^{3}\), Robert Nowak\({}^{1}\)**

\({}^{1}\)University of Wisconsin-Madison, \({}^{2}\)University of Washington, Seattle,

\({}^{3}\)Air Mail and Cartoon Collections

lalitj@uw.edu, {jifan,yguo}@cs.wisc.edu

Equal contribution.Equal contribution.

###### Abstract

We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human ratings on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker's weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even state-of-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.

## 1 Introduction

This paper presents a dataset and benchmark for investigating alignment in Large Language Models (LLMs). Our dataset contains over a quarter of a billion human ratings from the New Yorker's cartoon caption contest. Writing funny captions presents significant challenges due to the subjectivity of humor and variability in human judgments. This benchmark offers a unique challenge for AI alignment, reflecting complexities found in tasks where expert humans consistently outperform current AI systems. Our study examines fundamental questions about aligning them to generate funny captions similar to the winning captions that are most highly rated by the New Yorker readers.

We explore humor expression in LLMs, investigating whether these models can recognize humor and generate amusing captions that resonate with human audiences. While LLMs are not specifically designed for humor, their training on diverse content suggests a potential for humor recognition and expression. We propose a benchmark for evaluating a model's humor capabilities using advanced systems like GPT-4.

Our empirical analysis shows that current LLMs can generate humorous captions but significantly underperform compared to high-ranking human submissions in the New Yorker's caption contests. Generating successful captions requires multiple advanced capabilities: understanding of cultural references, recognition of humor patterns, logical reasoning, systematic planning, and visual analysis. The multi-component nature of caption generation makes this benchmark an effective test of broad LLM capabilities. Progress in aligning LLMs for this task will require both advancing these individual capabilities and developing methods to integrate them effectively. This benchmark therefore provides a comprehensive integration test for LLM capabilities.

Our main contributions and findings are:

**Dataset:** We present a large-scale dataset of human-rated cartoon captions from The New Yorker's weekly contest. Each week, the New Yorker hosts a contest with a new cartoon, where thousands submit their funny captions. Hundreds of thousands of ratings are collected for each contest, and the winning captions are determined by those receiving the highest ratings. This dataset enables researchers to explore humor generation in LLMs and represents the first large-scale dataset with human judgments for evaluating creative tasks. With over 250 million ratings, it offers diverse examples for studying humor expression and perception in AI systems.

**Benchmark:** We introduce new metrics for evaluating humor quality in LLM-generated content, using GPT4 and group based techniques. These metrics provide a standardized framework for assessing AI-generated humor. Our benchmark allows for systematic comparisons between human and AI-generated humor.

**Evaluation of State-of-the-Art Models:** We assess the performance of models such as GPT-4 and Claude in generating humorous content, comparing their outputs to human-generated examples. This analysis offers insights into the current capabilities and limitations of LLMs in humor generation, identifying areas of strength and potential improvement.

**Alignment Strategy Analysis:** We use our benchmark to evaluate various alignment strategies, including Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), and Best-of-N sampling (BoN). By comparing these strategies, we provide insights into their effectiveness in enhancing humor generation in LLMs and aligning AI systems with human preferences.

In summary, our paper advances LLM capabilities in humor evaluation and generation through a comprehensive dataset, a new evaluation benchmark, and analysis of model performance and alignment strategies. This work enhances our understanding of humor in AI systems and provides a foundation for future research in this field. We open-source our dataset and code as detailed in Appendix A.

## 2 Related Work

**New Yorker Caption Contest.** Since its original conception as part of the NEXT crowdsourcing system [25; 47], the New Yorker Caption Contest Dataset has been updated on a weekly basis for the last several years. During this time, the dataset has been primarily used for the evaluation of online algorithms and, similar to this work, to study the nature of humor. Works in the former camp include [34; 52; 59]. Perhaps the most relevant work to ours is . They formulated three tasks, matching, quality ranking and explanation generation for studying whether current AI systems _understand_ humor. Additional prior work includes [45; 40; 28], which utilize judgements made by the editors of the New Yorker directly to analyze a smaller number of contests (\(<50\)) and attempt to identify features that correlate with caption performance such as length, perplexity, readability and sentiment.

Figure 1: Overview of our workflow. During data collection, a new cartoon is released each week and thousands of captions are submitted. We then collect caption ratings through a crowd-sourcing procedure driven by a bandit algorithm. Our dataset is a collection of 365 contests, over 2.2M captions and over 250M human ratings. This dataset is utilized for our Humor generation task and benchmark. We experiment with finetuned open-source models and close-sourced API calls (both LLMs and MLLMs). Our novel and low-cost evaluator provides better reliability in evaluating captions.

**Alignment of LLMs.** Finetuning of LLMs has proved a critical step in aligning the behavior of pretrained models to downstream tasks. A standard pipeline is to first finetune the pretrained model via _supervised fine-tuning_ (SFT)--to imitate expert demonstrations--followed by _reinforcement learning from human feedback_ (RLHF) --where a reward model is trained on human preferences, and then the SFT model is trained to maximize this reward via PPO . This pipeline has been successfully applied for finetuning frontier models [68; 4; 38; 54], and has inspired a vast amount of follow-up work refining and extending the SFT [63; 64; 20; 35; 18] and RLHF [6; 16; 33; 48; 36; 51; 9; 11] methodologies. _Direct preference optimization_ (DPO) methods  have recently emerged as a simpler yet still effective replacement to the RLHF paradigm. Instead of training a reward model and then optimizing this reward, DPO combines these steps by directly optimizing the SFT model on offline human preference data, and has inspired a variety of extensions [21; 3; 49; 43; 53; 61]. While the aforementioned works focus on finetuning on human feedback, a related line of works has sought to finetune on AI-generated feedback [60; 57; 30; 8; 13; 62; 5]. Despite extensive research into various fine-tuning methodologies, understanding their effectiveness for creative tasks remains nascent. While several studies have explored when and why different methods are most effective [19; 29; 56; 10; 66; 46], they primarily address standard tasks like reducing harmfulness and increasing helpfulness, and do not assess fine-tuning methods for tasks requiring creativity, the focus of this work.

**RLHF Datasets.** Existing Reinforcement Learning with Human Feedback (RLHF) datasets, consisting of various responses to a prompt along with a preference ordering of those responses, have been critical for aligning existing AI systems to human preferences. We briefly review some of the most popular ones. Anthropic's HH-RLHF dataset  consists of chosen and rejected texts focusing on helpfulness and harmlessness. Stanford's SHP Dataset  and Stack Exchange preferences dataset  have aggregated questions and answers along with their ratings from various online platforms. OpenAI's summarization dataset  includes rankings of paired answers derived from human evaluations of text summaries. The data comes from a variety of sources, such as news articles and scientific papers, where human annotators compare the quality, coherence, and relevance of two different AI-generated summaries for the same text. The WebGPT comparisons  offer a dataset of human comparisons of AI-generated web search results, emphasizing the importance of high-quality, relevant information retrieval. Finally, we mention the Nectar dataset , which consists of a large series of prompts along with a list of five answers generated by various LLM's along with a ranking of these prompts by GPT-4.

**Humor in LLMs.** Several recent works have studied humor capabilities of large language models, in addition to the ones studying the New Yorker Caption Contest. Concurrent to our work, Zhong et al.  also studies humor in a multi-modal setting, focusing on a different humor game Oogiri. Their experiments also suggests that existing chain of thought techniques are insufficient for LLMs to generate and understand humor. Similarly, Jentzsch and Kersting  also shows GPT-3 still lacks humor abilities despite the good performance on other factual knowledge benchmarks. Furthermore, several works have focused on LLMs' capabilities in understanding humor, including humor detection , puns , and humor explanation .

## 3 New Yorker Caption Contest

Every week The New Yorker publishes an uncaptioned cartoon and solicits humorous captions from its readers through their website. The cartoon editors then review this list of captions and choose the top three funniest ones according to their judgement. The contest began in 2005, and at the time this work was written, there have been roughly 900 contests. For the last eight years, starting with contest 530, the New Yorker has utilized an online crowdsourced rating system (see Figure 2) where users are presented with captions and can rate whether the caption is funny (a reward of 3), somewhat funny (a reward of 2), or unfunny (a reward of 1). Each week a large number of captions are submitted (on average more than 6,000). These captions are first filtered by the New Yorker's editorial staff to remove captions that are not humorous or include personal information and/or offensive content, and then are sent to the crowdsourcing platform for large-scale rating. Finally, the New Yorker editors make their final decisions based on the crowdsourced ratings.

The rating process utilizes a multi-armed bandit-based algorithm, namely a UCB-variant (see [25; 52] and Appendix D for details), to present users with higher-performing captions more frequently in order to efficiently identify the best caption. Additionally, since many of the captions are unfunny,this keeps the rating engaging by presenting users interesting captions to rate compared to random sampling. On average the contest receives close to 780,000 ratings per week. The top 5% of captions receive an average of 821 ratings, and the bottom 50% of captions receive around 85 ratings.

The crowdsourced voting system for the New Yorker Caption Contest (NYCC) has resulted in an extensive dataset on human preferences and is a key contribution of this work. The dataset can be accessed at https://huggingface.co/datasets/yguooo/newyorker_caption_ranking. It consists of the cartoons, captions, and ratings for each one of 365 contests from contests 530 to 895. It provides an extensive labeled dataset on humor for researchers across multiple domains to study. In the related works, we describe some other works that have utilized this dataset. See Table 1 for more dataset statistics.

## 4 HumorousAI Benchmark: Funny Cartoon Caption Generation

In this section, we establish a benchmark method for evaluating the ability of large language models to generate funny captions. We start by describing the tasks in Section 4.1 followed by our proposed evaluation methods described in Section 4.2. Lastly, in Section 4.3, we give a brief overview of the various finetuning methods we explore in this paper.

### Task

We focus on the cartoon captioning task in this paper, where a model is given the information about the cartoon and is asked to generate funny captions about it. Specifically, we evaluate both multimodel large language models (MLLMs) and language-only models (LLMs). For MLLMs, we provide the raw cartoon images. For language-only models, we instead provide the descriptions and object entities of the cartoons. The text format of these descriptions are either written by human  or generated by MLLMs by given the images (see Appendix B.1 for details). See Table 7 for the example descriptions.

We hold out a set of \(91\) out of the 358 contests for evaluation by an evaluator (see Section 4.2). For each contest and its corresponding cartoon, we ask the language model to generate ten captions. This group of ten captions is then compared against four groups of past human submissions by the evaluator. For each contest, the four groups are captions ranked #1-10, #200-209, #1000-1009 and the ten captions that received median ranking. The evaluations are conducted along three dimensions:

1. **Overall comparison**: In this setting, the evaluator compares the overall funniness of the group of model-generated captions against each group of contestant-submitted captions. Win rates of the model-generated captions will be reported in Section 5 and Table 3.
2. **Best pick comparison**: We ask the evaluator to first pick the funniest caption from each of the two groups and then choose the funnier caption accordingly. Win rates are reported similarly to above.
3. **Caption diversity**: We measure the diversity of captions within each group of captions either generated by language models or submitted by human contestants in the past. Similarly to the study  on measuring the output diversity for non-creative tasks (summarization and instruction

   Number of contests & 365 \\ Number of cartoons & 365 \\ Average \#captions/contest & 6044 \\ STD \#captions/contest & 1794 \\ Total number of ratings & 284,183,913 \\ Average \#ratings/contest & 778,586 \\ STD \#ratings/contest & 325,156 \\ Max \#ratings/contest & 2,249,813 \\ Min \#ratings/contest & 31,173 \\ Average rating & 1.214(\(\)0.12) \\ Top 10 average rating & 1.824 (\(\)0.15) \\   

Table 1: Dataset statistics

Figure 2: Example voting page for contest 895

following), we use the expectation-adjusted distinct N-grams (denoted as **Average EAD**)  and the Sentence-BERT embedding cosine similarity (denoted as **SBERT**)  to measure the per-contest diversity. **Average EAD** measures the token-level similarity of the generated captions, while **SBERT** measures the semantic-level similarity. We do not use the NLI diversity from  as it is conversation-specific.

Our evaluation primarily focuses on comparing groups of captions since evaluation reliability can be significantly improved as we now discuss below.

### Evaluation Method

_Humor is notoriously subjective. Humans cannot infallibly predict what other humans will find funny._

_If they could, no joke would ever fall flat. We just do the best we can, always hoping we can do better._

_Likewise for these models._

-Bob Mankoff, former cartoon editor of The New Yorker

In this section, we aim to find a comparably reliable evaluation method for judging model-generated captions against human submissions. We experimented with various versions of GPT-4 and also human evaluations from Prolific . This task has been studied widely before within the context of humor [45; 40; 28; 22]. However, unlike these previous studies that only evaluate two candidate captions at a time (denoted by **Pairwise**), we introduce the novel group comparison techniques for evaluation (denoted by **Group Overall** and **Group Best Pick**). As described in Section 4.1, we compare groups of ten captions from different sources, such as human submissions from different ranking levels, or captions generated by different language models. To measure the reliability of different evaluators, as reported in Table 2, we compare their accuracy in judging human-submitted captions from top #10 versus #1000-1009 across \(200\) different contests. For the **Pairwise** comparisons, we uniformly at random choose one caption from each of the two groups, which exactly corresponds to the _ranking_ task proposed by Hessel et al. . For group comparisons, we provide all ten captions from each group to a single query to an LLM/human rater. The detailed prompts can be found in Appendix B for various language models. All of the prompts for evaluation utilize the 5-shot in-context prompting technique, which provides five caption comparison examples from other contests before asking the model to rank the pair/groups of captions for the given cartoon.

  
**Comparison Method** & Evaluator & Description/Image & Ranking Accuracy(\%) \\   & Human (worker) & GPT4o-vision & 61.67\(\)3.45 \\  & Human (worker) & Cartoon Image & 60.79\(\)3.46 \\  & GPT4-Turbo-vision & Cartoon Image & 61\(\)3.46 \\  & GPT4o-vision & Cartoon Image & 60.5\(\)3.47 \\  & GPT4o & GPT4o-vision & 65\(\)3.38 \\  & **GPT4-Turbo** & **GPT4o-vision** & **67\(\)3.33** \\  & GPT4-Turboo & GPT4-vision & 66\(\)3.36 \\  & GPT4-Turbo & Hessel et al.  & 66.5\(\)3.35 \\   & Human (worker) & GPT4o-vision & 59.23\(\)1.45 \\  & Human (worker) & Cartoon Image & 57.5.42\(\)1.37 \\   & Human (expert) & Cartoon Image & 94.28\(\)2.79 \\   & GPT4-Turbo-vision & Cartoon Image & 63\(\)3.42 \\   & GPT4o-vision & Cartoon Image & 74\(\)3.11 \\   & GPT4-Turbo & GPT4-vision & 73\(\)3.15 \\   & GPT4-Turbo & GPT4-vision & 74\(\)3.11 \\   & **GPT4-Turbo** & **Hessel et al. ** & **77.5\(\)2.96** \\   & Human (worker) & GPT4o-vision & 56 \(\) 2.22 \\   & Human (worker) & Cartoon Image & 63.66\(\)1.96 \\   & **GPT4o-vision** & **Cartoon Image** & **70.5\(\)3.23** \\   & GPT4-Turbo & GPT4-vision & 61.5\(\)3.45 \\   & GPT4-Turbo & Hessel et al.  & 60\(\)3.47 \\   

Table 2: **Evaluation reliability measure**: Ranking accuracy of captions ranked #1-10 vs captions ranked #1000-1009 averaged over 200 pairs. See Appendix B.1 for details on how the cartoon descriptions are generated.

As shown in Table 2, language models are generally more accurate in detecting the higher-ranked favorable captions in a group comparison paradigm compared to the pairwise paradigm. These models also outperform average humans (crowd workers) in judging the funniness across all three comparison settings. Notably, in the overall group comparisons we also included evaluations from a human expert (the former cartoon editor for The New Yorker). The expert significantly outperforms all other evaluators (AI and human), exposing a significant gap between human experts and SOTA AI systems in this domain. Also, the group comparisons are somewhat more challenging for crowd workers than pairwise comparisons, but group comparisons make the language model evaluations much more reliable and accurate. Further details about the evaluations can be found in Appendix C.

In conclusion, we establish two benchmark evaluation methods for the rest of this paper: **Group Comparison (Overall)** using GPT4-Turbo as evaluator with descriptions from Hessel et al.  and **Group Comparison (Best Pick)** using GPT4o-vision as evaluator with raw cartoon images.

### Alignment Finetuning Methods

In our study, we compare the performance of a 0-shot model (with standard and Best-of-N sampling) to that of an SFT finetuned model, an RLHF finetuned model, and a DPO finetuned model. We briefly outline these methods here, and refer the reader to [14; 4; 38; 41] for further details. In all cases, we adopt the implementation from the TRL package .

**Supervised Finetuning (SFT):** SFT assumes access to a dataset \(_{}=\{(x^{(i)},y^{(i)})\}_{i=1}^{N}\) of prompt-completion pairs, where \(y^{(i)}\) is assumed to be an "expert" completion for prompt \(x^{(i)}\). SFT then tunes the weight of the base model to maximize the likelihood of completions \(y^{(i)}\) given prompt \(x^{(i)}\).

**Reinforcement Learning from Human Feedback (RLHF):** RLHF assumes access to a preference dataset \(_{}=\{(x^{(i)},y^{(i)}_{w},y^{(i)}_{l})\}_{i=1}^{M}\), where \(x^{(i)}\) is a prompt, and \(y^{(i)}_{w},y^{(i)}_{l}\) two possible completions to \(x^{(i)}\), where \(y^{(i)}_{w}\) is preferred over \(y^{(i)}_{l}\). RLHF assumes these preferences are consistent with an (unknown) reward function \(r^{}\), typically assumed to follow the Bradley-Terry model . It first trains a reward model \(\) on \(_{}\), and then finetunes the base language model to maximize \(\), typically running PPO  and regularizing the training to ensure it does not deviate significantly from the SFT model.

**Direct Preference Optimization (DPO):** DPO operates under the same assumptions as RLHF, but skips the reward modeling step entirely, and instead finetunes the base language model on \(_{}\) directly, tuning it to produce next-token likelihoods with orderings consistent with \(_{}\).

**Best-of-N Sampling (BoN):** Best-of-N sampling does not modify the weights of the base model. Instead, it samples \(N\) completions from the base model for any prompt \(x\), and chooses the completion with the highest reward, as quantified by the reward \(\) obtained from the RLHF reward-learning step.

**Preference Dataset Construction:** In our setting, we take \(_{}\) to be a dataset of cartoon-caption pairs, where the captions \(y^{(i)}\) are drawn at random from the entire training set of captions for cartoon \(x^{(i)}\). \(_{}\) is constructed by taking a cartoon \(x^{(i)}\) and then two captions \(y^{(i)}_{w}\) and \(y^{(i)}_{l}\), where \(y^{(i)}_{w}\) is set to a caption with a higher human rating then \(y^{(i)}_{l}\). Specifically, we sample the pair to be at least 3 standard deviation apart from each other, i.e.

\[(y^{(i)}_{w})-(y^{(i)}_{l}) 3(y^{(i)}_{w})^{2}+(y^{(i)}_{l})^{2}},\] (1)

where \((y)\) is the average score of caption \(y\) from human raters according to rewards defined in Section 3 (note this is different from the rewards from the reward model of RLHF). \((y)\) is the corresponding standard deviation of scores from human raters.

## 5 Experiments

In this study, we evaluate the performance of caption generation. We experiment with two open-source large language models, Mistral 7b Instuct (mistralai/Mistral-7B-Instruct-v0.1) and the multimodal model LLaVa 7b (Ilava-hf/Ilava-v1.6-mistral-7b-hf) finetuned with methods in Section 4.3. We also evaluate state-of-the-art close-sourced models including GPT4o and Claude 3 Opus. See Appendix C.3 for more details. Our code is available at https://github.com/yguooo/cartoon-caption-generation.

### Experimental Results

In Table 3, we report the result for pretrained and finetuned model generations evaluated by GPT models. In Table 4, we ask human workers and expert to evaluate the captions generated by SOTA models. Below, we document some of our findings and research questions they inspire.

**MLLMs vs LLMs.** Surprisingly, language-only models such as the pretrained Mistral model outperform the multimodal LLaVa model that has access to the entire cartoon images. Similarly, for overall group comparison, GPT-4o is also preferred over GPT-4o with vision. To further investigate this issue, we conducted more experiments and obtained the following results:

1. GPT4-Turbo as evaluator given GPT4o descriptions (as reported in Table 2). Accuracy: 67%.
2. GPT4-Turbo-vision as evaluator given cartoon and GPT4o descriptions. Accuracy: 60.5%.
3. GPT4-Turbo-vision as evaluator given a blank image and GPT4o descriptions. Accuracy: 61.5%.

For bullet points 2 and 3 above, we are running the exact same model, the only difference is that one has access to the cartoon + text description, while the other has access only to the text description, thus isolating the effect of the image on the generation quality. We find that the visual element integration into the LLMs is negatively biasing the model's accuracy. This observation is also consistent for overall and best pick group comparisons. Since giving a blank image also hurts performance compared to bullet point 1, GPT4 without vision, it is unlikely that the performance of the vision model is dragged down by the visual understanding capabilities.

One possible reason for the above observation is that the training corpus for multimodal LLMs can be much less diverse than the training corpus for the LLM. For example, LLaVa is only trained on a small multi-modal instruction following dataset (\(\)80K unique images) , whereas generic LLMs like Mistral or LLama are trained on much larger dataset. Overall, these findings suggest there is still much research to be done in better integrating multi-modal capabilities into large language models.

Proposed Research Question #1: The multimodal large language models still underperform their language-only counterparts in caption generation. Can the vision-language integration in MLLMs be further improved to close this gap?

**Finetuning Open Source Models.** We observe that supervised fine-tuning hurts the model performance in the humor generation task in general. We believe this is primarily because we are aligning to captions in the top \(1000\), most of which are not particularly funny. However, we note this is an important step before RLHF and DPO training, as it trains the models to generate captions in the correct format. We also find that BoN sampling is able to substantially increase the Overall Win Rate metric, but falls short on the Best Pick Win Rate, which suggests the reward model is favoring a small set of good captions, but none of which generates particularly outstanding captions. We also observe in the next section that BoN indeed results in a less diverse group of generations.

    &  &  \\  Generated Caption &  & \#1000- &  &  & \#200- & \#1000- &  \\ Model & Top 10 & \#209 & \#1009 & Median & Top 10 & \#209 & \#1009 & Median \\  LLaVA & 3.85 & 2.20 & 4.40 & 13.19 & 2.75 & 6.59 & 4.95 & 12.64 \\ LLaVA SFT & 2.75 & 3.30 & 7.14 & 17.03 & 2.20 & 4.95 & 6.59 & 10.99 \\ Mistral-7B 0-Shot & 4.95 & 8.79 & 11.54 & 25.82 & 1.65 & 1.65 & 3.85 & 12.64 \\ Mistral-7B BoN & 6.59 & **16.48** & **21.43** & **35.71** & 1.65 & 2.20 & 3.30 & 10.44 \\ Mistral-7B SFT & 3.85 & 4.40 & 7.14 & 14.29 & 0.55 & 2.20 & 1.65 & 8.24 \\ Mistral-7B RLHF & 8.79 & 9.34 & 11.54 & 24.73 & 2.20 & 3.30 & 8.24 & 13.19 \\ Mistral-7B DPO & **9.34** & 13.74 & 17.58 & 31.32 & **10.44** & **15.93** & **14.29** & **30.22** \\  GPT-3.5 Turbo & 33.52 & 52.75 & 62.09 & 76.92 & 23.63 & 46.7 & 48.35 & 70.88 \\ GPT-4o & 44.51 & 69.23 & 79.12 & 86.81 & 42.86 & 59.89 & 73.63 & 79.67 \\ GPT-4o Vision & 42.31 & 63.74 & 76.92 & 85.16 & **47.80** & **65.93** & **79.67** & **85.71** \\ Claude-3-Opus & **54.40** & **70.88** & **81.87** & **88.46** & 40.11 & 59.89 & 63.74 & 79.67 \\   

Table 3: Evaluation of captions generated by various language models. We utilize group comparison strategies mentioned in Section 4.2. The generated captions are compared against four groups of human contestant entries at different ranking levels. Win rates are based on \(91\) held-out cartoons.

As compared to BoN, running RLHF on the same reward model is unable to achieve as high a level of performance. As we show in Appendix E, running PPO does indeed yield generations with higher reward score as given by the reward model, and as our BoN results indicate, filtering captions based on their reward does give better performance. This suggests that, while our reward model is able to effectively filter generations, tuning a model to maximize it does not necessarily lead to improved performance. We hypothesize that this is due to the complex nature of humor and the potential for out-of-distribution generations when running RLHF. While our reward model may effectively rank captions within a set of reasonable and in-distribution captions (for example those generated by the \(0\)-shot model), small deviations from the training distribution could lead to an erroneous reward signal. Furthermore, for tasks such as humor generation very subtle changes (for example, minor changes in word choice) can drastically change how humorous a caption is--the distribution of humorous captions is extremely sensitive. Together, we believe these phenomenon make it challenging for PPO to effectively finetune the weights to obtain significantly more humorous generations.

|}  Proposed Research Question \#2: Can we train a reward model able to better capture humor? Can RLHF still be effectively applied to settings where the distribution of correct responses is highly sensitive? \\  

In contrast to RLHF, DPO does yield a significant increase over the \(0\)-shot model for the Best Pick Win Rate metric. Note that DPO only optimizes the model on offline preference data and, as such, does not require an evaluation of any out-of-distribution samples. We hypothesize that, in settings such as humor generation where the desired distribution is extremely sensitive, this could lead to better performance, as it avoids the aforementioned issue where RLHF may quickly drift to producing out-of-distribution samples, for which the reward signal is erroneous.

|}  Proposed Research Question \#3: Does DPO lead to better in-distribution generation, and produce a model more effectively able to match the distribution of the finetuning data? \\  

Human Evaluation.We also ran a human evaluation using six workers from Prolific  along with a humor expert (a former New Yorker editor) to understand how often people preferred caption generations from Claude vs top 10 ranked captions generated by humans. We find that people only prefer Claude's generations 34% of the time. Our expert preferred Claude's generation only 1.6% of the time. He said, _"I think I preferred human captions because from my "expert" vantage point they were better phrased and more concise even independent from being funny. At this point AI tends to be too verbose in almost any task but, for me that is a liability when it comes to creating a good caption."_

This suggests that, though SOTA LLMs can generate a diverse set of funny captions, there remains a significant gap in their humor and creativity when judged from the perspective of human experts.

Example Generation and Qualitative Analysis.As shown in Table 5, we provide some generation samples for the cartoon in Figure 2. Indeed, we see that LLMs generally produce longer and more verbose captions than top human ones. Moreover, we generate multiple additional captions with GPT-4o-vision and Claude-3-Opus for the cartoon in Figure 2. Below, we make a qualitative analysis around the shortcoming of these generated captions from SOTA LLMs.

* **Missing visual details resulting in LLMs generating out-of-context captions.** As an example, GPT-4o-vision generated another caption of _"So the alien abduction statistics were right. Malls are the prime hunting grounds!"_ This caption does not match the cartoon though, since the aliens look missing and worried. Their bodies look skinny and weak. In other words, they don't seem to be here to hunt humans.
* **Many generated captions are forms of word/phrase modification and creation.** An example of this caption is **"Do they have a Black Hole Friday sale?"**, also generated by GPT4o-vision. Another example from Claude-3-opus is _"I don't see 'Ivasion Supplies' listed anywhere..."_ Both of these captions are inventing new words and phrases to make the caption funny. While these can be somewhat funny, they are usually not rated highly by the New Yorker audiences.

  Evaluator & Preference Rate \\  Human (expert) & **1.6\%** \\ Human (worker) & **35.4\%** \\   

Table 4: Rate of Claude-3-Opus generated captions preferred over Human Top 10.

* **Winning captions tend to appeal to readers with multiple interpretations through different lenses.** LLMs currently lack the ability to produce such captions. The New Yorker's editorial pick of the final caption was _"Oh sure, now you look at a map."_ While this caption makes fun of the aliens blaming each other, it also references the experience of a driver missing the direction but claiming they knew the way. On the contrary, GPT4o-vision generated the caption _"We travel light-years, and we still need directions!"_, which despite making fun of the characters in the cartoon with the same concept, lacks the relatability from an additional perspective.

Overall, we think the GPT4 models are capable of generating humorous content. However, to rank among the top requires much deeper understanding of cultural references and more steps of reasoning before arriving at a high quality caption.

### Diversity Evaluation

We evaluated the token-level and semantic-level diversity of the generation with results given in Table 6. We found the Average EAD and SBERT share the same trend when the base model is the same. Within the human generated caption group, we noticed that their diversity scores are very similar under both metrics. And the human generated texts regardless of their funniness are much more diverse than any model-generated captions.

For pretrained models, the commercial models like GPT, Claude-3 generally outperform the open-source model, like Mistral or LLaVa, in terms of diversity. Introducing the SFT and PPO procedure can moderately improve the diversity metrics for the Mistral model. This is in contrast to the findings of , which observed the opposite effect, that RLHF reduced diversity in regular text generation tasks. We also found that running DPO can yield a significant increase in the diversity of the model generations as compared to any other method. We hypothesize that this may be due to our finetuning dataset: for each cartoon, we run DPO with a variety of human-generated captions and it therefore learns not to prefer a single caption or type of caption, but a diversity of captions.

## 6 Future Work and Societal Impact

This paper opens a suite of research problems and challenges going forward and we are excited to continue working on multiple directions of future work.

**Improving creativity in LLM generation.** While LLMs are largely applauded for their creativity today, our experiments reveal there is still a significant gap between top human generated content and SOTA LLMs and MLLMs, especially when judged by an expert. We believe addressing the proposed research questions can not only improve funny caption generation, but also improve existing models on the creative generation tasks in general.

**Gamified evaluation of AI generated captions by a crowd.** As the nature of the funny cartoon captioning task is an engaging game by nature, we plan on building an AI versus Human battle ground rating game. Our envisioned game will allow users to submit their own captions. During rating, participants are presented with two sets of captions from different sources (human vs human, human vs AI and AI vs AI). This also provides us a more reliable system for evaluating new captions on new cartoons. At the same time, researchers are encouraged to submit AI model entries to test out their latest model/alignment methods.

**Humor vs offensiveness tradeoff.** Optimizing for humor abilities may result in increasing offensiveness and toxicity of model generated content. We believe an important next step is to study the challenge of balancing humor with potential offensiveness. As the boundary between humorous and offensive are often blurred, the subjective nature of humor and cultural sensitivities needs to be futher studied to ensure AI models align with human values.