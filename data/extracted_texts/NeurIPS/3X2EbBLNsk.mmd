# Birth of a Transformer: A Memory Viewpoint

Alberto Bietti

Flatiron Institute

&Vivien Cabannes

FAIR, Meta

&Diane Bouchacourt

FAIR, Meta

&Herve Jegou

FAIR, Meta

&Leon Bottou

FAIR, Meta

Work done while at FAIR, Meta.

###### Abstract

Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.

## 1 Introduction

As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors.

An important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context . These "in-context" learning capabilities are often attributed to the transformer architecture , in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require "global" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model.

In order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an _induction head_ mechanism [16; 40], namely a "circuit" of two attention heads that allows the transformer to predict b from a context \([,,,,]\), and which appears to be ubiquitous in transformer language models [40; 54].

In order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which mayrequire complex nonlinear dynamics [15; 33; 45]. This simplification also allows us to introduce a natural model for individual weight matrices as _associative memories_, which store input-output or key-value pairs of embeddings through their outer products. Random high-dimensional embeddings are particularly well-suited to this viewpoint thanks to their near-orthogonality. We provide a detailed empirical study of the training dynamics, by measuring how quickly each weight matrix learns to behave as the desired associative memory, studying how this is affected by data-distributional properties, and investigate the order in which layers are learned: the model first finds the right output associations from the current token and from uniform attention patterns, then the attention heads learn to focus on the correct key-value pairs. We then present theoretical insights on this top-down learning process through population gradient dynamics. Despite its simplicity, our setup already provides useful insights on the internal structure of transformer language models and its evolution throughout training, paving the way for a better understanding of LLMs. We hope that our insights may lead to future research and improvements for LLM practitioners, _e.g._, for optimization algorithms, data pre-processing and selection, interpretability, fine-tuning, and model editing.

In summary, we make the following contributions:

* We introduce a new synthetic setup to study global vs in-context learning: sequences follow bigram language models, where some bigrams change across sequences and others do not.
* We view the transformer's weight matrices as associative memories that learn to store specific pairs of embeddings, and use this to derive a simplified but more interpretable model for our task.
* We empirically study the training dynamics with careful probing: global bigrams are learned first, then the induction head is formed by learning appropriate memories in a top-down fashion.
* We give theoretical insights on training dynamics, showing how a few top-down gradient steps on the population loss can recover the desired associative memories by finding signal in noisy inputs.

Related work.After the success of transformer language models for in-context learning was found , several works have studied how in-context learning may arise in various contexts [8; 38; 43; 48; 57]. Multiple recent papers have introduced synthetic tasks in order to better understand and interpret transformers [9; 33; 39; 61]. Several works have attempted to understand internal mechanisms in transformers that are responsible for certain behaviors, an area known as "mechanistic interpretability" [16; 17; 35; 39; 40; 54]. Memory and neural networks have a long history of connections [5; 18; 19; 21; 26; 27; 30; 34; 50; 55; 56]. The associative memories we consider bear similarity to [29; 56], though we use continuous input/outputs. The reader may also be interested in Fast Weight programmers [46; 47]. The use of random vectors for storing memories is related to . Our approach to probing based on memory recall is related to techniques in [13; 17], though motivated differently. [14; 32; 36] study statistical and approximation properties of transformers, highlighting benefits of sparse attention patterns, but do not consider training dynamics. [25; 31; 49; 51] provide theoretical analyses of learning dynamics in transformers and other attention models, but consider different data setups and focus on single-layer architectures, while we focus on two-layer models and take a different viewpoint based on associative memories.

## 2 Background

This section provides background on transformer architectures and induction head mechanisms.

Transformer architecture.Transformers  operate on sequences of embeddings by alternating self-attention operations and token-wise feed-forward layers. We focus on decoder-only, auto-regressive architectures with a causal attention mask, which are commonly used in large language models trained for next-token prediction [6; 11; 41; 42]. We ignore normalization layers in order to simplify the architecture, since its stability benefits are not as crucial in the small models we consider. Given an input sequence of tokens \(z_{1:T}[N]^{T}\) of length \(T\), where \(N\) is the vocabulary size, the transformer operates as follows:

* **Token embeddings**: each discrete token is mapped to a \(d\)-dimensional embedding via an embedding map \(W_{E}^{d N}\). We will denote the embeddings of tokens \(z_{t}\) by \(x_{t}:=w_{E}(z_{t})\), where \(w_{E}(j)\) is the \(j\)-th column of \(W_{E}\).
* **Positional embeddings**: the positional embeddings \(p_{t}=w_{P}(t)^{d}\) are added to each token embedding depending on its position in the sequence, leading to the following input embeddings: \[x_{t}:=x_{t}+p_{t}=w_{E}(z_{t})+w_{P}(t).\] (1)* **Attention blocks**: given an input sequence \(x_{1:T}^{d T}\) of embeddings, the causal attention block computes, for \(W_{K},W_{Q},W_{V},W_{O}^{d d}\) (key, query, value, output), and for each \(t\), \[x_{t}^{}:=W_{O}W_{V}x_{1:t}(x_{1:t}^{}W_{K}^{}W_{Q}x_{t}) ^{d},\] (2) where \(\) takes the softmax of its elements, leading to an attention of the "values" \(W_{V}x_{t}\) with weights proportional to \(((W_{K}x_{s})^{}(W_{Q}x_{t}))\). Note that the attention operation usually considers multiple "heads" that each projects the input to a lower dimension. Here we stick to a single head for simplicity, since it will be sufficient for our purposes. Rewriting (2) on each \(t\) as \(x_{1:T}^{}=(x_{1:T};W_{K},W_{Q},W_{V},W_{O})\), the \(\)-th layer of the transformer applies attention with layer-specific parameters along with a residual connection as follows:2 * **Feed-forward blocks**: feed-forward blocks operate on individual token embeddings after each attention block, typically by applying a one-hidden-layer MLP to each token, denoted \((;W_{F})\), with a residual connection: at layer \(\), we have \[x_{t}:=x_{t}+(x_{t};W_{F}).\] Our simplified setup will linear feed-forward layers: \((x_{t};W_{F})=W_{F}x_{t}\).
* **Unembedding**: After the last transformer layer, the embeddings are mapped back to the vocabulary space \(^{N}\) through a linear "unembedding" layer \(W_{U}=[w_{U}(1),,w_{U}(N)]^{}^{N d}\), where we refer to the \(w_{U}(j)\) as "output embeddings". The output of this layer is then fed into a cross-entropy loss for predicting of \(z_{t+1}\) from each \(x_{t}\).

We will sometimes refer to the representations \(x_{t}\) for a given token \(t\) throughout layers as its _residual stream_, since they consist of sums of embeddings and layer outputs due to residual connections.

Induction head mechanism.Induction heads  are a particular type of mechanism (or "circuit") in transformers that allows basic in-context prediction of the form \([,,,,]\). These were found to be ubiquitous in transformer language models, playing a key role in enabling various forms of in-context learning. The basic mechanism consist of two attention heads in separate layers (see Figure 1 for an illustration): (i) the first is a _previous token head_ which attends to the previous token using positional information and copies its embedding to the next token; (ii) the second is the _induction head_ itself, which attends using the output of the previous token head, and outputs the original token. Our work focuses on this basic copy mechanism, but we note that richer behaviors are possible, particularly when combining multiple such mechanisms (_e.g._, ).

Figure 1: **Induction head mechanism**. Induction heads are a two-layer mechanism that can predict \(b\) from a context \([,a,b,,a]\). The first layer is a _previous token head_, which attends to the previous token based on positional embeddings (\(p_{t} p_{t-1}\)) and copies it after a remapping (\(w_{E}(a) w_{1}(a):=W_{O}^{}W_{V}^{}w_{E}(a)\)). The second layer is the _induction head_, which attends based on the output of the previous token head (\(w_{E}(a) w_{1}(a)\)) and outputs the attended token, remapped to output embeddings (\(w_{E}(b) w_{U}(b)\)). Boxes in the diagram represent different embeddings in superposition on each token’s residual stream (we omit some irrelevant ones for clarity, _e.g._, positional embeddings in upper layers), and attention and output associations are shown with the associative memory viewpoint presented in Section 4.

## 3 Synthetic Setup

In this section, we introduce our synthetic data setup, which allows us to carefully study how the induction head mechanism develops during training, and how transformers learn to use information from the context vs simple associations from the training data.

Bigram data model.Our model for sequences consists of a generic bigram language model (_i.e._, Markov chain), but where the transitions for a few _trigger tokens_ denoted \(q_{k}\) are modified in each sequence to always be followed by some _output tokens_\(o_{k}\). Let \(K\) be the number of trigger tokens, and fix the following distributions over the vocabulary \([N]\): \(_{b}(|i)\), \(_{u}\), \(_{o}(|i)\) and \(_{q}\), for \(i[N]\). \(_{b}(|i)\) are the global bigram conditionals, \(_{u}\) the global unigram distribution, while \(_{o}\) is used to sample output tokens at each sequence. The triggers are either fixed to some predefined set of tokens \(Q\), or sampled from \(_{q}\). Each sequence \(z_{1:T}^{n}\) is generated as follows:

* (optional) Sample \(q_{1},,q_{K}_{q}\), i.i.d. without replacement (_random triggers_)
* Sample \(o_{k}_{o}(|q_{k})\), i.i.d. with replacement.
* Sample \(z_{1}^{n}_{u}\) and \(z_{t}^{n}|z_{t-1}^{n} p_{n}(|z_{t-1}^{n})\) for \(t=2,,T\), where \[p_{n}(j|i)=_{b}(j|i),&i\{q_{k}\}_{k}\\ \{j=o_{k}\},&i=q_{k}.\]

Experimental setup and initial experiment.Our experiments take \(_{u}\) and \(_{b}\) to be unigram and bigram character-level distributions estimated from the tiny Shakespeare dataset, with vocabulary size \(N=65\). We generally sample triggers from \(_{q}=_{u}\) or fix them to the \(K\) most frequent tokens. We sample uniform outputs \(o_{k}\) in most cases, but also experiment with \(_{o}=_{b}\) in Section 5.

As a preliminary experiment, we train a two-layer vanilla transformer with single-head attention layers and MLP feed-forward layers, following the training setup described in Section 5. On our synthetic data, with fixed (resp. random) triggers and uniform outputs, the model achieves over 99% accuracy (resp. 95%) on output tokens after the first occurrence, versus around 55% for one layer. This gap may be related to the difficulty of modeling three-way interactions with a single attention layer . We visualize attention maps on test sequences in Figure 2, which shows that the model has learned an induction head mechanism. The sequence in the middle figure has \((q_{k},o_{k})\{(a,b),(t,s)\}\). For fixed triggers, the induction head is only active for the triggers used in training, which suggests the presence of a "memory" in the attention layer. For random triggers, it is active on every repeated token, so that the model then needs to disambiguate between in-context and global predictions. For instance, the model may choose to use the retrieved token when it is unlikely to be sampled from the global bigram distribution, something which we found to often be the case in practice.

## 4 The Associative Memory Viewpoint

In this section, we present our associative memory view on transformers: with nearly orthogonal embeddings, the weight matrices behave as associative memories which store pairs of embeddings as

Figure 2: **Induction head behavior in attention maps** observed on a 2-layer transformer trained on two variants of our synthetic dataset. Each row shows the attention pattern for predicting the next token. (left) The first layer head always attends to the previous token. (center) For fixed triggers \(Q=\{a,t\}\), the second layer head mainly attends to tokens following such triggers. (right) For random triggers, the induction head mechanism is active for any repeated token (here the only trigger is \(L\)). Red and green boxes highlight tokens following previous occurrences of the query, with red boxes corresponding to “correct” output tokens \(o_{k}\) following trigger tokens \(q_{k}\).

a weighted sum of their outer products. We then introduce a simplified transformer model with fixed random embeddings that will yield a precise understanding of learning dynamics using this viewpoint.

### Weight matrices as associative memories

While intermediate representations in the transformer consist of high-dimensional vectors in residual streams, they are often "collapsed" down to scalar measurements by testing against other representations, using operations of the form \(v_{j}^{}Wu_{i}\) for some matrix \(W\). For instance, \(u_{i}\) and \(v_{j}\) could be key and query vectors in an attention head, or input and output embeddings for predicting the next token. If \((u_{i})_{i}\) and \((v_{j})_{j}\) are orthonormal (or nearly-orthonormal) sets of embeddings, a natural way to store desired input-output associations \(i,j\) is through the following associative memory:

\[W=_{i,j}_{ij}v_{j}u_{i}^{},\] (3)

so that the scores \(v_{j}^{}Wu_{i}_{ij}\) may be used to assess the relevance of the \((i,j)\) pair, _e.g._, as part of a softmax operation in attention or next token prediction.

Random embeddings.A simple way to ensure that embeddings \((u_{i})_{i}\) and \((v_{j})_{j}\) are nearly-orthonormal is to set them to be random high-dimensional vectors, such as Gaussian vectors with variance \(1/d\) in \(d\) dimensions. Indeed, these are known to satisfy [23; 53]

\[u_{i}^{}u_{i} 1 u_{i}^{}u_{j} O (}),\]

so that (3) is a reasonable way to define an associative memory, without requiring an explicit activation function as employed in end-to-end memory networks . We may also easily create a "remapping" of an existing embedding \(u_{i}\) by multiplying it by a random matrix \(W_{0}^{d d}\) with Gaussian entries of variance \(1/d\), which is commonly used for initializing neural network parameters. The new **remapped embedding**\(W_{0}u_{i}\) is near-unit norm, and is near-orthogonal to \(u_{i}\) in addition to the other \(u_{j}\). Note that this fact implies that attention scores at initialization are near-uniform. See Appendix A for more details.

Learning associative memories.We now show that learning associations of input-output embeddings via gradient descent leads to a weighted associative memory of a form similar to (3).

**Lemma 1** (Gradients and associative memories).: _Let \(p\) be a data distribution over input-output tokens, and consider the following loss, where the input and output embeddings \(W_{E}\) and \(W_{U}\) are fixed:_

\[L(W)=_{(z,y) p}[(y,W_{U}Ww_{E}(z))],\] (4)

_with \(\) the cross-entropy loss. The gradients of the population loss \(L\) then take the form_

\[_{W}L(W)=_{k=1}^{N}_{z}[(_{W}(y=k|z)-p(y=k|z))w_{U }(k)w_{E}(z)^{}],\] (5)

_where \(_{W}(y\!=\!k|x)=(W_{U}Ww_{E}(z))_{k}\) are the model's predicted probabilities. Running gradient descent (with or without weight decay) from initialization \(W_{0}\) then leads to estimates of the following form, for some \(_{0}\) and \(_{ij}\) that vary with the number of iterations:_

\[=_{0}W_{0}+_{i,j}_{ij}w_{U}(j)w_{E}(i)^{}.\] (6)

Note that (4) is a convex problem in \(W\), thus with appropriate step-size and large enough number of steps (with no weight decay) we can expect gradient descent to be close to the global minimum. At the optimum, if the embeddings are nearly orthogonal, then (5) implies \(_{W}(y=k|z) p(y=k|z)\). We remark that if \(W_{0}\) is a Gaussian random matrix, as if often the case for neural network layers, the first term in (6) plays a minor role: testing \(W_{0}\) against an input-output pair \((i,j)\) with \(_{ij} 0\) will concentrate around zero when \(d\) is large, while the \((i,j)\) term in the sum will concentrate around \(_{ij}\). We also note that the gradient updates described above correspond to a so-called maximal feature learning regime similar to \(\)P updates in intermediate layers of deep networks [58; 59].

Handling superposition.In Lemma 1, we assumed that inputs to the matrix \(W\) are embeddings of a single token. Yet, in transformer models, the inputs to weight matrices are often sums, or _superpositions_ of embeddings. For instance, the initial representations of each token are sums of token and positional embeddings, and representations at later layers are sums of the outputs of each previous block, due to residual connections. Outputs of attention layers are also weighted sums of potentially many embeddings, at least initially when attention patterns are spread out. By linearity, associative memories of the form (6) simply operate individually on each embedding of a superposition, and return a new superposition (up to additional noise due to near-orthogonality). In practice, we will see that learned memories often focus on a single embedding and filter out the rest as noise when irrelevant (see also Section 6). We note that linearity can also be limiting, since it makes it difficult to map sets to specific output embeddings: \(u_{\{i,j\}}:=u_{i}+u_{j}\) needs to map to \(Wu_{i}+Wu_{j}\), and thus cannot map to a new embedding \(v_{\{i,j\}}\). Such mappings of sets thus require non-linear associative memories, for instance by leveraging a sparse decoding of which elements are actually present (_e.g._, using compressed sensing), or by using MLPs with non-linear activations [15; 30].

### A simplified two-layer transformer architecture

We consider a simpler two-layer transformer which is more interpretable with the memory viewpoint, and will help us analyze learning dynamics both empirically and theoretically.

* We freeze input, output and positional embeddings (\(W_{E},W_{U},W_{P}\)) to their random initialization throughout training. This brings us to the Gaussian random vector setup presented above.
* We fix \(W_{Q}^{1}=W_{Q}^{2}=I_{d}\), so that \(W_{K}^{1}\) and \(W_{K}^{2}\) play the role of both key and query matrices. This changes the gradient dynamics, but simplifies the model by avoiding the redundancy in (2). The pre-softmax attention scores then take the form \(x_{q}^{}W_{K}^{}x_{k}\), with \(x_{q}\) (resp. \(x_{k}\)) the query (resp. key) embeddings, which now directly resembles an associative memory lookup.
* We freeze \(W_{V}^{1}\), \(W_{Q}^{1}\), and \(W_{V}^{2}\) to random initialization. These play the role of _remapping_ attended tokens into new tokens, since for random \(W\) and large \(d\), \(Wx\) is nearly orthogonal to \(x\) and to any other random embeddings independent of \(x\).
* We train \(W_{Q}^{2}\), since the outputs of the induction head need to be mapped back into appropriate output embeddings in order to predict the output tokens \(o_{k}\) correctly.
* We use a single linear feedforward layer after the second attention block, with weight matrix \(W_{F}\).

This is plausibly the layer responsible for learning the global bigram distributions.

We remark that while this model freezes some parameters at initialization, it is richer than a "lazy" or neural tangent kernel approximation [10; 22; 24] since the model is still highly non-linear in its parameters and, as we will see, induces rich non-linear learning dynamics.

Solving the bigram problem with associative memories.We now show how the above architecture can solve the synthetic bigram problem from Section 3 with well-chosen weight matrices. While this is only a hypothetical model, we show in Section 5 that it is surprisingly faithful to the learned model.

Recall that due to residual connections, the inputs to the weight matrices typically consist of superpositions of various embeddings including token embeddings, positional embeddings, or "remapped" versions thereof. These may be viewed as sets, as illustrated in Figure 1, and associative memories can easily ignore certain elements of the set, _e.g._, ignore token embeddings by only focusing on positional embeddings. The induction head mechanism can be obtained by setting:

\[W_{K}^{1}=_{t=2}^{T}p_{t}p_{t-1}^{}, W_{K}^{2}=_{k Q}w_{E} (k)(W_{O}^{1}W_{V}^{1}w_{E}(k))^{}, W_{O}^{2}=_{k=1}^{N}w_{U}(k)(W _{V}^{2}w_{E}(k))^{},\] (7)

where \(Q\) is the set of triggers when they are fixed, or the support of \(_{q}\) when they are random. In words, the first attention layer matches a token to the previous tokens using positional embeddings. The second layer matches the trigger token to a remapping of itself by \(W_{O}^{1}W_{V}^{1}\), and the output matches a remapping of the input token by \(W_{V}^{2}\) to the corresponding output token. We remark that one can easily make the attention patterns more peaked on the correct associations by rescaling \(W_{K}^{1}\) and \(W_{K}^{2}\). The global bigram statistics can be encoded in the feed-forward layer as follows:

\[W_{F}=_{i=1}^{N}_{j=1}^{N}_{b}(j|i)w_{U}(j)w_{E}(i)^{}.\] (8)The question remains of how the model could trade-off predictions from the induction head and from the feed-forward layer, which are added together due to residual connections. With fixed triggers \(Q\), we may simply remove all \(i Q\) from the summation in (8), so that the model exclusively relies on the attention head for all triggers (indeed, the output of \(W_{O}^{2}\) is in the span of output embeddings, which are nearly orthogonal to the row space of \(W_{F}\)). When the triggers can vary across different sequences, choosing between the induction head and the feed-forward layer is more ambiguous as it depends on context, and \(W_{F}\) may try to learn more complex mappings that also use the outputs of \(W_{O}^{2}\). In practice, we observe that the model often prefers the induction head, unless its output agrees with one of the top predictions from the global bigram, in which case it tends to prefer those.

Beyond the simplified architecture.While our simplified architecture already captures the relevant aspects for the bigram model, it lacks some of the components that appear in standard transformers, such as non-linear MLPs, trained embeddings, layer normalization, and joint learning of a factorization \(W_{K}^{}W_{Q}\) (potentially with low rank matrices \(W_{K},W_{Q}^{d_{h} d}\) with \(d_{h}<d\) as in multi-head attention), instead of a single matrix \(W_{K}\). In practice, transformers also involve many more layers, as well as multiple heads at each self-attention layer. In Appendix D, we discuss how our memory viewpoint naturally extends to such architectural components, and we illustrate in Appendix E that they empirically lead to similar observations. Nonetheless, we focus on our simpler architecture in the main paper due to simplicity of exposition and better interpretability thanks to a clear identifiability of the role of each matrix, which is lost in models with more heads and layers.

## 5 Empirical Study

In this section, we present our empirical analysis of learning dynamics on the bigram data defined in Section 3, for the simplified architecture defined in Section 4.2. See Appendix E for additional results. Our code is available at https://github.com/albietz/transformer-birth.

Experimental setup.We train our models using mini-batch SGD with momentum, where each batch consists of 512 fresh sequences of length \(T=256\) sampled from our synthetic model. We use a fixed learning rate and weight decay. Hyperparameters are given in Appendix E. Unless otherwise noted, we use \(d=128\), random triggers with \(_{q}=_{u}\) and uniform output tokens. The reported accuracies and losses are computed over each fresh batch before it is used for optimization, and are averaged over relevant tokens: "in-context accuracy/loss" numbers only consider predictions of output tokens on triggers starting at the second occurrence (the first is non-deterministic), while "global loss" refers to average loss on non-trigger tokens.

Memory recall probes.In addition to loss and accuracy, we consider metrics to check whether individual matrices have learned the desired associative memories: for a desired target memory \(W_{*}=_{(i,j)}v_{j}u_{i}^{}\), the corresponding recall metric is computed from the empirical estimate \(\) as

\[R(,W_{*})=|}_{(i,j)}\{_{j^{}}v_{j^{}}^{}u_{i}=j\}.\] (9)

We use this for each matrix in (7) as target, and additionally test the previous token matrix \(W_{K}^{1}\) on smaller time windows. For the final feed-forward layer, we measure the average KL divergence between the predicted softmax distribution using only \(W_{F}\) and the global bigram distribution \(_{b}\):

\[d_{KL}(W_{F},_{b}):=_{k=1}^{N}d_{KL}((W_{U}W_{F}w_{E}( k)),_{b}(|k)).\] (10)

Emergence of the induction head via top-down learning.We begin our study by only training to minimize the loss on _trigger-output_ token predictions after their first occurrence. This should be predictable with 100% accuracy using the two-layer induction head mechanism according to Section 4. We also remove the feed-forward layer, in order to focus on the learning of attention matrices \(W_{K}^{1}\), \(W_{K}^{2}\) and \(W_{O}^{2}\) in isolation.

Figure 3 studies the effect of freezing different layers until iteration 300 on the training dynamics. By looking at memory recall probes, we see that training key-query matrices does not lead to any

learning unless \(W_{O}^{2}\) is learned first, and that \(W_{O}^{2}\) can learn the correct associations even when trained by itself with key-value matrices at random initialization. Recall that the attention weights are essentially uniform when \(W_{K}\) are at random initialization, so that training \(W_{O}^{2}\) alone resembles a bag-of-words models that aggregates representations throughout the sequence. While such a model has poor prediction accuracy, it is nevertheless sufficient to recover the correct associations in \(W_{O}^{2}\) (a similar observation was made in  in a different setup).

Then, these associations enable learning key-query matrices that focus the attention on relevant tokens, by storing relevant key-query pairs in the form of associative memories, which eventually recovers the desired induction head behavior and leads to near-perfect accuracy. The two rightmost plots suggest that the second layer is learned before the first, in the sense that \(W_{K}^{2}\) is easier to learn when \(W_{K}^{1}\) is frozen compared to the reverse, yet learning them together seems beneficial, possibly due to helpful feedback loops . We also observe that \(W_{K}^{1}\) fits previous token associations for early positions much faster than later positions (purple vs gray line). This is likely due to the fact that it should be enough for the previous token head to attend to the first appearance of each trigger \(q_{k}\), which is typically early in the sequence, so that most of the gradient will focus on early positions.

Overall, this provides a fine-grained understanding of the learning dynamics of induction heads. In Section 6, we analyze how a few gradient steps in a top-down fashion may suffice to recover appropriate associative memories in high dimension and with enough data. See Appendix E for additional experiments, including on the role of dimensionality.

Global vs in-context learning.Figure 4(left/right) shows that when training all layers jointly, the global bigram statistics tend to be learned more quickly than the induction head, as seen from the quick drop in loss and KL in early iterations. The \(W_{O}^{2}\) probe also seems to improve quickly initially, but only leads to mild improvements to in-context predictions. The full learning of the in-context mechanism takes longer, likely due to slower dynamics of the key-query matrices. We also observe a tension between \(W_{O}^{2}\) and \(W_{F}\) later in training, leading to slight degradations of our probe metrics. This may be due to the fact that the input to \(W_{F}\) now contains additional signal from the induction head which may be leveraged for better predictions, in particular for disambiguation in the case of random triggers, so that our guess of memories in Section 4.2 may no longer be accurate.

Role of the data distribution.We can see in Figure 4(left) that changes to the data distribution can have a significant effect on the speed of learning the in-context mechanism. We observe that the following may slow down in-context learning: (i) a smaller number of triggers \(K\), (ii) using only rare fixed triggers, and (iii) using random triggers instead of fixed triggers. By inspecting the individual memory probes (see Figure 5 in Appendix E), we hypothesize that (i) and (ii) are due to slow learning of \(W_{O}^{2}\), while (iii) is more related to slow learning of key-query matrices. This is reasonable since (i-ii) reduce the number of overall output tokens in the data, while (iii) increases the number of possible trigger tokens that should be stored in \(W_{K}^{2}\), thus increasing the data requirements in order to learn the full associative memory. We also show in Figure 4(center) that changing the output token distribution to bigram distributions at training time reduces the in-context accuracy

Figure 3: **Learning the induction head alone: in-context accuracy (top) and recall probes (bottom)** with some layers frozen until iteration 300. The output matrix \(W_{O}^{2}\) can and must be learned before the key-query matrices, but does not suffice for good accuracy. It is easier to learn \(W_{K}^{2}\) before \(W_{K}^{1}\), and \(W_{K}^{1}\) stores initial context positions (\(t<64\)) much faster than late positions.

when using out-of-distribution output tokens, while training on uniform outputs performs well on both distributions. This highlights that using a more diverse training distribution can lead to models with better generalization accuracy, with little additional training cost.

Additional experiments.In Appendix E, we provide additional experimental results for varying dimensionality, more complex architectures and training methods, as well as more fine-grained visualizations of the memory associations.

## 6 Theoretical Insights on Learning Dynamics

In this section, we provide theoretical insights on how gradients near initialization may allow the emergence of induction heads, and how this behavior is affected by data-distributional properties.

Finding signal in noisy inputs.In Lemma 1, we showed how gradient dynamics on a simple classification task with fixed embeddings of the inputs and outputs lead to associative memories. We now show that when inputs consist of superpositions of multiple embeddings, as is the case in the transformer residual streams, gradients may learn associative memories that filter out irrelevant components of these superpositions, focusing on useful signal instead.

**Lemma 2** (Gradient associative memory with noisy inputs).: _Let \(p\) be a data distribution on \((x,y)^{d}[N]\), and consider the following classification problem, with fixed output embeddings \(W_{U}\):_

\[L(W)=_{(x,y) p}[(y,W_{U}Wx)].\]

_The gradients take the following form: denoting \(_{k}:=[x|y=k]\) and \(_{k}:=_{x}[_{W}(k|x)}{p(y=k)}x]\),_

\[_{W}L(W)=_{k=1}^{N}p(y=k)w_{U}(k)(_{k}-_{k})^{}.\]

The key takeaway from this lemma is that with enough data (here infinite data), the associative memory arising from gradients can learn to filter out noise from inputs, since it only depends on its expectations or conditional expectations. In particular, \(_{k}\) can isolate relevant parts of \(x\) that are predictive of a label \(k\), and thus can lead to the right associations.

An illustrative example.To gain more intuition about this result, consider the following example: we would like to predict \(y\) from \(x=w_{E}(y)+p_{t}\), where \(p_{t}\) is a positional embedding at a random position \(t[T]\), which we would like to ignore. Further assume that \(y\) is uniformly distributed with \(p(y=k)=1/N\), and consider the matrix obtained after one population gradient step with step-size \(\) starting from an initialization \(W_{0}=0\) (so that \(_{W_{0}}(k|x)=1/N\)):

\[W_{1}=_{k=1}^{N}w_{U}(k)(_{k}-)^{},\]

Figure 4: **Global vs in-context learning and data-distributional effects. (left) Loss on global (dashed) vs in-context (solid) tokens throughout training, for fixed or random trigger tokens \(q_{k}\). The red curves fixes the trigger \(q_{1}\) to the most frequent token, while the fixed triggers in blue curves are less common. (center) In-context accuracy with different training and test distributions \(_{o}\) for output tokens. Uniform leads to better generalization than global bigrams \(_{b}\). (right) Probe metrics throughout training: \(W_{O}^{2}\) and \(W_{F}\) eventually compete and deviate from our natural estimates.**

with \(=[x]\). We show in Appendix B that when \(d\) is large enough to ensure near-orthonormal embeddings, we have

\[w_{U}(k)^{}W_{1}(w_{E}(y)+p_{t})\;\{k=y\}+O (}),\]

so that for large enough \(N\) and \(T\), we obtain a near-perfect classifier that ignores the positional embedding, after just one gradient step (but a highly idealized one). Understanding how this translates to the finite dimension and finite sample regime is an important theoretical question that we leave for future work (see  for an initial step in that direction). We note that data models related to the above have been useful to study gradient dynamics of neural networks on continuous data . Using a single gradient step to learn representations has also been fruitful in other contexts .

Learning the induction head with gradients.We may extend the arguments above to show how a few gradient steps can learn the induction head mechanism. We show the following in Appendix B.3.

**Theorem 3** (Learning induction head via three gradient steps, informal).: _In a simplified setup, the induction head mechanism as constructed in (7) can be learned via sequential gradient steps on the population loss from random initialization, on \(W^{2}_{O}\), then \(W^{2}_{K}\), followed by \(W^{1}_{K}\)._

To show this result, we use Lemma 2 in a similar manner to the illustrative example above to show how training \(W^{2}_{O}\) by itself at initialization, _i.e._, when the attention patterns are near-uniform, can recover the desired associative memory. This is possible because when predicting an output token at later occurrences of a trigger, the same output token is guaranteed to be present in the context, while other tokens need not appear more relative to other sequences. See also Figure 9 in Appendix E for numerical experiments verifying this for finite data and dimension. Once \(W^{2}_{O}\) has learned the correct associations, we show that the gradient with respect to the key-value matrix \(W^{2}_{K}\) at zero initialization can leverage the correctness of \(W^{2}_{O}\) to find the right associative memory that focuses the attention on correct triggers. Finally, by linearizing the second-layer attention around \(W^{2}_{K}=0\), we show how gradients w.r.t. \(W^{1}_{K}\) may learn correct associations for the previous token head.

## 7 Discussion

In this paper, we studied the question of how transformers develop in-context learning abilities, using a simplified setup that allows a fine-grained understanding the model and its training dynamics. While our model already captures rich phenomena at play in the bigram task we consider, more elaborate models are likely needed to understand transformers trained on more complex tasks like language modeling. This includes learning embeddings that are more adapted to the data and more structured (_e.g._, word embeddings , or grokking ), factorized key-query and value-output matrices that may induce additional regularization effects , and non-linear feedforward layers, which may provide richer associative memories between sets of embeddings. Understanding how transformers leverage such aspects to learn in richer settings is an important next step.