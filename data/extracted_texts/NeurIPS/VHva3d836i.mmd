# WizardArena: Post-training Large Language Models

via Simulated Offline Chatbot Arena

 Haipeng Luo\({}^{1}\)1 Qingfeng Sun\({}^{2}\)1 Can Xu\({}^{2}\)2 Pu Zhao\({}^{2}\)

Qingwei Lin\({}^{2}\)   Jianguang Lou\({}^{2}\)   Shifeng Chen\({}^{3}\)   Yansong Tang\({}^{1}\)   Weizhu Chen\({}^{2}\)

\({}^{1}\)Shenzhen International Graduate School, Tsinghua University  \({}^{2}\)Microsoft Corporation

\({}^{3}\)Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences

{luohp24@mails., tang.yansong@sz.}tsinghua.edu.cn, {shifeng.chen}@siat.ac.cn

{qins,caxu,puzhao,qlin,jlou,wzchen}@microsoft.com

###### Abstract

Recent work demonstrates that, post-training large language models with open-domain instruction following data have achieved colossal success. Simultaneously, human Chatbot Arena has emerged as one of the most reasonable benchmarks for model evaluation and developmental guidance. However, the processes of manually curating high-quality training data and utilizing online human evaluation platforms are both expensive and limited. To mitigate the manual and temporal costs associated with post-training, this paper introduces a Simulated Chatbot Arena named _WizardArena_, which is fully based on and powered by open-source LLMs. For evaluation scenario, _WizardArena_ can efficiently predict accurate performance rankings among different models based on offline test set. For the training scenario, we propose _Arena Learning_, an innovative offline strategy that simulates iterative arena battles among various state-of-the-art models on a large scale of instruction data using AI-driven annotations to evaluate and leverage battle results, thus continuously enhancing the weaknesses of the target model through both supervised fine-tuning and reinforcement learning. Experimental results demonstrate that our WizardArena aligns closely with the online human arena rankings, and our models, trained on extensive offline battle data through Arena Learning, demonstrate marked improvements in performance across the SFT, DPO, and PPO stages.

## 1 Introduction

In recent years, the field of natural language processing (NLP) has witnessed a remarkable transformation, driven by the rapid advancements in large language models (LLMs). These models, trained on vast amounts of text data, have demonstrated an exceptional ability to understand, generate, and interact with human language in a wide range of tasks . One of the most exciting applications of LLMs has been in the realm of conversational AI , where they have been utilized to create powerful chatbots capable of engaging in naturalistic dialogues. One of the key factors contributing to the success of LLM-powered chatbots is the ability to leverage large-scale high-quality instruction following data for effective post-training . By exposing these models to a diverse range of conversational tasks and instructional scenarios, researchers have been able to imbue them with a deep understanding of how to effectively communicate and assist humans.

In this context, the emergence of the Chatbot Arena [15; 16] has been a significant development. This is a platform that facilitates the assessment and comparison of different chatbot models by pitting them against each other in a series of conversational challenges and rank with Elo rating system . By leveraging a diverse set of human evaluators, the Chatbot Arena provides a more robust and comprehensive evaluation of chatbot performance, going beyond the limitations of traditional benchmarking approaches. At the same time, they also opened up some real direct chat and battle preferences data , which have been proven to be valuable resources for model post-training and developmental guidance . However, the manual nature of the human-based evaluation process poses its own set of challenges: Manually orchestrating and coordinating the interactions between chatbots and human evaluators can be time-consuming and resource-intensive, limiting the scale and frequency of evaluation and training data opensource cycles. On the other hand, due to their priority limitations , most models are unable to participate in arena evaluations, making it impossible to directly guide the development of the target model on it. So, the need for a more efficient and scalable arena-based pipeline to chatbot post-training and evaluation has become increasingly pressing.

To address these challenges, this paper introduces a novel approach called WizardArena, a simulated offline chatbot arena that is fully based on and powered by AI LLMs without human evaluators. The primary objective of WizardArena is to mitigate the manual and temporal costs associated with post-training LLMs while retaining the benefits of arena-based evaluation and training. As the running example shown in the Figure 1, the key is that WizardArena can efficiently predict accurate performance rankings among different battle models based on a powerful "judge model", which could automatically imitate the manner of human annotators in judging a responses pair of two models and provide rankings, scores, and explanation.

In the training scenario, We innovatively propose _Arena Learning_ strategy to simulate arena battles among target model (referred to as WizardLM-\(\)) and various state-of-the-art models on a large scale of instruction data. These synthetic battle results are then used to enhance WizardLM-\(\) through some training strategies, including supervised fine-tuning (SFT), direct preference optimization (DPO) , and proximal policy optimization (PPO) , enabling it to learn from the strengths and weaknesses of other models. Furthermore, _Arena Learning_ introduces an iterative battle and training process, where the WizardLM-\(\) is continuously updated and re-evaluated against SOTA models. This process allows for the WizardLM-\(\) to iteratively improve and adapt to the evolving landscape of the arena, ensuring that it remains competitive and up-to-date with the latest advancements in the field.

In the evaluation scenario, we firstly contribute a carefully prepared offline testset, it effectively balances the diversity and complexity of evaluation. By automating the pair judgement process with "judge model", WizardArena significantly reducing the associated costs and priority limitations, and could produce the Elo rankings and detailed win/loss/tie statistics.

The experimental results demonstrate that the Elo rankings produced by WizardArena align closely with the LMSys Chatbot Arena. This finding validates the effectiveness of WizardArena as a reliable and cost-effective alternative to human-based evaluation platforms. Moreover, the models trained on the extensive battle data generated by _Arena Learning_ exhibit significant performance improvements during the SFT, DPO, and PPO stages. In three iterative loops, our model can also scale up with more training data and achieve better performance. These results highlight the value and power

Figure 1: Overview of Running Example.

of _Arena Learning_ in post-training, which leverages the collective knowledge and capabilities of multiple models to drive the WizardLM-\(\)'s performance to new heights. Our main contributions are as follows:

* We introduce _Arena Learning_, a novel AI powered method which help us build an efficient data flywheel for large language models post-training by simulating offline chatbot arena, which leverages AI annotator to mitigate the manual and temporal costs.
* We contribute a carefully prepared offline testset of AI-based _WizardArena_, and demonstrate the highly consistent in accurately predicting Elo rankings among different LLMs compared to human-based LMSys Chatbot Arena.
* Experimental results demonstrate the effectiveness of _Arena Learning_ in producing large-scale synthetic data to continuously improve WizardLM-\(\), through various training strategies including SFT, DPO, and PPO.

## 2 Related Works

### LLM Benchmarks

Large Language Models (LLMs) have transformed the way people interact with computing systems and are extensively used in everyday life and work . The existing benchmarks [24; 25; 26] are mainly divided into two categories: 1) Specialized tasks. Knowledge and Capability: MMLU , CMMLU , and C-Eval ; Reasoning: ARC , HellaSwag , PIQA, GSM8k , MATH ; Programming: HumanEval , MBPP , LiveCodeBench ; Safety and Truthfulness: ToxicChat , OLID , BIG-Bench , TruthfulQA . They focus on assessing LLM performance in specific areas. 2) General tasks: like MT-Bench [15; 42] and AlpacaEval , encompass categories such as writing, role-playing, and mathematics, highlighting the models' comprehensive abilities and multi-turn dialogue performance.

Real-world benchmarks, (i.e., LMSYS ChatBot Arena  and Allenai WildBench ) use anonymous battles, ELO [17; 46] rankings, and human judgments, but have time delay and often do not timely reflect the models' true performance and require large time and human labor intensive. Additionally, most models overfit on leaderboards like MT-Bench , OpenLLM leaderboard [47; 48], showing inconsistent performance with real-world ChatBot scenarios and low differentiation among models. Therefore, we have developed Simulated Offline WizardArena, which not only effectively differentiates model performance but also aligns closely with the online live ChatBot Arena , making it suitable for selecting the optimal models while significantly enhancing model post-training through battle data.

### Large Language Models

LLMs have made significant strides in Natural Language Processing (NLP), serving as a versatile foundation for numerous applications [23; 49; 50]. These models, which often contain hundreds of billions of parameters, are trained on expansive text datasets. Notable examples include OpenAI's GPT-3 and GPT-4 [4; 51], Anthropic's Claude , Google's PaLM [53; 54], Gemini , and DeepMind's Chinchilla . The AI field has recently seen a surge in open-source LLMs, providing public access to model codes and parameters. Notable releases include BigScience's BLOOM , Mistral AI's Mistral , Meta's Llama family [3; 58] and GAL , Tsinghua University's ChatGLM , TII's Falcon  and Yi . New entries such as Baichuan, Qwen , DeepSeek , and Llemma  have also emerged. Presently, models like Alpaca , Vicuna , Guanaco , Orca , OpenChat , Tulu2 , WizardLM , and Zephyr  are being developed through supervised fine-tuning based on Llama [3; 58] and Mistral .

The alignment performance of Large Language Models (LLMs) is significantly influenced by the quality of Supervised Fine-Tuning (SFT) data, which encompasses task difficulty , query complexity [12; 69], semantic diversity [11; 14], and sample size . For instance,  generates diverse queries through self-instruct  methods, while  enhances model alignment by increasing query complexity.  boosts NLP task performance by optimizing FLAN  queries and responses with specialized LLMs, and  has introduced UltraChat. To select data efficiently, some strategies like IFD , INSTAG , DEITA , MODS , and ALPAGASUS  are adopted.  employs ChatGPT to label instructional data, ensuring both diversity and complexity. Here, we select training data using the judge pair method with different models.

To better adapt to preferences beyond SFT, models are trained with feedback-based methods like RLHF and RLAIF [2; 52; 58; 78; 79], employing Proximal Policy Optimization (PPO)  to align with model preferences. RLEIF  combines instruction evolution and reinforcement learning to enhance the mathematical reasoning capabilities of the model. Due to RLHF's complexity and instability, simpler alternatives like DPO , RRHF , KTO , IPO , sDPO , and ORPO  are utilized. DPO  merges reward modeling with preference learning, RRHF  uses ranking loss to prioritize preferred answers, and KTO  operates without needing paired preference datasets. In this paper, we use DPO and PPO for model post-training.

## 3 Approach

In this section, we elaborate on the details of the proposed _WizardArena_ and _Arena Learning_ strategy. As illustrated in Figure 2, the pipeline mainly contains three components: Offline Pair-size LLM Battle Arena, Model Evaluation, and Model Training.

### ChatBot Arena and Elo Ranking

The Chatbot Arena is a pioneering platform that has revolutionized the way chatbot models are evaluated and compared. It facilitates the assessment of different chatbot models by pitting them against each other in a series of conversational challenges. At the core of this Arena lies the concept of Elo rankings, a widely adopted rating system originally devised for chess players. Elo rankings  are used to quantify the relative performance of chatbot models based on a series of head-to-head battles. Each model is initially assigned an arbitrary Elo rating, which is then updated after every battle based on the outcome (win, loss, or tie) and the rating difference between the competing models. If a higher-rated model defeats a lower-rated one, its Elo rating increases slightly, while the loser's rating decreases by a corresponding amount.

### Using a Powerful LLM as Judge to Simulate Human Annotators

At the core of the simulated arena battles in WizardArena lies a powerful LLM that serves as the "judge model". This judge model is specifically prompted and adjusted by us on a diverse range of conversational pair data, enabling it to assess the quality, relevance, and appropriateness of the models' responses objectively and consistently. The judge model's role is to analyze and compare the responses provided by the competing models for each instruction or conversational sample. It considers various factors, such as coherence, factual accuracy, context-awareness, and overall quality, to determine whether one response is superior to the other or if they are of comparable quality (a tie). We show the details of its judgement prompt in Appendix A.

Figure 2: Overview of Arena Learning post-training data flywheel and WizardArena evaluation.

### Evaluation LLMs with WizardArena

#### 3.3.1 Constructing the Offline Test Set

To accurately evaluate the performance of chatbot models and predict their Elo rankings, WizardArena relies on a carefully curated offline mixed test set, which is designed to strike a balance between diversity and complexity, ensuring a comprehensive assessment of the models' capabilities across a wide range of conversational scenarios. This test set consists of the following two subsets:

**Diverse Subset** The diverse subset of the test set is constructed to capture a broad range of topics, styles, and conversational contexts. To achieve this, we employs text clustering techniques on a large corpus of instructions and conversational data. The clustering process begins by representing all the instructions in a conversation as a high-dimensional vector using state-of-the-art embedding models. These vectors capture the semantic and contextual information within the text, enabling the clustering algorithm to group similar samples together. Once the clustering is complete, we selects a representative sample from each cluster, ensuring that the diverse subset of the test set capture a broad range of scenarios. This approach helps to mitigate potential biases or blindspots that may arise from relying solely on simply random sampling.

**Hard Subset** This subset is specifically designed to challenge the capabilities of even the most advanced chatbot models. To construct this subset, we leverages the power of LLMs to predict the difficulty level of each instruction. We then selects the top-ranking samples according to the predicted difficulty scores, ensuring that the hard subset of the test set comprises the most challenging and complex scenarios. This data serves as a rigorous benchmark for evaluating the robustness and capability of chatbot models in handling intricate and nuanced conversational tasks.

#### 3.3.2 Simulating Offline Battling and Ranking Models on Test Set

With the above "judge" model and the offline test set in place, WizardArena proceeds to evaluate the performance of various chatbot models through a series of pair-wise battles. The outcomes of the battles are then used to compute the Elo rankings of the participating chatbot models. WizardArena adopts the same Elo rating system used in LMSys Chatbot Arena, which has proven effective in ranking players or entities based on their head-to-head performance.

### Iterative Training LLMs through _Arena Learning_

#### 3.4.1 Collecting Large-Scale Instruction Data

To facilitate leveraging the simulated arena battles among models to train WizardLM-\(\), _Arena Learning_ relies on a large-scale corpus of conversational data \(D\). The data collection process involves several stages of filtering, cleaning, and deduplication to ensure the quality and diversity of the instruction data. The simulated arena battle outcomes are then used to generate training data for the WizardLM-\(\), tailored to different training strategies: supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO). We split the data equally into some parts \(D=\{D_{0},D_{1},D_{2},...,D_{N}\}\) for following iterative training and updates respectively.

#### 3.4.2 Iterative Battle and Model Updating

_Arena Learning_ employs an iterative process for training and improving the WizardLM-\(\). After each round of simulated arena battles and training data generation, the WizardLM-\(\) is updated using the appropriate training strategies (SFT, DPO, and/or PPO). This updated model is then re-introduced into the arena, where it battles against the other SOTA models once again. This iterative process allows the WizardLM-\(\) to continuously improve and adapt to the evolving landscape of the arena. As the model becomes stronger, the simulated battles become more challenging, forcing the WizardLM-\(\) to push its boundaries and learn from the latest strategies and capabilities exhibited by the other models. Additionally, the iterative nature of _Arena Learning_ enables the researchers to monitor the progress and performance of the WizardLM-\(\) over time, providing valuable insights into the effectiveness of the different training strategies and potential areas for further improvement or refinement.

The following is the first iterative loop: For SFT, we first train the initial version of WizardLM-\(\) with \(D_{0}\), then select some state-of-the-art LLMs which ranking top on WizardArena testset, following we battle WizardLM-\(\) with them on \(D_{1}\), and focus on extracting instances where the WizardLM-\(\)'sresponse is considered inferior to the winning model's response, as determined by the judge model. These instances are collected, and the winning model's response is used as the target output for fine-tuning the next WizardLM-\(\)-SFT model. For DPO, we use WizardLM-\(\)-SFT to battle with SOTA LLMs on \(D_{2}\), and then we treat win and loss responses as the < choice, reject > pairs to training the WizardLM-\(\)-DPO model. For PPO, we leverage the same battle process on \(D_{3}\) to obtain the < choice, reject > pairs to train the reward model and WizardLM-\(\)-PPO.

In the second training iteration, we select the latest best WizardLM-\(\) on the WizardArena test set as the initial model, and adopt above battles on \(D_{4}\), \(D_{5}\), and \(D_{6}\) to get the training data of next version of SFT, DPO, and PPO models respectively. We will stop the iteration when we find the model can't achieve significantly better performance than previous iteration.

## 4 Experiments

### Experimental Setup

**Source Data.** We collected some instructions from open available datasets (i.e., Alpaca , FLAN , LMSYS-Chat-1M , OpenOrca , WizardLM ), and optimized them using the following steps: first, we filtered out all illegal conversations; second, we removed conversations with instruction lengths of less than 10; third, we eliminated duplicate instructions with prefixes of 10; next, we employed the MinHashLSH technique  (with a threshold of 0.4 and 128 permutation functions) for data deduplication; subsequently, we excluded instructions from the top 5 matches in semantic similarity with benchmarks (i.e., WizardArena, LMSYS-hard , MT-Bench , AlpacaEval , OpenLLM Leaderboard [27; 31; 41; 47; 91]) to prevent data leakage. Finally, we removed all non-English instructions. After completing these steps, we obtained the refined 276K dataset \(D\).

**Offline Diverse & Hard WizardArena test set.** Firstly, we processed the source data using K-Means clustering into 500 categories. From each category, we randomly selected two samples to construct 1,000 diversity samples, named as the Offline-Diverse WizardArena. Additionally, 20 samples from each category were selected at random to form a data set of 10,000 entries, we then used GPT-4-1106-preview to rate each instruction on a difficulty scale from 0 to 10 in descending order, and selected the top 1,000 entries to create the hard test set, denoted as the Offline-Hard WizardArena. Detailed scoring prompt is provided in Appendix B. The Offline-Mix WizardArena combines the Diverse and Hard test sets in 2,000 samples. Different from Arena-Hard-v1.0 , which mainly focuses on single-turn dialogue data, WizardArena-Mix incorporates multi-turn dialogue data.

**LLM Battle.** We selected 15 popular models from the LMSYS ChatBot Arena and conducted pairwise battles in the Offline-Mix WizardArena. Llama3-70B-Chat  served as the "judge" model, with the higher-scoring model declared the winner. To maintain consistency, the detailed judgement prompt is sourced from [15; 92] provided in A. Following LMSYS Chatbot Arena, we adopt the Bradley-Terry model  to calculate the final scores for each model. To mitigate potential position bias, we used a two-game setup, swapping the models between the first and second positions for each instance . We use multiple bootstraps (i.e., 100), and select the median as the model's ELO score. The 95% CI is determined from the 2.5% to 97.5% range of confidence interval scores.

**Training Data.** 1) we random sample 10k ShareGPT data to train a initial model WizardLM-\(\)-\(I_{0}\). 2) we randomly split the \(D\) into nine slices, each \(D_{i}\) contains around 30K multi-turn conversations. 3) In the first iteration \(I_{1}\), we inference three SOTA models Command R+, Qwen1.5-72B-chat , and OpenChat-3.5  as reference models for battle and We also inference our WizardLM-\(\)-\(I_{0}\) on \(D_{1}\). 4) We employ the judge model to judge between WizardLM-\(\)-\(I_{0}\) and each reference model. 5) The winning reference model's responses (threshold score > 1.0 for maintaining the distinction) are used as the target output to train WizardLM-SFT-\(\)-\(I_{1}\). 6) Immediately, we use this as initial model and re-battle with three SOTA models on \(D_{2}\) and \(D_{3}\) to produce the training data of WizardLM-DPO-\(\)-\(I_{1}\) and WizardLM-PPO-\(\)-\(I_{1}\) respectively. The best model from \(I_{1}\) will be the initial model of second iteration \(I_{2}\), and the third one \(I_{3}\) also operates the same way. Finally, we obtain 56.5K (\(D_{0}\) 10K, \(D_{1}\) 20K, \(D_{4}\) 14K, \(D_{7}\) 12.5K) data for SFT, 57.3K (\(D_{2}\) 20.4K, \(D_{5}\) 19.3K, \(D_{8}\) 17.6K) data for DPO, 57.3K (\(D_{3}\) 20.4K, \(D_{6}\) 19.3K, \(D_{9}\) 17.6K) data for PPO reward model, and 90k for PPO.

**Implementation Details.** We apply our method to the Mistral-7B  for post-training, use Llama3-70B-Chat  as judge models in WizardArena. In supervised fine-tuning, we trained three epochswith a learning rate of 5e-6, a batch size of 128, and a sequence length of 4096. For PPO reward model training, Mistral-7B was trained for one epoch at a learning rate of 1e-6. In PPO training, the learning rate was 1e-7 for one epoch with a KL coefficient of 0.4, and for DPO training, it was 5e-7 for two epochs with a beta of 0.3. We used the DeepSpeed  and TRL  for SFT and RL.

### Offline WizardArena closely align with the Online LMSYS ChatBot Arena.

Table 1 and Figure 3 presents the rankings for 16 popular models across several evaluation benchmarks: LMSYS ChatBot Arena-EN , MT-Bench , and three Offline-Diverse, Offline-Hard, and Offline-Mix (Diverse & Hard) of WizardArena. The results reveal that employing the LMSYS ChatBot Arena as the reference benchmark in the real-world scenarios, WizardArena displays the good ranking consistency, however MT-Bench shows the large fluctuations. We find some models that perform well on MT-Bench, such as GPT-3.5-Turbo-0613  and DeepSeek-LLM-67B-Chat , rank lower in the LMSYS ChatBot Arena. In addition, there is a significant difference in performance between WizardArena diverse and hard subsets, with Vicuna-33B  and Qwen1.5-32B-Chat  being more effective in diverse tasks, while Tulu-2-DPO-70B  and Nous-Hermes-2-Mixture-DPO  achieving better results in hard tasks.

[MISSING_PAGE_FAIL:8]

The results indicate that data selected via the pair-judge method yielded a 22-point improvement in the Offline-Mix Arena ELO over the all original 30k data, surpassed the diversity-based K-Means Cluster method by 25 points, and exceeded the instruction complexity-based INSTAG  method by 11 points. On MT-bench, the pair-judge method also demonstrated superior performance, with improvements of 0.17 points over Original, 0.13 points over K-Means Cluster, and 0.06 points over INSTAG. This advantage is attributed to that the pair-judge method focuses on instructions where the base model underperforms, particularly in diverse and complex tasks, effectively addressing the model's weaknesses. These results underscore the effectiveness of the pair-judge method in selecting high-quality data during the SFT phase to target and strengthen the weakness of the base model.

**Llama3-Chat Judge and GPT-4 Judge Consistency.** In most previous works, people were accustomed to use GPT-4 as a judge for evaluation or generating synthetic data, but the GPT-4 API cost required for large-scale data flywheel is enormous for most research and production scenarios. Therefore, we explore whether it is possible to replace GPT-4 with advanced open source models. Table 4 explores the consistency between Llama3-70B-Chat and GPT-4 as judge models in the Offline-Mix Arena. Using GPT-4 judge's ELO as the reference benchmark, the Spearman correlation coefficient between Llama3-70B-Chat judge and GPT-4 judge is 95.81%, and the Human Agreement with 95% CI is 88.46%. The overall average consistency between the two judge models is 92.14%. Consequently, employing Llama3-70B-Instruct as a cost-effective judge model achieves high consistency with both GPT-4 and LMSYS ChatBot Arena by human judgment, ensuring the reliability of the WizardArena evaluation and post-training with _Arena Learning_ in this paper.

**Training strategy.** Table 5 explores the impact of different training strategies in the first round during the SFT, DPO, and PPO stages. Iterative application of the pair-judge method consistently boosts SFT model performance, exemplified by the Offline-Arena Mix ELO score rising from 1063 to 1124 and the MT-bench score from 6.98 to 7.15. These outcomes confirm the effectiveness and scalability of the pair-judge approach for SFT data selection. In the RL stage, by continuing the post-training of DPO and PPO on top of SFT, the Offline-Arena Mix ELO score significantly increased by 135 points and 142 points, and MT-bench improved by 0.37 points and 0.31 points. Futhermore, SFT + DPO + PPO showed a modest 0.05-point improvement on MT-bench compared to SFT + DPO, but obviously increased by 21 points on Offline-Arena Mix ELO. These findings suggest that the continuous application of reinforcement learning strategies can further boost the model's intrinsic capabilities. Above results indicate that the data derived from the pair-judge battle method not only significantly enhanced the SFT phase training but also provided high-quality data pairs for the RL phase, continuously improving the training outcomes for DPO and PPO.

**Scaling Iterative SFT, DPO, PPO training.** Figure 5 explores the iterative training processes of SFT, DPO, and PPO, where \(I_{i}\) represents the \(i\)-th iteration. The results highlight that continuous battle with WizardArena and updating can progressively enhance model performance. Specifically, from SFT-\(I_{0}\) to DPO-\(I_{3}\) or PPO-\(I_{3}\), the WizardArena ELO score increased from 875 to 1274, achieves a huge gain of 399 points, and the MT-Bench score also rises from 6.41 to 7.81, achieves an increase of 1.4 points. These findings underscore the effectiveness and scalability of the _Arena Learning_ iterative training method in post-training LLMs.

    &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\  & & & & & & & & & & & \\  & & & & & & & & & & \\  SFT & \(D_{i}\) & 1065 (+4) & 6.98 & 5.04 & 5.03 & 5.04 & 4.15 & 11.0 & 10.0 & 6.57 & 39.27 \\ SFT & \(D_{i}\) & 1024 (+4) & 6.98 & 5.04 & 5.04 & 4.16 & 25.0 & 8.46 & 6.65 & 88.99 \\ SFT & \(D_{i}\) & 62.48 (+5) & 6.36 & 5.06 & 4.03 & 4.53 & 25.8 & 11.3 & 1.37 & 6.80 & 4.81 \\ SFT + DPO & \(D_{i}\) & 57.73 & 7.54 & 7.54 & 4.04 & 4.16 & 25.0 & 8.24 & 6.41 & 38.71 \\ SFT + DPO & \(D_{i}\) & 58.31 & 6.31 & 5.39 & 5.19 & 5.34 & 31.6 & 12.71 & 6.46 & 5.74 \\ SFT + DPO + PPO & \(D_{i}\) & 62.1 & 84.1 & 6.19 & 6.57 & 5.93 & 4.08 & 11.78 & 7.55 & 47.34 \\ SFT + DPO + PPO & \(D_{i}\) & 62.24 & 84.0 & 62.31 & 5.92 & 5.42 & 4.56 & 17.22 & 7.29 & 47.46 \\   

Table 5: Explore different alignment strategies for models in SFT and RL stages. We utilize three slices of data for SFT, DPO, and PPO training.

Figure 5: Explore the iterative training processes of SFT, DPO, and PPO. \(I_{i}\) represents the \(i\)-th iteration.

    &  &  &  &  &  &  &  &  \\  & & & & & & & & & & \\  & & & & & & & & & & \\  & & & & & & & & & & \\  SFT & \(D_{i}\) & 1065 (+4) & 6.98 & 5.04 & 5.04 & 4.16 & 25.0 & 8.46 & 6.55 & 8.99 \\ SFT & \(D_{i}\) & 1024 (+4) & 6.93 & 5.04 & 5.04 & 4.16 & 25.0 & 8.24 & 6.41 & 38.71 \\ SFT + DPO & \(D_{i}\) & 108 (+4) & 7.35 & 5.04 & 5.04 & 4.16 & 25.0 & 8.24 & 6.41 & 38.71 \\ SFT + PPO & \(D_{i}\) & 108 (+4) & 7.29 & 5.04 & 5.04 & 5.19 & 35.0 & 11.71 & 6.56 & 4.74 \\ SFT + DPO & \(D_{i}\) & 128 (+4) & 7.29 & 5.04 & 5.19 & 5.34 & 4.16 & 12.71 & 6.46 & 5.74 \\ SFT + DPO + PPO & \(D_{i}\) & 121 (+1) & 129 (+1) & 7.40 & 5.04 & 5.04 & 4.05 & 17.22 & 7.29 & 47.46 \\   

Table 6: Explore our modelâ€™s performance across various benchmarks, including the OpenLLM Leaderboard, GSM8K, AlpacaEval2.0 and MT-Bench.

**Count of data selected from each battle model during DPO.** Table 7 summarizes the sources of Choose and Reject responses during the DPO data construction. Command R+ selected 9.5k, 8.8k, and 7.8k data as Choose responses across three rounds, totaling 26.1k. The corresponding Reject responses were 1.1k, 0.9k, and 0.9k, totaling 2.9k. WizardLM-\(\)-SFT selected 1.6k, 1.9k, and 2.5k Choose responses across three rounds, totaling 6.0k(6.0k vs. 57.3k), with corresponding Reject responses of 8.5k, 6.4k, and 4.2k, totaling 19.1k (19.1k vs. 57.3k). This indicates that as WizardLM-\(\)-SFT improved through iterative training, the number of Choose responses increased, while Reject responses decreased.

**Llama3-70B-Chat vs. Human Judge.** To reduce time and annotation costs, we randomly selected 200 samples from Wizard Arena-Mix (100 diverse, 100 challenging). We evaluated four models: WizardLM-\(\)-PPO-\(I_{3}\) (reference model), OpenChat-3.5, Command R+, and Qwen1.5-72B-Chat (battle models), using Llama3-70B-Chat and professional human annotators, with results shown in Table 8. Llama3-70B-Chat's win rates for WizardLM-\(\)-PPO-\(I_{3}\) against Command R+, Qwen1.5-72B-Chat, and OpenChat-3.5 were 34.1%, 41.3%, and 79.7%, closely matching human evaluations (31.8%, 37.7%, 82.1%). The high consistency between them further validates Llama3-70B-Chat's reliability and accuracy as a judge model in Wizard Arena.

**Performance on more benchmarks.** Table 6 showcases our model's performance at the first iteration across various benchmarks, including the OpenLLM Leaderboard, GSM8k, AlpacaEval2.0, and MT-Bench for SFT, DPO and PPO stages. Utilizing the Wizard Arena method to produce training data has markedly improved model performance in both SFT and RL stages. Specifically, WizardLM-\(\)-SFT-\(I_{1}\) exceeds WizardLM-\(\)-SFT-\(I_{0}\) by 7.03 average points. More impressively, WizardLM-\(\)-PPO-\(I_{1}\) not only surpasses WizardLM-\(\)-SFT-\(I_{0}\) by 8.74 points but also exceeds WizardLM-\(\)-SFT-\(I_{1}\) by 1.71 points and outperforms Openchat-3.5 by 4.64 points. Particularly in the reasoning tasks, WizardLM-\(\)-PPO-\(I_{1}\) shows a 7.88 point increase on MMLU and a significant 20.26 point gain on GSM8k compared to WizardLM-\(\)-SFT-\(I_{0}\), demonstrating that the our method effectively enhances the model's weaknesses. The detailed scores of WizardLM-\(\)-\(I_{1}\) SFT, DPO, PPO in the 8 subtasks of MT-Bench refer to the Figure 6.

## 5 Conclusion

In conclusion, this paper presents _Wizard Arena_, a simulated offline chatbot arena that utilizes AI LLMs to eliminate the manual and temporal costs associated with post-training LLMs, while preserving the benefits of arena-based evaluation and training. The effectiveness of Wizard Arena is validated through the high consistency in predicting Elo rankings among different LLMs compared to the human-based LMSys Chatbot Arena. Furthermore, the model trained on synthetic data generated by _Arena Learning_ strategy exhibits significant performance improvements across various training strategies. This work showcases the potential of Wizard Arena as a cost-effective and reliable alternative to human-based evaluation and data production platforms for post-training chatbot models.

**Limitations and Broader Impacts.** If the judge model fails to accurately imitate human evaluators, the generated rankings and training data may be compromised. Moreover, similar to the other LLMs, our model could also generate potentially unethical or misleading information sometimes.

  Stage & Command R+ & Qwen1.5-72B-Chat & OpenChat-3.5 & WizardLM-\(\)-SFT & Total \\  DPO-\(I_{1}\)-Choose & 9.5k & 7.9k & 1.4k & 1.6k & 20.4k \\ DPO-\(I_{2}\)-Choose & 8.8k & 7.5k & 1.1k & 1.9k & 19.3k \\ DPO-\(I_{3}\)-Choose & 7.8k & 6.6k & 0.7k & 2.5k & 17.6k \\ DPO-\(Total\)-Choose & 26.1k & 22.0k & 3.2k & 6.0k & 57.3k \\  DPO-\(I_{1}\)-Reject & 1.1k & 1.6k & 9.2k & 8.5k & 20.4k \\ DPO-\(I_{2}\)-Reject & 0.9k & 1.5k & 10.5k & 6.4k & 19.3k \\ DPO-\(I_{3}\)-Reject & 0.9k & 1.3k & 11.2k & 4.2k & 17.6k \\ DPO-\(Total\)-Reject & 2.9k & 4.4k & 30.9k & 19.1k & 57.3k \\  

Table 7: Explore the quantity of Choose and Reject responses for each battle model across various rounds during the DPO stages.

   &  &  &  \\   \\  \)** vs. Command R+} & 55 & 39 & 106 \\ WizardLM-\(\)-PPO-\(I_{3}\) vs. Overall 5.72B-Chat & 64 & 45 & 91 \\ WizardLM-\(\)-PPO-\(I_{3}\) vs. OpenChat-3.5 & 141 & 23 & 36 \\   \\  \)** vs. Command R+} & 49 & 46 & 105 \\ WizardLM-\(\)-PPO-\(I_{3}\) vs. OpenChat-3.5 & 60 & 41 & 99 \\ WizardLM-\(\)-PPO-\(I_{3}\) vs. OpenChat-3.5 & 147 & 21 & 32 \\  

Table 8: The win/tie/loss counts of WizardLM-\(\)-PPO-\(I_{3}\) against Command R+, Qwen1.5-72B-Chat, OpenChat-3.5 evaluated by Llama3 70B Chat Judge and Human Judge.