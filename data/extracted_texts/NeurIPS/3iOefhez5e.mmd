# Low Degree Hardness for Broadcasting on Trees

Han Huang

Department of Mathematics

University of Missouri

Columbia, MO 65203

hhuang@missouri.edu &Elchanan Mossel

Department of Mathematics

MIT

Cambridge, MA 02139

elmos@mit.edu

###### Abstract

We study the low-degree hardness of broadcasting on trees. Broadcasting on trees has been extensively studied in statistical physics, in computational biology in relation to phylogenetic reconstruction and in statistics and computer science in the context of block model inference, and as a simple data model for algorithms that may require depth for inference.

The inference of the root can be carried by celebrated Belief Propagation (BP) algorithm which achieves Bayes-optimal performance. Recent works indicated that this algorithm in fact requires high level of complexity. Moitra, Mossel and Sandon constructed a chain for which estimating the root better than random (for a typical input) is \(NC1\) complete. Kohler and Mossel constructed chains such that for trees with \(N\) leaves, recovering the root better than random requires a polynomial of degree \(N^{(1)}\). Both works above asked if such complexity bounds hold in general below the celebrated _Kesten-Stigum_ bound.

In this work, we prove that this is indeed the case for low degree polynomials. We show that for the broadcast problem using any Markov chain on trees with \(N\) leaves, below the Kesten Stigum bound, any \(O( N)\) degree polynomial has vanishing correlation with the root.

Our result is one of the first low-degree lower bound that is proved in a setting that is not based or easily reduced to a product measure.

## 1 Introduction

Understanding the computational complexity inference problems of random instances has been extensively studies in different research areas including statistics, cryptography, computational complexity, computational learning theory and statistical physics. The emerging field of research is mainly devoted to the study of computational-to-statistical gaps. ()

Recently, low-degree polynomials have emerged as a popular tool for predicting computational-to-statistical gaps, especially in the context of the Bayesian framework. Our work follows  in studying the polynomial hardness of broadcasting on trees.

A very exciting line of work, including  recently showed that the "low-degree heuristic" can be used to predict computational-statistical gaps for a variety of problems such as recovery in general stochastic block models, sparse PCA, tensor PCA, the planted clique problem, certification in the zero-temperature Sherrington-Kirkpatrick model, the planted sparse vector problem, and for finding solutions in random \(k\)-SAT problems.

Interestingly, it was observed that the predictions from this method often agree with predictions from statistical physics heuristics based on the replica and cavity methods which are closely related to the analysis of BP/AMP fixed points, see e.g. ).

It is often argued that low-degree polynomials algorithms are relatively easy to use (e.g. compared to proving SOS lower bounds), and that low degree polynomials capture the power of the "local algorithms" framework used in e.g.  as well as algorithms which incorporate global information, such as spectral methods or a constant number of iterations of Approximate Message Passing .

In this work, we continue to study the power of low-degree polynomials for the (average case) broadcast on trees problem. In broadcasting on trees the goal is to estimate the value of the Markov process at the root given its value at the leaves and the goal is to do so for arbitrarily deep trees. Two key parameters of the model are the arity of the tree \(d\) and the magnitude of the second eigenvalue \(\) of the broadcast chain.

A fundamental result in this area  is that when \(d||^{2}>1\) nontrivial reconstruction of the root is possible by counting the number of the leaves of different types, an algorithm that could be described as a linear function of the leave values. In contrast, when \(d||^{2}<1\), such linear estimators have no mutual information with the root (but more complex statistics of the leaves may) .

This threshold \(d||^{2}=1\) is known as the _Kesten-Stigum threshold_. A series of works showed that the KS threshold is the information theory threshold for non-trivial root inference for some specific channels, including the binary symmetric channel  and binary channels that are close to symmetric , as well as \(3 3\) symmetric channels for large \(d\).

While the Kesten-Stigum bound is easy to compute, it turns out that in many cases, it is _not_ the information-theoretic threshold for root recovery. This was first established in  for symmetric channels with sufficiently many states, specifically when \(q C\) for some large constant \(C\). Later it was shown for symmetric channels with \(q 5\) states in . Recently, the results  provide more information about the case of \(q=3\) and \(q=4\). Many of the finer results in this area prove predictions from statistical physics. The connection between the broadcast problems and phase transitions in statistical physics was made in , and more recent predictions include . Moreover, the information-theoretic threshold may depend on the specific structure of the channel rather than solely on \(d\) and \(\). Notably,  also showed that there exists channels where non-trivial predictions of the root are achievable even when \(||=0\).

Much of the interest in Kesten-Stigum threshold comes from the fundamental role it plays in problems, such as algorithmic recovery in the stochastic block model  and phylogenetic reconstruction . Count statistics can be viewed as degree 1 polynomials of the leaves, which begs the question of what information more general polynomials can extract from the leaves. See  for surveys on the topic.

In  it was shown that \(=0\) even polynomials of degree \(N^{c}\), where \(N=d^{}\) is the number of leaves of for a \(d\)-ary tree of depth \(\), for a small \(c>0\) are not able to correlate with the root label (as \(\) tends to \(\)), whereas computationally efficient reconstruction is generally possible as long as \(d\) is a sufficiently large constant .

The main motivation of  was to prove that low degree polynomials fail below the Kesten Stigum bound: "It is natural to wonder if the Kesten-Stigum threshold \(d||^{2}=1\) is sharp for low-degree polynomial reconstruction, analogous to how it is sharp for robust reconstruction." However the main result of  only established this in the very special case of \(=0\). This problem is also stated in the ICM 2022 paper and talk on the broadcast process : " The authors of  ask if a similar phenomenon holds through the non-linear regime. For example, is it true that polynomials of bounded degree have vanishing correlation with \(X_{0}\) in the regime where \(d^{2}<1\)? " The main results of this paper prove that this is indeed the case. We proceed with formal definitions and statement of the main result.

### Definitions and Main Result

#### Rooted Tree

Recall that every rooted tree \(T\) inherently defines a partial order relation among its vertices: For a pair of distinct vertices, \(u\) is said to be an ancestor of \(v\) (and \(v\) a descendant of \(u\)), denoted as \(v<u\) in this paper, if \(u\) is contained in the unique path from \(v\) to the root \(\). Specifically, if \(v\) and \(u\) are directly connected by an edge, we also refer to \(v\) as a child vertex of \(u\) (and \(u\) as the parent vertex of \(v\)). By \(v u\) we mean that \(v\) is either a descendant of \(u\) or \(v=u\). In general, if \(v<u\) and the path distance between them is \(k\), \(v\) is referred to as a \(k\)th-descendant of \(u\) (and \(u\) as the \(k\)th ancestor of \(v\)).

For a nonnegative integer \(k\), the \(k\)th layer of the tree refers to the set of \(k\)th descendants of the root \(\). (The root here is considered at the 0th layer of the tree.) The depth of the tree is defined as the largest non-negative integer \(\) for which the \(\)th layer is not empty, and we denote the \(\)th layer by \(L\).

The height of a vertex \(u\) is defined as

\[(u)=-u.\] (1)

In particular, when \(L\) is the set of leaves, then \((u)\) is simply the distance from \(u\) to \(L\). In this paper, we may abuse the notation by writing \(x T\) or \(S T\) to mean that \(x\) is a vertex or \(S\) is a subset of vertices in the tree \(T\).

The standard rooted \(d\)-ary tree with depth \(\) is a tree where each vertex \(u L\) has exactly \(d\) children vertices. Let us start by defining the type of trees we will be investigating in this paper, which is a slight generalization of \(d\)-ary tree.

**Definition 1.1**.: _A rooted tree \(T\) with root \(\) has degree dominated by \(d 1\) with parameter \(R 1\) if for every vertex \(u\) and positive integer \(k\), the number of \(k\)th descendants of \(u\) is at most \(Rd^{k}\)._

With the above definition, a \(d\)-ary rooted tree has degree dominated by \(d 1\) with parameter \(R=1\). Further, a typical realization of a Galton-Watson tree of Poisson type with average degree \(d\) and of depth \(\) (a random tree in which each vertex \(u L\) has on average \(d\) children vertices) has a degree dominated by \(d 1\) with parameter \(R()\).

#### Broadcasting Process on Rooted Trees

Next, we will define the broadcasting process on a rooted tree \(T\) with root \(\). Consider a \(q q\) ergodic transition matrix \(M\), where \(q 2\). Recall that every eigenvalue of a transition matrix \(M\) has an absolute value at most 1. Let \(0 1\) represent the second largest absolute value among the eigenvalues of \(M\). Additionally, we define the stationary distribution of \(M\) as \(\).

The broadcasting process \(X=(X_{v})_{v T}\), with state space \([q]:=\{1,2,,q\}\) and transition matrix \(M\), can be formally described as follows: We initialize the value of \(X_{}\) according to some initial distribution \(\). As we reveal the values layer by layer, when the value \(X_{u}\) is revealed, the value \(X_{v}\) for any child vertex \(v\) of \(u\) is independently distributed according to a specific row of \(M\) depending on the value of \(X_{u}\):

\[\{X_{v}=t\,|\,X_{u}=s\}=M_{st}.\]

In other words, each vertex's value depends only on its parent vertex's value. The definition of the process is given below:

**Definition 1.2** (Broadcasting Process on Tree).: _Let \(q 2\) be a positive integer. For any rooted tree \(T\) with root \(\) and a \(q q\) ergodic transition matrix \(M\), the broadcasting process \(X=(X_{v})_{v T}\) with state space \([q]\), according to transition matrix \(M\) with an initial distribution \(\) for \(X_{}\), is a random process with joint distribution given by:_

\[ x=(x_{v})_{v T}[q]^{T},\ \ \{X=x\}=(x_{}) _{(v,u)}M_{x_{u},x_{v}},\]

Figure 1: An example of a binary rooted tree of depth 5 is shown. The vertex \(u\) is at the 3rd layer and \((u)=2\). Further, the following relationships hold: \(v<u\) and \(v\) is a child of \(u\), \(s L\) is a 2nd descendant of \(u\), \(w\) is the parent of \(u\), and \(t\) is the 2nd ancestor of \(u\).

_where the product is taken over all pairs \((v,u)\) such that \(v\) is a child vertex of \(u\)._

_For a subset of vertices \(A T\), let us denote_

\[X_{A}=(X_{v})_{v A}\,.\]

If the tree is just a path, then the process reduces to a Markov chain. If we assume \(=\), then \(X_{v}\) for every \(v T\), as \((X_{v})_{v P}\) for every downward path of \(T\) forms a Markov Chain with transition matrix \(M\). Further, let us make a remark about the Markov property of the process.

**Remark 1.3** (Markov Property).: The broadcasting process establishes a **Markov Random Field** on tree \(T\): Given any three disjoint subsets \(A,B,\) and \(C\) of \(T\), if every path from a vertex in \(A\) to a vertex in \(C\) passes through a vertex in \(B\), then the random variables \(X_{A}\) and \(X_{C}\) are conditionally independent given \(X_{B}\).

Polynomials of \(x_{L}\) and the Main Result

**Definition 1.4**.: _Let \(x[q]^{T}\). For \(u T\), let \(x_{ u}=(x_{v})_{v u}\). For subset \(U T\), let \(x_{U}=(x_{u})_{u U}\)._

The next definition is about the notion of degrees for functions with variables \(x_{L}=(x_{v})_{v L}\). This is the generalization of degree of a polynomial.

**Definition 1.5** (Efron-Stein Degree).: _A function \(f\) with variables \(x_{L}\) has Efron-Stein degree at most \(k\) if it can be expressed as_

\[f=_{}f_{},\]

_where the summation is over a finite set of indices \(\), and each \(f_{}\) is a function of the variable \(x_{S}\) for some \(S L\) with \(|S| k\)._

Now, we could properly formulate the main result of the paper:

**Theorem 1.6**.: _Let \(T\) be a rooted tree with root \(\) of depth \(\) and has degree dominated by \(d 1\) with parameter \(R 1\). Consider the broadcasting process on \(T\) with a \(q q\) transition matrix \(M\) and \(X_{}\). If \(M\) is ergodic and \(d^{2}<1\), then there exists a constant \(c>0\) which depends on \(M\) and \(d^{2}\) such that the following holds: For any function \(f(x_{L})\) of Efron-Stein degree \( c\), we have_

\[(f(X_{L})\,\,X_{})(\{d ^{2},\})^{/4}(f(X_{L})).\]

**Remark 1.7**.: Given that \(d^{2}<1\) implies \(^{2}<1\), and \(<1\) if and only if \(^{2}<1\), we can infer that \(<1\) from the given conditions. Consequently, the term \(\{d^{2},\}<1\) follows from the assumptions of the theorem. Therefore, the R.H.S. of the inequality decays exponentially with the depth of the tree.

Follows from the theorem and properties of conditonal expectation, we have the following corollary.

**Corollary 1.8**.: _With the same setting as in Theorem 1.6, for any function \(f(x_{L})\) of Efron-Stein degree \( c\), and any function \(g(x_{})\) of the root value, their correlation satisfies_

\[(f(X_{L}),g(X_{})):=(f(X_{L})-f(X_{L}))(g(X_{})-g(X_{}))}{(f(X_{L}) }(g(X_{}))}}(\{d^{2},\})^{/8}.\]

The proof of the theorem is based on recursion on a notion of _fractal capacity_ of functions. Indeed, the main result is optimal in the fractal sense (Theorem 1.16), as we will later demonstrate that all functions with fractal capacity up to a level proportional to \(\) exhibit vanishing correlation with the root, whereas all functions of the leaves have fractal capacity at most \(+1\). Let us introduce the necessary definitions and notations to introduce both the fractal capacity and the proof overview.

### Fractal Capacity and Proof Overview

To provide a clearer illustration, we establish a correspondence between the vertices of \(T\) and words of varying lengths from \(0\) to \(\), with vertices at the \(k\)th layer represented as words of length \(k\). Wedenote the root \(\) as the empty word \(()\). For each vertex \(u\), represented by the word \((b_{1},b_{2},,b_{k})\), we define \(d_{u}\) as the number of children vertices of \(u\), and we identify these children vertices as \((b_{1},b_{2},,b_{k},i)\) with \(i[d_{u}]:=\{1,2,,d_{u}\}\). Notice that \(v\) is a descendant of \(u\) is equivalent to \(u\) is a prefix of \(v\). For brevity, for each \(u=(b_{1},,b_{k}) T\) and \(i[d_{u}]\), let

\[u_{i}:=(u,i)=(b_{1},,b_{k},i).\]

For \(I[d_{u}]\), let

\[u_{I}=\{u_{i}\}_{i I}.\]

Furthermore, we denote the parent vertex of \(u\) as \((u)=(b_{1},,b_{k-1})\) and the set of children vertices of \(u\) as \((u)=u_{[d_{u}]}\).

**Definition 1.9**.: _For a non-empty subset \(S L\), we introduce the notation \((S)\) to represent the nearest common ancestor of the vertices in \(S\), meaning that \((S)\) is the vertex with smallest height that is an ancestor of all vertices in \(S\)._

_Here we consider the case when \(|S|>1\). Notice that \((S)\) is not at the \(\)th layer \(L\). For each child \((S)_{i}\) of \((S)\) for \(i[d_{(S)}]\), we define the set \(S_{i}\) as_

\[S_{i}=S\{v L\,:\,v(S)_{i}\},\]

_which is the collection of vertices in \(S\) that are descendants of \((S)_{i}\). Let_

\[I(S)=\{i[d_{(S)}]\,:\,S_{i}\}.\]

_Then, \(S\) can be expressed as the disjoint union of the sets \(S_{i}\) for \(i I(S)\):_

\[S=_{i I(S)}S_{i}.\]

_We call the above disjoint union the_ **branch decomposition** _of \(S\), and each \(S_{i}\) for \(i I(S)\) a_ **branch part** _of \(S\)._

The branch decomposition is a key concept in the proof, and we define the _fractal capacity_ according to the number of iterations to decompose \(S\) into singletons.

**Definition 1.10**.: _Let_

\[_{1}:=\{u\}\,:\,u L}^{L} \{\},\]

_be the collection of singletons of \(L\). We say a subcollection \(^{L}\{\}\) is_ **closed under decomposition** _with base \(_{1}\) if for every \(S_{1}\), we have \(S_{i}\) for \(i I(S)\)._

**Definition 1.11**.: _For any \(^{L}\{\}\), let_

\[()^{L}\{\}\]

_be a new subcollection defined according to the following rules:_

_For any \(S^{L}\{\}\), \(S\) if and only if one of the following two conditions holds_

Figure 2: In the these figures, we present the vertices as words and adapt the notations \(u_{1},u_{2}\),etc., for the descendants of \(u\). For the right figure, if \(S=\{u_{21},u_{22},u_{32}\}\), then \((S)=u\), \(I(S)=\{2,3\}\), and \(S_{2}=\{u_{21},u_{22}\}\), \(S_{3}=\{u_{32}\}\). Further, \(S_{1}_{3}\), \(S_{2}_{2}\), and \(S_{3}_{1}\).

1. \(S_{1}\)_._
2. \(S_{1}\) _and_ \(S_{i}\) _for_ \(i I(S)\)_._

For example, \(()\) contains sets of size \( 2\).

**Lemma 1.12**.: _If \(_{1}^{L}\{\}\) is a subcollection closed under decomposition with base \(_{1}\), then the collection \(=()\) contains \(\) and it is also closed under decomposition with base \(_{1}\)._

Proof.: To show \(\), it is sufficient to show \(_{1}\). For any \(S_{1}\), because \(\) is closed under decomposition, \(S_{i}\) for \(i I(S)\). Hence, \(S\) follows from the definition of \(\). Now, for \(S_{1}\), each \(S_{i}\) with \(i I(S)\) is contained in \(\), which in turn implies \(\) is closed under decomposition. 

Now, we define recursively that

\[_{k}=(_{k-1}),\] (2)

for positive integer \(k 2\). Observe the following two facts: Consider any non-singleton set \(S L\) and \(S_{i}\) with \(i I(S)\). First, \(S_{k} S_{i}_{k-1}\) by the definition of \(_{k}\). Second, \((S)>(S_{i})\) by the definition of branch decomposition. Given these two facts, we can prove inductively that

\[((S))=k S_{k+1}.\]

Since that there are only \(\) layers of the tree, we conclude that every non-emptyset of \(S L\) is in \(_{+1}\). Therefore, together with Lemma 1.12, we have the following chain of subcollections:

\[\{\{u\}\,:\,u L\}=_{1}_{2} _{+1}=^{L}\{\}.\]

**Definition 1.13** (Fractal Capacity).: _For any non-empty subset \(S L\), we define the_ **fractal capacity** _of \(S\) as the smallest \(k\) such that \(S_{k}\)._

We introduce the notion of fractal capacity, borrowing terminology from fractal geometry. The recursive nature of our definition on subsets of trees mirrors the self-similar complexity found in fractal structures. This recursive and inherently intricate structure motivates our choice of the term fractal capacity, capturing the fractal-like properties that emerge in the collections \((_{k})_{k[l+1]}\).

**Definition 1.14**.: _Given a collection \(^{L}\{\}\). A function \(f:[q]^{T}\) is called an \(\)-**polynomial** if we can express_

\[f(x)=_{S}f_{S}(x_{S})\]

_where each \(f_{S}\) is a function of \(x_{S}=(x_{v})_{v S}\). A function \(f:[q]^{T}\) has_ **fractal capacity** _\( k\) if it is a \(_{k}\)-polynomial._

**Remark 1.15**.: It is not hard to verify that \(_{k}\) contains all non-empty subsets \(S L\) with \(|S| k\). Thus, for any function \(f\) with variables \(x_{L}\),

\[_{k}$-polynomial}.\]

On the other hand, it is worth to remark that for the \(d\)-ary tree of depth \( k\), there exists \(S_{k}\) with \(|S|=d^{k-1}\). (Namely, taking \(S=\{v L\,:\,v<u\}\) for some \(u\) with \((u)=k-1\).) Thus, an \(_{k}\)-polynomial could have an Efron-Stein degree exponential in \(k\).

The main result of the paper in terms of the fractal capacity is the following:

**Theorem 1.16**.: _With the same setting as in Theorem 1.6, there exists a constant \(c>0\) which depends on \(M\) and \(d^{2}\) such that the following holds: For any function \(f(x_{L})\) with_ **fractal capacity**__\( c\), we have_

\[(f(X_{L})\,\,X_{})(\{d ^{2},\})^{/4}(f(X_{L})).\]

Indeed, Theorem 1.6 is a direct consequence of Theorem 1.16, as \(_{k}\)-polynomials contains all polynomials of Efron-Stein degree \( k\). Further, in terms of fractal capacity, the theorem is optimal because the correlation decay persists up to an order proportional to \(\), while a fractal capacity \(\) includes all functions of the leaves.

**Overview of the Proof Idea:** For illustration, let us consider the case where \(T\) is a binary tree of depth \(\) with \(M=&\\ &\), such matrix has eigenvalues \(\) and \(1\). Here we assume \(2^{2}<1\).

We recall that for this binary symmetric broadcasting process, it is information-theoretically impossible to recover the root label from the leaves below the KS bound. This implies that all polynomials of \(X_{L}\) have vanishing correlation with \(X_{}\). Still, we use this simple process to illustrate the proof idea as our arguments for low-degree polynomials generalize to general broadcasting processes below the Kesten-Stigum threshold, including cases where it is information theoretically possible to estimate the root from the leaves non-trivially (in such cases there exist functions \(f\) so that \(_{}(f(X_{L}),X_{})>0\)).

Now, let us consider degree-\(1\) polynomials. Suppose \(f\) is a \(_{1}\)-polynomial (equivalently, of Efron-Stein degree 1), we can express it in the form

\[f(x_{L})=_{u L}f_{u}(x_{u}),\]

where each \(f_{u}\) is a function of \(x_{u}\). Given our focus on the variance, we may assume \(f_{u}(X_{u})=0\) for each \(u L\). Then, our goal is to prove \([f(X_{L})\,|\,X_{}]^{2}\) is negligible comparing to \((f(X_{L}))^{2}\). Following from the Cauchy-Schwarz inequality that

\[(_{i[m]}a_{i})^{2}=(_{i[m]}1 a_{i})^{2} m _{i[k]}a_{i}^{2},\]

we have

\[[f(X_{L})\,|\,X_{}]^ {2} |L|_{u L}[f_{u}(X_{u})\,| \,X_{}]^{2}\] (3) \[ 2^{}_{u L}^{2}(f_ {u}(X_{u}))^{2}=(2^{2})^{}_{u L}(f_ {u}(X_{u}))^{2},\]

where the second inequality is derived from the variance decay property of in a Markov Chain. Thus, if we can establish \(_{u L}(f_{u}(X_{u}))^{2}\) is at the same order as \((f(X_{L}))^{2}\), the proof is complete. Notice that

\[(f(X_{L}))^{2}=_{u,v L}[f_{u}(X_{u}) f_{v}(X_{v})]=_{u L}(f_{u}(X_{u}))^{2}+_{u v  L}[f_{u}(X_{u})f_{v}(X_{v})].\]

Thus, the goal here is to show

\[_{u v L}[f_{u}(X_{u})f_{v}(X_{v})] <c_{u L}(f_{u}(X_{u}))^{2},\] (4)

for some constant \(c(0,1)\), which in turn implies the desired result:

\[[f(X_{L})\,|\,X_{}]^{2} (2^{2})^{}_{u L}(f_{u}(X_{u}))^{2} (2^{2})^{}(f(X_{L}))^ {2}.\]

Roughly speaking, (4) holds if for most pairs \(u\) and \(v\) within \(L\), the correlation between \(f_{u}(X_{u})\) and \(f_{v}(X_{v})\) is sufficiently small, which is the case for degree-1 polynomials.

Now, let us take a closer look. Fix any two vertices \(u\) and \(v\) in \(L\), with \(w\) as their nearest common ancestor. Suppose \(u w_{1}\) and \(v w_{2}\). Let \(X_{ w_{1}}=(X_{u^{}})_{u^{} w_{1}}\). We have

\[[f_{u}(X_{u})f_{v}(X_{v})]= |[[f_{u}(X_{u})\,|\,X_{ w_{1}}]f_{v}(X_{v })]|\] (5) \[ [([f_{u}(X_{u})\,|\,X_{ w_{1}}]) ^{2}]}[(f_{v}(X_{v}))^{2}]}\] \[ ^{h(w)}[(f_{u}(X_{u}))^{2}]}[(f_{v}(X_{v}))^{2}]},\]

where the last inequality follows from the variance decay of the Markov Chain for length \((w)\). The above inequality implies that the correlation between \(f_{u}(X_{u})\) and \(f_{v}(X_{v})\) is at most of order \(^{(w)}\).

The above bound can be improved to \(^{2k}\) by also taking the conditional expectation \([f_{v}(X_{v})\,|\,X_{ w_{2}}]\) into account, which requires the Markov Property that \(X_{u}\) and \(X_{v}\) are independent conditioned on \(X_{w}\). From here, one can properly arrange the terms and apply Cauchy-Schwarz inequality to show (4) holds.

Our proof of the main theorem tries to generalize the argument above to low degree polynomials. Let us summarize it by the following five pieces of descriptions.

**I. Bounding Covariance:** First, we generalized the idea on how (5) works for degree-1 polynomials. Suppose \(f_{}\) and \(f_{}\) are two functions so that the following holds.

1. \(f_{}(x_{S})\) is a function of \(x_{S}\) for some set \(S L\) such that \[[([f_{}(X_{S})\,|\,X_{ w^{}}])^{2}] ((f_{}(X_{S}))^{2},\] where we use \(a b\) to indicate \(a\) is much smaller than \(b\). We keep this not precise to avoid technical details, but expect that the ratio is exponentially small in \((w^{})\).
2. \(f_{}(x_{S^{}})\) is a function of \(x_{S^{}}\) with \(S^{} L\) satisfying \(S^{}\{v^{}\,:\,v^{} w^{}\}=\).

Then, following the same derivation as shown in (5) we have

\[[f_{}(X_{S})f_{}(X_{S^{}})]= [f_{}(X_{S})\,|\,X_{ w^{}}]f_{ }(X_{S^{}})[(f_{}(X_{S}))^{ 2}]}[(f_{}(X_{S^{}}))^{2}]}.\]

(See Figure 3 for an illustration.)

**II. Choosing a good decomposition of the function:** In essence, our proof strategy for any given function \(f(x_{L})\) with \(f(X_{L})=0\) revolves around decomposing \(f(x_{L})\) into a sum of functions \(f_{}(x_{L})\) for \(\) in some index set \(\), such that

1. \(|| 2^{}\),
2. For each \(\), \(f_{}(X_{L})=0\) and \([([f_{}(X_{L})\,|\,X_{}]^{2}][(f_{ }(X_{L}))^{2}]\),
3. Whenever \(\), we can find \(w T\) so that \(f_{}\) and \(f_{}\) satisfy the covariance bound in **I**. (Possibly with a switch of the roles of \(\) and \(\)).

Let us remark that while we are writing \(f_{}(x_{L})\), it does not mean that \(f_{}\) depends on every leave variables.

If this is the case, then we could follow the argument in the degree 1 case to show that desired result holds.

**III. From \(_{k}\) polynomials to \(_{k+1}\) polynomials:** The proof of the main theorem builds on **I** and **II** and advancing through a recursion on the fractal capacity of the function. This recursive approach relies on the following property:

Suppose we have shown the second moment decay \(_{k}\)-polynomials with mean \(0\) in the following sense: For every \(_{k}\)-polynomial \(f(x_{S})\) with mean \(0\) and variable \(x_{S}\) where \(S\{v L\,:\,v^{}\}\) for some vertex \(^{}\),

\[[([f(X_{L})\,|\,X_{^{}}])^{2}](2^{2} )^{(^{})-_{_{k}}}[(f(X_{L}) )^{2}],\] (6)

Figure 3: In the left figure, the purple dots represent the corresponding input variables for \(f_{}\), and the yellow dots represent the corresponding input variables for \(f_{}\). In the right figure, the purple dots represent the corresponding input variables for \([f_{}(X)\,|\,X_{ w_{1}}]\) and the variables do not involve \(x_{v}\) for vertex \(v\) not illustrated due to the Markov property.

where \({ h}_{{ A}_{k}}\) is some penalty constant depending on \({ A}_{k}\). (The bigger the value \({ h}_{{ A}_{k}}\), the weaker the second moment decay.)

Consider a specific type of \({ A}_{k+1}\)-polynomials. Fix a pivot vertex \(w\) with \({ h}(w)\) large enough, let \(x_{ w_{i}}=(x_{v})_{v w_{i}}\) for \(i\). Let \(g\) be a function of the form

\[g(x)=g(x_{ w_{1}},x_{ w_{2}})=_{}g_{,1}(x_{  w_{1}}) g_{,2}(x_{ w_{2}}),\]

where \({ J}\) is a finite index set and every \(g_{,i}\) is an \({ A}_{k}\)-polynomial whose variables are \(x_{ w_{i}}\) (more precisely, its variables are \(x_{S^{}}\) where \(S^{}\{v L\,:\,v w_{i}\}\)) and \({}g_{,i}(X_{ w_{i}})=0\). Then, the function \(g(x)\) is a \({ A}_{k+1}\)-polynomial with variable \(x_{S}\) for \(S\{v L\,:\,v w\}\).

Observe that, if we fix \(x_{ w_{2}}\), then \(x_{ w_{1}} g(x_{ w_{1}},x_{ w_{2}})\) is an \({ A}_{k}\)-polynomial with variable \(x_{S^{}}\) for \(S^{}\{v L\,:\,v w_{1}\}\) and mean \(0\). This allows us to apply the assumption for \({ A}_{k}\) polynomials to show

\[{}[({}[g(X_{ w_{1}},x_{ w_{2}})\,|\,X_{ w _{1}}])^{2}](d^{2})^{{ h}(w)-{ h}_{{ A}_{k}}}{ }[(g(X_{ w_{1}},x_{ w_{2}}))^{2}].\]

Clearly, the same inequality holds with the roles of \(x_{ w_{1}}\) and \(x_{ w_{2}}\) switched.

Base on this, one key step in the paper is to show \(g\) satisfies

\[{}[({}[g(X_{ w_{1}},X_{ w_{2}})\,|\,X_{ w _{i}}])^{2}](d^{2})^{{ h}(w)-{ h}_{{ A}_{k}}}{ }[(g(X_{ w_{1}},X_{ w_{2}}))^{2}]i.\] (7)

This inequality is immediate if \(X_{ w_{1}}\) and \(X_{ w_{2}}\) are independent, which is not the case in the broadcasting process. One of the main technical challenges in the proof is to show that the inequality holds when \(X_{ w_{1}}\) and \(X_{ w_{2}}\) are conditionally independent given \(X_{w}\) by the Markov Property. It turns out to impose a significant technical challenge when some entries of \(M\) can be \(0\).

Observe that (7) not only implies \(g\) satisfies the desired second moment decay for \({ II}(2)\), but also \({ I}(1)\) with \(w^{}\) to be either \(w_{1}\) or \(w_{2}\). Indeed, these two properties will also hold for \((x):=g(x)-{}g(X)\), the normalized \(g\) with mean \(0\), due to \(({}g(X))^{2}\) is negligible comparing to \({}g(X)^{2}\).

**IV. A closer look at the decomposition:**

Consider any \({ A}_{k+1}\)-polynomial \(f\) of variable \(x_{S}\) for \(S\{v L\,:\,v^{}\}\) for some vertex \(^{}\). Before we proceed to the discussion of the decomposition, we remark that one cannot simply express \(f\) in the form of \(\) described above with any pivot vertex \(w\).

Let us give an example to illustrate why: Consider two functions \(f_{1},f_{2}\), where \(f_{i}\) is a function of \(x_{S^{}}\) with \(S^{}\{v L\,:\,v^{}_{i}\}\) and \(S^{}_{k+1}_{k}\). Let \(f=f_{1}+f_{2}\). Then one can justify that \(f\) cannot be expressed in the form of \(\) with any pivot \(w\) by using the property "if we fix \(x_{ w_{2}}\), then \(x_{x_{ w_{1}}} g(x_{ w_{1}},x_{ w_{2}})\) is a \({ A}_{k}\)-polynomial" discussed in **III** to derive a contradiction.

The way we decompose \(f\) is to express it as a sum of functions of the form \((x)\) in **III**. While the actual decomposition requires a bit more adjustment, it follows from the idea to decompose \(f\) in the form

\[f=_{w}f_{w},\]

where the index set \({ I}\) is the set of vertices of \(T\) with height slightly greater than \({ h}_{{ A}_{k}}\) (to ensure correlation decay). Each \(f_{w}\) is a function with variable \(x_{S}\) for \(S\{u L\,:\,u w\}\) satisfies the description of \((x)\) in **III**.

Observe that this decomposition satisfies the description in **II**:

* The size of \({ I}\) is bounded by \(2^{}+2^{-1}+ 1 2^{+1}\).
* The second moment decay property of \(f_{w}\) follows from **III**.
* Finally, consider \(u,v\). If \(v<u\), say \(v u_{1}\), then \(f_{u}\) and \(f_{v}\) satisfies the covariance bound condition stated in **I** with \(f_{}=f_{u}\), \(f_{}=f_{2}\), and \(w^{}=u_{2}\). The case when \(u<v\) is similar. When \(u\) and \(v\) are not comparable, then following the Markov Property, \({}[f_{u}(X)f_{v}(X)]={}{}[f_{u}(X)\,|\,X_{ w }]{}[f_{v}(X)\,|\,X_{ w}]\) with \(w\) being the nearest common ancestor of \(u\) and \(v\) and \(X_{ w}=(X_{w^{}})_{w^{} w}\), which makes it easier to show the covariance bound. (See Figure 4 for an illustration.)With this desirable decomposition, one could try to apply some argument similar to the degree-1 case to show the second moment decay (6) for mean \(0\)\(_{k+1}\)-polynomials with a slightly bigger penalty constant \(_{_{k+1}}\) than \(_{_{k}}\).

**V. Overview on the induction** Given the decomposition of \(f\) as described in **IV**, together with the second moment assumption (6) on \(_{k}\)-polynomials described in **III**, the proof of the main theorem proceeds by induction. The goal is to show that the penalty constant \(_{_{k}}\) associated with \(_{k}\), which appeared (6), satisfies the following recursive inequality:

\[_{_{k+1}}_{_{k}}+C,\;\;_{_{1}} C\]

for some constant \(C\) depending on \(M\) and \(d^{2}\). If true, by taking \(k\) to be proportional to \(/C\), then the theorem follows. The proof of the theorem requires a careful analysis of the covariance and variance decay to demonstrate that it resembles the behavior observed in the degree-1 case, in order to capture the Kesten-Stigum bound.

**Comparison to other work in low-degree polynomials** While some high-level ideas align with previous work in low degree polynomials mentioned fore, our approach focuses on establishing low-degree lower bounds in a setting where direct comparisons are challenging due to structural differences. Specifically, our work addresses the broadcasting process on trees, where the underlying structures are highly correlated and do not naturally lend themselves to a product measure representation, presenting unique technical challenges not encountered in more independent setups.

Figure 4: In both figures, the purple dots represent the corresponding input variables for \(f_{u}\), and the yellow dots represent the corresponding input variables for \(f_{v}\). In the left figure, we have \(v u_{1}<u\). In the right figure, we have \(u\) and \(v\) are incomparable and \(w\) is the nearest common ancestor.