# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

labels in shallow transformer layers, and then uses the ICL function in deeper transformer layers to make predictions (Hendel et al., 2023). However, while these compressed vectors encapsulate learned information in a more condensed form and show significant promise in applying ICL, there still exists a considerable gap in understanding the operational mechanisms and optimization strategies of these vectors. This significant gap hinders the further grasping and utilization of ICL.

In this paper, we aim to bridge the existing gap by presenting a comprehensive analysis of compressed vectors. Specifically, we investigate their similarities with parameters trained via gradient descent and introduce the formulation of state vector that encapsulates the processing state of ICL stored in the attention activations. Building on the concept of state vector, and drawing insights from the model soup (Wortsman et al., 2022) and Polyak momentum-based gradient optimization algorithms (Qian, 1999, Sutskever et al., 2013), we propose inner optimization and momentum optimization strategies which are progressively applied to enhance the state vector. Moreover, we further exploit the demonstration compression capabilities of the state vector to address the practical challenges encountered when applying ICL in settings with multiple examples, where demonstrations are typically too lengthy for standard ICL, such as in the 100-shot setting which is prevalent in practice. Specifically, we introduce a divide-and-conquer aggregation method that effectively aggregates the ICL functions of these extensive examples. This approach enables us to scale up for processing extended examples by compressing them into a single state vector. We conduct extensive experiments using Llama-2 (Touvron et al., 2023) and GPT-J (Wang and Komatsuzaki, 2021) in both zero-shot and few-shot settings. The experimental results show that our method effectively enhances the state vector and achieves state-of-the-art performance on diverse tasks. This not only manifests the effectiveness of our approach but also paves the way for a more comprehensive understanding of ICL.

Our contributions are summarized as follows:

* We delve into the working mechanism of compressed vectors in ICL and highlight their similarities with parameters trained via gradient descent. Building on this observation, we propose the formulation of the state vector.
* We propose inner and momentum optimization to progressively refine the state vector as an efficient test-time adaptation. Additionally, we introduce a divide-and-conquer aggregation to effectively scale up to large numbers of examples.
* We show the practicality of our proposed methods across a wide range of tasks through extensive experiments. Our results also offer insights for future research aiming to fully understand the functionalities of ICL.

## 2 Related Work

Mechanistic Interpretability.Recent works have focused on the working mechanisms of ICL (Chan et al., 2022, Xie et al., 2022, Wang et al., 2023). Olsson et al. (2022) argue that induction heads may be the mechanistic source of general ICL in transformers. Akyurek et al. (2022) show that transformer-based in-context learners can implicitly implement standard optimization algorithms on linear models. A mainstream assumption posits that ICL has a similarity with the gradient descent. von Oswald et al. (2023) demonstrate how a linear attention-only transformer model can perform a gradient descent-like procedure implicitly. Dai et al. (2023) compare standard gradient descent based fine-tuning and ICL, and figure out that the transformer attention of ICL exhibits a dual form of gradient descent-based optimization. Moreover, some works revisit and modify this theory on the layer causality dependence (Natan et al., 2023) or training batch size (Shen et al., 2023). In contrast, we focus on the application of the dual form of gradient descent and ICL and present optimization methods with inspiration from the dual form.

Task Representation.Numerous studies have extensively explored the concept of compressing various tasks into task representations as a means of effectively manipulating tasks within ICL ability. Notably, Shao et al. (2023) and Mu et al. (2023) have successfully yielded compositional task representations by training a composition model. In a slightly different vein, some researchers have delved into the art of devising methodologies to compose minor parameter adjustments acquired through task fine-tuning (Ilharco et al., 2022, Panigrahi et al., 2023, Yu et al., 2023, Hu et al., 2024, Merullo et al., 2023). An alternative line of research finds that the task representation could be extracted in ICL (Liu et al., 2023, Hendel et al., 2023, Todd et al., 2023, Yang et al., 2023). Differentfrom these approaches, our work avoids the need for additional training and focuses more on analysing why these compressed vectors work and how to improve their performance.

## 3 Formalization

In this section, we first provide a detailed examination of attention activation which is found to contain the compressed ICL function by previous works (Hendel et al., 2023; Todd et al., 2023). Then, we highlight its inherent similarities with parameters trained through gradient descent. Finally, we introduce the concept of the state vector drawing inspiration from these observations.

A classic template of ICL has the following necessary components: (1) \(N\) examples that are used to form the demonstrations and each example contains an input query \(\) and its corresponding label \(\). (2) Separate tokens \(\) that separate the input query and the label for each example (e.g., \(\)). (3) A query \(_{q}\) for prediction. With the above components, the contextual model input of ICL could be written as follows:

\[_{1},,_{1},_{2},, _{2},,_{N},,_{N},_{q},.\]

Here we analyse the attention activation of the last separate token. In the \(l\)-th transformer layer, the output activation \(^{l}\) of the attention heads of the last separate token is:

\[^{l}=W_{V}[X^{};X]([X^{};X])^{T}}{}),\] (1)

where \(X^{}\) denotes the hidden state of demonstrations, \(X\) denotes the hidden state of the query and the last separate token (called zero-shot input), \(q\) denotes the attention query vector of the last separate token, \([X^{};X]\) denotes the matrix concatenation, \(\) is the scaling factor, \(W_{K}\) and \(W_{V}\) are parameter weight matrix.

Consistent with previous works (Dai et al., 2023; Natan et al., 2023), we omit the softmax operation and the scaling factor to approximate standard attention as relaxed linear attention for qualitative analysis. Consequently, the activation can be simplified as follows:

\[^{l}  W_{V}[X^{};X](W_{K}[X^{};X])^{T} \] (2) \[=(W_{V}X(W_{K}X)^{T}+W_{V}X^{}(W_{K}X ^{})^{T})\] \[=(W_{}+_{i}((W_{V}_{i}^{ })(W_{K}_{i}^{}))).\]

We define \(W_{}=W_{V}X(W_{K}X)^{T}\) as the initialized parameters since it is the attention result in the Zero-Shot Learning (ZSL) setting.

To draw a meaningful comparison between attention activation and parameters trained through gradient descent, we now shift our focus towards analyzing a simple linear transformation represented by \(_{i}=W_{i}\). Given a loss function \(\) and the learning rate \(\), the gradient of linear weight is:

\[_{W}(_{i})=(_{ i})}{_{i}}_{i}}{ W}=_{ _{i}}(_{i})_{i}^{T}.\] (3)

Denoting the back-propagated errors as \(_{i}=-_{_{i}}\), we can get the full batch gradient with training examples:

\[ W_{GD}=_{i}_{i}_{i}^{},\] (4)

where \(_{i}^{}\) is the input training examples. Hence, in the previous Eqn. 2, if we substitute \(W_{K}_{i}^{}\) as training examples, and take \(W_{V}_{i}^{}_{i}\) corresponding to some meta gradients (Dai et al., 2023; Natan et al., 2023). The activation can be written as:

\[^{l}=(W_{}+_{i}_{i} W_{K} _{i}^{})=(W_{}+ W_{GD} ).\] (5)

Hence, it can be inferred that the output activation \(^{l}\) can be regarded as parameters trained via gradient descent which utilizes the demonstrations as training instances.

With the above dual form between activation and trained parameters, and in light of observations that transformers tend to learn the ICL function primarily in their first \(L\) layers (Wang et al., 2023), we have the following hypothesis: During the process of ICL, the first \(L\) layers progressively update the flow of information using each example in the demonstration through forward computation. The processing state of ICL is then stored within the activation of the attention head. The subsequent layers access and utilize the processing state to reinstate the ICL function, which is used implicitly for predicting the queries. Therefore we concatenate the activation in the initial \(L\) layers and introduce the notation of the state vector:

\[_{N}^{L}=_{l=1}^{L}\ ^{l},\] (6)

where \(L\) is the number of layers and \(N\) is the number of examples in the demonstration. \(\|\) denotes the concatenation operation. Note that we have a completely different construction strategy and usage compared to the function vector (Todd et al., 2023). Although the task vector (Hendel et al., 2023) may be functionally equivalent in the forward process, the proposed state vector differs significantly in its integration into the model, making it easier and more effective to analyse and interpret.

## 4 Method

### Overview

As illustrated in Figure 1, our approach initially extracts the state vector from the attention head that corresponds to the final separate token in the first \(L\) layers using a demonstration and a dummy query. Then, with the view of treating the state vector as trained parameters, coupled with drawing inspiration from the model soup and the momentum-based gradient optimization algorithm, we introduce two methods that progressively optimize the state vector as test-time adaptation (Liang et al., 2023): (1) inner optimization (SS4.2) and (2) momentum optimization (SS4.3). Moreover, we propose a divide-and-conquer (D&C) state vector aggregation method for efficiently compressing the ICL function in the multiple example setting (SS4.4).

After the state vector optimization or aggregation, we utilize the processed state vector to intervene the model during the forward inference pass. In particular, we first input a test query in the zero-shot setting or with the demonstration in the few-shot setting. During the forward pass in the first \(L\) layers, we replace the attention activation of the last separate token with the corresponding activation in the state vector. In other words, the state vector is leveraged to intervene in the output of the first \(L\) transformer layers, blocking the attention of the last separate token to the previous context. With state vector intervention, the transformer learns the ICL function from the processing state stored in the state vector, and continues to make the prediction on the test query.

Figure 1: The overall framework of the proposed state vector. The state vectors are extracted from the output activations of attention heads. These state vectors are progressively optimized by _inner optimization_ and _momentum optimization_, or be aggregated through a _divide-and-conquer (D&C) aggregation_. Finally, the processed state vector is utilized to intervene the inference forward pass.

### Inner Optimization

Inspired by the works on the model soup (Wortsman et al., 2022; Chronopoulou et al., 2023) which show that weight-space averaging not only yields performance improvement but also often enhances robustness, we thus ask the following research question (**RQ1**): _Is it possible to optimize our state vector using the model soup approach?_ To explore this question, we propose an inner optimization method to improve the effectiveness and robustness of state vector. Specifically, we not only extract the state vector in each separate token of the dummy query but also extract the state vector from each example. Formally, with a forward pass in an \(N\) shot ICL setting, we extract the \(N\) state vector \(_{i}^{L}\) (\(1 i N\)) from last \(N\) separate token. Subsequently, we apply a uniform averaging process to these state vectors as follows:

\[}_{N}^{L}=_{i=1}^{N}_{i}^{L},\] (7)

where \(}_{N}^{L}\) is the inner optimized state vector, which can be directly utilized for inference intervention or serves as the initial state vector for later momentum optimization.

### Momentum Optimization

Since we view the state vector as parameters trained gradually through demonstration examples, the difference between two state vectors with adjacent corresponding separate tokens can also be regarded as the influence of the middle example, akin to the gradient. Motivated by this understanding, coupled with extensive studies of the gradient optimization algorithm (Sutskever et al., 2013; Duchi et al., 2010; Loshchilov and Hutter, 2019), we direct our focus toward a simple momentum-based gradient optimization algorithm, seeking to answer the following research question (**RQ2**): _Can our state vector be optimized using momentum-based optimization algorithm?_ To answer this question, we propose a momentum optimization. Formally, we first extract the influence of each example by subtracting two adjacent state vectors:

\[E_{i}^{L}=_{i}^{L}-_{i-1}^{L},\] (8)

where \(E_{i}^{L}\) is the influence of \(i\)-th (\(1<i N\)) example in the early \(L\) layer. Then, we apply the momentum gradient optimization algorithm to obtain optimized influence \(_{i}^{L}\), and add it to the last state vector:

\[}_{N}^{L}=}_{N}^{L}+^{L }=}_{N}^{L}+([E_{i}^{L}]_{i=1}^{N}),\] (9)

where \(}_{N}^{L}\) is the momentum optimized state vector and \(}_{N}^{L}\) is the inner optimized state vector. \(()\) denotes the momentum gradient optimization algorithm. We also explore various other gradient optimization algorithms in SS6.1.

### Divide-and-Conquer Aggregation

In addition to optimizing the state vector to more effectively represent the ICL function from a small number of examples, we also explore its capacity to encapsulate multiple examples within a single vector. However, regular ICL can not be directly used on multiple examples due to the context length limitation of current LLMs. This leads us to investigate the following question (**RQ3**): _Can we use the state vector to represent multiple examples that are unmanageable for regular ICL?_ To address this question, we propose a divide-and-conquer method for state vector aggregation. As depicted in Figure 1, our approach involves distinct aggregation processes (i.e. the divide stage and the conquer stage). In the divide stage, examples are randomly divided into groups, termed grouped demonstrations. Within each group, a random example is selected to serve as a dummy query, which allows us to extract a group-specific state vector. In the conquer stage, these dummy queries are paired with their corresponding labels to form input-label pairs. From these input-label pairs, we form an aggregated demonstration, add an additional dummy query, and subsequently extract the aggregated state vector. It is worth noting that during the forward pass of aggregated state vector extraction, we utilise the group-specific state vector to intervene the attention activation of the separate tokens of their corresponding examples. The divide and conquer approach allows us to aggregate the ICL function of each grouped demonstration into its respective group-specific state vector, and subsequently aggregate the ICL function of each group-specific state vector into a single,comprehensive aggregated state vector. This aggregated vector is then utilized for interventions during inference, similarly to the optimized state vector discussed in SS4.2 and SS4.3. Moreover, in the few-shot setting, the aggregated demonstrations are treated as inference demonstrations. The divide-and-conquer approach effectively circumvents the context-length constraints inherent in LLMs, thereby enabling a more effective and efficient aggregation of information across multiple examples.

## 5 Experiment

### Setup

We conduct the evaluation across 12 datasets that encompass different domains.

* **Linguistics** includes Antonym (Nguyen et al., 2017), Capitalize, Present-Past, and Singular-Plural (Todd et al., 2023), focusing on transformations in the form or meaning of words.
* **Translation** is represented by the English-French (Lample et al., 2018) dataset, which involves translating English words into their French counterparts.
* **Knowledge** comprises Country-Capital (Todd et al., 2023), AG News (Zhang et al., 2015), Person-Sport, Person-Instrument, Person-Occupation, Product-Company, and Landmark-Country (Hernandez et al., 2023), which are centred around question-to-answer mappings for commonsense knowledge queries.

We employ _Llama-2-7B_ and _GPT-J-6B_ as our LLMs, chosen for their moderate model sizes, open-source and capability for ICL. We also provide the results with larger models (i.e., Llama-2-13B) in the Appendix H. We use Llama-2-7B as the default model unless otherwise specified. Our method is orthogonal to the choice of transformer-based decoder-only autoregressive LLMs.

For simplicity evaluation, we restrict to single-token output and use first output token accuracy as the evaluation metric as in previous work (Hendel et al., 2023; Todd et al., 2023).

### Baseline

In the paper, we compare with the following methods:

* **Regular** is the baseline for the zero-shot setting that uses only the given query as input, while **ICL baseline**(Wei et al., 2022) makes predictions on the label by taking both the demonstrations and the given query.
* **Function vector**(Todd et al., 2023) is extracted from attention activation using the causal mediation method and is then added to the hidden state of certain transformer layers during inference.
* **Task vector**(Hendel et al., 2023) is extracted from the hidden state of the separate token and is leveraged for blocking the layer when inference.

### Inner Optimization(RQ1)

As shown in Table 1, the performance of our inner optimized state vector has a significant improvement comparing the task vector and function vector in both zero-shot and few-shot settings. Our state vector with inner optimization. In the zero-shot setting, the inner optimization shows an average improvement of 10.2% on Llama-2 and 5.9% on GPT-J across six datasets. In the few-shot setting, the inner optimization also achieves a 1.2% improvement on Llama-2 and 1.7% on GPT-J. The improvement demonstrates the effectiveness of inner optimization. However, although state vector (inn.) outperforms task vector, its few-shot performance on some datasets is inferior to the ICL baseline. We attribute this primarily to the introduction of query information from examples. While inner optimization enhances task-relevant information for the state vector, it also introduces noise of other dummy queries, hindering the model's ability to focus on the current predictive query, thereby reducing performance. In addition to the performance improvements, our inner optimization approach also effectively alleviates the phenomenon of high variance in the original task vector in the zero-shot setting. In practical use, the performance of the task vector is influenced by demonstrations and dummy queries, leading to weaker robustness. Our proposed inner optimization approach effectively mitigates this issue, similarly motivated as the model averaging method, thereby enhancing the robustness of the state vector.

### Momentum Optimization (RQ2)

As depicted in Table 1, building upon the inner optimized state vector, our proposed momentum optimization algorithm further enhances the effectiveness of the state vector, achieving the best performance on average in all settings. In the zero-shot setting, the momentum optimization boosts the performance of the inner-optimized state vector with an average increase of 1.3% on Llama-2 and 2.4% on GPT-J. In the few-shot setting, state vector with momentum optimization achieves a 0.8% average increase on Llama-2 and 1.0% on GPT-J. This reveals the effectiveness of our momentum optimization. With the combination of inner optimization and momentum optimization, our state

 
**Model** & **Method** & **Anym** & **Eng-F** & **Pers-Inst** & **Pers-Occ** & **Prod-Comp** & **Land-Cout** & **Average** \\   &  & Regular & 1.0\(\) 0.2 & 0.1\(\)0.0 & 0.0\(\)0.0 & 0.4\(\)0.2 & 0.0\(\)0.0 & 0.3 \\  & & Function vector & 45.1\(\)1.20 & 2.1\(\)0.13 & 11.0\(\)0.1 & 0.1\(\)0.1 & 25.6\(\)4.3 & 32.9\(\)21.6 & 22.8 \\  & & Task vector & 56.2\(\)2.8 & 63.2\(\)3.6 & 61.8\(\)8.4 & 27.9\(\)15.2 & 55.5\(\)20.1 & 57.8\(\)26.3 & 53.7 \\  & & State vector (inn.) & 61.0\(\)1.0 & 66.5\(\)2.2 & 67.4\(\)2.6 & 42.7\(\)2.4 & 64.5\(\)10.6 & 81.0\(\)1.7 & 63.9 \\  & & State vector (norm) & 60.4\(\)0.7 & 67.5\(\)1.8 & 68.7\(\)1.6 & 45.6\(\)5.9 & 71.3\(\)3.6 & 77.7\(\)1.8 & 65.2 \\   &  &  & 64.8\(\)4.8 & 74.3\(\)0.8 & 71.7\(\)3.7 & 56.1\(\)2.7 & 80.8\(\)0.8 & 87.0\(\)0.3 & 72.5 \\  & & Function vector & 54.5\(\)0.9 & 65.2\(\)1.4 & 60.8\(\)5.6 & 54.2\(\)2.2 & 76.0\(\)1.3 & 84.2\(\)2.9 & 65.8 \\  & & Task vector & 65.7\(\)1.8 & 73.8\(\)0.9 & 66.6\(\)5.2 & 56.4\(\)2.3 & 81.9\(\)1.8 & 86.7\(\)0.9 & 71.8 \\  & & State vector (norm) & 66.2\(\)1.6 & 74.6\(\)0.7 & 70.4\(\)1.3 & 57.0\(\)2.9 & **82.8\(\)**1.6 & 87.5\(\)0.9 & 73.0 \\  & & State vector (norm) & 65.8\(\)3.7 & 74.3\(\)1.1 & **74.9\(\)**2.9 & **58.2\(\)**0.4 & 82.0\(\)1.0 & **87.6\(\)**0.3 & **73.8** \\   &  & Regular & 8.1\(\)0.6 & 7.2\(\)0.6 & 0.0\(\)0.0 & 0.0\(\)0.0 & 1.9\(\)0.5 & 0.9\(\)0.2 & 3.0 \\  & & Function vector & 33.1\(\)1.8 & 32.9\(\)1.8 & 41.5\(\)8.1 & 31.1\(\)2.3 & 46.3\(\)5.7 & 22.5\(\)10.2 & 24.4 \\  & & Task vector & 22.6\(\)3.8 & 32.2\(\)5.1 & 44.4\(\)2.0 & 28.3\(\)18.6 & 43.8\(\)5.7 & 41.3\(\)12.3 & 35.6 \\  & & State vector (inn.) & 33.4\(\)1.9 & 31.7\(\)3.8 & 49.3\(\)2.0 & 30.0\(\)6.2 & **42.8\(\)**4.3 & 61.9\(\)1.6 & 41.5 \\  & & State vector (norm) & 31.1\(\)1.0 & 35.1\(\)2.4 & 50.3\(\)3.0 & 42.4\(\)1.5 & 44.2\(\)1.5 & 60.3\(\)0.9 & 43.9 \\   &  &  & 59.2\(\)1.4 & 69.9\(\)2.0 & 47.4\(\)6.7 & 29.3\(\)1.0 & 62.5\(\)1.0 & **69.3\(\)**0.5 & 55.8 \\  & & Function vector & 56.4\(\)1.9 & 65.8\(\)1.9 & 49.1\(\)1.2 & 30.3\(\)1.9 & 58.5\(\)3.3 & 69.2\(\)0.6 & 54.9 \\   & & Task vector & 58.5\(\)1.6 & 70.6\(\)1.2 & 42.3\(\)6.4 & 27.8\(\)3.3 & 66.0\(\)2.6 & 63.1\(\)5.3 & 54.7 \\   & & State vector (inn.) & 58.7\(\)2.2 & **70.9\(\)**1.3 & 46.5\(\)4.9 & 29.4\(\)1.7 & **66.3\(\)**2.1 & 66.4\(\)2.8 & 56.4 \\   & & State vector (norm) & **59.6\(\)**1.4 & 70.1\(\)2.2 & **51.9\(\)**2.4 & **30.4\(\)**1.1 & 63.8\(\)**0.8 & 68.6\(\)0.3 & **57.4** \\  

Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in **bold**. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (\(p<.05\)).

Figure 2: Performance of aggregation across number of examples. _Avg._ denotes the average aggregation baseline and _D&C._ denotes the divide-and-conquer aggregation. The **X** axis represents the number of examples, and the **Y** axis represents the accuracy.

vector (mom.) surpasses the original variant, showcasing a remarkable improvement of 11.5% for Llama-2 and 8.3% for GPT-J in the zero-shot setting. In the few-shot setting, our state vector (mom.) still outperforms the task vector with a 2.0% improvement for Llama-2 and 2.7% for GPT-J. Furthermore, without inputting demonstration during inference, the state vector (mom.) achieves an impressive 90% ICL performance on Llama-2 and 78% ICL performance on GPT-J. When compared to ICL with the same examples as the demonstration, state vector (mom.) outperforms ICL in both Llama-2 and GPT-J. These improvements verify the effectiveness of our progressive optimization strategy. Note that applying momentum optimization directly to task vectors does not yield average improvements across tasks in our preliminary experiment. We speculate that this inconsistency stems from the poor robustness of the task vectors, which hinders the stable optimization by momentum optimization and leads to poor performance in some tasks.

### Divide-and-Conquer Aggregation (RQ3)

In this experiment, we explore the performance of D&C state vector aggregation across varying numbers of examples. Besides the regular and ICL baseline mentioned, we introduce average aggregation as a strong baseline. This approach first extracts state vectors from the example group and subsequently employs their mathematical average for aggregation. We compare our D&C aggregation method with the baseline ranging from 10 to 100 examples across two models. Due to limited computational resources, we were not able to do an exhaustive search over all datasets. Thus, we only present the results for four tasks.

As illustrated in the Figure 2, both the D&C aggregation and average aggregation exhibit similar trends in both few-shot and zero-shot settings. The performance of both aggregation methods initially falls short of the ICL baseline. However, their performance boosts when examples increase. The initial poor performance can be attributed to the limited number of state vectors. Additionally, although the performance of the D&C aggregation initially falls behind that of the average aggregation, it exhibits a more substantial performance improvement when examples increase, ultimately outperforming average aggregation in the multiple example setting, highlighting the efficiency of D&C aggregation.

## 6 Analysis

### Ablation with Other Optimization Methods

We present an ablation study to investigate various classical gradient optimization algorithms, aiming to delve deeper into the inner state vector optimization. We compare the momentum-based gradient optimization algorithm with following additional first-order gradient optimization algorithms: Adagrad (adag.) (Duchi et al., 2010), RMSprop (rms.) (Graves, 2013) and Adam(adam.) (Kingma and Ba, 2015).

  
**Method** & **Zero-shot** & **Few-shot** \\  ICL baseline & 0.2\(\) 0.4 & 71.0\(\) 10.8 \\ Task vector & 52.9\(\) 9.4 & 68.5\(\) 10.5 \\ State vector (mom.) & 65.2\(\) 10.2 & 72.2\(\) 10.6 \\ State vector (adag.) & 11.7\(\) 12.0 & 16.1\(\) 10.2 \\ State vector (rms.) & 0.8\(\) 0.9 & 1.5\(\) 1.0 \\ State vector (adam.) & 6.7\(\) 6.1 & 10.6\(\) 8.5 \\   

Table 2: Performance comparison of gradient optimization algorithms. The method means the optimization algorithm applied to the \(()\) in Eqn. 9.

Figure 3: Average zero-shot performance across six datasets for each choice of the intermediate layer \(L\). The solid line means the average value, while the shaded area indicates the standard deviation.

As shown in Table 2, we observe a significant decrease in state vector performance when using first-order gradient optimization algorithms, in contrast to the more stable results obtained with momentum-based optimization. This discrepancy suggests that current first-order gradient optimization algorithms may not be well-suited for state vector optimization. We believe there are two main reasons for this. First, first-order gradient optimizers typically rely on adaptive learning rates, which depend on a significant amount of historical information. This can lead to instability and reduced effectiveness, especially when the available data is limited. Second, first-order gradient optimizers involve more complex hyper-parameters compared to the Momentum Optimizer, making it more difficult to identify optimal settings. Our experimental results confirm that directly applying hyper-parameter configurations commonly used in gradient descent leads to suboptimal performance in state vector optimization.

### Layer Selection

We investigate the impact of layer selection on the extraction of state vectors in transformer models. We evaluate the average performance across different datasets in the zero-shot setting, as illustrated in Figure 3. Our results reveal a dual-phase trend: initially, increasing the number of layers for state vector extraction improves performance, but this improvement reverses beyond the 14th layer. We correlate this with the dynamics of ICL function processing in transformers in line with previous works (Voita et al., 2019; Wang et al., 2023). In the initial layers, transformers are primarily engaged in learning and encapsulating the ICL function within state vector, where additional layers enhance the richness of the functional information in the state vector. In contrast, the later layers prioritize applying this learned information for prediction tasks. Here, additional layers tend to introduce noise, especially from predicted labels of dummy queries, which may negatively impact performance.

### Qualitative Study

We provide the visualization by Principal Component Analysis (PCA) of the original state vector in the Antonym, English-French and Product-Company task. As depicted in Figure 4, we have three observations: (1) State vectors corresponding to the examples occupying the same position tend to form distinct clusters. This clustering pattern suggests a high degree of similarity among state vectors within each example position, despite different contexts. (2) A notable separation is evident between the state vectors originating from the first example and other position examples. This demarcation implies that ICL may begin to effectively function with a few examples. (3) An interesting trend is observable in the movement of these clusters as the example position increases. This trend may be indicative of an accumulation of task-specific information, where each additional example contributes to a more nuanced understanding of the model. These findings suggest a progressive enhancement in the ability of model to internalize and reflect the subtleties of the task at hand. Moreover, these observations reflect the efficacy of momentum optimization to leverage the observed clustering trend.

Figure 4: The 2D PCA visualization of the state vector in the Antonym, English-French, and Product-Company tasks, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration, with the position of each point indicated by the adjacent number.

Conclusion

In this paper, we reveal that ICL compressed vector can be viewed as parameters trained through gradient descent on the demonstrations. Then, we introduce the concept of state vector coupled with two optimization methods to enhance the capability of ICL and conduct comprehensive experiments across two popular LLMs and multiple tasks to support our claim. Furthermore, our approach demonstrates the ability to compress context while maintaining lower variance. In the future, we aim to extend our methods to more complex ICL scenarios and apply them to larger LLMs and call for more nuanced and realistic studies of ICL.