# CORL: Research-oriented Deep Offline Reinforcement Learning Library

Denis Tarasov

Tinkoff

den.tarasov@tinkoff.ai &Alexander Nikulin

Tinkoff

a.p.nikulin@tinkoff.ai &Dmitry Akimov

Tinkoff

d.akimov@tinkoff.ai &Vladislav Kurenkov

Tinkoff

v.kurenkov@tinkoff.ai &Sergey Kolesnikov

Tinkoff

s.s.kolesnikov@tinkoff.ai

###### Abstract

CORL1 is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.

## 1 Introduction

Deep Offline Reinforcement Learning (Levine et al., 2020) has been showing significant advancements in numerous domains such as robotics (Smith et al., 2022; Kumar et al., 2021), autonomous driving (Diehl et al., 2021) and recommender systems (Chen et al., 2022). Due to such rapid development, many open-source offline RL solutions2 emerged to help RL practitioners understand and improve well-known offline RL techniques in different fields. On the one hand, they introduce offline RL algorithms standard interfaces and user-friendly APIs, simplifying offline RL methods incorporation into _existing_ projects. On the other hand, introduced abstractions may hinder the learning curve for newcomers and the ease of adoption for researchers interested in developing _new_ algorithms. One needs to understand the modularity design (several files on average), which (1) can be comprised of thousands of lines of code or (2) can hardly fit for a novel method3.

In this technical report, we take a different perspective on an offline RL library and also incorporate emerging interest in the offline-to-online setup. We propose CORL (Clean Offline Reinforcement Learning) - minimalistic and isolated single-file implementations of deep offline and offline-to-online RL algorithms, supported by open-sourced D4RL (Fu et al., 2020) benchmark results. The uncomplicated design allows practitioners to read and understand the implementations of the algorithms straightforwardly. Moreover, CORL supports optional integration with experiments tracking tools such as Weighs&Biases (Biewald, 2020), providing practitioners with a convenient way to analyzethe results and behavior of all algorithms, not merely relying on a final performance commonly reported in papers.

We hope that the CORL library will help offline RL newcomers study implemented algorithms and aid the researchers in quickly modifying existing methods without fighting through different levels of abstraction. Finally, the obtained results may serve as a reference point for D4RL benchmarks avoiding the need to re-implement and tune existing algorithms' hyperparameters.

## 2 Related Work

Since the Atari breakthrough (Mnih et al., 2015), numerous open-source RL frameworks and libraries have been developed over the last years: (Dhariwal et al., 2017; Hill et al., 2018; Castro et al., 2018; Gauci et al., 2018; Keng & Graesser, 2017; garage contributors, 2019; Duan et al., 2016; Kolesnikov & Hrinchuk, 2019; Fujita et al., 2021; Liang et al., 2018; Fujita et al., 2021; Liu et al., 2021; Huang et al., 2021; Weng et al., 2021; Stooke & Abbeel, 2019), focusing on different perspectives of the RL. For example, stable-baselines (Hill et al., 2018) provides many deep RL implementations that carefully reproduce results to back up RL practitioners with reliable baselines during methods comparison. On the other hand, Ray (Liang et al., 2018) focuses on implementations scalability and production-friendly usage. Finally, more nuanced solutions exist, such as Dopamine (Castro et al., 2018), which emphasizes different DQN variants, or ReAgent (Gauci et al., 2018), which applies RL to the RecSys domain.

At the same time, the offline RL branch and especially offline-to-online, which we are interested in this paper, are not yet covered as much: the only library that precisely focuses on offline RL setting is d3rlpy (Takuma Seno, 2021). While CORL also focuses on offline RL methods (Nair et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Fujimoto & Gu, 2021; An et al., 2021; Chen et al., 2021), similar to d3rlpy, it takes a different perspective on library design and provides _non-modular_ independent algorithms implementations. More precisely, CORL does not introduce additional abstractions to make offline RL more general but instead gives an "easy-to-hack" starter kit for research needs. Finally, CORL also provides recent offline-to-online solutions (Nair et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Wu et al., 2022; Nakamoto et al., 2023; Tarasov et al., 2023) that are gaining interest among researchers and practitioners.

Figure 1: The illustration of the CORL library design. Single-file implementation takes a yaml configuration file with both environment and algorithm parameters to run the experiment, which logs all required statistics to Weights&Biases (Biewald, 2020).

Although CORL does not represent the first non-modular RL library, which is more likely the CleanRL (Huang et al., 2021) case, it has two significant differences from its predecessor. First, CORL is focused on _offline_ and _offline-to-online_ RL, while CleanRL implements _online_ RL algorithms. Second, CORL intends to minimize the complexity of the requirements and external dependencies. To be more concrete, CORL does not have additional requirements with abstractions such as \(stable\)-\(baselines\)(Hill et al., 2018) or \(envpool\)(Weng et al., 2022) but instead implements everything from scratch in the codebase.

## 3 CORL Design

### Single-File Implementations

Implementational subtleties significantly impact agent performance in deep RL (Henderson et al., 2018; Engstrom et al., 2020; Fujimoto and Gu, 2021). Unfortunately, user-friendly abstractions and general interfaces, the core idea behind modular libraries, encapsulate and often hide these important nuances from the practitioners. For such a reason, CORL unwraps these details by adopting single-file implementations. To be more concrete, we put environment details, algorithms hyperparameters, and evaluation parameters into a single file4. For example, we provide

* \(any\_percent\_bc.py\) (404 LOC5) as a baseline algorithm for offline RL methods comparison,
* \(td3\_bc.py\) (511 LOC) as a competitive minimalistic offline RL algorithm (Fujimoto and Gu, 2021),
* \(dt.py\) (540 LOC) as an example of the recently proposed trajectory optimization approach (Chen et al., 2021)

Figure 1 depicts an overall library design. To avoid over-complicated offline implementations, we treat offline and offline-to-online versions of the same algorithms separately. While such design produces code duplications among realization, it has several essential benefits from the both educational and research perspective:

* **Smooth learning curve**. Having the entire code in one place makes understanding all its aspects more straightforward. In other words, one may find it easier to dive into 540 LOC of single-file Decision Transformer (Chen et al., 2021) implementation rather than 10+ files of the original implementation6. * **Simple prototyping**. As we are not interested in the code's general applicability, we could make it implementation-specific. Such a design also removes the need for inheritance from general primitives or their refactoring, reducing abstraction overhead to zero. At the same time, this idea gives us complete freedom during code modification.
* **Faster debugging**. Without additional abstractions, implementation simplifies to a single for-loop with a global Python name scope. Furthermore, such flat architecture makes accessing and inspecting any created variable easier during training, which is crucial in the presence of modifications and debugging.

### Configuration files

Although it is a typical pattern to use a command line interface (CLI) for single-file experiments in the research community, CORL slightly improves it with predefined configuration files. Utilizing YAML parsing through CLI, for each experiment, we gather all environment and algorithm hyperparameters into such files so that one can use them as an initial setup. We found that such setup (1) simplifies experiments, eliminating the need to keep all algorithm-environment-specific parameters in mind, and (2) keeps it convenient with the familiar CLI approach.

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

AWAC, initially proposed for finetuning purposes, appeared to be the worst of the considered algorithms, where the score is improved only on the most straightforward antmaze-umaze-v2 dataset. At the same time, on other datasets, performances either stay the same or even drop.

**Observation 5**: AWAC does not benefit from online tuning on the considered tasks.

Cal-QL was proposed as a modification of CQL, which is expected to work better in offline-to-online setting. However, in our experiments, after finetuning CQL obtained scores which are not very different from Cal-QL. At the same time, we could not make both algorithms solve Adroit tasks13.

**Observation 6**: There is no big difference between CQL and Cal-QL. On AntMaze, these algorithms perform the best but work poorly on Adroit.

IQL starts with good offline scores on AntMaze, but it is less efficient in finetuning than other algorithms except for AWAC. At the same time, IQL and ReBRAC are the only algorithms that notably improve its scores after tuning on Adroit tasks, making them the most competitive offline-to-online baselines considering the average score.

  
**Task Name** & **AWAC** & **CQL** & **IQL** & **SPOT** & **Cal-QL** & **ReBRAC** \\  antmaze-umaze-v2 & 0.04 \(\) 0.01 & 0.02 \(\) 0.00 & 0.07 \(\) 0.00 & 0.02 \(\) 0.00 & **0.01**\(\) 0.00 & 0.10 \(\) 0.20 \\ antmaze-umaze-diverse-v2 & 0.88 \(\) 0.01 & 0.09 \(\) 0.01 & 0.43 \(\) 0.11 & 0.22 \(\) 0.07 & **0.05**\(\) 0.01 & 0.04 \(\) 0.02 \\ antmaze-medium-play-v2 & 1.00 \(\) 0.00 & 0.08 \(\) 0.01 & 0.09 \(\) 0.01 & 0.06 \(\) 0.00 & **0.04**\(\) 0.01 & 0.02 \(\) 0.00 \\ antmaze-medium-diverse-v2 & 1.00 \(\) 0.00 & 0.08 \(\) 0.00 & 0.10 \(\) 0.01 & 0.05 \(\) 0.01 & **0.04**\(\) 0.01 & 0.03 \(\) 0.00 \\ antmaze-large-play-v2 & 1.00 \(\) 0.00 & 0.21 \(\) 0.02 & 0.34 \(\) 0.05 & 0.29 \(\) 0.07 & **0.13**\(\) 0.02 & 0.14 \(\) 0.05 \\ antmaze-large-diverse-v2 & 1.00 \(\) 0.00 & 0.21 \(\) 0.03 & 0.41 \(\) 0.03 & 0.23 \(\) 0.08 & **0.13**\(\) 0.02 & 0.29 \(\) 0.45 \\ 
**AntMaze avg** & 0.82 & 0.11 & 0.24 & 0.15 & **0.07** & 0.10 \\  pen-cloned-v1 & 0.46 \(\) 0.02 & 0.97 \(\) 0.00 & **0.37**\(\) 0.01 & 0.58 \(\) 0.02 & 0.98 \(\) 0.01 & 0.08 \(\) 0.01 \\ door-cloned-v1 & 1.00 \(\) 0.00 & 1.00 \(\) 0.00 & **0.83**\(\) 0.03 & 0.99 \(\) 0.01 & 1.00 \(\) 0.00 & 0.18 \(\) 0.06 \\ hammer-cloned-v1 & 1.00 \(\) 0.00 & 1.00 \(\) 0.00 & **0.65**\(\) 0.10 & 0.98 \(\) 0.01 & 1.00 \(\) 0.00 & 0.12 \(\) 0.03 \\ relocate-cloned-v1 & 1.00 \(\) 0.00 & 1.00 \(\) 0.00 & 1.00 \(\) 0.00 & 1.00 \(\) 0.00 & 1.00 \(\) 0.00 & 0.9 \(\) 0.06 \\ 
**Adroit avg** & 0.86 & 0.99 & **0.71** & 0.89 & 0.99 & 0.32 \\ 
**Total avg** & 0.84 & 0.47 & **0.43** & 0.44 & 0.44 & 0.19 \\   

Table 4: Cumulative regret of online finetuning calculated as \(1-\)_average success rate_ averaged over 4 random seeds.

Figure 3: (a) Performance profiles after online tuning (b) Probability of improvement of ReBRAC to other algorithms after online tuning. The curves (Agarwal et al., 2021) are for D4RL benchmark spanning AntMaze and Adroit cloned datasets.

**Observation 7**: Considering offline and offline-to-online results, IQL and ReBRAC appear to be the strongest baselines on average.

## 5 Conclusion

This paper introduced CORL, a single-file implementation library for offline and offline-to-online reinforcement learning algorithms with configuration files and advanced metrics tracking support. In total, we provided implementations of ten offline and six offline-to-online algorithms. All implemented approaches were benchmarked on D4RL datasets, closely matching (sometimes overperforming) the reference results, if available. Focusing on implementation clarity and reproducibility, we hope that CORL will help RL practitioners in their research and applications.

This study's benchmarking results and observations are intended to serve as references for future offline reinforcement learning research and its practical applications. By sharing comprehensive logs, researchers can readily access and utilize our results without having to re-run any of our experiments, ensuring that the results are replicable.