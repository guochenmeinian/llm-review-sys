# Stochastic Newton Proximal Extragradient Method

Ruichen Jiang

ECE Department

UT Austin

rjiang@utexas.edu &Michal Derezinski

EECS Department

University of Michigan

derezin@umich.edu &Aryan Mokhtari

ECE Department

UT Austin

mokhtari@austin.utexas.edu

###### Abstract

Stochastic second-order methods achieve fast local convergence in strongly convex optimization by using noisy Hessian estimates to precondition the gradient. However, these methods typically reach superlinear convergence only when the stochastic Hessian noise diminishes, increasing per-iteration costs over time. Recent work in  addressed this with a Hessian averaging scheme that achieves superlinear convergence without higher per-iteration costs. Nonetheless, the method has slow global convergence, requiring up to \(}(^{2})\) iterations to reach the superlinear rate of \(}((1/t)^{t/2})\), where \(\) is the problem's condition number. In this paper, we propose a novel stochastic Newton proximal extragradient method that improves these bounds, achieving a faster global linear rate and reaching the same fast superlinear rate in \(}()\) iterations. We accomplish this by extending the Hybrid Proximal Extragradient (HPE) framework, achieving fast global and local convergence rates for strongly convex functions with access to a noisy Hessian oracle.

## 1 Introduction

In this paper, we focus on the use of second-order methods for solving the optimization problem

\[_{^{d}}\ f(),\] (1)

where \(f:^{d}\) is strongly convex and twice differentiable. There is an extensive literature on second-order methods and their fast local convergence properties; e.g., . However, these results necessitate access to the exact Hessian, which can pose computational challenges. To address this issue, several studies have explored scenarios where only the exact gradient can be queried, while a stochastic estimate of the Hessian is available--similar to the setting we investigate in this paper. This oracle model is commonly encountered in large-scale machine learning problems, as computing the gradient is often much less expensive than computing the Hessian, and approximating the Hessian is a more affordable approach. Specifically, consider a finite-sum minimization problem \(_{x^{d}}_{i=1}^{n}f_{i}(x)\), where \(n\) denotes the number of data points and \(d\) denotes the dimension of the problem. To achieve a fast convergence rate, standard first-order methods need to compute one full gradient in each iteration, resulting in a per-iteration computational cost of \((nd)\). In contrast, implementing a second-order method such as damped Newton's method involves computing the full Hessian, which costs \((nd^{2})\). An inexact Hessian estimate can be constructed efficiently at a cost of \((sd^{2})\), where \(s\) is the sketch size or subsampling size . Hence, when the number of samples \(n\) significantly exceeds \(d\), the per-iteration cost of stochastic second-order methods becomes comparable to that of first-order methods. Moreover, using second-order information often reduces the number of iterations needed to converge, thereby lowering overall computational complexity.

A common template among stochastic second-order methods is to combine a deterministic second-order method, such as Newton's method or cubic regularized Newton method, with techniques such as Hessian subsampling  or Hessian sketching  that only require a noisy estimateof the Hessian. We refer the reader to  for recent surveys and empirical comparisons. In terms of convergence guarantees, the majority of these works, including , have shown that stochastic second-order methods exhibit a global linear convergence and a local linear-quadratic convergence, either with high probability or in expectation. The linear-quadratic behavior holds when

\[\|_{t+1}-^{*}\| c_{1}\|_{t}-^{*}\| +c_{2}\|_{t}-^{*}\|^{2},\] (2)

where \(^{*}\) denotes the optimal solution of Problem (1) and \(c_{1},c_{2}\) are constants depending on the sample/sketch size at each step. In particular, the presence of the linear term in (2) implies that the algorithm can only achieve linear convergence when the iterate is sufficiently close to the optimal solution \(^{*}\). Consequently, as discussed in , to achieve superlinear convergence, the coefficient \(c_{1}=c_{1,t}\) needs to gradually decrease to zero as \(t\) increases. However, since \(c_{1}\) is determined by the magnitude of the stochastic noise in the Hessian estimate, this in turn demands the sample/sketch size to increase across the iterations, leading to a blow-up of the per-iteration computational cost.

The only prior work addressing this limitation and achieving a superlinear rate for a stochastic second-order method without requiring the stochastic Hessian noise to converge to zero is by . It uses a weighted average of all past Hessian approximations as the current Hessian estimate. This approach reduces stochastic noise variance in the Hessian estimate, though it introduces bias to the Hessian approximation matrix. When combined with Newton's method, it was shown that the proposed method achieves local superlinear convergence with a non-asymptotic rate of \(()^{t}\) with high probability, where \(\) characterizes the noise level of the stochastic Hessian oracle (see Assumption 4). However, the method may require many iterations to achieve superlinear convergence. Specifically, with the uniform averaging scheme, it takes \(}(^{3})\) iterations before the method starts converging superlinearly and \(}(^{6}/^{2})\) iterations before it reaches the final superlinear rate. Here, \(=L_{1}/\) denotes the condition number of the function \(f\), where \(L_{1}\) is the Lipschitz constant of the gradient and \(\) is the strong convexity parameter. To address this,  proposed a weighted averaging scheme that assigns more weight to recent Hessian estimates, improving both transition points to \(}(^{2}+^{2})\) while achieving a slightly slower superlinear rate of \(((t)/)\).

**Our contributions.** In this paper, we improve the complexity of Stochastic Newton in  with a method that attains a superlinear rate in significantly fewer iterations. As shown in Table 1, our method requires fewer iterations for linear convergence, denoted as \(_{1}\), by a factor of \(^{2}\) compared to . Additionally, our method achieves a linear convergence rate of \((1-(1/))^{t}\), outperforming the \((1-(1/^{2}))^{t}\) rate in . Thus, our method reaches the local neighborhood of the optimal solution \(^{*}\) and transitions from linear to superlinear convergence faster. Specifically, the second transition point, \(_{2}\), is smaller by a factor of \(\) in both uniform and non-uniform averaging schemes when \(=()\). Similarly, our method's initial superlinear rate has a better dependence on \(\), leading to fewer iterations, \(_{3}\), to enter the final superlinear phase. To achieve this result, we use the hybrid proximal extragradient (HPE) framework  instead of Newton's method as the base algorithm. The HPE framework provides a principled approach for designing second-order methods with superior global convergence guarantees . However,  and subsequent works focus on cases where \(f\) is merely convex, not leveraging strong convexity. Thus, we modify the HPE framework to suit our setting. Specifically, we relax the error condition for computing the proximal step in HPE, enabling a larger step size when the iterate is close to the optimal solution, crucial for achieving the final superlinear convergence rate.

    &  &  &  &  \\   & & \(_{1}\) & rate \(\) & \(_{2}\) & rate \(_{t}^{(1)}\) & \(_{3}\) & rate \(_{t}^{(2)}\) \\   Stochastic \\ Newton  \\  } & Uniform & \(^{2}\) & \(1-^{-2}\) & \(^{3}\) & \(}{t}\) & \(}{t^{2}}\) & \(}\) \\  & Non-uniform & \(^{2}\) & \(1-^{-2}\) & \(^{2}\) & \(}{t^{(t)}}\) & \(^{2}\) & \(}\) \\   Stochastic \\ NPE (**Ours**) \\  } & Uniform & \(}{^{2}}\) & \(1-^{-1}\) & \(^{2}\) & \(}{t}\) & \(}{^{2}}\) & \(}\) \\  & Non-uniform & \(}{^{2}}\) & \(1-^{-1}\) & \(^{2}+\) & \(+)^{(^{2}+)+1}}{t^{(t)}}\) & \(^{2}+\) & \(}\) \\   

Table 1: Comparison between Algorithm 1 and the stochastic Newton method in , in terms of how many iterations it takes to transition to each phase, and the convergence rates achieved. We drop constant factors as well as logarithmic dependence and \(1/\), and assume \(1/()()\).

## 2 Preliminaries

In this section, we formally present our assumptions.

**Assumption 1**.: _The function \(f\) is twice differentiable and \(\)-strongly convex._

**Assumption 2**.: _The Hessian \(^{2}f\) satisfies \(\|^{2}f()-^{2}f()\| M_{1}\)._

**Assumption 3**.: _The Hessian \(^{2}f\) is \(L_{2}\)-Lipschitz, i.e., \(\|^{2}f()-^{2}f()\| L_{2}\|- \|_{2}\)._

Assumption 2 is more general than the assumption that \( f\) is \(L_{1}\)-Lipschitz. In particular, if the latter assumption holds, then \(M_{1} L_{1}\). Moreover, we define \( M_{1}/\) as the condition number.

To simplify our notation, we denote the exact gradient \( f()\) and the exact Hessian \(^{2}f()\) of the objective function by \(()\) and \(()\), respectively. As mentioned earlier, we assume that we have access to the exact gradient, but we only have access to a noisy estimate of the Hessian denoted by \(}()\). In fact, we require a mild assumption on the Hessian noise. We define the stochastic Hessian noise as \(()}()-( )\), where it is assumed to be mean zero and sub-exponential.

**Assumption 4**.: _If we define \(()}()-( )\), then \([()]=0\) and \([\|()\|^{p}] p!_{E}^{p}/2\) for all integers \(p 2\). Also, define \(_{E}/\) to be the relative noise level._

**Assumption 5**.: _The Hessian approximation matrix is positive semi-definite, i.e., \(}() 0\), \(^{d}\)._

**Stochastic Hessian construction.** The two most popular approaches to construct stochastic Hessian approximations are "subsampling" and "sketching". _Hessian subsampling_ is designed for a finite-sum objective of the form \(f()=_{i=1}^{n}f_{i}()\), where \(n\) is the number of samples. In each iteration, a subset \(S\{1,2,,n\}\) is drawn uniformly at random, and then the subsampled Hessian at \(\) is constructed as \(}()=_{i S}^{2}f_{i}()\). In this case, if each \(f_{i}\) is convex, then the condition in Assumption 5 is satisfied. Moreover, if we further assume that \(\|^{2}f_{i}()\| cM_{1}\) for some \(c>0\) and for all \(i\), then Assumption 4 is satisfied with \(=(+c(d)/|S|)\) (see [1, Example 1]). The other approach is _Hessian sketching_, applicable when the Hessian \(\) can be easily factorized as \(=^{}\), where \(^{n d}\) is the square-root Hessian matrix, and \(n\) is the number of samples. This is the case for generalized linear models; see . To form the sketched Hessian, we draw a random sketch matrix \(^{s n}\) with sketch size \(s\) from a distribution \(\) that satisfies \(_{}[^{}]=\). The sketched Hessian is then \(}=^{}^{}\). In this case, Assumption 5 is automatically satisfied. Moreover, for Gaussian sketch, Assumption 4 is satisfied with \(=((+d/s))\) (see [1, Example 2]).

_Remark 1_.: The above assumptions are common in the study of stochastic second-order methods, appearing in works on Subsampled Newton , Newton Sketch , and notably, . The strong convexity requirement is crucial as stochastic second-order methods have a clear advantage over first-order methods like gradient descent when the function is strongly convex. Specifically, stochastic second-order methods attain a superlinear convergence rate, as shown in this paper, which is superior to the linear rate of first-order methods.

## 3 Stochastic Newton Proximal Extragradient

Our approach involves developing a stochastic Newton-type method grounded in the Hybrid Proximal Extragradient (HPE) framework and its second-order variant. Therefore, before introducing our proposed algorithm, we will provide a brief overview of the core principles of the HPE framework. Following this, we will present our method as it applies to the specific setting addressed in this paper.

**Hybrid Proximal Extragradient.** Next, we first present the Hybrid Proximal Extragradient (HPE) framework for strongly convex functions. To solve problem (1), the HPE algorithm consists of two steps. In the first step, given \(_{t}\), we find a mid-point \(}_{t}\) by applying an inexact proximal point update \(}_{t}_{t}-_{t} f(}_{t})\), where \(_{t}\) is the step size. More precisely, we require

\[\|}_{t}-_{t}+_{t} f(}_{t}) \|}\|}_{t}-_{t}\|,\] (3)

where \(_{t}=1+2_{t}\), \(\) is the strong convexity parameter, and \((0,1)\) is a user-specified parameter. Then, in the second step, we perform the extra-gradient update and compute \(_{t+1}\) based on

\[_{t+1}=}(_{t}-_{t} f(}_{t}))+1-}}_{t},\] (4)The weights \(}\) in the above convex combination are chosen to optimize the convergence rate.

_Remark 2_.: When \(=0\), the algorithm outline above reduces to the original HPE framework studied in . Our modification in (3) is inspired by  and allows a larger error when performing the inexact proximal point update, which turns out to be crucial for achieving a fast superlinear convergence rate. Moreover, the modification in (4) has been adopted in .

**Stochastic Newton Proximal Extragradient (SNPE).** The HPE method described above provides a useful algorithmic framework, instead of a directly implementable method. The main challenge comes from implementing the first step in (3), which involves an inexact proximal point update. Specifically, the naive approach is to solve the _implicit nonlinear equation_\(-_{t}+_{t} f()=0\), which can be as costly as solving the original problem in (1). To address this issue,  proposed to approximate the gradient operator \( f()\) by its local linearization \( f(_{t})+^{2}f(_{t})(-_{t})\), and then compute \(}_{t}\) by solving the linear system of equations \(}_{t}-_{t}+_{t}( f(_{t})+^ {2}f(_{t})(}_{t}-_{t}))=0\). This leads to the Newton proximal extragradient method that was proposed and analyzed in .

However, in our setting, the exact Hessian \(^{2}f(_{t})\) is not available. Thus, we construct a stochastic Hessian approximation \(}_{t}\) from our noisy Hessian oracle as a surrogate of \(^{2}f(_{t})\). We will elaborate on the construction of \(}_{t}\) later, but for the present discussion assume that this stochastic Hessian approximation \(}_{t}\) is already provided. Then in the first step, we will compute \(}_{t}\) by

\[}_{t}=_{t}-_{t}( f(_{t})+}_{t}(}_{t}-_{t})),\] (5)

where we replace \( f(}_{t})\) by its local linear approximation \( f(_{t})+}_{t}(}_{t}-_{t})\). Moreover, (5) is equivalent to solving the following linear system of equations \((+_{t}}_{t})(-_{t})=-_ {t} f(_{t})\). For ease of presentation, we set \(}_{t}\) as the exact solution of this system, leading to

\[}_{t}=_{t}-_{t}(+_{t}}_{t})^{-1} f(_{t}).\] (6)

However, we note that an inexact solution to this linear system is also sufficient for our convergence guarantees so long as \(\|(+_{t}}_{t})(}_{t}-_{t})+_{t} f(_{t})\|\|}_{t}- _{t}\|\); We refer the reader to Appendix A.3 for details. Additionally, since we employed a linear approximation to determine the mid-point \(}_{t}\), the condition in (3) may no longer be satisfied. Consequently, it is crucial to verify the accuracy of our approximation after selecting \(}_{t}\). To achieve this, we implement a line-search scheme to ensure that the step size is not large and the linear approximation error is small.

Next, we discuss constructing the stochastic Hessian approximation \(}_{t}\). A simple strategy is using \(}(_{t})\) instead of \(^{2}f(_{t})\), but the Hessian noise would lead to a highly inaccurate approximation of the prox operator, ruining the superlinear convergence rate. To reduce Hessian noise, we follow  and use an averaged Hessian estimate \(}(_{t})\). We consider two schemes: (i) uniform averaging; (ii) non-uniform averaging with general weights. In the first case, \(}_{t}=_{i=0}^{t}}(_{t})\)uniformly averages past stochastic Hessian approximations. Motivated by the central limit theorem for martingale differences, we expect \(}_{t}\) to have smaller variance than \(}(_{t})\). It can be implemented online as \(}_{t}=}_{t-1}+ }(_{t})\), without storing past Hessian estimates. However, \(}(_{t})\) is a _biased_ estimator of \(^{2}f(_{t})\), since it incorporates stale Hessian information. To address the bias-variance trade-off, the second case uses non-uniform averaging to weight recent Hessian estimates more. Given an increasing non-negative weight sequence \(\{w_{t}\}_{t=-1}^{}\) with \(w_{-1}\!=\!0\), the running average is:

\[}_{t}=}{w_{t}}}_{t-1}+1- }{w_{t}}}(_{t}).\] (7)

Equivalently, with \(z_{i,t}=-w_{i-1}}{w_{t}}\), \(}_{t}\) can be written as \(_{i=0}^{t}z_{i,t}}(_{i})\). We discuss uniform averaging in Section 4 and non-uniform averaging in Section 5.

Building on the discussion thus far, we are ready to integrate all the components and present our Stochastic Newton Proximal Extragradient (SNPE) method. The steps of SNPE are summarized in Algorithm 1. Each iteration of our SNPE method includes two stages. In the first stage, starting with the current point \(_{t}\), we first query the noisy Hessian oracle and compute the averaged stochastic Hessian \(}_{t}\) from (7), as stated in Step 4. Then given the gradient \( f(_{t})\), the Hessian approximation \(}_{t}\), and an initial trial step size \(_{t}\), we employ a backtracking line search to obtain \(_{t}\) and \(}_{t}\), as stated in Step 6 of Algorithm 1. Specifically, in this step, we set \(_{t}_{t}\) and compute \(}_{t}\) as suggestedin (6). If \(}_{t}\) and its corresponding step size \(_{t}\) satisfy (3), meaning the linear approximation error is small, then the step size \(_{t}\) and the mid-point \(}_{t}\) are accepted and we proceed to the second stage of SNPE. If not, we backtrack the step size \(_{t}\) and try a smaller step size \(_{t}\), where \((0,1)\) is a user-specified parameter. We repeat the process until the condition in (3) is satisfied. The details of the backtracking line search scheme are summarized in Subroutine 1. After completing the first stage and obtaining the pair \((_{t},}_{t})\), we proceed to the extragradient step and follow the update in (4), as in Step 7 of Algorithm 1. Finally, before moving to the next time index, we follow a warm-start strategy and set the next initial trial step size \(_{t+1}\) as \(_{t}/\), as shown in Step 8 of Algorithm 1.

_Remark 3_.: Similar to the analysis in , we can show that the total number of line search steps after \(t\) iterations can be bounded by \(2t-1+(}{_{t-1}})\). Moreover, when \(t\) is large enough, on average the line search requires 2 steps per iteration. We defer the details to Appendix A.4.

_Remark 4_.: Our motivation behind the choice \(_{t+1}=_{t}/\) is to allow the step size to grow, which is necessary for achieving a superlinear convergence rate. Specifically, as shown in Proposition 1 below, we require the step size \(_{t}\) to go to infinity to ensure that \(_{t}_{t+1}-^{*}\|}{\|_{t}- ^{*}\|}=0\). Note that this would not be possible if we simply set \(_{t+1}=_{t}\), since it would automatically result in \(_{t+1}_{t+1}_{t}\). Moreover, this condition \(_{t+1}=_{t}/\) is explicitly utilized in Lemmas 8 and 16 in the Appendix, where we demonstrate that \(_{t}\) can be lower bounded by the minimum of \(_{0}/^{t}\) and another term. We should also note that this more aggressive choice of the initial step size at each round could potentially increase the number of backtracking steps. However, as mentioned above, this does not cause a significant issue, since the average number of backtracking steps per iteration can be bounded by a constant close to 2.

### Key properties of SNPE

This section outlines key properties of SNPE, applied in Sections 4 and 5 to determine its convergence rates. The first result reveals the connection between SNPE's convergence rate and the step size \(_{t}\).

**Proposition 1**.: _Let \(\{_{t}\}_{t 0}\) and \(\{}_{t}\}_{t 0}\) be the iterates generated by Algorithm 1. Then for any \(t 0\), we have \(\|_{t+1}-^{*}\|^{2}\|_{t}-^{*}\|^ {2}(1+2_{t})^{-1}\)._

Proposition 1 guarantees that the distance to the optimal solution is monotonically decreasing, and it shows a larger step size implies faster convergence. Hence, we need to provide an explicit lower bound on the step size. This task is accomplished in the next lemma. For ease of notation, we let \(\) be the set of iteration indices where the line search scheme backtracks, i.e., \(\{t:_{t}<_{t}\}\). Moreover, we use \(()\) and \(()\) to denote the gradient \( f()\) and the Hessian \(^{2}f()\), respectively.

**Lemma 1**.: _For \(t\), we have \(_{t}=_{t}\). For \(t\), let \(_{t}=_{t}/\) and \(}_{t}=_{t}-_{t}(+_{t}}_{t})^{-1} f(_{t})\). Then, \(\|}_{t}-_{t}\|\| }_{t}-_{t}\|\). Moreover,_

\[_{t}}_{t}-_{t}\|}{\|(}_{t})-(_{t})- }_{t}(}_{t}-_{t})\|},\|}_{t}-_{t}\|^{2}}{\| (}_{t})-(_{t})-}_{t}( }_{t}-_{t})\|^{2}}}.\]

As Lemma 1 demonstrates, in the first case where \(t\), we have \(_{t}=_{t}\). Moreover, since we set \(_{t}=_{t-1}/\) for \(t 1\), in this case the step size will increase by a factor of \(1/\). In the second case that \(t\), our lower bound on the step size \(_{t}\) depends inversely on the normalized approximation error \(_{t}=(}_{t})-( _{t})-_{t}(}_{t}-_{t})\|}{\|}_{t}-_{t}\|}\). Also, note that \(_{t}\) involves an auxiliary iterate \(}_{t}\) instead of the actual iterate \(}_{t}\) accepted by our line search. We use the first result to relate \(\|}_{t}-_{t}\|\) to \(\|}-_{t}\|\). To shed light on our analysis, we use the triangle inequality and decompose this error into two terms:

\[_{t}\] (8)

The first term in (8) represents the intrinsic error from the linear approximation in the inexact proximal update, while the second term arises from the Hessian approximation error. Using the smoothness properties of \(f\), we can upper bound the first term, as shown in the following lemma.

**Lemma 2**.: _Under Assumptions 2 and 3, we have_

\[(}_{t})-(_{t})- _{t}(}_{t}-_{t})\|}{\|}_ {t}-_{t}\|}\{M_{1},\|_{t}-^{*}\|}{2}}\}.\] (9)

Lemma 2 shows that the linear approximation error is upper bounded by \(M_{1}\). Moreover, the second upper bound is \(O(\|_{t}-^{*}\|)\). Thus, as Algorithm 1 converges to the optimal solution \(^{*}\), the second bound in (9) will become tighter than the first one, and the right hand side approaches zero.

To analyze the second term in (8), we isolate the noise component in our averaged Hessian estimate. Specifically, recall \(}_{t}=_{i=0}^{t}z_{i,t}}_{i}\) and \(}_{i}=_{i}+_{i}\). Thus, we have \(}_{t}=}_{t}+}_{t}\), where \(}_{t}=_{i=0}^{t}z_{i,t}_{i}\) is the aggregated Hessian and \(}_{t}=_{i=0}^{t}z_{i,t}_{i}\) is the aggregated Hessian noise, and it follows from the triangle inequality that \(\|_{t}-}_{t}\|\|_{t}-}_{t}\|+\|}_{t}\|\). We refer to the first part, \(\|_{t}-}_{t}\|\), as the _bias_ of our Hessian estimate, and the second part, \(\|}_{t}\|\), as the _averaged stochastic error_. There is an intrinsic trade-off between the two error terms. For the fastest error concentration, we assign equal weights to all past stochastic Hessian noises, i.e., \(z_{i,t}=1/(t+1)\) for all \(0 i t\), corresponding to the uniform averaging scheme discussed in Section 4. To eliminate bias, we assign all weights to the most recent Hessian matrix \(_{t}\), i.e., \(z_{t,t}=1\) and \(z_{i,t}=0\) for all \(i<t\), but this incurs a large stochastic error. To balance these, we present a weighted averaging scheme in Section 5, gradually assigning more weight to recent stochastic Hessian approximations.

## 4 Analysis of uniform Hessian averaging

In this section, we present the convergence analysis of the uniform Hessian averaging scheme, where \(w_{t}=t+1\). In this case, we have \(}_{t}=_{i=0}^{t}}_{i}\). As discussed in Section 3.1, our main task is to lower bound the step size \(_{t}\), which requires us to control the approximation error \(_{t}\) by analyzing the two error terms in (8). The first term is bounded by Lemma 2, and the second term can be bounded as \(\|_{t}-}_{t}\|\|_{t}-}_{t}\|+\|}_{t}\|\). Next, we establish a bound on \(\|}_{t}\|\), referred to as the Averaged Stochastic Error, and a bound on \(\|_{t}-}_{t}\|\), referred to as the Bias Term.

**Averaged stochastic error.** To control the averaged Hessian noise \(\|}_{t}\|\), we rely on the concentration of sub-exponential martingale difference, as shown in .

**Lemma 3** ([1, Lemma 2]).: _Let \((0,1)\) with \(d/ e\). Then with probability \(1-^{2}/6\), we have \(\|}_{t}\| 8_{E}}\) for any \(t 4(d/)\)._

Lemma 3 shows that, with high probability, the norm of averaged Hessian noise \(\|}_{t}\|\) approaches zero at the rate of \(}(_{E}/)\). As discussed in Section 4.1, this error eventually becomes the dominant factor in the approximation error \(_{t}\) and determines the final superlinear rate of our algorithm.

_Remark 5_.: Our subsequent results are conditioned on the event that the bound on \(\|}_{t}\|\) stated in Lemma 3 is satisfied for all \(t 4(d/)\). Thus, to avoid redundancy, we will omit the "with high probability" qualification in the following discussion.

**Bias.** We proceed to establish an upper bound on \(\|_{t}-}_{t}\|\). The proof can be found in Appendix B.1.

**Lemma 4**.: _If \(}_{t}=_{i=0}^{t}}_{i}\), then \(\|_{t}-}_{t}\|_{i=0}^{t}\| _{t}-_{i}\|\). Moreover, for any \(i 0\), we have \(\|_{t}-_{i}\|\{M_{1},2L_{2}\|_{i}- ^{*}\|\}\)._The analysis of the bias term is more complicated. Specifically, to obtain the best result, we break the sum in Lemma 4 into two parts, \(_{i=0}^{T-1}\|_{t}-_{i}\|\) and \(_{i=T}^{t}\|_{t}-_{i}\|\), where \(\) is an integer to be specified later. The first part corresponds to the bias from stale Hessian information and converges to zero at \((M_{1}/t)\), as shown by the first bound in Lemma 4. The second part is the bias from recent Hessian information when the iterates are near the optimal solution \(^{*}\). Using the second bound in Lemma 4, we show this part contributes less to the total bias and is dominated by the first part. Thus, we can conclude that \(\|_{t}-}_{t}\|=(}{t +1})\).

Based on the previous discussions, it is evident that the terms contributing to the upper bound of \(_{t}\) all converge to zero, albeit at different rates. Furthermore, the linear approximation error and bias term display distinct global and local convergence patterns, depending on the distance \(\|_{t}-^{*}\|\). Hence, this necessitates a multi-phase convergence analysis, which we undertake in the following section.

### Convergence analysis

Similar to , we consider four convergence phases with three transitions points \(_{1}\), \(_{2}\), and \(_{3}\), whose expressions will be specified later. Due to space limitations, in the following we provide an overview of the four phases and relegate the details to Appendix B.

**Warm-up phase \(0 t<_{1}\).** At the beginning of the algorithm, the averaged Hessian estimate is dominated by stochastic noise and provides little useful information for convergence. Thus, there are generally no guarantees on the convergence rate for \(0 t<_{1}\). However, due to the line search scheme, Proposition 1 ensures that the distance to \(^{*}\) is non-increasing, i.e., \(\|_{t+1}-^{*}\|\|_{t}-^{*}\|\) for all \(t 0\). During the warm-up phase, the averaged Hessian noise \(\|}_{t}\|\), which contributes most to the approximation error \(_{t}\), is gradually suppressed. Once the averaged Hessian noise is sufficiently concentrated, Algorithm 1 transitions to the linear convergence phase, denoted by \(_{1}\).

**Linear convergence phase \(_{1} t<_{2}\).** After \(_{1}\) iterations, Algorithm 1 starts converging linearly to the optimal solution \(^{*}\). Moreover, during this phase, all the three errors discussed in Section 3.1 continue to decrease. Specifically, Lemma 2 shows the linear approximation error is bounded by \((\|_{t}-^{*}\|)\), which converges to zero at a linear rate. Furthermore, Lemma 3 implies that the averaged Hessian error \(\|}_{t}\|\) diminishes at a rate of \(}(}{})\). Finally, regarding the bias term, it can be shown \(\|_{t}-}_{t}\|=(}{t })\) following the discussions after Lemma 4. Thus, once all the three errors are sufficiently small, Algorithm 1 moves to the superlinear phase, denoted by \(_{2}\).

**Superlinear phases \(_{2} t<_{3}\) and \(_{3} t<_{4}\).** After \(_{2}\) iterations, Algorithm 1 converges at a superlinear rate. Moreover, the superlinear rate is determined by the averaged noise \(\|}_{t}\|\), which decays at the rate of \(}(}{})\), and the bias of our averaged Hessian estimate \(}_{t}\), which decays at the rate of \((}{t})\). Hence, as the number of iterations \(t\) increases, the averaged noise will dominate and the algorithm transitions from the initial superlinear rate to the final superlinear rate.

We summarize our convergence guarantees in the following theorem and the proofs are in Appendix B.

**Theorem 1**.: _Suppose Assumptions 1-5 hold and the weights for Hessian averaging in SNPE are uniform, and define \(C:=}}+5\). Then, the followings hold:_

* _Warm-up phase:_ _If_ \(0 t<_{1}\)_, then_ \(\|_{t+1}-^{*}\|\|_{t}-^{*}\|\)_, where_ \(_{1}=}(}{^{2}})\)_._
* _Linear convergence phase:_ _, where_ \(_{2}=}(\{}{}+^{2 },^{2}\})=}(^{2}+^{2})\)_._
* _Initial superlinear phase:_ _For_ \(_{2} t<_{3}\)_, we have_ \(\|_{t+1}-^{*}\| C_{t}^{(1)}\|_{t}-^{*}\|\)_, where_ \(_{t}^{(1)}=}{(t+1)}=}(/+^{2}}{t})\) _with_ \(\) _defined in (_28_) and_ \(_{3}=}(/+^{2})^{2}} {^{2}})\)_._
* _Final superlinear phase:_ _Finally, for_ \(t_{3}\)_, we have_ \(\|_{t+1}-^{*}\| C_{t}^{(2)}\|_{t}-^{*}\|\)_, where_ \(_{t}^{(2)}=}{}}=}\)_._

**Comparison with **. As shown in Table 1, our method in Algorithm 1 with uniform averaging achieves the same final superlinear convergence rate as the stochastic Newton method in . However, it transitions to the linear and superlinear phases much earlier. Specifically, the initial transition point \(_{1}\) is improved by a factor of \(^{2}\), and our linear rate in Lemma 6 is faster. This reduces the iterations needed to reach the local neighborhood, cutting the time to reach \(_{2}\) and \(_{3}\) by factors of \(\) and \(^{2}\).

## 5 Analysis of weighted Hessian averaging

Previously, we showed Algorithm 1 with uniform averaging eventually achieves superlinear convergence. However, as per Theorem 1, it requires \(}(}{^{2}})\) iterations to reach this rate. To achieve a faster transition, we follow  and use Hessian averaging with a general weight sequence \(\{w_{t}\}\). We show this method also outperforms the stochastic Newton method in . Specifically, we set \(w_{t}=w(t)\) for all integer \(t 0\), where \(w():\) satisfies certain regularity conditions as in [1, Assumption 3].

**Assumption 6**.: _(i) \(w()\) is twice differentiable; (ii) \(w(-1)=0\), \(w(t)>0\), \( t 0\); (iii) \(w^{}(-1) 0\); (iv) \(w^{}(t) 0\), \( t-1\); (v) \(\{,(t+1)}{w^{}(t)}\}\), \( t 0\) for some \( 1\)._

Choosing \(w(t)=t^{p}\) for any \(p 1\) satisfies Assumption 6. Additionally, as discussed in , a suitable choice is \(w(t)=(t+1)^{(t+4)}\), allowing us to achieve the optimal transition to the superlinear rate. Since the analysis in this section closely resembles that in Section 4 on uniform averaging, we will only present the final result here for brevity. The four stages of convergence are detailed in the following theorem, with intermediate lemmas and proofs in the appendix. To simplify our bounds, we report results for non-uniform averaging with \(w(t)=(t+1)^{(t+4)}\).

**Theorem 2**.: _Suppose Assumptions 1-5 hold and the weights for Hessian averaging in SNPE are defined as \(w(t)=(t+1)^{(t+4)}\), and define \(C^{}:=()}}+})\). Then, the following hold:_

1. _Warm-up phase:_ _If_ \(0 t<_{1}\)_, then_ \(\|_{t+1}-^{*}\|\|_{t}-^{*}\|\)_, where_ \(_{1}=}(}{^{2}})\)_._
2. _Linear convergence phase:_ _If_ \(_{1} t<_{2}\)_, then_ \(\|_{t+1}-^{*}\|^{2}\|_{t}-^{*}\|^ {2}(1+)^{-1},\) _where_ \(_{2}=}(\{}{^{2}}+ ,^{2}\})=}(^{2}+)\)_._
3. _Initial superlinear phase:_ _If_ \(_{2} t<_{3}\)_, then_ \(\|_{t+1}-^{*}\| C^{}_{t}^{(1)}\|_{t}-^{*}\|\)_, where_ \(_{t}^{(1)}\!=\!()}{ w(t )}\!=\!}^{(^{2}+)^{( ^{2}+)}/t^{}}\) _with_ \(\) _defined in (_49_) and_ \(_{3}\!=\!(^{2}+)\)_._
4. _Final superlinear phase:_ _Finally, if_ \(t_{3}\)_, then_ \(\|_{t+1}-^{*}\| C^{}_{t}^{(2)}\|_{t}-^{*}\|\)_, where_ \(_{t}^{(2)}\!=\!}{}(t)(d)}{w(t)}}\!=\!(})\)_._

In the weighted averaging case, similar to the uniform averaging scenario, we observe four distinct phases of convergence. The warm-up phase for SNPE, during which the distance to the optimal solution does not increase, has the same duration as in the uniform averaging case but is shorter than the warm-up phase for the stochastic Newton method in  by a factor of \(1/^{2}\). The linear convergence rates of both uniform and weighted Hessian averaging methods are \(1-^{-1}\), improving over the \(1-^{-2}\) rate achieved by the stochastic Newton method in . The number of iterations to reach the initial superlinear phase is \(}(^{2}+)\), smaller than the \(}(^{2})\) needed for uniform averaging in SNPE when we focus on the regime where \(=()\). The non-uniform averaging method in  requires \(^{2}\) iterations to achieve the initial superlinear phase, whereas the non-uniform SNPE achieves an initial superlinear rate of \((^{()+1}/t)^{t}\), improving over the rate of \((^{4()+1}/t^{(t)})^{t}\) in . Finally, while the ultimate superlinear rates in all cases are comparable at approximately \(}((1/)^{t})\), the non-uniform version of SNPE requires \(}(^{2}+)\) iterations to attain this fast rate, whereas 's non-uniform version requires \(}(^{2})\) iterations, which is less favorable.

_Remark 6_.: As discussed in [1, Example 1], with a subsampling size of \(s\), we have \(=}(/s)\) for subsampled Newton. This implies that when \(s=()\), we achieve \(=()\).

## 6 Discussion and complexity comparison

In this section, we compare the complexity of our method with accelerated gradient descent (AGD), damped Newton, and stochastic Newton  methods. The iteration complexities of these methods are summarized in Table 2, which we use to establish their overall complexities. Note that since both stochastic Newton and our proposed SNPE method achieve superlinear convergence, their complexities depend on the target accuracy \(\) in the form \(()\), which is provably better than the complexity of AGD by at least a factor of \((^{-1})\). Further details are provided in Appendix D.1. To better compare them, we focus on the finite-sum problem with \(n\) functions: \(_{x^{d}}_{i=1}^{n}f_{i}(x)\). Let \(\) be the target accuracy, \(\) the condition number, and \(\) the noise level in Assumption 4. In this case, computing the exact gradient and Hessian costs \((nd)\) and \((nd^{2})\), respectively. Thus, the per-iteration cost for AGD is \((nd)\). Each iteration of damped Newton's method requires computing the full Hessian and solving a linear system, resulting in a total per-iteration cost of \((nd^{2}+d^{3})\). For both stochastic Newton in  and our SNPE method, the per-iteration cost depends on how the stochastic Hessian is constructed. For example, Subsampled Newton constructs the Hessian estimate with a cost of \((sd^{2})\), where \(s\) denotes the sample size. Newton Sketch has a similar computation cost (see ). Additionally, it takes \((nd)\) to compute the full gradient and \((d^{3})\) to solve the linear system. Since the sample/sketch size \(s\) is typically chosen as \(s=(d)\), the total per-iteration cost is \((nd+d^{3})\).

* Compared to AGD, SNPE achieves better iteration complexity. Specifically, when the noise level \(\) and target accuracy \(\) are relatively small (\(=()\) and \(=()\)), SNPE converges in fewer iterations. Additionally, when \(n d^{2}\) (indicating many samples), the per-iteration costs of both methods are \((nd)\), giving our method a better overall complexity.
* Compared to damped Newton's method, our method's iteration complexity depends better on the condition number \(\), while damped Newton's depends better on \(\). However, when \(n d^{2}\), the per-iteration cost of damped Newton is \((nd^{2})\), significantly more than our method's \((nd)\).
* Compared to the stochastic Newton method in , the per-iteration costs of both methods are similar. However, Table 2 shows that our iteration complexity is strictly better. Specifically, when the noise level is relatively small compared to the condition number, i.e., \(=()\), the complexity of SNPE improves by an additional factor of \(\) over the stochastic Newton method.

## 7 Numerical experiments

While our focus is on improving theoretical guarantees, we also provide simple experiments to showcase our method's improvements. All simulations are implemented on a Windows PC with an AMD processor and 16GB Memory. We consider minimizing the regularized log-sum-exp objective \(f(x)=(_{i=1}^{n}(_{i}^{}-b_{i}} {}))+\|\|^{2}\), a common test function for second-order methods [23; 24] due to its high ill-conditioning. Here, \(>0\) is a regularization parameter, \(>0\) is a smoothing parameter, and the entries of the vectors \(_{1},,_{n}^{d}\) and \(^{d}\) are randomly generated from the standard normal distribution and the uniform distribution over \(\), respectively. In our experiments, the regularization parameter \(\) is 10\({}^{-3}\), the dimension \(d\) is 500, and the number of samples \(n\) is chosen from 50,000, 10,000, and 150,000, respectively.

We compare our SNPE method with the stochastic Newton method in , using both uniform Hessian averaging (Section 4) and weighted averaging (Section 5). In addition, we evaluate it against accelerated gradient descent (AGD), damped Newton's method, and Newton Proximal Extragradient (NPE), which corresponds to our SNPE method with the exact Hessian. For the stochastic Hessian estimate, we use a subsampling strategy with a subsampling size of \(s=500\). Empirically, we found that the extragradient step (Line 7 in Algorithm 1) tends to slow down convergence. To address this, we consider a variant of our method without the extragradient step, modifying Line 7 to \(_{k+1}=}_{k}\). In Figure 3 of the Appendix, we further explore the effect of the extragradient step. We also note that similar observations were made in , where the simple iteration \(_{t+1}=_{t}-_{t}(+_{t}_{t})^{ -1}_{t}\) outperformed "accelerated" second-order methods. From Figures 1 and 2, we have the following observations:

   Methods & AGD & Damped Newton & Stochastic Newton  & SNPE (**Ours**) \\  Iteration & \(((1/))\) & \((^{2}+(1/))\) & \((^{2}+^{2}+)\) & \((^{2}++)\) \\   

Table 2: Comparisons in terms of overall iteration complexity to find an \(\)-accurate solution.

**Comparison with stochastic Newton.** In all cases, our SNPE method outperforms stochastic Newton in both the number of iterations and runtime, due to the problem's highly ill-conditioned nature and our method's better dependence on the condition number.

**Comparison with AGD.** From Figure 1, we observe that our SNPE method, with either uniform or weighted averaging, requires far fewer iterations to converge than AGD due to the use of second-order information. Consequently, while SNPE has a higher per-iteration cost than AGD, it converges faster overall in terms of runtime, as demonstrated in Figure 2.

**Comparison with the damped Newton's method and NPE**. As expected, since both damped Newton and NPE use exact Hessian, Figure 1 shows that they exhibit superlinear convergence and converge in fewer iterations than the other algorithms. However, since the exact Hessian matrix is expensive to compute, they incur a high per-iteration computational cost and overall take more time than our proposed SNPE method to converge (see Figure 2). Moreover, the gap between these two methods and SNPE widens as the number of samples \(n\) increases, demonstrating the advantage of our method in the large data regime.

## 8 Conclusions and limitations

We introduced a stochastic variant of the Newton Proximal Extragradient method (SNPE) for minimizing a strongly convex and smooth function with access to a noisy Hessian. Our contributions include establishing convergence guarantees under two Hessian averaging schemes: uniform and non-uniform. We characterized the computational complexity in both cases and demonstrated that SNPE outperforms the best-known results for the considered problem. A limitation of our theory is the assumption of strong convexity. Extending the theory to the convex setting would make it more general. This extension is left for future work due to space limitations.