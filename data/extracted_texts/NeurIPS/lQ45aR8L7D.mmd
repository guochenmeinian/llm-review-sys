# Order-Independence Without Fine Tuning

Reid McIlroy-Young

Department of Computer Science

Harvard University

&Katrina Brown

Department of Computer Science

Harvard University

Conlan Olson

Department of Computer Science

Columbia University

&Linjun Zhang

Department of Statistics

Rutgers University

Cynthia Dwork

Department of Computer Science

Harvard University

Corresponding author reidmcy@seas.harvard.edu

###### Abstract

The development of generative language models that can create long and coherent textual outputs via autoregression has lead to a proliferation of uses and a corresponding sweep of analyses as researches work to determine the limitations of this new paradigm. Unlike humans, these '_Large Language Models_' (LLMs) are highly sensitive to small changes in their inputs, leading to unwanted inconsistency in their behavior. One problematic inconsistency when LLMs are used to answer multiple-choice questions or analyze multiple inputs is _order dependency_: the output of an LLM can (and often does) change significantly when sub-sequences are swapped, despite both orderings being semantically identical. In this paper we present **Set-Based Prompting**, a technique that _guarantees_ the output of an LLM will not have order dependence on a specified set of sub-sequences. We show that this method _provably_ eliminates order dependency, and that it can be applied to _any_ transformer-based LLM to enable text generation that is unaffected by re-orderings. Delving into the implications of our method, we show that, despite our inputs being out of distribution, the impact on expected accuracy is small, where the expectation is over the order of uniformly chosen shuffling of the candidate responses, and usually significantly less in practice. Thus, Set-Based Prompting can be used as a '_dropped-in_' method on fully trained models. Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.

Code is available at github.com/reidmcy/set-based-prompting.

## 1 Introduction

Recent advances in machine learning have led to a paradigm shift, as new training methods, which rely on massive scale datasets to create general purpose 'base models', outperform specialized single purpose models (Achiam et al., 2023; Radford et al., 2019). A particularly notable development is in natural language processing, where self-supervised learning has led to a cornucopia of transformer based models that can string together multiple nexttoken predictions, via autoregression, to generate coherent text that approximates a human response. These generated responses are not human and exhibit non-human behavioral quirks due to the limitations of the transformer based LLM architecture. Some of these, like the inability to reason about letters, are due to the choice of tokenization (the process of converting text into a sequence of numbers that can be used as an input) which breaks words into multiple character chunks (McCoy et al., 2023), while others like the _Lost Middle Phenomenon_ are limitations of the training data (N. F. Liu et al., 2024). The solutions to these limitations are known (An et al., 2024), although often impractical (Sutton, 2019).

There are other limitations of LLMs that appear to be fundamental to the design of LLMs. One pernicious issue is the _order dependency problem_. This is the phenomenon where the output of an LLM can be significantly altered by changing the order of inputs, even when the re-ordering should have an identical response; see figure 1 for an example. This _order dependency problem_ is well studied for multiple choice questions (Pezeshkpour et al., 2023; Zheng et al., 2024), but can happen on any task.

Having systems that make or aid in decision making that are sensitive to factors of the input that are orthogonal to the goals of the operators presents significant concerns. Consider an LLM that reads medical papers and summarizes them to a doctor (Van Veen et al., 2024). If shuffling the papers changes the summary can the doctor trust the model's responses? Additionally, _order dependency_ presents algorithmic fairness concerns, such as in a case where an LLM being used to compare candidates has a slight bias for the first candidate (Adian Liusie, 2024; Li et al., 2023).

In this paper we present a solution to the order dependency problem. Our method **Set-Based Prompting** solves this problem by removing the ordering information from the inputs to the LLM, for a specified set of _sub-sequences_. We refer to this as _prompting_ to emphasize that our method does not modify the underlying model, it only changes the input. These sub-sequences of multiple tokens are (from the model's perspective) run in parallel, leading to the output being agnostic as to the ordering of the sub-sequences2. We show that our method guarantees the parallel treatment of sub-sequences for arbitrary transformer based LLMs. We then test it on a variety of models on multiple choice question tasks and show that while parallel sub-sequences can impact performance, the impact is usually within that caused by re-ordering the sub-sequences when run without our method. We also discuss how the success of Set-Based Prompting suggests many future potential contributions.

Figure 1: Illustration of order dependency in Llama 2, 7B. Using the order provided by (Measuring Massive Multitask Language Understanding) (MMLU) (Hendrycks et al., 2020) Llama 2 gets the question correct as seen in a), but if the order is reversed for the questions Llama 2, 7B predictions the wrong answer. In c) we use Set-Based Prompting to remove the ordering of the answers and Llama 2, 7B once again gets the question correct.

Related Works

Multiple choice questions (MCQs) serve as an important task format in the evaluation of large language models (LLMs). Previous work has shown that modern LLMs are vulnerable to re-ordering of options in MCQs. Pezeshkpour et al., 2023 identify inherent 'positional bias' in the MCQ setting, where LLMs prefer to select options in specific positions as answers, and propose reordering the options to position the most likely candidates in specific locations within the sub-sequence. This is viable in the case where both the prior positional bias and the correct answers are known a priori, but not necessarily in more general settings. Zheng et al., 2024 identifies a slightly different problem of 'token bias', wherein LLMs prefer to select specific option IDs A/B/C/D as answers. They propose estimating prior token bias via a label-free, inference-time debiasing method, which separates the model's prior bias for option IDs from the overall prediction distribution. Adian Liusie, 2024 likewise identifies positional bias in the setting of pairwise comparisons of natural language generation, and suggests debiasing using estimates of prior positional bias derived from observing comparison outcomes for a subset of candidates. These methods all assume the existence of a set of comparable samples with which to estimate prior ordering biases. Other work looking at how modifying word choice affects the model's outputs (Salewski et al., 2024) show that this issue extends beyond order-dependency, but this matches how humans answer questions (Tjuatja et al., 2023) so should be less unintuitive for non-experts.

There is also an example of this order-dependence being used to probe the training data of LLMs. Oren et al., 2023 shows that the ordering of questions in the training data affects the ordering during inference, allowing for detection of training data contamination.

One implication of this MCQ order-dependence is that it makes the relative comparisons of the performance of different models on benchmarks such as MMLU less reliable, as different evaluation runs may employ different MCQ orderings within the test sets (Alzahrani et al., 2024).

## 3 Set-Based Prompting

To provably ensure order-independence we introduce **Set-Based Prompting**, an elegant 2-pronged modification to the model at inference time touching, respectively, the attention mask and the positional encoding. As these areas are not usually considered to be variable, this pushes the model slightly out of distribution. Furthermore, we hypothesize that some output quality degradation may occur since in the order-independent case there is both less computation and less information, as no tokens in the set of parallel sequences can attend to tokens in other parallel sequences. However, we hypothesize that base models are robust to minor changes, and output coherence will not be noticeably impaired. In fact, to see the impact of our methods requires many queries. We begin with a review of the attention mechanism's attention mask and the positional encoding system used in transformer-based LLMs. Then we will show how we can use a _non-triangular_ mask combined, together with a modified positional encoding -- our two prongs -- to make the attention block unable to determine the order of sub-sequences. This results in completely removing all the ordering information _between_ sub-sequences, making the model's outputs perfectly order invariant.

### Background

Attention Mask.The attention mechanism is a means of taking in an arbitrary number of inputs and _attending_ to them in a weighted manner. This allows for longer sequences to be input and processed into a single output. When processing text, the attention mechanism is used on each input token in turn, creating an embedding for each token separately.

In practice, we say the tokens can see all other tokens in the input, but when generating text the tokens in the input are usually treated as if they are the next to be generated and future tokens are masked output. This is seen in LLMs including GPT-2 (Radford et al., 2019) and Llama(Touvron, Lavril, et al., 2023) with their self-attention mechanism, wherein the outputs for a certain position in a sequence are based only on the known outputs at previous positions and not on future positions. This is implemented by multiplying the inputs by a mask vector that is 0 for all future tokens. When these mask vectors are stacked (for parallel computation), they form a triangular matrix, the attention mask matrix for that input. Figure 2 shows this attention mask for a sequence of 7 tokens.

**Positional Encoding.** Different language models use different methods of representing the position of input tokens within a sequence. The original transformer introduced by Vaswani et al., 2017 proposed absolute positional embeddings, which involves assigning a specific value vector at each time step, or position, and then adding element-wise those values to the token embedding. This is the positional encoding method used in models such as GPT-2.

In contrast, most state of the art LLMs including lama2/3, and PaLM (Anil et al., 2023), use rotary positional embeddings (RoPE) as introduced by Su et al., 2023, which encode absolute positional information with a rotation matrix and naturally incorporate explicit relative position dependency in the self-attention formulation.

### Provable Order-Independence via Set-Based Prompting

We now describe Set-Based Prompting, our technique for modifying the positional encoding to provide parallel (order-free) representation of sequences of text, and prove that it ensures order-independence. Set-Based Prompting can be applied to both absolute and RoPE embeddings, on both GPT-2 and Llama2/3 models. In the absolute embedding case, the same value is added to the token embedding for each token located in the same position in the parallel representation of the input text. Likewise in RoPE, absolute positional information is the same for two tokens in the same "location" within parallel sequences, and a token within a parallel sequence retains relative positional dependency with other tokens within the same parallel sequence.

### Methodology and Theoretical Guarantees

Consider the Attention block with positional encoding (Ferrando et al., 2024; Kobayashi et al., 2021; Su et al., 2024) for single-headed attention3, the attention mechanism maps multiple vectors into a single vector in a differential way. We start with a fixed query vector \(_{i}\) which we give as an input column vector, \(=[_{1},,_{n}]^{d n}\) is the sequence of all \(n\) inputs that will be mapped to a single vector, and \(\) is the attention mask, an \(n n\) lower triangular matrix. The attention operator is defined as follows.

Figure 2: Visualization of the differences between order-dependant prompting (left) and Set-Based Prompting (right). Our input is the prompt ‘_the aptly quick light reddy brown fox_’ and ‘_aptly quick_’ is in parallel to ‘_light reddy brown_’. Each row represents a query to an attention block (we treat each word as a token), with the index of the query given by \(i\). \(\) and \(_{s}\) give the set of values over which the query is attending. \((i,j)\) is the vector-valued positional encoding which is added to the word’s embedding. The center of the diagram is the attention mask \(_{j,i}\).

\[(_{i},,)=_{_{j}}M_{i,j} _{i,j}(_{V}(_{j}+(i,j)))\] (1) \[_{i,j}:=1&j i\\ 0&_{i,j}:=*{softmax}_{ _{j}}(_{i}^{}_{Q}(_{K}( {x}_{j}+(i,j)))}{})\]

where \(_{Q}\) (query), \(_{K}\) (key), and \(_{V}\) (value) are \(d d\) matrices, with \(d\) being the internal dimensions of the matrices. \((i,j)\) is the vector-valued positional encoding for the token at position \(j\) relative to position \(i\); note that if it is absolute (GPT-2) then \(\) is only a function of \(j\), while if relative (RoPE) it is a function of \(i-j\).

Recall that \(_{i}\) is fixed. Equation (1) can therefore be considered to be a weighted sum over all columns of \(\), where the \(j\)th column is weighted by \(_{i,j}\), and where \(\) prevents future tokens from having any weight.

Since addition is _commutative_, the order in which the individual \(_{j}\) are provided is lost, _except_ as provided by \((i,j)\), which includes the order information both for the scaling (\(_{i,j}\)) and the unscaled vector \((_{j}+(i,j))\). The attention mask \(M\) also functionally encodes positional information when attention blocks are applied in series since 'future' \(_{j}\) are masked out and do not contribute to the sum.

Thus, to prove that Set-Based Prompting is order-independent, it suffices to show that the positional encoding \(((i,j))\) and attention mask \(()\) are unaffected by re-ordering by an arbitrary permutation of the parallel sub-sequences. Let \(S\) be the (unordered!) set of parallel sub-sequences, \(S=\{s_{1},,s_{}\}\) with \(_{k=1}^{l}|s_{k}|=n\). We start by defining the indices on each token (subscript) and which sequences the tokens are members of (superscript), with \(^{}\) indicating a token is a member of no sub-sequences. The tokens before the parallel sub-sequences are treated identically as in the normal case, and we denote them by \(_{start}=[_{1}^{},,_{r-1}^{}]\) where \(r\) is the lowest index in the parallel sub-sequences. The tokens after the parallel sub-sequences can be described similarly \(_{end}=[_{p+1}^{},,_{m}^{}]\) where \(p\) is the greatest index in the parallel sub-sequences and \(m\) is the final index. Note that \(m=|_{start}|+(p-r)+|_{end}|\), where \(||\) is the length of a sequence of vectors.

Then, for a parallel sub-sequence \(s_{k}\) we say that \(s_{k}=[_{r}^{k},,_{q}^{k}]\). Note that in our input representation there is no ordering on \(s_{k} S\), because we reset the subscripts (positional indices) at the beginning of each sub-sequence. We have simply indexed the sub-sequences for convenience.

By writing it this way we can directly input it into the \((_{i},_{s},)\) function with \(_{s}=_{start} s_{1} s_{}_{end}\), where \(\) is concatenation. If we do this we will obtain order independence since \((i,j)\) and \(_{i,j}\) only use the indices of the tokens, so they are unaffected by ordering of the sub-sequences.

To show how \((i,j)\) and \(_{i,j}\) (the unmodified \(\)) are order independent we need to consider the three possible cases for the input vector \(_{i}\): 1) \(_{i}\) is before the sub-sequences, 2) \(_{i}\) is in the sub-sequences, and 3) \(_{i}\) is after the sub-sequences. The **first case** is straightforward since \(_{i}^{}_{start}\). The output is unaffected by the ordering of \(S\) as all sub-sequence tokens are masked out by \(_{i,j}\); \(i<r\).

The **second case** is where \(_{i}^{k} s_{k}\) with \(r i p\). In this case if we naively evaluate \((_{i}^{k},_{s},)\) we will get a result that is unaffected by the ordering of \(S\), but the activations will be further from the training distribution than necessary, resulting in lowered performance as discussed in Section 4.3. Note that for any \(_{j}^{k}_{s}\), \((i,j)\) and \((i,j)\) are only affected by the positional index \(j\) (instead of \(k\)).

Finally, in the **third case** where \(_{i}^{}_{end}\), the same argument as the second case applies. The input vector \(_{i}^{}\) can'see' all previous tokens, but their positional indexing is unchanged under re-orderings of \(S\). Thus in all three cases we have sub-sequence order independence.

#### 3.3.1 Attention Mask

While the above method is sufficient for sub-sequence order independence, if used in practice the generated representations encountered by the learned weight matrices (\(_{V}\) and (\(_{K}\)) will be far from the training distribution in both case 2 and case 3 encounter. In case 2 this is due to input vectors attending to all sub-sequences' inputs, _e.g_, if there were three sub-sequences starting at index \(i=2\) the first token of sub-sequence 1 (\(_{2}^{1}\)) would have non-zero weight on 4 tokens, three of which would have the positional encoding of 2. In training, the model would never see multiple weights with the same positional encoding particularly multiple positional encodings that match the input token. Note that when used in an LLM ATTN() is applied sequentially, so even minor errors can be amplified. For case 3, the results are less severe since the positional encoding multiplicity only occurs for a set of previous tokens, it does not occur for the input token.

We can mitigate some of the issues by modifying the attention mask \((i,j)\), making it so that case 2 tokens do not encounter out of distribution inputs. To do this we define a new attention mask \(_{i,j}^{k,f}\) that also takes in the sub-sequences index each along with positional index of each token (\(_{i}^{k}\), \(_{j}^{f}\), etc), while still retaining the sub-sequence order independence we want. With this notation, when \(k=f\) we have that \(_{i}^{k}\) and \(_{j}^{f}\) are in the same parallel sub-sequence.

\[_{i,j}^{k,f}=_{i,j}&\\ _{i,j}&\\ _{i,j}&\\ 0&=1&\\ 1&\\ 1&\\ 0&\] (2)

Equation 2 still maintains order independence since it only differs from \(_{i,j}\) when \(i j\) and in that case does not depend on the values of \(i\) or \(j\).

If we consider ATTN(\(_{i}^{k},_{s},_{i,j}^{k,f}\)) we still retain order independence because ATTN(\(_{i}^{k},_{s},_{i,j}^{k,f}\)) does not change under re-orderings of the labels, it only considers equality or absence of labels. See Section 4.3 for results when we don't do this masking.

This modified attention mask means that only tokens in case 3 see out of training distribution representations, and that no tokens will'see' multiple other tokens at the same index.

This yields the following theorem.

**Theorem 1**: _Given \(_{i,j}^{k,f}\) as in Equation (2), fix any permutation function \(\) on the indices \(1,,\) of the sub-queries \(S=\{s_{1},,s_{k},,s_{}\}\) for the attention mechanism, so that applying \(\) to the blocks of column vectors corresponding to the \(\)-th parallel sub-sequences transforms \(_{s}=_{start}\{[_{1}^{1},...],,[_{1}^{k},...],,[_{1}^{},...]\}_{end}\) to \(_{s}^{}=_{start}\{[_{1}^{(1)},...],,[ _{1}^{(k)},...],,[_{1}^{()},...]\}_{end}\). Then_

\[(_{i}^{k},_{s},_{i,j}^{k,f})=(_ {i}^{(k)},_{s}^{},_{i,j}^{k,f})\] (3)

_and_

\[(_{i}^{},_{s},_{i,j}^{k,f})=( _{i}^{},_{s}^{},_{i,j}^{k,f}).\] (4)

## 4 Performance

While Set-Based Prompting is guaranteed to produce order independent results, we still need to test what other impacts it has on the generated text. To this end, we collected four different LLM families (GPT-2(Radford et al., 2019), Llama 2(Touvron, Martin, et al., 2023),Llama 3(AI@Meta, 2024), and Mistral(Jiang et al., 2023)) to conduct our experiments (see figure 1 for the list of models). Due to our method modifying the internals of the model, each family requires a custom implementation of the inference code, meaning we can test all models in a family, but adding new families is time consuming. Our implementation for inputting parallel sub-sequences is implemented like the special tokens already commonly used by most LLMs, with three special 'tokens' (start-parallel, new-sub-sequence, and end-parallel) that are removed during inference, allowing for Set-Based Prompting to be added in existing workflows directly.

For testing we used two standard test sets CommonsenseQA (CSQA) (Talmor et al., 2019) and (Measuring Massive Multitask Language Understanding) (MMLU) (Hendrycks et al., 2020); these were chosen because they are multiple choice question sets that allow us to use the multiple options as the parallel sub-sequences.

As we want the runs with and without Set-Based Prompting to be similar we modified the prompting for both datasets. Instead of using numbers or letters before each option we quote the options in double quotes (") and separate them with a space. This allows for the parallel queries to have the exact same set of tokens as the non-parallel queries. Of note, we implement the normal 1-shot evaluation for these tests that looks at the probability of each answer being generated, and we use the greatest value as the selected answer. We do observe that this slightly reduces the accuracy of the models from prompting with numbers or letters, so we label the methods'modified CSQA' and'modified MMLU' to differentiate our results. As we are concerned with the relative performance of our method, not in benchmarking the models, we believe this gives us results that are close to the existing literature.

### CommonSenseQA

As we are concerned with reducing the variation of models under reordering, we examine both the accuracy under the default ordering, and the potential accuracy under reordering. In Figure 2(a) we examine our model's accuracy on CSQA, both with and without Set-Based Prompting. For the order dependent results we run CSQA twice and divide the answers into 4 sets (as in a confusion matrix): questions that both orderings get correct are counted and added to the Worst of 2 count, while questions that only one gets correct are also added to the Best of 2 bar, with the ones the normal ordering got correct used for Best of 1. The counts are then divided by the total number of questions (9741) to give the mean accuracy. Thus, the range of the blue bars can be considered to be the possible range of accuracies under two possible orderings per question. Note that if the models understood the text the difference between orderings would be minuscule; the difference between just two orderings being large shows that the specific models are fundamentally incapable of solving multiple choice questions, they merely approximate a good response in expectation.

Figure 3: Per model accuracy on two different datasets, blue bars (left three) indicate runs done _without_ our method and green with Set-Based Prompting. The blue bars are constructed by running the test twice, once with the normal ordering and once with the reversed ordering. Worst of 2 and Best of 2 count when both orderings lead to an correct answer or only one ordering answered correctly, respectively. While Best of 1 indicates that the normal ordering led to correct answers. As Set-Based Prompting is invariant to reordering so we only show one bar for all orderings.

When we examine our method we see (1) that all ordering produce the same result, and (2) that the accuracy is within that of the variation of order dependent queries for all models except Llama 3, 8B Instruct. This result suggest that Set-Based Prompting is not adding additional information into the model, instead it is removing a dimension of variation. We discuss some hypotheses for how our method impacts the response generation in section 5.

### Measuring Massive Multitask Language Understanding

Figure 2(b) shows the same analysis as in section 4.1 but for MMLU (Hendrycks et al., 2020). We see similar results to CSQA, but with lower accuracy on the more complex MMLU.

To further explore the impact of reordering on outputs we ran all possible permutations of MMLU option orderings through a subset of our models. In Figure 4 we see that for all Llama 2 models Set-Based Prompting is above or within the inter-quartile range of the 24 samples. We do see that in Llama 3 our method is as bad as the worst ordering, but it is still within the range of orderings.

### Ablations

**Partial Implementations.** While we have a proof as to why both the the positional encoding and attention mask need to be modified, the transformer models are large enough that empirical observations are wanted to confirm the theory. We tested variations of Set-Based Prompting where only the attention mask or the positional encoding was modified to mask the positional information. Figure 5 shows the variation between normal ordering and reverse ordering of question options for the first 20 sets of questions in MMLU (ordered alphabetically). Interestingly, modifying the attention mask reduces variation much more than the positional encoding, although the effect varies significantly between models.

**Enumerated Options.** We ran a subset (first 20) of MMLU with the option numbers re-added, this lead to an improvement on all orderings. Note that adding numbers to inputs encoded with Set-Based Prompting implicitly removes the guarantee of order independence as there is an ordering implied by the numbers. See figure 10 in the appendix for this result.

**Chain of Thought.** We implemented a simple chain of thought prompt ("A: Let's think step by step") for the first 20 MMLU question sets. This lets us determine if the effects of Set-Based Prompting are limited to inputs near the end of the input sequence and check if an alternative prompting method would change the results. This method produced a moderate uplift in performance for both order dependent and order independent cases, note that this experiment was run once and not tuned. See figure 11 in the appendix for this result.

### Other Tests

**In Context Learning (Sentiment Analysis).** We implemented a sentiment classification task with in context learning. The model was provided with 4 samples with labels, then asked to classify a fifth. The dataset is _Financial Phrase Bank_ Malo et al., 2014 so all statements are finance related and classification is positive or negative. To look at the effects of ordering on the model performance we always had three samples with the same label and 1 with the other label, with the different label always being first or last. See figure 5(a) for the impacts of different orderings on the final classification task accuracy. Our experiment shows that Set-Based Prompting applied to the examples often improves performance over even the Best of 2 case.

**Long-Context Information Retrieval.** We implemented the long-context information retrieval task described in the paper by _Liu et al._N. F. Liu et al., 2024. To do this, we generated 10 document sequences with the answer hidden in one of them. Then we moved where the answer was located from the first to the last document and observed the retrieval accuracy. We used the same templates and dataset as _Liu et al._ for this. To test the effects of Set-Based Prompting we ran the documents in parallel, either all 10, 5 groups (2,2,2,2), four groups (3,3,3,1) or two groups (5,5). When running the sets of documents in parallel there are two opposing forces affecting the accuracy: (1) parallelism, naturally, reduces order-dependence, which helps accuracy; (2) at the same time, the intervention moves the inputs farther out of distribution, reducing accuracy. Figure 5(b) shows the results of this experiment. Our experiment suggests that limited intervention is a sort of'sweet spot'. The existence of a'sweet spot' suggests that our method can be used to evaluate the robustness of the model's learned manifold, since we now have a subtle measure of model sensitivity to perturbations.

**Extended Context Window.** We checked if our method allows for the context window to be _functionally_ extended since \(n\), the total number of tokens, is less than \(m\), the maximum positional index. Text generated where \(n\) was greater than the context window of the model maintains coherence when run, see the appendix section 6.12 for an example.

## 5 Discussion

Understanding why Set-Based Prompting works at all, without needing additional training, will require further research. We hope that adding Set-Based Prompting during training will reduce the impact on performance. As shown in Section 4, for all tested models Set-Based Prompting does not cause the model performance to degrade to an unusable state, although, as noted in Section 1, order dependency raises significant bias and reliability concerns.

We hypothesize three mechanisms of action for Set-Based Prompting to negatively impact an LLM's performance. First, Set-Based Prompting reduces the total amount of computation, thus reducing the model's ability to 'do work' leading to lower accuracy. Second, Set-BasedPrompting reduces the information available for some tokens, specifically those in the parallel sub-sequence, _i.e.,_ when the LLM is processing the second option it can see the first so can make a more nuanced embedding. Finally, Set-Based Prompting leads to out of distribution token embeddings. The out of distribution impact is suggested by the difference in impact on instruction tuned models, compared to their base models. For example, in Figure 2(b) we see that for Llama 3, Set-Based Prompting performance is almost identical between the instruct and base models, while the instruct performs significantly better on best-of-2. This suggests that Set-Based Prompting is moving the input embeddings such that they are 'outside of' the fine-tuning, but the base model is robust enough to still understand them. These modifications to the model's output may also be mitigated by including examples of Set-Based Prompting inputs during training or fine-tuning, but additional work will be needed to determine this. Notably we also see hints of this when examining the model outputs under our methods; Set-Based Prompting makes fine-tuned models appear closer to the non-fine-tuned models, in particular Set-Based Prompting tends to make the fine-tuned models more verbose. We believe that each of these mechanisms can be tested, allowing for more details to be revealed about the inner workings of language models, and hopefully build towards reducing biases and increasing robustness of these systems.

### Further Explorations

In this paper we present and evaluate a small change to the input representation. Our results suggest that larger changes may not only be possible, but may not require significant training overhead. We chose not to study fine-tuning as we believe that showing that our method is 'drop-in' is a significantly more powerful result than one that requires additional training. A next step would be to evaluate how to fine-tune models to be more receptive to this method, possibly even including the training as a part of base-model construction. Additionally, we chose to study the smallest possible version of this effect, a single parallel sub-sequence per prompt. Further study is merited to determine if more complex sub-sequence relationships can be input without significant degradation. In particular, we hypothesize that a version of this method that treats the input text as a directed graph of tokens (instead of a linear sequence) would be possible, _i.e._, allow nested parallel sub-sequences, supporting cycles will likely require training. This is a representation already used for graphs (Ju et al., 2024; Wang et al., 2020) and is the natural representation for transformers. This type of representation would allow for inputs that much more closely represent how humans are provided text, _e.g._, consider a class's syllabus, it is often a series of unordered sets of articles to read.Developing techniques that allow for reasoning about human behavior to be translatable to LLMs will greatly improve their utility to non-experts.

### Towards Metadata in LLM Inputs

The empirical results in section 4.2 show that, while the positional encoding is used by LLMs, they can nonetheless 'understand' inputs where the positional encoding is modified. This suggests that LLMs are generally robust to minor changes in the token embeddings, with the positional encoding being the only currently implemented example. This is an entirely unexplored area of research. If we consider the positional encoding as a learned type of metadata, then other types of metadata could be added to tokens by simply adding new vectors during training to the token embeddings, just as we do with positional encodings. Adding additional metadata to each token would allow for 'out-of-band' signals that are added to input texts. Consider, for example, tagging each token with a vector that encodes the privilege level of the token, allowing for the implementation of instruction hierarchies (Wallace et al., 2024) that are encoded per token, instead of contextually by tags. Another, example of the utility of adding token metadata is in typography. Many digital texts have **bold**, underlines, _italics_, etc.; each of these could have an associated vector during training, allowing the LLM to'see' the text more clearly. Instruction hierarchies and typography are just two possible uses of complex LLM inputs, and we believe that many more are possible, allowing for LLMs that can interact with the world's details and not just simple linear text.

As the usage of LLMs becomes more systemic, operators will need to be able to encode more complex representations, and have guarantees on the LLMs' behavior. We believe that this work presents a development towards that goal.