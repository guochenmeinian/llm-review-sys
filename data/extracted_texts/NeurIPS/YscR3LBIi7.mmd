# MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence

Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, Zhou Zhao

Zhejiang University

fumyou13@gmail.com

Corresponding Author

###### Abstract

Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains. The interaction between human motion and music is a reflection of advanced human intelligence, and establishing a unified relationship between them is particularly important. However, to date, there has been no work that considers them jointly to explore the modality alignment within. To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation. Firstly, to mitigate the huge computational costs raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent representations for both motion and music inputs. Subsequently, leveraging the aligned latent spaces, we introduce a multi-modal Transformer-based diffusion model and a cross-guidance sampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate that MoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively and quantitatively, and can synthesize realistic, diverse, long-term, and beat-matched music or motion sequences. The generated samples and codes are available at https://momu-diffusion.github.io/.

## 1 Introduction

Dancing to the musical beats or creating a variety of rhythmically synchronized music for a given motion is a fundamental aspect of human creativity. Music and human motions serve as universal languages that are shared by all civilizations, transcending cultural and geographical boundaries around the world . For computational methodologies, the motion-music generation poses several challenges: 1) maintaining long-term coherence in typically lengthy motion-music sequences 2)

Figure 1: The pipeline of MoMu-Diffusion. MoMu-Diffusion integrates the alignment of motion and music through the novel Bidirectional Contrastive Rhythmic Auto-Encoder (BiCoR-VAE). Leveraging the aligned latent space, MoMu-Diffusion facilitates both cross-modal and multi-modal generations.

ensuring temporal synchronization and rhythmic alignment between motion and music sequences, and 3) generating realistic, diverse, and variable-length human motions or music.

Existing works usually divide the motion-music generation into two distinct tasks: motion-to-music and music-to-motion. For motion-to-music, some methods compress the conditional video frames into a single image, in which the temporal information is lost [52; 53]. The state-of-the-art work, LORIS , employs a hierarchical conditional diffusion model to generate long-term musical waveforms. However, LORIS introduces huge computational costs and training difficulties since it generates long-term musical waveforms directly. For music-to-motion, the Dancing2Music (D2M)  framework divides the generation process into two stages: decomposing the dance into basic dancing movements with a VAE and compositing the basic movements into dance with a GAN. Nonetheless, D2M's approach of segmenting long-term music into short clips (approximately 1-2 seconds) diminishes the coherence of the synthesized motion sequences.

Motivated by the fact that human motions are highly associated with music yet existing computational methods often study them in isolation, we propose a novel multi-modal framework, termed MoMu-Diffusion, to address the aforementioned challenges jointly. Firstly, to mitigate the computational costs and optimization complexities raised by long sequences, we employ a VAE to encode both motion and music sequences into latent spaces. Subsequently, to investigate the relationship between human movements and musical beats, we propose rhythmic contrastive learning. This approach involves constructing contrast pairs with a kinematic amplitude indicator, which quantifies the temporal variation in motion and is derived from the spatial motion directrogram differences as detailed in . Given that the motion and music sequences are interactively aligned in the latent space to discern the correlation between kinematic shifts and musical rhythmic beats, we call our model as the Bidirectional Contrastive Rhythmic VAE (BiCoR-VAE).

With the aligned latent space, we introduce a Transformer-based diffusion model that captures long-term dependencies and facilitates sequence generation across variable lengths. Additionally, we introduce a simple cross-guidance sampling strategy that integrates different cross-modal generation models, enabling multi-modal joint generation without extra training. By incorporating the BiCoR-VAE and the diffusion Transformer model, our MoMu-Diffusion framework effectively models the long-term motion-music synchronization and correspondence, enabling motion-to-music, music-to-motion, and joint motion-music generation. Moreover, MoMu-Diffusion supports generating motion-music samples in variable lengths. The pipeline of MoMu-Diffusion is illustrated in Figure 1.

We have conducted extensive experiments on three motion-to-music and two music-to-motion datasets, including scenarios such as dancing and competitive sports. The experimental results demonstrate that MoMu-Diffusion attains state-of-the-art performance across both objective and subjective metrics, significantly enhancing music/motion quality and cross-modal rhythmic/kinematic alignment. Furthermore, we have carried out abundant ablation studies to validate the efficacy of the BiCoR-VAE and the DiT architecture. A comparative analysis with state-of-the-art motion-to-music methods CDCD  and LORIS , 2D music-to-motion method D2M , and general video-to-audio methods Diff-Foley  and MM-Diffusion , is presented in Table 1.

## 2 Related Works

**Neural Motion Synthesis.** Neural motion synthesis is often associated with audio, and we focus on two audio-driven scenarios: music-to-motion generation [12; 27; 35; 26]and co-speech gesture generation [48; 32; 50]. For music-to-motion, some methods [12; 27; 35] propose to retrieve the most related music for the given motion sequence. D2M  is a generative model that designs

   Method & Pub. & Joint Generation & Pretrain & Long-Term Synthesis & Latent Space \\  Diff-Foley & NeurIPS’23 & ✗ & ✓ & ✗ & ✓ \\ MM-Diffusion & CVPR’23 & ✓ & ✗ & ✗ & ✗ \\ LORIS & ICML’23 & ✗ & ✗ & ✓ & ✗ \\ D2M & NeurIPS’19 & ✗ & ✓ & ✗ & ✓ \\ CDCD & ICLR’23 & ✗ & ✗ & ✓ & ✓ \\  MoMu-Diffusion & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation.

a "decomposition-to-composition" method to learn the movement units and generate music from the learned units. Besides, some methods [30; 54; 29; 1] investigate synthesizing 3D motions from music. For co-speech gesture generation, DiffGesture  is a state-of-the-art model with a diffusion transformer architecture and diffusion gesture stabilizer. We study the 2D music-to-motion problem and compare the proposed MoMu-Diffusion with DiffGesture and D2M

**Neural Music Synthesis.** Neural music Synthesis aims to generate melodious music with generative neural networks. Various generative models have been successfully applied to music synthesis such as transformer-based autoregressive models [21; 39; 9], VAE [2; 40; 6], GAN [8; 24; 36], and diffusion models [16; 34]. Some efforts have been made to video-to-music which focuses on the cross-modal temporal alignment. For example, Foley Music  and Audeo  utilize Musical Instrument Digital Interface (MIDI) representations to generate music in a non-regressive manner. D2M-GAN  and CDCD  generate video-related music by compressing the video frames into a single image, in which the temporal information is neglected. LORIS  proposes a hierarchical conditional diffusion model to generate long-term musical waveforms.

**Multi-Modal Contrastive Learning.** Contrastive has been demonstrated effective in For example, Elizalde et al.  proposed Contrastive Language-Audio Pretraining (CLAP) to learn a unified latent representation for an audio or text input, facilitating the birth of text-to-audio models [31; 20]. For audio-visual generative tasks, DiffFoley  uses semantic and temporal contrastive learning to promote video-to-audio generation. In this paper, to improve the efficiency and generalization ability of our generative model, we propose the first motion-music pretraining model with a well-designed contrastive loss to learn beat synchronization and rhythm correspondence.

## 3 Bidirectional Contrastive Rhythmic VAE (BiCoR-VAE)

### Multi-Modality Model Architecture

**Motion Variational Auto-Encoder.** Let \(n^{T_{m} J 2}\) be the 2D motion keypoints extracted from the corresponding video, where \(T_{m}\) is the motion frames, \(J\) is the number of nodes containing the values of the \(x\)-coordinate and \(y\)-coordinate. Then, we encode the spatial positions into a latent by \(z_{m}=E_{m}(m)^{T_{zm} d}\), where \(T_{zm}<T_{m}\) is the downsampled motion frames and \(d\) is the latent motion dimension. The encoded latent can be decoded by a decoder to obtain the reconstructed motion sequence: \(m^{}=D_{m}(m)\).

**Music Variational AutoEncoder.** Music is a structured and complex audio signal, composed of various elements such as melody, harmony, rhythm, and dynamics. Some works [13; 43] utilize Musical Instrument Digital Interface (MIDI) representations, which yield highly formulated results. However, processing long-term music directly from the raw waveform is computationally intensive and challenging . To address this, we train a VAE on the mel-spectrogram derived from the music, coupled with a high-fidelity vocoder. Let \(u^{T_{u}}\) be a music input, where \(T_{u}\) denotes the waveform length. We can extract the mel-spectrogram of the music input: \(a=Mel(u)^{C_{a} T_{a}}\), where \(Mel()\) is the pre-defined mel-spectrogram extraction function, \(C_{a}\) is the channels, and \(T_{a} T_{u}\) is the frames. Then, an encoder is used to compress the mel-spectrogram into a latent: \(z_{a}=E_{a}(a)^{T_{za} d}\), where \(T_{za}\) is the downsampled music frames and \(d\) is the latent mel-spectrogram dimension. The encoded mel-spectrogram can be decoded by a decoder \(a^{}=D_{a}(a)\), and subsequently the musical waveform can be obtained by a high-fidelity vocoder \(x^{}=V(a^{})\).

### Rhythmic Contrastive Learning

Contrastive learning has proven effective for learning multi-modal representations, enhancing performance in downstream tasks . In the context of temporal alignment, a recent work  introduces temporal contrast, which seeks to maximize the similarity of audio-visual pairs from the same time segment while minimizing the similarity of pairs from different segments. However, this paradigm faces limitations in long-term motion-music synthesis, as musical pieces typically correspond to numerous rhythmic beats. The random selection process for constructing negative samples risks capturing similar rhythmic sequences, which undermines the learning objective. To address it, we propose rhythmic contrastive learning, designed to align cross-modal temporal synchronization and rhythmic correspondence. Based on the motion and music VAEs, we can obtain the motion latent \(z_{m}^{T_{z_{m}} d}\), and music mel-spectrogram latent \(z_{a}^{T_{za} d}\), respectively. To synchronize the motion and music, which are often sampled differently, we employ pre-processing techniques such as evenly dropping motion frames to match the number of music frames, ensuring that \(T_{zm}=T_{za}\).

In the domain of motion-guided music, the inherent irregularity of human movements, characterized by rapid and abrupt actions, can significantly influence rhythm. To synchronize these rhythmic patterns, we employ a kinematic amplitude indicator as a basis for constructing contrastive clips within each motion-music pair. Firstly, we extract the motion kinematic offsets  with the motion directogram , a metric that quantifies the variation in motion. We denote \(F(r,j)\) as the first-order difference of \(j\)-th node in the 2D motion at temporal timestep \(r\), and divide it into \(K\) bins based on their Euclidean angles with \(x\)-axis by \(^{-1}(y/x)\). Then, the 2D motion directogram \(D(r,)\) can be expressed as the aggregate of \(F(r,j)\) across each angular bin:

\[D(r,)=_{j=1}^{J}||F(r,j)||_{2}_{}(  F(r,j)),_{}():=1,& |-| 2/K,\\ 0,&.\] (1)

The indicator function \(_{}()\) distributes the motion nodes into \(K\) angular bins. Then, the kinematic amplitude indicator is computed by summing the bin-wise directrogram difference in each angular column:

\[Q(r)=_{k=1}^{K}(0,|D(r,k)|-|D(r-1,k)|),\] (2)

where \(D(r,k)\) is the directogram volume at temporal timestep \(r\) and \(k\)-th bin. The kinematic amplitude value is normalized within the range of (0,1).

With the kinematic amplitude indicator established, we proceed to prepare the temporal motion-music clips for contrastive rhythmic learning. For each motion-music latent pair, we randomly sample \(N_{T}\) motion-music clips and divide them into \(N_{C}\) categories according to the clip-wise maximum kinematic amplitude values. In order to maximize the similarity of motion-music pairs from the same timestep (i.e. temporal alignment) and minimize the similarity of motion-music pairs across different timesteps and rhythmic patterns, we randomly sample \(N_{S}\) motion-music latent clip \((c_{a}^{r_{s}:r_{e}},c_{m}^{r_{s}:r_{e}},Q(r_{s}:r_{e}))(^{d}, ^{d},(0,1))\) from different kinematic amplitude categories for the temporal and rhythmic alignment:

\[c_{a}^{r_{s}:r_{e}}=P_{}(z_{a}^{r_{s}}:z_{a}^{r_{e}}),\ c_{m} ^{r_{s}:r_{e}}=P_{}(z_{m}^{r_{s}}:z_{m}^{r_{e}}),\ Q(r_{s}:r_{e})=(Q(r_{ s}):Q(r_{e})),\] (3)

where \(r_{s}\) and \(r_{e}\) denote the start and end timesteps of the sampled clip, respectively, and \(P_{}\) denotes the max-pooling operation across the temporal dimension. Finally, based on the sampled motion-music clips \(\{(c_{a}^{i},c_{m}^{i})\}_{i=1}^{N_{C}}\), the contrastive objective can be formulated as:

\[_{}=- ^{i},c_{m}^{j})/)}{_{c=1}^{N_{C}}(sim(c_{a}^{i},c_{m}^{j})/)} -^{i},c_{m}^{j})/)}{_{c=1}^{N_{C}} (sim(c_{a}^{c},c_{m}^{j})/)}.\] (4)

Figure 2: An overview of the proposed MoMu-Diffusion framework. MoMu-Diffusion contains two integral components: a bidirectional contrastive rhythmic Variational Autoencoder (BiCoR-VAE) designed to learn the aligned latent space, and a Transformer-based diffusion model responsible for sequence generation. This framework is adept at facilitating both cross-modal and multi-modal joint generations, offering a robust approach to the integrated synthesis of motion and music.

### Training Strategy

In BiCoR-VAE, the goal is to learn two paired VAEs for motion and music inputs, with a focus on temporal and rhythmic alignment within the low-level latent space. However, the VAE's objective to preserve fine-grained details for accurate reconstruction often conflicts with contrastive rhythmic learning's aim to align latent representations across modalities. This presents a trade-off between representational fidelity and generative alignment, posing optimization challenges. To address it, we propose a two-stage training strategy: initially, we train the music VAE using both a VAE loss and a GAN loss to prevent over-smoothing of the mel-spectrogram; subsequently, we train the motion VAE with a VAE loss and the contrastive rhythmic loss, while keeping the music VAE's parameters fixed. The insight behind this strategy is that mel-spectrograms, with their rich and complex acoustic features, require a more intricate optimization process compared to motion VAE, which deals with a limited set of body joint data. An overview of BiCoR-VAE is illustrated in Figure 2 (a).

## 4 Transformer-based Diffusion Model with Aligned BiCoR-VAE

**Diffusion Formulation.** Recent works have revealed that the U-Net architecture is not essential for diffusion probabilistic modeling, and in fact, the transformer can achieve superior performance in text-to-image generation tasks [37; 11]. Additionally, the transformer architecture excels at capturing long-range dependencies within sequence data and offers flexibility for variable-length generation . Inspired by these findings, we opt for a Transformer-based architecture for our motion-music generation framework. Concretely, our approach involves initially concatenating the noisy input with the embedded conditional inputs and the embedded diffusion timesteps along the temporal dimension. This fused input is then padded to match a specified maximum length and combined with positional embeddings prior to being processed by the DiT model. The DiT output is subsequently truncated to the original temporal length and mapped to the output latent space. To illustrate the diffusion process, let's consider the motion-to-music task. During the forward diffusion, the latent data is gradually perturbed towards a standard Gaussian distribution according to a pre-defined schedule \(_{1},...,_{T}\), where \(T\) is the total diffusion timesteps and \(_{t}=_{i=1}^{t}_{i}\):

\[q(z_{a}(t)|z_{a}(t-1))=(z_{a}(t);}z_{a}(t-1),1- _{t}),\] (5)

where \(z_{a}(t)\) denotes the music latent at timestep \(t\). Then, the training objectives of our DiT-based cross-modal generation models are defined:

\[_{}=||_{_{a}}(z_{a}(t),t,z_{m})- ||_{2}^{2},_{}=||_{_{m}}(z_{ m}(t),t,z_{a})-||_{2}^{2},\] (6)

where \((0,1)\) denotes the noise in diffusion procedure, \(_{a}\) and \(_{m}\) are the parameterized DiT denoisers for motion-to-music and music-to-motion generation, respectively.

**Conditional Generation.** For the cross-modal generation such as motion-to-music and music-to-motion, we implement classifier-free guidance [5; 18]. This method adeptly combines conditional and unconditional scores to obtain a trade-off between quality and diversity. By interpreting the diffusion model output as a score function, the sampling procedure with classifier-free guidance of motion-to-music can be written as:

\[_{_{a}}(z_{a}(t),t,z_{m})=_{_{a}}(z_{a}(t), t,)+s(_{_{a}}(z_{a}(t),t,z_{m})-_{_{a}}(z _{a}(t),t,))\] (7)

where \(s>1\) denotes the classifier sampling scale to balance the diversity and quality of synthesized samples. The diffusion model with \(\) condition is achieved by randomly dropping \(z_{m}\) and replacing it with an embedded "null" representation. Exchanging the latent inputs enables the sampling procedure for music-to-motion generation since we have built a modality-aligned latent space.

**Joint Generation with Cross Guidance.** To accomplish multi-modal joint generation, we propose a cross-guidance sampling strategy. This approach leverages multiple 'expert' models and introduces a slight modification to the sampling procedure, rather than integrating multiple modalities into a single model. Let \(T\) be the total diffusion steps, \(_{_{a}}\) be the trained motion-to-music denoising model, and \(_{_{m}}\) be the trained music-to-motion denoising model, we perform unconditional generation before a defined diffusion step \(T_{c}\):

\[p_{_{a}}(z_{a}(t-1)|z_{a}(t))=(z_{a}(t-1),_{_{a}}(z _{a}(t),t,),_{t}^{2}),T>t>T_{c},\] (8)

\[_{_{a}}(z_{a}(t),t,)=}}(z_{a}(t)- }{_{t}}}_{_{a}}(z_{a }(t),t,)),^{2}=_{t-1}}{1- _{t}}(1-_{t}).\] (9)Eq (8) and Eq (9) delineate the reverse process for motion-to-music generation within the timestep range \(T t>T_{c}\). The reverse process for music-to-motion generation can be similarly constructed. For reverse timesteps \(T_{c} t>0\), we use the estimated clean motion/music latent to condition the generation process of music/motion with the classifier-free guidance defined in Eq (7). Given that the diffusion model adopts a coarse-to-fine refinement in the reverse process, we conduct unconditional generation before \(T_{c}\) and impose conditional generation with the cross-guidance strategy after \(T_{c}\), as the noise in the estimated clean latent is significantly reduced. Determining the value of \(T_{c}\) appears to be quite challenging; however, our empirical findings indicate that the joint generation maintains robust performance across a broad range of values for \(T_{c}\) (from 0.3\(T\) to 0.7\(T\)). The diversity in joint generation is sustained by the unconditional process and classifier-free guidance. An overview of cross-modal generation and joint generation is shown in Figure 2 (b) and (c).

## 5 Experiments

### Motion-to-Music Generation

**Experimental Settings.** We evaluate our method on the latest LORIS benchmark , which contains 86.43 hours of video samples synchronized with music. This benchmark presents three demanding scenarios: AIST++ Dance , Floor Exercise , and Figure Skating . In our experiments, each dataset is randomly split with a 90%/5%/5% proportion for training, validation, and testing. For model evaluation, we use five metrics to measure the beat-matching between synthesized music and ground-truth music : Beats Coverage Scores (**BCS**) and Beat Hit Scores (**BHS**), Coverage Standard Deviation (**CSD**), Hit Standard Deviation (**CSD**), and the **F1** scores. Besides, we use the Frechet Audio Distance (**FAD**)  and **Diversity** scores to evaluate the quality of synthesized music. Since the quality of the Floor Exercise and the Figure Skating datasets are poor, we only conduct motion-to-music generation on them with a learnable motion encoder, whose architecture is derived from . During sampling, we employ 50 DDIM sampling steps. More experimental settings are provided in Appendix B.

**Baselines.** We compare our proposed method to existing advanced video-to-music baselines: 1) **Foley Music**, a graph transformer framework with MIDI representations. 2) **CMT**, a controllable music transformer model to learn the rhythmic consistency between video and music. 3) **D2M-GAN**, a GAN-based model with vector quantized music representation. 4) **CDCD**, a diffusion-based model with an additional conditional discrete contrastive diffusion loss. 5) **LORIS**, a diffusion-based model with hierarchical conditional mechanism, yielding state-of-the-art performance on video-to-music synthesis.

   Subset &  \\ Metrics & BCS\(\) & CSD\(\) & BHS\(\) & HSD\(\) & F1\(\) \\  Foley & 96.4 & 6.9 & 41.0 & 15.0 & 57.5 \\ CMT & 97.1 & 6.4 & 46.2 & 18.6 & 62.6 \\ D2MGAN & 95.6 & 9.4 & 88.7 & 19.0 & 93.1 \\ CDCD & 96.5 & 9.1 & 89.3 & 18.1 & 92.7 \\ LORIS & **98.6** & 6.1 & 90.8 & 13.9 & 94.5 \\  Ours & 97.5 & **5.2** & **98.6** & **2.8** & **98.1** \\   

Table 2: Motion-to-music with **beat-matching** metrics.

Figure 3: Motion-to-music with **generation quality** metrics: FAD\(\) and Diversity\(\).

**Main Results.** The results of beat-matching are shown in Table 2, 3 and 4. From these tables, we can draw the following conclusions: 1) MoMu-Diffusion significantly surpasses existing state-of-the-art methods in cross-modal beat-matching. It demonstrates the effectiveness of BiCoR-VAE and the multi-modal Transformer-based model in synchronizing kinematic and rhythmic beats. 2) MoMu-Diffusion realizes a substantial improvement in Beat Hit Scores (BHS), which indicates the beats in the synthesized music are closely aligned with the ground truth. For example, MoMu-Diffusion gains 98.6% BHS on the AIST++ dancing subset, while previous methods usually gain about 90% BHS. An illustrative example of beat-matching for motion-to-music is presented in Figure 4. We can find the musical beats of synthesized music are aligned with the ground truth and the kinematic movements of the reference video.

The FAD and Diversity results are shown in Figure 3. In this comparison, we focus on LORIS, the current state-of-the-art method in motion-to-music generation. It is evident that MoMu-Diffusion consistently outperforms LORIS across these metrics, particularly in FAD scores. This superiority can be attributed to MoMu-Diffusion's architectural innovations for capturing long-term correspondence. Unlike text, music encompasses a richer sequence length due to its complex acoustic features, such as melody, rhythm, and driving beats. To address this, MoMu-Diffusion employs mel-spectrograms in place of raw waveforms, thereby mitigating sequence length. Additionally, the introduction of BiCoR-VAE facilitates modality alignment in latent spaces.

### Music-to-Motion Generation

**Experimental Settings.** We use two datasets: AIST++ Dance  and BHS Dance. About 71 hours BHS Dance videos are collected from , which contains three dancing types: "Ballet", "Zumba", and "Hip-Hop". For model evaluation, we compute the beat-matching metrics between synthesized motion beats and the reference musical beats with the aforementioned five beat-matching metrics. To validate the quality of synthesized motion sequences, we use Frechet Inception Distance (FID) , Mean KL-Divergence (Mean KLD), and the Diveristy scores. The feature extractor is based on MotionBert  and trained with a classification task on the BHS Dance dataset. For the BHS Dance dataset, we exclude the BiCoR-VAE since this dataset only contains the paired audio MFCC features and motion sequences without raw audio. In the generation process, the settings of the diffusion transformer model are the same as motion-to-music. More details are provided in Appendix B.

**Baselines.** We compare MoMu-Diffusion to two baselines: 1) **D2M**, the state-of-the-art music-to-motion work with a two-stage movement unit-based model; 2) **DiffGesture**, the state-of-the-art co-speech gesture generation work with a U-Net diffusion model. Dance Revolution  reports better performance on music-to-motion generation but is withdrawn by its authors.

   Subset &  &  \\ Metrics & BCS\(\) & CSD\(\) & BHS\(\) & HSD\(\) & F1\(\) & BCS\(\) & CSD\(\) & BHS\(\) & HSD\(\) & F1\(\) \\  Foley & 36.0 & 36.2 & 32.3 & 30.7 & 34.1 & 32.6 & 38.0 & 28.4 & 32.5 & 30.4 \\ CMT & 46.4 & 30.1 & 57.4 & 29.8 & 51.3 & 42.3 & 32.0 & 53.8 & 31.7 & 47.4 \\ D2MGAN & 45.3 & 27.7 & 58.7 & 30.1 & 51.1 & 41.9 & 29.2 & 54.7 & 32.7 & 47.5 \\ CDCD & 49.0 & 21.1 & 61.0 & 27.0 & 54.3 & 45.9 & 23.8 & 57.5 & 29.3 & 51.0 \\ LORIS & 58.8 & 19.4 & 67.1 & **21.1** & 62.7 & 54.7 & **21.6** & 63.8 & 24.5 & 58.9 \\  Ours & **63.5** & **16.3** & **75.6** & 28.8 & **69.0** & **59.9** & 22.6 & **68.7** & **22.2** & **64.0** \\   

Table 4: Results on the Figure Skating with **beat-matching** metrics.

Figure 4: Example of beat matching on the motion-to-music generation. The red dashes indicate the extracted musical beats. The red arrow points to the video frame at that particular moment.

**Main Results.** The beat-matching results are detailed in Table 5. An analysis of these results reveals that MoMu-Diffusion achieves superior scores across all evaluated tasks, outperforming the state-of-the-art music-to-motion method D2M and co-speech gesture generation method DiffGesture. This performance underscores the efficacy of our BiCoR-VAE in constructing an aligned latent space for cross-modal generation and the feed-forward diffusion model in capturing long-term correspondence. It should be noted that the metrics BCS (Beats Coverage Scores) and BHS (Beat Hit Scores) are defined differently in this context compared to motion-to-music scenarios. Specifically, BCS calculates the coverage score between the kinematic beats of the synthesized motions and the musical beats of the ground-truth music, rather than the kinematic beats of the ground-truth motions.

The generation quality results are presented in Table 6. It is observable that MoMu-Diffusion reports better FID, Mean KLD, and Diversity scores on both the AIST++ and BHS Dance datasets. It demonstrates that MoMu-Diffsuion can generate more realistic and high-quality motion sequences while maintaining the capability of diverse generations. We further present a qualitative example of music-to-motion beat-matching in Figure 5. We can find the kinematic beats of synthesized motion are highly associated with the reference musical beats. Additionally, the generated dance exhibits a high degree of diversity, encompassing lateral movements, rotations, squats, and so on.

### Analysis and Ablation Study

**User Study.** We conducted a user study with 20 annotators on the AIST++ Dance dataset to evaluate the generation performance. For each method, 200 samples were generated, and 20 paired samples were randomly selected for each comparison group. Annotators were asked to respond on site: "_Which dance/music is more realistic and matches the music/dance better?_". The human evaluation results, shown in Figure 6, indicate that our method outperforms SOTA approaches in both motion-to-music and music-to-motion generations. Notably, a preference drop is observed when BiCoR-VAE is not employed, highlighting the importance of an aligned latent space for cross-modal generation.

**Motion Encoding.** For motion sequence encoding, we compare the spatial position-based method with the directional vector-based method, which learns the unit directional vectors of the given adjacency set, and reconstructs the human pose with the calculated mean bone lengths . However, as shown in Table 7 (#1), the spatial position-based method proved superior, likely due to the error introduced by movements that alter bone length, such as squatting and bending.

   Subset &  &  \\ Metrics & FID\(\) & Diversity\(\) & Mean KLD\(\) & FID\(\) & Diversity\(\) & Mean KLD\(\) \\  D2M & 17.3 & 46.2 & 14.5 & 11.6 & 55.9 & 7.4 \\ DiffGesture & 18.6 & 37.1 & 12.6 & 13.8 & 38.9 & 7.0 \\  Ours & **7.3** & **52.7** & **4.9** & **6.5** & **67.4** & **4.2** \\   

Table 6: Results on the AIST++ Dance and BHS Dance datasets with **generation quality** metrics.

Figure 5: Example of beat matching on the music-to-motion generation. The red dashes indicate the extracted kinematic beats of the synthesized motion. The red arrow points to the frame of the synthesized motion sequence at that particular moment.

   Subset &  &  \\ Metrics & BCS\(\) & CSD\(\) & BHS\(\) & HSD\(\) & F1\(\) & BCS\(\) & CSD\(\) & BHS\(\) & HSD\(\) & F1\(\) \\  D2M & 23.7 & 13.8 & 42.8 & 23.6 & 30.5 & 35.1 & 15.9 & 57.5 & 35.0 & 43.6 \\ DiffGesture & 28.5 & 16.7 & 40.4 & 25.7 & 33.4 & 42.8 & 21.3 & 61.1 & 23.9 & 50.3 \\  Ours & **39.2** & **10.2** & **56.3** & **12.0** & **46.2** & **47.9** & **8.4** & **78.5** & **12.1** & **59.5** \\   

Table 5: Results on the AIST++ Dance and BHS Dance datasets with **beat-matching** metrics.

**Music Encoding.** For music encoding, we evaluated the spectrogram-based method against the raw waveform-based method. According to Table 7 (#2), the raw waveform-based method gains performance declines in both FAD and F1 metrics. This is attributed to the lengthy audio sequences introduced by the raw waveform, introducing difficulties for diffusion modeling training.

**Learning Techniques.** In MoMu-Diffusion, there are two key learning techniques: rhythmic contrastive learning (RCL) and Feed-Forward Transformer (FFT). From Table 7, we can observe that "Ours w/o RCL" gains a clear drop on the beat-matching metric F1 (#3) and "Ours w/o FFT" gains a drop on the synthesis quality metric FID/FAD (#4), respectively. "Ours w/o FFT" means we use a U-Net backbone for the diffusion model, which has been shown inferior to our FFT-based model in long sequence modeling. Equipped with both RCL and FFT, MoMu-Diffusion ensures both generation quality and cross-modal alignment.

**Joint Generation in Variable Length.** MoMu-Diffusion supports multi-modal joint generation in variable lengths, facilitated by a "pad-and-truncate" strategy in the diffusion model and the proposed cross-guidance sampling. To validate this capability, 1000 samples with varying lengths (10-30 seconds) are generated using different Gaussian noise vectors. With a cross-guidance sampling timestep set to \(T_{c}=0.5T\), Table 7 (#5, #6), we can find that for multi-modal joint generation, MoMu-Diffusion shows that MoMu-Diffusion achieves comparable performance to the conditional models with clean condition inputs and advanced performance on the joint generation scenario. More ablation studies are provided in Appendix D.

## 6 Conclusion

In this paper, we propose MoMu-Diffusion, the first multi-modal framework designed to learn the long-term synchronization and correspondence between human motions and music. In MoMu-Diffusion, we have two key designs: bidirectional contrastive rhythmic VAE (BiCoR-VAE) for learning modality-aligned latent spaces and Transformer-based diffusion model for learning long-term dependencies. Through extensive experiments, we demonstrate MoMu-Diffusion's efficacy across motion-to-music, music-to-motion, and joint motion-music generations.

    &  &  &  \\  & & FAD \(\) & F1 \(\) & FID \(\) & F1\(\) \\  \#1 & Ours w/ Directional Vectors & 10.9 & 91.4 & 14.7 & 38.0 \\ \#2 & Ours w/o Mel-spectrogram & 12.8 & 95.6 & 9.5 & 41.6 \\ \#3 & Ours w/o Rhythmic Contrastive Learning (RCL) & 8.5 & 93.1 & 8.1 & 37.9 \\ \#4 & Ours w/o Feed-Forward Transformer (FFT) & 11.0 & 95.8 & 11.6 & 41.4 \\  \#5 & Ours (Joint Generation) & 8.1 & 96.5 & 8.8 & 45.4 \\ \#6 & Ours (Joint Generation\&Variable Length) & 9.1 & 97.6 & 8.5 & 49.6 \\ \#7 & Ours (Cross Generation) & 8.9 & 98.1 & 7.3 & 46.2 \\   

Table 7: Ablation study on motion-to-music and music-to-motion generations. We use the FAD/FID as the quality assessment and the F1 score as the beat-matching assessment.

Figure 6: Results of human evaluation on motion-to-music and music-to-motion generations.