# Offline Primal-Dual Reinforcement Learning for Linear MDPs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Offline Reinforcement Learning (RL) aims to learn a near-optimal policy from a fixed dataset of transitions collected by another policy. This problem has attracted a lot of attention recently, but most existing methods with strong theoretical guarantees are restricted to finite-horizon or tabular settings. In constrast, few algorithms for infinite-horizon settings with function approximation and minimal assumptions on the dataset are both sample and computationally efficient. Another gap in the current literature is the lack of theoretical analysis for the average-reward setting, which is more challenging than the discounted setting. In this paper, we address both of these issues by proposing a primal-dual optimization method based on the linear programming formulation of RL. Our key contribution is a new reparametrization that allows us to derive low-variance gradient estimators that can be used in a stochastic optimization scheme using only samples from the behavior policy. Our method finds an \(\)-optimal policy with \(O(^{-4})\) samples, improving on the previous \(O(^{-5})\), while being computationally efficient for infinite-horizon discounted and average-reward MDPs with realizable linear function approximation and partial coverage. Moreover, to the best of our knowledge, this is the first theoretical result for average-reward offline RL.

## 1 Introduction

We study the setting of Offline Reinforcement Learning (RL), where the goal is to learn an \(\)-optimal policy without being able to interact with the environment, but only using a fixed dataset of transitions collected by a _behavior policy_. Learning from offline data proves to be useful especially when interacting with the environment can be costly or dangerous .

In this setting, the quality of the best policy learnable by any algorithm is constrained by the quality of the data, implying that finding an optimal policy without further assumptions on the data is not feasible. Therefore, many methods [23; 33] make a _uniform coverage_ assumption, requiring that the behavior policy explores sufficiently well the whole state-action space. However, recent work [17; 31] demonstrated that _partial coverage_ of the state-action space is sufficient. In particular, this means that the behavior policy needs only to sufficiently explore the state-actions visited by the optimal policy.

Moreover, like its online counterpart, modern offline RL faces the problem of learning efficiently in environments with very large state spaces, where function approximation is necessary to compactly represent policies and value functions. Although function approximation, especially with neural networks, is widely used in practice, its theoretical understanding in the context of decision-making is still rather limited, even when considering _linear_ function approximation.

In fact, most existing sample complexity results for offline RL algorithms are limited either to the tabular and finite horizon setting, by the uniform coverage assumption, or by lack of computational efficiency -- see the top section of Table 1 for a summary. Notable exceptions are the recent works ofXie et al.  and Cheng et al.  who provide computationally efficient methods for infinite-horizon discounted MDPs under realizable linear function approximation and partial coverage. Despite being some of the first implementable algorithms, their methods work only with discounted rewards, have superlinear computational complexity and find an \(\)-optimal policy with \(O(^{-5})\) samples - see the bottom section of Table 1 for more details. Therefore, this work is motivated by the following research question:

_Can we design a linear-time algorithm with polynomial sample complexity for the discounted and average-reward infinite-horizon settings, in large state spaces under a partial-coverage assumption?_

We answer this question positively by designing a method based on the linear-programming (LP) formulation of sequential decision making . Albeit less known than the dynamic-programming formulation  that is ubiquitous in RL, it allows us to tackle this problem with the powerful tools of convex optimization. We turn in particular to a relaxed version of the LP formulation [21; 2] that considers action-value functions that are linear in known state-action features. This allows to reduce the dimensionality of the problem from the cardinality of the state space to the number of features. This relaxation still allows to recover optimal policies in _linear MDPs_[37; 13], a structural assumption that is widely employed in the theoretical study of RL with linear function approximation.

Our algorithm for learning near-optimal policies from offline data is based on primal-dual optimization of the Lagrangian of the relaxed LP. The use of saddle-point optimization in MDPs was first proposed by Wang & Chen  for _planning_ in small state spaces, and was extended to linear function approximation by Chen et al. , Bas-Serrano & Neu , and Neu & Okolo . We largely take inspiration from this latter work, which was the first to apply saddle-point optimization to the _relaxed_ LP. However, primal-dual planning algorithms assume oracle access to a transition model, whose samples are used to estimate gradients. In our offline setting, we only assume access to i.i.d. samples generated by a possibly unknown behavior policy. To adapt the primal-dual optimization strategy to this setting we employ a change of variable, inspired by Nachum & Dai , which allows easy computation of unbiased gradient estimates.

Notation.We denote vectors with bold letters, such as \([x_{1},,x_{d}]^{}^{d}\), and use \(_{i}\) to denote the \(i\)-th standard basis vector. We interchangeably denote functions \(f:\) over a finite set \(\), as vectors \(^{||}\) with components \(f(x)\), and use \(\) to denote element-wise comparison. We denote the set of probability distributions over a measurable set \(\) as \(_{}\), and the probability simplex in \(^{d}\) as \(_{d}\). We use \(:^{d}_{d}\) to denote the softmax function defined as \(_{i}() e^{x_{i}}/_{j=1}^{d}e^{x_{j}}\). We use upper-case letters for random variables, such as \(S\), and denote the uniform distribution over a finite set of \(n\) elements as \((n)\). In the context of iterative algorithms, we use \(_{t-1}\) to denote the sigma-algebra generated by all events up to the end of iteration \(t-1\), and use the shorthand notation \(_{t}[]=[]_{t-1}\) to denote expectation conditional on the history. For nested-loop algorithms, we write \(_{t,i-1}\) for the sigma-algebra generated by all events up to the end of iteration \(i-1\) of round \(t\), and \(_{t,i}[]=[]_{t,i-1}\) for the corresponding conditional expectation.

**Algorithm** & **Partial** &  **Polynomial** \\ **Sample** \\  &  **Polynomial** \\ **Computational** \\ **Complexity** \\  &  **Function** \\ **Approximation** \\  & 
 **Infinite Horizon** \\ **Discounted** \\  \\  FQI  & ✗ & ✓ & ✓ & ✓ & ✓ & ✗ \\  Rashidinejad et al.  & ✓ & ✓ & ✓ & ✗ & ✓ & ✗ \\ Jin et al.  & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Zanette et al.  & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Uehara \& Sun  & ✓ & ✓ & ✗ & ✓ & ✓ & ✗ \\  Cheng et al.  & ✓ & \(O(^{-5})\) & superlinear & ✓ & ✓ & ✗ \\ Xie et al.  & ✓ & \(O(^{-5})\) & \(O(n^{7/5})\) & ✓ & ✓ & ✗ \\
**Ours** & ✓ & \(O(^{-4})\) & \(O(n)\) & ✓ & ✓ & ✓ \\ 

Table 1: Comparison of existing offline RL algorithms. The table is divided horizontally in two sections. The upper section qualitatively compares algorithms for easier settings, that is, methods for the tabular or finite-horizon settings or methods which require uniform coverage. The lower section focuses on the setting considered in this paper, that is computationally efficient methods for the infinite horizon setting with function approximation and partial coverage.

## 2 Preliminaries

We study discounted Markov decision processes [MDP, 29] denoted as \((,,p,r,)\), with discount factor \(\) and finite, but potentially very large, state space \(\) and action space \(\). For every state-action pair \((x,a)\), we denote as \(p( x,a)_{}\) the next-state distribution, and as \(r(x,a)\) the reward, which is assumed to be deterministic and bounded for simplicity. The transition function \(p\) is also denoted as the matrix \(^{||||}\) and the reward as the vector \(^{||}\). The objective is to find an _optimal policy_\(^{*}:_{}\). That is, a stationary policy that maximizes the normalized expected return \((^{*})(1-)_{^{*}}[_{t=0}^{}r(X_{t},A _{t})]\), where the initial state \(X_{0}\) is sampled from the initial state distribution \(_{0}\), the other states according to \(X_{t+1} p(|X_{t},A_{t})\) and where the notation \(_{}[]\) is used to denote that the actions are sampled from policy \(\) as \(A_{t}(|X_{t})\). Moreover, we define the following quantities for each policy \(\): its state-action value function \(q^{}(x,a)_{}[_{t=0}^{}^{t}r(X_{t},A_{t})  X_{0}=x,A_{0}=a]\), its value function \(v^{}(x)_{}[q^{}(x,A_{0})]\), its state occupancy measure \(^{}(x)(1-)_{}[_{t=0}^{}\{X _{t}=x\}]\), and its state-action occupancy measure \(^{}(x,a)(a|x)^{}(x)\). These quantities are known to satisify the following useful relations, more commonly known respectively as Bellman's equation and flow constraint for policy \(\):

\[^{}=+^{}^{}=(1- )_{0}+^{}^{}\] (1)

Given this notation, we can also rewrite the normalized expected return in vector form as \(()=(1-)_{0},^{}\) or equivalently as \(()=,^{}\).

Our work is based on the linear programming formulation due to Manne  (see also 29) which transforms the reinforcement learning problem into the search for an optimal state-action occupancy measure, obtained by solving the following Linear Program (LP):

\[&,\\ &^{}=(1-)_{0}+^{}\\ & 0\] (2)

where \(^{||||}\) denotes the matrix with components \(_{(x,a),x^{}}\{x=x^{}\}\). The constraints of this LP are known to characterize the set of valid state-action occupancy measures. Therefore, an optimal solution \(^{*}\) of the LP corresponds to the state-action occupancy measure associated to a policy \(^{*}\) maximizing the expected return, and which is therefore optimal in the MDP. This policy can be extracted as \(^{*}(a|x)^{*}(x,a)/_{}^{*}(x,)\). However, this linear program cannot be directly solved in an efficient way in large MDPs due to the number of constraints and dimensions of the variables scaling with the size of the state space \(\). Therefore, taking inspiration from the previous works of Bas-Serrano et al. , Neu & Okolo  we assume the knowledge of a _feature map_\(\), which we then use to reduce the dimension of the problem. More specifically we consider the setting of Linear MDPs .

**Definition 2.1** (Linear MDP).: An MDP is called linear if both the transition and reward functions can be expressed as a linear function of a given feature map \(:^{d}\). That is, there exist \(:^{d}\) and \(^{d}\) such that, for every \(x,x^{}\) and \(a\):

\[r(x,a)=(x,a),, p(x^{}  x,a)=(x,a),(x^{}).\]

We assume that for all \(x,a\), the norms of all relevant vectors are bounded by known constants as \((x,a)_{2} D_{}\), \(_{x^{}}(x^{})_{2} D_{}\), and \(_{2} D_{}\). Moreover, we represent the feature map with the matrix \(^{|| d}\) with rows given by \((x,a)^{}\), and similarly we define \(^{d||}\) as the matrix with columns given by \((x)\).

With this notation we can rewrite the transition matrix as \(=\). Furthermore, it is convenient to assume that the dimension \(d\) of the feature map cannot be trivially reduced, and therefore that the matrix \(\) is full-rank. An easily verifiable consequence of the Linear MDP assumption is that state-action value functions can be represented as a linear combinations of \(\). That is, there exist \(^{}^{d}\) such that:

\[^{}=+^{}=(+ ^{})=^{}.\] (3)

It can be shown that for all policies \(\), the norm of \(^{}\) is at most \(D_{}=D_{}+}}{1-}\) (cf. Lemma B.1 in 13). We then translate the linear program (2) to our setting, with the addition of the new variable \(^{d}\), resulting in the following new LP and its corresponding dual:\[&,&&(1-)_{0},\\ &^{T}=(1-)_{0}+^{T} &&=+\\ &=^{}&\\ & 0.\] (5)

It can be immediately noticed how the introduction of \(\) did not change neither the set of admissible \(\)s nor the objective, and therefore did not alter the optimal solution. The Lagrangian associated to this set of linear programs is the function:

\[(,,,) =(1-)_{0},+,+-+,-\] \[=,+,(1- )_{0}+^{T}-^{T}+ ,^{T}-.\] (6)

It is known that finding optimal solutions \((^{*},^{*})\) and \((^{*},^{*})\) for the primal and dual LPs is equivalent to finding a saddle point \((^{*},^{*},^{*},^{*})\) of the Lagrangian function . In the next section, we will develop primal-dual methods that aim to find approximate solutions to the above saddle-point problem, and convert these solutions to policies with near-optimality guarantees.

## 3 Algorithm and Main Results

This section introduces the concrete setting we study in this paper, and presents our main contributions.

We consider the offline-learning scenario where the agent has access to a dataset \(=(W_{t})_{t=1}^{n}\), collected by a behavior policy \(_{B}\), and composed of \(n\) random observations of the form \(W_{t}=(X_{t}^{0},X_{t},A_{t},R_{t},X_{t}^{})\). The random variables \(X_{t}^{0},(X_{t},A_{t})\) and \(X_{t}^{}\) are sampled, respectively, from the initial-state distribution \(_{0}\), the discounted occupancy measure of the behavior policy, denoted as \(_{B}\), and from \(p(\,|\,X_{t},A_{t})\). Finally, \(R_{t}\) denotes the reward \(r(X_{t},A_{t})\). We assume that all observations \(W_{t}\) are generated independently of each other, and will often use the notation \(_{t}=(X_{t},A_{t})\).

Our strategy consists in finding approximately good solutions for the LPs (4) and (5) using stochastic optimization methods, which require access to unbiased gradient estimates of the Lagrangian (Equation 6). The main challenge we need to overcome is constructing suitable estimators based only on observations drawn from the behavior policy. We address this challenge by introducing the matrix \(=_{X,A_{B}}[(X,A)(X,A)^{}]\) (supposed to be invertible for the sake of argument for now), and rewriting the gradient with respect to \(\) as

\[_{}(,;, ) =+-=^{-1} (+-)\] \[=^{-1}[(X_{t},A_{t})(X_{t},A_{t})^{}(+-)]\] \[=^{-1}[(X_{t},A_{t})( R_{t}+(X_{t}^{})-,(X_{t},A_{t}) )].\]

This suggests that the vector within the expectation can be used to build an unbiased estimator of the desired gradient. A downside of using this estimator is that it requires knowledge of \(\). However, this can be sidestepped by a reparametrization trick inspired by Nachum & Dai : introducing the parametrization \(=^{-1}\), the objective can be rewritten as

\[(,;,)=(1-)_ {0},+,+ {}-+,- .\]

This can be indeed seen to generalize the tabular reparametrization of Nachum & Dai  to the case of linear function approximation. Notably, our linear reparametrization does not change the structure of the saddle-point problem, but allows building an unbiased estimator of \(_{}(,;,)\) without knowledge of \(\) as

\[}_{}=(X_{t},A_{t})(R_{t}+( X_{t}^{})-,(X_{t},A_{t})).\]

In what follows, we will use the more general parametrization \(=^{-c}\), with \(c\{1/2,1\}\), and construct a primal-dual stochastic optimization method that can be implemented efficiently in the offline setting based on the observations above. Using \(c=1\) allows to run our algorithm without knowledge of \(\), that is, without knowing the behavior policy that generated the dataset, while using \(c=1/2\) results in a tighter bound, at the price of having to assume knowledge of \(\).

Our algorithm (presented as Algorithm 1) is inspired by the method of Neu & Okolo , originally designed for planning with a generative model. The algorithm has a double-loop structure, where at each iteration \(t\) we run one step of stochastic gradient ascent for \(\), and also an inner loop which runs \(K\) iterations of stochastic gradient descent on \(\) making sure that \((x,a),_{t}\) is a good approximation of the true action-value function of \(_{t}\). Iterations of the inner loop are indexed by \(k\). The main idea of the algorithm is to compute the unbiased estimators \(}_{,t,k}\) and \(}_{,t}\) of the gradients \(_{}(_{t},_{t};,_{t,k})\) and \(_{}(_{t},;_{t},_{t})\), and use them to update the respective variables iteratively. We then define a softmax policy \(_{t}\) at each iteration \(t\) using the \(\) parameters as \(_{t}(a|x)=(_{i=1}^{t-1}(x,a),_{i})\). The other higher-dimensional variables (\(_{t},_{t}\)) are defined symbolically in terms of \(_{t}\), \(_{t}\) and \(_{t}\), and used only as auxiliary variables for computing the estimates \(}_{,t,k}\) and \(}_{,t}\). Specifically, we set these variables as

\[v_{t}(x) =_{a}_{t}(a|x)(x,a),_{t},\] (7) \[_{t,k}(x,a) =_{t}(a|x)(1-)\{X_{t,k}^{0}=x\}+ _{t,k},^{c-1}_{t} \{X_{t,k}^{}=x\}.\] (8)

Finally, the gradient estimates can be defined as

\[}_{,t} =^{c-1}_{t}(R_{t}+ v_{t}(X_{t} ^{})-_{t},_{t}),\] (9) \[}_{,t,k} =^{}_{t,k}-^{c-1}_{t,k}_{t,k},_{t}.\] (10)

These gradient estimates are then used in a projected gradient ascent/descent scheme, with the \(_{2}\) projection operator denoted by \(\). The feasible sets of the two parameter vectors are chosen as \(_{2}\) balls of radii \(D_{}\) and \(D_{}\), denoted respectively as \((D_{})\) and \((D_{})\). Notably, the algorithm does not need to compute \(v_{t}(x)\), \(_{t,k}(x,a)\), or \(_{t}(a|x)\) for all states \(x\), but only for the states that are accessed during the execution of the method. In particular, \(_{t}\) does not need to be computed explicitly, and it can be efficiently represented by the single \(d\)-dimensional parameter vector \(_{i=1}^{t}_{i}\).

Due to the double-loop structure, each iteration \(t\) uses \(K\) samples from the dataset \(\), adding up to a total of \(n=KT\) samples over the course of \(T\) iterations. Each gradient update calculated by the method uses a constant number of elementary vector operations, resulting in a total computational complexity of \(O(||dn)\) elementary operations. At the end, our algorithm outputs a policy selected uniformly at random from the \(T\) iterations.

### Main result

We are now almost ready to state our main result. Before doing so, we first need to discuss the quantities appearing in the guarantee, and provide an intuitive explanation for them.

Similarly to previous work, we capture the partial coverage assumption by expressing the rate of convergence to the optimal policy in terms of a _coverage ratio_ that measures the mismatch between the behavior and the optimal policy. Several definitions of coverage ratio are surveyed by Uehara & Sun . In this work, we employ a notion of _feature_ coverage ratio for linear MDPs that defines coverage in feature space rather than in state-action space, similarly to Jin et al. , but with a smaller ratio.

**Definition 3.1**.: Let \(c\{}{{2}},1\}\). We define the generalized coverage ratio as

\[C_{,c}(^{*};_{B})=_{(X^{*},A^{*})^{^{*}}}[ (X^{*},A^{*})]^{}^{-2c} [(X^{*},A^{*})].\]

We defer a detailed discussion of this ratio to Section 6, where we compare it with similar notions in the literature. We are now ready to state our main result.

**Theorem 3.2**.: _Given a linear MDP (Definition 2.1) such that \(^{}(D_{})\) for any policy \(\). Assume that the coverage ratio is bounded \(C_{,c}(^{*};_{B}) D_{}\). Then, for any comparator policy \(^{*}\), the policy output by an appropriately tuned instance of Algorithm 1 satisfies \([^{^{*}}-^{_{ }}},]\) with a number of samples \(n_{}\) that is \(O(^{-4}D_{}^{4}D_{}^{8c }D_{}^{4}d^{2-2c}||)\)._

The concrete parameter choices are detailed in the full version of the theorem in Appendix A. The main theorem can be simplified by making some standard assumptions, formalized by the following corollary.

**Corollary 3.3**.: _Assume that the bound of the feature vectors \(D_{}\) is of order \(O(1)\), that \(D_{}=D_{}=\) and that \(D_{}=c C_{,c}(^{*};_{B})\) for some positive universal constant \(c\). Then, under the same assumptions of Theorem 3.2, \(n_{}\) is of order \(OC_{,c}(^{*};_{B})^{2}||}{d^{2c} (1-)^{4}^{4}}\)._

## 4 Analysis

This section explains the rationale behind some of the technical choices of our algorithm, and sketches the proof of our main result.

First, we explicitly rewrite the expression of the Lagrangian (6), after performing the change of variable \(=^{c}\):

\[(,;, ) =(1-)_{0},+ ,^{c}(+ -)+ ,- \] (11) \[=,^{c}+,(1-)_{0}+ ^{}^{c}- ^{}+, ^{}-^{c}.\] (12)

We aim to find an approximate saddle-point of the above convex-concave objective function. One challenge that we need to face is that the variables \(\) and \(\) have dimension proportional to the size of the state space \(||\), so making explicit updates to these parameters would be prohibitively expensive in MDPs with large state spaces. To address this challenge, we choose to parametrize \(\) in terms of a policy \(\) and \(\) through the symbolic assignment \(=_{,}\), where

\[_{,}(x,a)(a|x)(1-)_{0}(x)+ (x),^{c} .\] (13)

This choice can be seen to satisfy the first constraint of the primal LP (4), and thus the gradient of the Lagrangian (12) evaluated at \(_{,}\) with respect to \(\) can be verified to be \(0\). This parametrization makes it possible to express the Lagrangian as a function of only \(,\) and \(\) as

\[f(,,)(,_{,};,)=,^{c}+,^{}_{ ,}-^{c}.\] (14)

For convenience, we also define the quantities \(_{}=^{}_{ ,}\) and \(v_{,}(s)_{a}(a|s)\,,(x,a)\), which enables us to rewrite \(f\) as

\[f(,,)=^{c} ,-+ _{,},_{} =(1-)_{0},_{,}+^{c}, +_{, }-.\] (15)

The above choices allow us to perform stochastic gradient / ascent over the low-dimensional parameters \(\) and \(\) and the policy \(\). In order to calculate an unbiased estimator of the gradients, we first observe that the choice of \(_{t,k}\) in Algorithm 1 is an unbiased estimator of \(_{_{t},_{t}}\):

\[_{t,k}[_{t,k}(x,a)] =_{t}(a|x)(1-)(X^{0}_{t,k}=x)+ _{t,k}[\{X^{}_{t,k}=x\}_{t},^{c-1}_{t}]\] \[=_{t}(a|x)(1-)_{0}(x)+_{, {a}}_{B}(,)p(x|,)(,)^{ }^{c-1}_{t}\] \[=_{t}(a|x)(1-)_{0}(x)+(x)^{ }^{c-1}_{t}=_{,_{t}}(x,a),\]

where we used the fact that \(p(x|,)=(x),(,)\), and the definition of \(\). This in turn facilitates proving that the gradient estimate \(}_{,t,k}\), defined in Equation 10, is indeed unbiased:

\[_{t,k}[}_{,t,k}]=^{ }_{t,k}[_{t,k}]-^{c-1} _{t,k}[_{t,k}^{}_{t,k}] _{t}=^{}_{_{t},_{t}}-^{c}_{t}=_{}(_{t}, _{t};_{t},).\]

A similar proof is used for \(}_{,t}\) and is detailed in Appendix B.3.

Our analysis is based on arguments by Neu & Okolo , carefully adapted to the reparametrized version of the Lagrangian presented above. The proof studies the following central quantity that we refer to as _dynamic duality gap_:

\[_{T}(^{*},^{*};^{*}_{1:T})_{t=1}^{T}(f(^{*},^{*};_{t})-f(_{t}, _{t};^{*}_{t})).\] (16)

Here, \((_{t},_{t},_{t})\) are the iterates of the algorithm, \(^{*}_{1:T}=(^{*}_{t})_{t=1}^{T}\) a sequence of comparators for \(\), and finally \(^{*}\) and \(^{*}\) are fixed comparators for \(\) and \(\), respectively. Our first key lemma relates the suboptimality of the output policy to \(_{T}\) for a specific choice of comparators.

**Lemma 4.1**.: _Let \(^{*}_{t}^{_{t}}\), \(^{*}\) be any policy, and \(^{*}=^{-c}^{}^{^{*}}\). Then, \([^{^{*}}-^{_{_{ _{_{}}}}},]=_{T} ^{*},^{*};^{*}_{1:T}\)._

The proof is relegated to Appendix B.1. Our second key lemma rewrites the gap \(_{T}\) for _any_ choice of comparators as the sum of three regret terms:

**Lemma 4.2**.: _With the choice of comparators of Lemma 4.1_

\[_{T}(^{*},^{*};^{*}_{1:T}) =_{t=1}^{T}_{t}-^{*}_ {t},g_{,t}+_{t=1}^{T}^{*}- _{t},g_{,t}\] \[+_{t=1}^{T}_{s}^{^{*}}(s)_{a}( ^{*}(a|s)-_{t}(a|s))_{t},(x,a),\]

_where \(g_{,t}=^{}_{_{t},_{t}}-^{c}_{t}\) and \(g_{,t}=^{c}(+_{_{t},_{t}}-_{t})\)._

The proof is presented in Appendix B.2. To conclude the proof we bound the three terms appearing in Lemma 4.2. The first two of those are bounded using standard gradient descent/ascent analysis (Lemmas B.1 and B.2), while for the latter we use mirror descent analysis (Lemma B.3). The details of these steps are reported in Appendix B.3.

## 5 Extension to Average-Reward MDPs

In this section, we briefly explain how to extend our approach to offline learning in _average reward MDPs_, establishing the first sample complexity result for this setting. After introducing the setup, we outline a remarkably simple adaptation of our algorithm along with its performance guarantees for this setting. The reader is referred to Appendix C for the full details, and to Chapter \(8\) of Puterman  for a more thorough discussion of average-reward MDPs.

In the average reward setting we aim to optimize the objective \(^{}(x)=_{T}_{}_{t=1}^ {T}r(x_{t},a_{t})x_{1}=x\), representing the long-term average reward of policy \(\) when started from state \(x\). Unlike the discounted setting, the average reward criterion prioritizes long-term frequency over proximity of good rewards due to the absence of discounting which expresses a preference for earlier rewards. As is standard in the related literature, we will assume that \(^{}\) is well-defined for any policy and is independent of the start state, and thus will use the same notation to represent the scalar average reward of policy \(\). Due to the boundedness of the rewards, we clearly have \(^{}\). Similarly to the discounted setting, it is possible to define quantities analogous to the value and action value functions as the solutions to the Bellman equations \(^{}=-^{}+^{}\), where \(^{}\) is related to the action-value function as \(v^{}(x)=_{a}(a|x)q^{}(x,a)\). We will make the following standard assumption about the MDP (see, e.g., Section 17.4 of Meyn & Tweedie (2017)):

**Assumption 5.1**.: For all stationary policies \(\), the Bellman equations have a solution \(^{}\) satisfying \(_{x,a}q^{}(x,a)-_{x,a}q^{}(x,a)<D_{q}\).

Furthermore, we will continue to work with the linear MDP assumption of Definition 2.1, and will additionally make the following minor assumption:

**Assumption 5.2**.: The all ones vector \(\) is contained in the column span of the feature matrix \(\). Furthermore, let \(^{d}\) such that for all \((x,a)\), \((x,a),=1\).

Using these insights, it is straightforward to derive a linear program akin to (2) that characterize the optimal occupancy measure and thus an optimal policy in average-reward MDPs. Starting from this formulation and proceeding as in Sections 2 and 4, we equivalently restate this optimization problem as finding the saddle-point of the reparametrized Lagrangian defined as follows:

\[(,;,,)=+\,,^{c}[+--]+\,,-.\]

As previously, the saddle point can be shown to be equivalent to an optimal occupancy measure under the assumption that the MDP is linear in the sense of Definition 2.1. Notice that the above Lagrangian slightly differs from that of the discounted setting in Equation (11) due to the additional optimization parameter \(\), but otherwise our main algorithm can be directly generalized to this objective. We present details of the derivations and the resulting algorithm in Appendix C. The following theorem states the performance guarantees for this method.

**Theorem 5.3**.: _Given a linear MDP (Definition 2.1) satisfying Assumption 5.2 and such that \(^{}(D_{})\) for any policy \(\). Assume that the coverage ratio is bounded \(C_{,c}(^{*};_{B}) D_{}\). Then, for any comparator policy \(^{*}\), the policy output by an appropriately tuned instance of Algorithm 2 satisfies \(\{^{^{*}}-^{_{}}, \}\) with a number of samples \(n_{}\) that is \(O(^{-4}D_{}^{1}D_{}^{12c-2}D_{}^{4}d^{2-2c}||)\)._

As compared to the discounted case, this additional dependence of the sample complexity on \(D_{}\) is due to the extra optimization variable \(\). We provide the full proof of this theorem along with further discussion in Appendix C.

## 6 Discussion and Final Remarks

In this section, we compare our results with the most relevant ones from the literature. Our Table 1 can be used as a reference. As a complement to this section, we refer the interested reader to the recent work by Uehara & Sun (2017), which provides a survey of offline RL methods with their coverage and structural assumptions. Detailed computations can be found in Appendix E.

An important property of our method is that it only requires partial coverage. This sets it apart from classic batch RL methods like FQI (Krizhevsky et al., 2017; Sohn et al., 2018), which require a stronger uniform-coverage assumption. Algorithms working under partial coverage are mostly based on the principle of pessimism. However, our algorithm does not implement any form of explicit pessimism. We recall that, as shown by Xiao et al. (2018), pessimism is just one of many ways to achieve minimax-optimal sample efficiency.

Let us now compare our notion of coverage ratio to the existing notions previsouly used in the literature. Jin et al. (2018) (Theorem 4.4) rely on a _feature_ coverage ratio which can be written as

\[C^{}(^{*};_{B})=_{X,A^{*}}[(X,A)^ {}^{-1}(X,A)].\] (17)

By Jensen's inequality, our \(C_{,1/2}\) (Definition 3.1) is never larger than \(C^{}\). Indeed, notice how the random features in Equation (17) are coupled, introducing an extra variance term w.r.t. \(C_{,1/2}\). Specifically, we can show that \(C_{,1/2}(^{*};_{B})=C^{}(^{*};_{B})-_{X,A ^{*}}[^{-1/2}(X,A)]\), where \([Z]=\|Z-[Z]\|^{2} \) for a random vector \(Z\). So, besides fine comparisons with existing notions of coverage ratios, we can regard \(C_{,1/2}\) as a low-variance version of the standard feature coverage ratio. However, our sample complexity bounds do not fully take advantage of this low-varianceproperty, since they scale quadratically with the ratio itself, rather than linearly, as is more common in previous work.

To scale with \(C_{,1/2}\), our algorithm requires knowledge of \(\), hence of the behavior policy. However, so does the algorithm from Jin et al. . Zanette et al.  remove this requirement at the price of a computationally heavier algorithm. However, both are limited to the finite-horizon setting.

Uehara & Sun  and Zhang et al.  use a coverage ratio that is conceptually similar to Equation (17),

\[C^{}(^{*};_{B})=_{y^{d}}_ {X,A^{*}}[(X,A)(X,A)^{}]y}{y^{ }_{X,A_{B}}[(X,A)(X,A)^{ }]y}.\] (18)

Some linear algebra shows that \(C^{} C^{} dC^{}\). Therefore, chaining the previous inequalities we know that \(C_{,1/2} C^{} dC^{}\). It should be noted that the algorithm from Uehara & Sun  also works in the representation-learning setting, that is, with unknown features. However, it is far from being efficiently implementable. The algorithm from Zhang et al.  instead is limited to the finite-horizon setting.

In the special case of tabular MDPs, it is hard to compare our ratio with existing ones, because in this setting, error bounds are commonly stated in terms of \(_{x,a}(x,a)}}{{_{B}(x,a)}}\), often introducing an explicit dependency on the number of states [e.g., 17], which is something we carefully avoided. However, looking at how the coverage ratio specializes to the tabular setting can still provide some insight. With known behavior policy, \(C_{,1/2}(^{*};_{B})=_{x,a}(x,a)}}{{_{B} (x,a)}}\) is smaller than the more standard \(C^{}(^{*};_{B})=^{*}(x,a)}}{{_{B}(x,a)}}\). With unknown behavior, \(C_{,1}(^{*};_{B})=(^{*}(x,a)/_{B}(x,a ))^{2}}}{{2}}\) is non-comparable with \(C^{}\) in general, but larger than \(C_{,1/2}\). Interestingly, \(C_{,1}(^{*};_{B})\) is also equal to \(1+^{2}(^{*}\|_{B})\), where \(^{2}\) denotes the chi-square divergence, a crucial quantity in off-distribution learning based on importance sampling . Moreover, a similar quantity to \(C_{,1}\) was used by Lykouris et al.  in the context of (online) RL with adversarial corruptions.

We now turn to the works of Xie et al.  and Cheng et al. , which are the only practical methods to consider function approximation in the infinite horizon setting, with minimal assumption on the dataset, and thus the only directly comparable to our work. They both use the coverage ratio \(C_{}(^{*};_{B})=_{f}f\|_{n}^{2}}}{{\|f-f\|_{n}^{2}}}\), where \(\) is a function class and \(\) is Bellman's operator. This can be shown to reduce to Equation (18) for linear MDPs. However, the specialized bound of Xie et al.  (Theorem 3.2) scales with the potentially larger ratio from Equation (17). Both their algorithms have superlinear computational complexity and a sample complexity of \(O(^{-5})\). Hence, in the linear MDP setting, our algorithm is a strict improvement both for its \(O(^{-4})\) sample complexity and its \(O(n)\) computational complexity. However, It is very important to notice that no practical algorithm for this setting so far, including ours, can match the minimax optimal sample complexity rate of \(O(^{2})\). This leaves space for future work in this area. In particular, by inspecting our proofs, it should be clear the the extra \(O(^{-2})\) factor is due to the nested-loop structure of the algorithm. Therefore, we find it likely that our result can be improved using optimistic descent methods  or a two-timescale approach .

As a final remark, we remind that when \(\) is unknown, our error bounds scales with \(C_{,1}\), instead of the smaller \(C_{,1/2}\). However, we find it plausible that one can replace the \(\) with an estimate that is built using some fraction of the overall sample budget. In particular, in the tabular case, we could simply use all data to estimate the visitation probabilities of each-state action pairs and use them to build an estimator of \(\). Details of a similar approach have been worked out by Gabbianelli et al. . Nonetheless, we designed our algorithm to be flexible and work in both cases.

To summarize, our method is one of the few not to assume the state space to be finite, or the dataset to have global coverage, while also being computationally feasible. Moreover, it offers a significant advantage, both in terms of sample and computational complexity, over the two existing polynomial-time algorithms for discounted linear MDPs with partial coverage ; it extends to the challenging average-reward setting with minor modifications; and has error bounds that scale with a low-variance version of the typical coverage ratio. These results were made possible by employing algorithmic principles, based on the linear programming formulation of sequential decision making, that are new in offline RL. Finally, the main direction for future work is to develop a single-loop algorithm to achieve the optimal rate of \(^{-2}\), which should also improve the dependence on the coverage ratio from \(C_{,c}(^{*};_{B})^{2}\) to \(C_{,c}(^{*};_{B})\).