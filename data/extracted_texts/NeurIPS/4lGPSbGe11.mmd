# Is Cross-Validation the Gold Standard to Estimate Out-of-sample Model Performance?

Garud Iyengar, Henry Lam, Tianyu Wang

Department of Industrial Engineering and Operations Research

Columbia University

New York, NY 10027

{gi10,khl2114,tw2837}@columbia.edu

Authors ordered alphabetically. More information on the data and code are available at https://github.com/wangtianyu61/CV_GoldStandard.

###### Abstract

Cross-Validation (CV) is the default choice for estimate the out-of-sample performance of machine learning models. Despite its wide usage, their statistical benefits have remained half-understood, especially in challenging nonparametric regimes. In this paper we fill in this gap and show that, in terms of estimating the out-of-sample performances, for a wide spectrum of models, CV does not statistically outperform the simple "plug-in" approach where one reuses training data for testing evaluation. Specifically, in terms of both the asymptotic bias and coverage accuracy of the associated interval for out-of-sample evaluation, \(K\)-fold CV provably cannot outperform plug-in regardless of the rate at which the parametric or nonparametric models converge. Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account. We obtain our theoretical comparisons via a novel higher-order Taylor analysis that dissects the limit theorems of testing evaluations, which applies to model classes that are not amenable to previously known sufficient conditions. Our numerical results demonstrate that plug-in performs indeed no worse than CV in estimating model performance across a wide range of examples.

## 1 Introduction

Cross-validation (CV) is considered the default choice for estimating the out-of-sample performance of machine learning models [54; 31; 4] and more general data-driven optimization models . Its main rationale is to evaluate models using a testing set that is different from training, so as to provide a reliable estimate of the model generalization ability. Leave-one-out CV (LOOCV) [5; 7], which repeatedly evaluates models trained using all but one observation on the left-out observation, is a prime approach; however, it is computationally demanding as it requires model re-training for the same number of times as the sample size. Because of this, \(K\)-fold CV, which reduces the number of model re-training down to \(K\) times (where \(K\) is typically 5-10), becomes a popular substitute [34; 42].

Despite their wide usage, the statistical benefits of CV have remained understood mostly for parametric models. In nonparametric regimes, especially those involving slow model convergence rates, their general performances as well as comparisons with the naive "plug-in" approach, i.e., simply reuses all the same training data for model evaluation, stay essentially open. Part of the challenge comes from the subtle inter-dependence of model convergence rates and other characteristics withthe correlation between training and validation sets across folds. Consequently, existing results are either based on limit theorems designed to "center" at the average-of-folds instead of full-size model , or restricted to specific models (e.g., linear) [7; 9] or specific (fast) rates . Our goal in this paper, on a high level, is to fill in the challenging regimes beyond these established results, and as such answer the question: _Are LOOCV and \(K\)-fold CV a "must-use" in estimating out-of-sample model performance in general and, if not, then under what situations are they worthwhile?_

More precisely, in this paper we conduct a systematic analysis to compare the accuracies in estimating model performances using LOOCV, \(K\)-fold CV and plug-in. We focus on the asymptotic bias and coverage accuracy of the associated interval estimate for the out-of-sample evaluation. Our main messages are: First, in terms of these asymptotic criteria, \(K\)_-fold CV never outperforms plug-in, regardless of the rate at which a parametric or nonparametric model converges_. Second, while LOOCV can have a smaller bias than plug-in, this bias improvement can be negligible compared to the evaluation variability and therefore, _in a range of important cases, LOOCV again does not outperform plug-in._ In particular, we show that all parametric models, as well as some nonparametric models including random forests and kNN with sufficient smoothness, fall into this range. Since LOOCV requires significantly more computation resources, this raises the caution that its use is not always necessary, despite its robust performance for model evaluation.

As a simple illustration, Figure 1 shows the evaluation quality of the squared error of a random forest regressor using 2-fold CV, 5-fold CV, LOOCV and plug-in. We see that 2- and 5-fold CVs suffer from larger biases than plug-in (shown in the bar chart) especially for large sample sizes, and correspondingly also significantly poorer coverages of the associated interval estimates (shown by the lines). On the other hand, LOOCV exhibits smaller biases than plug-in, but these do not transform into better coverages since the bias improvement is negligible compared to the statistical variability in the evaluation. Both intervals provide valid coverage guarantees for large sample sizes (shown by the lines). We highlight that this example is not a "cherry pick": Section 5 and Appendix F show similar conclusions for a wide array of numerical examples.

We close this introduction by briefly discussing our technical novelty. Our analysis framework to conclude all our comparisons is propelled by a novel higher-order Taylor analysis on the out-of-sample evaluation that account for the dependence between the trained model and the testing data. In contrast to merely sufficient conditions in the literature, under which plug-in and CV variants exhibit low biases and valid coverages, this analysis helps us provide a complete breakdown of how bias and coverage depend on the convergence rate of the model at hand. This in turn fills in the gap in understanding which methods outperform which others, in regimes that have appeared challenging for previous works.

## 2 Problem Framework

We consider the supervised learning setting with observations \(_{n}:=\{(X_{i},Y_{i})\}_{i[n]}\) drawn i.i.d. from the joint distribution \(_{(X,Y)}:=_{X}_{Y|X}\). We obtain a predictor \((x)=(_{n};x)\) as a

Figure 1: Evaluation biases and coverage probabilities of interval estimates (with nominal level 90%) for the mean-squared error evaluation of a fitted random forest regressor (default setup in scikit-learn in  with \(n^{0.4}\) subsamples in each tree), across 500 experimental replications. The bar chart shows the evaluation bias, defined as the absolute mean difference between the estimated and true performance (the vertical line at the top of each bar shows the corresponding standard error). The lines show the coverage probabilities.

function of \(x\) with the output domain \(\), through a training procedure \(\) on \(_{n}\). We are interested in evaluating the out-of-sample performance \(_{_{(X,Y)}}[((X);Y)]\), where \((z;Y):\) is the cost function. This evaluation can be a point estimate, or more generally an interval estimate \(I()\) that covers \(_{_{(X,Y)}}[((X);Y)]\) with \(1-\) probability, i.e., we aim to satisfy \(_{_{m_{n}}}(_{_{(X,Y)}}[(( X);Y)] I()) 1-\), where the outer probability \(_{_{n}}\) is with respect to the data \(_{n}\) used to construct \(()\). We focus on the low-dimensional asymptotic setting where \(n\) and \(,,\) are of fixed dimensions and defer discussions to other regimes in Section 6.

Regarding the scope of our setup, \(\) can be the loss function for supervised learning (e.g., squared loss, cross-entropy loss), in which case \(\) naturally denotes the predicted label and \((X,Y)\) denotes the feature-label pair. More generally, \(\) can denote a downstream optimization objective in a decision-making problem, in which case \(Y\) denotes a random outcome that affects the objective given the contextual information \(X\). This latter setup, which is called _contextual stochastic optimization_, can be viewed as a generalization of supervised learning from building prediction models to prescriptive decision policies. For example, in the so-called newsvendor problem in operations management, the cost refers to monetary loss of a retailer determined by the order quantity \(z\), and covariate \(X\) refers to the market condition that drives stochastic demand \(Y\). Our framework in this paper applies to both the traditional supervised learning and prescriptive data-driven decision-making settings.

We consider three main methods: plug-in, LOOCV, and \(K\)-fold CV. We denote \(_{m}\) and \(I_{m}()\) as the point estimate and \((1-)\)-level interval estimate, using method \(m=\{p,loocv,kcv\}\) referring to plug-in, LOOCV and \(K\)-fold CV respectively. For convenience, we denote \(^{*}\) as the true joint distribution \(_{(X,Y)}\) and \(}_{n}=(1/n)_{i=1}^{n}_{(X_{i},Y_{i})}\) as the empirical distribution, and we denote \(c(z)\) and \(c_{n}(z)\) as the out-of-sample performance \(_{^{*}}[(z(X);Y)]\) and the plug-in evaluation of the out-of-sample performance \(_{_{n}}[(z(X);Y)]\) for any decision mapping \(z(x)\) respectively. We present our considered point and interval estimates, where the latter are all written in the form \(I_{m}()=[_{m}-z_{1-/2}_{m}/,_{m} +z_{1-/2}_{m}/], m\) with:

\[_{p}=c_{n}(),\ \ _{p}=_{i[n]} (((X_{i});Y_{i})-_{p})^{2}},\] (1)

\[_{kcv}=_{k[K]}_{i N_{k}}(^{(-N_{k} )}(X_{i});Y_{i}),\ \ _{kcv}=_{k[K]}_{i N_{k}}(( ^{(-N_{k})}(X_{i});Y_{i})-_{kcv})^{2}},\] (2)

where \(z_{1-/2}\) is the \((1-/2)\)-quantile of the standard normal distribution, \(\{N_{k},k[K]\}\) is the collection of \(K\) equal-length partitions of \([n]\) (for simplicity we assume \(n\) is divisible by \(K\)) and \(^{(-N_{k})}():=(_{(-N_{k})};)\), with \(_{(-N_{k})}\) denoting the data set that leaves out \(\{(X_{i},Y_{i})\}_{i N_{k}}\). For the \(K\)-fold CV estimates, \(_{kcv}\) and \(I_{kcv}\), we always assume \(K\) is fixed with respect to \(n\) (e.g., \(K=2,5,10\)). On the other hand, \(_{loocv}\) and \(I_{loocv}()\) are defined by setting \(K=n\) in (2). Note that there are alternative approaches to construct the interval estimates (e.g., nested cross validation in Appendix G.3), but the above are the most natural and have been shown to have statistical consistency properties as well as superior empirical performance over other intervals .

We impose the following regularity and optimality conditions on the cost function:

**Assumption 1** (Smoothness of Expected Cost): _For any \(x\), \(v(z;x):=_{_{Y|x}}[(z;Y)]\) is twice differentiable with respect to \(z\) everywhere, where \(_{Y|x}\) is the conditional distribution of \(Y\) given \(x\)._

**Assumption 2** (Regularity of Cost Function): _For any \(y\), \((z;y)\) is twice differentiable with respect to \(z\) for every \(y\). Moreover, \(|(z;y)|,\|_{z}(z;y)\|_{2}\) are uniformly bounded in \(z\) and almost surely in \(y\)._

**Assumption 3** (Optimality Conditions): \(\) _is a bounded open set. The best mapping \(z_{o}^{*}(x)\) that minimizes \(v(z;x), x\) satisfies the first and second-order optimality conditions. More precisely, \( x,_{z}v(z_{o}^{*}(x);x)=0\), and \(_{zz}v(z_{o}^{*}(x);x)\) is positive definite._

While Assumption 2 is standard, we can relax it further to some non-smooth objectives including piecewise linear functions (e.g., \((z;Y)=|z-Y|\); see Assumption 5 in Appendix B.2. The other assumptions above are commonly used in stochastic optimization . We further allow constrained problems in Assumption 6 in Appendix B.2.

**Example 1**: _The function \((z;Y)=(z-Y)^{2}\) (\(_{2}\)-regression), compact set \(\), and \(z_{o}^{*}(x)=[Y|X=x]\) with \(=\{z:|z|<B\}\) for any \(B>_{x}z_{o}^{*}(x)\) satisfy Assumptions 1, 2 and 3._

Next, we distinguish between parametric and nonparametric models in Definitions 1 and 2 below, leaving further details on technical regularity conditions in Appendix B.3.

**Definition 1** (Parametric Model): \((x)=G(;x)\)_, where \(=*{argmin}_{}_{i=1}^{n}(G( ;X_{i});Y_{i})+_{n}R()\) for some regularization function \(R()\). Regularity conditions of \(G(;X)\) are provided in Assumption 7 in Appendix B.3.1, which are satisfied by linear models (Example 2 of Section 3)._

**Definition 2** (Nonparametric Model): \(()\) _is obtained through \((x)*{argmin}_{z Z}_{i[n]}w_{n,i}(x)(z;Y _{i})\) with weights \(\{w_{n,i}(x)\}_{i[n]}\) depending on \(_{n}\) and \(x\). Regularity conditions of \(w_{n,i}()\) are provided in Assumption 8 in Appendix B.3.2, which are satisfied by the classical k-Nearest Neighbor (kNN) and forest learners (Examples 3, 4 of Section 3)._

Define \(z^{*}():=(_{};)\) as the _oracle_ best model using the training procedure \(\) with infinite data \(_{}\). We now define the notion of convergence rate order for model \(()\):

**Definition 3** (Convergence Rate): _For a model \(()\), we say it has a convergence rate of order \((0,1/2]\) if \(_{_{n}}[\|(x)-z^{*}(x)\|_{2}]=(n^{-})\) for almost every \(x\). Furthermore, we say \(()\) has a bias and variability convergence rate \(_{b},_{v}\) respectively if \(_{_{n}}[\|_{_{n}}[(x)]-z^{*} (x)\|_{2}]=(n^{-_{b}})\) and \(_{_{n}}[\|(x)-_{_{n}}[(x)]\|_{2}]=(n^{-_{v}})\) for almost every \(x\). Consequently, \(=\{_{b},_{v}\}\)._

The overall convergence order \(\) of \(()\) is determined by both its bias \(_{b}\) and variability \(_{v}\), whichever dominates. For parametric models in Definition 2, we naturally have \(=_{v}=1/2\) (see Proposition 1 in Appendix B.3.1). However, unless \(\{G(;x):\}\) contains the model \(z_{o}^{*}()\) that optimizes \(v(z;)\), there is a discrepancy between \(z_{o}^{*}()\) and the limiting model \(z^{*}()\). For nonparametric models in Assumption 2, both \(_{b}\) and \(_{v}\) depend on the hyperparameter configuration and are often smaller than \(1/2\). When their hyperparameters are properly chosen (e.g. Theorems 5 - 9 in ), we have \(z^{*}()=z_{o}^{*}()\) thanks to the nonparametric power in eliminating model misspecification.

Lastly, we introduce the following stability conditions:

**Definition 4** (Stability): _Denote two leave-one-out (LOO) stability notions \(_{n},_{n}\) by:_

\[_{n}:=_{i[n]} \{(_{^{*},_{n}}[\|(X)-^{(-i )}(X)\|^{2}])^{}\},\] (3) \[_{n}:=_{i[n]} \{_{_{n}}[\|(X_{i})-^{(-i)}(X_{i}) \|]\},\] (4)

_where the expectation in (3) is with respect to both the data \(_{n}\), used to construct \(\) and \(^{(-i)}\), and \(^{*}\) that generates \(X\), while (4) has expectation taken with respect to only \(_{n}\)._

Stability notions are first proposed in  and commonly used to provide generalization guarantees for CV  as well as refined bounds under more relaxed stability in . We assume the following:

**Assumption 4** (LOO Stability): \(()\) _satisfies the expected LOO stability with \(_{n}=o(n^{-1/2})\)._

This condition holds for many models in Definitions 1 and 2  and is often imposed for the validity of plug-in and CV, e.g. in . For illustration, we provide an example of 1-NN in Appendix B.4.

## 3 Main Results

We present our main results on the evaluation bias and interval coverage for plug-in, \(K\)-fold CV and LOOCV. Unless specified otherwise, \(\) and \(\) in the following are taken with respect to \(_{n}\).

**Theorem 1** (Bias): _Suppose Assumptions 1, 2, 3 and 4 hold. Recall \(,_{v}\) in Definition 3. Then, for \(()\) in Definitions 1 and 2:_

\[[c()-_{p}]=(n^{-2_{v}})>0,[c()-_{kcv}]=(n^{-2})<0,[c()-_{locov}]=o(n^{-1})<0.\]

**Theorem 2** (Coverage Validity): _Suppose Assumptions 1, 2, 3 and 4 hold. Recall \(=\{p,kcv,locov\}\). Then, for \(()\) and \(z^{*}()\) in Definitions 1 and 2:_

* _If_ \(>1/4\)_, then:_ \(_{n}(c() I_{m})=_{n}( c(z^{*}) I_{m})=1-,\;\;\;\;m\)_._
* _If_ \( 1/4\)_, then_ \(_{n}(c(z^{*}) I_{m})<1-\) _for_ \(m\)_._
* \(_{n}(c() I_{p}) 1-\)_, where equality holds if and only if_ \(_{n}=o(n^{-1/2})\) _(or_ \(_{v}>1/4\)_)._
* \(_{n}(c() I_{kcv})<1-\)_._
* \(_{n}(c() I_{locov})=1-\)_._

In the following, we use Theorems 1 and 2 to compare plug-in, \(K\)-fold CV and LOOCV, and highlight our novelty relative to what is known in the literature. In a nutshell, the regime \( 1/4\) has been wide open and comprises our major contribution and necessitates our new theory described in Section 4.

Comparing plug-in and \(K\)-fold CV.Theorems 1 and 2 together stipulate that _plug-in is always no worse than \(K\)-fold CV_ in terms of estimating out-of-sample performance. In terms of evaluation bias, plug-in is optimistic while \(K\)-fold CV is pessimistic. For such a bias direction, optimistic bias refers to underestimating the expected cost. Plug-in suffers such bias since it is estimated on the same training data. In contrast, pessimistic bias refers to overestimating the expected cost. CV suffers such bias since CV is unbiased for the evaluation using fewer training samples than the whole dataset, and appears more erroneous than it should be compared to the true evaluation using the whole dataset. For parametric models, since \(=_{v}=1/2\), their biases in Theorem 1 are both \((n^{-1})\). This recovers the results in  in which case the bias is negligible when constructing intervals. However, this bias size is unknown for general nonparametric models in the literature. For these models, our new results show that the bias of plug-in is \((n^{-2_{v}})\) which is no bigger than that of \(K\)-fold CV since \(_{v}\). This behavior arises because, even though plug-in incurs an underestimation of \((n^{-2_{v}})\) due to the reuse of training and evaluation set, \(K\)-fold CV loses efficiency due to a loss of training sample from the data splitting, thus leading to an even larger bias of \((n^{-2})\).

The above comparisons are inherited to interval coverage. While plug-in and \(K\)-fold CV both exhibit asymptotically exact coverage for parametric models (included in the case \(>1/4\)), their coverages differ for nonparametric models, with plug-in still always no worse than \(K\)-fold CV. Specifically, when \( 1/4\), \(K\)-fold CV incurs invalid coverage, whereas plug-in still yields valid coverage as long as \(_{v}>1/4\). This is because, in this regime, the bias of \(K\)-fold CV is bigger than its variability to affect coverage significantly while the bias of plug-in remains small enough to retain coverage validity. In the literature,  show valid coverage using plug-in under some stability conditions, but it is unclear regarding their applicability to general models. On the other hand, for CVs, central limit theorems and hence coverage guarantees have been derived generally, but they are centered at the average performance of trained models across folds , and thus bear a gap between such an averaged performance and the true model performance. Recently,  further show CV intervals can provide coverage guarantees when \(>1/4\), but they do not touch on the case \( 1/4\). Moreover, all the literature above do not demonstrate an explicit difference between \(K\)-fold and LOOCV in their results . From these, our results on the regime \( 1/4\) where we characterize and conclude the difference between \(K\)-fold and LOOCV appear the first in the literature.

Comparing plug-in and LOOCV.When comparing with plug-in, LOOCV has a smaller, and pessimistic, bias \(o(1/n)\). However, this bias improvement can be negligible compared to the evaluation variability captured in interval coverage, specifically when \(>1/4\) (which includes all parametric models) and when \( 1/4,_{v}>1/4\). In the latter case in particular, the bias of plug-in, even though larger than LOOCV, is small enough to ensure valid coverage.

We visually summarize our discussions on biases and interval coverages in Figure 2. In particular, Figure 2\((a)(b)\) display our new contributions on both plug-in and CVs under slow rate \(\). We also see that plug-in intervals provide valid coverages for \((b)(c)\), while \(K\)-fold CV is only valid for \((c)\) and LOOCV is valid across \((a)(b)(c)\).

Examples.We exemplify the above insights with several specific models. Denote \(d_{x},d_{y}\) as the dimensions of \(X\) and \(Y\). We consider a regression problem with \((z;Y)=(z-Y)^{2}\) and \(d_{x}=4,d_{y}=1\), where \(^{*}\) with:

\[=\{_{X}=U(^{4}),_{Y|x}=N(f(x),1), x f(x)\}.\] (5)

We consider the worst-case instance of \(^{*}\), in the sense that \(_{b},_{v}\) take the smallest attainable values in Chapter 3 of .

**Example 2** ((Regularized) Linear-ERM ): \(G(;x)=^{}x\)_, \(_{n}=1,R()=\|\|_{2}^{2}\) satisfy Assumption 1. Specifically, \(=_{v}=1/2\), and all of plug-in, \(K\)-fold CV and LOOCV provide valid coverages for \(c()\)._

**Example 3** (kNN-Learner): _Denote the nearest index set \(_{_{n},x}(k_{n})=\{i|X_{i}\) is a kNN of \(x\}\). Then \(w_{n,i}(x)=_{\{i_{_{n},x}(k_{n})\}}\) satisfies Assumption 2 with hyperparameter \(k_{n}\). Specifically, \(_{v}=- k_{n}/(2 n)\) and \(_{b}=(k_{n}/n)/(4 n)\) (from Chapter 6 in ). If \(k_{n}=()\), then LOOCV and plug-in provide valid coverages for \(c()\); otherwise, only LOOCV provide valid coverages._

**Example 4** (Forest Learner): _Consider a forest \(=\{_{1},,_{T}\}\), where each \(_{i}:^{d_{x}}\{1,,L_{i}\}\) is a (tree) partition of \(R^{d_{x}}\) into \(L_{i}\) regions. Then \(w_{n,i}(x)=_{j=1}^{T}_{\{_{j}(X_{i})=_{j}(x)\}}/T\) satisfies Assumption 2, where the hyperparameter is the subsampling ratio \(<2/3\) in each tree. Specifically, \(_{v}=(1-)/2\) from  and \(_{b}<1/6\) (from Lemma 5 in Appendix B.1). Then LOOCV and plug-in provide valid coverages for \(c()\)._

We summarize our theoretical comparisons in this section in Table 1, the first half of which shows our general comparisons in terms of both evaluation bias and interval coverage, while the second half illustrates our considered examples.

Finally, we point out that Theorem 2 also shows, in addition to our evaluation target \(c()\), the coverage on the _oracle_ best performance \(c(z^{*})\). The latter is generally a different quantity than \(c()\), but it plays an important role in our analysis. When \(>1/4\), \(c()\) and \(c(z^{*})\) are very close and an interval for \(c()\) is also valid to cover \(c(z^{*})\). On the other hand, when \( 1/4\), the statistical discrepancy

   & - &  &  \\  Model & Specifications & \(c()-_{p}\) & \(c()-_{kcv}\) & \(c()-_{loocv}\) & \(I_{p}\) & \(I_{kcv}\) & \(I_{loocv}\) \\   & \(>1/4\) & \(o(n^{-1/2})\) & \(o(n^{-1/2})\) & & ✓ & ✓ & ✓ \\  & \(_{b} 1/4,_{v}>1/4\) & \(o(n^{-1/2})\) & \((n^{-1/2})\) & \(o(n^{-1})\) & ✓ & ✗ & ✓ \\  & \(_{v} 1/4\) & \((n^{-1/2})\) & \((n^{-1/2})\) & & ✗ & ✗ & ✓ \\   Specific \\ \(d_{x}=4\) \\  } & Linear-ERM & \((n^{-1})\) & \((n^{-1})\) & & ✓ & ✓ & ✓ \\  & kNN with \(k_{n}=(n^{1/4})\) & \((n^{-1/8})\) & \((n^{-1/8})\) & \(o(n^{-1})\) & ✗ & ✗ & ✓ \\   & kNN with \(k_{n}=(n^{2/3})\) & \((n^{-2/3})\) & \((n^{-1/12})\) & & ✓ & ✗ & ✓ \\   & Forest with \(=0.4\) & \((n^{-0.3})\) & \((n^{-1/6})\) & & ✓ & ✗ & ✓ \\   

Table 1: Asymptotic bias and coverage for each approach, where ✓ and � denote valid and invalid coverages. \(o(),()\) and \(()\) follow the standard big O notation.

Figure 2: Concept plots of interval coverages of \(c()\), and also \(c(z^{*})\), for our considered approaches across model rates, where the black line represents the value of the expected cost and a point is considered covered if it falls within the corresponding interval.

between \(c()\) and \(c(z^{*})\) is too large for any interval estimates of \(c()\) to be valid for \(c(z^{*})\), but nonetheless LOOCV and plug-in for \(_{v}>1/4\) can still validly cover \(c()\) thanks to their small evaluation biases.

## 4 Roadmap of Theoretical Developments

We present the main theoretical ideas to show Theorems 1 and 2. Before going into details, we highlight the main novelties of our analyses: First, the biases for general nonparametric models in Theorem 1, which are unknown in the literature, require a different Taylor analysis compared with the parametric case available in . Second, parts of Theorem 2 come from verifying the central limit theorems (CLTs) in . However,  only shows that CLTs hold for \(c()\) when \(_{n},_{n}=o(n^{-1/2})\) and does not show exactly when CLT fails;  only gives CLTs for CVs with a different center than \(c(),c(z^{*})\). In this regard, our main technical contribution is to fill in these theoretical gaps and characterize necessary conditions, instead of merely sufficient conditions, to conclude interval (in)validity across the entire spectrum.

### Evaluation Bias

One key component of Theorem 1 hinges on the characterization of optimistic bias for plug-in, namely \(_{_{n}}[c_{n}()-c()]\), which captures the underestimation amount of the objective value relative to the truth when using an empirical estimator:

**Lemma 1** (Plug-in Bias): _Suppose Assumptions 1, 2, 3 and 4 hold. For \(()\) in Assumption 2, \(_{_{n}}[c_{n}()]-_{_{n}}[c()]=(n^{-2_{v}})<0\)._

The proof of Lemma 1 relies on a novel second-order Taylor expansion centered at the _deterministic_ decision \(z_{n}(x):=[(x)]\) on both the empirical gap \(c_{n}()-c_{n}(z_{n})\) and the true gap \(c(z_{n})-c()\) in nonparametric models \(()\). Here, we do not center them at the limiting decision \(z^{*}(x)\) compared with the parametric setup from  since in nonparametric models, \(()-z_{n}()\) already captures the variability term that leads to the plug-in evaluation bias. Besides, in these nonparametric models, we need to further analyze the second-order difference \([\|(x)-z_{n}(x)\|_{2}^{2}]\) and \([\|(X_{i})-z_{n}(X_{i})\|_{2}^{2}]\), which requires a more involved analysis through a comparison on the asymptotic expansion terms of \(()-z_{n}()\). To understand the optimistic bias further, we provide a constructive proof for kNN with an optimistic bias \((1/k_{n})\) in Proposition 4 in Appendix D.1.

The results for CVs follow from the observation that \(K\)-fold CV (where here \(K\) can be any number up to \(n\)) gives an unbiased evaluation for the model trained with \(n(1-1/K)\) samples:

**Lemma 2** (CV Bias): _Suppose Assumptions 1, 2, 3 and 4 hold. For \(()\) in Definitions 1 and 2, \(_{_{n}}[_{kcv}]-_{_{n}}[c( )]=(K^{-1}n^{-2})>0,_{_{n}}[_{ locov}]-_{_{n}}[c()]=o(n^{-1})>0\)._

### Interval Coverage

The proof of Theorem 2 hinges on the following equivalences of conditions among stability, convergence rate, and coverage validity. For plug-in, we have:

**Theorem 3** (Equivalence among Stability, Convergence Rate and Coverage Validity for Plug-in): _Suppose Assumptions 1, 2, 3 and 4 hold. Then the following three conditions are equivalent:_

\[:_{n}=o(n^{-1/2}),:_{n}(c() I_{p})=1-,:_{v}>1/4.\]

Theorem 3 is shown through three components of arguments, where the first two are our new technical contributions:

(1) \(\) is sufficient for \(\). Intuitively, a faster rate of \(_{v}\) means that the effect of one data point is usually small, implying a fast pointwise stability. This result follows from a refined decomposition of the variability term \((x)-^{(-i)}(x)\) through the influence of each point based on the asymptotic expansions of \(()\) in Assumption 2. We examine its bias and variability respectively, with formal results provided in Proposition 5 in Appendix E.1.1.

(2) S3 is necessary for S2. Since the variability of the interval width does not differ significantly (Lemma 7 in Appendix C), only the bias would lead to interval invalidity. From Lemma 1, if S3 does not hold, i.e., \(_{v} 1/4\), then \(_{_{n}}[c_{n}()-c()]=(n^{-1/2})\) and this implies that S2 does not hold from Proposition 6 in Appendix E.1.2.

(3) S1 is sufficient for S2. This follows from a direct verification of Lemma 2 in  to ensure asymptotic normality for plug-in and the small variability of the interval width (Lemma 7 in Appendix C). Furthermore, due to a small difference between \(c()\) and \(c(z^{*})\) (Lemma 6 in Appendix C), the plug-in interval can also cover the quantity \(_{_{n}}[c()]\) and \(c(z^{*})\) if \(>1/4\) (i.e. Corollary 1 in Appendix E.1.4).

In the above, we show that the condition \(_{n}=o(n^{-1/2})\) or \(_{v}<1/4\) is a necessary and sufficient condition to ensure a valid plug-in interval if Assumption 4 holds. Note that  demonstrate that in a specific nonparametric model, Assumption 4 is a necessary condition for the coverage invalidity of plug-in by providing a counterexample (Lemma 3 there). Our results are not directly comparable to theirs. First, we assume Assumption 4 throughout the entire paper and show that plug-in does not provide valid coverage guarantees when another stability notion, \(_{n}\), is large. Second, our results apply to a more general class of nonparametric models in Assumption 2 instead of the particular models and cost functions \(\) in .

**Theorem 4** (Equivalence between Convergence Rate and Coverage Validity for CV): _Suppose Assumptions 1, 2, 3 and 4 hold. Then for \(K\)-fold CV, the following two conditions are equivalent:_

\[:>1/4,:_{n}(c()  I_{kcv})=1-.\]

_For LOOCV, we always have \(_{n}(c() I_{loocv})=1-\). However, \(_{n}(c(z^{*}) I_{m})<1-, m\{kcv, loocv\}\) if \( 1/4\)._

To understand the necessary condition for \(K\)-fold CV, since \(\) is small, the difference between the expected performance between the decision \((x)\) and \(^{(-N_{k})}(x)\) is not negligible, leading to a larger overestimate of performance from \(c()\) compared with the interval width. However, following the stability condition from Assumption 4, LOOCV can still provide a valid coverage guarantee for \(c()\), since the difference between \(c_{n}()\) and \(_{loocv}\) is always \(o_{p}(n^{-1/2})\). In contrast, the sufficient condition for S5 follows by the asymptotic normality of CV through verifying Theorem 1 in .

## 5 Numerical Experiments

**Setups.** We consider two synthetic experiments to validate our theoretical results: (1) Regression problem: \((z;Y)=(z-Y)^{2}\); (2) Conditional value-at-risk (CVaR) portfolio optimization: \((z;Y)=z_{v}+(-z_{p}^{}Y-z_{v})^{+}\) with \(=\{z=(z_{p},z_{v})|^{}z_{p}=1,z_{p}\}\) for some \((0,1)\). In the regression example, we consider ridge regression, kNN with \(k_{n}=(n^{2/3})\), and Forest with \(=0.4\) (recall Example 4). In portfolio optimization, we consider sample average approximation (SAA, which belongs to Assumption 1) and kNN with \(k_{n}=(n^{1/4})\). We run plug-in, 5-fold CV and LOOCV with nominal level \(1-=0.9\). For each setting, we evaluate the following metrics with 500 experimental repetitions: (1) Coverage Probability (cov90): coverage probability of \(c()\) with parentheses denoting that of \(c(z^{*})\); (2) Interval Width (IW); and (3) bias: Difference between \(c()\) and the midpoint of the interval \([c()]-[_{}]\).

Full experimental setup details are deferred to Appendices F.1 and F.2 with results of more \(K\)-fold CV results with \(K=2,10,20\). Moreover, we present a real-world regression dataset in Appendix F.3.

**Results.** Table 2 shows that, in terms of coverage, parametric models including ridge regression and SAA cover both \(c()\) and \(c(z^{*})\) when \(n\) is large, across all methods. On the other hand, nonparametric models (kNN and Forest) incur invalid coverages, due to their slow rates \(<1/4\). When using kNN with \(k_{n}=(n^{1/4})\), only LOOCV provides valid coverage for \(c()\) (nearly 90%) while plug-in and 5-CV fail when \(n\) becomes larger. When using Forest and kNN with \(k_{n}=(n^{2/3})\), plug-in is valid while 5-CV does not work. This matches the theoretical coverage guarantees in Table 1.

Table 2 also reports interval widths and biases to understand how some of the intervals fail. Lengths are comparable across each method, with plug-in usually shorter than LOOCV and 5-CV. This can be

attributed to that plug-in approximates \(()\) better while extra variability arises from the data splitting in CVs, an observation in line with those in . Biases are relatively large for nonparametric models for all methods. However, when \(n\) is large, both kNN with \(k_{n}=(n^{2/3})\) and the Forest provide valid coverage for plug-in, attributed to the small bias relative to interval width, but not the case for other approaches (e.g., 5-CV for Forest).

## 6 Discussions, Limitations and Future Directions

We close this paper with some guidance to practitioners in light of the results we have obtained, as well as cautionary notes on the limitations of our study and future directions.

Guidance to Practitioners.In estimating out-of-sample model performances, our results suggest the following practical guidance in choosing different methods. First, in terms of the magnitude of bias, LOOCV is always smaller than plug-in, while plug-in is no larger than \(K\)-fold CV. Despite this bias ordering, the adoption of a method over another should also take into account the variability and computational demand, specifically:

* For parametric and nonparametric models with a fast rate (\(>1/4\), including the so-called sieve estimators in  when the true function \(f(x)\) is \(2d_{x}\)-th continuously differentiable under \(\) in (5)), the biases in all three considered procedures, plug-in, LOOCV and \(K\)-fold CV, are negligible compared to the variability captured in interval coverage. Correspondingly, all three intervals provide valid statistical coverages. Among them, plug-in is the most computationally efficient and should be preferred.
* For nonparametric models with a slow rate but small variability (\(_{v}>1/4, 1/4\)), which include kNN with \(k_{n}=()\) in Example 3 and the forest learner in Example 4, the biases in plug-in and LOOCV are negligible but \(K\)-fold is not. Correspondingly, both plug-in and LOOCV provide valid coverages but \(K\)-fold CV does not. Since plug-in is computationally much lighter than LOOCV, it is again preferred.
* For nonparametric models with slow rate (\(_{v} 1/4\)), which include kNN with \(k_{n}=()\) in Example 3, only LOOCV has a negligible bias and provides valid coverages, and hence should be adopted.

  method & \(n\) &  &  &  \\  - & - & cov90 & IW & bias & cov90 & IW & bias & cov90 & IW & bias \\  =10,d_{y}=1)\)} \\  Ridge & 1200 & 0.77 (**0.95**) & 0.16 & 0.02 & 0.55 (0.31) & 0.18 & -0.08 & 0.78 (0.90) & 0.17 & 0.00 \\  & 2400 & **0.85** (0.97) & 0.11 & 0.01 & 0.84 (**0.92**) & 0.12 & -0.02 & **0.86** (**0.95**) & 0.12 & -0.00 \\  & 4800 & **0.88** (**0.93**) & 0.08 & 0.00 & **0.89** (**0.92**) & 0.08 & -0.01 & **0.89** (**0.92**) & 0.08 & 0.00 \\  kNN \(n^{2/3}\) & 2400 & 0.84 (0.00) & 1.63 & 0.08 & 0.78 (0.00) & 1.68 & -0.38 & **0.85** (0.00) & 1.64 & -0.01 \\  & 4800 & **0.87** (0.00) & 1.11 & 0.03 & 0.66 (0.00) & 1.14 & -0.37 & **0.86** (0.00) & 1.11 & -0.03 \\  & 9600 & **0.88** (0.00) & 0.75 & 0.02 & 0.61 (0.00) & 0.77 & -0.28 & **0.88** (0.00) & 0.76 & -0.01 \\  Forest & 2400 & 0.77 (0.00) & 1.77 & 0.26 & 0.66 (0.00) & 1.83 & -0.37 & 0.72 (0.00) & 1.80 & 0.01 \\  & 4800 & **0.86** (0.00) & 1.19 & 0.47 & 0.47 (0.00) & 1.24 & -0.32 & **0.85** (0.00) & 1.20 & -0.03 \\  & 9600 & **0.85** (0.00) & 0.73 & 0.11 & 0.42 (0.00) & 0.76 & -0.28 & **0.90** (0.00) & 0.75 & -0.02 \\  =5,d_{y}=5)\)} \\  SAA & 1200 & 0.82 (**0.88**) & 0.04 & 0.00 & 0.82 (**0.87**) & 0.04 & 0.00 & **0.89** (**0.88**) & 0.04 & -0.01 \\  & 2400 & **0.90** (**0.89**) & 0.02 & -0.00 & **0.91** (**0.89**) & 0.02 & -0.00 & **0.92** (**0.89**) & 0.02 & -0.01 \\  kNN \(n^{1/4}\) & 2400 & 0.00 (0.00) & 0.17 & 1.72 & 0.76 (0.00) & 0.34 & -0.08 & **0.92** (0.00) & 0.33 & -0.00 \\  & 4800 & 0.00 (0.00) & 0.12 & 1.43 & 0.72 (0.00) & 0.23 & -0.04 & **0.88** (0.00) & 0.22 & -0.00 \\  & 9600 & 0.00 (0.00) & 0.09 & 1.11 & 0.66 (0.00) & 0.15 & -0.03 & **0.89** (0.00) & 0.14 & 0.000 \\  

Table 2: Evaluation performance of different methods, where boldfaced values mean **valid coverage** for \(c()\) (i.e., within [0.85, 0.95]) and boldfaced values in parantheses mean **valid coverage** for \(c(z^{*})\). IW and biases for kNN and Forest in the regression problem are presented in unit \( 10^{3}\). Results on other sample sizes and numerical reports on standard errors can be found in Tables 3 and 4 in Appendix F.

Our comparisons in model evaluation show that plug-in is preferable to \(K\)-fold CV, both statistically and computationally since plug-in works across a wider range of models and does not require additional model training. The above being said, we caution that, in terms of the direction of the bias, plug-in is optimistic while \(K\)-fold CV is pessimistic, and so the latter can be preferred if a conservative evaluation is needed to address high-stake scenarios. On the other hand, LOOCV provides valid coverages for the widest range of models, but it is computationally demanding.

Due to such computational complexity, some alternatives, including approximate leave-one-out (ALO) , bias-corrected \(K\)-fold CV  and bias-corrected plug-in , aim to control computation load while retaining the statistical benefit of LOOCV through analytical model knowledge. For example, ALO approximates each leave-one-out solution using the so-called influence function in parametric models. However, these ALO approaches are difficult to generalize in our problem setup due to difficulties in approximating the analytical form of influence function in nonparametric models (e.g., random forest). Other variants of cross-validation, e.g., fold number increasing with \(n\), can potentially help improve the statistical-computational tradeoff and we leave the investigations of these procedures as future work.

Model Evaluation versus Model Selection.We caution the distinction between model evaluation and selection, namely the selection of hyperparameter among a class of models. Depending on what this class is, our model evaluation comparisons may or may not translate into the performances in model selection. On a high level, this is because the evaluation bias may be correlated among different hyperparameter values and ultimately leading to a low error in the selection task. A general investigation of this issue in relation to model rates appears open, even though specific cases have been studied . For example, it has been pointed out that \(K\)-Fold CV can perform better for ridge or lasso linear models than plug-in for model selection . We provide further discussions on the failure cases of the plug-in procedure for model selection in Appendix G.1.

Asymptotic versus Nonasymptotic Behaviors.Our results are asymptotic and there is an obvious open question on extending to finite-sample results. Nonetheless, our results still shed light on the finite-sample performances of different approaches. For example, our numerical results, which show finite-sample coverage behaviors in Figure 1 and Table 2, conform to our asymptotic theories. Note that there are some non-asymptotic intervals based on concentration inequalities with exact coverage guarantees . However, they may be too loose as they are derived from a worst-case analysis.

High-Dimensional Problems.As we mentioned in Section 2, our paper focuses on the asymptotic setting where \(n\) and \(,,\) are of fixed dimensions. Future work includes investigating different model evaluation approaches under other regimes, including high-dimensional settings where both the dimension and sample size go to infinity. We provide some preliminary discussions of our three considered procedures in high-dimensional settings in Appendix G.2. In particular, in these situations,  find that standard cross-validation procedures (2) give low coverages and design a nested cross-validation procedure to remedy. However, this latter procedure is designed for high-dimensional parametric models and does not help in the slow rate regimes of nonparametric models in our setting. We provide further comparisons in Appendix G.3.

Smoothness and Stability.Despite the relative generality of our scope, we have assumed sufficient model smoothness and stability. Future work includes relaxations of these conditions to broader model classes and cost functions. This is also related to the extension of our analyses to ALO approaches, as these approaches require explicit smoothness, namely gradient-type estimates on the models, as well as other advanced CV approaches in, e.g., .