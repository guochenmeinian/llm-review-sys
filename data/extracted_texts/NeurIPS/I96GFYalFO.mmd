# FedSSP: Federated Graph Learning with Spectral Knowledge and Personalized Preference

Zihan Tan\({}^{1}\)   Guancheng Wan\({}^{1}\)   Wenke Huang\({}^{1}\)   Mang Ye\({}^{1,2}\)

\({}^{1}\) National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence,

Hubei Key Laboratory of Multimedia and Network Communication Engineering,

School of Computer Science, Wuhan University, Wuhan, China.

\({}^{2}\) Taikang Center for Life and Medical Sciences, Wuhan University, Wuhan, China

{zihantan,guanchengwan,wenkehuang,yemang}@whu.edu.cn

Equal contribution. Corresponding author.

###### Abstract

Personalized Federated Graph Learning (pFGL) facilitates the decentralized training of Graph Neural Networks (GNNs) without compromising privacy while accommodating personalized requirements for non-IID participants. In cross-domain scenarios, structural heterogeneity poses significant challenges for pFGL. Nevertheless, previous pFGL methods incorrectly share non-generic knowledge globally and fail to tailor personalized solutions locally under domain structural shift. We innovatively reveal that the spectral nature of graphs can well reflect inherent domain structural shifts. Correspondingly, our method overcomes it by sharing generic spectral knowledge. Moreover, we indicate the biased message-passing schemes for graph structures and propose the personalized preference module. Combining both strategies, we propose our pFGL framework **FedSSP** which **S**hares generic **S**pectral knowledge while satisfying graph **P**references. Furthermore, We perform extensive experiments on cross-dataset and cross-domain settings to demonstrate the superiority of our framework. The code is available at https://github.com/OakleyTan/FedSSP.

## 1 Introduction

Graph Neural Networks (GNNs) [56; 63; 48; 42] have demonstrated their superiority in modeling graph data which frequently emerges in a variety of scenarios [73; 71], as exemplified by graph clustering [45; 46; 44], graph contrastive learning [64; 80], anatomy detection [75; 88], knowledge graph [79; 78; 38], structural inference [66; 68; 65; 67] and so on. However, large amounts of graph data are generated by edge devices in reality, which brings in privacy concerns and the challenges of data silos [89; 24; 28]. To address these difficulties, federated learning (FL) has recently been applied to graph learning [18; 20; 25]. It allows models on various clients to collaborate without sharing local data [26; 15; 23; 27; 83] and makes federated graph learning (FGL) a promising direction. Nonetheless, the non-IID problem remains a major challenge in FGL, as graph data from different clients usually vary significantly. In such scenarios, a single global model struggles to adapt well to the local data of each client with inconsistent data distributions [60; 58]. To tackle these challenges, personalized federated graph learning (pFGL) has emerged, offering customized GNNs for each client to achieve satisfying local performance [1; 61].

However, pFGL still encounters substantial challenges from structural heterogeneity , especially in domain shift tasks, for instance, between social networks [94; 95] and molecular structures [59; 55]. There are two significant drawbacks to previous algorithms as Fig. 1 demonstrates. For globalcollaboration, the considerable domain structural shifts inevitably lead to non-generic knowledge, thus resulting in knowledge conflict. Both current methods suffer from conflict and are trapped in unpromising collaboration. Specifically,  share non-generic structural encoding and struggle with structural knowledge conflict, while strategy  mitigating conflicts by limiting the potential for collaboration. The key to addressing knowledge conflict is pursuing a way to share generic knowledge that benefits all clients. Based on this observation, we raise the question: 1) _how to address the **knowledge conflict** under domain structural shift by extracting and sharing generic knowledge?_

For local applications, each client owns its specific dataset with distinct structural characteristics in this cross-dataset scenario. Due to the GNN message-passing nature, distinct graph structures stored in different clients prefer different message-passing schemes. Consequently, the scheme provided by aggregated GNN exhibits biases from the optimum when applied locally, thus leading to unsuitable features. Both current methods neglect the preferences of various clients for specific graph structures. This deficiency leads us to consider: _2) how to design personalized plans to deal with **inconsistent preferences** of specific graph datasets from various clients?_

To address problem 1), given that structural shifts make it hard to directly achieve generic sharing at the structural level, we propose to explore the structure shifts from another spectral perspective since previous works have demonstrated the strong correlation between graph structure and spectra [2; 81; 43; 32]. The major advantage of spectra is the detailed propagation and processing of graph signals on the graph structure, thus facilitating the discovery of generic knowledge in several certain processes unaffected by structural shifts. To validate our assumption, we first conduct experiments and explicitly reveal the domain spectral biases that directly reflect domain structural shifts on spectra as Fig. 1 demonstrates. To tackle these spectral biases directly to overcome structural shifts, we design **Generic Spectral Knowledge Sharing** (GSK) to share generic spectral knowledge extracted from spectral encoders. It enables clients to benefit from others through collaboration without knowledge conflict. Conversely, other components containing non-generic knowledge are retained locally. Correspondingly, clients can customize powerful graph convolutions for their local graph characteristics while benefiting from generic knowledge without conflict. Through this strategy, we promote the sharing of generic spectral knowledge and the personalizing of non-generic knowledge, thus achieving effective collaboration against knowledge conflict.

Moreover, we attempt to achieve target 2) and design suitable personalized plans for each client graph structure locally. Specifically, we explore the message-passing nature of GNN [5; 16; 62]. From the spectral perspective, spectral encoders strongly affect message transmission. Therefore, when aggregated spectral encoders are applied to distinct graph structures locally, they tend to deviate from the optimal message-passing scheme for the client . Consequently, GNNs extract inappropriate frequency messages which lead to unsuitable features. To meet the inconsistent graph preferences, we innovatively configure a learnable preference for each client and propose **Personalized Graph Preference Adjustment** (PGPA). These personalized preference modules apply adjustments to the feature extracted with the participation of global spectral encoders. It allows the feature to suit the specific graph structures of each client. Moreover, to address the issue of over-reliance when applying the preference module independently, a regularization term is introduced. Combining both strategies

Figure 1: **Problem illustration.** We illustrate the challenges of the cross-domain scenario. (a) Considering the domain structural shifts, clients struggle with **knowledge conflict** caused by non-generic sharing which arises from the shifts, thus leading to unpromising global collaboration. (b) The aggregated message-passing scheme suffers from **inconsistent preferences** that remain **unsatisfied** of specific datasets in this scenario. Consequently, it leads to unsuitable features of graphs in local applications. (c) The heat map of Jensen-Shannon divergence of algebraic connectivity  and eigenvalues distributions among six datasets from three different domains. Spectral characteristics exhibit significant **biases across** domains but are more **similar** within a **same** domain.

for effective global collaboration and personalized local application, we propose our pFGL framework FedSSP. In conclusion, our key contributions are:

* We are the first to reveal domain structural shifts through spectral biases, as well as consider the inconsistent preferences of distinct datasets from various clients.
* We propose FedSSP, which innovatively overcomes knowledge conflicts from a spectral perspective and implements personalized graph preference adjustments for each client.
* We conduct experiments in various cross-dataset and cross-domain settings, proving that our approach outperforms several current state-of-the-art methods and achieves optimal results.

## 2 Related Work

### Spectral GNNs

Spectral GNNs [4; 12] are based on spectral graph filters set in the spectral domain, providing powerful models for graph neural networks [76; 87; 86; 47; 72; 10; 70; 8; 69; 7]. Spectral GNNs can generally be categorized into two types: those with fixed filters and those with learnable filters. Fixed filter spectral GNNs, such as APPNP , utilize personalized PageRank (PPR)  to construct their filtering functions. GNN-LF/HF  designs filter weights from the perspective of graph optimization functions. Learnable filter spectral GNNs include subclass that approximates arbitrary filters using various types of orthogonal polynomials, including Bernstein , Chebyshev , and Jacobi . Another subclass parameterizes the filters by neural networks, including LanczosNet  and Specformer. The robust modeling capability of spectral graph neural networks on data inspires us to leverage this foundation to tackle the issue of structural heterogeneity across domains.

### Personalized Federated Learning

Federated learning [84; 34; 28; 14; 82] facilitates privacy-preserving collaborative learning on local data, introducing methods like FedAvg  to address this. Yet, it struggles with non-IID data across clients. Several techniques aim to address the challenge [33; 35; 30], but achieving a global model that suits all local data remains difficult . Personalized Federated Learning (pFL) has attracted increasing attention for its ability to address the non-IID problem [13; 39; 60]. Research has approached improvements from various aspects. In personalized-aggregation-based methods, FedPHP aggregates the global model and old personalized models locally to preserve historical information , FedALA achieves personalized aggregation through personalized masks , and APPLE uploads only core models and employs directed relationship vectors for downloading . In model-splitting-based approaches, FedRoD  learns with a global feature extractor and two heads for both global and personalized tasks. FedCP decouples features suitable for global and local heads through a conditional policy scheme . Moreover, methods based on regularization and knowledge distillation have also been utilized to enhance pFL. However, pFL methods lack targeted strategy designs for graphs, making them not particularly suited for pFGL scenarios.

### Federated Graph Learning

Recent studies have utilized the FL framework for distributed training of GNNs without sharing graph data [20; 41; 28; 9]. Current Federated Graph Learning (FGL) research can be categorized into two types: intra-graph and inter-graph FGL. In inter-graph FGL, each client has distinct graphs, and they jointly participate in federated learning to either improve GNN modeling of local data or achieve a model that can generalize across different datasets [77; 61]. Intra-graph FGL, on the other hand, aims to deal with challenges such as missing link prediction , subgraph community detection [93; 1], and node classification [25; 37]. However, most FGL methods lack specific design considerations for our scenario. More precisely, there is a general absence of consideration for the heterogeneity of graph-level structures and the personalized needs of different clients brought about by structural characteristics. In this paper, we focus on inter-graph FGL, taking into account spectral domain biases and the uniqueness of graph structures that result in client-specific preferences, to customize a personalized optimal model for each client specifically for graph classification tasks.

Preliminary

### Graph Signal Filter

Assume that we have a graph \(=(,)\), where \(\) represents the node set with \(||=n\) and \(\) is the edge set. The corresponding adjacency matrix is defined as \(A\{0,1\}^{n n}\), where \(A_{ab}=1\) if there is an edge between nodes \(a\) and \(b\), and \(A_{ab}=0\) otherwise. The normalized graph Laplacian matrix is defined as \(=I_{n}-D^{-1/2}AD^{-1/2}\), where \(I_{n}\) denotes the \(n n\) identity matrix and \(D\) is the diagonal degree matrix. We assume \(\) is undirected. \(\) is a real symmetric matrix, whose spectral decomposition can be written as \(=U U^{T}\), where the columns of \(U\) are the eigenvectors and \(=(_{1},_{2},,_{n})\) are the corresponding eigenvalues ranged in \(\). The Graph Fourier transform of a signal \(x^{n d}\) is defined as \(=U^{T}x^{n d}\). The inverse transform is defined as \(x=U\). The \(i\)-th column of \(U\) denotes a frequency component corresponding to the eigenvalue \(_{i}\). Let \(_{}=U_{}^{T}x\), where \(U_{}\) represents the eigenvector corresponding to \(\), be the frequency component of \(x\) at \(\) frequency. We can use a function \(g:\) to filter each frequency component by multiplying \(g()\). By defining \(=diag()\), \(g\) implements filtering on \(\), thus ultimately implementing filtering on the graph signal \(x\). The whole process is defined as follows:

\[Ug()U^{T}x.\] (1)

By defining \(g()=_{k=0}^{K}_{k}^{k}\), where \(g\) is often set to be a polynomial of degree \(K\) for parameterizing the filter, the filtering process can be rewritten as follows:

\[Ug()U^{T}x=g()x.\] (2)

### Federated Learning and Personalized Federated Learning

Traditional FL leverages isolated data of distributed clients and collaboratively learns models \(\) for a generalizable global model without leaking privacy. Specifically, the goal is to minimize:

\[_{}f_{g}()=_{}_{i=1}^{N}w_{i}_{i}( ),\] (3)

where \(f_{g}()\) denotes the global objective. It is computed as the weighted sum of the \(N\) local objectives, with \(N\) being the number of clients and \(w_{i} 0\) being the weights. The local objective \(_{i}()\) is often defined as the expected error over all data under local data \(_{i}\).

In the context of personalized federated learning, the global objective takes a more flexible form:

\[_{}f_{p}()=_{_{i},i[N]}_{i=1}^{N}w _{i}_{i}(_{i}),\] (4)

where \(f_{p}()\) is the global objective for the personalized algorithms, and \(=[_{1},_{2},,_{N}]\) is the matrix with all personalized models. In this work, we aim to obtain the optimal \(^{}=_{}f_{p}()\), which equivalently represents the set of optimal personalized models \(_{i},i[N]\).

## 4 Methodology

### Generic Spectral Knowledge Sharing (GSKS)

**Motivation**. Current methods suffer from knowledge conflict arising from non-generic sharing under domain structural shifts. Since structural shifts impede the direct generic sharing at the structural level, we are the first to reveal and resolve knowledge conflicts from the spectral perspective. To explicitly address the spectral biases that reflect structural shifts in Fig. 1, we base our pFGL strategy on spectral GNNs and further propose Generic Spectral Knowledge Sharing (GSKS). Effective collaboration that overcomes spectral bias and structural shift is achieved, thereby addressing knowledge conflict. Details of GSKS are presented in Fig. 2 (a).

**Eigenvalue filtering**. Aiming at more expressive representations of frequency information, the eigenvalues are firstly mapped from scalars to meaningful vectors for subsequent learning of frequency interrelation in the multi-head attention module as follows:

\[^{e}(^{e};)=(/c^{q/d}),&,\\ (/c^{(q-1)/d}),&,\] (5)

where \(c\) keeps eigenvalues within a suitable numerical range to distinguish different eigenvalues for trigonometric functions. \(q[0,d-1]\) is the index for dimension \(d\) while \(\) controlling the importance of \(\) with defaulted value \(10000\). Moreover, \(^{e}\) denotes parameters of the eigenvalue encoder \(^{e}\), by which eigenvalues are mapped from scalars to vectors for richer frequency information. Consequently, they are more expressive for filtering by the attention module and decoder through \(^{1}^{d}\). The initial representations are the concatenation of eigenvalues and their encodings: \(^{}=[concat[_{1},^{e}(^{e};_{1})], ,concat[_{n},^{e}(^{e};_{n})]]^{T}^ {n(d+1)}\). Then the multi-head attention module is leveraged. After stacking multiple transformer blocks, spectral decoder \(^{d}\) for eigenvalue decoding can learn new eigenvalues from the expressive representations of spectra:

\[_{m}=^{d}((Q_{m}^{Q},K_{m}^{K},V _{m}^{V})),\] (6)

Where the representations learned by the \(m\)-th head are fed into \(^{d}\), while \(^{d}\) denotes a combination of liner and optional activation. \(_{m}^{n 1}\) is the filtered eigenvalues by the \(m\)-th head. The whole process in Eq. (6) acts as a spectral filter \(g\) for the origin eigenvalues in Eq. (1).

To address the challenge of knowledge conflict, we attempt to explore the functionality of each module. The eigenvalue encoder \(^{e}\) captures multi-scale representations of eigenvalues and provides meaningful vectors of distinct frequencies. Since the mapping from eigenvalues to vectors by \(^{e}\) is independent of the domain characteristics, \(^{e}\) of \(^{e}\) is considered to contain generic knowledge. In contrast, as the spectral biases we reveal in Figure 1 demonstrate, biases exist in eigenvalue distribution across domains. In contrast, spectral characteristics within the same domain are more similar. Therefore, the attention module learns the non-generic magnitudes and relative dependencies specific to the spectral characteristics of each client. Correspondingly, the eigenvalue decoder focuses on decoding the most suitable message-passing scheme and client-specific frequency components from the representation processed by the attention module. Attention module and decoder together formed \(g\) in Eq. (1), aiming at designing personalized filtered eigenvalue that guides message-passing at a personalized suitable frequency. Therefore, \(^{e}\) is shared in our strategy to achieve generic spectral knowledge sharing and effective collaboration unaffected by knowledge conflict.

Specifically, client \(i\) uploads its update of \(_{i}^{e}\). At the \(t\)-th iteration (\(t 0\)), the central server distributes global spectral encoder weights \(_{g}^{t}\). Accordingly, client \(i\) updates local GNN weights including \(_{i}^{e}\) with their dataset \(_{i}\) and send these updates as \(_{i}^{t}=_{i}^{t}-_{g}^{t}\) to the central server. Then the server aggregates the received local updates and modifies the global weight \(_{g}^{t+1}\) as follows:

\[_{g}^{t+1}=_{g}^{t}+^{N}_{i}^{t}}{N}( i[1..N]),\] (7)

notably, aggregation based on sample size is unsuitable in this scenario for effective collaboration across various domains and datasets. Since clients here possess specific datasets with significant

Figure 2: **Architecture illustration** of FedSSP. The left box (a) refers to Generic Spectral Knowledge Sharing (GSKS), where we address knowledge conflict and promote effective global collaboration by **sharing generic** spectral knowledge extracted from spectral encoders \(^{e}\) and \(^{f}\) while **retaining non-generic** in other components. The right box (b) represents Personalized Graph Preference Adjustment (PGPA), where we leverage **preference module** guided by \(_{i}^{PGPA}\) for satisfying inconsistent preferences and achieving suitable feature of datasets locally. These two boxes correspondingly refer to the two core strategies of our framework FedSSP.

quantitative variance, those with larger datasets tend to dominate the collaboration. Thereby preventing them from benefiting from the spectral and frequency knowledge of clients with fewer samples. Correspondingly, clients with fewer graphs are almost entirely dominated by knowledge that does not originate from their local data. To address the problem, we leverage a direct average of spectral encoder weights from all clients to achieve fair collaboration and cross-dataset knowledge sharing.

**Personalized graph convolution constructing.** After getting \(M\) filtered eigenvalues, filter encoder: \(^{f}(^{f};B)\)\(^{M+1}^{d}\) is leveraged to construct the bases for personalized graph convolution. To avoid confusion and distinguish from the mentioned filter \(g\) on eigenvalues in Eq. (1), filter here in filter encoder refers to the filtering on feature message-passing through bases in graph convolution. New bases are first reconstructed and concatenated along the channel dimension. Specifically, by defining \(_{m}=diag(_{m})\), they are fed into filter encoder \(^{f}\) as follows:

\[B_{m}=_{m}^{T}, m\{1,,M\},\] (8)

\[=^{f}(^{f};B),\] (9)

where \(B=[B_{1},B_{2},,B_{M}]^{n n M}\) while \(B_{m}^{n n}\) is the \(m\)-th new basis and \(^{n d}\) is for feature filtering in graph convolution ultimately. The original bases \(B_{m}\) are initially constructed from the eigenvectors \(U\) and the filtered eigenvalues \(_{m}\), with \(^{f}(^{f};B)\) responsible for the learnable transformation of bases from original to new. This transformation facilitates learning more suitable schemes for graph message-passing and processing at various frequencies. Filter encoders \(^{f}(^{f};B)\) in clients encapsulate knowledge of various frequency components which affects how much graph signal varies from nodes to their neighbors for better graph convolution construction. Due to restrictions on the sample size for certain clients, they are unable to adequately learn message-passing techniques for handling specific frequency components. As a solution, the filter encoder is shared among clients, enabling them to fully acquire the graph signal processing methods for frequencies that are challenging to learn locally. Specifically, collaboration on filter encoder can aid \(^{f}(^{f};B)\) of each client in learning how to construct suitable graph convolution from various message-passing schemes. Therefore, we design client \(i\) to upload the weights \(_{i}^{f}\) of its \(_{i}^{f}\) the same way as \(_{i}^{e}\) Eq. (7), thereby achieving a comprehensive understanding of different frequency messages in graphs. Subsequently, it enables the construction of powerful personalized graph convolutions as follows:

\[x_{v}^{}=(( x_{v})^{Conv})+x_{v},\] (10)

where \(x_{v}\) is the node \(v\) representation from the previous layer, while \(x_{v}^{}\), represents the output of the current layer. \(^{Conv}\) denotes weights of graph convolution, and \(\) refers to the optional activation. Ultimately, the representations of all nodes within a graph are aggregated by an average pooling layer to form the overall feature representation of graph \(_{l}\) in dataset \(_{i}\) of client \(i\) as follows:

\[h_{l}=|}_{v=1}^{||}x_{v}, l \{1,,|_{i}|\},\] (11)

where \(h_{l}\) is defined as the average of all node features in graph \(_{l}\), namely the graph feature, while \(\) refers to the node quantities in graph \(l\) here. By sharing generic spectral knowledge and retaining client-specific knowledge we successfully achieve effective collaboration that overcomes spectral bias, thereby domain structural shift from the spectral perspective.

### Personalized Graph Preference Adjustment (PGPA)

**Motivation.** Due to the GNN message-passing nature, distinct graph structures prefer different message-passing schemes, especially when meeting the specificity of datasets in cross-dataset and cross-domain scenarios. Consequently, The spectral encoders under global collaboration fail to satisfy the inconsistent local preferences of graphs. Correspondingly, graph convolutions tend to learn biased message-passing schemes, thereby extracting unsuitable graph features. Our approach provides personalized adjustments to address this challenge based on client preference. Moreover, we solve the over-reliance issue that arises from the process of satisfying various preferences. Details of GSK are presented in Fig. 2 (b).

To satisfy the various preferences and make the graph features more suitable for graphs, we propose a learnable preference module that adjusts to features extracted by client \(i\) to satisfy local graph structure preference explicitly. The module includes learnable parameters matched in size with the feature space, thus acting as a refined tool to flexibly satisfy the preferences of each client during local training. Considering local model \(\) including feature extractor \((^{F};)\), classification head \((^{C},h)\), and preference module \(()\), where \(\) represents graphs contained in dataset \(\) of a client. The whole graph feature-extracting process can be defined as follows in our strategy:

\[h=(^{F};), h^{}:=h+,\] (12)

by integrating the original feature \(h\) with preference adjustments \(\), \(h^{}\) becomes the ultimately suitable feature that satisfies client preference. Now we leverage adjusted feature \(h^{}\) for \(z^{}=(^{C};h^{})\) instead of the original unsuitable representation \(h\). Specifically, the local loss for client \(i\) is:

\[_{i}=_{(z^{}_{i},y_{i})_{i}} _{i}^{CE}=_{(z^{}_{i},y_{i})_{i}} CE(z^{}_{i},y_{i}),\] (13)

where the Cross-entropy (CE) loss measures the difference between the predicted probability and the true label. Nevertheless, the preference module learns not only the client-specific preference but also aspects that should be handled by the feature extractor \(\) without a guide for preference. As a result, the local feature extractor \(\) might overly rely on adjustments provided by the preference module during training, thereby hindering its capability. Correspondingly, this over-reliance can negatively impact federated collaboration. When the capability of \(\) is degraded, the shared spectral encoders fail to convey beneficial knowledge to others, leading to unpromising collaboration.

Therefore, it is essential to guide the preference module to focus solely on the aspects of client preferences, rather than interfering with the feature extraction guided by collaboration. We achieve this by guiding the output of the feature extractor to align more closely with global graph features. Consequently, the PGPA module is directed to concentrate on client preferences. To implement this, we first calculate the mean of local graph features in each iteration:

\[_{i}=(1-)_{i}^{}+_{i}^{},\] (14)

where \(\) denotes the momentum we introduce for bringing graph modeling patterns from previous batches to the current batch in the same local epoch. \(_{i}^{}\) and \(_{i}^{}\) represent the local mean graph feature of the previous batches and the current batch. It is necessary to distinguish between mean and prototype. In this scenario, clients own various datasets, thus the class information is client-specific. Correspondingly, the mean \(h_{i}\) which represents the average modeling for graphs in client \(i\) is unrelated to class information. After local training, client \(i\) uploads its mean to the server for global consensus aggregation. Based on our exploration of Eq. (7), a direct average is leveraged here:

\[_{g}=^{N}_{i}}{N},\] (15)

where \(_{g}\) refers to the global graph modeling consensus calculated from all samples across all clients. Accordingly, we employ the Mean Squared Error (MSE) to measure the distance between the local graph feature mean and the global graph mean obtained from the previous round. This measurement serves as a regularization term to encourage the local graph feature to align closer to the global modeling consensus, thus guiding the preference module to focus on preference and correspondingly address the over-reliance issue. Specifically, local feature extractors are encouraged to extract certain frequency messages in graphs that reflect the global modeling consensus, making the PGPA only responsible for client-specific preference. The local loss of client \(i\) is now defined as:

\[_{i}=_{(z^{}_{i},y_{i})_{i}}( _{i}^{CE}+_{i}^{PGPA})=_{(z^{}_{i},y_{i}) _{i}}[(z^{}_{i},y_{i})+(_{i},_{g})].\] (16)

By implementing \((},_{g})\), we explicitly align local graph modelings with the global consensus, thus guiding the preference module \(\) to focus on preference and addressing the considered issue of over-reliance by forcing the preference module to focus on client-specific graph preference.

## 5 Experiments

### Experimental Setup

We perform experiments on graph classification tasks in various cross-dataset and cross-domain scenarios to validate the superiority of our framework FedSSP.

**Datasets.** Follow the settings in , we use 15 public graph classification datasets from four different domains, including Small Molecules (MUTAG, BZR, COX2, DHFR, PTC_MR, AIDS,NCI1), Bioinformatics (PROTEIN, OHSU, Peking_1), Social Networks(IMDB-BINARY, IMDB-MULTI), and Computer Vision (Letter-low, Letter-high, Letter-med) . Node features are available in some datasets, and graph labels are either binary or multi-class. We create six different settings: (1) cross-dataset setting based on seven small molecules datasets (SM); (2)-(6) both cross-dataset and cross-domain settings based on datasets from two different domains(BIO-SM, SM-CV) and three different domains(BIO-SM-SN, BIO-SN-CV, CHEM-SN-CV)

**Baselines.** We compare ours with several SOTA federated approaches. (1)**Local** as the first baseline; (2)**FedAvg**; (3)**FedProx** which address heterogeneity issues in FL; (4)**APPLE** and (5)**FedCP**, two state-of-the-art pFL method;(6)**FedSage**, (7)**GCFL**,(8)**FGSSL**, and (9)**FedStar**, four state-of-the-art FGL methods.

**Implementation Details.** The experiments are conducted using NVIDIA GeForce RTX 3090 GPUs as the hardware platform, coupled with Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz. For each setting, every client holds its unique graph dataset, among which 10% are held out for testing, 10% for validation, and 80% for training. We leverage the AdamW optimizer  for local GNNs with learning rate 0.001, the default parameter of \(=1e-8\), and \((_{1},_{2})=(0.99,0.999)\), as suggested by [54; 85]. The number of communication rounds is 200 for all FL methods. We report the results with an average of over 5 runs of different random seeds.

### Experimental Results

**Performance Comparison** We show the federated graph classification results of all methods under six different non-IID settings, including one cross-dataset setting (SM), two cross-double-domain settings (BIO-SM, SM-CV) cross-multi-domain settings (BIO-SM-SN, BIO-SN-CV, SM-SN-CV). We summarize the final average test accuracy in Tab. 1. These results indicate that FedSSP outperforms all other baselines in five out of the six settings. Notably, traditional FL algorithms such as FedAvg and FedProx failed to outperform self-training due to the strong cross-datasets and cross-domain non-IID challenge of this scenario. Correspondingly, algorithms such as FedStar and FedCP which are designed specifically for pFGL or pFL scenarios perform better here.

**Convergence Analysis** Fig. 3 shows the curves of the average test accuracy with standard deviation during the training process across five random runs of three settings (SM, SM-CV, SM-SN-CV) representing single-domain, double-domain, and multi-domain scenarios, including the results of various baselines. As is shown, traditional FL methods such as FedAvg or FedProx own higher standard deviations and are more unstable while methods designed specifically for pFGL scenarios such as GCFL and FedStar are more stable with lower standard deviations.

### Ablation Study

**Effects of Key Components Mechanism of FedSSP** To better understand the impact of specific design components on the overall performance of FedSSP, we conducted an ablation study in which

    & single-domain &  &  \\   & SM & SM-BIO & SM-CV & SM-BIO-SN & BIO-SN-CV & SM-SN-CV \\   Local & \(77.33 1.15\) & \(72.52 1.86\) & \(82.24 1.73\) & \(71.13 1.32\) & \(72.59 2.70\) & \(77.83 0.54\) \\  FedAvg [ASTA171] & \(74.12 2.10\) & \(67.82 1.63\) & \(81.21 1.00\) & \(67.31 2.56\) & \(70.93 2.91\) & \(75.33 1.06\) \\ FedProx [arXiv18] & \(69.35 3.36\) & \(67.27 4.17\) & \(70.02 2.27\) & \(63.89 4.33\) & \(69.32 1.75\) & \(67.15 2.25\) \\ FedSage [NeurIPS21] & \(75.61 1.16\) & \(72.60 3.18\) & \(76.23 0.49\) & \(70.84 0.88\) & \(69.69 1.11\) & \(73.36 0.86\) \\ GCFL [NeurIPS21] & \(77.71 1.53\) & \(72.05 2.20\) & \(72.64 0.71\) & \(70.43 1.39\) & \(67.91 2.15\) & \(71.79 0.21\) \\ APPLE (IJCAI22) & \(74.29 1.89\) & \(70.40 2.13\) & \(76.07 2.55\) & \(71.07 1.64\) & \(72.52 1.03\) & \(72.33 0.42\) \\ FedCP (KDD23) & \(77.58 2.00\) & \(71.15 1.77\) & \(81.59 0.40\) & \(71.32 1.23\) & \(73.74 2.53\) & \(78.17 1.78\) \\ FGSSL (IJCAI23) & \(77.90 0.85\) & \(72.47 2.15\) & \(82.60 0.48\) & \(68.13 1.71\) & \(73.44 1.33\) & \(77.90 0.62\) \\ FedStar (AAAI23) & \(78.63 2.11\) & \(72.71 1.22\) & \(78.84 1.07\) & \(\) & \(69.51 3.24\) & \(75.94 0.40\) \\ 
**FedSSP (ours)** & \(\) & \(\) & \(\) & \(72.37 2.18\) & \(\) & \(\) \\   

Table 1: Comparison with the state-of-the-art methods on one cross-dataset and five cross-domain settings. Best in bold and second with underline. In each setting, each client owns a unique dataset.

we varied these components of single, double, and multi-domain settings(SM, SM-CV, SM-SN-CV). As shown in Tab. 2, compared to FedAvg, GSKS significantly enhances accuracy when applied independently. Correspondingly, as a further exploration of the nature of GNN message passing, PGPA achieves noticeable success in adjusting client preferences.

**Effects of Key Component Mechanism of GSKS** Tab. 3 discuss the key component of GSKS. We demonstrated the impact of different sharing strategies. Specifically, sharing only non-generic spectral GNN components or all components often fails to outperform self-training, while GSKS successfully dominates all the strategies. Accordingly, the results fully validate the effectiveness of GSKS in sharing universal knowledge and promoting effective collaboration in this scenario.

### Hyper-parameter Study

We compare the graph classification performance under different values of PGPA parameter \(\), momentum \(\), number of attention heads, and hidden dimension. Where Fig. 4 shows the results when these hyper-parameters are fixed at different scales and values. It indicates that the choosing of \(\) can affect the strength of PGPA while performance is not influenced much unless they are set to extreme values. All studies of \(\) and \(\) here outperform the baseline. We also test the performances under different values of attention heads and hidden dimensions. For results in Tab. 1, we set up \(4\) heads for the multi-head attention mechanism while \(128\) for the hidden dimension.

   &  &  \\  & & SM & SM-CV & SM-SN-CV \\   \(\) & \(\) & 74.12 & 81.21 & 75.33 \\ ✓ & \(\) & 77.83 & 82.78 & 78.54 \\ � & ✓ & 74.59 & 81.33 & 76.12 \\ ✓ & ✓ & **79.62** & **84.29** & **79.12** \\  

Table 2: **Ablation study of key components of FedSSP on single-domain, double-domain, and multi-domain settings (SM, SM-CV, SM-SN-CV).**

Figure 4: **Analysis on hyper-parameter in FedSSP. Graph classification results under different \(\), \(\), attention heads, and hidden dimensions. Colors green, blue, and yellow refer to performance on single, double, and multi-domain settings (SM, SM-CV, SM-SN-CV). The dashed lines of corresponding colors represent the baseline test accuracy for each setting, which includes only the GSKS strategy.**

Figure 3: Test accuracy curves of FedSSP and six other methods along the communication rounds on our three different settings(SM, SM-CV, SM-SN-CV). The y-axis range is from 65 to 85 for all settings.

   &  &  \\  & & SM & SM-CV & SM-SN-CV \\   \(\) & \(\) & 77.33 & 82.24 & 77.83 \\ ✓ & ✓ & 74.12 & 81.21 & 75.33 \\ ✗ & ✓ & 77.21 & 81.64 & 78.17 \\ ✓ & ✗ & **77.83** & **82.78** & **78.54** \\  

Table 3: **Ablation study of key components of GSKS on a single-domain, double-domain, and multi-domain settings (SM, SM-CV, SM-SN-CV).**Discussion

Even though FedSSP has achieved significant success in cross-domain federated graph learning collaborations, it still faces certain limitations as a spectral GNN-based approach. Compared to spatial GNNs, while spectral GNNs have the advantage of overcoming structural heterogeneity from the spectral domain, many spectral GNNs require eigenvectors and eigenvalues, which adds to the computational overhead of data preprocessing and subsequent storage burden.

Furthermore, we notice a similar approach in DBE  which employs static global consensus in FL to separate personalized and global information. Nevertheless, it inevitably struggles to handle scenarios where the message-passing of multiple GNNs is continuously updated. It merely provides a static anchor point, making it difficult to establish a global graph modeling consensus that could guide the local GNNs in capturing graph signals. Instead, we align the GNN backbone with dynamic global graph modeling to avoid the preference module from overly extracting features that should be captured by the GNN itself, which could lead to decreased GNN performance and hinder global collaboration. This approach allows for real-time adjustment of message-passing across different client GNNs, focusing the preference module solely on personalization. Additionally, to address issues such as sample size disparity between domains and dominance of large domains in model parameter aggregation, we adopt a direct averaging strategy in dynamic global aggregation instead of conventional weighted averaging to mitigate these concerns.

## 7 Conclusion

In this paper, we pioneer an innovative exploration of cross-domain personalized Federated Graph Learning. To achieve this goal, we achieve improvements in two aspects: seeking effective global collaboration and suitable local application, thus proposing a novel framework FedSSP. For global collaboration, GSK is leveraged to facilitate the sharing of generic spectral knowledge, overcoming knowledge conflict by domain structural shift from a spectral perspective. For local applications, we design PGPA to satisfy inconsistent preferences of specific datasets contained in clients. By integrating these two strategies, FedSSP outperforms various state-of-the-art methods on various cross-dataset and cross-domain pFGL scenarios in graph classification.

## 8 Acknowledgment

This work is supported by National Natural Science Foundation of China under Grant (62361166629, 62176188, 623B2080), Key Research and Development Project of Hubei Province (2022BAD175), and the Luojia Undergraduate Innovation Research Fund Project of Wuhan University. The numerical calculations in this paper have been supported by the super-computing system in the Supercomputing Center of Wuhan University.