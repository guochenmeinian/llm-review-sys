# Leveraging Visual Tokens for Extended Text Contexts

in Multi-Modal Learning

Alex Jinpeng Wang\({}^{1}\), Linjie Li\({}^{2}\), Yiqi Lin\({}^{1}\), Min Li\({}^{3}\),

**Lijuan Wang\({}^{2}\), and Mike Zheng Shou\({}^{1 0}\)**

\({}^{1}\)Show Lab, National University of Singapore \({}^{2}\)Microsoft \({}^{3}\)Central South University

###### Abstract

Training models with longer in-context lengths is a significant challenge for multi-modal machine learning due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present Visualized In-Context Text Processing (VisInContext), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs) for both training and inferencing stage. For instance, our method expands the pre-training in-context text length from 256 to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that model trained with VisInContext delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, VisInContext is complementary to existing methods for increasing in-context text length and enhances document understanding capabilities, showing great potential in document QA tasks and sequential document retrieval. The code is available at https://github.com/showlab/VisInContext.

## 1 Introduction

Large Language Models (LLMs), such as OPT, Mistral, and LLaMA-2 , have significantly advanced the field of Natural Language Processing (NLP). These advancements are partly due to the increased capability of LLMs to process long contexts, from 512 tokens  up to 16K tokens . Building on these developments, recent multi-modal learning research  has shifted focus from simple image-text pairs, like those in CC3M  and LAION-400M , to more complex and lengthy interleaved document datasets. Examples include web corpora like MMC4  and the OBELICS  dataset, as well as PDF corpora like DocVQA .

However, training models on these complex datasets presents significant challenges due to the increased GPU memory and computational demands of extended contexts. For instance, while processing just 5M data items from MMC4 and 10M from the OBELICS dataset, OpenFlamingo-9B  resorted to sub-sampling text and processing only 256 tokens at a time, yet it still requires 32 80GB A100 GPUs for over three days. This highlights the need for more computation-efficient methods to handle long context lengths effectively.

In the domain of LLMs, two popular methods to extend context length are the use of memorizing banks  and novel self-attention mechanisms . These methods have inspired advancements in the multi-modality domain as well. For example, the Large World Model  introduces Ring Attention , and MA-LMM  employs memory banks to process long video understanding tasks. While these techniques have shown promise, our approach aims to increase in-context text lengthby leveraging the strengths of visual encoders in MLLMs. **We first observe that existing MLLMs usually exploit a much lighter visual encoders, compared to its text decoders**. For instance, Flamingo-9B consists of a 304.4M ViT-L/16  as image encoder, and a 7.1B Chinchilla  model as the text decoder. Additionally, previous works [22; 23] have demonstrated that visual encoders trained on paired image-text data also **exhibit emergent OCR capabilities**.

Motivated by these observations, we propose Visualized In-Context Text Processing (VisInContext), a method that uses visual tokens to process extended textual contexts, which is complementary of existing methods in extending context length. Specifically, we **convert long textual content into images and use the visual encoders to extract textual representations**. In this way, we can efficiently and effectively enable models with much longer text contexts, as shown in Figure 1. With VisInContext, we show that the in-context text length can be increased by 7 times over the competing baseline. Additionally, we observe almost the same overall computation FLOPs even as in-context length extends significantly. Our extensive experiments will also show that VisInContext renders superior model performance on conventional in-context few-shot evaluations and document understanding, with much lower computational cost.

**Contributions.** In summary, our contributions are as follows: _i._ We introduce Visualized In-Context Text Processing (VisInContext), a novel method that increases in-context text length using visual tokens. VisInContext directly compresses text context at input-level, which is complementary to existing techniques with improved self-attention or memory banks. _ii._ We demonstrate that VisInContext is effective for both training and inference stage with much lower computational cost. _iii._ With extended text context brought by VisInContext, our model improves the average in-context few-shot performance from 55.8% to 57.8% over the competing baseline. _iv._ As a byproduct, our method also shows great potential in document understanding on popular document QA tasks and our newly proposed sequential document retrieval task.

## 2 Method

The goal of VisInContext is to process in-context text using visual tokens so that the model can handle long text context more efficiently. We primarily base our study on Flamingo-based architecture [1; 9; 14], as it has shown success in improving a model's ability to learn from long multimodal context that contains arbitrarily interleaved text and images.

### Terminology

Before diving into model details, we define the following terms:

Figure 1: **VisInContext significantly increases the in-context text length from 256 to 2048 during pre-training on NVIDIA H100 GPU. For our method, we incorporate VisInContext after 128 text tokens. We implement PyTorch Flamingo  models with different in-context length during pre-training. The language model is a 56B MOE  model loaded with 4-bit quantization and the batch size on each GPU is 32 with FP16. We train the model with DeepSpeed  Zero-2.**

_In-context Text Length_: The **actual length of text tokens observed by the model within a document**.

_Text Token Length_: The length of the **text sequence input directly to the LLM**, corresponding to the token count of this sequence.

With VisInContext, the _In-context Text Length_ is greater than the text token length, as part of the text is represented using visual tokens.

### Overall Architecture

The implementation and architecture of VisInContext are shown in Figure 2. It is based on a dual-stream encoder model that integrates both visual and textual data. To effectively handle long interleaved data, we use a pre-sampling strategy as in Flamingo-style works [1; 9; 14]. We sample \(m\) images \(I_{1},I_{2},,I_{m} I\) with corresponding texts \(T_{1},T_{2},,T_{m} T\). Tokens are concatenated in the form \(<_{1}><_{1}><_{m}><_{m }>\), where \(<>\) is a single placeholder token. A random 256-token sequence is then sampled. However, since the overall length of a web document is generally much longer than 256 tokens (_In-context Text Length_\(\)_Text Token Length_), this sampling approach can lead to the omission of a lot of related text context.

To address this issue, we convert these omitted text context into visual signals by rendering them into images. We first concatenate all omitted text segments and divide them into \(K\) parts to render text images, named \(T_{1}^{{}^{}},T_{2}^{{}^{}},,T_{m}^{{}^{}} T^{}\). Both the original images and the text-rendered images are then processed through a shared frozen vision encoder. Then, we employ two learnable resamplers to extract a fixed number of tokens from both the raw and text-rendered image features, respectively. To facilitate the model to learn from rendered text images, we introduce two novel model designs, Token Masking mechanism and Text-Centric Contrastive Learning (TCCL). Token Masking allows the model to only read from text image tokens by masking the raw image tokens with masking ratio 1, which ensures that the model won't simply be ignoring the text images during training, hence can learn the association between the rendered text images \(\{T_{i}^{{}^{}}\}\) and the text tokens \(\{T_{i}\}\). TCCL aligns the visual text representation from the resampler with the embeddings extracted from text tokenizers in LLM, which reduces the gap between our visual text tokens and the text tokens the LLM is trained to perceive. With these designs, VisInContext not only reduces computational demands--as evidenced by a reduction in flops and inference time--but also improves the OCR ability, as we will show in our experiments.

### Text Rendering

This module converts textual data into a visually rich RGB format, specifically rendering the text into an image size of \(p_{h} np_{w}\), where \(n\) is the number of patches. We employ the _HERSHEY_ font at a size of 10px. On average, one 16x16 patch accommodates approximately 1.5 OPT text tokens. A 224x224 text image contains about 294 text tokens. Consequently, a visual encoder operating on this rendered text image requires only \(1/3\) of tokens to encode an equivalent amount of text, compared

Figure 2: **VisInContext Pipeline**. The VisInContext pipeline builds upon the Flamingo model for in-context few-shot modeling (represented in gray). VisInContext processes interleaved image-text data by rendering portions of the in-context text into images. This approach maintains the _Text Token Length_ of the model while allowing for a significantly extended _In-context Text Length_.

to the text tokenizer in language models. The vision encoder is quite lightweight ViT-L (340M) compared to language model MOE (56B), which makes **the processing of rendered text images significantly more efficient than directly inputting the text into a language model**.

### Token Masking

In our initial experiments, we find that combining tokens from raw images and text images directly led to the network disregarding the text-image input. To address this issue, we introduce a Token Masking strategy to force the model to learn text semantics from visual inputs. During pretraining, the raw image and text image are first encoded into the same number of tokens after resampler, and then we mask the raw image tokens with a pre-defined probability. When masking out the raw image tokens, the model can focus on learning the association between rendered text images and the complementary text tokens. At inference time, we add the text-image tokens and image tokens together, to allow the model effectively leverage information from both sources.

### Text-Centric Contrastive Loss (TCCL)

Motivation.Given that the vision encoder, typically a frozen Vision Transformer (ViT) , never observes rendered text images during pretraining, it may struggle to derive text semantics from pixels. To mitigate this issue, we introduce a new training objective, Text-Centric Contrastive Loss (TCCL). This objective aims to guide the resampler on rendered text images to interpret visual representations of text with a proficiency comparable to traditional text tokenizers, so that the textual semantics can be effective extracted from the rendered text images.

Mechanism.TCCL utilizes raw text token embeddings from the text tokenizer as soft supervision signals to supervise the resampler to learn text-centric representation. To reduce the global semantic gap between text image embeddings and text token embeddings, we first aggregate these embeddings with average pooling and then align them with TCCL. Intuitively, TCCL is designed to turn the joint of the vision encoder and resampler into a "visual" text tokenizer, as it promotes the text image embeddings to share a similar global semantic as the text token embeddings. The core of TCCL is formulated as a contrastive loss:

\[_{ij}=-((f_{v_{i}},f_{t_{j}})/)}{ _{k=1}^{N}((f_{v_{i}},f_{t_{k}})/)})\] (1)

Where \(_{ij}\) denotes the contrastive loss for comparing the \(i^{th}\) text image against the \(j^{th}\) text, \(f_{v_{i}}\) and \(f_{t_{j}}\) represent the feature embeddings of the \(i^{th}\) text image and \(j^{th}\) text, respectively. \(\) is a parameter that control the sharpness of the output distribution. Note that \(f_{v_{i}}\) and \(f_{t_{i}}\) are different features extracted from the same text, as the \(i^{th}\) text image is a direct rendering of the \(i^{th}\) text.

## 3 Experiment

### Experimental Setup

Pretraining.We validate VisInContext with Open-Flamingo  and CosMo . To enhance computational efficiency, all models utilize float16 precision. For the 56B MOE  model, we employ DeepSpeed's  Zero-2 stage with CPU offloading and further optimize the model by quantizing it to 4-bit precision 1. We also use Flash Attention  to further improve memory efficiency. For all other experiments, we train the model using DeepSpeed Zero-2 without CPU off-loading. The Open-Flamingo 9B baseline is based on Mistral7B .

Our pretraining dataset includes a 180M subset of DataComp1B , MMC4 , the OBELICS  dataset, and OCR Rendered Text . (More details are provided in the Appendix B.1) For each input document or image-text pair, we render a text sequence into an image with a fixed size of 16x8192 (512 patches) by default, with \(p_{h}=p_{w}=16\).

[MISSING_PAGE_FAIL:5]

using in-context examples. For instance, in the VQA dataset, a few question-and-answer pairs are provided as in-context examples with visual signals. However, for zero-shot evaluation, two question-and-answer pairs are added as in-context examples without visual signals in . Follow the zero-shot setting, we examine the effect of having text-only in-context examples and extend it to multi-shot setting, by leaving out the corresponding images (See Appendix.E for more details). We compare model performance of the baseline Open-Flamingo 9B and our method under the same setting, where the differences lie in how these text-only in-context examples are processed. Specifically, Open-Flamingo directly takes in them as text tokens, while VisInContext takes in the corresponding rendered text images.

Table 2 summarizes the results across four VQA benchmarks and two captioning benchmarks. Notably, compared to the text-only 0-shot setting, our VisInContext with 32-shot significantly is improved on all VQA and captioning benchmarks considered. Though the 32-shot performance of VisInContext is slightly lower than the competing baseline, we cut down the input tokens to the LLM from 426 to only 10 _Text Token Length_, which lead to significant reduction in the inference cost. These outcomes highlight two key points: _i._**VisInContext can effectively understand text rendered in images**. _ii._ Text rendered as images can be comparably effective as raw text, when used as text-only in-context examples.

Comparison on Inference Cost.We then analyze the inference cost of VisInContext and compare to the baseline. Both models are based on a 56B MOE LLM with a batch size of one to explore the maximum manageable _In-context Text Length_. The results, shown in Figure 4, demonstrate that the _In-context Text Length_ can be extended up to 9192 tokens for the 56B MOE model on 80GB H100 GPUs with our method at inference stage. This result

Figure 4: VisInContext extends the in-context text length of MOE based MLLM from 1k to 9k at inference stage.

Figure 3: **VisInContext significantly improves the OCR ability of LLM. We present the Rendered Text  images and the corresponding next-word prediction accuracy on the validation set. Using the same pre-training steps, VisInContext achieves significantly better results in predicting words in visual images, even when the fonts are difficult to recognize.**

  
**Method** & **Text Source** &  &  \\  & & val & test & \\ 
**Open-Flamingo-9B Baseline ** & Raw Text & 45.3 & 48.2 & 51.5 \\ 
**+VisInContext** & Rendered Image & **48.5(3.2\(\))** & **52.2(4.0\(\))** & **58.4(6.9\(\))** \\   

Table 3: **VisInContext clearly boosting the baseline on document understanding tasks.**highlights the efficiency and advantages of VisInContext, also show its potential in understanding very long document.

### Document understanding

In this section, we evaluate the model on document understanding tasks. Unlike common vision-language tasks that usually short-form pairs, this task requires comprehension of long and complex document data. We evaluate our model on DocVQA and OCRVQA. All document images are of size \(384 384\). Following Pix2Struct , we finetune the model on DocVQA train data and report performance on the average normalized Levenshtein similarity (ANLS) metric.

Results in Table 3 show that our method significantly outperforms the baseline. For instance, we achieve a 6.9% improvement on OCRVQA. To further analyze why our method enhances document understanding, we present the validation accuracy of the LLM on the Rendered Text  dataset during pretraining in Figure 3. We observe a substantial improvement in next word prediction accuracy, with top-1 accuracy increasing from 67.37% to 85.25% (a 16% improvement) and top-5 accuracy rising from 80.76% to 93.38%. These findings indicate that the **LLM can effectively understand text embedded in visual signals with VisInContext**.

### Sequential Multi-modal Retrieval

In order to further analyze the benefit of having long text context in multimodal modeling, we propose a new task - Sequential Multimodal Retrieval (SMR), based on document data from interleaved OBELICS  dataset. The document is composed of interleaved data, consisting of images and texts arranged in a meaningful sequence.

We show one sample in Figure 5 and define the input and output of this task as below: **Input:** Given a pair of content items, an image and a corresponding text \((I_{1},T_{1},R_{1},I_{2},T_{2},R_{2})\), from a document \(D\). \(I\) is Image, \(T\) is the matched text and \(R\) is the surrounding text. **Output:** The task is to retrieve the next image \(I_{2}\) and the next text \(T_{2}\) in the sequence. Named as Seq-I and Seq-T, correspondingly.

We sample the first 1K documents that contain data like \(I_{1},T_{1},R_{1},I_{2},T_{2},R_{2}\) from OBELICS  and named it as OBELICS-Hybrid6, which have at least three frames and three texts. (See Sec. E in appendix for more details.) This task encourages the model to leverage the contextual and semantic relationship in interleaved sequences to effectively predict and retrieve the subsequent pair.

To enable our model with retrieval, we follow CosMo  to add a simple contrastive head between visual embedding and language embedding from the middle layers. Recall that visual embeddings are either from raw images or rendered images or the addition of the two in our method. Table 4

  
**Visual Input** & **Text Input** & **Surrounding Text Input** & **Seq-I** & **Seq-T** \\  Raw Image & Raw Text & - & 16.3 & 64.8 \\ Raw Image & Raw Text & Raw Text & 18.9 & **67.5** \\ Raw Image & Raw Text & Rendered Text Image & **22.7** & 66.5 \\   

Table 4: **The model pretrain with VisInContext significantly improves sequence understanding ability.** We report the sequence retrieval result on OBELICS-Hybrid6.

Figure 5: **Sequential multi-modal retrieval example. The input sequence is \(I_{1},T_{1},R_{1},I_{2},T_{2},R_{2}\) that from interleaved document in OBELICS  dataset.**

reports the results from our model with several input variants. We observe taking surrounding text input as rendered text image performs much better on the Sequence to image retrieval, while on par on Sequence to text retrieval, when compared with taking surrounding text input as raw text. These results further support the designs of VisInContext in the context of document understanding.

### Extension to MLLM with Linear Embedding

Beyond utilizing the visual encoder, some works [36; 8] also employ linear embedding to extract visual features directly from raw images. To show the generality of our method, we also explore FuYu  model as a baseline and integrate VisInContext into the model. (See Sec. A in the appendix for more details.) As indicated in Table 5, our method is successful in improving the performances on DocVQA dataset that require long-context understanding.

### Ablation Study

Ablations on Model Design.We conduct ablation studies on the following modeling components to demonstrate their effectiveness: Text Image, TCCL, and Token Masking. Results are detailed in Table 6, which reveal two findings: 1. Token Masking is crucial for the model to learn from rendered text images. Without Token Masking, the model can only perform comparably to the baseline. Forcing the model to learn text semantics from rendered text images via token masking significantly improves model performance. 2. Utilizing TCCL with Token Masking yields better performance than using Token Masking alone.

Ablations on Font Size and Interval Threshold.As shown in Table 7, optimal performance varies with changes in font size. We found that adjusting the font size impacts performance similarly to altering the patch size--both methods effectively increase the contextual information within each patch. We prefer modifying the font size over the patch size because it allows for more intuitive adjustments. Our findings indicate that the model does not need a highly detailed understanding of each word to perform effectively.

Another important factor is the font interval threshold. As shown in Table 8, we observed that a too-large interval leads to inferior results. This is intuitive because a larger threshold results in fewer texts in the rendered text image.

   Text Image & Token Masking & TCCL & Ok-VQA & TextVqa & VizWiz & VqaV2 \\   & & & & 11.5 & 15.3 & 8.7 & 24.2 \\ ✓ & & & & 11.3 & 15.0 & 9.4 & 30.1 \\ ✓ & ✓ & & & **17.8** & 18.3 & 15.3 & 33.5 \\ ✓ & & & ✓ & 13.5 & 15.3 & 10.3 & 30.9 \\ ✓ & & ✓ & ✓ & 17.2 & **21.8** & **19.7** & **35.2** \\   

Table 6: Ablation study of the component in our pipeline for text-only 4-shot example.

  
**Font Size** & **4** & **6** & **8** & **10** & **12** & \\  TextVQA & 15.4 & 17.2 & 18.5 & **21.8** & 20.3 \\ DocVQA & 39.8 & 42.5 & **45.6** & 44.3 & 36.2 \\   

Table 7: **Font size ablation. We report the result on DocVQA val dataset.**

  
**Method** & **Pretrain Text Source** & **Task** \\  & & DocVQA-val \\ 
**FuYu9B**† & Raw-Text & 42.3 \\ 
**+ VisInContext** & +Rendered Image** & **44.5** (2.2†) \\   

Table 5: **Pretraining with VisInContext helps on long-context understanding task for FuYu model. † means our implementation on 180M data.**

## 4 Related Work

Multimodal Language Models.Current mainstream Multimodal Large Language Models (MLLMs) [37; 38; 22; 39; 40; 41] leverage the capabilities of Large Language Models (LLMs) [42; 6] due to their strong reasoning abilities, as demonstrated by recent advancements. These models typically adopt one of two primary designs for integrating visual information. The first approach involves the effective adaptation of visual representations, which are acquired via a separate visual encoder, into the text-based LLM framework like CLIP, GIT, and BLIP2 [22; 43; 37]. The representative method in this category incorporates visual representations into the language model using cross-attention, as seen in the Flaming series models [1; 9; 14]. Along this line, recently some works like LLaVA , EMU2 , InternVL , DeepSeeker , and QWen  lead to superior results on multi-modality tasks with supervised finetuning on high-quality data. The second approach uses visual embeddings directly as input "tokens" for the LLMs, bypassing the traditional use of a separate visual encoder. This method processes visual patches with a linear layer and uses the resulting embeddings as direct inputs to the LLM, as implemented in models like ViLT  and FuYu . This strategy omits the need for an additional visual encoder and simplifies the architecture.

In this work, we adopt the Flamingo  architecture as our main baseline for the following reasons: First, the Flamingo model emphasizes in-context few-shot learning ability and designs comprehensive few-shot evaluation strategies. Second, our focus is on extending the in-context text length during pre-training rather than on supervised fine-tuning.

Enhancing Text Understanding through Visual Inputs.Traditional text tokenization processes raw text efficiently, but it faces challenges such as vulnerability to spelling errors and limited cross-lingual transferability [46; 47]. These issues have prompted the exploration of tokenizer-free models, which aim to improve robustness and facilitate better cross-language applicability. For instance, a single spelling error can lead to entirely different tokens using traditional tokenization methods, impacting model performance.

Recent developments have seen innovative approaches like the Pixel model , which proposes processing text as an image using both an image encoder and an image decoder. This approach has sparked a series of studies that process not only textual data but also images, charts, and tables through a unified visual input system [35; 46; 48; 47]. These models are trained on a diverse array of visual data, such as webpage screenshots and user interface images, sourced extensively from the internet. They are specifically designed to handle visually-situated text in an end-to-end manner, offering the potential to support a wide range of applications.

Long Context Modeling.The challenge of incorporating more tokens into LLMs is an active area of research [49; 50]. Common approaches involve novel self-attention mechanisms [18; 51; 52], compressed token [53; 54; 55] or memory banks . Some works  exploit tensor parallelism or sequence parallelism to reduce memory costs. There also have some works focus on position embedding [57; 58]. In multi-modality research, closed-source models like Gemini  and GPT-4V  support long context inference up to millions of tokens. Open-source models such as MA-LMM for Long-Term Video Understanding  can process up to one hour of video using a long memory bank. The most relevant work Large World Model  extends token length using Ring Attention.

In contrast to these methods, our method utilizes off-the-shelf LLMs and compresses text tokens into visual tokens for efficient processing. Our method is complementary to these existing techniques and can be integrated with them to achieve lower computational cost and longer context length.

## 5 Conclusion and Limitations

This paper centers on multi-modality learning and addresses the in-context length limitations presented by heavy computational cost of LLMs in MLLMs. Our contribution is a novel and efficient method named VisInContext, which enables the model to perceive long text context as rendered text images. Comprehensive experiments show that VisInContext is effective on conventional in-context few-shot evaluations and document understanding, while being much more efficient.

One limitation of our method is, currently our method requires processing a fixed size image even for brief texts. In future work, we plan to dynamically reduce token counts with variable image sizes by retaining only non-empty tokens during pre-training. We aim to expand this method to additional tasks and encourage the community to further explore this direction.