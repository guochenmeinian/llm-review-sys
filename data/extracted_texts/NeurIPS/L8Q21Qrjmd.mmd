# Pessimistic Backward Policy for GFlowNets

Hyosoon Jang\({}^{1}\), Yunhui Jang\({}^{1}\), Minsu Kim\({}^{2}\), Jinkyoo Park\({}^{2}\), Sungsoo Ahn\({}^{1}\)

\({}^{1}\)POSTECH \({}^{2}\)KAIST

{hsjang1205,uni5510,sungsoo.ahn}@postech.ac.kr,

{min-su,jinkyoo.park}@kaist.ac.kr

###### Abstract

This paper studies Generative Flow Networks (GFlowNets), which learn to sample objects proportionally to a given reward function through the trajectory of state transitions. In this work, we observe that GFlowNets tend to under-exploit the high-reward objects due to training on insufficient number of trajectories, which may lead to a large gap between the estimated flow and the (known) reward value. In response to this challenge, we propose a pessimistic backward policy for GFlowNets (PBP-GFN), which maximizes the observed flow to align closely with the true reward for the object. We extensively evaluate PBP-GFN across eight benchmarks, including hyper-grid environment, bag generation, structured set generation, molecular generation, and four RNA sequence generation tasks. In particular, PBP-GFN enhances the discovery of high-reward objects, maintains the diversity of the objects, and consistently outperforms existing methods.

## 1 Introduction

Generative Flow Networks [1, GFlowNets] are models that sample compositional objects from a Boltzmann distribution defined by some reward function. To this end, GFlowNets construct an object through a trajectory of state transitions, e.g., iteratively adding molecular fragments to construct a molecule. They are attractive for their ability to sample a diverse set of high-reward objects, as demonstrated in molecular discovery , biological sequence design , combinatorial optimization , and large language models .

In detail, GFlowNets aim to sample from the Boltzmann distribution using a _forward policy_ to decide the state transitions. However, this is challenging since the forward policy induces the distribution over trajectories, while the Boltzmann distribution is only defined on the terminal state of trajectories, i.e., objects. Hence, directly matching the two distributions with respect to the terminal state requires an intractable marginalization of the forward policy over the exponentially sized trajectory space.

To circumvent this issue, GFlowNets employ an auxiliary _backward policy_ that lifts the Boltzmann distribution to the trajectories via reversing the state transitions. In particular, the backward policy decomposes the unnormalized Boltzmann density of a terminal state into the unnormalized densities of trajectories, coined _backward flow_, associated with the terminal state. Then the forward policy learns the Boltzmann distribution by matching its unnormalized density, i.e., reward, coined _forward flow_, with the backward flow on the observed trajectories. We call this training scheme _flow matching_.1

The training objective of the flow matching has been investigated such as detailed balance  and trajectory balance , and sub-trajectory balance . To facilitate training, improved credit assignment techniques have been explored . Additionally, exploration methods  have been proposed to collect more diverse trajectories. Moreover, exploitation methods such as focusing on the higher-reward trajectories from the backward policy  and sampling high-reward trajectories with local search  have been presented for the collection of higher-reward trajectories.

In this work, we point out a pitfall of the flow matching objectives: when only a small portion of object-sharing trajectories is observed, GFlowNets tend to under-exploit the object. This pitfall stems from the under-determination of the forward flow, due to training only on the observed backward flow that partially represents the true high reward. Consequently, the forward policy tends to assign high probabilities to objects with high observed backward flow, rather than the high reward objects, as illustrated in (a) and (b) of Figure 1. This is counter-intuitive as the forward policy favors objects with low rewards despite possessing the knowledge of other objects with higher rewards. While one could bypass this issue at the cost of observing more trajectories , we pursue an alternative direction in this work.

We propose a simple remedy for the under-exploitation problem: a pessimistic backward policy for GFlowNets (PBP-GFN). Our key idea is the maximization of the observed backward flow to align the observed backward flow to the true reward. Consequently, PBP-GFN resolves the under-exploitation problem which favors the object with high observed backward flow while neglecting the true reward, as illustrated in (c) of Figure 1. We also note that our algorithm preserves the asymptotic optimality to induce the target Boltzmann distribution by simply modifying the backward policy while preserving the true rewards . Additionally, we analyze how our algorithm reduces the error bound in estimating the true Boltzmann distribution.

We extensively validate PBP-GFN on various benchmarks: hyper-grid benchmark , bag generation , maximum independent set problem , fragment-based molecule generation , and four RNA sequence generation tasks . In these experiments, we observe that PBP-GFN (1) improves the learning of target Boltzmann distribution and (2) enhances the discovery of high-reward objects, while (3) maintaining the diversity of the sampled high-reward objects.

To conclude, our contributions can be summarized as follows:

* We characterize the under-exploitation problem stemming from an under-determined flow that only learns the observed flow for partially observed trajectories (Example 1).
* To resolve this issue, we propose pessimistic training of backward policy that aims to reduce the amount of unobserved flow for the observed objects.
* Through extensive experiments, we show that our algorithm consistently improves the performance of GFlowNets compared to prior works for designing the backward policy, even higher than other training algorithms for discovering high-reward objects.2

Figure 1: **Flow matching for observed trajectories.****(a)** The task aims to reach the terminal state with a reward-proportional probability from the initial state, by incrementing one coordinate as a random action. The black line indicates the two observed trajectories for each terminal state. **(b-c)** The arrow (\(\)) length indicates the amount of the backward or forward flow. In **(b)**, the flow matching (\(\)) between the observed backward and forward flows underestimates the high-reward object due to the low observed backward flow. In **(c)**, PBP-GFN succeeds with the observed backward flow that fully represents the true rewards.

## 2 Preliminaries

Generative Flow Networks [1; 7], GFlowNets] generate an object \(x\) from the object space \(\) through a trajectory \(=(s_{0},s_{1},,s_{T})\) of state transitions, where the terminal state is the object \(s_{T}=x\) to be generated. Here, a forward policy \(P_{F}(s_{t+1}|s_{t})\) makes the transition from the state \(s_{t}\) to the next state \(s_{t+1}\) and assigns a probability of \(P_{F}()=_{t=0}^{T-1}P_{F}(s_{t+1}|s_{t})\) to the trajectory \(\).

Next, GFlowNets train the forward policy to sample objects from a Boltzmann distribution defined by a reward function \(R(x)\) that satisfies:

\[P_{F}^{}(x) R(x).\] (1)

Here, \(P_{F}^{}(x)\) is a distribution of an object \(x\) marginalized over exponentially sized non-terminal state spaces. To circumvent this intractability, GFlowNets train on the flow matching objectives.

**Flow matching for training GFlowNets.** To learn the Boltzmann distribution, the forward policy \(P_{F}\) aims to align to a backward policy \(P_{B}\). The backward policy \(P_{B}(|x)=_{t=0}^{T-1}P_{B}(s_{t}|s_{t+1})\) decomposes the reward into the unnormalized densities of object-sharing trajectories \((x)\) for an object \(x\), i.e., \(R(x)=_{(x)}R(x)P_{B}(|x)\).

To be specific, the forward policy learns to match the unnormalized densities to the backward policy, coined _flow matching_, over all trajectories in the trajectory space \(\):

\[ Z_{}P_{F}() R(x)P_{B}(|x),\] (2)

where \(Z_{}P_{F}()\) is a _forward flow_ defined with a learnable constant \(Z_{}\), and \(R(x)P_{B}(|x)\) is a _backward flow_. Equation (2) induces the forward policy following Boltzmann distribution, i.e., \(Z_{}P_{F}^{}(x) R(x)\), by marginalizing trajectory flows over set of trajectories \((x)\) inducing the object \(x\), i.e., \(_{(x)}Z_{}P_{F}() Z_{}P_{F}^{ }(x)\) and \(_{(x)}R(x)P_{B}(|x) R(x)\).

To satisfy Equation (2), GFlowNets minimize various training objectives. One such objective is the trajectory balance [8; 12], defined as follows:

\[_{}()=(P_{F}()}{R(x)P_{ B}(|x)})^{2},\] (3)

which is minimized over trajectories observed during training, e.g., trajectories sampled from the forward policy. The set of observed trajectories stored in the replay buffer inducing the object \(x\) is denoted as \((x)(x)\). Note that training objectives for Equation (2) can also be defined on a transition [7; 12] or sub-trajectories [8; subTB].

## 3 Method

In this section, we introduce our pessimistic backward policy for generative flow networks (PBP-GFN). First, we show that forward policies trained with flow matching tend to under-exploit high-reward object \(x\) with partially observed trajectories \((x)\) when the underdetermined forward flow only learns the small amount of observed backward flow for the high-reward object (Section 3.1). To address this issue, we propose pessimistic training of backward policy that increases the proportion of observed flow for the object, which leads to an accurate estimation of the reward (Section 3.2).

### Motivation: under-exploitation of objects with partially observed trajectorie

First, we explain how conventional flow matching may suffer from the under-exploitation of observed high-reward objects. To this end, we decompose the reward \(R(x)\) into two components: (1) _observed backward flow_\(R_{}(x)=_{(x)}R(x)P_{B}(|x)\) assigned to the partially observed trajectories \((x)\), and (2) _unobserved backward flow_\(R(x)-R_{}(x)\) assigned to the unobserved trajectories \((x)(x)\). Then (1) and (2) are paired with _observed forward flow_ and _unobserved forward flow_, respectively.

In detail, on the one hand, conventional flow matching aligns the observed forward flow to the observed backward flow for the first component, i.e., \(_{(x)}Z_{}P_{F}() R_{}(x)\). On the other hand, there exists degree of freedom for the unobserved forward flow \(_{(x)(x)}Z_{}P_{F}()\), as it is challenging to match the flow over unobserved trajectories, i.e. trajectories not in the buffer \(\).

Overall, flow matching induces a forward policy with the marginalized probability \(P_{F,}^{}(x)\):

\[P_{F,}^{}(x)(R_{}(x)+_{(x)(x)}Z_{}P_{F}()).\] (4)

Here, our key observation is that, for an observed object \(x\) with a _high reward_\(R(x)\) but _a small amount of observed backward flow_\(R_{}(x)\), the unobserved backward flow \(R(x)-R_{}(x)\) is likely to be much larger than the forward flow of the unobserved trajectories \(_{(x)(x)}Z_{}P_{F}()\). This leads to the under-exploitation of the high-reward object \(x\), since the forward policy assigns a higher probability to another object \(x^{}\) with a _lower reward_ but _a larger amount of observed flow_\(R_{}(x)\), as illustrated in (a) of Figure 2. As a result, the marginalized probability \(P_{F,}^{}(x)\) may converge to a local optimum that yields a smaller expected reward compared to the target Boltzmann distribution.

To further motivate our proposal regarding the under-exploitation problem, we present a failure case of flow matching converged to a local optimum contradicting the observed rewards in Example 1. We construct a particular instance of Equation (4) where the forward policy underestimates the high-reward object compared to the lower one. We depict this example in (a) and (b) of Figure 3.

**Example 1**.: _Consider two objects \(x_{1}\) and \(x_{2}\) with rewards of \(1\) and \(\), respectively, where \(x_{1}\) is reached by three trajectories (\(|(x_{1})|=3\)) and \(x_{2}\) is reached by one (\(|(x_{2})|=1\)). Here, one trajectory for each object is observed (\(|(x_{1})|=|(x_{2})|=1\)). Then, the probability to induce object \(x_{1}\) can be assigned as \(P_{F,}^{}(x_{1})\) since the forward flow still matches the backward flows for the observed trajectory \(_{1}\), i.e., \(P_{F,}^{}(x_{1})=P_{F,}(_{1}) R(x_{1})P_ {B}(_{1}|x_{1})=\). This is lower than \(P_{F,}^{}(x_{2})\) assigned with fully observed trajectories._

Example 1 is counter-intuitive, as a higher probability is assigned to the lower reward object \(x_{2}\) despite observing the higher reward object \(x_{1}\). The forward policy \(P_{F}()\) also assigns zero probability to unobserved trajectories. Consequently, the probability \(P_{F,}^{}(x)\) cannot be corrected even more trajectories are sampled from policy \( P_{F,}()\). This hints at the necessity of the remedy for the under-exploitation of objects due to the small amount of observed flow, with the fixed observation \(\).

### Pessimistic backward policy for GFlowNets

Here, we propose a pessimistic training method for the backward policy in GFlowNets, coined PBP-GFN, which aims to resolve the under-exploitation problem of the flow matching with partially observed trajectories introduced in Section 3.1. To address this challenge, the backward policy is trained to reduce the amount of unobserved backward flow, being pessimistic about unobserved trajectories inducing the observed object. It is notable that the total backward flow for the object, i.e., reward, is preserved by shifting the unobserved backward flow into the observed backward flow.

Figure 2: **Under-exploitation of objects with partially observed trajectories.** The reward \(R(x)\) consists of (1) observed backward flow \(R_{}(x)\) and (2) unobserved backward flow \(R(x)-R_{}(x)\). **(a)** Conventional flow matching may assign a higher probability to the lower-reward object as the observed forward flow is aligned only with a small amount of observed backward flow. This fails to assign the accurate probability proportional to the reward. **(b)** PBP-GFN assigns more accurate probability proportional to the reward, by increasing the proportion of observed flow.

To be specific, given a replay buffer \(\), the pessimistic training of backward policy \(P_{B}\) aims to increase the backward flow for the observed trajectories ending with the object \(x\). Specifically, it aims to maximize \(R_{}(x)=_{(x)}R(x)P_{B}(|x)\), thereby aligning the observed backward flow \(R_{}(x)\) to the true reward \(R(x)\), reaching the upper bound \(R_{}(x) R(x)\). Consequently, flow matching with such a backward flow for partially observed trajectories induces an observed forward flow that accurately estimates the true reward, thereby preventing the under-exploitation of the rewards due to the small amount of observed flow (Example 1), as illustrated in (b) of Figure 2 and (c) of Figure 3.

Furthermore, it is worth noting how PBP-GFN better estimates the Boltzmann density, i.e., reward. The high-level idea is that, given the fixed total flow, maximizing the observed forward and backward flows with PBP-GFN naturally minimizes the unobserved forward and backward flows, thereby reducing a flow matching error for the unobserved flows effectively. The detailed error bound in estimating the Boltzmann density is described in Appendix A.

**Pessimistic training of backward policy.** We train the parameterized backward policy \(P_{B}\) to increase the backward trajectory flows in observed trajectories, \(R(x)_{_{x}}P_{B}(|x)\), by assigning higher probabilities to the backward transitions \(P_{B}(|x)=_{t=0}^{T-1}P_{}(s_{t}|s_{t+1})\) of observed trajectories, i.e., \(_{ B(x)}P_{B}(|x) 1\). We achieve this by minimizing the negative log-likelihood:

\[_{}=-_{(x)}[ P_{B}(|x)],\] (5)

where \(x\) is the object induced by the trajectory \(\). It is notable that our approach only modifies the relative backward trajectory flows among trajectories inducing the same object and does not alter the total amount of backward flows, thereby preserving the asymptotic optimality of flow matching for learning the target Boltzmann distribution. Note that the training of the pessimistic backward policy is practical in most cases, as it only requires computing the stochastic gradients to minimize \(_{}\).

Figure 3: **Pessimistic backward policy for GFlowNets (PBP-GFN). The portion of the circle indicates the amount of flow, e.g., \(\) indicates the flow of 1, and \(\) indicates the half flow of \(\), i.e., the flow of 0.5. Additionally, the color of the flow indicates the flow inducing the same-colored reward, and the black and gray lines indicate the observed and unobserved trajectories, respectively. (a) Flow matching succeeds with the entire trajectories. One can observe that the true reward of \(x_{1}\) is 1 and the reward of \(x_{2}\) is 0.5 by the amount of flow. (b) Flow matching fails with partially observed trajectories. (c) PBP-GFN assigns high probabilities to the backward transitions of observed trajectories to keep a high probability to high-reward objects.**

Subsequently, we train the GFlowNets with the learned pessimistic backward policy. The pessimistic backward policy is learned online with the forward policy, as new trajectories are observed for the training in each round. The training algorithm is described in Algorithm 1.3 Note that the pessimistic training is agnostic to the choice of flow matching objectives [7; 8; 9].

## 4 Related work

**Generative Flow Networks (GFlowNets).** GFlowNets [1; 7] train a forward policy that sequentially constructs objects sampled from a Boltzmann distribution. They are closely related to reinforcement learning in soft Markov Decision Processes (soft MDPs) [15; 16; 17] and variational inference . Recently, there has been a surge in research on improving the training of GFlowNets, such as introducing novel flow matching objective functions [8; 9; 12; 13], enhancing off-policy exploration [14; 19; 20; 21], incorporating order information for enabling preference-based optimization , and improving credit assignment [10; 11]. Moreover, GFlowNets are increasingly applied across a wide range of fields such as molecular optimization [2; 3; 23], biological sequence design [4; 24], probabilistic modeling and inference [25; 26], combinatorial optimization [5; 27; 28], continuous stochastic control [29; 30; 31], and large language models .

Despite the advancements in training GFlowNets, there still exists the challenge of dealing with the vast number of trajectories. The number of trajectories grows exponentially with the increase in the number of state spaces and actions, making it impractical to observe all trajectories during training. This issue can be partially addressed by facilitating the discovery of unobserved trajectories . However, the problem of probability not matching the rewards remains unless sampled trajectories comprehensively cover all possible flows.

**Training GFlowNets with auxiliary backward policy.** GFlowNets train a forward policy to align with the auxiliary backward policy, which inverts the construction process of the object. Therefore, the choice of the backward policy directly impacts the training of GFlowNets and is vital to the improvement of the sampling performance. Despite its crucial role, the choice of backward policy has gained limited attention with only a few works [8; 13; 15], and none of these works tackle the under-exploitation of high-reward objects caused by unobserved backward flow.

For instance, while the uniform  and the MaxEnt  backward policies assign a fixed probability to the backward transition for enhancing exploration, our pessimistic backward policy learns the backward transition probability for enhancing exploitation. Next, conventional  and sub-structure  backward policies may enhance the exploitation by learning the backward flow to align with the forward flow or to improve the credit assignments. However, they do not directly reduce the unobserved backward flow and do not resolve the under-exploitation stemming from that.

## 5 Experiment

We evaluate our method on various domains, including a hyper-grid , bags , structured sets , molecules , and RNA sequences [13; 14]. As base metrics, we consider the number of modes, e.g., samples with rewards higher than a specific threshold, and the average top-100 score, which are measured via samples collected during training. We report the performances using three different random seeds. In these experiments, one can observe that:

* PBP-GFN improves learning of the target Boltzmann distribution (Figures 4 and 6).
* PBP-GFN enhances the discovery of high-reward objects (Figures 5, 7 and 8).
* PBP-GFN maintains the diversity of sampled high-reward objects and promotes the discovery of distinct diverse modes (Figures 7(c) and 9).

### Synthetic tasks

In synthetic environments, i.e., hyper-grid environment, bag generation, and maximum independent set, we first show how our method (PBP-GFN) improves the performance compared to the prior methods that proposed various designs of the backward policy, on both the trajectory balance [8, TB] and detailed balance-based implementations [7; DB]. As baselines, we consider the conventional backward policy trained with the TB or DB , the uniform backward policy , and the maximum entropy backward policy [15, MaxEnt].

**Hyper-grid .** We first consider the hyper-grid, where the target Boltzmann distribution is defined over the \(16 16 16\) grid illustrated in Figure 4(a). We also consider the \(20 20 20 20\) hyper-grid. The actions are incrementing one coordinate by one or terminating. The high-reward regions, i.e., modes, are defined as near the corners of the grid that are separated by regions with very small rewards. In this task, we consider the TB-based implementation following the prior work . The detailed experimental settings are described in Appendix B. In this task, we measure the L1 distance between the target Boltzmann distribution and the empirical distribution of \(P_{F}^{}(x)\), with the measurable likelihood of the Boltzmann distribution.

**Bag generation .** We next consider a simple bag generation task, where the action is adding an item to a bag. The bag yields a high reward when seven repeated items are included, i.e., modes. We apply our method to the prior TB-based implementation on this task  and compare it with TB and MaxEnt. The detailed setting is described in Appendix B.

**Maximum independent set .** We also consider solving maximum independent set problems, where the action is selecting a node and the reward is the size of the independent set. At each epoch, the GFlowNets train with the set of training graphs, and sample \(20\) solutions for each validation graph and measure the average reward and the maximum reward following Zhang et al. . We apply our method to the prior DB-based implementation of this task . The experimental setting is described in Appendix B. Note that the MaxEnt is equivalent to the uniform backward policy in this task.

**Results.** In Figure 4 and Figure 6, we depict the empirical sampling distribution and the L1 distance from the target Boltzmann distribution for each method in the hyper-grid environment. Here, one can see that our method (PBP-GFN) captures all modes and converges to the target Boltzmann distribution faster than the baselines. These results can be attributed to the capabilities of PBP-GFN, which enables us to effectively learn from the large amount of correct forward flow.

Figure 4: **The target distribution and empirical distributions of each model trained with \(10^{5}\) trajectories**. The empirical distributions are computed as rescaled products of the distribution over three runs. Our method (PBP-GFN) consistently discovers all modes over three runs and learns the target Boltzmann distribution correctly within the relatively small number of trajectories.

Figure 5: **The performance comparison with the prior backward policy design methods.** The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority in generating diverse high-reward objects, compared to the considered baselines for designing the backward policy.

In both bag generation and maximum independent set problems, one can see that our approach also shows (1) superior performance (2) or faster convergence compared to the baselines as illustrated in Figure 5. One can reason this result stems from the capabilities of PBP-GFN that facilitate the learning of correct Boltzmann distribution. Furthermore, it is worth noting that our method makes improvements over both TB and DB-based implementations.

### Molecular generation

Next, we evaluate our method in the fragment-based molecule generation , where the action is adding a molecular building block. The reward is the binding energy between the molecule and the target protein computed by a pre-trained oracle . Here, the mode is defined as a high-reward molecule with a low Tanimoto similarity  measured against previously accepted modes.

We consider a TB-based implementation for our method and compare with various baselines including GFlowNets and reinforcement learning algorithms: DB, sub trajectory balance [9, subTB], TB, TB defined with uniform and MaxEnt backward policies, generative augmented flow networks [10, GAFN], and Advantage Actor-Critic [33, A2C]. For the evaluation metric, we analyze the trade-off between the average score of the top 100 samples and the diversity of these samples. Additionally, to measure diversity, we compute the average pairwise Tanimoto similarity following prior works. The detailed setting is described in Appendix B.

**Results.** We depict the results in Figure 7. One can see that our method, i.e., PBP-GFN, outperforms the baselines in enhancing the average score of unique top-100 molecules and the number of modes found during training. These results highlight that PBP-GFN also can make improvements for environments with a huge state space. Furthermore, one can see that our method yields low Tanimoto similarities between top-100 molecules with respect to the average reward. This verifies that our algorithm not only generates high-scoring samples but also diverse molecules.

### Sequence generation

We consider four RNA sequence generation tasks that aim to discover diverse and promising sequences that bind to human transcription factors [4; 34; 35], where the action is appending or prepending an amino acid. As baselines, we consider the same baselines as in the fragment-based molecule generation. In this task, we conduct experiments on the following four benchmarks.

Figure 6: **L1 distance between Boltzmann distribution. PBP-GFN shows fastest learning target distribution with respect to the observed trajectories.**

Figure 7: **The performance on molecular generation. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority compared to the baselines in generating diverse high reward molecules while yielding similar Tanimoto similarities compared to other baselines with prior backward policy designs.**

**TFBind8.** This is the task to generate length-eight RNA sequences. The reward is computed by wet-lab measured DNA binding activity to Sine Oculis Homeobox Homology 6 . The mode is determined based on whether it is included in a predefined set of promising RNA sequences .

**RNA-Binding.** This task generates length-\(14\) RNA sequences. In this task, we consider three different target transcriptions: RNA-A, RNA-B, and RNA-C [14; 36]. The mode is defined as an RNA sequence with a reward higher than the threshold. In this task, we also consider the \(2\)-hamming ball modes , which is defined as the local maximum among its intermediate neighborhoods defined by modifying \(n\) components of the sequence.

**Results.** The results are presented in Figure 8. One can see that FBP-GFN shows faster convergence or superior performance compared to the considered baselines in enhancing the average score of unique top-100 sequences and the number of modes during training. Furthermore, in Figure 9, one can see that our method better discovers the diverse distinct modes that are separated far from each other, compared to the baselines.

### Ablation studies

**Comparing overall generated sample quality.** To further analyze the overall sample quality, we provide the relative mean error  which measures the distance between the mean values of the empirical generative distribution and the target Boltzmann distribution. We present the results in Figure 10. One can see that our method yields the lowest errors.

Figure 8: **The performance on RNA sequence generation. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority compared to the baselines in generating diverse high reward sequences.**

Figure 9: **The number of 2-hamming ball modes discovered during training. The solid line and shaded region represent the mean and standard deviation, respectively. The PBP-GFN shows superiority compared to the baselines in discovering diverse distinct modes.**

**Comparison with other exploitation methods.** We further validate PBP-GFN by comparing or combining the pessimistic backward policy with local search  and reward-prioritized buffer  that are off-policy sampling methods and orthogonal to our pessimistic backward policy. We present the experimental results in Figure 11. One can observe that our method (1) shows similar performance compared to them and (2) consistently improves the performance when combined with them.

## 6 Conclusion

In this work, we identify the under-exploitation problem in flow matching due to the large amount of unobserved flow lifted by the backward policy. To resolve this, we introduce a pessimistic training of the backward policy for GFlowNets (PBP-GFN) that reduces the probabilities for unobserved backward trajectories leading to observed trajectories. Our PBP-GFN shows a successful alternative to the prior backward policies and has demonstrated improved performance across eight benchmarks.

**Limitation.** As an exploitation method, our PBP-GFN makes a trade-off between obtaining high-reward trajectories and diversified trajectories, i.e., there is no free lunch in the exploitation-exploration trade-off. Although our approach maintains the diversity of high-reward sampled objects in the considered benchmarks, this may not hold for some environments where exploration is significant. We discuss such a setting in Appendix C. To relax this issue, one can reduce the learning rate for the pessimistic backward policy or incorporate an exploration-focused off-policy sampling method. One can further control the trade-off by interpolating PBP-GFN with explorative GFNs, e.g., MaxEnt, which can be an interesting future work direction.