# KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization

Tianyi Zhang

Dept. of Computer Science, Rice University

xMAD.ai

Houston, TX

tz21@rice.edu &Jonah Yi

Dept. of Computer Science, Rice University

xMAD.ai

Houston, TX

jwy4@rice.edu &Zhaozhuo Xu

Dept. of Computer Science,

Stevens Institute of Technology

xMAD.ai

Hooken, NJ

zxu79@stevens.edu &Anshumali Shrivastava

Dept. of Computer Science, Rice University

Ken Kennedy Institute

ThirdAI Corp.

xMAD.ai

Houston, TX

anshumali@rice.edu

###### Abstract

Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4-3.5\(\) relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.

## 1 Introduction

Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks without needing specific fine-tuning . These impressive capabilities have empowered LLMs to find applications in numerous domains . However, the high computational demands and prohibitive deployment costs of LLMs create significant barriers, hindering their widespread adoption [19; 4]. Particularly, as LLMs move towards larger model size  and longer context length , they require faster hardware accelerators such as graphics processing units (GPUs) with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs.

Key and value (KV) caching  has proven to be an effective technique for accelerating LLM inference without affecting model quality. In autoregressive LLMs, KV caching works through trading off memory to save computations: the key and value activations of all previous tokens in the current sequence are cached in memory to avoid their recomputation for generating the next token. However, KV cache can quickly overwhelm the memory capacity of GPUs as context length or batch size increases, since its storage scales linearly with these two factors. Consider the OPT-175D model , storing its KV cache for 128 sequences of 2048 tokens requires 1.2 terabytes of memory, which is around 3.5\(\) the storage of its weights. Because inference throughput scales with batch size, the substantial memory demands of KV caching can become a significant bottleneck to throughput. In addition, as KV cache is not shared across sequences within a batch, it has a low compute-to-memory ratio, making reading the KV cache from GPU memory the primary source of latency as opposed to the attention computation . KV cache compression can bring the following benefits: 1. speeding up LLM inference by improving the compute-to-memory ratio, 2. improving the serving throughput by fitting more sequences into memory and hence enabling larger batch sizes, 3. lowering the GPU requirements for inference for a given batch size, context length, and model size. Existing approaches typically achieve compression of KV cache through token eviction [47; 24] or activation quantization [25; 15]. While these methods can preserve model quality at moderate compression rates (4\(\) compression or 4 bits per activation), model quality quickly deteriorates at high compression rates (16\(\) compression or 1 bit per activation). In this work, we leverage the interdependency between key/value channels, an insight overlooked by existing approaches, to achieve higher compression rates of KV cache while maintaining model quality.

Our approach is motivated by the observation that distinct channels within the same key/value activation embedding are highly interdependent and correlated (see our analysis in Section 3.1). It is hence more information-efficient to encode multiple channels of KV cache at once, which we call _channel coupling_. Existing solutions, in contrast, only employ per-channel or per-token quantization strategies [25; 15] for compressing KV cache, which is not optimal for exploiting the dependency between channels. As a result, we observe significant model quality degradation at the extreme compression rate of 1 bit per activation. By leveraging channel coupling, we enable compression at the level of 1-bit quantization of KV cache while preserving model quality. In Figure 1, we show the perplexity of two models from the LLaMA family [36; 37] on WikiText-2  under 1-bit quantization with varying numbers of coupled channels. The full experimental setup is presented in Section 4. Coupling more channels significantly enhances model quality, as the perplexity quickly approaches the performance using uncompressed FP16 KV cache. By further combining KV cache quantization with a sliding window of 128 recent tokens cached in full precision, we achieve a negligible 0.3-0.33 increase in perplexity with 1-bit KV cache.

We summarize our contributions as follows.

1. We observe the phenomenon that distinct channels within the same key/value activation embedding share a high amount of dependency or mutual information, which has not been leveraged by existing approaches.
2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that jointly encodes multiple key/value channels to exploit the dependency across channels.
3. Through extensive experiments, we demonstrate the effectiveness of CQ at preserving model quality and speeding up LLM inference against competitive baselines. Furthermore, we demonstrate CQ reasonably preserves model quality at an extreme level of 1-bit quantization.

Figure 1: Perplexity of LLMs with 1-bit quantized KV cache approaches the uncompressed FP16 performance as the number of coupled K/V channels increases.

Background

This section introduces the relevant background information including the KV caching technique and per-channel quantization.

### LLM Attention and KV Cache

Decoder-only transformer-based LLMs employ masked self-attention , in which activations of the current token only depend on the previous tokens and are unaffected by future ones. This property enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient decoding during inference. Consider the decoding step for the \(t\)-th token in a single head of attention in an LLM. The input embedding of the \(t\)-th token (a column vector), \(e_{t}\), goes through three distinct transformations to become key, query, and value activation embeddings \(f_{K}(e_{t}),f_{Q}(e_{t}),f_{V}(e_{t})\), where the transformations \(f_{K},f_{Q},f_{V}\) are composed of linear projection and positional encoding methods such as RoPE . The output embedding of attention for the \(t\)-th token is computed as

\[(e_{t})=[f_{V}(e_{1})&&f_{V}( e_{t})]\!(\![f_{K}(e_{1})& &f_{K}(e_{t})]^{}f_{Q}(e_{t})\! )\] (1)

where \(d\) is the dimensionality of \(f_{K}(e_{t})\). Computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens, \(f_{K}(e_{i})\) and \(f_{V}(e_{i})\) where \(i\{1,,t-1\}\). These embeddings are cached in memory from previous decoding steps to eliminate redundant computations, a process known as KV caching. The size of KV cache can be calculated as \(b n l 2 h d\) floating-point numbers, where \(b\) is the batch size, \(n\) is the number of tokens in each sequence, \(l\) is the number of layers in the model, \(2\) is for key and value, \(h\) is the number of key/value attention heads, and \(d\) is the dimensionality of a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of the KV cache can quickly overwhelm the limited GPU memory. KV cache bottlenecks inference throughput since it limits the maximum batch size, and it is a major contributor to latency due to the low compute-to-memory ratio .

### Per-Channel Quantization

Existing KV cache quantization methods [15; 25] employ per-channel quantization for keys and per-token quantization for values. Per-channel and per-token quantization are similar, except the direction along which the quantization centroids are learned (or the direction along which the scaling factor and zero-point are determined for uniform quantization). Keys are quantized per-channel based on the observation that certain key channels have significantly higher magnitudes than others, while values are quantized per-token because value channels have no such outliers. In non-uniform per-channel quantization, a set of centroids is learned for each channel. Suppose \(A\) is a key or value activation matrix, and let \(A_{i,:}\) denote the \(i\)-th channel of \(A\). Then, non-uniform \(b\)-bit per-channel quantization aims to learn a set of centroids \(C^{*}_{i}\) for each channel \(i\) of \(A\) through the objective

\[C^{}_{i}=*{arg\,min}_{C\\ |C|=2^{b}}\|A_{i,:}-q(A_{i,:})\|_{2}^{2}\] (2)

where \(q\) quantizes each value in \(A_{i,:}\) to the nearest centroid in \(C\).

## 3 Methodology

In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression.

### Motivations

Our proposed approach is inspired by concepts in information theory . We consider channels in a key/value activation embedding as random variables \(X_{1},X_{2},\). The amount of information (or uncertainty) in channel \(X\) can be measured by _entropy_, defined as \(H(X)=-_{}p(x)_{2}p(x)\,dx\)where \(p()\) is the probability density function and \(\) is the support of \(X\). \(H(X)\) measures the theoretical number of bits needed for losslessly encoding the channel \(X\), hence it can be used to gauge how "quantizable" a channel is: if \(H(X_{1})<H(X_{2})\), then channel \(X_{1}\) may be quantized to fewer bits than channel \(X_{2}\) with the same quantization error.

Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channels \(X_{1},X_{2}\) is measured by _joint entropy_, defined as \(H(X_{1},X_{2})=-_{_{1}}_{_{2}}p(x_{1},x_{2})_{ 2}p(x_{1},x_{2})\,dx_{2}\,dx_{1}\), where \(p(,)\) is the joint probability density function. Equivalently, the joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e., \(H(X_{1},X_{2})=H(X_{1})+H(X_{2})-I(X_{1},X_{2})\), where \(I(,)\) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have

\[H(X_{1},X_{2}) H(X_{1})+H(X_{2})\] (3)

which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that deep neural networks  and attention-based networks  tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency.

It is hence beneficial to measure the difference between the sum of marginal entropies of multiple key/value channels and their joint entropy. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is difficult to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the "binning" trick  to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy of \(n\) channels \(X_{1},,X_{n}\) is estimated with the Riemann sum,

\[H(X_{1},,X_{n})_{x_{1}_{1}}_{x_{n} _{n}}(x_{1},,x_{n})_{2}(x_{1},,x_{n})\] (4)

Figure 2: (a) Growth rate of joint entropy versus sum of marginal entropies of the key/value activation embeddings of LLaMA-7b on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that quantizing more channels together is more information-efficient than quantizing fewer channels. (b) Correlation matrices of the first 32 channels of 5 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients.

where \(_{i}\) is the support of the binned or discretized \(X_{i}\) and \(()\) is the empirical probability mass function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b  into non-overlapping groups each containing \(c\) contiguous channels, where \(c\{1,2,3,4\}\), and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1(a) shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of four layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset , averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1(a), the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding them decreases. This phenomenon is the foundation that motivates our proposed approach.

In addition to studying the marginal and joint entropy, we also analyze the linear relationships between channels of a key/value activation embedding using Pearson correlation coefficient. Figure 1(b) presents the correlation matrices for the first 32 channels of 5 layers of LLaMA-7b keys and values on WikiText-2. The key and value channels exhibit high levels of linear dependency, and are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. In Section M of the appendix, we include the correlation matrices of all layers of LLaMA-7b, and present scatter plots to visualize the patterns in key and value activations.

### Coupled Quantization

Motivated by the finding that distinct key/value channels exhibit high amounts of dependency, we propose Coupled Quantization (CQ), an information-efficient KV cache quantization approach that couples multiple key/value channels for quantization. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group are _coupled_, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each channel group is quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c><b>b notation to denote the configuration of channel coupling and quantization bit width, where <c> is the number of channels in each group and <b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit per-channel quantization in terms of storage overhead of quantized codes. An illustrative comparison of per-channel quantization and CQ is shown in Figure 3. Although previous works [15; 25] opt to quantize keys per-channel and values per-token, we adopt channel-coupled quantization for both keys and values, which we empirically show is effective for both in Section 4.3. CQ quantizes keys before the positional encoding such as RoPE  is applied, which increases the quantization difficulty by introducing more outliers in key activations [15; 25].

Figure 3: Per-channel quantization (left) and our proposed Coupled Quantization (right). The 1-bit quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. CQ leverages the dependency between channels to achieve lower quantization errors than per-channel quantization.

#### 3.2.1 Centroid Learning

In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ- \(c\) c \(b\) b configuration, a set of centroids \(C_{i}^{}^{c}\) is learned independently for each channel group \(i\) through the objective

\[C_{i}^{}=*{arg\,min}_{C^{ c}\\ |C|=2^{b}}\|A_{(ic-c+1):ic_{c},:}-(A_{( ic-c+1):ic_{c},:})\|_{F}^{2}\] (5)

where \(A_{(ic-c+1):ic_{c},:}\) is the sub-matrix of \(A\) containing all coupled channels from the \(i\)-th group, and \(\) quantizes each column vector to the nearest centroid in \(C\) in terms of L2 distance. We use the k-means algorithm  with k-means++ initialization  to optimize the objective.

LLMs are more sensitive to the quantized precision of certain weights than others , hence centroids of CQ should be learned to bias towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrix \(\) to identify the more influential key/value activations and guide the centroid learning process. Using the diagonal Fisher information matrix for quantization was proposed in , and we extend it to the multi-channel case. For performing Fisher-guided centroid learning, we first save a key/value activation matrix \(A\) and its gradient \(g(A)=(A)\) on a calibration dataset, where \(\) is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, \(()=g(A) g(A)\), which is the element-wise square of the gradient matrix. We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of activations, and obtain the centroid set \(C_{i}^{}\) for the \(i\)-th channel group using the objective

\[C_{i}^{}=*{arg\,min}_{C ^{c}\\ |C|=2^{b}}_{j}A_{(ic-c+1):ic_{c},j} ^{}gA_{(ic-c+1):ic_{c},j}}_{()}\|A_{(ic-c+1):ic_{c},j}- (A_{(ic-c+1):ic,j})\|_{2}^{2}\] (6)

which we leverage weighted k-means to optimize. We discuss the overhead of centroid learning and centroid storage in Section E in the appendix.

### Efficient Inference Through Kernel Fusion

We design fused GPU kernels to enable efficient inference of CQ. During inference, dequantizing couple-quantized KV cache requires many random accesses for lookups of multi-dimensional centroids. If the centroids reside in GPU global memory, these random accesses would greatly hinder the inference efficiency. We circumvent this issue by caching centroids in the shared memory, which has significantly lower latency and higher bandwidth than global memory. Due to limited size of the shared memory for each thread block, we assign the work of a single channel group to each thread block, which only requires loading a single group of centroids into a block of shared memory. We perform kernel fusion to merge dequantization of key cache, positional encoding and KQ multiplication, as well as to merge dequantization of value cache and its multiplication with attention scores. We validate the inference efficiency of CQ empirically in Section 4.4.

## 4 Experiments

In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, datasets, metrics, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal.

**Hardware and Software** Experiments are performed on a Linux server equipped with 4 NVIDIA A100 40GB GPUs. Our software implementation of CQ is based on PyTorch  and the Hugging-Face Transformers library .

**Evaluation Metrics and Datasets** We compare different KV cache quantization by evaluating the quality of 5 LLMs on various benchmarks. The 5 LLMs considered are 1. LLaMA-7b, 2. LLaMA-13b, 3. LLaMA-2-7b, 4. LLaMA-2-13b , 5. Mistral-7b . We evaluate the quality of LLMs using the metric perplexity on 2 datasets: WikiText-2  and C4 , and zero-shot accuracy on 3 benchmarks: WinoGrande , PIQA , and ARC Challenge (Arc-C) . Furthermore, we evaluate on long-context benchmarks GSM8K  with chain-of-thought (CoT), and few-shot MMLU  with CoT. For perplexity and accuracy evaluations, the KV cache of all tokens in all layers is quantized, during both the prefill and decoding stages. The experimental details, including the procedures for perplexity and benchmark evaluations, are presented in Section A in the Appendix. More experimental results, including a comparison between CQ and KIVI  on LongBench , and results on more models and passkey retrieval , can be found in the Appendix.

**Baselines** We compare our proposed approach with uncompressed FP16 KV cache and competitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization  (without grouping and with a group size of 128), 3. KVQuant  (dense-only and with 1% outliers stored in sparse format). KVQuant-<b>b+1% sparse is a dense-and-sparse method that stores outlier activations in a sparse matrix and requires an additional sparse matrix multiplication during inference, which introduces extra computational overhead. For calibration, we use the same set of 16 sequences of WikiText-2, each with 2048 tokens, for KVQuant and CQ. Other methods do not require calibration. Calibration is performed only once and the learned centroids are used for all downstream evaluations. Calibration is done on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For 1-bit and 2-bit KVQuant, we employ Q-Norm to mitigate distribution shift, as recommended by . We report Bits Per Activation (BPA) to measure the compression rate of each method, where each activation in the uncompressed KV cache is a 16-bit float. Detailed calculations of bits per activation for CQ are presented in Section F in the appendix.

### Results

Table 1 presents the perplexity of LLMs on WikiText-2 under different KV quantization methods. CQ consistently outperforms baselines under the same quantization bit width. In low bit width regions of 1-bit and 2-bit quantization, dense-only quantization baselines quickly deteriorates in quality while CQ preserves quality well. We highlight that CQ-8c8b (1 bit per activation) outperforms KVQuant-2b (2 bits per activation) with only half the memory. CQ also compares favorably against

   & LLaMA-7b & LLaMA-13b & LLLaMA-2-7b & LLaMA-2-13b & Mistral-7b \\   FP16 & 16 & 5.68 & 5.09 & 5.12 & 4.57 & 4.76 \\  INT4 & 4.00 & 5.98 & 5.32 & 5.66 & 5.01 & 4.97 \\ INT4-g128 & 4.16 & 5.77 & 5.16 & 5.32 & 4.71 & 4.82 \\ NF4 & 4.00 & 5.87 & 5.23 & 5.47 & 4.90 & 4.91 \\ NF4-g128 & 4.25 & 5.77 & 5.17 & 5.30 & 4.71 & 4.83 \\ KVQuant-4b & 4.00 & 5.73 & 5.15 & 5.18 & 4.63 & 4.81 \\ KVQuant-4b+1% sparse & 4.32 & **5.70** & **5.11** & **5.14** & **4.59** & **4.78** \\ CQ-2c8b & 4.00 & **5.70** & **5.11** & **5.14** & **4.59** & 4.79 \\  INT2 & 2.00 & 11779 & 69965 & 4708 & 3942 & 573 \\ INT2-g128 & 2.14 & 37.37 & 41.77 & 117.88 & 93.09 & 51.96 \\ NF2 & 2.00 & 3210.5 & 5785.6 & 13601 & 4035.6 & 902.51 \\ NF2-g128 & 2.25 & 351.23 & 141.19 & 634.59 & 642.44 & 252.85 \\ KVQuant-2b & 2.00 & 8.17 & 7.29 & 9.75 & 29.25 & 7.33 \\ KVQuant-2b+1% sparse & 2.32 & 6.06 & 5.40 & 5.50 & 4.92 & 5.16 \\ CQ-4c8b & 2.00 & 5.97 & 5.32 & 5.42 & 4.81 & 5.11 \\ CQ-4c9b & 2.26 & **5.88** & **5.26** & **5.32** & **4.74** & **4.98** \\  KVQuant-1b & 1.00 & 321.58 & 1617.40 & NaN & 4709.83 & 203.73 \\ KVQuant-1b+1% sparse & 1.32 & 9.93 & 7.97 & 9.50 & 13.76 & 10.07 \\ CQ-8c8b & 1.00 & 8.09 & 7.02 & 7.75 & 6.55 & 7.25 \\ CQ-8c10b & 1.27 & **6.78** & **6.00** & **6.25** & **5.47** & **5.90** \\   

Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from . “NaN” means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.

dense-and-sparse baselines by outperforming KVQuant+1% sparse in most cases despite using less bits. We present the perplexity results on C4 in Table 7 in the appendix. Table 2 presents the accuracy results of KVQuant and CQ on different benchmarks. CQ consistently outperforms dense-only KVQuant at 1-bit and 2-bit, and performs better or on par with the dense-and-sparse KVQuant under the same bit width. Table 3 presents the accuracy comparison of KVQuant and CQ on long-context benchmarks, with CoT or few-shot CoT. CQ mostly outperforms KVQuant under similar bit widths.

    &  & LLaMA-7b & LLaMA-13b & LLaMA-2-7b & LLaMA-2-13b & Mistral-7b & Average \\    &  &  & 69.93 & 72.69 & 68.90 & 71.98 & 73.88 &  \\  & & PIQA & 78.67 & 79.16 & 78.07 & 79.16 & 80.58 & \\  & & ARC-C & 41.72 & 46.42 & 43.43 & 48.29 & 50.34 & \\   &  & WinGrande & 69.53 & 72.61 & 67.96 & 71.59 & **73.88** &  \\  & & PIQA & **78.62** & **79.22** & 77.86 & 78.94 & 80.58 & \\  & & ARC-C & **42.32** & 45.99 & 42.75 & 46.67 & 49.06 & \\   &  & WinGrande & **70.72** & **73.40** & **68.67** & 72.30 & 73.72 &  \\  & & PIQA & 78.40 & 79.16 & **78.07** & **79.27** & **80.74** & \\  & & ARC-C & 41.38 & **46.76** & 43.17 & **47.87** & **49.91** & \\   &  & WinGrande & 70.40 & 72.45 & 68.27 & **72.53** & 73.48 &  \\  & & PIQA & 78.61 & 79.11 & 77.91 & 78.62 & 80.52 & \\  & & ARC-C & 41.55 & 45.99 & **43.34** & 47.78 & 49.15 & \\   &  & WinGrande & 68.03 & **71.43** & **67.64** & 70.17 & **70.80** &  \\  & & PIQA & **77.69** & **78.51** & 76.60 & **78.51** & **79.65** & 63.79 \\  & & ARC-C & 38.74 & **45.14** & 41.47 & **44.97** & 47.53 & \\   &  & WinGrande & **68.51** & 69.93 & 67.40 & **71.67** & 70.71 &  \\  & & PIQA & 76.82 & **78.51** & **77.09** & 77.31 & 79.48 & 63.75 \\  & & ARC-C & **39.16** & **45.14** & **41.64** & **44.97** & **47.95** & \\   &  & WinGrande & 53.59 & 59.35 & 51.70 & 51.30 & 63.46 &  \\  & & PIQA & 72.47 & 74.81 & 63.38 & 65.40 & 75.46 & \\  & & ARC-C & 32.00 & 34.47 & 22.44 & 24.66 & 38.57 & \\   &  & WinGrande & **67.48** & **70.72** & **66.45** & **69.06** & **69.38** &  \\  & & PIQA & **76.11** & **78.29** & **76.12** & **77.42** & **79.49** & \\  & & ARC-C & **38.48** & **44.03** & **39.93** & **44.11** & **45.65** & \\   &  & WinGrande & 56.67 & 61.01 & 57.77 & 57.30 & 58.17 &  \\  & & PIQA & 71.38 & 75.46 & 69.91 & 70.89 & 73.83 & \\  & & ARC-C & 29.69 & 35.32 & 31.48 & 32.59 & 33.19 & \\   &  & WinGrande & **60.46** & **65.27** & **59.19** & **62.98** & **63.93** &  \\  & & PIQA & **73.45** & **75.90** & **73.07** & **74.37** & **77.31** & \\  & & ARC-C & **33.28** & **37.12** & **34.64** & **38.74** & **39.59** & \\   &  & WinGrande & 50.51 & 48.46 & 50.91 & 49.41 & 49.80 &  \\  & & PIQA & 53.26 & 53.54 & 53.37 & 50.92 & 54.73 & \\   & & ARC-C & 21.76 & 21.33 & 20.65 & 21.67 & 19.88 & \\   &  & WinGrande & **56.51** & **61.56** & **55.01** & **57.14** & **58.25** & \\   & & PIQA & **71.16** & **73.99** & **71.22** & **73.01** & **75.24** & \\   & & ARC-C & **30.20** & **33.79** & **30.20** & **34.30** & **33.79** \\   

Table 2: Accuracy of LLMs on 3 benchmarks under different KV cache quantization methods at varying bit widths.

    &  &  \\ {3-6

[MISSING_PAGE_FAIL:9]

We study the efficiency of CQ by comparing its decoding throughput with uncompressed FP16 KV cache using the HuggingFace Transformers implementation . We measure the decoding throughput by running LLaMA-2-7b on an A100-40GB GPU with CQ and FP16 KV cache to process a 100-token prompt and generate 1000 tokens. We increase the inference batch size until the GPU runs out of memory. Results are presented in Figure 4. CQ achieves 3.75\(\), 7.5\(\), and 15\(\) larger batch size than the FP16 baseline at 4-bit, 2-bit, 1-bit quantization, respectively. Moreover, CQ improves the decoding throughput by 1.4-3.5\(\) relative to the FP16 baseline. Additional latency measurements for CQ are presented in Section K in the Appendix.

## 5 Related Works

Existing works mitigate the high memory overhead of KV cache through token eviction, quantization, or tensor offloading. Scissorhands  and H2O  achieve KV cache compression while preserving model quality by storing only the pivotal tokens and evicting the unimportant ones. KIVI  proposes to quantize keys per-channel and values per-token using group-wise integer quantization. KVQuant  proposes sensitivity-based and dense-and-sparse quantization for KV cache to reduce quantization errors. Flexgen  proposes to offload KV cache instead of weights to enable high-throughput inference on a single GPU. Weight quantization  is an orthogonal line of work that reduce GPU memory requirements and improve effciency of LLM inference and fine-tuning . FlashAttention  and NoMAD-Attention  are system optimizations for speeding up LLM inference on GPUs and CPUs, respectively. Product quantization  is a method for nearest neighbor search that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and quantizing each subspace independently.

## 6 Conclusion

We propose Coupled Quantization (CQ) for mitigating the latency and throughput bottleneck of LLM inference by quantizing KV cache. We discover that channels of KV cache are highly interdependent, which implies the existing approach of per-channel quantization approach is sub-optimal. We propose channel coupling to exploit the interdependency across channels to achieve more information-efficient encoding of key/value activations. Extensive experiments demonstrate that our method outperforms competitive baselines in model quality in most cases, and can reasonably preserve model quality with KV cache quantized down to 1 bit.

## Limitations & Broader Impacts

KV cache quantization is a form of lossy compression which inevitably affects model quality. Although we study its effects on perplexity and accuracy, it remains unclear how it affects other aspects of the model such as hallucination and adversarial robustness. Making LLM inference more efficient contributes to the democratization of artificial intelligence and the reduction of carbon footprints. We expect no additional negative societal impacts other than the ones already posed by LLMs.

Figure 4: Inference throughput of CQ versus FP16 KV cache for LLaMA-2-7b.