# 1. Homogeneity:

Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals

Abhiram Iyer\({}^{1,\,3,\,4}\), **Sarthak Chandra\({}^{2,\,3}\)**, **Sugandha Sharma\({}^{2,\,3}\)**, and **Ila Fiete\({}^{2,\,3,\,4}\)**

\({}^{1}\)Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA

\({}^{2}\)Department of Brain and Cognitive Sciences, MIT, Cambridge, MA

\({}^{3}\)McGovern Institute for Brain Research, MIT, Cambridge, MA

\({}^{4}\)K. Lisa Yang Integrative Computational Neuroscience (ICon), MIT, Cambridge, MA,

{abiyer,sarthakc,susharma,fiete}@mit.edu

###### Abstract

Grid cells in the medial entorhinal cortex create remarkable periodic maps of explored space during navigation. Recent studies show that they form similar maps of abstract cognitive spaces. Examples of such abstract environments include auditory tone sequences in which the pitch is continuously varied or images in which abstract features are continuously deformed (e.g., a cartoon bird whose legs stretch and shrink). Here, we hypothesize that the brain generalizes how it maps spatial domains to mapping abstract spaces. To sidestep the computational cost of learning representations for each high-dimensional sensory input, the brain extracts self-consistent, low-dimensional descriptions of displacements across abstract spaces, leveraging the spatial velocity integration of grid cells to efficiently build maps of different domains. Our neural network model for abstract velocity extraction factorizes the content of these abstract domains from displacements within the domains to generate content-independent and self-consistent, low-dimensional velocity estimates. Crucially, it uses a self-supervised geometric consistency constraint that requires displacements along closed loop trajectories to sum to zero, an integration that is itself performed by the downstream grid cell circuit over learning. This process results in high fidelity estimates of velocities and allowed transitions in abstract domains, a crucial prerequisite for efficient map generation in these high-dimensional environments. We also show how our method outperforms traditional dimensionality reduction and deep-learning based motion extraction networks on the same set of tasks. This is the first neural network model to explain how grid cells can flexibly represent different abstract spaces and makes the novel prediction that they should do so while maintaining their population correlation and manifold structure across domains. Fundamentally, our model sheds light on the mechanistic origins of cognitive flexibility and transfer of representations across vastly different domains in brains, providing a potential self-supervised learning (SSL) framework for leveraging similar ideas in transfer learning and data-efficient generalization in machine learning and robotics.

## 1 Introduction

Grid cells in the medial entorhinal cortex are of paramount importance for navigating and representing spatial domains. Interestingly, a series of recent experiments have shown that the brain still uses the same cells to represent non-spatial environments that are continuously traversed. These include free viewing of naturalistic images, listening to and modifying a sound changing in pitch, navigating a conceptual 'Stretchy Bird' space (images of a bird that stretch or shrink via joystick input), or even traversing an 'odor' space among many others [8; 9; 10; 11; 12; 13; 14; 3; 15; 2] (Fig. 1a-d).

How does the brain transfer its ability to represent space to these non-spatial environments? More specifically, how does the brain infer a metric layout in these abstract domains? Answering this question involves understanding how the brain perceives and processes structure in different domains and modalities, and integrates them into coherent cognitive maps.

How the brain maps abstract cognitive environments has been a question of intense interest, with several proposed approaches [4; 5; 7; 6]. These works propose that when the brain learns the transition structure of a cognitive domain (such as the 'Stretchy Blob' space in Fig. 1e), it simultaneously builds a set of representations for these states as well as structures enabling transitions between them, Fig. 1f.

Our approach investigates how the brain exploits an existing scaffold of structured states, provided by grid cells, to represent new cognitive domains by projecting them onto the invariant grid coding space. We hypothesize that this projection involves anchoring a state in the cognitive domain to a grid phase and then extracting a self-consistent movement signal to measure displacements in the cognitive domain (Fig. 1g). This abstract velocity signal serves as the input to the grid cell network, updating grid phases and encoding transitions between high-dimensional states using the existing

Figure 1: **Conceptual understanding of learning velocities in abstract cognitive environments.****a.** Grid cells generate hexagonal tuning in spatial navigation experiments (illustration borrowed from ). **b.** Similar grid-like tuning has been found in auditory frequency space in rodents , (**c.**) in visual space in monkeys , and (**d.**) in an abstract cartoon bird space in human experiments. **e.** Example of an abstract, non-spatial domain called ‘Stretchy Blob’, a 2D Gaussian that can either stretch or shrink along two axes. **f.** SR, CSCG, and TEM [4; 5; 6; 7] learn transition structures of this cognitive domain by traversing it. These models require one to build a set of representations for these encountered states as well as structures enabling transitions between them. **g.** Our approach learns a self-consistent movement (velocity) signal that is independent of the states traversed and encodes the global transition structure of the environment in a minimally low-dimensional representation. **h.** How can the brain initially extract a general notion of velocity within abstract, non-spatial domains? Solving this important prerequisite challenge can show how grid cells flexibly map and organize various abstract spaces. **i.** The actual movement signals in this particular example are low-dimensional. These velocities are independent from the states they connect.

low-dimensional transition mechanism within the grid cell circuit. Crucially, rather than building a _new set of continuous stable states_ for each explored cognitive domain -- a process that is both difficult and slow -- this mechanism allows for the _efficient reuse of a canonical scaffold of cognitive states_ for the memory and coding of continuous variables.

In spatial domains, continuous attractor models of the grid cell circuit  efficiently reuse prestructured grid cell states to encode transitions between high-dimensional states. However, using such models to map non-spatial, abstract domains (and by extension, explaining how grid cells can be efficiently reused) requires the extraction of faithful representations of velocity in these domains. Thus, the fundamental, prerequisite challenge lies in extracting a general notion of a minimal dimensional velocity signal from high-dimensional, time-varying data from various abstract domains, Fig. 1h. Crucially, the representations of this extracted velocity must be independent of the specific states, and must be self-consistent with a net-zero velocity corresponding to a net identity transformation in the abstract space. These constraints point towards a self-supervised learning paradigm, wherein neither the coordinate system of the space nor the dimensionality of the estimated velocity are known a priori.

The following are the key contributions of the paper:

* **First neural network model for abstract velocity extraction.** We present the first neural network model explaining how grid cells flexibly represent different abstract spaces through providing a learning framework for velocity estimation in arbitrary spaces. This reuse of grid cells across domains leads to cognitive flexibility in mapping spaces, providing insights towards transfer learning and data-efficient generalization in machine learning and robotics.
* **State-independent velocity extraction.** Our framework for abstract velocity extraction generates state-independent, self-consistent low-dimensional velocity estimates by separating the content of abstract domains from their displacements (Fig. 1g,i).
* **Self-supervised geometric consistency constraint.** We show how consistent metric representations of velocity can be learned through a self-supervised geometric consistency constraint requiring displacements along closed loop trajectories to sum to zero -- an operation facilitated by the downstream grid cell circuit.
* **Superior dimensionality reduction performance.** Our method surpasses traditional dimensionality reduction and deep learning-based motion extraction networks, specifically in environments characterized by underlying low-dimensional transitions and motions between states.
* **Preservation of cell-cell relationships.** The model predicts that cell-cell relationships between grid cells are preserved _across_ different spatial and non-spatial domains. For example, if two grid cells are co-active in a spatial task, they should remain co-active in a non-spatial task. This prediction critically relies on the same continuous attractor-based grid module performing integration across domains -- a capability that can only be realized through extraction of a state-independent velocity from abstract domains. This prediction aligns with the observed invariance of internal neural representations across different spatial environments and brain states [17; 18; 19; 20; 21; 22; 23]. This forms a testable hypothesis for future experimental studies on neural representations in abstract cognitive domains.

### Related work

Our work on learning velocities and transition operators in abstract cognitive spaces can be compared to two key areas: neuroscience models of spatial mapping and dimensionality reduction models for high-dimensional data. We first discuss cognitive space mapping models and then relate them to dimensionality reduction work.

The Tolman-Eichenbaum Machine (TEM)  learns a map of a 'cognitive domain' via a recurrent neural network, predicting sensory observations based on given actions. However, TEM does not infer transition structures or affordances, relying instead on predefined actions at each time step. When presented with a new environment, TEM requires re-learning these affordances along with grid cell-like representations from scratch. Similarly, the successor representation (SR) [4; 5] predicts future states as a weighted sum of expected future occurrences and uses an underlying discrete action space. State-action variants of SR are dependent on these discrete action inputs to construct cognitive maps. Clone-structured cognitive graphs (CSCG)  learn hidden Markov models of sensory representations from observational inputs without prior domain structure assumptions but are limited to only discrete domains. By construction, these models are not guaranteed to preserve cell-cell correlations across domains and modalities and are unable to efficiently reuse the prestructured states and transition operators provided by grid cells.

More generally, these models posit that the brain simultaneously learns representations of the external states _and_ the transition structures to form cognitive maps. Here, we propose that approaching the cognitive mapping problem from a velocity-extraction-first perspective offers key benefits. By learning a low-dimensional velocity that captures transitions between external inputs, mapping the environment to reusable grid cell states can be performed simply by a continuous attractor network, instead of something more complex like TEM. More generally, via our self-supervised learning framework that infers velocity solely from sensory inputs in continuous domains, we can entirely avoid the computationally expensive task of representation and transition learning through capturing the minimal low-dimensional structure of these abstract observations.

Our work thus bridges the tasks of building cognitive maps of abstract domains and dimensionality reduction. Previous dimensionality reduction methods [24; 25; 26; 27; 28; 29] focus on preserving proximity in high-dimensional spaces on low-dimensional manifolds but do not take advantage of the structure imposed on the tangent spaces of these manifold (i.e., velocity spaces) through transitions between states. As a result, while these methods learn low-dimensional mappings, they often fail to reproduce the underlying metric space of abstract domains. Unlike these methods, our framework learns a global velocity operator applicable to any input within the environment's state manifold. Our work also relates to the task of motion decomposition and next-frame prediction of temporally varying inputs [30; 31; 32; 33; 34; 35]. However, these works have not attempted to decipher a minimally low dimensional description of velocity, making their learned representations unsuitable for grid cells to effectively map observed environments.

## 2 Self-supervision for velocity extraction

Learning action primitives purely through self-supervision, without access to true velocities in the space, requires careful consideration of the data generation process, the loss terms to extract meaningful neural representations, and the chosen neural network architecture. We will discuss each of these aspects in detail.

### Task setup

The initial step in our methodology involves the construction of tasks designed to facilitate the model's inference of the lowest-dimensional representations of velocity within cognitive domains.

We revisit the 'Stretchy Blob' environment that we previously introduced. This environment can be procedurally generated where state transitions are characterized by changes in the blob's width and height. As seen in Fig. 2a, given two images, \(i_{1}\) and \(i_{2}\), we aim to learn a function \(f\) that infers a low-dimensional velocity \(\) representing the transition from \(i_{1}\) to \(i_{2}\) (Fig. 2b).

To ensure that \(\) is a useful and faithful metric representation of the transition between two inputs, we demand that it satisfy two properties.

First, the estimated transition velocity \(\) can be used to transform a given input, i.e., there exists a function \(g\) that uses \(\) to transform \(i_{1}\) to predict \(i_{2}\). In practice, to ensure that \(f\) does not merely memorize image features of \(i_{2}\), we demand that \(g\) predict an unseen \(i_{3}\) obtained by transforming \(i_{2}\) using the same transition \(\). More generally, this ensures that the estimates of displacements, \(\), in this space must be independent of the route taken, ensuring both path and state independence. We refer to this requirement as 'next-state prediction'.

Second, the estimated velocities in the abstract space, \(\), must be geometrically self-consistent, i.e., the start and end point of a trajectory in the abstract space are identical if, and only if, the sum of estimated velocities is zero (Fig. 2c). (Note that this also ensures that the identity transformation should be represented by the zero-vector.) We refer to this requirement as 'loop-closure'. This summation of velocities must be performed by a neural integrator, such as grid cells, which results in a calibrated velocity input for grid cells to then themselves use to map out the input abstract domain.

These self-consistency constraints on \(\) suggests a _self-supervised learning paradigm, obviating the need for a specific coordinate system within the explored abstract space_.

To rigorously evaluate our system, we procedurally generate random trajectories across five different "abstract cognitive domains." These trajectories are generated by starting from an initial random state on the image manifold and then taking random velocities to determine the subsequent states on the same manifold. We assume for simplicity that states are unique at each point in space within these domains. We visualize a randomly sampled trajectory of each of these domains in Fig. 2d:

1. A 2D Stretchy Blob environment (discussed above) where a blob in the center of the visual field stretches or shrinks in height and/or width.
2. A 2D Stretchy Bird environment where a bird's legs and neck stretch and shrink. This environment is specifically constructed to mimic its experimental counterpart .
3. A 3D variant of Stretchy Bird where the two bird legs can independently transform.
4. A 2D Moving Blobs environment where a set of Gaussian blobs uniformly translate, moving in and out of the visual field.
5. A 1D Frequency Modulation task that emulates the experimental sound modulation task created by  with a sum of sine waves uniformly changing in frequency.

Detailed information regarding the generated data is provided in SI Sec. C.

### Encoder-Decoder architecture

We construct an encoder-decoder architecture, wherein the encoder \(f\) and decoder \(g\) are both multi-layer perceptrons (MLPs) (Fig. 3a). The encoder \(f\) processes two adjacent inputs \(i_{t}\) and \(i_{t+1}\) from any of our procedurally generated trajectories and maps them to a low-dimensional velocity space. It

Figure 2: **Task and fundamental problem setup.****a.** Two states in the ‘Stretchy Blob’ task that have a low-dimensional velocity representing the transition between them. **b.** If two consecutive images in this space \(i_{1}\) and \(i_{2}\) are separated by a velocity \(v_{1 2}\), can we learn a function \(f\) that estimates this velocity and a function \(g\) that ‘applies’ this quantity to \(i_{1}\) to predict \(i_{2}\)? **c.** The key self-consistency constraint we introduce called ‘loop-closure’: estimated velocities along a closed-loop trajectory must sum to zero. This computation is performed by a neural integrator, such as grid cells. **d.** Our various procedurally generated abstract cognitive domains: Stretchy Blob (2D), Moving Blobs (2D), Stretchy Bird (2D), Stretchy Bird (3D), and Frequency Modulation (1D).

is important to note that the low-dimensional velocity signal is not known a priori. The decoder \(g\) then upsamples this latent velocity representation and combines it with the last input to generate the subsequent \(_{t+2}\) which remains unseen to the model.

A single sample in a batch of data for this model comprises a trajectory of states \(=\{i_{t}\,|\,1 t N\}\) and velocities \(=\{v_{t t+1}\,|\,1 t N-1\}\) (individual velocities abbreviated as \(v_{t}\) for ease). The model is trained through triplets of frames of the input trajectory, using \(i_{t}\) and \(i_{t+1}\) to predict \(i_{t+2}\) for \(1 t N-1\). As discussed above, we train our models on triplets of states solely to ensure that the encoder does not memorize features of the image to be predicted. Training on pairs of images instead of triplets does not affect any of our results (SI Sec. B.2).

Detailed experimental procedures regarding the training of this model across various constructed domains are provided in SI Sec. C. We note that the same architectural motif was employed for training in all our experiments.

### Loss functions

We formalize our two requirements of the extracted low-dimensional velocity signal into two critical loss terms, a next-state prediction loss, and a loop-closure loss. These losses from the core of our self-supervised learning framework. To further refine the solution space, we also employ auxiliary losses in addition to our primary constraints. Our loss terms are visualized in Figure 3b and are described in detail in SI Sec. C.

* **Next-State Prediction Loss.** We quantify this based on the difference between the next-state prediction \(_{t+2}\) and the ground-truth frame \(i_{t+2}\). This loss term operates on individual samples of the generated trajectory, and ensures decodability of the estimated velocity \(\).
* **Loop-Closure Loss.** We quantify this as a norm of the sum of velocities along a closed loop trajectory \(\), i.e., the model must produce velocity estimates such that \(_{}=\). The error signal for this loss operates at the scale of the entire generated trajectory. For convenience, we construct all trajectories of our training data as random loops in the

Figure 3: **Self-supervised learning framework.****a.** Model diagram consisting of an encoder, decoder, and an integrator which acts on the low-dimensional velocity latent space. The model takes in two consecutive input frames and predicts an unseen frame with the learned velocity. **b.** Our various self-supervised loss terms. The two critical loss terms (_top_) are ‘next-state prediction’ and ‘loop-closure’. The auxiliary loss terms (_bottom_) which further refine the solution space are ‘shortcut estimation’ and ‘isotropy’.

considered abstract spaces, such that the start and end state of trajectories are identical. See SI Sec. B.2 for training data that is not solely loops.
* **Shortcut Estimation Loss.** The first of our two auxiliary loss terms is a shortcut estimation loss, which further tests the generalization ability of our decoder \(g\). From \(i_{t+1}\), we predict future states \(i_{t+3}\) or \(i_{t+4}\) by directly modifying \(\). Specifically, if \(_{2 3}\) is inferred from \(i_{t+2}\) to \(i_{t+3}\) and \(_{3 4}\) is inferred from \(i_{t+3}\) to \(i_{t+4}\), then \(_{t+4}\) should be \(_{2 3}+_{3 4}\) away from \(i_{t+2}\). This loss is important for further refining our velocity estimates and ensuring their validity during generalization.
* **Isotropy Loss.** The loss terms considered so far do not ensure isotropy in the inferred velocity space, allowing differential scaling factors for transformations in different input space directions. To induce isotropy, we introduce an auxiliary isotropy loss term that acts on the norm of the velocities, independent of direction. Since we don't assume access to the global velocity distribution in the training data, the isotropy loss is applied only near zero velocity. In particular, we minimize the variance of \(\{\|_{t t+1}\| d(i_{t},i_{t+1})<\}\), where \(d(i_{t},i_{t+1})\) is a similarity metric in the input image space, and \(\) is some small threshold. In practice, we use \(1-\) cosine similarity as our distance metric \(d\).

There is no direct supervision signal regressing the model outputs onto a known distribution of velocities; instead, the velocities are extracted and inferred automatically. Further, we do not a priori assume knowledge of the dimensionality of the underlying transition structure of the training domains.

Regardless of the training environment, the relative weighting ratio of the two critical loss terms remains consistent, with loop-closure loss always weighted ten times higher than next-state prediction loss. To stabilize the training of models with three-dimensional latent spaces, we include a small \(L_{1}\) regularization during training. All models are evaluated on an unseen testing dataset consisting of random walks within the domain. To prevent overfitting, all models are deliberately underparameterized relative to the training dataset. We conduct ablation studies on our loss terms, which can be found in SI Sec. B.1.

## 3 Experimental results

Single learning framework infers geometrically consistent representations of velocity across cognitive domains

For each abstract environment, we compare the model-inferred velocity representations (Fig. 4a) to the true distribution of velocities. Note that the self-supervised framework does not result in an exact identity mapping between the ground truth velocities and the model outputs -- it suffices for the obtained output to be a linear transformation of the ground-truth velocity space. Correspondingly, we

Figure 4: **Model results.****a.** Our model produces low-dimensional velocity latents that are similar to the ground truth (g.t.) distribution without knowing this distribution across a variety of cognitive environments. **b.** In cases where the model’s latent dimensionality is higher than the intrinsic velocity dimensionality of the environment, our model still identifies the lowest-dimensional representations embedded in higher-dimensional space.

construct an error metric on the inferred velocities as the mean squared error in mapping the predicted velocities to the ground-truth via a single linear transformation, after removing a small number of outliers via the DBSCAN clustering algorithm . More details about this error metric can be found in SI Sec. C.

In all cases, irrespective of the dimensionality of the input manifold space or the detailed statistics and structure of the environment states, we see that the inferred velocities are faithful metric representations of the ground-truth velocities, quantified in Table 1.

While we primarily consider cases where the latent dimensionality of the encoder output matches the underlying dimensionality of transitions in the input space, this is not a necessity. In Fig. 4b, we set up our framework to have latent dimensionalities larger than the true data manifold dimensionality. In all cases, the model outputs _automatically_ occupy a subspace of dimensionality that corresponds to the actual input manifold transition space, with a PCA of the inferred velocities capturing greater than \(97\%\) of the variance within the correct number of dimensions (cf. Fig. 4b). Thus, our model can effectively identify the appropriate low-dimensional structure within the high-dimensional embedding space of the inputs. Results from the other synthetic cognitive domains can be found in SI Sec. A.

### Comparison to existing dimensionality reduction methods

Our model estimates low-dimensional velocities between successive high-dimensional states. These velocities can then be integrated to determine a low-dimensional representation for each state. In this sense, it is possible to view our work as a dimensionality reduction method for continuously varying inputs. Traditional dimensionality reduction methods rely on the _statistics of distances between points_ across an ensemble of states. In contrast, our approach finds a _structured tangent manifold_ around each state that captures the low-dimensional transitions to successive states.

We can compare standard dimensionality reduction techniques such as PCA, Isomap, UMAP, and deep autoencoders with our method. To do so, we use these techniques to embed the inputs into the known

  
**Task** & **Our Model** & **MCNet** & **Autoencoder** & **PCA** & **Isomap** & **UMAP** \\  Stretchy Blob (2D) & **0.05 \(\) 0.01** & 1.95 \(\) 0.14 & (1.59 \(\) 3.40) \( 10^{3}\) & 0.63 & 0.42 & 0.79 \\ Stretchy Bird (2D) & **0.07 \(\) 0.03** & 2.01 \(\) 0.01 & 20.90 \(\) 38.04 & 0.21 & 0.36 & 0.46 \\ Stretchy Bird (3D) & **0.07 \(\) 0.02** & 2.96 \(\) 0.10 & (2.66 \(\) 5.08) \( 10^{2}\) & 0.31 & 0.64 & 1.05 \\ Moving Blobs (2D) & **0.02 \(\) 0.01** & 2.00 \(\) 0.01 & 2.03 \(\) 0.05 & 1.94 & 0.66 & 0.62 \\ Freq. Modulation (1D) & **0.02 \(\) 0.02** & 2.01 \(\) 0.01 & 2.00 \(\) 0.01 & 1.97 & 2.00 & 2.00 \\   

Table 1: **Mean and standard deviation of errors for various tasks and models.** Mean and standard deviation of errors are computed across 6 different runs for each experiment. Each run was seeded to ensure reproducibility. The 6 seeds were picked randomly and are the same seeds used across different experiments where multiple runs were run.

Figure 5: **Comparison to baselines.** We show our model comparisons to various dimensionality reduction and motion-prediction baselines in the **a.** 2D Moving Blobs and **b.** 1D Frequency Modulation tasks. Existing baselines cannot identify the low-dimensional velocity signals between arbitrary transitions in this space, even failing to do so in a simple one-dimensional domain. Meanwhile, our model produces results that closely match the true, underlying velocity distribution.

latent dimensionality of the data manifold, then compute an estimated low-dimensional velocity between states by taking the difference of their corresponding low-dimensional representations. Fig. 5 compares our model's estimated velocities with those produced by these standard baselines. Since our self-supervised framework uses a next-step prediction component, we also compared our results to MCNet  (a flexible deep network designed specifically for future frame prediction) and constrained the model to use low-dimensional representations of the transitions between frames. In all cases, our model significantly outperformed other baseline models (cf. Table 1) and produced velocities that were closely aligned with the true velocity distributions. Remarkably, for even one-dimensional manifolds embedded in high-dimensional spaces (as in our 1D Frequency Modulation task), existing dimensionality reduction techniques struggle to find a coherent representation for velocity and produce errors that are two orders of magnitude larger than our model.

We also examine dimensionality reduction through constructing representations of the original data via integrating the model velocity estimates, Fig. 6. For a sample trajectory through the high-dimensional states of the 2D Moving Blob environment, our models representation collapses onto a two-dimensional plane (consistent with the data, since the states can be minimally described through transitions in \(^{2}\)). PCA of the same set of states fails to capture this low-dimensional description, with \( 24\) principal components necessary to capture \(>95\%\) of the variance.

### Model outputs allow reuse of grid cells in mapping abstract domains

We hypothesize that the brain can map abstract cognitive domains to the manifold of grid cell states by learning low-dimensional velocity representations of the input space. Extracting this low-dimensional signal allows the brain to reuse continuous attractor dynamics and path integration functionalities to stably represent and traverse along the manifold of inputs in the mapped domain.

Figure 6: **Dimensionality reduction through our model, versus PCA.****a.** Schematic of the raw input states from a random trajectory of the 2D Moving Blob task, showing the states as points in a 16x16 dimensional space. **b.** We estimate 3-dimensional velocities between states, and integrate these estimated velocities to obtain a low-dimensional representation of the initial input states. A 2-dimensional plane is shown in gray for perspective, demonstrating our model-produced low-dimensional representations are approximately in a 2D subspace. **c.**_Left_: Computing PCA on the same dataset shows representations occupying a volume in a 3-dimensional space. _Right:_ Around 24 dimensions are required for PCA to capture 95% of the variance in the data, indicating that PCA is unable to find a low-dimensional space describing the dataset.

Figure 7: **Model outputs can be used to generate grid-like firing fields.** Our model’s outputs can be used as input to a synthetic grid cell network across a variety of cognitive domains. We predict a clear hexagonal-like firing field when traversing these environments, illustrating how the grid cell circuit is crucial to building these cognitive maps.

To verify that the model inferred velocities are faithful enough for accurate path integration, we construct the tuning curve of a random neuron from a synthetic grid cell module (details of which are briefly described in SI Sec. C) that is fed these inferred velocities (up to a best-fit linear transformation; for simplicity we choose our transformation to map to aligned velocities across environments, leading to orientation-aligned grid tuning curves -- this need not hold in experimentally observed tuning curves) as the path integration inputs. As seen in Fig. 7, our model results in hexagonal tuning curves in the abstract cognitive domains, consistent with previous work on grid representations [8; 9; 10; 11; 3; 15; 2] (grid firing fields from baseline models explored in SI Sec. A). Through extracting faithful low-dimensional representations of velocities across abstract domains, the _same_ continuous attractor-based grid modules can be used across tasks. As a result, if two grid cells fire in an overlapping (or non-overlapping) way in one mapped domain, they continue to be overlapping (or non-overlapping) in all other domains. Thus, cell-cell relationships are preserved between grid cells across abstract domains. This forms a testable hypothesis for future experimentation, that may be falsified if the brain were to use distinct, independent grid cell modules to organize information from different modalities, or if grid cells were not prestructured networks that function independently of the nature of the inputs.

We also note that while each grid cell module is a two-dimensional toroidal manifold, enabling integration of two-dimensional velocities, the grid cell system consisting of multiple modules can integrate velocities in higher dimensions. Thus, higher dimensional velocities extracted by our model (e.g., 3D Stretchy Bird) do not pose a problem for integration by grid cells.

## 4 Discussion

Our work introduces the first neural network model that can infer velocities within abstract cognitive domains. This enables circuits like grid cells to encode transitions between high-dimensional sensory states through low-dimensional path integration, mapping sensory inputs to prestructured states instead of learning independent representations for each state. Our velocity extraction mechanism itself requires path-integration (via the 'loop-closure' loss), necessitating a grid-cell-like neural model, highlighting how grid cells form the foundation of mapping abstract spaces.

**Future Work.** Our research offers new perspectives for neuroscientists on the flexible utilization of grid cells to organize and map non-spatial domains. Future neuroscience research may test our hypothesis on the conservation of cell-cell relationships across cognitive domains and identify brain regions that generate velocity signals, along with their experimental signatures. Future machine learning research directions include scaling our framework for naturalistic environments, learning non-Euclidean spaces like family trees, and possibly using our framework to augment existing cognitive space-mapping models like TEM or CSCG.

**Limitations.** While our core SSL loss terms (next-state prediction and loop-closure) are biologically plausible and may align with sensory prediction error and neural integration, the auxiliary losses (shortcut and isotropy) are less biologically supported. Additionally, we assume velocity vectors in our latent space commute, which prevents them from directly representing tangent vectors in non-Euclidean spaces like a sphere. However, non-Euclidean spaces can be represented by embedding them in a higher-dimensional Euclidean space where velocities commute (e.g., a sphere embedded in three-dimensional space)[38; 39].

**Broader Impact to AI.** Our novel SSL framework can be applied for invertible dimensionality reduction (by virtue of a generative decoder which generates high-dimensional states corresponding to points in a low-dimensional latent) and manifold learning tasks. Our model significantly outperforms non-invertible dimensionality reduction baselines on datasets that contain relatively lower-dimensional transitions (suggesting applications to video data, for example). Our method also naturally lends itself to manifold alignment-related challenges, which is particularly effective when the data exhibits a small number of continuous modes of variability. Moreover, with a small number of "gluing" points, our method allows for building one-to-one correspondences between different domains. Our work also shows how a fixed integrator circuit can leverage common velocity representations to navigate between abstract spaces efficiently. For example, in a complex maze where learning action strategies is costly, our model maps transitions and actions to the grid-cell representational space, enabling strategies learned in a simpler, topologically similar space to be effectively applied to the complex domain -- a relevant challenge in robotics.

Acknowledgements

We thank the McGovern Institute and the K. Lisa Yang Integrative Computational Neuroscience (ICoN) Center for supporting and funding this research. AI was supported by the K. Lisa Yang Integrative Computational Neuroscience (ICoN) Fellowship.