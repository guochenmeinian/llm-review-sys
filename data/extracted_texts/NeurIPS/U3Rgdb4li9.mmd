# Targeted Sequential Indirect Experiment Design

Elisabeth Ailer

Technical University of Munich

Helmholtz Munich

Munich Center for Machine Learning (MCML)

&Niclas Dern

Technical University of Munich

Jason Hartford

Valence Labs

&Niki Kilbertus

Technical University of Munich

Helmholtz Munich

Munich Center for Machine Learning (MCML)

###### Abstract

Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are multi-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.

## 1 Introduction

Experimentation is the ultimate arbiter of scientific discovery. While advances in machine learning (ML) and ever increasing amounts of observational data promise to accelerate scientific discovery in virtually all scientific disciplines, all hypotheses ultimately have to be supported or falsified by experiments. But the space of possible experiments is combinatorial, and as a result, experimentation in the physical world may become a major bottleneck of the scientific process and critically inhibit, for example, fast turnaround in drug discovery loops even in fully automated labs. Efficient adaptive data-driven strategies to propose the most useful experiments are thus of vital importance.

Targeted experimentation requires a well-formed hypothesis about the underlying system. We focus on hypotheses that are typically expressed in terms of causal effects or properties of functional relationships. For example, we may posit that there is some ground truth mechanism/function \(f\) that describes how a given phenotype depends on gene expression levels in a cell, or how the composition of microbes affects certain environmental health indicators. In the natural sciences, such mechanisms are typically (a) nonlinear, (b) dependent on multi-variate inputs, and (c) confounded by additional, unobserved quantities. For example, the way in which the gut microbiome composition affects energy metabolism in humans is likely confounded by various environmental and lifestyle choices. Due to

[MISSING_PAGE_FAIL:2]

identified form observational data. However, we argue that in such settings one should be aiming for a more targeted query, i.e., does not aim to identify \(f_{0}\) fully, but only a certain aspect of the mechanism.

**Scientific queries.** We represent the scientific query of interest by a functional \(Q:\) where \( L_{2}(X)\) is the space of considered mechanisms \(f\).1 Generic functionals can measure both local as well as global properties of any \(f\): simple average treatment effects via \(Q[f]:=[Y|do(X=x^{*})]=f(x^{*})\); when \(\) is a Hilbert space, projections of \(f\) onto a fixed basis function \(f^{*}\) via \(Q[f]:=(f^{*},f)_{}/\|f\|_{}^{2}\); the local causal effect of an individual component \(X_{i}\) on \(Y\) at \(x^{*}\) via \(Q[f]:=(_{i}f)(x^{*})\).2 Here, \(x^{*}\) could be the mean of the observed treatments, i.e., represent a 'base gene expression level' and we are interested in how a local change away from that base level affects the outcome. While our methodology applies to any functional, we focus our empirical experiments on causal effects of individual components as key scientific queries, such as how does up- or down-regulating a given gene affect the phenotype?

Learning experiments.Our goal is to sequentially learn a policy \(()\)3 such that data sampled from the joint distribution \(P_{}(X,Y,Z)=(Z)P(X\,|\,Z)P(Y\,|\,X)\) induced by the model in eq.1 optimally informs \(Q[f_{0}]\). We highlight that depending on \(f_{0},h\) and the distribution of \(U\), there may exist a policy \(\) such that \(Q[f_{0}]\) is identified from \(P_{}(X,Y,Z)\), but in general it may remain unidentified for all policies even in the infinite data limit. For a non-optimal policy, i.e., non-informative experimentation, we should expect \(Q[f_{0}]\) to be partially identified from \(P(X,Y,Z)\) at best. Therefore, we aim to estimate upper and lower bounds \(Q^{+}(),Q^{-}()\) of \(Q[f_{0}]\) and sequentially learn the policy \(\) that minimizes \(():=Q^{+}()-Q^{-}()\). In each round \(t[T]:=\{1,,T\}\), we observe \(n\) i.i.d. samples from \(P_{_{t}}(X,Y,Z)\) using policy \(_{t}\), which we use (potentially together with data collected under previous policies \(_{<t}\)) to estimate \(Q^{+}(_{t}),Q^{-}(_{t})\) and then aim to propose a new policy \(_{t+1}\) that yields a smaller gap in bounds: \((_{t+1})<(_{t})\).

Connections to other settings.Besides the 'indirect experimentation' lens, the setting in eq.1 can be viewed from different perspectives. While they all share the same mathematical formulation, they start from different conceptions of \(Z\). The literature on _invariant causal learning_(Peters et al., 2016; Heinze-Deml et al., 2018) may interpret \(Z\) in eq.1 as an environment indicator and \(f_{0}\) is invariant across these environments. Each policy corresponds to an environment (\(Z\) cannot be designed, but we can collect data for different \(Z\)) with its own distribution of \(X\). The heterogeneity across environments can be leveraged to learn about the invariant mechanism \(f\). In _instrumental variables_(Pearl, 2009; Angrist and Pischke, 2008), one typically assumes to encounter a situation as in eq.1 with the assumptions (1)-(3) where \(X,Y,Z\) are observed 'in the wild'. Valid instruments \(Z\) then sometimes allow for (partial) identification and estimation of \(f_{0}\) despite the unobserved confounder \(U\). Again, there is typically no active experimentation in IV estimation, but \(Z\) is taken 'as is'. Finally, experimentation on \(Z\) in our setting can also be interpreted as _soft interventions_(Jaber et al., 2020; Pearl, 2009) on \(X\), where we intervene on \(X\) setting it to follow a given distribution instead of a fixed value. Instead of allowing arbitrary soft interventions, eq.1 restricts us to distributions \(P(X\,|\,Z)(Z)\) for feasible experimental policies \((Z)\). Importantly, unlike proper soft interventions, we do not get rid of unobserved confounding. Ultimately, we face a sequence of IV estimation tasks. Experimental access to \(Z\) justifies the instrumental variable assumption (2) \(Z\,\!\!\,U\), and indirect experiments are setup by design such that (1) \(Z\,\!\!\,X\) holds. Assumption (3) \(Z\,\!\!\,Y\,|\,X,U\) instead may be hard to justify in practice and limits the types of experimentation allowed in our framework.

### Related Work

IV estimation.Instrumental variables are a cornerstone of cause-effect estimation in econometrics (Angrist and Pischke, 2008) at least since their use by Philip G. Wright in 1928 (Wright, 1928; Stock and Trebbi, 2003). Even though valid instruments are hard to come by in practice (Hernan and Robins, 2006), generalizations to settings with relaxed assumptions have sparked renewed interest in IV estimation from the machine learning community not least due to successful applications in Mendelian randomization (Sanderson et al., 2022; Didelez and Sheehan, 2007; Legault et al., 2024) or on microbiome data (Sohn and Li, 2019; Wang et al., 2020; Ailer et al., 2021). In particular, we build on recent advances in nonlinear/nonparametric IV estimation (Newey and Powell, 2003; Lewis and Syrgkanis, 2018; Singh et al., 2019; Hartford et al., 2017; Saengkyongam et al., 2022; Zhanget al., 2020; Xu et al., 2020), with a specific focus on the minimax formulation using the generalized method of moments (Liao et al., 2020; Dikkala et al., 2020; Bennett et al., 2023; Zhang et al., 2023; Liao et al., 2020; Bennett et al., 2019; Muandet et al., 2019; Bennett et al., 2022). In addition, we do not assume identifiability in line with recent approaches to partial identification and bounding causal effects in IV settings relaxing various discreteness and additivity assumptions (Kilbertus et al., 2020; Padh et al., 2022; Frauen et al., 2024; Melnychuk et al., 2024; Wang & Tchetgen Tchetgen, 2018; Gunsilius, 2019; Hu et al., 2021; Zhang & Bareinboim, 2021).

Sequential experiment design.The work closest to ours is perhaps by Ailer et al. (2023), where they consider a similar setting of sequential indirect experiment design via IV estimation. The key differences are that they only consider a fully linear setting (\(h,f_{0}\) linear), aim for full identification of \(f_{0}\) (even though the setting is underspecified), and only provide non-adaptive strategies, i.e., the sequence of experiments does not depend on data collected throughout the experiments. Other work on indirect experimentation either assumes no unobserved confounding (Singh, 2023) or focuses primarily on sample efficiency and variance reduction when aiming for full identification of \(f\)(Chandak et al., 2024). Adaptive learning has also been used in another context for cause effect estimation by 'deciding what to observe' Gupta et al. (2021). While clearly related, we highlight that most of the literature on active experimentation for causality aims at learning the causal structure (instead of estimating properties of \(f\)) from access to interventions, i.e., direct experiments (instead of more realistic indirect experiments), e.g., (von Kugelgen et al., 2019; Sverchkov & Craven, 2017; Agrawal et al., 2019; He & Geng, 2008; Gamella & Heinze-Deml, 2020; Elahi et al., 2024; Zemplenyi & Miller, 2021). Additionally, we require a strategy that is path-dependent, while in active learning the objective function remains the same over the different iterations.

## 3 Methodology

### Minimax Instrumental Variable Estimation

For the general instrumental variable setting in eq.1 that we face in each round, we aim to solve \([Y-(X)\,|\,Z]=0\). To solve this, we follow the conditional moment formulation (Bennett et al., 2019; Dikkala et al., 2020; Bennett et al., 2022, 2023), which builds on the observation that we only want to consider functions \(f\), such that the residuals, \(U_{f}:=Y-f(X)\) are independent of the instrument \(Z\). If two random variables are independent, \(U_{f}\!\!\! Z\), then \(E[g(Z)U_{f}]=E[g(Z)]E[U_{f}]\) for all test functions \(g\). We search for \(f\) that minimize the residual while maintaining the independence constraint by solving a minimax optimization that minimizes \(f\) over a worst case \(g\). The integral equation \([Y-(X)\,|\,Z]=0\) for \(f\) can be written as \(Tf=r_{0}\), where \(T\) is a linear, bounded operator mapping \(f:\) to \(Tf:=[f(X)\,|\,Z]:\) and \(r_{0}=[Y\,|\,Z]\). This is typically an ill-posed inverse problem where both \(T\) and \(r_{0}\) are not known but have to be estimated from i.i.d. data \(=\{(x_{i},y_{i},z_{i})\}_{i=1}^{n}\). Assuming \(f\) to come from a set of hypothesis functions \( L_{2}(X)\) and \( L_{2}(Z)\) a class of 'witness functions' or 'test functions', we can write the non-parametric IV problem as a minimax optimization problem of the form (Dikkala et al., 2020; Bennett et al., 2023)

\[*{arg\,inf}_{f}_{g}L(f,g)\] (2)

for some objective function \(L\) mapping tuples of hypotheses \(f\) and test functions \(g\) to \(\). While most existing approaches start by assuming that there exists a unique solution of \(Tf=r_{0}\)(Lewis & Syrgkanis, 2018; Liao et al., 2020; Muandet et al., 2019; Dikkala et al., 2020; Bennett et al., 2019), based on Lagrange multipliers Bennett et al. (2023) recently have shown that under a source condition on \(T\) and mild realizability assumptions regarding the capacity of the (statistically restricted) function classes \(\) and \(\)(Bennett et al., 2023, Assumptions 2, 3, 4), the solution of \(Tf=r_{0}\) is given by

\[f_{0}=*{arg\,min}_{f}_{g} {2}\|f\|_{L_{2}(X)}^{2}+ r_{0}-Tf,g_{L_{2}(Z)}=*{ arg\,min}_{}_{g}\,[f^{2}(X)]+ [(Y-f(X))g(Z)].\] (3)

This formulation of the objective is immediately amenable to empirical estimation from \(\) by using empirical averages for the expectations. Eq.3 can be viewed as a penalized optimization, where \(\,[f^{2}(X)]\) penalizes the solution towards the minimum \(L_{2}\) norm.

The inner maximization over test functions ensures that we only consider hypotheses \(f\) compatible with the conditional moment restrictions that enforce the required independence of the residuals. While there may still be multiple hypotheses \(f\) compatible with observational data in this way, Bennett et al. pick the \(f\) with the least \(L_{2}\) norm, yielding a unique solution. In the following, we build on this intuition to estimate bounds on a chosen scientific query \(Q\) among all \(f\) compatible with the observed data and assumed structure (via the moment restrictions).

### Sequential Policy Learning for Bounding Functionals

The source condition required for eq.3 is that \(r_{0}\) lies in the range of \(TT^{*}\), where \(T^{*}\) is the adjoint of \(T\). As Bennett et al. point out, this is a nontrivial restriction and paramount in ensuring uniqueness in eq.3. Without this source condition the IV problem is still ill-posed in that there may be multiple solutions \(f\) compatible with the observed data \(\) (even in the infinite data limit). One of our key realizations is that we can still meaningfully make use of the penalized optimization formulation in eq.3 to compute bounds on targeted queries about \(f\).

Let \(Q:\) be a functional on hypotheses \(\) that captures the scientific query. For any given joint distribution \(P_{}(X,Y,Z)\) induced by experimental policy \(\), to bound \(Q[f_{0}]\), we can compute

\[Q^{}()=_{f}_{g} Q[f]+[(Y-f(X))g(Z)]\;,\] (4)

where again the inner maximization aims at only considering hypotheses compatible with the conditional moment restrictions \(_{c}\) and the outer minimization aims at finding the \(f_{c}\) with the largest/smallest value of \(Q[f]\). The realizability assumption that \(f_{0}\) then clearly implies that \(f_{0}_{c}\) and the bounds \(Q^{}\) will contain \(Q[f_{0}]\). We note that specially when \(X\) is of higher dimension than \(Z\), in practice neither \(f\) nor the much more narrow query \(Q[f]\) are guaranteed to be identified. However, depending on \(\), we may shape \(P_{}\) such that \(_{c}\) becomes restricted enough to obtain informative bounds \(Q^{}\) on \(Q[f_{0}]\). Bennett et al. (2022) work out in detail when a linear functional of \(f_{0}\), but not \(f_{0}\) itself, is identified. If \(P_{}\) is such that \(T\) satisfies the source condition, certain queries \(Q\) (such as the \(Q[f]=\|f\|_{L_{2}}^{2}\) as a canonical example) may actually be identified and the bounds \(Q^{}\) will coincide.

We can now formulate our main policy learning goal as

\[_{()}()\;,()=Q^{+}()-Q^{-}()\;,\] (5)

with \(()()\) being a subset of implementable experimentation policies. In practice, we aim at approximating this optimization via sequential updates of \(_{t}\) over \(T\) rounds, where we observe \(n\) i.i.d. samples of \(P_{_{t}}\) at each round. This means that given \(_{t}\), we seek to choose \(_{t+1}\) such that \((_{t+1})<(_{t})\). The final output of our method is the interval \([Q^{-}(_{T}),Q^{+}(_{T})] Q[f_{0}]\). A key advantage of this formulation is that we need not assume identifiability of \(f\) nor of \(Q[f]\), will obtain valid (albeit potentially loose) bounds when the experimentation budget \(T\) is restricted, and that potential identifiability will be 'captured automatically' by the bounds coinciding. If after \(T\) rounds the bounds are still not informative, the experimenter can choose more expressive experiments (different \(()\) and even different \(\)), a more specific query \(Q\), or put stronger assumptions on the hypothesis class \(\).

### Solving the Minimax Optimization Problem

Let us unpack the sequential policy learning problem. At each round \(t[T]\), we need to solve two minimax problems for \(Q^{}(_{t})\) where the objective is estimated from finite data and then take a minimization step over the difference of the two solutions. To further complicate this 'tri-level' (min + minimax) optimization problem, we are optimizing over two function spaces \((,)\) and a family of distributions (\(()\)). In the remainder of this section we develop techniques to render these optimizations feasible in practice for suitable choices of \(,,()\).

Existing solutions for the minimax problems typically employ adversarial optimization (iterative gradient-based optimization over parametric function spaces) or kernel-based techniques. Since the former are usually hard and expensive to optimize reliably even without the outer minimization, we focus on the latter approach using reproducing kernel Hilbert spaces (RKHS) for test functions \(=_{Z}\) induced by a characteristic kernel \(k_{Z}:\). Then there exists a closed form expression for a consistent estimate of the inner supremum.

**Theorem 1** (Zhang et al. (2020)).: _If \([(Y-f(X))^{2}k_{Z}(Z,Z)]<\),_

\[}_{i=1}^{n}_{j=1}^{n}(y_{i}-f(x_{i}))k(z_{i},z_{j})( y_{j}-f(x_{j}))^{}\] (6)

_consistently estimates \(_{g_{Z},\|g\| 1} r_{0}-Tf,g_{_{Z}}\) from data \(\)._

Hence, for \(n\) samples \(}}{{}}P_{}\) we reduce the minimax optimizations in eq. (4) to

\[Q^{}()=_{f} Q[f]+}_{i=1}^{ n}_{j=1}^{n}(y_{i}-f(x_{i}))k(z_{i},z_{j})(y_{j}-f(x_{j}))^{}\,.\] (7)

One option to solve eq. (7) efficiently is via gradient-based methods leveraging highly efficient auto-differentiation packages for parametric function families, such as choosing neural networks for \(\), i.e., we optimize over a finite-dimensional real-vector \(\)--the weights of the neural network. Our framework may also benefit from other existing approaches to the minimax optimization from the literature, e.g., in Muandet et al. (2019); Dikkala et al. (2020). Parametric functions may have the advantage that they render the computation of \(Q[f_{}]\) feasible. For example, the causal effect of \(X_{i}\) on \(Y\) at \(x^{*}\) given by \(Q[f_{}]=(_{i}f_{})(x^{*})\) can be obtained directly from automatic differentiation as well. Global functionals such as \(Q[f_{}]=_{}(x)f_{}(x)\,dx\) for some function \(\) may pose greater difficulties as the integral over \(\) is typically infeasible. Still, this approach ultimately requires a bi-level optimization with a substantial amount of tuning.

In order to also obtain a closed-form estimate of the minimization over \(\), we have to specify the functional \(Q\). To this end, we focus on bounded linear functionals. One example is the causal-effect of an individual input \((_{i}f)(x^{*})\), because it is arguably one of the most common and relevant scientific queries: how does a small change of a specific variable affect the outcome? When following a fully non-parametric approach by also assuming hypotheses to come from an RKHS \(=_{X}\) induced by a characteristic kernel \(k_{X}:\), we can also estimate the minimization over \(\) in closed form for the causal effect--a local, linear functional. The key realization is that functionals of functions represented as linear combinations of RKHS functions can be written as linear functions of the coefficients.

**Theorem 2**.: _Assume functions in \(_{Z},_{X}\) to have bounded variation, (w.l.o.g.) images contained in \([-1,1]\), and for \(h_{Z}\) also \(-h_{Z}\). Denote by \(K_{XX},K_{ZZ}^{n n}\) the empirical kernel matrices of \(\), let \(k_{X}\) be continuously differentiable, and let \(_{g},_{f},_{c} 0\). Further, fix \(Q[f]\) to be a bounded linear functional, and write \(f_{}()=_{i=1}^{n}_{i}k(x_{i},)\). Then the solutions to the regularized minimax problems_

\[f^{}=*{arg\,min}_{f_{X}} Q[f]+_{c} _{g_{Z}} r_{0}-Tf,g_{_{Z}}+ _{f}\|f\|_{_{X}}-_{g}\|g\|_{_{Z}}\] (8)

_are consistently estimated by \(f_{}\) with_

\[^{}=(K_{XX}K_{ZZ}K_{XX}+4_{f}}{ _{c}}K_{XX})^{+}K_{XX}K_{ZZ}y}Q[K_{X}.](x^{*}) \,,\] (9)

_where \((Q[K_{X}.])(x^{*})^{n}\) is the vector \(((Q[k_{X}(x_{1},)])(x^{*}),,(Q[k_{X}(x_{n},)])(x^{*})\). Note that these general bounded linear functionals can be computed analytically for many common kernels such as linear, polynomial, or radial basis function (RBF) kernels._

One common example is the partial derivative at \(x^{*}\):

\[^{}=(K_{XX}K_{ZZ}K_{XX}+4_{f}}{ _{c}}K_{XX})^{+}K_{XX}K_{ZZ}y}(_{i}K_{X })(x^{*})\,,\] (10)

where \((_{i}K_{X})(x^{*})^{n}\) is the vector \(((_{i}k_{X}(x_{1},))(x^{*}),,(_{i}k_{X}(x_{n}, ))(x^{*})\).

We defer all proofs of our theoretical statements to Appendix A.

We can now substitute these closed-form estimates for the minimax problems into our policy learning objective \(()=Q[f_{^{+}}]-Q[f_{^{-}}]\) in eq. (5)

\[()=-}(K_{XX}K_{ZZ}K_{XX}+4_{f}}{_{c}}K_{XX})^{+}(Q[K_{X}])(x^{*})^{}(Q[K _{X}.])(x^{*})\,.\] (11)

**(In)validity of bounds.** Our general policy learning formulation in eqs. (4) and (5) involves nonlinear, nonconvex optimizations over function spaces and families of distributions. Since we cannot provably obtain global optima to those, we cannot guarantee valid bounds. Even for the RKHS setting in Theorem 2 for linear, while the estimates are consistent, for finite sample estimation we have to regularize both the hypotheses and witness functions () to obtain numerically stable and reliable estimates. Clearly, those pose restrictions on the search spaces for and may thus lead to invalid bounds. We may expect real-world mechanisms to be relatively smooth such that these regularization terms do not render our estimates invalid in practice. Moreover, since we made no explicit assumptions about the functional, using as a penalty (as compared to, e.g., may not yield a unique solution to eq. (8) even in the infinite data limit and for. While uniqueness can be recovered for strictly convex, coercive, and lower semicontinuous, this implies non-trivial restrictions on the allowed scientific queries, which we may not be willing to tolerate. Therefore, we introduce a 'penalization parameter' (in front of the supremum term rather than for convenience) that allows us to empirically trade off the scale/importance of and the conditional moment restriction in practice. Enforcing conditional moment restrictions via a minimax formulation from finite samples typically exhibits high variance. Therefore, a lack of provably valid bounds due to practical regularization does not impair the usefulness of our method in a setting where we aim to learn about a potentially unidentified scientific query from limited experimental access. In particular, we argue that overly conservative bounds are still more useful than likely invalid point estimates based on one-size-fits-all assumptions required for identifiability.

Continuing with the closed-form expression in eq. (11) for the gap between maximally and minimally possible values of the scientific query - here the causal effect of changing one treatment component around a base level - over all hypotheses that are compatible with the observed data, we can now seek to represent the experimentation strategy in a way that allows us to sequentially improve it.

### Sequential Experiment Selection

We now describe different approaches to sequentially update the experimentation policy. The experimentation budget and the number of samples per round are fixed. We denote by the policy in round and by () () the data collected in (up to) round. We denote by and the bounds and gap between them, respectively, estimated in round and explicitly mention which data was used for the estimates. Some strategies rely on a'meta-distribution' over policies, which we denote by. A sample is thus a policy in. While the 'final result' of all strategies are just the final bounds, we report also for suitable for different strategies to compare efficiency.

**Simple baseline.** In modern methods for experiment design, random exploration is often surprisingly competitive (Ailer et al., 2023). We consider a simple entirely non-adaptive baseline called **random**, which independently samples for and estimates on.

**Locally guided strategies.** Next, we look to simple adaptive strategies that leverage the locality of. If is determined in a small neighborhood around some, a useful guiding principle for is to aim at concentrating the mass of the marginal around. The causal effect or the average treatment effect are examples of local queries. Estimating such local relationships in indirect experimentation without unobserved confounding has recently been studied from a theoretical perspective by Singh (2023). They propose a simple explore-then-exploit strategy and prove favorable minimax convergence rates depending on the complexities of and as well as the noise levels in the unconfounded setting, i.e., and have independent noises and does not depend on. We highlight that these strategies do not use the intermediate estimated bounds as feedback to select the next policy. Pseudocode for the different strategies is in Appendix G.

1. **Explore then exploit (EE)** (inspired by Singh (2023)): Split the budget into. Sample for for (). Then, fit as a (parametric) distribution on the values of the samples in closest to (i.e., their values have the smallest distance to for some suitable distance measure on ). Set and estimate from. The split between and may be informed by to maximize exploration while retaining sufficiently many samples during exploitation for a low variance estimate of.
2. **Alternating explore exploit (AEE)**: Throughout all rounds, keep a list of all observed samples sorted by the distance of their values to. For odd independently sample\(_{t}\). For even \(t[T]\) and \(t=T\) fit \(_{t}\) as a (parametric) distribution to the \(\{K,n t\}\) nearest samples to \(x^{*}\) for some \(K\) and estimate \(Q_{t}^{}\) on \(_{\,t}^{(t)}\).

There is some freedom in which data is used to estimate \(Q_{t}^{}\) and using \(^{( t)}\) is always a valid choice. For local queries it can still be beneficial in practice to discard data far away from \(x^{*}\). More broadly, the above strategies could potentially be further improved by weighing samples in \(^{(t)}\) inversely proportional to the distance of (the mean of) \(_{t}\) from \(x^{*}\) when estimating \(Q_{t}^{}\) from \(^{( t)}\). We only report the vanilla versions described above in our experiments and leave further variance reduction techniques for future work. In all experiments, we use \(K=n\) and the Euclidean distance on \(\).

While these locally guided strategies appear rather limited in which information from previous rounds is used in updating the policy, Singh et al. (2019, Sec. 7) make a convincing case for why the simple types of active learning in EE and AEE greatly improve over the naive random strategy and may be competitive with fully adaptive experiments. For the fixed policy in \(\) and the meta-distribution \(\) one should arguably err on the side of 'uninformative' priors, i.e., covering all of \(\) mostly uniformly.

**Targeted Adaptive Strategy.** We now develop an adaptive strategy denoted by **adaptive** that actually uses the intermediate estimates of \(Q_{t}^{}\) to update \(_{t}\). A natural choice for policy updates is via gradient descent on our objective \(\)

\[_{t+1}=_{t}-_{t}_{}()|_{=_{t}}\,,\] (12)

for step sizes \(_{t}>0\) at round \(t\). In practice, we assume parametric policies \(_{t}:=_{_{t}}\) with parameters \(_{t}^{d}\). With the log-derivative trick (Williams, 1992) we then compute

\[_{}(_{})=_{X,Y,Z P_{_{}}(X,Y,Z)} [(_{})_{}_{}]\,.\] (13)

and can update \(_{_{t}}\) akin to the REINFORCE algorithm with horizon one. Since the gradient in eq. (12) is evaluated at \(_{_{t}}\) from which we have samples available, we can directly use the empirical counterpart of the expectation in eq. (13). One caveat is that our objective \(\) cannot be computed on the 'instance level' for individual samples \((x_{i},y_{i},z_{i})\), but is already an estimate based on multiple samples. In practice, we therefore split the \(n\) observations per round into random batches, and average over the \((_{})\) estimate for each batch times the mean score function across the batch in eq. (13).

An extension to use data \(^{()}\) from a previous round \(<t\) is to reweigh the term within the expectation in eq. (13) by \(_{_{t}}(Z)/_{_{t}}(Z)\) (and then average across rounds). While this allows us to use more data for the gradient estimates, this approach does not yield an unbiased estimator and typically also increases variance due to arbitrarily small reweighing terms. In practice, we may want to only consider data from a small number of previous rounds, where we expect the policy not to have changed too much. Again, we believe our technique could benefit from further variance reduction techniques (Chandak et al., 2024), but leave these for future work.

While our formulation in eqs. (12) and (13) can be efficiently implemented for any parametric choice of \(_{}\), in our experiments we choose a multivariate Gaussian mixture model (GMM) \(_{}(z)=_{m=1}^{M}_{m}(z;_{m},_{m})\) with weights \(^{M}\) with \(_{m=1}M=1\), means \(_{m}^{d_{z}}\), and covariances \(^{d_{z} d_{z}}\) for \(i[M]\). Hence, the parameters are \(=(,_{1},,_{M},_{1},,_{M})\). With the score function \(_{}_{}\) known analytically for GMMs, eq. (13) can be estimated efficiently (see Appendix B for details).

## 4 Experiments

**Setup.** We consider a low-dimensional setting (for visualization purpose) with \(d_{x}=d_{z}=2\) and

\[h_{j}(Z,U)=((Z_{j})(1+U)), f(X)=_{j=1} ^{d_{x}}(X_{j})(X_{j})+U\,, U(0,1)\,,\] (14)

where we use \(==20\). The local point of interest is \(x^{*}=0^{d_{x}}\) and we easily check that \(_{i}f(x^{*})=\). We use \(n=250\) samples in each round over a total of \(T=16\) experiments.

**Method parameters.** We use radial basis function (RBF) kernels \(k(x_{1},x_{2})=(\|x_{1}-x_{2}\|^{2})\) with a fixed \(=1\). Note, that the three hyperparameters are relative weights. Thus we set \(_{s}:=_{f}}{_{e}}=0.01\) and \(_{c}=0.04\), we refer to the Appendix C for a further comparison of different hyperparameter choices. For all strategies the variance of our policy is set at \(_{e}=0.001\). For explore then exploit, we chose \(T_{1}=10,T_{2}=6\) with \(_{t}=(_{t},_{e}_{d_{z}})\) and independent \(_{t}(_{d_{z}},_{d_{z}})\) for \(t[T_{1}]\). The Gaussian mixture of the adaptive strategy is initialized with \(M=3,=(}{{3}},}{{3}},}{{3}}), _{m}=_{d_{z}}\) and independent \(_{m}(_{d_{z}},_{d_{z}})\). We use a constant learning rate \(_{t}=0.01\) for all \(t\) and restrict ourselves to learning only weights \(_{m}\), means \(_{m}\) and the diagonal entries of the covariances \(_{m}\).

**Results.** We perform \(n_{}=50\) runs for different random seeds and report means with the \(10\)- and \(90\)-percentiles in Figure 1. The simple non-adaptive random baseline starts out with poor performance as expected, and improves mildly over multiple rounds. We note that the ultimate performance of the random baseline depends primarily on how much mass the random exploration (defined by the meta-distribution \(\)) puts near \(x^{*}\), i.e., whether we collect sufficiently many samples near \(x^{*}\) over the \(T\) rounds. Explore then exploit performs quite well as soon as we start exploitation. Again, whether sufficient informative examples have been collected during the exploration stage depends on the choice of \(\). However, explore then exploit does outperform random (with the same exploration distribution) indicating that the heuristic of focusing on the informative samples (the ones near \(x^{*}\)) does provide substantial improvements (as also found by (Singh, 2023)). The alternating strategy AEE provides bounds across all rounds and clearly shows step-wise improvement from the first round on--ultimately performing similar to EE. Finally, the adaptive strategy quickly narrows the bounds and achieves a fairly narrow gap \(\) already after few rounds. At \(T=16\) it has essentially identified the target query \(Q[f_{0}]=20\) with low variance across seeds. For completeness, we provide a runtime comparison of the different methods in Appendix D. The code to reproduce results is available at https://github.com/EAiler/targeted-iv-experiments.

## 5 Discussion and Conclusion

Summary.We formalized designing optimal experiments to learn about a scientific query as sequential instrument design with the goal of minimizing the gap between estimated upper and lower bounds of a target functional \(Q\) of the ground truth mechanism \(f_{0}\). We only assume indirect experimental access, allow for unobserved confounding, and consider nonlinear \(f_{0}\) with multi-variate inputs such that both \(f_{0}\) and \(Q[f_{0}]\) may be unidentified. For a broad set of queries, we derive closed form estimators for the bounds within each round of experimentation when \(f\) lies within an RKHS. Based on these estimates, we then develop adaptive strategies to sequentially narrow the bounds on the scientific query of interest and demonstrate the efficacy of our method in synthetic experiments. Given the increased amount of data collected in fully or partially automated labs, for example for drug

Figure 1: We compare the different strategies in our synthetic setting. Left and **right** only differ in the range of the y-axis. The black constant line represents the true value of \(Q[f_{0}]\). **Top:** Estimated upper and lower bounds \(Q^{}_{t}\) over \(t[T]\) for \(n_{}=50\) and two different ‘zoom levels’ on the y-axis. Lines are means and shaded regions are \((10,90)\)-percentiles. **Bottom:** The final estimated bounds \(Q^{}_{T}\) at \(T=16\). The dotted line is \(y=0\). Both locally guided heuristic (explore-then-exploit, alternating explore exploit) confidently bound the target query away from zero with a relatively narrow gap between them. Our targeted adaptive strategy is even better and essentially identifies the target query \(Q[f_{0}]=20\) after \(T=16\) rounds.

discovery, we believe that efficient, adaptive experiment design strategies will be a vital component of data-driven scientific discovery.

**Limitations and future work.** We have not validated our method in a real-world setting, as it would require access (and full control) over an actual experimental setup as well as the time and resources required to conduct these experiments. A key technical limitation of our work is that neither our general policy learning formulation in eqs. (4) and (5) nor the concrete closed form estimates in Theorem 2 obtain provably valid bounds for all queries \(Q\) from finite samples (see discussion right before Section 3.4). While our approach is reliable in synthetic experiments, we believe thorough theoretical analysis of necessary and sufficient conditions for (asymptotically) valid bounds (including ideally asymptotic normality with known asymptotic covariance or even finite sample guarantees) is a useful direction for future work beyond the scope of our current manuscript. In practice, the applicability of our approach may also limited by the assumptions that the underlying system is well described via a fixed, static function \(f_{0}:\) as opposed to, say, a temporally evolving and interacting systems governed by a differential equation. Similarly, while IV assumptions (1) and (2) can arguably be justified in our setting, the third assumption \(Z\!\!\! Y X,U\) limits the types of experimentation we can consider (see discussion at the end of Section 1).

Methodologically, we believe that our approach could benefit from advanced variance reduction techniques and be sped up by more efficient estimators (Chandak et al., 2024). Along these lines, it is worthwhile future work to analyze the optimal trade-off between exploration and retaining a large number of samples for exploitation and thus lower variance estimates. Finally, our current empirical validation is limited to linear local functionals, rather simple parametric choices for policies (such as (mixtures of) Gaussians), and we have not optimized the kernel choices and hyperparameters. Hence, a validation where all these choices have been tailored to a real-world application scenario is an important direction for future work.