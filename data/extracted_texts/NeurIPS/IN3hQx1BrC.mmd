# Task-aware world model learning

with meta weighting via bi-level optimization

Huining Yuan Hongkun Dou Xingyu Jiang Yue Deng

School of Astronautics, Beihang University, Beijing, China

{hnyuan, douhk, jxy33zrhd, ydeng}@buaa.edu.cn

Corresponding author

###### Abstract

Aligning the world model with the environment for the agent's specific task is crucial in model-based reinforcement learning. While value-equivalent models may achieve better task awareness than maximum-likelihood models, they sacrifice a large amount of semantic information and face implementation issues. To combine the benefits of both types of models, we propose Task-aware Environment Modeling Pipeline with bi-level Optimization (TEMPO), a bi-level model learning framework that introduces an additional level of optimization on top of a maximum-likelihood model by incorporating a meta weighter network that weights each training sample. The meta weighter in the upper level learns to generate novel sample weights by minimizing a proposed task-aware model loss. The model in the lower level focuses on important samples while maintaining rich semantic information in state representations. We evaluate TEMPO on a variety of continuous and discrete control tasks from the DeepMind Control Suite and Atari video games. Our results demonstrate that TEMPO achieves state-of-the-art performance regarding asymptotic performance, training stability, and convergence speed.

## 1 Introduction

Reinforcement learning (RL) achieves intelligent behavior by optimizing sequential decision-making through a trial-and-error process (Sutton and Barto, 2018). While RL has shown outstanding success in tasks like Go and video games, the enormous quantity of samples required to train such agents poses great limitations on RL's application in real-world scenarios involving human operators, real robots, or computationally expensive simulators (Moerland et al., 2023).

Model-based reinforcement learning (MBRL) aims to enhance the sample efficiency and generalization capability of RL agents through two interleaved stages: _model learning_ and _behavior learning_. In the _model learning_ stage, an approximate world model of the environment is learned using real environmental samples to provide the agent with the ability to generate simulated experiences or predict the outcome of actions. In the _behavior learning_ stage, the agent learns its policy by interacting with the model without having to take actions in the real environment. This paradigm has received a lot of attention, and significant progress has been made in both model learning and behavior learning. (Sutton, 1991; Ha and Schmidhuber, 2018; Janner et al., 2019; Kaiser et al., 2019; Hafner et al., 2019a; Schrittwieser et al., 2020).

One of the most common approaches to building a model is to learn a deep generative model through maximum likelihood estimation (MLE) on environmental trajectories (Buesing et al., 2018; Hafner et al., 2019b, a, 2020; Ozair et al., 2021). Such models can leverage advances in probabilistic modeling, and can easily be combined with state-of-the-art model-free RL agents for task behavior (Ha and Schmidhuber, 2018; Kaiser et al., 2019). However, MLE reconstructs all information fromenvironmental observations equally, overlooking the information needed for learning specific task behavior. Since the model is only an approximation of the real environment, model errors can create a gap between maximizing model return and maximizing environment return, leading to poorly learned policies (Lambert et al., 2020). Therefore, it is desirable to minimize such a gap by learning a model that can accurately predict those states that have a higher impact on the agent's task policy.

Driven by this goal, value equivalent models are designed to predict future state (or state-action) values rather than the raw observations, such that the model learns to preserve only value-relevant characteristics of the environment (Farahmand et al., 2017; Schrittwieser et al., 2020; Zhang et al., 2020; Grimm et al., 2020, 2021; Antonoglou et al., 2021; Nikishin et al., 2022). Nonetheless, such models often rely on specially-tailored planning algorithms for behavior learning (Schrittwieser et al., 2020), and may face challenges with implementation and optimization (Farahmand et al., 2017), constraining their scalability and robustness across different tasks and learning strategies. Additionally, the substantial amount of semantic information that is discarded during such value-focused learning can be useful for learning an effective policy.

Is there a favorable trade-off between MLE-based models and value equivalent models, where we can enjoy the merits of both worlds? In this work, inspired by recent advances in meta-learning (Nichol et al., 2018; Shu et al., 2019; Jiang et al., 2022), we propose a bi-level framework for model learning, in which we introduce an additional level of optimization on top of an MLE-based model by incorporating a meta weighter network that assigns importance weights to each training sample in the MLE objective function. The meta weighter is then trained to generate novel sample weights by minimizing a proposed task-aware model loss. Under this hierarchical framework, the meta weighter in the upper level of optimization learns to prioritize those samples with a positive impact on closing the task-relevant gap between the environment and model. The model in the lower level is then forced to focus on important samples, while still learning to reconstruct environmental observations and thus, form a state representation with rich semantic information to facilitate policy learning. We name our framework Task-aware Environment Modeling Pipeline with bi-level Optimization (TEMPO).

We build our bi-level framework TEMPO on top of DreamerV2 (Hafner et al., 2020), a powerful MBRL algorithm with a sequential VAE-like model and an actor-critic agent. A simple approximation of the gradient is proposed to update the meta weighter efficiently during implementation. We evaluate the novelty of TEMPO on challenging visual-based RL benchmarks, including both continuous control tasks from the DeepMind Control Suite (Tassa et al., 2018) and discrete control tasks from Atari video games (Bellemare et al., 2013). Results show that our framework achieves state-of-the-art performance compared with model-free RL algorithms, and exceeds the original DreamerV2 in terms of asymptotic performance, training stability, and convergence speed. Furthermore, we perform ablation studies to demonstrate the advantage of our proposed meta-weighting mechanism.

## 2 Task-aware Environment Modeling Pipeline with bi-level Optimization

In this work, we focus on visual-based RL control tasks, which are commonly formulated as partially observable Markov decision processes (POMDPs) with discrete time steps \(t[1:T]\), high-dimensional observations \(o_{1:T}\) (usually images in visual-based cases), continuous or discrete vector-valued actions \(a_{1:T}\), and scalar rewards \(r_{1:T}\). The observations and rewards are generated by the black-box environment \(o_{t},r_{t} p(o_{t},r_{t}|o_{<t},a_{<t})\), with actions generated by the agent \(a_{t} p(a_{t}|o_{ t},a_{<t})\). The goal of RL agents is to maximize the expected sum of rewards \(_{p}(_{t=1}^{T}r_{t})\).

In this section, we first briefly introduce the world model from DreamerV2 (Hafner et al., 2020) as the foundation of our work. Then, we propose a loss function for evaluating the task awareness of such a MLE-based world model. Finally, we introduce our main contribution, a bi-level model learning framework where a meta-weighting mechanism is proposed to subtly combine the world model with our task-aware model loss.

### The world model

We start with the Recurrent State-Space Model (RSSM) proposed in DreamerV2 (Hafner et al., 2020). The RSSM is a probabilistic graphical model similar to a sequential VAE (Kingma and Welling, 2013; Sohn et al., 2015). It conditions on past observations and actions to model the distribution of state transitions that have occurred in the environment.

Consider a trajectory \(\) in the environment, which comprises a sequence of observations, actions, and rewards \(=\{o_{t},a_{t},r_{t}\}_{t=1}^{T}\). The RSSM introduces deterministic states \(h_{1:T}\) and stochastic states \(s_{1:T}\) as latent variables for each time step. The observations and rewards are then conditionally generated from these states. Specifically, the RSSM consists of 4 main components

\[&h_{t}=f_{}(h_{t-1},s_{t-1},a_{t-1})\\ &s_{t} p_{}(s_{t}|h_{t})\\ &o_{t},r_{t} p_{}(o_{t},r_{t}|h_{t},s_{t})\\ &s_{t} q_{}(s_{t}|h_{t},o_{t})\] (1)

The deterministic state module recurrently outputs \(h_{t}\), whereas the stochastic state module, which corresponds to the prior network in a conditional VAE, predicts the prior \(s_{t}\) at each time step. These two state modules work together to perform the state transitions, while the observation & reward module acts as a decoder to reconstruct \(o_{t}\) and \(r_{t}\) from \(h_{t}\) and \(s_{t}\). To enable end-to-end training through variational inference (Kingma and Welling, 2013), an additional representation module, which serves as an encoder, is used to infer the posterior \(s_{t}\). For clarity, we denote the entire model together as \(M_{}\) with parameters \(\).

All components of the model are trained to maximize a variational bound on the trajectory log-likelihood (also known as the Evidence Lower Bound, ELBO) using gradient-based methods (Kingma and Welling, 2013). With Jensen's inequality, the variational bound is written as (see the Appendix for the full derivation)

\[& p_{}(o_{1:T},r_{1:T}|a_{1:T}) ^{}(;)=_{t=1}^{T}_{t}(o_{ t },r_{ t},a_{<t};)\\ &=_{t=1}^{T}_{q_{}(s_{t }|h_{t},o_{t})}p_{}(o_{t},r_{t}|s_{t},h_{t})}_{}-_{q_{}(s_{t-1}|h_{t-1},o_{t-1})} q_{}(s_{t}|h_{t},o_{t})\,\|\,p_{}(s_{t}|h_{t}) }_{}\] (2)

Here, we denote the objective function as \(^{}\). For each time step, the variational bound consists of two terms, i.e. the reconstruction accuracy of \(o_{t}\) and \(r_{t}\), and the KL-divergence between the variational posterior and the predictive prior as a regularization. During model learning, the RSSM learns the latent dynamics of the environment from real trajectories by predicting future observations and rewards based on past observations and actions. During behavior learning, the RSSM generates trajectories by unrolling the state vectors using the deterministic and stochastic state modules given actions from an agent, allowing the agent to learn task behavior from simulated experiences in the compact latent state space.

Figure 1: Main workflow of the proposed bi-level model learning paradigm TEMPO, corresponding to steps 3-8 in Algorithm 1. We start with (a) inferring the states and meta weights with model \(M_{}\), value function \(V\), and meta weighter \(N_{}\), then sequentially perform (b) the lower optimization on the model and (c) the upper optimization on the meta weighter.

### The task-aware model loss

The RSSM is a variational world model trained through MLE, which is designed to reconstruct all information in environmental observations, without taking into account the specific task of the agent. This can lead to a gap between maximizing model return and actually maximizing environment return. To understand this, consider a given state-action pair \((s,a)\) and a typical actor-critic RL agent. If the consequent state predicted by the model is off by \(\) from the ground truth \(s^{}\), the state value predicted by the critic will also have an error \(V(s^{}+)-V(s^{})\). As the actor's objective is to maximize state values, such errors in the predicted state values can lead to a sub-optimal policy. Therefore, if we want to truly align the model and environment for a specific task, it is intuitive to evaluate a model's performance using some type of value-relevant metric.

Different from MLE, Farahmand et al. (2017) proposed Value-Aware Model Learning (VAML), a loss that evaluates a model's performance by the impact of model errors on the value estimation accuracy. Given an environment transition distribution \(p\) and its model approximation \(\), a distribution over the state-action space \(\), and a value function \(V\), the VAML loss is written as

\[^{}(p,;,V)=(s,a)|s,a)V(s^{})s^{}}_{}-(s^{}|s,a)V(s^{})s^{}}_{ }^{2}(s,a)\] (3)

The primary concept behind VAML is to penalize a model based on the disparity between the state values of predicted states and those of the ground truth states. However, this loss function has several critical limitations when it comes to implementation on variational world models like the RSSM. In particular, VAML requires a value function that works in a pre-defined state space or the original observation space, while in DreamerV2, the critic from the agent works in the compact latent state space of the RSSM. Secondly, resolving or approximating the expectations/integrals is challenging in a realistic RL environment, where the state space is often vast and continuous. Additionally, VAML compels the model to conserve only the value-relevant characteristics, leading to the rejection of a considerable amount of semantic information contained in the environmental observations.

To construct a task-aware model loss suitable for evaluating the RSSM in the latent state space, we explicitly distinguish prior and posterior states, and replace the environment value estimation with values from the inferred posterior states, model value estimation with values from the predicted prior states, and the expectations with an empirical summation over a given dataset \(\), resulting in a parsimonious task-aware loss function, which we name V-VAML (Variational VAML)

\[^{}(;V,)=_{t=1}^{T}V(s^{}_{t})-V(s^{}_{t})^{2}\] (4)

Here, we introduce superscripts to differ posterior and prior states. Notice the states are outputs of the model, and depend on the model's parameters \(\) deterministically through reparameterization, i.e. \(s^{}_{t}(o_{ t},a_{<t};)\) and \(s^{}_{t}(o_{<t},a_{<t};)\). We leave out the inputs for simplicity.

The intuition of V-VAML loss is quite similar to the original VAML loss, yet far easier to implement given a dataset. It is also evident that \(^{}\) has some similarities with the KL regularizer in \(^{}\), where a large distance between the posterior and prior is undesirable in both objectives. The difference is that \(^{}\) evaluates the impact of such distance using the value disparity. One could try somehow replacing the KL regularizer with \(^{}\) to achieve task-aware model learning. We take a different approach and leave that to future works.

### The bi-level framework

We now introduce our proposed meta-weighting mechanism and bi-level framework to subtly fuse the RSSM and our task-aware loss function into a hierarchical optimization paradigm.

Our intention is to attain task awareness in model learning, while still maintaining an MLE foundation to preserve abundant semantic information. Inspired by advances in meta-learning (Nichol et al., 2018; Shu et al., 2019; Jiang et al., 2022), we propose to assign each training sample with differentimportance weight in a task-aware fashion. To achieve this, we introduce an additional meta weighter network \(N_{}\) (with parameters \(\)) that outputs a meta weight \(w_{t}\) for each \(_{t}(o_{ t},r_{ t},a_{<t};)\) in \(^{}\), based on the current states and values (Figure 1(a))

\[w_{t}=N_{}h_{t},s_{t}^{},s_{t}^{},V(s_{t}^{ }),V(s_{t}^{})\] (5)

With these meta weights, the model objective changes from \(^{}\) to a weighted sum of \(_{t}\)s. The meta weights as well as the meta weighter are trained to minimize our task-aware model loss \(^{}\). This adds another level of optimization on top of the default MLE model learning, forming a bi-level framework with two hierarchical optimizations of orthogonal objectives, which we name Task-aware Environment Modeling Pipeline with bi-level Optimization (TEMPO)

\[&_{}^{}(;V, ^{*})=\,^{}(;V,^{*})=_{t=1}^{T} Vs_{t}^{}(o_{ t},a_{<t};^{*})-V s_{t}^{}(o_{<t},a_{<t};^{*})^{2}\\ &\ ^{*}=\,*{arg\,max}_{} ^{}(;w_{1:T},)=*{arg\,max}_{ }_{t=1}^{T}w_{t}_{t}(o_{ t},r_{ t},a_{<t}; )\] (6)

Notice that \(^{*}\) depend on meta weights \(w_{1:T}\) and parameter \(\) through the gradients of \(^{}\). Under this bi-level model learning framework, the meta weighter learns to assign importance weight to training samples regarding their impact on minimizing the task-aware model loss; and the model learns the environment dynamics through reconstruction while focusing on important training samples.

In practice, as the task behavior of the agent gradually improves, the state values naturally rise, which will likely lead to larger differences between the posterior and prior state values. If we naively optimize the meta weighter to minimize \(^{}\) for the upper-level optimization, the meta weighter may learn to output minimal weight for all training samples to stop the values from rising. Since we are essentially interested in the relative importance of the samples, we perform a normalization with moving mean and variance on the state values \(\{V(s_{t}^{}),V(s_{t}^{})\}_{t=1}^{T}\) before computing \(^{}\).

To implement our TEMPO framework, we alternate between upper-level and lower-level optimization. That is, we first perform the lower-level optimization on the model parameters \(\) through a single gradient update (Figure 1(b))

\[^{}=+_{}^{}( ;w_{1:T},)\] (7)

where \(\) denotes the learning rate for the model. Then, we perform the upper-level optimization on the meta weighter parameters \(\), for which we can simply calculate \(^{}\) using \(^{}\) and take the gradient w.r.t \(\)\[_{}^{}(;V,^{})= ^{}(;V,^{})}{ ^{}}}{}\] (8) \[= ^{}(;V,^{})} {^{}}^{}(;w_{ 1:T},)}{\,}\]

However, the problem with this direct approach is that we need to infer the states (i.e. \(h_{1:T},s^{}_{1:T},s^{}_{1:T}\)) twice, one time for each objective, for they come from different model parameters (i.e. \(\) and \(^{}\)). Therefore, to cut down the computational cost, we propose to empirically approximate \(^{}\) with \(\), since they are only one update away. This results in an elegant update formula for \(\) (Figure 1(c))

\[^{}= \,-_{}^{}( ;V,^{})\] (9) \[ \,-^{}( ;V,)}{}^{}( ;w_{1:T},)}{\,}\]

It is evident that, with Equation 9, the upper-level objective and lower-level objective are both calculated with the same set of states, which allows us to infer the states only once during an epoch of bi-level optimization. See Algorithm 1 for a pseudocode of our bi-level model-learning paradigm and Figure 1 for a graphical demonstration.

## 3 Experiments

### Experimental setup

We implement our TEMPO framework on top of DreamerV2 using the official implementation of Hafner et al. (2020). Specifically, we build the meta weight to be a 5-layer dense network (MLP) with concatenated states as input (see Equation 5) and scalar meta weight as output. All hidden dimensions of the network are set to 400. Batch normalization (Ioffe and Szegedy, 2015) and ELU (Clevert et al., 2015) activation are performed after each hidden layer. A sigmoid function and an additive bias are applied to the meta weights after the output layer, so that the weights center around 1, i.e. weight \(=0.5()+0.75\). An Adam optimizer (Kingma and Ba, 2014) with a learning rate of \(1-4\) is used for updating the meta weight. We leave the RSSM, the actor-critic agent, as well as all related settings from DreamerV2 untouched and fix the above configuration across all following experiments.

Figure 2: Evaluation of TEMPO on continuous control tasks from DeepMind Control Suite. (a), a graphical demonstration of the 9 environments used in the evaluation, from left to right: Acrobot, Cartpole, Cheetah, Cup, Finger, Hopper, Pendulum, Quadruped, and Walker. (b), the evaluation curves of 3 seeds and their average significance, with lines showing the mean scores, and shaded areas showing the standard deviations.

We use 9 continuous control tasks from the DeepMind Control (DMC) Suite (Tassa et al., 2018) (i.e. Acrobot Swingup, Cartpole Swingup, Cheetah Run, Cup Catch, Finger Spin, Hopper Stand, Pendulum Swingup, Quadruped Walk, and Walker Walk), and 6 discrete control tasks from Atari video games (Bellemare et al., 2013) (i.e. Bank Heist, Crazy Climber, Freeway, Hero, Kangaroo, and Pong), to evaluate the performance of TEMPO.

We pit TEMPO against the original DreamerV2 and a number of state-of-the-art model-free RL agents including D4PG (Barth-Maron et al., 2018), A3C (Mnih et al., 2016), IQN (Dabney et al., 2018), and Rainbow (Hessel et al., 2018). For the model-free agents, we use the results reported by Tassa et al. (2018) and Castro et al. (2018). For DreamerV2, we use the default hyperparameters provided by Hafner et al. (2020) in all experiments. More importantly, we fix these hyperparameters in the corresponding parts of our TEMPO framework (i.e. the RSSM and the actor-critic agent) for a fair comparison.

We tuned TEMPO on DMC Walker Walk and Atari Pong such that meta weighter converges gradually alongside the RSSM. This is to ensure the weighter doesn't reach early convergence on a meaningless state representation. Empirically, TEMPO trains stably, and both objectives reach convergence after sufficient training (Figure 4). Refer to the Appendix for further detailed settings.

Following Henderson et al. (2018), we perform significance testing to examine TEMPO's superiority. In particular, we perform the one-tailed Welch's \(t\)-test (Welch, 1947) on TEMPO's and DreamerV2's results for every evaluation during the training process, and report the average \(t\) and \(p\) over all evaluations.

Furthermore, to verify the effectiveness and novelty of the proposed meta-weighting mechanism, we compare TEMPO's meta-weighting mechanism with naively weighting the training samples using the task-aware model losses. We also perform ablation studies on different inputs of the meta weighter network and different hyperparameters.

Figure 4: Training curve TEMPO on Cartpole Swingup from DeepMind Control Suite. The lines show the means and shaded areas show the standard deviations of 3 seeds.

Figure 3: Evaluation of TEMPO on discrete control tasks from Atari video games. (a), a graphical demonstration of the 6 Atari games used in the evaluation, from left to right: Bank Heist, Crazy Climber, Freeway, Hero, Kangaroo, and Pong. (b), the evaluation curves of 3 seeds and their average significance, with lines showing the mean scores, and shaded areas showing the standard deviations.

### Continuous control

A subset of the DeepMind Control Suite with 9 tasks is used to evaluate TEMPO's performance in continuous control situations. The tasks are each from a different environment, as illustrated in Figure 2(a). Environmental observations are RGB images of shape \(64 64 3\); actions range from 1 to 12 dimensions; each episode starts with a randomized initial state and lasts for 1000 steps. We follow the protocol described by Hafner et al. (2020) and set the stochastic latent variable (i.e. \(s_{t}\)) of DreamerV2 and TEMPO to be a 32-dim continuous vector following Gaussian distribution.

We evaluate the TEMPO agent's environment return every \(14\) environmental steps, and compare the results with those of DreamerV2 and the final performance of D4PG and A3C after \(18\) steps. The results of 3 tasks are illustrated in Figure 2(b), where TEMPO achieved state-of-the-art performance in all tasks. Specifically, TEMPO reached or exceeded the performance of D4PG and A3C within only \(16\) environmental steps, far less than \(18\) steps. Moreover, TEMPO exceeded DreamerV2 in terms of asymptotic performance, training stability, and convergence speed, which demonstrates the advantage of TEMPO's task-aware model learning paradigm under continuous task settings. Refer to the Appendix for full results on all 9 tasks.

### Discrete control

We then evaluate TEMPO's performance on discrete control tasks with 6 Atari video games, as illustrated in Figure 3(a). The actions range from 3 to 18 dimensions. We render the environmental observations as gray-scale images of shape \(64 64 1\), and set the stochastic latent variable (i.e. \(s_{t}\)) to be a 32-column discrete matrix with each column being a 32-dim one-hot vector from a categorical distribution, following Hafner et al. (2020).

Due to limited computational resources, we train TEMPO and DreamerV2 for \(17\) environmental steps. We evaluate the TEMPO agent's environment return every \(15\) steps, and compare the results with those of DreamerV2 and the final performance of IQN and Rainbow after \(17\) steps. The results of 3 games are shown in Figure 3(b), where TEMPO again achieved top performance and exceeded the original DreamerV2. This demonstrates TEMPO's superiority and robustness under both continuous and discrete task settings. Refer to the Appendix for full results on all 6 games.

Figure 5: Ablation studies. The lines show the mean scores and shaded areas show the standard deviations of 3 seeds. (a), comparison with naive weighting strategy. (b), Comparison of different meta weighter inputs. Capital letters denote different parts of input: deterministic states (D), stochastic states (S), and state values (V).

### Comparison with naive weighting strategy

In TEMPO, the meta weighter, a deep neural network, generates the meta weights based on state information. Given that the meta weighter is trained to minimize our task-aware model loss, one may naturally wonder: is it viable to naively use the losses themselves as sample weights, assigning greater weights to samples with larger value disparities? To compare this straightforward weighting approach with our TEMPO framework, we initially normalize the task-aware model losses of each time step in a training trajectory using a moving mean and variance. Subsequently, we add 1 to these normalized losses to obtain the final naive sample weights, roughly centering the weights around 1. As illustrated in Figure 5(a), the naive weighting strategy exhibits strong instability during training, and fails to gain comparable results as TEMPO.

### Comparison of different meta weighter inputs

In our initial design, the meta weighter generates sample weights by considering three key components of information: deterministic state \(h_{t}\), stochastic states \(s_{t}^{}\) and \(s_{t}^{}\), and state values \(V(s_{t}^{})\) and \(V(s_{t}^{})\) (see Equation 5). To justify our design, we conduct an ablation study on the inputs to assess the impact of these inputs on the agent's performance. We sequentially remove the state values and deterministic state from the input, and compare the outcomes with our original TEMPO configuration. As illustrated in Figure 5(b), removing the two parts of input causes the agent to converge slower, and introduces additional instability and variance in the agent's performance. Notably, removing the deterministic states severely affects the outcome performance. This demonstrates the significance of the information encapsulated within these deterministic states.

### Comparison of different hyperparameters

To examine the robustness of TEMPO's meta-weighting mechanism, we perform simple ablation studies on a number of important hyperparameters of the meta weighter, including hidden dimension, the number of dense layers, the choice of nonlinear activation function, and the normalization method. The results, depicted in Figure 6, clearly demonstrate TEMPO's robustness, especially in the face of reduced hidden dimensions and altered activation functions. While modification to the normalization method and decrease in the number of layers does affect performance, TEMPO can consistently maintain an advantage over DreamerV2 during the initial stages of training, ensuring quicker convergence. Overall, TEMPO proves to be highly adaptable to various hyperparameters.

Figure 6: Comparison of different hyperparameters of the meta weighter. The lines show the mean scores and shaded areas show the standard deviations of 3 seeds.

Related work

Model learning in partially observable MBRL problems has drawn large interest in related works. A straightforward way for model learning is to learn the dynamics of an environment by fitting the observations from before and after environment transitions, either deterministically or stochastically (Oh et al., 2015; Chiappa et al., 2017; Kaiser et al., 2019). We call this type of model observation-space models. Simulated trajectories can be obtained by unrolling the model in the original observation space, which facilitates behavior learning in a Dyna fashion (Sutton, 1991). For example, SimPLe (Kaiser et al., 2019) proposed a video prediction network for pixel-level prediction on environmental observations, where a PPO (Schulman et al., 2017) agent is trained using the predictions to achieve behavior learning for Atari games.

However, modeling high-dimensional image observations is unavoidably computationally intensive. Latent state-space models overcome this limitation by constructing a compact latent space to characterize the environmental states behind each observation, which minimizes the memory footprint during model unrolling (Walstrom et al., 2015; Wuhlstrom et al., 2015; Buesing et al., 2018; Gelada et al., 2019; Hafner et al., 2019, 2020; Ozair et al., 2021). Based on the RSSM, Dreamer (Hafner et al., 2019) and DreamerV2 (Hafner et al., 2020) enable behavior learning by constructing an actor-critic agent that maximizes state values by propagating their analytic gradients back through the dynamics learned by the model. Ozair et al. (2021) proposed a stochastic state-space model based on VQ-VAE (Van Den Oord et al., 2017), which is then combined with Monte Carlo tree search (MCTS (Coulom, 2007)) for task behavior.

Observation-space models and state-space models can both be categorized as MLE-based models that learn environment dynamics through reconstruction. With these models, model errors are likely to compound during unrolling. Value-equivalent models intend to minimize such errors and improve task awareness by focusing on end-to-end value prediction (Tamar et al., 2016; Silver et al., 2017; Farquhar et al., 2017; Farahmand, 2018; Hubert et al., 2021). Specifically, methods like the Predictron (Silver et al., 2017), Value Prediction Network (VPN (Oh et al., 2017)), MuZero (Schrittwieser et al., 2020), and Stochastic MuZero (Antonoglou et al., 2021) aims to form a state representation that can help a parametric value function make accurate value predictions. Other methods like VAML (Farahmand et al., 2017) and VaGraM (Voelcker et al., 2022) design task/value-aware model losses that minimize value prediction disparities from a given value function.

Although we base TEMPO on the RSSM and VAML in this work, TEMPO is orthogonal to the design of MLE-based world models (lower level) and task-aware model loss (upper level). On one hand, one can find a suitable task-aware model loss to evaluate a MLE-based model (e.g. VAML for observation-space models and our V-VAML for state-space models), and build a bi-level optimization to train the model in a TEMPO fashion. On the other hand, one can replace our V-VAML loss with another task-aware model loss (e.g. VaGraM) by making modifications similar to V-VAML.

## 5 Limitation and discussion

We present TEMPO, a task-aware framework for learning world models. The framework is driven by a meta-weighting mechanism and a novel task-aware loss function under a bi-level optimization. TEMPO achieves state-of-the-art performance on a variety of continuous and discrete control tasks, while demonstrating better asymptotic performance, training stability, and convergence speed. Ablation studies are performed to justify our method. The main limitation of our work is that TEMPO is more computationally demanding than previous model learning methods, as it involves two loops of optimization. In our experiments, we observed that TEMPO trains around 40% slower and requires 80% more RAM than DreamerV2. Nonetheless, TEMPO contributes significant insights and opens up an exciting new avenue for environment modeling. We aspire that TEMPO will serve as a catalyst for novel ideas and approaches for model-based RL in the future. A sample code of TEMPO is available at https://github.com/deng-ai-lab/TEMPO.

## 6 Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 62325101 and Grant 62031001.