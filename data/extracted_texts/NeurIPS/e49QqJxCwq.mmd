# PLIP: Language-Image Pre-training for Person Representation Learning

Jialong Zuo \({}^{1}\) &Jiahao Hong \({}^{1}\) &Feng Zhang \({}^{1}\) &Changqian Yu \({}^{2}\)

**Hanyu Zhou \({}^{1}\)** &Changxin Gao \({}^{1}\)1 &Nong Sang \({}^{1}\) &Jingdong Wang \({}^{3}\)

\({}^{1}\) National Key Laboratory of Multispectral Information Intelligent Processing Technology,

School of Artificial Intelligence and Automation, Huazhong University of Science and Technology,

\({}^{2}\) Skywork AI, \({}^{3}\) Department of Computer Vision, Baidu Inc.

{jlongzuo, cgao}@hust.edu.cn, {wangjingdong}@baidu.com

###### Abstract

Language-image pre-training is an effective technique for learning powerful representations in general domains. However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance. The reason is that they neglect critical person-related characteristics, i.e., fine-grained attributes and identities. To address this issue, we propose a novel language-image pre-training framework for person representation learning, termed PLIP. Specifically, we elaborately design three pretext tasks: 1) Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases. 2) Image-guided Attributes Prediction, aims to mine fine-grained attribute information of the person body in the image; and 3) Identity-based Vision-Language Contrast, aims to correlate the cross-modal representations at the identity level rather than the instance level. Moreover, to implement our pre-train framework, we construct a large-scale person dataset with image-text pairs named SYNTH-PEDES by automatically generating textual annotations. We pre-train PLIP on SYNTH-PEDES and evaluate our models by spanning downstream person-centric tasks. PLIP not only significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings.

## 1 Introduction

Person-centric tasks, such as image/text-based person re-identification, person attribute recognition, person search and human parsing, are becoming increasingly influential in widespread applications, such as security monitor, smart city, virtual reality and scene understanding. Benefiting from the advances in designing task-specific methods , these tasks have achieved significant progress for the past few years.

However, it has become evident through recent advancements in research that the mere development of sophisticated models based on specific tasks has already encountered a performance bottleneck. At the same time, some works  on general representation learning have shown great potential to further improve model performance. Due to the above reasons, some researchers  have attempted to learn generic person representations by utilizing rich person images. However, their pre-training method based on sparse visual information falls short in terms of both training performance and efficiency.

Meanwhile, some works  have demonstrated that introducing the language modality helps to learn better representations in general domains, for the language naturally enjoys higher information density. However, when it comes to person representation learning, these general language-image pre-training methods like CLIP  are often unsatisfactory in performance. We believe the reason is that they neglect some critical person-related characteristics, i.e., fine-grained attributes and identities.

On the one hand, these general language-image pre-training methods typically implement the global alignment between cross-modality representations and lack explicit consideration for fine-grained information. However, in person domain, the fine-grained information, such as the person attributes, plays a key role in distinguishing a specific person. Neglecting fine-grained attributes will easily lead to difficulty in learning discriminative person representations. On the other hand, they are based on instance level and only incorporate the concept of image-text pairs. They simply treat each image-text pair in the same person identity as different pairs, and assume that images and texts not in the same pair do not have a corresponding relationship. However, in person domain, there exists a notable concordance between the images and texts within the same person identity. If only conducting the optimization at the instance level, it will lead to instability to learn more meaningful representations.

To address these limitations mentioned above, we deeply consider the characteristics of persons and attempt to introduce the language modality into person representation learning. We propose a well-motivated language-image pre-training framework for learning generic and discriminative person representations, termed PLIP, to help the downstream person-centric tasks. Also, to implement the pre-training, we construct a large-scale person dataset with rich image-text pairs. The whole framework is illustrated in Fig. 1.

Specifically, to explicitly learn fine-grained and meaningful cross-modal associations, we design three pretext tasks in PLIP:

(1)_Text-guided Image Colorization_, given a complete textual description, is designed to restore the color information of a grayscale transformed person image. This task establishes the correspondence between the person-related image regions and the fine-grained color-part textual phrases, which robustly helps the model to learn the semantic concept of person body parts.

(2) _Image-guided Attributes Prediction_, by exploiting the paired colorful images, is designed to predict the masked attribute phrases in textual descriptions. This task primarily focuses on predicting attributes, rather than predicting any random masked words as in the general domains. Through this multi-modal masked language modeling, it helps the model to understand the key areas and fine-grained attribute information of the person body in the image, which is crucial to identifying a person.

(3) _Identity-based Vision-language Contrast_ is designed to associate representations between vision and language at the identity level rather than the instance level in general domains. This means it is optimized by narrowing the distance between any images and texts in the same person identity and widening the distance between those not in the same person identity. By taking identity into consideration, this task achieves more robust and meaningful association between different modalities.

As is well known, the quality and quantity of training data is essential for learning rich representations. However, there exists huge domain gap between the large-scale image-text pairs used in general domains and the specific person data. Also, the scale of the existing person datasets  with manual textual descriptions is limited due to expensive hand-labeled annotations. Therefore, we construct a new large-scale person dataset with image-text pairs named SYNTH-PEDES based on the LUPerson-NL and LPW datasets . The text annotations are automatically synthetized by our proposed person image captioner named Stylish Pedestrian Attributes-union Captioning (SPAC). The dataset contains 312,321 identities, 4,791,711 images and 12,138,157 textual descriptions. At the same time, extensive experiments have been conducted to verify the competitive quality of our synthetic dataset compared to manually annotated datasets , which guarantees the superior performance of the learned person representations.

Figure 1: Illumination of our framework. Based on the constructed dataset, we pre-train a language-image model by three pretext tasks and transfer the model to some downstream person-centric tasks.

We utilize PLIP to pre-train our models on the SYNTH-PEDES dataset, and then evaluate the model performance on spanning downstream person-centric tasks. Extensive experiments show that our pre-trained models have learned generic person representations, pushing many state-of-the-art methods to a higher level on a wide range of person-centric tasks without bells and whistles. For example, for unsupervised person Re-ID, by applying our pre-trained ResNet50 model on PPLR , we improve the mAP metric on Market1501  and MSMT17  by 5.1% and 14.7%, respectively. The key contributions of this paper can be summarized as follows:

(1) We propose a novel language-image pre-training framework with three pretext tasks, termed PLIP, which deeply takes person-related characteristics into consideration. It can facilitate fine-grained cross-modal association and learning generic person representations explicitly.

(2) To implement the pre-training, we construct a large-scale person dataset with generated text annotations, called SYNTH-PEDES. The dataset provides rich image-text pre-training data for this community.

(3) We pre-train PLIP on SYNTH-PEDES and the learned representations perform remarkable ability in various downstream person-centric tasks. It is demonstrated as generic to bring significant improvements to various baseline methods.

## 2 PLIP: Representation Learning Framework

This section presents our proposed language-image based person representation learning framework PLIP via three pretext tasks, _i.e.,_ text-guided image colorization (TIC), image-guided attributes prediction (IAP), and identity-based vision-language contrast (IVLC). As illustrated in Fig. 2, the whole architecture is a dual branch encoder structure, and generic person representations can be learned through joint training of these three pretext tasks.

### Text-guided Image Colorization

The TIC task is designed to restore the original color information of grayscale transformed images by exploiting the complete textual descriptions. Such cross-modal colorization promotes the construction of image-text association. The reason is that the attribute phrases in the descriptions generally contain fine-grained person-centric information especially color information and this colorization process naturally enables the model to understand the key components in textual descriptions and achieve a relationship construction between the textual phrases and visual regions. The overall task can be converted into a pixel-wise regression problem.

As illustrated in Fig. 2, for a pair of a gray image and a complete textual description \(\{_{gray},_{complete}\}\), in the encoding stage, the input gray image \(_{gray}\) is firstly fed into a hierarchical network, which is as the visual encoder. Secondly, the hierarchical features are up-sampled to the same scale and concatenated to produce the feature \(_{gray}\). Then, we feed the complete textual description \(_{complete}\) to the textual encoder  and adopts the average of the hidden-state outputs as the textual global embedding \(_{global}\).

In the decoding stage, the visual feature \(_{gray}\) and textual global embedding \(_{global}\) should be fused for colorization. Specifically, we adopt the multi-modal SE-blocks  as the cross-modal feature fusing module, so that the textual global embedding can play a role in the visual feature channels. In the block, the visual feature \(_{visual}\) is compressed into a feature vector \(_{f}\) through max pooling operation. Then we concatenate the visual feature vector and the textual global embedding \(_{global}\), and feed the concatenated vector into several fc layers and a softmax layer to generate an attention vector \(_{f}\). Finally, we utilize \(_{f}\) on the visual feature \(_{visual}\) to generate a multi-modal feature \(_{m}\). The decode is also made up of several de-convolution layers, which are employed to restore the feature dimensions. Finally, we generate a multimodal feature map with same dimensions as the input gray image \(_{gray}\) and it can be utilized to predict the target color image \(_{color}\).

We denote \(_{tic}\) as the parameters of the trainable regression model mentioned above. It maps the textual global embedding \(_{global}\) and the gray image extracted feature \(_{gray}\) to the output recovered color image \(_{color}\) as a target. TIC is supervised by:

\[_{tic}=^{N}(_{color},_{tic }(_{global},_{gray})),\] (1)

where \(\) can be any differentiable distance function such as Euclidean distance we adopt and \(N\) represents the total number of samples within a batch.

As displayed in Fig. 5 of the appendix, altering the color word in textual description significantly affects the colorization of image. However, our model may not fully understand the semantics of more detailed image regions. As shown in the last row, our model fails to distinguish between the blue clothing region and the red shoulder strap region (marked with yellow boxes), instead blending the two into a unified coloration. This is due to the fact that the level of detail in manually annotated datasets is still not sufficient, resulting in the model being unable to theoretically learn representations with higher levels of detail and greater discrimination capabilities. However, it is undeniable that our model has a preliminary understanding of the meaning of attributes and colors, and can associate them with related image regions, rather than simple memorization. This ability to distinguish between different parts of the person body guarantees the superior performance on the subsequent person-centric tasks.

### Image-guided Attributes Prediction

The IAP task requires utilizing original color images to predict the masked attribute phrases in textual descriptions. Unlike previous methods  that randomly mask any words or only color words in a description, our method focuses on masking attribute phrases. For each sentence, the attribute phrases are partially masked to create a masked textual description. In this multi-modal masked language modeling (MMLM) way, the correlation between images and texts can be bridged more in depth and more discriminative representations can be learned. The reason is that the prediction process enables the model to further understand the person-centric regions in images and extract the key semantic information. Meanwhile, by exploiting the visual representations, the MMLM enhances the perception of context and strengthens the interaction between vision and language modality.

In the encoding stage, as illustrated in Fig. 2, for a pair of a color image \(_{color}\) and a masked textual description \(_{masked}\) with masked words \(_{m}=\{_{m_{1}},,_{m_{M}}\}\) (\(M\) is the number of masked words), we feed them to respective encoders to extract the visual global embedding \(_{global}\) and textual hidden-state outputs \(_{t}=\{_{t_{1}},,_{t_{t}}\}\) (\(L\) is the length of the tokens in the masked textual description). The visual global embedding \(_{global}\) is obtained from a pooling operation on the last stage feature output of the visual encoder.

In the decoding stage, to perform the IAP task, we specially design a simple but effective multi-modal fusion module that mainly consists of self-attention blocks and a prediction head. Firstly, we adopt the element-wise summation of \(_{t}\) and \(_{global}\) as the preliminary fused embeddings. Then, the embeddings will be served as query(\(\)), key(\(\)) and value(\(\)) simultaneously. Finally, we obtain the multi-modal fused embeddings for each masked position by:

\[\{_{m_{i}}\}_{i=1}^{M}=Blocks(,,),\] (2)

where \(M\) is the total number of masked tokens in the masked textual description, and \(Blocks\) are the self-attention blocks.

For each embedding representing the masked word, we use a prediction head to realize the corresponding probability prediction. The IAP can be optimized by minimizing the negative log-likelihood:

Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.

\[_{iap}=-^{N}^{M}_{k=1} P(_{m_{k}}| _{m_{k}}),\] (3)

where \(M\) is the total number of masked words in a masked textual description, \(N\) denotes the number of samples within a batch and \(P\) denotes the probability distribution mapping.

### Identity-based Vision-language Contrast

In order to further strengthen the correlation between vision and language modalities, we must optimize the model to learn a unified cross-modal feature space. A preliminary approach based on contrastive learning  is to shorten the distance of the representations in image-text pairs and simultaneously amplify the distance of representations not in the same pair. This approach considers image-text pairs as instances and ignores the identity, where different image-text pairs of the same identity are treated as negative samples. However, in person-centric field, the identity plays a crucial role in distinguishing different people. Therefore, unlike the usual practices in general domain, we must take the identity into consideration.

For a group of a color image, a complete textual description and a corresponding identity \(\{_{color},_{complete},Id\}\). The color image is firstly fed into the visual encoder. Then the last-stage feature is pooled to get the visual global embedding \(_{global}\). The description is directly fed into the textual encoder and the average pooling of hidden-states will be served as the textual global embedding \(_{global}\).

Then, given a batch of \(N\) image-text pairs, for each visual global embedding \(^{i}_{global}\), we construct a set of visual-textual embedding pairs as \(\{(^{i}_{global},^{j}_{global}),y_{i,j}\}^{N}_{j=1}\), where \(y_{i,j}=1\) means that the pair is matched and from the same identity, and \(y_{i,j}=0\) indicates the unmatched pair. Let \(sim(,)=^{}/\|\|\| \|\) denotes the matching probability of \(\) and \(\). Then, the probability of matching pairs can be calculated with the following function:

\[p_{i,j}=^{i}_{global},^{j}_{global }))}{^{N}_{k=1}(sim(^{i}_{global}, ^{k}_{global}))}.\] (4)

Then, the IVLC loss from vision to language in a batch can be computed by:

\[_{v2l}=^{N}_{i=1}^{N}_{j=1}p_{i,j}( }{q_{i,j}+}),\] (5)

where \(q_{i,j}=y_{i,j}/^{N}_{k=1}y_{i,k}\) is the true matching probability and \(\) is a small number to avoid numerical problems.

Similarly, the IVLC loss from language to vision \(_{l2v}\) can be be computed by exchange \(V_{global}\) and \(T_{global}\) in above equations. Finally, the IVLC task can be optimized by:

\[_{IVLC}=_{v2l}+_{l2v},\] (6)

Indeed, the IVLC loss can be any cross-modal contrastive loss that takes identity into consideration such as the Cross-Modal Projection Matching (CMPM) loss  we adopt, which promotes the representation association between multiple modalities by incorporating the cross-projection into KL divergence.

Then, to supervise the model to learn discriminative and highly generic person representations, the overall multi-task loss \(\) is computed as:

\[=_{ivlc}+_{1}_{tic}+_{2} _{iap},\] (7)

where \(_{1},_{2}^{+}\) are hyper-parameters to control the importance of each pretext task.

## 3 Synth-Pedes: A Large-scale Image-text Person Dataset

We build the SYNTH-PEDES dataset to pre-train our PLIP models at a large-scale. In this section, we show the general process of constructing our SYNTH-PEDES dataset, which can be described as three steps. The complete construction details can be found in Sec. A.5 of the appendix.

Firstly, we collect and process two large-scale person datasets to form the image dataset. The first is LUPerson-NL . It is a new variant of LUPerson  on top of raw videos from LUPerson and assign the noisy labels to each person image with automatically generated tracklet. It consists of \(10M\) images with about \(430K\) identities collected from \(21K\) scenes. The second is LPW . It consists of 2,731 different persons and 592,438 images collected from three different crowded scenes.

Secondly, we propose an image captioner named SPAC to generate satisfactory textual descriptions for the images. Given an input person image, there is no specific work targeting at generating captions that detailedly describe the person's appearance. To this end, we propose a simple but effective method for person image captioning. It can generate attribute-annotations and stylish textual-descriptions, which simulate the diverse perspectives that different annotators may have on the same person image. The specific technics for SPAC can be found in Sec. A.5.1 of the appendix.

Thirdly, we adopt some post-processing approaches to eliminate the noises and improve the dataset quality. We propose Seed Filter Strategy to filter the noises in LUPerson-NL, which includes three processes of Filter-out, Reassignment and Merger. Meanwhile, we propose Data Distribution Strategy to ensure the quality of generated attributes, the consistency of gender annotation, and identity distribution balance. The specific details for the strategies can be found in Sec. A.7 of the appendix.

Thanks to the outstanding generating ability of our proposed SPAC, the SYNTH-PEDES dataset is full of high-quality textual descriptions in a variety of styles, which can be utilized to train the representation learning model. Compared with existing person datasets, SYNTH-PEDES has the following advantages:

**Diversified.** Our dataset contains a wide range of variations in the textual descriptions. Unlike the previous person datasets with only one or two image-text pairs, most images of our dataset are annotated with three textual descriptions.

**High-quality.** As some typical qualitative examples can be seen in Fig. 3, the generated annotations achieve an accurate and detailed description of the person appearance. The further experiments conducted on the dataset quality evaluation can be found in Sec. A.9.2 of the appendix. Researchers can use this dataset with confidence to conduct relevant studies.

**Large-scale.** In Tab. 11 of the appendix, we have compared the properties of SYNTH-PEDES with other popular person datasets. As we can see, SYNTH-PEDES is the largest real person dataset with high-quality image-text pairs by far, which contains 312,321 identities, 4,791,711 images, and 12,138,157 textual descriptions.

## 4 Experiments

**Implementation**. During the training of PLIP, we adopt four types of backbone as the visual encoder, _i.e.,_ ResNet50, ResNet101, ResNet152 and Swin Transformer Base. The pre-trained BERT  is utilized as the textual encoder with the last 5 layers unfrozen. We train our model on \(4\) 3090 GPUs for 70 epochs. For each person-centric downstream task, we reproduce a range of state-of-the art methods as the baselines. If not specially stated, we perform the experiments by just replacing the backbone in each baseline to the our pre-trained models. We perform in-depth experiments on eleven datasets for five downstream person-centric tasks. Meanwhile, we perform thorough ablation studies and analyses in Sec. A.9 of the appendix. More details and experiments can be found in the appendix.

Figure 3: Visualization of some examples in our SYNTH-PEDES dataset.

Comparison With Other Pre-trained models, with other SoTA pre-trained models on downstream tasks. The table is divided into two parts: the upper part covers some general-domain pre-trained models and the lower part focuses on some person-domain pre-trained models. The baseline downstream methods for the five tasks are CMPM/C , ABDNet , Rethinking , SeqNet  and SCHP , respectively. Due to the inability of certain non-hierarchical models to be applied to some downstream methods, their performance results are indicated as "\({}^{-}\)".

Comparison With Other Pre-trained models, with the latter typically exhibiting better performance in person-centric tasks. The performance metrics for these tasks are R@1/R@10, mAP/R@1, mAP/F1, mAP/R@1 and mIoU, respectively. As shown in Tab. 1, our PLIP models consistently demonstrate a significant performance advantage over other pre-trained models in a fair comparison with roughly equal parameters.

### Evaluation on Text-based Person Re-ID

**Transfer capability.** To evaluate the transfer capability of our pre-trained models, we conduct three different experiments with ResNet50 as the visual encoder. Firstly, we directly evaluate the model's zero-shot performance without any extra fine-tuning. Secondly, we perform linear probing by adding a trainable linear embedding layer to each modal frozen encoder. Finally, we unfreeze all encoders and perform fine-tuning. We use the simple CMPM loss  as the training target. From Tab. 2 we can see, our pre-trained model is not only competitive with some fully supervised methods  even without fine-tune training, but also greatly exceeds them by a large margin with a simple fine-tune. These results demonstrate that our pre-trained models have excellent transfer capability for this task.

**Domain generalization.** To verify our models' domain generalization capability, we carry out experiments with cross-domain settings. We use the CMPM loss as the training target. As illustrated in Tab. 3, our model achieves improvements by large margins when compared with all

    &  &  \\   & R@1 & R@5 & R@10 & R@10 & R@1 & R@5 & R@10 \\  VITAA  & 55.97 & 75.84 & 83.52 & 50.98 & 68.79 & 75.78 & 54.23 & 72.63 & 79.53 \\ SSA  & 61.37 & 80.15 & 86.73 & 54.23 & 72.63 & 79.53 & & & & \\ LapSCore  & 63.40 & - & 87.80 & - & - & - & - & - & - & - \\  TIPCB  & 63.63 & 82.82 & 89.01 & 54.96 & 74.22 & 81.89 & & & & \\ LGUR  & 64.21 & 81.94 & 87.93 & 57.42 & 74.97 & 81.45 & & & & \\  PLIP+\(\)\(\)\(\)\(5\) & 52.86 & 74.69 & 82.46 & 49.86 & 67.88 & 76.27 & & & & & \\ PLIP+\(\)\(\)\(p\) & 63.63 & 82.85 & 89.36 & 58.51 & 77.83 & 84.26 & 75.87 & & & & \\  PLIP+\(\)\(t\) & **70.11** & **86.60** & **91.89** & **64.58** & **81.30** & **86.82** & & & & \\   

Table 2: The results of our transfer experiments. We show the best score in bold. \(z\)-\(s\): zero-shot setting; \(l\)-\(p\): linear-probing setting; \(f\)-\(t\): fine-tune setting. \(\) stands for the results reproduced with public checkpoints released by the authors.

    &  &  \\   & R@1 & R@5 & R@10 & R@10 & R@1 & R@5 & R@10 \\  Dual Path  & 15.41 & 29.80 & 38.19 & 7.63 & 17.14 & 23.52 \\ SCAN  & 21.27 & 39.26 & 48.83 & 13.63 & 28.61 & 37.05 \\ SSA  & 29.24 & 4.90 & 58.53 & 21.07 & 38.94 & 48.54 \\ LGUR  & 34.25 & 52.58 & 60.85 & 25.44 & 44.48 & 54.39 \\ RaSa  & 50.59 & 67.46 & 74.49 & 50.70 & 72.40 & 79.58 \\ 
**PLIP** & **56.64** & **75.65** & **82.38** & **57.34** & **77.60** & **84.49** \\   

Table 3: Comparison on domain generalization. “C” and “I” denote CUHK-PEDES and ICFG-PEDES, respectively.

    &  &  \\   & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  VITAA  & 55.97 & 75.84 & 83.52 & 50.98 & 68.79 & 75.78 \\ SSA  & 61.37 & 80.15 & 86.73 & 54.23 & 72.63 & 79.53 \\ LapSCore  & 63.40 & - & 87.80 & - & - & - & - \\  TIPCB  & 63.63 & 82.82 & 89.01 & 54.96 & 74.72 & 81.89 \\ LGUR  & 64.21 & 81.94 & 87.93 & 57.42 & 74.97 & 81.45 \\  PLIP+\(\)\(\)\(5\) & 52.86 & 74.69 & 82.46 & 49.86 & 67.88 & 76.27 \\ PLIP+\(\)\(p\) & 63.63 & 82.85 & 89.36 & 58.51 & 77.83 & 84.24 \\  PLIP+\(\)\(t\) & **70.11** & **86.60** & **91.89** & **64.58** & **81.30** & **86.82** \\   

Table 3: Comparison on domain generalization. “C” and “I” denote CUHK-PEDES and ICFG-PEDES, respectively.

other methods. Specifically, our model outperforms LGUR by 22.4% and 31.9% in terms of Rank-1 metric on the \(C\!\!I\) and \(I\!\!C\) settings, respectively. These results demonstrate that our pre-trained models have great capability in domain generalization for this task.

**Improvement over existing methods.** We reproduce three representative baseline methods  and explore the performance difference by changing the encoders with different pre-trained models. From Tab. 4, we can see that equipped with our pre-trained model, all the baseline methods achieve higher and best accuracy on each dataset. It is worth noting that, due to the fact that our PLIP models have already learned an excellent joint visual-textual feature space through large-scale pre-training, achieving outstanding performance is easily attained by fine-tuning with the simple CMPM/C  loss rather than complicated designing such as SSAN  and LGUR .

**Comparison with state-of-the-art methods.** In Tab. 5, we compare our results with some SoTA methods on each dataset. The compared methods can be classified based on whether they rely on the multi-modal pre-trained models. Generally, multi-modal pre-trained models can bring about noticeable performance improvements for this task. It is worth noting that RaSa utilizes a larger image resolution of 384\(\)384, while APTM adopts a two-stage inference method similar to the re-rank mechanism. These approaches lead to RaSa and APTM achieving optimal performance, however, introducing additional training and inference costs. Instead, our PLIP achieves competitive performance without bells and whistles. Specifically, with ResNet50 as backbone, PLIP outperforms LGUR by 6.9% and 7.5% rank-1 on each dataset respectively.

### Evaluation on Image-based Re-ID

**Unsupervised methods achieve significant improvements.** With simply replacing the backbone, our pre-trained models benefit unsupervised image-based person Re-ID methods significantly. We evaluate the improvement brought by different pre-trained ResNet50 models to the SoTA unsupervised methods PPLR  and ISE . As shown in Tab. 6, PLIP outperforms all other pre-trained models by a large margin. Specifically, applied to ISE, PLIP achieves new SoTA performance, outperforming the previous SoTA by 2.9% and 11.4% mAP on Market1501 and MSMT17, respectively.

**Comparison with state-of-the-art methods.** We compare our results with existing SoTA image-based person Re-ID methods on Market1501 and DukeMTMC. Any results gained from post-processing techniques like rerank  are excluded for a fair comparison. As indicated in Tab. 7, by applying our PLIP

   &  &  &  \\   & &  &  &  &  &  &  &  &  \\  Baseline & R1.7 & 92.8 & 97.1 & 98.1 & 31.4 & 61.1 & 73.4 & 77.8 \\ MoCo(21) & 79.6 & 91.6 & 96.6 & 97.9 & 28.7 & 57.5 & 67.6 & 96.6 \\ CLP(1) & 75.0 & 89.0 & 97.6 & 95.7 & 7.6 & 20.3 & 29.9 & 34.9 \\ LP(1) & 57.5 & 78.2 & 85.9 & 85.9 & 22.5 & 48.9 & 62.0 & 66.9 \\ LP(1) & 57.2 & 84.2 & 97.7 & 98.6 & 25.0 & 50.3 & 61.3 & 68.7 \\ LP(1) & **86.6** & **94.5** & **98.0** & **96.7** & **46.1** & **73.4** & **73.2** & **87.0** \\  Baseline & 84.7 & 94.0 & 94.8 & 98.5 & 35.0 & 64.7 & 75.5 & 9.4 \\ MC(21) & 84.9 & 91.5 & 97.6 & 98.7 & 34.1 & 64.5 & 70.0 & 79.0 \\ LP(2) & 79.5 & 90.6 & 98.6 & 80.9 & 14.9 & 35.2 & 46.3 & 51.3 \\ LP(2) & 84.5 & 94.2 & 97.6 & 98.4 & 27.3 & 86.7 & 83.8 & 73.4 \\ LP(3) & **87.4** & 95.0 & 98.2 & 98.5 & 33.9 & 62.5 & 73.2 & 72.3 \\
**PLIP** & **87.6** & **95.3** & **95.2** & **96.8** & **96.4** & **74.9** & **84.3** & **77.5** \\  

Table 6: Comparison on two baseline methods by using different pre-trained models. The best results are shown in bold.

   &  &  &  \\   & Backbone & Pretrain & mAP & R@1 & mAP & R@1 \\  MGN  & R850 & IMG & 87.5 & 95.1 & 79.4 & 89.0 \\ ABDNet  & R850 & IMG & 88.3 & 95.6 & 78.6 & 89.0 \\ GCP  & R850 & IMG & 88.9 & 95.2 & 78.6 & 87.9 \\ ISP  & R850 & IMG & 88.6 & 95.3 & 80.0 & 89.6 \\ TransLR(1) & VIT-B & IMG & 88.2 & 95.0 & 80.6 & 89.6 \\ UPR(1) & 78.850 & LUP & 91.1 & 97.1 & 97.1 & \\ LIP  & R850 & LUP & 91.0 & 96.4 & 82.1 & 91.0 \\ LP(2) & RN50 & LUP+L & 91.9 & 96.6 & 84.3 & 92.0 \\ PASS  & VIT-B & LUP & 93.0 & 96.8 & – & – \\  PLIP & RN50 & SYNTH & 91.4 & 96.8 & 81.7 & 91.1 \\ PLIP & RN101 & SYNTH & 92.0 & 96.9 & 82.3 & 91.8 \\ PLIP & RN152 & SYNTH & 92.6 & 97.1 & 83.1 & 92.1 \\
**PLIP** & Swin-B & SYNTH & **93.2** & **97.3** & **84.4** & **92.2** \\  

Table 7: Comparison with SoTA methods on fully supervised image-based person Re-ID. The best results are shown in bold.

   &  &  &  &  \\   & &  &  &  &  &  &  &  &  &  \\  QRA-RNN  & VGG & LSTM & 19.05 & - & 53.4 & - & - & - & - \\ CMPMPMPC  & R850 & LSTM & 49.7 & - & 79.7 & 43.5 & 65.4 & 74.6 \\ VITAA  & R850 & LSTM & 58.7 & 58.7 & 75.4 & 83.2 & 68.9 & 79.5 & 75.8 \\ NARS  & R850 & BERT & 59.9 & 79.6 & 86.0 & - & - & - \\ SSAN  & R850 & LSTM & 61.37.0 & 80.1 & 86.7 & 54.2 & 72.6 & 79.5 \\ LapNet  & R850 & BERT & 63.0 & 87.8 & - & - & - \\ TCDG  & R850 & BERT & 63.0 & 82.8 & 82.0 & - & - & - \\ TCDG  & R850 & BERT & 64.2 & 81.1 & 87.3 & 57.3 & 75.2 & 74.9 & 81.45 \\ TVD  & TVD  & VIT-B & BERT & 63.9 & 83.1 & 81.1 & 89.1 & 62.0 & 76.0 & 80.2 \\ Cruz  & VIT-B & BERT & 66.9 & 89.9 & 89.1 & 91.6 & 80.6 & 76.5 & 82.2 \\ IRR  & VIT-B & Norm Norm & 73.8 & 89.9 & 93.3 & 93.7 & 64.0 & 80.5 & 85.5 & 85.2 \\ APTM  & Swin-B & BERT & 76.1 & 89.7 & 93.7 & 93.7 & **83.2** & 82.7 & 87.0 \\ Rasa  & VIT-B & BERT & **76.4** & **on ABD-Net , we achieve competitive performance on all datasets. Moreover, we achieve new SoTA performance with Swin-Base as the backbone. This demonstrates that the learned representations benefits this task significantly.

### Evaluation on Person Search

**Improvement over existing methods.** Our pre-trained models bring significant performance gain to existing person search methods. To verify this, we choose two representative methods SeqNet  and GLCNet  as the baselines, and evaluate the improvements brought by different pre-trained models. As shown in Tab. 8, under the ResNet50 setting, our model brings maximum performance improvements to all methods. Specifically, when applying our pre-trained ResNet50 to GLCNet, it achieves 96.3% and 53.7% mAP on the CUHK-SYSU and PRW datasets, respectively.

**Comparison with state-of-the-art methods.** We compare our results with existing SoTA person search methods in Tab. 9. By applying our pre-trained models on GLCNet , we achieve new SoTA performance on each dataset. Specifically, under ResNet50 setting, our method surpasses the previous SoTA GLCNet by 5.9% mAP on PRW dataset. Also, with Swin-Base as the backbone, we achieve the best performance compared with all other methods. These results demonstrate that our pre-training framework shows great potential in learning discriminative person representation for this task.

### Ablation studies and analyses

We perform ablation studies with ResNet-50 as the visual encoder and pre-training on the sub-dataset of SYNTH-PEDES, which has 10,000 identities, 139,564 images and 353,617 textual descriptions. To assess the **effectiveness of pre-text tasks** on the generalizability of our models, we directly evaluate the zero-shot performance of pre-trained models on downstream datasets. As we can see in Tab. 10, each task contributes to the model's zero-shot capability and combining all of the tasks leads to the best performance.

Meanwhile, to validate the **effectiveness of textual diversity** in SYNTH-PEDES, we have studied four different degrees of textual diversity from weak to strong. As shown in Fig. 4, the fourth case with the highest degree of textual diversity has the best performance. More thorough ablation studies and analyses about effectiveness of each component, dataset quality evaluation, pre-training settings and so on can be found in Sec. A.9 of the appendix.

## 5 Conclusion

In this paper, we propose a novel language-image self-supervised person representation learning framework named PLIP, which consists of three well-motivated pretext tasks. Also, we build a large-scale real-scenario image-text person dataset SYNTH-PEDES by auto-captioning procedures. We achieve good generic person representation learning by utilizing PLIP and SYNTH-PEDES. Equipped with our pre-trained models, we push many existing methods to a much higher level without bells and whistles. We hope that our simple and effective framework can inspire researchers to devote further attention to this area.

   &  &  &  \\   & YLC & TIC & LCP & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) \\ 
1 & ✓ & ✓ & 30.2 & 53.3 & 64.0 & 62.3 & 19.0 & 85.7 \\
2 & ✓ & ✓ & 31.4 & 55.2 & 65.9 & 62.9 & 80.7 & 86.2 \\
3 & ✓ & ✓ & 30.5 & 54.4 & 64.3 & 62.7 & 80.5 & 86.1 \\
4 & ✓ & ✓ & 3 & 39.3 & 61.4 & 70.4 \\
5 & ✓ & ✓ & 32.5 & **56.3** & **66.6** & **63.1** & **80.8** & **86.3** \\  

Table 10: Ablation study on the effectiveness of each pretext task, all using default settings.

   &  &  &  \\   & & mAP & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) \\   Backbone} & 94.8 & 95.7 & 98.1 & 98.7 & 47.8 & 87.6 & 94.4 & 95.4 \\  & \(_{2}\)  & 94.0 & 94.8 & 92.8 & 95.8 & 48.3 & 87.3 & 94.6 & 95.8 \\  & \(\)  & 92.8 & 93.4 & 97.8 & 98.2 & 48.0 & 86.7 & 34.5 & 95.9 \\  & \(\) & 93.4 & 94.9 & 98.2 & 96.2 & 47.7 & 82.3 & 94.5 & 95.7 \\  & \(\) & 95.4 & 95.9 & 98.2 & 98.3 & 49.3 & 87.3 & 87.3 & 94.3 & 95.9 \\  & \(\) & **96.0** & 96.7 & **98.7** & **98.7** & **93.5** & **83.8** & **89.9** & **86.3** \\   Baseline \\ \(\) \\  } & Baseline & 95.3 & 96.2 & 98.3 & 98.3 & 47.8 & 87.8 & 94.5 & 95.5 \\  & \(_{2}\) & 95.1 & 95.6 & 98.0 & 98.4 & 84.7 & 87.8 & 94.6 & 95.9 \\  & \(\) & 93.0 & 93.6 & 97.7 & 98.1 & 48.2 & 87.1 & 94.5 & 96.0 \\  & \(\) & 95.5 & 95.8 & 98.3 & 98.9 & 48.1 & 87.7 & 94.5 & 95.8 \\  & \(\) & 96.0 & 96.4 & 98.4 & 99.0 & 49.8 & 87.8 & 94.6 & 96.0 \\  & \(\) & **96.3** & **97.0** & **95.0** & **99.3** & **53.7** & **89.9** & **95.4** & **96.4** \\  

Table 8: Comparison on two baseline methods by using different pre-trained models. We show the best score in bold.

Figure 4: The diversity of textual descriptions matters. PC and GC mean prompt caption and generated caption, respectively.

   &  &  &  \\   & & mAP & R\(\) & R\(\) & mAP & R\(\) \\  OIM  & RN50 & 75.5 & 78.7 & 21.3 & 49.9 \\  & PNSI  & RN50 & 77.9 & 81.2 & 24.2 & 53.1 \\  & CTXGraph  & RN50 & 84.1 & 86.5 & 33.4 & 73.6 \\  & GEETS  & RN50 & 88.9 & 89.1 & 37.1 & 76.7 \\  & BNNet  & RN50 & 90.0 & 90.7 & 45.3 & 81.7 \\  & NAF+  & RN50 & 92.1 & 92.9 & 44.0 & 81.1 \\  & SeqNet  & RN50 & 94.8 & 95.7 & 47.6 & 87.6 \\  & GLCNet  & RN50 & 95.8 & 96.2 & 47.8 & 87.8 \\  & SOLIDE  & Swin-B & 94.9 & 95.5 & **59.7** & 86.8 \\   PLP \\ PLP \\ PLP \\  } & RNS0 & 96.3 & 97.0 & 53.7 & 89.0 \\  & RN101 & 96.5 & 97.2 & 55.1 & 89.4 \\  & PLIP & RN152 & **96.7** & 97.3 & 56.2 & 89.9 \\  & PLIP & Swin-B & 96.6 & **97.8** & **57.8** & **90.3** \\  

Table 10: Ablation study on the effectiveness of each pretext task, all using default settings.

**Acknowledgements.** This work was supported by the National Natural Science Foundation of China No.62176097, and the Hubei Provincial Natural Science Foundation of China No.2022CFA055.