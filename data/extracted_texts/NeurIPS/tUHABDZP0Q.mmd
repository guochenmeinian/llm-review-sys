# Reinforced Cross-Domain Knowledge Distillation on Time Series Data

Qing Xu

Institute for Infocomm Research

A*STAR, Singapore

Nanyang Technological University

Xu_Qing@i2r.a-star.edu.sg

&Min Wu

Institute for Infocomm Research

A*STAR, Singapore

wumin@i2r.a-star.edu.sg

&Xiaoli Li

Institute for Infocomm Research, A*STAR, Singapore

A*STAR Centre for Frontier AI Research, Singapore

xlli@i2r.a-star.edu.sg

&Kezhi Mao

Nanyang Technological University

EKZ Mao@ntu.edu.sg

&Zhenghua Chen

Institute for Infocomm Research, A*STAR, Singapore

A*STAR Centre for Frontier AI Research, Singapore

chen0832@e.ntu.edu.sg

Corresponding author

###### Abstract

Unsupervised domain adaptation methods have demonstrated superior capabilities in handling the domain shift issue which widely exists in various time series tasks. However, their prominent adaptation performances heavily rely on complex model architectures, posing an unprecedented challenge in deploying them on resource-limited devices for real-time monitoring. Existing approaches, which integrates knowledge distillation into domain adaptation frameworks to simultaneously address domain shift and model complexity, often neglect network capacity gap between teacher and student and just coarsely align their outputs over all source and target samples, resulting in poor distillation efficiency. Thus, in this paper, we propose an innovative framework named **R** Reinforced **C**ross-**D**omain **K**nowledge **D**istillation (**RCD-KD**) which can effectively adapt to student's network capability via dynamically selecting suitable target domain samples for knowledge transferring. Particularly, a reinforcement learning-based module with a novel reward function is proposed to learn optimal target sample selection policy based on student's capacity. Meanwhile, a domain discriminator is designed to transfer the domain invariant knowledge. Empirical experimental results and analyses on four public time series datasets demonstrate the effectiveness of our proposed method over other state-of-the-art benchmarks. Our source code is available at https://github.com/xuqing88/Reinforced-Cross-Domain-Knowledge-Distillation-on-Time-Series-Data.

## 1 Introduction

Recent years have witnessed great successes of deep neural networks (DNNs) in various time series applications [1; 2; 3; 4]. Nevertheless, a significant drawback impeding their scalability is the limitedgeneralization capability on unseen data. This challenge arises when there is a distribution disparity between the data used for training and deployment. For instance, a fault diagnosis model trained on certain machines may perform poorly on the data collected from other machines which have different working conditions and configurations. Collecting and annotating data for each machine would be very laborious and costly. To handle this, various unsupervised domain adaptation (UDA) methods have been extensively explored. These methods aim to transfer the domain invariant knowledge from an existing labeled data domain (_i.e._, _source domain_) to an unlabeled domain (_i.e._, _target domain_) either by explicitly minimizing certain pre-defined discrepancy metrics [5; 6] or implicitly learning domain-invariant representations with adversarial manners [7; 8]. However, these UDA methods heavily rely on the complex network architectures and their adaptation performance will significantly degrade with shallower networks [9; 10]. The over-parameterized DNNs will inevitably lead to another practical issue in industries. For many real-world time series tasks, the developed models are often required to be deployed on edge devices with very limited computational resources, such as smartphones and robots, for real-time and long-term monitoring. The intolerable computational and storage burdens make the deployment of those complex DNNs on edge devices become an unprecedented challenge.

Some pioneering efforts have been made to integrate knowledge distillation (KD) techniques into UDA frameworks to transfer the cross-domain knowledge from a cumbersome teacher to a compact student for the reduction of model complexity. However, we empirically find that simply integrating KD with UDA frameworks like existing works will make the compact student suffer from unsatisfying adaptation performance. The rationale behind this lies in the facts that: on the one hand, due to its limited network capacity, the compact student may fail to capture the same fine-grained patterns in data as the cumbersome teacher. Coarsely aligning its feature representations or outputs with the teacher like [11; 12] will impede its learning process and result in sub-optimal performance on the target domain. On the other hand, in the cross-domain scenario, teacher's knowledge on each individual target sample may not be always reliable and instructive due to the lack of label supervision in target domain. Blindly trusting teacher's knowledge for all samples, especially on target domain, will result in negative transfer. Therefore, to achieve good adaptation performance on the target domain, we have to adaptively transfer teacher's knowledge based on student's network capability.

Motivated by above insights, we propose a novel end-to-end framework for cross-domain knowledge distillation to simultaneously address domain shift and model complexity. To be specific, an adversarial discriminator module is designed to align teacher's and student's representations between source and target domains on latent feature space for domain-invariant knowledge transfer. Meanwhile, to adaptively transfer teacher's knowledge on the unlabeled target domain, we formulate the target sample selection problem under a reinforcement learning framework. For a specific target sample, if the student demonstrates the ability to attain the same uncertainty level as the teacher (_i.e._, uncertainty consistency), or can largely mimic teacher's outputs (_i.e._, sample transferability), we deem such a sample suitable for knowledge distillation. Based on that, we design a novel reward function according to student's learning capability. A dueling Double Deep Q-Network (DDQN) is then utilized to learn the optimal target sample selection policy for mitigating the negative effects of unsuitable knowledge from teacher. Our contributions are summarized as follows:

* An end-to-end framework named **R**einforced **C**ross-**D**omain **K**nowledge **D**istillation (**RCD-KD**) is proposed to not only effectively transfer the domain-invariant knowledge but also dynamically distill the adaptive target knowledge based on student's learning capability.
* We develop an innovative reinforcement learning-based module to learn the optimal target sample selection policy for robust knowledge distillation. A novel reward function is designed for assessing student's learning capability in terms of uncertainty consistency and sample transferability to dynamically transfer teacher's target knowledge.
* The extensive experimental results on four real-world time series tasks demonstrate the superior effectiveness of our approach compared to other SOTA methods.

## 2 Related Work

In recent years, there are some pioneering works to tackle both domain shift and model complexity simultaneously. A resource efficient domain adaptation (REDA) framework with multi-exit architectures is proposed in , where the 'easier' samples are inferred via early exits and 'harder' ones are inferred via top exit. Meanwhile, some other researchers leverage the knowledge distillation  to enhance the adaptation performance of the compact student. For instance, a framework named knowledge distillation for unsupervised single target domain adaptation (KD-STDA) is proposed in . Teacher's knowledge is gradually transferred via dynamically adjusting the contributions of UDA and KD loss. Similarly, a multi-level distillation for Domain Adaptation (MLD-DA) strategy is proposed in  to improve the distillation efficiency via a novel cross entropy loss. However, the above two methods transfer the knowledge from both source and target domains. We empirically show that the source domain-specific knowledge might have negative contribution to student's generalization. Besides, MobileDA  and adversarial adaptation with distillation (AAD)  employ the teacher trained on source-only domain to guide student's training, which have already been proved inefficient due to the limited and biased knowledge from teacher model by . Moreover, to achieve more reliable knowledge from teacher, in  a maximum cluster difference metric is proposed to estimate teacher's confidence on certain sample. In , a framework named universal and joint knowledge distillation (UNI-KD) is proposed to measure teacher's confidence on individual sample via the output of a data-domain discriminator. However, due to the compact network architecture of the domain-shared feature extractor from student, the estimated uncertainty is not reliable. In our work, we estimate teacher's knowledge with student's capacity and then utilize it as the reward for the learning process of RL-based target sample selection module. The experimental results demonstrate that our proposed method can better enhance student's performance on target domain. Meanwhile, our work also relates to active learning (AL) field specifically in terms of selecting the most critical instances from unlabeled data. Note that here we only discuss the uncertainty-based sampling strategies in active learning as other query strategies (e.g., instance correlation) are beyond the scope of our paper. In AL, the uncertainty can be measured by three metrics: least confidence [17; 18], sample margin , and sample entropy . Particularly, the entropy metric measures the uncertainty over the whole output prediction distribution [21; 22]. In our method, instead of explicitly utilizing entropy-based uncertainty as AL methods, we leverage the consistency between teacher' and student's entropy-based uncertainty to learn the optimal sample selection policy with dueling DDQN. See **Supplementary** for more comparison results.

## 3 Methodology

### Preliminaries

Following standard UDA setup, we consider data from two domains: a labeled _source_ domain \(^{L}_{src}=\{(x_{s}^{i},y_{s}^{i})\}_{i=1}^{n_{s}}\) and an unlabeled _target_ domain \(^{U}_{tgt}=\{x_{t}^{i}\}_{i=1}^{n_{t}}\) which shares the same label space as source domain but has different data distributions. Here, \(n_{s}\) and \(n_{t}\) are the number of training samples in source and target domains, respectively. A powerful teacher model \(T\) with superior adaptation performance is first pre-trained on \(^{L}_{src}^{U}_{tgt}\) with SOTA UDA methods. Our objective is to train a compact student model \(S\) which is not only shallower than the teacher model but also can achieve competitive performance on unlabeled target domain. To transfer the learned knowledge from teacher to student, one can just follow standard KD  and force the student to mimic teacher's soften logits via Eq. (1). Here, \(_{b}\) represents a batch of training samples and KL refers to the Kullback-Leibler divergence. \(^{S}\) and \(^{T}\) are the softmax outputs soften by a temperature factor \(\) from student \(S\) and teacher \(T\), respectively. They are calculated by \(q^{S}_{c}=exp(z_{c}/)/_{j=1}^{C}exp(z_{j}/)\), where \(C\) is the number of classes and \(q^{S}_{c}\) represents the student's prediction probability of a certain sample belonging to the \(c\)-th class.

\[_{KD}=_{x_{b}}KL(^{T}||^{S})=_{x _{b}}_{c}p^{T}_{c}log(p^{T}_{c}/q^{S}_{c}).\] (1)

However, since the teacher is trained on unlabeled target data, its prediction performance on specific target sample cannot be guaranteed. The compact architecture of student also limits its ability to fully accept teacher's knowledge. In other words, directly minimizing the distribution discrepancy between teacher's and student's predictions over all target samples might introduce inappropriate knowledge which will mislead the student's learning process. Thus, we propose to alleviate the above issue with a novel RL-based target sample selection module which can dynamically select suitable samples to assist the knowledge transferring. Fig. 1 illustrates the details of our proposed method.

### RL-based Target Sample Selection

Following [23; 24], we consider target sample selection task as a Markov Decision Process which can addressed by reinforcement learning. A RL-based target sample selection module is first designed to enhance the distilling efficiency of teacher's knowledge on target domain. Particularly, a dueling DDQN  is employed to learn the optimal target sample selection policy. The dueling architecture can effectively mitigate the risk of overestimation by separately estimating the state value and advantage function, which improves the accuracy of action-value predictions. Meanwhile, to tackle the instability issue often encountered in training deep reinforcement learning models, we leverage strategies such as target network and experience replay. Specifically, the target network provides more stable targets for updating the Q-values by maintaining a separate, slowly updated network for generating target values, while experience replay enables the model to learn from a diverse set of past experiences, further enhancing stability and convergence during training. In each training batch, we utilize the learned sample selection policy to adaptively transfer teacher's target knowledge according to student's learning capability. In the following, the detailed definition of state, action, reward and the optimization of dueling DDQN will be introduced.

State.Given a batch of target domain samples \(\{x_{i}^{tgt}\}_{i=1}^{n_{b}}\) and student's feature extractor \(F^{S}\), the state \(s_{k}\) at episode \(k[1,K]\) is defined as the feature representations from student's feature extractor \(s_{k}=[F^{S}_{k}(x_{1}^{tgt}),...,F^{S}_{k}(x_{n_{b}}^{tgt})]\). Here, \(n_{b}\) is the batch size and \(K\) represents the maximum episode length. The state \(s_{k}\) is forwarded to the dueling DDQN, generating a set of actions that decide whether to retain or discard the corresponding target samples. The student is subsequently optimized with the selected samples, and the next state \(s_{k+1}\) can be obtained with the updated student. A terminate state will be triggered if the target sample is not selected at time step \(k\) or the episode reaches the maximum episode length \(K\).

Action.For a specific target sample \(x_{i}^{tgt}\) in a batch, it only has two actions \(a_{i}\{0,1\}\) which is binary. Specifically, \(a_{i}=1\) means to retain the sample and \(a_{i}=0\) means to discard it. Since the output of dueling DDQN parameterized with \(_{q}\) is a two-dimensional \(Q\)-value vector, the optimal

Figure 1: Illustration of proposed RCD-KD. A Monte Carlo Dropout (MCD) based reward module is utilized to generate the reward for learning the optimal target sample selection policy. Specifically, the reward function consists of three parts. The first one is the action \(a_{k}\) which is the output of dueling DDQN. The second part is the uncertainty consistency, estimated by entropy from student’s logits \(^{S}\) and the averaged logits \(}^{T}\) of \(N\) teachers generated from MCD module. The third part is the sample transferability based on the KL divergence between \(^{S}\) and \(}^{T}\). The output of reward module \(r_{k}\) then will be utilized for the optimization of dueling DDQN for learning optimal sample selection policy. Meanwhile, a domain discriminator \(\) is employed to transfer the domain-invariant knowledge.

action \(a_{i}^{*}\) for sample \(x_{i}^{tgt}\) at current episode \(k\) thus can be calculated by Eq. (2). Subsequently, the binary weights for all target samples are formulated as \(w=[a_{1}^{*},...,a_{n_{b}}^{*}]\), which then are utilized to calculate the distillation loss \(_{RKD}\).

\[a_{i}^{*}=*{argmax}_{a}Q(F_{k}^{S}(x_{i}^{tgt}),a;_{q}).\] (2)

Reward.The reward function is pivotal in shaping the learning process for target sample selection policy, as it offers essential feedback to DDQN regarding the value associated with selecting a specific action in the current state. To achieve reliable knowledge transferred from selected target samples, we propose to utilize model uncertainty and sample transferability to design the reward function.

The first component constructing our reward function is a Boolean function \(_{1}=(a_{i}==1)\), which indicates whether the sample \(x_{i}\) is retained or not. The second component of our reward function is called uncertainty consistency reward. The motivate is straightforward: due to the lacks of label in target domain, we expect that the student should have the same uncertainty level as the teacher for a specific sample \(x_{i}\). For the teacher model, we employ the Monte Carlo Dropout (MCD)  to estimate its uncertainty, which utilizes a dropout distribution to approximate the posterior distribution (See **Supplementary** for more details). Practically, it means to enable the dropout of teacher model and forward \(N\) times for each sample \(x_{i}\) and the averaged prediction \(}_{i}^{T}=_{n}p(y=c|x_{i},^{n})\) can be utilized to calculate the entropy \(_{i}^{T}=-_{c}_{i,c}^{T}log(_{i,c}^{T})\) for measuring its uncertainty. Here, \(p(y=c|x_{i},^{n})\) represents the probability of sample belonging to class \(c\) and it is the softmax outputs of teacher model on the \(n\)-th forward pass. For the student, the uncertainty is calculated with \(_{i}^{S}=-_{c}q_{i,c}^{S}log(q_{i,c}^{S})\). Intuitively, a higher value of the predictive entropy \(\) will be obtained when all classes are predicted to have equal probabilities, which means the model is less confident about the specific data. To ensure the student has consistent uncertainty level as the teacher, we formulate the uncertainty consistency reward \(_{2}=(_{i}^{S}>}_{j=1}^{n_{b}}_{j}^{S})(_{i}^{T}>}_{j=1}^{n_{b}}_{j} ^{T})\), where \(\) is the exclusive-nor operation. The third component of our reward function is the transferability reward formulated as \(_{3}=(_{i}<}_{j=1}^{n_{b}} _{j})\). Here, \(_{i}=KL(}_{i}^{T}||_{i}^{S})\) is the KL divergence between student's prediction and the averaged MCD teacher prediction. Apparently, the samples whose KL divergence are lower than the averaged divergence are easier ones for the compact student to learn. With the above three Boolean functions, our reward function is defined as Eq. (3) shows:

\[r_{k}=_{1}*(_{1}_{2}-0.5)+_{2}*( _{1}_{3}-0.5),\] (3)

where \(\) is the exclusive-or operation. For the first part of Eq. (3), a positive reward value will be assigned if a sample is retained and student shows consistent uncertainty as the teacher, or it is discarded and student and teacher show inconsistent uncertainty about it. Otherwise, a negative reward will be assigned. Similarly, for the second part of Eq. (3), a positive reward will be assigned if its transferability is higher than others and being selected, or its transferability is lower than others and not selected. We utilize \(_{1}\) and \(_{2}\) to adjust the contribution of each part. We constrain the reward within the range of -1 to 1 to offer explicit guidance to the DDQN so that it can efficiently learn to distinguish between good and bad actions.

Dueling DDQN Optimization.The dueling deep Q-network consists of two streams as shown in Fig. 1: state-value estimation stream \(V(s;_{E},_{V})\) parameterized with \(_{E}\) and \(_{V}\) and advantages estimation stream \(A(s,a;_{E},_{A})\) for each action which is parameterized with \(_{E}\) and \(_{A}\). Here, \(_{E}\) is a shared encoder. Furthermore, to balance the exploitation and exploration, we adopt the NoisyNet  for the fully-connected layers in \(_{E}\), \(_{V}\) and \(_{A}\). Besides, a replay buffer \(\) is designed to store the historical experience \((s_{k},a_{k},r_{k},s_{k+1},d)\), where \(d\{0,1\}\) indicates whether the next step \(k+1\) is the terminal step (\(d=0\)) or not (\(d=1\)). A batch of entries in \(\) will be randomly sampled out for DDQN optimization.

To train the dueling DDQN (_i.e._, the online network \(\)), another target Q-network \(^{}\) is desired, which has identical network architecture as \(\) but is optimized in a different way. Specifically, the online network \(\) is to estimate the \(Q\)-values \(Q_{est}\) by aggregating two steams via Eq. (4):

\[Q_{est}=V(s;_{E},_{V})+A(s,a;_{E},_{A})- _{a_{i}}A(s,a_{i};_{E},_{A}).\] (4)The target Q-network \(^{}\) is to generate the target \(Q\)-values as Eq. (5) shows. Here, \(=\{_{E},_{V},_{A}\}\), \(^{}=\{^{}_{E},^{}_{V},^{}_{A}\}\) are the parameters of \(\) and \(^{}\), respectively. \(\) is the discount factor to balance the immediate and future rewards.

\[Q_{tar}=r_{k}+d**Q(s_{k+1},*{argmax}_{a_{k+1}}Q(s_{k+1},a_{ k+1};);^{{}^{}}).\] (5)

The online network \(\) is optimized by minimizing the Huber loss between \(Q_{est}\) and \(Q_{tar}\). The target network \(^{}\) is updated with a moving average method as shown in Eq. (6), where \(\) is a smoothing parameter determining how much historical information of the online network to be transferred to the target network.

\[^{}*^{}+(1-)*.\] (6)

### Student Optimization

With the proposed RL module, we can efficiently transfer adaptive knowledge from the teacher model to the student model by dynamically eliminating target samples which are unsuitable for student learning. Particularly, we reformulated Eq. (1) to Eq. (7), where \(w=[a^{*}_{1},...,a^{*}_{n_{h}}]\) is the output of online Q-network \(\). By minimizing \(_{RKD}\), student's generalization capability on target domain can be effectively enhanced.

\[_{RKD}=_{x_{b}}w_{j}*_{i}p_{i}^{T}log(p_{i}^{ T}/q_{i}^{S}).\] (7)

Meanwhile, to transfer the domain-invariant knowledge between two domains from teacher to student, we design an adversarial leaning-based module as depicted in Fig. 1, followed . Particularly, a domain discriminator \(\) is employed to distinguish the source of input feature maps (_i.e._, whether the feature maps are generated from the teacher with source data as inputs or the student with target data as inputs). Since the dimensions of student's and teacher's feature maps are different, an adaptor layer \(\) is employed to match their dimensions. The domain confusion loss is then formulated as Eq. (8). It is worth noting that in our experiments, we utilize the DANN  to pre-train the teacher. Although other DA methods can also be adopted in our framework, the DANN can essentially provide a pre-trained accurate domain discriminator after teacher's training. During transferring domain-invariant knowledge, we can re-utilize it and only optimize the student and the adaptor layer \(\), which will significantly improve the training efficiency. Meanwhile, it is also possible that one may utilize some other UDA methods to pre-train the teacher. In this case, the domain discriminator \(\) has to be adversarially trained against the student by minimizing loss \(_{adv}\) as Eq. (9). More experimental results in terms of utilizing other UDA methods to train the teacher can be found in Experiments section.

\[_{adv}=-[log(F^{T}(x_{src}))]-[log(1- ((F^{S}(x_{tgt}))))].\] (9)

The overall loss for student optimization is calculated via Eq. (10). \(\) is a hyperparameter to balance the contribution of each part. Algorithm 1 shows details of proposed **RCD-KD**.

\[=_{DC}+*^{2}*_{RKD}.\] (10)

## 4 Experiments

### Experimental Setup

Datasets.To evaluate our method, extensive experiments are conducted on four public datasets across three different tasks, namely human activity recognition, rolling bearing fault diagnosis and sleep stage classification. To be specific, the first dataset is called human activity recognition (**HAR**)  for identifying subject's activities (_i.e._, _walk_, _walk upstairs_, _walk downstairs_, _stand_ and _sit_). Sensory measurements from the accelerometer and gyroscope embedded in a smartphone were collected from 30 subjects. Considering the variability among subjects, each subject is considered as a single domain and the adaptation is performed between two subjects. Here, we follow existing works [30; 4] and select five transfer scenarios. The second evaluation dataset is Heterogeneity human activity recognition (**HHAR**) . Compared to **HAR**, the sensory measurements are collected with various models of smartphones from different manufacturers which are positioned with various orientations on subjects. Thus, the domain gaps between different subjects are generally considered to be larger than **HAR**. Five transfer scenarios are selected for evaluation same as previous work . The third dataset is rolling bearing fault diagnosis (**FD**)  which aims to classify the health status of rolling bearing from _healthy_, _artificial damages_, _damages from accelerated lifetime tests_. The rolling bearing are tested under various operation conditions. Same as [4; 34], five transfer scenarios between different operational configurations are selected for fair comparison. The last evaluation dataset is sleep stage classification (**SSC**) dataset , which intends to recognize subject's sleep stages (_i.e._, _wake, non-rapid eye movement N1, N2, N3 and rapid eye movement stage_) with electroencephalography waveform. Five scenarios are evaluated following previous study .

Implementations.For the proposed RL-based sample selection module, we set \(=0.9\) and \(=0.999\) following  in Eq. (5) and (6), respectively. We set \(N=10\) to calculate teacher's entropy for the reward function and \(K=5\) for the episodes to generate historical experience. Note that to guarantee fair comparison, we ensure the total training steps of ours and benchmark methods are same. Furthermore, we adopted the **1D-CNN** as the backbone of the teacher and student models following [4; 34], where student is a shallow version of teacher with less filters (See **Supplementary** for detailed network architectures of \(T\), \(S\), \(\) and dueling DDQN). For \(_{1}\), \(_{2}\) in Eq. (3) and \(\), \(\) in Eq. (10), we use the grid search and set \(_{1}=0.2\), \(_{2}=1.8\), \(=1.0\), \(=2\) for all experiments. More sensitivity analysis regarding \(N\), \(K\), \(\), \(_{1}\), \(_{2}\) and \(\) can be found in **Supplementary**. The averaged macro F1-score with three independent running is reported.

### Benchmark with UDA methods

To demonstrate the effectiveness of our proposed RCD-KD, we first compare it with other advanced UDA methods as shown in Table 1. Note that all of the benchmark UDA methods are directly applied to the compact student. From Table 1, some observations can be found. In most transfer scenarios, directly applying UDA methods (either the metric-based or adversarial-based) can improve the performance of compact student model on target domain. However, these methods perform inconsistently across different tasks. For instance, HoMM performs best on **SSC**, but worst on **FD** compared to other UDA methods. Meanwhile, the improvement of these UDA methods is

    &  &  &  & **Ours** \\   & & &   & SASA  & DANN  & CoDMS  & AdvSVM  & **Ours** \\  HAR & 55.94\(\)8.99 & 83.62\(\)1.82 & 84.89\(\)6.29 & 83.37\(\)3.23 & 82.42\(\)3.82 & 75.72\(\)8.62 & 70.72\(\)4.06 & **94.68\(\)1.62** \\ HHAR & 58.74\(\)10.79 & 68.02\(\)6.59 & 73.26\(\)3.85 & 77.13\(\)4.09 & 76.03\(\)1.97 & 74.64\(\)4.18 & 63.24\(\)5.99 & **82.37\(\)1.84** \\ FD & 66.78\(\)4.38 & 74.52\(\)6.00 & 81.80\(\)5.43 & 86.75\(\)2.39 & 77.95\(\)8.52 & 77.54\(\)9.45 & 77.83\(\)5.71 & **92.63\(\)0.62** \\ SSC & 50.39\(\)7.67 & 59.79\(\)5.51 & 57.45\(\)3.68 & 59.36\(\)3.69 & 57.39\(\)5.51 & 57.21\(\)5.61 & 57.28\(\)4.77 & **67.49\(\)1.83** \\   

Table 1: Performance comparison with other UDA methods.

very marginal and large variance can be observed, indicating the challenge of performing domain adaptation with shallow networks. On the contrary, the compact student trained with our proposed RCD-KD consistently performs better than other methods.

### Benchmark with KD-based DA methods

We compare our proposed method with other KD-based DA methods as shown in Table 2 and Table 3. We applied above methods on our teacher-student settings. We also report the performance of pre-trained teacher (generally considered as the upper limit) and the student trained on source domain but tested on target domain (_namely_, Student-Only) as the lower limit. We highlight the best performance with **bold** for each scenario and the averaged performance. Note that this comparison does not include the teacher as it benefits from more complex network architecture. See **Supplementary** for more experimental results of additional transfer scenarios.

Some observations can be found from above two tables. Firstly, compared to Student-Only, all the benchmark methods can obviously improve compact student's generalization on target domain. However, some of them (_e.g._, KD-STDA in **HAR**, **FD** and **SSC**, KA-MCD in **HAR** and **FD**) even perform worse than directly applying UDA on compact student (_e.g._, MDDA and SASA in Table 1). The reason is that those methods blindly trust teacher's predictions on target domain as mentioned and learning with such unreliable knowledge will result in inferior performance. Secondly, the methods using source-only teachers (_i.e._, AAD and MobileDA) failed to achieve better performance than others (_e.g._, UNI-KD and MLD-DA) which employ teachers trained on both source and target domains. This observation indicates that for cross-domain KD, it is critical for the teacher to possess the knowledge of both domains. Thirdly, introducing the source domain specific knowledge to the student like KD-STDA and MLD-DA apparently cannot guarantee better generalization on target data. Intuitively, the student still needs to pay more attention on target domain or focus on domain-shared knowledge as UNI-KD suggested. Lastly, our proposed method consistently outperforms other benchmarks over all the datasets and achieves the highest score in most of transfer scenarios. Meanwhile, our RCD-KD can even achieve comparable performance as the teacher model in some datasets (_e.g._, **HAR**, **HHAR** and **FD**) with more compact model architectures. It indicates the effectiveness of transferring the adaptive knowledge via proposed RL-based sample selection module and domain-invariant knowledge via domain discriminator.

   &  &  \\   & \(-2 11\) & \(6 23\) & \(7 13\) & \(9 18\) & \(2 16\) & Avg & 0\( 6\) & \(-2\) & \(7 3\) & \(-8\) & \(4 5\) & Avg \\  Teacher & 100.0 & 100.0 & 99.92 & 93.69 & 81.65 & 95.05 & 64.47 & 94.23 & 57.22 & 98.88 & 97.69 & 82.50 \\ Student-Only & 68.51 & 59.57 & 78.88 & 21.02 & 51.71 & 55.94 & 50.46 & 65.95 & 43.22 & 58.84 & 75.22 & 58.74 \\  KD-STDA  & 98.31 & 89.55 & 89.28 & 67.41 & 63.13 & 81.54 & 46.15 & 92.19 & 41.69 & 96.51 & 89.79 & 73.27 \\ KA-MCD  & 89.46 & 95.96 & 63.62 & 58.93 & 45.67 & 63.39 & **65.25** & 90.59 & 42.57 & 85.17 & 85.48 & 73.92 \\ MLD-DA  & **100.0** & 99.11 & 92.96 & 82.78 & 64.08 & 87.79 & 61.53 & 94.32 & 47.91 & 91.07 & 92.74 & 77.51 \\ REDA  & 99.44 & 93.81 & 92.43 & 74.55 & 55.77 & 83.20 & 32.05 & 93.85 & 36.10 & 90.24 & 95.41 & 69.53 \\ AAD  & 83.74 & 90.89 & 83.05 & 75.96 & 61.67 & 79.06 & 53.25 & 81.22 & 48.35 & 87.00 & 86.36 & 71.24 \\ MobileDA  & 92.71 & 90.19 & 91.39 & 77.95 & 64.34 & 83.32 & 46.60 & 93.31 & 49.13 & 98.30 & 96.84 & 76.84 \\ UNI-KD  & **100.0** & 96.33 & 93.20 & 79.77 & 64.91 & 86.84 & 46.66 & **94.89** & **59.20** & **98.45** & **97.42** & 79.32 \\ 
**Ours** & **100.0** & **100.0** & **99.64** & **92.87** & **80.87** & **94.68** & 64.47 & 94.24 & 57.59 & **98.45** & 97.11 & **82.37** \\  

Table 2: Marco F1-scores on HAR and HHAR across three independent runs.

   &  &  \\   & \(0 1\) & \(0 3\) & \(2 1\) & \(-12\) & \(2 3\) & Avg & 0\( 11\) & \(12 5\) & \(16 1\) & \(7 18\) & \(9 14\) & Avg \\  Teacher & 88.36 & 86.46 & 88.82 & 99.84 & 99.92 & 92.68 & 51.43 & 68.71 & 73.48 & 72.48 & 76.59 & 68.54 \\ Student-Only & 34.94 & 42.14 & 75.27 & 90.41 & 91.13 & 66.78 & 53.62 & 35.87 & 60.15 & 61.24 & 59.05 & 50.39 \\  KD-STDA  & 53.17 & 50.95 & 76.76 & 89.24 & 98.66 & 73.76 & 43.75 & 53.45 & 49.04 & 67.23 & 65.56 & 55.81 \\ KA-MCD  & 57.96 & 65.26 & 61.66 & 81.75 & 91.79 & 71.68 & **50.85** & 56.73 & 51.01 & 64.18 & 65.95 & 57.74 \\ MLD-DA  & 78.16 & 75.49 & 83.34 & 99.86 & 96.83 & 86.74 & 45.36 & 66.17 & 58.37 & 63.87 & 70.71 & 60.90 \\ REDA  & 86.70 & 81.08 & 88.92 & 92.35 & 88.77 & 87.58 & 44.07 & 52.01 & 60.14 & 60.46 & 64.67 & 56.27 \\ AAD  & 52.50 & 60.00 & 80.86 & 89.84 & 95.99 & 75.84 & 32.71 & 62.92 & 63.34 & 64.46 & 72.15 & 59.12 \\ MobileDA  & 76.19 & 58.77 & 83.74 & 97.56 & 97.84 & 82.82 & 41.83 & 57.14 & 59.41 & 64.38 & 61.55 & 56.86 \\ UNI-KD  & 78.85 & 82.68 & **92.14** & 97.29 & **99.34** & 90.06 & 44.48 & 60.13 & 62.99 & 71.03 & 72.21 & 62.17 \\ 
**Ours** & **89.88** & **85.63** & 88.57 & **99.92** & 99.13 & **92.63** & 49.73 & **70.74** & **72.14** & **71.73** & **76.95** & **68.26** \\  

Table 3: Marco F1-scores on FD and SSC across three independent runs.

[MISSING_PAGE_FAIL:9]

select target sample (as shown in column \(_{2}\) and \(_{3}\)) would improve student's performance in most of datasets. Secondly, dynamically selecting target samples using our RL-base module with either of proposed rewards (as shown in column \(_{2}^{}\) and \(_{3}^{}\)) will further improve student's performance, indicating the effectiveness of RL-based module in mitigating negative transfer. Lastly, combining model uncertainty and transferability as the reward to dynamically select suitable target samples based on student's capacity yields best performance.

Computational Complexity.We performed the time complexity analysis for our method and the results are shown in Table 7. Specifically, we measure the training time for our proposed method and other benchmarks with a NVIDIA 2080Ti GPU. The reported results are measured with one epoch on single transfer scenario on FD dataset, which has the largest training samples (about 1,800 samples per transfer scenario) among evaluated datasets. We can see that our method does require more training time compared to other benchmarks, reflecting its greater complexity. The primary computational costs arise from two factors. The first part is the generation of \(K\) historical experiences at each step. This could be significantly reduced by using a smaller K. The second factor is the MCD module which conducts multiple inference processes for uncertainty estimation. This computational burden could be further decreased by adopting alternative uncertainty estimation methods. Nevertheless, although our training time is longer than other benchmarks, we argue that it is still within an acceptable range, especially considering the performance improvement it could bring. Meanwhile, we also evaluate the scalability of our approach to larger dataset. See **Supplementary** for more scalability analysis.

## 5 Conclusion and Limitations

In this paper, we propose a framework for cross-domain knowledge distillation on time series. Specifically, we utilize an adversarial domain discriminator to assist the compact student learn the domain-invariant knowledge from the cumbersome teacher. Meanwhile, we design a reinforcement learning-based target sample selection module to effectively transfer teacher's knowledge which is suitable for compact student. The experimental results demonstrate the effectiveness of our proposed method in enhancing the generalization of compact student on target domain. There are also some limitations for our proposed framework. On the one hand, we still need to pre-train a cumbersome teacher with advanced UDA methods, which involves more training time than others. On the other hand, we only utilize the distance between teacher's and student's logits to assess sample's transferability, which might overlook some intrinsic information from feature space. In the future, we will extend our work to (1) jointly train teacher and student for cross-domain knowledge distillation, and (2) consider feature representations into sample transferability assessment.

    &  &  \\   & Samples & \(_{2}\) & \(_{2}^{}\) & \(_{3}\) & \(_{1}^{}\) & \(_{2}+_{3}\) & \((_{2}+_{3})^{}\) \\  HAR & 89.32 & 91.65 & 93.91 & 92.31 & 93.96 & 93.53 & **94.68** \\ HHAR & 78.99 & 78.30 & 81.73 & 80.33 & 82.29 & 81.04 & **82.37** \\ FD & 89.13 & 90.17 & 91.93 & 89.51 & 91.08 & 91.85 & **92.63** \\ SSC & 60.65 & 63.16 & 62.98 & 65.81 & 67.49 & 65.20 & **68.26** \\   

Table 6: Reinforced sample selection ablation. “Full samples” denotes utilizing whole target samples for KD; ‘\(_{2}\)’, ‘\(_{3}\)’ denote directly utilizing proposed uncertainty and transferability for sample selection; ‘\(_{2}\)’, ‘\(_{3}\)’ denote utilizing RL with \(_{2}\) and \(_{3}\) as reward for sample selection; \((_{2}+_{3})^{}\) is **our** proposed method.

   Methods & KD-STDA & KA-MCD & MLD-DA & REDA & AAD & MobileDA & UNI-KD & **Ours** \\  Time (sec) & 1.68 & 4.55 & 1.91 & 1.78 & 0.91 & 1.28 & 3.26 & 16.42 \\   

Table 7: Comparison of Computational Complexity.