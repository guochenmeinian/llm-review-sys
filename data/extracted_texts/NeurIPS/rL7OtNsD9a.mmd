# Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning

Dongsu Lee\({}^{}\) and Minhae Kwon\({}^{}\)

\({}^{}\)Department of Intelligent Semiconductors

\({}^{*}\)School of Electronic Engineering

Soongsil University, Seoul, South Korea

movementwater@soongsil.ac.kr, minhae@ssu.ac.kr

Corresponding author: M. Kwon

\({}^{}\)Department of Intelligent Semiconductors

\({}^{*}\)School of Electronic Engineering

Soongsil University, Seoul, South Korea

movementwater@soongsil.ac.kr, minhae@ssu.ac.kr

###### Abstract

Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce _episodic future thinking (EFT) mechanism_ for a reinforcement learning (RL) agent, inspired by cognitive processes observed in animals. To enable future thinking functionality, we first develop a _multi-character policy_ that captures diverse characters with an ensemble of heterogeneous policies. Here, the _character_ of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and uses the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent interactions. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario with diverse driving traits and multiple particle environments. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.2

## 1 Introduction

Understanding human decision-making in multi-agent interactions is a significant focus in cognitive science. It provides valuable insights into designing interactions among diverse AI agents within multi-agent systems. Research has shown that humans employ counterfactual or future scenario simulation to enhance decision-making . While _counterfactual thinking_, simulating alternative consequences of past events, has been extensively explored in multi-agent RL (MARL) , _episodic future thinking_, the ability to anticipate future events, remains underexplored in literature despite its importance in handling multi-agent interactions.

Human beings strive to anticipate future situations to prevent costly mistakes. One naive approach to incorporate this ability into AI is through future trajectory prediction using model-based RL . However, this approach is feasible only if the state transition model is known or easily learnable, which is often not the case in multi-agent systems. The complexity arises fromthe interdependence of state transitions on the actions of both the agent and other agents, making learning the state transition model challenging. Additionally, diverse agent characteristics exacerbate this challenge by introducing a wide range of action combinations and subsequent states. Thus, explicitly integrating character inference functionality regarding other agents into AI is more suitable for accurate future state prediction and optimal decision-making.

Our goal is to develop an EFT mechanism for RL agents, enabling them to make adaptive decisions in a society where agents have heterogeneous characteristics. We formalize this task as a Multi-agent Partially Observable Markov Decision Process (MA-POMDP), a framework tailored to address the RL problem wherein multiple agents operate under partial observation [35; 55]. This study defines a character by reflecting the behavioral preferences of RL agents, which come from different weight combinations on reward components. For instance, in a driving scenario, some drivers prioritize safety, while others prioritize speed, leading to heterogeneous policies and behavioral patterns across agents.

Implementing the EFT mechanism requires two functional modules: a multi-character policy and a character inference module. The multi-character policy embeds behavioral patterns corresponding to characters. It allows the agent to observe partial information of the state in continuous space and handles a hybrid action space consisting of discrete and continuous actions. The character inference module leverages the concept of inverse rational control (IRC) [18; 25] to infer target agents' characters by maximizing the log-likelihood of their observation-action trajectories. Combining these modules equips the agent with EFT functionality, enabling proactive behavior under heterogeneous multi-agent interactions.

To activate the EFT mechanism, the agent initially acts as an observer, collecting observation-action trajectories of target agents. Utilizing the character inference module and collected trajectories, the agent infers target agents' characters. With this knowledge and leveraging a multi-character policy, the agent predicts others' actions and simulates future observations with its action fixed as 'no action.' This mental simulation allows the agent to estimate the observation at the time point when all target agents have taken actions, but the agent still needs to (_i.e._, has yet to). It enables the agent to select the best action corresponding to the estimated future observation. In summary, the EFT mechanism empowers the agent to behave proactively in heterogeneous multi-agent interactions.

**Summary of contributions:**

* We introduce character diversity in a multi-agent system by parameterizing the reward function. We propose to build the multi-character policy and equip the agent with it to infer the character of the target agent (Section 3).
* We introduce the EFT mechanism for social decision-making. The agent infers the characters of other agents using the multi-character policy, predicts their future actions based on the inferred characters, simulates the corresponding future observations and selects foresighted actions. This mechanism enables the agent to consider multi-agent interactions in its decision-making process (Section 4).
* We verify the proposed mechanism by increasing character diversity in society. Extensive experiments confirm that the proposed mechanism enhances group rewards no matter how high a character diversity level exists in society (Section 5).

## 2 Related Works

**Episodic Future Thinking.** Cognitive neuroscience aims to understand how humans use memory in decision-making. Interestingly, the trend of the brain's regional neural activation regarding counterfactual reasoning (_i.e._, simulating alternative consequences of the last episode) and future thinking (_i.e._, simulating episodes that may occur in the future) is similar . In , the authors study the relationship between future thinking and decision-making and confirm that humans perform future-oriented decision-making. The decision-making abilities, such as strategy formulation, are also significant in scenarios that require multi-agent interactions, _e.g._, social decision-making.

There are several studies to endow this ability with an AI agent [62; 37; 61; 30]. In  and , the authors forecast the next state from a macroscopic standpoint without a prediction of each agent's behavior. In , the authors predict the behavior of an agent through a deep Bayesian network considering the dynamics and the previous surrounding environment information. Even though these studies can infer future information, no strategy formulation incorporated with prediction is suggested. Namely, most existing approaches use future predictions as auxiliary information for the optimization process without incorporating these predictions into the policy explicitly. In this study, we propose the ETF mechanism can predict future observations based on the current state and predicted actions of surrounding agents. Consequently, the agent equipped with this mechanism can select a foresighted action corresponding to the anticipated future observation.

**Model-based Reinforcement Learning.** Model-based RL incorporates an explicit module representing system dynamics, contrasting with model-free RL. Within model-based RL, two approaches exist: utilizing a known dynamic model and learning it during training . Using the dynamic model, the model-based RL approaches predict future trajectories, a pivotal step for network optimization . Notably, approaches such as Dreamer  and Model-Based Policy Optimization (MBPO)  demonstrate the practical application of these predictions. Dreamer optimizes a value function using the return of the predicted future trajectories, and MBPO trains the policy using the predicted future trajectories as augmented data samples. Furthermore, to tackle multi-agent problems,  and  extend these concepts by integrating a global model or communication block.

While these methods often exhibit outstanding performance, they assume ideal conditions such as a small number of homogeneous agents and full observability. In reality, agents encounter incomplete and noisy data, and accurately modeling system dynamics is challenging due to complex interactions between multiple agents with unique behavioral characteristics. This work addresses a partially observable agent in a multi-agent environment with heterogeneous characteristics across agents. We allow the agent to infer other agents' characters and make decisions based on predictions of upcoming observations.

**Machine Theory of Mind.** Human decision-making in social contexts often involves considering multiple perspectives, including the behavioral characteristics of others. This capacity, known as Theory of Mind (ToM) in cognitive science, primarily involves deducing internal models of others and predicting their future actions . AI research aimed at providing machines with this capability has gained attention for enhancing multi-agent system performance, such as machine ToM , inverse learning , and Bayesian ToM . These approaches aim to reconstruct the target agent's belief, reward function, or dynamic model based on its trajectories. However, they often operate in simple settings, limiting their applicability to scenarios with a small number of agents, a small discrete action space, or minimal character diversity across agents.

In contrast to previous work, this study explicitly develops a character inference module focusing on establishing a link between trajectories and characters. This module allows the target agent's behavior to be explained by character, aligning with the researcher's interests. Additionally, it extends the action space from continuous to hybrid.

**False Consensus Effect.** Psychological research has identified a cognitive bias in humans to assume that their character, beliefs, and actions are common among the general population , termed the False Consensus Effect (FCE) . Recent studies suggest that AI may exhibit this false belief . To underscore the importance of character inference in heterogeneous multi-agent scenarios, we compare the performance of the EFT mechanism with two types of agents: the proposed agent, equipped with the character inference module, and the FCE-based agent, which assumes that target agents share the same character as the agent.

## 3 Character Inference Using Multi-character Policy

We aim to build an agent to make optimal decisions under multi-agent interactions. It requires the agent to be able to anticipate the near future by predicting other agents' actions. The agent should possess the ability to infer the others' characters, leveraging observation of their behaviors.

Figure 1: A block diagram of an agent \(i\) with a multi-character policy \((_{t,i};)\), where \(\) is character space. The agent can infer the character \(\) of others by using the maximum likelihood estimation. Herein, \(K\) means the dimension of character vector \(\).

Accurate character inference is a prerequisite for the EFT mechanism since the character is a crucial clue to predicting future action. Therefore, this section proposes two functional modules for character inference: a multi-character policy and character inference. An illustrative explanation of these functionalities is presented in Figure 1.

### Problem Formulations for Multi-agent Decision-making

We consider multi-agent scenarios where RL agents adaptively behave to each other. All agents have to make decisions and execute actions simultaneously, unlike the extensive-form game  in which the agents alternate executing the actions.

The multi-agent decision-making problem can be formalized as a MA-POMDP \(M= E,,\{_{i}\},\{_{i}\},, \{_{i}\},\{R_{i}\},_{i E}\) that includes an index set of agents \(E=\{1,2,,N\}\), continuous states \(s_{t}\), continuous observations \(o_{t,i}_{i}\), hybrid actions \(a_{t,i}=\{a_{t,i}^{c},a_{t,i}^{d}\}_{i}\), where continuous action \(a_{t,i}^{c}_{i}^{c}\) and a discrete action \(a_{t,i}^{d}_{i}^{d}=\{w:|w| W,\ w,\ W\}\), where the size of discrete action space is \(|_{i}^{d}|=2W+1\), \(\) denotes the set of integers, and \(\) denotes the set of natural numbers. Let \(:=_{1}_{2}_{N}\). Subsequently, \(:\) is the state transition probability; \(_{i}:_{i}\) is the observation probability; \(R_{i}:_{i}\) denotes the reward function that evaluates the agent's action \(a_{t,i}\) for a given state \(s_{t}\) and the outcome state \(s_{t+1}\); \([0,1)\) is the temporal discount factor.

An unordered set of the actions of all agents at time \(t\) is denoted as

\[_{t}= a_{t,1},,a_{t,i},,a_{t,N}= a _{t,i},_{t,-i},\]

where subscript \(-i\) represents the indices of all agents in \(E\) except \(i\). Thus, \(_{t,-i}= a_{t,1},,a_{t,i-1},a_{t,i+1},a_{t,N}\) represents a set of all agents' actions at time \(t\) without \(a_{t,i}\). The state transition probability denotes \((s_{t+1}|s_{t},_{t})\). Note that state transition is based on the action combination of all agents \(_{t}\), not on the action of a single agent \(a_{t,i}\).

Next, \(_{i}=\{c_{i,1},c_{i,2},,c_{i,K}\}^ {K}\) denotes a \(K\)-dimensional character vector for the agent \(i\). Character \(_{i}\) can parameterize the reward function of the agent \(i\), _i.e._, \(R_{t,i}=R_{i}(s_{t},a_{t,i},s_{t+1};_{i})\). The agent aims to learn the optimal policy that returns the optimal action \(a_{t,i}^{c}^{*}(|o_{t,i};_{i})\) given observation and character. Specifically, the objective of the agent aims to maximize the expected discounted cumulative reward \(()=_{}_{t}^{t}R_{i}(s_{t},a_{t,i},s_{t+1};_{i})\) by building the best policy \(\). This defines the state-action value function \(Q^{}(s,a;_{i})=_{}_{t}^{t}R_{i}(s_{ t},a_{t,i},s_{t+1};_{i})|s_{0}=s,a_{0}=a\). In the next section, we discuss the details of the multi-character policy in terms of neural network design and its training.

### Training a Multi-character Policy

The multi-character policy includes inputs in continuous space (_e.g._, observation \(o_{t,i}\) and character \(_{i}\)) and outputs in hybrid space (_e.g._, action \(a_{t,i}\)). To build the policy generalized over continuous space, the actor-critic architecture is used. It approximates the policy \(_{}(|o_{t,i};_{i})\) and Q-function \(Q_{}(o_{t,i},a_{t,i};_{i})\), where \(\) denotes parameters of the actor network and \(\) denotes the parameters of the critic network.

The loss functions used to train the actor and critic networks are \(()=-Q_{}(o_{t,i},_{}(|o_{t,i};_{i})),\) and \(()=|y_{t}-Q_{}(o_{t,i},_{}(|o_{t,i}; _{i}))|^{2}\), respectively. Herein, \(y_{t}=R_{t,i}+Q_{^{}}(o_{t+1,i},_{^{}}(|o_{t+1, i};_{i}))\) represents the Temporal Difference (TD) target, where \(^{}\) and \(^{}\) denote the target networks.

Next, we propose a post-processor \(g()\) to handle hybrid action space. Let a proto-action \(_{t,i}^{d}\) be the output of the actor network. The post-processor \(g()\) performs quantization process by discretizing the continuous proto-action \(_{t,i}^{d}\) into discrete post-action \(a_{t,i}^{d}\), _i.e._,

\[a_{t,i}^{d}=g(_{t,i}^{d},W)=( {a}_{t,i}^{d}+),W,\] (1)

where \(\) denotes a floor function. The derivation of (1) is presented in Appendix D.

We summarize the multi-character policy training process in Algorithm 1. In the next subsection, we introduce the character inference module that infers the characters of other agents.

### Inferring Character of Target Agent

After completing the training on the multi-character policy, our next objective is to infer the character \(_{j}\) of the target agent \(j E\). The agent first collects observation-action trajectories of the target for character inference. Subsequently, it utilizes the multi-character policy to identify the character \(_{j}\) that best explains the collected data. To elaborate, \(_{j}\) can be estimated by maximizing the log-likelihood of observation-action trajectories \( P(o_{1:T,j},a_{1:T,j}|_{j})\). This can be formulated as follows.

\[}_{j}=_{} P(o_{1:T,j},a_{1:T,j}| )=_{}_{t=1}^{T}[(a_{t,j}^{c}|o_{t,j};)+(a_{t,j}^{d}|o_{t,j};)]\]

The derivation of (2) can be found in Appendix E.

To efficiently perform the inference task, we use the gradient ascent method. It runs the iteration by changing \(\) toward the direction to increase \(()=(a_{t,j}^{c}|o_{t,j};)+(a_{t,j}^ {d}|o_{t,j};)\), which is summarized in Algorithm 2.3

```  Initialize: Actor network \(\), critic network \(\)  Require: Total episode \(M\), total time steps per episode \(T\), discrete action space \(W\), agent \(i\) for episode \(m=1\), \(M\)do  Reset \(s_{1}\) and get \(o_{1,i}_{i}(|s_{1})\)  Sample character \(_{i}\) for timestep \(t=1\), \(T\)do  Get proto-action \(\{a_{t,i}^{c},_{t,i}^{d}\}_{}(|o_{t,i};_{i})\)  Get post-action \(a_{t,i}^{d} g(_{t,i}^{d},W)\)  Execute \(a_{t,i}=\{a_{t,i}^{c},a_{t,i}^{d}\}\), Update \(s_{t+1}\)  Receive \(R_{t,i}\), Get \(o_{t+1,i}_{i}(|s_{t+1})\)  Calculate \((),()\), Update \(,\) endfor endfor return\(,\) ```

**Algorithm 1** Multi-character policy training

## 4 Foresight Action Selection Based on Episodic Future Thinking Mechanism

This section presents the proposed EFT mechanism that enables the agent to simulate the subsequent observations and to select a foresighted action. The proposed EFT mechanism comprises a future thinking module and an action selection module.

The future thinking module includes two steps: action prediction and the next observation simulation. With these two steps, the agent can foresee the next observation. This process is illustrated in Figure 2. Subsequently, the action selection module enables the agent to decide the current action corresponding to the simulated next observation.

### Future Thinking: Step I - Action Prediction

In this step, the agent with the multi-character policy predicts the actions of the neighbor agents by using pre-inferred characters and observations. The agent can predict the action of the target agent \(j\) (\(E,j\)

Figure 2: Diagram of POMDP with EFT mechanism. The future thinking and action selection modules are included to obtain action from the observation. The solid lines and circles represent the actual event. The dashed ones depict the virtual event in the simulated world of the agent \(i\).

\(i)\)4 using the trained multi-character policy \(_{}\) and inferred character \(}_{j}\), _i.e._, \(_{t,j}_{}(|o_{t,j};}_{j})\). Therefore, the predicted action set of others \(}_{t,-i}\) is as follows.

\[}_{t,-i}=_{}(o_{t,1};}_{1}), ,_{}(o_{t,i-1};}_{i-1}),_{}(o_{t,i+1};}_{i+1}),,_{}(o_{t,N};}_{N})\]

### Future Thinking: Step II - Next Observation Simulation

In this step, we introduce how the agent simulates its next observation by using the predicted action \(}_{t,-i}\). Note that this prediction is the result of the mental simulation of agent \(i\), when \(a_{t,i}=\) is satisfied. Herein, \(\) denotes null action, meaning that no action is performed. This is to simulate the observation of the time point when all target agents performed the action, but the agent has not yet.

The simulated next observation \(_{t+1,i}\) can be determined based on the predicted action set \(}_{t,-i}\) and the current observation \(o_{t,i}\). The function of the next observation simulation \(()\) is defined as \(_{t+1,i}=(o_{t,i},}_{t,-i},a_{t,i}=)\). The action selection using the simulated next observation \(_{t+1,i}\) allows the agent to ignore the influence of others' actions. This is because the next state is determined solely by its own action \(a_{t,i}\) in the agent's mental simulation, as \(_{t+1,i}\) has already applied the others' actions \(}_{t,-i}\).

### Action Selection

Once the agent has simulated the next observation \(_{t+1,i}\), the agent can make a foresighted decision. The agent uses the multi-character policy \(_{}\) with the input of the simulated next observation \(_{t+1,i}\) and its own character \(_{i}\), and finally gets the action \(a_{t,i}=\{a_{t,i}^{c},_{t,i}^{d}\}=_{}(|_{t+1,i}; _{i})\). In other words, the agent can select an adaptive action with consideration for others' upcoming behaviors. The decision-making procedure with the proposed EFT mechanism is summarized in Algorithm 3.

``` Require: Trained actor-network \(\), discrete action space parameter \(W\), set of inferred characters \(}_{-i}\), character of agent \(_{i}\), initial state \(s_{1}\) for\(t=1\), \(T\)do  Get observation \(o_{t,i}_{i}(s_{t})\)  // Start future simulation // for\(j=1\), \(N(j i)\)do  Get observation \(o_{1,j}_{j}(s_{t})\)  Predict action of agents \(j\) \(_{t,j}_{}(|o_{t,j};_{j})\)  Store \(_{t,j}\) in predicted action set \(}_{t,-i}\) endfor  Simulate future observation of agent \(i\) \(_{t+1,i}=(o_{t},}_{t,-i},a_{t,i}=)\)  // End simulation //  Get proto-action \(\{a_{t,i}^{c},_{t,i}^{d}\}_{}(|_{t+1,i}; _{i})\)  Get post-action \(a_{t,i}^{d} g(_{t,i}^{d},W)\)  Execute \(a_{t,i}=\{a_{t,i}^{c},a_{t,i}^{d}\}\), Update \(s_{t+1}\) endfor ```

**endfor**

Simulate future observation of agent \(i\) \(_{t+1,i}=(o_{t},}_{t,-i},a_{t,i}=)\)  // End simulation //  Get proto-action \(\{a_{t,i}^{c},_{t,i}^{d}\}_{}(|_{t+1,i}; _{i})\)  Get post-action \(a_{t,i}^{d} g(_{t,i}^{d},W)\)  Execute \(a_{t,i}=\{a_{t,i}^{c},a_{t,i}^{d}\}\), Update \(s_{t+1}\) endfor ```

**Algorithm 3** Episodic future thinking mechanism

## 5 Experiments

To select a suitable task that can verify the effectiveness of the proposed solution, we consider the following requirements. There should be multiple approaches to achieving character diversity, as well as interactions between agents. The agent should have only partial observations of the state, and the action space should be both continuous and discrete.

We chose the autonomous driving task, which has numerous automated vehicles on the road. The task can consider the driving character of the agent based on driving preferences (_e.g._, one agent prioritizes safety, and the other prioritizes speed) . Additionally, it is realistic for a driver to behave under the partial observation of the road state, and the driver makes a decision in a hybrid action space. To implement this task, we use the FLOW framework . The scenario includes multiple automated vehicles on the highway. The number of agents \(|E|=21\), and each agent decides on acceleration and lane change control at a given observation. Here, we express the driving character using weights of three reward terms, _i.e._, \(_{i}=[c_{i,1},c_{i,2},c_{i,3}]\).5 The target agent \(j\) is limited to the vehicles located in the observable area.

To confirm the scalability of the proposed solution, we also provide simulation results with a multiple particle environment (MPE)  and starcraft multi-agent challenge (SMAC) , a popular MARL testbed. All results in this section are averaged results of over \(10\) independent experiments. The markers indicate the average value, and the shaded area represents the confidence interval within one standard deviation.

### Performance Evaluation: Character Inference

To make the EFT mechanism more effective, an accurate character inference should be preceded. In this subsection, we investigate the character inference module with two questions.

* How many iterations does it require to achieve an accurate inference (in terms of repetition in Algorithm 2)?
* How long should the agent collect the observation-action trajectories of target agents (in terms of trajectory length \(T\) in Algorithm 2)?

In Figure 3, the performance of the character inference module is presented. To ignore the effect of the initial point in convergence, the initial point of the character is randomly selected. More results regarding the initial point are provided in Appendix I.

Figure 3**A** illustrates the convergence of the estimated character to the true one. The inaccuracy of inference is evaluated based on the L1-norm between the estimated character and the true one. Thus, a lower L1-norm implies higher inference accuracy. As the number of iterations increases, the L1-norm quickly decreases to approximately zero, meaning that the estimated value quickly converges to the true one. Specifically, if the number of iterations is set to over \(500\), high accuracy of the character inference can be achieved.

Figure 3**B** shows the trade-off between the length of observation-action trajectory \(T\) and the number of iterations required for the convergence. The convergence criterion is set to L1-norm \( 5 10^{-4}\). The results demonstrate that the number of iterations for convergence decreases as longer trajectories are provided. Thus, the length of trajectories and the number of iterations can be jointly determined by considering system requirements.

### Ablation Study: Character Inference and EFT Modules

We investigate the impact of two main modules (the character inference module and the EFT module) on performance by increasing character diversity levels of the heterogeneous society. The following three cases are compared.

* Proposed: the agent enables the EFT with the inferred character of other agents based on the character inference module.
* FCE-EFT: the agent experiences the FCE by assuming that all other agents have equal character to itself (_i.e._, \(_{j}=_{i}\), \( j E\)). So, no character inference is required. The agent performs the EFT, but action prediction is performed based on the same character \(_{i}\).
* without EFT (baseline) : the agent performs neither character inference nor the EFT mechanism. It treats the problem as a single agent RL and selects the best action given observation. The policy is trained based on the TD3.

In Figure 4, the average rewards of entire agents are presented over increasing the number of character groups.6 The higher number of character groups means that more diverse characters coexist in society, and the higher reward implies better performance. Because the number of agents is fixed to \(|E|=21\), the number of members per character group is \(|E|/n\), where \(n\) denotes the number of groups. The members belonging to the same group have the same character \(\). Note that a character of each group is randomly sampled from character space \(\) in every independent experiment.

Figure 3: The performance of the character inference module. **A**. L1-norm between estimated and true characters over the number of iterations (\(T=1000\)). **B**. The number of required iterations for convergence over the length of the observation-action trajectory \(T\).

Figure 4 highlights the amount of reward enhancement or degradation by equipping the proposed modules. The proposed approach consistently outperforms the baseline (without EFT), and the FCE-EFT is inferior to the baseline when character diversity exists. These results verify that the EFT mechanism with accurate character inference always enhances the reward. However, the naive employment of the EFT mechanism with the incorrect character degrades the reward. This is because incorrect character inference leads to incorrect action prediction and next observation simulation, which leads to improper action selection of the agent, leading to low reward. Therefore, accurate character inference is crucial in the EFT mechanism.

### Investigating the Effects of Trajectory Noise

To infer the character of the target agent, the EFT agent needs to collect observation-action trajectories of the target agent. Since the observations made by the EFT agent towards the target agent may not be perfect (_i.e._, they could be a noisy version of the target agent's true observations), we further investigate the performance of the proposed EFT framework concerning the accuracy of the collected trajectories. This investigation consists of two steps. First, we look deeply into the effect of trajectory accuracy on character inference, and thereafter, we examine the EFT performance regarding character inference accuracy.

**Character inference with trajectory accuracy.** Table 1 shows the character inference accuracy as the noise level for a collected trajectory increases. As expected, the character inference accuracy decreases as the noise variance increases. Please be aware that the considered standard deviation is not trivial given that our observation range is \([-1,1]\). Specifically, we provide the signal-to-noise ratio (SNR) with a quality level (Qual) across each standard deviation. We label the quality of each level based on .

We believe that this result provides valuable insights into the expected performance of our proposed solution, particularly in scenarios where observation prediction technology is deployed.

**EFT performance with character accuracy.** In Figure 5, \(x\) and \(y\) axes are the accuracy of character inference and average reward, and \(n\) is diversity level. As expected, the result shows that the performance of the EFT agent naturally increases when the accuracy of predicted observation increases. Interestingly, the proposed solution holds up the performance even at a character inference accuracy of approximately 90\(\%\) (_i.e._, the error rate of 10\(\%\)). It is also worth mentioning that the performance has a similar trend across the diversity levels, which confirms that the proposed method is robust against diversity levels.

### Assessing Generalizability: Inference on Out-of-Distribution Character

It can be impractical and challenging to train all characters within a pre-defined range, and a trained agent can confront an out-of-distribution (OOD) character in the deployment phase. This subsection demonstrates the inference performance on the OOD range of pre-trained agents with specific character samples. To this end, we consider the following two cases:

    &  \\   & \(0.01\) & \(0.05\) & \(0.1\) & \(0.2\) & \(0.3\) \\   & \(99.6\) & \(98.3\) & \(91.8\) & \(81.1\) & \(69.5\) \\  & \( 0.01\) & \( 0.07\) & \( 0.23\) & \( 0.52\) & \( 0.66\) \\   &  & \(21.3\) & \(14.7\) & \(9.2\) & \(4.7\) \\  & Excellent & Good & Fair & Poor & Poor \\   

Table 1: Character inference accuracy over the standard deviation of trajectory noise. (Accuracy: ACC)

Figure 4: The amount of reward enhancement for two EFT approaches by setting without EFT as a baseline (_i.e._, the reward of other approaches - the reward of without EFT).

Figure 5: The average reward for increasing the accuracy of character inference.

1. train on \([0.0,0.6]\) and \([0.8,1.0]\), thereby inferring on \(\{0.65,0.7,0.75\}\),
2. train on \([0.2,0.8]\), thereby inferring on \(\{0.0,0.1,0.9,1.0\}\).

Figure 6 shows the average of estimated characters over true ones. The gray dimmed area is the OOD range, which is an unseen character range in the training phase, and red and blue circles present an OOD and in-distribution estimated character value, respectively. Figure 6**A** represents case 1, where the model appears to perform well in predicting characters in unseen regions. Figure 6**B** represents case 2, which performs inference on the points outside of the trained range. It is observed that the inference accuracy is slightly declined compared to case 1, but it can still successfully capture the overall pattern by predicting the extreme values that are close to the true ones.

### Performance Comparisons

We compare the performance of the proposed solution to the following popular MARL, model-based RL, and agent modeling algorithms: MADDPG , MAPPO , Q-MIX , Dreamer , MBPO , ToMC2 , and LIAM . In baseline algorithms, we go through independent policy training regarding the diversity level of society.7 Note that the proposed method does not need plural training for different heterogeneity settings. See Appendix J for an additional explanation of the baseline algorithms and standard deviation for Table 2.

Table 2 shows the average reward of the entire agents as the number of character groups increases. This result verifies that the proposed solution outperforms all popular MARL algorithms. Note that the MARL algorithms assume centralized training, which requires access to the observations and actions of all agents in policy training. In contrast, our solution trains the policy with only local observations and actions, which can be a more practical solution. The Q-MIX has the lowest performance since it operates in a discrete action space, whereas our task is in a hybrid action space.

Table 2 also demonstrates the performance of popular model-based RL algorithms as the diversity level increases. It is obvious that the performance gap between model-based RL and the proposed solution increases as the diversity level increases. In addition, the standard deviation of model-based RL algorithms (provided in Table J1 in Appendix J) is much larger than the proposed solution, which shows the difficulty of learning a dynamic model without understanding others in multi-agent systems. Specifically, Dreamer cannot adapt to high diversity levels, and it has a broader variance than other algorithms. Additionally, the result of MBPO exhibits that it is hard to trust generated transitions from a dynamic model.

In the case of agent-modeling algorithms, ToMC2 achieves the best score in the \(n=1\) scenario, but its performance decreases as the diversity level increases; LIAM fails at all diversity levels. On the other hand, the proposed solution is robust to changes in the surrounding agents and maintains high performance across diversity levels. We conjecture why two baselines fail in this setup, as follows.

    &  \\  & \(1\) & \(2\) & \(3\) & \(4\) & \(5\) \\  Proposed & \(2899\) & **3047** & **2976** & **2948** & **3051** \\ FCE-EFT & \(2899\) & \(2784\) & \(2646\) & \(2566\) & \(2629\) \\  MADDPG  & \(2763\) & **3006** & \(2800\) & **2933** & \(2856\) \\ MAPPO  & \(2753\) & \(2862\) & \(2597\) & \(2529\) & \(2763\) \\ Q-MIX  & \(2199\) & \(2310\) & \(2288\) & \(2118\) & \(1861\) \\  Dreamer  & \(2911\) & \(2813\) & \(2733\) & \(2631\) & \(2701\) \\ MBPO  & \(2089\) & \(1964\) & \(1523\) & \(1893\) & \(1633\) \\  ToMC2  & **3016** & \(2812\) & \(2683\) & \(2691\) & \(2511\) \\ LIAM  & \(1913\) & \(1792\) & \(1771\) & \(1683\) & \(1733\) \\   

Table 2: Performance comparison across diversity level.

Figure 6: The performance of the character inference module on OOD character range.

ToMC2 requires retraining or adjusting the ToM module as surrounding agents change. The ToM module is tailored to other agents for the prediction of information (_e.g._, goals, observations, and actions). Next, LIAM also necessitates a new opponent modeling process for each test environment. In addition, prior works on opponent modeling rarely involve more than four players.

### Additional Evaluation on MPE and SMAC

Beyond the autonomous driving task, we run the performance comparison on the MPE and SMAC testbed.

**Multiple Particle Environment.** The MPE tasks consider a small number of agents (three or four) and groups (one or two). Therefore, we set the character for each group as a single character, that is, the diversity level \(n=1\). Table 3 shows the performance comparison across each task of MPE. Even though our method is specialized for a high level of character diversity environment, the results demonstrate that the proposed solution is competitive in a simple environment by achieving the best score in two out of three tasks. We provide additional information on the MPE task in Appendix K.

**StarCraft Multi-agent Challenge.** The setup of SMAC tasks is similar to MPE tasks, _i.e._, the EFT agent does not need to infer the character because they have the same (character diversity as \(n=1\)). Table 4 exhibits the performance on the SMAC tasks. The proposed solution demonstrates superior performance across SMAC tasks, particularly excelling in more complex scenarios like 3s5z_vs_3s6z and 6h_vs_8z. Although MAPPO shows competitive performance, especially in simpler tasks like 2s3z, the proposed method proves more effective overall in handling both simple and complex multi-agent tasks. Additional information in terms of SMAC tasks can be shown in Appendix L.

## 6 Discussion

**Conclusion.** In this paper, we propose the EFT mechanism, which is a social decision-making approach for a multi-agent scenario. The EFT mechanism enables the agent to behave by considering current and near-future observations. To achieve this functionality, we first build a multi-character policy that is generalized over character space. Then, the agent with the multi-character policy can infer others' characters using the observation-action trajectory. Next, the agent predicts the others' behaviors and simulates its future observation based on the proposed EFT mechanism. In the simulation result, we confirm that the proposed solution outperforms existing solutions across all diversity levels of the heterogeneous society.

**Broader Impacts.** The proposed EFT idea paves the way for research on multi-agent scenarios. The proposed method enables the agent to simulate other agents' upcoming actions, which is analogous to humans' decision-making. Furthermore, we believe the proposed method can be broadened by combining counterfactual thinking, current information, and future thinking.

**Limitations.** Even though this work shows promising results with a novel method, there are a few limitations to tackle. In our experiments, there is only one EFT agent, and all other agents do not have the EFT functionality. This is an inevitable setting to make the problem tractable. Additionally, we follow the non-stationary regarding the agent's policy in the training phase and stationary in the execution phase. Since the character is mapped into policy, this stationary property has a connection to the character itself. To improve practicality, we should further investigate how the proposed solution works when the other agent's policy is non-stationary in the execution phase.

   Task & MAPPO\({}^{}\) & MADDPG & Q-MIX\({}^{}\) & Proposed \\ 
2s3z & \(\) & \(90.3\) & \(95.3\) & \(98.8\) \\
3s5z\_vs\_3s6z & \(63.3\) & \(18.9\) & \(82.8\) & \(\) \\
6h\_vs\_8z & \(85.9\) & \(68.0\) & \(9.4\) & \(\) \\   

Table 4: Performance comparison with MARL baseline algorithms on SMAC tasks. Performance of \(\) denoted algorithm is based on .

   Task & MAPPO\({}^{}\) & MADDPG\({}^{}\) & Q-MIX\({}^{}\) & Proposed \\  Spread & \(-149.26\) & \(-157.10\) & \(-154.70\) & \(-\) \\ Adversary & \(9.61\) & \(7.80\) & \(8.11\) & \(\) \\ Tag & \(13.78\) & \(6.65\) & \(\) & \(14.57\) \\   

Table 3: Performance comparison with MARL baseline algorithms on MPE tasks. Performance of \(\) denoted algorithm is based on .