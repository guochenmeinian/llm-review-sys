# VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use

**Yonatan Bitton*\({}^{1,2}\)****Hritik Bansal*\({}^{3}\)****Jack Hessel*\({}^{4}\)****Rulin Shao\({}^{5}\)****Wanrong Zhu\({}^{6}\)****Anas Awadalla\({}^{5}\)****Josh Gardner\({}^{5}\)****Rohan Taori\({}^{7}\)****Ludwig Schimdt\({}^{4,5,8}\)****

###### Abstract

We introduce VisIT-Bench (**Vis**ual **Ins**Tr**uction **Bench**mark), a benchmark for evaluating instruction-following vision-language models for real-world use. Our starting point is curating 70 "instruction families" that we envision instruction tuned vision-language models _should_ be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multi-modal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.

Figure 1: A sample from the 70 instruction families in VisIT-Bench representing tasks we envision instruction-following vision-language models _should_ be able to follow.

Introduction

A long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks . For textual tasks, several recent works [2; 3; 4; 5; 6; 7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8; 9; 10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, but - perhaps more-so - for creative, useful, and real-world queries that downstream users of language technologies are likely to make.

On the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., "how many apples are in this image?" or "What is this?" or "Write a poem in the style of Robert Frost about this scene.") a textual response is provided. Recent works like OpenFlamingo [11; 12], LLaVA  and others [14; 15; 16; 17; 18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2  and COCO captioning  are commonly used to assess performance, less is known about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.

To support quantitative evaluation for this setting, we present VisIT-Bench (**V**isual **Ins**Truction **Bench**mark), a dynamic benchmark consisting of 592 challenging vision-language instructions. Each instance contains an instruction, input image(s), a instruction-conditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 2). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks _"how can I cook this in a healthy way?"_. Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., _"write a poem..."_ or _"what should I bring if I were to visit here?"_).

We created VisIT-Bench to cover a wide array of "instruction families". Our starting point was a set of 70 "wish-list" tasks such as "home renovation" and "gardening tips" collected by the authors: each requiring varied high-level skills from recognition to complex reasoning (Figure 1). We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA)  and robust change captioning  into a chatbot-style format (this reformatting differs from prior work [14; 17; 13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.

We started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions - unlike prior work [23; 24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.

We conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench (SS4). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals

Figure 2: An example from VisIT-Bench displays an image, instruction, an instruction-conditioned caption based on the instruction, a GPT-4 suggested response, and a label confirming its accuracy. All 678 entries in VisIT-Bench have such labels, with 592 confirming accurate GPT-4 responses. These components aid in assessing multimodal chatbots and updating a dynamic leaderboard.

significant differences between models (e.g., that LLaVA-13b  is generally preferred to Panda ), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25; 26], which provide _relative_ "skill" rating estimates encoding the probability that model A will be preferred to model B; and 2) win rate versus our references, which provides an _absolute_ metric. The best model according to human judgement is LLaMA-Adapter-v2 , yet it only wins in a pairwise setting against the reference in 27.4% of cases.

Finally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs of model responses based on factors like correctness, relevance, and fluency. Using the instruction-conditioned caption and the instruction, GPT-4 determines the better response between two options, expediting iteration compared to human preferences. We explore _reference-free_ and _reference-backed_ versions of this metric. Compared to various metrics (BLEU-4 , ROUGE-L , METEOR , CIDEr , and BERTScore ), our evaluation aligns best with human preferences. For example, it achieves a 94% agreement rate in the cases where all five annotators agree. Figure 6 illustrates the process.

While it is difficult to _a priori_ envision all possible scenarios under which more performant multimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving vision-language models "in the wild." Table 1 presents a summary of our contributions in comparison to the recent works [32; 14; 17; 33; 34; 35] in the evaluation of multimodal chatbots. We publicly release VisIT-Bench data, code, and automatic metrics in https://visit-bench.github.io/.

## 2 VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark

VisIT-Bench was built to emulate real-world applications of multimodal models through image-text tasks, creating an extensive and practical benchmark. These tasks, or 'instruction families', are seen as key capabilities of a high-performing vision-and-language model. Although our selections are not exhaustive, they provide a broad basis for evaluating beyond academic benchmarks. We prioritize family coverage vs. number of instances-per-task. The final corpus, comprising 678 instances and 1,159 public images, can be found at VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45 newly assembled instruction families or reformatted from 25 existing datasets (see Table 5). Notably, 10 instruction families cater to _multi-image_ query scenarios (e.g., Figure 4).

### Data Collection

The authors of this work perform an initial annotation step of curating instruction families. For each instruction family not derived from an existing task (45 out of 70), we designate a name for the family (e.g., "Contextual Knowledge of Events") and identify an image-instruction pair that exemplifies the category, along with a sample response ("Martin Luther King Jr. is waving to acknowledge and greet the crowd of protesters [...]"). 10 sample familes are in Figure 1.

We work with crowdworkers at $18/hour to execute the annotation steps, as outlined in Figure 3: (1) taking the image/instruction example as a guiding seed task crowdworkers formulate a new instruction that examines the same instruction family ("instruction generation"); (2) crowdworkers create detailed image captions that describe the image and allow an entity, relying solely on this

    & MultiInstruct  & Oost  & InstructHILP  & MFT  & LVLM  & GAVE  & **VisIT-Bench** \\  Number of Models & 1 & 5 & 3 & 4 & 8 & 5 & 10 \\ Number of Skills Tested & 9 & 6 & 13 & 13 & 47 & 16 & 70 \\  Multiple-Images & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Video & ✗ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ Multi-Turn Conversations & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Multilingual Conversations & ✗ & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\  Instruction-conditioned Captions & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Chatbot: Visit Rephones & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\  Dataset-specific Evaluation & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Human Evaluation & ✗ & ✓ & ✗ & ✗ & ✓ & ✗ & ✓ \\ AutoGPT-4 Evaluation & ✗ & ✓ & ✗ & ✓ & ✓ & ✓ \\  Win-rates* & ✗ & ✓ & ✗ & ✓ & ✗ & ✓ \\ Elo Rating & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ \\   

Table 1: Comparison with related works for evaluating instruction-following vision-language models. Win-rates* refers to the model win-rates against a reference output/model.

text, to interpret and execute the given instruction successfully ("instruction-conditioned caption generation"); (3) crowdworkers assess the correctness of GPT-4's response to the instruction ("model output evaluation"). We further elaborate on these steps using human annotators below.

Re-formatting existing datasets.25/70 instruction families (corresponding to 25*10=250 instances) are re-formatted versions of existing vision-language tasks (See Appendix D for full list).This process involves re-formatting tasks into chatbot-style instruction/response versions. In re-formatting, we re-write instructions to retain the original task's goal while maintaining the original images, see Figure 4. These repurposed tasks are integrated into our data collection process, ensuring uniformity between the chatbot-style answers in the full VisIT-Bench instances and the reinterpreted tasks.

Instruction Generation.Here, annotators create a new instance from the same instruction family as a given example, along with an instruction and corresponding image. For instance, in Figure 3 (left), the instruction family is "Contextual Knowledge of Events", and the example instruction is _"Why is he waving? What happened in this event?"_ alongside an image of Martin Luther King, Jr. To collect images, annotators were instructed to use openverse for Creative Commons licened images.

Instruction-Conditioned Caption Generation.Annotators are provided with the image and instruction, and are tasked to construct a caption that is rich enough to allow an entity, solely receiving the text they author, to follow the instruction. These captions, termed _instruction-conditioned captions_, aid GPT-4 reference generation and text-only evaluation. See Figure 3 (middle) for an example: an annotator doesn't just mention the skittles and a spoon, but, given the query regarding specific colors, they indicate the exact colors in detail.

Model Output Evaluation.The goal of this stage is to gather human-validated reference chatbot responses for each multimodal instruction query. We initially obtain response candidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT4's prompt is: _"Consider an image depicted by: <caption>'. Now, briefly follow this instruction, and you can add a short explanation: <instruction>'. Response:_ This prompt is employed for both single and multiple image instances, with appropriate modifications for the latter. Then we verify each response with human annotators. If a response is marked incorrect, the annotator identifies whether the issue lies with the detail level of the instruction-conditioned captions or with GPT-4's response itself. For VisIT-Bench, we discard any case marked as incorrect for either reason. An example is given in Figure 3 (right), where GPT-4's

Figure 3: Data collection steps: (1) **Instruction Generation** from a seed task (left). (2) **Caption Generation** creates rich _instruction-conditioned captions_ for GPT-4 (middle). (3) **Model Evaluation** with human-validated GPT-4 responses (right). Top: rater instructions; bottom: outputs.

candidate reference response aims to answer a question about a chess position (which it does so incorrectly, and thus, the instance is discarded).

### Data Collection Annotation and Results

We conduct the data collection steps in Figure 3 using Amazon's Mechanical Turk (MTurk) platform. Prior to annotating, each MTurk worker passed a qualification test, which involved five to ten sample tasks designed to assess their ability to generate high-quality annotations. More detailed information about the execution process and full user interface examples can be found in Appendix C.

Annotation results are in Table 2. We assess our collection and filtration efficiency. For single-image tasks, our pipeline's yield was 91.5% from the original candidate set. However, the success rate dropped to 63.0% for multi-image tasks, accompanied by an uptick in issues either in the captions (6.0%) or GPT-4's responses (30.0%). This drop suggests that multi-image queries may pose a more difficult data collection challenge.

## 3 VisIT-Bench Analysis

We analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench.

Are instruction-conditioned captions necessary?To elucidate instruction-conditioned captions' role, we conduct an experiment on 150 single-image instances. We replace our instruction-conditioned captions with BLIP2  captions, a leading image captioning model, and feed them to GPT-4 for a chatbot response. See Figure 5 for this process. We manually evaluate whether the resulting output accurately followed the instructions. We find instruction-conditioned captions yielded 91% correct results, but with BLIP2 captions, success dropped to 31% (Table 2). This underscores the importance of instruction-conditioned captions in the construction of VisIT-Bench, and shows that the instances in our dataset are sophisticated enough such that most are not solvable by using a simple Socratic model  baseline of caption \(\) LLM.

  
**Metrics** & **Overall** & **Single** & **Multi** \\  GPT-4 Correct (\%) & 87.3 & 91.5 & 63.0 \\ Problem in Caption (\%) & 4.0 & 3.6 & 6.0 \\ Problem in GPT-4 (\%) & 7.7 & 3.8 & 30.0 \\   

Table 2: Human rating metrics for the VisIT-Bench dataset: overall, single-, and multi-image tasks.

Figure 4: An example multi-image task from VisIT-Bench, sourced from NLVR2 , tests visual reasoning. While NLVR2 uses a sentence, two images, and a binary answer, we add a zero-shot prompt, a instruction-conditioned caption per image, and a verified GPT-4 reply. This chatbot-style design aids automatic evaluation of future chatbot interactions.

What skills are required for VisIT-Bench?The full list of instruction families we cover are in Appendix Table 6. Following , for the VisIT-Bench instructions, we extract the most frequent root verbs and their direct nouns (a full plot is in Figure 10). The most common include: _'answer question'_, _'write story/poem'_, _'create title'_, etc. There's also a long-tail of diverse requests that demand comprehension, commonsense, and cross-modal understanding, e.g., _'identifying objects'_ to _need ingredient'_ to _'connect device'_. Additional examination reveals a range of underlying skills required ranging from _'emotion identification'_ to complex reasoning tasks such as _'paper folding'_.

What is contained in VisIT-Bench images?We detect all the COCO  objects present in the images from our dataset using Yolov5-L ; The most common detected objects in VisIT-Bench are "person" (\(\) 900 detections), chair, and car (\(\) 100). But, a long tail of rarer objects exists as well: full distribution in Appendix Figure 9. Overall, to perform well at VisIT-Bench, a model must account for a broad range of scenes and objects.

## 4 Experiments

We evaluate a range of state-of-the-art publicly accessible vision-and-language chatbots on the 592 instances in VisIT-Bench. In SS4.1, we provide the details of the instruction-following models in our benchmark. Following this, we collect the human preferences for pairwise model generations to achieve a human-guided Elo ranking and the win-rates against the reference of the models in SS4.2. We then develop automatic evaluation on VisIT-Bench in SS4.3, that can be scaled and improved given new and improved models. Finally, we establish the trustworthiness of our automatic evaluation method by performing agreement analysis with the human judgments in SS4.3

Figure 5: This experiment evaluates the value of instruction-conditioned captions in accurate instruction-following tasks. Given an image and instruction, GPT-4 generates responses using both a instruction-conditioned caption and a less detailed BLIP-2  caption. The latter’s imprecision leads to an error, emphasizing the need for detailed, task-specific captions.

Figure 6: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4 evaluator to compare two instruction-following models with an instruction and a instruction-conditioned caption. The instance is obtained from an existing dataset, WHOOPS! .

### Models

We evaluate LLaVA-13B , InstructBLIP-13B , MiniGPT4-7B , mPLUG-Owl-7B , LlamaAdapter-v2-7B , PandaGPT-13B , VisualChatGPT , Multimodal GPT , OpenFlamingo v1 [44; 11] and Otter v1 . For the execution-based VisualChatGPT , we implement a chat window for each sample, hold inputs and intermediate chains of thoughts and actions in memory, and feed the images and the instruction sequentially. For OpenFlamingo  and Otter , we feed the image(s) and the instruction in an interleaved format. For the others, we feed the image to the vision feature extractor and feed the instruction as a prompt to the text encoder.

### Human Evaluation

We collect 5K pairwise human preference judgements across an initial set of 6 models and the human-verified references. For 1K uniformly randomly sampled tuples of (query, model A, model B), we collect 5 crowdworker judgements each. Preferences are collected in a "forced choice" setting, annotators are instructed to decide based on accuracy, helpfulness, and detail. We provide the template for the human annotation process in Appendix Figure 15. We summarize the results with two metrics:

**Relative metric: Elo** We follow  and compute Elo ratings, treating each pairwise human judgement as a "match."The difference between the Elo ratings of two different models provides an estimate for the win probability when pitting model A vs. model B. More details are in Appendix E.

**Absolute metric: Win rate vs. reference.** We provide a win-rate vs. the human-verified reference. We use the 1.4K pairwise human judgments where one of A or B is the reference. We report the percent of cases where the human judge prefers the output from that model vs. the human-verified GPT-4 reference output. Because we do not allow for ties in our forced-choice setup, if the annotator believes the responses are of equal quaity, they choose one arbitrarily.

ResultsTable 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human Verified GPT-4 reference achieves a higher rating than all alternatives, validating the quality of our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an estimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and an estimated win rate of 93% against the lowest performing model in this setup, PandaGPT (Elo =786). This result can partly be explained by the training process of the underlying models: The improved performance of LLaVA (13B) might be attributed to its fine-tuning process, which utilized 150K instruction-tuning data that is rich in both diversity and quality. Interestingly, despite achieving a slightly lower Elo (the computation of which is based on _all_ head-to-head "matches", rather than just ones against the human reference), LlamaAdapter-v2 (7B) wins with the highest rate against the reference. However, the complexity of models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors influencing performance. We conduct an initial exploration of this result in Section 4.3.

    & **Model** & **Elo** & **matches** & **Win-rate vs. reference** (w/ \# ratings) \\  Single Image & Human Verified GPT-4 Reference & 1223 & 1439 & — \\  & LLaVA (13B) & 1085 & 1462 & 26.23\% (n=244) \\  & LlamaAdapter-v2 (7B) & 1061 & 1507 & **27.41\%** (n=259) \\  & mPLUG-Owl (7B) & 995 & 1345 & 14.95\% (n=214) \\  & InstructBLIP (13B) & 957 & 1315 & 12.37\% (n=194) \\  & MiniGPT-4 (7B) & 893 & 1513 & 14.72\% (n=299) \\  & PandaGPT (13B) & 786 & 1441 & 10.48\% (n=229) \\  Multiple Images & Human Verified GPT-4 Reference & 1193 & 210 & — \\  & mPLUG-Owl & 997 & 190 & 15.38\% (n=78) \\  & Otter v1 & 917 & 147 & 3.17\% (n=63) \\  & OpenFlamingo v1 & 893 & 171 & 4.35\% (n=69) \\   

Table 3: Human scores for models are displayed as ELO ratings and win-rates against the reference, summarizing 5.0K pairwise judgments. The ’matches’ column shows each model’s participation count, and ’win-rate vs. reference’ denotes its win rate against reference outputs.

### Automatic Evaluation and Leaderboard

Because it is costly to gather human pairwise preference judgements for new model submissions, to support faster model development, we seek an automatic evaluation procedure that produces high correlation with our human evaluation setup.

**Automatic evaluation metric candidates.** We consider several existing reference-backed evaluation metrics: BLEU-4 , ROUGE-L , METEOR , CIDEr , and BERTScore , we use the RoBERTa-Large english version , treating the human-verified GPT-4 reference as the evaluation reference. We additionally report two baseline metrics: random, which assigns a random score without accounting for the candidate, and length, which assigns a score equal to the number of non-whitespace tokens in the candidate. Beyond existing metrics and baselines, following the recent line of work utilizing API-accessed LLMs with a prompt for automatic evaluation [6; 47], we consider two GPT-41 backed evaluation metrics.

Specifically, we provide the LLM with: 1) a system prompt describing the desired evaluation behavior; 2) the instruction-conditioned caption for the image; 3) the instruction to be followed; and 4) two candidate generations dubbed "Response A" and "Response B". We also consider a reference-backed version where the human-verified reference is provided as well. We provide our prompts in Appendix F. To mitigate potential biases in "A" and "B" positioning, for all pairs of candidates, we run two queries covering both possible orderings. Our prompt encourages the model to think step-by-step so that its chain-of-thought process is made explicit [48; 49]. Despite strongly encouraging the model to select between the two references in a forced-choice setup, it sometimes

    & **Model** & **Elo** & **matches** & **Win vs. Reference** (w/ \# ratings) \\  Single Image & Human Verified GPT-4 Reference & 1370 & 5442 & - \\  & LLaVA (13B) & 1106 & 5446 & **17.81\%** (n=494) \\  & LlamaAdapter-v2 (7B) & 1082 & 5445 & 13.75\% (n=502) \\  & mPLUG-Owl (7B) & 1081 & 5452 & 15.29\% (n=497) \\  & InstructBLP (13B) & 1011 & 5444 & 13.73\% (n=517) \\  & Oter v1 (9B) & 991 & 5450 & 6.84\% (n=512) \\  & VisualGPT (Da Vinci 003) & 972 & 5445 & 1.52\% (n=527) \\  & MiniGPT-4 (7B) & 921 & 5442 & 3.26\% (n=522) \\  & OpenFlaming v1 (9B) & 877 & 5449 & 2.86\% (n=524) \\  & PandaGPT (13B) & 826 & 5441 & 2.63\% (n=533) \\  & Multimodal GPT & 763 & 5450 & 0.18\% (n=544) \\  Multiple Images & Human Verified GPT-4 Reference & 1192 & 180 & - \\  & mPLUG-Owl & 995 & 180 & 6.67\% (n=60) \\  & Oter v1 & 911 & 180 & 1.69\% (n=59) \\  & OpenFlamingo v1 & 902 & 180 & 1.67\% (n=60) \\   

Table 4: As of July 19th, 2023, reference-free Elo rankings summarize 12K matches between models, each with 2 GPT-4 queries. With the dynamic VisIT-Bench, rankings update as more models join the leaderboard and more head-to-head evaluations occur.

Figure 7: Correlations between evaluation metrics and human preferences ranked by performance, with our reference free evaluation (GPT-4-no-ref) showing the strongest alignment. Bottom: random chance (50%), top: upper performance bound.

refuses and outputs "tie" which we account for later. We call the reference-free version of this metric "GPT4-no-ref", and the reference-backed version of this metric "GPT4-ref".

Evaluating evaluation metrics.We measure the correlation between the candidate metrics and human judgements using a pairwise framework. Specifically, we use a subset of the 5K pairwise human judgements in SS 4.2. For 690 pairwise instances where both candidate instances are model-generated (rather than human-verified references), we have 5 pairwise judgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there is 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the percent of time the metric is able to accurately reconstruct a majority vote judgement from the 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs "tie" (this happens in 10-15% of cases overall) - for fair comparison with the other metrics in forced choice setting, we randomly choose one of the two options when GPT-4 reports a tie.

The results are in Figure 7, with GPT-4-no-ref best aligns with human correlation. The best performing metric is our newly proposed GPT-4 based metric, which accurately reconstructs majority-vote pairwise human judgments better than alternatives (\(p<.05\); binomial proportion CI nonoverlapping). For example, for instances where 5/5 annotators agree, GPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time, whereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately 80%/78%/70% of the time; A length baseline metric achieves only 60%. Notably, the reference-backed version of the newly proposed GPT-4 based metric achieves comparable (but slightly worse) performance compared to the reference-free version. Thus, we adopt the reference-free version, which additionally enables us to place the references themselves into the the Elo setup, because they are not used in the prompts.

**System-level Correlation.** We summarize the LLM's pairwise judgements using the same metrics as introduced in SS4.2, Elo ratings and win rate vs. reference, but instead of using a human judge, we use our reference-free GPT-4 based metric. The results are in Table 4. Notably, among the 7 systems for which we gathered human ratings for, the automatic metric produces the same ordering compared to human evaluation (\(=1.0\), \(p<.01\)).

**Shortcomings of proposed metric.** While the relative ranking of models produced by the automatic metric correlates strongly with the ranking produced by human judgements, the win rate vs. reference according to human judgement (Table 3) are higher overall compared to the win-rate vs. reference according to the automatic metric Table 4. One plausible explanation for this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely match its own response distribution.

**Per-category results.** In Figure 8, we plot the win-rate vs reference for the models across all the single-image instruction families. We find that there is no model that performs the best and

Figure 8: Reference-free assessment win rate vs. human-verified GPT4 response for each instruction category. Axes: win rate (Y), instruction categories (X). Categories are from-the-wild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks.

worst across all the instruction families. Thus, VisIT-Bench aids in highlighting the strengths and weaknesses of the instruction-following models along various real-world use-cases.

## 5 Related Work

Our work builds on prior multimodal image-text models and instruction-following benchmarks in machine learning. We provide a detailed overview of related work in SSB. Multi-model models for image-text understanding have recently emerged as powerful and useful methods for many image-language reasoning tasks [12; 15; 13; 50; 18; 14; 17; 11; 7]. Both language and multimodal models are often trained to follow language instruction, a paradigm known as "instruction following" [5; 16; 51; 14; 13; 17]. Despite the success of these approaches on existing vision-language datasets (GQA, Image Captioning [21; 52; 20]), there is no quality benchmarking dataset for multimodal instruction-following tasks that reliably replicates the way in which humans would interact with multimodal chatbots in the wild. The absence of benchmarking data impedes reliable progress assessments  and limits empirical evaluations of multimodal LLMs.

## 6 Conclusion

We present VisIT-Bench, a benchmark assessing multimodal chatbot skills. Going beyond prior efforts, VisIT-Bench's collection process centers potential real-world use cases, and 70 diverse instruction families encompassing a range of tasks from recognition to complex reasoning. Besides human-verified outputs, it features an Elo ranking aligning with human judgments. Our data reveals a performance gap between models and humans. Releasing data, code, and metrics, we aim for community engagement and believe VisIT-Bench will quantify progress and gaps in multimodal AI.

## 7 Limitations

Although VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate every possible vision-language task. We hope to add more categories of tasks over time. In terms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction and response. This does not encompass multi-turn interactions between users and chatbots, which presents a direction for future research. Our study focuses on image-text modalities. Future extensions could expand the scope to include other modalities like audio and video, enabling a more comprehensive evaluation. Additionally, while the dataset offers a wide variety of tasks, a larger number of examples per category could provide more depth. Finally, while our GPT-4 based metric correlates well with human judgement at instance and system level, we see some evidence that the GPT-4 based metric has a stronger preference for GPT-4 based generations compared to humans. Thus, models which train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our evaluation.