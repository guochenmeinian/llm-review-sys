# Online Adaptive Policy Selection in Time-Varying Systems: No-Regret via Contractive Perturbations

Yiheng Lin

California Institute of Technology

Pasadena, CA, USA

yihengl@caltech.edu

&James A. Preiss

California Institute of Technology

Pasadena, CA, USA

japreiss@caltech.edu

Emile Anand

California Institute of Technology

Pasadena, CA, USA

eanand@caltech.edu

&Yingying Li

University of Illinois Urbana-Champaign

Urbana, IL, USA

yl101@illinois.edu

Yisong Yue

California Institute of Technology

Pasadena, CA, USA

yyue@caltech.edu

&Adam Wierman

California Institute of Technology

Pasadena, CA, USA

adamw@caltech.edu

This work is supported by NSF Grants CNS-2146814, CPS-2136197, CNS-2106403, NGSDI-2105648, CCF-1918865, and Gift from Latitude AI, with additional support for Yiheng Lin provided by Amazon AI4Science Fellowship and PIMCO Graduate Fellowship in Data Science.

###### Abstract

We study online adaptive policy selection in systems with time-varying costs and dynamics. We develop the Gradient-based Adaptive Policy Selection (GAPS) algorithm together with a general analytical framework for online policy selection via online optimization. Under our proposed notion of contractive policy classes, we show that GAPS approximates the behavior of an ideal online gradient descent algorithm on the policy parameters while requiring less information and computation. When convexity holds, our algorithm is the first to achieve optimal policy regret. When convexity does not hold, we provide the first local regret bound for online policy selection. Our numerical experiments show that GAPS can adapt to changing environments more quickly than existing benchmarks.

## 1 Introduction

We study the problem of online adaptive policy selection for nonlinear time-varying discrete-time dynamical systems. The dynamics are given by \(x_{t+1}=g_{t}(x_{t},u_{t})\), where \(x_{t}\) is the state and \(u_{t}\) is the control input at time \(t\). The policy class is a time-varying mapping \(_{t}\) from the state \(x_{t}\) and a policy parameter \(_{t}\) to a control input \(u_{t}\). At every time step \(t\), the online policy incurs a stage cost \(c_{t}=f_{t}(x_{t},u_{t})\) that depends on the current state and control input. The goal of policy selection is to pick the parameter \(_{t}\) online to minimize the total stage costs over a finite horizon \(T\).

Online adaptive policy selection and general online control have received significant attention recently [1; 2; 3; 4; 5; 6; 7; 8] because many control tasks require running the policy on a single trajectory, as opposed to restarting the episode to evaluate a different policy from the same initial state. Adaptivity is also important when the dynamics and cost functions are time-varying. For example, in robotics, time-varying dynamics arise when we control an aircraft under changing wind conditions .

In this paper, we are interested in developing a unified framework that can leverage a broad suite of theoretical results from online optimization and efficiently translate them to online policy selection, where efficiency includes both preserving the tightness of the guarantees and computational considerations. A central issue is that, in online policy selection, the stage cost \(c_{t}\) depends on all previously selected parameters \((_{0},,_{t-1})\) via the state \(x_{t}\). Many prior works along this direction have addressed this issue by finite-memory reductions. This approach led to the first regret bound on online policy selection, but the bounds are not tight, the computational cost can be large, and the dynamics and policy classes studied are restrictive .

**Contributions.** We propose and analyze the algorithm Gradient-based Adaptive Policy Selection (GAPS, Algorithm 1) to address three limitations of existing results on online policy selection. First, under the assumption that \(c_{t}\) is a convex function of \((_{0},,_{t})\), prior work left a \( T\) regret gap between OCO and online policy selection. We close this gap by showing that GAPS achieves the optimal regret of \(O()\) (Theorem 3.3) 1. Second, many previous approaches require oracle access to the dynamics/costs and expensive resimulation from imaginary previous states. In contrast, GAPS only requires partial derivatives of the dynamics and costs along the visited trajectory, and computes \(O( T)\) matrix multiplications at each step. Third, the application of existing regret analysis frameworks is limited to specific policy classes and systems because they require \(c_{t}\) to be convex in \((_{0},,_{t})\). We address this limitation by showing the first local regret bound for online policy selection when the convexity does not hold. Specifically, GAPS achieves the local regret of \(O()\), where \(V\) is a measure of how much \((g_{t},f_{t},_{t})\) changes over the entire horizon.

To derive these performance guarantees, we develop a novel proof framework based on a general exponentially decaying, or "contractive", perturbation property (Definition 2.6) on the policy-induced closed-loop dynamics. This generalizes a key property of disturbance-action controllers [e.g. 1, 8] and includes other important policy classes such as model predictive control (MPC) [e.g. 11] and linear feedback controllers [e.g. 12]. Under this property, we prove an approximation error bound (Theorem 3.2), which shows that GAPS can mimic the update of an ideal online gradient descent (OGD) algorithm  that has oracle knowledge of how the current policy parameter \(_{t}\) would have performed if used exclusively over the whole trajectory. This error bound bridges online policy selection and online optimization, which means regret guarantees on OGD for online optimization can be transferred to GAPS for online policy selection.

In numerical experiments, we demonstrate that GAPS can adapt faster than an existing follow-the-leader-type baseline in MPC with imperfect disturbance predictions, and outperforms a strong optimal control baseline in a nonlinear system with non-i.i.d. disturbances. The source code for all experiments is published at https://www.github.com/jpreiss/adaptive_policy_selection.

**Related Work.** Our work is related to online control and adaptive-learning-based control , especially online control with adversarial disturbances and regret guarantees . For example, there is a rich literature on policy regret bounds for time-invariant dynamics . There is also a growing interest in algorithms for time-varying systems with small adaptive regret , dynamic regret , and competitive ratio . Many prior works study a specific policy class called disturbance-action controller (DAC) . When applied to linear dynamics \(g_{t}\) with convex cost functions \(f_{t}\), DAC renders the stage cost \(c_{t}\) a convex function in past policy parameters \((_{0},,_{t})\). Our work contributes to the literature by proposing a general contractive perturbation property that includes DAC as a special case, and showing local regret bounds that do not require \(c_{t}\) to be convex in \((_{0},,_{t})\). A recent work also handles nonconvex \(c_{t}\), but it studies an episodic setting and requires \(c_{t}\) to be "nearly convex", which holds under its policy class .

In addition to online control, this work is also related to online learning/optimization , especially online optimization with memory and/or switching costs, where the cost at each time step depends on past decisions. Specifically, our online adaptive policy selection problem is related to online optimization with memory . Our analysis for GAPS provides insight on how to handle indefinite memory when the impact of a past decision decays exponentially with time.

Our contractive perturbation property and the analytical framework based on this property are closely related to prior works on discrete-time incremental stability and contraction theory in nonlinear systems , as well as works that leverage such properties to derive guarantees for (online) controllers . In complicated systems, it may be hard to design policies that provably satisfy these properties. This motivates some recent works to study neural-based approaches that can learn a controller together with its certificate for contraction properties simultaneously [50; 51]. Our work contributes to this field by showing that, when the system satisfies the contractive perturbation property, one can leverage this property to bridge online policy selection with online optimization.

**Notation.** We use \([t_{1}:t_{2}]\) to denote the sequence \((t_{1},,t_{2})\), \(a_{t_{1}:t_{2}}\) to denote \((a_{t_{1}},a_{t_{1}+1},,a_{t_{2}})\) for \(t_{1} t_{2}\), and \(a_{}\) for \((a,,a)\) with \(a\) repeated \( 0\) times. We define \(q(x,Q)=x^{}Qx\). Symbols \(\) and \(\) denote the all-one and all-zero vectors/matrices respectively, with dimension implied by context. The Euclidean ball with center \(\) and radius \(R\) in \(^{n}\) is denoted by \(B_{n}(0,R)\). We let \(\|\|\) denote the (induced) Euclidean norm for vectors (matrices). The diameter of a set \(\) is \(():=_{x,y}\|x-y\|\). The projection onto the set \(\) is \(_{}(x)=_{y}\|y-x\|\).

## 2 Preliminaries

We consider online policy selection on a single trajectory. The setting is a discrete-time dynamical system with state \(x_{t}^{n}\) for time index \(t[0:T-1]\). At time step \(t\), the policy picks a control action \(u_{t}^{m}\), and the next state and the incurred cost are given by:

\[x_{t+1}=g_{t}(x_{t},u_{t}),c_{t} f_{t}(x_{t},u_{t}),\]

respectively, where \(g_{t}(,)\) is a time-varying dynamics function and \(f_{t}(,)\) is a time-varying stage cost. The goal is to minimize the total cost \(_{t=0}^{T-1}c_{t}\).

We consider parameterized time-varying policies of the form of \(u_{t}=_{t}(x_{t},_{t})\), where \(x_{t}\) is the current state at time step \(t\) and \(_{t}\) is the current policy parameter. \(\) is a closed convex subset of \(^{d}\). We assume the dynamics, cost, and policy functions \(\{g_{t},f_{t},_{t}\}_{t}\) are oblivious, meaning they are fixed before the game begins. The online policy selection algorithm optimizes the total cost by selecting \(_{t}\) sequentially. We illustrate how the policy parameter sequence \(_{0:T-1}\) affects the trajectory \(\{x_{t},u_{t}\}_{t}\) and per-step costs \(c_{0:T-1}\) in Figure 1. The online algorithm has access to the partial derivatives of the dynamics \(f_{t}\) and cost \(g_{t}\)_along the visited trajectory_, but does not have oracle access to the \(f_{t},g_{t}\) for arbitrary states and actions.

We provide two motivating examples for our setting. Appendix H contains more details and a third example. The first example is MPC with confidence coefficients, a generalization of .

**Example 2.1** (MPC with Confidence Coefficients).: _Consider a linear time-varying (LTV) system \(g_{t}(x_{t},u_{t})=A_{t}x_{t}+B_{t}u_{t}+w_{t}\), with time-varying costs \(f_{t}(x_{t},u_{t})=q(x_{t},Q_{t})+q(u_{t},R_{t})\). At time \(t\), the policy observes \(\{A_{t:t+k-1},B_{t:t+k-1},Q_{t:t+k-1},R_{t:t+k-1},w_{t:t+k-1|t}\}\), where \(w_{|t}\) is a (noisy) prediction of the future disturbance \(w_{}\). Then, \(_{t}(x_{t},_{t})\) commits the first entry of_

\[&*{arg\,min}_{u_{t+t+k-1|t}}_{=t} ^{t+k-1}f_{}(x_{|t},u_{|t})+q(x_{t+k|t},)\\ &\ x_{t|t}=x_{t}, x_{+1|t}=A_{}x_{ |t}+B_{}u_{|t}+_{t}^{[-t]}w_{|t}:\ t<t+k, \] (1)

_where \(_{t}=_{t}^{},_{t}^{},,_{t}^{[k-1 ]},=^{k}\) and \(\) is a fixed positive-definite matrix. Intuitively, \(_{t}^{[i]}\) represents our level of confidence in the disturbance prediction \(i\) steps into the future at time step \(t\), with entry \(1\) being fully confident and \(0\) being not confident at all._

The second example studies a nonlinear control model motivated by [12; 47].

**Example 2.2** (Linear Feedback Control in Nonlinear Systems).: _Consider a time-varying nonlinear control problem with dynamics \(g_{t}(x_{t},u_{t})=Ax_{t}+Bu_{t}+_{t}(x_{t},u_{t})\) and costs \(f_{t}(x_{t},u_{t})=q(x_{t},Q)+_{t}(x_{t},u_{t})\)._

Figure 1: Diagram of the causal relationships between states, policy parameters, control inputs, and costs.

\(q(u_{t},R)\). Here, the nonlinear residual \(_{t}\) comes from linearization and is assumed to be sufficiently small and Lipschitz. Inspired by , we construct an online policy based on the optimal controller \(u_{t}=-x_{t}\) for the linear-quadratic regulator \((A,B,Q,R)\). Specifically, we let \(_{t}(x_{t},_{t})=-K(_{t})x_{t}\) where \(K\) is a mapping from \(\) to \(^{n m}\) such that \(K(_{t})-\) is uniformly bounded._

### Policy Class and Performance Metrics

In our setting, the state \(x_{t}\) at time \(t\) is uniquely determined by the combination of 1) a state \(x_{}\) at a previous time \(<t\), and 2) the parameter sequence \(_{:t-1}\). Similarly, the cost at time \(t\) is uniquely determined by \(x_{}\) and \(_{:t}\). Since we use these properties often, we introduce the following notation.

**Definition 2.3** (Multi-Step Dynamics and Cost).: _The multi-step dynamics \(g_{t|}\) between two time steps \( t\) specifies the state \(x_{t}\) as a function of the previous state \(x_{}\) and previous policy parameters \(_{:t-1}\). It is defined recursively, with the base case \(g_{|}(x_{}) x_{}\) and the recursive case_

\[g_{t+1|}(x_{},_{:t})=g_{t}(z_{t},_{t}(z_{t},_{t})), \;\;t,\]

_in which \(z_{t} g_{t|}(x_{},_{:t-1})\).2 The multi-step cost \(f_{t|}\) specifies the cost \(c_{t}\) as function of \(x_{}\) and \(_{:t}\). It is defined as \(f_{t|}(x_{},_{:t}) f_{t}(z_{t},_{t}(z_{t}, _{t}))\)._

In this paper, we frequently compare the trajectory of our algorithm against the trajectory that would arise from applying a fixed parameter \(\) since time step \(0\), which we denote as \(_{t}() g_{t|0}(x_{0},_{ t})\) and \(_{t}()_{t}(_{t}(),)\). A related concept that is heavily used is the _surrogate cost_\(F_{t}\), which maps a single policy parameter to a real number.

**Definition 2.4** (Surrogate Cost).: _The surrogate cost function is defined as \(F_{t}() f_{t}(_{t}(),_{t}())\)._

Figure 1 shows the overall causal structure, from which these concepts follow.

To measure the performance of an online algorithm, we adopt the objective of _adaptive policy regret_, which has been used by [7; 52]. It is a stronger benchmark than the static policy regret [1; 6] and is more suited to time-varying environments. We use \(\{x_{t},u_{t},_{t}\}_{t}\) to denote the trajectory of the online algorithm throughout the paper. The adaptive policy regret \(R^{A}(T)\) is defined as the maximum difference between the cost of the online policy and the cost of the optimal fixed-parameter policy over any sub-interval of the whole horizon \(\), i.e.,

\[R^{A}(T)_{I=[t_{1}:t_{2}]}_{t I }f_{t}(x_{t},u_{t})-_{}_{t I}F_{t}().\] (2)

In contrast, the (static) policy regret defined in [1; 6] restricts the time interval \(I\) to be the whole horizon \(\). Thus, a bound on adaptive regret is strictly stronger than the same bound on static regret. Adaptive regret is particularly useful in time-varying environments like Examples 2.1 and 2.2 because an online algorithm must adapt quickly to compete against a comparator policy parameter that can change indefinitely with every time interval [32, Section 10.2].

In the general case when surrogate costs \(F_{0:T-1}\) are nonconvex, it is difficult (if not impossible) for online algorithms to achieve meaningful guarantees on classic regret metrics like \(R^{A}(T)\) or static policy regret because they do not have oracle optimization solvers or even the exact knowledge of the surrogate costs. Therefore, we introduce the metric of _local regret_, which bounds the sum of squared gradient norms over the whole horizon:

\[R^{L}(T)_{t=0}^{T-1} F_{t}(_{t})^{2}.\] (3)

Similar metrics have been adopted by previous works on online nonconvex optimization . Intuitively, \(R^{L}(T)\) measures how well the online agent chases the (changing) stationary point of the surrogate cost sequence \(F_{0:T-1}\). Since the surrogate cost functions are changing over time, the bound on \(R^{L}(T)\) will depend on how much the system \(\{g_{t},f_{t},_{t}\}_{t}\) changes over the whole horizon \(\). We defer the details to Section 3.3.

### Contractive Perturbation and Stability

In this section, we introduce two key properties needed for our sub-linear regret guarantees in adaptive online policy selection. We define both with respect to trajectories generated by "slowly" time-varying parameters, which are easier to analyze than arbitrary parameter sequences.

**Definition 2.5**.: _We denote the set of policy parameter sequences with \(\)-constrained step size by_

\[S_{}(t_{1}:t_{2})\{_{t_{1}:t_{2}}^{t_{2}-t_{ 1}+1}\|_{+1}-_{}\|,[t_{1} :t_{2}-1]\}.\]

The first property we require is an exponentially decaying, or "contractive", perturbation property of the closed-loop dynamics of the system with the policy class. We now formalize this property.

**Definition 2.6** (\(\)-Time-varying Contractive Perturbation).: _The \(\)-time-varying contractive perturbation property holds for \(R_{C}>0,C>0\), \((0,1)\), and \( 0\) if, for any \(_{:t-1} S_{}(:t-1)\),_

\[\|g_{t|}(x_{},_{:t-1})-g_{t|}(x_{}^{}, _{:t-1})\| C^{t-}\|x_{}-x_{}^{}\|\]

_holds for arbitrary \(x_{},x_{}^{} B_{n}(0,R_{C})\) and time steps \( t\)._

Intuitively, \(\)-time-varying contractive perturbation requires two trajectories starting from different states (in a bounded ball) to converge towards each other if they adopt the same slowly time-varying policy parameter sequence. We call the special case of \(=0\)_time-invariant contractive perturbation_, meaning the policy parameter is fixed. Although it may be difficult to verify the time-varying property directly since it allows the policy parameters to change, we show in Lemma 2.8 that time-invariant contractive perturbation implies that the time-varying version also holds for some small \(>0\).

The time-invariant contractive perturbation property is closely related to discrete-time incremental stability [e.g. 45] and contraction theory [e.g. 46], which have been studied in control theory. While some specific policies including DAC and MPC satisfy \(\)-time-varying contractive perturbation globally in linear systems, in other cases it is hard to verify. Our property is local and thus is easier to establish for broader applications in nonlinear systems (e.g., Example 2.2).

Besides contractive perturbation, another important property we need is the stability of the policy class, which requires \(_{0:T-1}\) can stabilize the system starting from the zero state as long as the policy parameter varies slowly. This property is stated formally below:

**Definition 2.7** (\(\)-Time-varying Stability).: _The \(\)-time-varying stability property holds for \(R_{S}>0\) and \( 0\) if, for any \(_{:t-1} S_{}(:t-1)\), \(\|g_{t|}(0,_{:t-1})\| R_{S}\) holds for any time steps \(t\)._

Intuitively, \(\)-time-varying stability guarantees that the policy class \(_{0:T-1}\) can achieve stability if the policy parameters \(_{0:T-1}\) vary slowly.3 Similarly to contractive perturbation, one only needs to verify time-invariant stability (i.e., \(=0\) and the policy parameter is fixed) to claim time-varying stability holds for some strictly positive \(\) (see Lemma 2.8). The reason we still use the time-varying contractive perturbation and stability in our assumptions is that they hold for \(=+\) in some cases, including DAC and MPC with confidence coefficients. Applying Lemma 2.8 for those systems will lead to a small, overly pessimistic \(\).

### Key Assumptions

We make two assumptions about the online policy selection problem to achieve regret guarantees.

**Assumption 2.1**.: _The dynamics \(g_{0:T-1}\), policies \(_{0:T-1}\), and costs \(f_{0:T-1}\) are differentiable at every time step and satisfy that, for any convex compact sets \(^{n},^{m}\), one can find Lipschitzness/smoothness constants (can depend on \(\) and \(\)) such that:_

1. _The dynamics_ \(g_{t}(x,u)\) _is_ \((L_{g,x},L_{g,u})\)_-Lipschitz and_ \((_{g,x},_{g,u})\)_-smooth in_ \((x,u)\) _on_ \(\)_._
2. _The policy function_ \(_{t}(x,)\) _is_ \((L_{,x},L_{,})\)_-Lipschitz and_ \((_{,x},_{,})\)_-smooth in_ \((x,)\) _on_ \(\)_._
3. _The stage cost function_ \(f_{t}(x,u)\) _is_ \((L_{f},L_{f})\)_-Lipschitz and_ \((_{f,x},_{f,u})\)_-smooth in_ \((x,u)\) _on_ \(\)_._

Assumption 2.1 is general because we only require the Lipschitzness/smoothness of \(g_{t}\) and \(f_{t}\) to hold for bounded states/actions within \(\) and \(\), where the coefficients may depend on \(\) and \(\). Similar assumptions are common in the literature of online control/optimization .

Our second assumption is on the contractive perturbation and the stability of the closed-loop dynamics induced by a slowly time-varying policy parameter sequence.

**Assumption 2.2**.: _Let \(\) denote the set of all possible dynamics/policy sequences \(\{g_{t},_{t}\}_{t}\) the environment/policy class may provide. For a fixed \(_{ 0}\), the \(\)-time-varying contractive perturbation (Definition 2.6) holds with \((R_{C},C,)\) for any sequence in \(\). The \(\)-time-varying stability(Definition 2.7) holds with \(R_{S}<R_{C}\) for any sequence in \(\). We assume that the initial state satisfies \(\|x_{0}\|<(R_{C}-R_{S})/C\). Further, we assume that if \(\{g,\}\) is the dynamics/policy at an intermediate time step of a sequence in \(\), then the time-invariant sequence \(\{g,\}_{ T}\) is also in \(\).4_

Note that Assumption 2.2 is on the joint properties of both the dynamical system and the policy class when composed together in a closed loop. The motivation is to generalize two key properties of linear systems under typical reasonable controllers: 1) the effect of past decisions on the current state decays exponentially fast, and 2) if the system is initialized near the origin, it remains near the origin. We generalize these properties via \(\)-time-varying contractive perturbation (Definition 2.6) and \(\)-time-varying stability (Definition 2.7) respectively. Although Assumption 2.2 may seem complicated to understand, it is less restrictive than the assumptions in the most closely related work (e.g., ) that focus on linear dynamics.

Compared to other settings where contractive perturbation holds globally , Assumption 2.2 only requires it to hold locally in a bounded ball \(B(0,R_{C})\), which becomes important in nonlinear settings. This brings a new challenge because we need to guarantee that the starting state stays within \(B(0,R_{C})\) whenever we apply this property in the proof. Therefore, in Assumption 2.2, we assume \(R_{C}>R_{S}+C\|x_{0}\|\). Similarly, to leverage the Lipschitzness/smoothness property, we require \( B(0,R_{x})\) where \(R_{x} C(R_{S}+C\|x_{0}\|)+R_{S}\) and \(=\{(x,) x,,\}\). Since the coefficients in Assumption 2.1 depend on \(\) and \(\), we will set \(=B(0,R_{x})\) and \(R_{x}=C(R_{S}+C\|x_{0}\|)+R_{S}\) by default when presenting these constants. The goal is to ensure that the controller never leaves the region where contractive perturbation applies, which is critical for our analysis and again generalizes properties found in the literature (e.g., Examples 2.1, 2.2, and H.1).

For some systems, verifying Assumption 2.2 is straightforward (e.g., Example 2.1). In other cases, we can rely on the following lemma, which can convert a time-invariant version of the property to general time-varying one. We defer its proof to Appendix C.

**Lemma 2.8**.: _Suppose Assumption 2.2 holds for \(=0\) and \((R_{C},C,,R_{S})\), which satisfies \(R_{G}>(C+1)R_{S}\). Suppose Assumption 2.1 also holds and let \( B(0,R_{x})\), where \(R_{x}=(C+1)^{2}R_{S}\). Then, Assumption 2.2 also holds for \(>0\), \((_{C},,,_{S})\), and \(x_{0}\) that satisfies \((_{C}-_{S})/C\). Here, \(_{S},_{C},\) are arbitrary constants that satisfies \(R_{S}<_{S}<_{C}<R_{C}/(C+1)\) and \(<<1\). The positive constants \(\) and \(\) are given detailed expressions in Appendix C._

**Remark 2.9**.: _Lemma 2.8 can also be useful when applied to some parameterized controllers for time-invariant nonlinear systems. For example, the well-known "computed torque control" feedback linearization controllers for robotic manipulators (see, e.g., ) renders the closed-loop dynamics exponentially stable about an equilibrium, and the feedback gains can be parameterized. Thus, it satisfies Assumption 2.2 in a neighborhood about the equilibrium, via Lemma 2.8. Even with time-invariant dynamics, the time-varying costs (such as tracking a trajectory determined online) provide a setting where selecting the policy parameters online can be beneficial._

## 3 Method and Theoretical Results

Our algorithm, Gradient-Based Adaptive Policy Selection (GAPS), is inspired by the classic online gradient descent (OGD) algorithm , with a novel approach for approximating the gradient of the surrogate stage cost \(F_{t}\). In the context of online optimization, OGD works as follows. At each time \(t\), the current stage cost describes how good the learner's current decision \(_{t}\) is. The learner updates its decision by taking a gradient step with respect to this cost. Mapping this intuition to online policy selection, the _ideal_ OGD update rule would be the following.

**Definition 3.1** (Ideal OGD Update).: _At time step \(t\), update \(_{t+1}=_{}(_{t}- F_{t}(_{t}))\)._

This is because the surrogate cost \(F_{t}\) (Definition 2.4) characterizes how good \(_{t}\) is for time \(t\) if we had applied \(_{t}\) from the start, i.e., without the impact of other historical policy parameters \(_{0:t-1}\). However, since the complexity of computing \( F_{t}\) exactly grows proportionally to \(t\), the ideal OGD becomes intractable when the horizon \(T\) is large.

As outlined in Algorithm 1, GAPS uses \(G_{t}\) to approximate \( F_{t}(_{t})\) efficiently. To see this, we compare the decompositions, with key differences highlighted in colored text:

\[ F_{t}(_{t})=_{b=0}^{t}}{_{t -b}}_{x_{0},(_{t})_{(t+1)}} G_{t}=_{b=0}^{\{B-1,t\}}}{ _{t-b}}_{x_{0},_{0,t}}.\] (4)

GAPS uses two techniques to efficiently approximate \( F_{t}(_{t})\). First, we _replace the ideal sequence \((_{t})_{(t+1)}\) by the actual sequence \(_{0:t}\)_. This enables computing gradients along the actual trajectory experienced by the online policy without re-simulating the trajectory under \(_{t}\). Second, we _truncate the whole historical dependence to at most \(B\) steps_. This bounds the memory used by GAPS. \(\)-time-varying contractive perturbation is the key to bound the bias of \(G_{t}\): Intuitively, in the first step, although \(_{}\) becomes more different with \(_{t}\) as \(\) decreases, its impact on \(f_{t|0}\) decays more quickly (exponentially); In the second step, the terms that we discard are exponentially small with respect to \(B\). We provide a rigorous bound of the bias in Theorem 3.2 and a proof outline in Appendix D.

```
0: Learning rate \(\), buffer length \(B\), initial \(_{0}\).
1:for\(t=0,,T-1\)do
2: Observe the current state \(x_{t}\).
3: Pick the control action \(u_{t}=_{t}(x_{t},_{t})\).
4: Incur the stage cost \(c_{t}=f_{t}(x_{t},u_{t})\).
5: Compute the approximated gradient: \(G_{t}=_{b=0}^{\{B-1,t\}}.}{_{ t-b}}|_{x_{0},_{0,t}}\).
6: Perform the update \(_{t+1}=_{}(_{t}- G_{t})\).
7:endfor ```

**Algorithm 1** Gradient-based Adaptive Policy Selection (GAPS)

Algorithm 1 presents GAPS in its simplest form. Although the expression of the partial derivatives contains \(_{0:t}\), the time- and space-efficient implementation of GAPS only requires to store \(B\) partial derivatives for \(B\) previous time steps. Details are given in Algorithm 2 in Appendix B.

Compared to many previous online control algorithms that take a reduction approach based on OCO with Memory, our algorithm can be much more computationally efficient (see Appendix I.4 for an empirical comparison). Specifically, these works [1; 3; 6] take a different _finite-memory reduction_ approach toward reducing the online control problem to OCO with Memory  by completely removing the dependence on policy parameters before time step \(t-B\) for a fixed memory length \(B\). In the finite-memory reduction, one must "imaginarily" reset the state at time \(t-B\) to be \(\) and then use the \(B\)-step truncated multi-step cost function \(f_{t|t-B}(,_{t-B:t})\) in the OGD with Memory algorithm . When applied to our setting, this is equivalent to replacing \(G_{t}\) in line 1 of Algorithm 1 by \(G^{}_{t}=_{b=0}^{B-1}}{_{t-b}} _{0,(_{t})_{(B+1)}}\). However, the estimator \(G^{}_{t}\) has limitations compared with \(G_{t}\) in GAPS. First, computing \(G^{}_{t}\) requires oracle access to the partial derivatives of the dynamics and cost functions for arbitrary state and actions. Second, even if those are available, \(G^{}_{t}\) is less computationally efficient than \(G_{t}\) in GAPS, especially when the policy is expensive to execute. Taking MPC (Example 2.1) as an example, computing \(G^{}_{t}\) at every time step requires solving \(B\) MPC optimization problems when re-simulating the system, where \(B=( T)\). In contrast, computing \(G_{t}\) in GAPS only requires solving one MPC optimization problem and \(O(B)\) matrix multiplications to update the partial derivatives. One may wonder how significant this improvement is, and if it affects the regret. To address this concern, we compare GAPS with Ideal OGD and OGD with Memory in the setting of MPC with confidence coefficients for a 2D double integrator. The simulation results show that GAPS achieve very similar regret with the two benchmarks, while the improvement on computation efficiency is significant (see Appendix I.4).

### Bounds on Truncation Error

We now present the first part of our main result, which states that the actual stage cost \(f_{t}(x_{t},u_{t})\) incurred by GAPS is close to the ideal surrogate cost \(F_{t}(_{t})\), and the approximated gradient \(G_{t}\) is close to the ideal gradient \( F_{t}(_{t})\). In other words, GAPS mimics the ideal OGD update (Definition 3.1).

**Theorem 3.2**.: _Suppose Assumptions 2.1 and 2.2 hold. Let \(\{(x_{t},u_{t},_{t})\}_{t}\) denote the trajectory of GAPS (Algorithm 1) with buffer size \(B\) and learning rate \(((1-))\). Then, we have_

\[|f_{t}(x_{t},u_{t})-F_{t}(_{t})|=O(1-)^{-3}\|G_{t}- F_{t}(_{t})\|=O(1-)^{-5}+(1-)^{-1}^{ B},\]

_where \(()\) and \(O()\) hide the dependence on the Lipschitz/smoothness constants defined in Assumption 2.1 and \(C\) in contractive perturbation -- see details in Appendix D.1._

We defer the proof of Theorem 3.2 to Appendix D.1. Note that this result does not require any convexity assumptions on the surrogate cost \(F_{t}\).

### Regret Bounds for GAPS: Convex Surrogate Cost

The second part of our main result studies the case when the surrogate cost \(F_{t}\) is a convex function. This assumption is explicitly required or satisfied by the policy classes and dynamical systems in many prior works on online control and online policy selection .

The error bounds in Theorem 3.2 can reduce the problem of GAPS' regret bound in control to the problem of OGD's regret bound in online optimization, where the following result is well known: When the surrogate cost functions \(F_{t}\) are convex, the ideal OGD update (Definition 3.1) achieves the regret bound \(_{t=0}^{T-1}F_{t}(_{t})-_{}_{t=0}^{T-1}F_{t}( )=O(),\) when the step size \(\) is of the order \(1/\). By taking the biases on the stage costs and the gradients into consideration, we derive the adaptive regret bound in Theorem 3.3. Besides the adaptive regret, one can use a similar reduction approach to "transfer" other regret guarantees for OGD in online optimization to GAPS in control. We include the derivation of a dynamic regret bound as an example in Appendix E.

**Theorem 3.3**.: _Under the same assumptions as Theorem 3.2, if we additionally assume \(F_{t}\) is convex for every time \(t\) and \(()\) is bounded by a constant \(D\), then GAPS achieves adaptive regret_

\[R^{A}(T)=O^{-1}+(1-)^{-5} T+(1-)^{-1}^{B}T+(1- )^{-10}^{3}T+(1-)^{-2}^{2B} T,\]

_where \(O()\) hides the same constants as in Theorem 3.2 and \(D\) -- see details in Appendix D.1._

We discuss how to choose the learning rate and the regret it achieves in the following corollary.

**Corollary 3.4**.: _Under the same assumptions as Theorem 3.3, suppose the horizon length \(T\) and the buffer length \(B(T)/(1/)\). If we set \(=(1-)^{}T^{-}\), then GAPS achieves adaptive regret \(R^{A}(T)=O((1-)^{-}T^{})\)._

We defer the proof of Theorem 3.3 to Appendix D.1. Compared to the (static) policy regret bounds of , our bound is tighter by a factor of \( T\). The key observation is that the impact of a past policy parameter \(_{t-b}\) on the current stage cost \(c_{t}\) decays exponentially with respect to \(b\) (see Appendix D for details). In comparison, the reduction-based approach first approximates \(c_{t}\) with \(_{t}\) that depends on \(_{t-B+1:t}\), and then applies general OCO with memory results on \(_{t}\). General OCO with memory cannot distinguish the different magnitudes of the contributions that \(_{t-B+1:t}\) make to \(_{t}\), which leads to the regret gap of \(B=O( T)\).

In the more restrictive setting of linear time-invariant dynamics with the DAC policy class, the results of a concurrent work  can also be used to close the \( T\) gap on static regret of online policy selection. In comparison, Theorem 3.3 considers more general time-varying dynamics and adopts the stronger metric of adaptive regret. As a practical matter, the follow-the-regularized-leader type of algorithm used by  is often (much) less computationally efficient than a gradient-based algorithm like GAPS. Nevertheless,  made distinct contributions by allowing the state space to be a general Banach space and providing a lower bound for OCO with unbounded memory.

### Regret Bounds for GAPS: Nonconvex Surrogate Cost

The third part of our main result studies the case when the surrogate cost \(F_{t}\) is nonconvex. Before presenting the result, we formally define the variation intensity that measures how much the system changes over the whole horizon.

**Definition 3.5** (Variation Intensity).: _Let \(\{g_{t},_{t},f_{t}\}_{t}\) be a sequence of dynamics/policy/cost functions that the environment provides. The variation intensity \(V\) of this sequence is defined as_

\[_{t=1}^{T-1}_{x,u} g_{t}(x,u)-g_{t- 1}(x,u)+_{x,}_{t}(x,)- _{t-1}(x,)+_{x,u} f_{t }(x,u)-f_{t-1}(x,u).\]Variation intensity is used as a measure of hardness for changing environments in the literature of online optimization that often appear in regret upper bounds (see  for an overview). Definition 3.5 generalizes one of the standard definitions to online policy selection. Using this definition, we present our main result for GAPS applied to nonconvex surrogate costs using the metric of local regret (3).

**Theorem 3.6**.: _Under the same assumptions as Theorem 3.2, if we additionally assume that \(=^{d}\) for some integer \(d\), then GAPS satisfies local regret_

\[R^{L}(T)=O}+}+ T}{(1-)^{2}}+T}{(1-)^{13}}+  T}{(1-)^{5}},\]

_where \(O()\) hides the same constants as in Theorem 3.2 -- see details in Appendix F._

We discuss how to choose the learning rate and the regret it achieves in the following corollary.

**Corollary 3.7**.: _Under the same assumptions as Theorem 3.6, suppose the horizon length \(T\) and the buffer length \(B(T)/(1/)\). If we set \(=(1-)^{}(1+V)^{}T^{-}\), GAPS achieves local regret \(R^{L}(T)=O((1-)^{-}(1+V)^{}T^{})\)._

We defer the proof of Theorem 3.6 to Appendix F. Note that the local regret will be sublinear in \(T\) if the variation intensity \(V=o(T)\). To derive the local regret guarantee in Theorem 3.6, we address additional challenges compared to the convex case. First, we derive a local regret guarantee for OGD in online nonconvex optimization. We cannot directly apply results from the literature because they do not use ordinary OGD, and it is difficult to apply algorithms like Follow-the-Perturbed-Leader [e.g. 58] to online policy selection due to constraints on information and step size. Then, to transfer the regret bound from online optimization to online policy selection, we show how to convert the measure of variation defined on \(F_{0:T-1}\) to our variation intensity \(V\) defined on \(\{g_{t},_{t},f_{t}\}_{t}\).

A limitation of Theorem 3.6 is that we need to assume \(\) is a whole Euclidean space so that GAPS will not converge to a point at the boundary of \(\) that is not a stationary point. Example 2.2 and Appendix H.3 show that one can re-parameterize the policy class to satisfy this assumption in some cases. Relaxing this assumption is our future work.

## 4 Numerical Experiments

In this section we compare GAPS to strong baseline algorithms in settings based on Examples 2.1 and 2.2. Details are deferred to Appendix I due to space limitations. Appendix I also includes a third experiment comparing GAPS to a bandit-based algorithm for selecting the planning horizon in MPC, and a computation time comparison between GAPS and the alternative gradient approximation of .

MPC confidence parameter.We compare GAPS to the follow-the-leader-type method of  for tuning a scalar confidence parameter in model-predictive control with noisy disturbance predictions. The setting is close to Example 2.1 but restricted to satisfy the conditions of the theoretical guarantees in . We consider the scalar system \(x_{t+1}=2x_{t}+u_{t}+w_{t}\) under non-stochastic disturbances \(w_{t}\) with the cost \(f_{t}(x_{t},u_{t})=x_{t}^{2}+u_{t}^{2}\). For \(t=0\) to \(100\), the predictions of \(w_{t}\) are corrupted by a large amount of noise. After \(t>100\), the prediction noise is instantly reduced by a factor of \(100\). In this setup, an ideal algorithm should learn to decrease confidence level at first to account for the noise, but then increase to \( 1\) when the predictions become accurate.

Figure 2: Comparing GAPS and baseline  for online adaptation of a confidence parameter for MPC with disturbance predictions. _Left:_ Confidence parameter. _Right:_ Per-step cost. Shaded bands show \(10\%\)-\(90\%\) quantile range over randomized disturbance properties. See body for details.

Figure 2 shows the values of the confidence parameter \(\) and the per-timestep cost generated by each algorithm. Both methods are initialized to \(=1\). The method of  rapidly adjusts to an appropriate confidence level at first, while GAPS adjusts more slowly but eventually reaches the same value. However, when the accuracy changes, GAPS adapts more quickly and obtains lower costs towards the end of the simulation. In other words, we see that GAPS behaves essentially like an instance of Ideal OGD with constant step size, which is consistent with our theoretical results (Theorem 3.2).

Linear controller of nonlinear time-varying system.We apply GAPS to tune the gain parameters of a linear feedback controller in a nonlinear inverted pendulum system. Every \(100\) seconds, the pendulum mass changes. The system reflects the smooth nonlinear dynamics and nonconvex surrogate costs in Example 2.2, although it differs in other details (see Appendices H.3 and I.2). We compare GAPS to a strong and dynamic baseline that deploys the infinite-horizon linear-quadratic regulator (LQR) optimal controller for the linearized dynamics at each mass. We simulate two disturbances: 1) i.i.d. Gaussian, and 2) Ornstein-Uhlenbeck random walk.

Figure 2(a) shows the controller parameters tuned by GAPS, along with the baseline LQR-optimal gains, for each disturbance type. The derivative gain \(k_{d}\) closely follows LQR for i.i.d. disturbances but diverges for random-walk disturbances, where LQR is no longer optimal. This is reflected in the cumulative cost difference between GAPS and LQR, shown in Figure 2(b). GAPS nearly matches LQR under i.i.d. disturbances, but significantly outperforms it when the disturbance is a random walk. The results show that GAPS can both 1) adapt to step changes in dynamics on a single trajectory almost as quickly as the comparator that benefits from knowledge of the near-optimal analytic solution, and 2) outperform the comparator in more general settings where the analytic solution no longer applies.

## 5 Conclusion and Future Directions

In this paper, we study the problem of online adaptive policy selection under a general contractive perturbation property. We propose GAPS, which can be implemented more efficiently and with less information than existing algorithms. Under convexity assumptions, we show that GAPS achieves adaptive policy regret of \(O()\), which closes the \( T\) gap between online control and OCO left open by previous results. When convexity does not hold, we show that GAPS achieves local regret of \(O()\), where \(V\) is the variation intensity of the time-varying system. This is the first local regret bound on online policy selection attained without any convexity assumptions on the surrogate cost functions. Our numerical simulations demonstrate the effectiveness of GAPS, especially for fast adaptation in time-varying settings.

Our work motivates interesting future research directions. For example, a limitation is that GAPS assumes _all_ policy parameters can stabilize the system and satisfy contractive perturbation. A recent work on online policy selection relaxed this assumption by a bandit-based algorithm but requires \(\) to be a finite set . An interesting future direction to study is what regret guarantees can be achieved when \(\) is a continuous parameter set and not all of the candidate policies satisfy these assumptions.

Figure 3: Comparing GAPS and LQR baseline in nonlinear inverted pendulum system. Shaded bands show \( 1\) standard deviation over the randomness of the disturbances. See body for details.