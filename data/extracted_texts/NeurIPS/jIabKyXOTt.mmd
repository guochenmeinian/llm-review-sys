# Sparsity-Agnostic Linear Bandits

with Adaptive Adversaries

 Tianyuan Jin

Department of Electrical and Computer Engineering

National University of Singapore

Singapore

tianyuan@nus.edu.sg

&Kyoungseok Jang

Dipartimento di Informatica

Universita degli Studi di Milano

Milano, Italy

ksajks@gmail.com

&Nicolo Cesa-Bianchi

Universita degli Studi di Milano

Politecnico di Milano

Milano, Italy

nicolo.cesa-bianchi@unimi.it

###### Abstract

We study stochastic linear bandits where, in each round, the learner receives a set of actions (i.e., feature vectors), from which it chooses an element and obtains a stochastic reward. The expected reward is a fixed but unknown linear function of the chosen action. We study _sparse_ regret bounds, that depend on the number \(S\) of non-zero coefficients in the linear reward function. Previous works focused on the case where \(S\) is known, or the action sets satisfy additional assumptions. In this work, we obtain the first sparse regret bounds that hold when \(S\) is unknown and the action sets are adversarially generated. Our techniques combine online to confidence set conversions with a novel randomized model selection approach over a hierarchy of nested confidence sets. When \(S\) is known, our analysis recovers state-of-the-art bounds for adversarial action sets. We also show that a variant of our approach, using Exp3 to dynamically select the confidence sets, can be used to improve the empirical performance of stochastic linear bandits while enjoying a regret bound with optimal dependence on the time horizon.

## 1 Introduction

\(K\)-armed bandits are a basic model of sequential decision-making in which a learner sequentially chooses which arm to pull in a set of \(K\) arms. After each pull, the learner only observes the reward returned by the chosen arm. After \(T\) pulls, the learner must obtain a total reward as close as possible to the reward obtained by always pulling the overall best arm. Linear bandits extend \(K\)-armed bandits to a setting in which arms belong to a \(d\)-dimensional feature space. In each round \(t\) of a linear bandit problem, the learner receives an action set \(_{t}^{d}\) from the environment, chooses an arm \(A_{t}_{t}\) based on the past observations, and then receives a reward \(X_{t}\). In this work, we consider the stochastic setting in which rewards are defined by \(X_{t}=_{*},A_{t}+_{t}\), where \(_{*}^{d}\) is a fixed latent parameter and \(_{t}\) is zero-mean independent noise. In linear bandits, the learner's goal is to minimize the difference between the total reward obtained by pulling in each round \(t\) the arm \(a_{t}\) maximizing \(_{*},a\) and the total reward obtained by the learner.

In stochastic linear bandits, the regret after \(T\) rounds is known to be of order \(d\) up to logarithmic factors. The linear dependence on the number \(d\) of features implies that the learner is better off by ignoring features corresponding to negligible components of the latent target vector \(_{*}\). Hence,one would like to design algorithms that depend on the number \(S d\) of relevant features without requiring any preliminary knowledge on \(_{*}\). This is captured by the setting of sparse linear bandits, where \(_{*}\) is assumed to have only \(0<S d\) nonzero components.

In the sparse setting, Lattimore and Szepesvari [17, Section 24.3] show that a regret of \(\) is unavoidable for any algorithm, even with knowledge of \(S\). When \(S\) is known, this lower bound is matched (up to log factors) by an algorithm of Abbasi-Yadkori et al.  who, under the same assumptions and for the same algorithm, also prove an instance-dependent regret bound of \(\). Here \(\) is the minimum gap, over all \(T\) rounds, between the expected reward of the optimal arm and that of any suboptimal arm. In this work we focus on the sparsity-agnostic setting, i.e., when \(S\) is unknown. Fewer results are known for this case, and all of them rely on additional assumptions on the action set, or assumptions on the sparsity structure. For example, if the action set is stochastically generated, Oh et al.  prove a \(S\) sparsity-agnostic regret bound. More recently, Dai et al.  showed a sparsity-agnostic bound \(S^{2}+S\) when the action set is fixed and equal to the unit sphere. In a model selection setting, Cutkosky et al.  prove a \(S^{2}\) sparsity-agnostic regret bound for adversarial action sets, but under an additional nestedness assumption: \((_{*})_{i} 0\) for \(i=1,,S\). Surprisingly, no bounds improving on the \(d\) regret of the OFUL algorithm  in the sparsity-agnostic case are known that avoid additional assumptions on the sparsity structure or on the action set generation.

**Main contributions.** Here is the summary of our main contributions. All the proofs of our results can be found in the appendix.

\(\) We introduce a randomized sparsity-agnostic linear bandit algorithm, SparseLinUCB, achieving regret \(S\) with no assumptions on the sparsity structure (e.g., nestedness) or on the action set (which may be controlled by an adaptive adversary). When \(S=o()\), our bound is strictly better than the OFUL bound \(d\).

\(\) Our analysis of SparseLinUCB simultaneously guarantees an instance-dependent regret bound \(\{d^{2},S^{2}d\}/\), where \(\) is the smallest suboptimality gap over the \(T\) rounds.

\(\) If the sparsity level is known, our algorithm recovers the optimal bound \(()\).

\(\) We also introduce AdaLinUCB, a variant of SparseLinUCB that uses Exp3 to learn the probability distribution over a hierarchy of confidence sets in stochastic linear bandits. Unlike previous works, which only showed a \(T^{2/3}\) regret bound for similar approaches, AdaLinUCB has a \(\) regret bound. In experiments on synthetic data, AdaLinUCB performs better than OFUL.

**Technical challenges.** Recall that the arm chosen in each round by OFUL is

\[A_{t}=*{argmax}_{a_{t}} a,_{t}+}\|a\|_{V_{t-1}^{-1}}\] (1.1)

where \(_{t}\) is the regularized least-squares estimate of \(_{*}\), \(V_{t-1}=I+_{s<t}A_{s}A_{s}^{}\) is the regularized covariance matrix of past actions, and \(}\) is the radius of the confidence set

\[\{^{d}:\|-_{t-1}\|_{V_{t-1}}^{2} _{t}\}\;.\] (1.2)

The squared radius \(_{t}=O(d t)\) is such that \(_{*}\) belongs to (1.2) with high probability simultaneously for all \(t 1\). Our approach, instead, builds on the online to confidence set conversion technique of Abbasi-Yadkori et al. , where they show how to design a different confidence set for \(_{*}\) based on the predictions of an arbitrary algorithm for online linear regression, such that the squared radius of the confidence set is roughly equal to the regret bound of the algorithm. Using the algorithm SeqSEW for sparse online linear regression , whose regret bound is \(O(S T)\), they obtain the optimal regret \(\) for sparse linear bandits. Unfortunately, this result requires knowing \(S\) to properly set the radius of the confidence set. Our strategy SparseLinUCB (Algorithm 1) bypasses this problem by running the online to confidence set conversion technique over a hierarchy of nested confidence sets with radii \(_{i}=2^{i} T\) for \(i=1,,n=( d)\). The framework of Abbasi-Yadkori et al.  guarantees that, for any sparsity value \(S[d]\), there is a critical radius \(_{o}=O(S T)\) such that, with high probability, \(_{*}\) lies in the set with radius \(_{i}\) for all \(i o\). SparseLinUCB randomizes the choice of the index \(i\) of the confidence radius \(_{i}\), used for selecting the action at time \(t\). Ifthe random index \(I_{t}\) is such that \(I_{t} o\), then we can bound the regret incurred at step \(t\) using standard techniques . By choosing \((I_{t}=i)\) proportional to \(2^{-i}\), we make sure that larger confidence sets (delivering suboptimal regret bounds) are chosen with exponentially small probability. If \(I_{t}<o\), then \(_{*}\) is not guaranteed to lie in the confidence set of radius \(_{I_{t}}\) with high probability. Our main technical contribution is to show that the regret summed over these bad rounds is bounded by \(\), where \(Q=(I_{t} o)\). The proof of this bound requires showing that the regret in a bad round \(t\) (when \(I_{t}<o\)) can be bounded by \(}\|A_{t}^{o}\|_{V_{t-1}^{-1}}\). Proving that \(\|A_{t}^{o}\|_{V_{t-1}^{-1}}\) shrinks fast enough uses the fact that \( V_{t}\) grows fast enough due to the exploration in the good rounds \(t\) (when \(I_{t} o\)). This is done by a carefully designed peeling technique that partitions \([T]\) in blocks based on the value of \( V_{t}\).

To extend our analysis of SparseLinUCB and obtain instance-dependent regret bound, we apply the techniques of Abbasi-Yadkori et al.  to show that the regret over the good rounds is bounded by \(d^{2}/\). The regret over a bad round \(t\) is controlled by \((_{o}/)\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2}\) and--using techniques similar to the instance-independent analysis--we bound the regret summed over all bad rounds with \(Sd/(Q)\).

Given that SparseLinUCB uses a fixed probability of order \(2^{-i}\) to choose its confidence radius \(_{i}\), it is tempting to explore adaptive probability assignments, that increase the probability of a confidence set proportionally to the rewards obtained by the actions that were selected based on that set. Algorithm AdaLinUCB (see Algorithm 2) is a variant of SparseLinUCB using Exp3  to assign probabilities to confidence sets. The analysis of AdaLinUCB combines--in a non-trivial way--the analysis of Exp3 (including a forced exploration term \(q\)) with that of SparseLinUCB. Although the resulting regret bound does not improve on OFUL, our algorithm provides a new principled solution to the problem of tuning the radius in (1.1). Experiments show that SparseLinUCB can perform better than OFUL.

   Reference &  Sparsity \\ Agnostic \\  & 
 Adaptive \\ Adversary \\  & Expected Regret & Assumptions \\  Abbasi-Yadkori et al.  & ✓ & ✓ & \((\{d,d^{2}/\})\) & - \\  Abbasi-Yadkori et al.  & ✗ & ✓ & \((\{,dS/\})\) & - \\  Pacchiano et al.  & ✓ & ✗ & \((S^{2})\) & Nested, i.i.d. actions \\  Cutkosky et al.  & ✓ & ✓ & \((S^{2})\) & Nested \\  Pacchiano et al.  & ✓ & ✓ & \((S^{2})\) & Nested \\  & ✓ & ✗ & \((S^{2}d^{2}/)\) & Nested, i.i.d. actions \\  Lattimore et al.  & ✗ & ✗ & \((S)\) & \(_{t}[-1,1]\) \\  Sivakumar et al.  & ✗ & ✓ & \((S)\) & Smoothed adversary \\  Hao et al.  & ✗ & ✗ & \(()\) & \(}}}\) set spans \(^{d}\) \\  Oh et al.  & ✓ & ✗ & \((S)\) & Compatibility \\  Dai et al.  & ✓ & ✗ & \((S^{2}+S)\) & Action set is unit sphere \\  Lower bound  & ✗ & ✓ & \(\) & - \\ 
**This paper** & ✓ & ✓ & \(\{S,\{d^{2},S^{2}d\} \}\) & - \\
**This paper** & ✗ & ✓ & \(\) & - \\   

Table 1: Comparison with other sparse linear bandit works. \(S[d]\) is the sparsity level and \(\) is the suboptimality gap (3.3). The nested assumption refers to \((_{*})_{i} 0\) for \(i=1,,S\). The minimum signal and the compatibility condition refer to assumptions on the distribution of the action set and on the smallest value of the non-zero elements in \(_{*}\). Smoothed adversary refers to adversarially selected action sets with added Gaussian noise. The regret bounds listed in  are high-probability bounds: with high probability, the regret is of the same order as the bound in the table.

### Additional related work

**Sparse linear bandits.** With the goal of obtaining sparsity-agnostic regret bounds, different types of assumptions on the action set have been considered in the past. Starting from the \((S)\) regret upper bound of , where the action set is assumed fixed and equal to the hypercube, some works considered stochastic action sets and proved regret bounds depending on spectral parameters of the action distribution, such as the minimum eigenvalue of the covariance matrix [9; 15; 16; 20]. Others assumed a stochastic action set with strong properties, such as compatibility conditions or margin conditions [3; 6; 7; 19; 22]. As far as we know, there has been no research on adaptive adversarial action sets after .

**Model selection.** Sparse linear bandits can be naturally viewed as a bandit model selection problem. For example, Ghosh et al.  establish a regret bound of \((+d^{2}/^{4.65})\) for a fixed action set, where \(\) is the minimum absolute value of the nonzero components of \(_{*}\). Quite a bit of work has been devoted to sparse regret bounds in the nested setting. With i.i.d. and fixed-size actions sets, Foster et al.  achieve a regret bound of order \(S^{1/3}T^{2/3}/^{3}\) in the nested setting, where \(\) is the smallest eigenvalue of the covariance matrix of \(_{t}\). Under the same assumption on the action set, Pacchiano et al. [24; 23] obtain a regret bound of \((S^{2})\). For adversarial action sets, Cutkosky et al.  obtain \((S^{2})\) in the nested setting. When actions are sampled i.i.d., Cutkosky et al.  and Pacchiano et al.  obtain a regret bound of \((S^{2})\) for nested settings. They also obtain simultaneous instance-dependent bounds, in particular, Pacchiano et al.  achieve \((Sd)^{2}/\). Compared to the instance-dependent regret bound, our results are more general, as we allow the action set to be adaptively chosen by an adversary and do not require the nested assumption.

**Parameter tuning.** Although the theoretical analysis of OFUL only holds for \(_{t}=O(d t)\), smaller choices of the radius in (1.2) are known to perform better in practice. Our design of SparseLinUCB and AdaLinUCB borrows ideas from the parameter tuning setting, which is typically addressed using a set of base algorithms and a randomized master algorithm that adaptively changes the probability of selecting each base algorithm [21; 23; 24]. In particular, AdaLinUCB builds on , where they show that running Exp3 as the master algorithm over instances of OFUL with different radii has a better empirical performance than Thompson Sampling and UCB. Yet, they only show a regret bound of \(T^{2/3}\) when the action set is drawn i.i.d. in each round (they also prove a bound of order \(\), but only under additional assumptions on the best model). This is consistent with the results of Pacchiano et al. , who also obtained a regret of the same order using Exp3 as master algorithm.

## 2 Problem definition

In linear bandits, a learner and an adversary interact over \(T\) rounds. In each round \(t=1,,T\):

1. The adversary chooses an arm set \(_{t}^{d}\);
2. The learner choose an arm \(A_{t}_{t}\);
3. The learner obtains a reward \(X_{t}\).

We assume the adversary is adaptive, i.e., \(_{t}\) can depend in an arbitrary way on the (possibly randomized) past choices of the learner. The reward in each round \(t\) satisfies

\[X_{t}= A_{t},_{*}+_{t}\.\] (2.1)

Here \(_{*}^{d}\) is a fixed and unknown target vector and \(\{_{t}\}_{t[T]}\) are independent conditionally 1-subgaussian random variables. We also assume \(\|_{*}\|_{2} 1\) and \(\|a\|_{2} 1\) for all \(a_{t}\) and all \(t[T]\).1 The regret of a strategy over \(T\) rounds is defined as the difference between the reward obtained by the optimal policy, always choosing the best arm in \(_{t}\), and the reward obtained by the strategy choosing arm \(A_{t}_{t}\) for \(t[T]\),

\[R_{T}=_{t=1}^{T}_{a_{t}} a,_{*}- _{t=1}^{T} A_{t},_{*}.\]In the sparse setting, we would like to devise strategies whose regret depends on

\[S=\|_{*}\|_{0}=_{i=1}^{d}\{_{i} 0\}\]

corresponding to the number of nonzero components of \(_{*}\).

### Online to confidence set conversions

Establishing a confidence set including the target vector with high probability is at the core of linear bandit algorithms, and our approach for designing a sparsity-agnostic algorithm is based on a result by Abbasi-Yadkori et al. . They show how to construct a confidence set for OFUL  based on the predictions of a generic algorithm for online linear regression, a sequential decision-making setting defined as follows. For \(t=1,,T\):

1. The adversary privately chooses input \(A_{t}^{d}\) and outcome \(X_{t}\);
2. The learner observes \(A_{t}\) and chooses prediction \(_{t}\);
3. The adversary reveals \(X_{t}\) and the learner suffers loss \((_{t}-X_{t})^{2}\).

The learner's goal in online linear regression is to minimize the following notion of regret against any comparator \(^{d}\)

\[_{T}()=_{t=1}^{T}(X_{t}-_{t})^{2}-_{t=1}^{T}(X_{t }- A_{t},)^{2}.\]

The confidence set proposed by Abbasi-Yadkori et al.  is established by the following result.

**Lemma 2.1** (Abbasi-Yadkori et al. [2, Corollary 2]).: _Let \((0,1/4]\) and \(\|_{*}\|_{2} 1\). Assume a sequence \((A_{t},X_{t})\}_{t[T]}\), where \(X_{t}\) satisfies (2.1) for all \(t[T]\), is fed to an online linear regression algorithm \(\) generating predictions \(_{t}}_{t[T]}\). Then \(( t[T]\,:\,_{*}_{t})\), where_

\[_{t} =\{^{d}:\|\|_{2}^{2}+_{s=1}^{t- 1}(_{s}- A_{s},)^{2}(,_{* })\}\] (2.2) \[(,_{*}) =2+2B_{T}(_{*})+32(+( _{*})}}{})\]

_and \(B_{T}(_{*})\) is an upper bound on the regret \(_{T}(_{*})\) of \(\). When understood from the context, we will abuse the notation and denote the best radius in hindsight by \(():=(,_{*})\) and the regret bound by \(B_{T}\)._

Gerchinovitz  designed an algorithm, SeqSEW, for _sparse_ linear regression that bounds \(_{T}()\) in terms of \(\|\|_{0}\) simultaneously for all comparators \(^{d}\). Below here, we state his bound in the formulation of Lattimore and Szepesvari .

**Lemma 2.2** (Lattimore and Szepesvari [17, Theorem 23.6]).: _Assume \(_{t[T]}\|A_{t}\|_{2} 1\) and \(_{t[T]}|X_{t}| 1\). There exists a universal constant \(c\) such that algorithm SeqSEW achieves, for any \(^{d}\),_

\[_{T}() B_{T}():=c\|\|_{0}\{(e+T^{1/2})+C_{T }(1+}{\|\|_{0}})\}\]

_where \(C_{T}=2+_{2}(e+T^{1/2})\)._

Using the confidence set (2.2) with \(\) set to SeqSEW, Abbasi-Yadkori et al.  achieved the minimax optimal regret bound of \(()\). However, to construct \(_{t}\), the learner must know \(()\), which depends on the unknown sparsity level \(S=\|_{*}\|_{0}\) through \(B_{T}\).

## 3 A multi-level sparse linear bandit algorithm

In this section, we introduce our main algorithm, SparseLinUCB, whose pseudo-code is shown in Algorithm 1. The algorithm, which runs SeqSEW as base algorithm \(\), uses a hierarchy of confidence sets of increasing radius. In each round \(t=1,,T\), after receiving the action set \(_{t}\), the algorithm draws the index \(I_{t}\) of the confidence set for time \(t\) by sampling from the distribution \(_{n}:=\{(q_{1},,q_{n})^{n}:_{i=1}^{n}q_{i }=1\}\). Then the algorithm plays the action \(A_{t}\) using the confidence set \(_{t}^{I_{t}}:=\{^{d}:\|-_ {t-1}\|_{V_{t-1}}^{2} 2^{I_{t}} T\}\) (where a larger \(I_{t}\) implies a larger radius, and thus more exploration). Following the online to confidence set approach, upon receiving the reward \(X_{t}\), the algorithm feeds the pair \((A_{t},X_{t})\) to SeqSEW and uses the prediction \(_{t}\) to update the regularized least squares estimate \(_{t}\).

Let \(_{i}=2^{i} T\) for all \(i\) and set \(n\) as

\[n=_{2}^{d}:\|\|_{2} 1 }(1/T,)}{ T}\] (3.1)

where \((,)\) is defined in Lemma 2.1 for \(=\).

One can check that \(n=( d)\) (when \(\|\|_{0}=d\)), which gives \(_{n}=(d T)\). Our bounds depend on the following quantity, which defines the index of the smallest "safe" confidence set (i.e., the smallest \(i[n]\) such that \(_{*}_{t}^{i}\) for all \(t[T]\)),

\[o:=*{argmin}_{i[n]}\{(1/T)_{i}\}\] (3.2)

The choice of our confidence set (Line 4 in Algorithm 1) is justified by the following result, which implies that \(o\) is safe.

**Lemma 3.1**.: _For \(_{t}\) defined in (2.2), we have that \(_{t}_{t}^{o}\) for all \(t[T]\)._

As we use SeqSEW as base algorithm \(\), \(_{o}=O(S T)\). Our main result is an upper bound on the regret of SparseLinUCB.

**Theorem 3.2**.: _The expected regret of SparseLinUCB run with the number of models \(n\) in (3.1) and a distribution \(=\{q_{s}\}_{s[n]}\) satisfies_

\[[R_{T}]=O(( T)_{s o}Tq_{s}}+( T) )\]

_where \(Q=_{s o}q_{s}\)._

If the sparsity level \(S\) is indeed known, then \(o\) in (3.2) can be computed and we get the following bound, which is tight up to log factors .

**Corollary 3.3**.: _Assume that the sparsity level \(S\) is known and choose the number of models \(n>o\) and the distribution \(\{q_{s}\}_{s[n]}\) with \(q_{o}=1\), where \(o\) is set as in (3.2). Then, the expected regret of \(\) is \([R_{T}]=O T\)._

**An instance-dependent bound.**\(\) also enjoys an instance-dependent regret bound comparable to that of OFUL. Let \(\) be the minimum gap between the optimal arm and any suboptimal arms over all rounds,

\[=_{t[T]}_{a_{t} A_{t}^{*}} A_{t }^{*}-a,_{*},\] (3.3)

where \(A_{t}^{*}=_{a_{t}} a,_{*}\) is the optimal arm for round \(t\).

**Theorem 3.4**.: _The expected regret of \(\) run with the number of models \(n\) in (3.1), a distribution \(=\{q_{s}\}_{s[n]}\) and using \(\) as base algorithm satisfies_

\[[R_{T}]=O(}{}( T)^{2})\]

_where \(Q=_{s o}q_{s}\)._

**Sparsity-agnostic tuning of randomization.** Next, we look at a specific choice of **q**. Fix \(C 1\) and let

\[q_{s}=\{C^{2}2^{-s}&C^{2}2^{-s}<1\\ &.\] (3.4)

where \(>0\) is chosen so to normalize the probabilities. It is easy to verify that for any \(C 1\),

\[_{s[n]}C^{2}2^{-s}<1}q_{s} 1\]

implying that \(\) can be chosen in \(\). Combining Theorem 3.2 and 3.4, we obtain the following corollary providing a hybrid distribution-free and distribution-dependent bound.

**Corollary 3.5**.: _Pick any \(C 1\). Let the number of models \(n\) as in (3.1) and \(=\{q_{s}\}_{s[n]}\) be chosen as in (3.4). Then the expected regret of \(\) is_

\[[R_{T}]=(\{C,S/C} ,\,d^{2},S^{2}d/C^{2}}}{}\})\]

For \(C=1\) the above bound is \((S)\), which is tight up to the factor \(\) due to the lower bound of \(()\). However, as mentioned in Lattimore and Szepesvari [17, Section 23.5], no algorithm can enjoy the regret of \(()\) simultaneously for all possible sparsity levels \(S\). While our worst-case regret bound improves with a smaller \(S\), the problem-dependent regret bound scales at least as \((d^{2}/) T\), which is independent of \(S\). This raises an interesting question: could the problem-dependent bound also benefit from sparsity? Even with a very small probability \(p\) of choosing radius \(_{n}\), the expected number of steps using \(_{n}\) would be \(pT\). The results in  demonstrate that running the OFUL algorithm with \(_{n}\) over \(pT\) steps results in a regret of \((d^{2}/)\). One simple way is to decrease the frequency of selecting radius \(_{n}\). However, selecting \(_{n}\) less than \(d^{2}/^{2}\) times may prevent the algorithm from obtaining a good enough estimate of \(_{*}\) in certain settings.

**Remark 3.6**.: _At first glance, it may seem straightforward to select \(C\) in Corollary 3.5, as setting \(C=\) yields a regret of \((d^{2}/)\) without apparent trade-offs. However, the trade-off lies in balancing the instance-dependent and worst-case regret bounds. Opting for \(C=d\) indeed yields an instance-dependent bound of \((d^{2}/)\). However, this comes at the expense of the worst-case bound, which remains \((d)\), negating any advantages derived from the sparsity assumption \(S d\)._

If the sparsity level \(S\) is indeed known, then \(o\) in (3.2) can be computed and we get the following bound.

**Corollary 3.7**.: _Assume that the sparsity level \(S\) is known and choose \(\{q_{s}\}_{s[n]}\) with \(q_{o}=1\), where \(o\) is set as in (3.2). Then, the expected regret of \(\) is \([R_{T}]=\)._

We note that by setting \(q_{o}=1\) in Theorem 3.4, the regret bound becomes \((d^{2}/)\). This result, as detailed in Theorem 3.4, arises from the parameter \(q_{n}>0\). However, in this case, \(q_{n}=0\), which allows us to achieve a more favorable regret bound.

```
1:Input:\(T\), \(>0\), \(q(0,1]\)
2:Initialization: Let \(S_{i,0}=0\) for all \(i[n]\), \(V_{0}=I\), \(_{0}=(0,,0)\)
3:for\(t=1,2,,T\)do
4: Receive action set \(_{t}\) and draw a Bernoulli random variable \(Z_{t}\) with \((Z_{t}=1)=q\)
5:if\(Z_{t}=1\)then
6: Choose optimistic action \(A_{t}=*{argmax}_{a_{t}}( a,_{t-1}+\|a\|_{V_{t-1}^{-1}} T})\)
7: Receive reward \(X_{t}\);
8:else
9: Draw \(I_{t}\) from the distribution \(P_{t,i}=)}{_{j=1}^{n}( S _{t-1,j})}\) for \(i[n]\);
10: Choose action \(A_{t}=*{argmax}_{a_{t}}( a,_{t-1}+\|a\|_{V_{t-1}^{-1}}} T})\)
11: Receive reward \(X_{t}\);
12: Compute \(S_{t,j}=S_{t-1,j}-\{I_{t}=j\}(2-X_{t})/4}{P_{t,j}}\) for \(j[n]\);
13:endif
14:\(V_{t}=V_{t-1}+A_{t}A_{t}^{}\);
15: Feed \((A_{t},X_{t})\) to SeqSEW and obtain prediction \(_{t}\);
16: Compute regularized least squares estimate \(_{t}=*{argmin}_{^{d}}( \|\|_{2}^{2}+_{s=1}^{t}(_{s}-,A_{s} )^{2})\);
17:endfor ```

**Algorithm 2** AdaLinUCB

## 4 Adaptive model selection for stochastic linear bandits

SparseLinUCB is also designed to handle adaptive adversarial action sets. A crucial parameter of SparseLinUCB is \(\{q_{s}\}_{s[n]}\), the distribution from which the radius of the confidence set is drawn. It is a natural question whether there exists an algorithm that adaptively updates this distribution based on the observed rewards. In this section we introduce AdaLinUCB (Algorithm 2), which runs Exp3 to dynamically adjust the distribution used by SparseLinUCB.

AdaLinUCB takes as input a forced exploration term \(q\) and the learning rate \(\) for Exp3. Similarly to SparseLinUCB, AdaLinUCB designs confidence sets of various radii, but its selection method differs in two aspects. First, with probability \(q\), the algorithm performs exploration based on the confidence set with the largest radius. With probability \(1-q\), the algorithm instead draws the action based on Exp3. The distribution \(P_{t}\) used by Exp3 at round \(t\) is based on exponential weights applied to the total estimated loss, denoted by \(S_{t}\) (for technical reasons, we translate losses into rewards). The algorithm then draws \(I_{i}\) from \(P_{t}\) and selects the action \(A_{t}\) based on the confidence set with radius \(2^{I_{t}} T\). Finally, reward \(X_{t}\) is observed and the pair \((A_{t},X_{t})\) is fed to SeqSEW. The prediction \(_{t}\) returned by SeqSEW is used to update the regularized least squares estimate \(_{t}\).

The following theorem states the theoretical regret upper bound of AdaLinUCB.

**Theorem 4.1**.: _If the random independent noise \(_{t}\) in (2.1) satisfies \(_{t}[-1,1]\) for all \(t[T]\), then the regret of AdaLinUCB run with \(=\) for \(n\) in (3.1) and \(q(0,1]\) satisfies_

\[[R_{T}] (q}+4)/q}) }{d})+1}+O\] \[=(\{,\} {dT})\.\]

Although AdaLinUCB dynamically adjusts the distribution used by SparseLinUCB and may achieve better empirical performance, its regret bound is no better than that of SparseLinUCB. The issue is that the action chosen by AdaLinUCB in Line 10 does not ensure enough exploration to control the regret. Consequently, the algorithm needs to choose the optimistic action in Line 6 with constant probability \(q\). SparseLinUCB has a similar parameter, \(Q\), that bounds from the above the probability of choosing the optimistic action. The key difference is that \(Q\) can be optimized for an unknown \(S\) by carefully selecting the distribution \(=\{q_{s}\}_{s 1}\), whereas the parameter \(q\) does not provide a similar flexibility.

## 5 Model selection experiments

In this section we describe some experiments we performed on synthetic data to verify whether AdaLinUCB could outperform OFUL in a model selection task. We also test the empirical performance of SparseLinUCB on the same data (additional details on all the algorithms and the experimental setting are in Appendix E). The data for our model selection experiments are generated using targets \(_{s}\) with different sparsity levels, as we know that sparsity affects the radius of the optimal confidence set. On the other hand, since no efficient implementation of SeqSEw is known (17, Section 23.5), we cannot implement the online to confidence set approach as described in  to capture sparsity. Instead, we run SparseLinUCB and AdaLinUCB with \(_{t}=X_{t}\) for all \(t[T]\), which--due to the form of our confidence sets--amounts to running the algorithms over multiple instances of OFUL with different choices of radius \(_{i}\) for \(i[n]\).

We run SparseLinUCB and AdaLinUCB with \(_{i}=_{i,t}=2^{i} t\) (a mildly time-dependent choice) for \(i=0,1,,_{2}d\). We also include \(_{0}=0\) corresponding to the greedy strategy \(A_{t}=*{argmax}_{a_{t}} a,_{t-1}\). The suffix _Unif indicates \(\{q_{s}\}_{s[n]}\) set to \((,,)\). The suffix _Theory indicates \(q_{s}=(2^{-s})\) for \(s=0,,n\) as prescribed by (3.4). Finally, we included SparseLinUCB_Known using \(q_{i}=1\{i=o\}\) to test the performance when the optimal index \(o\) (for the given \(S\)) is known in advance (see Corollary 3.3). We run our experiments with a fixed set of random actions, \(_{t}=\) for all \(t[T]\), where \(||=30\) and \(\) is a set of vectors drawn i.i.d. from the unit sphere in \(^{16}\). The target vector \(_{s}\) is a \(S\)-sparse (\(S=1,2,4,8,16\)) vector whose non-zero coordinates are drawn from the unit sphere in \(^{S}\). The noise \(_{t}\) is drawn i.i.d. from the uniform distribution over \([-1,1]\). Each curve is an average over \(20\) repetitions with \(T=10^{4}\) where, in each repetition, we draw fresh instances of \(\) and \(_{*}\).

As our implementations are not sparsity-aware, we cannot expect the regret to strongly depend on the sparsity level. Indeed, only the regret of SparseLinUCB_Known (which is tuned to the sparsity \(S\)) is significantly affected by sparsity. The theory-driven choice of \(\{q_{s}\}_{s[n]}\) (SparseLinUCB_Theory) performs better than the uniform assignment (SparseLinUCB_Unif), and is in the same ballpark as OFUL. On the other hand, AdaLinUCB_Unif and AdaLinUCB_Theory outperform all the competitors, including OFUL. This provides evidence that using Exp3 for adaptive model selection may significantly boost the empirical performance of stochastic linear bandits.

## 6 Limitations and open problems

Unlike previous works, we prove sparsity-agnostic regret bounds with no assumptions on the action sets or on \(_{*}\) (other than boundedness of \(\|_{*}\|\) and \(\|a\|\) for \(a_{t}\), which are rather standard assumptions). For AdaLinUCB, however, we do require boundedness of the noise \(_{t}\) (instead of just subgaussianity). We conjecture this requirement could be dropped at the expense of a further \( T\) factor in the regret. Finally, for efficiency reasons our implementations are not designed to capture sparsity. Hence our experiments are limited to testing the impact of model selection.

Our work leaves some open problems:

1. Proving a lower bound on the regret of sparse linear bandits when the sparsity level is unknown to the learner would be important. Citing again (17, Section 23.5), no algorithm can enjoy regret \(()\) simultaneously for all sparsity levels \(S\). However, we do not know whether the known lower bound \(()\) can be strengthened to \((S)\) in the agnostic case.
2. Our instance-dependent regret bound is of order \(\{S^{2},d\}\). It would be interesting to prove an upper bound that improves on the factor \(d^{2}/\), or a lower bound showing that \(d^{2}/\) cannot be improved on.
3. Our bound on the regret of AdaLinUCB looks pessimistic due to the presence of the constant exploration probability \(q\). It would be interesting to prove a bound that more closely reflects the good empirical performance of this algorithm.