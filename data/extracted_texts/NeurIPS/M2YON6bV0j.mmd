# Spectral structure learning for clinical time series

Ivan Lerner

Anita Burgun

Francis Bach

SIERRA, Inria Paris, F-75015 Paris, France

###### Abstract

We develop and evaluate a structure learning algorithm for clinical time series. Clinical time series are multivariate time series observed in multiple patients and irregularly sampled, challenging existing structure learning algorithms. We assume that our times series are realizations of StructGP, a \(k\)-dimensional multi-output or multi-task stationary Gaussian process (GP), with independent patients sharing the same covariance function. StructGP encodes ordered conditional relations between time series, represented in a directed acyclic graph. We implement an adapted NOTEARS algorithm, which based on a differentiable definition of acyclicity, recovers the graph by solving a series of continuous optimization problems. Simulation results show that up to mean degree 3 and 20 tasks, we reach a median recall of 0.93% [IQR, 0.86, 0.97] while keeping a median precision of 0.71% [0.57-0.84], for recovering directed edges. We further show that the regularization path is key to identifying the graph. With StructGP, we proposed a model of time series dependencies, that flexibly adapt to different time series regularity, while enabling us to learn these dependencies from observations.

## 1 Introduction

Structure learning is the task of learning the dependency structure of either time-independent variables or, in this study, time series . The structure of dependency is usually represented as a directed acyclic graph (DAG), in which nodes represent variables, and links between nodes represent relations between variables. These links encode conditional or marginal independence relations , or under certain strong additional assumptions (e.g., no hidden cofounders), have a causal interpretation and the structure can be used for _causal modeling_[1, Chapter 6].

Structure learning algorithms are typically classified into constraint-based methods and score-based methods . Score-based methods assume a parametric model in which the parameters support the graph, and then search for the graph that best fits the data . The main limitation of this approach is that the acyclicity of the graph requires an iterative search over graph structures that satisfy the constraint. And that the number of acyclic graphs grows superexponentially with the number of nodes in the graph . Formulating the acyclicity of a directed acyclic graph (DAG) as a differentiable function of its adjacency matrix allowed Zheng et al.  to frame the problem as a continuous optimization problem. Following this line of work, DYNOTEARS  uses the acyclicity constraint to identify the structure of linear structural vector autoregressive models (SVAR) .

In this study, we aim to develop a structure learning algorithm for clinical time series collected from Electronic Health Records (EHRs). EHRs time series are collections of irregularly sampled multivariate time series, collected in multiple patients. To learn graphical models of dependence between those, we work in continuous time and develop StructGP, a structured multi-task Gaussian process (GP) .

Methods

### Structured Gaussian process

We consider the \(k\)-dimensional multi-output Gaussian process \((t)\) defined as the filtration by \((t)\) of white noise \((t)\):

\[(t)=(*)(t),\]

where \((t)\) is a sparse \(k k\) lower triangular matrix-valued impulse response function, and \((t)\) is a \(k\)-dimensional white noise vector. Taking the Fourier transform of \((t)\), we find \(()\) a non-stationary complex white noise multi-output process [11, p. 418]:

\[()=}()(),\] (1)

where \(}()\) is the Fourier transform of \((t)\), and \(()\) complex independent white noise processes. In addition, the covariance of \(()\) or _spectral density_ is:

\[((),(^{}))= }()}^{T}()&=^{}\\ 0&^{}.\]

Thus, it identify \(}()\) as the Cholesky factor of the covariance matrix of \(()\). Given Equation 6 in Appendix A.3, we find that the sparsity pattern of \(\) parameterizes ordered conditional relations between time series:

\[_{vu}(t)=0, t\] \[ }_{vu}()=0,(-,)\] \[ Z_{u}\!\!\! Z_{v}_{\{1,2,...,u-1\}}\] \[ _{uv|C}()=_{uv}()-_{uC }()_{CC}^{-1}()_{Cv}()=0, (-,)\] \[ Y_{u}\!\!\! Y_{v}_{\{1,2,...,u-1\}},\] \[_{uv|C}_{u}_{v}C=_{\{1,2,...,u-1\}}.\]

We therefore parameterize \((t)\) as follows:

\[(t)=(-)(t),\] (2)

where \(_{vu}(t)=-}{_{vu}}\), \(\) is a sparse lower triangular matrix up to permutation, \(\) is a positive square matrix, \(\) is the identity matrix, and \(\) the Hadamard product. The support of \(\) can then be interpreted as the adjacency matrix of a directed acyclic graph (DAG) \(\) that encodes ordered conditional independence relation:

\[_{vu} 0\] (3) \[ Y_{u}}}{{}}Y_{v}\] \[ Y_{u}\!\!\! Y_{v}_{\{1,2,...,u-1\}}.\]

And the distribution \(P()\) satisfy the Markov factorization property with respect to the graph \(\)[4, Theorem 2.49]:

\[P()=_{u=1}^{k}PY_{u}(Y_{u}).\] (4)

### Learning the graph

As our time series are irregularly sampled from multiple patients we switch here to a set-based indexing, thanks to the marginalization properties of GP. Our data is then a collection of \(n\) scalar observations \(y\) from \(r\) individuals and \(k\) tasks, \(\{y():(,,^{+})\}\). Each observation is indexed by the input vector \(\), a triplet composed of the patient index \(i\), task index \(j\), and time \(t\), such that \(=(i,j,t)\), with \(i\{1,...,r\}\), \(j\{1,...,k\}\) and \(t^{+}\). We observe multiple independent patients, and the intra-patient covariance of the process for patient \(i\) can be written:

\[[(i,u,t),(i,v,t^{})]=(_{u}* _{v}^{T})(t-t^{}).\]With classical GP, the set of free parameters \(=\{,\}\) is learned by maximizing \( p(|,)\), the marginal likelihood of the training observations \(\) given inputs \(\). With structured GP, we follow the NOTEARS algorithm to learn the graph, order of tasks, and sparsity pattern  and impose an acyclicity constraint on \(\). NOTEARS leverages the trace of the matrix exponential of the adjacency matrix as a differentiable acyclicity constraint. Our objective is therefore to solve the constrained optimization problem below:

\[^{}= *{argmin}_{}- p(,, )+\|\|_{1}\] (5) \[}\ (( ))-k=0,\]

where \(\) is the penalty strength.

The above is solved by dual ascent following the augmented Lagrangian method , such that the constrained problem is equivalent to solving a series of unconstrained problems, the primal and the dual. The primal is the penalized objective function augmented with a quadratic penalty term, and is solved with a proximal gradient method (see Appendix A.6.5), the dual is solved by gradient ascent (see Appendix A.6.4). Finally, we find by grid search \(_{*}\), the sparsity penalty that minimizes an equivalent of the Akaike information criterion (AIC): \(=2\|\|_{0}-2(,,)\). Grid search is conducted from \(_{max}\) to \(_{min}\) on a log-scale, with warm-start. Because solving the augmented Lagrangian problem to small error is computationally expensive, and leads to numerical instability when \(\) becomes large, we choose to only loosely solve it, typically with a large tolerance for the acyclicity constraint (\(=0.1\), see Appendix A.6.4). Thus, to ensure adgness, we apply a hard-threshold operation, i.e. we mask elements in \(\) lower than the minimal threshold that ensures adgness.

### Simulation study

We empirically assess the accuracy of the algorithm to identify a graph from observations through a series of simulations. For each simulation, we sample a graph, the covariance parameters \(\), and observations from the sampled prior GP. The definition of the GP follows that of section 2.1, with the additional constraint that lengthscales parameters are tied for each task and exponentiated ( \(_{vu}=(_{v})\) for all \(v\{1,2,,k\},\ u\{1,2,,k\}\)). We then fit the model using the overall algorithm from section 2.2 (including the grid search), and compare the predicted graph \(}\) with the true simulated graph \(\). The comparison is made with the structural hamming distance (SHD). SHD is the "edit distance" of graphs, it counts the number of edge modifications (insertion, deletion, inversion) necessary to transform a predicted graph into the true simulated graph. We also compare the root mean square error (RMSE) between the predicted \(}\) and true (simulated) \(\) parameters. Each simulation is repeated 100 times and we report the average metric along with its bootstrapped 95% confidence intervals. For comparison purposes, we also report the same metrics for a random graph from the same distribution as simulated.

In all simulations, the support of \(\) is sampled from a random (Erdos-Renyi) graph in which the sparsity level is controlled by the mean degree of the graph \(md\). \(\) parameters are uniformly sampled in \([-2,-0.5][0.5,2]\). \(_{u}\) parameters are uniformly sampled in \([-0.5,0.5]\). The observation times, \(t\), are uniformly sampled in \(\). The observation noise level is fixed at \(=0.01\) and given as oracle when learning. We first report results from one simulation 'TOY', a toy model with 4 tasks that illustrate how to compute counterfactual trajectories and how we recover the graph from observations. We then report 3 experiments each varying specific parameters (see Table 1), while the number of observations per task is fixed at (\(n_{k}=10\)). Code available at gitlab.

 
**Experiment** & **Number of tasks** & **Mean degree** & **Grid search steps** & **Number of patients** \\  & \(k\) & \(md\) & \(n_{}\) & \(r\) \\  TOY & 4 & 2 & 256 & \(50\) \\ EXP1 & 10 & 2 & 50 & \([1,,100]\) \\ EXP2 & 10 & 3 & \([2,,512]\) & 50 \\ EXP3 & \([2,,20]\) & \(\) & 50 & 50 \\  

Table 1: Summary of simulation parameters

## 3 Results

### Toy model

In Figure 0(a), we show a sampled random DAG of mean degree 2 for 4 tasks, with 4 links. A link can be interpreted as the presence of a direct or indirect effect. It corresponds to the following output scale parameters of the impulse response function \((t)\):

\[-=1&0&0&0\\ 0&1&0&0\\ -1.18&-1.45&1&0\\ 0.82&0&0.57&1.\]

This graph encodes the following ordered independence relations:

\[Y_{4}\ -7.499886pt-7.499886pt-7.499886pt -7.499886pt-7.499886 pt-7.499886pt-7.499886pt-7.499886pt-7.499886pt-7.499886pt -7.

most errors are spurious links. Table 2 shows that, for recovering directed edges, we reach a median recall of 0.93% [IQR, 0.86, 0.97] while keeping a median precision of 0.71% [0.57-0.84] on \(20\) nodes graphs.

## 4 Discussion

We developed a structure learning algorithm that learns ordered conditional independence relations between irregularly sampled time series. These relations are parametrized by StructGP, a GP model built upon the convolution between a sparse lower-triangular matrix-valued impulse response function and independent white noises. It corresponds to assuming a linear additive Gaussian SCM for the Fourier representation of the time series, whose structure is invariant over all frequencies. Based on a differentiable definition of acyclicity, this algorithm recovers the true graph by solving a series of continuous optimization problems with high sensitivity and good precision on simulated data.

The recent work by Dallakyan  is the closest to ours. They develop a structure learning algorithm for time series by imposing a Gaussian linear additive SCM on the discrete Fourier transform of the time series. However, they learn different weight matrices at each frequency, whereas we assume an invariant structure across continuous frequencies, parametrized by only one weight matrix.

More work will be needed to bridge the gap between simulated data and real-world data with regard to the sensitivity to standardization and unmeasured cofounders. Indeed, when standardizing the time series, it was reported that with time-independent variables, standardization affected graph recovery [15; 16]. However, it is possible to re-parameterise the SCM to ensure a marginal unit variance without losing the identifiability of the graph from observations . Furthermore, scaling to large datasets could be achieved, for instance, with GPU-friendly solvers that exploit block sparsity induced by independence between patients .

   \(k\) & **P** (Predicted) & **P** (Random) & **R** (Predicted) & **R** (Random) \\ 
4 & 1.00 [0.83-1.00] & 0.50 [0.33-0.67] & 1.00 [0.83-1.00] & 0.50 [0.33-0.67] \\
10 & 0.82 [0.69-0.93] & 0.14 [0.08-0.25] & 0.94 [0.87-1.00] & 0.16 [0.07-0.25] \\
20 & 0.71 [0.57-0.84] & 0.07 [0.04-0.11] & 0.93 [0.86-0.97] & 0.07 [0.04-0.12] \\   

Table 2: Precision and recall

Precision (**P**) and recall (**R**) median and interquartile for varying mean degree and number of task parameters, from 100 replications of ’EXP3’ simulations with \(md=3\). False positives include extra links and reversed links. Metrics are computed from models learned (Predicted) and compared with metrics for random graphs of the same distribution as the simulated data (Random).

Figure 2: Average metrics for an increasing number of patients

Reports of ’EXP1’, increasing the number of patients for 10 tasks and 10 observations per task. The predicted graph (purple dots) is compared with a random graph from the same graph distribution (orange dots). The simulated graphs are random graphs with mean degree 2. Error bars represent bootstrap 95% confidence intervals.