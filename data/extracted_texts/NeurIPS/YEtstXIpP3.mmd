# Model-Free Active Exploration

in Reinforcement Learning

Alessio Russo

Division of Decision and Control Systems

KTH Royal Institute of Technology

Stockholm, SE &Alexandre Proutiere

Division of Decision and Control Systems

KTH Royal Institute of Technology

Stockholm, SE

###### Abstract

We study the problem of exploration in Reinforcement Learning and present a novel model-free solution. We adopt an information-theoretical viewpoint and start from the instance-specific lower bound of the number of samples that have to be collected to identify a nearly-optimal policy. Deriving this lower bound along with the optimal exploration strategy entails solving an intricate optimization problem and requires a model of the system. In turn, most existing sample optimal exploration algorithms rely on estimating the model. We derive an approximation of the instance-specific lower bound that only involves quantities that can be inferred using model-free approaches. Leveraging this approximation, we devise an ensemble-based model-free exploration strategy applicable to both tabular and continuous Markov decision processes. Numerical results demonstrate that our strategy is able to identify efficient policies faster than state-of-the-art exploration approaches.

## 1 Introduction

Efficient exploration remains a major challenge for reinforcement learning (RL) algorithms. Over the last two decades, several exploration strategies have been proposed in the literature, often designed with the aim of minimizing regret. These include model-based approaches such as Posterior Sampling for RL (PSRL) and Upper Confidence Bounds for RL (UCRL), along with model-free UCB-like methods . Regret minimization is a relevant objective when one cares about the rewards accumulated during the learning phase. Nevertheless, an often more important objective is to devise strategies that explore the environment so as to learn efficient policies using the fewest number of samples . Such an objective, referred to as Best Policy Identification (BPI), has been investigated in simplistic Multi-Armed Bandit problems  and more recently in tabular MDPs . For these problems, tight instance-specific sample complexity lower bounds are known, as well as model-based algorithms approaching these limits. However, model-based approaches may be computationally expensive or infeasible to obtain. In this paper, we investigate whether we can adapt the design of these algorithms so that they become model-free and hence more practical.

Inspired by , we adopt an information-theoretical approach, and design our algorithms starting from an instance-specific lower bound on the sample complexity of learning a nearly-optimal policy in a Markov decision process (MDP). This lower bound is the value of an optimization problem, referred to as the lower bound problem, whose solution dictates the optimal exploration strategy in an environment. Algorithms designed on this instance-specific lower bound, rather than minimax bounds, result in truly adaptive methods, capable of tailoring their exploration strategy according to the specific MDP's learning difficulty. Our method estimates the solution to the lower bound problem and employs it as our exploration strategy. However, we face two major challenges: (1) thelower bound problem is non-convex and often intractable; (2) this lower bound problem depends on the initially unknown MDP. In , the authors propose MDP-NaS, a model-based algorithm that explores according to the estimated MDP. They convexify the lower bound problem and explore according to the solution of the resulting simplified problem. However, this latter problem still has a complicated dependency on the MDP. Moreover, extending MDP-NaS to large MDPs is challenging since it requires an estimate of the model, and the capability to perform policy iteration. Additionally, MDP-NaS employs a _forced exploration_ technique to ensure that the _parametric_ uncertainty (the uncertainty about the true underlying MDP) diminishes over time -- a method, as we argue later, that we believe not to be efficient in handling this uncertainty.

We propose an alternative way to approximate the lower bound problem, so that its solution can be learnt via a model-free approach. This solution depends only on the \(Q\)-function and the variance of the value function. Both quantities can advantageously be inferred using classical stochastic approximation methods. To handle the parametric uncertainty, we propose an ensemble-based method using a bootstrapping technique. This technique is inspired by posterior sampling and allows us to quantify the uncertainty when estimating the \(Q\)-function and the variance of the value function.

Our contributions are as follows: (1) we shed light on the role of the instance-specific quantities needed to drive exploration in uncertain MDPs; (2) we derive an alternate upper bound of the lower bound problem that in turn can be approximated using quantities that can be learned in a model-free manner. We then evaluate the quality of this approximation on various environments: (_i_) a random MDP, (_ii_) the Riverswim environment , and (_iii_) the Forked Riverswim environment (a novel environment with high sample complexity); (3) based on this approximation, we present Model Free Best Policy Identification (MF-BPI), a model-free exploration algorithm for tabular and continuous MDPs. For the tabular MDPs, we test the performance of MF-BPI on the Riverswim and the Forked Riverswim environments, and compare it to that of Q-UCB [19; 56], PSRL, and MDP-NAS. For continuous state-spaces, we compare our algorithm to IDS and BSP  (Boostrapped DQN with randomized prior value functions) and assess their performance on hard-exploration problems from the DeepMind BSuite  (the DeepSea and the Cartpole swingup problems).

## 2 Related Work

The body of work related to exploration methods in RL problems is vast, and we mainly focus on online discounted MDPs (for the generative setting, refer to the analysis presented in [17; 28]). Exploration strategies in RL often draw inspiration from the approaches used in multi-armed bandit problems [26; 49], including \(\)-greedy exploration, Boltzmann exploration [57; 49; 26; 1], or more advanced procedures, such as Upper-Confidence Bounds (UCB) methods [2; 3; 26] or Bayesian procedures [52; 58; 14; 44]. We first discuss tabular MDPs, and then extend the discussion to the case of RL with function approximation.

**Exploration in tabular MDPs.** Numerous algorithms have been proposed with the aim of matching the PAC sample complexity minimax lower bound \(((1-)^{3}})\). In the design of these algorithms, model-free approaches typically rely on a UCB-like exploration [2; 26], whereas model-based methods leverage estimates of the MDP to drive the exploration. Some well-known model-free algorithms are Median-PAC , Delayed Q-Learning  and Q-UCB [56; 19]. Some notable model-based algorithms include: DEL , an algorithm that achieves asymptotically optimal instance-dependent regret; UCRL , an algorithm that uses extended value-iteration to compute an optimistic MDP; PSRL , that uses posterior sampling to sample an MDP. Other algorithms include MB1E , E3 , R-MAX [9; 20], and MORMAX . Most of existing algorithms are designed towards regret minimization. Recently, however, there has been a growing interest towards exploration strategies with minimal sample complexity, see e.g. [60; 28]. In [28; 29], the authors showed that computing an exploration strategy with minimal sample complexity requires to solve a non-convex problem. To overcome this challenge, they derived a tractable approximation of the lower bound problem, whose solution provides an efficient exploration policy under the generative model  and the forward model . This policy necessitates an estimate of the model, and includes a forced exploration phase (an \(\)-soft policy to guarantee that all state-action pairs are visited infinitely often). In , the above procedure is extended to linear MDPs, but there again, computing an optimal exploration strategy remains challenging. On a side note, in , the authors provide an alternative bound in the tabular case for episodic MDPs, and later extend it to linear MDPs . The episodic setting is further explored in  for deterministic MDPs.

**Exploration in Deep Reinforcement Learning (DRL).** Exploration methods in DRL environments face several challenges, related to the fact that the state-action spaces are often continuous, and other issues related to training deep neural architectures . The main issue in these large MDPs is that good exploration becomes extremely hard when either the reward is sparse/delayed or the observations contain distracting features [10; 59]. Numerous heuristics have been proposed to tackle these challenges, such as (1) adding an entropy term to the optimization problem to encourage the policy to be more randomized [31; 18] or (2) injecting noise in the observations/parameters [15; 43]. More generally, exploration techniques generally fall into two categories: _uncertainty-based_ and _intrinsic-motivation-based_[59; 24]. Uncertainty-based methods decouple the uncertainty into _parametric_ and _aleatoric_ uncertainty. Parametric uncertainty [14; 32; 23; 59] quantifies the uncertainty in the parameters of the state-action value. This uncertainty vanishes as the agent explores and learns. The aleatoric uncertainty accounts for the inherent randomness of the environment and of the policy [32; 23; 59]. Various methods have been proposed to address the parametric uncertainty, including UCB-like mechanisms [11; 59], or TS-like (Thompson Sampling) techniques [38; 36; 5; 35; 37; 40]. However, computing a posterior of the \(Q\)-values is a difficult task. For instance, Bayesian DQN  extends Randomized Least-Squares Value Iteration (RLSVI)  by considering the features prior to the output layer of the deep-\(Q\) network as a fixed feature vector, in order to recast the problem as a linear MDP. Non-parametric posterior sampling methods include Bootstrapped DQN (and Bootstrapped DQN with prior functions) [37; 39; 40], which maintains several independent \(Q\)-value functions and randomly samples one of them to explore the environment. Bootstrapped DQN was extended in various ways by integrating other techniques [6; 27]. For the sake of brevity, we refer the reader to the survey in  for an exhaustive list of algorithms. Most of these algorithms do not directly account for aleatoric uncertainty in the value function. This uncertainty is usually estimated using methods like Distributional RL [8; 13; 30]. Well-known exploration methods that account for both aleatoric and epistemic uncertainties include Double Uncertain Value Network (DUVN)  and Information Directed Sampling (IDS) [23; 33]. The former uses Bayesian dropout to measure the epistemic uncertainty, and the latter uses distributional RL  to estimate the variance of the returns. In addition, IDS uses bootstrapped DQN to estimate the parametric uncertainty in the form of a bound on the estimate of the suboptimality gaps. These uncertainties are then combined to compute an exploration strategy. Similarly, in , the authors propose UA-DQN, an approach that uses QR-DQN  to learn the parametric and aleatoric uncertainties from the quantile networks. Lastly, we refer the reader to [59; 45; 7] for the class of intrinsic-motivation-based methods.

## 3 Preliminaries

**Markov Decision Process.** We consider an infinite-horizon discounted Markov Decision Process (MDP), defined by the tuple \(=(S,A,P,q,,p_{0})\). \(S\) is the state space, \(A\) is the action space, \(P:S A(S)\) is the distribution over the next state given a state-action pair \((s,a),q:S A()\) is the distribution of the collected reward (with support in \(\)), \([0,1)\) is the discount factor and \(p_{0}\) is the distribution over the initial state.

Let \(:(A)\) be a stationary Markovian policy that maps a state to a distribution over actions, and denote by \(r(s,a)=_{r q(|s,a)}[r]\) the average reward collected when an action \(a\) is chosen in state \(s\). We denote by \(V^{}(s)=_{}^{*}[_{t 0}^{t}r(s_{t},a_{t})|s_{0 }=s]\) the discounted value of policy \(\). We denote by \(^{}\) an optimal stationary policy: for any \(s\), \(^{}(s)*{arg\,max}_{}V^{}(s)\) and define \(V^{}(s)=_{}V^{}(s)\). For the sake of simplicity, we assume that the MDP has a unique optimal policy (we extend our results to more general MDPs in the appendix). We further define \(_{}^{}()=\{:\|V^{}-V^{^{}}\|_{} \}\), the set of \(\)-optimal policies in \(\) for \( 0\). Finally, to avoid technicalities, we assume (as in ) that the MDP \(\) is communicating (that is, for every pair of states \((s,s^{})\), there exists a deterministic policy \(\) such that state \(s^{}\) is accessible from state \(s\) using \(\)).

We denote by \(Q^{}(s,a) r(s,a)+_{s^{} P(|s,a)}[V^{ }(s^{})]\) the \(Q\)-function of \(\) in state \((s,a)\). We also define the sub-optimality gap of action \(a\) in state \(s\) to be \((s,a) Q^{}(s,^{}(s))-Q^{}(s,a)\), where \(Q^{}\) is the \(Q\)-function of \(^{}\), and let \(_{}_{s,a^{}(s)}(s,a)\) be the minimum gap in \(\). For some policy \(\), we define \(*{Var}_{sa}[V^{}]*{Var}_{s^{} P (|s,a)}[V^{}(s^{})]\) to be the variance of the value function \(V^{}\) in the next state after taking action \(a\) in state \(s\). More generally, we define \[_{s^{} P(|s,a)}[(V^{}(s^{})-_{  P(|s,a)}[V^{}()])^{2^{k}}]2^{k}_{sa}[V^{}]\|V^{}-_{s^{} P( |s,a)}[V^{}]\|_{}\) be the span of \(\) under \(\), _i.e._, the maximum deviation from the mean of the next state value after taking action \(a\) in state \(s\).

**Best policy identification and sample complexity lower bounds.** The MDP \(\) is initially unknown, and we are interested in the scenario where the agent interacts sequentially with \(\). In each round \(t\), the agent selects an action \(a_{t}\) and observes the next state and the reward \((s_{t+1},r_{t})\): \(s_{t+1} P(|s_{t},a_{t})\) and \(r_{t} q(|s_{t},a_{t})\). The objective of the agent is to learn a policy in \(^{*}_{}()\) (possibly \(^{*}\)) as fast as possible. This objective is often formalized in a PAC framework where the learner has to stop interacting with the MDP when she can output an \(\)-optimal policy with probability at least \(1-\). In this formalism, the learner strategy consists of (i) a sampling rule or exploration strategy; (ii) a stopping time \(\); (iii) an estimated optimal policy \(\). The strategy is called \((,)\)-PAC if it stops almost surely, and \(_{}[^{*}_{}()] 1-\). Interestingly, one may derive instance-specific lower bounds of the sample complexity \(_{}[]\) of any \((,)\)-PAC algorithm [28; 29], which involves computing an optimal allocation vector \(_{}(S A)\) (where \((S A)\) is the set of distributions over \(S A\)) that specifies the proportion of times an agent needs to sample each pair \((s,a)\) to confidently identify the optimal policy:

\[_{ 0}_{}[]}{(,1- )} T_{}(_{})T_{}()^{-1} _{_{}()}_{(s,a) }[_{|}(s,a)],\] (1)

and \(_{}=_{()}T_{}() ^{-1}\). Here, \(_{}()\) is the set of confusing MDPs \(\) such that the \(\)-optimal policies of \(\) are not \(\)-optimal in \(\), i.e., \(_{}()\{:,^{*}_{ }()^{*}_{}()=\}\). In this definition, if the next state and reward distributions under \(\) are \(P^{}(s,a)\) and \(q^{}(s,a)\), we write \(\) if for all \((s,a)\) the distributions of the next state and of the rewards satisfy \(P(s,a) P^{}(s,a)\) and \(q(s,a) q^{}(s,a)\).We further let \(_{|}(s,a)(P(s,a),P^{}(s,a))+ (q(s,a),q^{}(s,a))\). \(()\) is the set of possible allocations; in the generative case it is \((S A)\), while with navigation constraints we have \(()\{(S A):(s)=_{s^{},a ^{}}P(s|s^{},a^{})(s^{},a^{})\}, s  S\}\), with \((s)_{a}(s,a)\). Finally, \((a,b)\) is the KL-divergence between two Bernoulli distributions of means \(a\) and \(b\).

## 4 Towards Efficient Exploration Allocations

We aim to extend previous studies on best policy identification to online model-free exploration. In this section, we derive an approximation to the bound proposed in , involving quantities learnable via stochastic approximation, thereby enabling the use of model-free approaches.

The optimization problem (1) leading to instance-specific sample complexity lower bounds has an important interpretation [28; 29]. An allocation \(_{}\) corresponds to an exploration strategy with minimal sample complexity. To devise an efficient exploration strategy, one could then think of estimating the MDP \(\), and solving (1) for this estimated MDP to get an approximation of \(_{}\). There are two important challenges towards applying this approach:

1. Estimating the model can be difficult, especially for MDPs with large state and action spaces, and arguably, a model-free method would be preferable.
2. The lower bound problem (1) is, in general, non-convex [28; 29].

A simple way to circumvent issue (ii) involves deriving an upper bound of the value of the sample complexity lower bound problem (1). Specifically, one may derive an upper bound \(U()\) of \(T_{}()\) by convexifying the corresponding optimization problem. The exploration strategy can then be the \(^{*}\) that achieves the infimum of \(U()\). This approach ensures that we identify an approximately optimal policy, at the cost of _over-exploring_ at a rate corresponding to the gap \(U(^{*})-T_{}(_{})\). Note that using a lower bound of \(T_{}()\) would not guarantee the identification of an optimal policy, since we would explore "less" than required. The aforementioned approach was already used in  where the authors derive an explicit upper bound \(U_{0}()\) of \(T_{0}()\). We also apply it, but derive an upper bound such that implementing the corresponding allocation \(^{*}\) can be done in a model-free manner (hence solving the first issue (i)).

### Upper bounds on \(T_{}()\)

The next theorem presents the upper bound derived in .

**Theorem 4.1** ().: _Consider a communicating MDP \(\) with a unique optimal policy \(^{}\). For all vectors \((S A)\),_

\[T_{0}() U_{0}()_{(s,a):a^{}(s)} {H_{0}(s,a)}{(s,a)}+_{s}^{}}{(s,^{}(s ))},\] (2)

_with_

\[\{H_{0}(s,a)=}+( {16\,_{sa}[V^{}]}{(s,a)^{2}},_{sa}[V ^{}]^{4/3}}{(s,a)^{}}),\\ H_{0}^{}=^{2}(1-)^{2}}+(^{2}(1-)^{3}},(_{s} _{s^{}(s)}[V^{}]}{_{}^{2}(1-)^{2}}, _{sa^{}(s)}[V^{}]^{4/3}}{_{}^{4/ 3}(1-)^{4/3}}))..\]

In the upper bound presented in this theorem, the following quantities characterize the _hardness_ of learning the optimal policy: \((s,a)\) represents the difficulty of learning that in state \(s\) action \(a\) is sub-optimal; the variance \(_{sa}[V^{}]\) measures the aleatoric uncertainty in future state values; and the span \(_{sa}[V^{}]\) of the optimal value function can be seen as another measure of aleatoric uncertainty, large whenever there is a significant variability in the value for the possible next states.

Estimating the span \(_{sa}[V^{}]\), in an online setting, is a challenging task for large MDPs. Our objective is to derive an alternative upper bound that, in turn, can be approximated using quantities that can be learned in a model-free manner. We observe that the variance of the value function, and more generally its moments \(M_{sa}^{k}[V^{}]^{2^{-k}}\) for \(k 1\) (see Appendix C), are smaller than the span. By refining the proof techniques used in , we derive the following alternative upper bound.

**Theorem 4.2**.: _Let \( 0\) and let \(k(s,a)_{k}M_{sa}^{k}[V^{}]^{2^{-k}}\) (for brevity, we write \(k\) instead of \(k(s,a)\)). Then, \((S A)\), we have \(T_{}() U()\), with_

\[U()_{s,a^{}(s)}(M_{sa }^{k}[V^{}]^{2^{1-k}}}{(s,a)(s,a)^{2}}+_{s^{}} )(1+)^{2}}{(s^{},^{}(s^{})) (s,a)^{2}(1-)^{2}}),\] (3)

_where \(C(s^{})=(4,16^{2}^{2}M_{s^{},^{}(s^ {})}^{k}[V^{}]^{2^{1-k}})\) and \(\) is the golden ratio._

We can observe that in the worst case, the upper bound \(U(^{})\) of the sample complexity lower bound, with \(^{}=_{}U()\), scales as \(O(_{s,^{}(s)}[V^{}]^{2}}{_{ }^{2}(1-)^{2}})\). Since \(_{sa}[V^{}](1-)^{-1}\), then \(U(^{})\) scales at most as \(O(^{2}(1-)^{4}})\). However, the following questions arise: (1) Can we select a single value of \(k\) that provides a good approximation across all states and actions? (2) How much does this bound improve on that of Theorem 4.1? As we illustrate in the example presented in the next subsection, we believe that actually selecting \(k=1\) for all states and actions leads to sufficiently good results. With this choice, we obtain the following approximation:

\[U_{1}()_{s,a^{}(s)}( \,_{sa}[V^{}]}{(s,a)(s,a)^{2}}+_{s^{}} (s^{})(1+)^{2}}{(s^{},^{}(s^{ }))(s,a)^{2}(1-)^{2}}),\] (4)

where \(C^{}(s^{})=(4,16^{2}^{2}\,_{s^{ },^{}(s^{})}[V^{}])\). \(U_{1}()\) resembles the term in Theorem 4.1 (note that we do not know whether \(U_{1}\) is a valid upper bound for \(T_{}\)). For the second question, our numerical experiments (presented below) suggest that \(U()\) is a tighter upper bound than \(U_{0}()\).

### Example on Tabular MDPs

In Figure 1, we compare the characteristic time upper bounds obtained in the previous subsection. These upper bounds correspond to the allocations \(^{}\), \(_{0}^{}\), and \(_{1}^{}\) obtained by minimizing, over \((S A)\)1, \(U()\), \(U_{0}()\), and \(U_{1}()\), respectively. We evaluated these characteristic times on various MDPs: (1) a random MDP (see Sec. A in the appendix); (2) the RiverSwim environment; (3) the Forked RiverSwim, a novel environment where the agent needs to constantly explore two different states to learn the optimal policy (compared to the RiverSwim environment, the sample complexity is higher; refer to Appendix A for a complete description).

We note that across all plots, the optimal allocation \(_{0}^{}\) has a quite large characteristic time (black cross). Instead, the optimal allocation \(^{}\) (blue circle) computed using our new upper bound (3) achieves a lower characteristic time. When we evaluate \(_{0}^{}\) on the new bound (3) (orange star), we observe similar characteristic times.

Finally, to verify that we can indeed choose \(k=1\) uniformly across states and actions, we evaluated the characteristic time \(_{1}^{}\) computed using (4) (green triangle). Our results indicate that the performance is not different from those obtained with \(^{}\), suggesting that the quantities of interest (gaps and variances) are enough to learn an efficient exploration allocation. We investigate the choice of \(k\) in more detail in Appendix A.

## 5 Model-Free Active Exploration Algorithms

In this section we present MF-BPI, a model-free exploration algorithm that leverages the optimal allocations obtained through the previously derived upper bound of the sample complexity lower bound. We first present an upper bound \(()\) of \(U()\), so that it is possible to derive a closed form solution of the optimal allocation (an idea previously proposed in ).

**Proposition 5.1**.: _Assume that \(\) has a unique optimal policy \(^{}\). For all \((S A)\), we have:_

\[U()():=_{s,a^{}(s)}+}(s^{},^{}(s^{ }))},\]

_with \(H(s,a)M_{sa}^{k}[V^{}]^{2^{1-k}}}{(s, a)^{2}}\) and \(H}C(s^{})(1+)^{2}}{_{}^{2 }(1-)^{2}}\). The minimizer \(^{}_{}()\) satisfies \(^{}(s,a) H(s,a)\) for \(a^{}(s)\) and \(^{}(s,^{}(s))(s)}H(s,a)/|S|}\) otherwise._

In the MF-BPI algorithm, we estimate the gaps \((s,a)\) and \(M_{sa}^{k}[V^{}]\) for a fixed small value of \(k\) (we later explain how to do this in a model-free manner.) and compute the corresponding allocation \(^{}\). This allocation drives the exploration under MF-BPI. Using this design approach, we face two issues:

**(1) Uniform \(k\) and regularization.** It is impractical to estimate \(M_{sa}^{k}[V^{}]\) for multiple values of \(k\). Instead, we fix a small value of \(k\) (_e.g._, \(k=1\) or \(k=2\)) for all state-action pairs (refer to the previous section for a discussion on this choice). Then, to avoid excessively small values of the gaps in the denominator, we regularize the allocation \(^{}\) by replacing, in the expression of \(H(s,a)\) (resp. \(H_{}\)), \((s,a)\) (resp. \(_{}\)) by \(((s,a)+)\) (resp. \((_{}+)\)) for some \(>0\).

**(2) Handling parametric uncertainty via bootstrapping.** The quantities \((s,a)\) and \(M_{sa}^{k}[V^{}]\) required to compute \(^{}\) remain unknown during training, and we adopt the Certainty Equivalence principle, substituting the current estimates of these quantities to compute the exploration strategy.

Figure 1: Comparison of the upper bounds (2) and (3) for different sizes of \(S\) and \(=0.95\). We evaluated different allocations using \(U_{0}()\) and \(U()\). The allocations are: \(_{0}^{}\) (the optimal allocation in (2), \(^{}\) (the optimal allocation in (3) and \(_{1}^{}\) (the optimal allocation in (4) by setting \(k=1\) uniformly across states and actions). For the random MDP we show the median value across \(30\) runs.

By doing so, we are inherently introducing parametric uncertainty into these terms that is not taken into account by the allocation \(^{}\). To deal with this uncertainty, the traditional method, as used e.g. in [28; 29], involves using \(\)-soft exploration policies to guarantee that all state-action pairs are visited infinitely often. This ensures that the estimation errors vanish as time grows large. In practice, we find this type of forced exploration inefficient. In MF-BPI, we opt for a bootstrapping approach to manage parametric uncertainties, which can augment the traditional forced exploration step, leading to more principled exploration.

### Exploration in tabular MDPs.

The pseudo-code of MF-BPI for tabular MDPs is presented in Algorithm 1. In round \(t\), MF-BPI explores the MDP using the allocation \(^{(t)}\) estimating \(^{}\). To compute this allocation, we use Proposition 5.1 and need (i) the sub-optimality gaps \((s,a)\), which can be easily derived from the \(Q\)-function; (ii) the \(2^{k}\)-th moment \(M_{sa}^{k}[V^{}]\), which can always be learnt by means of stochastic approximation. In fact, for any Markovian policy \(\) and pair \((s,a)\) we have \(M_{sa}^{k}[V_{}^{}]=}}_{s^{} P (|s,a)}[^{}(s,a,s^{})^{2^{k}}],\) where \(^{}(s,a,s^{})=r(s,a)+_{a^{}(| s^{})}[Q^{}(s^{},a^{})]-Q^{}(s,a)\) is a variant of the TD-error. MF-BPI then uses an asynchronous two-timescale stochastic approximation algorithm to learn \(Q^{}\) and \(M_{sa}^{k}[V^{}]\),

\[Q_{t+1}(s_{t},a_{t}) =Q_{t}(s_{t},a_{t})+_{t}(s_{t},a_{t})(r_{t}+ _{a}Q_{t}(s_{t+1},a)-Q_{t}(s_{t},a_{t})),\] (5) \[M_{t+1}(s_{t},a_{t}) =M_{t}(s_{t},a_{t})+_{t}(s_{t},a_{t})((^{} _{t}/)^{2^{k}}-M_{t}(s_{t},a_{t})),\] (6)

where \(^{}_{t}=r_{t}+_{a}Q_{t+1}(s_{t+1},a)-Q_{t+1}(s_{t},a_{t})\), and \(\{(_{t},_{t})\}_{t 0}\) are learning rates satisfying \(_{t 0}_{t}(s,a)=_{t 0}_{t}(s,a)=,_{t 0 }(_{t}(s,a)^{2}+_{t}(s,a)^{2})\), and \((s,a)}{_{t}(s,a)} 0\).

MF-BPI uses bootstrapping to handle parametric uncertainty. We maintain an ensemble of \((Q,M)\)-values, with \(B\) members, from which we sample \((_{t},_{t})\) at time \(t\). This sample is generated by sampling a uniform random variable \(()\) and, for each \((s,a)\) set \(_{t}(s,a)=_{}(Q_{t,1}(s,a),,Q_{t,B}(s,a))\) (assuming a linear interpolation). This method is akin to sampling from the parametric uncertainty distribution (we perform the same operation also to compute \(_{t}\)). This sample is used to compute the allocation \(^{(t)}\) using Proposition 5.1 by setting \(_{t}(s,a)=_{a^{}}_{t}(s,a^{})-_{t}(s,a)\), \(^{}_{t}(s)=_{a}_{t}(s,a)\) and \(_{,t}=_{s,a^{}_{t}(s)}_{t}(s,a)\). Note that, the allocation \(^{(t)}\) can be mixed with a uniform policy, to guarantee asymptotic convergence of the estimates. Upon observing an experience, with probability \(p\), MF-BPI updates a member of the ensemble using this new experience. \(p\) tunes the rate at which the models are updated, similar to sampling with replacement, speeding up the learning process. Selecting a high value for \(p\) compromises the estimation of the parametric uncertainty, whereas choosing a low value may slow down the learning process.

```
0: Parameters \((,k,p)\); ensemble size \(B\); learning rates \(\{(_{t},_{t})\}_{t}\).
1: Initialize \(Q_{1,b}(s,a)([0,1/(1-)])\) and \(M_{1,b}(s,a)([0,1/(1-)^{2^{k}}])\) for all \((s,a) S A\) and \(b[B]\).
2:for\(t=0,1,2,\),do
3: Bootstrap a sample \((_{t},_{t})\) from the ensemble, and compute the allocation \(^{(t)}\) using Proposition 5.1. Sample \(a_{t}^{(t)}(s_{t},)\); observe \((r_{t},s_{t+1}) q(|s_{t},a_{t}) P(|s_{t},a_{t})\).
4:for\(b=1,,B\)do
5: With probability \(p\), using the experience \((s_{t},a_{t},r_{t},s_{t+1})\), update \(Q_{t,b}\) and \(M_{t,b}\) using Equations (5) and (6).
6:endfor
7:endfor ```

**Algorithm 1** Boostrapped MF-BPI (Boostrapped Model Free Best Policy Identification)

**Exploration without bootstrapping?** To illustrate the need for our bootstrapping approach, we tried to use the allocation \(^{(t)}\) mixed with a uniform allocation. In Figure 2, we show the results on Riverswim-like environments with \(5\) states. While forced exploration ensures infinite visits to all state-action pairs, this guarantee only holds asymptotically. As a result, the allocation mainly focuses on the current MDP estimate, neglecting other plausible MDPs that could produce the same data. This makes the forced exploration approach too sluggish for effective convergence, suggesting its inadequacy for rapid policy learning. These results highlight the need to account for the uncertainty in \(Q,M\) when computing the allocation.

### Extension to Deep Reinforcement Learning

To extend bootstrapped MF-BPI to continuous MDPs, we propose DBMF-BPI (see Algorithm 2, or Appendix B). DBMF-BPI uses the mechanism of prior networks from BSP (bootstrapping with additive prior) to account for uncertainty that does not originate from the observed data. As before, we keep an ensemble \(\{Q_{_{1}},,Q_{_{B}}\}\) of \(Q\)-values (with their target networks) and an ensemble \(\{M_{_{1}},,M_{_{B}}\}\) of \(M\)-values, as well as their prior networks. We use the same procedure as in the tabular case to compute \((_{t},_{t})\) at time \(t\), except that we sample \(()\) every \(T_{s}(1-)^{-1}\) training steps (or at the end of an episode) to make the training procedure more stable. The quantity \(_{t}\) is used to compute \(^{}_{t}(s_{t})\) and \(_{t}(s_{t},a)\). We estimate \(_{,t}\) via stochastic approximation, with the minimum gap from the last batch of transitions sampled from the replay buffer serving as a target. To derive the exploration strategy, we compute \(H_{t}(s_{t},a)=_{t}(s_{t},a)^{2^{1-k}}}{(_{ ,t}(s_{t},a)+)^{2}}\) and \(H_{t}=(1,4^{2}^{2}_{t}(s_{t},^ {}_{t}(s_{t}))^{2^{1-k}})}{(_{,t}+)^{2}(1-) ^{2}}\). Next, we set the allocation \(^{(t)}_{o}\) as follows: \(^{(t)}_{o}(s_{t},a)=H_{t}(s_{t},a)\) if \(a^{}_{t}(s_{t})\) and \(^{(t)}_{o}(s_{t},a)=_{a^{}_{t}(s_{t})}H_{t}(s _{t},a)}\) otherwise. Finally, we obtain an \(_{t}\)-soft exploration policy \(^{(t)}(s_{t},)\) by mixing \(^{(t)}_{o}(s_{t},)/_{a}^{(t)}_{o}(s_{t},a)\) with a uniform distribution (using an exploration parameter \(_{t}\)).

```
0: Parameters \((,k)\); ensemble size \(B\); exploration rate \(\{_{t}\}_{t}\); estimate \(_{,0}\); mask probability \(p\).
1: Initialize replay buffer \(\), networks \(Q_{_{b}},M_{_{b}}\) and targets \(Q_{^{}_{b}}\) for all \(b[B]\).
2:for\(t=0,1,2,,\)do
3: Sampling step.
4: Compute allocation \(^{(t)}(s_{t},\{Q_{_{b}},M_{_{b}}\}_{b [B]},_{,t},,,k,_{t})\).
5: Sample \(a_{t}^{(t)}(s_{t},)\) and observe \((r_{t},s_{t+1}) q(|s_{t},a_{t}) P(|s_{t},a_{t})\).
6: Add transition \(z_{t}=(s_{t},a_{t},r_{t},s_{t+1})\) to the replay buffer \(\).
7: Training step.
8: Sample a batch \(\) from \(\), and with probability \(p\) add the \(i^{th}\) experience in \(\) to a sub-batch \(_{b}\), \( b[B]\). Update the \((Q,M)\)-values of the \(b^{th}\) member in the ensemble using \(_{b}\): \(\{Q_{_{b}},Q_{^{}_{b}},M_{_{b}}\}_{b[B]} (\{_{b},Q_{_{b}},Q_{^{}_{b}},M_{ _{b}}\}_{b[B]})\).
9: Update estimate \(_{,t+1}(_{,t},,\{Q_{_{b}}\}_{b[B]})\).
10:endfor ```

**Algorithm 2** DBMF-BPI (Deep Bootstrapped Model Free BPI)

Figure 2: Forced exploration example with \(5\) states. We explore according to \(^{(t)}(s_{t},a)=(1-_{t})_{t}(s_{t},a)}{_{a^ {}}^{*}_{t}(s_{t},a^{})}+_{t}\), mixing the estimate of the allocation \(^{}\) from Proposition 5.1 with a uniform policy, with \(_{t}=(10^{-3},1/N_{t}(s_{t}))\) where \(N_{t}(s)\) indicates the number of times the agent visited state \(s\) up to time \(t\). Shade indicates \(95\%\) confidence interval.

Numerical Results

We evaluate the performance of MF-BPI on benchmark problems and compare it against state-of-the-art methods (details can be found in Appendix A).

**Tabular MDPs.** In the tabular case, we compared various algorithms on the Riverswim and Forked Riverswim environments. We evaluate MF-BPI with (1) bootstrapping and with (2) the forced exploration step using an \(\)-soft exploration policy, MDP-NAS , PSRL  and Q-UCB . For MDP-NAS, the model of the MDP was initialized in an optimistic way (with additive smoothing).

In both environments, we varied the size of the state space. In Figure 3, we show \(1--V^{}\|_{}}{\|V^{*}\|_{}}\), a performance measure for the estimated policy \(_{T}^{*}\) after \(T=|S| 10^{4}\) steps with \(=0.99\). Results (the higher the better) indicate that bootstrapped MF-BPI can compete with model-based and model-free algorithms on hard-exploration problems, without resorting to expensive model-based procedures. Details of the experiments, including the initialization of the algorithms, are provided in Appendix A.

**Deep RL.** In environments with continuous state space, we compared DBMF-BPI with BSP  (Bootstrapped DQN with randomized priors) and IDS  (Information-Directed Sampling). We also evaluated DBMF-BPI against BSP2, a variant of BSP that uses the same masking mechanism as DBMF-BPI for updating the ensemble. These methods were tested on challenging exploration problems from the DeepMind behavior suite  with varying levels of difficulty: (1) a stochastic version of DeepSea and (2) the Cartpole swingup problem. The DeepSea problem includes a \(5\%\) probability of the agent slipping, i.e., that an incorrect action is executed, which increases the aleatoric variance.

The results for the Cartpole swingup problem are depicted in Figure 4 for various difficulty levels \(k\) (see also Appendix A.5 for more details), demonstrating the ability of DBMF-BPI to quickly learn an efficient policy. While BSP generally performs well, there is a notable difference in performance when compared to DBMF-BPI. For a fair comparison, we used the same network initialization across all methods, except for IDS. Untuned, IDS performed poorly; proper initialization improved its performance, but results remained unsatisfactory. In Figure 5, we present two exploration metrics

Figure 4: Cartpole swingup problem. On the left: total upright time at a difficulty level of \(k=10\). On the right: total upright time after \(200\) episodes for different difficulties \(k\). To observe a positive reward, the pole’s angle must satisfy \(()>k/20\), and the cart’s position should satisfy \(|x| 1-k/20\). Bars and shaded areas indicate \(95\%\) confidence intervals.

Figure 3: Evaluation of the estimated optimal policy \(_{T}^{*}\) after \(T\) steps for MF-BPI, Q-UCB, MDP-NAS and PSRL. Results are averaged across 10 seeds and lines indicate \(95\%\) confidence intervals.

for difficulty \(k=5\). The frequency of visits measures the uniformity and dispersion of visits across the state space, while the second metric evaluates the recency of visits to different regions, capturing how frequently the methods keep visiting previously visited states (a smaller value indicates that the agent tends to concentrate on a specific region of the state space). For detailed analysis, please refer to appendix A.

For the slipping DeepSea problem, results are depicted in Fig. 6 (see also Appendix A.4 for more details). Besides the number of successful episodes, we also display the standard deviation of \((t_{})_{ij}\) across all cells \((i,j)\), where \((t_{})_{ij}\) indicates the last timestep \(t\) that a cell \((i,j)\) was visited (normalized by \(NT\), the product of the grid size, and the number of episodes). The right plot shows \((t_{})\) for different problem sizes, highlighting the good exploration properties of DBMF-BPI. Additional details and exploration metrics can be found in Appendix A.

## 7 Conclusions

In this work, we studied the problem of exploration in Reinforcement Learning and presented MF-BPI, a model-free solution for both tabular and continuous state-space MDPs. To derive this method, we established a novel approximation of the instance-specific lower bound necessary for identifying nearly-optimal policies. Importantly, this approximation depends only on quantities learnable via stochastic approximation, paving the way towards model-free methods. Numerical results on hard-exploration problems highlighted the effectiveness of our approach for learning efficient policies over state-of-the-art methods.

Figure 5: Exploration in Cartpole swingup for \(k=5\). On the left, we show the entropy of visitation frequency for the state space \((x,,,)\) during training. On the right, we show a measure of the dispersion of the most recent visits; smaller values indicate that the agent is less explorative as \(t\) increases.

Figure 6: Slipping DeepSea problem. On the left: total number of successful episodes (_i.e._, that the agent managed to reach the final reward) for a grid with \(30^{2}\) input features. On the right: standard deviation of \(t_{}\) at the last episode, depicting how much each agent explored (the lower the better).