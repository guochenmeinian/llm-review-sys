# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Introduction

**Visual reasoning** is a crucial task that demands models to not only comprehend and interpret visual information but also to apply high-level cognition to derive logical solutions [40; 120; 60]. The field has received significant attention from the machine learning community because of its potential to enable a wide range of intelligent applications, such as intelligent tutoring systems [5; 94; 69], automated image captioning , and virtual assistants [88; 50]. To perform visual reasoning effectively, a model must possess both visual perception capabilities and strong logic reasoning abilities.

While classic visual reasoners typically rely on complex architectures [117; 61; 116] or are unable to generalize beyond the training dataset [121; 72], recent advancements in large pretrained models have shown that vision-language models (VLMs) can achieve impressive performance on visual reasoning tasks even under zero-shot settings [104; 52; 51; 49; 48; 3]. Meanwhile, large language models (LLMs) have also demonstrated robust zero-shot commonsense reasoning abilities on the natural language processing (NLP) applications [109; 8; 15]. Several recent studies have attempted to combine such complementary VLMs and LLMs for visual reasoning. For example, PICA  utilizes image captioning models to generate textual prompts for GPT-3 , and adapts GPT-3 to solve the visual question answering (VQA) tasks in an in-context few-shot learning manner. Socratic Models  allow VLMs and LLMs to communicate through prompt engineering to unlock zero-shot multimodal reasoning capabilities.

On the premise that current studies have focused on the interactions among heterogeneous models (specifically, among VLM and LLMs), in this work, we shift to examine how to reconcile **homogeneous expert models** (_e.g._, multiple VLMs) with an LLM in a _coordinative_ paradigm. Inspired by the findings in CICERO  that LLMs capture strong strategic planning and negotiation abilities in coordinating multiple agents, we propose \(}\)**Cola**, a novel **model ensemble** approach that utilizes an LLM as the coordinator in between multiple VLMs. A key finding of this study is that _given multiple VLMs with different preferred patterns in describing the visual context and predicting plausible answers in natural languages, an LLM can coordinate and integrate their respective strengths efficiently and effectively_. We present two variants of Cola, namely \(}\)**Cola-FT** and \(}\)**Cola-Zero**, where FT corresponds to an instruction finetuning approach and Zero stands for an in-context learning approach to adapt the coordinator LLM for visual reasoning. Figure 1 provides an overview of Cola and conventional model ensemble approaches.

Existing work on model ensembles usually focuses on manipulating model weights  or predictions [111; 22], while remaining cumbersome, if possible, to implement on prevalent end-to-end black box model APIs, like GPT-4 , Google Bard, Anthropic Claude, etc. To address this issue, prompt ensembles [107; 106; 73] sample model outputs (_e.g._, rationales) in natural languages to boost chain-of-thought reasoning . Recent studies on augmented LLM such as [86; 93; 58] have delved into developing a comprehensive strategy that enables LLMs to utilize external tools. These tools comprise multiple off-the-shelf models, web search engines, Python functions , and rule-based modules, which are instrumental in performing complex tasks. Despite these efforts, the power of prompt ensembles to aggregate multiple models remains untouched. In contrast, we show that Cola leverages language prompts generated from multiple expert models to make model ensembles.

Systematic experiments demonstrate that Cola performs at the pinnacle of ability on VQA, outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Specifically, Cola-FT achieves state-of-the-art performance on A-OKVQA , OK-VQA , e-SNLI-VE , and VSR datasets , even when compared with methods that adopt larger models or require more training computations. Cola-FT also demonstrates competitive capabilities on VQA v2 , and compositional reasoning tasks (GQA  and CLEVR ). Perhaps surprisingly, we find that Cola-Zero demonstrates comparable performance without finetuning, as an emerging ability of larger language models. Compared to a single VLM and ensemble modeling, both Cola-FT and Cola-Zero improve the performance substantially across most datasets. They are even effective with the recent large multimodal models like InstructBLIP  which embeds a pretrained LLM inside itself. Besides, we conduct a thorough analysis of perturbed VLM caption or plausible answer labels and saliency visualization to investigate how Cola recognizes each VLM's individual functionalities and then performs coordination behaviors. We conjecture that, in principle, any language-expressing reasoning task can be usefully augmented with coordinative language models that learn to aggregate multiple expert models, even via in-context learning.

In summary, our contributions are as follows:

* **Cola**: a novel paradigm that utilizes a language model as a coordinator between multiple vision-language models to integrate their respective strengths for visual reasoning (SS2).
* **State-of-the-art performance**: Cola achieves the pinnacles on a challenging suite of diverse visual reasoning tasks and datasets(SS3.2).
* **Systematic analysis**: our experiments reveal how Cola comprehends the instruction prompts, then coordinates them to capture impressive visual reasoning capabilities (SS3.3, SS3.4, SS3.6).

## 2 Cola

We formulate various visual reasoning tasks as a multi-class classification problem for simplicity. Given an image \(v\) and a question-like prompt \(q\), the reasoner is required to select an answer \(a\) from the candidate set \(=\{a\}\). In the case that the reasoner outputs a text sequence \(s_{v,q}\), we map \(s\) to a prediction \(P(v,q)=(T(s_{v,q}),T(\{a\}))\) where \(T\) transforms text sequences into text embeddings (we use a all-mpnet-base-v2 model  here), and \(\) denotes cosine similarity.

Ensemble Modelingaggregates multiple models' predictions in order to improve the overall performance (Figure 0(a)). For instance, one common practice is averaging over \(n\) models:

\[P(v,q)=_{i=1}^{n}P_{i}(v,q), \]

where \(P_{i}(v,q)\) denotes the prediction of the \(i^{th}\) model on input \((v,q)\).

### 2.1 Cola & Templates

An overview of Cola is shown in Figure 0(c). We use OFA  and BLIP  as the VLMs. LLMs include encoder-decoder (FLAN-T5 ) and decoder-only (Vicuna-1.5 , Mistral ) transformers. We first prompt each VLM to output captions and plausible answers independently. We then concatenate the instruction prompt, the question with choices, captions, and plausible answers to fuse all contexts for the LLM to reason, coordinate, and answer.

Image captioninggives important visual context to reason from. We first employ \(i^{th}\) VLM to describe each image respectively to get visual descriptions \(c_{i}(v)\). We use ofa-large for OFA and blip-image-captioning-large for BLIP, both implemented by the Hugging Face Transformers library .

Plausible answersby the VLMs to the question provide clues and patterns of VLMs for the LM to consider and coordinate. Similar to captioning, we prompt each \(i^{th}\) VLM using the image-question pair to get a plausible answer \(_{i}(v,q)\). We use ofa-large for OFA and blip-vqa-base for BLIP. Following OFA, our prompt template varies by task category. For the VQA tasks, we leave the original question unchanged. For the visual entailment and visual spatial reasoning tasks, our prompt template is _"does the image describe "<text premise>"?"_.

Prompt templateis shown in Table 1. First, we designed an instruction prompt for LM to understand the requirement to coordinate VLMs to answer the visual reasoning question. We then concatenate the captions from each VLM model, with the VLM identification labels in natural languages (referred to as VLM caption labels in Figure 3), such as _"OFA's description: <OFA caption>"_. Next, the question and its plausible answers provided by VLMs (with similar identification labels referred to as VLM answer labels in Figure 3) are concatenated. We follow  to

  
**General Prompt Template** \\  Answer the following multiple-choice question by OFA and BLIP’s description and their answers to the visual question. OFA and BLIP are two different vision-language models to provide clues. \\ OFA’s description: OFA caption\$ BLIP’s description: BLIP caption\$ Q: Question\$ Q: Question\$ QFA’s answer: OFA answers\$ BLIP’s answer: BLIP answers\$ Choices: Choices to the questions\$ A: \\   

Table 1: **LM prompt template.** The LM is instructed to coordinate VLMs. Each question set defines _visual context_, _question (and choices)_, and _plausible answers_.

include the choices of question (for multiple-choice questions in A-OKVQA, e-SNLI-VE, and VSR) and "_A:"_ to prompt for answers. Overall, the prompt for LLM input is given by:

\[(v,q)=(\{(c_{i}(v),_{i}(v,q)) i=1, ,n\}). \]

More specific prompt templates on each dataset are provided in Appendix A.7.

### Cola-FT

Instruction Tuningof Cola is initialized with pretrained checkpoints. Given the question \(q\) based on the image \(v\), the LM predicts the answer in the form of sequence

\[s_{v,q}=((v,q)). \]

To optimize the LLM, we use the language modeling loss for next-token prediction, with the teacher forcing strategy. We only finetune the LLM (while not the VLMs) to follow the common paradigm of ensemble modeling and simplify the method (Figure 1).

Inferencedeploys the same prompt as Table 1 to align with instruction tuning. We resort to the greedy decoding strategy for conditional sequence generation at both instruction tuning and inference.

### Cola-Zero

In-context learningis an emerging ability of the LLM models pretrained on documents of long-range coherence. By learning input and output format from demonstration, in-context learners learn to perform a downstream task simply by conditioning on a prompt consisting of input-output examples . The coordinator LLM, finetuned on instruction prompts with examples, is capable of in-context few-shot learning and zero-shot learning (see Figures 6 and 7).

Cola-Zerois the in-context few-shot/zero-shot learning variant of Cola, without instruction tuning. For in-context \(k\)-shot learning, we modify the prompt (Table 1) to include \(k\) input-output examples sampled from the training set. For zero-shot learning, the prompt remains the same as Table 1.

## 3 Experiments

First, the experimental setups and basic methods are described in this section. The main quantitative results are then presented in Table 2. Next, we analyze qualitative visualizations and scaling to verify the effectiveness of the Cola paradigm in different settings. Further details on datasets, training, evaluation, and experimental analysis can be found in Appendix A.

### Baseline Methods

State-of-the-art Methodsare in two broad categories, VLM alone, and VLM combined with LLM. In Table 2, for a fair comparison, we detail the techniques (whether finetuning or in-context learning is required) used for training VLMs and LLMs, and the number of training epochs.

Ensemble Modelingcan be considered the most basic baseline for aggregating VLMs. It represents the base performance that the combination of VLMs can achieve on the target task when not trained. We implement an averaging ensemble (Equation (1)) of cosine similarity between VLM output and each choice of a question as our ensemble baseline.

### Overall Performance

In Table 2, we first observe that Cola-FT achieves state-of-the-art (SOTA) performance on four datasets (A-OKVQA, OK-VQA, e-SNLI-VE, VSR), with merely 1 epoch of instruction tuning and a medium-sized language model. In contrast, many previous SOTA methods require finetuning more epochs than Cola-FT (_e.g._, VLC-BERT, PromptCap on A-OKVQA). Some also use much larger language models, such as GPT-3 (175B)  and OPT (175B) . Cola-FT outperforms OFA-X on e-SNLI-VE, although the latter is finetuned on much more related tasks and data (c.f.

   &  &  &  \\    & **Model Spec.** & **FT \(\)** & & **Model Spec.** & **ICL \(\)** & **FT \(\)** \\    \\  MetaLM  & Pretrained Encoder & 350k steps & MetaLM (1.3B) & - & 350k steps & 41.1 \\ PNP-VQA  & BLIP-Caption (446M) & - & UnifiedQAv2  (11B) & 0-shot & - & 63.3 \\ BLIP-2  & CLIP  (1.2B trainable) & 5 epochs & FLAN-TS (3B) & - & - & 81.6 \\ BLIP-2  & CLIP  (1.2B trainable) & 5 epochs & OPT  (6.7B) & - & - & 82.2 \\  Ensemble &  & - & - & - & - & - & 68.0 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 69.1 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **83.7** \\   \\  PromptCap  & OFA (472M) & 2 epochs & GPT-3 (175B) & 0-shot & - & - 73.2 \\ Img2Prompt  & BLIP (384M) & - & OPT (175B) & 0-shot & - & 42.9 / 40.7 \\ Prophet-MC  & MCAN-large  (56M) & 6 epochs & GPT-3 (175B) & 16-shot & - & 76.4 / 73.6 \\  Ensemble &  & - & - & - & - & 56.6 / 54.9 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 65.4 / 61.6 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 70.4 / 66.5 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & 77.7 / 74.0 \\ 
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 68.0 / 66.5 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 72.3 / 72.3 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **78.1 / 76.7** \\
**Cola-Zero** & XL+XXL & - & Vicuna (7B) & 2-shot & - & 63.9 / 63.0 \\
**Cola-FT** & (3B+11B) & - & Vicuna (7B) & - & 1 epoch & 68.6 / 66.9 \\
**Cola-Zero** & & - & Mistral (7B) & 2-shot & - & 69.3 / 66.2 \\
**Cola-FT** & & - & Mistral (7B) & - & 1 epoch & 74.3 / 71.8 \\   \\  PromptCap  & OFA (472M) & 2 epochs & GPT-3 (175B) & 0-shot & - & 58.8 \\ Prophet  & MCAN-large  (56M) & 6 epochs & GPT-3 (175B) & 16-shot & - & 61.1 \\  Ensemble &  & - & - & - & - & 39.2 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 39.4 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 39.4 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **62.4** \\    \\  e-UG  & UNITE (86M) & 400 epochs & GPT-2 (117M) & - & 400 epochs & 79.5 \\ OFA-X  & OFA (472M) & 10 epochs & - & - & 80.9 \\  Ensemble &  & - & - & - & - & 48.8 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 56.2 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 57.8 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **81.6** \\   \\  VisualBERT  & VisualBERT (110M) & 100 epochs & - & - & - & 54.0 \\ LXMERT  & LXMERT (110M) & 100 epochs & - & - & - & 63.2 \\ ViLT  & ViLT (88M) & 30 epochs & - & - & - & 62.4 \\  Ensemble &  & - & - & - & - & 51.4 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 55.8 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 54.9 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **67.0** \\   \\  BLIP  & BLIP (384M) & - & - & - & - & 41.7 \\ OFA  & OFA (472M) & - & - & - & - & 58.0 \\ VisProg  & ViLT (88M) & - & GPT-3 (175B) & 8-shot & - & 50.5 \\ 
**Cola-FT** & BLIP+OFA (384M+472M) & - & FLAN-TS (11B) & - & 1 epoch & **60.3** \\   \\  InstructBLIP  & XL (3B) & - & - & - & - & 33.7 \\ XXL (11B) & - & - & - & - & 16.6 \\ 
**Cola-Zero** & InstructBLIP & - & FLAN-TS (11B) & 2-shot & - & 34.4 \\
**Cola-FT** & XL+XXL (3B+11B) & - & FLAN-TS (11B) & - & 1 epoch & **54.3** \\  

Table 2: **Overall performance.** Model Spec. denotes specification where we summarize the detailed VLMs and LMs adopted in each method and their parameters. FT and ICT denote finetuning and in-context learning, respectively. Downward arrows indicate that fewer FT and ICT are more efficient. The accuracy metric varies slightly in different datasets. In A-OKVQA, we report both val/test accuracies, and val accuracy in VQA v2, OK-VQA, e-SNLI-VE, GQA, and CLEVR; test (zero-shot split) accuracy in VSR. Upward arrows indicate higher accuracy is better. We mark the best performance on each dataset with **bold font** and second-best with underlines.

Cola-FT is trained on each one dataset only in Table 2). In addition, the lighter variant Cola-Zero also achieves comparable performance to most baseline methods through in-context few-shot and zero-shot learning, without training any model parameter. To evaluate the performance of Cola with large multimodal models that embed large language models inside, we also assembled InstructBLIP  models based on FLAN-T5 XL and XXL and tested on A-OKVQA dataset. In Table 2, we report the multiple-choice accuracies on A-OKVQA. See Appendix A.4 for direct answer results.

### Qualitative Examples

In Figure 2, we exhibit several qualitative examples. The language coordinator determines the correctness of VLM plausible answers implicitly, given their captions and the caption and answer labels. The leftmost example (a tennis player playing) demonstrates a case when captions are not informative to guide the LLM for predictions. Between OFA and BLIP's plausible answers, the LLM follows the answer of BLIP. In contrast, in the left example (an oven next to a fridge), again with trivial captions, the LLM follows OFA's plausible answer instead.

It's all plausible answers, captions, VLM answer/caption labels, and the world knowledge the LLM encodes in itself, that contribute to the final decision of the language coordinator. The rightmost example presents the scenario of inconsistency between captions and answers. OFA describes the image as _"an elephant is loaded onto a truck in yangon."_ Though, it agrees that _"the truck is away from the elephant"_. With Cola-FT, The LLM coordinates OFA's correct caption and BLIP's correct answer to make a reasonable prediction.

Notably, we observe a scenario in which captions can be more informative than plausible answers to guide LLM. The right example (a puppy running) presents an uninformative image. Though neither OFA nor BLIP succeeds in answering the question, the LLM chooses to answer with "maybe" based on the given visual context. See Appendix A.5 and Appendix A.6 for more analysis on qualitative examples and failure cases.

Figure 2: **Qualitative examples. The correct choices are underlined. Leftmost: a commonsense question example of A-OKVQA; LLM follows the answer of BLIP. Left: a visual question example of A-OKVQA; LLM follows the answer of OFA. Right: an example of e-SNLI-VE; LLM chooses another option after coordination. Rightmost: an example of VSR; LLM predicts based on the caption of OFA and the answer of BLIP. Cola-Zero answers are referenced in zero-shot settings. The bottom row, Cola-FT (swapped VLM answer labels), indicates that the LLM follows the answer of certain VLMs based on their separate functionalities. LLM answers are associated with the distribution of VLM answer labels.**

### Coordination Analysis

Overall, Figure 3 validates the efficacy of Cola to coordinate VLMs. All the experiments use the same prompt template as in Table 1 unless otherwise stated. On A-OKVQA validation set, the performance of a single VLM (w/o FLAN-T5) is 50.83% for BLIP, or 54.75% for OFA. To validate the effectiveness of multi-VLM collaboration, we first ablate single-VLM variants of Cola-FT, shown as #1 (OFA only, without BLIP) and #2 (OFA only, without OFA) from the top. As expected, both fall behind Cola-FT (#8) by a large margin. With both VLMs, we ablate VLMs' captions (#3) and VLMs' plausible answers (#4), which reveal that plausible answers are much more significant in helping the LLM answer visual reasoning questions. Next, we perturb caption labels by swapping the VLM caption labels at instruction tuning and evaluation (#5), specifically _"OFA's description: "_ and _"BLIP's description: "_, by a chance of 50%. Under such settings, the LLM fails to acquire the preferred patterns of VLM for captioning, though the overall visual context is preserved. The results underperform Cola-FT, which verifies that VLM caption labels improve Cola-FT performance. Notably, the VLM (plausible) answer labels are more important to the LLM's decision: a considerable gap exists between (#6) and Cola-FT. In #6, the LLM fails to learn the separate functionalities of VLM when answer labels are perturbed. This highlights that the performance gained from the _coordination_ between BLIP and OFA, but not the strong reasoning capabilities of the LLM, FLAN-T5.

Naturally, we ask _what if the LLM can learn the patterns each VLM answers, but they cannot apply it at inference?_ We input correct VLM answer labels at instruction tuning and swap labels at evaluation (#7). Consequently, #7 falls behind Cola-FT with a smaller but still considerable margin. The results suggest that learning and applying the separate functionalities of VLMs is important for the coordinator LLM to make predictions. See Appendix A.9 for more ablation studies.

### Scaling Cola with More VLMs.

By decoding the top-k (k=5) results from three identical (OFA-base) models on the A-OKVQA validation set, both the answers and captions may exhibit slight variations. Cola demonstrated significant performance improvements compared to a single VLM or ensemble, as shown in Table 3. Furthermore, the performance gap between the ensemble baselines and Cola based on three different models (OFA-tiny/medium/base) is even more substantial, as depicted in Table 4.

  
**Methods** & **A-OKVQA** & **e-SNLI-VE** \\  OFA-base (1) & 45.76 & 52.60 \\ OFA-base (2) & 46.07 & 51.70 \\ OFA-base (3) & 45.73 & 52.33 \\  Ensemble (majority voting) & 44.79 & 52.71 \\ Ensemble (average) & 46.04 & 52.25 \\ 
**Cola-Zero (2-shot)** & 47.71 & 54.42 \\
**Cola-FT** & 48.85 & 56.92 \\   

Table 3: **Performance of ensemble methods based on three identical models.**

Figure 3: **Ablation study results** using a single VLM (#1, #2 from top), VLMs’ plausible answers only (w/o captions, #3), VLMs’ captions (w/o plausible answers, #4), perturbed VLM caption/answer labels at instruction tuning (#5, #6), and swapped answer labels at evaluation (#7). In #6, the coordination prior cannot be learned by the LLM. In #7, the coordination prior can be learned by the LLM, but cannot be properly applied at evaluation. FT: instruction tuning; Eval: evaluation.

  
**Methods** & **A-OKVQA** & **e-SNLI-VE** \\  OFA-base (1) & 45.76 & 52.60 \\ OFA-base (2) & 46.07 & 51.70 \\ OFA-base (3) & 45.73 & 52.33 \\  Ensemble (majority voting) & 44.79 & 52.71 \\ Ensemble (average) & 46.04 & 52.25 \\ 
**Cola-Zero (2-shot)** & 47.71 & 54.42 \\
**Cola-FT** & 48.85 & 56.92 \\   

Table 4: **Performance of ensemble methods based on three different models.**We further scale the number of OFA-base and find that Cola-FT saturates at 5 VLMs (49.77%) and Cola-Zero (2-shot in-context learning) sutures at 3 VLMs (47.71%). We observe that long input harms the performance of Cola-Zero, which is negative for scaling with more VLMs (Figure 4). LMs with a larger context window size  are promising to further improve the performance of Cola, which we leave for future works.

Scaling Model Size.We conduct experiments on scaling the coordinator LLM size to see if there are ramifications when operating at a larger scale. Figure 6 reveals that Cola-FT performance increases as the LLM (FLAN-T5) model size increases. Notably, Cola-FT/small, with only 80M parameters, could achieve 65% MC accuracy on A-OKVQA validation set, which is far beyond our baseline methods (55%). Cola-Zero, under the in-context learning paradigm, achieves competitive performance when the model grows to a billion-parameter scale. This observation on Cola-Zero can be regarded as a proof-of-concept that potentially reveals Cola-Zero's emerging abilities (inherited from FLAN-T5 ) on visual reasoning tasks at a relatively large scale. Cola-FT is effective with small models, but Cola-Zero is an emerging ability on larger models only.

In-context Learning & Low-data Instruction Tuning.We conduct experiments on different data scales to verify Cola's performance varying from zero-shot to full-shot under in-context learning and full-finetune paradigm. As shown in Figure 6, with Cola-Zero, few-shot exemplars substantially improve performance compared to zero-shot learning. As  revealed, exemplars potentially help the model better understand the output format and understand the instructions in Table 1. Cola-Zero for in-context few-shot learning outperforms zero-shot learning by a large margin, being on par with low-data Cola-FT without instruction tuning. We also observe Cola-FT's substantial performance gain when finetuning shots increase to 1000 and beyond.

### Saliency Visualization

As shown in Figure 7, we visualize the importance of the input prompt tokens by input-gradient saliency feature attribution , implementing with Ecco . The input tokens that are more relevant to predict the output token "grass" are highlighted in darker colors. In the given example, both Cola-FT and Cola-Zero predict the correct answer and find the relevant clues from visual context and plausible answers. Figure 7(b) shows that Cola-Zero attributes the output more to the instructions in the prompt template. This explains Cola-Zero's competitive performance, a consequence of FLAN instruction tuning . After instruction tuning, Cola-FT focuses more on the most informative parts of input: the question, choices, as well as VLMs' plausible answers.

### Can \(}\) Cola-FT Explain its Answers?

We modify the prompt template of Cola so that the model would output the logical conduction process, allowing us to observe the specific behaviors of the LLM during its coordination of VLMs.

Figure 4: **Scaling of # Models.**

Figure 5: **Cola performances versus the LLM (FLAN-T5) sizes.** Figure 6: **Low-data Cola-FT and Cola-Zero performances.**\(x\)-axis is distorted for optimal display.

We finetune Cola-FT to output rationales before answers, by A-OKVQA ground truth rationales. In the modified prompt, we ask the model to provide rationales. For the leftmost example of Figure 8, Cola-FT outputs _"Rationale: People might sit here to rest. The umbrellas are on the riverwalk. The answer is:rest "._ OFA gives a reasonable answer (but out of choices) to the question while BLIP gives an irrelevant answer. In this case, both answers are wrong. However, either Cola-Zero or Cola-FT is able to infer from captions and plausible answers to give the correct answer "to rest". The rationale suggests that the LLM understands the scene that the umbrellas are on the riverwalk and guesses that people might sit here to rest based on commonsense. The final answer is correct. For the leftmost example of Figure 11, the rationale output is _"The bike is parked in a no parking zone. The bike is parked next to a pedestrian crossing sign. The answer is:no parking"_. Both VLMs are wrong in their plausible answers. OFA's answer "boating" is semantically correct as the correct answer is "kayaking", though it's not the correct answer because this is a multiple-choice question. Cola-Zero gives a wrong answer "OFA" which is obviously wrong because "OFA" is the name of one of the VLMs given in the prompt and it's out of the choices too. However, Cola-FT gives the correct answer "kayaking", recognizing the correct choice based on prompts of captions and plausible answers after being finetuned. Even though the OFA and BLIP captions fail to identify that the people in the water are on a canoe. The LLM identifies that the people in the water are associated with the canoe. The rationale is valid and helpful, though repetitive. The final answer is correct.

To force the LLM to output rationale does not improve the reasoning performance of Cola (w/t rationale 74.3% vs. w/o rationale 77.7%, on A-OKVQA val set). This might be attributed to the low-quality ground truth rationales provided by the A-OKVQA dataset that we use to train the LLM. Such rationales are just short and objective descriptions of the scene, without suggesting the underlying outside knowledge to answer the question. Therefore, training the LLM to output rationale is harmful, though it derives insights into the LLM's behaviors during reasoning.

### Does \(\) Cola-FT Transfer across Tasks?

We examine Cola-FT's generalization ability across tasks. From Table 5, we observe the zero-shot performances on target datasets after instruction tuning on a certain source dataset. Although each dataset varies in question types and prompt templates (see detailed comparisons in Appendix A.7), we find that Cola-FT maintains competitive performance when zero-shot transferred to a new task, outperforming Cola-Zero in-context 2-shot learning and ensemble baselines (see also Table 2).

## 4 Related Work

**Visual Reasoning.** Beyond unimodal reasoning tasks such as question answering (QA) [100; 14; 119; 7], visual reasoning extends high-level cognition to visual domains, requiring an intelligent agent to derive rational solutions [40; 35; 84; 120; 38; 112]. Several tasks have been introduced to address

Figure 7: **Visualization of input token saliency. We visualize the relevancy between input tokens and the output token "grass" by feature attribution . The more salient tokens are highlighted in darker boxes. Cola-FT focuses on the question, choices, and VLMs’ plausible answers in (a). While as shown in (b), Cola-Zero pays extra attention to instructions and VLM labels, as a consequence of FLAN-T5 instruction tuning .**

visual reasoning, such as VQA , in which models are expected to provide answers to questions related to an image, and visual entailment , where the model is required to determine if a text description is consistent with the visual content provided.

Classic visual reasoning methods employ an image encoder along with a reasoning block that utilizes attention mechanisms , neuro-symbolic methods , or external knowledge .

Recent progress in large pretrained models has led to the development of LLMs that capture exceptional commonsense reasoning capabilities . These LLMs can potentially replace the reasoning module in visual reasoning tasks, and LLMs' lack of perception can be compensated by incorporating multiple VLMs trained on different domains . However, there is still a lack of research on how to harness the collective power of these separate VLMs for visual reasoning tasks. More related works are in Appendix B.

**Model Ensemble.** Model ensemble is a powerful machine learning technique that combines the predictions of multiple models to improve the overall performance of a given task . The variance and bias of the final predictions decrease, resulting in a more robust and accurate model . To this end, common methods include averaging , voting , interpolation , weighting the predictions based on model performance , or stacking the models .

Ensemble methods have been challenging for _generative_ tasks like visual reasoning, where a simple combination is not applicable to heterogeneous models due to their enormous and varying input/output token spaces. To address the issue, Socratic Models (SMs)  use prompt engineering to guide the heterogeneous pretrained multimodal models through natural language discussions. With a similar goal,  proposes a closed-loop iterative consensus optimization method to utilize the strengths of individual models. However, previous methods do not fully adapt to the intrinsic patterns of different models, particularly in the visual reasoning scenario. Recent studies, such as CICERO , have shown that LLMs possess strong social intelligence in coordinating multiple agents, which inspires us to reorganize pretrained mixed-modal models with a focus on adapting LLMs. More recently, Toolformer  and HuggingGPT  further demonstrate LLMs' abilities to leverage, coordinate, and incorporate the results from external sources such as other models or even APIs to solve complex tasks. While the external tools are called in sequential order in existing work, we study coordinating multiple tools (specifically, expert models) in parallel in this work.

## 5 Discussion

**Question Format.** Datasets like VQA v2 and OK-VQA contain open-ended questions, while A-OKVQA, e-SNLI-VE, and VSR use multiple-choice. Converting VQA v2 and OK-VQA to classification introduces complexities for traditional ensemble methods, as evident in Table 2. Classic methods struggle with generative models like API-based GPT-4, underscoring Cola's value as an end-to-end ensemble strategy for extensive (vision-)language models. Moreover, Cola-Zero's efficiency also relies on the question format - it's easier for LLMs to answer when given choices like in A-OKVQA. Conversely, Cola-FT finetunes LLMs to discern answer formats (Figure 7).

**Limitations.** Visual reasoning is a diverse topic. This work demonstrates the first step toward applying end-to-end language models for visual reasoning. While the LLMs perform well on the discussed datasets, there is a large body of visual reasoning tasks to evaluate in future works, such as intention prediction and rationale explanation.

**Future Works.** First, exploring the use of non-parametric tools for visual reasoning would be useful to enhance Cola's performance. Second, Cola's use can be extended to other reasoning and planning tasks, such as image generation and action planning, by coordinating multiple models in parallel. Third, by improving inter-model communications, Cola can be more interpretable and safe for high-stakes applications.

**Conclusion.** In this paper, we have proposed a novel paradigm for visual reasoning that harnesses the power of multiple VLMs by utilizing a coordination mechanism, where an LLM acts as a coordinator who communicates with VLMs to integrate their respective strengths. Experiments show that reasoning performance is substantially improved by LLM finetuning or in-context learning. Our results provide a promising step towards building multi-component intelligent systems that capture multimodal reasoning capabilities in a human-like way.