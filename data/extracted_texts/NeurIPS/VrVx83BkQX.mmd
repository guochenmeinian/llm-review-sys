# Stepwise Alignment for Constrained

Language Model Policy Optimization

 Akifumi Wachi\({}^{*}\)   Thien Q. Tran\({}^{*}\)   Rei Sato\({}^{}\)   Takumi Tanabe\({}^{}\)   Youhei Akimoto\({}^{@sectionsign}\)

\({}^{}\)LY Corporation  \({}^{}\)University of Tsukuba  \({}^{@sectionsign}\)RIKEN AIP

{akifumi.wachi, tran.thien, sato.rei, takumi.tanabe}@lycorp.co.jp

akimoto@cs.tsukuba.ac.jp

Equal contribution.

###### Abstract

Safety and trustworthiness are indispensable requirements for real-world applications of AI systems using large language models (LLMs). This paper formulates human value alignment as an optimization problem of the language model policy to maximize reward under a safety constraint, and then proposes an algorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One key idea behind SACPO, supported by theory, is that the optimal policy incorporating reward and safety can be directly obtained from a reward-aligned policy. Building on this key idea, SACPO aligns LLMs step-wise with each metric while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO offers several advantages, including simplicity, stability, computational efficiency, and flexibility of algorithms and datasets. Under mild assumptions, our theoretical analysis provides the upper bounds on optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness. Code and models are available at https://github.com/line/sacpo.

Warning: This paper contains content that may be offensive or harmful.

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable capabilities in diverse real-world applications  such as translation , content creation , coding , summarization , medicine , and robotics , among others. As the utilization of LLMs in artificial intelligence (AI) systems permeates our daily lives, the importance of responsible and ethical use grows; _safety_ issues have been highlighted . Consequently, as AI continues to evolve and become more integrated into society, it is crucial that we actively research and develop solutions to ensure that the benefits of AI are realized while minimizing negative societal impacts.

To address this challenge, _alignment_ has been used to embed human values and goals into LLMs to enhance their utility and safety. Notably, alignment based on human feedback has emerged as a key mechanism in making LLMs more helpful and harmless, as exemplified by reinforcement learning from human feedback (RLHF, ). Standard RLHF training flows fit a reward model to a human preference dataset and then optimize a language model (LM) policy to maximize the reward without overly diverging from the original policy. However, RLHF measures the quality of outputs in terms of a single metric (i.e., reward); thus, the achieved level of safety is not usually high, and a model that refuses to answer, while technically considered harmless, renders the response quite unhelpful .

Safety and trustworthiness in AI are inherently multifaceted concepts [5; 11]. To ensure that AI systems are accepted by society, we must consider multiple metrics on safety and trustworthiness beyond harmlessness, encompassing notions such as bias, security, robustness, fairness, and privacy . For instance, even if an LLM generates helpful outputs, we cannot deploy it if toxic, biased, or prejudiced outputs are likely to be generated. Given the complexity of modeling such diverse metrics using a singular reward function, it is a natural approach to formulate this problem using safety constraints.

Safe RLHF  is a pioneering approach for introducing the (constrained) safe RL paradigm into the alignment of LLMs. As with the standard RLHF pipeline, Safe RLHF trains separate reward and safety models from the human preference datasets and then employs RL to optimize an LM policy. This approach facilitates the acquisition of LLMs that strike a well-balanced compromise between reward (i.e., helpfulness) and safety (i.e., harmlessness). However, the Safe RLHF pipeline is inherently more complex than the standard RLHF, as it necessitates 1) fitting separate reward and safety models to preference data and then 2) learning a policy via PPO-Lagrangian  that simultaneously optimizes an additional parameter (i.e., Lagrangian multiplier) to balance helpfulness and harmlessness. In addition, Safe RLHF often suffers from an issue called _exaggerated safety behaviors_, which results in the model generating harmless but unhelpful responses.

**Our contributions.** We propose an algorithm called Stepwise Alignment for Constrained Policy Optimization (SACPO) for human value alignment of LLMs while incorporating decoupled reward and safety metrics. As shown in Figure 1, SACPO is a stepwise approach that sequentially aligns an LLM with one metric (e.g., reward) and subsequently with another (e.g., safety). Our theoretical analysis allows us to employ simple RL-free alignment algorithms such as direct preference optimization (DPO, ) or Kahneman-Tversky optimization (KTO, ) for each alignment without necessitating explicit reward or safety modeling. In a theoretically justified way, SACPO enables us to use different alignment algorithms or parameters for each alignment, thereby enhancing the flexibility of the format or volume of the datasets. To enhance the practicality of SACPO, we further propose an efficient approach called P-SACPO using model merging to balance the trade-off between reward and safety performance. We provide theoretical results on the optimality and safety of the LM policy of SACPO under mild assumptions. Finally, our experimental results show that SACPO can fine-tune Alpaca-7B better than Safe RLHF in terms of both helpfulness and harmlessness.

## 2 Preliminaries

Given a pre-trained LLM, alignment is conventionally conducted in two stages [7; 34; 57]. In the first stage, called _supervised fine-tuning (SFT)_, a pre-trained LLM is fine-tuned with a cross-entropy loss over high-quality human completion, resulting in a model \(_{}\). This stage enables the model to predict the next token more properly on data more relevant for downstream tasks (e.g., dialogue, summarization). The second stage, _learning from human feedback_, aims to better align LLMs to human desiderata . Because this paper focuses on the second stage, we will review the existing representative approaches and algorithms for the second stage. Specifically, we will briefly review RLHF  and then explain subsequent RL-free approaches such as DPO  or KTO .

Figure 1: Safe RLHF  respectively fits reward and safety models to reward and safety datasets with human preferences, and then leverages PPO-Lagrangian to optimize an LM policy and a Lagrangian multiplier to balance helpfulness and harmlessness. In contrast, SACPO first aligns an LM policy with the reward metric and then realigns the resulting reward-aligned policy with the safety metric (or vice versa). In this process, we can use simple RL-free algorithms (e.g., DPO, KTO) for each step, which leads to simplicity, stability, and flexibility.

### Reinforcement Learning from Human Feedback (RLHF)

The standard RLHF pipeline consists of the following two phases: 1) _reward modeling_ and 2) _RL fine-tuning_ phases. With a prompt \(x\), an LLM is regarded as a (stochastic) policy to return an output \(y\), where \(\) and \(\) are respectively the finite spaces of prompts and outputs. Here, we assume access to a dataset of preferences \(:=\{(x^{(i)},y_{w}^{(i)},y_{l}^{(i)})\}_{i=1}^{N}\), where \(y_{w}\) and \(y_{l}\) denote preferred and dispreferred outputs (i.e., \(y_{w} y_{l}\)) for a prompt \(x\) and \(N_{+}\) is the number of data. Paired outputs \((y_{w},y_{l})\) are typically sampled from \(_{}\).

**Reward modeling.** In the first stage, the preference dataset \(\) is assumed to be generated by a latent (unknown) reward model \(r^{}\). A typical choice is the Bradley-Terry (BT) model , which stipulates that the human preference distribution \(p^{}\) is written as

\[p^{}(y_{w} y_{l} x)=(x,y_{w}))}{ (r^{}(x,y_{w}))+(r^{}(x,y_{l}))}= r^{}(x,y_{w})-r^{}(x,y_{l}),\] (1)

where \(()\) is the logistic function. A reward model \(r_{}\) is trained to serve as a proxy for minimizing the following negative log-likelihood of the human preference data:

\[_{(x,y_{w},y_{l})}[-r_{}(x, y_{w})-r_{}(x,y_{l})].\] (2)

**RL fine-tuning.** In the second stage, we aim to maximize the reward while leveraging a reverse KL divergence penalty to restrict how far the LM policy can drift from the base reference policy \(_{}\), namely the SFT model \(_{}\). Let \(_{}\) denote the LM policy we are optimizing. We then solve the following policy optimization problem to maximize

\[_{,_{}}[\,r_{}(x,y)\,]- _{}[\,_{}(y x)\,\|\,_{}(y x)\,],\] (3)

where \(\) is a distribution of prompts used in the RL phase, and \(_{,}[]\) is an abbreviated notation for \(_{x,y( x)}[]\) for any policy \(\). Also, \( 0\) is a hyperparameter for the KL penalty. Since this objective is not differentiable, RLHF leverages common RL algorithms such as PPO  as in Ouyang et al.  or REINFORCE  as in Ahmadian et al.  to optimize it.

Direct Learning from Human Feedback _without_ RLHF (especially when based on PPO) is computationally expensive and unstable in practice; thus, many algorithms (e.g., [36; 6; 20]) have been proposed to overcome the issues. A common idea is to analytically derive the optimal policy of (3) and parameterize it using the reward function as follows:

\[_{r^{}}^{}(y x)=}(x;_{})} _{}(y x)(r^{}(x,y)).\] (4)

Here, for any function \(f:\) and policy \(\), \(Z_{f}(x;)\) is a normalization term or constant defined as \(Z_{f}(x;)_{y}(y x)( f(x,y))\). For the proof, see Appendix A. Note that, to derive the optimal policy (4), we do _not_ assume a specific reward structure such as the BT model. Thus, the overall structure of the optimal policy results from the problem setting (3) characterized by the KL divergence, which is common to the representative algorithms listed below.

**DPO.** Direct preference optimization (DPO, ) uses a functional mapping between the reward model and the optimal policy under the reverse KL divergence constraint as in (4). This algorithm has emerged as a more stable alternative to RLHF with competitive performance. DPO applies reparametrization to a reward function \(r\) using the parametrized policy \(_{}\):

\[r(x,y)=(y x)}{_{}(y x)}+  Z_{r}(x;_{}).\] (5)

Since \(Z_{r}(x;_{})\) neither depends on \(y\) nor \(\), by simply plugging the reward (5) into the loss function associated with the BT model (2), the resulting objective of DPO is given by

\[_{}(_{},_{},)=-_{(x, y_{w},y_{l})}[\,((y_{w}  x)}{_{}(y_{w} x)}-(y_{l}  x)}{_{}(y_{l} x)})\,].\] (6)

As a generalized extension of DPO, Azar et al.  later proposed \(\)PO characterized by a more general objective function exclusively expressed for pairwise preferences. Also, IPO is proposed as a specific case of \(\)PO to make the reward function bounded to avoid overfitting.

Kto.The algorithms discussed above need a preference dataset, which is costly for humans to collect. Ethayarajh et al.  proposed Kahneman-Tversky optimization (KTO) that needs only a binary signal of whether the output \(y\) is desirable (i.e., \(y_{+}\)) or undesirable (i.e., \(y_{-}\)) for a given prompt \(x\). With an unpaired dataset \(}\{(x^{(i)},y^{(i)})\}_{i=1}^{N}\), the loss function for KTO is calculated as

\[_{}(_{},_{},)=_{x,y }}[\,v_{}(x,y,)\,],\] (7)

where \(v_{}\) is called a value function that maps a latent reward \(r_{}(x,y)(y\,x)}{_{ }(y\,|x)}\), relative to some reference point \(\,_{}[\,_{}(y\, x)\,\|\,_ {}(y\, x)\,]\), to its perceived value:

\[v_{}(x,y,)w_{+}(1-(r_{}( x,y)-))& y y_{+} x\\ w_{-}(1-(-r_{}(x,y)))& y y_{-} x.\]

In the above equation, \(w_{+}\) and \(w_{-}\) are weight coefficients for desired and undesired outputs.

### _Safe_ and _Multi-objective_ Learning from Human Feedback

Though all the algorithms discussed above consider only a singular reward function, several algorithms incorporating constraints or multiple objectives have been recently proposed [56; 16; 55; 33].

Safe RLHF.To improve the helpfulness and harmlessness of LLMs, Dai et al.  introduce a safety function \(g^{}\) and then formulate the LLM alignment problem as a policy optimization task of maximizing the reward \(r^{}\) under a safety constraint. They propose Safe RLHF that first trains reward and safety models (i.e., \(r_{}\) and \(g_{}\)) using two datasets containing reward and safety information, and then solves the following problem using a popular safe RL algorithm called PPO-Lagrangian :

\[_{}\ _{,_{}}[\,r_{}(x,y)\,]- _{}[\,_{}(y x)\,\|\,_{}(y x )\,]_{,_{}}[\,g_{}(x,y) \,] 0.\] (8)

Safe RLHF requires us to fit separate reward and safety models and then concurrently optimize the LM policy and Lagrangian multiplier to balance helpfulness and harmlessness. Hence, Safe RLHF is a more complex and unstable procedure, even when compared to standard RLHF.

Multi-objective and constrained DPO.Zhou et al.  and Liu et al.  respectively propose extensions of DPO, called the multi-objective DPO (MODPO) and constrained DPO (C-DPO). A challenge common to both algorithms is the lack of flexibility regarding algorithms or datasets. Specifically, MODPO and C-DPO optimize LM policies using DPO while incorporating weighted summations of reward and safety. Hence, we must use DPO for each alignment 2 and prepare a dataset that contains the set of outputs \(\{y\}\) characterizing both reward and safety for each prompt \(\{x\}\). As an individual shortcoming, while MODPO still necessitates reward and safety modeling, C-DPO needs to iteratively apply DPO while updating the Lagrangian multiplier via gradient descent.

## 3 Problem Formulation

We consider a _safety-constrained_ LM policy optimization problem. Though conventional alignment is conducted only with respect to a single reward function \(r^{}:\), we additionally incorporate a safety function \(g^{}:\) and a threshold \(b\). We now define the following two functions:

\[R(,)_{,}r^{}(x,y)- _{}[\,(y x)\,\|\,_{}(y x)\,]  G()_{,}g^{}(x,y) .\]

Note that \(R(,)\) is the typical objective used in conventional (unconstrained) LM policy optimization methods such as RLHF, DPO, and KTO. To incorporate safety requirements, this paper considers the following constrained policy optimization problem, which is formulated as follows:

\[_{}\ R(,) G() b.\] (9)

Though this paper focuses on the case with a single safety function for ease of understanding, our key ideas can be easily extended to multiple safety functions. For more details, see Appendix B.1.

Datasets.To accommodate a wide range of situations, we relax an assumption about the dataset. We assume two different datasets exist: one for reward \(_{r}\) and the other for safety \(_{g}\). These two datasets do not have to share the same prompts \(\{x\}\). Crucially, we do _not_ restrict \(_{r}\) and \(_{g}\) to be (paired) preference datasets; that is, we accept such unpaired datasets that are used for KTO.

**Example scenarios.** Our problem formulation covers many real-world problems that LLMs face. Let us discuss the importance and potential usage of our formulation. One of the most direct scenarios is to reduce the _harmfulness_ of LLMs as in Dai et al. . When we define \(g^{}\) as a function to return a small value for a harmful (e.g., toxic, discriminative) answer \(y\) for a given prompt \(x\), our problem formulation and algorithm can be used for aligning LLMs to improve their helpfulness and harmlessness. Also, LLMs are known to be vulnerable to a variety of _bias_ in terms of politics , gender , verbosity , and so on. When \(g^{}\) is defined as a function to return a small value for a biased answer \(y\) for a given prompt \(x\), our problem formulation will help suppress such biases. A recently identified problem is that RLHF significantly reduces output _diversity_ compared to SFT . When we define \(g^{}(x,y)=-(y x)(y x)\), we obtain \(G()=()\) for increasing the diversity of an LM policy, where \(\) measures policy entropy. The above are only a few examples, but our problem formulation (9) has the potential to deal with a variety of real problems LLMs face.

## 4 Stepwise Alignment for Constrained Policy Optimization

We propose an algorithm called SACPO to solve the constrained policy optimization problem (9), which is outlined in Algorithm 1. SACPO takes a stepwise approach for reward and safety alignments of LLMs; that is, an LLM is first aligned for reward and then for safety (or vice versa). This operation is backed by theory, and the resulting optimal LM policy is guaranteed to be identical to the one aligned with reward and safety metrics simultaneously. In addition, by taking the stepwise approach, we can enjoy several practical advantages of enhanced flexibility regarding algorithms and datasets. Though we formulate a problem with a single safety function for simplicity, the arguments in this section are valid in the case of multiple safety functions. For more details, see Appendix B.2 and B.3. SACPO uses a standard Lagrangian  defined as \(L(,,) R(,)+(G()-b)\), where \(\) is the primal variable and \(_{+}\) is a dual variable or the Lagrangian multiplier. Note that, for any dual variable \(_{+}\), by using a composite function \(r^{}+ g^{}\), we can convert the original constrained policy optimization problem (9) into the following max-min problem:

\[_{}_{ 0}\ L(,,)=_{,} r^{}(x,y)+ g^{}(x,y)-_{}[\,(y x)\,\|\,_{}(y x)\,]- b.\] (10)

Unfortunately, it is not always advisable to solve the above problem as an unconstrained policy optimization problem by fixing \(\), which is known as _scalarization fallacy_. To proceed with our theoretical analysis, we thus make a mild assumption regarding the Slater conditions.

**Assumption 1** (Slater condition).: There exist a policy \(\) and \(_{+}\) such that \(G()-b\).

Practically, it is not hard to obtain such a conservative policy \(\). If the usefulness (i.e., reward \(r\)) can be ignored, it is easy to acquire policies that refuse to generate potentially unsafe answers and output safe answers conservatively. Based on Assumption 1, we present the following two lemmas.

**Lemma 1** (Strong duality).: _Define the dual function \(D(,)_{}L(,,)\) and the optimal dual variable \(^{}_{ 0}D(,)\). Under Assumption 1, there exists a primal-dual pair \((^{},^{})\) such that \(R(^{},)=^{}()=L(^{},^{},)\)._

**Lemma 2** (Boundness of \(^{}\)).: _Define \(,)-R(,)}{}\). Under Assumption 1, \(0^{}\) holds._

For the proofs, see Appendix C. Our problem setting is a special case of those in typical constrained Markov decision process (CMDP, ) literature. Thus, Lemma 1 follows from Theorem 3 in Paternain et al. , and Lemma 2 follows from Lemma 1 in Ding et al. .

### Optimal Policy Can be Directly Obtained from Reward-aligned Policy

To obtain the optimal policy \(^{}\) of the constrained policy optimization problem (9), we first present a theorem regarding the relation with \(^{}_{r^{}}\) defined in (4), which will lead to the key idea behind SACPO.

**Theorem 1** (Relation between \(^{}_{r^{}}\) and \(^{}\)).: _The optimal policy of (9) is represented as_

\[^{}(y x)=^{}_{r^{}}(y x)( }{}g^{}(x,y)) Y(x) +^{}g^{}}(x;_{})}{Z _{r^{}}(x;_{})}.\] (11)

**Remark 1** (Importance of reverse KL in (3) and (9)).: Though there are attempts (e.g., Wang et al. ) to use different divergences (i.e., \(f\)-divergence) in (3), Theorem 1 holds only for reverse KL constraint \(_{}\) since we used \((+)=()()\) for the derivation.

**Remark 2** (Commutative law).: Since the commutative law holds, alignment does _not_ have to be conducted in the order from reward to safety.

For the proof, see Appendix D. Intuitively, Theorem 1 states that we do _not_ have to align a policy for multiple metrics simultaneously and thus we can sequentially align the policy _stepwise_. Specifically, (11) means that the optimal policy \(^{}\) is identical to the one obtained by realignment of \(^{}_{r^{}}\) for the safety function \(g^{}\) with a parameter \(/^{}\). Thus, (11) justifies realigning the reward-aligned model \(^{}_{r^{}}\) for the safety function \(g^{}\). After a simple mathematical transformation of (11), we have

\[g^{}(x,y)=}(y x)}{ ^{}_{r^{}}(y x)}+} Y(x).\] (12)

Based on the fact that \( Y(x)\) neither depend on \(y\) nor \(\) for all \(x\), we then have

\[g^{}(x,y_{w})-g^{}(x,y_{l})=}(y_{w} x)}{^{}_{r^{}}(y_{w} x)}+}-}(y_{l} x)}{^{}_{r^{}}(y_{l} x)}-}.\]

Therefore, when realigning \(^{}_{r^{}}\) with respect to safety function \(g^{}\), we are allowed to optimize an LM policy in almost the same manner as presented in Section 2.2 with only difference from \((_{},_{},)\) to \((_{},^{}_{r^{}},/^{})\). For example, suppose we use DPO for this purpose, the resulting DPO loss is:

\[_{}(_{},^{}_{r^{}},/^{ })=-_{(x,y_{w},y_{l})_{y}}[( }(y_{w} x)}{^{} _{r^{}}(y_{w} x)}-}(y_{l} x)}{^{}_{r^{}}(y_{l} x)})].\]

The loss function slightly changed from \((_{},)\) in (6) to \((^{}_{r^{}},/^{})\). Such modification of the loss function is valid with other algorithms that explicitly use (4) for the deviation (e.g., IPO, KTO).

### Advantages of SACPO

By taking a stepwise approach, we can enjoy practical benefits. Let us highlight three major advantages of SACPO. The first advantage is the flexibility of alignment algorithms (e.g., DPO or KTO) and datasets. In practice, depending on the metric, appropriate human feedback should be different (e.g., paired vs. unpaired). SACPO takes a stepwise approach, which allows us to use different algorithms, parameters, or datasets for each metric. Second, we can evaluate the resulting LM policy after each alignment regarding the target metric. This process enables us to prevent starting over the alignment from the beginning. Finally, SACPO justifies us to realign pre-aligned LLMs with our desired metric. This property is practically desirable because we now have easy access to high-quality, open-source LLMs that have been already aligned.

## 5 Theoretical Results

This section provides theoretical results. Specifically, we provide the upper bounds on the optimality and safety constraint violation of the policy obtained by SACPO. While SACPO does _not_ explicitly estimate the reward and safety, (4) and (11) tell us that the policies are secretly reward or safety models. Hence, we first analyze the uncertainty of the estimated reward and safety functions and then derive the bounds on the performance of the policy trained via SACPO. As a key notion in our theoretical analysis, let us define an uncertainty quantifier as follows.

**Definition 1** (\(\)-uncertainty quantifier).: Let \(_{}\) be the data-collecting process. Let \(f^{}\) and \(\) denote the true function and its maximum likelihood estimator (MLE), respectively. For a dataset \(\), we say \(_{f,}:_{+}\) is a \(\)-uncertainty quantifier if the event \(=|\,f^{}(x,y)-(x,y)\,|_{f, }(x,y)(x,y)}\) satisfies \(_{}() 1-\).

Note that \(f\) represents \(r\), \(g\), or their weighted summation. In RLHF pipelines, the reward model is usually initialized from the SFT model by adding a linear layer on top of the final transformer layer to generate an estimated reward value. Recently, Xiong et al.  have provided theoretical analysis for RLHF and DPO under preference data and linear realizability assumptions. We extend their theory from unconstrained to constrained settings and from preference to a more general dataset.

**Assumption 2** (Linear reward and safety functions).: The reward and safety functions are parameterized by \((x,y)= w_{r},(x,y)\) and \((x,y)= w_{g},(x,y)\) for a shared feature mapping function \(:^{d}\). In addition, the true reward and safety functions satisfy \(r^{}(x,y)= w^{}_{r},(x,y)\) and \(g^{}(x,y)= w^{}_{g},(x,y)\) for some \(w^{}_{r},w^{}_{g}^{d}\). For regularization, we additionally assume \(\|(x,y)\| 1\) for any \((x,y)\) and \(\{\|w_{r}\|,\|w_{g}\|\} B\).

Based on Assumption 2, we can analytically construct \(\)-uncertainty quantifiers regarding the reward and safety estimations for both the paired (i.e., preference) and unpaired datasets.

**Lemma 3** (Reward and safety \(\)-uncertainty quantifiers).: _With a dataset \(\), define the covariance matrix estimation \(_{}+_{(x,y_{1},y_{2}) }(x,y_{1})-(x,y_{2})(x,y_{1})- (x,y_{2})^{}\) for paired dataset and \(_{}+_{(x,y)} (x,y)(x,y)^{}\) for unpaired dataset, where \(_{+}\) is a fixed positive value and \(^{d d}\) is the identity matrix. Also, define, \(_{}(x,y)\|(x,y)\|_{_{D}^{-1}}\), where \(\|\|_{}^{}}\) is the matrix Mahalanobis seminorm. For paired dataset, with \( 2+e^{B}+e^{-B}\), set \(=((d+(1/))+ B^{2}})\). For unpaired dataset, set \(=B(1+)\). Then, MLEs for reward and safety functions (i.e., \(\) and \(\)) respectively satisfy_

\[|\,r^{}(x,y)-(x,y)\,|_{_{ r}}(x,y)|\,g^{}(x,y)-(x,y)\,| _{_{g}}(x,y).\]

_for all \((x,y)\), with probability at least \(1-\)._

For the proof, see Appendix E. Lemma 3 implies that \(\)-uncertainty quantifiers can be constructed by defining \(_{r,_{r}}(x,y)_{_{ r}}(x,y)\) for reward and \(_{g,_{g}}(x,y)_{_{g}}(x,y)\) for safety. Let \([0,]\) denote an estimated Lagrangian multiplier. For any positive scalar \(c[0,]\), define

\[_{}(x,y,c)(\,_{ _{r}}(x,y)+c\,_{_{g}}(x,y))+|\,c- \,|B,\] (13)

We finally provide two main theorems regarding optimality and safety constraint violation.

**Theorem 2** (Optimality).: _Let \(\) denote the optimal policy induced by \((x,y)(x,y)+(x,y)\); that is \((y x)=}(x;_{})}_{ }(y x)(((x,y)))\). Then, the following inequality holds:_

\[R(^{},)-R(,)\] \[-^{}b+_{,^{}}[\, _{}(x,y,0)(_ {}(x,y,^{}))]+(_{ ,^{}}[(_{}( x,y,^{}))]).\]

**Theorem 3** (Safety constraint violation).: _Suppose that the SACPO algorithm identifies that \(\) satisfies the safety constraint based on its evaluation; that is, \(_{,}\,(x,y)\, b\). Then, we have_

\[[\,b-G()\,]_{+}\,_{,^{}}[\, _{_{g}}(x,y)(_{ }(x,y,^{}))].\]

For the full proofs, see Appendix F. The proof sketch is as follows. Define \(h^{}(x,y) r^{}(x,y)+^{}g^{}(x,y)\). Then, for any function \(f:\), we have the following equation:

\[J_{f}(^{})-J_{f}()=_{}[_{^{ }}f(x,y)-h^{}(x,y)+_{} \,(x,y)-f(x,y)+}(x;_{ {ref}})}{Z_{}(x;_{})}].\]

The second term on the right-hand side appears hard to handle due to \(_{}[]\) but is upper-bounded since \((y x)}{^{}(y x)}=}(x;_{ })}{Z_{}(x;_{})}((x,y) -h^{}(x,y)}{})\) holds by definition. Because each term is upper-bounded by leveraging the uncertainty quantifiers, we obtain the total upper bound by adding them together.

Theorems 2 and 3 suggest that, regarding both optimality and safety violation, the performance degradation compared to \(^{}\) is exponential to

\[\,_{}(x,y,^{})=\,_{_{r}}(x,y)+^{} \,_{_{g}}(x,y)+|^{}- |B.\]

Our theoretical results imply that accurate estimation of the reward and safety functions and a high-quality Lagrangian multiplier are required to achieve high reward and safety performance.

## 6 Practical Implementation

An unresolved issue remains about SACPO, namely _how to optimize_\(\). In the typical CMDP settings, since \(L(,,)\) is linear in \(\), primal-dual methods are popular for optimizing \(\) and \(\)[17; 27]. In this process, online convex optimization approaches (e.g., ) are often used for optimizing \(\) while evaluating the reward and safety performance of the current policy during training.

In the context of constrained LM policy optimization, however, a serious difficulty in optimizing \(\) is an unstable and noisy evaluation of the performance of an LM policy, which is inevitable given the nature of natural language. Although primal-dual approaches have been applied in constrained LM policy optimization problems , we suffer from large computational time and unstable learning due to repeated policy optimizations and inconsistent and noisy evaluation of the LM policy. Therefore, we should avoid updating \(\) while optimizing and evaluating an LM policy.

We now introduce a practical variant of SACPO  called P-SACPO. After obtaining a reward-aligned policy \(_{r}\), P-SACPO realigns it with the safety metric \(g\) while setting \(\) as a conservatively large scalar \(=\) such that \(>^{}\). We now own two LM policies: a reward-aligned policy \(_{r}\) (this can be regarded as an LM policy with \(=0\)) and a conservatively safety-realigned policy \(_{r+g}\). Under the assumption that \(^{}\) is between \(0\) and \(\), P-SACPO aims to find \(^{}\) without optimizing new LM policies. Specifically, we merge \(_{r+g}\) and \(_{r}\) by simply averaging their weights as in Wortsman et al.  with a mixing ratio of \(q:1-q\) for a scalar \(q_{+}\) (\(0 q 1\)). It is known that such a simple weight-averaging works well in the case of the same base model [19; 3]. All the models obtained by SACPO derive from the same SFT model. Therefore, SACPO is particularly compatible with model merging, and P-SACPO empirically performs well as evidenced by our experiments in Section 7.

## 7 Experiments

We empirically evaluate the effectiveness of SACPO and P-SACPO in enhancing multiple criteria stepwise. This experiment focuses on improving helpfulness and safety (i.e., harmlessness).

### Experiment Setups

We use the same experimental setup as in Safe RLHF  wherever possible for fair comparisons. We employ the same SFT model (i.e., a reproduced version of Alpaca-7B ). This model is trained to function as a proficient conversational assistant, generating both benign and harmful responses. We utilize the PKU-SafeRLHF preference dataset  with more than 30,000 expert evaluations. Each record in this dataset presents a pair of responses to a specific prompt, and response pairs are ranked according to helpfulness and harmlessness. While the harmlessness of a response is determined by its neutrality concerning 14 different risk categories, the helpfulness is judged based on factors such as clarity, relevance, and overall quality.

**Implementations.**  In this experiment, we apply DPO and KTO for each alignment on helpfulness and safety (i.e., harmlessness). Specifically, we implement the following four variants of SACPO: DPO (H) \(\) DPO (S), DPO (H) \(\) KTO (S), KTO (H) \(\) DPO (S), and DPO (S) \(\) DPO (H), where H and S are abbreviations of helpfulness and safety (i.e., harmlessness). We use TRL  for implementing DPO and KTO. As for the parameter associated with the reverse KL divergence penalty, we first set \(\) and then test a wide range of values of \(/\). As a result, we set \(=0.1\) when helpfulness is the first alignment metric and \(=0.01\) otherwise. In addition, to evaluate the performance of P-SACPO presented in Section 6, we implement linear model merging  between the helpfulness-aligned model and conservatively safety-realigned model with \(/=0.01\)trained via \(()()\). We use MergeKit  and test three different mixing ratios; that is, \(q\{0.25,0.5,0.75\}\). For more implementation details (e.g., hyperparameters), see Appendix G.

Baselines.We evaluate the models trained via (P-)SACPO compared with the SFT model and those trained via Safe RLHF. Safe RLHF owns three models (i.e., beaver-7b-v1.0, -v2.0, and -v3.0), depending on the number of iterations regarding data collection and fine-tuning. Crucially, our (P-)SACPO optimizes LM policies under the _same_ conditions as v1.0 and _less favorable_ conditions than v2.0 and v3.0, in terms of the quality and quantity of data. For \(\), the baseline method also includes naive linear merging that simply averages the weights of DPO (H) and DPO (S).

Evaluation.We use GPT-4  to measure the helpfulness and harmlessness (i.e., safety) of the responses generated by the LM policies. We base our prompts on those in the Safe RLHF study with a slight adjustment in output format requirements to get more reliable evaluations (for more details, see Appendix G.4). As for the prompts of the LLMs to be evaluated, we employ two non-overlap sets of prompts for helpfulness and safety, unlike the previous Safe RLHF study that used the same red-teaming prompts for evaluating both helpfulness and safety. Specifically, for assessing helpfulness, we use all the \(129\) prompts from the "helpful_base" subset of the AlpacaEval dataset  that are unlikely to result in harmful content. To evaluate safety, we use all the \(83\) (red-teaming) prompts in the Safe RLHF study, which has a high risk of inducing unsafe responses. When evaluations of helpfulness and harmfulness are coupled, safe models are likely to be evaluated as helpful. This means that safety-aligned models potentially obtain an unreasonably high evaluation regarding helpfulness. This is based on our observations in early experiments that DPO (S) or beaver-7b-v2.0 were valued as more helpful than we humans thought. In real applications with AI systems based on LLM, most of the prompts are benign and it is also important to generate helpful answers for benign prompts. Therefore, we decided to use benign prompts from the AlpacaEval dataset to assess the helpfulness and red-teaming prompts from Safe RLHF studies to assess the harmlessness, considering that the quality of the prompts is preferable for each evaluation.

### Experimental Results

Figure 2 shows the win rates of each model against the base SFT model. 34 First, Figure 2(a) illustrates the experimental results for DPO (H) \(\)DPO (S), DPO (H) \(\)KTO (S), and KTO (H) \(\)DPO (S). We

Figure 2: Win rate against the SFT model. H and S are abbreviations for helpfulness and safety (i.e., harmlessness), respectively. Crosses represent SFT and Safe RLHF, and blue circles represent models aligned with a single metric. (a) DPO (H) \(\)DPO (S), DPO (H) \(\)KTO (S), and KTO (H) \(\)DPO (S). (b) DPO (S) \(\)DPO (H). (c) P-SACPO based on linear model merging. In (a) and (b), the numbers indicate \(/\). In (c), the numbers for the red triangles represent \(/\), while those for the green and purple squares represent \(q\).

observe that DPO (H) and KTO (H) improve the performance on helpfulness at the first step. In DPO (H) \(\) DPO (S) and KTO (H) \(\) DPO (S), subsequent alignment for safety obtains a substantial improvement on harmlessness with a slight decrease in helpfulness. These models obtained by SACPO perform better than those obtained by Safe RLHF in terms of helpfulness and harmlessness. Notably, KTO (H) \(\) DPO (S) performs well, which supports our main claim that different types of datasets or algorithms can be used for each alignment. Also, we observe that varying the \(/\) ratio allows us to fine-tune the equilibrium (i.e., a near Pareto-front) between helpfulness and safety. This result indicates the flexibility of the proposed method in obtaining a model with a desired trade-off between multiple criteria. However, DPO (H) \(\) KTO (S) performs significantly worse than DPO (H) \(\) DPO (S). We guess this is because KTO is inappropriate for the safety alignment, but we will leave it to future work to identify the detailed reasons.

Effect of alignment order.Figure 2(b) shows the effect of "order" of the stepwise alignment. This experimental result shows that we basically obtain the models with comparable performance regardless of the order of alignments, which is consistent with our theory (i.e., Remark 2). On the other hand, we also observed that different alignment orders often lead to varying performance gaps, which is particularly noticeable at DPO (S) \(\) DPO (H) with \(/=0.01\). We hypothesize that the poor representation ability of the LLMs or optimization error regarding DPO might lead to this phenomenon, though we do not have a definitive explanation. This represents an interesting direction for future research to analyze the gap between theory and practice.

Performance of P-SACPO.Finally, Figure 2(c) shows the effectiveness of P-SACPO proposed in Section 6, showing that P-SACPO performs better than the naive method that simply averages the weights of DPO (H) and DPO (S). Linear model merging allows us to balance helpfulness and harmlessness by averaging reward-aligned and conservatively safety-realigned policies without optimizing new ones. Therefore, we can approximately find \(^{}\) for the constrained LM policy optimization problem (10) with reduced computational time and stable learning.

## 8 Conclusion

We have introduced SACPO, a simple algorithm for constrained language model policy optimization. SACPO takes a stepwise approach that sequentially aligns an LM policy using off-the-self alignment algorithms (e.g., DPO, KTO) and datasets for each metric. This procedure is theoretically justified and provides many practical benefits such as simplicity, stability, and flexibility. Our theoretical results include the upper bounds regarding near-optimality and safety constraint violations. Empirically, SACPO performs better than Safe RLHF in enhancing helpfulness and harmlessness, and we further show the effectiveness of a practical variant called P-SACPO based on linear model merging.

Limitations.SACPO has several limitations. First, while we evaluate SACPO using the models with 7B parameters, there is room for discussion on whether SACPO works well for state-of-the-art models with many more parameters. Second, although this paper focuses on safety alignment from the perspective of RLHF or DPO, it is more desirable to additionally incorporate the standard SFT as well as pre-check and post-check strategies. Finally, although SACPO is more efficient than existing methods, it still requires substantial computational cost or a large amount of high-quality data.

Broader Impacts.We believe SACPO contributes to the safety or trustworthiness of LLMs and will reduce the barrier to aligning future LLMs to enhance the benefits of AI while minimizing negative impacts. However, any LLMs are open to abuse, and models obtained by SACPO are not exceptions. Also, we must recognize that the core idea behind SACPO can be used to make LLMs more unsafe.