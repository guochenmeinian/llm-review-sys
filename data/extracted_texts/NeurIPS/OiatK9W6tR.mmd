# Quantum speedups for stochastic optimization

Aaron Sidford and Chenyi Zhang

{sidford,chenyiz}@stanford.edu

###### Abstract

We consider the problem of minimizing a continuous function given given access to a natural quantum generalization of a stochastic gradient oracle. We provide two new methods for the special case of minimizing a Lipschitz convex function. Each method obtains a dimension versus accuracy trade-off which is provably unachievable classically and we prove that one method is asymptotically optimal in low-dimensional settings. Additionally, we provide quantum algorithms for computing a critical point of a smooth non-convex function at rates not known to be achievable classically. To obtain these results we build upon the quantum multivariate mean estimation result of Cornelissen et al.  and provide a general quantum variance reduction technique of independent interest.

## 1 Introduction

Stochastic optimization is central to modern machine learning. Stochastic gradient descent (SGD), and its many variants, are used broadly for solving challenges in data science and learning theory. In theory, SGD and stochastic optimization, have been the subject of decades of extensive study, [19; 68; 27] established that SGD achieves optimal rates for minimizing Lipschitz convex functions1 (even in one dimension) and stochastic optimization methods have been established for a range of problems [72; 70; 73; 74]. More recently, the complexity of stochastic gradient methods for smooth non-convex optimization, e.g., critical point computation, were established [3; 29; 30; 49; 50].

Given the foundational nature of stochastic optimization and the potential promise and increased study of quantum algorithms, it is natural to ask whether quantum computation could enable improved rates for solving these problems. There has been work studying whether access to quantum counterparts of classic optimization oracle can yield faster rates for semidefinite programs [13; 12; 43; 75; 76; 54], convex optimization [21; 5; 20], and non-convex optimization [23; 63; 80]. Notably,  showed that with just access to the quantum analog of an _zeroth-order oracle_, i.e., an oracle that when queried at a point outputs the value of the function at that point, it is possible to simulate access to a classic gradient oracle with a single query. This tool immediately yields improved rates for, e.g., convex function minimization, with a zeroth-order oracle.

Unfortunately, despite this progress and the established power of quantum evaluation oracles, obtaining further improvements has been challenging. A line of work established a variety of striking lower bounds ruling out quantum speedups for fundamental optimization problems [31; 32; 82]. For example,  showed that when given access to the quantum analog of a _first-order oracle_, i.e., an oracle that when queried at a point outputs the value of the function as well as the gradient at that point, quantum algorithms have no improved rates for non-smooth convex optimization over GD and SGD when the dimension is large.  extended this result to the non-convex setting andshowed that given access to the quantum analog of a stochastic gradient oracle, quantum algorithms have no improved rates for finding critical points over SGD when the dimension is large.

In spite of these negative results, we nevertheless ask, _for stochastic optimization are quantum-speedups obtainable?_ Our main result is an answer to this question in the affirmative for dimension-dependent algorithms. We provide two different quantum algorithms for stochastic convex optimization (SCO) which provably outperform optimal classic algorithms. Furthermore, we provide a quantum algorithm for computing the critical point of a smooth non-convex function which improves upon the state-of-the-art. We obtain these results through a new general quantum-variance reduction technique built upon the quantum multivariate mean estimation result of Cornelissen et al.  and the multilevel Monte Carlo (MLMC) technique . We complement these results with lower bounds showing that one of them is asymptotically optimal in low-dimensional settings.

General notation.We use \(\|\|\) to denote the Euclidean norm and let \(_{R}():=\{^{d}\|- \| R\}\) and \([T]:=\{1,,T\}\). We use bold letters, e.g., \(,\), to denote vectors and capital letters, e.g., \(A,B\), to denote matrices. For a \(d\)-dimensional random variable \(X\), we refer to the trace of the covariance matrix of \(X\) as its variance, denoted by \([X]\). For \(f^{d}\), we let \(f^{*}:=_{}f()\) and call \(^{d}\)_\(\)-(sub)optimal_ if \(f() f^{*}+\) and \(\)_-critical_ if \(\| f()\|\). Moreover, we call a random point \(^{d}\)_expected \(\)-(sub)optimal_ if \(f() f^{*}+\) and _expected \(\)-critical_ if \(\| f()\|\). \(f^{d}\) is \(L\)_-Lipschitz_ if \(f()-f() L\|-\|\) for all \(,^{d}\) and \(\)_-smooth_ if \(\| f()- f()\|\|-\|\) for all \(,^{d}\). For two matrices \(A,B^{d d}\), \(A B\) denotes that \(^{T}A^{T}B\) for all \(^{d}\). We use \(}\) to denote the big-\(\) notation omitting poly-logarithmic factors in \(,^{},d,,,R\), and \(L\). When applicable, we use \(|()\) to denote possible garbage states2 that arise during the implementation of a quantum oracle (e.g., in Definitions (1) and (2)).

### Quantum stochastic optimization oracles

Here we formally define quantum stochastic optimization oracles that we study in this work.

Qubit notation.We use \(|\) to represent input or output registers made of qubits that could be in _superpositions_. In particular, given \(m\) points \(_{1},,_{m}^{d}\) and a coefficient vector \(^{m}\) with \(_{i[m]}|c_{i}|^{2}=1\), the quantum register could be in the quantum state \(|=_{i[m]}c_{i}|_{i}\), which is superposition over all these \(m\) points at the same time. If we measure this state, we will get each \(_{i}\) with probability \(|c_{i}|^{2}\). Furthermore, to model a classical probability distribution \(p\) over \(^{d}\) quantumly, we can prepare the quantum state \(_{^{d}})} |\). If we measure this state, the measurement outcome would follow the probability density function \(p\).

Quantum random variable access.We say that we have _quantum access to a \(d\)-dimensional random variable \(X\)_ if we can query the following _quantum sampling oracle_ of \(X\) that returns a quantum superposition over the probability distribution of \(X\) defined as follows.3

**Definition 1** (Quantum sampling oracle).: _For a \(d\)-dimensional random variable \(X\), its quantum sampling oracle \(O_{X}\) is defined as_

\[O_{X}|0_{^{d}}( )}|| (),\] (1)

_where \(p_{X}()\) represents the probability density function of \(X\)._

The garbage state in Definition 1 is a quantum analogue of classical garbage information that arises when preparing the classical sampling oracle of \(X\). When implementing the quantum sampling oracle in quantum superpositions however, this garbage information will appear in a quantum state and cannot be erased or uncomputed in general. In this work, we consider a general model where we make no assumption on the garbage state. See e.g.,  for a similar discussion of this standard use of garbage quantum states.

Observe that if we directly measure the output of \(O_{X}\), it will collapse to a classical sampling access to \(X\) that returns random vectors with respect to the probability distribution \(p_{X}\).

Quantum stochastic gradient oracle.When considering the problem of optimizing a function \(f^{d}\), we are often given access to a stochastic gradient oracle that returns a random vector from the probability distribution of the stochastic gradient.

**Definition 2** (Stochastic gradient oracle (Sgo)).: _For \(f^{d}\), its stochastic gradient oracle (SGO) \(_{}\) is defined as a random function that when queried at \(\), samples a vector \(()\) from a probability distribution \(p_{f,}()\) over \(^{d}\) that satisfies_

\[*{}_{() p_{f,}} ()= f(),^{d}.\]

_We say the oracle is \(L\)-bounded if_

\[*{}_{() p_{f,}}\| ()\|^{2} L^{2},^{d},\]

_and we say the oracle has variance \(^{2}\) if_

\[*{}_{() p_{f,}}\| ()- f()\|^{2}^{2}, ^{d}.\]

In this paper, we further assume quantum access to a stochastic gradient oracle, or access to a _quantum stochastic gradient oracle_ for brevity, that upon query returns a quantum superposition over the probability distribution \(p_{f,}()\).

**Definition 3** (Quantum stochastic gradient oracle (Qsgo)).: _For \(f^{d}\), its quantum stochastic gradient oracle (Qsgo) is defined as_

\[O_{}||0 |_{^{d}}}()}| |(),\] (2)

_where \(p_{f,}()\) is as defined in Definition 2._

Observe that if we directly measure the output of \(O_{}\), it will collapse to a classical stochastic gradient oracle that randomly returns a stochastic gradient at \(\).

Another standard assumption in previous works  that the stochastic gradient can be queried _simultaneously_, which means that the algorithm can choose the random seed \(\) that is being queried. We assume that in such setting there is an explicit probability distribution \(\) such that \(*{}_{}[(,)]= f( )\). Similarly, we define the quantum access to stochastic gradients allowing simultaneous queries, or access to a _quantum stochastic gradient oracle with simultaneous queries_ for brevity, that upon query returns \((,)\) in a quantum state.

**Definition 4** (\(\)-Sqggo).: _For \(f^{d}\) with its stochastic gradient \((,)\) indexed by random seed \(\) that satisfies_

\[*{}_{}(,)= f( )*{}_{}\|( ,)- f()\|^{2}^{2}, ^{d},\]

\[*{}_{}\|(,)-( ,)\|^{2}^{2}\|-\|^{2}, ,^{d}\] (3)

_its simultaneously queriable, \(\)-mean-squared smooth quantum stochastic gradient oracle (\(\)-SQ-Qsgo) is defined as_

\[O_{}^{S}|| |0|| |(,) |(,).\] (4)

### Results

Here we present our main results on new quantum algorithms for stochastic optimization. Our results and the prior state-of-the-art are summarized in Table1. Further, we discuss new quantum lower bounds that we establish for quantum variance reduction and stochastic convex optimization. Our algorithmic results leverage a common technique for quantum variance reduction introduced in the next Section2. This technique uses a combination of the quantum multivariate mean estimation result of Cornelissen et al.  and multilevel Monte Carlo (MLMC) .

Stochastic convex optimization.In this work we consider the quantum analog of the standard stochastic convex optimization (SCO) problem defined as follows.

**Problem 1** (Quantum stochastic convex optimization (QSCO)).: _In the quantum stochastic convex optimization (QSCO) problem we are given query access to an \(L\)-bounded QSCO \(O_{}\) (see Definition3) for a convex function \(f^{d}\) whose minimum is achieved at \(^{*}\) with \(\|^{*}\| R\) and must output an expected \(\)-optimal point._

Classically, it is known that simple stochastic gradient descent, e.g. \(x_{t+1}=x_{t}- g_{t}\), can solve QSCO with \((^{-2})\) queries. Further, this bound is known to be optimal in the worst case .

Nevertheless, we develop two quantum algorithms for Problem1 in Section3 and Section4, respectively. The query complexities of these algorithms are summarized in the following Theorem1. In comparison to the optimal classical query complexity of \((^{-2})\) queries, our algorithms obtain an improved dependence in terms of \(\) at the cost of a worse dependence on the dimension, \(d\); Theorem1 shows that a quadratic speedup is achievable when \(d\) is constant.

**Theorem 1** (Informal version of Theorem5 and Corollary1).: _Problem1 can be solved using an expected \(}(\{d^{5/8}(LR/)^{3/2},d^{3/2}LR/\})\) queries._

We complement Theorem1 with the following lower bound on the query complexity for QSCO.

**Theorem 2** (Informal version of Theorem8).: _For any \((d^{-1/2})\), any quantum algorithm that solve Problem1 with probability at least \(2/3\) makes at least \((d^{1/2}^{-1})\) queries in the worst case. For any \((d^{-1/2}) 1\), any quantum algorithm that solves Problem1 with success probability at least \(2/3\) must make at least \((^{-2})\) queries in the worst case._

Theorem2 shows that the \(}(d^{3/2}LR/)\) rate that we obtain is asymptotically optimal for \(d=}(1)\). A key open problem is whether the dimension dependence in either our upper bounds (Theorem5 and Corollary1) or our lower bound (Theorem8) can be improved.

Stochastic critical point computation.We also develop quantum algorithms for finding critical points, i.e., points with small gradients, of (possibly) non-convex functions.

  Setting & Queries & Output & Method \\   Convex & \(^{-2}\) & \(\)-optimal & SGD \\  Convex & \(d^{5/8}^{-3/2}\) & \(\)-optimal & **Our Result** (Q-AC-SA, Algorithm2) \\  Convex & \(d^{3/2}^{-1}\) & \(\)-optimal & **Our Result** (Q-SCP, Section4) \\  Non-convex (Bounded variance) & \(^{-4}\) & \(\)-critical & Randomized SGD  \\  Non-convex (Bounded variance) & \(d^{1/2}^{-3}\) & \(\)-critical & **Our Result** (Q-SGD, Algorithm6) \\  Non-convex (Mean-squared smoothness) & \(^{-3}\) & \(\)-critical & SPIDER  \\  Non-convex (Mean-squared smoothness) & \(d^{1/2}^{-5/2}\) & \(\)-critical & **Our Result** (Q-SPIDER, Algorithm7) \\  

Table 1: Comparison between our quantum algorithms and state-of-the-art classical algorithms for Problem1 (the convex setting) and Problem2 (the non-convex setting) where, for simplicity, we assume \(R=L=1\) for Problem1 and \(===1\) for Problem2. The \(}\) symbol was omitted in the “Queries” column.

**Problem 2** (Quantum stochastic critical point computation (QSCP)).: _In the quantum stochastic critical point computation (QSCP) problem, for an \(\)-smooth (possibly) non-convex \(f^{d}\) satisfying \(f()-_{}f()\) we are given query access to one of the following two oracles_

1. _(Bounded variance setting). A QSGO_ \(O_{}\) _with variance_ \(^{2}\) _(see_ Definition 3_), or_
2. _(Mean-squared smoothness setting). A_ \(\)_-SQ-QSGO_ \(O_{}^{S}\) _(see_ Definition 4_),_

_and must output an expected \(\)-critical point._

Leveraging  and  we develop two quantum algorithms that solve Problem 2 in the bounded variance setting and the mean-squared smoothness setting, and obtain the following result.

**Theorem 3** (Informal version of Theorem 6 and Theorem 7).: _In the bounded variance setting, Problem 2 can be solved using an expected \(}( d^{1/2}^{-3})\) queries. In the mean-squared smoothness setting, Problem 2 can be solved using an expected \(}((d)^{1/2}^{-5/2})\) queries._

In the bounded variance setting, Problem 2 can be solved using \((^{-4})\) queries to a classical SGO with variance \(^{2}\) (Definition 2) , which is known to be optimal . In comparison, our algorithm improves in terms of \(\) and achieves a quantum speedup when \(d(^{-2})\). In the mean-squared smoothness setting, Problem 2 can be solved using \((^{-3})\) queries to a classical stochastic gradient oracle with variance \(^{2}\) that satisfies if it satisfies (3) (Definition 2) , which is known to be optimal . In comparison, our algorithm improves in terms of \(\) and achieves a quantum speedup when \(d(^{-1})\).

Quantum zeroth-order oracles.Throughout the paper, we focus on stochastic gradient oracles in correspondence with classical work on stochastic optimization. However, it is worth noting that in certain cases our results extend gracefully to quantum stochastic zeroth order oracles. For example, when the objective function exhibits a finite-sum structure and we have access each component function individually through a quantum zeroth-order oracle, we can achieve an SQ-QSGO (Definition 4) with just a single query, utilizing quantum gradient estimation . However, in other cases, the correspondence is less clear. For instance, if we are given a quantum stochastic zeroth-order oracle where the function value is obfuscated by some external noise, quantum gradient estimation  is not directly applicable. Further study could be an interesting direction for future work.

Practicality.Regarding the utility of our algorithm in practical situations, note that our quantum oracles in Definition 3 and Definition 4 are defined as direct, natural generalizations of the corresponding classical oracles. Considering such quantum generalizations of classical oracles is standard in the literature, see e.g., . There are standard techniques for implementing such quantum analogs of classical oracles (in theory for now given the current state-of-the-art in implementing quantum algorithms in practice). In particular, if there is a classical circuit for the classical oracle, there is a standard technique to obtain a quantum circuit of the same size which implement the corresponding quantum oracle and its inverse. Hence, our quantum algorithms have the potential to surpass blackbox classical algorithms in low dimensional settings where the oracle is given as an explicit circuit.

Dimension dependence.Regarding potential concerns regarding the dependence of our quantum algorithms on the problem dimension, below we provide several supplementary points of context.

* As discussed, in the classical setting, prior research  demonstrated that when optimizing an 1-Lipschitz convex function, SGD has a query complexity of \((^{-2})\) which is optimal, even in the one-dimensional case. In the quantum setting, we showed that, theoretically, quantum speedups which offer a different tradeoff between \(\) and \(d\) dependencies are possible. Additionally, from our lower bound presented in Theorem 2 we know that some dimension dependence is inherent in obtaining an improvement.
* Classically, the complexity of dimension dependent optimization methods is well studied. In particular, there are parallel and private stochastic convex settings where the dimension dependencies are discussed, see e.g., , and there exists works on critical point computation in low dimension settings, see e.g., .

* Even for high-dimensional problems, our algorithms can potentially be used as a subroutines for low-dimensional subproblems. For instance, in this paper we apply these method to the approximately best point problem (Problem5), wherein we utilize our algorithm repeatedly within a one-dimensional setting.

### Paper organization

In the next Section2 we propose and develop a new algorithm for a new problem _quantum variance reduction_; this algorithm is the basis of our quantum speedups for stochastic optimization problems. We then discuss the application of quantum variance reduction for stochastic convex optimization by presenting two quantum algorithms in Section3 and Section4, respectively. Technically, it is possible to obtain our result in Section4 just using the quantum mean estimation routine of  rather then quantum variance reduction, however our use of quantum variance reduction facilitates our presentation. Furthermore, we present quantum algorithms for non-convex optimization based on quantum variance reduction in Section5. Finally, in Section6 we prove quantum lower bounds for quantum variance reduction and quantum stochastic convex optimization that establish the optimality of our algorithms, and conclude the paper in Section7.

## 2 Quantum variance reduction

We obtain our quantum speedups for stochastic optimization problems by proposing and developing new algorithms for a new problem which we call the _quantum variance reduction problem_.

**Problem 3** (Variance reduction).: _For a \(d\)-dimensional random variable \(X\) with \([X] L^{2}\) and some \( 0\), suppose we are given access to its quantum sampling oracle \(O_{X}\) defined in Definition1. The goal is to output an unbiased estimate \(\) of \([X]\) satisfying \(\|-\|^{2}^{2}\)._

Classically, Problem3 can be solved by averaging \((L^{2}/^{2})\) samples of \(X\); this query complexity is optimal among classical algorithms . However, if we have access to the _quantum sampling oracle_ defined in Definition1,  showed that a (possibly) biased estimate \(\) with error \(\|-\|\) can be computed using \(}(L^{-1})\) queries.

**Lemma 1** ([25, Theorem 3.5]).: _Given access to the quantum sampling oracle \(O_{X}\), for any \(, 0\) there is a procedure \((X,,)\) that uses \(}(L(1/)/)\) queries and outputs an estimate \(\) of the expectation \(\) of any \(d\)-dimensional random variable \(X\) satisfying \([X] L^{2}\) with error \(\|-\| L\) and success probability \(1-\)._

QuantumMeanEstimation proceeds by introducing a directional mean function that reduces a multivariate mean estimation problem to a series of univariate mean estimation problem through quantum Fourier transform, which in the bounded norm case can be solved by quantum algorithms with a quadratic speedup using phase estimation. In terms of error rate, \(\) in Lemma1 improves over any classical sub-Gaussian estimator when \(}{L}}\). However, its bias hinders its combination with various optimization algorithms assuming unbiased inputs, see e.g., [57; 2; 29; 30]. In this work we show how to carefully combine their algorithm with a classic multi-level Monte-Carlo (MLMC) technique from [10; 7] to obtain an unbiased estimate \(\) and success probability 1 with the same rate as  and prove the following theorem.

**Theorem 4**.: _Algorithm1 solves Problem3 using an expected \(}(Ld^{1/2}^{-1})\) queries._

In the following Table2 we provide a comparison between our result and previous works and in Section6 we prove that our algorithm is optimal up to a poly-logarithmic factor. Notably, our algorithm does not depend on the detailed implementation of \(\) but only its query complexity. Hence, we believe that our approach may also be useful in removing the bias of other quantum mean estimation algorithms with similar expressions of query complexities, e.g., quantum phase estimation [55; 24] and quantum amplitude estimation .

Our algorithm consists of two components. First, we show that we can obtain a variant of the quantum mean estimation algorithm, denoted as \(^{+}(X,)\), that outputs a low variance estimate with probability 1. This procedure compares the outcomes of the quantum estimator and a classical estimate, and in the event of significant disparity, generates a new independent classical estimate.

Second, we use the MLMC technique , specifically a variant of the methods described in [10; 7], to carefully invoke the biased subroutine \(^{+}\) and compute the unbiased estimate. The algorithm, Algorithm 1, simply invokes \(^{+}\) for three randomly chosen accuracies and combines them to obtain the result. Though there are an infinite number of possible accururacies chosen, we show that the expectation, the variance, and expected number of queries are all suitable to prove Theorem 4, whose proof is deferred to Appendix B.

``` Input: Random variable \(X\), target variance \(^{2}\) Output: An unbiased estimate \(\) of \([X]\) with variance at most \(^{2}\)
1 Set \(_{0}^{+}(X,/10)\)
2 Randomly sample \(j()\)
3\(_{j}^{+}(X,2^{-3j/4}/10)\)
4\(_{j-1}^{+}(X,2^{-3(j-1)/4} /10)\)
5\(_{0}+2^{j}(_{j}-_{j-1})\)
6return\(\) ```

**Algorithm 1**Quantum variance reduction

## 3 Quantum accelerated stochastic approximation

In this section, we present our \(}(d^{5/8}^{-3/2})\) query quantum algorithm for Problem 1. Our approach builds upon the framework proposed by Duchi et al. [28; 15], which involves performing a Gaussian convolution on the objective function \(f\) and then optimizing the resulting smooth convoluted function. Compared to their algorithm, our algorithm differs by replacing the variance reduction step by our quantum variance reduction technique (Algorithm 1).

As with a variety of prior work on parallel and private SCO [28; 33; 15], we consider the smooth function \(F_{r}\) that is the Gaussian convolution of the objective function \(f\):

\[F_{r}()_{^{d}}_{r}( )f(-), _{r}()r)^{d}}-\|^{2}}{2r^{2}}.\] (5)

As shown in Lemma 4, when the radius \(r\) of the convolution is sufficiently small, \(F_{r}\) closely approximates \(f\) pointwise. Consequently, to find an \(\)-optimal point of \(f\) it suffices to find an \(/4\)-optimal point of \(F_{r}\) for \(r=L}\). Moreover, Lemma 4 shows that the stochastic gradient \(_{F}\) of \(F_{r}\) can be defined and obtained based on the stochastic gradient \(\) of \(f\) as follows:

\[_{F}()=(-), _{r},\] (6)

which satisfies

\[*{}_{_{r}}_ {F}()= F_{r}()\|_{F}( )\| L,.\]

Hence,

\[\| F_{r}()\|_{_{r}} \|_{F}()\| L\]

indicating that \(F_{r}\) is also \(L\)-Lipschitz.

   Queries & Bias & Variance & Method \\   \(^{-2}\) & 0 & \(^{2}\) & Classical Variance Reduction \\  \(d^{1/2}^{-1}\) & \(\) & \(^{2}\) & Quantum Multivariate Mean estimation  \\  \(^{2}(1/)^{-1}\) & \(\) & \(^{2}\) & One-dimensional Quantum Mean estimation  \\  \(d^{1/2}^{-1}\) & 0 & \(^{2}\) & **Our Result** (Theorem 4) \\   

Table 2: Comparison between different methods for variance reduction in the case of \(L=1\) and \((0,1)\). The \(}\) symbol was omitted in the “Queries” column.

To optimize this smooth convex function \(F_{r}\), we leverage the accelerated stochastic approximation (AC-SA) algorithm introduced in , which applys an accelerated proximal descent method on the objective function using unbiased estimates of gradients. Our algorithm given in Algorithm 2 is a specialization of the AC-SA algorithm, where we implement those unbiased estimates of gradients using quantum variance reduction (Algorithm 1). In the classical setting, one query to the stochastic gradient \(_{F}()\) of \(F\) can be implemented by a random sampling a vector \(^{d}\) from the Gaussian \(_{r}\) followed by a query to the SGO \(_{}\) (Definition 2) at \(-\). Similarly, we can show that one query to a QSGO of \(F_{r}\) (Definition 3) can also be implemented by one query to the QSGO of \(f\). The subsequent theorem presents the query complexity of Algorithm 2.

``` Input: Function \(f^{d}\), precision \(\) Parameters: Domain Size \(R\), total iteration budget \(=LR}{}\), target variance \(=}{8}}\), convolution radius \(r=L}\), \(=}}{(f+2)^{3/2}}\) Output: an \(\)-optimal point of \(F\)
1 Denote \(F_{r}()_{^{d}}_{r}()f( -)\) as in (5)
2 Set \(_{1}\), \(_{1}^{ag}_{1}\)
3for\(t=1,2,,\)do
4\(_{t},_{t}\)
5\(_{t}^{md}_{t}^{-1}_{t}+(1-_{t}^{-1}) _{t}^{ag}\)
6 Call Algorithm 1 for an unbiased estimate \(}_{t}\) of \( F_{r}(_{t}^{md})\) with variance at most \(^{2}\)
7\(_{t+1}_{R}()}{ }\{_{t}}_{t},-_ {t}^{md}+L\|_{t}^{md}-\|^{2}/(2r)\}\)
8\(_{t+1}^{ag}=_{t}^{-1}_{t+1}+(1-_{t}^{-1}) _{t}^{ag}\)
9return\(_{T+1}^{ag}\) ```

**Algorithm 2**Quantum accelerated stochastic approximation (Q-AC-SA)

**Theorem 5** (Formal version of Theorem 1, Part 1).: _Algorithm 2 solves Problem 1 using an expected \(}(d^{5/8}(LR/)^{3/2})\) queries._

The proof of Theorem 5 is deferred to Appendix C.

## 4 Quantum stochastic cutting plane method (Q-SCP)

In this section, we develop our \(}(d^{3/2}LR/)\) query algorithm for Problem 1 which is based on a stochastic version of the cutting plane method. We introduce the key properties and related concepts of cutting plane methods, and then provide a procedure for efficiently post-processing the outcomes obtained from the stochastic cutting plane method using quantum variance reduction (Algorithm 1). Then, we analyze the overall query complexity for solving Problem 1. Technically, it is possible to obtain the results of this section using the quantum mean estimation routine of , rather then quantum variance reduction, however using quantum variance reduction facilitates our presentation.

We begin by introducing some notation and concepts on cutting plane methods. Cutting plane methods solve the _feasibility problem_ defined as follows. Note that this problem is slightly easier to solve then the one in, e.g., , however it is simple suffices for our purposes.

**Problem 4** (Feasibility Problem).: _We are given query access to a separation oracle for a set \(K^{d}\) such that on query \(^{d}\) the oracle outputs a vector \(\) and either \(=\), in which case \( K\), or \(\), in which case \(H\{^{}^{} \} K\). The goal is to query a point \( K\)._

 showed that Problem 4 can be solved by cutting plane method using \((d(dR/r))\) queries to a separation oracle where \(R\) and \(r\) denote bounds on \(K\).

**Lemma 2** ([48, Theorem 1.1]).: _There is a cutting plane method which solves Problem 4 using at most \(C d(dR/r)\) queries for some constant \(C\), given that the set \(K\) is contained in the ball of radius \(R\) centered at the origin and it contains a ball of radius \(r\)._[67; 58] demonstrated that, running cutting plane method on a convex function \(f\) with the separation oracle being its gradient yields a sequence of points where at least one of them is an \(\)-optimal point of \(f\). This follows from the fact that there exists a ball of radius \(()\) around \(^{*}\) such that every point in this ball is \(\)-optimal. In the stochastic setting, although we cannot access the precise gradient, we show that it suffices to use an \((/R)\)-approximate gradient oracle of \(f\) (formally defined in Appendix D.1) as the separation oracle. Specifically, we prove the following result.

**Proposition 1**.: _For any \(0 LR\), with success probability at least \(5/6\) we can obtain \(=(d(dLR/))\) points \(_{1},,_{}_{R}()\) such that one of the \(x_{i}\) is \(\)-optimal using \(}(d^{3/2}LR/)\) queries to the QSGO \(O_{}\) defined in Definition 3._

The proof of Proposition1 is deferred to Appendix D.1. After applying Proposition1, it is not clear which query \(_{i}\) is an \(()\)-optimal point. This difficulty arises because we lack access to the function value of \(f\), which sets our problem apart from the feasibility problem discussed in , where there is a clear indication when a query successfully lies within the feasible region. Consequently, we next focus on identifying the optimal solution within the finite set \(\) of points using access to the QSGO. We conceptualize this task as the _approximately best point_ problem, formulated as follows.

**Problem 5** (Approximately best point).: _For a \(L\)-Lipschitz convex function \(f^{d}\) and \(\) points \(_{1},,_{}_{R}()\), find a convex combination \(}^{d}\) of the points satisfying_

\[f(})_{t}f(_{t})+.\]

In this work, we develop an algorithm that solves creftype5 by making pairwise comparisons in a hierarchical order, where each pairwise comparison is computed by running binary search on the segment along the segment between the two points. Formally, we prove the following result.

**Proposition 2**.: _For any accuracy parameter \(>0\), with success probability at least \(5/6\) Algorithm5 solves creftype5 using \(}(RLT/)\) queries to an \(L\)-bounded QSGO \(O_{}\) defined in Definition3._

The proof of Proposition2 and the corresponding quantum algorithm can be found in Appendix D.2. Next, we present the main result of this section, which describes the query complexity of solving creftype1 using quantum stochastic cutting plane method.

**Corollary 1** (Formal version of creftype1, Part 2).: _With success probability at least \(2/3\), creftype1 can be solved using an expected \(}(d^{3/2}LR/)\) queries._

The proof of Corollary1 is deferred to Appendix D.3.

## 5 Quantum stochastic non-convex optimization

In this section, we present our quantum algorithms for creftype2 in the bounded-variance setting and the mean-squared smoothness setting, respectively, using our quantum variance reduction technique.

To solve creftype2 in the bounded variance setting, we leverage the randomized SGD method introduced in , which is a variant of SGD where the number of iterations is randomized. Our algorithm is a specialization of the randomized stochastic gradient algorithm, where we replace the classical variance reduction step by quantum variance reduction (Algorithm1). The query complexity of our quantum algorithm is given in the following theorem.

**Theorem 6** (Formal version of creftype3, bounded variance setting).: _For any \(>0\), creftype6 solves creftype2 in the bounded variance setting using an expected \(}(^{-3})\) queries._

The proof of Theorem6 and the corresponding quantum algorithm can be found in AppendixE.

To solve creftype2 in the mean-squared smoothness setting, we leverage the SPIDER algorithm introduced in , which is a variance reduction technique that allows us to estimate the gradient of a function with lower cost by utilizing the smoothness structure and reuse the stochastic gradient samples at nearby points. Our algorithm is a specialization of the SPIDER algorithm, where we replace the classical variance reduction step by quantum variance reduction (Algorithm1). The query complexity of our quantum algorithm is given in the following theorem.

**Theorem 7** (Formal version of creftype3, mean-squared smoothness setting).: _For any \(0\), creftype7 solves creftype2 in the mean-squared smoothness setting using an expected \(}(^{-2.5})\) number of queries._The proof of Theorem7 and the corresponding quantum algorithm can be found in AppendixF.

## 6 Lower bounds

In this section we present two quantum lower bounds for solving quantum variance reduction (Problem3) and stochastic convex optimization (Problem1), respectively.

We first establish the following quantum lower bound for the variance reduction problem (Problem3) which shows that our Algorithm1 is optimal up to a poly-logarithmic factor when \(=(d^{-1/2})\). By Markov's inequality, Proposition3 equivalently states that any quantum algorithm that solves Problem3 must make an expected \((L^{-1})\) queries. This matches our algorithmic result provided in Theorem4, up to a poly-logarithmic factor.

**Proposition 3**.: _There is a constant \(\) such that for any \(}\), any quantum algorithm that solves Problem3 with success probability at least \(2/3\) must make at least \((L^{-1})\) queries in the worst case._

The proof of Proposition3 is deferred to AppendixG.1

Next, we establish the following quantum lower bounds for stochastic convex optimization (Problem1) in the low-dimension regime and the high-dimension regime, respectively, which show that our quantum stochastic cutting plane method in Section4 is optimal up to a poly-logarithmic factor when the dimension \(d\) is a constant, and there is no quantum speedup over SGD when \(d(^{-2})\).

**Theorem 8**.: _For any \(}\), any quantum algorithm that solves Problem1 with success probability at least \(2/3\) must make at least \((RL/)\) queries in the worst case. For any \(} 1\), any quantum algorithm that solves Problem1 with success probability at least \(2/3\) must make at least \((R^{2}L^{2}/^{2})\) queries in the worst case._

The proof of Theorem8 is deferred to AppendixG.2.

## 7 Conclusion

We presented improved quantum algorithms for stochastic optimization. We developed a new technical tool which we call _quantum variance reduction_ and show how to use it to improve upon the query complexity for stochastic convex optimization and for critical point computation in smooth, stochastic, non-convex functions. Further, we provided lower bounds which establish both the optimality of our quantum variance reduction technique and of one of our stochastic convex optimization algorithms in low dimensions. A natural open problem suggested by our work is to establish the optimal complexity of the problems we study, e.g., stochastic convex optimization and stochastic non-convex optimization with quantum oracle access, in higher dimensions. We hope this paper fuels further study of these problems.