# On the Power of Decision Trees

in Auto-Regressive Language Modeling

 Yulu Gan

Massachusetts Institute of Technology

yulu@csail.mit.edu

&Tomer Galanti

Texas A&M University

galanti@tamu.edu

&Tomaso Poggio

Massachusetts Institute of Technology

tp@csail.mit.edu

&Eran Malach

Harvard University

eran.malach@gmail.com

###### Abstract

Originally proposed for handling time series data, Auto-regressive Decision Trees (ARDTs) have not yet been explored for language modeling. This paper explores both the theoretical and practical applications of ARDTs in this new context. We theoretically demonstrate that ARDTs can compute complex functions, such as simulating automata, Turing machines, and sparse circuits, by leveraging "chain-of-thought" computations. Our analysis provides bounds on the size, depth, and computational efficiency of ARDTs, highlighting their surprising computational power. Empirically, we train ARDTs on simple language generation tasks, showing that they can learn to generate coherent and grammatically correct text on par with a smaller Transformer model. Additionally, we show that ARDTs can be used on top of transformer representations to solve complex reasoning tasks. This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development.

## 1 Introduction

In recent years, Large Language Models (LLMs) have achieved outstanding results in tasks such as natural language understanding, coding, and mathematical reasoning. LLMs predominantly utilize the Transformer architecture Vaswani et al. (2023), establishing it as the standard in this field. However, recent initiatives (Gu & Dao, 2023; Sun et al., 2023; Ma et al., 2023; De et al., 2024) have begun to challenge the dominance of Transformers. These alternatives, while not yet matching Transformer performance, offer advantages in terms of inference time efficiency. Moreover, some works are revisiting traditional non-neural network models for language modeling, such as classical symbolic models (Wong et al., 2023). These developments indicate a shift towards diverse, efficient, and interpretable language modeling methodologies.

Tree-based models, particularly favored for handling tabular data (Grinsztajn et al., 2022), continue to hold significant importance. While tree-based methods are mostly used for classification and regression tasks, Auto-regressive Decision Trees (ARDTs) (Meek et al., 2002) have been studied for time-series prediction, offering a simpler and more interpretable alternative to complex nonlinear approaches. Although the ARDT approach was not originally designed for language tasks, it has demonstrated considerable promise in various time-series datasets, outperforming traditional auto-regressive models while maintaining ease of interpretation. Motivated by these results, our study seeks to explore the potential of ARDTs for language prediction tasks, assessing whether they could serve as a viable, interpretable alternative to complex, resource-intensive language models.

To understand the power of ARDTs, we first conduct theoretical studies demonstrating that ARDTs, using decision trees as next-token predictors, can compute more complex functions than traditional decision trees. We explore the classes of functions ARDTs can compute, showing their ability to simulate functions computed by automata, Turing machines, or sparse circuits through intermediate "chain-of-thought" computations. We provide bounds on the size, depth, and run-time (measured by the number of intermediate tokens) required for ARDTs to simulate these function classes. Our findings highlight the surprising computational capabilities of ARDTs, underscoring their potential as a powerful and interpretable alternative for language prediction tasks requiring complex function computations.

Our experimental results further demonstrate the practical utility of ARDTs in language generation tasks. Utilizing standard auto-regressive inference methods, these models generate output sequences token-by-token, appending each new token to the input of the subsequent iteration. When trained on the TinyStories dataset Eldan & Li (2023), ARDTs produce coherent and grammatically accurate text (see in Fig 1). Notably, decision tree ensembles with approximately 0.3 million parameters outperform a Transformer model with around 1 million parameters on the same TinyStories dataset, highlighting their efficiency despite a smaller size. We discuss our approach to training interpretable decision trees, which enhances the transparency of the decision-making process in language generation. Furthermore, we assess the ability of tree-based models to execute various logical reasoning tasks. Notably, tree ensembles built on top of transformer embeddings and trained on specific downstream tasks perform comparably to larger general models like InstructGPT Ouyang et al. (2022) and PaLM-540B Chowdhery et al. (2022), under the conditions of these particular tasks.

Our contribution can be summarized as follows:

* We extend the application of ARDTs to language prediction tasks, adopting a novel approach that capitalizes on their inherent simplicity and interpretability. This aims to broaden the architectural diversity in language model development.
* Through theoretical analysis, we demonstrate that ARDTs can compute a broader array of complex functions than previously recognized, including the simulation of automata, Turing machines, and sparse circuits. These theoretical findings deepen our understanding of ARDTs' computational capabilities.
* Our experimental results offer empirical evidence that ARDTs are capable of generating coherent and grammatically correct text, perform well compared to more complex models like small Transformers, and demonstrate solid reasoning abilities.

Figure 1: **(a) An example of story continuation generated by our Auto-Regressive Decision Trees. We use decision trees and, remarkably, attain results comparable to Transformer-based models in terms of linguistic fluency. (b) The decision process of the decision trees. We visualize part of the tree ensemble, and can observe which word is most relevant for the splitting rule at each node.**

Related Work

**Decision Trees.** Tree based models have been widely used for solving different classification and regression tasks in machine learning (Navada et al., 2011). The ID3 algorithm was introduced by Quinlan (1986), and has been widely used for decision tree learning, along with the CART (Breiman et al., 1984; Lewis, 2000) algorithm. Decision tree ensembles, such as random forests (Breiman, 2001) and gradient boosted trees (Friedman, 2002), are also very popular. Despite continuous advancements in deep learning, decision tree ensembles still outperform neural network based models on tabular datasets (Shwartz-Ziv & Armon, 2022). Different from traditional decision trees, we use auto-regressive decision trees to perform language prediction tasks more efficiently.

**Learning Theory for Decision Trees.** There are a few theoretical works studying the power of decision trees in solving machine learning problems. The work of Brutzkus et al. (2020) shows that the ID3 algorithm can learn sparse functions in some setting. Kearns & Mansour (1996) show that decision trees are equivalent to boosting methods for amplifying the performance of weak learners on the distribution. Other works focus on other aspects of decision tree learnability (Rivest, 1987; Blum, 1992; Ehrenfeucht & Haussler, 1989; Bshouty & Burroughs, 2003). We note that from the approximation point of view, decision trees can be regarded as splines with free knots. For instance, piecewise constant hierarchical splines functions, similar to neural networks with threshold activation can also be seen as decision trees. Note that ReLU networks can be viewed as piecewise hierarchical linear splines (Anselmi et al., 2015; Yarotsky, 2016), and so decision trees can represent ReLU networks (see Aytekin (2022)), though possibly with an exponential number of parameters. We note that none of the works mentioned above studies the theory of auto-regressive decision trees, which is a novel contribution of our paper.

**Decision Trees for Language.** Despite gaining popularity in several fields of machine learning, tree based models are not widely used for language generation. Past works have utilized auto-regressive decision trees for time-series analysis (Meek et al., 2002), or use trees for basic language modeling (Potamianos & Jelinek, 1998). Decision trees were also used in parsing (Magerman, 1995; Heeman, 1999; Nallapati & Allan, 2002), modeling syntax (Filimonov, 2011) and language identification (Hakkinen & Tian, 2001).

## 3 Theory

To explore the capabilities of ARDTs, we initially undertake theoretical studies demonstrating that using decision trees as next-token predictors enables ARDTs to process significantly more complex functions than "standard" decision trees. Firstly, we define the theoretical setting of our analysis in Section 3.1. We then examine the various classes of functions that an ARDT can compute, as discussed in Sections 3.2, 3.3, and 3.4. Here, the computation involves the ARDT receiving an input sequence, such as a question, generating a series of intermediate tokens that describe the thought process, and finally producing the output token. Specifically, we demonstrate that functions computed by Automata, Turing machines, or sparse circuits can be emulated by an ARDT using these intermediate "chain-of-thought" computations. Additionally, we provide bounds on the size, depth, and runtime (measured by the number of intermediate tokens) required for ARDTs to simulate these classes of interest. Our findings affirm that ARDTs, by leveraging decision trees for next-token prediction, can handle far more complex functions than "standard" decision trees.

**Comment 1**.: _The results in this section are representation results. That is, we study which functions can, in theory, be represented by auto-regressive decision trees. We do not provide any formal results on whether such functions can be learned from data. The question of how decision trees can be trained to produce "chain-of-thought" responses to input questions is beyond the scope of this work._

### Setting

We adapt the standard definition of a decision tree, as described by Quinlan (1986), to include modifications that allow for the processing of vector sequences of arbitrary lengths. Firstly, we establish a vocabulary \(\), which serves as our token dictionary. Next, we define an input embedding \(:^{d}\). For any sequence of tokens \(^{n}\), \(()^{n d}\) represents the embedding applied individually to each token. The space comprising sequences of d-dimensional vectors is denoted by \(=^{* d}\). Subsequently, we define a decision tree \(\) that receives an input \(\) and outputs a token \(y\).

In our experiments, detailed in Section 4, we apply a weighted-average operator to the word vectors of the sequence, where the average vectors are used as an input to the decision trees. For the theoretical analysis we study a different approach for using decision trees over vector sequences, where instead of averaging word vectors we "concatenate" them. That is, the decision tree is applied to the \(L\) most recent words, in a "sliding window" fashion. We note that experimentally we observed that both the "sliding-window" and the weighted-average approach produced similar results, and use the weighted-average technique in our experiments for computational reasons.

We start by defining a decision tree \(\) that gets inputs of a fixed length \(L\), namely \(:^{L d}\). We refer to the value \(L\) as the _context length_ of \(\), and this value will correspond to the maximal length of a sequence that affects the computation of the tree. In this case, we treat the input \(^{L d}\) as a vector, and let \(\) be a standard decision tree operating on vectors of size \(L d\). Namely, \(\) is defined by a binary tree, where each node corresponds to an input feature \(x_{i,j}\) and some threshold \(\). Each leaf corresponds to some output token \(y\). The output of the tree \(\) is computed by starting at the root, and for each internal node with feature \(x_{i,j}\) and threshold \(\), moving to the right node if \(x_{i,j}\) and otherwise moving to the left node. When reaching a leaf, we output the value \(y\) corresponding to the leaf. The _size_ of the tree \(\) is the number of leaves in the tree, and its _depth_ is the maximum length of a path from root to leaf. Note that the runtime of computing the output of \(\) corresponds to the _depth_ of the tree.

Now, given some tree over length-\(L\) inputs \(:^{L d}\), we apply \(\) to an input of arbitrary length \(\) using the following simple rule: if \(\) has length shorter than \(L\), we pad it to length \(L\) by prepending the input, adding additional padding (\(\)) tokens at the beginning; if \(\) is longer than \(L\), we apply \(\) only to the last \(L\) tokens in \(\). This induces a decision tree with arbitrary length inputs \(:\).

Finally, we use the tree \(\) as a next-token predictor function, applied over some input using auto-regressive computation. That is, we define a sequence-to-sequence predictor \(^{}:^{*}^{*}\) induced from the tree \(\) as follows: for every input \(^{n}\), recursively define \(s_{n+i+1}=((s_{1},,s_{n+i}))\), and let \(^{}(s_{1},,s_{n})=(s_{n+1},s_{n+2},)\). We call \(^{}\) an _auto-regressive decision tree_ (ARDT).

In the rest of this section, we will analyze the capacity of ARDTs to simulate some function classes. Following Malach (2023), we give the following definition:

**Definition 2**.: _For some class \(\) of functions \(f:^{n}\), we say \(\) can be simulated by auto-regressive decision-trees in length complexity \(T\), if for every \(f\) there exists \(^{}\) s.t. for all \(^{n}\), we have \(^{}_{}()=f()\) (where \(^{}_{}\) indicates the output of \(^{}\) at iteration \(T\))._

In other words, we say that the tree \(^{}\) can compute the function \(f\), if given some input sequence \(\), it generates \(T\) tokens followed by the correct output \(f()\). That is, we allow the tree to use \(T\) intermediate tokens as "chain-of-thought" before outputting the correct answer.

### Simulating Automata

An automaton \(\) is defined over an alphabet \(\), using a set of states \(Q\), an initial state \(q_{0} Q\) and a transition function \(:Q Q\). We always assume that \(|| 2\) and \(|Q| 2\). The automaton \(\) gets an input string \(^{*}\), and computes an output state \(() Q\) by starting at state \(q_{0}\) and at each iteration \(i\) transitioning to the next state based on the \(i\)-th token \(x_{i}\), namely \(q_{i}=(q_{i-1},x_{i})\). The automaton then returns the state reached at the final iteration.

Let \(^{}_{}\) is the class of all functions computed by automata over strings of length \(n\). Namely, \(^{}_{}\) is the class of functions \(f:^{n} Q\) s.t. for all \(f^{}_{n}\) there exists an automaton \(\) s.t. \(()=f()\) for all \(^{n}\).

The class of functions computed by Automata has been well-studied from the early days computer science theory (Hopcroft et al., 2001), and has various important connections to language problems. This class of functions is also interesting in the context of reasoning tasks for language modeling. For example, the _Web-of-Lies_ and _Navigate_ problems in the Big-Bench Hard dataset (Srivastava et al., 2023) can be solved by finite state Automata.

We show that ARDTs can simulate Automata:

**Theorem 3**.: _Let \(= Q\{\}\). Then, \(_{n}^{}\) can be simulated by ARDTs of size \(O(||^{2})\), depth \(O(||)\) and context length \(L n\), in length complexity \(O(n)\)._

Note that ARDTs simulate Automata very efficiently: the total run-time of the ARDT guaranteed by Theorem 3 is \(O(n||)\), which corresponds to the time it takes to read all the input bits. In this sense, no algorithm can simulate Automata significantly faster than ARDT.

In the proof, we construct an ARDT that, at every iteration \(i\), outputs the state of the Automaton at step \(i\) (denoted \(q_{i}\)). The state at step \(i+1\) is only a function of the \(i\)-th state, given by the most recent token generated by the model; and the \(i\)-th input, which is always given by looking back \(n+1\) tokens. Therefore, a simple tree, applied as a _sliding-window_ over the input, can compute the transition matrix to find the next state. The full proof is given in Appendix A.

Next, we show that the above result implies a _separation_ between ARDTs and standard decision trees. Specifically, we show that if we use a decision-tree over the input to directly predict the final output of the Automata, without outputting intermediate states, then the size of the decision tree must be exponential in the length of the input:

**Theorem 4**.: _There exists some \(f_{n}^{}\) s.t. any decision tree that computes \(f\) has size \((2^{n})\)._

This shows that the fact that ARDTs can perform intermediate computations auto-regressively (e.g., perform _chain-of-thought_) significantly improves their efficiency1. To prove the result, we show that computing the _parity_ of a sequence of bits (i.e., whether the number of bits is even or odd) requires a tree of exponential size, but can be easily computed by a simple 2-state Automaton.

Proof of Theorem 4.: Consider the binary alphabet \(=\{0,1\}\) and the state set \(Q=\{,\}\), with \(||=2\) and \(|Q|=2\). We define a function \(f:^{n} Q\) as follows:

\[f()=& x_{i} 2=0,\\ &.\]

The function \(f\) describes the parity of the sum of bits in \(\) and can be efficiently computed by an automaton that toggles between states even and odd upon encountering a \(1\).

Suppose a decision tree \(\) computes \(f\). We claim that the size of \(\) must be at least \(2^{n}\). Assume for contradiction that \(\) has fewer than \(2^{n}\) leaves. Since \(\) is a decision tree, we assume that all its leaves are reachable by some input \(\{0,1\}^{n}\).

Consider a leaf \(l\) of \(\) reached by some input \(\), at a depth less than \(n\). This implies that there exists at least one bit index \(j[n]\) such that no decision node in \(\) queries \(x_{j}\) on the path to \(l\). Define \(^{}\{0,1\}^{n}\) by flipping \(x_{j}\) in \(\), while keeping all other bits unchanged:

\[x^{}_{i}=x_{i}&i j,\\  x_{j}&i=j.\]

Since \(^{}\) alters \(\) only at the unqueried index \(j\), it follows the same path in \(\) and reaches the same leaf \(l\). Therefore, \(()=(^{})\). However, the definition of \(f\) guarantees \(f() f(^{})\) as their parities are different, leading to a contradiction. Thus \(\) cannot compute \(f\) with fewer than \(2^{n}\) leaves.

### Simulating Turing Machines

A Turing machine \(\) is defined over an alphabet \(\), using a space set \(Q\), initial state \(q_{0} Q\) and transition function \(:Q Q\{,\}\). The Turing machine has a tape, where each cell contains a symbol from \(\). The _head_ of the Turing machine is initialized at the leftmost cell on the tape in state \(q_{0} Q\). At each iteration of the machine, it reads the symbol \(s\) and given the head state \(q Q\) uses \((q,s)\) to determined the new state of the head, the symbol to write under the head, and whether to move the head left or right on the tape.

In our setting, we consider Turing machines with fixed memory \(M\), i.e. Turing machines with access to a tape with \(M\) cells. In particular, this means that the Turing machine \(\) operate on inputs with \(<M\) tokens. At the initial step, the input is written on the tape. If the input size is shorter than \(M\), we add empty tokens \(\{\}\) after the input sequence. We consider Turing machines with fixed runtime \(T\), namely we let the machine run for \(T\) iterations and then halt it. The output of the machine is the rightmost symbol on the tape after \(T\) iterations. So, we define \(:^{M}\) to be the function computed by the machine after \(T\) steps. We denote by \(_{M,T}^{}\) the class of functions computed by Turing machines with memory of size \(M\) and runtime \(T\).

**Comment 5**.: _Turing machines are typically defined with infinite number of tape cells, and are allowed to run arbitrarily long before halting. However, for every given input length, any computable function always uses a fixed memory and run-time (which depend on the input length)._

We now show any Turing machine with fixed memory and run-time can be simulated by an ARDT:

**Theorem 6**.: _Let \(= Q\{, \}\)2. Then, \(_{M,T}^{}\) can be simulated by ARDTs of size \(O(||^{4})\), depth \(O(||)\) and context length \(L=M+3\), with length complexity \(O(MT)\)._

To prove the result, we show that an ARDT can compute the state of the Turing machine at each iteration. Specifically, we encode the state of the machine as a sequence of tokens from \(\), where we put a token \(q Q\) indicating the state of the head before the token that the head reads. This way, the transition between states is a function that only depends locally on the tokens surrounding the position of the head, where all other (non-state) tokens can be copied as-is from one state to the next. Similarly to the proof in the previous section, this operation can be realized by a small _sliding-window_ tree. The full proof is given in Appendix A.

### Simulating Sparse Circuits

A circuit \(\) over some alphabet \(\) is defined as a directed-acyclic-graph (DAG), with \(n\) input nodes and one output node. Each internal (non-input) node with \(k\) incoming edges corresponds to some function \(g:^{k}\) computed by the node over its incoming inputs. For some input \(^{n}\), the output of the circuit \(\) is the value of the output node, when setting the input nodes of \(\) to \(x_{1},,x_{n}\). The size of the circuit \(\) is the number of nodes in the computational graph. We say that \(\) is \(k\)-sparse, if the maximal in-degree of every node in the graph is \(k\). Denote by \(_{N,k}^{}\) the class of functions computed by \(k\)-sparse circuits of size \(N\).

We note that sparse circuits are an extension of sparse Boolean circuits, and so can represent Turing machines with bounded memory (Arora Barak, 2009). In this sense, this class is "equivalent" to the class of functions computed by Turing machines. However, some functions may be more efficient to compute using sparse circuits, and so it is interesting to understand how ARDTs can directly simulate sparse circuits, as demonstrated in the following theorem:

**Theorem 7**.: _Let \(=\{\}\). Then, \(_{N,k}^{}\) can be simulated by ARDTs of size \(O(N||^{k}||)\) and context length \(L N\), in length complexity \(O(N)\)._Proof of Theorem 7.: Consider a \(k\)-sparse circuit \(\) with \(N\) total nodes, where \(N-n\) are internal nodes. Let \(g_{1},,g_{N-n}:^{k}\) be the functions computed at the internal nodes, ordered topologically so that each function depends only on the inputs or the results of preceding nodes. Let \(g_{N-n}\) denote the function computed by the output node.

Define \(f_{i}:^{n+i-1}\) as the output of the \(i\)-th node in this ordering, considering all inputs and outputs from previous nodes. Each \(f_{i}\) is effectively a \(k\)-Junta. By Lemma 10, there exists a decision tree \(_{i}\) of size \(O(\|\|^{k})\) such that \(_{i}(())=f_{i}()\) for all \(^{n+i-1}\).

To accommodate inputs \(^{N}\), we modify each tree \(_{i}\) to ignore the first \(N-n-i+1\) inputs. This adaptation does not affect the size of the tree.

Let \(=()\{0,1\}^{d}\). Construct a tree as follows: begin with the rightmost branch of the tree, using functions \(h_{1,1},,h_{1,d},,h_{N-n,1},,h_{N-n,d}\). For each node \(i[N-n]\) and each bit \(j[d]\), define:

\[h_{i,j}=1\{()_{i,j} 1\}&z_{j}=1,\\ 1\{()_{i,j}<1\}&z_{j}=0.\]

Attach tree \(_{N-n-i+1}\) at each left node \((i,j)\).

Observe that during the \(i\)-th iteration, the string begins with \(N-n-i\) tokens, allowing \(_{i}\) to process the pertinent part of the input. After \(N-n\) iterations, the constructed tree calculates the output token as specified by \(\). 

## 4 Experiments

In this section, we experimentally validate the capabilities of ARDTs as demonstrated in the previous section and prove their language modeling potential. In Section 4.2, we first train a model based on ARDTs and test its ability to continue stories on Tinysteries Eldan & Li (2023), which involves extending narratives similar to a finite state automaton. ARDTs generate coherent text that builds on existing stories, also requiring the interpretation of complex contexts and emotions. This showcases the effectiveness of sparse circuits in managing significant yet limited inputs.

Additionally, in Section 4.3, we assess the model's reasoning abilities on the Big-Bench-Hard Suzgun et al. (2022) dataset, where tasks often involve evaluating the truthfulness of propositions, effectively emulating a Turing machine as it processes inputs to determine a definitive outcome (true or false).

### Setting

To align with the theory section, we designed our experiments to closely mirror the theoretical settings as closely as possible. We here provide a detailed description of our implementation of Auto-regressive Decision Trees (ARDTs) for next-token prediction tasks. Our objective is to utilize ARDTs as a language model that receives a sequence of input tokens \(x_{1},,x_{n}\) and predicts the subsequent token \(x_{n+1}\). Initially, we employ a Word2Vec embedding Mikolov et al. (2013), denoted by \(\), to convert the sequence tokens into word embeddings \((x_{1}),,(x_{n}),(x_{n+1})^{100}\). We then compute a weighted average of these embeddings with exponential decay, prioritizing the most recent tokens: \(}=_{i=1}^{n}^{n-i+1}(x_{i})\), where \((0,1)\). Using XGBoost Chen & Guestrin (2016), we train an ensemble of decision trees, \(\), which takes the input vector \(}\) and predicts the embedding of the next token \((x_{n+1})\), aiming to minimize the mean squared error (MSE) loss. We train this model using sequences of varying lengths sampled from our dataset. During inference, the model generates text auto-regressively. At each step, it receives the current sequence \(}\), outputs the predicted embedding of the next token \(}=(})\), and identifies the token whose embedding is closest to this prediction, i.e., \(=_{x}\|(x)-}\|_{2}\). This token is then used as the next token in the sequence. The input vector is updated with the new token using \(}}+()\), and the process repeats for the next iteration. Figure 2 illustrates the training and inference pipeline.

**Comment 8**.: _We note that the setting described above deviates from the theory setting. 1) While the theoretical analysis focuses on the representational power of a single auto-regressive decision tree, the experiments utilize ensembles of decision trees. Notably, tree ensembles are more expressive, which suggests that our positive findings should also extend to these ensembles. 2) For simplicity, our theoretical study examines trees that generate a single output token in each iteration, rather than producing a word vector, which is the approach used in the experiments. 3) The decision treesdiscussed theoretically operate on concatenated token vectors within a sliding window, in contrast to the use of vector averages in the experimental setting._

### The Ability to Generate Coherent Stories

We test ARDTs' ability to generate stories with the _TinyStories_Eldan and Li (2023) dataset, which is a widely-used high-quality synthetic dataset of short stories that contain words that a 3 to 4-year-old child can understand, generated by GPT-3.5 and GPT-4. Details can be found in Appendix B.2.

For experiments conducted on TinyStories, we strictly follow Eldan and Li (2023) and employ the multidimensional score provided by GPT-4, as detailed in Appendix B.5.

For baselines to compare with ARDTs, we selected several Transformer-based models. These include two small Transformers trained on the TinyStories dataset (TinyStories-1M and TinyStories-33M Eldan and Li (2023)), as well as GPT-4 OpenAI et al. (2023), to illustrate the performance differences between non-neural network methods and the Transformer architecture.

For our evaluation, we provide the models with 100 story beginnings (refer to examples in Appendix B.4), each consisting of fewer than 6 words, generated by GPT-4. We use these beginnings as inputs to the model, allowing the it to perform next token prediction, ultimately generating outputs of 20 words. For the ground truth row in Table 1, we grade complete stories from the dataset.

As shown in Table 1, ARDTs achieved performance comparable to GPT-4 and TinyStories-33M on four metrics: grammar, creativity, consistency, and plot. Our model outperforms TinyStories-1M, a Transformer-based model with 1M parameters, despite being smaller in size. These results demonstrate that although tree-based models are generally considered inferior to large neural networks, surprisingly, they can compete with small Transformers when trained on the TinyStories dataset.

### Evaluating ARDTs in Language Reasoning Tasks

We now explore the potential of using decision trees for logical reasoning tasks using the Big-Bench-Hard dataset. The Big-Bench-Hard dataset, detailed in Appendix B.2, contains 23 challenging

Figure 2: **The Pipeline of Our Method.****(a) Training.** First, we employ a Word2Vec model to convert words into embeddings. Next, we utilize a sliding window approach to construct a dataset for training decision trees. Within this window, we performed a weighted average calculation, and the following token after the window was used as the label. **(b) Inference.** We use our trained Decision Trees for the purpose of next-token prediction.

reasoning tasks from the BIG-Bench benchmark. We selected four representative reasoning tasks for evaluation, with examples provided in Appendix B.2.

Each task involves training a separate decision tree ensemble. These ensembles utilize a weighted average of input word embeddings, as described in Section 4.1, using the word embedding layer from a pre-trained GPT-2 model trained on WebText. Each model is trained with 200 examples and tested on 50 examples. We also experiment with decision trees trained on top of a pre-trained GPT-2 Transformer model, where the output vectors from GPT-2 serve as input features for the decision trees, combining GPT-2's advanced language understanding with the analytical capabilities of decision trees.

For establishing baselines, we follow the methodology of Suzgun et al. (2022) and use accuracy as the metric. InstructGPT, Codex, and PaLM 540B are used as baselines.

As presented in Table 2, our model demonstrates substantial effectiveness in reasoning tasks, with performance comparable to state-of-the-art methods. For instance, we observe improvements of 7.4% in Boolean Expression tasks, 2% in Navigate tasks, and 7.8% in Sports Understanding tasks. Moreover, we find that further enhancements are possible by integrating decision trees with the GPT-2 Transformer, underscoring the significant impact of word embeddings on performance. However, his paper focuses on highlighting the potential of the ARDT's architecture, not word embeddings. Our results show that the ARDT's model has strong reasoning abilities.

## 5 Discussion

The findings in this paper demonstrate that tree-based models have potential in language generation. Although they do not yet match the performance of large language models, they possess certain advantages that make them valuable for studying the emergence of intelligence on a smaller scale. Decision trees are easier to interpret (see Appendix C for more on interpretability using ARDT's), simpler to understand and analyze mathematically, and fast to train. Moreover, unlike standard neural

    & Model Architecture & Parameters\({}^{*}\) & Grammar\(\) & Creativity\(\) & Consistency\(\) & Plot\(\) \\  TinyStories-1M & GPT-Neo & 1M & 4.42 & 2.70 & 6.32 & 3.65 \\ TinyStories-33M & GPT-Neo & 33M & 7.80 & 6.87 & 9.10 & 7.65 \\ GPT-4 & GPT-4 & 1800B & 9.93 & 8.51 & 9.32 & 8.24 \\ Ground Truth & / & / & 8.21 & 6.32 & 7.87 & 7.56 \\ ARDT's (Ours) & Decision Tree & 0.3M & 7.85 & 4.10 & 7.36 & 5.39 \\   

* For our decision trees, we report the total number of tree nodes in the ensemble as the parameter count.

\(\) To minimize the impact of inconsistency on our results and enhance the robustness of our evaluation metrics, we calculated the average scores from ten assessments for each of the 100 stories. Each story was evaluated ten times using the same prompt provided to GPT-4.

Table 1: Experiment Results on TinyStories: The results show that an auto-regressive tree can achieve better performance as the GPT-Neo architecture and exhibit competitive performance compared to both GPT-4 and TinyStories-33M.

    & & Srivastava et al. (2023) & & & & & Ours \\  BIG-Bench Hard & Random & SOTA & Human-Rater & InstructGPT & Codex & PaLM 540B & 
 Lin \\  & GPT \\  Boolean Expressions & 50 & 68.5 & 79.4 & 90 & 88.4 & 83.2 & 72.0 & 85.3 \\ Navigate & 50 & 56 & 81.9 & 68 & 50.4 & 62.4 & 55.4 & 69.2 \\ Web-of-Lies & 50 & 59.6 & 81.3 & 51.6 & 51.6 & 51.2 & 53.2 & 71.1 \\ Sports Understanding & 50 & 68.1 & 70.8 & 71.6 & 72.8 & 80.4 & 72.3 & 83.9 \\  All Tasks (avg) & 50 & 63.1 & 78.4 & 70.3 & 65.8 & 69.3 & 63.2 & 77.4 \\   

Table 2: Experimental Results on BIG-Bench-Hard. Lin: Linear Embedding; GPT: GPT-2 Embedding. The results demonstrate that ARDT’s possess good reasoning capabilities.

networks, the inference time for decision trees typically increases _logarithmically_ with their size: a tree with depth \(d\) can have \(2^{d}\) nodes but only requires traversing \(O(d)\) nodes per input.

This paper serves as a preliminary exploration into using ARDTs for language modeling tasks. We aim to inspire further research that integrates tree-based models into current language model pipelines, leveraging their unique strengths to enhance language generation capabilities. We believe incorporating tree-structured models into hybrid models with Transformers could be a promising direction for future research.