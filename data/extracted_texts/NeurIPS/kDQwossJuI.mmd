# Limits, approximation and size transferability for GNNs on sparse graphs via graphops

Thien Le

MIT

thienle@mit.edu

&Stefanie Jegelka

TU Munich and MIT

stefje@csail.mit.edu

###### Abstract

Can graph neural networks generalize to graphs that are different from the graphs they were trained on, e.g., in size? In this work, we study this question from a theoretical perspective. While recent work established such transferability and approximation results via graph limits, e.g., via graphons, these only apply nontrivially to dense graphs. To include frequently encountered sparse graphs such as bounded-degree or power law graphs, we take a perspective of taking limits of operators derived from graphs, such as the aggregation operation that makes up GNNs. This leads to the recently introduced limit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how the operator perspective allows us to develop quantitative bounds on the distance between a finite GNN and its limit on an infinite graph, as well as the distance between the GNN on graphs of different sizes that share structural properties, under a regularity assumption verified for various graph sequences. Our results hold for dense and sparse graphs, and various notions of graph limits.

## 1 Introduction

Since the advent of graph neural networks (GNNs), deep learning has become one of the most promising tools to address graph-based tasks (Gilmer et al., 2017; Scarselli et al., 2009; Kipf and Welling, 2017; Bronstein et al., 2017). Following the mounting success of applied GNN research, theoretical analyses follow with many works studying GNNs' representational power (Azizian and Lelarge, 2021; Morris et al., 2019; Xu et al., 2019, 2020; Garg et al., 2020; Chen et al., 2020; Maron et al., 2019; Loukas, 2020, 2020).

A hitherto less addressed question of practical importance is the possibility of size generalization, i.e., transferring a learned GNN to graphs of different sizes (Ruiz et al., 2023; Levie et al., 2022; Xu et al., 2021; Yehudai et al., 2021; Bevilacqua et al., 2021; Chuang and Jegelka, 2022; Roddenberry et al., 2022; Maskey et al., 2022, 2023), especially for sparse graphs. For instance, it would be computationally desirable to train a GNN on small graphs and apply it to large graphs. This question is also important to judge the reliability of the learned model on different test graphs. To answer the size generalization question, we need to understand under which conditions such transferability is possible - since it may not always be possible (Xu et al., 2021; Yehudai et al., 2021; Jegelka, 2022) - and what output perturbations we may expect. For a formal analysis of perturbations and conditions, we need a suitable graph representation that captures inductive biases and allows us to compare models for graphs of different sizes. _Graph limits_ can help to formalize this, as they help understand biases as the graph size tends to infinity.

Formally, _approximation theory_ asks for bounds between a GNN on a finite graph and its infinite counterpart, while _transferability_ compares model outputs on graphs of different sizes.

The quality of the bounds depends on how the two GNNs (and corresponding graphs) are intrinsically linked, in particular, to what extent they share relevant structure. This yields conditions for size generalization. For example, the graphs could be sampled from the same graph limit (Ruiz et al., 2023) or from the same random graph model (Keriven et al., 2020).

In particular, Ruiz et al. (2023) study approximation and transferability via the lens of _graphons_(Lovasz, 2012; Lovasz and Szegedy, 2006), which characterize the limits of _dense_ graphs. Yet, many real-world graphs are not dense, for instance, planar traffic networks, power law graphs, polymer graphs, Hamming graphs (including hypercubes for error-correcting code), or grid-like graphs e.g., for images. For _sparser_ graphs, the correct notion of limit suitable for deep learning is still an open problem, as typical bounded-degree graph limits such as the Benjamini-Schramm limit of random rooted graphs (Benjamini and Schramm, 2001), or graphings (Lovasz, 2012) are less well understood and often exhibit pathological behaviors (see Section 2.1). Limits of intermediate graphs, such as the hypercubes, are even more obscure. Hence, understanding limits, inductive biases and transferability of GNNs for sparse graphs remains an open problem in understanding graph representation learning.

This question is the focus of this work. To obtain suitable graph limits for sparse graphs and to be able to compare GNNs on graphs of different sizes while circumventing challenges of sparse graph limits, we view a graph as an _operator_ derived from it. This viewpoint is naturally compatible with GNNs, as they are built from convolution/aggregation operations. We show how the operator perspective allows us to define limits of GNNs of infinite sequences of graphs. We achieve this by exploiting the recently defined notion of _graphop_, which generalizes graph shift operators, and the _action convergence_ defined in the space of graphops (Backhausz and Szegedy, 2022). Our definition of GNN limits enables us to prove rigorous bounds for approximation and transferability of GNNs for sparse graphs. Since graphops encompass both graphons and graphings, we generalize similar bounds for graphon neural networks (Ruiz et al., 2023; Maskey et al., 2023) to a much wider set of graphs.

Yet, using graphops requires technical work. For instance, we need to introduce an appropriate discretization of a graphop to obtain its corresponding finite graph shift operators. We use these operators to define a generalized graphop neural network that acts as a limit object, with discretizations that become finite GNNs. Then we prove approximation and transferability results for both the operators (graphops and their discretizations) and GNNs.

**Contributions.** To the best of our knowledge, this is the first paper to provide approximation and transferability theorems specifically for sparse graph limits. Our main tool, graphops, has not been used to study GNNs before, although viewing graphs as operators is a classic theme in the literature. Our specific contributions are as follows:

1. We define a _graphop convolution_, i.e., an operator that includes both finite graph convolutions and a limit version that allows us to define a limit object for GNNs applied to graphs of size \(n\).
2. We rigorously prove an approximation theorem (Theorem 2) that bounds a distance between a graphop \(A\) (acting on infinite-dimensional space) and and appropriate discretization \(A_{n}\) (acting on \(^{n}\)), in the \(d_{M}\) metric introduced by Backhausz and Szegedy (2022). Our result applies to a more general set of nonlinear operators, and implies a transferability bound between finite graphs (discretizations) of different sizes.
3. For neural networks, we present a quantitative approximation and transferability bound that guarantees outputs of graphop neural networks are close to those of the corresponding GNNs (obtained from discretization).

### Related work

A summary of comparisons between our framework and related papers is in Table 1.

In structure, the closest related work is (Ruiz et al., 2023), which derives approximation and transferability theorems for _graphon_ neural networks, i.e., _dense_ graphs. For graphons, the convolution kernel has a nice spectral decomposition, which is exploited by Ruiz et al.

(2023a). In contrast, _sparse_ graph limits are not known to enjoy nice convergence of the spectrum (Backhausz and Szegedy, 2022; Aldous and Lyons, 2007), also Appendix C.1, so we need to use different techniques. Since the notion of graphop generalizes both dense graph limits and certain sparse graph limits, our results apply to dense graphs as well. Our assumptions and settings are slightly different from Ruiz et al. (2023a). For instance, they allow the convolution degree \(K\) and perform the analysis in the spectral domain, whereas our \(K\) is assumed to be a fixed finite constant. As a result, their bound has better dependence of \(O(1/n)\) on \(n\)-the resolution of discretization, but does not go to \(0\) as \(n\). Ours have extra dependence on \(K\) and a slower rate of \(O(n^{-1/2})\) but our bounds go to \(0\) as \(n\). Maskey et al. (2023) obtains further results for graphons on unbounded domain. Ruiz et al. (2023b) studies the spectrum of sparser (but still \((n^{2})\) edges) graphons.

Other works use other notions than graph limits to obtain structural coherence. Levie et al. (2022) obtain a transferability result for spectral graph convolution networks via analysis in frequency domains. They sample finite graphs from general topologies as opposed to a graph limit. Their graph signals are _assumed_ to have finite bandwidth while ours is only assumed to be in \(L^{2}\). Their signal discretization scheme is assumed to be close to the continuous signals, while ours is proven to be so. Roddenberry et al. (2022) address sparse graphs and give a transferability bound between the loss functions of two random rooted graphs. However, the metric under which they derive their result is rather simple: if the two graphs are not isomorphic then their distance is constant, otherwise, they use the Euclidean metric between the two graph signals. This metric hence does not capture combinatorial, structural differences of functions on non-isomorphic graphs. To study transferability, Keriven et al. (2020) sample from standard random graph models, resulting in a bound of order \(O(n^{-1/2})\) for dense graph and \(O(( n)^{-1/2})\) for relatively-sparse graphs. They do not cover bounded-degree graphs and their bounds hold with high probability. In general, having access to deterministic graph limit is considered a weaker assumption than having access to truly random graphs since large graphs satisfy some notion of 'almost randomness' (e.g. via Szemeredi regularity lemma). There are fascinating, tangle separations between graph limits and random graphs, especially among sparse graphs, that are outside the scope of this paper.

Adjacent to transferability studies, Bevilacqua et al. (2021) (also inspired by Lovasz and Szegedy (2006)) uses induced homomorphism density to construct a graph representation that works across graph sizes and demonstrates empirical gains in using this larger representation. Maskey et al. (2022) uses graph limits to deduce generalization bounds for GNNs.

## 2 Background

NotationLet \(\) be \(\{1,2,\}\) and write \([n]=\{1,,n\}\) for any \(n\). For a scalar \(\) and a set \(S\), let \( S=\{ s:s S\}\). 'A.e.' stands for 'almost everywhere'.

   &  &  \\   & Bounded-degree & Relatively-sparse &  \\  Number of edges & \((n)\) & \((n n)\) & \((n^{2})\) \\  Examples covered under our assumptions &  infinite grids, \\ polymer graphs \\  &  hypercubes, \\ Hamming graphs \\  & 
 graphons \\  \\   Graphons (Ruiz et al., 2023a) & & & & \(O(n^{-1})^{1}\) \\  Unbounded graphons (Maskey et al., 2023) & & & inexplicit \\  Random graph model (Keriven et al., 2020) & & \(O(( n)^{-1/2})\) & \(O(n^{-1/2})\) \\  Spectral methods (1 layer) (Levie et al., 2022) & & inexplicit & inexplicit \\  Graphings (1 layer) (Roddenberry et al., 2022) & inexplicit & & \\   & \(O(n^{-1/2})\) & \(O(n^{-1/2})\) & \(O(n^{-1/2})\) \\  

Table 1: Summary of our results compared to related work. Quantitative results (e.g. \(O(n^{-1/2})\)) upper-bound the distance between GNNs on sampled graphs of size \(n\) and the limiting object in term of \(n\) (in an appropriate metric and limit notion). Empty cells are graph models where the approaches in the corresponding papers do not apply to or give trivial bounds (e.g. bounds that compare to a constant-\(0\) graphon). â€™Inexplicit* refers to asymptotic results where rates of convergence is not explicit.

For a measure space \((,,)\) and \(p[1,]\), denote by \(L^{p}()\) the corresponding \(L^{p}\) function spaces with norm \(\|\|_{p}:f(_{}|f|^{p}d)^{1/p}\). For any \(p,q[1,]\), define the operator norms \(\|\|_{p q}:A_{v L^{}}\|vA\|_{q}/\|v\|_{p}\).

For function spaces, we use \(=L^{2}()\) and \(_{n}=L^{2}([n]/n)\), for any \(n\). For any \(L^{p}\) space \(\), denote by \(_{[-1,1]}\) the restriction to functions with range in \([-1,1]\) a.e. and \(_{(L)}\) the restriction to functions that are \(L\)-Lipschitz a.e. and \(_{(L)}=_{[-1,1]}_{( L)}\).

Graph neural networks (GNNs)GNNs2 are functions that use graph convolutions to incorporate graph structure into neural network architectures. Given a finite graph \(G=(V,E)\) and a function \(X:V\) (called _graph signal_ or _node features_), a GNN \(_{F}\) (\(F\) for 'finite') with \(L\) layers, \(n_{i}\) neurons at the \(i\)-th layer, nonlinearity \(\) and learnable parameters \(h\), is:

\[_{F}(h,G,X) =X_{L}(h,G,X),\] (1) \[[X_{l}(h,G,X)]_{f} =_{g=1}^{n_{l-1}}A_{l,f,g}(h,G)[X_{l-1 }]_{g}, l[L],f[n_{l}]\] (2) \[X_{0}(h,G,X) =X,\] (3)

where \([X_{l}]_{f}\) is the output of the \(f\)-th neuron in the \(l\)-th layer, which is another graph signal. The input graph information is captured through order \(K\)_graph convolutions_\(A_{l,f,g}(h,G):=_{k=0}^{K}h_{l,f,g,k}GSO(G)^{k}\), where \(GSO(G)\) is a _graph shift operator_ corresponding to \(G\) -- popular examples include the adjacency matrix or the Laplacian (Kipf and Welling, 2017; Levie et al., 2022). The power notation is the usual matrix power, while the notation \(h_{l,f,g,k}\) highlights that there is a learnable parameter for each convolution order \(k\), between each neuron \(f\) and \(g\) from layer \(l-1\) to layer \(l\) of the neural network. Thus, the number of learnable parameters in a GNN does not depend on the number of vertices of the graph.

### Graph limits

Graph limit theory involves embedding discrete graphs into rich topological or geometric spaces and studying the behavior of convergent (e.g. in size) graph sequences.

Dense graphsA popular example of graph limits are graphons - symmetric \(L^{1}(^{2})\) (Lebesgue-measurable) functions whose value at \((x,y)\) can be thought of (intuitively) as the weight of the \(xy\)-edge in a graph with vertices in \(\). Convergence in this space, under the _cut metric_ (see Appendix A for the exact definition), is dubbed _dense graph convergence_ because for any \(W L^{1}(^{2}),\|W\|_{}=0\) iff \(W=0\) outside a set of Lebesgue measure \(0\). This implies that graphs with a subquadratic number of edges, such as grids or hypercubes, are identified with the empty graph in the cut norm. Dense graph convergence is very well understood theoretically and is the basis for recent work on GNN limits (Ruiz et al., 2023a).

Sparse graphs_Graphing_(Lovasz, 2012), is a direct counterpart of a graphon for sparse graphs. Recall that graphons are not suitable for sparse graphs because the Lebesgue measure on \(L^{2}(^{2})\) is not fine enough to detect edges of bounded-degree graphs. Therefore, one solution is to consider other measure spaces. Graphings are quadruples \((V,,,E)\) where \(V\) and \(E\) are interpreted as the usual vertex and edge sets and \((V,,)\) together form a Borel measure such that \(E\) is in \(\) satisfying a symmetry condition. While Lebesgue measures are constructed from a specific topology of open sets on \(\), for graphings, we are allowed the freedom to choose a different topological structure (for instance a _local topology_) on \(V\). The definition of graphings is theoretically elegant but harder to work with since the topological structures are stored in the \(\)-algebra.

Furthermore, a famous open conjecture by Aldous and Lyons (2007) asks whether all graphings are weak local limits of some sequence of bounded-degree graphs. The unresolved conjecture of Aldous and Lyons means that one cannot simply take an arbitrary graphing and be guaranteed a finite bounded-degree graph sequence converging to said graphing, which is the main approach in Ruiz et al. (2023a) for dense graphs. As a result, we expect some regularity assumptions on the graph sequence for any work that handles sparse graph limits, unless the authors try to tackle this conjecture itself. A self-contained summary of graphings within the scope of this paper is provided in Appendix C. Infinite paths and cycles also have nice descriptions in terms of graphings (also in Appendix C), which we will use in our constructions for Lemma 2.

### Graphops and comparing across graph sizes

More recently, Backhausz and Szegedy (2022) approach graph limits from the viewpoint of limits of operators, called _graphops_. This viewpoint is straightforward for finite graphs: both the adjacency matrix and Laplacian, each defining a unique graph, are linear operators on \(^{\#}\). Moreover, viewing graphs as operators is exactly what we do with GSOs and graph convolutions. Hence, graphop seems to be an appropriate tool to study GNN approximation and transferability. On the other hand, there are challenges with this approach: being related to graphings, they inherit some of graphings' limitations, such as the conjecture of Aldous and Lyons (2007) and discontinuity of eigenvalues at the limit (Appendix C.1). Moreover, to understand GNN transferability from size \(m\) to \(n\), one needs to compare an \(m m\) matrix with an \(n n\) matrix, which is nontrivial. This is done by comparing their actions on \(^{m}\) versus \(^{n}\). It turns out that these actions, under an appropriate metric, define a special mode of operator convergence called _action convergence_. The resulting limit objects are well-defined and nontrivial for sparse graphs and intermediate graphs, while also generalizing dense graphs limits. We will describe this mode of convergence, the corresponding metric, and our own relaxation of it later in this section.

We now describe how graphs of different sizes can be compared through the actions of their corresponding operators on some function spaces.

Nonlinear (not necessarily linear) \(P\)-operatorsFor an \(n\)-vertex graph, its adjacency matrix, Laplacian, or random walks kernels are examples of operators on \(L^{p}([n]/n)\). To formally generalize to the infinite-vertex case, Backhausz and Szegedy (2022) use \(P\)_-operators_, which are linear operators from \(L^{}()\) to \(L^{1}()\) with finite \(\|A\|_{ 1}\). In this paper, we further assume they have finite \(\|\|_{2 2}\) norm but are not necessarily linear. This allows us to consider nonlinear GNN layers and the whole GNN itself in the same framework.

Graphops\(P\)-operators lead to a notion of graph limit that applies to both dense and sparse graphs. _Graphops_(Backhausz and Szegedy, 2022) are positivity-preserving3, self-adjoint \(P\)-operators. Adjacency matrices of finite graphs, graphons (Lovasz and Szegedy, 2006), and graphings (Lovasz, 2012) are all examples of graphops.

\((k,L)\)-profile of a nonlinear \(P\)-operatorActions of graphops are formally captured through their \((k,L)\)-profiles, and these will be useful to compare different graphops. Pick \(k\), \(L[0,]\) and \(A\) a \(P\)-operator on \((,,)\). Intuitively, we will take \(k\) samples from the space our operators act on, apply our operator to get \(k\) images, and concatenate samples and images into a joint distribution on \(^{2k}\), which gives us one element of the profile. For instance, for \(n\)-vertex graphs, the concatenation results in a matrix \(M^{n 2k}\), so each joint distribution is a sum (over rows of \(M\)) of \(n\) Dirac distributions. In the limit, the number of atoms in each element of the profile increases, and the measure converges (weakly) to one with density. More formally, denote by \((v_{1},,v_{k})\) the pushforward of \(\) via \(x(v_{1}(x),,v_{k}(x))\) for any tuple \((v_{i})_{i[k]} L^{2}()\). The \((k,L)\)_-profile_ of \(A\) is:

\[_{k,L}(A):=\{(v_{1},,v_{k},Av_{1},,Av_{k}): v_{i} L^{}_{(L)}(),i=1 k\}.\] (4)

Formally, denote by \((^{k})\) the set of Borel probability distributions over \(^{2k}\). Regardless of the initial graph size, or the space on which the operators act, \((k,L)\)-profiles of \(A\) are always some subsets of \((^{2k})\) which allow us to compare operators acting on different spaces.

Convergence of \(P\)-operatorsWe compare two profiles (closed subsets \(X,Y(^{2k})\)) via a Hausdorff metric \(d_{H}(X,Y):=(_{x X}_{y Y}d_{LP}(x,y),_{y Y}_{x  X}d_{LP}(x,y))\).

Here, \(d_{LP}\) is the Levy-Prokhorov metric on \((^{2k})\) (see exact definition in Appendix A), which metrizes weak convergence of Borel probability measures, and translates action convergence to weak convergence of measures. Finally, given any two \(P\)-operators \(A,B\), we can compare their profiles across all different \(k\) at the same time as

\[d_{M}(A,B):=_{k=1}^{}2^{-k}d_{H}(_{k,L}(A), _{k,L}(B)).\] (5)

Intuitively, we allow \(d_{H}\) to grow subexponentially in \(k\) by the scaling \(2^{-k}\). Our definition of profile slightly differs from that of Backhausz and Szegedy (2022), using \(L^{}_{(L)}\) instead of their \(L^{}_{[-1,1]}\). However, we will justify this deviation in Section 4.4, Theorem 4.5: by letting \(L\) grow slowly in \(n\), we recover the original limits in Backhausz and Szegedy (2022).

This _action convergence_ turns out to be one of the 'right' notions of convergence that capture both sparse and dense graph limits, as well as some intermediate density graphs:

**Theorem 1** (Theorem 1.1 Backhausz and Szegedy (2022)).: _Convergence under \(d_{M}\) is equivalent (results in the same limit) to dense graph convergence when restricted to graphons and equivalent to local-global convergence when restricted to graphings._

## 3 Graphop neural networks

Graph limits allow us to lift finite graphs onto the richer space of graphops to discuss convergent graph sequences \(G_{i} G\). For finite GNNs (Eqn (2)), fixing the graph input \(G_{i}\) and learnable parameter \(h\) results in a function \(_{F}(h,G_{i},)\) that transforms the input graph signal (node features) into an output graph signal. The transferability question asks how similar \(_{F}(h,G_{i},)\) is to \(_{F}(h,G_{j},)\) for some \(i j\). In our approach using approximation theory, we will compare both functions to the limiting function on \(G\). This is done by an appropriate lift of the GNN onto a larger space that we call _graphop neural networks_.

We then introduce a discretization scheme of graphop neural networks to obtain finite GNNs, similar to graphon sampling (Ruiz et al., 2023a) and sampling from topological spaces (Levie et al., 2022). Finally, Lemma 1 asserts that, restricted to self-adjoint \(P\)-operators, discretizations of graphops are indeed graph shift operators (GSOs).

### Convolution and graphop neural networks

Similar to how GSOs in a GNN act on graph signals, graphops act on some \(L^{2}\) signals (called _graphop signals_). The generalization is straightforward: replacing GSOs in the construction of the GNN in Eqn. (2) with graphops results in _graphop convolution_ and replacing graph convolution with graphop convolution gives _graphop neural networks_.

Formally, fix a maximum order \(K\). For some measure space \((,,)\), select a graphop \(A:L^{2}() L^{2}()\) and a graphop signal \(X L^{2}()\). We define a _graphop convolution_ operator as a weighted sum of at most \(K-1\) applications of \(A\): \(H(h,A)[X]:=_{k=0}^{K-1}(h_{k}A^{k})[X]\), where \(h^{K}\) are (learnable) filter parameters and \(A^{k}\) is the composition of \(k\) duplicates of \(A\). The square bracket \([v]_{i}\) indicates the \(i\)-th entry of a tuple \(v\).

For some number of layers \(L,\{n_{i}\}_{i[L]},n_{0}:=1\), define a _graphop neural network_\(\) with \(L\) layers and \(n_{i}\) features in layer \(i\) as:

\[(h,A,X) =X_{L}(h,A,X),\] (6) \[X_{l}(h,A,X) =[_{g=1}^{n_{l-1}}H(h_{f,g}^{l},A)[ X_{l-1}]_{g}]_{f[n_{l}]}, l[L],\] (7) \[X_{0}(h,A,X) =X\] (8)

with filter parameter tuple \(h=(h^{1},,h^{L})\), \(h^{l}(^{K})^{n_{l} n_{l-1}}\) for any \(l[L]\), and graphop signal tuple \(X_{l}(L^{2}())^{n_{l}}\) for any \(l[L]\{0\}\). Eqn (7) and Eqn (2) are almost identical, with the only difference being the input/output space: graphops replacing finite graphs, and graphop signals replacing graph signals.

### From graphop neural networks to finite graph neural networks

We are specifically interested in finite GNNs that are discretizations of a graphop (for instance finite grids as discretizations of infinite grids), so as to obtain a quantitative bound that depends on the resolution of discretization. To sample a GNN from a given graphop \(A:\), we first sample a GSO and plug it into Eqn (2). Choose a resolution \(m\) and define the GSO \(A_{m}\), for any graph signal \(X_{m}\) (defined in Section 2) as:

\[A_{m}X(v) :=m_{v-}^{v}(A),  v[m]/m,\] (9) \[_{m}(h,A,X) :=(h,A_{m},X),\] (10)

where graphop signal \(\) is an extension of graph signal \(X_{m}\) defined as

\[(u):=X(),  u.\] (11)

Shared structural properties of sampled graphsThe discretization scheme introduced for graphop can be intuitively understood as partitioning the vertex set into finitely many sets and merging nodes and their connections in these sets, with an appropriate scaling of the edge weights. Imagine blurring an \(n n\) matrix into an \(n/2 n/2\) matrix by average-pooling over each disjoint \(2 2\) square. As \(n\) (and even for uncountable \(\)), this makes rigorous the notion of making a high-resolution graph on a huge number of vertices more 'blurry' by merging nodes in a way that still maintains some smoothness conditions, which is reminiscent of the real-world procedures of training with low-resolution images before fine-tuning with higher-resolution ones, or sampling from low-frequency graph Fourier transform domain.

Note that if \(A\) is linear then \(A_{m}\) is necessarily linear, but our definition of graphop does not require linearity. Therefore, \(A_{m}\) is strictly more general than the matrix representation of graph shift operators. We have the following well-definedness result:

**Lemma 1**.: _If a graphop \(A:\) is self-adjoint, then for each resolution \(m\), the discretization \(A_{m}:_{m}_{m}\) defined above is also self-adjoint._

The proof can be found in Appendix B. Compared to previous works, our discretization scheme in Eqn (9) looks slightly different. In Ruiz et al. (2023), given a graphon \(W:^{2}\), the discretization at resolution \(n\) was defined by forming the matrix \(S^{n n}:S_{i,j}=W(i/n,j/n)\). A related discretization scheme involving picking the interval endpoints at random was also used, but the resulting matrix still takes values at discrete points in \(W\). These two sampling schemes rely crucially on their everywhere continuous assumptions for the graphon \(W\). Indeed, but for continuity requirements, two functions that differ only at finite discrete points \((i/n,j/n),i,j[n]\) are in the same \(L^{2}\) class of functions, but will give rise to completely different samples. Furthermore, not every \(L^{2}\) class of functions has a continuous representative. This means that our discretization scheme is strictly more general than that used by Ruiz et al. (2023) even when restricted to graphons. This difference comes from the fact that we are discretizing an operator and not the graph itself. For our purpose, taking values at discrete points for some limiting object of sparse graphs will likely not work, since sparsity ensures that most discrete points are trivial.

## 4 Main result: Approximation and transferability

### Results for \(P\)-operators

Our first set of theorems address approximation and transferability of \(P\)-operators: under certain regularity assumptions to be discussed later, \(P\)-operators are well approximated by their discretizations:

**Theorem 2** (Approximation theorem).: _Let \(A:\) be a \(P\)-operator satisfying Assumption 2 with constant \(C_{A}\); Assumption 3.1 or 3.2 with resolutions in \(\). Fix \(n\) and consider \((k,C_{v})\)-profiles. Let \(A_{n}:_{n}_{n}\) be a discretization of \(A\) as defined in Eqn (9). Then:_

\[d_{M}(A,A_{n}) 2C_{v}}{n}}++1}{n}.\] (12)Compared to theorems in Ruiz et al. (2023a), our explicit dependence on \(n\) has an extra \(n^{-1/2}\) term that stems from techniques used to bound the Levy-Prokhorov distance between two entry distributions obtained from functions that differ by at most \(O(n^{-1})\) in \(L^{2}\) norm.

As an immediate corollary, invoking the triangle inequality yields a transferability bound.

**Corollary 1** (Transferability).: _Let \(A:\) be a \(P\)-operator satisfying assumptions of Theorem 2 with constant \(C_{A}\) and resolutions \(\). For any \(n,m\), let \(A_{n}:_{n}_{n}\) and \(A_{m}:_{m}_{m}\) be discretizations as defined in Eqn (9). Then:_

\[d_{M}(A_{m},A_{n})(m^{-}+n^{-}) 2C_{v}}+(m^{-1}+n^{-1})(C_{v}+1).\] (13)

We emphasize that these theorems work for general nonlinear \(P\)-operators and not only the linear graphops defined in (Backhausz and Szegedy, 2022).

Proof sketchThe full proof of Theorem 2 is in Appendix D. To bound the distance in \(d_{M}\) between two operators, for each sample size \(k\), we give a bound on the Hausdorff metric \(d_{H}\) between the two \((k,C_{v})\)-profiles. As long as the dependence on \(k\) of these bounds is polynomial, the infinite sum in the definition of \(d_{M}\) converges. We do this by picking an arbitrary distribution \(\) from \(_{k,C_{v}}(A)\), which by definition is given by a \(k\)-tuple \(F\) of functions in \(L^{}_{(C_{v})}\). Discretize each element of \(F\) and consider its entry distribution results in \(_{n}_{k,C_{v}}(A_{n})\). We show that we can give an upper bound of \(d_{LP}(,_{n})\) that is independent of the choice of \(\) and thus same upper bound holds for \(_{_{k,C_{v}}(A)}_{_{n}_{k,C_{v}}(A _{n})}d_{LP}(,_{n})\). By also selecting an arbitrary element of \(_{k,C_{v}}(A_{n})\) and extending it to an element of \(_{k,C_{v}}(A)\), we obtain another upper bound for \(_{_{n}_{k,C_{v}}(A_{n})}_{_{k,C_{v} }(A)}d_{LP}(,_{n})\) and thus for \(d_{H}\). The different assumptions come in via different techniques used to bound \(d_{LP}\) by a high probability bound on the \(L^{2}\) norm of the functions in \(F\) and their discretization/extension.

### Results for graphop neural networks

Not only are graphops and their discretizations close in \(d_{M}\), but, as we show next, neural networks built from a graphop are also close to those built from graphop discretizations in \(d_{M}\). We iterate that here we are comparing nonlinear operators (graphop neural networks) that are acting on different spaces (\(L^{2}([n]/n)\) for some finite \(n\) versus \(L^{2}()\)).

Before stating theoretical guarantees for graphop neural networks, let us introduce some assumptions on the neural network activation function and parameters:

**Assumption 1**.: _Let the activation function \(:\) in the definition of graphop neural networks be \(1\)-Lipschitz and the convolution parameters \(h\) be such that \(|h| 1\) element-wise._

**Theorem 3** (Graphop neural network discretization).: _Let \(A:\). Assume that \(A\) satisfies Assumption 2 with constant \(C_{A}\) and Assumption 3.A or 3.B with resolutions in \(\). Fix \(n\) and consider \((k,C_{v})\)-profiles. Under Assumption 1, we have:_

\[d_{M}((h,A,),(h,A_{n},)) P_{1}C_{v}}{n}}++1}{n},\] (14)

_where \(_{A}:=(n_{}_{i=1}^{K}C_{A}^{i})^{L}\), \(n_{}=_{l[L]}n_{l}\), and \(P_{1}\) is a constant depending on \(K,L\). Furthermore, we can invoke the triangle inequality to compare outputs of graphop neural networks built from two different discretizations of \(A\). For any \(m,n\),_

\[d_{M}((h,A_{m},),(h,A_{n},)) P_{1}C_{v}}(m^{-}+n^{-})+(C_{v}+1 )(n^{-1}+m^{-1}).\] (15)

Compared to the main theorems of Ruiz et al. (2023a), there are two main differences in our results. First, our rate of \(O(n^{-1/2})\) is slower than the rate of \(O(n^{-1})\) in Ruiz et al. (2023a) as a function of \(n\). Yet, second, their bounds contain a small term that is independent of \(n\) and does not go to \(0\) as \(n\) goes to infinity. This small term depends on the variability of small eigenvalues in the spectral decomposition of the convolution operator associated with a graphon. The bound in Theorem 3, in contrast, goes to zero. We tested this bound in Figure 1 for GNNs on polymer graphs, which suggest that \(O(n^{-1})\) rate may be possible.

The proof for this theorem is in Appendix D.3 for a more general Theorem 6. Note that it does not suffice to simply use the fact that the assumptions play well with composition with Lipschitz function \(\), which would result in a bound involving \((h,A,)\) and its discretization \(((h,A,))_{n}\) as a nonlinear operator, as opposed to a bound between \((h,A,)\) and \((h,A_{n},)\).Our proof shares the same structure as that of Theorem 2 while making sure that the mismatch from discretizing/extending operators does not blow up with composition.

### Assumptions

We discuss the main assumptions of our \(P\)-operators.

**Assumption 2** (Lipschitz mapping).: _An operator \(A:\) is \(C_{A}\)-Lipschitz if \(\|Af-Ag\|_{2} C_{A}\|f-g\|_{2}\) for any \(f,g\)._

We have already had a finite bound on the operator norm in the definition of \(P\)-operators. For linear operators, Assumption 2 is equivalent to a bounded operator norm and is thus automatically satisfied by linear \(P\)-operators.

The next few assumptions are alternatives; only one needs be satisfied by our \(P\)-operators. Intuitively, they ensure that the images of our operator are not too discontinuous:

**Assumption 3.A** (Maps constant pieces to constant pieces).: _We say that an operator \(A:\) maps constant pieces to constant pieces at resolutions in \(\) if for any \(n\), and for any \(f_{[-1,1]}\) that is a.e. constant on each interval \((u-1/n,u]\) for \(u[n]/n\), \(Af\) is also constant on \((u-1/n,u]\) for each \(u\)._

**Assumption 3.B** (Maps Lipschitz functions to Lipschitz functions).: _We say that an operator \(A:\) maps Lipschitz functions to Lipschitz functions at resolutions in \(\) if for any \(n\), and for any \(f_{(C_{v})}\), \(Af\) is \(C_{v}\)-Lipschitz._

This is the most restrictive assumption. However, the next lemma (proof in Appendix D) describes some dense, sparse and intermediate graphs that satisfy these assumptions.

**Lemma 2** (Well-behaved operators).: _The following examples satisy our assumptions:_

1. _Bounded-degree graphings: Let_ \(G\) _be a graphing corresponding_ \(k\)_-D grids or polymer graphs (not necessarily regular graphs) with fixed monomers (Appendix A). For each_ \(N\)_, there exists a locally equivalent graphing_ \(G^{}_{N}\) _such that its adjacency operator satisfies Assumption_ 3.A _with some resolution set (see Lemma_ 5 _and Lemma_ 7_)._
2. _Lipschitz graphons: Let_ \(W\) _be a_ \(C_{v}\)_-Lipschitz graphon_ \(_{(C_{v})}\)_. Then the Hilbert-Schmidt operator_ \(f_{0}^{1}W(,y)g(y)y\) _satisfies Assumption_ 3.B _with resolution in_ \(\)_._
3. _General graphs: Let_ \(G\) _be a (potentially infinite) graph with a vertex coloring of_ \(N\) _colors such that if two vertices have the same color, then the multisets of their neighbors' colors are the same. Then_ \(G\)_'s adjacency operator satisfies Assumption_ 3.A _with resolution_ \(N\)_. An_ \(N\)_-d hypercube (more generally, Hamming graphs) which is neither bounded-degree nor dense, satisfies the above condition with resolutions in_ \(\{2^{n}\}_{n[N]}\)_._

Figure 1: Hausdorff metric between samples from 1-profiles of 2-hidden-layer GNN on finite polymer graphs vs on large polymer graphs (see Appendix A for polymer graphs). The GNN uses GSO \(A_{n}^{2}+A_{n}\) where \(A_{n}\) is the normalized adjacency matrix on \(n\) nodes and ReLU nonlinearities at each layer. Different solid lines are different random draws of functions that make up the estimated 1-profile. See Appendix A for details.

All our results also hold with a less restrictive assumption that allows for a failure of Assumption 3.A and 3.B in a small set (see Assumption 4.A and 4.B in the Appendix). The most general results are proven in Appendix D and hold in even slightly more relaxed conditions which require the operators to map constant pieces to _Lipschitz_ pieces (Assumption 5.A, 5.B in Appendix D).

### Deviations and Justifications

All our theorems hold in slightly modified settings than those by Backhausz and Szegedy (2022). Namely, we allowed for nonlinear \(P\)-operators, assumed that they have finite \(\|\|_{2 2}\) norm, and used \((k,L)\)-profiles where we focus on Lipschitz functions (while Backhausz and Szegedy (2022) consider all measurable functions in their profiles). Therefore, we need to ensure that our changes still give us a useful mode of convergence that generalizes dense and sparse graph convergences.

First, without the linearity assumption, the convergence proof by Backhausz and Szegedy (2022) does not hold: we do not know if all limits of nonlinear graphops are still graphops. However, our approximation results (Theorem 2) show special convergent sequences of nonlinear operators, which go beyond the settings in (Backhausz and Szegedy, 2022). Studying special nonlinear operator sequences is interesting since graphop NNs themselves are nonlinear operators. We also assert that our restriction to operators acting on \(L^{2}\) spaces does not affect convergence guarantees (Theorem 2.14 in (Backhausz and Szegedy, 2022)).

Next, we show that restriction to Lipschitz profiles, which is necessary for our proof technique, does not affect the original convergence either, if we allow our Lipschitz constant to grow:

**Theorem 4** (Growing profiles).: _Let \(L:\) be a strictly increasing sequence such that \(L(n)\). Consider a sequence of \(P\)-operators \((A_{n}:_{n}_{n})_{n}\) that is Cauchy in the sense that \(d^{}_{M}(A_{n},A_{m}):=_{k=1}^{}2^{-k}d_{H}(_{k,L(n) }(A_{n}),_{k,L(m)}(A_{m})) 0\) as \(m,n\). If \(A_{n} A\) under action convergence (Backhausz and Szegedy, 2022), then \((A_{n})_{n}\) converges to the same limit under \(d^{}_{M}\)._

This theorem allows us to replace the \(C_{v}\) constant in our bound with a slowly growing function in \(n\) and get back 'action convergence' as described in Backhausz and Szegedy (2022) so that we can inherit its useful properties while still be able to draw on Lipschitz assumptions in the profiles, without any realistic slowdown in the bound.

Proof sketchFor some \(k\), by the completeness of the Hausdorff metric over the closed subsets of the space of probability measures supported on \(^{2k}\), the statement is equivalent to showing \(d_{H}(_{k}(A),_{k,L(n)}(A_{n})) 0\) as \(n\). The proof uses a Lipschitz mollification argument to smooth out arbitrary measurable functions \(f_{1},,f_{k}\) that witness a measure in the \(k\)-profile of \(A\). By selecting a Lipschitz mollifier \(\), we ensure that convolving \(f_{j}\) with \(_{}:x^{-1}(x^{-1})\) results in a Lipschitz function that converges to \(f\) in \(L^{2}\) as \(\) goes to \(0\).

## 5 Discussion and Future directions

In this paper, we study size transferability of finite GNNs on graphs that are discretizations of graphop, a recent notion of graph limit introduced by Backhausz and Szegedy (2022). We achieve this by viewing GNNs as operators that transform one graph signal into another. Under regularity assumptions, we proved that two GNNs, using two different-resolution GSOs discretized from the same graphop, are close in an operator metric built from weak convergence of measures.

For future direction, a principled study of spectral properties of graphops and graphop neural networks would open doors for techniques from Fourier analysis as used in (Ruiz et al., 2023; Levie et al., 2022). This leads to distinct challenges, e.g., the spectral gap is not continuous with respect to local-global limits and thus action convergence, but many more properties of spectral measures of bounded-degree graphs are recently studied (Virag, 2018).