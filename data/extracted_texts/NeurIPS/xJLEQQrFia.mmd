# Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks

Minki Kang\({}^{1,2}\)

Work done at AITRICS. \({}^{}\)Code is available at https://github.com/Nardien/KARD.

Seanie Lee\({}^{2}\)

Jinheon Baek\({}^{2}\)

Kenji Kawaguchi\({}^{3}\)

Sung Ju Hwang\({}^{2,4}\)

\({}^{1}\)KRAFTON, \({}^{2}\)KAIST, \({}^{3}\)National University of Singapore, \({}^{4}\)DeepAuto.ai

{zzxc1133, lsnfamily02, jinheon.baek}@kaist.ac.kr,

kenji@comp.nus.edu.sg, sjhwang82@kaist.ac.kr

###### Abstract

Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose **K**nowledge-**A**ugmented **R**easoning **D**istillation (**KARD**), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks.

## 1 Introduction

Large Language Models (LLMs) [5; 8] have excelled at various tasks across diverse domains with in-context learning. Recently, scaling up the number of parameters of LLMs has been shown to significantly improve their knowledge encoding and reasoning capability [54; 24]. Moreover, such LLMs have achieved remarkable performance on knowledge-intensive tasks in professional domains which are highly challenging, since they require a considerable depth of domain knowledge and reasoning [34; 48]. For example, in Figure 1 top, answering a medical question requires both domain knowledge and reasoning ability. The LLM should understand that the patient likely has ALS based on the symptoms and recognize SOD1 is the main cause of motor neuron diseases. Furthermore, it needs to reason over the knowledge that a mutation in SOD1 is highly associated with the symptoms.

Despite its effectiveness, deploying LLMs can still be challenging, especially in real-world applications. Firstly, utilizing LLMs to make predictions is computationally expensive. It requires 326GB GPU memory to load the GPT3-175B model . Moreover, deployment of the LLM potentially poses a risk of privacy leakage since most of the production-grade LLMs [5; 44; 8; 43] operate in a black-box manner. That is, users cannot access the parameters of LLMs but only their output via some Application Programming Interfaces. Consequently, the need for _white-box Small Language Models_ tailored to address problems requiring domain-specific knowledge will continue to gainprominence. To tackle the above challenges of deploying models, previous works [33; 17; 38; 12; 18] have proposed to transfer the reasoning ability of large models to small models through _reasoning distillation_ (See Figure 1 left). In particular, they leverage the LLM to generate high-quality rationales and fine-tune a small LM to generate the rationale obtained from the LLM. This reasoning distillation improves the performance of small LMs on tasks that require complex reasoning ability (e.g., arithmetic and symbolic reasoning [10; 55]). Based on this observation, we pose a research question: _"Is it possible to transfer both the domain knowledge and reasoning ability of LLMs through reasoning distillation, for tasks requiring specific knowledge to reason for answering a question?"_

Existing reasoning distillation is suboptimal to solve such knowledge-intensive reasoning tasks since small, distilled LMs are limited in their capacity to memorize the knowledge that is necessary to solve the tasks due to the small number of parameters. This motivates us to develop a method that distills the reasoning ability of LLMs into smaller LMs while injecting the specific task-relevant knowledge. Specifically, we augment a small LM with the knowledge retrieved from an external Knowledge Base (KB) as a non-parametric memory, and we theoretically show that the non-parametric memory can reduce the number of bits to memorize training data for performing well.

Based on this intuition and the theoretical analysis, we propose **Knowledge-Augmented Reasoning Distillation (KARD)** which enables to transfer the reasoning ability of an LLM to a small LM while injecting the knowledge, for knowledge-intensive reasoning tasks. Specifically, we utilize a retriever  to obtain passages containing relevant knowledge for generating a rationale from an external knowledge base (e.g., Wikipedia). We then fine-tune the small LM to generate the rationale, obtained from the LLM, based on the question and the retrieved document, and predict the answer.

During training, using a rationale as a query helps retrieve pertinent knowledge for generating rationales. However, during inference, relying on the question as a query may result in poor retrieval. As shown in Figure 1, the passage retrieved with the question is not relevant to generating the rationale. To mitigate the issue, we introduce a _neural reranker_ to prioritize passages useful for rationale generation, ensuring a retrieval of relevant documents even with the question as the query.

To verify the efficacy of KARD, we empirically show that it significantly improves the performance of small LMs (OPT [20; 59] and T5 [45; 53]) on medical Question Answering (QA) (MedQA-USMLE ), multi-step factual QA (StrategyQA ), and commonsense reasoning (OpenbookQA ) datasets compared to few-shot in-context learning, fine-tuning, and reasoning distillation without knowledge augmentation. Also, our extensive analyses demonstrate that our KARD is efficient in terms of both the training data and the model size. Specifically, KARD with 250M models achieves higher accuracy than the fine-tuned 3B models, and KARD outperforms the fine-tuning only with a quarter of the full training data in 780M models.

Our findings and contributions are as follows:

* We demonstrate that fine-tuning small LMs to generate rationales from large LMs is insufficient for knowledge-intensive reasoning tasks and a non-parametric external knowledge base plays a crucial role in complementing the lack of knowledge in small LMs.
* Moreover, we address the limitations of the existing retriever method by introducing a reranker, in order to obtain pertinent passages for generating rationales in knowledge-intensive reasoning tasks.

Figure 1: **Concept. An example of a knowledge-intensive reasoning task (medical QA ) on the top. On the bottom, we provide the conceptual illustration of our KARD, compared to existing reasoning distillation. On the right, we provide examples of passages retrieved with rationale and question from the external KB.**

* In widely-used medical, multi-step factual, and commonsense QA benchmark datasets, we empirically show that the proposed KARD significantly improves the performance of small LMs.

## 2 Related Works

Large Language ModelsLarge Language Models (LLMs) have shown impressive capabilities across various tasks. One of their notable strengths is their ability to memorize knowledge and leverage that knowledge to solve knowledge-intensive reasoning tasks. For example, LLMs like GPT-3.5, Med-PaLM, ChatGPT, and GPT-4 have shown the promising performance on the challenging medical question answering task, the United States Medical Licensing Examination (USMLE) , even surpassing the passing score by a large margin . However, deploying LLMs in offline and privacy-sensitive environments is still challenging since most of these models are in black-box (accessible via APIs), and computationally expensive. Thus, we need alternative solutions that can leverage the capabilities of LLMs for knowledge-intensive reasoning tasks.

Reasoning Distillation from LLMsRecent works [33; 17; 38; 12; 18] have attempted to distill the reasoning ability of LLMs into small LMs, where the reasoning ability is an _emergent property_ which enables LLMs to perform better in reasoning tasks through Chain-of-Thought (CoT) prompting (e.g., _Let's think step-by-step_) [28; 55]. Unlike arithmetic or symbolic reasoning tasks, however, previous works [33; 17] have shown that reasoning distillation is less effective for knowledge-intensive reasoning tasks  where factual knowledge is important to generate accurate rationale. Therefore, we augment small LMs with documents retrieved from the external knowledge base so that the models can leverage knowledge to generate better rationales that lead to correct answers.

Knowledge-Augmented LMsKnowledge-augmented LMs have utilized an external Knowledge Base (KB) to supplement their intrinsic knowledge [16; 32; 3; 22; 58]. One common approach to incorporate external knowledge is by retrieving relevant passages from a KB, such as Wikipedia, based on the input query . Retrieving the correct evidence is crucial to generate accurate answers and factually grounded rationales. However, previous works usually have not explored the use of knowledge-augmented LMs for tasks that require complex reasoning over knowledge. Recently, BehnamGhader et al.  examined the reasoning ability of existing retrieval-augmented LMs and found that the existing retriever  is insufficient for retrieving relevant passages to solve the knowledge-intensive reasoning tasks. To address this limitation, we propose a re-ranker for rationale generation that prioritizes passages relevant to the rationale generated by LLMs given the query. This approach can be seen as a form of knowledge distillation for the retriever, as we use the rationale to guide the reranker to retrieve more relevant passages for reasoning, instead of using plain queries.

## 3 Motivation: Effect of Knowledge-Augmentation on Memorization

Large language models are known to memorize its training data [6; 49] and the memorization capacity is proven to increase as the size of the model increases [27; 57]. The previous work  showed that the memorization of training data is indeed necessary to perform well in a language problem. These results suggest that the reasoning distillation with a small language model (without knowledge augmentation) will degrade the performance because of (1) the incapability of memorizing training data and (2) the necessity of the memorization to perform well. In this section, we demonstrate that using an external Knowledge Base (KB) as a non-parametric memory with a retriever reduces the amount of the memorization needed to perform well and thus allows us to use small models.

### Background without Knowledge-Augmentation

We adopt the exact same problem setting used in Brown et al. . A task distribution \(P q\) is drawn from meta-distribution \(q\). Given a \(P\), the training dataset \(X=((Z_{i},Y_{i}))_{i=1}^{n}\) and the test sample \((Z,Y)\) are drawn as \(X P^{ n}\) and \((Z,Y) P\). Here, \(Z\) is the input (i.e., the sequence of symbols) and \(Y\) is the label (i.e., the next symbol to be predicted). The overall error of a learning algorithm \(\) on the meta-distribution \(q\) with sample size \(n\) is defined by

\[_{q,n}()=_{P q,X P^{  n},\\ (Z,Y) P}(M(Z) YM=(X)).\]

Given \(q\) and \(n\), there exists an optimal learner \(_{}\) that minimizes this overall error, which will be used as our reference. We adopt the abstracted language problem, i.e., the next-symbol prediction problem with \(N\) reference strings \(\{c_{j}\}_{j=1}^{N}\) where \(c_{j}(\{0,1\}^{d})\), considered in the main text of Brown et al.  with no symbol corruption (see  or **Appendix A.1** for the details). Under this setting, Brown et al.  proved that any algorithm \(\) needs to memorize the \(nd\) bits of training data to achieve \(\)-suboptimality where \(I\) denotes the mutual information:

**Theorem 1** (Brown et al. ).: _Let \(N=n\). Then, any learning algorithm \(\) that satisfies \(_{q,n}()_{q,n}(_{})+\) for \(=o(1)\) also satisfies \(I(X;(X)|P)=(nd)\)._

### Memorization with Knowledge-Augmentation

In Theorem 1, \(d\) corresponds to the size of KB. Thus, it shows that if the size of KB is small, then a small model can just memorize all KB by memorizing \((nd)\) information to perform well. However, if the size of KB is large, then a small model cannot memorize \((nd)\) information and hence the performance is expected to drop significantly when replacing a large model with a small model. In this subsection, we show that knowledge-augmentation reduces the memorization requirement of \((nd)\) bits to that of \(O(n_{2}(N+R))\) bits, allowing the use of small models.

We consider an inference algorithm \(\) that uses a KB with a non-parametric retriever as follows:

\[_{q,n}^{}()=_{P  q,X P^{ n}\\ (Z,Y) P}((Z,M,S) YM=(X)).\]

An inference algorithm \(\) has no learnable parameters and makes prediction based on both the result of learning algorithm \(M=(X)\) and a KB denoted by \(S\), which is defined as follows. Given a task instance \(P q\), we choose a KB such that \(|S|=N+R\) and \(\{c_{j}\}_{j=1}^{N} S\) where \(R\) is the number of extra references that are irreverent to this task \(P\); i.e., \(R=0\) in the best scenario.

Theorem 2 shows that the knowledge-augmentation reduces the amount of memorization to achieve \(\)-suboptimality, from the \(nd\) to \((N,n)m\) bits, under the same problem setting as Theorem 1:

**Theorem 2**.: _There exists a pair of inference and learning algorithms \((,)\) such that for any \(>0\), \(_{q,n}^{}()_{q,n}( _{})+\) and \(I(X;(X)|P)=O((N,n)m)\) where \(m=_{2}((1-()^{n})^{-(N+R)}{2}}) _{2}(}{2})\)._

With \(n=N\) and \(=o(1)\), we have \(I(X;(X)|P)=O((N,n)m)=O(n_{2}(N+R))\) (see **Appendix A.2** for proof). Thus, it shows that knowledge-augmentation allows the reduction from the \(nd\) bits to \(n_{2}(N+R)\) bits for the amount of memorization needed to perform well.

## 4 Knowledge-Augmented Reasoning Distillation

We propose Knowledge-Augmented Reasoning Distillation (**KARD**), which consists of two learning processes: (1) reasoning distillation where we leverage Large Language Models (LLMs) to generate a rationale with black-box APIs and then fine-tune small models to generate both rationale and answer given a question and knowledge, in which the knowledge is retrieved from Knowledge Base (KB) with the rationale as a query; (2) reranker training to retrieve relevant passages for the question as a query at the inference time, for generating effective rationales. Our approach is illustrated in Figure 2.

### Teach Small Models to Generate Rationales with External Knowledge

Rationale Generation with LLMsIn our problem setup, we assume that training dataset \(((_{i},_{i}))_{i=1}^{n}\) for the target task is given, where \(_{i}\) is input sequence (question in QA) and \(_{i}\) is label (answer in QA). Additionally, there are LLMs accessible through black-box APIs [5; 8; 44; 43; 42]. In other words, the parameters and the architecture of the LLM are unknown and we can only access text sequences generated by the LLM. Since the ability to generate high-quality rationale is known as the emergent ability of LLMs [55; 28], we want to transfer such ability to a small language model with reasoning distillation. Firstly, we leverage the main-of-thought prompt  to elicit the proper \(l\) rationales for each training data point with LLMs: \(_{ij}=(,_{i},_{i})\) for all \(i[n]:=\{1,,n\}\) and \(j[l]\), where \(\) is the generated rationale and \(\) is the chain-of-thought prompt [55; 28; 48].

Fine-tuning Small Models on RationalesThen we fine-tune a small language model \(p_{}\) with trainable parameters \(\) to generate both rationale \(_{ij}\) obtained from the LLM and answer \(_{i}\), given the question \(_{i}\). In other words, we minimize the negative log-likelihood of the sequence of rationale \(_{ij}\) and the answer \(y_{i}\) where the rationale must be generated first prior to the answer generation:

\[_{}()=-_{i=1}^{n}_{ j=1}^{l} p_{}(_{ij},_{i}|_{i}).\] (1)Intuitively, the rationale provides a deeper and more comprehensive understanding of the reasoning behind the answer associated with the question, which better guides the small model to correctly answer the question . Although previous works [33; 17; 38; 12; 18] have also leveraged the rationales generated by LLMs to make small models excel at diverse reasoning tasks, generating rationales for knowledge-intensive tasks with a small LM requires additional care. As previously described in Section 3, the reasoning distillation with a small model but without knowledge augmentation may degrade the quality of the rationale generation due to the incapability of memorizing training data with the small model [57; 27] and the necessity of the memorization for better performance in language tasks . Therefore, the rationale generation should be evidenced by extrinsic knowledge from external memory to enhance the capability of the small LM for generating a high-quality rationale.

Integrating External Knowledge BaseMotivated by Theorem 2, we propose to retrieve a passage from an external Knowledge Base (KB) which is a corpus of over millions of documents \(=\{_{1},,_{K}\}\) to support memorization capacity of the small LM. Note that the acquisition of the relevant document from KB is crucial for training the small LM to generate high-quality rationale which leads to correct answers for given questions. As done in open-domain QA task , we retrieve a set of relevant passages for a given query with the sparse retriever BM25 . In order to obtain the document which is the most relevant to the rationale \(_{ij}\) generated by the LLM, we utilize the rationale as a query to retrieve a set of passages \(}_{ij}=((|_{ij};),k) \), where \(\) denotes a retriever scoring the document \(\) based on relevance to the query \(_{ij}\) and \(\) yields the \(k\) passages with the top-\(k\) highest relevance scores. Finally, we utilize the retrieved documents \(}_{ij}\) for fine-tuning the small LM to generate the rationale \(_{ij}\) and answer \(_{i}\) for the question \(_{i}\) as follows:

\[_{}()=-_{i=1}^{n} _{j=1}^{l} p_{}(_{ij},_{i}|_{i},}_{ij}),\] (2)

where the rationale and answer are sequentially generated as we did in Equation 1.

### Training Neural Reranker for Rationale Generation

The remaining issue is that we cannot use the rationale as a query at the inference time. As an alternative, we can use the question \(_{i}\) instead of the rationale \(_{ij}\) as a query to retrieve a set of passages with the retriever. However, there is no guarantee that the top-\(k\) passages retrieved by the input \(_{i}\) as a query contain relevant information to generate correct rationales. In detail, based on the question as a query, the retriever can obtain a set of passages that contain relevant documents for generating rationales with a sufficiently large \(k\) but \(k K\). However, the target documents we want for rationale generation may be assigned with low rankings and thus they may not be chosen for knowledge augmentation at the inference time. To remedy this issue, we propose to leverage a neural reranker \(f_{}\) with parameter \(\) to re-rank the set of passages retrieved by the retriever \(\) so that we can acquire more relevant documents for generating rationale at the inference time.

In order to train the neural reranker, we might manually construct a ground truth passage for each question. However, we assume a realistic setting where the ground truth passage for reranker training

Figure 2: **Overview of KARD.** (Left, § 4.1) Illustration of training (top) and inference (bottom) of knowledge-augmented reasoning distillation, where, during training, the small LM learns to generate rationales given the training data and the retrieved knowledge by the rationale. (Right, § 4.2) Illustration of reranker training (top) and inference (bottom). Reranker learns to prioritize the passage which has knowledge relevant to the rationale.

is not given. Instead, we train the reranker to imitate how the retriever scores the passage \(\) with the rationale \(_{ij}\) as a query. Specifically, we first utilize the retriever \(\) to obtain a set of passages from \(\) with the rationale \(_{ij}\) as a query as follows: \(}_{ij}=((|_{ij};), _{1})((|_{i};),_{2})\) where \(_{1}\) and \(_{2}\) are the number of candidate documents (Figure 2 is the case where \(_{2}=0\)). Then, we normalize the score \((|_{ij};)\) of the document from \(}_{ij}\), denoted as \(Q(|_{ij})\). Similarly, we use the reranker \(f_{}\) to score each document in \(}_{ij}\) with the given question \(_{i}\) and normalize the score denoted as \(P_{}(|_{i})\). We use softmax for normalization with hyperparameters \(_{1},_{2}>0\) as follows:

\[Q(|_{ij})=|_{ij};)/_{ 1})}}{_{^{}}_{ij}}^{ }|_{ij};)/_{1})}},\;P_{}(|)=(,_{i})/_{2})}{_{^{}}_{ij}}(f_{}(^{},_{i})/_{2})},\]

where \(}_{ij}\). Finally, we minimize the KL divergence between \(Q(|_{ij})\) and \(P_{}(|_{i})\):

\[_{}()=_{i=1}^{n}_{j=1} ^{l}D_{}(Q(|_{ij})\|P_{}(|_{i})).\] (3)

Intuitively, the objective function guides the reranker to assign higher scores to passages that are similar to the rationale \(_{ij}\). Note that both objective \(_{}()\) and \(_{}()\) are independent; therefore, we do not need to jointly update both of the small LM and the reranker.

### Inference

After training, we obtain the small LM with the parameter \(^{*}*{argmin}_{}_{}()\) and the reranker with the parameter \(^{*}*{argmin}_{}_{}()\). At the test time, to answer the question \(_{*}\), we first get a set of candidate documents \(}_{*}=((|_{*};), ^{*})\) with the retriever \(\) and \(^{*}=100\). Then we re-rank all the document \(d}_{*}\) with \(f_{^{*}}\) and choose top-\(k\) relevant documents w.r.t the question \(_{*}\) as follows: \(}_{*}=(\{f_{^{*}}(,_{*})}_{*}\},k)\). Finally, we generate a rationale \(_{*}=*{argmax}_{}p_{^{*}}(|_{*}, _{*})\) and an answer \(_{*}=*{argmax}_{}p_{^{*}}(|_{*}, _{*},}_{*})\).

## 5 Experiments

Task and DatasetIn our experiments, we focus on knowledge-intensive reasoning tasks which require both the reasoning ability over the knowledge and the compound knowledge of the specific domain. As our primary benchmark, we use the medical multiple-choice question dataset -- **MedQA-USMLE**. The dataset contains 12,723 4-option multiple-choice question answering problems from US medical licensing exam. This dataset is the best fit to evaluate our method since 98% of the questions simulate the realistic clinical settings by presenting patient cases that require extensive professional domain-specific knowledge and complex reasoning ability over multiple evidence sources. To further validate our approach, we employ **StrategyQA** dataset, which involves 2,780 yes/no questions that demand sophisticated multi-step reasoning skills and the ability to gather supporting evidence from various domains. We additionally validate our approach on commonsense reasoning with **OpenbookQA** dataset, which consists of 5,957 elementary-level science questions with 4 multiple-choice options.

BaselinesWe compare our method against relevant baselines. **Few-shot In-context Learning** (ICL) utilizes a few training samples as a prompt to make a prediction . **Few-shot ICL + Chain-of-Thought** (CoT) leverages chain-of-thought prompting to generate a rationale and generate an answer based on the rationale . **Fine-tuning** refers to the one that fine-tunes a pre-trained model to generate an answer given only a question. The performance of the above baselines represents the capability of a small language model to solve knowledge-intensive reasoning tasks using only training data but without any extrinsic guidance on reasoning or external knowledge.

To assess the impact of external knowledge, we augment the above three baselines with documents retrieved from the knowledge base (Wikipedia), denoted as **Knowledge-Augmented** models. For the knowledge augmentation, we append retrieved passages along with the question at both training and inference time. These baselines help us understand how much external knowledge can improve the performance of each baseline. We also compare our KARD against the standard **Reasoning Distillation** without knowledge-augmentation [33; 17; 38; 12; 18].

As **oracle** models, we present a variant of KARD that receives better knowledge as input. In particular, at the inference time, we augment KARD with the silver document which is the passage retrieved with the gold rationale generated by the LLM as a query. This model represents an upper bound of the neural reranker performance. Additionally, we directly provide the small instruction fine-tuned language models (Flan-T5  and OPT-IML ) with the rationale from the LLM in inference, to assess the upper bound of the performance gain on small models with high-quality rationales.

Language ModelsFor all the experiments, we use the T5 models  including Flan-T5 , and OPT models  including OPT-IML . For the reranker, we use LinkBERT models . As for the teacher LLM, we employ GPT-3.5-turbo (ChatGPT)  through the proprietary API.

See **Appendix B** for experimental settings in detail.

### Experimental Results

Table 1 shows that KARD consistently outperforms all the baselines on the MedQA-USMLE dataset on both encoder-decoder (Flan-T5) and decoder-only (OPT) language models. Remarkably, KARD exhibits a substantial positive effect on smaller models, as evident from the significant performance gain of the Flan-T5 Base model, which has 250 million parameters, over a fine-tuning baseline on the MedQA-USMLE dataset. Regarding the analysis of the model size, please refer to Section 5.3. The impact of KARD decreases as the size of the model increases since larger models can better memorize knowledge during pre-training and fine-tuning. Moreover, we empirically show that knowledge augmentation consistently improves performance not only in reasoning distillation but also in few-shot chain-of-thought and fine-tuning. It is worth noting that this empirical evidence supports our theoretical analysis in Section 3 that knowledge augmentation enhances the performance of small models. Furthermore, our experimental results indicate that the reranker consistently improves the performance of models for all sizes, over the retrieval with BM25. From the experimental results with silver knowledge (oracle), there is room for improvement by retrieving more relevant documents, which can help the model generate a high-quality rationale.

We also present additional experimental results on StrategyQA and OpenbookQA datasets in Table 2. Once again, KARD outperforms all baselines in experiments with both datasets. Notably, compared to MedQA-USMLE, the few-shot methods on StrategyQA and OpenbookQA exhibit performance similar to random guessing, as T5 lacks the ability of in-context learning . Furthermore, fine-tuning

    &  &  \\ 
**Method** & Base (250M) & Large (780M) & XL (3B) & 350M & 1.3B-IML \\ 
**Few-shot** & 23.49 & 31.50 & 35.66 & 27.42 & 29.14 \\
**Few-shot + Chain-of-Thought (CoT)** & 25.22 & 32.21 & 32.99 & 25.06 & 26.39 \\ _Knowledge-Augmented Few-shot + CoT_ & 31.34 & 32.60 & 34.41 & 25.84 & 28.75 \\
**Fine-tuning** & 30.71 & 34.49 & 37.39 & 26.47 & 25.77 \\ _Knowledge-Augmented Fine-tuning_ & 33.39 & 37.71 & 39.12 & 25.84 & 28.67 \\ 
**Reasoning Distillation** & 31.05\({}_{-4.0}\) & 39.62\({}_{-2.6}\) & 46.32\({}_{-6.2}\) & 29.43\({}_{-1.1}\) & 34.30\({}_{-9.5}\) \\
**KARD (ours, BM25)** & 33.14\({}_{-2.3}\) & 41.87\({}_{-3.9}\) & 47.27\({}_{-6.7}\) & 30.79\({}_{-7.8}\) & 35.48\({}_{-8.37}\) \\
**KARD (ours, Rearker)** & **38.15\({}_{-3.9}\)** & **44.59\({}_{-4.7}\)** & **48.94\({}_{-3.22}\)** & **32.86\({}_{+1.12}\)** & **38.83\({}_{+4.66}\)** \\ 
**KARD (Silver Knowledge, Oracle)** & 40.30 & 49.80 & 53.50 & 35.90 & 42.18 \\
**CoT from ChatGPT (Teacher, Oracle)** & 61.59 & 65.51 & 67.16 & - & 50.27 \\   

Table 1: Experimental results on the **MedQA-USMLE** dataset with Flan-T5  and OPT  models. We report the mean and standard deviation of accuracy with 3 different runs for reasoning distillation methods.

    &  &  \\ 
**Method** & Base & Large & XL & Base & Large & XL \\ 
**Few-shot** & 48.47 & 48.47 & 51.67 & 23.00 & 27.60 & 25.00 \\
**Few-shot + CoT** & 48.47 & 48.31 & 48.76 & 27.60 & 27.40 & 27.80 \\
**Fine-tuning** & 52.26 & 56.33 & 51.53 & 54.00 & 62.00 & 74.60 \\ _KA Fine-tuning_ & 52.11 & 58.81 & 53.38 & 53.80 & 64.60 & 73.80 \\
**Reasoning Distillation** & 53.56 \({}_{-6.7}\) & 54.79\({}_{-5.5}\) & 42.84 & 58.87 & 66.63 & 13.77\({}_{-0.39}\) \\
**KARD (ours, BM25)** & 55.90\({}_{-4.6}\) & 65.94\({}_{-1.8}\) & 68.81\({}_{-5.8}\) & 58.93\({}_{-3.8}\) & 54.04\({}_{-1.6}\) & 77.00\({}_{-0.04}\) \\
**KARD (ours, Rearker)** & **56.57\({}_{-1.2}\)** & **66.04\({}_{-0.0}\)** & **70.55\({}_{-1.8}\)** & **59.33\({}_{-1.1}\)** & **64.00\({}_{-1.16}\)** & **78.53\({}_{-3.22}\)** \\ 
**KARD (Silver Kn., Oracle)** & 57.50 & 68.65 & 27.34 & 63.40 & 72.40 & 82.00 \\
**CoT from ChatGPT (\(\)racle)** & 66.38 & 67.10 & 72.05 & 58.60 & 78.80 & 87.80 \\   

Table 2: Experimental results on the **StrategyQA** and **OpenbookQA** dataset with T5 models . \(\) indicates experiments with Flan-T5 having the same size. We report sequential results as in Table 1.

T5-XL on StrategyQA results in poor performance since it fails to generalize to the test data. On the other hand, reasoning distillation improves the performance of models across all different sizes on both datasets. Our KARD further yields performance improvement over the reasoning distillation baseline, demonstrating the effectiveness of knowledge augmentation in both datasets.

### Analysis

Experiments with DAPTDomain-Adaptive Pre-Training (DAPT)  is the useful strategy to adapt Pre-trained Language Models (PLMs) on the specific domain to effectively tackle the tasks on it, which is done by further pre-training the PLM on a large-scale domain-specific text corpus . As it is interesting to observe whether the DAPT can enhance the capacity of PLMs for reasoning distillation in domain-specific knowledge-intensive tasks by further performing training on relevant domain-specific data before distillation, we conduct experiments with models from DAPT. Specifically, we further pre-train the Flan-T5 Base model on two moderate-scale biomedical corpora, Pubmed abstracts and MedWiki , respectively. Then, we apply reasoning distillation and KARD to PLMs with further pre-trained parameters. In Figure 3, we observe that DAPT on Pubmed marginally enhances the performance of reasoning distillation. On the other hand, KARD contributes more substantially to performance improvement than DAPT. This result indicates that KARD offers a distinct advantage in knowledge-intensive reasoning tasks compared to DAPT.

Efficiency on Dataset and Model SizesTo validate the efficiency of our KARD in terms of training data and model size, we measure the test accuracy on the MedQA-USMLE dataset while varying the number of training data and model parameters. As shown in Figure 3(a), our KARD can effectively transfer the reasoning ability of the LLM with the proposed KARD mechanism, using only a small number of training data. Moreover, the gaps between the naive fine-tuning and our KARD become much larger when increasing the number of training data, which confirms that we can potentially increase the effectiveness of KARD with more training data for knowledge-augmented distillation from LLMs. Furthermore, it is worth noting that KARD is a _sample-efficient_. With \(25\%\) of the training data, KARD outperforms the same model fine-tuned on the full data.

For the efficiency in terms of the model size, as shown in Figure 3(b), KARD with 250M parameters achieves higher accuracy than the fine-tuned model with 3B parameters (14 times larger). Moreover, KARD with 780M parameters outperforms the 11B in-context learning baseline. These results show the significant practical advantage of our KARD in resource-restricted settings since the small LM with KARD requires significantly less computational cost yet it outperforms the LLM.

Retrieval PerformanceTo evaluate the performance of the reranker on MedQA-USMLE, we consider the top-3 silver documents retrieved with the rationales generated by LLM as the ground

Figure 4: **(a) Efficiency on training data and (b) model size.** On MedQA-USMLE, we compare KARD against the fine-tuning baseline by varying either the number of training data with Flan-T5 Large or the number of parameters, including the few-shot in-context learning performance of Flan-T5 XXL (11B). **(c)** Considering silver documents as ground truth, we measure **Hits@k** on the documents retrieved by BM25 and the reranker.

    &  \\ Rationales & Base & Large \\  \(l=3\) & 30.09 & 35.43 \\ \(l=5\) & 32.13 & 39.04 \\ \(l=10\) & 32.91 & 41.79 \\    
    &  \\ Passages & Base & Rare \\  \(k=1\) & 32.91 & 36.76 \\ \(k=2\) & 32.84 & 37.71 \\ \(k=3\) & 32.36 & 37.39 \\   

Table 3: Analysis on rationale diversity.

truth, and measure Hits@k on the documents retrieved by BM25 and reranker with \(^{*}=100\). In Figure 4c, the reranker achieves significantly better Hits@k than BM25. This result indicates that the reranker successfully learns to prioritize passages that are helpful to generate correct rationale at the test time, which leads to performance improvement on the knowledge-intensive reasoning tasks.

The Number of Rationales During TrainingFollowing Ho et al. (2019), we generate multiple rationales for each training sample in order to facilitate diverse reasoning in small language model training. In Table 3, we present the impact of rationale diversity during training on both Flan-T5 base and large models using the MedQA-USMLE dataset. As the number of rationales per training data increases, the performance also improves, demonstrating the benefit of employing multiple rationales. However, the performance gains become small when we increase the number of rationals from 5 to 10. This suggests that utilizing more diverse rationales beyond 10 may not yield significant further improvements, at least in the MedQA-USMLE dataset.

The Number of Candidate Documents for RerankerIt is crucial to determine the size of the candidate document set (\(^{*}\)) to which the reranker assigns the relevance scores w.r.t a question. In Table 4, we present the performance of both Flan-T5 base and large models on MedQA-USMLE, while varying \(^{*}\). The results indicate that increasing the number of candidate documents tends to be beneficial, as it allows the reranker to consider a broader range of diverse candidate documents.

The Number of Passages Used for InferenceEven LLMs tend to be easily distracted by irrelevant context (Kang et al., 2019). Therefore, simply adding more passages during inference may not necessarily enhance performance if relevant knowledge is not selected. In Table 5, we present the impact of the number of passages used in KARD during inference (\(k\) in Section 4.3) on Flan-T5 Base and MedQA-USMLE. We observe that the performance of KARD (BM25) without the re-ranker decreases with increasing \(k\). This result implies that using additional passages does not always result in generating better rationales. In contrast, using two passages (\(k=2\)) with the reranker is better than a single passage (\(k=1\)). This result indicates that the reranker effectively selects more suitable knowledge than BM25, thereby contributing to performance improvement in the MedQA-USMLE benchmark.

Qualitative AnalysisIn Table 6, we provide an example comparing the rationale generated by our KARD against the rationale by the baseline model with reasoning distillation but without knowledge augmentation. We choose one sample from the MedQA-USMLE dataset and generate the rationale using the Flan-T5 XL model. The model without knowledge augmentation generates the rationale that seems plausible based on the given instruction and question. However, it mistakenly generates the hallucination that Glucocorticoids are the first-line treatment for Graves' disease, which is incorrect. As a result, it fails to predict the correct answer (B) Methimazole. In contrast, when the model is fine-tuned with KARD, it generates a correct rationale that is supported by the retrieved knowledge indicating that methimazole is the best single treatment option for Graves' disease. Consequently, it successfully predicts the correct answer. This example highlights the effectiveness of our KARD method for generating accurate rationales by incorporating relevant knowledge, which leads to an improved question answering performance on knowledge-intensive reasoning benchmarks.

 p{113.8pt} p{113.8pt}}  
**Question**: A 29-year-old woman presents to the clinic after several months of weight loss. She noticed a 6.8 kg (15 lb) unintentional weight loss over the preceding several months. She has not changed her diet or exercise habits. She also reports reuding with her boyfriend over the temperature of their shared apartment, as she always reaffirmed him does. The vital signs include: heart rate 110min and blood pressure 146/78 mm Hg. The physical exam is notable for warm and slightly most skin. Which of the following is the best single treatment option for this patient? (A) Glucocorticoids (B) **Methimazole** (C) Propranolol (D) Radiodieme therapy & **Outputs** \\ 
**Reasoning Distillation (without knowledge)** & _No knowledge_ & The patient’s symptoms are consistent with hyperthyroidism, specifically Graves’ disease. Glucocorticoids are the first-line treatment for Graves’ disease. Glucocorticoids are the first-line treatment for Graves’ disease. Glucocorticoids are the first-line treatment for Graves’ disease. Consequently, it is difficult to predict the correct answer. This example highlights the effectiveness of our KARD method for generating accurate rationales by incorporating relevant knowledge, which leads to an improved question answering performance on knowledge-intensive reasoning benchmarks.

Table 6: **Rationale generation example of KARD, compared with Reasoning Distillation in MedQA-USMLE with Flan-T5 XL. The answer is in blue. We mark the hallucination in the generated rationale as yellow and highlight the factually correct statements in the generated rationale and its supporting fact in the passage as red.**

## 6 Discussion

### Comparison to Retrieval-augmented Generation

Retrieval-Augmented Generation (RAG)  primarily focuses on solving knowledge-intensive tasks (e.g., open-domain QA), where the accurate knowledge retrieval is important to achieve higher performance. In terms of the methodology, the key differences between KARD and RAG are that RAG utilizes the question as a query and jointly fine-tunes the generator and retriever. To quantitatively analyze the advantage of our KARD against RAG in reasoning distillation, we conduct experiments with RAG on the reasoning distillation with two datasets that we used in the main experiment, where we use Flan-T5 base for MedQA-USMLE and T5 base for StrategyQA as base LMs and DPR  as the trainable retriever for RAG. In Table 7, experimental results show that using RAG in reasoning distillation achieves lower accuracy than KARD, showing that our KARD is more tailored approach to reasoning distillations.

### Failure Case Analysis

In Table 1, we can see significant differences between KARD with reranker on the Flan-T5 XL and the ChatGPT in MedQA-USMLE. Our investigation focuses on understanding the cause of these gaps by examining samples where our method fails while ChatGPT succeeds. We collect 30 samples from corresponding cases and categorize them into two groups. The first group consists of cases where the reranker fails to obtain the document relevant to generating the correct rationale. The second group includes cases where the small language model fails to produce correct rationales and makes incorrect predictions, despite having access to relevant knowledge in the retrieved document. Out of 30 samples, 15 fall into the first category, while the remaining 15 belong to the second category. This observation indicates the need for further improvements in both retriever and distillation methods to enhance the performance of small language models in knowledge-intensive reasoning tasks.

### Limitations

We have shown substantial improvements in small LMs' performance on knowledge-intensive reasoning tasks through our KARD. However, it is important to acknowledge the limitations of our study. First, in terms of methodology, the effectiveness of our knowledge augmentation heavily relies on the quality of the document retrieved from the external knowledge base. As indicated in Table 1 and Figure 3(c), our reranker substantially improves the performance of small models by retrieving better knowledge. Despite the diminishing performance gap between BM25 and the reranker as the model size increases, there is still a significant difference between the document retrieved by the reranker and the silver knowledge. This indicates that the reranker might miss important passages that can augment the knowledge of even large LMs. Therefore, further advancements in retrieval methods are necessary to generate better rationale, as this remains an important research challenge even for large language models . Second, regarding experiments, we have tested our approach on relatively small LMs having under 3B parameters given our limited computational budgets. However, exploring the use of relatively larger language models like GPT-3 [5; 44] or LLaMA [50; 51] with KARD could be of great interests, which is a promising direction for future research.

## 7 Conclusion

In this work, we proposed Knowledge-Augmented Reasoning Distillation (KARD) which enhances the capabilities of small Language Models (LMs) on knowledge-intensive reasoning tasks that demand both knowledge and reasoning abilities. Our approach involves generating rationales from large LMs and fine-tuning small LMs on these rationales, while augmenting small LMs with external knowledge from a non-parametric memory. Our theoretical analysis motivates our method by demonstrating the effectiveness of external memory in reducing the memorization requirements of small LMs. Through empirical experiments, we showed that KARD outperforms traditional approaches such as fine-tuning and reasoning distillation, thereby providing a pathway to improve small LMs in reasoning tasks that require a comprehensive understanding of domain-specific knowledge.

    &  \\  _KA_ Fine-tuning & 33.39 & 52.11 \\ RAG + RD & 24.84 & 54.24 \\ KARD (_Rearker_) & **38.15** & **56.57** \\   

Table 7: Experimental results including RAG on Reasoning Distillation (RD) with (Flan-)T5 base.