# Sample-efficient Antibody Design through Protein Language Model for Risk-aware Batch Bayesian Optimization

Sample-efficient Antibody Design through Protein Language Model for Risk-aware Batch Bayesian Optimization

 Yanzheng Wang

University of Bristol

&Boyue Wang

University of Wisconsin - Madison

&Tianyu Shi

University of Toronto

&Jie Fu

Hong Kong University of Science and Technology

&Yi Zhou

BioMap (Beijing) Intelligence Limited

&Zhizhuo Zhang

BioMap (Beijing) Intelligence Limited

Primary Corresponding Author, email: ty.shi@mail.utoronto.ca

###### Abstract

Antibody design is a time-consuming and expensive process that often requires extensive experimentation to identify the best candidates. To address this challenge, we propose an efficient and risk-aware antibody design framework that leverages protein language models (PLMs) and batch Bayesian optimization (BO). Our framework utilizes the generative power of protein language models to predict candidate sequences with higher naturalness and a Bayesian optimization algorithm to iteratively explore the sequence space and identify the most promising candidates. To further improve the efficiency of the search process, we introduce a risk-aware approach that balances exploration and exploitation by incorporating uncertainty estimates into the acquisition function of the Bayesian optimization algorithm. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing that our framework outperforms state-of-the-art methods in terms of both efficiency and quality of the designed sequences. Our framework has the potential to accelerate the discovery of new antibodies and reduce the cost and time required for antibody design.

## 1 Introduction

Antibodies, also known as immunoglobulins, are proteins produced by the immune system to recognize and neutralize foreign substances. They play a critical role in the body's defence against infections and diseases . The variable regions of an antibody are responsible for antigen recognition, are highly diverse, and consist of three complementarity-determining regions (CDRs) named CDR1, CDR2, and CDR3. Among these CDRs, CDR3 exhibits the greatest variability and is often referred to as the "hypervariable" region . Efficient antibody design is becoming more and more important because it has the potential to accelerate the development of effective treatments and vaccines .

Throughout the antibody design process, we strive to harness the full potential of antibodies by tailoring their properties to meet specific requirements. By optimizing their affinity, stability, and other attributes, these designed antibodies offer promising prospects for targeted therapy, diagnostics, and various biomedical applications [18; 2].

Typically, Experimental antibody design and screening can be time-consuming and expensive. Simulation allows researchers to test a large number of potential antibody structure candidates and select the most promising candidates for further experimental validation, saving time and resources. Improving the process of simulations  can further provide insight into the properties and behaviour of antibodies, such as binding affinity and specificity, which may be difficult to determine experimentally [16; 8]. However, the sheer number of possible CDRH3 sequences in a combinatorial space makes it infeasible to exhaustively examine any antibody simulation framework . Therefore, we need computational tools to guide our exploration of the protein landscape

Recently, Bayesian optimization has demonstrated its efficiency in exploring the sequence design space [16; 3]. Bellamy et al  compared how noise affects different batched Bayesian optimization techniques and introduced a retest policy to mitigate the effect of noise. Wang et al  discussed using Bayesian optimization (BO) to design chemical-based products and functional materials, showing that BO can significantly reduce the number of experiments required compared to traditional approaches. However, for antibody sequence design where the search space dimension is extremely large, it is very ineffective for Bayesian optimization. The choice of the acquisition function used to guide the optimization process can also impact its effectiveness, and there may be a trade-off between exploration and exploitation that must be carefully balanced.

We propose GLMAb-BO, an efficient way for antibody sequence optimization to address the above challenges. Our main contributions are improving exploration efficiency by using protein language models to filter out mutants with low fitness scores and designing a risk-aware acquisition function based on the uncertainty of the prediction to improve the explorer's ability. We demonstrate the effectiveness of our proposed method on multiple antibody datasets. Our model can identify the sequence with the best fitness score in the fewest rounds compared to other baselines.

## 2 Related work

Specially, we can use fitness scores to evaluate the bio function of the sequence, which play a crucial role in antibody design as they serve as important indicators of the functional and structural quality of antibodies. Higher fitness scores generally indicate better binding affinity, stability, and other desirable properties. Many novel frameworks have been proposed to model various protein sequences. Especially for pre-trained language models which demonstrate transfer learning ability to predict fitness scores [29; 21]. In the context of antibody design, predicting fitness scores can be highly beneficial. It provides a cost-effective alternative to conducting time-consuming and expensive wet-lab experiments. By utilizing computational models and machine learning techniques, researchers can efficiently evaluate the fitness of a large number of antibody sequences, prioritizing those with higher predicted fitness scores for further experimental validation. The need for better exploration algorithms, such as batch Bayesian optimization (BO), has gained attention in addressing the challenges of sequence design. Belanger et al  explored the application of batched Bayesian optimization in the context of biological sequence design, addressing the unique challenges and investigating design choices for robust and scalable design. Furthermore, Gonzalez et al  proposed a heuristic method based on an estimate of the function's Lipschitz constant to capture the interaction between evaluations in a batch. A penalized acquisition function is used to collect batches of points, minimizing non-parallelizable computational effort. Khan et al  used a CDRH3 trust region to restrict the search to sequences with favourable developability scores.

These studies highlight the ongoing efforts to address the challenges in sequence design for antibody engineering. By incorporating bayesian optimization, researchers aim to enhance the efficiency and effectiveness of antibody design and improve the sequence diversity.

## 3 Problem Formulation and Background

### Antibody Sequence Design

Antibody Sequence Design can be formulated as a constrained optimization problem [1; 31; 16; 22]. Let \(x\) be a vector representing the CDRH3 amino acid sequence, and let \(f(x)\) be a fitness function that quantifies the quality of the antibody sequence in terms of target specificity and developability. The problem is to find the optimal sequence \(x^{*}\) that maximizes the scoring function subject to constraints:

\[_{x}\,f(x)\,x,\,g(x) 0,\] (1)

where \(\) is the set of all possible amino acid sequences for the CDRH3 region and \(g(x)\) represents constraints on the biophysical properties of the sequence, such as stability and solubility. The optimization problem aims to find the best antibody sequence that satisfies the biophysical constraints and has the highest target specificity and developability scores. Bayesian optimization methods can be used to efficiently solve this optimization problem by iteratively proposing candidate sequences that are subsequently evaluated by a surrogate model and passed to an acquisition function that balances exploration and exploitation.

### Bayesian optimization

Bayesian Optimization (BO) is a sequential model-based optimization technique used to solve expensive black-box optimization problems with a limited budget of function evaluations, which has been applied to sequence modelling [16; 30].

We can express the BO process as follows: Let \(f(x)\) be the unknown fitness function we aim to optimize, where \(x\) is the input variable. Our goal is to find the global optimum \(x^{*}\) that maximizes \(f(x)\). However, doing a wet lab experiment to evaluate \(f(x)\) is expensive and time-consuming. The acquisition function, denoted by \((x)\), measures the utility of evaluating a point \(x\) based on the current surrogate model. \((x)\) balances exploration and exploitation by favouring points with high uncertainty (exploration) or high expected improvement (exploitation). Popular acquisition functions include expected improvement (EI), upper confidence bound (UCB), and probability of improvement (PI) [34; 16].

The next evaluation point is selected by optimizing the acquisition function over the input space \(\):

\[x_{n+1}=_{x X}(x)\] (2)

After evaluating \(f(x_{n+1})\), we update the surrogate model with the new observation \((x_{n+1},y_{n+1})\) and repeat the process until the budget of function evaluations is exhausted or a satisfactory solution is found. Batch BO improves this by minimizing the exploration rounds.

## 4 Method

### General language model guided candidate pool generation

Intuitively, we propose to use the General language model (GLM) trained on diverse antibody datasets to score the candidate pool and filter out the sequence with lower fitness values in the vast sequence space. Let \(\) be the candidate pool consisting of \(N\) protein sequences, and let \(f(x_{i})\) be the fitness score of sequence \(x_{i}\) from candidate pool \(\) obtained from the protein language model. We determine the threshold fitness score \(t\) that filters out \(m\%\) of the sequences with fitness scores less than or equal to \(t\). In the process of training our protein model GLM-Ab, we randomly mask one or two of the CDR regions by replacing the entire region with a random mask. We also conduct random mask fragments, by randomly masking one or more sections of the sequence.

Then, we can use GLM-Ab to score the sequences and determine an index \(k\) such that \(f(x_{k}) t<f(x_{k-1})\). Furthermore, by setting \(t=f(x_{k})\), the filtered set of sequences \(^{}\) with small search space and higher naturalness is obtained as:

\[^{}=x_{i} f(x_{i}) t\] (3)

In other words, \(^{}\) contains all sequences in \(\) with fitness scores greater than or equal to \(t\) based on GLM scoring.

### Risk aware Bayesian optimization

Many previous works have been proposed to leverage uncertainty for biological discovery and sequence design [12; 34; 16]. However, using traditional Gaussian processes  to measure the uncertainty for a large sequence is extremely inefficient. Due to the complexity of the antibody design space, we propose a risk-aware exploration to balance exploration and exploitation by selecting points with high expected improvement and lower risk. In each round of optimization, we train an ensemble of models to estimate the uncertainty, similar to the approach taken by PEX .

We assume the output of \(M\) surrogate models follows a normal distribution \((_{s},_{s})\). We can divide the uncertainty of those model predictions as epistemic uncertainty (EU), \(_{e}^{2}\); and aleatoric uncertainty (AU), \(_{a}^{2}\)[25; 28]:

\[_{e}^{2}=(-_{s})^{2},_{a}^{2}=_{s}_{s}^{2}(x),\] (4)

where EU is based on the variance between the predictions of different surrogate models, and the AU-estimated standard deviation provides a measure of the uncertainty associated with the predicted values. EU quantifies the uncertainty associated with the lack of knowledge or variability in the models themselves. EU can be reduced by increasing the number or quality of models.

Unlike PEX, we use a UCB acquisition function to evaluate sequence \(x\). The UCB acquisition function is defined as:

\[(x)=(x)+(x),\] (5)

where \((x)\) is the mean ensemble prediction generates from surrogate models for a sequence \(x\), and \(\) is a hyperparameter ( as 0.5 in our experiment) that controls the trade-off between exploration and exploitation, and \((x)\) is the ensemble standard deviation function of the surrogate model for sequence \(x\). In other words, \((x)\) represents the aleatoric uncertainty of the prediction for sequence \(x\).

The risk-aware modification based on Equation 5 introduces a penalty term that depends on the aleatoric uncertainty of the fitness values in the candidate pool:

\[_{risk}(x)=(x)+(x)\] (6)

where \(risk\) is the parameter that measures the variability, i.e., epistemic uncertainty, of the fitness values prediction based on the surrogate model for the whole candidate pool. we select \(m=0.5\) is a constant to avoid dividing by a very small value. The general purpose of this acquisition function is to discourage the selection of points with high variability, which can lead to unstable and unpredictable performance. To be more specific, the measure for risk is defined as:

\[risk=_{i=1}^{|C|}_{i}\] (7)

It is calculated as the average of aleatoric uncertainty for the fitness values evaluation in the whole generated candidate pool, where \(C^{}\) is the filtered candidate pool, and \(_{i}\) is the standard deviation of the fitness values prediction for the \(i^{th}\) candidate sequence.

In each round, we train the surrogate model \(f_{}\) on the queried sequences with true fitness scores from wet lab experiments (same as ). In the first few rounds, the surrogate model lacks good prediction ability for the candidate pool and could have a higher epistemic uncertainty . The rationale for the risk measure is to consider epistemic uncertainty for the whole candidate pool, which indicates a high risk of selecting a suboptimal point that may lead to a performance drop.

### GLMab-Bo

The full algorithm of our proposed algorithm can be found in Algorithm 1. In each round of black-box optimization, the whole framework is required to generate a query batch based on the measuredfitness score through wet lab experiments. We first utilize the pre trained unsupervised GLM-Ab model to narrow down the candidate pool sequence space. Then, we integrate risk-aware batch Bayesian optimization to propose a query batch for web lab experiments. The visualization of the whole framework is in Figure 1.

## 5 Experiments

Absolut! framework  is used as a computational alternative to wet lab experiments for generating antibody-antigen binding datasets. It provides a deterministic simulation of binding affinity using coarse-grained lattice representations of proteins, allowing evaluation of all possible binding conformations between a CDRH3 sequence and an antigen. The framework has been benchmarked and shown to produce consistent results compared to experimental data [16; 14]. And we use this framework to generate the initial whole candidate pool.

Figure 1: Framework overview. In our proposed GLMAb-BO framework, we first use the pre-trained GLM-Ab model \(\) to filter out the sequence with unsatisfying naturalness in the candidate pool and acquire \(^{}\), then we train an ensemble of surrogate models with GLM-Abâ€™s feature encoding to predict the fitness the remaining sequences. When we acquire the ensemble mean \(\), prediction standard deviation \(\), and the \(risk\), we utilize the proposed risk-aware Bayesian Optimization (BO) acquisition function to further evaluate the sequences. Finally, we use the top 100 sequences with high predicted naturalness to conduct a wet-lab experiment (we use a hypothetical scenario due to time constraints for replacement in this study) and perform another round of exploration until we reach the exploration rounds limits.

### Baseline methods

In this study, several methods for antibody design optimization are compared. The **Combinatorial Bayesian Optimization for Antibody Design (antbo)** approach employs combinatorial Bayesian optimization to efficiently design antibody CDRH3 regions, using a trust region and a black-box oracle for scoring specificity and affinity. **Proximal Exploration (pex)** introduces the Proximal Exploration algorithm and the Mutation Factorization Network architecture, which prioritize high-fitness mutants with low mutation counts for protein sequence design. The **Batch Bayes Optimization (batchbo)** method uses a neural network ensemble with uncertainty estimates to guide sequence batch selection using expected improvement. **Random Search** is employed as a baseline for method comparison, randomly selecting subsets of sequences for reference. These diverse methods provide insights into the optimization landscape and guide the development of more advanced algorithms for protein sequence design.

### Ablative study methods

In the ablative study, we assess the effectiveness of our proposed enhancements in the GLMAb-BO method through various ablations. These include **GLMAb-score**, which focuses solely on the highest predicted score from GLMAb on the candidate pool, and **GLMAb-select**, which removes the acquisition function and relies solely on the surrogate model for sequence selection. Additionally, **GLMAb-random** eliminates both the acquisition function and surrogate model, utilizing the GLM model to filter sequences and then randomly selecting the top 100. **GLMAb(w/o emb)-BO** removes the embedding of GLMAB's CNN surrogate model to evaluate the feature embedding module.

Figure 2: Experimental results comparison on antibody datasets, each round of black-box optimization can generate 100 proposal sequences. We use maximum measured fitness in each round as the evaluation metric. The shaded area indicates the standard deviation given 5 random seeds.

 
**Method** & **JIML_A** & **IADO_A** & **IPSN A** & **IPKQ J** & **IRDL C** & **TTOB A** & **IC3S** & **CR9S** & **A & **DRS4** & **A & **DUZL R** & **NYQ A** & **2WWg A** & **overall** \\  GXMA/Low-end (10) & 100.78 & 102.76 & 123.37 & 106.96 & 94.61 & 120.84 & 112.56 & 103.50 & 104.69 & 111.78 & 98.77 & 105.34 & 103.52 \\ GXMA-random (10) & 95.78 & 102.04 & 123.56 & 106.57 & 93.92 & 117.38 & 110.10 & 100.85 & 101.15 & 113.15 & 95.64 & 101.14 & 105.11 \\ GXMA-based (10) & 100.18 & 106.18 & 105.48 & 128.97 & 109.64 & 94.48 & 120.83 & 110.94 & 103.50 & 110.88 & 117.38 & 98.77 & 105.34 & 108.10 \\ GXMA-select (10) & 100.12 & 105.76 & 128.57 & 100.47 & 95.54 & 120.84 & 110.05 & 102.99 & 102.06 & 116.48 & 98.11 & 104.21 & 107.83 \\ GXMA-BD (10) & 100.18 & 105.76 & 129.78 & 110.70 & 95.95 & 120.84 & 112.89 & 103.50 & 104.69 & 117.38 & 98.77 & 105.34 & **108.68** \\ GXMA-BD (10) & 100.18 & 105.76 & 129.78 & 110.70 & 95.95 & 120.84 & 112.89 & 103.50 & 104.69 & 117.38 & 98.77 & 105.34 & **108.68** \\ GXMA-random (5) & 93.09 & 97.70 & 110.98 & 204.82 & 98.41 & 111.99 & 103.06 & 97.80 & 97.91 & 101.96 & 95.38 & 101.13 & 101.82 \\ GXMA-BD (5) & 95.60 & 100.51 & 123.69 & 93.55 & 92.23 & 115.20 & 104.25 & 97.80 & 97.89 & 104.04 & 94.56 & 95.88 & 102.00 \\ GXMA-BD (5) & 97.45 & 100.89 & 122.53 & 105.18 & 98.34 & 112.99 & 103.71 & 98.04 & 100.76 & 110.72 & 98.77 & 103.73 & 104.32 \\ GXMA-BD (5) & 97.34 & 103.84 & 122.07 & 106.93 & 92.11 & 114.19 & 108.52 & 99.30 & 100.76 & 111.77 & 98.77 & 103.73 & **104.79** \\ GXMA-BD (0) & 96.59 & 102.12 & 123.06 & 106.54 & 92.23 & 116.72 & 107.78 & 99.73 & 99.30 & 111.74 & 94.46 & 101.62 & 104.23 \\ GXMA-random (mg) & 92.49 & 97.39 & 118.25 & 103.44 & 89.57 & 111.92 & 105.60 & 97.08 & 96.15 & 109.21 & 93.13 & 98.94 & 109.92 \\ GXMA-select (10) & 95.15 & 101.67 & 122.85 & 104.15 & 92.12 & 115.61 & 104.36 & 98.55 & 97.05 & 111.15 & 95.21 & 101.27 & 103.21 \\ GXMA-select (10) & 96.05 & 101.63 & 122.45 & 103.68 & 92.01 & 115.65 & 106.40 & 98.29 & 98.53 & 110.97 & 94.58 & 101.05 & 103.44 \\ GXMA-BD (10) & 96.83 & 102.25 & 123.63 & 106.40 & 92.19 & 115.56 & 108.50 & 99.30 & 100.60 & 112.38 & 95.63 & 102.37 & **104.66** \\  

Table 1: Comparison of sequence optimization results on different datasets, we summarized maximum trimers over 5 rounds, 10 rounds, and average maximum fitness over 10 rounds.

 
**Method** & **JIML_A** & **IADO_A** & **IPSN A** & **IPKQ J** & **IRDL C** & **TTOB A** & **IC3S** & **CR9S** & **A & **DUZL R** & **NYQ A** & **2WWg A** & **overall** \\  GXMA/Low-end (10) & 100.78 & 102.76 & 123.72 & 106.96 & 94.61 & 119.84 & 112.56 & 103.50 & 104.69 & 117.38 & 98.77 & 10Moreover, the **Antibetry-BO** model replaces the GLM module with a different antibody-specific transformer language model to gauge its impact on active learning efficiency.

### Result analysis

#### 5.3.1 Analysis of GLMAb-BO performance

The comparison results of different methods are presented in Figure 2 and Table 1, highlighting notable findings. Firstly, batch-mode optimization methods (such as PEX and BatchBO) outperform non-batch-mode methods (like AntBO) in terms of discovering sequences with higher fitness scores. This advantage stems from the inherent diversity introduced by considering multiple sequences simultaneously in batch mode optimization. In contrast, non-batch mode methods are more susceptible to being trapped in local optima due to their limited diversity. Additionally, the utilization of GLMAb to filter the extensive sequence optimization space facilitates the exploration process, enabling the identification of optimal sequences within a few rounds. Moreover, leveraging feature embedding pretrained from the GLMAb model enhances the performance of the surrogate model in predicting fitness scores for unknown sequences, even with limited training data.

#### 5.3.2 Analysis of submodule performance

For the second question, the comparison results with different ablative methods are shown in Figure 2 and detailed in Table 2. We find GLMAb-BO to perform better than Antibetry-BO in the first few rounds, which indicates our pretrained GLMAb model's ability to filter out more sequences with unsatisfying naturalness. Meanwhile, we can find that with the help of the embedding feature from GLMAb, the performance of GLMAb-BO is better than GLMAb(w/o emb)-BO on most datasets.

By comparing GLMAb-BO with GLMAb-select and GLMAb-random, we can find that they have similar performance in the first few rounds thanks to the pre-trained GLM. However, given more rounds, GLMAb-BO can find the sequence with the overall best fitness score which indicates that our whole exploration framework can be helpful for exploring sequences with better naturalness. By comparing only GLMAb-select and GLMAb-random, we can find that with the help of the trained surrogate model, it can also greedily improve the searched sequence naturalness since it could have overall better fitness in the last few rounds.

## 6 Conclusion

In conclusion, we have presented an efficient and risk-aware antibody design framework that combines the power of protein language models and batch Bayesian optimization. Our approach addresses the challenges of time-consuming and expensive experimentation by leveraging predictive models to generate candidate sequences with higher naturalness and employing Bayesian optimization to explore the sequence space effectively. By incorporating uncertainty estimates into the acquisition function, our framework achieves a balance between exploration and exploitation, resulting in the identification of promising antibody candidates. Through extensive experiments on benchmark datasets, we have demonstrated the effectiveness of our method. Our framework surpasses state-of-the-art approaches in terms of both efficiency and the quality of designed sequences. By reducing the cost and time required for antibody design, our framework has the potential to expedite the discovery of new antibodies and contribute to advancements in the field.