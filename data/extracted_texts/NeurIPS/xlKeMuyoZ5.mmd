# Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM)

Jakob Hauser\({}^{1,}\), Daniel Kondor\({}^{1}\), Jenny Reddish\({}^{1}\), Majid Benam\({}^{1}\), Enrico Cioni\({}^{2}\), Federica Villa\({}^{2}\), James S. Bennett\({}^{3}\), Daniel Hoyer\({}^{4}\), Pieter Francois\({}^{2,5}\), Peter Turchin\({}^{1}\), R. Maria del Rio-Chanona\({}^{1,6,}\)

\({}^{1}\) Complexity Science Hub, Vienna, Austria \({}^{2}\) University of Oxford, Oxford, UK

\({}^{3}\) University of Washington, Seattle, WA, USA \({}^{4}\) George Brown College, Toronto, Canada

\({}^{5}\) The Alan Turing Institute, London, UK

\({}^{6}\) Department of Computer Science, University College London, London, UK

\({}^{}\)hauser@csh.ac.at; m.delriochanona@ucl.ac.uk

###### Abstract

Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.

## 1 Introduction

Large Language Models (LLMs) have the potential to simulate human behavior , label data, perform legal tasks , extract information on historical events , perform thematic analysis , advance psychological measurement , among other applications ; thus many authors expect LLMs to revolutionize social science and humanities research . However, to fully reap the benefits of LLMs we must also overcome biases and unbalanced distribution of knowledge that may originate from inherent biases in training datasets . In this work, we introduce a unique data set of graduate and expert academic level global history knowledge, Seshat: Global History Databank . We make this dataset publicly available and use it to benchmark historical knowledge of large language models. We name this benchmark History Seshat Test for LLMS (HiST-LLM)1.

Our knowledge of history is inherently unbalanced and partial, complicating the fair measurement of LLMs' historical knowledge. We know more about the history of the Global North than the GlobalSouth, and recent periods are better documented than ancient ones. This imbalance is likely reflected in LLMs; for instance, GPT-3.5 and GPT-4 perform excellently on US history at a high school level but only reasonably well on global history 2. There are also several gaps in our knowledge about the past. Consequently, experts and historians make inferences based on context and other known information. This in turn can lead to disagreements among even professional historians; for instance, estimates of the peak population of the Roman Empire range from 50 to over 150 million . At a graduate and expert level, we may expect the knowledge gap between Global North and Global South countries to increase given that the former has been researched more widely than the latter [15; 16]. This imbalance makes it difficult to assess how well LLMs represent historical knowledge. How can we measure their balance of knowledge when our own knowledge is unbalanced?

To perform a fair assessment of LLMs' historical knowledge, we need a systematic compilation of existing historical knowledge across regions. However, compiling such data poses several challenges. First, given the wide range of theoretical questions and approaches in history and archaeology, deciding which variables to record and whether they are best conceptualized as attributes  or events  and whether they should be recorded as numerical or categorical already poses a challenge. Second, covering more than one or a few regions across different time periods is challenging since experts in history and archaeology specialize on particular regions and time periods. Therefore a comprehensive regional and temporal coverage requires engaging with multiple experts and academic resources. Third, most history and archaeology research focuses on recording well-established facts, but scarcely records that a vast amount of historical knowledge is inferred.

To address these challenges, we present the Seshat Global History Databank: a structured and systematic dataset that aims to capture the state of historical knowledge worldwide. The Seshat Databank has a hierarchically structured codebook with numerical and categorical values, recorded for over 600 unique past societies ("polities") spread across all UN geographic regions 3. All variables are recorded as "evidenced" or "inferred", with data collected over more than a decade in collaboration with expert historians and archaeologists. Each variable's value is linked to at least one academic or expert reference and a textual explanation. Overall, the dataset used in the current article comprises 383 variables with 36,000 data points across more than 600 polities, and over 2,700 references.

We use the Seshat Databank to go beyond questions that are part of "general knowledge" and focus on history on the graduate and academic level. To test LLMs' knowledge, we convert the dataset into multiple-choice questions that we then feed into LLMs via prompts. An important component of our benchmarks is that we not only test whether LLMs can identify correct facts, but our questions also explicitly ask whether a fact is evidenced or inferred based on indirect evidence. We find that while LLMs outperform random guessing, they fall short of expert comprehension. LLMs' knowledge is relatively evenly spread between global regions. Nonetheless, OpenAI's models perform better for Latin America and the Caribbean, while Llama performs best for Northern America. Both OpenAI's and Llama models' performance is worse for Sub-Saharan Africa. Across time, there is a clear pattern: LLMs perform best for earlier periods.

## 2 Related Work

### History and anthropology datasets

This work is part of and builds upon the Seshat Global History Databank project, which has collected historical data since 2011 with the purpose of documenting what is currently known and unknown about the social and political organization of past human societies across the globe and how they have evolved over time [11; 12]. To date, these data have been used primarily to test different historical theories and hypotheses about long-term cultural evolution and the role of agriculture, warfare and religion in human history [19; 20; 21; 22; 23]. While Seshat kept records of references and written justifications, previous permanent 4 releases of these data did not link each data point to an academic or expert source in a machine-readable way nor make the explanatory text readily usable for machine processing, and were not used to benchmark or train LLMs.

In addition to Seshat, there have been other global data collection efforts for anthropology. Notable examples are the Standard Cross-Cultural Sample and the Ethnographic Atlas . Originally published as printed volumes, and later as digital data tables, these datasets were created for comparative research among societies with diverse environmental and social conditions, and thus include a wide range of categorical variables whose values were assigned by experts, based on a reading of a wide literature that typically included textual descriptions of the societies in question. Recently, D-PLACE was created as a digital, online resource that collects these and related datasets  and allows quantitative and statistical analysis of a wide range of questions. In total, D-PLACE contains over 600 thousand data points, each of which represents an individual "fact" about a certain society. However, these datasets mostly focus on small-scale societies (tribes and chiefdoms between 100 and 100,000 people), do not link specific facts to their sources individually, do not have textual explanations, and do not record changes of variables across time, nor when a variable is unknown or disputed.

Our work also relates to datasets that focus on specific historical topics. The Database of Religious History (DRH) collects knowledge about historical and modern religious practices based on expert knowledge . The latest version contains over 250,000 data points, each pertaining to one specific aspect of religious practice of a specific group or setting. Data points are supported by either expert comments (textual descriptions) or citations to academic literature. Another specific topic dataset is PAGES, which compiles data on past land use, a valuable input for models that connect human activities to environmental conditions . Focusing on modern times, the Correlates of War project gathered detailed data about military conflicts from the past two centuries, including several categorical variables and numerical estimates for each of hundreds of conflicts included .

Last, our work relates to datasets that do not record events, but compile original historical texts. These include the Perseus Digital Library, a corpus focusing mainly on classical (Mediterranean) antiquity, with a total corpus size of over 68 million words, along with English translations for a large part of the corpus ; the Trismegistos project that provides a database of all known names from classical (Mediterranean) antiquity and references to their sources ; the Papproological Navigator, a digitized corpus of texts written on papyrus, typically from the dry areas of the Mediterranean ; or the ORACC corpus of cuneiform texts . While traditionally these have been the domain of specialists who would work on reconstructing, translating and interpreting them, recent advances in natural language processing (NLP) techniques provide several promising applications . Languages with larger corpora, such as Latin and Akkadian, have already been used for training language models with a realistic aim of machine-assisted translations ; in the case of smaller corpora, machine learning models can still assist in more specialized tasks, such as the reconstruction of text fragments on inscriptions or papyri . These works could potentially be used as training or benchmark resources for LLMs, but currently lack the structure and broad focus to applications beyond linguistics or highly specialized applications.

### Benchmarking LLMs knowledge

Our work is also related to the growing research benchmarking LLMs' knowledge and performance in specific disciplines. The MMLU benchmark of Hendrycks et al.  measured the performance of GPT-3 and related models across a wide range of disciplines using multiple-choice questions. Models generally performed better for social sciences and humanities than natural sciences. The benchmarks included separate categories for high-school level knowledge of European, US and world history; however, performance was similar across these categories, with accuracy scores between 50% and 65% for the best performing GPT-3 and UnifiedQA models5. Recent models have substantially improved upon this: GPT-3.5-Turbo achieved 85.7% accuracy, GPT-4 Turbo, the best current model, achieved 95.8% accuracy, and Llama-3-70b, the best open-weight model, achieved 94.1% accuracy on the High School World History subtask of MMLU . However, because world history taught in high school is biased towards Western history , these results are not informative regarding the balance of LLMs knowledge in comparison to what is actually known of societies by expert historians. In contrast, our work compares the knowledge of LLMs across all expert knowledge.

Recent efforts have focused on benchmarking LLM performance in specific fields. For instance, Guo et al.  demonstrated LLMs' capabilities in various chemistry tasks. Guha et al.  confirmed their proficiency in legal tasks, and Gandhi et al.  evaluated their social reasoning abilities. Chen et al.  introduced a QA benchmark dataset for time-sensitive questions, relevant to historical knowledge. Several papers also address integrating general and domain-specific knowledge. Onoe et al.  presented the CREAK dataset, combining specific facts with common-sense reasoning, and a training procedure to enhance LLM performance. Ge et al.  offered a framework for interfacing general-purpose LLMs with domain expertise. These studies often use few-shot learning, leveraging models' ability to generalize from limited examples [51; 53].

Literature on biases in language models and AI systems is growing, assessing the values and norms they represent. Early research highlighted how biases in training data manifest in word embeddings and models [54; 55], with warnings about the potential for harms and discriminatory outcomes in algorithmic systems . Recent studies propose complementary approaches to mitigate biases during LLM training and deployment, including explicit safety pipelines [13; 57], detailed data collection on social biases , and model benchmarks on their prevalence . However, most focus on stereotypes and associations about specific groups rather than biases in knowledge distribution across space and time.

## 3 Dataset

The Seshat database contains historical knowledge dating from the mid-Holocene (around 10,000 years before present) up to contemporary societies. However, the bulk of the data pertains to agrarian societies in the period between the Neolithic and Industrial Revolutions, roughly 4000 BCE to 1850 CE. The information contained in the dataset runs the gamut from the most basic historical information (e.g., the presence or absence of writing) to highly complex topics (e.g., certain religious and ideological systems) for which it is vital to record differing interpretations, provide nuance, and historical context. Seshat encodes what is known about global history, but also what is unknown, poorly known, and contested [10; 11; 12].

The dataset covers all UN geographic regions, with data collected about polities that occupied certain "Natural Geographic Areas" (NGAs) in them over time. In total, the Seshat team defined 35 NGAs for the purpose of data collection. For an illustration of the the distribution of data points across time, geographic regions and NGAs, see Figure S2 in the Appendix. In the current paper, for the purpose of studying performance across geography, we use a regional division scheme based on the UN regions, with two important modifications: (i) We split North America and Europe into two regions; (ii) We include Hawaii in Oceania. We made these modifications since most of the focus of Seshat is on polities that existed before colonization and Europe and North America had very different historical pathways in that period. Similarly, the first inhabitants of Hawaii are of Polynesian descent and thus until the nineteenth century, its history was more related to other regions in Oceania. We present the location of NGAs, along with the regional division scheme in Fig. 1. Overall, the dataset used in the current article comprises 383 variables and 36,000 data points, and over 2,700 references.

### Data Format

For each polity, data is coded for a set of variables clustered around 11 themes or research topics, which we denote _Variable category_. For instance, there are variables describing the social complexity of polities - levels of administrative hierarchy, territorial scale of polities, information technology - as well as variables for military technology and organization, religion and rituals, agricultural techniques and productivity, institutions such as court systems and measures of biological well-being such as life expectancy or average adult stature. While the target of coding is individual polities, changes in variable values within a polity's lifespan are also recorded and time-stamped; for instance, noting a polity's population as 12 million from 100 to 150 CE, then 14 million from 150 to 220 CE.

Each datapoint ("Seshat record") consists of a coded value for a certain variable, linked to a named polity and date range, a justification text, which describes the evidential basis for the code, and citations to reliable scholarly source(s) including personal communication from an expert. Where no date range is given, the full date range of the polity is implied. Sometimes the justification texts include direct quotes of copyrighted material, in this cases, we keep the first 5 and last 5 words as indicators and replace the middle text with ellipsis "<...>". In the subset of the Seshat databank we use for Benchmarking possible values of data points are:

* Absent, present: evidence points to the absence or presence of a certain variable * Inferred absent, inferred present: Direct evidence is lacking or sparse, but indirect evidence points towards an absent or present code.

### Data Collection

The data collection process comprises four main steps, illustrated in Figure 2. First, the Seshat Board6 defines the variables and polities for study. The board aims to ensure spatial balance in the polities included in the sample, representing societies from all inhabited continents and UN geographic regions. To select variables, the board focuses on outstanding and contested research questions in

Figure 1: World map displaying our division of regions inspired by the UN geographic regions. Each marker on the map corresponds to an NGA, as defined by Seshat experts for the purpose of data collection. Researchers identified and collected data about each policy that occupied or overlapped with each NGA over the course of history. Colors correspond to the regional division scheme used in the current paper based on the UN geographic regions.

Figure 2: Illustration of Seshat’s data collection process. RA stands for research assistants. Circular arrows denote iterative process and discussion. The blue arrow from step 3 to step 1 indicates that sometimes more polities and variables were added to the dataset and collection process after discussions among the Seshat team as well as external academic experts.

history and archaeology, asking which data is needed to answer them. The resulting variables are compiled into a Codebook.

Second, data collection begins. Under the supervision of established scholars (professors and PhD-level researchers), research assistants (RAs) populate the database with accessible information from scholarly publications. While coding variables, RAs write narrative paragraphs explaining the rationale for each coding decision, citing relevant scholarship. While the language of these scholarly publications varies, there is a predominance of English sources. This mostly is due to the inherent imbalance in the language of history scholarship in the Global North, which is increasingly in English. The Seshat team is currently working on expanding the number of non-English sources.

Third, for variables used in previous Seshat publications , experts review and provide feedback. This feedback comes through various methods, including reviewing coded data sheets, emails, video meetings, and in-person workshops. Steps 2 and 3 are iterative, often requiring multiple rounds of expert feedback and revisions. Variables in the Codebook are also revised during these discussions, as indicated by the arrow from step 3 to step 1 in Figure 2. In the current dataset, 14,000 out of 36,000 data points underwent this process of expert review and cleaning prior to being made publicly available online  and used in articles published by the Seshat team. The remaining 22,000 were coded by graduate-level research assistants under the supervision of the Seshat board but have not yet been subjected to the same level of scrutiny. However, the selection of variables for use in articles was guided by research questions, which have focused on social complexity and warfare, rather than quality of data, and past experience indicates that only a small proportion of data (less than 5 percent) will change as a result of expert review. The review status of each data point is indicated in a dedicated column of the released dataset.

In the fourth step, we assemble and curate all data, ensuring each reference is uniquely linked to a Zotero database. For the current publication, we have avoided copyright infringement by making publicly available only narrative paragraphs that do not directly quote from references.

### Data Limitations and Ethical Considerations

The Seshat database has several limitations. First, the data was collected mostly, though not exclusively, from English-language sources. This limitation likely results in less comprehensive coverage for non-English-speaking regions and limits diversity which is an ethical consideration and an area of improvement. Second, while the Seshat Board aimed for broad cross-cultural applicability when defining variables, their choices may reflect biases related to their expertise and backgrounds, and research assistants may have subjective interpretations when coding variables. Third, new evidence can update historical knowledge, so this benchmark reflects only the current understanding. However, the team consulted a wide range of experts for diverse perspectives and continuously revises the dataset. Regarding ethical considerations, users should be aware of copyright issues; while the dataset accompanying the current article is released under creative commons license, going beyond it, and scraping a considerable amount of text from cited references to train models should only be done in consultation with a legal team and ethical board approval to avoid infringing copyright of the cited references.

Seshat takes special care in the data collection process, providing detailed qualitative justifications and soliciting expert feedback; however, the dataset may still simplify important historical knowledge (see Slingerland et al.  for issues and solutions in creating systematic datasets for cultural comparison). Therefore, using this dataset for historical analysis and interpretation should only be done in collaboration with relevant experts . While the Seshat dataset has been used in several quantitative Cliodynamic7 studies , it remains an independent dataset that can be used for different research approaches and is not tied to any single theoretical framework.

We acknowledge that the Seshat project involves the study of myriad different communities and populations from the past. Some peoples living today trace their ancestry to one or more of these past groups. As researchers, we have an obligation to present fair-minded, responsible, and respectful information concerning the past. While maintaining a commitment to scientific enquiry, we are committed to avoiding biased interpretation or representation of past or contemporary cultures, to refraining from using harmful or disrespectful terminology, and to treating sensitive information or topics with appropriate nuance and respect for the density and lived experiences of descendant communities.

## 4 Evaluation Process Setting

We benchmark the historical knowledge of chat-based models GPT-3.5, GPT-4-Turbo, GPT-4o, Llama-3-70b, Llama-3.1-70B (FP8), Llama-3.1-8B (FP8) and Gemini-1.5-flash. We selected these models for their state-of-the-art capabilities, and accessibility. While our model selection is not exhaustive, it includes a balanced set of both open and closed weight models. All models are queried using temperature 0 and for OpenAI models we set the random seed to 42. Models are evaluated by checking if their answers match exactly with the correct answers. Our evaluations were executed in August 2024, and for OpenAI models, we report model fingerprints in the dataset.

To benchmark the LLMs' historical knowledge, we converted the dataset into multiple-choice questions and prompted LLMs to answer these. Specifically, we asked whether a certain variable (e.g., writing) was present, absent, inferred present, or inferred absent in a particular policy and time frame. We used a multi-shot approach, providing four examples demonstrating solutions to other questions, in order to aid the models understanding of the task and additionally ask the model to provide its own reasoning before giving an answer. The exact prompts alongside an extended description of the conversion process can be found in Appendix B. We also used personification, instructing the LLMs to act as history experts to enhance their performance. Although data contamination is a central issue for benchmarks, we believe it is less critical for our benchmark, which primarily assesses the ability of LLMs to retain factual knowledge. In our evaluation, we exclusively utilized APIs that prevent the leakage of our benchmark to cloud providers and AI companies.

Given the unbalanced distribution of answers in our dataset8, we measure performance using balanced accuracy, which is standard in several benchmark studies [3; 48]. We also use adjusted balanced accuracy, normalizing the metric between 0 and 1, where 0 represents the average accuracy of random guessing. To assess whether differences in performance across models, time, regions, and categories were significant, we used bootstrap resampling (with 1000 resamples) to estimate the standard error of the balanced accuracy. We report 95% confidence intervals in all our results. We analyze the LLMs' overall performance across UN World regions, time periods, and Seshat variable categories. To determine whether the limitations of the models pertain to general knowledge or the distinction between inferred and known facts, we also measure balanced accuracy using only the "Present" and "Absent" categories. We refer to this as the 2-choice balanced accuracy, whereas the test including inferred categories is the 4-choice balanced accuracy.

## 5 Results

With respect to the entire dataset, all models exhibit similar performance, with balanced accuracy ranging from 37.3% for GPT-3.5 to 43.8% for GPT-4-Turbo (random performance would result in 25%). Table 1 shows the balanced accuracy and adjusted balance accuracy of all models across the whole dataset. This table also includes the benchmark in a 2-choice format (absent/present), for which models balanced accuracy ranges from 57.6 to 63.2 (random performance would result in 50%).

Overall, GPT-4-Turbo performs the best and Llama-3.1-8B the worst in the 4-choice test. Gains in performance are related to model size, with the GPT-4 based models having a 10%-15% advantage over the much smaller GPT-3.5-turbo and Llama-3.1-8B. The larger Llama models and Gemini are placed in between. In the 2-choice balanced accuracy, GPT-4o performs best with a score of 63.2%, while Llama-3.1-8B performs worst. Our results indicate that while all models perform significantly above random, they still fall short of encompassing expert-level knowledge of history.

### Comparison across time, regions, and categories

Here we explore the performance of models across time, regions, and categories. In Tables 2-4 we display results for the better performing models: the two variants of GPT-4, Llama-3.1-70B (FP8)

[MISSING_PAGE_FAIL:8]

performance decreases from 55.8% in the period 8,000 BCE - 6,000 BCE to 35% in the period 1,500 CE - 2,000 CE. The confidence intervals in brackets show this decrease is significant. These results suggest that while historical knowledge of early periods may be limited, LLMs perform better in these periods once this limitation is accounted for.

With respect to regions, GPT-4-Turbo is the leading model in six of the eight regions assessed (see Table 3 and S3), though some differences are within confidence intervals. GPT-4o has the second best average performance and also the best performance in Latin America and Oceania. Models typically perform best in the Americas, with the exepction of Llama-3.1-8B (East and Southeast Asia), while they perform the worst in Oceania and Sub-Saharan Africa. Somewhat surprisingly, Europe is not the region where models perform best. While regional performance differences are moderate, they may indicate a bias towards the American continent and a neglect of Oceania and Sub-Saharan Africa. The decrease in performance from GPT-4-Turbo and GPT-4o to GPT-3.5 in Sub-Saharan Africa underscores potential biases in model training and development.

Analysis across different categories also has some heterogeneity across models (see Table 4 and S4). GPT-4-Turbo achieves the highest scores in seven out of ten categories, including "Cults and Rituals" and "Legal System". GPT-4o leads in the remaining three categories, specifically "Economy", "Social Complexity", and "Warfare" variables. Gemini and Llama-3-70b follow with relatively good performance in the "Social Complexity", "Warfare" and "Well-being" categories in the case of both models, and especially bad performance for "Economy" in the case of Llama.

## 6 Discussion and Conclusion

In this paper presents a dataset derived from the Seshat Global History Databank to benchmark the historical knowledge of LLMs. This dataset comprises 36,000 data points of historical knowledge across over 600 polities, with references and explanations for each data point. As LLMs are increasingly used for information retrieval and search, monitoring LLMs knowledge is of crucial avoid replicate existing geospatial and historiographical biases [66; 67]. The Seshat dataset is structured a representation of historical knowledge and was designed to have regional and temporal representativeness, explicitly working to avoid Eurocentrism and other biases, making it a good candidate for a first benchmark of LLMs' global history knowledge.

Our benchmarking results reveal that, while the overall performance of the LLMs on expert historical knowledge is better than random guessing, it falls short of comprehensive expert-level knowledge. GPT-4-Turbo outperforms the other models in overall knowledge. We observe significant variation in model performance across different regions and time periods. Models perform best for earlier

  Region & Gemini-1.5-flash & GPT-4-turbo & GPT-4o & Llama-3.1-70B \\  Central, S Asia & 40.6 [39.6, 41.6] & **46.7**[45.6, 47.7] & 45.3 [44.3, 46.1] & 41.5 [40.5, 42.4] \\ E, SE Asia & 38.9 [37.9, 39.9] & **44.7**[43.5, 45.7] & 44.1 [43.2, 45.1] & 39.0 [38.0, 39.9] \\ Europe & 37.9 [36.7, 39.2] & **44.4**[43.0, 46.0] & 43.4 [42.2, 44.9] & 41.0 [39.8, 42.3] \\ Latin America & 39.2 [37.2, 41.0] & 49.2 [47.0, 51.3] & **49.4**[47.2, 51.6] & 43.3 [41.3, 45.2] \\ N Africa, W Asia & 38.6 [37.6, 39.5] & **46.1**[45.0, 47.3] & 43.8 [42.9, 44.8] & 40.4 [39.6, 41.4] \\ N America & 40.9 [38.4, 43.3] & **46.3**[43.7, 48.8] & 45.2 [42.7, 47.6] & 43.4 [41.2, 45.8] \\ Oceania & 32.8 [30.3, 35.1] & 38.0 [35.1, 40.8] & **40.5**[37.7, 43.5] & 37.3 [35.0, 39.8] \\ Sub-Saharan & 34.9 [32.7, 36.9] & **42.6**[40.2, 44.9] & 40.8 [38.5, 43.1] & 35.8 [33.9, 37.7] \\  Best performance & N America & Latin America & Latin America & North America \\ Worst performance & Oceania & Oceania & Oceania & Sub-Saharan Africa \\  Mean & 38.0 & **44.7** & 44.1 & 40.2 \\ Std. deviation & 2.6 & 3.1 & 2.6 & 2.5 \\  

Table 3: Balanced accuracy across geographic regions for four models. Brackets show confidence intervals using bootstrap resampling. Bold values indicate the best performing model (out of all seven) in that region. See Table S3 in the Appendix for the other three models, mean balanced accuracy values across models and the number of data points in each region; also see Tables S5 and S6 for finer grained results on the level of NGAs.

historical periods, particularly those before 3000 BCE, with accuracy declining as the timeframe approaches the present day. This suggests that while LLMs handle limited early historical knowledge well, they struggle with the complexities of more recent data. Regionally, GPT-4-Turbo leads in six of the eight regions assessed, with Llama-3-70b leading in Northern America and GPT-3.5 in Sub-Saharan Africa. The consistent underperformance in Sub-Saharan Africa highlights potential biases in LLM training and development, indicating a need for more balanced training data.

Going forward this first benchmark should be taken as a lower bound on historical knowledge performance. The Seshat team continues to expand the data sources with explicit efforts to consult and cite non-English literature. Important future work includes increasing collaborations with universities in the Global South and indigenous groups, which will contribute to a more accurate understanding of global history [16; 47]. For regions and countries such as Latin America, China, Japan, Egypt, and the Middle East with a wider scholarship on languages other than English, we also expect the stringency of the benchmark of historical knowledge to increase as more data is gathered. Future work could explore the relationship between LLMs training data and their performance on specialized datasets like Seshat, to better understand the influence of training data on model accuracy. In addition, assessing global historical knowledge across diverse populations, with balanced representation of societies worldwide, would be a valuable study for the humanities and provide a meaningful human benchmark for LLMs to compare to.

While the underwhelming performance of the LLMs on expert historical knowledge may be disappointing, there is a silver lining. The dataset includes references and indicates the part of the reference where the information was gathered from. With proper legal and ethical checks, this dataset may be used to train models to compile future data. Given recent research showing declines in public data sharing after the release of ChatGPT  and the challenges of training LLMs with synthetic data , this public dataset could be an important step for improving LLMs history knowledge. The Seshat project continues to collect and review data, and it can be amplified by LLMs and expert review. This dataset can serve as a foundation for future applications, including training data acquisition models to automate the collection of historical data. By leveraging LLMs for preliminary data gathering and having experts review and refine the information, we can enhance both the efficiency and accuracy of historical data collection. Overall, while our results highlight areas where LLMs need improvement, they also underscore the potential for these models to aid in historical research.