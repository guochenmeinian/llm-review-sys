# D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion

Jialin Chen

Yale University

jialin.chen@yale.edu

&Shirley Wu

Stanford University

shirwu@cs.stanford.edu

&Abhijit Gupta

Yale University

abhijit.gupta@yale.edu

&Rex Ying

Yale University

rex.ying@yale.edu

###### Abstract

The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness. 1

## 1 Introduction

Graph neural networks (GNNs) have rapidly gained popularity recently due to their ability to model relational data . However, when it comes to critical decision-making and high-stake applications, such as healthcare, finance, and autonomous systems, the explainability of GNNs is fundamental for humans to understand the model's decision-making logic and build trust in the deployment of GNNs in real-world scenarios .

Counterfactual and model-level explanations.Existing methods mainly focus on factual and instance-level explanations , while the significance of counterfactual and model-level explanations are equally noteworthy, yet under-explored. Counterfactual explanation considers"what-if" scenarios of model predictions, addressing the question of how slight adjustments to the input graph can lead to different model predictions . Model-level explanation, on the other hand, aims to generate the most discriminative graph pattern for a target class, thus shedding light on the overall decision-making behavior and internal functioning of the model . Counterfactual and model-level explanations present a distinct challenge concerning the distribution constraint imposed on generated explanations. An explanation that is faithful and reliable should adhere to the distribution of the underlying dataset. This becomes particularly crucial in real-world scenarios where domain-specific rules exist, such as in drug design and molecule generation. In such cases, explanations should conform to the true distribution of the dataset .

However, the existing methods typically extract explanatory subgraphs from the input graph, ignoring additional possible edges. This prevailing paradigm heavily relies on the out-of-distribution (OOD) effect to influence the model's prediction. To illustrate this point, in Figure 1, we show the t-SNE projection of the Tree-Cycle dataset, where graphs are labeled as _Tree_ or _Cycle_ based on whether they present the corresponding structures. Specifically, CF-GNNExplainer  generates counterfactual explanations for a node with _Tree_ label by removing its neighbor edges. While the explanation doesn't maintain any discriminative information on the Cycle class, it could still be predicted as _Cycle_ with high probability due to the OOD effect, making the explanation unreliable.

On the other hand, generating in-distribution graphs is challenging, due to the difficulty of encoding complex graph distributions, _e.g.,_ the distribution of node degrees, cycle counts and edge homogeneity. Recently, graph diffusion models have shown to be a powerful technique to encode such complex distribution on graphs , which trains a powerful denoising model that progressively removes noise from a random noise graph and then tractably recovers in-distribution samples.

**Proposed work**. Inspired by the success of graph diffusion models, we propose a novel GNN explainability approach, **D4Explainer**, in-**D**istribution GNN explanations via **D**iscrete **D**enoising **D**iffusion. Through a forward diffusion process that progressively introduces random noise, we enable D4Explainer to optimize for alternative and diverse explanations based on multiple noisy versions of the given graph. A powerful denoising model is trained to remove noise and eliminate redundant edges that are irrelevant to the target property, thereby ensuring the model's robustness. By employing a carefully designed loss function that incorporates both the preservation of the counterfactual property and generative graph distribution learning, D4Explainer is capable of generating in-distribution counterfactual explanations. As highlighted in green in the bottom left of Figure 1, D4Explainer adds essential edges that complete the truly counterfactual motif, _i.e., Cycle_. With a slight modification to the loss function, D4Explainer can also perform model-level explanations for a specific target class.

Empirical experiments on eight synthetic and real-world datasets show that D4Explainer achieves state-of-the-art performance in both counterfactual and model-level explanations, with a strong counterfactual accuracy (\(>\)\(80\%\) ) when only \(5\%\) of the edges are modified. Maximum mean discrepancy (MMD) metrics show that the distribution of explanations generated by D4Explainer is the closest to the original distribution of the dataset, compared with all baselines. D4Explainer obtains the highest Top-\(K\) accuracy in the robustness evaluation, which further illustrates that D4Explainer is capable of generating consistent explanations with the presence of noise.

**Our contributions** are in three-folds: (1) A novel approach to generate in-distribution, diverse and robust explanations is proposed, which leverages the denoising diffusion model to capture the underlying distributions of explanation graphs; (2) D4Explainer explores counterfactual explanations in a larger search space by allowing adding edges, which provides high-level understandings of how edge addition helps to create truly counterfactual motifs; (3) D4Explainer represents the first

Figure 1: t-SNE Projection of Tree-Cycle dataset, where _Cycle_ is a counterfactual motif for _Tree_.

framework that unifies counterfactual and model-level explanations, providing faithful explanations for both settings.

## 2 Related Work

Explainability of GNNsCompared with the explainability methods in image domain [21; 22; 23; 24; 25; 26; 27], explainability in GNNs  remains a challenging problem due to the discrete structure of graphs. Here, we focus on post-hoc and model-agnostic explanations. **Non-parameterized methods** rely on gradient-like signals [28; 29], relevant walks [30; 31], perturbation [9; 32; 33; 34] to identify important node/edge features or graph structures as explanations, without learnable parameters. **Score-based explainability**[6; 8; 7; 35] formulate a trainable model to obtain the importance scores on node features or edge as the explanations by maximizing the mutual information between the explanatory subgraph and the target prediction. **Counterfactual explanation methods** find minimal perturbation to the graph instance such that the prediction changes. However, most existing methods [13; 36] only consider edge deletion on the original graph without any distribution constraints, thus easily creating out-of-distribution samples and overfitting the noise over each individual instance. CLEAR  is the only explainer that also considers adding edges in generating counterfactual explanations. However, the intrinsic effect of edge addition to counterfactual properties is under-explored by CLEAR. **Generation-based explanations** is a recently popular trend that trains graph generators to generate GNN explanations. Existing works train policy networks for the sequential graph generation process based on the reinforcement learning approach [38; 10; 15] or explicitly parameterize the distribution of model-level explanations . The differences of our method are (1) we prevent explicit modeling and sequential decision-making learning but incorporate the generative graph distribution learning implicitly into the training procedure and (2) the more stable and robust generative backbone _i.e.,_ diffusion model ensures better properties of the generated explanations, _e.g.,_ diversity and robustness.

Graph Diffusion ModelsDenoising diffusion probabilistic models [39; 40; 41] are shown to be powerful for a wide range of generative tasks, including images , language , and discrete graph domain [19; 20; 44]. Recent work  proposes to use discrete noise for the forward Markov process without relying on continuous Gaussian perturbations. Another related work  formulates the diffusion process on the categorical node and edge attributes and successfully generates real and in-distribution graphs. Recently, the score-based model  and stochastic differential equations formulation have been applied to the field of graph generation [46; 47]. These related works highlight the effectiveness of diffusion models for graph denoising and generation tasks. In our paper, we design the pipeline of the diffusion-based model for explanation task scenarios, as well as devise a novel classifier-guided sampling algorithm for model-level explanations.

## 3 Preliminaries

### Problem Formulation

**Counterfactual explanation**. Given an instance (_i.e.,_ a node or a graph) and a well-trained GNN, the goal of counterfactual explanation is to identify the minimal modification to the original instance that alters GNN's prediction [12; 13; 36]. Without loss of generality, we consider the explanation problem for the graph classification task. Formally, let \(f\) denote a well-trained GNN classifier to be explained, \(_{G}\) denote the label of graph \(G\) predicted by \(f\). The counterfactual explanation \(G^{c}\) satisfies that \(_{G^{c}}_{G}\), while the difference between \(G^{c}\) and \(G\) is minimal. This problem is usually formulated as an optimization problem that minimizes the mutual information between \(G^{c}\) and \(_{G}\)[6; 13].

**Model-level explanation**. Model-level explanation aims to identify recurring and discriminant graph patterns that can trigger a specific prediction from the model \(f\)[15; 16]. Formally, given a class \(C_{i}\{C_{1},,C_{l}\}\), model-level explanation for the target class \(C_{i}\) can be defined as \(G^{m}=*{argmax}_{G}P_{f}(C_{i}|G)\), where \(P_{f}(C_{i}|G)\) denotes the probability for the class \(C_{i}\) predicted by the GNN \(f\), given the graph \(G\). See Appendix B for more descriptions of the explanation task setting.

### Discrete Diffusion Process for Graph

**Forward diffusion process**. In this work, we focus on discrete structural diffusion and leave the diffusion over continuous features in future work. Let \(t[0,T]\) denote the timestep of the diffusion process, which is also a noise level indicator. Let \(_{t}\) denote the one-hot version of the adjacency matrix at timestep \(t\), where each element \(_{t}^{ij}\) is a 2-dimensional one-hot encoding of the presence or absence of the \(ij\)-th element in the adjacency matrix. The forward diffusion process is a Markov chain with a transition matrix \(_{t}^{2 2}\), that progressively transforms the input graph into pure noise. Mathematically, the forward diffusion process can be written as \(q(_{t}^{ij}|_{t-1}^{ij})=(_{t}^{ij};=_{ t-1}^{ij}_{t})\), where \((;)\) is a categorical distribution over the one-hot vector \(\) with probability vector \(\). The multi-step diffusion has a closed form as \(q(_{t}^{ij}|_{0}^{ij})=(_{t}^{ij};=_{0} ^{ij}}_{t})\), where \(}_{t}=_{i=1}^{t}_{i}\). See Appendix C for detailed derivation.

**Graph-level expression**. The forward diffusion process is identically and independently performed over each edge in the full adjacency matrix. Therefore, the graph-level diffusion \(q(G_{t}|G_{t-1})\) is the product of element-wise categorical distributions as

\[q(G_{t}|G_{t-1})=_{ij}q(_{t}^{ij}|_{t-1}^{ij})q(G_{t}|G_{0})=_{ij}q(_{t}^{ij}|_{0}^{ij})\] (1)

Denoising diffusion models have shown a powerful ability to recover complex distributions accurately [48; 41; 20; 44], by leveraging the diffusion process to capture intricate dependencies and generate samples that exhibit high-quality in-distribution property and diversity.

## 4 Proposed Method: D4Explainer

D4Explainer is designed for two distinct explanation scenarios: counterfactual explanation and model-level explanation. In counterfactual explanation (Sec. 4.1), D4Explainer employs a Forward diffusion process to create a sequence of noisy versions and trains a Denoising model to effectively capture the desired distribution of counterfactual graphs. For model-level explanation (Sec. 4.2), D4Explainer trains a Denoising model to recover the underlying original distribution and leverages a well-trained GNN to progressively enhance the explanation confidence during the reverse sampling. An overview is shown in Figure 2. The notation used throughout this work is summarized in Appendix A.

Figure 2: Overview of D4Explainer. (a) Diffusion Model for counterfactual explanations. The diffusion process \(q(G_{t}|G_{t-1})\) transforms an input graph \(G_{0}\) to the pure noise \(G_{T}\). Then the Denoising Model \(p_{q}()\) outputs the clean graph \(_{0}\) given a noisy graph \(G_{t}\), under the constraints of the counterfactual loss \(_{cf}\) and the distribution loss \(_{dist}\). (b) Reverse Sampling for model-level explanations. We leverage a well-trained GNN to select a temporary graph with the highest confidence score from the candidate graphs and obtain \(G_{t-1}^{r}\) from \(G_{t}^{r}\) recursively until we achieve the final model-level explanation \(G_{0}^{r}\).

### Counterfactual Explanation Generation

**Forward diffusion process**. We build on the discrete diffusion process over graphs as introduced in Sec. 3.2. The forward Diffusion Process enables D4Explainer to optimize with a sequence of perturbed graphs \(\{G_{0},G_{1},,G_{T}\}\) with increasing levels of noise, which essentially enable D4Explainer to thoroughly explore possible counterfactual explanations for the given graph.

**Denoising model**. To generate a counterfactual graph that closely resembles the input graph, the Denoising Model \(p_{}(G_{0}|G_{t})\) takes as input the noisy adjacency matrix \(A_{t}\) corresponding to a noisy graph \(G_{t}\), the node features of the original graph \(_{0}\), noise level indicator \(t\), and then predicts the dense adjacency matrix. Through sampling from the dense adjacency matrix with the reparameterization trick , we arrive at the discrete adjacency matrix \(_{0}\) and the corresponding explanation graph \(_{0}\). The Denoising Model is set as an extension of the provably powerful Graph Network (PPGN) . To incorporate time information, an MLP module is employed to process the noise level indicator \(t\) and learn time-related latent features, thereby enhancing the denoising capability. The edge features, node features, and time-related latent features are concatenated and updated by the powerful layers (PPGN). We refer to Appendix D.1 for a complete and detailed description of the PPGN used in our D4Explainer.

**Loss function**. Different from traditional graph generation tasks , counterfactual explanations necessitate both counterfactual property and proximity to the original graph. To address these challenges, we propose a specifically designed loss function that simultaneously optimizes these two properties. Instead of iteratively recovering the intermediate noisy graph \(G_{t}\) in the traditional manner, we employ a re-weighted version of the evidence lower bound (ELBO) on the negative log-likelihood that directly reconstructs the initial distribution at \(t=0\) in our distribution-learning term \(_{dist}\). The re-weighting strategy prioritizes more challenging denoising tasks at larger timesteps:

\[_{dist}=-_{q(G_{0})}_{t=1}^{T}(1-2_{t}+)_{q(G_{t}|G_{0})} p_{}( G_{0} G_{t}),\] (2)

where \(_{t}\) is the transitioning probability (the off-diagonal element in the transition matrix \(}_{t}\)) and \(q(G_{0})\) is the distribution of the training dataset. The distribution loss \(_{dist}\) is equivalent to the cross-entropy loss between \(G_{0}\) and \(p_{}(G_{0}|G_{t})\) over the full adjacency matrix, which guarantees the proximity of generated counterfactual explanations to the original graph. To optimize the counterfactual property, we design a specific counterfactual loss \(_{cf}\) as follows,

\[_{cf}=-_{q(G_{0})}_{t[0,T]}_{q(G _{t}|G_{0})}_{p_{}(_{0}|G_{t})}(1-f(_{0})[_{G_{0}}]),\] (3)

where \(f\) is the well-trained GNN classifier, \(f(_{0})[_{G_{0}}]\) denotes the probability for the original label \(_{G_{0}}\) predicted by \(f\), given the generated graph \(_{0}\). Our total loss function is formulated as \(()=_{dist}+_{cf}\), where \(\) is a hyper-parameter that balances the counterfactual and in-distribution properties. Achieving the desired counterfactual property while maintaining proximity to the true data distribution involves a trade-off. For instance, making drastic modifications to the original graph may easily alter the model's prediction, but it can also lead to an explanation that deviates significantly from the original graph. The distribution loss \(_{dist}\) and the counterfactual loss \(_{cf}\) together encourage the denoising model to eliminate redundant edges that are irrelevant to the counterfactual property while reconstructing the original edges to preserve the true distribution.

**Working principle of D4Explainer**. D4Explainer not only preserves the **in-distribution** property but also introduces **diversity** and **robustness** to the generated counterfactual explanations. **Diversity** enables the explainer to provide multiple alternative explanations for model predictions, while **robustness** ensures consistent effectiveness of the explanations even in the presence of noise. Existing explainers often optimize a singular explanation per instance, leading to overfitting on noise and bias attribution issues . On the contrary, D4Explainer's objective is to search for counterfactual graphs within the distribution of the original graphs, adhering to the constraints imposed by \(_{dist}\) and \(_{cf}\). Through an iterative process of adding noise and removing counterfactual-irrelevant edges, D4Explainer captures the underlying distribution of counterfactual explanations. This denoising strategy also enhances the **robustness** of D4Explainer. Moreover, the inherent stochasticity in the forward processes introduces **diversity** into the generated explanations.

### Model-level Explanation

**Motivation**. The goal of model-level explanation is to generate class-wise graph patterns. Let \(C\) denote the target class. Each reverse sampling step \(q_{C}(G_{t-1}^{r}|G_{t}^{r})\) can be formulated as a conditional generation satisfying the following equation,

\[q_{C}(G_{t-1}^{r}|G_{t}^{r}) p_{}(_{0}|G_{t}^{r})q(G_{t-1 }^{r}|_{0})f(C|_{0}),\] (4)

where \(f(C|_{0})\) can be computed by the target class probability predicted by the well-trained GNN \(f\), conditioned on the given graph \(_{0}\). Existing sampling methods [20; 41] cannot perform conditional sampling in the discrete context, as we cannot sample all possible \(_{0}\) to obtain \(f(C|_{0})\) and then compute the normalized probabilities. To overcome these challenges, we propose to utilize the well-trained GNN as guidance toward the target class. At each step, we generate a set of candidates by \(p_{}(_{0}|G_{t}^{r})\) and refer to the GNN to select a temporarily optimal \(_{0}\) with the highest \(f(C|_{0})\).

**Multi-step sampling**. We repeat the sampling steps and progressively increase the explanation confidence (i.e., \(f(C|_{0})\)) in the process. Figure 3 shows an empirical visualization of the reverse generation process for the _house_ motif. We observe that the temporary graph \(_{0}\) gets closer to the target motif with increasing explanation confidence \(p\) during the reverse sampling process.

The proposed model-level explanation generation utilizes a denoising model trained with a similar procedure as Sec. 4.1 (Figure 2(a)). The difference is that the training loss is only \(_{dist}\), since \(_{cf}\) leads to a counterfactual graph that changes the label. To start with, given a predefined number of nodes \(N\) in the target explanation, we randomly sample an Erdos-Renyi graph with \(N\) nodes and edge probability \(\) as \(G_{T}^{r}_{N,1/2}\). Then we sample a set of candidates from the distribution \(p_{}(G_{0}|G_{T}^{r})\). The well-trained GNN computes the explanation confidences for these candidates and selects the temporary explanation \(_{0}\) with the highest score.

Then, we sample \(G_{T-1}^{r}\) through the same Diffusion Process \(G_{T-1}^{r} q(G_{T-1}|_{0})\) as Equation 1. Sampling steps iteratively reverse the chain until we obtain the final model-level explanation \(G_{0}^{r}\) after \(T\) steps. Apart from explanation confidence \(f(C|_{0})\), model-level explanations should also satisfy sparsity and succinctness. It is worth noting that the proposed algorithm is capable of preserving the sparsity level similar to the training graphs in the generated explanations. For real-world datasets that are densely self-connected, it is suggested to plug regularization constraints in the selection policy for the temporary explanation at each step. The complete sampling algorithm is shown in Appendix D.4.

### Complexity Analysis

D4Explainer has a search space of \((N^{2K})\) for modifying \(K\) edges in an \(N\)-nodes graph, which is larger than previous counterfactual explainers that only consider deleting edges. By framing the explanation task as a generation problem, the space complexity of each layer in D4Explainer is reduced to \((N^{2})\). The time complexity is \((N^{3})\) due to the matrix multiplication. Despite the large search space, the complexity of D4Explainer is still acceptable and faster than some generation-based explanations . Runtime and more complexity analysis are given in Appendix E.6. Furthermore, we directly recover the terminal explanation \(_{0}\) in the training procedure, rather than intermediate \(G_{t}\), which greatly increases the efficiency of D4Explainer. The Denoising Model can also be trained in parallel under different noise levels without iterative optimization from \(t=0\) to \(t=T\).

## 5 Experiments

### Experimental Setup

We test the proposed approach to explain the performance of node classification models and graph classification models. Dataset statistics and classifier information are summarized in Appendix E.1.

Figure 3: Visualization of the temporary \(_{0}\) at \(t=T;3T/4;T/2;T/4\) and the terminal model-level explanation for BA-3Motif (_house_ motif). Different node colors indicate different labels.

**Node classification**. For synthetic datasets, we use BA-Shapes, Tree-Cycle, Tree-Grids . There exists a motif that plays an important role in the model's prediction. The node labels are determined by the structural roles. We train a vanilla GCN for synthetic datasets, achieving over \(95\%\) accuracy on each synthetic dataset. Additionally, we use Cornell  dataset, a highly heterophilous real-world webpage graph. Wherein, more complex relationships exist between a node and its neighbors, thus posing a more significant challenge to the explanation tasks. We train an EGNN , which is specifically designed for heterophilous graphs, achieving \(83\%\) accuracy on Cornell.

**Graph classification**. We use one synthetic dataset, BA-3Motif  and three real-world molecule datasets, Mutag , BBBP  and NCI1  for graph-classification task explanation. BA-3Motif contains 3 graph classes: graphs with cycle motif, grid motif, and house motif. Mutag, BBBP, and NCI1 are molecular datasets where nodes represent atoms and graphs represent molecules. Specifically, the chemical functionalities of molecules determine the graph labels. We train a vanilla GCN for BA-3Motif, BBBP, and NCI1. For the Mutag dataset, GIN  is used as the target GNN.

**Baselines**. For the counterfactual explanation task, we take the same baseline setup as CF-GNNExplainer and involve more recent state-of-the-art explainers as our baselines, including GNNExplainer, SAExplainer, GradCam , IGExplainer, PGExplainer, PGMExplainer, and CXPlain . For the methods that are originally designed for the factual explanation, we construct a subgraph with the least important edges as the counterfactual explanation. For the model-level explanation task, we compare with XGNN , which is a state-of-the-art model-level explanation method for GNNs. More implementation details are given in Appendix E.3.

### Counterfactual Explanations

**Metrics**. Following evaluation protocols of prior works , we adopt Counterfactual Accuracy, Fidelity, and Modification Ratio (MR) as our metrics. Let \(G^{o}\) and \(G^{c}\) denote the original input graph and generated counterfactual graph, respectively. \(\) is the test dataset. Counterfactual Accuracy is defined as the proportion of generated explanations that change the model's prediction, \(=1-1/||_{G^{o}}(1(_{G^{c}}= _{G^{o}})\). Fidelity measures the change in output probability over the original class, _i.e.,_\(=1/||_{G^{o}}[f(G^{o})[_{G^{o}}]-f(G^{c})[ _{G^{o}}]]\). Modification Ratio refers to the proportion of changed edges as \(=(\#+)/|E|\). Higher counterfactual accuracy and fidelity with lower modification ratios indicate better performance.

**Results**. CF-ACC and Fidelity are sensitive to the modification ratio, we thus compute the areas under CF-ACC curve and Fidelity curve over \(10\) different modification ratios from \(0\) to \(0.3\). We run 10 different seeds for each approach and report the average in Table 1. As can be seen from the table, D4Explainer achieves the best performances on seven out of eight datasets, with especially strong CF-ACC AUC values (\(>90\%\)) on Tree-Cycle, Tree-Grids, and BA-3Motif. Notably, D4Explainer consistently works well on explaining both node classification and graph classification tasks, while the efficacy of baselines is unstable across datasets. For instance, most baselines fail to generate effective counterfactual explanations for complex graphs with multiple motifs or heterophilous edge relations, _e.g.,_ Cornell and BA-3Motif.

To further investigate the relation between CF-ACC and the modification ratio, we show the change of CF-ACC _w.r.t._ modification ratios from \(0\) to \(0.3\) in Figure 4, where the X-axis is in the \(\) scale.

    &  &  &  &  &  &  &  &  \\
**Models** &  &  & CF-ACC &  & CF-ACC &  & CF-ACC &  & CF-ACC &  & CF-ACC &  & CF-ACC &  & CF-ACC &  \\  Random & 0.251 & 0.261 & 0.260 & 0.281 & 0.337 & 0.375 & 0.138 & 0.172 & 0.404 & 0.452 & 0.192 & 0.256 & 0.073 & 0.113 & 0.288 & 0.352 \\ GNNExplainer & 0.473 & 0.444 & 0.652 & 0.580 & 0.672 & 0.622 & 0.075 & 0.120 & 0.250 & 0.253 & 0.450 & 0.449 & 0.212 & 0.241 & 0.375 & 0.443 \\ SAExplainer & 0.773 & 0.773 & 0.405 & 0.408 & 0.547 & 0.544 & 0.199 & 0.241 & 0.447 & 0.500 & 0.303 & 0.338 & 0.110 & 0.133 & 0.421 & 0.446 \\ GradCam & 0.552 & 0.570 & 0.673 & 0.631 & 0.590 & 0.578 & 0.138 & 0.189 & 0.459 & 0.495 & 0.202 & 0.250 & 0.274 & 0.301 & 0.467 & 0.488 \\ IGExplainer & 0.208 & 0.240 & 0.198 & 0.226 & 0.308 & 0.372 & 0.233 & 0.281 & 0.440 & 0.474 & 0.231 & 0.280 & 0.159 & 0.183 & 0.347 & 0.389 \\ PGExplainer & 0.361 & 0.357 & 0.353 & 0.322 & 0.293 & 0.340 & 0.128 & 0.204 & 0.203 & 0.232 & 0.238 & 0.282 & 0.338 & 0.366 \\ PQMExplainer & 0.208 & 0.210 & 0.242 & 0.212 & 0.237 & 0.206 & 0.274 & 0.212 & 0.213 & 0.128 & 0.251 & 0.105 & 0.154 & 0.348 & 0.390 \\ CXPlainer & 0.125 & 0.168 & 0.245 & 0.220 & 0.222 & 0.274 & 0.132 & 0.180 & 0.235 & 0.239 & 0.187 & 0.305 & 0.067 & 0.311 & 0.489 & 0.484 \\ CF-GNNExplainer & 0.773 & 0.728 & 0.812 & 0.178 & 0.537 & 0.527 & 0.328 & 0.297 & 0.302 & 0.304 & **0.797** & **0.512** & 0.632 & 0.715 & 0.674 \\
**D4Explainer** & **0.538** & **0.828** & **0.917** & **0.862** & **0.905** & **0.832** & **0.623** & **0.559** & **0.912** & **0.922** & 0.765 & 0.675 & **0.781** & **0.739** & **0.737** & **0.690** \\   

Table 1: CF-ACC AUC and Fidelity (FID) AUC of D4Explainer and baseline explainers over eight datasets. We report AUC values computed over 10 modification ratios from \(0\) to \(0.3\). The best result is in **bold** and the second best result is underlined.

As illustrated in Figure 4, D4Explainer consistently achieves the highest CF-ACC with the smallest modification ratio (see the right side of the X-axis). Especially for Tree-Cycle and BBBP dataset, D4Explainer obtains a significant boost compared to the baselines. It demonstrates that D4Explainer can generate counterfactual explanations that can strongly influence the prediction of the target GNN and reflect the effective counterfactual properties.

#### 5.2.1 In-Distribution Evaluation

To evaluate the in-distribution property of the generated explanations, we adopt the maximum mean discrepancy (MMD) to compare distributions of graph statistics between the generated counterfactual explanations and original test graphs. Following the evaluation setting in prior works , we use Gaussian Earth Mover's Distance kernel to compute MMDs of degree distributions, clustering coefficients, and spectrum distributions. Smaller MMDs mean that the two distributions are more similar and close, which indicates a better in-distribution property.

**Results**. Table 2 shows the MMD results on three real-world molecular datasets. We observe that D4Explainer outperforms baselines in general. Especially for BBBP and NCI1 datasets, D4Explainer achieves the lowest MMD distances across all metrics. The MMD results verify the effectiveness of D4Explainer in capturing the underlying distribution of datasets and generating in-distribution and more faithful explanations. We refer to Appendix E.4 for more results.

#### 5.2.2 Additional Faithfulness Aspects

**Explanation Diversity Evaluation.** We evaluate the diversity of counterfactual explanations in Figure 5 and Appendix E.5. The first row shows the original graphs. The second row shows the generated counterfactual explanations by CF-GNNExplainer , where only edge deletion is allowed. With edge addition, D4Explainer is capable of generating alternative counterfactual explanations from a different perspective. As can be found from Figure 5, there are two main approaches to generating counterfactual explanations. The first one is deleting determinant edges and destroying the original motif, thus greatly influencing the model's prediction. The second one is converting the original motifs to truly counterfactual motifs through both deleting and adding essential edges. Previous methods can only produce the first type of counterfactual explanations, while D4Explainer makes the second approach possible and successful, leading to alternative and diverse counterfactual

    &  &  &  \\  & Deg. & Clus. & Spec. & Sum. & Deg. & Clus. & Spec. & Sum. & Deg. & Clus. & Spec. & Sum. \\  RandomCaster & 0.1593 & 0.0247 & 0.0417 & 0.2257 & 0.1693 & 0.0072 & 0.0397 & 0.2162 & 0.1847 & 1.9769 & 0.0404 & 2.2020 \\ GNNExplainer & 0.1614 & 0.0002 & 0.0409 & 0.2025 & 0.1615 & 0.0002 & 0.0395 & 0.2012 & 0.1577 & 0.0005 & 0.0405 & 0.1987 \\ SAExplainer & **0.0940** & 0.0032 & 0.0412 & **0.1384** & 0.1594 & 0.0032 & 0.0402 & 0.2028 & 0.189 & 0.0002 & 0.0408 & 0.2300 \\ GradCam & 0.1122 & 0.0083 & 0.0416 & 0.1621 & 0.0690 & 0.0026 & 0.0384 & 0.1109 & 0.1638 & 0.0003 & 0.0404 & 0.2045 \\ IGExplainer & 0.1292 & **0.0000** & 0.0411 & 0.1703 & 0.0098 & **0.0000** & 0.0394 & 0.1302 & 0.4288 & 0.0002 & 0.0398 & 0.4688 \\ PGExplainer & 0.1475 & 0.0002 & 0.0418 & 0.1895 & 0.2014 & 0.0018 & 0.0403 & 0.2435 & 0.1937 & **0.0000** & 0.0396 & 0.2333 \\ PGMExplainer & 0.1800 & 0.0002 & 0.0419 & 0.2221 & 0.1916 & 0.0003 & 0.0403 & 0.2322 & 0.1299 & **0.0000** & 0.0404 & 0.2603 \\ CXplain & 0.1734 & 1.2706 & 0.0417 & 1.4857 & 0.1768 & 0.0001 & 0.0394 & 0.2163 & 0.1629 & 0.0001 & 0.0404 & 0.2034 \\ CF-GNNExplainer & 0.1172 & **0.0000** & 0.0380 & 0.1552 & 0.0870 & 0.0001 & 0.0393 & 0.1264 & 0.1224 & 0.0001 & 0.0404 & 0.1629 \\
**D4Explainer** & 0.1172 & **0.0000** & **0.0244** & 0.1416 & **0.0530** & **0.0000** & **0.0331** & **0.0861** & **0.1006** & **0.0000** & **0.0353** & **0.1359** \\   

Table 2: MMD distances between the generated explanations and test graphs. We report MMD distances of degree distributions (_Deg._), cluster coefficients (_Clus._), spectrum distributions (_Spec._), and the summation (_Sum._). We bold the best value and underlined the second-best value.

Figure 4: CF-ACC Curves of all explainers over different modification ratios from \(0\) to \(0.3\). The \(x\)-axis is shown in the \(\) scale. CF-ACC tends to increase as the modification ratio increases in general.

explanations. We ascribe the success to the special training mechanism of D4Explainer. The intrinsic stochasticity in the forward process allows D4Explainer to take as input a sequence of noisy versions of the original graph, instead of a singular input graph. This enlarges the search space of possible counterfactual explanations for D4Explainer.

**Robustness Evaluation.** To evaluate the robustness of all methods, we compare the counterfactual explanations produced on the original graph and its perturbed counterpart, respectively. A robust model would predict the same explanation for both inputs. Following previous setup , we identify the \(K\) most relevant edges in the original counterfactual explanation and compute the fraction of these edges present in the explanation of its noisy version, denoted by Top-\(K\) Accuracy. We apply noise by randomly adding or removing edges with probability \(\). A consistent \(20\%\) modification ratio is used across all methods. Results on BBBP dataset are shown in Figure 6. We observe that D4Explainer outperforms all baselines over different noise levels from 0 to \(10\%\). We restrict that \(<10\%\), as the larger noise may cause the noisy graph to switch the predicted label. Overall, results in Figure 6 verify D4Explainer's strong ability to generate consistently effective counterfactual explanations despite the noise. See Appendix E.7 for complete results.

### Model-level Explanations

In each step of the reverse sampling, we denoise \(K\) candidate graphs from the noisy graph and select a temporary explanation. Following the setting in XGNN , we qualitatively evaluate the generated explanations with different pre-defined numbers of nodes \(N\), shown in Figure 7. \(p\) denotes the target class probability predicted by the GNN. A higher \(p\) indicates higher explanation confidence. We observe that D4Explainer can produce more determinant graph patterns with nearly \(100\%\) confidence for synthetic datasets, _e.g.,_ BA-shapes and BA-3Motif.

**Quantitative evaluation**. We adopt the target class probability \(p\) and \(\) as the quantitative metrics. Density measures the sparsity level of the explanations, which is defined as \(=||/||^{2}\), where \(\) and \(\) denote the set of edges and nodes in the explanation. Quantitative comparisons between XGNN and D4Explainer under different numbers of nodes are shown in Table 3. Hyperparameter sensitivities of the \(K\) (number of candidates in each step) and \(T\) (number of reverse sampling steps) are shown in Table 4. The results are averaged over 100 generated model-level explanations without

Figure 5: Counterfactual explanations comparison. Red labels represent the motifs in the graph. Figure 6: Top-K accuracy _w.r.t._ noise levels on BBBP dataset

any regularization constraints in the selection policy. We find that (1) D4Explainer is capable of generating sparse and succinct model-level explanations with high target class probabilities, even without any regularization constraints on the explanation size. The superiority can be attributed to our distribution learning objective. However, it is worth noting that training graphs might be noisy and densely self-connected in some real-world applications. A regularization constraint can be easily plugged into the selection policy if required by downstream tasks; (2) smaller \(T\) and \(K\) both degrade the performance and quality of model-level explanations, which further emphasize the effectiveness of candidates and multi-step sampling. In the implementation, we ensure \(K 20\) and \(T 50\) for a balance between the quality and time complexity.

## 6 Conclusion and Broader Impacts

In this work, we propose D4Explainer, a novel generative approach for counterfactual and model-level explanations based on a discrete denoising diffusion model. By framing the explanation problem as a distribution learning task, D4Explainer can generate more reliable explanations with better in-distribution property, diversity and robustness. Additionally, D4Explainer can simultaneously perform model-level explanations with a pre-trained denoising model.

While denoising diffusion models show promise for explaining Graph Neural Networks (GNNs), they face potential scalability concerns on large graphs. Additionally, the explanations rely on the specific GNN architecture, limiting their generalizability across different GNN models. This work has dual social impacts. It enhances the transparency and interpretability of GNNs. However, it is vital to acknowledge the limitations and potential risks of relying solely on these explanations. They may not always capture the complete causal relationships in complex graph structures, which could lead to unintended consequences, reinforce biases, or make incorrect assumptions about the model's behavior. Looking ahead, an interesting direction for future research is to consider the node attributes and edge attributes during the explanation generation, _e.g.,_ by performing diffusion processes over continuous features.