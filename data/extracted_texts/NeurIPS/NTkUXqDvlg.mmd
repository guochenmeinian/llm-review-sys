# Using Unity to Help Solve Reinforcement Learning

Connor Brennan

Mila-Quebec AI Institute

Universite de Montreal

&Andrew Robert Williams

Mila-Quebec AI Institute

Universite de Montreal

&Omar G. Younis

Mila-Quebec AI Institute

Universite de Montreal

&Vedant Vyas

Department of Computing Science

University of Alberta

&Daria Yasafova

Mila-Quebec AI Institute

Universite de Montreal

&Irina Rish

Mila-Quebec AI Institute

Universite de Montreal

Equal contribution.

Equal contribution.

###### Abstract

Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe, an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments. This toolkit includes a robust implementation of OpenXLand, a framework for meta-RL based on XLand 2.0 , complemented by a user-friendly interface which allows users to modify the details of procedurally generated terrains and task rules with ease. Along with a ready-to-use implementation of OpenXLand, we provide a curated selection of terrains and rule sets, accompanied by implementations of reinforcement learning baselines to facilitate quick experimentation with novel architectural designs for adaptive agents. Furthermore, we illustrate how the United Unity Universe serves as a high-level language that enables researchers to develop diverse and endlessly variable 3D environments within a unified framework. This functionality establishes the United Unity Universe (U3) as an essential tool for advancing the field of reinforcement learning, especially in the development of adaptive and generalizable learning systems.

## 1 Introduction

Reinforcement learning (RL) and meta-reinforcement learning (meta-RL) foundation models have shown potential as adaptive agents that quickly adapt to diverse tasks in open-ended settings . However, open-source options for open-ended environment generation rarely go beyond constrained subsets of scenarios, with limited control or open-endedness. The availability of frameworks for diverse, controllable, and extendable environment generation would be a boon to the development of generally adaptive agents and meta-RL in general.

Inspired by the use of the Unity Game Engine for reinforcement learning development , we propose the _United Unity Universe_ (U3), an open-source environment development ecosystem based on the Unity game engine, which is freely available for personal use.1

Figure 1: **United Unity Universe.** U3 is a single open-sourced ecosystem with which the machine learning community can easily construct novel 3D environments without sacrificing fine-grain control over the task or the world. U3 provides a high level language to easily define rules and procedurally generate new worlds layouts, resulting in effectively infinite variety, even within a single environment.

Figure 2: **An example of world generation with U3.** We use U3 to create OpenXLand, our open-source implementation of the XLand environment space for meta-reinforcement learning . **Bottom left**: tiles make up metatiles, which we can sample to procedurally generate 3D worlds, within which an agent acts. **Right**: U3 can also procedurally generate tasks: we sample objects and conditions on the objects that define the rules of a task, with a final goal state that rewards the agent. In this environment, the agent receives reward when the orange sphere makes contact with the blue pyramid. We see that the orange sphere is elevated, and therefore the agent must find it and use the ramps to access it. As to the blue pyramid, we do not see it because it is not there: the agent must first get the orange sphere near the black rounded cube to spawn the blue pyramid. This environment also contains a grey pyramid that serves as a distraction. Importantly, if the agent brings the grey pyramid near the orange sphere, both will disappear, making it impossible for the agent to spawn a blue pyramid and subsequently obtain its reward. **Top left:** The agent’s perspective at initialization. The agent does not see the orange sphere, and so will have to search for it. However, the grey pyramid, which could lead to a dead end, is visible. As pointed out in , tasks in OpenXLand require many skills such as navigation, exploration, experimentation, and avoiding irreversible transitions. However, since the innumerable tasks are composed of a small number of components, the agent can improve performance by learning to reuse previously acquired knowledge in new tasks.

**Concretely, our main contributions are:**

* **United Unity Universe (U3), an open-source Unity-based environment development ecosystem.*
* Extendable world and task generation processes for rich environmental dynamics, as well as direct access to free Unity assets for building real-world environments.
* A two-way pipeline connecting U3 and Python for efficient training, agent evaluation and runtime adaptive auto-curricula.
* Scalable environments that can be run on multiple CPUs simultaneously.
* **OpenXLand, an XLand-style environment space of 3D environments in Unity with rich dynamics and compositional tasks.*
* A 3D world procedural generation system that makes an XLand-style environment space available to the reinforcement learning community.
* A production rule procedural generation system that extends XLand 2.0 with conjunctions of predicates, additional actions and additional objects.
* Six million OpenXLand datasets and production rules for easy comparison across models over 36 trillion possible environment combinations.

Section 2 contains related work for environment procedural generation, including the main line of Unity-based meta-RL work inspiring U3 and OpenXLand. In Section 3, we cover the requisite background, particularly on Unity-based environments for RL  and XLand [24; 23]. We then go over U3 in Section 4, focusing on its use of procedural generation, scalability and extendability. Section 5 describes our OpenXLand implementation, its datasets and implemented RL baselines. Section 6 reviews current limitations, and we conclude in Section 7.

## 2 Related Work

### Meta-Reinforcement Learning

The fast adaptation of foundation models has garnered attention recently as "in-context learning" . Significant efforts have been directed toward creating environments to study fast adaptation in the context of meta-RL. Training agents to continuously learn across an open-ended distribution of task spaces requires such an open-ended collection of environments. Frameworks that generate collections provide a tradeoff between collection size and diversity. Dennis et al.  put forward the formulation of Unsupervised Environment Design, where underspecified RL environments can generate diverse RL environments by instantiating unspecified parameters. For example, environment generation frameworks that use procedural generation  can create large environment collections.

### Procedural Generation in Reinforcement Learning Environments

The implementation of the procedural generation process determines the constraints on the environment collection's diversity and the open-endedness of the tasks. The eponymous Procgen benchmark  includes 16 different games with generated environments, enabling a controlled diversity. Minihack procedurally generates environments based on NetHack's pre-existing broad set of gridworld assets . The Avalon benchmark consists of 20 tasks in 3D procedurally generated worlds . Notably, these tasks can be composed. Malmo  creates an API for Minecraft to enable RL training in its procedurally generated 3D worlds, while MineRL  expands the API to leverage datasets of pre-existing content, an effort that Minedojo  considerably expands. A recent, closely related work is XLand-minigrid , a framework similar to our OpenXLand implementation, but constrained to the gridworld environments of minigrid. Another closely related work is Neural-MMO 2.0 , which includes a task generation system similar to that of  and Team et al. . We also mention MemoryMaze , a framework which generates random 3D mazes to evaluate the long-term memory capabilities of agents, because we reimplement MemoryMaze with U3 to demonstrate the framework's extendability in Section 4.4.

### The XLand Environment Spaces

The frameworks that we draw our inspiration from are the family of XLand-related works from Deepmind on open-ended learning. Team et al.  first presents XLand, a Unity-based systemto produce endless environments for open-ended learning. In XLand, agents must interact with procedurally generated environments to move the state toward a "goal state" represented by a set of conditions that depend on the environment. Alchemy  is another Unity-based RL suite, where the agent must discover latent causal structures between stones and potions to compose state transitions toward a goal state. Combining concepts found in both XLand and Alchemy, the Deepmind Team et al.  introduces XLand 2.0, which leverages both diverse world and compositional task generation to create rich environment dynamics. In particular, they use XLand 2.0 as a testbed to study RL foundation models for agent adaptation and find that, with correct the learning design choices, RL agents can adapt to new environments and tasks on a timescale similar to humans. Unfortunately, neither XLand nor XLand 2.0 are open-source. The machine learning community can neither build upon the work, nor validate their promising results.

## 3 Background

In this section, we go over the necessary concepts from the Unity-related work of Ward et al.  and Team et al. [24; 23]. Existing foundation models depend on large datasets, but agents trained for fast adaptation through meta-RL may provide an alternative path to general intelligence . However, careful consideration is required when designing the environments for these agents. If the environment is too simple, the agent will learn a direct mapping from the environment state to action, destroying any chance of generalization . If the environment is too complex, sparse rewards may prevent the agent from learning any patterns . The approach favored by XLand 2.0 is to employ _procedural generation_ to create complex environments from a set of simple rules.Once the agent learns these rules, it can compose them together to generalize what it has learned to new environments.

In Ward et al. , agents rely on sensors to provide observations, such as a pixel rendering of their line of sight or their current score. Agents use actuators to act in the environment, including moving, jumping and grabbing objects. We act on the assumption that both Team et al.  and Team et al.  use a similar system, and we include RL policies as components of agents.

Both XLand and XLand 2.0 procedurally generate the 3D _world_ in which agents act, including the floor topology, manipulable objects and agent spawn locations. XLand 2.0 has a second procedural generation process that creates _production rules_, which act as a grammar for a subset of state transitions. Specifically, production rules are made of 1) _conditions_ on agents and objects, and 2) _actions_, such as spawning or removing objects. To obtain rewards, agents must manipulate objects according to production rules to reach a _goal state_, which is a boolean function on the environment state. Although production rules in XLand do not allow conjunctions or disjunctions of multiple conditions, "predicate" refers to both the goal state and the production rule conditions. Together, the agents, the world, goal states and production rules for XLand 2.0 make up a task.

The environment in XLand is partially occluded, which forces the agent to learn to engage in exploration and planning . In XLand's 3D world, this occlusion occurs when terrain tiles block the agent's line of sight. For production rules, this occlusion occurs because the rules active in each environment are different. Thus, compositional generalization alone is insufficient for the agent to solve the task. The agent must also learn across a number of _trials_, which are sequential repetitions of the same task, in order to uncover the hidden information in the environment. The results from XLand 2.0  suggest that their agents learned to explore the environment in a reasoned manner that took into account the underlying structure of the environment. For example, the agent would systematically explore the world to find all production rule objects. It would also attempt to combine production rule pairs in a systematic way and remember pairs that reacted to form new objects. In this way, the agent was able to quickly learn the layout and rules of the environment, and adapt to the new task on the same timescale as human participants.

A foundation model of adaptive agents, trained on a sufficiently complex procedurally generated environment may be able to apply this type of systematic learning to meta-learn novel tasks even beyond its original training domain. XLand 2.0 showed promise of out of domain learning in an experiment in which the agent had to learn to push, and not pick up, an object. However, the extent of the adaptive agent's generalization capabilities can be studied further. U3 provides a potential test bed in which such an agent can be tested not only on different configurations of the XLand 2.0 environment, but also on novel environments with different rules, graphics and mechanics.

## 4 United Unity Universe (U3)

U3 uses _procedural generation_ to create a large number of reinforcement learning environments made of _worlds_ and _production rules_ in a fashion similar to XLand 2.0 . A visual interface can control the active world and production rules, as well as their procedural generation processes. Both processes can also be extended with new components, enabling endlessly diverse environments.

### Procedural generation in U3

We implement a _world generation process_ that determines the static 3D floor plan. Inspired by the Wave Function Collapse (WFC)  and Model Synthesis  algorithms, the process samples environment components that are locally compatible with neighboring components until the world is complete. The world is made up of _voxels_ that tile the entire world in 3 dimensions. The width, length and height of this voxel grid defines the size of the world. During the world procedural generation process each voxel is populated by a single _tile_, the base unit of the U3environment. In order to increase the amount of structure available to the procedural generation algorithm we do not place tiles directly. Instead, we define _metalules_ which consists of multiple tiles grouped together to form a pre-existing piece of the floor plan (e.g. "platform" or "ramp"). Each of the tile's six faces has a _type_, and we define legal adjacencies between face types such that, for any given step in the world generation process, the set of permissible metatiles that can be sampled is clearly defined. We also make it possible to dynamically reweight the probability distribution across permissible metatiles, e.g. to favor the sampling of specific environment features or to encourage metatile diversity within an environment.

Production rules are used to define a task on the generated world. We allow for statically defined rules, as well as a _production rule generation process_ that controls the scaffold for how tasks are defined. We also offer an implementation to enable and disable rules during runtime, allowing for a huge variety of different task definitions. We explore the details of OpenXLand's production rule procedural generation in section 5

The combination of the world and production rule generation processes makes available a large, diverse set of environment dynamics. To facilitate access to these environments, U3 provides a direct visual interface to not only _control_ the world, production rules, and visual aspects, but also _extend_ the generation processes for the world and production rules. For example, users can modify the world generation process by changing the set of metatiles or the size of the world. They can also expand the production rule generation process with new conditions or actions. The fine-grained control and extendability of U3 result in an ideal ecosystem for scalable RL methods (See Section 4.4).

Figure 3: **Scalability.** We explore the scalability of U3 across 3 dimensions. The x-axis explores the effect of increasing the number of Unity instances running in parallel. The plot coloring indicates the number of CPUs available to the computation. Finally, the dotted vs dashed lines show the resolution of the visual observation.

### U3 scales to multiple instances

In this section, we explore the scalability capabilities of our computational environment. The focus is on evaluating the performance in terms of the number of steps per second (SPS) that can be executed under varying configurations. Specifically, we assess the impact of the number of asynchronous environment instances, the number of CPU cores, and the image size of the observations.

Figure 3 illustrates the relationship between the number of asynchronous environment instances and the SPS. As expected, the SPS increases with the number of environments, but it saturates with a high number of environments. However, the saturation point increases with the number of cores per CPU, making the U3 scalable with the increase of compute capabilities. Finally, we tested the effect of the image observation size, specifically varying the size among 64x64, 128x128, and 256x256 pixels. As expected, increasing the image size results in a marginal decrease in the SPS.

### U3 is easily extendable

Although our primary contribution is a U3 implementation of OpenXLand, U3 is also a powerful ecosystem for generating novel environments due to its extendability. Firstly, U3 can incorporate new meatiles with novel assets, be they acquired from the Unity Assets Store  or made by the user. We use only free assets on the Asset Store under the Asset Store's standard End User License Agreement. The world generation process can then be set up to construct novel environments with these new assets. The production rule generation process can also integrate new production rule conditions, actions and objects. Along with the fact that U3 enables production rules with conjunctions of conditions, this results in an infinitely rich space of possible production rules. As both the world and task generation processes are extendable, U3 can efficiently reimplement existing RL benchmarks.

We demonstrate the power and ease of use of U3's interface by implementing MemoryMaze . Figure 4 details the process and user interfaces involved in the process of setting up the world generation. The task itself is set up by defining static production rules (one for each goal). The condition of these rules is AGENT_NEAR and the action is a custom defined action that is a composite of REWARD and TOGGLE_RULE. Note that by defining rules in terms of composites of existing

Figure 4: **Implementation of MemoryMaze. A walk-through of the steps required to implement MemoryMaze in U3. A) First step is to define the tile face palette. You can assign a name and color to each face type for easy identification later. The face matching matrix defines what faces can be adjacent to each other, and how likely that adjacency should be. B) The next step is to define a weighted metallic pool. These pools are nestable, allowing for easy modification of weights across categories. C) and D) Next we set up the metatiles that populate the pool. Tiles type is used to calculate the permissible area of the generated terrain. Each face of the tile is assigned a face type. These tiles make up the structure of a metatile. E) Final result of the U3 set up for MemoryMaze. Note that the colored dots in the image are mushrooms that act as the goals.**rules U3 is able to create modular rules that can be reused by the community. Only one of the static rules is active at a time, and the active rule is communicated to the agent through a simple UI widget.

The U3 implementation of MemoryMaze (see C) uses free assets from the "Free Low Poly Nature Forest" asset pack under the Standard Unity Asset Store EULA.

### U3 allows dynamic, adaptive control over the environment

U3 utilizes ML-Agents to provide a two-way communication channel between Unity and Python. Specifically, U3 provides a protocol for high-level manipulation of the environment by defining the parameters of the environment generation process, including the world generation process and production rule generation process. For example, it is possible to dynamically reweight the environment generation parameter distributions, enabling the possibility for agent-environment co-evolution and runtime adaptive auto-curricula. It also allows Unity to send meta-data about the state of the environment beyond the main training observations. This can be useful for designing repeatable experiments in the environment for evaluating trained agents.

## 5 OpenXLand

To demonstrate the utility of U3, we use it to implement OpenXLand, a task space similar to that of XLand  and XLand 2.0 . We define XLand-style procedural generation processes for both environments and production rules. We also make available datasets of world and production rules, which can be used to build up to 36 trillion unique environments. In the simulated environment, the agent perceives a first-person visual representation of its surroundings. The agent is capable of executing both continuous and discrete actions. Continuous actions include moving forward or backward and rotating its body left or right, with the ability to modulate the velocity of each movement. Discrete actions encompass picking up objects, placing them down, TODO. The agent is positively rewarded with a reward of +1 for each time step during which the predefined goal condition is met. Each trial concludes after a user-specified number of steps, and the episode terminates

Figure 5: **Procedural generation in OpenXLand. Illustration of the building blocks that make up U3’s procedural generation system. A) The primary building block of the world is a tile. Tiles have faces that can be assigned face types. Metatiles allow the user to arrange tiles into distinct structures. Here we have illustrated one of the ramp metatiles used in OpenXLand. Metatiles also allow the user to define a payload, which is deposited into the scene after WFC. B) Production rules are generated based on a set of initial production rule objects. A production rule is created by sampling a condition and an action (from SPAWN or SWAP). The effect of these rules on the set of active production rule objects is calculated and the process is repeated. At the end of the chain, a final production rule is created by sampling a condition and applying the REWARD action.**

following a user-defined number of trials. To bootstrap the development of adaptive agents based on OpenXLand, we also provide clean implementations of common RL algorithms such as SAC, as well as a training system that supports repeated iterations and sequences of environments. Details on the metatiles, production objects, conditions, and actions used in OpenXLand can be found in the Appendix.

### Datasets

In an effort to reduce the computational load of procedural generation and filtering at run-time, we provide a series of curated datasets for the U3 implementation of XLand. These datasets serve as a community benchmark suite that researchers can use to test and compare reinforcement learning algorithms and network architectures on a standardized set of XLand environments.

We provide 6 datasets of one million pre-generated worlds and 6 datasets of one million pre-generated production rules for a total of 36 trillion possible environment combinations. Curriculum learning has been shown to be important for task learning . Thus, we designed our datasets to facilitate the construction of a smooth learning curve, while still providing quantifiable increases in difficulty measures (Easy, Medium, and Hard). Datasets are drawn from a distribution of difficulties with some overlap between each progressive increase (See Figure 6). Information on the amount of time it takes to generate the datasets can be found in Table 5.

#### 5.1.1 World datasets

The difficulty of the world datasets is given by two measures: world size, and world height. World size is the primary difficulty metric that defines the Easy, Medium, and Hard datasets. Each world was constructed by drawing a world width and length from a Gaussian distribution with parameters defined by the dataset. We set the minimum size of the world to be 5x5, so that all dataset distributions were truncated at that minimum bound. Details on the parameters used for each dataset can be found in Table 3 in the Appendix.

World height serves to modulate the topological complexity of the worlds. Low heights lead to better visibility, more simply connected terrain, and larger accessible areas. We provide two types of world heights for each world size: Low and High. World height cannot be less than 1, so all distributions are truncated appropriately. World height provides an orthogonal measure of complexity that can be used to further enhance the curriculum of training.

#### 5.1.2 Production rule datasets

Production rule datasets also include two measures of difficulty: chain length and distractor count. Chain length is the number of SWAP or SPAWN rules that must be activated to reach the REWARD rule. Similarly to the world datasets we have drawn chain lengths from Gaussian distributions with parameters defined by the dataset difficulty. We also draw the number of initial objects from the same Gaussian distribution as the chain length. We lock chain length and number of initial objects to the same distribution to simplify the complexity of the datasets. All chains terminate in a REWARD rule, so the minimum chain length is 1. Similarly, there must be at least 1 initial object, so all distributions for chain length and initial objects were truncated at 1. Details can be found in Table 4 in the Appendix and in Figure 6.

To ensure a consistent set of production rules that allow for a valid solution, we first sample production rules with mutually exclusive conditions, such that there is always a valid path to the reward. We subsequently sample "distractors" rules that branch off from the main production rule. Distractor count adds additional complexity by creating rules that act as distractors or potentially cause the agent to become unable to achieve the reward. A distractor becomes a dead end when the production rule removes an object from the environment such that the production rule chain can no longer be completed. Thus, the severity of these distractors can be dependent on the progress of the agent through the chain. We provide two levels of distractor counts: Low and High. We truncate all distributions at a minimum value of 0 distractors.

#### 5.1.3 Dataset deployment

All datasets are hosted on Huggingface for easy download and usage. Datasets are distributed as a tar file of 1 million JSON files. The individual JSON files are labeled from 1 to 1 million, allowing easy splitting of the dataset into train and test partitions. While the generation of novel environments requires the user to download the Unity Editor and work directly with U3's built-in UI and classes, we provide Python scripts that allow the generation of new OpenXLand (or any existing environment) datasets without the need to install the Unity Editor.

### Evaluation protocol

To standardize future performance comparisons on U3 and OpenXLand, we propose an evaluation protocol that assesses the agent's ability to generalize to out-of-distribution tasks. We designate a held-out set of production rule objects. Then, we filter each dataset for instances that contain at least one of those elements. The remaining instances form the training set, while the testing set contains the excluded elements. Therefore, the training split does not contain any instances with these specific objects or conditions, ensuring that the rules and goals encountered during the testing phase are entirely novel.

Following the methodology of previous works [23; 17], we evaluate the agent using the 20th percentile of the normalized return for each environment instance, as this avoids simpler tasks dominating the average performance of the agent. Furthermore, we propose restricting the method comparison to within datasets, as opposed to across datasets, as this avoids the costly normalization by compute-heavy fine-tuned baselines proposed in .

Figure 6: **Dataset distributions.** Histograms showing the distributions of key dataset metrics. Note that all datasets have overlap. **A)** Distribution of width and lengths in the world datasets. Note that the 6 datasets are split into ”easy”, ”medium” and ”hard” for this metric. **B)** Distribution of heights in the world datasets. Note that now the 6 datasets are split into ”low” and ”high”. **C)** Distribution of the number of initial objects in the rule datasets. The 6 datasets are split into ”short”, ”middle” and ”long”. **D)** Distribution of the number of distractors in the rule datasets. Note that the ”few” datasets collapse to the same distribution. The ”many” datasets all have unique distributions. We have outlined the ”many” datasets to make their overlap more clear.

### Results

We tested our environment using the Soft Actor-Critic (SAC) implementation available in CleanRL. Each episode consists of 8 trials, each having 481 steps, corresponding to 40 human seconds. The procedurally generated world is loaded from the easy_low dataset at the start of an episode and kept constant across all 8 trials. The production rules are loaded from the short_few dataset. The initial positions of the production rules and the agent are randomized to locations in the largest accessible area of the world at reset at the beginning of each trial. The agent's context is kept across trials but reset when the episode ends. Results of the training are available in the Appendix.

## 6 Limitations

One of the main obstacles to using Unity for RL is correctly setting up Unity. Despite U3 including all relevant software files, hardware and platform differences can complicate installation. In the future, we plan to implement a docker environment to ease installation considerations. Another obstacle to using Unity as an RL environment can be the compute required to train an agent in Unity.2 We expand on the efficiency of the current implementation of U3 in Appendix F. We also note that an important line of future work involves optimizing the efficiency of the current implementation of U3 to make it more accessible, such as environments like  and [21; 22].

This initial release of U3 and OpenXLand is missing some features. Despite the current implementation of production rules generalizing Xland 2.0, certain conditions are missing (e.g. "seeing" for agents and objects). Our implementation is also missing the ability to pass information about the active production rules to the agent. Another feature we plan to implement in future work is the ability to test human participants on U3 tasks using a web interface (This functionality was present in an early version of U3 ). Finally, our training scripts currently lack teacher models and auto-curcurcula. Future work should also include implementing features beyond the original XLand 2.0. Production rules are a powerful system that we can generalize far beyond the simple chains of XLand 2.0. Although the current production rule system already goes beyond that of XLand 2.0 with conjunctions of conditions, enabling even more generality would require updating the implementation.

## 7 Conclusion

We present U3, an open-source ecosystem capable of implementing and defining procedurally generated 3D environments with a simple user interface. U3 is scalable to many instances and extendable with new assets for the generation of both environments and production rules. We also provide OpenXLand, an open-source implementation in the style of XLand 2.0 , an open-ended environment that has shown promise in training adaptive agents. As the science of foundation models progresses, the adaptation capacities of AI agents in novel and open-ended environments stand out as a key yardstick of general intelligence. We hope that U3 will facilitate the training and evaluation of such foundation models in the open-source community.

## Broader Impact

As far as we know, Unity has yet to solve intelligence . However, XLand 2.0  suggests a promising new approach to general intelligence with adaptive agents that are able to actively explore and meta-learn their environment. By improving the adaptive ability of agents, we can reduce the risk of unintended consequences from agents in unfamiliar settings. We hope that U3 will bolster the study of adaptive agents by the open-source community, and facilitate research on the strengths, limitations and human correlates of such foundational models. An important limitation of U3 is its accessibility due to the compute requirements of training agents such as Ada. Our commitment to open-source ensures that any work stemming from the U3 ecosystem will be transparently assessable against ethical guidelines, ensure robustness across many distinct tasks, be reproducible across research groups and available to all researchers across the globe. However, since the code is open source and users can create any assets they see fit, we have limited control and visibility over the downstream uses of U3.