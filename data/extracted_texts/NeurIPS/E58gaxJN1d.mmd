# Learning from Visual Observation via Offline Pretrained State-to-Go Transformer

Bohan Zhou\({}^{1}\) Ke Li\({}^{2}\) Jiechuan Jiang\({}^{1}\) Zongqing Lu\({}^{1,2}\)

\({}^{1}\) School of Computer Science, Peking University

\({}^{2}\) Beijing Academy of Artificial Intelligence

Corresponding author <zongqing.lu@pku.edu.cn>

###### Abstract

Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potential of utilizing video-only data to solve difficult visual reinforcement learning tasks rather than relying on complete offline datasets containing states, actions, and rewards. The project's website and code can be found at https://sites.google.com/view/stgtransformer.

Figure 1: A two-stage framework for learning from visual observation. The first stage involves three concurrently pretrained components. A feature encoder is trained in a self-supervised manner to provide easily predicted and temporally aligned representations for stacked-image states. State-to-Go (**STG**) Transformer is trained in an adversarial way to accurately predict transitions in latent space. A discriminator is updated simultaneously to distinguish state transitions of prediction from expert demonstrations, which provides high-quality intrinsic rewards for downstream online reinforcement learning in the next stage.

Introduction

Reinforcement learning (RL) from scratch imposes significant challenges due to sample inefficiency and hard exploration in environments with sparse rewards. This has led to increased interest in imitation learning (IL). IL agents learn policies by imitating expert demonstrations in a data-driven manner rather than through trial-and-error processes. It has been proven effective in various domains, including games  and robotics .

However, acquiring demonstrated actions can be expensive or impractical, _e.g._, from videos that are largely available though, leading to the development of learning from observation (LfO) [3; 4; 5; 6; 7; 8; 9]. This line of research utilizes observation-only data about agent behaviors and state transitions for policy learning. Humans naturally learn from visual observation without requiring explicit action guidance, such as beginners in video games improving their skills by watching skilled players' recordings. However, LfO agents face challenges in extracting useful features from raw visual observations and using them to train a policy due to the lack of explicit action information. Thus, further study of learning from visual observation (LFVO) has the potential to grant agents human-like learning capabilities.

In this paper, we investigate a reinforcement learning setting in which agents learn from visual observation to play challenging video games, such as Atari and Minecraft. Existing LfO approaches like [4; 5; 6; 7; 8] primarily apply to vector-observation environments such as MuJoCo. Others like [3; 9; 10; 11; 12; 13; 14] explicitly consider or can be applied to high-dimensional visual observations. Among them, representation-learning methods [9; 10; 11] learn effective visual representations and recover an intrinsic reward function from them. Adversarial methods [3; 12; 13] learn an expert-agent observation discriminator online to directly indicate visual differences. However, noises or local changes in visual observations may easily cause misclassification . In [12; 13], additional proprioceptive features (_e.g._, joint angles) are used to train a discriminator, which are unavailable in environments that only provide visual observations. Moreover, as these methods require online training, sample efficiency is much lower compared to offline learning. Goal-oriented methods, like , evaluate the proximity of each visual observation to expert demonstrations or predefined goals. However, defining explicit goals is often impractical in open-ended tasks . Furthermore, the continuity of observation sequences in video games cannot be guaranteed due to respawn settings or unanticipated events.

To address these limitations and hence enable RL agents to effectively learn from visual observation, we propose a general two-stage framework that leverages visual observations of expert demonstrations to guide online RL. In the first stage, unlike existing online adversarial methods, we introduce and pretrain **State-to-Go (STG) Transformer**, a variant of Decision Transformer (DT) , for **offline** predicting transitions in latent space. In the meanwhile, **temporally-aligned and predictable** visual representations are learned. Together, a discriminator is trained to differentiate expert transitions, generating intrinsic rewards to guide downstream online RL training in the second stage. That said, in the second stage, agents learn merely from generated intrinsic rewards without environmental reward signals. Our empirical evaluation reveals significant improvements in both sample efficiency and overall performance across various video games, demonstrating the effectiveness of the proposed framework.

**Our main contributions are as follows:**

* We propose a general two-stage framework, providing a novel way to enable agents to effectively learn from visual observation. We introduce State-to-Go Transformer, which is pretrained offline merely on visual observations and then employed to guide online reinforcement learning without environmental rewards.
* We simultaneously learn a discriminator and a temporal distance regressor for temporally-aligned embeddings while predicting latent transitions. We demonstrate that the jointly learned representations lead to enhanced performance in downstream RL tasks.
* Through extensive experiments in Atari and Minecraft, we demonstrate that the proposed method substantially outperforms baselines and even achieves performance comparable to the policies learned from environmental rewards in some games, underscoring the potential of leveraging offline video-only data for reinforcement learning.

Related Work

**Learning from Observation (Lfo)** is a more challenging setting than imitation learning (IL), in which an agent learns from a set of demonstrated observations to complete a task without the assistance of action or reward guidance. Many existing works [5; 18; 19; 20] attempt to train an inverse dynamic model (IDM) to label observation-only demonstrations with expert actions, enabling behavior cloning. However, these methods often suffer from compounding error . On the other hand,  learns a latent policy and a latent forward model, but the latent actions can sometimes be ambiguous and may not correspond accurately with real actions. GAIfO , inspired by , is an online adversarial framework that couples the process of learning from expert observations with RL training. GAIfO learns a discriminator to evaluate the similarity between online-collected observations and expert demonstrations. Although helpful in mitigating compounding error , it shows limited applicability in environments with high-dimensional observations. Follow-up methods [12; 13] pay more attention to visual-observation environments, but require vector-state in expert observations to either learn a feasible policy or proper visual representations. More importantly, learning a discriminator online is less sample-efficient, compared to Lfo via offline pretraining. A recent attempt  demonstrates some progress in action-free offline pretraining, but return-to-gos are indispensable in addition to observations because of upside down reinforcement learning (UDRL) framework . Moreover, it only shows satisfactory results in vector-observation environments like MuJoCo. In this work, we focus on learning from offline visual observations (LfVO), a more general and practical setting, which is also considered by the concurrent work VIPER . STG and VIPER both offline pretrain a reward function and then use it for online reinforcement learning. STG learns discriminative intrinsic rewards in WGAN style while VIPER leverages logarithm predictive probability as guidance.

**Visual Representation Learning in RL.** High-quality visual representations are crucial for LfVO. Many previous works [26; 27; 28; 9; 10; 11] have contributed to this in various ways. For example,  employs GANs to learn universal representations from different viewpoints, and [27; 28] learn representations via contrastive learning to associate pairs of observations separated by a short time difference. In terms of LfVO, a wide range of methods such as [9; 10; 11] learn temporally continuous representations in a self-supervised manner and utilize temporal distance to assess the progress of demonstrations. Nevertheless, in games like Atari, adjacent image observations may exhibit abrupt or subtle changes due to respawn settings or unanticipated events, not following a gradual change along the timeline. Moreover, over-reliance on temporal information often results in over-optimistic estimates of task progress , potentially misleading RL training.

**Transformer in RL.** Transformer  is widely acknowledged as a kind of powerful structure for sequence modeling, which has led to domination in a variety of offline RL tasks. Decision Transformer (DT)  and Trajectory Transformer (TT)  redefine the offline RL problem as a context-conditioned sequential problem to learn an offline policy directly, following the UDRL framework . DT takes states, actions, and return-to-gos as inputs and autoregressively predicts actions to learn a policy. TT predicts the complete sequence dimension by dimension and uses beam search for planning. MGDT  samples from a learned return distribution to avoid manually selecting expert-level returns as DT. ODT  extends DT to bridge the gap between offline pretraining and online fine-tuning. MADT  extends DT to a multi-agent reinforcement learning setting.

## 3 Methodology

### Preliminaries

**Reinforcement Learning.** The RL problem can be formulated as a Markov decision process (MDP) , which can be represented by a tuple \(=<,\), \(\), \(\), \(,_{0}>\). \(\) denotes the state space and \(\) denotes the action space. \(:[0,1)\) is the state transition function and \(:\) is the reward function. \(\) is the discount factor and \(_{0}:\) represents the initial state distribution. The objective is to find a policy \((a|s):\), which maximizes the expected discounted return:

\[J()=_{_{0},a_{t}(|s_{t}),s_{t}} [_{t=0}^{}^{t}r(s_{t},a_{t})].\] (1)

**Transformer.** Stacked self-attention layers with residual connections in Transformer is instrumental in processing long-range dependencies, each of which embeds \(n\) tokens \(\{x_{i}\}_{i=1}^{n}\) and outputs \(n\) embeddings \(\{z_{i}\}_{i=1}^{n}\) of the same dimensions considering the information of the whole sequence. In this study, we utilize the GPT  architecture, an extension of the Transformer model, that incorporates a causal self-attention mask to facilitate autoregressive generation. Specifically, each input token \(x_{i}\) is mapped to a key \(k_{i}\), a query \(q_{i}\), and a value \(v_{i}\) through linear transformations, where \(z_{i}\) is obtained by computing the weighted sum of history values \(v_{1:i}\), with attention weights determined by the normalized dot product between the query \(q_{i}\) and history keys \(k_{1:i}\):

\[z_{i}=_{j=1}^{i}(\{q_{i}^{},k_{j^{}}\}_{j^ {}=1}^{i})_{j} v_{j}.\] (2)

The GPT model only attends to the previous tokens in the sequence during training and inference, thereby avoiding the leakage of future information, which is appropriate in state prediction.

**Learning from Observation.** The goal is to learn a policy from an expert state sequence dataset \(^{e}=\{^{1},^{2},,^{m}\},^{i}=\{s_{1}^{i},s_{ 2}^{i},,s_{n}^{i}\},s_{j}^{i}\). Denote the transition distribution as \((s,s^{})\). The objective of LfO can be formulated as a distribution matching problem, finding a policy that minimizes the \(f\)-divergence between \(^{}(s,s^{})\) induced by the agent and \(^{e}(s,s^{})\) induced by the expert :

\[J_{}()=_{^{i}^{e},(s,s^{})^{i}}_{f}[^{}(s,s^{} )\|^{e}(s,s^{})].\] (3)

It is almost impossible to learn a policy directly from the state-only dataset \(^{e}\). However, our delicately designed framework (see Figure 1) effectively captures transition features in expert demonstrations to provide informative guidance for RL agents, which will be expounded in the following.

### Offline Pretraining Framework

**STG Transformer** is built upon GPT  similar to DT , but with a smaller scale and more structural modifications to better handle state sequence prediction tasks. Unlike DT, in our setting, neither the action nor the reward can be accessible, so the STG Transformer primarily focuses on predicting the next state embedding given a sequence of states.

As depicted in Figure 2, first we concatenate a few consecutive image frames in the expert dataset to approximate a single state \(s_{t}\). Then, a sequence of \(n\) states \(\{s_{t},,s_{t+n-1}\}\) are encoded into a sequence of \(n\) token embeddings \(\{e_{t},,e_{t+n-1}\}\) by the feature encoder \(E_{}\) composed of several CNN layers and a single-layer MLP, where \(e_{t}=E_{}(s_{t})\). A group of learnable positional embedding parameters is added to the token embedding sequence to remember temporal order. These positional-encoded embeddings are then processed by the causal self-attention module which excels in incorporating information about the previous state sequence to better capture temporal dependencies, followed by layer normalization. The linear decoder outputs the final latent prediction sequence \(\{_{t+1},,_{t+n}\}\). Denote the positional encoding, transition predicting, and linear decoding model together as \(T_{}\). It is worth noting that instead of predicting the embeddings of the next state sequence directly, we predict the embedding change and combine it with token embeddings in a residual way, which is commonly applied in transition prediction  and trajectory forecasting  to improve prediction quality.

For brevity, in further discussion we will refer to \(T_{}(e_{t})\) directly as the predicted \(_{t+1}\).

**Expert Transition Discrimination.** Distinguishing expert transiting patterns is the key to leveraging the power of offline expert datasets to improve sample efficiency in downstream RL tasks. Traditional online adversarial methods [3; 12; 13] employ a discriminator to maximize the logarithm probability of transitions sampled from expert datasets while minimizing that from transitions collected online,

Figure 2: State-to-Go Transformerwhich is often sample-inefficient in practice. Moreover, in the case of visual observation, the traditional discriminator may rapidly and strictly differentiate expert transitions from those collected online within a few updates. As a result, the collected observations will be assigned with substantially low scores, making it arduous for policy improvement.

To overcome these limitations, we draw inspiration from WGAN  and adopt a more generalized distance metric, known as the Wasserstein distance, to measure the difference between the distributions of expert and online transitions. Compared to the sigmoid probability limited in \(\), the Wasserstein distance provides a wider range and more meaningful measure of the difference between two transition distributions, as it captures the underlying structure rather than simply computing the probability. More importantly, unlike traditional online adversarial methods like GAIfO  that use the Jensen-Shannon divergence or Kullback-Leibler divergence, the Wasserstein distance is more robust to the issues of vanishing gradients and mode collapse, making offline pretraining possible. Specifically, two temporally adjacent states \(s_{t},s_{t+1}\) are sampled from the expert dataset, then we have \(e_{t}=E_{}(s_{t}),e_{t+1}=E_{}(s_{t+1})\), and \(_{t+1}=T_{}(E_{}(s_{t}))\). The WGAN discriminator \(D_{}\) aims to maximize the Wasserstein distance between the distribution of expert transition \((e_{t},e_{t+1})\) and the distribution of predicted transition \((e_{t},_{t+1})\), while the generator tries to minimize it. The objective can be formulated as:

\[_{,}_{w}_{^{i}^{e _{}},(s_{t},s_{t+1})^{i}}[D_{}(E_{} (s_{t}),E_{}(s_{t+1}))-D_{}(E_{} (s_{t}),T_{}(E_{}(s_{t}))) ].\] (4)

\(\{D_{}\}_{}\) represents a parameterized family of functions that are 1-Lipschitz, limiting the variation of the gradient. We clamp the weights to a fixed box (\(=[-0.01,0.01]^{l}\)) after each gradient update to have parameters \(w\) lie in a compact space. Besides, to suppress the potential pattern collapse, an additional \(L_{2}\) norm penalizes errors in the predicted transitions, constraining all \(e_{t}\) and \(_{t}\) in a consistent representation space. Thus, the loss functions can be rewritten as follows.

For discriminator:

\[_{w}_{dis}=_{^{i}^{ e_{}},(s_{t},s_{t+1})^{i}}[D_{}(E_{} (s_{t}),T_{}(E_{}(s_{t})))-D_ {}(E_{}(s_{t}),E_{}(s_{t+1})) ].\] (5)

For STG Transformer (generator):

\[_{,}_{adv}+_{mse}= -_{^{i}^{e_{}},s_{t}^{ i}}D_{}(E_{}(s_{t}),T_{}(E_{}(s_{t} )))\] \[+_{^{i}^{e_{}},(s_{t},s_{t +1})^{i}}\|T_{}(E_{}(s_{t}))-E_{ }(s_{t+1})\|_{2}.\] (6)

By such an approach, the discriminator can distinguish between expert and non-expert transitions without collecting online negative samples, providing an offline way to generate intrinsic rewards for downstream reinforcement learning tasks.

**Temporally-Aligned Representation Learning.** Having a high-quality representation is crucial for latent transition prediction. To ensure the embedding is temporally aligned, we devise a self-supervised auxiliary module, named temporal distance regressor (TDR). Since the time span between any two states \(s_{i}\) and \(s_{j}\) in a state sequence may vary significantly, inspired by , we define symlog temporal distance between two embeddings \(e_{i}=E_{}(s_{i})\) and \(e_{j}=E_{}(s_{j})\):

\[t_{ij}=(j-i)(1+|j-i|).\] (7)

This bi-symmetric logarithmic distance helps scale the value and accurately capture the fine-grained temporal variation. The TDR module \(P_{}\) consists of MLPs with 1D self-attention for symlog prediction. The objective of TDR is to simply minimize the MSE loss:

\[_{,}_{tr}=_{^{i}^{e_{} },(s_{i},s_{j})^{i}}\|P_{}(E_{}(s_{i} ),E_{}(s_{j}))-t_{ij}\|_{2}.\] (8)

**Offline Pretraining.** In our offline pretraining, the transition predictor \(T_{}\) and transition discriminator \(D_{}\) share the same feature encoder \(E_{}\) similar to online methods , which allows them to both operate in an easily-predictable and temporally-continuous representation space.

At each training step, a batch of transitions is randomly sampled from the expert dataset. The model is trained autoregressively to predict the next state embedding without accessing any future information. When backpropagating, \(_{mse}\) and \(_{adv}\) concurrently update \(E_{}\) and \(T_{}\) to provide high-quality visual embeddings as well as accurate embedding prediction. \(_{trd}\) is responsible for updating the \(E_{}\) and \(P_{}\) as an auxiliary component, and \(_{dis}\) updates \(D_{}\). Algorithm 1 in Appendix A details the offline pretraining of the STG Transformer.

### Online Reinforcement Learning

**Intrinsic Reward.** For downstream RL tasks, our idea is to guide the agent to follow the pretrained STG Transformer to match the expert state transition distribution. Unlike , our experimental results show that the pretrained WGAN discriminator is robust enough to clearly distinguish between the sub-optimal online transitions and expert transitions from datasets.

Thus, there is no need for fine-tuning and we just use the discrimination score as the intrinsic reward for online RL. Moreover, we do not use 'progress' like what is done in . This is because, in games with multiple restarts, progress signals can easily be inaccurate and hence mislead policy improvement, while the WGAN discriminator mastering the principle of transitions can often make the correct judgment. The intrinsic reward at timestep \(t\) is consequently defined as follows:

\[r_{t}^{i}=-D_{}E_{}(s_{t}),T_{}( E_{}(s_{t}))-D_{}E_{}(s_{t} ),E_{}(s_{t+1}).\] (9)

A larger \(r_{t}^{i}\) means a smaller gap between the current transition and the expert transition.

**Online Learning Procedure.** Given an image observation sequence collected by an agent, the feature encoder first generates corresponding visual representations, followed by the STG Transformer predicting the latent one-step expert transition. Then the discriminator compares the difference between real transitions and predicted transitions. Their Wasserstein distances, as intrinsic rewards \(r^{i}\), is used to calculate generalized advantage, based on which the agent policy \(_{}\) is updated using PPO . It is worth noting that the agent learns the policy merely from intrinsic rewards and environmental rewards are not used.

## 4 Experiments

In this section, we conduct a comprehensive evaluation of our proposed STG on diverse tasks from two environments: a classical **Atari** environment and an open-ended **Minecraft** environment. Among the three mainstream methods mentioned in Section 1, goal-oriented methods are not appropriate for comparison because there is no pre-defined target state. Therefore, we choose IDM-based method **BCO**, online adversarial method **GAIfO** and its follow-ups **AMP**, **IDDM**, and representation-learning method **ELE**.

For each task, we conduct 4 runs with different random seeds and report the mean and standard deviation. The same network architecture is applied for each algorithm to maintain consistency. Similar to , the discriminator and policy network share the same visual encoder for all online adversarial methods, and we choose one-step prediction for ELE to align with STG. Through extensive experiments, we answer the following questions:

* Is our proposed framework effective and efficient in visual environments?
* Is our offline pretrained discriminator better than the ones which are trained online?
* Does TDR make a difference to visual representations? And do we need to add 'progress' rewards, as is done in ELE?

### Atari

**Atari Datasets.** Atari is a well-established benchmark for visual control tasks and also a popular testbed for evaluating the performance of various LFO algorithms. We conduct experiments on four Atari games: Breakout, Freeway, Qbert, and Space Invaders. To ensure the quality of expert datasets, two approaches are utilized to collect expert observations. For Qbert and SpaceInvaders, we collect the last \(10^{5}\) transitions (around \(50\) trajectories) from Google Dopamine  DQN replay experiences. For Breakout and Freeway, we find the quality of transitions from Dopamine is far from expert. Therefore, we alternatively train a SAC agent  from scratch for \(5 10^{6}\) steps and leverage the trained policy to gather approximately \(50\) trajectories (around \(10^{5}\) transitions) in each game to construct the expert dataset.

**Performance in Atari Games.** As illustrated in Figure 3, STG is on par with BCO in Freeway but outperforms all baselines in the rest tasks. Table 1 displays the final scores of all methods along with expert datasets and PPO learned from environmental rewards. Remarkably, in Breakoutand Qbert, the performance of STG surpasses offline datasets. In Breakout, we observe that STG learns to consecutively bounce the ball up into the top gaps to hit the upper bricks to obtain high intrinsic rewards, while online adversarial methods and ELE fail. In Qbert, STG achieves a prominent breakthrough in the later stages, substantially outperforming other baselines.

**Analysis.** During the training process, we observe that online adversarial methods tend to easily get stuck in a suboptimal policy and struggle to explore a better policy. This is because the discriminator can easily distinguish between the visual behavior of the expert and the imitator based on relatively insignificant factors within just a few online interactions. In contrast, STG learns better temporally-aligned representations in an offline manner, enabling the discriminator to detect more substantial differences. Besides, instead of relying on probability, STG employs the Wasserstein distance metric to provide more nuanced and extensive reward signals. Consequently, even without fine-tuning during the online RL process, STG can offer valuable guidance to the RL agent. Additionally, in Breakout and Freeway we observe that ELE drops in final performance primarily due to the over-optimistic progress, which will be further investigated in Section 4.3. In comparison, STG ensures precise expert transition prediction and discriminative transition judgment, avoiding over-optimistically driving the agent to transfer to new states.

### Minecraft

**Brief Introduction.** Built upon Minecraft, Minedojo  provides a simulation platform with thousands of diverse open-ended tasks, which propose new challenges for goal completion. Furthermore, the 3D egocentric view and intricate scenes make it extremely hard to extract task-relevant visual signals for LfVO. We evaluate STG on four Minecraft tasks including "pick a flower", "milk a cow",

  
**Environment** & **GAIIO** & **AMP** & **IDDM** & **ELE** & **BCO** & **STG** & **Expert** & **PPO** \\  Breakout & 1.5 & 0.6 & 1.2 & 22.0 & 180.4 & **288.8** & 212.5 & 274.8 \\ Freeway & 0.6 & 3.0 & 10.5 & 2.7 & 21.6 & 21.8 & 31.9 & 32.5 \\ Qbert & 394.4 & 874.9 & 423.3 & 4698.6 & 234.1 & **27234.1** & 15620.7 & 14293.3 \\ Space Invaders & 260.2 & 268.1 & 290.4 & 384.6 & 402.2 & 502.1 & 1093.9 & 942.5 \\   

Table 1: Mean final scores of last 100 episodes in Atari games. The last two columns display the average episodic scores of expert datasets and PPO with environmental rewards reported in .

Figure 4: Average success rates of STG, ELE, and BCO in Minecraft tasks. STG shows superiority over the two baselines in challenging open-ended tasks.

Figure 3: STG is compared with BCO, GAÃ­O, AMP, IDDM, and ELE in four Atari games. The learning curves demonstrate that STG combines the advantage of adversarial learning and the benefit of representation learning, showing substantially better performance in four Atari games.

"harvest tallgrass", and "gather wool". All tasks are sparse-reward, where only a binary reward is emitted at the end of the episode, thus the performance is measured by success rates.

**Minecraft Datasets.** Recently, various algorithms, _e.g._, Plan4MC  and CLIP4MC  have been proposed for Minecraft tasks. To create expert datasets, for each task, we utilize the learned policies of these two algorithms to collect 100 trajectories (around \(5 10^{4}\) observations).

**Performance in Minecraft.** The results on Atari show that online adversarial methods can hardly tackle LfVO tasks well. Therefore, in Minecraft, we focus on comparing BCO, ELE, and STG. As depicted in Figure 4, the success rates across four Minecraft tasks reveal a consistent superiority of STG over ELE. In terms of IDM-based BCO, albeit equipped with an additional online-learned inverse dynamic module, it only shows a negligible advantage over STG in "gather wool" and fails to achieve satisfactory performance in the rest three tasks, and even suffers from a severe performance drop in "harvest tallgras".

Notably, we observe that in "milk a cow", STG attains a success rate approaching 25%, significantly eclipsing the 5% success rate of ELE or BCO. The reasons for this stark contrast are not yet entirely elucidated. However, a plausible conjecture could be attributed to the task's primary objective, _i.e._ locating the cow. STG, which is proficient in memorizing high-quality transitions, can effectively accomplish this subgoal, while ELE may lose the intended viewpoint due to its tendency for over-optimistic progress estimations and BCO can easily get stuck in local online sub-optimal transitions.

### Ablations

**TDR Ablation.** Since STG outperforms other baselines in Atari and Minecraft domain, it raises a question regarding the contribution of the designed temporal distance regressor. To answer this question, we conduct an ablation study, denoted as **STG-,** where we remove the TDR loss \(dtr\) from STG to figure out how the TDR module contributes to enhancing performance and representation. Consequently, the feature encoder \(E_{}\) and the STG Transformer \(T_{}\) are pretrained under the guidance of a linear combination of \(_{mse}\) and \(_{adv}\). We keep the other settings fixed and leverage the pretrained model to tackle the respective downstream tasks. The results are shown in Figure 5, where STG is substantially superior to STG- in most tasks.

Figure 5: The removal of the TDR loss from STG, denoted as STG-, induces a decline in performance and sample efficiency in Atari and Minecraft.

Figure 6: T-SNE visualization of a sampled Qbert trajectory embeddings.

In order to figure out the underlying reasons for their discrepancy in performance, we compare the visualization of embeddings encoded by STG and STG-. We randomly select an expert trajectory from Qbert and utilize t-SNE projection to visualize their embedding sequences. As illustrated in Figure 6, the embeddings learned by STG exhibit remarkable continuity in adjacent states, significantly different from the scattered and disjoint embeddings produced by STG-. The superior temporal alignment of the STG representation plays a critical role in capturing latent transition patterns, thereby providing instructive information for downstream RL tasks. For more multi-trajectory visualizations, see Appendix G.

**Multi-Task STG.** In the previous experiments, the demonstrations used for pretraining come from agents solving the same tasks, it raises a question on how well do STG generalize to a variety of downstream tasks. To this end, we pretrain a new instance of the STG Transformer named **STG-Multi**, with the same network architecture, on the whole Atari datasets encompassing all four downstream tasks to further assess the efficacy of multi-task adaptation. Considering the four times increase in datasets, we enlarge the model capacity by about four times via stacking more multi-head causal self-attention modules. More details are available in Appendix E. As shown in Figure 7, STG-Multi shows comparable performance across four Atari games. These results strongly reveal the potential of pretraining STG on multi-task datasets for guiding downstream tasks.

**Pretraining Loss Design.** During pre-training, STG concurrently minimizes \(_{tot}=_{mse}+_{adv}+ _{tot}\). We set \(=0\) to investigate the contribution of \(_{mse}\), denoted as **STG(rm MSE)**, and \(=0\) to investigate the contribution of WGAN, denoted as **STG(rm Adv)**. For STG(rm Adv), we can only use prediction error as intrinsic rewards like what has been done in . The final performance of STG(rm MSE) and STG(rm Adv) in SpaceInvaders are listed in Figure 7(a) and 7(b). The sample efficiency slightly drops without \(L_{2}\) penalty while the final performance declines heavily without WGAN. Thus, we can conclude each item in \(_{tot}\) makes an indispensable contribution to the final performance.

It is worth noting that we arbitrarily choose \(==0.5\) and \(=0.1\) in primary experiments according to the value scales of these losses. We discover that these coefficients have a slight influence on training stability and final performance so we do not bother to tune these parameters and leave them consistent across all tasks.

**Datasets.** Given that the reward guidance thoroughly originates from offline dataset, undoubtly the capacity and the quality of the pretraining dataset have a non-negligible impact on performance of LFVO tasks. We first conduct extra ablation studies on reduction of dataset size. 50,000 transitions, i.e. half of the original dataset are randomly sampled to train **STG(Half-Data)** in SpaceInvaders with other settings unchanged. The outcomes of this experiment are presented in Figure 7(c). The ablation clearly demonstrates that an increased number of demonstrations contributes to enhanced performance. This observation aligns seamlessly with prior investigations, as illustrated in Figure 4 of  and Figure 3 of .

Furthermore, we ablate the quality of dataset in Breakout. We manually select 50 trajectories with an episodic return greater than 250 in Breakout to construct expert datasets to train **STG(Expert)** to solve the Breakout game. Figure 7(d) shows that higher-quality offline datasets contribute to better sample efficiency. Meanwhile, it also reflects that STG can achieve excellent performance learning from sub-optimal observations.

**Progression Reward.** Considering ELE follows the same scenario of pretraining to enhance downstream tasks as STG, we conduct experiments to figure out whether we can additionally add progres

Figure 7: Multi-task STG (STG-Multi) is pretrained on the whole Atari datasets to guide RL training in downstream tasks.

sion rewards derived from TDR like ELE to further the performance. To this end, we train the agent under the guidance of both the discriminative and progression rewards from the same pretrained STG Transformer in Atari tasks, denoted as **STG***. As illustrated in Figure 9, STG outperforms STG* in all tasks. We analyze that, similar to ELE, progression rewards from TDR over-optimistically urge the agent to "keep moving" to advance task progress, which however can negatively impact policy learning. For example, on certain conditions such as Breakout or Freeway, maintaining a stationary position may facilitate catching the ball or avoiding collision more easily, thereby yielding higher returns in the long run. Therefore, we do not include the over-optimistic progression rewards in our design.

In summary, our experimental results provide strong evidence for the ability of STG to learn from visual observation, substantially outperforming baselines in a variety of tasks. The ablation study highlights the importance of the TDR module for temporally aligned representations. However, TDR may not be used to generate progression rewards that drive over-optimistic behaviors.

## 5 Conclusion and Future Work

In this paper, we introduce the State-To-Go (STG) Transformer, offline pretrained to predict latent state transitions in an adversarial way, for learning from visual observation to boost downstream reinforcement learning tasks. Our STG, tested across diverse Atari and Minecraft tasks, demonstrates superior robustness, sample efficiency, and performance compared to baseline approaches. We are optimistic that STG offers an effective solution in situations with plentiful video demonstrations, limited environment interactions, and where labeling action is expensive or infeasible.

In future work, STG is likely to benefit from more powerful large-scale vision foundation models to facilitate generalization across a broader range of related tasks. Besides, it can extend to a hierarchical framework where one-step predicted rewards can be employed for training low-level policies and multi-step rewards for the high-level policy, which is expected to improve performance and solve long-horizon tasks.