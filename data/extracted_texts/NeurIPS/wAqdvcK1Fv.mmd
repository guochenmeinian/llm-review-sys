# Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces

Tobias Schroder

Imperial College London

Equal contributions

&Zijing Ou

Imperial College London

&Yingzhen Li

Imperial College London

&Andrew Duncan

Imperial College London

The Alan Turing Institute

###### Abstract

Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on data in discrete or mixed state spaces poses significant challenges due to the lack of robust and fast sampling methods. In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo. We introduce perturbations of the data distribution by simulating a diffusion process on the discrete state space endowed with a graph structure. This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including the estimation of discrete densities with non-binary vocabulary and binary image modelling. Finally, we train EBMs on tabular data sets with applications in synthetic data generation and calibrated classification.

## 1 Introduction

Discrete structures are intrinsic to most types of data such as text, graphs, and images. Estimating the data generating distribution \(p_{}\) of discrete data sets with a probabilistic model can contribute greatly to downstream inference and generation tasks, and plays a key role in synthetic data generation of tabular, textual or network data (Raghunathan, 2021). Energy-based models (EBMs) are probabilistic generative models of the form \(p_{}(-U)\), where the flexible choice of the energy function \(U\) allows great control in the modelling of different data structures. However, energy-based models are, by definition, unnormalised models and notoriously difficult to train due to the intractability of their normalisation, especially in discrete spaces.

Energy-based models are typically trained with the contrastive divergence (CD) algorithm (Hinton, 2002) which performs approximate maximum likelihood estimation by approximating the gradient of the log-likelihood with Markov Chain Monte Carlo (MCMC) techniques. This method motivated rich research results on sampling from discrete distributions to enable fast and accurate estimation of energy-based models (Zanella, 2020; Grathwohl et al., 2021; Zhang et al., 2022; Sun et al., 2022; Sun et al., 2022; Wu et al., 2022; Wu et al., 2022). However, the training of energy-based models with CD remains challenging as it relies on

[MISSING_PAGE_FAIL:2]

_The_ energy discrepancy _between_\(p_{ data}\) _and_\(U\) _induced by_\(q\) _is defined as_

\[_{q}(p_{ data},U):=_{p_{ data}()}[U()]-_{p_{ data}()}_{q(|)}[U_{q}( )].\] (4)

We will refer to \(q\) as the _perturbation_. The validity of this loss functional was proven in Schroder et al. (2023) in large generality: In particular, it is sufficient for \(U^{*}=*{argmin}_{q}(p_{ data},U) (-U^{*}) p_{ data}\) that any two points \(,\) are \(q\)-equivalent, i.e. there exists a chain of states \((^{i})_{i=1}^{j}\) with \(^{1}=,^{T}=\) such that \(q(^{i+1}|^{i})>0\) for all \(i=1,,T-1\).

Energy discrepancy can also be understood from seeing it as a type of Kullback-Leibler divergence. Specifically, the loss function defined in (4) is equivalent to the expected Kullback-Leibler divergence

\[*{argmin}_{U}_{q}(p_{ data},U) *{argmin}_{U}_{p_{ data}()}_{q( |)}[(p_{ data}(|),p_{  ebm}(|))]\] (5)

where \(p_{}(|) p_{}()q(|)\). This relates energy discrepancy to diffusion recovery likelihood objectives (Gao et al., 2021) and Kullback-Leibler contractions (Lyu, 2011). Energy discrepancy has demonstrated notable effectiveness in training EBMs in continuous spaces (Schroder et al., 2023). In the next section, we show how the loss can be defined in discrete spaces.

## 3 Energy Discrepancies for Discrete Data

For this work we will first consider a state space for the data distribution that can be written as the product of \(d\) discrete variables with \(S_{k}\) classes each, i.e. \(=_{k=1}^{d}\{1,,S_{k}\}\). Examples for spaces of this type are the categorical entries of a data table for which \(d\) denotes the number of features, or binary image data sets for which we typically write \(=\{0,1\}^{d}\). To define energy discrepancy on such spaces we need to specify a perturbation process under the following considerations: 1) The negative samples obtained through \(q\) are informative for training the EBM when only finite amounts of data are available. 2) The contrastive potential \(U_{q}()\) has a numerically tractable approximation.

Let us consider one component \(=\{1,,S\}\), only. Inspired from previous works on diffusion modelling for discrete data (Campbell et al., 2022; Sun et al., 2023; Lou et al., 2024; Campbell et al., 2024) we model the perturbation as a continuous time Markov chain (CTMC) with transition probability

\[q_{t}(y=b|x=a)=(tR)_{ba}\,, a,b\{1,2,,S\}\]

where \(R^{S S}\) is the so-called rate matrix which satisfies \(R_{bb}=-_{a b}^{S}R_{ba}\) and \((tR)\) is the matrix exponential. For a given rate matrix \(R\), this approach then leaves us with a single tunable hyperparameter \(t\) characterising the magnitude of perturbation applied. We first analyse how the choice of rate matrix and time parameter affect the statistical properties of the energy discrepancy loss. In fact, under weak conditions, the energy discrepancy loss converges to maximum likelihood estimation for \(t\), thus achieving the same loss function implemented by contrastive divergence:

**Theorem 1**.: _Let \(q_{t}(|x)\) be a Markov transition density defined by the rate matrix \(R\) with eigenvalues \(0=_{1}(R)_{2}(R)_{S}(R)\) and uniform stationary distribution. Then, there exists a constant \(z_{t}\) independent of \(\) such that energy-discrepancy converges to the maximum-likelihood loss_

\[|_{q_{t}}(p_{ data},U_{})-_{ MLE}()-z_ {t}|(-|_{2}(R)|t)\,(p_{ data} p_{ })\,.\]

_with the loss of maximum-likelihood estimation \(_{ MLE}():=-_{p_{ data}()}  p_{}()\)._

Here, \(z_{t}\) is a constant independent of \(\), so the optimisation landscapes of energy discrepancy estimation and maximum likelihood estimation in \(\) align at an exponential rate, except for a shift by \(z_{t}\) which does not affect the optimisation. This result improves the linear convergence rate in Schroder et al. (2023) and relates it to the _spectral gap_\(|_{2}(R)|\) of the rate matrix. Such a result is meaningful as the maximum-likelihood estimator is generally statistically preferable with better sample efficiency, and Theorem 1 suggests that energy discrepancy estimation can approximate maximum likelihood estimation without resorting to MCMC like in classical EBM training methods. The proof is given in Appendix A.1.

### Heat Equation in Structured Discrete Spaces

In principle, Theorem 1 establishes that energy discrepancy converges to the loss of maximum likelihood estimation in the limit \(t\) for any choice of rate matrix with spectral gap. In practice, however, large perturbations of data can produce high-variance parameter gradients and provide little training signal. Instead, it is desirable to construct perturbations that allow a fine-grained trade-off between the statistical properties of the loss function and the variance of the gradients. For this reason, we investigate the perturbation for small \(t\) which, as we will see, can be informed by the assumed _graph structure_ of the underlying discrete space.

The infinitesimal perturbations of the CTMC are characterised by the rate matrix. Notice that for small \(t\), the Euler discretisation of the heat equation yields

\[q_{t}(y=b|x=a)(b,a)+tR_{ba}\,.\]

with \((b,a)=1 a=b\) and zero otherwise. To model the relationship between values \(a,b\{1,,S\}\) we endow the space with a graph structure with adjacency matrix \(A\) and out degree matrix \(D_{}\) and model the rate matrix as the graph Laplacian \(R:=(A-D_{})\). By definition, the columns of the graph Laplacian matrix sum to zero \(_{b=1}^{S}R_{ba}=0\). The smallest possible perturbation is then characterised as the transition to an adjacent neighbour. The characterisation of the CTMC in terms of the graph Laplacian is implicitly assumed in previous work. Campbell et al. (2022) describe a diffusion via a uniform perturbation which corresponds to a fully connected graph and Lou et al. (2024) describe the rate matrix associated to a star graph with absorbing (masking) state \(\):

\[R_{ba}^{}=1-S\,(b,a)\,, R_{ba}^{}= (b,)-(b,a)\,.\]

For a visualisation, see Section 3.1. In addition to these fairly unstructured rate matrices we model state spaces with a cyclical or ordinal structure:

\[R^{}=-2&1&0&&0&1\\ 1&-2&1&0&&0\\ 0&1&-2&1&0&\\ &&&&&\\ 0&&0&1&-2&1\\ 1&0&&0&1&-2 R^{}=-1&1&0& &0&0\\ 1&-2&1&0&&0\\ 0&1&-2&1&0&\\ &&&&&\\ 0&&0&1&-2&1\\ 0&0&&0&1&-1.\]

We restrict ourselves to uniform, cyclical, and ordinal structures as these structures can typically be trivially inferred from the type of data modelled. For example, periodically changing quantities (e.g. seasons) would display a cyclical structure and ordered information like age an ordinal structure. It is possible, however, to extend our framework to arbitrary graphical structures of the state space as long as eigenvalue decompositions of the graph Laplacian are feasible.

Since the Gaussian perturbation on Euclidean space used in Schroder et al. (2023) is also the solution of a heat equation, these choices allow us to model the perturbation on a vector of mixed entries including numerical values, unstructured categorical values, and structured categorical values with a single differential equation

\[_{t}q_{t}(|)=(^{} R^{ } R^{} R^{} R^{ })q_{t}(|)\,, q_{0}(|)= (,)\] (6)

where \(^{}\) denotes the standard Laplace operator \(_{k=1}^{d}^{2}/_{}^{2}\) and the product \(\) denotes that each operator acts on the corresponding component of the state space.

Figure 1: Visualisation of a typical state space of a tabular dataset: Numerical entries taking values in \(^{d}\), cyclical categorical entries (e.g. season), ordinal categorical entries (e.g. age), unstructured categorical entries, and variables with an absorbing state associated with masking the entry.

Estimating the Energy Discrepancy Loss

We now discuss how discrete energy discrepancy can be estimated. We will typically assume that each dimension of the data point is perturbed independently, i.e. the perturbation \(q(y|)\) is modelled as the product of component-wise perturbations. On Euclidean data, we resort to the implementation in Schroder et al. (2023) and obtain perturbed samples by adding isotropic Gaussian noise to the samples. We are now left with the heat equation on discrete space.

### Solving the Heat Equation

In the case of the uniform Laplacian \(R_{}=_{S}_{S}^{T}-S\), with \(_{S}\) denoting an all ones vector of length \(S\), the heat equation has the closed form solution

\[q_{t}(y|x=a)=e^{-St}(y,a)+}{S}_{b=1}^{S}(y,b)\,.\] (7)

Practically speaking, this perturbation remains in its state with probability \(e^{-St}\) and samples uniformly from the state space otherwise. The case of the cyclical and ordinal structure is more delicate. We first note that the heat equation can be solved in terms of its eigenvalue expansion \((Rt)=( t)^{*}\), where \(\) is the matrix with the eigenvalues \(_{p}\) along its diagonal and \(\) is a matrix of orthogonal eigenvectors with each column containing the corresponding eigenvector \(_{p}\). The perturbation for \(R^{}\) and \(R^{}\) can then be computed by means of a discrete Fourier transform:

**Proposition 1**.: _Assume the density \(q_{t}(b|a):=q_{t}(y=b|x=a)\) is defined by the rate matrices \(R^{}\) or \(R^{}\). The transition density for all \(a,b\{1,2,,S\}\) is given by_

\[q_{t}^{}(b|a) =_{p=1}^{S}(2b_{p}^{ })\,((2(2_{p}^{})-2)t)\, (-2a_{p}^{})\] (8) \[q_{t}^{}(b|a) =_{p=1}^{S}}\,((2b-1)_ {p}^{})\,((2(2_{p}^{})-2)t) \,((2a-1)_{p}^{})\] (9)

_where \(_{p}^{}=(p-1)/S\) and \(_{p}^{}=(p-1)/2S\), respectively, and \(z_{p}=(2,1,,1)\)._

For the derivation, see Appendix A.2. Due to this result, the heat equation can be efficiently solved in parallel without requiring any sequential operations like multiple Euler steps. In addition, the transition matrices can be computed and saved in advance, thus reducing the computational complexity to the matrix multiplication with a batch of one-hot encoded data points.

Gaussian limit and choice of time parameter.For tabular data sets the cardinality \(S\) changes between different dimensions which raises the question how \(t\) should be scaled with \(S\). To answer this question we observe the following scaling limit of the perturbation:

**Theorem 2** (Scaling limit).: _Let \(y_{t} q_{t}(|x= S)\) with \(\{1/S,2/S,,1\}\), where \(q_{t}\) is either the transition density of the cyclical or ordinal perturbation. Let \(:(0,1]\), where for all \(n\) and \(x(0,1]\)\(^{}(n+x)=x\) and \(^{}(2n+x)=x\), \(^{}(2n+1+x)=-x\). Then,_

\[y_{S^{2}t}/S(_{t}) _{t}(,2t)\,.\]

Consequently, under the rescaling of time and space prescribed, the perturbation behaves independently of the state space size like a Gaussian with variance \(2t\) that is reflected or periodically continued at the boundary states of \((0,1]\). The phenomenon is visualised in Figure 5. Based on this scaling limit we typically choose a quadratic rule \(t=S^{2}t_{}\). Alternatively, we may choose a linear rule \(t=St_{}\) in which case the limit becomes a regular Gaussian on \(_{+}\), thus recovering the Euclidean case from Schroder et al. (2023). The theorem is proven in Appendix A.3.

As a byproduct of this result we can also approximate the perturbation with discretised rescaled samples from a standard normal distribution and applying either periodic or reflecting mappings on perturbed states outside the domain. This may be computationally favourable for spaces of the form \(\{1,,S\}^{d}\) where the vocabulary size \(S\) and dimension of the state space \(d\) grow very large.

Localisation to random grid.For unstructured categorical variables the uniform perturbation may introduce too much noise to inform the EBM about the correlations in the data set. In these cases, it can be beneficial to sample a random dimension \(k\{1,,d\}\) and apply a larger perturbation in this dimension, only. This effectively means to replace the product of categorical distributions with a mixture perturbation

\[q_{t}(|)=_{k=1}^{d}q_{t}(y_{k}|x_{k}) _{k=1}^{d}q_{t}(y_{k}|x_{k})\,.\]

In our experiments we only consider the case of perturbing the randomly chosen dimension uniformly. We call this grid perturbation due to connections with concrete score matching (Meng et al., 2022). The resulting loss can be understood as a variation of pseudo-likelihood estimation.

Special case of binary state space.In the special case of \(=\{0,1\}^{d}\), the structures of the cyclical, ordinal, and uniform graph coincide, and the perturbation \(q_{t}(|)\) becomes the product of identical Bernoulli distributions with parameter \(=0.5(1-^{-2t})\). We also explore the grid perturbation which assumes that a dimension is selected at random and the entry is flipped deterministically from zero to one or one to zero. For details, see Appendix B.1.

### Estimation of the Contrastive Potential

The final challenge in turning energy discrepancy into a practical loss function lies in the estimation of the contrastive potential \(U_{q}\). We use the fact that for a symmetric rate matrix \(R\), the induced perturbation is symmetric as well, i.e. \(q_{t}(|)=q_{t}(|)\). Thus, we first write the contrastive potential as an expectation \(U_{q}()=-_{}(-U())q( |)=-_{q(|)}[(-U ())]\) and subsequently approximate the energy discrepancy loss as in Schroder et al. (2023) as \(_{q,M,w}(U):=_{i=1}^{N}(w+_{j=1}^{M} (U(^{i})-U(^{i,j}_{-}))\) with \(^{i} p_{}\), \(^{i} q_{t}(|^{i})\), and \(^{i,j}_{-} q_{t}(|^{i})\). The details of the training procedure are provided in Appendix C.

## 5 Related Work

**Contrastive loss functions.** Our work is based on energy discrepancies first introduced in (Schroder et al., 2023). Energy discrepancies are equivalent to certain types of KL contraction divergences whose theory was studied in Lyu (2011), however, without proposing a training algorithm for EBM's. On Euclidean data, ED is related to diffusion recovery-likelihood (Gao et al., 2021) which uses a CD-type training algorithm. For a masking perturbation, ED estimation can be understood as a Monte-Carlo approximation of pseudo-likelihood (Besag, 1975). Furthermore, the structure of the stabilised energy discrepancy loss shares similarities with other contrastive losses such as Ceylan and Gutmann (2018); Gutmann and Hyvarinen (2010); Oord et al. (2018); Foster et al. (2020) due to their close connection to the Kullback-Leibler divergence.

**Discrete diffusion models.** We extend the continuous time Markov chain framework introduced and developed in Campbell et al. (2022, 2024); Lou et al. (2024) and provides a geometric interpretation thereof. Similar to us, Kotelnikov et al. (2023) defines a flow on mixed state spaces as the product of a Gaussian and a categorical flow, utilising multinomial flows (Hoogeboom et al., 2021). Our work has connections to concrete score matching (Meng et al., 2022) through the usage of neighbourhood structures to define a replacement of the continuous score function.

**Contrastive divergence and sampling.** Contrastive divergence (CD) is commonly utilised for training energy-based models in continuous spaces with Langevin dynamics (Xie et al., 2016, 2018, 2022; Du et al., 2021; Xiao et al., 2020). In discrete spaces, EBM training heavily relies on CD methods as well, which is a major driver for the development of discrete sampling strategies. The standard Gibbs method was improved by Zanella (2020) through locally informed proposals. This method was extended to include gradient information (Grathwohl et al., 2021) to drastically reduce the computational complexity of flipping bits in several places (Sun et al., 2022; Emami et al., 2023; Sun et al., 2022). Moreover, a discrete version of Langevin sampling was introduced based on this idea (Zhang et al., 2022; Rhodes and Gutmann, 2022; Sun et al., 2023). Consequently, most current implementations of contrastive divergence use multiple steps of a gradient-based discrete sampler. Alternatively, EBMs can be trained using generative flow networks which learn a Markov chain that construct data optimising the energy as reward function (Zhang et al., 2022).

**Other training methods and applications of EBMs for discrete and mixed data.** A sampling-free approach for training binary discrete EBMs is ratio matching (Hyvarinen, 2007; Lyu, 2009; Liu et al., 2023). Dai et al. (2020) propose to apply variational approaches to train discrete EBMs instead of MCMC. Eikema et al. (2022) replace the widely-used Gibbs algorithms with quasi-rejection sampling to trade off the efficiency and accuracy of the sampling procedure. The perturb-and-map (Papandreou and Yuille, 2011) is also recently utilised to sample and learn in discrete EBMs (Lazaro-Gredilla et al., 2021). Tran et al. (2011) introduce mixed-variate restricted Boltzmann machines for energy-based modelling on mixed state spaces. Deep architectures, on the other hand, have been mostly limited to a single categorical target variable which is modelled via a classifier (Grathwohl et al., 2020). Moreover, Ou et al. (2022) apply discrete EBMs on set function learning, in which the discrete energy function is learned with approximate marginal inference (Domke, 2013).

## 6 Experiments

To evaluate our proposed approach, we conduct experiments across diverse scenarios: i) estimating probability distributions on discrete data; ii) handling mixed-state features in tabular data; and iii) modelling binary images. We also explore Ising model training and graph generation in binary spaces, but leave the detailed evaluation of these in Appendix D.

### Discrete Density Estimation

We first demonstrate the effectiveness of energy discrepancy on density estimation using synthetic discrete data. Following Dai et al. (2020), we initially generate 2D floating-point data from several two-dimensional distributions. Each dimension of the data is then converted into a \(16\)-bit Gray code, resulting in a dataset with \(32\) dimensions and \(2\) states. To construct datasets beyond binary cases, we follow Zhang et al. (2024) and transform each dimension into \(8\)-bit \(5\)-base code and \(6\)-bit decimal code. This process creates two additional datasets: one with \(16\) dimensions and \(5\) states, and another with \(12\) dimensions and \(10\) states. The experimental details are given in Appendix D.1.

Figure 2 illustrates the estimated energies as well as samples that are synthesised with Gibbs sampling for energy discrepancy (ED) and contrastive divergence (CD) on the dataset with \(16\) dimensions and \(5\) states. It can be seen that ED excels at capturing the multi-modal nature of the distribution, consistently learning sharper energy landscape in the data support compared to CD. This coincides with the previous observations in continuous spaces (Schroder et al., 2023), suggesting ED's advantage in handling complex data structures. For more results of additional datasets with \(5\) and \(10\) states, we deferred them to Figures 7 and 8, respectively.

For binary cases with \(2\) states, we compare our approaches to three baselines: PCD (Tieleman, 2008), ALOE+ (Dai et al., 2020), and EB-GFN (Zhang et al., 2022). In Tables 3 and 4, we quantitatively evaluate different methods by evaluating the negative log-likelihood (NLL) and the exponential Hamming MMD (Gretton et al., 2012), respectively. We observe that energy discrepancy outperforms the baseline methods in most settings, but without relying on MCMC simulations (as in PCD) or the training of additional variational networks (as in ALOE and EB-GFN). This performance gain is likely explained by the good theoretical guarantees of energy discrepancy for well-posed estimation tasks. In contrast, the baselines introduce biases due to their reliance on variational proposals and short-run MCMC sampling that may not have converged.

Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with \(16\) dimensions and \(5\) states. Rows \(1\) and \(2\) show the estimated density and synthesised samples, respectively.

### Tabular Data Synthesising

In this experiment, we assess our methods on synthesising tabular data, which presents a challenge due to its mix of numerical and categorical features, making it more difficult to model compared to conventional data formats. To demonstrate the efficacy of energy discrepancies, we first conduct experiments on synthetic examples before proceeding to real-world tabular data. Additional details regarding the experimental setup are deferred to Appendix D.2.

Synthetic Dataset.We first showcase the effectiveness of our methods on mixed data types by learning EBMs on a synthetic ring dataset. The dataset consists of four columns, with the first two columns indicating numerical coordinates of data points. The third column categorizes data points into four circles whereas the last column specifies the 16 colours each data point could be classified into. Therefore, each row in the tabular contains \(2\) numerical features and \(2\) categorical features.

To train an EBM on a dataset comprising mixed types of data, we employ either contrastive divergence or energy discrepancy. For CD, we adopt a strategy involving a replay buffer in conjunction with a short-run MCMC using \(20\) steps. Specifically, we utilise Langevin dynamics and Gibbs sampling for numerical and categorical features, respectively. In the case of ED, we perturb the numerical features with a Gaussian perturbation and the categorical features with grid perturbation. Figure 3 illustrates the results of synthesised samples generated from the learned energy using Gibbs sampling. These findings align with those depicted in Figure 2, where CD struggles to capture a faithful energy landscape, leading to synthesized samples potentially lying outside the data distribution support. Instead, by leveraging a combination of perturbation techniques tailored to the data types present, ED offers a more robust and reliable framework for training EBMs in mixed state spaces.

Real-world Dataset.We then evaluate our methods by benchmarking them against various baselines across \(6\) real-world datasets. Following Xu et al. (2019), we first split the real datasets into training and testing sets. The generative models are then learned on the real training set, from which synthetic samples of equal size are generated. This synthetic dataset is subsequently used to train a classification/regression XGBoost model, which is evaluated using the real test set.

We compare the performance, as measured by the AUC score for classification tasks and RMSE for regression tasks, against CTGAN, TVAE, (Xu et al., 2019) and TabDDPM (Kotelnikov et al., 2023) baselines which utilise generative adversarial networks, variational autoencoders, and denoising diffusion probabilistic models, respectively. The results are reported in Table 1. Here, TabED-Str refers to an ED loss for which the perturbation was chosen with prior knowledge about the structure of the modelled feature, i.e. ordinal and cyclical features were hand-picked. We do not report results for TabED-Str on the Cardio, Churn, and Mushroom datasets, since the state spaces only consist

    & **Adult** & **Bank** & **Cardio** & **Churn** & **Mushroom** & **Beijing** & **Avg. Rank** \\   & AUC \(\) & AUC \(\) & AUC \(\) & AUC \(\) & AUC \(\) & RMSE \(\) & – \\  Real & \(.927_{.000}\) & \(.935_{.002}\) & \(.834_{.001}\) & \(.819_{.001}\) & \(1.00_{.000}\) & \(.423_{.003}\) & – \\  CTGAN & \(.861_{.005}\) & \(.774_{.006}\) & \(.787_{.002}\) & \(.792_{.003}\) & \(.781_{.007}\) & \(1.01_{.038}\) & \(6.33\) \\ TVAE & \(.873_{.001}\) & \(.868_{.002}\) & \(.676_{.009}\) & \(.793_{.006}\) & \(.999_{.000}\) & \(1.05_{.012}\) & \(5.17\) \\ TabCD & \(.619_{.026}\) & \(.604_{.021}\) & \(.765_{.008}\) & \(.584_{.021}\) & \(.561_{.048}\) & \(1.06_{.037}\) & \(8.83\) \\ TabDDPM & \(.910_{.001}\) & \(.922_{.001}\) & \(.801_{.001}\) & \(.806_{.007}\) & \(.999_{.000}\) & \(.556_{.005}\) & \(1.5\) \\  TabED-Uni & \(.884_{.003}\) & \(.842_{.013}\) & \(.786_{.002}\) & \(.810_{.008}\) & \(.998_{.001}\) & \(1.04_{.013}\) & \(3.83\) \\ TabED-Grid & \(.833_{.003}\) & \(.831_{.004}\) & \(.791_{.001}\) & \(.803_{.007}\) & \(.985_{.005}\) & \(.978_{.015}\) & \(4.83\) \\ TabED-Cyc & \(.831_{.005}\) & \(.823_{.007}\) & \(.796_{.001}\) & \(.807_{.007}\) & \(.971_{.004}\) & \(1.01_{.024}\) & \(4.83\) \\ TabED-Ord & \(.853_{.005}\) & \(.845_{.004}\) & \(.792_{.002}\) & \(.806_{.004}\) & \(.926_{.010}\) & \(1.02_{.017}\) & \(4.83\) \\ TabED-Str & \(.879_{.004}\) & \(.819_{.001}\) & - & - & - & \(.978_{.012}\) & \(3.67\) \\   

Table 1: Results on real-world datasets.

Figure 3: Comparison of the energy discrepancy and contrastive divergence on the synthetic tabular datasets.

of unstructured features. To compute the average ranking we use the rank of TabED-Uni on these datasets since on unstructured features TabED-Uni and TabED-Str coincide.

The variants of ED show promising results on diverse datasets, thus demonstrating the suitability of ED for EBM training on mixed-state spaces. While TabDDPM outperforms the other approaches, TabED shows comparable performance to the CTGAN and TVAE baselines and outperforms both in average ranking. Furthermore, the contrastive divergence approach performs poorly which highlights its limitations in accurately modelling distributions on mixed state spaces. Surprisingly, the unstructured perturbation TabED-Uni performs slightly better than the structured approaches. This may partially be attributed to the fact that the state spaces of the discrete features are relatively small. Consequently, the uniform perturbation might be a good approximation of maximum likelihood estimation in agreement with Theorem 1, while not producing high-variance gradients on these specific datasets.

Improving Calibration.Despite the improving accuracy of neural-network-based classifiers in recent years, they are also becoming increasingly recognised for their tendency to exhibit poor calibration due to overconfident outputs Guo et al. (2017); Mukhoti et al. (2020). Since energy-based model on mixed state spaces can capture the likelihood of tuples of features and target labels, they implicitely quantify the confidence in a prediction and can be adapted into classifiers with better calibration than deterministic methods. This opens up a new avenue for applying EBMs in deterministic tabular data modelling methods.

Let \(y\) and \(\) be the target label and the rest features in the tabular data, an EBM \(U_{}(,y)\) learned on the joint probability \(p_{}(,y)\) can be transformed into a deterministic classifier: \(p_{}(y|)(-U_{}(,y))\). As a baseline for comparison, we additionally train a classifier \(p_{}(y|)\) with the same architecture by maximising the conditional likelihood: \(_{p_{}}[ p_{}(y|)]\). Results on the adult dataset can be seen in Figure 4. We find that the EBM and the baseline exhibit comparable accuracy. However, the baseline model is less calibrated, generating over-confident predictions. In contrast, the EBM learned through ED achieves better calibration, as evidenced by lower expected calibration error Guo et al. (2017). Further details and results are provided in Appendix D.2.

### Discrete Image Modelling

In this experiment, we evaluate our methods on high-dimensional binary spaces. Following the settings in Grathwohl et al. (2021), we conduct experiments on various image datasets and compare against contrastive divergence using various sampling methods, namely vanilla Gibbs sampling, Gibbs-With-Gradient Grathwohl et al. (2021), GWG), Generative-Flow-Network Zhang et al. (2022a), GFN), and Discrete-Unadjusted-Langevin-Algorithm Zhang et al. (2022b), DULA). The training details are provided in Appendix D.3. After training, annealed importance sampling Neal (2001) is employed to estimate the negative log-likelihood (NLL).

Table 2 displays the NLLs on the test dataset. It is evident that energy discrepancy achieves comparable performance to the baseline methods on the Omniglot dataset. Despite the performance gap compared to the contrastive divergence methods on the MNIST dataset, energy discrepancy stands out for its efficiency, requiring only \(M\) evaluations of the energy function in parallel (see

   Dataset  Method & Gibbs & GWG & EB-GFN & DULA & ED-Bern & ED-Grid \\  Static MNIST & \(117.17\) & **80.01** & \(102.43\) & \(80.71\) & \(96.11\) & \(90.61\) \\ Dynamic MNIST & \(121.19\) & **80.51** & \(105.75\) & \(81.29\) & \(97.12\) & \(90.19\) \\ Omniglot & \(142.06\) & \(94.72\) & \(112.59\) & \(145.68\) & \(97.57\) & \(\) \\   

Table 2: Experimental results for discrete image modelling. We report the negative log-likelihood (NLL) on the test set for different models. The results of Gibbs, GWG, and DULA are taken from Zhang et al. (2022b), and the result of EB-GFN is from Zhang et al. (2022a).

Figure 4: Calibration results comparison between the baseline (left) and energy discrepancy (right) on the adult dataset.

Table 7 for the comparison of running time complexity). This represents a significant computational reduction compared to contrastive divergence, which lacks the advantage of parallelisation and involves simulating multiple MCMC steps. Additionally, our methods show superiority over CD-1 by a substantial margin, as demonstrated in Table 8, affirming the effectiveness of our approach. For further insights, we provide visualisations of the generated samples in Figure 10.

## 7 Conclusion and Limitations

In this paper we extend the training of energy-based models with energy discrepancy to discrete and mixed state spaces in a systematic way. We show that the energy-based model can be learned jointly on continuous and discrete variables and how prior assumptions about the geometry of the underlying discrete space can be utilised in the construction of the loss. Our method achieves promising results on a wide range of discrete modelling applications at a significantly lower computational cost than MCMC-based approaches. To the best of our knowledge, our approach is also the first working training method for energy-based models on tabular data sets, unlocking a wide range of inference applications for tabular data sets beyond the scope of classical joint energy-based models.

**Limitations:** Similar to prior work on energy discrepancy in continuous spaces (Schroder et al., 2023), our training method is sensitive to the assumption that the data distribution is positive on the whole state space. While our method scales to high-dimensional datasets like binary image data, where the positiveness of the data distribution is assumed to be violated due to the manifold hypothesis, the large difference between intrinsic and ambient dimensionality poses challenges to our approach and may explain why energy discrepancy cannot match the performance of contrastive divergence with a large number of MCMC steps on binary image data.

**Broader Impact:** In principle, our method can be used for imputation and prediction in tabular data sets and can thus have discriminating or excluding effects if used irresponsibly.

**Outlook:** For future work, we are interested in extensions to highly structured types of data such as molecules, text, or data arising from networks. So far, our work only considers cyclical and ordinal structures on the discrete space, while incorporating more complex structures as prior information into the rate function may be beneficial. Furthermore, interesting downstream applications ranging from table imputation with confidence bounds, simulation-based inference involving discrete variables, or reweighting of language models with residual EBMs have been left unexplored in this work.

#### Author Contributions

TS and ZO conceived the project idea to use an ED loss for the training of EBMs on discrete and mixed data. TS devised the main conceptual ideas, developed the theory, conducted the proofs and implemented the ED loss. ZO contributed to the conceptual ideas and designed and carried out the experiments. ABD supervised the conceptualisation and execution of the research project and contributed proof ideas. ZO, YL, and ABD checked derivations and proofs. TS and ZO equally contributed to the writing under the supervision of YL and ABD.

#### Acknowledgements

TS was supported by the EPSRC-DTP scholarship partially funded by the Department of Mathematics, Imperial College London. ZO was supported by the Lee Family Scholarship. We thank the anonymous reviewer for their comments.