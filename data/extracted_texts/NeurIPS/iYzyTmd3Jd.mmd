# CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics

Jiawei Gao 1,2, Ziqin Wang 1,3, Zeqi Xiao1,4, Jingbo Wang1, Tai Wang1, Jinkun Cao5,

**Xiaolin Hu2, Si Liu13, Jifeng Dai2, Jiangmiao Pang**1

1Shanghai AI Laboratory, 2Tsinghua University, 3Beihang University,

4Nanyang Technological University, 5Carnegie Mellon University,

Equal contributions. Email: winstongu20@gmail.com, wzqin@buaa.edu.comCorresponding author. Email: pangjiangmiao@gmail.com

###### Abstract

Enabling humanoid robots to clean rooms has long been a pursued dream within humanoid research communities. However, many tasks require multi-humanoid collaboration, such as carrying large and heavy furniture together. Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios. In this paper, we introduce **Co**operative **H**uman-**O**pject **I**nteraction (**CoHOI**), a framework designed to tackle the challenge of multi-humanoid object transportation problem through a two-phase learning paradigm: individual skill learning and subsequent policy transfer. First, a single humanoid character learns to interact with objects through imitation learning from human motion priors. Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms. When one agent interacts with the object, resulting in specific object dynamics changes, the other agents learn to respond appropriately, thereby achieving implicit communication and coordination between teammates. Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.

## 1 Introduction

Imagine the scenario where you're moving home and seeking help from humanoid robots. However, certain items, like beds and sofas, are too large and heavy for a single robot to manage effectively. Despite significant advancements in learning and control of physics-based human-object interactions [6; 43; 21; 37; 20; 45; 7] and humanoid robots performing complex operations [29; 19; 26; 2; 42; 10; 9; 39], the area of **multiple humanoids collaboratively transporting objects**--particularly when a single individual may find it challenging to handle heavy or long items--remains relatively under-investigated. Some previous efforts  tried to address these challenges using tracking-based methods, capturing interactions between two characters during tasks like box carrying, and training policies to mimic these actions. However, obtaining comprehensive motion data on interactions involving multiple participants is costly, and tracking-based methods struggle to adapt to scenarios with varying object sizes and an increased number of agents. Another straightforward idea might be to directly train a multi-agent policy from scratch, using an approachsimilar to that for single agents. However, the expanded action sampling space significantly slows down the overall training process, preventing the policy from converging properly.

Inspired by how humans learn cooperation skills, _i.e._ beginning with mastering tasks independently and then developing strategies for collaborative efforts in coordination tasks, we propose a two-stage framework for training multi-agent cooperation strategies, named **Co**operative **H**uman-**O**bject **I**nteraction, **CoHOI**. In the initial stage, we train a single-agent object carrying policy utilizing the AMP framework [25; 7]. To ensure agents focus significantly on the **dynamics** of the objects they are tasked with transporting, we include the state and velocity information of the object's bounding box in the agent's observation space. The second stage aims to transfer these individual carrying skills into collaborative strategies. When two humans collaborate to carry a long object, they typically hold it at opposite ends. Moreover, when one individual takes action, it affects the **dynamics** of the object, enabling the other individual to perceive this change and adjust their actions accordingly for effective cooperation. Therefore, we adjust the observation for both agents to focus on the bounding boxes located at the ends of the long objects. This refinement aligns with the observation space used during the single-agent training phase, effectively leveraging the previously developed single carrying skills. Additionally, the inherent rigid body characteristic of long objects facilitates implicit communication between agents. This setup allows an agent to adeptly adjust to their teammate's actions by observing changes in the object's **dynamics**, thereby enhancing coordination and cooperation in carrying tasks. The overall framework is illustrated in Figure 2.

To validate the effectiveness of our framework, we conducted experiments where we trained control policies for two humanoid characters to carry various long objects, such as boxes and sofas. Our results demonstrate that our framework enables these characters to exhibit natural-looking behaviors while successfully completing cooperative tasks, utilizing only motion capture data from one single agent. We compared our approach against the baseline method of training from scratch and performed detailed ablation studies to evaluate the impact of our design decisions, also testing the limitations of our framework. In summary, our main contributions are as follows: 1)We have developed an efficient and robust framework, **CoHOI**, for training physically simulated characters in cooperative object transporting tasks, demonstrating significant effectiveness. 2)We have established that utilizing object dynamics for communication proves to be an effective strategy in learning cooperative object transporting tasks.

Figure 1: Our framework empowers physically simulated characters to execute multi-agent human-object interaction (HOI) tasks with naturalness and precision.

Related Work

Physics-based Human-Object Interaction Motion Synthesis.Synthesizing natural and physically plausible human-scene interactions, such as humanoid characters sitting on chairs, lying on beds, and carrying boxes, is crucial for advancements in character animation and robotics. Physics-based methods leverage physics simulators [18; 35] to control characters modeled as interconnected rigid bodies through joint torques and deep reinforcement learning methods. To facilitate the training process, some strategies employ tracking-based techniques [16; 23; 1; 38; 20; 44], which rely on the availability of high-quality reference motions. This reliance often limits their application in human-scene interactions due to the scarcity of suitable data and affects their versatility across different scenarios. Recently, the Adversarial Motion Priors(AMP) framework  introduced the use of a discriminator to ensure that generated motions align with the distribution of reference motions. This approach has shown success in various downstream tasks [11; 24; 34], including human-scene interactions [7; 21; 37]. Despite these advancements, there has been limited focus on synthesizing cooperative behaviors among multiple characters interacting with objects--a gap that our work aims to address.

Multi-Character Control.While significant advancements have been made in synthesizing motions for single agents, the realm of multi-character animation remains relatively unexplored. Existing approaches predominantly rely on kinematic-based methods [36; 14; 32; 30; 5] and datasets of multi-character interactions [13; 31; 4]. These methods, however, require high-quality interaction data and often fall short in ensuring physical plausibility or adequately handling multi-character cooperative interactions with objects. Physics-based techniques for character animation have primarily focused on aspects like crowd navigation [8; 27], limiting themselves to behaviors such as pedestrian movement and collision avoidance. Although  showcases interactions among multiple characters and object manipulation, such as carrying long boxes, these tracking-based approaches necessitate motion capture data of human interactions or human-object interactions and struggle to scale to an arbitrary number of agents or different types of objects. In contrast, our framework requires only single agent motion capture data for multi-character object transporting tasks and can easily extend to different types of objects and different numbers of agents.

## 3 Methodology

Figure 2 shows the overall framework of our approach. Our method utilizes motion data from individual agents and involves a two-stage learning process to create cooperative control policies. First, we train a policy for single-agent carrying tasks, using the dynamics of the manipulated object

Figure 2: Our framework employs a two-phase learning paradigm. In the first phase, depicted on the left, we train single-agent carrying skills by imitating from human motion priors. In the second phase, we transfer these single-agent skills to a cooperative context. Notably, we use the dynamics of the object as feedback information, as illustrated by the bounding box shown in the figures.

as feedback, as described in Section 3.2. Then, we use parallel training to develop cooperative strategies, with changes in the object's dynamics acting as a form of implicit communication. This is further explained in Section 3.3.

### Preliminary

Physics-based Character Control.We formulate physics-based character control as a goal-conditioned reinforcement learning task. At each time step \(t\), the agent samples an action from its policy \((a_{t}_{t},_{t})\) based on the current state \(_{t}\) and the task-specific goal feature \(_{t}\). When this action is applied to the character, the environment transitions to the next state \(_{t+1}\), and the agent receives a task reward \(r^{G}(_{t},_{t},_{t+1})\). To train control policies that enable characters to achieve high-level tasks in a natural and life-like manner, we adopt the AMP framework . While this framework aims to optimize the expected cumulative task reward \(J()\), it introduces a discriminator to encourage the character to produce behaviors similar to those in the dataset by providing a style reward \(r^{S}(_{t},_{t+1})\). The agent's reward \(r_{t}\) at each time step \(t\) is defined by \(r_{t}=w^{G}r^{G}(_{t},_{t},_{t+1})+w ^{S}r^{S}(_{t},_{t+1})\). More details can be found in our appendix and the original AMP paper .

Multi-Agent Reinforcement Learning.We formulate our cooperative task as a Partially Observable Markov Decision Process (POMDP) . A POMDP with \(n\) agents is defined by \(\{,_{1},,_{n},_{1},, _{n},,,\}\), where \(\) is the state space, \(_{i}\) is the action space for agent \(i\), \(O_{i}\) is the local observation for agent \(i\), and \(\) is the shared reward function. Each agent uses its own policy \(_{}(_{i}_{i})\) to take action \(_{i}_{i}\) based on its local observation \(_{i} O_{i}\). The environment transitions according to the function \((_{t+1}_{t},_{1},, _{n})\), where \(_{t}\), \(_{t+1}\) are states of time step \(t\) and \(t+1\), respectively. The agents then get a reward \(_{t}\) based on the states \(_{t}\) and \(_{t+1}\). The goal of multi-agent reinforcement learning algorithms is to jointly optimize the discounted accumulated reward \(J()=_{_{t}^{},,_{t}^{},s^{}}[_{t=0}^{T}^{t}_{t}]\).

### Single Agent Carrying Skills Training

In developing our approach for single-agent object manipulation, we introduce several advancements based on previous methods. We integrate the dynamics of the manipulated object into the observation space and introduce a reward function framework for object manipulation tasks. These enhancements allow the trained policy to easily adapt to multi-agent settings.

#### 3.2.1 Enriched Goal Feature with Manipulated Object Dynamics as Feedback.

For successful object carrying tasks, we emphasize the critical role of using the dynamics of the manipulated object as feedback. These dynamics are captured through the eight vertices of the object \(o\)'s bounding box \(b_{t}^{}\), its rotation angle \(b_{t}^{}\), its velocity \(b_{t}^{v}\) and its angular velocity \(b_{t}^{w}\), as described in Equation (1).

\[_{t}=(b_{t}^{},b_{t}^{},b_ {t}^{v},b_{t}^{w}).\] (1)

By incorporating these dynamics information into the observation, we establish a feedback mechanism that keeps agents continually informed about the outcomes of their actions. This also equips agents with the capability to react appropriately, whether engaged in single-agent tasks or collaborative multi-agent environments. Along with the state of the agent \(s_{t}\) and the position of the target \(d_{t}^{}\), we formulate the dynamics-aware task observation as:

\[_{t}=(s_{t},d_{t}^{},_{t}).\] (2)

#### 3.2.2 Enriched Task Design facilitating Efficient Skill Transfer.

To facilitate the transition from single-agent to multi-agent object carrying tasks, we decompose the carrying process into three sub-tasks: walking towards the objects, lifting the object from the ground, and carrying the object to its intended destination. Consequently, the reward system is structured into three components: \(r_{}\), \(r_{}\)  and \(r_{}\).

To encourage the agents to choose the face of the object they will face while carrying, we introduce an additional goal feature called **stand points**\(x_{t}^{}\). During training, these stand points are randomly allocated to positions directly in front of the various faces of the object at time step \(t\). This strategy is designed to facilitate multi-agent cooperative training, helping the agents learn to avoid walking to the long side of the object where it is difficult to grasp and carry. We then define \(r_{}\) as the distance between the agent and the stand point, as specified in Equation (3).

\[r_{}=\|x_{t}^{}-x_{t}^{}\|^{2} \] (3)

Additionally, in scenarios where multiple agents are transporting an object, the object's considerable size often makes it difficult for agents to find an appropriate grip point for lifting. To address this, we introduce a novel concept called **held points**. Specifically, we select the geometric center \(h_{t}\) of each end of the object as the held point and encourage the agents to interact at these points. The reward function \(r_{}\), as defined in Equation (4), uses \(x_{t}^{}\) to represent the mean position of the agent's two hands. This design aids the transition from individual to collective task environments by encouraging agents to identify the held points at the geometric center of each end of the long object.

\[r_{}=\|x_{t}^{}-h_{t}\|^{2}\] (4)

We then define \(r_{}\) to encourage agents to carry the object to the destination:

\[r_{}=\|x_{t}^{}-x_{t}^{}\|^ {2}.\] (5)

### Cooperation Strategy Training

Upon mastering single-agent tasks involving object carrying, it is crucial to effectively transfer and further enhance these abilities to boost the cooperative learning process. Initially, we facilitate the skill transfer by guiding each agent to lift and transport one end of a large object. Subsequently, we refine the control policy through the application of the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm , leveraging the dynamics of manipulated objects for feedback and implicit communication.

#### 3.3.1 Efficient Skill Transfer Using Dynamics Information.

Initially, we replicate the single-agent object-carrying policy for all agents and then fine-tune it within a cooperative framework. However, handling a long object poses a challenge due to its size, which complicates the agents' ability to identify suitable lifting points, thereby hindering the efficient application of their object-carrying skills. To address this issue, we encourage agents to observe the dynamics information--specifically, the state and velocity data of the bounding box at each end of the long object, as outlined in Section 3.2.1. This approach allows agents to simulate carrying a smaller box positioned at the long object's ends, using the dynamics of this "smaller box" as **feedback**, similar to the single-agent skill training process. Additionally, we incorporate previous methodologies, such as stand points and held points described in Section 3.2.2, to enhance the smooth transition of skills.

Moreover, this design of dynamics as observation acts as **implicit communication channel** between agents. When an agent takes an action, the object dynamics will change, and because we use the manipulated object dynamics as feedback in single-agent training, the change in object dynamics will result in a corresponding change in strategy for the other agents. This method of implicit communication presents a straightforward yet effective approach for enhancing teamwork in multi-agent settings. Furthermore, this framework proves to be adaptable with variations in the number of participating agents.

#### 3.3.2 Cooperation Training using CTDE Scheme.

The non-stationary environment of multi-agent reinforcement learning, coupled with sample efficiency issues, presents a significant challenge in training agents to collaborate effectively. During the cooperation training phase, we continue to use a reward function similar to that used in the single-agent training phase and employ the centralized training and decentralized execution (CTDE)  scheme for coordination training. During the training phase, we utilize the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm  to develop cooperative strategies among agents. This approach involves updating the value function network \(V_{}\) according to Equation (6), using the trajectories \(\) that are accumulated and shared among all agents. More details can be found in our appendix and the original MAPPO paper .

\[_{k+1}=_{}_{k}|\,T}_{ _{k}}_{t=0}^{T}V_{}(o_{t},s_{t},}^{-})- _{t}^{2}\] (6)

In addition, given the homogeneous roles of agents in the collaborative task of carrying long objects, we adopt the strategy of parameter sharing, a method proven to enhance performance across various cooperative tasks [33; 3; 40]. Specifically, we share the parameters of both the policy and value networks among all agents to improve the training of cooperative behaviors.

## 4 Experiments

We conducted extensive experiments to test the effectiveness and also the boundary of capabilities of our framework. The basic experiment setups are explained in Section 4.1 and in Appendix. We evaluate our framework on various object-carrying tasks in Section 4.2. To better understand the importance of different design decisions in our framework, we performed extensive ablation studies in Section 4.3. Since our method primarily focuses on interactions between characters and objects, we also provide extensive visual analysis and presentations to demonstrate our framework.

### Experiments Setup

Datasets and Initialization.Our primary source of motion data is the AMASS dataset , which provides motions encoded in SMPL  parameters. We collected a total of four types of basic reference motion data, including 9 motions related to walking, 5 related to picking up, 4 related to carrying, and 5 related to putting down. To enhance the robustness of the carrying process, we randomly initialized the weight and size of the object, as well as the distance from the person to the destination. Specifically, for a single individual, the object's weight ranged from 5KG to 25KG, its size varied between 0.5 to 1.5 times its original scale, and the distances between the agent and the object, as well as between the object and the destination, ranged from 1 m to 20 m. To enhance the robustness of the carrying process, we randomly varied the weight, size, and of the object, as well as the distance from the person to the endpoint. Furthermore, considering that the process of multiple people carrying might involve situations where someone walks backward, we ensured that the single agent mastered the basic movements for multi-person collaboration by introducing additional motion data involving backward and side walking. More training details can be found in the appendix.

   Agent Number & Methods & Weight (kg) & Distance(m) & Success Rate(\%) & Precision (cm) \\   & InterPhys  &  &  & 94.3 & 8.3 \\  & CoHOI+WeightAug(Ours) &  &  & 93.98 & **4.8** \\  & CooHOI(Ours) &  &  & **96.48** & 6.9 \\   & From Scratch &  &  & 0 & NAN \\  & CoHOI(Ours) &  &  & **89.54** & **3.86** \\   

Table 1: This table presents our results for single-agent and two-agent carrying of the Box object. “CooHOI” refers to the policy trained using the complete CooHOI framework in both single-agent and two-agent settings. “CooHOI+WeightAug” indicates that we applied the same weight augmentation design as InterPhys .

Figure 3: Carrying performance for objects of different categories. From left to right: Table, Armchair, and High Stools. All objects were required to be moved to a location 4 meters away.

[MISSING_PAGE_FAIL:7]

experiments in Table 2, which indicate that our policy can effectively handle most daily life objects with a high success rate. Due to the greater variability in the shape and weight of sofas, the accuracy in Line 4 shows a slight drop compared to the result in Line 4 of Table 1. It is important to note that the significant variations in object shapes make it challenging to learn a unified object representation with a limited amount of data, especially since our input is merely a simple bounding box. Therefore, for single-person transport, different types of objects need to be trained separately from scratch, similar to how we handle boxes. For multi-person transport, objects like sofas, similar to boxes, require fine-tuning based on the weights obtained from single-person box transport.

### Ablation Studies

To evaluate and understand the importance of different design choices in CooHOI, we conduct a detailed analysis of scenarios involving single and multiple agents. This includes boundary analysis, which explores factors that lead to a decrease in the success rate during object carrying. Furthermore, to fully validate the scalability of our method, we also tested and included the results in a four-person scenario. To ensure the reliability of our results, our experiments continue to utilize the average of 4 random seeds as previously mentioned.

Single-Agent-Based Box Carrying.We have demonstrated our framework's effectiveness in Table 1. Moreover, we aim to further investigate the maximum potential of our approach. We examined the robustness of the single-agent policy from various perspectives, including object shape, scale, weight, and trajectory length, as shown in Figure 5. First, we observed that a single agent can handle approximately 13 kg of weight. Continuously increasing the weight makes it difficult for the agent to lift the object. As mentioned in 4.2, since our policy input does not include object density, it is challenging for the agents to generalize across different weights. When the weight reaches 20 kg, the accuracy drops to just 30%. Additionally, due to the limited reach of human arms and the lack of dexterous hands in our agent, it is challenging for the agent to lift boxes that are either too large or too small. To validate this hypothesis, we restricted the object's width to 1x while scaling the length and height to 1.5x. As shown by the green circle in the second figure of Figure 5, the success rate could reach **97.8%**. Furthermore, our strategy is quite robust to distance variations and is generally unaffected by them.

Multi-Agent-Based Box Carrying.We conducted upper-limit testing on the multi-agent policy using a similar evaluation method as the single-agent case. As shown in Figure 5, the conclusions for two agents are similar to those for a single agent. The curves in the first figure demonstrate that with an increase in the number of agents, we can easily lift larger and heavier objects that a single person cannot handle, highlighting the necessity of having multiple agents. However, due to the increased complexity of coordinating two agents, boundary conditions have a more pronounced impact on the policy. For instance, objects that are excessively large or small can cause the policy to fail. As shown by the purple circle in the second figure of Figure 5, we conducted a similar experiment to the single-agent scenario. By restricting the object's width to 1x and scaling the length and height to 1.5x, the success rate increased from 0 to **88.67%**.

Figure 5: Detailed ablation experiments on single and two agents cases. “Step” measures the average consumed time in the successful cases. In the 2nd figure, the green circle represents the single-agent scenario without scaling the object’s width, while the purple circle represents the multi-agent scenario.

Analysis of CooHOI Framework.To thoroughly investigate the contribution of CooHOI, we analyzed the results of each method mentioned in Section 3 separately, as shown by the training curves in Figure 8. The first factor is the influence of the **Stand Point**, which refers to whether an extra point is introduced in front of the object to encourage the agent to walk toward it. During the experiments, we discovered that without this, the agent sometimes fails to approach the shortest edge of the object, resulting in a lower hold reward. This leads to an incomplete lift and the subsequent inability to carry the object. **Dynamic Observation** is the second factor, indicating whether we use dynamic information as observation for each agent. Without it, the observation for each agent is limited to the state information of the long object. We found that without the dynamics information, the agent just stands in front of the object, seemingly unsure of what to do. **Reverse Walk** indicates whether the training process includes motion data for walking backward and a reward function focused on learning this movement. We found that if the policy for a single agent is restricted to only forward movement, training with two agents then leads to a deadlock state. As shown in Figure 8, the agents might be able to contact the box, but they cannot carry it to the destination. **Initialization** refers to whether the two-agent policy is initialized using the single-agent policy and then be fine-tuned. In our experiment, even with extended training duration, training the two-agent policy from scratch still failed to achieve successful carrying, as shown in Figure 8. Based on the results above, the absence of any of the aforementioned methods causes our policy to fall into a locally suboptimal solution, preventing the completion of the transportation task. Moreover, you can also find more interesting visual examples of the above failure cases while extending agent numbers from one to two in the appendix.

## 5 Conclusion and Limitation

In this paper, we present Cooperative Human-Object Interaction (CooHOI), a framework designed to address cooperative object transporting tasks through a two-phase learning approach. By initially focusing on individual skill mastery via the Adversarial Motion Priors (AMP) method, followed by a strategic transition to multi-agent collaboration using Multi-Agent Proximal Policy Optimization (MAPPO), our approach facilitates a sophisticated interplay of shared dynamics and implicit communication among agents, resulting in an efficient and generalized performance.

However, within the scope of this paper, our huamnoid characters lack dexterous hands, which limit their ability to manipulate slippery objects or perform more precise actions. We also utilize object bounding box information as the goal feature for our task, which limits our framework's capacity to generalize to a diverse range of object shapes. Additionally, our experiments are limited to humanoid characters in simulation, without any real-world deployment. In future research, we aim to incorporate dexterous hands to enable the manipulation of a wider variety of objects, as well as integrate active perception and navigational abilities to make our framework more generalizable.