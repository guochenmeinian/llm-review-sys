# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

token distribution derived from nearest-neighbor matches in a corpus, which provide more direct attribution but has also been shown to degrade the quality of text generation (Wang et al., 2023a). Additionally, retrieval augmentation can significantly increase the _generation latency_ due to the time required for the retrieval processes to complete and the subsequent expansion of the LM's context.

In this work, we propose **N**earest Neighbor **S**peculative Decoding (Nest). This new semi-parametric language modeling approach is capable of incorporating real-world text spans of arbitrary length into the generations of an off-the-shelf LM, leading to improved quality and latency. Nest extends the standard \(k\)NN-LM approach, which interpolates the output distribution of an LM using the distribution of possible next tokens retrieved from a corpus (Khandelwal et al., 2020). It conducts an additional passage retrieval step at the beginning to limit the need to store and search over all tokens in the corpus, offering a balanced trade-off between search accuracy and efficiency. At each inference step, Nest performs content generation with three sub-steps:

**1) Confidence-based interpolation.** We use a novel _Relative Retrieval Confidence_ (RRC) score to measure the uncertainty of the token retriever and use it as the interpolation coefficient of the output probability mixture. This enables flexible adaptation of the LM's output to different downstream tasks through dynamic interpolation with the token retrieval results.

**2) Dynamic span selection.** Inspired by the Copy Generator (CoG) (Lan et al., 2023), Nest selects not only the best token predicted by the mixture probability but also extends to the span continuing from that token in the corpus when the token retrieval confidence exceeds a predefined threshold.

**3) Relaxed speculative decoding.** If a span of more than one token is selected, it undergoes evaluation based on the mixture probability. Through a rejection procedure similar to that in speculative decoding (Leviathan et al., 2023), only a prefix deemed highly likely by the mixture probability is accepted.

Evaluated on various free-form generation tasks--including question answering, text completion, and factuality-aware generation--using Llama-2-Chat models (Touvron et al., 2023b) of different sizes, Nest demonstrates superior performance compared to both the base LM and the standard \(k\)NN-LM under a zero-shot setting. For example, combined with Nest, the Llama-2-Chat 70B model demonstrates \(42.3\%\) improvement of ROUGE-1 on WikText-103 and \(21.6\%\) improvement of FactScore on Biography. Furthermore, Nest performs competitively with respect to in-context retrieval-augmentation on MMLU, Pile-of-Law, and TruthfulQA. We further demonstrate that the two approaches can be combined to enhance generation quality and attribution. Additionally, by generating multiple tokens at each time step, Nest significantly improves the efficiency of long-form generation. For Llama-2-Chat 70B, it achieves a 1.8\(\) speedup in inference time without compromising attribution or fluency.

## 2 Background

### Problem Definition

Given an input \(x\), a mixture model \(\) predicts the output \(y\) consisting of segments \(\{y_{1},y_{2},...,y_{T}\}\). In our setting, \(\) may produce multiple tokens at a time step \(t\), and therefore \(y_{t}\) indicates the \(t\)-th segment consisting of at most \(n\) tokens where \(1|y_{t}| n\). Let \(\{w_{t}^{(1)},w_{t}^{(2)},...,w_{t}^{(n)}\}\) be the tokens in segment \(y_{t}\), we use \(p_{}(w|x,y_{<t})\) to denote the distribution of the next token, and use \(p_{}(w=w_{t}^{(1)}|x,y_{<t})\) to denote the probability of \(w_{t}^{(1)}\) of the next segment \(y_{t}\).

### Nearest Neighbor Language Models (\(k\)NN-LM)

The mixture model \(\) involves a pre-trained LM and key-value datastore \((,)\) that enables approximate nearest neighbors search without further training or fine-tuning.

Key-value datastore.To create the datastore \((,)\) using the LM for a corpus \(\), let \(f()\) be the mapping from input sequence \(c\) to the hidden states \(h\) of the LM at some fixed layer. Let \(w\) be the next word of \(c\) in the corpus \(\). For a sample \((c_{i},w_{i})\) in \(\) after segmentation, we define the \(i\)-th key-value pair \((k_{i},v_{i})\) in \((,)\) as \((h_{i},w_{i})\), where \(h_{i}=f(c_{i})\). The whole datastore is thus defined as the set of all possible key-value pairs in \(\):

\[(,)=\{(h_{i},w_{i})|(c_{i},w_{i})\}.\] (1)

The size of the datastore \((,)\) is proportional to the total number of tokens in corpus \(\). This brings difficulty in scaling the size of the corpus and the model, which may require massive storage space and computational resources.

Probability interpolation.During inference, the language model outputs the token distribution \(p_{}(w|x,y_{<t})\), together with the hidden state \(q_{t}\). The model uses \(q_{t}\) as a query to search the datastore \((,)\) and retrieve the \(r\)-nearest neighbors \(\) according to the similarity \(s(q,k)\) between a query \(q\) and a key \(k\). The final non-parametric distribution \(p_{k}(w|x,y_{<t})\) is computed using a softmax function over the similarity of all retrieved neighbors:

\[p_{k}(w|x,y_{<t})_{(k_{i},w_{i})}_{w=v_{ i}}( s(q_{t},k_{i})),\] (2)

where \(\) is the inverse temperature. We use \(1/(q_{t})}\) for \(\) in practice where \((q_{t})\) is the hidden state dimension. This is similar to computing attention in the Transformer model (Vaswani et al., 2017). For similarity \(s(q,k)\), we follow Khandelwal et al. (2020) and use the negative squared \(_{2}\) distance. Items not in \(\) are assigned with 0 probability based on the indicator function \(_{w=v_{i}}\).

Finally, the next token is sampled from the mixture distribution \(p_{}\) of the non-parametric distribution \(p_{k}\) and the parametric distribution \(p_{}\) using a fixed hyper-parameter \(\):

\[p_{}(w|x,y_{<t})= p_{}(w|x,y_{<t})+(1- ) p_{k}(w|x,y_{<t}).\] (3)

## 3 Nearest Neighbor Speculative Decoding

### Two-Stage \(k\)-NN Search

As mentioned in Section 2.2, maintaining a token-level key-value store can be expensive in terms of both latency and storage. To provide a better trade-off between latency and accuracy, we adopt the two-stage design, which is widely applied in information retrieval and search engines.

Figure 1: The Nest approach first locates the tokens in the corpus using the LM hidden states. The retrieval distribution \(p_{k}\) is dynamically interpolated with \(p_{}\) based on the retriever’s uncertainty \(_{t}\). The token and its \(n\)-gram continuation are then selected from the mixture distribution \(p_{}\), while the final span length is determined by speculative decoding to remove undesired tokens. The spans incorporated in the final generation provide direct attribution and amortize the generation latency.

First-stage passage retrievalGiven the corpus \(\), we segment the documents into separate passages of less than \(m\) tokens each. We then encode the corpus and use a hybrid retrieval system to select the relevant passages, as dense retrievers are good at handling semantics in queries (Karpukhin et al., 2020) and sparse retrievers are good at lexical matching (Sciavolino et al., 2021).

Second-stage \(k\)-NN token searchAfter obtaining the top-\(b\) retrieved passages \(\{d_{1},d_{2},...,d_{b}\}\) at time step \(t\), we use the encoder \(f()\) of LM to encode the prefixes of all tokens as keys as shown in Figure 1. The key-value datastore \((,)\) therefore is created _on the fly_. Similarly, we use the negative squared \(_{2}\) distance as the similarity function and \(q_{t}\) as the queries to search for the top-\(r\) nearest neighbors \(\) in \((^{},^{})\).

The two-stage design provides a trade-off between search latency and accuracy and the passage-level index only takes a fraction of the token-level index in Section 2.2. In addition, the first-stage passage search also acts as a filter to remove deceptively similar tokens in non-relevant contexts.

### Confidence-Based Output Interpolation

Similar to Equation (3), we linearly interpolate the language model's distribution \(p_{}\) and non-parametric distribution \(p_{k}\) using a coefficient \(_{t}\) for a time step \(t\) in generation. The difference is that we use the token retrieval score to compute \(_{t}\):

\[_{t}=((|s(q_{t},k_{i})|}{_{i}|s(q_{t}, k_{i})|}-)/),\] (4)

where \(\) is the sigmoid function and the min-max ratio expresses the uncertainty of the \(k\)-NN component. We use the sigmoid activation to re-center and re-scale this uncertainty, where \(\) is the offset and \(\) is the scale for the sigmoid function. We refer to this method as Relative Retrieval Confidence (RRC).

If the downstream task does not involve generation, such as perplexity evaluation and multi-choice tasks, our method will end at Equation (4). The mechanisms introduced in the following sections are only applied to generation, including token/span selection and post-hoc revision.

### Dynamic Span Selection

Directly sampling tokens from the mixture distribution \(p_{}\) might escalate the exposure bias since the seemingly coherent tokens might be retrieved from completely different sources. To maintain coherence, we extend the context of the current sampled token by using its \(n\)-gram continuation in the corpus. Given the current time step \(t\), we first select the next token \(w_{t}\) from the mixture distribution \(p_{}\). However, the sampled token \(w_{t}\) may correspond to multiple retrieved \(w_{i}\) (i.e., the value \(v_{i}\)), in the neighbors \(\) which have different \(n\)-gram continuations. We use a simple max-pooling strategy2 to select the starting token \(w_{t}^{(1)}\) of the \(n\)-gram from \(\):

\[w_{t}^{(1)}=|w_{i}=w_{t},w_{i}\}}{}p_{k }(w=w_{i}|x,y_{<t})\] (5)

The corresponding \(n\)-gram for time step \(t\) is \(\{w_{t}^{(1)},w_{t}^{(2)},...,w_{t}^{(n)}\}\) where \(n\) is fixed hyper-parameter. The final output is determined by the interpolation coefficient \(_{t}\) in Equation (4):

\[y_{t}=w_{t},&_{t}>;\\ \{w_{t}^{(1)},w_{t}^{(2)},...,w_{t}^{(n)}\},&\] (6)

where \(\) is a threshold and \(y_{t}\) is the segment output at time step \(t\).

### Relaxed Speculative Decoding

Despite the dynamic selection, the hyper-parameter \(n\) is hard to control over different tasks. To produce spans with adaptive length, we take inspiration from Leviathan et al. (2023), where we use \(\) to revise the proposed \(n\)-gram. However, the proposal distribution \(q(w|x,y_{<t})\) is unknown besides the first token \(w_{t}^{(1)}\). Therefore, we use a relaxed version of speculative decoding that upper bounds the acceptance probability. The probability of accepting the token \(w_{t}^{(i)}\) in a span is:

\[P(w_{t}^{(i)})=(1,\ }(w=w_{t}^{(i)}  x,y_{<t},w_{t}^{(1)},w_{t}^{(2)},...,w_{t}^{(i-1)})}{_{w}p _{}(w x,y_{<t},w_{t}^{(1)},w_{t}^{(2)},...,w_{t}^{(i-1)})}),\] (7)

where \((0,1]\) is the relaxation factor, which is referred to as "leniency" by Leviathan et al. (2023). The smaller \(\) is, the less often \(\) rejects the draft. If token \(w_{t}^{(i)}\) is rejected, we will remove all the tokens from \(w_{t}^{(i)}\) to \(w_{t}^{(n)}\), and then re-select a token \(w_{t}^{(i)}\) from the distribution \(p_{}\) without going through the span selection. The computation for processing multiple tokens can be parallelized and Nest can thus maintain the latency or even accelerate the generation. Moreover, suppose all tokens in the draft are not rejected. In that case, we will directly fetch the \(n\)-gram's continuation in the corpus and use it for the next draft proposal until rejection, removing the reliance on the hyper-parameter \(n\).

Once the \(n\)-gram is accepted, the corresponding parts are masked in the corpus and will never be used again in this generation. This is to prevent the \(k\)-NN component from repetitively retrieving the same segments in a small key-value store \((^{},^{})\). We provide the complete procedure in Algorithm 1.

## 4 Experiments

We evaluate Nest and other baselines on various tasks including text completion, question-answering, fact-verification, and multi-choice tasks, providing a comprehensive picture of factuality, fluency, and attribution of Nest in different domains. In all experiments, we focus on evaluating instruction-following models. We use Llama-2-chat under a zero-shot setting, where we remove the few-shot demonstrations from the instructions to simulate the realistic usage of these models.

### Benchmark Datasets

Text completion.**WikiText-103**Merity et al. (2017) is a standard benchmark for language modeling, extracted from the set of verified articles on Wikipedia. **Pile of Law**Henderson et al. (2022) is a growing dataset of legal and administrative data. We use the datasets3 from Huggingface and further split the test data into validation and test sets. For language modeling, we report the perplexity score. For free-form generation, we report ROUGE-1, 2, L Lin (2004) and MAUVE (Pillutla et al., 2021).

Question answering.We select four knowledge-intensive question-answering datasets, including Natural Questions (NQ) Kwiatkowski et al. (2019), TriviaQA (TQA) Joshi et al. (2017), HotpotQA (HQA) Yang et al. (2018), and MedMCQA (MQA) Pal et al. (2022). Since the in-context demonstrations are removed for free-form generation, we use answer-level recall (i.e., Hit@1) Karpukhin et al. (2020) which checks if the output contains any correct answers instead of exact match.

Fact verification.We evaluate a biography-generation task Min et al. (2023) and TruthfulQA Lin et al. (2022) which is a benchmark for testing false beliefs or misconceptions. We use FActScoreMin et al. (2023) for biography. For TruthfulQA, we follow Lin et al. (2022) which uses the difference between the max similarity to a true reference answer and the max similarity to a false reference answer for BLEU and ROUGE-1.

Closed-set tasks.MMLU (Massive Multitask Language Understanding) Hendrycks et al. (2021) benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. We report the macro accuracy for each domain.

### Implementation

Knowledge Sources.Wikipedia (CC BY-SA 3.0): For tasks except text completion on Pile of Law, we use the Wikipedia 2021 dump released by Izacard et al. (2024) as the knowledge source and follow the same pre-processing procedures in RA-DIT Lin et al. (2024), yielding \(\)33M passages with each less than 200 tokens. Pile of Law (CC BY-NC-SA 4.0): We use the training split from Huggingface and select only the English data. We then follow the same procedure applied in Wikipedia, yielding a corpus containing \(\)15M passages after filtering. More details are provided in Appendix A.

Inference setting.\(k\)NN-LM and Nest share the same first-stage retriever. We use Dragon+ (Lin et al., 2023) and BM25 (Robertson and Zaragoza, 2009) to encode the segments into dense and sparse vectors, respectively. Given the input, we query both the dense and sparse indexes at the same time and retrieve their corresponding top-\((b l)\) passages. We linearly interpolate the similarity scores between the two search results (also known as fusion) and sort them before selecting the top-\(b\) passages. The number of passage candidates \(b\) is set to be 40 and the scaling factor \(l\) is set to be 100. For RA, we use the top-3 passages in the prompt due to the context window limit. We further combine Nest and RA since they are independent methods. Greedy decoding is used during generation. More details about retrieval, decoding, and hyper-parameters are described in Appendix B.

Evaluation setting.For text completion tasks and perplexity evaluation, we use 128 tokens as the prefix and the consecutive 256 tokens as the target. For the other tasks, we use 128 tokens as the max generation length for question answering and 512 for fact verification. For retrieval-based models, only the prefix will be used for retrieval. Hyper-parameters of all baselines and Nest are tuned on the dev set of WikiText-103, NQ, and Biography. Each baseline uses the same hyper-parameters for all tasks evaluated. We first tune the related hyper-parameters for perplexity and then tune the rest for generation metrics to reduce the search space. More details are provided in Appendix B.

### Baselines

Base LMs.We evaluate publicly available, instruction-tuned language models, Llama-2-chat series4, with model sizes ranging from 7B, 13B to 70B.

Two-Stage \(k\)NN-LM.We apply the two-stage strategy described in Section 3.1 to \(k\)NN-LM as well, where we retrieve the top-\(b\) passages and encode a key-value datastore \((^{},^{})\) on the fly.

In-Context Retrieval Augmentation (RA).A common retrieval-augmentation method is adding the retrieved evidence into the prompt. We perform retrieval given the only input instead of retrieving new passages every \(k\) step due to the expense of refreshing the kv-cache.

### Main Results

Table 1 shows the main results of Nest and other baselines. For **language modeling**, RA-Nest is able to achieve the lowest complexity on both WikiText-103 and Pile of Law. For **text completion**, RA has the best MAUVE scores and ROUGE scores in Wikitext-103 while RA-NEST works better for 7B and 13B models on Pile of Law. We observe that for legal documents, quoting the exact clauses from the source might be more favourable compared to Wikipedia.

For **question-answering**, RA-Nest tends to work better for smaller models (7B and 13B) in general. The gap between base LMs and other methods diminishes for 70B LMs, which is consistent with previous work where retrieval is found most useful for smaller models (Borgeaud et al., 2022).

For **fact-verification**, Nest is able to outperform the base LMs but underperform RA in terms of the FActScore. RA-NEST is able to outperform RA for the 70B model. The degradation for RA-70B is caused by generating shorter claims which is punished by the FActScore. On TruthfulQA, the semi-parametric LMs consistently outperform base LMs and RAs where in-context retrieval seems to have a negative effect on the scores. This is because TruthfulQA is an adversarial dataset containing difficult questions where in-context RA is more susceptible to the "evidence" in the prompt (e.g., astrology and myths). In contrast, Nest only interpolates the results at the output level and therefore performs better in this case. The combination RA-Nest is also affected by the in-context retrieval.

For **closed-set tasks**, Nest is comparable to RA and RA-Nest manages to achieve the best macro scores on average. Overall, Nest is able to outperform base LMs and \(k\)NN-LM's on most tasks while being on par with RA. The combination of RA and Nest further improves over the two methodson some tasks. Despite the limited improvement, we will show that Nest is able to provide better attribution and latency in the following sections.

### Latency Analysis

Latency breakdown.The combination of dynamic span selection and relaxed speculative decoding can improve the latency of the LLM generation by quick draft proposal and processing multiple tokens at a time step. Figure 1(a) shows the latency breakdown of a Nest-70B model (\(=0.3,=0.1,=0.5\)) for different relaxation factors on the Biography validation data. The latency experiment is done on 8\(\)A100 GPUs (for model parallelization) and 32 CPU threads (for search). The batch size is set to 1. We use internal, research-purpose implementation of the base Llama-2-chat model which did not optimize for latency. As we can see, the LM encoding time takes about half of the latency, while the sum of the others takes the rest. Noticeably, the cost of passage search and token index building stay relatively constant per query, while the others are related to the number of tokens processed per time step. Still, even with extra retrieval overheads, the slowest Nest model is faster than the base LM, showing the efficacy of span selection and speculative decoding.

Latency-accuracy trade-off.To understand why Nest can accelerate generation, we first show the latency-accuracy trade-off by tuning the relaxation factor in Figure 1(b). The smaller \(\) is, the less often Nest rejects a segment retrieved from the corpus, which enables more tokens to be processed in parallel. The average proposed span length in Figure 1(a) can increase from 5 tokens to 35 tokens at each time step as the relaxation factor gets smaller. Combined with Figure 1(a), we can reach the conclusion that fetching longer spans from the corpus results in lower generation latency per query. For the accuracy, the FActScore on Biography validation data shows that there is a sweet spot around \(=5e-2\) where both low latency and high accuracy can be achieved at the same time.

    &  &  \\  &  &  &  & RG-2 & RG-L & Avg. Len & PPL(\(\)) &  &  & RG-2 & RG-L & Avg. Len \\  Llama-2-Chat\({}_{19}\) & 14.6 & 58.8 & 15.8 & 3.7 & 14.4 & 175.4 & 10.1 & 80.7 & 19.1 & 5.5 & 17.1 & 211.4 \\ +RA & 7.2 & 74.6 & **35.7** & **23.1** & **34.4** & 204.5 & 7.1 & 84.7 & 23.1 & 8.9 & 21.1 & 222.0 \\ +NN-LM & 9.8 & **82.5** & 23.7 & 7.7 & 21.7 & **23.8** & 8.8 & 81.1 & 19.4 & 5.7 & 17.4 & 214.3 \\ +NNST & 8.4 & 73.2 & 28.4 & 14.2 & 27.1 & 218.4 & 8.1 & 88.0 & 23.7 & 8.7 & 21.5 & 226.5 \\ +RA-NIST & **6.4** & 72.6 & 35.2 & 22.7 & 34.0 & 20.0 & **6.7** & **90.0** & **24.4** & **9.0** & **22.2** & **232.1** \\  Llama-2-Chat\({}_{19B}\) & 12.0 & 75.9 & 19.9 & 4.9 & 18.0 & 218.4 & 8.2 & 72.8 & 17.5 & 5.3 & 15.7 & 181.7 \\ +RA & 6.5 & **91.5** & **38.9** & **24.2** & **37.2** & **24.9** & 5.9 & 86.6 & 23.6 & 9.1 & 21.5 & 228.7 \\ +NN-LM & 8.6 & 76.3 & 23.7 & 8.2 & 21.9 & 238.5 & 7.4 & 71.5 & 17.7 & 5.3 & 15.9 & 183.7 \\ +NEST & 7.2 & 67.1 & 29.3 & 15.6 & 28.1 & 207.1 & 6.8 & 86.0 & 22.9 & 8.7 & 20.9 & 212.3 \\ +RA-NIST & **5.8** & 86.8 & 38.6 & 24.0 & 37.0 & 245.5 & **5.7** & **90.1** & **24.7** & **9.2** & **22.4** & **229.4** \\  Llama-2-Chat\({}_{20B}\) & 9.9 & 88.6 & 22.9 & 6.2 & 20.8 & 239.6 & 6.9 & 93.4 & 23.0 & 7.1 & 20.7 & 250.1 \\ +RA & 5.3 & **91.6** & **40.5** & **26.1** & **38.8** & 235.9 & 4.9 & 95.5 & 26.3 & **10.1** & **24.0** & 253.2 \\ +NN-LM & 7.1 & 83.6 & 26.1 & 9.6 & 24.1 & **25.3** & 6.3 & 9.4 & 23.1 & 7.2 & 20.8 & 251.3 \\ +NNST & 6.3 & 82.6 & 32.6 & 7.2 & 31.7 & 23.1 & 236.3 & 5.9 & 95.4 & 25.6 & 9.4 & 23.2 & 251.3 \\ +RA-NIST & **4.8** & 90.0 & 40.2 & 25.9 & 38.6 & 233.1 & **4.7** & **97.6** & 26.2 & 9.5 & 23.7 & **253.6** \\    
  
**Models** & **TQA** & **NQ** & **HQA** & **MQA** & **Avg.** &  &  &  \\  &  &  & FS & \# Facts & Human. & STEM & Social & Other & Avg. \\  Llama-2-Chat\({}_{19B}\) & 61.1 & 38.9 & 30.6 & 9.3 & 35.0 & -0.02 & 0.42 & 27.2 & **71.2** & 37.8 & 32.6 & 38.9 & 39.6 & 37.2 \\ +RA & **69.5** & 48.4 & 44.1 & 12.8 & 43.7 & -0.34 & 0.18 & **56.5** & 67.1 & 41.8 & 35.3 & **42.2** & 43.3 & 40.7 \\ +NN-LM & 63.4 & 42.4 & 33.5 & 9.5 & 37.2 & **0.13** & **0.66** & 30.6 & 59.8 & 38.0 & 33.1 & 39.2 & 40.1 & 37.6 \\ +N8rST & 61.5 & 43.2 & 33.5 & 10.2 & 37.1 & 0.03 & 0.45 & 38.9 & 58.2 & **42.0** & **35.4** & 42.0 & **43.4** & **40.7** \\ +RA-NIST & 69.0 & **40.8** & **45.3** & **13.3** & **44.1** & -0.32 & 0.21 & 55.1 & 57.7 & 37.9 & 32.7 & 39.3 & 39.8 & 37.4 \\  Llama-2-Chat\({}_{19B}\) & 63.5 & 42.3 & 32.6 & 10.2 & 37.2 & 0.13 & 0.81 & 28.8 & 49.9 & 41.5 & 35.0 & 40.2 & 43.8 & 40.1 \\ +RA-NIST & 70.9 & 51.6 & 44.6 & 14.0 & 45.3 & -0.16 & 0.25 & **59.1** & 51.2 & 43.4 & 37.4 & 43.5 & 46.4 & 42.7 \\ +NN-LM & 64.7 & 43.5 & 34.2 & 11.2 & 38.4 & 0.20 & 0.95 & 31.1 & 46.1 & 41.4 & 34.7 & 40.6 & 44.2 & 40.2 \\ +NEST & **64.2** & **44.2** & **34.3** & **10.9** & **39.4** & **0.29** & **0.35** & 35.7 & 47.2 & **41.3** & **34.9** & 40.2 & 43.7 & 40.0 \\ +RA-NIST & **70.9** & **51.7** & **45.3** & **14.7** & **45.7** & -0.14 & 0.25 & 58.4 & **52.4** & **43.5** & **37.7** & **43.5** & **46.7** & **42.8** \\

### Attribution and Qualitative Analysis

One of the most important features of Nest is providing attribution directly at a span level, where the reference for the corresponding statement is accurate since it is directly taken from the corpus. Table 2 shows the attribution ratio, average attributed span length, and two examples for analysis. For NQ and Biography tasks, depending on the model and hyper-parameters in Equation (4) and (7), the ratio of tokens that can be traced back to the corpus ranges from 33.2% to 95.5%. In addition, it is more desirable to have consecutive segments that come from the same source so that consistent attribution can be provided, and the average length of spans taken from the corpus ranges from 3.0 to 27.9 tokens. This feature provides span-level attribution for most claims in the LLM generation. To our knowledge, neither of the baselines can achieve the same granularity and preciseness for the attribution as Nest. We provide more analyses on sensitivity and ablation for Nest in Appendix C.

## 5 Related Work

### Retrieval-Augmentation

Retrieval Augmentation involves external knowledge sources to improve the effectiveness of language models on knowledge-intensive tasks. Chen et al. (2017) propose DrQA which combines extractive models and independent retrievers for open-domain question-answering. Follow-up works on retrieval-augmentation such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and Atlas (Izacard

   Task & Models & Ratio (\%) & Length & Example (Generated by Nest-70B) \\   & Nest\({}_{}\) & 93.4 & 18.4 & Question: who sings does he love me with reba? \\  & Nest\({}_{}\) & 52.4 & 4.4 & Answer: The song “Does He Love You”  is a duet \\  & Nest\({}_{}\) & 58.8 & 4.3 & between Reba McEntree and Linda Davis. It was \\  & RA-Nest\({}_{}\) & 77.5 & 3.1 & released in August 1993 as the first single from Reba’s \\  & RA-Nest\({}_{}\) & 33.2 & 3.0 & compilation album Greatest Hits Volume 2. It is one of \\  & RA-Nest\({}_{}\) & 44.9 & 3.9 & country music’s several songs about a love triangle.  \\   & Nest\({}_{}\) &  &  & Question: Tell me a bio of Tral Hicks. \\  & Nest\({}_{}\) & 53.9 & 10.6 & Answer: Tral Hicks (born September 21, 1974) is an American actress \\   & Nest\({}_{}\) & 58.6 & 7.0 & American crime drama film A Bronx Tale and her singing in such work \\   & RA-Nest\({}_{}\) & 50.3 & 5.1 & as her 1997  debut studio album This Time which peaked at No. 4 on \\   & RA-Nest\({}_{}\) & 48.5 & 5.9 & Billboard Bubbling Under Hot 100 Singles. Raised in Teaneck, \\   & RA-Nest\({}_{}\) & 80.7 & 11.0 & New Jersey, Hicks graduated from Teaneck High School in 1994 . \\    & & & She is the younger sister of actress and singer D’atra Hicks. \\   

Table 2: Attribution analysis. (Attribution) Ratio: Proportion of tokens that are taken from the corpus. (Attribution) Length: Average length of consecutive spans in the generation that are taken from the same document. Green: Segments taken from the corpus. Gray: Reference.

Figure 2: Latency-accuracy trade-off and breakdown on Biography using Llama-2-Chat 70B+Nest. As the relaxation factor \(\) decreases, Nest tends to accept longer spans from the corpus. We choose \(=5e-2\) in our main experiments, which accelerates the generation and improves the FACTScore.

et al., 2024) further combine the retrieval component in pre-training and fine-tuning for downstream knowledge-intensive tasks. Asai et al. (2024) further divide them into three categories:

**Input augmentation.** REPLUG (Shi et al., 2024) and in-context RALM (Ram et al., 2023) propose to pre-pend the retrieved passages in the prompts for zero-shot factual support. Recently, Self-RAG (Asai et al., 2023) leverages special tokens to perform adaptive retrieval and different critics to iterative refine the RALM's output. RA-DIT (Lin et al., 2024) retrofits LLMs with retrieval capabilities via instruction fine-tuning.

**Intermediate fusion.** RETRO (Borgeaud et al., 2022) employs a novel attention mechanism to incorporate multiple pre-processed text fragments in intermediate layers for more efficient integration of retrieved results. This approach has been successfully applied to larger decoder-only language models as demonstrated by RETRO++ (Wang et al., 2023) and InstructRetro (Wang et al., 2024). FiD (Izacard and Grave, 2021) applies similar an encoder-decoder structure in a zero-shot manner and achieves better effectiveness at a document level.

**Output integration.**\(k\)NN-LM (Khandelwal et al., 2020) pioneers this direction and proposes to interpolate the retrieval distribution and LM's prediction. Follow-up works further propose adaptive interpolation methods which involve training (He et al., 2021; Bhardwaj et al., 2023) and excessive tuning (Drozdov et al., 2022). Another line of work proposes to joint train the phrase encoder and LM to expand the vocabulary dynamically using the retrieved phrases, such as Copy-Generator (Lan et al., 2023) and its follow-up work (Cao et al., 2024). Martins et al. (2022) proposes a chunk-based \(k\)NN machine translation model which retrieves chunks of tokens from the datastore.

### Inference-Time Revision

Speculative decoding (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023; Spector and Re, 2023) is an acceleration method that leverages a small model to generate drafts for a large model to evaluate. The latency is improved as the larger model can process multiple tokens in parallel at each time step. Recently, REST (He et al., 2024) proposes to draw multiple drafts from a datastore and leverages a prefix trie tree to compute the proposal distribution, which is the closest concurrent work. Yang et al. (2023) also utilizes prefix matching to select draft sentences from a datastore, and keep the continuation of the draft sentence as long as the token matches with the model generation.

In general, speculative decoding can be categorized as an unbiased self-revision method. In comparison, Nest changes the LM output distribution through interpolation with a non-parametric probability distribution. Previous work focusing on fact-checking follows a similar idea to generate factually consistent texts with a set of evidence via post-hoc editing, such as FRUIT (Iv et al., 2022) and PEER (Schick et al., 2022). Recently, RARR (Gao et al., 2023) leverages more complex planning with LLMs to verify the retrieved evidence and generate attribution reports.

## 6 Limitations

While being able to directly retrieve segments from the corpus and apply them in the generation, the output of Nest might still contain factual errors depending on the accuracy of the first-stage passage retrieval and the second-stage token retrieval. Moreover, as a plug-and-play method, our main goal is to provide a flexible solution that can combine different LLMs and data stores in zero- and few-shot manners. Without further fine-tuning, the integrated system might be sub-optimal and the results can be better if it is fine-tuned on appropriate tasks. Lastly, such semi-parametric LMs may not improve the ability of in-context learning, since the demonstrations in the prompts are unlikely to appear in any contexts that can be found in the database. An observation from preliminary experiments is that the current neural retrievers do not have the capability to process the in-context few-shot information, where techniques such as query reformulation might be needed for parsing the demonstrations.

## 7 Conclusion

This paper presents Nest, an inference-time revision method for LMs that improve their factuality and attribution through nearest neighbor speculative decoding. Leveraging two-stage \(k\)-NN search, relative retrieval confidence, dynamic span selection, and relaxed speculative decoding, Nest improves both validation perplexity and free-form generation quality on nine different tasks. Itseffectiveness can be further improved when combined with in-context retrieval augmentation. With these results, we demonstrate that Nest is capable of generating text grounded to real-world sources in low latency while maintaining fluency.

## 8 Broader Impact

The ability to copy real-world texts from existing data stores is useful for finding the source of the claim (credibility), preventing hallucination (factuality), as well as protecting copyright (risk management). It helps to resolve the dispute that often happens in AI tools by acknowledging the contents that are borrowed from existing human works (e.g., arts, books, and other creative content). Meanwhile, the information on the Internet is mixed and it is important to filter out false and sensitive information before directly injecting them into the generation.