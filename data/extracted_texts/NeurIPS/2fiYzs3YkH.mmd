# Unleashing the Denoising Capability of Diffusion Prior

for Solving Inverse Problems

 Jiawei Zhang

Tsinghua University

jiawei-z23@mails.tsinghua.edu.cn &Jiaxin Zhuang

Tsinghua University

zhuangjx23@mails.tsinghua.edu.cn &Cheng Jin

Tsinghua University

jinc21@mails.tsinghua.edu.cn &Gen Li

CUHK

genli@cuhk.edu.hk &Yuantao Gu

Tsinghua University

gyt@tsinghua.edu.cn

###### Abstract

The recent emergence of diffusion models has significantly advanced the precision of learnable priors, presenting innovative avenues for addressing inverse problems. Previous works have endeavored to integrate diffusion priors into the maximum a posteriori estimation (MAP) framework and design optimization methods to solve the inverse problem. However, prevailing optimization-based rithms primarily exploit the prior information within the diffusion models while neglecting their denoising capability. To bridge this gap, this work leverages the diffusion process to reframe noisy inverse problems as a two-variable constrained optimization task by introducing an auxiliary optimization variable that represents a 'noisy' sample at an equivalent denoising step. The projection gradient descent method is efficiently utilized to solve the corresponding optimization problem by truncating the gradient through the \(\)-predictor. The proposed algorithm, termed ProjDiff, effectively harnesses the prior information and the denoising capability of a pre-trained diffusion model within the optimization framework. Extensive experiments on the image restoration tasks and source separation and partial generation tasks demonstrate that ProjDiff exhibits superior performance across various linear and nonlinear inverse problems, highlighting its potential for practical applications. Code is available at https://github.com/weigerzan/ProjDiff/.

## 1 Introduction

Denoising diffusion models have achieved tremendous success in the field of generative modeling . Their remarkable ability to capture data priors provides promising avenues for solving inverse problems , which are widely exploited in image restoration , medical image processing , 3D vision , audio processing  and beyond.

Numerous endeavors have sought to harness diffusion models to address inverse problems . Since the sampling process of diffusion models is a reverse Markov chain, most approaches attempt to integrate the guidance provided by the observation equation into the sampling chain. For instance, DDRM  achieves favorable results for linear inverse problems with low complexity by introducing a new variational distribution. DDNM  capitalizes on the concept of null-range decomposition, effectively rectifying the range-space component of the intermediate steps byleveraging the observation information. DPS  guides the generation process using the gradients of intermediate steps with respect to the observation error. Moreover, methods based on Monte Carlo Particle Filtering [25; 26; 27] are employed to approximate the true posterior.

Since inverse problems are often modeled as maximum a posteriori (MAP) estimations, recent efforts have been made to explicitly integrate diffusion models as the prior term in the optimization framework [28; 24].  proposed to employ the Evidence Lower Bound (ELBO) to approximate the prior, facilitating the application of diffusion models in inverse problems in a plug-and-play manner.  achieved analogous results from a variational perspective. In these optimization-based methods, due to the presence of the observation noise, a common practice is to adopt a Gaussian likelihood. However, it's worth noting that, since diffusion models are inherently effective at denoising, considering the observation noise in the likelihood term fails to fully leverage diffusion models' denoising capability.

To fully utilize both the prior information and the denoising capability inherent in diffusion models within the optimization framework, we introduce an auxiliary optimization variable to accommodate the influence of the observation noise. We derive a novel two-variable objective based on the properties of the diffusion process and transform the inverse problem into a constrained optimization task. Through gradient truncation, we obtain a more practical approximation of the stochastic gradient of the objective which sidesteps significant computational overhead. The proposed algorithm, termed ProjDiff, tackles the inverse problem by employing the concept of projection gradient descent to solve the corresponding optimization task. We also discuss the noise-free version of ProjDiff as a special case, which, compared to other optimization-based methods, ensures superior consistency between the generated results and observations. ProjDiff's applicability also extends beyond linear observations to encompass nonlinear functions, which enhances its competitiveness and versatility.

We demonstrate the outstanding performance of ProjDiff through comprehensive experiments on various benchmarks. In both linear and nonlinear image restoration tasks, ProjDiff exhibites superior performance among existing state-of-the-art (SOTA) algorithms. In the music separation task, ProjDiff shows for the first time, to the best of our knowledge, that a diffusion-based separation algorithm can surpass previous SOTA, which further demonstrates the powerful potential of diffusion models and provides insights to better harness their capabilities in inverse problems for future research.

## 2 Backgrounds

### Denoising diffusion models

Denoising diffusion models [1; 2; 3] are a class of latent variable generative models tailored to capture a targeted data distribution \(q()\). Diffusion models typically predefine a forward process characterized as a Markov chain, wherein the transition probability is stipulated as Gaussian distribution:

Figure 1: Framework of ProjDiff. We introduce an auxiliary variable \(_{t_{a}}\) and transform the inverse problem into a two-variable constrained optimization problem which can be solved using the projection gradient method.

\[q(_{0:T})=q(_{0})_{t=1}^{T}q(_{t}|_{t- 1}), q(_{t}|_{t-1})=(_{t};a_{t} _{t-1},b_{t}),\] (1)

where \(a_{t}\) and \(b_{t}\) represent the scale and variance parameters, respectively. The sampling process of diffusion models aims to invert the forward chain via a parameterized reverse process, which is also modeled as a Markov chain with learnable parameter \(\):

\[p_{}(_{0:T})=p_{}(_{T})_{t=1}^ {T}p_{}(_{t-1}|_{t}), p_{}( _{t-1}|_{t})=(_{t-1};_{ }(_{t},t),_{t}^{2}),\] (2)

where \(_{}\) denotes the mean function and the variance \(_{t}^{2}\) is predefined. In this work, we primarily focus on the Variance Preservation (VP) diffusion 1, characterized by parameters \(a_{t}=}\), \(b_{t}=1-_{t}\) with \(_{0} 1\) and \(_{t=0}^{T}_{t} 0\). With these parameters, we have \(q(_{t}|_{0})=(_{t};_{t}}_{0},(1-_{t}))\), where \(_{t}=_{t=0}^{t}_{t}\). The initial distribution of the reverse process is set to match the forward process, i.e., \(p_{}(_{T})=(,)\), while the mean function is chosen as

\[_{}(_{t},t)=_{t-1}}_ {}(_{t},t)+_{t-1}-_{t}^{2}} _{t}-_{t}}_{}( _{t},t)}{_{t}}}.\] (3)

Here \(_{}\) serves as an estimation of \(_{0}\) at time \(t\), typically implemented through a parameterized neural network referred to as a "\(\)-predictor"2. The variance \(_{t}^{2}[0,1-_{t-1}]\) can take various values. When \(_{t}=_{t-1}}{1-_{t}}}_{t}}{_{t-1}}}\), the reverse process aligns with DDPM ; while when retaining \(_{t}=_{t}\) as an adjustable parameter, one obtains DDIM . We refer to [19; 29; 30; 31] for more theoretical guarantees of diffusion models.

### Diffusion models as data prior

As stated in [32; 28], for a well-trained diffusion model, the Evidence Lower Bound (ELBO) can effectively approximate the log-likelihood of the samples, i.e.

\[ q(_{0}) C-_{t=1}^{T}(t)_{q( _{t}|_{0})}||_{0}-_{}(_{t},t)||^{2},\] (4)

where \((t)\) represents the coefficient within the ELBO and \(C\) is a constant. Utilizing the ELBO as the log-prior term enables diffusion models to serve as a plug-and-play prior for general inverse problems.  handled the ELBO via stochastic gradient descent and the reparameterization method. By sampling \(t\{1,2,,T\}\) and \((,)\), a stochastic gradient of the ELBO can be derived as

\[_{t,_{0}}=(_{_{0}}_{}(_{t}}_{0}+_{t}} ,t)-)(_{0}-_{}(_{t}}_{0}+_{t}},t)),\] (5)

where \(_{_{0}}_{}()\) denotes the Jocabian of the \(\)-predictor.  derived a similar proxy objective from the perspective of variational inference, and proposed a more computationally efficient approximation of the stochastic gradient by reweighting the objective. Notably, when the variational distribution is a delta function, variational inference aligns with the MAP estimation. In this work, we introduce a novel two-variable ELBO by constructing an auxiliary variable that accounts for the observation noise, thereby utilizing both the prior information and the denoising capability in diffusion models simultaneously.

### Equivalent noisy samples

As stated in , the noisy observation of a sample can be regarded as a noise-free observation of an equivalent noisy sample at certain steps in the diffusion process. For linear observations \(=_{0}+\), where \(_{0}^{m_{}}\), \(^{m_{}}\), and \((,_{m_{}})\), a common practice [20; 21; 26] involves the Singular Value Decomposition (SVD) to attain a decoupled form. Without loss of generality, assume \(\) has full row rank. Utilizing the SVD \(=^{T}\) yields \(^{T}=(^{T}_{0})+ \), thus transforming the observation equation to \(}=}_{0}+\), where \(}=^{T}\) and \(}_{0}=^{T}_{0}\), which can be further written element-wise as

\[}_{i}}{s_{i}}=}_{0,i}+}_{i},1 i m_{},\] (6)

where \(}_{0,i}\) and \(}_{i}\) represent the \(i\)th component of \(}_{0}\) and \(}\), respectively, and \(s_{i}\) denotes the \(i\)th singular value of \(\). Assuming there exist \(t_{i}\{1,2,,T\},1 i m_{}\) such that \(_{t_{i}}=1/(1+(/s_{i})^{2})\), let \(}_{i}^{}=_{t_{i}}}_{i}}/s_{i}\) and then the observation function can be rewritten as

\[}_{i}^{}=_{t_{i}}}_{0,i}}+_{t_{i}}_{i}}.\] (7)

\(_{t_{i}}}_{0,i}}+_{t_{i}}_{i}}\) can be interpreted as the \(i\)th component of a sample at time step \(t_{i}\), namely \(}_{t_{i},i}\). Thus  proposed to employ Monte Carlo Particle Filtering to restore the samples up to the equivalent noise level, and then apply the backward transition to map these noisy samples back to the clean samples. In this work, we introduce this equivalent noisy sample as an auxiliary variable, thus better handling the observation noise in the optimization framework.

## 3 Method

In this section, we introduce the ProjDiff algorithm. The main idea behind ProjDiff lies in constructing an auxiliary variable to align the noisy observations to specific steps of the diffusion process, thereby forming a two-variable constrained optimization task, which can be tackled by approximating the stochastic gradients and employing the projection gradient descent method. We begin by deriving the ProjDiff algorithm for noisy linear observations and then discuss its noise-free version as a special case and the extension of ProjDiff to accommodate nonlinear observations.

### ProjDiff

Consider the Gaussian observation equation \(=_{0}+\). The goal of the inverse problem is to recover the original data \(_{0}\) given \(\), \(\), and \(\). The decoupled observation equation in (7) indicates that one can apply SVD to reduce a linear observation into the simple form with \(=[_{m_{}},]\) in the spectral domain, which implies the observation \(\) represents the first \(m_{}\) components of \(\). Consequently, we can focus on this simplest observation function to declare our algorithm. Under this condition, assuming there exists some \(t_{a}[0,1,,T]\)3 such that \(_{t_{a}}=1/(1+^{2})\), we rewrite the observation function as

\[^{}=[_{m_{}},]( _{t_{a}}}_{0}+_{t_{ a}}}^{}),\] (8)

where \(^{}=/}\) and \(^{}(,_{m_{}})\). The noisy observation \(^{}\) can be interpreted as a noise-free observation of \(_{t_{a}}}_{0}+_{t_{ a}}}^{}\), which in turn can be viewed as a sample in the manifold of \(q(_{t_{a}})\). Thus, we introduce an auxiliary variable \(_{t_{a}}\) which denotes a noisy sample at time step \(t_{a}\), and propose to optimize the log-posterior term of the two variables as

\[_{_{0},_{t_{a}}} q(_{0}, _{t_{a}}|) =_{_{0},_{t_{a}}} q(|_{0},_{t_{a}})+ q(_{0},_{t_{a}})- q( )\] (9) \[_{_{0},_{t_{a}}} q(| _{0},_{t_{a}})+ p_{}(_{0}, _{t_{a}})- q(),\] (10)

where in (10) we use \(p_{}\) to approximate the true prior \(q\). By the construction of the auxiliary variable \(_{t_{a}}\), the likelihood term embodies a stringent consistency between this auxiliary variable and the noisy observation, which serves as a constraint. Note that \( q()\) is independent of the optimization variables, thus (10) is transformed into the following constrained optimization task:

\[_{_{0},_{t_{a}}} p_{}(_{0}, _{t_{a}})^{}=[_ {m_{}},0]_{t_{a}}.\] (11)

Now we seek the ELBO as a proxy of the joint log-prior term of \(_{0}\) and \(_{t_{a}}\) to render it tractable. Leveraging Jensen's inequality and the transition probability of the diffusion model, we reach the following proposition.

**Proposition 1**.: _Considering the DDIM reference distribution \(q_{}\), we have the variational lower bound of the log-prior term as_

\[ p_{}(_{0},_{t_{a}}) C-^{t_{a}}_{q_{}(_{t}|_{0},_{t_{a} })}(g(t)||_{0}-_{}(_{t},t)||^{2} )}_{}\] (12)

_where \(C\) is a constant independent of \(_{0}\) and \(_{t_{a}}\), the weight \(g(t),w(t)\) are functions of \(_{t}\) and the DDIM variance \(_{t}\), and \(=(_{t}-_{t}}{_{t_{a}}}}_{t_{a}})/_{t }}{_{t_{a}}}}\)._

The complete derivation of the lower bound is provided in Appendix A. Note that the summation on the right-hand side of (12) is partitioned into two parts. The first part, the **denoising matching term**, pertains to the approximation of \( p_{}(_{0}|_{t_{a}})\), signifying the consistency between the noise-free variable \(_{0}\) and the auxiliary variable \(_{t_{a}}\). The second part, the **noisy prior term**, corresponds to the approximation of the log-prior of the noisy auxiliary variable \(_{t_{a}}\). Conceptually, the workflow of utilizing this ELBO for inverse problems can be delineated as follows: the noisy prior term, coupled with the constraint on the auxiliary variable, exploits the prior information of the diffusion model and the observation information to recover a noisy sample \(_{t_{a}}\) that satisfies the observation equation, while the denoising matching term leverages the denoising capability of the diffusion model to restore the clean sample \(_{0}\) from the noisy sample \(_{t_{a}}\).

Now, we address the ELBO outlined in Proposition 1 utilizing the principle of the stochastic gradient method. The constant \(C\) is independent of the optimization variables and can therefore be disregarded. The following proposition is obtained through reparameterization.

**Proposition 2**.: _Randomly sampling \(t\{1,2,,T\},,^{ }(,)\), the stochastic gradients of the proxy objective (for minimization) with respect to \(_{0}\) and \(_{t_{a}}\) are, respectively,_

\[_{_{0},t}=_{_{0}}| |_{0}-_{}(_{t},t) |^{2}&t t_{a};\\ &t>t_{a},\] (13)

\[_{_{t_{a}},t}=_{_{t_{a}}} ||_{0}-_{}(_{t},t) |^{2}&t t_{a};\\ _{_{t_{a}}}||_{t_{a}}-(_{t_{a}}}_{}(_{t},t)+_{t_{a}}}^{})|^{2}+w(t)/g(t)_ {_{t_{a}}}_{}(_{t},t ),&t>t_{a},\] (14)

_where_

\[_{t}=_{t}}_{0}+ _{t}(_{t_{a}}-_{t_{a}}}_{0})/ _{t_{a}}}+_{t}&t t_{a};\\ _{t}/_{t_{a}}}_{t_{a}}+ _{t}/_{t_{a}}}&t>t_{a},\] (15)

_and \(_{t}\), \(_{t}\) are determined by \(_{t}\) and the DDIM variance \(_{t}\)._

The coefficients are omitted as they can be scaled into the step sizes. Note that in such stochastic gradients, the Jacobian of \(_{}()\) is involved, which necessitates backpropagation through the neural network and incurs significant computational and storage overhead. Therefore, we use an approximate yet effective and practical method by truncating the gradients in \(_{}()\), i.e. \(_{_{0}}_{}()\) and \(_{_{t_{a}}}_{}()\). Intuitively, the \(\)-predictor of the diffusion model should be resilient to small perturbations in the input. For instance, when we feed a noisy image into a pre-trained diffusion model and introduce minor perturbations to the original image, we anticipate the predicted image to maintain consistency with the original one. Thus the approximation for \(_{0}\) is acceptable, and similarly for \(_{t_{a}}\) if the noise level of \(_{t_{a}}\) is not too large. By such gradient truncation, the approximation of the stochastic gradients for \(_{0}\) and \(_{t_{a}}\) are, respectively,

\[}_{_{0},t}=_{0}-_{ }(_{t},t)&t t_{a};\\ &t>t_{a},\] (16)

\[}_{_{t_{a}},t}=&t t_{a} ;\\ _{t_{a}}-_{t_{a}}}_{}( _{t},t)-_{t_{a}}}^{}&t>t_{ a}.\] (17)Note that \(_{0}\) is not subject to the constraint and can be updated directly by stochastic gradient descent. \(_{t_{a}}\) is involved in the equality constraint, so we apply the projection gradient descent method. Considering the projection operator of the observation in (11), an iteration step of the proposed algorithm is outlined as follows

\[_{0}_{0}-_{1}} _{_{0},t},\] (18) \[_{t_{a}}[_{m_{}}, _{m_{}(m_{}-m_{})}]^{T} ^{}+(_{m_{} m_{ }},_{m_{}-m_{}})(_{t_{a}}-_{2}}_{_{t_{a}},t}),\] (19)

where \(_{1}\) and \(_{2}\) are selected step sizes and \(()\) denotes the operation of arranging entries into a diagonal matrix. We term this algorithm ProjDiff.

```
0: Observation \(^{}\), pre-trained diffusion model \(}\), step sizes \(_{1},_{2}\), total steps \(T\), noise schedule \(_{1}_{T}\), equivalent noise level \(_{t_{a}}\).
1: Sample \(_{T},_{T}^{}(, )\);
2: Initialize \(_{t_{a}}_{t_{a}}}}( _{T},T)+_{t_{a}}}_{T}^{ }\);
3:for\(t=T\) to \(t_{a}+1\)do
4: Sample \(_{}^{}(,)\);
5: Calculate the approximate stochastic gradient \(}_{_{t_{a}},t}\) as (17);
6: Update \(_{t_{a}}\) as (19);
7:endfor
8: Initialize \(_{0}}(_{t_{a}},t_{a})\);
9:for\(t=t_{a}\) to \(1\)do
10: Sample \((,)\);
11: Calculate the approximate stochastic gradient \(}_{_{0},t}\) as (16);
12: Update \(_{0}\) as (18);
13:endfor
14:return\(_{0}\) ```

**Algorithm 1** ProjDiff for VP diffusion (noisy observation).

### Noise-free observations: a special case

In the noise-free scenario, the observation equation becomes \(=_{0}\) and the auxiliary variable \(_{t_{a}}\) degrades to \(t_{a}=0\), thus the constraint is directly applied on \(_{0}\). The corresponding optimization problem can be expressed as

\[_{_{0}}_{t=1}^{T}(t)_{q(_{t}| _{0})}||_{0}-}(_{t},t)||^{2} =_{0}.\] (20)

Utilizing reparameterization and gradient truncation, the approximate stochastic gradient of the objective is

\[}_{t,_{0}}=_{0}-}( _{t}}_{0}+_{t}},t).\] (21)

Then the iteration of ProjDiff in the noise-free scenario is given by:

\[_{0}^{}+(- ^{})(_{0}-}_ {t,_{0}}),\] (22)

where \(^{}\) is the Moore-Penrose pseudo-inverse of matrix \(\) and \(\) is the selected step size. The principle of the ProjDiff algorithm in noise-free scenarios resides in treating the observation as a constraint. In contrast to  where the authors used Gaussian likelihood for noise-free observations, ProjDiff guarantees better consistency between the generated results and the observations.

### Extension to nonlinear observation

Nonlinear inverse problems are inherently more ill-posed than linear ones. For instance, in our experiments, we consider the phase retrieval task, which aims to recover the original image given only the Fourier magnitude spectrum. The observation equation is:

\[=|(_{0})|,\] (23)

[MISSING_PAGE_FAIL:7]

### Image restoration

**Linear tasks.** We demonstrate the performance of ProjDiff across three linear image restoration tasks on ImageNet : \(4\) super-resolution (with average pooling), \(50\)% random inpainting, and Gaussian deblurring. Comparisons are conducted against prominent diffusion-based image restoration algorithms in recent years, including DDRM , DDNM+ , DPS , and RED-diff . The performance of the least square solution (\(}_{0}=^{}\)) is reported as a baseline. For a fair comparison, DDRM, DDNM, RED-diff, and ProjDiff utilize \(100\) function evaluations, while DPS uses \(1000\) function evaluations as its performance significantly declines with \(100\) steps. The pre-trained model on ImageNet is sourced from , specifically the \(256 256\) model without classifier guidance. Testing is conducted on a 1k sub-testset of ImageNet consistent with . Metrics include PSNR, SSIM , LPIPS , and FID , with all LPIPS values multiplied by \(100\) for clarity.

Table 1 presents the restoration metrics for noisy observations with standard deviation \(=0.05\) (doubled when pixels are rescaled to \([-1,1]\)) on ImageNet. ProjDiff demonstrated highly competitive performance compared to other algorithms. Some visualization results are shown in Figure 2. More experiments on ImageNet and CelebA  alongside ablation studies are shown in Appendix C.

**Nonlinear tasks.** We evaluate the effectiveness of ProjDiff for nonlinear observations through the challenging phase retrieval task on the FFHQ dataset . We compare ProjDiff against DPS  and RED-diff  as they are applicable to nonlinear observations. Traditional baseline algorithms are also reported, including the Error Reduction (ER) algorithm , Hybrid Input-Output (HIO) , and Oversampling Smoothness (OSS) . Table 2 presents the results for both noise-free and noisy scenarios. One sample is generated per image from each algorithm. Given the inherently ill-posedness of phase retrieval, ProjDiff requires more steps to achieve superior results. We set the number of function evaluations to \(1000\) for all three algorithms. The results demonstrate that ProjDiff excels in addressing nonlinear inverse problems, regardless of whether the observation is noise-free or noisy. Some visualization results are shown in Figure 3. Further experiments on the high dynamic range (HDR) task can be found in Appendix C.

   &  &  &  \\  & & PSNR\(\) SSIM\(\) LPIPS\(\) FID\(\) & PSNR\(\) SSIM\(\) LPIPS\(\) FID\(\) \\  - & ER & 11.15 / 0.19 / 84.48 / 409.91 & 11.16 / 0.19 / 84.43 / 412.59 \\ - & HIO & 11.97 / 0.25 / 82.06 / 342.52 & 11.87 / 0.24 / 82.18 / 339.13 \\ - & OSS & 12.57 / 0.35 / 81.08 / 360.98 & 12.55 / 0.25 / 81.20 / 364.52 \\
1000 & DPS & 13.19 / 0.20 / 67.14 / 172.56 & 12.58 / 0.18 / 67.56 / 134.64 \\
1000 & RED-diff & 14.19 / 0.28 / 63.42 / 173.52 & 12.36 / 0.14 / 75.98 / 221.11 \\
1000 & **ProjDiff** & **33.39 / 0.74 / 20.42 / 35.91** & **23.84 / 0.60 / 38.55 / 76.20** \\  

Table 2: Phase retrieval results. The LPIPS metrics are multiplied by \(100\).

   &  &  &  &  \\  & & PSNR\(\) SSIM\(\) LPIPS\(\) FID\(\) & PSNR\(\) SSIM\(\) LPIPS\(\) FID\(\) & PSNR\(\) SSIM\(\) LPIPS\(\) FID\(\) \\  - & \(A^{}\)y & 21.85 / 0.45 / 65.34 / 183.32 & 14.21 / 0.24 / 67.42 / 176.52 & 17.78 / 0.29 / 60.25 / 100.05 \\
1000 & DPS & 24.44 / 0.67 / **31.81 / 36.17** & 30.15 / 0.86 / 17.76 / 22.03 & 24.26 / 0.64 / 37.03 / 50.17 \\
100 & DDRM & 25.66 / 0.72 / 34.88 / 55.71 & 29.99 / 0.87 / 17.11 / 19.88 & 27.82 / 0.79 / 28.62 / 45.96 \\
100 & DDNM+ & 25.61 / 0.72 / 34.45 / 54.10 & 30.00 / 0.87 / 17.09 / 20.08 & 27.90 / **0.79** / 28.63 / 46.54 \\
100 & RED-diff & 22.74 / 0.49 / 53.24 / 96.26 & 9.85 / 0.17 / 83.92 / 281.65 & 23.74 / 0.50 / 48.12 / 68.23 \\
100 & **ProjDiff** & **25.73 / 0.73 / 34.12 / 55.03 & **31.09 / 0.89 / 13.65 / 13.69** & **27.91 / 0.79 / 25.11 / 32.27** \\  

Table 1: Noisy restoration on ImageNet with \(=0.05\). The LPIPS metrics are multiplied by \(100\).

### Source separation and partial generation

We evaluate ProjDiff on source separation and partial generation tasks following [45; 46]. The SLACK2100 dataset  is employed and the pre-trained diffusion models are sourced from .

The objective of the source separation task is to separate the audio tracks of four instruments (piano, bass, guitar, and drums) from a mixed audio sequence. We compare ProjDiff against the methods reported in , along with RED-diff  due to its relevance to this work. Performance is assessed using the scale-invariant SDR improvement (SI-SDR\({}_{i}\))  for each instrument as well as their average. Tabel 3 presents the performance on the ISDM model  of different algorithms. ProjDiff demonstrates significant performance improvements compared to other diffusion-based algorithms. The particularly noteworthy result is that ProjDiff surpasses the Demucs method , marking the first instance, to the best of our knowledge, where a diffusion-based separation algorithm significantly outperforms previous SOTA, not to mention that the diffusion model is not specifically designed or trained for the separation task.

The partial generation task aims to generate tracks of other instruments given partial of the instruments tracks while ensuring harmony. We employ the sub-FAD metrics [50; 45] for different partial generation tasks. Note that partial generation poses a weak observation problem as discussed in Section 3.4, thus the proposed Restricted Encoding method is applied for ProjDiff. The results are shown in Table 4. ProjDiff outperforms other algorithms across different partial generation tasks. Additional results and in-depth analysis including ablation studies can be found in Appendix D.

## 5 Conclusion

In this work, we introduce ProjDiff, a versatile inverse problem solver that harnesses the power of diffusion models to capture intricate data prior and denoise simultaneously. By deriving a novel two-variable ELBO as a proxy for the log-prior, we reframe the inverse problems as constrained optimization tasks and address them via the projection gradient method. We meticulously explore the implementation details of ProjDiff and propose refined initialization methods and optional noise schedules to enhance its performance. Through extensive experiments across image restoration tasks and source separation and partial generation tasks, we demonstrate the competitive performance and versatility of ProjDiff in linear, nonlinear, noise-free, and noisy problems. ProjDiff provides insights into better leveraging diffusion priors for inverse problems for future work.

Figure 3: Nonlinear restoration on FFHQ (noise-free).

**Limitations and future work.** One of the possible limitations of ProjDiff is the potential challenge of effectively handling noisy observations with highly intricate functions. Also, its reliance on Gaussian observation noise may limit its applicability to other noise types like Poisson or multiplicative noise. Additionally, ProjDiff needs manual tuning for the step size. Future research could focus on extending ProjDiff to tackle intricate noisy nonlinear observations more effectively, as well as developing adaptive step size strategies to further enhance the performance while reducing the need for manual intervention.