# CADet: Fully Self-Supervised Anomaly Detection

With Contrastive Learning

Charles Guille-Escuret

ServiceNow Research, Mila,

Universite de Montreal

guillech@mila.quebec

&Pau Rodriguez

ServiceNow Research

pau.rodriguez@servicenow.com

&David Vazquez

ServiceNow Research

david.vazquez@servicenow.com

&Ioannis Mitliagkas

Mila, Universite de Montreal,

Canada CIFAR AI chair

ioannis@mila.quebec

&Joao Monteiro

ServiceNow Research

joao.monteiro@servicenow.com

###### Abstract

Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed samples on ImageNet and achieves comparable performance to unseen label detection methods on two challenging benchmarks: ImageNet-O and iNaturalist. Significantly, CADet is fully self-supervised and requires neither labels for in-distribution samples nor access to OOD examples.1

## 1 Introduction

While modern machine learning systems have achieved countless successful real-world applications, handling out-of-distribution (OOD) inputs remains a tough challenge of significant importance. Theproblem is especially acute for high-dimensional problems like image classification. Models are typically trained in a close-world setting but inevitably faced with novel input classes when deployed in the real world. The impact can range from displeasing customer experience to dire consequences in the case of safety-critical applications such as autonomous driving  or medical analysis . Although achieving high accuracy against all meaningful distributional shifts is the most desirable solution, it is particularly challenging. An efficient method to mitigate the consequences of unexpected inputs is to perform anomaly detection, which allows the system to anticipate its inability to process unusual inputs and react adequately.

Anomaly detection methods generally rely on one of three types of statistics: features, logits, and softmax probabilities, with some systems leveraging a mix of these . An anomaly score \(f(x)\) is computed, and then detection with threshold \(\) is performed based on whether \(f(x)>\). The goal of a detection system is to find an anomaly score that efficiently discriminates between in-distribution and out-of-distribution samples. However, the common problem of these systems is that different distributional shifts will unpredictably affect these statistics. Accordingly, detection systems either achieve good performance on specific types of distributions or require tuning on OOD samples. In both cases, their practical use is severely limited. Motivated by these issues, recent work has tackled the challenge of designing detection systems for unseen classes without prior knowledge of the unseen label set or access to OOD samples [68; 63; 66].

We first investigate the use of maximum mean discrepancy two-sample test (MMD)  in conjunction with self-supervised contrastive learning to assess whether two sets of samples have been drawn from the same distribution. Motivated by the strong testing power of this method, we then introduce a statistic inspired by MMD and leveraging contrastive transformations. Based on this statistic, we propose CADet (Contrastive Anomaly Detection), which is able to detect OOD samples from single inputs and performs well on both label-based and adversarial detection benchmarks, _without requiring access to any OOD samples to train or tune the method._

Only a few works have addressed these tasks simultaneously. These works either focus on particular in-distribution data such as medical imaging for specific diseases  or evaluate their performances on datasets with very distant classes such as CIFAR10 , SVHN , and LSUN , resulting in simple benchmarks that do not translate to general real world applications [33; 51].

**Contributions** Our main contributions are as follows:

* We use similarity functions learned by self-supervised contrastive learning with MMD to show that the test sets of CIFAR10 and CIFAR10.1  have different distributions.
* We propose a novel improvement to MMD and show it can also be used to confidently detect distributional shifts when given a small number of samples.
* We introduce CADet, a fully self-supervised method for OOD detection inspired by MMD, and show it outperforms current methods in adversarial detection tasks while performing well on label-based OOD detection.

The outline is as follows: in Section 2, we discuss relevant previous work. Section 3 describes the self-supervised contrastive method based on SimCLRv2  used in this work. Section 4 explores the application of learned similarity functions in conjunction with MMD to verify whether two independent sets of samples are drawn from the same distribution. Section 5 presents CADet and evaluates its empirical performance. Finally, we discuss results and limitations in Section 6.

## 2 Related work

We propose a self-supervised contrastive method for anomaly detection (both unknown classes and adversarial attacks) inspired by MMD. Thus, our work intersects with the MMD, label-based OOD detection, adversarial detection, and self-supervised contrastive learning literature.

**MMD** two-sample test has been extensively studied [19; 67; 18; 62; 8; 29], though it is, to the best of our knowledge, the first time a similarity function trained via contrastive learning is used in conjunction with MMD. Liu et al.  uses MMD with a deep kernel trained on a fraction of the samples to argue that CIFAR10 and CIFAR10.1 have different test distributions. We build upon that work by confirming their finding with higher confidence levels, using fewer samples. Dong et al.  explored applications of MMD to OOD detection.

**Label-based OOD detection methods** discriminate samples that differ from those in the training distribution. We focus on unsupervised OOD detection in this work, _i.e._, we do not assume access to data labeled as OOD. Unsupervised OOD detection methods include density-based [74; 45; 46; 7; 12; 53; 59; 17; 37; 10], reconstruction-based [56; 75; 9; 50; 49; 7], one-class classifiers [57; 54], self-supervised [15; 25; 2; 63], and supervised approaches [34; 23], though some works do not fall into any of these categories [66; 60]. We refer to Yang et al.  for an overview of the many recent works in this field.

**Adversarial detection** discriminates adversarial samples from the original data. Adversarial samples are generated by minimally perturbing actual samples to produce a change in the model's output, such as a misclassification. Most works rely on the knowledge of some attacks for training [1; 43; 13; 39; 76; 48; 40], with the exception of .

**Self-supervised contrastive learning** methods [69; 22; 4; 5] are commonly used to pre-train a model from unlabeled data to solve a downstream task such as image classification. Contrastive learning relies on instance discrimination trained with a contrastive loss  such as infoNCE .

**Contrastive learning for OOD detection** aims to find good representations for detecting OOD samples in a supervised [36; 30] or unsupervised [68; 44; 58] setting. Perhaps the closest work in the literature is CSI , which found SimCLR features to have good discriminative power for unknown classes detection and leveraged similarities between transformed samples in their score. However, this method is not well-suited for adversarial detection. CSI ignores the similarities between different transformations of a same sample, an essential component to perform adversarial detection (see Section 6.2). In addition, CSI scales their score with the norm of input representations. While efficient on samples with unknown classes, it is unreliable on adversarial perturbations, which typically increase representation norms. Finally, we failed to scale CSI to ImageNet.

## 3 Contrastive model

We build our model on top of SimCLRv2  for its simplicity and efficiency. It is composed of an encoder backbone network \(f_{}\) as well as a \(3\)-layer contrastive head \(h_{^{}}\). Given an in-distribution sample \(\), a similarity function _sim_, and a distribution of training transformations \(_{train}\), the goal is to simultaneously maximize

\[_{x;t_{0},t_{1}_{train}}[sim(h_{ ^{}} f_{}(t_{0}(x)),h_{^{}} f_{ }(t_{1}(x)))],\]

and minimize

\[_{x,y;t_{0},t_{1}_{train}}[sim(h _{^{}} f_{}(t_{0}(x)),h_{^{}} f_{ }(t_{1}(y)))],\]

i.e., we want to learn representations in which random transformations of a same example are close while random transformations of different examples are distant.

To achieve this, given an input batch \(\{x_{i}\}_{i=1,,N}\), we compute the set \(\{x_{i}^{(j)}\}_{j=0,1;i=1,,N}\) by applying two transformations independently sampled from \(_{train}\) to each \(x_{i}\). We then compute the embeddings \(z_{i}^{(j)}=h_{^{}} f_{}(x_{i}^{(j)})\) and apply the following contrastive loss:

\[L()=_{i=1,,N}-}{_{j\{1,,N\}}(u _{i,j}+v_{i,j})},\] (1)

where

\[u_{i,j}=e^{sim(z_{i}^{(0)},z_{j}^{(1)})/} v_{i,j}= _{i j}e^{sim(z_{i}^{(0)},z_{j}^{(0)})/}.\]

\(\) is the temperature hyperparameter and \(sim(x,y)=\|y\|_{2}}\) is the _cosine_ similarity.

**Hyperparameters:** We follow as closely as possible the setting from SimCLRv2 with a few modifications to adapt to hardware limitations. In particular, we use the LARS optimizer  with learning rate \(1.2\), momentum \(0.9\), and weight decay \(10^{-4}\). Iteration-wise, we scale up the learning rate for the first \(40\) epochs linearly, then use an iteration-wise cosine decaying schedule until epoch \(800\), with temperature \(=0.1\). We train on \(8\)\(V100\) GPUs with an accumulated batch size of \(1024\). We compute the contrastive loss on all batch samples by aggregating the embeddings computed by each GPU. We use synchronized BatchNorm and fp32 precision and do not use a memory buffer. We use the same set of transformations, i.e., Gaussian blur and horizontal flip with probability \(0.5\), colorjittering with probability \(0.8\), random crop with scale uniformly sampled in \([0.08,1]\), and grayscale with probability \(0.2\).

For computational simplicity and comparison with previous work, we use a ResNet50 encoder architecture with final features of size \(2048\). Following SimCLRv2, we use a three-layer fully connected contrastive head with hidden layers of width \(2048\) using ReLU activation and batchNorm and set the last layer projection to dimension \(128\). For evaluation, we use the features produced by the encoder without the contrastive head. We do not, at any point, use supervised fine-tuning.

## 4 MMD two-sample test

The **Maximum Mean Discrepancy (MMD)** is a statistic used in the MMD two-sample test to assess whether two sets of samples \(S_{}\) and \(S_{}\) are drawn from the same distribution. It estimates the expected difference between the intra-set distances and the across-sets distances.

**Definition 4.1** (Gretton et al. ).: Let \(k:\) be the kernel of a reproducing Hilbert space \(_{k}\), with feature maps \(k(,x)_{k}\). Let \(X,X^{}\) and \(Y,Y^{}\). Under mild integrability conditions,

\[MMD(,;_{k}) :=_{f,\|f\|_{_{k} 1}}| [f(X)]-[f(Y)]|\] (2) \[=[k(X,X^{})+k(Y,Y^{})-2k(X,Y) ]}.\] (3)

Given two sets of \(n\) samples \(S_{}=\{X_{i}\}_{i n}\) and \(S_{}=\{Y_{i}\}_{i n}\), respectively drawn from \(\) and \(\), we can compute the following unbiased estimator :

\[_{u}^{2}(S_{},S_{};k):=_ {i j}(k(X_{i},X_{j})+k(Y_{i},Y_{j})-k(X_{i},Y_{j})-k(Y_{i},X_{j})).\] (4)

Under the null hypothesis \(_{0}:=\), this estimator follows a normal distribution of mean \(0\). Its variance can be directly estimated , but it is simpler to perform a permutation test as suggested in Sutherland et al. , which directly yields a \(p\)-value for \(_{0}\). The idea is to use random splits \(X,Y\) of the input sample sets to obtain \(n_{perm}\) different (though not independent) samplings of \(_{u}^{2}(X,Y;k)\), which approximate the distribution of \(_{u}^{2}(S_{},S_{};k)\) under the null hypothesis.

Liu et al.  trains a deep kernel to maximize the test power of the MMD two-sample test on a training split of the sets of samples to test. We propose instead to use our learned similarity function without any fine-tuning. Note that we return the \(p\)-value \(+1}(1+_{i=1}^{n_{perm}}(p_{i} est ))\) instead of \(}_{i=1}^{n_{perm}}(p_{i} est)\). Indeed, under the null hypothesis \(=\), \(est\) and \(p_{i}\) are drawn from the same distribution, so for \(j\{0,1,,n_{perm}\}\), the probability for \(est\) to be smaller than exactly \(j\) elements of \(\{p_{i}\}\) is \(+1}\). Therefore, the probability that \(j\) elements or less of \(\{p_{i}\}_{i}\) are larger than \(est\) is \(_{i=0}^{j}+1}=+1}\). While this change has a small impact for large values of \(n_{perm}\), it is essential to guarantee that we indeed return a correct \(p\)-value. Notably, the algorithm of Liu et al.  has a probability \(}>0\) to return an output of \(0.00\) even under the null hypothesis.

Additionnally, we propose a novel version of MMD called MMD-CC (MMD with Clean Calibration). Instead of computing \(p_{i}\) based on random splits of \(S_{} S_{}\), we require as input two disjoint sets of samples drawn from \(\) and compute \(p_{i}\) based on random splits of \(S_{}^{(1)} S_{}^{(2)}\) (see Algorithm 1). This change requires to use twice as many samples from \(\), but reduces the variance induced by the random splits of \(S_{} S_{}\), which is significant when the number of samples is small. Note that \(S_{}^{(1)}\), \(S_{}^{(2)}\) and \(S_{}\) must always have the same size. Under the null hypothesis, \(=\), \(S_{}^{(1)}\), \(S_{}^{(2)}\) and \(S_{}\) are identically distributed, so the \(p_{i}\) conserve the same distribution as for MMD (this is not the case when \(\), hence the difference in testing power). Thus, the validity of MMD-CC follows from the validity of MMD.

### Distribution shift between CIFAR-10 and CIFAR-10.1 test sets

After years of evaluation of popular supervised architectures on the test set of CIFAR-10 , modern models may overfit it through their hyperparameter tuning and structural choices. CIFAR-10.1  was collected to verify the performances of these models on a _truly_ independent sample from the training distribution. The authors note a consistent drop in accuracy across models and suggest it could be due to a distributional shift, though they could not demonstrate it. Recent work  leveraged the two-sample test to provide strong evidence of distributional shifts between the test sets of CIFAR-10 and CIFAR-10.1. We run MMD-CC and MMD two-sample tests for \(100\) different samplings of \(S_{}^{(1)},S_{}^{(2)},S_{}\), using every time \(n_{perm}=500\), and rejecting \(_{0}\) when the obtained \(p\)-value is below the threshold \(=0.05\). We also report results using cosine similarity applied to the features of supervised models as a comparative baseline. We report the results in Table 1 for a range of sample sizes. We compare the results to three competitive methods reported in Liu et al. : Mean embedding (ME) [8; 29], MMD-D , and C2ST-L . Finally, we show in Figure 0(a) the ROC curves of the proposed model for different sample sizes.

Other methods in the literature do not use external data for pre-training, as we do with ImageNet, which makes a fair comparison difficult. However, it is noteworthy that our learned similarity can very confidently distinguish samples from the two datasets, even in settings with fewer samples available. Furthermore, while we achieve excellent results even with a supervised network, our model trained with contrastive learning outperforms the supervised alternative very significantly. We note however that with such high number of samples available, MMD-CC performs slightly worse than MMD. Finally, we believe the confidence obtained with our method decisively concludes that CIFAR10 and CIFAR10.1 have different distributions, which is likely the primary explanation for the significant drop in performances across models on CIFAR10.1, as conjectured by Recht et al. . The difference in distribution between CIFAR10 and CIFAR10.1 is neither based on label set nor adversarial perturbations, making it an interesting task.

### Detection of distributional shifts from small number of samples

Given a small set of samples with potential unknown classes or adversarial attacks, we can similarly use the two-sample test with our similarity function to verify whether these samples are in-distribution . In particular, we test for samples drawn from ImageNet-O, iNaturalist, and PGD perturbations, with sample sizes ranging from \(3\) to \(20\). For these experiments, we sample \(S_{}^{(1)}\) and \(S_{}^{(2)}\)\(5000\) times across all of ImageNet's validation set and compare their MMD and MMD-CC estimators to the one obtained from \(S_{}\) and \(S_{}\). We report in Table 2 the AUROC of the resulting detection and compare it to the ones obtained with a supervised ResNet50 as the baseline.

Such a setting where we use several samples assumed to be drawn from a same distribution to perform detection is uncommon, and we are not aware of prior baselines in the literature. Despite using very few samples (\(3 n 20\)), our method can detect OOD samples with high confidence. We observe particularly outstanding performances on iNaturalist, which is easily explained by the fact that the subset we are using (cf. Section 1) only contains plant species, logically inducing an abnormally high similarity within its samples. Furthermore, we observe that MMD-CC performs significantly better than MMD, especially on detecting samples perturbed by PGD.

Although our method attains excellent detection rates for sufficient numbers of samples, the requirement to have a set of samples all drawn from the same distribution to perform the test makes it unpractical for real-world applications. In the following section, we present CADet, a detection method inspired by MMD but applicable to anomaly detection with single inputs.

## 5 CADet: Contrastive Anomaly Detection

While the numbers in Section 4 demonstrate the reliability of two-sample test coupled with contrastive learning for identifying distributional shifts, it requires several samples from the same distribution, which is generally unrealistic for practical detection purposes. This section presents CADet, a method to leverage contrastive learning for OOD detection on single samples.

Self-supervised contrastive learning trains a similarity function \(s\) to maximize the similarity between augmentations of the same sample, and minimize the similarity between augmentations of different samples. Given an input sample \(x^{test}\), we propose to leverage this property to perform anomaly detection on \(x^{test}\), taking inspiration from MMD two-sample test. More precisely, given a transformation distribution \(_{val}\), we compute \(n_{trs}\) random transformations \(x_{t}^{test}\) of \(x_{test}\), as well as \(n_{trs}\) random transformations \(x_{i}^{(k)}\) on each sample \(x^{(k)}\) of a held-out validation dataset \(X_{val}^{(1)}\). We then compute the

    &  &  &  \\  n\_samples & 3 & 5 & 10 & 20 & 3 & 5 & 10 & 20 & 3 & 5 & 10 & 20 \\  MMD + SimCLRv2 & 64.3 & 72.4 & 86.9 & 97.6 & 88.3 & 97.6 & **99.5** & **99.5** & 35.2 & 53.8 & 86.6 & 98.8 \\ MMD-CC + SimCLRv2 & **65.3** & **73.2** & **88.0** & **97.7** & 95.4 & 99.2 & **99.5** & **99.5** & **70.5** & **84.0** & **96.6** & **99.5** \\ MMD + Supervised & 62.7 & 69.7 & 83.2 & 96.4 & 91.8 & 98.7 & **99.5** & **99.5** & 20.0 & 22.5 & 33.0 & 57.5 \\ MMD-CC + Supervised & 62.6 & 71.0 & 85.5 & 97.2 & **98.0** & **99.5** & **99.5** & **99.5** & 57.4 & 61.3 & 70.5 & 85.8 \\   

Table 2: AUROC for detection using two-sample test on 3 to 20 samples drawn from ImageNet and from ImageNet-O, iNaturalist or PGD perturbations, with a ResNet50 backbone.

    & n=2000 & n=1000 & n=500 & n=200 & n=100 & n=50 \\  ME  & 0.588 & - & - & - & - & - \\ C2ST-L  & 0.529 & - & - & - & - & - \\ MMD-D  & 0.744 & - & - & - & - & - \\ MMD + SimCLRv2 (ours) & **1.00** & **1.00** & **0.997** & **0.702** & **0.325** & **0.154** \\ MMD-CC + SimCLRv2 (ours) & **1.00** & **1.00** & **0.997** & 0.686 & 0.304 & 0.150 \\ MMD + Supervised (ours) & **1.00** & **1.00** & 0.884 & 0.305 & 0.135 & 0.103 \\ MMD-CC + Supervised (ours) & **1.00** & **1.00** & 0.870 & 0.298 & 0.131 & 0.096 \\   

Table 1: Average rejection rates of \(_{0}\) on CIFAR-10 vs CIFAR-10.1 for \(=0.05\) across different sample sizes \(n\), using a ResNet50 backbone.

intra-similarity and out-similarity:

\[m^{in}(x^{test}):=s(x_{i}^{test},x_{j}^{ test})}{n_{trs}(n_{trs}+1)}, m^{out}(x^{test}):= X_{val}^{(1)}}_{i,j}s(x_{i}^{test},x_{j}^{(k)})} {n_{trs}^{2}|X_{val}^{(1)}|}.\] (5)

We finally define the following statistic to perform detection:

\[score_{C}:=m^{in}+ m^{out}.\] (6)

Intuition:note that while CADet is inspired by MMD, it has some significant differences. \(m_{in}\) is computed across transformations of a same sample, while \(m_{out}\) is computed across transformations and across samples. Therefore, even under the null hypothesis, \(m_{in}\) and \(m_{out}\) cannot be expected to have the same distribution. Besides, both score can be expected to be smaller on OOD samples. Indeed, OOD samples can be expected to have representations further from clean data in average, thus decreasing \(m_{out}\). Furthermore, since the self-supervised model was trained to maximize the similarity between transformations of a same sample, it is reasonable to expect that the model will perform this task better in-distribution than out-of-distribution, and thus that \(m_{in}\) will be higher in-distribution than on OOD data. This intuition justifies adding \(m_{in}\) to \(m_{out}\), instead of the subtraction operated by MMD. Finally, note that we do not consider the intra-similarity between validation samples: the validation is fixed, so it is necessarily a constant.

Calibration:since we do not assume knowledge of OOD samples, it is difficult _a priori_ to tune \(\), although crucial to balance information between intra-sample similarity and cross-sample similarity. As a workaround, we calibrate \(\) by equalizing the variance between \(m^{in}\) and \( m^{out}\) on a second set of validation samples \(X_{val}^{(2)}\):

\[=(x),x X_{val}^{(2)}\}}{Var\{m^{out}(x),x X _{val}^{(2)}\}}}.\] (7)

It is important to note that the calibration of \(\) is essential, because \(m_{in}\) and \(m_{out}\) can have very different means and variances. Since \(m_{out}\) measures the out-similarity between transformations of _different_ samples, it can be expected to be low in average, with high variance. On the contrary, \(m_{in}\) measures the similarity between transformations of a _same_ sample, and can thus be expected to have higher mean and lower variance, which is confirmed in Table 5. Therefore, simply setting \(=1\) would likely results in one of the score dominating the other.

Rather than evaluating the false positive rate (FPR) for a range of possible thresholds \(\), we use the hypothesis testing approach to compute the p-value:

\[p_{v}(x^{test})=^{(2)}\,s.t.\,score_{C}(x)<score_{C}(x^{ test})\}|+1}{|\{X_{val}^{(2)}\}|+1}.\] (8)

The full pseudo-codes of the calibration and testing steps are given ins Appendix A. Setting a threshold \(p\) for \(p_{v}\) will result in a FPR of mean \(p\), with a variance dependant of \(|X_{val}^{(2)}|\).

Section 5.1 further describes our experimental setting.

### Experiments

For all evaluations, we use the same transformations as SimCLRv2 except color jittering, Gaussian blur and grayscaling. We fix the random crop scale to \(0.75\). We use \(|\{X_{val}^{(2)}\}|=2000\) in-distribution samples, \(|\{X_{val}^{(1)}\}|=300\) separate samples to compute cross-similarities, and \(50\) transformations per sample. We pre-train a ResNet50 with ImageNet as in-distribution.

**Unknown classes detection:** we use two challenging benchmarks for the detection of unknown classes. iNaturalist using the subset in  made of plants with classes that do not intersect ImageNet. Wang et al.  noted that this dataset is particularly challenging due to proximity of its classes. We also evaluate on ImageNet-O ; explicitly designed to be challenging for OOD detection with ImageNet as in-distribution. We compare to recent works and report the AUROC scores in Table 3.

For Mahalanobis , since tuning on ood samples is not permitted, we use as score the mahalanobis score of the last layer as done in ViM . Furthermore, to isolate the effect of the training scheme from the detection method, we present in Appendix B the performance of previous methods using the same self-supervised backbone as CADet, in conjunction with a trained linear layer.

**Adversarial detection:** for adversarial detection, we generate adversarial attacks on the validation partition of ImageNet against a pre-trained ResNet50 using three popular attacks: PGD , CW , and FGSM . We follow the tuning suggested by Abusnaina et al. , i.e. PGD: norm \(L_{}\), \(=0.02\), step size \(0.002\), 50 iterations; CW: norm \(L_{2}\), \(=0.10\), learning rate of \(0.03\), and \(50\) iterations; FGSM: norm \(L_{}\), \(=0.05\). We compare our results with ODIN , which achieves good performances in Lee et al.  despite not being designed for adversarial detection, Feature Squeezing (FS) , Local Intrinsic Dimensionality (LID) , Hu et al. , and Mahalanobis  (using the mahalanobis score of the last layer as for unknown classes detection). Most existing adversarial detection methods assume access to adversarial samples during training (see Section 2). We additionally propose a modification to Hu et al.  to perform auto-calibration based on the mean and variance of the criterions on clean data, similarly to CADet's calibration step. We report the AUROC scores in Table 4 and illustrate them with ROC curves against each anomaly type in Figure 0(b).

   & Tuned on Adv & Training & PGD & CW & FGSM & Average \\  ODIN  & Yes & Supervised & 62.30 & 60.29 & 68.10 & 63.56 \\  & & Contrastive & 59.91 & 60.23 & 64.99 & 61.71 \\ FS  & Yes & Supervised & 91.39 & 87.77 & 95.12 & 91.43 \\  & & Contrastive & 94.71 & 88.00 & 95.98 & 92.90 \\ LID  & Yes & Supervised & 93.13 & 91.08 & 93.55 & 92.59 \\  & & Contrastive & 91.94 & 89.50 & 93.56 & 91.67 \\ Hu  & Yes & Supervised & 84.31 & 84.29 & 77.95 & 82.18 \\  & & Contrastive & 94.80 & 95.19 & 78.18 & 89.39 \\ Hu  + self-calibration & **No** & Supervised & 66.40 & 59.58 & 71.02 & 65.67 \\  & & Contrastive & 75.69 & 75.74 & 69.20 & 73.54 \\ Mahalanobis  & **No** & Supervised & 92.71 & 91.01 & 96.92 & 93.55 \\  & & Contrastive & 84.14 & 83.78 & 79.90 & 82.61 \\ 
**CADet (ours)** & **No** & Supervised & 75.25 & 71.02 & 83.45 & 76.57 \\  & & Contrastive & **94.88** & **95.93** & **97.56** & **96.12** \\  

Table 4: AUROC for adversarial detection on ImageNet against PGD, CW and FGSM attacks, with ResNet50 backbone.

   & Training & iNaturalist & ImageNet-O & Average \\  MSP  & & 88.58 & 56.13 & 72.36 \\ Energy  & & 80.50 & 53.95 & 67.23 \\ ODIN  & & 86.48 & 52.87 & 69.68 \\ MaxLogit  & & 86.42 & 54.39 & 70.41 \\ KL Matching  & Supervised & 90.48 & 67.00 & 78.74 \\ ReAct  & & 87.27 & 68.02 & 77.65 \\ Mahalanobis  & & 89.48 & 80.15 & 84.82 \\ Residual  & & 84.63 & 81.15 & 82.89 \\ ViM  & & 89.26 & 81.02 & **85.14** \\ 
**CADet (ours)** &  Supervised \\ Self-supervised \\ (contrastive) \\  & **95.28** & 70.73 & 83.01 \\  & 
 Self-supervised \\ (contrastive) \\  & 83.42 & **82.29** & 82.86 \\  

Table 3: AUROC for OOD detection on ImageNet-O and iNaturalist with ResNet50 backbone.

## 6 Discussion

### Results

CADet performs particularly well on adversarial detection, surpassing alternatives by a wide margin. We argue that self-supervised contrastive learning is a suitable mechanism for detecting classification attacks due to its inherent label-agnostic nature. Interestingly, Hu et al.  also benefits from contrastive pre-training, achieving much higher performances than with a supervised backbone. However, it is very reliant on calibrating on adversarial samples, since we observe a significant drop in performances with auto-calibration.

We explain the impressive performances of CADet on adversarial detection by the fact that adversarial perturbations will negatively affect the capability of the self-supervised model to match different transformations of the image. In comparison, the model has high capability on clean data, having been trained specifically with that objective.

While CADet does not outperform existing methods on label-based OOD detection, it performs comparably, an impressive feat of generality considering adversarially perturbed samples and novel labels have significantly different properties, and that CADet does not require any tuning on OOD samples.

Notably, applying CADet to a supervised network achieves state-of-the-art performances on iNaturalist with ResNet50 architecture, suggesting CADet can be a reasonable standalone detection method on some benchmarks, independently of contrastive learning. In addition, the poor performances of the supervised network on ImageNet-O and adversarial attacks show that contrastive learning is essential to address the trade-off between different type of anomalies.

Overall, our results show CADet achieves an excellent trade-off when considering both adversarial and label-based OOD samples.

### The predictive power of in-similarities and out-similarities

Table 5 reports the mean and variance of \(m^{in}\) and \(m^{out}\), and the rescaled mean \( m^{out}\) across all distributions. Interestingly, we see that out-similarities \(m^{out}\) better discriminate label-based OOD samples, while in-similarities \(m^{in}\) better discriminate adversarial perturbations. Combining in-similarities and out-similarities is thus an essential component to simultaneously detect adversarial perturbations and unknown classes.

### Limitations

**Computational cost:** To perform detection with CADet, we need to compute the features for a certain number of transformations of the test sample, incurring significant overhead. Figure 2 shows that reducing the number of transformations to minimize computational cost may not significantly affect performances. While the calibration step can seem expensive, it only required less than 10 minutes on a single A100 GPU, which is far from prohibitive. Moreover, it only needs to be run once for a given in-distribution. The

Figure 2: AUROC score of CADet against the number of transformations.

   & & IN-Ix & iNat & IN-O & PGD & CW & FGSM \\   & \(m^{out}\) & 0.972 & 0.967 & 0.969 & 0.954 & 0.954 & 0.948 \\  & \(m^{out}\) & 0.321 & 0.296 & 0.275 & 0.306 & 0.302 & 0.311 \\  & \( m^{out}\) & 0.071 & 0.066 & 0.061 & 0.068 & 0.067 & 0.069 \\   & \(m^{out}\) & 8.3e-05 & 7.8e-05 & 1.0e-04 & 2.1e-04 & 2.0e-04 & 2.1e-04 \\  & \(m^{out}\) & 1.7e-03 & 7.0e-04 & 2.3e-03 & 7.0e-04 & 1.7e-03 & 1.1e-03 \\  

Table 5: Mean and variance of \(m^{in}\) and \(m^{out}\).

coefficient \(\) and scores are all one-dimensional values that can be easily stored, and we purposely use a small number of validation samples \(|\{X^{(1)}_{val}\}|=300\) to make their embedding easy to memorize.

**Architecture scale:** as self-supervised contrastive learning is computationally expensive, we only evaluated our method on a ResNet50 architecture. In Wang et al. , the authors achieve significantly superior performances when using larger, recent architectures. The performances achieved with a ResNet50 are insufficient for real-world usage, and the question of how our method would scale to larger architectures remains open.

**Adaptive attacks:** we do not evaluate on adaptive attacks, and as such our detection method should be considered vulnerable to them. However, as shown in Tramer et al. , modern detection methods are typically weak to adaptive attacks. Since robustness to _any_ adaptive attack is impossible to prove, we contend this is a limitation of detection methods in general, unspecific to CADet.

### Future directions

While spurious correlations with background features are a problem in supervised learning, it is aggravated in self-supervised contrastive learning, where background features are highly relevant to the training task. We conjecture the poor performances of CADet on iNaturalist OOD detection are explained by the background similarities with ImageNet images, obfuscating the differences in relevant features. A natural way to alleviate this issue is to incorporate background transformations to the training pipeline, as was successfully applied in Ma et al. . This process would come at the cost of being unable to detect shifts in background distributions, but such a case is generally less relevant to deployed systems. We leave to future work the exploration of how background transformations could affect the capabilities of CADet.

### Conclusion

We have presented CADet, a method for both OOD and adversarial detection based on self-supervised contrastive learning. CADet achieves an excellent trade-off in detection power across different anomaly types, without requiring tuning on OOD samples. Additionally, we discussed how MMD could be leveraged with contrastive learning to assess distributional discrepancies between two sets of samples, and proved with high confidence that CIFAR10 and CIFAR10.1 have different distributions.