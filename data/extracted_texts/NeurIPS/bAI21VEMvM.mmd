# March: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data

Pratik Karmakar

School of Computing, National University of Singapore

CNRS@CREATE Ltd, 1 Create Way, Singapore

pratik.karmakar@u.nus.edu

A significant portion of the work has been done as a part of P. Karmakar's masters in Ramakrishna Mission Vivekananda Educational and Research Institute, Belur, India.

Debabrota Basu

Equipe Scool, Univ. Lille, Inria, CNRS, Centrale Lille

UMR 9189- CRIStAL, F-59000 Lille, France

debabrota.basu@inria.fr

###### Abstract

We study design of black-box model extraction attacks that can _send minimal number of queries from_ a _publicly available dataset_ to a target ML model through a predictive API with an aim _to create an informative and distributionally equivalent replica_ of the target. First, we define _distributionally equivalent_ and _Max-Information model extraction_ attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the token models. This leads to _an active sampling-based query selection algorithm_, March, which is _model-oblivious_. Then, we evaluate March on different text and image data sets, and different models, including CNNs and BERT. March extracts models that achieve \( 60-95\%\) of true model's accuracy and uses \( 1,000-8,500\) queries from the publicly available datasets, which are different from the private training datasets. Models extracted by March yield prediction distributions, which are \( 2-4\) closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to 84-96\(\%\) accuracy under membership inference attacks. Experimental results validate that March is _query-efficient_, and capable of performing task-accurate, high-fidelity, and informative model extraction.

## 1 Introduction

In recent years, Machine Learning as a Service (MLaaS) is widely deployed and used in industries. In MLaaS , an ML model is trained remotely on a private dataset, deployed in a Cloud, and offered for public access through a prediction API, such as Amazon AWS, Google API, Microsoft Azure. An API allows an user, including a potential _adversary_, _to send queries to the ML model and fetch corresponding predictions_. Recent works have shown such models with public APIs can be stolen, or extracted, by designing black-box model extraction attacks . In model extraction attacks, an adversary queries the target model with a query dataset, which might be same or different than the private dataset, collects the corresponding predictions from the target model, and builds areplica model of the target model. The goal is to construct a model which is almost-equivalent to the target model over input space .

Often, ML models are proprietary, guarded by IP rights, and expensive to build. These models might be trained on datasets which are expensive to obtain  and consist of private data of individuals . Also, extracted models can be used to perform other privacy attacks on the private dataset used for training, such as membership inference . Thus, understanding susceptibility of models accessible through MLaaS presents an important conundrum. This motivates us to _investigate black-box model extraction attacks while the adversary has no access to the private data or a perturbed version of it_. Instead, _the adversary uses a public dataset to query the target model_.

Based on the above, we propose a model extraction based on the query-efficient black-box model extraction based on the query-efficient black-box model extraction poses a tension between the number of queries sent to the target model and the accuracy of extracted model . With more queries and predictions, an adversary can build a better replica. But querying an API too much can be expensive, as each query incurs a monetary cost in MLaaS. Also, researchers have developed algorithms that can detect adversarial queries, when they are not well-crafted or sent to the API in large numbers . Thus, designing a query-efficient attack is paramount for practical deployment. Also, it exposes how more information can be leaked from a target model with less number of interactions.

In this paper, _we investigate effective definitions of efficiency of model extraction and corresponding algorithm design for query-efficient black-box model extraction attack with public data, which is oblivious to deployed model and applicable for any datatype._

**Our contributions.** Our investigation yields three contributions.

_1. Formalism: Distributional equivalence and Max-Information extraction._ Often, the ML models, specifically classifiers, are stochastic algorithms. They also include different elements of randomness during training. Thus, rather than focusing on equivalence of extracted and target models in terms of a fixed dataset or accuracy on that dataset , we propose _a distributional notion of equivalence._ We propose that if the joint distribution induced by a query generating distribution and corresponding prediction distribution due to both the target and the extracted models are same, they will be called distributionally equivalent (Sec. 3). Another proposal is to reinforce the objective of the attack, i.e. to extract as much information as possible from the target model. This allows us to formulate the Max-Information attack, where the adversary aims to maximise the mutual information between the extracted and target models' distributions. _Our hypothesis is that if we can extract the predictive distribution of a model, it would be enough to replicate other properties of the model (e.g. accuracy) and also to run other attacks (e.g. membership inference), rather than designing specific attacks to replicate the task accuracy or the model weights_. We show that both the attacks can be performed by sequentially solving a single variational optimisation  problem (Eqn. (6)).

_2. Algorithm: Adaptive query selection for extraction with_Marich. We propose an algorithm, March (Sec. 4), that optimises the objective of the variational optimisation problem (Eqn. (6)). Given an extracted model, a target model, and previous queries, March adaptively selects a batch of queries enforcing this objective. Then, it sends the queries to the target model, collects the predictions (i.e. the class predicted by target model), and uses them to further train the extracted model (Algo. 1). In order to select the most informative set of queries, it deploys three sampling strategies in cascade. These strategies select: a) the most informative set of queries, b) the most diverse set of queries in the first selection, and c) the final subset of queries where the target and extracted models mismatch the most. Together these strategies allow March to select a small subset of queries that both maximise the information leakage, and align the extracted and target models (Fig. 1).

Figure 1: Black-box model extraction with March.

_3. Experimental analysis._ We perform extensive the most for a given modelevaluation with both image and text datasets, and diverse model classes, such as Logistic Regression (LR), ResNet, CNN, and BERT (Sec. 5). Leveraging March's model-obliviousness, we even extract a ResNet trained on CIFAR10 with a CNN and out-of-class queries from ImageNet. Our experimental results validate that March extracts more accurate replicas of the target model and high-fidelity replica of the target's prediction distributions in comparison to existing active sampling algorithms. While March uses a small number of queries (\( 1k-8.5k\)) selected from publicly available query datasets, the extracted models yield accuracy comparable with the target model while encountering a membership inference attack. This shows that March can extract alarmingly informative models query-efficiently. Additionally, as March can extract the true model's predictive distribution with a different model architecture and a mismatched querying dataset, it allows us to design a model-oblivious and dataset-oblivious approach to attack.

### Related works

**Taxonomy of model extraction.** Black-box model extraction (or model stealing or model inference) attacks aim to _replicate_ of a target ML model, commonly classifiers, deployed in a remote service and accessible through a public API . The replication is done in such a way that the extracted model achieves one of the three goals: a) _accuracy close to that of the target model on the private training data_ used to train the target model, b) _maximal agreement in predictions_ with the target model on the _private training data_, and c) maximal agreement in prediction with the target model over the _whole input domain_. Depending on the objective, they are called _task accuracy_, _fidelity_, and _functional equivalence model extractions_, respectively . Here, _we generalise these three approaches using a novel definition of distributional equivalence and also introduce a novel information-theoretic objective of model extraction which maximises the mutual information between the target and the extracted model over the whole data domain._

**Frameworks of attack design.** Following , researchers have proposed multiple attacks to perform one of the three types of model extraction. The attacks are based on two main approaches: _direct recovery_ (target model specific)  and _learning_ (target model specific/oblivious). The learning-based approaches can also be categorised into supervised learning strategies, where the adversary has access to both the true labels of queries and the labels predicted by the target model , and online active learning strategies, where the adversary has only access to the predicted labels of the target model, and actively select the future queries depending on the previous queries and predicted labels . _As query-efficiency is paramount for an adversary while attacking an API to save the budget and to keep the attack hidden and also the assumption of access true label from the private data is restrictive, we focus on designing an online and active learning-based attack strategy that is model oblivious._

**Types of target model.** While  focus on performing attacks against linear models, all others are specific to neural networks  and even a specific architecture . In contrast, March is _capable of attacking both linear models and neural networks._ Additionally, March is _model-oblivious_, i.e. it can attack one model architecture (e.g. ResNet) using a different model architecture (e.g. CNN).

**Types of query feedback.** Learning-based attacks often assume access to either the probability vector of the target model over all the predicted labels , or the gradient of the last layer of the target neural network , which are hardly available in a public API. In contrast, following , _we assume access to only the predicted labels of the target model for a set of queries_, which is always available with a public API. Thus, experimentally, we cannot compare with existing active sampling attacks requiring access to the whole prediction vector , and thus, compare with a wide-range of active sampling methods that can operate only with the predicted label, such as \(K\)-center sampling, entropy sampling, least confidence sampling, margin sampling etc. . Details are in Appendix C.

**Choices of public datasets for queries.** There are two approaches of querying a target model: _data-free_ and _data-selection based_. In _data-free attacks_, the attacker begins with noise. The informative queries are generated further using a GAN-like model fed with responses obtained from an API . Typically, it requires almost a million queries to the API to start generating sensible query data (e.g. sensible images that can leak from a model trained on CIFAR10). But since one of our main focus is query-efficiency, we focus on _data-selection based attacks_, where an adversary has access to a query dataset to select the queriesfrom and to send it to the target model to obtain predicted labels. In literature, researchers assume three types of query datasets: _synthetically generated samples_ , _adversarially perturbed private (or task domain) dataset_ , and _publicly available (or out-of-task domain) dataset_. As we do not want to restrict March to have access to the knowledge of the private dataset or any perturbed version of it, _we use publicly available datasets, which are different than the private dataset_. To be specific, we only assume whether we should query the API with images, text, or tabular data and not even the identical set of labels. For example, we experimentally attack models trained on CIFAR10 with ImageNet queries having different classes.

Further discussions on related active sampling algorithms and distinction of March with the existing works are deferred to Appendix C.

## 2 Background: Classifiers, model extraction, membership inference attacks

Before proceeding to the details, we present the fundamentals of a classifier in ML, and two types of inference attacks: Model Extraction (ME) and Membership Inference (MI).

**Classifiers.** A classifier in ML  is a function \(f:\) that maps a set of input features \(\) to an output \(Y\).2 The output space is a finite set of classes, i.e. \(\{1,,k\}\). Specifically, a classifier \(f\) is a parametric function, denoted as \(f_{}\), with parameters \(^{d}\), and is trained on a dataset \(^{T}\), i.e. a collection of \(n\) tuples \(\{(_{i},y_{i})\}_{i=1}^{n}\) generated IID from an underlying distribution \(\). Training implies that given a model class \(=\{f_{}|\}\), a loss function \(l:_{ 0}\), and training dataset \(^{T}\), we aim to find the optimal parameter \(^{*}*{arg\,min}_{}_{i=1}^{n} l(f_{}(_{i}),y_{i})\). We use cross-entropy, i.e. \(l(f_{}(_{i}),y_{i})-y_{i}(f_{}( _{i}))\), as the loss function for classification.

**Model extraction attack.** A model extraction attack is an inference attack where an adversary aims to steal a target model \(f^{T}\) trained on a private dataset \(^{T}\) and create another replica of it \(f^{E}\). In the black-box setting that we are interested in, the adversary can only query the target model \(f^{T}\) by sending queries \(Q\) through a publicly available API and to use the corresponding predictions \(\) to construct \(f^{E}\). The goal of the adversary is to create a model which is either (a) as similar to the target model as possible for all input features, i.e. \(f^{T}(x)=f^{E}(x)\; x\) or (b) predicts labels that has maximal agreement with that of the labels predicted by the target model for a given data-generating distribution, i.e. \(f^{E}=*{arg\,min}_{x_{}}[l(f^{E}(x),f^{T}(x))]\). The first type of attacks are called the functionally equivalent attacks. The later family of attacks is referred as the fidelity extraction attacks. The third type of attacks aim to find an extracted model \(f^{E}\) that achieves maximal classification accuracy for the underlying private dataset used to train the \(f^{T}\). These are called task accuracy extraction attacks . In this paper, _we generalise the first two type of attacks by proposing the distributionally equivalent attacks and experimentally show that it yields both task accuracy and fidelity._

**Membership inference attack.** Another popular family of inference attacks on ML models is the Membership Inference (MI) attacks . In MI attack, given a private (or member) dataset \(^{T}\) to train \(f^{T}\) and another non-member dataset \(S\) with \(|^{T} S|\), the goal of the adversary is to infer whether any \(x\) is sampled from the member dataset \(^{T}\) or the non-member dataset \(S\). Effectiveness of an MI attacks can be measured by its accuracy of MI, i.e. the total fraction of times the MI adversary identifies the member and non-member data points correctly. Accuracy of MI attack on the private data using \(f^{E}\) rather than \(f^{T}\) is considered as a measure of effectiveness of the extraction attack . We show that the model \(f^{E}\) extracted using March allows us to obtain similar MI accuracy as that obtained by directly attacking the target model \(f^{T}\) using even larger number of queries. This validates that _the model \(f^{E}\) by March in a black-box setting acts as an information equivalent replica of the target model \(f^{T}\)._

## 3 Distributional equivalence and Max-Information model extractions

In this section, we introduce the distributionally equivalent and Max-Information model extractions. We further reduce both the attacks into a variational optimisation problem.

**Definition 3.1** (Distributionally equivalent model extraction).: For any query generating distribution \(^{Q}\) over \(^{d}\), an extracted model \(f^{E}:^{d} Y\) is distributionally equivalent to a target model \(f^{T}:^{d} Y\), if the joint distributions of input features \(Q^{d}^{Q}\) and predicted labels induced by both the models are same almost surely. This means that for any divergence \(D\), two distributionally equivalent models \(f^{E}\) and \(f^{T}\) satisfy \(D((f^{T}(Q),Q)\|(f^{E}(Q),Q))=0\;\;^{Q}\).

To ensure query-efficiency in distributionally equivalent model extraction, an adversary aims to choose a query generating distribution \(^{Q}\) that minimises it further. If we assume that the extracted model is also a parametric function, i.e. \(f^{E}_{}\) with parameters \(\), we can solve the query-efficient distributionally equivalent extraction by computing

\[(^{*}_{},^{Q}_{})*{ arg\,min}_{}*{arg\,min}_{^{Q}}D((f^{T}_{ ^{*}}(Q),Q)\|(f^{E}_{}(Q),Q)).\] (1)

Equation (1) allows us to choose a different class of models with different parametrisation for extraction till the joint distribution induced by it matches with that of the target model. For example, the extracted model can be a logistic regression or a CNN if the target model is a logistic regression. This formulation also enjoys the freedom to choose the data distribution \(^{Q}\) for which we want to test the closeness. Rather the distributional equivalence pushes us to find the best query distribution for which the mismatch between the posteriors reduces the most and to compute an extracted model \(f^{E}_{^{*}}\) that induces the joint distribution closest to that of the target model \(f^{T}_{^{*}}\).

**Connection with different types of model extraction.** For \(D=D_{}\), our formulation extends the fidelity extraction from label agreement to prediction distribution matching, which addresses the future work indicated by . If we choose \(^{Q}_{}=^{T}\), and substitute \(D\) by prediction agreement, distributional equivalence retrieves the fidelity extraction attack. If we choose \(^{Q}_{}=()\), distributional equivalent extraction coincides with functional equivalent extraction. Thus, a distributional equivalence attack can lead to both fidelity and functional equivalence extractions depending on the choice of query generating distribution \(^{Q}\) and the divergence \(D\).

**Theorem 3.2** (Upper bounding distributional closeness).: _If we choose KL-divergence as the divergence function \(D\), then for a given query generating distribution \(^{Q}\)_

\[D_{}((f^{T}_{^{*}}(Q),Q)\|(f^{E}_{_{} }(Q),Q))_{}_{Q}[l(f^{T}_{^{*}}(Q),f^{E}_{} (Q))]-H(f^{E}_{}(Q)).\] (2)

By variational principle, Theorem 3.2 implies that _minimising the upper bound_ on the RHS leads to an extracted model which minimises the KL-divergence for a chosen query distribution.

**Max-Information model extraction.** Objective of any inference attack is to leak as much information as possible from the target model \(f^{T}\). Specifically, in model extraction attacks, we want to create an informative replica \(f^{E}\) of the target model \(f^{T}\) such that it induces a joint distribution \((f^{E}_{}(Q),Q)\), which retains the most information regarding the target's joint distribution. As adversary controls the query distribution, we aim to choose a query distribution \(^{Q}\) that maximises information leakage.

**Definition 3.3** (**Max-Information model extraction).** A model \(f^{E}:^{d} Y\) and a query distribution \(^{Q}\) are called a Max-Information extraction of a target model \(f^{T}:^{d} Y\) and a Max-Information query distribution, respectively, if they maximise the mutual information between the joint distributions of input features \(Q^{d}^{Q}\) and predicted labels induced by \(f^{E}\) and that of the target model. Mathematically, \((f^{E}_{^{*}},^{Q}_{})\) is a Max-Information extraction of \(f^{T}_{^{*}}\) if

\[(^{*}_{},^{Q}_{})* {arg\,max}_{}*{arg\,max}_{_{Q}}I((f^{T}_{ ^{*}}(Q),Q)\|(f^{E}_{}(Q),Q))\] (3)

Similar to Definition 3.1, Definition 3.3 also does not restrict us to choose a parametric model \(\) different from that of the target \(\). It also allows us to compute the data distribution \(^{Q}\) for which the information leakage is maximum rather than relying on the private dataset \(^{T}\) used for training \(f^{T}\).

**Theorem 3.4** (Lower bounding information leakage).: _For any given distribution \(^{Q}\), the information leaked by any Max-Information attack (Equation 3) is lower bounded as:_

\[I((f^{T}_{^{*}}(Q),Q)\|(f^{E}_{^{*}_{}}(Q),Q ))_{}-_{Q}[l(f^{T}_{^{*}}(Q),f^{E}_{}(Q))]+ H(f^{E}_{}(Q)).\] (4)

By variational principle, Theorem 3.4 implies that _maximising the lower bound_ in the RHS will lead to an extracted model which maximises the mutual information between target and extracted joint distributions for a given query generating distribution.

**Distributionally equivalent and Max-Information extractions: A variational optimisation formulation.** From Theorem 3.2 and 3.4, we observe that the lower and upper bounds of the objective functions of distribution equivalent and Max-Information attacks are negatives of each other. Specifically, \(-D_{}((f_{^{*}}^{T}(Q),Q)\|(f_{_{}^{E} }^{E}(Q),Q))_{}-F(,^{Q})\) and \(I((f_{^{*}}^{T}(Q),Q)\|(f_{_{}^{E}}^{E}(Q),Q ))_{}F(,^{Q})\), where

\[F(,^{Q})-_{Q}[l(f_{^{*}}^{T}(Q),f_ {}^{E}(Q))]+H(f_{}^{E}(Q)).\] (5)

Thus, following a variational approach, we aim to solve an optimisation problem on \(F(,^{Q})\) in an online and frequentist manner. We do not assume a parametric family of \(^{Q}\). Instead, we choose a set of queries \(Q_{t}^{d}\) at each round \(t T\). This leads to an empirical counterpart of our problem:

\[_{,Q_{[0,T]}^{Q}|_{T}|}(,Q_{[0,T ]})_{,Q_{[0,T]}}-_{t=1}^{T}l(f_{^{*} }^{T}(Q_{t}),f_{}^{E}(Q_{t}))]+_{t=1}^{T}H(f_{}^{E}(Q_{t})).\] (6)

As we need to evaluate \(f_{^{*}}^{T}\) for each \(Q_{t}\), we refer \(Q_{t}\)'s as _queries_, the dataset \(^{Q}^{d}\) from where they are chosen as the _query dataset_, and the corresponding unobserved distribution \(^{Q}\) as the _query generating distribution_. Given the optimisation problem of Equation 6, we propose an algorithm Marich to solve it effectively.

## 4 Marich: A query selection algorithm for model extraction

In this section, we propose an algorithm, Marich, to solve Equation (6) in an adaptive manner.

**Algorithm design.** We observe that once the queries \(Q_{[0,T]}\) are selected, the outer maximisation problem of Eq. (6) is equivalent to regualrised loss minimisation. Thus, it can be solved using any standard empirical risk minimisation algorithm (e.g. Adam, SGD). Thus, to achieve query efficiency, we focus on designing a query selection algorithm that selects a batch of queries \(Q_{t}\) at round \(t T\):

\[Q_{t}*{arg\,max}_{Q^{Q}}-_{t}_{i=1}^{t-1}l(f_{^{*}}^{T}(Q_{i} Q),f_{_{t-1}}^{E}(Q _{i} Q))]+^{t-1}H(f_{_{t-1}}^{E}(Q_{i} Q ))}_{}.\] (7)

```
0: Target model: \(f^{T}\), Query dataset: \(^{Q}\), \(\#\)Classes: \(k\)
0: Parameter: \(\#\)initial samples: \(n_{0}\), Training epochs: \(E_{max}\), \(\#\)Batches of queries: \(T\), Query budget: \(B\), Subsampling ratios: \(_{1},_{2}(0,1]\)
0: Extracted model \(f^{E}\)
1: //* Initialisation of the extracted model*// \(\) Phase 1
2: \(Q_{0}^{train} n_{0}\) datapoints randomly chosen from \(D^{Q}\)
3: \(Y_{0}^{train} f^{T}(Q_{0}^{train})\)\(\) Query the target model \(f^{T}\) with \(Q_{0}^{train}\)
4:for epoch \(\) 1 to \(E_{max}\)do
5: \(f_{0}^{E}\) Train \(f^{E}\) with \((Q_{0}^{train},Y_{0}^{train})\)
6:endfor
7: //* Adaptive query selection to build the extracted model*/ \(\) Phase 2
8:for\(t\) 1 to \(T\)do
9: \(Q_{t}^{entropy}\)EntropySampling\((f_{t-1}^{E},^{Q} Q_{t-1}^{train},B)\)
10: \(Q_{t}^{grad}\)EntropyGradientSampling\((f_{t-1}^{E},Q_{t}^{entropy},_{1}B)\)
11: \(Q_{t}^{loss}\)LossSampling\((f_{t-1}^{E},Q_{t}^{grad},Q_{t-1}^{train},Y_{t-1}^{train},_{1}_{2}B)\)
12: \(Y_{t}^{new} f^{T}(Q_{t}^{loss})\)\(\) Query the target model \(f^{T}\) with \(Q_{t}^{loss}\)
13: \(Q_{t}^{train} Q_{t-1}^{train} Q_{t}^{loss}\)
14: \(Y_{t}^{train} Y_{t-1}^{train} Y_{t}^{new}\)
15:for epoch \(\) 1 to \(E_{max}\)do
16: \(f_{t}^{E}\) Train \(f_{t-1}^{E}\) with \((Q_{t}^{train},Y_{t}^{train})\)
17:endfor
18:endfor
19:return Extracted model \(f^{E} f_{T}^{E}\) ```

**Algorithm 1**MarichHere, \(f^{E}_{_{t-1}}\) is the model extracted by round \(t-1\). Equation (7) indicates two criteria to select the queries. With the **entropy term**, we want to select a query that maximises the entropy of predictions for the extracted model \(f^{E}_{_{t-1}}\). This allows us to select the queries which are most informative about the mapping between the input features and the prediction space. With the **model-mismatch term**, Eq. (7) pushes the adversary to select queries where the target and extracted models mismatch the most. Thus, minimising the loss between target and extracted models for such a query forces them to match over the whole domain. Algorithm 1 illustrates a pseudocode of Marich (Appendix A).

**Initialisation phase.** To initialise the extraction, we select a set of \(n_{0}\) queries, called \(Q^{train}_{0}\), uniformly randomly from the query dataset \(^{Q}\). We send these queries to the target model and collect corresponding predicted classes \(Y^{train}_{0}\) (Line 3). We use these \(n_{0}\) samples of input-predicted label pairs to construct a primary extracted model \(f^{E}_{0}\).

**Active sampling.** As the adaptive sampling phase commences, we select \(_{1}_{2}B\) number of queries at round \(t\). To _maximise_ the **entropy term** and _minimise_ the **model-mismatch term** of Eq. (7), we sequentially deploy EntropySampling and LossSampling. To achieve further query-efficiency, we refine the queries selected using EntropySampling by EntropyGradientSampling, which finds the most diverse subset from a given set of queries. Now, we describe the sampling strategies.

EntropySampling. First, we aim to select the set of queries which unveil most information about the mapping between the input features and the prediction space. Thus, we deploy EntropySampling. In EntropySampling, we compute the output probability vectors from \(f^{E}_{t-1}\) for all the query points in \(^{Q} Q^{train}_{t-1}\) and then select top \(B\) points with highest entropy:

\[Q^{entropy}*{arg\,max}_{X X_{in},|X|=B}H(f^{E}( X_{in})).\]

Thus, we select the queries \(Q^{entropy}_{t}\), about which \(f^{E}_{t-1}\) is most confused and training on these points makes the model more informative.

EntropyGradientSampling. To be frugal about the number of queries, we refine \(Q^{entropy}_{t}\) to compute the most diverse subset of it. First, we compute the gradients of entropy of \(f^{E}_{t-1}(x)\), i.e. \(_{x}H(f^{E}_{t-1}(x))\), for all \(x Q^{entropy}_{t}\). The gradient at point \(x\) reflects the change at \(x\) in the prediction distribution induced by \(f^{E}_{t-1}\). We use these gradients to embed the points \(x Q^{entropy}_{t}\). Now, we deploy K-means clustering to find \(k\) (= #classes) clusters with centers \(C_{in}\). Then, we sample \(_{1}B\) points from these clusters:

\[Q^{grad}*{arg\,min}_{X Q^{entropy}_{t},|X|= _{1}B}_{x_{i} X}_{x_{j} C_{in}}\|_{x_{i}}H(f^{E}(.) )-_{x_{j}}H(f^{E}(.))\|_{2}^{2}.\]

Selecting from \(k\) clusters ensures diversity of queries and reduces them by \(_{1}\).

LossSampling. We select points from \(Q^{grad}_{t}\) for which the predictions of \(f^{T}_{^{*}}\) and \(f^{E}_{t-1}\) are most dissimilar. To identify these points, we compute the loss \(l(f^{T}(x),f^{E}_{t-1}(x))\) for all \(x Q^{train}_{t-1}\). Then, we select top-\(k\) points from \(Q^{train}_{t-1}\) with the highest loss values (Line 11), and sample a subset \(Q^{loss}_{t}\) of size \(_{1}_{2}B\) from \(Q^{grad}_{t}\) which are closest to the \(k\) points selected from \(Q^{train}_{t-1}\). This ensures that \(f^{E}_{t-1}\) would better align with \(f^{T}\) if it trains on the points where the mismatch in predictions are higher.

At the end of Phase 2 in each round of sampling, \(Q^{loss}_{t}\) is sent to \(f^{T}\) for fetching the labels \(Y^{train}_{t}\) predicted by the target model. We use \((Q^{loss}_{t},Y^{loss}_{t})\) along with \((Q^{train}_{t-1},Y^{train}_{t-1})\) to train \(f^{E}_{t-1}\) further. Thus, Marich performs \(n_{0}+_{1}_{2}BT\) number of queries through \(T+1\) number of interactions with the target model \(f^{T}\) to create the final extracted model \(f^{E}_{T}\). We experimentally demonstrate effectiveness of the model extracted by Marich to achieve high task accuracy and to act as an informative replica of the target for extracting private information regarding private training data \(^{T}\).

**Discussions.** Eq. (7) dictates that the active sampling strategy should try to select queries that maximise the entropy in the prediction distribution of the extracted model, while decreases the mismatch in predictions of the target and the extracted models. We further use the EntropyGradientSampling to choose a smaller but most diverse subset. As Eq. (7) does not specify any ordering between these objectives, one can argue about the sequence of using these three sampling strategies. We choose to use sampling strategies in the decreasing order of runtime complexity as the first strategy selects the queries from the whole query dataset, while the following strategies work only on the already selected queries. We show in Appendix E that LossSampling incurs the highest runtime followed by EntropyGradientSampling, while EntropySampling is significantly cheaper.

_Remark 4.1_.: Previously, researchers have deployed different active sampling methods to efficiently select queries for attacks [PMG\({}^{+}\)17, CCG\({}^{+}\)20, PGS\({}^{+}\)20]. But our derivation shows that the active query selection can be grounded on the objectives of distributionally equivalent and max-information extractions. Thus, though the end result of our formulation, i.e. Marich, is an active query selection algorithm, our framework is different and novel with respect to existing active sampling based works.

## 5 Experimental analysis

Now, we perform an experimental evaluation of models extracted by Marich. Here, we discuss the experimental setup, the objectives of experiments, and experimental results. We defer the source code, extended results, parametric similarity of the extracted models, effects of model-mismatch, details of different samplings, and hyperparameters to Appendix.

**Experimental setup.** We implement a prototype of Marich using Python 3.9 and PyTorch 1.12, and run on a NVIDIA GeForce RTX 3090 24 GB GPU. We perform attacks against _four target models_ (\(f^{T}\)), namely Logistic Regression (LR), CNN , ResNet , BERT , trained on _three private datasets_ (\(^{T}\)): MNIST handwritten digits , CIFAR10 [KH\({}^{+}\)09] and BBC News, respectively. For model extraction, we use EMNIST letters dataset , CIFAR10, ImageNet , and AGNews , as publicly-available, mismatched query datasets \(^{Q}\).

To instantiate task accuracy, we compare accuracy of the extracted models \(f^{E}_{}\) with the target model and models extracted by K-Center (KC) , Least-Confidence sampling (LC) , Margin sampling (MS) , Entropy Sampling (ES) , and Random Sampling (RS). To instantiate informativeness of the extracted models , we compare the Membership Inference (MI), i.e. MI accuracy and MI agreements (\(\%\) and AUC), performed on the target models, and the models extracted using Marich and competitors with same query budget. For MI, we use in-built membership attack from IBM ART . For brevity, we discuss Best of Competitors (BoC) against Marich for each experiment (except Fig. 2- 3) The objectives of the experiments are:

1. _How do the accuracy of the model extracted using Marich on the private dataset compare with that of the target model, and RS with same query budget?_
2. _How close are the prediction distributions of the model extracted using Marich and the target model? Can Marich produce better replica of target's prediction distribution than other active sampling methods, leading to better distributional equivalence?_
3. _How do the models extracted by Marich behave under Membership Inference (MI) in comparison to the target models, and the models extracted by RS with same budget?_ The MI accuracy achievable by attacking a model acts as a proxy of how informative is the model.
4. _How does the performance of extracted models change if Differentially Private (DP) mechanisms  are applied on target model either during training or while answering the queries?_

**Accuracy of extracted models.** Marich extracts LR models with 1,863 and 959 queries selected from EMNIST and CIFAR10, while attacking a target LR model, \(f^{T}_{}\) trained on MNIST (test accuracy: \(90.82\%\)). The models extracted by Marich using EMNIST and CIFAR10 achieve test accuracy \(73.98\%\) and \(86.83\%\) (\(81.46\%\) and \(95.60\%\) of \(f^{T}_{}\)), respectively (Figure 1(a)-1(b)). The models extracted using BoC show test accuracy \(52.60\%\) and \(79.09\%\) (\(57.91\%\) and \(87.08\%\) of \(f^{T}_{}\)), i.e.

Figure 2: Accuracy of the extracted models (mean \(\) std. over 10 runs) w.r.t. the target model using Marich, and competing active sampling methods (KC, LC, MS, ES, RS). Each figure represents (a target model, a query dataset). Models extracted by Marich are closer to the target models.

significantly less than that of Marich. Marich attacks a ResNet, \(f^{T}_{ResNet}\), trained on CIFAR10 (test accuracy: \(91.82\%\)) with 8,429 queries from ImageNet dataset, and extracts a CNN. The extracted CNN shows \(56.11\%\) (\(61.10\%\) of \(f^{T}_{ResNet}\)) test accuracy. But the model extracted using BoC achieves \(42.05\%\) (\(45.79\%\) of \(f^{T}_{ResNet}\)) accuracy (Figure 1(c)). We also attack a CNN with another CNN, which also reflects Marich's improved performance (Figure 1(d)). _To verify_ Marich_'s effectiveness for text data_, we also attack a BERT, \(f^{T}_{BERT}\) trained on BBCNews (test accuracy: \(98.65\%\)) with queries from the AGNews dataset. By using only 474 queries, Marich extracts a model with \(85.45\%\) (\(86.64\%\) of \(f^{T}_{BERT}\)) test accuracy. The model extracted by BoC shows test accuracy \(79.25\%\) (\(80.36\%\) of \(f^{T}_{BERT}\)). _For all the models and datasets,_ Marich _extracts models that achieve test accuracy closer to target models, and are more accurate than models extracted by the other algorithms._

**Distributional equivalence of extracted models.** One of our aims is to extract a distributionally equivalent model of the target \(f^{T}\) using Marich. Thus, in Figure 3, we illustrate the KL-divergence (mean\(\)std. over 10 runs) between the prediction distributions of the target model and the model extracted by Marich. Due to brevity, we show two cases in the main paper: when we attack i) an LR trained on MNIST with EMNIST with an LR, and ii) a ResNet trained on CIFAR10 with ImageNet with a CNN. In all cases, we observe that the models extracted by Marich achieve \( 2-4\) lower KL-divergence than the models extracted by all other active sampling methods. _These results show that_ Marich _is extracts high-fidelity distributionally equivalent models than competing algorithms._

**Membership inference with extracted models.** In Table 1, we report _accuracy_, _agreement_ in inference with target model, and _agreement AUC_ of membership attacks performed on different target models and extracted models with different query datasets. The models extracted using Marich demonstrate higher MI agreement with the target models than the models extracted using its competitors in most of the cases. They also achieve MI accuracy close to the target model. _These results indicate that the models extracted by_ Marich _act as informative replicas of the target models._

**Performance against privacy defenses.** We test the impact of DP-based defenses deployed in the target model on the performance of Marich. First, we train four target models on MNIST using _DP-SGD_ with privacy budgets \(=\{0.2,0.5,1,\}\) and \(=10^{-5}\). As illustrated in Figure 3(a), accuracy of the models extracted by querying DP target models are \( 2.3-7.4\%\) lower than the model extracted from non-private target models. Second, we apply an _output perturbation_ method , where a calibrated Laplace noise is added to the responses of the target model against Marich's queries. This ensures \(\)-DP for the target model. Figure 3(b) shows that performance of the extracted models degrade slightly for \(=2,8\), but significantly for \(=0.25\). Thus, _performance of_ Marich _decreases while operating against DP defenses but the degradation varies depending on the defense mechanism._

   &  &  &  &  &  &  &  &  &  &  &  &  \\  MNIST & LR & EMNIST & MARICH & EMNIST & \(85.00\)(\(\%\)) & \(\) & \(\) & \(\) & \(\) \\  MNIST & LR & EMNIST & BaC\({}^{*}\) & EMNIST & \(185.83\)(\(\%\)) & \(\) & \(\) & \(\) & \(\) \\  MNIST & LR & EMNIST & BaC\({}^{*}\) & EMNIST & \(185.83\)(\(\%\)) & \(\) & \(\) & \(\) & \(\) \\  MNIST & LR & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 \\  MNIST & LR & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 & CAR10 \\  MNIST & CNN & EMNIST & MARICH & EMNIST & \(61.22\)(\(\%\)) & \(\) & \(\) & \(\) & \(\) \\  MNIST & CNN & EMNIST & BaC\({}^{*}\) & EMNIST & \(61.67\)(\(\%\)) & \(\) & \(\) & \(\) & \(\) \\  CIFAR10 & ResNet & EMNIST & \(53.00\)(\(\%\)) & \(\) & \(\) & \(\) \\  CIFAR10 & ResNet & ImageNet & MARICH & ZEMNIST & \(8425\) & \(\) & \(\) & \(\) & \(\) \\  CIFAR10 & ResNet & ImageNet & BaC\({}^{*}\) & EMNIST & \(8425\) & \(\) & \(\) & \(\) & \(\) \\  BBCNews & BERT & ABCNews & BaC\({}^{*}\) & \(\) & \(\) & \(_{}\) are higher than the models extracted with the competing algorithms, and are \( 60-95\%\) of the target models (Fig. 2). This shows effectiveness of Marich as a task accuracy extraction attack, while solving distributional equivalence and max-info extractions.

_2. Distributional equivalence._ We observe that the KL-divergence between the prediction distributions of the target model and \(f^{E}_{}\) are \( 2-4\) lower than the models extracted by other active sampling algorithms. This confirms that Marich conducts more accurate distributionally equivalent extraction than existing active sampling attacks.

_3. Informative replicas: Effective membership inference._ The agreement in MI achieved by attacking \(f^{E}_{}\) and the target model in most of the cases is higher than that of the BoC* (Table 1). Also, MI accuracy for \(f^{E}_{}\)'s are \(84.74\%-96.32\%\) (Table 1). This shows that the models extracted by Marich act as informative replicas of the target model.

_4. Query-efficiency._ Table 1 shows that Marich uses only \(959-8,429\) queries from the public datasets, i.e. a small fraction of data used to train the target models. Thus, Marich is significantly query efficient, as existing active learning attacks use \(10\)k queries to commence [PGS\({}^{+}\)20, Table 2].

_5. Performance against defenses._ Performance of Marich decreases with the increasing level of DP applied on the target model, which is expected. But when DP-SGD is applied to train the target, the degradation is little (\( 7\%\)) even for \(=0.2\). In contrast, the degradation is higher when the output perturbation is applied with similar \(\) (\(0.25\)).

_6. Model-obliviousness and out-of-class data._ By construction, Marich is model-oblivious and can use out-of-class public data to extract a target model. To test this flexibility of Marich, we try and extract a ResNet trained on CIFAR10 using a different model, i.e. CNN, and out-of-class data, i.e. ImageNet. We show CNNs extracted by Marich are more accurate, distributionally close, and also lead to higher MI accuracy that the competitors, validating flexibility of Marich.

_7. Resilience to mismatch between \(^{T}\)and \(^{Q}\)._ For Marich, the datasets \(^{T}\)and \(^{Q}\)can be significantly different. For example, we attack an MNIST-trained model with EMNIST and CIFAR10 as query datasets. MNIST contains handwritten digits, CIFAR10 contains images of aeroplanes, cats, dogs etc., and EMNIST contains handwritten letters. Thus, the data generating distributions and labels are significantly different between the private and query datasets. We also attack a CIFAR10-trained ResNet with ImageNet as \(^{Q}\). CIFAR10 and ImageNet are also known to have very different labels and images. Our experiments demonstrate that Marich can handle data mismatch as well as model mismatch, which is an addendum to the existing model extraction attacks.

## 6 Conclusion and future directions

We investigate the design of a model extraction attack against a target ML model (classifier) trained on a private dataset and accessible through a public API. The API returns only a predicted label for a given query. We propose the notions of distributional equivalence extraction, which extends the existing notions of task accuracy and functionally equivalent model extractions. We also propose an information-theoretic notion, i.e. Max-Info model extraction. We further propose a variational relaxation of these two types of extraction attacks, and solve it using an online and adaptive query selection algorithm, Marich. Marich uses a publicly available query dataset different from the private dataset. We experimentally show that the models extracted by Marich achieve \(56-86\%\) accuracy on the private dataset while using 959 - 8,429 queries. For both text and image data, we demonstrate that the models extracted by Marich act as informative replicas of the target models and also yield high-fidelity replicas of the targets' prediction distributions. Typically, the functional equivalence attacks require model-specific techniques, while Marich is model-oblivious while performing distributional equivalence attack. This poses an open question: is distributional equivalence extraction 'easier' than functional equivalence extraction, which is NP-hard [JCB\({}^{+}\)20]?