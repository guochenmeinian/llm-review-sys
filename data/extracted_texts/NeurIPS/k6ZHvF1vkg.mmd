# Beyond Optimism:

Exploration With Partially Observable Rewards

Simone Parisi

University of Alberta; Amii

parisi@ualberta.ca &Alireza Kazemipour

University of Alberta

kazemipo@ualberta.ca &Michael Bowling

University of Alberta; Amii

mbowling@ualberta.ca

###### Abstract

Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are _unobservable_, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.

## 1 Introduction

Reinforcement learning (RL) has developed into a powerful paradigm where agents can tackle a variety of tasks, including games , robotics , and medical applications . Agents trained with RL learn by trial-and-error interactions with an environment: the agent tries different actions, the environment returns a numerical reward, and the agent adapts its behavior to maximize future rewards. This setting poses a well-known dilemma: how and for how long should the agent explore, i.e., try new actions and visit new states in search for rewards? If the agent does not explore enough it may miss important rewards. However, prolonged exploration can be expensive, especially in real-world problems where instrumentation (e.g., cameras or sensors) or a human expert may be needed to provide rewards. Classic dithering exploration  is known to be inefficient , especially when informative rewards are sparse . Among the many exploration strategies that have been proposed to improve RL efficiency, perhaps the most well-known and grounded is optimism. With optimism, the agent assigns to each state-action pair an optimistically biased estimate of future value and selects the action with the highest estimate . This way, the agent is encouraged to explore unvisited states and perform different actions, where either its optimistic expectation holds -- the observed reward matches the optimistic estimate -- or not. If not, the agent adjusts its estimate and looks for rewards elsewhere. Optimistic algorithms range from count-based exploration bonuses , to model-based planning , bootstrapping , and posterior sampling . Nonetheless, optimism can fail when the agent has limited observability of the outcome of its actions. Consider the example in Figure 0(b), where the agent observes rewards only under some condition and upon paying a cost. To fully explore the environment and learn an optimal policy the agent must first take a _suboptimal_ action (to push the button and pay a cost) to learn about the rewards. However, optimistic algorithms would never select suboptimal actions, thus never pushing the button and never learning how to accumulate positive rewards . While alternatives to optimism exist, such as intrinsic motivation , they often lack guarantees of convergence and their efficacy strongly depends on the intrinsic reward used, the environment, and the task .

[MISSING_PAGE_EMPTY:2]

learn \(Q^{*}\) by iteratively updating a function \(\) as follows,

\[(s_{t},a_{t})(1-_{t})(s_{t},a_{t})+_ {t}(r_{t}+(s_{t+1},a)}^{}), \]

where \(_{t}\) is the learning rate. \(\) is guaranteed to converge to \(Q^{*}\) under an appropriate learning rate schedule \(_{t}\) and if every state-action pair is visited infinitely often . Thus, how the agent explores plays a crucial role. For example, a random policy eventually visits all states but is clearly inefficient, as states that are hard to reach will be visited less frequently (e.g., because they can be reached only by performing a sequence of specific actions). How frequently the policy explores is also crucial -- to behave optimally we also need the policy to act optimally eventually. That is, at some point, the agent should stop exploring and act greedy instead. For these reasons, we are interested in _greedy in the limit with infinite exploration (GLIE)_ policies , i.e., policies that guarantee to explore all state-action pairs enough for \(\) to converge to \(Q^{*}\), and that eventually act greedy as in Eq. (1). While a large body of work in RL literature has been devoted to developing efficient GLIE policies in MDPs, an important problem still remains largely unaddressed: _the agent explores in search of rewards, but what if rewards are not always observable?_ MDPs, in fact, assume rewards are always observable. Thus, we consider the more general framework of Monitored MDPs (Mon-MDPs) .

### Monitored MDPs: When Rewards Are Not Always Observable

Mon-MDPs (Figure 3) are defined by the tuple \(^{},^{},^{}, ^{},,^{},^{},^{},^{},\), where \(^{},^{},^{}, ^{},\) is the same as classic MDPs and the superscript E stands for "environment", while \(,^{},^{},^ {},^{}\) is the "monitor", a separate process with its own states, actions, rewards, and transition. The monitor works similarly to the environment -- performing a monitor action \(a^{}_{t}^{}\) affects its state \(s^{}_{t}^{}\) and yields a bounded reward \(r^{}_{t}^{}(s^{}_{t},a^{}_{t})\). However, there are two crucial differences. First, its Markovian transition depends on the environment as well, i.e., \(^{}(s^{}_{t+1}|s^{}_{t},s^{}_{t},a ^{}_{t},a^{}_{t})\). Second, the _monitor function_\(\) stands in between the agent and the environment, dictating what the agent sees about the environment reward -- rather than directly observing the _environment reward_\(r^{}_{t}^{}(s^{}_{t},a^{}_{t})\), the agent observes a _proxy reward_\(^{}_{t}(r^{}_{t},s^{}_{t},a^{ }_{t})\). Most importantly, the monitor may not always show a numerical reward, and the agent may some receive \(^{}_{t}=\), i.e., _"unobservable reward"_.

In Mon-MDPs, the agent repeatedly executes a joint action \(a_{t}(a^{}_{t},a^{}_{t})\) according to the joint state \(s_{t}(s^{}_{t},s^{}_{t})\). In turn, the environment and monitor states change and produce a joint reward \((r^{}_{t},r^{}_{t})\), but the agent observes \((r^{}_{t},r^{}_{t})\). The agent's goal is to select joint actions to maximize \(_{t=1}^{}^{t-1}(r^{}_{t}+r^{}_{t})\)_even though it observes \(r^{}_{t}\) instead of \(r^{}_{t}\)_. For example, in Figure 1b the agent can turn monitoring on/off by pushing the button: if monitoring is off, the agent cannot observe the environment rewards; if on, it observes them but receives negative monitor rewards. Formally, \(^{}=\{,\}\), \(^{}=\{,\}\), \(^{}(,)=-1\), \((r^{}_{t},,)=r^{}_{t}\), \((r^{}_{t},,)=\). The optimal policy is to move to the rightmost cell without turning monitoring on -- the agent still receives the positive reward even though it does not observe it.1

The presence of the monitor and the partial observability of rewards highlight two fundamental differences between Mon-MDPs and existing MDPs variations. First, _the observability of the reward is dictated by a "Markovian entity" (the monitor)_, thus actions can have either immediate or long-term effects on the reward observability. For example, the agent may immediately observe the reward upon explicitly asking for it as in active RL , e.g., with a dedicated action \(a^{}=\). Or, rewards may become observable only after changing the monitor state through a sequence of actions, e.g., as in Figure 1b by reaching and pushing the button. Second, _reward unobservability goes beyond reward sparsity_. In sparse-reward MDPs , rewards are _always_ observable even though informative non-zero rewards are sparse. Mon-MDPs with dense informative observable rewards may still be challenging even if a few rewards are partially observable. Consider the Mon-MDP in Figure 1b, but

Figure 3: **The Mon-MDP framework.**

this time zero-rewards of empty tiles are replaced with negative rewards proportional to the distance to the large coin (dense and informative). Because coin rewards are still observable only after pressing the button, an optimistic agent will still collect the small coin: the optimistic value of the small coin is as high as the optimistic value of the large coin, but collecting the small coin takes fewer steps, thus ending the series of dense negative rewards sooner. _The agent must first push the button (a suboptimal action) to learn the optimal policy regardless of the presence of dense observable rewards._

Mon-MDPs provide a comprehensive framework encompassing existing areas of research, such as active RL  and partial monitoring . In their introductory work, Parisi et al.  showed the existence of an optimal policy and the convergence of a variant of Q-Learning under some assumptions.2 Yet, many research questions remain open, most prominently how to explore efficiently in Mon-MDPs. In the next section, we review exploration strategies in RL and discuss their shortcomings, especially when rewards are partially observable as in Mon-MDPs. For the sake of simplicity, in the remainder of the paper we use the notation \((s,a)\) to refer to both the environment state-action pairs (in MDPs) and the _joint_ environment-monitor state-action pairs (in Mon-MPDs). All the related work on exploration for MDPs can be applied to Mon-MDPs as well (not albeit with limited efficacy, as we discuss below). Similarly, the exploration strategy we propose in Section 3 can be applied to both MDPs and Mon-MDPs.

### Related Work

**Optimism.** In tabular MDPs, many provably efficient algorithms are based on optimism in the face of uncertainty (OFU) . In OFU, the agent acts greedily with respect to an optimistic estimate composed of the action-value estimate (e.g., \(\)) and a bonus term that is proportional to the uncertainty of the estimate. After executing an optimistic action, the reward either validates the agent's optimism or proves it wrong, thus discouraging the agent from trying it again. RL literature provides many variations of these algorithms, each with different convergence bounds and assumptions. Perhaps the best known OFU algorithms are based on the upper confidence bound (UCB) algorithm , where the optimistic value is computed from visitation counts (the lower the count, the higher the bonus). In model-free RL, Jin et al.  proved convergence bounds for episodic MDPs, and later Dong et al.  extended the proof to infinite horizon MDPs, but both are often intractable. In model-based RL, algorithms like R-MAX  and UCRL  consider a set of plausible MDP models (i.e., plausible reward and transition functions) and optimistically select the best action over what they believe to be the MDP yielding the highest return. Unfortunately, this optimistic selection can fail when rewards are not always unobservable , as the agent may have to select actions that are _known_ to be suboptimal to discover truly optimal actions. For example, in Figure 0(b) pushing the button is suboptimal (the agent pays a cost) but is the only way to discover the large positive reward.

**Posterior sampling for RL (PSRL).** PSRL adapts Thompson sampling  to RL. The agent is given a prior distribution over a set of plausible MDPs, and rather than sampling the MDP optimistically as in OFU, PSRL samples it from a distribution representing how likely an MDP is to yield the highest return. Only then, the agent acts optimally for the sampled MDP. PSRL can be orders of magnitude more efficient than UCRL , but its efficiency strongly depends on the choice of the prior . Furthermore, the computation of the posterior (by Bayes' theorem) can be intractable even for small tasks, thus algorithms usually approximate it empirically with an ensemble of randomized Q-functions . Nonetheless, if rewards are partially observable PSRL suffers from the same problem as OFU: the agent may never discover rewards if actions needed to observe them (and further identify the MDP) are suboptimal for all MDPs with posterior support .

**Information gain.** These algorithms combine PSRL and UCB principles. On one hand, they rely on a posterior distribution over plausible MDPs, on the other hand they augment the current value estimate with a bound that comes in terms of an information gain quantity, e.g., the difference between the entropy of the posterior after an update . This allows to sample suboptimal but informative actions, thus overcoming some limitations of OFU and PSRL. However, similar to PSRL, performing explicit belief inference can be intractable even for small problems .

**Intrinsic motivation.** While many of above strategies are often computationally intractable, they inspired more practical algorithms where exploration is conducted by adding an _intrinsic reward_ to the environment. This is often referred to as intrinsic motivation . For example, the intrinsic reward can depend on the visitation count , the impact of actions on the environment , interest or surprise , information gain , or can be computed from prediction errors of some quantity . However, intrinsic rewards introduce non-stationarity to the Q-function (they change over time), are often myopic (they are often computed only on the immediate transition), and are sensitive to noise . Furthermore, if they decay too quickly exploration may vanish prematurely, but if they outweigh the environment reward the agent may explore for too long .

It should be clear that learning with partially observable rewards is still an open challenge. In the next section, we present a practical algorithm that (1) performs deep exploration -- it can reason long-term, e.g., it tries to visit states far away, (2) does not rely on optimism -- addressing the challenges of partially observable rewards, (3) explicitly decouples exploration and exploitation -- thus exploration does not depend on the accuracy of \(\), which is known to be problematic (e.g., overestimation bias) for off-policy algorithms (even more if rewards are partially observable). After proving its asymptotic convergence we empirically validate it on a collection of tabular MDPs and Mon-MDPs.

## 3 Directed Exploration-Exploitation With The Successor Representation

```
1\((s^{},a^{})=_{s,a}N_{t}(s,a)\)// tie-break by deterministic ordering
2\(_{t}=}{{N_{t}(s^{},a^{})}}\)
3if\(_{t}>\)thenreturn\((a\,|\,s_{t},s^{},a^{})\)// explore
4elsereturn\(_{a}(s_{t},a)\)// exploit
```

**Algorithm 1**Directed Exploration-Exploitation

Algorithm 1 outlines our idea for step-wise exploration.3\(N(s,a)\) is the state-action visitation count and, at every timestep, the agent selects the least-visited pair as "goal" \((s^{},a^{})\). The ratio \(_{t}\) decides if the agent explores (goes to the goal) or exploits according to a threshold \(>0\). If two or more pairs have the same lowest count, ties are broken consistently according to some deterministic ordering. For example, if actions are {LEFT, RIGHT, UP, DOWN}, LEFT is the first and will be selected if the count of all actions is the same. This ensures that once the agent starts exploring to reach a goal, it will continue to do so until the goal is visited -- once its count is minimal under the deterministic tie-break, it will continue to be minimal until the goal is visited and its count is incremented. Exploration is driven by \((a\,|\,s_{t},s^{},a^{})\), a _goal-conditioned policy_ that selects the action according to the current state \(s_{t}\) and goal \((s^{},a^{})\). Intuitively, as the agent explores, if every state-action pair is visited infinitely often, \( t\) will grow at a slower rate than \(N_{t}(s,a)\) and the agent will increasingly often exploit (formal proof below). Furthermore, infinite exploration ensures \(\) converges to \(Q^{*}\), i.e., that its greedy actions are optimal. Intuitively, the role of \(_{t}\) is similar to the one of \(_{t}\) in \(\)-greedy policies . However, rather than manually tuning its decay, \(_{t}\) naturally decays as the agent explores.

The goal-conditioned policy \(\) is the core of Algorithm 1. For efficient exploration we want \(\) to move the agent as quickly as possible to the goal, i.e., we want to minimize the _goal-relative diameter_ under \(\). Intuitively, this diameter is a bound on the expected number of steps the policy would take to visit the goal from any state (formal definition below). Moreover, we want to untie \(\) from the Q-function, the rewards, and their observability. In the next section we propose a policy \(\) that satisfies these criteria, but first we prove that Algorithm 1 is a GLIE policy under some assumptions on \(\).

**Definition 1** (Singh et al. ).: _An exploration policy is greedy in the limit (GLIE) if (1) each action is executed infinitely often in every state that is visited infinitely often, and (2) in the limit, the learning policy is greedy with respect to the Q-function with probability 1._

**Definition 2**.: _Let \(\) be a goal-conditioned policy \((a\,|\,s,s^{},a^{})\). Let \(T(s^{},a^{}\,|\,,s)\) be the first timestep \(t\) in which \((s_{t},a_{t})=(s^{},a^{})\) given \(s_{0}=s\) and \(a_{t}(a\,|\,s_{t},s^{},a^{})\). The goal-relative diameter of \(\) with respect to \((s^{},a^{})\), if it exists, is_

\[D^{}(s^{},a^{})=_{s}[T(s^{},a^{ }\,|\,,s)\,|\,,], \]

_i.e., the maximum expected number of steps to reach \((s^{},a^{})\) from any state in the MDP. We say the goal-relative diameter is bounded by \(\) if \(_{s^{},a^{}}D^{}(s^{},a ^{})\)._There exist goal-conditioned policies with bounded goal-relative diameter if and only if the MDP is communicating (i.e., for any two states there is a policy under which there is non-zero probability to move between the states in finite time; see Puterman  and Jaksch et al. ). While not all goal-conditioned policies have bounded goal-relative diameter, one such policy is the random policy or similarly an \(\)-greedy policy for \(>0\). Different goal-conditioned policies will have different bounds on their goal-relative diameter \(D^{}(s^{},a^{})\).4

**Theorem 1**.: _If the goal-relative diameter of \(\) is bounded by \(\) then Algorithm 1 is a GLIE policy._

Proof.: Let \(Z_{t}(s,a)\) be the number of timesteps before time \(t\) that the agent is in an exploration step with \((s^{},a^{})=(s,a)\). Let \(X_{t}=Z_{t}(s,a)}}{{t}}\) be the fraction of time up to time \(t\) that the agent has spent exploring. We want to show that \(>0\,_{t}[X_{t}>]=0\), i.e., the probability that the agent explores as frequently as any positive \(\) approaches 0. Hence, the policy is greedy in the limit. Note that, if the agent's frequency of exploring approaches zero this also must mean that \(_{t}(1)\), implying \(N_{t}(s,a)\,(s,a)\), i.e., all state-action pairs are visited infinitely often.

Let us focus on \([Z_{t}(s,a)]\). Once the agent starts exploring to visit \((s,a)\), it will do so until it actually visits \((s,a)\). Let \(I_{t}(s,a)\) be the number of times the agent started exploring to visit \((s,a)\) before time \(t\). We know that \(I_{t}(s,a) N_{t}(s,a)+1\), as the agent must visit \((s,a)\) before it starts exploring to visit it again. Let \(t^{} t\) be the last time it started exploring to visit \((s,a)\). We have

\[I_{t}(s,a)=I_{t^{}}(s,a) N_{t^{}}(s,a)+1< }{}+1}+1, \]

where the strict inequality is due to \(_{t^{}}>\) (condition to explore). Since the agent cannot have started exploration \(}{{}}+1\) times, and the goal-relative diameter of \(\) is at most \(\), then

\[[Z_{t}(s,a)]<(}+1), [X_{t}]<||| }{t}(}+1). \]

We can now apply Markov's inequality with threshold \(}{{}}\),

\[[X_{t}}]\,[X_{t }]<|||}{}(}+1). \]

Since \(}{{}}<\) for sufficiently large \(t\),

\[_{t}[X_{t}]_{t }[X_{t}}]_ {t}|||}{}( }+1)=0, \]

hence the Algorithm 1 is a GLIE policy. 

**Corollary 1**.: _As a consequence of Theorem 1, \(\) converges to \(Q^{*}\) (infinite exploration) and therefore the algorithm's behavior converges to the optimal policy (greedy in the limit).5_

While we have noted that the random policy is a sufficient choice for \(\) to meet the criteria of Theorem 1, the bound in Equation 7 shows a direct dependence on the goal-relative diameter \(D^{}(s^{},a^{})\). Thus, the optimal \(\) to explore efficiently in Algorithm 1 is \(^{*}(a\,|s,s^{},a^{})_{}D^{} (s^{},a^{})\), i.e., we want to reach the goal from any state as quickly as possible in expectation. Furthermore, we want to unite \(\) from the learned Q-function and the reward observability. However, learning such a policy can be challenging because the diameter of the MDP is usually unknown. In the next section, we present a suitable alternative based on the successor representation .

### Successor-Function: An Exploration Compass

_The successor representation (SR)_ is a generalization of the value function, and represents the cumulative discounted occurrence of a state \(s_{i}\) under a policy \(\), i.e., \([_{k=t}^{}}}{{\{s_{k}= s_{i}\}}}\,|\,,,s_{t}]\), where \(\{s_{k}=s_{i}\}\) is the indicator function returning 1 if \(s_{k}=s_{i}\) and 0 otherwise. The SR does not depend on the reward function and can be learned with temporal-difference in a model-free fashion, e.g., with Q-Learning. Despite their popularity in transfer RL , to the best of our knowledge, only Machado et al.  and Jin et al.  used the SR to enhance exploration by using it as an intrinsic reward. Here, we follow the idea of the SR -- to predict state occurrences under a policy -- to learn a value function that the goal-conditioned policy in Algorithm 1 can use to guide the agent towards desired state-action pairs. We call this the _state-action successor-function_ (or S-function) and we formalize it as a general value function  representing the cumulative discounted occurrence of a state-action pair \((s_{i},a_{j})\) under a policy \(\), i.e.,

\[S^{}_{s_{i}a_{j}}(s_{t},a_{t})=_{k=t}^{}^{k -t}_{\{s_{k}=s_{i},a_{k}=a_{j}\}}\,|\,,P,s_{t},a_{t}, \]

where \(_{\{s_{k}=s_{i},a_{k}=a_{j}\}}\) is the indicator function returning \(1\) if \(s_{k}=s_{i}\) and \(a_{k}=a_{j}\), and 0 otherwise.

Prior work considered the SR relative to either an \(\)-greedy exploration policy  or a random policy [44; 61; 38]. Instead, we consider a different "successor policy" for every state-action pair \((s_{i},a_{j})\), and define the optimal successor policy as the one maximizing the respective S-function, i.e.,

\[^{*}_{s_{i}a_{j}}_{}\;S^{}_{s_{i}a_{j}}(s,a). \]

The switch from \(\) to \(\) is intentional: to maximize the S-function means to maximize the occurrence of \((s_{i},a_{j})\) under \(\) discounting, which can been seen as visiting \((s_{i},a_{j})\) as fast as possible from any other state.6 Thus, Eq. (9) is a suitable goal-conditioned policy \(^{*}\) discussed in Algorithm 1. Similar to the Q-function, we can learn an approximation of the S-functions using Q-Learning, i.e.,

\[_{s_{i}a_{j}}(s_{t},a_{t})(1-_{t})_{s_{i }a_{j}}(s_{t},a_{t})+_{t}(_{\{s_{t}=s_{i},a_{t}=a_{j}\}}+ _{_{s_{i}a_{j}}(s_{t+1},a)}). \]

Because the initial estimates of \(_{s_{i}a_{j}}\) can be inaccurate, in practice we let \(\) to be \(\)-greedy over \(_{s^{}a^{}}\). That is, at every time step, the agent selects the action \(a=_{a}_{s^{}a^{}}(s_{t},a)\) with probability \(1-\), or a random action otherwise. As discussed in the previous section, any \(\)-greedy policy with \(>0\) is sufficient to meet the criteria of Theorem 1.

### Directed Exploration via Successor-Functions: A Summary

Our exploration strategy can be applied to any off-policy algorithm where the temporal-difference update of Eq. (2) and (10) is guaranteed to convergence under the GLIE assumption (pseudocode for Q-Learning in Appendix A.5). Most importantly, our exploration does not suffer from the limitations of optimism discussed in Section 2.2 -- _the agent will eventually explore all state-action pairs even when rewards are partially observable, because exploration is not influenced by the Q-function (and, thus, by rewards), but fully driven by the S-function_. Consider the example in Figure 0(b) again, and an optimistic agent (i.e., with high initial \(\)) that learns a reward model and uses its estimate when rewards are not observable. As discussed, classic exploration strategies will likely fail -- their exploration is driven by \(\) that can be highly inaccurate, either because of its optimistic estimate or because the reward model queried for Q-updates is inaccurate (and will stay so without proper exploration). This can easily lead to premature convergence to a suboptimal policy (to collect the left coin). On the contrary, if the agent follows our exploration strategy it will always push the button, discover the large coin, and eventually learn the optimal policy -- it does not matter if \(\) is initialized optimistically high and would (wrongly) believe that collecting the left coin is optimal, because the agent follows \(\). And even if \(\) is initialized optimistically (or even randomly), the indicator reward \(1\) of Eq. (10) _is always observable_, thus the agent can always update \(\) at every step. As \(\) becomes more accurate, the agent will eventually explore optimally, i.e., maximizing visitation occurrences -- it will be able to visit states more efficiently, to observe rewards more frequently and update its reward model properly, and thus to update \(\) estimates with accurate rewards. To summarize, the explicitly decoupling of exploration (\(\)) and exploitation (\(\)) together with the use of SR (whose reward is always observable) is the key of our algorithm. Experiments in the next section empirically validate both the failure of classic exploration strategies and the success of ours. In Appendix B.4 we further report a deeper analysis on a benchmark similar to the example of Figure 1.

**Related Work.** In reward-free RL, Jin et al.  proposed an algorithm where the agent explores by maximizing SR rewards. After collecting a sufficient amount of data, the agent no longer interacts with the environment and uses the data to learn policies maximizing different reward functions. In model-based RL, the agent of Hu et al.  alternates between exploration and exploitation at every training episode. During exploration, the agent follows a goal-conditioned policy randomly sampledfrom a replay buffer with the goal of minimizing the number of actions needed to reach a goal. Finally, in sparse-reward RL, Parisi et al.  learn two value functions -- one using extrinsic sparse rewards, one using dense visitation-count rewards -- and combine them to derive a UCB-like exploration policy. While the use of SR of Jin et al. , the goal-conditioned policies of Hu et al. , and the interaction of two separate value functions of Parisi et al.  are related to our work, there are significant differences to what we presented in this paper. First, our directed exploration does not strictly separate exploration and exploitation into two phases  or between episodes , but rather interleaves them step-by-step using either the Q-function or the S-functions. However, the two value functions are never combined together , as the agent follows either one or the other according to the coefficient \(_{t}\). Second, our goal-conditioned policy is chosen according to the least-visited state-action pair, rather than randomly sampled from a set  or from a replay buffer . Third, none of them have considered the setting of Mon-MDPs and partially observable rewards.

## 4 Experiments

We validate our exploration strategy on tabular MDPs (Figure 4) characterized by different challenges, e.g., sparse rewards, distracting rewards, stochastic transitions. For each MDP, we propose the following Mon-MDP versions of increasing difficulty. The first has a simple **random monitor**, where positive and negative rewards are unobservable (\(^{}_{t}=\)) with probability \(50\%\) and observable otherwise (\(^{}_{t}=r^{}_{t}\)), while zero-rewards are always observable. In the second, the agent can **ask to observe** the current reward at a cost (\(r^{}_{t}=-0.2\)). In the third, the agent can turn monitoring ON/OFF by pushing a \(\)-norm. In the fourth, there are **many experts** the agent can ask for rewards from at a cost (\(r^{}_{t}=-0.2\)), but only a random one is available at the time. In the fifth and last, the agent has to **level up** the monitor state by doing a correct sequence of costly (\(r^{}_{t}=-0.2\)) monitor actions to observe rewards (a wrong action resets the level). In all Mon-MDPs, the optimal policy does not need monitoring (\(r^{}_{t}=-0.2\) to observe \(^{}_{t}=r^{}_{t}\)). However, prematurely doing so would prevent observing rewards and, thus, learning \(Q^{*}\). More experiments on more environments are presented in Appendix B.

**Baselines.** We evaluate Q-Learning with the following exploration policies (details in Appendix A.5): (1) ours; (2) greedy with optimistic initialization; (3) naive \(\)-greedy; (4) \(\)-greedy with count-based intrinsic reward ; (5) \(\)-greedy with UCB bonus ; (6) \(\)-greedy with Q-Counts . Note that if rewards and transitions are deterministic and fully observable, then (2) is very efficient , thus serves as best-case scenario baseline. For all algorithms, \(=0.99\) and \(_{t}\) starts at 1 and linearly decays to 0. Because in Mon-MDPs environment rewards are always unobservable for some monitor state-action pairs (e.g., when the monitor is OFF), all algorithms learn a reward model to replace \(^{}_{t}=\), initialized to random values in \([-0.1,0.1]\) and updated when \(^{}_{t}\).7

**Why Is This Hard?** Q-function updates rely on the reward model, but the model is inaccurate at the beginning. To learn the reward model and produce accurate updates the agent must perform suboptimal actions and observe rewards. This results in a vicious cycle in exploration strategies that rely on the Q-function: the agent must explore to learn the reward model, but the Q-function will mislead the agent and provide unreliable exploration (especially if optimistic).

**Results.** For each baseline, we test the _greedy policy_ at regular intervals (full details in Appendix A). Figure 5 shows that Our exploration outperforms all the baselines, being the only one converging to the optimal policy in the vast majority of the seeds for all environments and monitors. When

Figure 4: **Environments.** The goal is to collect the large coin (\(r^{}_{t}=1\)) instead of small “distracting” coins (\(r^{}_{t}=0.1\)). In Hazard, the agent must avoid quicksand (it prevents any movement) and toxic clouds (\(r^{}_{t}=-10\)). In One-Way, the agent must walk over toxic clouds (\(r^{}_{t}=-0.1\)) to get the large coin. In River Swim, the stochastic transition pushes the agent to the left. More details in Appendix A.2.

the environment is deterministic and rewards are always observable (first column), pure Optimism is optimal (as expected). However, if the environment is stochastic (River Swim) or rewards are unobservable, Optimism fails. Even just random observability of the reward (second column) is enough to cause convergence to suboptimal policies -- without observing rewards the agent must rely on its model estimates, but these are initially wrong and mislead \(\) updates, preventing exploration. Even though mitigated by the \(\)-randomness, exploration with Q-Counts, UCB, **Intrinsic Reward**, and **Naive** is suboptimal as well, as these baselines learn suboptimal policies in most of the seeds, especially in harder Mon-MDPs. On the contrary, Our exploration is only slightly affected by the rewards unobservability because it relies on \(\) rather than \(\) -- the agent visits all state-action pairs as uniformly as possible, observes rewards frequently, and ultimately learns accurate \(\) that make the policy optimal. Figure 6 strengthens these results, showing that indeed Our exploration observes significantly more rewards than the baselines in all Mon-MDPs -- because we decouple exploration and exploitation, and exploration does not depend on \(\), Our agent does not avoid suboptimal actions and discovers more rewards.8 More plots and discussion in Appendix B. Source code at [https://github.com/AmiThinks/mon_mdp_neurips24](https://github.com/AmiThinks/mon_mdp_neurips24).

## 5 Discussion

In this paper, we discussed the challenges of exploration when the agent cannot observe the outcome of its actions, and highlighted the limitations of existing approaches. While partial monitoring is a well-known and studied framework for bandits, prior work in MDPs with unobservable rewards is still limited. To fill this gap, we proposed a paradigm change in the exploration-exploitation trade-off and investigated its efficacy on Mon-MDPs, a general framework where the reward observability is governed by a separate process the agent interacts with. Rather than relying on optimism, intrinsic motivation, or confidence bounds, our approach explicitly decouples exploration and exploitation through a separate goal-conditioned policy that is fully in charge of exploration. We proved the convergence of this paradigm under some assumptions, presented a suitable goal-conditioned policy based on the successor representation, and validated it on a collection of MDPs and Mon-MDPs.

Figure 5: **Episode return \((r_{t}^{}+r_{t}^{})\) of greedy policies averaged over 100 seeds (shades denote 95% confidence interval). Our exploration clearly outperforms all baselines, as it is the only one learning in all Mon-MDPs. Indeed, while all baselines learn relatively quickly when rewards are fully observable (first column), their performance drastically decreases with rewards partial observability.**

**Advantages.** Directed exploration via the S-functions benefits from implicitly estimating the dynamics of the environment and being independent of the reward observability. First, the focus on the environment (states, actions, and dynamics) rather than on rewards or optimism (e.g., optimistic value functions) disentangles efficient exploration from imperfect or absent feedback -- not observing the rewards may compromise exploratory strategies that rely on them. Second, the goal-conditioned policy learns to explore the environment regardless of the reward, i.e., of a task. This has potential application in transfer RL, in the same spirit of prior use of the successor representation . Third, the goal-conditioned policy can guide the agent to any desired goal state-action pair, and by choosing the least-visited pair as the goal the agent implicitly explores as uniformly as possible.

**Limitations and Future Work.** First, we considered tabular MDPs, thus we plan to follow up on continuous MDPs. This will require extending the S-function and the visitation count to continuous spaces, e.g., by following prior work on universal value function approximators , successor features , and pseudocounts . For example, the finite set of S-functions \(_{s_{i}a_{j}}(s_{t},a_{t})\) could be replaced by a neural network \((s_{t},a_{t},s_{i},a_{j})\). For discrete actions, the network would take the (current state, goal state) pair and output the value for all (action, goal action) pairs, similarly to how deep Q-networks  work. Extending visitation counts to continuous spaces is more challenging, because Algorithm 1 relies on the \(\) operator to select the goal. To make it tractable, one solution would be to store samples into a replay buffer and prioritize sampling according to pseudocounts, similarly to what prioritized experience replay does with TD errors .

Second, our exploration policy requires some stochasticity to explore as we learn the S-functions. In our experiments, we used \(\)-greedy noise for a fair comparison against the baselines, but there may be better alternatives (e.g., "balanced wandering" ) that could improve our exploration.

Third, we assumed that the MDP has a bounded goal-relative diameter but this may not be true, e.g., in weakly-communicating MDPs . Thus, we will devote future work developing algorithms for less-constrained MDPs, following recent advances in convergence of reward-free algorithms .

Finally, we focused on model-free RL (i.e., Q-Learning) but we believe that our exploration strategy can benefit from model-based approaches. For example, we could use algorithms like UCRL _to learn the S-functions_ (not the Q-function), whose indicator reward is always observable. And, by learning the model of the monitor, the agent could plan _what rewards to observe_ (e.g., the least-observed) rather than what state-action pair to visit. Furthermore, our strategy could be combined with model-based methods where exploration is driven by uncertainty  rather than visits. That would allow the agent to plan visits to states where rewards are more likely to be observed.

Figure 6: **Number of rewards observed (\(^{k}_{t}\)) during training by the exploration policies. In Mon-MDPs, only Ours observes enough rewards because it does not avoid suboptimal actions.**